{"cell_type":{"72b4d4a9":"code","c904b6b8":"code","abb30557":"code","645b7879":"code","a0c2ae2c":"code","705a55bf":"code","c2fbd2e3":"code","66fbc736":"code","12dd4a11":"code","2864aaec":"code","8012122d":"code","d34f56df":"code","f74e64c1":"code","45546372":"code","2e8f8eaf":"code","22ba1d2a":"code","4cc7267b":"code","b3d6d579":"code","711e1f6d":"code","e6e3b888":"code","8c293535":"code","0a970b52":"code","fc5dce04":"code","645a361b":"code","10f5731d":"code","c5afe1a2":"code","35e400b1":"code","b53e42df":"code","f022be8e":"code","36365dc3":"code","ae71cbe3":"code","5efc0d10":"code","8e5a7941":"code","51d36a1e":"markdown","cab3f255":"markdown","688bdb73":"markdown","0ac26e2b":"markdown","4fed46ff":"markdown","2ade950a":"markdown","86e0c6a2":"markdown","b13b1df4":"markdown","3cca559a":"markdown","d7918224":"markdown","69f1e0de":"markdown","2fe2fd90":"markdown","0a43cf1e":"markdown","3592e9fe":"markdown","08893599":"markdown","952e3ec3":"markdown","add52a77":"markdown","1b6c65ec":"markdown","2d185f71":"markdown","67024eaa":"markdown","abf14847":"markdown","27f2cf47":"markdown","b913ce9f":"markdown","b12d129d":"markdown","d0d79460":"markdown","a177b811":"markdown","eaba4851":"markdown","40f5d113":"markdown","b8015c2c":"markdown","f253d842":"markdown","2e1fd508":"markdown","ebc94199":"markdown","0862157a":"markdown","50202b3e":"markdown"},"source":{"72b4d4a9":"import warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet, BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.cluster import DBSCAN\n\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_log_error, make_scorer\n\nfrom scipy.stats import *\nfrom scipy.special import boxcox1p\nfrom scipy.optimize import minimize_scalar\nimport statsmodels.api as sm\nimport statsmodels.stats.outliers_influence as oi\n\nimport scipy.stats as stats\n\n\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\n#Options\nrandom_state = 10\npd.options.display.max_rows = 999\npd.options.display.max_columns = 999\nwarnings.filterwarnings(\"ignore\")","c904b6b8":"def col_types(df, drop_feats = []):\n    data_describe = df.describe(include=\"all\")\n    if len(drop_feats):\n        cat_cols = [c for c in df.drop(drop_feats, axis=1).columns if df[c].dtype.name == 'object']\n        num_cols = [c for c in df.drop(drop_feats, axis=1).columns if df[c].dtype.name != 'object']\n    else:\n        cat_cols = [c for c in df.columns if df[c].dtype.name == 'object']\n        num_cols = [c for c in df.columns if df[c].dtype.name != 'object']\n        \n    bin_cols    = [c for c in cat_cols if data_describe[c]['unique'] == 2]\n    nonbin_cols = [c for c in cat_cols if data_describe[c]['unique'] > 2]\n    return cat_cols, num_cols, bin_cols, nonbin_cols","abb30557":"path = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\"\n#path= \"\"   #local path\n\ntrain = pd.read_csv(path + 'train.csv', index_col='Id')\n\nX_test_s = pd.read_csv(path + \"test.csv\", index_col='Id')\ny_test_s = pd.read_csv(path + \"sample_submission.csv\", index_col='Id')\ntest = pd.merge(y_test_s, X_test_s, how='inner', left_index=True, right_index=True)\n\ntrain_idx = train.index\ntest_idx = test.index\n\ndata = pd.concat([train, test], axis=0, sort=False)","645b7879":"fig, ax = plt.subplots(figsize=(20,4), nrows=1, ncols=2)\nax[0].set_title(\"SalePrice\")\nax[1].set_title(\"QQ Plot\")\nsns.distplot(data.loc[train_idx, \"SalePrice\"], bins=40, ax=ax[0], norm_hist=True, kde=True).grid()\nfig = sm.qqplot(data.loc[train_idx, \"SalePrice\"], stats.t, fit=True, line='45', ax=ax[1])","a0c2ae2c":"data[\"SalePrice\"] = np.log(data[\"SalePrice\"])","705a55bf":"fig, ax = plt.subplots(figsize=(20,4), nrows=1, ncols=2)\nax[0].set_title(\"SalePrice\")\nax[1].set_title(\"QQ Plot\")\nsns.distplot(data.loc[train_idx, \"SalePrice\"], bins=40, ax=ax[0], norm_hist=True, kde=True).grid()\nfig = sm.qqplot(data.loc[train_idx, \"SalePrice\"], stats.t, fit=True, line='45', ax=ax[1])","c2fbd2e3":"feats = [\"MSZoning\", \"Electrical\", \"Functional\", \"Utilities\", \"SaleType\", \"KitchenQual\", \"Exterior2nd\", \"Exterior1st\"]\nfor feat in feats:\n    data[feat].fillna(data[feat].mode()[0], inplace=True)","66fbc736":"feats = [\"GarageType\", \"GarageYrBlt\", \"GarageFinish\", \"GarageCars\", \"GarageArea\", \"GarageQual\", \"GarageCond\", \n         \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinSF1\", \"BsmtFinType2\", \"BsmtFinSF2\", \n         \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtHalfBath\", \"BsmtFullBath\", \"Fireplaces\", \"FireplaceQu\", \"MasVnrArea\", \"MasVnrType\",\n        \"Alley\", \"Fence\", \"MiscFeature\", \"MiscVal\", \"PoolQC\", \"PoolArea\", \"LotFrontage\"]\nfor feat in feats:\n    data[feat].fillna(0, inplace=True)","12dd4a11":"data['LotFrontage'] = data.groupby(['MSSubClass', \"MSZoning\"])['LotFrontage'].transform(lambda x: x.fillna(x.mean()))","2864aaec":"data[\"hasGarage\"] = 1\ndata.loc[data[\"GarageArea\"]==0, [\"hasGarage\"]]=0\ndata[\"hasGarage\"] = data[\"hasGarage\"].astype(object)\n\ndata[\"hasBsmt\"] = 1\ndata.loc[data[\"TotalBsmtSF\"]==0, [\"hasBsmt\"]]=0\ndata[\"hasBsmt\"] =  data[\"hasBsmt\"].astype(object)\n\ndata[\"hasPool\"] = 1\ndata.loc[data[\"PoolArea\"]==0, [\"hasPool\"]]=0\ndata[\"hasPool\"] = data[\"hasPool\"].astype(object)\n\ndata[\"has2Floor\"] = 0\ndata.loc[data[\"2ndFlrSF\"]>0, [\"has2Floor\"]]=1\ndata[\"has2Floor\"] = data[\"has2Floor\"].astype(object)","8012122d":"data['LivingAreaSF'] = data[\"GrLivArea\"] + data[\"TotalBsmtSF\"] + data[\"MasVnrArea\"] ","d34f56df":"#drop outliars \ntrain_idx = train_idx.drop(train[train[\"GrLivArea\"]>4500].index)","f74e64c1":"data[\"AgeOnSoldMoment\"] = data[\"YrSold\"]-data[\"YearBuilt\"]\ndata[\"RemodAgeOnSoldMoment\"] = data[\"YrSold\"]-data[\"YearRemodAdd\"]\ndata[\"GarageOnSoldMoment\"] = data[\"YrSold\"]-data[\"GarageYrBlt\"]\n\n\n#fix some negative values\ndata.loc[data[\"AgeOnSoldMoment\"]<0,\"AgeOnSoldMoment\"]=0\ndata.loc[data[\"RemodAgeOnSoldMoment\"]<0,\"RemodAgeOnSoldMoment\"]=0\ndata.loc[data[\"GarageOnSoldMoment\"]<0,\"GarageOnSoldMoment\"]=0\ndata.loc[data[\"GarageOnSoldMoment\"]>250,\"GarageOnSoldMoment\"]=0","45546372":"data[\"Baths\"] = data[\"HalfBath\"] + data[\"FullBath\"]  + data[\"BsmtFullBath\"] + data[\"BsmtHalfBath\"]","2e8f8eaf":"data[\"PorchSF\"] = data[\"OpenPorchSF\"]+data[\"ScreenPorch\"] +data[\"3SsnPorch\"] +data[\"EnclosedPorch\"]\n\ndata[\"hasPorch\"] = 0\ndata.loc[data[\"PorchSF\"]>0, [\"hasPorch\"]]=1\ndata[\"hasPorch\"] = data[\"hasPorch\"].astype(object)","22ba1d2a":"data[\"hasWoodDeck\"] = 0\ndata.loc[data[\"WoodDeckSF\"]>0, [\"hasWoodDeck\"]]=1\ndata[\"hasWoodDeck\"] = data[\"hasWoodDeck\"].astype(object)","4cc7267b":"#data.drop([\"GrLivArea\", \"TotalBsmtSF\", \"MasVnrArea\"], axis=1, inplace=True, errors='ignore')\n#data.drop([\"1stFlrSF\", \"2ndFlrSF\"], axis=1, inplace=True, errors='ignore')\n#data.drop([\"OpenPorchSF\", \"ScreenPorch\",\"3SsnPorch\", \"EnclosedPorch\"], axis=1, inplace=True, errors='ignore')\n#data.drop([\"YearBuilt\", \"YearRemodAdd\"], axis=1, inplace=True, errors='ignore')\n#data.drop([\"HalfBath\", \"FullBath\",\"BsmtFullBath\", \"BsmtHalfBath\"], axis=1, inplace=True, errors='ignore')","b3d6d579":"data[\"MSSubClass\"] = data[\"MSSubClass\"].astype(object)\ndata[\"MoSold\"] = data[\"MoSold\"].astype(object)\ndata[\"YrSold\"] = data[\"YrSold\"].astype(object)","711e1f6d":"def optimize_skew(df, edge=0.5):\n    coeffs = {}\n    feat_skew = df.skew()\n    correction_list = feat_skew[abs(feat_skew)>edge].index.tolist()\n    for feat in correction_list:\n        coef = boxcox_normmax(df[feat] + 1)\n        coeffs[feat] = coef\n    return coeffs\n\ndef skew_corr_info(df, target):\n    skew_value = np.mean(np.abs(df.skew()))\n    corr_value = 0.0\n    df_corr = pd.merge(df, target, how='outer', left_index=True, right_index=True)\n    df_corr = df_corr.corr().iloc[-1,:][:-1]\n    corr_value = np.mean(np.abs(df_corr))\n    print(\"Corr: {0:.4}  |  Skew: {1:.4}\".format(corr_value, skew_value))\n    #return skew_value, corr_value\n    ","e6e3b888":"#My solution may seem a little unusual, but I tried several options and in final left some of the universal code to correct the skewness\n\ncat_cols, num_cols, bin_cols, nonbin_cols = col_types(data, drop_feats=['SalePrice'])\n\nskew_corr_info(data.loc[train_idx, num_cols], data.loc[train_idx, \"SalePrice\"])\ncoeffs = optimize_skew(data.loc[train_idx, num_cols], edge=0.5)\n\nfor key in coeffs:\n    data[key] = boxcox1p(data[key], coeffs[key])\n    \nskew_corr_info(data.loc[train_idx, num_cols], data.loc[train_idx, \"SalePrice\"])","8c293535":"display(data.head(10))","0a970b52":"cat_cols, num_cols, bin_cols, nonbin_cols = col_types(data, drop_feats=['SalePrice'])\n\ndisplay(data[num_cols].head())\ndisplay(data[bin_cols].head())\ndisplay(data[nonbin_cols].head())","fc5dce04":"data_work = data.copy()\n\nscaler = RobustScaler()\n\ndata_X = data_work.drop(['SalePrice'], axis=1)\ndata_y = data_work['SalePrice']\n\nscaler.fit(data_X.loc[train_idx, num_cols].values)\n\ndata_X[num_cols] = scaler.transform(data_X[num_cols].values)\n\ndata_X = pd.get_dummies(data_X, columns=cat_cols) #, drop_first=True\n\n\nX = data_X.loc[train_idx, :]\ny = data_y.loc[train_idx]\n\ntest_X = data_X.loc[test_idx, :]\ntest_y = data_y.loc[test_idx]\n\ndisplay(X.head())\ndisplay(test_X.head())","645a361b":"#Hyperopt functions\n\ndef rmsle_func(y_test, y_pred):\n    return np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_pred)))\n\nrmsle_scorer = make_scorer(rmsle_func)\n    \ndef hyperopt_run(X_, y_, params):\n    try:\n        Model = globals()[params.pop(\"model\")]\n        if 'mask' in params:\n            mask = np.array(params.pop('mask'))\n            feats = X_.columns[mask]\n        else:\n            feats = X_.columns\n        \n        model = Model(**params)\n        score = cross_val_score(model, X_[feats], y_, cv=4, scoring=rmsle_scorer)\n        return score.mean()\n    \n    except Exception as ex :\n        print(ex)\n        return np.inf\n\ndef f_model(params):\n    global best\n    global best_params\n    acc = hyperopt_run(X, y, params.copy())\n    if (acc < best):\n        best = acc\n        best_params = params\n        print(\"new best: {0:.4} {1}\".format(best, params))\n    return {'loss': acc, 'status': STATUS_OK}","10f5731d":"space = {\n        'model': 'Lasso',\n        'alpha': hp.uniform('alpha', 0, 2.0)\n    }\n\nbest,  best_params = np.inf,None \nres = fmin(f_model, space, algo=tpe.suggest, max_evals=200, rstate=np.random.RandomState(random_state))\nprint(\"---------------------------\\nBest_params: \\n\", best_params)\n\nModel = globals()[best_params.pop(\"model\")]\nmp_alpha = best_params[\"alpha\"]\nm_lasso = Model(random_state=random_state, **best_params)\nm_lasso.fit(X,y)\nfeat_mask = m_lasso.coef_ != 0\n","c5afe1a2":"fig, ax = plt.subplots(figsize=(29,5), ncols=3)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=33)\nm_lasso_ = Model(random_state=random_state, **best_params)\nm_lasso_.fit(X_train,y_train)\n\ny_pred = m_lasso_.predict(X_test)\nresid = y_test - y_pred\n\nax[0] = sns.scatterplot(x=y_pred, y=y_test, ax=ax[0], hue = X_test.loc[:, \"OverallQual\"])\nax[1] = sns.scatterplot(y=resid, x=y_pred, ax=ax[1], hue = X_test.loc[:, \"OverallQual\"])\nfig = sm.qqplot(resid, stats.t, fit=True, line='45', ax=ax[2])","35e400b1":"model = sm.OLS(y, X.loc[:,feat_mask])\nfitted = model.fit()\nprint(fitted.summary())","b53e42df":"influence = fitted.get_influence()\n(c, p) = influence.cooks_distance\ndistances = pd.DataFrame(c, index=train_idx)\n\n#fig, ax = plt.subplots(figsize=(24,8), ncols=1)\n#ax.set_yscale('log')\n#plt.stem(np.arange(len(c)), c, markerfmt=\",\")","f022be8e":"drop_idx = distances[distances[0]>0.01].index\nprint(drop_idx)\ntrain_idx = train_idx.drop(drop_idx)\n\nX, y = X.loc[train_idx,:], y.loc[train_idx]","36365dc3":"space = {\n        'model': 'Lasso',\n        'alpha': hp.uniform('alpha', 0, 0.001)\n    }\n\nbest,  best_params = np.inf,None \nres = fmin(f_model, space, algo=tpe.suggest, max_evals=20, rstate=np.random.RandomState(random_state))\nprint(\"---------------------------\\nBest_params: \\n\", best_params)\n\nModel = globals()[best_params.pop(\"model\")]\nmp_alpha = best_params[\"alpha\"]\nm_lasso = Model(random_state=random_state, **best_params)\nm_lasso.fit(X,y)\n","ae71cbe3":"fig, ax = plt.subplots(figsize=(29,5), ncols=3)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=33)\nm_lasso_ = Model(random_state=random_state, **best_params)\nm_lasso_.fit(X_train,y_train)\n\ny_pred = m_lasso_.predict(X_test)\nresid = y_test - y_pred\n\nax[0] = sns.scatterplot(x=y_pred, y=y_test, ax=ax[0], hue = X_test.loc[:, \"OverallQual\"])\nax[1] = sns.scatterplot(y=resid, x=y_pred, ax=ax[1], hue = X_test.loc[:, \"OverallQual\"])\nfig = sm.qqplot(resid, stats.t, fit=True, line='45', ax=ax[2])","5efc0d10":"y_pred = m_lasso.predict(test_X)\nres = pd.DataFrame(np.exp(y_pred), index = test_X.index, columns=['SalePrice'])\nres.to_csv('Submission.csv')\nprint(\">>>subm file saved<<<\")","8e5a7941":"#y_pred = m_lasso.predict(X)\n#fig, ax = plt.subplots(figsize=(29,5), ncols=3)\n\n#resid = y - y_pred\n#ax[0] = sns.scatterplot(x=y_pred, y=y, ax=ax[0], hue = data.loc[train_idx, \"OverallQual\"])\n##ax[1] = sns.residplot(y_pred, y, ax=ax[1])\n#ax[1] = sns.scatterplot(y=resid, x=y_pred, ax=ax[1], hue = data.loc[train_idx, \"OverallQual\"])\n#fig = sm.qqplot(resid, stats.t, fit=True, line='45', ax=ax[2])\n#print(stats.kstest(resid, 'norm'))","51d36a1e":"<a class=\"anchor\" id=\"45\"><\/a>\n## Skew correction\n[on top](#0)","cab3f255":"### With speciefic values","688bdb73":"<a class=\"anchor\" id=\"41\"><\/a>\n## Target variable \n[on top](#0)","0ac26e2b":"### hasFeature","4fed46ff":"### Year group","2ade950a":"<a class=\"anchor\" id=\"63\"><\/a>\n## Residual analysis\n[on top](#0)","86e0c6a2":"### With 0","b13b1df4":"<a class=\"anchor\" id=\"46\"><\/a>\n## Result\n[on top](#0)","3cca559a":"<a class=\"anchor\" id=\"50\"><\/a>\n# Modelling #1... and feature selection\nFirst approach.\n\nI chose a model with regularization - lasso regression. In this section, I will simply train the model and select the best alpha for adjusting the regularization strength. The coefficients of the model will allow us to select the best features that we use in the next section.\n\n\n[on top](#0)","d7918224":"### Living Area","69f1e0de":"<a class=\"anchor\" id=\"43\"><\/a>\n## New features\n[on top](#0)","2fe2fd90":"<a class=\"anchor\" id=\"40\"><\/a>\n# Data transformation\nIn this section I transformed the dataset using the knowledge gained from performing EDA\n\nInformation about the year (build, remod), I replaced the age at the time of sale. It seems to me that this approach made it possible to better predict the final price.\n\n[on top](#0)","0a43cf1e":"<a class=\"anchor\" id=\"30\"><\/a>\n# EDA\nYou can find my exploratory data analysis (EDA) by clicking on [this link](https:\/\/www.kaggle.com\/zosimovaa\/house-prices-eda).\n\nI did not cite EDA here, so as not to amplify the volume of the notebook. Based on the conclusions of EDA, I built a training dataset.\n\n[on top](#0)","3592e9fe":"<a class=\"anchor\" id=\"20\"><\/a>\n# Read the data\n[on top](#0)","08893599":"<a class=\"anchor\" id=\"12\"><\/a>\n## Useful functions\n[on top](#0)","952e3ec3":"### WoodDeck","add52a77":"<a class=\"anchor\" id=\"51\"><\/a>\n## Dataset\n[on top](#0)","1b6c65ec":"<a class=\"anchor\" id=\"70\"><\/a>\n# Prediction\n[on top](#0)","2d185f71":"<a class=\"anchor\" id=\"42\"><\/a>\n## Missing values\n[on top](#0)","67024eaa":"<a class=\"anchor\" id=\"53\"><\/a>\n## Residual analysis\n[on top](#0)","abf14847":"<a class=\"anchor\" id=\"61\"><\/a>\n## Outliars\n[on top](#0)","27f2cf47":"<a class=\"anchor\" id=\"62\"><\/a>\n## Model fitting\n[on top](#0)","b913ce9f":"### Porch","b12d129d":"<a class=\"anchor\" id=\"60\"><\/a>\n# Outliars... and model fitting #2\n\nTo increase the accuracy of the model, I want to remove the samples \u200b\u200bthat have the most influence - outliers. This will help me Cook's distance .\n\nTo do this, I fitted OLS regression on the features obtained in the previous step.\n\nThe threshold for removing outliers I selected experimentally, this is another possible way to improve the model. \n\n\n[on top](#0)","d0d79460":"# Introducing\n\n### Hi everyone!\n\n![The picture to attract attention](https:\/\/marketinginsiders.com\/wp-content\/uploads\/2017\/07\/how-to-get-attention-on-social-media.png)\n(The picture to attract attention)\n\nIn the beginning I would like to say a few words about my solution.\n\nThe main idea of my solution is - get Lasso regression, tune it with hyperopt, select features and remove the outliers according to the Cook's distance value, fit Lasso regression again. This allowed me to enter the **TOP 8%** results. Not perfect, but not so bad :)\n\nIn each section you can find a brief description of what is happening in this block.\n\n\nIn the future I'm going to try training other models(e.g. randomforest, xgboost) and model stacking technique to improove my results.\n\n\nI would be glad for your comments and criticism, that will be very helful for me!\n\nThanks for your time","a177b811":"<a class=\"anchor\" id=\"0\"><\/a>\n## TABLE OF CONTENTS\n\n1. [Preparing](#10)\n    1. [Imports](#11)\n    1. [Useful functions](#12)\n1. [Read the data](#20)\n1. [EDA](#30)\n1. [Data transformation](#40)\n    1. [Target variable](#41)\n    1. [Missing values](#42)\n    1. [New features](#43)\n    1. [Types](#44)\n    1. [Skew correction](#45)\n    1. [Result](#46)\n1. [Modelling #1... and feature selection](#50)    \n    1. [Dataset](#51)\n    1. [Model fitting](#52)\n    1. [Residual analysis](#53)\n1. [Outliars... and model fitting #2](#60)    \n    1. [Outliars](#61)\n    1. [Model fitting](#62)\n    1. [Residual analysis](#63)\n1. [Predictions](#70)","eaba4851":"# Houses prices with Lasso - TOP 8%\n\n(train rmsle 0.09001 | test rmsle 0.11467)\n\n\n\n","40f5d113":"<a class=\"anchor\" id=\"11\"><\/a>\n## Imports\n[on top](#0)","b8015c2c":"<a class=\"anchor\" id=\"52\"><\/a>\n## Model fitting\n[on top](#0)","f253d842":"<a class=\"anchor\" id=\"10\"><\/a>\n# Preparing \n\nNothing special - just imports and function defining. [on top](#0)","2e1fd508":"### With mode","ebc94199":"### Baths","0862157a":"### Drop duplicate features","50202b3e":"<a class=\"anchor\" id=\"44\"><\/a>\n## Types\n[on top](#0)"}}