{"cell_type":{"16a1cfa9":"code","cfd493fe":"code","677f4583":"code","90ab8624":"code","f4a9cf31":"code","81864c31":"code","05ee4d3d":"code","3f69a0aa":"code","1eecd976":"code","d33526b7":"code","0982b1f9":"code","27a57fc2":"code","0be8fe28":"code","309d56ac":"code","475df8dd":"code","81264ff5":"code","24ad4e95":"code","95ae0da6":"code","fa921e13":"code","d85b4049":"code","43499090":"code","938068fb":"code","a3e4428d":"code","2adb8c64":"code","83b36742":"code","573d1e3f":"code","ad898763":"code","d64d23e6":"code","631dc257":"code","97f23d61":"code","88a17e0f":"code","23a4d46f":"code","d81d3f9f":"code","ce3e421d":"code","831c68e2":"code","b3a83490":"code","a1e9b007":"code","1bcb2a82":"code","4f349fdb":"code","9128bbff":"code","13b72e85":"code","3b037b5f":"code","098b42a8":"code","09bf06bd":"code","fc80e16f":"code","fee6837b":"code","3ddef542":"code","87375935":"code","0392a9e8":"markdown","3669496f":"markdown","f40f106c":"markdown","562a8f2b":"markdown","e3961bd7":"markdown","d607842d":"markdown","77a2730e":"markdown","587144a4":"markdown","01d2fd82":"markdown","2e78ca77":"markdown"},"source":{"16a1cfa9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nfrom sklearn.linear_model import Ridge, Lasso, LinearRegression\nimport lightgbm as lgb\nfrom sklearn.ensemble import AdaBoostRegressor\nimport catboost as cgb\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nfrom sklearn.ensemble import StackingRegressor\n\nfrom sklearn.metrics import mean_absolute_error\n\nfrom matplotlib import pyplot as plt \n%matplotlib inline\nimport seaborn as sns","cfd493fe":"# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\ntrain_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv') ","677f4583":"train_df.info()","90ab8624":"train_df.describe()","f4a9cf31":"plt.figure(figsize=(20,10))\nsns.heatmap(train_df.corr(),annot=True)\nplt.show()","81864c31":"np.linalg.det(train_df.corr())","05ee4d3d":"X_cols = [x for x in train_df.columns.tolist() if 'cont' in x]\nplt.figure(figsize=(20,10))\nsubplot_count = 1\nfor i in range(7): \n    for j in range(2): \n        plt.subplot(2, 7, subplot_count)\n        train_df[X_cols[subplot_count-1]].plot.box()\n        subplot_count += 1\nplt.show()","3f69a0aa":"test_df.info()","1eecd976":"test_df.describe()","d33526b7":"plt.figure(figsize=(20,10))\nsns.heatmap(test_df.corr(),annot=True)\nplt.show()","0982b1f9":"np.linalg.det(test_df.corr())","27a57fc2":"# Code from https:\/\/www.kaggle.com\/tosinabase\/jan-21-regularized-regression-ridge-and-lasso\n\ny = train_df['target']\nX_lr = train_df.drop(['id', 'target'], axis=1)\n\nscaler = StandardScaler()\nscaler.fit(X_lr)\n\nX_lr = scaler.transform(X_lr)\nX_lr_test = scaler.transform(test_df.drop('id', axis=1).values)\n\nX_train_lr, X_val_lr, y_train_lr, y_val_lr = train_test_split(X_lr, y, test_size=0.3, random_state=17, shuffle=False)","0be8fe28":"y = train_df['target']\nX = train_df.drop(['target'], axis=1)\n\nX_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X, y, test_size =0.3, shuffle=False)\n\ndel train_df","309d56ac":"m1 = Lasso(alpha=0.001, random_state=123)","475df8dd":"m1_fit = m1.fit(X_train_lr, y_train_lr)\nprint('Score reached: {} '.format(m1.score(X_train_lr, y_train_lr)))\n\n# Score without scaling: 0.0176265428750495 \n# Score with scaling: 0.01859676573900082 ","81264ff5":"X_test_lr = test_df.drop(['id'], axis=1)\ny_test_lasso = m1.predict(X_test_lr)","24ad4e95":"plt.figure(figsize=(20,10))\nplt.bar(height=m1_fit.coef_, x=X.columns.values[1:])\nplt.title(\"Feature importances via coefficients\")\nplt.show()","95ae0da6":"# Parameter from https:\/\/www.kaggle.com\/tosinabase\/jan-21-regularized-regression-ridge-and-lasso\nm2 = Ridge(alpha=0.1)","fa921e13":"m2_fit = m2.fit(X_train_lr, y_train_lr)\nprint('Score reached: {} '.format(m2.score(X_train_lr, y_train_lr)))\n# Score 0.01865954277402282 ","d85b4049":"X_test_lr = test_df.drop(['id'], axis=1)\ny_test_ridge = m2.predict(X_test_lr)","43499090":"plt.figure(figsize=(20,10))\nplt.bar(height=m2_fit.coef_, x=X.columns.values[1:])\nplt.title(\"Feature importances via coefficients\")\nplt.show()","938068fb":"lgb_train = lgb.Dataset(X_train_df, y_train_df, free_raw_data=False)\nlgb_eval = lgb.Dataset(X_val_df, y_val_df, free_raw_data=False)","a3e4428d":"# # param values c.f. https:\/\/www.kaggle.com\/zephyrwang666\/riiid-lgbm-bagging2\n# param = {'num_leaves': sp_randint(10, 500), 'n_estimators': sp_randint(10, 6000), 'max_bin':sp_randint(100, 800), 'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4], \n#          'feature_fraction': sp_uniform(0, 1), 'bagging_fraction': sp_uniform(0, 1), \"bagging_seed\": [47], \n#          'objective': ['regression'], 'max_depth': [-1], \n#          'learning_rate': sp_uniform(0, 1), \"boosting_type\": [\"gbdt\"], \n#          'metric': ['rmse'], \"verbosity\": [-1], \n#          'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100], 'reg_lambda': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100], \n#          'random_state': [47]}\n\n# m3 = lgb.LGBMRegressor(verbose_eval = 30, num_boost_round = 10000, early_stopping_rounds = 10, n_estimators=3000)\n\n# '''\n# Hyperparameter optimisation\n# '''\n# # Code from https:\/\/www.kaggle.com\/rtatman\/lightgbm-hyperparameter-optimisation-lb-0-761#Model-fitting-with-HyperParameter-optimisation\n# #This parameter defines the number of hyperparameter points to be tested\n# n_HP_points_to_test = 150\n\n# gsLGBM = RandomizedSearchCV(\n#     estimator=m3, param_distributions=param, \n#     n_iter=n_HP_points_to_test,\n#     cv=5,\n#     refit=True,\n#     random_state=47,\n#     verbose=True)","2adb8c64":"# gsLGBM.fit(X_train_df, y_train_df, eval_set = (X_val_df, y_val_df), eval_metric = 'rmse')\n# print('Best score reached: {} with params: {} '.format(gsLGBM.best_score_, gsLGBM.best_params_))","83b36742":"gsLGBM.best_params_\n# Best score: 0.0937872506205801\n# opt_parameters_LGBM = {'bagging_fraction': 0.7997942505658034,\n#  'bagging_seed': 47, 'boosting_type': 'gbdt',\n#  'feature_fraction': 0.31477581669804067, 'learning_rate': 0.03875307567633712,\n#  'max_bin': 491, 'max_depth': -1,\n#  'metric': 'rmse', 'min_child_weight': 100.0,\n#  'n_estimators': 2559, 'num_leaves': 272,\n#  'objective': 'regression',\n#  'random_state': 47, 'reg_alpha': 10,\n#  'reg_lambda': 1, 'verbosity': -1}","573d1e3f":"m3 = lgb.LGBMRegressor(valid_sets = [lgb_train, lgb_eval], verbose_eval = 30, num_boost_round = 10000, early_stopping_rounds = 10, n_jobs=4, **opt_parameters_LGBM)\nm3.fit(X_train_df, y_train_df, eval_set = (X_val_df, y_val_df), eval_metric = 'rmse')","ad898763":"X_test = test_df\ny_test_lgbm = m3.predict(X_test)","d64d23e6":"plt.figure(figsize=(20,10))\nlightgbm.plot_importance(m3)\nplt.title(\"Feature importances\")\nplt.show()","631dc257":"# m4 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=3, min_samples_leaf=1, min_impurity_decrease=10, random_state=47), random_state=47)","97f23d61":"# param = {'learning_rate': sp_uniform(0, 1), 'n_estimators': sp_randint(5, 100)}\n\n# '''\n# Hyperparameter optimisation\n# '''\n# # Code from https:\/\/www.kaggle.com\/rtatman\/lightgbm-hyperparameter-optimisation-lb-0-761#Model-fitting-with-HyperParameter-optimisation\n# # This parameter defines the number of HP points to be tested\n# n_HP_points_to_test = 50\n\n# gsADA = RandomizedSearchCV(\n#     estimator=m4, param_distributions=param, \n#     n_iter=n_HP_points_to_test,\n#     cv=3,\n#     refit=True,\n#     random_state=47,\n#     verbose=True)","88a17e0f":"# gsADA.fit(X_train_df, y_train_df)\n# print('Best score reached: {} with params: {} '.format(gsADA.best_score_, gsADA.best_params_))","23a4d46f":"# Just in case, the parameters should be printed in here. \n# Score: -3.3217493390580444e-05\nopt_parameters_ADA = {'learning_rate': 0.028555288989857153, 'n_estimators': 36} ","d81d3f9f":"m4 = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=3, min_samples_leaf=1, min_impurity_decrease=10, random_state=47), random_state=47, **opt_parameters_ADA)\nm4.fit(X_train_df, y_train_df)","ce3e421d":"X_test = test_df\ny_test_ada = m4.predict(X_test)","831c68e2":"m5 = CatBoostRegressor(random_seed=47)","b3a83490":"param = {'learning_rate': sp_uniform(0, 1), 'n_estimators': sp_randint(5, 100), eta=sp_uniform(0, 1), num_trees=sp_randint(5, 100)}\n\n'''\nHyperparameter optimisation\n'''\n# Code from https:\/\/www.kaggle.com\/rtatman\/lightgbm-hyperparameter-optimisation-lb-0-761#Model-fitting-with-HyperParameter-optimisation\n# This parameter defines the number of HP points to be tested\nn_HP_points_to_test = 50\n\ngsCB = RandomizedSearchCV(\n    estimator=m5, param_distributions=param, \n    n_iter=n_HP_points_to_test,\n    cv=3,\n    refit=True,\n    random_state=47,\n    verbose=True)","a1e9b007":"gsCB.fit(X_train_df, y_train_df)\nprint('Best score reached: {} with params: {} '.format(gsCB.best_score_, gsCB.best_params_))","1bcb2a82":"X_test = test_df\ny_test_cb = m5.predict(X_test)","4f349fdb":"# Code from https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python#Second-Level-Predictions-from-the-First-level-Output\ngbm = xgb.XGBRegressor(\n learning_rate = 0.01,\n n_estimators= 100,\n max_depth= 4,\n min_child_weight= 2,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'reg:squaredlogerror',\n nthread= -1,\n verbosity=3,\n random_state=20)\n\n# Code from https:\/\/stackoverflow.com\/questions\/65713104\/sklearn-asking-for-eval-dataset-when-there-is-one\/65714374?noredirect=1#comment116194594_65714374\nlgbm_params = m1.get_params()\n\n# remove early_stopping_rounds as your model is already fitted the data\nlgbm_params[\"early_stopping_rounds\"] = None\nm1.set_params(**lgbm_params)\n\nestimators = [('lgbm', m1), ('ada', m2), ('lasso', m3), ('ridge', m4)]\n\ngbm = StackingRegressor(estimators=estimators, final_estimator=gbm, cv=5, verbose=1)","9128bbff":"# del m1\n# del m2\n# del gsLGBM\n# del gsADA\n# del opt_parameters_LGBM\n# del opt_parameters_ADA","13b72e85":"gbm.fit(X_train_df, y_train_df)","3b037b5f":"# gbm.score(X_train_df, y_train_df)","098b42a8":"X_test = test_df\ny_test_gbm = gbm.predict(X_test)","09bf06bd":"lasso_submission = pd.DataFrame({'id': test_df['id'], 'target': y_test_lasso})\nlasso_submission.to_csv('lasso_submission.csv', index=False)","fc80e16f":"ridge_submission = pd.DataFrame({'id': test_df['id'], 'target': y_test_ridge})\nridge_submission.to_csv('ridge_submission.csv', index=False)","fee6837b":"lgbm_submission = pd.DataFrame({'id': test_df['id'], 'target': y_test_lgbm})\nlgbm_submission.to_csv('lgbm_submission.csv', index=False)","3ddef542":"ada_submission = pd.DataFrame({'id': test_df['id'], 'target': y_test_ada})\nada_submission.to_csv('ada_submission.csv', index=False)","87375935":"gbm_submission = pd.DataFrame({'id': test_df['id'], 'target': y_test_gbm})\ngbm_submission.to_csv('gbm_submission.csv', index=False)","0392a9e8":"## Lasso","3669496f":"## Ridge","f40f106c":"# EDA","562a8f2b":"## Ensembling the Models","e3961bd7":"## ADABoost","d607842d":"## LightGBM","77a2730e":"# Submission","587144a4":"# Model","01d2fd82":"## CatBoost","2e78ca77":"## Data Processing for Lasso and Ridge Regression"}}