{"cell_type":{"ed5fdaba":"code","358b5f1a":"code","b275781c":"code","b8e347dc":"code","9faedcda":"code","16c10174":"code","de1ab847":"code","7959141b":"code","ab195eb0":"code","4ef7ce8f":"code","5ffc691d":"code","d1645470":"code","8a6fe7bc":"code","a90b3bdb":"code","9c5452ab":"code","95983773":"code","7eb3f7bf":"code","e57a0896":"code","6e018ab5":"code","4a90ee6d":"markdown","16f2e7ba":"markdown","f01f53c0":"markdown","08ace7ff":"markdown","12eb4584":"markdown","d1078c09":"markdown","eb50f5a5":"markdown","340344e9":"markdown","f5251c43":"markdown","410752a6":"markdown","acd36c8d":"markdown","96d88d0a":"markdown","cb768931":"markdown"},"source":{"ed5fdaba":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.metrics import accuracy_score\nfrom kaggle.competitions import twosigmanews\n\n\n#***********************************import keras\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ZeroPadding2D\nfrom keras.losses import binary_crossentropy\nimport keras.callbacks as callbacks\nfrom keras.callbacks import Callback\nfrom keras.applications.xception import Xception\nfrom keras.layers import multiply\n\nimport keras\nfrom keras import optimizers\nfrom keras.legacy import interfaces\nfrom keras.utils.generic_utils import get_custom_objects\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom keras.regularizers import l2\nfrom keras.layers.core import Dense, Lambda\nfrom keras.layers.merge import concatenate, add\nfrom keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\nfrom keras.optimizers import SGD","358b5f1a":"env = twosigmanews.make_env()\n(market_train, _) = env.get_training_data()","b275781c":"cat_cols = ['assetCode']\nnum_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']\n\n","b8e347dc":"train_indices = market_train.index[market_train['time'].dt.year < 2016].values\nval_indices = market_train.index[market_train['time'].dt.year >= 2016].values","9faedcda":"print('lets print something')","16c10174":"def encode(encoder, x):\n    len_encoder = len(encoder)\n    try:\n        id = encoder[x]\n    except KeyError:\n        id = len_encoder\n    return id\n\nencoders = [{} for cat in cat_cols]\n\n\nfor i, cat in enumerate(cat_cols):\n    print('encoding %s ...' % cat, end=' ')\n    encoders[i] = {l: id for id, l in enumerate(market_train.loc[train_indices, cat].astype(str).unique())}\n    market_train[cat] = market_train[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    print('Done')\n\nembed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets\n","de1ab847":"from sklearn.preprocessing import StandardScaler\n \nmarket_train[num_cols] = market_train[num_cols].fillna(0)\nprint('scaling numerical columns')\n\nscaler = StandardScaler()\n\n#col_mean = market_train[col].mean()\n#market_train[col].fillna(col_mean, inplace=True)\nscaler = StandardScaler()\nmarket_train[num_cols] = scaler.fit_transform(market_train[num_cols])\n","7959141b":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization\nfrom keras.losses import binary_crossentropy, mse\n\ncategorical_inputs = []\nfor cat in cat_cols:\n    categorical_inputs.append(Input(shape=[1], name=cat))\n\ncategorical_embeddings = []\nfor i, cat in enumerate(cat_cols):\n    categorical_embeddings.append(Embedding(embed_sizes[i], 10)(categorical_inputs[i]))\n\n#categorical_logits = Concatenate()([Flatten()(cat_emb) for cat_emb in categorical_embeddings])\ncategorical_logits = Flatten()(categorical_embeddings[0])\n#categorical_logits = Dense(32,activation='relu')(categorical_logits)\n#categorical_logits =Dropout(0.5)(categorical_logits)\n#categorical_logits =BatchNormalization()(categorical_logits)\ncategorical_logits = Dense(32,activation='relu')(categorical_logits)\n\nnumerical_inputs = Input(shape=(11,), name='num')\nnumerical_logits = numerical_inputs\nnumerical_logits = BatchNormalization()(numerical_logits)\n\nnumerical_logits = Dense(128,activation='relu')(numerical_logits)\n#numerical_logits=Dropout(0.3)(numerical_logits)\n#numerical_logits = BatchNormalization()(numerical_logits)\n#numerical_logits = Dense(128,activation='relu')(numerical_logits)\nnumerical_logits = Dense(64,activation='relu')(numerical_logits)\n\nlogits = Concatenate()([numerical_logits,categorical_logits])\nlogits = Dense(64,activation='relu')(logits)\nout = Dense(1, activation='sigmoid')(logits)\n\nmodel = Model(inputs = categorical_inputs + [numerical_inputs], outputs=out)\nmodel.compile(optimizer='adam',loss=binary_crossentropy)","ab195eb0":"# Lets print our model\nmodel.summary()","4ef7ce8f":"def get_input(market_train, indices):\n    X_num = market_train.loc[indices, num_cols].values\n    X = {'num':X_num}\n    for cat in cat_cols:\n        X[cat] = market_train.loc[indices, cat_cols].values\n    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'time'].dt.date\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\nX_train,y_train,r_train,u_train,d_train = get_input(market_train, train_indices)\nX_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train, val_indices)","5ffc691d":"class SWA(keras.callbacks.Callback):\n    \n    def __init__(self, filepath, swa_epoch):\n        super(SWA, self).__init__()\n        self.filepath = filepath\n        self.swa_epoch = swa_epoch \n    \n    def on_train_begin(self, logs=None):\n        self.nb_epoch = self.params['epochs']\n        print('Stochastic weight averaging selected for last {} epochs.'\n              .format(self.nb_epoch - self.swa_epoch))\n        \n    def on_epoch_end(self, epoch, logs=None):\n        \n        if epoch == self.swa_epoch:\n            self.swa_weights = self.model.get_weights()\n            \n        elif epoch > self.swa_epoch:    \n            for i in range(len(self.swa_weights)):\n                self.swa_weights[i] = (self.swa_weights[i] * \n                    (epoch - self.swa_epoch) + self.model.get_weights()[i])\/((epoch - self.swa_epoch)  + 1)  \n\n        else:\n            pass\n        \n    def on_train_end(self, logs=None):\n        self.model.set_weights(self.swa_weights)\n        print('Final model parameters set to stochastic weight average.')\n        self.model.save_weights(self.filepath)\n        print('Final stochastic averaged weights saved to file.')","d1645470":"class SnapshotCallbackBuilder:\n    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1):\n        self.T = nb_epochs\n        self.M = nb_snapshots\n        self.alpha_zero = init_lr\n\n    def get_callbacks(self, model_prefix='Model'):\n\n        callback_list = [\n            callbacks.ModelCheckpoint(\"model.hdf5\",monitor='val_my_iou_metric', \n                                   mode = 'max', save_best_only=True, verbose=1),\n            swa,\n            callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule)\n        ]\n\n        return callback_list\n\n    def _cosine_anneal_schedule(self, t):\n        cos_inner = np.pi * (t % (self.T \/\/ self.M))  # t - 1 is used when t has 1-based indexing.\n        cos_inner \/= self.T \/\/ self.M\n        cos_out = np.cos(cos_inner) + 1\n        return float(self.alpha_zero \/ 2 * cos_out)","8a6fe7bc":"from keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n\"\"\"\nepochs = 10\nsnapshot = SnapshotCallbackBuilder(nb_epochs=epochs,nb_snapshots=1,init_lr=1e-3)\nbatch_size = 32\nswa = SWA('model_swa.hdf5',6)\nhistory = model.fit(X_train,y_train.astype(int),\n                    validation_data=(X_valid,y_valid.astype(int)),\n                    epochs=epochs,\n                    #batch_size=batch_size,\n                    callbacks=snapshot.get_callbacks(),shuffle=True,verbose=2)\n                    \nearly_stop = EarlyStopping( mode = 'max',patience=15, verbose=1)\ncheck_point = ModelCheckpoint('model.hdf5', mode = 'max', save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau( mode = 'max',factor=0.5, patience=5, min_lr=0.0001, verbose=1)\n#check_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\n#early_stop = EarlyStopping(patience=5,verbose=True)\nmodel.fit(X_train,y_train.astype(int),\n                    validation_data=(X_valid,y_valid.astype(int)), \n                    epochs=15,\n                    callbacks=[check_point,reduce_lr,early_stop], \n                    verbose=2)\n\nmodel.fit(X_train,y_train.astype(int),\n          validation_data=(X_valid,y_valid.astype(int)),\n          epochs=10,\n          verbose=True,\n          callbacks=[early_stop,check_point]) \n\"\"\"\n\n\ncheck_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\nearly_stop = EarlyStopping(patience=5,verbose=True)\nmodel.fit(X_train,y_train.astype(int),\n          validation_data=(X_valid,y_valid.astype(int)),\n          epochs=5,\n          verbose=True,\n          callbacks=[early_stop,check_point]) ","a90b3bdb":"\"\"\"\ntry:\n    print('using swa weight model')\n    model.load_weights('model_swa.hdf5')\nexcept:\n    model.load_weights('model.hdf5')\n\"\"\"","9c5452ab":"# distribution of confidence that will be used as submission\nmodel.load_weights('model.hdf5')\nconfidence_valid = model.predict(X_valid)[:,0]*2 -1\nprint(accuracy_score(confidence_valid>0,y_valid))\nplt.hist(confidence_valid, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","95983773":"# calculation of actual metric that is used to calculate final score\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean \/ std\nprint(score_valid)","7eb3f7bf":"days = env.get_prediction_days()","e57a0896":"n_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\npredicted_confidences = np.array([])\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    \n    t = time.time()\n\n    market_obs_df['assetCode_encoded'] = market_obs_df[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n\n    market_obs_df[num_cols] = market_obs_df[num_cols].fillna(0)\n    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n    X_num_test = market_obs_df[num_cols].values\n    X_test = {'num':X_num_test}\n    X_test['assetCode'] = market_obs_df['assetCode_encoded'].values\n    \n    prep_time += time.time() - t\n    \n    t = time.time()\n    market_prediction = model.predict(X_test)[:,0]*2 -1\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n\nenv.write_submission_file()\ntotal = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')","6e018ab5":"# distribution of confidence as a sanity check: they should be distributed as above\nplt.hist(predicted_confidences, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","4a90ee6d":"# Recall that the original script scores like 0.65, but here it says your validation is going to be below 0.40 ... what is going on?","16f2e7ba":"# This kernel is mostly a copy of Dieter's Neural Network found at https:\/\/www.kaggle.com\/christofhenkel\/market-data-nn-baseline","f01f53c0":"# Train NN model","08ace7ff":"# Instead of just doing the regular old train_test_split that many others have been doing to check the validation metric, I had my train as before 2016 and validation as after 2016, to simulate that the test data is in the future. I changed it in a high-scoring public kernel to see how the validation result changes... ","12eb4584":"# Define NN Architecture","d1078c09":"## Some background: I have developed a working RNN (LSTM) that scores around the same on Log Loss when training, but then when I obtain the validation metric score I was receiving only 0.39 (???).","eb50f5a5":"# Create train and validation indices like how I have mine:","340344e9":"# Handling categorical variables","f5251c43":"# Handling numerical variables","410752a6":"# Prediction","acd36c8d":"# I hypothesized that my model really was doing well, but in fact that the other public kernels whose validation metrics I was basing mine off of, were lying in some sense. The only thing I could think of was that I had split my train and validation differently.","96d88d0a":"Todo: add explanaition of architecture","cb768931":"# Evaluation of Validation Set"}}