{"cell_type":{"3d7eda50":"code","f665be27":"code","d324f5f7":"code","a8e818fd":"code","b1cdd1d7":"code","41982bbc":"code","28359151":"code","8de9dee3":"code","82bb7535":"code","b2b1c35c":"code","bfd3b79c":"code","976e6ba7":"code","95098e3e":"code","337f26b5":"code","a4fa7a4c":"code","940be832":"code","1d07984a":"code","8a30cd87":"code","462dcf9e":"code","0dc47951":"code","8416c2d8":"code","5af40316":"code","141439cd":"code","8995c2db":"code","af623812":"markdown","43212c7f":"markdown","73fff432":"markdown","55f0fc40":"markdown","2f9f9e89":"markdown","9c9f78ee":"markdown","8e865cfe":"markdown","30aa87b7":"markdown","3117d2e0":"markdown","09e93045":"markdown","cff9e369":"markdown","c6d87cf7":"markdown","e8b0f7a4":"markdown","aeef2848":"markdown","008bc72b":"markdown","aa35d4b4":"markdown","b25cc2b2":"markdown","56042697":"markdown"},"source":{"3d7eda50":"import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder","f665be27":"dataset_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","d324f5f7":"dataset_train.head()","a8e818fd":"dataset_train['family_size'] = dataset_train['SibSp'] + dataset_train['Parch'] + 1 \ndataset_train.head()","b1cdd1d7":"dataset_train[['family_size', 'Survived']].groupby(['family_size'], as_index=False).mean().sort_values(by='Survived', ascending=False)","41982bbc":"dataset_train['alone'] = 0\ndataset_train.loc[dataset_train['family_size'] == 1, 'alone'] = 1\ndataset_train.head()","28359151":"dataset_train[['alone', 'Survived']].groupby(['alone'], as_index=False).mean().sort_values(by='Survived', ascending=False)","8de9dee3":"X_train = dataset_train.iloc[:, [2,4,5,9,11,13]].values\ny_train = dataset_train.iloc[:, 1].values","82bb7535":"print(dataset_train.isnull().sum())","b2b1c35c":"# For Age\nimputer_1 = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer_1.fit(X_train[:, [2]])\nX_train[:, [2]] = imputer_1.transform(X_train[:, [2]])\n\n# For Embarked\nimputer_2 = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputer_2.fit(X_train[:, [4]])\nX_train[:, [4]] = imputer_2.transform(X_train[:, [4]])","bfd3b79c":"# Encoding P Class\nct_1 = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\nX_train = np.array(ct_1.fit_transform(X_train))\nX_train = X_train[: ,1:]\n\n# Encoding Embarked\nct_2 = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [5])], remainder='passthrough')\nX_train = np.array(ct_2.fit_transform(X_train))\nX_train = X_train[: ,[0,1,3,4,5,6,7,8]]\n\n# Encoding Gender\nle_train = LabelEncoder()\nX_train[:, 4] = le_train.fit_transform(X_train[:, 4])","976e6ba7":"dataset_test= pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","95098e3e":"dataset_test.head()","337f26b5":"dataset_test['family_size'] = dataset_test['SibSp'] + dataset_test['Parch'] + 1 \ndataset_test.head()","a4fa7a4c":"dataset_test['alone'] = 0\ndataset_test.loc[dataset_train['family_size'] == 1, 'alone'] = 1\ndataset_test.head()","940be832":"X_test = dataset_test.iloc[:, [1,3,4,8,10,12]].values","1d07984a":"print(dataset_test.isnull().sum())","8a30cd87":"# For Age\nimputer_3 = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer_3.fit(X_test[:, [2]])\nX_test[:, [2]] = imputer_3.transform(X_test[:, [2]])\n\n# For Fare\nimputer_4 = SimpleImputer(missing_values=np.nan, strategy='median')\nimputer_4.fit(X_test[:, [3]])\nX_test[:, [3]] = imputer_4.transform(X_test[:, [3]])","462dcf9e":"# Encoding P Class\nct_3 = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\nX_test = np.array(ct_3.fit_transform(X_test))\nX_test = X_test[: ,1:]\n\n# Encoding Embarked\nct_4 = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [5])], remainder='passthrough')\nX_test = np.array(ct_4.fit_transform(X_test))\nX_test = X_test[: ,[0,1,3,4,5,6,7,8]]\n\n# Encoding Gender\nle_test = LabelEncoder()\nX_test[:, 4] = le_test.fit_transform(X_test[:, 4])","0dc47951":"from sklearn.model_selection import train_test_split\nX_1, X_2, y_1, y_2 = train_test_split(X_train, y_train, test_size = 0.20)","8416c2d8":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_1[:, [5,6]] = sc_X.fit_transform(X_1[:, [5,6]])\nX_2[:, [5,6]] = sc_X.transform(X_2[:, [5,6]])\nX_test[:, [5,6]] = sc_X.transform(X_test[:, [5,6]])","5af40316":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\nclassifier.fit(X_1,y_1)","141439cd":"y_pred_train = classifier.predict(X_2)\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report \n\nprint('Confusion Matrix :')\nprint(confusion_matrix(y_2, y_pred_train)) \nprint('Accuracy Score :',accuracy_score(y_2, y_pred_train))\nprint('Report : ')\nprint(classification_report(y_2, y_pred_train))","8995c2db":"y_pred_test = classifier.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': dataset_test.PassengerId, 'Survived': y_pred_test})\noutput.to_csv('my_submission_4.csv', index=False)\nprint(\"Your submission was successfully saved!\")","af623812":"# Importing the libraries","43212c7f":"# **TITANIC MACHINE LEARNING FROM DISASTERS MODEL**","73fff432":"# Checking Missing values in our training dataset\n","55f0fc40":"# Encoding categorical data in training set","2f9f9e89":"# Encoding categorical data in test dataset","9c9f78ee":"# Prediction for test Set","8e865cfe":"# Splitting the dataset into the Training set and Test set","30aa87b7":"# Creating another new type from family_size","3117d2e0":"# Prediction for Training Set","09e93045":"# Inserting new Values at the place of missing data in training set","cff9e369":"# Creating a new type of variable from parch and sibsp","c6d87cf7":"# Inserting new Values at the place of missing data in test set","e8b0f7a4":" # Now traing our Machine learning model on training dataset and fitting it over test data set to predict survival\n","aeef2848":"# Importing the test dataset","008bc72b":"# Applying Feature Scaling","aa35d4b4":"# Now Doing the above whole preprocessing on our test dataset","b25cc2b2":"# Checking Missing values in our test dataset","56042697":"# Importing the training dataset"}}