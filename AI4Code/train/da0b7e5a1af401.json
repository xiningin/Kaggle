{"cell_type":{"438020db":"code","0d84a444":"code","37784398":"code","f19dcc79":"code","9b1bb447":"code","3b4da2b3":"code","f3bdac79":"code","c5ff8324":"code","6001d2a1":"code","0c4ca436":"code","6370a759":"code","af15f762":"code","32c6cba6":"markdown","445ab095":"markdown","65c842ef":"markdown","3b1b83a9":"markdown","3fa41b27":"markdown","f55b8508":"markdown","7900a374":"markdown"},"source":{"438020db":"%matplotlib inline \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","0d84a444":"from sklearn.datasets import make_regression\nplt.figure(figsize=(10,8))\nplt.title(\"regression dataset example\")\nX_R1,y_R1=make_regression(n_samples=100,n_features=1,n_informative=1,bias=150.0,noise=30,random_state=0)\n'''\nThe number of informative features, i.e., the number of features used to build the linear model used to generate the output.\n\n'''\nplt.grid()\nplt.scatter(X_R1,y_R1,marker='o',s=50)\nplt.show()","37784398":"from matplotlib.colors import ListedColormap\ncmap_bold=ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])","f19dcc79":"from sklearn.datasets import make_classification\nplt.figure(figsize=(8,6))\nplt.title('simple classification example')\nX_C1,y_C1=make_classification(n_samples=100,n_features=2,n_informative=2,n_redundant=0,n_classes=2,n_clusters_per_class=1,flip_y=0.1,class_sep=0.5,random_state=0)\nplt.grid()\nplt.scatter(X_C1[:,0],X_C1[:,1],c=y_C1,marker='o',s=50,cmap=cmap_bold)\nplt.show()","9b1bb447":"from sklearn.datasets import make_blobs\nX_D2,y_D2=make_blobs(n_samples=100,n_features=2,centers=8,cluster_std=1.3,random_state=4)\ny_D2=y_D2%2\nplt.figure(figsize=(12,8))\nplt.title('complex classification example ')\nplt.scatter(X_D2[:,0],X_D2[:,1],c=y_D2,s=50,marker='o',cmap=cmap_bold)\nplt.grid()\nplt.show()","3b4da2b3":"X,y=make_regression(n_samples=60,n_features=1,n_targets=1,n_informative=1,bias=10,noise=20,random_state=0)\nfrom sklearn.neighbors import KNeighborsRegressor\nknn_reg=KNeighborsRegressor(n_neighbors=5)# as n_neighbors increase model_complexity will reduce and reduce overfitting \nknn_reg.fit(X[:40],y[:40])\nplt.figure(figsize=(13,8))\nplt.scatter(X[:40],y[:40],marker='o',s=50,label='training_set')\n\nplt.plot(np.linspace(min(X[:40]),max(X[:40]),num=500),knn_reg.predict(np.linspace(min(X[:40]),max(X[:40]),num=500)),'r-',label='knn_model')\nplt.plot(X[40:],knn_reg.predict(X[40:]),'go',label='Test_prediction')\nplt.scatter(X[40:],y[40:],marker='*',s=50,c='y',label='Test_set')\nplt.title('Knn regressor, k=5')\nplt.legend(loc='best')\nplt.grid()\nplt.show()\nprint('train_socre ',knn_reg.score(X[:40],y[:40]))\nprint('test_score ',knn_reg.score(X[40:],y[40:]))","f3bdac79":"from sklearn.neighbors import KNeighborsRegressor\nknn_reg=KNeighborsRegressor(n_neighbors=10)   \nknn_reg.fit(X[:40],y[:40])\nplt.figure(figsize=(10,6))\nplt.scatter(X[:40],y[:40],marker='o',s=50,label='training_set')\n\nplt.plot(np.linspace(min(X[:40]),max(X[:40]),num=500),\n         knn_reg.predict(np.linspace(min(X[:40]),max(X[:40]),num=500)),'r-',label='knn_model')\nplt.plot(X[40:],knn_reg.predict(X[40:]),'go',label='Test_prediction')\nplt.scatter(X[40:],y[40:],marker='*',s=50,c='y',label='Test_set')\nplt.title('Knn regressor, k=10')\nplt.legend(loc='best')\nplt.grid()\nplt.show()\nprint('train_socre ',knn_reg.score(X[:40],y[:40]))\nprint('test_score ',knn_reg.score(X[40:],y[40:]))","c5ff8324":"from sklearn.linear_model import LinearRegression\nlinear_model=LinearRegression()\nlinear_model.fit(X[:40],y[:40])\nplt.figure(figsize=(10,6))\nplt.scatter(X[:40],y[:40],s=50,marker='o',label='training_set')\nplt.plot(X[:40],(linear_model.coef_)*X[:40]+linear_model.intercept_,'r-',label='linear_model')\nplt.plot(X[40:],(linear_model.coef_)*X[40:]+linear_model.intercept_,'go',label='test_prediction')\nplt.scatter(X[40:],y[40:],s=50,marker='*',c='y',label='test_set')\nplt.title('linear_regressor')\nplt.legend(loc='best')\nplt.grid()\nplt.show()\nprint('intercept  ',linear_model.intercept_)\nprint('coef  ',linear_model.coef_)\nprint('train_score ',linear_model.score(X[:40],y[:40]))\nprint('test_score',linear_model.score(X[40:],y[40:]))","6001d2a1":"from sklearn.linear_model import Ridge\nlinridge=Ridge(alpha=2)       \nlinridge.fit(X[:40],y[:40])\nprint('train_score ',linridge.score(X[:40],y[:40]))\nprint('test_score ',linridge.score(X[40:],y[40:]))","0c4ca436":"from sklearn.linear_model import Lasso\nlinlasso=Lasso(alpha=1,max_iter=2000)\nlinlasso.fit(X[:40],y[:40])\nprint('train_score ',linlasso.score(X[:40],y[:40]))\nprint('test_score ',linlasso.score(X[40:],y[40:]))","6370a759":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nscaled_X_train=scaler.fit_transform(X[:40])\nscaled_X_test=scaler.transform(X[40:])\nlinear_model2=LinearRegression()\nlinear_model2.fit(scaled_X_train,y[:40])\nprint('linear model train_score ',linear_model2.score(scaled_X_train,y[:40]))\nprint('linear model test_score',linear_model2.score(scaled_X_test,y[40:]))","af15f762":"from sklearn.preprocessing import PolynomialFeatures\npoly=PolynomialFeatures(degree=2)\npoly_X=poly.fit_transform(X)\nlinear_model3=LinearRegression()\nlinear_model3.fit(poly_X[:40],y[:40])\nprint('linear model train_score ',linear_model3.score(poly_X[:40],y[:40]))\nprint('linear model test_score',linear_model3.score(poly_X[40:],y[40:]))","32c6cba6":"# Synthetic dataset\n****Some of the synthetic dataset provided by sklearn.dataset bunch type ****\nIt will be helpful for beginner to generate their own dataset quickly and practice\nbefore going to real world dataset.","445ab095":"# MINMAXSCALER\nminmaxscaler convert features to range 0 to 1.some model which work better with features scaling.\n\nIn general, algorithms that exploit distances or similarities (e.g. in form of scalar product) between data samples, such as k-NN and SVM, are sensitive to feature transformations.\n\nGraphical-model based classifiers, such as Fisher LDA or Naive Bayes, as well as Decision trees and Tree-based ensemble methods (RF, XGB) are invariant to feature scaling, but still it might be a good idea to rescale\/standartize your data.\n\nMinmaxscaler prone to outliers.So the dataset must remove outliers for effective use of minmaxscaler.","65c842ef":"# Visit BasicAppliedMachineLearning01\nhttps:\/\/www.kaggle.com\/aman2000jaiswal\/basicappliedmachinelearning01\n","3b1b83a9":"# **Polynomial features**\nSome of the dataset which are not linear. For those complex dataset we have to add some of the quadratic or polynomial features\nvalue by raising the degree of the features.\nSuch that features X to X^2 or X^3.\nFor this purpose we transform the dataset to polynomial features which gives all the possible specified degree\nfeatures to the features set.","3fa41b27":"# RIDGE REGRESSION\n Ridge Regression use l2 penality, It penalize the model to become over weight. If model having different weights having same accuracy or prediction then it make tends to choose less weight model. \n Penality term is given as :\n                                n\n            L2_Penality = alpha*\u2211 w^2\n                                j=1 \n                                \n as alpha increase model will tends to simpler model and generalization.\n It reduces the overfitting of model.\n \n","f55b8508":"# LASSO REGRESSION\n    Lasso Regression uses L1 Penality.\n\n                                n\n            L1_Penality = alpha*\u2211 w\n                                j=1 \n   as alpha increase model will tends to simpler model and generalization. It reduces the overfitting of model.\n   The difference between ridge and lasso is that ridge Penality tends weight to zero. But lasso make weight zero of the    less relvalnt features of dataset  ","7900a374":"# BasicAppliedMachineLearning02"}}