{"cell_type":{"f2f1b6ed":"code","9127467b":"code","a447efe4":"code","1d2f546e":"code","a8ba341e":"code","531713c7":"code","d407f103":"code","f7b524f4":"code","66f383d8":"code","128c49f6":"code","6546baa4":"code","ccfa59eb":"code","b178206f":"code","55b25048":"code","6b138c0c":"code","8c78d4c3":"code","7c5483ba":"code","12331555":"code","650142cf":"code","4b52eec9":"code","2b68d773":"code","9daa92ed":"code","b265faa5":"code","49b57747":"code","8b246883":"code","b36712f3":"markdown","55f497cf":"markdown"},"source":{"f2f1b6ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9127467b":"import os \nimport sys\n\n#data processing library\nimport numpy as np\nimport pandas as pd\n\n#data visualization library\nimport matplotlib.pyplot as plt\n\n#deep learning\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\n\n#metrics\nfrom sklearn.metrics import roc_auc_score","a447efe4":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip","1d2f546e":"!unzip glove.6B.zip ","a8ba341e":"ls -l ","531713c7":"train = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train-processed-seqlen128.csv\")","d407f103":"train.head()","f7b524f4":"train[\"sentence_length\"] = train[\"comment_text\"].str.len()","66f383d8":"train.head()","128c49f6":"train[\"comment_text\"][0]","6546baa4":"len(train[\"comment_text\"][0])","ccfa59eb":"fig, ax = plt.subplots(figsize =(12, 8))\nax.hist(train[\"sentence_length\"],bins=20)","b178206f":"# some configuration\nMAX_SEQUENCE_LENGTH = 100\nMAX_VOCAB_SIZE = 20000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2\nBATCH_SIZE = 128\nEPOCHS = 10","55b25048":"word2vec = {}\nwith open(os.path.join('.\/glove.6B.100d.txt')) as f:\n    \n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n    for line in f:\n        \n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))","6b138c0c":"sentences = train[\"comment_text\"].fillna(\"DUMMY_VALUE\").values\npossible_labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntargets = train[possible_labels].values","8c78d4c3":"# convert the sentences (strings) into integers\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)","7c5483ba":"print(\"max sequence length:\", max(len(s) for s in sequences))\nprint(\"min sequence length:\", min(len(s) for s in sequences))\ns = sorted(len(s) for s in sequences)\nprint(\"median sequence length:\", s[len(s) \/\/ 2])\n\nprint(\"max word index:\", max(max(seq) for seq in sequences if len(seq) > 0))","12331555":"# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))","650142cf":"# pad sequences so that we get a N x T matrix\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', data.shape)","4b52eec9":"# prepare embedding matrix\nprint('Filling pre-trained embeddings...')\nnum_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))","2b68d773":"for word, i in word2idx.items():\n    \n    if i < MAX_VOCAB_SIZE:\n        embedding_vector = word2vec.get(word)\n        if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n            embedding_matrix[i] = embedding_vector","9daa92ed":"# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(\n  num_words,\n  EMBEDDING_DIM,\n  weights=[embedding_matrix],\n  input_length=MAX_SEQUENCE_LENGTH,\n  trainable=False\n)","b265faa5":"print('Building model...')\n\n# train a 1D convnet with global maxpooling\ninput_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\nx = embedding_layer(input_)\nx = Conv1D(128, 3, activation='relu')(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(128, 3, activation='relu')(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(128, 3, activation='relu')(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(128, activation='relu')(x)\noutput = Dense(len(possible_labels), activation='sigmoid')(x)\n\nmodel = Model(input_, output)\nmodel.compile(\n  loss='binary_crossentropy',\n  optimizer='rmsprop',\n  metrics=['accuracy']\n)","49b57747":"print('Training model...')\nr = model.fit(\n  data,\n  targets,\n  batch_size=BATCH_SIZE,\n  epochs=EPOCHS,\n  validation_split=VALIDATION_SPLIT\n)","8b246883":"# plot some data\nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()\n\n# accuracies\nplt.plot(r.history['accuracy'], label='acc')\nplt.plot(r.history['val_accuracy'], label='val_acc')\nplt.legend()\nplt.show()\n\n# plot the mean AUC over each label\np = model.predict(data)\naucs = []\nfor j in range(6):\n    auc = roc_auc_score(targets[:,j], p[:,j])\n    aucs.append(auc)\nprint(np.mean(aucs))","b36712f3":"## choosing some hyperparameter. ","55f497cf":"# Download The Word Vector"}}