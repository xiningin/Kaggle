{"cell_type":{"eea5be4c":"code","0ba01ac3":"code","9019e036":"code","845e08a9":"code","837952c7":"code","ade829d8":"code","640d4322":"code","e299659a":"code","beaa36d7":"code","0215267d":"code","ab727537":"code","0e23e307":"code","b16ebf64":"code","edef78f4":"code","3d6d8876":"code","0c27595c":"code","98b843a2":"code","56794475":"code","bbda9938":"markdown","b63c31b5":"markdown","90db51e0":"markdown","79fe1765":"markdown","88951e02":"markdown","4314da70":"markdown","610cedcb":"markdown","414baabc":"markdown","2c5221fd":"markdown","9675c7e7":"markdown","1f0dee30":"markdown","310ee47c":"markdown","43834b22":"markdown"},"source":{"eea5be4c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom wordcloud import WordCloud,STOPWORDS\nimport spacy as sp\nimport string\nimport nltk\nimport re\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nnlps = sp.load('en')\nplt.rc('figure',figsize=(17,11))","0ba01ac3":"p_data = pd.read_excel('\/kaggle\/input\/3500-popular-creepypastas\/creepypastas.xlsx')\np_data.head(3)","9019e036":"pre_processed = p_data.copy()\n\ntag_df = p_data[['tags','average_rating']].copy()\n\nall_tags = ' '.join(tag_df[tag_df.tags.notna()].tags)\nall_tags = all_tags.replace('\\n',' ').strip('\\t')\nall_tags = all_tags.strip().split(',')\ntag_dict = {}\nfor tag in all_tags:\n    t = tag.strip().lower()\n    tag_dict[t] = tag_dict.get(t,0)+1        \n\n    \n    \nall_categories = ' '.join(p_data[p_data.categories.notna()].categories)\nall_categories = all_categories.replace('\\n',' ').strip('\\t')\nall_categories = all_categories.strip().split(',')\ncategories_dict = {}\nfor category in all_categories:\n    t = category.strip().lower()\n    categories_dict[t] = categories_dict.get(t,0)+1        \n\n    \n\n\ndef ert_preprocess(sir):\n    if sir.find('<') == -1:\n        if sir.find('minutes') != -1:\n            return  np.int(sir.replace(' minutes',''))\n        else:\n            return  np.int(sir.replace(' minut',''))\n    else:\n        return 1\n    \npre_processed.loc[pre_processed.estimated_reading_time.notna(),'estimated_reading_time'] = pre_processed.loc[pre_processed.estimated_reading_time.notna(),\n                                                                                                             'estimated_reading_time'].apply(ert_preprocess)\n\n#bad value\npre_processed.drop(index = 2511,inplace=True)\n\n\n#covert to day column \npre_processed.loc[pre_processed.publish_date.notna(),'publish_date'] = pd.to_datetime(pre_processed[pre_processed.publish_date.notna()].publish_date)\npre_processed['Year_Published'] = pre_processed.publish_date.apply(lambda x: x.year)\npre_processed['Month_Published'] = pre_processed.publish_date.apply(lambda x: x.month)\npre_processed['Day_Published'] = pre_processed.publish_date.apply(lambda x: x.dayofweek)\n\npre_processed.body =pre_processed.body.str.lower()\npre_processed.story_name =pre_processed.story_name.str.lower()\n\n# Remove all the special characters\npre_processed.body = pre_processed.body.apply(lambda x:re.sub(r'\\W', ' ', x))\npre_processed.story_name = pre_processed.story_name.apply(lambda x:re.sub(r'\\W', ' ', x))\n#remove all single characters\npre_processed.body = pre_processed.body.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\npre_processed.story_name = pre_processed.story_name.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\n# Substituting multiple spaces with single space\npre_processed.body = pre_processed.body.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\npre_processed.story_name = pre_processed.story_name.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))","845e08a9":"sid = SIA()\npre_processed['sentiments']           = pre_processed['body'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\npre_processed['Positive Sentiment']   = pre_processed['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \npre_processed['Neutral Sentiment']    = pre_processed['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\npre_processed['Negative Sentiment']   = pre_processed['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n\npre_processed.drop(columns=['sentiments'],inplace=True)","837952c7":"pre_processed.head(3)","ade829d8":"plt.subplot(2,1,1)\nax = sns.kdeplot(pre_processed.average_rating)\n\ntextstr = '\\n'.join(\n    \n        (   r'$\\mu=%.2f$' % (pre_processed.average_rating.mean(),)\n          , r'$\\mathrm{median}=%.2f$' % (pre_processed['average_rating'].median(),)\n          , r'$\\sigma=%.2f$' % (pre_processed['average_rating'].std(),)\n          , r'Skew=%.2f' % (pre_processed['average_rating'].skew(),)\n          , r'Kurtosis=%.2f' % (pre_processed['average_rating'].kurt(),)\n\n        )\n    \n                  )\n\nprops = dict(boxstyle='round', facecolor='red', alpha=0.5)\nax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=17,\n        verticalalignment='top', bbox=props)\nax.set_title('Distribution Of Average Ratings In Our Data',fontsize=16)\n\n\n\nplt.subplot(2,1,2)\nax=sns.kdeplot(pre_processed.average_rating,cumulative=True)\nax.set_title('CDF')\nax.set_xlabel('Average Rating',fontsize=16)\n\n\n\nplt.show()","640d4322":"plt.subplot(2,1,1)\nax = sns.kdeplot(pre_processed.estimated_reading_time,color='red')\n\ntextstr = '\\n'.join(\n    \n        (   r'$\\mu=%.2f$' % (pre_processed.average_rating.mean(),)\n          , r'$\\mathrm{median}=%.2f$' % (pre_processed['estimated_reading_time'].median(),)\n          , r'$\\sigma=%.2f$' % (pre_processed['estimated_reading_time'].std(),)\n          , r'Skew=%.2f' % (pre_processed['estimated_reading_time'].skew(),)\n          , r'Kurtosis=%.2f' % (pre_processed['estimated_reading_time'].kurt(),)\n\n        )\n    \n                  )\n\nprops = dict(boxstyle='round', facecolor='red', alpha=0.5)\nax.text(0.25, 0.95, textstr, transform=ax.transAxes, fontsize=17,\n        verticalalignment='top', bbox=props)\nax.set_title('Distribution Of Estimated Reading Time In Our Data',fontsize=16)\n\n\n\nax.set_xticks(np.arange(0,220,20))\nplt.subplot(2,1,2)\nax = sns.kdeplot(pre_processed.estimated_reading_time,color='red',cumulative=True)\nax.set_xticks(np.arange(0,220,20))\nplt.show()","e299659a":"plt.title('Distriubtion Of Sentiments Across Our Creepypastas',fontsize=19,fontweight='bold')\nsns.kdeplot(pre_processed['Negative Sentiment'],bw=0.1)\nsns.kdeplot(pre_processed['Positive Sentiment'],bw=0.1)\nsns.kdeplot(pre_processed['Neutral Sentiment'],bw=0.1)\nplt.xlabel('Sentiment Value',fontsize=19)\nplt.show()","beaa36d7":"fig = ex.scatter(pre_processed,x='average_rating',y='estimated_reading_time', trendline=\"ols\",title='Is ERT Connected to The Average Rating?')\nresults = ex.get_trendline_results(fig)\nfig.show()","0215267d":"results.px_fit_results.iloc[0].summary()\n","ab727537":"\npre_processed.estimated_reading_time = pre_processed.estimated_reading_time.astype(np.int)\nbyear = pre_processed.groupby(by='Year_Published').mean()\nbyear = byear.reset_index()\nbyear\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Daily Average Positive Sentiment',  'Daily Average Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=byear['Year_Published'], y=byear['Positive Sentiment'],name='Positive Sentiment Mean'),\n    row=1, col=1\n)\n\n\n\nfig.add_trace(\n    go.Scatter(x=byear['Year_Published'], y=byear['Negative Sentiment'],name='Negative Sentiment Mean'),\n    row=2, col=1\n)\n\n\n\nfig['layout']['xaxis2']['title'] = 'Year Published'\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Average Change With Time\")\nfig.show()","0e23e307":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Yearly Average Rating',  'Yearly Average Reading Time'))\n\nfig.add_trace(\n    go.Scatter(x=byear['Year_Published'], y=byear['average_rating'],name='Mean Rating'),\n    row=1, col=1\n)\n\n\n\nfig.add_trace(\n    go.Scatter(x=byear['Year_Published'], y=byear['estimated_reading_time'],name='Mean Reading Time'),\n    row=2, col=1\n)\n\n\n\nfig['layout']['xaxis2']['title'] = 'Year Published'\nfig.update_layout(height=700, width=900, title_text=\"Creepypasta Rating And ERT Yearly Averages\")\nfig.show()","b16ebf64":"\nbmonth = pre_processed.groupby(by='Month_Published').mean()\nbmonth = bmonth.reset_index()\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Monthly Average Rating',  'Monthly Average Reading Time'))\n\nfig.add_trace(\n    go.Scatter(x=bmonth['Month_Published'], y=bmonth['average_rating'],name='Mean Rating'),\n    row=1, col=1\n)\n\n\n\nfig.add_trace(\n    go.Scatter(x=bmonth['Month_Published'], y=bmonth['estimated_reading_time'],name='Mean Reading Time'),\n    row=2, col=1\n)\n\n\n\nfig['layout']['xaxis2']['title'] = 'Month Published'\nfig.update_layout(height=700, width=900, title_text=\"Creepypasta Rating And ERT Monthly Averages\")\nfig.show()","edef78f4":"all_text = ' '.join(pre_processed.body)\n\n\npwc = WordCloud(width=600,height=400,collocations = False,stopwords=STOPWORDS).generate(all_text)\nplt.title('Common Words Among All Creepypastas',fontsize=16,fontweight='bold')\nplt.imshow(pwc)\nplt.axis('off')\nplt.show()","3d6d8876":"all_text = ' '.join(pre_processed.story_name)\n\n\npwc = WordCloud(width=600,height=400,collocations = False,stopwords=STOPWORDS).generate(all_text)\nplt.title('Common Words Among All Creepypastas Names',fontsize=16,fontweight='bold')\nplt.imshow(pwc)\nplt.axis('off')\nplt.show()","0c27595c":"ticks = np.linspace(pre_processed.average_rating.min(),pre_processed.average_rating.max(),6)\n\ns = 0\nfor index,p in enumerate(ticks):\n    plt.subplot(2,3,index+1)\n    t_text = pre_processed[pre_processed.average_rating.between(s,p)].body\n    plt.title('Most Common Words - Rating : ({:.2f},{:.2f}]'.format(s,p))\n    s=p\n    pwc = WordCloud(width=600,height=400,collocations = False,stopwords=STOPWORDS).generate(' '.join(t_text))\n    plt.imshow(pwc)\n    plt.axis('off')\n","98b843a2":"plt.title('Top 10 Creepypasta Categories',fontsize=18,fontweight='bold')\ncat_df = pd.DataFrame({'Key':categories_dict.keys(),'Value':categories_dict.values()}).sort_values(by='Value',ascending=False)\nsns.barplot(x=cat_df.iloc[:10,1],y=cat_df.iloc[:10,0])\nplt.xlabel('Number Of Creepypastas',fontsize=13,fontweight='bold')\nplt.ylabel('Category',fontsize=13,fontweight='bold')\nplt.show()","56794475":"plt.title('Top 10 Creepypasta Tags',fontsize=18,fontweight='bold')\ncat_df = pd.DataFrame({'Key':tag_dict.keys(),'Value':tag_dict.values()}).sort_values(by='Value',ascending=False)\nsns.barplot(x=cat_df.iloc[:10,1],y=cat_df.iloc[:10,0])\nplt.xlabel('Number Of Creepypastas',fontsize=13,fontweight='bold')\nplt.ylabel('Tag',fontsize=13,fontweight='bold')\nplt.show()","bbda9938":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Text Based Analysis<\/h3>","b63c31b5":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Numeric Feature Analysis<\/h3>","90db51e0":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h3>","79fe1765":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>There is no significant correlation between the estimated reading time to the average rating given to a creepypasta.<\/span><\/p>\n<p><br><\/p>","88951e02":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Interestingly on average, stories published in January are the longest, and there is a constant decrement in average rating and reading time as we approach August.<\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>I suspect that our data has little to no stories published in August; hence we see such results.<\/span><\/p>","4314da70":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We can observe that our average rating is about 7.5, and the ratings are fairly normally distributed with a slight negative skew.<\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>For a random creepypasta taken from our dataset, a probability of 0.8 that it will be above 6 stars out of 10.<\/span><\/p>\n<p><br><\/p>","610cedcb":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Our sentiments are less stable between 2008 and 2012, but I assume it is because of noise or outliers; I want to point your attention to the weak but amusing trend that can be observed starting from 2012.<\/span><\/p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">Starting from 2012, the average yearly positive sentiment increases with each year ever so slightly as the negative sentiment decreases.<\/span><\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Does it mean our creepypastas slowly become more positive?<\/span><\/p>","414baabc":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Data Preprocessing<\/h3>","2c5221fd":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Time Based Analysis<\/h3>","9675c7e7":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The observation made from looking at the distributions of the sentiments is quite surprising; what would expect that the dominant sentiment in creepypastas would be negative, but we see that the positive and negative sentiments are almost centered around the same mean. The neutral sentiment is the dominant sentiment in all our stories.<\/span><\/p>","1f0dee30":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The estimated reading time has some outliers; as we can observe in the PDF and CDF above, on average, a creepypasta from our dataset takes about 8 minutes to read, but some longer ones can take up more than an hour.<\/span><\/p>\n<p><br><\/p>","310ee47c":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Wow, that&apos;s really interesting! We can see that even though there is no clear trend in the average yearly scores except the two drops in 2012 and 2018 but at the same time, there is a clear and constant growing trend, with each year the average estimated reading time gets bigger, meaning that with each year the creepypastas become longer and longer!<\/span><\/p>","43834b22":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities<\/h3>"}}