{"cell_type":{"18ee0fae":"code","92f7282f":"code","f3648efb":"code","bc496884":"code","ccce20bd":"code","5aa2f288":"code","1b7f8667":"code","7e4ca486":"code","529cc68a":"code","bb93bf97":"code","6de62abf":"code","4d08b239":"code","6d9d4b9f":"code","8e718802":"code","19c8b511":"code","6a4d81f6":"code","df16880c":"code","3b65b7a8":"code","f718110c":"code","af8f8099":"code","0ce73616":"code","6a6b7d90":"code","dbeef075":"code","f91b7c41":"code","4bc4c2fe":"code","c8e6516e":"code","e350cb05":"code","f70ae801":"code","07d53c22":"code","df1717fb":"code","ac55d3ba":"code","f376ba25":"code","330f2480":"code","30e5a0ae":"code","685fd6e5":"code","74880478":"code","b5948657":"code","9078e7ac":"code","f79fabf9":"code","a9897828":"code","86b8c109":"code","955837a8":"code","87dd79e9":"code","9bc18835":"code","62b9fb2d":"code","f21f5851":"code","e67ddf44":"code","aca61d53":"code","949fcc22":"code","2e08cf75":"code","14546efb":"code","45661d0c":"markdown","75b307ad":"markdown","fb315ae9":"markdown","668c1368":"markdown","e3130c5d":"markdown","a7c5d6d2":"markdown","ad3060ba":"markdown","7d4ad4c0":"markdown","94f5f616":"markdown","d8f80a9d":"markdown","1580b421":"markdown","148fda0d":"markdown","16d86352":"markdown","75506381":"markdown","260fee62":"markdown","8cd77e30":"markdown","2c9ef834":"markdown","ebab39d2":"markdown","5ba9b0ed":"markdown","92c7e2ec":"markdown","1d6f42d7":"markdown","ba407d8c":"markdown","27eab3e7":"markdown","684501c3":"markdown","f6a53700":"markdown","9001283f":"markdown","66bb3ffb":"markdown","fe93d49b":"markdown","fa0aa513":"markdown","9b666bd7":"markdown","42e4d943":"markdown","9b8dc3d1":"markdown","2cf46e8b":"markdown","9d1b0f4f":"markdown","d6cfa8e6":"markdown","d5683d55":"markdown","38faf628":"markdown","17f24594":"markdown","accd9364":"markdown","00f5a0d5":"markdown","0c0f1453":"markdown","cc2af65c":"markdown","73870fff":"markdown","1dca402d":"markdown","01f4021e":"markdown","0f7755ef":"markdown","0786ae0f":"markdown","bb1b9366":"markdown","6d9ae88d":"markdown","9eae383a":"markdown","babb2e2c":"markdown","1c3c1f61":"markdown","463903a5":"markdown","405467d3":"markdown","4dd26349":"markdown","463e4fbb":"markdown"},"source":{"18ee0fae":"import warnings\nwarnings.simplefilter(action='ignore', category=UserWarning)\n\nimport os\nimport random\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom scipy.stats import skew, kurtosis\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\nfrom sklearn.decomposition import PCA\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, GaussianDropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.regularizers import L1, L2\n\nimport lightgbm as lgb\n\nimport sys\nsys.path.append('..\/input\/moa-models-and-packages\/site-packages')\nfrom gauss_rank_scaler import GaussRankScaler\n\nSEED = 721991\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)    \n    tf.random.set_seed(seed)","92f7282f":"df_train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ndf_test = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\ndf_train_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ndf_train_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ndf_test_targets = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\ntarget_features_scored = list(df_train_targets_scored.columns[1:])\ntarget_features_nonscored = list(df_train_targets_nonscored.columns[1:])\ndf_train_targets_scored[target_features_scored] = df_train_targets_scored[target_features_scored].astype(np.uint8)\ndf_train_targets_nonscored[target_features_nonscored] = df_train_targets_nonscored[target_features_nonscored].astype(np.uint8)\ndf_test_targets[target_features_scored] = df_test_targets[target_features_scored].astype(np.float32)\n\ndf_train = df_train.merge(df_train_targets_scored, on='sig_id', how='left')\ndf_train = df_train.merge(df_train_targets_nonscored, on='sig_id', how='left')\ndf_test = df_test.merge(df_test_targets, on='sig_id', how='left')\n\ndel df_train_targets_scored, df_train_targets_nonscored, df_test_targets\n\nprint(f'Training Set Shape = {df_train.shape}')\nprint(f'Training Set Memory Usage = {df_train.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\nprint(f'Test Set Shape = {df_test.shape}')\nprint(f'Test Set Memory Usage = {df_test.memory_usage().sum() \/ 1024 ** 2:.2f} MB')","f3648efb":"g_features = [feature for feature in df_train.columns if feature.startswith('g-')]\nc_features = [feature for feature in df_train.columns if feature.startswith('c-')]\nother_features = [feature for feature in df_train.columns if feature not in g_features and \n                                                             feature not in c_features and \n                                                             feature not in target_features_scored and\n                                                             feature not in target_features_nonscored]\n\nprint(f'Number of g- Features: {len(g_features)}')\nprint(f'Number of c- Features: {len(c_features)}')\nprint(f'Number of Other Features: {len(other_features)} ({other_features})')","bc496884":"print(f'Number of Scored Target Features: {len(target_features_scored)}')\nprint(f'Number of Non-scored Target Features: {len(target_features_nonscored)}')","ccce20bd":"def mean_columnwise_logloss(y_true, y_pred):        \n    y_pred = np.clip(y_pred, 1e-15, (1 - 1e-15))\n    score = - np.mean(np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred), axis=1))\n    return score","5aa2f288":"scored_targets_classified = df_train[target_features_scored].sum(axis=1)\nnonscored_targets_classified = df_train[target_features_nonscored].sum(axis=1)\n\nfig, axes = plt.subplots(figsize=(32, 8), ncols=2)\n\nsns.countplot(scored_targets_classified, ax=axes[0])\nsns.countplot(nonscored_targets_classified, ax=axes[1])\n\nfor i in range(2):\n    axes[i].tick_params(axis='x', labelsize=20)\n    axes[i].tick_params(axis='y', labelsize=20)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    \naxes[0].set_title(f'Training Set Unique Scored Targets per Sample', size=22, pad=22)\naxes[1].set_title(f'Training Set Unique Non-scored Targets per Sample', size=22, pad=22)\n\nplt.show()","1b7f8667":"fig = plt.figure(figsize=(12, 60))\n\nsns.barplot(x=df_train[target_features_scored].sum(axis=0).sort_values(ascending=False).values,\n            y=df_train[target_features_scored].sum(axis=0).sort_values(ascending=False).index)\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.xlabel('')\nplt.ylabel('')\nplt.title('Training Set Scored Targets Classification Counts', size=18, pad=18)\n\nplt.show()","7e4ca486":"fig = plt.figure(figsize=(12, 110))\n\nsns.barplot(x=df_train[target_features_nonscored].sum(axis=0).sort_values(ascending=False).values,\n            y=df_train[target_features_nonscored].sum(axis=0).sort_values(ascending=False).index)\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.xlabel('')\nplt.ylabel('')\nplt.title('Training Set Non-scored Targets Classification Counts', size=18, pad=18)\n\nplt.show()","529cc68a":"fig, axes = plt.subplots(figsize=(32, 15), ncols=2, dpi=150)\n\nsns.heatmap(df_train[target_features_scored].corr(),\n            annot=False,\n            square=True,\n            cmap='coolwarm',\n            cbar=False,\n            yticklabels=False,\n            xticklabels=False,\n            ax=axes[0])  \n\nsns.heatmap(df_train[target_features_nonscored].corr(),\n            annot=False,\n            square=True,\n            cmap='coolwarm',\n            cbar=False,\n            yticklabels=False,\n            xticklabels=False,\n            ax=axes[1])   \n\naxes[0].set_title('Training Set Scored Target Correlations', size=25, pad=25)\naxes[1].set_title('Training Set Non-scored Target Correlations', size=25, pad=25)\n\nplt.show()","bb93bf97":"fig, axes = plt.subplots(figsize=(24, 24), nrows=3, ncols=2)\n\nsns.countplot(df_train['cp_type'], ax=axes[0][0])\nsns.countplot(df_test['cp_type'], ax=axes[0][1])\n\nsns.countplot(df_train['cp_time'], ax=axes[1][0])\nsns.countplot(df_test['cp_time'], ax=axes[1][1])\n\nsns.countplot(df_train['cp_dose'], ax=axes[2][0])\nsns.countplot(df_test['cp_dose'], ax=axes[2][1])\n\nfor i in range(3):\n    for j in range(2):\n        axes[i][j].tick_params(axis='x', labelsize=15)\n        axes[i][j].tick_params(axis='y', labelsize=15)\n        axes[i][j].set_xlabel('')\n        axes[i][j].set_ylabel('')\n        \nfor i, feature in enumerate(['cp_type', 'cp_time', 'cp_dose']):\n    for j, dataset in enumerate(['Training', 'Test']):\n        axes[i][j].set_title(f'{dataset} Set {feature} Distribution', size=18, pad=18)\n\nplt.show()","6de62abf":"df_control = df_train[df_train['cp_type'] == 'ctl_vehicle']\ndf_compound = df_train[df_train['cp_type'] == 'trt_cp']\n\nprint(f'{len(df_control)}\/{len(df_train)} samples are treated with a control perturbation and {len(df_control[df_control[target_features_scored].sum(axis=1) == 0])}\/{len(df_control)} of those samples have all zero targets')\nprint(f'{len(df_compound)}\/{len(df_train)} samples are treated with a compound and {len(df_compound[df_compound[target_features_scored].sum(axis=1) == 0])}\/{len(df_compound)} of those samples have all zero targets')","4d08b239":"df_target_counts_by_cp_time = pd.DataFrame(columns=['target', 'cp_time', 'count'])\n\nfor target_feature in target_features_scored:    \n    for cp_time in [24, 48, 72]:\n        count = len(df_train[(df_train['cp_time'] == cp_time) & (df_train[target_feature] == 1)])\n        df_target_counts_by_cp_time = df_target_counts_by_cp_time.append({'target': target_feature, 'cp_time': cp_time, 'count': count}, ignore_index=True)\n        \ndf_target_counts_by_cp_time['total_count'] = df_target_counts_by_cp_time.groupby('target')['count'].transform('sum')\ndf_target_counts_by_cp_time.sort_values(by=['total_count', 'target'],ascending=False, inplace=True)\n\nfig = plt.figure(figsize=(15, 75), dpi=100)\n\nsns.barplot(x=df_target_counts_by_cp_time['count'],\n            y=df_target_counts_by_cp_time['target'],\n            hue=df_target_counts_by_cp_time['cp_time'])\n\nplt.xlabel('')\nplt.ylabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0, prop={'size': 20})\nplt.title('Training Set cp_time Distribution in Scored Targets', size=18, pad=18)\n\nplt.show()\n\ndel df_target_counts_by_cp_time","6d9d4b9f":"df_target_counts_by_cp_dose = pd.DataFrame(columns=['target', 'cp_dose', 'count'])\n\nfor target_feature in target_features_scored:    \n    for cp_dose in ['D1', 'D2']:\n        count = len(df_train[(df_train['cp_dose'] == cp_dose) & (df_train[target_feature] == 1)])\n        df_target_counts_by_cp_dose = df_target_counts_by_cp_dose.append({'target': target_feature, 'cp_dose': cp_dose, 'count': count}, ignore_index=True)\n        \ndf_target_counts_by_cp_dose['total_count'] = df_target_counts_by_cp_dose.groupby('target')['count'].transform('sum')\ndf_target_counts_by_cp_dose.sort_values(by=['total_count', 'target'],ascending=False, inplace=True)\n\nfig = plt.figure(figsize=(15, 75), dpi=100)\n\nsns.barplot(x=df_target_counts_by_cp_dose['count'],\n            y=df_target_counts_by_cp_dose['target'],\n            hue=df_target_counts_by_cp_dose['cp_dose'])\n\nplt.xlabel('')\nplt.ylabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0, prop={'size': 20})\nplt.title('Training Set cp_dose Distribution in Scored Targets', size=18, pad=18)\n\nplt.show()\n\ndel df_target_counts_by_cp_dose","8e718802":"df_train[(['sig_id'] + c_features)]","19c8b511":"def scatterplot_cfeature(feature_group, seeds):\n        \n    fig, axes = plt.subplots(ncols=5, figsize=(36, 5), dpi=100, constrained_layout=True)\n    title_size = 25\n    label_size = 25\n\n    for i, feature in enumerate(feature_group):\n        \n        np.random.seed(seeds[i])\n        target = np.random.choice(target_features_scored)\n        if len(target) > 25:\n            target_title = target[:25]\n        else:\n            target_title = target\n            \n        sns.scatterplot(df_train[feature], df_train[target], s=100, ax=axes[i])\n        axes[i].set_xlabel('')\n        axes[i].set_ylabel('')\n        axes[i].tick_params(axis='x', labelsize=label_size)\n        axes[i].tick_params(axis='y', labelsize=label_size)\n        \n        for label in axes[i].get_yticklabels():\n            if i % 5 == 0:\n                label.set_visible(True)\n            else:\n                label.set_visible(False)\n                \n        axes[i].set_title(f'{feature} vs {target_title}', size=title_size, pad=title_size)\n    \n    plt.show()\n    \nfor i, feature_group in enumerate(np.array_split(c_features, len(c_features) \/\/ 5), 2):\n    scatterplot_cfeature(feature_group, seeds=np.arange(1, 6) * i)","6a4d81f6":"def distplot_cfeature(feature_group):\n        \n    fig, axes = plt.subplots(ncols=5, figsize=(36, 5), dpi=100, constrained_layout=True)\n    title_size = 25\n    label_size = 25\n\n    for i, feature in enumerate(feature_group):\n        sns.distplot(df_train[feature], label='Training', ax=axes[i], hist_kws={'alpha': 0.25})\n        sns.distplot(df_test[feature], label='Test', ax=axes[i], hist_kws={'alpha': 0.25})\n        axes[i].set_xlabel('')\n        axes[i].tick_params(axis='x', labelsize=label_size)\n        axes[i].tick_params(axis='y', labelsize=label_size)\n        \n        if i % 5 == 0:\n            axes[i].legend(prop={'size': 25})\n            \n        axes[i].set_title(f'{feature} Distribution', size=title_size, pad=title_size)\n    \n    plt.show()\n    \nfor feature_group in np.array_split(c_features, len(c_features) \/\/ 5):\n    \n    for c_feature in feature_group: \n        train_mean = df_train[c_feature].mean()\n        train_median = df_train[c_feature].median()\n        train_std = df_train[c_feature].std()\n        train_min = df_train[c_feature].min()\n        train_max = df_train[c_feature].max()\n        train_skew = skew(df_train[c_feature])\n        train_kurt = kurtosis(df_train[c_feature])\n        train_var = np.var(df_train[c_feature])\n\n        test_mean = df_test[c_feature].mean()\n        test_median = df_test[c_feature].median()\n        test_std = df_test[c_feature].std()\n        test_min = df_test[c_feature].min()\n        test_max = df_test[c_feature].max()\n        test_skew = skew(df_test[c_feature])\n        test_kurt = kurtosis(df_test[c_feature])\n        test_var = np.var(df_test[c_feature])\n\n        print(f'{c_feature} Train - Mean: {train_mean:.4} - Median: {train_median:.4} - Std: {train_std:.4} - Min: {train_min:.4} - Max: {train_max:.4} - Skew {train_skew:.4} - Kurt {train_kurt:.4} - Var {train_var:.4}')\n        print(f'{c_feature} Test - Mean: {test_mean:.4} - Median: {test_median:.4} - Std: {test_std:.4} - Min: {test_min:.4} - Max: {test_max:.4} - Skew {test_skew:.4} - Kurt {test_kurt:.4} - Var {test_var:.4}\\n')\n\n    distplot_cfeature(feature_group)\n","df16880c":"fig = plt.figure(figsize=(20, 20))\n\nax = sns.heatmap(df_train[c_features].corr(),\n                 annot=False,\n                 square=True)\n\nax.tick_params(axis='x', labelsize=20, rotation=0, pad=20)\nax.tick_params(axis='y', labelsize=20, rotation=0, pad=20)\n\nfor idx, label in enumerate(ax.get_xticklabels()):\n    if idx % 5 == 0:\n        label.set_visible(True)\n    else:\n        label.set_visible(False)\n        \nfor idx, label in enumerate(ax.get_yticklabels()):\n    if idx % 5 == 0:\n        label.set_visible(True)\n    else:\n        label.set_visible(False)\n        \ncbar = ax.collections[0].colorbar\ncbar.ax.tick_params(labelsize=30, pad=20)\n\nplt.title('Cell Viability Features Correlations', size=30, pad=30)\nplt.show()","3b65b7a8":"df_train[(['sig_id'] + g_features)]","f718110c":"def scatterplot_gfeature(feature_group, seeds):\n        \n    fig, axes = plt.subplots(ncols=4, figsize=(36, 5), dpi=100, constrained_layout=True)\n    title_size = 25\n    label_size = 25\n\n    for i, feature in enumerate(feature_group):\n                \n        np.random.seed(seeds[i])\n        target = np.random.choice(target_features_scored)\n        if len(target) > 25:\n            target_title = target[:25]\n        else:\n            target_title = target\n            \n        sns.scatterplot(df_train[feature], df_train[target], s=100, ax=axes[i])\n        axes[i].set_xlabel('')\n        axes[i].set_ylabel('')\n        axes[i].tick_params(axis='x', labelsize=label_size)\n        axes[i].tick_params(axis='y', labelsize=label_size)\n        \n        for label in axes[i].get_yticklabels():\n            if i % 5 == 0:\n                label.set_visible(True)\n            else:\n                label.set_visible(False)\n                \n        axes[i].set_title(f'{feature} vs {target_title}', size=title_size, pad=title_size)\n    \n    plt.show()\n    \nshuffled_g_features = np.copy(g_features)\nnp.random.shuffle(shuffled_g_features)\nfor i, feature_group in enumerate(np.array_split(shuffled_g_features, len(shuffled_g_features) \/\/ 4)[:10], 1):\n    scatterplot_gfeature(feature_group, seeds=np.arange(1, 6) * i)","af8f8099":"def distplot_gfeature(feature_group):\n        \n    fig, axes = plt.subplots(ncols=4, figsize=(36, 5), dpi=100, constrained_layout=True)\n    title_size = 25\n    label_size = 25\n\n    for i, feature in enumerate(feature_group):\n        sns.distplot(df_train[feature], label='Training', ax=axes[i], hist_kws={'alpha': 0.25})\n        sns.distplot(df_test[feature], label='Test', ax=axes[i], hist_kws={'alpha': 0.25})\n        axes[i].set_xlabel('')\n        axes[i].tick_params(axis='x', labelsize=label_size)\n        axes[i].tick_params(axis='y', labelsize=label_size)\n        \n        if i % 5 == 0:\n            axes[i].legend(prop={'size': 25})\n            \n        axes[i].set_title(f'{feature} Distribution', size=title_size, pad=title_size)\n    \n    plt.show()\n\nshuffled_g_features = np.copy(g_features)\nnp.random.shuffle(shuffled_g_features)\nfor feature_group in np.array_split(shuffled_g_features, len(shuffled_g_features) \/\/ 4)[:10]:\n    \n    for c_feature in feature_group: \n        train_mean = df_train[c_feature].mean()\n        train_median = df_train[c_feature].median()\n        train_std = df_train[c_feature].std()\n        train_min = df_train[c_feature].min()\n        train_max = df_train[c_feature].max()\n        train_skew = skew(df_train[c_feature])\n        train_kurt = kurtosis(df_train[c_feature])\n        train_var = np.var(df_train[c_feature])\n\n        test_mean = df_test[c_feature].mean()\n        test_median = df_test[c_feature].median()\n        test_std = df_test[c_feature].std()\n        test_min = df_test[c_feature].min()\n        test_max = df_test[c_feature].max()\n        test_skew = skew(df_test[c_feature])\n        test_kurt = kurtosis(df_test[c_feature])\n        test_var = np.var(df_test[c_feature])\n\n        print(f'{c_feature} Train - Mean: {train_mean:.4} - Median: {train_median:.4} - Std: {train_std:.4} - Min: {train_min:.4} - Max: {train_max:.4} - Skew {train_skew:.4} - Kurt {train_kurt:.4} - Var {train_var:.4}')\n        print(f'{c_feature} Test - Mean: {test_mean:.4} - Median: {test_median:.4} - Std: {test_std:.4} - Min: {test_min:.4} - Max: {test_max:.4} - Skew {test_skew:.4} - Kurt {test_kurt:.4} - Var {test_var:.4}\\n')\n\n    distplot_gfeature(feature_group)\n","0ce73616":"fig = plt.figure(figsize=(20, 20))\n\nax = sns.heatmap(df_train[g_features].corr(),\n                 annot=False,\n                 square=True)\n\nax.tick_params(axis='x', labelsize=20, rotation=90, pad=20)\nax.tick_params(axis='y', labelsize=20, rotation=0, pad=20)\n\nfor idx, label in enumerate(ax.get_xticklabels()):\n    if idx % 5 == 0:\n        label.set_visible(True)\n    else:\n        label.set_visible(False)\n        \nfor idx, label in enumerate(ax.get_yticklabels()):\n    if idx % 5 == 0:\n        label.set_visible(True)\n    else:\n        label.set_visible(False)\n        \ncbar = ax.collections[0].colorbar\ncbar.ax.tick_params(labelsize=30, pad=20)\n\nplt.title('Gene Expression Features Correlations', size=30, pad=30)\nplt.show()","6a6b7d90":"df_train['target'] = 0\ndf_test['target'] = 1\n\nX = pd.concat([df_train.loc[:, g_features + c_features], df_test.loc[:, g_features + c_features]]).reset_index(drop=True)\ny = pd.concat([df_train.loc[:, 'target'], df_test.loc[:, 'target']]).reset_index(drop=True)","dbeef075":"K = 5\nskf = StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED)\n\nscores = []\noof_predictions = pd.DataFrame(np.zeros((X.shape[0], 1)), columns=['target'])\nfeature_importance = pd.DataFrame(np.zeros((X.shape[1], K)), columns=[f'Fold_{i}_Importance' for i in range(1, K + 1)], index=X.columns)\n\nparameters = {\n    'num_iterations': 500,\n    'early_stopping_round': 50,\n    'num_leaves': 2 ** 5, \n    'learning_rate': 0.05,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 1,\n    'feature_fraction': 0.9,\n    'feature_fraction_bynode': 0.9,\n    'lambda_l1': 0,\n    'lambda_l2': 0,\n    'max_depth': -1,\n    'objective': 'binary',\n    'seed': SEED,\n    'feature_fraction_seed': SEED,\n    'bagging_seed': SEED,\n    'drop_seed': SEED,\n    'data_random_seed': SEED,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'auc',\n    'n_jobs': -1,   \n}\n\nprint('Running LightGBM Adversarial Validation Model\\n' + ('-' * 45) + '\\n')\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X, y), 1):\n\n    trn_data = lgb.Dataset(X.iloc[trn_idx, :], label=y.iloc[trn_idx])\n    val_data = lgb.Dataset(X.iloc[val_idx, :], label=y.iloc[val_idx])      \n    model = lgb.train(parameters, trn_data, valid_sets=[trn_data, val_data], verbose_eval=50)\n    feature_importance.iloc[:, fold - 1] = model.feature_importance(importance_type='gain')\n\n    predictions = model.predict(X.iloc[val_idx, :], num_iteration=model.best_iteration)\n    oof_predictions.loc[val_idx, 'target'] = predictions\n    \n    score = roc_auc_score(y.iloc[val_idx], predictions)\n    scores.append(score)            \n    print(f'\\nFold {fold} - ROC AUC Score {score:.6}\\n')\n    \noof_score = roc_auc_score(y, oof_predictions)\nprint(f'\\n{\"-\" * 30}\\nLightGBM Adversarial Validation Model Mean ROC AUC Score {np.mean(scores):.6} [STD:{np.std(scores):.6}]')\nprint(f'LightGBM Adversarial Validation Model OOF ROC AUC Score: {oof_score:.6}\\n{\"-\" * 30}')\n\n\nplt.figure(figsize=(20, 20))\nfeature_importance['Mean_Importance'] = feature_importance.sum(axis=1) \/ K\nfeature_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=feature_importance.index[:50], data=feature_importance[:50])\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('LightGBM Adversarial Validation Model Top 50 Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()\n\ndel X, y, oof_predictions, feature_importance, parameters, scores, oof_score","f91b7c41":"df_all = pd.concat([df_train[c_features + g_features], df_test[c_features + g_features]], axis=0, ignore_index=True).astype(np.float32)\n\nvariances = {}\n\nfor feature in df_all.columns:\n    variances[feature] = df_all[feature].var()\n    \ndel df_all\n\ndf_variances = pd.DataFrame.from_dict(variances, orient='index', columns=['Variance'])\ndf_variances['Variance'] = df_variances['Variance'].astype(np.float32)\ndf_variances.sort_values(by='Variance', ascending=True, inplace=True)\ndf_variances.head(10)","4bc4c2fe":"def distplot_variances(feature_group):\n        \n    fig, axes = plt.subplots(ncols=4, figsize=(36, 5), dpi=100, constrained_layout=True)\n    title_size = 30\n    label_size = 25\n\n    for i, feature in enumerate(feature_group):\n        sns.distplot(df_train[feature], label='Training', ax=axes[i], hist_kws={'alpha': 0.25})\n        sns.distplot(df_test[feature], label='Test', ax=axes[i], hist_kws={'alpha': 0.25})\n        axes[i].set_xlabel('')\n        axes[i].tick_params(axis='x', labelsize=label_size)\n        axes[i].tick_params(axis='y', labelsize=label_size)\n        \n        if i % 4 == 0:\n            axes[i].legend(prop={'size': 25})\n            \n        axes[i].set_title(f'{feature} - Variance: {df_variances.loc[feature, \"Variance\"]:.6}', size=title_size, pad=title_size)\n    \n    plt.show()\n    \nfor i in np.arange(0.3, 1.5, 0.1):\n    feature_group = list(df_variances.loc[(df_variances['Variance'] >= i) & (df_variances['Variance'] < (i + 0.1)), 'Variance'].index)[:4]\n    distplot_variances(feature_group)","c8e6516e":"df_all = pd.concat([df_train[c_features + g_features], df_test[c_features + g_features]], axis=0, ignore_index=True).astype(np.float32)\n\nnp.random.seed(SEED)\nscale_features = list(np.random.choice(c_features, 25, replace=False)) + list(np.random.choice(g_features, 25, replace=False))\n\ndf_scaled = df_all.loc[:, scale_features]\ndel df_all\n\nmms = MinMaxScaler()\nss = StandardScaler()\ngrs = GaussRankScaler()\n\nfor feature in scale_features:\n    df_scaled[f'{feature}_mms'] = mms.fit_transform(df_scaled[feature].values.reshape(-1, 1))\n    df_scaled[f'{feature}_ss'] = ss.fit_transform(df_scaled[feature].values.reshape(-1, 1))\n    df_scaled[f'{feature}_grs'] = grs.fit_transform(df_scaled[feature].values.reshape(-1, 1))\n\ndf_scaled = df_scaled.astype(np.float32)\ndf_scaled.head(10)","e350cb05":"def distplot_scaled_features(feature):\n        \n    fig, axes = plt.subplots(ncols=4, figsize=(36, 5), dpi=100, constrained_layout=True)\n    title_size = 30\n    label_size = 25\n    \n    sns.distplot(df_scaled.loc[:len(df_train), feature], label='Training', ax=axes[0], hist_kws={'alpha': 0.25})\n    sns.distplot(df_scaled.loc[len(df_train):, feature], label='Test', ax=axes[0], hist_kws={'alpha': 0.25})\n    sns.distplot(df_scaled.loc[:len(df_train), f'{feature}_mms'], label='Training', ax=axes[1], hist_kws={'alpha': 0.25})\n    sns.distplot(df_scaled.loc[len(df_train):, f'{feature}_mms'], label='Test', ax=axes[1], hist_kws={'alpha': 0.25})\n    sns.distplot(df_scaled.loc[:len(df_train), f'{feature}_ss'], label='Training', ax=axes[2], hist_kws={'alpha': 0.25})\n    sns.distplot(df_scaled.loc[len(df_train):, f'{feature}_ss'], label='Test', ax=axes[2], hist_kws={'alpha': 0.25})\n    sns.distplot(df_scaled.loc[:len(df_train), f'{feature}_grs'], label='Training', ax=axes[3], hist_kws={'alpha': 0.25})\n    sns.distplot(df_scaled.loc[len(df_train):, f'{feature}_grs'], label='Test', ax=axes[3], hist_kws={'alpha': 0.25})\n\n    for i in range(4):\n\n        axes[i].set_xlabel('')\n        axes[i].tick_params(axis='x', labelsize=label_size)\n        axes[i].tick_params(axis='y', labelsize=label_size)\n        \n        if i % 4 == 0:\n            axes[i].legend(prop={'size': 25})\n            \n    axes[0].set_title(f'{feature} Raw', size=title_size, pad=title_size)\n    axes[1].set_title(f'{feature} Min Max Scaled', size=title_size, pad=title_size)\n    axes[2].set_title(f'{feature} Standard Scaled', size=title_size, pad=title_size)\n    axes[3].set_title(f'{feature} Gaussian Rank Scaled', size=title_size, pad=title_size)\n    \n    plt.show()\n    \nfor feature in scale_features:\n    distplot_scaled_features(feature)","f70ae801":"df_cell_viability = pd.concat([df_train.loc[:, c_features], df_test.loc[:, c_features]], axis=0, ignore_index=True)\ndf_cell_viability.loc[:, :] = np.float32(StandardScaler().fit_transform(df_cell_viability))\n\ndf_gene_expression = pd.concat([df_train.loc[:, g_features], df_test.loc[:, g_features]], axis=0, ignore_index=True)\ndf_gene_expression.loc[:, :] = np.float32(StandardScaler().fit_transform(df_gene_expression))\n\nCELL_N_DIMS = len(c_features) \/\/ 2\nGENE_N_DIMS = len(g_features) \/\/ 2","07d53c22":"seed_everything(SEED)\n\nfig, axes = plt.subplots(figsize=(32, 8), ncols=2)\n\npca = PCA(n_components=CELL_N_DIMS)\npca.fit(df_cell_viability)\npca_cell_viability_error = mean_squared_error(df_cell_viability, pca.inverse_transform(pca.transform(df_cell_viability)))\ncell_viability_explained_variance = np.sum(pca.explained_variance_) \/ len(c_features) * 100\nprint(f'PCA Cell Viability Reconstruction Error {pca_cell_viability_error:.6} - Explained Variance: {cell_viability_explained_variance:.6}%')\n\naxes[0].plot(np.cumsum(pca.explained_variance_ratio_))\naxes[0].set_xlabel('Number of Components', size=20, labelpad=20)\naxes[0].set_ylabel('CPV', size=20, labelpad=20)\naxes[0].set_title(f'Cell Viability Features CPV Curve', size=22, pad=22)\n\npca = PCA(n_components=GENE_N_DIMS)\npca.fit(df_gene_expression)\npca_gene_expression_error = mean_squared_error(df_gene_expression, pca.inverse_transform(pca.transform(df_gene_expression)))\ngene_expression_explained_variance = np.sum(pca.explained_variance_) \/ len(g_features) * 100\nprint(f'PCA Gene Expression Reconstruction Error {pca_gene_expression_error:.6} - Explained Variance: {gene_expression_explained_variance:.6}%')\n\naxes[1].plot(np.cumsum(pca.explained_variance_ratio_))\naxes[1].set_xlabel('Number of Components', size=20, labelpad=20)\naxes[1].set_ylabel('CPV', size=20, labelpad=20)\naxes[1].set_title('Gene Expression Features CPV Curve', size=22, pad=22)\n\nfor i in range(2):\n    axes[i].tick_params(axis='x', labelsize=20)\n    axes[i].tick_params(axis='y', labelsize=20)\n    \nplt.show()","df1717fb":"class CellViabilityAutoencoder(Model):\n    \n    def __init__(self, n_inputs, encoding_dim):\n        \n        super(CellViabilityAutoencoder, self).__init__()\n        self.encoder = Sequential([\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(2 ** 9, activation='swish', activity_regularizer=L2(0.001)),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(2 ** 8, activation='swish', activity_regularizer=L2(0.001)),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(encoding_dim, activation='swish'),\n        ])\n        self.decoder = Sequential([\n            Dense(2 ** 8, activation='swish', activity_regularizer=L2(0.001)),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(2 ** 9, activation='swish', activity_regularizer=L2(0.001)),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(n_inputs)\n        ])\n        \n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\nseed_everything(SEED)\n\ncell_viability_ae = CellViabilityAutoencoder(len(c_features), CELL_N_DIMS)\ncell_viability_ae.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=0)\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=100, restore_best_weights=True, verbose=0)\n\nhistory = cell_viability_ae.fit(df_cell_viability,\n                                df_cell_viability,\n                                batch_size=128,\n                                verbose=0,\n                                validation_data=(df_cell_viability, df_cell_viability),\n                                epochs=100,\n                                shuffle=True,\n                                callbacks=[reduce_lr, early_stopping])\n\ncell_viability_encoded = cell_viability_ae.encoder(df_cell_viability.values).numpy()\ncell_viability_decoded = cell_viability_ae.decoder(cell_viability_encoded).numpy()\ncell_viability_error = mean_squared_error(df_cell_viability.values, cell_viability_decoded)\nprint(f'Auto-encoder Cell Viability Reconstruction Error {cell_viability_error:.6}')\n\nplt.figure(figsize=(32, 8))\n\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\n\nplt.xlabel('Epochs', size=20, labelpad=20)\nplt.ylabel('Mean Squared Error', size=20, labelpad=20)\nplt.tick_params(axis='x', labelsize=20)\nplt.tick_params(axis='y', labelsize=20)\nplt.legend(prop={'size': 20})\nplt.title('Cell Viability Features Auto-encoder Loss', size=22, pad=22)\n\nplt.show()","ac55d3ba":"class GeneExpressionAutoencoder(Model):\n    \n    def __init__(self, n_inputs, encoding_dim):\n        \n        super(GeneExpressionAutoencoder, self).__init__()\n        self.encoder = Sequential([\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(2 ** 9, activation='swish', activity_regularizer=L2(0.001)),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(2 ** 8, activation='swish', activity_regularizer=L2(0.001)),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(encoding_dim, activation='swish'),\n        ])\n        self.decoder = Sequential([\n            Dense(2 ** 8, activation='swish', activity_regularizer=L2(0.001)),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(2 ** 9, activation='swish', activity_regularizer=L2(0.001)),\n            BatchNormalization(),\n            Dropout(0.1),\n            Dense(n_inputs)\n        ])\n        \n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\nseed_everything(SEED)\n\ngene_expression_ae = GeneExpressionAutoencoder(len(g_features), GENE_N_DIMS)\ngene_expression_ae.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=0)\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=100, restore_best_weights=True, verbose=0)\n\nhistory = gene_expression_ae.fit(df_gene_expression,\n                                 df_gene_expression,\n                                 batch_size=128,\n                                 verbose=0,\n                                 validation_data=(df_gene_expression, df_gene_expression),\n                                 epochs=100,\n                                 shuffle=True,\n                                 callbacks=[reduce_lr, early_stopping])\n\ngene_expression_encoded = gene_expression_ae.encoder(df_gene_expression.values).numpy()\ngene_expression_decoded = gene_expression_ae.decoder(gene_expression_encoded).numpy()\ngene_expression_error = mean_squared_error(df_gene_expression.values, gene_expression_decoded)\nprint(f'Auto-encoder Gene Expression Reconstruction Error {gene_expression_error:.6}')\n\nplt.figure(figsize=(32, 8))\n\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\n\nplt.xlabel('Epochs', size=20, labelpad=20)\nplt.ylabel('Mean Squared Error', size=20, labelpad=20)\nplt.tick_params(axis='x', labelsize=20)\nplt.tick_params(axis='y', labelsize=20)\nplt.legend(prop={'size': 20})\nplt.title('Gene Expression Features Auto-encoder Loss', size=22, pad=22)\n\nplt.show()","f376ba25":"df_train_drug = pd.read_csv('..\/input\/lish-moa\/train_drug.csv')\nprint(f'Training Drugs Shape = {df_train_drug.shape} - Unique Drugs {df_train_drug[\"drug_id\"].nunique()}')\nprint(f'Training Drugs Memory Usage = {df_train_drug.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\n\ndf_train = df_train.merge(df_train_drug, how='left', on='sig_id')\ncols = list(df_train.columns)\ncols = [cols[-1]] + cols[:-1]\ndf_train = df_train[cols]\n\ndf_train_drug.head(10)","330f2480":"df_drug_frequencies = df_train['drug_id'].value_counts()\n\nfig = plt.figure(figsize=(15, 75))\n\nsns.barplot(x=df_drug_frequencies.head(200).values,\n            y=df_drug_frequencies.head(200).index)\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.xlabel('')\nplt.ylabel('')\nplt.title('Training Set Drug Frequencies', size=22, pad=22)\n\nplt.show()\n\ndel df_drug_frequencies","30e5a0ae":"df_drug_occurences = df_train['drug_id'].value_counts().value_counts()\ndf_drug_occurences.sort_values(ascending=False, inplace=True)\n\nfig = plt.figure(figsize=(32, 6))\n\nsns.barplot(x=df_drug_occurences.index,\n            y=df_drug_occurences.values,\n            order=df_drug_occurences.index)\n\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.xlabel('')\nplt.ylabel('')\nplt.title('Training Set Drug Occurence Frequencies', size=22, pad=22)\n\nplt.show()\n\ndel df_drug_occurences","685fd6e5":"df_drug_counts_by_cp_time = pd.DataFrame(df_train.groupby(['drug_id', 'cp_time'])['drug_id'].count())\ndf_drug_counts_by_cp_time.rename(columns={'drug_id': 'count'},inplace=True)\ndf_drug_counts_by_cp_time.reset_index(inplace=True)\ndf_drug_counts_by_cp_time['count_max'] = df_drug_counts_by_cp_time.groupby('drug_id')['count'].transform('max')\ndf_drug_counts_by_cp_time.sort_values(by=['count_max', 'drug_id'], ascending=False, inplace=True)\n\nfig = plt.figure(figsize=(15, 75), dpi=100)\n\nsns.barplot(x=df_drug_counts_by_cp_time['count'].values[:300],\n            y=df_drug_counts_by_cp_time['drug_id'].values[:300],\n            hue=df_drug_counts_by_cp_time['cp_time'].values[:300])\n\nplt.xlabel('')\nplt.ylabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0, prop={'size': 20})\nplt.title('Training Set cp_time Distribution in Drugs', size=22, pad=22)\n\nplt.show()\n\ndel df_drug_counts_by_cp_time","74880478":"df_drug_counts_by_cp_dose = pd.DataFrame(df_train.groupby(['drug_id', 'cp_dose'])['drug_id'].count())\ndf_drug_counts_by_cp_dose.rename(columns={'drug_id': 'count'},inplace=True)\ndf_drug_counts_by_cp_dose.reset_index(inplace=True)\ndf_drug_counts_by_cp_dose['count_max'] = df_drug_counts_by_cp_dose.groupby('drug_id')['count'].transform('max')\ndf_drug_counts_by_cp_dose.sort_values(by=['count_max', 'drug_id'], ascending=False, inplace=True)\n\nfig = plt.figure(figsize=(15, 75), dpi=100)\n\nsns.barplot(x=df_drug_counts_by_cp_dose['count'].values[:300],\n            y=df_drug_counts_by_cp_dose['drug_id'].values[:300],\n            hue=df_drug_counts_by_cp_dose['cp_dose'].values[:300])\n\nplt.xlabel('')\nplt.ylabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0, prop={'size': 20})\nplt.title('Training Set cp_dose Distribution in Drugs', size=22, pad=22)\n\nplt.show()\n\ndel df_drug_counts_by_cp_dose","b5948657":"def plot_signature(sig_id):\n    \n    if sig_id in df_train['sig_id'].values:\n        g = df_train.loc[df_train['sig_id'] == sig_id, g_features].T\n        c = df_train.loc[df_train['sig_id'] == sig_id, c_features].T\n        t_scored = df_train.loc[df_train['sig_id'] == sig_id, target_features_scored].T\n        t_notscored = df_train.loc[df_train['sig_id'] == sig_id, target_features_nonscored].T\n        \n        print(f'{sig_id}\\n{\"-\" * (len(sig_id))}')\n        print(f'Gene Expression Mean: {np.mean(g.values):.4} - Median: {np.median(g.values):.4} - Std: {np.std(g.values):.4} - Min: {np.min(g.values):.4} - Max: {np.max(g.values):.4} - Skew {skew(g.values)[0]:.4} - Kurt {kurtosis(g.values)[0]:.4} - Var {np.var(g.values):.4}')\n        print(f'Cell Viability Mean: {np.mean(c.values):.4} - Median: {np.median(c.values):.4} - Std: {np.std(c.values):.4} - Min: {np.min(c.values):.4} - Max: {np.max(c.values):.4} - Skew {skew(c.values)[0]:.4} - Kurt {kurtosis(c.values)[0]:.4} - Var {np.var(c.values):.4}')\n        print(f'Target Features Scored Mean {np.mean(t_scored.values):.4} - Target Features Not Scored Mean {np.mean(t_notscored.values):.4}')\n\n        fig, axes = plt.subplots(nrows=2, figsize=(32, 12), dpi=100)\n        plt.subplots_adjust(left=None, bottom=None, right=None, top=1.50, wspace=None, hspace=None)\n        \n        axes[0].plot(g, label='Gene Expression')\n        axes[0].plot(c, label='Cell Viability')\n        axes[1].plot(t_scored, label='Target Features Scored')\n        axes[1].plot(t_notscored, label='Target Features Not Scored')\n        \n        axes[0].tick_params(axis='x', labelsize=20, rotation=0, pad=15, length=0)\n        axes[1].tick_params(axis='x', labelsize=20, rotation=90, pad=15, length=0)\n        for i in range(2):            \n            axes[i].tick_params(axis='y', labelsize=20, rotation=0, pad=15, length=0)\n            axes[i].legend(prop={'size': 20})\n\n        for idx, label in enumerate(axes[0].get_xticklabels()):\n            if idx % 50 == 0:\n                label.set_visible(True)\n            else:\n                label.set_visible(False)\n                \n        for idx, label in enumerate(axes[1].get_xticklabels()):\n            if df_train.loc[df_train['sig_id'] == sig_id, (target_features_scored + target_features_nonscored)[idx]].values[0] == 1:\n                label.set_visible(True)\n            else:\n                label.set_visible(False)\n                \n        axes[0].set_title(f'Signature {sig_id} Gene Expression and Cell Viability Sequence', size=25, pad=15)\n        axes[1].set_title(f'Signature {sig_id} Target Features Sequence', size=25, pad=15)\n    else:\n        g = df_test.loc[df_test['sig_id'] == sig_id, g_features].T\n        c = df_test.loc[df_test['sig_id'] == sig_id, c_features].T\n        \n        fig = plt.figure(figsize=(32, 8), dpi=100)\n        \n        plt.plot(g, label='Gene Expression')\n        plt.plot(c, label='Cell Viability')\n            \n    plt.show()\n","9078e7ac":"plot_signature('id_344ef5019')","f79fabf9":"plot_signature('id_063e8e7d6')","a9897828":"plot_signature('id_8c4b726ff')","86b8c109":"plot_signature('id_0c830d384')","955837a8":"plot_signature('id_003603254')","87dd79e9":"plot_signature('id_0da23653d')","9bc18835":"plot_signature('id_000a6266a')","62b9fb2d":"plot_signature('id_163bd7f3e')","f21f5851":"plot_signature('id_0024bcd70')","e67ddf44":"plot_signature('id_001bd861f')","aca61d53":"plot_signature('id_001626bd3')","949fcc22":"plot_signature('id_002d31e2c')","2e08cf75":"plot_signature('id_000779bfc')","14546efb":"plot_signature('id_000644bb2')","45661d0c":"There are 36 unique signatures that are classified to **3** not scored targets. They belong to 5 unique drugs, and this signature belongs to drug `fe8625cad`. There is no clear anomaly in this signature, but cell viability variance and standard deviation are lesser than gene expression variance and standard deviation just like the previous signature.","75b307ad":"### **5.4. Cell Viability Feature Correlations**\n\nCell viability feature correlations are extremely high. This can be related to drug effects being similar on most cells lines. Lowest correlations (0.65) are the black lines which belong to `c-37`, `c-58`, `c-69`, `c-74` and `c-76` features due to their unusual distributions.","fb315ae9":"### **5.3. Cell Viability Feature Distributions**\n\nCell viability feature distributions are very similar in training and test set, that's because, quantile normalization and z-scoring steps are done at the plate level once, not separately for training and test set. A typical plate contains 1,200 signatures and those signatures can be placed in both training or test set. Plate information is not provided in the dataset.\n\nMost common cell viability feature distribution looks like a left-skewed bell curve with mean close to 0.5. Features with most different distributions are `c-37`, `c-58`, `c-69`, `c-74` and `c-76` because of their shorter tails. They have higher overall cell viability. Minimum values in other features are clipped at -10, but it wasn't required for formerly mentioned features.","668c1368":"There are 6 unique signatures that are classified to **7** not scored targets. All of them belong to drug `838575b1d`. Those 6 signatures are classified to 4 scored targets. Unlike the previous signature, this one doesn't have any anomalies.","e3130c5d":"### **9.2. Auto-encoder**\n\nAuto-encoders are neural networks used for reducing data into a low dimensional latent space. Most important features lie in this low dimensional latent space because they are capable of reconstructing it. Auto-encoders are slower and computationally expensive compared to PCA, and they are also prone to overfitting.","a7c5d6d2":"### **6.4. Gene Expression Feature Correlations**\n\nGene expression feature correlations are more diverse compared to cell viability feature correlations. Correlation coefficients vary from -0.75 to 1, because of different distribution types.","ad3060ba":"Different scalers scale features into different ranges. Min max scaled data are between 0 and 1, while standard scaled data and gaussian rank scaled data are zero centric. Zero centric data perform better in algorithms like PCA, on the other hand min max scaled data may perform better in neural networks.\n\nMin max scaler and standard scaler are heavily affected by outliers, however gaussian rank scaler yields more robust results. Gaussian rank scaled data have symmetrical tails unlike others. ","7d4ad4c0":"There are two groups of target features; scored target features and non-scored target features. Both of those groups consist of binary MoA targets but only the first group is used for the scoring, so this is a multi-label classification problem. Even though non-scored target features don't have any impact on the score, they can be useful.","94f5f616":"There are 4282 unique signatures that are classified to **1** not scored target. They belong to 704 unique drugs, and this signature belongs to drug `e0cd5c091`. In this case, cell viability values have higher standard deviation and variance, and they are on different scales.","d8f80a9d":"0 target signatures are the most common group for not scored targets in training set. There are 19224 unique signatures that are classified to **0** scored targets. They belong to 2534 unique drugs, and this signature belongs to drug `b68db1d53`. There is no clear anomaly in this signature, but cell viability variance and standard deviation are lesser than gene expression variance and standard deviation just like the previous signature.","1580b421":"### **4.1. cp_type**\n\n`cp_type` is the first categorical feature in the dataset and it is a binary feature. It either means that samples are treated with a compound (`trt_cp`) or with a control perturbation (`ctl_vehicle`). Samples treated with control perturbations have no MoAs, thus all of their scored and non-scored target labels are zeros. However, all zero labeled samples are not entirely treated with a control perturbation, more than 1\/3 of the compound samples are also labeled as zeros.","148fda0d":"There are very few unique drug occurences. Most common one is 6 because there are 2 different doses and 3 different durations to begin with. 2774 out of 3289 unique drugs are in this group. 196 unique drugs are seen 7 times and 129 drugs are seen only one time.\n\n1866 is the drug (`cacb2b860`) with the highest amount of occurence and it is only seen in control group (`ctl_vehicle`). This explains the unusual number of occurences for this one. This drug can be omitted from cross-validation schemes. Other large values are also very unusual and they are not in the control group.","16d86352":"Duration counts are consistent between different drugs just like they were between different targets. Same behavior is expected in test set because some level of stratification is applied while splitting train\/test sets.","75506381":"### **3.1. Scored Target Features**\n\nThe most commmonly classified scored targets are nfkb inhibitor, proteasome inhibitor, cyclooxygenase inhibitor, dopamine receptor antagonist, serotonin receptor antagonist and dna_inhibitor, and there are more than 400 samples classified to each of them. The most rarely classified scored targets are atp-sensitive potassium channel antagonist and erbb2 inhibitor, and there is only one sample classified to each of them. A similar classification distribution is expected in test set.\n\nThere are lots of scored targets classified with the same number of times which suggests there might be a relationship between them.","260fee62":"## **3. Target Features**\n\nTarget features are categorized into two groups; scored and non-scored target features, and features in both of those groups are binary. The competition score is based on the scored target features but non-scored group can still be used for model evaluation, data analysis and feature engineering.\n\nIt is a multi-label classification problem but one sample can be classified to multiple targets or none of the targets as well. Most of the time, samples are classified to 0 or 1 target, but a small part of the training set samples are classified to 2, 3, 4, 5 and 7 different targets at the same time. Classified targets distributions are not very similar for scored targets and non-scored targets since there is a huge discrepancy of 0 and 1 classified targets.","8cd77e30":"### **11.6 Signatures with 1 Target**\n\n1 target signatures are the most common group for scored targets in training set. There are 12532 unique signatures that are classified to **1** scored target. They belong to 1904 unique drugs, and this signature belongs to drug `b68db1d53`. There is no clear anomaly in this signature, and it is very clean.","2c9ef834":"### **1.1. Mechanisms of Action**\n\n**[(\"Mechanism of action\", n.d.)](https:\/\/en.wikipedia.org\/wiki\/Mechanism_of_action)**\n\nIn pharmacology, the term **Mechanism of Action** (MoA) refers to the specific biochemical interaction through which a drug substance produces its pharmacological effect. A mechanism of action usually includes mention of the specific molecular targets to which the drug binds, such as an enzyme or receptor. Receptor sites have specific affinities for drugs based on the chemical structure of the drug, as well as the specific action that occurs there.\n\nIn this competition, the task is predicting multiple targets of the Mechanism of Action (MoA) responses of different samples. Samples are drugs profiled at different time points and doses, and there are more than 5,000 drugs in dataset. Dataset also consists of various group of features and there are more than two hundred targets of enzymes and receptors.","ebab39d2":"## **4. Categorical Features**\n\nThere are three categorical features; `cp_type`, `cp_time` and `cp_dose`. Two of them are binary features and one of them has three unique values, so the cardinality among those features, is very low.\nAll of the categorical features have almost identical distributions in training and public test set, and that suggests training and public test set are taken from the same sample. Samples are probably stratified on those features while splitting training and public test set, and same distributions are also expected in private test set.","5ba9b0ed":"Adversarial validation model yields **0.5275** ROC AUC score which suggests that training and public test set are similar. Features at the top of importance plot have higher gain, because they have different means in training and public test set due to distribution tail extremities. This could be related to small sample size of public test set, and it's not necessarily have to be expected in private test set.","92c7e2ec":"### **6.3. Gene Expression Feature Distributions**\n\nGene expression feature distributions are also very similar in training and test set, that's because, same quantile normalization and z-scoring steps used on cell viability features, are also applied to them at the plate level.\n\nGene expression feature distributions are more diverse than cell viability feature distributions. There are both left\/right tailed and long\/short tailed distributions exist.","1d6f42d7":"There are 6 unique signatures that are classified to **5** not scored targets, and it is a different target group compared to previous signature. All of them belong to drug `91dc8bab8`, and those 6 signatures are classified to 2 scored targets.","ba407d8c":"### **1.2. Features**\n\n* `sig_id` is the unique sample id\n* Features with `g-` prefix are gene expression features and there are 772 of them (from `g-0` to `g-771`)\n* Features with `c-` prefix are cell viability features and there are 100 of them (from `c-0` to `c-99`)\n* `cp_type` is a binary categorical feature which indicates the samples are treated with a compound or with a control perturbation (`trt_cp` or `ctl_vehicle`)\n* `cp_time` is a categorical feature which indicates the treatment duration (`24`, `48` or `72` hours)\n* `cp_dose` is a binary categorical feature which indicates the dose is low or high (`D1` or `D2`)","27eab3e7":"### **7.2. Variance Threshold**\n\nAnother approach for feature selection is eliminating features that have lower variance than a predefined threshold. Variance is the average of the squared differences from the mean, so it measures how far the data points are spread out from the mean. If the variance is low or close to zero, then a feature is approximately constant and will not improve the performances of the models. In that case, it should be removed.\n\nVariances of continuous features are calculated for training and test set together, because they could yield different results if they are calculated separately. There are lots of low variance features that can be seen from below, but they are not close to zero.","684501c3":"### **5.1. Cell Viability Features**\n\nCell viability is a measure of the proportion of live, healthy cells within a population. Cell viability assays are used to determine the overall health of cells, optimize culture or experimental conditions, and to measure cell survival following treatment with compounds, such as during a drug screen.\n\nCell-viability assessment is based on PRISM technology. PRISM is a high-throughput screen for assessing cell viability in which cell lines that have each been labelled with a unique 24-nucleotide barcode are pooled and treated with the experimental condition, and surviving cells are \u201ccounted\u201d through identification of the cognate barcode. PRISM is an acronym for Profiling Relative Inhibition Simultaneously in Mixture. \n\nThere are 100 cell-viability features and they have c- prefix (`c-0` to `c-99`). Each cell-viability feature represents viability of one particular cell line, and all experiments are based on a set of similar cells. These are mostly cancer cells.","f6a53700":"### **3.2. Non-scored Target Features**\n\nThe most commmonly classified non-scored targets are ace inhibitor, purinergic receptor antagonist, map kinase inhibitor, sterol demethylase inhibitor, and there are more than 70 samples classified to each of them. There are 71 non-scored targets that are classified to 0 samples. The classification counts of non-scored targets are very different than scored targets because most of the training samples are not classified to any of them.\n\nTargets classified same number of times is more significant in non-scored targets and they are more likely to be in a relationship.","9001283f":"### **4.2. cp_time**\n\n`cp_time` is the second categorical feature in the dataset and it has three unique values; `24`, `48` and `72` hours. It indicates the treatment durations of the samples. Sample counts of different `cp_time` values are very consistent and close to each other in different targets. Sample counts are either extremely close to each other or `48` is slightly higher than the others.\n\nAll targets have samples with three durations except erbb2 inhibitor and atp-sensitive potassium channel antagonist, because those two targets are only classified in one sample.","66bb3ffb":"### **11.3 Signatures with 4 Targets**\n\nThere are 55 unique signatures that are classified to **4** scored targets. They belong to 8 unique drugs, and this signature belongs to drug `3cda750b5`. There is no clear anomaly in this signature, and it is very clean.","fe93d49b":"## **11. Signatures**\n\nFinally, different samples (signatures) should be analyzed individually for finding feature engineering ideas and detecting outliers. Even though gene expression and cell viability features are independent, signatures can be visualized as a sequence. For this analysis, 14 random signatures are selected. 7 of them are classified to 0, 1, 2, 3, 4, 5 or 7 scored targets, and the other 7 of them are classified to same number of not scored targets. ","fa0aa513":"## **9. Dimensionality Reduction**\n\nTwo common dimensionality reduction techniques are PCA and auto-encoders. Those techniques are sensitive to scale, so it is important to standardize the data and make it unitless. For this purpose, cell viability and gene expression features are standardized with standard scaler. For evaluating information loss in different dimensionality reduction techniques, latent space dimensions are set to half of cell viability and gene expression dimensions.","9b666bd7":"Drug frequencies were expected to be diverse but somewhat balanced. It looks like that's the case, but there are so many unique drugs. Most common drug is `cacb2b860`, and it has 1866 occurences. Other most common drugs are `87d714366`, `9f80f3f77`, `8b87a7a83`, `5628cb3ee`, `d08af5d4b`, `292ab2c28`, `d50f18348`, and `d1b47f29d` in this order. ","42e4d943":"For gene expression features, reconstruction error is worse than PCA reconstruction error, but features created in the gene expression latent space might be still useful for supervised learning models.","9b8dc3d1":"## **6. Gene Expression**","2cf46e8b":"## **5. Cell Viability**","9d1b0f4f":"### **11.1 Signatures with 7 Targets**\n\nThere are 6 unique signatures that are classified to **7** scored targets. All of them belong to drug `91dc8bab8`. Those 6 signatures are classified to 3 not scored targets. Most unusual thing about this drug is, cell viability and gene expression values are on completely different scales. Gene expression features have extremely high standard deviation and variance. Those two statistical measures can be used for outlier detection because ideally cell viability and gene expression values should on same scale.","d6cfa8e6":"### **4.3. cp_dose**\n\n`cp_dose` is the final categorical feature in the dataset and it is also a binary feature. It indicates whether the dose of the samples are either low (`D1`) or high (`D2`). Sample counts of different `cp_dose` values are very consistent and close to each other in different targets. Sample counts are even closer to each other compared to `cp_time` and lots of targets have equal sample counts for both doses.\n\nAll targets have samples with two doses except erbb2 inhibitor and atp-sensitive potassium channel antagonist, because those two targets are only classified in one sample. ","d5683d55":"To sum up, statistical features calculated on signature sequences may yield hidden information about targets. There might be connections between cell viability\/gene expression difference and number of classified targets. Those types of engineered features are definitely worth trying.","38faf628":"For cell viability features, reconstruction error is close to PCA reconstruction error, but there are lots of spikes on learning curve. Features created in the cell viability latent space might be useful for supervised learning models.","17f24594":"There are 13 unique signatures that are classified to **4** not scored targets. They belong to 3 unique drugs, and this signature belongs to drug `7cf4a548c`. There is no clear anomaly in this signature, and it is very clean.","accd9364":"### **11.5 Signatures with 2 Targets**\n\nThere are 1538 unique signatures that are classified to **2** scored targets. They belong to 139 unique drugs, and this signature belongs to drug `b877aa048`. There is no clear anomaly in this signature, and it is very clean.","00f5a0d5":"### **11.4 Signatures with 3 Targets**\n\nThere are 303 unique signatures that are classified to **3** scored targets. They belong to 21 unique drugs, and this signature belongs to drug `18bb41b2c`. There is no clear anomaly in this signature, but cell viability variance and standard deviation are lesser than gene expression variance and standard deviation.","0c0f1453":"There are 247 unique signatures that are classified to **2** not scored targets. They belong to 41 unique drugs, and this signature belongs to drug `746ca1f5a`. Cell viability\/gene expression variance and standard deviation mismatch exists in this signature as well, and the gap is huge.","cc2af65c":"## **7. Feature Selection**","73870fff":"### **6.1. Gene Expression Features**\n\nGene expression is the amount and type of proteins that are expressed in a cell at any given point in time. Gene expression level is based on a protocol similar to L1000 which is a high-throughput gene expression assay that measures the mRNA transcript abundance of 978 \"landmark\" genes from human cells. (The \"L\" in L1000 refers to the Landmark genes measured in the assay.)\n\nThere are 772 gene expression features and they have g- prefix (`g-0` to `g-771`). Each gene expression feature represents the expression of one particular gene, so there are 772 individual genes are being monitored in this assay.","1dca402d":"### **9.1. PCA**\n\nPCA is a linear transformation that projects the data into another space, where vectors of projections are defined by variance of the data. PCA results can be evaluated with reconstruction error and cumulative percent variance.\n\nIdeally latent variables should explain around 75% of the total variance. For cell viability features, CPV curve resembles to linear growth. First 10 components contain approximately 89% of the variance, while 50 components are needed to explain close to 100% of the variance. For gene expression features, CPV curve is closer to logarithmic growth. First 10 components contain approximately 50% of the variance, while 400 components are needed to describe close to 100% of the variance. Optimal dimensions can be found for cell viability and gene expressions by evaluating CPV curves.","01f4021e":"Dose counts are also consistent between different drugs just like they were between different targets. Same behavior is expected in test set because some level of stratification is applied while splitting train\/test sets.","0f7755ef":"Feature distributions are visualized from low to high variance. Features with variance between 0.3 and 0.4 can be removed since they contain the least information. For other ranges, feature removal should be done with trial and error based on model performances.","0786ae0f":"### **7.1. Adversarial Validation**\n\nCategorical features; `cp_type`, `cp_time` and `cp_dose` are omitted in adversarial validation. Only gene expression and cell viability features are used. A ROC AUC score between **0.5** and **0.55** is expected from adversarial validation model because gene expression and cell viability features are normalized before training and test sets are splitted.","bb1b9366":"### **11.2 Signatures with 5 Targets**\n\nThere are 13 unique signatures that are classified to **5** scored targets, and they are very similar groups which are mainly inhibitors. They belong to drugs `3f9dd627f`, `228f08c3d`, and `809527b9d`, and this signature belongs to drug `3f9dd627f`. Those 13 signatures are classified to 1 not scored target. In addition to that, there is nothing unusual about the sequence.","6d9ae88d":"### **5.2. Cell Viability and Target Interactions**\n\nCell viability features are plotted against only a random target feature since there are 206 targets. It can be seen that, there are positive relationships between cell viability features and target features in most of the cases. However, some of them have no relationship or negative relationship with target features. This could be related to, most of the cells are being cancer cells while some of them are not. Another pattern that can be seen in cell viability features is, positive target values are clustered around zero means in most of the cases.","9eae383a":"### **6.2. Gene Expression and Target Interactions**\n\nRandom gene expression features are plotted against a random target feature because it is not possible to visualize 772x206 feature interactions. Gene expression features and targets have weaker relationships compared to cell viability features, because data points of positive target values are more spread along the x axis.\n\nAll gene expression features have one thing in common; positive target values are clustered around zero means just like cell viability features. High absolute values in gene expression features (>2 or <-2) indicate that the drug or perturbation had a significant effect on the current cell, whereas values close to zero mean means that the effect for that cell was non-measurable.","babb2e2c":"### **3.3. Target Features Correlations**\n\nBoth scored target and non-scored target correlations are very close to 0, but there are very few bright red and light red dots that are worth exploring. Those red dots may yield relationships between targets. Horizontal and vertical white lines in non-scored target correlations occur due to NaNs.","1c3c1f61":"## **2. Objective and Metric**\n\nThis is a multi-label binary classification problem, and metric used for the evaluation is mean columnwise log loss. For every row, a probability that the sample had a positive response for each target, has to be predicted. For $N$ rows and $M$ targets, there will be $N\u00d7M$ predictions. Submissions are scored by the log loss:\n\n$\\Large \\text{log loss} = - \\frac{1}{M}\\sum_{m=1}^{M} \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_{i,m} \\log(\\hat{y}_{i,m}) + (1 - y_{i,m}) \\log(1 - \\hat{y}_{i,m})\\right]$\n\n* $N$ is the number of rows ($i=1,\u2026,N$)\n* $M$ is the number of targets ($m=1,\u2026,M$)\n* $\\large \\hat{y}_{i,m}$ is the predicted probability of the ith row and mth target\n* $\\large y_{i,m}$ is the ground truth of the ith row and mth target (1 for a positive response, 0 otherwise)\n* $log()$ is the natural logarithm\n\nSubmitted predicted probabilities are replaced with $max(min(p,1-10^{-15}),10^{-15})$. This means that probabilities greater than 0.999999999999999, and probabilities less than 0.000000000000001 are clipped. When all of those details are added to the equation, function defined below can be used both as a metric and loss function. It should be safer to optimize competition metric directly.","463903a5":"## **8. Feature Scaling**\n\nIt can seen from the prior visualizations that continuous features are between different ranges. Feature scaling can both improve model performance and speed up convergence at the same time. In order to observe scaling effects of different scalers, 25 cell viability and 25 gene expression features are randomly selected and scaled with `StandardScaler`, `MinMaxScaler`, and `GaussRankScaler`.","405467d3":"## **1. Introduction**","4dd26349":"### **11.7 Signatures with 0 Targets**\n\nThere are 9367 unique signatures that are classified to **0** scored targets. They belong to 1213 unique drugs, and this signature belongs to drug `df89a8e5a`. There is no clear anomaly in this signature, but cell viability variance and standard deviation are lesser than gene expression variance and standard deviation.","463e4fbb":"## **10. Drugs**\n\nTraining set drug ids are added to the competition dataset by today. `train_drug.csv` contains anonymous drug ids of every signature id in training set. There are **3289** unique drugs while there are **23814** unique signatures. This means some drugs are used more than 6 times (2 different doses x 3 different durations). This data can be useful for cross-validations and outlier detection."}}