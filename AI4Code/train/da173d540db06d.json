{"cell_type":{"e5e4aeb0":"code","b7f197c5":"code","ea1cf911":"code","b27d344d":"code","ff214242":"code","4eb452aa":"code","68699f60":"code","c3af4a09":"code","10c6795f":"code","ec7602ff":"code","4fb0b37d":"code","79e61343":"code","1cb94573":"code","560e9183":"code","db86c43c":"code","f79570b9":"markdown","dbfadf07":"markdown","c5283dba":"markdown","66a824c6":"markdown","3b5835b4":"markdown","3dcfbd31":"markdown","3dcf8cf0":"markdown","38215f14":"markdown","1e526f7d":"markdown","12bad95d":"markdown","a79201d3":"markdown","2606935b":"markdown","668fc160":"markdown","0aa55d11":"markdown","bfe10b4b":"markdown"},"source":{"e5e4aeb0":"pip install  mglearn","b7f197c5":"import mglearn\n\nmglearn.plots.plot_knn_classification(n_neighbors=1)","ea1cf911":"mglearn.plots.plot_knn_classification(n_neighbors=4)","b27d344d":"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nprint(\"cancer.keys(): \\n{}\".format(cancer.keys()))","ff214242":"#The dataset consists of 569 data points, with 30 features each:\nprint(\"Shape of cancer data: {}\".format(cancer.data.shape))","4eb452aa":"#Of these 569 data points, 212 are labeled as malignant and 357 as benign:\nprint(\"Sample counts per class:\\n{}\".format(\n{n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))","68699f60":"#To get a description of the semantic meaning of each feature, we can have a look at the feature_names attribute:\nprint(\"Feature names:\\n{}\".format(cancer.feature_names))","c3af4a09":"#Split Data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=66)\n\n#Choose a Model\nfrom sklearn.neighbors import KNeighborsClassifier","10c6795f":"#Let\u2019s investigate whether we can confirm the connection between model complexity and generalization.\ntraining_accuracy = []\ntest_accuracy = []\n# try n_neighbors from 1 to 10\nneighbors_settings = range(1, 11)","ec7602ff":"for n_neighbors in neighbors_settings:\n    # build the model\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    clf.fit(X_train, y_train)\n    # record training set accuracy\n    training_accuracy.append(clf.score(X_train, y_train))\n    # record generalization accuracy\n    test_accuracy.append(clf.score(X_test, y_test))\n    \nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()","4fb0b37d":"mglearn.plots.plot_animal_tree()","79e61343":"# Get Data\ncancer = load_breast_cancer()\n# Split Data\nX_train, X_test, y_train, y_test = train_test_split(\ncancer.data, cancer.target, stratify=cancer.target, random_state=42)\n\n# Choose a Model\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Build the Model\ntree = DecisionTreeClassifier(max_depth=4,random_state=0)\ntree.fit(X_train, y_train)\n\n#Evaluate the Model\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))","1cb94573":"from sklearn.tree import export_graphviz\nexport_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\nfeature_names=cancer.feature_names, impurity=False, filled=True)\n\nimport graphviz\nwith open(\"tree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)\n","560e9183":"print(\"Feature importances:\\n{}\".format(tree.feature_importances_))","db86c43c":"def plot_feature_importances_cancer(model):\n    n_features = cancer.data.shape[1]\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), cancer.feature_names)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\nplot_feature_importances_cancer(tree)","f79570b9":"## Building decision trees","dbfadf07":"## Feature importance in trees\n* Instead of looking at the whole tree, which can be taxing, there are some useful properties that we can derive to summarize the workings of the tree.\n* the most commonly used summary is *feature importance*,which rates how important each feature is for the decision a tree makes.\n* It is a number between 0 and 1 for each feature, where 0 means \u201cnot used at all\u201d and 1 means \u201cperfectly predicts the target.\u201d\n","c5283dba":"# 5- **Classification**\n\n* Supervised machine learning is one of the most commonly used and successful types of machine learning.\n* There are two major types of supervised machine learning problems, called classification and regression.\n* In **classification**, the goal is to predict a class label, which is a choice from a predefined list of possibilities.\n* For **regression** tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms).\n* We will explain two classification algorithms :\n    1. k-Nearest Neighbors.\n    1. Decision Trees\n\n# 1-k-Neighbors classification","66a824c6":"# 2-Decision Trees\n* Decision trees are widely used models for classification and regression tasks. Essentially, they learn a hierarchy of if\/else questions, leading to a decision.\n* These questions are similar to the questions you might ask in a game of 20 Questions.Imagine you want to distinguish between the following four animals: bears, hawks, penguins, and dolphins.","3b5835b4":"![breast_cancer_detail_large.jpg](attachment:832fa3d1-5fb4-4b5b-a1de-9d4f72998656.jpg)","3dcfbd31":"* We can visualize the tree using the export_graphviz function from the tree module.\n* Next Figure is a visualization of the decision tree built on the Breast Cancer dataset.\n","3dcf8cf0":"* Here we see that the feature used in the top split (\u201cworst radius\u201d) is by far the most important feature.","38215f14":"* If we don\u2019t restrict the depth of a decision tree, the tree can become arbitrarily deep and complex.\n* Here we set max_depth=4, meaning only four consecutive questions can be asked.\n* Limiting the depth of the tree decreases overfitting. This leads to a lower accuracy on the training set, but an improvement on the test set.","1e526f7d":"## Breast Cancer Classification\n* We will use the Breast Cancer dataset, which records clinical measurements of breast cancer tumors.\n* Each tumor is labeled as \u201cbenign\u201d (for harmless tumors) or \u201cmalignant\u201d (for cancerous tumors), and the task is to learn to predict whether a tumor is malignant based on the measurements of the tissue.","12bad95d":"### [**Back to course home**](https:\/\/www.kaggle.com\/ammarnassanalhajali\/machine-learning-in-python-course-1)","a79201d3":"## Analyzing decision trees","2606935b":"* The plot shows the training and test set accuracy on the y-axis against the setting of n_neighbors on the x-axis.\n* The test set accuracy for using a single neighbor is lower than when using more neighbors,indicating that using the single nearest neighbor leads to a model that is too complex. \n* On the other hand, when considering 10 neighbors, the model is too simple and performance is even worse. \n* The best performance is somewhere in the middle, using around six neighbors.","668fc160":"* We can visualize the feature importances.","0aa55d11":"* When considering more than one neighbor, we use voting to assign a label. \n* This means that for each test point, we count how many neighbors belong to class 0 and how many neighbors belong to class 1. \n* We then assign the class that is more frequent.See next Figure.","bfe10b4b":"* In its simplest version, the k-NN algorithm only considers exactly one nearest neighbor, which is the closest training data point to the point we want to make a prediction for.\n* The prediction is then simply the known output for this training point. See next Figure.\n* we added three new data points, shown as stars. For each of them, we marked the closest point in the training set. \n* The prediction of the one-nearest-neighbor algorithm is the label of that point."}}