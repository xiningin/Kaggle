{"cell_type":{"0339865b":"code","e1dc6690":"code","e58b93ca":"code","d4212d3a":"code","8689acd6":"code","6cd290b4":"code","11ff49b7":"code","1d453afe":"code","03735bd6":"code","18646706":"markdown","cd24cdee":"markdown","9e11cfb6":"markdown","9272909b":"markdown","e4a6d0cc":"markdown","0bbf6675":"markdown","2f95e7a9":"markdown","0ff68c3f":"markdown","6a457ad5":"markdown","5451d4b8":"markdown"},"source":{"0339865b":"# Used packages\n\nimport numpy as np\nimport scipy as sc\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_circles\n\nfrom IPython.display import display, Math, Latex\n\nimport time\nfrom IPython.display import clear_output\n\nfrom sklearn.metrics import accuracy_score","e1dc6690":"# Train set and test set are created by using make_circles\nn = 500\np = 2\n\nnp.random.seed(42)\nX, y = make_circles(n_samples=n, factor=0.5, noise=0.06)\ny = y[:,np.newaxis]\n\nX_test, y_test = make_circles(n_samples=n, factor=0.5, noise=0.06)\ny_test = y_test[:,np.newaxis]\n\nplt.scatter(X[y[:, 0] == 0, 0], X[y[:, 0] == 0, 1], c='skyblue')\nplt.scatter(X[y[:, 0] == 1, 0], X[y[:, 0] == 1, 1], c='salmon')\nplt.axis('equal')\nplt.show()","e58b93ca":"class neural_layer():\n    \n    def __init__(self, n_conn, n_neur, act_f):\n        \n        self.act_f = act_f\n        # Bias term\n        self.b = np.random.randn(1, n_neur) * 2 - 1 # To normalize it from -1 to 1\n        # Weights\n        self.w = np.random.randn(n_conn, n_neur) * 2 - 1","d4212d3a":"sigm = (lambda x: 1\/(1+ np.e**(-x)),\n        lambda x: x * (1 - x))\n\n\n_x = np.linspace(-5, 5, 100)\nplt.plot(_x, sigm[0](_x))\nplt.show()","8689acd6":"# A frunction to create the layers\n\ndef create_nn(topology, act_f):\n    \n    nn =  []\n    \n    for l, layer in enumerate(topology[:-1]):\n        \n        nn.append(neural_layer(topology[l], topology[l + 1], act_f=act_f))\n        \n    return nn","6cd290b4":"topology = [p, 4, 8, 4, 1]\nneural_net = create_nn(topology, sigm)\n\n# Mean square error\nl2_cost = (lambda Yp, Yr: np.mean((Yp - Yr)**2), \n           lambda Yp, Yr: (Yp - Yr))\n\ndef train(neural_net, X, y, l2_cost, learning_rate=0.5, train=True):\n    \n    out = [(None, X)]\n    \n    # Forward pass\n    for l, layer in enumerate(neural_net):\n        Z = out[-1][1] @ neural_net[l].w + neural_net[l].b\n        a = neural_net[l].act_f[0](Z)\n    \n        out.append((Z, a))\n\n    \n    if train:\n        \n        # Backward pass\n        deltas = []\n        \n        for l in reversed(range(0, len(neural_net))):\n            \n            Z = out[l + 1][0]\n            a = out[l + 1][1]\n            \n            #print(a.shape)\n            \n            if l == len(neural_net) - 1:\n                deltas.insert(0, l2_cost[1](a, y) * neural_net[l].act_f[1](a))\n                \n            else:      \n                deltas.insert(0, deltas[0] @ _W.T * neural_net[l].act_f[1](a))\n                \n            _W = neural_net[l].w\n            \n            # Gradient descent\n            neural_net[l].b = neural_net[l].b - np.mean(deltas[0], axis=0, keepdims=True) * learning_rate\n            neural_net[l].w = neural_net[l].w -  out[l][1].T @ deltas[0] * learning_rate\n    \n    return out[-1][1]","11ff49b7":"neural_n = create_nn(topology, sigm)\n\nloss = []\nepoch = []\n\nfor i in range(500):\n    \n    pY = train(neural_n, X, y, l2_cost, learning_rate=0.06)\n    \n    if i % 25 == 0:\n\n\n        loss.append(l2_cost[0](pY, y))\n        epoch.append(i)\n\n        res = 50\n\n        _x0 = np.linspace(-1.5, 1.5, res)\n        _x1 = np.linspace(-1.5, 1.5, res)\n\n        _Y = np.zeros((res, res))\n\n        for i0, x0 in enumerate(_x0):\n            for i1, x1 in enumerate(_x1):\n                _Y[i0, i1] = train(neural_n, np.array([[x0, x1]]), y, l2_cost, train=False)[0][0]    \n            \n        plt.pcolormesh(_x0, _x1, _Y, cmap=\"coolwarm\")\n        plt.axis(\"equal\")\n\n        plt.scatter(X[y[:,0] == 0, 0], X[y[:,0] == 0, 1], c=\"skyblue\")\n        plt.scatter(X[y[:,0] == 1, 0], X[y[:,0] == 1, 1], c=\"salmon\")\n\n        clear_output(wait=True)\n        plt.show()\n        plt.plot(epoch, loss)\n        plt.xlabel('Epoch')\n        plt.ylabel('mse')\n        plt.show()\n        time.sleep(0.5)  ","1d453afe":"logits = train(neural_n, X_test, y, l2_cost, train=False)\nlogits[logits >= 0.5] = 1\nlogits[logits < 0.5] = 0\n\naccuracy_score(logits, y_test) #100% of accuracy","03735bd6":"plt.scatter(X_test[logits[:, 0] == 0, 0], X_test[logits[:, 0] == 0, 1], c='skyblue')\nplt.scatter(X_test[logits[:, 0] == 1, 0], X_test[logits[:, 0] == 1, 1], c='salmon')\nplt.axis('equal')\nplt.show()","18646706":"## Creation of functions for the network","cd24cdee":"For the activation function (Simgoid), I create a lambda function that returns not only the value of the activation, but also the value of its derivative","9e11cfb6":"## Creating train and test set","9272909b":"Each instance of class neural_layer contains each layer of the neural network, as well as the activation function of that layer\n","e4a6d0cc":"This notebook explains how to create a neural network from batch for a task of binary classification, without deep learning packages such as tensorflow or pytorch.","0bbf6675":"## Testing the neural network","2f95e7a9":"# Neural Network from Batch","0ff68c3f":"## Training with real time visualization","6a457ad5":"## Visualization of the test set classification","5451d4b8":"I create the function train for training the neural network, making three steps:\n\n* Forward pass: calculation of the mse with actual weights\n* Backward pass: calculation of the gradients\n* Gradient descent"}}