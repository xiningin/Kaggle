{"cell_type":{"c8cbcec2":"code","903edf77":"code","9a6e73ad":"code","6ecfc206":"code","73b6f590":"code","8b72cf32":"code","e70e70b9":"code","971974a8":"code","b549b348":"code","b89e1a68":"code","f6f81793":"code","45466dce":"code","813fb57a":"code","9bb9761b":"code","8f44e5bf":"code","ccd44d3d":"code","45bf2067":"code","7dfa3061":"code","4c6e2dc4":"code","53a4b454":"code","1a4375a0":"code","0948c58f":"code","495826f8":"code","2653f37f":"code","4560b2fb":"code","eeedd7f7":"code","6d953214":"code","58afbbc1":"code","ff539ee9":"code","0a5e8f20":"code","e2a5cce8":"code","908b1985":"code","625a74c8":"code","87711578":"code","aca87cc3":"code","c6cabd4e":"code","3bbd5703":"code","2ec10e4b":"code","2764e2ae":"code","bddb37b7":"code","e2214645":"code","03f7078a":"code","e301b78f":"code","e842b705":"code","f0229651":"code","1cc78112":"code","74927fc2":"code","9d11b8ff":"code","b4a18a12":"code","5489a398":"code","68650c8b":"code","93c94fb8":"code","eea04154":"code","43d1a2a1":"code","820ab796":"code","18312963":"code","666580ac":"code","6676637d":"code","9a8ed653":"code","e14b3571":"code","4bf4a656":"code","da987c55":"code","e98f60df":"code","4db55294":"code","faeba0b6":"code","74e4a7a8":"code","4ed26989":"code","86bfe983":"code","2e59eb63":"code","5f78b823":"code","538bc7f1":"code","6ddf87cf":"code","fd5f4ac0":"code","d5f251a1":"code","3aa5413d":"code","3bf5adbe":"code","fec37215":"code","d95c6cb6":"code","9499b21e":"code","9a409115":"code","39b86ec4":"code","23eb8d72":"code","d7416098":"code","90180104":"markdown","add033dd":"markdown","8a786614":"markdown","ed4e7401":"markdown","6da48c04":"markdown","a475b0bc":"markdown","55ef8b8c":"markdown","ab4c925a":"markdown","152bf2c1":"markdown","cf414124":"markdown","28b8efe3":"markdown","ff2e6699":"markdown","1d3080fc":"markdown","1f9b3cb0":"markdown","ac23f75a":"markdown","4dfcc402":"markdown","f63b254b":"markdown","a6aea1a3":"markdown","fa4a850c":"markdown","b7a9a5f0":"markdown","9ff4b2b2":"markdown","c8290ed1":"markdown","654374c6":"markdown","17c8d1d7":"markdown","51c959e6":"markdown","91d6cac4":"markdown","8f6ebedb":"markdown","7b52ed6d":"markdown","ac1b619b":"markdown","f8dc431f":"markdown","c93d69b7":"markdown","a7f8a0b6":"markdown","c51566d5":"markdown","fe399c72":"markdown","c5e35d74":"markdown","b15f7294":"markdown","1aa27d47":"markdown","c822f418":"markdown"},"source":{"c8cbcec2":"from IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\")","903edf77":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a6e73ad":"data = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')\ndata.head()","6ecfc206":"data.info()","73b6f590":"data.describe()","8b72cf32":"%matplotlib inline\nimport matplotlib.pyplot as plt\ndata.hist(bins=50,figsize=(12,9))\nplt.show()\n\n","e70e70b9":"corr_matrix= data.corr()\ncorr_matrix['Response'].sort_values(ascending=False)","971974a8":"import seaborn as sns\nf, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(data.corr(),annot=True, linewidths=.5, ax=ax)\nplt.show()\n\n","b549b348":"import matplotlib.pyplot as plt\n\nlabels ='Not-responed', 'Responed'\nsizes = [len(data[data['Response']==0]),len(data[data['Response']==1])]\nexplode = (0, 0.04) \n\nfig1, ax1 = plt.subplots(figsize=(8,8))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, colors=('r','yellow'), startangle=90)\nax1.set_title(\"Response Events\")\nax1.axis('equal') \n\nplt.show()\n\n","b89e1a68":"cat_2=['Gender','Previously_Insured','Vehicle_Damage']\n\ntypes=[['Women','Men'],['No','Yes'],['No','Yes']]\nfor i,c in enumerate(cat_2):\n    alive = data[data['Response']==0]\n    died= data[data['Response']==1]\n    plt.figure(figsize=(8,5))\n    bar1=plt.bar(np.arange(len(data[c].unique())), alive.groupby(c).count()['Age'], width=0.1, color='orange', align='center', label=\"Not responed\")\n    bar2= plt.bar(np.arange(len(data[c].unique()))+0.1, died.groupby(c).count()['Age'], width=0.1, color='green', align='center', label=\"responed\")\n    plt.title(c)\n    #plt.ylim(0,160)\n    plt.xticks([0,1], types[i])\n    plt.grid()\n    plt.legend()\n\n    hights_odd=[]\n    hights_even=[]\n    for i,rect in enumerate (bar1 + bar2):\n        height = rect.get_height()\n        if (i+1)%2==0:\n            hights_even.append(height)\n        if (i+1)%2!=0:\n            hights_odd.append(height)\n\n    for i,rect in enumerate (bar1 + bar2):\n        height = rect.get_height()\n        if (i+1)%2==0:\n            plt.text(rect.get_x() + rect.get_width()\/2.0, height, '%s' % str(round((height\/sum(hights_even)*100),2))+\"%\", ha='center', va='bottom')\n        if (i+1)%2!=0:\n            plt.text(rect.get_x() + rect.get_width()\/2.0, height, '%s' % str(round((height\/sum(hights_odd))*100,2))+\"%\", ha='center', va='bottom')","f6f81793":"plt.figure(figsize=(10,7))\nplt.title('Region Code')\nplt.grid()\nmaxx=0\nhigh=[]\nxs=[]\nfor i in sorted(data['Region_Code'].unique()):\n    bar= plt.bar(i,len(data[data['Region_Code']==i]))    \n","45466dce":"import matplotlib.pyplot as plt\n\nlabels ='region 28', 'all but not 28'\nsizes = [len(data[data['Region_Code']==28]),len(data[data['Region_Code']!=28])]\nexplode = (0, 0.04) \n\nfig1, ax1 = plt.subplots(figsize=(8,8))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, colors=('b','c'), startangle=90)\nax1.set_title(\"Response Events\")\nax1.axis('equal') \n\nplt.show()","813fb57a":"data['Vehicle_Age'].unique()","9bb9761b":"time_1= data[data['Vehicle_Age']=='< 1 Year']\ntime_2= data[data['Vehicle_Age']=='1-2 Year']\ntime_3= data[data['Vehicle_Age']=='> 2 Years']\n\nexplode = (0, 0.05)\nlabels = 'Not responsive', 'Responsive'\n\ntypes= [time_1,time_2,time_3,]\nfig, (ax1, ax2,ax3) = plt.subplots(1, 3,figsize=(13,7))\nax= (ax1, ax2,ax3)\nfig.suptitle('Vehicle Age virsus Reponses',fontsize=20)\ntitles= ['Less than a year', '1-2 years', 'More than two years']\n\n\n\nfor ax, typ,title in zip(ax,types,titles ):\n    \n    sizes = [len(typ[typ['Response']==0]),len(typ[typ['Response']==1])]\n    wedges, texts,autopct = ax.pie(sizes, autopct='%1.1f%%', explode=explode,colors=['r','y'], labels=labels)\n    ax.set_title(title)\n    \n    ax.axis('equal') \nplt.show()","8f44e5bf":"plt.figure(figsize=(10,7))\nplt.title('Policy Sales Channel')\nplt.grid()\nmaxx=0\nfor i in sorted(data['Policy_Sales_Channel'].unique()):\n    bar= plt.bar(i,len(data[data['Policy_Sales_Channel']==i]))\n","ccd44d3d":"import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ntypes= [data[data['Response']==0]['Age'], data[data['Response']==1]['Age']]\ntitles= [ 'Age Distribution for People Who Doesnt Responded with ', 'Age Distribution for People Who Responded with ']\ncolors=['r','blue']\n#age= data['Age']\n\nfor age, tit,color in zip(types, titles,colors):\n    mu, std = norm.fit(age)\n    plt.figure(figsize=(12,7))\n    plt.hist(age, bins=25, density=True, alpha=0.6, color=color)\n\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2)\n    tit +=\"mu = %.2f,  std = %.2f\" % (mu, std)\n    plt.title(tit)\n    plt.grid()\n    plt.show()                \n","45bf2067":"plt.figure(figsize=(10,8))\nplt.xticks([1,2], ['Responeded', 'Not'])\nplt.boxplot(types)\nplt.title('Boxplot for Age cat')\nplt.grid()\nplt.show()\n","7dfa3061":"v_1= data[data['Vintage']<100]\nv_2= data[data['Vintage']>100][data['Vintage']<200]\nv_3= data[data['Vintage']>200]","4c6e2dc4":"\nexplode = (0, 0.05)\nlabels = 'Not responsive', 'Responsive'\n\ntypes= [v_1,v_2,v_3]\nfig, (ax1, ax2,ax3) = plt.subplots(1, 3,figsize=(13,7))\nax= (ax1, ax2,ax3)\nfig.suptitle('Vehicle Age vs Reponses',fontsize=20)\ntitles= ['Less than a year', '1-2 years', 'More than two years']\n\n\n\nfor ax, typ,title in zip(ax,types,titles ):\n    \n    sizes = [len(typ[typ['Response']==0]),len(typ[typ['Response']==1])]\n    wedges, texts,autopct = ax.pie(sizes, autopct='%1.1f%%', explode=explode,colors=['r','y'], labels=labels)\n    ax.set_title(title)\n    \n    ax.axis('equal') \nplt.show()","53a4b454":"plt.figure(figsize=(8,6))\nplt.bar([0,1,2], [len(v_1),len(v_2),len(v_3)], color='g')\nplt.xticks([0,1,2], ['Group1','Group2', 'Group3'])\nplt.title(\"Vintage Groups Data Distribution\")\nplt.grid()","1a4375a0":"min(data['Annual_Premium'])\nmax(data['Annual_Premium'])\n#Convert to US Dollar\ndata['Annual_Premium_$']= data['Annual_Premium']*0.014","0948c58f":"print(min(data['Annual_Premium_$']))\nprint(max(data['Annual_Premium_$']))\nprint(np.median(data['Annual_Premium_$']))","495826f8":"import numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\nap= data['Annual_Premium_$']\n#age= data['Age']\n\nmu, std = norm.fit(age)\nplt.figure(figsize=(10,7))\nplt.hist(ap, bins=25, density=True, alpha=0.6, color='r')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, std)\ntit =\"mu = %.2f,  std = %.2f\" % (mu, std)\nplt.title(\"Annual Premium--\"+ tit)\nplt.grid()\nplt.show() ","2653f37f":"a_1= data[data['Annual_Premium_$']<=442]\na_2= data[data['Annual_Premium_$']>442]\n\nexplode = (0, 0.05)\nlabels = 'Not responsive', 'Responsive'\n\ntypes= [a_1,a_2]\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12,7))\nax= (ax1, ax2)\nfig.suptitle('Annual Premium ',fontsize=20)\ntitles= ['Less than mean value', 'More than mean value']\n\n\n\nfor ax, typ,title in zip(ax,types,titles ):\n    \n    sizes = [len(typ[typ['Response']==0]),len(typ[typ['Response']==1])]\n    wedges, texts,autopct = ax.pie(sizes, autopct='%1.1f%%', explode=explode,colors=['r','y'], labels=labels)\n    ax.set_title(title)\n    \n    ax.axis('equal') \nplt.show()","4560b2fb":"plt.figure(figsize=(8,6))\nplt.bar([0,1], [len(a_1),len(a_2)], color='g')\nplt.xticks([0,1], ['Group1','Group2'])\nplt.title(\"Vintage Groups Data Distribution\")\nplt.grid()","eeedd7f7":"data['Gender']=data['Gender'].astype('category').cat.codes\ndata['Vehicle_Age']= [0 if data['Vehicle_Age'][i]=='< 1 Year' else 1 if data['Vehicle_Age'][i]=='1-2 Year' else 2 for i in range(len(data['Vehicle_Age']))]\n#data['Vehicle_Age'] = data['Gender'].astype('category')\ndata['Vehicle_Damage']=data['Vehicle_Damage'].astype('category').cat.codes\ndata['Region_Code']= data['Region_Code'].astype(int)\n#data['Policy_Sales_Channel']= data['Policy_Sales_Channel'].astype('category')","6d953214":"data.columns","58afbbc1":"features=[ 'Gender', 'Age','Region_Code',\n       'Previously_Insured', 'Vehicle_Age','Vehicle_Damage', 'Annual_Premium',\n       'Policy_Sales_Channel', 'Vintage', 'Response']\n\nnum=[ 'Age','Annual_Premium','Vintage']\n\ntrain_prep=data[features]","ff539ee9":"from category_encoders import TargetEncoder\n\nencoder = TargetEncoder()\ntrain_prep['Region_Code'] = encoder.fit_transform(train_prep['Region_Code'], train_prep['Response'])\ntrain_prep['Policy_Sales_Channel'] = encoder.fit_transform(train_prep['Policy_Sales_Channel'], train_prep['Response'])\n","0a5e8f20":"#train_prep['Region_Code'] = train_prep['Region_Code'].astype('category',copy=False)\n#train_prep= pd.get_dummies(train_prep)\n\nfrom sklearn.preprocessing import StandardScaler\nstd=StandardScaler()\ntrain_prep[num]= std.fit_transform(train_prep[num])\n","e2a5cce8":"train_prep","908b1985":"from sklearn.model_selection import StratifiedShuffleSplit \nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, valid_index in split.split(train_prep, train_prep[\"Response\"]):\n    train = train_prep.loc[train_index]\n    valid = train_prep.loc[valid_index]","625a74c8":"from imblearn.over_sampling import SMOTE\n\n# Resample the minority class. You can change the strategy to 'auto' if you are not sure.\nsm = SMOTE(sampling_strategy='minority', random_state=7)\n\n# Fit the model to generate the data.\noversampled_trainX, oversampled_trainY = sm.fit_sample(train.drop('Response', axis=1), train['Response'])\noversampled_train = pd.concat([oversampled_trainX, oversampled_trainY], axis=1)","87711578":"oversampled_train","aca87cc3":"y_train= oversampled_train['Response']\ny_valid= valid['Response']\n\nX_train= oversampled_train.drop('Response', axis=1)\nX_valid= valid.drop('Response', axis=1)\n\nX_train.index = np.arange(len(X_train))\nX_valid.index = np.arange(len(X_valid))\n\ny_train.index = np.arange(len(y_train))\ny_valid.index = np.arange(len(y_valid))","c6cabd4e":"from sklearn.tree import DecisionTreeClassifier\ntree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree_clf.fit(X_train, y_train)\n\ntree_preds = tree_clf.predict(X_valid)","3bbd5703":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import roc_auc_score\nprint(\"Acc:\",accuracy_score(y_valid, tree_preds))\n\nprint(\"Precision:\",precision_score(y_valid, tree_preds))\n\nprint(\"Recall:\",recall_score(y_valid, tree_preds))\n\nprint('f1-score', f1_score(y_valid, tree_preds))\n\nprint('ROC score', roc_auc_score(y_valid, tree_preds))","2ec10e4b":"import os\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"decision_trees\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n    \nfrom graphviz import Source\nfrom sklearn.tree import export_graphviz\n\nexport_graphviz(\n        tree_clf,\n        out_file=os.path.join(IMAGES_PATH, \"iris_tree.dot\"),\n        feature_names=X_train.columns,\n        class_names=['not resp', 'resp'],\n        rounded=True,\n        filled=True\n    )\n\nSource.from_file(os.path.join(IMAGES_PATH, \"iris_tree.dot\"))","2764e2ae":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nkf = KFold(n_splits=2)\n\nmax_features = ['auto', 'sqrt','log2', None]\n\nrf_Model = RandomForestClassifier()\n\nrf_Grid = GridSearchCV(estimator = rf_Model, param_grid = {'max_features':max_features}, cv = kf,  scoring='accuracy',n_jobs=-1, verbose=4)\n\nrf_Grid.fit(X_train, y_train)","bddb37b7":"grid_results = pd.concat([pd.DataFrame(rf_Grid.cv_results_[\"params\"]),pd.DataFrame(rf_Grid.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\ngrid_results","e2214645":"rf_Grid.best_params_","03f7078a":"scores_test = rf_Grid.cv_results_['mean_test_score']\n#scores = np.array(scores).reshape(len(Cs), len(n_estimators))\nplt.figure(figsize=(10,6))\nplt.plot([0,1,2,3], scores_test, label=\"Testing Error\")\nplt.xticks([0,1,2,3], ['auto', 'sqrt', 'log2', 'None'])\nplt.legend()\nplt.xlabel('n_estimators')\nplt.ylabel('Mean score')\nplt.grid()\nplt.show()","e301b78f":"min_samples_leaf=[1,2,4, 6]\n    \nrf_leaf_Model = RandomForestClassifier()\n\nrf_leaf_Grid = GridSearchCV(estimator = rf_leaf_Model, param_grid = {'min_samples_leaf':min_samples_leaf}, cv = kf, verbose=5, n_jobs = -1)\n\nrf_leaf_Grid.fit(X_train, y_train)\n","e842b705":"grid_leaf_results = pd.concat([pd.DataFrame(rf_leaf_Grid.cv_results_[\"params\"]),pd.DataFrame(rf_leaf_Grid.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\ngrid_leaf_results","f0229651":"rf_leaf_Grid.best_params_","1cc78112":"max_depth = [None,2,4,6]\n\nrf_depth_Model = RandomForestClassifier()\n\nrf_dep_Grid = GridSearchCV(estimator = rf_depth_Model, param_grid = {'max_depth':max_depth}, cv = kf, verbose=2, n_jobs = -1)\n\nrf_dep_Grid.fit(X_train, y_train)\n","74927fc2":"grid_results = pd.concat([pd.DataFrame(rf_dep_Grid.cv_results_[\"params\"]),pd.DataFrame(rf_dep_Grid.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\ngrid_results.head()","9d11b8ff":"rf_dep_Grid.best_params_","b4a18a12":"min_samples_split = [2,5,7]\n\nrf_mss_Model = RandomForestClassifier()\n\nrf_mss_Grid = GridSearchCV(estimator = rf_mss_Model, param_grid = {'min_samples_split':min_samples_split}, cv = kf, verbose=2, n_jobs = -1)\n\nrf_mss_Grid.fit(X_train, y_train)\n","5489a398":"grid_results = pd.concat([pd.DataFrame(rf_mss_Grid.cv_results_[\"params\"]),pd.DataFrame(rf_mss_Grid.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\ngrid_results.head()","68650c8b":"rf_mss_Grid.best_params_","93c94fb8":"rnd_clf = RandomForestClassifier( max_features=None,max_depth= None,\n                                 min_samples_leaf=1,min_samples_split=2, random_state=42)\n\nrnd_clf.fit(X_train, y_train)","eea04154":"rnf_preds= rnd_clf.predict(X_valid)\nprint(\"Acc:\",accuracy_score(y_valid, rnf_preds))\n\nprint(\"Precision:\",precision_score(y_valid, rnf_preds))\n\nprint(\"Recall:\",recall_score(y_valid, rnf_preds))\n\nprint('f1-score', f1_score(y_valid, rnf_preds))\n\nprint('ROC score', roc_auc_score(y_valid, rnf_preds))","43d1a2a1":"from sklearn.metrics import roc_curve,auc\ny_score = rnd_clf.predict_proba(X_valid)[:,1]\nfpr, tpr, _ = roc_curve(y_valid,y_score)\nimport matplotlib.pyplot as plt\n\nplt.title('Random Forest ROC curve')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr)\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', auc(fpr,tpr))","820ab796":"import pickle\nfilename = 'rf_clf.sav'\npickle.dump(rnd_clf, open(filename, 'wb'))\n\nfilename = 'rf_clf.sav'\nrf_load = pickle.load(open(filename, 'rb'))","18312963":"from sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier()\nlr= [0.01, 0.1,0.5, 1.0]\nsearch_grid={'learning_rate':lr}\nsearch=GridSearchCV(estimator=ada,param_grid=search_grid,scoring='accuracy',n_jobs=1,cv=kf,verbose=2)","666580ac":"search.fit(X_train, y_train)","6676637d":"grid_results = pd.concat([pd.DataFrame(search.cv_results_[\"params\"]),\n                          pd.DataFrame(search.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\ngrid_contour = grid_results.groupby(['learning_rate']).mean()\ngrid_contour","9a8ed653":"print(search.best_score_)\nprint(search.best_params_)","e14b3571":"search.best_params_","4bf4a656":"from sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1), n_estimators=300, learning_rate=1, random_state=42)\nada_clf.fit(X_train, y_train)","da987c55":"ada_pred= ada_clf.predict(X_valid)\nprint(\"Acc:\",accuracy_score(y_valid, ada_pred))\n\nprint(\"Precision:\",precision_score(y_valid, ada_pred))\n\nprint(\"Recall:\",recall_score(y_valid, ada_pred))\n\nprint('f1-score', f1_score(y_valid, ada_pred))\n\nprint('ROC score', roc_auc_score(y_valid, ada_pred))","e98f60df":"from sklearn.metrics import roc_curve,auc\ny_score = ada_clf.predict_proba(X_valid)[:,1]\nfpr, tpr, _ = roc_curve(y_valid,y_score)\nimport matplotlib.pyplot as plt\n\nplt.title('AdaBoost ROC curve')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr)\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', auc(fpr,tpr))","4db55294":"filename = 'ada_clf.sav'\npickle.dump(ada_clf, open(filename, 'wb'))\n\nfilename = 'ada_clf.sav'\nrf_load = pickle.load(open(filename, 'rb'))","faeba0b6":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(random_state = 42)\ngb.fit(X_train, y_train)","74e4a7a8":"gbrt_pred= gb.predict(X_valid)\nprint(\"Acc:\",accuracy_score(y_valid, gbrt_pred))\n\nprint(\"Precision:\",precision_score(y_valid, gbrt_pred))\n\nprint(\"Recall:\",recall_score(y_valid, gbrt_pred))\n\nprint('f1-score', f1_score(y_valid, gbrt_pred))\n\nprint('ROC score', roc_auc_score(y_valid, gbrt_pred))","4ed26989":"y_score = gb.predict_proba(X_valid)[:,1]\nfpr, tpr, _ = roc_curve(y_valid,y_score)\n\nplt.title('Gadient Boosting ROC curve')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr)\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', auc(fpr,tpr))","86bfe983":"filename = 'gb_clf.sav'\npickle.dump(gb, open(filename, 'wb'))\n","2e59eb63":"from xgboost import XGBClassifier\n\nparam_grid = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        }\nxgboost_model = XGBClassifier()\n\nxgboost_search= GridSearchCV(estimator = xgboost_model, param_grid = param_grid, cv = 2, verbose=10, n_jobs = -1)\n\nxgboost_search.fit(X_train, y_train)\n","5f78b823":"grid_results = pd.concat([pd.DataFrame(xgboost_search.cv_results_[\"params\"]),\n                          pd.DataFrame(xgboost_search.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\ngrid_contour = grid_results.groupby(['gamma','min_child_weight']).mean()\ngrid_contour","538bc7f1":"xgboost_search.best_params_","6ddf87cf":"xgboost_clf = XGBClassifier(gamma= 2, min_child_weight=1)\nxgboost_clf.fit(X_train, y_train)","fd5f4ac0":"xgb_pred= xgboost_clf.predict(X_valid)\nprint(\"Acc:\",accuracy_score(y_valid, xgb_pred))\n\nprint(\"Precision:\",precision_score(y_valid, xgb_pred))\n\nprint(\"Recall:\",recall_score(y_valid, xgb_pred))\n\nprint('f1-score', f1_score(y_valid, xgb_pred))\n\nprint('ROC score', roc_auc_score(y_valid, xgb_pred))","d5f251a1":"y_score = xgboost_clf.predict_proba(X_valid)[:,1]\nfpr, tpr, _ = roc_curve(y_valid,y_score)\n\nplt.title('XGBoost ROC curve')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr)\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', auc(fpr,tpr))","3aa5413d":"filename = 'xgb_clf.sav'\npickle.dump(xgb_pred, open(filename, 'wb'))\n","3bf5adbe":"\nproba_tree,proba_rnd, proba_ada, proba_gb, proba_xg = tree_clf.predict_proba(X_valid)[:,1], rnd_clf.predict_proba(X_valid)[:,1],ada_clf.predict_proba(X_valid)[:,1],gb.predict_proba(X_valid)[:,1],xgboost_clf.predict_proba(X_valid)[:,1]\n\npreds= [proba_tree,proba_rnd, proba_ada, proba_gb, proba_xg]\n\nlabels= ['DT','Random Forest', 'AdaBoost',\"Gradient Boosting\",'XGBoost']\nplt.figure(figsize=(10,8))\n\nfor pred, label in zip(preds,labels):\n    fpr, tpr, thresholds = roc_curve(y_valid, pred)\n    plt.plot(fpr, tpr, linewidth=2, label=label)\nplt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\nplt.axis([0, 1, 0, 1])                                    \nplt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) \nplt.ylabel('True Positive Rate (Recall)', fontsize=16)    \nplt.grid(True)  \nplt.legend()","fec37215":"result = []\n\nresults = pd.DataFrame(columns= [\"Models\",\"Accuracy\"])\n\nfor model in [tree_clf, rnd_clf, ada_clf, gb, xgboost_clf]:\n    names = model.__class__.__name__\n    y_pred = model.predict(X_valid)\n    accuracy = accuracy_score(y_valid, y_pred)    \n    result = pd.DataFrame([[names, accuracy*100]], columns= [\"Models\",\"Accuracy\"])\n    results = results.append(result)\n    \n    \nsns.barplot(x= 'Accuracy', y = 'Models', data=results, color=\"r\")\nplt.xlabel('Accuracy %')\nplt.title('Accuracy Ratios of Models'); \n","d95c6cb6":"test = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv')\ntest.head()","9499b21e":"test['Gender']=test['Gender'].astype('category').cat.codes\ntest['Vehicle_Age']= [0 if test['Vehicle_Age'][i]=='< 1 Year' else 1 if test['Vehicle_Age'][i]=='1-2 Year' else 2 for i in range(len(test['Vehicle_Age']))]\n#data['Vehicle_Age'] = data['Gender'].astype('category')\ntest['Vehicle_Damage']=test['Vehicle_Damage'].astype('category').cat.codes\ntest['Region_Code']= test['Region_Code'].astype(int)","9a409115":"features=[ 'Gender', 'Age','Region_Code',\n       'Previously_Insured', 'Vehicle_Age','Vehicle_Damage', 'Annual_Premium',\n       'Policy_Sales_Channel', 'Vintage']\n\nnum=[ 'Age','Annual_Premium','Vintage']\n\ntest_prep=test[features]\n\nstd=StandardScaler()\ntest_prep[num]= std.fit_transform(test_prep[num])\ntest_prep.head()\n","39b86ec4":"models_preds={}\n\nmodels= [tree_clf, rnd_clf, ada_clf, gb, xgboost_clf]\n\nfor model in models:\n    name = model.__class__.__name__\n    \n    models_preds[name]= model.predict(test_prep)","23eb8d72":"models_preds.keys()","d7416098":"sub_rnd= pd.concat([pd.DataFrame(test['id']), pd.DataFrame(models_preds['RandomForestClassifier'])] ,axis=1)\nsub_rnd.columns=['id', 'Response']\nsub_rnd.to_csv(r'sub_rnd.csv')\n\nsub_ada= pd.concat([pd.DataFrame(test['id']), pd.DataFrame(models_preds['AdaBoostClassifier'])] ,axis=1)\nsub_ada.columns=['id', 'Response']\nsub_ada.to_csv(r'sub_ada.csv')\n\nsub_gb= pd.concat([pd.DataFrame(test['id']), pd.DataFrame(models_preds['GradientBoostingClassifier'])] ,axis=1)\nsub_gb.columns=['id', 'Response']\nsub_gb.to_csv(r'sub_gb.csv')\n\nsub_xgb= pd.concat([pd.DataFrame(test['id']), pd.DataFrame(models_preds['XGBClassifier'])] ,axis=1)\nsub_xgb.columns=['id', 'Response']\nsub_xgb.to_csv(r'sub_xgb.csv')","90180104":"## Balancing The Trainig Set (Oversampling)","add033dd":"## XGBoost","8a786614":"### Train Using The Best Hyperparameters","ed4e7401":"### Annual Premium","6da48c04":"### Hyperparameter Tuning","a475b0bc":"### Two-Categories Data (Gender, Previously_Insured, Vehicle_Damage)","55ef8b8c":"### Train Using The Best Hyperparameters","ab4c925a":"## Data Cleaning","152bf2c1":"### Vintage","cf414124":"![](https:\/\/www.researchgate.net\/profile\/Zhuo_Wang8\/publication\/288699540\/figure\/fig9\/AS:668373486686246@1536364065786\/Illustration-of-AdaBoost-algorithm-for-creating-a-strong-classifier-based-on-multiple.png)","28b8efe3":"### Region Code","ff2e6699":"# Compare Models\n","1d3080fc":"## Categorical Data","1f9b3cb0":"### Hyperparameter Tuning: max_features","ac23f75a":"# Content:\n\n1. Data Exploration\n2. Data Analysis\n    * Categorical Data\n    * Numerical Data\n2. Preprocessing\n    * Data Cleaning\n    * Split to Training and Validation\n    * Balancing The Training Set (Oversampling)\n4. Ensemble Learning Models\n    * Descision Tree (The Base Estimator)\n    * Reinforcement Learning\n    * Adaboost \n    * Gradient Boosting\n    * XGBoost","4dfcc402":"### Hyperparameters Tuning","f63b254b":"![](https:\/\/www.harvestassure.com\/wp-content\/uploads\/2018\/11\/Vehicle-Insurance-Banner-1400x411.jpg)\n","a6aea1a3":"### Policy Sales Channel","fa4a850c":"# Get The Test Data Predictions","b7a9a5f0":"# Data Exploration","9ff4b2b2":"![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/02\/rfc_vs_dt1.png)","c8290ed1":"### Hyperparameter Tuning: min_samples_leaf","654374c6":"## The Base Estimator: Decision Tree","17c8d1d7":"## Training Using Best Hyperparameters","51c959e6":"## Gradient Boosting","91d6cac4":"### Hyperparameter Tuning: min_samples_split","8f6ebedb":"# Modeling","7b52ed6d":"# Preprocessing","ac1b619b":"## Random Forest","f8dc431f":"Seems like a huge percentage of people are in region 28:","c93d69b7":"### Hyperparameter Tuning: max_depth","a7f8a0b6":"## Split to Traing and Validation Sets","c51566d5":"![](https:\/\/s3.amazonaws.com\/assets.datacamp.com\/production\/course_7714\/datasets\/Gradient_Boosting3.png)","fe399c72":"# Data Analysis","c5e35d74":"### Vehicle Age","b15f7294":"## AdaBoost","1aa27d47":"## Numerical Data","c822f418":"### Age"}}