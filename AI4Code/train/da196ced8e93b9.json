{"cell_type":{"acfcdc25":"code","49a27f83":"code","794ad224":"code","3ca19f30":"code","bffd1253":"markdown"},"source":{"acfcdc25":"import numpy as np, pandas as pd, os\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm_notebook as tqdm\nnp.random.seed(42)\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","49a27f83":"# the following fast shuffling is from https:\/\/stackoverflow.com\/questions\/50554272\/randomly-shuffle-items-in-each-row-of-numpy-array dicussed in this rich kernel: https:\/\/www.kaggle.com\/jiweiliu\/fast-inplace-shuffle-for-augmentation\ndef disarrange(a, axis=-1):\n    \"\"\"\n    Shuffle `a` in-place along the given axis.\n\n    Apply numpy.random.shuffle to the given axis of `a`.\n    Each one-dimensional slice is shuffled independently.\n    \"\"\"\n    b = a.swapaxes(axis, -1)\n    # Shuffle `b` in-place along the last axis.  `b` is a view of `a`,\n    # so `a` is shuffled in place, too.\n    shp = b.shape[:-1]\n    for ndx in np.ndindex(shp):\n        np.random.shuffle(b[ndx])\n    return\n\ndef augment_fast2(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        disarrange(x1,axis=0)\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        disarrange(x1,axis=0)\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y\n    ","794ad224":"# INITIALIZE VARIABLES\ncols = [c for c in train.columns if c not in ['id', 'target']]\ncols.remove('wheezy-copper-turtle-magic')\ninteractions = np.zeros((512,255))\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor i in tqdm(range(512)):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    test2.reset_index(drop=True,inplace=True)\n    \n    skf = StratifiedKFold(n_splits=25, random_state=42)\n    for train_index, test_index in skf.split(train2.iloc[:,1:-1], train2['target']):\n        # LOGISTIC REGRESSION MODEL\n        clf = LogisticRegression(solver='liblinear',penalty='l1',C=0.05)\n        X_t, y_t = augment_fast2(train2.loc[train_index][cols].values, train2.loc[train_index]['target'].values)\n        clf.fit(X_t, y_t)\n        oof[idx1[test_index]] = clf.predict_proba(train2.loc[test_index][cols])[:,1]\n        preds[idx2] += clf.predict_proba(test2[cols])[:,1] \/ 25.0\n        # RECORD INTERACTIONS\n        for j in range(255):\n            if clf.coef_[0][j]>0: interactions[i,j] = 1\n            elif clf.coef_[0][j]<0: interactions[i,j] = -1\n    #if i%25==0: print(i)\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('LR with interactions scores CV =',round(auc,5))","3ca19f30":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('submission.csv',index=False)","bffd1253":"# Does augmentation work? \nwe have so many features that are uncorrelated and independent of each other (after filtering by `wheezy-copper-turtle-magic` feature). This kernel is a fork of [Chris' kernel](https:\/\/www.kaggle.com\/cdeotte\/logistic-regression-0-800) with the only exeption that it has out-of-fold data augmentation in it.\n\n- Chris' kernel results: CV 0.805 and LB 0.808\n- This kernel results: CV 0.786 and LB 0.??? (we will see in two hours from now)"}}