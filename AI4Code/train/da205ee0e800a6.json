{"cell_type":{"123f5e12":"code","62fca408":"code","c5e6c919":"code","dd08b214":"code","c04e4c15":"code","bc43fcf0":"code","42e8beb4":"code","7c122f23":"code","12a87330":"code","edd0cb8e":"code","9b959e8f":"code","2a3af952":"code","6212aa38":"code","7921ed82":"code","362952f5":"code","a4777d1e":"code","bee7a81d":"code","9ebdf4a1":"code","245f6ad3":"code","5c486cf7":"code","e62007c9":"code","1a6758d1":"code","adeb490b":"code","4b6444bc":"code","cf0d6f1d":"code","cf0f4d97":"code","54f71f41":"code","c2513e83":"code","a34c35cd":"code","348f56e5":"code","a286d784":"code","d0c97517":"markdown","116b1910":"markdown","b6772d4b":"markdown","e2bd9edc":"markdown","ceced6a4":"markdown","fda21f13":"markdown","758d13c3":"markdown","709e6304":"markdown","f52a6611":"markdown","43f44345":"markdown","85c79cce":"markdown","5850fe25":"markdown","b531f9bd":"markdown","3d4897f8":"markdown","2e120fbc":"markdown","cdd5dff0":"markdown"},"source":{"123f5e12":"import numpy as np \nimport pandas as pd","62fca408":"data_true = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")\ndata_false = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")","c5e6c919":"data_true[\"label\"]= 0\ndata_false[\"label\"]= 1","dd08b214":"data_true.head()","c04e4c15":"data_false.head()","bc43fcf0":"print(data_true.shape)\nprint(data_false.shape)","42e8beb4":"data_true.info()","7c122f23":"data_false.info()","12a87330":"data = pd.concat([data_true, data_false])\ndata.shape","edd0cb8e":"from sklearn.utils import shuffle\ndata = shuffle(data)\ndata = data.reset_index(drop=True)","9b959e8f":"data.head(10)","2a3af952":"y = data[\"label\"]\ny.shape","6212aa38":"from sklearn.feature_extraction.text import CountVectorizer","7921ed82":"copied_data = data.copy()","362952f5":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []\nfor i in range(0, len(copied_data)):\n    review = re.sub('[^a-zA-Z]', ' ', copied_data['title'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","a4777d1e":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=5000, ngram_range=(1,3))\nX = cv.fit_transform(corpus).toarray()","bee7a81d":"X.shape","9ebdf4a1":"y = copied_data['label']","245f6ad3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=2)","5c486cf7":"cv.get_feature_names()[:10]","e62007c9":"cv.get_params()","1a6758d1":"cout_df = pd.DataFrame(X_train, columns=cv.get_feature_names())","adeb490b":"cout_df.head()","4b6444bc":"import matplotlib.pyplot as plt","cf0d6f1d":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    See full source and example: \n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","cf0f4d97":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nimport itertools\nclassifier = MultinomialNB()","54f71f41":"classifier.fit(X_train, y_train)\npred = classifier.predict(X_test)\nscore = metrics.accuracy_score(y_test, pred)\nprint(\"accuracy: %0.3f\" % score)\ncm = metrics.confusion_matrix(y_test, pred)\nplot_confusion_matrix(cm, classes=['Fake', 'Real'])","c2513e83":"classifier.fit(X_train, y_train)\npred = classifier.predict(X_test)\nscore = metrics.accuracy_score(y_test, pred)\nscore","a34c35cd":"from sklearn.linear_model import PassiveAggressiveClassifier\nlinear_clf = PassiveAggressiveClassifier(max_iter = 50)","348f56e5":"linear_clf.fit(X_train, y_train)\npred = linear_clf.predict(X_test)\nscore = metrics.accuracy_score(y_test, pred)\nprint(\"accuracy: %0.3f\" % score)\ncm = metrics.confusion_matrix(y_test, pred)\nplot_confusion_matrix(cm, classes=['Fake', 'Real'])","a286d784":"pred = linear_clf.predict(X_test)\nscore = metrics.accuracy_score(y_test, pred)\nscore","d0c97517":"### Using the CountVectorizer with copied data","116b1910":"### Function for ploting  ","b6772d4b":"Fake News is a serious issue. It is resulting in many illegal and dangerous activity. From Facebook to Youtube there are mountains of these fake news. \n\n\u2620\ufe0f\u2620\ufe0f\u2620\ufe0f\u2620\ufe0f\u2620\ufe0f\u2620\ufe0f\u2620\ufe0f\u2620\ufe0f\u2620\ufe0f\u2620\ufe0f\n\nSo, for my contious streak of 100 day's Kaggle I started with a begginer's friendly Fake News Classifier. I hope this would motivate you and you will also start developing solutions to Real World Problems which would impact a lot of humans.","e2bd9edc":"### A look over data","ceced6a4":"# Import of general libraries ","fda21f13":"## Test Train Split","758d13c3":"### Added both dataframes","709e6304":"### Shuffled for a a unbiased result","f52a6611":"### Labeling of each rows ","43f44345":"Oh! Great! an accuracy above 90% without any extra use of parameters. \ud83d\ude0e","85c79cce":"# Fake News Classifier\n## Using Count Vectorizer","5850fe25":"Not much difference, but in lot of places Passive Aggressive Classifier works better than MultinomialNB.\nIn futher Notebooks I would ttry to use more different ways to solve this type of problems.\n\nTill then Happy Coding\n\nIf you liked my notebook then please do upvote (seriously, it gives motivation and I would try to complete the challenge of 100daysKaggle. \ud83d\ude0e\ud83d\ude0e\ud83d\ude0e","b531f9bd":"### Defined Y (output column)","3d4897f8":"# Reading of Data \n### As we have 2 different csv, 1 for True news and other for Fake we first labeled them and then combined both of them with shuffle.","2e120fbc":"# Use of Passive Aggressive Classifier","cdd5dff0":"# Use of Multinomial NB Classifier"}}