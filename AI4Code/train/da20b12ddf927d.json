{"cell_type":{"8ddeed9b":"code","5928f506":"code","54f7beec":"code","3837aabd":"code","aa053843":"code","6be8cdb6":"code","5e919f4c":"code","d4b4037b":"code","a488a8a8":"code","a661ccdb":"code","be24a2fc":"markdown","fe657ead":"markdown","7f0f280d":"markdown","79bd1d02":"markdown","8b41dd03":"markdown"},"source":{"8ddeed9b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow import keras\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5928f506":"from sklearn.model_selection import train_test_split\ndf = pd.read_csv('..\/input\/car-price-prediction\/CarPrice_Assignment.csv', index_col = 'car_ID')\ny = df.price\nX = df\nX.drop(['price'], axis = 1, inplace = True)\nX_train, X_valid, y_train, y_valid = train_test_split(df, y, train_size = 0.8, test_size = 0.2)\nprint(f\"Shape of X_train = {X_train.shape}\\nShape of y_train = {y_train.shape}\\nShape if X_valid : {X_valid.shape}\\nShape of y_valid = {y_valid.shape}\")","54f7beec":"# Preprocessing\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnum_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\ncat_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['object']]\n\nsi = SimpleImputer(strategy = 'constant')\nohe = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n\npp_pipeline = ColumnTransformer(\ntransformers = [\n    ('num', si, num_cols),\n    ('cat', ohe, cat_cols)\n])\n\npp_X_train = pp_pipeline.fit_transform(X_train)\npp_X_valid = pp_pipeline.transform(X_valid)\nprint(pp_X_train.shape)\nprint(pp_X_valid.shape)","3837aabd":"from keras.models import Sequential\nfrom keras.layers import Dense\nmodel = Sequential()\nmodel.add(Dense(pp_X_train.shape[-1], input_shape = (pp_X_train.shape[-1],)))\nmodel.add(Dense(45, activation = 'relu'))\nmodel.add(Dense(1, activation  = 'linear'))\n\nmodel.compile(optimizer = 'adam', loss = 'mean_absolute_error', metrics = ['mean_absolute_error'])\nprint(model.summary())","aa053843":"epochs = 100\ntrained_model = model.fit(pp_X_train, y_train, epochs = epochs, validation_data = (pp_X_valid, y_valid))","6be8cdb6":"from sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import accuracy_score\n\n#Plot the decrease of MAE with each iteration\nprint(f\"Training MAE : {trained_model.history['loss'][-1]}\")\nprint(f\"Validation MAE : {trained_model.history['val_loss'][-1]}\")\nimport matplotlib.pyplot as plt\nplt.clf()\nfig = plt.figure()\nfig.suptitle('Graph of training loss and validation loss')\nplt.plot(range(epochs), trained_model.history['loss'], 'b', range(epochs), trained_model.history['val_loss'], 'r')","5e919f4c":"import kerastuner as kt\nimport IPython\nimport tensorflow as tf\ndef model_builder(hp):\n    model = Sequential()\n    model.add(Dense(pp_X_train.shape[-1], input_shape = (pp_X_train.shape[-1],)))\n    hp_units = hp.Int('units', min_value = 10, max_value = 180, step = 45)\n    model.add(Dense(units = hp_units, activation = 'relu'))\n    model.add(Dense(units = hp_units, activation = 'relu'))\n    # Give the last layer\n    model.add(Dense(1, activation = 'linear'))\n    \n    #Create HP for learning rate\n    hp_learn_rate = hp.Choice('learning rate', values = [1e-4, 1e-3, 1e-2, 1e-1])\n    model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learn_rate), \n                  loss = keras.losses.MeanAbsoluteError(reduction=\"auto\", name=\"mean_absolute_error\"),\n                  metrics = [keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\", dtype=None)])\n    return model","d4b4037b":"tuner = kt.RandomSearch(model_builder,\n                        objective = 'val_loss',\n                        max_trials=5,\n                        executions_per_trial=4,\n                        directory = 'output',\n                        project_name = 'MLPRegressor')\n# Code Taken from intro to tensorflow core\nclass ClearTrainingOutput(tf.keras.callbacks.Callback):\n  def on_train_end(*args, **kwargs):\n    IPython.display.clear_output(wait = True)\n    \ntuner.search(pp_X_train, y_train, epochs = 10, validation_data = (pp_X_valid, y_valid), callbacks = [ClearTrainingOutput()])\n# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials = 3)[0]","a488a8a8":"at_model = tuner.hypermodel.build(best_hps)\nprint(at_model.summary())\nepochs = 100\nat_trained_model = at_model.fit(pp_X_train, y_train, epochs = epochs, validation_data = (pp_X_valid, y_valid))","a661ccdb":"#Plot the decrease of MAE with each iteration\nprint(f\"Training MAE : {at_trained_model.history['loss'][-1]}\")\nprint(f\"Validation MAE : {at_trained_model.history['val_loss'][-1]}\")\nimport matplotlib.pyplot as plt\nplt.clf()\nfig = plt.figure()\nfig.suptitle('Graph of training loss and validation loss')\nplt.plot(range(epochs), at_trained_model.history['loss'], 'b', range(epochs), at_trained_model.history['val_loss'], 'r')","be24a2fc":"## Fitting the model","fe657ead":"# Creating Autotuned model with Keras Tuner\n","7f0f280d":"## Use custom function to build auto-tuned MLP","79bd1d02":"### Prepocessing done here. Creating a DNN using keras\n","8b41dd03":"## Checking for overfitting"}}