{"cell_type":{"5f28e5d7":"code","10b0de89":"code","3fc44660":"code","3034a5f0":"code","3ba43b69":"code","9a938959":"code","126cd4c6":"code","564b3af3":"code","0eb715e5":"markdown","df8c7e17":"markdown","3ab3fa89":"markdown","2e729760":"markdown","5fd1cd6a":"markdown","463506e0":"markdown","c3b2db42":"markdown","9afc8d94":"markdown","829daeed":"markdown","e32e3de8":"markdown","a2d486d5":"markdown","3775f447":"markdown"},"source":{"5f28e5d7":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\npd.set_option('max_columns', 1000)\npd.set_option('max_rows', 10)","10b0de89":"data= pd.read_csv('..\/input\/quotes.csv',encoding='latin1')\ndata.describe()\ndata.shape\ndata.columns\ntypes=data.dtypes\n","3fc44660":"data.drop_duplicates(['ID_Variable'], keep='first', inplace=True)\npd.set_option('max_rows', 100)\n(data.shape[0]-data.count())\/data.shape[0]\n\ndata.drop(['ID_Variable'],axis=1,inplace=True)\n\ndata.drop(['YEARS_AS_PRINCIPAL_DRIVER','MARKING_SYSTEM','TRACKING_SYSTEM'],axis=1,inplace=True)\n\ndata.rename(columns={'VEHICLE_OWNERSHIP':'OWNERSHIP'},inplace=True) \ndata['OWNERSHIP'].fillna('Unkown',inplace=True)\ndata.rename(columns={'VEHICLE_VALUE':'Value'},inplace=True) \ndata['Value'].fillna('Unkown',inplace=True)\ndata['OCCUPATION'].fillna('Unkown',inplace=True)\n\nfor i in set(data['VEHICLEUSE'].values):\n    data.loc[(data['COMMUTE_DISTANCE'].isnull())&(data['VEHICLEUSE']==i),'COMMUTE_DISTANCE']= data.loc[data['VEHICLEUSE']==i,'COMMUTE_DISTANCE'].mean()\n    \na=data['GENDER'].value_counts()\np_male=a['Male']\/(a['Female']+a['Male'])\ncount=len(data.loc[data['GENDER'].isnull(),'GENDER'])\nfill = np.random.choice(['Male','Female'],count,p=[p_male,1-p_male])\ndata.loc[data['GENDER'].isnull(),'GENDER'] = fill\n\ndata['VEHICLEMAKE']=data['VEHICLEMAKE'].str.upper()\ndata['VEHICLEMODEL']=data['VEHICLEMODEL'].str.upper()\n\ndata['QUOTEDATE']=pd.to_datetime(data['QUOTEDATE'])\ndata['YEAR']=[i.year for i in data['QUOTEDATE']]\ndata['month']=[i.month for i in data['QUOTEDATE']]\ndata['DAY']=[i.day for i in data['QUOTEDATE']]\ndata['QUOTER'] = data['month'].astype('int') \/ 4 + 1\ndata['WEEK']=[i.weekday() for i in data['QUOTEDATE']]\ndata.drop('QUOTEDATE',axis=1,inplace=True)\n\ndata['age']=2019-data['YEAR_OF_BIRTH']\ndata.drop('YEAR_OF_BIRTH',axis=1,inplace=True)\n\nfor i in set(data['age'].values):\n    data.loc[(data['YEARS_LICENSED'].isnull())&(data['age']==i),'YEARS_LICENSED']=  data.loc[data['age']==i,'YEARS_LICENSED'].mean()\n\ndata['MARITAL_STATUS'].replace(['Separated','Divorced','Widow\/Widower'],'Single', inplace=True)\n\na=pd.Series(np.array(list([\"A\", \"B\", \"C\", \"E\",\"G\",\"H\",\"J\",\"K\",\"L\",\"M\",\"N\",\"P\",\"R\",\"S\",\"T\",\"V\",\"X\",\"Y\"])))\nb=pd.Series(np.array(list([\"NL\",\"NS\",\"PE\",\"NB\",\"QC\",\"QC\",\"QC\",\"ON\",\"ON\",\"ON\",\"ON\",\"ON\",\"MB\",\"SK\",\"AB\",\"BC\",\"NT\",\"YT\"])))\ndata['province'] = data['POSTAL_CODE'].str[0]\ndata['province'].replace(a.values,b.values,inplace=True)\n\ndata=data.loc[data['age']>16,]\n","3034a5f0":"import copy as copy\ndata2=copy.deepcopy(data)","3ba43b69":"data=data.loc[data['ANNUAL_KM']<350000,]\ndata=data.loc[data['COMMUTE_DISTANCE']<40000,]\ndata=data.loc[data['age']<100,]\ndata=data.loc[data['YEARS_LICENSED']<100,]","9a938959":"gap = (data['age'].max()-data['age'].min())\/5\npt = []\n\nfor i in range(6):\n    pt.append(data2['age'].min() + i*gap)\nlabel = ['young','adult','mid_age','senior','xsenior']\ndata['age_group']= pd.cut(data['age'], pt, labels=label)\ndata.drop('age_group',axis=1,inplace=True)\n\nimport copy as copy\ndata2=copy.deepcopy(data)\n","126cd4c6":"y = data.IS_BOUND\nx = data.drop('IS_BOUND', axis=1)\n\nfrom sklearn.preprocessing import LabelEncoder\nL = LabelEncoder()\nfor i in x.columns:\n    if x[i].dtypes == 'object':\n        x[i] = L.fit_transform(x[i].astype(str))","564b3af3":"from scipy import stats\ndescribe=data.describe()\n\ndata['VEHICLEYEAR'].value_counts()\n\nIQR=(np.percentile(data['age'],75))-(np.percentile(data['age'],25))\nMin=(np.percentile(data['age'],25)-1.5*IQR)\nMax=(np.percentile(data['age'],75)+1.5*IQR)   \n\nmedian=[]\nfor i in describe.columns:\n    median.append(np.median(describe[i]))\nfrom scipy.stats import mode\nmode(x)\ndata.cov()\nnp.std(x)\nnp.var(x)\nnp.max(x)\nnp.min(x)\nskew = stats.skew(x)\nkurtosis = stats.kurtosis(x)\n\ndata.corr()\n\nfrom scipy.stats import pearsonr\nfor I in x.columns:\n   print(I,pearsonr(x[i],y))\n\npopulation=data['age']\nstats.ttest_1samp(population,45)\n\npopulation1=data.loc[data['GENDER']=='Male','ANNUAL_KM']\npopulation2=data.loc[data['GENDER']=='Female','ANNUAL_KM']\nstats.ttest_ind(population1,population2)\n\nstats.normaltest(x)\n\nfrom matplotlib import pyplot as plt\nplt.subplot(2, 2,1)\nstats.probplot(x['age'], dist=\"norm\", plot=plt)\nplt.subplot(2, 2,2)\nstats.probplot(x['ANNUAL_KM'], dist=\"norm\", plot=plt)\nplt.subplot(2, 2,3)\nstats.probplot(x['COMMUTE_DISTANCE'], dist=\"norm\", plot=plt)\nplt.subplot(2, 2,4)\nstats.probplot(x['Value'], dist=\"norm\", plot=plt)\nplt.show()\n\nstats.chi2_contingency(data['age'],data['IS_BOUND'])\nstats.chi2_contingency(data['VEHICLEYEAR'],data['IS_BOUND'])\nstats.chi2_contingency(x['month'],y)\nstats.chi2_contingency(x['DAY'],y)\n\npopulation1=data.loc[data['OWNERSHIP']=='Owned','ANNUAL_KM']\npopulation2=data.loc[data['OWNERSHIP']=='Leased','ANNUAL_KM']\npopulation3=data.loc[data['OWNERSHIP']=='Non-owned','ANNUAL_KM']\nstats.ttest_ind(population1,population2)\nstats.f_oneway(population1,population2,population3)\n\npivot=pd.pivot_table(data,'IS_BOUND',index='VEHICLEUSE',columns='OCCUPATION',aggfunc='count',fill_value=0)\n\n","0eb715e5":"![image.png](attachment:image.png)","df8c7e17":"**1.Import Data**","3ab3fa89":"**Statistic Analysis**","2e729760":"* 1.**Delete Duplication**\n* 2.**impute missing**\n*    2.1 **if mising >90%: delete**\n*    2.2 **if mising <90% and >50%:replace with unknown**\n*    2.3 **if mising <50%: impute misssing with partial imputation or probability imputation**\n* 3.**Delete the index variable: ID_Variable (primary key is not useful for prediction modeling)**\n* 4.**Replace the missing of ownersip and vehicle-value with unknow**\n* 5.**Use partial imputation to impute the missing value of COMMUTE_DISTANCE because commute distance has high relationship with vehicle use**\n* 6.**Use probability imputation to impute the mising of gender**\n* 7.**Upcase the column VEHICLEMAKE and VEHICLEMODEL**\n* 8.**split the quotedate into month, year,day and week**\n* 9.**Create new variable age instead of year of birth and use partial imputation to impute the missing value of YEARS_LICENSED**\n* 10.**Replace 'Separated','Divorced','Widow\/Widower' with single**\n* 11.**Create province for postcode variable**\n* 12.**delete Noise Data**\n* 13.**detect outlier data**\n* 14.**Data accuracy and consistency**\n* 15.**Discretization**\n* 16.**create calculation field**\n","5fd1cd6a":"![image.png](attachment:image.png)","463506e0":"**Discretization**","c3b2db42":"![image.png](attachment:image.png)","9afc8d94":"**2.Data Cleaning**","829daeed":"**Export cleaned data file into Tableau and from the boxplot, we can identify there are some outliers\/noises data exists and should be solved**","e32e3de8":"* 1.**Describe staistics**\n* 2.**Frequency Table**\n* 3.**Box Plot: 25th percentile, 75 percentile, Interquartile Range(IQR), Max,Min**\n* 4.**Median,Mode, Covariance,standard deviation,Variance,max,min,skewness,kurtosis **\n* 5.**correlation among attributes**\n* 6.**pearson correlation between input variables and target variables**\n* 7.**Factor Analysis**\n* 8.**T-test(one-sample)**\n* 9.**T-test(independent group: Whether the male's mean of annual km has significant difference with that of female)**\n* 10.**Normal distribution testing**\n* 11.**QQ Plot**\n* 12.**Chi-square Test(chi2,p,dof,expected)**\n* 13.**ANOVA(whether there is huge difference in means of different yypes of ownership )**\n* 14.**Pivot table**\n","a2d486d5":"**5. identify input variables and target variable and Feature preprocessing(label encoding:change categorical variable into numerical)**","3775f447":"![image.png](attachment:image.png)"}}