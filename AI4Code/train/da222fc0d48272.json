{"cell_type":{"0d9b48cd":"code","c53947fb":"code","280b1997":"code","0fef3f0d":"code","0f3cb483":"code","2b024ed5":"code","73344296":"code","2c4215f1":"code","83957d75":"code","8c5d66a7":"code","599019b5":"code","cc80fbd8":"code","55555c50":"code","669f5cc1":"code","fe1ee3d7":"code","ef5261dd":"code","40712870":"code","cf0fd233":"code","cd0db60d":"code","72d5af2d":"code","7d055c03":"code","f37d836b":"code","85a9427f":"code","fa61fbb7":"code","56f0eb44":"code","1c965270":"code","5db371b7":"code","5c4c9a3f":"code","2437c261":"code","222351d9":"code","65dca83e":"code","16237a6c":"code","385c8578":"code","9f23a77a":"code","39f076f3":"code","d6e5a1b0":"code","f7ff61ce":"code","571dacf0":"markdown","0fce2d38":"markdown","0ba754b4":"markdown","42cf13ee":"markdown","9035bc05":"markdown","420d3bc9":"markdown","753a0855":"markdown","4b65dd4c":"markdown","c53223b2":"markdown","e2bceebf":"markdown","c4914bec":"markdown","17a6a707":"markdown","3a1bfdd5":"markdown","4b3b3a6b":"markdown","08e6f60a":"markdown","a0071729":"markdown","3ee84dd8":"markdown","ee58de84":"markdown","dc849b8f":"markdown","75e8126a":"markdown","61f5861a":"markdown","bf6b3324":"markdown","cea9286e":"markdown","2344947f":"markdown","2a45b4eb":"markdown","bccb13c7":"markdown","80ab04e2":"markdown","715d0dca":"markdown","593c0056":"markdown","1e42b362":"markdown","7a1a93be":"markdown"},"source":{"0d9b48cd":"import pandas as pd \nimport seaborn as sns # for data visualization\nimport matplotlib.pyplot as plt # for data visualization\n%matplotlib inline","c53947fb":"df = pd.read_csv(\"..\/input\/Breast_cancer_data.csv\", delimiter=\",\")","280b1997":"df.head() #gives first 5 entries of a dataframe by default","0fef3f0d":"df.columns","0f3cb483":"df.isnull().sum()","2b024ed5":"count = df.diagnosis.value_counts()\ncount","73344296":"count.plot(kind='bar')\nplt.title(\"Distribution of malignant(1) and benign(0) tumor\")\nplt.xlabel(\"Diagnosis\")\nplt.ylabel(\"count\");","2c4215f1":"y_target = df['diagnosis']","83957d75":"df.columns.values","8c5d66a7":"df['target'] = df['diagnosis'].map({0:'B',1:'M'}) # converting the data into categorical","599019b5":"g = sns.pairplot(df.drop('diagnosis', axis = 1), hue=\"target\", palette='prism');","cc80fbd8":"sns.scatterplot(x='mean_perimeter', y = 'mean_texture', data = df, hue = 'target', palette='prism');","55555c50":"features = ['mean_perimeter', 'mean_texture']","669f5cc1":"X_feature = df[features]","fe1ee3d7":"# X_feature = df.drop(['target','diagnosis'], axis = 1)","ef5261dd":"from sklearn.model_selection import train_test_split","40712870":"X_train, X_test, y_train, y_test= train_test_split(X_feature, y_target, test_size=0.3, random_state = 42)","cf0fd233":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","cd0db60d":"model = LogisticRegression()","72d5af2d":"model.fit(X_train, y_train)","7d055c03":"from mlxtend.plotting import plot_decision_regions","f37d836b":"# !pip install mlxtend","85a9427f":"plot_decision_regions(X_train.values, y_train.values, clf=model, legend=2)\nplt.title(\"Decision boundary for Logistic Regression (Train)\")\nplt.xlabel(\"mean_perimeter\")\nplt.ylabel(\"mean_texture\");","fa61fbb7":"y_pred = model.predict(X_test)","56f0eb44":"acc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy score using Logistic Regression:\", acc*100)","1c965270":"plot_decision_regions(X_test.values, y_test.values, clf=model, legend=2)\nplt.title(\"Decision boundary for Logistic Regression (Test)\")\nplt.xlabel(\"mean_perimeter\")\nplt.ylabel(\"mean_texture\");","5db371b7":"from sklearn.metrics import confusion_matrix","5c4c9a3f":"conf_mat = confusion_matrix(y_test, y_pred)","2437c261":"conf_mat","222351d9":"from sklearn.neighbors import KNeighborsClassifier","65dca83e":"clf = KNeighborsClassifier()","16237a6c":"clf.fit(X_train, y_train)","385c8578":"y_pred = clf.predict(X_test)","9f23a77a":"acc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy score using KNN:\", acc*100)","39f076f3":"confusion_matrix(y_test, y_pred)","d6e5a1b0":"plot_decision_regions(X_train.values, y_train.values, clf=clf, legend=2)\nplt.title(\"Decision boundary using KNN (Train)\")\nplt.xlabel(\"mean_perimeter\")\nplt.ylabel(\"mean_texture\");","f7ff61ce":"plot_decision_regions(X_test.values, y_test.values, clf=clf, legend=2)\nplt.title(\"Decision boundary using KNN (Test)\")\nplt.xlabel(\"mean_perimeter\")\nplt.ylabel(\"mean_texture\");","571dacf0":"**Accuracy**\n\nThe predicted values and the actual test values are compared to compute the accuracy.","0fce2d38":"**Predictions are done on the Test set**","0ba754b4":"**The features mean_perimeter and mean_texture seem to be most relevant**","42cf13ee":"---\n### Binary classification using K Nearest Neighbours\n\nKNN Algorithm is based on feature similarity, i.e how closely out-of-sample features resemble our training set determines how we classify a given data point.\n\n<img src = \"https:\/\/cdn-images-1.medium.com\/max\/800\/0*Sk18h9op6uK9EpT8.\">","9035bc05":"### Importing the necessary libraries","420d3bc9":"**Let us now plot out the pairplot of different features to determine which features are better at classifying the 2 classes of our problem.**","753a0855":"# AI FOR SOCIAL GOOD: WOMEN CODERS' BOOTCAMP\n## Breast cancer prediction \n\n> [Merishna Singh Suwal](https:\/\/www.linkedin.com\/in\/merishna-ss\/) and [Pragyan Subedi](https:\/\/www.linkedin.com\/in\/pragyanbo\/)","4b65dd4c":"### Data dictionary\n- diagnosis: The diagnosis of breast tissues (1 = malignant, 0 = benign)\n- mean_radius: mean of distances from center to points on the perimeter\n- mean_texture: standard deviation of gray-scale values\n- mean_perimeter: mean size of the core tumor\n- mean_area\n- mean_smoothness: mean of local variation in radius lengths\n","c53223b2":"<img src = \"https:\/\/www.dataschool.io\/content\/images\/2015\/01\/confusion_matrix2.png\">","e2bceebf":"**Now, we will be looking at the distribution of classes( Malignant and Benign) in our dataset.**","c4914bec":"**The distribution can be visualized as well by using a simple plot function of the matplotlib library.**","17a6a707":"### Splitting the data into training and test set\nWe use Cross Validation to assess the predictive performance of the models and and to judge how they perform outside the sample to a new data set also known as test data. So our classifier is first trained on the train set( usually 70% of the total data) and then tested on the test set( usually rest 30% of the data which the classifier has not seen) on the basis of which accuracy is computed.","3a1bfdd5":"<img src = \"https:\/\/mapr.com\/blog\/churn-prediction-sparkml\/assets\/Picture14.png\">","4b3b3a6b":"### Feature Selection","08e6f60a":"---\n### Target variable\/ class\nThe main motive of our predictor is to correctly predict on the basis of the data available, if the breast cancer is \n- Malignant(1) i.e. Harmful ,or\n- Benign(0) i.e. Not Harmful.\n\nHence, our target class is **Diagnosis**","a0071729":"Most datasets that we work on will not be as clean as this one. **Data cleaning** is an important part of any problem in Data Science.  Go through [this](https:\/\/www.kaggle.com\/rtatman\/data-cleaning-challenge-handling-missing-values) exercise to learn how to handle missing values in a dataset.","3ee84dd8":"**Plotting the decision boundaries**","ee58de84":" Data used:\n\nhttps:\/\/www.kaggle.com\/merishnasuwal\/breast-cancer-prediction-dataset ","dc849b8f":"### THANK YOU","75e8126a":"Now, among all the features available, we need to select the best set of features inorder to train our predictor. A typical dataset might have features ranging from 30 to even about 100 and more. In such a case, feature selection plays an important role in the accuracy of the prediction.\n\nLet's see what features are available on our dataset.","61f5861a":"**Plotting decision boundaries for 2 features**","bf6b3324":"Basic notebook commands:\n- Shift+Enter: Execute a cell\n-  a : Add new cell above\n- b: Add new cell below\n- x: Cut the cell\n- c: Copy the cell\n- m: Markdown\n- y: Code\n- z: Undo\n\n**Fork this Notebook** to get started.","cea9286e":"---\n#### Binary classification using Logistic Regression\n\nLogistic Regression is mostly used for binary classifications where the dependent variable(target) which are dichotomous in nature( yes or no). ","2344947f":"---\n**Always make a habit to check for null values in a dataset**","2a45b4eb":"Steps in any Machine Learning classification problem\n- Exploring the dataset\n- Preprocessing the dataset and feature selection\n- Splitting the dataset into training and testing set\n- Building the model\n- Evaluating the model","bccb13c7":"Data is trained to fit on the train set.","80ab04e2":"#### Checking the columns","715d0dca":"#### Reading the dataset using pandas","593c0056":"### Confusion matrix","1e42b362":"# If using from Google colaboratory\nfrom google.colab import files\n\nuploaded = files.upload()","7a1a93be":"**Taking all features**"}}