{"cell_type":{"dae73e70":"code","cd0bb1b9":"code","d5cac9c0":"code","fd030296":"code","e0178d2b":"code","4ed407e7":"code","979bd1c0":"code","f23a1999":"code","b132c9f9":"code","17c4a1df":"code","403cebc7":"code","9d539f65":"code","4c629c0b":"code","26821ea2":"code","4e3c8385":"code","a01a066c":"code","c33e3135":"code","0d52bfc5":"code","c2cfacd9":"code","10e91ea0":"code","09507c3d":"code","85790746":"code","019ed5a6":"code","d6253e20":"code","e0161c6e":"code","b73bb777":"code","4081b317":"code","ddca5bbf":"code","86257468":"code","690f8928":"code","1d198b1b":"code","5e6ae1ec":"code","bc7e4c8a":"markdown","9dcc71f6":"markdown","08f9e1d1":"markdown","e15015f2":"markdown","f377fd5c":"markdown","9972ac29":"markdown","1bb38d1f":"markdown","819c078d":"markdown","07f043c0":"markdown","26ca3b47":"markdown","4e50d429":"markdown","5df22961":"markdown","c4b9b712":"markdown","91e52376":"markdown","1e823aaf":"markdown","5a89c01f":"markdown","81117880":"markdown","a75dbf8e":"markdown","a5f2e492":"markdown","314dbdaf":"markdown","a0117dc2":"markdown","49efe6e0":"markdown","fe65c20a":"markdown","a0a02a2b":"markdown","10394a06":"markdown","b10b4e8c":"markdown"},"source":{"dae73e70":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\n\nfrom catboost import CatBoostRegressor, cv, Pool\n\nfrom collections import defaultdict\n\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\n\nimport shap\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.pipeline import Pipeline\n\nfrom tqdm import tqdm","cd0bb1b9":"data_dir = \"\/kaggle\/input\/tabular-playground-series-feb-2021\/\"\ntrain_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\ntest_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\ntrain_df.head()","d5cac9c0":"test_df.head()","fd030296":"class DataProcessor(object):\n    def __init__(self):\n        self.encoder = None\n        self.standard_scaler = None\n        self.num_cols = None\n        self.cat_cols = None\n        \n    def preprocess(self, data_df, train=True, one_hot_encode=False,\n                   combine_min_cats=False, add_pca_feats=False):\n        \"\"\" Preprocess train \/ test as required \"\"\"\n        \n        # if training, fit our transformers\n        if train:\n            self.train_ids = data_df.loc[:, 'id']\n            train_cats = data_df.loc[:, data_df.dtypes == object]\n            self.cat_cols = train_cats.columns\n            \n            # if selected, combine minority categorical feats\n            if combine_min_cats:\n                self._find_minority_cats(train_cats)\n                train_cats = self._combine_minority_feats(train_cats)\n            \n            # if selected, one hot encode our cat features\n            if one_hot_encode:\n                self.encoder = OneHotEncoder(handle_unknown='ignore')\n                oh_enc = self.encoder.fit_transform(train_cats).toarray()\n                train_cats_enc = pd.DataFrame(oh_enc, columns=self.encoder.get_feature_names(), \n                                              dtype=np.int64)\n                self.final_cat_cols = list(train_cats_enc.columns)\n            \n            # otherwise just encode our cat feats with ints\n            else:\n                # encode all of our categorical variables\n                self.encoder = defaultdict(LabelEncoder)\n                train_cats_enc = train_cats.apply(lambda x: \n                                                  self.encoder[x.name].fit_transform(x))\n                self.final_cat_cols = list(self.cat_cols)\n            \n            \n            # standardise all numerical columns\n            train_num = data_df.loc[:, data_df.dtypes != object].drop(columns=['target', 'id'])\n            self.num_cols = train_num.columns\n            self.standard_scaler = StandardScaler()\n            train_num_std = self.standard_scaler.fit_transform(train_num)\n            \n            # add pca reduced num feats if selected, else just combine num + cat feats\n            if add_pca_feats:\n                pca_feats = self._return_num_pca(train_num_std)\n                self.final_num_feats = list(self.num_cols)+list(self.pca_cols)\n                \n                \n                X = pd.DataFrame(np.hstack((train_cats_enc, train_num_std, pca_feats)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols)+list(self.pca_cols))\n            else:   \n                self.final_num_feats = list(self.num_cols)\n                X = pd.DataFrame(np.hstack((train_cats_enc, train_num_std)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols))\n        \n        # otherwise, treat as test data\n        else:\n            # transform categorical and numerical data\n            self.test_ids = data_df.loc[:, 'id']\n            cat_data = data_df.loc[:, self.cat_cols]\n            if combine_min_cats:\n                cat_data = self._combine_minority_feats(cat_data)\n        \n            if one_hot_encode:\n                oh_enc = self.encoder.transform(cat_data).toarray()\n                cats_enc = pd.DataFrame(oh_enc, \n                                        columns=self.encoder.get_feature_names(), \n                                        dtype=np.int64)\n            else:\n                cats_enc = cat_data.apply(lambda x: self.encoder[x.name].transform(x))\n                \n            # transform test numerical data\n            num_data = data_df.loc[:, self.num_cols]\n            num_std = self.standard_scaler.transform(num_data)\n            \n            if add_pca_feats:\n                pca_feats = self._return_num_pca(num_std, train=False)\n                \n                X = pd.DataFrame(np.hstack((cats_enc, num_std, pca_feats)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols)+list(self.pca_cols))\n            \n            else:\n                X = pd.DataFrame(np.hstack((cats_enc, num_std)), \n                        columns=list(self.final_cat_cols)+list(self.num_cols)) \n        return X\n    \n    \n    def _find_minority_cats(self, data_df, composite_category='z', threshold=0.05):\n        \"\"\" Find minority categories for each feature column, and create a \n            dictionary that maps those to selected composite category \"\"\"\n        self.min_col_dict = {}\n        self.min_cat_mappings = {}\n    \n        # find all feature categories with less than 5% proportion\n        for feature in self.cat_cols:\n            self.min_col_dict[feature] = []\n            self.min_cat_mappings[feature] = {}\n        \n            for category, proportion in data_df[feature].value_counts(normalize=True).iteritems():\n                if proportion < threshold:\n                    self.min_col_dict[feature].append(category)\n                \n                    # map those minority cats to chosen composite feature\n                    self.min_cat_mappings[feature] = {x : composite_category for x \n                                                    in self.min_col_dict[feature]}\n    \n    \n    def _combine_minority_feats(self, data_df, replace=False):\n        \"\"\" Combine minority categories into composite for each cat feature \"\"\"\n        new_df = data_df.copy()\n        for feat in self.cat_cols:\n            col_label = f\"{feat}\" if replace else f\"{feat}_new\"\n            new_df[feat] = new_df[feat].replace(self.min_cat_mappings[feat])\n        return new_df\n    \n    \n    def _return_num_pca(self, num_df, n_components=0.85, train=True):\n        \"\"\" return dim reduced numerical features using PCA \"\"\"\n        if train:\n            self.pca = PCA(n_components=n_components)\n            num_rd = self.pca.fit_transform(num_df)\n            \n            # create new col names for our reduced features\n            self.pca_cols = [f\"pca_{x}\" for x in range(num_rd.shape[1])]\n            \n        else:\n            num_rd = self.pca.transform(num_df)\n        \n        return pd.DataFrame(num_rd, columns=self.pca_cols)","e0178d2b":"PCA_FEATS = True\nONE_HOT_ENCODE = True\n\ndata_proc = DataProcessor()\nX = data_proc.preprocess(train_df, add_pca_feats=PCA_FEATS, one_hot_encode=ONE_HOT_ENCODE)\ny = train_df.loc[:, 'target']\nX_test = data_proc.preprocess(test_df, train=False, add_pca_feats=PCA_FEATS, one_hot_encode=ONE_HOT_ENCODE)\n\nprint(f\"X: {X.shape} \\ny: {y.shape} \\nX_test: {X_test.shape}\")","4ed407e7":"# convert all of our categorical columns to ints before using GBMs\ncat_feat_dtype_dict = { x : \"int\" for x in data_proc.final_cat_cols}\nX = X.astype(cat_feat_dtype_dict)","979bd1c0":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=13)\nprint(f\"X_train: {X_train.shape} \\ny_train: {y_train.shape} \\nX_val: {X_val.shape}, \\ny_val: {y_val.shape}\")","f23a1999":"_, X_sub, _, y_sub = train_test_split(X, y, test_size=0.05)\nX_sub.shape, y_sub.shape","b132c9f9":"X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(X_sub, y_sub, test_size=0.2)\nX_train_sub.shape, X_val_sub.shape, y_train_sub.shape, y_val_sub.shape","17c4a1df":"cb_reg_1 = CatBoostRegressor(task_type='GPU', random_seed=13, verbose=200)\n#cb_reg = CatBoostRegressor(random_seed=13, verbose=200)","403cebc7":"cb_reg_1.fit(X_train, y_train)\ncb_preds = cb_reg_1.predict(X_val)","9d539f65":"cb_preds = cb_reg_1.predict(X_val)\n\n# calculate mean squared error on val sub-set preds\nnp.sqrt(mean_squared_error(cb_preds, y_val))","4c629c0b":"params = {'loss_function':'RMSE', 'verbose': 200, 'random_seed': 13, 'task_type':'GPU'}\n\nX_pool = Pool(data=X, label=y, cat_features=data_proc.final_cat_cols)","26821ea2":"#%%time\n\n#scores = cv(pool=X_pool, params=params, fold_count=4, seed=13, \n#            shuffle=True, stratified=True, plot=True)","4e3c8385":"# Catboost hyperparameters to search\ncatboost_hyperparams = { 'learning_rate': hp.choice('learning_rate', np.arange(0.05, 0.31, 0.05)),\n                         'max_depth': hp.choice('max_depth', np.arange(5, 16, 1, dtype=int)),\n                         'colsample_bylevel': hp.choice('colsample_bylevel', np.arange(0.3, 0.8, 0.1)),\n                         'n_estimators': 100, 'eval_metric': 'RMSE'}\n\ncatbooast_fit_params = { 'early_stopping_rounds': 10, 'verbose': False }\n\ncatboost_params = dict()\ncatboost_params['reg_params'] = catboost_hyperparams\ncatboost_params['fit_params'] = catbooast_fit_params\ncatboost_params['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))","a01a066c":"class HPOptimiser(object):\n    \"\"\" Class to optimiser hyper-parameters using hyperopt on a given\n        set of training and validation inputs and labels \"\"\"\n\n    def __init__(self, X_train, X_val, y_train, y_val):\n        self.X_train = X_train\n        self.X_val  = X_val\n        self.y_train = y_train\n        self.y_val  = y_val\n\n    def process(self, fn_name, space, trials, algo, max_evals):\n        fn = getattr(self, fn_name)\n        try:\n            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n        except Exception as e:\n            return {'status': STATUS_FAIL,\n                    'exception': str(e)}\n        return result, trials\n\n    def catboost_reg(self, para):\n        reg = CatBoostRegressor(**para['reg_params'])\n        return self.train_reg(reg, para)\n\n    def train_reg(self, reg, params):\n        reg.fit(self.X_train, self.y_train,\n                eval_set=[(self.X_train, self.y_train), (self.X_val, self.y_val)],\n                **params['fit_params'])\n        pred = reg.predict(self.X_val)\n        loss = params['loss_func'](self.y_val, pred)\n        return {'loss': loss, 'status': STATUS_OK}","c33e3135":"#obj = HPOptimiser(X_train_sub, X_val_sub, y_train_sub, y_val_sub)\n\n#catboost_opt = obj.process(fn_name='catboost_reg', space=catboost_params, \n#                           trials=Trials(), algo=tpe.suggest, max_evals=100)","0d52bfc5":"# print optimal parameters found\n#catboost_opt","c2cfacd9":"cb_learn_rate = 0.006\nn_iterations = 80000\nearly_stop_rounds = 400\n\nopt_catboost_params = {'iterations' : n_iterations,\n                       'learning_rate' : cb_learn_rate,\n                       'depth': 7,\n                       'bootstrap_type' : 'Bernoulli',\n                       'random_strength': 1,\n                       'min_data_in_leaf': 10,\n                       'l2_leaf_reg': 3,\n                       'loss_function' : 'RMSE', \n                       'eval_metric' : 'RMSE',\n                       'grow_policy' : 'Depthwise',\n                       'max_bin' : 1024, \n                       'model_size_reg' : 0,\n                       'task_type' : 'GPU',\n                       'od_type' : 'IncToDec',\n                       'od_wait' : 100,\n                       'metric_period' : 500,\n                       'verbose' : 500,\n                       'subsample' : 0.8,\n                       'od_pval' : 1e-10,\n                       'max_ctr_complexity' : 8,\n                       'has_time': False,\n                       'simple_ctr' : 'FeatureFreq',\n                       'combinations_ctr': 'FeatureFreq',\n                       'random_seed' : 13}","10e91ea0":"cb_reg = CatBoostRegressor(**opt_catboost_params)\n\ncb_reg.fit(X_train, y_train, eval_set=(X_val, y_val), \n           use_best_model=True, plot=True, \n           early_stopping_rounds=early_stop_rounds)","09507c3d":"val_preds = cb_reg.predict(X_val)\n\n# calculate mean squared error on val sub-set preds\nnp.sqrt(mean_squared_error(val_preds, y_val))","85790746":"def residual_plot(train_labels, train_preds, test_labels=None, test_preds=None, \n                  title=\"Residual Plot\", figsize=(9,6), xlim=[6.5, 9.5]):\n    \"\"\" Residual plot to evaluate performance of our simple linear regressor \"\"\"\n    plt.figure(figsize=figsize)\n    plt.scatter(train_preds, train_preds - train_labels, c='blue', alpha=0.1,\n                marker='o', edgecolors='white', label='Training')\n    \n    if test_labels is not None:\n        plt.scatter(test_preds, test_preds - test_labels, c='red', alpha=0.1,\n                    marker='^', edgecolors='white', label='Test')\n    plt.xlabel('Predicted values')\n    plt.ylabel('Residuals')\n    plt.hlines(y=0, xmin=xlim[0], xmax=xlim[1], color='black', lw=2)\n    plt.xlim(xlim)\n    if test_labels is not None:\n        plt.legend(loc='best')\n    plt.title(title)\n    plt.show()\n    return","019ed5a6":"train_preds = cb_reg.predict(X_train)\n\nresidual_plot(y_train[:10000], train_preds[:10000], \n              y_val[:10000], val_preds[:10000], \n              title=\"CatBoost Residual Plot\")","d6253e20":"feat_importances = cb_reg.get_feature_importance(prettified=True)\nfeat_importances","e0161c6e":"plt.figure(figsize=(12, 10))\nsns.barplot(x=\"Importances\", y=\"Feature Id\", data=feat_importances.loc[:30, :])\nplt.title('CatBoost features importance:')","b73bb777":"train_data = Pool(data=X_train, label=y_train)\n\nval_data = Pool(data=X_val, label=y_val)","4081b317":"explainer = shap.TreeExplainer(cb_reg_1) \nshap_values = explainer.shap_values(train_data)\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[:100,:], X_train.iloc[:100,:])","ddca5bbf":"shap.summary_plot(shap_values, X_train)","86257468":"N_FOLDS = 5\nk_folds = KFold(n_splits=N_FOLDS, shuffle=True)\n\n# convert y values into appropriate form\ny_array = y.values.reshape(-1, 1)","690f8928":"model_rmses = []\ntest_preds = np.zeros((X_test.shape[0], 1))\n\nfor train_idx, val_idx in tqdm(k_folds.split(X, y)):\n    train_split = X.iloc[train_idx].copy()\n    train_labels = y.iloc[train_idx].astype(np.float64).copy()\n    val_split = X.iloc[val_idx].copy()\n    val_labels = y.iloc[val_idx].astype(np.float64).copy()\n    \n    temp_model = CatBoostRegressor(**opt_catboost_params)\n    \n    # train model for 100 epochs with early stopping\n    temp_model.fit(train_split, train_labels, \n               eval_set=(val_split, val_labels), \n               use_best_model=True,\n               early_stopping_rounds=early_stop_rounds)\n    \n    # find log loss for out of fold val data\n    model_val_preds = temp_model.predict(val_split)\n    \n    # calculate mean squared error on val sub-set preds\n    fold_rmse = np.sqrt(mean_squared_error(model_val_preds, val_labels))\n    model_rmses.append(fold_rmse)\n    print(f'Current Fold validation RMSE: {fold_rmse:.4f}')\n    \n    # make predictions on test set for each fold\n    temp_test_preds = temp_model.predict(X_test).reshape(-1, 1)\n    test_preds += (temp_test_preds \/ N_FOLDS)\n\n# convert results to np array\nmodel_rmses = np.array(model_rmses)","1d198b1b":"plt.figure(figsize=(8,4))\nsns.lineplot(x=range(1, model_rmses.shape[0]+1), y=model_rmses)\nplt.xlabel(\"Fold Number\", weight='bold')\nplt.ylabel(\"Validation RMSE\", weight='bold')\nplt.grid()\nplt.show()","5e6ae1ec":"submission_df = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\nsubmission_df['target'] = test_preds\nsubmission_df.to_csv('submission.csv', index=False)","bc7e4c8a":"<a id=\"data-preprocessing\"><\/a>\n## 2. Data Preprocessing: Creation of a data loader and preprocessor","9dcc71f6":"<a id=\"load\"><\/a>\n## 1. Load Data and Analyse Overall Dataset Features","08f9e1d1":"Lets now train on our full training set:","e15015f2":"It seems the best hyper-parameter values found were: {'colsample_bylevel': 0, 'learning_rate': 2, 'max_depth': 1}.\n\nLets now see how our model performs using these hyper-parameters on the full dataset.","f377fd5c":"**Table of Contents:**\n\n1. [Load Data and Analyse Overall Dataset Features](#load)\n2. [Data Preparation and Preprocessing](#data-preprocessing)\n3. [CatBoost Regressor](#catboost-model)\n4. [Test set predictions using KFolds](#test-preds)","9972ac29":"For interest, we can make use of our feature importances to plot the most important features used for our model:","1bb38d1f":"### 3.3 Training and evaluation on the full training dataset","819c078d":"### 3.1 Basic CatBoost model production","07f043c0":"Lets try and optimise our hyper-parameters using the small sub-sets we created above:","26ca3b47":"Lets plot the top 30 importances:","4e50d429":"Very pretty.... Now lets make our final model again, and submit a set of predictions on the test set.","5df22961":"As shown, CatBoost can have remarkable performance, even without lots of manual hyper-paramter tuning. This is due to its built in optimisation performed during training.\n\nMany thanks for reading through this notebook - I hope you enjoyed!","c4b9b712":"We'll make a set of predictions on the test set through the use of KFolds cross validation. For each fold, we'll make predictions on the test set. \n\nWe'll then take the overall average of these predictions across all models for the final test predictions, and submit them accordingly.","91e52376":"Final model and predictions on test set:","1e823aaf":"Great, we now have our final predictions, and so can submit these to the competition:","5a89c01f":"<a id=\"catboost-model\"><\/a>\n## 3. Production of a CatBoost Regressor","81117880":"Lets quickly visualise the residuals of our predictions on both the training and validation sets, to get an idea of how well our model is performing in addition to the MSE \/ RMSE.","a75dbf8e":"Expanding on this concept further, we can make use of SHAP to visualise how our model is actually using each faeture to make predictions.","a5f2e492":"Its important that we handle our numerical and categorical features appropriately prior to producing our models.\n\nWe'll put together some preprocessing functions to encode our categorical features and standardise our numerical features. Whilst doing this, we'll also add support for combining some of the minority categories within our data features (since some are very imbalanced), and add support for producing additional dimensionality-reduced features (using PCA) to our dataset.\n\nThese extra features will allow us to experiment and tune to find the best combinations of feature engineering to perform for this problem.","314dbdaf":"<a id=\"test-preds\"><\/a>\n## 4. Final CatBoost Test Set Predictions using KFolds","a0117dc2":"### 3.2 Optimising our hyper-parameters using Bayesian Optimisation","49efe6e0":"Performance is not bad by default! However, performing cross-validation is this way can become time consuming, and is not amenable for searching a large number of hyper-parameter combinations. For this we'll make use of our smaller split to approximate performance.","fe65c20a":"There is room to improve this through hyper-parameter optimisation. We'll do this using the much smaller sub-set from above, so that this process is a lot faster. We'll use bayesian optimisation for this using Hyperopt.\n\nWe first need to define our hyper-parameter ranges through which we'll search using bayesian optimisation. In addition, we'll speed up this process considerably through the use of the GPU for training our CatBoost regressors:","a0a02a2b":"# CatBoost Regression","10394a06":"Lets now try KFolds cross-validation on the entire training set, and see how well our model performs:","b10b4e8c":"### 3.4 Analysis of feature importances and explainability of predictions"}}