{"cell_type":{"70abc6b9":"code","e7a4001b":"code","14bedf32":"code","dcc9d33d":"code","8e55043a":"code","5326fe0c":"code","12f37867":"code","a7ca6905":"code","9a278b2d":"code","790c26e8":"code","69c142a7":"code","d80317c7":"markdown","ba5bb695":"markdown","928bfa99":"markdown","b67d64eb":"markdown","474b69d9":"markdown","1d581957":"markdown","d73f27e3":"markdown","25e17065":"markdown","2cd87f59":"markdown","66f64396":"markdown","1acb0b5b":"markdown","00253aeb":"markdown","c1890a76":"markdown","7dbb4e81":"markdown","c351f69b":"markdown","60835b72":"markdown","8be0817e":"markdown","fa7b5062":"markdown"},"source":{"70abc6b9":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n%matplotlib inline","e7a4001b":"url = \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/iris\/iris.data\"\n\n# Assign colum names to the dataset\nnames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']\n\n# Read dataset to pandas dataframe\ndataset = pd.read_csv(url, names=names)","14bedf32":"dataset.head()","dcc9d33d":"X = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 4].values","8e55043a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)","5326fe0c":"scaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","12f37867":"classifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(X_train, y_train)","a7ca6905":"y_pred = classifier.predict(X_test)","9a278b2d":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","790c26e8":"error = []\n\n# Calculating error for K values between 1 and 40\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i != y_test))","69c142a7":"plt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')","d80317c7":"# Feature Scaling\nBefore making any actual predictions, it is always a good practice to scale the features so that all of them can be uniformly evaluated.","ba5bb695":"# Preprocessing","928bfa99":"The results show that our KNN algorithm was able to classify all the 30 records in the test set with 93% accuracy, which is excellent. Although the algorithm performed very well with this dataset, don't expect the same results with all applications. As noted earlier, KNN doesn't always perform as well with high-dimensionality or categorical features.","b67d64eb":"# Importing Libraries","474b69d9":"# Evaluating the Algorithm\nFor evaluating an algorithm, confusion matrix, precision, recall and f1 score are the most commonly used metrics. The confusion_matrix and classification_report methods of the sklearn.metrics can be used to calculate these metrics. Take a look at the following script:","1d581957":"# Conclusion\nKNN is a simple yet powerful classification algorithm. It requires no training for making predictions, which is typically one of the most difficult parts of a machine learning algorithm. The KNN algorithm have been widely used to find document similarity and pattern recognition. It has also been employed for developing recommender systems and for dimensionality reduction and pre-processing steps for computer vision, particularly face recognition tasks.","d73f27e3":"# Train Test Split\nTo avoid over-fitting, we will divide our dataset into training and test splits, which gives us a better idea as to how our algorithm performed during the testing phase. This way our algorithm is tested on un-seen data, as it would be in a production application.","25e17065":"The first step is to import the KNeighborsClassifier class from the sklearn.neighbors library. In the second line, this class is initialized with one parameter, i.e. n_neigbours. This is basically the value for the K. There is no ideal value for K and it is selected after testing and evaluation, however to start out, 5 seems to be the most commonly used value for KNN algorithm.","2cd87f59":"<img src=\"data:image\/png;base64,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\" alt = \"KNN-Classifier\">","66f64396":"# Training and Predictions\nIt is extremely straight forward to train the KNN algorithm and make predictions with it, especially when using Scikit-Learn.","1acb0b5b":"# Comparing Error Rate with the K Value\nIn the training and prediction section we said that there is no way to know beforehand which value of K that yields the best results in the first go. We randomly chose 5 as the K value and it just happen to result in 100% accuracy.\n\nOne way to help you find the best value of K is to plot the graph of K value and the corresponding error rate for the dataset.\n\nIn this section, we will plot the mean error for the predicted values of test set for all the K values between 1 and 40.\n\nTo do so, let's first calculate the mean of error for all the predicted values where K ranges from 1 and 40. Execute the following script:","00253aeb":"The above script splits the dataset into 80% train data and 20% test data. This means that out of total 150 records, the training set will contain 120 records and the test set contains 30 of those records.","c1890a76":"# Main Concept of the K-Nearest Neighbors\nThe intuition behind the KNN algorithm is one of the simplest of all the supervised machine learning algorithms. It simply calculates the distance of a new data point to all other training data points. The distance can be of any type e.g Euclidean or Manhattan etc. It then selects the K-nearest data points, where K can be any integer. Finally it assigns the data point to the class to which the majority of the K data points belong.","7dbb4e81":"# Pros and Cons of KNN\nIn this section we'll present some of the pros and cons of using the KNN algorithm.\n\n# Pros \nIt is extremely easy to implement\nAs said earlier, it is lazy learning algorithm and therefore requires no training prior to making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g SVM, linear regression, etc.\nSince the algorithm requires no training before making predictions, new data can be added seamlessly.\nThere are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)\nCons\nThe KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate distance in each dimension.\nThe KNN algorithm has a high prediction cost for large datasets. This is because in large datasets the cost of calculating distance between new point and each existing point becomes higher.\nFinally, the KNN algorithm doesn't work well with categorical features since it is difficult to find the distance between dimensions with categorical features.","c351f69b":"The K-nearest neighbors (KNN) algorithm is a type of supervised machine learning algorithms. KNN is extremely easy to implement in its most basic form, and yet performs quite complex classification tasks. It is a lazy learning algorithm since it doesn't have a specialized training phase. Rather, it uses all of the data for training while classifying a new data point or instance. KNN is a non-parametric learning algorithm, which means that it doesn't assume anything about the underlying data. This is an extremely useful feature since most of the real world data doesn't really follow any theoretical assumption e.g. linear-separability, uniform distribution, etc.\n\nIn this article, we will see how KNN can be implemented with Python's Scikit-Learn library. But before that let's first explore the theory behind KNN and see what are some of the pros and cons of the algorithm.","60835b72":"# Implementing KNN Algorithm with Scikit-Learn\n\nIn this section, we will see how Python's Scikit-Learn library can be used to implement the KNN algorithm in less than 20 lines of code. The download and installation instructions for Scikit learn library are available at here[https:\/\/archive.ics.uci.edu\/ml\/datasets\/Iris]","8be0817e":"# K-Nearest Neighbors Algorithm in Python and Scikit-Learn","fa7b5062":"error = []\n\n# Calculating error for K values between 1 and 40\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i != y_test))"}}