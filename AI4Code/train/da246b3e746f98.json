{"cell_type":{"aae81bd9":"code","d8ec9a1e":"code","b6e13416":"code","aa474129":"code","8e166330":"code","d8e9022d":"code","c0e0fee6":"code","786c921c":"code","9cb81a0b":"code","4897dc60":"code","debec860":"code","bbeb62d6":"code","b8801ffa":"code","1b22833b":"code","3445426b":"code","1c30aac3":"code","7521e1d8":"code","a0fc4a18":"code","7cb2421c":"code","9fa3e432":"code","28883891":"code","d76bd538":"code","03e1bbde":"code","0288365a":"code","59175fa0":"code","ed4f6168":"code","f529d474":"markdown","c2ec2706":"markdown","7251d23a":"markdown","42cc21cc":"markdown","1fbaf83e":"markdown","dba27c94":"markdown","a7ffb644":"markdown","62b626a2":"markdown","74e1e920":"markdown","24c89fdb":"markdown","5f434068":"markdown","f316fd55":"markdown","aa2243d0":"markdown","8ddd715a":"markdown","f1c9b0b3":"markdown","83762548":"markdown","d81691f0":"markdown","8cc62998":"markdown","dfabe917":"markdown","1841b5ce":"markdown","05a7f987":"markdown","0a592cdd":"markdown","b2f27efa":"markdown","4a2c761f":"markdown","0d46f977":"markdown","5daaea79":"markdown","5f510a44":"markdown","5ec15056":"markdown","005a1283":"markdown","981bc7fb":"markdown","af2c909f":"markdown","35cc5be5":"markdown"},"source":{"aae81bd9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.utils import resample\n\n%matplotlib inline\npd.options.display.max_columns = 500\n\nimport warnings\nwarnings.filterwarnings('ignore')","d8ec9a1e":"df = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","b6e13416":"df.head()","aa474129":"df.info()","8e166330":"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\ndf['TotalCharges'] = df['TotalCharges'].fillna(value=0)\n\ndf['tenure'] = df['tenure'].astype('float64')","d8e9022d":"df.drop('customerID', axis=1, inplace=True)","c0e0fee6":"col_cat = df.select_dtypes(include='object').drop('Churn', axis=1).columns.tolist()\ncol_num = df.select_dtypes(exclude='object').columns.tolist()","786c921c":"for c in col_cat:\n    print('Column {} unique values: {}'.format(c, len(df[c].unique())))","9cb81a0b":"plt.figure(figsize=(20,20))\nfor i,c in enumerate(col_cat):\n    plt.subplot(5,4,i+1)\n    sns.countplot(df[c], hue=df['Churn'])\n    plt.title(c)\n    plt.xlabel('')","4897dc60":"plt.figure(figsize=(20,5))\nfor i,c in enumerate(['tenure', 'MonthlyCharges', 'TotalCharges']):\n    plt.subplot(1,3,i+1)\n    sns.distplot(df[df['Churn'] == 'No'][c], kde=True, color='blue', hist=False, kde_kws=dict(linewidth=2), label='No')\n    sns.distplot(df[df['Churn'] == 'Yes'][c], kde=True, color='Orange', hist=False, kde_kws=dict(linewidth=2), label='Yes')\n    plt.title(c)","debec860":"plt.figure(figsize=(20,5))\nfor i,c in enumerate(col_num):\n    plt.subplot(1,4,i+1)\n    sns.violinplot(x=df['Churn'], y=df[c])\n    plt.title(c)","bbeb62d6":"df.head()","b8801ffa":"dfT = pd.get_dummies(df, columns=col_cat)\ndfT.head()","1b22833b":"dfT['Churn'] = dfT['Churn'].map(lambda x: 1 if x == 'Yes' else 0)","3445426b":"plt.figure(figsize=(5, 5))\nsns.countplot(dfT['Churn'])\nplt.title('Imbalanced dataset, it seems ratio is 2:5 for Yes:No')\nplt.show()","1c30aac3":"minority = dfT[dfT.Churn==1]\nmajority = dfT[dfT.Churn==0]\n\nminority_upsample = resample(minority, replace=True, n_samples=majority.shape[0])\ndfT = pd.concat([minority_upsample, majority], axis=0)\ndfT = dfT.sample(frac=1).reset_index(drop=True)","7521e1d8":"plt.figure(figsize=(10, 5))\nplt.subplot(1,2,1)\nsns.countplot(df['Churn'])\nplt.title('Imbalanced dataset')\n\nplt.subplot(1,2,2)\nsns.countplot(dfT['Churn'])\nplt.title('Balanced dataset')\nplt.show()","a0fc4a18":"rs = RobustScaler()\ndfT['tenure'] = rs.fit_transform(dfT['tenure'].values.reshape(-1,1))\ndfT['MonthlyCharges'] = rs.fit_transform(dfT['MonthlyCharges'].values.reshape(-1,1))\ndfT['TotalCharges'] = rs.fit_transform(dfT['TotalCharges'].values.reshape(-1,1))","7cb2421c":"X_train, X_test, y_train, y_test = train_test_split(dfT.drop('Churn', axis=1).values, dfT['Churn'].values, test_size=0.2)","9fa3e432":"xg = XGBClassifier()\nxg.fit(X_train, y_train)\ny_test_hat_xg = xg.predict(X_test)","28883891":"print(classification_report(y_test, y_test_hat_xg))","d76bd538":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau","03e1bbde":"model = Sequential()\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.45))\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, verbose=1,patience=10, min_lr=0.0000000001)\nearly_stopping_cb = EarlyStopping(patience=10, restore_best_weights=True)\n\nmodel.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(x=X_train, y=y_train, batch_size=128, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping_cb, reduce_lr])\n","0288365a":"y_test_hat_tf = model.predict(X_test)","59175fa0":"y_test_hat_tf2 = [1 if x > 0.5 else 0 for x in y_test_hat_tf ]","ed4f6168":"print(classification_report(y_test, y_test_hat_tf2))","f529d474":"## Thanks for checking my notebook, if you liked it, make sure to vote for for this notebook!","c2ec2706":"Let's try also violin plot.","7251d23a":"We use sequential model with multiple dense & dropout layers.","42cc21cc":"## Balanced or imbalanced?\nCheck if our dataset is balanced or imbalanced and if any action is needed. You will find out that data are highly imbalanced, we will use resample function to upsample minority group.","1fbaf83e":"And finally checkout classification report!","dba27c94":"Drop customer ID as it's not relevant field for analysis.","a7ffb644":"That's it, feel free to post your comments ;)","62b626a2":"Take a look on distribution of Churn across all categorical variables. This is really nice view where you can see that i.e. gender is not correlated with Churn at all, but Contract is highly correlated with churn and customers having month-to-month contract are much more likely to churn, comparing to customers with 1-year and 2-years contracts. That's intresting fact and can help company to make 1 & 2 years contract more attractive!","74e1e920":"# Exploratory data analysis","24c89fdb":"## Time to scale!\nML algorithms are sensitive on data that are not normalized to same scale. You might try that deep net (at the end of kernel) will have much lower accuracy when using unscaled data... accuracy can go down even by 10%! I will use robust scaler that can nicely handle outliers, but standard scaler might work well too.","5f434068":"Now do simple label encoding of our target variable Churn.","f316fd55":"For our categorical fields, check how many unique values has each column so we will decide if feature engineering (and merging values in case there is too many of them) is needed.\nYou will see we have 2-4 unique values that is ideal.","aa2243d0":"# Data preprocessing\nWe've completed our quick and simple EDA, it's time to cook our data for taste of machine learning algorithms those like just numerical data, not text data :)","8ddd715a":"![](https:\/\/osclasspoint.com\/images\/customer-churn.png)","f1c9b0b3":"# Load libraries\nNothing extraordinary will be used - numpy, pandas, sklearn, matplotlib, seaborn, xgboost and tensorflow","83762548":"Split our fields to categorical and numerical so we can do EDA & preprocessing faster. Churn, our target variable, will not be included in categorical fields.","d81691f0":"Do just quick check how it looked like before balance and after balance.","8cc62998":"## Deep neural networks\nYes it should be fun. Using simple net? No, we will use something more complex... Let's do it!","dfabe917":"What you think? It seems xgboost is slightly better, but this net was almost catching it ;)","1841b5ce":"It seems we have no null values, that is great and save us some time, however TotalCharges seems to have incorrect format (object), fix this converting field to float (float64) and filling missing values those will be generated during conversion with 0.","05a7f987":"Not bad! We got really good precission as well as recall and f1 score! Yes you are right, I should try some hyperparameter tunning, but for now let's keep this notebook simple. You may find hyperparameter optimization in other of my kernels ;)","0a592cdd":"## XGBoost\nLet's start with popular XGB Classifier and check it's performance.","b2f27efa":"# Telco customer churn - binary classification problem\nAncient problem of machine learning - will customer churn or not? Let's do some analysis, preprocessing, feature engineering and then apply XGB Classifier & Tensorflow on our data to predict churn.","4a2c761f":"## Data Split\nSplit our data into train & test partitions. Train partition will be used to train ML model, test will be used to validate it's performance. 80% goes to train, 20% goes to test. It could be also 70:30 or 60:40.","0d46f977":"## Explore our data\nFirst chech top rows, then columns format and missing values.","5daaea79":"Checkout distribution of our numerical features. We again want to find out some interesting relations in data.\nIt seems tenure is correlated with Churn.","5f510a44":"# Modeling\nOur first try will be XGBoost. We could try Random Forest or Light GBM, but these will not lead to better results comparing to XGBoost, therefore second choice will be deep neural network consisting of multiple layers.","5ec15056":"First, do one hot encoding of our categorical features.","005a1283":"# Load data\nLoad data using pandas, we have just one dataset here that makes things easier","981bc7fb":"Model is trained, you might see it's overfitting during training, so increasing dropout would solve this problem... yes I've tried it and once I solved problem with overfitting, accuracy on test data was decreased :) ...ok, predict our test data and compare it to actuals.","af2c909f":"Output of prediction are probabilities, let's convert probabilities into 0\/1","35cc5be5":"Divide our data into 2 groups, majority (0) and minority (1) and create new dataset by upsampling minority group."}}