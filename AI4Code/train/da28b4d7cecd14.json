{"cell_type":{"939bbe2c":"code","6d1051b1":"code","50da9d43":"code","ac2b1916":"code","ae68ac68":"code","4105ae5b":"code","5d6a9e2a":"code","d7a9829e":"code","135264dd":"code","9f4b5669":"code","2cc2e43b":"code","32fd5bac":"code","a83820b5":"code","68d91d14":"code","775a3703":"code","ac52b20c":"code","3fed3470":"code","7fdf8b1a":"code","b3d94aae":"code","25cc4942":"code","56f55942":"code","98c9f992":"code","e49e4fc0":"markdown","ed4f5fe3":"markdown","0e679386":"markdown","28e1b583":"markdown","22d7fd02":"markdown","ab416927":"markdown"},"source":{"939bbe2c":"import sklearn\nimport numpy as np\nimport os\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import  DecisionTreeClassifier\niris = load_iris()\nx = iris.data[:, 2 :] # petala altura\/largura\ny = iris.target\ntree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\ntree_clf . fit ( x , y )\n","6d1051b1":"from graphviz import Source\nfrom sklearn.tree import export_graphviz\nexport_graphviz(tree_clf, \n                out_file=os.path.join(\".\/\", \"iris_tree.dot\"),\n                feature_names=iris.feature_names[2:],\n                class_names=iris.target_names,\n                rounded=True,\n            filled=True\n                )\nSource.from_file(os.path.join(\".\/\", \"iris_tree.dot\"))","50da9d43":"from matplotlib.colors import ListedColormap\ndef plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):\n    x1s = np.linspace(axes[0], axes[1], 100)\n    x2s = np.linspace(axes[2], axes[3], 100)\n    x1, x2 = np.meshgrid(x1s, x2s)\n    X_new = np.c_[x1.ravel(), x2.ravel()]\n    y_pred = clf.predict(X_new).reshape(x1.shape)\n    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n    if not iris:\n        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n    if plot_training:\n        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Setosa\")\n        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Versicolor\")\n        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\", label=\"Virginica\")\n        plt.axis(axes)\n    if iris:\n        plt.xlabel(\"Petal length\", fontsize=14)\n        plt.ylabel(\"Petal width\", fontsize=14)\n    else:\n        plt.xlabel(r\"$x_1$\", fontsize=18)\n        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n    if legend:\n        plt.legend(loc=\"lower right\", fontsize=14)\n\nplt.figure(figsize=(8, 4))\nplot_decision_boundary(tree_clf, x, y)\nplt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\nplt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\nplt.plot([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2)\nplt.plot([4.85, 4.85], [1.75, 3], \"k:\", linewidth=2)\nplt.text(1.40, 1.0, \"Depth=0\", fontsize=15)\nplt.text(3.2, 1.80, \"Depth=1\", fontsize=13)\nplt.text(4.05, 0.5, \"(Depth=2)\", fontsize=11)\n","ac2b1916":"tree_clf.predict_proba([[5, 1.5]])","ae68ac68":"tree_clf.predict([[5, 1.5]])","4105ae5b":"x[(x[:,1]==x[:,1][y==1].max()) & (y==1)]","5d6a9e2a":"not_widest_versicolor = (x[:, 1]!=1.8) | (y==2)\nx_tweaked = x[not_widest_versicolor]\ny_tweaked = y[not_widest_versicolor]\ntree_clf_tweaked = DecisionTreeClassifier(max_depth=2, random_state=40)\ntree_clf_tweaked.fit(x_tweaked, y_tweaked)\n","d7a9829e":"plt.figure(figsize=(8, 4))\nplot_decision_boundary(tree_clf_tweaked, x_tweaked, y_tweaked, legend=False)\nplt.plot([0, 7.5], [0.8, 0.8], \"k-\", linewidth=2)\nplt.plot([0, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\nplt.text(1.0, 0.9, \"Depth=0\", fontsize=15)\nplt.text(1.0, 1.80, \"Depth=1\", fontsize=13)\n","135264dd":"from sklearn.datasets import make_moons\nXm, ym = make_moons(n_samples=100, noise=0.25, random_state=53)\n\ndeep_tree_clf1 = DecisionTreeClassifier(random_state=42)\ndeep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)\ndeep_tree_clf1.fit(Xm, ym)\ndeep_tree_clf2.fit(Xm, ym)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\nplt.sca(axes[0])\nplot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-1.5, 2.4, -1, 1.5], iris=False)\nplt.title(\"No restrictions\", fontsize=16)\nplt.sca(axes[1])\nplot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-1.5, 2.4, -1, 1.5], iris=False)\nplt.title(\"min_samples_leaf = {}\".format(deep_tree_clf2.min_samples_leaf), fontsize=14)\nplt.ylabel(\"\")\n","9f4b5669":"angle = np.pi \/ 180 * 20\nrotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\nXr = x.dot(rotation_matrix)\n\ntree_clf_r = DecisionTreeClassifier(random_state=42)\ntree_clf_r.fit(Xr, y)\n\nplt.figure(figsize=(8, 3))\nplot_decision_boundary(tree_clf_r, Xr, y, axes=[0.5, 7.5, -1.0, 1], iris=False)\n","2cc2e43b":"np.random.seed(6)\nXs = np.random.rand(100, 2) - 0.5\nys = (Xs[:, 0] > 0).astype(np.float32) * 2\n\nangle = np.pi \/ 4\nrotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\nXsr = Xs.dot(rotation_matrix)\n\ntree_clf_s = DecisionTreeClassifier(random_state=42)\ntree_clf_s.fit(Xs, ys)\ntree_clf_sr = DecisionTreeClassifier(random_state=42)\ntree_clf_sr.fit(Xsr, ys)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\nplt.sca(axes[0])\nplot_decision_boundary(tree_clf_s, Xs, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False)\nplt.sca(axes[1])\nplot_decision_boundary(tree_clf_sr, Xsr, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False)\nplt.ylabel(\"\")\n","32fd5bac":"# Quadratic training set + noise\nnp.random.seed(42)\nm = 200\nx = np.random.rand(m, 1)\ny = 4 * (x - 0.5) ** 2\ny = y + np.random.randn(m, 1) \/ 10\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\ntree_reg.fit(x, y)","a83820b5":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)\ntree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3)\ntree_reg1.fit(x, y)\ntree_reg2.fit(x, y)\n\ndef plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\n    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n    y_pred = tree_reg.predict(x1)\n    plt.axis(axes)\n    plt.xlabel(\"$x_1$\", fontsize=18)\n    if ylabel:\n        plt.ylabel(ylabel, fontsize=18, rotation=0)\n    plt.plot(X, y, \"b.\")\n    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n\nfig, axes = plt.subplots(ncols=2, figsize=(13, 5), sharey=True)\nplt.sca(axes[0])\nplot_regression_predictions(tree_reg1, x, y)\nfor split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n\nplt.text(0.21, 0.65, \"Depth=0\", fontsize=15)\nplt.text(0.01, 0.2, \"Depth=1\", fontsize=13)\nplt.text(0.65, 0.8, \"Depth=1\", fontsize=13)\nplt.legend(loc=\"upper center\", fontsize=18)\nplt.title(\"max_depth=2\", fontsize=14)\n\nplt.sca(axes[1])\nplot_regression_predictions(tree_reg2, x, y, ylabel=None)\nfor split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\nfor split in (0.0458, 0.1298, 0.2873, 0.9040):\n    plt.plot([split, split], [-0.2, 1], \"k:\", linewidth=1)\nplt.text(0.3, 0.5, \"Depth=2\", fontsize=13)\nplt.title(\"max_depth=3\", fontsize=14)\n","68d91d14":"IMAGES_PATH=\".\/\"\nexport_graphviz(\n        tree_reg1,\n        out_file=os.path.join(IMAGES_PATH, \"regression_tree.dot\"),\n        feature_names=[\"x1\"],\n        rounded=True,\n        filled=True\n    )\nSource.from_file(os.path.join(\".\/\", \"regression_tree.dot\"))","775a3703":"tree_reg1 = DecisionTreeRegressor(random_state=42)\ntree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)\ntree_reg1.fit(x, y)\ntree_reg2.fit(x, y)\n\nx1 = np.linspace(0, 1, 500).reshape(-1, 1)\ny_pred1 = tree_reg1.predict(x1)\ny_pred2 = tree_reg2.predict(x1)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n\nplt.sca(axes[0])\nplt.plot(x, y, \"b.\")\nplt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\nplt.axis([0, 1, -0.2, 1.1])\nplt.xlabel(\"$x_1$\", fontsize=18)\nplt.ylabel(\"$y$\", fontsize=18, rotation=0)\nplt.legend(loc=\"upper center\", fontsize=18)\nplt.title(\"No restrictions\", fontsize=14)\n\nplt.sca(axes[1])\nplt.plot(x, y, \"b.\")\nplt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\nplt.axis([0, 1, -0.2, 1.1])\nplt.xlabel(\"$x_1$\", fontsize=18)\nplt.title(\"min_samples_leaf={}\".format(tree_reg2.min_samples_leaf), fontsize=14)\n","ac52b20c":"x[:1]","3fed3470":"heads_proba = 0.51\ncoin_tosses = (np.random.rand(10000, 10) < heads_proba).astype(np.int32)\ncumulative_heads_ratio = np.cumsum(coin_tosses, axis=0) \/ np.arange(1, 10001).reshape(-1, 1)\nplt.figure(figsize=(8,3.5))\nplt.plot(cumulative_heads_ratio)\nplt.plot([0, 10000], [0.51, 0.51], \"k--\", linewidth=2, label=\"51%\")\nplt.plot([0, 10000], [0.5, 0.5], \"k-\", label=\"50%\")\nplt.xlabel(\"Number of coin tosses\")\nplt.ylabel(\"Heads ratio\")\nplt.legend(loc=\"lower right\")\nplt.axis([0, 10000, 0.42, 0.58])","7fdf8b1a":"from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons\n\nx, y = make_moons(n_samples=500, noise=0.30, random_state=42)\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n","b3d94aae":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nlog_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\nrnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nsvm_clf = SVC(gamma=\"scale\", random_state=42)\n\nvoting_clf = VotingClassifier(\n    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n    voting='hard')\n\nvoting_clf.fit(x_train, y_train)","25cc4942":"from sklearn.metrics import accuracy_score\n\nfor clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))","56f55942":"log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\nrnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nsvm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n\nvoting_clf = VotingClassifier(\n    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n    voting='soft')\nvoting_clf.fit(x_train, y_train)\n","98c9f992":"from sklearn.metrics import accuracy_score\n\nfor clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))","e49e4fc0":"## Regress\u00e3o de \u00e1rvore","ed4f5fe3":"# Conjunto de aprendizado e random forest","0e679386":"## Estimando probabilidades de classes","28e1b583":"# \u00c1rvore de Decis\u00e3o\n","22d7fd02":"## Treinando e visualizando uma \u00e1rvore de decis\u00e3o\n\n> Indented block\n\n","ab416927":"## Voting classifiers\n"}}