{"cell_type":{"895d0752":"code","6fe0023b":"code","ad66272c":"code","30502fbb":"code","e2d6f200":"code","c12e6732":"code","51a7d997":"code","afac7487":"code","41ae880c":"code","97481eb7":"code","5247521a":"code","2db586b5":"code","9e3db427":"code","e4f78035":"code","d88d5323":"code","54411263":"code","33e3ac3a":"code","3264b54a":"code","93af04e2":"code","30789b3a":"code","65414fc3":"code","9c6961d5":"code","674fe1ea":"code","7a2c6c8c":"code","53f821a2":"code","aa447b65":"code","3c6a41d1":"code","d48f9e5b":"code","12238b1e":"code","cafeb60b":"code","f532e59c":"code","06c6e5cd":"code","5fc0f7f5":"code","78bf3128":"code","9eb698e1":"code","98b82634":"code","f6e2f888":"code","b995528c":"code","86e43149":"code","320629f5":"code","8c3aab37":"code","7e9c8564":"code","649bbc99":"code","b7650c33":"code","5b34d20c":"code","10b00c8e":"code","414649ed":"code","d3bff140":"code","7154374e":"code","217914cf":"markdown","91add6b8":"markdown","4f737ea7":"markdown","7290c30c":"markdown","fa9d970a":"markdown","54c3477e":"markdown","dd6de867":"markdown","b46df148":"markdown","90bbfc27":"markdown","af7ea8eb":"markdown","e416babd":"markdown","ebe6c837":"markdown","029a29f3":"markdown","ba51922b":"markdown","beac862b":"markdown","40dcf19b":"markdown","9d32dd4f":"markdown","dc2351e4":"markdown","6697d421":"markdown","eebe101b":"markdown","b8a679a4":"markdown","3d71bd2a":"markdown","9ea9274e":"markdown","092025c6":"markdown"},"source":{"895d0752":"import pandas as pd","6fe0023b":"trainDF = pd.read_csv('\/kaggle\/input\/adult-pmr3508\/train_data.csv', \n        names=[\n          \"Age\", \"Workclass\", \"Samp_weight\", \"Education\", \"Education-Num\", \"Martial Status\",\n          \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n          \"Hours per week\", \"Country\", \"Target\"],\n        sep=r'\\s*,\\s*',\n        engine='python',\n        na_values=\"?\")","ad66272c":"trainDF = trainDF.drop(['Id'])\ntrainDF.head()","30502fbb":"import matplotlib as mpl\n%matplotlib inline\nmpl.rcParams['figure.dpi'] = 100\n\nfrom matplotlib import pyplot as plt\n\n\nplt.figure(figsize=(18,5))\n\nfig = plt.bar(sorted(trainDF.Age.unique()), trainDF['Age'].value_counts().sort_index(), alpha=0.8)\n\n\nplt.xlabel('Age').set_color('black')\nplt.xticks(rotation=90)\n[i.set_color(\"black\") for i in plt.gca().get_xticklabels()]\n\n\nplt.ylabel('Frequency').set_color('black')\n[i.set_color(\"black\") for i in plt.gca().get_yticklabels()]\n\n\nplt.title('Age distribution').set_color('black')\nplt.margins(x=0, y=None, tight=True)","e2d6f200":"print(trainDF['Age'].mean()) # n sei oq aconteceu aki\ntrainDF.groupby(by=\"Age\").describe()","c12e6732":"print(trainDF.shape)\nclean_tDF = trainDF.dropna()\nprint(clean_tDF.shape)","51a7d997":"fig = plt.bar(clean_tDF.Workclass.unique(), clean_tDF['Workclass'].value_counts(), alpha=0.8)\n\n\nplt.xlabel('Workclass').set_color('black')\nplt.xticks(rotation=90)\n[i.set_color(\"black\") for i in plt.gca().get_xticklabels()]\n\n\nplt.ylabel('Frequency').set_color('black')\n[i.set_color(\"black\") for i in plt.gca().get_yticklabels()]\n\n\nplt.title('Workclass distribution').set_color('black')\nplt.margins(x=0, y=None, tight=True)","afac7487":"fig = plt.figure(figsize=(25,20))\n\nfor ed_num,tmpdf in clean_tDF.groupby(by='Education-Num'):\n    plt.scatter(sorted(tmpdf['Age']),sorted(tmpdf['Hours per week']),label=ed_num)\n\nplt.legend()\nplt.title('Age x Hours per Week')\nplt.xlabel('Age')\nplt.ylabel('Hours per week')","41ae880c":"clean_tDF.Target.value_counts(normalize=True).plot(kind=\"bar\")","97481eb7":"X = clean_tDF[[\"Age\", \"Education-Num\", \"Capital Gain\", \"Capital Loss\" ,\"Hours per week\"]]\nY = clean_tDF.Target","5247521a":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\n\nbest_meanScore = 0\n\n#for n in range(1,20):\n#  neigh = KNeighborsClassifier(n_neighbors=n)\n#  neigh.fit(X, Y) \n#  for folds in range(2,10):\n#    score_rf = cross_val_score(neigh, X, Y, cv=folds, scoring='accuracy').mean()\n#    if score_rf > best_meanScore:\n#      best_meanScore = score_rf\n#      best_pair = [n, folds] #best pair of n and cv\n      \n#print(best_pair)\n#best_meanScore","2db586b5":"neigh = KNeighborsClassifier(n_neighbors=14)\nneigh.fit(X, Y) \nscore_rf = cross_val_score(neigh, X, Y, cv=9, scoring='accuracy')\nscore_rf","9e3db427":"testDF =  pd.read_csv('\/kaggle\/input\/adult-pmr3508\/test_data.csv',\n        names=[\n          \"Age\", \"Workclass\", \"Samp_weight\", \"Education\", \"Education-Num\", \"Martial Status\",\n          \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n          \"Hours per week\", \"Country\"],\n        sep=r'\\s*,\\s*',\n        engine='python',\n        na_values=\"?\")","e4f78035":"testDF = testDF.drop([\"Id\"])","d88d5323":"testDF.head()","54411263":"Xtest = testDF[[\"Age\", \"Education-Num\", \"Capital Gain\", \"Capital Loss\" ,\"Hours per week\"]]","33e3ac3a":"Ypred = neigh.predict(Xtest)","3264b54a":"#Salvando as previsoes do knn\nsavepath = \"predictions_knn.csv\" \nprev = pd.DataFrame(Ypred, columns = [\"income\"]) \nprev.to_csv(savepath, index_label=\"Id\") ","93af04e2":"prev.income.value_counts(normalize=True).plot(kind=\"bar\")","30789b3a":"import seaborn as sns\ncat_attributes = trainDF.select_dtypes(include=['object'])","65414fc3":"sns.set(rc={\"font.style\":\"normal\",\n            \"text.color\":\"black\",\n            \"xtick.color\":\"black\",\n            \"ytick.color\":\"black\",\n            \"axes.labelcolor\":\"black\",\n            \"axes.grid\":False,\n            'axes.labelsize':30,\n            'figure.figsize':(22.0, 7.0)})\nsns.countplot(x='Age', hue='Target', data = cat_attributes)","9c6961d5":"sns.set(rc={\"font.style\":\"normal\",\n            \"text.color\":\"black\",\n            \"xtick.color\":\"black\",\n            \"ytick.color\":\"black\",\n            \"axes.labelcolor\":\"black\",\n            \"axes.grid\":False,\n            'axes.labelsize':30,\n            'figure.figsize':(17.0, 7.0)})\noc_plot = sns.countplot(x='Occupation', hue='Target', data = cat_attributes)\noc_plot.set_xticklabels(oc_plot.get_xticklabels(), rotation=40, ha=\"right\")","674fe1ea":"sns.set(rc={\"font.style\":\"normal\",\n            \"text.color\":\"black\",\n            \"xtick.color\":\"black\",\n            \"ytick.color\":\"black\",\n            \"axes.labelcolor\":\"black\",\n            \"axes.grid\":False,\n            'axes.labelsize':30,\n            'figure.figsize':(17.0, 7.0)})\noc_plot = sns.countplot(x='Country', hue='Target', data = cat_attributes)\noc_plot.set_xticklabels(oc_plot.get_xticklabels(), rotation=40, ha=\"right\")","7a2c6c8c":"sns.set(rc={\"font.style\":\"normal\",\n            \"text.color\":\"black\",\n            \"xtick.color\":\"black\",\n            \"ytick.color\":\"black\",\n            \"axes.labelcolor\":\"black\",\n            \"axes.grid\":False,\n            'axes.labelsize':30,\n            'figure.figsize':(17.0, 7.0)})\noc_plot = sns.countplot(x='Education-Num', hue='Target', data = cat_attributes)\noc_plot.set_xticklabels(oc_plot.get_xticklabels(), rotation=40, ha=\"right\")","53f821a2":"sns.set(rc={\"font.style\":\"normal\",\n            \"text.color\":\"black\",\n            \"xtick.color\":\"black\",\n            \"ytick.color\":\"black\",\n            \"axes.labelcolor\":\"black\",\n            \"axes.grid\":False,\n            'axes.labelsize':30,\n            'figure.figsize':(17.0, 7.0)})\noc_plot = sns.countplot(x='Workclass', hue='Target', data = cat_attributes)\noc_plot.set_xticklabels(oc_plot.get_xticklabels(), rotation=40, ha=\"right\")","aa447b65":"sns.set(rc={\"font.style\":\"normal\",\n            \"text.color\":\"black\",\n            \"xtick.color\":\"black\",\n            \"ytick.color\":\"black\",\n            \"axes.labelcolor\":\"black\",\n            \"axes.grid\":False,\n            'axes.labelsize':30,\n            'figure.figsize':(17.0, 7.0)})\noc_plot = sns.countplot(x='Sex', hue='Target', data = cat_attributes)\noc_plot.set_xticklabels(oc_plot.get_xticklabels(), rotation=40, ha=\"right\")","3c6a41d1":"categorical_subset = clean_tDF.drop(columns =['Target','Age', 'Samp_weight' , 'Education-Num', 'Capital Gain', 'Capital Loss', 'Hours per week'])\n\ncategorical_subset.columns","d48f9e5b":"# One hot encode\nrelevant_categorical_subset = pd.get_dummies(categorical_subset.drop(columns =['Country']))","12238b1e":"missing = clean_tDF[['Target','Age', 'Samp_weight' , 'Education-Num', 'Capital Gain', 'Capital Loss', 'Hours per week']]\nTurboTrainDF = pd.concat([relevant_categorical_subset,missing], axis=1)\nTurboTrainDF.shape","cafeb60b":"# I can use the same X and Y\nX = TurboTrainDF.drop(columns =['Samp_weight', 'Target'])\nY = TurboTrainDF.Target\nX.shape","f532e59c":"neigh2 = KNeighborsClassifier(n_neighbors=14)\nneigh2.fit(X, Y) \nscore_rf = cross_val_score(neigh2, X, Y, cv=9, scoring='accuracy')\nscore_rf","06c6e5cd":"#one hot on test DF\ncs_tst= testDF.drop(columns =['Age', 'Samp_weight' , 'Education-Num', 'Capital Gain', 'Capital Loss', 'Hours per week'])\nrelevant_cs_tst = pd.get_dummies(cs_tst.drop(columns =['Country']))\n\nmissing_tst = testDF[['Age', 'Samp_weight' , 'Education-Num', 'Capital Gain', 'Capital Loss', 'Hours per week']]\nTurboTestDF = pd.concat([relevant_cs_tst,missing_tst], axis=1)\n\n#for some reason 'Workclass_Never-worked' do not appear on X, so i`ll drop it in X test\nXtest = TurboTestDF.drop(columns =['Samp_weight', 'Workclass_Never-worked'])\nXtest.shape","5fc0f7f5":"Ypred_knn2 = neigh2.predict(Xtest)","78bf3128":"#Salvando as previsoes do knn\nsavepath = \"predictions_knn2.csv\" \nprev = pd.DataFrame(Ypred_knn2, columns = [\"income\"]) \nprev.to_csv(savepath, index_label=\"Id\") \nprev.income.value_counts(normalize=True).plot(kind=\"bar\")","9eb698e1":"from sklearn.linear_model import LogisticRegression","98b82634":"logR = LogisticRegression(random_state=0)\nscores = cross_val_score(logR, X, Y, cv=15, scoring='accuracy')\nscores","f6e2f888":"import sklearn as sklearn\nlogReg = sklearn.linear_model.LogisticRegression()\n\nlogReg.fit(X, Y)\ncoefs = pd.Series(logReg.coef_[0], index=X.columns)\n\ncoefs.sort_values(ascending = False)","b995528c":"coefs = pd.Series(logReg.coef_[0], index=X.columns)\ncoefs = coefs.sort_values()\nplt.subplot(1,1,1)\ncoefs.plot(kind=\"bar\")\nplt.show()\ncoefs.sort_values(ascending = False)","86e43149":"#Salvando as previsoes da Regressao logistica\nsavepath = \"predictions_logReg.csv\" \nprev = pd.DataFrame(logReg.predict(X), columns = [\"income\"]) \nprev.to_csv(savepath, index_label=\"Id\") \nprev.income.value_counts(normalize=True).plot(kind=\"bar\")","320629f5":"scores2 = cross_val_score(logR, X.drop(columns =['Age', 'Hours per week', \"Capital Gain\", \"Capital Loss\"]), Y.drop(columns =['Age', 'Hours per week', \"Capital Gain\", \"Capital Loss\"]), cv=15, scoring='accuracy')\nscores2","8c3aab37":"from sklearn.ensemble import RandomForestClassifier\nranFo = RandomForestClassifier(random_state=60)\nranFo.fit(X, Y)","7e9c8564":"accuracy_score(Y, ranFo.predict(X))","649bbc99":"cross_val_score(ranFo, X, Y, cv=5, scoring='accuracy')","b7650c33":"cross_val_score(ranFo, X, Y, cv=10, scoring='accuracy')","5b34d20c":"cross_val_score(ranFo, X, Y, cv=15, scoring='accuracy')","10b00c8e":"#Salvando as previsoes do random forest\nsavepath = \"predictions_ranFo.csv\" \nprev = pd.DataFrame(ranFo.predict(X), columns = [\"income\"]) \nprev.to_csv(savepath, index_label=\"Id\") \nprev.income.value_counts(normalize=True).plot(kind=\"bar\")","414649ed":"from time import time\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom numpy import mean as ar_mean\n\ndef train_predict(learner, sample_size, X_train, y_train, X_test): \n    results = {}\n    \n    start = time() \n    learner =  learner.fit(X_train[:sample_size], y_train[:sample_size])\n    end = time() \n    \n    results['train_time'] = end - start\n        \n    start = time() # Get start time\n    predictions_test = learner.predict(X_test)\n    predictions_train = learner.predict(X_train[:301])\n    end = time() # Get end time\n    \n    results['pred_time'] = end - start\n            \n    results['acc_train'] = accuracy_score(y_train[:301], predictions_train)\n    results['cv_avg_acc-5folds'] = ar_mean(cross_val_score(learner,  X_train[:sample_size], y_train[:sample_size], cv=5, scoring='accuracy' ))\n    results['cv_avg_acc-10folds'] = ar_mean(cross_val_score(learner,  X_train[:sample_size], y_train[:sample_size], cv=10, scoring='accuracy' ))\n    \n    results['prec_train'] = precision_score(y_train[:301], predictions_train, average=None)\n    \n    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n\n    return results","d3bff140":"s100 = int(len(X))\ns10 = int(len(X) \/ 10)\ns1 = int(len(X) \/ 100)\n\nresults = {}  \nfor clf in [neigh2, logR, ranFo]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([s1, s10, s100]):\n        results[clf_name][i] = \\\n        train_predict(clf, samples, X, Y, Xtest)\n              \nfor i in results.items():\n    print (i[0])\n    display(pd.DataFrame(i[1]).rename(columns={0:'1%', 1:'10%', 2:'100%'}))","7154374e":"s100 = int(len(X))\ns10 = int(len(X) \/ 10)\ns1 = int(len(X) \/ 100)\n\nresults = {}  \nfor clf in [neigh2, logR, ranFo]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([s1, s10, s100]):\n        results[clf_name][i] = \\\n        train_predict(clf, samples, X.drop(columns =['Age', 'Hours per week', \"Capital Gain\", \"Capital Loss\"]), Y.drop(columns =['Age', 'Hours per week', \"Capital Gain\", \"Capital Loss\"]), Xtest.drop(columns =['Age', 'Hours per week', \"Capital Gain\", \"Capital Loss\"]))\n\nfor i in results.items():\n    print (i[0])\n    display(pd.DataFrame(i[1]).rename(columns={0:'1%', 1:'10%', 2:'100%'}))","217914cf":"Aqui a proporcaose aproxima mais da proporcao da base de treino, em relacao ao knn e a logistic regression","91add6b8":"# Analise","4f737ea7":"# Treinamento Knn","7290c30c":"Devido a grande diferenca de quantidade de pessoas da amostra em cada pais, nenhuma conclusao pode ser tirada.","fa9d970a":"### Sera que seria interessante retirar essas features que aparentemente nao tem grande influencia?","54c3477e":"#### *Conclusoes 2:*\n* Ao contrario dos outros modelos, estranhamente, os tempos de predicao do Knn aumentaram, apesar da base ser menor\n* Todas as medidas de acuracia e precisao diminuiram para todos os modelos em todos os casos em comparacao com o uso das features retiradas, seguindo a tendencia vista anteriormente na regressao logistica\n","dd6de867":"**Por curiosidade, vou testar retirando as features que tiveram menos influencia na lin reg:**","b46df148":"# Conclusoes e comparacoes","90bbfc27":"### Podemos facilmente ver a incluencia de cada coluna(parametro) no y previsto:","af7ea8eb":"Nos testes de acuracia, tudo indica que retirar essas features piora o modelo","e416babd":"* Claramente podemos ver claramente que a distribuicao de quem ganha mais que 50k se aproxima de uma normal platicurtica, se concentrando entre 30 e 60 anos. \n* Aqueles que recebem menos de 50k tem uma concentracao entre 18 e 40 anos, se aproximando de uma normla com obliquidade negativa.\n* Tambem ve-se que as maiores proporcoes de pessoas que recebem >=50k entre 44 e 54 anos.","ebe6c837":"Vamos usar somente as categorias que apresentaram relevancia na diferenciacao entre <=50k e >=50k","029a29f3":"Nao esta clara a diferenca entre as proporcoes do knn sem o uso das variaveis categoricas e com o uso das mesmas, apesar de haver um aumento na acuracia no cv do treino","ba51922b":"## Conclusoes:\n* Claramente a regressao logistica tem maior velocidade, apesar de que a random forest se compara a regressao logistica, enquanto o Knn tem um tempo muito superior\n* Ela tambem tem maior interpretabilidade, pois conseguimos ver a influencia de cada parametro para a previsao\n* Tanto a randomForest quanto a logistic Regression perdem acuracie e precisao a medida que se aumenta o uso da base\n* A random forest mantem a maior precisao e acuracia em todos os cenarios\n* Somente A random forest tem uma perda consideravel de acuracia com o uso da validacao cruzada","beac862b":"Aqui a proporcaose aproxima mais da proporcao da base de treino, em relacao ao knn","40dcf19b":"O Knn feito no EP1 preve uma proporcao de <=50k maior que a proporcao vista na base de treino, o que pode ser um indicio de que existe margem para melhora","9d32dd4f":"# Random Forest","dc2351e4":"Ao contrario de outros modelos, nao ve-se um aumento claro nas medias das cross-validations da random forest","6697d421":"* Conclui-se que as maiores proporcoes de >=50k estao nas ocupacoes de: exec-managerial e prof-specialty, enquanto as menores se encontram em: other services e priv-house-serv","eebe101b":"# Regressao Logistica","b8a679a4":"# Knn com Colunas categoricas","3d71bd2a":"* ***Observou-se que, seguindo a intuicao da analise exploratoria, temos que o fator \"sexo feminino\" tem grande influencia na renda, fazendo com que as pessoas que sao do sexo feminino, tenham maior chance de pertencer ao grupo <=50k ***\n* Por outro lado, vemos que a idade e o numero de horas trabalhadas por semana, ao contrario da intuicao da analise, nao tem grande relevancia para a predicao do modelo","9ea9274e":"* Pelo grafico, fica evidente que a proporcao de homens que ganham >=50k se mostra maior que a de mulheres que ganham >=50k","092025c6":"# Analise mais profunda"}}