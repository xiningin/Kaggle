{"cell_type":{"f7649a7e":"code","7ee11c99":"code","3280b656":"code","c2c3635d":"code","a442f2aa":"code","951d33cd":"code","b497fb92":"code","9587358e":"code","71c3981b":"code","6a2497c4":"code","565f0f93":"code","963bb5ff":"code","1bf27f84":"code","91ec3083":"code","9f27553a":"code","37aadba6":"code","ea73321c":"code","9453b07c":"code","b6743234":"code","443b6a3c":"code","61d49ae3":"code","34d2d618":"code","784d79af":"code","4016ef37":"code","1b228f1d":"code","24ba710d":"code","98bd6437":"code","094bef0c":"code","fc87702e":"code","c5572de0":"code","8fffedab":"code","33e12ae8":"code","4d12dceb":"code","8a43ffd0":"code","eadde073":"code","98113fca":"code","09467466":"code","d62d4e65":"code","844cc4d1":"code","07a7f737":"code","29fe810f":"code","072f4860":"code","9fe29be0":"markdown","c0a468db":"markdown"},"source":{"f7649a7e":"import os\nimport numpy as np\nimport pandas as pd\n\ndatadir = '..\/input\/shopee-code-league-20\/_DS_Title_Translation'\n\ndev_en = pd.read_csv(os.path.join(datadir, 'dev_en.csv'))\ndev_en.head()","7ee11c99":"dev_tcn = pd.read_csv(os.path.join(datadir,'dev_tcn.csv'))\ndev_tcn.head()","3280b656":"import string\nimport re\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, RepeatVector\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom keras import optimizers\nimport matplotlib.pyplot as plt\nimport jieba","c2c3635d":"en_tcn = pd.concat([dev_en['translation_output'], dev_tcn['text']], axis=1).values\nen_tcn","a442f2aa":"# Remove punctuation, symbols, any non words\nen_tcn[:,0] = [re.sub(r'[^\\w]', ' ', s) for s in en_tcn[:,0]]\nen_tcn[:,1] = [re.sub(r'[^\\w]', ' ', s) for s in en_tcn[:,1]]\nen_tcn","951d33cd":"# convert text to lowercase\nfor i in range(len(en_tcn)):\n    en_tcn[i,0] = en_tcn[i,0].lower()\n    en_tcn[i,1] = en_tcn[i,1].lower()\nen_tcn","b497fb92":"## Cut with jieba before tokenize with keras tokenizer, remove extra spaces\nen_tcn[:,0] = [re.sub(' +', ' ', s) for s in en_tcn[:,0]]\nen_tcn[:,1] = [re.sub(' +', ' ', \" \".join(jieba.cut(s, cut_all=False))) for s in en_tcn[:,1]]\nen_tcn","9587358e":"# empty lists\neng_l = []\ntcn_l = []\n\n# populate the lists with sentence lengths\nfor i in en_tcn[:,0]:\n      eng_l.append(len(i.split()))\n\nfor i in en_tcn[:,1]:\n      tcn_l.append(len(i.split()))\n\nlength_df = pd.DataFrame({'eng':eng_l, 'tcn':tcn_l})\n\nlength_df.hist(bins = 30)\nplt.show()","71c3981b":"max(eng_l), max(tcn_l)","6a2497c4":"# function to build a tokenizer\ndef tokenization(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer","565f0f93":"# prepare english tokenizer\neng_tokenizer = tokenization(en_tcn[:, 0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\n\neng_length = max(eng_l)\nprint('English Vocabulary Size: %d' % eng_vocab_size)","963bb5ff":"# prepare Chinese tokenizer\ntcn_tokenizer = tokenization(en_tcn[:, 1])\ntcn_vocab_size = len(tcn_tokenizer.word_index) + 1\n\ntcn_length = max(tcn_l)\nprint('Chineese Vocabulary Size: %d' % tcn_vocab_size)","1bf27f84":"# encode and pad sequences\ndef encode_sequences(tokenizer, length, lines):\n    # integer encode sequences\n    seq = tokenizer.texts_to_sequences(lines)\n    # pad sequences with 0 values\n    seq = pad_sequences(seq, maxlen=length, padding='post')\n    return seq","91ec3083":"from sklearn.model_selection import train_test_split\n\n# split data into train and test set\ntrain, test = train_test_split(en_tcn, test_size=0.2, random_state = 12)","9f27553a":"# prepare training data\ntrainX = encode_sequences(tcn_tokenizer, tcn_length, train[:, 1])\ntrainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n\n# prepare validation data\ntestX = encode_sequences(tcn_tokenizer, tcn_length, test[:, 1])\ntestY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])","37aadba6":"# build NMT model\ndef define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n    model = Sequential()\n    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n    model.add(LSTM(units))\n    model.add(RepeatVector(out_timesteps))\n    model.add(LSTM(units, return_sequences=True))\n    model.add(Dense(out_vocab, activation='softmax'))\n    return model","ea73321c":"# model compilation\nmodel = define_model(tcn_vocab_size, eng_vocab_size, tcn_length, eng_length, 512)","9453b07c":"rms = optimizers.RMSprop(lr=0.001)\nmodel.compile(optimizer=rms, loss='sparse_categorical_crossentropy')","b6743234":"filename = 'model.h1.31_jul_20'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\n# train model\nhistory = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n                    verbose=1)","443b6a3c":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['train','validation'])\nplt.show()","61d49ae3":"model = load_model('model.h1.31_jul_20')\npreds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))","34d2d618":"def get_word(n, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == n:\n            return word\n    return None","784d79af":"preds_text = []\nfor i in preds:\n    temp = []\n    for j in range(len(i)):\n        t = get_word(i[j], eng_tokenizer)\n        if j > 0:\n            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n                 temp.append('')\n            else:\n                 temp.append(t)\n        else:\n            if(t == None):\n                  temp.append('')\n            else:\n                  temp.append(t) \n\n    preds_text.append(' '.join(temp))","4016ef37":"pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})\npred_df.sample(10)","1b228f1d":"!pip install -q sacrebleu","24ba710d":"# product_title_translation_eval_script.py\n\"\"\"Sample evaluation script for product title translation.\"\"\"\nfrom typing import List\nimport regex\n# !pip install sacrebleu\nfrom sacrebleu import corpus_bleu\n\nOTHERS_PATTERN: re.Pattern = regex.compile(r'\\p{So}')\n\n\ndef eval(preds: List[str], refs: List[str]) -> float:\n    \"\"\"BLEU score computation.\n\n    Strips all characters belonging to the unicode category \"So\".\n    Tokenize with standard WMT \"13a\" tokenizer.\n    Compute 4-BLEU.\n\n    Args:\n        preds (List[str]): List of translated texts.\n        refs (List[str]): List of target reference texts.\n    \"\"\"\n    preds = [OTHERS_PATTERN.sub(' ', text) for text in preds]\n    refs = [OTHERS_PATTERN.sub(' ', text) for text in refs]\n    return corpus_bleu(\n        preds, [refs],\n        lowercase=True,\n        tokenize='13a',\n        use_effective_order= False\n    ).score","98bd6437":"eval(pred_df['actual'], pred_df['predicted'])","094bef0c":"test_data = pd.read_csv(os.path.join(datadir,'test_tcn.csv'))\ntest_data.head()","fc87702e":"test_data.shape","c5572de0":"def process_test_data(arr):\n    ## Remove punctuation, symbols, extra spaces\n    arr = arr.apply(lambda x: re.sub(r'[^\\w]', ' ', x))\n    arr = arr.apply(lambda x: re.sub(' +', ' ', \" \".join(jieba.cut(x, cut_all=False))))\n    ## To lowercase\n    arr = arr.apply(lambda x: x.lower())\n    ## Get max length\n    tcn_l = []\n    for i in arr:\n        tcn_l.append(len(i.split()))\n    tcn_length = max(tcn_l)\n    \n    return arr, tcn_length","8fffedab":"tcn_arr, tcn_length_test = process_test_data(test_data['text'])","33e12ae8":"print(tcn_arr.head())\nprint(tcn_length_test)","4d12dceb":"# prepare test data\ntestX_test = encode_sequences(tcn_tokenizer, tcn_length_test, tcn_arr)","8a43ffd0":"preds = model.predict_classes(testX_test.reshape((testX_test.shape[0],testX_test.shape[1])))\npreds","eadde073":"preds.shape","98113fca":"preds_text = []\nfor i in preds:\n    temp = []\n    for j in range(len(i)):\n        t = get_word(i[j], eng_tokenizer)\n        if j > 0:\n            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n                 temp.append('')\n            else:\n                 temp.append(t)\n        else:\n            if(t == None):\n                  temp.append('')\n            else:\n                  temp.append(t) \n\n    preds_text.append(' '.join(temp))","09467466":"res = pd.DataFrame({\"translation_output\": preds_text})\nres.shape","d62d4e65":"str_zero = res['translation_output'].value_counts().index[0]\nstr_zero\nstr_most = 'baby'","844cc4d1":"res['translation_output'] = res['translation_output'].apply(lambda x: \"baby\" if x == str_zero else x)\nres['translation_output'].value_counts()","07a7f737":"res.to_csv(\"submission.csv\", index=False)","29fe810f":"pd.read_csv(\".\/submission.csv\").shape","072f4860":"!head submission.csv","9fe29be0":"### Credits to [Bahy](https:\/\/www.kaggle.com\/bahyhelmihp\/kudo-bahy-title-translation)","c0a468db":"## Use all val data to predict test data"}}