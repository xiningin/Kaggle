{"cell_type":{"416fb74e":"code","2df070bc":"code","222cb7da":"code","b91fedd1":"code","15ab5864":"code","a4269e89":"code","70a36f1c":"code","abd0e4d9":"code","1f6b14ba":"code","32681238":"code","1ab56e72":"code","49acf6f0":"code","ed372604":"code","64173ae8":"code","e900a037":"code","967654d3":"code","9725026d":"code","876ba917":"code","63740b0c":"code","932dba2a":"code","5487b303":"code","1eb9f72b":"code","92242d04":"code","fc5bfdbc":"markdown","2f2231f8":"markdown","cf83321d":"markdown","9fb6b56b":"markdown"},"source":{"416fb74e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2df070bc":"# libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA, IncrementalPCA","222cb7da":"sample_submission = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\ntest_data = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntraining_data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")","b91fedd1":"training_data.shape","15ab5864":"training_data.info()","a4269e89":"training_data.head()","70a36f1c":"training_data.max().sort_values()","abd0e4d9":"training_data.isna().sum().sort_values(ascending=False)","1f6b14ba":"training_data.duplicated().sum()","32681238":"training_data.columns","1ab56e72":"count_table = training_data.label.value_counts()\ncount_table = count_table.reset_index().sort_values(by='index')\ncount_table","49acf6f0":"plt.figure(figsize=(10, 5))\nsns.barplot(x='index', y='label', data=count_table)","ed372604":"digit_means = training_data.groupby('label').mean()\ndigit_means.head()\nplt.figure(figsize=(18, 10))\nsns.heatmap(digit_means)","64173ae8":"# average feature values\nround(training_data.drop('label', axis=1).mean(), 2).sort_values()","e900a037":"# splitting into X and y\nX = training_data.drop(\"label\", axis = 1)\ny = training_data['label']","967654d3":"# scaling the features\nX_scaled = scale(X)\n\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.3, random_state = 101)","9725026d":"# applying PCA to find number of Principal components to use\npca = PCA(svd_solver='randomized', random_state=42)\npca.fit(X_train)","876ba917":"#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","63740b0c":"pca = IncrementalPCA(n_components=400)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","932dba2a":"X_train.shape","5487b303":"# model with optimal hyperparameters\n\n# model\nmodel = SVC(C=10, gamma = 0.001, kernel=\"rbf\")\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# metrics\nprint(\"accuracy\", metrics.accuracy_score(y_test, y_pred), \"\\n\")","1eb9f72b":"# scaling test data\n# splitting into X and y\nX_test_data = test_data\nX_test_data = scale(X_test_data)\nX_test_data = pca.transform(X_test_data)\ny_test_pred = model.predict(X_test_data)\ny_test_pred","92242d04":"output = pd.DataFrame({\"ImageId\": i+1 , \"Label\": y_test_pred[i]} for i in range(0, X_test_data.shape[0]))\noutput.to_csv('submission.csv', index=False)","fc5bfdbc":"### Building and Evaluating the Final Model\n\nLet's now build and evaluate the final model, i.e. the model with highest test accuracy.","2f2231f8":"There are no duplicated rows in the dataframe","cf83321d":"## Handwritten Digit Recognition\n\n#### Problem Statement\nA classic problem in the field of pattern recognition is that of handwritten digit recognition. Suppose that you have images of handwritten digits ranging from 0-9 written by various people in boxes of a specific size - similar to the application forms in banks and universities.\n\n \n\nThe goal is to develop a model that can correctly identify the digit (between 0-9) written in an image. \n\n#### Objective\nYou are required to develop a model using Support Vector Machine which should correctly classify the handwritten digits from 0-9 based on the pixel values given as features. Thus, this is a 10-class classification problem. \n\n \n\n#### Data Description\nFor this problem, we use the MNIST data which is a large database of handwritten digits. The 'pixel values' of each digit (image) comprise the features, and the actual number between 0-9 is the label. \n\n \n\nSince each image is of 28 x 28 pixels, and each pixel forms a feature, there are 784 features. MNIST digit recognition is a well-studied problem in the ML community, and people have trained numerous models (Neural Networks, SVMs, boosted trees etc.) achieving error rates as low as 0.23% (i.e. accuracy = 99.77%, with a convolutional neural network).\n","9fb6b56b":"It has 42000 rows and 785 columns (features)"}}