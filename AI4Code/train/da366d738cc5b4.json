{"cell_type":{"e1a2b7bf":"code","d411d15f":"code","688a0800":"code","1dfb753b":"code","769d2b19":"code","12a46672":"code","0d177a3f":"code","f7d542c1":"code","4519ee64":"code","6c614d2b":"code","f88787b1":"code","a0397f3d":"code","339f918a":"code","e13500df":"code","88ee3bf9":"markdown","99ed4e78":"markdown","9d67b0e9":"markdown"},"source":{"e1a2b7bf":"!pip install feature_engine","d411d15f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom feature_engine.imputation import AddMissingIndicator\nfrom feature_engine.encoding import MeanEncoder, RareLabelEncoder\nfrom sklearn.pipeline import Pipeline\n\nfrom lightgbm import LGBMRegressor\nimport shap\nrandom_state = 123","688a0800":"train = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/train.parquet')\ntest = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/test.parquet')\nsub = pd.read_csv('..\/input\/kaggle-pog-series-s01e01\/sample_submission.csv')","1dfb753b":"train.isnull().sum()","769d2b19":"# categorical features with missing values\ncategorical_nan = [feature for feature in train.columns if train[feature].isna().sum()>0 and train[feature].dtypes=='O']\nprint(categorical_nan)","12a46672":"# replacing missing values in categorical features\nfor feature in categorical_nan:\n    train[feature] = train[feature].fillna('None')","0d177a3f":"# Lets first handle numerical features with nan value\nnumerical_nan = [feature for feature in train.columns if train[feature].isna().sum()>1 and train[feature].dtypes!='O']\nnumerical_nan","f7d542c1":"## Replacing the numerical Missing Values\n\nfor feature in numerical_nan:\n    ## We will replace by using median since there are outliers\n    median_value=train[feature].median()\n    \n    train[feature].fillna(median_value,inplace=True)\n    \ntrain[numerical_nan].isnull().sum()","4519ee64":"cols_to_drop=['publishedAt', 'trending_date']\ntrain=train.drop(cols_to_drop,axis=1)\ntrain.columns","6c614d2b":"#Code by Diego Risco https:\/\/www.kaggle.com\/ggxgboostgg\/wids-2022-shap-values-validation\/notebook\n\ncat_var = train.select_dtypes(include=[object]).columns.to_list()\ncat_var.append('categoryId')\nx_train = train.drop(['target'], axis = 1)\ny_train = train.copy()['target']","f88787b1":"#Code by Diego Risco https:\/\/www.kaggle.com\/ggxgboostgg\/wids-2022-shap-values-validation\/notebook\n\naddBinary_imputer = AddMissingIndicator()\nrare_encoder = RareLabelEncoder(tol=0.02, n_categories=2, variables=cat_var,\n                           replace_with=-999, ignore_format = True)\nmean_encoder = MeanEncoder(variables=cat_var, ignore_format = True)\n\npipe = Pipeline([('indicator', addBinary_imputer),\n                 ('RareLabelEncoder1', rare_encoder),\n                 ('MeanEncoder', mean_encoder)])","a0397f3d":"lgbm_model = LGBMRegressor(random_state = random_state)","339f918a":"x_train_processed = pipe.fit_transform(x_train, y_train)\nlgbm_model.fit(x_train_processed,y_train)","e13500df":"#Code by Diego Risco https:\/\/www.kaggle.com\/ggxgboostgg\/wids-2022-shap-values-validation\/notebook\n\nshap_values = shap.TreeExplainer(lgbm_model).shap_values(x_train_processed)\nshap.summary_plot(shap_values, x_train_processed)","88ee3bf9":"\"The shap values are presented to observate the most relevant features to see if we need to make some temporal cross validation to train our models\"","99ed4e78":"#I know that when I dropped the cols below I cheated with the code. Though I was so frustrated with my last attempt that I had to do something. I promise I'll learn how to deal with this datetime (publishAt) that revealed to be a pain in my ass. ","9d67b0e9":"#Acknowledgement\n\nDiego Risco https:\/\/www.kaggle.com\/ggxgboostgg\/wids-2022-shap-values-validation\/notebook"}}