{"cell_type":{"ad9485bf":"code","2153100d":"code","302e4d10":"code","2f331f34":"code","cbaeb3ed":"code","4b8d9044":"code","324a421e":"code","dd79d082":"code","34f47cf5":"code","87656082":"code","6aebb064":"code","9708cbd7":"code","03c6feca":"code","9d877e69":"code","89e109ab":"code","aad0e9d3":"markdown"},"source":{"ad9485bf":"import pandas as pd\ndata_train = pd.read_csv(\"train.csv\")\ndata_test  = pd.read_csv(\"test.csv\")\ndata_all   = pd.concat([data_train,data_test]).reset_index(drop=True)\nprint('Train Data Size =',np.shape(data_train))\nprint('Test Data Size  =',np.shape(data_test))\nprint('Full Data Size  =',np.shape(data_all))\nprint( data_all.isnull().sum()) # Age, Cabin & Embarked are NULL\ndata_all.head(3)","2153100d":"new1= pd.DataFrame(data_all.Name.str.split(',').tolist(),columns=list('AB'))\nnew1 = pd.DataFrame(new1.B.str.split().tolist())\ndata_all ['Title']= new1.iloc[:, 0]\ndata_all.head(3)","302e4d10":"print( data_all.isnull().sum()) \ndata_all['Age']   = data_all.groupby(['Pclass','Sex','SibSp'])['Age'].apply(lambda x: x.fillna(x.median()))\ndata_all['Fare']  = data_all.groupby(['Pclass','Sex','SibSp'])['Fare'].apply(lambda x: x.fillna(x.median()))\ndata_all['Deck']  = data_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ndata_all.loc[data_all['Embarked'].isnull() ,'Embarked']='M'\nprint('=========================')\nprint( data_all.isnull().sum())","2f331f34":"data_all['AgeCat'] = ''\ndata_all['AgeCat'].loc[(data_all['Age'] < 16)                    ] = 'young'\ndata_all['AgeCat'].loc[(data_all['Age'] >= 16) & (data_all['Age'] < 50)] = 'mature'\ndata_all['AgeCat'].loc[(data_all['Age'] >= 50)                   ] = 'senior'\n\ndata_all['FamilySize'] = ''\ndata_all['FamilySize'].loc[(data_all['SibSp'] <= 2)                     ] = 'small'\ndata_all['FamilySize'].loc[(data_all['SibSp'] > 2) & (data_all['SibSp'] <= 5 )] = 'medium'\ndata_all['FamilySize'].loc[(data_all['SibSp'] > 5)                      ] = 'large'\n\ndata_all['IsAlone'] = ''\ndata_all['IsAlone'].loc[((data_all['SibSp'] + data_all['Parch']) > 0)] = '0'\ndata_all['IsAlone'].loc[((data_all['SibSp'] + data_all['Parch']) == 0)] = '1'\n\ndata_all['SexCat'] = ''\ndata_all['SexCat'].loc[(data_all['Sex'] == 'male'  ) & (data_all['Age'] <  50)] = 'youngmale'\ndata_all['SexCat'].loc[(data_all['Sex'] == 'male'  ) & (data_all['Age'] >= 50)] = 'seniormale'\ndata_all['SexCat'].loc[(data_all['Sex'] == 'female') & (data_all['Age'] <  50)] = 'youngfemale'\ndata_all['SexCat'].loc[(data_all['Sex'] == 'female') & (data_all['Age'] >= 50)] = 'seniorfemale'\n\n#categorical_columns = ['Pclass', 'Sex', 'Embarked', 'AgeCat', 'FamilySize', 'IsAlone', 'SexCat']\n#for col in categorical_columns:\n#    data_all[col] = data_all[col].astype('category')\ndata_all.drop(['SibSp','Parch','Age', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ndata_all.head(3)","cbaeb3ed":"from sklearn.preprocessing import LabelEncoder\nle_Sex               = LabelEncoder()\nle_Embarked          = LabelEncoder()\nle_SexCat            = LabelEncoder()\nle_FamilySize        = LabelEncoder()\nle_AgeCat            = LabelEncoder()\nle_Title             = LabelEncoder()\nle_Deck              = LabelEncoder()\ndata_all['le_Deck']        = le_Sex.fit_transform(data_all.Deck)\ndata_all['le_Title']       = le_Sex.fit_transform(data_all.Title)\ndata_all['le_Sex']         = le_Sex.fit_transform(data_all.Sex)\ndata_all['le_Embarked' ]   = le_Embarked.fit_transform(data_all.Embarked)\ndata_all['le_SexCat' ]     = le_SexCat.fit_transform(data_all.SexCat)\ndata_all['le_FamilySize' ] = le_FamilySize.fit_transform(data_all.FamilySize)\ndata_all['le_AgeCat' ]     = le_AgeCat.fit_transform(data_all.AgeCat)\ndata_all.drop(['Deck', 'Title', 'Sex','Embarked','SexCat','FamilySize','AgeCat'], axis=1, inplace=True)\ndata_all.head(3)","4b8d9044":"data_train=data_all.loc[:890]\ndata_train.drop(['PassengerId'], axis=1, inplace=True)\ndata_test=data_all.loc[891:]\nprint('Train Data Size =',np.shape(data_train))\nprint('Test Data Size  =',np.shape(data_test))","324a421e":"data_train.head(3)","dd79d082":"from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\n\nX, y = data_train.iloc[:, 1:].values, data_train.iloc[:, 0].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\nX_train_norm = mms.fit_transform(X_train)\nX_test_norm  = mms.transform(X_test)","34f47cf5":"# SBS Algorithm \nfrom sklearn.base import clone\nfrom itertools import combinations\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nclass SBS():\n    def __init__(self, estimator, k_features, scoring=accuracy_score, test_size=0.25, random_state=1):\n        self.scoring = scoring\n        self.estimator = clone(estimator)\n        self.k_features = k_features\n        self.test_size = test_size\n        self.random_state = random_state\n    \n    def fit(self, X, y):\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state)\n            dim = X_train.shape[1]\n            self.indices_ = tuple(range(dim))\n            self.subsets_ = [self.indices_]\n            score = self._calc_score(X_train, y_train,\n            X_test, y_test, self.indices_)\n            self.scores_ = [score]\n            while dim > self.k_features:\n                scores = []\n                subsets = []\n                for p in combinations(self.indices_, r=dim - 1):\n                    score = self._calc_score(X_train, y_train, X_test, y_test, p)\n                    scores.append(score)\n                    subsets.append(p)\n                    best = np.argmax(scores)\n                    self.indices_ = subsets[best]\n                    self.subsets_.append(self.indices_)\n                    dim -= 1\n                    self.scores_.append(scores[best])\n                    self.k_score_ = self.scores_[-1]\n            return self\n    \n    def transform(self, X):\n        return X[:, self.indices_]\n    \n    def _calc_score(self, X_train, y_train, X_test, y_test,indices):\n        self.estimator.fit(X_train[:, indices], y_train)\n        y_pred = self.estimator.predict(X_test[:, indices])\n        score = self.scoring(y_test, y_pred)\n        return score","87656082":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\n#LOGISTIC REGRESSION\nlr = LogisticRegression(penalty='l2', C=100.0, solver='liblinear')\nlr.fit(X_train_norm, y_train)\ny_train_pred = lr.predict(X_train_norm)\ny_test_pred = lr.predict(X_test_norm)\nlr_train = accuracy_score(y_train, y_train_pred)\nlr_test = accuracy_score(y_test, y_test_pred)\n# DECISION TREE CLASSIFIER\ntree = DecisionTreeClassifier(criterion='entropy',random_state=1,max_depth=None)\ntree = tree.fit(X_train_norm, y_train)\ny_train_pred = tree.predict(X_train_norm)\ny_test_pred = tree.predict(X_test_norm)\ntree_train = accuracy_score(y_train, y_train_pred)\ntree_test = accuracy_score(y_test, y_test_pred)\n# BAGGING CLASSIFIER\nbag = BaggingClassifier(base_estimator=tree,n_estimators=500,max_samples=1.0,max_features=1.0,bootstrap=True,\n                        bootstrap_features=False,n_jobs=1,random_state=1)\nbag = bag.fit(X_train_norm, y_train)\ny_train_pred = bag.predict(X_train_norm)\ny_test_pred = bag.predict(X_test_norm)\nbag_train = accuracy_score(y_train, y_train_pred)\nbag_test = accuracy_score(y_test, y_test_pred)\n#ADABOOST CLASSIFIER\nada = AdaBoostClassifier(base_estimator=tree, n_estimators=500, learning_rate=0.1, random_state=1)\nada = ada.fit(X_train_norm, y_train)\ny_train_pred = ada.predict(X_train_norm)\ny_test_pred = ada.predict(X_test_norm)\nada_train = accuracy_score(y_train, y_train_pred)\nada_test = accuracy_score(y_test, y_test_pred)\n#KNN CLASSIFIER\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nsbs = SBS(knn, k_features=1).fit(X_train_norm, y_train)\nsbs.fit(X_train_norm, y_train)\nk3 = list(sbs.subsets_[8])\nknn.fit(X_train_norm[:, k3], y_train)\nknn_train=knn.score(X_train_norm[:, k3], y_train)\nknn_test=knn.score(X_test_norm[:, k3], y_test)\n#RANDOM FOREST CLASSIFIER\nfrom sklearn.ensemble import RandomForestClassifier\nfeat_labels = df.columns[1:]\nforest = RandomForestClassifier(n_estimators=500, random_state=1, criterion='entropy')\nforest.fit(X_train_norm, y_train)\nforest_train=forest.score(X_train_norm, y_train)\nforest_test=forest.score(X_test_norm, y_test)","6aebb064":"print('                                            Train Accuracy  Test Accuracy')\nprint('Logistic Regression                       : %.3f            %.3f' % (lr_train, lr_test))\nprint('KNN train\/test accuracies                 : %.3f            %.3f' % (knn_train, knn_test))\nprint('AdaBoost train\/test accuracies            : %.3f            %.3f' % (ada_train, ada_test))\nprint('Bagging train\/test accuracies             : %.3f            %.3f' % (bag_train, bag_test))\nprint('Decision Tree train\/test accuracies       : %.3f            %.3f' % (tree_train, tree_test))\nprint('Random Forest train\/test accuracies       : %.3f            %.3f' % (forest_train, forest_test))","9708cbd7":"print('Test Data Size  =',np.shape(data_test))\ndata_test.head()","03c6feca":"#from sklearn.preprocessing import MinMaxScaler\nXt = data_test.iloc[:, 2:].values\nmms = MinMaxScaler()\nXt_test_norm = mms.fit_transform(Xt)\nprint('Test Data Size  =',np.shape(Xt))","9d877e69":"lr_y_test_pred = lr.predict(Xt_test_norm)\ndt_y_test_pred = tree.predict(Xt_test_norm)\nbag_y_test_pred = bag.predict(Xt_test_norm)\nada_y_test_pred = ada.predict(Xt_test_norm)\nforest_test_pred=forest.predict(Xt_test_norm)","89e109ab":"data_test['Survived'] = forest_test_pred\ndata_test.drop(['Pclass','Fare','IsAlone','le_Deck','le_Title','le_Sex','le_Embarked','le_SexCat','le_FamilySize','le_AgeCat'], axis=1, inplace=True)\ndata_test.reset_index(drop=True)\nfinal_data = data_test.to_csv('Titanic_Result.csv', index = True)\ndata_test.head()","aad0e9d3":"# TEST RESULT"}}