{"cell_type":{"a074d70d":"code","341236ae":"code","e4d6756e":"code","23e2c867":"code","58f5092b":"code","5eda3913":"code","340dda53":"code","69bd6eac":"code","146fd718":"code","3db45be1":"code","6ea92469":"code","f9a41b2e":"code","24fcb42a":"code","628152db":"code","8cc2b383":"code","fab1b7a9":"code","761d97bb":"code","948178fb":"code","5d19d7c7":"code","a419b5a8":"code","aee23d29":"code","ce5ffc0e":"code","b57fd725":"code","240ea0d8":"code","419c8f13":"code","0311aef0":"code","8314506f":"code","2468a5ba":"code","181c366a":"code","3115ea3a":"code","37045d13":"code","ea708955":"code","ff8bd5d4":"code","39e6d289":"code","7cd13bc6":"code","ebe13925":"code","9316ba37":"code","edf7d2bf":"code","6920df49":"code","bd27ce95":"code","2978bbcd":"code","7ab19f8a":"code","473055fe":"code","9ac26f61":"code","eb7cf077":"code","7dfa64f6":"code","c4760a09":"code","46f7a461":"code","578d32b2":"code","a1a5e034":"code","8e1345cb":"code","38dc7d57":"code","2f01dd98":"code","ff4b058d":"code","e12ed4e0":"code","db099543":"code","51321470":"code","9f817785":"code","60b29d6d":"code","86ff9b1a":"code","1c66f71c":"code","330046e3":"code","52ba7e5e":"code","5db46c44":"code","1dac66ca":"code","ce504a44":"code","5f2ccfa4":"code","a1b7f699":"code","549355f7":"code","718e188f":"code","7112f750":"code","f625891b":"code","8e3a27c9":"code","f83445bc":"code","a0b50e5f":"code","b8f6badc":"code","104ac699":"code","a42ec3ef":"code","f62de74e":"markdown","53bd9a42":"markdown","94058774":"markdown","7b8fc449":"markdown","9314c05f":"markdown","d8774df1":"markdown","9a2abcb4":"markdown","85d36a27":"markdown","7bc6e7ad":"markdown","0a94520f":"markdown","eb0ec444":"markdown","fcd18cda":"markdown","0606060e":"markdown","5185d32e":"markdown","9751cad8":"markdown","6b86f547":"markdown","a078b65f":"markdown","809ed4bd":"markdown","ba1fb65b":"markdown","aad48ae1":"markdown","b79d22ed":"markdown","ab031c75":"markdown","284df17f":"markdown","f8de6a89":"markdown","be1bc66f":"markdown","ab8eb816":"markdown","5b3cb50f":"markdown","bebf899d":"markdown","41cdb76b":"markdown","4dc707ea":"markdown","7c7779e3":"markdown","df6afa1c":"markdown","a7ca185f":"markdown","1af3c482":"markdown","0e0c9f79":"markdown","4a1ba711":"markdown","b0bd60ac":"markdown","303be3a7":"markdown","77e93ee5":"markdown","02c0d296":"markdown","d9bbeda8":"markdown","04da4d0a":"markdown","4eb8cbb9":"markdown","ff0787fe":"markdown","d7f034fb":"markdown","45860bca":"markdown","f8167998":"markdown","ab95a40f":"markdown","d7fe7c30":"markdown","e2cf5694":"markdown","4eb01bce":"markdown","000225dd":"markdown","10dc1b75":"markdown","0f172ace":"markdown","2e5194d7":"markdown","d730712e":"markdown","bb1a6eff":"markdown","e567a214":"markdown","a673577d":"markdown"},"source":{"a074d70d":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","341236ae":"pd.plotting.register_matplotlib_converters()\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt","e4d6756e":"df_crude = pd.read_csv(\"..\/input\/argentina-venta-de-propiedades\/ar_properties_crude.csv\", index_col=\"id\")","23e2c867":"df_crude.head(5)","58f5092b":"#CABA = Ciudad Aut\u00f3noma de Buenos Aires = Buenos Aires City\ndf_CABA_dolar = df_crude[(df_crude[\"l1\"]==\"Argentina\") & (df_crude[\"l2\"]==\"Capital Federal\") & (df_crude[\"currency\"]==\"USD\")]\ndf_CABA_dolar","5eda3913":"df_CABA_dolar.columns","340dda53":"df_CABA_dolar[\"ad_type\"].unique()","69bd6eac":"df_CABA_dolar = df_CABA_dolar.drop(columns=[\"ad_type\"])","146fd718":"df_CABA_dolar = df_CABA_dolar.drop(columns=[\"start_date\", \"end_date\", \"created_on\", \"price_period\"])","3db45be1":"df_CABA_dolar = df_CABA_dolar.drop(columns=[\"l1\", \"l2\" ,\"currency\"])\nmissing_percentage = df_CABA_dolar.isnull().sum()*100\/len(df_CABA_dolar.index)\nmissing_percentage","6ea92469":"df_CABA_dolar = df_CABA_dolar.drop(columns=[\"l4\",\"l5\",\"l6\"])","f9a41b2e":"df_CABA_dolar[\"operation_type\"].value_counts()","24fcb42a":"df_CABA_dolar = df_CABA_dolar[df_CABA_dolar[\"operation_type\"]==\"Venta\"]\ndf_CABA_dolar = df_CABA_dolar.drop(columns=[\"title\", \"description\",\"operation_type\"])\ndf_CABA_dolar","628152db":"import folium\nfrom folium import Marker\nfrom folium.plugins import HeatMap","8cc2b383":"map_2 = folium.Map(width = 700, height = 500, location=[-34.586662, -58.436620], titles=\"cartodbposition\", zoom_start=12)\ndf_CABA_dolar_noLatNorLonMissing = df_CABA_dolar[df_CABA_dolar[\"lat\"].notnull() & df_CABA_dolar[\"lon\"].notnull()]\nHeatMap(data=df_CABA_dolar_noLatNorLonMissing[[\"lat\",\"lon\"]], radius=12).add_to(map_2)","fab1b7a9":"map_2","761d97bb":"df_CABA_dolar[\"l3\"].value_counts().head(8)","948178fb":"df_CABA_dolar.groupby(by=[\"l3\"], axis=0)[\"price\"].median().sort_values(ascending=False).head(7)","5d19d7c7":"df_CABA_dolar.groupby(by=[\"l3\"], axis=0)[\"price\"].median().sort_values(ascending=False).tail(7)","a419b5a8":"\nplt.figure(figsize=(10,7))\nplt.ticklabel_format(style='plain', axis='x')\nsns.distplot(df_CABA_dolar[\"price\"])\n\nplt.ylim(0,10**-7)\nplt.xlim(0,4000000)","aee23d29":"df_CABA_dolar.describe()\n","ce5ffc0e":"plt.figure(figsize=(15,10))\nplt.hist(x=df_CABA_dolar[\"property_type\"])","b57fd725":"properties = df_CABA_dolar\ncurrent_missing_percentages = (properties.isnull().sum()\/properties.shape[0]).sort_values(ascending=False)\ncurrent_missing_percentages","240ea0d8":"print(properties[properties[\"surface_covered\"].isnull() & properties[\"surface_total\"].isnull()].shape[0])\nproperties[properties[\"surface_covered\"].isnull() & properties[\"surface_total\"].isnull()].shape[0]\/properties.shape[0]","419c8f13":"def columns_correlation_with_target(df,target):\n    for feature in df.select_dtypes(exclude=[\"object\"]).columns:\n        if feature!=target:\n            print(\"Correlation between \", feature, \" and \", target, \": \", df[target].corr(df[feature]))","0311aef0":"columns_correlation_with_target(properties,\"price\")","8314506f":"properties = properties.drop(columns=[\"bedrooms\"])","2468a5ba":"properties","181c366a":"core_properties = properties.drop(columns=[\"lat\",\"lon\"])\ncore_properties = core_properties.dropna(axis=0)\ncore_properties.shape","3115ea3a":"from sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nX_core = core_properties.drop(columns=[\"price\"])\ny_core = core_properties[\"price\"]\n","37045d13":"X_core_train, X_core_valid, y_core_train, y_core_valid = train_test_split(X_core,y_core)\n\nOHE4 = OneHotEncoder(handle_unknown=\"ignore\",sparse=False)\n\nobj_cols = [col for col in X_core_valid.columns if X_core_valid[col].dtype==\"object\"]\n\nOHE4_cat_train = pd.DataFrame(OHE4.fit_transform(X_core_train[obj_cols]))\nOHE4_cat_valid = pd.DataFrame(OHE4.transform(X_core_valid[obj_cols]))\n\nOHE4_cat_train.index = X_core_train.index\nOHE4_cat_valid.index = X_core_valid.index\n\nnum_cols = [col for col in X_core_valid.columns if X_core_valid[col].dtype==\"float64\"]\nnum_cols\nencoded_X_core_train = pd.concat([OHE4_cat_train,X_core_train[num_cols]],axis=1)\nencoded_X_core_valid = pd.concat([OHE4_cat_valid, X_core_valid[num_cols]], axis=1)\n\n","ea708955":"RFR2 = RandomForestRegressor(random_state=4).fit(encoded_X_core_train,y_core_train)\nRFR2_preds = RFR2.predict(encoded_X_core_valid)\nRFR2_MAE = mean_absolute_error(RFR2_preds,y_core_valid)\nRFR2_MAE","ff8bd5d4":"bath_rooms_ratio = core_properties[\"bathrooms\"]\/core_properties[\"rooms\"]\nsurf_covered_by_total= core_properties[\"surface_covered\"]\/core_properties[\"surface_total\"]\nl3_type = core_properties[\"l3\"]+core_properties[\"property_type\"]","39e6d289":"def newFeatureTester(df, new_column):\n    X_core[\"new_feature\"] = new_column\n    \n    X_train, X_test, y_train, y_test = train_test_split(X_core,y_core)\n    \n    objec_cols = [col for col in X_test.columns if X_test[col].dtype==\"object\"]\n    \n    OHE = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n    OHEncoded_cats_train = pd.DataFrame(OHE.fit_transform(X_train[objec_cols]))\n    OHEncoded_cats_test = pd.DataFrame(OHE.transform(X_test[objec_cols]))\n    \n    OHEncoded_cats_train.index = X_train.index\n    OHEncoded_cats_test.index = X_test.index\n    \n    numericals_train = X_train.select_dtypes(exclude=[\"object\"])\n    numericals_test = X_test.select_dtypes(exclude=[\"object\"])\n    \n    OHEncoded_train = pd.concat([OHEncoded_cats_train,numericals_train], axis=1)\n    OHEncoded_test = pd.concat([OHEncoded_cats_test, numericals_test], axis=1) \n    \n    model = RandomForestRegressor(random_state=3).fit(OHEncoded_train,y_train)\n    preds = model.predict(OHEncoded_test)\n    \n    mae = mean_absolute_error(preds,y_test)\n    \n    print(\"MAE with \", new_column.name,\": \" , mae)\n    \n    mae_avg_price = mae\/(y_core.mean())\n    \n    print(\"MAE\/AVG PRICE: \", new_column.name,\": \",  mae_avg_price)\n    \n    return [mae,mae_avg_price]\n    ","7cd13bc6":"res = pd.DataFrame({\"bath_rooms_ratio\" : newFeatureTester(core_properties,bath_rooms_ratio),\n\"surf_covered_by_total\" : newFeatureTester(core_properties,surf_covered_by_total),\n\"l3_type\":newFeatureTester(core_properties,l3_type)},index=[\"MAE\",\"MAE\/AVG Price\"])","ebe13925":"res","9316ba37":"dict_new_features = {\"bath_rooms_ratio\":bath_rooms_ratio, \"surf_covered_by_total\":surf_covered_by_total, \"l3_type\":l3_type}\ndf_new_features = pd.DataFrame(dict_new_features)\ndf_new_features\n\nX_core_plus_new = pd.concat([X_core,df_new_features], axis=1)\nX_core_plus_new.isnull().sum()","edf7d2bf":"import eli5\nfrom eli5.sklearn import PermutationImportance","6920df49":"X_train, X_test, y_train, y_test = train_test_split(X_core_plus_new,y_core)\nobject_cols = [col for col in X_train.columns if X_train[col].dtype==\"object\"]\n\nOHE2 = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n\nlabeled_obj_cols_train = pd.DataFrame(OHE2.fit_transform(X_train[object_cols]))\nlabeled_obj_cols_test = pd.DataFrame(OHE2.transform(X_test[object_cols]))\n","bd27ce95":"#OneHotEncoder removed indexes, put them back...\nlabeled_obj_cols_train.index = X_train.index\nlabeled_obj_cols_test.index= X_test.index\n\nnumeric_X_train = X_train.select_dtypes(exclude=[\"object\"])\nnumeric_X_test = X_test.select_dtypes(exclude=[\"object\"])","2978bbcd":"labeled_X_train  =  pd.concat([labeled_obj_cols_train,numeric_X_train], axis=1)\nlabeled_X_test = pd.concat([labeled_obj_cols_test,numeric_X_test], axis=1)","7ab19f8a":"RFReg = RandomForestRegressor(random_state=7).fit(labeled_X_train,y_train)\n\npermutator = PermutationImportance(RFReg,random_state=2).fit(labeled_X_test,y_test)","473055fe":"permutator.estimator","9ac26f61":"colnames_labeled = labeled_X_train.columns.tolist()\ncolnames_labeled_all_as_strings = [str(name) for name in colnames_labeled]","eb7cf077":"eli5.show_weights(permutator, feature_names=colnames_labeled_all_as_strings, top=len(colnames_labeled_all_as_strings))","7dfa64f6":"preds = permutator.predict(labeled_X_test)\nmae = mean_absolute_error(preds,y_test)\nprint(\"Mean absolute error: \",mae, \" . Error in relation to mean price: \", mae\/y_test.mean(), \"% .\")","c4760a09":"X_core_plus_new = X_core_plus_new.drop(columns=[\"surf_covered_by_total\"])","46f7a461":"from sklearn.impute import SimpleImputer\nproperties_with_missing = properties.drop(columns=[\"lat\",\"lon\"])\nproperties_with_missing_numericals = properties_with_missing.dropna(subset=[\"l3\",\"property_type\"])\nproperties_with_missing_numericals","578d32b2":"X = properties_with_missing_numericals.drop(columns=[\"price\"])\ny=properties_with_missing_numericals[\"price\"]\nX_train, X_test, y_train, t_test = train_test_split(X,y)","a1a5e034":"obj_cols = [col for col in X_test.columns if X_test[col].dtype==\"object\"]\nOHE3 = OneHotEncoder(handle_unknown=\"ignore\",sparse=False)\n\nOHE3_cat_X_train = pd.DataFrame(OHE3.fit_transform(X_train[obj_cols]))\nOHE3_cat_X_test = pd.DataFrame(OHE3.transform(X_test[obj_cols]))\n#One Hot Encoder lost indexes, put them back...\nOHE3_cat_X_train.index = X_train.index\nOHE3_cat_X_test.index = X_test.index\n\nnumerical_X_train = X_train.select_dtypes(exclude=[\"object\"])\nnumerical_X_test = X_test.select_dtypes(exclude=[\"object\"])\n\nOHE3_X_train = pd.concat([OHE3_cat_X_train,numerical_X_train], axis=1)\nOHE3_X_test = pd.concat([OHE3_cat_X_test,numerical_X_test],axis=1)","8e1345cb":"imputer = SimpleImputer(strategy=\"median\")\n\nimputed_X_train = pd.DataFrame(imputer.fit_transform(OHE3_X_train))\nimputed_X_test = pd.DataFrame(imputer.transform(OHE3_X_test))\n#imputer removed column names, put them back    \nimputed_X_train.columns = OHE3_X_train.columns\nimputed_X_test.columns = OHE3_X_test.columns","38dc7d57":"RFR_new = RandomForestRegressor(random_state = 1).fit(imputed_X_train,y_train)","2f01dd98":"predictions_RFR_new = RFR_new.predict(imputed_X_test)\nmae_RFR_new = mean_absolute_error(predictions_RFR_new,t_test)\nmae_RFR_new","ff4b058d":"from joblib import dump","e12ed4e0":"dump(RFR_new,\"imputed_RFR.joblib\")","db099543":"\"\"\"X_core_train, X_core_valid, y_core_train, y_core_valid = train_test_split(labeled_X_train,y_train)\nOHE4 = OneHotEncoder(handle_unknown=\"ignore\",sparse=False)\n\nOHE4_cat_train = pd.DataFrame(OHE4.fit_transform(X_core_train[obj_cols]))\nOHE4_cat_valid = pd.DataFrame(OHE4.transform(X_core_valid[obj_cols]))\n\nOHE4_cat_train.index = X_core_train.index\nOHE4_cat_valid.index = X_core_valid.index\n\nnum_cols = [col for col in X_core_valid.columns if X_core_valid[col].dtype==\"float64\"]\nnum_cols\nencoded_X_core_train = pd.concat([OHE4_cat_train,X_core_train[num_cols]],axis=1)\nencoded_X_core_valid = pd.concat([OHE4_cat_valid, X_core_valid[num_cols]], axis=1)\"\"\"","51321470":"from xgboost import XGBRegressor\nXGBR2 = XGBRegressor(random_state=6,n_estimators=900,early_stopping_rounds=10, \n                     eval_set=[encoded_X_core_valid,y_core_valid],verbose=False).fit(encoded_X_core_train,y_core_train)","9f817785":"XGBR2_preds = XGBR2.predict(encoded_X_core_valid)\nXGBR2_MAE = mean_absolute_error(y_core_valid,XGBR2_preds)\nXGBR2_MAE","60b29d6d":"dump(XGBR2,\"XGBR2.joblib\")","86ff9b1a":"encoded_X_core_train","1c66f71c":"labeled_X_train","330046e3":"RFR2 = RandomForestRegressor(random_state=4).fit(encoded_X_core_train,y_core_train)\nRFR2_preds = RFR2.predict(encoded_X_core_valid)\nRFR2_MAE = mean_absolute_error(RFR2_preds,y_core_valid)\nRFR2_MAE","52ba7e5e":"RFR3 = RandomForestRegressor(random_state=4).fit(labeled_X_train,y_core_train)\nRFR3_preds = RFR3.predict(labeled_X_test)\nRFR3_MAE = mean_absolute_error(RFR3_preds,y_core_valid)\nRFR3_MAE","5db46c44":"from joblib import dump, load\ndump(RFR2, 'baseline_random_forest.joblib')#37.6k","1dac66ca":"for col in X_train.columns:\n    if X_train[col].dtype==\"object\":\n        X_train[col] = X_train[col].astype('category')\n        X_test[col] = X_test[col].astype('category')","ce504a44":"LGBM = LGBMRegressor(random_state=12).fit(X_train,y_train)\npreds_LGBM = LGBM.predict(X_test)\nmean_absolute_error(preds_LGBM,y_test)","5f2ccfa4":"LGBM2 = LGBMRegressor(random_state=12).fit(labeled_X_train,y_train)\npreds_LGBM2 = LGBM2.predict(labeled_X_test)\nmean_absolute_error(preds_LGBM2,y_test)","a1b7f699":"from catboost import CatBoostRegressor\n\nCBR = CatBoostRegressor(random_state=9,cat_features=[\"l3\",'l3_type', 'property_type']).fit(X_train,y_train)\npreds = CBR.predict(X_test)\nMAE = mean_absolute_error(preds,y_test)","549355f7":"MAE","718e188f":"CBR2 = CatBoostRegressor(random_state=9).fit(labeled_X_train,y_train)\npreds = CBR2.predict(labeled_X_test)\nMAE = mean_absolute_error(preds,y_test)","7112f750":"MAE","f625891b":"dump(RFR2, 'baseline_random_forest.joblib')#37.6k\ndump(RFR3, 'l3_types_random_forest.joblib')#37.6k","8e3a27c9":"dump(XGBR2,\"XGBR2.joblib\")#40k","f83445bc":"dump(RFR_new,\"imputed_RFR.joblib\")#70k","a0b50e5f":"from sklearn.model_selection import GridSearchCV","b8f6badc":"parameters_forsearch = {\n    \"n_estimators\" : [100,250,500,750]\n}\nsearch = GridSearchCV(RandomForestRegressor(),parameters_forsearch,cv=2)\nsearch.fit(encoded_X_core_train,y_core_train)","104ac699":"print(search.best_params_)","a42ec3ef":"current_model = load(\"baseline_random_forest.joblib\")","f62de74e":"Note that it is still a huge dataset.  \n<a id=\"fmt\"><\/a>\n## First model training + cross validation","53bd9a42":"\"l3\" gives information in regards to te neighboorhood. \"l4\", \"l5\" and \"l6\" give even more accurate details, but they have a huge amount of missing values (almost all missing) and we\u00b4ve still got \"lat\" and \"lon\" give high-precision information in regards to the location of the house, this could be useful to increase the precition of the prediction. We\u00b4ll keep \"lat\", \"lon\" and \"l3\". ","94058774":"We\u00b4ll also drop \"l1\", \"l2\" and \"currency\" (they were used to filter by country and by city previously)","7b8fc449":"<a id=\"s1p2\"><\/a>\n## 1.2. Plots and distributions","9314c05f":"## 1. Exploratory Data Analysis (EDA)<a id=\"s1\"><\/a>","d8774df1":"Property type histogram","9a2abcb4":"One hot encoded version...","85d36a27":"<a id=\"prwp\"><\/a>\n## (Applications)Predictions on real world properties:\n","7bc6e7ad":"The number of properties published look quite well distributed. Let\u00b4s look at the unique value counts from \"l3\"...","0a94520f":"<a id=\"ascs\"><\/a>\n## A second column selection","eb0ec444":"Not bad for a first attempt.","fcd18cda":"<a id=\"fc\"><\/a>\n## Feature Creation","0606060e":"We\u00b4ll compare the following model with the one trained on the core DataFrame...","5185d32e":"Training WITH labeled \"l3_type column\"","9751cad8":"There doesn\u00b4t seem to be substantial improvements (if any). Let\u00b4s use them all into one training set and measure the feature importance to choose with more confidence.\n<a id=\"fem\"><\/a>\n## Feature importance","6b86f547":"<a id=\"s3\"><\/a>\n# 3. Model selection","a078b65f":"###### This notebook is part of a wider project that aims to create property price predictors. The previous notebook tackled this whithin <a href=\"https:\/\/www.kaggle.com\/msorondo\/property-price-predictions-great-buenos-aires-n\">northern Great Buenos Aires<\/a>. This notebook continues with the previous project by taking some insights for filtering and selecting the features and models... Still, it will introduce lots of modifications in order to increase the performance of the models to train. This notebook will be also used to refine the previous one.\n","809ed4bd":"We\u00b4ll keep RFR2 model by now, it has the best n_estimators.","ba1fb65b":"I\u00b4ll first train a Gradient Boosting Regressor, then compare with the Random Forest with no parameter tuning and then select the one that best performed for further optimization.","aad48ae1":"<a id=\"ms\"><\/a>\n## Model saving","b79d22ed":"<a id=\"rfr\"><\/a>\n## Random Forest Regressor","ab031c75":"***Table of contents***  \n1. [Exploratory data analysis](#s1)  \n    1. [A first look to the columns, column selection by relevance](#s1p1) \n    \n        * [Temporary columns](#tcols)\n        * [Geospatial columns](#gcols)\n        * [Other columns](#ocols)\n        \n    2. [Feature plots and distributions](#s1p2)  \n    \n        * [Publication density map](#pdm)  \n        * [Median prices diagram](#mpd)  \n        * [Price distribution](#pd)  \n        * [<font color=\"red\">(to-do)Boxplots (useful for imputation criteria)<\/font>](#b)\n    \n2. [Feature engineering](#s2)   \n\n    * [A second column selection](#ascs)  \n    * [Core training data](#ctd)   \n    * [<font color=\"green\">First model training and results<\/font>](#fmt)\n    * [Feature creation](#df)\n    * [Feature importance measurement](#fem)\n    * [<font color=\"red\">Detailed Spatial Clustering<\/font>](#dsc)\n    * [Imputation](#idt)\n    \n    \n3. [Model selection](#s3)\n    * [XGBoost](#xgb)\n    * [Random Forest Regressor](#rfr)\n    * [LGBMRegressor](#LGBM)\n    * [CatBoostingRegressor](#LGBM)  \n    * [Model saving](#ms)\n\n4. [Model optimization](#mo)\n\n5. [Applications and conclusions](#s4)\n    * [Predictions on real-world properties](#prwp)\n    * [Conclusions](#conclusions)\n","284df17f":"### Geospatial columns <a id=\"gcols\"><\/a>","f8de6a89":"The error seems to be considerably lower (~ USD 5K less) when combining all of the new features. \nThere are some columns resulting from the One Hot Encoding that deteriorate the prediction but the totality of them increases it quite a bit. The only feature that seems not worth using is \"surf_covered_by_total\".\nWe\u00b4ll update the DataFrame to eliminate this feature.","be1bc66f":"<a id=\"s2\"><\/a>\n# 2. Feature engineering\n\nLet\u00b4s start by renaming the dataframe to simplify it.","ab8eb816":"<a id=\"mo\"><\/a>\n# 4. Model Optimization","5b3cb50f":"Take only Buenos Aires properties, valuated in dollars (ARS have been hugely devaluated).","bebf899d":"newFeatureTester function","41cdb76b":"### 1.1. A first look at the columns, column selection by relevance<a id=\"s1p1\">","4dc707ea":"<a id=\"ocols\"><\/a>\n### Other columns\nEven though we could use NLP techniques to analyze \"title\" and \"description\" columns, this would tremendously increase the length and complexity of this notebook with probably not much more benefits. Rent operations will be excluded as well.","7c7779e3":"We\u00b4ll drop this one, it carries no relevant information at all.","df6afa1c":"There are some features that add important information to perform prediction, but still have huge amounts of missing values %, we\u00b4ll examine correlation to see how to deal with them.","a7ca185f":"<a id=\"lgbm\"><\/a>\n## LGBMRegressor","1af3c482":"<h1 align=\"center\">Machine Learning regressions to predict Buenos Aires City housing prices<\/h1>  \n\n<font color=\"green\">Upvotes and suggestions are highly appreciated :)<\/font>  \n***Note: sections 4 and 5 are still under development, I really do appreciate advice in regards to them.***","0e0c9f79":"We\u00b4ll create and test new features to increase the performance...","4a1ba711":"<a id=\"xgb\"><\/a>\n## XGBoosting Regressor","b0bd60ac":"<a id=\"tcols\"><\/a>\n### Temporary columns\nStart date, end date and creation date give some relevant information, but they are not good predictors of the price. We want to predict the price and these dates depend on the owner\u00b4s choice (which obviously is not significantly correlated with the price). Same as the price period.","303be3a7":"All of the gradient boosting techniques were outperformed by the random forest regressor, yet the extreme gradient booster got quite near and has a bigger tuning margin. We\u00b4ll optimize both.","77e93ee5":"<a id=\"dsc\"><\/a>\n## (for later stages of development)Detailed Spatial Clustering\nWe\u00b4ll use Density-Based Spatial Clustering of Applications with Noise algorithm to unsupervisedly find highly detailed clusters that relate numeric geospatial columns with price. I chose this one over the others because of its capability to detect multiple clusters in high-density maps without having to pre-establish the number of clusters (like K-Means does).\nAnother advantage of this algorithm is that it performs very well with high-dimentional spaces, this lets us go further and add the price component.","02c0d296":"This way we almost double the amount of properties to perform prediction.","d9bbeda8":"<a id=\"mpd\"><\/a>\n### Median prices diagram ","04da4d0a":"The imputation icreased the error. We\u00b4ll avoid it.","4eb8cbb9":"<a id=\"conclusions\"><\/a>\n# Conclusions","ff0787fe":"As expected, most of the properties are apartments.","d7f034fb":"We\u00b4ll first take a look to the dataset and to have an idea of the data in it...","45860bca":"The difference isn\u00b4t worth the extra training time.","f8167998":"<a id=\"cbr\"><\/a>\n## CatBoostRegressor","ab95a40f":"<a id=\"pdm\"><\/a>\n### Publication density map (excepting publications with missing geolocation)","d7fe7c30":"<a id=\"ctd\"><\/a>\n## Core training data\nIn order to proceed to deal with missing values and then create features we\u00b4ll start by tasting how a basic, not so much pocessed dataframe performs in a model. It will include all of the latter features except for lat and lon, and we\u00b4ll remove rows with missing values.\nThen we\u00b4ll compare this approach with another one that imputes the values.","e2cf5694":"OneHotEncode categoricals...","4eb01bce":"### On numerical columns only","000225dd":"With this function, we\u00b4ll try to separately measure the impact of each new feature on the model\u00b4s prediction.","10dc1b75":"<a id=\"idt\"><\/a>\n## Imputation","0f172ace":"Lat and lon were still not clustered so there\u00b4s no problem with them not correlating with price.\nRooms, bedrooms and bathrooms are significantly correlated with price, with bedrooms being the least ones.  \nBedrooms columns will have to be dropped, they do correlate with price but they have a huge amount of missing values and are not worth of imputation (mainly because in Argentina \"bedrooms\" isn\u00b4t usually used as a reference, and rooms and bathrooms already give substantial information).","2e5194d7":"<a id=\"s4\"><\/a>\n# 5. Applications and conclusions.","d730712e":"Given the vast amount of outliers across all columns, we\u00b4ll impute for the median.","bb1a6eff":"Let\u00b4s try with one hot encoded data...","e567a214":"<a id=\"pd\"><\/a>\n### Price distribution","a673577d":"Training WITHOUT labeled \"l3_type column\""}}