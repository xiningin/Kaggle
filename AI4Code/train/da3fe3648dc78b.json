{"cell_type":{"22d2c08a":"code","9f321396":"code","62453607":"code","1abd76a5":"code","abd4bb7f":"code","03cea8c6":"code","38967672":"code","d8e4cf34":"code","6733f324":"code","518bf2c7":"code","f2fb7b7e":"code","43d7e23b":"code","ea81dfc3":"code","356faf8a":"code","1303f38e":"code","b3ec5093":"markdown","4c40cd7b":"markdown","954e91af":"markdown","8352b2d0":"markdown","85de5273":"markdown","2054a34e":"markdown","f8136899":"markdown","c3dff84c":"markdown","4c3afe21":"markdown","01ec2526":"markdown","83886cba":"markdown","20affa8e":"markdown"},"source":{"22d2c08a":"!pip install numpy pandas tqdm sklearn transformers\n!pip install sentencepiece","9f321396":"import gc\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_recall_fscore_support as score\nimport matplotlib.pyplot as plt\nfrom numpy.random import random\nfrom transformers import AutoTokenizer\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    tpu = False","62453607":"print(tpu)","1abd76a5":"NR_EPOCHS=2\nBATCH_SIZE=4\ncheckpoint_filepath = '.\/checkpoint\/'\nMODEL_NAME = 'jplu\/tf-xlm-roberta-large'\nMAX_LENGTH = 624\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","abd4bb7f":"def aggregate(sample,schema,predictions):\n    results = []\n    i = 1\n    for token, y_true in sample:\n        nr_subtoken = len(tokenizer(token)['input_ids']) - 2\n        pred = predictions[i:i+nr_subtoken]\n        i += nr_subtoken\n        y_pred = schema[np.argmax(np.sum(pred, axis=0))]\n        results.append((token, y_true, y_pred))\n    return results\n\ndef prepare_dataset(PATH,convert2bio=False):\n  sents = []\n  chunks = open(PATH,'r').read().split('\\n\\n')\n  for chunk in chunks:\n    lines = chunk.split('\\n')\n    sent = []\n    current_tag = None\n    previous_tag = None\n    for line in lines:\n        if line != '':\n            token = line.split('\\t')\n            previous_tag = current_tag \n            current_tag = token[1]\n            if convert2bio:\n                if current_tag == '.':\n                    sent.append((token[0],'O'))\n                elif previous_tag == current_tag and current_tag != 'O':\n                    sent.append((token[0],'I-'+token[1]))\n                elif previous_tag != current_tag and current_tag != 'O':\n                    sent.append((token[0],'B-'+token[1]))\n                else:\n                    sent.append((token[0],token[1]))\n            else:\n                sent.append((token[0],token[1]))\n    sents.append(sent)\n  return sents\n\ndef tokenize_sample(sample, max_len=None):\n    seq = [\n            (subtoken, tag)\n            for token, tag in sample\n            for subtoken in tokenizer(token)['input_ids'][1:-1]\n        ]\n    if max_len != None:\n        if len(seq) > max_len:\n            seq = seq[:max_len-2]\n    return [(3, 'O')] + seq + [(4, 'O')]\n\ndef preprocess(samples,schema,max_len):    \n    tag_index = {tag: i for i, tag in enumerate(schema)}\n    tokenized_samples = list(tqdm(map(tokenize_sample, samples, [max_len]*len(samples))))\n    X = np.zeros((len(tokenized_samples), max_len), dtype=np.int32)\n    y = np.zeros((len(tokenized_samples), max_len), dtype=np.int32)\n    for i, sentence in enumerate(tokenized_samples):\n        for j, (subtoken_id, tag) in enumerate(sentence):\n            try:\n                X[i, j] = subtoken_id\n                y[i, j] = tag_index[tag]\n            except:\n                print(f'i={i} ; j={j}')\n    return X, y","03cea8c6":"ner_samples_train = prepare_dataset('..\/input\/ner-ontonotes5\/ner_train.txt',convert2bio=True)\nner_samples_valid = prepare_dataset('..\/input\/ner-ontonotes5\/ner_valid.txt',convert2bio=True)\nner_samples_test = prepare_dataset('..\/input\/ner-ontonotes5\/ner_test.txt',convert2bio=True)\n\npos_samples_train = prepare_dataset('..\/input\/pos-ontonotes5\/pos_train.txt',convert2bio=True)\npos_samples_valid = prepare_dataset('..\/input\/pos-ontonotes5\/pos_valid.txt',convert2bio=True)\npos_samples_test = prepare_dataset('..\/input\/pos-ontonotes5\/pos_test.txt',convert2bio=True)\n\nsyntax_samples_train = prepare_dataset('..\/input\/syntax-ontonotes5\/syntax_train.txt',convert2bio=True)\nsyntax_samples_valid = prepare_dataset('..\/input\/syntax-ontonotes5\/syntax_valid.txt',convert2bio=True)\nsyntax_samples_test = prepare_dataset('..\/input\/syntax-ontonotes5\/syntax_test.txt',convert2bio=True)\n\nner_samples = ner_samples_train + ner_samples_test + ner_samples_valid\nner_schema = ['_'] + sorted({tag for sentence in ner_samples \n                             for _, tag in sentence})\n\npos_samples = pos_samples_train + pos_samples_test + pos_samples_valid\npos_schema = ['_'] + sorted({tag for sentence in pos_samples \n                             for _, tag in sentence})\n\nsyntax_samples = syntax_samples_train + syntax_samples_test + syntax_samples_valid\nsyntax_schema = ['_'] + sorted({tag for sentence in syntax_samples \n                             for _, tag in sentence})\n\n\nX_train, y_train = {}, {}\nX_valid, y_valid = {}, {}\nX_test, y_test = {}, {}\n\n\n#X_valid['syntax'], y_valid['syntax'] = preprocess(syntax_samples_valid,syntax_schema,MAX_LENGTH)\n#X_train['syntax'], y_train['syntax'] = preprocess(syntax_samples_train,syntax_schema,MAX_LENGTH)\nX_test['syntax'], y_test['syntax'] = preprocess(syntax_samples_test,syntax_schema,MAX_LENGTH)\n\n\n#X_valid['pos'], y_valid['pos'] = preprocess(pos_samples_valid,pos_schema,MAX_LENGTH)\n#X_train['pos'], y_train['pos'] = preprocess(pos_samples_train,pos_schema,MAX_LENGTH)\nX_test['pos'], y_test['pos'] = preprocess(pos_samples_test,pos_schema,MAX_LENGTH)\n\n\n#X_valid['ner'], y_valid['ner'] = preprocess(ner_samples_valid,ner_schema,MAX_LENGTH)\n#X_train['ner'], y_train['ner'] = preprocess(ner_samples_train,ner_schema,MAX_LENGTH)\nX_test['ner'], y_test['ner'] = preprocess(ner_samples_test,ner_schema,MAX_LENGTH)\n","38967672":"ner_samples_train[0]","d8e4cf34":"from transformers import XLMRobertaConfig\nfrom transformers import TFAutoModelForTokenClassification\n\ncheckpoint_path = '..\/input\/weights123\/SMT_xlm1 (1).h5'\n\nif tpu:\n    print('with TPU')\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\n    checkpoints_cb = tf.keras.callbacks.ModelCheckpoint('.\/checkpoints', options=save_locally)\n    with tpu_strategy.scope():\n        xlmroberta_config = XLMRobertaConfig.from_pretrained(MODEL_NAME)\n        xlmroberta_config.output_hidden_states = True\n        backbone = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME,config=xlmroberta_config)\n\n        input_ids1 = tf.keras.layers.Input(shape=(len(X_test['pos'][4]),), name='input_token_ner', dtype='int32')\n        input_ids2 = tf.keras.layers.Input(shape=(len(X_test['pos'][4]),), name='input_token_syntax', dtype='int32')\n        input_ids3 = tf.keras.layers.Input(shape=(len(X_test['pos'][4]),), name='input_token_pos', dtype='int32')\n        #input_masks = tf.keras.layers.Input(shape=(389,), name='masked_token', dtype='int32')\n\n        features1 = backbone(input_ids1)[1][-1]\n        features2 = backbone(input_ids2)[1][-1]\n        features3 = backbone(input_ids3)[1][-1]\n\n        dp1 = tf.keras.layers.Dropout(0.1)(features1)\n        dp2 = tf.keras.layers.Dropout(0.1)(features2)\n        dp3 = tf.keras.layers.Dropout(0.1)(features3)\n\n        d1 = tf.keras.layers.Dense(len(ner_schema), name='output_token_ner')(dp1)\n        d2 = tf.keras.layers.Dense(len(syntax_schema), name='output_token_syntax')(dp2)\n        d3 = tf.keras.layers.Dense(len(syntax_schema), name='output_token_pos')(dp3)\n        model = tf.keras.Model(inputs=[input_ids1,input_ids2,input_ids3], outputs = [d1,d2,d3])\n        if checkpoint_path != None:\n            model.load_weights(checkpoint_path)\n        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-6)\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n        model.compile(optimizer=optimizer,loss=loss, metrics='accuracy')\nelse:\n    print('w\\o TPU')\n    xlmroberta_config = XLMRobertaConfig.from_pretrained(MODEL_NAME)\n    xlmroberta_config.output_hidden_states = True\n    backbone = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME,config=xlmroberta_config)\n\n    input_ids1 = tf.keras.layers.Input(shape=(len(X_test['pos'][4]),), name='input_token_ner', dtype='int32')\n    input_ids2 = tf.keras.layers.Input(shape=(len(X_test['pos'][4]),), name='input_token_syntax', dtype='int32')\n    input_ids3 = tf.keras.layers.Input(shape=(len(X_test['pos'][4]),), name='input_token_pos', dtype='int32')\n    #input_masks = tf.keras.layers.Input(shape=(389,), name='masked_token', dtype='int32')\n\n    features1 = backbone(input_ids1)[1][-1]\n    features2 = backbone(input_ids2)[1][-1]\n    features3 = backbone(input_ids3)[1][-1]\n\n    dp1 = tf.keras.layers.Dropout(0.1)(features1)\n    dp2 = tf.keras.layers.Dropout(0.1)(features2)\n    dp3 = tf.keras.layers.Dropout(0.1)(features3)\n\n    d1 = tf.keras.layers.Dense(len(ner_schema), name='output_token_ner')(dp1)\n    d2 = tf.keras.layers.Dense(len(syntax_schema), name='output_token_syntax')(dp2)\n    d3 = tf.keras.layers.Dense(len(syntax_schema), name='output_token_pos')(dp3)\n    model = tf.keras.Model(inputs=[input_ids1,input_ids2,input_ids3], outputs = [d1,d2,d3])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-6)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(optimizer=optimizer,loss=loss, metrics='accuracy')","6733f324":"model.fit(x = {'input_token_ner' : tf.constant(X_train['ner']),\n               'input_token_syntax' : tf.constant(X_train['syntax']),\n               'input_token_pos' : tf.constant(X_train['pos'])},\n          y = {'output_token_ner' : tf.constant(y_train['ner']),\n               'output_token_syntax' : tf.constant(y_train['syntax']),\n               'output_token_pos' : tf.constant(y_train['pos'])},\n          validation_data = ({'input_token_ner' : X_valid['ner'],\n                            'input_token_syntax' : X_valid['syntax'],\n                            'input_token_pos' : X_valid['pos']},\n                           {'output_token_ner' : y_valid['ner'],\n                            'output_token_syntax' : y_valid['syntax'],\n                            'output_token_pos' : y_valid['pos']}), \n          epochs=2, batch_size=16)\nmodel.save_weights('.\/SMT_xlm.h5')","518bf2c7":"def evaluate(predictions):\n    result = {}\n    for task in predictions.keys():\n        y = []\n        y_hat = []\n        for pred in predictions[task]:\n          for token in pred:\n            y.append(token[1])\n            y_hat.append(token[2])\n        result[task + ' micro f1'] = f1_score(y,y_hat,average='micro')\n        result[task + ' macro f1'] = f1_score(y,y_hat,average='macro')\n    return result","f2fb7b7e":"# compute f1 scores by chunks because kaggle can't handle big input at one time.\n\ny_probs = model.predict([X_test['ner'][:5000],X_test['syntax'][:5000],X_test['pos'][:5000]])\n\ny_probs_ner = y_probs[0]\ny_probs_syntax = y_probs[1]\ny_probs_pos = y_probs[2]\n\npredictions = {'ner' : [aggregate(t,ner_schema,p) for t, p in zip(ner_samples_test[:5000], y_probs_ner)],\n               'syntax' : [aggregate(t,syntax_schema,p) for t, p in zip(syntax_samples_test[:5000], y_probs_syntax)],\n               'pos' : [aggregate(t,pos_schema,p) for t, p in zip(pos_samples_test[:5000], y_probs_pos)]}\n\nscores1 = evaluate(predictions)\ndel y_probs, y_probs_ner, y_probs_syntax, y_probs_pos, predictions\ngc.collect()\n\ny_probs = model.predict([X_test['ner'][5000:10000],X_test['syntax'][5000:10000],X_test['pos'][5000:10000]])\n\ny_probs_ner = y_probs[0]\ny_probs_syntax = y_probs[1]\ny_probs_pos = y_probs[2]\n\npredictions = {'ner' : [aggregate(t,ner_schema,p) for t, p in zip(ner_samples_test[5000:10000], y_probs_ner)],\n               'syntax' : [aggregate(t,syntax_schema,p) for t, p in zip(syntax_samples_test[5000:10000], y_probs_syntax)],\n               'pos' : [aggregate(t,pos_schema,p) for t, p in zip(pos_samples_test[5000:10000], y_probs_pos)]}\n\nscores2 = evaluate(predictions)\ndel y_probs, y_probs_ner, y_probs_syntax, y_probs_pos, predictions\ngc.collect()\n\ny_probs = model.predict([X_test['ner'][10000:],X_test['syntax'][10000:],X_test['pos'][10000:]])\n\ny_probs_ner = y_probs[0]\ny_probs_syntax = y_probs[1]\ny_probs_pos = y_probs[2]\n\npredictions = {'ner' : [aggregate(t,ner_schema,p) for t, p in zip(ner_samples_test[10000:], y_probs_ner)],\n               'syntax' : [aggregate(t,syntax_schema,p) for t, p in zip(syntax_samples_test[10000:], y_probs_syntax)],\n               'pos' : [aggregate(t,pos_schema,p) for t, p in zip(pos_samples_test[10000:], y_probs_pos)]}\n\nscores3 = evaluate(predictions)\ndel y_probs, y_probs_ner, y_probs_syntax, y_probs_pos, predictions\ngc.collect()\n\ntotal_scores = {}\nfor key in scores1.keys():\n    total_scores[key] = (scores1[key] + scores2[key] + scores3[key])\/3\nprint(total_scores)","43d7e23b":"del backbone\ndel model\ngc.collect()","ea81dfc3":"from transformers import XLMRobertaConfig\nfrom transformers import TFAutoModelForTokenClassification\n\ncheckpoint_path = \"..\/input\/weights123\/HMT_xlm1 (1).h5\"\n\nif tpu:\n    print('with TPU')\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\n    checkpoints_cb = tf.keras.callbacks.ModelCheckpoint('.\/checkpoints', options=save_locally)\n    with tpu_strategy.scope():\n        xlmroberta_config = XLMRobertaConfig.from_pretrained(MODEL_NAME)\n        xlmroberta_config.output_hidden_states = True\n        backbone = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME,config=xlmroberta_config)\n\n        input_ids1 = tf.keras.layers.Input(shape=(len(X_test['pos'][4]),), name='input_token_ner', dtype='int32')\n        input_ids2 = tf.keras.layers.Input(shape=(len(X_test['pos'][4]),), name='input_token_syntax', dtype='int32')\n        input_ids3 = tf.keras.layers.Input(shape=(len(X_test['pos'][4]),), name='input_token_pos', dtype='int32')\n\n        fourth_layer_pos = backbone(input_ids1)[1][3]\n        eighth_layer_syntax = backbone(input_ids2)[1][7]\n        last_layer_ner = backbone(input_ids3)[1][-1]\n\n        dp1 = tf.keras.layers.Dropout(0.1)(fourth_layer_pos)\n        dp2 = tf.keras.layers.Dropout(0.1)(eighth_layer_syntax)\n        dp3 = tf.keras.layers.Dropout(0.1)(last_layer_ner)\n\n        d1 = tf.keras.layers.Dense(len(ner_schema), name='output_token_ner')(dp1)\n        d2 = tf.keras.layers.Dense(len(syntax_schema), name='output_token_syntax')(dp2)\n        d3 = tf.keras.layers.Dense(len(syntax_schema), name='output_token_pos')(dp3)\n\n        model = tf.keras.Model(inputs=[input_ids1,input_ids2,input_ids3], outputs = [d1,d2,d3])\n        \n        if checkpoint_path != None:\n            model.load_weights(checkpoint_path)\n            \n        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-6)\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n        model.compile(optimizer=optimizer,loss=loss, metrics='accuracy')\nelse:\n    print('w\\o TPU')\n    xlmroberta_config = XLMRobertaConfig.from_pretrained(MODEL_NAME)\n    xlmroberta_config.output_hidden_states = True\n    backbone = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME,config=xlmroberta_config)\n\n    input_ids1 = tf.keras.layers.Input(shape=(len(X_test['pos'][4]),), name='input_token_ner', dtype='int32')\n    input_ids2 = tf.keras.layers.Input(shape=(len(X_test['pos'][4]),), name='input_token_syntax', dtype='int32')\n    input_ids3 = tf.keras.layers.Input(shape=(len(X_test['pos'][4]),), name='input_token_pos', dtype='int32')\n\n    fourth_layer_pos = backbone(input_ids1)[1][3]\n    eighth_layer_syntax = backbone(input_ids2)[1][7]\n    last_layer_ner = backbone(input_ids3)[1][-1]\n\n    dp1 = tf.keras.layers.Dropout(0.1)(fourth_layer_pos)\n    dp2 = tf.keras.layers.Dropout(0.1)(eighth_layer_syntax)\n    dp3 = tf.keras.layers.Dropout(0.1)(last_layer_ner)\n\n    d1 = tf.keras.layers.Dense(len(ner_schema), name='output_token_ner')(dp1)\n    d2 = tf.keras.layers.Dense(len(syntax_schema), name='output_token_syntax')(dp2)\n    d3 = tf.keras.layers.Dense(len(syntax_schema), name='output_token_pos')(dp3)\n\n    model = tf.keras.Model(inputs=[input_ids1,input_ids2,input_ids3], outputs = [d1,d2,d3])\n\n    if checkpoint_path != None:\n        model.load_weights(checkpoint_path)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-6)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(optimizer=optimizer,loss=loss, metrics='accuracy')","356faf8a":"model.fit(x = {'input_token_ner' : tf.constant(X_train['ner']),\n               'input_token_syntax' : tf.constant(X_train['syntax']),\n               'input_token_pos' : tf.constant(X_train['pos'])},\n          y = {'output_token_ner' : tf.constant(y_train['ner']),\n               'output_token_syntax' : tf.constant(y_train['syntax']),\n               'output_token_pos' : tf.constant(y_train['pos'])},\n          validation_data = ({'input_token_ner' : X_valid['ner'],\n                            'input_token_syntax' : X_valid['syntax'],\n                            'input_token_pos' : X_valid['pos']},\n                           {'output_token_ner' : y_valid['ner'],\n                            'output_token_syntax' : y_valid['syntax'],\n                            'output_token_pos' : y_valid['pos']}), \n          epochs=2, batch_size=BATCH_SIZE)\nmodel.save_weights('.\/HMT_xlm1.h5')","1303f38e":"# compute f1 scores by chunks because kaggle can't handle big input at one time.\n\ny_probs = model.predict([X_test['ner'][:5000],X_test['syntax'][:5000],X_test['pos'][:5000]])\n\ny_probs_ner = y_probs[0]\ny_probs_syntax = y_probs[1]\ny_probs_pos = y_probs[2]\n\npredictions = {'ner' : [aggregate(t,ner_schema,p) for t, p in zip(ner_samples_test[:5000], y_probs_ner)],\n               'syntax' : [aggregate(t,syntax_schema,p) for t, p in zip(syntax_samples_test[:5000], y_probs_syntax)],\n               'pos' : [aggregate(t,pos_schema,p) for t, p in zip(pos_samples_test[:5000], y_probs_pos)]}\n\nscores1 = evaluate(predictions)\ndel y_probs, y_probs_ner, y_probs_syntax, y_probs_pos, predictions\ngc.collect()\n\ny_probs = model.predict([X_test['ner'][5000:10000],X_test['syntax'][5000:10000],X_test['pos'][5000:10000]])\n\ny_probs_ner = y_probs[0]\ny_probs_syntax = y_probs[1]\ny_probs_pos = y_probs[2]\n\npredictions = {'ner' : [aggregate(t,ner_schema,p) for t, p in zip(ner_samples_test[5000:10000], y_probs_ner)],\n               'syntax' : [aggregate(t,syntax_schema,p) for t, p in zip(syntax_samples_test[5000:10000], y_probs_syntax)],\n               'pos' : [aggregate(t,pos_schema,p) for t, p in zip(pos_samples_test[5000:10000], y_probs_pos)]}\n\nscores2 = evaluate(predictions)\ndel y_probs, y_probs_ner, y_probs_syntax, y_probs_pos, predictions\ngc.collect()\n\ny_probs = model.predict([X_test['ner'][10000:],X_test['syntax'][10000:],X_test['pos'][10000:]])\n\ny_probs_ner = y_probs[0]\ny_probs_syntax = y_probs[1]\ny_probs_pos = y_probs[2]\n\npredictions = {'ner' : [aggregate(t,ner_schema,p) for t, p in zip(ner_samples_test[10000:], y_probs_ner)],\n               'syntax' : [aggregate(t,syntax_schema,p) for t, p in zip(syntax_samples_test[10000:], y_probs_syntax)],\n               'pos' : [aggregate(t,pos_schema,p) for t, p in zip(pos_samples_test[10000:], y_probs_pos)]}\n\nscores3 = evaluate(predictions)\ndel y_probs, y_probs_ner, y_probs_syntax, y_probs_pos, predictions\ngc.collect()\n\ntotal_scores = {}\nfor key in scores1.keys():\n    total_scores[key] = (scores1[key] + scores2[key] + scores3[key])\/3\nprint(total_scores)","b3ec5093":"## DEFINE MODEL","4c40cd7b":"## TRAIN","954e91af":"# HIERARCHICAL MULTI-TASKING (HMT)","8352b2d0":"## TRAIN","85de5273":"# CONCLUSION\n\nExperiemnts has shown that simple multi-tasking is better than hierarchical one. More over, SMT is about the level of classical approach to NER.","2054a34e":"# SIMPLE MULTI-TUSKING (SMT)\n","f8136899":"## DEFINE MODEL","c3dff84c":"# PREPARE DATASETS","4c3afe21":"## TEST","01ec2526":"## TEST","83886cba":"This notebook contains imlementations of two approaches to multitasking in NLP.  Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately.\n\nIn this work, I am going to apply MTL for three NLP classification tasks such as POS tagging, NER tagging and syntax-dependency tagging. As natural language understanding component (NLU-component) I use XML-RoBERTa.\n\n![](https:\/\/drive.google.com\/uc?id=1TCdyyoHInbiZtSOUmyJN1miCj1iysygU)\n\n**Simple Multitasking**\n\nIn this approach, take use the last layer of NLU-component to make branches for each of three tasks.\n\n**Hierarchical Multitasking**\n\nThe model is trained in a hierarchical fashion to introduce an inductive bias by supervising a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. In this case, I use the forth layer for POS-tagging, the eighth layer for syntax-tagging and the last for NER-tagging.","20affa8e":"## CONFIG"}}