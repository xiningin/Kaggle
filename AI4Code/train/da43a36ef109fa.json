{"cell_type":{"32a42256":"code","acff125c":"code","13d310c1":"code","777e0d0d":"code","504a045c":"code","0264ae8e":"code","5304193f":"code","f0e4aab2":"code","bf6cf7eb":"code","0b6299de":"code","a914ff51":"code","26e9dd2d":"code","fb1d8923":"code","0666a28d":"code","faa720c9":"code","01951721":"code","f16fd188":"code","f42893f1":"code","574a8fb4":"code","a959a357":"code","a0c84fe9":"code","1a99ae49":"code","fb960722":"code","17294e8f":"code","59960039":"code","342aaf1e":"code","e0a0a43c":"code","a5549290":"code","798b54d4":"code","d161f7f0":"code","51b5bc9c":"code","aa53777e":"code","5ece68d9":"code","7057fa77":"code","5f7be7c1":"code","74d59484":"code","0ff858ac":"code","73a5c583":"code","1ca03e4c":"code","1b061a57":"code","d41981ea":"code","5faa9858":"code","9164119d":"code","e7aef0fb":"code","27782421":"code","77701d8e":"code","d792197c":"code","19baf865":"code","d978049e":"code","64535e5b":"code","c1b7a3f4":"markdown","a728e6eb":"markdown","955c25c3":"markdown","70bc124f":"markdown","c515bb32":"markdown","b845df87":"markdown","131eb2ce":"markdown","d1fd2959":"markdown","76fdc411":"markdown","5701f69d":"markdown","f8df6632":"markdown","1c87471b":"markdown","ddde95b1":"markdown","c6dfc4e4":"markdown","b9b0e704":"markdown","d0203774":"markdown","fcdd8197":"markdown","46072658":"markdown","6dfb9858":"markdown","bd46b5da":"markdown","a4cdf524":"markdown","17bf2037":"markdown","fbb7a58a":"markdown","341e8e4f":"markdown","7db201ca":"markdown","376655f2":"markdown","96011859":"markdown","82f3d483":"markdown","06f492e2":"markdown","c30a9418":"markdown","076f6a42":"markdown","80b6db3d":"markdown","15b0d65b":"markdown","c5547cd5":"markdown","cb1d62f3":"markdown","e0f00b0f":"markdown","b85b37a0":"markdown","fdb3da60":"markdown"},"source":{"32a42256":"#Data handling Imports\nimport pandas as pd \nimport numpy as np\n\n#Notebook arrange Imports\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Calculation Imports\nimport math\nimport random\n\n#Visualisation Imports\nimport seaborn as sns\nimport pylab\npylab.style.use('seaborn-pastel')\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#Feature Selection Imports\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import GenericUnivariateSelect\n\n#Outlier Handling Imports\nfrom scipy.stats.mstats import winsorize\n\n#Normalization & Scaler Imports\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import normalize\nfrom scipy.stats import boxcox, probplot, norm, shapiro\nfrom sklearn.decomposition import PCA\n\n#Sampling Imports\nfrom sklearn.model_selection import KFold\n\n#Encoding Imports\nfrom sklearn.preprocessing import LabelEncoder\n\n#Modeling Imports\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n#Clustering Imports\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import Birch\nfrom sklearn.cluster import MiniBatchKMeans\nimport scipy.cluster.hierarchy as shc\n\n#Accuracy Validation Imports\nfrom sklearn import metrics\nfrom sklearn.metrics import auc, roc_curve, f1_score, accuracy_score,precision_recall_curve,\\\nconfusion_matrix, classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.model_selection import cross_val_score\n\n#Other required libraries\nimport time\nstart = time. time()","acff125c":"#Loading dataset from UCI repository using URL\n# url = \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00292\/Wholesale%20customers%20data.csv\"\n# column_names = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicatessen','Channel','Region']\n# df = pd.read_csv(url,names=column_names)\ndf = pd.read_csv('..\/input\/wholesale-customer-datacsv\/Wholesale customers data.csv')\n#Taking copy for missing value handling purpose\ndf_MV = df.copy()\n# df.head()\ndf_MV.sample(5)","13d310c1":"#Checking the size of the dataset\ndf_MV.shape","777e0d0d":"#Checking for missing values\ndf_MV.isnull().sum()","504a045c":"#As we don't have any missing values in dataset . checking structure to put sunthetic nulls\ndf_MV.info()","0264ae8e":"#Count of unique values and categories featurewise for Channel & Region Column\ncolumn_list = ['Channel','Region']\n#print(column_list)\nfor col in column_list:\n    print('Feature: {:<9s} | Unique-Count: {:<3} | Categories: {:}'.format(col,df_MV[col].nunique(),df_MV[col].unique()))","5304193f":"#As dataset is not having any null values imputing null values to explain importance of preprocessing of null value handling\n#Inserting 3% of each column's values as null... [Synthetic]\n#As we have only 440 records.. doesn't want to distort..\nix = [(row, col) for row in range(df_MV.shape[0]) for col in range(df_MV.shape[1])]\nfor row, col in random.sample(ix, int(round(.03*len(ix)))):\n    df_MV.iat[row, col] = None","f0e4aab2":"#Checking null values after synthetic null insert\ndf_MV.isnull().sum().sort_values(ascending=False)","bf6cf7eb":"#Assigning unique category to region & Channel feature as '3' & '4' recpectively. For null handling\ndf_MV['Channel'] = df_MV['Channel'].fillna(3)\ndf_MV['Region'] = df_MV['Region'].fillna(4)\ndf_MV.isnull().sum().sort_values(ascending=False)","0b6299de":"#For rest null value imputation let's check insight of data\ndf_MV.describe()","a914ff51":"#Median and Mean by grouping cannel & region\ndf_MVal = df_MV.drop(['Channel','Region'], axis=1)\ndf_MV.groupby(['Channel', 'Region']).agg(['median','mean']).round(1)","26e9dd2d":"#As per above we can easily understood filling by mean is optimal as grouping with Channel & Region won't help here.\n#Vast difference between median and mean\ndf_MV.fillna(df_MV.mean(), inplace=True)\ndf_MV.isnull().sum().sort_values(ascending=False)","fb1d8923":"#Null heatmap to visualize the dataset\nsns.heatmap(df.isnull())","0666a28d":"#Checking the correlation \ncorr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(9, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n#Drawing heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0.5, square=True, linewidths=.5, cbar_kws={\"shrink\": .6},annot=True)\nplt.title(\"Correlation\", fontsize =10)","faa720c9":"#Target variable 'Channel'. let's see other feature selection techniques.\nX = df.drop('Channel', axis=1)\ny = df['Channel']\n\n# Apply SelectKBest Algorithm using chi2 score function\nkbest_features = SelectKBest(score_func=chi2, k=6)\nord_features = kbest_features.fit(X, y)\ndf_scores = pd.DataFrame(ord_features.scores_, columns=[\"Score\"])\ndf_columns = pd.DataFrame(X.columns)\nk_features = pd.concat([df_columns, df_scores], axis=1)\nk_features.columns=['Features','Score']\nk_features","01951721":"# Lets try an ensemble model to find the top features in our dataset\nmodel = ExtraTreesClassifier()\nmodel.fit(X, y)\nprint(model.feature_importances_)","f16fd188":"# Finding the top ranked fetures and plotting\nranked_features = pd.Series(model.feature_importances_, index = X.columns)\nplt.figure(figsize=(12, 5))\nranked_features.nlargest(15).plot(kind='barh', color='blue',  width =0.9 , linewidth = 0.05, alpha=0.4)\nplt.show()","f42893f1":"#the sorted values of mean higer score\nmutual_info = mutual_info_classif(X, y)\nmutual_data = pd.Series(mutual_info, index = X.columns)\nmutual_data.sort_values(ascending=False)","574a8fb4":"#Let's encode and select the best features\n\nlabel_encoder = LabelEncoder()\ndf_1 = df.apply(label_encoder.fit_transform)\n# X_feature = df.drop('Channel', axis=1)\n# Y_label = df['Channel']\nX = df_1.drop('Channel', axis=1)\nY = df_1['Channel']\n\n\n#trans = GenericUnivariateSelect(score_func=mutual_info_classif, mode='k_best', param=15)\ntrans = GenericUnivariateSelect(score_func = mutual_info_classif, mode='percentile', param = 70)\ntrans_feat = trans.fit_transform(X, Y)\ncolumns_ = df_1.iloc[:, 1:].columns[trans.get_support()].values\n\n# X_feature as tranformed top feature variables\nX_feature = pd.DataFrame(trans_feat, columns=columns_)\n\n# Y_label with only target variable\nY_label = Y\n\nX_feature.columns","a959a357":"#1. Univariate analysis [Each feature individually]\n#Plotting all features stacked\ndf.plot.area(stacked=False,figsize=(15,5))\npylab.grid(); pylab.show()","a0c84fe9":"#Histplot for eac feature\ndef plot_draw(df, cols=5, width=10, height=10, hspace=0.2, wspace=0.5):\n    \"\"\"Ploting the individual feature histplot\"\"\"\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width,height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(float(df.shape[1]) \/ cols)\n    for i, column in enumerate(df.columns):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.set_title(column)\n        if df.dtypes[column] == np.object:\n            g = sns.countplot(y=column, data=df)\n            substrings = [s.get_text()[:18] for s in g.get_yticklabels()]\n            g.set(yticklabels=substrings)\n            plt.xticks(rotation=25)\n        else:\n            g = sns.distplot(df[column])\n            plt.xticks(rotation=25)\n    \nplot_draw(df, cols=3, width=20, height=10, hspace=0.45, wspace=0.5)","1a99ae49":"#2. Bivariate analysis [Pairwise]\n#This is not needed for our dataset perspective but for visulization purpose let's plot\nsns.set(style=\"ticks\")\ngraph = sns.pairplot(df,corner=True,kind='reg')\ngraph.fig.set_size_inches(15,10)","fb960722":"#Let's check central tendency for each feature\ndf.agg(['median','mean','std']).round(2)","17294e8f":"#Plotting box plots for outliers, std values shown above\nplt.figure(figsize=(15,5))\nsns.boxplot(x='variable', y='value', data=df.melt())\nplt.show()","59960039":"#Outlier detection, measure in percentage\nnum_col = df.columns.tolist()\n#Function to detect the outliers using IQR\ndef outlier_count(col, data=df):\n    #q75, q25 = np.percentile(data[col], [25, 75])\n    # calculate the interquartile range(Q1,Q3)\n    Q1 = data[col].quantile(0.25)\n    Q3 = data[col].quantile(0.75)\n    IQR = Q3 - Q1\n    min_val = Q1 - (IQR*1.5)\n    max_val = Q3 + (IQR*1.5)\n    #Finding the length of data that is more than max threshold and lesser than min threshold\n    outlier_count = len(np.where((data[col] > max_val) | (data[col] < min_val))[0])\n    outlier_percent = round(outlier_count\/len(data[col])*100, 2)\n    print('{:<20} {:<20} {:.2f}%'.format(col,outlier_count,outlier_percent))\n    \n#Looping over all the numerical columns to outlier count function to find the total count of outliers in data.\nprint(\"\\n\"+20*'*' + ' Outliers ' + 20*'*'+\"\\n\")\nprint('{:<20} {:<20} {:<20}'.format('Variable Name','Number Of Outlier','Outlier(%)'))\nfor col in num_col:\n    outlier_count(col)","342aaf1e":"#Using function applying winsorize technique to cap the outliers and adding the new winsorized column to winsor_dict \n# which can be used for futher implementation.\ndef winsor(col, lower_limit=0, upper_limit=0, show_plot=True):\n    \n    #Using scipy.stats.mstats.winsorize to each column\n    winsor_data = winsorize(df[col], limits=(lower_limit, upper_limit))\n    \n    #Assigning the winsorized data from each column to  dict\n    winsor_dict[col] = winsor_data\n    \n    #Using box plot, visializing the data to check the outliers before and after winsorizing\n    if show_plot == True:\n        plt.figure(figsize=(10,3))\n        \n        #draw plot with original dataset\n        plt.subplot(121)\n        plt.boxplot(df[col])\n        plt.title('Original {}'.format(col))\n        \n        #draw plot with winsorized dataset\n        plt.subplot(122)\n        plt.boxplot(winsor_data)\n        \n        #assigning titile to the plot\n        plt.title('Winsorized {}'.format(col))\n        plt.show()\n\n\n#Creating an empty dict to load all the winsorised data\nwinsor_dict = {}\n\n#From the analysis found from the box plot, based on the outliers position, \n#various limit has been experimented to limit the outlier count. \n\n#In boxplot 2 ['Fresh'], It is seen that the outliers are in the upper boundanday of the plot,\nwinsor(num_col[2], upper_limit = 0.0455, show_plot=True)\n\n#In boxplot 3 ['Milk'], It is seen that the outliers are in the upper boundanday of the plot,\nwinsor(num_col[3], upper_limit = 0.067, show_plot=True)\n\n#In boxplot 4 ['grocery'], It is seen that the outliers are in the upper boundanday of the plot,\nwinsor(num_col[4], upper_limit = 0.06, show_plot=True)\n\n#In boxplot 5 ['Frozen'], It is seen that the outliers are in the upper boundanday of the plot,\nwinsor(num_col[4], upper_limit = 0.0977, show_plot=True)\n\n#In boxplot 6 ['Detergents_Paper'], It is seen that the outliers are in the upper boundanday of the plot,\nwinsor(num_col[4], upper_limit = 0.0682, show_plot=True)\n\n#In boxplot 7 ['Delicassen'], It is seen that the outliers are in the upper boundanday of the plot,\nwinsor(num_col[4], upper_limit = 0.0614, show_plot=True)","e0a0a43c":"#Let's check the statistical distribution of the variables.\nshapiro_test = {}\nj=2\nplt.figure(figsize=(20, 10))\nfor i in range(0,6):\n    ax = plt.subplot(2,3,i+1)\n    probplot(x = df[df.columns[j]], dist=norm, plot=ax)\n    plt.title(df.columns[j])\n    shapiro_test[df.columns[j]] = shapiro(df[df.columns[j]])\n    j += 1\n    \nplt.show()\n\npd.DataFrame(shapiro_test, index=['Statistic', 'p-value']).transpose()","a5549290":"#All the variable are statistically non normally distributed.Let's try BoxCox transformation\nshapiro_test = {}\nlambdas = {}\nj=2\nplt.figure(figsize=(20, 10))\nfor i in range(6):\n    ax = plt.subplot(2,3,i+1)\n    x, lbd = boxcox(df[df.columns[j]])\n    probplot(x = x, dist=norm, plot=ax)\n    plt.title(df.columns[j])\n    shapiro_test[df.columns[j]] = shapiro(x)\n    lambdas[df.columns[j]] = lbd\n    j+=1\n    \nplt.show()\n\npd.DataFrame(shapiro_test, index=['Statistic', 'p-value']).transpose()","798b54d4":"#Using standard scalaer let's Transform & Normalize the data and visulize it.\nsc=StandardScaler()\nscaled_data=sc.fit_transform(df)\n\nnorm_data=normalize(df)\n\ndf=pd.DataFrame(scaled_data,columns=df.columns)\ndf_SN=pd.DataFrame(norm_data,columns=df.columns)\ndf_SN.head()","d161f7f0":"#using 5 folds split the data.\nskf = KFold(n_splits=5)\nfor train_index,test_index in skf.split(X_feature, Y_label):\n#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X_feature.loc[train_index],X_feature.loc[test_index]\n    Y_train, Y_test = Y_label.loc[train_index],Y_label.loc[test_index]\n\n# from sklearn.model_selection import train_test_split\n# X_train, X_test, Y_train, Y_test = train_test_split(X_feature, Y_label, test_size = 0.3, random_state = 0)\n\nprint('{:<15} {:<15} {:<15}'.format('DataSet','Features','Label'))\nprint('{:<15} {:<15} {:<15}'.format('Train',X_train.shape[0],Y_train.shape[0]))\nprint('{:<15} {:<15} {:<15}'.format('Test',X_test.shape[0],Y_test.shape[0]))\n","51b5bc9c":"# Dataframe to store Model Performance Results\nperf_cols = ['Model','Train Accuracy','Test Accuracy','F1-Score','Recall','Precision', 'AUC']\ndf_perf = pd.DataFrame(columns = perf_cols)\n\n# DataFrame for Confusion matrix\nconf_mat_cols = ['Model','False Negatives','False Positives','True Negatives','True Positives']\ndf_cm = pd.DataFrame(columns = conf_mat_cols)\n\n# DataFrame for cross validation scores\ncv_cols = ['Model','Best_Cross_Val_Score']\ndf_cv = pd.DataFrame(columns = cv_cols)","aa53777e":"# Function to print Confusion Matrix and Classification Report\ndef model_perf(model, Y_test, Y_predict, algorithm, df_cm, df_perf, ax=None, plot = True):\n    # confusion_matrix\n    conf_mat = confusion_matrix(Y_test, Y_predict)\n    \n    # get accuracy of model\n    acc_score = accuracy_score(Y_test, Y_predict)\n    print(f\"Accuracy of {algorithm} for Test data is {acc_score*100}\\n\")\n\n    # get F1-score of model\n    f1score = f1_score(Y_test, Y_predict) \n    print(f\"F1-score of {algorithm} for Test data is {f1score*100}\\n\")\n    \n    # get the classification report\n    class_report = classification_report(Y_test, Y_predict)\n    print(f\"Classification report for {algorithm} is: \\n {class_report}\")\n    \n    # AUC Calculations - false positive rates, true positive rates and thresholds\n    fpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_predict, pos_label=1)\n    \n    #area_under_curve\n    roc_auc = round(metrics.auc(fpr, tpr)*100,2)\n    print(f\"AUC for {algorithm}: {roc_auc}\\n\")\n    \n    #Train Accuracy score\n    train_acc = round(model.score(X_train,Y_train) * 100,2)\n\n    #Test Accuracy score\n    test_acc = round(model.score(X_test,Y_test) * 100,2)\n\n    precision,recall,fscore,support = precision_recall_fscore_support(Y_test,Y_predict)\n\n    #Appending into the dataframe\n    df_perf = df_perf.append({'Model' : algorithm,'Train Accuracy' : train_acc,'Test Accuracy' : test_acc,\n                          'F1-Score' : fscore[1],'Recall' : recall[1], 'Precision' : precision[1], 'AUC' : roc_auc}, ignore_index=True)\n\n    df_cm = df_cm.append({\"Model\" : algorithm, \"True Positives\" : conf_mat[1][1], \"True Negatives\" : conf_mat[0][0],\n                      \"False Positives\" : conf_mat[0][1], \"False Negatives\" : conf_mat[1][0]}, ignore_index=True, sort=False)\n\n##########################################--PLOT---###########################################\n    if plot:\n        def conf_plot1(ax = None):\n            if ax is None:\n                fig, ax = plt.subplots()\n            # For label annotations in confusion_matrix\n            label_names = ['True -ve','False +ve','False -ve','True +ve']\n            label_counts = ['{0:0.0f}'.format(value) for value in conf_mat.flatten()]\n            labels = [f'{v1}\\n{v2}' for v1, v2 in zip(label_names,label_counts)]\n            labels = np.asarray(labels).reshape(2,2)\n\n            # Draw heatmap using confusion matrix\n            sns.heatmap(conf_mat, cmap = cmap, annot=labels, fmt='')\n            ax.set_xlabel('Actual Values')\n            ax.set_ylabel('Predicted Values')\n            #ax.show()\n\n        #Line plot for ROC curve using fpr and tpr value\n        def roc_plot2(ax = None):\n            if ax is None:\n                fig, ax = plt.subplots()\n            ax.plot(fpr, tpr, color='red', label = 'AUC = %0.3f' % roc_auc)  \n            ax.set_title('Receiver Operating Characteristic (ROC)')    \n            ax.legend(loc = 'lower right')\n            ax.plot([0, 1], [0, 1],linestyle='--') #Intersection line\n            ax.set_xlabel('False Negative Rate')\n            ax.set_ylabel('True Positive Rate')\n            ax.set_xlim([0,1])\n            ax.set_ylim([0,1])\n            ax.set_xticks([i for i in np.arange(0,1.1,0.1)])\n            ax.set_yticks([i for i in np.arange(0,1.1,0.1)])\n        \n        prec, rec, thres = precision_recall_curve(Y_test, Y_predict)\n        prec, rec, thres = list(prec), list(rec), list(thres)\n        prec.pop()\n        rec.pop()\n            \n        def rec_plot3(ax = None):\n            if ax is None:\n                fig, ax = plt.subplots()\n            #Plot Precision-Recall curve\n            fig, axis = (None, ax) if ax else plt.subplots()\n            axis_twin = axis.twinx()\n\n            #Threshold vs Precision\n            sns.lineplot(x = thres, y = prec, label='Precision', ax=axis)\n            axis.set_xlabel('Threshold')\n            axis.set_ylabel('Precision')\n            axis.legend(loc='lower left')\n\n            #Threshold vs Recall\n            sns.lineplot(x = thres, y = rec, color='limegreen', label='Recall', ax=axis_twin)\n            axis_twin.set_ylabel('Recall')\n            axis_twin.set_ylim(0, 1)\n            axis_twin.legend(bbox_to_anchor=(0.32, 0.20),loc='lower right')\n\n            axis.set_xlim(0, 1)\n            axis.set_ylim(0, 1)\n            axis.set_title('Precision Vs Recall')\n        \n                    \n        fig = plt.figure(figsize = (15,4))\n        ax1 = fig.add_subplot(1,3,1)\n        conf_plot1(ax1)\n        ax2 = fig.add_subplot(1,3,2)\n        roc_plot2(ax2)\n        axis = fig.add_subplot(1,3,3)\n        rec_plot3(axis)\n    \n    \n    return df_cm, df_perf","5ece68d9":"#Choosing K value\nerror_rate = []\n\nfor i in range(1,30):\n    KNN_class = KNeighborsClassifier(n_neighbors = i) \n    KNN_class.fit(X_train, Y_train)\n    Y_predict = KNN_class.predict(X_test)\n    #appending error rate that is not equal to test dataset\n    error_rate.append(np.mean(Y_predict != Y_test))\n    \n#Plotting the plot to check the K value from graph\nplt.figure(figsize = (12,6))\nplt.plot(range(1,30), error_rate, color = 'blue', linestyle = 'dashed', marker = 'o', markerfacecolor = 'yellow', markersize = 10)\nplt.title('Error Value vs K-Value')\nplt.xlabel('K-Value')\nplt.ylabel('Error Value')   ","7057fa77":"#As per above graph it seems neig\nKNN_class = KNeighborsClassifier(n_neighbors=7)\n\nKNN_class.fit(X_train, Y_train)\n\nY_predict = KNN_class.predict(X_train)\n\nprint(\"Train Data Model Accuracy: {0:.4f}\".format(metrics.accuracy_score(Y_train, Y_predict)*100))\n\nY_predict = KNN_class.predict(X_test)","5f7be7c1":"#Draw Model Performace, Confusion Matrix and Classification Report for KNN\ndf_cm, df_perf = model_perf(KNN_class, Y_test, Y_predict, \"KNN\", df_cm, df_perf, plot = True)","74d59484":"knn_score = cross_val_score(KNeighborsClassifier(), X_train, Y_train, cv= 3,verbose = True, scoring = 'accuracy')\nprint(\"Accuracy for K-Neighbors Classifier using Cross Validation: %0.3f (+\/- %0.3f)\" \n      % (knn_score.mean(), knn_score.std() * 2))\n\ndf_cv = df_cv.append({'Model':'KNeighborsClassifier','Best_Cross_Val_Score':\"%0.2f (+\/- %0.2f)\" \n                      % ((knn_score.mean(), knn_score.std() * 2))},ignore_index=True, sort=False)","0ff858ac":"SVM = SVC(kernel='linear')\n\nSVM.fit(X_train, Y_train)\n\nY_predict = SVM.predict(X_train)\n\nprint(\"Train Data Model Accuracy: {0:.3f}\".format(metrics.accuracy_score(Y_train, Y_predict)*100))\n\nY_predict = SVM.predict(X_test)","73a5c583":"#Draw Model Performace, Confusion Matrix and Classification Report for SVM\ndf_cm, df_perf = model_perf(SVM, Y_test, Y_predict, \"SVM\", df_cm, df_perf, plot = True)","1ca03e4c":"svm_score = cross_val_score(SVC(kernel='linear'), X_train, Y_train, cv= 3,verbose = True, scoring = 'accuracy')\nprint(\"Accuracy for SVM using Cross Validation: %0.3f (+\/- %0.3f)\" \n      % (svm_score.mean(), svm_score.std() * 2))\n\ndf_cv = df_cv.append({'Model':'SVM','Best_Cross_Val_Score':\"%0.2f (+\/- %0.2f)\" \n                      % ((svm_score.mean(), svm_score.std() * 2))},ignore_index=True, sort=False)","1b061a57":"GNB = GaussianNB()\n\nGNB.fit(X_train, Y_train)\n\nY_predict = GNB.predict(X_train)\n\nprint(\"Train Data Model Accuracy: {0:.3f}\".format(metrics.accuracy_score(Y_train, Y_predict)*100))\n\nY_predict = GNB.predict(X_test)","d41981ea":"#Draw Model Performace, Confusion Matrix and Classification Report for Naive Bayes\ndf_cm, df_perf = model_perf(GNB, Y_test, Y_predict, \"Naive Bayes\", df_cm, df_perf, plot = True)","5faa9858":"gnb_score = cross_val_score(GaussianNB(), X_train, Y_train, cv= 3, verbose = True, scoring = 'accuracy')\nprint(\"Accuracy for GaussianNB using Cross Validation: %0.3f (+\/- %0.3f)\" \n      % (gnb_score.mean(), gnb_score.std() * 2))\n\ndf_cv = df_cv.append({'Model':'GaussianNB','Best_Cross_Val_Score':\"%0.2f (+\/- %0.2f)\" \n                      % ((gnb_score.mean(), gnb_score.std() * 2))},ignore_index=True, sort=False)","9164119d":"#Results for All three models comparision\n# print('Confusion Matrix of all the models:')\n# df_cm\n\n# print('Performance metrics of all the models:')\n# df_perf.round(2)\n\n# print('Cross Validation Scores of all the models:')\n# df_cv\n\nresult = pd.concat([df_perf.round(2), df_cv['Best_Cross_Val_Score'], df_cm.drop('Model', axis=1)], axis=1, sort=False)\nresult","e7aef0fb":"#As we already have our scaled data ready, lets do principle component analysis\n#and print elbow plot to determine the optimal number of clusters.\n\nPCA_train = PCA(2).fit_transform(scaled_data)\nps = pd.DataFrame(PCA_train)\n\nle = {}\nfor k in range(2,11):\n    kmeans = KMeans(n_clusters = k, random_state=123)\n    Y_label = kmeans.fit_predict(X_feature)\n    le[k] = kmeans.inertia_\n   \nplt.figure(figsize=(10,5))\nplt.title('Elbow Plot')\nsns.pointplot(x = list(le.keys()), y = list(le.values()))\n\nplt.show()","27782421":"#Let's parallely plot the heatmap and scatter plot to see the segmentation\n#Cluster=3\nkmeans = KMeans(n_clusters=3, random_state=123).fit(ps)\ny_kmeans = kmeans.predict(ps)\ndf = df.assign(segment = kmeans.labels_)\nkmeans_3_means = df.drop(['Channel','Region'], axis=1).groupby('segment').mean()\n\nlab = kmeans.labels_\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.heatmap(kmeans_3_means.T, cmap=cmap)\n\nplt.subplot(1,2,2)\nplt.scatter(ps[0], ps[1],c = y_kmeans, s=80, cmap='rainbow',alpha=0.7)\nplt.scatter(kmeans.cluster_centers_[:,0] ,kmeans.cluster_centers_[:,1], marker = '*', color='black', s=200)","77701d8e":"#Let's parallely plot the heatmap and scatter plot to see the segmentation\n#Cluster=4\nkmeans = KMeans(n_clusters=4, random_state=123).fit(ps)\ny_kmeans = kmeans.predict(ps)\ndf = df.assign(segment = kmeans.labels_)\nkmeans_3_means = df.drop(['Channel','Region'], axis=1).groupby('segment').mean()\n\nlab = kmeans.labels_\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.heatmap(kmeans_3_means.T, cmap=cmap)\n\nplt.subplot(1,2,2)\nplt.scatter(ps[0], ps[1],c = y_kmeans, s=80, cmap='rainbow',alpha=0.7)\nplt.scatter(kmeans.cluster_centers_[:,0] ,kmeans.cluster_centers_[:,1], marker = '*', color='black', s=200)","d792197c":"#Let's parallely plot the heatmap and scatter plot to see the segmentation\n#Cluster=5\nkmeans = KMeans(n_clusters=5, random_state=123).fit(ps)\ny_kmeans = kmeans.predict(ps)\ndf = df.assign(segment = kmeans.labels_)\nkmeans_3_means = df.drop(['Channel','Region'], axis=1).groupby('segment').mean()\n\nlab = kmeans.labels_\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.heatmap(kmeans_3_means.T, cmap=cmap)\n\nplt.subplot(1,2,2)\nplt.scatter(ps[0], ps[1],c = y_kmeans, s=80, cmap='rainbow',alpha=0.7)\nplt.scatter(kmeans.cluster_centers_[:,0] ,kmeans.cluster_centers_[:,1], marker = '*', color='black', s=200)","19baf865":"#Agglomerative Clustering\nagc = AgglomerativeClustering(n_clusters=3,affinity='euclidean',linkage='ward')\ny_agc_pred = agc.fit_predict(ps)\nplt.figure(figsize =(18,5))\n\nplt.subplot(1,2,1)\nplt.scatter(ps[0], ps[1],c = y_agc_pred, s=80, cmap=cmap,alpha=0.6,marker='s')\n\nplt.subplot(1,2,2)\ndend=shc.dendrogram(shc.linkage(ps,method='ward') ,truncate_mode='level', p=3) \nplt.show()\n","d978049e":"#Birch clustering\nbrc = Birch(branching_factor=500, n_clusters=3, threshold=1.5)\nbrc.fit(ps)\nlabels = brc.predict(ps)\nplt.figure(figsize =(18,5))\n\nplt.subplot(1,2,1)\nplt.scatter(ps[0], ps[1], c=labels, cmap='brg',alpha=0.6,marker='D')\n\n#MiniBatchKMeans\nmb = MiniBatchKMeans(n_clusters=3, random_state=0)\nmb.fit(ps)\nlabels = mb.predict(ps)\n\nplt.subplot(1,2,2)\nplt.scatter(ps[0], ps[1], c=labels, cmap='brg',alpha=0.6,marker='D')\n\n\nplt.show()","64535e5b":"end = time. time()\nsec = (end - start)\nprint(f'Total time taken to complete the execution :{sec} seconds(s)')","c1b7a3f4":"Function for Model Performance, Confusion Matrix and Classification Report with Plots","a728e6eb":"<u>**5 Ways to Handle Missing Values**<\/u><br><br>\n**1.Deleting Rows:**<i> Only 440 rows deletion will cause data-loss. 8 features deletion of higher null value feature will add bias. This won't help.<\/i><br>\n**2.Replacing With Mean\/Median\/Mode\/Interpolate:**<i> This can help here as spending on product can be in range. <\/i><br>\n**3.Assigning An Unique Category:**<i> Dataset doesn't have unique categories except region & Channel.. Adding new category make sense here as 2 and 3 unique value respectively value as new '4' as new region and channel can be added.<\/i><br>\n**4.Predicting The Missing Values:**<i> Using ML algorithm predict won't help here as with 440 rows still making our ready for fitting into model. <\/i> <br>\n**5.Using Algorithms Which Support Missing Values:**<i> Here we are going for Clustering & Classification already. For null values it won't be suitable. <\/i> ","955c25c3":"# [4] Unsupervised learning","70bc124f":"## [3.1] K Neighbors Classifier","c515bb32":"# [3] Supervised learning","b845df87":"After bivariance analysis, we need to transform skewed distribution to normalise the data.","131eb2ce":"**Feature Transformation & Normalization**","d1fd2959":"# Wholesale Customers | Segmentation","76fdc411":"**Using GenericUnivariateSelect** <br> It works by selecting the best features based on univariate statistical tests and allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator.Select KBest removes all but the  highest scoring features.Select Percentile removes all but a user-specified highest scoring percentage of features.<br>\nIn our dataset only 7 independent features, let see how many relevent here.","5701f69d":"# [2] Data visualization","f8df6632":"It estimates mutual information for a discrete target variable between two random variables which is a non-negative value, \nwhich measures the mutual information between a matrix containing a set of feature vectors and the target.\n\nIt is equal to zero if and only if two random variables are dependent, and higher values mean higher dependency.","1c87471b":"Dataframe to store Model Performance Results","ddde95b1":"**Using ExtraTreesClassifier**","c6dfc4e4":"Here, we can try 3, 4 or 5 clusters as per above. strong declination plot.","b9b0e704":"### b) Are there any missing values in the dataset? How to handle that ","d0203774":"As per box plot and above result, we have outliers in our dataset but, removing outlier will cause data loss. We can't afford data loss for such small dataset. Instead of removing let's apply winsorize to cap the outliers here.","fcdd8197":"### - Use Data visualization techniques to plot in graphs","46072658":"### - Use a measure of central tendency for each feature","6dfb9858":"## [3.2] Support Vector Machines (SVM)","bd46b5da":"### - Use the training data set to train the classification mode.\n### - Observe the performance of the model with test data set.\n### - Create a confusion matrix to present the result.\n","a4cdf524":"For using K-Means algorithm. Let's determine the optimal value of clusters here.","17bf2037":"## [3.4] Result Analysis","fbb7a58a":"### - Explore random split of data as test and training set using Python.","341e8e4f":"As per above. Null values can be imputed.","7db201ca":"### - Show the dispersion (standard deviation and IQR) of features","376655f2":"For supervised learning, 'Channel' is the target variable here. Lets split the dataset.","96011859":"We have normalized the dataset. but, here, for modeling purpose for our dataset, it would be better to go without normalization.","82f3d483":"**Abstract**<br>\n<i>The data set refers to clients of a wholesale distributor. It includes the annual spending in monetary units (m.u.) on diverse product categories.<\/i><br><br>\n**Data Source**<br> \nUCI Machine Learning Repository<br><br>\n**Origin**<br>\nMargarida G. M. S. Cardoso, margarida.cardoso '@' iscte.pt, ISCTE-IUL, Lisbon, Portugal\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Wholesale+customers<br><br>\n**Attribute Information**\n1) FRESH: annual spending (m.u.) on fresh products (Continuous); <br>\n2) MILK: annual spending (m.u.) on milk products (Continuous);<br>\n3) GROCERY: annual spending (m.u.)on grocery products (Continuous);<br>\n4) FROZEN: annual spending (m.u.)on frozen products (Continuous)<br>\n5) DETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous)<br>\n6) DELICATESSEN: annual spending (m.u.)on and delicatessen products (Continuous);<br>\n7) CHANNEL: customers\u00e2\u20ac\u2122 Channel - Horeca (Hotel\/Restaurant\/Caf\u00c3\u00a9) or Retail channel (Nominal)<br>\n8) REGION: customers\u00e2\u20ac\u2122 Region \u00e2\u20ac\u201c Lisnon, Oporto or Other (Nominal)<br>\n\n**Purpose**<br>\n<i>Data preprocessing, visualization and using Supervised & Unsupervised learning model to discover different segments of customers.<\/i><br><br>","06f492e2":"**Using Mutual_info_classif**","c30a9418":"### a) Load the data set into a data frame  ","076f6a42":"## [3.3] Naive Bayes","80b6db3d":"Let's start exploring data\n1. Univariate analysis [Each feature individually]\n2. Bivariate analysis [Pairwise]","15b0d65b":"Let's try other kind of clustering.","c5547cd5":"### c) Use feature selection and pruning techniques.","cb1d62f3":"<u>**Two Ways For Feature Selection**<\/u><br><br>\n**1.Unsupervised:**<i> Using Correlation and removing redundant variables. We don't have any redundant variable here.<\/i><br>\n**2.Supervised:**<i> Removing irrelevant variables.Region & Channel seems irrelevant here. <\/i><br>\n<i>Wrapper [RFE], Filter-relationship with targetvarible [Statistical Methods, Feature Importance Methods], Intrinsic [Decision Trees]. Here, we are clustering & classifying segments, none defined target variables, so this method won't help.<\/i>","e0f00b0f":"# [1] Data preprocessing","b85b37a0":"**Using SelectKBest**","fdb3da60":"References<br>\nhttps:\/\/www.kaggle.com\/gfesta\/wholesale-customer-segmentation<br>\nhttps:\/\/www.kaggle.com\/olgabelitskaya\/customer-segments"}}