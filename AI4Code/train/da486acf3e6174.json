{"cell_type":{"a4232bad":"code","9b89306a":"code","b7ed84b0":"code","e788a462":"code","85aab13d":"code","1d94d8e0":"code","9f042dee":"code","90ffa71c":"code","9907b0ca":"code","1c459bf4":"code","5f50ee97":"code","61adb359":"code","3e0360ff":"code","928b034f":"code","67fab756":"code","4b1548da":"code","569e7562":"code","4e771363":"code","734c777a":"code","6abf0eb1":"code","19628ffa":"code","a12b4ac8":"code","804f3552":"code","545437d4":"code","42f5d4f5":"code","c81ebfaa":"code","d2fe7000":"code","4d5b8b8f":"code","9902a733":"code","acdb8779":"code","4c26cbc8":"code","d35579e2":"code","2bcd5d56":"code","a99855fe":"code","c3fd25cd":"code","4e467ec6":"code","a4d3c82d":"code","3923152a":"code","c4902416":"markdown","06d9d76a":"markdown","3a40f710":"markdown","c86bfb39":"markdown","640b6bc4":"markdown","caeb3c79":"markdown","6d7b26ab":"markdown","4f85ee6d":"markdown","441dc382":"markdown","cf8e3ec7":"markdown","faf0bffa":"markdown","a580fbec":"markdown","774f3076":"markdown","1d9709b2":"markdown","177951fd":"markdown","8b568e49":"markdown","96be3d16":"markdown","dccdae11":"markdown","19293a28":"markdown","04dc2c50":"markdown","287c1f5c":"markdown","c79ffa4e":"markdown","aa91463e":"markdown","c978f9f6":"markdown","59dbcb29":"markdown","30b3f0ab":"markdown","73b6472b":"markdown","b3eaf793":"markdown","33287b3a":"markdown","44914b46":"markdown","d1ea1bee":"markdown","771d0c86":"markdown","1dcae3f9":"markdown","12f185ae":"markdown","67a4cc7e":"markdown","96f1fc8f":"markdown","18bddb0d":"markdown","cc1da61a":"markdown","5e4e2a29":"markdown","cea1e767":"markdown","5f0c5211":"markdown","0192dca4":"markdown","ed32cca4":"markdown","a0b852b0":"markdown"},"source":{"a4232bad":"import pandas as pd\nimport numpy as np\nimport string\nimport re\nimport nltk\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import naive_bayes\nfrom sklearn import svm\nfrom sklearn import metrics\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.utils import resample\nfrom sklearn.ensemble import VotingClassifier\n!pip install Unidecode\n\nnltk.download('punkt')\nnltk.download('words')\nnltk.download('stopwords')\n\nimport unidecode","9b89306a":"path= '..\/input\/boardgamegeek-reviews\/bgg-13m-reviews.csv'\n\nreviews=pd.read_csv(path, usecols = ['comment','rating'],engine='python')","b7ed84b0":"reviews.head()","e788a462":"reviews = reviews.dropna()","85aab13d":"def deep_cleaner(comment):\n  clean_number = re.compile(r'^(\\d|-)?(\\d|,)*\\.?\\d*$')\n  clean_html = re.compile(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)')\n  clean_url = re.compile(r'https?:\/\/\\S+\\www\\.\\S+')\n  clean_punctuation = re.compile(r'[^\\w\\s]+')\n  comment = clean_number.sub(r'',comment)\n  comment = clean_html.sub(r'', comment)\n  comment = clean_url.sub(r'', comment)\n  comment = clean_punctuation.sub(r'',comment)\n  comment = unidecode.unidecode(comment)\n  \n  return comment\n\n\nreviews[\"comment\"] = reviews[\"comment\"].apply(lambda comment: deep_cleaner(comment))","1d94d8e0":"reviews[\"comment\"].replace('', np.nan, inplace=True)\n#removing rows with NaN\nreviews.dropna(inplace =True)","9f042dee":"words = set(nltk.corpus.words.words())\n\ndef onlyEnglish(comment):\n  return \" \".join(w for w in nltk.wordpunct_tokenize(comment) if w.lower() in words or not w.isalpha())\n\nreviews[\"comment\"] = reviews[\"comment\"].apply(lambda comment: onlyEnglish(comment))","90ffa71c":"stop_words = set(stopwords.words('english'))\n\ndef noStopwords(comment):\n  return \" \".join(w for w in nltk.wordpunct_tokenize(comment) if w not in stop_words)\n\nreviews[\"comment\"] = reviews[\"comment\"].apply(lambda comment: noStopwords(comment))","9907b0ca":"reviews['rating'].value_counts()","1c459bf4":"reviews['rating'] = reviews['rating'].astype(np.int64)\n\nreviews['rating'].value_counts(sort=False).plot(kind = 'bar')\nplt.ylabel('Frequency')\nplt.xlabel('Rating')\nplt.show()","5f50ee97":"reviews = reviews[reviews['rating'] >= 1]\n\nreviews['rating'].value_counts(sort=False).plot(kind = 'bar')\nplt.ylabel('Frequency')\nplt.xlabel('Rating')\nplt.title('After removal of zero')\nplt.show()","61adb359":"reviews = reviews.reset_index(drop=True)\nreviews.head()","3e0360ff":"sample = reviews.sample(n=100000)\nsample = sample.reset_index(drop=True)","928b034f":"sample['rating'].value_counts(sort=False).plot(kind = 'bar')\nplt.ylabel('Frequency')\nplt.xlabel('Rating')\nplt.title('Sample of 100K')\nplt.show()","67fab756":"X_train, X_test, y_train, y_test = train_test_split(sample['comment'], sample['rating'], test_size=0.2,random_state=np.random)","4b1548da":"vectorizer = TfidfVectorizer(\n    ngram_range=(1, 2), \n    encoding='latin-1',\n    max_features = 500000\n)\n\nvectorizer.fit(X_train)\nX_train_vec = vectorizer.transform(X_train)\nX_test_vec = vectorizer.transform(X_test)","569e7562":"NB = naive_bayes.MultinomialNB()\nNB.fit(X_train_vec,y_train)\n\npredictions_NB = NB.predict(X_test_vec)\n\n\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(predictions_NB,y_test )*100)","4e771363":"conf_matrix_NB = confusion_matrix(y_test, predictions_NB)\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(conf_matrix_NB.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('Original')\nplt.ylabel('Predicted');","734c777a":"alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\nmodel_NB = naive_bayes.MultinomialNB()\ngrid = GridSearchCV(cv= 5, estimator=model_NB, param_grid=dict(alpha=alphas))\ngrid.fit(X_train_vec, y_train)\n#print(grid)\n# summarize the results of the grid search\nprint(grid.best_score_)\nprint(grid.best_estimator_.alpha)","6abf0eb1":"Linear_SVC = svm.LinearSVC()\nLinear_SVC.fit(X_train_vec,y_train)\n\npredictions_SVM = Linear_SVC.predict(X_test_vec)\n\n\nprint(\"SVM Accuracy Score -> \",metrics.accuracy_score(predictions_SVM, y_test)*100)","19628ffa":"conf_matrix_SVM= confusion_matrix(y_test, predictions_SVM)\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(conf_matrix_SVM.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('Original')\nplt.ylabel('Predicted');","a12b4ac8":"#from sklearn.svm import SVC  \n#params = {'C': [0.1,1, 10, 100]}\n#model_SVM = svm.LinearSVC()\n#grid = GridSearchCV(estimator = model_SVM, param_grid=params, cv= 5 )\n#grid.fit(X_train_vec, y_train)\n#print(grid)\n# summarize the results of the grid search\n#print(grid.best_score_)\n#print(grid.best_estimator_)","804f3552":"reviews['rating'].value_counts()","545437d4":"reviews_1 = reviews[reviews['rating'] == 1]\nreviews_2 = reviews[reviews['rating'] == 2]\nreviews_3 = reviews[reviews['rating'] == 3]\nreviews_4 = reviews[reviews['rating'] == 4]\nreviews_5 = reviews[reviews['rating'] == 5]\nreviews_6 = reviews[reviews['rating'] == 6]\nreviews_7 = reviews[reviews['rating'] == 7]\nreviews_8 = reviews[reviews['rating'] == 8]\nreviews_9 = reviews[reviews['rating'] == 9]\nreviews_10 = reviews[reviews['rating'] == 10]\n\nreview_1_downsampled = resample(reviews_1, \n                                 replace=False,    # sample without replacement\n                                 n_samples=10000,     # to match minority class\n                                 random_state=np.random) # reproducible result\nreview_2_downsampled = resample(reviews_2, \n                                 replace=False,    # sample without replacement\n                                 n_samples=10000,     # to match minority class\n                                 random_state=np.random) # reproducible results\nreview_3_downsampled = resample(reviews_3, \n                                 replace=False,    # sample without replacement\n                                 n_samples=10000,     # to match minority class\n                                 random_state=np.random) # reproducible results\nreview_4_downsampled = resample(reviews_4, \n                                 replace=False,    # sample without replacement\n                                 n_samples=10000,     # to match minority class\n                                 random_state=np.random) # reproducible results\nreview_5_downsampled = resample(reviews_5, \n                                 replace=False,    # sample without replacement\n                                 n_samples=10000,     # to match minority class\n                                 random_state=np.random) # reproducible results\nreview_6_downsampled = resample(reviews_6, \n                                 replace=False,    # sample without replacement\n                                 n_samples=10000,     # to match minority class\n                                 random_state=np.random) # reproducible results\nreview_7_downsampled = resample(reviews_7, \n                                 replace=False,    # sample without replacement\n                                 n_samples=10000,     # to match minority class\n                                 random_state=np.random) # reproducible results\nreview_8_downsampled = resample(reviews_8, \n                                 replace=False,    # sample without replacement\n                                 n_samples=10000,     # to match minority class\n                                 random_state=np.random) # reproducible results\nreview_9_downsampled = resample(reviews_9, \n                                 replace=False,    # sample without replacement\n                                 n_samples=10000,     # to match minority class\n                                 random_state=np.random) # reproducible results\nreview_10_downsampled = resample(reviews_10, \n                                 replace=False,    # sample without replacement\n                                 n_samples=10000,     # to match minority class\n                                 random_state=np.random) # reproducible results\n\nreviews_downsampled = pd.concat([review_1_downsampled,review_2_downsampled,review_3_downsampled,review_4_downsampled,review_5_downsampled,review_6_downsampled,review_7_downsampled,review_8_downsampled,review_9_downsampled,review_10_downsampled])\nreviews_downsampled.shape","42f5d4f5":"reviews_downsampled['rating'].value_counts(sort=False).plot(kind = 'bar')\nplt.ylabel('Frequency')\nplt.xlabel('Rating')\nplt.title('Undersampled Data Set with Balanced Classes')\nplt.show()","c81ebfaa":"X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(reviews_downsampled['comment'], reviews_downsampled['rating'], test_size=0.2,random_state=np.random)","d2fe7000":"vectorizer.fit(X_train_bal)\nX_train_bal_vec = vectorizer.transform(X_train_bal)\nX_test_bal_vec = vectorizer.transform(X_test_bal)","4d5b8b8f":"NB_Bal = naive_bayes.MultinomialNB()\nNB_Bal.fit(X_train_bal_vec,y_train_bal)\n\npredictions_NB_Bal = NB_Bal.predict(X_test_bal_vec)\n\n\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(predictions_NB_Bal,y_test_bal )*100)","9902a733":"conf_matrix_NB_Bal = confusion_matrix(y_test_bal, predictions_NB_Bal)\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(conf_matrix_NB_Bal.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('Original')\nplt.ylabel('Predicted');","acdb8779":"alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\nmodel_NB_Bal = naive_bayes.MultinomialNB()\ngrid = GridSearchCV(cv= 5, estimator=model_NB_Bal, param_grid=dict(alpha=alphas))\ngrid.fit(X_train_bal_vec, y_train_bal)\n#print(grid)\n# summarize the results of the grid search\nprint(grid.best_score_)\nprint(grid.best_estimator_.alpha)","4c26cbc8":"X_test_vec.shape, X_test_bal_vec.shape","d35579e2":"predictions_NB_Bal_2 = NB_Bal.predict(X_test_vec)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(predictions_NB_Bal_2,y_test )*100)\n\nconf_matrix_NB_Bal_2 = confusion_matrix(y_test, predictions_NB_Bal_2)\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(conf_matrix_NB_Bal.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('Original')\nplt.ylabel('Predicted');","2bcd5d56":"Linear_SVC_Bal = svm.LinearSVC()\nLinear_SVC_Bal.fit(X_train_bal_vec,y_train_bal)\n\npredictions_SVM_bal = Linear_SVC_Bal.predict(X_test_bal_vec)\n\n\nprint(\"SVM Accuracy Score -> \",metrics.accuracy_score(predictions_SVM_bal, y_test_bal)*100)","a99855fe":"conf_matrix_SVM_bal= confusion_matrix(y_test_bal, predictions_SVM_bal)\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(conf_matrix_SVM_bal.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('Original')\nplt.ylabel('Predicted');","c3fd25cd":"predictions_SVM_Bal_2 = Linear_SVC_Bal.predict(X_test_vec)\n\n\nprint(\"SVM Accuracy Score -> \",metrics.accuracy_score(predictions_SVM_Bal_2, y_test)*100)\n\nconf_matrix_SVM_Bal_2 = confusion_matrix(y_test, predictions_SVM_Bal_2)\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(conf_matrix_SVM_Bal_2.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('Original')\nplt.ylabel('Predicted');","4e467ec6":"estimators=[('SVC_Bal', Linear_SVC_Bal), ('SVC', Linear_SVC)]\n\nensemble = VotingClassifier(estimators, voting='hard')\n\n#fit model to training data\nensemble.fit(X_train_vec, y_train)\n\npredictions_Ensemble = ensemble.predict(X_test_vec)\n\n#test our model on the test data\nprint(\"Ensemble SVM Accuracy Score -> \",ensemble.score(X_test_vec, y_test)*100)\n\nconf_matrix_Ensemble = confusion_matrix(y_test, predictions_Ensemble)\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(conf_matrix_Ensemble.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('Original')\nplt.ylabel('Predicted');","a4d3c82d":"def relaxedAccuracy(pred,target):\n  correct = 0\n  for i in range(len(target)):\n    if (target[i]==pred[i] or target[i]==(pred[i]+1) or target[i]==(pred[i]-1)):\n      correct+=1\n\n  return (correct\/len(target))*100","3923152a":"accuracy = relaxedAccuracy(predictions_Ensemble,y_test.values)\nprint(\"Relaxes Ensemble SVC accuracy ->\",accuracy)\naccuracy = relaxedAccuracy(predictions_NB,y_test.values)\nprint(\"Relaxed NB accuracy ->\",accuracy)\naccuracy = relaxedAccuracy(predictions_SVM,y_test.values)\nprint(\"Relaxed SVM accuracy ->\",accuracy)","c4902416":"#Preprocesing the Dataset","06d9d76a":"##Imports","3a40f710":"## References\n\nhttps:\/\/stackoverflow.com\/questions\/50444346\/fast-punctuation-removal-with-pandas\n\nhttps:\/\/stackoverflow.com\/questions\/9662346\/python-code-to-remove-html-tags-from-a-string\n\nhttps:\/\/stackoverflow.com\/questions\/37335598\/how-to-get-the-length-of-a-cell-value-in-pandas-dataframe\n\nhttps:\/\/hackersandslackers.com\/pandas-dataframe-drop\/\n\nhttps:\/\/stackoverflow.com\/questions\/40650065\/removing-http-and-www-from-url-python\/40650141\n\nhttp:\/\/regexlib.com\/Search.aspx?k=number&c=-1&m=5&ps=20\n\nhttps:\/\/www.johndcook.com\/blog\/2019\/01\/09\/projecting-unicode-to-ascii\/\n\nhttps:\/\/www.machinelearningplus.com\/nlp\/lemmatization-examples-python\/\n\nhttps:\/\/stackoverflow.com\/questions\/41290028\/removing-non-english-words-from-text-using-python\n\nhttps:\/\/stackabuse.com\/removing-stop-words-from-strings-in-python\/\n\nhttps:\/\/elitedatascience.com\/imbalanced-classes\n\nhttps:\/\/thispointer.com\/python-pandas-how-to-drop-rows-in-dataframe-by-conditions-on-column-values\/\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.sample.html\n\nhttps:\/\/stackoverflow.com\/questions\/29314033\/drop-rows-containing-empty-cells-from-a-pandas-dataframe\n\nhttps:\/\/stackoverflow.com\/questions\/59224687\/how-to-remove-whole-string-if-it-consists-of-non-english-words-in-python\n\nhttps:\/\/stackoverflow.com\/questions\/38913965\/make-the-size-of-a-heatmap-bigger-with-seaborn\n\nhttps:\/\/medium.com\/@dtuk81\/confusion-matrix-visualization-fc31e3f30fea\n\nhttp:\/\/www.davidsbatista.net\/blog\/2018\/02\/28\/TfidfVectorizer\/\n\nhttps:\/\/medium.com\/@bedigunjit\/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34\n\nhttps:\/\/medium.com\/datadriveninvestor\/an-introduction-to-grid-search-ff57adcc0998\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/naive-bayes-explained\/\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/understaing-support-vector-machine-example-code\/\n","c86bfb39":"Looks like our balanced Linear SVM performed poorly on the unbalanced Dataset, but was able to capture the lower ratings on the Data set. Accuracy dropped more than 50%","640b6bc4":"###Vectorization\n\nWe now convert our sample['comments'] into numerical feature vectors. The method of our choice is TF-IDF, which is Term Frequency-Inverse Document Frequency.","caeb3c79":"##Loading the Dataset\n\nFor text classification, the comment and rating columns were useful, hence only those were read from the csv file. \n\n*This in turn might help with minimizing running time, and in-memory storage of the Dataframe","6d7b26ab":"##Classifiers","4f85ee6d":"As we can see, our dataset has almost 13M+ rows. We need to pre-process the data to refine our dataset, as much as possible, without influencing the end result.","441dc382":"###Naive Bayes Classifier\n\nIt is a classification technique based on Bayes\u2019 Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n\nOur goal is multi-class text classification. A well known algorithm for text classification is Naive Bayes, which takes vector inputs from TF-IDF and predicts its class.","cf8e3ec7":"Best estimate of alpha, according to our grid search, that used 5 fold cross validation was: 0.1\n","faf0bffa":"### Final Data Set","a580fbec":"The ensemble voting classifer with hard voting shows an accuracy of 27% approximately\n\n### Relaxation of Accuracy Measure\n\nUptill now, only count a rating to be a correct prediction, if its equal to the original rating. \nIf we relax this measure of accuracy, by +\/-1 point, we might see an increment in accuracy.\n\nThe sentiment of a rating from a 7 to an 8 isnot that drastic, that it might change the meaning behind the rating.","774f3076":"As we can see, the rating column has float values, and they also have very low frequencies, which will complicate our problem by increasing the number of classes to more than 10. \n\nTo tackle this problem we can simply cast the whole column as an int type, and discretize them","1d9709b2":"*We can have approximately 10,000 samples from each class, with undersampling, as the class with lowest count '1' has 20K+ records.\nThis will result in a Data set of 100K+ records","177951fd":"##Conclusion:\n\nEnsemble SVM, which contained had Linear SVMs from both balanced and imbalanced dataset, had the highest accuracy in our study. It manages the class imbalance, while maintaining somewhat satisfactory prediction ratings.","8b568e49":"Now that our Naive Bayes classifier can classify lower ratings, lets look at how it performs on the original unbalanced Data set","96be3d16":"Now that our Linear SVC classifier can classify lower ratings, lets look at how it performs on the original unbalanced Data set","dccdae11":"## Handling Imbalanced Classes\n\nAs we can see from the results of Naive Bayes, the classifier's predictions for lower ratings are poor. Imbalanced Data set can be attributed for this result, as their were lesser number of comments with lower ratings.\n\nWe can perform undersampling on this data, as we have a huge Data set to overcome this problem of imbalanced Data set, and try using the classifiers on that data.","19293a28":"##Linear SVM\n\nIn the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well ","04dc2c50":"###Hyper Parameter Tuning\n\nWe need to find the optimal value of alpha for our Data set.\nWe will use gridsearch for this purpose","287c1f5c":"We can see, the data is imbalanced, and certain classes have higher frequency compareed to the others.\n\n*Optionally, we can also remove the rating 0, since the frequency of that class, compared to others is miniscule","c79ffa4e":"The bar graph for the original Data set and the sample looks similar, indicating the sample is somewhat an accurate representation of the original Data set ","aa91463e":"After applying the relaxation function, our correct predictions more than doubled.\n\nThis proves that our relaxed accuracy idea was a success.","c978f9f6":"### Hyper Parameter Tuning","59dbcb29":"## Ensemble Classifiers\n\nConsidering the performances of Naive Bayes and Linear SVM on our balanced and unbalanced Data set, we will now try and build a classifier, based on Balanced and Unbalanced Linear SVM","30b3f0ab":"### Naive Bayes on Balanced Data Set","73b6472b":"### Vectorization","b3eaf793":"###Cleaning Comments:\n\nTo cleaning our Data set for classification, we need to take care of the following\n\n*   HTML tags\n*   URL tags\n*   String containing only numbers\n*   String containing punctuations\n*   Empty Strings\n*   Lowercase\n*   Non-english words\n*   Stopwords\n\n1. HTML Tags and URL tags to websites with reviews are useless and are therefore omitted.\n\n2. String containing only numbers, and not words cannot be used for text classification purposes and therefore are removed.\n\n3. Strings with punctutaions when tokenized might make token out of punctutations and in cases of contractions such as \"can't\" and \"cant\" would be identified as two separate words.\n\n4. Empty Strings can also be counted under missing values and be removed,by making them NaN and dropna().\n\n5. Applying lowercase to all text ensures that, words like 'the' and 'The' arent identified separately.\n \n6. Comments written in roman english, or in chinese or spanish are noise for a dictionary based in english, hence those words need to removed. So we can convert unicode to ascii using  unidecode, and use nltk corpus\n\n7. Stop words are those words in natural language that have a very little meaning, such as \"is\", \"an\", \"the\", etc. They provide little to no unique information that can be used for classification","33287b3a":"###Linear SVM on Balanced Data set","44914b46":"###Removing Missing rows\n\nThe Data set has many rows with NaN values, and since for our usage we cannot handle the missing values, we will simply drop them.","d1ea1bee":"Looks like our balanced Naive bayes performed poorly on the unbalanced Dataset, but was able to capture the lower ratings on the Data set","771d0c86":"### Balanced Test & Train Data sets","1dcae3f9":"###Prepare Train and Test Data sets\n\nWe need to split our Data set into train and test splits to later assess its performance.","12f185ae":"### Ratings Column:","67a4cc7e":"Term Project, Data Minig, Spring 2020\n\nIn this project, we will be working with Board Game reviews dataset, we have acquired from kaggle. In this dataset we have been provided with three files. Our major concern is with the board game review file, which contains user comments and ratings for a game. Our goal is, given the board game review, we have to predict what rating out of 10 it will have.\n\nFor multi-class text classification Naive Bayes theorem has proved to be the best algorithm out there, but a lesser known fact is that a machine learning model is only as good as the data it has been trained on.\n\nWe are dealing with a massive dataset with a file size of more than 1GB. Alot of preprocessing and cleaning of data will be required before we can feed the model the data for it to train on. Since this is a textual dataset, we will be making use of regexp and many other libraries throughout this project to keep our focus on the actual problem, of building a good machine learning model.\n\nDisclaimer: This notebook is not created on jupyter, but on Google Colab, since i kept running into memory issues in pd.read_csv, even with reading the inputs in chunks. It might take longer for the notebook to run on kaggle.","96f1fc8f":"##Challenges:\n\n\n\n1.   The sheer volume of Data set, was a major hurdle. Reading the dataset on my personal computer wasnt even possible. I had to use google colab to complete all of this work. Even then i had to sample the data, instead of using the whole data as the time it would take to proces the data was insanely long\n2.   Preprocessing the data was also cumbersome, since this was a text classification problem, it was critical the data was as clean as possible. With the help of regex and python's apply function, the data was cleaned, but tasks such as lemmatizing and stemming were too tedious to perform\n3.   The imbalanced dataset was a problem on its own, and training pretrained classifiers on it dropped accuracies to single digits. I was able to undersample a dataset of relatively large size and use ensemble method to get a ensemble classifier which performed perfectly\n4.   The low accuracy score of the coveted Naive Bayes algorithm was a shock, which opened my eyes to the fact that data can make or break an algorithm, and SVM performed better than our Naive Bayes\n5.   Deploying the model on live website was a challenge on its own. 1GB dataset was way beyond the limitations of free hosting websites.\n\n","18bddb0d":"## Cleaning the Data set","cc1da61a":"*Here we can see, our classifier has dropped in accuracy, but is now able to classify lower ratings too","5e4e2a29":"###Hyper Parameter Tuning\n\nWe need to find optimal values of C\n\n1. C is the penalty parameter which represents our missclassification, and tells us how much error is bearable. This is how we can control the trade-off between decision boundary and misclassification term.\n\n*This was taking a considerable amount of time. \nWe have used C = 1, for our purposes","cea1e767":"This is one of the key parts of our project, because a model is only as efficent as the Data set it has been trained on.","5f0c5211":"###Sampling the Data set\n\nSince the Data set is absolutely massive, we can take a random sample using pandas's sample function. This will make this project more manageable, and will be able to run on smaller machines.\n\nIve selected size of N as 100,000. This will sample N rows from the original dataset, at random","0192dca4":"#Board Game Review Classifier","ed32cca4":"In the heatmap, we can visualize that most of our results overfit to classes around 7, since the data we provided was imbalanced, and that class 7 had the highest frequency followed by 6 & 8\n\nUnlike our initial hypothesis, Naive bayes isnt performing as expected.\n","a0b852b0":"Muhammad Arham\n\n1001686912"}}