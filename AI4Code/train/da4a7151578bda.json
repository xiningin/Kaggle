{"cell_type":{"e818563f":"code","a45f8447":"code","f7c52d46":"code","b647ab9e":"code","2092cde5":"code","cf4158ba":"code","35c808e1":"code","9fb8aaa9":"code","d1719a3a":"code","9a226743":"code","7d8293c6":"code","c5ea61aa":"code","676553b3":"code","33131f65":"code","668f02c3":"code","c4d5feec":"code","dea29bf5":"code","dc3da443":"code","7c84abe4":"code","f1a05fa8":"code","da0691bd":"code","45ac7b23":"code","5fe5a400":"code","d80d0e0a":"code","ca7deb31":"code","4e3a8eef":"code","f26eb676":"code","735db70d":"code","f221a57b":"code","25a52423":"code","71ff425a":"code","70da4fa4":"code","d5c58164":"code","707456ac":"code","80e42533":"code","83a2e24b":"code","30e6fc71":"code","531898bf":"code","619afd56":"code","31e559d5":"code","5589e72d":"code","25412f2d":"code","6b6bcb9d":"code","aaaa5ec5":"code","1b6758ef":"code","6d27b23c":"code","d28d686a":"code","b6e40c12":"code","bf7e28b8":"code","54a75662":"code","5181890e":"markdown","47ed12dd":"markdown","37d579d5":"markdown","b8fb977d":"markdown","2c966511":"markdown","f19eb70e":"markdown","9504e91e":"markdown","75850d44":"markdown","db706327":"markdown","9b546a33":"markdown","8cf032b9":"markdown","b78c1823":"markdown","c435930b":"markdown","6500dbb1":"markdown","eab0fbf9":"markdown","74118404":"markdown","2729a785":"markdown","30b62cf0":"markdown","104f4644":"markdown","98975320":"markdown","b0cdd330":"markdown","6b1088a8":"markdown","f704425d":"markdown","e6fdffbc":"markdown","dececcbb":"markdown","452f853f":"markdown","158ebc3f":"markdown","d58708ed":"markdown","084ac066":"markdown","fe76023a":"markdown","3e5410bb":"markdown","4cfe6730":"markdown","2b8a39ab":"markdown","0e56621b":"markdown","ac47fbfd":"markdown","0be3219d":"markdown","0d1ec3d6":"markdown","5ae31b7a":"markdown","2cbe90f6":"markdown","4d2243db":"markdown","1b1ffd36":"markdown","a769caa3":"markdown","94212994":"markdown","e619ee03":"markdown"},"source":{"e818563f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings \nwarnings.filterwarnings('ignore')","a45f8447":"df=pd.read_csv('..\/input\/world-happiness\/2015.csv')","f7c52d46":"df.head()","b647ab9e":"df.shape","2092cde5":"df.dtypes","cf4158ba":"df.isnull().sum()","35c808e1":"df.nunique()","9fb8aaa9":"df.Region.unique()","d1719a3a":"print('Percentage of Cardinality in Region Column')\nprint((df['Region'].value_counts()\/df['Region'].value_counts().sum())*100)","9a226743":"df.skew()","7d8293c6":"df.describe()","c5ea61aa":"plt.figure(figsize=(10,8))\nsns.countplot(df['Region'])\nplt.xticks(rotation = 90)\nplt.title('Cardinality of Region column')","676553b3":"numeric_features=['Happiness Score','Standard Error', 'Economy (GDP per Capita)', 'Family',\n       'Health (Life Expectancy)', 'Freedom', 'Trust (Government Corruption)',\n       'Generosity', 'Dystopia Residual']\nprint(len(numeric_features))","33131f65":"fig,ax = plt.subplots(3,3,figsize=(12,12))\nrow = col = 0\nfor n,i in enumerate(numeric_features):\n    if (n%3 == 0) & (n > 0):\n        row += 1\n        col = 0\n    df[i].plot(kind=\"box\",ax=ax[row,col])\n    col += 1","668f02c3":"fig,ax = plt.subplots(3,3,figsize=(12,12))\nrow = col = 0\nfor n,i in enumerate(numeric_features):\n    if (n%3 == 0) & (n > 0):\n        row += 1\n        col = 0\n    sns.histplot(df[i],kde=True,ax=ax[row,col])\n    col += 1","c4d5feec":"sns.scatterplot(x='Happiness Score',y='Generosity',data=df)","dea29bf5":"plt.figure(figsize=(10,10))\nsns.scatterplot(x='Happiness Score',y='Economy (GDP per Capita)',data=df,hue='Region')","dc3da443":"plt.figure(figsize=(10,10))\nsns.lmplot(x='Happiness Score',y='Health (Life Expectancy)',data=df)","7c84abe4":"sns.pairplot(df)","f1a05fa8":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),annot=True)","da0691bd":"df.head()","45ac7b23":"print(df.shape)\ndf.drop(['Country','Happiness Rank'],axis=1,inplace=True)\ndf.shape","5fe5a400":"from sklearn.preprocessing import LabelEncoder\ne=LabelEncoder()","d80d0e0a":"df['Region']=e.fit_transform(df['Region'])\ndf.head()","ca7deb31":"from scipy.stats import zscore\nz=np.abs(zscore(df))\ndf=df[(z<3).all(axis=1)]","4e3a8eef":"fig,ax = plt.subplots(3,3,figsize=(12,12))\nrow = col = 0\nfor n,i in enumerate(numeric_features):\n    if (n%3 == 0) & (n > 0):\n        row += 1\n        col = 0\n    df[i].plot(kind=\"box\",ax=ax[row,col])\n    col += 1","f26eb676":"df.skew()","735db70d":"df.head()","f221a57b":"from sklearn.preprocessing import PowerTransformer\npt=PowerTransformer()\ndfpt=pt.fit_transform(df)\ndf=pd.DataFrame(dfpt,columns=df.columns)","25a52423":"df.skew()","71ff425a":"y=df['Happiness Score']\nx=df.copy()\nx.drop('Happiness Score',axis=1,inplace=True)","70da4fa4":"from sklearn.preprocessing import MinMaxScaler\ns=MinMaxScaler()","d5c58164":"xs=s.fit_transform(x)\nx=pd.DataFrame(xs,columns=x.columns)\nx.head()","707456ac":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor","80e42533":"from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.25,random_state=7)","83a2e24b":"from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score","30e6fc71":"models=[KNeighborsRegressor(),SVR(),DecisionTreeRegressor(),LinearRegression(),Ridge(),Lasso(),\n        RandomForestRegressor(),AdaBoostRegressor(),GradientBoostingRegressor(),XGBRegressor()]","531898bf":"maelist=[]\nmselist=[]\nrmselist=[]\nr2list=[]\ndef create_model(model):\n    m=model\n    m.fit(xtrain,ytrain)\n    p=m.predict(xtest)\n    \n    mae=mean_absolute_error(p,ytest)\n    mse=mean_squared_error(p,ytest)\n    rmse=np.sqrt(mean_squared_error(p,ytest))\n    r2=r2_score(ytest,p)\n    \n    maelist.append(mae)\n    mselist.append(mse)\n    rmselist.append(rmse)\n    r2list.append(r2)\n    \n    print(m)\n    print('Mean absolute error',mae)\n    print('Mean squared error',mse)\n    print('Root Mean squared error',rmse)\n    print('R2 Score',r2)\n    print('---------------------------------------------------------------------------------------------------------')","619afd56":"for i in models:\n    create_model(i)","31e559d5":"print('Minimum Mean Absolute error is shown by ',models[maelist.index(min(maelist))],min(maelist))\nprint('Minimum Mean squared error is shown by ',models[mselist.index(min(mselist))],min(mselist))\nprint('Minimum Root Mean squared error is shown by ',models[rmselist.index(min(rmselist))],min(rmselist))\nprint('Maximun R2 Score is shown by ',models[r2list.index(max(r2list))],max(r2list))","5589e72d":"from sklearn.model_selection import GridSearchCV","25412f2d":"ridge=Ridge()\nparam_grid={'alpha':[1e-15,1e-10,1e-8,1e-5,1e-3,0.1,1,5,10,15,20,30,35,45,50,55,65,100,110,150,1000]}\nm=GridSearchCV(ridge,param_grid,cv=10)\nm.fit(xtrain,ytrain)\np=m.predict(xtest)\nmae=mean_absolute_error(p,ytest)\nmse=mean_squared_error(p,ytest)\nrmse=np.sqrt(mean_squared_error(p,ytest))\nr2=r2_score(ytest,p)\nprint('Mean absolute error',mae)\nprint('Mean squared error',mse)\nprint('Root Mean squared error',rmse)\nprint('R2 Score',r2)\n","6b6bcb9d":"param={'n_estimators':[50, 100, 150, 200, 250, 300],'learning_rate':[0.0001,0.001,0.01,.1,1,2,5,10]}\nm=GridSearchCV(AdaBoostRegressor(base_estimator=LinearRegression()),param,cv=10,n_jobs=-2)\nm.fit(xtrain,ytrain)\np=m.predict(xtest)\nmae=mean_absolute_error(p,ytest)\nmse=mean_squared_error(p,ytest)\nrmse=np.sqrt(mean_squared_error(p,ytest))\nr2=r2_score(ytest,p)\nprint('Mean absolute error',mae)\nprint('Mean squared error',mse)\nprint('Root Mean squared error',rmse)\nprint('R2 Score',r2)","aaaa5ec5":"param_grid={'C':[1,20,40,60,80,100,200,300,500,1000],'kernel':['linear', 'poly', 'rbf', 'sigmoid'],'degree':[1,2,3,4,5,6]}\ngrid=GridSearchCV(SVR(),param_grid)\ngrid.fit(xtrain,ytrain)\np=grid.predict(xtest)","1b6758ef":"mae=mean_absolute_error(p,ytest)\nmse=mean_squared_error(p,ytest)\nrmse=np.sqrt(mean_squared_error(p,ytest))\nr2=r2_score(ytest,p)\nprint('Mean absolute error',mae)\nprint('Mean squared error',mse)\nprint('Root Mean squared error',rmse)\nprint('R2 Score',r2)","6d27b23c":"x_new=x.drop('Region',axis=1)\nx_new.head()","d28d686a":"xtrain,xtest,ytrain,ytest=train_test_split(x_new,y,test_size=0.25,random_state=7)","b6e40c12":"m=LinearRegression()\nm.fit(xtrain,ytrain)\np=m.predict(xtest)\nmae=mean_absolute_error(p,ytest)\nmse=mean_squared_error(p,ytest)\nrmse=np.sqrt(mean_squared_error(p,ytest))\nr2=r2_score(ytest,p)\nprint('Mean absolute error',mae)\nprint('Mean squared error',mse)\nprint('Root Mean squared error',rmse)\nprint('R2 Score',r2)","bf7e28b8":"m=LinearRegression()\nm.fit(x,y)","54a75662":"import joblib\njoblib.dump(m,'Happinessmodel.obj')","5181890e":"### Hypertuning SVR model as its default kernel values is rbf maybe by changing it to linear may give us good result as Linear regression model is performing well","47ed12dd":"Here we can see that Region columns have imbalanced categories","37d579d5":"Happiness Score show a strong correlation between Economy,Family,Health and Dystopia while a negative correlation with Happiness Rank","b8fb977d":"It is also giving the same result as a single Linear Regression model","2c966511":"Region column has the following categories","f19eb70e":"Since count of each value is 158, therefore no missing values.\nData is symmetrically or normally distributed since mean and median are close to each other in all the columns.\nEach column has variance close to zero except for Happiness rank and Happiness score column.\nThere seem to be very few or no outliers present as there not much difference in the interquartile ranges.","9504e91e":"After Removing Outliers, Skewness is also reduced but not completely","75850d44":"NOTE - For now I am not performing Feature Selection as the no. of features are already very low. I will perform this step if our model accuracy is low","db706327":"Only Region seems to be a Discrete variable, all other are Continuous","9b546a33":"Ridge regression even after Hypertuning gives same result as Linear Regression model","8cf032b9":"Our dataset has no missing values","b78c1823":"We converted column Region from Object to int type","c435930b":"We separate are Dependent and Independet Features","6500dbb1":"Still not better than Linear Regression model","eab0fbf9":"Country and Happiness Rank column dosent seem to be of importance","74118404":"## Finally creating the best model","2729a785":"![](https:\/\/agora.md\/cdn\/p\/news\/big\/republica-moldova--pe-locul-52-in-clasamentul-celor-mai-fericite-tari-8066.jpg)","30b62cf0":"The score seems to go down even with this. Therefore we find that our simple linear model is the best model with region column included","104f4644":"# EDA ","98975320":"## Multivariate Analysis","b0cdd330":"From above histograms we can conclude that Dytopia Residual feature is normally distributed and all other a little bit right skewed or left skewed","6b1088a8":"Here we segregate columns having numerical values to plot various graphs","f704425d":"Removing Outliers present in the dataset by zscore","e6fdffbc":"All the columns have numerical data except for Country and Region column","dececcbb":"In the above scatterplot we find no correlation between Generosity and Happiness Score","452f853f":"From above boxplots we can conclude that there are outliers in Standard Error and Trust (Government Corruption) feature whereas very few in Generosity and Dystopia Residual features","158ebc3f":"Since negative skewness is present i.e. data is left skewed we cannot use log transformation. Hence we use Power Transformer","d58708ed":"## Univariate Analysis","084ac066":"Now all the skew values are between -0.5 to +0.5","fe76023a":"importing Label Encoder and creationg an instance of it","3e5410bb":"Even after removing outliers, there are some present.","4cfe6730":"The above scatter plot shows that there is a positive correlation between Happiness Score and Economy (GDP per Capita) i.e. as the Economy of a country increases, Happiness score of that country also increases","2b8a39ab":"# Feature Engineering ","0e56621b":"Dataframe has 158 rows and 12 columns","ac47fbfd":"## Bivariate Analysis","0be3219d":"There is a positive correlation between Health of people and Happiness Score but there are outliers also present","0d1ec3d6":"### Hypertuning Adaboost classifier using base learner as Linear regression model","5ae31b7a":"Even though there not much need of scaling still there is a big difference between Standard Error and Dystopia features hence we scale the values","2cbe90f6":"Standard Error,Family,Health (Life Expectancy),Trust (Government Corruption),Generosity show skewness which is need to be dealt with.","4d2243db":"### We try to perform hyperparameter tuning on ridge since it is also performing well","1b1ffd36":"'Sub-Saharan Africa' and 'Central and Eastern Europe' region are frequently occuring while 'Australia and New Zealand' and 'North America' regions occur rarely. ","a769caa3":"#### Now we see if our model can perform better if we drop Region feature from our dataset","94212994":"Since there are only 9 columns we donot perform PCA","e619ee03":"Above Pairplot show that our target feature(Happiness Score) shows positive linear correlation with almost all features except for a few which show no correlation and a negative correlation with happiness rank "}}