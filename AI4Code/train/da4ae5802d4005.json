{"cell_type":{"6e8a603e":"code","d01dfb78":"code","fe8d4eff":"code","21464789":"code","6b59ef06":"code","b412e1ed":"code","2bfff77d":"code","4b4faef2":"code","036f2ed9":"code","75d3c5ed":"code","ca13d9b6":"code","1c0e8c40":"code","cd760876":"code","e95258e0":"code","c20e4929":"code","e0a60f6b":"code","1326f922":"code","0c0ff5db":"code","5b1a1b9a":"code","5c77bcab":"code","9ababd03":"code","553905d4":"code","0f82a0af":"code","84338a73":"code","fa20e498":"code","10906cd9":"code","473de8e5":"code","8a1c284c":"code","6fd7f2a9":"code","c3e24e0e":"code","b3b97e29":"code","02af671b":"code","830fdde1":"code","51cc7b2d":"code","13ead1be":"code","1ddced3b":"code","84368da2":"code","f5798f1d":"code","f7560aee":"code","966cffaa":"code","9848509c":"code","703a538f":"code","4b2ee6e2":"code","a7b5f4d5":"code","e9804f81":"code","f2ffa21d":"code","63999711":"code","0750e7d3":"code","bad9a895":"code","d61ff64f":"code","09b4a188":"code","5b1cfbba":"code","e1ef2c11":"code","afccafeb":"code","f2ec8d91":"code","4ba1f2f8":"code","afb4919d":"code","24b34550":"code","02519037":"code","e055cad6":"code","8d0686f8":"code","502eb7e0":"code","9759b682":"code","795334d7":"code","fabe5e68":"code","49a2dac2":"code","42ee677e":"code","e028e994":"code","166722b3":"code","8c0e29cd":"code","86c9de07":"code","c0250fd0":"code","2d5bbaaf":"code","02dd8cc4":"code","7c05e0b5":"code","05dca57d":"code","bc2c6b44":"markdown","76132dd5":"markdown","5a877b51":"markdown","31255f89":"markdown","18a107b2":"markdown","e9dbcbae":"markdown","f6537e2f":"markdown","ae08f387":"markdown","57b3861c":"markdown","92c4f5e8":"markdown","ce265e79":"markdown","f66dd3b4":"markdown","a5e82539":"markdown","44f763d2":"markdown","de2a781f":"markdown","5a09d439":"markdown","7c1cc21b":"markdown","227a6bc0":"markdown","21b0c4aa":"markdown","54b1db9a":"markdown","5e2968e1":"markdown","34a54ea2":"markdown","2316b5bf":"markdown","91fe3587":"markdown","1c5e339e":"markdown","2d903d86":"markdown","dad78de8":"markdown","3c4b343b":"markdown","c3eb1caa":"markdown","9090742a":"markdown","78eb11b6":"markdown","6d815031":"markdown","492c82a5":"markdown","03f647fc":"markdown","ff39a340":"markdown","5afddfad":"markdown","d0b611b0":"markdown","d29905d1":"markdown","fdf2f284":"markdown","7eee7321":"markdown","08762d43":"markdown"},"source":{"6e8a603e":"### \u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\nimport pandas as pd\nimport numpy as np\n### \u041c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import precision_score, f1_score\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split \n### \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n","d01dfb78":"### \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0438 \u0432\u044b\u0432\u0435\u0434\u0435\u043c \u043f\u0435\u0440\u0432\u044b\u0435 5 \u0441\u0442\u0440\u043e\u043a \u0434\u043b\u044f \u0432\u0438\u0437\u0443\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430\ntelecom_df = pd.read_csv(\"..\/input\/telecom-users-dataset\/telecom_users.csv\")\ntelecom_df.head()","fe8d4eff":"### \u0422\u0438\u043f \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u0445 \ntelecom_df.dtypes","21464789":"### \u0420\u0430\u0437\u043c\u0435\u0440 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\ntelecom_df.shape","6b59ef06":"### K\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u0438\u0437 \u043a\u043e\u043b\u043e\u043d\u043e\u043a\ntelecom_df.isna().sum()","b412e1ed":"### \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0432 \u0441\u0442\u043e\u043b\u0431\u0446\u0435 'Unnamed: 0'\ntelecom_df['Unnamed: 0'].nunique()","2bfff77d":"### \u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 Describe \u0432\u044b\u0432\u0435\u0434\u0435\u043c \u043e\u0431\u0449\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043f\u043e \u0432\u0441\u0435\u043c \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u043c \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c \ntelecom_df.describe(include =['float', 'int'])","4b4faef2":"### \u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 Describe \u0432\u044b\u0432\u0435\u0434\u0435\u043c \u043e\u0431\u0449\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043f\u043e \u0432\u0441\u0435\u043c \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c \u0442\u0438\u043f\u0430 object\ntelecom_df.describe(include ='object')","036f2ed9":"### \u041f\u0440\u0438\u0432\u0435\u0434\u0435\u043c \"TotalCharges\" \u043a \u0442\u0438\u043f\u0443 \"int\" \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\n\ntelecom_df['TotalCharges'] = pd.to_numeric(telecom_df['TotalCharges'], errors='coerce')\ntelecom_df['TotalCharges'].isna().value_counts()","75d3c5ed":"### \u0412\u044b\u0432\u0435\u0434\u0435\u043c \u0441\u0442\u0440\u043e\u043a\u0438 \u0441 \u043d\u0443\u043b\u0435\u0432\u044b\u043c\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438\ntelecom_df[pd.isnull(telecom_df['TotalCharges'])]","ca13d9b6":"### \u0417\u0430\u043c\u0435\u043d\u0430 NaN \u043d\u0430 0\ntelecom_df.fillna(0, inplace=True)","1c0e8c40":"### \u0423\u0434\u0430\u043b\u0438\u043c \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \"Unnamed: 0\" \u0438 \"customerID\"\ntelecom_df.drop(['Unnamed: 0', 'customerID'], axis=1, inplace=True)","cd760876":"### \u0417\u0430\u043c\u0435\u043d\u0430 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 male \u0438 female\ntelecom_df = telecom_df.replace('Male', 1)\ntelecom_df = telecom_df.replace('Female', 0)\n\n### \u0417\u0430\u043c\u0435\u043d\u0430 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 Yes \u0438 No\ntelecom_df = telecom_df.replace('Yes', 1)\ntelecom_df = telecom_df.replace('No', 0)","e95258e0":"### \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\ntelecom_df.head(10)","c20e4929":"### \u0417\u0430\u043c\u0435\u043d\u0430 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 'No internet service'\ntelecom_df = telecom_df.replace('No internet service', 0)\ntelecom_df = telecom_df.replace('No phone service', 0)","e0a60f6b":"telecom_df = telecom_df.replace('DSL', 1)\ntelecom_df = telecom_df.replace('Fiber optic', 2)\n\ntelecom_df = telecom_df.replace('Month-to-month', 0)\ntelecom_df = telecom_df.replace('One year', 1)\ntelecom_df = telecom_df.replace('Two year', 2)\n\ntelecom_df = telecom_df.replace('Credit card (automatic)', 0)\ntelecom_df = telecom_df.replace('Bank transfer (automatic)', 1)\ntelecom_df = telecom_df.replace('Electronic check', 2)\ntelecom_df = telecom_df.replace('Mailed check', 3)\n\ntelecom_df.isna().sum()","1326f922":"### \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u0448\u0435\u0434\u0448\u0438\u0445 \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432 (\u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0430 \u043f\u043e \u043f\u043e\u043b\u0443)\ncount_gender = telecom_df[['gender', 'Churn']].groupby(['gender']\n                                         ).sum().sort_values(by='Churn')\n\n\nprint(telecom_df['gender'].value_counts(normalize=True))\ncount_gender","0c0ff5db":"### \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u0448\u0435\u0434\u0448\u0438\u0445 \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432 (\u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0430 \u043f\u043e \u043f\u0435\u043d\u0441\u0438\u043e\u043d\u043d\u043e\u043c\u0443 \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0443)\nprint(telecom_df[['SeniorCitizen']].value_counts(normalize=True))\ncount_senior = telecom_df[['SeniorCitizen', 'Churn']].groupby(['SeniorCitizen']\n                                         ).sum().sort_values(by='Churn')\ncount_senior","5b1a1b9a":"### \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u0448\u0435\u0434\u0448\u0438\u0445 \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432 (\u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0430 \u043f\u043e \u043f\u043e\u043b\u0443 \u0438 \u043f\u0435\u043d\u0441\u0438\u043e\u043d\u043d\u043e\u043c\u0443 \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0443)\ncount_senior = telecom_df[['gender', 'SeniorCitizen', 'Churn']].groupby(['gender', 'SeniorCitizen']\n                                         ).sum().sort_values(by='Churn')\n\ncount_senior","5c77bcab":"count = telecom_df[['Partner', 'Churn']].groupby(['Partner']\n                                         ).sum().sort_values(by='Churn')\nprint(telecom_df['Partner'].value_counts(normalize=True))\ncount","9ababd03":"### \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u0448\u0435\u0434\u0448\u0438\u0445 \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432 (\u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0430 \u043f\u043e \u043f\u043e\u043b\u0443 \u0438 \u0441\u0435\u043c\u0435\u0439\u043d\u043e\u043c\u0443 \u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u044e)\ncount = telecom_df[['gender', 'Partner', 'Churn']].groupby(['gender', 'Partner']\n                                         ).sum().sort_values(by='Churn')\n\ncount","553905d4":"count = telecom_df[['Dependents', 'Churn']].groupby(['Dependents']\n                                         ).sum().sort_values(by='Churn')\nprint(telecom_df['Dependents'].value_counts(normalize=True))\ncount","0f82a0af":"### \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u0448\u0435\u0434\u0448\u0438\u0445 \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432 (\u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0430 \u043f\u043e \u043f\u043e\u043b\u0443 \u0438 \u043d\u0430\u043b\u0438\u0447\u0438\u044e \u0438\u0436\u0434\u0438\u0432\u0435\u043d\u0446\u0435\u0432)\ncount = telecom_df[['gender', 'Dependents', 'Churn']].groupby(['gender', 'Dependents']\n                                         ).sum().sort_values(by='Churn')\n\ncount","84338a73":"### \u0413\u0440\u0430\u0444\u0438\u043a \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432 \u043e\u0442 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0443\u0441\u043b\u0443\u0433\u0430\u043c\u0438 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438 \u0434\u043b\u044f \u0434\u0435\u0439\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0438 \u0443\u0448\u0435\u0434\u0448\u0438\u0445 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432\ngrid = sns.FacetGrid(telecom_df, col='Churn', row='gender', height=4.2, aspect=1.6)\ngrid.map(plt.hist, 'tenure', bins=100)\ngrid.add_legend()\nplt.show()","fa20e498":"### \u0413\u0440\u0430\u0444\u0438\u043a \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432 \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u0435\u0436\u0435\u043c\u0435\u0441\u044f\u0447\u043d\u043e\u0439 \u043f\u043b\u0430\u0442\u044b \u0434\u043b\u044f \u0434\u0435\u0439\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0438 \u0443\u0448\u0435\u0434\u0448\u0438\u0445 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432\ngrid = sns.FacetGrid(telecom_df, col='Churn', height=4.2, aspect=1.6)\ngrid.map(plt.hist, 'MonthlyCharges', bins=300)\ngrid.add_legend()\nplt.show()","10906cd9":"### \u0413\u0440\u0430\u0444\u0438\u043a \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432 \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u043e\u0431\u0449\u0435\u0439 \u043f\u043b\u0430\u0442\u044b \u0437\u0430 \u0432\u0441\u0435 \u0432\u0440\u0435\u043c\u044f \u0434\u043b\u044f \u0434\u0435\u0439\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0438 \u0443\u0448\u0435\u0434\u0448\u0438\u0445 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432\ngrid = sns.FacetGrid(telecom_df, col='Churn', height=4.2, aspect=1.6)\ngrid.map(plt.hist, 'TotalCharges', bins=300)\ngrid.add_legend()\nplt.show()","473de8e5":"### \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u0448\u0435\u0434\u0448\u0438\u0445 \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432 (\u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0430 \u043f\u043e \u0441\u043f\u043e\u0441\u043e\u0431\u0443 \u043e\u043f\u043b\u0430\u0442\u044b)\ncount = telecom_df[['PaymentMethod', 'Churn']].groupby(['PaymentMethod']\n                                         ).sum().sort_values(by='Churn')\nprint(telecom_df['PaymentMethod'].value_counts(normalize=True))\n\ncount","8a1c284c":"### \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u0448\u0435\u0434\u0448\u0438\u0445 \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432 (\u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0430 \u043f\u043e \u043d\u0430\u043b\u0438\u0447\u0438\u044e \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u043a \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0438\u0438)\ncount = telecom_df[['PhoneService', 'Churn']].groupby(['PhoneService']\n                                         ).sum().sort_values(by='Churn')\n\nprint(telecom_df['PhoneService'].value_counts(normalize=True))\n\ncount","6fd7f2a9":"### \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u0448\u0435\u0434\u0448\u0438\u0445 \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432 (\u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0430 \u043f\u043e \u0443\u0441\u043b\u0443\u0433\u0430\u043c \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0430)\ncount = telecom_df[['InternetService', 'Churn']].groupby(['InternetService']\n                                         ).sum().sort_values(by='Churn')\n\ncount","c3e24e0e":"### \u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043d\u043e\u0435 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u0432\u0438\u0434\u043e\u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u0441\u0435\u0440\u0432\u0438\u0441\u043e\u0432\nprint(telecom_df['InternetService'].value_counts(normalize=True))\nprint(telecom_df['OnlineSecurity'].value_counts(normalize=True))\nprint(telecom_df['OnlineBackup'].value_counts(normalize=True))\nprint(telecom_df['DeviceProtection'].value_counts(normalize=True))\nprint(telecom_df['TechSupport'].value_counts(normalize=True))\nprint(telecom_df['StreamingTV'].value_counts(normalize=True))\nprint(telecom_df['StreamingMovies'].value_counts(normalize=True))","b3b97e29":"### \u041f\u0440\u043e\u0446\u0435\u043d\u0442\u043d\u043e\u0435 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0443\u0448\u0435\u0434\u0448\u0438\u0445 \u043b\u044e\u0434\u0435\u0439 \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u0434\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043a\u043e\u043d\u0442\u0440\u0430\u043a\u0442\u0430\n\ncount = telecom_df[['Contract', 'Churn']].groupby(['Contract']\n                                         ).sum().sort_values(by='Churn')\n\nprint(telecom_df['Contract'].value_counts(normalize=True))\ncount","02af671b":"count = telecom_df[['PaperlessBilling', 'Churn']].groupby(['PaperlessBilling']\n                                         ).sum().sort_values(by='Churn')\n\nprint(telecom_df['PaperlessBilling'].value_counts(normalize=True))\ncount","830fdde1":"telecom_df['Gender_sum'] = telecom_df['SeniorCitizen'] + telecom_df['Partner'] + telecom_df['Dependents']","51cc7b2d":"telecom_df['Internet_sum'] = telecom_df['OnlineSecurity'] + telecom_df['OnlineBackup'\n                        ] + telecom_df['TechSupport'] + telecom_df['StreamingTV'\n                        ] + + telecom_df['StreamingMovies'] + telecom_df['DeviceProtection']","13ead1be":"telecom_df_1 = telecom_df.drop(['Gender_sum', 'Internet_sum'], 1)","1ddced3b":"telecom_df_2 = telecom_df.drop([\"SeniorCitizen\", \"Partner\", \"Dependents\"], 1)","84368da2":"telecom_df_3 = telecom_df.drop(['OnlineSecurity', 'OnlineBackup'\n                        , 'TechSupport', 'StreamingTV'\n                        , 'StreamingMovies', 'DeviceProtection'], 1)","f5798f1d":"telecom_df_4 = telecom_df.drop(['OnlineSecurity', 'OnlineBackup'\n                        , 'TechSupport', 'StreamingTV'\n                        , 'StreamingMovies', 'DeviceProtection',\n                               \"SeniorCitizen\", \"Partner\", \"Dependents\",\n                               'Gender_sum', 'Internet_sum'], 1)","f7560aee":"telecom_df_1.head()","966cffaa":"### \u0420\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438 \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e\nfeauters = telecom_df_1.drop(\"Churn\", 1)\nlabels = telecom_df_1[\"Churn\"]\n\nprint(labels.shape)\nprint(feauters.shape)\n","9848509c":"### \u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\nX_train, X_test, y_train, y_test = train_test_split(feauters, labels, test_size=0.20, random_state = 17) ","703a538f":"### \u041c\u0435\u0442\u043e\u0434 \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0445 \u0441\u043e\u0441\u0435\u0434\u0435\u0439\n\nfrom sklearn.metrics import roc_auc_score\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn_model = knn.fit(X_train, y_train)\nknn_predictions = knn.predict(X_test)\n\nconf_mtrx = confusion_matrix(y_test, knn_predictions)\nreport = classification_report(y_test, knn_predictions)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test, knn_predictions))","4b2ee6e2":"### \u0411\u0430\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\n\nnb = GaussianNB()\nnb_model = nb.fit(X_train, y_train)\nnb_predictions = nb.predict(X_test)\n\nconf_mtrx = confusion_matrix(y_test, nb_predictions)\nreport = classification_report(y_test, nb_predictions)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test, nb_predictions))","a7b5f4d5":"### \u0414\u0435\u0440\u0435\u0432\u043e \u0440\u0435\u0448\u0435\u043d\u0438\u0439\n\ndtc = DecisionTreeClassifier()\ndtc_model = dtc.fit(X_train, y_train)\ndtc_predictions = dtc.predict(X_test)\n\nconf_mtrx = confusion_matrix(y_test, dtc_predictions)\nreport = classification_report(y_test, dtc_predictions)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test, dtc_predictions))","e9804f81":"###LogisticRegression\n\nlr_bin = LogisticRegression(solver=\"lbfgs\", random_state=17, max_iter=1000)\nlr_bin_model = lr_bin.fit(X_train, y_train)\nlr_bin_prediction = lr_bin_model.predict(X_test)\n\nconf_mtrx = confusion_matrix(y_test, lr_bin_prediction)\nreport = classification_report(y_test, lr_bin_prediction)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test, lr_bin_prediction))","f2ffa21d":"# Stochastic Gradient Descent\n\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd_model = sgd.fit(X_train, y_train)\nsgd_prediction = sgd.predict(X_test)\n\nconf_mtrx = confusion_matrix(y_test, sgd_prediction)\nreport = classification_report(y_test, sgd_prediction)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test, sgd_prediction))","63999711":"### Support Vector Machines\n\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc_model = svc.fit(X_train, y_train)\nsvc_prediction = svc.predict(X_test)\n\nconf_mtrx = confusion_matrix(y_test, svc_prediction)\nreport = classification_report(y_test, svc_prediction)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test, svc_prediction))","0750e7d3":"### \u0414\u043b\u044f \u043d\u0430\u0448\u0435\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430\u0439\u0434\u0435\u043c \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 n_neighbors, \u043f\u0440\u0438 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u043d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0438\u0439 recall\n\nfor i in range(1, 21):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn_model = knn.fit(X_train, y_train)\n    knn_predictions = knn.predict(X_test)\n    recall = recall_score(y_test, knn_predictions)\n    auc = roc_auc_score(y_test, knn_predictions)\n    print(i, \"recall - \", recall)\n    print(i, \"auc - \", auc)\n    print(\"________________\")","bad9a895":"### \u0414\u043b\u044f \u043d\u0430\u0448\u0435\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 n = 1. \u041e\u0431\u0443\u0447\u0438\u043c \u043d\u0430\u0448\u0443 \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u044d\u0442\u043e\u0433\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\n\nknn = KNeighborsClassifier(n_neighbors=1)\nknn_model = knn.fit(X_train, y_train)\nknn_predictions = knn.predict(X_test)\n\nconf_mtrx = confusion_matrix(y_test, knn_predictions)\nreport = classification_report(y_test, knn_predictions)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test, knn_predictions))","d61ff64f":"### \u0417\u0430\u0434\u0430\u0434\u0438\u043c \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 Decision Trees\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\ntree_params = {'max_depth': range(1, 10), \n               'max_features': range(1, 10)}\n\ntree_grid = GridSearchCV(dtc, tree_params, cv=5)","09b4a188":"tree_grid.fit(X_train, y_train)\ntree_grid.best_params_","5b1cfbba":"### \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0441 \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438\ndtc_model = tree_grid.fit(X_train, y_train)\ndtc_predictions = tree_grid.predict(X_test)\n\nconf_mtrx = confusion_matrix(y_test, dtc_predictions)\nreport = classification_report(y_test, dtc_predictions)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test, dtc_predictions))","e1ef2c11":"### \u0417\u0430\u0434\u0430\u0434\u0438\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\nfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\nCs_list = np.logspace(-1, 2, num = 500)","afccafeb":"LogRegCV = LogisticRegressionCV(Cs=Cs_list, max_iter = 500\n    ,scoring='roc_auc'\n    ,cv=fold\n    ,random_state=17\n)\n\nlr = LogRegCV.fit(X_train, y_train)\nlr_predictions = lr.predict(X_test)\n\nconf_mtrx = confusion_matrix(y_test, lr_predictions)\nreport = classification_report(y_test, lr_predictions)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test, lr_predictions))\n\nprint(LogRegCV.scores_[1].shape)\nprint('Max auc_roc:', LogRegCV.scores_[1].max())","f2ec8d91":"feauters2 = telecom_df_2.drop(\"Churn\", 1)\nlabels2 = telecom_df_2[\"Churn\"]\n\nprint(labels2.shape)\nprint(feauters2.shape)","4ba1f2f8":"X_train2, X_test2, y_train2, y_test2 = train_test_split(feauters2, labels2, test_size=0.20, random_state = 17) ","afb4919d":"lr_bin = LogisticRegression(solver=\"lbfgs\", random_state=17, max_iter=1000)\nlr_bin_model = lr_bin.fit(X_train2, y_train2)\nlr_bin_prediction = lr_bin_model.predict(X_test2)\n\nconf_mtrx = confusion_matrix(y_test2, lr_bin_prediction)\nreport = classification_report(y_test2, lr_bin_prediction)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test2, lr_bin_prediction))","24b34550":"nb = GaussianNB()\nnb_model = nb.fit(X_train2, y_train2)\nnb_predictions = nb.predict(X_test2)\n\nconf_mtrx = confusion_matrix(y_test2, nb_predictions)\nreport = classification_report(y_test2, nb_predictions)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test2, nb_predictions))","02519037":"feauters3 = telecom_df_3.drop(\"Churn\", 1)\nlabels3 = telecom_df_3[\"Churn\"]\n\nprint(labels3.shape)\nprint(feauters3.shape)","e055cad6":"X_train3, X_test3, y_train3, y_test3 = train_test_split(feauters3, labels3, test_size=0.20, random_state = 17) ","8d0686f8":"lr_bin = LogisticRegression(solver=\"lbfgs\", random_state=17, max_iter=1000)\nlr_bin_model = lr_bin.fit(X_train3, y_train3)\nlr_bin_prediction = lr_bin_model.predict(X_test3)\n\nconf_mtrx = confusion_matrix(y_test3, lr_bin_prediction)\nreport = classification_report(y_test3, lr_bin_prediction)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test3, lr_bin_prediction))","502eb7e0":"nb = GaussianNB()\nnb_model = nb.fit(X_train3, y_train3)\nnb_predictions = nb.predict(X_test3)\n\nconf_mtrx = confusion_matrix(y_test3, nb_predictions)\nreport = classification_report(y_test3, nb_predictions)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test3, nb_predictions))","9759b682":"dtc_model = tree_grid.fit(X_train, y_train)\ndtc_predictions = tree_grid.predict(X_test)\n\nconf_mtrx = confusion_matrix(y_test, dtc_predictions)\nreport = classification_report(y_test, dtc_predictions)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test, dtc_predictions))","795334d7":"feauters4 = telecom_df_4.drop(\"Churn\", 1)\nlabels4 = telecom_df_4[\"Churn\"]\n\nprint(labels4.shape)\nprint(feauters4.shape)","fabe5e68":"X_train4, X_test4, y_train4, y_test4 = train_test_split(feauters4, labels4, test_size=0.20, random_state = 17) ","49a2dac2":"lr_bin = LogisticRegression(solver=\"lbfgs\", random_state=17, max_iter=1000)\nlr_bin_model = lr_bin.fit(X_train4, y_train4)\nlr_bin_prediction = lr_bin_model.predict(X_test4)\n\nconf_mtrx = confusion_matrix(y_test4, lr_bin_prediction)\nreport = classification_report(y_test4, lr_bin_prediction)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test4, lr_bin_prediction))","42ee677e":"nb = GaussianNB()\nnb_model = nb.fit(X_train4, y_train4)\nnb_predictions = nb.predict(X_test4)\n\nconf_mtrx = confusion_matrix(y_test4, nb_predictions)\nreport = classification_report(y_test4, nb_predictions)\nprint(report)\nprint(conf_mtrx)\nprint(roc_auc_score(y_test4, nb_predictions))","e028e994":"from sklearn.metrics import roc_auc_score, roc_curve\n\n### \u041c\u0435\u0442\u043e\u0434 \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0445 \u0441\u043e\u0441\u0435\u0434\u0435\u0439\n\nprobas = knn.fit(X_train, y_train).predict_proba(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.figure(figsize=(10,5))\nplt.plot(fpr, tpr, label=\"auc=\" + str(auc))\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=4)\nplt.show()","166722b3":"### \u0411\u0430\u0435\u0441\u043e\u0432\u0441\u043a\u0438\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\n\nprobas = nb.fit(X_train, y_train).predict_proba(X_test)\nfpr, tpr, _ = roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.figure(figsize=(10,5))\nplt.plot(fpr, tpr, label=\"auc=\" + str(auc))\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=4)\nplt.show()","8c0e29cd":"### \u0414\u0435\u0440\u0435\u0432\u043e \u0440\u0435\u0448\u0435\u043d\u0438\u0439\n\n\nprobas = tree_grid.fit(X_train, y_train).predict_proba(X_test)\nfpr, tpr, _ = roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.figure(figsize=(10,5))\nplt.plot(fpr, tpr, label=\"auc=\" + str(auc))\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=4)\nplt.show()","86c9de07":"### \u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f\n\n\nprobas = lr_bin.fit(X_train, y_train).predict_proba(X_test)\nfpr, tpr, _ = roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.figure(figsize=(10,5))\nplt.plot(fpr, tpr, label=\"auc=\" + str(auc))\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=4)\nplt.show()","c0250fd0":"probas = LogRegCV.fit(X_train, y_train).predict_proba(X_test)\nfpr, tpr, _ = roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.figure(figsize=(10,5))\nplt.plot(fpr, tpr, label=\"auc=\" + str(auc))\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=4)\nplt.show()","2d5bbaaf":"### \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 over-sampling \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 SMOTE\n\nX = telecom_df_1.loc[:, telecom_df_1.columns != 'Churn']\ny = telecom_df_1.loc[:, telecom_df_1.columns == 'Churn']\n\nfrom imblearn.over_sampling import SMOTE\n\nos = SMOTE(random_state=17)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\ncolumns = X_train.columns\n\nos_data_X,os_data_y=os.fit_resample(X_train, y_train)\nos_data_X = pd.DataFrame(data=os_data_X,columns=columns )\nos_data_y= pd.DataFrame(data=os_data_y,columns=['Churn'])\n\nprint(\"\u0412\u0435\u043b\u0438\u0447\u0438\u043d\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \",len(os_data_X))\nprint(\"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0441\u0442\u0430\u0432\u0448\u0438\u0445\u0441\u044f \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432\",len(os_data_y[os_data_y['Churn']==0]))\nprint(\"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u0448\u0435\u0434\u0448\u0438\u0445 \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432\",len(os_data_y[os_data_y['Churn']==1]))\n","02dd8cc4":"### \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438\n\nX = os_data_X\ny = os_data_y\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\nlogreg = LogisticRegression(max_iter=1000, random_state=17)\nlogreg.fit(X_train, y_train.values.ravel())","7c05e0b5":"### \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 classification_report\n\nlr_predictions = logreg.predict(X_test)\nreport = classification_report(y_test, lr_predictions)\n\nprint(report)","05dca57d":"probas = logreg.fit(X_train, y_train.values.ravel()).predict_proba(X_test)\nfpr, tpr, _ = roc_curve(y_test, probas[:, 1])\nauc = roc_auc_score(y_test, probas[:, 1])\nplt.figure(figsize=(10,5))\nplt.plot(fpr, tpr, label=\"auc=\" + str(auc))\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=4)\nplt.show()","bc2c6b44":"The difference in the churn of subscribers with dependents depending on their gender is not significant","76132dd5":"# 2. Researching dependencies and formulating hypotheses","5a877b51":"#### Primary data\n\nThis dataset contains information about 22 parameters for 5986 users. 3 columns have class (int), one column has class (float), other columns have class (object). The dataset does not contain missing values. There is also a column Unnamed: 0, which is obviously the internal subscriber numbering used in the table.\n\n* Parameters Partner, Dependents, PhoneService, PaperlessBilling and Churn have 2 unique values \u200b\u200bYes and No;\n* Parameters MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies have 3 unique parameters, 2 of which are also Yes and No;\n* Parameter InternetService and Contract have 3 unique values, parameter PaymentMethod 4;\n\n\nFor further work, we will solve the following tasks:\n\n* Column \"TotalCharges\" is of type \"object\", although it contains numeric values. It is necessary to cast it to the \"float\" type;\n* The number of unique values \u200b\u200bfor each column to bring them to numerical values \u200b\u200bfor further work;\n* Determine the importance of each of the parameters;\n","31255f89":"Summarizing the first stage of the analysis, we can draw preliminary conclusions:\nSubscriber churn is not directly correlated with gender;\nAlso, the churn of subscribers does not correlate with marital status or the presence of dependents, depending on the gender of the client;\nIn general, in this group, it is now difficult to identify features that can have a significant impact on user churn. It is worth keeping this in mind when building models for training.","18a107b2":"Analyzing the data on payment methods, the following conclusions can be drawn:\n* Credit card replenishment, bank transfer and postal check each account for about 22%, while the largest number of users pay by electronic check;\n* As for churn, here the first 3 methods also occupy almost equal positions in 15-16% of the total number of left users. At the same time, the number of abandoned subscribers making payments by electronic check is almost 57%.","e9dbcbae":"### 3) Analysis of the dependence of the churn on the monthly and total fees, as well as on the payment method","f6537e2f":"For a complete dataset without transformations and hyperparameter tuning, the Bayesian classifier and LogisticRegression perform best in terms of the roc_auc metric. At the same time, the Bayesian classifier shows the best indicators of the recall metric to determine the subscribers who can leave. Let's try to use some of our models by adjusting the HitParameters.\n\n1) For the \"Method of nearest neighbors\" we apply the selection of the best hyperparameter n, at which we get the best recall and auc","ae08f387":"The number of subscribers not using communication services is 10%. There are about the same number of them among the customer churn. ","57b3861c":"For further transformation, let's take:\n    * Yes = 1\n    * No = 0\n    * Male = 1\n    * Female = 0","92c4f5e8":"The columns (OnlineSecurity OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies) obviously have the same 'No internet service' parameter if the subscriber does not have an Internet subscription.\nLikewise for the MultipleLines column. Since the data on the availability of a subscription to Internet and telephony services can be important for our forecasting, we will replace these values with 0.","ce265e79":"For this item, we will find a graphical dependence of the subscriber outflow on the duration of the use of services. To do this, we will form 4 graphs: current and departed users, additionally divided by gender, in order to once again test our hypothesis about the small influence of gender on refusal to use services.","f66dd3b4":"So far, there is no way to accurately answer the question of which of the parameters will affect the construction of machine learning models, so it is proposed to consider a number of hypotheses.\n1) At the first stage, build models for a standard dataset without any transformations.\n\n2) Combine the number of all connected Internet services for each subscriber into a separate column \"Internet_sum\"\n\n3) Also combine all the signs related to personal life in a separate column \"Gender_sum\"","a5e82539":"With hyperparameter setting n, the best result on the roc_auc metric does not exceed 0.67\n\n2) We construct the following model with the given hyperparameters for the Decision Tree","44f763d2":"### Findings:\nAfter the final transformations, using the SMOTE sampling algorithm for the training data and using the auc_roc estimation metric with the distribution probability predict_proba, we obtained the accuracy of our model auc = 0.92.\nAt the same time, the recall metric for predicting potential subscribers who may leave is 0.87. ","de2a781f":"We can now move on to building models. For our purposes, we will use the following set of methods:\n* Nearest Neighbor Method\n* Bayesian classifier\n* Decision tree\n* Logistic regression\n* Support Vector Machines\n* Stochastic Gradient Descent\n\nFor the estimation method at the first stage, we will use classification_report, which calculates the precision, recall and f1-score metrics, as well as visualize the error matrix. Our main goal is to improve the recall metric. That is, it is important for us that the algorithm identifies people who refuse to use the services as best as possible.\n\nFor our calculations, we will take random_state for all calculations equal to 17. At the first stage, we will perform predictions on the full dataset without any transformations.\n","5a09d439":"In total, 48.5% of subscribers are married, while the churn on this basis was 36.3%","7c1cc21b":"The model trained taking into account the selection of the best parameters shows a significant increase in the auc_score metric.\n\n3) Build a logistic regression with cross-validation parameters","227a6bc0":"From the data obtained, it can be seen that the difference in the outflow of pensioners, in contrast to their gender, is not significant.","21b0c4aa":"Comparison of the quality of the best models using the auc_roc metric and the distribution probability of predict_proba","54b1db9a":"### 1) Analysis of personal data dependencies","5e2968e1":"\u041f\u0440\u0438\u043c\u0435\u043c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f:\n\n* InternetService\n    * DSL = 1\n    * Fiber optic = 2\n    \n* Contract\n    * Month-to-month = 0\n    * One year = 1\n    * Two year = 2\n    \n* PaymentMethod\n    * Credit card (automatic) = 0\n    * Bank transfer (automatic) = 1\n    * Electronic check = 2\n    * Mailed check = 3\n    \n    ","34a54ea2":"# 3. \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u043b\u044f \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043e\u0442\u0442\u043e\u043a\u0430","2316b5bf":"### At the beginning of work on the study of the provided dataset, we will form hypotheses that we have to confirm or refute\n\n* From the available data, we can distinguish several large groups in which the signs are in one way or another related to each other. We will carry out the first analysis in these groups:\n\n1) The group with personal data includes information about gender, retirement age, marriage and dependents. This group is binary, that is, the data in it answers \"yes\" or \"no\" questions. It proposes to find the correlation between churn and features.\n\n2) The second group is related to payment. Here it is interesting to study the churn of subscribers depending on the payment methods, the increase in the monthly payment, as well as the total payment for the entire time.\n\n3) The third group is associated with connected services: mobile and Internet communications, as well as additional options related to this.\n\n4) The temporary group should be noted separately. The column with data on the duration of the use of services can give us the opportunity to understand at what stage users abandon the service. The column with data on the period for which the contract is concluded will allow us to determine which contract term holds the most users.\n\n* In our data, the Churn column is responsible for churn information. A value of 1 in it means that the user has refused services. In relation to this column, we will conduct research on hypotheses.","91fe3587":"On the graphs obtained, we clearly see that the largest number of subscribers leave in the first 20 months of using the services, then the outflow levels out. As we assumed earlier, the data also does not depend on the gender of the subscriber.\n\nWe can conclude that data on how many months a person has used the services of a company is important for building our future models.","1c5e339e":"The resulting graphs are similar to the first graph - the number of subscribers versus the time of using the company's services. Thus, it can be seen that the largest number of customers leave before the amount of payment for the entire time exceeds $ 1000.\n","2d903d86":"# 4. Comparison of the quality of models","dad78de8":"We can see that all subscribers are new users (tenure = 0), so zero values in the monthly billing column are quite understandable. For further work, replace them with 0.\n\n","3c4b343b":"### 2) Analysis of the dependence of the churn depending on the duration of the use of services","c3eb1caa":"PaymentMethod\n* Credit card (automatic) = 0\n* Bank transfer (automatic) = 1\n* Electronic check = 2\n* Mailed check = 3","9090742a":"The resulting graphs show a spike in churn among users whose monthly subscriptions exceed $ 70, which is an important sign for building future forecasting models.","78eb11b6":"### 4) Analysis of outflow dependence on connected additional services","6d815031":"In the dataframe, the number of male subscribers is 51%, female subscribers 49%. We can see the same dependence by the number of customers who left: 796 subscribers to m \/ c and 791 to w \/ p. That is, at this stage, we can conclude that churn does not directly correlate with the gender of the client.\n\nLet's analyze the rest of the \"family variables\" separately and in conjunction with the sex of the subscriber.","492c82a5":"The outflow of retirees is 25.3% relative to the total number of leaving users, which is almost 9% more than the total number of retirees among the data we studied - 16.14%. Hence, we can conclude that people of retirement age are more likely to refuse the services of the company. This data should be taken into account in our forecasting models.","03f647fc":"# 1. Description","ff39a340":"It follows from the results obtained that there is no data on payment for 10 subscribers. To make a decision about deleting these rows, let's take a look at them.\n\n","5afddfad":"Let's check the quality for the best models on the converted telecom_df_2 and telecom_df_3 datasets","d0b611b0":"As you can see, about half of all subscribers renew their contract every month, about 21% have a one-year contract and 24% have a two-year one. At the same time, the overwhelming majority of those who left were subscribers with a monthly contract.","d29905d1":"About 30% of subscribers have dependents. At the same time, the outflow of such subscribers is 17.7% of the total number of abandoned customers. ","fdf2f284":"From the obtained estimation methods, we see that the logistic regression model has the best auc indicator.\n\nLet's try to improve the results of the model using the SMOTE sampling algorithm. That is, having an unbalanced sample, we will synthetically increase the number of subscribers who have left. To prevent hitting test data, let's build an over-sampling model based solely on training data.","7eee7321":"Let's move on to transforming our dataframe for further work.\n\nThe columns \"Unnamed: 0\" and \"customerID\" do not carry any semantic load for building prediction models, so we will delete them.","08762d43":"The difference in the churn of subscribers in marriage depending on their gender is not significant"}}