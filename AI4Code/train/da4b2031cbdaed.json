{"cell_type":{"641d9c17":"code","e5c9f62a":"code","9ca25d93":"code","a5ece426":"code","e5649781":"code","50b7b75c":"code","637a2325":"code","336541c5":"code","5df7f83f":"code","3ebbb7d4":"code","e4bb56df":"code","3eb49c25":"code","4422c58d":"code","c46d6679":"code","83e40598":"code","0600023d":"code","a08af25e":"code","98984f90":"code","218414c8":"code","9c423928":"code","08911008":"code","a8670bb7":"code","f521b5a7":"code","0d79a213":"code","8e273c80":"code","a0a0d62f":"code","20d6b1c3":"code","54839635":"code","e166ddca":"code","22107814":"code","c38040a1":"code","e6f9ffaa":"code","14f3569b":"code","acda5eaa":"code","c706b6a3":"code","37ed407a":"code","fe419d0a":"markdown","2abe7dc2":"markdown","00099b40":"markdown","962b16f6":"markdown","5c2e907c":"markdown","afbdbe7e":"markdown","bc8df4d5":"markdown","57784894":"markdown","edeee545":"markdown","1b2f563f":"markdown","dccb54b8":"markdown","75677f0b":"markdown","f775ec58":"markdown","5806d5e2":"markdown","f1932198":"markdown"},"source":{"641d9c17":"import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score","e5c9f62a":"!head -2 '..\/input\/housesclean\/madrid_houses_clean.csv'","9ca25d93":"data = pd.read_csv('..\/input\/housesclean\/madrid_houses_clean.csv', sep=',', header=0, index_col=0)\ndata.drop(columns=['id'], inplace=True)\ndata.reset_index(drop=True,inplace=True)\ndata.head()","a5ece426":"data.info()","e5649781":"data[['is_renewal_needed', 'has_lift', 'is_exterior', 'has_parking']] = data[['is_renewal_needed', 'has_lift', 'is_exterior', 'has_parking']].astype('int64')\n\ndata.head()","50b7b75c":"print(data.buy_price.describe())\ndata.buy_price.hist(bins=10)\nplt.show()","637a2325":"fig, ax = plt.subplots(figsize=(15,10))\nsns.scatterplot(x='sq_mt_built', y='buy_price',hue='house_type', style='house_type',data=data)\nplt.show()","336541c5":"data = data[data.buy_price < 4000000]\ndata = data[data.sq_mt_built < 1000]\ndata.describe()","5df7f83f":"train_x, test_x = train_test_split(data, test_size=0.2, random_state=15)\ntrain_x, val_x = train_test_split(train_x, test_size=0.2, random_state=15)\nprint(\"{} train examples\".format(len(train_x)))\nprint(\"{} val examples\".format(len(val_x)))\nprint(\"{} test examples\".format(len(test_x)))","3ebbb7d4":"train_stats = train_x.describe().transpose()\ntrain_stats","e4bb56df":"def norm(x, stats):\n    '''Normalize dataframe's columns using the mean and standard deviation for each one\n       Parameters: Dataframe to normalize; Dataframe with statistics\n       Return: The normalized dataframe '''\n    \n    return (x - stats['mean']) \/ stats['std']\n\ntrain_xn = norm(train_x, train_stats)\nval_xn = norm(val_x, train_stats)\ntest_xn = norm(test_x, train_stats)","3eb49c25":"def data_tensor(df, shuffle=True, batch_size=64):\n    '''Transforms a dataframe into a tensor object, shuffling the data and grouping it into batches.\n       Parameters: Dataframe; shuffle option; batch size.\n       Returns: A group of batch size's tensors'''\n    \n    df = df.copy()\n    labels = df.pop('buy_price')\n    ts = tf.data.Dataset.from_tensor_slices((df.values, labels.values))\n    if shuffle:\n        ts = ts.shuffle(buffer_size=len(df))\n    ts = ts.batch(batch_size)\n    return ts\n","4422c58d":"train_ts = data_tensor(train_xn)\nval_ts = data_tensor(val_xn)","c46d6679":"model = tf.keras.Sequential()\nmodel.add(tf.keras.Input(shape=(14,)))\nmodel.add(tf.keras.layers.Dense(264, activation=tf.nn.tanh))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Dense(128, activation=tf.nn.sigmoid))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\nmodel.add(tf.keras.layers.Dropout(0.4))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dense(32, activation=tf.nn.tanh))\nmodel.add(tf.keras.layers.Dense(16, activation=tf.nn.sigmoid))\nmodel.add(tf.keras.layers.Dense(1))\n","83e40598":"model.summary()","0600023d":"\noptimizer = tf.keras.optimizers.Nadam()\n\nmodel.compile(loss='mean_squared_error', optimizer=optimizer)\n","a08af25e":"lrr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n                         patience = 50,\n                         verbose = 1,\n                         factor = 0.75,\n                         min_lr = 1e-6)\n\nes = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                   mode='min',\n                   verbose=1,\n                   patience=100,\n                   restore_best_weights=True)\n","98984f90":"history = model.fit(train_ts, validation_data=val_ts, epochs=500, callbacks=[lrr, es])","218414c8":"plt.figure(figsize=(15,5))\n\nplt.plot(history.history['loss'], label='loss')\n\nplt.plot(history.history['val_loss'], label='val_loss')\n\n\nplt.title('Model loss', weight='bold')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend()\nplt.show()","9c423928":"def de_norm(x, stats, target):\n    '''Transforms a normalized columns into its original values.\n       Parameters: Column to transform; Dataframe with statistics, Target column's name\n       Returns: Denormalized column'''\n    \n    return (x * stats.loc[target, 'std']) + stats.loc[target, 'mean']","08911008":"val_xn.pop('buy_price')\n\npredict = de_norm(model.predict(val_xn).flatten(), train_stats, 'buy_price')\nreal_labels = val_x.buy_price\nr2_score(real_labels, predict)","a8670bb7":"test_xn.pop('buy_price')\n\npredict_test = de_norm(model.predict(test_xn).flatten(), train_stats, 'buy_price')\ntest_labels = test_x.buy_price\nr2_score(test_labels, predict_test)","f521b5a7":"fig, ax = plt.subplots(figsize=(25, 15))\n\nplt.plot(predict, real_labels.values, 'ro')\nplt.xlabel('Predictions', fontsize = 30)\nplt.ylabel('Reality', fontsize = 30)\nplt.title('Predictions vs Reality on Validation', fontsize = 30)\nax.plot([real_labels.min(), real_labels.max()], [real_labels.min(), real_labels.max()], 'k--', lw=4)\nplt.show()","0d79a213":"rel_val = val_x.copy()\nrel_val['predicted'] = predict","8e273c80":"rel_val['error'] = rel_val.buy_price - rel_val.predicted\nsns.histplot(rel_val.error, bins=200)\nplt.xlim(-200000, 200000)\nplt.show()","a0a0d62f":"mode_error_small = rel_val.query('-50000 < error < 50000')\nmode_error_small.describe()","20d6b1c3":"mode_error_big = rel_val[(rel_val.error < -50000)]\nmode_error_big = mode_error_big.append(rel_val[(rel_val.error > 50000)])\nmode_error_big.describe()","54839635":"sns.pairplot(data=mode_error_small, x_vars=['sq_mt_built', 'n_rooms','n_bathrooms', 'neighborhood', 'district', 'house_type', 'buy_price', 'predicted'],\n            y_vars=['house_type', 'buy_price', 'predicted','error'])\nplt.show()","e166ddca":"sns.pairplot(data=mode_error_big, x_vars=['sq_mt_built', 'n_rooms','n_bathrooms', 'neighborhood', 'district', 'house_type', 'buy_price', 'predicted'],\n            y_vars=['house_type', 'buy_price', 'predicted','error'])\nplt.show()","22107814":"sns.pairplot(data=mode_error_small, x_vars=['n_floors', 'sq_mt_allotment', 'floor','is_renewal_needed', 'has_lift','is_exterior','energy_certificate',\n                                            'has_parking'], y_vars=['house_type', 'buy_price', 'predicted', 'error'])\nplt.show()","c38040a1":"sns.pairplot(data=mode_error_big, x_vars=['n_floors', 'sq_mt_allotment', 'floor','is_renewal_needed', 'has_lift','is_exterior','energy_certificate',\n                                          'has_parking'], y_vars=['house_type', 'buy_price', 'predicted','error'])\nplt.show()","e6f9ffaa":"rel_val.sort_values(by=['error'], ascending=False)","14f3569b":"pos_error = rel_val.query('error > 0')\npos_error.error.count()","acda5eaa":"neg_error = rel_val.query('error < 0')\nneg_error.error.count()","c706b6a3":"sns.pairplot(data=pos_error, x_vars=['sq_mt_built', 'n_rooms','n_bathrooms', 'neighborhood', 'district', 'house_type', 'buy_price', 'predicted'],\n            y_vars=['house_type', 'buy_price', 'predicted','error'])\nplt.show()","37ed407a":"sns.pairplot(data=neg_error, x_vars=['sq_mt_built', 'n_rooms','n_bathrooms', 'neighborhood', 'district', 'house_type', 'buy_price', 'predicted'],\n            y_vars=['house_type', 'buy_price', 'predicted','error'])\nplt.show()","fe419d0a":"We'll change the boolean columns into integers to work better with them.","2abe7dc2":"Outliers are affecting the model's ability to predict prices. We'll apply some techniques to deal with them.","00099b40":"With bigger errors, the prices' distributions are slightly different. The predicted price tends to be bigger than the correct one.\n\nThere are more 'houses' with this error, probably because they are the most expensive ones.\n\nWe can see the same effects on the other columns","962b16f6":"We split the data into three groups train, validation and test.","5c2e907c":"The first line contains the columns' names separated by commas and the first column is the index. We'll give this info while reading the dataset into a pandas Dataframe. We'll drop the column 'id' which is not necessary and reset the index.","afbdbe7e":"The mode is small, less than 50000\u20ac error per house.\n\nLet's see how these errors are distributed.\n\nWe'll divide the error in two groups, those under 50000\u20ac both positive (bellow correct price) and negative (above the correct price) and the rest.","bc8df4d5":"We obtained an accuracy of 88% for validation and test sets.\n\nLet's check the results.","57784894":"We normalize the data","edeee545":"Those with small errors are small houses, under 200 m\u00b2. The prices' distributions are very similar. \n\nThese errors are scattered among all neighborhoods, districts and specially flats.","1b2f563f":"We managed a low error in the training and validation sets.","dccb54b8":"We transform the dataframes into tensors that we shuffle and group into batches.","75677f0b":"Over 1 million, points are more scattered. As we had seen, this is outlier's territory and it becomes more difficult to calculate the right price as we don't have enough examples.","f775ec58":"If we analyse the kind of error, positive or negative, we find that there are, indeed, more negative errors (overpriced).\n\nHowever,the biggest errors are positive. Possibly because there are very few houses with very high prices. The model can't adjust for these, essentially, outliers.","5806d5e2":"We'll remove all houses over 4 million and over 1000 m\u00b2. It's a small group but very scattered. About 281 houses, 1.3%.\n\nThis will allow us to reduce the errors' values.","f1932198":"The majority of the data is under 2000000\u20ac. The rest of the data is extremely skewed."}}