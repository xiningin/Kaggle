{"cell_type":{"a4f85d73":"code","0f962856":"code","263a5ea0":"code","494adf2c":"code","121d27a4":"code","a40b0ebd":"code","646e83f8":"code","53fbb898":"code","7f41fb11":"code","8ecb89a6":"code","9d0df2bb":"code","9e5af266":"code","0831c643":"code","428fdf4d":"code","75785613":"code","2b07acf7":"code","5bc8978f":"code","ee371d2a":"code","7b7a8578":"code","a63f876d":"code","3c5913dd":"code","c8022f5d":"code","c2ee0d0d":"code","2cf2db87":"code","fdd8c1eb":"code","a1c5d71c":"code","d0732865":"code","70c56f5e":"code","50e31057":"code","d0a6dbc3":"code","e050e862":"code","3ca9c4e0":"code","c1fca992":"code","a1137aa0":"code","2b65adb7":"code","7df5b523":"code","5a265bcd":"code","0bbcadf6":"code","b45299b6":"code","568ac700":"code","0fa66c81":"code","84dd46d7":"code","267a167d":"code","a7e7ec7d":"code","804194e9":"code","bda5985e":"code","b7c14c68":"code","9e4d8063":"code","15338bd2":"code","c8b74124":"code","512c5dab":"code","43bc0e4d":"code","902736c0":"code","ee5a3cfb":"code","2087e9fb":"code","29dfbb53":"code","5a2eaa7f":"code","8947f7f6":"code","81e85d5b":"code","b35f771a":"code","9d9fbf3b":"code","38f86622":"code","f4965635":"code","2b833ad4":"code","32438cbe":"code","214cdeb8":"code","67d22834":"code","7e443961":"code","4fedb661":"code","f13c396c":"code","e9eebc3d":"code","affd2dd1":"code","5898cddf":"code","4d14f97d":"code","955b080a":"code","c154b6dd":"code","da69f878":"code","f94e7c0b":"code","e94149e1":"code","33acc476":"code","204d7a19":"code","a26ef82d":"code","2d9ae066":"code","96988584":"code","5299a662":"code","1d943410":"code","9b0f50c4":"markdown","4c0b068d":"markdown","31d36a97":"markdown","70524fa9":"markdown","8248d28f":"markdown","30a1cdc2":"markdown","83462ae2":"markdown","a5e1983e":"markdown","4ad5035f":"markdown","91b2063d":"markdown","f1b1badf":"markdown","64d169a3":"markdown","ba16a13d":"markdown","c282fd4d":"markdown","c86e7f21":"markdown","6d057d8d":"markdown","572d0a51":"markdown","ab1441a1":"markdown","caf62d79":"markdown","0e833a5c":"markdown","40603e05":"markdown","0b3fc97b":"markdown","9175923a":"markdown","ad9db29a":"markdown","beebc951":"markdown","ff1625bc":"markdown","ae58dba6":"markdown","1dc6b478":"markdown","ec73844b":"markdown","5eda7d97":"markdown"},"source":{"a4f85d73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0f962856":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas import plotting\n\n#plotly \nimport plotly.offline as py\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\nplt.style.use('fivethirtyeight')\n\n# warning\nimport sys\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")","263a5ea0":"df=pd.read_csv('..\/input\/car-data\/CarPrice_Assignment.csv')\ndf.head().T","494adf2c":"df.shape","121d27a4":"k= pd.DataFrame()\nk['df']= df.isnull().sum()\nk.T","a40b0ebd":"df.info()","646e83f8":"df.columns","53fbb898":"object=['CarName', 'fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel','enginelocation',\n       'enginetype', 'cylindernumber', 'fuelsystem', ]\n        ","7f41fb11":"for i in object:   \n    col = i\n    grouped = df[col].value_counts().reset_index()\n    grouped = grouped.rename(columns = {col : \"count\", \"index\" : col})\n\n    ## plot\n    trace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0], marker=dict(colors=[\"#6ad49b\", \"#a678de\"]))\n    layout = go.Layout(title=i, height=600, legend=dict(x=0.1, y=1.1))\n    fig = go.Figure(data = [trace], layout = layout)\n    iplot(fig)","8ecb89a6":"from scipy import stats\n","9d0df2bb":"kendal_df=pd.DataFrame(columns=['tau','p-value'])\nfor col in df:\n   # print(col)\n    if pd.api.types.is_numeric_dtype(df[col]):\n        t, p=stats.kendalltau(df['price'], df[col])\n        kendal_df.loc[col]=[t, p]\nkendal_df","9e5af266":"df.info()","0831c643":"df.columns","428fdf4d":"numeric=['symboling', 'wheelbase', 'carlength', 'carwidth', 'carheight', 'curbweight','enginesize',\n        'boreratio', 'stroke',\n       'compressionratio', 'horsepower', 'peakrpm', 'citympg', 'highwaympg']","75785613":"object=['fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel','enginelocation',\n       'enginetype', 'cylindernumber', 'fuelsystem', ]","2b07acf7":"for i in object:\n    #plt.figure(figsize=(10,10))\n    sns.kdeplot(df['price'], fill=True, hue=df[i], log_scale=10)\n    plt.show()","5bc8978f":"from scipy import stats\n","ee371d2a":"r, p=stats.pearsonr(df['symboling'], df['price'])\ngraph=sns.jointplot(df['symboling'], df['price'], kind=\"reg\", color=\"#ce1414\",)\nphantom, =graph.ax_joint.plot([],[], linestyle=\"\", alpha=0)\ngraph.ax_joint.legend([phantom],['r={:f}, p={:f}'.format(r,p)])\nplt.show()","7b7a8578":"for i in numeric:  \n    r, p=stats.pearsonr(df[i], df['price'])\n    graph=sns.jointplot(df[i], df['price'], kind=\"reg\", color=\"#ce1414\",)\n    phantom, =graph.ax_joint.plot([],[], linestyle=\"\", alpha=0)\n    graph.ax_joint.legend([phantom],['r={:f}, p={:f}'.format(r,p)])\n    plt.show()","a63f876d":"for i in numeric:  \n    r, p=stats.spearmanr(df[i], df['price'])\n    graph=sns.jointplot(df[i], df['price'], kind=\"reg\", color=\"#ce1414\",)\n    phantom, =graph.ax_joint.plot([],[], linestyle=\"\", alpha=0)\n    graph.ax_joint.legend([phantom],['r={:f}, p={:f}'.format(r,p)])\n    plt.show()","3c5913dd":"sns.set()\nsns.pairplot(df,size = 2.5, kind = \"reg\",corner=True)","c8022f5d":"df.columns","c2ee0d0d":"object=['fueltype',\n 'aspiration',\n 'doornumber',\n 'carbody',\n 'drivewheel',\n 'enginelocation',\n 'enginetype',\n 'cylindernumber',\n 'fuelsystem', 'CarName']","2cf2db87":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n#cols=['car_ID', 'price']\nX=df.drop(object, axis=1)","fdd8c1eb":"X.info()","a1c5d71c":"vif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                          for i in range(len(X.columns))]\nprint(vif_data)","d0732865":"def grubbs_test(x):\n    n = len(x)\n    mean_x = np.mean(x)\n    sd_x = np.std(x)\n    numerator = max(abs(x-mean_x))\n    g_calculated = numerator\/sd_x\n    print(\"Grubbs Calculated Value:\",g_calculated)\n    t_value = stats.t.ppf(1 - 0.05 \/ (2 * n), n - 2)\n    g_critical = ((n - 1) * np.sqrt(np.square(t_value))) \/ (np.sqrt(n) * np.sqrt(n - 2 + np.square(t_value)))\n    print(\"Grubbs Critical Value:\",g_critical)\n    if g_critical > g_calculated:\n        print(\"From grubbs_test we observe that calculated value is lesser than critical value, Accept null hypothesis and conclude that there is no outliers\\n\")\n    else:\n        print(\"From grubbs_test we observe that calculated value is greater than critical value, Reject null hypothesis and conclude that there is an outliers\\n\")","70c56f5e":"for i in numeric:\n    print(i)\n    grubbs_test(df[i])","50e31057":"out=[]\ndef Zscore_outlier(data):\n    m=np.mean(data)\n    sd=np.std(data)\n    for i in data:\n        z=(i-m)\/sd\n        if np.abs(z)>3:\n            out.append(i)\n    print(\"Outlier\", out)\n    ","d0a6dbc3":"for i in numeric:\n    print(i)\n    Zscore_outlier(df[i])","e050e862":"out=[]\ndef iqr_outliers(data):\n    q1=data.quantile(0.25)\n    q3=data.quantile(0.75)\n    iqr=q3-q1\n    Lower_tail = q1 - 1.5 * iqr\n    Upper_tail = q3 + 1.5 * iqr\n    for i in data:\n        if i>Upper_tail or i<Lower_tail:\n            out.append(i)\n    print(\"Outlier\", out)\n    ","3ca9c4e0":"for i in numeric:\n    print(i)\n    iqr_outliers(df[i])","c1fca992":"# y=b0+b1x1\n\nx=sm.add_constant(X)\n\nresults=sm.OLS(df['price'],x).fit()\n#Contain Ordinary Least Square Regression \nresults.summary()","a1137aa0":"df.columns","2b65adb7":"df['enginelocation'].value_counts()","7df5b523":"fueltype={'gas':1,'diesel':0}\ndf['fueltype']=[fueltype[x] for x in df['fueltype']]\n\n# aspiration \naspiration={'std':1,'turbo':0}\ndf['aspiration']=[aspiration[x] for x in df['aspiration']]\n# doornumber\ndoornumber={'four':1,'two':0}\ndf['doornumber']=[doornumber[x] for x in df['doornumber']]\n\n#drive wheel\nenginelocation={'front':1,'rear':0}\ndf['enginelocation']=[enginelocation[x] for x in df['enginelocation']]\n\n\n\n","5a265bcd":"df.info()","0bbcadf6":"df.columns","b45299b6":"col=['car_ID','CarName','carbody', 'drivewheel',  'enginetype',\n       'cylindernumber','fuelsystem', 'price']","568ac700":"cols=[ 'symboling', 'fueltype', 'aspiration',\n       'doornumber', 'enginelocation', 'wheelbase',\n       'carlength', 'carwidth', 'carheight', 'curbweight',  'enginesize',  'boreratio', 'stroke',\n       'compressionratio', 'horsepower', 'peakrpm', 'citympg', 'highwaympg',\n       ]","0fa66c81":"\nX=df.drop(col, axis=1)\ny=df['price']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","84dd46d7":"from sklearn import metrics\nfrom sklearn.linear_model import LinearRegression","267a167d":"lr=LinearRegression()\nlr.fit(X_train, y_train)","a7e7ec7d":"y_pred=lr.predict(X_train)\n","804194e9":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_train, y_pred))\nprint(\"Max Error:-\", metrics.max_error(y_train, y_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_train, y_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_train, y_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_train, y_pred))","bda5985e":"y_test_pred=lr.predict(x_test)","b7c14c68":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_test, y_test_pred))\nprint(\"Max Error:-\", metrics.max_error(y_test, y_test_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_test, y_test_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_test, y_test_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_test, y_test_pred))","9e4d8063":"X_train.shape, y_train.shape","15338bd2":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.model_selection import cross_val_score, train_test_split\n","c8b74124":"r2scores=[]\nadjustedr2=[]\n\nfor i in range(1, 19):\n    R2 = cross_val_score(lr, X=X_train, y=y_train,cv=10, scoring='r2').mean()    \n    r2scores.append(R2)\n    \n    n= len(X_train)\n    p = i #len(X.columns)\n    adj_R2 = 1- ((1-R2) * (n-1)\/(n-p-1)) #Adj R2 = 1-(1-R2)*(n-1)\/(n-p-1)\n    # print(r2, adjustedr2)\n    adjustedr2.append(adj_R2)\n    ","512c5dab":"scoring_df = pd.DataFrame(np.column_stack((r2scores, adjustedr2)), columns=['R2', 'Adj_R2'])\n#scoring_df['feature_names'] = train.columns\nscoring_df['features'] = range(1, 19)\nscoring_df","43bc0e4d":"fig, ax = plt.subplots(figsize=(8, 6))\n#convert data frame from wide format to long format so that we can pass into seaborn line plot function to draw multiple line plots in same figure\n# https:\/\/stackoverflow.com\/questions\/52308749\/how-do-i-create-a-multiline-plot-using-seaborn\nlong_format_df = pd.melt(scoring_df.loc[:, ['features','R2', 'Adj_R2']], ['features'])\nsns.lineplot(x='features', y='value', hue='variable', data=long_format_df, ax=ax)\nax.set_xlabel('No of features')\nax.set_ylabel('Cross validated R2 and Adj R2 scores')\nax.set_title('Plot between number of features and R2\/Adj R2 scores')","902736c0":"from sklearn.neighbors import KNeighborsRegressor","ee5a3cfb":"error_rate=[]\n\nfor i in range(1,11):\n    knn=KNeighborsRegressor(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred=knn.predict(x_test)\n    error_rate.append(np.mean(pred!=y_test))\n    \nplt.figure(figsize=(15,10))\nplt.plot(range(1,11), error_rate,marker='o', markersize=9)","2087e9fb":"knn=KNeighborsRegressor(n_neighbors=2)\nknn.fit(X_train, y_train)","29dfbb53":"y_pred=knn.predict(X_train)","5a2eaa7f":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_train, y_pred))\nprint(\"Max Error:-\", metrics.max_error(y_train, y_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_train, y_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_train, y_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_train, y_pred))","8947f7f6":"y_test_pred=knn.predict(x_test)","81e85d5b":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_test, y_test_pred))\nprint(\"Max Error:-\", metrics.max_error(y_test, y_test_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_test, y_test_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_test, y_test_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_test, y_test_pred))","b35f771a":"error_rate=[]\n\nfor i in range(1,50):\n    knn=KNeighborsRegressor(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred=knn.predict(x_test)\n    error_rate.append(metrics.mean_absolute_error(y_test, pred))\n    \nplt.figure(figsize=(15,10))\nplt.plot(range(1,50), error_rate,marker='o', markersize=9)","9d9fbf3b":"error_rate=[]\n\nfor i in range(1,10):\n    knn=KNeighborsRegressor(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred=knn.predict(x_test)\n    error_rate.append(metrics.mean_absolute_error(y_test, pred))\n    \nplt.figure(figsize=(15,10))\nplt.plot(range(1,10), error_rate,marker='o', markersize=9)","38f86622":"knn=KNeighborsRegressor(n_neighbors=3)\nknn.fit(X_train, y_train)","f4965635":"y_pred=knn.predict(X_train)","2b833ad4":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_train, y_pred))\nprint(\"Max Error:-\", metrics.max_error(y_train, y_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_train, y_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_train, y_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_train, y_pred))","32438cbe":"y_test_pred=knn.predict(x_test)","214cdeb8":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_test, y_test_pred))\nprint(\"Max Error:-\", metrics.max_error(y_test, y_test_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_test, y_test_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_test, y_test_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_test, y_test_pred))","67d22834":"r2scores=[]\nadjustedr2=[]\n\nfor i in range(1, 19):\n    R2 = cross_val_score(knn, X=X_train, y=y_train,cv=10, scoring='r2').mean()    \n    r2scores.append(R2)\n    \n    n= len(X_train)\n    p = i #len(X.columns)\n    adj_R2 = 1- ((1-R2) * (n-1)\/(n-p-1)) #Adj R2 = 1-(1-R2)*(n-1)\/(n-p-1)\n    # print(r2, adjustedr2)\n    adjustedr2.append(adj_R2)\n    ","7e443961":"scoring_df = pd.DataFrame(np.column_stack((r2scores, adjustedr2)), columns=['R2', 'Adj_R2'])\n#scoring_df['feature_names'] = train.columns\nscoring_df['features'] = range(1, 19)\nscoring_df","4fedb661":"fig, ax = plt.subplots(figsize=(8, 6))\n#convert data frame from wide format to long format so that we can pass into seaborn line plot function to draw multiple line plots in same figure\n# https:\/\/stackoverflow.com\/questions\/52308749\/how-do-i-create-a-multiline-plot-using-seaborn\nlong_format_df = pd.melt(scoring_df.loc[:, ['features','R2', 'Adj_R2']], ['features'])\nsns.lineplot(x='features', y='value', hue='variable', data=long_format_df, ax=ax)\nax.set_xlabel('No of features')\nax.set_ylabel('Cross validated R2 and Adj R2 scores')\nax.set_title('Plot between number of features and R2\/Adj R2 scores')","f13c396c":"from sklearn.tree import DecisionTreeRegressor","e9eebc3d":"dc=DecisionTreeRegressor()\ndc.fit(X_train, y_train)","affd2dd1":"y_pred=dc.predict(X_train)","5898cddf":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_train, y_pred))\nprint(\"Max Error:-\", metrics.max_error(y_train, y_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_train, y_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_train, y_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_train, y_pred))","4d14f97d":"y_test_pred=dc.predict(x_test)","955b080a":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_test, y_test_pred))\nprint(\"Max Error:-\", metrics.max_error(y_test, y_test_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_test, y_test_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_test, y_test_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_test, y_test_pred))","c154b6dd":"y_test_pred=dc.predict(x_test)","da69f878":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_test, y_test_pred))\nprint(\"Max Error:-\", metrics.max_error(y_test, y_test_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_test, y_test_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_test, y_test_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_test, y_test_pred))","f94e7c0b":"r2scores=[]\nadjustedr2=[]\n\nfor i in range(1, 19):\n    R2 = cross_val_score(dc, X=X_train, y=y_train,cv=10, scoring='r2').mean()    \n    r2scores.append(R2)\n    \n    n= len(X_train)\n    p = i #len(X.columns)\n    adj_R2 = 1- ((1-R2) * (n-1)\/(n-p-1)) #Adj R2 = 1-(1-R2)*(n-1)\/(n-p-1)\n    # print(r2, adjustedr2)\n    adjustedr2.append(adj_R2)","e94149e1":"scoring_df = pd.DataFrame(np.column_stack((r2scores, adjustedr2)), columns=['R2', 'Adj_R2'])\n#scoring_df['feature_names'] = train.columns\nscoring_df['features'] = range(1, 19)\nscoring_df","33acc476":"fig, ax = plt.subplots(figsize=(8, 6))\n#convert data frame from wide format to long format so that we can pass into seaborn line plot function to draw multiple line plots in same figure\n# https:\/\/stackoverflow.com\/questions\/52308749\/how-do-i-create-a-multiline-plot-using-seaborn\nlong_format_df = pd.melt(scoring_df.loc[:, ['features','R2', 'Adj_R2']], ['features'])\nsns.lineplot(x='features', y='value', hue='variable', data=long_format_df, ax=ax)\nax.set_xlabel('No of features')\nax.set_ylabel('Cross validated R2 and Adj R2 scores')\nax.set_title('Plot between number of features and R2\/Adj R2 scores')","204d7a19":"from sklearn.ensemble import RandomForestRegressor","a26ef82d":"rff=DecisionTreeRegressor()\nrff.fit(X_train, y_train)","2d9ae066":"y_pred=rff.predict(X_train)","96988584":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_train, y_pred))\nprint(\"Max Error:-\", metrics.max_error(y_train, y_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_train, y_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_train, y_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_train, y_pred))","5299a662":"y_test_pred=dc.predict(x_test)","1d943410":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_test, y_test_pred))\nprint(\"Max Error:-\", metrics.max_error(y_test, y_test_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_test, y_test_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_test, y_test_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_test, y_test_pred))","9b0f50c4":"# Calculate the Adjusted- R square","4c0b068d":"# VIF- Variance Inflation Factor\nThe variance inflation factor is another way to express exactly the same information found in the coefficient of multiple correlation. A variance inflation factor is computed for each independent variable, using the following formula:\n\nVIFk = 1 \/ ( 1 - R2k )\n\nwhere VIFk is the variance inflation factor for variable k, and R2k is the coefficient of multiple determination for variable k.\n\nIn many statistical packages (e.g., SAS, SPSS, Minitab), the variance inflation factor is available as an optional regression output. In MiniTab, for example, the variance inflation factor can be displayed as part of the regression coefficient table.","31d36a97":"# Shape of the dataset","70524fa9":"# [Multicolinearity](https:\/\/www.kaggle.com\/omgavy\/multicollinearity-analysis-eda)\n* Number of way to check the multicolinearity \n* VIF\n* Correlation Mtarix\n* ETC","8248d28f":"# Conclusion:-\n* This is how the categorical variables are visualize","30a1cdc2":"# HYPOTHESIS TESTING(GRUBBS TEST)\n* Grubbs' test is defined for the hypothesis:  \n* Ho:  There are no outliers in the data set \n* H1: There is exactly one outlier in the data set \n*  If the calculated value is greater than critical, you can reject the null hypothesis and conclude that one of the values is an outlier ","83462ae2":"# Decision Tree","a5e1983e":"# Missing Values","4ad5035f":"# Observation:-\n* Wheelbase, Enginesize, stroke, horsepower, citympg-- in these features we can see that outliers \n* we need to find out the outliers","91b2063d":"# Z-SCORE METHOD\n* Using Z score method,we can find out how many standard deviations value away from the mean.\n* Figure in the left shows area under normal curve and how much area that standard deviation covers.\n* 68% of the data points lie between + or - 1 standard deviation.\n* 95% of the data points lie between + or - 2 standard deviation\n* 99.7% of the data points lie between + or - 3 standard deviation\n* Z-score=(X-Mean)\/(standard Deviation)\n","f1b1badf":"# K=3 is best ","64d169a3":"# Conclusion \n* We can see that the MSE is important factor to consider while regression.\n","ba16a13d":"# Visualizationthe categorical features","c282fd4d":"# IQR METHOD\n* Q1 represents the 1st quartile\/25th percentile of the data.\n* Q2 represents the 2nd quartile\/median\/50th percentile of the data.\n* Q3 represents the 3rd quartile\/75th percentile of the data.\n* (Q1\u20131.5*IQR) represent the smallest value in the data set and (Q3+1.5*IQR) represnt the largest value in the data set.","c86e7f21":"# Linear Regression","6d057d8d":"# VIsualizing the Numerical Features","572d0a51":"# Observation \n* The features which are highly correlated with response variable are ---WheelBase, Carlength, carwidth, curbweight, Enginesize, boreratio,\n* Less correlated- Symboling, stroke, compressionratio, peakrpm,\n* Negativly correlated- citympg, highwaympg, ","ab1441a1":"# KNN","caf62d79":"# Observing \n* The distribution of the Price given under the different condition.\n* Performing the logarithmic conversion we see that the distribution under the specific condition is almost almost gaussian.","0e833a5c":"# [OUTLIER DETECTION](https:\/\/www.kaggle.com\/nareshbhat\/outlier-the-silent-killer)\nOutlier can be of two types: Univariate and Multivariate. Above, we have discussed the example of univariate outlier. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space.\n# DIFFERENT OUTLIER DETECTION TECHNIQUE.\n1. Hypothesis Testing\n2. Z-score method\n3. Robust Z-score\n4. I.Q.R method\n5. Winsorization method(Percentile Capping)\n6. DBSCAN Clustering\n7. Isolation Forest\n8. Visualizing the data","40603e05":"# Converting Categorical Variables","0b3fc97b":"# joint plot with Pearson's correlation ","9175923a":"# Observation \n* As we can see that MSE is very high, SInce it is the error function which is supposed to be minimized. \n* As we can see that there is no case of overfitting as both test and train error are almost equals. ","ad9db29a":"# Observation \n* As I can see that  ","beebc951":"# Converting categorical into numerical","ff1625bc":"# Cross Validation ","ae58dba6":"# Random Forest ","1dc6b478":"# Joint Plot with spearman coeeficient","ec73844b":"# Observation:\n* As we can see that VIF of the most of the numerical features are greater than 10.\n* The VIF is maximum for carwidth.Least for symboling. \n* Since as we know that VIF>10, those features are removed from the dataset to find out the modelling.","5eda7d97":"# Ordinary Least Square Method\nA Model Summary\n\nDep. Variable means variable that we want to predict and also called target variable, which is Marks\nModel : OLS (Ordinary Least Square) , OLS is the common method to estimate the linear regression , this method will find the line which minimises the Sum of the Squared Error ( Lower Error = better explanatory power )"}}