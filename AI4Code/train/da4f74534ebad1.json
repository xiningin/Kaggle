{"cell_type":{"d5f0baf3":"code","62fbdd94":"code","b6df03ff":"code","fcca0289":"code","b8b27fd4":"code","c79b17c6":"code","155146a5":"code","691a4ddc":"code","135cd765":"code","8d1b4c2f":"code","57c84880":"code","e1f41a32":"code","cd437991":"code","16716510":"code","97af4cbf":"code","2018c6fd":"code","66ebffa2":"code","53e8cb82":"markdown","69f00a6a":"markdown","6a0e3ddb":"markdown","4f3e0d15":"markdown","1edf1a51":"markdown","39556030":"markdown","1036e7a7":"markdown","0bf7aa14":"markdown","0f0be19f":"markdown","9e918269":"markdown","d14959ff":"markdown","481aa20e":"markdown"},"source":{"d5f0baf3":"import pandas as pd\nimport numpy as np","62fbdd94":"df = pd.read_csv('..\/input\/winemag-data-130k-v2.csv')","b6df03ff":"df.info()","fcca0289":"#Amount of uniques varieties\nlen(df.variety.value_counts().index.values)","b8b27fd4":"#What varieties are the most described or tested by the wine taster\ncount = 0\nfor l in df.variety.value_counts()>1000:\n    if l:\n        count += 1\nprint (count , '  Are de varieties with more than 1000 descriptions')","c79b17c6":"data = list(df.description.values)","155146a5":"from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer","691a4ddc":"no_features = 1000","135cd765":"tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.95, max_features=no_features, stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(data)\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()","8d1b4c2f":"tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\ntf = tf_vectorizer.fit_transform(data)\ntf_feature_names = tf_vectorizer.get_feature_names()","57c84880":"from sklearn.decomposition import NMF , LatentDirichletAllocation","e1f41a32":"no_topic = 28","cd437991":"#for NMF\n\nnmf = NMF(n_components = no_topic , random_state=1 , alpha=.1 , l1_ratio=.5 , init='nndsvd').fit(tfidf)\n\n\n#for LDA\n\nlda = LatentDirichletAllocation(n_components=no_topic , max_iter=10 , learning_method='online' , learning_offset=50. , random_state=0)\n\nlda_v = lda.fit(tf)","16716510":"def display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print (\"Topic %d:\" % (topic_idx))\n        print (\" \".join([feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\nno_top_words = 10\ndisplay_topics(nmf, tfidf_feature_names, no_top_words)\ndisplay_topics(lda_v, tf_feature_names, no_top_words)","97af4cbf":"import pyLDAvis.sklearn","2018c6fd":"pyLDAvis.enable_notebook()\npanel = pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer, mds='tsne')","66ebffa2":"panel","53e8cb82":"Here I will be focused on \"description\" and \"variety\", firstly in the \"description\" class because I want to generate groups of words and make an analisis of them. The goal here is to understand what the varieties have in common and which are the first revealed topics.","69f00a6a":"Let\u00b4s see what we have.","6a0e3ddb":"**Term frequency \u2013 Inverse document frequency**","4f3e0d15":"**So, the next step is to interpret the topics to reduce the dimensionality of the dataset and group the most similar varieties.**\n\n","1edf1a51":"**CountVectorizer**","39556030":"**First topics**","1036e7a7":"Using this tool we can vary the lambda parameter to observe how the terms are grouped and their frequency of appearancem","0bf7aa14":"**DATA - BAG OF WORDS**","0f0be19f":"Parameters **min_df** and **max_df** are the minimum and maximum number of apparition of one particular word to be counted in the algorithm also I am using StopWords to reduce the noise in the output.\n\n** Float numbers are in percent and integer are absolute apparition.*","9e918269":"**Hi everybody!**\n\nHere I will try to understand how the wines variety are related which each other and how we can identify the varieties by observing the description.\n\nThis work was done in a few separated notebooks. My final goal is to predict de wine variety through the description included in the dataset provided.\n\nFirst I will test some Topic Modeling with LDA, NMF and PyLDAvis\n\nI will work with Term frequency \u2013 Inverse document frequency (tf-idf) and Count Vectorizer ","d14959ff":"**pyLDAvis for visualization**","481aa20e":"Ok, I can\u00b4t work with more than 100 topics. So I will work with the most relevant varieties making a set of 28 topics to analyse."}}