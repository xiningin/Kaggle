{"cell_type":{"7f494064":"code","e89bf0ea":"code","edd327aa":"code","004415f0":"code","daaf32c7":"code","21cb6a5f":"code","cc00069d":"code","b5900615":"code","ee4d0d02":"code","bb60edf5":"code","632c2e53":"code","d370d130":"code","2b58512a":"code","e4128634":"code","bb10cd7a":"code","61b518d7":"code","c741f662":"code","fe09e1cf":"markdown","1156d871":"markdown","dc201f82":"markdown","4742f206":"markdown","c74f882e":"markdown","b25556d0":"markdown","643b62d0":"markdown","100d6611":"markdown","b1fae512":"markdown","f68f5666":"markdown","c59d0919":"markdown"},"source":{"7f494064":"# Utility Functions\ndef trim(s) -> str:\n    if s == None:\n        return s\n\n    return re.sub(r'(^\\s+|\\s+$)', '', str(s))\n\ndef is_empty(s) -> bool:\n    return s == None or trim(s) == '' \n\ndef clean_str(s:str) -> str:\n    if s == None or str(s) == 'nan':\n        return None\n\n    s = re.sub(r'[^a-zA-Z0-9\\s.-_#:,]', '', str(s), re.DOTALL)\n    return None if len(s) == 0 else s\n\ndef to_bool(s):\n    rx = re.match(\"(1|true|yes)\", trim(str(s).lower()), re.DOTALL)\n    if rx:\n        return rx.group() in ['1', 'yes', 'true']\n\n    return False\n\ndef hash_author(author) -> str:\n\n    def _in(prop:str, o) -> str:\n        return trim(o[prop]) if prop in o else ''\n\n    f = _in('first', author)\n    l = _in('last', author)\n    m = _in('middle', author)\n\n    combined = str.format(\"{0}{1}{2}\", f, l, m)\n    hashed = hashlib.md5(combined.encode())\n    return hashed.hexdigest()\n\ndef hash_strings(strings) -> str:\n    a = [trim(s) for s in strings if not is_empty(s)]\n    return hashlib.md5(''.join(a).encode()).hexdigest()\n\ndef is_iter(o):\n    try:\n        iter(o)\n        return True\n    except:\n        return False\n\ndef to_bool(s):\n    rx = re.match(\"^(1|true|yes)$\", trim(str(s).lower()))\n    if rx:\n        return rx.group() in ['1', 'yes', 'true']\n\n    return False\n\ndef clean_str(s):\n    if s == None or str(s) == 'nan':\n        return None\n\n    s = re.sub( r'\\n', ' ', str(s), re.DOTALL)\n    s = re.sub( r'\\r\\n', ' ', s, re.DOTALL)\n    s = re.sub( r'(^\\'+|\\'+$)', \"\", s, re.DOTALL)\n    s = re.sub( r'\\\\+', \"\", s, re.DOTALL)\n    s = re.sub( r'\\'+', '\\'\\'', s, re.DOTALL)\n    s = s.encode('ascii', 'ignore').decode()\n    return None if len(s) == 0 else s\n\ndef to_int(s) -> int:\n    try:\n        n = int(is_dbnull(s))\n        return n\n    except:\n        return None\n\ndef convert_to_json(contents):\n    return json.loads(contents)\n\ndef read_json(path):\n    infile = open(path, \"r\")\n    return convert_to_json(infile.read())\n\ndef read_files(path):\n    file_list = []\n    for root, folders, docs in os.walk(path):\n        file_list.extend( [os.path.join(root, doc) for doc in docs if '.json' in doc] )\n\n    return file_list\n\ndef is_dbnull(s):\n    if s == None:\n        return 'NULL'\n\n    return 'NULL' if clean_str(s) == None else str.format(\"'{0}'\", clean_str(s))","e89bf0ea":"import json\nimport os\nimport re\nimport hashlib\nimport pyodbc \n\n\"\"\"\nImport JSON articles into SQL Server\n\"\"\"\n# change to your params\n\nROOT = \"\/data\/\"\nFOLDER = ROOT+\"json\/\"\n\nbatch_info = {\n    'commands': [],\n    'paper_count': 0\n}\n\ndef execute_batch(sql):\n    commands = batch_info['commands']\n    commands.append(sql)\n    \n    if len(commands) < 100:\n        return\n        \n    print('Executing batch...')\n    batch = ';\\r\\n'.join(commands)\n\n    conn = pyodbc.connect('Driver={SQL Server};'\n                          'Server=.\\MSSQLSERVER19;'\n                          'Database=covid19;'\n                          'Trusted_Connection=yes;')\n    \n    cursor = conn.cursor()\n    try:\n        cursor.execute(batch)\n        conn.commit()\n    except:\n        with open(ROOT+\"sql_log.sql\", \"w\") as f:\n            f.writelines(batch)\n            \n        raise Exception(\"SQL syntax error. Exiting.\")\n        \n    print('Batch executed.')\n    print(str.format(\"Papers imported {0}\", batch_info['paper_count']))\n    commands.clear()\n    \ndef insert_institution(affiliation):\n    if 'laboratory' not in affiliation and 'institution' not in affiliation:\n        return None\n\n    laboratory = clean_str(affiliation['laboratory'])\n    institution = clean_str(affiliation['institution'])\n\n    if laboratory == None and institution == None:\n        return None\n\n    hash_id = hash_strings([institution, laboratory])\n    addrLine = None\n    postCode = None\n    region = None\n    country = None\n\n    if 'location' in affiliation:\n        loc = affiliation['location']\n        addrLine = loc['addrLine'] if 'addrLine' in loc else None\n        postCode = loc['postCode'] if 'postCode' in loc else None\n        region = loc['region'] if 'region' in loc else None\n        country = loc['country'] if 'country' in loc else None\n    \n    inst = {\n        'hash_id': hash_id, \n        'institution_name': institution, \n        'laboratory': laboratory, \n        'addrLine': addrLine, \n        'postCode': postCode, \n        'region': region, \n        'country': country\n    }\n    \n    o =  json.loads(json.dumps(inst), encoding='utf-8')\n    \n    s = \"\"\"INSERT INTO import.Institution(hash_id, institution_name, laboratory, addrLine, \n            postCode, region, country) VALUES({0}, {1}, {2}, {3}, {4}, {5}, {6})\"\"\"\n    \n    sql = str.format(s, is_dbnull(o['hash_id']), is_dbnull(o['institution_name']), is_dbnull(o['laboratory']), \\\n            is_dbnull(o['addrLine']), is_dbnull(o['postCode']), is_dbnull(o['region']), is_dbnull(o['country']))\n    \n    execute_batch(sql)\n\n    return hash_id\n\ndef insert_journal(name, issn):\n    journal_name = clean_str(name)\n    journal_issn = clean_str(issn)\n\n    if journal_name == None and journal_issn == None:\n        return\n\n    hash_id = hash_strings([journal_name, journal_issn])\n    \n    journal = {\n        'hash_id': hash_id, \n        'journal_name': journal_name, \n        'issn': journal_issn\n    }\n    \n    o = json.loads(json.dumps(journal), encoding='utf-8')\n    \n    sql = str.format(\"INSERT INTO import.Journal(hash_id, journal_name, issn) VALUES({0}, {1}, {2})\", \\\n        is_dbnull(o['hash_id']), is_dbnull(o['journal_name']), is_dbnull(o['issn']))\n    \n    execute_batch(sql)\n   \n    return hash_id\n\ndef insert_author(auth):\n    hash_id = hash_author(auth)\n    first = clean_str(auth['first']) if 'first' in auth else None  \n    last = clean_str(auth['last']) if 'last' in auth else None\n    suffix = clean_str(auth['suffix']) if 'suffix' in auth else None\n    email = clean_str(auth['email']) if 'email' in auth else None  \n    middle = None\n\n    if 'middle' in auth and len(auth['middle']) > 0:\n        middle = clean_str( \".\".join( list( map(trim, auth['middle']) ) ) )\n        \n    author = {\n        'hash_id': hash_id,\n        'first_name': first,\n        'last_name': last,\n        'middle': middle,\n        'suffix': suffix,\n        'email': email\n    }\n    \n    o = json.loads(json.dumps(author), encoding='utf-8')\n    \n    s = \"\"\"INSERT INTO import.Author(hash_id,first_name,last_name,middle,suffix,email) \n            VALUES({0}, {1}, {2}, {3}, {4}, {5})\"\"\"\n    sql =  str.format(s, is_dbnull(o['hash_id']), is_dbnull(o['first_name']), is_dbnull(o['last_name']), is_dbnull(o['middle']), \\\n                      is_dbnull(o['suffix']), is_dbnull(o['email']))   \n    \n    execute_batch(sql)\n    \n    return hash_id\n\ndef insert_affiliation(institution_hash, author_hash):\n    if institution_hash == None or author_hash == None:\n        return \n\n    sql = str.format(\"INSERT INTO import.Affiliation(institution_hash, author_hash) VALUES('{0}', '{1}')\", \\\n                     institution_hash, author_hash) \n    \n    execute_batch(sql)\n    \ndef insert_citation(author_hash, paper_id):\n    if paper_id == None:\n        return \n\n    sql = str.format(\"INSERT INTO import.Citation(author_hash, paper_id) VALUES('{0}', '{1}')\", author_hash, paper_id)\n    \n    execute_batch(sql)\n    \ndef insert_authored(paper_id, author_hash):\n    if paper_id == None or author_hash == None:\n        return\n\n    sql = str.format(\"INSERT INTO import.Authored(author_hash, paper_id) VALUES('{0}', '{1}')\", author_hash, paper_id)\n    \n    execute_batch(sql)\n\ndef insert_publishedby(journal_hash, paper_id):\n    if journal_hash == None or paper_id == None:\n        return\n        \n    sql = str.format(\"INSERT INTO import.PublishedBy(journal_hash, paper_id) VALUES('{0}', '{1}')\", \\\n                     journal_hash, paper_id)\n    \n    execute_batch(sql)\n    \ndef insert_paper(json_paper):\n    paper_id = json_paper['paper_id']\n    metadata = json_paper['metadata']\n    authors = metadata['authors']\n    bib_entries = json_paper['bib_entries']\n    title = clean_str(metadata['title'])\n    body_text = None\n    abstract = None\n    abstract_nodes = []\n    text_nodes = []\n\n    # Combine all sections in abstract array\n    for ab in json_paper['abstract']:\n        s = clean_str(ab['text'])\n        if s != None:\n            abstract_nodes.append(s)\n\n    if len(abstract_nodes) > 0:\n        abstract = \" \".join(abstract_nodes)\n\n    # Combine all sections in body_text array    \n    for section in json_paper['body_text']:\n        s = clean_str(section['text'])\n        if s != None:\n            text_nodes.append(s)\n\n    if len(text_nodes) > 0:\n        body_text = \" \".join(text_nodes)\n        \n    paper = {\n        'paper_id': paper_id,\n        'title': title,\n        'abstract': abstract,\n        'body_text': body_text\n    }\n    \n    obj = json.loads(json.dumps(paper), encoding='utf-8')\n    body = json.dumps(obj['body_text'], ensure_ascii=True)\n    abst = json.dumps(obj['abstract'], ensure_ascii=True)\n    body = body.replace(\"'\", \"\")\n    abst = abst.replace(\"'\", \"\")\n    \n    sql = str.format(\"INSERT INTO import.Paper(paper_id, title, abstract, body_text) VALUES({0}, {1}, {2}, {3})\", \\\n       is_dbnull(paper_id), is_dbnull(obj['title']), is_dbnull(abst), is_dbnull(body))\n    \n    execute_batch(sql)\n    batch_info['paper_count'] += 1\n    \n\n    for author in authors:\n        author_hash = insert_author(author)\n        insert_authored(paper_id, author_hash)\n\n        if 'affiliation' in author:\n            inst_hash = insert_institution(author['affiliation'])\n            insert_affiliation(inst_hash, author_hash)\n\n    for bibref in bib_entries:\n        bib = bib_entries[bibref]\n        journal_hash = insert_journal(bib['venue'], bib['issn'])\n        insert_publishedby(journal_hash, paper_id)\n\n        for author in bib['authors']:\n            author_hash = insert_author(author)\n            insert_citation(author_hash, paper_id)\n\n    return paper_id\n\n\ndef insert_sql():\n    json_papers = read_files(FOLDER)\n    i = 0\n    for path in json_papers:\n        #print(path)\n        json_paper = read_json(path)\n        insert_paper(json_paper)\n        i += 1\n        print(i)\n\n# Uncomment to run\n# insert_sql()","edd327aa":"import re\nimport json\nimport pandas as pd\nimport hashlib\n\ndef authors_to_list(paper_id:str, s:str):\n    \"\"\"\n    Parse the different list types of authors in CSV data.\n    \"\"\"\n    s = str(s)\n\n    def strip_quotes(s):\n        return trim(re.sub(r'\\\"', '', s, re.DOTALL))\n\n    def _to_dict(s):\n        if s == None:\n            return {} \n\n        auth = {\n            \"paper_id\": paper_id,\n            \"hash_id\": None,\n            \"first\": None,\n            \"last\": None,\n            \"middle\": None\n        }\n\n        if s.find(',') > -1:\n            names = list( map(strip_quotes, s.split(',')) )\n            first = re.sub(r'[^a-zA-Z\\s\\-]', '', names[1], re.DOTALL)\n            tmp = first.split(' ')\n            if len(tmp) > 1:\n                auth['middle'] = tmp.pop()\n                first = trim(' '.join(tmp))\n\n            auth['first'] = first\n            auth['last'] = names[0]\n\n        else:\n            auth['last'] = strip_quotes(s)\n\n        auth['hash_id'] = hash_author(auth)\n        return auth\n\n    if s == None or len(trim(s)) == 0:\n        return []\n\n    s = re.sub(r'\\',\\s\\'', ';', s, re.DOTALL) \n    s = re.sub(r'[^a-z0-9A-Z\\-,\\s\\;]', '', s, re.DOTALL)         \n    lst = list(map(_to_dict, s.split(';')))\n\n    return lst\n\n\ndef import_csv_metadata():\n    file_path = \"..\/data\/all_sources_metadata_2020-03-13_clean.csv\"\n    df = pd.read_csv(file_path)\n    authors = df[['sha', 'authors', 'has_full_text']].values.tolist()\n    \n    data = []\n\n    i = 0\n    for row in authors:\n        if to_bool(row[2]) and str(row[1]) != 'nan':\n            paper_id = str.format('na_paper_id_{0}', i) if str(row[0]) == 'nan' else row[0]\n            lst = authors_to_list(paper_id, row[1])\n            data.extend(lst)\n        \n        i += 1\n\n    \n    author_df = pd.DataFrame(columns=['paper_id', 'hash_id', 'first', 'last', 'middle'], data=data)\n    author_df.to_csv(\"..\/data\/authors.csv\")\n\n# Uncomment to run\n# import_csv_metadata()\n","004415f0":"# Import libraries\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\n%matplotlib inline\nimport pandas as pd\nimport math\nimport seaborn as sns","daaf32c7":"# Change data_dir to folder on your computer.\ndata_dir = \"..\/data\/\"\nfilepath = data_dir+\"paper_ranks.csv\"\nnum_clusters = 9\ntext_columns = ['paper_id'] # \nnumeric_columns = ['AuthorRank', 'JournalRank', 'Recency', 'CitedAuthorsRank', 'CitedInstitutionRank', \\\n                   'q01', 'q02', 'q03', 'q04', 'q05', 'q06', 'q07', 'q08', 'q09']\nmodel_data = None\nlogical_cores = 4 # how many logical CPU cores you have","21cb6a5f":"# Functions\ndef normalize(df):\n    \"\"\"\n    Normalize and Scale the numerical columns\n    \"\"\"\n    data = df.copy().values   \n    min_max_scaler = preprocessing.MinMaxScaler()\n    scaled_data = min_max_scaler.fit_transform(data)\n    return pd.DataFrame(columns=df.columns, data=scaled_data)\n\ndef plot_scree(percent_variance):\n    \"\"\"\n    Create Scree Plot of Principal Components Variance\n    \"\"\"  \n    # Visualize the variance explained for each observation by the principal components\n    cols = []\n    for i in range(len(percent_variance)):\n        cols.append('PC'+str(i+1))\n\n    sns.barplot(y=cols, x=percent_variance)\n    plt.title('PCA Scree Plot - larger explains more variance')\n    plt.show()","cc00069d":"# Import XDF Online Retail data set\ndf = pd.read_csv(filepath)\ndf = df.drop('q10', 1)\ndf = df.fillna(1)\ndf.head()","b5900615":"df.describe()","ee4d0d02":"model_df = df.copy().drop('paper_id', 1)\n# Normalize data\nmodel_df = normalize(model_df)\nmodel_df.head()","bb60edf5":"# Use visual diagnostic to estimate optimal number of clusters\nwcss = []\nfor i in range(1, 10):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(model_df)\n    wcss.append(kmeans.inertia_)\n    \nplt.plot(range(1, 10), wcss)\nplt.title('Optimal #Clusters')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","632c2e53":"num_clusters = 4","d370d130":"km = KMeans(n_clusters=num_clusters, \n            random_state=123, \n            verbose=True, \n            n_jobs=logical_cores, \n            algorithm='full')\n\nkmeans_data = km.fit_transform(model_df)\nclusters = km.predict(model_df)\nlabels = km.labels_","2b58512a":"# Create dataframe for our PC variables.\nk_columns = []\nfor i in range(len(kmeans_data[0])):\n    k_columns.append('PC'+str(i+1))\n    \nkmeans_df = pd.DataFrame(columns=k_columns, data=kmeans_data)\nkmeans_df.head()","e4128634":"#Plot the clusters obtained using k means\n# https:\/\/thatascience.com\/learn-machine-learning\/kmeans\/\nfig = plt.figure(1, figsize=(12, 12))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot Cluster Centroids\nscatter = ax.scatter(km.cluster_centers_[:, 0], # get 1st N of each row\n                     km.cluster_centers_[:, 1],\n                     km.cluster_centers_[:, 2],\n                     s = 250, \n                     marker='o', \n                     c='red', \n                     label='centroids')\n\n# Plot Cluster X, Y, Z points\nscatter = ax.scatter(kmeans_data[:, 0], \n                     kmeans_data[:, 1], \n                     kmeans_data[:, 2],\n                     c=clusters, \n                     s=20, \n                     cmap='winter')\n\nax.set_title('K-Means Clustering')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.legend()\nplt.show()\n","bb10cd7a":"paper_id = df['paper_id'].values\ndata = list( zip(paper_id, clusters) )\nlabeld_df = pd.DataFrame(columns=['paper_id', 'cluster'], data=data)\ndf['ClusterLabel'] = clusters\nlabeld_df.head()","61b518d7":"labeld_df.to_csv(data_dir+\"paper_kmeans_clusters.csv\")","c741f662":"df.head()","fe09e1cf":"## Import JSON files to SQL Server","1156d871":"# Import CSV and JSON Files to SQL Server","dc201f82":"## K-Means Clustering\nBy using unsupervised learning ML technique of K-Means clustering, provided us methods for creating predictors to increase accuracy of our model.\n\n![image.png](attachment:image.png)![](http:\/\/)\n\n![table6.PNG](attachment:table6.PNG)","4742f206":"# K-Means Clustering\n\nWe will use the unsupervised learning ML technique of K-Means clustering, where we seek to identity primary groups of papers where their cluster is unknown. K-Means also provides us a method for *feature engineering* where it's desirable to create more predictors to increase accuracy of our model.","c74f882e":"# Empowering Data Analysis through Visualization\n","b25556d0":"![image.png](attachment:image.png)","643b62d0":"GitHub repo: https:\/\/github.com\/jbonfardeci\/dl-covid19-kaggle","100d6611":"### Pros\n* Empowers users to identify key elements and prioritize them for better result sets\n* Provides a refinement approach by using cluster centers for modeling the data\n\n### Cons\n* The accuracy of the results is dependent on the quality of the intial dataset\n","b1fae512":"## Approach\n\nWith the amount of data available at our fingertips, cyphering though it all requires a significant amount time. At times this information is needed with the outmost importance to formulate a solution or complete a research.\n\nBy reviewing the original dataset, the team identified key elements that could assist in identifying the best possible results. Each of this elements have different level of weight that contribute to the relevance of the results. By identifying these metrics and their level of importance, data could be search with a weighted algorithm.","f68f5666":"## Data Visualization Tool\n\n![image.png](attachment:image.png)\n    \n#### https:\/\/public.tableau.com\/profile\/james.eselgroth3249#!\/vizhome\/Covid19_15870982837470\/Covid-19KaggleChallenge","c59d0919":"![image.png](attachment:image.png)"}}