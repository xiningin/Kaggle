{"cell_type":{"466de8e6":"code","3d6c2fa9":"code","cfd0de79":"code","63368fce":"code","97e51cc5":"code","54bcf4f7":"code","d151a692":"code","f69c774b":"code","9edcbf4b":"code","62c0bb4f":"code","b7434fcd":"code","d93a1618":"code","c9bbfab6":"code","8ae7fbc0":"code","7101270c":"code","9d987cfb":"code","5cf40491":"code","ded47f20":"code","680a3349":"code","d7da5c2a":"code","bf0d5ce2":"code","e72d2eaf":"code","e977cda3":"code","6a883a86":"code","7a7f22c2":"code","7ac65ecc":"code","4ff746da":"code","391ecdca":"code","242ae622":"code","53519483":"code","27afef95":"code","9f2fa376":"code","c5964d5e":"code","cabcda92":"code","8adda4be":"code","1d63c0bf":"code","fb8d8443":"code","95774ece":"code","070a6ad0":"code","0141a4b0":"code","18df7bbb":"code","f0a20b6a":"code","cb2b78a5":"code","9683a4f3":"code","b049dea3":"code","4350554e":"markdown","27370d48":"markdown","0f6e3d7e":"markdown","0101548e":"markdown","7b4cb8fe":"markdown","bf0a6b55":"markdown","44681377":"markdown","b248654c":"markdown","8f97813d":"markdown","420fa16b":"markdown","59693c65":"markdown","431039cd":"markdown","b7b6439b":"markdown","9d11f285":"markdown","8d671517":"markdown","fc2504ed":"markdown","e361d293":"markdown","9c543785":"markdown","c97236ce":"markdown","60abed3d":"markdown","aefd9e5b":"markdown","202c7afb":"markdown","66f42351":"markdown","95207cbf":"markdown","e56c6eb8":"markdown","948e3f4a":"markdown","d3eacd1a":"markdown","b693b000":"markdown","7681da33":"markdown","8af57ae2":"markdown","f57e270a":"markdown","a558ebc7":"markdown","99cc86f6":"markdown","226aefbb":"markdown"},"source":{"466de8e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d6c2fa9":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport optuna # optuna, hypter paramater optimisation, basiclally will test lots of different variations of XGB settings\nimport shap # SHAP (SHapley Additive exPlanations) explains the output of the machine learning model.\nimport pickle\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\nfrom pathlib import Path\n\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import accuracy_score\n\n","cfd0de79":"# Kaggle data links\n\np1g = pd.read_csv('..\/input\/solar-power-generation-data\/Plant_1_Generation_Data.csv')\n\np1w = pd.read_csv('..\/input\/solar-power-generation-data\/Plant_1_Weather_Sensor_Data.csv')\n\np2g = pd.read_csv('..\/input\/solar-power-generation-data\/Plant_2_Generation_Data.csv')\n\np2w = pd.read_csv('..\/input\/solar-power-generation-data\/Plant_2_Weather_Sensor_Data.csv')\n","63368fce":"# # local data links\n\n# p1g = pd.read_csv('\/Users\/cjpw\/Documents\/jupyter\/solar\/data\/Plant_1_Generation_Data.csv')\n# p1w = pd.read_csv('\/Users\/cjpw\/Documents\/jupyter\/solar\/data\/Plant_1_Weather_Sensor_Data.csv')\n# p2g = pd.read_csv('\/Users\/cjpw\/Documents\/jupyter\/solar\/data\/Plant_2_Generation_Data.csv')\n# p2w = pd.read_csv('\/Users\/cjpw\/Documents\/jupyter\/solar\/data\/Plant_2_Weather_Sensor_Data.csv')\n","97e51cc5":"dfs = [p1g, p1w, p2g,p2w]\n# getting the info of each df\nfor i in dfs:\n\n    i.info()\n    print('\\n')\n    print('\\n')","54bcf4f7":"p1g.columns= ['dt_time', 'plant_id', 'source_key', 'dc_power', 'ac_power', 'daily_yield', 'total_yield']\np1w.columns = ['dt_time', 'plant_id', 'source_key', 'amb_temp', 'mod_temp', 'irrad']\np2g.columns = ['dt_time', 'plant_id', 'source_key', 'dc_power', 'ac_power', 'daily_yield', 'total_yield']\np2w.columns = ['dt_time', 'plant_id', 'source_key', 'amb_temp', 'mod_temp', 'irrad']","d151a692":"# checking the format\nfor i in dfs:\n    print('start date ',i.dt_time.min(), 'end date ',i.dt_time.max())","f69c774b":"# fixing the date time\n\np1g['dt_time']= pd.to_datetime(p1g['dt_time'],format='%d-%m-%Y %H:%M')\np1w['dt_time']= pd.to_datetime(p1w['dt_time'],format='%Y-%m-%d %H:%M:%S')\np2g['dt_time']= pd.to_datetime(p2g['dt_time'],format='%Y-%m-%d %H:%M')\np2w['dt_time']= pd.to_datetime(p2w['dt_time'],format='%Y-%m-%d %H:%M:%S')","9edcbf4b":"# checking the source key column, it should relate to each inverters at each plant. Some inverters have more data which is strange\nfor i in dfs:\n    print (i.source_key.value_counts())\n    print('\\n')\n    print('\\n')","62c0bb4f":"for i in dfs:\n    i[\"source_key\"] =  i[\"source_key\"].astype('category')\n    i[\"simp_source\"] = i[\"source_key\"].cat.codes\n    \n#   lets also make the plant id more simple\np1g['plant_id_new'] = 1\np1w['plant_id_new'] = 1\np2g['plant_id_new'] = 2\np2w['plant_id_new'] = 2\n\n\n# Let's make a colum that shows us plant and solar array together\n\nfor i in dfs:\n    i['plant_id_new'] = i['plant_id_new'].astype('string')\n    i['simp_source'] = i['simp_source'].astype('string')\n    i['plant_array'] = i['plant_id_new'] + '\/' + i['simp_source']\n    i['plant_id_new'] = i['plant_id_new'].astype('category')\n    i['simp_source'] = i['simp_source'].astype('category')\n","b7434fcd":"for i in dfs:\n    print(i.dt_time.count())\n\n# Merging the weather data into the power data\np1g_merge = pd.merge(p1g, p1w, how='inner', on = 'dt_time')\np2g_merge = pd.merge(p2g, p2w, how='inner', on = 'dt_time')\n\n# Combining the merged data into one dataframe\ndf = pd.concat([p1g_merge,p2g_merge], axis = 0)","d93a1618":"# re organising columns and removing extra columns\ndf = df[['dt_time',\n        'plant_array_x',\n        'dc_power',\n        'ac_power',\n       'daily_yield',\n        'total_yield',\n        'amb_temp',\n        'mod_temp',\n       'irrad',\n        'simp_source_x',\n        'plant_id_new_x']]","c9bbfab6":"# Check for zero power produced data, e.g. at night\n\nfull = df.daily_yield.count() \nzeros = df.loc[df['daily_yield']==0].daily_yield.count()\n\nprint('num rows full and zeros', full, zeros)\nprint ('num rows after removed zeros: ', full - zeros)\nprint ('percentage zeros: ', zeros \/ full)\n\n# so 20% of our data isnt really useful if we are trying to \n# 1. identify how weather effects the generation\n# 2. identify poor performing equiptment\n\n# so, we should drop the zeros, we need to think about also dropping the days that contain zeros\n\n# we will drop the 0 power produced days, from the metadata i can see that the plan was shut down on those days\ndf.reset_index(inplace = True)\n\n# im not dropping the zero's for now\n# df = df.loc[df['daily_yield']!=0].copy()","8ae7fbc0":"p2g.plot(x = 'dt_time', y = 'daily_yield')\np1g.plot(x = 'dt_time', y = 'daily_yield')","7101270c":"# i should try running these withoutthe night time data\n# adding a column that shows the date without the time\ndf['date'] = pd.to_datetime(df['dt_time']).dt.date","9d987cfb":"def plot_scatter(df, feat_1, feat_2):\n    plt.figure(figsize=(9,9))\n    sns.scatterplot(data=df, x=feat_1, y = feat_2)\n    #Title = f'{feat_1, feat_2}'\n    #plt.title(Title, fontsize=15)\n    plt.show()","5cf40491":"df1 = df.loc[df['ac_power'] > 0]\n\nplot_scatter(df1, 'amb_temp', 'ac_power')\nplot_scatter(df1, 'amb_temp', 'dc_power')\n\nplot_scatter(df1, 'mod_temp', 'ac_power')\nplot_scatter(df1, 'mod_temp', 'dc_power')\n\nplot_scatter(df1, 'irrad', 'ac_power')\nplot_scatter(df1, 'irrad', 'dc_power')\n","ded47f20":"def fancy_plot(df, method = 'pearson', annot = True, **kwargs):\n    sns.clustermap(df.corr(method), vmin = -1.0, vmax = 1.0, cmap = 'icefire', method = 'complete', annot = annot, **kwargs)\n\nfancy_plot(df)","680a3349":"df.simp_source_x = df.simp_source_x.astype('category')\ndf.plant_id_new_x = df.plant_id_new_x.astype('category')","d7da5c2a":"df.corr()\n# strong corrleations, there isnt really much need for a ML model but we can make one just for fun","bf0d5ce2":"# lets try correlation when we only consider hours when there is some generation (probably means daytime)\n\ndf1 = df.loc[df['dc_power']>0].copy()\ndf1 = df.loc[df['ac_power']>0].copy()\ndf1 = df1[['ac_power', 'dc_power', 'amb_temp', 'mod_temp', 'irrad', 'simp_source_x', 'plant_id_new_x']].copy()\nprint(df1.corr())\nfancy_plot(df1)\n\n# correlation of items with just > 0 power production values\n# i dont understand why the correlation with dc_power is so different..i probably need to learn more about electricity\n","e72d2eaf":"# function to plot predicted vs actual\ndef plot_pred_actual(y_pred, y_test, scale):\n    \n    predicted_value = y_pred\n    true_value = y_test\n    plt.figure(figsize=(10,10))\n    plt.scatter(true_value, predicted_value, c='crimson')\n    plt.yscale(scale)\n    plt.xscale(scale)\n    p1 = max(max(predicted_value), max(true_value))\n    p2 = min(min(predicted_value), min(true_value))\n    plt.plot([p1, p2], [p1, p2], 'b-')\n\n    plt.title = f'{y_pred}'\n    \n\n    plt.xlabel('True Values', fontsize=15)\n    plt.ylabel('Predictions', fontsize=15)\n    plt.axis('equal')\n    plt.show()","e977cda3":"# function to split data into train test and run a basic linear regression model\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n\ndef lin_reg(X,y,test_size,scale):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n    reg = LinearRegression().fit(X, y)\n    pred = reg.predict(X_test)\n    print ('rmse: ',mean_squared_error(y_test, pred, squared = False))\n    print('r2: ', r2_score(y_test, pred))\n    \n    plot_pred_actual(y_test, pred, scale)\n    return pred\n    ","6a883a86":"# preparing the data to put into the model\n\nX = df1[['amb_temp', 'mod_temp', 'irrad',\n       'simp_source_x', 'plant_id_new_x']].copy().to_numpy()\n\ny_ac = df1['ac_power'].copy().to_numpy()\ny_dc = df1['dc_power'].copy().to_numpy()\n\npred_ac = lin_reg(X,y_ac,20,'linear')\n\n","7a7f22c2":"from sklearn.preprocessing import MinMaxScaler\n\nX = df1[['amb_temp', 'mod_temp', 'irrad',\n       'simp_source_x', 'plant_id_new_x', 'ac_power']].copy().to_numpy()\n\nscaler = MinMaxScaler()\nscaler.fit(X)\nX = scaler.transform(X)\n\ny_ac = X[:,5]\nX = X[:,0:5]\n\n\npred_ac2 = lin_reg(X,y_ac,20,'linear')\n\n","7ac65ecc":"# so we now have a daily yield per day per array\n\nbla = df.pivot_table(index = ('date', 'plant_id_new_x'), values = 'daily_yield',aggfunc = sum)\nbla","4ff746da":"# Default paramaters for XHBoost, we will use Optuna to optimise these parameters later\n# remember when putting a dictionary in a function you need to proceed it with **\n\nxgb_params = dict(\n    max_depth=3,                           # maximum depth of each tree - try 2 to 10\n    learning_rate=000.1,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=200,                     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=1,                    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=1,   # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=1,          # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0,        # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=1,         # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,                   # set > 1 for boosted random forests\n)","391ecdca":"# function to do some boosting\n\ndef score_please(X, y, model=XGBRegressor(**xgb_params)):\n    \n    #changing the categorical labels into numbers for the model\n    for name in X.select_dtypes(['category']):\n        X[name] = X[name].cat.codes\n        \n    #Changing y to a log, we will have to change it back before we submit exp\n    y_log = np.log(y)\n    \n    score = cross_val_score(model,X, y_log, cv=5, scoring = 'neg_mean_squared_error')\n    score = -1 *score.mean()\n    score = np.sqrt(score)\n    return(score)\n    print(score)","242ae622":"# getting the score!\n\ny_ac = df1['ac_power'].copy()\ny_dc = df1['dc_power'].copy()\nX = df1[['amb_temp', 'mod_temp', 'irrad', 'simp_source_x', 'plant_id_new_x']].copy()\n\nscore_please(X,y_dc, model=XGBRegressor(**xgb_params))","53519483":"y_ac = df1['ac_power'].copy()\ny_dc = df1['dc_power'].copy()\nX = df1[['amb_temp', 'mod_temp', 'irrad', 'simp_source_x', 'plant_id_new_x']].copy()\nfor name in X.select_dtypes(['category']):\n        X[name] = X[name].cat.codes\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_ac, test_size=0.5, random_state=8)\n\nxgb = XGBRegressor(**xgb_params)\nxgb.fit(X_train, y_train)","27afef95":"y_ac_pred = xgb.predict(X_test)\n\nprint ('rmse: ',mean_squared_error(y_test, y_ac_pred, squared = False))\nprint('r2: ', r2_score(y_test, y_ac_pred))\n\nplot_pred_actual(y_test, y_ac_pred, 'linear')","9f2fa376":"# # trial parameters telling optuna where to test, this will take a long time\n\n# y_ac = df1['ac_power'].copy()\n# y_dc = df1['dc_power'].copy()\n# X = df1[['amb_temp', 'mod_temp', 'irrad', 'simp_source_x']].copy()\n\n# def objective(trial):\n#     xgb_params = dict(\n#         max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n#         learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n#         n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n#         min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n#         colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n#         subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n#         reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n#         reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n#     )\n#     xgb = XGBRegressor(**xgb_params)\n    \n#     return score_please(X_train, y_train, xgb)\n\n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials=50)\n# xgb_params = study.best_params","c5964d5e":"# parameters from the best performing test\n\nxgb_params = dict(\n    max_depth=8,                        \n    learning_rate=0.007370619279795246,\n    n_estimators=2222,\n    min_child_weight=7,\n    colsample_bytree=0.894850599575649,\n    subsample=0.8510498418045354,\n    reg_alpha=0.10371479505123118,\n    reg_lambda=0.10371479505123118,\n    num_parallel_tree=1,\n)","cabcda92":"# y_ac = df1['ac_power'].copy()\n# y_dc = df1['dc_power'].copy()\n# X = df1[['amb_temp', 'mod_temp', 'irrad', 'simp_source_x', 'plant_id_new_x']].copy()\n\n# score_please(X, y_ac, model=XGBRegressor(**xgb_params))","8adda4be":"y_ac = df1['ac_power'].copy()\ny_dc = df1['dc_power'].copy()\nX = df1[['amb_temp', 'mod_temp', 'irrad', 'simp_source_x', 'plant_id_new_x']].copy()\nfor name in X.select_dtypes(['category']):\n        X[name] = X[name].cat.codes\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_ac, test_size=0.5, random_state=8)\n\nxgb = XGBRegressor(**xgb_params)\nxgb.fit(X_train, y_train)","1d63c0bf":"y_ac_pred = xgb.predict(X_test)\n\nprint ('rmse: ',mean_squared_error(y_test, y_ac_pred, squared = False))\nprint('r2: ', r2_score(y_test, y_ac_pred))\n\nplot_pred_actual(y_test, y_ac_pred, 'linear')","fb8d8443":"# sticking the two dataframes together and starting from scratch\n\np1g_merge = pd.merge(p1g, p1w, how='inner', on = 'dt_time')\np2g_merge = pd.merge(p2g, p2w, how='inner', on = 'dt_time')\ndf = pd.concat([p1g_merge,p2g_merge])\n# re organising columns and removing extra columns\ndf = df[['dt_time',\n        'plant_array_x',\n        'dc_power',\n        'ac_power',\n       'daily_yield',\n        'total_yield',\n        'amb_temp',\n        'mod_temp',\n       'irrad',\n        'simp_source_x',\n        'plant_id_new_x']]\ndf['date'] = pd.to_datetime(df['dt_time']).dt.date\ndf.columns = ['dt', 'plant_array', 'dc_power', 'ac_power', 'daily_yield',\n       'total_yield', 'amb_temp', 'mod_temp', 'irrad', 'array',\n       'plant', 'date']","95774ece":"avg_period_power = df.pivot_table(index = ['plant', 'array'], columns = 'date', values = 'daily_yield', aggfunc = max)\navg_period_power.reset_index(inplace = True)\n\n","070a6ad0":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\navg_period_power['average_daily_power_yield'] = avg_period_power.iloc[:,-34:].mean(axis = 1).copy()\n\navg_period_power","0141a4b0":"# checking the average of all the arrays\navg_period_power.average_daily_power_yield.mean()\n","18df7bbb":"def performance_to_mean (df ,plant, Title):\n    \n    array_monthly_average = df.loc[df['plant']==plant].copy()\n    array_monthly_average = array_monthly_average[['plant','array', 'average_daily_power_yield']]\n    avg = array_monthly_average['average_daily_power_yield'].mean()\n    array_monthly_average['performance_%'] = ((array_monthly_average['average_daily_power_yield'] - avg) \/ avg) *100\n    array_monthly_average.sort_values(by = 'performance_%', inplace=True)\n    \n    array_monthly_average.plot(x = 'array', y = 'performance_%', kind = 'bar', figsize = (20,10), title = Title, ylabel = '%' )  ","f0a20b6a":"performance_to_mean(avg_period_power, '1', 'Plant 1 AC production Compared to Plant average')\nperformance_to_mean(avg_period_power, '2', 'Plant 2 AC production Compared to Plant average')","cb2b78a5":"df.head()\ndf['irrad_yield'] = df['daily_yield'] \/ df['irrad']\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf['irrad_yield'] = df['irrad_yield'].fillna(0)\ndf.irrad_yield.value_counts()\n\nirrad_df = df.pivot_table(index = ['plant', 'array'], values = ['irrad_yield'], aggfunc = np.mean)\nirrad_df.reset_index(inplace = True)\nirrad_df = pd.DataFrame(data = irrad_df)\n","9683a4f3":"def performance_to_mean_irrad (df,col,plant, Title):\n    df[col] = df[col].round(2).copy()\n    df.sort_values(col, inplace = True)\n    df1 = df.loc[df['plant'] == plant].copy()\n    avg = df1[col].mean()\n    df1['performance_%'] = ((df1[col] - avg) \/ avg) *100\n    return df1.plot(x = 'array', y = 'performance_%', kind = 'bar', figsize = (20,10), title = Title)\n    \n    \n    ","b049dea3":"performance_to_mean_irrad(irrad_df, 'irrad_yield', '1', 'Plant 1 Irradiance to AC Power Production Compared to Plant Average')\nperformance_to_mean_irrad(irrad_df, 'irrad_yield', '2', 'Plant 1 Irradiance to AC Power Production Compared to Plant Average')","4350554e":"# Libraries","27370d48":"Same as above but with min max scaling","0f6e3d7e":"# Questions to answer:\n\n- Can we predict the power generation based on the weather\n- can we identify faulty or suboptimally performing equipment?\n- Can we identify the need for panel cleaning\/maintenance?\n","0101548e":"Making the column names more friendly","7b4cb8fe":"Why have we got negative predictions?","bf0a6b55":"# Let's take a look at the data\nPlotting Dependent vs independent\n","44681377":"function to make performance chart","b248654c":"Ploting results of LR model","8f97813d":"Plotting model predictions","420fa16b":"above we can see that 2\/4, 2\/8, 2\/14, 2\/20 all generated no power from 21 - 28 May","59693c65":"# Importing Data","431039cd":"## XGB Model passing in the x and y values plus the model and model parameters\n","b7b6439b":"Lets seperate the plants, work out the mean daily yield per plant and check how each array performs against the mean","9d11f285":"i think i should double check that the units for each plant look the same","8d671517":"Simplifying the data \/ making it easier to use","fc2504ed":"# Checking if some solar pannels are not working\nCan we identify broken pannels, if any?\nFor arrays that are not working, their daily_yield would be lower than the average of the other arrays.\nplan:\n\n1. did any arrays produce zero output for any individual day?\n3. how does each array performa compared to the plant average\n","e361d293":"# Merging & combining the datasets\n\n\nWe merge on datetime, weather data is not solar arrray specific so no merging there\n","9c543785":"## Hyper parameter tuning","c97236ce":"# Inital data exploration and fixing small issues","60abed3d":"Fixing the data time format","aefd9e5b":"Running a regual linear regression model","202c7afb":"# Using ML predict power produced by weather\nThere isnt really much point to this as we can already see a .96 corelation between irradiation and AC power","66f42351":"Checking the source key column (one source for each array)","95207cbf":"# Lets look at the correlation of items","e56c6eb8":"Trial 49 finished with value: 0.16540890969688846 and parameters: {'max_depth': 8, 'learning_rate': 0.007370619279795246, 'n_estimators': 2222, 'min_child_weight': 7, 'colsample_bytree': 0.894850599575649, 'subsample': 0.8510498418045354, 'reg_alpha': 0.10371479505123118, 'reg_lambda': 0.10371479505123118}. Best is trial 48 with value: 0.1650462969089919.","948e3f4a":"Getting the model to make predictions","d3eacd1a":"Function to test the model with parameters","b693b000":"## checking which arrays had down days when they produced zero output","7681da33":"default parameters","8af57ae2":"Getting performance score benchmark","f57e270a":"Making predictions with the new parameters and plotting the results","a558ebc7":"##\u00a0Preparing the data","99cc86f6":"Average perofmance of plant 1 arrays and plant 2 arrays","226aefbb":"# Can we see which arrays need cleaning?\n\nLets look at array performance compared to the irridation it gets, if an array is getting high irrad but producing low power this may infer that the array is dirty."}}