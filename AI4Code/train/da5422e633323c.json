{"cell_type":{"f22fcd22":"code","10dbe821":"code","f9408ed9":"code","9ab140c0":"code","8c670727":"code","44025f05":"code","db45ec1a":"code","3b41e8e9":"code","84c5d651":"code","8d033946":"code","545d3569":"code","d33fd040":"code","b04c3585":"code","ea9c0633":"markdown","ab32ba1d":"markdown","a4e2b464":"markdown","ed827628":"markdown","1109f215":"markdown"},"source":{"f22fcd22":"# This Python 3 environment comes with many helpful analytics libraries installed  lgebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","10dbe821":"import random\nimport math\nfrom statistics import mean as rerata\nimport numpy as np","f9408ed9":"############################################################################################################\n\n\"\"\"\nREADME:\n\nkode decision tree ini diilhami dari program decision tree serta kuliah yang diberikan oleh Josh Gordon melalui youtube\nchannel google developer dan github dengan username random-forests\n\nlink kuliah :\nhttps:\/\/www.youtube.com\/watch?v=LDRbO9a6XPU&t=323s\n\nlink GitHub :\nhttps:\/\/github.com\/random-forests\/tutorials\/blob\/master\/decision_tree.ipynb\n\ndengan perubahan perubahan di antaranya tapi bukan sebatas :\n\n- pembuatan dokumentasi\n- mengubah kode sehingga dapat mengakomodasi dataframe pandas\n- mengubah kode untuk memenuhi kebutuhan dataset yang dipenuhi oleh fitur kategori\n- mengembangkan algoritma sehingga mampu menerima max_depth, dan kolom kelas untuk \n- perubahan lain yang mungkin belum tercantum\n\nalgoritma dari decision tree sendiri ialah [1][2][3] :\n\n1. buat root\n2. cari kondisi yang paling tinggi info_gainnya menggunakan gini diversity index\n3. buat cabang berdasarkan kondisi tersebut\n4. jika tercapai max_depth atau tidak terdapat informasi baru yang bisa didapat dari kondisi, hentikan membuat branch\nbaru\n\n\"\"\"\n\n#Decision tree yang digunakan di bawah merupakan decision tree dengan algoritma CART, CART dipilih \n#karena kecepatan yang konsisten dalam melakukan training dan test [1]\n\ndef elemen_unik(dataframe, fitur):\n    #mengembalikan elemen unik dalam suatu fitur\n    return set(dataframe[fitur])\n\ndef hitung_kelas(dataframe, kolomkelas = \"class\"):\n    #mengembalikan dictionary fitur class dari dataframe dengan value jumlah\n    return dict(dataframe[kolomkelas].value_counts())\n\n#node daun untuk mereturn dictionary dari kelas\nclass daun:\n    def __init__(self, dataframe, kolomkelas):\n        self.klasifikasi = hitung_kelas(dataframe,kolomkelas)\n\n#node menyimpan kondisi, dataframe cabang benar, dan dataframe cabang salah\nclass node_pilihan:\n    def __init__(self,\n                 kondisi,\n                 cabang_benar,\n                 cabang_salah):\n        self.kondisi = kondisi\n        self.cabang_benar = cabang_benar\n        self.cabang_salah = cabang_salah\n\nclass kondisi:\n    \n    #inisialisasi class\n    def __init__(self, fitur, pembanding):\n        self.fitur = fitur\n        self.pembanding = pembanding\n    \n    #menyimpan kondisi\n    def banding(self, data):\n        return data[self.fitur] == self.pembanding\n    \n    #digunakan untuk mencetak kondisi\n    def __repr__(self):\n        condition = \"==\"\n        return \"%s %s %s?\" % (self.fitur, condition, self.pembanding)\n\n#membagi dataframe berdasarkan kondisi\ndef bagidataframe(dataframe, kond):\n    \n    data_benar = dataframe[dataframe[kond.fitur] == kond.pembanding]\n    data_salah = dataframe[dataframe[kond.fitur] != kond.pembanding]\n    \n    return data_benar, data_salah\n\n#ginidx digunakan untuk menemukan gini impurity (i)= 1-zigma(p(kelas)^2) sumber : [2][3]\ndef giniimpurity(dataframe, kolomkelas):\n    kamuskelas = hitung_kelas(dataframe, kolomkelas)\n    impurity = 1\n    \n    for kelas in kamuskelas:\n        \n        prob_kelas = kamuskelas[kelas] \/ float(len(dataframe))\n        impurity -= prob_kelas**2\n        \n    return impurity\n\n#mencari information gain (delta i = impurity prior - prob1*impurity1 - prob2*impurity2)sumber: [2][3]\ndef info_gain(cabang_benar, cabang_salah, current_uncertainty, kolomkelas):\n    \n    prior_prob = float(len(cabang_benar)) \/ (len(cabang_benar) + len(cabang_salah))\n    info =\\\n    current_uncertainty - (prior_prob * giniimpurity(cabang_benar, kolomkelas)) -\\\n    (1 - prior_prob) * giniimpurity(cabang_salah,kolomkelas)\n\n    return info\n\n#metode ini digunakan untuk mencari kondisi terbaik serta \ndef cabang_terbaik(dataframe, kolomkelas):\n    kondisi_terbaik = None\n    gain_terbaik = 0\n    ketidakpastian = giniimpurity(dataframe, kolomkelas)\n    \n    #mengiterasi tiap fitur dalam dataclass\n    for fitur in dataframe:\n        \n        #skip fitur class\n        if fitur==\"class\":\n            continue\n        \n        #mengambil elemen unik dalam suatu fitur    \n        kumpulan_tes = elemen_unik(dataframe,fitur)\n        \n        for tes in kumpulan_tes:\n            \n            #mengisi kondisi dan membagi dataframe berdasar kondisi\n            Kondisi = kondisi(fitur, tes)\n            kumpulan_benar,kumpulan_salah = bagidataframe(dataframe, Kondisi)\n            \n            #jika kondisi tidak membagi dataset continue\n            if len(kumpulan_benar) == 0 or len(kumpulan_salah) == 0:\n                continue\n            \n            #mencari information gain untuk kondisi terbaik\n            gain = info_gain(kumpulan_benar, kumpulan_salah, ketidakpastian, kolomkelas)\n            \n            #menyimpan information gain terbaik\n            if gain > gain_terbaik : gain_terbaik, kondisi_terbaik = gain, Kondisi\n                \n    return gain_terbaik, kondisi_terbaik\n\n        \ndef bangun_pohon(dataframe, max_tinggi, kolomkelas):\n    #init gain\/prior impurity dan kondisi\n    gain, kondisi = cabang_terbaik(dataframe, kolomkelas)\n    \n    #jika tidak didapat info atau mencapai max tinggi\n    \n    if gain == 0 or max_tinggi==0:\n        return daun(dataframe, kolomkelas)\n    \n    #mengurangi max tinggi tiap kali bangun pohon dipanggil\n    max_tinggi -= 1\n    \n    #membagi data frame\n    cabang_benar, cabang_salah = bagidataframe(dataframe, kondisi)\n    \n    #perintah untuk  meneruskan pembangunan tree\n    cabang_benar = bangun_pohon(cabang_benar, max_tinggi, kolomkelas)\n    cabang_salah = bangun_pohon(cabang_salah, max_tinggi, kolomkelas)\n    \n    #return kondisi dan node benar, salah\n    return node_pilihan(kondisi, cabang_benar, cabang_salah)\n\n###############################################################################################################\n\n#digunakan untuk mengklasifikasi data test\ndef classify(data, node):\n     \n    #jika node yang dicek adalah daun\n    if isinstance(node, daun):\n        return node.klasifikasi\n    \n    #jika data yang di banding dengan kondisi dalam node maka data masukkan klasify ke node yang cabangnya benar\n    if node.kondisi.banding(data):\n        return classify(data, node.cabang_benar)\n    \n    #jika if salah\n    else:\n        return classify(data, node.cabang_salah)\n    \n##############################################################################################################\n\n\n#mencetak tree hasil training\ndef print_tree(node, jeda=\"\"):\n    \n    #jika  node yang dipilih merupakan daun print hasil prediksi\n    if isinstance(node, daun):\n        print (jeda + \"Prediksi\", node.klasifikasi)\n        return\n    \n    print (jeda + str(node.kondisi))\n\n    print (jeda + '--> True:')\n    print_tree(node.cabang_benar, jeda + \"  \")\n\n    print (jeda + '--> False:')\n    print_tree(node.cabang_salah, jeda + \"  \")\n    \n#############################################################################################################","9ab140c0":"\"\"\"\nBerikut dalah algoritma Random Forest (ensemble classifier) dengan bagging pada bagian fitur dataframe\nserta sampling berupa N data dengan replacement (Briemann) serta mencoba pohon sehingga bisa sedalam mungkin[4]\n\nRandom forest sendiri dipilih karena jumlah fitur dan data dari data yang cukup banyak, sera tipe data yang kategorikal\nseluruhnya sangatlah cocok untuk digunakan oleh Random Forest dibandingkan beberapa data lain misal\nADAboost, Fuzzy C-means, dan DBScan\n\ntidak dipilihnya\n\n- Fuzzy C-means : memakan waktu komputasi yang besar, tidak bisa handle missing value [5]\n\n- DBScan        : kurang baik menghadapi data berdimensi tinggi dan densitas yang bervariasi \n                  serta random forest lebih kuat dalam pengerjaan data category [6]\n                  \n- ADAboost      : Algoritma adaboost yang melakukan boosting terhadap tiap feature serta memberikan optimisasi\n                  bahkan terhadap data noise sekalipun dirasa kurang dibutuhkan [7][8][9]\n\n- Ripper        : Random Forest lebih robust terhadap overfit dan jumlah data yang cukup besar 8000++ memberi\n                  Random Forest semakin kecil kemungkinan overfit\n\nMasih banyak hipotesis di atas masih membutuhkan pengujian lebih lanjut, alasan utama Random Forest dipilih karena \nmemilki reputasi yang sangat baik sebagai klasifier out of the box.\n\n\"\"\"\n\nclass CARTRF:\n\n    def __init__ (self, dataframe, pohon = 3, tinggi_pohon = 5, kolomkelas = \"class\"):\n        \n        kumpulan_fitur = list(dataframe.columns)\n        #menyimpan kumpulan fitur namun menghapus kolom target\n        kumpulan_fitur.remove(kolomkelas)\n        self.trees = []\n        \n        #training pohon dengan sample with replacement\n        for index in range(pohon):\n            \n            idxfitur = kumpulan_fitur.index(random.choice(kumpulan_fitur))\n            \n            #menghapus satu fitur secara acak\n            train = dataframe.drop(kumpulan_fitur.pop(idxfitur), axis = 1)\n            \n            #membuat sample with replacement\n            sample = train.sample(n = len(train), replace = True)\n            \n            #menggabung tree yang sudah jadi ke dalam list trees\n            self.trees.append(bangun_pohon(sample,tinggi_pohon,kolomkelas))\n    \n    #klasifikasi\n    def klasif(self, data):\n        predik = []\n        \n        #mengiterasi semua pohon di Random Forest trees\n        for tree in self.trees:\n            #classify data ke dalam tree\n            hasil = classify(data ,tree)\n            \n            #mengambil key dengan value tertinggi\n            hasil = max(hasil, key=hasil.get)\n            \n            #list kunci misal [\"e\", \"e\", \"p\"]\n            predik.append(hasil)\n            \n            #return prediksi yang anggotanya paling sering keluar [\"e\", \"e\", \"p\"] == \"e\"\n        return max(set(predik), key=predik.count)","8c670727":"def cross_val(dataframe, metode, n = 5, classname = \"class\"):\n    \n    #init variabel yang dibutuhkan untuk membagi data menjadi n bagian\n    panjang_data = len(dataframe)\n    bantu_kelompok = math.floor(panjang_data\/n)\n    index = 0\n    data = []\n    \n    for i in range(n):\n        #jika mencapai i akhir maka data[i] = data ke index sampai data terakhir\n        if i == n-1:\n            data.append(dataframe[index:(panjang_data)])\n            \n        #selain itu data ke i adalah data ke index ditambah bantu_kelompok\n        else:\n            data.append(dataframe[index : (index+bantu_kelompok)])\n        \n        #tambah index    \n        index += bantu_kelompok\n    \n    #buat list untuk menyimpan akurasi tiap iterasi untuk diprint\n    akurasi = []\n    \n    #test dan train dilakukan n kali\n    for i in range(n):\n        \n        #buat train dan tes\n        train = None\n        tes = data[i]\n        \n        #concat data train\n        for iterasi in range(n):\n            #jika iterasi sama dengan nilai iterasi sebelumnya, maka tidak perlu di concat karena menjadi tes\n            if iterasi != i:  \n                train = pd.concat([train, data[i]], join = \"inner\")\n        \n        #membuat classifier dan variabel prediksi\n        classifier = metode(train)\n        prediksi = []\n        kelas_asli = tes[classname]\n        \n        #prediksi\n        for ID, coba in tes.iterrows():\n            prediksi.append(classifier.klasif(coba))\n            \n        jumlah_benar = (np.array(prediksi) == np.array(tes[classname])).sum()\n        akurasi.append((jumlah_benar\/len(tes))*100)\n        \n    return(akurasi)","44025f05":"dataframe = pd.read_csv(\"..\/input\/mushrooms.csv\")","db45ec1a":"dataframe.describe(include=\"O\")","3b41e8e9":"dataframe.head()","84c5d651":"(dataframe==\"?\").sum()","8d033946":"#drop Stalk root karena missing value yang besar, dan veil-type karena hanya ada 1 unique value\n\nprint(2480\/len(dataframe))\n\ndataframe  = dataframe.drop([\"stalk-root\", \"veil-type\"], axis = 1)","545d3569":"#perbedaan kelas tidak terlalu besar\n\nprint(dataframe.groupby(\"class\").size())","d33fd040":"#menggabungkan fitur yang memliki awalan sama untuk mereduksi fitur\n\ncap = dataframe[\"cap-shape\"]+dataframe[\"cap-surface\"]+dataframe[\"cap-color\"]\ngill = dataframe[\"gill-attachment\"]+dataframe[\"gill-spacing\"]+dataframe[\"gill-size\"]+dataframe[\"gill-color\"]\nstalk = dataframe[\"stalk-shape\"]+dataframe[\"stalk-surface-above-ring\"]+dataframe[\"stalk-surface-below-ring\"]+dataframe[\"stalk-color-above-ring\"]+dataframe[\"stalk-color-below-ring\"]\nring = dataframe[\"ring-number\"]+dataframe[\"ring-type\"]\n\n\n#drop fitur yang telah di gabung\nfor fitur in dataframe:\n    if \"gill\" in fitur or \"stalk\" in fitur or \"ring\" in fitur or \"cap\" in fitur:\n        dataframe = dataframe.drop(fitur, axis =1)\n        \n#memasukkan fitur yang sudah digabung ke dalam dataframe\n\ndataframe[\"gill\"] = gill\ndataframe[\"cap\"] = cap\ndataframe[\"stalk\"] = stalk\ndataframe[\"ring\"] = ring","b04c3585":"%%time\n\nrerata(cross_val(dataframe, CARTRF))","ea9c0633":"Windows 10, RAM 4 GB, intel core i5 6400u\n\nEvaluasi dengan Pre-process dasar = drop veil type dan max depth = 5 pohon = 3:\n\n1. tanpa pre process tanpa sampling replacement n = 5 [(27.6 second, 99.97%), (26.2 second, 99.97%), (30.7 second, 99.97%)]\n2. pre process tanpa sampling replacement n = 5 [(34.5 second, 99.93%), (34.3 second, 100%), (32.1 second, 100%)]\n3. tanpa pre process sampling replacement n = 5 [(27.5 second, 99.97%), (28.1 second, 99.96%), (29 second, 99.97%)]\n4. pre process sampling replacement n = 5 [(36.1 second, ~100%), (31.8 second, ~100%), (35.7 second, ~100%)]","ab32ba1d":"# Tolong gunakan apapun yang anda dapat dari NoteBook ini sebebas-bebasnya, dengan mencantumkan penulis","a4e2b464":"Sumber rujukan :\n\n1. Kumar, Sunil, 2016.  \"A Survey on Decision Tree Algorithms of Classification on Data Mining\"\n2. Leszek Rutkowski, Maciej Jaworski, Lena Pietruczuk, Piotr Duda, 2018. \"The CART Decision Tree for Mining Data Streams\"\n3. Breiman, L., Friedman, J.H., Olshen, R., and Stone, C.J., 1984. Classification and Regression, TreeWadsworth & Brooks\/Cole Advanced Books & Software, Pacific California. \n4. Eesha Goel, Er. Abhilasha, 2017. \"Random Forest: A Review\"\n5. R.Suganya, R.Shanthi, 2012. \"Fuzzy C- Means Algorithm-  A Review\"\n6. K. Nafees Ahmed, T. Abdul Razak, 2016. \"An Overview of Various Improvements of DBSCAN Algorithm in Clustering Spatial Databases\"\n7. Abraham J.Wyner, Matthew Olson, Justin Bleich, 2017. \"Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers\"\n8. Artur Ferreira, M\u00b4ario Figueiredo, ----.\"Boosting Algorithms: A Review of Methods, Theory, and Applications\"\n9. Robert E. Schapire, ----. \"Explaining ADABoost\"\n10. Wesam S. Bhaya, 2017. \"Review of Data Preprocessing Techniques in Data Mining\"\n11. Ms. Shweta Srivastava, Ms. Nikita Joshi, Ms. Madhvi Gaur ,2013 . \"A Review Paper on Feature Selection Methodologies and Their Applications\"\n12. Isabelle Guyon, Andre Elisseeff , 2013. \"An Introduction to Variable and Feature Selection\"\n13. Jianyu Miao, Lingfeng Niu, 2016. \"A Survey on Feature Selection\"","ed827628":"\"\"\"\nDalam melakukan pre-process di atas adalah menggunakan metode seleksi fitur manual dengan kejelasan:\n1. Veil-Type hanya memiliki 1 unique value, sehingga tidak bisa digunakan sebagai fitur seleksi\n2. Penggabungan beberapa fitur sehingga terbentuk kumpulan pola bentuk tiap bagian jamur\n[10][11][12][13]\n\"\"\"","1109f215":"With love and passion :\n\nContact Info:\n\n- LinkedIn : https:\/\/www.linkedin.com\/in\/fais-alqorni-9a7642172\/\n- Kaggle   : https:\/\/www.kaggle.com\/paperbagz\/\n- e-Mail   : faisqorni12@gmail.com\n\nFais Alqorni \nMalang, 24\/04\/2019"}}