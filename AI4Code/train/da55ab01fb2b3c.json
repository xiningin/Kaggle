{"cell_type":{"7c146752":"code","9ba4773b":"code","05ffb4f8":"code","2c0aa199":"code","fe0e3f65":"code","0bbabc42":"code","763340e2":"code","30266ac8":"code","9e5ff81a":"code","31148e9f":"code","b1d69195":"code","760f345d":"code","b16ae69c":"code","0d138722":"code","f4b8d360":"code","22a4d5f5":"code","9c0fdd9e":"code","15adc676":"code","68ec73d4":"code","dd697f8d":"code","edc3983a":"code","79d7b614":"code","3a00852e":"code","f8440948":"code","5b471df3":"code","d73fbd0b":"code","2b60293a":"code","bb9c643a":"code","95b94719":"code","0cf9ddce":"code","2237f41a":"code","464faf7c":"code","c969006a":"code","6ebfb588":"code","c8102f42":"code","77f8ced2":"code","c575f781":"code","2040ef7a":"code","5e464c0c":"code","4800a5df":"markdown","06a1da36":"markdown","a3a395b2":"markdown","17ff6623":"markdown","ba026187":"markdown","47202cad":"markdown","52cdb816":"markdown","2e6f3441":"markdown","49b63d37":"markdown","c82d3b53":"markdown","3199c07a":"markdown","ed1f3ef4":"markdown","9c6e40d4":"markdown","7cae9a7d":"markdown","9ca53375":"markdown","74da3743":"markdown","b9d161b6":"markdown","a1940755":"markdown","8151080f":"markdown","801cb7cc":"markdown","8b214af8":"markdown","499c143f":"markdown","57a9a356":"markdown","c90b243f":"markdown","5d8dfd71":"markdown","fc488a4c":"markdown","8476cbbe":"markdown","7f6cac4c":"markdown","7a4f5ff6":"markdown","2d72c4d5":"markdown","6eeff608":"markdown","a821be94":"markdown","2332d71d":"markdown","50068aba":"markdown","96d85e6f":"markdown","b6bb0b4b":"markdown","c5c7a692":"markdown","b015e7b4":"markdown","fe7b4a02":"markdown","06ae9d31":"markdown","96a90c32":"markdown","5e0d06f3":"markdown","8c3959dc":"markdown","55acb542":"markdown","a2aa5548":"markdown"},"source":{"7c146752":"pip install pydotplus","9ba4773b":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nplt.rcParams['figure.figsize'] = (10, 8)\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport collections\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom ipywidgets import Image\nfrom io import StringIO\nimport pydotplus \nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom tqdm import tqdm_notebook","05ffb4f8":"#Creating dataframe with dummy variables\ndef df(dic,features):\n    out=pd.DataFrame(dic)\n    out=pd.concat([out,pd.get_dummies(out[features])],axis=1)\n    out.drop(features,axis=1,inplace=True)\n    return out\n\n#Intersecting features in train and train as absent in each.\ndef int_features(train,test):\n    com_feat=list(set(train.keys()) & set(test.keys())) \n    return train[com_feat],test[com_feat]","2c0aa199":"feat=['Looks', 'Alcoholic_beverage','Eloquence','Money_spent']","fe0e3f65":"df_train = {}\ndf_train['Looks'] = ['handsome', 'handsome', 'handsome', 'repulsive',\n                         'repulsive', 'repulsive', 'handsome'] \ndf_train['Alcoholic_beverage'] = ['yes', 'yes', 'no', 'no', 'yes', 'yes', 'yes']\ndf_train['Eloquence'] = ['high', 'low', 'average', 'average', 'low',\n                                   'high', 'average']\ndf_train['Money_spent'] = ['lots', 'little', 'lots', 'little', 'lots',\n                                  'lots', 'lots']\ndf_train['Will_go'] = LabelEncoder().fit_transform(['+', '-', '+', '-', '-', '+', '+'])\n\ndf_train = df(df_train, feat)\ndf_train","0bbabc42":"df_test = {}\ndf_test['Looks'] = ['handsome', 'handsome', 'repulsive'] \ndf_test['Alcoholic_beverage'] = ['no', 'yes', 'yes']\ndf_test['Eloquence'] = ['average', 'high', 'average']\ndf_test['Money_spent'] = ['lots', 'little', 'lots']\ndf_test = df(df_test, feat)\ndf_test","763340e2":"#features present in train but not in test need to be taken care of\ny=df_train['Will_go']\ndf_train.pop('Will_go')\ndf_train","30266ac8":"tree=DecisionTreeClassifier(criterion='entropy', random_state=12) \ntree.fit(df_train, y) ","9e5ff81a":"data = pd.read_csv('..\/input\/edadata\/telecom_churn.csv')","31148e9f":"data.head()","b1d69195":"data['International plan']= data['International plan'].map({'Yes':1,'No':0})\n\ndata['Voice mail plan']= data['Voice mail plan'].map({'Yes':1,'No':0})","760f345d":"data['Churn']=data['Churn'].astype('int')","b16ae69c":"data.info()","0d138722":"data.isna().sum()","f4b8d360":"state=data.pop('State')","22a4d5f5":"x,y = data.drop(['Churn'], axis=1),data['Churn']\n    ","9c0fdd9e":"x.shape,y.shape","15adc676":"x_train,x_valid,y_train,y_valid=train_test_split(x,y,test_size=0.3,random_state=19)\nx_train.shape,x_valid.shape,y_train.shape,y_valid.shape","68ec73d4":"tk=DecisionTreeClassifier(random_state=19)","dd697f8d":"tk.fit(x_train,y_train)","edc3983a":"pre_valid=tk.predict(x_valid)","79d7b614":"pre_valid.shape,y_valid.shape","3a00852e":"accuracy_score(pre_valid,y_valid)","f8440948":"y.value_counts(normalize=True)","5b471df3":"param = {'max_depth': np.arange(2,11),'min_samples_leaf': np.arange(1,11)}","d73fbd0b":"Kfold= StratifiedKFold(n_splits=5,shuffle=True,random_state=19)","2b60293a":"best = GridSearchCV(estimator=tk,param_grid=param,cv=Kfold,n_jobs=-1,verbose=1)","bb9c643a":"best.fit(x_train,y_train)","95b94719":"best.best_params_","0cf9ddce":"best.best_estimator_","2237f41a":"best.best_score_","464faf7c":"pred_val=best.predict(x_valid)","c969006a":"accuracy_score(pred_val,y_valid)","6ebfb588":"dot_data=StringIO()\nexport_graphviz(decision_tree=best.best_estimator_,out_file=dot_data,filled=True,feature_names=data.drop(['Churn'], axis=1).columns)\ngraph=pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(value=graph.create_png())","c8102f42":"tk9=DecisionTreeClassifier(random_state=19,max_depth=3).fit(x_train,y_train)","77f8ced2":"dot2_data=StringIO()\nexport_graphviz(decision_tree=tk9,out_file=dot2_data,filled=True,feature_names=data.drop(['Churn'], axis=1).columns)\ngraph=pydotplus.graph_from_dot_data(dot2_data.getvalue())\nImage(value=graph.create_png())","c575f781":"Kfold2= StratifiedKFold(n_splits=5,shuffle=True,random_state=19)","2040ef7a":"#Lists of CV accuracies and Validation accuracies\n\nacc_depth , valid_acc=[],[]\nmax_depth_val= np.arange(2,10)\n#for each value of max_depth\nfor new_max_depth in tqdm_notebook(max_depth_val):\n    new=DecisionTreeClassifier(random_state=19,max_depth=new_max_depth)\n    \n    #performing cross validation\n    val=cross_val_score(estimator=tk,X=x_train,y=y_train,cv=Kfold2)\n    \n    #Appending all CV scores after each split to get their mean.\n    acc_depth.append(val.mean())\n    \n    #Asses the model on validation set\n    new.fit(x_train,y_train)\n    val2=new.predict(x_valid)\n    valid_acc.append(accuracy_score(val2,y_valid))","5e464c0c":"acc_depth,valid_acc","4800a5df":"Split x & y into train and test data.","06a1da36":"\n\nChurn depicting 1 says the percentage of clients about to churn out. 14.4 % is much bad as compared to 8.7 % which is determined by Decision Tree Classifier.","a3a395b2":"81 candidate refer to 9 sets of permutations for max_depth with 10 sets of min_samples_leaf","17ff6623":"Model fit with training.","ba026187":"Model learning on validation set for prediction.\n","47202cad":"Our best set of parameters.","52cdb816":"### The \"Telecom\" dataset","2e6f3441":"Fit the train data into the classifier","49b63d37":"Creation of Grid","c82d3b53":"**Split the dataframe into x matrix and y target vector.**","3199c07a":"Our CV score for 'best' tree.","ed1f3ef4":"Each row represents a customer; each column contains customer\u2019s attributes. The datasets have the following attributes or features:\nState: string\nAccount length: integer\nArea code: integer\nInternational plan: string\nVoice mail plan: string\nNumber vmail messages: integer\nTotal day minutes: double\nTotal day calls: integer\nTotal day charge: double\nTotal eve minutes: double\nTotal eve calls: integer\nTotal eve charge: double\nTotal night minutes: double\nTotal night calls: integer\nTotal night charge: double\nTotal intl minutes: double\nTotal intl calls: integer\nTotal intl charge: double\nCustomer service calls: integer\nChurn: string\n\n\nThe dataset contains 667 rows (customers) and 20 columns (features).\n\nThe \"Churn\" column is the target to predict.","9c6e40d4":"HYPERPARAMETER TUNING using hands on CV without any GRID SEARCH.","7cae9a7d":"## <center>  Decision trees with a toy task and the Telecom dataset \n","9ca53375":"Check for missing values in training set.","74da3743":"For CV, we require number of splits and shuffling after each also known as K Fold Stratified CV.","b9d161b6":"Primary Data Analysis of the data.","a1940755":"# Tree visuals","8151080f":"The matrix and the vector have the same number of instances with all the columns in the matrix.\n","801cb7cc":"### Part 1. Toy dataset \"Will They? Won't They?\"","8b214af8":"Our best estimator.","499c143f":"Train data","57a9a356":"Time to check accuracy of the model on validation set.","c90b243f":"S0 = -3\/4*log2(3\/4) - 1\/4*log(1\/4) =0.311+0.5=0.811","5d8dfd71":"**Slight preprocessing**","fc488a4c":"The tree we see here is problematic for the naked eye, let us reduce the max_depth to 3 just for visualization purpose.","8476cbbe":"So, orange boxes resemble optimism ,i.e. there is scope of retention of more clients and vice versa for blue. Above figure is the threshold for split. gini again like entropy is worse if tends towards 1. Out of 2333 clients 1984 are loyal and rest would churn out. And accordingly we go down the tree depth via thresholds and splits.","7f6cac4c":"\nCreating the dataset\n","7a4f5ff6":"For sets of 8 max_depth parameter we observe the respective accuracy scores of cross validation and validation sets.","2d72c4d5":"Test Data","6eeff608":"Removing target variable &  saving State as a series.","a821be94":"HYPERPARAMETER TUNING using GridSearch Cross validation","2332d71d":"Your goal is to figure out how decision trees work by walking through a toy problem. While a single decision tree does not yield outstanding results, other performant algorithms like gradient boosting and random forests are based on the same idea. That is why knowing how decision trees work might be useful.\nWe'll go through a toy example of binary classification - Person A is deciding whether they will go on a second date with Person B. It will depend on their looks, eloquence, alcohol consumption (only for example), and how much money was spent on the first date.","50068aba":"Loading all necessary libraries.","96d85e6f":"We would be performing Decision Tree Algorithms on 1.Built in Dataset  2. Telecom Churn Dataset 2.1 without Hyperparameter Tuning 2.2 Hyperparameter Tuning with GridSearch Stratified K-Fold Cross Validation 2.3 Without GridSearch Stratified K-Fold Cross Validation.","b6bb0b4b":"**Train decision tree using sklearn classifier**","c5c7a692":"Convet churn variable into 0s & 1s","b015e7b4":"**So now our accuracy has increased from 91.3% to 94.2 %. This is always a better result.","fe7b4a02":"Reading the dataset","06ae9d31":"Set the combination of parameters for grid creation.","96a90c32":"Building the classifier.","5e0d06f3":"**Validation Assesment**","8c3959dc":"What is the entropy  S0  of the initial system? By system states, we mean values of the binary feature \"Will_go\" - 0 or 1 - two states in total.","55acb542":"Cross Validation Assesment on model quality.","a2aa5548":"Labelling for the International plan & Voice mail Plan."}}