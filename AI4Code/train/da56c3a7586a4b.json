{"cell_type":{"db19736a":"code","635aab29":"code","e4cdbfe8":"code","ffe5d779":"code","4884e5d0":"code","0f9d4338":"code","0d365739":"code","424b26ad":"code","a2190099":"code","f95aa1fd":"code","17c26692":"code","4b57b9c7":"code","003bd327":"code","b316ec11":"code","a3f52ce1":"code","e66ae57d":"code","17c6e395":"code","2489f135":"code","96aadc3d":"code","cd791f82":"code","d7e36a4f":"code","423f27b0":"code","5b05fc4a":"code","155d574a":"code","7c922a1d":"code","3f120a74":"code","dece638b":"code","5aa6d729":"code","62870a02":"code","1664c03d":"code","cd28677b":"code","5d178ce9":"code","7f697aed":"code","0fa54694":"code","05a305d5":"code","415b559b":"code","656bc5e8":"code","65be8192":"code","ed9317c8":"code","53c9449d":"code","e9251225":"code","461a8d9b":"code","6f47b187":"code","1d0b3c7f":"code","d2b1c6cc":"code","4029f33e":"code","cae63232":"code","6952b5f0":"markdown","1bdde7c1":"markdown","9f4373ac":"markdown","5c402fbe":"markdown","0279b10e":"markdown","b0c9d077":"markdown","17ec7af9":"markdown","46e3caab":"markdown","6c6bd0ab":"markdown","cead2a0e":"markdown","d648f5ee":"markdown","3be636bf":"markdown","fffb78fc":"markdown","2b0a5aac":"markdown","1dfac297":"markdown","652a27f3":"markdown","70ab7e16":"markdown","88d018bc":"markdown","afbbd32d":"markdown","216b5d11":"markdown","7b0fc9e5":"markdown","0b05c975":"markdown","b5e1ad80":"markdown","4b32f98c":"markdown","1f4d2523":"markdown","b4a54bb8":"markdown","9619d682":"markdown","d4ad7bbb":"markdown"},"source":{"db19736a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","635aab29":"#import other packages\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e4cdbfe8":"path = '\/kaggle\/input\/white-wine-quality\/winequality-white.csv'\n\ndf = pd.read_csv(path,sep=';')","ffe5d779":"df.head(5)","4884e5d0":"df.describe()","0f9d4338":"#Count the missing values in the dataset\ndf.isnull().sum()","0d365739":"#unique values for quality\ndf.quality.unique()","424b26ad":"len(df)","a2190099":"#Divide the quality of wine into good and bad wine\n#Binary classification\nwinequality_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = (2, 5.5, 9), labels = winequality_names)\ndf.head(3)","f95aa1fd":"df.quality.value_counts()","17c26692":"#Boxplot\nfig, axes = plt.subplots(4, 3, figsize=(20,20))\n\nfig.suptitle(\"White Wine Quality Distribution\")\nsns.boxplot(ax=axes[0, 0], data=df, x='quality', y='fixed acidity')\nsns.boxplot(ax=axes[0, 1], data=df, x='quality', y='volatile acidity')\nsns.boxplot(ax=axes[0, 2], data=df, x='quality', y='citric acid')\nsns.boxplot(ax=axes[1, 0], data=df, x='quality', y='residual sugar')\nsns.boxplot(ax=axes[1, 1], data=df, x='quality', y='chlorides')\nsns.boxplot(ax=axes[1, 2], data=df, x='quality', y='free sulfur dioxide')\nsns.boxplot(ax=axes[2, 0], data=df, x='quality', y='total sulfur dioxide')\nsns.boxplot(ax=axes[2, 1], data=df, x='quality', y='density')\nsns.boxplot(ax=axes[2, 2], data=df, x='quality', y='pH')\nsns.boxplot(ax=axes[3, 0], data=df, x='quality', y='sulphates')\nsns.boxplot(ax=axes[3, 1], data=df, x='quality', y='alcohol')","4b57b9c7":"#Make a correlation diagram\ncorr = df.corr()\n\nax = sns.heatmap(corr,vmin=-1, vmax=1, center=0,cmap=sns.diverging_palette(20, 220, n=200),square=True)\n\nax.set_xticklabels(ax.get_xticklabels(),rotation=45,horizontalalignment='right')","003bd327":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","b316ec11":"df.columns","a3f52ce1":"#Define the independent variables\nFeatures = df[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']]\nX = Features","e66ae57d":"#Define the dependent variable\ny = df['quality'].values\ny[0:5]","17c6e395":"X= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]","2489f135":"#split the data in a train and test set\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2)\nprint ('The length of the train set is:', X_train.shape,  y_train.shape)\nprint ('The lenth of the test set equals:', X_test.shape,  y_test.shape)","96aadc3d":"#import sklearn package for the random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier","cd791f82":"#import score and metric packages\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport itertools","d7e36a4f":"rfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_test)","423f27b0":"#What is the accuracy of the above model\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","5b05fc4a":"#Other performance metrics: how did the model perform?\nprint(classification_report(y_test,y_pred))","155d574a":"#Confusion Matrix\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\n\nsns.heatmap(cm,cbar=False,annot=True,cmap='Blues',fmt=\"d\")\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_test\")\nplt.title(\"Confusion Matrix: Random Forest Classifier\")\nplt.show()","7c922a1d":"#import the logistic regression packages\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss","3f120a74":"#liblinear regression\nLR_a = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nyhat_prob_a = LR_a.predict_proba(X_test)\nlog_loss(y_test, yhat_prob_a)","dece638b":"LR_b = LogisticRegression(C=0.01, solver='saga').fit(X_train,y_train)\nyhat_prob_b = LR_b.predict_proba(X_test)\nlog_loss(y_test, yhat_prob_b)","5aa6d729":"LR_c = LogisticRegression(C=0.01, solver='newton-cg').fit(X_train,y_train)\nyhat_prob_c = LR_c.predict_proba(X_test)\nlog_loss(y_test, yhat_prob_c)","62870a02":"LR_d = LogisticRegression(C=0.01, solver='lbfgs').fit(X_train,y_train)\nyhat_prob_d = LR_d.predict_proba(X_test)\nlog_loss(y_test, yhat_prob_d)","1664c03d":"LR_e = LogisticRegression(C=0.01, solver='sag').fit(X_train,y_train)\nyhat_prob_e = LR_e.predict_proba(X_test)\nlog_loss(y_test, yhat_prob_e)","cd28677b":"LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR","5d178ce9":"#log loss score\nlr_ypred = LR.predict_proba(X_test)\nlog_loss(y_test,lr_ypred)","7f697aed":"#import the decision tree packages \nfrom sklearn.tree import DecisionTreeClassifier","0fa54694":"decision_tree = DecisionTreeClassifier(criterion=\"entropy\",random_state=42)\ndecision_tree = decision_tree.fit(X_train,y_train)","05a305d5":"param_grid = {'max_depth':range(1, decision_tree.tree_.max_depth+1, 2),'max_features': range(1, len(decision_tree.feature_importances_)+1)}\n\nwine_gr = GridSearchCV(DecisionTreeClassifier(criterion=\"entropy\",random_state=42),param_grid=param_grid,scoring='accuracy',n_jobs=-1)\n\nwine_gr = wine_gr.fit(X_train, y_train)","415b559b":"wine_gr.best_estimator_.tree_.node_count, wine_gr.best_estimator_.tree_.max_depth","656bc5e8":"# Create Decision Tree classifer object with the optimal depth\nclf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42, max_depth=24)\n\n# Train Decision Tree Classifer\nDT_Model = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\nypred_tree = DT_Model.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, ypred_tree))","65be8192":"#Calculate the f1-score\nfscore_tree = f1_score(y_test,ypred_tree, average='weighted') \nprint(\"Accuracy:\",fscore_tree)","ed9317c8":"#Other accuracy measures\naccuracy_dt = classification_report(y_test,ypred_tree)\nprint(accuracy_dt)","53c9449d":"#decision tree visualization\nimport graphviz\nfrom sklearn.tree import export_graphviz \nfrom IPython.display import Image  \nfrom sklearn import tree\n","e9251225":"featureNames = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']\nclass_names = df['quality'].unique().tolist()\n\ndot_data = tree.export_graphviz(clf,feature_names = featureNames,class_names=class_names,filled=True, rounded=True)\n\ngraph = graphviz.Source(dot_data)  \ngraph","461a8d9b":"#import support vector machine learning packages from sklearn\nfrom sklearn import svm\nfrom sklearn.svm import SVC","6f47b187":"kernel = ['rbf','linear','poly','sigmoid']   \nacc_score_list = []\n\nfor k in kernel:\n    clf = svm.SVC(kernel=k)\n    clf.fit(X_train, y_train)\n    ypred = clf.predict(X_test)\n    acc_score_list.append(f1_score(y_test, ypred, average='weighted')) \n    \nacc_score_list","1d0b3c7f":"#Best model \nsvmmodel = svm.SVC(kernel='rbf')\nsvmmodel  = svmmodel.fit(X_train, y_train) \n","d2b1c6cc":"svm_ypred = svmmodel.predict(X_test)\nsvm_score = f1_score(y_test, svm_ypred, average='weighted')\nprint(\"Accuracy using F-score: \",svm_score)","4029f33e":"#Other performance metrics: how well did the support vector model perform?\nprint(classification_report(y_test,svm_ypred))","cae63232":"#Confusion Matrix\ncm_svm = confusion_matrix(y_test,svm_ypred)\nprint(cm_svm)\n\nsns.heatmap(cm_svm,cbar=False,annot=True,cmap='Blues',fmt=\"d\")\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_test\")\nplt.title(\"Confusion Matrix: Support Vector Machine Learning\")\nplt.show()","6952b5f0":"### 1.Random Forest Classifier","1bdde7c1":"The differences in accuracy are rounding differences! So, let's pick the model with liblinear logistic regression.","9f4373ac":"*Data Normalization*","5c402fbe":"Let's select the independent variables; these are the variables that contribute to wine quality.","0279b10e":"The wine has been classified into *'good'* wine and *'bad'* wine. A wine is a good wine when its score is higher than 5.5; it is a bad wine when its score is lower than 5.5.","b0c9d077":"The optimal depth includes 24 leaves. ","17ec7af9":"### Data Preprocessing for Machine Learning","46e3caab":"**Best Model: Decision Tree**","6c6bd0ab":"The overall accuracy score of the decision tree classifier equals 79%.","cead2a0e":"### 4.Support Vector Machine Learning","d648f5ee":"*Approach: Grid Search*","3be636bf":"Welcome and thanks for opening this notebook! This notebook is excellent for beginners; I use classification machine learning techniques and build a model to predict the white wine quality. This is a 4-way classification analysis. I have built 4 models to predict the quality of white wine, including the following machine learning techniques: random forest classifier, logistic regression, decision tree, and support vector machine learning. I start with some data exploration. Next, I prepare the data for machine learning. Lastly, I create 4 classification models to predict white wine quality.","fffb78fc":"### 2.Logistic Regression","2b0a5aac":"The data is ready for a classification machine learning analysis. With classification machine learning techniques, we can predict the quality of a specific white wine. ","1dfac297":"# White Wine Quality: Classification","652a27f3":"The logistic regression model has a logg loss score of 52%. The closer the logg loss to zero, the higher the accuracy.","70ab7e16":"It is a pretty large dataset.","88d018bc":"Let's select the best model for SVM (support vector machine learning abbreviated). Support Vector Machine Learning is typically used for smaller dataset. This dataset is a quite large, but let's give it a try.","afbbd32d":"**Find the Optimal Decision Tree Length**","216b5d11":"The overall weighted average classification model score equals 78%. The random forest classifier model has a higher accuracy score than the support vector machine learning model.","7b0fc9e5":"The training dataset is 80% of the total data; the test set is 20% of the total data.","0b05c975":"Let's find the optimal logistic regression model first. Which model has the highest accuracy score?","b5e1ad80":"Now, let's define 'quality' as the dependent variable.","4b32f98c":"The random forest model gives an overall accuracy of 85%.","1f4d2523":"**Thanks for going through this notebook! This is the end of this analysis. Hope you enjoyed it! If you find this notebook useful or if you enjoyed it please upvote :)**","b4a54bb8":"There are no missing values in the dataset.","9619d682":"The *'rbf'* kernel has the best accuracy for support vector machine learning. The accuracy is 77%.","d4ad7bbb":"### 3.Decision Tree"}}