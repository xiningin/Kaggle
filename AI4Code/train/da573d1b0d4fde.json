{"cell_type":{"fa0d63a8":"code","5ae4a9fb":"code","6a9223ea":"code","5346247f":"code","ceea3a58":"code","54a0dc47":"code","e3d72788":"code","2631a9d5":"code","126c1166":"code","5524a26f":"markdown","739278d2":"markdown","8b1853ff":"markdown","02c68049":"markdown","55e2970c":"markdown","29e6d39f":"markdown"},"source":{"fa0d63a8":"# Set-up libraries\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nfrom tensorflow import keras","5ae4a9fb":"# Check source\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6a9223ea":"# Load data\ndf = pd.read_json('..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json', lines=True)\ndf.head()","5346247f":"# Look at some details\ndf.info()","ceea3a58":"# Look at breakdown of label\ndf['is_sarcastic'].value_counts()","54a0dc47":"# Split data into 80% training and 20% validation\nsentences = df['headline']\nlabels = df['is_sarcastic']\n\ntrain_sentences, val_sentences, train_labels, val_labels = train_test_split(sentences, labels, test_size=0.2, random_state=0)\n\nprint(train_sentences.shape)\nprint(val_sentences.shape)\nprint(train_labels.shape)\nprint(val_labels.shape)","e3d72788":"# Tokenize and pad\nvocab_size = 10000\noov_token = '<00V>'\nmax_length = 100\npadding_type = 'post'\ntrunc_type = 'post'\n\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\nval_sequences = tokenizer.texts_to_sequences(val_sentences)\nval_padded = pad_sequences(val_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","2631a9d5":"# Build and train neural network\nembedding_dim = 16\nnum_epochs = 10\nbatch_size = 100\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy']\n             )\n\nhistory = model.fit(train_padded, train_labels, batch_size=batch_size, epochs=num_epochs, \n                    verbose=2)","126c1166":"# Apply neural network\nval_loss, val_accuracy = model.evaluate(val_padded, val_labels)\nprint('Val loss: {}, Val accuracy: {}'.format(val_loss, val_accuracy*100))\n\nquick_test_sentence = [\n    'canada is flattening the coronavirus curve',\n    'canucks take home the cup',\n    'safety meeting ends in accident'\n    \n]\n\nquick_test_sequences = tokenizer.texts_to_sequences(quick_test_sentence)\nquick_test_padded = pad_sequences(quick_test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\nquick_test_sentiments = model.predict(quick_test_padded)\n\nfor i in range(len(quick_test_sentiments)):\n    print('\\n' + quick_test_sentence[i])\n    if 0 < quick_test_sentiments[i] < .50:\n        print('Unlikely sarcasm. Sarcasm score: {}'.format(quick_test_sentiments[i]*100))\n    elif .50 < quick_test_sentiments[i] < .75:\n        print('Possible sarcasm. Sarcasm score: {}'.format(quick_test_sentiments[i]*100))\n    elif .75 >  quick_test_sentiments[i] < 1:\n        print('Sarcasm. Sarcasm score:  {}'.format(quick_test_sentiments[i]*100))\n    else:\n        print('Not in range')","5524a26f":"## Step 2: Prepare data and understand some more\nIn this step, we perform the necessary transformations on the data so that the neural network would be able to understand it. Real-world datasets are complex and messy. For our purposes, most of the datasets we work on in this series require minimal preparation.","739278d2":"## Step 1: Set-up and understand data\nIn this step, we layout the tools we will need to solve the problem identified in the previous step. We want to inspect our data sources and explore the data itself to gain an understanding of the data for preprocessing and modeling.","8b1853ff":"## Step 0: Understand the problem\nWhat we're trying to do here is to classify whether a news headline is sarcastic.","02c68049":"## About \nThis notebook contains a very fast and simple NLP example in Python.\n\nThis work is part of a series called [NLP in minutes - fast, simple examples](https:\/\/www.kaggle.com\/jamiemorales\/nlp-in-minutes-fast-simple-example)\n\nThe approach is designed to help grasp the applied artificial intelligence workflow in minutes. It is not an alternative to actually taking the time to learn. What it aims to do is help someone get started fast and gain intuitive understanding of the typical steps early on.","55e2970c":"## Step 3: Build, train, and evaluate neural network\nFirst, we design the neural network, e.g., sequence of layers and activation functions. \n\nSecond, we train the neural network, we iteratively make a guess, calculate how accurate that guess is, and enhance our guess. The first guess is initialised with random values. The goodness or badness of the guess is measured with the loss function. The next guess is generated and enhanced by the optimizer function.\n\nLastly, we apply use the neural network on previously unseen data and evaluate the results.","29e6d39f":"## Learn more\nIf you found this example interesting, you may also want to check out:\n\n* [Deep learning - very fast fundamental examples](https:\/\/www.kaggle.com\/jamiemorales\/deep-learning-very-fast-simple-examples)\n* [Machine learning in minutes - very fast fundamental examples in Python](https:\/\/www.kaggle.com\/jamiemorales\/machine-learning-in-minutes-very-fast-examples)\n* [List of machine learning methods & datasets](https:\/\/www.kaggle.com\/jamiemorales\/list-of-machine-learning-methods-datasets)\n\nThanks for reading. Don't forget to upvote."}}