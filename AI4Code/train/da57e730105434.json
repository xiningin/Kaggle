{"cell_type":{"2373922d":"code","71d74d96":"code","98201c78":"code","71395021":"code","edf11f5e":"code","8b08f0ac":"code","1efb720c":"code","7634084e":"code","74786f3f":"code","d2e24d1f":"markdown","5a7aa977":"markdown","373cae1a":"markdown"},"source":{"2373922d":"!pip -q install transformers --upgrade\n!pip show transformers","71d74d96":"import os\n\nimport numpy as np\nimport pandas as pd\nimport transformers as trfm\nfrom tokenizers import BertWordPieceTokenizer\nfrom tqdm.notebook import tqdm","98201c78":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512, enable_padding=False):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \n    ---\n    \n    Inputs:\n        tokenizer: the `fast_tokenizer` that we imported from the tokenizers library\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    if enable_padding:\n        tokenizer.enable_padding(max_length=maxlen)\n    \n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","71395021":"def combine_qa_ids(q_ids, a_ids, tokenizer, maxlen=512):\n    \"\"\"\n    Given two arrays of IDs (questions and answers) created by\n    `fast_encode`, we combine and pad them.\n    Inputs:\n        tokenizer: The original tokenizer (not the fast_tokenizer)\n    \"\"\"\n    combined_ids = []\n\n    for i in tqdm(range(q_ids.shape[0])):\n        ids = []\n        ids.append(tokenizer.cls_token_id)\n        ids.extend(q_ids[i])\n        ids.append(tokenizer.sep_token_id)\n        ids.extend(a_ids[i])\n        ids.append(tokenizer.sep_token_id)\n        ids.extend([tokenizer.pad_token_id] * (maxlen - len(ids)))\n\n        combined_ids.append(ids)\n    \n    return np.array(combined_ids)","edf11f5e":"df = pd.concat([\n    pd.read_csv(f\"\/kaggle\/input\/stackexchange-qa-pairs\/pre_covid\/{group}.csv\")\n    for group in ['general', 'expert', 'biomedical']\n])\n\nquestions = df.title + ' [SEP] ' + df.question","8b08f0ac":"tokenizer = trfm.ElectraTokenizer.from_pretrained(\"google\/electra-small-discriminator\")\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n\n# Reload it with the huggingface tokenizers library\nMAX_LEN = 512\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True, add_special_tokens=False)","1efb720c":"q_ids = fast_encode(questions.values, fast_tokenizer, maxlen=MAX_LEN\/\/2 - 2)\na_ids = fast_encode(df.answer.values, fast_tokenizer, maxlen=MAX_LEN\/\/2 - 2)\nwa_ids = fast_encode(df.wrong_answer.values, fast_tokenizer, maxlen=MAX_LEN\/\/2 - 2)","7634084e":"correct_ids = combine_qa_ids(q_ids, a_ids, tokenizer, maxlen=MAX_LEN)\nwrong_ids = combine_qa_ids(q_ids, wa_ids, tokenizer, maxlen=MAX_LEN)","74786f3f":"np.save(\"correct_ids.npy\", correct_ids)\nnp.save(\"wrong_ids.npy\", wrong_ids)","d2e24d1f":"## Load tokenizer","5a7aa977":"## Start encoding","373cae1a":"## Load Data"}}