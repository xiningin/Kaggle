{"cell_type":{"253c2c0c":"code","b41cb1b4":"code","ea1ee0c3":"code","1abafbad":"code","b03999f4":"code","57557a8c":"code","c2380882":"code","92720d7d":"code","24c14690":"code","4ca01b6f":"code","f4a7eb7d":"code","261fa256":"code","8287c89f":"code","eb7dedf7":"code","fdb74140":"code","0f1259e8":"code","bb0f76e5":"code","e7a8b220":"code","cbd02ed4":"code","14b16f6a":"code","a7fc3217":"code","32e8c8a8":"code","b07417a7":"code","3e004176":"markdown","104fac5f":"markdown","52f726c4":"markdown","53b5ee60":"markdown","04616fc8":"markdown","b46ab55d":"markdown","6d028289":"markdown","b5f2f7a7":"markdown","4327c4fb":"markdown","0f4d994c":"markdown","a33f76ab":"markdown","639c8165":"markdown","e616f027":"markdown","10944f9b":"markdown","a402dd11":"markdown","abef9018":"markdown","ff5188ac":"markdown","8c463c2b":"markdown","93292d4b":"markdown"},"source":{"253c2c0c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nplt.style.use('fivethirtyeight')","b41cb1b4":"pjme = pd.read_csv('..\/input\/hourly-energy-consumption\/PJME_hourly.csv', index_col=[0], parse_dates=[0])","ea1ee0c3":"color_pal = [\"#F8766D\", \"#D39200\", \"#93AA00\", \"#00BA38\", \"#00C19F\", \"#00B9E3\", \"#619CFF\", \"#DB72FB\"]\n_ = pjme.plot(style='.', figsize=(15,5), color=color_pal[0], title='PJM East')","1abafbad":"split_date = '01-Jan-2015'\npjme_train = pjme.loc[pjme.index <= split_date].copy()\npjme_test = pjme.loc[pjme.index > split_date].copy()","b03999f4":"_ = pjme_test \\\n    .rename(columns={'PJME_MW': 'TEST SET'}) \\\n    .join(pjme_train.rename(columns={'PJME_MW': 'TRAINING SET'}), how='outer') \\\n    .plot(figsize=(15,5), title='PJM East', style='.')","57557a8c":"pjme_train.head(7)","c2380882":"pjme_train.head(7).shift(1)","92720d7d":"from IPython.display import Image\nImage(filename='..\/input\/rollingwindow\/rollingwindow.jpg') ","24c14690":"pjme_train.head(7)","4ca01b6f":"pjme_train.head(7).rolling(window = 2).mean()","f4a7eb7d":"def create_features(df, label=None):\n    \"\"\"\n    Creates time series features from datetime index\n    \"\"\"\n    df['date'] = df.index\n    df['hour'] = df['date'].dt.hour\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    df['weekofyear'] = df['date'].dt.weekofyear\n    df['pjme_6_hrs_lag'] = df['PJME_MW'].shift(6)\n    df['pjme_12_hrs_lag'] = df['PJME_MW'].shift(12)\n    df['pjme_24_hrs_lag'] = df['PJME_MW'].shift(24)\n    df['pjme_6_hrs_mean'] = df['PJME_MW'].rolling(window = 6).mean()\n    df['pjme_12_hrs_mean'] = df['PJME_MW'].rolling(window = 12).mean()\n    df['pjme_24_hrs_mean'] = df['PJME_MW'].rolling(window = 24).mean()\n    df['pjme_6_hrs_std'] = df['PJME_MW'].rolling(window = 6).std()\n    df['pjme_12_hrs_std'] = df['PJME_MW'].rolling(window = 12).std()\n    df['pjme_24_hrs_std'] = df['PJME_MW'].rolling(window = 24).std()\n    df['pjme_6_hrs_max'] = df['PJME_MW'].rolling(window = 6).max()\n    df['pjme_12_hrs_max'] = df['PJME_MW'].rolling(window = 12).max()\n    df['pjme_24_hrs_max'] = df['PJME_MW'].rolling(window = 24).max()\n    df['pjme_6_hrs_min'] = df['PJME_MW'].rolling(window = 6).min()\n    df['pjme_12_hrs_min'] = df['PJME_MW'].rolling(window = 12).min()\n    df['pjme_24_hrs_min'] = df['PJME_MW'].rolling(window = 24).min()\n    \n    X = df[['hour','dayofweek','quarter','month','year',\n           'dayofyear','dayofmonth','weekofyear' , 'pjme_6_hrs_lag' , 'pjme_24_hrs_lag' , 'pjme_6_hrs_mean',\n           \"pjme_12_hrs_mean\" ,\"pjme_24_hrs_mean\" ,\"pjme_6_hrs_std\" ,\"pjme_12_hrs_std\" ,\"pjme_24_hrs_std\",\n           \"pjme_6_hrs_max\",\"pjme_12_hrs_max\" ,\"pjme_24_hrs_max\" ,\"pjme_6_hrs_min\",\"pjme_12_hrs_min\" ,\"pjme_24_hrs_min\"]]\n    if label:\n        y = df[label]\n        return X, y\n    return X","261fa256":"X_train, y_train = create_features(pjme_train, label='PJME_MW')\nX_test, y_test = create_features(pjme_test, label='PJME_MW')","8287c89f":"reg = xgb.XGBRegressor(n_estimators=1000)\nreg.fit(X_train, y_train,\n        eval_set=[(X_train, y_train), (X_test, y_test)],\n        early_stopping_rounds=50,\n       verbose=False) # Change verbose to True if you want to see it train","eb7dedf7":"_ = plot_importance(reg, height=0.9 ,max_num_features = 10)","fdb74140":"pjme_test['MW_Prediction'] = reg.predict(X_test)\npjme_all = pd.concat([pjme_test, pjme_train], sort=False)","0f1259e8":"_ = pjme_all[['PJME_MW','MW_Prediction']].plot(figsize=(15, 5))","bb0f76e5":"# Plot the forecast with the actuals\nf, ax = plt.subplots(1)\nf.set_figheight(5)\nf.set_figwidth(15)\n_ = pjme_all[['MW_Prediction','PJME_MW']].plot(ax=ax,\n                                              style=['-','.'])\nax.set_xbound(lower='01-01-2015', upper='02-01-2015')\nax.set_ylim(0, 60000)\nplot = plt.suptitle('January 2015 Forecast vs Actuals')","e7a8b220":"# Plot the forecast with the actuals\nf, ax = plt.subplots(1)\nf.set_figheight(5)\nf.set_figwidth(15)\n_ = pjme_all[['MW_Prediction','PJME_MW']].plot(ax=ax,\n                                              style=['-','.'])\nax.set_xbound(lower='01-01-2015', upper='01-08-2015')\nax.set_ylim(0, 60000)\nplot = plt.suptitle('First Week of January Forecast vs Actuals')","cbd02ed4":"f, ax = plt.subplots(1)\nf.set_figheight(5)\nf.set_figwidth(15)\n_ = pjme_all[['MW_Prediction','PJME_MW']].plot(ax=ax,\n                                              style=['-','.'])\nax.set_ylim(0, 60000)\nax.set_xbound(lower='07-01-2015', upper='07-08-2015')\nplot = plt.suptitle('First Week of July Forecast vs Actuals')","14b16f6a":"mean_squared_error(y_true=pjme_test['PJME_MW'],\n                   y_pred=pjme_test['MW_Prediction'])","a7fc3217":"mean_absolute_error(y_true=pjme_test['PJME_MW'],\n                   y_pred=pjme_test['MW_Prediction'])","32e8c8a8":"def mean_absolute_percentage_error(y_true, y_pred): \n    \"\"\"Calculates MAPE given y_true and y_pred\"\"\"\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","b07417a7":"mean_absolute_percentage_error(y_true=pjme_test['PJME_MW'],\n                   y_pred=pjme_test['MW_Prediction'])","3e004176":"1. Let us create a lag of 1 hour the entire dataframe is shifted by 1 hour , hence the entire data frame will get shifted by 1 hour , eg the first entry will be NaN , second entry will change to 26498.0 from 25147.0 , third entry will change to 25147.0 from 24574.0 , Lags can be thought as we are using past information to forecast future , below we demonstrate lag of 1\n\n2. Lags can be created seamlessly using `.shift()` function from pandas\n\n3. For demonstration I am creating lags of size 1 , but in practice lags can be created of various size , they are hyperparamters of a model","104fac5f":"# Error Metrics On Test Set\n1. Our RMSE error is 426144.6905834089 \n2. Our MAE error is 421\n3. Our MAPE error is 1.35%","52f726c4":"1. Lag features are added to convert time series forecasting as a supervised Machine Learning Problem.\n2. Rolling Window statistic are added ,typical rolling window statistic includes mean , median , std deviation over a fixed window sized statistic","53b5ee60":"1. We can see that lag is the most important features followed , by min , max ,mean and std deviation.\n2. From Feature importance plot we can say that definitely this features improves model perforamnce.\n3. I have added only few features here at 6 , 12 and 24 hrs , as recent past values are more important to predict near future.\n4. You can play with lags and rolling window of 3hrs , 4hrs and 5 hrs and see if it improves performance further","04616fc8":"# Create Time Series Features","b46ab55d":"#### Demostration of Rolling Window Statistics","6d028289":"# Look at first month of predictions","b5f2f7a7":"1. I will be using date time features used by Rob Mulla \n2. I will be adding additional lags features and rolling window statistics \n3. Note that I am using window size of 6 ,12 ,24 as data is sampled hourly this corresponds to lags of 6 ,12 ,24 hours","4327c4fb":"# Create XGBoost Model","0f4d994c":"# Forecast on Test Set","a33f76ab":"1. In pandas `.rolling()` functions can be used to create all this statistics\n2. Lets demonstrate , rolling mean with window size = 2","639c8165":"#### Demostration of Lag Features","e616f027":"# Data\nThe data we will be using is hourly power consumption data from PJM. Energy consumtion has some unique charachteristics. It will be interesting to see how prophet picks them up.\n\nPulling the `PJM East` which has data from 2002-2018 for the entire east region.","10944f9b":"I like using mean absolute percent error because it gives an easy to interperate percentage showing how off the predictions are.\nMAPE isn't included in sklearn so we need to use a custom function.","a402dd11":"## Rolling and Lag Features","abef9018":"# Hourly Time Series Forecasting using XGBoost\n\n1. In this notebook we will walk through time series forecasting using XGBoost. The data we will be using is hourly energy consumption.\n\n2. Original notebook can be found by Rob Mulla [here](https:\/\/www.kaggle.com\/robikscube\/tutorial-time-series-forecasting-with-xgboost).\n\n3. I added some of the Lags and Rolling window statistic features here to improve MAPE score . Aim is to beat the simple baseline score of 8.9%\n\n4. Lastly if you find this notebook useful **Please Do Upvote** :) , it will motivate me to write more notebooks :) !","ff5188ac":"Rolling window consists of fixed size windows over which statistics like mean , median , mode , min ,max ,std deviation etc , the below image demonstrates ,  how rolling window statistics is calculated ","8c463c2b":"# Train\/Test Split\nCut off the data after 2015 to use as our validation set.","93292d4b":"## Feature Importances\nFeature importance is a great way to get a general idea about which features the model is relying on most to make the prediction. This is a metric that simply sums up how many times each feature is split on.\n\nWe can see that the day of year was most commonly used to split trees, while hour and year came in next. Quarter has low importance due to the fact that it could be created by different dayofyear splits."}}