{"cell_type":{"7bb83143":"code","92cc6d02":"code","d3f00e0a":"code","0af1357d":"code","7bf0ba5e":"code","2ea43c12":"code","0e1bbca8":"code","b7845a6f":"code","cd4ac59a":"code","5e70135b":"code","235abf9e":"code","bb2962b3":"code","dd5afe01":"code","31c3799c":"code","b3c1d275":"code","97f41b55":"code","661ec8e0":"code","33ef3a25":"code","14078018":"code","2c484072":"code","34831ec1":"code","15321e87":"code","bbe2090e":"code","dae574e0":"code","06f40805":"code","7ad06699":"code","2731ac94":"code","982cf7d5":"code","4ded6e8f":"code","9ec4f86c":"code","c7a16d2d":"code","4cdfac6d":"code","7f074de0":"code","d28a79ca":"code","5c612f0a":"code","7120af83":"code","941cd936":"code","9dcdc344":"code","b72c67ad":"code","7dc533ab":"code","bc99bb5f":"code","8add55c0":"code","e2c4ace8":"code","2acc3e92":"code","9752019f":"code","b4e2f4da":"code","8e8866cd":"code","33166a90":"code","16278cf1":"code","bf1f6d3f":"code","3f0b977c":"code","36c2014c":"code","b8697838":"code","3bb43f64":"code","8fdf74a8":"code","e2189f30":"code","c4b77caa":"code","25e9aaf4":"code","a25a78b4":"code","b00eda22":"code","21b1da10":"code","123804e3":"code","fd1d2f8a":"code","3fe20460":"markdown","e294b1dc":"markdown","4b0040fd":"markdown","ab7aa909":"markdown","c23c4538":"markdown","be02d54c":"markdown","ee3fd265":"markdown","ed2ce682":"markdown","3e7dc633":"markdown","d3e1fe95":"markdown","c7de8114":"markdown","6813832f":"markdown","79501baf":"markdown","1703214d":"markdown","591e201c":"markdown","1d0d64c0":"markdown","d1bfebda":"markdown","ba99b866":"markdown","160d3017":"markdown","e5ad0b39":"markdown","b4ba3470":"markdown","970ab37b":"markdown","bf0729ca":"markdown","4e13246f":"markdown","0cc77e57":"markdown","4804c654":"markdown","52132584":"markdown"},"source":{"7bb83143":"#IMPORT THE LIBRARIES\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nimport IPython.display as ipd\nfrom IPython.display import Audio\n\n# for cnn model\nimport keras\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM,BatchNormalization\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Input, Flatten, Dropout, Activation\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n","92cc6d02":"# path to the directory of the dataset\nRAV = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/\"","d3f00e0a":"dir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAV + i + '\/' + f)\n\n        \nRAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\nRAV_df['source'] = 'RAVDESS'  \nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\nRAV_df.labels.value_counts()","0af1357d":"plt.figure(figsize=(12, 5))\nplt.title('Count of Emotions', size=16)\nsns.countplot(RAV_df.labels)\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nplt.xticks(rotation=45)\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.show()","7bf0ba5e":"# MALE NEUTRAL\nfname1=RAV+'Actor_01\/03-01-01-01-01-01-01.wav'\ndata, sr = librosa.load(fname1)\nipd.Audio(fname1) \n","2ea43c12":"# CREATE LOG MEL SPECTROGRAM\nplt.figure(figsize=(10, 5))\nspectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000) \nspectrogram = librosa.power_to_db(spectrogram)\nlibrosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\nplt.title('Mel Spectrogram - Male Neutral')\nplt.colorbar(format='%+2.0f dB');","0e1bbca8":"# FEMALE NEUTRAL\nfname2=RAV+'Actor_14\/03-01-01-01-01-01-14.wav'\ndata, sr = librosa.load(fname2)\nipd.Audio(fname2) ","b7845a6f":"# CREATE LOG MEL SPECTROGRAM\nplt.figure(figsize=(10, 5))\nspectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000) \nspectrogram = librosa.power_to_db(spectrogram)\nlibrosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\nplt.title('Mel Spectrogram - Female Neutral')\nplt.colorbar(format='%+2.0f dB');","cd4ac59a":"# Pick a fearful track\nfname3 = RAV + 'Actor_14\/03-01-06-02-02-02-14.wav'  \ndata, sr = librosa.load(fname3)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(data, sr=sr)\nplt.title('Waveplot - Female Fearful')\n# Lets play the audio \nipd.Audio(fname3)","5e70135b":"# Pick a happy track\nfname4 = RAV + 'Actor_14\/03-01-03-02-02-02-14.wav'  \ndata, sr = librosa.load(fname4)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(data, sr=sr)\nplt.title('Waveplot - Female Happy')\n\n# Lets play the audio \nipd.Audio(fname4)","235abf9e":"# Gender - Female; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_08\/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n\n# MFCC\nplt.figure(figsize=(16, 10))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()","bb2962b3":"# Gender - Male; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_09\/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n\n# MFCC\nplt.figure(figsize=(16, 10))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","dd5afe01":"# Gender - Female; Emotion - angry\npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_08\/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Gender - Male; Emotion - angry\npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_09\/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# Plot the two audio waves together\nplt.figure(figsize=(16,10))\nplt.subplot(3,1,1)\nplt.plot(female, label='female')\nplt.plot(male, label='male')\nplt.legend()","31c3799c":"# NOISE\ndef noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n# STRETCH\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(data, rate)\n# SHIFT\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n# PITCH\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)","b3c1d275":"# Trying different functions above\npath = np.array(RAV_df['path'])[303]\ndata, sample_rate = librosa.load(path)","97f41b55":"# NORMAL AUDIO\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(y=data, sr=sample_rate)\nAudio(path)","661ec8e0":"# AUDIO WITH NOISE\nx = noise(data)\nplt.figure(figsize=(12,5))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","33ef3a25":"# STRETCHED AUDIO\nx = stretch(data)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","14078018":"# SHIFTED AUDIO\nx = shift(data)\nplt.figure(figsize=(12,5))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","2c484072":"# AUDIO WITH PITCH\nx = pitch(data, sample_rate)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","34831ec1":"\n# def feat_ext(data):\n#     mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n#     return mfcc\n\n# def get_feat(path):\n#     data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n#     # normal data\n#     res1 = feat_ext(data)\n#     result = np.array(res1)\n#     #data with noise\n#     noise_data = noise(data)\n#     res2 = feat_ext(noise_data)\n#     result = np.vstack((result, res2))\n#     #data with stretch and pitch\n#     new_data = stretch(data)\n#     data_stretch_pitch = pitch(new_data, sample_rate)\n#     res3 = feat_ext(data_stretch_pitch)\n#     result = np.vstack((result, res3))\n#     return result","15321e87":"#RAV_df.head()","bbe2090e":"# X, Y = [], []\n# for path, emotion in zip(RAV_df['path'], RAV_df['labels']):\n#     feature = get_feat(path)\n#     for ele in feature:\n#         X.append(ele)\n#         Y.append(emotion)","dae574e0":"# Features = pd.DataFrame(X)\n# Features['labels'] = Y\n# Features.to_csv('features.csv', index=False)\n# Features.head()","06f40805":"# this step can used directly from saved feature .csv file\nFeatures = pd.read_csv('..\/input\/features\/features.csv')\nFeatures.head()","7ad06699":"X = Features.iloc[: ,:-1].values\nY = Features['labels'].values","2731ac94":"# As this is a multiclass classification problem onehotencoding our Y.\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()","982cf7d5":"# splitting data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","4ded6e8f":"# scaling our data with sklearn's Standard scaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","9ec4f86c":"x_traincnn =np.expand_dims(x_train, axis=2)\nx_testcnn= np.expand_dims(x_test, axis=2)\nx_traincnn.shape, y_train.shape, x_testcnn.shape, y_test.shape","c7a16d2d":"model = Sequential()\nmodel.add(Conv1D(2048, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\nmodel.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel.add(BatchNormalization())\n\nmodel.add(LSTM(256, return_sequences=True))\n\nmodel.add(LSTM(128))\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(16, activation='softmax'))\n\noptimiser = keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=optimiser,\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\nmodel.summary()","4cdfac6d":"history = model.fit(x_traincnn, y_train, batch_size=64, epochs=200, validation_data=(x_testcnn, y_test))","7f074de0":"# computing the accuracy\nprint(\"Accuracy of our model on test data : \" , model.evaluate(x_testcnn,y_test)[1]*100 , \"%\")","d28a79ca":"# training and test accuracy vs epochs plot\nepochs = [i for i in range(200)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()","5c612f0a":"\npred_test = model.predict(x_testcnn)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)\n\ndf = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","7120af83":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","941cd936":"model_name = 'Emotion_Voice_Detection_Model.h5'\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\n# Save model and weights\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)","9dcdc344":"import json\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)","b72c67ad":"from keras.models import model_from_json\njson_file = open('model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"\/kaggle\/working\/saved_models\/Emotion_Voice_Detection_Model.h5\")\nprint(\"Loaded model from disk\")","7dc533ab":"data, sampling_rate = librosa.load('..\/input\/output\/output10.wav')","bc99bb5f":"plt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)","8add55c0":"#livedf= pd.DataFrame(columns=['feature'])\nX, sample_rate = librosa.load('..\/input\/output\/output10.wav', res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\nsample_rate = np.array(sample_rate)\nmfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\nfeaturelive = mfccs\nlivedf2 = featurelive","e2c4ace8":"livedf2= pd.DataFrame(data=livedf2)\nlivedf2 = livedf2.stack().to_frame().T\nlivedf2","2acc3e92":"twodim= np.expand_dims(livedf2, axis=2)","9752019f":"livepreds = loaded_model.predict(twodim, \n                         batch_size=32, \n                         verbose=1)","b4e2f4da":"livepreds","8e8866cd":"livepreds.shape","33166a90":"livepredictions = (encoder.inverse_transform((livepreds)))\nprint(livepredictions)","16278cf1":"livepredictions=livepredictions.tolist()","bf1f6d3f":"string_version = \" \".join(str(x) for x in livepredictions)\nprint(string_version)","3f0b977c":"# the emotion detected previously from live demo is stored in 'emotion' variable which is then used further in the code\nemotion=string_version.split(\"'\")\nemotion=emotion[1].split(\"_\")\ngender=emotion[0]\nemotion=emotion[1]","36c2014c":"pip install bs4","b8697838":"from bs4 import BeautifulSoup as SOUP\nimport re\nimport requests as HTTP","3bb43f64":"male_genre=[\"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=history,war&colors=color\",\n           \"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=adventure,mystery&colors=color\",\n           \"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=drama&colors=color\",\n           \"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=biography,documentary&colors=color\",\n           \"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=action,comedy,crime&colors=color\",\n           \"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=action,thriller&colors=color\",\n           \"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=family&colors=color\",\n           \"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=horror&colors=color\"]","8fdf74a8":"female_genre=[\"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=adventure&colors=color\",\n             \"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=adventure,comedy,romance&colors=color\",\n             \"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=drama,fantasy&colors=color\",\n             \"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=music,musical&colors=color\",\n             \"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=action,comedy,crime&colors=color\",\n             \"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=action,thriller&colors=color\",\n             \"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=family,game-show&colors=color\",\n             \"https:\/\/www.imdb.com\/search\/title\/?title_type=feature,tv_movie,tv_series,tv_episode,tv_special,tv_miniseries,documentary,short,tv_short&release_date=2016-01-01,&user_rating=7.5,10.0&genres=horror&colors=color\"]","e2189f30":"if(gender == \"male\"):    \n    # IMDb Url for movie against emotion Neutral\n    if(emotion == \"neutral\"):\n        urlhere = male_genre[0]\n  \n    # IMDb Url for movie against emotion Happy\n    elif(emotion == \"happy\"):\n        urlhere = male_genre[1]\n  \n    # IMDb Url for movie against emotion Sad\n    elif(emotion == \"sad\"):\n        urlhere = male_genre[2]\n  \n    # IMDb Url for movie against emotion Calm\n    elif(emotion == \"calm\"):\n        urlhere = male_genre[3]\n  \n    # IMDb Url for movie against emotion Surprised\n    elif(emotion == \"surprised\"):\n        urlhere = male_genre[4]\n\n    # IMDb Url for movie against emotion Angry\n    elif(emotion == \"angry\"):\n        urlhere = male_genre[5]\n\n    # IMDb Url for  movie against emotion Fearful\n    elif(emotion == \"fear\"):\n        urlhere = male_genre[6]\n\n    # IMDb Url for movie against emotion Disgust\n    elif(emotion == \"disgust\"):\n        urlhere = male_genre[7]\n        \nelif(gender==\"female\"):\n    if(emotion == \"neutral\"):\n        urlhere = female_genre[0]\n  \n    # IMDb Url for movie against emotion Happy\n    elif(emotion == \"happy\"):\n        urlhere = female_genre[1]\n  \n    # IMDb Url for movie against emotion Sad\n    elif(emotion == \"sad\"):\n        urlhere = female_genre[2]\n  \n    # IMDb Url for movie against emotion Calm\n    elif(emotion == \"calm\"):\n        urlhere = female_genre[3]\n  \n    # IMDb Url for movie against emotion Surprised\n    elif(emotion == \"surprised\"):\n        urlhere = female_genre[4]\n\n    # IMDb Url for movie against emotion Angry\n    elif(emotion == \"angry\"):\n        urlhere = female_genre[5]\n\n    # IMDb Url for movie against emotion Fearful\n    elif(emotion == \"fear\"):\n        urlhere = female_genre[6]\n\n    # IMDb Url for movie against emotion Disgust\n    elif(emotion == \"disgust\"):\n        urlhere = female_genre[7]\n        ","c4b77caa":"response = HTTP.get(urlhere)\ndata = response.text\nsoup = SOUP(data, \"lxml\")","25e9aaf4":"vbLf='\\n'\nTITLES = []\ntitles = soup.find_all('h3')\nfor t in titles:\n      TITLES.append(t.text[4:].replace(vbLf,\"\"))  ","a25a78b4":"URL_list = []\nfor item in soup.find_all(attrs={'class':'lister-item-header'}):\n    for link in item.find_all('a',href=True):\n        href=link.get('href')\n        URL_list.append(\"https:\/\/www.imdb.com\/\"+href+\"?ref_=adv_li_tt\")","b00eda22":"TITLES","21b1da10":"RATINGS = []\nratings =soup.find_all(\"div\",{'class':\"inline-block ratings-imdb-rating\"})\nfor r in ratings:\n    RATINGS.append(r.text.replace(vbLf,\"\"))\nRATINGS","123804e3":"# created the dataframe to store the scrapped information\ndata = pd.DataFrame(zip(TITLES, RATINGS, URL_list), columns = [\"Title\", \"Ratings\", \"URL\"])","fd1d2f8a":"data","3fe20460":"# DATA AUGMENTATION\n\n* Data augmentation is the process by which we create new synthetic data samples by adding small perturbations on our initial training set.\n* To generate syntactic data for audio, we can apply noise injection, shifting time, changing pitch and speed.\n* The objective is to make our model invariant to those perturbations and enhace its ability to generalize.\nIn order to this to work adding the perturbations must conserve the same label as the original training sample.","e294b1dc":"**PROBLEM STATEMENT**\n\nThe purpose is to recognize the emotion and affective state of the speaker from his\/her speech signal. ","4b0040fd":"# PROJECT NAME: EMOTION BASED MOVIE RECOMMENDATION SYSTEM","ab7aa909":"# COMPONENT 2: MOVIE RECOMMENDATION SYSTEM\n","c23c4538":"For the same sentence being uttered, there is a clear distint difference between male and female in that females tends to have a higher pitch.","be02d54c":"For the happy track it actually felt like it was a fearful tone at the start, up until the end. We had to play it 3 or 4 times to finally be convienced that it is indeed a happy sound. Looking at the wave plot between the 2 files, we noticed the only difference is the amplitute wherein this happy track has a higher amplituted at various points.\n\n\nNext, we compare the MFCC feature for male and female angry audio clips","ee3fd265":"### SCRAPPING THE RATINGS OF EACH MOVIE","ed2ce682":"### SCRAPING THE URLs CORRESPONDING TO EACH MOVIE","3e7dc633":"# **TEAM DNCE**","d3e1fe95":"# IMPLOYING CNN MODEL","c7de8114":"### PREDICTION","6813832f":"We have conducted a survey and obtained following results for male and female preference <br>\n![image.png](attachment:4a788ef3-b887-4041-b199-fd5d9ab5344e.png)","79501baf":"Next, we compare the waveplots of happy and fearful tracks","1703214d":"TEAM LEAD: ANUVA GOYAL <br>\nEMAIL ID: anuvagoyal111@gmail.com\n\nTEAM MEMBER: KRITIKA<br>\nEMAIL ID: kritika4142@gmail.com\n\nTEAM MEMBER: SHIVANI GUPTA<br>\nEMAIL ID: shivanigupta103131@gmail.com \n","591e201c":"# INTRODUCTION","1d0d64c0":"# **COMPONENT 1: SPEECH EMOTION RECOGNITION**\n\n\n**TABLE OF CONTENTS**\n* INTRODUCTION\n* EXPLORATORY DATA ANALYSIS (EDA)\n* DATA AUGMENTATION\n* FEATURE EXTRACTION\n* MODEL","d1bfebda":"# EXPLORATORY DATA ANALYSIS\n\nThe key features of the audio data are namely, MFCC (Mel Frequency Cepstral Coefficients), Mel Spectrogram and Chroma.\n\n* MFCC (Mel Frequency Cepstral Coefficients)- \nMFCC is taken on a Mel scale which is a scale that relates the perceived frequency of a tone to the actual measured frequency. It scales the frequency in order to match more closely what the human ear can hear. The envelope of the temporal power spectrum of the speech signal is representative of the vocal tract and MFCC accurately represents this envelope.\n\n\n* Mel Spectrogram- \nA Fast Fourier Transform is computed on overlapping windowed segments of the signal, and we get what is called the spectrogram. This is just a spectrogram that depicts amplitude which is mapped on a Mel scale.\n\n* Chroma- \nA Chroma vector is typically a 12-element feature vector indicating how much energy of each pitch class is present in the signal in a standard chromatic scale.\n\nFor the EDA we have used MFCC and Mel Spectogram","ba99b866":"# FEATURE EXTRACTION","160d3017":"Now, we will compare mel spectograms of male and female neutral audio clips.","e5ad0b39":"# LIVE DEMO","b4ba3470":"### SCRAPING THE TITLE OF THE MOVIE FROM THE IMBb WEBSITE","970ab37b":"# DATA PREPROCESSING","bf0729ca":"### LIST OF LINKS FOR DIFFERENT GENRES","4e13246f":"### IMPORTING LIBRARIES","0cc77e57":"After listening to all augmented audio, it's decided to use noise, stretch and pitch for augmenting data.","4804c654":"## SAVING THE MODEL","52132584":"**DATA SOURCE USED**\n\nWe have used the RAVDESS dataset in this project.It is one of the more common dataset used for this excercise by others. It's well liked because of its quality of speakers, recording and it has 24 actors of different genders.\nHere's the filename identifiers as per the official RAVDESS website:\n\n* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n* Vocal channel (01 = speech, 02 = song).\n* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n\nHere's an example of an audio filename. 02-01-06-01-02-01-12.mp4"}}