{"cell_type":{"47771a9c":"code","a4733243":"code","cc940d5b":"code","c1c38fc1":"code","8644491c":"code","c00f95e1":"code","229c9f92":"code","8e60c282":"code","47144dd7":"code","923d911f":"code","e63de3ea":"code","ec2889fd":"code","0b11001b":"code","6291202b":"code","774f3966":"code","9bac9232":"code","20b27dbd":"code","b1eb0dae":"code","453f13b0":"code","07bfec2d":"code","1084c9d6":"code","9a495cfa":"code","f0ab9029":"code","ced8b669":"code","341837f6":"code","de04373d":"code","1092e47a":"code","c75f934a":"markdown","7dac938f":"markdown","2247406d":"markdown","e12fd2c8":"markdown","bc482b09":"markdown","f7aa869e":"markdown","13037bcc":"markdown","e75eb39e":"markdown","20154723":"markdown","5b9caa3b":"markdown","30816d64":"markdown","61fa4827":"markdown","0203f3cf":"markdown","a936537e":"markdown","7ce249ab":"markdown","cf95c095":"markdown","183ee184":"markdown","edfee646":"markdown","09d8a714":"markdown"},"source":{"47771a9c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a4733243":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","cc940d5b":"data = pd.read_csv('\/kaggle\/input\/pokemon\/pokemon.csv')\ndata.head()","c1c38fc1":"data.shape","8644491c":"data.info()","c00f95e1":"# check number of null values in each column\ndata.isna().sum()[data.isna().sum() != 0]","229c9f92":"sns.displot(data['height_m'], kde=True)","8e60c282":"sns.displot(data['percentage_male'], kde=True)","47144dd7":"sns.displot(data['weight_kg'], kde=True)","923d911f":"data['height_m'].fillna(value=data['height_m'].median(), inplace=True)\ndata['percentage_male'].fillna(value=data['percentage_male'].median(), inplace=True)\ndata['weight_kg'].fillna(value=data['weight_kg'].median(), inplace=True)","e63de3ea":"# check number of null values in each column\ndata.isna().sum()[data.isna().sum() != 0]","ec2889fd":"data.isna().sum()[data.isna().sum() != 0] \/ len(data)","0b11001b":"data.drop('type2',axis=1, inplace = True)","6291202b":"sns.histplot(data['is_legendary'])","774f3966":"for label,content in data.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        data[label] = data[label].astype('category')","9bac9232":"for label,content in data.items():\n    if pd.api.types.is_categorical_dtype(content):\n        data[label] = pd.Categorical(content).codes + 1","20b27dbd":"data.head()","b1eb0dae":"data.loc[data.is_legendary == 1][:20]","453f13b0":"y = data['is_legendary'] # store the target variable value.\nX = data.drop('is_legendary',axis=1)","07bfec2d":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)","1084c9d6":"from sklearn.metrics.pairwise import linear_kernel \n\ncosine_similarities = linear_kernel(data, data) # Measure Cosine Similarity between elements","9a495cfa":"cosine_similarities","f0ab9029":"# map pokemon indecies\nmapping = pd.Series(data.index,index = data['name'])","ced8b669":"def recommend_pokemon(name):\n    poke_index = mapping[name]\n    \n    #get similarity values with other pokemons\n    #similarity_score is the list of index and similarity matrix\n    similarity_score = list(enumerate(cosine_similarities[name]))\n    # print(similarity_score)\n    #sort in descending order the similarity score of pokemon inputted with all the other pokemons\n    similarity_score = sorted(similarity_score, key=lambda x: x[1], reverse=True)\n    \n    # Get the scores of the 10 most similar pokemons.\n    similarity_score = similarity_score[1:10] # first pokemon is the most similar because it is the same pokemon we entered almost\n    \n    #return pokemon names using the mapping series\n    pokemon_indices = [i[0] for i in similarity_score]\n    return (data['name'].iloc[pokemon_indices])","341837f6":"recommend_pokemon(793)","de04373d":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)\nrf.score(X_test,y_test)","1092e47a":"## TO DO\n# check the classification score of the RF Model\n# may use f1 score\n# check better tree paratemters using randomsearch or gridsearch","c75f934a":"#### Impute the three numerical variables with median","7dac938f":"## Content Based vs Collabortaive Filtering","2247406d":"# Label Encoding","e12fd2c8":"our three numerical variables are skewed.","bc482b09":"#### Collaborative Filtering","f7aa869e":"# Importing Data","13037bcc":"#### Content-Based","e75eb39e":"## Content-Based","20154723":"We have 4 columns that have nulls, lets handel them and see how to impute these nulls.","5b9caa3b":"Now lets create a function that takes an id and see whether it is a legendary or not","30816d64":"# Target Variable Analysis","61fa4827":"Collaborative filtering uses the combined power of ratings provided by many users\/customers to present recommendations.\n1. Memory-based methods \n    - User-based collaborative filtering\n    - Item-based collaborative filtering\n2. Model-based methods \n    - machine learning methods to extract predictions for rating data by treating the problem as a normal machine learning problem. (NN, PCA and Clustering).\n\n**Disadvantages**\n* Cold-start for new users\n* New-item problem","0203f3cf":"*To do*\n- similar names may indicate something related to legendary","a936537e":"* In our problem, we need to know which pokemon is legendary, we can know such an information by comparing the features of every pokemon and recommend the similar ones legendaries to be legendaries.  \n* To achieve this, we will use content-based appraoch on numerical variables","7ce249ab":"type two column has many null values, so we may drop it.","cf95c095":"# Modeling","183ee184":"## Random Forest","edfee646":"# Split the data","09d8a714":"* Content-based filtering involves recommending items based on the attributes of the items themselves. \n* Recommendations made by content-based filters use an individual\u2019s historical information to inform choices displayed. \n* Such recommenders look for similarities between the items to recommend options in the future.  \n\n\n**Disadvantages**\n* The model does not learn from transactions. It will recommend items similar to those already consumed\n* There isn\u2019t much improvement in the performance of content-based systems over time"}}