{"cell_type":{"5dbed60f":"code","e8452bb4":"code","7101c21d":"code","7baf5c9b":"code","e0a83b12":"code","377f8352":"code","7c27976f":"code","cfd3da7d":"code","3797a891":"markdown","5678ecb5":"markdown","114de1d8":"markdown","a916deba":"markdown","9a9c9ca8":"markdown","5388de71":"markdown","a7b0fdb4":"markdown","a85a9369":"markdown","2621e86a":"markdown"},"source":{"5dbed60f":"#import libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.ensemble import RandomForestClassifier","e8452bb4":"#import dataset \ndata = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndata.info()","7101c21d":"data.drop(['Unnamed: 32','id'], axis = 1 , inplace=True)","7baf5c9b":"#get correlations of each features in dataset\ncorr_matrix = data.corr()\nfeatures_corr = corr_matrix.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(data[features_corr].corr(),annot=True,cmap=\"RdYlGn\")","e0a83b12":"X = data.loc[:, data.columns != 'diagnosis']\ny = data['diagnosis']\n#apply SelectKBest class to extract top 10 best features\nkbest = SelectKBest(score_func=chi2, k=10)\nfit = kbest.fit(X,y)\ndf_scores = pd.DataFrame(fit.scores_)\ndf_columns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfScores = pd.concat([df_columns,df_scores],axis=1)\nfScores.columns = ['Feature','Score']  #naming the dataframe columns\nprint(fScores.nlargest(10,'Score')) \n#print 10 best features","377f8352":"indices = np.argsort(kbest.scores_)[::-1]\n\nfeatures = []\nfor i in range(10):\n    features.append(X.columns[indices[i]])\n\nfig, ax = plt.subplots(figsize=(16,6))     \nsns.barplot(x=features, y=kbest.scores_[indices[range(10)]],\\\n            label=\"Importtant Categorical Features\", palette=(\"rocket\"),ax=ax).set_title('Top 10 Features Selected with Chi-square')\nax.set(xlabel=\"Feature\", ylabel = \"Importance\")","7c27976f":"#Information Gain\n\n#the model possesses tuned hyperparameter\nmodel = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n            max_depth=None, max_features=10, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=20, min_samples_split=20,\n            min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=None,\n            oob_score=False, random_state=42, verbose=0,\n            warm_start=False)\nmodel.fit(X,y)\n#print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nimp_feat = feat_importances.nlargest(10)\nprint(\" Feature                  Score\")\nprint(imp_feat)\n\n# determine 20 most important features\nimp_feat = feat_importances.nlargest(10)\nplt.title(f'Importance for the Top 10 Features (Information Gain) ', fontweight='bold')\nimp_feat.plot(kind='barh')\nplt.show()","cfd3da7d":"#Gini Index\n\nmodel = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                               max_features=10, min_samples_leaf=20,\n                               min_samples_split=20, n_estimators=600, random_state= None)\nmodel.fit(X,y)\n\n\nfeature_importance = {}\nbest_estimator_fi = model.feature_importances_\n\nfor feature, importance in zip(X.columns, best_estimator_fi):\n    feature_importance[feature] = importance\n\nf_importances = pd.DataFrame.from_dict(feature_importance, orient='index').rename(columns={0: 'Gini Score'})\nimportances = f_importances.sort_values(by='Gini Score', ascending=False)\n\n\n# Plot for feature importance\nplt.figure(figsize=(20, 8))\nplt.style.use('fivethirtyeight')\nsns.set_style(\"white\")\nsns.barplot(x=importances.index[0:10], y=importances['Gini Score'].iloc[0:10], palette='muted')\nplt.title(f'Importance for the Top 10 Features (Gini Index) ', fontweight='bold')\nplt.grid(True, alpha=0.1, color='black')\nplt.show()","3797a891":"TCB. . . . .","5678ecb5":"**0. Feature Correlation Matrix with Heatmap Method**\n\nThis is the most common and efficient way to track down the important features in a dataset. Correlation states how the features are related to each other or the target variable. Correlation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)\n\nWith visual representation, Heatmap makes it easy to identify which features are most related to the target variable, here the heatmap of correlated features is ploted using the seaborn library.","114de1d8":"# A Concise overview of Feature Selection Techniques (Part 1)","a916deba":"Top reasons to use feature selection are: It enables the machine learning algorithm to train faster. It reduces the complexity of a model and makes it easier to interpret. It improves the accuracy of a model if the right subset is chosen. [3]\n**Feature Selection is one of the core concepts in Data Science & Machine Learning which have hugely impact the performance of a model**. The data features that are used to train machine learning models have a huge influence on the performance.\nThe problem we often face is to identify the related features from a set of data and removing the irrelevant or less important features with do not contribute much to our target variable in order to achieve better accuracy for our model. [1]\n\nIn this notebook, I shed some light on some useful feature selection techniques simply you can use in Machine Learning. I will be using Python programming language in this case with sklearn.\n\nFeature Selection Methods\n\nHere, three feature selection techniques that are easy to use and also gives good results will be discussed.\n\n0. Feature Correlation Matrix [.corr()]\n1. Univariate Selection [SelectKBest with chi\u00b2]\n2. Feature Importance [.feature_importances_]\n\nLet\u2019s explore these techniques one by one with an example dataset, \"Breast Cancer Risk Prediction\". You can download the dataset from [HERE](http:\/\/https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data). The dataset contains 32 features, but here I will try to select top 10 important features from the dataset.","9a9c9ca8":"**1. Univariate Selection Method**\n\nStatistical tests can be used to select those features that have the strongest relationship with the target variable. The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features. The example below uses the Chi-squared (chi\u00b2) statistical test for non-negative features to select 10 of the best features from the Breast Cancer Risk Prediction dataset.","5388de71":"**2. Feature Importance Method**\n\nThe feature importance of each feature from the dataset can be estimated by using the feature importance property of the model. Feature importance gives a score for each feature of the data, the higher the score more important or relevant is the feature towards the target variable.\n\nFeature importance is an inbuilt class that comes with Tree Based Classifiers, here Random Forest Classifier is used for extracting the top 10 features form the dataset. We will be calculating the **Information Gain** and **Gini Index** to select the most important features from the dataset.","a7b0fdb4":"Ref. \n1. https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e?gi=d00ef7465ca2\n2. https:\/\/medium.com\/@nmscott14\/3-feature-selection-methods-e7ccd6dbf316\n3. https:\/\/www.analyticsvidhya.com\/blog\/2016\/12\/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables\/","a85a9369":"On this Notebook I will discuss some of the traditional Filter methods for feature selection. In the next part I will try to cover the wrapper and embedded methods.","2621e86a":"In this notebook, it is discussed how to select relevant features from data using Correlation matrix, Univariate selection and Feature importance technique. Previous studies showed that generating a heatmap for the correlation method is very useful as making decisions about which features to ultimately include in the new dataset.\n\nIf you find this discussion insighful, let me know, I will keep sharing stuff like this. \n@ThankYou"}}