{"cell_type":{"215429ef":"code","8053ddf8":"code","e78f2299":"code","aebfb5dc":"code","e8b92ee9":"code","9d8457df":"code","d46f0e26":"code","275a42c3":"code","02c79ccd":"code","f34d2507":"code","76406318":"code","23d41678":"code","f6bf263e":"code","acce3861":"code","64ce9abf":"code","d55ae8c9":"code","e3805700":"code","d5406b32":"code","cb6486a7":"code","2ad246b5":"code","91220f8f":"code","eaa294fd":"code","26c21ceb":"code","e3f94e9b":"code","c993f657":"code","5eadb5f9":"code","d8b40151":"code","59e0fc1c":"code","0af1ddf4":"markdown","e1994ed8":"markdown","8beb77f4":"markdown","b8b777d6":"markdown","510ada2e":"markdown","23b934bd":"markdown","c571370a":"markdown","154e745a":"markdown","dae09311":"markdown","02b5455a":"markdown","397da91c":"markdown","0320eaad":"markdown","7283a916":"markdown","9f7f3ca3":"markdown","fe83f703":"markdown","660f1436":"markdown","974dc69d":"markdown","a57c1766":"markdown"},"source":{"215429ef":"import os, sys\nimport itertools, time\nimport numpy as np \nimport pandas as pd\n\n# preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom pandas_profiling import ProfileReport\n\n# postprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, explained_variance_score, mean_squared_log_error\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","8053ddf8":"test = pd.read_csv('..\/input\/test.csv')\ntest.set_index('Id', inplace=True, drop=True)\ndata = pd.read_csv('..\/input\/train.csv')\ndata.set_index('Id', inplace=True, drop=True)\n\ny = data[['SalePrice']]\nX = data.drop('SalePrice', axis=1)","e78f2299":"ProfileReport(data)","aebfb5dc":"numerical = X._get_numeric_data().columns\ncategorical = X.columns.drop(numerical)","e8b92ee9":"# Transformations must be applied to both training and testing set.\nXtot = pd.concat((X, test))\nlen(X), len(test), len(Xtot)","9d8457df":"# Empty dataframe for building features\nX_eng = pd.DataFrame()","d46f0e26":"# Find skewed features\nfrom scipy.stats import skew\n\nskewed_feats = Xtot[numerical].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index","275a42c3":"for header in numerical:\n    # Set nan values to mean\n    column = Xtot[header].copy()\n    colmean = np.mean(column)\n    colnan = np.isnan(column)\n    column[colnan] = colmean\n    \n    # Take log1p of skewed features\n    if header in skewed_feats:\n        column = np.log1p(column)\n        header = header+'_log1p'\n    \n    # Feature scaling\n    colstd = np.std(column)\n    column = (column - colmean)\/colstd\n    \n    # Set column in engineered dataframe\n    X_eng[header] = column","02c79ccd":"def ohe_cols(column, label='', index='index'):\n    \n    # Combine rare values (values with counts less than threshold)\n    threshold = 0.025\n    unique_values = column.unique()\n    for value in unique_values:\n        if (np.sum(column==value)\/len(column))<threshold:\n            column[column==value] = 'rare'\n\n    # Encode values into integers\n    label_encoder = LabelEncoder()\n    try:integer_encoded = label_encoder.fit_transform(column)\n    except TypeError:\n        integer_encoded = label_encoder.fit_transform(column.astype(str))\n    headers = label_encoder.classes_.astype(str)\n    \n    if len(headers)>2:\n        # Encode integers into onehot\n        onehot_encoder = OneHotEncoder(sparse=False)\n        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n        onehot_encoded\n\n        headers = [str(label)+\"_\"+x for x in headers]\n    \n        df = pd.DataFrame(onehot_encoded, columns=headers, index=column.index)\n        \n    else:\n        df = pd.DataFrame(integer_encoded, columns=[str(label)+'_binary'], index=column.index)\n    \n    return df","f34d2507":"for header in categorical:\n    # One hot encode data\n    column  = Xtot[header].copy()\n    ohe_column = ohe_cols(column, label=header)\n    \n    # Merge data into dataframe\n    X_eng = pd.merge(X_eng, ohe_column, left_index=True, right_index=True)","76406318":"log1p_y = np.log1p(y)","23d41678":"# Recover input data and test data\nXinput = X_eng[:len(X)].copy()\ntest = X_eng[len(X):].copy()\n\n# Train test split - 20% testing\nXtrain, Xtest, ytrain, ytest = train_test_split(Xinput, y, test_size=0.2, random_state=15)\n\n# Train cross-validation split - overall 20% cross-validation\nXtrain, Xcv, ytrain, ycv = train_test_split(Xtrain, ytrain, test_size=(0.2\/0.8), random_state=15)\n\n# So we have a 60-20-20 train-cv-test split\nlen(Xtrain), len(Xcv), len(Xtest), len(test)","f6bf263e":"# machine learning\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.svm import SVR, LinearSVR, NuSVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor","acce3861":"models = {'svm-SVR': SVR,\n          'svm-LinearSVR': LinearSVR, \n          'svm-NuSVR': NuSVR,\n          'RandomForest': RandomForestRegressor,\n          'KNeighbors': KNeighborsRegressor,\n          'DecisionTree': DecisionTreeRegressor, \n          'ExtraTree': ExtraTreeRegressor,\n          'GaussianProcess': GaussianProcessRegressor}","64ce9abf":"def RUN(X_train, y_train, X_test, y_test, model, cm=False, label='', target=''):\n\n    \"\"\"\n    Fits model to X_train and y_train\n    Predicts targets for X_test\n    Provides metrics for prediction success of y_test\n    \"\"\"\n    \n    start = time.time()\n    \n    print(label)\n    \n    model.fit(X_train, np.array(y_train[target]))\n    \n    pred = model.predict(X_test)\n    y_test = np.array(y_test[target])\n    \n    now = time.time()\n    timetaken = now-start\n    \n    acc = explained_variance_score(y_test, pred)\n    error = np.sqrt(mean_squared_log_error(y_test, pred))\n    print(\"Accuracy: %f, Error: %f, time: %.3f\" % (acc, error, timetaken))\n    \n    if cm:\n        cm = confusion_matrix(y_test, pred)\n        plot_confusion_matrix(cm, np.arange(1), normalize=True)\n        ","d55ae8c9":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","e3805700":"for value in models:\n    _=RUN(Xtrain, np.log1p(ytrain), Xcv, np.log1p(ycv), models[value](), label=value, target='SalePrice')","d5406b32":"# Use random forest to find the most important features\n\nforest = RandomForestRegressor()\n\nforest.fit(Xtrain, np.log1p(np.array(ytrain)[:,0]))\n    \npred = forest.predict(Xcv)\ny_test = np.array(np.log1p(ycv.SalePrice))\n    \nacc = explained_variance_score(y_test, pred)\nerror = np.sqrt(mean_squared_log_error(y_test, pred))\nprint(\"Accuracy: %f, Error: %f\" % (acc, error))\n    \nimportances = forest.feature_importances_\n\nfeatureimportance = np.vstack((Xtrain.columns.values, importances))\nfeatureimportance = pd.DataFrame(featureimportance.T, columns=['Feature', 'Importance'])\n\nplt.figure(figsize=(20,10))\n_=plt.bar(Xtrain.columns.values, importances)\n_=plt.xticks(rotation=45, fontsize=10)","cb6486a7":"featureimportance.sort_values('Importance', ascending=False)","2ad246b5":"clf = NuSVR()\n\nclf.fit(Xtrain, np.log1p(np.array(ytrain)[:,0]))\n    \npred = clf.predict(Xcv)\ny_test = np.log1p(np.array(ycv.SalePrice))\n    \nacc = explained_variance_score(y_test, pred)\nerror = np.sqrt(mean_squared_log_error(y_test, pred))\n\nerror = np.sqrt(mean_squared_log_error(np.exp(y_test)-1, np.exp(pred)-1))\nprint(\"Accuracy: %f, Error: %f\" % (acc, error))","91220f8f":"kernels = ['linear', 'poly', 'rbf', 'sigmoid']\nfor i in np.linspace(0.001, 0.1, 10):\n    clf = NuSVR(nu=0.5, C=i, kernel='linear')\n\n    clf.fit(Xtrain, np.log1p(np.array(ytrain)[:,0]))\n\n    pred = clf.predict(Xcv)\n    y_test = np.log1p(np.array(ycv.SalePrice))\n\n    acc = explained_variance_score(y_test, pred)\n    error = np.sqrt(mean_squared_log_error(np.exp(y_test)-1, np.exp(pred)-1))\n    print(\"Accuracy: %f, Error: %f, Parameter: %s\" % (acc, error, i))","eaa294fd":"Xnew = Xtrain.copy()\nXcv_new = Xcv.copy()\nXtest_new = Xtest.copy()\ntest_new = test.copy()\n\nthreshold = 0.005\nmin_importance = 0.\n\nwhile min_importance<threshold:\n\n    forest = RandomForestRegressor()\n    forest.fit(Xnew, np.array(ytrain)[:,0])\n\n    pred = forest.predict(Xcv_new)\n    y_test = np.array(ycv.SalePrice)\n\n    acc = explained_variance_score(y_test, pred)\n    error = np.sqrt(mean_squared_log_error(y_test, pred))\n    print(\"Accuracy: %f, Error: %f\" % (acc, error))\n\n    importances = forest.feature_importances_\n\n    min_importance = np.min(importances)\n    \n    if min_importance<threshold:\n        col = Xnew.columns.values[importances == min_importance][0]\n        print(col)\n        \n        Xnew.drop(col, inplace=True, axis=1)\n        Xcv_new.drop(col, inplace=True, axis=1)\n        Xtest_new.drop(col, inplace=True, axis=1)\n        test_new.drop(col, inplace=True, axis=1)","26c21ceb":"forest = RandomForestRegressor()\n\nforest.fit(Xnew, np.array(ytrain)[:,0])\n    \npred = forest.predict(Xcv_new)\ny_test = np.array(ycv.SalePrice)\n    \nacc = explained_variance_score(ycv, pred)\nerror = np.sqrt(mean_squared_log_error(ycv, pred))\nprint(\"Accuracy: %f, Error: %f\" % (acc, error))\n    \nimportances = forest.feature_importances_\n\nplt.figure(figsize=(20,10))\n_=plt.bar(Xnew.columns.values, importances)\n_=plt.xticks(rotation=45, fontsize=15)","e3f94e9b":"for value in models:\n    _=RUN(Xnew, ytrain, Xcv_new, ycv, models[value](), label=value, target='SalePrice')","c993f657":"# Running the Regressor on the test sample tells us what we expect the error\/accuracy to be.\n\nclf = RandomForestRegressor()\n\nclf.fit(Xtrain, np.array(ytrain)[:,0])\n    \npred = clf.predict(Xtest)\ny_test = np.array(ytest.SalePrice)\n    \nacc = explained_variance_score(y_test, pred)\nerror = np.sqrt(mean_squared_log_error(y_test, pred))\nprint(\"Accuracy: %f, Error: %f\" % (acc, error))","5eadb5f9":"pred = clf.predict(test)\npi = test.index.values.astype(int)\n\nprediction = pd.DataFrame(np.vstack((pi, pred)).T, columns=['Id', 'SalePrice'])\nprediction.Id = prediction.Id.astype(int)\nprediction","d8b40151":"clf = NuSVR(nu=0.5, C=0.01, kernel='linear')\n\nclf.fit(Xtrain, np.log1p(np.array(ytrain)[:,0]))\n\npred = clf.predict(Xtest)\ny_test = np.log1p(np.array(ytest.SalePrice))\n\nacc = explained_variance_score(np.exp(y_test)-1, np.exp(pred)-1)\nerror = np.sqrt(mean_squared_log_error(np.exp(y_test)-1, np.exp(pred)-1))\nprint(\"Accuracy: %f, Error: %f, Parameter: %s\" % (acc, error, i))","59e0fc1c":"pred = clf.predict(test)\npred = np.exp(pred)-1\npi = test.index.values\nprediction = pd.DataFrame(np.vstack((pi, pred)).T, columns=['Id', 'SalePrice'])\nprediction.Id = prediction.Id.astype(int)\nprediction","0af1ddf4":"## Random Forest","e1994ed8":"# Optimise parameters for NuSVR which provide best performance on cross-validation set\n* 'Error' root mean square logarithmic error on SalePrice (not log SalePrice now)\n* Optimal parameters: nu=0.5, C=0.01, kernel='linear'","8beb77f4":"---\n<a name=\"data\"><\/a>\n# 1) Analyse data","b8b777d6":"## Numerical\n* Include all numerical columns in dataframe\n* Replace missing values with mean\n* Feature scaling\n* Take log of all values to manage skewness","510ada2e":"\n*    SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n*    MSSubClass: The building class\n*    MSZoning: The general zoning classification\n*    LotFrontage: Linear feet of street connected to property\n*    LotArea: Lot size in square feet\n*    Street: Type of road access\n*    Alley: Type of alley access\n*    LotShape: General shape of property\n*    LandContour: Flatness of the property\n*    Utilities: Type of utilities available\n*    LotConfig: Lot configuration\n*    LandSlope: Slope of property\n*    Neighborhood: Physical locations within Ames city limits\n*    Condition1: Proximity to main road or railroad\n*    Condition2: Proximity to main road or railroad (if a second is present)\n*    BldgType: Type of dwelling\n*    HouseStyle: Style of dwelling\n*    OverallQual: Overall material and finish quality\n*    OverallCond: Overall condition rating\n*    YearBuilt: Original construction date\n*    YearRemodAdd: Remodel date\n*    RoofStyle: Type of roof\n*    RoofMatl: Roof material\n*    Exterior1st: Exterior covering on house\n*    Exterior2nd: Exterior covering on house (if more than one material)\n*    MasVnrType: Masonry veneer type\n*    MasVnrArea: Masonry veneer area in square feet\n*    ExterQual: Exterior material quality\n*    ExterCond: Present condition of the material on the exterior\n*    Foundation: Type of foundation\n*    BsmtQual: Height of the basement\n*    BsmtCond: General condition of the basement\n*    BsmtExposure: Walkout or garden level basement walls\n*    BsmtFinType1: Quality of basement finished area\n*    BsmtFinSF1: Type 1 finished square feet\n*    BsmtFinType2: Quality of second finished area (if present)\n*    BsmtFinSF2: Type 2 finished square feet\n*    BsmtUnfSF: Unfinished square feet of basement area\n*    TotalBsmtSF: Total square feet of basement area\n*    Heating: Type of heating\n*    HeatingQC: Heating quality and condition\n*    CentralAir: Central air conditioning\n*    Electrical: Electrical system\n*    1stFlrSF: First Floor square feet\n*    2ndFlrSF: Second floor square feet\n*    LowQualFinSF: Low quality finished square feet (all floors)\n*    GrLivArea: Above grade (ground) living area square feet\n*    BsmtFullBath: Basement full bathrooms\n*    BsmtHalfBath: Basement half bathrooms\n*    FullBath: Full bathrooms above grade\n*    HalfBath: Half baths above grade\n*    Bedroom: Number of bedrooms above basement level\n*    Kitchen: Number of kitchens\n*    KitchenQual: Kitchen quality\n*    TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n*    Functional: Home functionality rating\n*    Fireplaces: Number of fireplaces\n*    FireplaceQu: Fireplace quality\n*    GarageType: Garage location\n*    GarageYrBlt: Year garage was built\n*    GarageFinish: Interior finish of the garage\n*    GarageCars: Size of garage in car capacity\n*    GarageArea: Size of garage in square feet\n*    GarageQual: Garage quality\n*    GarageCond: Garage condition\n*    PavedDrive: Paved driveway\n*    WoodDeckSF: Wood deck area in square feet\n*    OpenPorchSF: Open porch area in square feet\n*    EnclosedPorch: Enclosed porch area in square feet\n*    3SsnPorch: Three season porch area in square feet\n*    ScreenPorch: Screen porch area in square feet\n*    PoolArea: Pool area in square feet\n*    PoolQC: Pool quality\n*    Fence: Fence quality\n*    MiscFeature: Miscellaneous feature not covered in other categories\n*    MiscVal: Value of miscellaneous feature\n*    MoSold: Month Sold\n*    YrSold: Year Sold\n*    SaleType: Type of sale\n*    SaleCondition: Condition of sale\n","23b934bd":"# Drop lowest importance features\n* NuSVR handles high dimensionality ok so we don't need to reduce the number of features\n* RandomForest Error doesn't reduce when removing features therefore we continue to calculate based on full feature set.","c571370a":"## Categorical\n* Collect rare values\n* One Hot encode","154e745a":"------\n------\n<a name='models'><\/a>\n# 4) Run models","dae09311":"---\n<a name=\"feature\"><\/a>\n# 2) Feature engineering","02b5455a":"---\n<a name='split'><\/a>\n# 3) Train - CV - Test split","397da91c":"# Test and submission\n* Submission1 - Random Forest - Expect a rms log error of ~0.15\n* Submission2 - NuSVR - Expect a rms log error of ~0.12","0320eaad":"# Workflow\n\n* [Analyse data](#data)\n* [Feature engineering](#feature)\n    * Relevant and new features\n    * Clean given features\n* [Train - CV - test split](#split)\n* [Trial models](#models)","7283a916":"* PassengerId (Numerical, Discrete) - Set as member identifier","9f7f3ca3":"## NuSVR","fe83f703":"# Target\n* log transform the target","660f1436":"# Find most successful models for log1p SalePrice\n* 'Error' is the root mean square log error for log 1p\n* svm-NuSVR generates the lowest error","974dc69d":"# Reference\n\n* Code for finding skewed features and taking np.log1p of skewed features and SalePrice taken from Alexandru Papiu's notebook: https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models","a57c1766":"# HOUSE PRICING COMPETITION"}}