{"cell_type":{"5ee6d339":"code","71dd93ab":"code","c6ffc9b8":"code","5aea22aa":"code","27ee61d7":"code","d64cc582":"code","9e10b513":"code","c11f9597":"code","7b01c5ae":"code","d56e38f6":"code","87194201":"code","0a2002eb":"code","ebc0d4b6":"code","02877b68":"code","b04cd302":"code","dc82cc57":"code","4c67e036":"code","d7c82243":"code","1e081a8e":"code","cb1dedaf":"code","68b95ffd":"code","b5192581":"markdown","3970e9b1":"markdown","163b1eb4":"markdown","452afa1f":"markdown","d0e35fd6":"markdown","4e16ede2":"markdown"},"source":{"5ee6d339":"!pip install efficientnet -q","71dd93ab":"import os\nimport numpy as np\nimport pandas as pd\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nimport efficientnet.tfkeras as efn","c6ffc9b8":"def auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return strategy\n\n\ndef build_decoder(with_labels=True, target_size=(256, 256), ext='jpg'):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        if ext == 'png':\n            img = tf.image.decode_png(file_bytes, channels=3)\n        elif ext in ['jpg', 'jpeg']:\n            img = tf.image.decode_jpeg(file_bytes, channels=3)\n        else:\n            raise ValueError(\"Image extension not supported\")\n\n        img = tf.cast(img, tf.float32) \/ 255.0\n        img = tf.image.resize(img, target_size)\n\n        return img\n    \n    def decode_with_labels(path, label):\n        return decode(path), label\n    \n    return decode_with_labels if with_labels else decode\n\n\ndef build_augmenter(with_labels=True):\n    def augment(img):\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_saturation(img, 0.7, 1.3)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        img = tf.image.random_brightness(img, 0.1)\n        return img\n    \n    def augment_with_labels(img, label):\n        return augment(img), label\n    \n    return augment_with_labels if with_labels else augment\n\n\ndef build_dataset(paths, labels=None, bsize=32, cache=True,\n                  decode_fn=None, augment_fn=None,\n                  augment=True, repeat=True, shuffle=1024, \n                  cache_dir=\"\"):\n    if cache_dir != \"\" and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n    \n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n    \n    if augment_fn is None:\n        augment_fn = build_augmenter(labels is not None)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = paths if labels is None else (paths, labels)\n    \n    dset = tf.data.Dataset.from_tensor_slices(slices)\n    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n    dset = dset.cache(cache_dir) if cache else dset\n    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n    dset = dset.repeat() if repeat else dset\n    dset = dset.shuffle(shuffle) if shuffle else dset\n    dset = dset.batch(bsize).prefetch(AUTO)\n    \n    return dset","5aea22aa":"COMPETITION_NAME = \"ranzcr-clip-catheter-line-classification\"\nGCS_DS_PATH = KaggleDatasets().get_gcs_path(COMPETITION_NAME)\nprint(GCS_DS_PATH)\nstrategy = auto_select_accelerator()\nBATCH_SIZE = strategy.num_replicas_in_sync * 16","27ee61d7":"load_dir = f\"\/kaggle\/input\/{COMPETITION_NAME}\/\"\ndf = pd.read_csv(load_dir + 'train.csv')\n\n# paths = load_dir + \"train\/\" + df['StudyInstanceUID'] + '.jpg'\npaths = GCS_DS_PATH + \"\/train\/\" + df['StudyInstanceUID'] + '.jpg'\n\nsub_df = pd.read_csv(load_dir + 'sample_submission.csv')\n\n# test_paths = load_dir + \"test\/\" + sub_df['StudyInstanceUID'] + '.jpg'\ntest_paths = GCS_DS_PATH + \"\/test\/\" + sub_df['StudyInstanceUID'] + '.jpg'\n\n# Get the multi-labels\nlabel_cols = sub_df.columns[1:]\nlabels = df[label_cols].values","d64cc582":"# Train test split\nrandom_state = 2020\nfold_nums = 0\nskf = KFold(n_splits=5, random_state=random_state, shuffle=True)\nfor fold, (train_id, test_id) in enumerate(skf.split(paths , labels)):\n    if fold == fold_nums:\n        train_paths = paths[train_id]\n        valid_paths = paths[test_id]\n        train_labels = labels[train_id]\n        valid_labels = labels[test_id]","9e10b513":"# Build the tensorflow datasets\nIMSIZE = (224, 240, 260, 300, 380, 456, 528, 600)\nimg_size = IMSIZE[7]\ndecoder = build_decoder(with_labels=True, target_size=(img_size, img_size))\ntest_decoder = build_decoder(with_labels=False, target_size=(img_size, img_size))\n\ntrain_dataset = build_dataset(\n    train_paths, train_labels, bsize=BATCH_SIZE, decode_fn=decoder\n)\n\nvalid_dataset = build_dataset(\n    valid_paths, valid_labels, bsize=BATCH_SIZE, decode_fn=decoder,\n    repeat=False, shuffle=False, augment=False\n)\n\ntest_dataset = build_dataset(\n    test_paths, cache=False, bsize=BATCH_SIZE, decode_fn=test_decoder,\n    repeat=False, shuffle=False, augment=False\n)","c11f9597":"\ncfg = dict(\n    net_count         =  range(6,8),\n    epochs            =  18,\n    label_smooth_fac  =   0.005,   \n    LR_START          =   0.000005,\n    LR_MAX            =   0.000200,\n    LR_MIN            =   0.00001,\n    LR_RAMPUP_EPOCHS  =   5,\n    LR_SUSTAIN_EPOCHS =   0,\n    LR_EXP_DECAY      =   0.8,\n    tta_steps         =   25,\n)","7b01c5ae":"def get_lr_callback(cfg):\n    lr_start   = cfg['LR_START']\n    lr_max     = cfg['LR_MAX'] * strategy.num_replicas_in_sync\n    lr_min     = cfg['LR_MIN']\n    lr_ramp_ep = cfg['LR_RAMPUP_EPOCHS']\n    lr_sus_ep  = cfg['LR_SUSTAIN_EPOCHS']\n    lr_decay   = cfg['LR_EXP_DECAY']\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback","d56e38f6":"\nlr_start   = cfg['LR_START']\nlr_max     = cfg['LR_MAX'] * 16\nlr_min     = cfg['LR_MIN']\nlr_ramp_ep = cfg['LR_RAMPUP_EPOCHS']\nlr_sus_ep  = cfg['LR_SUSTAIN_EPOCHS']\nlr_decay   = cfg['LR_EXP_DECAY']\n\ndef lrfn(epoch):\n    if epoch < lr_ramp_ep:\n        lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n\n    elif epoch < lr_ramp_ep + lr_sus_ep:\n        lr = lr_max\n\n    else:\n        lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n\n    return lr\n\nlr_epochs = [lrfn(ep) for ep in range(cfg['epochs'])]\nplt.plot(lr_epochs)\nplt.show()","87194201":"\ndef get_model(cfg):\n    model_input = tf.keras.Input(shape=(img_size, img_size, 3), name='imgIn')\n\n    dummy = tf.keras.layers.Lambda(lambda x:x)(model_input)\n    \n    outputs = []    \n    for i in cfg['net_count']:\n        constructor = getattr(efn, f'EfficientNetB{i}')\n        \n        x = constructor(include_top=False, weights='noisy-student', \n                        input_shape=(img_size, img_size, 3),  pooling='avg')(dummy)\n        x = tf.keras.layers.Dense(n_labels, activation='sigmoid')(x)\n        outputs.append(x)\n        \n    model = tf.keras.Model(model_input, outputs, name='aNetwork')\n    model.summary()\n    return model","0a2002eb":"n_labels = labels.shape[1]\nwith strategy.scope():\n    model = get_model(cfg)\n\n    losses = [tf.keras.losses.BinaryCrossentropy(label_smoothing = cfg['label_smooth_fac'])\n              for i in cfg['net_count']]\n\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=[tf.keras.metrics.AUC(multi_label=True)])\n\n    model.summary()","ebc0d4b6":"steps_per_epoch = train_paths.shape[0] \/\/ BATCH_SIZE\ncheckpoint = tf.keras.callbacks.ModelCheckpoint( 'model.h5', save_best_only=True,monitor='val_auc', mode='max')\nlr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", patience=3, min_lr=1e-6, mode='max')","02877b68":"train_dataset     = train_dataset.map(lambda img, label: (img, tuple([label] * len(cfg['net_count']))))\nhistory = model.fit(\n    train_dataset, \n    epochs=20,\n    verbose=2,\n    callbacks=[checkpoint, get_lr_callback(cfg)],\n    steps_per_epoch=steps_per_epoch,\n    validation_data=valid_dataset)","b04cd302":"model.save_weights('easy_checkpoint.h5')\nprint(\"success save checkpoint!\")","dc82cc57":"hist_df = pd.DataFrame(history.history)\nhist_df.to_csv('history.csv')","4c67e036":"cfg['batch_size'] = 256\n\ncnt_test   = count_data_items(files_test)\nsteps      = cnt_test \/ (cfg['batch_size'] * REPLICAS) * cfg['tta_steps']\nds_testAug = get_dataset(files_test, CFG, augment=True, repeat=True, \n                         labeled=False, return_image_names=False)\n\nprobs = model.predict(ds_testAug, verbose=1, steps=steps)\n\nprobs = np.stack(probs)\nprobs = probs[:,:cnt_test * cfg['tta_steps']]\nprobs = np.stack(np.split(probs, cfg['tta_steps'], axis=1), axis=1)\nprobs = np.mean(probs, axis=1)","d7c82243":"ds = get_dataset(files_test, CFG, augment=False, repeat=False, \n                 labeled=False, return_image_names=True)\n\nimage_names = np.array([img_name.numpy().decode(\"utf-8\") \n                        for img, img_name in iter(ds.unbatch())])","1e081a8e":"for i in range(cfg[\"net_count\"]):\n    submission = pd.DataFrame(dict(\n        image_name = image_names,\n        target     = probs[i,:,0]))\n\n    submission = submission.sort_values('image_name') \n    submission.to_csv(f'submission_model_{i}.csv', index=False)","cb1dedaf":"submission = pd.DataFrame(dict(\n    image_name = image_names,\n    target     = np.mean(probs[:,:,0], axis=0)))\n\nsubmission = submission.sort_values('image_name') \nsubmission.to_csv('submission_models_blended.csv', index=False)","68b95ffd":"!ls -l .","b5192581":"## Variables and configurations","3970e9b1":"### Loading and preprocess CSVs","163b1eb4":"## Modeling","452afa1f":"## Save history","d0e35fd6":"## Preparing dataset","4e16ede2":"## Helper functions"}}