{"cell_type":{"c3bd0322":"code","cc9f4a4d":"code","8884a31e":"code","a125a36a":"code","4ed6055c":"code","1115b7a6":"code","aca505ac":"code","0cff7291":"code","3a0ac575":"code","e32ec360":"code","2e9d4b24":"code","299868e0":"code","3a55714a":"code","467e89f3":"code","6acf7c72":"code","64298459":"code","4432f008":"code","629f041a":"code","f513d6fe":"code","0306c153":"code","52663e39":"code","7d12aeb0":"code","d103263f":"code","8bbc7869":"code","22c1d588":"code","9ee41ec4":"code","99c97af6":"code","4b5a899e":"code","9e17fa3f":"code","bac7097c":"code","16087ac9":"code","3ae3e0e6":"code","0377e865":"code","728cfe0d":"code","f497ee9d":"code","1933717c":"code","6401dfc7":"code","705025cc":"code","20093e56":"code","dcf819f1":"code","67c52f8d":"code","00e26250":"code","eb1a0dbe":"code","ccda8bbd":"code","0fce4769":"code","e89e437d":"code","0647433b":"code","077fe3d9":"markdown","8e573306":"markdown","e09716c1":"markdown","37afc39a":"markdown","543c2c7f":"markdown","00538b79":"markdown","35825bec":"markdown","e7da3c7b":"markdown","c53afec0":"markdown","0a0089e5":"markdown","d6540e87":"markdown","0c781a12":"markdown","9adfebc3":"markdown","a5ba36e8":"markdown","39bb053d":"markdown","69c1aa79":"markdown","bde97dc3":"markdown","4aed3e75":"markdown","820e993e":"markdown","a5efb37c":"markdown","adffe64d":"markdown","b3db5987":"markdown","ab83ca86":"markdown","044e7f17":"markdown","ec6b1167":"markdown","5dba3760":"markdown","32387718":"markdown","e3d31350":"markdown","f5d76566":"markdown","a69e75d8":"markdown","cf78a1fc":"markdown","49e85492":"markdown"},"source":{"c3bd0322":"import os\nimport sys\nimport glob\nfrom os import listdir\nimport glob\nimport tqdm\nfrom typing import Dict\nimport cv2\nimport pydicom as dicom\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\n#plotly\n# !pip install chart_studio\nimport plotly.express as px\n# import chart_studio.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\nimport plotly.figure_factory as ff\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True,theme='pearl')\n\n#pydicom\nimport pydicom\n\n#supress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# from bokeh.layouts import row, column\n# from bokeh.models import ColumnDataSource, CustomJS, Label,Range1d,Slider,Span\n# from bokeh.plotting import figure, output_notebook, show\n\n#used for changing color of text in print statement\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL\n\n# output_notebook()","cc9f4a4d":"folder_path = '..\/input\/osic-pulmonary-fibrosis-progression'\ntrain_csv = folder_path + '\/train.csv'\ntest_csv = folder_path+ '\/test.csv'\nsample_csv = folder_path + '\/sample_submission.csv'\n\ntrain_data = pd.read_csv(train_csv)\ntest_data = pd.read_csv(test_csv)\nsample = pd.read_csv(sample_csv)\n\nprint(f\"{y_}Number of rows in train data: {r_}{train_data.shape[0]}\\n{y_}Number of columns in train data: {r_}{train_data.shape[1]}\")\nprint(f\"{g_}Number of rows in test data: {r_}{test_data.shape[0]}\\n{g_}Number of columns in test data: {r_}{test_data.shape[1]}\")\nprint(f\"{b_}Number of rows in submission data: {r_}{sample.shape[0]}\\n{b_}Number of columns in submission data:{r_}{sample.shape[1]}\")\n\ntrain_data.head().style.applymap(lambda x: 'background-color:lightgreen')","8884a31e":"def distribution(feature, color):\n    plt.figure(dpi=100)\n    sns.distplot(train_data[feature],color=color)\n    print(\"{}Max value of {} is: {} {:.2f} \\n{}Min value of {} is: {} {:.2f}\\n{}Mean of {} is: {}{:.2f}\\n{}Standard Deviation of {} is:{}{:.2f}\"\\\n      .format(y_,feature,r_,train_data[feature].max(),g_,feature,r_,train_data[feature].min(),b_,feature,r_,train_data[feature].mean(),m_,feature,r_,train_data[feature].std()))","a125a36a":"distribution(\"FVC\",\"blue\")","4ed6055c":"distribution(\"Age\",\"brown\")","1115b7a6":"distribution(\"Percent\",\"blue\")","aca505ac":"distribution(\"Weeks\",\"yellow\")","0cff7291":"plt.figure(dpi=100)\nsns.countplot(data=train_data,x='SmokingStatus',hue='Sex');","3a0ac575":"def distribution2(feature):\n    plt.figure(figsize=(15,7))\n    plt.subplot(121)\n    for i in train_data.Sex.unique():\n        sns.distplot(train_data[train_data['Sex']==i][feature],label=i)\n    plt.title(f\"Distribution of {feature} based on Sex\")\n    plt.legend()\n\n    plt.subplot(122)\n    for i in train_data.SmokingStatus.unique():\n        sns.distplot(train_data[train_data['SmokingStatus']==i][feature],label=i)\n    plt.title(f\"Distribution of {feature}  based on Smoking Status\")\n    plt.legend()\n","e32ec360":"distribution2(\"FVC\")","2e9d4b24":"distribution2(\"Percent\")","299868e0":"distribution2(\"Age\")","3a55714a":"distribution2(\"Weeks\")","467e89f3":"def vs(feature1,feature2,color=None):\n    fig = px.scatter(train_data,x=feature1,y=feature2,color=color)\n    fig.show()","6acf7c72":"vs(\"FVC\",\"Percent\",'SmokingStatus')","64298459":"vs(\"FVC\",\"Age\",'SmokingStatus')","4432f008":"vs(\"FVC\",\"Weeks\",\"SmokingStatus\")","629f041a":"rn = np.random.randint(0,train_data.Patient.nunique()-20,1)[0]\npatients_ids = train_data.Patient.unique()[rn:rn+20]\nfig =go.Figure()\n\nfor patient in patients_ids:\n    df = train_data[train_data[\"Patient\"] == patient]\n    fig.add_trace(go.Scatter(x=df.Weeks,y=df.FVC,\n                            mode='lines',\n                            name=str(patient)))\nfig.show()","f513d6fe":"print(f\"{y_}Number of unique patient is {r_}{train_data.Patient.nunique()}\")\n\ndf = train_data.Patient.value_counts()\nfig = px.bar(x=[f\"Patient {i}\" for i in range(len(df.index))],y=df.values)\nfig.show()","0306c153":"def box(feature1,feature2,color=None):\n    fig = px.box(train_data,x=feature2,y=feature1,color=color)\n    fig.show()","52663e39":"box(\"FVC\",\"Sex\",\"SmokingStatus\")","7d12aeb0":"box(\"Percent\",\"Sex\",\"SmokingStatus\")","d103263f":"box(\"Age\",\"Sex\",\"SmokingStatus\")","8bbc7869":"plt.figure(dpi=100)\nsns.heatmap(train_data.corr(),annot=True);","22c1d588":"train_image_path = folder_path + '\/train\/'\ntest_image_path = folder_path + '\/test\/'\n\ntrain_images = os.listdir(train_image_path)\ntest_images = os.listdir(test_image_path)\n\nimage = train_image_path+train_images[0]+\"\/1.dcm\"\n\ndef show_image(image):\n    print(f\"{y_} Image {r_}{image}\")\n    image = dicom.dcmread(image)\n    image = image.pixel_array    \n    plt.figure(figsize=(7,7))\n    plt.imshow(image,cmap='gray')\n    plt.axis('off')\n    plt.show()\n\nshow_image(image)\n\n    ","9ee41ec4":"def show_grid(cmap='gray'):\n    rn = np.random.randint(0,len(train_images),1)[0]\n    path= train_image_path+train_images[rn]\n    images = [dicom.read_file(path+\"\/\"+img) for img in os.listdir(path)]\n    images.sort(key=lambda x: float(x.ImagePositionPatient[2]))\n    plt.figure(figsize=(10,10))\n    for i,image in enumerate(images[:100]):\n        plt.subplot(10,10,i+1)\n        plt.imshow(image.pixel_array,cmap=cmap)\n        plt.axis('off')\n    plt.show()\n\nshow_grid()","99c97af6":"show_grid(cmap='jet')","4b5a899e":"show_grid(cmap='RdYlBu')","9e17fa3f":"import matplotlib.animation as animation\nfrom IPython.display import HTML\n\ndef show_animation():\n    rn = np.random.randint(0,len(train_images),1)[0]\n    fig = plt.figure()\n    path= train_image_path+train_images[0]\n    images = [dicom.read_file(path+\"\/\"+img) for img in os.listdir(path)]\n    images.sort(key=lambda x: float(x.ImagePositionPatient[2]))\n    ims = list()\n    for image in images:\n        image = plt.imshow(image.pixel_array,cmap='gray',animated=True)\n        plt.axis('off')\n        ims.append([image])\n    ani = animation.ArtistAnimation(fig,ims,interval=100,blit=False,repeat_delay=1000)\n    return ani\n\nani = show_animation()","bac7097c":"HTML(ani.to_jshtml())","16087ac9":"# load the DICOM files\ndef reconstruct():\n    files = []\n    path= train_image_path+train_images[0]\n\n    for fname in os.listdir(path):\n    #     print(\"loading: {}\".format(fname))\n        files.append(pydicom.dcmread(path+\"\/\"+fname))\n\n    print(\"file count: {}\".format(len(files)))\n\n    # skip files with no SliceLocation (eg scout views)\n    slices = []\n    skipcount = 0\n    for f in files:\n        if hasattr(f, 'SliceLocation'):\n            slices.append(f)\n        else:\n            skipcount = skipcount + 1\n\n    print(\"skipped, no SliceLocation: {}\".format(skipcount))\n\n    # ensure they are in the correct order\n    slices = sorted(slices, key=lambda s: s.SliceLocation)\n\n    # pixel aspects, assuming all slices are the same\n    ps = slices[0].PixelSpacing\n    ss = slices[0].SliceThickness\n    ax_aspect = ps[1]\/ps[0]\n    sag_aspect = ps[1]\/ss\n    cor_aspect = ss\/ps[0]\n\n    # create 3D array\n    img_shape = list(slices[0].pixel_array.shape)\n    img_shape.append(len(slices))\n    img3d = np.zeros(img_shape)\n\n    # fill 3D array with the images from the files\n    for i, s in enumerate(slices):\n        img2d = s.pixel_array\n        img3d[:, :, i] = img2d\n\n    # plot 3 orthogonal slices\n    plt.figure(figsize=(15,7))\n    a1 = plt.subplot(1,3,1)\n    plt.imshow(img3d[:, :, img_shape[2]\/\/2])\n    a1.set_aspect(ax_aspect)\n    plt.axis('off')\n\n\n    a2 = plt.subplot(1, 3, 2)\n    plt.imshow(img3d[:, img_shape[1]\/\/2, :])\n    a2.set_aspect(sag_aspect)\n    plt.axis('off')\n\n\n    a3 = plt.subplot(1, 3, 3)\n    plt.imshow(img3d[img_shape[0]\/\/2, :, :].T)\n    a3.set_aspect(cor_aspect)\n    plt.axis('off')\n    plt.show()","3ae3e0e6":"reconstruct()","0377e865":"from sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder,PowerTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score,cross_validate, KFold,GroupKFold\nfrom sklearn.metrics import make_scorer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline","728cfe0d":"#getting base week for patient\ndef get_baseline_week(data):\n    df = data.copy()\n    df['Weeks'] = df['Weeks'].astype(int)\n    df['min_week'] = df.groupby('Patient')['Weeks'].transform('min')\n    df['baseline_week'] = df['Weeks'] - df['min_week']\n    return df\n\n#getting FVC for base week and setting it as bas_FVC of patient\ndef get_base_FVC(data):\n    df = data.copy()\n    base = df.loc[df.Weeks == df.min_week][['Patient','FVC']].copy()\n    base.columns = ['Patient','base_FVC']\n    \n    base['nb']=1\n    base['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\n    \n    base = base[base.nb==1]\n    base.drop('nb',axis =1,inplace=True)\n    df = df.merge(base,on=\"Patient\",how='left')\n    df.drop(['min_week'], axis = 1)\n    return df ","f497ee9d":"train_data.drop_duplicates(keep=False,inplace=True,subset=['Patient','Weeks'])\ntrain_data = get_baseline_week(train_data)\ntrain_data = get_base_FVC(train_data)\n\nsample = pd.read_csv(sample_csv)\nsample.drop(\"FVC\",axis=1,inplace=True)\nsample[[\"Patient\",\"Weeks\"]] = sample[\"Patient_Week\"].str.split(\"_\",expand=True) \nsample = sample.merge(test_data.drop(\"Weeks\",axis=1),on=\"Patient\",how=\"left\")\n\n#we have to predict for all weeks \nsample[\"min_Weeks\"] = np.nan\nsample = get_baseline_week(sample)\nsample = get_base_FVC(sample)\n\ntrain_columns = ['baseline_week','base_FVC','Percent','Age','Sex','SmokingStatus']\ntrain_label = ['FVC']\nsub_columns = ['Patient_Week','FVC','Confidence']\n\ntrain = train_data[train_columns]\ntest = sample[train_columns]\n#Preprocessing\ntransformer = ColumnTransformer([('s',StandardScaler(),[0,1,2,3]),('o',OneHotEncoder(),[4,5])])\ntarget = train_data[train_label].values\ntrain = transformer.fit_transform(train)\ntest = transformer.transform(test)","1933717c":"train_data.head()","6401dfc7":"distribution(\"baseline_week\",'green');","705025cc":"import torch\nimport torch.nn as nn\nimport torch.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader","20093e56":"class Model(nn.Module):\n    def __init__(self,n):\n        super(Model,self).__init__()\n        self.layer1 = nn.Linear(n,100)\n        self.relu1 = nn.ReLU()\n        self.layer2 = nn.Linear(100,100)\n        self.relu2 = nn.ReLU()\n        \n        self.out1 = nn.Linear(100,3)\n#         self.relu3 = nn.ReLU()\n#         self.out2 = nn.Linear(100,3)\n            \n    def forward(self,xb):\n        x1 = self.relu1(self.layer1(xb))\n        x1 = self.relu2(self.layer2(x1))\n        \n        o1 = self.out1(x1)\n#         o2 = self.relu3(self.out2(x1))\n        return o1 \n        ","dcf819f1":"def run():\n    \n    def score(outputs,target):\n        confidence = outputs[:,2] - outputs[:,0]\n        clip = torch.clamp(confidence,min=70)\n        target=torch.reshape(target,outputs[:,1].shape)\n        delta = torch.abs(outputs[:,1] - target)\n        delta = torch.clamp(delta,max=1000)\n        sqrt_2 = torch.sqrt(torch.tensor([2.])).to(device)\n        metrics = (delta*sqrt_2\/clip) + torch.log(clip*sqrt_2)\n        return torch.mean(metrics)\n    \n    def qloss(outputs,target):\n        qs = [0.2,0.5,0.8]\n        qs = torch.tensor(qs,dtype=torch.float).to(device)\n        e =  outputs = target\n        e.to(device)\n        v = torch.max(qs*e,(qs-1)*e)\n        return torch.mean(v)\n\n    \n    def loss_fn(outputs,target,l):\n        return l * qloss(outputs,target) + (1- l) * score(outputs,target)\n        \n    def train_loop(train_loader,model,loss_fn,device,optimizer,lr_scheduler=None):\n        model.train()\n        losses = list()\n        metrics = list()\n        for i, (inputs, labels) in enumerate(train_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n            with torch.set_grad_enabled(True):           \n                outputs = model(inputs)                 \n                metric = score(outputs,labels)\n\n                loss = loss_fn(outputs,labels,0.8)\n                metrics.append(metric.cpu().detach().numpy())\n                losses.append(loss.cpu().detach().numpy())\n\n                loss.backward()\n\n                optimizer.step()\n                if lr_scheduler != None:\n                    lr_scheduler.step()\n            \n        return losses,metrics\n    \n    def valid_loop(valid_loader,model,loss_fn,device):\n        model.eval()\n        losses = list()\n        metrics = list()\n        for i, (inputs, labels) in enumerate(valid_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)                 \n            metric = score(outputs,labels)\n            \n            loss = loss_fn(outputs,labels,0.8)\n            metrics.append(metric.cpu().detach().numpy())\n            losses.append(loss.cpu().detach().numpy())\n            \n        return losses,metrics    \n\n    NFOLDS =5\n    kfold = KFold(NFOLDS,shuffle=True,random_state=42)\n    \n    #kfold\n    for k , (train_idx,valid_idx) in enumerate(kfold.split(train)):\n        batch_size = 64\n        epochs = 500\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"{device} is used\")\n        x_train,x_valid,y_train,y_valid = train[train_idx,:],train[valid_idx,:],target[train_idx],target[valid_idx]\n        n = x_train.shape[1]\n        model = Model(n)\n        model.to(device)\n        lr = 0.1\n        optimizer = optim.Adam(model.parameters(),lr=lr)\n        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n\n        train_tensor = torch.tensor(x_train,dtype=torch.float)\n        y_train_tensor = torch.tensor(y_train,dtype=torch.float)\n\n        train_ds = TensorDataset(train_tensor,y_train_tensor)\n        train_dl = DataLoader(train_ds,\n                             batch_size = batch_size,\n                             num_workers=4,\n                             shuffle=True\n                             )\n\n        valid_tensor = torch.tensor(x_valid,dtype=torch.float)\n        y_valid_tensor = torch.tensor(y_valid,dtype=torch.float)\n\n        valid_ds = TensorDataset(valid_tensor,y_valid_tensor)\n        valid_dl = DataLoader(valid_ds,\n                             batch_size = batch_size,\n                             num_workers=4,\n                             shuffle=False\n                             )\n        \n        print(f\"Fold {k}\")\n        for i in range(epochs):\n            losses,metrics = train_loop(train_dl,model,loss_fn,device,optimizer,lr_scheduler)\n            valid_losses,valid_metrics = valid_loop(valid_dl,model,loss_fn,device)\n            if (i+1)%50==0:\n                print(f\"epoch:{i} Training | loss:{np.mean(losses)} score: {np.mean(metrics)}| \\n Validation | loss:{np.mean(valid_losses)} score:{np.mean(valid_metrics)}|\")\n        torch.save(model.state_dict(),f'model{k}.bin')\n    ","67c52f8d":"run()","00e26250":"def inference():\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    nfold = 5\n    all_prediction = np.zeros((730,3))\n    \n    for i in range(nfold):\n        n = train.shape[1]\n        \n        model = Model(n)\n        model.load_state_dict(torch.load(f\"model{i}.bin\"))\n        predictions = list()\n        model.to(device)\n        test_tensor = torch.tensor(test,dtype=torch.float)\n        test_dl = DataLoader(test_tensor,\n                        batch_size=64,\n                        num_workers=2,\n                        shuffle=False)\n    \n        with torch.no_grad():\n            for i, inputs in enumerate(test_dl):\n                inputs = inputs.to(device, dtype=torch.float)\n                outputs= model(inputs) \n                predictions.extend(outputs.cpu().detach().numpy())\n\n        all_prediction += np.array(predictions)\/nfold\n        \n    return all_prediction  ","eb1a0dbe":"prediction = inference()\nsample[\"Confidence\"] = np.abs(prediction[:,2] - prediction[:,0])\nsample[\"FVC\"] = prediction[:,1]\nsub = sample[sub_columns]","ccda8bbd":"sub.to_csv(\"submission.csv\",index=False)","0fce4769":"sns.distplot(sub.Confidence);","e89e437d":"print(sub.shape)\nsub.head()","0647433b":"sub.to_csv(\"submission.csv\",index=False)","077fe3d9":"### 3.6 Distribution of FVC based on smoking status  and Sex","8e573306":"### 3.10 FVC vs Percent \n**Note:now will make graphs in plotly just for change**","e09716c1":"### 4.2 Grid of sorted images of some random patient","37afc39a":"### 3.16 Box plot of Percent based on sex and smoking status","543c2c7f":"### 3.7 Distribution of Percent based on Sex and SmokingStatus","00538b79":"### 3.15 Box plot of FVC bases on sex and smokingstatus \ud83d\udc40","35825bec":"## 3. EDA\n\n### 3.1 FVC Distribution\ud83d\udcc8\n","e7da3c7b":"## 4.4 3-D reconstruction.\n\nThis code for converting 2-D slices to a 3-D image is written by [mrbean-bremen](https:\/\/github.com\/mrbean-bremen).\n**Orignal Code** [here](https:\/\/github.com\/pydicom\/pydicom\/blob\/master\/examples\/image_processing\/reslice.py).","c53afec0":"### 3.9 Distribution of Weeks based on SmokingStatus and Sex","0a0089e5":"## 5. Preprocessing Data\n\nLet us create a baseline pytorch model.\n\n### 5.1 Importing some more libraries \ud83d\udcd8","d6540e87":"### 3.3 Distribution of Percent\ud83d\udcc8","0c781a12":"### Importing Libraries \ud83d\udcd8","9adfebc3":"### 4.1 Single Image","a5ba36e8":"### 3.2 Distribution of Age \ud83d\udc76\ud83e\uddd2\ud83e\uddd1\ud83e\uddd3","39bb053d":"### 6.4 submission","69c1aa79":"\n### 3.13 FVC vs weeks of 20 random patient \ud83c\udfb2","bde97dc3":"### 6.3 Inference","4aed3e75":"### 3.17 Correlation matrix of train data ","820e993e":"### 3.4 Distribution of weeks\ud83d\udcc5","a5efb37c":" ### 3.5 Number of smokers  sex \ud83d\udeac","adffe64d":"## 4. Visulizing Images \ud83d\uddbc\ufe0f\n\n* What is DICOM image ?\n**DICOM(Digital Image and Communication in Medecine)** is a standard developed and maintained by  **National Electrical Manufacturers Association (NEMA)** for storing and transfering the medical images like CT(computerised Tomography), Magnetic resonanse image(MRI) and other types of medical images.\n\nDICOM is very good protocol and intresting to read further click [here](https:\/\/en.wikipedia.org\/wiki\/DICOM)\n","b3db5987":"## Todo \n\n* How to use image to make prediction.","ab83ca86":"### 3.11 FVC vs Age","044e7f17":"### 4.3 Animation","ec6b1167":"### 3.16 Box plot of Age based on sex and smoking status","5dba3760":"### Getting data\ud83d\udcbd","32387718":" Please tell me in the comments if anything is wrong or if you did not understand something.\ud83e\udd17<br\/>\n Please upvote if you found it useful.","e3d31350":" ### 3.8 Distribution of Age based on SmokingStatus and Sex","f5d76566":"### 3.12 FVC vs Weeks","a69e75d8":"### 3.14 count of weeks of each patient \u231b","cf78a1fc":"### 6.2 Training Model","49e85492":"## 6 Simple Pytorch Model\n\n### 6.1 importing pytorch libraries \ud83d\udcd8"}}