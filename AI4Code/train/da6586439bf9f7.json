{"cell_type":{"1e90412c":"code","d462d7a4":"code","11f45735":"code","61df489e":"code","fe942794":"code","bed18d00":"code","b1c4f04f":"code","4e39c2a0":"code","e541f4b8":"code","34005c3b":"code","2886eae8":"code","21e8d90c":"code","c7bf1181":"code","c3bd3e9f":"code","c33e1a88":"code","fafe0835":"code","f7b4182d":"code","fa219443":"code","51bfeca0":"code","9a3f4cde":"code","0f58747a":"code","b550e6df":"code","c29ff782":"code","2b8dc862":"code","fd94bd46":"code","79c2757f":"code","1a8cd945":"code","0d4b8505":"code","49d0f0de":"code","a6fef998":"code","e4d07358":"code","25b0e60a":"code","991ae228":"code","ecca7a02":"code","4a3543f3":"code","f81f1d20":"code","f97a3037":"code","e555de71":"code","b6bff658":"code","2853eca5":"code","3933313b":"code","8d6f80b1":"code","e21ba21f":"code","0cfb5654":"code","839e8a63":"code","fea48e28":"code","ea80b5ab":"code","f21d254f":"markdown","c6f6f2d6":"markdown","a63b1299":"markdown","2e1532d4":"markdown","90570edb":"markdown","dd371a42":"markdown","7806fc55":"markdown","877c9dab":"markdown","b3088d2b":"markdown","5f78f711":"markdown","e2cbe68a":"markdown","544ec041":"markdown","9b235b68":"markdown","c3e3684a":"markdown","50fb5701":"markdown","1b1f04aa":"markdown","700bc147":"markdown","110ba906":"markdown","bf7ee1de":"markdown","3a3c6779":"markdown","5ac84beb":"markdown","6daec566":"markdown","ed07e083":"markdown","f9ecb5da":"markdown","514d5f4a":"markdown","c1d42149":"markdown","457d3e80":"markdown","0372c136":"markdown","69208a80":"markdown","fc14bd36":"markdown","a5cae961":"markdown","ad75eee5":"markdown","d57ebaf8":"markdown","4db06473":"markdown","6c03bc76":"markdown","3f805d38":"markdown","b645d585":"markdown","dc803dc3":"markdown","c9cb9c0a":"markdown","e79471d0":"markdown","2569417d":"markdown","c77b198f":"markdown","670a4f2d":"markdown","881321d3":"markdown","3bafd717":"markdown","e5b1877f":"markdown","9c0aaaa7":"markdown","56b2ff15":"markdown","81ddff6e":"markdown","7f72c809":"markdown"},"source":{"1e90412c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ntrain_df= pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest_df = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/test.csv')","d462d7a4":"train_df.head()","11f45735":"train_df.info()","61df489e":"test_df.info()","fe942794":"train_df.isnull().sum()","bed18d00":"test_df.isnull().sum()","b1c4f04f":"plt.figure(figsize=(8,6))\nsns.countplot(x='Response', data= train_df)\nplt.show()","4e39c2a0":"plt.figure(figsize=(8,6))\nsns.countplot(x='Gender', data= train_df)\nplt.show()","e541f4b8":"tmp_df = train_df.copy()\ntmp_df = tmp_df.groupby('Gender')['Response'].sum() \/ tmp_df.groupby('Gender')['Response'].count()\n\nplt.figure(figsize=(8,6))\nsns.pointplot(x='Gender', y='Response', data=tmp_df.reset_index())\nplt.show()","34005c3b":"plt.figure(figsize=(8,6))\nsns.distplot(train_df['Age'])\nplt.show()\n\nprint('The maximum age is {}'.format(train_df['Age'].max()))\nprint('The minimum age is {}'.format(train_df['Age'].min()))","2886eae8":"tmp_df = train_df.copy()\ntmp_df = tmp_df.groupby('Age')['Response'].sum() \/ tmp_df.groupby('Age')['Response'].count()\n\nplt.figure(figsize=(8,6))\nsns.jointplot(x='Age', y='Response', data=tmp_df.reset_index())\nplt.show()","21e8d90c":"plt.figure(figsize=(8,6))\nsns.countplot(train_df['Driving_License'])\nplt.show()","c7bf1181":"tmp_df = train_df.copy()\ntmp_df = tmp_df.groupby('Age')['Driving_License'].sum() \/ tmp_df.groupby('Age')['Driving_License'].count()\n\nplt.figure(figsize=(8,6))\nsns.jointplot(x='Age', y='Driving_License', data=tmp_df.reset_index())\nplt.show()","c3bd3e9f":"tmp_df = train_df.copy()\ntmp_df = tmp_df.groupby('Driving_License')['Driving_License'].sum() \/ tmp_df.groupby('Driving_License')['Response'].count()\n\nplt.figure(figsize=(8,6))\nsns.pointplot(x='Driving_License', y='Driving_License', data=tmp_df.reset_index())\nplt.show()","c33e1a88":"print(set(train_df['Region_Code']))\nprint('Number of regions: {}'.format(len(set(train_df['Region_Code']))))","fafe0835":"tmp_df = train_df.copy()\ntmp_df_count = tmp_df.groupby('Region_Code')['Response'].count()\nprint('Number of regions (sample size >= 30): {}'.format(len(tmp_df_count[tmp_df_count >= 30])))\n\ntmp_df = tmp_df.groupby('Region_Code')['Response'].sum() \/ tmp_df.groupby('Region_Code')['Response'].count()\ntmp_df = tmp_df.reset_index()\ntmp_df['Region_Code'] = tmp_df['Region_Code'].apply(lambda x: str(int(x)))\n\nplt.figure(figsize=(20, 6))\nsns.pointplot(x='Region_Code', y='Response', data=tmp_df)\nplt.show()","f7b4182d":"plt.figure(figsize=(8,6))\nsns.countplot(train_df['Previously_Insured'])\nplt.show()","fa219443":"tmp_df = train_df.copy()\ntmp_df = tmp_df[tmp_df['Driving_License'] == 1]\ntmp_df = tmp_df.groupby('Age')['Previously_Insured'].sum() \/ tmp_df.groupby('Age')['Previously_Insured'].count()\n\nplt.figure(figsize=(8,6))\nsns.jointplot(x='Age', y='Previously_Insured', data=tmp_df.reset_index())\nplt.show()","51bfeca0":"tmp_df = train_df.copy()\ntmp_df = tmp_df[tmp_df['Driving_License'] == 1]\ntmp_df = tmp_df.groupby('Previously_Insured')['Response'].sum() \/ tmp_df.groupby('Previously_Insured')['Response'].count()\n\nplt.figure(figsize=(8,6))\nsns.pointplot(x='Previously_Insured', y='Response', data=tmp_df.reset_index())\nplt.show()","9a3f4cde":"plt.figure(figsize=(8,6))\nsns.countplot(train_df['Vehicle_Age'])\nplt.show()","0f58747a":"tmp_df = train_df.copy()\ntmp_df = tmp_df.groupby('Vehicle_Age')['Response'].sum() \/ tmp_df.groupby('Vehicle_Age')['Response'].count()\n\nplt.figure(figsize=(8,6))\nsns.pointplot(x='Vehicle_Age', y='Response', data=tmp_df.reset_index())\nplt.show()","b550e6df":"plt.figure(figsize=(8,6))\nsns.countplot(train_df['Vehicle_Damage'])\nplt.show()","c29ff782":"tmp_df = train_df.copy()\ntmp_df = tmp_df.groupby('Vehicle_Damage')['Response'].sum() \/ tmp_df.groupby('Vehicle_Damage')['Response'].count()\n\nplt.figure(figsize=(8,6))\nsns.pointplot(x='Vehicle_Damage', y='Response', data=tmp_df.reset_index())\nplt.show()","2b8dc862":"plt.figure(figsize=(8,6))\nsns.distplot(train_df['Annual_Premium'])\nplt.show()\n\nprint('The mean annual premium is {:.2f}'.format(train_df['Annual_Premium'].mean()))\nprint('The median annual premium is {:.2f}'.format(train_df['Annual_Premium'].median()))\nprint('The maximum annual premium is {:.2f}'.format(train_df['Annual_Premium'].max()))\nprint('The minimum annual premium is {:.2f}'.format(train_df['Annual_Premium'].min()))\nninety_nineth_percentile = train_df['Annual_Premium'].quantile(0.99)\nprint('99% of people\\'s annual premium is less than {:.2f}'.format(ninety_nineth_percentile))","fd94bd46":"plt.figure(figsize=(8,6))\nsns.jointplot(x='Age', y='Annual_Premium', data=train_df)\nplt.show()","79c2757f":"train_df[['Age', 'Annual_Premium']].corr()","1a8cd945":"train_df[['Annual_Premium', 'Response']].corr()","0d4b8505":"tmp_df = train_df.copy()\ntmp_df['Annual_Premium'] = tmp_df['Annual_Premium'].apply(lambda x: round(x, -3))\ntmp_df = tmp_df.groupby('Annual_Premium')['Response'].sum() \/ tmp_df.groupby('Annual_Premium')['Response'].count()\n\nplt.figure(figsize=(8,6))\nsns.jointplot(x='Annual_Premium', y='Response', data=tmp_df.reset_index())\nplt.show()","49d0f0de":"tmp_df = train_df.copy()\ntmp_df = tmp_df[tmp_df['Annual_Premium'] < ninety_nineth_percentile]\ntmp_df['Annual_Premium'] = tmp_df['Annual_Premium'].apply(lambda x: round(x, -3))\ntmp_df = tmp_df.groupby('Annual_Premium')['Response'].sum() \/ tmp_df.groupby('Annual_Premium')['Response'].count()\n\nplt.figure(figsize=(8,6))\nsns.jointplot(x='Annual_Premium', y='Response', data=tmp_df.reset_index())\nplt.show()","a6fef998":"print(set(train_df['Policy_Sales_Channel']))\nprint('Number of policy sales channels: {}'.format(len(set(train_df['Policy_Sales_Channel']))))","e4d07358":"tmp_df = train_df.copy()\ntmp_df_count = tmp_df.groupby('Policy_Sales_Channel')['Response'].count()\nprint('Number of policy sales channels (sample size >= 30): {}'.format(len(tmp_df_count[tmp_df_count >= 30])))\n\ntmp_df = tmp_df.merge(\n    tmp_df_count.rename('Policy_Sales_Channel_Count'),\n    how='left',\n    left_on='Policy_Sales_Channel',\n    right_on='Policy_Sales_Channel'\n    )\ntmp_df = tmp_df[tmp_df['Policy_Sales_Channel_Count'] >= 30]\n\ntmp_df = tmp_df.groupby('Policy_Sales_Channel')['Response'].sum() \/ tmp_df.groupby('Policy_Sales_Channel')['Response'].count()\ntmp_df = tmp_df.reset_index()\n\nplt.figure(figsize=(25, 6))\nsns.pointplot(x='Policy_Sales_Channel', y='Response', data=tmp_df)\nplt.xticks(rotation='vertical')\nplt.show()","25b0e60a":"plt.figure(figsize=(8,6))\nsns.distplot(train_df['Vintage'])\nplt.show()","991ae228":"tmp_df = train_df.copy()\ntmp_df['Vintage'] = tmp_df['Vintage'].apply(lambda x: round(x, -1))\ntmp_df = tmp_df.groupby('Vintage')['Response'].sum() \/ tmp_df.groupby('Vintage')['Response'].count()\n\nplt.figure(figsize=(8,6))\nsns.jointplot(x='Vintage', y='Response', data=tmp_df.reset_index())\nplt.show()","ecca7a02":"GENDER_MAPPING = {'Female': 0, 'Male': 1}\nVEHICLE_AGE_MAPPING = {'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2}\nVEHICLE_DAMAGE_MAPPING = {'No': 0, 'Yes': 1}\n\ndef preprocessing(train_df, test_df):\n    # categorical\n    train_df['Gender'] = train_df['Gender'].map(GENDER_MAPPING).astype(int)\n    test_df['Gender'] = test_df['Gender'].map(GENDER_MAPPING).astype(int)\n    train_df['Vehicle_Age'] = train_df['Vehicle_Age'].map(VEHICLE_AGE_MAPPING).astype(int)\n    test_df['Vehicle_Age'] = test_df['Vehicle_Age'].map(VEHICLE_AGE_MAPPING).astype(int)\n    train_df['Vehicle_Damage'] = train_df['Vehicle_Damage'].map(VEHICLE_DAMAGE_MAPPING).astype(int)\n    test_df['Vehicle_Damage'] = test_df['Vehicle_Damage'].map(VEHICLE_DAMAGE_MAPPING).astype(int)\n\n    train_df['Region_Code'] = train_df['Region_Code'].astype(int)\n    test_df['Region_Code'] = test_df['Region_Code'].astype(int)\n\n    tmp_df_count = train_df.groupby('Policy_Sales_Channel')['Response'].count()\n    tmp_count_dict = tmp_df_count.to_dict()\n    for index, val in tmp_count_dict.items():\n        tmp_count_dict[index] = index if val >= 30 else 9999\n    train_df['Policy_Sales_Channel'] = train_df['Policy_Sales_Channel'].map(tmp_count_dict).astype(str)\n    test_df['Policy_Sales_Channel'] = test_df['Policy_Sales_Channel'].apply(\n        lambda x: tmp_count_dict[x] if x in tmp_count_dict else 9999\n    ).astype(int)\n    \n    # numerical\n    train_df['Age_Squared'] = train_df['Age'].apply(lambda x: x ** 2)\n    test_df['Age_Squared'] = test_df['Age'].apply(lambda x: x ** 2)\n    min_age_squared = train_df['Age_Squared'].min()\n    max_age_squared = train_df['Age_Squared'].max()\n    train_df['Age_Squared'] = train_df['Age_Squared'].apply(lambda x: (x - min_age_squared) \/ (max_age_squared - min_age_squared))\n    test_df['Age_Squared'] = test_df['Age_Squared'].apply(lambda x: (x - min_age_squared) \/ (max_age_squared - min_age_squared))\n\n    min_age = train_df['Age'].min()\n    max_age = train_df['Age'].max()\n    train_df['Age'] = train_df['Age'].apply(lambda x: (x - min_age) \/ (max_age - min_age))\n    test_df['Age'] = test_df['Age'].apply(lambda x: (x - min_age) \/ (max_age - min_age))\n\n    min_vintage = train_df['Vintage'].min()\n    max_vintage = train_df['Vintage'].max()\n    train_df['Vintage'] = train_df['Vintage'].apply(lambda x: (x - min_vintage) \/ (max_vintage - min_vintage))\n    test_df['Vintage'] = test_df['Vintage'].apply(lambda x: (x - min_vintage) \/ (max_vintage - min_vintage))\n    \n    min_annual_premium = train_df['Annual_Premium'].min()\n    ninety_nineth_percentile = train_df['Annual_Premium'].quantile(0.99)\n    max_annual_premium = train_df['Annual_Premium'].max()\n\n    train_df['Premium_Below_Ninety_Nineth_Percentile'] = train_df['Annual_Premium'].apply(lambda x: (min(x, ninety_nineth_percentile) - min_annual_premium) \/ (ninety_nineth_percentile - min_annual_premium))\n    test_df['Premium_Below_Ninety_Nineth_Percentile'] = test_df['Annual_Premium'].apply(lambda x: (min(x, ninety_nineth_percentile) - min_annual_premium) \/ (ninety_nineth_percentile - min_annual_premium))\n    train_df['Premium_Above_Ninety_Nineth_Percentile'] = train_df['Annual_Premium'].apply(lambda x: max(0, x - ninety_nineth_percentile) \/ (max_annual_premium - ninety_nineth_percentile))\n    test_df['Premium_Above_Ninety_Nineth_Percentile'] = test_df['Annual_Premium'].apply(lambda x: max(0, x - ninety_nineth_percentile) \/ (max_annual_premium - ninety_nineth_percentile))\n\n    return train_df, test_df","4a3543f3":"numerical_cols = ['Age_Squared', 'Age', 'Premium_Below_Ninety_Nineth_Percentile', 'Premium_Above_Ninety_Nineth_Percentile', 'Vintage']\ncategorical_cols = ['Gender', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Policy_Sales_Channel']","f81f1d20":"numerical_cols.remove('Vintage')","f97a3037":"X_col = numerical_cols + categorical_cols\nprint(X_col)","e555de71":"training_sets = []\nvalidation_sets = []\n\nK = 5\nkf = KFold(n_splits=K, shuffle=True)\n\nfor train_index, test_index in kf.split(train_df):\n    print(train_index, test_index)\n    training_set = train_df[train_df.index.isin(train_index)]\n    validation_set = train_df[train_df.index.isin(test_index)]\n\n    training_set, validation_set = preprocessing(training_set, validation_set)\n\n    training_sets.append(training_set)\n    validation_sets.append(validation_set)","b6bff658":"results = []\n\nfor i in range(len(X_col)):\n    result = {}\n    print('Features included in this iteration: {}'.format(X_col))\n    result['features'] = X_col\n\n    model_scores = []\n    feature_scores = [0] * len(X_col)\n    \n    fold = 1\n    for training_set, validation_set in zip(training_sets, validation_sets):\n        print('Fold #{}'.format(fold))\n        X_train = training_set[X_col]\n        X_val = validation_set[X_col]\n        y_train = training_set[['Response']]\n        y_val = validation_set[['Response']]\n\n        model = CatBoostClassifier()\n\n        model = model.fit(\n            X_train,\n            y_train,\n            cat_features=categorical_cols,\n            eval_set=(X_val, y_val),\n            early_stopping_rounds=10,\n            verbose=False\n        )\n\n        y_pred = [i[1] for i in model.predict_proba(X_val)]\n\n        model_score = roc_auc_score(y_val, y_pred)\n        print('ROC AUC score: {}'.format(model_score))\n        model_scores.append(model_score)\n\n        feature_importance = model.get_feature_importance()\n        print('Feature importance: {}'.format(feature_importance))\n        for i in range(0, len(X_col)):\n            feature_scores[i] += feature_importance[i]\n\n        fold += 1\n\n    print('Overall:')\n    print('ROC AUC score: {}'.format(np.mean(model_scores)))\n    result['score'] = np.mean(model_scores)\n    print('Feature importance: {}'.format(feature_scores))\n    results.append(result)\n    if len(X_col) > 1:\n        least_importance_feature = X_col[feature_scores.index(min(feature_scores))]\n        print('The least important feature is: {}'.format(least_importance_feature))\n        print('Thus, for the next iteration, we are going to drop {}'.format(least_importance_feature))\n\n        if least_importance_feature in numerical_cols:\n            numerical_cols.remove(least_importance_feature)\n        else:\n            categorical_cols.remove(least_importance_feature)\n        X_col = numerical_cols + categorical_cols\n\n        print()\n        print()","2853eca5":"results_df = pd.DataFrame.from_records(results)\nprint(results_df)","3933313b":"selected_features = results_df.iloc[8]['features']\nprint(selected_features)","8d6f80b1":"training_set, test_set = preprocessing(train_df, test_df)\n\nX_train = training_set[selected_features]\nX_test = test_set[selected_features]\ny_train = training_set['Response']\n\nmodel = CatBoostClassifier()\n\nmodel = model.fit(\n    X_train,\n    y_train,\n    cat_features=['Previously_Insured', 'Vehicle_Damage'],\n    early_stopping_rounds=10,\n    verbose=100\n)","e21ba21f":"feature_importance = model.get_feature_importance()\nfeature_importance_df = pd.DataFrame(\n    data={'feature_importance': feature_importance},\n    index=selected_features\n)\nfeature_importance_df.sort_values(by=['feature_importance'], ascending=False, inplace=True)\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x=feature_importance_df['feature_importance'], y=feature_importance_df.index)\nplt.title('Feature Importance')\nplt.show()","0cfb5654":"model.plot_tree(\n    tree_idx=0,\n    pool=X_train\n)","839e8a63":"y_pred_submit = [i[1] for i in model.predict_proba(X_test)]\nsubmission_df = pd.DataFrame(data={'Passenger': test_df['id'], 'Response': y_pred_submit})","fea48e28":"submission_df.head()","ea80b5ab":"submission_df.to_csv('submission.csv', index=False)","f21d254f":"This is obvious. We should target those who have a driving license.","c6f6f2d6":"The maximum age of the dataset is 85. This makes me wonder whether it is suitable for them to drive at all.","a63b1299":"**Vintage**\n\nThe number of days that they have been associated with the company","2e1532d4":"In the following, we are going to build a model using the library **CatBoost**.\n\n**Cross-validation**\n* When training the model, we are going to perform a 5-fold cross-validation.\n* The training dataset in each subsample will be proprocessed with the function defined above.\n* We are going to evaluate the perfomance of the model using the validation dataset in each subsample by the **ROC AUC score**.\n\n**Backward stepwise selection**\n* In the first round of model training, we are going to include all features in our dataset.\n* In subsequent rounds, the least important features are going to be dropped one by one.\n* The final selected model will be based on **ROC AUC score**.","90570edb":"# Output","dd371a42":"In fact I'd like to drop Vintage because there is no sensible correlation (the number of days is too low).","7806fc55":"They are still positively correlated but very weakly. This is perhaps because there is great differentiation between health insurance policies. There are policies with different limits and benefits so they are charged differently.","877c9dab":"Normally we would expect premium to be positively correlated with age because older people are charged higher premium.","b3088d2b":"# Exploratory Data Analysis","5f78f711":"**Age of the vehicle**","e2cbe68a":"This is quite positively skewed.","544ec041":"There is a clear positive correlation. It is just distorted by the outliers. 72963 is the 99th percentile as discussed above.","9b235b68":"**Region**","c3e3684a":"**Gender**","50fb5701":"# Afterthought\n\nWhile the model result is satisfactory, it does not necessarily translate to good decision making. As discussed over and over, adverse selection is an issue that needs to be tackled.\n\nBesides, the insurance company should obtain more data in order to have more insightful analysis:\n* Investigate the relationship between age and willingness to buy insurance. Does risk tolerance play a role? What is the implication of the **quadratic** relationship?\n* Develop a profile of regions (we've only got pure numerical identifier). Perhaps people in busier regions are more willing to buy insurance.\n* Obtain more accurate age of the vehicle.\n* Obtain the type of health insurance policy that the individual is buying. This can reveal how rich they are.\n* Policy sales channels can play a role (e.g. agent channels are more effective due to interpersonal relationship). The number of channels in this dataset is too high. Perhaps they can be grouped.\n* Vintage can play a role (e.g. we would expect loyal customers to be more willing to buy a policy). However, this dataset only contains new customers.","1b1f04aa":"There is not a clear direction as to how this will drive their decision. Perhaps more insight can be extracted if the vehicle age is given in number.\n\nWe may consider these questions:\n* Will new car owners be more interested in buying vehicle insurance because they are more vulnerable to any damage to their vehicle?\n* Will old car owners be more interested in buying vehicle insurance because they expect higher probability of accidents?","700bc147":"Let's split the dataset and perform preprocessing.","110ba906":"CatBoost allows us to easily plot the importance of features.","bf7ee1de":"This is in fact an extremely important factor for the insurance company. If individuals have got their vehicle damaged before, it usually indicates that they possess higher-than-average risks, thus **adverse selection**.\n\nThe insurance company may charge them higher premium. While our \"task\" is to predict who will be interested in buying insurance policy, the insurance company ought to consider whether they want to attract higher risks.","3a3c6779":"Although the age of the vehicle is numerical in nature, it is given as categorical input.","5ac84beb":"People under 50 almost always have a driving license and then the proportion starts to decrease when they get older.","6daec566":"**Whether they have vehicle insurance already**","ed07e083":"There is a weak positive correlation between annual premium and response. This is reasonable between those who can afford an expensive health insurance policy are probably richer.","f9ecb5da":"# Final Model","514d5f4a":"**Whether they have a driving license**","c1d42149":"# Introduction\n\nOur task is to predict whether an individual is interested in buying a vehicle insurance policy.\n\nWe are given the following variables:\n* **id**: Unique ID for the customer\n* **Gender**: Gender of the customer\n* **Age**: Age of the customer\n* **Driving_License**: 0 : Customer does not have DL, 1 : Customer already has DL\n* **Region_Code**: Unique code for the region of the customer\n* **Previously_Insured**: 1 : Customer already has Vehicle Insurance, 0 : Customer doesn't have Vehicle Insurance\n* **Vehicle_Age**: Age of the Vehicle\n* **Vehicle_Damage**: 1 : Customer got his\/her vehicle damaged in the past. 0 : Customer didn't get his\/her vehicle damaged in the past.\n* **Annual_Premium**: The amount customer needs to pay as premium in the year\n* **Policy_Sales_Channel**: Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n* **Vintage**: Number of Days, Customer has been associated with the company\n* **Response**: 1 : Customer is interested, 0 : Customer is not interested\n\nOf course, only making good prediction does not necessarily means we are going to **make good decisions**. There are many factors underlying the insurance industry. I hope that below can shed some light on this topic.","457d3e80":"Here is a summary of the ROC AUC scores of all models above.","0372c136":"**Policy Sales Channel**","69208a80":"**Age**","fc14bd36":"**Categorical input**\n* As noted above, there are 53 region codes and 153 policies sales channels. Transforming them into dummy variables would result in a lot of columns.\n* As this notebook is going to use CatBoost, there is no need to transform the categorical input into dummy variables. CatBoost provides us with a convenient way to model the data. But be aware that the float has to been converted to int.\n* **Policy_Sales_Channel**: I'd like to encode those whose sample sizes are below 30 as *9999*, instead of treating each of them as being significant as its own.\n\n\n**Numerical input**\n* Here applies the **min-max normalisation**. Another common way of preprocessing numerical input is **standardisation**, which subtracts the mean from each datum and divides it by the standard deviation. **Min-max normalisation** puts the input in a confined range, i.e. from 0 to 1, whereas **standardisation** would result in variables that range from negative to positive.\n* For our preprocessing purposes, we are going to use the min and max of the **training** dataset. This is because the model is supposed to be built upon the training dataset and then the model is used to make prediction with the testing dataset.\n* **Age**: I am implementing a degree 2 variables, given the quadratic relationship we have discussed above.\n* **Premium**: I'd like to divide the premium into the portion below the 99th percentile and the portion above.\n\n**Notes for cross-validation**\n* When we build our model below, we are going to perform cross-validation.\n* As a result, we need to perform the preprocessing upon the training dataset of each subsample. Notably, the sample size of each distinct categorical value (as in **Policy_Sales_Channel**) and the **min** and **max** of each numerical input (we need to perform the **min-max normalisation**) would be different in different subsamples.\n* Thus, instead of preprocessing the whole dataset at once, we are going to create a function that can be reused below.","a5cae961":"Region can be correlated to response:\n* Some region may be busier, thus higher demand of vehicle insurance.\n* However, there are too many regions. If the insurance company has more information about each region, that might be more insightful.\n* We can incorporate regions in our analysis given that all regions have a sample size >= 30.","ad75eee5":"**Response**\n\nThe majority are not interested in the vehicle insurance","d57ebaf8":"Here are all our numerical and categorical columns respectively.","4db06473":"Males are more likely to be interested. However, this is not intuitive. It can be due to pure randomness or other factors.","6c03bc76":"# CatBoost Modelling (with Cross Validation and Backward Stepwise Selection)","3f805d38":"# Preprocessing","b645d585":"While this is self-explanatory. We should be careful of those who already have vehicle insurance. Because why would they want another vehicle insurance policy? Does it indicate **adverse selection**?\n\n**Adverse selection** means that individuals with higher-than-average risks are more willing to buy insurance, especially insurance with high limit. This is not **actuarially equitable** to those with average risks if these individuals are charged the same amount of premium. Underwriters have to pay attention to this issue when they indicate that they want to buy another policy.","dc803dc3":"Unsurprisingly, those who are very old are not interested in buying the vehicle insurance.\n\nThose who are very young also are not interested. This may be because they are more risk tolerant and underestimate the importance of insurance.\n\nIt makes sense that people in their middle age are more interested.\n\nThe relationship between age and response seems to be **quadratic**.","c9cb9c0a":"I am not surprised that there is no clear correlation because the vintage is too low (< 300 days). I would expect loyal customers to be more willing to buy a policy from this insurance company (whether they already have one or not). However, these seem to be new customers.","e79471d0":"**Checking for null value**","2569417d":"The resulted models have only 3 variables:\n* whether they already have vehicle insurance\n* their age squared\n* whether they have got their vehicle damaged before\n\nAs discussed above, the contribution of **Vehicle_Damage** can imply adverse selection. Even if we correctly predict their willingness to buy, it may not be desirable. The insurance company might charge them higher premium or not target them at all.","c77b198f":"**Whether they have got their vehicle damaged before**","670a4f2d":"We can also plot the tree.","881321d3":"**Annual Premium**","3bafd717":"# Results","e5b1877f":"Most people have a driving license","9c0aaaa7":"There is correlation between policy sales channel and response. However, because there are too many channels, the sample size of each is small. For example, channel 123 has a response rate of 100% but the count is only 1. That does not mean anything significant.","56b2ff15":"Lastly, let's use everything of the training dataset to train the model and apply it on the testing dataset.","81ddff6e":"All models have ROC AUC scores close to or above 80%, which indicates that their predictive powers are high.\n\nWe can observe a material difference between a 3-factor model (#8) and models with fewer factors (#9-10). However, when more factors are included, they do not exhibit material difference (the difference in ROC AUC score is within 1%). It is justifiable to select the model with high predictive power while being simple enough, i.e. #8.\n\nNote that this score has been calculated with cross-validation, so it indicates the predictive power of the model when applied to a new set of data. Features to be included are:","7f72c809":"Those between age 40 and age 50 are most unlikely to have a vehicle insurance policy already."}}