{"cell_type":{"7f8356ae":"code","cf12c315":"code","493c0f60":"code","9eea19a4":"code","b16889ab":"code","addad941":"code","f79b85cb":"code","58f1f3c6":"code","c8ce5c74":"code","ad2eedc5":"code","69bd9fee":"code","010d8d4c":"code","1b5a15db":"code","89322198":"code","77fba0fb":"code","78f30629":"code","6808054f":"code","57d21799":"code","1b77d471":"code","10d68921":"code","ed16e0f2":"code","cc765866":"markdown","c18dd58b":"markdown","7c615b2c":"markdown","53b5fb17":"markdown","85001b40":"markdown","208158bb":"markdown","218f1dfc":"markdown","7965e646":"markdown","42602969":"markdown","948ea033":"markdown","a1f965d3":"markdown"},"source":{"7f8356ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cf12c315":"import pandas as pd\nimport matplotlib.pyplot as plt \nsonar=pd.read_csv(\"\/kaggle\/input\/sonardata\/sonar.csv\")\nsonar.head(5)","493c0f60":"from sklearn.model_selection import train_test_split\nX=sonar.iloc[:,0:60]\ny=sonar.iloc[:,60]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","9eea19a4":"from sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nrnd_clf.fit(X_train, y_train)","b16889ab":"score=rnd_clf.feature_importances_\nfname=sonar.columns[:60]\ndf = pd.Series(score,index=fname)\ndf[0:5]","addad941":"df.plot.barh(x='Method', y='Accuracy',figsize=(15,12))","f79b85cb":"df.sort_values(ascending=False, inplace=True)\n","58f1f3c6":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(X_train, y_train)\ny_pred_tree = tree_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred_tree))","c8ce5c74":"nc=np.arange(5,60,5)\nacc=np.empty(11)\ni=0\nfor k in np.nditer(nc):\n    topf=df.index[1:k]\n    tree_clf = DecisionTreeClassifier(random_state=42)\n    tree_clf.fit(X_train[topf], y_train)\n    y_pred_tree = tree_clf.predict(X_test[topf])\n    acc[i]=accuracy_score(y_test, y_pred_tree)\n    i = i + 1\nacc","ad2eedc5":"x=pd.Series(acc,index=nc)\nx.plot()\n# Add title and axis names\nplt.title('Top Features versus accuracy')\nplt.xlabel('Numer of Fetures')\nplt.ylabel('Accuracy')\nplt.show()","69bd9fee":"top10=df.index[1:30]\nfrom sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\nrnd_clf.fit(X_train[top10], y_train)\ny_pred_rf = rnd_clf.predict(X_test[top10])\nprint(accuracy_score(y_test, y_pred_rf))","010d8d4c":"from sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\nada_clf.fit(X_train, y_train)\ny_pred_ad = ada_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred_ad))","1b5a15db":"dfig = pd.DataFrame()\nnc=np.arange(0.1,0.7,0.05)\n \nfor k in np.nditer(nc):\n    ada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n    algorithm=\"SAMME.R\", learning_rate=k, random_state=42)\n    ada_clf.fit(X_train, y_train)\n    y_pred_tree = ada_clf.predict(X_test)\n    a_row = pd.Series([k, accuracy_score(y_test, y_pred_tree)])\n    row_df = pd.DataFrame([a_row])\n    dfig = pd.concat([row_df, dfig], ignore_index=False)\n","89322198":"dfig.columns=['Learning Rate','Accuracy']\ndfig.sort_values(ascending=True, inplace=True,by=['Learning Rate'])\ndfig.plot.line(x='Learning Rate', y='Accuracy',figsize=(15,8))\ndfig.head(8)","77fba0fb":"from sklearn.ensemble import GradientBoostingClassifier\ngb_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5, max_depth=2, random_state=42)\ngb_clf.fit(X_train, y_train)\ny_pred_gb = gb_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred_gb))","78f30629":"dfig = pd.DataFrame()\nnc=np.arange(2,10,1)\n \nfor k in np.nditer(nc):\n    gb_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5, max_depth=k, random_state=42)\n    gb_clf.fit(X_train, y_train)\n    y_pred_tree = gb_clf.predict(X_test)\n    a_row = pd.Series([k, accuracy_score(y_test, y_pred_tree)])\n    row_df = pd.DataFrame([a_row])\n    dfig = pd.concat([row_df, dfig], ignore_index=False)","6808054f":"dfig.columns=['Max Depth','Accuracy']\ndfig.sort_values(ascending=True, inplace=True,by=['Max Depth'])\ndfig.plot.line(x='Max Depth', y='Accuracy',figsize=(15,8))\ndfig.head(10)","57d21799":"dfig = pd.DataFrame()\nnc=np.arange(20,0,-2)\n \nfor k in np.nditer(nc):\n    gb_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5, max_depth=3,min_samples_leaf=int(k), random_state=42)\n    gb_clf.fit(X_train, y_train)\n    y_pred_tree = gb_clf.predict(X_test)\n    a_row = pd.Series([k, accuracy_score(y_test, y_pred_tree)])\n    row_df = pd.DataFrame([a_row])\n    dfig = pd.concat([row_df, dfig], ignore_index=False)\n","1b77d471":"dfig.columns=['Min Samples Leaves','Accuracy']\ndfig.sort_values(ascending=True, inplace=True,by=['Min Samples Leaves'])\ndfig.plot.line(x='Min Samples Leaves', y='Accuracy',figsize=(15,8))","10d68921":"dfig = pd.DataFrame()\nnc=np.arange(10,300,20)\n \nfor k in np.nditer(nc):\n    gb_clf = GradientBoostingClassifier(n_estimators=k, learning_rate=0.5, max_depth=3,min_samples_leaf=1, random_state=42)\n    gb_clf.fit(X_train, y_train)\n    y_pred_tree = gb_clf.predict(X_test)\n    a_row = pd.Series([k, accuracy_score(y_test, y_pred_tree)])\n    row_df = pd.DataFrame([a_row])\n    dfig = pd.concat([row_df, dfig], ignore_index=False)","ed16e0f2":"dfig.columns=['Number of Stages','Accuracy']\ndfig.sort_values(ascending=True, inplace=True,by=['Number of Stages'])\ndfig.plot.line(x='Number of Stages', y='Accuracy',figsize=(15,8))\ndfig.head(15)","cc765866":"# Question 9: How to regularize using number of stages <a id=\"9\"><\/a>","c18dd58b":"# Question 3: Can we use random forest to improve random forest <a id=\"3\"><\/a>","7c615b2c":"# Question 4: How to fit an AdaBoost <a id=\"4\"><\/a>","53b5fb17":"# Question 1: How to find fetaure importance from random forest <a id=\"1\"><\/a>","85001b40":"# Question 8: How to regularize using min sample leaves <a id=\"8\"><\/a>","208158bb":"# Question 6: How to fit GradientBoost <a id=\"6\"><\/a>","218f1dfc":"# Question 7: How to regularize using max depth <a id=\"7\"><\/a>","7965e646":"# Using random forests feature importance","42602969":"# Question 5: Effect of learning rate on AdaBoost <a id=\"1\"><\/a>\n\n","948ea033":"* [<font size=4>Question 1:How to find fetaure importance from random forest<\/font>](#1)\n* [<font size=4>Question 2: Can we use top features from random forest to improve decision tree<\/font>](#2)   \n* [<font size=4>Question 3: Can we use random forest to improve random forest<\/font>](#3)   \n* [<font size=4>Question 4: How to fit an AdaBoost](#4)  \n* [<font size=4>Question 5: Effect of learning rate on AdaBoost<\/font>](#5)  \n* [<font size=4>Question 6: How to fit GradientBoost <\/font>](#6)   \n* [<font size=4>Question 7: How to regularize using max depth<\/font>](#7)  \n* [<font size=4> Question 8: How to regularize using min sample leaves<\/font>](#8)\n* [<font size=4>Question 9: How to regularize using number of stages<\/font>](#9)   ","a1f965d3":"# Question 2: Can we use top features from random forest to improve decision tree <a id=\"2\"><\/a>"}}