{"cell_type":{"090079c3":"code","a659dcb2":"code","b850b613":"code","a587977c":"code","053b7a5a":"code","7d33cc4b":"code","f529d2da":"code","553739bd":"code","339be4ae":"code","a69610da":"code","f56846b2":"code","5722d8f5":"code","4d227ca4":"code","bd6db55d":"code","a7062965":"code","d4e530f6":"code","ac0e58a2":"code","f4973e73":"code","59b6dc2c":"code","39888f6b":"code","6e056890":"code","dd0bdbdb":"code","29964d4e":"code","da415a18":"code","19a67570":"code","69dfdd9e":"code","fe00fee4":"code","e6fb13d7":"code","695e0186":"code","5ac858f9":"code","bef36d0b":"markdown","4cda1c8f":"markdown","713c9e59":"markdown"},"source":{"090079c3":"# Visualization Libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Preprocessing Libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n\n# ML Libraries\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Evaluation Metrics\nfrom yellowbrick.classifier import ClassificationReport\nfrom sklearn import metrics","a659dcb2":"df = pd.concat([pd.read_csv('..\/input\/Chicago_Crimes_2001_to_2004.csv', error_bad_lines=False), pd.read_csv('..\/input\/Chicago_Crimes_2005_to_2007.csv', error_bad_lines=False)], ignore_index=True)\ndf = pd.concat([df, pd.read_csv('..\/input\/Chicago_Crimes_2008_to_2011.csv', error_bad_lines=False)], ignore_index=True)\ndf = pd.concat([df, pd.read_csv('..\/input\/Chicago_Crimes_2012_to_2017.csv', error_bad_lines=False)], ignore_index=True)\ndf.head()","b850b613":"df.info()","a587977c":"# Preprocessing\n# Remove NaN Value (As Dataset is huge, the NaN row could be neglectable)  \ndf = df.dropna()","053b7a5a":"# As the dataset is too huge is size, we would just subsampled a dataset for modelling as proof of concept\ndf = df.sample(n=100000)","7d33cc4b":"# Remove irrelevant\/not meaningfull attributes\ndf = df.drop(['Unnamed: 0'], axis=1)\ndf = df.drop(['ID'], axis=1)\ndf = df.drop(['Case Number'], axis=1) \n\ndf.info()","f529d2da":"# Splitting the Date to Day, Month, Year, Hour, Minute, Second\ndf['date2'] = pd.to_datetime(df['Date'])\ndf['Year'] = df['date2'].dt.year\ndf['Month'] = df['date2'].dt.month\ndf['Day'] = df['date2'].dt.day\ndf['Hour'] = df['date2'].dt.hour\ndf['Minute'] = df['date2'].dt.minute\ndf['Second'] = df['date2'].dt.second \ndf = df.drop(['Date'], axis=1) \ndf = df.drop(['date2'], axis=1) \ndf = df.drop(['Updated On'], axis=1)\ndf.head()","553739bd":"# Convert Categorical Attributes to Numerical\ndf['Block'] = pd.factorize(df[\"Block\"])[0]\ndf['IUCR'] = pd.factorize(df[\"IUCR\"])[0]\ndf['Description'] = pd.factorize(df[\"Description\"])[0]\ndf['Location Description'] = pd.factorize(df[\"Location Description\"])[0]\ndf['FBI Code'] = pd.factorize(df[\"FBI Code\"])[0]\ndf['Location'] = pd.factorize(df[\"Location\"])[0] ","339be4ae":"Target = 'Primary Type'\nprint('Target: ', Target)","a69610da":"# Plot Bar Chart visualize Primary Types\nplt.figure(figsize=(14,10))\nplt.title('Amount of Crimes by Primary Type')\nplt.ylabel('Crime Type')\nplt.xlabel('Amount of Crimes')\n\ndf.groupby([df['Primary Type']]).size().sort_values(ascending=True).plot(kind='barh')\n\nplt.show()","f56846b2":"# At previous plot, we could see that the classes is quite imbalance\n# Therefore, we are going to group several less occured Crime Type into 'Others' to reduce the Target Class amount\n\n# First, we sum up the amount of Crime Type happened and select the last 13 classes\nall_classes = df.groupby(['Primary Type'])['Block'].size().reset_index()\nall_classes['Amt'] = all_classes['Block']\nall_classes = all_classes.drop(['Block'], axis=1)\nall_classes = all_classes.sort_values(['Amt'], ascending=[False])\n\nunwanted_classes = all_classes.tail(13)\nunwanted_classes","5722d8f5":"# After that, we replaced it with label 'OTHERS'\ndf.loc[df['Primary Type'].isin(unwanted_classes['Primary Type']), 'Primary Type'] = 'OTHERS'\n\n# Plot Bar Chart visualize Primary Types\nplt.figure(figsize=(14,10))\nplt.title('Amount of Crimes by Primary Type')\nplt.ylabel('Crime Type')\nplt.xlabel('Amount of Crimes')\n\ndf.groupby([df['Primary Type']]).size().sort_values(ascending=True).plot(kind='barh')\n\nplt.show()","4d227ca4":"# Now we are left with 14 Class as our predictive class\nClasses = df['Primary Type'].unique()\nClasses","bd6db55d":"#Encode target labels into categorical variables:\ndf['Primary Type'] = pd.factorize(df[\"Primary Type\"])[0] \ndf['Primary Type'].unique()","a7062965":"# Feature Selection using Filter Method \n# Split Dataframe to target class and features\nX_fs = df.drop(['Primary Type'], axis=1)\nY_fs = df['Primary Type']\n\n#Using Pearson Correlation\nplt.figure(figsize=(20,10))\ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","d4e530f6":"#Correlation with output variable\ncor_target = abs(cor['Primary Type'])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.2]\nrelevant_features","ac0e58a2":"# At Current Point, the attributes is select manually based on Feature Selection Part. \nFeatures = [\"IUCR\", \"Description\", \"FBI Code\"]\nprint('Full Features: ', Features)","f4973e73":"#Split dataset to Training Set & Test Set\nx, y = train_test_split(df, \n                        test_size = 0.2, \n                        train_size = 0.8, \n                        random_state= 3)\n\nx1 = x[Features]    #Features to train\nx2 = x[Target]      #Target Class to train\ny1 = y[Features]    #Features to test\ny2 = y[Target]      #Target Class to test\n\nprint('Feature Set Used    : ', Features)\nprint('Target Class        : ', Target)\nprint('Training Set Size   : ', x.shape)\nprint('Test Set Size       : ', y.shape)","59b6dc2c":"# Random Forest\n# Create Model with configuration\nrf_model = RandomForestClassifier(n_estimators=70, # Number of trees\n                                  min_samples_split = 30,\n                                  bootstrap = True, \n                                  max_depth = 50, \n                                  min_samples_leaf = 25)\n\n# Model Training\nrf_model.fit(X=x1,\n             y=x2)\n\n# Prediction\nresult = rf_model.predict(y[Features])","39888f6b":"# Model Evaluation\nac_sc = accuracy_score(y2, result)\nrc_sc = recall_score(y2, result, average=\"weighted\")\npr_sc = precision_score(y2, result, average=\"weighted\")\nf1_sc = f1_score(y2, result, average='micro')\nconfusion_m = confusion_matrix(y2, result)\n\nprint(\"========== Random Forest Results ==========\")\nprint(\"Accuracy    : \", ac_sc)\nprint(\"Recall      : \", rc_sc)\nprint(\"Precision   : \", pr_sc)\nprint(\"F1 Score    : \", f1_sc)\nprint(\"Confusion Matrix: \")\nprint(confusion_m)","6e056890":"# Classification Report\n# Instantiate the classification model and visualizer\ntarget_names = Classes\nvisualizer = ClassificationReport(rf_model, classes=target_names)\nvisualizer.fit(X=x1, y=x2)     # Fit the training data to the visualizer\nvisualizer.score(y1, y2)       # Evaluate the model on the test data\n\nprint('================= Classification Report =================')\nprint('')\nprint(classification_report(y2, result, target_names=target_names))\n\ng = visualizer.poof()             # Draw\/show\/poof the data","dd0bdbdb":"# Neural Network\n# Create Model with configuration \nnn_model = MLPClassifier(solver='adam', \n                         alpha=1e-5,\n                         hidden_layer_sizes=(40,), \n                         random_state=1,\n                         max_iter=1000                         \n                        )\n\n# Model Training\nnn_model.fit(X=x1,\n             y=x2)\n\n# Prediction\nresult = nn_model.predict(y[Features]) ","29964d4e":"# Model Evaluation\nac_sc = accuracy_score(y2, result)\nrc_sc = recall_score(y2, result, average=\"weighted\")\npr_sc = precision_score(y2, result, average=\"weighted\")\nf1_sc = f1_score(y2, result, average='micro')\nconfusion_m = confusion_matrix(y2, result)\n\nprint(\"========== Neural Network Results ==========\")\nprint(\"Accuracy    : \", ac_sc)\nprint(\"Recall      : \", rc_sc)\nprint(\"Precision   : \", pr_sc)\nprint(\"F1 Score    : \", f1_sc)\nprint(\"Confusion Matrix: \")\nprint(confusion_m)","da415a18":"# Classification Report\n# Instantiate the classification model and visualizer\ntarget_names = Classes\nvisualizer = ClassificationReport(nn_model, classes=target_names)\nvisualizer.fit(X=x1, y=x2)     # Fit the training data to the visualizer\nvisualizer.score(y1, y2)       # Evaluate the model on the test data\n\nprint('================= Classification Report =================')\nprint('')\nprint(classification_report(y2, result, target_names=target_names))\n\ng = visualizer.poof()             # Draw\/show\/poof the data","19a67570":"# K-Nearest Neighbors\n# Create Model with configuration \nknn_model = KNeighborsClassifier(n_neighbors=3)\n\n# Model Training\nknn_model.fit(X=x1,\n             y=x2)\n\n# Prediction\nresult = knn_model.predict(y[Features]) ","69dfdd9e":"# Model Evaluation\nac_sc = accuracy_score(y2, result)\nrc_sc = recall_score(y2, result, average=\"weighted\")\npr_sc = precision_score(y2, result, average=\"weighted\")\nf1_sc = f1_score(y2, result, average='micro')\nconfusion_m = confusion_matrix(y2, result)\n\nprint(\"========== K-Nearest Neighbors Results ==========\")\nprint(\"Accuracy    : \", ac_sc)\nprint(\"Recall      : \", rc_sc)\nprint(\"Precision   : \", pr_sc)\nprint(\"F1 Score    : \", f1_sc)\nprint(\"Confusion Matrix: \")\nprint(confusion_m)","fe00fee4":"# Classification Report\n# Instantiate the classification model and visualizer\ntarget_names = Classes\nvisualizer = ClassificationReport(knn_model, classes=target_names)\nvisualizer.fit(X=x1, y=x2)     # Fit the training data to the visualizer\nvisualizer.score(y1, y2)       # Evaluate the model on the test data\n\nprint('================= Classification Report =================')\nprint('')\nprint(classification_report(y2, result, target_names=target_names))\n\ng = visualizer.poof()             # Draw\/show\/poof the data","e6fb13d7":"# Ensemble Voting Model\n# Combine 3 Models to create an Ensemble Model\n\n# Create Model with configuration\neclf1 = VotingClassifier(estimators=[('knn', knn_model), ('rf', rf_model), ('nn', nn_model)], \n                         weights=[1,1,1],\n                         flatten_transform=True)\neclf1 = eclf1.fit(X=x1, y=x2)   \n\n# Prediction\nresult = eclf1.predict(y[Features])","695e0186":"# Model Evaluation\nac_sc = accuracy_score(y2, result)\nrc_sc = recall_score(y2, result, average=\"weighted\")\npr_sc = precision_score(y2, result, average=\"weighted\")\nf1_sc = f1_score(y2, result, average='micro')\nconfusion_m = confusion_matrix(y2, result)\n\nprint(\"============= Ensemble Voting Results =============\")\nprint(\"Accuracy    : \", ac_sc)\nprint(\"Recall      : \", rc_sc)\nprint(\"Precision   : \", pr_sc)\nprint(\"F1 Score    : \", f1_sc)\nprint(\"Confusion Matrix: \")\nprint(confusion_m)","5ac858f9":"# Classification Report\n# Instantiate the classification model and visualizer\ntarget_names = Classes\nvisualizer = ClassificationReport(eclf1, classes=target_names)\nvisualizer.fit(X=x1, y=x2)     # Fit the training data to the visualizer\nvisualizer.score(y1, y2)       # Evaluate the model on the test data\n\nprint('================= Classification Report =================')\nprint('')\nprint(classification_report(y2, result, target_names=target_names))\n\ng = visualizer.poof()             # Draw\/show\/poof the data","bef36d0b":"**Further Elaboration of Correlation**\n\nThe correlation coefficient has values between -1 to 1\n* A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n* A value closer to 1 implies stronger positive correlation\n* A value closer to -1 implies stronger negative correlation","4cda1c8f":"**In this notebook, we demonstrate the application of basic Machine Learning Models and some basic Model Evaluation Metrics to perform classification task with Chicago Crime Dataset**\n\nIn general, An Ensemble Voting Model consist of Random Forest, Neural Network and KNN are used for the classification task of predicting the Type of Crime. Feel free to drop a comment and feedback.","713c9e59":"**Machine Learning Modelling**"}}