{"cell_type":{"502464ef":"code","69d00a69":"code","a3b0c9da":"code","b5b75269":"code","4d1ab6e6":"code","70858cdc":"code","7839006b":"code","01077854":"code","5bc19b46":"code","86ef4598":"code","e2adf026":"code","121abd83":"code","7eb44b8d":"code","1f934e41":"code","79b2e363":"code","90de4c7d":"code","6f14f0b2":"code","0d2dbb54":"code","e45064d4":"code","03126699":"code","c7ed4af4":"code","8af6547d":"code","ac7b17b3":"code","1ec8ddce":"code","2f0515b0":"code","8665c331":"code","50547a65":"code","10e2aab0":"code","6aadc9d6":"code","1a169191":"code","7e03c9b5":"code","516b6423":"code","80b76dac":"code","81a70ec6":"code","23cd9c1d":"code","459e5dac":"code","f8f63288":"code","c2bc80d5":"code","0ea70ea6":"code","055c6c4e":"code","c4f02dec":"code","d5484106":"code","726659b9":"code","3d843661":"code","fe14e739":"code","5902c6ab":"code","d29254d5":"code","1f380574":"code","06166a1d":"markdown","a6469b87":"markdown","c3501963":"markdown","02cd42b0":"markdown"},"source":{"502464ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","69d00a69":"data = pd.read_csv('..\/input\/cancer\/cancer.csv') #CSV dosyas\u0131 olan datay\u0131 okuma.","a3b0c9da":"data.head() #Datan\u0131n ilk 5'ini getirir.","b5b75269":"data.tail() #datan\u0131n son 5'ini getirir.","4d1ab6e6":"data.info() #Data hakk\u0131nda bilgiler almak i\u00e7in.","70858cdc":"data.describe() #Datadaki verilerin her column i\u00e7in ayr\u0131nt\u0131l\u0131 hesaplamalar. ","7839006b":"data.drop(['Unnamed: 32'],axis=1,inplace=True) # Verilerin hespi NaN oldu\u011fu i\u00e7in bize bir katk\u0131s\u0131 yok.(axis=1 t\u00fcm column sil demek)","01077854":"data.drop([\"id\"],axis=1,inplace=True) #classification yaparken idnin bize katk\u0131s\u0131 olmad\u0131\u011f\u0131 i\u00e7in verisetimizden c\u0131k\u0131r\u0131yoruz.","5bc19b46":"data.diagnosis = [1 if each==\"M\" else 0 for each in data.diagnosis]\n#verilerimiz ya kategorical yada numeric olmal\u0131 classification yapabilmek i\u00e7in. Bu y\u00fczden numerice \u00e7eviriyoruz.","86ef4598":"data.info() #diagnosis object veri tipinden integer tipine d\u00f6\u00fc\u015ft\u00fc.","e2adf026":"y = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis= 1) #normalizasyon yap\u0131lmam\u0131\u015f hali","121abd83":"x_data.shape","7eb44b8d":"y.shape","1f934e41":"# Classification yaparken baz\u0131 de\u011ferlerin etkisi geri planda kalmas\u0131n diye verisetimize normalization i\u015flemi uyguluyoruz.\nx = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values\nx","79b2e363":"def bar_plot(variable):\n    \"\"\"\n    input: variable ex: \"radius_mean\"\n    output: bar plot & value count\n    \"\"\"\n    #get feature\n    var = data[variable]\n    #count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    #visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index,varValue) #x = 0 or 1 y = varValue\n    plt.xticks(varValue.index,varValue.index.values)\n    plt.ylabel('Frequency')\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}:\".format(variable,varValue))","90de4c7d":"category1 = ['radius_mean']\nfor c in category1:\n    bar_plot(c)","6f14f0b2":"data[['radius_mean','diagnosis']].groupby(['radius_mean']).mean()\n\n","0d2dbb54":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state=42)","e45064d4":"x_train.shape #(455,32)\nx_test.shape #(114,32)\ny_train.shape #(455,)\ny_test.shape #(114,)","03126699":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 4)\nknn.fit(x_train,y_train)\nprint(\"KNN Score : \",knn.score(x_test,y_test))","c7ed4af4":"y_pred = knn.predict(x_test)\nfrom sklearn.metrics import f1_score,accuracy_score\nKNN = f1_score(y_test,y_pred)\nKNN","8af6547d":"score_list = []\nfor each in range(1,15):\n    knn_deneme = KNeighborsClassifier(n_neighbors = each)\n    knn_deneme.fit(x_train,y_train)\n    score_list.append(knn_deneme.score(x_test,y_test))\nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"Accuracy\")\nplt.show()","ac7b17b3":"tahminlerimiz = knn.predict(x_test)\ntahminlerimiz","1ec8ddce":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,tahminlerimiz))","2f0515b0":"print(confusion_matrix(y_test,tahminlerimiz))","8665c331":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=2, random_state=None)\nkmeans.fit(x_train,y_train)\ny_pred_kmeans = kmeans.predict(x_test)\nKMeans = f1_score(y_test,y_pred_kmeans)\nKMeans","50547a65":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_pred_kmeans,y_test))","10e2aab0":"from sklearn.cluster import KMeans\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(x_train)\n    # inertia method returns wcss for that model\n    wcss.append(kmeans.inertia_)","6aadc9d6":"import seaborn as sns\nplt.figure(figsize=(10,5))\nsns.lineplot(range(1, 11), wcss,marker='o',color='red')\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","1a169191":"\nkmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)\ny_kmeans = kmeans.fit_predict(x)\nprint(accuracy_score(y_kmeans,y))\nkmeans = (accuracy_score(y_kmeans,y))","7e03c9b5":"import seaborn as sns\n\nclassifer_results = [KNN,kmeans]\nclassifer_result = pd.DataFrame({'CLassification Results': classifer_results, 'ML Models': [\n              'KNeighborsClassifier','KMeans'\n              \n                ] })\nclassifer_result","516b6423":"g = sns.barplot('CLassification Results','ML Models',data = classifer_result)\ng.set_xlabel('Mean Score')\ng.set_title('CLassification Results ')\nplt.show()","80b76dac":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=42,max_iter=150)\nlr.fit(x_train,y_train)\nprint(\"Logistic Regression Score : \" ,lr.score(x_test,y_test))","81a70ec6":"y_pred = lr.predict(x_test)\nfrom sklearn.metrics import f1_score,accuracy_score\nLogicticRegression = f1_score(y_test,y_pred)\nLogicticRegression","23cd9c1d":"LogicticRegression1 = accuracy_score(y_test,y_pred)\nLogicticRegression1","459e5dac":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"Decision Tree Score : \", dt.score(x_test,y_test))","f8f63288":"y_pred = dt.predict(x_test)\nfrom sklearn.metrics import f1_score,accuracy_score\nDecisionTree = f1_score(y_test,y_pred)\nDecisionTree\n","c2bc80d5":"DecisionTree1 = accuracy_score(y_test,y_pred)\nDecisionTree1","0ea70ea6":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100,random_state =1)\nrf.fit(x_train,y_train)\nprint(\"Random Forest Score : \", rf.score(x_test,y_test))","055c6c4e":"y_pred = rf.predict(x_test)\nfrom sklearn.metrics import f1_score,accuracy_score\nRandomForest = f1_score(y_test,y_pred)\nRandomForest","c4f02dec":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"Naive Bayes Score : \", nb.score(x_test,y_test))","d5484106":"y_pred = nb.predict(x_test)\nfrom sklearn.metrics import f1_score,accuracy_score\nNaiveBayes = f1_score(y_test,y_pred)\nNaiveBayes","726659b9":"import seaborn as sns\n\nclassifer_results = [LogicticRegression,DecisionTree,RandomForest,NaiveBayes,KNN]\nclassifer_result = pd.DataFrame({'CLassification Results': classifer_results, 'ML Models': ['LogisticRegression',\n              'DecisionTreeClassifier',\n              'RandomForestClassifier',\n              'NaiveBayes',\n              'KNeighborsClassifier'\n              \n                ] })\nclassifer_result","3d843661":"g = sns.barplot('CLassification Results','ML Models',data = classifer_result)\ng.set_xlabel('Mean Score')\ng.set_title('CLassification Results ')\nplt.show()","fe14e739":"dt_param_grid = {'min_samples_split': range(10,500,20),\n                'max_depth': range(1,20,2)}\nknn_param_grid = {'n_neighbors': np.linspace(1,19,10, dtype = int).tolist(),\n                 'weights': ['uniform','distance'],\n                 'metric': ['euclidean','manhattan']}\nknn_param_grid","5902c6ab":"tahminlerimiz = knn.predict(x_test)\ntahminlerimiz","d29254d5":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,tahminlerimiz))","1f380574":"print(confusion_matrix(y_test,tahminlerimiz))","06166a1d":"# **EKSTRA KMEANS**","a6469b87":"# **G\u00f6rselle\u015ftirme**","c3501963":"# **EKSTRA Confussion Matrix**","02cd42b0":"# **Di\u011fer Classification Y\u00f6ntemleri**"}}