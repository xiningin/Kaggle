{"cell_type":{"38bdb454":"code","f6eb3bba":"code","a7ea9bcb":"code","207597fa":"code","e4a4576b":"code","df9a96db":"code","825e3153":"code","90c88b5b":"code","f820d6de":"code","3de55a23":"code","7c57a88e":"code","499d5d29":"code","82a9f785":"code","fe2962c2":"code","c10048eb":"code","146c3857":"code","5c85b8a1":"code","16d5f102":"code","65a7ea79":"code","429e72a5":"code","965bf523":"code","800bf8c6":"code","cd4adac4":"code","60850d92":"code","b5b00c31":"code","6f5841b3":"code","98603229":"code","a23393fd":"code","13988f02":"code","1d6d5b47":"code","03a207d4":"code","8699e4ea":"code","bab02fc3":"code","4b5d01f3":"markdown","54bfe7a5":"markdown","eb417f13":"markdown","1325b3a1":"markdown","c6b70c62":"markdown","ada15f56":"markdown","c4828936":"markdown"},"source":{"38bdb454":"#Importing libraries\nimport nltk, re, pprint\nimport numpy as np\nimport pandas as pd\nimport requests\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint, time\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize","f6eb3bba":"# reading the Treebank tagged sentences\nwsj = list(nltk.corpus.treebank.tagged_sents())","a7ea9bcb":"# first few tagged sentences\nwsj[:3]","207597fa":"# Splitting into train and test\nrandom.seed(1234)\ntrain_set, test_set = train_test_split(wsj,test_size=0.2)\n\nprint(len(train_set))\nprint(len(test_set))\nprint(train_set[:40])","e4a4576b":"# Getting list of tagged words from train set\ntrain_tagged_words = [(word,tag) for sent in train_set for word,tag in sent]\nlen(train_tagged_words)","df9a96db":"# tokens in train set\ntokens = [word for word,tag in train_tagged_words]\ntokens[:10]","825e3153":"# unique vocabulary from train set\nV = set(tokens)\nprint(len(V))","90c88b5b":"# number of tags in train set\nT = set([tag for word,tag in train_tagged_words])\nlen(T)","f820d6de":"# . -> represent the start of a sentence\n# $ -> represent the end of the sentence\nprint(T)","3de55a23":"t = len(T)\nv = len(V)\nw_given_t = np.zeros((t, v))\nw_given_t.shape","7c57a88e":"# compute word given tag: Emission Probability\n# p(w|t) = (#word w tagged with tag t in the corpus) \/ (#tag t appearing in the corpus)\n\ndef word_given_tag(word, tag, train_bag = train_tagged_words):\n    tag_list = [(w,t) for w,t in train_bag if t==tag]\n    count_tag = len(tag_list) # count of tag t present in the corpus\n    w_given_tag_list = [w for w,t in tag_list if w==word] #word w with the tag t present in the corpus\n    count_w_given_tag = len(w_given_tag_list) #count of word w with the tag t in the corpus\n    \n    return (count_w_given_tag, count_tag)","499d5d29":"# examples\n\n# large\nprint(\"\\n\", \"large\")\nprint(word_given_tag('large', 'JJ'))\nprint(word_given_tag('large', 'VB'))\nprint(word_given_tag('large', 'NN'), \"\\n\")\n\n# will\nprint(\"\\n\", \"will\")\nprint(word_given_tag('will', 'MD'))\nprint(word_given_tag('will', 'NN'))\nprint(word_given_tag('will', 'VB'))\n\n# book\nprint(\"\\n\", \"book\")\nprint(word_given_tag('book', 'NN'))\nprint(word_given_tag('book', 'VB'))\n\n# Android\nprint(\"\\n\", \"android\")\nprint(word_given_tag('android', 'NN'))\n","82a9f785":"\"\"\"word_with_tag_matrix = np.zeros((len(T), len(V)), dtype='float32')\nfor i, t in enumerate(list(T)):\n    for j, w in enumerate(list(V)): \n        word_with_tag_matrix[i, j] = word_given_tag(w, t)[0]\/word_given_tag(w, t)[1]\"\"\"","fe2962c2":"# convert the matrix to a df for better readability\n#word_with_tags_df = pd.DataFrame(word_with_tag_matrix, columns = list(V), index=list(T))\n#word_with_tags_df","c10048eb":"# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n# p(t2|t1)= (#tag t1 is followed by tag t2)\/ (#tag t1 appearing in corpus)\n\ndef t2_given_t1(t2, t1, train_bag = train_tagged_words):\n    tags = [t for w,t in train_bag] #get all the tags from training set\n    count_t1 = len([t for t in tags if t==t1]) #count of t1 appearing in the corpus\n    count_t2_t1 = 0  #count of t2 coming after t1 -> t1 followed by t2\n    for index in range(len(tags)-1):\n        if tags[index]==t1 and tags[index+1] == t2:\n            count_t2_t1 += 1 #increment count if t1 is followed by t2\n    return (count_t2_t1, count_t1)","146c3857":"def t2_given_t1_prob(t2,t1,train_bag = train_tagged_words):\n    count_t2_t1, count_t1 = t2_given_t1(t2,t1,train_bag)\n    return count_t2_t1\/count_t1","5c85b8a1":"# examples\nprint(t2_given_t1(t2='NNP', t1='JJ'))\nprint(t2_given_t1('NN', 'JJ'))\nprint(t2_given_t1('NN', 'DT'))\nprint(t2_given_t1('NNP', 'VB'))\nprint(t2_given_t1(',', 'NNP'))\nprint(t2_given_t1('PRP', 'PRP'))\nprint(t2_given_t1('VBG', 'NNP'))\nprint(t2_given_t1('VB', 'MD'))","16d5f102":"#Please note P(tag|start) is same as P(tag|'.')\nprint(t2_given_t1('DT', '.'))\nprint(t2_given_t1('VBG', '.'))\nprint(t2_given_t1('NN', '.'))\nprint(t2_given_t1('NNP', '.'))","65a7ea79":"# creating t x t transition matrix of tags\n# each column is t2, each row is t1\n# thus M(i, j) represents P(tj given ti)\n\ntags_matrix = np.zeros((len(T), len(T)), dtype='float32')\nfor i, t1 in enumerate(list(T)): \n    for j, t2 in enumerate(list(T)): \n        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]\/t2_given_t1(t2, t1)[1]","429e72a5":"# convert the matrix to a df for better readability\ntags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))\ntags_df #row->t1, col->t2","965bf523":"# Let's see the prob for tags appearing at start of the sentence represented by tag .\ntags_df.loc['.', :]","800bf8c6":"# heatmap of tags matrix\n# T(i, j) means P(tag j given tag i)\nplt.figure(figsize=(18, 12))\nsns.heatmap(tags_df)\nplt.show()\n","cd4adac4":"# frequent tags\n# filter the df to get P(t2, t1) > 0.5\ntags_frequent = tags_df[tags_df>0.5]\nplt.figure(figsize=(18, 12))\nsns.heatmap(tags_frequent)\nplt.show()","60850d92":"len(train_tagged_words)","b5b00c31":"# Viterbi Heuristic\ndef Viterbi(words, train_bag = train_tagged_words):\n    state = [] #state\/tag for each word\n    T = list(set([tag for word,tag in train_bag])) #tags in the corpus\n    \n    for index, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        state_probalities = [] #prob for each state\/word in corpus for each word\n        for t2 in T:\n            if index == 0:\n                transition_p = t2_given_t1_prob(t2, '.') #transition prob. for start tag\n            else:\n                t1 = state[-1]\n                transition_p = t2_given_t1_prob(t2,t1) #transition prob. for tag t1 followed by t2\n                \n            # compute emission and state probabilities\n            emission_p = word_given_tag(words[index], t2)[0]\/word_given_tag(words[index], t2)[1]  # p(w|tag) -> count of word with tag t2 \/ total number of t2\n            state_probalities.append(emission_p * transition_p) \n            \n        # getting state for which probability is maximum\n        state_with_max_prob = T[state_probalities.index(max(state_probalities))] \n        state.append(state_with_max_prob)\n    return list(zip(words, state))\n\n","6f5841b3":"# Running on entire test dataset would take more than 3-4hrs. \n# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n\nrandom.seed(100)\n\n# choose random 5 sents index from test set\nrndom_index = [random.randint(1,len(test_set)) for x in range(50)]\n\n# get the 5 sent from test set using the 5 random_index we picked above\ntest_run = [test_set[i] for i in rndom_index]\n\n# list of tagged words - this we will use for evaluation purpose\ntest_run_base = [(word,tag) for sent in test_run for word,tag in sent]\n\n# list of untagged words\ntest_words = [word for word,tag in test_run_base] \ntest_words","98603229":"# tagging the test sentences\nstart = time.time()\ntagged_seq = Viterbi(test_words)\nend = time.time()\ndifference = end-start","a23393fd":"print(\"Time taken in seconds: \", difference)\nprint(tagged_seq)\n#print(test_run_base)","13988f02":"# accuracy\ncheck = [(i,j) for i, j in zip(tagged_seq, test_run_base) if i == j] \naccuracy = len(check)\/len(tagged_seq)\naccuracy","1d6d5b47":"incorrect_tagged_cases = [(test_run_base[tagged_seq.index(i)-1],i,j) for i, j in zip(tagged_seq, test_run_base) if i != j] \nincorrect_tagged_cases","03a207d4":"## Testing\nsentence_test = 'Twitter is the best networking social site. Man is a social animal. Data science is an emerging field. Data science jobs are high in demand.'\nwords = word_tokenize(sentence_test)\n\nstart = time.time()\ntagged_seq = Viterbi(words)\nend = time.time()\ndifference = end-start","8699e4ea":"print(tagged_seq)\nprint(difference)","bab02fc3":"sentence = \"Donald Trump is the current President of US. Before entering politics, he was a domineering businessman and television personality.\"\nwords = word_tokenize(sentence)\n\ntagged_seq = Viterbi(words)\ntagged_seq","4b5d01f3":"## POS Tagging, HMMs, Viterbi\n\nLet's learn how to do POS tagging by Viterbi Heuristic using tagged Treebank corpus. Before going through the code, let's first understand the pseudo-code for the same. \n\n1. Tagged Treebank corpus is available (Sample data to training and test data set)\n   - Basic text and structure exploration\n2. Creating HMM model on the tagged data set.\n   - Calculating Emission Probabaility: P(observation|state)\n   - Calculating Transition Probability: P(state2|state1)\n3. Developing algorithm for Viterbi Heuristic\n4. Checking accuracy on the test data set\n\n\n## 1. Exploring Treebank Tagged Corpus","54bfe7a5":"### Steps for Viterbi algorithm\n1. Calculate Transition probability - p(tag|prev tag) <br>\n   - If the word is at the start of the sentence i.e at index 0 :-> t2 given start -> p(t2|start) -> p(t2|.) -> for all t2 in corpus\n   - If word is not at the start of the sentence that is not at index 0 :-> t2 given t1 -> p(t2|prev state) -> p(t2|(tag of previous word in sen)) -> p(t2|state[-1]) -> for all t2 in corpus\n2. Calculate Emission probability :-> p(word|tag) -> for all tags in the corpus\n3. Calculate state probability :-> p(tag|word) -> emission_p * transition_p \n4. pick the state which have maximum state probability for each word in the sentence\n\nWe basically iterate over the words collection (comes from sentence) and calculate the transition and emission probability for each tag in the corpus (iterating over tags) and based on these two probabilities we record the state probabilities and picks the tag which have the maximun state probility for the given word.","eb417f13":"## 2. POS Tagging Algorithm - HMM\n\nWe'll use the HMM algorithm to tag the words. Given a sequence of words to be tagged, the task is to assign the most probable tag to the word. \n\nIn other words, to every word w, assign the tag t that maximises the likelihood P(t\/w). Since P(t\/w) = P(w\/t). P(t) \/ P(w), after ignoring P(w), we have to compute P(w\/t) and P(t).\n\n\nP(w\/t) is basically the probability that given a tag (say NN), what is the probability of it being w (say 'building'). This can be computed by computing the fraction of all NNs which are equal to w, i.e. \n\nP(w\/t) = count(w, t) \/ count(t). \n\n\nThe term P(t) is the probability of tag t, and in a tagging task, we assume that a tag will depend only on the previous tag. In other words, the probability of a tag being NN will depend only on the previous tag t(n-1). So for e.g. if t(n-1) is a JJ, then t(n) is likely to be an NN since adjectives often precede a noun (blue coat, tall building etc.).\n\n\nGiven the penn treebank tagged dataset, we can compute the two terms P(w\/t) and P(t) and store them in two large matrices. The matrix of P(w\/t) will be sparse, since each word will not be seen with most tags ever, and those terms will thus be zero. \n","1325b3a1":"## 3. Viterbi Algorithm\n\nLet's now use the computed probabilities P(w, tag) and P(t2, t1) to assign tags to each word in the document. We'll run through each word w and compute P(tag\/w)=P(w\/tag).P(tag) for each tag in the tag set, and then assign the tag having the max P(tag\/w).\n\nWe'll store the assigned tags in a list of tuples, similar to the list 'train_tagged_words'. Each tuple will be a (token, assigned_tag). As we progress further in the list, each tag to be assigned will use the tag of the previous token.\n\nNote: P(tag|start) = P(tag|'.') ","c6b70c62":"### Transition Probabilities","ada15f56":"## 4. Evaluating on Test Set","c4828936":"### Emission Probabilities"}}