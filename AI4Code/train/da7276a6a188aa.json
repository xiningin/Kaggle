{"cell_type":{"07fa65c7":"code","f0c3f3d3":"code","35c33a6f":"code","afa46318":"code","2e30bd00":"code","78592dea":"code","40818369":"code","ad59d9fa":"code","c79de423":"code","3f6da43f":"code","80eac2ca":"code","90c4d0e7":"code","4dafb8be":"code","2c6e8f46":"code","3e946496":"code","9b5fed42":"code","e82d10d7":"code","871ddf19":"code","ad7d9611":"code","75a89374":"code","5fcb1aa7":"code","4ba68723":"code","578983c4":"code","02cd2d88":"code","49a4d737":"markdown","df29f575":"markdown","8c4480ea":"markdown","19531e67":"markdown","cb49f25d":"markdown","78cb5ed3":"markdown","306faa58":"markdown","20e30387":"markdown","655f3db5":"markdown","1f712052":"markdown","4468d0e2":"markdown","dbf98706":"markdown","67ac5c63":"markdown","4b806d7b":"markdown","d5e22a13":"markdown","30263156":"markdown","14e2520c":"markdown","4aaa6ff7":"markdown","04d814b6":"markdown","e5ac7d53":"markdown","6ad5a07c":"markdown","b24659a4":"markdown","14bdf6b0":"markdown"},"source":{"07fa65c7":"#Imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler, OneHotEncoder\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import RobustScaler\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)","f0c3f3d3":"#Pandas Adjusments\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 20)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.width', 170)","35c33a6f":"#Reading The Dataset\ndf = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\ndf.head()","afa46318":"def check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\ncheck_df(df)","2e30bd00":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n    \ncat_cols, num_cols, cat_but_car = grab_col_names(df)","78592dea":"def target_summary_with_num(dataframe, target, numerical_col):\n    print(dataframe.groupby(target).agg({numerical_col: \"mean\"}), end=\"\\n\\n\\n\")\nfor col in num_cols:\n    target_summary_with_num(df,'Salary',col)","40818369":"def num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n\n    if plot: \n        dataframe[numerical_col].hist(bins=20)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show()\nnum_summary(df, num_cols)","ad59d9fa":"def target_summary_with_cat(dataframe, target, categorical_col):\n    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")\n\ntarget_summary_with_cat(df,'Salary',cat_cols)","c79de423":"def outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\nfor col in num_cols:\n    print(col, check_outlier(df, col))","3f6da43f":"df.loc[(df['Years'] <= 5), 'EXPERIENCE'] = 'Rookie'\ndf.loc[(df['Years'] > 5) & (df['Years'] <= 10), 'EXPERIENCE'] = 'Incipient'\ndf.loc[(df['Years'] > 10) & (df['Years'] <= 15), 'EXPERIENCE'] = 'Average'\ndf.loc[(df['Years'] > 15) & (df['Years'] <= 20), 'EXPERIENCE'] = 'Experienced'\ndf.loc[(df['Years'] > 20), 'EXPERIENCE'] = 'Senior'\ndf[\"OrtCHmRun\"] = df[\"CHmRun\"] \/ df[\"Years\"]\ndf[\"WalksYears\"] = df[\"Walks\"] \/ df[\"Years\"]\ndf[\"AvgCHmRun\"] = df[\"HmRun\"] \/ df[\"CHmRun\"]\ndf[\"Errors\"] = df[\"Errors\"] + 0.00001\ndf[\"CHmRun\"] = df[\"CHmRun\"] + 0.00001\ndf[\"RBI\"] = df[\"RBI\"] + 0.00001\ndf[\"PutOuts\"] = df[\"PutOuts\"] + 0.00001\ndf[\"Walks\"] = df[\"Walks\"] + 0.00001\ndf[\"AssistsErrors\"] = df[\"Assists\"] \/ df[\"Errors\"]\ndf[\"HmRunCHmRun\"] = df[\"HmRun\"] \/ df[\"CHmRun\"]\ndf[\"AtBatPutOuts\"] = df[\"AtBat\"] \/ df[\"PutOuts\"]\n\ndf[\"BattingAverage\"] = df[\"CHits\"] \/ df[\"CAtBat\"]\ndf[\"TotalBases\"] =  ((df[\"CHits\"] * 2) + (4 * df[\"CHmRun\"]))\ndf[\"SluggingPercentage\"] = df[\"TotalBases\"] \/ df[\"CAtBat\"]\ndf[\"ISO\"] = df[\"SluggingPercentage\"] - df[\"BattingAverage\"]\ndf[\"TripleCrown\"] = (df[\"CHmRun\"] * 0.4) + (df[\"CRBI\"] * 0.25) + (df[\"BattingAverage\"] * 0.35)\ndf[\"BattingAverageOnBalls\"] = (df[\"CHits\"] - df[\"CHmRun\"]) \/ (df[\"CAtBat\"] - df[\"CHmRun\"])\ndf[\"RunsCreated\"] = df[\"TotalBases\"] * (df[\"CHits\"] + df[\"CWalks\"]) \/ (df[\"CAtBat\"] + df[\"CWalks\"])\ndf[\"FieldingPercentage\"] = 1 - ((df[\"PutOuts\"] + df[\"Assists\"]) \/ (df[\"PutOuts\"] + df[\"Assists\"] + df[\"Errors\"] + 1))\n\ndf[\"New_CRunsYearsRatio\"] = df[\"CRuns\"] \/ df[\"Years\"]\ndf['New_PutOutsYears'] = df['PutOuts'] * df['Years']\ndf[\"New_RBIWalks\"] = df[\"RBI\"] * df[\"Walks\"]\ndf[\"New_RBIWalksRatio\"] = df[\"RBI\"] \/ df[\"Walks\"]\ndf[\"New_CHmRunCAtBatRatio\"] = df[\"CHmRun\"] \/ df[\"CAtBat\"]\n\ndf.head()","80eac2ca":"df.corr()","90c4d0e7":"def high_correlated_cols(dataframe, plot=False, corr_th=0.90):\n    corr = dataframe.corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n    if plot:\n        import seaborn as sns\n        import matplotlib.pyplot as plt\n        sns.set(rc={'figure.figsize': (15, 15)})\n        sns.heatmap(corr, cmap=\"RdBu\")\n        plt.show()\n    return drop_list\n\nhigh_correlated_cols(df, plot=True, corr_th=0.90)","4dafb8be":"df.dropna(inplace=True)\ndf.isnull().sum()\n#Now there are no NA values in the data.","2c6e8f46":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","3e946496":"def rare_encoder(dataframe, rare_perc, cat_cols):\n    rare_columns = [col for col in cat_cols if (dataframe[col].value_counts() \/ len(dataframe) < 0.01).sum() > 1]\n\n    for col in rare_columns:\n        tmp = dataframe[col].value_counts() \/ len(dataframe)\n        rare_labels = tmp[tmp < rare_perc].index\n        dataframe[col] = np.where(dataframe[col].isin(rare_labels), 'Rare', dataframe[col])\n\n    return dataframe\n\ndf = rare_encoder(df, 0.01,cat_cols)\n\ndef rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\nrare_analyser(df, \"Salary\", cat_cols)","9b5fed42":"def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\nohe_cols = [col for col in df.columns if 10 >= df[col].nunique() >= 2]\ndf = one_hot_encoder(df, ohe_cols,drop_first=True)\ndf.head()","e82d10d7":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","871ddf19":"rs = RobustScaler()\nnum_cols = [col for col in num_cols if col != 'Salary']\ndf[num_cols] = rs.fit_transform(df[num_cols])\ndf.head()","ad7d9611":"y = df[\"Salary\"]\nX = df.drop([\"Salary\"], axis=1)\nX","75a89374":"models = [('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('KNN', KNeighborsRegressor()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          ('SVR', SVR()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor()),\n          (\"CatBoost\", CatBoostRegressor(verbose=False))\n          ]","5fcb1aa7":"for name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")","4ba68723":"rf_params = {\"max_depth\": [5, 8, 15, None],\n             \"max_features\": [5, 7, \"auto\"],\n             \"min_samples_split\": [8, 15, 20],\n             \"n_estimators\": [200, 500, 1000]}\n\ngbm_params = {\"learning_rate\": [0.01, 0.1],\n              \"max_depth\": [3, 8],\n              \"n_estimators\": [500, 1000],\n              \"subsample\": [1, 0.5, 0.7]} \n\ncatboost_params = {\"iterations\": [200, 500],\n                   \"learning_rate\": [0.01, 0.1],\n                   \"depth\": [3, 6]}\n\nregressors = [(\"RF\", RandomForestRegressor(), rf_params),\n              ('GBM', GradientBoostingRegressor(), gbm_params),\n              ('CatBoost', CatBoostRegressor(verbose=False), catboost_params)]\nregressors\n","578983c4":"best_models = {}\nfor name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n\n    gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n\n    final_model = regressor.set_params(**gs_best.best_params_)\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n    best_models[name] = final_model\nbest_models","02cd2d88":"voting_reg = VotingRegressor(estimators=[('GBM', best_models[\"GBM\"]),\n                                         ('CatBoost', best_models[\"CatBoost\"])])\nvoting_reg.fit(X, y)\n\nnp.mean(np.sqrt(-cross_val_score(voting_reg, X, y, cv=10, scoring=\"neg_mean_squared_error\")))","49a4d737":"<a id=\"section-nine\"><\/a>\n# Creating The Model","df29f575":"<a id=\"section-four\"><\/a>\n# Outliers\n\n***This is the most important part of the \"Hitters\" salary prediction. There are several different ways to work on it. Now let's observe the outliers.***","8c4480ea":"****We have 20 variables including the \"Salary\" also, which is our target variable. But we have to decide categorical and numerical ones first. Thus, we can analyze them seperately and we can reshape them more accurate. For this, we're gonna use the following function.****","19531e67":"<a id=\"section-eight\"><\/a>\n# Scaling The Numerical Variables","cb49f25d":" ***Let's take a closer look to \"Hitters\" data and get to know it.***","78cb5ed3":"<a id=\"section-three\"><\/a>\n# Numerical Variable Analysis","306faa58":"***We see that there is no outlier in our data. This can be a bit of a disappointment, we already don't have any of the materials to improve a dataset with so few observations. But in the function we described above, we chose quantiles between 0.05 and 0.95, and a narrower range than that would mean distorting the data. The most rational thing we can do is to accept the situation :)***","20e30387":"***As we are used to, we encode our data before inserting it into the machine learning model.***\n*******************************************************************************************************************\n***The Rare Encoder will found the infrequent labels. Then it'll group them under the rare_labels list.***\n******************************************************************************************************************\n***With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector.***","655f3db5":"<a id=\"section-seven\"><\/a>\n# Rare Encoding & One-Hot Encoding","1f712052":"<a id=\"section-ten\"><\/a>\n# Hyperparameter Optimization\n","4468d0e2":"***We're in Feature Engineering, the most important part of an ML model in general, after taking a disappointing hit with outliers :). We already observed the relationship between \"Years\" and \"Salary\" in the preprocessing section. Then let's categorize the \"EXPERIENCE\" and try to get more accurate results. Also we created variables to take average values of our variables. We added 0.00001 some of them because as we saw in the preprocessing part, some of them have 0 values. So we avoided to crate infinite values. \nAfter that we did some research on baseball literature and we found important statistical titles which are; ***\n******************************************************************************************************************\n***-Triple Crown: In baseball, a player earns a Triple Crown when he leads a league in three specific statistical categories in the same season.***\n******************************************************************************************************************\n***-Slugging Percentage: Slugging percentage represents the total number of bases a player records per at-bat***\n******************************************************************************************************************\n***-Isolated Power(ISO): ISO measures the raw power of a hitter by taking only extra-base hits***\n******************************************************************************************************************\n***-Fielding Percantage(FPCT): The total number of putouts and assists by a defender, divided by the total number of chances (putouts, assists and errors).***\n******************************************************************************************************************","dbf98706":"***We saw clearly what type of variables we have.***\n\n***|Variables: 20|***\n***|cat_cols: 3|***\n***|num_cols: 17|***\n***|cat_but_car: 0|***\n***|num_but_cat: 0|***\n********************************************************************************************************************\n***Now let's observe the relation between numerical cols\/variables and the target\/dependent variable.***\n","67ac5c63":"***It can be very tempting to fill the NA values in the dependent variable with KNN because this will lower the error metric. But this will obviously corrupt the data. When we drop NA values, the RMSE values cannot be compared to those of the raw data. For this reason, we will drop the NA values(we had to).***","4b806d7b":"<a id=\"section-six\"><\/a>\n# Missing Values","d5e22a13":"<a id=\"section-five\"><\/a>\n# Feature Engineering","30263156":"![baseball.jpg](attachment:6321d937-e796-417d-b620-aeefeead71c2.jpg)\n***Photo: 1986 Mets***","14e2520c":"***Let's talk a little about our findings. First of all, we can say that variables such as \"years\", \"errors\", \"CWalks\" have positive-negative correlations with \"Salary\". But none of them are decisive on their own. Because the current value of a player cannot be understood only with individual statistics. We were already expecting this. Let's see how we can improve our prediction methods.***","4aaa6ff7":"<a id=\"section-one\"><\/a>\n# Introduction\n**This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.*                                                                       *A data frame with 322 observations of major league players on the following 20 variables.**\n****************************************\nAtBat: Number of times at bat in 1986\n****************************************\nHits: Number of hits in 1986\n****************************************\nHmRun: Number of home runs in 1986\n****************************************\nRuns: Number of runs in 1986\n****************************************\nRBI: Number of runs batted in in 1986\n****************************************\nWalks: Number of walks in 1986\n****************************************\nYears: Number of years in the major leagues\n****************************************\nCAtBat: Number of times at bat during his career\n****************************************\nCHits: Number of hits during his career\n****************************************\nCHmRun: Number of home runs during his career\n****************************************\nCRuns: Number of runs during his career\n****************************************\nCRBI: Number of runs batted in during his career\n****************************************\nCWalks: Number of walks during his career\n****************************************\nLeague: A factor with levels A and N indicating player's league at the end of 1986\n****************************************\nDivision: A factor with levels E and W indicating player's division at the end of 1986\n****************************************\nPutOuts: Number of put outs in 1986\n****************************************\nAssists: Number of assists in 1986\n****************************************\nErrors: Number of errors in 1986\n****************************************\nSalary: 1987 annual salary on opening day in thousands of dollars\n****************************************\nNewLeague: A factor with levels A and N indicating player's league at the beginning of 1987\n\n\n**In this project, we try to predict player's salaries using ML models. This is a very limited dataset with few observations(also the dependent variable has 59 NA values). And we know that predicting sportsman salary is also a very difficult concept but we'll do our best :)**","04d814b6":"***We will select the best of the models we have determined above and optimize their hyperparameters. Thus, we will get better results. We will use the well-known GridSearchCV algorithm for this.***","e5ac7d53":"***We added the highly correlated values \u200b\u200bto the drop_list. Since we will use tree methods, we are not sensitive to multicollinearity. However, the presence of these variables in the drop list gives us a more robust perspective.***","6ad5a07c":"<a id=\"section-eleven\"><\/a>\n# Ensemble Learning\n***Now we'll combine our models for ensemble learning, let's choose best two of them.***","b24659a4":"<a id=\"section-twelve\"><\/a>\n# CONCLUSION\n***We have reached the RMSE value of 235.54984403773665. This may be much lower in different projects, but we tried to interfere with the data as little as possible. Estimating a baseball player's salary is a difficult matter anyway, except that the dataset consists of few observations. Because although the statistics seem important, we do not know how weak the team that will pay the player is in that position. For this reason, we always witness that in professional competition, the player is paid salaries that are far below or above their worth. Still, if we had more comprehensive data, we could have made much better predictions. Thank you for reading. :)***\n\n***Also your recommendations are important to us. Please leave a comment.***\n","14bdf6b0":"<a id=\"section-two\"><\/a>\n# Preprocessing"}}