{"cell_type":{"6cef7830":"code","0f50efc0":"code","2894229a":"code","1e10e37f":"code","80d47b3d":"code","1a4a2c78":"code","ba3a0df2":"code","a1143fed":"code","7f059391":"code","e04add6c":"markdown","1152970c":"markdown","a9f6ceec":"markdown","df1df201":"markdown","a86cd19a":"markdown"},"source":{"6cef7830":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.decomposition import PCA,KernelPCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nimport warnings\n\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n    \n\n\n","0f50efc0":"# Importing the dataset\ndata = pd.read_csv('..\/input\/winequality-red.csv')\ndata.head(5)\n","2894229a":"reviews = []\nfor i in data['quality']:\n    if i >= 1 and i <= 3:\n        reviews.append('1')\n    elif i >= 4 and i <= 7:\n        reviews.append('2')\n    elif i >= 8 and i <= 10:\n        reviews.append('3')\ndata['Reviews'] = reviews\n\ndata.head()","1e10e37f":"x = data.iloc[:, 0:-2].values\ny = data.iloc[:, -1].values","80d47b3d":"sc_x=StandardScaler()\nx = sc_x.fit_transform(x)\n\npca=PCA()\nx_pca = pca.fit_transform(x)\nplt.figure(figsize=(10,10))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), 'ro-')\nplt.grid()","1a4a2c78":"pca_new = PCA(n_components=8)\nx_new = pca_new.fit_transform(x)\n","ba3a0df2":"\n# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(x_new, y, test_size = 0.2, random_state = 1)","a1143fed":"def classifier(model):\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    score=accuracy_score(y_pred,y_test)\n    return score*100","7f059391":"classifier(KNeighborsClassifier(n_neighbors=100)),classifier(RandomForestClassifier(n_estimators=100)),classifier(LogisticRegression()),classifier(GaussianNB())\n","e04add6c":"### Creatting the Model\n here we create pip line with Standard scaler step(1) and KernelPCA step(2) and logistic regression model step(3) we will change it later any way","1152970c":"### Splitting Data \nx = independent values\ny = depended values review is the only one depended yea it's review\n**notice:\nyou can select last index into coulmn index by : -1**","a9f6ceec":" ### A-Dimensionality reduction\nyou can read more about it here [Principal component](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis) , but simply you can reduce your Dimensions easly , the    dimensions is your columns count and this process try to reduce it with out effect with your data.\nPCA , PCA KERNEL and plotting explained_variance to see the best components number for setting !","df1df201":"Like we see the best for this Case is Random Forest classifier  like always  with estimators count = 100 it's great to see that 99,375% can you do more than that  with the same case lets see what you can do then ?\n\n**Notice: you can get a better result when you Learn how to tunning hyperparametric , Comment if there is thing i miss , and try to get more than that result :\"D**","a86cd19a":"#### for improving model performance we need to split all wines types in three category good,fine and bad"}}