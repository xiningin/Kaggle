{"cell_type":{"d4f7b0bd":"code","e9c41d17":"code","434c75e2":"code","f37f8039":"code","9205f351":"code","d1eb590c":"code","55402f91":"code","4c132ebc":"code","2a5536f3":"code","b60712ca":"code","1632e92f":"code","8fdae573":"code","36556263":"code","82fd16a0":"code","c526160f":"code","dac3ff73":"code","c13c6aef":"code","e6deeb33":"code","3329ed20":"code","073a8bd9":"markdown","f9bb2e7d":"markdown","3cbf7d7f":"markdown","935d4f8d":"markdown","1234910b":"markdown","9b2db58e":"markdown"},"source":{"d4f7b0bd":"!pip install nlpaug","e9c41d17":"import numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.sentence as nas\nimport nlpaug.flow as nafc\nfrom nlpaug.util import Action\n\nfrom transformers import (AutoModel, \n                          AutoModelForMaskedLM,\n                          AutoTokenizer,\n                          AutoConfig,\n                          AdamW,\n                          LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer,\n                          TrainingArguments)\n\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nimport warnings, os, gc, random, re \nwarnings.filterwarnings(\"ignore\")","434c75e2":"def set_seed(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(42)","f37f8039":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\ntest_df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")","9205f351":"df.head()","d1eb590c":"pretrain_text = pd.concat([df.less_toxic, df.more_toxic, test_df.text])\npretrain_text.drop_duplicates(inplace = True)\npretrain_text.reset_index(drop = True, inplace = True)","55402f91":"def clean(data):\n    # Clean some punctutations\n    data = data.str.replace('\\n', ' ')\n    data = data.str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters\n    data = data.str.replace(r'(\")\\1+',r'\\1')    \n    data = data.str.replace(r'([*!?\\'])\\1\\1+\\B',r'\\1\\1')    \n    data = data.str.replace(r'(\\w)\\1\\1+\\B',r'\\1\\1')    \n    data = data.str.replace(r'(\\w)\\1+\\b',r'\\1').str.strip()\n    return data","4c132ebc":"pretrain_text = clean(pretrain_text)\n\nwith open('text.txt','w') as f:\n    text  = '\\n'.join(pretrain_text.tolist())\n    f.write(text)","2a5536f3":"class cfg:\n    model_name = 'GroNLP\/hateBERT'\n    epochs = 3 # adjust\n    learning_rate = 5e-05\n    train_batch_size = 32\n    eval_batch_size = 32\n    eval_steps = 200\n    block_size = 256\n    gradient_accum_steps = 1\n    mlm_prob = 0.15\n    fp16 = True\n    output_dir = '.\/hatebert_mlm'","b60712ca":"model = AutoModelForMaskedLM.from_pretrained(cfg.model_name)\ntokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\ntokenizer.save_pretrained(cfg.output_dir);","1632e92f":"# Sequences are truncated to block size\ntrain_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\",\n    block_size=cfg.block_size)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\",\n    block_size=cfg.block_size)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, \n    mlm=True, \n    mlm_probability=cfg.mlm_prob)","8fdae573":"training_args = TrainingArguments(\n    output_dir=cfg.output_dir+'_chk',\n    overwrite_output_dir=True,\n    num_train_epochs=cfg.epochs,\n    per_device_train_batch_size=cfg.train_batch_size,\n    per_device_eval_batch_size=cfg.eval_batch_size,\n    learning_rate=cfg.learning_rate,\n    gradient_accumulation_steps=cfg.gradient_accum_steps,\n    fp16=cfg.fp16,\n    eval_steps=cfg.eval_steps,\n    evaluation_strategy='steps',\n    save_total_limit=2,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end=True,\n    prediction_loss_only=True,\n    report_to='none')","36556263":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset)","82fd16a0":"trainer.train()\ntrainer.save_model(cfg.output_dir)","c526160f":"example_text = pretrain_text[7500]\naug = naw.SynonymAug(aug_src='wordnet')\naugmented_text = aug.augment(example_text)\n\nprint(\"Original:\")\nprint(example_text)\nprint('\\n')\nprint(\"Augmented Text:\")\nprint(augmented_text)","dac3ff73":"back_translation_aug = naw.BackTranslationAug(\n    from_model_name='facebook\/wmt19-en-de', \n    to_model_name='facebook\/wmt19-de-en')","c13c6aef":"example_text = pretrain_text[200]\naugmented_text = back_translation_aug.augment(example_text)\n\nprint(\"Original:\")\nprint(example_text)\nprint('\\n')\nprint(\"Augmented Text:\")\nprint(augmented_text)","e6deeb33":"# substitute is peforming MLM augmentation\naug = naw.ContextualWordEmbsAug(model_path=cfg.output_dir,\n                                action='substitute',\n                                aug_p=0.15,\n                                device='cuda')","3329ed20":"example_text = pretrain_text[7]\naugmented_text = aug.augment(example_text)\n\nprint(\"Original:\")\nprint(example_text)\nprint('\\n')\nprint(\"Augmented Text:\")\nprint(augmented_text)","073a8bd9":"# MLM Pretraining\nWe'll perform MLM pretraining on our transformer model.  \nThis usually improves downstream performance when fine-tuning\/ensembling.  \nWe'll also explore using the pretrained model for contextual data augmentation  \n\nThis code was based on maunish's excellent CommonLit MLM notebook:    \nhttps:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-pretrain","f9bb2e7d":"# Data Augmentation\n`nlpaug` helps with generating synthetic data for augmentating NLP pipelines.  \nThe library also makes it simple to use context-aware augmentations such as MLM for sentence augmentation  \nWe'll go through some augmentation methods, before testing out our pretrained `HateBert` model  \n\nA list of supported augmentation strategies can be found in the documentation:  \nhttps:\/\/github.com\/makcedward\/nlpaug\/blob\/master\/example\/textual_augmenter.ipynb","3cbf7d7f":"## Back Translation","935d4f8d":"## Contextual (Word Embeddings) Augmentation","1234910b":"# Preprocessing and Cleaning\nHere we concatenate all the text data and remove any duplicate rows.  \nWe also clean some noise and punctuation from the input data  \n`\\n` characters are removed when cleaning, which is important when using `LineByLineTextDataset`","9b2db58e":"## Synonym Replacement"}}