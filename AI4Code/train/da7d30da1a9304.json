{"cell_type":{"09b3a675":"code","bfb55858":"code","8bb69d73":"code","70926d1b":"code","2a5eb8a2":"code","b5f39346":"code","0024d7a2":"code","695ac46c":"code","c8bd7dab":"code","1b060fa4":"code","80d52f4f":"code","ad861fb0":"code","75fbc81b":"code","5cd5baae":"code","65d5e28f":"code","f8cd71ea":"code","10fb323f":"code","1fdda93d":"code","3fa99eef":"code","e38f221a":"code","e4c91ff8":"code","7162ea4d":"code","f7f9fa65":"code","c20092e2":"code","7a39e73a":"code","3fa04e2b":"code","9be5de55":"code","82160c15":"code","175a4eb1":"code","0eccc9ae":"code","52de2416":"code","7769a8a8":"code","e04e7ee8":"code","30e0747c":"code","40a7f229":"code","9b89c020":"code","ca80e825":"code","921607c7":"code","223fe9f7":"code","29f05757":"code","5371d66f":"markdown","1e0a5bc7":"markdown","35326b09":"markdown","95643244":"markdown","8177a282":"markdown","f78cbb2c":"markdown","ec5a2eac":"markdown","40db7b88":"markdown","6267284f":"markdown","d0a62f2d":"markdown"},"source":{"09b3a675":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport gensim \nfrom nltk.tokenize import word_tokenize\nfrom sklearn.decomposition import TruncatedSVD\nfrom matplotlib.pyplot import figure\nfrom matplotlib.figure import Figure\nimport plotly\nimport plotly.graph_objs as go\nplotly.offline.init_notebook_mode(connected=True)\nimport re\nfrom stop_words import get_stop_words\nstop_words = get_stop_words('en')\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport seaborn as sb\nimport random\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","bfb55858":"data_mail = pd.read_csv('..\/input\/Emails.csv', header=0)\ndata_mail.head(5)","8bb69d73":"timeStamp_split = data_mail['MetadataDateSent'].str.split(\"T\")\nTimes = timeStamp_split.str[1]\n\ndata_mail['Dates'] = timeStamp_split.str[0]\ndata_mail['Times'] = Times.str.split(\"+\").str[0]\n\ndata_mail['Dates'] = pd.Series(data_mail['Dates'])\ndata_mail['Times'] = pd.Series(data_mail['Times'])\n\ndata_sub = data_mail[[\"MetadataSubject\", \"ExtractedBodyText\", \"MetadataTo\", \"MetadataFrom\",\"Times\",\"Dates\"]]\ndata_sub.head(5)","70926d1b":"print('The total number of emails sent is', len(data_sub))","2a5eb8a2":"figure(figsize =(20,6))\ndata_sub['MetadataTo'].value_counts()[0:19].plot('bar') #generate top 20 persons emails","b5f39346":"figure(figsize =(20,6))\ndata_sub['MetadataFrom'].value_counts()[0:19].plot('bar') #generate top 20 persons emails","0024d7a2":"figure(figsize =(30,6))\ncountDates = data_sub.Dates.groupby([data_sub.Dates]).agg(['count'])\ntype(countDates)\nlist(countDates)\ncountDates2= [go.Scatter(x = list(countDates.index), y=countDates['count'])]\n\nlayout = dict(title = 'Count mails per day',\n              xaxis= dict(title= 'year',ticklen= 10,zeroline= False)\n             )\nfig = dict(data = countDates2, layout = layout)\nplotly.offline.iplot(fig)","695ac46c":"## **Find similar words in corpus using Word2vec :**\n\n#Word2vec is a two-layer neural net that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. Word2vec is a prediction based model rather than frequency. It uses predictive analysis to make a weighted guess of a word co-occurring with respect to it\u2019s neighbouring words.","c8bd7dab":"data_rel  = data_sub[pd.notnull(data_sub['ExtractedBodyText'])]\nprint(data_rel.head(5)['ExtractedBodyText'])","1b060fa4":"moreThan4Words = data_rel['ExtractedBodyText'].str.split().apply(len) > 4\ndata_rel = data_rel[list(moreThan4Words)]\nlen(data_rel)","80d52f4f":"words = gensim.utils.simple_preprocess (str(data_rel['ExtractedBodyText']))\nwords2 = [word_tokenize(i) for i in data_rel['ExtractedBodyText']]","ad861fb0":" model = gensim.models.Word2Vec(\n            words2,\n            size=350,\n            window=10,\n            min_count=15,\n            workers=30)\nmodel.train(words, total_examples=len(words), epochs=10)\n\n#model.wv.vocab","75fbc81b":"#model.wv.most_similar(positive = 'Minister')","5cd5baae":"type(data_rel.ExtractedBodyText)\ndata_rel[\"ExtractedBodyText\"] = data_rel.ExtractedBodyText.apply(lambda x : str.lower(x))#tolower\ndata_rel[\"ExtractedBodyText\"] = data_rel[\"ExtractedBodyText\"].apply(lambda x : \" \".join(re.findall('[\\w]+',x)))#remove punctuation\n\ndef remove_stopWords(s):\n    '''For removing stop words\n    '''\n    s = ' '.join(word for word in s.split() if word not in stop_words)\n    return s\n\ndata_rel[\"ExtractedBodyText\"]= data_rel[\"ExtractedBodyText\"].apply(lambda x: remove_stopWords(x)) # remove stopwords\ndata_rel[\"ExtractedBodyText\"] = data_rel[\"ExtractedBodyText\"].str.replace('\\d+', '') # remove numbers\n\nprint(data_rel[\"ExtractedBodyText\"])\n","65d5e28f":"wordcloud2 = WordCloud(width = 550, height = 500, max_font_size=50, max_words=100, background_color=\"white\").generate(' '.join(data_rel[\"ExtractedBodyText\"]))# Generate plot\nplt.imshow(wordcloud2,interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","f8cd71ea":"vect = TfidfVectorizer(stop_words='english', max_df=0.50, min_df=2,ngram_range=(1,2))\nX = vect.fit_transform(data_rel.ExtractedBodyText)","10fb323f":"X_dense = X.todense()\npca = PCA(n_components = 3)\ncoords = pca.fit_transform(X_dense)\n","1fdda93d":"trace1 = go.Scatter3d(\n    x=coords[:, 0],\n    y=coords[:, 1],\n    z=coords[:, 2],\n    mode='markers',\n    marker=dict(\n        size=12,\n        opacity=0.8\n    )\n)\n\ndata = [trace1]\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    )\n)\nfig = go.Figure(data=data, layout=layout)\nplotly.offline.iplot(fig)","3fa99eef":"#Explained variance\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","e38f221a":"def top_tfidf_feats(row, features, top_n=20):\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats, columns=['features', 'score'])\n    return df\ndef top_feats_in_doc(X, features, row_id, top_n=25):\n    row = np.squeeze(X[row_id].toarray())\n    return top_tfidf_feats(row, features, top_n)","e4c91ff8":"features = vect.get_feature_names()\ntop_feats_in_doc(X, features, 3, 10)","7162ea4d":"def top_mean_feats(X, features,\n grp_ids=None, min_tfidf=0.1, top_n=25):\n    if grp_ids:\n        D = X[grp_ids].toarray()\n    else:\n        D = X.toarray()\n        D[D < min_tfidf] = 0\n    tfidf_means = np.mean(D, axis=0)\n    return top_tfidf_feats(tfidf_means, features, top_n)","f7f9fa65":"top_mean_feats(X, features, top_n=30)","c20092e2":"n_clusters = 8\nclf = KMeans(n_clusters=n_clusters, max_iter=10000, init='k-means++', n_init=1)\nlabels = clf.fit_predict(X)\n#print(labels)\n\nX_dense = X.todense()\ncoords = PCA(n_components=3).fit_transform(X_dense)\n\ntrace1 = go.Scatter3d(\n    x=coords[:, 0],\n    y=coords[:, 1],\n    z=coords[:, 2],\n    mode='markers',\n    marker=dict(\n        size=12,\n        opacity=0.8,\n        color = labels\n    )\n)\n\ndata = [trace1]\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    )\n)\nfig = go.Figure(data=data, layout=layout)\nplotly.offline.iplot(fig)","7a39e73a":"def top_feats_per_cluster(X, y, features, min_tfidf=0.1, top_n=25):\n    dfs = []\n    labels = np.unique(y)\n    for label in labels:\n        ids = np.where(y==label) \n        feats_df = top_mean_feats(X, features, ids,    min_tfidf=min_tfidf, top_n=top_n)\n        feats_df.label = label\n        dfs.append(feats_df)\n    return dfs","3fa04e2b":"print(top_feats_per_cluster(X, labels, features, min_tfidf=0.1, top_n=5))\n","9be5de55":"#Preprocessing\nsmall_count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\nX2 = small_count_vectorizer.fit_transform(data_rel.ExtractedBodyText)\nn_topics = 50","82160c15":"lsa_model = TruncatedSVD(n_components= n_topics, n_iter =350)","175a4eb1":"# Define helper functions\ndef get_keys(topic_matrix):\n    '''returns an integer list of predicted topic categories for a given topic matrix'''\n    keys = []\n    for i in range(topic_matrix.shape[0]):\n        keys.append(topic_matrix[i].argmax())\n    return keys\n\ndef keys_to_counts(keys):\n    '''returns a tuple of topic categories and their accompanying magnitudes for a given list of keys'''\n    count_pairs = Counter(keys).items()\n    categories = [pair[0] for pair in count_pairs]\n    counts = [pair[1] for pair in count_pairs]\n    return (categories, counts)","0eccc9ae":"lsa_topic_matrix = lsa_model.fit_transform(X2)","52de2416":"lsa_keys = get_keys(lsa_topic_matrix)\nlsa_categories, lsa_counts = keys_to_counts(lsa_keys)\nprint(lsa_keys)","7769a8a8":"# Define helper functions\ndef get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n    '''returns a list of n_topic strings, where each string contains the n most common \n        words in a predicted category, in order'''\n    top_word_indices = []\n    for topic in range(n_topics):\n        temp_vector_sum = 0\n        for i in range(len(keys)):\n            if keys[i] == topic:\n                temp_vector_sum += document_term_matrix[i]\n        temp_vector_sum = temp_vector_sum.toarray()\n        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n        top_word_indices.append(top_n_word_indices)   \n    top_words = []\n    for topic in top_word_indices:\n        topic_words = []\n        for index in topic:\n            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n            temp_word_vector[:,index] = 1\n            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n        top_words.append(\" \".join(topic_words))         \n    return top_words","e04e7ee8":"top_n_words_lsa = get_top_n_words(10, lsa_keys, X2, small_count_vectorizer)\n\nfor i in range(len(top_n_words_lsa)):\n    print(\"Topic {}: \".format(i), top_n_words_lsa[i])","30e0747c":"top_3_words = get_top_n_words(3, lsa_keys, X2, small_count_vectorizer)\nlabels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lsa_categories]\n\nfig, ax = plt.subplots(figsize=(16,8))\nax.bar(lsa_categories, lsa_counts)\nax.set_xticks(lsa_categories)\nax.set_title('LSA Topic Category Counts')","40a7f229":"def get_mean_topic_vectors(keys, two_dim_vectors):\n    '''returns a list of centroid vectors from each predicted topic category'''\n    mean_topic_vectors = []\n    for t in range(n_topics):\n        articles_in_that_topic = []\n        for i in range(len(keys)):\n            if keys[i] == t:\n                articles_in_that_topic.append(two_dim_vectors[i])    \n        \n        articles_in_that_topic = np.vstack(articles_in_that_topic)\n        mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)\n        mean_topic_vectors.append(mean_article_in_that_topic)\n    return mean_topic_vectors","9b89c020":"from sklearn.manifold import TSNE\n\ntsne_lsa_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=1500, verbose=1, random_state=0, angle=0.75)\ntsne_lsa_vectors = tsne_lsa_model.fit_transform(lsa_topic_matrix)\n\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\noutput_notebook()\n","ca80e825":"## create colormap\ndef rand_Hexcode(n):\n    for i in range(n):\n        r = lambda: random.randint(0,255)\n        hcode =  '#%02X%02X%02X' % (r(),r(),r())\n        yield hcode\n        \ncolormap = np.fromiter(rand_Hexcode(n_topics), dtype='U25', count=n_topics)","921607c7":"top_3_words_lsa = get_top_n_words(3, lsa_keys, X2, small_count_vectorizer)\nlsa_mean_topic_vectors = get_mean_topic_vectors(lsa_keys, tsne_lsa_vectors)\n\n#plot = figure(title=\"t-SNE Clustering of {} LSA Topics\".format(n_topics), plot_width=1500, plot_height=900)\n#plot.scatter(x=tsne_lsa_vectors[:,0], y=tsne_lsa_vectors[:,1], color=colormap[lsa_keys])\n\n#for t in range(n_topics):\n #   label = Label(x=lsa_mean_topic_vectors[t][0], y=lsa_mean_topic_vectors[t][1], \n  #                text=top_3_words_lsa[t], text_color=colormap[t])\n  #  plot.add_layout(label)\n    \n#show(plot)\n\ntrace1 = go.Scattergl(\n        x=np.array(tsne_lsa_vectors[:,0]),\n        y=np.array(tsne_lsa_vectors[:,1]),\n        mode='markers',\n        marker=dict(\n            size=5,\n             color = colormap[lsa_keys]\n        )\n    )\nfor t in range(n_topics):\n    trace2 = go.Scatter(\n        x=np.array(list(zip(*lsa_mean_topic_vectors))[0]),\n        y=np.array(list(zip(*lsa_mean_topic_vectors))[1]),\n        mode='markers+text',\n        text = np.array(top_3_words_lsa[0:n_topics]),\n         textfont=dict(\n        family='sans serif',\n        size=15,\n        color=colormap[0:n_topics]\n        ))\n    \ntype(lsa_mean_topic_vectors) \n[item[0] for item in lsa_mean_topic_vectors]\ndata = [trace1,trace2]\n\nlayout = go.Layout(#title=\"t-SNE Clustering of {} LSA Topics\".format(n_topics),\n   margin=dict(\n           l=0,\n           r=0,\n           b=0,\n           t=0\n       \n        )\n    )\nfig = go.Figure(data=data, layout=layout)\nplotly.offline.iplot(fig)","223fe9f7":"reindexed_data = data_rel[\"ExtractedBodyText\"]\nreindexed_data.index = data_rel[\"Dates\"]\nreindexed_data.index  = pd.to_datetime(reindexed_data.index )\ndata_range = pd.date_range('2009-07-01','2010-04-01' , freq='1M')\n\n\ntype(reindexed_data)\n\nyearly_data = []\nfor i in data_range: # range(2009,2012+1):\n    yearly_data.append(reindexed_data['{}'.format(i)].as_matrix())\n    \nyearly_topic_matrices = []\nfor monthly in yearly_data:\n    document_term_matrix = small_count_vectorizer.transform(monthly)\n    topic_matrix = lsa_model.transform(document_term_matrix)\n    yearly_topic_matrices.append(topic_matrix)\n    \nyearly_keys = []\nfor topic_matrix in yearly_topic_matrices:\n    yearly_keys.append(get_keys(topic_matrix))\n    \nyearly_counts = []\nfor keys in yearly_keys:\n    categories, counts = keys_to_counts(keys)\n    yearly_counts.append(counts)\n\nyearly_topic_counts = pd.DataFrame((yearly_counts), index=data_range)\nyearly_topic_counts = yearly_topic_counts.add_prefix('Topic')\nprint(yearly_topic_counts)","29f05757":"labels = ['Topic {}: \\n '.format(i) + ' '.join([topic.split() for topic in top_n_words_lsa][i][:3]) for i in range(n_topics)]\n\nfig, ax = plt.subplots(figsize=(14,10))\nsb.heatmap(yearly_topic_counts, xticklabels=labels, ax=ax, annot=True, annot_kws={\"size\": 12})","5371d66f":"The provided plot looks rather fancy however, when you look at the scree plot of the PCA you can see that only 1% of variance is explained for a PCA with only 3 components:","1e0a5bc7":"**4. When are the emails send and received**","35326b09":"## **Analyzing textbody with TF-IDF to find clusters of mails**\n\n TF-IDF  is short for term frequency\u2013inverse document frequency and is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. F\n \n*  Firstly I made a quick plot to visualize this matrix. To do this I first needed to make a 3d representation of the DTM (document-term matrix) using PCA. \n* Secondly, I wanted to find out what the top keywords were in all the emails. \n*  Thirdly KMeans is a popular clustering algorithm used in machine learning, where K stands for the number of clusters. I created a KMeans classifier with 8 clusters and 10000 iterations. Because I now knew which emails were assigned to each cluster, I was able to extract the top terms per cluster.","95643244":"   # Exploratory Analysis and Topic Modelling\n\n*Daan van Kooten 09-2018\n*\n\n**This Kernel consists of:**\n1. a quick exploratory analysis of the Clinton e-mails\n2. Analyzing textbody with TF-IDF to find clusters of mails\n3. Latent semantic analysis (LSA), analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. \n\n***","8177a282":"**3. Who send how many emails.\n**","f78cbb2c":"**Example: 'Minister'**","ec5a2eac":"**Import and select relevant data**\n\nSelect subject, bodytext, receiver and sender information.","40db7b88":"# Preprocessing data\n* Lower casing\n* Punctuation removal\n* Stopwords removal\n* Remove numbers","6267284f":"\n\n## **Exporatory Analysis**\n\nMain topics:\n*  How many emails were send\n*  Who received how many emails.\n*  Who send how many emails.\n*  When are the emails send","d0a62f2d":"## **LSA**\n\nLatent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.\n\nI created 15 different possible clusters of mails with their concepts according LSA. Further Analysis to the different conceptual clusters might be interesting."}}