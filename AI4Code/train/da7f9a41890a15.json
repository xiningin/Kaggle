{"cell_type":{"e5220a73":"code","8089f34a":"code","27d6e6cd":"code","f8abb8c9":"code","0e8bcbe2":"code","3fa0c9cc":"code","494ca370":"code","80f89178":"code","5e7d7b54":"code","b9c6a0a9":"code","f4d283fa":"code","c000d87e":"code","d1688d42":"code","19d833da":"code","921d19f7":"code","67b304b2":"code","5d24785a":"code","23667144":"code","59125d6f":"code","03780c3a":"code","afe76604":"code","6cc4daed":"code","7a39da46":"code","854d22ae":"code","26b94217":"code","9a9ae312":"code","da282bfd":"code","453a28a1":"code","555b1e19":"code","b9d6e5c1":"code","1d96422a":"code","4cbdaf53":"code","065ff8c4":"code","63183fff":"code","ff8166e3":"code","d5fbc9fe":"code","0e42d02e":"code","248fc2ed":"code","7aad6069":"code","43f9fec3":"code","a0e4433e":"code","552bb1fe":"markdown","d77d9dbc":"markdown","86d36406":"markdown","89bece14":"markdown","81923d73":"markdown","58a9a4e1":"markdown"},"source":{"e5220a73":"import numpy as np\nimport pandas as pd\nimport sqlite3\nimport pickle","8089f34a":"conn=sqlite3.connect(\"..\/input\/amazon-fine-food-reviews\/database.sqlite\")\ndata=pd.read_sql_query(\"\"\"\nSELECT *\nfrom Reviews\nWHERE Score!=3\n\"\"\",conn)\nconn.close()\ndata.head()","27d6e6cd":"data.shape","f8abb8c9":"def partition(x):\n    if x>3:\n        return \"positive\"\n    return \"negative\"\ndata['Score']=data['Score'].map(partition)","0e8bcbe2":"data.head()","3fa0c9cc":"data[\"Time\"].nunique()","494ca370":"data[\"Time\"].unique()","80f89178":"data[data[\"Time\"]==  1303862400]","5e7d7b54":"data[(data['Time'] == 1303862400) & (data['ProfileName'] == 'R. Ellis \"Bobby\"')]","b9c6a0a9":"sorted_data =data.sort_values(\"ProductId\")","f4d283fa":"final=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Text\",\"Time\"})\nfinal.shape","c000d87e":"final[final['HelpfulnessNumerator'] > final['HelpfulnessDenominator']]","d1688d42":"final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\nfinal.shape","19d833da":"final['Score'].value_counts()","921d19f7":"final.shape","67b304b2":"conn = sqlite3.connect('final.sqlite')\nc=conn.cursor()\nfinal.to_sql('Reviews', conn, if_exists='replace')\nconn.close()","5d24785a":"conn = sqlite3.connect('final.sqlite')\nfinal = pd.read_sql_query(\"\"\"SELECT * FROM Reviews\"\"\", conn)\nconn.close()\nfinal.head()","23667144":"final.shape","59125d6f":"import re","03780c3a":"import nltk\nfrom nltk.corpus import stopwords","afe76604":"def cleanhtml(sentence):\n    '''This function removes all the html tags in the given sentence'''\n    cleanr = re.compile('<.*?>')    ## find the index of the html tags\n    cleantext = re.sub(cleanr, ' ', sentence)  ## Substitute <space> in place of any html tag\n    return cleantext","6cc4daed":"def cleanpunc(sentence):\n    '''This function cleans all the punctuation or special characters from a given sentence'''\n    cleaned = re.sub(r'[?|@|!|^|%|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    return  cleaned","7a39da46":"stop = set(stopwords.words('english')) #set of stopwords","854d22ae":"sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer","26b94217":"def preprocessing(series):\n    '''The function takes a Pandas Series object containing text in all the cells\n       And performs following Preprocessing steps on each cell:\n       1. Clean text from html tags\n       2. Clean text from punctuations and special characters\n       3. Retain only non-numeric Latin characters with lenght > 2\n       4. Remove stopwords from the sentence\n       5. Apply stemming to all the words in the sentence\n       \n       Return values:\n       1. final_string - List of cleaned sentences\n       2. list_of_sent - List of lists which can be used as input to the W2V model'''\n    \n    i = 0\n    str1=\" \"\n    final_string = []    ## This list will contain cleaned sentences\n    list_of_sent = []    ## This is a list of lists used as input to the W2V model at a later stage\n    \n    ## Creating below lists for future use\n    all_positive_words=[] # store words from +ve reviews here\n    all_negative_words=[] # store words from -ve reviews here\n    \n    \n    for sent in series.values:\n        ## \n        filtered_sent = []\n        sent = cleanhtml(sent)    ## Clean the HTML tags\n        sent = cleanpunc(sent)    ## Clean the punctuations and special characters\n        ## Sentences are cleaned and words are handled individually\n        for cleaned_words in sent.split():\n            ## Only consider non-numeric words with length at least 3\n            if((cleaned_words.isalpha()) and (len(cleaned_words) > 2)):\n                ## Only consider words which are not stopwords and convert them to lowet case\n                if(cleaned_words.lower() not in stop):\n                    ## Apply snowball stemmer and add them to the filtered_sent list\n                    s = (sno.stem(cleaned_words.lower()))#.encode('utf-8')\n                    filtered_sent.append(s)    ## This contains all the cleaned words for a sentence\n                    if (final['Score'].values)[i] == 'positive':\n                        all_positive_words.append(s) #list of all words used to describe positive reviews\n                    if(final['Score'].values)[i] == 'negative':\n                        all_negative_words.append(s) #list of all words used to describe negative reviews\n        ## Below list is a list of lists used as input to W2V model later\n        list_of_sent.append(filtered_sent)\n        ## Join back all the words belonging to the same sentence\n        str1 = \" \".join(filtered_sent)\n        ## Finally add the cleaned sentence in the below list\n        final_string.append(str1)\n        #print(i)\n        i += 1\n    return final_string, list_of_sent","9a9ae312":"## This takes around 1 hour\nfinal_string, list_of_sent = preprocessing(final['Text'])","da282bfd":"final['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review","453a28a1":"final.head()","555b1e19":"conn = sqlite3.connect('final.sqlite')\nc=conn.cursor()\nfinal.to_sql('Reviews', conn, if_exists='replace', index = False)\nconn.close()","b9d6e5c1":"with open('list_of_sent_for_input_to_w2v.pkl', 'wb') as pickle_file:\n    pickle.dump(list_of_sent, pickle_file)","1d96422a":"list_of_sent","4cbdaf53":"from sklearn.feature_extraction.text import CountVectorizer","065ff8c4":"bow_vec=CountVectorizer()\nbow=bow_vec.fit_transform(final['CleanedText'].values)","63183fff":"bow.shape","ff8166e3":"with open(\"bad_of_word_model.pkl\",'wb') as bow_model:\n    pickle.dump(bow,bow_model)","d5fbc9fe":"final.shape","0e42d02e":"from sklearn.feature_extraction.text import TfidfVectorizer","248fc2ed":"tf_idf_vec=TfidfVectorizer()\ntf_idf=tf_idf_vec.fit_transform(final['CleanedText'].values)","7aad6069":"tf_idf.shape","43f9fec3":"type(tf_idf)","a0e4433e":"with open(\"TF_IDF_model.pkl\",\"wb\") as tf_idf_model:\n    pickle.dump(tf_idf,tf_idf_model)","552bb1fe":"## Function to remove the special character","d77d9dbc":"## Final sqlite file after preprocessing","86d36406":"## Pickle file for input to w2v model(List of sentenses)","89bece14":"## Tf-idf model","81923d73":"## Function to remove HTML tags","58a9a4e1":"## Bag of word model"}}