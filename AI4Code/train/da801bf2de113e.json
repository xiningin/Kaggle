{"cell_type":{"fefc2632":"code","b0550cbb":"code","92c3617c":"code","35f799b3":"code","32e506b6":"code","33c7b6c9":"code","c8caf803":"code","6461f82b":"code","7a10d823":"code","d2fb3c09":"code","488b3743":"code","7dbeafe0":"code","382c3515":"code","f630fe87":"code","b32fcb51":"code","d9e6b520":"code","207dc190":"code","7174c37e":"code","a91ce93c":"code","c30964be":"code","039d27b1":"code","efe15627":"code","8eea6a3d":"code","bca88473":"code","3d40f8fa":"code","97549070":"code","2f77604f":"code","c3caf657":"code","c41783bb":"code","0115d96a":"code","8d303dc4":"code","f6f0fbe5":"code","ba562832":"code","f24ae675":"code","d84e5adf":"code","66755d75":"code","a3438a74":"code","1bae4dfd":"code","1bb6e3b7":"markdown","d251291a":"markdown","7608e1a2":"markdown","5f98d9e8":"markdown","3ca4b7fd":"markdown","2a901482":"markdown","3cdef34b":"markdown","b04fbb30":"markdown","a8a4da9e":"markdown","0ca3ef89":"markdown","8b0e1c2b":"markdown","7b048aab":"markdown","53bf5a5e":"markdown","747c2082":"markdown","c22ffae9":"markdown","55c37049":"markdown"},"source":{"fefc2632":"# Import\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport collections\nimport time\nfrom sklearn.preprocessing import LabelEncoder\nprint(os.listdir(\"..\/input\"))","b0550cbb":"# Load files\ntrain = pd.read_csv(\"..\/input\/tmdb-box-office-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tmdb-box-office-prediction\/test.csv\")\nimdb = pd.read_csv(\"..\/input\/imdb-web-scraping-4\/imdb.csv\")\ntrain.head()","92c3617c":"imdb.head()","35f799b3":"# Combine train and test to allow feature engineering on the combined data. Store the ids and the revenue for future use\ntrain_rows = train.shape[0]\ny_train = np.log1p(train[\"revenue\"]).values\ntrain_ID = train['id']\ntest_ID = test['id']\n\nall_data = pd.concat([train, test], sort=False)\nall_data.drop(['revenue'], axis=1, inplace=True)\nall_data.drop(['id'], axis=1, inplace=True)\n\n# Combine the result with the imdb data\nall_data = pd.merge(all_data, imdb, how=\"left\", on=\"imdb_id\")\nall_data.head()","32e506b6":"# Several columns (e.g. genres) are lists of values - split them to dictionaries for easier processing\nimport ast\nfor c in ['belongs_to_collection', 'genres', 'production_companies', 'production_countries', 'spoken_languages', \n          'Keywords', 'cast', 'crew']:\n    all_data[c] = all_data[c].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))","33c7b6c9":"# The EDA and feature engineering for the dict columns seems similar. Create a few basic common utilities\ndef print_few_values(col_name):\n    print(\"Sample values for\", col_name)\n    all_data[col_name].head(5).apply(lambda x: print(x))\n    \ndef dictionary_sizes(col_name):\n    return (all_data[col_name].apply(lambda x: len(x)).value_counts())\n\ndef print_dictionary_sizes(col_name):\n    print(\"\\n===================================================\")\n    print(\"Distribution of sizes for\", col_name)\n    print(dictionary_sizes(col_name))\n    \n# returns a list of tuples of names for a given row of a column\ndef dict_name_list(d, name=\"name\"):\n    return ([i[name] for i in d] if d != {} else [])\n\n# returns a list of tuples of the (id,name) pairs for a given row of a column\ndef dict_id_name_list(d, name=\"name\"):\n    return ([(i[\"id\"],i[name]) for i in d] if d != {} else [])\n\n# returns a list of names for a given column\ndef col_name_list(col_name, name=\"name\"):\n    # Get a list of lists of names\n    name_list_list = list(all_data[col_name].apply(lambda x: dict_name_list(x, name)).values)\n    # Merge into 1 list\n    return ([i for name_list in name_list_list for i in name_list])\n\n# returns a list of tuples of the (id,name) pairs for a given column\ndef col_id_name_list(col_name, name=\"name\"):\n    # Get a list of lists of (id,name) tuples\n    tuple_list_list = list(all_data[col_name].apply(lambda x: dict_id_name_list(x, name)).values)\n    # Merge into 1 list\n    return ([i for tuple_list in tuple_list_list for i in tuple_list])\n\ndef get_names_counter(col_name, name=\"name\"):\n    name_list = col_name_list(col_name, name)\n    return (collections.Counter(name_list))\n    \ndef print_top_names(col_name, name=\"name\"):\n    print(\"\\n===================================================\")\n    print(\"Top {0}s for {1}\".format(name, col_name))\n    c = get_names_counter(col_name, name)\n    print(c.most_common(20))\n    \ndef EDA_dict(col_name):\n    print_few_values(col_name)\n    print_dictionary_sizes(col_name)\n    print_top_names(col_name)\n    \ndef add_dict_size_column(col_name):\n    all_data[col_name + \"_size\"] = all_data[col_name].apply(lambda x: len(x) if x != {} else 0)\n\ndef add_dict_id_column(col_name):\n    c = col_name + \"_id\"\n    all_data[c] = all_data[col_name].apply(lambda x: x[0][\"id\"] if x != {} else 0)\n    all_data[c] = all_data[c].astype(\"category\")\n\n# for each of the top values in the dictionary, add an column indicating if the row belongs to it\ndef add_dict_indicator_columns(col_name):\n    c = get_names_counter(col_name)\n    top_names = [x[0] for x in c.most_common(20)]\n    for name in top_names:\n        all_data[col_name + \"_\" + name] = all_data[col_name].apply(lambda x: name in dict_name_list(x))\n        \ndef drop_column(col_name):\n    all_data.drop([col_name], axis=1, inplace=True)\n    \ndef feature_engineer_dict(col_name):\n    add_dict_size_column(col_name)\n    max_size = dictionary_sizes(col_name).index.max()\n    if max_size == 1:\n        add_dict_id_column(col_name)\n    else:\n        add_dict_indicator_columns(col_name)\n    drop_column(col_name)\n    \ndef encode_column(col_name):\n    lbl = LabelEncoder()\n    lbl.fit(list(all_data[col_name].values)) \n    all_data[col_name] = lbl.transform(list(all_data[col_name].values))","c8caf803":"col_name = \"belongs_to_collection\"\n# EDA\nEDA_dict(col_name)","6461f82b":"# Feature engineering\nfeature_engineer_dict(col_name)","7a10d823":"col_name=\"genres\"\nEDA_dict(col_name)","d2fb3c09":"# Feature engineering\nfeature_engineer_dict(col_name)","488b3743":"col_name=\"production_companies\"\nEDA_dict(col_name)","7dbeafe0":"# Feature engineering\nfeature_engineer_dict(col_name)","382c3515":"col_name=\"production_countries\"\nEDA_dict(col_name)","f630fe87":"# Feature engineering\nfeature_engineer_dict(col_name)","b32fcb51":"col_name=\"spoken_languages\"\nEDA_dict(col_name)","d9e6b520":"# Feature engineering\nfeature_engineer_dict(col_name)","207dc190":"# Check for nulls\nall_data.loc[all_data[\"release_date\"].isnull()]\n# There is 1 movie w\/o a release date. Looking it up in imdb by imdb_id = tt0210130, it was release in march 2000\nall_data.loc[all_data[\"release_date\"].isnull(), \"release_date\"] = \"05\/01\/2000\"\n# Parse the string to a date\nall_data[\"release_date\"] = pd.to_datetime(all_data[\"release_date\"])\n# Create columns for each part of the date\nall_data[\"release_date_weekday\"] = all_data[\"release_date\"].dt.weekday.astype(int)\nall_data[\"release_date_month\"] = all_data[\"release_date\"].dt.month.astype(int)\nall_data[\"release_date_year\"] = all_data[\"release_date\"].dt.year.astype(int)\n# The year is formatted as yy as opposed to yyyy, and therefore the century is sometimes incorrect.\nall_data[\"release_date_year\"] = np.where(all_data[\"release_date_year\"]>2019, all_data[\"release_date_year\"]-100, all_data[\"release_date_year\"])\n# Compare with imdb\nall_data[\"imdb_year\"]=all_data[\"imdb_year\"].astype(int)\n# Compare the values\nprint(all_data[abs(all_data[\"release_date_year\"]-all_data[\"imdb_year\"])>2][[\"imdb_id\",\"release_date_year\",\"imdb_year\"]])\n# There are 4 movies from the 20th century that appear as 21st century due to the issue above. Fix them.\nall_data.loc[abs(all_data[\"release_date_year\"]-all_data[\"imdb_year\"])>2, \"release_date_year\"] = all_data[\"imdb_year\"]\ndrop_column(\"release_date\")\ndrop_column(\"imdb_year\")","7174c37e":"all_data.loc[all_data[\"runtime\"].isnull(), \"imdb_id\"]\nall_data.loc[all_data[\"runtime\"]==0, \"imdb_id\"]\n# A few movies don't have runtime. Get their imdb_id to look them up on imdb.\nall_data.loc[all_data[\"runtime\"]==0, \"runtime\"] = all_data[\"imdb_runtime\"]\nall_data.loc[all_data[\"runtime\"].isnull(), \"runtime\"] = all_data[\"imdb_runtime\"]\n# Most of them have values in imdb. Found one more online.Filling the remaining ones with the column average \nall_data[\"runtime\"].fillna(all_data[\"runtime\"].mean(), inplace=True)\ndrop_column(\"imdb_runtime\")","a91ce93c":"encode_column(\"original_language\")","c30964be":"# Fix Gender\n# Both cast and crew have lots of entries with gender == 0 (i.e. unknown). \n# Use the rest of the entries to decide if a first name is male or female, and categorize the unknown ones by that\n\n# Create a DataFrame of all the names in the cast and crew columns and how many times they appear as each gender\ndef get_names():\n    tuple_list_list = list(all_data[\"cast\"].apply(lambda x: [(cm[\"name\"].split(\" \")[0],cm[\"gender\"]) for cm in x]).values)\n    tuple_list_list.extend(list(all_data[\"crew\"].apply(lambda x: [(cm[\"name\"].split(\" \")[0],cm[\"gender\"]) for cm in x]).values))\n    tuple_list = [i for tuple_list in tuple_list_list for i in tuple_list]\n    names = set()\n    c = [None] * 3\n    for gender in range (0, 3):\n        t = [i[0] for i in tuple_list if i[1] == gender]\n        names.update(set(t))\n        c[gender] = collections.Counter(t)\n    names_df = pd.DataFrame(data = list(names), columns=[\"name\"])\n    names_df[\"female_count\"] = names_df[\"name\"].apply(lambda x: c[1][x])\n    names_df[\"male_count\"] = names_df[\"name\"].apply(lambda x: c[2][x])\n    names_df[\"unknown_count\"] = names_df[\"name\"].apply(lambda x: c[0][x])\n    names_df[\"total_count\"] = names_df.apply(lambda x: x[\"female_count\"]+x[\"male_count\"]+x[\"unknown_count\"], axis=1)\n    return (names_df)\n\nnames_df = get_names()\n\n# Classify each name with a gender (or leave as 0 if there is not enough data to decide or the name is unisex)\ndef ClassifyName(row):\n    fcount = row[\"female_count\"]\n    mcount = row[\"male_count\"]\n    cl = \"TBD\"\n    gender = 0\n    if (fcount + mcount < 5):\n        cl = \"too few names\"\n    elif (fcount == 0):\n        gender = 2\n        cl = \"high confidence\"\n    elif (mcount == 0):\n        gender = 1\n        cl = \"high confidence\"\n    else:  # both are > 0\n        # If a name is 90+% male or female, even if unisex, we'll bet on the majority\n        if (mcount \/ float(fcount) < 0.1):\n            gender = 1\n            cl = \"unisex - pick one\"\n        elif (fcount \/ float(mcount) < 0.1):\n            gender = 2\n            cl = \"unisex - pick one\"\n        else: #unisex, no sex more than 90% - leave as undefined\n            cl = \"unisex - TBD\"\n    return [gender, cl]\n\nnames_df = names_df.merge(names_df.apply(lambda x: ClassifyName(x), axis=1, result_type=\"expand\"), left_index=True, right_index=True)\nnames_df.rename(columns={0:\"gender\", 1:\"classification\"}, inplace=True)\n# pd.pivot_table(names_df, index=[\"classification\"], values=\"total_count\", aggfunc=[np.sum, \"count\"])\n\n# Create a dictionary that maps each name that as male or female (to not add names with undefined gender)\nnames_to_gender = dict()\ndef update_names_to_gender(row):\n    if (row[\"gender\"] > 0):\n        names_to_gender[row[\"name\"]] = row[\"gender\"]\nj = names_df.apply(lambda x: update_names_to_gender(x), axis=1)\n# names_to_gender\n\n# Method to fix the entries with gender == 0 (in cast or crew)\ndef fix_unknown_gender_row(row):\n    for cm in row:\n        if cm[\"gender\"] == 0:\n            name = cm[\"name\"]\n            first_name = name.split(\" \")[0]\n            if (first_name in names_to_gender):\n                cm[\"gender\"] = names_to_gender[first_name]\n    \ndef fix_unknown_gender_col(col_name):\n    all_data[col_name].apply(lambda x: fix_unknown_gender_row(x))","039d27b1":"col_name = \"crew\"\nEDA_dict(col_name)\n# The crew column is different than others as it has much more properties. Let's look at jobs\nprint_top_names(col_name, name=\"job\")","efe15627":"fix_unknown_gender_col(col_name)\n\ndef get_most_common_gender(row):\n    genders = [mc[\"gender\"] for mc in row]\n    if (genders == []):\n        return -1\n    return (collections.Counter(genders).most_common(1)[0][0])\n        \n# Split by job\ntop_jobs = [x[0] for x in get_names_counter(col_name, \"job\").most_common(10)]\nfor job in top_jobs:\n    c = \"crew_\" + job\n    print(\"-------------\\n\",job,\"\\n-------------\")\n    all_data[c] = all_data[col_name].apply(lambda x: [crew_member for crew_member in x if crew_member[\"job\"]==job])\n    all_data[c + \"_common_gender\"] = all_data[c].apply(lambda x: get_most_common_gender(x))\n    EDA_dict(c)\n    feature_engineer_dict(c)\ndrop_column(col_name)","8eea6a3d":"col_name = \"cast\"\n# EDA_dict(col_name)","bca88473":"fix_unknown_gender_col(col_name)\n\n# Split by gender\ngenders = [x[0] for x in get_names_counter(col_name, \"gender\").most_common(2)]\nfor gender in genders:\n    c = \"cast_gender_\" + str(gender)\n    all_data[c] = all_data[col_name].apply(lambda x: [cast_member for cast_member in x if cast_member[\"gender\"]==gender])\n    #print(\"-------------\\n\",gender,\"\\n-------------\")\n    EDA_dict(c)\n    feature_engineer_dict(c)\ndrop_column(col_name)","3d40f8fa":"# Add an indicator for lack of homepage\nall_data[\"no_homepage\"] = all_data[\"homepage\"].isnull()\nprint(all_data[\"no_homepage\"].value_counts())\n# Create a list of all homepages that appear more than once\nmultiple_homepages = all_data[\"homepage\"].value_counts()\nmultiple_homepages = multiple_homepages[multiple_homepages>1].index.tolist()\nprint(\"There are \", len(multiple_homepages), \"homepages which appear more than once\")\n# All the unique homepages are not interesting, replace them with null\nall_data.loc[all_data[\"homepage\"].isin(multiple_homepages) == False, \"homepage\"] = \"\" \n# Encode the strings\nencode_column(\"homepage\")","97549070":"# all_data[\"budget\"].value_counts().sort_index()\n# all_data[(all_data.budget>0) & (all_data.budget<1000)][[\"budget\",\"release_date_year\",\"title\"]]\nall_data[\"no_budget\"] =(all_data[\"budget\"] == 0)\n# all_data[(all_data.budget>0) & (all_data.budget<1000), \"budget\"] = all_data[\"budget\"] * 1000000\n# all_data.head()\nall_data[\"log_budget\"]=np.log1p(all_data[\"budget\"])","2f77604f":"# Ratings have the following format: Rated <rating> for <reason>\n# Remove the word \"Rating at the beginning\"\nall_data[\"mpaa_rating\"] = all_data[\"mpaa_rating\"].apply(lambda s: s.split(\" \", 1)[1] if isinstance(s, str) else s)\n# Set the reason to be the sentence after \"\"<rating> for\"\nall_data[\"mpaa_rating_reason\"] = all_data[\"mpaa_rating\"].apply(lambda s: s.split(\" \", 2)[2] if isinstance(s, str) else s)\n# Set the rating to be the first word\nall_data[\"mpaa_rating\"] = all_data[\"mpaa_rating\"].apply(lambda s: s.split(\" \", 1)[0] if isinstance(s, str) else s)\nall_data[\"mpaa_rating\"].value_counts()","c3caf657":"encode_column(\"mpaa_rating\")","c41783bb":"def SplitReason(r):\n    s = r.strip().replace(\"  \", \" \").replace(\", and for \", \", \").replace(\", and \", \", \")\n    rl = s.rsplit(\" and \", 1)\n    rl2 = rl[0].split(\", \")\n    if len(rl)>1:\n        rl2.append(rl[1])\n    return (rl2)\n\n# Split the reasons, create an indicator column for the top reasons\ncol_name = \"mpaa_rating_reason\"\nall_data[col_name] = all_data[col_name].apply(lambda s: SplitReason(s) if isinstance(s, str) else [])\nreason_list_list = list(all_data[col_name].values)\ntop_reasons = [reason[0] for reason in collections.Counter([i for reason_list in reason_list_list for i in reason_list]).most_common(100)]\nfor reason in top_reasons:\n    all_data[col_name + \"_\" + reason] = all_data[col_name].apply(lambda x: reason in x)\nall_data.head()\ndrop_column(col_name)","0115d96a":"# Drop unneeded columns\n\n# Drop all columns which haven't been handled yet. We'll gradually add them back later on\nall_data.drop(['imdb_id'], axis=1, inplace=True)\nall_data.drop(['original_title'], axis=1, inplace=True)\nall_data.drop(['overview'], axis=1, inplace=True)\nall_data.drop(['poster_path'], axis=1, inplace=True)\nall_data.drop(['status'], axis=1, inplace=True)\nall_data.drop(['tagline'], axis=1, inplace=True)\nall_data.drop(['title'], axis=1, inplace=True)\nall_data.drop(['Keywords'], axis=1, inplace=True)\n\ndrop_column(\"imdb_release\")\ndrop_column(\"imdb_budget\")\ndrop_column(\"imdb_country\")","8d303dc4":"# Split back to train and test\ntrain = all_data[:train_rows]\ntest = all_data[train_rows:]\ntrain.describe()","f6f0fbe5":"# Create a model\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\n#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n    return(rmse)\n\ndef eval_model(model, name):\n    start_time = time.time()\n    score = rmsle_cv(model)\n    print(\"{} score: {:.4f} ({:.4f}),     execution time: {:.1f}\".format(name, score.mean(), score.std(), time.time()-start_time))","ba562832":"mod_lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.005, random_state=1))\neval_model(mod_lasso, \"lasso\")\nmod_enet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\neval_model(mod_enet, \"enet\")\nmod_cat = CatBoostRegressor(iterations=10000, learning_rate=0.01,\n                            depth=5, eval_metric='RMSE',\n                            colsample_bylevel=0.7, random_seed = 17, silent=True,\n                            bagging_temperature = 0.2, early_stopping_rounds=200)\neval_model(mod_cat, \"cat\")\nmod_gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=5, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state=5)\neval_model(mod_gboost, \"gboost\")\nmod_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state=7, nthread=-1)\neval_model(mod_xgb, \"xgb\")\nmod_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=8,\n                              learning_rate=0.05, n_estimators=650,\n                              max_bin=58, bagging_fraction=0.80,\n                              bagging_freq=5, feature_fraction=0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf=7, min_sum_hessian_in_leaf=11)\neval_model(mod_lgb, \"lgb\")","f24ae675":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n    \nmod_stacked = StackingAveragedModels(base_models = (mod_cat, mod_xgb, mod_gboost, mod_lgb), meta_model = mod_lasso)\neval_model(mod_stacked, \"stacked\")","d84e5adf":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef predict(model):\n    model.fit(train.values, y_train)\n    train_pred = model.predict(train.values)\n    pred = np.expm1(model.predict(test.values))\n    print(rmsle(y_train, train_pred))\n    return (pred)","66755d75":"# Predict\n# Start with a simple model\nprediction = predict(mod_lasso)\nprediction = predict(mod_enet)\nprediction = predict(mod_xgb)\nprediction = predict(mod_gboost)\nprediction = predict(mod_lgb)\nprediction = predict(mod_stacked)","a3438a74":"# Submit\nsubmission = pd.DataFrame()\nsubmission['id'] = test_ID\nsubmission['revenue'] = prediction\nsubmission.to_csv('submission.csv', index=False)","1bae4dfd":"# Log                     Estimate Public score\n# baseline:               2.6454   2.68084    v2\n# belong_to_collection:   2.6437   2.67052    v5\n# genres:                 2.5476   2.59730    v6\n# production_companies:   2.4396   2.49865    v7\n# production_countries:   2.4340   2.47048    v8\n# spoken_languages:       2.4316   2.46744    v9\n# release_date:           2.4274   2.46433    v10\n# runtime:                2.4034   2.42784    v11\n# original_language       2.4050   2.42811    v12 - no improvement\n# crew                    2.3649   2.38779    v13\n# cast                    2.3714   2.40004    v14 - no improvement. Maybe it means that we are overfitting with so many columns.\n# stacked model           2.0624   2.02996    v15\n# Fix years (century)     2.0632   2.03133    v16 - didn't help\n# homepage                2.0623   2.02096    v17\n# added no_budget         2.0551   2.02131    v19 - didn't help\n# added log(budget)       2.0417   1.99055    v20\n# filled runtime from imdb2.0424   1.99148    v21 - didn't help\n# fixed century of 4 items2.0448   1.99040    v22\n# add mpaa_rating         2.0333   1.97524    v23\n# add mpaa_rating_reason  2.0473   1.97890    v24\n# add cat model, rem lasso2.1055   2.01712    v25\n# bring back lasso        2.0402   1.97147    v26\n# manual model param tune 2.0367   1.96204    v27","1bb6e3b7":"# homepage","d251291a":"# production_countries","7608e1a2":"# cast","5f98d9e8":"# original_language","3ca4b7fd":"# mpaa_rating","2a901482":"# belongs_to_collection","3cdef34b":"# EDA and Feature Engineering","b04fbb30":"# budget","a8a4da9e":"# crew","0ca3ef89":"# Step by step\nI will start from a basic, almost empty model, and work to gradually improve it.\n# IMDB\nThe competition rules state that \"You can collect other publicly available data to use in your model predictions, but in the spirit of this competition, use only data that would have been available before a movie's release.\". I have (in a separate Kernel) read the following data from imdb: year, budget, runtime, country, etc. - all data that was known before the release, and will try to merge it with the training data.","8b0e1c2b":"# release_date","7b048aab":"# genres","53bf5a5e":"# spoken_languages","747c2082":"# In this iteration\n\nfix budget? entries with 0? make it log? Some are in millions - need to multiple by 1000000\nBudgets are available on imdb, at least for some movies\nAre budgets in USD?\nOther stuff to get from imdb: technical details, etc.\n# Log\n* Adding mpaa_ratings improved the score to 1.97524\n* Fixing a few runtime and year values based on imdb data barely bade a difference: 1.99040\n* Adding a log(budget) improved the public score 1.99055\n* Doing EDA and feature engineering on homepage improved the public score to 2.02096\n* Fixing the century of the release year didn't help (actually made it a little worse)\n* A stacked model improved the public score by a lot to 2.02996!!!\n* Doing EDA and feature engineering on cast dit not improve the public score (overfitting)\n* Doing EDA and feature engineering on crew (barely) improved the public score to 2.38779\n* Doing EDA and feature engineering on original_language dit not improve the public score (probably because it's highly correlated with spoken_languages)\n* Doing EDA and feature engineering on runtime improved the public score to 2.42784\n* Doing EDA and feature engineering on release_date improved the public score to 2.46433\n* Doing EDA and feature engineering on spoken_languages improved the public score to 2.46744\n* Doing EDA and feature engineering on production_countries improved the public score to 2.47048\n* Doing EDA and feature engineering on production_companies improved the public score to 2.49865\n* Created a set of utilities for EDA and feature engineering for all dictionary columns, it should make the next columns much faster.\n* Doing EDA and feature engineering on genres improved the public score to 2.59730\n* Doing EDA and feature engineering on belongs_to_collection improved the public score to 2.67052\n* A baseline based only on budget and popularity, with a simple lasso model, resulted in a 2.68084 public score\n# References\nI've learned a lot from these kernels:\n[Stacked Regressions to predict House Prices] by Serigne - how to stack models\n[1]: https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard","c22ffae9":"# runtime","55c37049":"# production_companies"}}