{"cell_type":{"41d420a5":"code","6e5fe9ac":"code","1e86f75c":"code","4fe13db2":"code","fe7e33ad":"code","43d522a8":"code","00b9ed70":"code","98b077cb":"code","413da30a":"code","75a11237":"code","a3fce11c":"code","85e8651d":"code","f039383a":"code","43fbdc0f":"code","6f129e9d":"code","2341054f":"code","a266b6c5":"code","f7ac5f92":"code","9fb2a92d":"code","1484694c":"code","3c523a2c":"code","2770396a":"markdown","579f335d":"markdown","3c4f1b11":"markdown","eaafacc5":"markdown"},"source":{"41d420a5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, KFold, GroupKFold, GridSearchCV, StratifiedKFold\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import BayesianRidge,LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier, OrthogonalMatchingPursuit\nfrom sklearn.svm import SVR, NuSVR, LinearSVR\nfrom sklearn.mixture import BayesianGaussianMixture, GaussianMixture\nfrom sklearn.neighbors import KNeighborsRegressor, KernelDensity, KDTree\nfrom sklearn.metrics import *\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n\nimport sys, os\nimport random \n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n    \nfrom IPython import display, utils\n\n\ndef set_seed(seed=4242):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed()","6e5fe9ac":"train  = pd.read_csv('..\/input\/imbalanced-data-practice\/aug_train.csv')\ntrain.head()","1e86f75c":"train.shape","4fe13db2":"train.info()","fe7e33ad":"train.describe(include=['O'])","43d522a8":"test  = pd.read_csv('..\/input\/imbalanced-data-practice\/aug_test.csv')\ntest.head()","00b9ed70":"target = train.Response\nsns.set()\nplt.figure(figsize=(8, 4))\nsns.countplot(target)","98b077cb":"cats = [c for c in train.columns if train[c].dtypes =='object']\nprint('Categories', cats)\n\nnums = [c for c in train.columns if c not in cats]\nprint('Numerics', nums)","413da30a":"t_nums = ['Age', 'Annual_Premium','Vintage']\nt_cats = ['Gender', 'Vehicle_Age', 'Vehicle_Damage', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Policy_Sales_Channel' ]","75a11237":"\n\ndef analyse_cats(df, cat_cols):\n    d = pd.DataFrame()\n    cl = []\n    u = []\n    s =[]\n    nans =[]\n    for c in cat_cols:\n        #print(\"column:\" , c ,\"--Uniques:\" , train[c].unique(), \"--Cardinality:\", train[c].unique().size)\n        cl.append(c)\n        u.append(df[c].unique())\n        s.append(df[c].unique().size)\n        nans.append(df[c].isnull().sum())\n        \n        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n        sns.countplot(train[c], ax=axes[0], label='train', palette='bone');\n        sns.countplot(test[c],  ax=axes[1], label='test');\n\n\n        #fig.suptitle(features, fontsize=18);\n        axes[0].set_title('train');\n        axes[1].set_title('test');\n\n\n    d['\"feat\"'] = cl\n    d[\"uniques\"] = u\n    d[\"cardinality\"] = s\n    d[\"nans\"] = nans\n\n    return d\nplt.style.use('ggplot')\ncatanadf = analyse_cats(train, t_cats)\ncatanadf","a3fce11c":"cats = [c for c in train.columns if train[c].dtypes =='object']\nprint('Categories', cats)\n\nnums = [c for c in train.columns if c not in cats]\nprint('Numerics', nums)","85e8651d":"for c in cats:\n    le=LabelEncoder()\n    le.fit(list(train[c].astype('str')) + list(test[c].astype('str')))\n    train[c] = le.transform(list(train[c].astype(str))) \n    test[c] = le.transform(list(test[c].astype(str))) \ntrain.head()","f039383a":"del train['id']\ndel test['id']\n\ntarget = train.pop('Response')\n\ntrain.shape, test.shape","43fbdc0f":"\nlgb_params = {\n    \n    'objective': 'binary', \n    'boosting': 'gbdt', \n    'bagging_fraction': 0.9,\n    'max_depth':-1,\n    'bagging_frequency': 1,\n \n    'feature_fraction': 0.9,\n    'learning_rate': 0.02,\n    'min_child_samples': 100,\n \n    'num_leaves': 50,\n    'metric':'auc', \n    'unbalance': True}\n    \n    \n    \n    \n\n\noof_lgb = np.zeros(len(train))\npred_lgb = np.zeros(len(test))\n\nscores = []\n\nfeature_importances_gain = pd.DataFrame()\nfeature_importances_gain['feature'] = train.columns\n\nfeature_importances_split = pd.DataFrame()\nfeature_importances_split['feature'] = train.columns\n\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_, (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print(\"fold : ---------------------------------------\", fold_)\n    trn_data = lgb.Dataset(train.iloc[train_ind], label=target.iloc[train_ind])#, categorical_feature=cat_cols)\n    val_data= lgb.Dataset(train.iloc[val_ind], label=target.iloc[val_ind])#, categorical_feature=cat_cols)\n    \n    lgb_clf = lgb.train(lgb_params, trn_data, num_boost_round=2000, valid_sets=(trn_data, val_data), verbose_eval=100, early_stopping_rounds=100)\n    oof_lgb[val_ind] = lgb_clf.predict(train.iloc[val_ind], num_iteration= lgb_clf.best_iteration)\n    print(\"fold:\", fold_, \"roc_auc ==\", roc_auc_score(target.iloc[val_ind], oof_lgb[val_ind]))\n    scores.append(roc_auc_score(target.iloc[val_ind], oof_lgb[val_ind]))\n    \n    feature_importances_gain['fold_{}'.format(fold_ + 1)] = lgb_clf.feature_importance(importance_type='gain')\n    feature_importances_split['fold_{}'.format(fold_ + 1)] = lgb_clf.feature_importance(importance_type='split')\n    \n    pred_lgb += lgb_clf.predict(test, num_iteration=lgb_clf.best_iteration)\/folds.n_splits\n    \nprint(' \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ model roc_auc \/\/\/\/\/\/\/\/\/\/\/\/\/\/ : ' , np.mean(scores))\n    \nnp.save('oof_lgb', oof_lgb)\nnp.save('pred_lgb', pred_lgb)","6f129e9d":"oof_lgb_01 = np.where(oof_lgb > 0.5, 1, 0)\n\nconfusion_matrix(target, (oof_lgb_01)) \n\n\ny_true = pd.Series([1, 0, 1])\ny_pred = pd.Series([0, 0, 1]) \nconfusion_matrix(y_true, y_pred)","2341054f":"pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","a266b6c5":"cf_matrix = confusion_matrix(target, (oof_lgb_01)) \ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nplt.figure(figsize=(8, 6))\nsns.set(font_scale=1.4)\nplt.style.use('seaborn-poster')\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='vlag')","f7ac5f92":"feature_importances_gain['average'] = feature_importances_gain[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\nfeature_importances_gain.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(20, 10))\nsns.barplot(data=feature_importances_gain.sort_values(by='average', ascending=False).head(100),palette='bone',  x='average', y='feature');\nplt.title('TOP n feature importance over {} folds average'.format(folds.n_splits));","9fb2a92d":"import xgboost as xgb\nxgb_params = {\n    \n    'objective':'binary:logistic', \n    'max_depth': 6, \n    'learning_rate': 0.01, \n    'booster':'gbtree', \n    'eval_metric': 'auc', \n    'max_leaves': 16, \n    'colsample_bytree': 0.7, #feature fraction\n    'subsample':0.6, # bagging fraction\n    'lambda': 2, \n    \n   \n}\n\n\n\nxgb_scores = []\n\noof_xgb = np.zeros(len(train))\npred_xgb = np.zeros(len(test))\n\nimportances = pd.DataFrame()\n\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4242)\n\nfor fold_, (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold : ----------------------------------------', fold_)\n    trn_data = xgb.DMatrix(data=train.iloc[train_ind], label=target.iloc[train_ind])\n    val_data = xgb.DMatrix(data= train.iloc[val_ind], label=target.iloc[val_ind])\n    \n       \n    xgb_model = xgb.train(xgb_params, trn_data, num_boost_round=1000, evals=[(trn_data, 'train'), (val_data, 'test')], verbose_eval=100, early_stopping_rounds=100)\n    oof_xgb[val_ind] = xgb_model.predict(xgb.DMatrix(train.iloc[val_ind]),  ntree_limit= xgb_model.best_ntree_limit)\n    \n    print(roc_auc_score(target.iloc[val_ind], oof_xgb[val_ind]))\n    xgb_scores.append(roc_auc_score(target.iloc[val_ind], oof_xgb[val_ind]))\n        \n    importance_score = xgb_model.get_score(importance_type='gain')\n    importance_frame = pd.DataFrame({'Importance': list(importance_score.values()), 'Feature': list(importance_score.keys())})\n    importance_frame['fold'] = fold_ +1\n    importances = pd.concat([importances, importance_frame], axis=0, sort=False)\n    \n    pred_xgb += xgb_model.predict(xgb.DMatrix(test), ntree_limit= xgb_model.best_ntree_limit)\/folds.n_splits\n    \nprint('model auc:------------------>', np.mean(xgb_scores))","1484694c":"oof_xgb_01 = np.where(oof_xgb > 0.5, 1, 0)\ncf_matrix = confusion_matrix(target, oof_xgb_01) \ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nplt.figure(figsize=(8, 6))\nsns.set(font_scale=1.4)\nplt.style.use('seaborn-poster')\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Greens')","3c523a2c":"#importances['gain_log'] = importances['gain']\nmean_gain = importances[['Importance', 'Feature']].groupby('Feature').mean()\n#importances['mean_score'] = importances['Feature'].map(mean_gain['Importance'])\nmean_gain = mean_gain.reset_index()\nplt.figure(figsize=(12, 10))\nsns.barplot(x='Importance', y='Feature', data=mean_gain.sort_values('Importance', ascending=False).head(30), palette='bone')","2770396a":"### *Next Series kernels will be included techniques to handle imbalanced data. Stay tuned...*","579f335d":"### XGBoost","3c4f1b11":"### LGB","eaafacc5":"### Categories"}}