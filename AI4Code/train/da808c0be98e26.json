{"cell_type":{"4c30bdb6":"code","29e9bb46":"code","0a46522f":"code","240ad6d6":"code","01e7736e":"code","d371bdb5":"code","7ce584ca":"code","fccab648":"code","cad316a2":"code","a3f23565":"code","6b24699c":"code","2d40e6a5":"code","550c0da0":"code","a73cba12":"code","466eb1fa":"code","a2159083":"code","649b8c7e":"code","ce5a0a67":"code","1c7a1736":"code","fcb0e317":"code","0e98abe6":"code","1ccb735a":"code","07458a87":"code","ef2f51b7":"code","96e91b06":"code","e0223a7f":"code","c447cf4c":"code","76593576":"code","eb0f3769":"code","8bc56bcd":"code","8ccb72c1":"code","c9d238b3":"code","34cc0808":"code","25a7d7fb":"markdown","88478f4b":"markdown","68fe1d75":"markdown","bfc87e5a":"markdown","057a794b":"markdown","ce232984":"markdown","0f120520":"markdown","ff07fd3d":"markdown","0caaf343":"markdown","54c10ef1":"markdown","e675c2a7":"markdown","43d7b7b6":"markdown","84525ed0":"markdown","877cb1a8":"markdown","cf9edd06":"markdown","4857b0e3":"markdown"},"source":{"4c30bdb6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","29e9bb46":"# Carregando o Arquivo\n\ndf = pd.read_csv('\/kaggle\/input\/hmeq-data\/hmeq.csv')\n\ndf.shape","0a46522f":"# Visualizando qtde e tipos\ndf.info()","240ad6d6":"#Visualizando os valores do DataFrame\ndf.head()","01e7736e":"#Empr\u00e9stimos Adimplentes\ndf[df['BAD']==0].describe()","d371bdb5":"#Emprestimos Inadimplentes\ndf[df['BAD']==1].describe()","7ce584ca":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfig, axs = plt.subplots(1,2,figsize=(14,7))\nsns.countplot(x='BAD',data=df,ax=axs[0])\naxs[0].set_title(\"Frequ\u00eancia do Status de Pagamento\")\ndf.BAD.value_counts().plot(x=None,y=None, kind='pie', ax=axs[1],autopct='%1.2f%%')\naxs[1].set_title(\"Porcentagem de Pagamento\")\nplt.show()\n","fccab648":"#Tipo de emprego X Pagou emprestimo\nsns.catplot(x='JOB', hue='BAD', data=df, kind='count')\n#Motivo Emprestimo X Pagou emprestimo\nsns.catplot(x='REASON', hue='BAD', data=df, kind='count')\n","cad316a2":"# Rela\u00e7\u00e3o PAGOU x VALOR EMPRESTIMO + REASON\nsns.catplot(x='BAD', y='LOAN', hue='REASON', data=df, height=7, aspect=.8).set(title=\"Valor X Situa\u00e7\u00e3o do Emprestimo e Raz\u00e3o\")\nsns.catplot(x='BAD', y='LOAN', hue='JOB', data=df, height=7, aspect=.8).set(title=\"Valor X Situa\u00e7\u00e3o do Emprestimo e Trabalho\")","a3f23565":"# Rela\u00e7\u00e3o do Trabalho X Tempo de Trabalho e Situa\u00e7\u00e3o do Emprestimo\nplt.figure(figsize=(15,5))\nsns.violinplot(x='JOB', y='YOJ', hue='BAD',split=True, inner=\"quart\", data=df)  \n# Rela\u00e7\u00e3o do Trabalho X Tempo do Emprestimo Mais Antigo e Situa\u00e7\u00e3o do Emprestimo\nplt.figure(figsize=(15,5))\nsns.violinplot(x='JOB', y='CLAGE', hue='BAD',split=True, inner=\"quart\",data=df)   \n# Rela\u00e7\u00e3o do Trabalho X Valor da Hipoteca e Situa\u00e7\u00e3o do Emprestimo\nplt.figure(figsize=(15,5))\nsns.violinplot(x='JOB', y='MORTDUE', hue='BAD', split=True, inner=\"quart\",data=df)\n","6b24699c":"# Analisando campos Nulos\ndf.isna().sum()\n","2d40e6a5":"#Analisando os registros com mais da metade de valores nulos das YOJ a DEBTINC\ndf[df.iloc[:,6:].isnull().all(axis=1)]","550c0da0":"#Excluidno do dataFrame dados com 7 ou mais colunas com valores nulos\ndf = df.dropna(axis=0,thresh=df.shape[1]-6)\ndf[df.iloc[:,6:].isnull().all(axis=1)]","a73cba12":"# Analisando campos Nulos\ndf.isna().sum(), df.shape\n","466eb1fa":"#Analisando colunas REASON do Tipo Object\ndf['REASON'].value_counts(),print(\"Nulos Campo REASON:\", df['REASON'].isna().sum())\n","a2159083":"#Analisando colunas JOB do Tipo Object\ndf['JOB'].value_counts(),print(\"Nulos Campo JOB:\", df['JOB'].isna().sum())\n","649b8c7e":"#Criando categoria Dummie para coluna REASON e JOB\n\ndumies_reason=pd.get_dummies(df['REASON'],prefix='REASON')\ndf = df.merge(dumies_reason,left_index=True, right_index=True)\ndumies_job=pd.get_dummies(df['JOB'],prefix='JOB')\ndf = df.merge(dumies_job,left_index=True, right_index=True)\ndf.head()","ce5a0a67":"#preenchendo campos nulos que restaram com 0\ndf.fillna(0,inplace=True)","1c7a1736":"df.isna().sum(), df.shape","fcb0e317":"# Correla\u00e7\u00e3o das vari\u00e1veis num\u00e9ricas\nplt.figure(figsize= (15, 15))\n\nsns.heatmap(df.corr(), square=True, annot=True, linewidth=0.5)\n","0e98abe6":"# Dividindo o DataFrame\nfrom sklearn.model_selection import train_test_split\n\n# Treino e teste\ntrain, test = train_test_split(df, test_size=0.199, random_state=42)\n\n# Veificando o tanho dos DataFrames\ntrain.shape, test.shape","1ccb735a":"# definindo colunas de entrada para a predi\u00e7\u00e3o\nfeats = [c for c in df.columns if c not in ['BAD','JOB', 'REASON']]","07458a87":"# Bibliotecas RandomForest\nfrom sklearn.ensemble import RandomForestClassifier\n# Bibliotecas GBM\nfrom sklearn.ensemble import GradientBoostingClassifier\n# Trabalhando com XGBoost\nfrom xgboost import XGBClassifier\n\n#Valida\u00e7\u00e3o do Modelo, Acur\u00e1cia\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score\n\n# importando a bilbioteca para plotar o gr\u00e1fico de Matriz de Confus\u00e3o\nimport scikitplot as skplt\n","ef2f51b7":"#Trabalhando com Random Forest\n\nrf = RandomForestClassifier(n_estimators=200, min_samples_split=5, oob_score=True,max_depth=4, random_state=42)\nrf.fit(train[feats], train['BAD'])\nrf_predict=rf.predict(test[feats])\nrf_accuracy=accuracy_score(test['BAD'], rf_predict)\nrf_scores = cross_val_score(rf, test[feats], rf_predict, n_jobs=-1, cv=5)\nrf_model_f1=cross_validate(rf, test[feats] ,rf_predict, scoring='f1',n_jobs=-1, cv=5)\ntemp = pd.Series([rf_accuracy, rf_scores.mean(), rf_model_f1['test_score'].mean()], index=['ACCURACY', 'K-FOLD', 'F1'])\nval_model_rf = pd.DataFrame(temp, columns=['Resultado_RF'])","96e91b06":"# Importancia das Vari\u00e1veis - Modelo Random Forest\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()\n\n#Matrix de Confus\u00e3o - Modelo Random Forest\nskplt.metrics.plot_confusion_matrix(test['BAD'] ,rf_predict, normalize=True)\n","e0223a7f":"#Valida\u00e7\u00e3o Modelo Random Forest\n\nval_model_rf.head()\n","c447cf4c":"# Trabalhando com GBM\n\ngbm = GradientBoostingClassifier(n_estimators=200, learning_rate=1.0, max_depth=1, random_state=42)\ngbm.fit(train[feats], train['BAD'])\ngbm_predict=gbm.predict(test[feats])\ngbm_accuracy = accuracy_score(test['BAD'], gbm_predict)\ngbm_scores = cross_val_score(gbm, test[feats], gbm_predict, n_jobs=-1, cv=5)\ngbm_model_f1=cross_validate(gbm, test[feats] ,gbm_predict, scoring='f1',n_jobs=-1, cv=5)\n\ntemp = pd.Series([gbm_accuracy, gbm_scores.mean(), gbm_model_f1['test_score'].mean()], index=['ACCURACY', 'K-FOLD', 'F1'])\nval_model_gbm = pd.DataFrame(temp, columns=['Resultado_GBM'])","76593576":"# Importancia das Vari\u00e1veis - GBM\npd.Series(gbm.feature_importances_, index=feats).sort_values().plot.barh()\n#Matrix de Confus\u00e3o - Modelo GBM\nskplt.metrics.plot_confusion_matrix(test['BAD'] ,gbm_predict, normalize=True)\n","eb0f3769":"#Valida\u00e7\u00e3o Modelo GBM\n\nval_model_gbm.head()","8bc56bcd":"# Trabalhando com XGBoost\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(n_estimators=200, learning_rate=0.09, random_state=42)\nxgb.fit(train[feats], train['BAD'])\nxgb_predict=xgb.predict(test[feats])\nxgb_accuracy=accuracy_score(test['BAD'], xgb_predict)\nxgb_model_f1=cross_validate(xgb, test[feats] ,xgb_predict, scoring='f1',n_jobs=-1, cv=5)\nxgb_scores = cross_val_score(xgb, test[feats], xgb_predict, n_jobs=-1, cv=5)\n\ntemp = pd.Series([xgb_accuracy, xgb_scores.mean(), xgb_model_f1['test_score'].mean()], index=['ACCURACY', 'K-FOLD', 'F1'])\nval_model_xgb = pd.DataFrame(temp, columns=['Resultado_XGB'])","8ccb72c1":"# Importancia das Vari\u00e1veis - XGB\npd.Series(xgb.feature_importances_, index=feats).sort_values().plot.barh()\n#Matrix de Confus\u00e3o - Modelo XGB\nskplt.metrics.plot_confusion_matrix(test['BAD'] ,xgb_predict, normalize=True)\n\n","c9d238b3":"#Valida\u00e7\u00e3o Modelo XGB\n\nval_model_xgb.head()","34cc0808":"# Compila\u00e7\u00e3o dos Resultados da Cross Validation dos modelos utilizados\n\nfinal_result = pd.concat([val_model_rf,val_model_gbm,val_model_xgb],axis=1)\n\nfinal_result.head().T","25a7d7fb":"## Random Forest\n\nPara obter melhor acur\u00e1rica, o algoritimo de RF vai criar diversas \u00e1rvores de decis\u00e3o (par\u00e2metro n_estimators) e chegar ao resultado final com base no resultado de cada \u00e1rvore criada. A id\u00e9ia b\u00e1sica \u00e9 separar o conjunto de dados diversas vezes e para cada sub-conjunto treinar um novo regressor\/classificador. Os diferentes regressores\/classificadores ir\u00e3o produzir resultados diferentes, e o resultado final ser\u00e1 determinado com base nessas regress\u00f5es\/classifica\u00e7\u00f5es.","88478f4b":"* Em rela\u00e7\u00e3o as estat\u00edticas descritivas acima, podemos verificar:\n\n\nO valor do empr\u00e9stimo solicitado(LOAN), a hipoteca(MORTDUE) e a garantia subjacente s\u00e3o estatisticamente consistentes para os empr\u00e9stimos PAGOS e que resultaram em PADR\u00c3O. Isso sugere que essas vari\u00e1veis podem n\u00e3o fornecer um poder de discrimina\u00e7\u00e3o significativo para separar as duas classes.\n\nAinda, \u00e9 poss\u00edvel verificar que a m\u00e9dia de tempo no emprego (YOJ) e menor nos empr\u00e9stimos inadimplentes em rela\u00e7\u00e3o aos adimplentes. Da mesma forma, temos que o n\u00famero de relat\u00f3rios ruins(DEROG), o tempo de credito mais antigo (CLAGE), n\u00famero de linhas de cr\u00e9dito inadimplentes(DLINQ)  e o n\u00famero de solicita\u00e7\u00f5es de empr\u00e9stimos, possuem influencia na vari\u00e1vel resposta (BAD).","68fe1d75":"## An\u00e1lise da Matrix de Correla\u00e7\u00e3o\n\nVari\u00e1veis relacionadas ao hist\u00f3rico de cr\u00e9dito (DELINQ, DEROG, NINQ) tem mais correla\u00e7\u00e3o com a situa\u00e7\u00e3o do empr\u00e9stimo (BAD), sugerindo que essas ser\u00e3o as vari\u00e1veis mais relevantes nos modelos de previs\u00e3o.\n\nPor outro lado, o valor do empr\u00e9stimo solicitado, bem como a garantia oferecida sugerem n\u00e3o possuir rela\u00e7\u00e3o com o pagamento ou n\u00e3o do empr\u00e9stimo. ","bfc87e5a":"## Estat\u00edstica Descritiva dos Empr\u00e9stimos Obtidos","057a794b":"### Resultados XGB\n\nA vari\u00e1vel mais importante para o modelo foi a capacidade de pagamento (DEBTINC), seguida pelo n\u00famero de linhas inadimplentes(DELINQ) e a quantidade de Relat\u00f3rios ruins do solicitante. Diferente dos demais modelos testastos (GBM e RF), as vari\u00e1veis dummies tiveram maior import\u00e2ncia para o modelo.\n\nQuanto ao resultado da matriz de confus\u00e3o, o modelo gerado previu de forma equivocada em 3% dos resultados para adimplente, ou seja, falsos positivos. Entretanto, ao se analisar os falsos negativos gerados foi de 22% das previs\u00f5es para inadimplente, o melhor resultado para os tr\u00eas modelos.","ce232984":"## An\u00e1lise Gr\u00e1fica\n\nEm uma an\u00e1lise inicial da base, \u00e9 poss\u00edvel perceber que h\u00e1 mais de 80% de emprestimos adimplentes (BAD=0)","0f120520":"## An\u00e1lise Gr\u00e1fica\n\n1. \u00c9 poss\u00edvel observar influ\u00eancia do tipo de trabalho da pessoal na rela\u00e7\u00e3o com a situa\u00e7\u00e3o do empr\u00e9stimo. Assim, pode-se afirmar que a propor\u00e7\u00e3o de empr\u00e9timos inandimplentes \u00e9 maior entre os vendedores;\n\n2. Da mesma forma, os emprestimos solicitados para renegocia\u00e7\u00e3o de dividas tem maior inadiplemento.","ff07fd3d":"# Conclus\u00e3o \n\nA tabela acima, demostra o desempenho dos modelos considerados no estudo. O modelos que tiveram melhores desempenhos foram o **GBM e XGB**. Contudo, o XGB foi o que menos apresentou a menor quantidade de falsos negativos, por isso a maior acur\u00e1cia do modelo. J\u00e1 o **GBM** teve os melhores indices de cross validation se comparado ao resultado dos demais modelos.\n\nAssim, pelo melhor resultado na valida\u00e7\u00e3o atrav\u00e9s do K-Fold e pelo F1, o modelo GBM teria melhor desempenho na previsibilidade dos resultados.\n\n","0caaf343":"### Resultados Random Forest\n\nA vari\u00e1vel mais importante para o modelo foi a capacidade de pagamento (DEBTINC), seguida pelo n\u00famero de linhas inadimplentes(DELINQ) e o valor da garantia (VALUE).\n\nQuanto ao resultado da matriz de confus\u00e3o, o modelo gerado previu de forma equivocada **1%** dos resultados para **adimplente**, ou seja, **falsos positivos**. Entretanto, ao se analisar os **falsos negativos** gerados foi de **55%** das previs\u00f5es para **inadimplente**.\n\n","54c10ef1":"## XGBoost\nXGB \u00e9 uma implementa\u00e7\u00e3o espec\u00edfica do GBM, dita melhor e mais r\u00e1pida que a implementa\u00e7\u00e3o padr\u00e3o do scikit-learn. Tanto o GBM quanto o XGB precisam de maior trabalho de interpreta\u00e7\u00e3o dos dados e tunning do modelo.","e675c2a7":"## Gradient Boosting\nGBM \u00e9 um m\u00e9todo de boosting, constru\u00eddo em cima de regressores\/classificadores fracos. A id\u00e9ia \u00e9 adicionar um regressor\/classificador de cada vez, ent\u00e3o o pr\u00f3ximo regressor\/classificador \u00e9 treinado para melhorar o resultado atingido at\u00e9 o momento ('soma de resultados'). Ao contr\u00e1rio do RF, que treina cada regressor\/classificador de forma independente, no GBM eles s\u00e3o treinados em conjunto, um ligado ao outro.","43d7b7b6":"# Introdu\u00e7\u00e3o - Dataset HMEQ\n\nA base cont\u00e9m dados de 5.960 empr\u00e9stimos concedidos por uma determinada empresa. A vari\u00e1vel resposta, a qual tentaremos prever, ser\u00e1 (BAD), que \u00e9 bin\u00e1ria, em que \u00e9 informado se o cliente \u00e9 adimplente ou n\u00e3o nos pagamentos do cr\u00e9dito obtido. \n\nPara cada linha da base, temos as seguintes vari\u00e1veis:\n\n* BAD - 1 = Cliente n\u00e3o pagou o empr\u00e9stimo ou est\u00e1 seriamente inadimplente; 0 = Cliente com pagamentos em dia;\n* CLAge\t- Tempo da linha de cr\u00e9dito mais antiga em meses;\n* CLNo - Quantidade de linhas de credito\n* DebtInc - A propor\u00e7\u00e3o d\u00edvida \/ renda (DTI)  - Mede a quantidade de renda que uma pessoa ou organiza\u00e7\u00e3o gera para atender a uma d\u00edvida\n* Delinq - Quantidade de linhas de cr\u00e9dito inadimplente\n* Derog\t - Quantidade de Relat\u00f3rios depreciativos\n* Job - Categoria Profissional\n* Loan - Montante do empr\u00e9stimo solicitado\n* MortDue\t-  Valor Devido da Hipoteca Existente\n* nInq\t- N\u00famero de pedidos de cr\u00e9dito recentes\n* Reason - DebtCon\u00a0= Renegocia\u00e7\u00e3o de d\u00edvidas; HomeImp\u00a0= Melhorias em casa\n* Value - Valor da Garantia Oferecida\n* YoJ -\tAnos de trabalho no emprego atual\n\n","84525ed0":"# Tratando Valores Nulos da Base","877cb1a8":"# An\u00e1lise Explorat\u00f3ria\n\nA partir de agora, faremos algumas an\u00e1lises na base para entender melhor a vari\u00e1vel resposta - BAD. Assim, tentaremos identificar poss\u00edveis correla\u00e7\u00f5es entre as vari\u00e1veis preditoras e a vari\u00e1vel resposta;\n","cf9edd06":"# Modelos de Predi\u00e7\u00e3o\n\nA partir de agora, ap\u00f3s a an\u00e1lise explorat\u00f3ria da base, utilizaremos alguns modelos de aprendizado de m\u00e1quina para tentar prever a situa\u00e7\u00e3o do empr\u00e9stimo, adimplente (BAD=0) ou inadimplente (BAD=1). \n\nAssim, utilizaremos alguns dos modelos aprendidos na disciplina de ***Data Mining e Machine Learning II***: \n* *Ramdon Forest* (***RF***)\n* *Gradient Boosting* (***GBM***)\n* *XGBoost* (***XGB***)\n","4857b0e3":"### Resultados **GBM**\n\nA vari\u00e1vel mais importante para o modelo foi a capacidade de pagamento (DEBTINC), seguida pelo n\u00famero de linhas inadimplentes(DELINQ)\ne o valor da garantia (VALUE).\n\nQuanto ao resultado da matriz de confus\u00e3o, o modelo gerado previu de forma equivocada em **3%** dos resultados para **adimplente**, ou seja, \n**falsos positivos**. Entretanto, ao se analisar os **falsos negativos** gerados foi de **29%** das previs\u00f5es para **inadimplente**, um pouco melhor que os resultados **RF**.\n"}}