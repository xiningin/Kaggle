{"cell_type":{"71e42979":"code","f3e67e4d":"code","f81569d3":"code","3412dbf3":"code","2bf0acba":"code","ef3b516b":"code","b4d8125d":"code","1555cb82":"code","7dccf802":"code","61fddaac":"code","f9f74bf3":"code","3d307677":"code","68d320ee":"code","68579f19":"code","6f69a24d":"code","f273bf65":"code","a3cd5e71":"code","de1a767a":"code","8a7629bf":"code","4fcf3734":"code","38d8af5f":"code","624b542d":"markdown","ff361363":"markdown"},"source":{"71e42979":"!pip install opendatasets\n!pip install nltk\n!pip install TurkishStemmer\n!pip install sklearn\n!pip install keras\n!pip install tensorflow\n!pip install googletrans==4.0.0-rc1","f3e67e4d":"import opendatasets as od\nod.download(\"https:\/\/www.kaggle.com\/metin2\/sentimentanalysis\")","f81569d3":"import ssl\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\nimport codecs\nimport os\nimport re \nimport pandas as pd\nimport seaborn as sn\nimport numpy as np\nimport tensorflow as tf\nimport codecs\nimport string \nimport nltk\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt')\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split","3412dbf3":"#df = pd.read_csv('sentimentAnalysis.csv')\ndf = pd.read_csv('..\/input\/sentimentanalysis\/sentimentAnalysis.csv')\ndf[[\"TranslatedText\",\"SentimentAnalysis\"]].head(10)\n\n#when SentimentAnalysis=0 then 'Ignore'\n#when SentimentAnalysis=1 then 'Negative'\n#when SentimentAnalysis=2 then 'Neutral'\n#when SentimentAnalysis=3 then 'Positive' end as SentimentAnalysis","2bf0acba":"def textCleaning(text):\n    # lower\n    text = text.lower()\n\n    # remove hyperlinks\n    text = re.sub(r\"<.*?a>\", \"\", text)\n\n    # remove html\n    text = re.sub(r\"<[^>]+>\", \"\", text)\n\n    # remove urls\n    text = re.sub(r\"(https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\", \"\", text)\n    \n    # encoding the text - cleaning the emoji\n    text_encode = text.encode(encoding=\"utf-8\", errors=\"ignore\")\n    # text_encode = text.encode(encoding=\"latin5\", errors=\"ignore\")\n\n    # decoding the text\n    text_decode = text_encode.decode(\"ascii\", errors=\"ignore\")\n    # text_decode = text_encode.decode(\"latin5\", errors=\"replace_with_space\")\n    text = text_decode\n    \n    # Remove ticks and the next character\n    text = re.sub(r\"\\'\\w+\", \"\", text)\n\n    # Remove ticks and the next character\n    text = re.sub(r\"\\'\\w+\", \"\", text)\n\n    # cleaning the text to remove extra whitespace \n    text = re.sub(r'\\s{2,}', \" \", text)\n\n    # # removing mentions \n    text = re.sub(r\"@\\S+\", \"\", text)\n\n    # # Remove Hashtags\n    text = re.sub(r\"#\\S+\", \"\", text)\n    \n    # remove market tickers\n    text = re.sub(r\"\\$\", \"\", text)\n\n    # remove punctuation\n    stringPunct= '''!\"$%&'()*+,-.\/:;<=>?[\\]^_`{|}~'''\n    text = text.translate(str.maketrans('', '',stringPunct))\n\n    # Remove numbers\n    text = re.sub(r'\\w*\\d+\\w*', '', text)\n\n    # Tokenizer\n    tt = TweetTokenizer()\n    words = tt.tokenize(text)\n\n    #hastags = []\n    #[hastags.append(token) for token in words if token.startswith(\"#\") ]\n\n    #mentions = []\n    #[mentions.append(token) for token in words if token.startswith(\"@\") ]\n\n    words = [word for word in words if word not in stopwords.words('english')]\n    #the stemmer requires a language parameter\n    snow_stemmer = SnowballStemmer(language='english')\n    \n    #stem's of each word\n    stem_words = []\n    for w in words:\n        x = snow_stemmer.stem(w)\n        stem_words.append(x)\n\n    return (stem_words)\n\n#print(stopwords.words('english'))\n#textCleaning(\"I always tell my female clients to chat\/text for a day or so then move it to\")","ef3b516b":"X=df[\"TranslatedText\"].tolist()\ny=df[\"SentimentAnalysis\"].tolist()\n#train, test = train_test_split(df, test_size=0.2, random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n\nText_X_train, Text_X_test, Text_y_train, Text_y_test = X_train, X_test, y_train, y_test \n\ny_train = np.array(y_train)","b4d8125d":"print(X_train[0])\nprint(y[0])\n\nprint(len(X_train))\nprint(len(y_train))","1555cb82":"max_features = 7000\n\ntokenized = X_train\n#df1[\"AnswerText\"].tolist()\n\nvectorizer = TfidfVectorizer(analyzer='word',\n                        tokenizer=textCleaning,\n                        vocabulary=None,\n                        # preprocessor=dummy_fun,\n                        lowercase = False,\n                        use_idf = True,\n                        token_pattern=None,\n                        ngram_range=(1,3),\n                        max_df=0.9, min_df=2,\n                        max_features=max_features\n                    )\ntfs = vectorizer.fit_transform(tokenized).todense()","7dccf802":"vocabulary = vectorizer.vocabulary_\n# print(vocabulary)\n# print(vectorizer.get_feature_names_out())\nprint(len(vocabulary))","61fddaac":"print(tfs.shape)\nprint(y_train.shape)","f9f74bf3":"X_test = vectorizer.transform(X_test).todense()\ny_test = np.array(y_test)","3d307677":"#initial model\nmodel = keras.Sequential([\n    keras.layers.Dense(128, input_shape=(7000,),activation='relu'),\n    keras.layers.Dense(64),\n    keras.layers.Dense(16),\n    keras.layers.Dense(4,activation='sigmoid')\n])\nmodel.summary()","68d320ee":"#compiling of the model\nmodel.compile(\n    optimizer='adam',\n    loss = 'sparse_categorical_crossentropy',\n    metrics = ['accuracy']\n)\n\nmodel.fit(tfs, y_train, epochs=5)","68579f19":"model.save(\"sentimentAnalysis.h5\")\nimport pickle\nfilename=\"vectorizer\"\nwith open(filename, 'wb') as fp:\n    pickle.dump(vectorizer, fp, protocol=pickle.HIGHEST_PROTOCOL)","6f69a24d":"model.evaluate(X_test, y_test)","f273bf65":"y_predicted = model.predict(X_test)\ny_predicted[0]","a3cd5e71":"print(\"Text: \", Text_X_test[0])\nprint(\"label: \",np.argmax(y_predicted[0]))","de1a767a":"#Draw a graph\ny_predicted = model.predict(X_test)\ny_predicted_labels = [np.argmax(i) for i in y_predicted]\ncm = tf.math.confusion_matrix(labels=y_test, predictions= y_predicted_labels )\nplt.figure(figsize = (10,7))\nsn.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","8a7629bf":"text = [\"I'm sleeping regularly 8 hours for health and I'm considering very balanced nutrition.I also use regular Oalrak food supplies\",\"it is ok, love it\"]\nx = vectorizer.transform(text).todense()\nnp.argmax(model.predict(x)[0])","4fcf3734":"from tensorflow import keras\nimport pickle\n\nm = keras.models.load_model(\"sentimentAnalysis.h5\")\n\nwith open(\"vectorizer\", 'rb') as pfile:\n    v = pickle.load(pfile)","38d8af5f":"text = [\"I'm sleeping regularly 8 hours for health and I'm considering very balanced nutrition.I also use regular Oalrak food supplies\",\"it is ok, love it\"]\nx = v.transform(text).todense()\nnp.argmax(m.predict(x)[0])","624b542d":"# Test Part\n---","ff361363":"# Model Creation \n---"}}