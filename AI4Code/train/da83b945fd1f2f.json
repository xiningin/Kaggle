{"cell_type":{"df9830d2":"code","4bccfcd8":"code","0e80713f":"code","0f68c06a":"code","9b469aea":"code","b87481d1":"markdown","cd9d3c95":"markdown","eab6c9d4":"markdown","75d14479":"markdown","cb736161":"markdown"},"source":{"df9830d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4bccfcd8":"from google.cloud import bigquery\n\n# creating a client object\nclient = bigquery.Client()\n\n# then let's construct a reference to the hacker_news data set\ndataset_reference = client.dataset(\"hacker_news\", project = \"bigquery-public-data\")\n\n# then we need to set up an API request to fetch the data\ndataset = client.get_dataset(dataset_reference)\n\n# then let's construct a reference for the comments table in the dataset\ntable_reference = dataset_reference.table(\"comments\")\n\n# then we need to set up an API request to fetch the data\ntable = client.get_table(table_reference)\n\n# create a viweable table of the dataframe created\nclient.list_rows(table, max_results = 5).to_dataframe()","0e80713f":"query_comment_replies = \"\"\"\n                        SELECT parent, count(id)\n                        FROM `bigquery-public-data.hacker_news.comments`\n                        GROUP BY parent\n                        HAVING count(id) > 15\n                        \"\"\"","0f68c06a":"# setting up the query to set a data limit of 10gb\n# hashing out the two lines given below will not effect the outcome of the code\n# however are good practices to make sure the query is not producing abnormally large data frames\nsafe_configuration = bigquery.QueryJobConfig(maximum_bytes_billed = 10**10)\nquery_job = client.query(query_comment_replies, job_config = safe_configuration)\n\n# API request - run the query, and convert the results to a pandas DataFrame\npopular_comments = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\npopular_comments.head()","9b469aea":"# this should be a better version that includes aliasing\nquery_comment_replies_v2 = \"\"\"\n                           SELECT parent, count(id) AS replies\n                           FROM `bigquery-public-data.hacker_news.comments`\n                           GROUP BY parent\n                           HAVING count(id) > 15\n                           \"\"\"\n\nsafe_configuration = bigquery.QueryJobConfig(maximum_bytes_billed = 10**10)\nquery_job = client.query(query_comment_replies_v2, job_config = safe_configuration)\n\n# API request - run the query, and convert the results to a pandas DataFrame\ndf_v2 = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\ndf_v2.head()","b87481d1":"This table shows a list of comments that are made using the following taxonomy:\n\n* The **parent** column represents the overarching comment that starts a reply chain\n* Using the **group_by** clause will allow us to sort the data by number of replies to specific comments\n* We can get a number for this using the **count()** function","cd9d3c95":"This should be the end of the exercise that includes a slight introduction into the **GROUP BY** clauses as well as **aliasing** using the **AS** clause.","eab6c9d4":"Now let's talk a little about **aliasing** which can help make the data frames themselves a bit more legible!","75d14479":"## SQL Practice 3\n\nJust some code to learn using SQL integrated within the *Kaggle environment*. \n\nFirst we're going to have to create a client and a connection with the **Google BigQuery** server. Then we will establish a table as well as data set reference that we will use to query. Let's work with the *hacker_news* data set as a part of the publicly available data provided by **bigquery**.","cb736161":"Now that we have run the query, let's create a pandas data frame to see the results. The *HAVING* clause inequality statement can be set to any specific number to increase the threshold."}}