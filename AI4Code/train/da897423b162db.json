{"cell_type":{"09282b29":"code","878195d6":"code","28d56e18":"code","b693fa7b":"code","41b71aa7":"code","d408c13d":"code","db1f6fd6":"code","3643ec28":"code","5a604215":"code","8d2c9358":"code","0425808c":"code","7cffcaa2":"code","0e613785":"code","bdb70deb":"code","774065db":"code","82c8d2c9":"code","c5eb3e47":"code","2dd7984a":"code","898f3a26":"code","f0ae6f08":"code","574c54ba":"code","377ab4f2":"code","ea6c8709":"code","a6e46f9e":"code","57f90041":"code","ce056100":"code","7b3a986a":"code","a9648cfd":"code","4b5fe21d":"code","700bd218":"code","95b03d7f":"code","eedcbd0b":"code","83cc7437":"code","15ffc437":"code","cf24702b":"code","a1aebe84":"code","5061dd30":"code","50ecefd8":"code","1e251fd5":"code","5e5cf698":"code","d4c0a510":"code","b030091c":"code","33fefa83":"code","1795afe3":"code","a69fc9c5":"code","86cd3b47":"code","90db9eae":"code","d8a3c8e3":"code","d4ad6dde":"code","6333e0cb":"code","918f8119":"code","1f5d79b8":"code","592a4adc":"code","6b79fd21":"code","de894b3e":"code","28e79807":"code","f8b78f4d":"code","f73d13a2":"code","18b2c3c2":"code","e1ac0f7f":"code","bdc84f75":"code","174e2dc8":"code","6c5dbc11":"code","22db1734":"code","de238f23":"code","26f53dc6":"code","0c794ab4":"code","8b05f83b":"code","6bc11c77":"code","18a40a89":"code","f9449923":"code","dfde5e21":"code","39f5b0e7":"code","940d6b46":"code","f49f9813":"code","581db3bd":"code","7de4f719":"code","c9295559":"code","4bfcff1c":"code","32c89a06":"code","bb09a7fa":"code","5984f378":"code","fd325b5b":"code","34d3e55a":"code","a432eda7":"code","ad4ae910":"code","47e4c9e5":"code","f870ca9e":"code","f1ff84d1":"code","78fe8b8b":"code","f151c6c4":"markdown","cfb29ac2":"markdown","a286946e":"markdown","37b08fee":"markdown","a3ebbf0a":"markdown","4e8a8035":"markdown","988eb17d":"markdown","87a4ba65":"markdown","63ff4d9a":"markdown","98ee444b":"markdown","47af053f":"markdown","bc6df5a5":"markdown","bdbe9540":"markdown","9bba184d":"markdown","1e51dd23":"markdown","120299c0":"markdown","5991f97d":"markdown","e653717e":"markdown","9cd18acb":"markdown","33837134":"markdown","9d5a8047":"markdown","26e64141":"markdown","35f8f090":"markdown","f019b7a6":"markdown","3945f479":"markdown","38b68745":"markdown","ec043084":"markdown","a03915cd":"markdown","47eb79fc":"markdown","a4da1dc8":"markdown","1c4a02dc":"markdown","4b2460a8":"markdown","2f01a84a":"markdown","df580587":"markdown","275c7f9e":"markdown","6c9f0022":"markdown","ac9535f9":"markdown","e2a8a88c":"markdown","9b13a592":"markdown","d84c70e8":"markdown","17ec41e0":"markdown","ba390385":"markdown","4ee6c885":"markdown","e4da8a42":"markdown","8382aaef":"markdown"},"source":{"09282b29":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nsns.set(style='whitegrid')\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\n\nimport re\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\n\nimport tensorflow as tf\nimport warnings\nwarnings.simplefilter('ignore')","878195d6":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","28d56e18":"!apt-get install unzip\n!unzip ..\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip test.tsv\n!unzip ..\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip train.tsv","b693fa7b":"df=pd.read_csv('train.tsv', sep='\\t')\ndf_test=pd.read_csv('test.tsv', sep='\\t')\nsample_submission=pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/sampleSubmission.csv')","41b71aa7":"df.shape, df_test.shape","d408c13d":"df","db1f6fd6":"df_test","3643ec28":"df.info()","5a604215":"df.isnull().sum()","8d2c9358":"df.Sentiment.value_counts()","0425808c":"df2=df.copy(deep=True)\npie1=pd.DataFrame(df2['Sentiment'].replace(0,'Negative').replace(1,'Somewhat negative').replace(2,'Neutral').replace(3,'Somewhat positive').replace(4,'Positive').value_counts())\npie1.reset_index(inplace=True)\npie1.plot(kind='pie', title='Pie chart of Sentiment Class',y = 'Sentiment', \n          autopct='%1.1f%%', shadow=False, labels=pie1['index'], legend = False, fontsize=14, figsize=(12,12))","7cffcaa2":"f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5,figsize=(25,8))\n\nax1.hist(df[df['Sentiment'] == 0]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='b')\nax1.set_title('Negative Reviews')\n\nax2.hist(df[df['Sentiment'] == 1]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='r')\nax2.set_title('Somewhat Negative Reviews')\n\nax3.hist(df[df['Sentiment'] == 2]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='g')\nax3.set_title('Neutral Reviews')\n\nax4.hist(df[df['Sentiment'] == 3]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='y')\nax4.set_title('Somewhat Positive Reviews')\n\nax5.hist(df[df['Sentiment'] == 4]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='k')\nax5.set_title('Positive Reviews')\n\nf.suptitle('Histogram number of words in reviews')","0e613785":"df['Phrase'].str.split().map(lambda x: len(x)).max()","bdb70deb":"dfff=pd.DataFrame(df['Phrase'].str.split().map(lambda x: len(x))>=20)\nprint('Number of sentences which contain more than 20 words: ', dfff.loc[dfff['Phrase']==True].shape[0])\nprint(' ')\ndfff=pd.DataFrame(df['Phrase'].str.split().map(lambda x: len(x))>=30)\nprint('Number of sentences which contain more than 30 words: ', dfff.loc[dfff['Phrase']==True].shape[0])\nprint(' ')\ndfff=pd.DataFrame(df['Phrase'].str.split().map(lambda x: len(x))>=40)\nprint('Number of sentences which contain more than 40 words: ', dfff.loc[dfff['Phrase']==True].shape[0])\nprint(' ')\ndfff=pd.DataFrame(df['Phrase'].str.split().map(lambda x: len(x))>=50)\nprint('Number of sentences which contain more than 50 words: ', dfff.loc[dfff['Phrase']==True].shape[0])\nprint(' ')\ndfff=pd.DataFrame(df['Phrase'].str.split().map(lambda x: len(x))==52)\nprint('Number of sentences which contain 52 words: ', dfff.loc[dfff['Phrase']==True].shape[0])\nprint(' ')\n#dfff.loc[dfff['Phrase']==True]","774065db":"print(df.loc[87835,'Phrase'])","82c8d2c9":"!pip install transformers","c5eb3e47":"from tensorflow.keras.layers import Input, Dropout, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","2dd7984a":"data = df[['Phrase', 'Sentiment']]\n\n# Set your model output as categorical and save in new label col\ndata['Sentiment_label'] = pd.Categorical(data['Sentiment'])\n\n# Transform your output to numeric\ndata['Sentiment'] = data['Sentiment_label'].cat.codes","898f3a26":"data_train, data_val = train_test_split(data, test_size = 0.1)","f0ae6f08":"from transformers import TFBertModel,  BertConfig, BertTokenizerFast","574c54ba":"# Name of the BERT model to use\nmodel_name = 'bert-base-uncased'\n\n# Max length of tokens\nmax_length = 45\n\n# Load transformers config and set output_hidden_states to False\nconfig = BertConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n\n# Load BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n\n# Load the Transformers BERT model\ntransformer_bert_model = TFBertModel.from_pretrained(model_name, config = config)","377ab4f2":"### ------- Build the model ------- ###\n\n# Load the MainLayer\nbert = transformer_bert_model.layers[0]\n\n# Build your model input\ninput_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\ninputs = {'input_ids': input_ids}\n\n# Load the Transformers BERT model as a layer in a Keras model\nbert_model = bert(inputs)[1]\ndropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\npooled_output = dropout(bert_model, training=False)\n\n# Then build your model output\nSentiments = Dense(units=len(data_train.Sentiment_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='Sentiment')(pooled_output)\noutputs = {'Sentiment': Sentiments}\n\n# And combine it all in a model object\nmodel = Model(inputs=inputs, outputs=outputs, name='BERT_MultiClass')\n\n# Take a look at the model\nmodel.summary()","ea6c8709":"### ------- Train the model ------- ###\n\n# Set an optimizer\noptimizer = Adam(learning_rate=5e-05,epsilon=1e-08,decay=0.01,clipnorm=1.0)\n\n# Set loss and metrics\nloss = {'Sentiment': CategoricalCrossentropy(from_logits = True)}\n\n# Compile the model\nmodel.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n\n# Ready output data for the model\ny_train = to_categorical(data_train['Sentiment'])\n\n# Tokenize the input (takes some time)\nx_train = tokenizer(\n          text=data_train['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = True,\n          verbose = True)\n\ny_val = to_categorical(data_val['Sentiment'])\n\nx_val = tokenizer(\n          text=data_val['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = True,\n          verbose = True)\n\n# Fit the model\nhistory = model.fit(\n    x={'input_ids': x_train['input_ids']},\n    y={'Sentiment': y_train},\n    validation_data=({'input_ids': x_val['input_ids']},{'Sentiment': y_val}),\n    batch_size=64,\n    epochs=2,\n    verbose=1)","a6e46f9e":"model_eval = model.evaluate(\n    x={'input_ids': x_val['input_ids']},\n    y={'Sentiment': y_val}\n)","57f90041":"y_val_predicted = model.predict(\n    x={'input_ids': x_val['input_ids']},\n)","ce056100":"y_val_predicted['Sentiment']","7b3a986a":"y_val","a9648cfd":"y_val_pred_max=[np.argmax(i) for i in y_val_predicted['Sentiment']]","4b5fe21d":"y_val_actual_max=[np.argmax(i) for i in y_val]","700bd218":"from sklearn.metrics import classification_report\n\nreport = classification_report(y_val_pred_max, y_val_actual_max)\n\nprint(report)","95b03d7f":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_val_pred_max, y_val_actual_max), display_labels=np.unique(y_val_actual_max))\ndisp.plot(cmap='Blues') \nplt.grid(False)","eedcbd0b":"x_test = tokenizer(\n          text=df_test['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = False,\n          verbose = True)","83cc7437":"label_predicted = model.predict(\n    x={'input_ids': x_test['input_ids']},\n)","15ffc437":"label_predicted['Sentiment']","cf24702b":"label_pred_max=[np.argmax(i) for i in label_predicted['Sentiment']]","a1aebe84":"label_pred_max[:10]","5061dd30":"from transformers import RobertaTokenizer, TFRobertaModel, RobertaConfig  ","50ecefd8":"### --------- Setup Roberta ---------- ###\n\nmodel_name = 'roberta-base'\n\n# Max length of tokens\nmax_length = 40\n\n# Load transformers config and set output_hidden_states to False\nconfig = RobertaConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n\n# Load Roberta tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n\n# Load the Roberta model\ntransformer_roberta_model = TFRobertaModel.from_pretrained(model_name, config = config)","1e251fd5":"### ------- Build the model ------- ###\n\n# Load the MainLayer\nroberta = transformer_roberta_model.layers[0]\n\n# Build your model input\ninput_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\ninputs = {'input_ids': input_ids}\n\n# Load the Transformers RoBERTa model as a layer in a Keras model\nroberta_model = roberta(inputs)[1]\ndropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\npooled_output = dropout(roberta_model, training=False)\n\n# Then build your model output\nSentiments = Dense(units=len(data_train.Sentiment_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='Sentiment')(pooled_output)\noutputs = {'Sentiment': Sentiments}\n\n# And combine it all in a model object\nmodel2 = Model(inputs=inputs, outputs=outputs, name='RoBERTa_MultiClass')\n\n# Take a look at the model\nmodel2.summary()","5e5cf698":"### ------- Train the model ------- ###\n\n# Set an optimizer\noptimizer = Adam(learning_rate=5e-05,epsilon=1e-08,decay=0.01,clipnorm=1.0)\n\n# Set loss and metrics\nloss = {'Sentiment': CategoricalCrossentropy(from_logits = True)}\n\n# Compile the model\nmodel2.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n\n# Ready output data for the model\ny_train = to_categorical(data_train['Sentiment'])\n\n# Tokenize the input (takes some time)\nx_train = tokenizer(\n          text=data_train['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = True,\n          verbose = True)\n\ny_val = to_categorical(data_val['Sentiment'])\n\nx_val = tokenizer(\n          text=data_val['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = True,\n          verbose = True)\n\n# Fit the model\nhistory = model2.fit(\n    x={'input_ids': x_train['input_ids']},\n    y={'Sentiment': y_train},\n    validation_data=({'input_ids': x_val['input_ids']},{'Sentiment': y_val}),\n    batch_size=64,\n    epochs=2,\n    verbose=1)","d4c0a510":"model_eval = model2.evaluate(\n    x={'input_ids': x_val['input_ids']},\n    y={'Sentiment': y_val}\n)","b030091c":"y_val_predicted = model2.predict(\n    x={'input_ids': x_val['input_ids']},\n)","33fefa83":"y_val_predicted['Sentiment']","1795afe3":"y_val","a69fc9c5":"y_val_pred_max=[np.argmax(i) for i in y_val_predicted['Sentiment']]","86cd3b47":"y_val_actual_max=[np.argmax(i) for i in y_val]","90db9eae":"from sklearn.metrics import classification_report\n\nreport = classification_report(y_val_pred_max, y_val_actual_max)\n\nprint(report)","d8a3c8e3":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_val_pred_max, y_val_actual_max), display_labels=np.unique(y_val_actual_max))\ndisp.plot(cmap='Blues') \nplt.grid(False)","d4ad6dde":"x_test = tokenizer(\n          text=df_test['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = False,\n          verbose = True)","6333e0cb":"label_predicted = model2.predict(\n    x={'input_ids': x_test['input_ids']},\n)","918f8119":"label_predicted['Sentiment']","1f5d79b8":"label_pred_max=[np.argmax(i) for i in label_predicted['Sentiment']]","592a4adc":"label_pred_max[:10]","6b79fd21":"from transformers import DistilBertTokenizer, TFDistilBertModel, DistilBertConfig ","de894b3e":"### --------- Setup DistilBERT ---------- ###\n\nmodel_name = 'distilbert-base-uncased'\n\n# Max length of tokens\nmax_length = 45\n\n# Load transformers config and set output_hidden_states to False\nconfig = DistilBertConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n\n# Load Distilbert tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n\n# Load the Distilbert model\ntransformer_distilbert_model = TFDistilBertModel.from_pretrained(model_name, config = config)","28e79807":"### ------- Build the model ------- ###\n\n# Load the MainLayer\ndistilbert = transformer_distilbert_model.layers[0]\n\n# Build your model input\ninput_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\ninputs = {'input_ids': input_ids}\n\n# Load the Transformers DistilBERT model as a layer in a Keras model\ndistilbert_model = distilbert(inputs)[0][:,0,:]\ndropout = Dropout(0.1, name='pooled_output')\npooled_output = dropout(distilbert_model, training=False)\n\n# Then build your model output\nSentiments = Dense(units=len(data_train.Sentiment_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='Sentiment')(pooled_output)\noutputs = {'Sentiment': Sentiments}\n\n# And combine it all in a model object\nmodel3 = Model(inputs=inputs, outputs=outputs, name='DistilBERT_MultiClass')\n\n# Take a look at the model\nmodel3.summary()","f8b78f4d":"### ------- Train the model ------- ###\n\n# Set an optimizer\noptimizer = Adam(learning_rate=5e-05,epsilon=1e-08,decay=0.01,clipnorm=1.0)\n\n# Set loss and metrics\nloss = {'Sentiment': CategoricalCrossentropy(from_logits = True)}\n\n# Compile the model\nmodel3.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n\n# Ready output data for the model\ny_train = to_categorical(data_train['Sentiment'])\n\n# Tokenize the input (takes some time)\nx_train = tokenizer(\n          text=data_train['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = True,\n          verbose = True)\n\ny_val = to_categorical(data_val['Sentiment'])\n\nx_val = tokenizer(\n          text=data_val['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = True,\n          verbose = True)\n\n# Fit the model\nhistory = model3.fit(\n    x={'input_ids': x_train['input_ids']},\n    y={'Sentiment': y_train},\n    validation_data=({'input_ids': x_val['input_ids']},{'Sentiment': y_val}),\n    batch_size=64,\n    epochs=2,\n    verbose=1)","f73d13a2":"model_eval = model3.evaluate(\n    x={'input_ids': x_val['input_ids']},\n    y={'Sentiment': y_val}\n)","18b2c3c2":"y_val_predicted = model3.predict(\n    x={'input_ids': x_val['input_ids']},\n)","e1ac0f7f":"y_val_predicted['Sentiment']","bdc84f75":"y_val","174e2dc8":"y_val_pred_max=[np.argmax(i) for i in y_val_predicted['Sentiment']]","6c5dbc11":"y_val_actual_max=[np.argmax(i) for i in y_val]","22db1734":"from sklearn.metrics import classification_report\n\nreport = classification_report(y_val_pred_max, y_val_actual_max)\n\nprint(report)","de238f23":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_val_pred_max, y_val_actual_max), display_labels=np.unique(y_val_actual_max))\ndisp.plot(cmap='Blues') \nplt.grid(False)","26f53dc6":"x_test = tokenizer(\n          text=df_test['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = False,\n          verbose = True)","0c794ab4":"label_predicted = model3.predict(\n    x={'input_ids': x_test['input_ids']},\n)","8b05f83b":"label_predicted['Sentiment']","6bc11c77":"label_pred_max=[np.argmax(i) for i in label_predicted['Sentiment']]","18a40a89":"label_pred_max[:10]","f9449923":"!pip install sentencepiece ","dfde5e21":"from transformers import XLNetTokenizer, TFXLNetModel, XLNetConfig\nimport sentencepiece","39f5b0e7":"### --------- Setup XLNet ---------- ###\n\nmodel_name = 'xlnet-base-cased'\n\n# Max length of tokens\nmax_length = 45\n\n# Load transformers config and set output_hidden_states to False\nconfig = XLNetConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n\n# Load XLNet tokenizer\ntokenizer = XLNetTokenizer.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n\n# Load the XLNet model\ntransformer_xlnet_model = TFXLNetModel.from_pretrained(model_name, config = config)","940d6b46":"### ------- Build the model ------- ###\n\n# Load the MainLayer\nxlnet = transformer_xlnet_model.layers[0]\n\n# Build your model input\ninput_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\ninputs = {'input_ids': input_ids}\n\n# Load the Transformers XLNet model as a layer in a Keras model\nxlnet_model = xlnet(inputs)[0]\nxlnet_model = tf.squeeze(xlnet_model[:, -1:, :], axis=1)\ndropout = Dropout(0.1, name='pooled_output')\npooled_output = dropout(xlnet_model, training=False)\n\n# Then build your model output\nSentiments = Dense(units=len(data_train.Sentiment_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='Sentiment')(pooled_output)\noutputs = {'Sentiment': Sentiments}\n\n# And combine it all in a model object\nmodel4 = Model(inputs=inputs, outputs=outputs, name='XLNet_MultiClass')\n\n# Take a look at the model\nmodel4.summary()","f49f9813":"### ------- Train the model ------- ###\n\n# Set an optimizer\noptimizer = Adam(learning_rate=5e-05,epsilon=1e-08,decay=0.01,clipnorm=1.0)\n\n# Set loss and metrics\nloss = {'Sentiment': CategoricalCrossentropy(from_logits = True)}\n\n# Compile the model\nmodel4.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n\n# Ready output data for the model\ny_train = to_categorical(data_train['Sentiment'])\n\n# Tokenize the input (takes some time)\nx_train = tokenizer(\n          text=data_train['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = False,\n          verbose = True)\n\ny_val = to_categorical(data_val['Sentiment'])\n\nx_val = tokenizer(\n          text=data_val['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = True,\n          verbose = True)\n\n# Fit the model\nhistory = model4.fit(\n    x={'input_ids': x_train['input_ids']},\n    y={'Sentiment': y_train},\n    validation_data=({'input_ids': x_val['input_ids']},{'Sentiment': y_val}),\n    batch_size=64,\n    epochs=2,\n    verbose=1)","581db3bd":"model_eval = model4.evaluate(\n    x={'input_ids': x_val['input_ids']},\n    y={'Sentiment': y_val}\n)","7de4f719":"y_val_predicted = model4.predict(\n    x={'input_ids': x_val['input_ids']},\n)","c9295559":"y_val_predicted['Sentiment']","4bfcff1c":"y_val","32c89a06":"y_val_pred_max=[np.argmax(i) for i in y_val_predicted['Sentiment']]","bb09a7fa":"y_val_actual_max=[np.argmax(i) for i in y_val]","5984f378":"from sklearn.metrics import classification_report\n\nreport = classification_report(y_val_pred_max, y_val_actual_max)\n\nprint(report)","fd325b5b":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_val_pred_max, y_val_actual_max), display_labels=np.unique(y_val_actual_max))\ndisp.plot(cmap='Blues') \nplt.grid(False)","34d3e55a":"x_test = tokenizer(\n          text=df_test['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = False,\n          verbose = True)","a432eda7":"label_predicted = model4.predict(\n    x={'input_ids': x_test['input_ids']},\n)","ad4ae910":"label_predicted['Sentiment']","47e4c9e5":"label_pred_max=[np.argmax(i) for i in label_predicted['Sentiment']]","f870ca9e":"label_pred_max[:10]","f1ff84d1":"sample_submission['Sentiment'] = label_pred_max\nsample_submission.head(10)","78fe8b8b":"sample_submission.to_csv(\"submission_movie.csv\", index=False, header=True)","f151c6c4":"Effectively was 52 words, this means if we would Tokenize by word the max_length should be 52, however as transformers consider sub-words tokenization such number could be increased depending on the words being used which can increase such length to 60 or even more, thus we have to take that into account when modeling as it could cause our model to take significatively a long time to train, therefore we have to find a trade-off between training time and performance.","cfb29ac2":"In the 5 histograms we can see the distribution behaves like a negative exponential function decreasing significatively as the x-axis increases. It seems like the longest sentence in Phrase column corresponds to a class 'Negative Reviews' and is around 52 words, now let's  obtain the longest one by using the max() function: ","a286946e":"Now we have to gather from the dataset only the two columns useful for training (Phrase and Sentiment):","37b08fee":"Just to confirm the number of instances and features in each file:","a3ebbf0a":"The model took 27 minutes and 20 seconds to train for 2 epochs.","4e8a8035":"DistilBERT does not consider a pooling layer in the default model which converts the output (None,45,768) to (None,768), this is why we will select the first and third dimension of the 'layer 0' so as to have such output shape required, the next layers are the same as before:","988eb17d":"# XLNet\n\nThe tokenizer corresponding to XLNet requires an extra library called sentencepiece which we have to install and import as follows:","87a4ba65":"## Inference:\n\nIn this step we will predict the classes corresponding to the test set (out-of-bag) instances, because of the huge dataset we can expect to have almost same performance.","63ff4d9a":"Submission with pre-trained model:","98ee444b":"The model we will use is 'bert_base_uncased' and the max_length chosen is 45 as there are only a very small number of larger sequences.","47af053f":"I would like to know any feedback in order to increase the performance of the models or tell me if you found a different one even better!\n\nIf you liked this notebook I would appreciate so much your upvote if you want to see more projects\/tutorials like this one. I encourage you to see my projects portfolio, am sure you will love it.\n\nThank you!","bc6df5a5":"## Discussion","bdbe9540":"## Evaluate on validation set:","9bba184d":"# Bert","1e51dd23":"# DistilBERT","120299c0":"# Transformers for Sentiment Classification\n\nWelcome folks!, in the current project I will show you in detail how to implement four types of well-known transformer models making use of the transformers HuggingFace library and Keras API. \n\nThe main task corresponds to a multi-class text classification on Movie Reviews Competition and the dataset contains 156.060 instances for training, whereas the testing set contains 66.292 from which we have to classify among 5 classes. The sentiment labels are:\n\n- 0 \u2192 Negative\n- 1 \u2192 Somewhat negative\n- 2 \u2192 Neutral\n- 3 \u2192 Somewhat positive\n- 4 \u2192 Positive\n\nAt the end of the project we will summarize and compare their performance according to our requirements and metrics. \n\n","5991f97d":"Above we see 352 reviews contain more than 40 words and only 18 reviews above 50 words, these two number corresponds to a tiny proportion of the total number of instances (156.060), therefore setting to these numbers will not affect too much the classification. Below we can see an example of a sentence containing 52 words, observe there are misspelled words, acronyms and some of them can be decomposed into sub-words:","e653717e":"Split into train and validation set, as the file contains more than 150 thousand instances we can consider only a small portion of it as validation and still the number is relatively long, because of this we will set the test_size to 10% as follows:","9cd18acb":"# RoBERTa","33837134":"The next cell considers the tokenization of training and validation sentences, setting of label as categorical and finally model training.","9d5a8047":"In order to compute the classification report and confusion matrix we will convert the matrices to one column representing the argmax for each row:","26e64141":"The dataset looks good and we need to know how are distributed the 5 classes in the label so as to know it's balanced or not.\n","35f8f090":"In general the performance of the four models was similar, supporting the idea that BERT is the middle term of trade-off between accuracy and training time, whereas DistilBERT was the fastest by far, but having a lower accuracy than the previous as is explained by HuggingFace it achieves 95% accuracy of BERT, finally RoBERTa and XLNet were the models with highest accuracy and at the same time the slowest. \n\nI have submitted the predition of the testing set for all models and the best one was RoBERTa reaching 68.62% of accuracy and the lowest was DistilBERT reaching 67.90%. We can say there is a slight difference but in terms of number of misclassifications the gap is huge, however the big challenge of the current task is how to deal with an unbalanced dataset, this is the main and perhaps the unique reason why we have such a poor performance even in the best one, despite the fact that increasing the max_length of sequences can increase a little bit the accuracy too, but not significatively. The method I would apply to solve this problem is undersampling in which we reduce the number of instances to the less frequent class which corresponds to 7.072 (Negative) as such number of instances is not too small and having 5 classes the dataset should finally have 35.360 sentences to compute, but obviously we are getting rid randomly of a big portion of the data.  \n\nAnother possible solution could be to get rid of those reviews which are \"vague\" such as those with only one or two words classified as neutral, those really does not add too much to the training, but in contrast are others which have just a couple of words and are useful. This process would take a long time to do because it have to be done one by one, but it surely solves the problem.\n\nAlso I have to inform that I have trained for more than 2 epochs each model but the accuracy didn't increase or even decreased after the 3rd or 4th epoch, this is why in order to avoid more complex functions or early stopping I set to 2 epochs.\n","f019b7a6":"## Evaluate on validation set:\n\nWe will compute the error metrics on the validation set in order to have an idea of the model performance.","3945f479":"Something similar to DistilBERT happens to the current model, because we have to convert the output shape of the default model first layer to the appropriate (None, 768), in this case we will use tf.squeeze function as can be seen below:","38b68745":"## Evaluate on validation set:","ec043084":"## Inference:","a03915cd":"## Inference:","47eb79fc":"We will build the next 3 models the same way as the previous one, notice there are some lines which includes extra functions proper for the model:","a4da1dc8":"In case there is a null or empty value in any column we should have to get rid of it, in order to find it out we will use info() as follows:","1c4a02dc":"Let's start by unzipping and reading the 3 tsv files containing training, testing and sample submission:","4b2460a8":"The model took 31 minutes and 16 seconds to train for 2 epochs.","2f01a84a":"y_val_predicted corresponds to a numpy array representing the instances and the prediction as one-hot encoded, the actual label is formatted in the same manner, let's them see in detail:","df580587":"## Inference:","275c7f9e":"The fact that our dataset is unbalanced in classes makes our prediction absolutely sidetracked towards the most frequent class, in this case (2: 'Neutral'), because of this the performance of the model is poor when predicting classes 0 or 4, making our model almost unuseful for this task. Below we can see for these 2 classes the number of misclassifications is huge.","6c9f0022":"Above we can see that Phrase and Sentiment columns are all we need from the file in order to train the models later, therefore we will use these as feature (X) and label (Y) when fitting the transformer.","ac9535f9":"As first step we have to import the Model, Config and Tokenizer corresponding to Bert in order to build properly the model. ","e2a8a88c":"Then what we need from tensorflow.keras:","9b13a592":"The label is considerably unbalanced, only 'Neutral' corresponds to more than 50% of instances and there is a slightly skewed towards positive reviews. Early we can say the class to be predicted will be sidetracked toward the more frequent classes, this is why we need a text balancing technique just like 'SMOTE' for numerical features.","d84c70e8":"The model took 14 minutes to train for 2 epochs.","17ec41e0":"## Evaluate on validation set:","ba390385":"Now that our model has been loaded we can start the processes of building and tuning according to our dataset and task using the functional API of keras.\n\nAs we see below the input layer must consider the max_length of sequences and then this is fed to the bert model, a dropout layer to reduce overfitting (0.1) and finally a dense layer with number of neurons equal to number of classes in our label (5).","4ee6c885":"# Modeling\n\nIn this step we will build, train and compare the following algorithms:\n\n- BERT (Bidirectional Encoder Representation from Transformers)\n- RoBERTa (Robustly Optimized BERT Pre-training Approach)\n- DistilBERT (Distilled BERT)\n- XLNet (Generalized Auto-Regressive model)\n\nEach one of the mentioned have its pros and cons, the most preferred and widely used model is the BERT for being the middle term in performance, whereas RoBERTa and .. are known for their better error metrics and DistilBERT for its faster training. We will consider all of these chracteristics and choose the best one for our dataset.\n\nFirstly, we have to install the transformers library offered by HuggingFace so as enable all useful functions when building the four models.","e4da8a42":"Time now to find out the number of words in reviews, in order to understand a bit better we will plot histograms for each class ","8382aaef":"The model took 26 minutes to train for 2 epochs."}}