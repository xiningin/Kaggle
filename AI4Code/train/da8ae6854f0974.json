{"cell_type":{"a7b37be0":"code","c23e387f":"code","ba397440":"code","500fa865":"code","6c463a86":"code","4ce2eb0e":"code","af7295ce":"code","d4592410":"code","a863cab1":"code","566ae613":"code","cb529dd4":"code","cf2fb336":"code","bbfbda61":"markdown","4d5914cb":"markdown","04d84210":"markdown","3aca7caa":"markdown","53de2560":"markdown","8e267617":"markdown","34dcdc66":"markdown"},"source":{"a7b37be0":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import backend as K\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","c23e387f":"# import train & test data\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')\n\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\n\n# drop underrepresented class\ndf_train = df_train[df_train['Cover_Type'] != 5]\n\n# split dataframes for later modeling\nX = df_train.drop(columns=['Id','Cover_Type','Soil_Type7','Soil_Type15','Soil_Type1']).copy()\ny = df_train['Cover_Type'].copy()\n\nX_test = df_test.drop(columns=['Id','Soil_Type7','Soil_Type15','Soil_Type1']).copy()\n\n# create label-encoded one-hot-vector for softmax, mutliclass classification\nle = LabelEncoder()\ntarget = keras.utils.to_categorical(le.fit_transform(y))\n\ndel df_train, df_test\ngc.collect()\n\nprint(X.shape, y.shape, target.shape, X_test.shape)","ba397440":"# define helper functions\ndef set_seed(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    print(f\"Seed set to: {seed}\")\n\ndef plot_eval_results(scores, n_splits):\n    cols = 5\n    rows = int(np.ceil(n_splits\/cols))\n    \n    fig, ax = plt.subplots(rows, cols, tight_layout=True, figsize=(20,2.5))\n    ax = ax.flatten()\n\n    for fold in range(len(scores)):\n        df_eval = pd.DataFrame({'train_loss': scores[fold]['loss'], 'valid_loss': scores[fold]['val_loss']})\n\n        sns.lineplot(\n            x=df_eval.index,\n            y=df_eval['train_loss'],\n            label='train_loss',\n            ax=ax[fold]\n        )\n\n        sns.lineplot(\n            x=df_eval.index,\n            y=df_eval['valid_loss'],\n            label='valid_loss',\n            ax=ax[fold]\n        )\n\n        ax[fold].set_ylabel('')\n\n    sns.despine()\n\ndef plot_cm(cm):\n    metrics = {\n        'accuracy': cm \/ cm.sum(),\n        'recall' : cm \/ cm.sum(axis=1),\n        'precision': cm \/ cm.sum(axis=0)\n    }\n    \n    fig, ax = plt.subplots(1,3, tight_layout=True, figsize=(15,5))\n    ax = ax.flatten()\n\n    mask = (np.eye(cm.shape[0]) == 0) * 1\n\n    for idx, (name, matrix) in enumerate(metrics.items()):\n\n        ax[idx].set_title(name)\n\n        sns.heatmap(\n            data=matrix,\n            cmap=sns.dark_palette(\"#69d\", reverse=True, as_cmap=True),\n            cbar=False,\n            mask=mask,\n            lw=0.25,\n            annot=True,\n            fmt='.2f',\n            ax=ax[idx]\n        )\n    sns.despine()","500fa865":"# define callbacks\nlr = keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", \n    factor=0.5, \n    patience=5, \n    verbose=True\n)\n\nes = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", \n    patience=10, \n    verbose=True, \n    mode=\"min\", \n    restore_best_weights=True\n)","6c463a86":"class SwapRowNoise:\n    def __init__(self, proba):\n        self.proba = proba\n    \n    def apply(self, X):\n        random_idx = np.random.randint(low=0, high=X.shape[0], size=1)[0]\n        swap_matrix = K.random_bernoulli(shape=X.shape, p=self.proba) * tf.ones(shape=X.shape)    \n        corrupted = tf.where(swap_matrix==1, X.iloc[random_idx], X)\n        return corrupted.numpy()\n    \n# create autoencoder\nclass EncodingLayer(layers.Layer):\n    def __init__(self, encoding_dim, activation='relu'):\n        super().__init__()\n        self.enc1 = layers.Dense(encoding_dim, activation)\n        self.enc2 = layers.Dense(encoding_dim, activation)\n        self.enc3 = layers.Dense(encoding_dim, activation)\n        self.concat = layers.Concatenate()\n    \n    def call(self, inputs):\n        enc1 = self.enc1(inputs)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        merge = self.concat([enc1, enc2, enc3])\n        return merge\n\nclass DecodingLayer(layers.Layer):\n    def __init__(self, num_outputs, activation='linear'):\n        super().__init__()\n        self.dec = layers.Dense(num_outputs, activation)\n    \n    def call(self, inputs):\n        return self.dec(inputs)\n    \nclass AutoEncoder(keras.Model):\n    def __init__(self, encoding_dim, num_outputs, activation='relu'):\n        super().__init__()\n        self.encoder = EncodingLayer(encoding_dim, activation,)\n        self.decoder = DecodingLayer(num_outputs)\n    \n    def call(self, inputs):\n        encoder = self.encoder(inputs)\n        decoder = self.decoder(encoder)\n        return decoder\n    \n    def get_encoder(self):\n        return self.encoder","4ce2eb0e":"# create custom layer\nclass DenseBlock(layers.Layer):\n    def __init__(self, units, activation='relu', dropout_rate=0, l2=0):\n        super().__init__()\n        self.dense = layers.Dense(\n            units, activation,\n            kernel_regularizer=keras.regularizers.l2(l2)\n        )\n        self.batchn = layers.BatchNormalization()\n        self.dropout = layers.Dropout(dropout_rate)\n    \n    def call(self, inputs):\n        x = self.dense(inputs)\n        x = self.batchn(x)\n        x = self.dropout(x)\n        return x\n\n# create fully-connected NN\nclass MLP(keras.Model):\n    def __init__(self, hidden_layers, autoencoder, activation='relu', dropout_rate=0, l2=0):\n        super().__init__()\n        self.encoder = autoencoder.get_encoder()\n        self.hidden_layers = [DenseBlock(units, activation, l2) for units in hidden_layers]\n        self.softmax = layers.Dense(units=target.shape[-1], activation='softmax')\n        self.concat = layers.Concatenate()\n        \n    def call(self, inputs):\n        x = self.encoder(inputs)\n        for layer in self.hidden_layers:\n            x = layer(x)\n        x = self.softmax(x)\n        return x","af7295ce":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tf_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"Running on TPU:\", tpu.master())\nexcept:\n    tf_strategy = tf.distribute.get_strategy()\n    print(f\"Running on {tf_strategy.num_replicas_in_sync} replicas\")\n    print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","d4592410":"seed = 2021\nset_seed(seed)\n\nnoise_maker = SwapRowNoise(0.10)\nX_noise = noise_maker.apply(X)\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n\npredictions = []\noof_preds = {'y_valid': list(), 'y_hat': list()}\nscores_ae = {fold:None for fold in range(cv.n_splits)}\nscores_nn = {fold:None for fold in range(cv.n_splits)}\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n    X_train, y_train = X.iloc[idx_train], target[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], target[idx_valid]\n    X_noise_train, X_noise_valid = X_noise[idx_train], X_noise[idx_valid]\n\n    # scale data\n    scl = StandardScaler()\n    X_train = scl.fit_transform(X_train)\n    X_noise_train = scl.transform(X_noise_train)\n    X_valid = scl.transform(X_valid)\n    X_noise_valid = scl.transform(X_noise_valid)\n\n    # train autoencoder\n    with tf_strategy.scope():\n        ae = AutoEncoder(\n            encoding_dim=128,\n            num_outputs=X.shape[-1],\n            activation='relu'\n        )\n\n        ae.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n            loss=keras.losses.MeanSquaredError()\n        )\n\n    print('_'*65)\n    print(f\"Fold {fold+1} || Autoencoder Training\")\n    print('_'*65)\n\n    history_ae = ae.fit(\n        X_noise_train, X_train,\n        validation_data=(X_noise_valid, X_valid),\n        epochs=500,\n        batch_size=4096,\n        shuffle=True,\n        verbose=False,\n        callbacks=[lr,es]\n    )\n\n    scores_ae[fold] = history_ae.history\n\n    print('_'*65)\n    print(f\"Fold {fold+1} || AE Min Val Loss: {np.min(scores_ae[fold]['val_loss'])}\")\n    print('_'*65)\n\n    # train fully-connected nn\n    with tf_strategy.scope():\n        model = MLP(\n            hidden_layers=[32,32,32],\n            autoencoder=ae,\n            activation='relu'\n        )\n\n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n            loss=keras.losses.CategoricalCrossentropy(),\n            metrics=['acc']\n        )\n\n    print('_'*65)\n    print(f\"Fold {fold+1} || NN Training\")\n    print('_'*65)\n\n    history_nn = model.fit(\n        X_train, y_train,\n        validation_data=(X_valid, y_valid),\n        epochs=500,\n        batch_size=4096,\n        shuffle=True,\n        verbose=False,\n        callbacks=[lr,es]\n    )\n\n    scores_nn[fold] = history_nn.history\n\n    oof_preds['y_valid'].extend(y.iloc[idx_valid])\n    oof_preds['y_hat'].extend(model.predict(X_valid, batch_size=4096))\n\n    prediction = model.predict(scl.transform(X_test), batch_size=4096)\n    predictions.append(prediction)\n\n    del ae, model, prediction\n    gc.collect()\n    K.clear_session()\n\n    print('_'*65)\n    print(f\"Fold {fold+1} || NN Min Val Loss: {np.min(scores_nn[fold]['val_loss'])}\")\n    print('_'*65)\n\noverall_score_ae = [np.min(scores_ae[fold]['val_loss']) for fold in range(cv.n_splits)]\noverall_score_nn = [np.min(scores_nn[fold]['val_loss']) for fold in range(cv.n_splits)]\n\nprint('_'*65)\nprint(f\"Overall AE Mean Validation Loss: {np.mean(overall_score_ae)} || Overall NN Mean Validation Loss: {np.mean(overall_score_nn)}\")","a863cab1":"plot_eval_results(scores_nn, cv.n_splits)","566ae613":"# prepare oof_predictions\noof_y_true = np.array(oof_preds['y_valid'])\noof_y_hat = le.inverse_transform(np.argmax(oof_preds['y_hat'], axis=1))\n\n# create confusion matrix, calculate accuracy, recall & precision\ncm = pd.DataFrame(data=confusion_matrix(oof_y_true, oof_y_hat, labels=le.classes_), index=le.classes_, columns=le.classes_)\nplot_cm(cm)","cb529dd4":"# create final prediction, inverse labels to original classes\nfinal_predictions = le.inverse_transform(np.argmax(sum(predictions), axis=1))\n\nsample_submission['Cover_Type'] = final_predictions\nsample_submission.to_csv('.\/baseline_nn.csv', index=False)\n\nsns.countplot(final_predictions)\nsns.despine()","cf2fb336":"sample_submission.head()","bbfbda61":"<div style=\"font-size:110%;line-height:155%\">\n<p>Hi,<\/p>\n<p>while I'm still enjoying my deep-learning adventure, reading and learning a ton of stuff - I decided it's time to implement different kinds of networks with this month competition. I've been trying to implement a denoising autoencoder since TPS-November, but with last month data I wasn't so sure if it would be any success. This time I finally came around to try my luck with a <b>Denoising Autoencoder<\/b>. My implementation is heavily based on the <a href=\"https:\/\/www.kaggle.com\/springmanndaniel\/1st-place-turn-your-data-into-daeta\">TPS-January winning solution by Danzel<\/a>. <p>The idea here is to use an autoencoder to learn more meaningful features by discovering latent variables. The architecture here will create three encoding layers, concatenated and then fed into a fully-connected neural network. To avoid learning the identity-function while blowing up the dimensionality, noise will be injected. In this case I implemented the Swap-Row-Noise function using a noise probability of around 15%.<\/p>\n<p><em>Disclaimer: I am still testing different setups, since training and finetuning the autoencoder does not seem to be so trivial.<\/em><\/p>\n    \n<blockquote><img src=\"https:\/\/i.ibb.co\/j8n07rn\/Deepstack-DAE.png\" width=\"50%\" alt=\"Deepstack-DAE\" border=\"0\"><\/blockquote>\n    \n<p>Feel free to take a look at my other notebooks, covering some different ideas and architectures:\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-simple-nn-baseline-keras\">Simple NN Baseline<\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-deep-wide-nn-keras\">Deep & Wide NN <\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-bn-autoencoder-nn-keras\">Bottleneck Autoencoder<\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-deep-cross-nn-keras\">Deep & Cross NN<\/a><\/li>\n<\/p>\n    \n<em>Thank you very much for taking some time to read my notebook. Please leave an upvote if you find any of this information useful.<\/em>\n<\/div>","4d5914cb":"# Import & Prepare Data","04d84210":"# Training","3aca7caa":"# Evaluation & Submission","53de2560":"<img src=\"https:\/\/i.ibb.co\/PWvpT9F\/header.png\" alt=\"header\" border=\"0\" width=800 height=300>","8e267617":"# Model Setup","34dcdc66":"# Introduction"}}