{"cell_type":{"49b3007e":"code","83f19290":"code","5195ae58":"code","11abbd11":"code","518affb0":"code","17cac0b4":"code","5ede4692":"code","b1e5776f":"code","34032400":"code","fb9f568a":"code","64212afe":"code","6ec3dc42":"code","73297f31":"code","24c64b81":"code","d377b3cf":"code","861f7336":"code","e0ff19db":"code","5ba5a1ea":"code","e77bf255":"code","b0026788":"code","66cd176d":"code","bd2663a1":"code","ac5432ee":"code","770ecb1d":"code","cb844b4f":"code","231d97a1":"code","1e73ec6f":"code","d3119537":"code","dabcdde5":"code","391e05bb":"code","6a9a6f9d":"code","40704eaa":"code","fcd21407":"code","2fed7df4":"code","26550f64":"code","8b838f49":"code","42ea4811":"code","fc865cda":"code","7af34dd3":"code","888602f1":"code","ad4eb2f7":"code","3bc2d138":"code","5a4a6a29":"code","937977d1":"code","4bc713d8":"code","f8fc1849":"code","d8b71de4":"code","e50fb01d":"code","6acfca8f":"code","d6590096":"code","037b697d":"code","2c650edd":"code","91474264":"code","da02031b":"code","86cefee9":"code","4f7b3136":"code","b0fb997e":"code","28c4013d":"code","d790d379":"markdown","ad43f58c":"markdown","6d5830df":"markdown","a33cd6fd":"markdown","00712e87":"markdown","52e4f4ef":"markdown","a3915b21":"markdown","13bc9bc1":"markdown","805cfa23":"markdown","87d999c1":"markdown","21f7d300":"markdown","8ee288f9":"markdown","68bc1e75":"markdown","596b3267":"markdown","f2456d0d":"markdown","d4bfdd4a":"markdown","d19046d5":"markdown","88e380a6":"markdown","5b59cea6":"markdown","b0cd228e":"markdown","715a09a1":"markdown","3cd218d3":"markdown","b041855e":"markdown","c5db066e":"markdown","19a320cb":"markdown","5cda14c1":"markdown","5e86dc03":"markdown","763879b3":"markdown","3c19b16c":"markdown","2c80c373":"markdown"},"source":{"49b3007e":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Models\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\nimport warnings\nwarnings.filterwarnings('ignore')","83f19290":"test_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")","5195ae58":"train_df.info()","11abbd11":"train_df.describe()","518affb0":"train_df.head(15)","17cac0b4":"total = train_df.isnull().sum().sort_values(ascending=False)\npercent_1 = train_df.isnull().sum()\/train_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","5ede4692":"train_df.columns.values","b1e5776f":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","34032400":"FacetGrid = sns.FacetGrid(train_df, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","fb9f568a":"sns.barplot(x='Pclass', y='Survived', data=train_df)","64212afe":"grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","6ec3dc42":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)","73297f31":"train_df['not_alone'].value_counts()","24c64b81":"axes = sns.factorplot('relatives','Survived', \n                      data=train_df, aspect = 2.5, )","d377b3cf":"train_df = train_df.drop(['PassengerId'], axis=1)","861f7336":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int) ","e0ff19db":"# we can now drop the cabin feature\ntrain_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)","5ba5a1ea":"data = [train_df, test_df]\n\nfor dataset in data:\n    mean = train_df[\"Age\"].mean()\n    std = test_df[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_df[\"Age\"].astype(int)","e77bf255":"train_df[\"Age\"].isnull().sum()","b0026788":"train_df['Embarked'].describe()","66cd176d":"common_value = 'S'\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)","bd2663a1":"train_df.info()","ac5432ee":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","770ecb1d":"data = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)","cb844b4f":"train_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","231d97a1":"genders = {\"male\": 0, \"female\": 1}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","1e73ec6f":"train_df['Ticket'].describe()","d3119537":"train_df = train_df.drop(['Ticket'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)","dabcdde5":"ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","391e05bb":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6","6a9a6f9d":"# let's see how it's distributed\ntrain_df['Age'].value_counts()","40704eaa":"train_df.head(10)","fcd21407":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)","2fed7df4":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()","26550f64":"# stochastic gradient descent (SGD) learning\nsgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nsgd.score(X_train, Y_train)\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n\n\nprint(round(acc_sgd,2,), \"%\")","8b838f49":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","42ea4811":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\n\nY_pred = knn.predict(X_test)\n\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint(round(acc_knn,2,), \"%\")","fc865cda":"# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\n\nY_pred = gaussian.predict(X_test)\n\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint(round(acc_gaussian,2,), \"%\")","7af34dd3":"# Linear SVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nprint(round(acc_linear_svc,2,), \"%\")","888602f1":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\n\nY_pred = decision_tree.predict(X_test)\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(round(acc_decision_tree,2,), \"%\")","ad4eb2f7":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN',  \n              'Random Forest', 'Naive Bayes',  \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, \n              acc_random_forest, acc_gaussian, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","3bc2d138":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")","5a4a6a29":"print(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","937977d1":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')","4bc713d8":"importances.head(15)","f8fc1849":"importances.plot.bar()","d8b71de4":"# param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \n#               \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \n#               \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \n#               \"n_estimators\": [100, 400, 700, 1000, 1500]}\n\n\n# from sklearn.model_selection import GridSearchCV, cross_val_score\n\n# rf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\n# clf = GridSearchCV(estimator=rf, param_grid=param_grid,\n#                     n_jobs=-1)\n\n# clf.fit(X_train, Y_train)        \n\n# clf.best_params_","e50fb01d":"# Random Forest\nrandom_forest = RandomForestClassifier(criterion = \"gini\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 10,   \n                                       n_estimators=100, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)","6acfca8f":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)","d6590096":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))","037b697d":"from sklearn.metrics import f1_score\nf1_score(Y_train, predictions)","2c650edd":"from sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(Y_train, y_scores)","91474264":"def plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","da02031b":"def plot_precision_vs_recall(precision, recall):\n    plt.plot(recall, precision, \"g--\", linewidth=2.5)\n    plt.ylabel(\"recall\", fontsize=19)\n    plt.xlabel(\"precision\", fontsize=19)\n    plt.axis([0, 1.5, 0, 1.5])\n\nplt.figure(figsize=(14, 7))\nplot_precision_vs_recall(precision, recall)\nplt.show()","86cefee9":"from sklearn.metrics import roc_curve\n# compute true positive rate and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, y_scores)","4f7b3136":"# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","b0fb997e":"from sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(Y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","28c4013d":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_prediction\n    })\nsubmission.to_csv('submission.csv', index=False)","d790d379":"### Fare:\n","ad43f58c":"## Converting Features:","6d5830df":"## K-Fold Cross Validation:","a33cd6fd":"## Precision Recall Curve\n","00712e87":"### Age:\n\n","52e4f4ef":"# **Import Libraries**","a3915b21":"# **Submission**","13bc9bc1":"## Hyperparameter Tuning\n\n","805cfa23":"## ROC AUC Score","87d999c1":"**3. Embarked, Pclass  and Sex:**","21f7d300":"### Embarked:\n\n","8ee288f9":"# **Data Exploration\/Analysis**","68bc1e75":"**4. Pclass:**","596b3267":"## Feature Importance\n","f2456d0d":"## Precision and Recall:","d4bfdd4a":"**Test new paramters:**","d19046d5":"**1. Age and Sex:**","88e380a6":"### Ticket:","5b59cea6":"##  Which is the best Model ?","b0cd228e":"Since the Ticket attribute has 681 unique tickets, it will be a bit tricky to convert them into useful categories. So we will drop it from the dataset.","715a09a1":"# **Building Machine Learning Models**","3cd218d3":"# **Data Preprocessing**","b041855e":"### Name:\n","c5db066e":"### Embarked:\n","19a320cb":"### Sex:\n\n","5cda14c1":"### Fare:\n\n","5e86dc03":"## F-Score\n","763879b3":"## ROC AUC Curve\n","3c19b16c":"## Confusion Matrix:","2c80c373":"# **Getting the Data**"}}