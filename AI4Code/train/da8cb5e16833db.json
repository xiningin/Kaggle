{"cell_type":{"60cf10f2":"code","f2f5a3db":"code","98ee2e9a":"code","40f23afc":"code","ca3f575a":"code","4844b29a":"code","5f7122e3":"code","dc60a4b6":"code","9b84dda9":"code","4437b7eb":"code","e78feaf6":"code","4c641e69":"code","b928432f":"code","83bc2699":"code","41af3477":"code","8a3b3e57":"code","eca75614":"code","108e305c":"code","7ef0f159":"code","cb030552":"code","5fc05c01":"code","ffca05f3":"code","a8f00a3f":"code","4b8f08fe":"code","db609be5":"code","83740749":"markdown","7139aa13":"markdown","cfa61812":"markdown","28d217af":"markdown","5efe92bb":"markdown","6c65e286":"markdown","31818cd5":"markdown","27fe9faf":"markdown","531806f0":"markdown","4969a74f":"markdown","5e20839a":"markdown","385cae8a":"markdown","c62fcc7f":"markdown","994e2aff":"markdown","da974f7b":"markdown","cb6d9006":"markdown","cfec3590":"markdown","978dee08":"markdown","4f1f8ba9":"markdown","8400e69a":"markdown","1e98331f":"markdown"},"source":{"60cf10f2":"!pip install -q --upgrade pip\n!pip install -q seaborn==0.11.1","f2f5a3db":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgbm\n\nimport optuna\nfrom optuna import Trial, visualization","98ee2e9a":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')","40f23afc":"train.head()","ca3f575a":"test.head()","4844b29a":"sub.head()","5f7122e3":"print(f\"Train shape - {train.shape}\\nTest shape - {test.shape}\")","dc60a4b6":"train.describe()","9b84dda9":"feats = [col for col in train.columns.tolist() if col.startswith('cont')]\n\n\nfig, ax = plt.subplots(nrows=7, ncols=2, figsize=(15,15))\n\nfig.tight_layout()\nplt.subplots_adjust(bottom=0.1, top=1.5)\ni = 0\nfor row in ax:\n    for col in row:\n        sns.histplot(data = train, x=feats[i], ax=col)\n        col.xaxis.label.set_visible(False)\n        col.yaxis.label.set_visible(False)\n        col.set_title(feats[i])\n        i += 1\n        \nplt.show()","4437b7eb":"# separating the target and features\n\nX = train.drop(['target', 'id'], axis=1).values\ny = train['target'].values\nXtest = test.drop('id', axis=1).values\n\nX.shape, y.shape, Xtest.shape","e78feaf6":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# making 5 splits\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\n\n# defining model\nxgbreg = XGBRegressor(tree_method='gpu_hist')\n\n# taking positive because cross val score returns -ve values\nscores = np.abs(cross_val_score(xgbreg, X, y, cv=kf, scoring='neg_root_mean_squared_error'))\n\n# printing the output\nprint(f\"Scores - {scores}\\nMean - {sum(scores)\/len(scores)}\")","4c641e69":"\"\"\"This function will return our cv score which we are aiming to minimize\"\"\"\ndef Objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [150, 200, 250, 300]),\n        \"eta\": trial.suggest_loguniform(\"eta\",1e-2,0.1),\n        \"subsample\": trial.suggest_discrete_uniform(\"subsample\", 0.6,1,0.1),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\",5,11),\n        \"colsample_bytree\": trial.suggest_discrete_uniform(\"colsample_bytree\", 0.6,1,0.1),\n        \"max_depth\": trial.suggest_categorical(\"max_depth\",[5,7,9,11,13]),\n        \"random_state\": 2021\n    }\n    \n    model = XGBRegressor(**params, tree_method='gpu_hist')\n    \n    \n    # taking positive because cross val score returns -ve values\n    scores = np.abs(cross_val_score(model, X, y, cv=kf, scoring='neg_root_mean_squared_error'))\n    \n    return sum(scores)\/len(scores)","b928432f":"study = optuna.create_study(direction=\"minimize\", study_name='Xgboost optimization')\nstudy.optimize(Objective, n_trials=20)","83bc2699":"XGB_params = study.best_params\n\nstudy.best_params","41af3477":"# defining model\ncatreg = CatBoostRegressor(task_type='GPU', verbose=0)\n\n# taking positive because cross val score returns -ve values\nscores = np.abs(cross_val_score(catreg, X, y, cv=kf, scoring='neg_root_mean_squared_error'))\n\n# printing the output\nprint(f\"Scores - {scores}\\nMean - {sum(scores)\/len(scores)}\")","8a3b3e57":"\"\"\"This function will return our cv score which we are aiming to minimize\"\"\"\ndef Objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [150, 200, 250, 300, 350]),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.05, 0.5),\n        \"subsample\": trial.suggest_discrete_uniform(\"subsample\", 0.6,1,0.1),\n        \"max_depth\": trial.suggest_categorical(\"max_depth\",[5,7,9,11,13]),\n        #\"colsample_bylevel\": trial.suggest_categorial(\"colsample_bylevel\", [0.3,0.4,0.5,0.6,0.7,0.8])\n        # the colsample_bylevel also improves the score a lot, but its commented out because its not supported on GPU currently.\n        # If you are interested, do fork the kernel, and try it out. I reached a score of 0.7004 while tuning with it.\n        \"random_state\": 2021\n    }\n    \n    \n    model = CatBoostRegressor(**params, task_type=\"GPU\", verbose=0)\n    \n    \n    # taking positive because cross val score returns -ve values\n    scores = np.abs(cross_val_score(model, X, y, cv=kf, scoring='neg_root_mean_squared_error'))\n    \n    return sum(scores)\/len(scores)","eca75614":"study = optuna.create_study(direction=\"minimize\", study_name='Catboost optimization')\nstudy.optimize(Objective, n_trials=20)","108e305c":"catboost_params = study.best_params\nstudy.best_params","7ef0f159":"lgbreg = lgbm.LGBMRegressor(device_type='gpu')\n\n# taking positive because cross val score returns -ve values\nscores = np.abs(cross_val_score(lgbreg, X, y, cv=kf, scoring='neg_root_mean_squared_error'))\n\n# printing the output\nprint(f\"Scores - {scores}\\nMean - {sum(scores)\/len(scores)}\")","cb030552":"\"\"\"This function will return our cv score which we are aiming to minimize\"\"\"\ndef Objective(trial):\n    params = {\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n    \n    model = lgbm.LGBMRegressor(**params, device='GPU')\n    \n    \n    # taking positive because cross val score returns -ve values\n    scores = np.abs(cross_val_score(model, X, y, cv=kf, scoring='neg_root_mean_squared_error'))\n    \n    return sum(scores)\/len(scores)","5fc05c01":"study = optuna.create_study(direction=\"minimize\", study_name='LGBM optimization')\nstudy.optimize(Objective, n_trials=20)","ffca05f3":"lgb_params = study.best_params\nstudy.best_params","a8f00a3f":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import StackingRegressor\n\nfinal_estimator = GradientBoostingRegressor(n_estimators=200, \n                                            random_state=42)\n\nestimators = [('xgb', XGBRegressor(tree_method='gpu_hist', **XGB_params)),\n              ('lgb', lgbm.LGBMRegressor(device_type='gpu',**lgb_params)),  \n              ('cat', CatBoostRegressor(verbose=0, task_type='GPU', **catboost_params))] \n\nreg = StackingRegressor(\n        estimators=estimators,\n         final_estimator=final_estimator)","4b8f08fe":"# taking positive because cross val score returns -ve values\nscores = np.abs(cross_val_score(reg, X, y, cv=kf, scoring='neg_root_mean_squared_error'))\n\n# printing the output\nprint(f\"Scores - {scores}\\nMean - {sum(scores)\/len(scores)}\")","db609be5":"# training the final model and generating the submission.\n\nreg.fit(X, y)\nsub['target']=reg.predict(Xtest)\nsub.to_csv('submission.csv', index=False)","83740749":"<div id=\"fin\"> <\/div>\n\n## Training the final model and submission","7139aa13":"Plotting the distributions for the features will give us a fair idea about them","cfa61812":"<div id=\"cat\"> <\/div>\n\n### CatBoost","28d217af":"<div id=\"tunelgb\"> <\/div>\n\n### Tuning the LightGBM Model","5efe92bb":"**If you find the notebook useful or if it helps you, Consider UPVOTING my notebook!**","6c65e286":"<div id=\"tunecat\"> <\/div>\n\n### Tuning the Catboost model","31818cd5":"We improved our model from 0.7116 to 0.7089!","27fe9faf":"## Intro\n\nWelcome to my Notebook! <br>\n\nToday we will explore **Ensembling**. <br>\nEnsembling is a technique in which we combine various models in a defined way to increase the performance of the final model. <br>\nThe Dataset that we have here is a Continous Target dataset, which is a **Regression** Problem, and  Ensembling Regression models can be quite tricky. \n\nIn the classification task, just a simple average of probabilities can give a great score boost to our model, but in the regression part taking average might actually decrease your accuracy as we are predicting a continous value and a slight fluctuation can deviate the prediction by a fair amount.\n\nHence, We need to find best weights to combine our predictions. Here we use a **Gradient boosting** model to find these weights.\nThis way of combining models is called **Stacking**\n\nImplementing multiple models and then combining their predictions to learning weights again can be daunting, but Scikit-learn provides nice Classes and functions for us to implement them without much hardwork.\n\n\nRead on to find out more!","531806f0":"<div id=\"xgb\"> <\/div>\n\n### XGBoost","4969a74f":"### Table of contents\n\n* [Basic EDA](#basiceda) <br>\n* [Finding the best Models](#findbestindi) <br>\n    * [XGboost](#xgb)<br>\n    * [Tuning XGboost with optuna](#tunexgb)<br>\n    * [Catboost](#cat)<br>\n    * [Tuning Catboost with optuna](#tunecat)<br>\n    * [LightGBM](#lgb)<br>\n    * [Tuning LightGBM with optuna](#tunelgb)<br>\n* [Stacking the models](#stack)<br>\n* [Training the Final model](#fin)<br>","5e20839a":" We improved our model from 0.7027 to 0.6984! That is again a lot of improvement","385cae8a":"We improved our model from 0.7031 to 0.6988! thats a lot of improvement","c62fcc7f":"<div id=\"lgb\"> <\/div>\n\n### LightGBM","994e2aff":"<div id=\"findbestindi\"> <\/div>\n\n## Finding the Best Individual Models\n\nFirst we will try to find best individual Models for Regression","da974f7b":"We can see that all the distributions are skewed, hence if we use Linear models we should make it normal by applying transformations!","cb6d9006":"The Score is good, but I am sure we can do better. \n\n<div id=\"tunexgb\"> <\/div>\n\n### Tuning the XGBoost Model\n\nTuning with optuna is really simple, we just need to make a function that we need to optimize and then start optimizing. Here I have made an **Objective** to optimize. Optuna can also produce some amazaing visuals, check [this](https:\/\/www.kaggle.com\/debarshichanda\/optuna-automated-hyperparameter-tuning) out for example.","cfec3590":"### This score is better than any of our previous models!!! That is the power of ensembling.","978dee08":"Things to note about the features -- \n* All the Variables are numerical here.\n* They have similar mean and std (We have to analyze their distributions to arrive at a specific conclusion)\n* They might have negative values too\n* They max out around 1","4f1f8ba9":"<div id=\"stack\"> <\/div>\n\n## Stacking our Models\n\nNow that we have got our models built, we will combine them\n\nWe will keep the Models that we built before as **LEVEL 1** models and keep a Gradient Boosting Regressor as a **LEVEL 2** Model.\n\n![STACKING.png](attachment:STACKING.png)\n\nThis System might look hard to build, but Scikit-learn provides us a Class **Stacking Regressor** To attach various base estimators with a final estimators.","8400e69a":"<div id=\"basiceda\"> <\/div>\n\n### Basic EDA ","1e98331f":"This submission scores 0.69931! "}}