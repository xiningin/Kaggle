{"cell_type":{"c23edc73":"code","82e2ffc7":"code","da0fec6f":"code","4917ff61":"code","67f63a2c":"code","3c941923":"code","7bac8fd9":"code","77d40c8a":"code","0ead5e82":"code","b946f36d":"code","f688109f":"code","da8b74d4":"code","357dc982":"markdown","a746887c":"markdown","8ef991fb":"markdown","e5e81c4c":"markdown","533f2381":"markdown","b8f63054":"markdown","31d85cac":"markdown","6e55f9f0":"markdown","9f060152":"markdown","56966b10":"markdown","edc16bd1":"markdown","ab82b593":"markdown","ddf677c4":"markdown","07869366":"markdown"},"source":{"c23edc73":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nplt.rcParams['figure.figsize'] = [15, 10]\nimport warnings\nwarnings.filterwarnings('ignore')\ndt = pd.DataFrame()","82e2ffc7":"# This function uses 3 points coordinates, with a random_state 0 taken by defauls\ndef reset_clusters(M1 = [0,0], M2 = [2,2], M3 = [-2,2], random_state_cl = 0): \n    \n    # First cluster ( 50 samples )\n    np.random.seed(random_state_cl)\n    X1 = M1[0] + np.random.randn(1,50)[0]\n    Y1 = M1[1] + np.random.randn(1,50)[0]\n    L1 = [1]*(X1.size)\n\n\n    # Second cluster ( 60 samples )\n    X2 = M2[0] + np.random.randn(1,60)[0]\n    Y2 = M2[1] + np.random.randn(1,60)[0]\n    L2 = [2]*(X2.size)\n\n\n    # Third cluster ( 40 samples )\n    X3 = M3[0] + np.random.randn(1,40)[0]\n    Y3 = M3[1] + np.random.randn(1,40)[0]\n    D1 = {'X3' : X3,\n          'Y3' : Y3}\n    L3 = [3]*(X3.size)\n\n\n    X = np.concatenate((X1,X2,X3))\n    xmin = min(X) \n    xmax = max(X)\n\n    Y = np.concatenate((Y1,Y2,Y3))\n    ymin = min(Y) \n    ymax = max(Y)\n\n    L = np.concatenate((L1,L2,L3))\n\n    # Creating the data frame \n    dt['X'] = X          # x coordinates\n    dt['Y'] = Y          # y coordinates\n    dt['L_true'] = L     # true label showing which cluster each point of the set is really belonging to\n    dt['L_pred'] = 0     # will serve us to store the label to which cluster each point is predicted to belong \n\n    return dt","da0fec6f":"print(\"Original Data\")\ndt = reset_clusters()\nplt.figure()\nplt.scatter(dt['X'], dt['Y'], c = dt['L_true'])","4917ff61":"def reset_centroids(random_state_ce = 0):\n    np.random.seed(random_state_ce)\n    \n    # it is obvious that our centroids are related to the data points, that's why we call the reset_cluster funcion first\n    dt = reset_clusters(random_state_cl = random_state_ce) \n    \n    xmax = np.max(dt['X'])\n    xmin = np.min(dt['X'])\n    ymax = np.max(dt['Y'])\n    ymin = np.min(dt['Y'])\n    \n     #I used random() to generate random numbers between 0 and 1, so we can ajust the centroids depending on data intervals\n    \n    ax, bx, cx = xmin + np.random.random()*xmax, np.random.random()*xmax, np.random.random()*xmax\n    ay, by, cy = ymin + np.random.random()*ymax, np.random.random()*ymax, np.random.random()*ymax\n    \n    # besides returning the centroids coordinates, it is better to return the data frame for a later use\n    return dt, [ax, ay], [bx, by], [cx, cy] ","67f63a2c":"dt, c1, c2, c3 = reset_centroids()\nM1 = [0,0]\nM2 = [2,2]\nM3 = [-2,2]\nplt.scatter(dt['X'], dt['Y'], c = dt['L_true'], label = 'Original Data')\nplt.scatter([c1[0], c2[0], c3[0]], [c1[1], c2[1], c3[1]], c= 'r', s= 150, marker = '^', label = 'Initial Centroids')\nplt.scatter([M1[0],M2[0],M3[0]], [M1[1],M2[1],M3[1]], s = 200, marker = '*', c = 'orange', label = 'Real centers')\nplt.legend( loc='lower left')\nplt.show()","3c941923":"def k_means(d, random_state_km = 0, plotting = False):\n    \n    # the real centers\n    M1 = [0,0]\n    M2 = [2,2]\n    M3 = [-2,2]\n    \n    # initiate centroids and data points\n    dt, c1, c2, c3 = reset_centroids(random_state_ce = random_state_km)\n    ax, bx, cx, ay, by, cy = c1[0], c2[0], c3[0], c1[1], c2[1], c3[1]\n    \n    # plotting original data with initial centroids\n    if plotting :\n\n        plt.figure()\n        plt.scatter(dt['X'], dt['Y'], c = dt['L_true'], label = 'Original Data')\n        plt.scatter([c1[0], c2[0], c3[0]], [c1[1], c2[1], c3[1]], c= 'r', s= 150, marker = '^', label = 'Initial Centroids')\n        plt.title(\"Original Data\")\n        plt.legend( loc='lower left')\n        plt.show()\n    \n    \n    # we create lists where to store centroid coordinates in order to visualize their movement path\n    axs = [ax]\n    ays = [ay]\n    bxs = [bx]\n    bys = [by]\n    cxs = [cx]\n    cys = [cy]\n    if plotting :\n        plt.scatter(ax, ay, c= 'r', s= 100, alpha = 0.2, marker = '^')\n        plt.scatter(bx, by, c= 'b', s= 100, alpha = 0.2, marker = '^')\n        plt.scatter(cx, cy, c= 'g', s= 100, alpha = 0.2, marker = '^')\n            \n            \n    # int the scores list we will store the accuracy of the model after evry update ( iteration )\n    scores = []\n    \n    # list of iteration order ( useful for the plot of the scores )\n    iterations = []\n    for k in range(d):\n\n        # we create lists to store the points assigned to each centroid\n        Anx = []\n        Bnx = []\n        Cnx = []\n\n        Any = []\n        Bny = []\n        Cny = []\n        for i in range(dt.shape[0]): # loop over x and y coordinates for all points\n            x = dt['X'][i]\n            y = dt['Y'][i]\n            \n            # calculate the distances between a point and the 3 centroids\n            da = np.sqrt((x-ax)**2 + (y-ay)**2)\n            db = np.sqrt((x-bx)**2 + (y-by)**2)\n            dc = np.sqrt((x-cx)**2 + (y-cy)**2)\n            \n            # classify the point depending on the closest centroid (the minimal distance)\n            # we don't know yet which real cluster ( 1, 2 or 3 ) this point is belonging to\n            # that's why we just note ( class_A class_B or class_C )\n            # we will decode this classification later using the maximal accuracy \n            if da == min(da,db,dc):\n                dt['L_pred'][i] = 'class_A'\n                Anx.append(x)\n                Any.append(y)\n            elif db == min(da,db,dc):\n                dt['L_pred'][i] = 'class_B' \n                Bnx.append(x)\n                Bny.append(y) \n            elif dc == min(da,db,dc):\n                dt['L_pred'][i] = 'class_C'\n                Cnx.append(x)\n                Cny.append(y)\n\n        # Updating the centroids x and y coordinates, and adding the new values to the path lists\n        ax = np.mean(np.array(Anx))\n        axs.append(ax)\n        ay = np.mean(np.array(Any))\n        ays.append(ay)\n        bx = np.mean(np.array(Bnx))\n        bxs.append(bx)\n        by = np.mean(np.array(Bny))\n        bys.append(by)\n        cx = np.mean(np.array(Cnx))\n        cxs.append(cx)\n        cy = np.mean(np.array(Cny))\n        cys.append(cy)\n        \n        \n        labelize(dt)             # helps us decode which label ( 1, 2 or 3 ) every class ( A, B and C ) are really meaning \n        switch_labels(dt)        # uses the the results of \"labelize\" then assign each class to the real label\n        # those functions are defined later\n        \n\n        # updating scores and itertions lists\n        scores.append(test_score(dt))\n        iterations.append(k)\n        \n        \n        # plotting the path of centroids every iteration\n        if plotting :\n            pt = plt.figure\n            plt.scatter(dt['X'], dt['Y'], c = dt['L_pred'])\n            plt.scatter(ax, ay, c= 'r', s= 150, marker = '^', alpha = ((k+d)\/2)\/(2*d))\n            plt.scatter(bx, by, c= 'b', s= 150, marker = '^', alpha = ((k+d)\/2)\/(2*d))\n            plt.scatter(cx, cy, c= 'g', s= 150, marker = '^', alpha = ((k+d)\/2)\/(2*d))\n\n    # plotting the coordinates of centroids and centers for the last time ( convergence supposed )\n    if plotting :\n        pt = plt.figure\n        plt.scatter(dt['X'], dt['Y'], c = dt['L_pred'])\n        plt.scatter(ax, ay, c= 'r', s= 100, marker = '^', label = \"Centroid 1\")\n        plt.scatter(bx, by, c= 'b', s= 100, marker = '^', label = \"Centroid 2\")\n        plt.scatter(cx, cy, c= 'g', s= 100, marker = '^', label = \"Centroid 3\")\n        plt.scatter([M1[0],M2[0],M3[0]], [M1[1],M2[1],M3[1]], s = 200, marker = '*', c = 'orange', label = 'Real centers')\n        plt.title(\"Data after 3 Means clustering\")\n        plt.legend()\n\n    \n    # plotting a line of the centroids paths\n    if plotting : \n        axxx = plt.plot(axs, ays, c = 'r')\n        axxx = plt.plot(bxs, bys, c = 'b')\n        axxx = plt.plot(cxs, cys, c = 'g')\n        \n        # plotting the accuracy calculated in the scores list\n        d = plt.figure()\n        plt.plot(scores, 'o-', label = \"accuracy\")\n        plt.plot(iterations, [np.max(scores)]*len(iterations), label = \"max score\")\n        plt.title(\"score\")\n        plt.xlabel(\"iterations\")\n        plt.ylabel(\"score\")\n        plt.legend( )\n\n        plt.show()\n        \n        #print(\"The maximum score is = \", np.max(scores))\n        #print(\"The score at convergence is = \", scores[-1])  # those two scores are not always equal\n        \n    return scores[-1]\n        \ndef test_score(dt):\n    # the score is calculated by cumulating the sum of a boolean mask generated by the test dt['L_true'] == dt['L_pred']\n    # It reflects how well the predicted class matches with the real data labels after preprocessing the labels \n    return ((dt['L_true'] == dt['L_pred']).cumsum()\/dt.shape[0])[dt.shape[0]-1]\n\n\ndef labelize(dt):\n    # this function helps us recognize and match predicted classes with their adequate label\n    # it is using an iterative principle, we try all possibilities ( 3\u00b2 = 9 in case k = 3)\n    # then we choose the couples (class, label) giving the maximum possible accuracy\n    # it finally returns a dictionnary { key : class, value : label }\n    s_labels = ['class_A', 'class_B', 'class_C']\n    i_labels = [1, 2, 3]\n    pred_labels = []\n    for s in s_labels :\n        c_scores = []\n        for i in i_labels : \n            c = np.max((dt[dt['L_pred'] == s]['L_true'] == i).cumsum())\n            c_scores.append(c)\n        pred_labels.append(1 + np.argmax(np.array(c_scores)))\n    \n    return dict(zip(s_labels,pred_labels))\n\ndef switch_labels(dt):\n    # this function is acting on the dataframe using the results of the \"labelize\" function \n    # it applies the the modifications from 'labelize' dictionnary on the 'L_pred' series in the data\n    dt['L_pred'] = dt['L_pred'].map(labelize(dt))","7bac8fd9":"k_means(20, 5, plotting = True)","77d40c8a":"rdm = [] # here we store random states\nsc = []  # here we store accuracies\n    \nfor i in range(0, 20):\n    rdm.append(i)\n    sc.append(k_means(20, i))\n\nplt.xlabel(\"Random State values\")\nplt.ylabel(\"Accuracy value\")\nplt.plot(rdm, sc, 'o-')","0ead5e82":"from sklearn.cluster import KMeans # importing the model\n\n# we must convert the data into a set of lists of each point coordinates so we can fit it to th model ( [[x1,y1],.....] )\nD = []\nfor x, y in zip(dt['X'], dt['Y']):\n    D.append([x, y])\n    \nD = np.array(D)\nprint(len(D))","b946f36d":"model = KMeans(n_clusters= 3, random_state=5)\nmodel.fit(D)\nmodel.predict(D)\nplt.scatter(D[:,0], D[:,1], c = model.predict(D))","f688109f":"df = pd.DataFrame()\n\ndf['X'] = D[:, 0]\ndf['Y'] = D[:, 1]\ndf['L'] = model.predict(D)\n\ndic = { 1 : 1, \n        0 : 2,\n        2 : 3}\ndf['L'] = df['L'].map(dic)\n","da8b74d4":"model_score = ((df['L'] == dt['L_true']).cumsum()\/df.shape[0])[df.shape[0]-1] \nprint(\"Kmeans built in has a score of \", model_score)  \n# the same method of evaluation is done for the sklearn kmeans model predictions\n\ncompare_score = ((df['L'] == dt['L_pred']).cumsum()\/df.shape[0])[df.shape[0]-1] \nprint(\"The rate of harmony between our model and the sklean's is \", compare_score)  \n# the same method to compare the built in model and our iterative one","357dc982":"Now it's time to deeply understand the idea behind the algorithm and how those centroids get to ( most likely ) know where approximatively the original centers are placed.\n\nActually the algorithm uses a loop of 3 steps :\n\n* Initializing : It's what we have just done, by generating a number of centroids equal to number of cluster we want to classify.\n* Cluster assignment : Then we assign each point of the data set to the closest centroid\n* Update centroids : We move each centroid to the mean coordinate of the points allocated to this centroid \n\nWe repeat this process untill the centroids got their stable coordinates where the stop moving from, in this case we obtain the convergence results of the algorithm, and we're ready to classify each point.\n","a746887c":"Now let's see the influence of the intial positions of centroids, and how it affects the the convergence state and its accuracy\nTo illustrate that, we must try the process for different random states and store the accuracy each time we update the random_state.","8ef991fb":"This simulation allow as to see practically the movement path of each centroid, and how it's heading towards the closest center and converges to a position near it.\n\nAlso we see the evolution of the accuracy as long as we iterate, and after a certain iteration order we can observe that the accuracy no longer evolves, this is a sign of convergence of the algorithm","e5e81c4c":"# Random State","533f2381":"Next, we initilize three different random points, we will call them \"centroids\".         \nThe idea behind those centroids is using some algorithmic iterations ( we will see that later ) in order to make them converge to the exact original clusters centers.\n\nIn order to do that, we can impelement a function which generates three points depending on the x interval and y interval of variety of our data points\n\nNB : The choice of those centroids does not has an important influence on the results ","b8f63054":"Now our functions are complete, let's try to call the main function 'k_means' with 20 as number of iteration  with the same initial conditions","31d85cac":"We can visualize our original data before any processing, we use the true label ( 'L_true' ) to specify the color of each point depending on its original cluster","6e55f9f0":"Let's start with generating random clusters to process on along this explanaition.                        \nIt will be simplier if we use 2D normally distributed data with different mean coordinates, so we can apply our understanding of the algorithm on those points.\n\nThe function bellow is designed to create and return a pandas data frame, containing the 2D coordinates ( 'X' and 'Y' ) of three clusters normally distributed, a series of labels ( 'L_true' ) which allow us to classify each point and link it to its original cluster ( labels values are in {1, 2, 3} ), and it also contains a series ( 'Y_pred' ) initialized to the value of 0, so we can modify the valuess later by predicting the classification of each point of the set with our implementations of K Means Clustering algorithm.\n","9f060152":"## Importing essential libraries","56966b10":"## Defining needed functions","edc16bd1":"We can visualize the initial placements of our random centroids relatively to the original points and the original centers.\n","ab82b593":"# K Means Clustering\n\nK Means Cluster is a famous unsupervised learning method of hierarchical clustering. It allows to group a distinct observations in a data set into k different clusters.\nThe algorithm uses the distance ( euclidian, manhattan ... ) as the criteria of similarity between.\n\nIn this notebook, I tried to explain and simulate the principle of the algorithm step by step with my personal functions based on my understanding of the method.\n\nLet's do it !","ddf677c4":"It appears that the sklearn KMeans model is so close to the one wer just implemented ( 94% of similarity )","07869366":"## Compare to Sklearn model "}}