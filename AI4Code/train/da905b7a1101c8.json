{"cell_type":{"ac541ab4":"code","b5df135d":"code","cf29b1f6":"code","dd2c0d64":"code","14b4897d":"code","c050ca77":"code","40c38030":"code","0a949777":"code","769c5053":"code","1e5059ed":"code","86dc7fc6":"code","8542ed98":"code","ec7c1ff1":"code","c6cd3c0d":"code","a1f9cc2b":"code","012e5480":"code","90d5087d":"code","6d2257c1":"code","0487e8be":"markdown","3449a394":"markdown","01cbfdfb":"markdown","997c2c05":"markdown","ca4e96d8":"markdown","11769c35":"markdown","9bc495a2":"markdown","23afa6fe":"markdown","9c34328c":"markdown","e7171847":"markdown","93ed92be":"markdown","6dd6a3eb":"markdown","69fff4ac":"markdown","6cc80b96":"markdown","85dfbf0b":"markdown"},"source":{"ac541ab4":"%%time\nimport pandas as pd\nimport numpy as np\nimport gc\nimport os\nimport random\nimport glob\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings \nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\nfrom matplotlib_venn import venn2\n%matplotlib inline\n\ninputPath = '\/kaggle\/input\/used-car-price-forecasting\/'\ntrain = pd.read_csv(inputPath + 'train.csv')\ntest = pd.read_csv(inputPath + 'test.csv')\ntrain['flag'] = 'train'\ntest['flag'] = 'test'\n\ndf = pd.concat([train,test],axis=0)\ndel train,test\ngc.collect()","b5df135d":"%%time\n# fillna with most frequent value\ndf['year'].fillna(df['year'].mode()[0], inplace=True)\n\n# fillna with new category\ndf['model'] = df['model'].fillna('nan')\n\n# fillna with new category\ndf['condition'] = df['condition'].fillna('nan')\n\n# fillna with new value\ndf['cylinders'] = df['cylinders'].fillna('-2 cylinders')\ndf['cylinders'] = df['cylinders'].map(lambda x:x.replace('other','-1 cylinders'))\n\n# fillna with new category\ndf['fuel'] = df['fuel'].fillna('nan')\n\n# fillna with new value\ndf['odometer'] = df['odometer'].fillna('-1')\ndf['odometer'] = df['odometer'].astype(float)\n\n# fillna with new category\ndf['title_status'] = df['title_status'].fillna('nan')\n\n# fillna with new category\ndf['transmission'] = df['transmission'].fillna('nan')\n\n# fillna with new category\ndf['vin'] = df['vin'].fillna('nan')\n\n# fillna with new category\ndf['drive'] = df['drive'].fillna('nan')\n\n# fillna with new category\ndf['size'] = df['size'].fillna('nan')\n\n# fillna with new category\ndf['type'] = df['type'].fillna('nan')\n\n# fillna with new category\ndf['paint_color'] = df['paint_color'].fillna('nan')","cf29b1f6":"%%time\ndf['cylinders'] = df['cylinders'].map(lambda x:x.split(' ')[0])\ndf['cylinders'] = df['cylinders'].astype(int)","dd2c0d64":"%%time\ndf = pd.get_dummies(df, columns=['paint_color'])","14b4897d":"%%time\nfor c in ['region','manufacturer','model','condition','fuel','title_status','transmission', 'vin', 'drive', 'size', 'type', 'state']:\n    lbl = LabelEncoder()\n    df[c] = lbl.fit_transform(df[c].astype(str))","c050ca77":"%%time\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\ndef rmse(y_true, y_pred):\n    return (mean_squared_error(y_true, y_pred))** .5\n\ntrain_df = df[df['flag']=='train']\ntrain_df['price'] = np.log1p(train_df['price'])\ntest_df = df[df['flag']=='test']\ndrop_features = ['id', 'price', 'description', 'flag']\nfeatures = [f for f in train_df.columns if f not in drop_features]\n\ntrain_x, valid_x, train_y, valid_y = train_test_split(train_df[features], train_df['price'], test_size=0.2, random_state=1,stratify=train_df['manufacturer'])\nmodel = RandomForestRegressor(n_estimators=50,max_depth=10,random_state=1,verbose=1,n_jobs=-1)\nmodel.fit(train_x, train_y)\nvalid_preds = model.predict(valid_x)\nprint('Valid RMSE Score:', rmse(valid_y, valid_preds))","40c38030":"importances = model.feature_importances_\nindices = np.argsort(importances)[-20:]\nplt.figure(figsize=(20, 10))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","0a949777":"from sklearn.model_selection import KFold,StratifiedKFold,GroupKFold\n\ndef run_rf_kfold(train_df,test_df,features,target,folds,params):\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n\n    cv_list = []\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[features], train_df['manufacturer'])):\n        print ('FOLD:' + str(n_fold))\n        \n        train_x, train_y = train_df[features].iloc[train_idx], train_df[target].iloc[train_idx]\n        valid_x, valid_y = train_df[features].iloc[valid_idx], train_df[target].iloc[valid_idx]\n \n        model = params\n        model.fit(train_x, train_y)\n        oof_preds[valid_idx] = model.predict(valid_x)\n        oof_cv = rmse(valid_y,  oof_preds[valid_idx])\n        cv_list.append(oof_cv)\n        print (cv_list)\n        sub_preds += model.predict(test_df[features]) \/ folds.n_splits\n \n    cv = rmse(train_df[target],  oof_preds)\n    print('Full OOF RMSE %.6f' % cv)  \n    \n    train_df['prediction'] = oof_preds\n    test_df['prediction'] = sub_preds\n    \n    return train_df,test_df\n\ntrain_df = df[df['flag']=='train']\ntrain_df['price'] = np.log1p(train_df['price'])\ntest_df = df[df['flag']=='test']\n\ntarget = 'price'\ndrop_features = ['id', 'price', 'description', 'flag']\nfeatures = [f for f in train_df.columns if f not in drop_features]\nprint ('features:', len(features),features)\n\nn_splits = 5\nseed = 817\nfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\nparams = RandomForestRegressor(n_estimators=50,max_depth=10,random_state=1,verbose=1,n_jobs=-1)\ntrain_rf,test_rf = run_rf_kfold(train_df,test_df,features,target,folds,params)","769c5053":"import lightgbm as lgb\n\ndef run_lgb_kfold(train_df,test_df,features,target,folds,params):\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n\n    cv_list = []\n    feature_imps = pd.DataFrame()\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[features], train_df['manufacturer'])):\n        print ('FOLD:' + str(n_fold))\n        \n        train_x, train_y = train_df[features].iloc[train_idx], train_df[target].iloc[train_idx]\n        valid_x, valid_y = train_df[features].iloc[valid_idx], train_df[target].iloc[valid_idx]\n \n        dtrain = lgb.Dataset(train_x, label=train_y)\n        dval = lgb.Dataset(valid_x, label=valid_y, reference=dtrain) \n        bst = lgb.train(params, dtrain, num_boost_round=10000,\n            valid_sets=[dval,dtrain], verbose_eval=500,early_stopping_rounds=100, ) \n        \n        oof_preds[valid_idx] = bst.predict(valid_x, num_iteration=bst.best_iteration)\n        oof_cv = rmse(valid_y,  oof_preds[valid_idx])\n        cv_list.append(oof_cv)\n        print (cv_list)\n        sub_preds += bst.predict(test_df[features], num_iteration=bst.best_iteration) \/ folds.n_splits\n \n        feature_imp = pd.DataFrame(sorted(zip(bst.feature_importance('gain'),features)), columns=['Value','Feature'])\n        feature_imp['fold'] = n_fold\n        feature_imps = pd.concat([feature_imps,feature_imp],axis=0)\n        \n    cv = rmse(train_df[target],  oof_preds)\n    print('Full OOF RMSE %.6f' % cv)  \n    \n    train_df['prediction'] = oof_preds\n    test_df['prediction'] = sub_preds\n    \n    return train_df,test_df,feature_imps\n\ntrain_df = df[df['flag']=='train']\ntrain_df['price'] = np.log1p(train_df['price'])\ntest_df = df[df['flag']=='test']\n\ntarget = 'price'\ndrop_features = ['id', 'price', 'description', 'flag']\nfeatures = [f for f in train_df.columns if f not in drop_features]\nprint ('features:', len(features),features)\n\nn_splits = 5\nseed = 817\nfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\nparams = {\n               \"objective\" : \"regression\", \n               \"boosting\" : \"gbdt\", \n               \"metric\" : \"rmse\",  \n               \"max_depth\": -1,\n               \"min_data_in_leaf\": 30, \n               \"reg_alpha\": 0.1, \n               \"reg_lambda\": 0.1, \n               \"num_leaves\" : 31, \n               \"max_bin\" : 256,\n               \"learning_rate\" :0.2,\n               \"bagging_fraction\" : 0.9,\n               \"feature_fraction\" : 0.9\n}\ntrain_lgb,test_lgb,feature_imps = run_lgb_kfold(train_df,test_df,features,target,folds,params)","1e5059ed":"feature_imp = feature_imps.groupby(['Feature'])['Value'].mean().reset_index()\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[:10])\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()","86dc7fc6":"%%time\nfor c in ['region','manufacturer','model','condition','fuel','title_status','transmission', 'vin', 'drive', 'size', 'type', 'state']:\n    df['count_' + c] = df.groupby([c])['flag'].transform('count')","8542ed98":"%%time\ndf['mean_manufacturer_odometer'] = df.groupby(['manufacturer'])['odometer'].transform('mean')\ndf['std_manufacturer_odometer'] = df.groupby(['manufacturer'])['odometer'].transform('std')\ndf['max_manufacturer_odometer'] = df.groupby(['manufacturer'])['odometer'].transform('max')\ndf['min_manufacturer_odometer'] = df.groupby(['manufacturer'])['odometer'].transform('min')\ndf['maxmin_manufacturer_odometer'] = df['max_manufacturer_odometer'] - df['min_manufacturer_odometer']","ec7c1ff1":"%%time\ndf['num_chars'] = df['description'].apply(len) \ndf['num_words'] = df['description'].apply(lambda x: len(x.split()))\ndf['num_unique_words'] = df['description'].apply(lambda x: len(set(w for w in x.split())))","c6cd3c0d":"train_df = df[df['flag']=='train']\ntrain_df['price'] = np.log1p(train_df['price'])\ntest_df = df[df['flag']=='test']\n\ntarget = 'price'\ndrop_features = ['id', 'price', 'description', 'flag']\nfeatures = [f for f in train_df.columns if f not in drop_features]\nprint ('features:', len(features),features)\n\nn_splits = 5\nseed = 817\nfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\nparams = {\n               \"objective\" : \"regression\", \n               \"boosting\" : \"gbdt\", \n               \"metric\" : \"rmse\",  \n               \"max_depth\": -1,\n               \"min_data_in_leaf\": 30, \n               \"reg_alpha\": 0.1, \n               \"reg_lambda\": 0.1, \n               \"num_leaves\" : 31, \n               \"max_bin\" : 256,\n               \"learning_rate\" :0.2,\n               \"bagging_fraction\" : 0.9,\n               \"feature_fraction\" : 0.9\n}\ntrain_lgb,test_lgb,feature_imps = run_lgb_kfold(train_df,test_df,features,target,folds,params)","a1f9cc2b":"feature_imp = feature_imps.groupby(['Feature'])['Value'].mean().reset_index()\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[:10])\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()","012e5480":"params = {\n               \"objective\" : \"regression\", \n               \"boosting\" : \"gbdt\", \n               \"metric\" : \"rmse\",  \n               \"max_depth\": -1,\n               \"min_data_in_leaf\": 30, \n               \"reg_alpha\": 0.1, \n               \"reg_lambda\": 0.1, \n               \"num_leaves\" : 31, \n               \"max_bin\" : 256,\n               \"learning_rate\" :0.1,# 0.2 -> 0.1\n               \"bagging_fraction\" : 0.9,\n               \"feature_fraction\" : 0.9\n}\ntrain_lgb,test_lgb,feature_imps = run_lgb_kfold(train_df,test_df,features,target,folds,params)","90d5087d":"feature_imp = feature_imps.groupby(['Feature'])['Value'].mean().reset_index()\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[:10])\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()","6d2257c1":"%%time\ntest_lgb['price'] = np.expm1(test_lgb['prediction'])\ntest_df[['id','price']].to_csv('submission.csv',index=False)","0487e8be":"# Improve score by hyperparameter tuning","3449a394":"## Filling Missing Values","01cbfdfb":"## Text Count","997c2c05":"## One Hot Encoding","ca4e96d8":"# Improve score by feature engineering","11769c35":"## Ordinal Encoding","9bc495a2":"# Improve score by cross validation","23afa6fe":"## Training","9c34328c":"# Improve score by better algorythm(Lightgbm)","e7171847":"# Start From RandomForest Baseline","93ed92be":"## Count Encoding","6dd6a3eb":"## Aggregation","69fff4ac":"## Feature Importance","6cc80b96":"## Submission","85dfbf0b":"## Label Encoding"}}