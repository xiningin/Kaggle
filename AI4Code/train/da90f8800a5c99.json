{"cell_type":{"046f5719":"code","adc35d70":"code","f7af9b54":"code","5dadb60b":"code","988c88a9":"code","d611bc47":"code","928e83e8":"code","91cb3f58":"code","57af2bb1":"code","9e9e9be5":"code","c12ab3aa":"code","3d8b4210":"code","b8681edb":"code","7043716a":"code","050f80bf":"code","58a81717":"code","784dfba2":"code","7af3f23f":"code","d72072fb":"code","a42a4563":"code","cd00b73e":"code","2e6e0de9":"code","d339b751":"code","151c762c":"code","999fb255":"code","41556a40":"code","505606c3":"code","d3312cd1":"code","948f53b5":"code","2cea0ca0":"code","65162429":"code","2013dc0c":"code","fa654ac6":"code","aea53df0":"code","9574a9d1":"code","42f7a349":"code","4c1caffb":"code","e360b578":"code","fa8f0923":"code","7ef95a1a":"code","5ea19fa1":"code","db408bbf":"markdown","acda40e6":"markdown","a520e08a":"markdown","711881cd":"markdown","b686f67f":"markdown","35f55d4d":"markdown","1f4f3d3c":"markdown","3d5a5306":"markdown","da4922df":"markdown","e818495f":"markdown","1cbb90c4":"markdown","3d86e53b":"markdown","0d0ae7a0":"markdown","89a129e4":"markdown","f4095ee9":"markdown","ef5d81ae":"markdown","4221f388":"markdown","a037624b":"markdown","e264bc50":"markdown","24235891":"markdown","bec4928d":"markdown","61e278b6":"markdown","29180eb7":"markdown","b0a47085":"markdown","75a511ef":"markdown","90015969":"markdown"},"source":{"046f5719":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, StackingRegressor\nfrom sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV, ElasticNet, ElasticNetCV\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.svm import SVR\nimport warnings\nwarnings.filterwarnings('ignore')","adc35d70":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntotal = [train, test]","f7af9b54":"train.shape","5dadb60b":"print('Int64 columns are: ' + str(len(train.loc[:,train.dtypes == np.int64].columns)))\nprint('Str columns are: ' + str(len(train.loc[:,train.dtypes == np.object].columns)))\nprint('Float64 columns are: ' + str(len(train.loc[:,train.dtypes == np.float64].columns)))","988c88a9":"train.isnull().sum().sort_values(ascending=False)[train.isnull().sum().sort_values(ascending=False) > 0] \/ train.shape[0] * 100","d611bc47":"test.isnull().sum().sort_values(ascending=False)[test.isnull().sum().sort_values(ascending=False) > 0] \/ test.shape[0] * 100","928e83e8":"sns.distplot(train.SalePrice)\nsns.distplot(np.random.normal(train.SalePrice.mean(), train.SalePrice.std(), 1000))","91cb3f58":"print('Skewness: ', train.SalePrice.skew())\nprint('Kurtosis: ', train.SalePrice.kurt())","57af2bb1":"train['SalePrice'] = np.log1p(train['SalePrice'])\nsns.distplot(train.SalePrice)\nsns.distplot(np.random.normal(train.SalePrice.mean(), train.SalePrice.std(), 1000), color='green')","9e9e9be5":"print('Skewness: ', train.SalePrice.skew())\nprint('Kurtosis: ', train.SalePrice.kurt())","c12ab3aa":"for dataset in total:\n    dataset['MSSubClass'] = dataset['MSSubClass'].astype(np.object)\n    dataset['MoSold'] = dataset['MoSold'].astype(np.object)\n    dataset['YrSold'] = dataset['YrSold'].astype(np.object)","3d8b4210":"numeric_features = train.loc[:,train.dtypes == np.int64].columns.append(train.loc[:,train.dtypes == np.float64].columns)","b8681edb":"skew_feats = []\nfor feat in numeric_features:\n    if train[feat].skew() > 0.75:\n        skew_feats.append(feat)\n        \nfor dataset in total:\n    for feat in skew_feats:\n        dataset[feat] = dataset[feat].apply(np.log1p)","7043716a":"mapping = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, np.nan:0}\nmapping_1 = {'Gd': 4, 'Av': 3, 'Mn':2, 'No':1, np.nan: 0}\nmapping_2 = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, np.nan: 0}\nmapping_3 = {'Typ': 7, 'Min1': 6, 'Min2': 5, 'Mod': 4, 'Maj1': 3, 'Maj2': 2, 'Sev': 1, 'Sal': 0}\nmapping_4 = {'Fin': 3, 'RFn': 2, 'Unf': 1, np.nan: 0}\nmapping_5 = {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, np.nan: 0}\n\nfor dataset in total:\n    for column in ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']:\n        dataset[column] = dataset[column].map(mapping)\n    dataset['BsmtExposure'] = dataset['BsmtExposure'].map(mapping_1)\n    dataset['BsmtFinType1'] = dataset['BsmtFinType1'].map(mapping_2)\n    dataset['BsmtFinType2'] = dataset['BsmtFinType2'].map(mapping_2)\n    dataset['Functional'] = dataset['Functional'].map(mapping_3)\n    dataset['GarageFinish'] = dataset['GarageFinish'].map(mapping_4)\n    dataset['Fence'] = dataset['Fence'].map(mapping_5)\n    dataset[['LotFrontage','GarageYrBlt','MasVnrArea','BsmtFullBath','BsmtHalfBath']] = dataset[['LotFrontage','GarageYrBlt','MasVnrArea','BsmtFullBath','BsmtHalfBath']].fillna(0)\n    dataset[['MiscFeature','Alley','GarageType']] = dataset[['MiscFeature','Alley','GarageType']].fillna('No')\n    dataset['MasVnrType'] = dataset['MasVnrType'].fillna('None')\n\ntrain['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\ntest.at[1150, 'MasVnrType'] = 'BrkFace'\ntest.at[1116, 'GarageCars'] = 0\ntest.at[1116, 'GarageArea'] = 0\ntest.at[1116, 'GarageType'] = 0\ntest['BsmtFinSF1'] = test['BsmtFinSF1'].fillna(0)\ntest['BsmtUnfSF'] = test['BsmtUnfSF'].fillna(0)\ntest['BsmtFinSF2'] = test['BsmtFinSF2'].fillna(0)\ntest['TotalBsmtSF'] = test['TotalBsmtSF'].fillna(0)\ntest['MSZoning'] = test['MSZoning'].fillna(train['MSZoning'].mode()[0])\ntest['Utilities'] = test['Utilities'].fillna(train['Utilities'].mode()[0])\ntest['Functional'] = test['Functional'].fillna(train['Functional'].mode()[0])\ntest['SaleType'] = test['SaleType'].fillna(train['SaleType'].mode()[0])\ntest['Exterior1st'] = test['Exterior1st'].fillna(train['Exterior1st'].mode()[0])\ntest['Exterior2nd'] = test['Exterior2nd'].fillna(train['Exterior2nd'].mode()[0])","050f80bf":"for dataset in total:\n    dataset['HasPool'] = dataset['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    dataset['Has2ndFloor'] = dataset['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    dataset['HasGarage'] = dataset['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    dataset['HasBsmt'] = dataset['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    dataset['HasFireplace'] = dataset['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    dataset['BltSoldYrDiff'] = dataset['YrSold'].astype(np.int64) - dataset['YearBuilt']\n    dataset['TotalSF'] = dataset['TotalBsmtSF'] + dataset['1stFlrSF'] + dataset['2ndFlrSF']\n    dataset['TotalBathr'] = dataset['FullBath'] + 0.5 * dataset['HalfBath'] + dataset['BsmtFullBath'] + 0.5 * dataset['BsmtHalfBath']\n    dataset['TotalPorchSF'] = dataset['OpenPorchSF'] + dataset['3SsnPorch'] + dataset['EnclosedPorch'] + dataset['ScreenPorch'] + dataset['WoodDeckSF']","58a81717":"numeric_features = ['LotArea', 'OverallQual', 'OverallCond', 'YearBuilt',\n       'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n       '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',\n       'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr',\n       'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n       'MiscVal', 'LotFrontage', 'MasVnrArea', 'GarageYrBlt', 'SalePrice', 'BltSoldYrDiff',\n        'TotalSF', 'TotalBathr', 'TotalPorchSF']","784dfba2":"sns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[:5])\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[5:10])\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[10:15])\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[15:20])\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[20:25])\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[25:30])\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[30:35])\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[35:40])","7af3f23f":"sns.pairplot(data=train, y_vars=['SalePrice'], x_vars=['OverallQual','OverallCond','OpenPorchSF','TotalPorchSF'])\ntrain = train.drop(train[(train['OverallQual'] == 10) & (train['SalePrice'] < 12.5)].index)\ntrain = train.drop(train[(train['OverallCond'] == 2) & (train['SalePrice'] > 12)].index)\ntrain = train.drop(train[(train['OpenPorchSF'] > 3.5) & (train['SalePrice'] < 11)].index)\ntrain = train.drop(train[(train['TotalPorchSF'] > 6) & (train['SalePrice'] < 11)].index)\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=['OverallQual','OverallCond','OpenPorchSF','TotalPorchSF'])","d72072fb":"train.isnull().sum().sort_values(ascending=False)[train.isnull().sum().sort_values(ascending=False) > 0]","a42a4563":"test.isnull().sum().sort_values(ascending=False)[test.isnull().sum().sort_values(ascending=False) > 0]","cd00b73e":"columns = train.loc[:,train.dtypes == np.object].columns","2e6e0de9":"df_dummies = pd.get_dummies(data=pd.concat([train, test]), columns=columns)","d339b751":"df_dummies.shape","151c762c":"train = df_dummies.iloc[:train.shape[0]]\ntest = df_dummies.iloc[train.shape[0]:].drop('SalePrice', axis=1)","999fb255":"X = pd.DataFrame(StandardScaler().fit_transform(train), columns=train.columns).drop(['Id', 'SalePrice'], axis=1)\ny = train['SalePrice']\nscaled_test = pd.DataFrame(StandardScaler().fit_transform(test),columns=test.columns).drop('Id', axis=1)","41556a40":"reg = RidgeCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in RidgeCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in RidgeCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)\nprint(\"Ridge picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","505606c3":"reg = ElasticNetCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in ElasticNetCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in ElasticNetCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)\nprint(\"ElasticNet picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\nridge_coef = coef[coef != 0]","d3312cd1":"enet_coef = coef[coef != 0]\nimp_coef = enet_coef.sort_values()\nimport matplotlib\nplt.figure(figsize=(8,18))\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using ElasticNet Model\")","948f53b5":"reg = LassoCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","2cea0ca0":"lasso_coef = coef[coef != 0]\nimp_coef = lasso_coef.sort_values()\nimport matplotlib\nplt.figure(figsize=(8,18))\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")","65162429":"models = [('DTR', DecisionTreeRegressor()),\n          ('RFR', RandomForestRegressor()),\n          ('KNR', KNeighborsRegressor()),\n          ('GBR', GradientBoostingRegressor()),\n          ('LR', LinearRegression()),\n          ('XGB', XGBRegressor()),\n          ('LGBM', LGBMRegressor()),\n          ('SVR', SVR()),\n          ('Ridge', Ridge(alpha=10)),\n          ('Lasso', Lasso(alpha=0.003487)),\n          ('ENet', ElasticNet(alpha=0.006974))]\n","2013dc0c":"results = []\nnames = []\n\nfor coef in [ridge_coef.index, enet_coef.index, lasso_coef.index]:\n    for name, model in models:\n        kfold = KFold(n_splits=10, random_state=21)\n        cv_results = cross_val_score(model, X[coef], y, cv=kfold, scoring='neg_mean_squared_error')\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)","fa654ac6":"params = {\n    'alpha': [0.001]\n    }\n\nreg = Lasso()\nrs = GridSearchCV(estimator = reg, param_grid = params, \n                               cv = 10, verbose= 5, n_jobs = -1, scoring='neg_mean_squared_error')\nrs.fit(X[lasso_coef.index],y)\nprint(rs.best_score_)\nprint(rs.best_estimator_)\nlasso = rs.best_estimator_","aea53df0":"params = {\n    'learning_rate': [0.01],\n    'n_estimators': [2000],\n    'max_depth': [11],\n    'min_samples_split': [200],\n    'min_samples_leaf': [10],\n    'max_features': ['sqrt'],\n    'subsample': [0.85]\n    }\n\nreg = GradientBoostingRegressor()\nrs = GridSearchCV(estimator = reg, param_grid = params, \n                               cv = 10, verbose= 5, n_jobs = -1, scoring='neg_mean_squared_error')\nrs.fit(X[lasso_coef.index],y)\nprint(rs.best_score_)\nprint(rs.best_estimator_)\ngbr = rs.best_estimator_","9574a9d1":"params = {\n    'alpha': [0.001]\n    }\n\nreg = ElasticNet()\nrs = GridSearchCV(estimator = reg, param_grid = params, \n                               cv = 10, verbose= 5, n_jobs = -1, scoring='neg_mean_squared_error')\nrs.fit(X[lasso_coef.index],y)\nprint(rs.best_score_)\nprint(rs.best_estimator_)\nelasticnet = rs.best_estimator_","42f7a349":"params = {\n    'learning_rate': [0.01],\n    'n_estimators': [3000],\n    'max_depth': [3],\n    'min_child_weight': [5],\n    'gamma': [0],\n    'colsample_bytree': [0.65],\n    'subsample': [0.6],\n    'reg_alpha':[1e-6]\n    }\n\nreg = XGBRegressor()\nrs = GridSearchCV(estimator = reg, param_grid = params, \n                               cv = 10, verbose= 5, n_jobs = -1, scoring='neg_mean_squared_error')\nrs.fit(X[lasso_coef.index],y)\nprint(rs.best_score_)\nprint(rs.best_estimator_)\nxgboost = rs.best_estimator_","4c1caffb":"params = {\n    'learning_rate': [0.01],\n    'n_estimators': [3000],\n    'max_depth': [3],\n    'min_child_weight': [1],\n    'gamma': [0],\n    'colsample_bytree': [0.8],\n    'subsample': [0.6],\n    }\n\nreg = LGBMRegressor()\nrs = GridSearchCV(estimator = reg, param_grid = params, \n                               cv = 10, verbose= 5, n_jobs = -1, scoring='neg_mean_squared_error')\nrs.fit(X[lasso_coef.index],y)\nprint(rs.best_score_)\nprint(rs.best_estimator_)","e360b578":"level_0 = [('ENet',elasticnet),('GBR', gbr)]\nlevel_1 = lasso\n\nmodel = StackingRegressor(estimators=level_0, final_estimator=level_1, cv=10)\n\ncv = KFold(n_splits=10, random_state=21)\nscores = cross_val_score(model, X[lasso_coef.index],y,cv=cv,scoring='neg_mean_absolute_error')\nprint(scores.mean())","fa8f0923":"model.fit(X[lasso_coef.index],y)","7ef95a1a":"pred_stacked = model.predict(scaled_test[lasso_coef.index])\npred = np.expm1(pred_stacked)\nsub = test[['Id']]\nsub['SalePrice'] = pred\nsub[['Id', 'SalePrice']].to_csv('pred_submission.csv', index=False, encoding='utf-8')","5ea19fa1":"sub.head()","db408bbf":"Getting dummy variables for all categorical features","acda40e6":"Checking missing values","a520e08a":"Ridge keeps 270 of 272 variables, so we won't use it.","711881cd":"I mapped all the object type variables that had ordinality, for example variables that have an evaluation scale. I filled other missing data with the mode of the variable or 0. This cell is a summary of all the work done to clean the dataset.","b686f67f":"Lasso keeps 96 variables.","35f55d4d":"This dataset has 81 columns with 38 numeric variables and 43 text variables. I won't explain every variable in this notebook, but I'll give some intuitions behind every move I did in order to clean the dataset.","1f4f3d3c":"Evaluating model performances, highly skewed variables affected results negatively, so I applied the same transformation made on SalePrice.","3d5a5306":"LightGBM is not giving good performances so I won't use it for final predictions","da4922df":"Plotting all variables vs target variable to check for some outliers","e818495f":"I submitted the results of every model I tuned and the results were good for the Gradient Boosting and XGBoost, but the best result for me (0.12173) in the Kaggle competition (top 13% of the leaderboard) was given by stacking three models: this is an ensemble technique, ElasticNet and Gradient Boost are trained individually on the training set, then their predictions are stacked to fit a final estimator, that in this case is Lasso.","1cbb90c4":"Standard scaling data","3d86e53b":"I added some new variables:\n- HasPool: 1 if house has pool, 0 otherwise\n- Has2ndFloor: 1 if house has 2nd floor, 0 otherwise\n- HasGarage: 1 if house has garage, 0 otherwise\n- HasBsmt: 1 if house has basement, 0 otherwise\n- HasFireplace: 1 if house has fireplace, 0 otherwise\n- BltSoldYrDiff: time passed between year of built and last sale year in years\n- TotalSF: house total surface\n- TotalBathr: total bathrooms\n- TotalPorchSF: porch total surface","0d0ae7a0":"Let's test all these models using the Ridge, ElasticNet and Lasso feature selection.","89a129e4":"For XGBoost and LightGBM I proceeded in the same way of Gradient Boosting for the hyperparameter optimization, some hyperparameters are different.\n\nI found really helpful this page for the tuning procedure:\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n\nHere you can find a deep explanation of XGBoost parameters:\nhttps:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html","f4095ee9":"Removing some outliers","ef5d81ae":"I searched the best alpha from 1e-10 and 1 resulting to be 0.001","4221f388":"For Elastic Net, I used the same technique used for Lasso.","a037624b":"For Gradient Boosting hyperparameter tuning, I started finding the best n_estimator for 0.1 learning rate in order to reduce the numerical computation of high n_estimators, then i tuned in a few steps:\n- max_depth and min_samples_split together\n- min_samples_split and min_sample_leaf together\n- max_features\n- subsample\n\nAfter all I reduced learning rate to 0.01 increasing the n_estimetors, to reach better performances.\n\nHyperparameters detailed explanation can be found here:\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html\n\nI found really helpful for hyperparameter tuning detailed explanation this page:\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/","e264bc50":"Transforming categorical variables into dummies, increased the numeber of columns, so we need to do some feature selection. In this case I did it in three ways, using Ridge, Lasso and ElasticNet models.","24235891":"Checking the SalePrice, our target variable, we can see that it's highly positive skewed and leptokurtic. So this distribution is far from a normal distribution. Let's log(1+x) transform the dependent variable and solve normality issues.","bec4928d":"We can see that columns like PoolQC, MiscFeature, Alley and Fence have an high rate of missing values. We won't delete this columns because we can keep some information and we can use them for creating other features.","61e278b6":"We can see that with Lasso feature selection we generally meet better performances by negative mean squared error. Using features picked by Lasso model, we can exclude some models: Decision Tree, Linear Regression, K-nearest neighbour, Support Vector Regression and Random Forest. XGBoost, LightGBM and Gradient Boosting can be good but they need hyperparameter tuning to reach better performances. I also excluded Ridge because better performances are reached at higher value of the alpha hyperparameter: high values of alpha (like 10 or more) highly reduce the complexity of the model fit, so it's almost averaging the points, giving bad performances on the test set.","29180eb7":"These numerical variable are categorical so we can transform them in object type.","b0a47085":"Importing some libraries and reading train and test datasets.","75a511ef":"ElasticNet keeps 98 variables.","90015969":"# House Price Kaggle Competition"}}