{"cell_type":{"d50f9405":"code","014016eb":"code","4dca0297":"code","71844eea":"code","11b5add0":"code","29796846":"code","e7b25cfd":"code","5b4aec0c":"code","5dba9bc8":"code","6c8b291e":"code","f98e706a":"code","d98d08cc":"code","f22c28a0":"code","e85dea18":"code","fdaec51b":"code","01657f04":"code","4ed04ba3":"code","1cda3d89":"code","664f0fbd":"code","96d81c07":"code","01dab3cb":"code","696c55c8":"code","c4a36789":"code","4e3aa84c":"code","85246b94":"code","ea5c41b6":"code","d761d52d":"code","fda962c2":"code","8311e560":"code","efee060e":"code","bcd14a64":"code","3eaca761":"code","4987b592":"code","6f336b78":"code","b6a7c104":"code","a6a5b052":"code","a755e78e":"code","e2a5c748":"code","90ed68aa":"code","0ec9906c":"code","c41a2e00":"code","f3b34454":"code","a5668f83":"code","35c327b2":"code","b92356a5":"code","f97693ea":"code","7499aad5":"code","cdf48a68":"code","85eae622":"code","f9240d1d":"code","5c721d4b":"code","6482fd78":"code","09876d93":"markdown","4b14d127":"markdown","d28a2d61":"markdown","784bc5fa":"markdown","a3b7e2c8":"markdown","3940d748":"markdown","f68a11ca":"markdown","f08cabae":"markdown","7a27d632":"markdown","85ba5f69":"markdown","405e0639":"markdown","dff041ea":"markdown","39e317c1":"markdown","f76047f4":"markdown","29568dbc":"markdown","d28ea15c":"markdown","c80decfc":"markdown","1e600fbd":"markdown","16334172":"markdown","eeaa6427":"markdown","7b1412c4":"markdown","94dcbb5c":"markdown","ad8cda63":"markdown","479b9f40":"markdown","eda6d6f4":"markdown","096c06d4":"markdown","531180c3":"markdown","d9194c62":"markdown","185e1bea":"markdown","26d3fb2c":"markdown","21bfd8ce":"markdown","f8fd39f8":"markdown","d8945b78":"markdown","9790e6f4":"markdown","15f73391":"markdown","b7f057f2":"markdown","a881f983":"markdown","16647a4a":"markdown","fa520228":"markdown","c6b931da":"markdown","0ef550ad":"markdown","21e17944":"markdown","bf6dc19c":"markdown","2de52df5":"markdown","52484759":"markdown","a32be2dc":"markdown","d8356307":"markdown","33a8c73c":"markdown","171bf098":"markdown","17eee43a":"markdown","c0ee73e4":"markdown","d956a655":"markdown","3ee1b585":"markdown","d551c6c8":"markdown","492a66b8":"markdown","6c47528e":"markdown","347aaf52":"markdown","d0cd711d":"markdown","db7bfdc1":"markdown"},"source":{"d50f9405":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.basemap import Basemap","014016eb":"conda install -c conda-forge basemap-data-hires #com esse pacote baixado \u00e9 possivel usar mapas com defini\u00e7\u00e3o melhor","4dca0297":"train = pd.read_csv(\"..\/input\/atividade-regressao-PMR3508\/train.csv\", index_col='Id')\nXtest = pd.read_csv(\"..\/input\/atividade-regressao-PMR3508\/test.csv\", index_col='Id')","71844eea":"train.head()","11b5add0":"ytrain = train['median_house_value'] #target da an\u00e1lise","29796846":"Xtrain = train[['longitude', 'latitude', 'median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']] #vari\u00e1veis onde os modelos ser\u00e3o treinados","e7b25cfd":"sns.set() #esse comando faz com que as visuliza\u00e7\u00f5es fiquem visualmente melhores","5b4aec0c":"Xtrain.hist(bins=60, figsize=(20,20))","5dba9bc8":"Xtrain['population'].skew()","6c8b291e":"plt.figure(figsize=(12, 6))\nsns.distplot(Xtrain['population'])","f98e706a":"plt.figure(figsize=(12, 6))\nsns.distplot(np.log(Xtrain['population']))","d98d08cc":"sns.heatmap(train.corr())","f22c28a0":"plt.figure(figsize=(18, 14))\nm = Basemap(projection='lcc', resolution='i', lat_0=37.5, lon_0=-119, width=1.2E6, height=1.3E6) #desenhando o map \nm.drawlsmask(resolution='f', grid=1.25, ocean_color='lightcyan', land_color='beige') #colocando cores no mar e terra\nm.drawcoastlines() #desenhando a costa\nm.drawcountries(color='black') #desenhando paises\nm.drawstates(color='gray') #desenhando os estados\nm.scatter(Xtrain['longitude'].values, Xtrain['latitude'].values, latlon=True, s=Xtrain['population']\/50, c=ytrain, cmap='viridis', alpha=0.4, label='population') #colocando os dados na imagem\nplt.title(\"Valor Mediano de Im\u00f3veis e Popula\u00e7\u00e3o na Calif\u00f3rnia\", fontsize=20)\nplt.annotate('Los Angeles', fontweight=550, xy=m(-118.24,34.05), xytext=m(-118.24-3,34.05-1), arrowprops=dict(facecolor='black', width=3))\nplt.annotate('San Diego', fontweight=550, xy=m(-117.15,32.71), xytext=m(-117.15-3,32.71-0.5), arrowprops=dict(facecolor='black', width=3))\nplt.annotate('San Francisco', fontweight=550, xy=m(-122.41,37.77), xytext=m(-122.41-3,37.77-1), arrowprops=dict(facecolor='black', width=3))\n#escolhi apontar no mapa as 3 cidades mais conhecidas da California para melhorar a visualizacao\ncbar = plt.colorbar(extend='both')\ncbar.set_label('Median House Value', fontsize=16)\nlegends = []\nlabels = [100, 1000, 10000]\nfor a in labels:\n    legends.append(plt.scatter([], [], c='k', alpha=0.4, s=a\/50))\nplt.legend(legends, labels, scatterpoints=1, labelspacing=1, edgecolor='black', frameon=True, shadow=True, loc='upper right', title='Population') #colocando legenda na figura","e85dea18":"Xtrain_original = Xtrain.copy()\nXtest_original = Xtest.copy()","fdaec51b":"Xtrain.isna().any()","01657f04":"Xtest.isna().any()","4ed04ba3":"features_assimetricas = []\nskew_min = 1  #qualquer coluna com assimtria maior que 1 ser\u00e1 selecionada\nfor col in Xtrain.columns:\n    skew = Xtrain[col].skew()\n    if skew < 0:\n        skew = -skew\n    if skew > skew_min:\n        features_assimetricas.append(col)\nprint(features_assimetricas)","1cda3d89":"for col in features_assimetricas:\n    Xtrain[col] = np.log(Xtrain[col])\n    Xtest[col] = np.log(Xtest[col])","664f0fbd":"Xtrain.hist(bins=60, figsize=(20,20))","96d81c07":"Xtrain['pop_ratio'] = Xtrain['population']\/Xtrain['households']\nXtrain['rooms_ratio'] = Xtrain['total_rooms']\/Xtrain['households']\nXtrain['bedrooms_ratio'] = Xtrain['total_bedrooms']\/Xtrain['households']","01dab3cb":"Xtest['pop_ratio'] = Xtest['population']\/Xtest['households']\nXtest['rooms_ratio'] = Xtest['total_rooms']\/Xtest['households']\nXtest['bedrooms_ratio'] = Xtest['total_bedrooms']\/Xtest['households']","696c55c8":"Xtrain.isna().any()","c4a36789":"Xtest.isna().any()","4e3aa84c":"Xtest['bedrooms_ratio'].fillna(0, inplace=True)\nXtest['pop_ratio'].fillna(0, inplace=True)\nXtest['rooms_ratio'].fillna(0, inplace=True)","85246b94":"Xtrain.isin([np.inf]).any()","ea5c41b6":"Xtest.isin([np.inf]).any()","d761d52d":"Xtest['pop_ratio'].replace([np.inf], np.nan, inplace=True)\nXtest['rooms_ratio'].replace([np.inf], np.nan, inplace=True)\nXtest['pop_ratio'].fillna(Xtest['pop_ratio'].max(), inplace=True)\nXtest['rooms_ratio'].fillna(Xtest['rooms_ratio'].max(), inplace=True)","fda962c2":"def haversine(lon1, lat1, lon2, lat2):\n    '''formula adaptada de https:\/\/stackoverflow.com\/questions\/40452759\/pandas-latitude-longitude-to-distance-between-successive-rows\n    e de https:\/\/medium.com\/@petehouston\/calculate-distance-of-two-locations-on-earth-using-python-1501b1944d97'''\n    lon1, lat1, lon2, lat2 = np.radians([lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = np.sin(dlat \/ 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon \/ 2) ** 2\n    return 2 * 6371 * np.arcsin(np.sqrt(a))","8311e560":"Xtrain['dla'] = haversine(Xtrain['longitude'], Xtrain['latitude'], pd.Series(-118.24, index=Xtrain.index), pd.Series(34.05, index=Xtrain.index))\nXtrain['dsd'] = haversine(Xtrain['longitude'], Xtrain['latitude'], pd.Series(-117.15, index=Xtrain.index), pd.Series(32.71, index=Xtrain.index))\nXtrain['dsf'] = haversine(Xtrain['longitude'], Xtrain['latitude'], pd.Series(-122.41, index=Xtrain.index), pd.Series(37.77, index=Xtrain.index))","efee060e":"Xtest['dla'] = haversine(Xtest['longitude'], Xtest['latitude'], pd.Series(-118.24, index=Xtest.index), pd.Series(34.05, index=Xtest.index))\nXtest['dsd'] = haversine(Xtest['longitude'], Xtest['latitude'], pd.Series(-117.15, index=Xtest.index), pd.Series(32.71, index=Xtest.index))\nXtest['dsf'] = haversine(Xtest['longitude'], Xtest['latitude'], pd.Series(-122.41, index=Xtest.index), pd.Series(37.77, index=Xtest.index))","bcd14a64":"Xtrain['d_grande_centro'] = Xtrain[['dla', 'dsd', 'dsf']].min(axis=1)\nXtest['d_grande_centro'] = Xtest[['dla', 'dsd', 'dsf']].min(axis=1)","3eaca761":"sns.heatmap(pd.merge(Xtrain[['dla', 'dsf', 'dsd', 'd_grande_centro', 'pop_ratio', 'rooms_ratio', 'bedrooms_ratio']], ytrain, left_index=True, right_index=True).corr())","4987b592":"pd.merge(Xtrain, ytrain, left_index=True, right_index=True).corr()['median_house_value'].sort_values(ascending=False)","6f336b78":"from sklearn.preprocessing import StandardScaler","b6a7c104":"scaler = StandardScaler()","a6a5b052":"Xtrain_n = scaler.fit_transform(Xtrain)\nXtest_n = scaler.fit_transform(Xtest)","a755e78e":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score","e2a5c748":"lr = LinearRegression()\nscores = cross_val_score(lr, Xtrain_n, ytrain, cv=5, scoring='neg_mean_squared_error')\nprint( np.sqrt(-scores).mean())","90ed68aa":"from sklearn.linear_model import Ridge, Lasso, ElasticNet","0ec9906c":"lasso = Lasso()\nscores = cross_val_score(lasso, Xtrain_n, ytrain, cv=5, scoring='neg_mean_squared_error')\nprint( np.sqrt(-scores).mean())","c41a2e00":"ridge = Ridge()\nscores = cross_val_score(ridge, Xtrain_n, ytrain, cv=5, scoring='neg_mean_squared_error')\nprint( np.sqrt(-scores).mean())","f3b34454":"eln = ElasticNet()\nscores = cross_val_score(eln, Xtrain_n, ytrain, cv=5, scoring='neg_mean_squared_error')\nprint( np.sqrt(-scores).mean())","a5668f83":"from sklearn.neighbors import KNeighborsRegressor","35c327b2":"knr = KNeighborsRegressor()\nscores = cross_val_score(knr, Xtrain_n, ytrain, cv=5, scoring='neg_mean_squared_error')\nprint( np.sqrt(-scores).mean())","b92356a5":"knr = KNeighborsRegressor(weights='distance')\nscores = cross_val_score(knr, Xtrain_n, ytrain, cv=5, scoring='neg_mean_squared_error')\nprint( np.sqrt(-scores).mean())","f97693ea":"from sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=2, include_bias=False) #por simplicidade foi escolhido que o grau ser\u00e1 2\nX_poly = poly_features.fit_transform(Xtrain_n)","7499aad5":"lr = LinearRegression()\nscores = cross_val_score(lr, X_poly, ytrain, cv=5, scoring='neg_mean_squared_error')\nprint( np.sqrt(-scores).mean())","cdf48a68":"from sklearn.model_selection import GridSearchCV","85eae622":"param_grid = {'n_neighbors': [i for i in range(3, 16)]}\nknr = KNeighborsRegressor(weights='distance')\ngrid_search = GridSearchCV(knr, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(Xtrain_n, ytrain)","f9240d1d":"grid_search.best_params_","5c721d4b":"knr = KNeighborsRegressor(weights='distance', n_neighbors=10)\nscores = cross_val_score(knr, Xtrain_n, ytrain, cv=5, scoring='neg_mean_squared_error')\nprint( np.sqrt(-scores).mean())","6482fd78":"knr = KNeighborsRegressor(weights='distance', n_neighbors=10)\nknr.fit(Xtrain_n, ytrain)\npredictions = knr.predict(Xtest_n)\nsub = pd.DataFrame(Xtest.index)\nsub['median_house_value'] = predictions\nsub.to_csv('submissao.csv', index=False, index_label='Id')","09876d93":"### 4.2.1 Lasso","4b14d127":"Assim como no caso do Lasso o modelo Ridge tamb\u00e9m \u00e9 uma regress\u00e3o linear regularizado, mas seu fator de regulariza\u00e7\u00e3o tem grau 2.","d28a2d61":"## 2.2 Correla\u00e7\u00f5es","784bc5fa":"Tamb\u00e9m ao fazer a divis\u00e3o de features houve a cria\u00e7\u00e3o de \"infinito\", provavelmente divis\u00e3o por zero, estes ser\u00e3o imputados usando o maior valor finito do set.","a3b7e2c8":"O modelo Lasso mostrou-se pouco superior a regress\u00e3o linear simples em termos de desempenho.","3940d748":"Este notebook tem por objetivo analisar o conjunto de dados California Housing e contruir um modelo de regress\u00e3o que consiga prever o valor mediano de imoveis. Inicialmente se realizar\u00e1 uma an\u00e1lise explorat\u00f3ria dos dados fazendo uso de visualiza\u00e7\u00f5e que ajudar\u00e3o a obter insights sobre os dados e suas rela\u00e7\u00f5es, ap\u00f3s isso ser\u00e1 realizada uma fase preparat\u00f3ria dos dados, isto inclui limpeza, normaliza\u00e7\u00e3o, feature engineering e afins, a fase de prepara\u00e7\u00e3o de dados ser\u00e1 seguida pela contru\u00e7\u00e3o e compara\u00e7\u00e3o de modelos. Depois de estabelecido o melhor modelo, este ser\u00e1 utilizado para se realizar previs\u00f5es sobre o set de testes.","f68a11ca":"## 1.2 Carregando Dados","f08cabae":"Pela figura \u00e9 poss\u00edvel ver que a assimetria das features foi reduzida.","7a27d632":"### 3.3.1 Raz\u00f5es entre Features","85ba5f69":"### 3.3.2 Features Geogr\u00e1ficas\n\nComo foi visto as features geograficas possuem uma correla\u00e7\u00e3o com o valor do imovel, por\u00e9m dif\u00edcil de captar, por esse motivo faz sentido criar novas features que tentem traduzir melhor essa reala\u00e7\u00e3o. No caso foram escolhidas 3 novas features as dist\u00e2ncias das amostras a Los Angeles, San Francisco e San Diego. \u00c9 esperado que por serem as regi\u00f5es onde se concentram os imoveis de pre\u00e7o alto se consiga utilizar essas features para melhorar os modelos de regress\u00e3o. Para o calculo da dist\u00e2ncia ser\u00e1 utlizada a f\u00f3rmula de Haversine, que \u00e9 uma das formas de se calcular dist\u00e2ncias de pontos na terra com uma precis\u00e3o razo\u00e1vel (a f\u00f3rmula sup\u00f5e a Terra uma esfera perfeita, por isso algumas imprecis\u00f5es pequenas que ser\u00e3o \u00ednfimas para este cas0).","405e0639":"Como se pode ver pelo mapa a localiza\u00e7\u00e3o gerogr\u00e1fica tem uma alta correla\u00e7\u00e3o com o valor, percebe-se qua as regi\u00f5es metropolitanas de San Francisco, Los Angeles e San Diego tem no geral valores mais alto do que nos resto do estado (muitas vezes a combina\u00e7\u00e3o desses regi\u00f5es \u00e9 chamada de San-San). Apesar da correla\u00e7\u00e3o clara \u00e9 bem poss\u00edvel que um modelo de regress\u00e3o linear n\u00e3o consiga aprend\u00ea-la porque ela n\u00e3o \u00e9 uma correla\u00e7\u00e3o simples. Essa visualiza\u00e7\u00e3o usou como base para o c\u00f3digo o livro Python Data Science Handbook de Jake VandePlas e o seguinte Jupyter Notebook de Aur\u00e9lien Geron https:\/\/github.com\/ageron\/handson-ml2\/blob\/master\/02_end_to_end_machine_learning_project.ipynb. Coordenadas das cidades forma obtidas de https:\/\/dateandtime.info\/ .","dff041ea":"Agora com os dados normalizados pode-se prosseguir para a cria\u00e7\u00e3o e teste de modelos","39e317c1":"Agora sem infinitos os nans pode-se prosseguir com a an\u00e1lise.","f76047f4":"## 3.1 Dados Faltantes\n\nPara a correta aplica\u00e7\u00e3o dos algoritmos \u00e9 necess\u00e1rio remover ou imputar os dados faltantes","29568dbc":"Como ser\u00e3o feitas modifica\u00e7\u00f5es nos conjuntos de treino e teste faz sentido guardar c\u00f3pias deles.","d28ea15c":"Como se pode ver pelo mapa de calor e pela tabela de fato a feature d_grande_centro se correlaciona com o valor mediano, em valores absolutos \u00e9 a segunda feature que melhor se correlaciona. Tamb\u00e9m se pode ver que com a excess\u00e3o de pop_ratio as features novas n\u00e3o se correlacionam melhor do que as antigas.","c80decfc":"## 4.1 Regress\u00e3o Linear Simples ","1e600fbd":"Viu-se na parte anterior que o modelo cujo desempenho foi melhor foi o regressor KNN com k=10, portanto este ser\u00e1 o modelo aplicado ao conjunto de testes.","16334172":"## 3.3 Feature Engineering","eeaa6427":"## Regress\u00e3o Polinomial \n\nOutro tipo de regress\u00e3o muito \u00fatil \u00e9 a regress\u00e3o polinomial, basicamente tenta-se explicar os dados atrav\u00e9s de um polin\u00f4mio de grau n, onde n torna-se um novo hiperpar\u00e2metro. Na pr\u00e1tica a implementa\u00e7\u00e3o acontece expandindo o espac\u00e7o vetorial para engolobar as features originais mais essas features elevadas a uma pot\u00eancia, ao se realizar uma regress\u00e3o linear nesse espa\u00e7o vetorial na pr\u00e1tica se realiza uma regress\u00e3o polinomial no espa\u00e7o original","7b1412c4":"O modelo com o melhor desempenho foi o com k=10.","94dcbb5c":"H\u00e1 dois sets o de treino e o de testes, no caso o set de treino \u00e9 a jun\u00e7\u00e3o das features de treino com a target que ser\u00e1 usada no treino. Por n\u00e3o conter a target o test de testes foi chamado de Xtest.","ad8cda63":"As suposi\u00e7\u00f5es sobre o regressor mostraram-se corretas, o desempenho do regressor KNN foi de fato muito superior ao dos regressores lineares, isso demonstra como foi antevisto que os regressores lineares n\u00e3o eram os mais apropriados para esse dataset.","479b9f40":"Aqui fez-se o teste com uma leve varia\u00e7\u00e3o do regressor, neste caso ao estimar o valor, n\u00e3o \u00e9 tomada a m\u00e9dia simples e sim a m\u00e9dia ponderada pela dist\u00e2ncia, esta diferen\u00e7a esta em conson\u00e2ncia com o que foi observado anteriormente na an\u00e1lise explorat\u00f3ria dos dados. Como esperado o desempenho foi levemente superior ao do regressor KNN padr\u00e3o.","eda6d6f4":"# 5 Aplicando Modelo ao Set de Testes","096c06d4":"Foi criado novas features que s\u00e3o raz\u00f5es entre uma feature e o numero de casas no local, \u00e9 esperado que elas contenham rela\u00e7\u00f5es de correla\u00e7\u00e3o mais fortes com o valor mediano do imovel do que a feature original sozinha","531180c3":"## 4.3 Regress\u00f5es N\u00e3o Lineares\n\nTodas as regress\u00f5es anteriormente testadas foram variantes do mesmo modelo, a regress\u00e3o linear, por\u00e9m o desempenho delas n\u00e3o foi satisfat\u00f3rio, logo faz sentido supor que os melhores modelos para esse conjunto de dados n\u00e3o se encontram no espectro de regress\u00f5es lineares, mas sim em modelos que n\u00e3o tentam explicar os dados atrav\u00e9s de hiperplanos. Alguns desses modelos ser\u00e3o testados e seus desempenhos comparados com os obtidos anteriormente pelos modelos lineares.\n\n### 4.3.1 Regress\u00e3o KNN\n\nUm dos modelos interessantes para se testar \u00e9 a regress\u00e3o KNN, este algoritmo, como o nome indica, \u00e9 semelhante ao classificador KNN, por\u00e9m para regress\u00e3o. Basicamente o algoritmo busca as K observa\u00e7\u00f5es mais pr\u00f3ximas e estima que a amostra a ser classificada ter\u00e1 o valor desejado igual a m\u00e9dia dos valores das K amostras mais pr\u00f3ximas. Pela explora\u00e7\u00e3o dos dados percebeu-se im\u00f3veis tendem a ter pre\u00e7os semelhantes a outros im\u00f3veis pr\u00f3ximos, portanto faz sentido supor que o desempenho do KNN Regressor tem o potencial de ser substancialmente superior ao dos regressores lineares.","d9194c62":"A feature dist\u00e2ncia a grande centro, \u00e9 a m\u00ednima dist\u00e2ncia daquela amostra a Los Angeles, San Diego ou San Francisco. Com ela \u00e9 esperado que se possa conseguir uma boa correla\u00e7\u00e3o com a target.","185e1bea":"## 1.1 Importando Pacotes","26d3fb2c":"Como se pode ver pelo n\u00e3o h\u00e1 nenhum dado faltante nos sets, portanto nenhum procedimento a mais ser\u00e1 necess\u00e1rio.","21bfd8ce":"H\u00e1 algumas correla\u00e7\u00f5es de destaque entre as vari\u00e1veis, por\u00e9m entre elas e o target nem tanta, isso indica que um modelo de regress\u00e3o linear simples n\u00e3o deve ter um desempenho muito bom.","f8fd39f8":"O modelo obteve melhor desempenho do que as regress\u00f5es lineares por\u00e9m ele n\u00e3o superou a regress\u00e3o KNN.","d8945b78":"# 4 Implementa\u00e7\u00e3o e Compara\u00e7\u00e3o de Modelos","9790e6f4":"## 2.4 Conclus\u00f5es sobre a An\u00e1lise Explorat\u00f3ria\n\nPela an\u00e1lise explorat\u00f3ria \u00e9 poss\u00edvel perceber que o set conta com alguns problemas que podem ser dif\u00edceis de algoritmos de regress\u00e3o linear superar. Primeiramente o set conta com algumas features cujas s\u00e3o distribui\u00e7\u00f5es muito assim\u00e9tricas. Em segundo lugar as vari\u00e1veis n\u00e3o possuem correla\u00e7\u00f5es lineares altas com a target, isso \u00e9 um forte indicativo de que regress\u00e3o linear n\u00e3o obter\u00e1 um bom desempenho. Em \u00faltimo lugar, uma das correla\u00e7\u00f5es claras \u00e9 das vari\u00e1veis geogr\u00e1fica, mais especificamente, a proximidade de im\u00f3veis mostrou-se um fator importante ao se tratar de pre\u00e7os de im\u00f3veis, por\u00e9m mais uma vez essa rela\u00e7\u00e3o \u00e9 longe de ser linear, \u00e9 uma rela\u00e7\u00e3o cuja geometria \u00e9 complexa e de dif\u00edcil captura por modelos simples como a regress\u00e3o linear.","15f73391":"# ****Tarefa 2 - Regress\u00e3o com Dataset California Housing****\n\n### ****Autor: Vitor Canineo Komar  PMR3208-2020-161****\n\n\n# 1 Introdu\u00e7\u00e3o","b7f057f2":"Como pode-se obervar a otimiza\u00e7\u00e3o de hiperpar\u00e2metros melhorou um pouco o desempenho, como era esperado.","a881f983":"Ao realizar a divis\u00e3o foi criado valores nan no set de testes, eles ser\u00e3o imputados com zero.","16647a4a":"H\u00e1 in\u00fameros algoritmos de regress\u00e3o, por\u00e9m para come\u00e7ar e se ter uma ideia do que se esperar \u00e9 recomend\u00e1vel come\u00e7ar com o modelo mais simples: regress\u00e3o linear. Ela servir\u00e1 como um bom modelo de base de compara\u00e7\u00e3o. Ap\u00f3s isso ser\u00e1 testado algumas variantes da regres\u00e3o linear, nelas s\u00e3o inclusos termos de regulariza\u00e7\u00e3o para o c\u00e1lculo dos melhores par\u00e2metros, esses termos tem a fun\u00e7\u00e3o de reduzir overfitting. Al\u00e9m disso ser\u00e3o testados dois outros modelos de regress\u00e3o, regress\u00e3o KNN e regress\u00e3o polinomial.","fa520228":"# 3 Prepara\u00e7\u00e3o de dados","c6b931da":"## 3.2 Assimetria\n\nComo foi visto na an\u00e1lise explorat\u00f3ria h\u00e1 muita assimetria presente nos dados, para resolver isso features com assimetria superior a 1 ser\u00e3o escaladas numa escala logaritimica.","0ef550ad":"### 4.2.3 Elastic Net","21e17944":"Pela distribui\u00e7\u00f5es pode-se perceber que as features no geral tem distribui\u00e7\u00f5es assimetricas, al\u00e9m diso em algumas delas v\u00ea-se na ultima barra um pico \u00e9 poss\u00edvel especular que nos dados reais os valores eram mais altos, por\u00e9m foram limitados por um valor escolhido. A quest\u00e3o da assimetria deve ser explorada mais a fundo para se tomar uma decis\u00e3o quanto ao que deve ser feito.","bf6dc19c":"### 3.3.4 Analisando as Novas Features","2de52df5":"Inesperadamente o desempenho do modelo Elastic Net foi consideravelmente inferior ao das outras regress\u00f5es testadas at\u00e9 agora.","52484759":"Realizou-se a transforma\u00e7\u00e3o para escala logar\u00edtmica e percebeu-se que houve uma grande redu\u00e7\u00e3o na assimetria (ela at\u00e9 se inverteu, a feature agora \u00e9 mais assim\u00e9trica para a esquerda do que para a direita). Deve-se procurar mais features assim\u00e9tricas e realizar a mesma opera\u00e7\u00e3o.","a32be2dc":"O m\u00e9todo skew() retorno uma medida da assimetria dos dados, quanto maior mais assim\u00e9trico no caso, a assimetria pode levar a perda de acur\u00e1cia de modelos. H\u00e1 algumas alternativas para se diminuir a assimetria, uma delas \u00e9 utilizar uma escala logaritmica na feature assim\u00e9trica.","d8356307":"### 3.3.3 Dist\u00e2ncia a Grande Centro","33a8c73c":"## 2.3 Visualiza\u00e7\u00e3o Geogr\u00e1fica","171bf098":"O dataframe Xtrain \u00e9 formado pelo set de treino sem a target. A separa\u00e7\u00e3o da target e features ser\u00e1 \u00fatil para a implementa\u00e7\u00e3o de modelos.","17eee43a":"Por se tratarem de dados que cont\u00e9m informa\u00e7\u00f5es geogr\u00e1ficas, visualiza\u00e7\u00f5es com mapas podem ajudar a entender melhor o conjunto de dados. No caso foi escolhido a popula\u00e7\u00e3o e o pre\u00e7o da casa como boas feautures para se plotar em um mapa.","c0ee73e4":"## 3.4 Normalizando Dados\n\nPara que uma das features n\u00e3o tenha um peso muito maior do que outras \u00e9 preciso que as features estejam na mesma escala, neste caso ser\u00e1 utilizado o m\u00e9todo de normaliza\u00e7\u00e3o, i.e., subtrai-se a m\u00e9dia das features e ap\u00f3s isso divide-se pelo desvio padr\u00e3o.","d956a655":"O modelo Elastic Net \u00e9 uma mistura de Lasso e Ridge, ele possui ambos os fatores de regulariza\u00e7\u00e3o e a raz\u00e3o entre eles \u00e9 dada por um novo hiperpar\u00e2mtro. No caso da biblioteca scikit-learn esse hiperpar\u00e2metro \u00e9 chamado de l1_ratio, i.e., a raz\u00f5e de L1 em rela\u00e7\u00e3o a L2, ou seja, quando l1_ratio = 1, temos um modelo equivalente ao Lasso.","3ee1b585":"## 4.2 Outras Regress\u00f5es Lineares\n\nAl\u00e9m do modelo b\u00e1sico de regress\u00e3o linear ser\u00e3o testadas alguns outros algoritmos, s\u00e3o eles LassoRegression, RidgeRegression e ElasticNet. Dentra essas aquela julgada ser a com melhor desempenho ser\u00e1 escolhida como modelo utilizado para fornecer a classifica\u00e7\u00e3o do set de test.","d551c6c8":"Como de costume em an\u00e1lise de dados ser\u00e3o utlizados os pacotes numpy, pandas e matplotlib. Al\u00e9m disso o pacote seaborn ser\u00e1 utilzado para melhorar a qualidade das visualiza\u00e7\u00f5es e tamb\u00e9m o pacote Basemap que ser\u00e1 utlizado para contruir visualiza\u00e7\u00f5es de dados geogr\u00e1ficos.","492a66b8":"## 4.4 Otimizando o Modelo\n\nFoi conclu\u00eddo que o melhor modelo \u00e9 a regress\u00e3o KNN com a estimativa feita ponderando-se as dist\u00e2ncias, por\u00e9m ainda h\u00e1 espa\u00e7o para melhora. Ser\u00e1 realizada uma busca pelos melhores hiperpar\u00e2metros, com os hiperpar\u00e2metros otimizados espera-se obter uma pequena melhora de desempenho.","6c47528e":"Analogamente ao modelo Lasso, o modelo Ridge n\u00e3o obteve um desempenho superior a regress\u00e3o simples.","347aaf52":"O modelo Lasso \u00e9 um variante da regress\u00e3o linear tradiocional, no caso adiciona-se um fator de regulariza\u00e7\u00e3o de grau 1 ao modelo.","d0cd711d":"# 2 An\u00e1lise Explorat\u00f3ria\n\nUma das fases mais importantes de qualquer an\u00e1lise de dados \u00e9 a an\u00e1lise explorat\u00f3ria, nela \u00e9 poss\u00edvel atrav\u00e9s de visualiza\u00e7\u00f5es e\/ou tabelas obter informa\u00e7\u00f5es sobre os dados que podem ser muito \u00fateis na esscolha de algoritmos que deve ser usados.\n\n## 2.1 Distribui\u00e7\u00e3o dos dados","db7bfdc1":"### 4.2.2 Ridge"}}