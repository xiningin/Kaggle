{"cell_type":{"55c04e67":"code","e3116ad4":"code","34c5f6db":"code","ac3007f1":"code","3267cf2c":"code","f4826a5b":"code","0b2e5125":"code","91e45bed":"code","2d6e4c50":"code","12886444":"code","3c72d8ec":"code","511b1225":"code","fec01944":"code","2ca1f11f":"code","aa14ec37":"code","08d3f84f":"code","8da7e91b":"code","531c9bc6":"code","558cafba":"code","19497b53":"code","3ae27c8f":"code","542e658e":"code","60b154e6":"markdown","63941fff":"markdown","908016c4":"markdown","6444e585":"markdown","907b1fb5":"markdown","5863b3aa":"markdown"},"source":{"55c04e67":"from datetime import datetime\n\nprint(\"last update: {}\".format(datetime.now())) ","e3116ad4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","34c5f6db":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split,  GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier,  ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import VotingClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score, roc_auc_score\nfrom sklearn.svm import SVC \nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMModel,LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn import metrics\nimport numpy as np\nnp.random.seed(0)\n\n","ac3007f1":"# Read the data\nX_original = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('\/kaggle\/input\/learn-together\/test.csv', index_col='Id')\nX = X_original.copy()","3267cf2c":"X.head(1)","f4826a5b":"X_test_full.head(1)","0b2e5125":"Experiments = {\"Algo\":[\"RandomForestClassifier\", \"XGBClassifier\", \"MLPClassifier\",  \"ExtraTreesClassifier\", 'LGBMClassifier'],\n              \"object\": [lambda: RandomForestClassifier(n_estimators = 1000, max_features = 'sqrt'),\n                        lambda: XGBClassifier(learning_rate =0.05, n_estimators=1000, n_jobs=-1),\n                        lambda: MLPClassifier(solver='adam', activation='relu', max_iter=3000, hidden_layer_sizes=(100,100), \n                                              random_state=1),\n                        lambda: ExtraTreesClassifier(n_estimators = 1000),\n                        lambda: LGBMClassifier(learning_rate =0.05, n_estimators=1000)],\n               \"F1_score\": [],\n               \"prediction\": [[] for _ in range(5)]}\n","91e45bed":"def reverse_onehot(df, subset, reverse_name):\n    df_new = pd.DataFrame()\n    df_new = df.drop(subset, axis = 1)\n    temp = df[subset]\n    df_new[reverse_name] = temp.idxmax(axis = 1).astype(str)\n    df_new[reverse_name] = df_new[reverse_name].apply(lambda x: int(str(x)[9:]))\n    return df_new  \n\nsubset = ['Soil_Type'+ str(i) for i in range(1,41)]\nDs = reverse_onehot(X, subset, 'Soil_Type')\nDs.tail()","2d6e4c50":"# scale before mlp\nX_train, X_valid, y_train, y_valid = train_test_split(X.drop('Cover_Type', axis = 1), X['Cover_Type'], test_size = 0.2)\nscale = StandardScaler()\npreprocessor = ColumnTransformer(transformers = [('scaling numeric', scale, list(X.drop('Cover_Type', axis = 1).columns[0:10]))])\n","12886444":"# Get F1_scores of differents models\nfor i, obj in enumerate(Experiments[\"object\"]):\n    if i == 2:\n        model = obj()\n        my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', model)])\n        my_pipeline.fit(X_train, y_train)\n        y_val_pred = my_pipeline.predict(X_valid)\n        Experiments['prediction'].append(y_val_pred)\n        Experiments['F1_score'].append(f1_score(y_valid, y_val_pred, average='weighted')) \n    else:\n        model = obj()\n        model.fit(X_train, y_train)\n        y_val_pred = model.predict(X_valid)\n        Experiments['prediction'].append(y_val_pred)\n        Experiments['F1_score'].append(f1_score(y_valid, y_val_pred, average='weighted'))","3c72d8ec":"# Print the F1_scores of the five selected mdels\nExperiments['F1_score']","511b1225":"mlp =  MLPClassifier(solver='adam', activation='relu', max_iter=3000, hidden_layer_sizes=(50,50), random_state=1)\nMLP_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', mlp)])","fec01944":"list_estimators = [RandomForestClassifier(n_estimators = 1000, max_features = 'sqrt'), \n                   XGBClassifier(learning_rate =0.05, n_estimators=1000),\n                   MLP_pipeline,\n                   ExtraTreesClassifier(n_estimators = 1000, max_features = 'sqrt'), \n                   LGBMClassifier(learning_rate =0.05, n_estimators=1000)]\nbase_methods = list(zip(Experiments[\"Algo\"], list_estimators))\nbase_methods ","2ca1f11f":"# Hard voting decides according to vote number which is the majority wins\nVoting_model_hard = VotingClassifier(estimators= base_methods, voting='hard')\nVoting_model_hard.fit(X_train, y_train)\ny_vp_val = Voting_model_hard.predict(X_valid)\ny_vp_train = Voting_model_hard.predict(X_train)\nprint('f1_score', f1_score(y_valid, y_vp_val, average='weighted'))\nprint('acc_score_train', accuracy_score(y_train, y_vp_train))\nprint('acc_score_valid', accuracy_score(y_valid, y_vp_val))","aa14ec37":"# save the model to file and load it to make predictions on the unseen test set\nimport pickle\nfilename = 'Voting_model_hard.sav'\npickle.dump(Voting_model_hard, open(filename, 'wb'))","08d3f84f":"loaded_model_hard = pickle.load(open(filename, 'rb'))\nresult_hard = loaded_model_hard.score(X_valid, y_valid)\nprint(result_hard)","8da7e91b":"# In soft voting, we can set weight value to give more priorities to certain classifiers according to their performance\nVoting_model_soft = VotingClassifier(estimators= base_methods, voting='soft', weights=[15, 9, 8, 14, 15])\nVoting_model_soft.fit(X_train, y_train)\ny_vp_val = Voting_model_soft.predict(X_valid)\ny_vp_train = Voting_model_soft.predict(X_train)\nprint('f1_score', f1_score(y_valid, y_vp_val, average='weighted'))\nprint('acc_score_train', accuracy_score(y_train, y_vp_train))\nprint('acc_score_valid', accuracy_score(y_valid, y_vp_val))","531c9bc6":"#save the model to file and load it to make predictions on the unseen test set\nfilename_soft = 'Voting_model_soft.sav'\npickle.dump(Voting_model_soft, open(filename_soft, 'wb'))","558cafba":"loaded_model_soft = pickle.load(open(filename_soft, 'rb'))\nresult_soft = loaded_model_soft.score(X_valid, y_valid)\nprint(result_soft)","19497b53":"Ds_test = reverse_onehot(X_test_full, subset, 'Soil_Type')","3ae27c8f":"Ds_test.head()","542e658e":"preds_test = Voting_model_soft.predict(X_test_full)\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test_full.index,\n                       'Cover_Type': preds_test})\noutput.to_csv('submission.csv', index=False)","60b154e6":"# 1. Introduction And Motivation\n\n*Differents algoritm are better in differents situations, they make kind of differents mistakes and overfit in differents ways. Combine algorithm lets us leverage all of these*.\n\n\nConsidering this statment, It's time to perform machine learning to our classification task \n\nWe are going to present here four combining techinques **(Voting, Bagging, Boosting, Stacking)** with differents models associated with their best parameters and their F1 score.\n\nWe are 5 members in our team:\n\nSuppose that the member 1 **achieve Random Forest** with **best parameter {'n_estimators': 100, 'max_features': 'sqrt'}**\n\nThe member two achieve **xgboost Classifier** with best **parameters {'n_estimators': 1000, 'learning_rate': 0.05}**\n\nThe third member achieve **CatBoostClassifier**\n\nThe fourth achieve **MLP classifier with  parameters {'hidden_layer_sizes':  (50,50)}**\n\nAnd me with an intuition model\n\n","63941fff":"# I. FIRST PART ENSEMBLE VOTING","908016c4":"# 2. Hard and Soft Voting","6444e585":"# 3. First submit using Voting_model_soft ","907b1fb5":"# II- Second Technique Bootstrap Aggregating","5863b3aa":"We will use the five above differents models to put into our Voting Classifier. Hard voting decides according to vote number which is the majority wins.  In soft voting, we can set weight value to give more priorities to certain classifiers according to their performance. The weights we will use are the F1_scores of differents models."}}