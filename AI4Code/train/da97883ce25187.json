{"cell_type":{"0b268c6a":"code","29d58194":"code","9a756d12":"code","546077e9":"code","a718ac53":"code","6cce9db3":"code","eb7075ab":"code","3ef84a56":"code","a40388d3":"code","15a97638":"code","7359c3c3":"code","58ab321e":"code","2f171695":"code","b373e78c":"markdown","55f9969d":"markdown","e27b1461":"markdown","cf6a4ace":"markdown","89949aca":"markdown","ae6a1c4a":"markdown","09bf6db1":"markdown","3e549d82":"markdown","d05d8702":"markdown"},"source":{"0b268c6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29d58194":"#Reading the Datasets\nsales_train = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nitems = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nshops= pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\nitem_categories = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nprint(sales_train.shape)\nsales_train.head()\n","9a756d12":"sales_train.drop(sales_train[(sales_train['item_cnt_day']<=0)|(sales_train['item_price']<=0)].index ,axis=0,inplace=True)","546077e9":"# sales_train['date']=pd.to_datetime(sales_train['date'],dayfirst=True)\n","a718ac53":"data=sales_train.groupby([\"date\",\"date_block_num\",\"shop_id\",\"item_id\"])[\"item_cnt_day\"].sum().reset_index()\nts=data.groupby(['date'])['item_cnt_day'].sum()\nts.astype('float')\nplt.figure(figsize=(12,10))\nplt.title('Total Sales of the item')\nplt.xlabel('Month-Year')\nplt.ylabel('Quantity of Sales')\nplt.plot(ts)","6cce9db3":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries):\n    \n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window=12,center=False).mean()\n    rolstd = timeseries.rolling(window=12,center=False).std()#window=12, because of yearly trend for both mean and variance\n#Plot rolling statistics:\n    plt.figure(figsize=(15,10))\n    plt.plot(timeseries, color='blue',label='Original')\n    plt.plot(rolmean, color='red', label='Rolling Mean')\n    plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show()\n    #Perform Dickey-Fuller test:\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)","eb7075ab":"test_stationarity(ts)","3ef84a56":"# from sklearn.preprocessing import LabelEncoder\n# le = LabelEncoder()\n# def encoding_categorical(dataset):\n#     categorical_columns=['shop_id','item_id']\n#     for column in categorical_columns:\n#         dataset[str(column)]=le.fit_transform(dataset[str(column)])\n#     return dataset\n# complete_data=encoding_categorical()\n\n","a40388d3":"ts_data = pd.DataFrame(ts)\n","15a97638":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(ts_data,period=100)\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\nplt.figure(figsize=(15,8))\nplt.subplot(411)\nplt.plot(ts,label='Orginial')\nplt.subplot(412)\nplt.plot(trend,label='Trend')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonal')\nplt.subplot(414)\nplt.plot(residual,label='Residual')","7359c3c3":"from statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.stattools import acf,pacf\n\nlag_acf=acf(ts,fft=False)\nlag_pacf=pacf(ts,method='ols')\nplt.figure(figsize=(11,8))\nplt.subplot(121)\nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(ts)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(ts)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\n\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(ts)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(ts)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","58ab321e":"model=ARIMA(ts,order=(2,0,2))\nresult = model.fit(disp=-1)\nplt.plot(ts,label=\"Original\")\nplt.plot(result.fittedvalues,color='red',label=\"Predicted\")","2f171695":"forecast_errors = [ts[i]-result.fittedvalues[i] for i in range(len(ts))]\nbias = sum(forecast_errors) * 1.0\/len(ts)\nprint('Bias: %f' % bias)\ntest=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\npredictions = pd.DataFrame(result.fittedvalues).reset_index()\npredictions.columns=[\"date\",\"predictions\"]\npredictions.head()# Monthly sales forecasting","b373e78c":"**Decomposing:**\n***we model both the trend and the seasonality, then the remaining part of the time series is returned.***","55f9969d":"***Since our two columns Item_price and Item_count have negative values i.e. probably the items have been returned back to the shop, the shop is not making any sales on these orders. hence, I will drop the rows that have negative value for item_price and item_count.***","e27b1461":"**I will not focussing on the exploratory data analysis for this notebook and would dive into time-series modelling and explaining the concepts better.**\n\n*In practice we can assume the series to be stationary if it has constant statistical properties over time and these properties can be:*\n\n*\u2022 constant mean*\n\n*\u2022 constant variance*\n\n*\u2022 an auto co-variance that does not depend on time.*\n\n*These details can be easily retrieved using stat commands in python.*\n\n*The best way to understand you stationarity in a Time Series is by eye-balling the plot:*","cf6a4ace":"***convert the date to datetime object.***","89949aca":"**Forecasting a Time Series**:\n\nNow that we have made the Time series stationary, let\u2019s make models on the time series using differencing because it is easy to add the error , trend and seasonality back into predicted values .\nWe will use statistical modelling method called ARIMA to forecast the data where there are dependencies in the values.\nAuto Regressive Integrated Moving Average(ARIMA) \u2014 It is like a liner regression equation where the predictors depend on parameters (p,d,q) of the ARIMA model .\n\n\n\u2022 p : This is the number of AR (Auto-Regressive) terms . Example \u2014 if p is 3 the predictor for y(t) will be y(t-1),y(t-2),y(t-3).\n\n\n\u2022 q : This is the number of MA (Moving-Average) terms . Example \u2014 if p is 3 the predictor for y(t) will be y(t-1),y(t-2),y(t-3).\n\n\n\u2022 d :This is the number of differences or the number of non-seasonal differences .\n\n\nNow let\u2019s check out on how we can figure out what value of p and q to use. We use two popular plotting techniques; they are:\n\n\n\u2022 Autocorrelation Function (ACF): It just measures the correlation between two consecutive (lagged version). example at lag 4, ACF will compare series at time instance t1\u2026t2 with series at instance t1\u20134\u2026t2\u20134\n\n\n\u2022 Partial Autocorrelation Function (PACF): is used to measure the degree of association between y(t) and y(t-p).\n","ae6a1c4a":"Performing Dicker Fuller Test","09bf6db1":"**We can see a decreasing trend from the above line plot.** \n ***In order to apply a time series model, it is important for the Time series to be stationary; in other words all its statistical properties (mean,variance) remain constant over time***","3e549d82":"*** if The data is not stationary because the test statistic is greater than the critical value.***\n***There are two major factors that make a time series non-stationary. They are:***\n\n***\u2022 Trend: non-constant mean***\n\n***\u2022 Seasonality: Variation at specific time-frames***\n***Trend:***\n***The first step is to reduce the trend using transformation, as we can see here that there is a strong positive trend. These transformation can be log, sq-rt, cube root etc . Basically it penalizes larger values more than the smaller. In this case we will use the logarithmic transformation.***\n\nWe will not be doing this as the test-statistic in our case is lesser than all the critical values.\n","d05d8702":"The two dotted lines on either sides of 0 are the confidence intervals. These can be used to determine the \u2018p\u2019 and \u2018q\u2019 values as:\n\n\n\u2022 p: The first time where the PACF crosses the upper confidence interval, here its close to 2. hence p = 2.\n\n\n\u2022 q: The first time where the ACF crosses the upper confidence interval, here its close to 2. hence q = 2."}}