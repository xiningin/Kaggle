{"cell_type":{"17bd0876":"code","79a40a64":"code","222329d5":"code","9e1f9bc0":"code","2799917e":"code","78fc9a67":"code","4db9b555":"code","9e3315aa":"code","59688956":"code","4b35a9fc":"code","45876e46":"code","5781c406":"code","1ebf7520":"code","de4d8a2f":"code","83b8c3df":"code","6c2e863e":"code","65e689b9":"code","6b904003":"code","954398db":"code","60ac2a2c":"code","b3c8a6d9":"code","9a8764d0":"code","628a4335":"code","9c9eaa7c":"code","1beaee2f":"code","34d6093a":"code","0128de55":"code","aa61a9d1":"code","bccf579a":"code","c21b9e0f":"code","d3a4f54d":"code","0f8d2e90":"code","e584f050":"code","340e2a89":"code","6ba1ea44":"code","134e1006":"code","c788e018":"code","72606f1b":"code","a2314fad":"code","93be33e9":"code","d544e34d":"code","8899d3c3":"code","04feb618":"code","01659cc9":"code","140cc2b3":"code","0626720f":"code","f94adf74":"code","77af350d":"code","c29ec6c7":"markdown","13cfdc67":"markdown"},"source":{"17bd0876":"import numpy as np \nimport pandas as pd","79a40a64":"# parse the train and test file\n# \ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","222329d5":"train_df.shape\n# \u663e\u793a\u8bad\u7ec3\u96c6\u7684shape","9e1f9bc0":"test_df.shape\n# \u663e\u793a\u6d4b\u8bd5\u96c6\u7684shape","2799917e":"train_df.columns\n# \u663e\u793aheader","78fc9a67":"test_df.columns","4db9b555":"train_df.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]\n# \u663e\u793a\u90e8\u5206\u6570\u636e","9e3315aa":"# \u5c06\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u7279\u5f8179\u4e2a\u7279\u5f81\u8fde\u63a5\u8d77\u6765\n# \u53bb\u9664id\u7684\u5217\uff0c\u4ee5\u53ca\u8bad\u7ec3\u96c6\u7684saleprice\u5217\nall_feature = pd.concat([train_df.iloc[:, 1:-1], test_df.iloc[:, 1:]], keys=[\"train\", \"test\"])","59688956":"# \u9884\u5904\u7406\u6570\u636e\nnumeric_features = all_feature.dtypes[all_feature.dtypes != 'object'].index\nnumeric_features, all_feature.dtypes\n","4b35a9fc":"all_feature[numeric_features] = all_feature[numeric_features].apply(lambda x: (x - x.mean()) \/ (x.std()))","45876e46":"all_feature[numeric_features] = all_feature[numeric_features].fillna(0)","5781c406":"all_feature.shape","1ebf7520":"all_feature = pd.get_dummies(all_feature, dummy_na=True)","de4d8a2f":"all_feature.shape","83b8c3df":"all_feature.columns","6c2e863e":"# \u7279\u5f81\u7684index\u662fmultiIndex\u6240\u4ee5\u9700\u8981\nall_feature.index.levels[0], all_feature.index","65e689b9":"all_feature.index.levels[1]","6b904003":"all_feature.index.labels[0]","954398db":"all_feature.index.labels[1],all_feature.index.names","60ac2a2c":"all_feature.loc['train', slice(None), :].head()","b3c8a6d9":"train_feature = np.array(all_feature.loc['train',].values)","9a8764d0":"train_feature.size, train_feature.shape","628a4335":"\ntest_feature = np.array(all_feature.loc['test',].values)","9c9eaa7c":"# \u5c06label\u8f6c\u6362\u6210\u6570\u636e\u8f93\u5165\u683c\u5f0f[shape * 1]\ntrain_labels = np.array(train_df.SalePrice.values).reshape((-1,1))","1beaee2f":"import torch","34d6093a":"# \u56fa\u5b9a\u79cd\u5b50, \u4ee5\u4fbf\u91cd\u73b0\u7ed3\u679c\nseed = 2018\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","0128de55":"torch.cuda.is_available()\n# pytorch \u6765\u8bad\u7ec3\u4e00\u4e2a","aa61a9d1":"torch.cuda.get_device_name()","bccf579a":"torch.cuda.device_count()","c21b9e0f":"torch.cuda.get_device_name(0)","d3a4f54d":"torch.cuda.current_device()\n# \u8fd4\u4f1a\u5f53\u524d\u7684\u8bbe\u5907\u7684\u7d22\u5f15","0f8d2e90":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","e584f050":"# \u5bfc\u5165nn module\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.utils.data as data\nimport torch.nn.functional as F","340e2a89":"# \u5b9a\u4e49\u4e00\u4e2a\u7ebf\u6027\u7ebf\u6027\u56de\u5f52\u7684\u7f51\u7edc\n\nclass LinearRegression(nn.Module):\n    def __init__(self):\n        super(LinearRegression, self).__init__()\n        self.fc1 = nn.Linear(331, 496)\n        self.fc2 = nn.Linear(496, 248)\n        self.fc3 = nn.Linear(248, 124)\n        self.fc4 = nn.Linear(124, 32)\n        self.fc5 = nn.Linear(32, 1)\n    def forward(self, x):\n        x = F.relu(self.fc1(x)).clamp(min=0)\n        x = F.dropout(x, p=0.1)\n        x = F.relu(self.fc2(x)).clamp(min=0)\n        x = F.dropout(x, p=0.1)\n        x = F.relu(self.fc3(x)).clamp(min=0)\n#         x = F.dropout(x, p=0.1)\n        x = F.relu(self.fc4(x)).clamp(min=0)\n#         x = F.dropout(x, p=0.1)\n        x = F.relu(self.fc5(x)).clamp(min=0)\n        return x","6ba1ea44":"# \u5b9e\u4f8b\u5316\u540e\u4f7f\u7528.to\u65b9\u6cd5\u5c06\u7f51\u7edc\u79fb\u52a8\u5230GPU\nmodel = LinearRegression().to(DEVICE)","134e1006":"# \u67e5\u770bmodel\nmodel","c788e018":"import math \n# \u5b9a\u4e49\u6570\u636e\u96c6\u7684\u8bfb\u53d6\nclass MyData(data.Dataset):\n    def __init__(self, feature, label):\n        self.feature = feature\n        self.label = label\n\n    def __len__(self):\n        return len(self.feature)\n\n    def __getitem__(self, idx):\n        return self.feature[idx], self.label[idx]\n\n# data_loader = DataLoader(MyData(train_feature, label), batch_size=45, shuffle=True)\n# K\u6298\u4ea4\u53c9\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u83b7\u5f97\ndef get_k_fold_data(k, i, X, y):\n    assert k > 1\n    fold_size = X.shape[0] \/\/ k\n    X_train, y_train = None, None\n    for j in range(k):\n        idx = slice(j * fold_size, (j + 1) * fold_size)\n        X_part, y_part = X[idx, :], y[idx]\n        if j == i:\n            X_valid, y_valid = X_part, y_part\n        elif X_train is None:\n            X_train, y_train = X_part, y_part\n        else:\n            X_train = np.concatenate((X_train, X_part), axis=0)\n            y_train = np.concatenate((y_train, y_part), axis=0)\n    return X_train, y_train, X_valid, y_valid\n\nclass RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        \n    def forward(self,out,label):\n        return torch.sqrt(self.mse(out.float().log(), label.float().log()))\n\ndef train(net, train_features, train_labels, test_features, test_labels, \n          num_epochs, learning_rate, weight_decay, batch_size):\n    train_ls, test_ls = [], []\n    train_dataset = MyData(torch.from_numpy(train_features), torch.from_numpy(train_labels))\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    for epoch in range(num_epochs):\n        loss_lst = []\n        model.train() # set up the train model\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(DEVICE), target.to(DEVICE)\n            output = net(data.float())\n            # loss = torch.sqrt(F.mse_loss(output.float().log(), target.float().log()))\n            criterion  = RMSELoss()\n            loss = criterion(output, target)\n            loss_lst.append(loss.item())\n            optimizer.zero_grad() # optimizer 0\n            loss.backward() # back propragation\n            optimizer.step() # update the paramters \n        print('Train Epoch: {} \\tLoss: {}'.format(epoch, sum(loss_lst)\/batch_idx))\n        train_ls.append(sum(loss_lst)\/batch_idx)\n        \n        if test_labels is not None:\n            # val\n            val_dataset = MyData(torch.from_numpy(test_features), torch.from_numpy(test_labels))\n            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n            model.eval()\n            val_loss = 0\n            with torch.no_grad():\n                for data, target in val_loader:\n                    data, target = data.to(DEVICE), target.to(DEVICE)\n                    output = net(data.float())\n                    val_loss += F.mse_loss(output.float().log(), target.float().log(),reduction='sum').item() # \u5c06\u4e00\u6279\u7684\u635f\u5931\u76f8\u52a0\n            val_loss = math.sqrt(val_loss\/len(val_loader.dataset))      \n            print('\\nTest set: Average loss: {:.4f}\\n'.format(val_loss))\n            test_ls.append(val_loss)\n    return train_ls, test_ls\n    \n    ","72606f1b":"import matplotlib.pyplot as plt","a2314fad":"# \u7ed8\u56fe\n    ","93be33e9":"\n\n# \u6743\u91cd\u548cbias\u7684\u521d\u59cb\u5316\nfor m in model.modules():\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)\n\ndef k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay, batch_size):\n    train_l_sum, valid_l_sum = 0, 0\n    plt.figure(figsize=(15,8))\n    for i in range(k):\n#         print('fold {}'.format(str(i)))\n        model = LinearRegression().to(DEVICE)\n        data = get_k_fold_data(k, i, X_train, y_train)\n        trainloss, valloss = train(model, *data, num_epochs, learning_rate, weight_decay, batch_size)\n        plt.subplot(5,1,i+1)\n        plt.plot(trainloss, 'g-')\n        plt.plot(valloss, 'r-')\n        train_l_sum += trainloss[-1]\n        valid_l_sum += valloss[-1]\n        print('fold %d, train rmse %f, valid rmse %f'% (i, trainloss[-1], valloss[-1]))\n       \n    plt.show()\n#     return train_l_sum \/ k, valid_l_sum \/ k","d544e34d":"k, num_epochs, lr, weight_decay, batch_size =  5, 1000, 0.001, 0, 64 # hyper paramters\nk_fold(k, train_feature, train_labels, num_epochs, lr, weight_decay, batch_size)","8899d3c3":"# pip install nvidia-ml-py3","04feb618":"import pynvml\npynvml.nvmlInit()\n# \u8fd9\u91cc\u76840\u662fGPU id\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)\nmeminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\nprint(meminfo.used\/1024**3)","01659cc9":"print(meminfo.total\/1024**3)","140cc2b3":"type(meminfo.total)","0626720f":"def train_and_pred(train_features, test_features, train_labels, test_data, \n                   num_epochs, lr, weight_decay, batch_size):\n    net = LinearRegression().to(DEVICE)\n    # \u6743\u91cd\u548cbias\u7684\u521d\u59cb\u5316\n    for m in net.modules():\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            nn.init.xavier_uniform_(m.weight)\n            m.bias.data.fill_(0.01)\n    train(net, train_features, train_labels, None, None,num_epochs, lr, weight_decay, batch_size)\n    test_features = torch.from_numpy(test_features).to(DEVICE)\n    with torch.no_grad():\n        net.eval()\n        preds = net(test_features.float())\n    preds = preds.cpu().numpy()\n#     print(preds.shape)\n    test_data['SalePrice'] = pd.Series(preds.reshape(-1))\n    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n    submission.to_csv('submission.csv', index=False)","f94adf74":"torch.__version__\nk, num_epochs, lr, weight_decay, batch_size =  5, 500, 0.001, 0.0005, 64 # hyper paramters","77af350d":"train_and_pred(train_feature, test_feature, train_labels, test_df, num_epochs, lr, weight_decay, batch_size)","c29ec6c7":"### **\u8f6c\u6362\u6210numpy\u683c\u5f0f**","13cfdc67":"### **\u9884\u5904\u7406\u6570\u636e**\n\u6211\u4eec\u5bf9\u8fde\u7eed\u6570\u503c\u7684\u7279\u5f81\u505a\u6807\u51c6\u5316\uff08standardization\uff09\uff1a\u8bbe\u8be5\u7279\u5f81\u5728\u6574\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5747\u503c\u4e3a\u00b5\uff0c\u6807\u51c6\n\u5dee\u4e3a\u03c3\u3002\u90a3\u4e48\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u8be5\u7279\u5f81\u7684\u6bcf\u4e2a\u503c\u5148\u51cf\u53bb\u00b5\u518d\u9664\u4ee5\u03c3\u5f97\u5230\u6807\u51c6\u5316\u540e\u7684\u6bcf\u4e2a\u7279\u5f81\u503c\u3002\u5bf9\u4e8e\u7f3a\n\u5931\u7684\u7279\u5f81\u503c\uff0c\u6211\u4eec\u5c06\u5176\u66ff\u6362\u6210\u8be5\u7279\u5f81\u7684\u5747\u503c\u3002"}}