{"cell_type":{"db339fa2":"code","ec6be223":"code","9d7b249e":"code","5ed9f193":"code","06b2bff3":"code","80a33d78":"code","f88c540d":"code","672454d0":"code","c8c58e58":"code","e782fa17":"code","b90f98c2":"code","cb2e58f3":"code","21a84ebf":"code","98c2de2e":"code","4ef56e5d":"code","40ae2570":"code","35fac317":"code","ca79446a":"code","9af16dce":"code","cbc45754":"code","cbbf7f59":"code","15c18518":"code","900f5e97":"code","1f9a2724":"code","76ad9191":"code","35a0dc0c":"code","c426a713":"code","0c381c35":"code","922a0aca":"code","933e31fc":"code","e735a804":"code","8bf3e468":"code","df0a3961":"code","745e49b9":"code","f2015e84":"code","32c86e3a":"code","55925fa9":"code","eb64382e":"code","bfca4852":"code","e1066cce":"code","26a396bc":"code","2c594b04":"code","8043ad3b":"code","8d7a9fa5":"code","0d876412":"code","01bc36be":"code","35408214":"code","817eaccf":"code","da7f6fba":"code","e52b8332":"code","393a3420":"code","8833882b":"code","9db4c1a4":"code","db6d84f0":"code","a8c1b4c6":"code","7ba2c4fc":"code","060c9b4b":"code","d6c4f9dd":"code","ff073324":"code","230694f6":"code","324bef1d":"code","c7c5c66e":"code","cda891b2":"code","bfb45325":"code","df789e61":"code","29895cc3":"code","15e74037":"code","00bb5ac7":"code","7ef9c004":"code","d131c910":"code","519a697a":"code","087930be":"code","089ba9fd":"code","62511fc4":"code","7a240762":"code","645264a0":"code","bdef3975":"code","1c2b99d4":"code","becc6e09":"code","432a4dea":"code","936f81f1":"code","bcf753d4":"code","451bbb3a":"code","dd320b52":"code","b1360ff4":"code","6bdfa31e":"code","b4780baf":"code","c0777006":"code","def7fd56":"code","ce3a23cf":"code","3911a280":"code","dca59139":"code","f1fb9612":"code","8b22e3fa":"code","462b5deb":"code","097e2d26":"code","d1a27226":"code","042a3ebb":"code","64bf7c11":"code","448e6f1e":"code","71c90c86":"code","0f30e2bc":"code","662cf719":"code","41a5eaea":"code","e785067f":"code","718f7ca5":"code","9b536dfe":"code","be8d92d9":"code","192a224b":"code","8671338a":"code","a06a9739":"code","d3815419":"code","43a3216d":"code","f6d87338":"code","c2df61af":"code","a255476b":"code","2ea064fb":"code","e2243708":"code","195b9c87":"code","1eae4634":"code","645fd6de":"code","56eeaf6f":"code","fe408877":"code","dd1af5c5":"code","2cd837fe":"code","9122d392":"code","b8e73983":"code","0d2eac3e":"code","a46efb97":"code","a6b3529f":"code","1dca3a7d":"code","0733a6b9":"code","25729293":"code","9a59896e":"code","ca341595":"code","0b0cddeb":"code","d1d8d36c":"code","17dddde5":"code","7fef90ce":"code","f9d1a0a9":"code","7666bafe":"code","e702b9d4":"code","bb58686a":"code","b4420644":"code","093fbf2b":"code","a3f599c1":"code","870703d2":"markdown","356bf0e6":"markdown","50d94685":"markdown","ca8b17eb":"markdown","e2ad9e7a":"markdown","2075edda":"markdown","d95e4de0":"markdown","02f29f07":"markdown","f7aa11c8":"markdown","1e55cd8e":"markdown","fabf538e":"markdown","afebe133":"markdown","169f3b24":"markdown","6914fc6d":"markdown","b175f7c6":"markdown","3bef6af6":"markdown","dc1e205f":"markdown","4cffdd2a":"markdown","62adff6f":"markdown","f21e2367":"markdown","8bdb6b7e":"markdown","6d6d65f1":"markdown","0714ff4a":"markdown","7856a155":"markdown","bc7f6123":"markdown","7dae16e4":"markdown","a23c1572":"markdown","380b198e":"markdown","c4dc4c58":"markdown","054ccb27":"markdown","70aada18":"markdown","f8f041a7":"markdown","ede42004":"markdown","93bb5edf":"markdown","35b693ba":"markdown","6f323943":"markdown","934bc83a":"markdown","672dd786":"markdown","f80c595d":"markdown","bc857544":"markdown","11dec852":"markdown","2bde41ba":"markdown","8d1fa211":"markdown","49f1a2af":"markdown","e0ca21d0":"markdown","e3d18a13":"markdown","3b2dac9f":"markdown","2f3c29b0":"markdown","456831e1":"markdown","dc4b6a6a":"markdown","fdd360e4":"markdown","4d698153":"markdown","25b879e6":"markdown","94a48dd9":"markdown","8ca5b969":"markdown","825e3171":"markdown","57673005":"markdown","b27c6a70":"markdown","5fceba33":"markdown","5b099a69":"markdown","74e208dc":"markdown","5824b38c":"markdown","3432e9c9":"markdown","c3aabafe":"markdown","88c91b81":"markdown","81f0bb3f":"markdown","d01baa86":"markdown","7007c662":"markdown","9450892e":"markdown","afb2dc88":"markdown","eb922d96":"markdown","5411d09f":"markdown","db127e05":"markdown","0ca346be":"markdown","0d9b69af":"markdown","7347526c":"markdown","96aeeaf9":"markdown","1f4eb674":"markdown","9882b476":"markdown","0d64de4e":"markdown"},"source":{"db339fa2":"#important libraries\nimport numpy as np \nimport pandas as pd\n\n%matplotlib inline \nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n#import seaborn as sns\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","ec6be223":"# Importing provided dataset one for prediction and one for training and testing\ntrain_frame = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_frame = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","9d7b249e":"train_frame.head()","5ed9f193":"print(train_frame.shape)\nprint(test_frame.shape)","06b2bff3":"# to see what types of data each col contains\ntrain_frame.dtypes","80a33d78":"train_frame.describe()","f88c540d":"#include all provide an understanding of category in form of unique values in each col and freq of most common value\ntrain_frame.describe(include='all')","672454d0":"train_null = train_frame.isnull().sum().sort_values(ascending=False)\ntrain_null","c8c58e58":"null_test = test_frame.isnull().sum().sort_values(ascending=False)\nnull_test","e782fa17":"#performing EDA \ntrain_frame['Parch'].value_counts()","b90f98c2":"#initial visualization, droping missing rows to avoid errors in visuals\nmale = train_frame[train_frame['Sex']=='male']\nfemale = train_frame[train_frame['Sex']=='female']\nx = male[male['Survived']==1].Age.dropna()\nx1 = male[male['Survived']==0].Age.dropna()\ny = female[female['Survived']==1].Age.dropna()\ny1 = female[female['Survived']==0].Age.dropna()","cb2e58f3":"#lets have some visualization on sex and survial ratio\n\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nax = sns.distplot(x, bins=15, label = 'survived', ax = axes[0], kde = False, color = 'g')\nax = sns.distplot(x1, bins=30, label = 'not survived', ax = axes[0], kde = False, color = 'b')\nax.legend()\nax.set_title('Male')\nax = sns.distplot(y, bins=15, label = 'survived', ax = axes[1], kde = False, color = 'y')\nax = sns.distplot(y1, bins=30, label = 'not survived', ax = axes[1], kde = False, color = 'r')\nax.legend()\nax.set_title('Female')\nplt.show()","21a84ebf":"#survival ration w.r.t to class\nsns.barplot(x='Pclass', y='Survived', data=train_frame)","98c2de2e":"sns.barplot(x='Parch', y='Survived', data=train_frame)","4ef56e5d":"dft1 = train_frame.copy()\ncat_cols = ['Sex', 'Embarked', 'Survived']\n\ndft1[cat_cols]= dft1[cat_cols].astype('category')","40ae2570":"f,ax =plt.subplots(len(cat_cols),1,figsize=(5,10))\nfor idx,col in enumerate(cat_cols):\n    if col!='Survived':\n        sns.countplot(x=col,data=dft1[cat_cols],hue='Survived', ax=ax[idx])","35fac317":"#Advance Vis\na = sns.FacetGrid(train_frame, hue = 'Survived', aspect=4, palette=\"Set1\" )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , train_frame['Age'].max()))\na.add_legend()","ca79446a":"b = sns.FacetGrid(train_frame, row = 'Sex', col = 'Pclass', hue = 'Survived', palette=\"Set2\")\nb.map(plt.hist, 'Age', alpha = .75)\nb.add_legend()","9af16dce":"add_1 = sns.FacetGrid(train_frame, col = 'Embarked')\nadd_1.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\nadd_1.add_legend()","cbc45754":"sns.scatterplot(x=\"Age\", y=\"Fare\",\n                     hue=\"Survived\", palette=\"Set2\",\n                     sizes=(20, 200), hue_norm=(0, 7),\n                     legend=\"full\", data=train_frame)","cbbf7f59":"sns.scatterplot(x=\"Age\", y=\"Fare\", hue=\"Survived\", size=\"Sex\", palette=\"Set2\",\n                     sizes=(20, 200), hue_norm=(0, 7),\n                     legend=\"full\", data=train_frame)","15c18518":"sns.scatterplot(x=\"Pclass\", y=\"Parch\",\n                     hue=\"Survived\", palette=\"Set1\",\n                     sizes=(20, 200), hue_norm=(0, 7),\n                     legend=\"full\", data=train_frame)","900f5e97":"cols = ['Survived', 'Pclass', 'Sex', 'Parch', 'Fare', 'Embarked']\nsns.pairplot(train_frame[cols], diag_kind=\"hist\", hue = 'Survived', palette=\"Set2\")","1f9a2724":"sns.jointplot(train_frame['Age'],train_frame['Survived'], kind=\"hex\")\nplt.title('Age Vs Survived')\nplt.tight_layout()\nplt.show()","76ad9191":"sns.jointplot(train_frame['Age'],train_frame['Fare'], kind=\"reg\")\nplt.title('Age Vs fare')\nplt.tight_layout()\nplt.show()","35a0dc0c":"sns.jointplot(data=train_frame, x='Parch', y='Age',\n            kind='hex')\nplt.tight_layout()\nplt.show()","c426a713":"pip install seaborn==0.11.0","0c381c35":"import seaborn as sns","922a0aca":"sns.set()\nsns.displot(data=train_frame, x='Age', y='SibSp', kind='hist',hue='Survived', height=6, aspect=1.2)","933e31fc":"sns.displot(data=train_frame, x='Age', kind='hist',col='Embarked', hue='Survived')","e735a804":"sns.displot(\n    data=train_frame, kind=\"hist\", kde=True,\n    x=\"Survived\", col=\"Embarked\", hue=\"Pclass\",\n)","8bf3e468":"sns.displot(\n    data=train_frame, kind=\"kde\", rug=True,\n    x=\"Age\", y=\"Pclass\",\n    col=\"Embarked\", hue=\"Survived\",\n)","df0a3961":"df_train1 = train_frame.copy()\ndf_test1 = test_frame.copy()","745e49b9":"Age = [df_train1, df_test1]\n\nfor dataset in Age:\n    #making use of both test and train frame\n    mean = train_frame[\"Age\"].mean()\n    std = test_frame[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # computing random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated in range of mean +\/- std\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = df_train1[\"Age\"].astype(int)\ndf_train1[\"Age\"].isnull().sum()","f2015e84":"#re-checking if any nulls of Age feautes \ntrain_null = df_train1.isnull().sum().sort_values(ascending=False)\ntrain_null","32c86e3a":"df1 = train_frame['Age'].isnull()\ndf2 = pd.DataFrame(df1)\ndf2.columns = [\"new\"]\ndf2.head()","55925fa9":"#using boolean expressions to divide Nan rows\ndf3 = train_frame[df2['new']==True] #for nan val of age\ndf4 = train_frame[df2['new']==False]\ndf3.head()","eb64382e":"df4.head()","bfca4852":"! pip install datawig","e1066cce":"import datawig","26a396bc":"#Using a SimpleImputer model\nimputer = datawig.SimpleImputer(\n    input_columns = ['Sex','Pclass', 'Parch', 'Embarked', 'Survived'], output_column = 'Age',\n    output_path = 'imputer_model')\n#input cols serves as data on whose basis imputation is performed\n#output col is one on which imputation will be performed","2c594b04":"#Fit an imputer model on the train data\nimputer.fit(train_df=df4, num_epochs=50)","8043ad3b":"#Impute missing values and return original dataframe with predictions\nimputed = imputer.predict(df3)","8d7a9fa5":"imputed[\"Age_imputed\"].isnull().sum()","0d876412":"del imputed['Age']","01bc36be":"imputed = imputed.rename(columns={'Age_imputed' : 'Age'})","35408214":"imputed.head()","817eaccf":"imputed = imputed[['PassengerId','Survived', 'Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin',\n                   'Embarked']]","da7f6fba":"df6 = imputed.append(df4)\ndf6.sort_values(by=['PassengerId'])","e52b8332":"df6[\"Age\"].isnull().sum()","393a3420":"from fancyimpute  import KNN ","8833882b":"cols = ['Survived', 'Pclass', 'SibSp', 'Parch', 'Age']","9db4c1a4":"df_t1 = train_frame.copy()\ndf_te1 = test_frame.copy()","db6d84f0":"k_n = KNN(k=7).fit_transform(df_t1[cols]) ","a8c1b4c6":"k_n","7ba2c4fc":"df1 = pd.DataFrame(KNN(k=5).fit_transform(df_t1[cols]) )","060c9b4b":"df1.head()","d6c4f9dd":"df_train2 = train_frame.copy()\ndf_test2 = test_frame.copy()","ff073324":"df_corr = df_train2.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_corr[df_corr['Feature 1'] == 'Age']","230694f6":"age_by_pclass_sex = df_train2.groupby(['Sex', 'Pclass']).median()['Age']\n\nfor pclass in range(1, 4):\n    for sex in ['female', 'male']:\n        print('Median age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\nprint('Median age of all passengers: {}'.format(df_train1['Age'].median()))","324bef1d":"df_train2['Age'] = df_train2.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","c7c5c66e":"# Filling the missing values in test frame as well for Age with the medians of Sex and Pclass groups\nage_by_pclass_sex = df_test2.groupby(['Sex', 'Pclass']).median()['Age']\n\nfor pclass in range(1, 4):\n    for sex in ['female', 'male']:\n        print('Median age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\nprint('Median age of all passengers: {}'.format(df_test2['Age'].median()))","cda891b2":"# Filling the missing values in Age with the medians of Sex and Pclass groups\ndf_test2['Age'] = df_test2.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","bfb45325":"train_null = df_train2.isnull().sum().sort_values(ascending=False)\ntrain_null","df789e61":"test_null = df_test2.isnull().sum().sort_values(ascending=False)\ntest_null","29895cc3":"#filling embarked with most frquent value\ncommon_value = 'S'\nembark = [df_train2, df_test2]\n\nfor dataset in embark:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)","15e74037":"med_fare = df_test2.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\nmed_fare","00bb5ac7":"# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\nfare = [df_test2]\n\nfor dfall in fare:\n    dfall['Fare'] = dfall['Fare'].fillna(med_fare)","7ef9c004":"#take title out of name\nFeature = [df_train2, df_test2]\nmin_feature = 10\n\nfor dataset in Feature:\n     dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n        \ntitle = (df_train2['Title'].value_counts() < min_feature)\ndf_train2['Title'] = df_train2['Title'].apply(lambda x: 'Misc' if title.loc[x] == True else x)\n\ndf_train2['Title'].value_counts()","d131c910":"Survived_female = df_train2[df_train2.Sex=='female'].groupby(['Sex','Title'])['Survived'].mean()\nSurvived_female","519a697a":"Survived_male = df_train2[df_train2.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nSurvived_male","087930be":"! pip install pywaffle","089ba9fd":"from pywaffle import Waffle","62511fc4":"waf = {'Mr':517, 'Miss':182, 'Mrs':125, 'Master':40, 'Misc':27}\nwaf1 = pd.DataFrame(waf.items(), columns=['Title', 'Value'])","7a240762":"total_values = sum(waf1['Value'])\ncategory_proportions = [(float(value) \/ total_values) for value in waf1['Value']]\n\n# print out proportions\nfor i, proportion in enumerate(category_proportions):\n    print (waf1.Title.values[i] + ': ' + str(proportion))","645264a0":"#add waffle for title\nwidth = 40 # width of chart\nheight = 10 # height of chart\n\ntotal_num_tiles = width * height # total number of tiles\n\n# compute the number of tiles for each catagory\ntiles_per_category = [round(proportion * total_num_tiles) for proportion in category_proportions]\n\n# print out number of tiles per category\nfor i, tiles in enumerate(tiles_per_category):\n    print (waf1.Title.values[i] + ': ' + str(tiles))","bdef3975":"data = {'Mr': 232, 'Miss': 82, 'Mrs': 56, 'Master': 18, 'Misc': 12}\nfig = plt.figure(\n    FigureClass=Waffle, \n    rows=10, \n    columns=40,\n    values=data, \n    cmap_name=\"tab10\",\n    legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)},\n    icons='male', icon_size=18, \n    icon_legend=True,\n    figsize=(14, 18)\n)","1c2b99d4":"#encoding\nFeature1 = [df_train2, df_test2]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Misc\": 5}\n\nfor dataset in Feature1:\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \ndf_train2['Title'].value_counts()","becc6e09":"sns.scatterplot(x=\"Age\", y=\"Title\", hue=\"Survived\", size=\"Sex\", palette=\"Set1\",\n                     sizes=(20, 200), hue_norm=(0, 7),\n                     legend=\"full\", data=df_train2)","432a4dea":"pd.qcut(df_train2['Age'], 5).value_counts()","936f81f1":"pd.qcut(df_train2['Fare'], 5).value_counts()","bcf753d4":"age_bin = [df_train2, df_test2]\nfor dataset in age_bin:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 20, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 25), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 25) & (dataset['Age'] <= 30), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 30) & (dataset['Age'] <= 40), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 80), 'Age'] = 4\n","451bbb3a":"df_test2['Age'].value_counts()","dd320b52":"Fare_bin = [df_train2, df_test2]\nfor dataset in Fare_bin:\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    dataset.loc[ dataset['Fare'] <= 7.854, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.854) & (dataset['Fare'] <= 10.5), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 10.5) & (dataset['Fare'] <= 21.679), 'Fare'] = 2\n    dataset.loc[(dataset['Fare'] > 21.679) & (dataset['Fare'] <= 39.688), 'Fare'] = 3\n    dataset.loc[(dataset['Fare'] > 39.688) & (dataset['Fare'] <= 513), 'Fare'] = 4","b1360ff4":"df_train2['Fare'].value_counts()","6bdfa31e":"Fig = sns.FacetGrid(df_train2, col = 'Fare', col_wrap=3)\nFig.map(sns.pointplot, 'Age', 'Survived', 'Sex', ci=95.0, palette = 'deep')\nFig.add_legend()","b4780baf":"## some learning\nfemale_mean = df_train2[df_train2.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','Fare'])['Survived'].mean()\nfemale_mean","c0777006":"male_mean = df_train2[df_train2.Sex=='male'].groupby(['Sex','Pclass', 'Embarked','Fare'])['Survived'].mean()\nmale_mean","def7fd56":"# Creating Deck column from the first letter of the Cabin column, For Missing using M\ndf_train2['Deck'] = df_train2['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n\ndf_decks = df_train2.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp',\n                                                                      'Parch','Fare', 'Embarked', 'Cabin', 'PassengerId', \n                                                                          'Ticket', 'Title']).rename(columns={'Name': 'Count'}).transpose()","ce3a23cf":"df_decks","3911a280":"deck = df_train2[df_train2['Deck'] == 'T'].index\ndf_train2.loc[deck, 'Deck'] = 'M'","dca59139":"waf2 = {'A':15, 'B':47, 'C':59, 'D':33, 'E':32, 'F':13, 'G':4, 'M':647, 'T':1}\nwaf2 = pd.DataFrame(waf2.items(), columns=['Deck', 'Value'])","f1fb9612":"total_values = sum(waf2['Value'])\ncategory_proportions = [(float(value) \/ total_values) for value in waf2['Value']]\n\nwidth = 30 # width of chart\nheight = 10 # height of chart\n\ntotal_num_tiles = width * height # total number of tiles\n\n# compute the number of tiles for each catagory\ntiles_per_category = [round(proportion * total_num_tiles) for proportion in category_proportions]\n\n# print out number of tiles per category\nfor i, tiles in enumerate(tiles_per_category):\n    print (waf2.Deck.values[i] + ': ' + str(tiles))\n","8b22e3fa":"data = {'A':5, 'B':17, 'C':21, 'D':12, 'E':11, 'F':5, 'G':1, 'M':228, 'T':0}\nfig = plt.figure(\n    FigureClass=Waffle, \n    rows=10, \n    columns=40,\n    values=data, \n    cmap_name=\"tab10\",\n    legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)},\n    icons='ship', icon_size=18, \n    icon_legend=True,\n    figsize=(14, 18)\n)","462b5deb":"df_train2['Deck'] = df_train2['Deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_train2['Deck'] = df_train2['Deck'].replace(['D', 'E'], 'DE')\ndf_train2['Deck'] = df_train2['Deck'].replace(['F', 'G'], 'FG')\n\ndf_train2['Deck'].value_counts()","097e2d26":"df_test2['Deck'] = df_test2['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n\ndft_decks = df_test2.groupby(['Deck', 'Pclass']).count().drop(columns=['Sex', 'Age', 'SibSp',\n                                                                      'Parch','Fare', 'Embarked', 'Cabin', 'PassengerId', \n                                                                          'Ticket', 'Title']).rename(columns={'Name': 'Count'}).transpose()","d1a27226":"df_test2['Deck'] = df_test2['Deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_test2['Deck'] = df_test2['Deck'].replace(['D', 'E'], 'DE')\ndf_test2['Deck'] = df_test2['Deck'].replace(['F', 'G'], 'FG')\n\ndf_test2['Deck'].value_counts()","042a3ebb":"###ckeck corr between difernt class\nTarget = ['Survived']\ncorr_cols = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare']\n\nfor corr in corr_cols:\n    if df_train2[corr].dtype != 'float64' :\n        print('Surviving Correlation by:',corr)\n        print(df_train2[[corr, Target[0]]].groupby(corr, as_index=False).mean())\n        print('-'*20, '\\n')#'*20 by -' is to make bottom line of each corr","64bf7c11":"corr_train = df_train2.drop(['PassengerId'], axis=1).corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ncorr_train.rename(columns={\"level_0\": \"Attribute 1\", \"level_1\": \"Attribute 2\", 0: 'Correlation Coefficient'}, inplace=True)\ncorr_train.drop(corr_train.iloc[1::2].index, inplace=True)\ncorr_train1 = corr_train.drop(corr_train[corr_train['Correlation Coefficient'] == 1.0].index)","448e6f1e":"#Train frame correlations check\ncorr = corr_train1['Correlation Coefficient'] > 0.3\ncorr_train1[corr]","71c90c86":"corr_test = df_test2.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ncorr_test.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ncorr_test.drop(corr_test.iloc[1::2].index, inplace=True)\ncorr_test1 = corr_test.drop(corr_test[corr_test['Correlation Coefficient'] == 1.0].index)","0f30e2bc":"# Test frame correlations check\ncorr1 = corr_test1['Correlation Coefficient'] > 0.3\ncorr_test1[corr1]","662cf719":"fig, axs = plt.subplots(nrows=2, figsize=(25, 20))\n\nsns.heatmap(df_train2.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True, cmap=\"YlGnBu\", annot_kws={'size': 14})\nsns.heatmap(df_test2.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n\nfor i in range(2):    \n    axs[i].tick_params(axis='x', labelsize=14)\n    axs[i].tick_params(axis='y', labelsize=14)\n    \naxs[0].set_title('Correlations for Training data features', size=15)\naxs[1].set_title('Correlations for Test data features', size=15)\n\n\naxs[0].set_ylim(6.0, 0)\naxs[1].set_ylim(6.0, 0)\nplt.show()","41a5eaea":"#Models for checking, might not use all\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier","e785067f":"df_train2.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ndf_test2.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","718f7ca5":"df_train2.head()","9b536dfe":"df_test2.head()","be8d92d9":"from sklearn.preprocessing import LabelEncoder","192a224b":"class FeatureEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n    \n    def fit(self,X,y=None):\n        return self \n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)","8671338a":"df_tr = df_train2.copy()\ndf_tr = FeatureEncoder(columns = ['Sex','Embarked', 'Deck']).fit_transform(df_tr)","a06a9739":"df_te = df_test2.copy()\ndf_te = FeatureEncoder(columns = ['Sex','Embarked', 'Deck']).fit_transform(df_te)","d3815419":"df_te.head()","43a3216d":"df_tr.dtypes","f6d87338":"cols = ['Survived', 'Pclass', 'Sex', 'Parch', 'Fare', 'Embarked']\nsns.pairplot(df_tr[cols], diag_kind=\"hist\", hue = 'Survived', palette=\"Set1\")","c2df61af":"sns.swarmplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", data=df_tr)","a255476b":"df_te = df_te.astype(int)\ndf_te.dtypes","2ea064fb":"x_train = df_tr.iloc[:,1:]\ny_train = df_tr.iloc[:,:1]","e2243708":"x_test = df_te.copy()","195b9c87":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer","1eae4634":"#start with PF of degree 3 then we will check till 5 if efficieny increase else we will leave it\npr=PolynomialFeatures(degree=3)\nz = pr.fit_transform(x_train)\nz.shape","645fd6de":"Input=[('scale',PowerTransformer()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model',LogisticRegression())]\npipe=Pipeline(Input)\npipe.fit(x_train,y_train)\nlr_score = round(pipe.score(x_train, y_train) * 100, 2)\nprint(\"score:\", lr_score, \"%\")","56eeaf6f":"decision_tree = DecisionTreeClassifier(criterion='entropy', splitter='random') \ndecision_tree.fit(x_train, y_train)  \ndt_pred = decision_tree.predict(x_test)  \ndt_score = round(decision_tree.score(x_train, y_train) * 100, 2)\nprint(\"score:\", dt_score, \"%\")","fe408877":"knn = KNeighborsClassifier(n_neighbors = 5) \nknn.fit(x_train, y_train)  \nknn_pred = knn.predict(x_test)  \nknn_score1 = round(knn.score(x_train, y_train) * 100, 2)\nprint(\"score:\", knn_score1, \"%\")","dd1af5c5":"knn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(x_train, y_train)  \nknn_pred = knn.predict(x_test)  \nknn_score = round(knn.score(x_train, y_train) * 100, 2)\nprint(\"score:\", knn_score, \"%\")","2cd837fe":"# Random Forest\nrandom_forest = RandomForestClassifier(criterion = \"gini\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 10,   \n                                       n_estimators=100, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(x_train, y_train)\nrf_pred = random_forest.predict(x_test)\n\nrf_score = round(random_forest.score(x_train, y_train) * 100, 2)\nprint(\"score:\", rf_score, \"%\")","9122d392":"from sklearn import svm\nfrom sklearn.svm import SVC\n\nmodel = svm.SVC(C=1, kernel='poly', random_state=0, gamma = 'auto', degree = 5)\nmodel.fit(x_train, y_train)\nsvm_pred = model.predict(x_test)\nsvm_score = round(model.score(x_train, y_train) * 100, 2)\nprint(\"score:\", svm_score, \"%\")","b8e73983":"clf = XGBClassifier()\nclf.fit(x_train, y_train, eval_metric='auc', verbose=True)\nxgb_pred = clf.predict(x_test)\nxgb_score = round(clf.score(x_train, y_train) * 100, 2)\nprint(\"score:\", xgb_score, \"%\")","0d2eac3e":"clf1 = XGBClassifier(booster='dart', min_split_loss = 1, max_depth= 7)\nclf1.fit(x_train, y_train, eval_metric='auc', verbose=True)\nxgb_pred = clf1.predict(x_test)\nxgb1_score = round(clf.score(x_train, y_train) * 100, 2)\nprint(\"score:\", xgb1_score, \"%\")","a46efb97":"perceptron = Perceptron(max_iter=5)\nperceptron.fit(x_train, y_train)\n\npercep_pred = perceptron.predict(x_test)\n\npercep_score = round(perceptron.score(x_train, y_train) * 100, 2)\nprint(\"score:\", percep_score, \"%\")","a6b3529f":"gaussian = GaussianNB() \ngaussian.fit(x_train, y_train)  \nnb_pred = gaussian.predict(x_test)  \nnb_score = round(gaussian.score(x_train, y_train) * 100, 2)\nprint(\"score:\", nb_score, \"%\")","1dca3a7d":"#area for Model Score visual\nresults = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN_3 ', 'KNN_5', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'XGB Classifier',\n              'Decision Tree'],\n    'Score': [svm_score, knn_score, knn_score1, lr_score, \n              rf_score, nb_score, percep_score, xgb_score, dt_score]})\nresult_df = results.sort_values(by='Score', ascending=False)\n#result_df = result_df.set_index('Score')\nresult_df.head(9)","0733a6b9":"ax = sns.barplot(x=\"Model\", y=\"Score\", data=result_df, palette=\"Set2\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.tight_layout()\nplt.show()","25729293":"#area for feature importance visual\nimportances = pd.DataFrame({'feature':x_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(9)","9a59896e":"plt.figure(figsize=(10, 5))\nsns.barplot(x='importance', y=importances.index, data=importances)\n\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.title('Random Forest Classifier', size=15)\n\nplt.show()","ca341595":"nb_p = gaussian.predict_proba(x_train)","0b0cddeb":"nb_p[0]","d1d8d36c":"prob_nb = pd.DataFrame(nb_p, columns=['Prob_NotSurvived', 'Prob_Survived']) ","17dddde5":"prob_nb.head()","7fef90ce":"pd.cut(prob_nb['Prob_NotSurvived'], 5).value_counts()","f9d1a0a9":"## lets probability which may be cuasing issue for classifier ie .41 to 0.59\nprob_issue = prob_nb[prob_nb['Prob_NotSurvived']>=0.41]","7666bafe":"prob_issue = prob_issue[prob_issue['Prob_NotSurvived']<=0.59]","e702b9d4":"prob_issue.head()","bb58686a":"dfp = decision_tree.predict_proba(x_train)","b4420644":"prob_dt = pd.DataFrame(dfp, columns=['Prob_NotSurvived', 'Prob_Survived'])","093fbf2b":"pd.cut(prob_dt['Prob_NotSurvived'], 5).value_counts()","a3f599c1":"#then end at df test pred\n\ndf_submit = pd.DataFrame(columns=['PassengerId', 'Survived'])\ndf_submit['PassengerId'] = test_frame['PassengerId']\ndf_submit['Survived'] = dt_pred\n#Since i already submited\n#df_submit.to_csv('submit01.csv', header=True, index=False)\n","870703d2":"## Working on Mising data","356bf0e6":"##### We'll use multiple model and see they behave w.r.t to training data and choose the best model to submit for the competetion","50d94685":"#### see how this will effect model performace, even humans will have issue with such odds :D","ca8b17eb":"##### Note: Apart from heatmaps which are used further in this notebook we can also sortout corr between different features for feature selections as highly correlated feature sometime exibit no extra information than their counterparts","e2ad9e7a":"##### The main reason to have seaborn apart from matplot lib is It is used to create more attractive and informative statistical graphics","2075edda":"##### Since Age feature has the most num of NaN, easiest way is to take mean and fill is by +,_ of its std ","d95e4de0":"#### waffle visual for deck","02f29f07":"### Predict probabilty via NaiveBayes and Decision Tree","f7aa11c8":"### 2. Imputations\n","1e55cd8e":"##### See how a combination of multiple feature tell us details about target. We can actually create our own probabalistic model with these ratios.","fabf538e":"##### LabelEncoder:  \n##### can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average of dog and mouse is cat. Still there are algorithms like decision trees and random forests that can work with categorical variables just fine and LabelEncoder can be used to store values using less disk space.\n\n##### One-Hot-Encoding:   \n##### has the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space. The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality. In these cases, I typically employ one-hot-encoding followed by PCA for dimensionality reduction. I find that the judicious combination of one-hot plus PCA can seldom be beat by other encoding schemes. PCA finds the linear overlap, so will naturally tend to group similar features into the same feature.","afebe133":"##### We'll start with Name Feature which doesn't hold much information for a model but a have deeper look and you'll find we can actually extract title from it like 'Mr.' , 'Mrs' etc which can be made useful","169f3b24":"##### Note:\n\n##### shape function provide number of rows and column in a given dataset like (rows, cols)","6914fc6d":"##### Dataframe.describe(include='all') is used to provide information about categorical data. This helps us to identify what categories we are seeing the most ","b175f7c6":"## A Guide on how to perform different feature engineering tasks\n### From Imputations, implementing statistics to making use of Probability prediction ","3bef6af6":"## Model Training and Evaluation","dc1e205f":"##### Now this can be re-added to orignal data frame and processed as rest","4cffdd2a":"##### One more thing to make note of is the higher in terms of 'degree' you go in Polynomial the the more feature it will create and you might end up breaking your code, so it is better to add a break for this model","62adff6f":"##### pipelines are set up with the fit\/transform\/predict functionality, so that we can fit the whole pipeline to the training data and transform to the test data without having to do it individually for everything","f21e2367":"##### This is a very powerful function of NaiveBayes and Decision Tree (also employed in Random Forest, XGB and so on). Having a strong understanding of probablities and distribution functions combine with with this may beat some of the top models. If you can somehow work on probablities that are in range of 0.4-0.6 which are the real problem for model and create your function\/layer between models say: Naive Bayes -> Your Function -> New features -> XGB\/LightGBM might actually perform exceptionally well. \n##### If you want to get going with this i recommed read about probablities and distributions functions","8bdb6b7e":"### <span style=\"color:green\"> Version 3.0: Added new plots from Seaborn 0.11 release","6d6d65f1":"##### PowerTransformer provides non-linear transformations in which data is mapped to a normal distribution to stabilize variance and minimize skewness.","0714ff4a":"### Some fun with waffle chart","7856a155":"## Feature Creation","bc7f6123":"**See how extracting titles from name has given meaning to this new feature and how much it impact**","7dae16e4":"**Understand how a feature creation 'Title' has impacted on our model performance**","a23c1572":"##### Notice the difference of Medians for train and test data","380b198e":"##### now check if any pending nan left in age feature","c4dc4c58":"##### Adding all model performance in a single df","054ccb27":"##### Feel free to contact me here or at : https:\/\/www.linkedin.com\/in\/muhammad-saad-31740060\/","70aada18":"##### As Embarked has only few missing values we can use the most frequent occuring strategy here","f8f041a7":"##### Since Deck T  is negligible assigning it to frequent deck value","ede42004":"### <span style=\"color:green\"> If you like my work, learned something out of it or found it useful , an upvotes would be really appreciated :-) <\/span>","93bb5edf":"##### Note: Here we are going to check correlation or odds of surviving with respect to all features. This will give us a boost in understanding which charcteristics of features have more importance.","35b693ba":"#### Note: \n##### We now have some understanding of what we are expecting from different features like for 'Parch' we have decision boundires for survial indications of which classes are going with 100% survival and which are with 0% and so on.","6f323943":"##### Notice how applying Polynomial featuring have increase dimensions\/variables to the equation","934bc83a":"##### Explanation:\n##### Dataframe.Describe()\n\n##### provide a good EDA understanding of the dataset in hand, it provide mean, std and fragments of each 25% and is good to have a glimpse of outliers in data prior to visualzations","672dd786":"##### iLoc : iloc returns a Pandas Series when one row is selected, and a Pandas DataFrame when multiple rows are selected, or if any column in full is selected   \n\nwhereas,  \n\n##### Loc : loc is label-based, which means that you have to specify rows and columns based on their row and column labels\n","f80c595d":"### 3. Using The power of Statistics","bc857544":"##### Same as above but with heatmaps","11dec852":"##### It is a part of Feature Engineering and it is process of creating features that don't already exist in the dataset or creating meaning out of not so important features","2bde41ba":"##### Lets check how many null\/missing values we have in both dataset so we can define our strategy of how to treat them","8d1fa211":"#### Explanations:\n##### %matplotlib inline:\n##### is specifically used here for jupyter notebook to store plots in notebook document \n\n##### warnings: \n#####    has been imported to avoid raise of warning when a function is deprecated","49f1a2af":"### 1. Taking mean & std ","e0ca21d0":" \n \n#### We will start from exploring the data to see what we have to do clean the provided data set. \"Always consider in any data science problem we have to perform some exploratory data analysis as first steps\" (if data is readily available, if its not like for data mining strategy will be a bit different)\n\n","e3d18a13":"**it is important to have a clear understanding of what libs you will be using and import\/install them beforehand**","3b2dac9f":"##### Splitting target and features","2f3c29b0":"Same for Test dataset","456831e1":"##### Let's think more logically and find certain features which have significant impact on missing data, lets start corelations and find which features are similar to Age","dc4b6a6a":"##### As we now have 2 features of 'Age', 1 is orignal with Nan and second with imputated values so droping NaN col.","fdd360e4":"****As it is seen output is array which can be coverted to dataframe as shown below****","4d698153":"#### 1. Filling NaN with Mean and Standard Deviation\n\n#### 2. Imputations using simple impute\/KNN\n\n#### 3. Using statistics to fill NaN ","25b879e6":"##### For learning purpose will use 2 imputations Simple Impute and KNN since others are out of context for this task\n##### (You are free to experiment with others as this improve your understanding of different process)","94a48dd9":"##### This seems much better as we used some statistical understanding to fill missing data","8ca5b969":"#### Note:\n##### The main reason behind employing visualization is because it is helpful in understanding data\/distributions by placing it in a visual context so that patterns, trends and correlations that might not otherwise be detected can be worked with","825e3171":"#### 2.1 Imputing Using Simple Impute","57673005":"\n\n##### so our train and pred data have 891 and 418 rows and 12 and 11 cols respectively, df_pred has obviously 1 col less because that is the target col which we have to predict","b27c6a70":"#### Explanation:\n##### Filling missing values in dataset is of utmost importance because the fate and accuracy of your model rely heavily on your strategy. There are numerous ways for filling your missing data with most easy is to replace it by mean\/median or most common value.","5fceba33":"To avoid encoding feature by feautre we employ above method which is written With help of : https:\/\/stackoverflow.com\/questions\/24458645\/label-encoding-across-multiple-columns-in-scikit-learn","5b099a69":"### Installing the newest release of Seaborn","74e208dc":"##### This part is just to understand how KNN can be useful for filling NaN values, we'll just cover how its done and move forward to next strategy","5824b38c":"##### Imputation is the process of replacing missing data with substituted values\n\n#### Imputations can be performed in many ways as describe below:","3432e9c9":"# An Example of Data Science work on Titanic Dataset","c3aabafe":"#### 2.2 Imputing Using KNN","88c91b81":"Encoding the feature manually, we can employ One Hot or Label Encoder for this task but this is done to show it can be mapped manually as well if you are not sure if encoders will perform differently on validations\/test\/train set.","81f0bb3f":"##### SimpleFill: Replaces missing entries with the mean or median of each column.\n\n##### \u2022KNN: Nearest neighbor imputations which weights samples using the mean squared difference on features for which two rows both have observed data.\n\n##### \u2022SoftImpute: Matrix completion by iterative soft thresholding of SVD decompositions. Inspired by the softImpute package for R, which is based on Spectral Regularization Algorithms for Learning Large Incomplete Matrices by Mazumder et. al.\n\n##### \u2022IterativeSVD: Matrix completion by iterative low-rank SVD decomposition. Should be similar to SVDimpute from Missing value estimation methods for DNA microarrays by Troyanskaya et. al.\n\n##### \u2022MICE: Reimplementation of Multiple Imputation by Chained Equations.\n\n##### \u2022MatrixFactorization: Direct factorization of the incomplete matrix into low-rank U and V, with an L1 sparsity penalty on the elements of U and an L2 penalty on the elements of V. Solved by gradient descent.\n\n##### \u2022NuclearNormMinimization: Simple implementation of Exact Matrix Completion via Convex Optimization by Emmanuel Candes and Benjamin Recht using cvxpy. Too slow for large matrices.\n\n##### \u2022BiScaler: Iterative estimation of row\/column means and standard deviations to get doubly normalized matrix. Not guaranteed to converge but works well in practice. Taken from Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares.","d01baa86":"##### Using Median approach for Fare as it is also have few occuring","7007c662":"##### The only downside is we cannot track if the correlation is negative (if you want to do it, remove abs() from the code)","9450892e":"##### lets visualize our newly created feature in waffle","afb2dc88":"##### Lastly, there are tonnes of options I didn't discuss here, some of the prominent are Cross Validations, Grid Seacrh for paramters tuning, PCA. But remember that they are also essential part of any ML project.","eb922d96":"##### Bining Deck feature ","5411d09f":"You can see how the imputer has transformed our Age bin","db127e05":"#### Note: \n##### sometime it is wise to drop missing data rows\/cols mainly when there are too much nan values or if filling them not make any sense. This is part where our understanding of statistics and visualization skills provide us the insights about how to deal with your datset","0ca346be":"#### Note:\n##### This is not the most intelligent or accurate approach since there are different groups w.r.t to pclass, embarked and etc so the model understanding will be biased. So we will not be continuing with this approach","0d9b69af":"#### What we will do\n##### Here i have taken 3 dif strategies just for knowledge sharing as how we can fill nan\/missing values. \n\n","7347526c":"##### Dropping Features not relevant for performace","96aeeaf9":"##### Note: The major distinction between 'pd.cut & pd.qcut' is that 'qcut' will calculate the size of each bin in order to make sure the distribution of data in the bins is equal. In other words, all bins will have (roughly) the same number of observations. While 'cut' will create bins on average and each bin may have different vaulues(unequal samples).","1f4eb674":"##### As evident age is mostly correlated with Pclass, this will help us. This time i will use median age by Pclass feature and fill missing value by this correlation","9882b476":"##### we can further join index with orignal dataframe to check where on which features our Naive Bayes and decision tree model is having issue and with further analysis we can tune our model better but this will be out of scope for this notebook.","0d64de4e":"##### predict_proba gives you the probabilities for the target (0 and 1 in our case) in array form. The number of probabilities for each row is equal to the number of categories in target variable"}}