{"cell_type":{"842b4e37":"code","95a4938d":"code","7f167d2a":"code","66830871":"code","a4f4a585":"code","22b0f3bc":"code","8415ffa7":"code","3adfe2f6":"code","43bd47d9":"code","209ca921":"code","e215c122":"code","594c8317":"code","d2099346":"code","9e421ebc":"code","e5387868":"code","d22c7756":"code","e01f3b02":"code","d43b7d4f":"code","57389d49":"code","3ec8bdf7":"code","9bf73a5d":"markdown","5f598e49":"markdown","7749b808":"markdown","4b740d6b":"markdown","5bb3e6dd":"markdown","ebaacc77":"markdown","d5206efc":"markdown","155b1294":"markdown","c758fbe9":"markdown","6325aea3":"markdown","c23e4295":"markdown","7567fa58":"markdown","769a3aa6":"markdown","b4d5a66a":"markdown","0fd6ff04":"markdown","7a2da52f":"markdown"},"source":{"842b4e37":"!pip install nlpaug #numpy matplotlib python-dotenv","95a4938d":"# DataFrame\nimport pandas as pd\n\n# Matplot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\n\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\n\n# Utility\nimport re\nimport numpy as np\nimport os\nfrom collections import Counter\nimport logging\nimport time\n\n# Set log\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","7f167d2a":"# DATASET\ndataset_path = \"..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\"\nDATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\nTRAIN_SIZE = 0.9\nSAMPLING_FRAC = 0.01\n\n# TEXT CLENAING\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"","66830871":"df = pd.read_csv(dataset_path, encoding =DATASET_ENCODING , names=DATASET_COLUMNS).sample(frac=SAMPLING_FRAC, random_state=1)","a4f4a585":"print(\"Dataset size:\", len(df))","22b0f3bc":"df.head(5)","8415ffa7":"decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\ndef decode_sentiment(label):\n    return decode_map[int(label)]","3adfe2f6":"%%time\ndf.target = df.target.apply(lambda x: decode_sentiment(x))","43bd47d9":"target_cnt = Counter(df.target)\n\nplt.figure(figsize=(16,8))\nplt.bar(target_cnt.keys(), target_cnt.values())\nplt.title(\"Dataset labels distribuition\")","209ca921":"nltk.download('stopwords')\nstop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")","e215c122":"def preprocess(text, stem=False):\n    # Remove link,user and special characters\n    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)","594c8317":"%%time\ndf.text = df.text.apply(lambda x: preprocess(x))","d2099346":"df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\ndf_train.name = \"Original\"\nprint(\"TRAIN size:\", len(df_train))\nprint(\"TEST size:\", len(df_test))","9e421ebc":"import nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.char as nac","e5387868":"aug = nac.KeyboardAug() ##define th text augmentation method\ndf_keyboard = df_train.copy(deep=True)\ndf_keyboard.text= df_keyboard.text.apply(lambda _text: aug.augment(_text))\ndf_keyboard.name =  \"random keyboard\"\n\nprint(\"Original:\")\nprint(df_train.text.iloc[1])\nprint(\"Augmented Text:\")\nprint(df_keyboard.text.iloc[1])","d22c7756":"aug = naw.WordEmbsAug(\n    model_type='word2vec', model_path=\"..\/input\/sentiment140-w2v\/sentiment140_w2v.bin\",action=\"substitute\")\n\ndf_Sub = df_train.copy(deep=True)\ndf_Sub.text = df_Sub.text.apply(lambda _text: aug.augment(_text))\ndf_Sub.name = \"Synonym Replacement\"\nprint(\"Original:\")\nprint(df_train.text.iloc[1])\nprint(\"Augmented Text:\")\nprint(df_Sub.text.iloc[1])","e01f3b02":"aug = naw.RandomWordAug(action=\"swap\")\ndf_RS = df_train.copy(deep=True)\ndf_RS.text= df_RS.text.apply(lambda _text: aug.augment(_text))\ndf_RS.name = \"Random Swap\"\nprint(\"Original:\")\nprint(df_train.text.iloc[1])\nprint(\"Augmented Text:\")\nprint(df_RS.text.iloc[1])","d43b7d4f":"aug = naw.RandomWordAug(action=\"delete\")\ndf_RD = df_train.copy(deep=True)\ndf_RD.text= df_RD.text.apply(lambda _text: aug.augment(_text))\ndf_RD.name = \"Random Deletion\"\nprint(\"Original:\")\nprint(df_train.text.iloc[1])\nprint(\"Augmented Text:\")\nprint(df_RD.text.iloc[1])","57389d49":"aug = naw.WordEmbsAug(\n    model_type='word2vec', model_path=\"..\/input\/sentiment140-w2v\/sentiment140_w2v.bin\",action=\"insert\")\n\ndf_RI = df_train.copy(deep=True)\ndf_RI.text = df_RI.text.apply(lambda _text: aug.augment(_text))\ndf_RI.name = \"Random Insertion\"\nprint(\"Original:\")\nprint(df_train.text.iloc[1])\nprint(\"Augmented Text:\")\nprint(df_RI.text.iloc[1])","3ec8bdf7":"def word_feats(words):\n    return dict([(word, True) for word in words.split() if word not in stop_words])\n\nfeatureset_test = [(word_feats(row.text), row.target) for index, row in df_test.iterrows()]\n\n\n\nfor df_train in (df_train,df_keyboard, df_Sub, df_RS, df_RD, df_RI):\n    featureset_train = [(word_feats(row.text), row.target) for index, row in df_train.iterrows()]\n    classifier = nltk.NaiveBayesClassifier.train(featureset_train)\n    print(\"Testset accuracy of\",df_train.name,\"is: \",nltk.classify.accuracy(classifier, featureset_test))\n\n","9bf73a5d":"## Substitute character by keyboard distance","5f598e49":"### Map target label to String\n* **0** -> **NEGATIVE**\n* **4** -> **POSITIVE**","7749b808":"### Settings","4b740d6b":"# Character Augmenter","5bb3e6dd":"## Synonym Replacement (SR)\nRandomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms  hosen at random.","ebaacc77":"### Dataset details\n* **target**: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n* **ids**: The id of the tweet ( 2087)\n* **date**: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n* **flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n* **user**: the user that tweeted (robotickilldozr)\n* **text**: the text of the tweet (Lyx is cool)","d5206efc":"### Pre-Process dataset","155b1294":"# Twitter Sentiment Analysis\nstarer code from https:\/\/www.kaggle.com\/paoloripamonti\/twitter-sentiment-analysis","c758fbe9":"## Random Insertion (RI)\n\nFind a random synonym of a random word in the sentence that is\nnot a stop word. Insert that synonym into a ran-\ndom position in the sentence. Do this n times","6325aea3":"## Random Swap (RS)\n Two words in the sentences are randomly swapped.","c23e4295":"## Random Deletion (RD)\nRandom removal for each word in the sentence with a probability p.","7567fa58":"### Read Dataset","769a3aa6":"## Naive Bayes Classifier","b4d5a66a":"# Word Augmenter","0fd6ff04":"# EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\nPaper by Jason Wei & Kai Zou (https:\/\/www.aclweb.org\/anthology\/D19-1670.pdf)\n\ncode from https:\/\/github.com\/makcedward\/nlpaug\/blob\/master\/example\/textual_augmenter.ipynb","7a2da52f":"## Split train and test"}}