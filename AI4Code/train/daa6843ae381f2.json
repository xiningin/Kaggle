{"cell_type":{"5ea33278":"code","afea0cf1":"code","798e14fa":"code","ac819ee1":"code","d908b080":"code","0191a0b3":"code","73e83a99":"code","bcf6a1ca":"code","ff491763":"code","9bb1d2aa":"code","24b67870":"code","7f555161":"markdown","33ed053c":"markdown","82f276d1":"markdown","034e476f":"markdown","02d8c611":"markdown","2f0716e0":"markdown","7c1473a8":"markdown","7a689f99":"markdown","cae65faf":"markdown","6b59d601":"markdown","4de42676":"markdown"},"source":{"5ea33278":"from IPython.display import Image\nimport os\n!ls '..\/input\/'","afea0cf1":"Image('..\/input\/Multilayerp.png')","798e14fa":"import numpy as np\n\ndef initialize_neural_network(n_inputs, n_hidden_layers, n_nodes_hidden, n_nodes_outputs):\n    \n    '''\n    Creating a function to initialize neural network.\n    n_inputs = Number of inputs\n    n_hidden_layers = Number of hidden layers\n    n_nodes_hidden = Number of nodes in each hidden layer\n    n_nodes_outputs = Number of outputs.\n    '''\n    \n    num_nodes_previous = n_inputs #Whatever outputs we get from previous layers will be the inputs for the next layers.\n    network = {} #Let's initialize a network with empty dictionary. This will be used later to display\/describe the whole network.\n    \n    #let's create a loop to randomly intitialize the weights and biases in each layer\n    #we're adding 1 to every hidden layer so as to include the output layer.\n    \n    for layer in range(n_hidden_layers + 1):\n        #let's define layer names.\n        if layer == n_hidden_layers:\n            layer_name = 'output' #name of our last layer\n            n_nodes = n_nodes_outputs\n        else:\n            layer_name = 'layer_{}'.format(layer + 1) #This will iterate the number of hidden layers until you reach output layer\n            n_nodes = n_nodes_hidden[layer]\n            \n        # let's intitialize weights and bias for each node\n        \n        network[layer_name] = {}\n        for node in range(n_nodes):\n            node_name = 'node_{}'.format(node+1) #define node names\n            network[layer_name][node_name] = {\n                'weights' : np.around(np.random.uniform(size=num_nodes_previous), decimals=2),\n                'bias' : np.around(np.random.uniform(size=1), decimals=2)\n            }\n            \n        num_nodes_previous = n_nodes\n            \n    return network","ac819ee1":"my_network = initialize_neural_network(4, 3, [3,2,3], 2) # 4 inputs, 3 hidden layers, 3 nodes in first hidden layer, 2 nodes in second hidden layer, 3 nodes in third hidden layer,\n                                                        # and finally 2 outputs\nmy_network","d908b080":"def summation(inputs, weights, bias):\n    return np.sum(inputs * weights) + bias","0191a0b3":"from random import seed\nnp.random.seed(5) #setting the seed to reproduce the same results whenever we run the code\n\ninputs = np.around(np.random.uniform(size=4),decimals=2)\n\nprint('The inputs to my_network are {}'.format(inputs))","73e83a99":"node_weights = my_network['layer_1']['node_1']['weights']\nnode_bias = my_network['layer_1']['node_1']['bias']\n\nweighted_sum = summation(inputs, node_weights, node_bias)\nprint('The weighted sum at the first node in the hidden layer is {}'.format(np.around(weighted_sum[0], decimals=4)))","bcf6a1ca":"def node_activation(weighted_sum):\n    return 1.0 \/ (1.0 + np.exp(-1 * weighted_sum))","ff491763":"node_output  = node_activation(summation(inputs, node_weights, node_bias))\nprint('The output of the first node in the hidden layer is {}'.format(np.around(node_output[0], decimals=4)))","9bb1d2aa":"def forward_propagate(network, inputs):\n    \n    layer_inputs = list(inputs) # 1. Start with the input layer as the input to the first hidden layer\n    \n    for layer in network:\n        \n        layer_data = network[layer]\n        \n        layer_outputs = [] \n        for layer_node in layer_data:\n        \n            node_data = layer_data[layer_node]\n        \n            # 2. & 3. Compute the weighted sum and the output of each node at the same time \n            node_output = node_activation(summation(layer_inputs, node_data['weights'], node_data['bias']))\n            layer_outputs.append(np.around(node_output[0], decimals=4))\n            \n        if layer != 'output':\n            print('The outputs of the nodes in hidden layer number {} is {}'.format(layer.split('_')[1], layer_outputs))\n    \n        layer_inputs = layer_outputs # 4. set the output of this layer to be the input to next layer\n\n    network_predictions = layer_outputs # 5. Move to the next layer in the network until you get an output from output layer\n    return network_predictions","24b67870":"final_network = initialize_neural_network(4, 3, [3, 2, 3], 4)\ninputs = np.around(np.random.uniform(size=4), decimals=2)\nprint('The input values for the neural network are {}'.format(inputs))\npredictions = forward_propagate(final_network, inputs)\nprint('The predicted values by the network for the given input are {}'.format(predictions))","7f555161":"# *Welcome to FIO Labs*\n\n## More Tutorials are available in Video format on our [YouTube Channel - FIOLabs](https:\/\/www.youtube.com\/channel\/UC6Vn_nUJeJ7PrvsayHPVOag). Follow for more AI | ML | DL Tutorials.","33ed053c":"Time to define *inputs* to *my_network*","82f276d1":"# Structure of Multi-Layer Perceptron\n\nLet us first look at the structure of Multi-Layer Perceptron to get a better understanding on how it works internally.","034e476f":"Now that we defined function to intialize the network, let's give some values to see what result we get when we call this function.","02d8c611":"# 4. Forward Propagation\n\nLet's create a function that applies the summation and node_activation functions to each node in the network and runs the data all the way to the output layer and outputs a prediction for each node in the output layer. Follow these steps in creating this forward propagation function.\n\n1. Start with the input layer as the input to the first hidden layer.\n2. Compute the weighted sum at the nodes of the current layer.\n3. Compute the output of the nodes of the current layer.\n4. Set the output of the current layer to be the input to the next layer.\n5. Move to the next layer in the network.\n6. Repeat steps 2 - 4 until we compute the output of the output layer.\n\nForward Propagation by definition means the inputs go from the first input layer all the way to the output layer without any loops.","2f0716e0":"As you can see, the structure gets more and more complex when number of hidden layers and nodes are increased in each layer. This is the reason why it is computationally challenging to run few neural networks.\n\nLet's go ahead and start coding.","7c1473a8":"# 2. Summation\n\nNow that we have all the randomly assigned weights at each node for every hidden layer, we can compute the summation to feed into an activation function. Let's do that!","7a689f99":"# 1. Initialize Neural Network\n\nFor a neural network, we need to have *number of inputs, number of hidden layers, number of nodes in each hidden layer, and number of outputs*. So, we need to define a function where we can just input these numbers and it would initialize the network with randomly assigned weights.","cae65faf":"# 5. Predictions\n\nThen, we compute the network predictions.\n\n1. Initialize the neural network and define it's weights and biases.\n2. Take random inputs for the given input size.\n3. Use the forward_propagate function to run the data all the way to output layer from input layer.\n4. Print predictions.","6b59d601":"# 3. Activation Function\n\nLet's use sigmoid function to activate nodes in the layers. This activation function yields output which is just non-linear transformation of the summation.","4de42676":"# Multi-Layer Perceptron\n\nOur last tutorial on [Shallow Neural Network](https:\/\/www.kaggle.com\/prashfio\/shallow-neural-network-ai-tutorials-fio-labs) points out a major drawback, i.e., we cannot extract more features from a given set of data when only one hidden layer is used. So, the solution is to apply **Multi-Layer Perceptron** to the problem when we have more high level features to extract. The reason is that for real-world problems, we need neural networks which can take *n* number of inputs, have *n* number of hidden layers and also *n* number of nodes in each hidden layer. The same applies to output layer. So, we will code accordingly. A multi-layer perceptron is also called as **Deep Neural Network**."}}