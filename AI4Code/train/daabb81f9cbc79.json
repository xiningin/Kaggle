{"cell_type":{"eb09a76a":"code","653dadc5":"code","1650e1a5":"code","5b21129f":"code","aaf272b5":"code","1a71b201":"code","212160ae":"code","a73cdb46":"code","83905080":"code","6fbaa467":"code","d94ac7b5":"code","4b5124fa":"code","fdd9588e":"code","9d915785":"code","8b18f73d":"code","afe93aff":"code","7d2528f5":"code","c1a6662b":"code","03309e73":"code","52f93851":"code","f6cde3f2":"code","f35cf030":"code","f9e25a98":"code","72822cfb":"code","4e60f802":"code","ef94c5b0":"code","331ea71e":"code","10dbf70c":"code","8821a488":"code","01e91297":"code","6ae18c32":"code","89c8be17":"code","28c604a0":"code","97606795":"markdown","4732ecb0":"markdown","2dd6fe76":"markdown","195f40b0":"markdown","b417e007":"markdown","e78b09cf":"markdown","88f4c2a1":"markdown","aa1b3356":"markdown","932d4fdf":"markdown","57eb1a1e":"markdown","04774d07":"markdown","904eae14":"markdown","a768e8c2":"markdown","f624fddf":"markdown","00e33ef9":"markdown","4d19f385":"markdown","e83db0f4":"markdown","3a084cec":"markdown","00eae416":"markdown","563cfdd2":"markdown"},"source":{"eb09a76a":"import pandas as pd ","653dadc5":"df = pd.read_csv('..\/input\/titanic\/train.csv')\ndf.head() ","1650e1a5":"df.isna().sum()","5b21129f":"train = df.dropna(subset = ['Embarked'])","aaf272b5":"train = train.drop('Cabin', axis = 1)","1a71b201":"train.isna().sum().sum()\/len(df)","212160ae":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ntest","a73cdb46":"test.isna().sum()","83905080":"median = train['Age'].median() \ntrain['Age'].fillna(median, inplace = True)","6fbaa467":"train.isna().sum().sum()","d94ac7b5":"train.head()","4b5124fa":"train = train.drop(['Name', 'Ticket', 'PassengerId'], axis = 1)","fdd9588e":"train = pd.get_dummies(train)\ntrain","9d915785":"X_train = train.drop(\"Survived\", axis = 1)\nY_train = train[\"Survived\"]","8b18f73d":"passid = test['PassengerId'] #we'll save this Series for our final csv file","afe93aff":"X_test = test.drop(['Name', 'Cabin', 'Ticket', 'PassengerId'], axis = 1)","7d2528f5":"median = X_test['Age'].median() \nX_test['Age'].fillna(median, inplace = True)","c1a6662b":"median = X_test['Fare'].median() \nX_test['Fare'].fillna(median, inplace = True)","03309e73":"X_test = pd.get_dummies(X_test)","52f93851":"X_test","f6cde3f2":"from sklearn.svm import SVC","f35cf030":"svc = SVC() ","f9e25a98":"from sklearn.preprocessing import MinMaxScaler #StandartScaler may be used as well \nscaling = MinMaxScaler(feature_range=(0,1)).fit(X_train)\nX_train = scaling.transform(X_train)\nX_test = scaling.transform(X_test)","72822cfb":"from sklearn.model_selection import GridSearchCV","4e60f802":"params = {'kernel' : ('rbf', 'poly', 'sigmoid'), 'C' : [0.0000001, 0.00001, 0.0001, 0.001, 0.01, 1, 10, 100], 'gamma' : [0.1, 1, 5]}\nclf = GridSearchCV(svc, params)","ef94c5b0":"from time import time\n\nstart = time()\n\nclf.fit(X_train, Y_train)\nprint(\"Search time:\", time() - start, \"seconds.\")","331ea71e":"clf.best_estimator_","10dbf70c":"Y_pred = clf.predict(X_test)\nY_pred","8821a488":"Y_pred = pd.Series(Y_pred, name = \"Survived\")\nY_pred ","01e91297":"result = passid.to_frame().join(Y_pred)","6ae18c32":"result.to_csv('result.csv', index = None)","89c8be17":"pd.read_csv('result.csv')","28c604a0":"df['Name']","97606795":"Sex and Embarked columns are categorical, so they'll be replaced with dummy variables ","4732ecb0":"Problem is not resolved yet. We can improve our result in many ways. For example, instead of filling Nan values of age with median we can use the titles of passengers to predict the age of different people. Ages of \"Mr\", \"Mrs\" or \"Miss\" can be predicted using the mean or median of each category. ","2dd6fe76":"There are 2 solutions: remove the age column entirely or replace missing values in test and train with medians. Since the age is very important, we'll replace missing values with median. ","195f40b0":"Finally, we can split our features and target values:","b417e007":"Age is missing from nearly 20 % of the entire data. There are 3 ways to deal with this problem: simply remove the entire column, remove only rows with nan values or replace nan values with median. Removing the entire column is a bad idea, because women and children were (or would've been) saved first, so the age is important. \n\nWe'll make our choice based on what the test data looks like: maybe age is missing there too and hence there is no point in using this column for training.","e78b09cf":"<b> Data dictionary: <\/b> \n\n<li> survival:\tSurvival\t0 = No, 1 = Yes\n<li> pclass:\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n<li> sex:\tSex\t\n<li> Age:\tAge in years\t\n<li> sibsp:\t# of siblings \/ spouses aboard the Titanic\t\n<li> parch:\t# of parents \/ children aboard the Titanic\t\n<li> ticket:\tTicket number\t\n<li> fare:\tPassenger fare\t\n<li> cabin:\tCabin number\t\n<li> embarked:\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton ","88f4c2a1":"We'll use GridSearchCV for optimizing the parameters. It'll work faster and more accurate, if we scale our data (otherwise training on this relatively small dataset may take hours). All the features are non-negative, so the (0,1) range is a way to go. ","aa1b3356":"Resulting file should contain passengerID and predicted target values","932d4fdf":"Data is ready, It's time to fit a model. We'll choose SVM for now. Logistic Regression, Decision Trees and other models may be added later. ","57eb1a1e":"There is not enough valid data for Cabin column hence we'll remove it entirely","04774d07":"If we try to optimize our parameters manually, the accuracy seems to be decreasing when the C increases. We'll try the grid search with smaller C values and some random gammas, and with different kernels as well. ","904eae14":"Passengers PassengerId, Name and Ticket number seem to have no influence whether or not they survived, so we'll drop them","a768e8c2":"# Titanic competition","f624fddf":"Additionally, other models and\/or parameters may be used ","00e33ef9":"# Predictions for all 418 passengers should be submitted, hence we can't remove any rows from the test dataframe","4d19f385":"We'll do the same for the test data later\n\n<p> For now, we'll continue tackling the train dataset","e83db0f4":"First check the dataset for nan values:","3a084cec":"## Now do the same for the test data: \n\n<p> \n\n<li> Remove Name, Cabin and Ticket columns\n<li> Replace nan values in Age and Fare columns with medians\n<li> Get dummy variables for categorical features","00eae416":"There are only 2 nan values in Embarked column, so we can safely drop them ","563cfdd2":"Given information about passengers, the model should predict whether or not they survived the Sinking of the Titanic. Data is splitted: train.csv contains actual information about their survival (0 for no and 1 for yes), and test.cvs does not. \n\nFor more information about competition visit https:\/\/www.kaggle.com\/c\/titanic\/overview "}}