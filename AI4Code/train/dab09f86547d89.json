{"cell_type":{"61a8fac3":"code","2a10c44b":"code","a778eba5":"code","7d7b1204":"code","575c25e3":"code","b14c7107":"code","fdca39f4":"code","699f7f92":"code","e80a4efa":"code","84bb6e59":"code","85715f0f":"code","d1c9c055":"code","eb1a5a6d":"code","1edbd6af":"code","93c262f2":"code","612e8bbd":"code","4406dd5e":"code","802b520b":"markdown","a2d3ac04":"markdown","325d2ec9":"markdown","bbbcf907":"markdown","19f47a48":"markdown","32605273":"markdown"},"source":{"61a8fac3":"# importing modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2a10c44b":"# importing data\ndf = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndf.describe()\nplt.scatter(df.age[df.target == 1], df.thalach[df.target ==1], c =\"salmon\");\n\nplt.scatter(df.age[df.target == 0], df.thalach[df.target == 0], c= \"lightblue\");\n\nplt.title( \"Heart disease age and max heart rate co-relation\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Heart rate\");\nplt.legend([1,0]);\n","a778eba5":"corr =df.corr()\nfig, ax =plt.subplots(figsize =(16,8))\nax  = sns.heatmap(corr,annot =True,fmt = \".2f\", cmap = 'gray_r');","7d7b1204":"# splitting data\nX = df.drop(\"target\", axis =1)\nY = df[\"target\"]\nX.corrwith(df.target).plot(kind='bar',\n                           grid=True, \n                           figsize=(12, 8),\n                           title=\"Correlation with target\");\n","575c25e3":"from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nmodels = {\"log\": LogisticRegression(max_iter = 1001), \n          \"knn\": KNeighborsClassifier(), \n          \"rfc\": RandomForestClassifier()}\n\ndef fit_and_score(models, x_train, x_test, y_train, y_test):\n    \n    np.random.seed(17)\n    model_scores = {}\n    \n    for name, model in models.items():\n        model.fit(x_train, y_train)\n        \n        model_scores[name] = model.score(x_test, y_test)\n\n    return model_scores","b14c7107":"model_scores = fit_and_score(models, x_train, x_test, y_train, y_test)\n\nmodel_scores","fdca39f4":"model_comp = pd.DataFrame(model_scores, index = [\"accuracy\"])\nmodel_comp.T.plot.bar();","699f7f92":"train_scores = []\ntest_scores = []\nneighbors = range(1,21)\n\n\nknn = KNeighborsClassifier()\n\nfor i in neighbors:\n    knn.set_params(n_neighbors = i)\n    knn.fit(x_train, y_train)\n    train_scores.append(knn.score(x_train, y_train))\n        \n    test_scores.append(knn.score(x_test, y_test))\n\nplt.plot(neighbors, train_scores, label = \"train scores\")\nplt.plot(neighbors, test_scores, label=\"test scores\")\nplt.xlabel(\"no. of neighbors\")\nplt.ylabel(\"model_score\")\nplt.legend();\n\nprint(f\"maximum accuracy: {max(test_scores)*100:.2f}%\");","e80a4efa":"LR_grid = {\"C\": np.logspace(-4, 4, 20),\n           \"solver\": [\"liblinear\"]}\n\nRF_grid = {\"n_estimators\" : [1, 200,500,1000],\n          \"max_depth\": [None, 3, 5, 10],\n          \"min_samples_split\": np.arange(2, 20, 2),\n          \"min_samples_leaf\": np.arange(1, 20, 2)}\n\nnp.random.seed(17)\nLR = RandomizedSearchCV(LogisticRegression(),\n                           param_distributions = LR_grid,\n                           cv =5,\n                           n_iter = 10,\n                           verbose =True)\n\nRF = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions = RF_grid,\n                           cv =5,\n                           n_iter = 10,\n                           verbose = True)\n\nLR.fit(x_train, y_train).best_params_, RF.fit(x_train, y_train).best_params_","84bb6e59":"LR.score(x_test, y_test), RF.score(x_test, y_test)","85715f0f":"LGCV = {\"C\": np.logspace(-4, 4, 30),\n           \"solver\": [\"liblinear\"]}\n\nRFGCV = {\"n_estimators\" : [1000,1200],\n          \"max_depth\": [10, 12],\n          \"min_samples_split\": [6, 12],\n          \"min_samples_leaf\": [20, 22]}\n\n\nLRGS = GridSearchCV(LogisticRegression(),param_grid = LGCV, cv = 5, verbose =True)\n\nRFGS = GridSearchCV(RandomForestClassifier(), param_grid = RFGCV, cv =5, verbose = True)\n\nLRGS.fit(x_train,y_train).best_params_, RFGS.fit(x_train, y_train).best_params_\n\n","d1c9c055":"LRGS.score(x_test, y_test), RFGS.score(x_test, y_test)","eb1a5a6d":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, plot_roc_curve\nfrom sklearn.metrics import confusion_matrix, classification_report\ny_pred = LRGS.predict(x_test)\n\ny_pred","1edbd6af":"plot_roc_curve(LRGS, x_test, y_test);\nplot_roc_curve(RFGS, x_test, y_test);","93c262f2":"sns.set(font_scale = 1.5)\ndef plot_conf_mat(y_test,y_pred):\n    fig, ax = plt.subplots(figsize= (5,5))\n    ax = sns.heatmap(confusion_matrix(y_test, y_pred),\n                    annot = True,\n                    cbar = False)\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predictedlabel\")\n    \nplot_conf_mat(y_test, y_pred)\n","612e8bbd":"print(classification_report(y_test, y_pred))","4406dd5e":"est = LogisticRegression(solver =\"liblinear\", C = 0.23357214690901212)\n\n\ncvs_p = cross_val_score(est, X, Y, cv = 10, scoring = \"precision\")\ncvs_p = np.mean(cvs_p)\ncvs_p\n\n\n","802b520b":"## Different metrics","a2d3ac04":"## EDA","325d2ec9":"**The recall_score can be increased with more data. so that there are no false negetives i.e: there are no missing medical condition who has heart_disease**","bbbcf907":"## **using RandmoizedCV to find better parameters**","19f47a48":"## checking coorelation with features and labels","32605273":"## GridsearchCV"}}