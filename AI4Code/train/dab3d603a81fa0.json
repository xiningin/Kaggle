{"cell_type":{"cea9594a":"code","9c91f7b1":"code","03a825ac":"code","26a5d8f8":"code","e5186a23":"code","7bc12a45":"code","8bef897e":"code","d9843139":"code","908e78c4":"code","e7dee42d":"code","3afcab8d":"code","d54c21b0":"code","52912f3e":"code","39bf833f":"code","f5fa9dd3":"code","04726048":"code","c5d70396":"code","70226c90":"code","a3cfddc8":"code","7d4ca07c":"code","189fe33a":"code","636d8226":"code","2ebcac83":"code","347060bb":"code","ec1c1614":"code","50f13b8b":"code","4451adbf":"code","5545b910":"code","d9be5a3b":"code","e27428f1":"code","689b1866":"code","dfecccab":"code","91e70b7f":"code","e0581a70":"code","54d5e256":"code","d837f77f":"code","26f9741e":"code","2343a27e":"code","c4829b8a":"code","148aef7d":"code","c54964ac":"code","f7f320a2":"code","fbd7dcaf":"code","a80927d0":"code","eca348ce":"code","8c753b2f":"code","1038d467":"code","a6d6c3a1":"code","6189c54b":"code","d0fd3100":"code","ff5f7c0f":"code","90d4a1ae":"code","d12d77d5":"code","b51b4463":"code","ac765c48":"code","0ba5d693":"code","38b4f570":"code","6c8a0ffe":"code","a938485f":"code","9896e559":"code","b8e7c07b":"code","765e10c4":"code","c4e51718":"code","65b0a311":"code","a216054f":"code","c8bad3c3":"code","8829bf00":"code","e73c79fc":"code","05fd3be1":"code","b9d928b4":"code","983ef667":"code","3052f08e":"code","5d8f2914":"code","b376072f":"code","70083fc4":"code","6d3aec9a":"code","9d936622":"code","f9e57394":"code","8f5f8c1a":"code","bd0cd138":"code","4df0d710":"code","cfcdcdea":"code","28dca07e":"code","d427ec9f":"code","85254a2a":"code","4e912f73":"code","5b6e08b4":"code","897f636b":"code","0f3bbd4c":"code","53c66e87":"code","1b142d6b":"code","00c9c485":"code","bd27a05e":"code","3ad71a2a":"code","75806943":"code","566e26fd":"code","05a6a988":"code","73d4f071":"code","941cdb23":"code","359b5ff7":"code","909384b4":"code","76df228e":"code","7a1190ca":"code","76c5c0b3":"code","ebe76455":"code","5eeae678":"code","7706473c":"code","ed1f4019":"code","9282eb77":"code","df2cca69":"code","bb05b7c7":"code","9eb752ec":"code","27dae489":"code","91af092a":"code","850f668f":"code","c062fb17":"code","84d52cbe":"code","17a981bb":"code","595c1689":"code","7f8578bd":"code","4bc8cab9":"code","0bec279c":"code","76d4c0a3":"code","e69a10dd":"code","20edb9fa":"code","d6a5910e":"code","728a61c3":"code","286a0f8d":"code","1714ca9b":"code","840d939d":"code","e5c7e212":"code","42352f94":"code","1863c59e":"code","98c006fc":"code","c1e0e89e":"code","54082685":"code","d6c9cdf2":"code","d0fa2a77":"code","277c7906":"code","12fea36d":"code","3483add5":"code","0f8ac905":"code","5f44824b":"code","8df6d15c":"code","d41caf1d":"markdown","a763c9ee":"markdown","45035e39":"markdown","a8248ab7":"markdown","cd395459":"markdown","3063d21a":"markdown","6ee6abe7":"markdown","bfd83791":"markdown","b16fd935":"markdown","c753c695":"markdown","eda089aa":"markdown","95528d52":"markdown","06d64aaa":"markdown","2d45ba89":"markdown","a78bd930":"markdown","8094fdbc":"markdown","fc5d4074":"markdown","aa55d24c":"markdown","1b542a38":"markdown","c7d25167":"markdown","e9212046":"markdown","8ab1979f":"markdown"},"source":{"cea9594a":"import warnings\nwarnings.filterwarnings('ignore')","9c91f7b1":"import pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","03a825ac":"#lead_df = pd.read_csv(\"Leads.csv\")\nlead_df = pd.read_csv(\"..\/input\/lead-scoring-dataset\/Lead Scoring.csv\")\nlead_df.head()","26a5d8f8":"lead_df.shape","e5186a23":"lead_df.describe()","7bc12a45":"lead_df.info()","8bef897e":"\nlead_df.drop(['Prospect ID', 'Lead Number'], 1, inplace = True)\n\n#Converting 'Select' values to NaN.\n\nlead_df = lead_df.replace('Select', np.nan)\n","d9843139":"lead_df.isnull().sum()","908e78c4":"\n\n#checking percentage of null values in each column\n\nround(100*(lead_df.isnull().sum()\/len(lead_df.index)), 2)\n","e7dee42d":"cols = lead_df.columns\n\nfor i in cols:\n    if((100*(lead_df[i].isnull().sum()\/len(lead_df.index))) >= 40):\n        lead_df.drop(i, 1, inplace = True)\n\nlead_df.drop('Tags', 1, inplace = True)","3afcab8d":"lead_df['City'].value_counts(dropna=False)\n","d54c21b0":"lead_df['City'] = lead_df['City'].replace(np.nan,'Mumbai')","52912f3e":"#plotting spread of City columnn after replacing NaN values\n\nplt.figure(figsize=(10,5))\ns1=sns.countplot(lead_df.City, hue=lead_df.Converted)\ns1.set_xticklabels(s1.get_xticklabels(),rotation=90)\nplt.show()","39bf833f":"len(lead_df[lead_df['City'] == 'Mumbai'])\/lead_df.shape[0]","f5fa9dd3":"lead_df['Country'].value_counts(dropna = False)","04726048":"lead_df['Country'] = lead_df['Country'].replace(np.nan,'India')\n# Replacing NaN with mode of all the countries -> India\n\nlead_df['Country'].value_counts()","c5d70396":"len(lead_df[lead_df['Country'] == 'India'])\/lead_df.shape[0]\n\n#97% percentage of india, this will be skewed towards India, so this is better removed from the dataset","70226c90":"#plotting spread of Country columnn \nplt.figure(figsize=(15,5))\ns1=sns.countplot(lead_df.Country, hue=lead_df.Converted)\ns1.set_xticklabels(s1.get_xticklabels(),rotation=90)\nplt.show()","a3cfddc8":"lead_df.drop('Country', 1, inplace = True)","7d4ca07c":"lead_df['What matters most to you in choosing a course'].value_counts(dropna=False)","189fe33a":"#replacing Nan values with Mode \"Better Career Prospects\"\n\nlead_df['What matters most to you in choosing a course'] = lead_df['What matters most to you in choosing a course'].replace(np.nan,'Better Career Prospects')","636d8226":"plt.figure(figsize=(15,5))\ns1=sns.countplot(lead_df['What matters most to you in choosing a course'], hue=lead_df.Converted)\ns1.set_xticklabels(s1.get_xticklabels(),rotation=90)\nplt.show()","2ebcac83":"\n\n#checking value counts of variable\nlead_df['What matters most to you in choosing a course'].value_counts(dropna=False)\n\n","347060bb":"len(lead_df[lead_df['What matters most to you in choosing a course'] == 'Better Career Prospects'])\/lead_df.shape[0]","ec1c1614":"lead_df.drop('What matters most to you in choosing a course', 1, inplace = True)","50f13b8b":"lead_df['What is your current occupation'].value_counts(dropna=False)","4451adbf":"#imputing Nan values with mode \"Unemployed\"\n\nlead_df['What is your current occupation'] = lead_df['What is your current occupation'].replace(np.nan, 'Unemployed')","5545b910":"lead_df['What is your current occupation'].value_counts(dropna=False)","d9be5a3b":"plt.figure(figsize=(15,5))\ns1=sns.countplot(lead_df['What is your current occupation'], hue=lead_df.Converted)\ns1.set_xticklabels(s1.get_xticklabels(),rotation=90)\nplt.show()","e27428f1":"lead_df.shape","689b1866":"lead_df['Specialization'] = lead_df['Specialization'].replace(np.nan, 'Not Specified')","dfecccab":"#checking percentage of missing values\nround(100*(lead_df.isnull().sum()\/(lead_df.shape[0])), 2)","91e70b7f":"lead_df[\"Last Activity\"].value_counts()","e0581a70":"\n\n#Drop all rows which have Nan Values. Since the number of Dropped rows is less than 2%,  will not affect the model\nlead_df = lead_df.dropna()\n\n","54d5e256":"#checking percentage of missing values\nround(100*(lead_df.isnull().sum()\/(lead_df.shape[0])), 2)","d837f77f":"lead_df.shape\n","26f9741e":"lead_df['Do Not Call'].value_counts(dropna=False)","2343a27e":"lead_df.drop('Do Not Call', 1, inplace = True)","c4829b8a":"lead_df.Search.value_counts(dropna=False)","148aef7d":"lead_df['X Education Forums'].value_counts(dropna=False)","c54964ac":"lead_df.Magazine.value_counts(dropna=False)","f7f320a2":"lead_df['Newspaper Article'].value_counts(dropna=False)","fbd7dcaf":"lead_df['Through Recommendations'].value_counts(dropna=False)","a80927d0":"lead_df.Newspaper.value_counts(dropna=False)","eca348ce":"lead_df['Digital Advertisement'].value_counts(dropna=False)","8c753b2f":"lead_df['Receive More Updates About Our Courses'].value_counts(dropna=False)","1038d467":"lead_df['Update me on Supply Chain Content'].value_counts(dropna=False)","a6d6c3a1":"lead_df['I agree to pay the amount through cheque'].value_counts(dropna=False)","6189c54b":"lead_df['Get updates on DM Content'].value_counts(dropna=False)","d0fd3100":"lead_df.info()","ff5f7c0f":"#adding imbalanced columns to the list of columns to be dropped\n\ncols_drop = []\n\ncols_drop.extend(['Search','Magazine','Newspaper Article','X Education Forums','Newspaper','Digital Advertisement','Through Recommendations','Receive More Updates About Our Courses','Update me on Supply Chain Content','Get updates on DM Content','I agree to pay the amount through cheque'])\ncols_drop","90d4a1ae":"lead_df = lead_df.drop(cols_drop,1)\nlead_df.shape","d12d77d5":"lead_df","b51b4463":"plt.figure(figsize=(10,8))\n\n# heatmap\nsns.heatmap(lead_df.corr(), cmap=\"YlGnBu\", annot=True)\nplt.show()","ac765c48":"#boxplot of Total Visits vs Converted variable\nsns.boxplot(y = 'TotalVisits', x = 'Converted', data = lead_df)\nplt.show()","0ba5d693":"#boxplot of Total Time Spent on Website vs Converted variable\n\nsns.boxplot(x=lead_df.Converted, y=lead_df['Total Time Spent on Website'])\nplt.show()","38b4f570":"#boxplot of Page Views Per Visit vs Converted variable\n\nsns.boxplot(x=lead_df.Converted,y=lead_df['Page Views Per Visit'])\nplt.show()","6c8a0ffe":"# List of variables to map\n\nvarlist =  ['A free copy of Mastering The Interview','Do Not Email']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\n# Applying the function to the housing list\nlead_df[varlist] = lead_df[varlist].apply(binary_map)","a938485f":"dummy = pd.get_dummies(lead_df['Specialization'], prefix  = 'Specialization')\ndummy = dummy.drop(['Specialization_Not Specified'], 1)\nlead_df = pd.concat([lead_df, dummy], axis = 1)","9896e559":"dummy1 = pd.get_dummies(lead_df['Lead Source'], prefix  = 'Lead Source', drop_first=True)\nlead_df = pd.concat([lead_df, dummy1], axis = 1)","b8e7c07b":"dummy2 = pd.get_dummies(lead_df['Last Activity'], prefix  = 'Last Activity', drop_first=True)\nlead_df = pd.concat([lead_df, dummy2], axis = 1)","765e10c4":"dummy3 = pd.get_dummies(lead_df['Last Notable Activity'], prefix  = 'Last Notable Activity', drop_first=True)\nlead_df = pd.concat([lead_df, dummy3], axis = 1)","c4e51718":"dummy4 = pd.get_dummies(lead_df[['Lead Origin','What is your current occupation',\n                             'City']], drop_first=True)\n\nlead_df = pd.concat([lead_df,dummy4],1)","65b0a311":"lead_df.info()","a216054f":"\ncategorical_cols= (['Specialization','Lead Source', 'Last Activity', 'Last Notable Activity', 'Lead Origin','What is your current occupation',\n                             'City'])\ncategorical_cols","c8bad3c3":"\nlead_df.drop(categorical_cols,1,inplace = True)","8829bf00":"# this column needs to be droped we duplicate column for same purpose \nlead_df.drop('Lead Source_google', 1, inplace = True)","e73c79fc":"# this column needs to be droped we duplicate column for same purpose \nlead_df.drop('Last Notable Activity_Had a Phone Conversation', 1, inplace = True)","05fd3be1":"# this column needs to be droped we duplicate column for same purpose \nlead_df.drop('Last Notable Activity_Olark Chat Conversation', 1, inplace = True)","b9d928b4":"from sklearn.model_selection import train_test_split\n\n# Putting response variable to y\ny = lead_df['Converted']\n\ny.head()\n\nX=lead_df.drop('Converted', axis=1)","983ef667":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=50)","3052f08e":"#scaling numeric columns\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nnum_cols=X_train.select_dtypes(include=['float64', 'int64']).columns\n\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n\nX_train.head()","5d8f2914":"lead_st = (sum(lead_df['Converted'])\/len(lead_df['Converted'].index))*100\nlead_st","b376072f":"import statsmodels.api as sm\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(solver='liblinear')\n\n# solver = liblinear solves the error - str has no attribute decode\n#https:\/\/stackoverflow.com\/questions\/65682019\/attributeerror-str-object-has-no-attribute-decode-in-fitting-logistic-regre\n\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 15)             # running RFE with 15 variables as output\nrfe = rfe.fit(X_train, y_train)","70083fc4":"rfe.support_","6d3aec9a":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","9d936622":"#list of RFE supported columns\ncol = X_train.columns[rfe.support_]\ncol","f9e57394":"X_train.columns[~rfe.support_]","8f5f8c1a":"#model building through GLM of statsmodel\n\nX_train_sm = sm.add_constant(X_train[col])\nlogm1 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm1.fit()\nres.summary()","bd0cd138":"y_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","4df0d710":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","cfcdcdea":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_Prob':y_train_pred})\ny_train_pred_final['Prospect ID'] = y_train.index\ny_train_pred_final.head()","28dca07e":"y_train_pred_final['predicted'] = y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","d427ec9f":"from sklearn import metrics","85254a2a":"# ConfusionMatrix\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nprint(confusion)","4e912f73":"print(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","5b6e08b4":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","897f636b":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","0f3bbd4c":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","53c66e87":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","1b142d6b":"# Let us calculate specificity\nTN \/ float(TN+FP)","00c9c485":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","bd27a05e":"# positive predictive value \nprint (TP \/ float(TP+FP))","3ad71a2a":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","75806943":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","566e26fd":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Converted_Prob, drop_intermediate = False )","05a6a988":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)","73d4f071":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","941cdb23":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","359b5ff7":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","909384b4":"y_train_pred_final['final_predicted'] = y_train_pred_final.Converted_Prob.map( lambda x: 1 if x > 0.34 else 0)\n\ny_train_pred_final.head()","76df228e":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","7a1190ca":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion2","76c5c0b3":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","ebe76455":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","5eeae678":"# Let us calculate specificity\nTN \/ float(TN+FP)","7706473c":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","ed1f4019":"# Positive predictive value \nprint (TP \/ float(TP+FP))","9282eb77":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","df2cca69":"#Looking at the confusion matrix again\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nconfusion","bb05b7c7":"confusion[1,1]\/(confusion[0,1]+confusion[1,1])","9eb752ec":"confusion[1,1]\/(confusion[1,0]+confusion[1,1])","27dae489":"from sklearn.metrics import precision_score, recall_score","91af092a":"precision_score(y_train_pred_final.Converted, y_train_pred_final.predicted)","850f668f":"recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted)","c062fb17":"from sklearn.metrics import precision_recall_curve","84d52cbe":"y_train_pred_final.Converted, y_train_pred_final.predicted","17a981bb":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)","595c1689":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","7f8578bd":"num_cols1=X_test.select_dtypes(include=['float64', 'int64']).columns\nX_test[num_cols1] = scaler.transform(X_test[num_cols1])","4bc8cab9":"X_test = X_test[col]\nX_test.head()","0bec279c":"X_test_sm = sm.add_constant(X_test)","76d4c0a3":"y_test_pred = res.predict(X_test_sm)","e69a10dd":"y_test_pred[:10]","20edb9fa":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","d6a5910e":"# Let's see the head\ny_pred_1.head()","728a61c3":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","286a0f8d":"# Putting CustID to index\ny_test_df['Prospect ID'] = y_test_df.index","1714ca9b":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","840d939d":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","e5c7e212":"y_pred_final.head()","42352f94":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_Prob'})","1863c59e":"# Rearranging the columns\n#y_pred_final = y_pred_final.reindex_axis(['Prospect ID','Converted','Converted_Prob'], axis=1)","98c006fc":"# Let's see the head of y_pred_final\ny_pred_final.head()","c1e0e89e":"y_pred_final['final_predicted'] = y_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.40 else 0)","54082685":"y_pred_final.head()","d6c9cdf2":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted)","d0fa2a77":"confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\nconfusion2","277c7906":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","12fea36d":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","3483add5":"# Let us calculate specificity\nTN \/ float(TN+FP)","0f8ac905":"Converted = (sum(lead_df['Converted'])\/len(lead_df['Converted'].index))*100\nConverted","5f44824b":"Converted_pred = (sum(y_pred_final['final_predicted'])\/len(y_pred_final['final_predicted'].index))*100\nConverted_pred","8df6d15c":"y_pred_final.drop('Converted',1 , inplace = True)\ny_pred_final['Lead_Score'] = y_pred_final.Converted_Prob.map(lambda x: round(x*100, 2))\ny_pred_final.set_index('Prospect ID')\n\n","d41caf1d":"## Model Building and RFE","a763c9ee":"### Recall","45035e39":"### Precision And Recall Tradeoff","a8248ab7":"### Creating new column 'predicted' with 1 if Converted_Prob > 0.5 else 0","cd395459":"The columns having imbalance in the dataset","3063d21a":"### Dummy variable creation","6ee6abe7":"### Precision and Recall","bfd83791":"## Standard Scaler","b16fd935":"### Numerical variables analysis","c753c695":"We can drop the **Do Not Call** Column since > 90% is of only value 'No'","eda089aa":"## Test-train split ","95528d52":"### Finding Optimal Cutoff Point","06d64aaa":"We have almost 37.8% conversion rate.","2d45ba89":"### Creating a dataframe with actual converted flag and the predicted probability","a78bd930":"### Step 3: Data Preparation\n","8094fdbc":"## Lead Score\n\n### Objective \n- To help the X education select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The requirement is to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. \n- Build a logistic regression model such that we are able to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.\n\n\n### Method Implemented\n- We have been provided with a leads dataset from the past with around 9000 data points. This dataset consists of various attributes such as Lead Source, Total Time Spent on Website, Total Visits, Last Activity, etc. which may or may not be useful in ultimately deciding whether a lead will be converted or not.\n- As a first step, exploratory data analysis is conducted to figure out the trends among the dataset, and to find the outliers on the data. Columns with high null values are discarded and columns with less percentage are imputed to suit the model creation.\n- Out of all the variables created, Recursive Feature Elimination (RFE) is done to reduce the feature set to 15. Checking Variance Inflation Factor (VIF) we find that none of the left-over columns for analysis have a high value, thus are not correlated to each other.\n-  Logistic regression model is then created and checked for the p-value (using stats model's General linear model) to check if any more columns have a high value of p\n- Receiver operating characteristic (ROC) curve, Accuracy-Sensitivity-Specificity graph, and Precision- Recall plots are created. The optimal value is found to be 0.34 and 0.4 from the latter two graphs.\n- If the probability of conversion is greater than 0.4 (cutoff value), it is considered that the person is a suitable lead for conversion into a possible client to buying the program.\n- Lead score is calculated by expressing the conversion probability as a percentage.","fc5d4074":"# Assessing the model with StatsModels","aa55d24c":"### Metrics beyond simply accuracy","1b542a38":"### From the curve above, 0.3 is the optimum point to take it as a cutoff probability.","c7d25167":"### Checking VIF's","e9212046":"### Precision","8ab1979f":"## Plotting ROC Curve"}}