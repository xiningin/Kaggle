{"cell_type":{"65ad08e7":"code","83fb271b":"code","e167d129":"code","aa0bab6b":"code","1554319c":"code","f3a9c83f":"code","3903c399":"code","12e70051":"code","89c84584":"code","88374b4d":"code","a5c3c244":"code","d9eed822":"code","8da232ce":"code","d7afa5ff":"code","b8815842":"code","0ef05961":"code","36d42abc":"code","f6feb6c2":"code","eaf9e7d9":"code","e7c8e9ce":"code","23f3effc":"code","64041077":"code","eb3943e3":"code","101db77f":"code","791304ba":"code","73741f0c":"code","666075ef":"code","b2a3179c":"code","9c902b68":"code","49658e64":"code","79382ea3":"code","55f0c8b9":"code","0199664e":"code","b4a21a46":"code","fbd7dc5c":"code","11fa9ee8":"code","60a12270":"code","4e032853":"code","9a8142e2":"code","401939d9":"code","4ab1dcb7":"code","7cf6a132":"code","dc7ffc5b":"code","0415b906":"code","8d2550b5":"code","8a0d7e7b":"code","719fe803":"markdown","55e79086":"markdown","72397347":"markdown","e7f74087":"markdown","d815fa94":"markdown","4e15bc4d":"markdown","ff45eb0a":"markdown","90cbdcb4":"markdown","6214429f":"markdown","56268997":"markdown","aae724dd":"markdown","67a25191":"markdown","2ffe4ac6":"markdown","cb75e58f":"markdown","50f370ff":"markdown","83fee3d9":"markdown","269bac88":"markdown","4ffe2230":"markdown","21a56b70":"markdown"},"source":{"65ad08e7":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport re\n%matplotlib inline\n\n!pip install nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import PorterStemmer\n\n!pip install wordcloud\nfrom wordcloud import WordCloud\n\n!pip install tweet-preprocessor\nimport preprocessor as p\n\nfrom gensim.models import KeyedVectors\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report","83fb271b":"data_dir = \"..\/input\"","e167d129":"!ls {data_dir}","aa0bab6b":"encoding = 'ISO-8859-1'\ncol_names = ['target', 'id', 'date', 'flag', 'user', 'text']\n\ndataset = pd.read_csv(os.path.join(data_dir, 'sentiment140\/training.1600000.processed.noemoticon.csv'), encoding=encoding, names=col_names)","1554319c":"dataset.head()","f3a9c83f":"df = dataset.copy().sample(8000, random_state=42)\ndf[\"label\"] = 0\ndf = df[['text', 'label']]\ndf.dropna(inplace=True)\ndf.head()","3903c399":"col_names = ['id', 'text']\ndf2 = pd.read_csv(os.path.join(data_dir, 'd\/dongphilyoo\/depressive-tweets-processed\/depressive_tweets_processed.csv'), sep = '|', header = None, usecols = [0,5], nrows = 3200, names=col_names)","12e70051":"df2.info()","89c84584":"# add `label` colum with value 1's\ndf2['label'] = 1\ndf2 = df2[['text', 'label']]","88374b4d":"df = pd.concat([df,df2]) # merge the dataset on normal tweets and depressive tweets\ndf = df.sample(frac=1)  # shuffle the dataset","a5c3c244":"df.info()","d9eed822":"contractions = pd.read_json(os.path.join(data_dir, 'english-contractions\/contractions.json'), typ='series')\ncontractions = contractions.to_dict()","8da232ce":"c_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return contractions[match.group(0)]\n    return c_re.sub(replace, text)","d7afa5ff":"BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n\ndef clean_tweets(tweets):\n    cleaned_tweets = []\n    for tweet in tweets:\n        tweet = str(tweet)\n        tweet = tweet.lower()\n        tweet = BAD_SYMBOLS_RE.sub(' ', tweet)\n        tweet = p.clean(tweet)\n        \n        #expand contraction\n        tweet = expandContractions(tweet)\n\n        #remove punctuation\n        tweet = ' '.join(re.sub(\"([^0-9A-Za-z \\t])\", \" \", tweet).split())\n\n        #stop words\n        stop_words = set(stopwords.words('english'))\n        word_tokens = nltk.word_tokenize(tweet) \n        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n        tweet = ' '.join(filtered_sentence)\n        \n        cleaned_tweets.append(tweet)\n        \n    return cleaned_tweets","b8815842":"X = clean_tweets([tweet for tweet in df['text']])","0ef05961":"depressive_tweets = [clean_tweets([t for t in df2['text']])]\ndepressive_words = ' '.join(list(map(str, depressive_tweets)))\ndepressive_wc = WordCloud(width = 512,height = 512, collocations=False, colormap=\"Blues\").generate(depressive_words)","36d42abc":"plt.figure(figsize = (10, 8), facecolor = 'k')\nplt.imshow(depressive_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","f6feb6c2":"MAX_NUM_WORDS = 10000\ntokenizer= Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(X)","eaf9e7d9":"word_vector = tokenizer.texts_to_sequences(X)","e7c8e9ce":"word_index = tokenizer.word_index","23f3effc":"vocab_size = len(word_index)\nvocab_size   # num of unique tokens","64041077":"MAX_SEQ_LENGTH = 140\ninput_tensor = pad_sequences(word_vector, maxlen=MAX_SEQ_LENGTH)","eb3943e3":"input_tensor.shape","101db77f":"corpus = df['text'].values.astype('U')\ntfidf = TfidfVectorizer(max_features = MAX_NUM_WORDS) \ntdidf_tensor = tfidf.fit_transform(corpus)","791304ba":"tdidf_tensor.shape","73741f0c":"# Split data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(tdidf_tensor, df['label'].values, test_size=0.3)","666075ef":"baseline_model = SVC()\nbaseline_model.fit(x_train, y_train)","b2a3179c":"predictions = baseline_model.predict(x_test)","9c902b68":"accuracy_score(y_test, predictions)","49658e64":"print(classification_report(y_test, predictions, digits=5))","79382ea3":"EMBEDDING_FILE = os.path.join(data_dir, 'googles-trained-word2vec-model-in-python\/GoogleNews-vectors-negative300.bin.gz')\nword2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)","55f0c8b9":"EMBEDDING_DIM = 300\nembedding_matrix = np.zeros((MAX_NUM_WORDS, EMBEDDING_DIM))","0199664e":"for (word, idx) in word_index.items():\n    if word in word2vec.vocab and idx < MAX_NUM_WORDS:\n        embedding_matrix[idx] = word2vec.word_vec(word)","b4a21a46":"inp = Input(shape=(MAX_SEQ_LENGTH,))\nx = Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, weights=[embedding_matrix])(inp)\nx = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(100, activation=\"relu\")(x)\nx = Dropout(0.25)(x)\nx = Dense(1, activation=\"sigmoid\")(x)","fbd7dc5c":"# Compile the model\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","11fa9ee8":"# Split data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(input_tensor, df['label'].values, test_size=0.3)","60a12270":"model.fit(x_train, y_train, batch_size=16, epochs=10)","4e032853":"preds = model.predict(x_test)","9a8142e2":"preds  = np.round(preds.flatten())\nprint(classification_report(y_test, preds, digits=5))","401939d9":"x_train, x_test, y_train, y_test = train_test_split(X, df.label, test_size=0.3, random_state = 42)","4ab1dcb7":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n\nnb = Pipeline([('vect', CountVectorizer()),\n               ('tfidf', TfidfTransformer()),\n               ('clf', MultinomialNB()),\n              ])\nnb.fit(x_train, y_train)","7cf6a132":"y_pred = nb.predict(x_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred, digits=5))","dc7ffc5b":"from sklearn.linear_model import SGDClassifier\n\nsgd = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n               ])\nsgd.fit(x_train, y_train)","0415b906":"y_pred = sgd.predict(x_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred, digits=5))","8d2550b5":"from sklearn.linear_model import LogisticRegression\n\nlogreg = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n               ])\nlogreg.fit(x_train, y_train)","8a0d7e7b":"y_pred = logreg.predict(x_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred, digits=5))","719fe803":"### Training","55e79086":"## LTSM model\n\nLet's improve our model with LTSM. ","72397347":"## Linear Support Vector","e7f74087":"### Training","d815fa94":"### Preprocessing","4e15bc4d":"# Introduction\nOriginal notebook credits to @HIEN NGUYEN<br\/>The goal of this study is to investigate how Twitter, a popular social media platform, can be leveraged in detecting early risk of depression of its users. The study is inspired by [this article](https:\/\/time.com\/1915\/how-twitter-knows-when-youre-depressed\/).","ff45eb0a":"## Classification of depressive and normal tweets","90cbdcb4":"### Datasets\n\nFor this analysis, the [Sentiment140](https:\/\/www.kaggle.com\/kazanova\/sentiment140) dataset is used. ","6214429f":"## Logistic Regression","56268997":"## Word analysis","aae724dd":"For this experiment, I took a random sample of 8000 tweets.","67a25191":"It's easy to spot words that are indicative of depression in these tweets: depression, treatment, suffering, crying, help, struggle, risk, hate, sad, anxiety, disorder, suicide, stress, therapy, mental health, emotional, bipolar.","2ffe4ac6":"Since there is no readily available public dataset on depression, I found a dataset scraped by [Twint](https:\/\/github.com\/twintproject\/twint).  ","cb75e58f":"## Baseline model","50f370ff":"## Playing with other models","83fee3d9":"### Word embedding","269bac88":"### Naive Baye's","4ffe2230":"### TF-IDF classifier","21a56b70":"### Tokenization"}}