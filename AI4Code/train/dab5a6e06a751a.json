{"cell_type":{"6b37227e":"code","d249c434":"code","48ecb950":"code","7d0223ea":"code","6e9a5dc7":"code","c53e6208":"code","8671b96d":"code","ad086233":"code","017f0799":"code","7ea1110e":"code","9b76befb":"code","845ea6bc":"code","73e58cb4":"code","a4db99c4":"code","1ddf0935":"code","7bdf0de5":"code","c62210cc":"code","8c85b676":"code","84385e91":"code","faa42f3a":"code","54bb434c":"code","3c71918c":"code","dec252e3":"code","a8a652f6":"code","f782351d":"code","3855bbf5":"code","0547b814":"code","e49bb44a":"code","2e59199e":"markdown","4c2adf97":"markdown","8ab6e5f3":"markdown","572fc99d":"markdown","14d8c32b":"markdown","b9616f06":"markdown","3a3f76c2":"markdown","e2d13480":"markdown","f9f692c2":"markdown","58ae8814":"markdown","cb87d7b4":"markdown","04a58d94":"markdown","7da38149":"markdown","d26395f7":"markdown","f5f14aac":"markdown","6a2f3368":"markdown","6149dabe":"markdown","e55a00e1":"markdown","3bd6245b":"markdown","8f86b4c7":"markdown","02ccc4d4":"markdown","25083942":"markdown","5768f9d4":"markdown","8d1a5f9d":"markdown","a17d7b65":"markdown","0f10404f":"markdown","a211390a":"markdown"},"source":{"6b37227e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d249c434":"# important packages\n\t\nimport pandas as pd\t\t\t\t\t# data manipulation using dataframes\nimport numpy as np\t\t\t\t\t# data statistical analysis\n\nimport seaborn as sns\t\t\t\t# Statistical data visualization\nimport matplotlib.pyplot as plt\t\t# data visualisation\n%matplotlib inline","48ecb950":"import librosa\t\t\t\t\t\t\t# package for music and audio analysis\nimport librosa.display","7d0223ea":"import IPython.display as ipd\t\t\t# public api for display tool in ipython","6e9a5dc7":"audio_file = \"..\/input\/urbansound8k\/fold3\/102105-3-0-0.wav\"","c53e6208":"data,sample_rate = librosa.load(audio_file)","8671b96d":"ipd.Audio(audio_file)","ad086233":"plt.figure(figsize=(14,5))\nlibrosa.display.waveplot(data, sr = sample_rate)\nplt.show()","017f0799":"X = librosa.stft(data)\n\n#converting into energy levels(dB)\nXdb = librosa.amplitude_to_db(abs(X))\n\nplt.figure(figsize=(20, 5))\nlibrosa.display.specshow(Xdb, sr=sample_rate, x_axis='time', y_axis='hz')\nplt.colorbar()\nplt.show()","7ea1110e":"plt.figure(figsize=(20, 5))\nlibrosa.display.specshow(Xdb, sr=sample_rate, x_axis='time', y_axis='log')\nplt.colorbar()\nplt.show()","9b76befb":"raw_df = pd.read_csv(\"..\/input\/urbansound8k\/UrbanSound8K.csv\")\ndf = raw_df.copy()","845ea6bc":"df.shape","73e58cb4":"df.head()","a4db99c4":"df.tail()","1ddf0935":"df.columns","7bdf0de5":"df.info()\t\t\t# for concise summary of dataset","c62210cc":"### MISSING DATA ###\n\ndf.isnull().sum()","8c85b676":"df['class'].value_counts()","84385e91":"sns.countplot(y=\"class\", data=df, order = df['class'].value_counts().index)\nplt.show()","faa42f3a":"df.hist( bins = 10, figsize = (10,10), color = 'r')\nplt.show()","54bb434c":"# Calculate correlations\ncorr = df.corr()\n\n# Heatmap\nsns.heatmap(corr,  annot=True, fmt=\".2f\")","3c71918c":"df.head()","dec252e3":"classID = list(df['classID'].unique())","a8a652f6":"audio_list = []\n\nfor ID in classID:\n    for i in range(len(df)):\n        if(df.classID[i] == ID):\n            \n            file = df['slice_file_name'][i]\n            folder = str(df['fold'][i])\n            class_id = ID\n            class_ = df['class'][i]\n            audio_file = \"..\/input\/urbansound8k\/\" + \"fold\" + folder + \"\/\" + file\n            audio_list.append(audio_file)\n            break","f782351d":"audio_list","3855bbf5":"import random\naudio_file = random.choice(audio_list)\ndata,sample_rate = librosa.load(audio_file)\nipd.Audio(audio_file)","0547b814":"df.head()","e49bb44a":"for ID in classID:\n    for i in range(len(df)):\n        if(df.classID[i] == ID):\n            \n            file = df['slice_file_name'][i]\n            folder = str(df['fold'][i])\n            class_id = ID\n            class_ = df['class'][i]\n            audio_file = \"..\/input\/urbansound8k\/\" + \"fold\" + folder + \"\/\" + file\n            print(audio_file)\n            \n            data,sample_rate = librosa.load(audio_file)\n\n            plt.figure(figsize=(14,5))\n            librosa.display.waveplot(data, sr = sample_rate)\n            plt.title(class_)\n            plt.show()\n            break","2e59199e":"## Log-frequency axis: \nFeatures can be obtained from a spectrogram by converting the linear frequency axis, as shown above, into a logarithmic axis. The resulting representation is also called a log-frequency spectrogram","4c2adf97":"# Data Inspection","8ab6e5f3":"## Code and Resources used\n\n- Python version: 3.7.6\n- Packages: Pandas, Numpy, Seaborn, Matplotlib, Scikit, Keras, Tensorflow, Librosa, Ipython\n- Resources used:\n\n  * Medium : https:\/\/mikesmales.medium.com\/sound-classification-using-deep-learning-8bc2aa1990b7\n  * Heartbeat : https:\/\/heartbeat.fritz.ai\/working-with-audio-signals-in-python-6c2bd63b2daf\n","572fc99d":"## Target Variable","14d8c32b":"Dataset is almost balanced","b9616f06":"# Data Cleaning","3a3f76c2":"## Playing audio","e2d13480":"random sound file of different class","f9f692c2":"## Loading file","58ae8814":"# Importing Dataset","cb87d7b4":"![](https:\/\/cdn.lucidsamples.com\/c\/15-category_default\/sound-effects-packs.jpg)","04a58d94":"## Sounds of different classes","7da38149":"## Waveform visualization\ndepicts the waveform visualization of the amplitude vs the time representation of the signal.","d26395f7":"# About Project\n\nAutomatic environmental sound classification is a growing area of research with numerous real world applications. Whilst there is a large body of research in related audio fields such as speech and music, work on the classification of environmental sounds is comparatively scarce.\n\nThere is a plethora of real world applications for this research, such as:\n\n\u2022 Content-based multimedia indexing and retrieval\n\u2022 Assisting deaf individuals in their daily activities\n\u2022 Smart home use cases such as 360-degree safety and security capabilities\n\u2022 Industrial uses such as predictive maintenance","f5f14aac":"## Sampling Frequency\nThe sampling frequency (or sample rate) is the number of samples (data points) per second in a sound. For example: if the sampling frequency is 44 khz, a recording with a duration of 60 seconds will contain 2,646,000 samples. In practice, sampling even higher than 10x helps measure the amplitude correctly in the time domain.","6a2f3368":"## Other variables : Predictors","6149dabe":"# Importing Libraries","e55a00e1":"# Explanatory Data Analysis\n\nI looked at the distributions of the data and the value counts for the various categorical variables. Below are a few highlights :","3bd6245b":"## Librosa library\nLibrosa is a Python package for music and audio processing by Brian McFee and will allow us to load audio in our notebook as a numpy array for analysis and manipulation.\n\nFor much of the preprocessing we will be able to use Librosa\u2019s load() function, which by default converts the sampling rate to 22.05 KHz, normalise the data so the bit-depth values range between -1 and 1 and flattens the audio channels into mono.","8f86b4c7":"## Waveforms of different classes","02ccc4d4":"## Spectogram :  \nA spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. They are time-frequency portraits of signals. Using a spectrogram, we can see how energy levels (dB) vary over time.","25083942":"# Project Overview\n\n- Objective : \n  - **To classifiy urban sound**\n  - used **librosa** library for music and audio analysis\n- Classification Problem\n- Data cleaning\n- Exploratory Data Analysis\n- Data Preprocessing : Feature Extraction using MFCC\n- **Artificial Neural Network (ANN)** Training and Prediction","5768f9d4":"# Loading and Visualizing an audio file","8d1a5f9d":"### Obervations\n\n- start and end timings are correlated to each other\n- most of the voice are foreground as backgruond voices\n- data in fold folders are almost equal","a17d7b65":"# Web Scraping\n\nDataset URL: https:\/\/urbansounddataset.weebly.com\/urbansound8k.html\n\nFor this we will use a dataset called Urbansound8K. The dataset contains 8732 sound excerpts (<=4s) of urban sounds from 10 classes, which are:\n\n\u2022 Air Conditioner\n\u2022 Car Horn\n\u2022 Children Playing\n\u2022 Dog bark\n\u2022 Drilling\n\u2022 Engine Idling\n\u2022 Gun Shot\n\u2022 Jackhammer\n\u2022 Siren\n\u2022 Street Music\n","0f10404f":"This is dog sound. Let us visualize in waveform using librosa library","a211390a":"# Audio file overview\n\nSound are pressure waves, and these waves can be represented by numbers over a time period. These air pressure differences communicates with the brain.\n\nThese sound excerpts are digital audio files in .wav format. Sound waves are digitised by sampling them at discrete intervals known as the sampling rate (typically 44.1kHz for CD quality audio meaning samples are taken 44,100 times per second).\n\nEach sample is the amplitude of the wave at a particular time interval, where the bit depth determines how detailed the sample will be also known as the dynamic range of the signal (typically 16bit which means a sample can range from 65,536 amplitude values)."}}