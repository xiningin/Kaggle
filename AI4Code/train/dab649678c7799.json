{"cell_type":{"b4458184":"code","8c17ba58":"code","61c43e84":"code","e54e6a38":"code","a1827bf8":"code","c26a401e":"code","76fc8988":"code","d295efad":"code","ea2099ec":"code","8a5a148c":"code","355762bd":"code","5cf4aff0":"code","77aaab6d":"code","456bcd1c":"code","6b7d4f60":"code","c18c26bd":"code","83fc4b62":"code","d7135d6f":"code","939c2c50":"code","6cfaf7e2":"code","e80b16e2":"code","22c01958":"code","1da562af":"code","8493fb89":"code","8251886d":"code","5b5fce03":"code","144a6ec1":"code","8abba72c":"code","f85de84e":"code","43843bf7":"code","6378203d":"code","e430aeed":"code","32fefef4":"code","14b9d3f3":"code","03b178bc":"code","1fc6a103":"code","8c44d605":"code","9c58ddcf":"code","ce2d47af":"code","394ef6de":"code","5ef23518":"code","d8542670":"code","72b258a5":"code","ce9744a3":"code","ddcda910":"code","0c1eb268":"code","993a6165":"code","5f29b573":"code","0dd05bb6":"code","b1abb3c7":"markdown","7922557f":"markdown","48a21b31":"markdown","9c8d3254":"markdown","82585375":"markdown","4be443d2":"markdown","0e06a07c":"markdown","f80edc49":"markdown","d245f580":"markdown","9697ed4c":"markdown","50f2de3c":"markdown","686cba75":"markdown","b1d90f85":"markdown","80fe6ad8":"markdown","5dc5a35b":"markdown","0d6d1525":"markdown","84b124e9":"markdown","daf53620":"markdown","2f9dc09d":"markdown","0b0c639c":"markdown","47b7b45f":"markdown","70f9061c":"markdown","5efdb235":"markdown","7eb9e727":"markdown"},"source":{"b4458184":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory+\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c17ba58":"import tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom keras.utils import to_categorical\nfrom keras.layers import *\nfrom sklearn.model_selection import StratifiedShuffleSplit \nimport numpy as np\nfrom keras.preprocessing import image","61c43e84":"def load_data():\n    path = '\/kaggle\/input\/digit-recognizer\/'\n    train = pd.read_csv(path+'train.csv')\n    test = pd.read_csv(path+'test.csv')\n    train_X,train_y = train.drop(columns=['label']),train['label']\n    train_X = np.array(train_X)\n    \n    train_y = np.array(train_y)\n    \n    test_X = test\n    test_X = np.array(test_X)\n    print(train_X.shape)\n    train_X = train_X.reshape((train_X.shape[0],28,28,1))\n    print(train_X.shape)\n    test_X = test_X.reshape((test_X.shape[0],28,28,1))\n    train_y = to_categorical(train_y)\n    return train_X, train_y,test_X\ndef normalize(trainX,testX):\n    trainX_trans = trainX \/ 255\n    testX_trans = testX \/ 255\n    return trainX_trans,testX_trans\ndef make_model():\n    keras.backend.clear_session()\n    model = keras.models.Sequential()\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ##\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ###\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Flatten())\n    model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n    return model\n\n    \n","e54e6a38":"trainX, trainy,testX = load_data()\ntrainX,testX = normalize(trainX,testX)\nmodel = make_model()\nmodel.summary()","a1827bf8":"score_acc = []\nscore_loss = []\nscore_val_loss = []\nscore_val_accuracy = []\nsss = StratifiedShuffleSplit(20,test_size=.2,random_state=115)\nfor index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n    print(len(train_indices))\n    his = model.fit(trainX[train_indices],trainy[train_indices],validation_data=(trainX[val_indices],trainy[val_indices]),epochs=1,batch_size=512)\n    score_acc.append(his.history['accuracy'])\n    score_loss.append(his.history['loss'])\n    score_val_loss.append(his.history['val_loss'])\n    score_val_accuracy.append(his.history['val_accuracy'])\n","c26a401e":"plt.figure(figsize=(12,8))\nplt.plot(score_acc)\nplt.plot(score_val_accuracy)\nplt.axis([0,20,.96,1.05])","76fc8988":"pred = np.argmax(model.predict(testX),axis=1)","d295efad":"pred = pd.DataFrame(pred,columns=['Label'])\nxyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n\npred = pd.concat([xyz,pred],axis=1)\npred.to_csv('CONV 2D.csv',index=False)","ea2099ec":"trainX, trainy,testX = load_data()\ntrainX,testX = normalize(trainX,testX)\nmodel = make_model()\nmodel.summary()","8a5a148c":"datagen = image.ImageDataGenerator()","355762bd":"\nscore_acc = []\nscore_loss = []\nscore_val_loss = []\nscore_val_accuracy = []\nsss = StratifiedShuffleSplit(10,test_size=.2,random_state=115)\nfor index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n    aa = trainX[train_indices]\n    ab = trainy[train_indices]\n    cc = trainX[val_indices]\n    cd = trainy[val_indices]\n    training_batch = datagen.flow(aa, ab, batch_size=512)\n    val_batches = datagen.flow(cc, cd, batch_size=128)\n    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n                    validation_data=val_batches, validation_steps=128)\n    score_acc.append(his.history['accuracy'])\n    score_loss.append(his.history['loss'])\n    score_val_loss.append(his.history['val_loss'])\n    score_val_accuracy.append(his.history['val_accuracy'])\n","5cf4aff0":"model.evaluate(trainX,trainy)","77aaab6d":"pred = np.argmax(model.predict(testX),axis=1)\npred = pd.DataFrame(pred,columns=['Label'])\nxyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n\npred = pd.concat([xyz,pred],axis=1)\npred.to_csv('DATA AUG.csv',index=False)\nmodel.save('DATA AUG.h5')","456bcd1c":"plt.figure(figsize=(12,8))\nplt.plot(score_acc,label='train')\nplt.plot(score_val_accuracy,label='test')\nplt.axis([0,20,.96,1.])\nplt.legend()","6b7d4f60":"plt.figure(figsize=(12,8))\nplt.plot(score_loss,label='train')\nplt.plot(score_val_loss,label='test')\nplt.axis([0,11,.0,.1])\nplt.legend()","c18c26bd":"\nscore_acc = []\nscore_loss = []\nscore_val_loss = []\nscore_val_accuracy = []\nsss = StratifiedShuffleSplit(20,test_size=.2,random_state=115)\nfor index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n    aa = trainX[train_indices]\n    ab = trainy[train_indices]\n    cc = trainX[val_indices]\n    cd = trainy[val_indices]\n    training_batch = datagen.flow(aa, ab, batch_size=512)\n    val_batches = datagen.flow(cc, cd, batch_size=128)\n    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n                    validation_data=val_batches, validation_steps=128)\n    score_acc.append(his.history['accuracy'])\n    score_loss.append(his.history['loss'])\n    score_val_loss.append(his.history['val_loss'])\n    score_val_accuracy.append(his.history['val_accuracy'])\n","83fc4b62":"pred = np.argmax(model.predict(testX),axis=1)\npred = pd.DataFrame(pred,columns=['Label'])\nxyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n\npred = pd.concat([xyz,pred],axis=1)\npred.to_csv('DATA AUG_size.csv',index=False)\nmodel.save('DATA AUG_size.h5')","d7135d6f":"def make_model():\n    keras.backend.clear_session()\n    model = keras.models.Sequential()\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ##\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ###\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    \n    model.add(Flatten())\n    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n    return model\ntrainX, trainy,testX = load_data()\ntrainX,testX = normalize(trainX,testX)\nmodel = make_model()\nmodel.summary()\nscore_acc = []\nscore_loss = []\nscore_val_loss = []\nscore_val_accuracy = []\nsss = StratifiedShuffleSplit(10,test_size=.2,random_state=115)\nfor index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n    aa = trainX[train_indices]\n    ab = trainy[train_indices]\n    cc = trainX[val_indices]\n    cd = trainy[val_indices]\n    training_batch = datagen.flow(aa, ab, batch_size=512)\n    val_batches = datagen.flow(cc, cd, batch_size=128)\n    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n                    validation_data=val_batches, validation_steps=128)\n    score_acc.append(his.history['accuracy'])\n    score_loss.append(his.history['loss'])\n    score_val_loss.append(his.history['val_loss'])\n    score_val_accuracy.append(his.history['val_accuracy'])\n","939c2c50":"model.evaluate(trainX,trainy)","6cfaf7e2":"pred = np.argmax(model.predict(testX),axis=1)\npred = pd.DataFrame(pred,columns=['Label'])\nxyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n\npred = pd.concat([xyz,pred],axis=1)\npred.to_csv('DATA AUG_size_arch.csv',index=False)\nmodel.save('DATA AUG_size_arch.h5')","e80b16e2":"datagen = image.ImageDataGenerator(featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2)\ndef make_model():\n    keras.backend.clear_session()\n    model = keras.models.Sequential()\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ##\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ###\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    \n    model.add(Flatten())\n    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n    return model\ntrainX, trainy,testX = load_data()\ntrainX,testX = normalize(trainX,testX)\nmodel = make_model()\nmodel.summary()\nscore_acc = []\nscore_loss = []\nscore_val_loss = []\nscore_val_accuracy = []\nsss = StratifiedShuffleSplit(10,test_size=.2,random_state=115)\nfor index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n    aa = trainX[train_indices]\n    ab = trainy[train_indices]\n    cc = trainX[val_indices]\n    cd = trainy[val_indices]\n    training_batch = datagen.flow(aa, ab, batch_size=512)\n    val_batches = datagen.flow(cc, cd, batch_size=128)\n    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n                    validation_data=val_batches, validation_steps=128)\n    score_acc.append(his.history['accuracy'])\n    score_loss.append(his.history['loss'])\n    score_val_loss.append(his.history['val_loss'])\n    score_val_accuracy.append(his.history['val_accuracy'])\n","22c01958":"model.evaluate(trainX,trainy)","1da562af":"pred = np.argmax(model.predict(testX),axis=1)\npred = pd.DataFrame(pred,columns=['Label'])\nxyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n\npred = pd.concat([xyz,pred],axis=1)\npred.to_csv('DATA AUG_size_arch_custom_gen.csv',index=False)\nmodel.save('DATA AUG_size_arch_custom_gen.h5')","8493fb89":"datagen = image.ImageDataGenerator(featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2)\ndef make_model():\n    keras.backend.clear_session()\n    model = keras.models.Sequential()\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ##\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ###\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    \n    model.add(Flatten())\n    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n    return model\ntrainX, trainy,testX = load_data()\ntrainX,testX = normalize(trainX,testX)\nmodel = make_model()\nmodel.summary()\nscore_acc = []\nscore_loss = []\nscore_val_loss = []\nscore_val_accuracy = []\nsss = StratifiedShuffleSplit(20,test_size=.2,random_state=115)\nfor index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n    aa = trainX[train_indices]\n    ab = trainy[train_indices]\n    cc = trainX[val_indices]\n    cd = trainy[val_indices]\n    training_batch = datagen.flow(aa, ab, batch_size=512)\n    val_batches = datagen.flow(cc, cd, batch_size=128)\n    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n                    validation_data=val_batches, validation_steps=128)\n    score_acc.append(his.history['accuracy'])\n    score_loss.append(his.history['loss'])\n    score_val_loss.append(his.history['val_loss'])\n    score_val_accuracy.append(his.history['val_accuracy'])\n","8251886d":"model.evaluate(trainX,trainy)","5b5fce03":"pred = np.argmax(model.predict(testX),axis=1)\npred = pd.DataFrame(pred,columns=['Label'])\nxyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n\npred = pd.concat([xyz,pred],axis=1)\npred.to_csv('DATA AUG_size_arch_custom_gen_20.csv',index=False)\nmodel.save('DATA AUG_size_arch_custom_gen_20.h5')","144a6ec1":"datagen = image.ImageDataGenerator(featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2)\ndef make_model():\n    keras.backend.clear_session()\n    model = keras.models.Sequential()\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ##\n    \n    ##\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ###\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    \n    model.add(Flatten())\n    model.add(Dropout(.25))\n    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n    return model\ntrainX, trainy,testX = load_data()\ntrainX,testX = normalize(trainX,testX)\nmodel = make_model()\nmodel.summary()\nscore_acc = []\nscore_loss = []\nscore_val_loss = []\nscore_val_accuracy = []\nsss = StratifiedShuffleSplit(20,test_size=.2,random_state=115)\nfor index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n    aa = trainX[train_indices]\n    ab = trainy[train_indices]\n    cc = trainX[val_indices]\n    cd = trainy[val_indices]\n    training_batch = datagen.flow(aa, ab, batch_size=512)\n    val_batches = datagen.flow(cc, cd, batch_size=128)\n    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n                    validation_data=val_batches, validation_steps=128)\n    score_acc.append(his.history['accuracy'])\n    score_loss.append(his.history['loss'])\n    score_val_loss.append(his.history['val_loss'])\n    score_val_accuracy.append(his.history['val_accuracy'])\n","8abba72c":"model.evaluate(trainX,trainy)","f85de84e":"pred = np.argmax(model.predict(testX),axis=1)\npred = pd.DataFrame(pred,columns=['Label'])\nxyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n\npred = pd.concat([xyz,pred],axis=1)\npred.to_csv('DATA AUG_size_arch_custom_gen_20_DROPOUT.csv',index=False)\nmodel.save('DATA AUG_size_arch_custom_gen_20_DROPOUT.h5')","43843bf7":"datagen = image.ImageDataGenerator(featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,zoom_range=0.08,shear_range=0.3)\ndef make_model():\n    keras.backend.clear_session()\n    model = keras.models.Sequential()\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ##\n    \n    ##\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ###\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    \n    model.add(Flatten())\n    model.add(Dropout(.25))\n    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n    return model\ntrainX, trainy,testX = load_data()\ntrainX,testX = normalize(trainX,testX)\nmodel = make_model()\nmodel.summary()\nscore_acc = []\nscore_loss = []\nscore_val_loss = []\nscore_val_accuracy = []\nsss = StratifiedShuffleSplit(20,test_size=.2,random_state=115)\nfor index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n    aa = trainX[train_indices]\n    ab = trainy[train_indices]\n    cc = trainX[val_indices]\n    cd = trainy[val_indices]\n    training_batch = datagen.flow(aa, ab, batch_size=512)\n    val_batches = datagen.flow(cc, cd, batch_size=128)\n    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n                    validation_data=val_batches, validation_steps=128)\n    score_acc.append(his.history['accuracy'])\n    score_loss.append(his.history['loss'])\n    score_val_loss.append(his.history['val_loss'])\n    score_val_accuracy.append(his.history['val_accuracy'])\n","6378203d":"model.evaluate(trainX,trainy)","e430aeed":"pred = np.argmax(model.predict(testX),axis=1)\npred = pd.DataFrame(pred,columns=['Label'])\nxyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n\npred = pd.concat([xyz,pred],axis=1)\npred.to_csv('DATA AUG_size_arch_custom_gen_20_DROPOUT_gen.csv',index=False)\nmodel.save('DATA AUG_size_arch_custom_gen_20_DROPOUT_gen.h5')","32fefef4":"datagen = image.ImageDataGenerator(featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,zoom_range=0.08,shear_range=0.3)\ndef make_model():\n    keras.backend.clear_session()\n    model = keras.models.Sequential()\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ##\n    \n    ##\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ###\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    \n    model.add(Flatten())\n    model.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n    return model\ntrainX, trainy,testX = load_data()\ntrainX,testX = normalize(trainX,testX)\nmodel = make_model()\nmodel.summary()\nscore_acc = []\nscore_loss = []\nscore_val_loss = []\nscore_val_accuracy = []\nsss = StratifiedShuffleSplit(30,test_size=.2,random_state=115)\nfor index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n    aa = trainX[train_indices]\n    ab = trainy[train_indices]\n    cc = trainX[val_indices]\n    cd = trainy[val_indices]\n    training_batch = datagen.flow(aa, ab, batch_size=512)\n    val_batches = datagen.flow(cc, cd, batch_size=128)\n    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n                    validation_data=val_batches, validation_steps=128)\n    score_acc.append(his.history['accuracy'])\n    score_loss.append(his.history['loss'])\n    score_val_loss.append(his.history['val_loss'])\n    score_val_accuracy.append(his.history['val_accuracy'])\n","14b9d3f3":"model.evaluate(trainX,trainy)","03b178bc":"pred = np.argmax(model.predict(testX),axis=1)\npred = pd.DataFrame(pred,columns=['Label'])\nxyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n\npred = pd.concat([xyz,pred],axis=1)\npred.to_csv('DATA AUG_size_arch_custom_gen_20_DROPOUT_gen_dense-30.csv',index=False)\nmodel.save('DATA AUG_size_arch_custom_gen_20_DROPOUT_gen_dense-30.h5')","1fc6a103":"datagen = image.ImageDataGenerator(featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=30,\n    width_shift_range=0.25,\n    height_shift_range=0.25,zoom_range=0.08,shear_range=0.3)\ndef make_model():\n    keras.backend.clear_session()\n    model = keras.models.Sequential()\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ##\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    ###\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    \n    model.add(Flatten())\n    model.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n    return model\ntrainX, trainy,testX = load_data()\ntrainX,testX = normalize(trainX,testX)\nmodel = make_model()\nmodel.summary()\nscore_acc = []\nscore_loss = []\nscore_val_loss = []\nscore_val_accuracy = []\nsss = StratifiedShuffleSplit(30,test_size=.2,random_state=115)\nfor index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n    aa = trainX[train_indices]\n    ab = trainy[train_indices]\n    cc = trainX[val_indices]\n    cd = trainy[val_indices]\n    training_batch = datagen.flow(aa, ab, batch_size=512)\n    val_batches = datagen.flow(cc, cd, batch_size=128)\n    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n                    validation_data=val_batches, validation_steps=128)\n    score_acc.append(his.history['accuracy'])\n    score_loss.append(his.history['loss'])\n    score_val_loss.append(his.history['val_loss'])\n    score_val_accuracy.append(his.history['val_accuracy'])\n","8c44d605":"model.evaluate(trainX,trainy)","9c58ddcf":"pred = np.argmax(model.predict(testX),axis=1)\npred = pd.DataFrame(pred,columns=['Label'])\nxyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n\npred = pd.concat([xyz,pred],axis=1)\npred.to_csv('DATA AUG_size_arch_custom_gen_20_DROPOUT_gen_dense_gen_30.csv',index=False)\nmodel.save('DATA AUG_size_arch_custom_gen_20_DROPOUT_gen_dense_gen_30.h5')","ce2d47af":"# ..\/input\/digit-recogniser-my-answers\/DATA AUG.csv","394ef6de":"import os\nfiles = []\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        if 'csv' in filename:\n            files.append(os.path.join(dirname, filename))\n       \n","5ef23518":"label_data = []\nfor i in files:\n    aa = pd.read_csv(i,index_col=False).values[:,1]\n    print(aa)\n    label_data.append(aa)\nlabel_data = np.array(label_data)","d8542670":"pd.DataFrame(label_data)","72b258a5":"len(label_data)","ce9744a3":"err = []\nfor i,a in enumerate(zip(label_data[0,:],label_data[1,:],label_data[2,:],label_data[3,:],label_data[4,:],label_data[5,:],label_data[6,:],label_data[7,:],label_data[8,:],label_data[9,:])):\n    s = a[0]\n    for b in a:\n        if b!=s:\n            err.append(i)","ddcda910":"plt.imshow(testX[165,:].reshape(28,28))","0c1eb268":"len(set(err))","993a6165":"for i in err[:20]:\n    print(i,end=' ')\n    for j in range(10):\n        print(label_data[j,i],end=' ')\n    print()","5f29b573":"plt.imshow(testX[165,:].reshape(28,28))","0dd05bb6":"stacked_pred = pd.DataFrame(label_data).mode(axis=0).iloc[0]\nstacked_pred = stacked_pred.apply(lambda x:int(x))\nstacked_pred = pd.DataFrame({'Label': stacked_pred})\nxyz = pd.DataFrame(np.arange(1,stacked_pred.shape[0]+1),columns=['ImageId'])\n\nstacked_pred = pd.concat([xyz,stacked_pred],axis=1)\nstacked_pred.to_csv('STACK_OF_Ten.csv',index=False)","b1abb3c7":"AUGMENTATION","7922557f":"This was just a rough code as i wanted to see the images it generated .Curiousity got the best of me.","48a21b31":"Now i tried to see where my models were giving different answers , and some images were even difficult for me to understand .\nLOL","9c8d3254":"Now that i was in zone and read about VGG 16 please look into it if you have not and i saw the architecture \/ layers layout they used and i tried to give it a chance . This did certainly help me get past 99.2 accuracy and break my own record.","82585375":"I thought it was not running my model enough as it was kind of underfiting so i changed it to 20 splits.","4be443d2":"I used GPU for these fittings and it saved me hours of time .Hatsoff to kaggle GPU for this , i would also suggest you to learn how to use it as it will help to make your code execute faster but only if you are training deep Networks.","0e06a07c":"This is a very beginner friendly kernel and my first kernel . I hope it helps you as much as i learned while implementing it , So lets Begin.","f80edc49":"We begin by importing libraries that i would use generally. Also i would recommend all beginners to keep all their imports at one place for ease and presentation wise too .","d245f580":"You will see that i save my files in csv and their names might be a bit confusing but i do this to keep track of what i did to get the results like data aug means i added augmentation to my code of data .It is a little unorthodox but i will make sure to find better way.","9697ed4c":"Now i read about data augmentation and this was easy to understand but i was dumbstruck on how to use it and keras documentaion did help me.\nIf you are hearing about Data Augmentation for first time , please go through the link to get an Idea about this and i Promise you will love it.\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/image-augmentation-on-the-fly-using-keras-imagedatagenerator\/","50f2de3c":"The load data function simply loads the dataset. \n\nI also reshaped the dataset's (which was converted into numpy array as they are fast and easy to deal) each sameple from 784 into 28 x 28 x 1 shape. For those who dont know the reason for this step can look at this article which exlain in detail and other details about convulotion layer.\nhttps:\/\/towardsdatascience.com\/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2\n\nI used to_categorical to implement one hot encoding in my labels .(It made 3 to a array (0,0,0,1,0,0,0,0,0,0) ).\n\nAnd Normalize function normalizes the image pixels , as the value of pixels can be from 0 to 255.(I Reduced it to 0 to 1.This step was important and it did increase my score in the leader board).\n\nMake model created a model and returned it . The model i used was a simple convnet with 3 Conv2D layers ,1 Dense layer ,2 Max Pooling layer and a Dense layer for showing probabiliy of being the digit (Hence it outputs an array of size 10,probability for each).\n\nOptimizer is the common RMSprop one can try SGD too.Loss function is crosscategorical_crossentropy which is best suited for such conditions and accuracy as metrics to monitor performance of my model.\n\n","686cba75":"I have made functions that will help me load my data , normalise it , use encoding . So that i dont have to do same thing over and over again . This practise helped me to monitor more things at once and keep my code clean.","b1d90f85":"Now i my model was giving 99.99 something accuracy but it was not performing as good in the scoreboard so i decided to add a dropout layer with probability .\nFor those who heard this term the first time,it is a simple way to prevent overfitting in your model as it randomly selects some nodes and shuts them off , which makes the other node to learn on their own .Hence in a way no node resides on top as each learns the same amount .This link can help with a more better explaintation.\nhttps:\/\/machinelearningmastery.com\/dropout-for-regularizing-deep-neural-networks\/","80fe6ad8":"Notebooks that helped me , please give them a read .\nhttps:\/\/www.kaggle.com\/poonaml\/deep-neural-network-keras-way#Import-all-required-libraries","5dc5a35b":"Here i did two things :-\n1. I increased splits to 30 .\n2. I increased nodes in Dense layer to 512 .\n\nMy model was neither underfitting\/overfitting but for some reason i had a feeling that i can do better .  ","0d6d1525":"I thought to add some customizations to my Image generator , in order to give my score a push . I used the dafault example given in keras documentation .\nhttps:\/\/keras.io\/api\/preprocessing\/image\/#imagedatagenerator-class","84b124e9":"Here i added some touch to the image generator like zoom,shear transfomation ,etc .which made it past 99.3 .","daf53620":"The model above gave me a good 99. something accuracy which i was suprised to see as which my normal Nueral network i got 97. something at max.\nI would like the readers who are starting in kaggle to learn more about Convulation Network as it blew me away. I was reluctant to use it as i thought of it as some advanced topic and kept on delaying to learn it .I wish you would not make the same mistake as me.","2f9dc09d":"Some plots to see the performance of my model.","0b0c639c":"In this i just tweaked the geneareator a bit and not much as i was getting good results.","47b7b45f":"At Last i used stacked my different result that i had and used simple mode to decide my answer .\nMy submissions were completed at this point but i have good feeling about this predictions.","70f9061c":"Instead of running a simple model.fit ,I used Stratified Shuffle Split for 2 reasons:-\n1. It divides the data properly , assume i have 50 images of cat and dogs. And i decided to use 50 for training ,25 for validation and 25 for testing . There is no way to tell if 50 training images are of 25 cats and dog each .Maybe its 40 cats and 10 dogs ,Surely this is a problem as it wont be truly proportional sample. This problem is taken care by Stratified Shuffle Split .\n\n2. I got better score using this method.POINT BLANK.","5efdb235":"Here i increase number of splits as data aug do generate more than one image by altering it so i increased the splits,also i could see that it was not reaching upto its true potential.","7eb9e727":"This marks the end of the notebook , hope it helps in your journey and hope we meet again soon .\n\nTHANK YOU FOR READING ...."}}