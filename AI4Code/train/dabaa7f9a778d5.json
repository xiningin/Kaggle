{"cell_type":{"b48b134a":"code","9e26fade":"code","945f07f2":"code","f84e812d":"code","f2049f54":"code","46e730f0":"code","c9b92bb5":"code","4cbd7df4":"code","1681c42f":"code","137e8bc6":"code","f51d44fd":"code","0a6b3cf7":"code","6459892a":"markdown","d2118c7d":"markdown","70193777":"markdown","86df5ab6":"markdown","3c2eb4f5":"markdown","4596f584":"markdown","80c505c8":"markdown","0e5e7d0a":"markdown","9644a062":"markdown","c58294a9":"markdown","9b42c155":"markdown","9e31b4ae":"markdown"},"source":{"b48b134a":"#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\/house-prices-advanced-regression-techniques\"]).decode(\"utf8\")) #check the files available in the directory\n\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb","9e26fade":"def delete_id(train, test):\n    train_ID = train['Id']\n    test_ID = test['Id']\n\n    #Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n    train.drop(\"Id\", axis = 1, inplace = True)\n    test.drop(\"Id\", axis = 1, inplace = True)\n    \n    return train_ID,test_ID,train,test\n\ndef delete_outliers(train):\n    train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n    return train\n\ndef log_target(train):\n    train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n    return train\n\ndef construct_all_data(train, test):\n    ntrain = train.shape[0]\n    ntest = test.shape[0]\n    y_train = train.SalePrice.values\n    all_data = pd.concat((train, test)).reset_index(drop=True)\n    all_data.drop(['SalePrice'], axis=1, inplace=True)\n    return ntrain,ntest,all_data\n\ndef data_filling(all_data):\n    all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n    all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n    all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n    all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n    all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n    all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n    for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n        all_data[col] = all_data[col].fillna('None')\n    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        all_data[col] = all_data[col].fillna(0)\n    for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n        all_data[col] = all_data[col].fillna(0)\n    all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n    all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n    all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n    all_data = all_data.drop(['Utilities'], axis=1)\n    all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n    all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n    all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n    all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n    all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n    all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n    all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n    return all_data\n\ndef change_data_type(all_data):\n    all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n    all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n    all_data['YrSold'] = all_data['YrSold'].astype(str)\n    all_data['MoSold'] = all_data['MoSold'].astype(str)\n    return all_data\n\ndef label_encode(all_data):\n    \n    cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n    # process columns, apply LabelEncoder to categorical features\n    for c in cols:\n        lbl = LabelEncoder() \n        lbl.fit(list(all_data[c].values)) \n        all_data[c] = lbl.transform(list(all_data[c].values))\n    return all_data\n\ndef add_features(all_data):\n    all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n    return all_data\n    \ndef fix_skew_features(all_data):\n    numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n    # Check the skew of all numerical features\n    skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n    #print(\"\\nSkew in numerical features: \\n\")\n    skewness = pd.DataFrame({'Skew' :skewed_feats})\n    \n    skewness = skewness[abs(skewness) > 0.75]\n    #print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\n    from scipy.special import boxcox1p\n    skewed_features = skewness.index\n    lam = 0.15\n    for feat in skewed_features:\n        #all_data[feat] += 1\n        all_data[feat] = boxcox1p(all_data[feat], lam)\n    return all_data\n\ndef get_dummys_data(all_data):\n    all_data = pd.get_dummies(all_data)\n    return all_data\n\ndef get_train_test(all_data,ntrain):\n    train = all_data[:ntrain]\n    test = all_data[ntrain:]\n    return train,test\n\ndef rmsle_cv(model,n_folds = 5):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","945f07f2":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_ID,test_ID,train,test = delete_id(train, test)\ntrain = delete_outliers(train)\ntrain = log_target(train)\ny_train = train.SalePrice.values\nntrain,ntest,all_data = construct_all_data(train, test)\nall_data = data_filling(all_data)\nall_data = change_data_type(all_data)\nall_data = label_encode(all_data)\nall_data = add_features(all_data)\nall_data = fix_skew_features(all_data)\nall_data = get_dummys_data(all_data)\ntrain,test = get_train_test(all_data,ntrain)\nmodel = xgb.XGBRegressor()\nmodel.fit(train, y_train)\nxgb_train_pred = model.predict(train)\nprint(rmsle(y_train, xgb_train_pred))","f84e812d":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_ID,test_ID,train,test = delete_id(train, test)\ntrain = log_target(train)\ny_train = train.SalePrice.values\nntrain,ntest,all_data = construct_all_data(train, test)\nall_data = data_filling(all_data)\nall_data = change_data_type(all_data)\nall_data = label_encode(all_data)\nall_data = add_features(all_data)\nall_data = fix_skew_features(all_data)\nall_data = get_dummys_data(all_data)\ntrain,test = get_train_test(all_data,ntrain)\nmodel = xgb.XGBRegressor()\nmodel.fit(train, y_train)\nxgb_train_pred = model.predict(train)\nprint(rmsle(y_train, xgb_train_pred))","f2049f54":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_ID,test_ID,train,test = delete_id(train, test)\ntrain = delete_outliers(train)\ny_train = train.SalePrice.values\nntrain,ntest,all_data = construct_all_data(train, test)\nall_data = data_filling(all_data)\nall_data = change_data_type(all_data)\nall_data = label_encode(all_data)\nall_data = add_features(all_data)\nall_data = fix_skew_features(all_data)\nall_data = get_dummys_data(all_data)\ntrain,test = get_train_test(all_data,ntrain)\nmodel = xgb.XGBRegressor()\nmodel.fit(train, y_train)\nxgb_train_pred = model.predict(train)\nprint(rmsle(np.log1p(y_train), np.log1p(xgb_train_pred)))","46e730f0":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_ID,test_ID,train,test = delete_id(train, test)\ntrain = delete_outliers(train)\ntrain = log_target(train)\ny_train = train.SalePrice.values\nntrain,ntest,all_data = construct_all_data(train, test)\nall_data = all_data.fillna(0)\nall_data = change_data_type(all_data)\nall_data = label_encode(all_data)\nall_data = add_features(all_data)\nall_data = fix_skew_features(all_data)\nall_data = get_dummys_data(all_data)\ntrain,test = get_train_test(all_data,ntrain)\nmodel = xgb.XGBRegressor()\nmodel.fit(train, y_train)\nxgb_train_pred = model.predict(train)\nprint(rmsle(y_train, xgb_train_pred))","c9b92bb5":"def temp_label_encode(all_data):\n    cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir')\n    # process columns, apply LabelEncoder to categorical features\n    for c in cols:\n        lbl = LabelEncoder() \n        lbl.fit(list(all_data[c].values)) \n        all_data[c] = lbl.transform(list(all_data[c].values))\n    return all_data\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_ID,test_ID,train,test = delete_id(train, test)\ntrain = delete_outliers(train)\ntrain = log_target(train)\ny_train = train.SalePrice.values\nntrain,ntest,all_data = construct_all_data(train, test)\nall_data = data_filling(all_data)\nall_data = temp_label_encode(all_data)\nall_data = add_features(all_data)\nall_data = fix_skew_features(all_data)\nall_data = get_dummys_data(all_data)\ntrain,test = get_train_test(all_data,ntrain)\nmodel = xgb.XGBRegressor()\nmodel.fit(train, y_train)\nxgb_train_pred = model.predict(train)\nprint(rmsle(y_train, xgb_train_pred))","4cbd7df4":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_ID,test_ID,train,test = delete_id(train, test)\ntrain = delete_outliers(train)\ntrain = log_target(train)\ny_train = train.SalePrice.values\nntrain,ntest,all_data = construct_all_data(train, test)\nall_data = data_filling(all_data)\nall_data = change_data_type(all_data)\nall_data = label_encode(all_data)\nall_data = fix_skew_features(all_data)\nall_data = get_dummys_data(all_data)\ntrain,test = get_train_test(all_data,ntrain)\nmodel = xgb.XGBRegressor()\nmodel.fit(train, y_train)\nxgb_train_pred = model.predict(train)\nprint(rmsle(y_train, xgb_train_pred))","1681c42f":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_ID,test_ID,train,test = delete_id(train, test)\ntrain = delete_outliers(train)\ntrain = log_target(train)\ny_train = train.SalePrice.values\nntrain,ntest,all_data = construct_all_data(train, test)\nall_data = data_filling(all_data)\nall_data = change_data_type(all_data)\nall_data = label_encode(all_data)\nall_data = add_features(all_data)\nall_data = get_dummys_data(all_data)\ntrain,test = get_train_test(all_data,ntrain)\nmodel = xgb.XGBRegressor()\nmodel.fit(train, y_train)\nxgb_train_pred = model.predict(train)\nprint(rmsle(y_train, xgb_train_pred))","137e8bc6":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_ID,test_ID,train,test = delete_id(train, test)\ntrain = delete_outliers(train)\ntrain = log_target(train)\ny_train = train.SalePrice.values\nntrain,ntest,all_data = construct_all_data(train, test)\nall_data = data_filling(all_data)\nall_data = change_data_type(all_data)\nall_data = label_encode(all_data)\nall_data = add_features(all_data)\nall_data = fix_skew_features(all_data)\nall_data = get_dummys_data(all_data)\ntrain,test = get_train_test(all_data,ntrain)\nmodel = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel.fit(train, y_train)\nxgb_train_pred = model.predict(train)\nprint(rmsle(y_train, xgb_train_pred))","f51d44fd":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_ID,test_ID,train,test = delete_id(train, test)\ntrain = delete_outliers(train)\ntrain = log_target(train)\ny_train = train.SalePrice.values\nntrain,ntest,all_data = construct_all_data(train, test)\nall_data = data_filling(all_data)\nall_data = change_data_type(all_data)\nall_data = label_encode(all_data)\nall_data = add_features(all_data)\nall_data = fix_skew_features(all_data)\nall_data = get_dummys_data(all_data)\ntrain,test = get_train_test(all_data,ntrain)\n\nxgb_model = xgb.XGBRegressor()\nxgb_model.fit(train, y_train)\nxgb_train_pred = xgb_model.predict(train)\n\nlgb_model = lgb.LGBMRegressor()\nlgb_model.fit(train, y_train)\nlgb_train_pred = lgb_model.predict(train)\n\nprint(rmsle(y_train, xgb_train_pred*0.5 + lgb_train_pred*0.5))","0a6b3cf7":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_ID,test_ID,train,test = delete_id(train, test)\ntrain = delete_outliers(train)\ntrain = log_target(train)\ny_train = train.SalePrice.values\nntrain,ntest,all_data = construct_all_data(train, test)\nall_data = data_filling(all_data)\nall_data = temp_label_encode(all_data)\nall_data = add_features(all_data)\nall_data = fix_skew_features(all_data)\nall_data = get_dummys_data(all_data)\ntrain,test = get_train_test(all_data,ntrain)\n\nxgb_model = xgb.XGBRegressor()\nxgb_model.fit(train, y_train)\nxgb_train_pred = xgb_model.predict(train)\n\nlgb_model = lgb.LGBMRegressor()\nlgb_model.fit(train, y_train)\nlgb_train_pred = lgb_model.predict(train)\n\nprint(rmsle(y_train, xgb_train_pred*0.5 + lgb_train_pred*0.5))\n\nxgb_pred = np.expm1(xgb_model.predict(test))\nlgb_pred = np.expm1(lgb_model.predict(test))\nensemble = xgb_pred*0.5 + lgb_pred*0.5\n\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","6459892a":"4. Ignore change the fake numerical data to category data, the loss is 0.08533225721354491.","d2118c7d":"**Vanguard To**\n2019\/10\/31\n\nAfter reading some notebook of predict house price, we know that there are so many methods to optimize the arithmetic. But we don't know which is the most important factor that lead the result better.\n\nSo I try to analyse this by actual experiment. If this notebook is valuable for you, please vote the star.The code in this notebook is reference so many codes of the following notebook which more than 3000 votes:\n\n[Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/comments)\n\nThese are some important steps to predict house price. Let use analyse these step now:\n1. Delete outlines\n2. Change the target with log\n3. Fill the null data\n4. Change the fake numerical data to category data\n5. Add new features\n6. Fix skew numerical data\n7. Model hyperparameters optimizing\n8. Model Stacking","70193777":"7. If the model use hyperparameters optimizing, the loss is 0.07856009453221159.","86df5ab6":"6. If we ignore fix skew numerical data, the loss is 0.0858717892341178.","3c2eb4f5":"2. After we remove the log target step, the loss is 0.08921271760589457.","4596f584":"8. If we stacking use XGBoost, LightGBM with simple adding, the loss is 0.06038047104714868.","80c505c8":"Fine! Now we remove \"4. Change the fake numerical data to category data\" and retrain the mode, after that submit the result to Kaggle!","0e5e7d0a":"3. If we use simple way to fill na data, the loss is 0.08644949011952527.","9644a062":"0. We set the base line is all feature engineering with basic xgboost without any hyperparameters optimizing. The score is 0.0858717892341178.","c58294a9":"5. If we ignore add new features, the loss is 0.08628056522017158.","9b42c155":"1. After we remove the delete_outliers step, the loss is 0.08691944078117156.","9e31b4ae":"**Let me markdown the final result here, the most important step is \"8. Model Stacking\".** \n\nThe second important step is \"7. Model hyperparameters optimizing\". The third important step is \"2. Change the target with log\"\n\nAnother interesting thing is \"4. Change the fake numerical data to category data\" lead to a worse result."}}