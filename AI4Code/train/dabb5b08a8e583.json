{"cell_type":{"3adfe691":"code","f51369d7":"code","8e1622d2":"code","e804df8e":"code","19d0e30a":"code","d8434906":"code","4cf097c0":"code","5ea7003f":"code","76a2cfce":"code","cdc24a52":"code","24a4fda7":"code","c2431759":"code","3d2537d2":"code","6afbca3e":"code","85c70db1":"code","f166e45f":"code","7d7391bd":"code","cecde334":"code","313266c4":"code","bad9d93e":"code","55c1de80":"code","543be1e7":"code","e4e7e6e4":"code","38e31a5b":"code","19531604":"code","4f61d76c":"code","5d66655b":"code","f398b508":"markdown","2fc9cb16":"markdown","f6f98068":"markdown","448e1b36":"markdown","bbc95660":"markdown","217b73f7":"markdown","8386225c":"markdown","8c0b657e":"markdown","b5c3649b":"markdown","11167be5":"markdown","f86fbcd7":"markdown","102dbac2":"markdown","40614e15":"markdown","16c78daa":"markdown","8b93dea2":"markdown","e79540ec":"markdown","99557be1":"markdown","5b5cc3c1":"markdown","0bf0fc83":"markdown","0a322679":"markdown","951489b3":"markdown","0f2f799a":"markdown"},"source":{"3adfe691":"# Import the default:\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# Now let's load the data.\ndf = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict.csv')","f51369d7":"# First we take a look at the general shape of the data\nprint('df shape: ' , df.shape)\n\n# Then we take a look at the first couple rows:\ndf","8e1622d2":"# first drop the Serial No.\ndf = df.drop('Serial No.', axis=1)\ndf","e804df8e":"# Now we first take a look at the different variables. The most important one is the one we are trying to predict.\ndf['Chance of Admit '].describe()\n\n# histogram\nsns.distplot(df['Chance of Admit '])","19d0e30a":"# skewness:\nprint('Skewness: %f' %df['Chance of Admit '].skew())","d8434906":"num_cols = ['GRE Score', 'TOEFL Score', 'CGPA', 'Chance of Admit ']\ncat_cols = ['University Rating', 'SOP', 'LOR ', 'Research']","4cf097c0":"for col in num_cols:\n    plt.figure()\n    sns.distplot(df[col])\n","5ea7003f":"# as for your skewness:\n\nfor col in num_cols:\n    print('Skewness of ' + col +': %f' %df[col].skew())","76a2cfce":"sns.set()\nsns.pairplot(df[num_cols])\nplt.show()","cdc24a52":"for col in cat_cols:\n    sns.distplot(df[col])\n    plt.figure()","24a4fda7":"# Box plots:\nfor col in cat_cols:\n    data = pd.concat([df['Chance of Admit '],df[col]], axis=1)\n    f, ax = plt.subplots(figsize=(8,6))\n    fig = sns.boxplot(x=col, y='Chance of Admit ', data=data)\n ","c2431759":"# Correlation matrix\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize = (12,9))\nsns.heatmap(corrmat, square=True)","3d2537d2":"# total NA per column:\ntotal = df.isnull().sum()\ntotal","6afbca3e":"df_1 = df.copy()\nfor col in cat_cols:\n    dummy = pd.get_dummies(df_1[col], prefix=col)\n    df_1 = pd.concat([df_1, dummy], axis=1)\ndf = df_1.drop(cat_cols, axis=1)\n","85c70db1":"from sklearn.model_selection import train_test_split\n# split off y\ny = df['Chance of Admit ']\nX = df.drop('Chance of Admit ', axis=1)\n\n# train_test_split:\nX_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size = 0.33, random_state = 42)\n\n#Let's make sure we did it correctly:\nX_train.shape\nprint ('X_train shape : ' ,(X_train.shape))\nprint('y_train shape ; ', y_train.shape)\nprint('X_test shape :', X_test.shape)\nprint('y_test shape :', y_test.shape)\n","f166e45f":"# Standarize numerical features\nfrom sklearn.preprocessing import StandardScaler\nstdSc = StandardScaler()\nnum_cols_1 = ['GRE Score', 'TOEFL Score', 'CGPA']\nX_train[num_cols_1] = stdSc.fit_transform(X_train[num_cols_1])\nX_test[num_cols_1] = stdSc.fit_transform(X_test[num_cols_1])","7d7391bd":"from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.model_selection import cross_val_score\n","cecde334":"# Define error scorer:\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\ndef rmse_cv_train(model):\n    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv=10))\n    return(rmse)\n\ndef rmse_cv_test(model):\n    rmse = np.sqrt(-cross_val_score(model, X_test, y_test, scoring = scorer, cv=10))\n    return(rmse)\n    ","313266c4":"# Linear Regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n# RMSE scores on training and test set:\nprint(\"RMSE on Training set: \", rmse_cv_train(lr).mean())\nprint('RMSE on Test set: ', rmse_cv_train(lr).mean())\n\n#actual predictions\ny_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)\n\n#plot residues\nplt.scatter(y_train_pred, y_train_pred - y_train, c = 'blue', marker = 's', label = 'train set')\nplt.scatter(y_test_pred, y_test_pred - y_test, c = 'green', marker = 's', label= 'validation set')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residues')\nplt.title('OLS')\nplt.legend(loc = 'upper left')\nplt.hlines(y = 0, xmin=0, xmax=1, color='red')\nplt.show()\n\n#plot pred vs. actual values\nplt.scatter(y_train_pred, y_train, c = 'blue', marker='s', label='train set')\nplt.scatter(y_test_pred, y_test, c='green', marker='s', label='Validation set')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('OLS')\nplt.plot([0,1], [0,1], c='red')\nplt.show()\n\n#Actual RMSE:\nprint('Actual RMSE on Training set: ', mean_squared_error(y_train_pred, y_train, squared=False))\nprint('Actual RMSE on Test set: ', mean_squared_error(y_test_pred, y_test, squared=False))\n\n","bad9d93e":"coefs = pd.Series(lr.coef_, index=X_train.columns)\ncoefs.plot(kind='barh')","55c1de80":"ridge = RidgeCV(alphas=[0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha\", alpha)\n\nprint('Try some more alpha centered around ', alpha)\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                cv = 10)\nridge.fit(X_train, y_train)\nalpha= ridge.alpha_\nprint('Best alpha :', alpha)\n\n","543be1e7":"#Now having alpha, we can use our rmse_cv_train function:\nprint('Ridge RMSE on training set :', rmse_cv_train(ridge).mean())\nprint('Ridge RMSE on test set :', rmse_cv_test(ridge).mean())","e4e7e6e4":"#actual predictions\ny_train_pred = ridge.predict(X_train)\ny_test_pred = ridge.predict(X_test)\n\n#plot residues\nplt.scatter(y_train_pred, y_train_pred - y_train, c = 'blue', marker = 's', label = 'train set')\nplt.scatter(y_test_pred, y_test_pred - y_test, c = 'green', marker = 's', label= 'validation set')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residues')\nplt.title('OLS')\nplt.legend(loc = 'upper left')\nplt.hlines(y = 0, xmin=0, xmax=1, color='red')\nplt.show()\n\n#plot pred vs. actual values\nplt.scatter(y_train_pred, y_train, c = 'blue', marker='s', label='train set')\nplt.scatter(y_test_pred, y_test, c='green', marker='s', label='Validation set')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('OLS')\nplt.plot([0,1], [0,1], c='red')\nplt.show()\n\n#Actual RMSE:\nprint('Actual RMSE on Training set: ', mean_squared_error(y_train_pred, y_train, squared=False))\nprint('Actual RMSE on Test set: ', mean_squared_error(y_test_pred, y_test, squared=False))\n\n# Coefficients\ncoefs = pd.Series(lr.coef_, index=X_train.columns)\ncoefs.plot(kind='barh')","38e31a5b":"\nlasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)","19531604":"print(\"Lasso RMSE on Training set :\", rmse_cv_train(lasso).mean())\nprint(\"Lasso RMSE on Test set :\", rmse_cv_test(lasso).mean())\ny_train_las = lasso.predict(X_train)\ny_test_las = lasso.predict(X_test)","4f61d76c":"elasticNet = ElasticNetCV(l1_ratio = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n                                    0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Try again for more precision with l1_ratio centered around \" + str(ratio))\nelasticNet = ElasticNetCV(l1_ratio = [ratio * .85, ratio * .9, ratio * .95, ratio, ratio * 1.05, ratio * 1.1, ratio * 1.15],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Now try again for more precision on alpha, with l1_ratio fixed at \" + str(ratio) + \n      \" and alpha centered around \" + str(alpha))\nelasticNet = ElasticNetCV(l1_ratio = ratio,\n                          alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n                                    alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, \n                                    alpha * 1.35, alpha * 1.4], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"ElasticNet RMSE on Training set :\", rmse_cv_train(elasticNet).mean())\nprint(\"ElasticNet RMSE on Test set :\", rmse_cv_test(elasticNet).mean())","5d66655b":"models = [lr, ridge, lasso, elasticNet]\nfor model in models:\n    #actual predictions\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    print('Actual RMSE on Training set: ', mean_squared_error(y_train_pred, y_train, squared=False))\n    print('Actual RMSE on Test set: ', mean_squared_error(y_test_pred, y_test, squared=False))\n    #Coefficients\n    coefs = pd.Series(model.coef_, index=X_train.columns)\n    coefs.plot(kind='barh')\n    plt.show()\n    ","f398b508":"We see that GRE Score, TOEFL Score, and CGPA, Chance of Admit should be treated as numerical, and others categorical","2fc9cb16":"Conclusion:\n\nAll variables seems to be in linear relationship with DV. I used four different linear models, however, they are all very similar to each other. DV is strongly related with CGPA, TOEFL, and GRE.","f6f98068":"We see that they are very much linear with each other, and there not not too many outliers. It seems like there is a strong correlation between all three of these variables and Chance of Admit, as well as each other. If there are more than just 7 variables, at this point I might consider deleting or just create a new feature that is the mean of the three features (after normalizing ofc). But since we have soo little variables to begin with, I will just leave is this way.","448e1b36":"We don't need to do the same for y because it is linear regression.\nNow let's implement some models!\nNote that there seems to be a roughly linear dependence between all. \nThus we will only use linear regressions: OLS, L^2, L^1, L^2+ L^1 in this notebook. Of course there are plenty more.\nFirst we bring in the models:","bbc95660":"It is a little skewed, but not that much. Thus I will just keep it that way.\nThe next thing we will do is to take a look at the other variables, how they relate to the target variable and each other. \nFirst we will have to separate the numeric features and categorical features:","217b73f7":"Note that the graph is alomst identical. This tells us that the regularization is not really doing that much.\nLet's try Ridge and Elastic Net, however, probably they will give every similar answers. \nThis time we will just find the regularization parameters first, then we simply plot the coefficients of all four diagrams:","8386225c":"As none of them are that skewed, I will not log transform any of them. Now let's see their scatter plot:","8c0b657e":"Now let's do OLS: aka linear regression without regularization","b5c3649b":"Because Ridge L^2, Lasso L^1, and Elastic Net L^2 + L^1 all have regularization parameters, so we are going to use a error function with CV in them:","11167be5":"Boringly, there is no NA to fill. \n\nNow we need to add dummy variables, do this before splitting the data.","f86fbcd7":"Note that CGPA, TOEFL, and GRE score have strong coefficients. Others that stand out is FOR-1.5 and SOP-1.5. Interestingly for scores lower than that actually have less correlation, this might be because for those data they also have bad CGPA etc.\n\nNote that OLS is very sensitive to outliers, which is why we normally regularised it. \nLet's do it, the first one is L^2 (ridge). As L^2 has a regularization parameter, we need to choose it first:\nfor this step we follow (...)","102dbac2":"Now we normalize the numerical variables. Note that we do this after splitting because this is more accurate. In real world you don't get to combine all your train and test data and then normalize and then run your model. Get dummy variables before, but standarize after.","40614e15":"Now for the categorical features. Note that these categorical features are kind of special, because for example, university rating greater means better. This is not true for general categorical features, such as location. The truth is these features are categorical because they are discrete, not because they are not ordered. Thus we expect them to still have positive correlation with the Chance of Admit variable.\n\nFirst we look at their distributions.","16c78daa":"Now we split the data:","8b93dea2":"Remember that the RMSE is scaled-dependent! That is, it has the same unit as the dependent varible (DV). Thus the RMSE should be viewed in the same scale as DV. Since we didn't normalize our DV, on first glance we can't tell if this is good or bad. However, we know our DV, 'Chance of Admit ''s scale is from 0 to 1 (but in actuality from 0.4-1), we can get a rough sense of what the RMSE gives. \n\nNow let's take a look at the coefficients:","e79540ec":"As noted before, greater numbers does mean better chances of admission.\n\nThe last thing I want to do before moving on to fill in NA is just to look at the correlation between all features.","99557be1":"Next we look at their box plots with 'Chance of Admit ':","5b5cc3c1":"We see that all 4 of them give similar answers. Especially OLS and L^2, L^1 and ElasticNet. \nI think I will end the modellings here. Of course there are many many more models to try. But for my first step I will just end it here.","0bf0fc83":"We see that the thing that correlates with chance of admit the most is --- chance of admit. Other than that, the ones that stood out is CGPA, GRE score, and TOEL score. Again, not a surprise there.\n\nNow let's move on to fill in NA:","0a322679":"Now we take a look at how the numerical variables behave. Are they normal? Are they skewed?","951489b3":"# Graduate admisssion.\n\nThis is my first notebook. Since I am currently applying to graduate school myself, it is fitting that I choose this data. I will do a simple case of looking at linear regression models. I will use Linear regression, L^2(Ridge), L^1(Lasso), and L^2 + L^1 elastic net. There is an emphasizes on doing basic data preprocessing stuff.\nAny comments are of course welcome!\n\nAcknowledgement:\nhttps:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python Comprehensive data exploration with Python.\n","0f2f799a":"We see that the target is bounded from 0 to 1, which makes sense. It also has a little lump. Let's check if is it skew (to the left) much? If so, we need to log transform it:"}}