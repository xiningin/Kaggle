{"cell_type":{"0e9a98e1":"code","bff59861":"code","df050560":"code","52d3c83c":"code","7b2e6ada":"code","48492c9c":"code","0b46de64":"code","68023e07":"code","46bbb812":"code","c691194f":"code","8319b4c6":"code","db30d266":"code","c9c045d5":"markdown"},"source":{"0e9a98e1":"!pip install gdown","bff59861":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport gdown\n","df050560":"dataset = keras.preprocessing.image_dataset_from_directory(\n    \"..\/input\/animefacedataset\", label_mode=None, image_size=(64, 64), batch_size=128\n)\ndataset = dataset.map(lambda x: x \/ 255.0)","52d3c83c":"for x in dataset:\n    plt.axis(\"off\")\n    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n    break","7b2e6ada":"discriminator = keras.Sequential(\n    [\n        keras.Input(shape=(64, 64, 3)),\n        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        # layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n        # layers.LeakyReLU(alpha=0.2),\n        layers.Flatten(),\n        layers.Dropout(0.2),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ],\n    name=\"discriminator\",\n)\ndiscriminator.summary()","48492c9c":"latent_dim = 100\n\ngenerator = keras.Sequential(\n    [\n        keras.Input(shape=(latent_dim,)),\n        layers.Dense(8 * 8 * 128),\n        layers.Reshape((8, 8, 128)),\n        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(30, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n    ],\n    name=\"generator\",\n)\ngenerator.summary()","0b46de64":"class GAN(keras.Model):\n    def __init__(self, discriminator, generator, latent_dim):\n        super(GAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n\n    def compile(self, d_optimizer, g_optimizer, loss_fn):\n        super(GAN, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.loss_fn = loss_fn\n        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n\n    @property\n    def metrics(self):\n        return [self.d_loss_metric, self.g_loss_metric]\n\n    def train_step(self, real_images):\n        # Sample random points in the latent space\n        batch_size = tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n\n        # Decode them to fake images\n        generated_images = self.generator(random_latent_vectors)\n\n        # Combine them with real images\n        combined_images = tf.concat([generated_images, real_images], axis=0)\n\n        # Assemble labels discriminating real from fake images\n        labels = tf.concat(\n            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n        )\n        # Add random noise to the labels - important trick!\n        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n\n        # Train the discriminator\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(combined_images)\n            d_loss = self.loss_fn(labels, predictions)\n        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n        self.d_optimizer.apply_gradients(\n            zip(grads, self.discriminator.trainable_weights)\n        )\n\n        # Sample random points in the latent space\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n\n        # Assemble labels that say \"all real images\"\n        misleading_labels = tf.zeros((batch_size, 1))\n\n        # Train the generator (note that we should *not* update the weights\n        # of the discriminator)!\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(self.generator(random_latent_vectors))\n            g_loss = self.loss_fn(misleading_labels, predictions)\n        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n\n        # Update metrics\n        self.d_loss_metric.update_state(d_loss)\n        self.g_loss_metric.update_state(g_loss)\n        return {\n            \"d_loss\": self.d_loss_metric.result(),\n            \"g_loss\": self.g_loss_metric.result(),\n        }","68023e07":"class GANMonitor(keras.callbacks.Callback):\n    def __init__(self, num_img=3, latent_dim=100):\n        self.num_img = num_img\n        self.latent_dim = latent_dim\n\n    def on_epoch_end(self, epoch, logs=None):\n        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n        generated_images = self.model.generator(random_latent_vectors)\n        generated_images *= 255\n        generated_images.numpy()\n        for i in range(self.num_img):\n            img = keras.preprocessing.image.array_to_img(generated_images[i])\n            plt.imshow(generated_images[i])\n            img.save(\"generated_img_%03d_%d.png\" % (epoch, i))","46bbb812":"epochs = 50 # In practice, use ~100 epochs\n\ngan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\ngan.compile(\n    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    loss_fn=keras.losses.BinaryCrossentropy(),\n)\n","c691194f":"gan.fit(\n    dataset, epochs=epochs,\n)","8319b4c6":"test = generator.predict( tf.random.normal(shape=(2, latent_dim)))\n#52s 103ms\/step - d_loss: 0.5867 - g_loss: 1.2118","db30d266":"plt.imshow(test[0])","c9c045d5":"Following https:\/\/keras.io\/examples\/generative\/dcgan_overriding_train_step\/"}}