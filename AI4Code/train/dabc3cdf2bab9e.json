{"cell_type":{"e52a6f58":"code","76876624":"code","bb52a9ed":"code","9ffee808":"code","50dc68bd":"code","33da7505":"code","4a30b9a2":"code","9875e8fb":"code","7635db3e":"code","75795a2c":"code","f3aa7bd6":"code","749fc25d":"code","90253ca3":"code","381071ec":"code","e8a533a2":"code","fb0e3da6":"code","3c991dda":"code","32f7eb06":"code","eb0e314c":"code","395209dd":"code","a4f7af7e":"code","a715bb87":"code","5e911a1a":"code","948ff8a0":"code","9def9a19":"code","f6fe37f9":"code","43073d11":"code","21b389c5":"markdown","a409664c":"markdown","520978e1":"markdown","f490666b":"markdown","db7895c5":"markdown","0e3860e3":"markdown","991144e5":"markdown","5d01ec4b":"markdown","77231d72":"markdown","d3e5c306":"markdown","ab67c618":"markdown","cd564b2c":"markdown"},"source":{"e52a6f58":"# Basic\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport os\nimport random\nfrom tqdm.autonotebook import tqdm\nimport string\nfrom collections import Counter\nimport re\n\n# Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS\n\n# NLP\nimport spacy\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])","76876624":"data_dir = '..\/input\/shopee-product-matching'\n\ntrain_file_path = os.path.join(data_dir, 'train.csv')\ntest_file_path = os.path.join(data_dir, 'test.csv')\nsample_sub_file_path = os.path.join(data_dir, 'sample_submission.csv')\ntrain_images_path = os.path.join(data_dir, 'train_images')\ntest_images_path = os.path.join(data_dir, 'test_images')\n\nprint(f'Train file: {train_file_path}')\nprint(f'Test file: {test_file_path}')\nprint(f'Sample Sub file: {sample_sub_file_path}')\nprint(f'Train Imaes Path: {train_images_path}')\nprint(f'Test Images Path: {test_images_path}')","bb52a9ed":"RANDOM_SEED = 42","9ffee808":"def seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)","50dc68bd":"seed_everything()","33da7505":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)\nsub_df = pd.read_csv(sample_sub_file_path)","4a30b9a2":"train_df.sample(10)","9875e8fb":"train_df.shape","7635db3e":"train_df.nunique()","75795a2c":"def show_image(class_num, examples=2, train_df=train_df, train_images_path=train_images_path):\n    image_list = train_df[train_df['label_group'] == class_num]['image'].sample(frac=1)[:examples].to_list()\n    plt.figure(figsize=(20,10))\n    for i, img in enumerate(image_list):\n        full_path = os.path.join(train_images_path, img)\n        img = Image.open(full_path)\n        plt.subplot(1 ,examples, i%examples +1)\n        plt.axis('off')\n        plt.imshow(img)\n        plt.title(f'Class: {class_num}')","f3aa7bd6":"nums = random.sample(list(train_df.label_group.unique()), 3)\nfor num in nums:\n    show_image(num)","749fc25d":"nums = random.sample(list(train_df.label_group.unique()), 3)\nfor num in nums:\n    show_image(num)","90253ca3":"word_count = [len(x.split()) for x in train_df['title'].tolist()]\nbarplot_dim = (12, 6)\nax = plt.subplots(figsize =barplot_dim);\nax = sns.distplot(word_count, kde=False);\nax.set_ylabel('No. of Observations', size=15)\nax.set_xlabel('No. of Words', size=15)\nax.set_title('Title Word Count Distribution', size=20);","381071ec":"train_df.sample(10)","e8a533a2":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = str(text).lower()\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text","fb0e3da6":"tqdm.pandas()\ntrain_df['title'] = train_df['title'].progress_apply(text_cleaning)","3c991dda":"tqdm.pandas()\ntest_df['title'] = test_df['title'].progress_apply(text_cleaning)","32f7eb06":"word_count = [len(x.split()) for x in train_df['title'].tolist()]\nbarplot_dim = (12, 6)\nax = plt.subplots(figsize =barplot_dim);\nax = sns.distplot(word_count, kde=False);\nax.set_ylabel('No. of Observations', size=15)\nax.set_xlabel('No. of Words', size=15)\nax.set_title('Title Word Count Distribution', size=20);","eb0e314c":"temp_df = pd.DataFrame()\ntemp_df['temp_list'] = train_df['title'].apply(lambda x :str(x).split())\ntop = Counter([item for sublist in temp_df['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp.columns = ['Common Words', 'Count']\ntemp.style.background_gradient(cmap='Reds')","395209dd":"text = ' '.join(train_df['title'])\nwordcloud = WordCloud(background_color='white', stopwords=STOPWORDS, width=2560, height=1440).generate(text)\n\nbarplot_dim = (15, 15)\nax = plt.subplots(figsize=barplot_dim, facecolor='w')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","a4f7af7e":"def prepare_text(text, nlp=nlp):\n    '''\n    Returns the text after stop-word removal and lemmatization.\n    text - Sentence to be processed\n    nlp - Spacy NLP model\n    '''\n    doc = nlp(text)\n    lemma_list = [token.lemma_ for token in doc if not token.is_stop]\n    lemmatized_sentence = ' '.join(lemma_list)\n        \n    return lemmatized_sentence","a715bb87":"tqdm.pandas()\ntest_df['title'] = test_df['title'].progress_apply(prepare_text)","5e911a1a":"# from https:\/\/www.kaggle.com\/isaienkov\/shopee-data-understanding-and-analysis\n\nmask = test_df.groupby(['title']).count().reset_index()['title'].tolist()\na = []\nb = []\nfor item in mask:\n    res = test_df[test_df['title']== item]['posting_id'].tolist()\n    ans = ''\n    for id_item in res:\n        ans = ans + str(id_item) + ' '\n    ans = ans[:-1]\n    for id_item in res:\n        a.append(id_item)\n        b.append(ans)","948ff8a0":"submission = pd.DataFrame()\nsubmission['posting_id'] = a\nsubmission['matches'] = b\nsubmission.head()","9def9a19":"mapping_dict_phash = test_df.groupby('image_phash')['posting_id'].apply(list).to_dict()\ntest_df['matches_temp'] = test_df['image_phash'].map(mapping_dict_phash)\ntest_df['matches_temp'] = test_df['matches_temp'].apply(lambda x: ' '.join(x))\n\nsubmission_map = test_df[['posting_id', 'matches_temp']].set_index('posting_id').to_dict()['matches_temp']","f6fe37f9":"submission['matches_temp'] = submission['posting_id'].map(submission_map)\nsubmission['matches_temp'] = submission['matches_temp'] + ' ' + submission['matches']\nsubmission['matches_temp'] = submission['matches_temp'].apply(lambda x: x.split())\nsubmission['matches_temp'] = submission['matches_temp'].apply(lambda x: set(x))\nsubmission['matches'] = submission['matches_temp'].apply(lambda x: ' '.join(x))\nsubmission.drop('matches_temp', axis=1, inplace=True)\nsubmission.head()","43073d11":"submission.to_csv('submission.csv', index=False)","21b389c5":"**If you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels.** \ud83d\ude0a","a409664c":"Let's check the size of the dataset...","520978e1":"Some texts look messy and having irrelevant texts like special charecters... So let's clean that...","f490666b":"So we have 11014 unique groups...  \n\nWhat do similar images in same group look like?","db7895c5":"And number of unique values in each column...","0e3860e3":"# Training Data","991144e5":"# About this Notebook\nThis is a first run through the compeition to try and understand the datatset and realise the problem at hand.","5d01ec4b":"# Data Description\n\ntrain\/test.csv - the training set metadata. Each row contains the data for a single posting. Multiple postings might have the exact same image ID, but with different titles or vice versa.\n* `posting_id` - the ID code for the posting.\n* `image` - the image id\/md5sum.\n* `image_phash` - a perceptual hash of the image.\n* `title` - the product description for the posting.\n* `label_group` - ID code for all postings that map to the same product. Not provided for the test set.\n\ntrain\/test images - the images associated with the postings.\n\nsample_submission.csv - a sample submission file in the correct format.\n* `posting_id` - the ID code for the posting.\n* `matches` - Space delimited list of all posting IDs that match this posting. Posts always self-match. Group sizes were capped at 50, so there's no need to predict more than 50 matches.","77231d72":"Another feature we can consider is the phash feature... If both images have same phash we can naively assume them to be same... So let's take care of that too...","d3e5c306":"Got it... So there can be subtle differences in the image like background, test and stickers. But the main product to remain exactly same. But the image can be upside down or a different angle. This looks like an interesting problem...  \n\nLet's see what the titles tell us...","ab67c618":"# Naive Model\nLet's assume the products with the exact same title are similar products. So let's group them basis that...","cd564b2c":"Let's see some of the most commonly used words..."}}