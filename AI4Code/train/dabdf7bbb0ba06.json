{"cell_type":{"2fa8a61f":"code","61819d07":"code","337e2d23":"code","ecca09a6":"code","0d92f64c":"code","e6cbe875":"code","fefae81f":"code","c9ae2940":"code","17e4eba7":"code","d42f4380":"code","9413d58f":"code","16f5e2e8":"code","9fa47d61":"code","fde7f3ba":"markdown","2977a9e2":"markdown","56e829ae":"markdown","ab9d40f4":"markdown"},"source":{"2fa8a61f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","61819d07":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n# Encode categorical feature 'Sex'\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ntrain['Sex'] = labelencoder.fit_transform(train['Sex'])\ntest['Sex'] = labelencoder.fit_transform(test['Sex'])\n\n# Fill AgeNA with mean\ntrain = train.fillna(29.7)\ntest = test.fillna(29.7)\n\n# get data for limited list of features only\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n\n# get training and test data\nX = train[features]\ny = train['Survived']\n\n# Get Test and Training data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n","337e2d23":"# Check if missing values\ntrain.describe()","ecca09a6":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(random_state=0, solver='lbfgs', \n                         multi_class='multinomial').fit(X_train, y_train)\nlogreg.score(X_test, y_test)","0d92f64c":"# Naive Bayes Classifier\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB().fit(X_train, y_train)\ngnb.score(X_test, y_test)","e6cbe875":"# Support Vector Machines\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='linear').fit(X_train, y_train)\nsvc.score(X_test, y_test)","fefae81f":"# Decision Trees\nfrom sklearn import tree\ndtc = tree.DecisionTreeClassifier().fit(X_train, y_train)\ndtc.score(X_test, y_test)","c9ae2940":"# Boosted Trees\nfrom sklearn import ensemble\ngbc = ensemble.GradientBoostingClassifier().fit(X_train, y_train)\ngbc.score(X_test, y_test)","17e4eba7":"# Random Forest\nfrom sklearn import ensemble\nrfc = ensemble.RandomForestClassifier().fit(X_train, y_train)\nrfc.score(X_test, y_test)","d42f4380":"# Neural Networks\nfrom sklearn import neural_network\nmlpc = neural_network.MLPClassifier().fit(X_train, y_train)\nmlpc.score(X_test, y_test)","9413d58f":"# Nearest Neighbor\nfrom sklearn import neighbors\nknn = neighbors.KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)\nknn.score(X_test, y_test)","16f5e2e8":"# What if we merge all the results in one table?\nlogreg_result = logreg.predict(test[features])\ngnb_result = gnb.predict(test[features])\nsvc_result = svc.predict(test[features])\ndtc_result = dtc.predict(test[features])\ngbc_result = gbc.predict(test[features])\nrfc_result = rfc.predict(test[features])\nmlpc_result = mlpc.predict(test[features])\nknn_result = knn.predict(test[features])\n\nOutputMatrix = pd.concat([test['PassengerId'],\n                          pd.DataFrame(logreg_result),\n                          pd.DataFrame(gnb_result),\n                          pd.DataFrame(svc_result),\n                          pd.DataFrame(dtc_result),\n                          pd.DataFrame(gbc_result),\n                          pd.DataFrame(rfc_result),\n                          pd.DataFrame(mlpc_result),\n                          pd.DataFrame(knn_result)],\n                         axis=1)\n\nOutputMatrix.columns = ['PassengerId', 'LogReg', 'Gaussian', 'SVC', 'Tree', 'BoostedTree', 'RandomForest', 'NeuralNet', 'KNN']\nOutputMatrix['Survived'] = round((OutputMatrix['LogReg']\n                                   + OutputMatrix['Gaussian']\n                                   + OutputMatrix['SVC']\n                                   + OutputMatrix['Tree']\n                                   + OutputMatrix['BoostedTree']\n                                   + OutputMatrix['RandomForest']\n                                   + OutputMatrix['NeuralNet']\n                                   + OutputMatrix['KNN']) \/ 8)\n\nOutputResult = OutputMatrix[['PassengerId', 'Survived']].astype('int64')\n\n# OutputResult.head()\nOutputResult.to_csv('Consensus Classifier output.csv', index = False)","9fa47d61":"# Best is boosted trees\n# clf = ensemble.GradientBoostingClassifier().fit(X_train, y_train)\n# y_predicted = clf.predict(test[features])\n\n# OutputResult = pd.concat([test['PassengerId'], pd.DataFrame(y_predicted)], axis=1)\n# OutputResult.columns = ['PassengerId', 'Survived']\n# OutputResult.to_csv('Boosted Trees Classifier output.csv', index = False)\n","fde7f3ba":"# Testing the different classification algos\nThe goal of this notebook is to test the different classifiers, with very simple data input and test the difference in output.\nThis is based on [this article](https:\/\/medium.com\/@sifium\/machine-learning-types-of-classification-9497bd4f2e14)\n\nThere is a twist though, once I got the output of the different Algo I wondered if I can get the average suggested value to see if this would be better. The intuition being that the consensus solution would be better than any single individual.\n\nSo here it goes (spoiler alert, it isn't).\n\nFirst we import the data and do super basic clean up (fillna and encode some values to be processed by the models).","2977a9e2":"As you can see, the boosted tree ended up with the best score (and it is what I submitted).\n\n\nBut what if we create a new version based on the consensus of all the models, would that improve the score?\n\nI applied a simple logic of getting the average of the scores and rounding it up to the nearest digit to have 0 or 1.","56e829ae":"We should now be ready to apply every model from the article mentioned and see the score associated.\nThe logic is really similar between models so you can just apply the same logic as is.","ab9d40f4":"Turns out, the consensus model is not better in this case.\n\nI am curious though if this is specific to this case or a more generalized rule.\n\nLet me know if you have any comments or feedback, I am trying to learn ML more and appreciate any feedback to make me better at it! :D"}}