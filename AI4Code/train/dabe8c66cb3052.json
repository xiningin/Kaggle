{"cell_type":{"b4e11362":"code","61535534":"code","8067a488":"code","408b1fc1":"code","2a9ec45b":"code","ff7e89a5":"code","59ff6d1e":"code","a5d90b62":"code","c27b2eaf":"code","8acf62f8":"code","b7944bae":"code","bcefae98":"code","00d20034":"code","4da5305d":"code","883c1c45":"code","f2224b10":"code","cecf4466":"code","9243ef26":"code","eb19027c":"code","f33a7bea":"code","6552b120":"code","da310e73":"code","b7d33783":"code","90ce9a01":"code","3389e564":"code","473137cb":"code","30b62f8a":"markdown","ed464342":"markdown","20680d89":"markdown","bd1c855d":"markdown","04319570":"markdown","a2b3173c":"markdown","cda276c7":"markdown","cb4dbe9e":"markdown","4aa537d9":"markdown","28c0ead5":"markdown","adab7f41":"markdown","b8ac769d":"markdown","31f16aa1":"markdown","041cc19e":"markdown","c26c6bfa":"markdown","1b3f610a":"markdown","4e567e24":"markdown"},"source":{"b4e11362":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","61535534":"dt = pd.read_csv(\"..\/input\/heart.csv\")","8067a488":"dt.head(10)","408b1fc1":"dt.isnull().sum()","2a9ec45b":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = dt\nX_test_full = dt\ntest=dt\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['target'], inplace=True)\ny = X.target              \nX.drop(['target'], axis=1, inplace=True)\n  \n\n    \n    # Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n# low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n#                         X_train_full[cname].dtype == \"object\"]\n\n# # Select numeric columns\n# numeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# # Keep selected columns only\n# my_cols = low_cardinality_cols + numeric_cols\n# X_train = X_train_full[my_cols].copy()\n# X_valid = X_valid_full[my_cols].copy()\n\n# One-hot encode the data\n","ff7e89a5":"len(X.columns)","59ff6d1e":"len(X_train_full.columns)","a5d90b62":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score","c27b2eaf":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n#from xgboost import XGBRegressor\n\n\nmodel2 = RandomForestClassifier(n_estimators=150, max_depth=4, random_state=1)\nmodel = GradientBoostingClassifier(random_state=1)\n#model = DecisionTreeClassifier(random_state=1)\n#model=SGDClassifier(random_state=1)\n#model=ExtraTreesClassifier(random_state=1)\n#model = XGBRegressor()\n\n\n\nmodel = model.fit(X_train_full, y_train)\npredictions = model.predict(X_valid_full)\n\nprint('model accuracy score',accuracy_score(y_valid, predictions))\n\n#eventuelle problemer kan opst\u00e5 n\u00e5r modellen tr\u00e6ner p\u00e5 samme dataset, som der bliver lavet prodictions p\u00e5.","8acf62f8":"model2 = model2.fit(X_train_full, y_train)\npredictions = model2.predict(X_valid_full)\nprint('model accuracy score',accuracy_score(y_valid, predictions))","b7944bae":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors =3) #n_neighbors Number of neighbors to use\nneigh.fit(X_train_full, y_train)\npredictions = neigh.predict(X_valid_full)\n\nprint('model accuracy score',accuracy_score(y_valid, predictions))","bcefae98":"X_train_full.shape","00d20034":"from sklearn.feature_selection import SelectKBest, chi2\nX_new = SelectKBest(chi2, k =10 ).fit_transform(X_train_full, y_train)\nX_new_test = SelectKBest(chi2, k =10 ).fit_transform(X_valid_full, y_valid)","4da5305d":"X_new.shape","883c1c45":"neigh = KNeighborsClassifier(n_neighbors =3)\nneigh.fit(X_new, y_train)\npredictions = neigh.predict(X_new_test)\nprint('model accuracy score',accuracy_score(y_valid, predictions))","f2224b10":"from sklearn.cluster import KMeans \nKmean = KMeans(n_clusters =9, random_state = 0).fit(X_train_full)\npredictions = Kmean.predict(X_valid_full)\n\nprint('model accuracy score',accuracy_score(y_valid, predictions))","cecf4466":"Kmean.cluster_centers_","9243ef26":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=90)\nkf.get_n_splits(dt)","eb19027c":"KFold(n_splits=90, random_state=0, shuffle=True)\nfor train_index, test_index in kf.split(dt):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]","f33a7bea":"neigh = KNeighborsClassifier(n_neighbors = 2) #n_neighbors Number of neighbors to use\nneigh.fit(X_train, y_train)\npredictions = neigh.predict(X_test)\n\nprint('model accuracy score',accuracy_score(y_test, predictions))","6552b120":"import pandas as pd\ndf = pd.read_csv(\"..\/input\/heart.csv\")","da310e73":"X=df.copy()\nX.dropna(axis=0, subset=['target'], inplace=True)\ny = X.target              \nX.drop(['target'], axis=1, inplace=True)","b7d33783":"from sklearn.decomposition import PCA\npca = PCA(n_components='mle') # how many features you want mle means it wil atomatically find the best amount for you\npca_train = pca.fit_transform(X)\nX_df_pca = pd.DataFrame(data = pca_train)                     ","90ce9a01":"from sklearn.model_selection import train_test_split \n    # Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_df_pca, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","3389e564":"X_train_full.head()","473137cb":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nneigh = KNeighborsClassifier(n_neighbors =3) #n_neighbors Number of neighbors to use\nneigh.fit(X_train_full, y_train)\npredictions = neigh.predict(X_valid_full)\n\nprint('model accuracy score',accuracy_score(y_valid, predictions))","30b62f8a":"![image.png](attachment:image.png)","ed464342":"selects K best parameters to use","20680d89":"# Model\/Predictions","bd1c855d":"# Data visualisering","04319570":"# K-Nearest Neighbours Classifier","a2b3173c":"# Data cleaning","cda276c7":"<a id='section2'><\/a>","cb4dbe9e":"# Data importering","4aa537d9":"# PCA","28c0ead5":"PCA is Principal komponent analyse is to reduce dimentuallity for making simpler model and reduce overfitting. In all my models this has given a few procent better accuracy \n## Remember look up dkumentation for extra ticks and better understadning when using PCA its  scikit-learn i used here","adab7f41":"Reset run time here to make PCA to work","b8ac769d":"# K means cluster","31f16aa1":"# Diagnosing Heart Disease\n","041cc19e":"# KFold","c26c6bfa":"<a id='section1'><\/a>","1b3f610a":"# SelectKBest","4e567e24":"# This kernel was made for educational purposes and for fun therefore the content of this kernel should not be taken seriouly!"}}