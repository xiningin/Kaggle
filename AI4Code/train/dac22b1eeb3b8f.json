{"cell_type":{"94533752":"code","58b4ddbf":"code","2323dce8":"code","f2c56fa1":"code","36299275":"code","bc69f289":"code","ab3ce4f2":"code","75493a7e":"code","4c0b1568":"code","abb3be4c":"code","e84e0e9d":"code","01b60d71":"markdown"},"source":{"94533752":"import pandas as pd\nimport numpy as np\nimport time\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom hyperopt.pyll.base import scope\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler","58b4ddbf":"train = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/test.csv\")\n\n\n# The test dataset doesn't have \"G\" value in cat6 column\ntrain = train[train['cat6'] != 'G']\ntarget = train.target","2323dce8":"train.head()","f2c56fa1":"def preprocess(df, encoder=None,\n               scaler=None, cols_to_drop=None,\n               cols_to_encode=None, cols_to_scale=None):\n    \"\"\"\n    Preprocess input data\n    :param df: DataFrame with data\n    :param encoder: encoder object with fit_transform method,\n                    or dummies value for pd.get_dummies method\n    :param scaler: scaler object with fit_transform method\n    :param cols_to_drop: columns to be removed\n    :param cols_to_encode: columns to be encoded\n    :param cols_to_scale: columns to be scaled\n    :return: DataFrame\n    \"\"\"\n\n    if encoder:\n        if encoder == 'dummies':\n            for col in cols_to_encode:\n                encoded = pd.get_dummies(df[col], prefix='dummy_' + col)\n\n            df = df.drop(cols_to_encode, axis=1)\n            df = df.join(encoded)\n        else:\n            for col in cols_to_encode:\n                df[col] = encoder.fit_transform(df[col].values.reshape(-1, ))\n\n    if scaler:\n        for col in cols_to_scale:\n            df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n\n    if cols_to_drop:\n        df = df.drop(cols_to_drop, axis=1)\n\n    return df","36299275":"cat_cols = ['cat' + str(i) for i in range(10)]\ncont_cols = ['cont' + str(i) for i in range(14)]\n\ntrain = preprocess(train, scaler=StandardScaler(), encoder=LabelEncoder(),\n                   cols_to_drop=['id', 'target'], cols_to_encode=cat_cols,\n                   cols_to_scale=cont_cols)\n\ntest = preprocess(test, scaler=StandardScaler(), encoder=LabelEncoder(), \n                  cols_to_encode=cat_cols, cols_to_drop=['id'], \n                  cols_to_scale=cont_cols)","bc69f289":"class EnsembleModel:\n    def __init__(self, params):\n        \"\"\"\n        LGB + XGB model\n        \"\"\"\n        self.lgb_params = params['lgb']\n        self.xgb_params = params['xgb']\n\n        self.lgb_model = LGBMRegressor(**self.lgb_params)\n        self.xgb_model = XGBRegressor(**self.xgb_params)\n\n    def fit(self, x, y, *args, **kwargs):\n        return (self.lgb_model.fit(x, y, *args, **kwargs),\n                self.xgb_model.fit(x, y, *args, **kwargs))\n\n    def predict(self, x, weights=[1.0, 1.0]):\n        \"\"\"\n        Generate model predictions\n        :param x: data\n        :param weights: weights on model prediction, first one is the weight on lgb model\n        :return: array with predictions\n        \"\"\"\n        return (weights[0] * self.lgb_model.predict(x) +\n                weights[1] * self.xgb_model.predict(x)) \/ 2","ab3ce4f2":"# ensemble_params = {\n#     \"lgb\" : {\n#         \"num_leaves\": scope.int(hp.quniform(\"num_leaves\", 31, 200, 1)),\n#         \"max_depth\": scope.int(hp.quniform(\"max_depth\", 10, 24, 1)),\n#         'learning_rate': hp.uniform('learning_rate', 0.01, 0.05),\n#         'min_split_gain': hp.uniform('min_split_gain', 0, 1.0),\n#         'min_child_samples': scope.int(hp.quniform(\"min_child_samples\", 2, 700, 1)),\n#         \"subsample\": hp.uniform(\"subsample\", 0.2, 1.0),\n#         \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n#         'reg_alpha': hp.uniform('reg_alpha', 1e-5, 1.0),\n#         'reg_lambda': hp.uniform('reg_lambda', 0, 50),\n#         'n_jobs': -1,\n#         'n_estimators': 2000},\n#     'xgb': {\n#         'max_depth': scope.int(hp.quniform('xgb.max_depth', 10, 24, 1)),\n#         'learning_rate': hp.uniform('xgb.learning_rate', 0.01, 0.05),\n#         'gamma': hp.uniform('xgb.gamma', 1, 10),\n#         'min_child_weight': scope.int(hp.quniform('xgb.min_child_weight', 2, 700, 1)),\n#         'n_estimators': 2000,\n#         'colsample_bytree': hp.uniform('xgb.colsample_bytree', 0.5, 0.9),\n#         'subsample': hp.uniform('xgb.subsample', 0.5, 1.0),\n#         'reg_lambda': hp.uniform('xgb.reg_lambda', 0, 100),\n#         'reg_alpha': hp.uniform('xgb.reg_alpha', 1e-5, 0.5),\n#         'objective': 'reg:squarederror',\n#         'tree_method': 'gpu_hist',\n#         'n_jobs': -1}\n# }\n\n# def ensemble_search(params):\n#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22)\n\n#     model = EnsembleModel(params)\n\n#     evaluation = [(X_test, y_test)]\n\n#     model.fit(X_train, y_train,\n#               eval_set=evaluation, eval_metric='rmse',\n#               early_stopping_rounds=100, verbose=False)\n\n#     val_preds = model.predict(X_test)\n#     rmse = mean_squared_error(y_test, val_preds, squared=False)\n\n#     return {\"loss\": rmse, \"status\": STATUS_OK}","75493a7e":"# X = train.copy()\n# y = target\n\n# trials = Trials()\n\n# best_hyperparams = fmin(fn=ensemble_search,\n#                        space=ensemble_params,\n#                        algo=tpe.suggest,\n#                        max_evals=100,\n#                        trials=trials)","4c0b1568":"# best_hyperparams","abb3be4c":"# All params taken from previous version\n\nsince = time.time()\ncolumns = train.columns\n\nensemble_params = {\n    \"lgb\" : {\n        \"num_leaves\": 36,\n        \"max_depth\": 21,\n        'learning_rate': 0.049019854828962754,\n        'min_split_gain': 0.2579555416739361,\n        'min_child_samples': 500,\n        \"subsample\": 0.2595537456780356,\n        \"colsample_bytree\": 0.6203517996970486,\n        'reg_alpha': 0.33867231210286647,\n        'reg_lambda': 42.071411120949854,\n        'n_jobs': -1,\n        'n_estimators': 10000},\n    'xgb': {\n        'max_depth': 13,\n        'learning_rate': 0.020206705089028228,\n        'gamma': 3.5746731812451156,\n        'min_child_weight': 564,\n        'n_estimators': 10000,\n        'colsample_bytree': 0.5015940592112956,\n        'subsample': 0.6839489639112909,\n        'reg_lambda': 18.085502002853246,\n        'reg_alpha': 0.17532087359570606,\n        'objective': 'reg:squarederror',\n        'tree_method': 'gpu_hist',\n        'n_jobs': -1}\n}\n    \npreds = np.zeros(test.shape[0])\nkf = KFold(n_splits=10, random_state=22, shuffle=True)\nrmse = []\nn = 0\n\nfor trn_idx, test_idx in kf.split(train[columns], target):\n\n    X_tr, X_val=train[columns].iloc[trn_idx], train[columns].iloc[test_idx]\n    y_tr, y_val=target.iloc[trn_idx], target.iloc[test_idx]\n\n    model = EnsembleModel(ensemble_params)\n\n    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], early_stopping_rounds=150, verbose=False)\n\n    preds += model.predict(test[columns], [0.5, 1.5]) \/ kf.n_splits\n    rmse.append(mean_squared_error(y_val, model.predict(X_val, [0.5, 1.5]), squared=False))\n    \n    print(f\"Fold {n+1}, RMSE: {rmse[n]}\")\n    n += 1\n\n\nprint(\"Mean RMSE: \", np.mean(rmse))\nend_time = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(\n        end_time \/\/ 60, end_time % 60))","e84e0e9d":"ss = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv\")\nss['target'] = preds\n\nss.to_csv(\"ensemble_model_5.csv\", index=False)","01b60d71":"ver. 1.0 - Mean RMSE (10 fold) - 0.84297; Public lb - 0.84308  \nver. 2.0 - Mean RMSE (10 fold) - **0.84280**; Public lb - 0.84291  \nver. 3.0 - Mean RMSE (10 fold) - 0.84292; Public lb - 0.84290  \nver. 4.0 - Mean RMSE (10 fold) - 0.84290; Public lb - **0.84287**"}}