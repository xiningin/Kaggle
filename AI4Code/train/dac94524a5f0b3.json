{"cell_type":{"cde3097f":"code","b727f913":"code","01ff9bb7":"code","8af29323":"code","c1062cf2":"code","1b375200":"code","a0859bc3":"code","29c5bf97":"code","3f78b2dc":"code","0c13d05d":"code","a1bcd7d2":"code","5e5fc23f":"code","8fbc5e42":"code","b508f508":"code","f1ab7de0":"code","f01c6d40":"code","7341dced":"code","c453b42a":"code","b00d73f4":"code","647a43c8":"code","e105ae79":"code","64306b49":"code","bffc201c":"code","6d6a2ed1":"code","756bb8fc":"code","3f2119bc":"code","c22b2007":"code","c93142f0":"code","a8ddbb29":"code","175b24b5":"code","e352f180":"code","7d39d72a":"code","565f0695":"code","2cc3c2be":"code","a6e2dcf1":"code","b9a96911":"code","7fc44a42":"code","eebab851":"code","7272a24a":"code","d0685427":"code","46fe2dff":"code","1ffaa0df":"code","572862ea":"code","5ba4117f":"code","8fbd22b7":"code","fad8fa85":"code","cb90c1ed":"code","5ebf8110":"code","c5b043a3":"code","dcd5e403":"code","be68b13c":"code","cea101cc":"code","b1fe4ee7":"code","050d8afb":"code","8ec6c8f7":"code","76a4b1ee":"code","591f4ef8":"code","a7a9fd8d":"code","fd76cf61":"code","43a09725":"code","824a118c":"code","edbfba79":"code","16b528d3":"code","3f49fea2":"code","c946b043":"code","2889a4e5":"code","542b8735":"code","cda3bebe":"code","7eddb3fc":"code","1ee22903":"code","3c273316":"code","af230f99":"code","7f89ac57":"code","aa09afc6":"code","bacd0f99":"code","3ffb983d":"code","3f48a00b":"code","bbc4f969":"code","15217ea2":"code","c60089bd":"code","0f191dde":"code","11bbda78":"code","9770929e":"code","afee7b99":"code","aeddc705":"code","90c12a9c":"code","d9302026":"code","e9eb6457":"code","87dacbcc":"code","e4fbbcb1":"code","466c69e2":"code","4005ed05":"code","c428d1dc":"code","84e88771":"code","e63546c1":"code","316feb4d":"code","eee94936":"code","9843413b":"code","b47ef51a":"code","ac98e9aa":"code","ded0df18":"code","5fed7e4b":"code","b37dd189":"code","809efdbf":"code","f09aa624":"code","5bc48d75":"code","d8cf62c8":"code","a60e01eb":"code","4283def5":"code","476ef982":"code","c71725c7":"markdown","b8e2e50f":"markdown","c6f2092d":"markdown","ed79f249":"markdown","24f87c33":"markdown","7c18ff78":"markdown","6f91e098":"markdown","03533c4a":"markdown","01bd5184":"markdown","cc931ee5":"markdown","f5507349":"markdown","6d288a24":"markdown","1f082aa0":"markdown","b6280c38":"markdown","b8e1091a":"markdown","9ee52e20":"markdown","aa5517f9":"markdown","6482b5f0":"markdown","38c5cae2":"markdown","dfe94eb0":"markdown","95371989":"markdown","b4544786":"markdown","91481046":"markdown","bcef1218":"markdown","a59b13cd":"markdown","b35f553d":"markdown","9c99f525":"markdown","ca2ef79f":"markdown","1742a3f9":"markdown","fd5eda19":"markdown","b931f6fa":"markdown","422422a3":"markdown","4738905b":"markdown","c0cdbddb":"markdown","385e6a22":"markdown","2d610a6f":"markdown","b2978858":"markdown","006e1ea9":"markdown","ea19ca9a":"markdown","22bcd07c":"markdown","96d89b09":"markdown","83c7d8b3":"markdown","f4370d9c":"markdown","572dbdbc":"markdown","3029ccf0":"markdown","83cb1123":"markdown","99ba740c":"markdown","ec4258fc":"markdown","97e25880":"markdown","7d262832":"markdown","d130ed76":"markdown","a87e1a36":"markdown","33711b9a":"markdown","702f9c62":"markdown","7e00df26":"markdown","f8ae6f11":"markdown","b38aafd3":"markdown","77efb876":"markdown","1c6fa538":"markdown","37eefcdb":"markdown","32685a9c":"markdown","9b3008d3":"markdown","90c9e116":"markdown","54d730eb":"markdown","76d58244":"markdown","352ed7a5":"markdown","a671f8ce":"markdown","17896e83":"markdown","e83cb366":"markdown","63e970cc":"markdown","a493f2e2":"markdown","882c5295":"markdown","320c4a3a":"markdown","abf3bcf1":"markdown","1b465386":"markdown","69e834e4":"markdown","601da377":"markdown","e11485e1":"markdown","d7ea9068":"markdown","8dba7b2e":"markdown","9a633699":"markdown","261e38c0":"markdown","a9a2407e":"markdown","bac2e223":"markdown","d45eba1d":"markdown","07f45d25":"markdown","09a5fee4":"markdown","1f94ae91":"markdown","b999bb57":"markdown","e2979358":"markdown","6796c9f6":"markdown","0a6dfcc1":"markdown","a2c3ca9f":"markdown"},"source":{"cde3097f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b727f913":"#Importing the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For splitting \nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n\n# For Feature Selection\nfrom mlxtend.feature_selection import SequentialFeatureSelector\nfrom sklearn.feature_selection import RFECV\n\n# Modeling & Accuracy Metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# For HyperParameter Tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')","01ff9bb7":"#Reading Train data set\ndftrain = pd.read_csv(\"\/kaggle\/input\/hranalysis\/train.csv\")\ndftrain.head()","8af29323":"#Reading Test data set\ntest=pd.read_csv(\"\/kaggle\/input\/hranalysis\/test.csv\")\ntest.head()","c1062cf2":"#Shapes of both the data sets\nprint(\"Original Train Set Shape : \",dftrain.shape)\nprint(\"Test Set Shape :  \",test.shape)","1b375200":"dftrain.describe()","a0859bc3":"#checking for duplicate rows\ndupl=dftrain[dftrain.duplicated()]\ndupl.shape","29c5bf97":"#Data types of features\ndftrain.dtypes","3f78b2dc":"#Unique values summary of total data set\ndftrain.nunique()","0c13d05d":"#Null values summary of total data set\ndftrain.isnull().sum()","a1bcd7d2":"dftrain['previous_year_rating'].fillna(0,inplace=True)\ndftrain['previous_year_rating'].isnull().sum()","5e5fc23f":"dftrain[dftrain['is_promoted']==1].shape","8fbc5e42":"dftrain[dftrain.education.isnull()].is_promoted.sum()","b508f508":"#Dropping records with education=NULL\ndftrain=dftrain.dropna()\ndftrain.shape #2409 records dropped","f1ab7de0":"dftrain.isnull().sum()","f01c6d40":"test.isnull().sum()","7341dced":"test['previous_year_rating'].fillna(0,inplace=True)\ntest['previous_year_rating'].isnull().sum()","c453b42a":"#Dropping records with education=NULL\ntest=test.dropna()\ntest.shape #1034 records dropped","b00d73f4":"#Splitting dataset into train set and validation set\ntrain,val=train_test_split(dftrain,test_size=0.30,random_state=1,stratify=dftrain.is_promoted)","647a43c8":"print(\"After Splitting: \\n\")\nprint(\"Train set shape : \\t\",train.shape)\nprint(\"Validation set shape : \\t\",val.shape)\nprint(\"\\nTarget Variable in Original Data :\\t\",Counter(dftrain.is_promoted))\nprint(\"Target Variable in Train Data : \\t\",Counter(train.is_promoted))\nprint(\"Target Variable in Validation Data :\\t\",Counter(val.is_promoted))\n","e105ae79":"#Further splitting Validation set in Val1 and Val2 set for better analysis\nval1,val2=train_test_split(val,test_size=0.50,random_state=1,stratify=val.is_promoted)","64306b49":"print(\"After Further Splitting: \\n\")\nprint(\"Validation set shape : \\t\",val.shape)\nprint(\"Val1 set shape : \\t\",val1.shape)\nprint(\"Val2 set shape : \\t\",val2.shape)\n\n\nprint(\"\\nTarget Variable in Validation Data: \\t\",Counter(val.is_promoted))\nprint(\"Target Variable in Val1 Data :  \\t\",Counter(val1.is_promoted))\nprint(\"Target Variable in Val2 Data :  \\t\",Counter(val2.is_promoted))","bffc201c":"x_train=train.drop('is_promoted',axis=1)\ny_train = train.is_promoted\nprint(\"train :\",train.shape)\nprint(\"xtrain :\",x_train.shape)\nprint(\"ytrain :\",y_train.shape)","6d6a2ed1":"x_val1=val1.drop('is_promoted',axis=1)\ny_val1 = val1.is_promoted\nprint(\"val1 :\",val1.shape)\nprint(\"xval1 :\",x_val1.shape)\nprint(\"yval1 :\",y_val1.shape)","756bb8fc":"x_val2=val2.drop('is_promoted',axis=1)\ny_val2 = val2.is_promoted\nprint(\"val1 :\",val2.shape)\nprint(\"xval1 :\",x_val2.shape)\nprint(\"yval1 :\",y_val2.shape)","3f2119bc":"x_test=test\nprint(\"x_test :\",x_test.shape)","c22b2007":"plt.figure(figsize=(14,8))\nsns.countplot(x='department',hue='is_promoted',data=train)\nplt.title('Department-wise Promotion Count')\nplt.xlabel('Department')\nplt.ylabel('Employees')\nplt.show()","c93142f0":"x=train.groupby(['department','is_promoted'])['is_promoted'].count().unstack().fillna(0) #we can also use crosstab function\n                                                                                        #here as well\nx['%']=x[1]\/(x[0]+x[1]) #calculation of promotion %\nplt.figure(figsize=(12,6))\nplt.plot(x['%'],color='goldenrod', linewidth=2.5)\nplt.grid(color = 'black', linestyle = '-', linewidth = 0.5)\nplt.title('Department wise Promotion Rates')\nplt.xlabel('Department')\nplt.ylabel('% of employees promoted')\nplt.show()","a8ddbb29":"y=train.groupby(['region','is_promoted'])['is_promoted'].count().unstack().fillna(0)\ny['%']=y[1]\/(y[0]+y[1])#calculation of promotion %\ny=y.sort_values('%',ascending=False) #sorting by promotion rate to get insights easily\nplt.figure(figsize=(20,10))\nplt.plot(y['%'],color='coral', linewidth=2.5)\nplt.grid(color = 'black', linestyle = '-', linewidth = 0.5)\nplt.title('Region wise Promotion Rates')\nplt.xlabel('Region')\nplt.xticks(rotation=75)\nplt.ylabel('% of employees promoted')\nplt.show()","175b24b5":"plt.figure(figsize=(14,8))\nsns.countplot(x='education',hue='is_promoted',data=train)\nplt.title('Education-wise Promotion Count')\nplt.xlabel('Education Level')\nplt.ylabel('Employees')\nplt.show()","e352f180":"labels = 'f','m'\nsizes = list(train.groupby('gender')['gender'].count())\nexplode=(0.1,0)\nfig1, ax1 = plt.subplots()\nax1.pie(sizes,explode=explode,labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  \nplt.title(\"Gender Distribution of workforce\")\nplt.show()","7d39d72a":"y=train.groupby(['gender','is_promoted'])['is_promoted'].count().unstack().fillna(0)\ny1=y.div(y.sum(1).astype('float'), axis = 0)\ny1.plot(kind = 'bar', stacked = True, figsize = (20, 8), color = ['orange', 'lightblue'])\nplt.xlabel('Gender')\nplt.ylabel('% of employees getting promoted(or not)')\nplt.title(\"Gender wise Distribution of employees getting promoted or not\")","565f0695":"z=train.groupby(['gender','education','is_promoted'])['is_promoted'].count().unstack().fillna(0)\nz['%']=z[1]\/(z[0] + z[1])#calculation of promotion %\nplt.figure(figsize=(12,6))\nz['%'].plot.bar()\nplt.title('Gender & Education Level-wise Promotion Rate')\nplt.xlabel('Gender & Education Level')\nplt.ylabel('% of employees promoted')\nplt.show()","2cc3c2be":"labels='Other','Referred','Sourcing'\nsizes = list(train.groupby('recruitment_channel')['recruitment_channel'].count())\nexplode=(0.035,0.05,0.035)\nplt.figure(figsize=(7,7))\nplt.pie(sizes,explode=explode,labels=labels, autopct='%1.1f%%',\n        shadow=False, startangle=90)\nplt.title(\"Contribution of Recruiment Channels in hiring of employees\")\nplt.show()","a6e2dcf1":"z=train.groupby(['recruitment_channel','is_promoted'])['is_promoted'].count().unstack().fillna(0)\nz['%']=z[1]\/(z[0] + z[1])#calculation of promotion %\nplt.figure(figsize=(7,7))\nz['%'].plot.bar()\nplt.title('Recruitment Channel wise Promotion Rate')\nplt.xlabel('Recruitment Channel')\nplt.ylabel('% of employees promoted')\nplt.show()","b9a96911":"x=train.groupby(['no_of_trainings'])['is_promoted'].value_counts().unstack().fillna(0)\nx['%']=x[1]\/(x[0] + x[1])#calculation of promotion %\nplt.figure(figsize=(8,8))\nplt.plot(x['%'],color='coral', linewidth=2.5)\nplt.grid(color = 'black', linestyle = '-', linewidth = 0.5)\nplt.title('No. of trainings vs Promotions')\nplt.xlabel('No. of trainings')\nplt.ylabel('% of employees promoted')\nplt.show()","7fc44a42":"plt.figure(figsize=(10,7))\nsns.histplot(train.age,color='coral')\nplt.title(\"Age Distribution of Employees\")\nplt.show()","eebab851":"y=train.groupby(['age','is_promoted'])['is_promoted'].count().unstack().fillna(0)\ny1=y.div(y.sum(1).astype('float'), axis = 0)\ny1.plot(kind = 'bar', stacked = True, figsize = (20, 8), color = ['lightcoral', 'green'])\nplt.xlabel('AGE')\nplt.ylabel('% of employees getting promoted(or not)')\nplt.title(\"Age wise Distribution of employees getting promoted or not\")\n","7272a24a":"z=train.groupby(['previous_year_rating','is_promoted'])['is_promoted'].count().unstack().fillna(0)\nz['%']=z[1]\/(z[0] + z[1])#calculation of promotion %\nplt.figure(figsize=(10,8))\nplt.plot(z['%'],color='coral', linewidth=2.5)\nplt.grid(color = 'black', linestyle = '-', linewidth = 0.5)\nplt.title('Previous Year Ratings vs Promotions')\nplt.xlabel('Previous Year Ratings')\nplt.ylabel('% of employees promoted')\nplt.show()","d0685427":"z=train.groupby(['KPIs_met >80%','is_promoted'])['is_promoted'].count().unstack().fillna(0)\nz['%']=z[1]\/(z[0]+z[1])#calculation of promotion %\nz","46fe2dff":"sns.barplot(x=train['KPIs_met >80%'],y=train['is_promoted'])\nplt.title(\"Promotions vs KPIs Met >80%\")\nplt.ylabel(\"Promotion %\")","1ffaa0df":"x=train.groupby(['awards_won?','is_promoted'])['is_promoted'].count().unstack().fillna(0)\nx['%']=x[1]\/(x[0]+x[1])#calculation of promotion %\nx","572862ea":"sns.barplot(x=train['awards_won?'],y=train['is_promoted'])\nplt.title(\"Promotions vs Awards Won\")\nplt.ylabel(\"Promotion %\")\nplt.xlabel('Awards Won ?')","5ba4117f":"y=train.groupby(['avg_training_score','is_promoted'])['is_promoted'].count().unstack().fillna(0)\ny1=y.div(y.sum(1).astype('float'), axis = 0)\ny1.plot(kind = 'bar', stacked = True, figsize = (20, 8), color = ['lightcoral', 'lightblue'])\nplt.xlabel('Training Score')\nplt.ylabel('% of employees getting promoted(or not)')\nplt.title(\"Avg Training Score wise Distribution of employees getting promoted or not\")","8fbd22b7":"train.corr()","fad8fa85":"plt.figure(figsize=(11,8))\nsns.heatmap(train.corr(),annot=True)\nplt.show()","cb90c1ed":"train[train.select_dtypes(include=['object']).columns.tolist()].nunique()\n","5ebf8110":"# ONE HOT ENCODING \"education\" column across all datasets\n\n\ndummy1=pd.get_dummies(x_train['education'])\nx_train=x_train.join(dummy1)\n\ndummy2=pd.get_dummies(x_val1['education'])\nx_val1=x_val1.join(dummy2)\n\ndummy3=pd.get_dummies(x_val2['education'])\nx_val2=x_val2.join(dummy3)\n\ndummy4=pd.get_dummies(x_test['education'])\nx_test=x_test.join(dummy4)","c5b043a3":"# Education has been encoded so we can drop it\n\nx_train.drop('education',axis=1,inplace=True)\nx_val1.drop('education',axis=1,inplace=True)\nx_val2.drop('education',axis=1,inplace=True)\nx_test.drop('education',axis=1,inplace=True)\n","dcd5e403":"x_train.head()\n#x_val1.head()\n#x_val2.head()","be68b13c":"# ONE HOT ENCODING \"gender\" column across all datasets\n\n\ndummy1=pd.get_dummies(x_train['gender'])\nx_train=x_train.join(dummy1)\n\ndummy2=pd.get_dummies(x_val1['gender'])\nx_val1=x_val1.join(dummy2)\n\ndummy3=pd.get_dummies(x_val2['gender'])\nx_val2=x_val2.join(dummy3)\n\ndummy4=pd.get_dummies(x_test['gender'])\nx_test=x_test.join(dummy4)","cea101cc":"# Gender has been encoded so we can drop it\n\nx_train.drop('gender',axis=1,inplace=True)\nx_val1.drop('gender',axis=1,inplace=True)\nx_val2.drop('gender',axis=1,inplace=True)\nx_test.drop('gender',axis=1,inplace=True)\n","b1fe4ee7":"x_train.head()\n#x_val1.head()\n#x_val2.head()","050d8afb":"# ONE HOT ENCODING \"recruitment_channel\" column across all datasets\n\n\ndummy1=pd.get_dummies(x_train['recruitment_channel'])\nx_train=x_train.join(dummy1)\n\ndummy2=pd.get_dummies(x_val1['recruitment_channel'])\nx_val1=x_val1.join(dummy2)\n\ndummy3=pd.get_dummies(x_val2['recruitment_channel'])\nx_val2=x_val2.join(dummy3)\n\ndummy4=pd.get_dummies(x_test['recruitment_channel'])\nx_test=x_test.join(dummy4)","8ec6c8f7":"# Recruitment Channel has been encoded so we can drop it\n\nx_train.drop('recruitment_channel',axis=1,inplace=True)\nx_val1.drop('recruitment_channel',axis=1,inplace=True)\nx_val2.drop('recruitment_channel',axis=1,inplace=True)\nx_test.drop('recruitment_channel',axis=1,inplace=True)\n","76a4b1ee":"x_train.head()\n#x_val1.head()\n#x_val2.head()","591f4ef8":"#Target Encoding Department\nx_train['target'] = y_train\ntarget_mapper = x_train.groupby('department')['target'].mean().to_dict()","a7a9fd8d":"#mapping to all datasets\nx_train['dept_enc'] = x_train['department'].map(target_mapper)\nx_val1['dept_enc'] = x_val1['department'].map(target_mapper)\nx_val2['dept_enc'] = x_val2['department'].map(target_mapper)\nx_test['dept_enc'] = x_test['department'].map(target_mapper)","fd76cf61":"x_train.drop('target',axis=1,inplace=True) #dropping temporary column","43a09725":"# Department has been encoded so we can drop it\n\nx_train.drop('department',axis=1,inplace=True)\nx_val1.drop('department',axis=1,inplace=True)\nx_val2.drop('department',axis=1,inplace=True)\nx_test.drop('department',axis=1,inplace=True)\n","824a118c":"x_train.head()","edbfba79":"#Target Encoding Region\nx_train['target'] = y_train\ntarget_mapper = x_train.groupby('region')['target'].mean().to_dict()","16b528d3":"#mapping to all datasets\nx_train['region_enc'] = x_train['region'].map(target_mapper)\nx_val1['region_enc'] = x_val1['region'].map(target_mapper)\nx_val2['region_enc'] = x_val2['region'].map(target_mapper)\nx_test['region_enc'] = x_test['region'].map(target_mapper)","3f49fea2":"x_train.drop('target',axis=1,inplace=True) #dropping temporary column","c946b043":"# Department has been encoded so we can drop it\n\nx_train.drop('region',axis=1,inplace=True)\nx_val1.drop('region',axis=1,inplace=True)\nx_val2.drop('region',axis=1,inplace=True)\nx_test.drop('region',axis=1,inplace=True)\n","2889a4e5":"x_train.head()","542b8735":"# Dropping employee_id column\n\nx_train.drop('employee_id',axis=1,inplace=True)\nx_val1.drop('employee_id',axis=1,inplace=True)\nx_val2.drop('employee_id',axis=1,inplace=True)\nx_test.drop('employee_id',axis=1,inplace=True)\n","cda3bebe":"x_train.head()\n#x_val1.head()\n#x_val2.head()","7eddb3fc":"#x_train.isnull().sum()\n#x_val1.isnull().sum()\n#x_val2.isnull().sum()\n#x_test.isnull().sum()\n","1ee22903":"print(\"train:\",x_train.shape)\nprint(\"val1:\",x_val1.shape)\nprint(\"val2:\",x_val2.shape)","3c273316":"# create the SequentialFeatureSelector object, and configure the parameters.\n#sfs = SequentialFeatureSelector(RandomForestClassifier(), \n           #k_features=8, \n           #forward=True, \n           #floating=False,\n           #scoring='accuracy',\n           #cv=4)\n\n# fit the object to the training data.\n#sfs = sfs.fit(x_train, y_train)\n\n# the selected features.\n#selected_features = x_train.columns[list(sfs.k_feature_idx_)]\n\n#Constucting a list of selected featues and score\n#forward=pd.Series(list(selected_features)).to_frame().append(pd.Series(sfs.k_score_),ignore_index=True)\n#forward","af230f99":"# create theSequentialFeatureSelector object, and configure the parameters.\n#sbs = SequentialFeatureSelector(RandomForestClassifier(), \n           #k_features=8, \n           #forward=False, \n           #floating=False,\n           #scoring='accuracy',\n           #cv=5)\n\n# fit the object to our training data.\n#sbs = sbs.fit(x_train, y_train)\n\n# print the selected features. \n#selected_features = x_train.columns[list(sbs.k_feature_idx_)]\n\n#Constucting a list of selected featues and score\n#backward=pd.Series(list(selected_features)).to_frame().append(pd.Series(sbs.k_score_),ignore_index=True)\n#backward","7f89ac57":"# The \"accuracy\" scoring is proportional to the number of correct classifications\n#clfrf = RandomForestClassifier() \n#rfecv = RFECV(estimator=clfrf, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\n#rfecv = rfecv.fit(x_train, y_train)\n\n#print('Optimal number of features :', rfecv.n_features_)\n#print('Best features :', x_train.columns[rfecv.support_])","aa09afc6":"#plt.figure()\n#plt.xlabel(\"Number of features selected\")\n#plt.ylabel(\"Cross validation score of number of selected features\")\n#plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n#plt.show()","bacd0f99":"def tree_based_feature_importance(x_train,y_train):\n    from sklearn.ensemble import RandomForestClassifier\n    # create the random forest model\n    model = RandomForestClassifier()\n\n    # fit the model to start training.\n    model.fit(x_train, y_train)\n\n    # get the importance of the resulting features.\n    importances = model.feature_importances_\n\n    # create a data frame for visualization.\n    final_df = pd.DataFrame({\"Features\": x_train.columns, \"Importances\":importances})\n    final_df.set_index('Importances')\n\n    # sort in descending order \n    final_df = final_df.sort_values('Importances',ascending=False)\n    \n    #visualising feature importance\n    pd.Series(model.feature_importances_, index=x_train.columns).nlargest(6).plot(kind='barh')  \n    return final_df","3ffb983d":"feature_importance=tree_based_feature_importance(x_train,y_train)","3f48a00b":"display(feature_importance)","bbc4f969":"# Making a list of selected features\nselected_features = ['avg_training_score','dept_enc','age',\n                     'length_of_service','previous_year_rating',\n                     'KPIs_met >80%','no_of_trainings','awards_won?']\nx_train[selected_features].head()","15217ea2":"#Creating new datasets with just the selected features\nx_train_new=x_train[selected_features]\nx_val1_new=x_val1[selected_features]\nx_val2_new=x_val2[selected_features]\nx_test_new=x_test[selected_features]\n","c60089bd":"# # Using Standard Scaler\n\n# scaler = StandardScaler() # create an object\n# x_train_scaled = scaler.fit_transform(x_train_new) # fit the scaler to the train set, and then transform it\n\n# x_val1_scaled = scaler.transform(x_val1_new) # transform the val1 set\n# x_val2_scaled = scaler.transform(x_val2_new) # transform the val2 set\n# x_test_scaled = scaler.transform(x_test_new) # transform the test set\n# x_train\n\n# Using MinMax Scaler\n\nscaler = MinMaxScaler() # create an instance\nx_train_scaled = scaler.fit_transform(x_train_new) #  fit  the scaler to the train set and then transform it\nx_val1_scaled = scaler.transform(x_val1_new) # transform (scale) the val1 set\nx_val2_scaled = scaler.transform(x_val2_new) # transform (scale) the val2 set\nx_test_scaled = scaler.transform(x_test_new) # transform (scale) the test set\n\n\nx_train_scaled[:5]\n","0f191dde":"# Fitting Logistic Regression to the Training Set\nclassifier = LogisticRegression()\nclassifier.fit(x_train_scaled, y_train)\n# Predicting the Train Set Results\ny_pred = classifier.predict(x_train_scaled)\n# Classification Report\nreptrain=classification_report(y_train, y_pred, output_dict=True)\nreptrain = pd.DataFrame(reptrain).transpose()","11bbda78":"# Predicting the Val1 Set Results\ny_pred = classifier.predict(x_val1_scaled)\n# Classification Report\nrepval1=classification_report(y_val1, y_pred, output_dict=True)\nrepval1 = pd.DataFrame(repval1).transpose()\n\n# Predicting the Val2 Set Results\ny_pred = classifier.predict(x_val2_scaled)\n# Classification Report\nrepval2=classification_report(y_val2, y_pred, output_dict=True)\nrepval2 = pd.DataFrame(repval2).transpose()\n\nres = pd.concat([reptrain[:3],repval1[:3], repval2[:3]], keys=['LGR - Train','LGR - Val1','LGR - Val2']).round(2)\nres","9770929e":"# Fitting Decision Tree Classifier to the Training Set\nclassifier = DecisionTreeClassifier()\nclassifier.fit(x_train_scaled, y_train)\n# Predicting the Train Set Results\ny_pred = classifier.predict(x_train_scaled)\n# Classification Report\nreptrain=classification_report(y_train, y_pred, output_dict=True)\nreptrain = pd.DataFrame(reptrain).transpose()","afee7b99":"# Predicting the Val1 Set Results\ny_pred = classifier.predict(x_val1_scaled)\n# Classification Report\nrepval1=classification_report(y_val1, y_pred, output_dict=True)\nrepval1 = pd.DataFrame(repval1).transpose()\n\n# Predicting the Val2 Set Results\ny_pred = classifier.predict(x_val2_scaled)\n# Classification Report\nrepval2=classification_report(y_val2, y_pred, output_dict=True)\nrepval2 = pd.DataFrame(repval2).transpose()\n\nres = pd.concat([reptrain[:3],repval1[:3], repval2[:3]], keys=['DT - Train','DT - Val1','DT - Val2']).round(2)\nres","aeddc705":"# Fitting Random Forest Classifier to the Training Set\nclassifier = RandomForestClassifier()\nclassifier.fit(x_train_scaled, y_train)\n# Predicting the Train Set Results\ny_pred = classifier.predict(x_train_scaled)\n# Classification Report\nreptrain=classification_report(y_train, y_pred, output_dict=True)\nreptrain = pd.DataFrame(reptrain).transpose()","90c12a9c":"# Predicting the Val1 Set Results\ny_pred = classifier.predict(x_val1_scaled)\n# Classification Report\nrepval1=classification_report(y_val1, y_pred, output_dict=True)\nrepval1 = pd.DataFrame(repval1).transpose()\n\n# Predicting the Val2 Set Results\ny_pred = classifier.predict(x_val2_scaled)\n# Classification Report\nrepval2=classification_report(y_val2, y_pred, output_dict=True)\nrepval2 = pd.DataFrame(repval2).transpose()\n\nres = pd.concat([reptrain[:3],repval1[:3], repval2[:3]], keys=['RF - Train','RF - Val1','RF - Val2']).round(2)\nres","d9302026":"# Fitting XGBoost to the Training Set\nclassifier = XGBClassifier()\nclassifier.fit(x_train_scaled, y_train)\n# Predicting the Train Set Results\ny_pred = classifier.predict(x_train_scaled)\n# Classification Report\nreptrain=classification_report(y_train, y_pred, output_dict=True)\nreptrain = pd.DataFrame(reptrain).transpose()","e9eb6457":"# Predicting the Val1 Set Results\ny_pred = classifier.predict(x_val1_scaled)\n# Classification Report\nrepval1=classification_report(y_val1, y_pred, output_dict=True)\nrepval1 = pd.DataFrame(repval1).transpose()\n\n# Predicting the Val2 Set Results\ny_pred = classifier.predict(x_val2_scaled)\n# Classification Report\nrepval2=classification_report(y_val2, y_pred, output_dict=True)\nrepval2 = pd.DataFrame(repval2).transpose()\n\nres = pd.concat([reptrain[:3],repval1[:3], repval2[:3]], keys=['XGB - Train','XGB - Val1','XGB - Val2']).round(2)\nres","87dacbcc":"# Fitting KNN to the Training Set\nclassifier = KNeighborsClassifier()\nclassifier.fit(x_train_scaled, y_train)\n# Predicting the Train Set Results\ny_pred = classifier.predict(x_train_scaled)\n# Classification Report\nreptrain=classification_report(y_train, y_pred, output_dict=True)\nreptrain = pd.DataFrame(reptrain).transpose()","e4fbbcb1":"# Predicting the Val1 Set Results\ny_pred = classifier.predict(x_val1_scaled)\n# Classification Report\nrepval1=classification_report(y_val1, y_pred, output_dict=True)\nrepval1 = pd.DataFrame(repval1).transpose()\n\n# Predicting the Val2 Set Results\ny_pred = classifier.predict(x_val2_scaled)\n# Classification Report\nrepval2=classification_report(y_val2, y_pred, output_dict=True)\nrepval2 = pd.DataFrame(repval2).transpose()\n\n\n#Overall Report\nres = pd.concat([reptrain[:3],repval1[:3], repval2[:3]], keys=['KNN - Train','KNN - Val1','KNN - Val2']).round(2)\nres","466c69e2":"from pprint import pprint\n\n# To look at the available hyperparameters, we can create a random forest and examine the default values.\n\nrf = RandomForestClassifier(random_state = 42)\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","4005ed05":"#To use RandomizedSearchCV, we first need to create a parameter grid to sample from during fitting:\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4, 6]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)\n\n","c428d1dc":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\n#rf = RandomForestClassifier(random_state = 1)\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\n#rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 4,scoring='f1',verbose=3, random_state=42, n_jobs = -1)\n# Fit the random search model\n#rf_random.fit(x_train_scaled, y_train)","84e88771":"#rf_random.best_params_ #Best values of selected parameters","e63546c1":"#rf_random.best_estimator_","316feb4d":"# Fitting Random Forest Classifier to the Training Set\nclassifier = RandomForestClassifier(bootstrap=False, max_depth=50, max_features='sqrt',\n                       min_samples_split=15, n_estimators=400, random_state=1)\n\nclassifier.fit(x_train_scaled, y_train)\n# Predicting the Train Set Results\ny_pred = classifier.predict(x_train_scaled)\n# Classification Report\nreptrain=classification_report(y_train, y_pred, output_dict=True)\nreptrain = pd.DataFrame(reptrain).transpose()","eee94936":"# Predicting the Val1 Set Results\ny_pred = classifier.predict(x_val1_scaled)\n# Classification Report\nrepval1=classification_report(y_val1, y_pred, output_dict=True)\nrepval1 = pd.DataFrame(repval1).transpose()\n\n# Predicting the Val2 Set Results\ny_pred = classifier.predict(x_val2_scaled)\n# Classification Report\nrepval2=classification_report(y_val2, y_pred, output_dict=True)\nrepval2 = pd.DataFrame(repval2).transpose()\n\nres = pd.concat([reptrain[:3],repval1[:3], repval2[:3]], keys=['RF - Train','RF - Val1','RF - Val2']).round(2)\nres","9843413b":"# To look at the available hyperparameters, we can create a random forest and examine the default values.\n\nxgb = XGBClassifier(random_state = 42)\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(xgb.get_params())","b47ef51a":"params={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7, 9, 11 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ,0.5],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    \n}","ac98e9aa":"#Running random search\n#xgb= XGBClassifier()\n#xgb_random=RandomizedSearchCV(xgb,param_distributions=params,n_iter=50,scoring='f1',n_jobs=-1,cv=4,verbose=3)\n#xgb_random.fit(x_train_scaled,y_train)\n","ded0df18":"#xgb_random.best_params_ #Best values of selected parameters","5fed7e4b":"#xgb_random.best_estimator_","b37dd189":"# Fitting XGBoost to the Training Set\nclassifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.7, gamma=0.3, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.25, max_delta_step=0, max_depth=6,\n              min_child_weight=3,monotone_constraints='()',\n              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\nclassifier.fit(x_train_scaled, y_train)\n# Predicting the Train Set Results\ny_pred = classifier.predict(x_train_scaled)\n# Classification Report\nreptrain=classification_report(y_train, y_pred, output_dict=True)\nreptrain = pd.DataFrame(reptrain).transpose()","809efdbf":"# Predicting the Val1 Set Results\ny_pred = classifier.predict(x_val1_scaled)\n# Classification Report\nrepval1=classification_report(y_val1, y_pred, output_dict=True)\nrepval1 = pd.DataFrame(repval1).transpose()\n\n# Predicting the Val2 Set Results\ny_pred = classifier.predict(x_val2_scaled)\n# Classification Report\nrepval2=classification_report(y_val2, y_pred, output_dict=True)\nrepval2 = pd.DataFrame(repval2).transpose()\n\nres = pd.concat([reptrain[:3],repval1[:3], repval2[:3]], keys=['XGB - Train','XGB - Val1','XGB - Val2']).round(2)\nres","f09aa624":"#We will now make final predictions on XGBoost Classifier model\n\nxgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.7, gamma=0.3, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.25, max_delta_step=0, max_depth=6,\n              min_child_weight=3,monotone_constraints='()',\n              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\nxgb.fit(x_train_scaled, y_train)\n\ny_test_predicted = xgb.predict(x_test_scaled) #predicting test data\ny_test_predicted\n","5bc48d75":"promotions_pred= pd.Series(y_test_predicted)\npromotions_pred.shape","d8cf62c8":"promotions_pred.value_counts().to_frame()","a60e01eb":"id=pd.Series(test.employee_id).reset_index().drop('index',axis=1)","4283def5":"# Mapping employee ids of test set with their promotion status for a better understanding\n\nresult=pd.concat([id,promotions_pred],axis=1,ignore_index=True)","476ef982":"#final answer on test set\nresult.columns=['employee_id','is_promoted']\nresult","c71725c7":"### Forward Feature Selection","b8e2e50f":"## 2.) EDA BASED ON REGION","c6f2092d":"### We see that after predicting on the tuned Random Forest Classifier, the evaluation metrics have imporved significantly.","ed79f249":"\n# <center> ----- MAKING PREDICTIONS ON TEST SET ----- <\/center>\n<br>","24f87c33":"## 4. XGBoost Classifier","7c18ff78":"***FOR TEST SET***","6f91e098":"*The columns having less number of categories can be One Hot Encoded. Other columns can be Target encoded*","03533c4a":"## 5.) EDA BASED ON NO. OF TRAININGS","01bd5184":"***Grid Size : 2 x 12 x 2 x 4 x 4 x 10 = 7680***","cc931ee5":"## 8.) EDA BASED ON KPIs MET > 80%","f5507349":"***Constructing Grid***","6d288a24":"# <center> ----- FEATURE SELECTION ----- <\/center>","1f082aa0":"**Performing One Hot Encoding**","b6280c38":"***There is no class ratio imbalance after splitting so stratified splitting has worked successfully***","b8e1091a":"<b>Insights drawn from the above plot:\n1. As expected , as the ratings increase from 1 to 5 , the % of employees getting  promoted also increases.\n  > Employees should work hard to get a good rating in order to get promoted.\n\nNOTE - NOT TAKING RATING = 0 INTO CONSIDERATION\n","9ee52e20":"<h4>\n<ol>\n    <li> Loading required libraries and datasets<br>\n    <br><li> Preprocessing \n        <ol>\n            <li> Basic Preprocessing\n            <li> Splitting the datasets\n        <\/ol>\n        <br>\n    <li> EDA <br>\n   <br><li> Feature Engineering<br>\n   <br><li> Feature Selection\n        <ol> \n            <li> Forward Selection\n            <li> Backward Selection\n            <li> Recursive Elimination\n            <li> Tree Based Selection\n            <li> Feature Scaling\n         <\/ol><br>\n    <br> <li> Model Training\n         <ol> \n            <li> Logistic Regression Classification\n            <li> Decision Trees\n            <li> Random Forest\n            <li> XGBoost\n            <li> KNN\n         <\/ol><br> \n     <li> Hyperparameter Tuning <br> \n    <br>  <li> Predictions on Test Set \n <\/ol>\n    <\/h4>","aa5517f9":"<b>Insights drawn from the above plot:\n1. 55.5% of employees have been hired through other channels\n2. 2.2% employees hired through referrals\n  > Indicates that only a handful of people are given referrals\n3. 42.3% employees hired through sourcing","6482b5f0":"<br>","38c5cae2":"# <center> HR ANALYTICS  <\/center>\n\n##### MADE BY - Ashutosh Sahay\n","dfe94eb0":"***This shows a summary of numerical columns***","95371989":"***None of the columns have unique value =1 ; this means there are no constant columns***","b4544786":"<b>Insights drawn from the above plot:\n1. Promotion rates decline with an increase in number of trainings done.\n    > Promotions and No. of Trainings done may be considered as negatively correlated\n   ","91481046":"## 1.) EDA BASED ON DEPARTMENT","bcef1218":"***Grid Size : 7 x 8 x 6 x 6 x 4 = 8064 ***","a59b13cd":"<b>Insights drawn from the above plot:\n1. Almost an even distribution of promotions throughout all ages\n2. Even the freshers and the veterans are getting equal shares in promotions\n  > This shows that the company really values the work done by employee regardless of age and experience.","b35f553d":"***Evaluation Report for KNN Classifier***","9c99f525":"It is desirable to split the dataset in such a way that preserves the same proportions of examples in each class as observed in the original dataset.","ca2ef79f":"***Since out of 54808 records in the train set, 2409 records(4.39% of 54808) have NULL in education and out of those 2409 records, 122 records(2.6% of 4668) have 'is_promoted' value as 1. So we can drop these records as they won't bring a significant change in our predictions in the future.***","1742a3f9":"***ii) On the other hand, 'previous_year_rating' is a numerical feature and it signifies the rating of the employee in the previous year. The NULL value in this column indicates that the employee has been a part of the company for LESS THAN 1 YEAR and does not have a previous year score. Hence we can inmpute these null values with 0*** ","fd5eda19":"#### We have our best set of tuned hyperparameters for the Random Forest Classifier.\n#### Now applying these and again predicting","b931f6fa":"<b>Insights drawn from the above plot:\n1. Even though number of employees hired through referrals is the least, the promotion rate is the highest for them.\n    > This indicates that employees only give referrals to those people who are expected to excel at their work after getting selected","422422a3":"***No null value remaining in 'previous_year_rating'***","4738905b":"# <center> ----- FEATURE ENGINEERING ----- <\/center>","c0cdbddb":"***Checking default parameters :***","385e6a22":"## 4.) EDA BASED ON RECRUITMENT CHANNEL","2d610a6f":"## 1. Logistic Regression Classifier\n","b2978858":"<b>Insights drawn from the above plot:\n1. Very high chance of getting promoted if the employee has won an award.\n  > Company checks whether the employee has won an award or not while promoting so employees should keep in mind to give their best performance.","006e1ea9":"**TOTAL FEATURES - 14<br>\n    Categorical Features - 5<br>\n    Numerical Featues - 9<br>**","ea19ca9a":"***Checking default parameters***","22bcd07c":"# <center> TABLE OF CONTENTS <\/center>","96d89b09":"***There are no duplicate rows in the dataframe***","83c7d8b3":"***There are 4668 records where the value of target variable i.e, 'is_promoted' is 1.***","f4370d9c":"***Evaluation Report for Decision Tree Classifier***","572dbdbc":"<b>Insights drawn from the above plot:\n1. Employees whose KPIs Met is > 80% have a very high chance of getting promoted with respect to those who have < 80%.","3029ccf0":"## THANK YOU...","83cb1123":"## 5. KNN Classifier","99ba740c":"#### We have our best set of tuned hyperparameters for the XGB Classifier.\n#### Now applying these and again predicting","ec4258fc":"***Evaluation Report for Logistic Regression Classifier***","97e25880":"*After performing FE we see that the number of features has not drastically gone up which is a good sign as excess features make the learning process slow*","7d262832":"<b>Insights drawn from the above plot:\n1. Almost 2\/3 rd of the workforce holds a Bachelor's Degree or equivalent.\n2. Employees with just Secondary Level Education have also been hired.\n  > Indicates that the company focuses on skill rather than degree.\n","d130ed76":"### Tree Based FS","a87e1a36":"### Recursive Elimination of features with Cross Validation","33711b9a":"***Applying Random Search***","702f9c62":"   ***i) Since 'education' is a categorical feature here and it also signifies whether the person has obtained a particular level of education or not, we cannot impute the null values as we can't assign an education level to a person on our own. The person may or may not have achieved the assigned level and this will result in an inaccurate analysis***","7e00df26":"<b>Insights drawn from the above plot:\n1. As avg training score increases, chances of promotion increases highly.","f8ae6f11":"<b>Insights drawn from the above plot:\n1. There is a huge difference between the number of male employees and and female employees. 70% of the workforce is made up of male employees.\n    > The management should take some steps to bridge this gap and promote gender equality. \n","b38aafd3":"<br>**We see that out of the 5 models used Random Forest Classifier and XGB Classifier have the best performance.**\n\n**So we will proceed with these 2 models and tune their hyperparameters and then select the best model for our final test set prediction** <br>","77efb876":"***Analysing all the above results and filtering out unimportant features***","1c6fa538":"# Upvote and drop a comment if you liked it.","37eefcdb":"***All the required datasets have been created, moving on to EDA.....***","32685a9c":"## 1. Random Forest Classifier","9b3008d3":"## 10. EDA BASED ON AVG TRAINING SCORE","90c9e116":"***Evaluation Report for XGBoost Classifier***","54d730eb":"***Evaluation Report for Random Forest Classifier***","76d58244":"<b>Insights drawn from the above plots:\n1. Technology Dept. has the highest promotion rate across all deparments.\n2. Legal Dept. has the lowest promotion rate across all departments.\n3. Top 3 Departments to work in on the basis of Promotions are : Technology, Procurement, Analysis<\/b>","352ed7a5":"### Backward Feature Selection","a671f8ce":"## 2. Decision Tree Classifier","17896e83":"#### Out of all the numerical columns, ''previous_year_rating'', ''KPIs_met>80%'',''awards_won?'' & ''avg_training_score'' seem to be more correlated to the target feature than the others","e83cb366":"***Constructing a grid:***","63e970cc":"<b>Insights drawn from the above plot:\n1. The age group of 30-35 has the most number of employees as compared to any other age groups.\n  > Signifies that even after gaining moderate amount of experience in the industry, employees prefer to work for this company.\n  ","a493f2e2":"***1. Preprocessing lets us to clean data and remove any inconsistencies like null value, duplicate values etc to make further model bulding process hassle free***","882c5295":"## 9.) EDA BASED ON AWARDS WON","320c4a3a":"<br>\n\n# <center> ----- HYPERPARAMETER TUNING ----- <\/center>","abf3bcf1":"## 6.) EDA BASED ON AGE","1b465386":"# <center> ----- MODEL TRAINING ----- <\/center>","69e834e4":"## 7.) EDA BASED ON PREVIOUS YEAR RATING","601da377":"### Feature Scaling for better processing in models","e11485e1":" **Performing Target Encoding**","d7ea9068":"<b>Insights drawn from the above plot:\n1. No disparity in promotions of males and females.\n    \nMoving on to a combined analysis of education and gender.","8dba7b2e":"<b>Insights drawn from the above plot:\n1. Top 3 Regions in terms of Promotion Rates : Region_4, Region_25, Region_17.\n    > Employees in these regions are working hard and are getting rewarded for the same.\n2.Bottom 3 Regions in terms of Promotion Rates: Region_18, Region_24, Region_34. \n    > The management should have a discussion with the employees of these regions to figure out a new strategy to increase productivity and employee satisfaction. <\/b>\n\n","9a633699":"<br>\n\n# <center> ----- Loading required libraries and datasets ----- <\/center>\n\n<br>","261e38c0":"<br>\n\n# <center> ----- PREPROCESSING ----- <\/center>\n\n<br>","a9a2407e":"## 2. XGB Classifier","bac2e223":"<b>Insights drawn from the above plot:\n1. Female employees with 'Masters & above' education level have the highest rate of getting promoted in the entire company.\n  >The company can use this fact to attract more female employees to work for the company \n\n2. Among the male employees, those with 'Bachelor's' education level have the least chance of getting promoted due to high competition.\n  > Employees should upskill themselves to beat the competition\n\n   ","d45eba1d":"### Splitting the dataset","07f45d25":"## We see that the XGB Classfier performs better than Random Forest. So we select it for our final predictions.","09a5fee4":"***1. EDA helps us to detect patterns and trends which might be useful in drawing insights***\n\n***2. EDA is always done on the \"train\" dataset and not on Validation\/Test sets because it will lead to data leakage and would be inappropriate***\n","1f94ae91":"### Correlation Matrix","b999bb57":"## 3. Random Forest Classifier","e2979358":"**The model has predicted that out of 22456 employees in the test set, 21749 will not get promotions and 707 employees will be promoted**","6796c9f6":"## 3.) EDA BASED ON EDUCATION & GENDER","0a6dfcc1":"### Final Result showing the employees and their promotion status","a2c3ca9f":"# <center>----- EXPLORATORY DATA ANALYSIS (EDA) -----<\/center> "}}