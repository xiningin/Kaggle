{"cell_type":{"0063a491":"code","da9acfac":"code","8e0b7a0e":"code","660c86e7":"code","eab55f35":"code","39ce7667":"code","3420e725":"code","ee1716cc":"code","1658807b":"code","f5622a3f":"code","9cd5e9c3":"code","4312420a":"code","3b7447ef":"code","42b5093f":"code","c2f93dce":"code","9f382af4":"code","e930beb2":"code","3e0ca5b6":"code","f3000db5":"code","20a28697":"code","e2e00de6":"code","61db27c6":"markdown","f2df8459":"markdown","4a5ab2fb":"markdown","afb8220e":"markdown"},"source":{"0063a491":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport time\n\nimport joblib\n\nfrom sklearn.preprocessing import StandardScaler    # RobustScaler\nfrom sklearn.model_selection import KFold    # GroupKFold\n\n# Warning\u306e\u7121\u52b9\u5316\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n# \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0colum\u306e\u5168\u8868\u793a\npd.set_option(\"display.max_columns\", None)","da9acfac":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            #else:\n            elif str(col_type)[:5] == \"float\":\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","8e0b7a0e":"def read_data_strict(file_name=\"\/kaggle\/input\/ump-train-picklefile\/train.pkl\"):\n    df = pd.read_pickle(file_name).pipe(reduce_mem_usage)\n    assert df.isnull().any().sum() == 0, \"null exists.\"\n    return df","660c86e7":"# df_train = pd.read_pickle(\"..\/input\/ump-train-picklefile\/train.pkl\")\ndf_train = read_data_strict()\ndf_train","eab55f35":"# df_train.info()","39ce7667":"# df_train.describe()","3420e725":"keys = [\"time_id\", \"investment_id\"]\nfeatures = list(df_train.filter(like=\"f_\").columns)","ee1716cc":"# \u30c7\u30fc\u30bf\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\nscaler = StandardScaler()    # RobustScaler()\ndf_train[features] = scaler.fit_transform(df_train[features])","1658807b":"joblib.dump(scaler, \"scaler.joblib\")","f5622a3f":"train_x = df_train[keys + features] #.values\ntrain_y = df_train[\"target\"] #.values","9cd5e9c3":"# time_id\u5217\u306etime_id\u3092\u5358\u4f4d\u3068\u3057\u3066\u5206\u5272\u3059\u308b\u3053\u3068\u306b\u3059\u308b\ntime_id = train_x[\"time_id\"]\nunique_time_ids = time_id.unique()","4312420a":"n_folds = 5","3b7447ef":"# tensorflow\u306e\u8b66\u544a\u6291\u5236\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\nimport tensorflow as tf\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)","42b5093f":"# -----------------------------------\n# \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u5b9f\u88c5\n# -----------------------------------\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.metrics import log_loss","c2f93dce":"# -----------------------------------\n# \u30a2\u30fc\u30ea\u30fc\u30b9\u30c8\u30c3\u30d4\u30f3\u30b0\n# -----------------------------------\nfrom keras.callbacks import EarlyStopping\n\n# \u5b66\u7fd2\u306e\u5b9f\u884c\n# \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3082\u30e2\u30c7\u30eb\u306b\u6e21\u3057\u3001\u5b66\u7fd2\u306e\u9032\u884c\u3068\u3068\u3082\u306b\u30b9\u30b3\u30a2\u304c\u3069\u3046\u5909\u308f\u308b\u304b\u30e2\u30cb\u30bf\u30ea\u30f3\u30b0\u3059\u308b\nbatch_size = 1024\nepochs = 50","9f382af4":"train_x[\"preds\"] = 0","e930beb2":"# KFold\u30af\u30e9\u30b9\u3092\u7528\u3044\u3066\u3001time_id\u5358\u4f4d\u3067\u5206\u5272\u3059\u308b\nkf = KFold(n_splits = n_folds, shuffle = False, random_state = 71)\nfor fold, (tr_group_idx, va_group_idx) in enumerate(kf.split(time_id)):\n    # time_id\u3092train\/valid\uff08\u5b66\u7fd2\u306b\u4f7f\u3046\u30c7\u30fc\u30bf\u3001\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\uff09\u306b\u5206\u5272\u3059\u308b\n    tr_x, tr_y = train_x.iloc[tr_group_idx], train_y.iloc[tr_group_idx]\n    va_x, va_y = train_x.iloc[va_group_idx], train_y.iloc[va_group_idx]\n    # \u5404\u30ec\u30b3\u30fc\u30c9\u306etime_id\u304ctrain\/valid\u306e\u3069\u3061\u3089\u306b\u5c5e\u3057\u3066\u3044\u308b\u304b\u306b\u3088\u3063\u3066\u5206\u5272\u3059\u308b\n\n    # \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\n    model = Sequential()\n    model.add(Dense(256, activation=\"relu\", input_shape=(tr_x.shape[1],)))\n    model.add(Dropout(0.2))\n    model.add(Dense(256, activation=\"relu\"))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation=\"sigmoid\"))\n\n    model.compile(loss=\"mean_squared_error\",   # \"mean_squared_logarithmic_error\",\n                  optimizer=\"adam\", metrics=[\"accuracy\"])\n\n    # \u30a2\u30fc\u30ea\u30fc\u30b9\u30c8\u30c3\u30d4\u30f3\u30b0\u306e\u89b3\u5bdf\u3059\u308bround\u309220\u3068\u3059\u308b\n    # restore_best_weights\u3092\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001\u6700\u9069\u306a\u30a8\u30dd\u30c3\u30af\u3067\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3059\u308b\n    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)\n\n    model.fit(tr_x, tr_y,\n            batch_size=batch_size, epochs=epochs,\n            verbose=1, validation_data=(va_x, va_y), callbacks=[early_stopping])\n\n    joblib.dump(model, f\"catb_{fold}.pkl\")\n    \n    # \u4e88\u6e2c\n    va_pred = model.predict(va_x[features])\n    train_x.loc[va_group_idx, \"preds\"] = va_pred","3e0ca5b6":"import ubiquant","f3000db5":"env = ubiquant.make_env()                   # initialize the environment\niter_test = env.iter_test()                 # an iterator which loops over the test set and sample submission","20a28697":"scaler = joblib.load(\"scaler.joblib\")\nmodels = [joblib.load(f\"catb_{fold}.pkl\") for fold in range(n_folds)]","e2e00de6":"for (test_df, sample_prediction_df) in iter_test:\n    test_df[features] = scaler.fit_transform(test_df[features]) \n    final_pred = [models[fold].predict(test_df[features]) for fold in range(n_folds)]\n    sample_prediction_df[\"target\"] = np.mean(np.stack(final_pred), axis = 0)\n    env.predict(sample_prediction_df) ","61db27c6":"## Libraries","f2df8459":"## Load Data","4a5ab2fb":"## Predict & submit","afb8220e":"## Training"}}