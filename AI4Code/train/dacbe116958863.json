{"cell_type":{"b9a5a688":"code","950f3665":"code","adba6e0f":"code","58d1e745":"code","e63960a4":"code","129c8813":"code","7c5e0821":"code","b5a99ef2":"code","ec6c19ab":"code","46e4b5a0":"code","c24fa042":"code","cc0bcde6":"code","b9c3621c":"code","c9b846a7":"code","e545f0e7":"code","6c6d844c":"code","ca2bd448":"code","2f859a8c":"markdown","bb7cbed4":"markdown","3ef2a6df":"markdown","e987b50d":"markdown","f9b0e126":"markdown","946bd9c4":"markdown","919f5c97":"markdown","71932170":"markdown","6f13da9d":"markdown","7021e74a":"markdown","f6e6794b":"markdown","9bc53175":"markdown"},"source":{"b9a5a688":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","950f3665":"from sklearn.datasets import make_blobs\nimport numpy as np\nfrom matplotlib import pyplot as plt\ncolors = ['royalblue','red','deeppink', 'maroon', 'mediumorchid', 'tan', 'forestgreen', 'olive', 'goldenrod', 'lightcyan', 'navy']\nvectorizer = np.vectorize(lambda x: colors[x % len(colors)])","adba6e0f":"from sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=200, random_state=8,cluster_std=[1.0, 2.5, 0.5])\nplt.scatter(X[:,0], X[:,1],c=vectorizer(y))","58d1e745":"from sklearn import mixture\ngmm = mixture.GaussianMixture(n_components=3, covariance_type='tied',max_iter=100,init_params='random')\ngmm.fit(X)\nyclust=gmm.predict(X)","e63960a4":"plt.scatter(X[:,0], X[:,1],c=vectorizer(yclust))","129c8813":"from sklearn import mixture\ngmm = mixture.GaussianMixture(n_components=3, covariance_type='tied',max_iter=100,init_params='random')\ngmm.fit(X)\nyclust=gmm.predict(X)","7c5e0821":"plt.scatter(X[:,0], X[:,1],c=vectorizer(yclust))","b5a99ef2":"from sklearn import metrics\ndef purity_score(y_true, y_pred):\n    # compute contingency matrix (also called confusion matrix)\n    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n    # return purity\n    return np.sum(np.amax(contingency_matrix, axis=0)) \/ np.sum(contingency_matrix) ","ec6c19ab":"from sklearn import datasets\nmyiris = datasets.load_iris()\nx = myiris.data\ny = myiris.target","46e4b5a0":"lab=list()\npure=list()\ngmm = mixture.GaussianMixture(n_components=3, covariance_type='tied',max_iter=10,init_params='random')\ngmm.fit(x)\nyclust=gmm.predict(x)\nlab.append('Random 10 tied')\npure.append(purity_score(y,yclust))","c24fa042":"pure","cc0bcde6":"gmm = mixture.GaussianMixture(n_components=3, covariance_type='tied',max_iter=100,init_params='random')\ngmm.fit(x)\nyclust=gmm.predict(x)\nlab.append('Random 100 tied')\npure.append(purity_score(y,yclust))\n\ngmm = mixture.GaussianMixture(n_components=3, covariance_type='full',max_iter=10,init_params='random')\ngmm.fit(x)\nyclust=gmm.predict(x)\nlab.append('Random 10 full')\npure.append(purity_score(y,yclust))","b9c3621c":"gmm = mixture.GaussianMixture(n_components=3, covariance_type='full',max_iter=100,init_params='random')\ngmm.fit(x)\nyclust=gmm.predict(x)\nlab.append('Random 100 full')\npure.append(purity_score(y,yclust))\n\ngmm = mixture.GaussianMixture(n_components=3, covariance_type='full',max_iter=100,init_params='kmeans')\ngmm.fit(x)\nyclust=gmm.predict(x)\nlab.append('Init kmeans 100 full')\npure.append(purity_score(y,yclust))","c9b846a7":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 3, init = 'random', max_iter = 300, n_init = 10, random_state = 0)\nyclust = kmeans.fit_predict(x)\nlab.append('kmeans')\npure.append(purity_score(y,yclust))","e545f0e7":"yclust.shape","6c6d844c":"d = {'Method':lab,'Purity':pure}\nimport pandas as pd\ndf = pd.DataFrame(d)\ndf.plot.barh(x='Method',y='Purity',title='EM with various paramters', color=tuple([\"g\", \"b\",\"r\",\"y\",\"k\"]))","ca2bd448":"n_components = np.arange(1, 15,2)\nmodels = [mixture.GaussianMixture(n, covariance_type='full', random_state=0).fit(x)\n          for n in n_components]\nplt.plot(n_components, [m.bic(x) for m in models], label='BIC')\nplt.xlabel('n_components');","2f859a8c":"![image.png](attachment:image.png)","bb7cbed4":"### BIC has two components","3ef2a6df":"# Queston 4: Clustering on iris dataset","e987b50d":"# Question 3: Effect of different paramters","f9b0e126":"* [<font size=4>Question 1:What are the parameters of GMM?<\/font>](#1)\n* [<font size=4>Question 2:Clustering using GMM<\/font>](#2)\n* [<font size=4>Question 3: Studying the Paramters of GMM <\/font>](#3)   \n* [<font size=4>Question 4: Clustering on iris<\/font>](#4)  \n* [<font size=5>Question 5: How to find the number of components<\/font>](#5)  ","946bd9c4":"### Cluster Evaluation using Purity","919f5c97":"* n_component : No of probability distributions\n* Covariance_type{\u2018full\u2019 (default), \u2018tied\u2019, \u2018diag\u2019, \u2018spherical\u2019} Full indicates both have diffrent covariance matrix and tied means same.  \n* max_iter int, defaults to 100.\n* n_init int, defaults to 1.\n* init_params{\u2018kmeans\u2019, \u2018random\u2019}\n* verbose_intervalint, default to 10.","71932170":"![image.png](attachment:image.png)","6f13da9d":"# Question 5: How to determine number of componets?","7021e74a":"### Creating sample data","f6e6794b":"# Question 2: Clsutering on Guassian Mixtures","9bc53175":"# Question 1: What are the parameters of Gaussian Mixture Model?"}}