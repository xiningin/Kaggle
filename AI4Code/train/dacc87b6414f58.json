{"cell_type":{"c39af578":"code","859f7384":"code","e82cd688":"code","b8be450f":"code","5dd80d71":"code","ab986aba":"markdown","07ddff5d":"markdown","0064bf8b":"markdown","6f9a2e9c":"markdown","014a5f5b":"markdown","b078eda4":"markdown","ef3d73c8":"markdown","e4d4a029":"markdown","cb5b00c9":"markdown","bbbb23bb":"markdown","7344aece":"markdown"},"source":{"c39af578":"# Import glob to parse filenames\nimport glob\n\n# Import numpy\nimport numpy as np\n\n# Plot\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [10,10]\n\n# Opencv\nimport cv2\n\n# Path to data\ndata_folder = f\"\/kaggle\/input\/global-wheat-detection\/train\/\"\n\n# Read filenames in the data folder\nfilenames = glob.glob(f\"{data_folder}*.jpg\")\n\n# Read first 10 filenames\nimage_paths = filenames[:10]\n\n# Display a sample image\nplt.imshow(cv2.cvtColor(cv2.imread(image_paths[0]), cv2.COLOR_BGR2RGB)); plt.show();\n","859f7384":"# Create batch of 4 images\nimage_batch = []\nimage_batch_labels = []\n\nn_images = 4\n\nfor i in range(n_images):\n    image = cv2.cvtColor(cv2.imread(image_paths[i]), cv2.COLOR_BGR2RGB)\n    image_batch.append(image)\n    \n    label_temp = list(np.floor(np.random.rand(1)*2.99).astype(int))[0]\n    if label_temp == 0:\n        label = [1,0,0]\n    elif label_temp == 1:\n        label = [0,1,0]\n    else: # label_temp == 2\n        label = [0,0,1]\n    \n    image_batch_labels.append(label)\n\n# Convert image_batch to numpy array\nimage_batch = np.array(image_batch)\n# Conver image_batch_labels to numpy array\nimage_batch_labels = np.array(image_batch_labels)\n    \n# Print labels\nprint()\nprint(f\"Image labels: {image_batch_labels}\\n\")\n\n# Show images\nfor i in range(2):\n    for j in range(2):\n        plt.subplot(2,2,2*i+j+1)\n        plt.imshow(image_batch[2*i+j])\nplt.show()\n","e82cd688":"def rand_bbox(size, lamb):\n    \"\"\" Generate random bounding box \n    Args:\n        - size: [width, breadth] of the bounding box\n        - lamb: (lambda) cut ratio parameter\n    Returns:\n        - Bounding box\n    \"\"\"\n    W = size[0]\n    H = size[1]\n    cut_rat = np.sqrt(1. - lamb)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w \/\/ 2, 0, W)\n    bby1 = np.clip(cy - cut_h \/\/ 2, 0, H)\n    bbx2 = np.clip(cx + cut_w \/\/ 2, 0, W)\n    bby2 = np.clip(cy + cut_h \/\/ 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2","b8be450f":"# Read an image\nimage = cv2.cvtColor(cv2.imread(image_paths[0]), cv2.COLOR_BGR2RGB)\n\n# Crop a random bounding box\nlamb = 0.3\nsize = image.shape\nbbox = rand_bbox(size, lamb)\n\n# Draw bounding box on the image\nim = image.copy()\nx1 = bbox[0]\ny1 = bbox[1]\nx2 = bbox[2]\ny2 = bbox[3]\ncv2.rectangle(im, (x1, y1), (x2, y2), (255, 0, 0), 3)\nplt.imshow(im);\nplt.title('Original image with random bounding box')\nplt.show();\n\n# Show cropped image\nplt.imshow(image[y1:y2, x1:x2]);\nplt.title('Cropped image')\nplt.show()","5dd80d71":"def generate_cutmix_image(image_batch, image_batch_labels, beta):\n    \"\"\" Generate a CutMix augmented image from a batch \n    Args:\n        - image_batch: a batch of input images\n        - image_batch_labels: labels corresponding to the image batch\n        - beta: a parameter of Beta distribution.\n    Returns:\n        - CutMix image batch, updated labels\n    \"\"\"\n    # generate mixed sample\n    lam = np.random.beta(beta, beta)\n    rand_index = np.random.permutation(len(image_batch))\n    target_a = image_batch_labels\n    target_b = image_batch_labels[rand_index]\n    bbx1, bby1, bbx2, bby2 = rand_bbox(image_batch[0].shape, lam)\n    image_batch_updated = image_batch.copy()\n    image_batch_updated[:, bbx1:bbx2, bby1:bby2, :] = image_batch[rand_index, bbx1:bbx2, bby1:bby2, :]\n    \n    # adjust lambda to exactly match pixel ratio\n    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) \/ (image_batch.shape[1] * image_batch.shape[2]))\n    label = target_a * lam + target_b * (1. - lam)\n    \n    return image_batch_updated, label\n\n\n# Generate CutMix image\n# Let's use the first image of the batch as the input image to be augmented\ninput_image = image_batch[0]\nimage_batch_updated, image_batch_labels_updated = generate_cutmix_image(image_batch, image_batch_labels, 1.0)\n\n# Show original images\nprint(\"Original Images\")\nfor i in range(2):\n    for j in range(2):\n        plt.subplot(2,2,2*i+j+1)\n        plt.imshow(image_batch[2*i+j])\nplt.show()\n\n\n# Show CutMix images\nprint(\"CutMix Images\")\nfor i in range(2):\n    for j in range(2):\n        plt.subplot(2,2,2*i+j+1)\n        plt.imshow(image_batch_updated[2*i+j])\nplt.show()\n\n# Print labels\nprint('Original labels:')\nprint(image_batch_labels)\nprint('Updated labels')\nprint(image_batch_labels_updated)","ab986aba":"## Generating a Batch of Data","07ddff5d":"# <center> Improving Classification and Object Detection <\/center> \n# <center> using CutMix Augmentation<\/center>\n\n[Supplementary to my medium article: https:\/\/medium.com\/@ZenFeather\/cutmix-augmentation-in-python-bf099a97afac]\n\nThe paper CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features published in ICCV 2019 proposed the CutMix augmentation strategy to improve deep learning-based classification and localization tasks.\u00a0\n\nIn this augmentation technique, patches are cut and pasted among training images where interestingly the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperformed the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task.\n\nThe paper also shows that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances.\n\nIn the following table, we can see the performance boost obtained by adopting CutMix augmentation.\n\n![image.png](attachment:image.png)","0064bf8b":"## A CutMix Image Generation","6f9a2e9c":"## References:\n- https:\/\/arxiv.org\/pdf\/1905.04899.pdf\n- https:\/\/github.com\/clovaai\/CutMix-PyTorch","014a5f5b":"## Generating Random Bounding Box\n\nHere is a function to generate a random bounding box in an image:","b078eda4":"In CutMix algorithm, for each image of batch a random region (in our case, a random bounding box region) is replaced with a patch from another training image. \n- The parameter ```lambda``` that determines the size of the bounding box is stochastically sampled from a Beta distribution(https:\/\/en.wikipedia.org\/wiki\/Beta_distribution).\n- The label of each resultant augmented image is estimated as a weighted sum of the original label and the label of the image from which the modified patch is borrowed, where weights are ```lambda``` and ```(1-lambda)``` respectively.","ef3d73c8":"Let's start.\n\n## Loading Data","e4d4a029":"To show the result of CutMix augmentation we will use a set of images containing images of wheat field showing wheat heads. These images are takes from Kaggle's Glbal Wheat Detection competition dataset.","cb5b00c9":"Now, let's test the function by generating a random bounding box and crop an image using that bounding box.","bbbb23bb":"Now, we will create a set or \"batch\" of 4 images. Let's assume that this batch is a minibacth generated by a dataloader that loads minibatches during the training of the deep neural network model. In general, the bacth loaded by a dataloader also includes a batch of image labels if we are solving a classification problem. We will randomly assign a label to each image from from the set of {0,1,2}. We will further one-hot encode(https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) these labels to {[1,0,0], [0,1,0], [0,0,1]}. Please follow this((https:\/\/stanford.edu\/~shervine\/blog\/pytorch-how-to-generate-data-parallel)) to learn more about dataloaders in PyTorch.","7344aece":"Now, you can use CutMix augmentation in training you deep neural network model. For more details please follow the references. If you have any questions, please put them in the comment section."}}