{"cell_type":{"7bec8458":"code","ca7feab7":"code","cb8a433a":"code","cb0671c7":"code","b749a604":"code","f946ee67":"code","931b9cbf":"code","4dc387d8":"code","14735951":"code","3d7874cf":"code","e72b2ec0":"code","93617bfd":"code","d96767ef":"code","09568385":"code","17e40ecc":"code","2b4fbec4":"code","d17585f5":"code","54d5a519":"code","24e3695f":"code","4a01bba0":"code","1f67a19c":"code","8fb2d2e4":"code","dfb48b00":"code","9f0b67e7":"code","8d40e9f8":"code","1326dae1":"code","eb593614":"code","9d2b43f5":"markdown","4c648c1e":"markdown","c6ed229e":"markdown","1efe38ed":"markdown","ebba4485":"markdown","13f4ff33":"markdown","d344cf84":"markdown","f97e57e7":"markdown","ddf51b41":"markdown","4e4518ca":"markdown","88ce0101":"markdown","8fc22666":"markdown","6ed3d0cb":"markdown","7297aa48":"markdown","dd5a413d":"markdown","aff976c1":"markdown","864dd34e":"markdown","4020c82a":"markdown","a21ab904":"markdown","b8f4735a":"markdown","d5b5cef5":"markdown","109b4ffc":"markdown","dbc1c8aa":"markdown","c66ca85c":"markdown"},"source":{"7bec8458":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nfrom IPython.display import HTML\nimport os\nprint(os.listdir(\"..\/input\"))\n\n%matplotlib inline\n\ndf=pd.read_csv('..\/input\/train.csv')\ndf.head()","ca7feab7":"df.count()","cb8a433a":"df['Path']=df['Image'].map(lambda x:'..\/input\/train\/{}'.format(x))\ndf.head()","cb0671c7":"df['Id'].nunique()","b749a604":"df['Id'].value_counts().head(20)","f946ee67":"random_whale=np.random.choice(df['Path'],2)\nfor whale in random_whale:\n    image=Image.open(whale)\n    plt.imshow(image)\n    plt.show()","931b9cbf":"from keras.preprocessing import image\nfrom keras.applications.imagenet_utils import preprocess_input\n\ndef add_img(dataset,shape,img_size):\n    \n    x_train = np.zeros((shape, img_size[0], img_size[1], img_size[2]))\n    count = 0\n    \n    for fig in dataset.itertuples():\n        \n        #load train data images into images of specified size\n        img = image.load_img(fig.Path, target_size=img_size)\n        x = image.img_to_array(img)\n        x = preprocess_input(x)\n        x_train[count] = x\n        count += 1\n    \n    return x_train","4dc387d8":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils.np_utils import to_categorical\ndef label(y):\n    y_train=np.array(y)\n    label_encoder = LabelEncoder()\n    y_train = label_encoder.fit_transform(y_train)\n    y_train = to_categorical(y_train, num_classes = 5005)\n    return y_train,label_encoder","14735951":"x_train=add_img(df,df.shape[0],(100,100,3))\ny_train,encoder=label(df['Id'])\nx_train\/=255 #Normalizing the data","3d7874cf":"y_train.shape","e72b2ec0":"# Importing the Keras packages\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator","93617bfd":"classifier = Sequential()","d96767ef":"classifier.add(Convolution2D(16, 5, 5, input_shape = (100,100, 3), activation = 'relu'))","09568385":"classifier.add(MaxPooling2D(pool_size = (2, 2)))","17e40ecc":"classifier.add(Convolution2D(16, 5, 5, activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\nclassifier.add(Dropout(0.25))","2b4fbec4":"classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\nclassifier.add(Dropout(0.25))","d17585f5":"classifier.add(Convolution2D(64, 3, 3, activation = 'relu'))\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\nclassifier.add(Dropout(0.25))","54d5a519":"classifier.add(Flatten())","24e3695f":"classifier.add(Dense(output_dim = 240, activation = 'relu'))\nclassifier.add(BatchNormalization())\nclassifier.add(Dense(output_dim = y_train.shape[1], activation = 'sigmoid'))","4a01bba0":"from keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau\n\n# Define the optimizer\nadam_optimizer = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)\n\n# Set a learning rate annealer\nlearning_rate = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","1f67a19c":"classifier.compile(optimizer = adam_optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])","8fb2d2e4":"classifier.summary()","dfb48b00":"whale_detector = classifier.fit(x_train, y_train, epochs=60, batch_size=1000, verbose=10, callbacks=[learning_rate])","9f0b67e7":"test = os.listdir(\"..\/input\/test\/\")\ntest_df = pd.DataFrame(test, columns=['Image'])\ntest_df['Path']=test_df['Image'].map(lambda x:'..\/input\/test\/{}'.format(x))\nx_test=add_img(test_df,test_df.shape[0],(100,100,3))\nx_test\/255\npred=classifier.predict(np.array(x_test),verbose=1)#Since numpy array is faster than df","8d40e9f8":"# Plot the loss curve for training\nplt.plot(whale_detector.history['loss'], color='r', label=\"Train Loss\")\nplt.title(\"Train Loss\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","1326dae1":"# Plot the accuracy curve for training\nplt.plot(whale_detector.history['acc'], color='g', label=\"Train Accuracy\")\nplt.title(\"Train Accuracy\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","eb593614":"test_df['Id']=''\nfor index,prediction in enumerate(pred):\n    test_df.loc[index, 'Id'] = ' '.join(encoder.inverse_transform(prediction.argsort()[-5:][::-1]))\ntest_df.drop(['Path'],axis=1,inplace=True)\ntest_df.to_csv('submission.csv', index=False)\ntest_df.head()","9d2b43f5":"# Part 1  - Keras Pre Processing","4c648c1e":"The method listdir() returns a list containing the names of the entries in the directory given by path. The list is made a data frame and the rest is same as we did for the train data. ","c6ed229e":"The feature 'Id' is categorical since it is the label for each Image. It represents the category\/species in which each whale in the train data belongs. Since machine learning models need numerical data for processing, we have toencode the categorical content into numerical values. ","1efe38ed":"Let's add a new column into the data frame which indicates the path of each file.","ebba4485":"# Part 2 - Fitting the CNN to the images","13f4ff33":"# Step 4 - Full connection","d344cf84":"# Step 1 - Convolution","f97e57e7":"Convolution is the first layer to extract features from an input image. Convolution preserves the relationship between pixels by learning image features using small squares of input data. It is a mathematical operation that takes two inputs such as image matrix and a filter or kernal\n\nMax pooling is a type of operation that is typically added to CNNs following individual convolutional layers. When added to a model, max pooling reduces the dimensionality of images by reducing the number of pixels in the output from the previous convolutional layer.\n\nDropout is a technique where randomly selected neurons are ignored during training. They are \u201cdropped-out\u201d randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. As a neural network learns, neuron weights settle into their context within the network. Weights of neurons are tuned for specific features providing some specialization. Neighboring neurons become to rely on this specialization, which if taken too far can result in a fragile model too specialized to the training data. This reliant on context for a neuron during training is referred to complex co-adaptations. You can imagine that if neurons are randomly dropped out of the network during training, that other neurons will have to step in and handle the representation required to make predictions for the missing neurons. This is believed to result in multiple independent internal representations being learned by the network. The effect is that the network becomes less sensitive to the specific weights of neurons. This in turn results in a network that is capable of better generalization and is less likely to overfit the training data.\n\nFlatten() flattens the output and feed into a fully connected layer (FC Layer)","ddf51b41":"Now let's prepare the y_train. y_train contain the labels, whale name of each whale image in x_train. The labels are categorical values and hence they need to be numrically encoded. For this LabelEncoder,OneHotEncoder functions from the ScikitLearn library is used. ","4e4518ca":"# Optimizer And Annealer","88ce0101":"The Adam optimization algorithm is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing.\n\nStochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training.\n\n**Adaptive Gradient Algorithm** (AdaGrad) that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n\n**Root Mean Square Propagation** (RMSProp) that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems","8fc22666":"# Step 2 - Pooling","6ed3d0cb":"# Let's Predict the model for test Images","7297aa48":"# **Introduction**\n\nAfter centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food.\n\nTo aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales\u2019 tails and unique markings found in footage to identify what species of whale they\u2019re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized.\n\nIn this competition, we\u2019re challenged to build an algorithm to identify individual whales in images. we\u2019ll analyze Happywhale\u2019s database of over 25,000 images, gathered from research institutions and public contributors. By contributing, we\u2019ll help to open rich fields of understanding for marine mammal population dynamics around the globe.","dd5a413d":"# Initialising the CNN","aff976c1":"# Submission","864dd34e":"Now let's prepare the data for Keras CNN. Let's create x_train and y_train which will be fitted to the keras for training the model. x_train will contain all the images in train dataset and y_train will contain the corresponding Id\/label of each whale image.  img_to_array converts a PIL image instance to numpy array. The images will all be reshaped. ","4020c82a":"# Lets Plot the Loss and Accuracy changers per epoch","a21ab904":"Let's open 2 random whale Image fron the train data. ","b8f4735a":"# **Available Data**\n\nThis training data contains thousands of images of humpback whale flukes. Individual whales have been identified by researchers and given an Id. The challenge is to predict the whale Id of images in the test set. What makes this such a challenge is that there are only a few examples for each of 3,000+ whale Ids.\n\n# File descriptions\n\n* **train.zip** - a folder containing the training images\n* **train.csv** - maps the training Image to the appropriate whale Id. Whales that are not predicted to have a label identified in the training data should be labeled as new_whale.\n* **test.zip** - a folder containing the test images to predict the whale Id\n","d5b5cef5":"# Compiling the CNN","109b4ffc":"# Step 3 - Flattening","dbc1c8aa":"In the csv file, the feature 'Image' represents the file name of each photos in the train.zip. The feature 'Id' represents the category of the whale in the correspond row feature 'Image. Those whale in the image that doesn't have a label isto be represented as a new_whale. ","c66ca85c":"# Adding a second convolutional layer"}}