{"cell_type":{"657da9b1":"code","3e444701":"code","1cfdb825":"code","7de9764e":"code","04d1c36f":"code","ba25c9b4":"code","96ebefaa":"code","ef4a7611":"code","48af751a":"code","f053b590":"code","08b28fe2":"code","9ea52c23":"code","019218e8":"code","ac492058":"code","1aadb44d":"code","29264349":"code","05a57254":"code","cfe9b339":"code","3402bbd3":"code","213f2968":"code","088cbae5":"code","6e207c0e":"code","2c53673d":"code","47fd82cf":"code","f2c9d914":"code","d1a582c0":"code","80328657":"code","f1ccb09d":"code","4c03c65f":"code","4d913475":"code","40ac3a11":"code","02775b23":"code","dd4203f3":"code","770c4fc9":"code","d9090ee1":"code","72493887":"code","acdeef17":"code","6b37a938":"code","6fc45166":"code","ab78b2c6":"code","1c86878e":"code","6f975928":"code","b2d4cbfd":"code","0ffa880f":"code","f1b79b28":"code","07b17ad5":"code","9cb8b0ef":"code","a19c674d":"code","7ecc8fcc":"code","d7d192fb":"code","1853d94a":"code","47434207":"code","9570001f":"code","c7b6b4e0":"code","b96b5ab7":"code","3b9385b4":"code","ad0130c3":"code","ad3c48d0":"code","7e51f6e3":"markdown","2eaab957":"markdown","f31b610d":"markdown","6cd2bb9f":"markdown","aadcffc3":"markdown","84749f0e":"markdown","0e95d1e5":"markdown","25659162":"markdown","4a1e536b":"markdown","4b10d956":"markdown","4daabca8":"markdown","4b28f628":"markdown","5da809dc":"markdown","264dee63":"markdown","9ac70dd1":"markdown","630d1137":"markdown","693e271b":"markdown","7a962811":"markdown","138e2f91":"markdown","73ce4923":"markdown","fd8ec241":"markdown","eee372c3":"markdown","689d303d":"markdown","3ed0b219":"markdown","1dc13680":"markdown","8632c726":"markdown","d6ff9454":"markdown","14f834be":"markdown","9474e8e6":"markdown","373b778d":"markdown","5e0dddda":"markdown"},"source":{"657da9b1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors  import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV,cross_val_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, auc\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n","3e444701":"new_data=pd.read_csv('..\/input\/ML_Parkinson.csv')","1cfdb825":"new_data.head(5)","7de9764e":"new_data.tail(5)","04d1c36f":"new_data.sample(5)","ba25c9b4":"new_data.sample(frac=0.01)","96ebefaa":"new_data.describe","ef4a7611":"new_data.info()","48af751a":"train_col=[]\nfor col in new_data.columns:\n    train_col.append(col)\ntrain_col","f053b590":"new_data.shape","08b28fe2":"sns.countplot(new_data['Class'].values)\nplt.xlabel('Class Values')\nplt.ylabel('Class Counts')\nplt.show()","9ea52c23":"index_class=[]\nfor i in range(100):\n    index_class.append(i)","019218e8":"plt.scatter(x=index_class,y=new_data[new_data['Class']==1].f1[:100],color='b')\nplt.scatter(x=index_class,y=new_data[new_data['Class']==0].f1[:100],color='r')\nplt.show()","ac492058":"plt.scatter(x=index_class,y=new_data[new_data['Class']==1].f2[:100],color='b')\nplt.scatter(x=index_class,y=new_data[new_data['Class']==0].f2[:100],color='r')\nplt.show()","1aadb44d":"plt.scatter(x=index_class,y=new_data[new_data['Class']==1].f3[:100],color='b')\nplt.scatter(x=index_class,y=new_data[new_data['Class']==0].f3[:100],color='r')\nplt.show()","29264349":"plt.scatter(x=index_class,y=new_data[new_data['Class']==1].f4[:100],color='b')\nplt.scatter(x=index_class,y=new_data[new_data['Class']==0].f4[:100],color='r')\nplt.show()","05a57254":"plt.scatter(x=index_class,y=new_data[new_data['Class']==1].f5[:100],color='b')\nplt.scatter(x=index_class,y=new_data[new_data['Class']==0].f5[:100],color='r')\nplt.show()","cfe9b339":"plt.scatter(x=index_class,y=new_data[new_data['Class']==1].f6[:100],color='b')\nplt.scatter(x=index_class,y=new_data[new_data['Class']==0].f6[:100],color='r')\nplt.show()","3402bbd3":"new_data.shape","213f2968":"pd.plotting.scatter_matrix(new_data.loc[:,new_data.columns != 'Class'], c=['green','blue','red'],\n                            figsize= [15,15],\n                            diagonal='hist',\n                            alpha=0.8,\n                            s = 200,\n                            marker = '.',\n                            edgecolor= \"black\")\nplt.show()","088cbae5":"liste_data=[]\nfor i in range(0,252):\n    liste_data.append(i)","6e207c0e":"for i, col in enumerate(new_data.iloc[:,0:6].columns.values):\n    plt.subplot(6, 2, i+1)\n    plt.plot(new_data[col].values.tolist())\n    plt.title(col)\n    fig, ax = plt.gcf(), plt.gca()\n    fig.set_size_inches(10, 10)\n    plt.tight_layout()\n    plt.show()","2c53673d":"new_data[new_data.Class==1].head()","47fd82cf":"new_data[new_data.Class==0].head()","f2c9d914":"df = pd.DataFrame(np.random.randn(646, 24), columns=new_data.columns)\nscatter_matrix(df, alpha=0.2, figsize=(20, 20), diagonal='kde')\nplt.show()","d1a582c0":"new_data.iloc[:,1:5].corr()","80328657":"new_data.isnull().sum()","f1ccb09d":"data=pd.read_csv('..\/input\/ML_Parkinson_son_dataset.csv')","4c03c65f":"data.shape","4d913475":"data.Class.value_counts()","40ac3a11":"import statsmodels.formula.api as sm\nX=np.append(arr=np.ones((727,1)).astype(int),values=data,axis=1)\n\nX_l=data.iloc[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]].values\nr=sm.OLS(endog=data.iloc[:,-1:],exog=X_l).fit()\nprint(r.summary())","02775b23":"dataX=data.drop('Class',axis=1)\ndataY=data['Class']","dd4203f3":"X_train,X_test,y_train,y_test=train_test_split(dataX,dataY,test_size=0.2,random_state=0)","770c4fc9":"print('X_train',X_train.shape)\nprint('X_test',X_test.shape)\nprint('y_train',y_train.shape)\nprint('y_test',y_test.shape)","d9090ee1":"sc=StandardScaler()\nX_train=sc.fit_transform(X_train)\nX_test=sc.transform(X_test)","72493887":"from sklearn.decomposition import PCA\npca=PCA(n_components=19)\nX_train=pca.fit_transform(X_train)\nX_test=pca.transform(X_test)","acdeef17":"print('X_train',X_train.shape)\nprint('X_test',X_test.shape)\nprint('y_train',y_train.shape)\nprint('y_test',y_test.shape)","6b37a938":"X_train=pd.DataFrame(data=X_train,columns=['f1','f2','f3','f4','f5','f6','f7','f8','f9','f10','f11','f12','f13','f14','f15','f16','f17','f18','f19'])\nX_test=pd.DataFrame(data=X_test,columns=['f1','f2','f3','f4','f5','f6','f7','f8','f9','f10','f11','f12','f13','f14','f15','f16','f17','f18','f19'])\n#y_train=pd.DataFrame(data=y_train,columns=['Class'])\n#y_test=pd.DataFrame(data=y_test,columns=['Class'])","6fc45166":"print('X_train',X_train.shape)\nprint('X_test',X_test.shape)\nprint('y_train',y_train.shape)\nprint('y_test',y_test.shape)","ab78b2c6":"def plot_roc_(false_positive_rate,true_positive_rate,roc_auc):\n    plt.figure(figsize=(5,5))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],linestyle='--')\n    plt.axis('tight')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","1c86878e":"def plot_feature_importances(gbm):\n    n_features = X_train.shape[1]\n    plt.barh(range(n_features), gbm.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), X_train.columns)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.ylim(-1, n_features)","6f975928":"combine_features_list=[\n    ('f1','f2','f3','f4','f5','f6'),\n    ('f6','f7','f8','f9','f10','f11'),\n    ('f9','f10','f11','f12','f13','f14'),\n    ('f15','f16','f17','f18','f19')\n]","b2d4cbfd":"parameters=[\n{\n    'penalty':['l1','l2'],\n    'C':[0.1,0.4,0.5],\n    'random_state':[0]\n    },\n]\n\nfor features in combine_features_list:\n    print(features)\n    print(\"*\"*50)\n    \n    X_train_set=X_train.loc[:,features]\n    X_test_set=X_test.loc[:,features]\n    \n    gslog=GridSearchCV(LogisticRegression(),parameters,scoring='accuracy')\n    gslog.fit(X_train_set,y_train)\n    print('Best parameters set:')\n    print(gslog.best_params_)\n    print()\n    predictions=[\n    (gslog.predict(X_train_set),y_train,'Train'),\n    (gslog.predict(X_test_set),y_test,'Test1'),\n    ]\n    \n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1],pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n\n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=LogisticRegression(),X=X_train,y=y_train,cv=12)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","0ffa880f":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression(C=0.1,penalty='l1',random_state=0)\nlr.fit(X_train,y_train)\n\ny_pred=lr.predict(X_test)\n\n\ny_proba=lr.predict_proba(X_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\n#print('Hata Oran\u0131 :',r2_score(y_test,y_pred))\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"Logistic TRAIN score with \",format(lr.score(X_train, y_train)))\nprint(\"Logistic TEST score with \",format(lr.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","f1b79b28":"parameters=[\n{\n    'n_neighbors':np.arange(2,33),\n    'n_jobs':[2,6]\n    },\n]\nprint(\"*\"*50)\nfor features in combine_features_list:\n    print(\"*\"*50)\n    \n    X_train_set=X_train.loc[:,features]\n    X_test_set=X_test.loc[:,features]\n   \n    gsknn=GridSearchCV(KNeighborsClassifier(),parameters,scoring='accuracy')\n    gsknn.fit(X_train_set,y_train)\n    print('Best parameters set:')\n    print(gsknn.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (gsknn.predict(X_train_set), y_train, 'Train'),\n    (gsknn.predict(X_test_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=KNeighborsClassifier(),X=X_train,y=y_train,cv=12)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50) ","07b17ad5":"knn=KNeighborsClassifier(n_jobs=2, n_neighbors=22)\nknn.fit(X_train,y_train)\n\ny_pred=knn.predict(X_test)\n\ny_proba=knn.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"KNN TRAIN score with \",format(knn.score(X_train, y_train)))\nprint(\"KNN TEST score with \",format(knn.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","9cb8b0ef":"n_neighbors = range(1, 17)\ntrain_data_accuracy = []\ntest1_data_accuracy = []\nfor n_neigh in n_neighbors:\n    knn = KNeighborsClassifier(n_neighbors=n_neigh,n_jobs=5)\n    knn.fit(X_train, y_train)\n    train_data_accuracy.append(knn.score(X_train, y_train))\n    test1_data_accuracy.append(knn.score(X_test, y_test))\nplt.plot(n_neighbors, train_data_accuracy, label=\"Train Data Set\")\nplt.plot(n_neighbors, test1_data_accuracy, label=\"Test1 Data Set\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Neighbors\")\nplt.legend()\nplt.show()","a19c674d":"n_neighbors = range(1, 17)\nk_scores=[]\nfor n_neigh in n_neighbors:\n    knn = KNeighborsClassifier(n_neighbors=n_neigh,n_jobs=5)\n    scores=cross_val_score(estimator=knn,X=X_train,y=y_train,cv=12)\n    k_scores.append(scores.mean())\nprint(k_scores)","7ecc8fcc":"plt.plot(n_neighbors,k_scores)\nplt.xlabel('Value of k for KNN')\nplt.ylabel(\"Cross-Validated Accurancy\")\nplt.show()","d7d192fb":"parameters = [\n    {\n        'kernel': ['linear'],\n        'random_state': [2]\n    },\n    {\n        'kernel': ['rbf'],\n        'gamma':[0.9,0.06,0.3],\n        'random_state': [0],\n        'C':[1,2,3,4,5,6],\n        'degree':[2],\n        'probability':[True]\n    },\n]\n\nfor features in combine_features_list:\n    print(\"*\"*50)\n    X_train_set=X_train.loc[:,features]\n    X_test_set=X_test.loc[:,features]\n  \n    svc = GridSearchCV(SVC(), parameters,\n    scoring='accuracy')\n    svc.fit(X_train_set, y_train)\n    print('Best parameters set:')\n    print(svc.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (svc.predict(X_train_set), y_train, 'Train'),\n    (svc.predict(X_test_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=SVC(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","1853d94a":"svc=SVC(C=5,degree=2,gamma=0.06,kernel='rbf',probability=True,random_state=0)\nsvc.fit(X_train,y_train)\n\ny_pred=svc.predict(X_test)\n\ny_proba=svc.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"SVC TRAIN score with \",format(svc.score(X_train, y_train)))\nprint(\"SVC TEST score with \",format(svc.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","47434207":"parameters = [\n{\n    'learning_rate': [0.01, 0.02, 0.002],\n    'random_state': [0],\n    'n_estimators': np.arange(3, 20)\n    },\n]\nfor features in combine_features_list:\n    print(\"*\"*50)\n    X_train_set=X_train.loc[:,features]\n    X_test1_set=X_test.loc[:,features]\n   \n    gbc = GridSearchCV(GradientBoostingClassifier(), parameters, scoring='accuracy')\n    gbc.fit(X_train_set, y_train)\n    print('Best parameters set:')\n    print(gbc.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (gbc.predict(X_train_set), y_train, 'Train'),\n    (gbc.predict(X_test1_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=GradientBoostingClassifier(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","9570001f":"gbc=GradientBoostingClassifier(learning_rate=0.02,n_estimators=18,random_state=0)\ngbc.fit(X_train,y_train)\n\ny_pred=gbc.predict(X_test)\n\ny_proba=gbc.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"GradientBoostingClassifier TRAIN score with \",format(gbc.score(X_train, y_train)))\nprint(\"GradientBoostingClassifier TEST score with \",format(gbc.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","c7b6b4e0":"plot_feature_importances(gbc)\nplt.show()","b96b5ab7":"parameters = [\n    {\n        'max_depth': np.arange(1, 10),\n        'min_samples_split': np.arange(2, 5),\n        'random_state': [3],\n        'n_estimators': np.arange(10, 20)\n    },\n]\n\nfor features in combine_features_list:\n    print(\"*\"*50)\n    \n    X_train_set=X_train.loc[:,features]\n    X_test1_set=X_test.loc[:,features]\n    \n    tree=GridSearchCV(RandomForestClassifier(),parameters,scoring='accuracy')\n    tree.fit(X_train_set, y_train)\n    \n    print('Best parameters set:')\n    print(tree.best_params_)\n    print(\"*\"*50)\n    predictions = [\n        (tree.predict(X_train_set), y_train, 'Train'),\n        (tree.predict(X_test1_set), y_test, 'Test1')\n    ]\n    \n    for pred in predictions:\n        \n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n    \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=RandomForestClassifier(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)\n   \n   \n    ","3b9385b4":"rfc=RandomForestClassifier(max_depth=7,min_samples_split=4,n_estimators=19,random_state=3)\nrfc.fit(X_train,y_train)\n\ny_pred=rfc.predict(X_test)\n\ny_proba=rfc.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"RandomForestClassifier TRAIN score with \",format(rfc.score(X_train, y_train)))\nprint(\"RandomForestClassifier TEST score with \",format(rfc.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","ad0130c3":"for i in range(1,11):\n    rf = RandomForestClassifier(n_estimators=i, random_state = 3, max_depth=7)\n    rf.fit(X_train, y_train)\n    print(\"TEST set score w\/ \" +str(i)+\" estimators: {:.5}\".format(rf.score(X_test, y_test)))","ad3c48d0":"plot_feature_importances(rf)\nplt.show()","7e51f6e3":"<p> The code snippet on the top represents the numbers we will write in the x index to plot our data. <\/p>\n<p> In the drawing process in the lower part, there are sling and discrete data found within the f1 property found in our data. We want to get a better data set by clearing this data. <\/p>","2eaab957":"<p> The info function at the bottom is a function that identifies which variable our data has and the way in which our variable names are defined. <\/p>","f31b610d":"<p> The difference in value between our data is too big. It increases both the volume and the results are very bad. To reduce this situation we need to use the StandardScaler function. A value of -1.1 will be obtained after using it. <\/p>","6cd2bb9f":"<p> Another function is the tail function. This function is a function that helps us get the last five lines in our data.<\/p>","aadcffc3":"<p> In the process below, we need to delete unnecessary features in the content of our train_data. In addition, it is also found in test_data. These features can be characterized as person id and sound id. If we use them in the model we will train them, our model has a higher memorization rate. Because we had to delete the subject_id value because it was an incremental value and did not contribute in any way. In this way, we have equalized the number of features of our data. <\/p>","84749f0e":"<p> All correlation values between the data are listed in the previous sections. As a result of this listing, it is aimed to ensure that these properties are used in different places by performing different operations. Thus, the p-value process determines a hypothesis and a hypothesis thesis is presented between each characteristic according to this hypothesis. In this process, after determining the Class property as hypothesis, the relations between all the other properties are checked. This results in a different number for each property. What is important here is that these numbers are not close to 1.00. If the number is close to 1.00 this is very bad. <\/p>","0e95d1e5":"<p> After the data is loaded, we need to check what's in the content of our data before analyzing our data. Because our data in the old data, slingshot data, incompatible data, etc. can. If we use it in this way, we can get unexpected results in the model we will create. <\/p>\n<p> In this section, we'll look at recognizing the data. In this way, we will explore the uncertainties and determinations among our data. The first function is the head function. This function lists the first five lines in our data. <\/p>","25659162":"<p> In this section, shape fonsky is a method used to indicate the number of properties and the number of attributes that are contained in our data. This function can accurately count the content of our data. The reason we use this is the method used to measure the status of feature numbers. <\/p>","4a1e536b":"<table border=2>\n<tr>\n<th>\u00d6zellikler<\/th>\n<th>Parametreler<\/th>\n<th>Train Set<\/th>\n<th>Test Set<\/th>\n<\/tr>\n<tr>\n<td>(f1', 'f2', 'f3', 'f4', 'f5', 'f6')<\/td>\n<td>{'C': 5, 'degree': 2, 'gamma': 0.06, 'kernel': 'rbf', 'probability': True, 'random_state': 0}<\/td>\n<td>% 97<\/td>\n<td>% 92<\/td>\n<\/tr>\n\n<tr>\n<td>('f6', 'f7', 'f8', 'f9', 'f10', 'f11')<\/td>\n<td>{'C': 3, 'degree': 2, 'gamma': 0.9, 'kernel': 'rbf', 'probability': True, 'random_state': 0}<\/td>\n<td>% 93<\/td>\n<td>% 85<\/td>\n<\/tr>\n\n<tr>\n<td>('f9', 'f10', 'f11', 'f12', 'f13', 'f14')<\/td>\n<td>{'C': 1, 'degree': 2, 'gamma': 0.9, 'kernel': 'rbf', 'probability': True, 'random_state': 0}<\/td>\n<td>% 75<\/td>\n<td>% 54<\/td>\n<\/tr>\n\n<tr>\n<td>('f15', 'f16', 'f17', 'f18', 'f19')<\/td>\n<td>{'kernel': 'linear', 'random_state': 2}<\/td>\n<td>% 41<\/td>\n<td>% 36<\/td>\n<\/tr>\n\n<\/table>","4b10d956":"<p> Analyzes of the patients were taken for the first five lines. <\/p>\n<p>1 ill<\/p>\n<p>0 not ill<\/p>","4daabca8":"<p> Logistic Regression <\/p>\n<p> First we need parameters to use our data more effectively. Hyperthermatic technique was used for this condition. This technique is used to express different features in the process. <\/p>","4b28f628":"<p> We need to reduce the size of our data because our data is very large and increases the transaction volume. As a result, basic component analysis, known as PCA, is used. In this way, we are planning to reduce our number of features from 24 to 19. If we lower it more, this situation is not good at all. In other words, training and test values may decrease. <\/p>","5da809dc":"<p> It is necessary to show the relationships between all our data. This illustration is represented by scatter_matrix. The case shown here shows all the relationships due to a matrix-like structure created between each feature. In this way, the relationships between each data are seen clearly. <\/p>","264dee63":"<h1><b>DISEASE DIAGNOSIS SYSTEM WITH SOUND DATA OF PARKINSON PATIENTS<\/b><\/h1>\n\n<p>\nHello there,<\/p>\n\n<p>If you look at diseases around the world, you will find that there are too many chronic diseases. One of the most common causes of these disorders is the damage caused by the deterioration of cells due to the deterioration of the brain structure. Parkinson's disease is one of them. Factors such as tremors in hands, speech disorder, impaired liver system, invitations to this disease. In order to examine Parkinson's disease in more detail, a study on Parkinson's disease sound data set within UCI datasets was started. This data set was collected through speech exercises of people diagnosed with Parkinson's disease. The data set contains approximately 1200 audio data. Some of the algorithms that we use to analyze this data have shown low results. The reason for this situation is that the variation sizes between the different sound samples are very large. To reduce these variations, variations in the data set have been standardized by standardizing the distribution metrics in all of the data sets. However, logistic regression, SVM and KNN, which are popular learning algorithms in recent years, have been used. These algorithms were used for the classification of patients with Parkinson's disease such as disease analysis and whether they were patients. The success of each algorithm varies according to the method used. But the best result is the SVM algorithm. In addition, due to the low number of sound samples taken from patients with Parkinson's disease, we have identified the best three sound samples available for classification. In this way, we started to make our analysis more successful.\n<\/p>","9ac70dd1":"<p>Last Updated: 11.04.2019<\/p>\n<p>Since we will analyze first, we install some libraries that we will use in the system.<\/p>","630d1137":"<p> After all the impressions are finished, we've come to the pre-processing and analysis sections now. Our process data is found here. In this way, if we save our data from this slingshot data, we can operate more effectively. <\/p>","693e271b":"<p>Once all the libraries we use will be installed in the system, we upload our data into the system. For this, we are running the function read_csv in pandas. However, we are loading both our test data and train data as well as our current data.<\/p>","7a962811":"<p>There are 26 properties in total. In addition there are 1 classification feature.<\/p>\n<ul>\n<li>Milk: 1-5 Jitter Values<\/li>\n<li>Milk: 6-11 Shimmer Values<\/li>\n<li>Milk: 12-14 AC, NTH, HTN<\/li>\n<li>Milk: 15-19 median, mean, standard deviation, max, min values<\/li>\n<li>Milk: 20-23 Number of cycles, phase value, frequency value<\/li>\n<li>Milk: 24-26 Cross section of locally cut squares, Number of sound breaks, Sound breaks degree<\/li>\n<\/ul>","138e2f91":"<table border=5>\n<tr>\n<th>\u00d6zellikler<\/th>\n<th>Parametreler<\/th>\n<th>Train Set<\/th>\n<th>Test Set<\/th>\n<\/tr>\n<tr>\n<td>(f1', 'f2', 'f3', 'f4', 'f5', 'f6')<\/td>\n<td>{'C': 0.1, 'penalty': 'l1', 'random_state': 0}<\/td>\n<td>% 81<\/td>\n<td>% 78<\/td>\n<\/tr>\n\n<tr>\n<td>('f6', 'f7', 'f8', 'f9', 'f10', 'f11')<\/td>\n<td>{'C': 0.5, 'penalty': 'l2', 'random_state': 0}<\/td>\n<td>% 80<\/td>\n<td>% 77<\/td>\n<\/tr>\n\n<tr>\n<td>('f9', 'f10', 'f11', 'f12', 'f13', 'f14')<\/td>\n<td>{'C': 0.4, 'penalty': 'l2', 'random_state': 0}<\/td>\n<td>% 64<\/td>\n<td>% 61<\/td>\n<\/tr>\n\n<tr>\n<td>('f15', 'f16', 'f17', 'f18', 'f19')<\/td>\n<td>{'C': 0.1, 'penalty': 'l1', 'random_state': 0}<\/td>\n<td>% 41<\/td>\n<td>% 36<\/td>\n<\/tr>\n\n<\/table>","73ce4923":"<p> The code snippet on the down side performs an action to list the columns in our data set. The reason for this is that we have both train and test data. But these data may be irregular. To eliminate them, we have to list the column names in each of our data sets. <\/p>","fd8ec241":"<p> First, we have 19 properties. By grouping these features we will see which one is the most successful.<\/p>","eee372c3":"<table border=2>\n<tr>\n<th>\u00d6zellikler<\/th>\n<th>Parametreler<\/th>\n<th>Train Set<\/th>\n<th>Test Set<\/th>\n<\/tr>\n<tr>\n<td>(f1', 'f2', 'f3', 'f4', 'f5', 'f6')<\/td>\n<td>{'learning_rate': 0.02, 'n_estimators': 18, 'random_state': 0}<\/td>\n<td>% 83<\/td>\n<td>% 80<\/td>\n<\/tr>\n\n<tr>\n<td>('f6', 'f7', 'f8', 'f9', 'f10', 'f11')<\/td>\n<td>{'learning_rate': 0.02, 'n_estimators': 19, 'random_state': 0}<\/td>\n<td>% 80<\/td>\n<td>% 77<\/td>\n<\/tr>\n\n<tr>\n<td>('f9', 'f10', 'f11', 'f12', 'f13', 'f14')<\/td>\n<td>{'learning_rate': 0.02, 'n_estimators': 19, 'random_state': 0}<\/td>\n<td>% 77<\/td>\n<td>% 56<\/td>\n<\/tr>\n\n<tr>\n<td>('f15', 'f16', 'f17', 'f18', 'f19')<\/td>\n<td>{'learning_rate': 0.02, 'n_estimators': 14, 'random_state': 0}<\/td>\n<td>% 41<\/td>\n<td>% 36<\/td>\n<\/tr>\n\n<\/table>","689d303d":"<p> The code snippet on the down side performs an action to list the columns in our data set. The reason for this is that we have both train and test data. But these data may be irregular. To eliminate them, we have to list the column names in each of our data sets. <\/p>","3ed0b219":"<p> In this section, you can see the operation between the data in a certain period such as max, min, std. The name of this function is also <\/p>","1dc13680":"<p> Our data is a situation that summarizes the numerical relationships between each other. This corr () function reveals the correlation values between the data. As can be seen, the first five relationships are almost 1.0000 between each other. This is good for now. But if we do this for all features, this is not good. <\/p>","8632c726":"<table border=2>\n<tr>\n<th>\u00d6zellikler<\/th>\n<th>Parametreler<\/th>\n<th>Train Set<\/th>\n<th>Test Set<\/th>\n<\/tr>\n<tr>\n<td>(f1', 'f2', 'f3', 'f4', 'f5', 'f6')<\/td>\n<td>{'n_jobs': 2, 'n_neighbors': 22}<\/td>\n<td>% 88<\/td>\n<td>% 87<\/td>\n<\/tr>\n\n<tr>\n<td>('f6', 'f7', 'f8', 'f9', 'f10', 'f11')<\/td>\n<td>{'n_jobs': 2, 'n_neighbors': 12}<\/td>\n<td>% 88<\/td>\n<td>% 87<\/td>\n<\/tr>\n\n<tr>\n<td>('f9', 'f10', 'f11', 'f12', 'f13', 'f14')<\/td>\n<td>{'n_jobs': 2, 'n_neighbors': 12}<\/td>\n<td>% 69<\/td>\n<td>% 55<\/td>\n<\/tr>\n\n<tr>\n<td>('f15', 'f16', 'f17', 'f18', 'f19')<\/td>\n<td>{'n_jobs': 2, 'n_neighbors': 3}<\/td>\n<td>% 80<\/td>\n<td>% 62<\/td>\n<\/tr>\n\n<\/table>","d6ff9454":"<p> As a result of the direction from the Scatter matrix, inter-thread operations of each property are filled. For the first 6 properties, however, the same cannot be said for this case, listing them for each case and showing them around the chart. <\/p>","14f834be":"<h1><b>Model Training ve Testing<\/b><\/h1>\n\n<p> As a result of our initial evaluations, we have used a number of artificial learning algorithms. These are logistic regression, support vector machine (SVM), k close neighborhood (kNN), GradientBoostingClassifier and RandomForestClassifier algorithms. The first algorithm is logistic regression algorithm. To implement this algorithm model, we need to separate dependent and independent variables within our data sets. In addition, we created a combination of features between different features to make different experiments. While creating these parameters, the process of finding the best results was made by giving hyper parameter values. <\/p>","9474e8e6":"<P> data = (data-np.m the (data)) \/ ((np.max (data) -np.m the (data))) <\/p>\n<p> Above is the process normalization process. In other words, it is the process of pulling the huge differences between the properties to a number between 0-1. <\/p>","373b778d":"<table border=2>\n<tr>\n<th>\u00d6zellikler<\/th>\n<th>Parametreler<\/th>\n<th>Train Set<\/th>\n<th>Test Set<\/th>\n<\/tr>\n<tr>\n<td>(f1', 'f2', 'f3', 'f4', 'f5', 'f6')<\/td>\n<td>{'max_depth': 7, 'min_samples_split': 4, 'n_estimators': 19, 'random_state': 3}<\/td>\n<td>% 98<\/td>\n<td>% 87<\/td>\n<\/tr>\n\n<tr>\n<td>('f6', 'f7', 'f8', 'f9', 'f10', 'f11')<\/td>\n<td>{'max_depth': 8, 'min_samples_split': 2, 'n_estimators': 18, 'random_state': 3}<\/td>\n<td>% 97<\/td>\n<td>% 85<\/td>\n<\/tr>\n\n<tr>\n<td>('f9', 'f10', 'f11', 'f12', 'f13', 'f14')<\/td>\n<td>{'max_depth': 8, 'min_samples_split': 4, 'n_estimators': 12, 'random_state': 3}<\/td>\n<td>% 89<\/td>\n<td>% 57<\/td>\n<\/tr>\n\n<tr>\n<td>('f15', 'f16', 'f17', 'f18', 'f19')<\/td>\n<td>{'max_depth': 4, 'min_samples_split': 2, 'n_estimators': 11, 'random_state': 3}<\/td>\n<td>% 76<\/td>\n<td>% 46<\/td>\n<\/tr>\n\n<\/table>","5e0dddda":"<p> A different set of data is available through all operations. In this data set, there are voices of different people. These sounds were analyzed and processed. Finally, to install these files into the system as a different csv file is required to install. <\/p>"}}