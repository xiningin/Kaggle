{"cell_type":{"9aa0cbbf":"code","b634a2cf":"code","960bd5b6":"code","a34696cc":"code","339e859c":"code","51c3f40a":"code","067a6780":"code","f373c7d0":"code","9246a26a":"code","1afc1e09":"code","1f6a0b26":"code","2e12ce1f":"code","fd332a53":"code","5a4db427":"code","67d7b457":"code","58ab0e73":"code","59ba7732":"code","77bc1c2d":"code","c4facad3":"code","520e4db9":"code","09e48706":"code","57d55a94":"code","04039fd7":"code","7a07e360":"code","4f532442":"code","7479567f":"code","c8c41f48":"code","cf5858d8":"code","f637da2e":"code","600ab0c2":"code","6a61e45f":"code","f567efa2":"code","124a33ea":"code","1acb3524":"code","05c5fec0":"code","5d599bd4":"code","bb47d2c3":"code","55329653":"code","9648b950":"code","9334ab7b":"code","d69620ad":"code","0338090c":"code","1322c964":"code","f5bc7c3c":"code","f6ca196c":"code","e70d9e36":"code","44566253":"code","59c8e6a0":"code","ff763f37":"code","5bb9ae0e":"code","79ba9ea4":"code","e63b54ac":"code","3db8750c":"code","62912b84":"code","9b1e77f6":"code","3a17939a":"code","fd009143":"code","29112226":"code","39430c78":"markdown","4666e986":"markdown","105a9857":"markdown","e5f6798c":"markdown","d1607ba0":"markdown","3a550b1b":"markdown","1ac0126e":"markdown","3963d572":"markdown","66093cf3":"markdown","7bac2f16":"markdown","af3ec9e2":"markdown","d812a16d":"markdown","908bb11d":"markdown","69b6aa55":"markdown","dd4b87dc":"markdown","87fbd791":"markdown","7fee5af8":"markdown","03c95aae":"markdown"},"source":{"9aa0cbbf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b634a2cf":"# from google.colab import drive\n# drive.mount('\/content\/drive')","960bd5b6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","a34696cc":"df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\n\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nsubmission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","339e859c":"df.head()","51c3f40a":"print('Shape:', df.shape)","067a6780":"print('value counts of target\\n')\ndf.target.value_counts()","f373c7d0":"sns.countplot(df.target)\nplt.show()","9246a26a":"df.info()","1afc1e09":"col = df.columns\n\nfor i in range(len(col)-1):\n    if df[col[i]].isnull().sum() != 0:\n        c = df[col[i]].isnull().sum()\n        print('\\n{} column has {} null values'.format(col[i], c))","1f6a0b26":"print('\\nPercentage of null values in keyword:', round((df.keyword.isnull().sum() \/ df.shape[0]) * 100, 3))\nprint('\\nPercentage of null values in location:', round((df.location.isnull().sum() \/ df.shape[0]) * 100, 3))","2e12ce1f":"print('Number of unique values in keyword:', df.keyword.nunique(), '\\n')\nprint('Number of unique values in location:', df.location.nunique(), '\\n')","fd332a53":"print('\\n-> Preprocessed text data:\\n')\n\nfor i in range(0, df.shape[0], 300):\n    print('\\n{}'.format(i)+':', df['text'][i])\n","5a4db427":"# Importing libraries\n\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom tqdm import tqdm\n\n# Create an instance for SnowballStemmer\nss = SnowballStemmer('english')","67d7b457":"# Defining a function to convert short words like couldn't to full word could not\ndef short_form(full_form):\n    \n    full_form = full_form.lower()      \n    \n    full_form = re.sub(r\"won't\", \"will not\", full_form)\n    full_form = re.sub(r\"wouldn't\", \"would not\", full_form)\n    full_form = re.sub(r\"can't\", \"can not\", full_form)\n    full_form = re.sub(r\"don't\", \"don not\", full_form)\n    full_form = re.sub(r\"shouldn't\", \"should not\", full_form)\n    full_form = re.sub(r\"couldn't\", \"could not\", full_form)\n    full_form = re.sub(r\"\\'re\", \" are\", full_form)\n    full_form = re.sub(r\"\\'s\", \" is\", full_form)\n    full_form = re.sub(r\"\\'d\", \" would\", full_form)\n    full_form = re.sub(r\"\\'ll\", \" will\", full_form)\n    full_form = re.sub(r\"\\'ve\", \" have\", full_form)\n    full_form = re.sub(r\"\\'m\", \" am\", full_form)\n  \n    return full_form\n\n# To remove URL\ndef url(ur):\n    ur = re.sub(r\"http\\S+\", '', ur)\n    return ur\n\n# Defining a function to remove punctuations, numbers, stopwords and get stem of words\ndef punc(pun):\n    pun = re.sub('[^a-zA-Z]', ' ', pun)\n    pun = pun.lower()\n    pun = pun.split()\n    pun = [ss.stem(sw) for sw in pun if sw not in stopwords.words('english')]\n    pun = ' '.join(pun)\n    return pun","58ab0e73":"import nltk\nnltk.download(\"stopwords\")","59ba7732":"test.head()","77bc1c2d":"col = test.columns\n\nfor i in range(len(col)-1):\n    if test[col[i]].isnull().sum() != 0:\n        c = test[col[i]].isnull().sum()\n        print('\\n{} column has {} null values'.format(col[i], c))","c4facad3":"print('\\nPercentage of null values in keyword:', round((test.keyword.isnull().sum() \/ test.shape[0]) * 100, 3))\nprint('\\nPercentage of null values in location:', round((test.location.isnull().sum() \/ test.shape[0]) * 100, 3))","520e4db9":"print('Number of unique values in keyword:', test.keyword.nunique(), '\\n')\nprint('Number of unique values in location:', test.location.nunique(), '\\n')","09e48706":"import copy","57d55a94":"dfe = copy.deepcopy(df)","04039fd7":"# Filling na with previous data point\n# Before that we will fill null values with string 'null' for better detection\n\ndfe['keyword'].fillna('nul', inplace = True)\n\nfor i in range(32):\n    if dfe['keyword'][i] != 'nul':\n        dfe['keyword'][0] = dfe['keyword'][i]","7a07e360":"for i in range(dfe.shape[0]):\n    if dfe['keyword'][i] == 'nul':\n        dfe['keyword'][i] = dfe['keyword'][i-1]","4f532442":"# Location column\n\n# Filling na with previous data point\n# Before that we will fill null values with string 'null' for better detection\n\ndfe['location'].fillna('nul', inplace = True)\n\nfor i in range(32):\n    if dfe['location'][i] != 'nul':\n        dfe['location'][0] = dfe['location'][i]","7479567f":"for i in range(dfe.shape[0]):\n    if dfe['location'][i] == 'nul':\n        dfe['location'][i] = dfe['location'][i-1]","c8c41f48":"from tqdm import tqdm\n\nloc_train_clean = []\n\nfor i, s in enumerate(tqdm(dfe['location'].values)):\n    \n    u = url(s)\n    sf = short_form(u)\n    pu = punc(sf)\n    loc_train_clean.append(pu)","cf5858d8":"from tqdm import tqdm\n\ntext_train_clean = []\n\nfor i, s in enumerate(tqdm(dfe['text'].values)):\n    \n    u = url(s)\n    sf = short_form(u)\n    pu = punc(sf)\n    text_train_clean.append(pu)","f637da2e":"print('\\n-> Preprocessed text data:\\n')\n\ndfe['location'] = loc_train_clean\n\ndfe['text'] = text_train_clean","600ab0c2":"dfc = copy.deepcopy(dfe)\n\n# dfc.to_csv('train_clean.csv', index = False)","6a61e45f":"teste = copy.deepcopy(test)","f567efa2":"# Filling na with previous data point\n# Before that we will fill null values with string 'null' for better detection\n\nteste['keyword'].fillna('nul', inplace = True)\n\nfor i in range(20):\n    if teste['keyword'][i] != 'nul':\n        teste['keyword'][0] = teste['keyword'][i]","124a33ea":"for i in range(teste.shape[0]):\n    if teste['keyword'][i] == 'nul':\n        teste['keyword'][i] = teste['keyword'][i-1]","1acb3524":"# Location column\n\n# Filling na with previous data point\n# Before that we will fill null values with string 'null' for better detection\n\nteste['location'].fillna('nul', inplace = True)\n\nfor i in range(32):\n    if teste['location'][i] != 'nul':\n        teste['location'][0] = teste['location'][i]","05c5fec0":"for i in range(test.shape[0]):\n    if teste['location'][i] == 'nul':\n        teste['location'][i] = teste['location'][i-1]","5d599bd4":"\n\nfrom tqdm import tqdm\n\nloc_test_clean = []\n\nfor i, s in enumerate(tqdm(teste['location'].values)):\n    \n    u = url(s)\n    sf = short_form(u)\n    pu = punc(sf)\n    loc_test_clean.append(pu)","bb47d2c3":"from tqdm import tqdm\n\ntext_test_clean = []\n\nfor i, s in enumerate(tqdm(teste['text'].values)):\n    \n    u = url(s)\n    sf = short_form(u)\n    pu = punc(sf)\n    text_test_clean.append(pu)","55329653":"print('\\n-> Preprocessed text data:\\n')\n\nteste['location'] = loc_test_clean\n\nteste['text'] = text_test_clean","9648b950":"testc = copy.deepcopy(teste)\n\n# testc.to_csv('test_clean.csv', index = False)","9334ab7b":"dfe['key_loc_text'] = dfe['keyword'] + dfe['location'] + dfe['text']\n\nteste['key_loc_text'] = teste['keyword'] + teste['location'] + teste['text']","d69620ad":"from sklearn.preprocessing import LabelEncoder\n\nlb = LabelEncoder()\n\ndfe['keyword'] = lb.fit_transform(dfe['keyword'])\n\nteste['keyword'] = lb.transform(teste['keyword'])","0338090c":"# Import CountVectorizer library\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Create an instance\n# Bi-gram\ncv_klt = CountVectorizer(ngram_range = (1, 2))\n\n# Fit and transform train data\ntr_klt_b = cv_klt.fit_transform(dfe['key_loc_text'])\n\n# Transform test data\nte_klt_b = cv_klt.transform(teste['key_loc_text'])","1322c964":"# Import normalize library\nfrom sklearn.preprocessing import normalize\n\n# Normalize train data\ntr_klt_n = normalize(tr_klt_b)\n\n# Normalize test data\nte_klt_n = normalize(te_klt_b)","f5bc7c3c":"# Import CountVectorizer library\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Create an instance\n# Bi-gram\ncv_l = CountVectorizer(ngram_range = (1, 2))\n\n# Fit and transform train data\ntr_l_b = cv_l.fit_transform(dfe['location'])\n\n# Transform test data\nte_l_b = cv_l.transform(teste['location'])","f6ca196c":"# Import normalize library\nfrom sklearn.preprocessing import normalize\n\n# Normalize train data\ntr_l_n = normalize(tr_l_b)\n\n# Normalize test data\nte_l_n = normalize(te_l_b)","e70d9e36":"# Import CountVectorizer library\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Create an instance\n# Bi-gram\ncv_t = CountVectorizer(ngram_range = (1, 2))\n\n# Fit and transform train data\ntr_t_b = cv_t.fit_transform(dfe['text'])\n\n# Transform test data\nte_t_b = cv_t.transform(teste['text'])","44566253":"# Import normalize library\nfrom sklearn.preprocessing import normalize\n\n# Normalize train data\ntr_t_n = normalize(tr_t_b)\n\n# Normalize test data\nte_t_n = normalize(te_t_b)","59c8e6a0":"tr_key = dfe['keyword'].values.reshape(-1, 1)\n\nte_key = teste['keyword'].values.reshape(-1, 1)","ff763f37":"from scipy.sparse import hstack\n\nx_tr_e = hstack((tr_key, tr_klt_n, tr_l_n, tr_t_n))\n\nx_te_e = hstack((te_key, te_klt_n, te_l_n, te_t_n))","5bb9ae0e":"y = dfe['target']","79ba9ea4":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\n\nnbe = MultinomialNB()\n\nparam = {'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 10, 100, 1000, 10000]}\n\n\nclf_nbe = GridSearchCV(estimator = nbe, param_grid = param, scoring = 'f1', cv = 4)\n\nclf_nbe.fit(x_tr_e, y)","e63b54ac":"print('\\n-> Best score:', clf_nbe.best_score_, '\\n')\nprint('*'*50, '\\n')\n\nprint('\\n-> Best estimators:', clf_nbe.best_estimator_)","3db8750c":"nbe_pre = clf_nbe.predict(x_te_e)\n\nsub_nb_e = copy.deepcopy(submission)","62912b84":"sub_nb_e['target'] = nbe_pre\n\n# sub_nb_e.to_csv('sub_nbe_2.csv', index = False)","9b1e77f6":"import pandas as pd","3a17939a":"# sub_nb_e = pd.read_csv('\/content\/drive\/My Drive\/Tweets Disaster or not\/nlp-getting-started\/sub_nbe_2.csv')","fd009143":"print(\"\\nShape of test predicted data:\", sub_nb_e.shape, '\\n')","29112226":"print(\"\\nHead of 10 of test predicted data:\\n\")\n\nsub_nb_e.head(10)","39430c78":"# BoW on text column data","4666e986":"# Working on Train data","105a9857":"# Naive Bayes","e5f6798c":"# Data set is well balanced","d1607ba0":"# Reshaping so as to hstack","3a550b1b":"<center><h1>~ Tweets Disaster or Not? ~<h1>","1ac0126e":"# Label encoding","3963d572":"# Feature Engineering","66093cf3":"# BoW on location column data","7bac2f16":"# Feature scaling","af3ec9e2":"# Feature Scaling","d812a16d":"# Working on Test data","908bb11d":"# Feature scaling","69b6aa55":"# BoW on feature engineered column data","dd4b87dc":"# Test data","87fbd791":"# Combining keyword, location and text columns","7fee5af8":"# Well balanced or not?","03c95aae":"<center><h1>~ Finish ~<h1>"}}