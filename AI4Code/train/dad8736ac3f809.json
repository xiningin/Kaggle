{"cell_type":{"db3dee05":"code","b014dea9":"code","0db5e568":"code","68d16601":"code","93631373":"code","941932af":"code","aa259868":"code","6dd60feb":"code","a79e42a8":"code","6987f717":"code","a8f57cff":"markdown","ea47c6a1":"markdown","8b9d208c":"markdown","ffd45276":"markdown","d202458a":"markdown","2f285042":"markdown","3b925392":"markdown","d491b51d":"markdown","b11aee27":"markdown","04083522":"markdown","616305ad":"markdown","ab972f4a":"markdown","7ae4236e":"markdown","8a97e910":"markdown","a6dc7bb5":"markdown"},"source":{"db3dee05":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\nfrom sklearn.metrics import confusion_matrix,classification_report","b014dea9":"df = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\ndf.head()","0db5e568":"print(df['Attrition_Flag'].value_counts())\nprint(df['Gender'].value_counts())\nprint(df['Education_Level'].value_counts())\nprint(df['Marital_Status'].value_counts())\nprint(df['Income_Category'].value_counts())\nprint(df['Card_Category'].value_counts())","68d16601":"df['Attrition_Flag'].replace({'Existing Customer' : 0, 'Attrited Customer' : 1},inplace = True)\ndf['Gender'].replace({'F': 0, 'M': 1}, inplace = True)\ndf['Education_Level'].replace({'Unknown' : 0, 'Uneducated' : 1, 'High School' : 2, 'College' : 3, \n                               'Graduate' : 4, 'Post-Graduate' : 5, 'Doctorate' : 6}, inplace = True)\ndf['Marital_Status'].replace({'Unknown' : 0, 'Single' : 1, 'Divorced' : 2, 'Married' : 3}, inplace = True)\ndf['Income_Category'].replace({'Unknown' : 0, 'Less than $40K' : 1, '$40K - $60K' : 2, '$60K - $80K' : 3,\n                              '$80K - $120K' : 4, '$120K +' : 5}, inplace = True)\ndf['Card_Category'].replace({'Blue' : 0, 'Silver' : 1, 'Gold' : 2, 'Platinum' : 3}, inplace = True)\ndf.drop(df.columns[[0,21,22]].values,axis=1,inplace = True)\ndf.dtypes","93631373":"df.head()","941932af":"df.corr()","aa259868":"dfm = df[['Attrition_Flag', 'Total_Relationship_Count', 'Months_Inactive_12_mon',\n          'Contacts_Count_12_mon', 'Total_Revolving_Bal', 'Total_Trans_Amt', 'Total_Trans_Ct',\n          'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']]\ndfm.corr()","6dd60feb":"x = dfm[['Total_Relationship_Count', 'Months_Inactive_12_mon', 'Contacts_Count_12_mon',\n        'Total_Revolving_Bal', 'Total_Trans_Amt', 'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1',\n        'Avg_Utilization_Ratio']]\ny = dfm['Attrition_Flag']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=2)","a79e42a8":"model = RandomForestClassifier(n_estimators=100, max_depth=13, random_state=2)\nmodel.fit(x, y)\nrfvalue = model.predict(x_test)\n\nprint('Model Accuracy : ', accuracy_score(y_test, rfvalue) *  100)\nprint('Model Recall : ', recall_score(y_test, rfvalue) *  100)\nprint('Model Precision : ', precision_score(y_test, rfvalue) *  100)","6987f717":"print(confusion_matrix(y_test, rfvalue))\nprint(classification_report(y_test, rfvalue))","a8f57cff":"### Import libraries and data","ea47c6a1":"With this model, we still have roughly two dozen false negatives - what we've been asked to avoid. That being said, we're still looking at pretty high accuracy, precision, recall, and f1 scores.","8b9d208c":"### Correlation","ffd45276":"# Credit Card Churn","d202458a":"## Conclusion","2f285042":"Let's start by seeing what features correlate most with Attrition, and which correlate with one another, in order to have an idea of what features may be more useful than others in an attempt to avoid data overload and overfitting.","3b925392":"By Eric Wilson","d491b51d":"### Addendum","b11aee27":"I value feedback, tips, and criticism highly - I'm still fairly new to DS and ML, so if I make a mistake or error, I would greatly appreciate knowing so; the best way to learn is by doing, and it's better to fix an error before it becomes a habit.\n\nThank you for taking the time to read this notebook!","04083522":"By narrowing down the data used to the factors which have the highest correlation to attrition, we're left with a pretty accurate model. I've tried to optimize it with larger and smaller train \/ test splits and random states, but the combination used in this notebook seemed to be pretty optimal.","616305ad":"### Model Building","ab972f4a":"The individual on Kaggle who submitted this data set said they need to predict customer churn, and have managed to get 62% as the highest accuracy. It's ok to predict someone who will stay as one who will churn, but the most important task is making sure everyone who will churn is not marked as someone who will stay. Let's see what we can do...","7ae4236e":"Now we have nothing but numbers. Let's start trying to build a model.","8a97e910":"First, we need to turn attrition\/churn into numeric values, followed by gender, education, income, marital status, and card catagory. We also need to get rid of the last two columns.","a6dc7bb5":"It appears that the most correlated fields to churn are transaction counts, count change from Q4 to Q1, revolving balance, 12 month contact count, inactive months, utilization ratio,relationship count, and transaction amount. That being said, none of them share a particularly strong correlation, but essentially all of the demographic information (gender, income, education level, marital status, dependant count) lack any real correlation with churn."}}