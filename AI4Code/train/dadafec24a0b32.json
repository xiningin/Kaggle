{"cell_type":{"c8a547bd":"code","b08a4245":"code","d8ce8a56":"code","d231cc78":"code","128c1dc9":"code","73634a65":"code","dbb86424":"code","76fe1b3e":"code","dfdcf359":"code","153fd2bb":"code","81d2f10a":"code","815a0650":"code","5da758ea":"code","f92d80e9":"code","69f10f5c":"code","d60e9b1f":"code","67983468":"code","eca0a756":"code","a4870eec":"code","61bb1df7":"code","696b6000":"code","27b2a145":"code","414ac9bc":"code","de819339":"code","9e0b21b6":"code","70b7b154":"code","4d836ede":"code","8f55d0e1":"code","708c02b9":"code","10aa183f":"code","cbd4fb6a":"code","823308ca":"code","743dbfc8":"code","38afb6bb":"code","f9305b2a":"code","f41252ab":"code","cd45a865":"code","a6364b70":"code","b4457eaa":"code","28ba25d4":"code","f00195f2":"code","d66d5083":"code","8bf020d2":"code","372917ed":"code","c6973a2b":"code","c7c213ee":"code","1a4ff1ef":"code","cd5c0880":"code","867138f2":"markdown","2f0c150a":"markdown","b1f44625":"markdown","ab2d6485":"markdown","c6d93110":"markdown","8d26ebd1":"markdown","cec3a2d5":"markdown","98a001d2":"markdown","9c96191d":"markdown","bf0987b8":"markdown","986c3960":"markdown","1ec054b2":"markdown","f9566e0e":"markdown","3177882d":"markdown","bab10a92":"markdown","97e288a3":"markdown","685ebb0c":"markdown","e8dd015b":"markdown","f32c5099":"markdown","858f4b74":"markdown","eb2202a3":"markdown"},"source":{"c8a547bd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","b08a4245":"#loading the data\ntrain_set = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_set = pd.read_csv(\"..\/input\/titanic\/test.csv\")","d8ce8a56":"train_set.head(890)","d231cc78":"plt.figure(figsize=(5, 5), dpi=150)\nsns.scatterplot(data=train_set, x='Age', y='Fare', hue='Survived')\nplt.tight_layout()\nplt.show()","128c1dc9":"#Remove the outliers:\nindex_drop=train_set[train_set['Fare']>450].index\ntrain_set=train_set.drop(index_drop, axis=0)","73634a65":"\nsns.scatterplot(x='Age', y='Fare', data=train_set)\n#plt.axhline(y=450, color='r')\n#plt.axvline(x=28, color='r')","dbb86424":"quantitative_col = [ 'Pclass', 'Age','Fare','SibSp','Parch']","76fe1b3e":"survived_people = train_set[train_set.Survived == 1]\nunsurvived_people = train_set[train_set.Survived == 0]","dfdcf359":"for col in quantitative_col :\n    plt.figure()\n    sns.histplot(survived_people[col], label='survived',kde=True,color=\"gold\", stat=\"density\", linewidth=0\n)\n    sns.histplot(unsurvived_people[col], label='unsurvived',kde=True, stat=\"density\", linewidth=0\n)\n    plt.legend()","153fd2bb":"# get correlations of each features in dataset\n# Plotting Heat Map to visualise correlation data better. \n# Drwan for only features having high correlation \n# (>0.0) with Target Variable\ncorr = train_set.corr()\ntop_corr_features = corr.index[abs(corr[\"Survived\"])>0.0]\n\nplt.figure(figsize=(10,10))\n#plot heat map\ng=sns.heatmap(train_set[top_corr_features].corr(),annot=True,cmap=\"YlGnBu\")","81d2f10a":"train_set= train_set.drop(['PassengerId'],axis=1)","815a0650":"train_set= train_set.drop(['Name'],axis=1)","5da758ea":"train_set= train_set.drop(['Ticket'],axis=1)","f92d80e9":"def missing_percent(train_set):\n    nan_percent = 100*(train_set.isnull().sum()\/len(train_set))\n    nan_percent = nan_percent[nan_percent>0].sort_values(ascending=False).round(1)\n    DataFrame = pd.DataFrame(nan_percent)\n    # Rename the columns\n    mis_percent_table = DataFrame.rename(columns = {0 : '% of Misiing Values'}) \n    # Sort the table by percentage of missing descending\n    mis_percent = mis_percent_table\n    return mis_percent","69f10f5c":"train_set.isnull().sum()","d60e9b1f":"miss = missing_percent(train_set)\nmiss","67983468":"train_set['Age'] = train_set['Age'].fillna(train_set.groupby('Sex')['Age'].transform('mean'))","eca0a756":"#check Embarked missing value\ntrain_set[train_set['Embarked'].isnull()]","a4870eec":"# we fill the missing value with S = southampton because this city has the most population amongst others it is not accurate but we try our luck \ntrain_set[\"Embarked\"]= train_set[\"Embarked\"].fillna(\"S\")","61bb1df7":"train_set= train_set.drop(['Cabin'],axis=1)","696b6000":"miss = missing_percent(train_set)\nmiss","27b2a145":"train_set['Sex'] = train_set['Sex'].replace(['female'],'1')\ntrain_set['Sex'] = train_set['Sex'].replace(['male'],'0')","414ac9bc":"train_set['Embarked'] = train_set['Embarked'].replace(['S'],'0')\ntrain_set['Embarked'] = train_set['Embarked'].replace(['Q'],'1')\ntrain_set['Embarked'] = train_set['Embarked'].replace(['C'],'2')","de819339":"X= train_set.drop('Survived', axis=1)\ny= train_set['Survived']","9e0b21b6":"from sklearn.model_selection import train_test_split","70b7b154":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)","4d836ede":"from sklearn.preprocessing import StandardScaler","8f55d0e1":"scaler= StandardScaler()","708c02b9":"scaler.fit(X_train)","10aa183f":"scaled_X_train= scaler.transform(X_train)\nscaled_X_test= scaler.transform(X_test)","cbd4fb6a":"# now we train our model\nfrom sklearn.linear_model import LogisticRegression\nlog_model= LogisticRegression()","823308ca":"log_model.fit(scaled_X_train, y_train)","743dbfc8":"y_pred= log_model.predict(scaled_X_test)","38afb6bb":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix","f9305b2a":"accuracy_score(y_test, y_pred)","f41252ab":"plot_confusion_matrix(log_model, scaled_X_test, y_test)","cd45a865":"#we already split the data and we scaled it so we just need to train the model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_model= KNeighborsClassifier(n_neighbors=1)","a6364b70":"knn_model.fit(scaled_X_train, y_train)","b4457eaa":"y_pred= knn_model.predict(scaled_X_test)","28ba25d4":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\naccuracy_score(y_test, y_pred)","f00195f2":"print(classification_report(y_test, y_pred))","d66d5083":"test_error_rate= []\n\n\nfor k in range (1, 30):\n    knn_model = KNeighborsClassifier(n_neighbors=k)\n    knn_model.fit(scaled_X_train, y_train)\n    \n    y_pred_test = knn_model.predict(scaled_X_test)\n    \n    test_error=1- accuracy_score(y_test, y_pred_test)\n    test_error_rate.append(test_error)","8bf020d2":"plt.figure(figsize=(10, 6))\nplt.plot(range(1, 30), test_error_rate, label='Test Error')\nplt.legend()\nplt.ylabel('Error Rate')\nplt.xlabel('K Value')","372917ed":"from sklearn.neighbors import KNeighborsClassifier\nknn_model= KNeighborsClassifier(n_neighbors=15)","c6973a2b":"knn_model.fit(scaled_X_train, y_train)","c7c213ee":"y_pred= knn_model.predict(scaled_X_test)","1a4ff1ef":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\naccuracy_score(y_test, y_pred)","cd5c0880":"print(classification_report(y_test, y_pred))","867138f2":"* passengerId has negetive correlaton so we drop that because Id is an unnecesery feature\n* we also drop more unnececeary fetures like Name and Ticket","2f0c150a":"* Missing Values","b1f44625":"we train the model with 15 neighbors","ab2d6485":"* Handling catagorical values","c6d93110":"# accuracy is 0.84 with 15 n neighbors, the performance improved","8d26ebd1":"* fill NAN valueS in the Age columns with the mean value according to the Gender","cec3a2d5":"# we get 0.82 accuracies with simple Logistic Regression","98a001d2":"# <div id=\"intLink3\"> K-nearest neighbors <\/div>","9c96191d":"* Survived - that's the target, 0 means the passenger did not survive, while 1 means he\/she survived.\n* Pclass - passenger class.\n* Name -**-----> Drop (they don't push you in the ocean acording to your ticketId, I hope ;)**\n* Sex\n* Age **-----> filling the missing values acording to mean of each gender**\n* SibSp - how many siblings & spouses of the passenger aboard the Titanic.\n* Parch - how many children & parents of the passenger aboard the Titanic.\n* Ticket - ticket id **-----> Drop**\n* Fare - the price paid (in pounds)\n* Cabin - passenger's cabin number **-----> Drop (just drop the Cabin, no one was at their Cabins they all were at the ballroom, LOL) too many missing values**\n* Embarked - where the passenger embarked the Titanic","bf0987b8":"* first we should scale our Features","986c3960":"* as you can see we have Outliner: pepole who paid more that 450$ survived !!! so we drop the them","1ec054b2":"# <div id=\"intLink2\"> Logistic Regression <\/div>\n","f9566e0e":"# <div id=\"intLink0\">EDA<\/div>","3177882d":"# <div id=\"intLink4\"> Elbow Method for Choosing Reasonable K Values <\/div>","bab10a92":" # <div id=\"intLink1\"> Feature Engineering \n * Handling missing values and useless features\n * Handling catagorical values and replacing them with Numbers <\/div>","97e288a3":"# **Titanic - ML (Logistic Regression vs KNN):**\n* **[EDA](#intLink0)**\n* **[Feature Engineering](#intLink1)** \n* **[Logistic Reggresion](#intLink2)**\n* **[k-nearest neighbors(KNN)](#intLink3)**\n* **[Elbow Method for Choosing Reasonable K Values)](#intLink4)**","685ebb0c":"# we get 0.76 accuracy using KNN with **1** neighbor ","e8dd015b":"acording to the plot we choose K=15 cause it has a significantly lower Error rate","f32c5099":"No missing value...","858f4b74":"* True Positive: 45\n* True Negetive: 11\n* False Positive: 5\n* False Negetive: 28\n","eb2202a3":"I am going to update the Notebook"}}