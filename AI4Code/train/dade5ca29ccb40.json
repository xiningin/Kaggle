{"cell_type":{"0cde3aa5":"code","f7d86044":"code","f0960039":"code","e35fcb93":"code","5d695b2b":"code","dd00d744":"code","bf8fbc93":"code","54d7c25e":"code","d1546ff7":"code","1a6e7612":"code","4c0a11f7":"code","3d1608bb":"code","af6aea09":"code","9115f230":"code","de18a19c":"code","4d1179c6":"code","d433e999":"code","394eb96d":"code","f3f110c9":"code","19324ccf":"code","dddc67df":"code","e13a2155":"code","9f2d6731":"code","58a2a943":"code","83d262d2":"markdown","2d3be5f9":"markdown","1388fd14":"markdown","d3dd97ac":"markdown","b941c4cf":"markdown","6c0c577c":"markdown","44ca2b13":"markdown"},"source":{"0cde3aa5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f7d86044":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\n# Any results you write to the current directory are saved as output.","f0960039":"# load data set\nx1 = np.load('..\/input\/linear-regression-example-dataset\/X.npy')\nY1 = np.load('..\/input\/linear-regression-example-dataset\/Y.npy')\nimg_size = 64\nplt.subplot(1, 2, 1)\nplt.imshow(x1[260].reshape(img_size, img_size))\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(x1[900].reshape(img_size, img_size))\nplt.axis('off')\n","e35fcb93":"# Join a sequence of arrays along an row axis.\nX = np.concatenate((x1[204:409], x1[822:1027] ), axis=0) # from 0 to 204 is zero sign and from 205 to 410 is one sign \nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \" , X.shape)\nprint(\"Y shape: \" , Y.shape)","5d695b2b":"# Then lets create x_train, y_train, x_test, y_test arrays\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42) #Try to use same raddom_state vallue.\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]\n","dd00d744":"X_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test .reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\nprint(\"X train flatten\",X_train_flatten.shape)\nprint(\"X test flatten\",X_test_flatten.shape)","bf8fbc93":"x_train = X_train_flatten.T\nx_test = X_test_flatten.T\ny_train = Y_train.T\ny_test = Y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","54d7c25e":"# short description and example of definition (def)\ndef dummy(parameter):\n    dummy_parameter = parameter + 5\n    return dummy_parameter\nresult = dummy(3)     # result = 8\n\n# lets initialize parameters\n# So what we need is dimension 4096 that is number of pixels as a parameter for our initialize method(def)\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b","d1546ff7":"# calculation of z\n#z = np.dot(w.T,x_train)+b\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head\n#y_head = sigmoid(z)","1a6e7612":"# Forward propagation steps:\n# find z = w.T*x+b\n# y_head = sigmoid(z)\n# loss(error) = loss(y,y_head)\n# cost = sum(loss)\ndef forward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    return cost ","4c0a11f7":"# In backward propagation we will use y_head that found in forward progation\n# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","3d1608bb":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n#parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.009,number_of_iterarion = 200)","af6aea09":" # prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","9115f230":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 4096\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)","de18a19c":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\nprint(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","4d1179c6":"# intialize parameters and layer sizes\ndef initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1,\n                  \"bias1\": np.zeros((3,1)),\n                  \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,\n                  \"bias2\": np.zeros((y_train.shape[0],1))}\n    return parameters","d433e999":"\ndef forward_propagation_NN(x_train, parameters):\n\n    Z1 = np.dot(parameters[\"weight1\"],x_train) +parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","394eb96d":"# Compute cost\ndef compute_cost_NN(A2, Y, parameters):\n    logprobs = np.multiply(np.log(A2),Y)\n    cost = -np.sum(logprobs)\/Y.shape[1]\n    return cost","f3f110c9":"# Backward Propagation\ndef backward_propagation_NN(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","19324ccf":"# update parameters\ndef update_parameters_NN(parameters, grads, learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters","dddc67df":"# prediction\ndef predict_NN(parameters,x_test):\n    # x_test is a input for forward propagation\n    A2, cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","e13a2155":"# 2 - Layer neural network\ndef two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n    cost_list = []\n    index_list = []\n    #initialize parameters and layer sizes\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n\n    for i in range(0, num_iterations):\n         # forward propagation\n        A2, cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost = compute_cost_NN(A2, y_train, parameters)\n         # backward propagation\n        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n         # update parameters\n        parameters = update_parameters_NN(parameters, grads)\n        \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\nparameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=2500)","9f2d6731":"# reshaping\nx_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T","58a2a943":"# Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","83d262d2":"* The shape of the X is (410, 64, 64)\n* The shape of the Y is (410,1)\n* We will seperate test = 15% and train = 75%.\n    * random_state = use same seed is important while randomizing. It always creates same train and test distribution because we have same random_state.","2d3be5f9":"# Deep Learning Example\n","1388fd14":"## Overview the Data Set\nWe will use \"sign language digits data set\" for this tutorial.\nIn this data there are 2062 sign language digits images.\nAs you know digits are from 0 to 9. Therefore there are 10 unique sign.\nAt the beginning of tutorial we will use only sign 0 and 1 for simplicity.\nIn data, sign zero is between indexes 204 and 408. Number of zero sign is 205.\nAlso sign one is between indexes 822 and 1027. Number of one sign is 206. Therefore, we will use 205 samples from each classes(labels).\nNote: Actually 205 sample is very very very little for deep learning. But this is tutorial so it does not matter so much.\nLets prepare our X and Y arrays. X is image array (zero and one signs) and Y is label array (0 and 1).","d3dd97ac":"# Artificial Neural Network (ANN)\n* It is also called deep neural network or deep learning.","b941c4cf":"## Logistic Regression with Sklearn\n* In sklearn library, there is a logistic regression method that ease implementing logistic regression.\n","6c0c577c":"# Logistic Regression\n* When we talk about binary classification( 0 and 1 outputs) what comes to mind first is logistic regression.\n","44ca2b13":"## Create Model"}}