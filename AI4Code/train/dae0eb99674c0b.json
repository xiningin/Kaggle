{"cell_type":{"865d531c":"code","ae1a303a":"code","99da72c3":"code","95337e25":"code","11aac825":"code","3311bc67":"code","5ebd4fd6":"code","fdc48dc9":"code","65f41fd4":"code","65ab6c9d":"code","f001132c":"code","ec8120e2":"code","0859070e":"code","82b47213":"code","b1888901":"code","9d0a7a08":"code","c10b6ef9":"code","db0f542c":"code","c5a23bef":"code","4ea93a41":"code","48c43c2a":"code","df1dba82":"code","bd354bd3":"code","cbf0631a":"code","3d2e8202":"code","c7f152d9":"code","a8934151":"code","d1c1cb0a":"code","08191d2c":"code","f3d5dfc0":"code","79703819":"code","de6eaba3":"code","b98c509a":"code","b9d3349c":"code","8e982109":"code","15c48cf3":"code","3bc6efd4":"code","9360eef3":"code","d374fc99":"code","2d759598":"code","f862e72e":"code","1a3900e2":"code","0695c3df":"code","fe94ea18":"code","30e4ab46":"code","f8a2b44e":"code","b1b27233":"code","1e9f6c82":"code","78bf774f":"code","15e6658d":"code","b459c6a0":"code","46267021":"code","11d4a024":"code","634caaa3":"code","461d0a7b":"code","e3bafa89":"code","f62057e8":"code","64ccd6a0":"code","e3805d08":"code","f85367de":"code","4463443a":"code","411cdded":"code","8325bec7":"code","9a93687c":"code","fd22aca8":"code","e7036059":"code","3561b9be":"code","f152edab":"code","87983081":"code","087e5ff9":"code","d117ee63":"code","1e969080":"code","bfe429b4":"code","f3055da9":"code","54eaed98":"code","cf9046b4":"code","7e49466c":"code","f90dccdb":"code","6492c19f":"code","0aab077f":"code","460e58d9":"code","3d3c35c6":"code","f1996418":"code","369254bf":"code","f50df7c3":"code","002ae73b":"code","11413ff9":"code","edb82659":"code","c0801f6b":"code","dbfc29c9":"code","8e3690fc":"code","f0dd0269":"code","9e5ae18c":"code","87e18324":"code","fcdd5a3d":"code","1f509d21":"code","89d25c12":"code","0bc3f34f":"code","8847efa2":"code","c55461be":"code","035cde3d":"code","f8b3add5":"code","23ab8f3a":"code","47abf647":"code","b5856f54":"code","099c33e5":"code","80805c65":"code","c4137afb":"code","4a183768":"code","95822775":"code","0a103df5":"code","f22874d8":"code","88d109f1":"code","10a9a80b":"code","aac6e8db":"code","7c89d344":"code","fe626999":"code","7e3eb963":"code","34b83b0b":"code","24720175":"code","e794b6bb":"code","e1c619b7":"code","7fa2aad1":"code","42463863":"code","a459632b":"code","31443152":"code","118cbc28":"code","86391bf9":"code","4172f7c5":"code","395b3993":"code","64bb0dda":"code","4a329f04":"markdown","e8b3d1cf":"markdown","056b2679":"markdown","fd423fe8":"markdown","124b8ea9":"markdown","bca2cd6e":"markdown","d40d4fc9":"markdown","a990d9aa":"markdown","d92c9f03":"markdown","57df8060":"markdown","a98f39a7":"markdown","12ac19ce":"markdown","581bd3ff":"markdown","047fe1c4":"markdown","1f1cac27":"markdown","c58ddddb":"markdown","9eadfd41":"markdown","3318a871":"markdown"},"source":{"865d531c":"import pandas as pd\nimport numpy as np\nimport re\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS","ae1a303a":"df = pd.read_csv('..\/input\/reviewuniversalstudio\/universal_studio_branches.csv')","99da72c3":"df.head()","95337e25":"df.shape","11aac825":"df.info()","3311bc67":"df.tail()","5ebd4fd6":"df['branch'].value_counts()","fdc48dc9":"df['rating'].value_counts()","65f41fd4":"df.columns","65ab6c9d":"df['target'] = df.rating.apply(lambda x: 1 if x>=4.0 else 0)","f001132c":"df.head()","ec8120e2":"df = df[['rating','title','review_text', 'branch','target']]","0859070e":"df.head()","82b47213":"df['wordcounts'] = df['title'].apply(lambda x: len(str(x).split()))\ndf['wordcounts_reviewtext'] = df['review_text'].apply(lambda x: len(str(x).split()))","b1888901":"df.head()","9d0a7a08":"df['charactercounts'] = df['title'].apply(lambda x: len(x))\ndf['charactercounts_reviewtext'] = df['review_text'].apply(lambda x: len(x))","c10b6ef9":"df.head()","db0f542c":"def get_avg_word_len(x):\n    words = x.split()\n    word_len = 0\n    for word in words:\n        word_len = word_len + len(word)\n    return word_len\/len(words)","c5a23bef":"df['average_wordlength'] = df['review_text'].apply(lambda x: get_avg_word_len(x))","4ea93a41":"df.head()","48c43c2a":"print(STOP_WORDS)","df1dba82":"df['Stopwords'] = df['review_text'].apply(lambda x: len([t for t in x.split() if t in STOP_WORDS]))","bd354bd3":"df.head()","cbf0631a":"df['hastagscounts'] = df['review_text'].apply(lambda x: len([t for t in x.split() if t.startswith('#')]))\ndf['mentions'] = df['review_text'].apply(lambda x: len([t for t in x.split() if t.startswith('@')]))","3d2e8202":"df.head()","c7f152d9":"df.tail()","a8934151":"df['numericcounts'] = df['review_text'].apply(lambda x: len([t for t in x.split() if t.isdigit()]))","d1c1cb0a":"df.head()","08191d2c":"df['uppercount'] = df['review_text'].apply(lambda x: len([t for t in x.split() if t.isupper() and len(x)>3]))","f3d5dfc0":"df.head()","79703819":"df['review_text'] = df['review_text'].apply(lambda x: x.lower())","de6eaba3":"df.head()","b98c509a":"contractions = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"I would\",\n\"i'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","b9d3349c":"def cont_to_exp(x):\n    if type(x) is str:\n        for key in contractions:\n            value = contractions[key]\n            x = x.replace(key,value)\n        return x\n    else:\n        return x","8e982109":"%%time\ndf['review_text'] = df['review_text'].apply(lambda x:cont_to_exp(x))","15c48cf3":"df.head()","3bc6efd4":"df['mentions']","9360eef3":"df['hastagscounts']","d374fc99":"import re","2d759598":"df['emails'] = df['review_text'].apply(lambda x: re.findall(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)',x))","f862e72e":"df['emails_count'] = df['emails'].apply(lambda x:len(x))","1a3900e2":"df[df['emails_count']>0]","0695c3df":"df['review_text'] = df['review_text'].apply(lambda x: re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '', x))","fe94ea18":"df[df['emails_count']>0]","30e4ab46":"df['urls_count'] = df['review_text'].apply(lambda x: len(re.findall(r'((http|ftp|https):\\\/\\\/)?([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\\/~+#-]*[\\w@?^=%&\\\/~+#-])?',x))) ","f8a2b44e":"df.head()","b1b27233":"df['urls_count']","1e9f6c82":"df['review_text'] = df['review_text'].apply(lambda x:re.sub(r'((http|ftp|https):\\\/\\\/)?([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\\/~+#-]*[\\w@?^=%&\\\/~+#-])?', '',x))","78bf774f":"df.head()","15e6658d":"df.loc[50899]['review_text']","b459c6a0":"df['review_text'] = df['review_text'].apply(lambda x: re.sub('RT',\"\",x))","46267021":"df.head()","11d4a024":"df['review_text'] = df['review_text'].apply(lambda x: re.sub('[^A-Z a-z 0-9-]+', '',x))","634caaa3":"df.head()","461d0a7b":"df.loc[50899]['review_text']","e3bafa89":"df['review_text'] = df['review_text'].apply(lambda x: \" \".join(x.split()))","f62057e8":"df.head()","64ccd6a0":"from bs4 import BeautifulSoup","e3805d08":"%%time\ndf['review_text'] = df['review_text'].apply(lambda x:BeautifulSoup(x,'lxml').get_text())","f85367de":"import unicodedata","4463443a":"def remove_accented_chars(x):\n    x = unicodedata.normalize('NFKD',x).encode('ascii', 'ignore').decode('utf-8','ignore')\n    return x","411cdded":"%%time\ndf['review_text'] = df['review_text'].apply(lambda x:remove_accented_chars(x))","8325bec7":"df['review_text'] = df['review_text'].apply(lambda x: \" \".join([t for t in x.split() if t not in STOP_WORDS]))","9a93687c":"df.head()","fd22aca8":"df.loc[0]['review_text']","e7036059":"#'some', 'quite', 'a', 'anything', 'for', 'be', 'none', 'from', 'others', '\u2018re', 'one', 'around', 'our', 'same', 'mine', 'somewhere', 'has', 'does', 'every', 'less', 'eight', 'above', 'own', 'two', 'another', 'were', 'namely', 'its', 'keep', 'very', 'behind', 'eleven', 'had', 'regarding', 'indeed', 'few', 'more', 'myself', 'unless', 'their', '\u2018d', 'everyone', 'bottom', 'thru', \"n't\", 'put', 'amount', 'my', 'over', 'various', 'n\u2018t', 'am', 'such', 'up', 'everywhere', 'there', 'towards', '\u2019s', 'nine', 'together', 'not', 'show', 'n\u2019t', 'someone', 'here', 'never', 'to', 'yourselves', 'so', 'hereafter', 'him', 'thereby', 'alone', 'whom', 'therefore', 'an', 'already', 'due', 'whether', 'under', 'who', 'but', 'four', 'name', 'whence', 'she', 'themselves', '\u2018s', 'either', 'six', 'we', 'can', 'whereas', 'anyway', 'it', 'on', 'neither', 'get', 'wherever', '\u2019d', 'been', 'into', 'latter', '\u2018ve', 'only', 'wherein', 'did', 'formerly', 'least', 'perhaps', 'say', 'beyond', 're', 'moreover', 'those', \"'s\", '\u2019ve', 'will', 'me', 'because', 'whole', 'other', '\u2019m', 'several', 'what', 'really', 'nothing', 'yours', 'used', 'while', 'that', 'therein', 'why', 'before', 'three', 'whereafter', 'yet', 'meanwhile', 'ca', 'how', '\u2018ll', 'using', \"'m\", \"'ve\", 'nobody', 'front', 'most', 'give', 'even', 'thereafter', 'else', 'mostly', 'nevertheless', 'of', 'former', 'is', 'beforehand', 'third', 'seem', 'whereupon', 'always', 'side', 'within', 'twelve', 'part', 'at', 'see', 'he', 'call', 'hereupon', 'also', 'per', 'are', 'must', 'besides', 'about', 'with', 'during', '\u2018m', 'noone', 'although', 'nor', 'would', 'the', 'you', 'thence', 'otherwise', 'ours', 'afterwards', 'elsewhere', 'go', 'down', 'five', 'hereby', 'whose', 'empty', 'thus', 'forty', 'ourselves', 'latterly', 'seemed', 'i', 'twenty', 'though', 'anyhow', 'just', 'anywhere', 'too', \"'d\", 'anyone', 'make', 'full', 'hence', 'ten', 'somehow', 'which', 'your', 'herein', 'top', 'move', 'everything', 'via', 'nowhere', 'cannot', 'out', 'as', 'next', 'further', 'itself', 'could', 'amongst', 'becomes', 'being', 'by', 'hers', 'upon', 'her', 'them', 'hundred', 'or', 'please', 'again', 'after', 'done', 'made', 'where', 'and', 'in', 'between', 'all', 'seeming', 'no', 'off', 'should', 'each', 'last', 'was', 'well', 'then', 'throughout', 'this', 'back', 'whoever', 'when', 'beside', 'us', 'among', 'whereby', \"'ll\", 'whither', 'thereupon', 'along', 'enough', 'ever', 'still', 'now', 'fifty', 'whatever', 'since', '\u2019re', 'rather', 'his', 'take', 'however', 'do', 'except', 'onto', 'these', 'they', 'almost', 'sometimes', 'yourself', 'become', 'than', 'without', 'became', 'sixty', 'toward', 'serious', '\u2019ll', 'himself', 'many', 'against', 'may', 'if', 'until', 'might', 'sometime', 'whenever', 'often', 'seems', 'fifteen', 'herself', 'across', 'once', \"'re\", 'doing', 'first', 'below', 'something', 'have', 'any', 'much', 'both', 'becoming', 'through'","3561b9be":"df","f152edab":"text = ' '.join(df['review_text'])\nlen(text)","87983081":"text = ' '.join(df['review_text'])","087e5ff9":"text = text.split()","d117ee63":"freq_common = pd.Series(text).value_counts()\nf20= freq_common[:20]","1e969080":"f20","bfe429b4":"df['review_text'] = df['review_text'].apply(lambda x: ' '.join([t for t in x.split() if t not in f20]))","f3055da9":"df.head()","54eaed98":"text = ' '.join(df['review_text'])\nlen(text)","cf9046b4":"rare20 = freq_common[-20:]","7e49466c":"rare20","f90dccdb":"rare = freq_common[freq_common.values == 1]","6492c19f":"rare\n","0aab077f":"df['review_text'] = df['review_text'].apply(lambda x: ' '.join([t for t in x.split() if t not in rare]))","460e58d9":"df.head()","3d3c35c6":"text = ' '.join(df['review_text'])\nlen(text)","f1996418":"17722510 - 14661963","369254bf":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n%matplotlib inline","f50df7c3":"from nltk.corpus import stopwords\ndf_positive = df[df['target']==1]['review_text']\n\nwordcloud1 = WordCloud(\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_positive))\n\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()","002ae73b":"df_negative = df[df['target']==0]['review_text']\n\nwordcloud1 = WordCloud(\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_negative))\n\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()","11413ff9":"import re\n\ndef tokenize(txt):\n    tokens = re.split('\\W+', txt)\n    return tokens\n\ndf['review_text'] = df['review_text'].apply(lambda x: tokenize(x.lower()))\n\ndf.head()","edb82659":"import nltk\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords","c0801f6b":"def remove_stopwords(txt_tokenized):\n    txt_clean = [word for word in txt_tokenized if word not in stopwords]\n    return txt_clean\n\ndf['review_text'] = df['review_text'].apply(lambda x: remove_stopwords(x))\n\ndf.head()","dbfc29c9":"import nltk\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()","8e3690fc":"def stemming(tokenized_text):\n    \n    text = [ps.stem (word) for word in tokenized_text]\n     \n    return text\n\ndf['review_text'] = df['review_text'].apply(lambda x: stemming(x))\n\ndf.head()","f0dd0269":"import string","9e5ae18c":"def clean_text(txt):\n    txt = ''.join([c for c in txt if c is not string.punctuation])\n    tokens = re.split('\\W+', txt)\n    text = [ps.stem (word) for word in tokens if word not in stopwords]\n    return txt","87e18324":"df.head()","fcdd5a3d":"df[df['target']== 1]['review_text'].count()","1f509d21":"df[df['target']== 0]['review_text'].count()","89d25c12":"pos_train = df[df['target']==1][['review_text', 'target']].head(29200)\nneg_train = df[df['target']==0][['review_text', 'target']].head(6430)","0bc3f34f":"pos_test = df[df['target']==1][['review_text', 'target']].tail(12513)\nneg_test = df[df['target']==0][['review_text', 'target']].tail(2755)","8847efa2":"train_df = pd.concat([pos_train, neg_train]).sample(frac = 1).reset_index(drop=True)\ntest_df = pd.concat([pos_test, neg_test]).sample(frac = 1).reset_index(drop=True)","c55461be":"train_df.head()","035cde3d":"\nX_train = train_df['review_text']\nX_test  = test_df['review_text']\ny_train = train_df['target']\ny_test  = test_df['target']","f8b3add5":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(binary=True,analyzer= clean_text)\nvectorizer.fit(X_train)\nX_train_onehot = vectorizer.transform(X_train)\nX_test_onehot = vectorizer.transform(X_test)","23ab8f3a":"print(X_train_onehot.shape)\nprint(X_test_onehot.shape)","47abf647":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\nimport seaborn as sns","b5856f54":"def fit_and_test(classifier, X_train, y_train, X_test, y_test, only_return_accuracy=False):\n  classifier.fit(X_train, y_train)\n  y_hat = classifier.predict(X_test)\n  print('accuracy:', accuracy_score(y_test, y_hat))\n  if not only_return_accuracy:\n    print('f1_score:', f1_score(y_test, y_hat))","099c33e5":"for c in [0.01, 0.02, 0.05, 0.25, 0.5, 0.75, 1,]:\n  lr = LogisticRegression(C=c, max_iter=1000) # 92.91%\n  print (f'At C = {c}:-', end=' ')\n  fit_and_test(lr, X_train_onehot, y_train, X_test_onehot, y_test, True)","80805c65":"logistic = LogisticRegression(C=1,max_iter=10000)\n\nlogistic.fit(X_train_onehot,y_train)","c4137afb":"prediction = logistic.predict(X_test_onehot)","4a183768":"from sklearn.metrics import accuracy_score, confusion_matrix\ncm_lgr1 = confusion_matrix(y_test,prediction) \nnames = np.unique(prediction)\nsns.heatmap(cm_lgr1, square=True, annot=True, cbar=False,xticklabels=names, yticklabels=names, cmap=\"YlGnBu\" ,fmt='g')\nplt.xlabel('Truth')\nplt.ylabel('Predicted')","95822775":"accuracy_score = accuracy_score(y_test,prediction)\nprint(\"Accuracy of  Logistic Regression :\",accuracy_score)","0a103df5":"df_feat = df.drop(labels =['rating','title','review_text','branch','target','emails'], axis =1)","f22874d8":"df_feat","88d109f1":"y = df['target']","10a9a80b":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(analyzer= clean_text)\ntext_counts = cv.fit_transform(df['review_text'])","aac6e8db":"text_counts.toarray().shape","7c89d344":"df_bagow = pd.DataFrame(text_counts.toarray(),columns = cv.get_feature_names())","fe626999":"df_bagow","7e3eb963":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier","34b83b0b":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler","24720175":"sgd = SGDClassifier(n_jobs=-1,random_state=42,max_iter=200)\nlgr = LogisticRegression(random_state=42,max_iter=200)\nlgrcv = LogisticRegressionCV(cv=10,random_state=42,max_iter=1000)\nsvm = LinearSVC(random_state=42,max_iter=200)\nrfc = RandomForestClassifier(random_state=42,n_jobs=-1,n_estimators=200)","e794b6bb":"clf = {'SGD':sgd, 'LGR':lgr, 'LGR-CV':lgrcv, 'SVM':svm, 'RFC':rfc}","e1c619b7":"clf.keys()","7fa2aad1":"def classify(X,y):\n    scaler = MinMaxScaler(feature_range=(0,1))\n    X = scaler.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)\n    for key in clf.keys():\n        clf[key].fit(X_train, y_train)\n        y_pred = clf[key].predict(X_test)\n        ac = accuracy_score(y_test, y_pred)\n        print(key, '---->', ac)","42463863":"%%time\nclassify(df_bagow,y)","a459632b":"df_feat.head()","31443152":"%%time\nclassify(df_feat,y)","118cbc28":"X = df_feat.join(df_bagow)","86391bf9":"X","4172f7c5":"%%time\nclassify(X,y)","395b3993":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(analyzer = clean_text)\nX = tfidf.fit_transform(df['review_text'])","64bb0dda":"%%time\nclassify(pd.DataFrame(X.toarray()),y)","4a329f04":"# STEMMING","e8b3d1cf":"# FEATURE EXTRACTION(BAG OF WORDS)","056b2679":"Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.Tokenization is the first step in the process of text preprocessing steps.It helps us to build the tokens(which is separating the words from the sentences),so that it will be easy to do the machine learning techniques.\nBy using nltk library we can easily tokenize the sentences and words.","fd423fe8":"As we already calculated the accuracy of model above,we will still calculate accuracy of the model on manual featuress which we extracted in the data cleaning part.\nIn this section we will test the model on 5 different MACHINE LEARNING MODELS to see whether the accuracy can be increased.","124b8ea9":"In this section of preprocessing we will pass the last commands befor extracting the bag of words for feature extraction.\nIn this part we will join the text in data,because the tokens in data set is separated by commas.\nWe need to remove those commas from tokens,so that there won't be any error while executing Countvectorizer.","bca2cd6e":"Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP).In this we will be using PORTERSTEMMER,say in simple words stemming can be understood in below steps\n1)run\n2)running\n3)ran\n4)runs\nSo when stemming is incorporated all these words will be changed to same form(i.e 'run')","d40d4fc9":"# CALCULATING ACCURACY ON MANUAL FEATURES","a990d9aa":"# STOPWORDS REMOVAL","d92c9f03":"# VISUALIZING WORDCLOUD","57df8060":"# MANUAL FEATURES + BAG OF WORDS","a98f39a7":"As its clear that the data cleaning and processing part is completed.I will be using COUNTVECTORIZER to convert the tokens into numerical data because machines donot understand the categorical data.it need to be converted to numbers.While converting the sparse matrix will be created and it is represented in sparse matrix.In this section we are going to built the model train and test it.\nHere I will use LOGISTIC REGRESSION model to train and test it.In this,I have used 70% of the data for training and 30% for testing. ","12ac19ce":"Removing stopwords is the important part of any nltk project,because there might be number of words which need to be processed and removed from the data.This can also be done with the help of nltk by incorporating \"stopwords\"","581bd3ff":"# MODEL BUILDING","047fe1c4":"We will be visualizing the Positive and Negative feedbacks that people have given in the rating. For this we can use WORDCLOUD for visualizing and understanding the behavioiur of visitors.\nThe below code is importing the WORDCLOUD and matplotlib library,In this we will define separately the positive and negative feedbacks and also we will be importing the stopwords by using nltk(which is natural language processing tool kit)","1f1cac27":"# TF-IDF","c58ddddb":"# TOKENIZATION","9eadfd41":"In this section,I will be extracting all the MAUAL FEATURES which will be useful in understanding the data and  PREPROCESSING AND CLEANING the data.\nThe steps will be\n\n    GENERAL FEATURE EXTRACTION:\n        1. WORD COUNTS\n        2. CHARACTER COUNTS\n        3. STOPWORDS COUNT\n        4. HASTAGS(#) AND MENTION (@) COUNTS\n        5. IF NUMERIC DIGITS ARE PRESENT\n        6. UPPER CASE WORD COUNTS\n            \n    PREPROCESSING AND CLEANING:\n        1. CONVERSION OF UPPER CASE TO LOWER CASE\n        2. CONTRACTION TO EXPANSION\n        3. EMAILS COUNT AND REMOVAL\n        4. REMOVAL OF SPECIAL CHARACTERS\n        5. REMOVAL OF HTML TAGS\n        6. REMOVAL OF ACCENTED CHARS\n        7. REMOVAL OF MULTIPLE SPACES\n        8. REMOVAL OF COMMON OCCURING AND RARE OCCURING WORDS","3318a871":"# GENERAL FEATURES AND PREPROCESSING"}}