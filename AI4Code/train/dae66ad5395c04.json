{"cell_type":{"755dbb4e":"code","8580be9e":"code","0a15a124":"code","e42b55b7":"code","6f530d36":"code","6b082608":"code","25a0c07f":"code","6ef02aed":"code","2f7567de":"code","125ce50e":"code","6f380807":"code","fbc9a0be":"code","d97016f3":"code","b5a93123":"code","8ecdeb62":"code","f0c0c80d":"code","98bb4b3e":"code","bba39fa7":"code","7e19b872":"code","96f3039c":"code","ddb9b289":"code","2d6abf5e":"code","47116239":"code","7b831045":"code","254d5dec":"code","24c81e28":"code","d5083c59":"code","724ae79f":"code","1767e292":"code","6a38d103":"code","7c88bbdc":"code","924a6ca9":"code","b1d5375d":"code","bca0f14b":"code","e888ab2c":"code","38322b9e":"code","e49aa610":"code","eab08228":"code","5e5ab9f6":"code","c2fa3f38":"code","96eaf7c6":"code","01199af6":"code","431d3222":"code","a2b65941":"code","1369b5e7":"code","c1b51a66":"code","70fba013":"code","28920b90":"code","97ffd31c":"code","1fb7d3c3":"code","ef9674ba":"code","a2f44028":"code","db385537":"code","9d6036fa":"code","e013b347":"code","1032d48c":"code","42415c4e":"code","1e85f134":"code","bb428994":"code","6012f909":"code","9ecfdbe0":"code","3eb9a755":"code","fbebc7fb":"code","27ff4dbd":"code","067e4a60":"code","6de7979e":"code","8292aa32":"code","40ef918a":"code","59249f56":"code","8b7ae095":"code","540b263b":"code","11b3a126":"code","07363035":"code","40077bc2":"code","91ba939d":"code","ebcbedf2":"code","fb90d06d":"code","b914f384":"code","16398c33":"code","0fc62f0f":"code","3a747812":"code","0c829f7b":"code","d020bc6e":"code","1242bdf1":"code","38a04309":"code","12df848b":"code","13f81644":"code","36a5b9e8":"code","9daed797":"code","30fbc137":"code","8f2e663f":"code","4abfab00":"code","e834ea3d":"code","ed602e9d":"code","89f680d5":"code","34d9f10d":"code","b2d612c0":"code","5ece0471":"code","71daa02b":"code","ee3a4d56":"code","4ffbcd89":"code","a013b5e9":"code","b10148a1":"code","7060be43":"code","2fd4a19c":"code","5776fae7":"code","9615f424":"code","b6dc5d11":"code","ed8a6d3e":"code","88e73be9":"code","8972dc70":"code","67cbd273":"code","82723569":"code","72959dd5":"code","608c5064":"code","0f117cb6":"code","2bccd575":"code","b4e02cd5":"code","f007cd5a":"code","0f774f92":"code","e3907aa3":"code","02379a60":"code","b1e8fbb9":"code","1682e114":"code","85d0f6f6":"code","4f923c39":"code","027d46e9":"code","bb158886":"code","4bd4f073":"code","67b3360e":"code","4add5525":"code","494e30da":"code","b2c2ee8c":"code","089a36d5":"code","7e70b2d3":"code","e3e08271":"code","7400597f":"code","0b2e417e":"code","4c973988":"markdown","7d5f8a46":"markdown","6d6eedb5":"markdown","f21595f9":"markdown","e821a222":"markdown","67568131":"markdown","d77ddd35":"markdown","1f541249":"markdown","44b44f2c":"markdown","c573b17d":"markdown","8ca8b440":"markdown","563a8c97":"markdown","b001ce39":"markdown","080bed22":"markdown","b9b04599":"markdown","7d197209":"markdown","ae239e78":"markdown","b468a49a":"markdown","8895245b":"markdown","3b09aa3b":"markdown","d4ead65a":"markdown","7e8a66a8":"markdown","1c253c2b":"markdown","35209f2e":"markdown","ce67ddfe":"markdown","f721cd13":"markdown","9d02b221":"markdown","d75a35a9":"markdown","4038012a":"markdown","583c638c":"markdown","84a6fdb3":"markdown","c6ec129a":"markdown","2490fd51":"markdown","0144ae55":"markdown","fb0f469d":"markdown","e37d7e78":"markdown","2280aac8":"markdown","159a3b87":"markdown","60b460f6":"markdown","b8a0dff7":"markdown","7d6ccc64":"markdown","07f82ad1":"markdown","784d0ddc":"markdown","60823b30":"markdown","99666718":"markdown","27100ecc":"markdown","37f4e631":"markdown","db12d67e":"markdown","8e1d4151":"markdown","0a2d7004":"markdown","e4382e05":"markdown","4cb41e99":"markdown","ac8c6f94":"markdown","e34675d6":"markdown","f01c0a9e":"markdown","c5d2d599":"markdown","a90d3460":"markdown","5e353c5c":"markdown","7db23d07":"markdown","28d9bfcf":"markdown","e6de8ee5":"markdown","63e71064":"markdown","3c5d4fcf":"markdown","75cadff8":"markdown","795e1db1":"markdown","ea0590d3":"markdown","3158dd7c":"markdown","068a7faa":"markdown","86845066":"markdown","18760265":"markdown","de8e6506":"markdown","be48db77":"markdown","1fb880a1":"markdown","782a10d9":"markdown","f3231b88":"markdown","b2228b5b":"markdown","ebbb992b":"markdown","a9f28775":"markdown","0cae648f":"markdown","59a36848":"markdown","6fc27081":"markdown","432eb657":"markdown","fd7f4740":"markdown","75c9aef9":"markdown","1867f9b6":"markdown","5a2acb47":"markdown","9da95b8e":"markdown","8a185baf":"markdown","a6ba429d":"markdown","cc0f86a9":"markdown","291b9d45":"markdown","89ebfbb5":"markdown","edea138c":"markdown","5e1e1fc6":"markdown","16bf2ca3":"markdown","cd1da7e0":"markdown","fec7aa55":"markdown","1898edd7":"markdown","a6fd6ec5":"markdown","0fd14add":"markdown","a81f0f9b":"markdown","73cae813":"markdown","d01bed79":"markdown","9647f014":"markdown","5aa03021":"markdown","200dd2a1":"markdown","66a3842c":"markdown","501bd7e5":"markdown","5939e653":"markdown","40bd4b97":"markdown","28b1350d":"markdown","851e292a":"markdown","e28af57c":"markdown","e3a38c52":"markdown","d42b0a10":"markdown","3af3e521":"markdown","7a44c36c":"markdown","70ef35fe":"markdown","4867fead":"markdown","f24c0c68":"markdown","5af0f494":"markdown","d0c2d880":"markdown","c7f95c2c":"markdown","3986834f":"markdown","60e1f578":"markdown","e0746976":"markdown","d6309cb5":"markdown","be90b2a4":"markdown","3bb165f0":"markdown","ece43d0f":"markdown","2d98ca63":"markdown","56750b45":"markdown","bbd9b2be":"markdown","36cb2e1a":"markdown","968e119d":"markdown"},"source":{"755dbb4e":"from IPython.display import Image\nurl = 'https:\/\/gisgeography.com\/wp-content\/uploads\/2014\/07\/rmse-formula1-300x96.png'\n# Image(url, width=300, height=350)","8580be9e":"import pandas as pd\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","0a15a124":"# Toolbox 101\nimport pandas_profiling\nimport numpy as np\nimport random as rand\nimport datetime as dt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom scipy import stats\n\n# Evaluation\nfrom sklearn.metrics import mean_squared_error #RMSE\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\n# Preset data display\npd.options.display.max_seq_items = 5000\npd.options.display.max_rows = 5000\n\n# Set palette\nflatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\nsns.set_palette(flatui)\nsns.palplot(sns.color_palette(flatui))\n#34495e","e42b55b7":"# Import data\ntrain = pd.read_csv('09-house-train.csv')\ntest = pd.read_csv('09-house-test.csv')","6f530d36":"# Check data\ntrain.head()","6b082608":"test.head()","25a0c07f":"# Drop\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","6ef02aed":"# Check\ntrain.iloc[0:5,:3]","2f7567de":"'''\n\nSome functions to start off with:\n\ntrain.sample()                                           \ntrain.describe()\n    train.describe(include=['O'])\n    train.describe(include='all')\ntrain.head()\ntrain.tail()\ntrain.value_counts().sum()\ntrain.isnull().sum()\ntrain.count()\ntrain.fillna()\n    train.fillna(train[col].mode(), inplace=True)\ntrain.mean()\ntrain.median()\ntrain.mode()\ntrain.shape\ntrain.info()\n\n'''","125ce50e":"# Get data shape, info, columns, & dimensions\nprint (\"*\"*40)\nprint('********** train shape: ' + str(train.shape) + '*'*10)\nprint (train.info())\nprint (\"*\"*40)\nprint('********** test shape: ' + str(test.shape) + '*'*10)","6f380807":"# Get null pct and counts\nnull_cols = pd.DataFrame(train.isnull().sum().sort_values(ascending=False), columns=['Null Data Count'])\nnull_cols_pct = pd.DataFrame(round(train.isnull().sum().sort_values(ascending=False)\/len(train),2)*100, columns=['Null Data Pct'])\n\n# Combine horizontally (axis=1) into a dataframe with column names (keys=[]) then to a data frame\nnull_cols_df = pd.DataFrame(pd.concat([null_cols, null_cols_pct], axis=1))\n\nall_nulls = null_cols_df[null_cols_df['Null Data Pct']>0]\n\nprint('There are', len(all_nulls), 'columns with missing values.')\nall_nulls","fbc9a0be":"# Create figure space\nplt.figure(figsize=(12,8))\n\n# Create plot\nsns.barplot(x=all_nulls.index,\n            y='Null Data Pct',\n            data=all_nulls)\n\n# Set plot features\nplt.xticks(rotation='90')\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of Missing Values', fontsize=15)\nplt.title('Percent of Missing Data by Feature', fontsize=15)","d97016f3":"# Create a dataframe to store the values\nsaleprice_df = pd.concat([train.SalePrice, np.log(train.SalePrice+1).rename('LogSalePrice')],\n                          axis=1, names=['SalePrice', 'LogSalePrice'])\nsaleprice_df.head()","b5a93123":"# Drop\ntrain = train.drop(train[(train.SalePrice>450000)].index)\n\n# Drop column example\n# .drop('Cabin', axis=1, inplace=True)","8ecdeb62":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\n\n# Create figure space\nfig, ax = plt.subplots(figsize=(18,5), ncols=2, nrows=1)\n\n# Create a distribution plot\nax1 = sns.distplot(saleprice_df.SalePrice, kde=False, fit=norm, ax=ax[0])\nax2 = sns.distplot(saleprice_df.LogSalePrice, kde=False, fit=norm, ax=ax[1])\n\n# Set plot features\nax1.set_title('SalePrice Distribution')\nax2.set_title('LogSalePrice Distribution')","f0c0c80d":"# Create figure space\nplt.figure(figsize=(10,5))\n\n# Create plot\nsns.distplot(train['SalePrice'] , fit=norm)\n\n# Get the fitted parameters (feels off without the params somewhere visible)\nmu, sigma = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.0f} and sigma = {:.0f}\\n'.format(mu, sigma))\n\n# Plot distribution\nplt.legend(['Norm Dist. ($\\mu=$ {:.0f} and $\\sigma=$ {:.0f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')","98bb4b3e":"# Skewness and kurtosis\nprint('Skewness: %f' % train['SalePrice'].skew())\nprint('Kurtosis: %f' % train['SalePrice'].kurt())","bba39fa7":"url = 'https:\/\/i.imgur.com\/yIqX5W5.jpg'\n# Image(url, width=500, height=500)","7e19b872":"# Return the natural logarithm of one plus the input array, element-wise.\ntrain['LogSalePrice'] = np.log1p(train.SalePrice)","96f3039c":"stats.probplot(train['SalePrice'], plot=plt)\nplt.show();","ddb9b289":"stats.probplot(np.log(train.SalePrice), plot=plt)\nplt.show();","2d6abf5e":"# Count data types in the train dataset\ntrain.dtypes.value_counts()","47116239":"# Find numeric features\nnum_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum_data = train.select_dtypes(include=num_dtypes)\n\n# Find all other features\ncol_data = train.select_dtypes(include=['object'])","7b831045":"print(num_data.head(2))\nprint(col_data.head(2))","254d5dec":"# Correlation to LogSalePrice feature\npd.DataFrame(abs(num_data.corr()['LogSalePrice']).sort_values(ascending=False))","24c81e28":"# Top Features\nr_squared = num_data.corr()**2\nr_squared.LogSalePrice.sort_values(ascending=False)","d5083c59":"plt.figure(figsize=(15, 10))\nflights = sns.load_dataset(\"flights\")\nflights = flights.pivot(\"month\", \"year\", \"passengers\")\nax = sns.heatmap(flights, annot=True, fmt=\"d\")\n\nfor text in ax.texts:\n    text.set_size(14)\n    if text.get_text() == '118':\n        text.set_size(18)\n        text.set_weight('bold')\n        text.set_style('italic')","724ae79f":"# Create a figure space\nplt.subplots(figsize=(12,9))\n\n# Create matrix\ncorr_plot = sns.heatmap(num_data.corr(),\n#                         annot=True,\n                          cmap='viridis', # YlGnBu, RdBu_r\n                          linewidths=0.20,\n                          linecolor='white',\n                          vmax=1,\n                          square=True,\n                          fmt='.1g',\n                          annot_kws={\"size\": 12})\ncorr_plot","1767e292":"sns.set_style('whitegrid')\nax = train.hist(bins=20, figsize=(15,15), grid=False)\nplt.show();","6a38d103":"# Some numerical features\nplot_list = ['SalePrice', 'YearBuilt', 'YearRemodAdd', 'OverallQual', 'TotalBsmtSF', 'GrLivArea']\n\n# Plot pairplot\nsns.pairplot(train[plot_list])","7c88bbdc":"### Train vs Test Distribution","924a6ca9":"fig, (ax1) = plt.subplots(ncols=1, figsize=(10, 5))\n\nax1.set_title('SalePrice')\nsns.kdeplot(train['SalePrice'], ax=ax1, label=\"train\")\nsns.kdeplot(test['SalePrice'], ax=ax1, label=\"test\")\n\nplt.show()\n","b1d5375d":"# Create figure space\nfig, ax = plt.subplots(figsize=(10,8))\n\n# Create boxplot\nax = sns.boxplot(x=train.OverallQual,\n                 y=train.SalePrice,\n                 data=train)\n\n# Set plot features\nax.set_title('Overall Quality vs. SalePrice', fontsize=15)","bca0f14b":"# Create figure space\nfig, ax = plt.subplots(figsize=(20,10))\n\n# Create boxplot\nax = sns.boxplot(x=train.YearBuilt,\n                 y=train.SalePrice,\n                 data=train)\n\n# Set plot features\nax.set_title('Year Built Sale Price Distribution', fontsize=20)\nax.set_xlabel('Year Built', fontsize=15)\nax.set_ylabel('Sale Price', fontsize=15)\nax.set(xticklabels=[])","e888ab2c":"# Remove white grid\nsns.set_style('whitegrid')\n\n# Set figure space\nfig = plt.figure(figsize=(8, 6))\n\n# Create scatterplot with a loess line\nimport statsmodels\nsns.regplot(x='YearBuilt', y='SalePrice', data=train, lowess=True,\n            color='#34495e', scatter=True,\n            line_kws={'color': 'red'},\n            scatter_kws={'alpha': 0.50})\n\n# sns.scatterplot(x='YearBuilt',\n#                 y='SalePrice',\n#                 data=train)\n\n# Set plot features\nplt.title('Year Built vs. SalePrice', fontsize=15)\nplt.xlabel('Year Built', fontsize=12)\nplt.ylabel('Sale Price', fontsize=12)\nplt.ylim(0, 800000)\nplt.xlim(1860, 2020)","38322b9e":"# Define the function\ndef hist_qq_plot(var):\n    # Get normal distribution \n    sns.distplot(var, fit=norm)\n    fig = plt.figure()\n    qq = stats.probplot(var, plot=plt,)\n#     qq.get_lines()[0].set_markerfacecolor('#34495e')","e49aa610":"hist_qq_plot(train.YearBuilt)","eab08228":"url = 'https:\/\/serving.photos.photobox.com\/3262242285a81460d733332055b13e4ce0aaffc306c6d4165167c76251183608696600e0.jpg'\nImage(url, width=500, height=500)","5e5ab9f6":"# Create figure space\nfig, ax = plt.subplots(figsize=(20,10))\n\n# Create boxplot\nax = sns.boxplot(x=train.MoSold,\n                 y=train.SalePrice,\n                 data=train)\n\n# Set plot features\nax.set_title('Month Sold Price Distribution', fontsize=20)\nax.set_xlabel('Month', fontsize=15)\nax.set_ylabel('Sale Price', fontsize=15)","c2fa3f38":"# Set figure space\nfig, ax = plt.subplots(figsize=(20,10), ncols=2)\n\n# Create countplot()\nax1 = sns.countplot(x='MoSold',\n                   data=train,\n                   linewidth=2,\n                   ax=ax[0]\n                   )\n\nax2 = sns.distplot(train.MoSold, rug=True, ax=ax[1]) \n\n# # Plot the distribution with a histogram and maximum likelihood gaussian distribution fit\n# ax2 = sns.distplot(train.MoSold, rug=True, fit=norm, kde=False, ax=ax[1]) \n\n# Set plot features\nax1.set_title('Homes Sold by Month', fontsize=15)\nax1.set_xlabel('Month', fontsize=15)\nax1.set_ylabel('Units', fontsize=15)\n\nax2.set_title('Homes Sold by Month Univariate Distribution', fontsize=15)\nax2.set_xlabel('Month', fontsize=15)\nax2.set_ylabel('Density', fontsize=15)","96eaf7c6":"# Create figure space\nfig, ax = plt.subplots(figsize=(20,10))\n\n# Create boxplot\nax = sns.boxplot(x=train.YrSold,\n                 y=train.SalePrice,\n                 data=train)\n\n# Set plot features\nax.set_title('Year Sold Price Distribution', fontsize=20)\nax.set_xlabel('Year', fontsize=15)\nax.set_ylabel('Sale Price', fontsize=15)","01199af6":"# Create figure space\nfig, ax = plt.subplots(figsize=(20, 10), ncols=2)\n\n# Create scatterplot\nax1 = sns.scatterplot(x='YearBuilt',\n                      y='SalePrice',\n                      data=train,\n                      ax=ax[0])\n\nax2 = sns.scatterplot(x='YearBuilt',\n                      y='YrSold',\n                      data=train,\n                      ax=ax[1])\n\n# Set plot features\nax1.set_title('Year Built vs. Sales Price', fontsize=15)\nax2.set_title('Year Built vs. Year Sold', fontsize=15)","431d3222":"# Remove outliers after viewing plots from below\n# Drop\ntrain = train.drop(train[(train['1stFlrSF']>2500)].index)","a2b65941":"# Set figure space\nfig, ax = plt.subplots(figsize=(16,10), ncols=2)\n\n# Create scatterplot with a loess line\nax1= sns.regplot(x='1stFlrSF', y='SalePrice', data=train,\n                 lowess=True, color='#34495e', scatter=True,\n                 line_kws={'color': 'red'}, ax=ax[0])\n\n# There looks to be a bit of outliers that skews the visual, let's plot without it\nax2 = sns.regplot(x='1stFlrSF', y='SalePrice',\n                  data=train[(train['1stFlrSF']<2500)], lowess=True,\n                  color='#34495e', scatter=True, line_kws={'color': 'red'},\n                  ax=ax[1])\n\n# Set plot features\nax1.set_title('1st Floor SqFt vs. SalePrice', fontsize=15)\nax1.set_xlabel('SqFt', fontsize=15)\nax1.set_ylabel('Sale Price', fontsize=15)\n\nax2.set_title('1st Floor SqFt vs. SalePrice', fontsize=15)\nax2.set_xlabel('SqFt', fontsize=15)\nax2.set_ylabel('Sale Price', fontsize=15)","1369b5e7":"hist_qq_plot((train['1stFlrSF']))","c1b51a66":"hist_qq_plot(np.log(train['1stFlrSF']))","70fba013":"# Remove outliers after viewing plots from below\n# Drop\ntrain = train.drop(train[(train['GrLivArea']>4000)].index)","28920b90":"# Set figure space\nplt.figure(figsize=(8, 6))\n\n# Create scatterplot with loess\nsns.regplot(x='GrLivArea', y='SalePrice', data=train,\n            lowess=True, scatter=True, color='#34495e',\n            line_kws={'color': 'red'},\n            scatter_kws={'alpha': 0.50})\n\n# Plot diagonal line\nplt.plot([0, 6000], [0, 800000], 'darkorange', lw=2, linestyle='--')\n\n# Set plot features\nplt.title('GrLivArea vs. SalePrice', fontsize=15)\nplt.xlabel('Area', fontsize=12)\nplt.ylabel('Sale Price', fontsize=12)\nplt.xlim(0, 4000)\nplt.ylim(0, 500000)","97ffd31c":"hist_qq_plot((train['GrLivArea']))","1fb7d3c3":"hist_qq_plot(np.log(train['GrLivArea']))","ef9674ba":"# Remove outliers after viewing plots from below\n# Drop\ntrain = train.drop(train[(train['TotalBsmtSF']>3000)].index)","a2f44028":"# Set figure space\nplt.figure(figsize=(8, 6))\n\n# Create scatterplot with loess\nsns.regplot(x='TotalBsmtSF', y='SalePrice', data=train[train.TotalBsmtSF<6000],\n            lowess=True, scatter=True, color='#34495e',\n            line_kws={'color': 'red'},\n            scatter_kws={'alpha': 0.50})\n\n# Plot diagonal line\n# plt.plot([0, 6000], [0, 800000], 'darkorange', lw=2, linestyle='--')\n\n# Set plot features\nplt.title('Total Basement SqFt vs. SalePrice', fontsize=15)\nplt.xlabel('SqFt', fontsize=12)\nplt.ylabel('Sale Price', fontsize=12)\n# plt.xlim(0, 6000)\n# plt.ylim(0, 800000)","db385537":"hist_qq_plot((train['TotalBsmtSF']))","9d6036fa":"hist_qq_plot(np.log(train[train['TotalBsmtSF']>0].TotalBsmtSF))","e013b347":"def pct_bar_labels():\n    '''\n    Function used to label the relative frequency on top of each bars\n    '''\n    # Set font size\n    fs=15\n    \n    # Set plot label and ticks\n    plt.ylabel('Relative Frequency (%)', fontsize=fs)\n    plt.xticks(rotation=0, fontsize=fs)\n    plt.yticks([])\n    \n    # Set individual bar labels in proportional scale\n    for x in ax1.patches:\n        ax1.annotate(str(x.get_height()) + '%', \n        (x.get_x() + x.get_width()\/2., x.get_height()), ha='center', va='center', xytext=(0, 7), \n        textcoords='offset points', fontsize=fs, color='black')\n\ndef freq_table(var):\n    '''\n    Define plot global variables\n    Create a function that will populate a frequency table (%)\n    Get counts per feature then get the percentage over the total counts\n    '''\n    global ax, ax1\n    \n    # Get Values and pct and combine it into a dataframe\n    count_freq = var.value_counts()\n    pct_freq = round(var.value_counts(normalize=True)*100, 2)\n    \n    # Create a dataframe\n    df = pd.DataFrame({'Count': count_freq, 'Percentage': pct_freq})\n    \n    # Print variable name\n    print('Frequency of', var.name, ':')\n    display(df)\n    \n    # Create plot\n    ax1 = pct_freq.plot.bar(title='Percentage of {}'.format(var.name), figsize=(12,8))\n    ax1.title.set_size(15)\n    pct_bar_labels()\n    plt.show()","1032d48c":"freq_table(train.BsmtQual)","42415c4e":"# Set figure size\n# fig, ax = plt.subplots(figsize=(16,10), ncols=2)\n\n# Create plots: 1 for bsmtqual\/totalbsmtsf & 1 for bsmtqual\/saleprice\nax1 = sns.catplot(x='BsmtQual',\n                  y='TotalBsmtSF',\n                  data=train[train.TotalBsmtSF<6000],\n                  kind='box',\n                  order=['Fa', 'TA', 'Gd', 'Ex']\n                  )\n\nax2 = sns.catplot(x='BsmtQual',\n                  y='SalePrice',\n                  data=train,\n                  kind='box',\n                  order=['Fa', 'TA', 'Gd', 'Ex']\n                  )\n\n# Set plot features\nax1.fig.suptitle('BsmtQual vs. TotalBsmtSF', fontsize=12)\n# ax1.subplots_adjust(top=0.80) # for facet only\nax2.fig.suptitle('BsmtQual vs. SalePrice', fontsize=12)","1e85f134":"# Create the figure space\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Create boxplots\nax = sns.boxplot(x='FireplaceQu',\n                 y='SalePrice',\n                 data=train,\n                 order=['Po', 'Fa', 'TA', 'Gd', 'Ex'])\n\n# Set plot features\nax.set_title('Fireplace Quality Distribution', fontsize=15)\nax.set_xlabel('Fireplace Quality', fontsize=12)\nax.set_ylabel('Sale Price', fontsize=12)","bb428994":"freq_table(train.FireplaceQu)","6012f909":"# Create the figure space\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Create boxplots\nax = sns.boxplot(x='MSZoning',\n                 y='SalePrice',\n                 data=train)\n#                  order=['Po', 'Fa', 'TA', 'Gd', 'Ex'])\n\n# Set plot features\nax.set_title('MSZoning Distribution', fontsize=15)\nax.set_xlabel('MSZoning', fontsize=12)\nax.set_ylabel('Sale Price', fontsize=12)","9ecfdbe0":"# Set figure space\nplt.figure(figsize=(8, 6))\n\n# Create scatterplot with loess line\nsns.regplot(x='LotArea',\n            y='SalePrice',\n            data=train,\n            lowess=True,\n            scatter=True,\n            color='#34495e',\n            line_kws={'color': 'red'},\n            scatter_kws={'alpha': 0.50})\n\n# Set plot features\nplt.title('Lot Area vs. Sale Price', fontsize=15)\nplt.xlabel('Lot Area', fontsize=12)\nplt.ylabel('Sale Price', fontsize=12)\n","3eb9a755":"# Drop outliers\ntrain = train.drop(train[(train.LotArea>30000)].index)","fbebc7fb":"# Set figure space\nplt.figure(figsize=(8, 6))\n\n# Create scatterplot with loess line\nsns.regplot(x='LotArea',\n            y='SalePrice',\n            data=train,\n            lowess=True,\n            scatter=True,\n            color='#34495e',\n            line_kws={'color': 'red'},\n            scatter_kws={'alpha': 0.50})\n\n# Set plot features\nplt.title('Lot Area vs. Sale Price', fontsize=15)\nplt.xlabel('Lot Area', fontsize=12)\nplt.ylabel('Sale Price', fontsize=12)","27ff4dbd":"# Looks more normal removing the additional points\nhist_qq_plot(train[train.LotArea<30000].LotArea)","067e4a60":"# See if I really want to remove them\ntrain[train.LotArea>25000]","6de7979e":"train['YrRemodel_Diff'] = train['YearRemodAdd'] - train['YearBuilt']\ntest['YrRemodel_Diff'] = test['YearRemodAdd'] - test['YearBuilt']","8292aa32":"# Create figure space\nfig, ax = plt.subplots(figsize=(20,10))\n\n# Create boxplot\nax = sns.boxplot(x='YrRemodel_Diff',\n                 y='SalePrice',\n                 data=train)\n\n# Set plot features\nplt.xticks(rotation='45')\nax.set_title('Year Remodel Difference', fontsize=15)\nax.set_xlabel('Year Difference', fontsize=12)\nax.set_ylabel('Sale Price', fontsize=12)\nax.set(xticklabels=[])","40ef918a":"hist_qq_plot(train.YrRemodel_Diff)","59249f56":"# Let's plot with the zeros\nhist_qq_plot(train[train.YrRemodel_Diff>1].YrRemodel_Diff)","8b7ae095":"# Define function\ndef create_remodel_diff_bin(var):\n    grp=''\n    if var==0:\n        grp='Never Remodeled'\n    elif var>=1 and var <11:\n        grp='1-10'\n    elif var>=11 and var <21:\n        grp='11-20'\n    elif var>=21 and var <31:\n        grp='21-30'\n    elif var>=31 and var <41:\n        grp='31-40'\n    elif var>=41 and var <51:\n        grp='41-50'\n    elif var>=51 and var <61:\n        grp='51-60'\n    elif var>=61 and var <71:\n        grp='61-70'\n    else:\n        grp='71+'\n    return grp","540b263b":"train['Yr_Remodel_Group'] = train.YrRemodel_Diff.map(create_remodel_diff_bin)\ntest['Yr_Remodel_Group'] = test.YrRemodel_Diff.map(create_remodel_diff_bin)\ntrain.head()","11b3a126":"# Create figure space\nfig, ax = plt.subplots(figsize=(15,10))\n\n# Create boxplot\nax = sns.boxplot(x='Yr_Remodel_Group',\n                 y='SalePrice',\n                 data=train)\n\n# Set plot features\nplt.xticks(rotation='45')\nax.set_title('Year Remodel Group', fontsize=15)\nax.set_xlabel('Year Group', fontsize=12)\nax.set_ylabel('Sale Price', fontsize=12)","07363035":"# Create figure space\nfig, ax = plt.subplots(figsize=(12,8))\n\n# Create jitter\nax = sns.stripplot(x='Yr_Remodel_Group',\n                   y='SalePrice',\n                   data=train,\n                   jitter=True,\n                   alpha=0.50)\n\n# Set plot features\nplt.xticks(rotation='45')\nax.set_title('Year Remodel Distribution', fontsize=15)\nax.set_xlabel('Year Group', fontsize=12)\nax.set_ylabel('Sale Price', fontsize=12)","40077bc2":"train.drop(['YrRemodel_Diff'], axis=1, inplace=True)\ntest.drop(['YrRemodel_Diff'], axis=1, inplace=True)","91ba939d":"# # Create facet grid\n# ax = sns.FacetGrid(train,\n#                    col='SaleType',\n#                    row='Yr_Remodel_Group',\n#                    margin_titles=True,\n#                    hue='SaleType')\n\n# # Create plot using map()\n# ax = ax.map(plt.hist, 'SalePrice', edgecolor='w')\n\n# Create figure space\nfig, ax = plt.subplots(figsize=(15,6), ncols=2, nrows=1)\n\n# Create plot\nax1 = sns.boxplot(x='SaleType',\n                 y='SalePrice',\n                 data=train,\n                 ax=ax[0])\n\nax2 = sns.stripplot(x='SaleType',\n                   y='SalePrice',\n                   data=train,\n                   jitter=True,\n                   alpha=0.50,\n                   edgecolor='w',\n                   ax=ax[1])\n\n# Set plot features\nax1.set_title('Sale Type Distribution by BoxPlot', fontsize=15)\nax2.set_title('Sale Type Distribution by Points', fontsize=15)","ebcbedf2":"# Create figure space\nfig, ax = plt.subplots(figsize=(15,6), ncols=2, nrows=1)\n\n# Create plot\nax1 = sns.scatterplot(x='LotFrontage',\n                      y='SalePrice',\n                      data=train,\n                      ax=ax[0])\n\nax2 = sns.stripplot(x='LotFrontage',\n                   y='SalePrice',\n                   data=train,\n                   jitter=True,\n                   alpha=0.50,\n                   edgecolor='w',\n                   ax=ax[1])\n\n# Set plot features\nax1.set_title('Lot Area Distribution', fontsize=15)\nax2.set_title('Lot Area Distribution', fontsize=15)\nax2.set(xticklabels=[])","fb90d06d":"# Features that pertains to area\nax = sns.pairplot(train[['SalePrice', 'LotArea', '1stFlrSF', 'LotFrontage']])\nax.set(xticklabels=[])\nplt.show()","b914f384":"train['HouseAge'] = train.YrSold.max() - train.YearBuilt\ntest['HouseAge'] = test.YrSold.max() - test.YearBuilt","16398c33":"# Set colors\n# colors = {1: 'lightblue', 0: 'gray'}\n\n# Create a figure\nplt.figure(figsize=(20,15))\n\n# Create facet grid\nax = sns.FacetGrid(train,\n                   col='MoSold',\n                   hue='CentralAir',\n                   margin_titles=True)\n#                    palette=colors)\n\n# Create scatter\nax.map(plt.scatter, 'HouseAge', 'SalePrice', edgecolor='w') # , s=100)\n\n# Add a legend\nax.add_legend()\n\n# Set plot features\nax.fig.suptitle('Sale Price by Month Sold, Central Air Conditioning & House Age', size=15)\n# plt.subplots_adjust(top=0.85)\nplt.show()","0fc62f0f":"# Calling corr_plot wasn't showing the plot so copy and pasting it again\n\n# Create a figure space\nplt.subplots(figsize=(12,9))\n\n# Create matrix\ncorr_plot = sns.heatmap(num_data.corr(),\n#                         annot=True,\n                          cmap='viridis', # YlGnBu, RdBu_r\n                          linewidths=0.20,\n                          linecolor='white',\n                          vmax=1,\n                          square=True,\n                          fmt='.1g',\n                          annot_kws={\"size\": 12})\ncorr_plot","3a747812":"# Set figure space\nplt.figure(figsize=(15,10))\n\n# Set k (number of variables for the heatmap)\nk = 15\n\n# Create correlation using corr.nlargest()\ntop_corr = num_data.corr().nlargest(k, 'SalePrice')['SalePrice'].index\n\n# Get correlation coefficient\ncm = np.corrcoef(train[top_corr].values.T)\n\n# Set plot scale\nsns.set(font_scale=1.25)\n\n# Create heatmap\ntop_corr_plot = sns.heatmap(cm, cbar=True, annot=True, square=True,\n                            fmt='.2f', annot_kws={'size': 12}, yticklabels=top_corr.values,\n                            xticklabels=top_corr.values)\nplt.show()","0c829f7b":"train['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\ntest['TotalSF'] = test['TotalBsmtSF'] + test['1stFlrSF'] + test['2ndFlrSF']","d020bc6e":"# Plot \nhist_qq_plot((train['TotalSF']))","1242bdf1":"# Check the outliers to make sure it was dropped already\ntrain[train.TotalSF>6000]","38a04309":"pandas_profiling.ProfileReport(train)\n\n# # Output to a html file if needed\n# profile = pandas_profiling.ProfileReport(train)\n# profile.to_file(outputfile='House regression data profiling.html')","12df848b":"# Find numeric features\nnum_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum_data = train.select_dtypes(include=num_dtypes)\n\n# Find all other features\ncol_data = train.select_dtypes(include=['object'])","13f81644":"# Copy data to make sure we don't mess up and if we do just rerun\nmodel_train = train.copy()\n\n# Replace categorical features with none\nfill_col_columns = ['Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleCondition', 'Yr_Remodel_Group']\n\n# Replace categorical features with mode\nfill_mode_columns = ['SaleType', 'Exterior1st', 'Exterior2nd', 'KitchenQual', 'Electrical', 'MSZoning']\n\n# None values\nfor i in fill_col_columns:\n    model_train[i] = model_train[i].fillna('None')\n\n# Mode values\nfor i in fill_mode_columns:\n    model_train[i] = model_train[i].fillna(model_train[i].mode())\n\n# Specific values\nmodel_train['Functional'] = model_train['Functional'].fillna('Typ')\nmodel_train['Electrical'] = model_train['Electrical'].fillna('SBrkr')","36a5b9e8":"fill_num_columns = ['MSSubClass','LotArea', 'OverallQual', 'OverallCond',\n       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n       'MoSold', 'YrSold', 'SalePrice', 'LogSalePrice', 'TotalSF', 'HouseAge']\n\n# Zero values\nfor i in fill_num_columns:\n    model_train[i] = model_train[i].fillna(0)\n    \n# Specific values\n'''\nLook up differences between these three\ntransform() is an operation used in conjunction with groupby (which is one of the most useful operations in pandas)\nvs. map()\nvs apply()\n'''\nmodel_train['LotFrontage'] = model_train.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median()))","9daed797":"model_train.head()","30fbc137":"# Check for nulls\n# model_train.isnull().sum().sort_values(ascending=False)","8f2e663f":"# train['NewHouse'] = [1 if train['YearBuilt'] == train['YrSold'] else 0]\n\n# NewHouse = []\n# for i in train:\n#     if train.loc[i, 'YearBuilt'] == train.loc[i, 'YrSold']:\n#         NewHouse.append('1')\n#     else:\n#         NewHouse.append('0')\n\n# # Define a function to get NewHouse\n# def new_house(df):\n#     text=''\n#     if df == train['YrSold']:\n#         text='Yes'\n#     else:\n#         text='No'\n#     return text\n\n# train['NewHouse'] = train.YearBuilt.map(new_house)","4abfab00":"# Create new features based on conditions\nmodel_train['NewHouse'] = np.where(model_train['YearBuilt']==model_train['YrSold'], 1, 0)\nmodel_train['TotalLotArea'] = model_train.LotFrontage + model_train.LotArea\nmodel_train['OverallQualityCondition'] = model_train.OverallCond + model_train.OverallQual\n\n# Basically flags if a house has a certain feature\nmodel_train['HasWoodDeck'] = (model_train['WoodDeckSF'] == 0) * 1 # found this from another kernel, interesting way to write method\nmodel_train['HasOpenPorch'] = (model_train['OpenPorchSF'] == 0) * 1\nmodel_train['HasEnclosedPorch'] = (model_train['EnclosedPorch'] == 0) * 1\nmodel_train['Has3SsnPorch'] = (model_train['3SsnPorch'] == 0) * 1\nmodel_train['HasScreenPorch'] = (model_train['ScreenPorch'] == 0) * 1\nmodel_train['HasPool'] = model_train['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nmodel_train['Has2ndfloor'] = model_train['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nmodel_train['HasGarage'] = model_train['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nmodel_train['HasBsmt'] = model_train['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nmodel_train['HasFireplace'] = model_train['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n# Total Bathrooms - a big feature, definitely something I would look at \nmodel_train['TotalBathrooms'] = (model_train['FullBath'] + (0.5*model_train['HalfBath']) +\n                                 model_train['BsmtFullBath'] + (0.5*model_train['BsmtHalfBath']))\n\n##########################################################\n# Create new features based on conditions\ntest['NewHouse'] = np.where(test['YearBuilt']==test['YrSold'], 1, 0)\ntest['TotalLotArea'] = test.LotFrontage + test.LotArea\ntest['OverallQualityCondition'] = test.OverallCond + test.OverallQual\n\n# Basically flags if a house has a certain feature\ntest['HasWoodDeck'] = (test['WoodDeckSF'] == 0) * 1\ntest['HasOpenPorch'] = (test['OpenPorchSF'] == 0) * 1\ntest['HasEnclosedPorch'] = (test['EnclosedPorch'] == 0) * 1\ntest['Has3SsnPorch'] = (test['3SsnPorch'] == 0) * 1\ntest['HasScreenPorch'] = (test['ScreenPorch'] == 0) * 1\ntest['HasPool'] = test['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntest['Has2ndfloor'] = test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntest['HasGarage'] = test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntest['HasBsmt'] = test['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntest['HasFireplace'] = test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n# Total Bathrooms - a big feature, definitely something I would look at \ntest['TotalBathrooms'] = (test['FullBath'] + (0.5*test['HalfBath']) +\n                          test['BsmtFullBath'] + (0.5*test['BsmtHalfBath']))\n","e834ea3d":"# Check new feature to see if this makes sense\nmodel_train.groupby('NewHouse').SalePrice.mean()","ed602e9d":"## Shorter version\n# numeric_features = model_train.dtypes[model_train.dtypes != \"object\"].index\n# Check the skew of all numerical features\n# skew_features = model_train[numeric_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n# print(\"\\nSkew in numerical features: \\n\")\n# skews = pd.DataFrame({'Skew' :skew_features})\n# skews.head()\n\n# For loop version\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnumeric_list = []\nfor i in model_train.columns:\n    if model_train[i].dtype in numeric_dtypes: \n        numeric_list.append(i)\n\n# Dataframe has no attribute map() so we have to use the apply() fuction\nskew_features = model_train[numeric_list].apply(lambda x: skew(x)).sort_values(ascending=False)\nskews = pd.DataFrame({'Skew':skew_features})\nskews.head(10)","89f680d5":"from scipy.special import boxcox1p # Compute the Box-Cox transformation of 1 + x\nfrom scipy.stats import boxcox_normmax # Compute optimal Box-Cox transform parameter for input data\n\n# Get high skews\nhigh_skew = skew_features[skew_features>0.50] # mid\nskew_index = high_skew.index \nprint('There are {} skewed features.'.format(high_skew.shape[0]))\n\n# Loop through the index and transform\nfor i in skew_index:\n    model_train[i] = boxcox1p(model_train[i], boxcox_normmax(model_train[i]+1))\n\n# Get new transformed features\nnew_skew_features = model_train[numeric_list].apply(lambda x: skew(x)).sort_values(ascending=False)\nnew_skews_df = pd.DataFrame({'Skew': new_skew_features})\nnew_skews_df.head(15)","34d9f10d":"# Check corr\npd.DataFrame(abs(train.corr()['SalePrice']).sort_values(ascending=False))","b2d612c0":"model_train.columns","5ece0471":"# Drop any feature that we have not yet\nmodel_train.drop(['SalePrice', 'YearRemodAdd'], axis=1, inplace=True)\ntest.drop(['YearRemodAdd'], axis=1, inplace=True)","71daa02b":"# Get data shape, info, columns, & dimensions\nprint (\"*\"*40)\nprint('********** train shape: ' + str(model_train.shape) + '*'*10)\nprint (\"*\"*40)\nprint('********** test shape: ' + str(test.shape) + '*'*10)","ee3a4d56":"# Convert to str\ntrain['MSSubClass'] = train['MSSubClass'].apply(str)\ntrain['YrSold'] = train['YrSold'].astype(str)\ntrain['MoSold'] = train['MoSold'].astype(str)\n\n# Wonder if I should update this here as well - i'll leave it for now\ntest['MSSubClass'] = test['MSSubClass'].apply(str)\ntest['YrSold'] = test['YrSold'].astype(str)\ntest['MoSold'] = test['MoSold'].astype(str)","4ffbcd89":"# Create a list of features for specific dummy variables\ndummy_list = []\nmodel_train = pd.get_dummies(model_train).reset_index(drop=True) \n                           # drop_first=False\n                            # Whether to get k-1 dummies out of k categorical levels by removing the first level\n    \n# Other parameters if there is a certain list: columns=dummy_list","a013b5e9":"model_train_dupe = model_train.copy()\nmodel_train.shape","b10148a1":"# Remove any duplicated column names\nmodel_train = model_train.loc[:, ~model_train.columns.duplicated()]","7060be43":"# ### Adding in top features from xgb and rf (this step is found after feature importance)\n# top_features_list = \n# ['1stFlrSF',\n#  '2ndFlrSF',\n#  'Alley_Grvl',\n#  'BedroomAbvGr',\n#  'BldgType_1Fam',\n#  'BsmtCond_Fa',\n#  'BsmtFinSF1',\n#  'BsmtFinType1_Unf',\n#  'BsmtQual_Ex',\n#  'BsmtQual_Gd',\n#  'BsmtUnfSF',\n#  'CentralAir_N',\n#  'CentralAir_Y',\n#  'EnclosedPorch',\n#  'ExterQual_Fa',\n#  'Exterior1st_BrkComm',\n#  'FireplaceQu_None',\n#  'Fireplaces',\n#  'Foundation_BrkTil',\n#  'Foundation_PConc',\n#  'FullBath',\n#  'Functional_Maj1',\n#  'Functional_Sev',\n#  'Functional_Typ',\n#  'GarageArea',\n#  'GarageCars',\n#  'GarageCond_TA',\n#  'GarageFinish_Fin',\n#  'GarageFinish_Unf',\n#  'GarageType_Attchd',\n#  'GarageType_Detchd',\n#  'GarageYrBlt',\n#  'GrLivArea',\n#  'HasFireplace',\n#  'HeatingQC_Ex',\n#  'HeatingQC_Fa',\n#  'Heating_GasA',\n#  'Heating_Grav',\n#  'HouseAge',\n#  'KitchenAbvGr',\n#  'KitchenQual_Ex',\n#  'KitchenQual_Fa',\n#  'KitchenQual_Gd',\n#  'KitchenQual_TA',\n#  'LotArea',\n#  'LotFrontage',\n#  'LotShape_Reg',\n#  'MSSubClass',\n#  'MSZoning_C (all)',\n#  'MSZoning_RL',\n#  'MSZoning_RM',\n#  'MasVnrArea',\n#  'MoSold',\n#  'Neighborhood_Crawfor',\n#  'Neighborhood_OldTown',\n#  'OpenPorchSF',\n#  'OverallCond',\n#  'OverallQual',\n#  'OverallQualityCondition',\n#  'SaleCondition_Abnorml',\n#  'SaleCondition_Normal',\n#  'SaleType_New',\n#  'TotRmsAbvGrd',\n#  'TotalBathrooms',\n#  'TotalBsmtSF',\n#  'TotalLotArea',\n#  'TotalSF',\n#  'WoodDeckSF',\n#  'YearBuilt',\n#  'YrSold',\n#  'Yr_Remodel_Group_31-40',\n#  'LogSalePrice']","2fd4a19c":"# # Top features\n# model_train = model_train[top_features_list]","5776fae7":"# Save the actual test set to another variable\ntest_dupe = test.copy()","9615f424":"# Split the model data to train and test\ntrain = model_train\nprint('Train data shape: ' + str(train.shape))\n\n\n# train = model_train[1:891]\n# test = model_train[892:model_train.shape[0]]\n# print('Test data shape: ' + str(test.shape))","b6dc5d11":"# Split\ny = train['LogSalePrice']\nX = train.drop(['LogSalePrice'], axis=1)","ed8a6d3e":"# Import split module\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)\n\n# Check\nX_train.head(3)","88e73be9":"X_test.shape","8972dc70":"y_test.shape","67cbd273":"from mlxtend.regressor import StackingCVRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n# from sklearn.ensemble import AdaBoostRegressor\n# from sklearn.ensemble import BaggingRegressor\n# from sklearn.ensemble import ExtraTreesRegressor\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.svm import SVR # SVC = classification\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor, plot_importance ","82723569":"# Set seed\nseed = 150\n\n# Instantiate baseline model\nlm = LinearRegression()\n\n# Instantiate additional base models\nrf = RandomForestRegressor(random_state=seed)\ngbr = GradientBoostingRegressor(random_state=seed)\n# abr =AdaBoostRegressor(random_state=seed)\n# br = BaggingRegressor(random_state=seed)\n# etr = ExtraTreesRegressor(random_state=seed)\nlasso_cv = LassoCV()\nridge_cv = RidgeCV()\nglmnet_cv = ElasticNetCV(random_state=seed)\nsvr = SVR()\nlgbmr = LGBMRegressor(random_state=seed)\nxgbr = XGBRegressor(random_state=seed)","72959dd5":"# Set CV method\ncv = KFold(n_splits=10, random_state=150, shuffle=True)","608c5064":"# Define the function for mean_squared_error() evaluation for the blended model\ndef base_rmse(y, y_pred):\n    ''' \n    Return the sqrt of mean the mean squared error between the two values\n    '''\n    rmse = np.sqrt(mean_squared_error(y, y_pred))\n    return rmse\n\n# Define the function for a cross-validation evaluation for meta-learners\ndef cv_rmse(model, X=X_train, y=y_train):\n    '''\n    Return the sqrt of mean the cross-validated mean squared error between the two values\n    Replace the default parameter arguments X & y when we use testing data\n    '''\n    cv_rmse = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv))\n    return cv_rmse","0f117cb6":"# Linear Regression\nlm.fit(X_train, y_train)\nlm_y_pred = lm.predict(X_test)\nlm_rmse = np.sqrt(mean_squared_error(y_test, lm_y_pred))\nprint('Linear Regression Base Model Score: {:.2f}'.format(lm_rmse))","2bccd575":"# Linear Regression\nstarttime = dt.datetime.now()\nlm = make_pipeline(RobustScaler(), lm).fit(X=X_train, y=y_train)\nlm_score = cv_rmse(lm).mean()\nendtime = dt.datetime.now()\nlm_build_time = endtime-starttime\n\n# Random Forest\nstarttime = dt.datetime.now()\nrf = RandomForestRegressor(n_estimators=1000,\n#                            max_depth=10,\n#                            min_samples_split=5,\n#                            min_samples_leaf=5,\n#                            max_features=None,\n#                            oob_score=True\n                          )\nrf_score = cv_rmse(rf).mean()\nendtime = dt.datetime.now()\nrf_build_time = endtime-starttime\n\n# Gradient Boosting\nstarttime = dt.datetime.now()\ngbr = GradientBoostingRegressor(n_estimators=1000,\n                                learning_rate=0.01,\n#                                 max_depth=5,\n#                                 max_features='sqrt',\n#                                 min_samples_leaf=15,\n#                                 min_samples_split=10, \n#                                 loss='huber',\n                                random_state=100\n                               )\ngbr_score = cv_rmse(gbr).mean()\nendtime = dt.datetime.now()\ngbr_build_time = endtime-starttime\n\n# Lasso CV\nstarttime = dt.datetime.now()\nlasso_cv = make_pipeline(RobustScaler(), LassoCV(cv=cv, random_state=seed))\nlasso_cv_score = cv_rmse(lasso_cv).mean()\nendtime = dt.datetime.now()\nlasso_cv_build_time = endtime-starttime\n\n# Ridge CV\nstarttime = dt.datetime.now()\nridge_cv = make_pipeline(RobustScaler(), RidgeCV(cv=cv))\nridge_cv_score = cv_rmse(ridge_cv).mean()\nendtime = dt.datetime.now()\nridge_cv_build_time = endtime-starttime\n\n# GLMNET CV\nstarttime = dt.datetime.now()\nglmnet_cv = make_pipeline(RobustScaler(), ElasticNetCV(cv=cv))\nglmnet_cv_score = cv_rmse(glmnet_cv).mean()\nendtime = dt.datetime.now()\nglmnet_cv_build_time = endtime-starttime\n\n# Support Vector\nstarttime = dt.datetime.now()\nsvr = make_pipeline(RobustScaler(), SVR())\nsvr_score = cv_rmse(svr).mean()\nendtime = dt.datetime.now()\nsvr_build_time = endtime-starttime\n\n# Light Gradient Boosting\nstarttime = dt.datetime.now()\nlgbmr = LGBMRegressor(objective='regression', \n#                       num_leaves=4,\n                      learning_rate=0.01, \n                      n_estimators=1000,\n#                       max_bin=200, \n#                       bagging_fraction=0.75,\n#                       bagging_freq=5, \n#                       bagging_seed=7,\n#                       feature_fraction=0.2,\n#                       feature_fraction_seed=7,\n                      verbose=-1\n                     )\nlgbmr_score = cv_rmse(lgbmr).mean()\nendtime = dt.datetime.now()\nlgbmr_build_time = endtime-starttime\n\n# Extreme Gradient Boosting\nstarttime = dt.datetime.now()\nxgbr = XGBRegressor(learning_rate=0.01,\n                    n_estimators=1000,\n#                     max_depth=3,\n#                     min_child_weight=0,\n#                     gamma=0,\n#                     subsample=0.7,\n#                     colsample_bytree=0.7,\n                    objective='reg:squarederror',\n#                     nthread=-1,\n#                     scale_pos_weight=1,\n#                     reg_alpha=0.00006, \n                    random_state=100\n                   )\nxgbr_score = cv_rmse(xgbr).mean()\nendtime = dt.datetime.now()\nxgbr_build_time = endtime-starttime\n\n# Stacking with a XGBoost optimizer (stacking seems pretty interesting)\nstarttime = dt.datetime.now()\nstacking_generate = StackingCVRegressor(regressors=(rf, gbr, lasso_cv, ridge_cv, glmnet_cv, lgbmr, xgbr),\n                                        meta_regressor=ridge_cv,\n                                        use_features_in_secondary=True)\nendtime = dt.datetime.now()\nstacking_build_time = endtime-starttime\n# stacking_score = cv_rmse(stacking_generate).mean() - value error feature mismatch","b4e02cd5":"# Create a dataframe to store the values and models\ninitial_scores_df = pd.DataFrame({'CV Score': [lm_score,\n                                               rf_score,\n                                               gbr_score,\n                                               lasso_cv_score,\n                                               ridge_cv_score,\n                                               glmnet_cv_score,\n                                               svr_score,\n                                               lgbmr_score,\n                                               xgbr_score]})\n\ninitial_scores_df.index = ['LM', 'RF', 'GBR', 'LASSO' ,'Ridge' ,'ElasticNet' ,'Support Vector' ,'Light GBM' , 'XGB']\nsorted_initial_scores_df = initial_scores_df.sort_values(by='CV Score', ascending=True)\nsorted_initial_scores_df","f007cd5a":"# Fit the new models so use for blending\nstarttime = dt.datetime.now()\n\nprint('Fitting stacking model...')\nstack_gen_model = stacking_generate.fit(np.array(X_train), np.array(y_train))\nprint('Fitting linear model...')\nlm_fit_model = lm.fit(X_train, y_train)\nprint('Fitting forest model...')\nrf_fit_model = rf.fit(X_train, y_train)\nprint('Fitting gradient boosting model...')\ngbr_fit_model = gbr.fit(X_train, y_train)\nprint('Fitting lasso model...')\nlasso_fit_model = lasso_cv.fit(X_train, y_train)\nprint('Fitting ridge model...')\nridge_fit_model = ridge_cv.fit(X_train, y_train)\nprint('Fitting elastic net model...')\nglmnet_fit_model = glmnet_cv.fit(X_train, y_train)\nprint('Fitting support vector model...')\nsvr_fit_model = svr.fit(X_train, y_train)\nprint('Fitting light gradient boosting model...')\nlgbr_fit_model = lgbmr.fit(X_train, y_train)\nprint('Fitting extreme gradient boosting model...')\nxgbr_fit_model = xgbr.fit(X_train, y_train)\n\nendtime = dt.datetime.now()\n\nprint('Model Fitting Time Elapsed: {}'.format(endtime-starttime))","0f774f92":"# Define blending function\ndef blend_models(X):\n    '''\n    Function will return predicted values from the test data with\n    weighted percentages per model\n    '''\n    return ( (0.14 * stack_gen_model.predict(np.array(X))) + \\\n             (0.13 * ridge_fit_model.predict(X)) + \\\n             (0.13 * lasso_fit_model.predict(X)) + \\\n             (0.13 * glmnet_fit_model.predict(X)) + \\\n             (0.11 * gbr_fit_model.predict(X)) + \\\n             (0.10 * xgbr_fit_model.predict(X)) + \\\n             (0.09 * lgbr_fit_model.predict(X)) + \\\n             (0.09 * svr_fit_model.predict(X)) + \\\n             (0.08 * rf_fit_model.predict(X))\n            )","e3907aa3":"print('RMSE Score:')\nprint(base_rmse(y_test, blend_models(X_test)))\nblended_score = base_rmse(y_test, blend_models(X_test))","02379a60":"scorelist = {}\n\nscorelist['LM'] = lm_score\nscorelist['RF'] = rf_score\nscorelist['GBR'] = gbr_score\nscorelist['Lasso'] = lasso_cv_score\nscorelist['Ridge'] = ridge_cv_score\nscorelist['Elastic Net'] = glmnet_cv_score\nscorelist['Support Vector'] = svr_score\nscorelist['Light GBM'] = lgbmr_score\nscorelist['XGB'] = xgbr_score\nscorelist['Blended'] = blended_score\n\nbuild_time_list = {'Model': ['Ridge', 'LM', 'Elastic Net', 'Lasso', 'GBR', 'XGB', 'Blended',\n                             'Light GBM', 'Support Vector', 'RF', 'Stacking'],\n                   \n                   'Build Time': [ridge_cv_build_time, lm_build_time, glmnet_cv_build_time,\n                                  lasso_cv_build_time, gbr_build_time, lgbmr_build_time,\n                                  xgbr_build_time, stacking_build_time]\n                  }\n\nmodel_names = [i for i in scorelist.keys()]\nmodel_scores = [i for i in scorelist.values()]\n\nscorelist_df = pd.DataFrame({'Model': model_names,\n                             'Score': model_scores})\n\nscorelist_df.sort_values(by='Score', ascending=True)","b1e8fbb9":"# Create figure space\nsns.set_style(\"white\")\n\n# Create catplot\nax = sns.catplot(x='Model',\n                 y='Score',\n                 data=scorelist_df,\n                 kind='point',\n                 height=6) \n\n# Set plot features\nplt.xticks(rotation='45')\nplt.title('Final Model Scores', fontsize=15)\nplt.xlabel('Model', fontsize=12)\nplt.ylabel('Score (RMSE)', fontsize=12)\nplt.show()","1682e114":"# Create figure space\nsns.set_style(\"white\")\nplt.figure(figsize=(8,3))\n\n# Create catplot\nsns.pointplot(x='Model',\n                 y='Score',\n                 data=scorelist_df,\n                 height=6) \n\n# Set plot features\nplt.xticks(rotation='45')\nplt.title('Final Model Scores', fontsize=18)\nplt.xlabel('Model', fontsize=15)\nplt.ylabel('Score (RMSE)', fontsize=15)\nplt.show()\n","85d0f6f6":"import matplotlib.pyplot as pyplot\npyplot.bar(range(len(xgbr_fit_model.feature_importances_)), xgbr_fit_model.feature_importances_)\npyplot.show()","4f923c39":"plot_importance(xgbr_fit_model)\npyplot.show()","027d46e9":"headers = X_train.columns\nlen(headers)","bb158886":"# Create a new function to capture feature importance for free models (RF, GB, XGB)\ndef feature_importance(model):\n    \n    importance = pd.DataFrame({'Feature': headers,\n                               'Importance': np.round(model.feature_importances_,5)})\n    \n    importance = importance.sort_values(by='Importance', ascending=False).set_index('Feature')\n    \n    return importance","4bd4f073":"feature_importance(xgbr_fit_model)","67b3360e":"feature_importance(rf_fit_model)","4add5525":"# Store top 50 features from xgb\nxgb_top_features = list(feature_importance(xgbr_fit_model).iloc[0:50,].index)\n\n# Store top 50 features from rf\nrf_top_features = list(feature_importance(rf_fit_model).iloc[0:50,].index)","494e30da":"# Create unique feature list\n# (set(xgb_top_features) | set(rf_top_features))\nunique_top_features = sorted(np.unique(xgb_top_features+rf_top_features))\nlen(unique_top_features)","b2c2ee8c":"unique_top_features","089a36d5":"# Import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV # RandomizedSearchCV() GridSearchCV\n\ndef tune_hyperparameters(model, x, y):\n    global best_params, best_score\n    '''\n    1. Create a grid search on all of the hyperparameters and score them\n    2. After creating the grid with the hyperparameters, we need to fit the model with training data\n    3. After fitting, get the best parameters and score\n    '''\n    # Grid search 10-fold cross-validation through hyperparameters\n    grid = RandomizedSearchCV(model, verbose=0, cv=10, scoring='neg_mean_squared_error', n_jobs=-1) # optional n_jobs=-1 to use all cores\n    \n    # Fit the model using the grid\n    grid.fit(x, y)\n    \n    # Get best parameters and scores\n    best_params, best_score = grid.best_params_, np.round(grid.best_score_*100, 5)","7e70b2d3":"# Import StandardScaler() from the preprocessing module\nfrom sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\n\n# Transform the variables to be on the same scale\nX_train_new = scaler.fit_transform(X_train) # 1. fit_transform(training data)\nX_test_new = scaler.transform(X_test)       # 2. transform(testing data)\n\n# Build param list  - np.arange(1, 15, 0.5)\nr_alphas=[ 1 ,  1.5,  2 ,  2.5,  3 ,  3.5,  4 ,  4.5,  5 ,  5.5,  6 ,\n        6.5,  7,  7.5,  8 ,  8.5,  9 ,  9.5, 10 , 10.5, 11 , 11.5,\n       12 , 12.5, 13 , 13.5, 14 , 14.5]\n\nridge_cv_new = RidgeCV(alphas=r_alphas, cv=10)\nridge_cv_new.fit(X_train_new, y_train)\ncv_rmse(ridge_cv_new).mean()","e3e08271":"l_alphas = [0.0001, 0.0003, 0.0005, 0.0007, 0.0009, 0.0011, 0.001, 0.01]\n\nlasso_cv_new = LassoCV(alphas=l_alphas, cv=10, n_jobs=-1)\nlasso_cv_new.fit(X_train_new, y_train)\ncv_rmse(lasso_cv_new).mean()","7400597f":"lasso_cv_new","0b2e417e":"# import dill\n# dill.dump_session('09-house-regression-env.db')\n# dill.load_session('09-house-regression-env.db')","4c973988":"### Blended model score","7d5f8a46":"3. __Tuning__\n    - Diagnostics\n        - Review learning curves to understand whether the method is over or underfitting the problem. Review what the algorithm is predicting right and wrong\n        \n    - Intuition\n        - Hopefully you can develop an intuition on how to configure an algorithm on a problem\n    \n    - Steal from literature\n        - What parameter ranges are used in the literature? Evaluating the performance of standard parameters is a great place to start any tuning activity\n    \n    - Random search\n        - Perform a random search of algorithm hyperparameters to expose configurations that you would never think to try\n    \n    - Grid search\n        - Perform a grid search on set hyperparameters which is a more direct approach\n    \n    - Optimize\n        - There are parameters like structure or learning rate that can be tuned using a direct search procedure\n    \n    - Alternative implementations\n        - Use an alternate implementation of the method that can achieve better results on the same data\n    \n    - Algorithm customizations\n        - Perform modifications that you can make to the algorithm for your data from the loss function, internal optimization methods, and algorithm specific decisions","6d6eedb5":"### Month Sold feature","f21595f9":"Let's begin the analysis.","e821a222":"### How does the SalePrice distribution look like?\n\nLet's take a look at both the SalePrice feature & create a log(SalePrice) since that's what we are going to be measured by. First let's make a data frame to store the two features.","67568131":"Let's drop them if not, since should not be not too many. Even if the dataset is kind of small the goal right now is to predict sale price. We can do a safer approach without dropping if anything for practice.","d77ddd35":"# Scoring Method\n\nfrom sklearn.metrics import mean_squared_error\n\nAs stated in the beginning, submissions are evaluated on Root-Mean-Squared-Error (RMSE) so we will be using just that on the log SalePrice variable.\n\nCreate a function that will evaluate y_train\/y_test values with the y_pred predicted value. Using mean_squared_error method, we will build the function.","1f541249":"### Scatter relationship between YrBuilt & SalePrice\n\nNeed statsmodels","44b44f2c":"# Feature Engineering\n\nSo far we have collected some information on some features from living area to certain house attributes. With some familiarity of houses and the exploration of the data, we could create additional features that may positively affect the model prediction. This is where creativity can kick in even more and as the saying goes \"garbage in means garbage out\" so we obviously want the best features we can get.\n\nSome information we have so far:\n- The newer the home usually equates to a higher price\n- Sqft is a large factor\n- Chances are that homes are more likely to be sold from spring to summer\n- Quality of the home is good for the sale price\n- An excellent fireplace quality has a higher mean price\n    - This could also indicate that the house has nice qualities overall with other attributes that we may not have looked at\n    - Imagine going into a home and it's new with a nice fireplace in a large living room space","c573b17d":"catplot or factorplot are figure level functions. This means that they are supposed to work on the level of a figure and not on the level of axes.","8ca8b440":"Distribution with the zeroes show a right skew distribution of the data. This means that there are more remodels that are newer, which makes sense. Let's bin the remodel years actually. I would want to see how it the distribution looks like.","563a8c97":"### Define a new function called freq_table()","b001ce39":"The loess line looked extremely positive and linear so I decided to add in a perfect slope line (dotted) to see how close it is to the actual data. It's fairly close in terms of the positiveness and strength since most of the data hovers around the 1,000 to about 2,300 area and just about 80,000 to about 250,000 in price.","080bed22":"### FirePlace Quality vs. Sale Price\n\nI've done some plotting before in R with some of these features so I am replicating them in Python. Would you assume that a better quality fire place would mean higher sale price? Let's see the distribution.","b9b04599":"Looks like the YearBuilt feature is left skewed (tail on the left). \n\nWe can create another feature that will be the log of the value if it was not a date value. I would want to read more about logging every feature that is not normal for linear algorithms. We wouldn't need to if we were using a random forest for example since it can handle non-linearity but let's see how the log value looks anyways.","7d197209":"### Drop YrRemodel_Diff\n\nWe will use the group feature instead.","ae239e78":"Either there is a huge error on my data setup or big multicollinearity involved, a 0.13 error rate is amazing for a linear regression. Now we have a baseline to look at and compare more complicated models with.\n\nNow we create more models, will be using some of the hyperparameters set by others just to see how well it can perform and see what hyperparameters there are. Not using a lot of estimators for the tree algorithms so let's see how that goes as well to save time on computing.\n\nAlso going to just remove all of the hyperparameters for now except a few.\n\n    Time code: dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')","b468a49a":"As expected, the sale price will be a lot higher when the overall quality is large as well. Who would want to pay a hefty price for a low quality house?","8895245b":"### Plot","3b09aa3b":"Let's plot the numerical features now and see if there are multicollinearity within the dataset. Multicollinearity occurs when independent variables in a regression model are correlated. This correlation is a problem because independent variables should be independent (causing bias if left alone). If the degree of correlation between the variables is high enough, it can cause problems when you fit a model and interpret the results (skew).\n\nWe can take a look at a correlation plot to identify variables that fit this category or check if the variance inflation factor (VIF) > 10 (highly correlated).\n\nHow do we choose between which variable should we keep if they are correlated? We can apply some statistical tests to see which variable may be more beneficial\nfor the dependent variable. Some tests involve one-way t-tests (2 independent variables) and ANOVA (multiple variables), choosing the one with the highest R^2 value or a simple regression and check the values after.\n\nAnother way is to do some form of dimensionality reduction. Principal component analysis is a popular method to reduce dimensionality when there are a lot of features.\n\nOne way I have not tested yet is to combine both variables to create a new super variable that be beneficial for the model. Beware that combining may have same results.\n\nNow let's do some correlation analysis and see if there is multicollinearity involved.\n","d4ead65a":"### MSZoning vs Sale Price\n\nThis feature requires a little research in understanding what exactly do each term means. I found something that may be useful in deciphering what it is. It's codes are for apparently certain zones of property development. \n\nHere's an excerpt I found that was under \"Property Development Standards\": \n\n    \"The following schedule prescribes development standards for residential zoning districts and subdistricts designated on the zoning map.\"\n\nCodes:\n\n    A       = Agriculture\n    C (all) = Commercial\n    FV      = Floating Village Residential\n    I       = Industrial\n    RH      = Residential High Density\n    RL      = Residential Low Density\n    RP      = Residential Low Density Park\n    RM      = Residential Medium Density","7e8a66a8":"### Create a house age feature","1c253c2b":"### Fit new models after creating the initial cv models\n\nDefinitely could create a for loop for this redundant amount of code.","35209f2e":"### Blending models after fitting all of our models\n\nSo it looks like the steps from stacking to blending involves:\n1. Getting all of the models\n2. Identifying what our evaluation metric is\n3. Cross-validating the models and obtaining scores\n4. Fitting cross-validated models to the training data\n5. Blend the models with percentages per model\n6. Predict and score using the first evaluation function: base_rmse\n\nCV Scores w\/o Stacked:\n1. 0.110567 - Ridge - ridge_fit_model        \n1. 0.110848 - LASSO - lasso_fit_model  \n1. 0.111013 - ElasticNet - glmnet_fit_model\n1. 0.117633 - GBR - gbr_fit_model\n1. 0.119114 - XGB - xgbr_fit_model\n1. 0.121303 - Light GBM - lgbr_fit_model\n1. 0.125251 - SVR - svr_fit_model\n1. 0.133196 - RF - rf_fit_model\n\nWith 71\/325 features only\n1. 0.110259 - ridge\n1. 0.111701 - lasso\n1. 0.111709 - elastic\n1. 0.117422 - gbr\n1. 0.117913 - xgb\n1. 0.121756 - lgb\n1. 0.124944 - svr\n1. 0.131985 - rf\n\nWe have a total of 9 models, so we must assign weights that will equal to 1.","ce67ddfe":"# Exploratory Data Analysis\n\nThe difficulty here isn\u2019t coming up with ideas to start; it\u2019s coming up with ideas that are likely to turn into useful insight.\n\nWe will begin plotting and identifying features that may be relevant to our dependent variable. It wil be easier to play around with the features if we split it\nby data type: numerical\/integer and character\/factor.\n\nThis section is going to be particularly long as I am going to go through a good amount of features for practice.","f721cd13":"So there is a unimodal left skew on the SalePrice feature. The shape of the LogSalePrice feature looks approximately normally distributed, which is a good thing. It's a lot easier to work with especially with outliers.\n\n### Distribution Shapes\nA left-skewed distribution has a long left tail. Left-skewed distributions are also called negatively-skewed distributions, because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak which is less than the median.\n\nA right-skewed distribution has a long right tail. Right-skewed distributions are also called positive-skew distributions since there is a long tail in the positive direction on the number line. The mean is also to the right of the peak which is larger than the median.\n\n#### Here's an image below indicating how each skew would look like and the summary statistic values would lie.","9d02b221":"### Pandas Profiling\n\nThis is very handy. In a single line of code, it displays the data profiling report. Some of the information is pretty detailed. Check it out below. Beware, it does get pretty long.","d75a35a9":"### Store results of initial cv models","4038012a":"So far for this dataset which has been pretty clean since all of our log values made the data pretty normal for it to meet part of the linearity assumptions. Usually real-world data is not that easy to transform so we have to use methods to combat the non-linearity and use algorithms that can handle such issues.","583c638c":"I believe the great areas represent null values. Correlation plots don't work well if the data is complete so we need to complete the dataset before being able to view the full matrix. There are a few 0.80 numbers and up which means that the variables are highly correlated. We can attempt to not use them in the final models depending on what we are trying to achieve here.\n\nTop 3: OverallQual, GrLivArea, GarageCars\n\nNext 3: GarageArea, TotalBsmtSF, 1stFlrSF\n\nWe haven't plotted GarageCars but I would think it would be correlated by the garage area. A surprising one was the FullBath variable. But if you think about it, who woudn't want a full bath in their home. A full bath could mean a better lifestyle for convenience sake. YearBuilt and YearRemodAdd is pretty close too.\n\nWe can create a YrRemodel_Diff which is the difference between YearRemodAdd & YearBuilt. Newer homes do tend to make more money.","84a6fdb3":"Would have thought that the areas would have a stronger linear relationship between them but it does not look that obvious based on these scatters.","c6ec129a":"To meet the linear assumptions, we need this qq-plot to be closer to a diagonal shape. We see that it's curve at the tails which means it's not quite as linear as we want it to be. That's why we also have a transformed version of the SalePrice.","2490fd51":"### Ending notes\n\nStill getting the hang of finding out what functions to use for regression problems. Next notebook will be on regression modeling, breaking the steps the long way then creating wrapper functions to automate the process.","0144ae55":"### Get relevant data","fb0f469d":"This is kind of hard to see with all of the variables but anything that is close to yellow or dark blue indicates a decent positive or negative correlation.","e37d7e78":"# What's next?\n\nWe've taken a look at some features that may have a good predictive ability for SalePrice, but it's going to be a bit too time consuming plotting every single chart and dissecting every bivariate analysis per 2 features. So, let's take a use the correlation plot.","2280aac8":"### Cross-Validation Method\n\nWe want to make sure that we the model is not a one time fluke so we validate it through several iterations. I usually use a KFold method which splits the training data into 9 parts training and 1 part validation if the K is 10.\n\nSome types of cv methods: KFold, StratifiedShuffleSplit, StratifiedKFold or ShuffleSplit\n\nIn KFolds, each test set should not overlap, even with shuffle. With KFolds and shuffle, the data is shuffled once at the start, and then divided into the number of desired splits. The test data is always one of the splits, the train data is the rest.\n\nIn ShuffleSplit, the data is shuffled every time, and then split. This means the test sets may overlap between the splits.\n\nCheck the sklearn guide for more info.","159a3b87":"### Plot the new remodel feature","60b460f6":"Although the distribution looks pretty even throughout the year, there are a lot more outliers as we approach spring throughout summer. The outliers are all higher prices with none under the first quartile value. \n\nI can make some assumptions that prices are likely going to be higher in the summer or that people are more likely going to purchase in the summer\/spring time.","b8a0dff7":"### House Age Plot","7d6ccc64":"There are large outliers that completely skewed the loess line so we plot it without a few large data points to see the a better idea of the relationship between the two features. The removal is only for the plots and not the actual data as we don't know how it may affect the model due to overfitting to the training set once we model it or it can be a good thing, who knows?\n\nThis scatterplot shows a moderately strong, positive, linear (the second one kind of curves like a nonlinear) association between SqFt of the 1st floor and the SalePrice. There appears to be some outliers in the data.\n","07f82ad1":"### Super Learner  Algorithm - [h2o](https:\/\/h2o-release.s3.amazonaws.com\/h2o\/rel-ueno\/2\/docs-website\/h2o-docs\/data-science\/stacked-ensembles.html)\n\nThe Super Learner ensemble is distinguished by the use of cross-validation to form what is called the \u201clevel-one\u201d data, or the data that the metalearning or \u201ccombiner\u201d algorithm is trained on. \n\nAlgorithm Process\n1. Set up the ensemble\n    - Specify a list of base algorithms and model parameters\n    - Specify a metalearning algorithm\n2. Train the ensemble\n    - Train each of the base algorithms on the training set (fit)\n    - Perform k-fold cross-validation on each of the learners ^ and collect the cross-validated pdicted values from each algorithm (cv & predict --> new data)\n    - The N cross-validated predicted values from each of the L algorithms can be combined to form a new N x L matrix. This matrix along with the original response vector, is called the \"level-one\" data. (N = number of rows in the training set)\n    - Train the metalearning algorithm (ex. logistic) on the level-one data. The \"ensemble model\" consists of the L base learning models and the metalearning model, which can then be used to generate predicts on a test set. \n        - ensemble model = (xgb predicted results + rf predicted results + (xgb+rf) result from logistic (metalearner alg)) \n3. Predict on new data\n    - To generate ensemble predictions, first generate predictions from base learners\n    - Feed those predictions into the metalearner to generate the ensemble prediction\n    \n__Classification Example Process__\n1. Import h2o algorithm libraries\n1. Initialize h2o to use all cores\n1. Import data\n1. Split data into X and y\n1. Split data into train and test\n1. Set kfold cv number\n1. Assemble a list of models to stack together\n    1. __Train individual models and put them in a list (example will be a 2-model ensemble (gbm + rf)__\n        1. Initialize gbm algorithm and then train\n            1. gbm = H2OGradientBoostingEstimator(params)\n            2. gbm.train(x, y, train)\n            \n        2. Initialize rf algorithm and then train\n            1. rf = H2ORandomForestEstimator(params)\n            2. rf.train(x, y, train)\n            \n        3. Train a stacked ensemble using gbm and rf\n            1. ensemble = H2OStackedEnsembleEstimator(model_id='init_ensemble', base_models=[gbm.model_id, rf.model_id])\n            2. ensemble.train(x, y, train)\n            \n        4. Evaluate ensemble performance on the test data\n            1. ensemble_perform = ensemble.model_performance(test)\n            \n        5. Compare ensemble to base learner performance on the test set\n            1. gbm_perform = gbm.model_performance(test)\n            2. rf_perform = rf.model_performance(test)\n            3. baselearner_best_auc = max(gbm_perform.auc(), rf_perform.auc())\n            4. stack_auc = ensemble_perform.auc()\n            5. print('Best Base-learner AUC: {0}'.format(base_learner_best_auc)\n            6. print('Ensemble AUC: {0}'.format(stack_auc)\n            \n        6. Generate predictions\n            1. ensemble_pred = ensemble.predict(test)\n            \n    1. __Train a grid of models (Generate a random grid of models and stack them together)__\n        1. Specifiy gbm hyperparameters for the grid\n            1. gbm_grid = H2OGridSearch()\n            2. gbm_grid.train(x, y, train)\n            \n        2. Specify rf hyperparameters for the grid\n            1. rf_grid = RFGridSearch()\n            2. rf_grid.train(x, y, train)\n            \n    1. __Train several grids of models__\n        1. Train a stacked ensemble using gbm and rf\n            1. ensemble_grid = H2OStackedEnsembleEstimator(model_id='ensemble_grid', base_models=[gbm_grid.model_ids, rf_grid.model_ids])\n            2. ensemble_grid.train(x, y, train)\n            \n    1. __Ensemble Grid Performance__\n        1. Evaluate ensemble performance on the test data\n            1. grid_stack_perf = ensemble_grid.model_performance(test)\n        \n    1. __Compare ensemble grid to base learner on test set__\n        1. Get base learner score\n            1. baselearner_best_auc = max([h2o.get_model(model).model_performance(test).auc() for model in grid.model_ids])\n\n        2. Get ensemble grid score\n            1. grid_stack_auc = grid_stack_perf.auc()\n    \n    1. Result\n        1. print('Best Base-learner AUC: {}'.format(base_learner_best_auc)\n        2. print('Final Ensemble AUC: {}'.format(grid_stack_auc)\n        \n    1. Predict\n        1. grid_stack_predict = ensemble_grid(test)\n        \n    1. __Note: All base models must have the same cv fold and cv predicted values must be stored__\n","784d0ddc":"### Log transform GrLivArea","60823b30":"### Are there any null values? Show only column names with a null data value","99666718":"Without the jitter plot, we wouldn't know just by plotting a boxplot that the distribution count from ConLD all the way to the right values do not have a large type count. We can try to group them together if they are similar or group them into the larger samples.","27100ecc":"### Create a new remodel diff group feature then drop it if it does not look useful","37f4e631":"### cannot pickle Dict key error\nIf you're using python3 , add list() to category_index.values() in model_lib.py about line 381 as this list(category_index.values()).","db12d67e":"There are a total of 80 columns and 1460 rows of data in the training set.","8e1d4151":"The coefficient of determination provides a 'goodness of fit' measure for the predictions to the observations. This is a value between 0 and 1. It's the proportion of the variance in the dependent variable (y) that is predictable from the independent variable (x).\n\nThere looks like ba a handful of numerical features that are highly correlated to the LogSalePrice feature. YrSold has a 99% explanation for the dependent variable. This could be an example of multicollinearity. ","0a2d7004":"### Feature importance","e4382e05":"### Probability (QQ Plot)","4cb41e99":"Drop Id column since that is not going to be useful in modeling.","ac8c6f94":"### Save notebook","e34675d6":"### Framing the problem\n\nPredict the sales prices of homes in Ames, IA.\n\n#### Competition Description\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n    \n### Method\n\nSince we are predicting price (numerical), this will be a regression problem so we will focus on algorithms like linear regression, random forest, boosting algorithms (gradient, light, extreme gradient), regularized linear algorithms (ridge, lasso, elastic-net), etc. We can attempt to try methods that will use meta-learners called stacking or blending.\n- Evaluation metric used will be Root-Mean-Squared-Error (RMSE) between the SalePrice feature and the actual SalePrice on a log scale to enture that errors predicting expensive houses and cheap houses will affect our score equally","f01c0a9e":"Now that we have our models, cross-validation method, and scoring method ready, we can start to create some baseline models and see how it performs without any tuning.\n\n### Process\nThe steps usually follows like this:\n1. Import algorithms\n2. Fit the model\n3. Predict\n4. Score\n5. If we are using multiple models then we want to store all of the stores in a dataframe and sort by the best\n\nAfter that we can make it a little less redundant by using make_pipeline() which will cut some of those steps out.","c5d2d599":"Curious to see how would a random search perform using ridge regression.","a90d3460":"### Kurtosis\nDoc Def: Kurtosis is the fourth central moment divided by the square of the variance. If Fisher\u2019s definition is used, then 3.0 is subtracted from the result to give 0.0 for a normal distribution.\n\nIf bias is False then the kurtosis is calculated using k statistics to eliminate bias coming from biased moment estimators","5e353c5c":"### Feature Descriptions\nHere's a brief version from the website.\n\n- SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n- MSSubClass: The building class\n- MSZoning: The general zoning classification\n- LotFrontage: Linear feet of street connected to property\n- LotArea: Lot size in square feet\n- Street: Type of road access\n- Alley: Type of alley access\n- LotShape: General shape of property\n- LandContour: Flatness of the property\n- Utilities: Type of utilities available\n- LotConfig: Lot configuration\n- LandSlope: Slope of property\n- Neighborhood: Physical locations within Ames city limits\n- Condition1: Proximity to main road or railroad\n- Condition2: Proximity to main road or railroad (if a second is present)\n- BldgType: Type of dwelling\n- HouseStyle: Style of dwelling\n- OverallQual: Overall material and finish quality\n- OverallCond: Overall condition rating\n- YearBuilt: Original construction date\n- YearRemodAdd: Remodel date\n- RoofStyle: Type of roof\n- RoofMatl: Roof material\n- Exterior1st: Exterior covering on house\n- Exterior2nd: Exterior covering on house (if more than one material)\n- MasVnrType: Masonry veneer type\n- MasVnrArea: Masonry veneer area in square feet\n- ExterQual: Exterior material quality\n- ExterCond: Present condition of the material on the exterior\n- Foundation: Type of foundation\n- BsmtQual: Height of the basement\n- BsmtCond: General condition of the basement\n- BsmtExposure: Walkout or garden level basement walls\n- BsmtFinType1: Quality of basement finished area\n- BsmtFinSF1: Type 1 finished square feet\n- BsmtFinType2: Quality of second finished area (if present)\n- BsmtFinSF2: Type 2 finished square feet\n- BsmtUnfSF: Unfinished square feet of basement area\n- TotalBsmtSF: Total square feet of basement area\n- Heating: Type of heating\n- HeatingQC: Heating quality and condition\n- CentralAir: Central air conditioning\n- Electrical: Electrical system\n- 1stFlrSF: First Floor square feet\n- 2ndFlrSF: Second floor square feet\n- LowQualFinSF: Low quality finished square feet (all floors)\n- GrLivArea: Above grade (ground) living area square feet\n- BsmtFullBath: Basement full bathrooms\n- BsmtHalfBath: Basement half bathrooms\n- FullBath: Full bathrooms above grade\n- HalfBath: Half baths above grade\n- Bedroom: Number of bedrooms above basement level\n- Kitchen: Number of kitchens\n- KitchenQual: Kitchen quality\n- TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n- Functional: Home functionality rating\n- Fireplaces: Number of fireplaces\n- FireplaceQu: Fireplace quality\n- GarageType: Garage location\n- GarageYrBlt: Year garage was built\n- GarageFinish: Interior finish of the garage\n- GarageCars: Size of garage in car capacity\n- GarageArea: Size of garage in square feet\n- GarageQual: Garage quality\n- GarageCond: Garage condition\n- PavedDrive: Paved driveway\n- WoodDeckSF: Wood deck area in square feet\n- OpenPorchSF: Open porch area in square feet\n- EnclosedPorch: Enclosed porch area in square feet\n- 3SsnPorch: Three season porch area in square feet\n- ScreenPorch: Screen porch area in square feet\n- PoolArea: Pool area in square feet\n- PoolQC: Pool quality\n- Fence: Fence quality\n- MiscFeature: Miscellaneous feature not covered in other categories\n- MiscVal: $Value of miscellaneous feature\n- MoSold: Month Sold\n- YrSold: Year Sold\n- SaleType: Type of sale\n- SaleCondition: Condition of sale\n","7db23d07":"### Lot Area vs. Sale Price","28d9bfcf":"### Sale Type vs Sale Price Jitter & BoxPlot","e6de8ee5":"### What is make_pipeline()?\n\nStack Answer - \n\nPipeline is a combination of several functions into one, which makes cleaner code and less steps to take. So similar to how you make transform your train set and fit to predict, pipeline performs the entire sequence from different transformations (finding set of features, generating new features, and feature selection) of the data before applying the final estimator.\n\nPipeline gives you a single interface for all 3 steps of transformation and resulting estimator and also encapsulates transformers and predictors inside.\n\nWith pipelines you can easily perform a grid-search over a set of params for each step of the meta-estimator.\n\nOld:\n\n    vect = CountVectorizer()\n    tfidf = TfidfTransformer()\n    clf = SGDClassifier()\n\n    vX = vect.fit_transform(Xtrain)\n    tfidfX = tfidf.fit_transform(vX)\n    predicted = clf.fit_predict(tfidfX)\n\n    # Now evaluate all steps on test set\n    vX = vect.fit_transform(Xtest)\n    tfidfX = tfidf.fit_transform(vX)\n    predicted = clf.fit_predict(tfidfX)\n    \nNew:\n\n    pipeline = Pipeline([\n        ('vect', CountVectorizer()),\n        ('tfidf', TfidfTransformer()),\n        ('clf', SGDClassifier()),\n    ])\n    \n    predicted = pipeline.fit(Xtrain).predict(Xtrain)\n    predicted = pipeline.predict(Xtest)","63e71064":"### Create a new feature called YrRemodel_Diff","3c5d4fcf":"Would an \"excellent\" type have a higher sales price? Would it also be correlated to the size as well?","75cadff8":"### Create a function to plot histogram and probability plot (hist_qq_plot())","795e1db1":"## Results\n\nI ended up with around 300 features after normalizing them and transforming to dummy variables. Here were my results from training the models. The best performing one was a ridge cross-validated model. I didn't tune the hyperparameters yet at my current stage. I would want to somehow get my blended model to be better, at least to 2nd or 1st. \"Garbage in, Garbage Out\" holds true indeed.\n\n__CV RMSE Scores:__\n1. 0.110567 - Ridge - ridge_fit_model\n1. 0.110848 - LASSO - lasso_fit_model\n1. 0.111013 - ElasticNet - glmnet_fit_model\n1. 0.117633 - GBR - gbr_fit_model\n1. 0.119114 - XGB - xgbr_fit_model\n1. 0.121303 - Light GBM - lgbr_fit_model\n1. 0.125251 - SVR - svr_fit_model\n1. 0.133196 - RF - rf_fit_model\n1. 0.116983 - Stacked\n\nSecond try - With 71 features only and included LM which was the a top model... sooo\n1. 0.110259 - Ridge \t\n1. 0.110830 - LM \t\n1. 0.111701 - ElasticNet \t\n1. 0.111709 - LASSO \t\n1. 0.117422 - GBR \t\n1. 0.117913 - XGB \t\n1. 0.119340 - Blended\n1. 0.121756 - Light GBM \t\n1. 0.124944 - Support Vector \t\n1. 0.131985 - RF \t","ea0590d3":"### Normalizing the test with train\nI did not combine my train and test data to mimic real world applications, so I would need to make sure all of my features that are in my train set are also in my test set including and transformations I did. In R, depending on how the data was extracted to cleaning, normalized the numerical train features and saved them then applied them to the test set. So far the kernels I have seen so far combined the data set, which is not what I'm aiming for. So more researching here I come.","3158dd7c":"# Preprocessing \/ Cleaning\nAre we able to use the data right away? Are there any null values? Is the data clean?","068a7faa":"### Sample heatmap viz - changing a particular value","86845066":"### Log transform the feature","18760265":"### Plot SalePrice below first then find outliers to drop","de8e6506":"### Use pointplot to adjust ","be48db77":"Split the training data for cross-validation\n\nSplit the train data into 4 parts, train_x, test_x, train_y, test_y\n\n    train_x and train_y first used to train the algorithm.\n    then, test_x is used in that trained algorithms to predict outcomes.\n    Once we get the outcomes, we compare it with test_x\n","1fb880a1":"Well this feature's distribution is not good at all but we do see that there is a large amount of remodels set at 0 which means there were none.","782a10d9":"### TotalBsmtSF vs. SalePrice\n\nThere is a single outlier that is over 6000 in total basement sqft so I am going to plot it without it.","f3231b88":"## Single model testing and validation\n","b2228b5b":"Can't really see anything from this.","ebbb992b":"### Plot the nulls","a9f28775":"### 1stFlrSF feature\n\nWe can start looking at the 1st floor ft^2 feature to see if the larger the sqft value the larger the SalePrice as well. We can combine all of the features that involves area later on to see if it's useful, but for now we will do some individual analyses and plot it out.","0cae648f":"# Post Review","59a36848":"### Define random search tuning function","6fc27081":"Definitely looks a lot better but there are still some outliers. We will leave them for now as there are always homes that may be bigger than others.","432eb657":"With the outlier data shown in the boxplots earlier combined with these count and density plots, we can confirm a little more on the assumptions that homes are more often bought starting from spring throughout the summer until autumn where it drops significantly. School time also begins in August for semester systems and September for quarter systems. This could be good to pair up with family size data including ages of each family member and any other demographic data.","fd7f4740":"This looks a lot better and we can see that it's extremely close alongside the red line. We can also do this with other variables as well.","75c9aef9":"Variables look fairly correlated. Let's move on to the next step.","1867f9b6":"### What is stacking \/ superlearner \/ metalearner? - [Info](http:\/\/blog.kaggle.com\/2017\/06\/15\/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova\/)\n\nStacking or Stacked Generalization is the process of combining various machine learning algorithms using holdout data. It's an ensemble algorthm where a new model is trained to combine the predictions from two or more models already trained.\n\nThe predictions from the existing models are combined using a new model. Stacking is often referred to as blending as well since the predictions from the sub-models are blended togther.\n\nWe could use a linear method to combine the predictions for the sub-models (meta-learners) such as averaging, voting, to a weighted sum using linear\/logistic regression.\n\nIt is important that sub-models produce different predictions, so-called uncorrelated predictions. Stacking works best when the predictions that are combined are all accurate in different methods. This may be achieved by using algorithms that use very different internal representations (trees compared to instances) and\/or models trained on different representations or projections of the training data.\n\nAdditionally, the advent of increased computing power and parallelism has made it possible to run many algorithms together. Most algorithms rely on certain parameters or assumptions to perform best, hence each one has advantages and disadvantages. Stacking is a mechanism that tries to leverage the benefits of each algorithm while disregarding (to some extent) or correcting for their disadvantages. In its most abstract form, stacking can be seen as a mechanism that corrects the errors of your algorithms.\n\n__Summary__\n\nStacking is a class of algorithms that involves training a second-level \u201cmetalearner\u201d to find the optimal combination of the base learners (weaker learners). Unlike bagging and boosting, the goal in stacking is to ensemble strong, diverse sets of learners together. [Multiple Weak Sets --> 1 Strong Final Set]\n\n__Process Ex__ \n\n1. Train various machine learning algorithms (regressors or classifiers) in dataset A\n1. Make predictions for each one of the algorithms for datasets B and C and we create new datasets B1 and C1 that contain only these predictions. So if we ran 10 models then B1 and C1 have 10 columns each.\n1. Train a new machine learning algorithm (often referred to as metalearner or superlearner) using B1\n1. Make predictions using the meta-learner on C1\n\n__Diff Process Ex__\n1. It first trains a list of models (kNN and Perceptron).\n1. It then uses the models to make predictions and create a new stacked dataset.\n1. It then trains an aggregator model (logistic regression) on the stacked dataset.\n1. It then uses the sub-models and aggregator model to make predictions on the test dataset.\n\n__Additional Links__\n- [MLM](https:\/\/machinelearningmastery.com\/implementing-stacking-scratch-python\/) - Shows code example\n","5a2acb47":"YrSold has the highest correlation based off of the pearson's correlation which is the standard correlation coefficient. The second is YrBuilt, which makes sense when the year built is closely correlated to the year sold.\n\n- In statistics, the correlation coefficient r measures the strength and direction of a linear relationship. don't get this mixed upwith coefficient of determination which is the r^2 which shows percentage of variation in y which is explained by the x variables.\n    - when Pearson's r is close to 1 that means there is a strong relationship between the two variables\n    - when Pearson's r is close to 0 that means there is a weak relationship between the two variables","9da95b8e":"As you can see, I am definitely going to drop some of these just because they are extreme points that are way different than the distribution. Would I do this on a real dataset I am working on for production? That's a maybe, I would attempt to see how the model would perform with the outlier, create the model with some robust scaler, or see initial performance on both methods then model again without.","8a185baf":"We can see the mean\/median values of prices going up as the year built is more recent compared to the past. This is useful as it gives us an idea that more modern homes are going to be more costly due to many home factors that we are going to plot out like square footage.","a6ba429d":"All the features seem to have some slight positive correlation to the sales price as the values move towards the right of the x-axis. The newer the year the house was built to the GrLiv\/Area feature increases the sale price.","cc0f86a9":"### Lot Frontage vs Sale Price\n\nLinear feet of street connected to property","291b9d45":"### Create a new feature called TotalSF","89ebfbb5":"### Check Skew","edea138c":"# Import libraries & data","5e1e1fc6":"### Start with linear regression\n\nThis will give us a rough idea on the error size.","16bf2ca3":"### Which transformation should we use on the features?\n\nMy go-to transformation is normally a boxcox transformation, but it does not handle zero values. There is another transformation that's [boxcox1p](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.special.boxcox1p.html). All it really does is add a 1 to the value so no values would be zero, which is what we did earlier to do a log transformtion for SalePrice.\n\nA Box Cox transformation is a way to transform non-normal dependent variables into a normal shape. This helps with any outliers and helps us meet the normality assumption for many statistical techniques. Remember that the test only works for positive data","cd1da7e0":"### Ways to clean up null values\nThere are several methods on how to handle missing values. There are always some instances where a particular element is absent. It could be due to corrupt data, failure to load the information, never inputted in the first place, or other invalid methods of data collection. It's extremely helpful to know a few methods on how to handle the NULL\/NaN values. Pandas library also provides a dropna() function that can be used to drop other columns or rows of missing data.\n\nSome methods of dealing with NA values below:\n1. Removing the rows with missing values using dropna() function\n    - Use the inplace parameter: if True, do operation inplace and return None\n    - Use the axis parameter: drop labels from the index (if 0) or columns (if 1)\n2. Impute missing values using fillna()\n    - Constant value that has meaning within the domain; ex: 0 for not present or 'Missing' for categorical\n    - Value from another randomly selected record (could be random or have some business insight to add a new data point)\n    - Mean, median, mode\n    - Value estimated by a predictive model (random forest)\n3. Binning\n    - Binning the vfalues that are not present which could provide insight\n4. Ignore and leave it as it is\n    - Sometimes it's useful to know that there is a missing value","fec7aa55":"Not sure how to make the facet grid larger.... but from this tiny chart there are more houses with air conditioning sold during the summer months and we can see that the air conditioning homes (purple points) have a higher spread and sale price than the ones without. This is definitely a useful determinant.","1898edd7":"### Correlation to LogSalePrice","a6fd6ec5":"### Scatter plot for Year Sold vs. Year Built","0fd14add":"### Lasso CV Score: .10942","a81f0f9b":"### BsmtQual feature\n\nSince there is some sort of linear relationship between price and the basement size, we can take a look at the height as well. This can be separated into two plots where we take a look into account of the height and the size with the quality and price.\n\nDescription of each category:\n- Ex - Excellent (100+ inches)\n- Gd - Good (90-99 inches)\n- TA - Typical (80-89 inches)\n- Fa - Fair (70-79 inches)\n- Po - Poor (<70 inches)\n\nBut first, let's create a function called freq_table() that will plot frequency of the labels and a quick bar plot. This was originally used in the Titanic data analysis.\n","73cae813":"### Some useful functions and examples:\n\nUsing map() to transform\n\n- level_map = {1: 'high', 2: 'medium', 3: 'low'}\n\n- df['c_level'] = df['c'].map(level_map)\n    \nUsing value_count()\n\n- df['c'].value_counts()\n\n- There\u2019re some useful tricks \/ arguments of it:\n    1. normalize = True: if you want to check the frequency instead of counts.\n    2. dropna = False: if you also want to include missing values in the stats.\n    3. df['c'].value_counts().reset_index(): if you want to convert the stats table into a pandas dataframe and manipulate it\n    4. df['c'].value_counts().reset_index().sort_values(by='index') : show the stats sorted by distinct values in column \u2018c\u2019 instead of counts.\n\n- df['c'].value_counts().sort_index()) # Using without .reset_index()\n\nSelecting rows with certain IDs\n\n- df_filter = df['ID'].isin(['A001','C022',...])\n- df[df_filter]","d01bed79":"# Modeling\n\nSo now we have some visual analyses done, feature engineered some new data, and cleaned up the data, we will move onto the modeling stage. This includes making sure we have our train and test dataset split up all the way to comparing tuned models. Some steps are listed below:\n\nModeling & Scoring\n    - Dropping any features we will not be using in training\n    - Standardizing data\n    - Splitting the data into train\/test\n    - What models are appropriate?\n        - Regression\n    - Pre-Tuning\n    - Cross-Validating\n    - Hyperarameter Tuning\n    - Tuned Models Using Best Hyperparameters\n    - Comparison: Pre-Tuning vs. CV vs. Tuned Model vs. CV Tuned","9647f014":"### Let's fix some of the NA features\n\nI would normally combine the train and test datasets, but to follow the modeling process, I will leave the test set out of this and just repeat any additional transformations after (not including the newly created features).","5aa03021":"### How to improve our machine learning performance?","200dd2a1":"This is a lot easier to visualize and digest. Homes remodeled with the last 10 years have a higher sale price. Maybe homes are more decked out in modern style remodels than the past which could be a reason in the price hike. I think this feature may be better than the actual quantitative feature. We may lose some informtion but it's worth a shot.","66a3842c":"Basement sqft plot looks very similar to the 1st floor sqft plot from earlier. We can assume that the higher the sqft the higher the sale price. We can confirm this assumption as we continue to visualize more of the features and do some statistical analysis on the features (uni, bi, multi).\n\nThere are also 0 sqft for basements for houses without. We can maybe engineer a new feature to flag homes without a basement?\n\nExtra analysis could include looking at the outliers over 3,000 sqft points. A question I would have is what type of home is the data point that is right under 200,000 sale price but has such a high basement sqft? I'll also check the 700,000 sale price homes as well. Checking these outliers also help see if these points are also the same outliers with other features that is plotted. Is there a correlation? If so, why how is it correlated and should we include the data points in the model?","501bd7e5":"### Plot YrRemodel_Diff vs. Sale Price","5939e653":"Looks like 5 features have 45% of NA data or more. We can try to explore the features to see if it can be useful if it's possible. PoolQC may be removeds since there the entire column has NA data.","40bd4b97":"Let's take a look visually now that we have all values in the feature populated. We can use matplotlib and its functions to:\n1. Create a figure space\n2. Plot the data\n3. Update any figure settings\n\n**Before modeling, create plots togther in 1 figure space that will pair up the train vs test distributions to see if distributions vary a lot later.**","28b1350d":"2. __Algorithms__ - machine learning is all about algorithms. Identify algorithms and data representations that perform above a baseline of performance and better than average. Remain skeptical of results and design experiments that make it hard to fool yourself.\n\n    - Resampling method\n        - Use a method and configuration that makes the best use of available data. The k-fold cross-validation method with a hold out validation dataset is popular\n        \n    - Evaluation metric\n        - Use best evaluation metric to make sure your models are performing well to the goal\n        \n    - Baseline performance\n        - Use an easy, random, or a zero rule algorithm (mean\/mode) to establish a baseline by which to rank all evaluated algorithms so we know if we are improving or not\n        \n    - Spot check linear algorithms\n        - Linear methods are often more biased and are easy to understand, also pretty fast\n        \n    - Spot check nonlinear algorithms\n        - Nonlinear algorithms often require more data, have greater complexity but can achieve better results\n        \n    - Steal from literature\n        - Get ideas of algorithm types or extensions of classical methods to explore on your problem\n        \n    - Standard configurations\n        - What is considered a standard configuration for your model before tuning","851e292a":"### Updating feature types\n\nThere are some features like MoSold and YrSold that I believe should be categorical so we should update the type on that.","e28af57c":"There are a few yellow or dark blue squares that indicate high correlation (positive or negative). We can actually use the nlargest method to get the top correlated features by setting the k value.","e3a38c52":"### Skewness\nDoc Def: For normally distributed data, the skewness should be about 0. For unimodal continuous distributions, a skewness value > 0 means that there is more weight in the right tail of the distribution. The function skewtest can be used to determine if the skewness value is close enough to 0, statistically speaking.","d42b0a10":"\n1. __Data Methods__\n\n    - Get more quality data\n        - The better the data that goes in your model, the better the result\n        \n    - Generate more data\n        - Augment or permute existing data or use a probabilitic model to generate new data\n        \n    - Clean your data\n        - Improve the signal in your data. Fix any missing or corrupt observations or remove them. Outliers values outside of reasonable ranges can be fixed or removed as well in order to lift the quality of your data\n        \n    - Resample data\n        - Resample data to change the size or distribution or use a smaller sample of data for your experiments to speed things up or under-sample or over-sample specific types to better represent your data\n        \n    - Reframe your problem\n        - Change the type of prediction problem you are solving\n        \n    - Rescale your data\n        - Rescaling numeric input variables. Normalization and standardization of input data can result in a lift in performance on algorithms that use weighted inputs or distance measures\n        \n    - Transform your data\n        - Reshape your data distribution. Making input data more gaussian or passing it through an exponential function may better expose features in the data to a learning algorithm\n        \n    - Feature selection\n        - Are all input variables equally important? Use FS and feature importance methods to create new views of your data to explore with modeling algorithms\n    \n    - Feature engineering\n        - Create and add new data features. Perhaps there are attributes that can be decomposed into multiple new values (like categories, dates, or strings) or attributes that can be aggregated to signify an event (count, binary flag, or statistical summary)\n    \n    - Lower the dimensionality of your data\n        - Project your data into a lower dimensional space. Use an unsupervised clustering or project method to create an entirely new compressed representation of your dataset","3af3e521":"### Dummy Variables\n\nDummy variables is an important pre-processing step. It's usually used when important features are not numerical so we have to transform them into binary values 0 or 1. Many models would prefer or need numerical features to be used in order for it to work properly. While working with a dataset, we want to make sure the computer is able to understand the differences. Computer processing power is also a lot better with straight numbers, vectors, or anything numer related vs strings\/text\/categorical.\n\nTo transform variables in Python, we must use pandas.get_dummies() function. In R, it's similar to model_matrix(~y). This will produce sparse columns which will grow horizontally and can get exponentially more difficult as we have to search through more dimensions.","7a44c36c":"### Models\n\nOur goal is to make some sort of stacking and blending model so we have to first instantiate the base models.","70ef35fe":"# Introduction\n\nPredict the sales prices of homes in Ames, Iowa. \n\nFamiliarize myself with scripting in Python data analysis and classical machine learning.\n\n*Note: Codes may also come from other kernels for practice\/workflow","4867fead":"### YearBuilt feature","f24c0c68":"# Modeling Framework\n\n1. Framing the problem\n    - What are we trying to solve?\n    - Understand the problem and ask questions\n2. Collecting Relevant Information & Data\n    - What type of data do we have?\n    - What other requirements are there?\n    - What is considered a success for this problem?\n3. Process for analysis (Preprocessing & Cleaning)\n    - How does the data structure look like?\n    - Is the data usable?\n    - Can the data be plotted?\n    - What changes do we need to do to make the data usable?\n    - What type of predictive problem are we trying to answer?\n        - Models we can use\n        - What are the model inputs? \n            - Figure out the data inputs needed for the model to work.\n    - Check for common errors like missing values, corrupted values, dates\n4. Explore the data (Exploratory Data Analysis)\n    - How does the data look like?\n    - Are there any patterns?\n        - Identify using summary statistics, plotting, counting, etc.\n    - Familiarizing process\n5. Feature Engineering (Applied Machine Learning)\n    - Can we create more features that will be helpful to the model?\n6. Statistical Analysis\n    - Univariate, bivariate, multivariate analysis of features\n        - Analysis of a single feature\n        - Analysis of two features together\n        - Analysis of more than two features together\n7. Modeling & Scoring\n    - Splitting the data into train\/test\n    - Standardizing data\n    - What models are appropriate?\n        - Regression\n    - Pre-Tuning\n    - Cross-Validating\n    - Hyperarameter Tuning\n    - Tuned Models Using Best Hyperparameters\n    - Comparison: Pre-Tuning vs. CV vs. Tuned Model vs. CV Tuned \n8. Evaluation\n    - How accurate are the models?\n    - What evaluation metric is appropriate?\n    - Is the model good enough?\n    - Iteration needed?\n9. Post Review\n    - Notes on models, ways to improve model performance","5af0f494":"### Coefficient of Determination (R-Squared)","d0c2d880":"### What is a linear regression?\n\nA linear regression is type of model that looks at the relationship between a dependent variable and one or more independent variable(s). The overall idea on how it's used is to see help answer the question: do the predictor variables do a good job in predicting the outcome? The simpleest form of the linear equation is shown below. Our dependent variable (SalePrice) is being predicted by the linear component (YearBuilt + YearBuilt coefficient + constant) and an error component (random, could not be modeled).\n\nAssumptions for the regression to work\n- Linear: Needs a linear relationship between the dependent and independent variables\n    - Check by plotting X to y\n- Normality of Residuals: Requires error term to be normally distributed\n    - Check by qq-plot\n- Homoscedasticity: Assumes that residuals are approximately equal for all predicted dependent variable values (variance of residual is the same for any value of X)\n    - Check by plotting fitted values & residuals\n- Multicollinearity: Variance Inflation Factor (VIF) \u2013 independent variables must not be highly correlated with each other\n    - Check by correlation matrix or statistical tests\n- Independence: Observations are independent of each other\n    - Check by correlation matrix","c7f95c2c":"### Feature scaling\n\nThis is an important concept and it has several methods to achieve this. Since numbers are not in the same scale and can have large effects where the model will read a higher number as better when it is actually not... and more issues. So we need to do feature scaling to get a better result.\n\n**Types of Scalers**\n- **MinMaxScaler** - Scales the data using the max and min values so that it fits between 0 and 1.\n- **StandardScaler** - Scales the data so that it has mean 0 and variance of 1.\n- **RobustScaler** - Scales the data similary to Standard Scaler, but makes use of the median and scales using the interquertile range so as to aviod issues with large outliers.\n\nCall the RobustScaler() function\n1. Use fit_transform() to scale the data\n2. Use transform() on all other datasets that will be used","3986834f":"### Dropping columns\n\nWe created a few features off of the current train data so there is certainly multicollinearity so we can remove some features first before we split our data. But first, as curiosity hits, let's leave it initially and I assume it will have a better score on the training data.","60e1f578":"### GrLivArea feature","e0746976":"### Create a heatmap for the dataset","d6309cb5":"Improve performance with:\n1. Data\n2. Algorithms\n3. Algorithm Tuning\n4. Ensembles    \n\n__Overview__\n\nQuality data -> Well-performing algorithms and data representations -> Tuned models -> Ensembles of Tuned models","be90b2a4":"### Year Sold feature","3bb165f0":"Here I plotted the data and a linear regression model fit since it's always good to see how well our independent variables are correlated with our dependent variable.","ece43d0f":"### Split the variables between X and y","2d98ca63":"Probably won't, I've already dropped anything over 30,000 in lot area. I need as much data as I can at this point, if the feature is bad in feature importance then I might reconsider.","56750b45":"### Log transform TotalBsmtSF\n\nlog 0 is undefined. The result is not a real number, because you can never get zero by raising anything to the power of anything else. You can never reach zero, you can only approach it using an infinitely large and negative power. So the base b logarithm of zero is not defined. So we have to make sure if we transform the variable, we make sure everything is greater than 0.","bbd9b2be":"4. __Ensembles__ - combine the predictions from multiple models. After algorithm tuning, this is the next big area for improvement. You can get good performance from combining the predictions from multiple models rather than from multiple highly tuned models\n\n    - Blend model predictions\n        - Combine predictions from multiple models directly. You could use the same or different algorithms to make multiple models. Take the mean or mode from the predictions of multiple well-performing models\n        \n    - Blend data representations\n        - Combine predictions from models trained on different data representations\n        \n    - Blend data samples\n        - Combine models trained on different views of your data. You can create multiple subsamples of your training data and train a well-performing algorithm, then combine predictions. This is called bootstrap aggregation or bagging and works best when the predictions from each model are skillful but in different uncorrelated ways \n        \n    - Correct predictions\n        - Correct the predictions of well-performing models. You can explicitly correct predictions or use a method like boosting to learn how to correct prediction errors\n        \n    - Learn to combine\n        - Use a new model to learn how to best combine the predictions from multiple well-performing models. This is called stacked generalization or stacking and often works well when the submodels are skillful but in different ways and the aggregator model is a simple linear weighting of the predictions. The process could be repeated mutiple layers deep.","36cb2e1a":"### Using probplot()","968e119d":"### OverallQual feature"}}