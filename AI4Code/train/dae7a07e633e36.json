{"cell_type":{"5b361687":"code","81ec4a38":"code","92a66db1":"code","e015e6c1":"code","312d8206":"code","a0e5d2c1":"code","94c37456":"code","191f4ce5":"code","55f1037f":"code","24c3ef76":"code","7964d198":"code","54acb417":"code","474243fb":"code","4a4a1d38":"code","15233714":"markdown","16ed0ff1":"markdown","54d63604":"markdown","26406806":"markdown","53ee4430":"markdown","2f7b8593":"markdown","5e8b6e02":"markdown","aea05ac0":"markdown","2809a96d":"markdown","af5f59a9":"markdown","455754f6":"markdown","98213517":"markdown","72beb5e4":"markdown","0f1ff1d9":"markdown"},"source":{"5b361687":"import numpy as np\nimport os\nimport torch\n\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torchvision.models.resnet import resnet18\nfrom tqdm import tqdm\nfrom typing import Dict\n\nfrom l5kit.data import LocalDataManager, ChunkedDataset\nfrom l5kit.dataset import AgentDataset, EgoDataset\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.geometry import transform_points\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR","81ec4a38":"import matplotlib.pyplot as plt\nfrom IPython.display import display, clear_output\nfrom IPython.core.display import HTML\nimport PIL\nfrom matplotlib import animation, rc","92a66db1":"DIR_INPUT = \"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\"\n\nSINGLE_MODE_SUBMISSION = f\"{DIR_INPUT}\/single_mode_sample_submission.csv\"\nMULTI_MODE_SUBMISSION = f\"{DIR_INPUT}\/multi_mode_sample_submission.csv\"\n\nDEBUG = True","e015e6c1":"cfg = {\n    'format_version': 4,\n    'model_params': {\n        'model_architecture': 'resnet50',\n        'history_num_frames': 10,\n        'history_step_size': 1,\n        'history_delta_time': 0.1,\n        'future_num_frames': 50,\n        'future_step_size': 1,\n        'future_delta_time': 0.1\n    },\n    \n    'raster_params': {\n        'raster_size': [224, 224],\n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map\/aerial_map.png',\n        'semantic_map_key': 'semantic_map\/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5\n    },\n    \n    'train_data_loader': {\n        'key': 'scenes\/train.zarr',\n        'batch_size': 12,\n        'shuffle': True,\n        'num_workers': 4\n    },\n    \n    'train_params': {\n        'max_num_steps': 100 if DEBUG else 10000,\n        'checkpoint_every_n_steps': 5000,\n        \n        # 'eval_every_n_steps': -1\n    }\n}","312d8206":"# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\ndm = LocalDataManager(None)","a0e5d2c1":"def animate_solution(images, timestamps=None):\n    def animate(i):\n        changed_artifacts = [im]\n        im.set_data(images[i])\n        if timestamps is not None:\n            time_text.set_text(timestamps[i])\n            changed_artifacts.append(im)\n        return tuple(changed_artifacts)\n\n    \n    fig, ax = plt.subplots()\n    im = ax.imshow(images[0])\n    if timestamps is not None:\n        time_text = ax.text(0.02, 0.95, \"\", transform=ax.transAxes)\n\n    anim = animation.FuncAnimation(fig, animate, frames=len(images), interval=60, blit=True)\n    \n    # To prevent plotting image inline.\n    plt.close()\n    return anim\n\ndef create_animate_for_indexes(dataset, indexes):\n    images = []\n    timestamps = []\n\n    for idx in indexes:\n        data = dataset[idx]\n        im = data[\"image\"].transpose(1, 2, 0)\n        im = dataset.rasterizer.to_rgb(im)\n        target_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n        center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n        draw_trajectory(im, target_positions_pixels, rgb_color=TARGET_POINTS_COLOR,  yaws=data[\"target_yaws\"])\n        clear_output(wait=True)\n        images.append(PIL.Image.fromarray(im[::-1]))\n        timestamps.append(data[\"timestamp\"])\n\n    anim = animate_solution(images, timestamps)\n    return anim\n\ndef create_animate_for_scene(dataset, scene_idx):\n    indexes = dataset.get_scene_indices(scene_idx)\n    return create_animate_for_indexes(dataset, indexes)","94c37456":"# ===== INIT DATASET\ntrain_cfg = cfg[\"train_data_loader\"]\n\n# Rasterizer\nrasterizer = build_rasterizer(cfg, dm)\n\n# Train dataset\/dataloader\ntrain_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()","191f4ce5":"ego_train_dataset = EgoDataset(cfg, train_zarr, rasterizer)\nprint(ego_train_dataset)\nprint(\"Ego dataset length: \", len(ego_train_dataset))","55f1037f":"data = ego_train_dataset[0]\n\nprint(\"agent_dataset[0]=data is \", type(data))\n\ndef _describe(value):\n    if hasattr(value, \"shape\"):\n        return f\"{type(value).__name__:20} shape={value.shape}\"\n    else:\n        return f\"{type(value).__name__:20} value={value}\"\n\nfor key, value in data.items():\n    print(\"  \", f\"{key:25}\", _describe(value))","24c3ef76":"agent_train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\nprint(agent_train_dataset)\nprint(\"Agent dataset length: \", len(agent_train_dataset))","7964d198":"ego_scene_0 = ego_train_dataset.get_scene_dataset(0)\nanim = create_animate_for_indexes(ego_scene_0, np.arange(len(ego_scene_0)))\nHTML(anim.to_jshtml())","54acb417":"# The frames in the scene are in a sequence and each frame has a unique timestamp and has track_id = -1 (-1 is for ego_car)\nfor i in range(len(ego_scene_0)):\n    print(\"Track ID: \", ego_scene_0[i]['track_id'], \" Frame ID: \",  ego_scene_0[i]['timestamp'])\n","474243fb":"agent_scene_0 = agent_train_dataset.get_scene_dataset(0)\n\nfor i in range(len(agent_scene_0)):\n    print(\"Track ID: \", agent_scene_0[i]['track_id'], \" Frame ID: \",  agent_scene_0[i]['timestamp'])","4a4a1d38":"anim = create_animate_for_indexes(agent_scene_0, np.arange(len(agent_scene_0)))\nHTML(anim.to_jshtml())","15233714":"Now, to understand the relation between EgoDataset and AgentDataset, let us consider scene 0. \nScene 0 in EgoDataset has 248 frames and we can visualize it as follows:","16ed0ff1":"I will use train.zarr here which has","54d63604":"# Imports and Configs","26406806":"# Util Functions","53ee4430":"Scene 0 in AgentDataset has 35 frames where the first 5 frames are for agent with track_id=1, next 6 frames for agent with track_id=328 and so on. It is important to note that each element of a scene in the AgentDataset corrsponds to some element in the same scene of EgoDataset. (And this relation can be seen using timestamp as each frame is identified using timestamp)\n\nFor instance, the 1st elemet of scene 0 of AgentDataset (track_id=1 and timestamp=1572643685901838786) corresponds to the 12th element of scene 0 of EgoDataset (track_id=-1 and timestamp=1572643685901838786)","2f7b8593":"Before we take a look at AgentDataset, you should understand what the internal data structure of a frame is (Also this is the same for Ego and Agent Dataset)","5e8b6e02":"#### Let's understand the high-level structure of each Agent and Ego Dataset. Thanks to @[corochann](https:\/\/www.kaggle.com\/corochann\/lyft-deep-into-the-l5kit-library\/notebook#1.-Understanding-Rasterizer-class) for this.\n### EgoDataset\n![image.png](attachment:image.png)\n1. Here the subject is the ego car (all the data is for the ego car)\n2. EgoDataset (for train.zarr) has a total length of 4,039,527 and each element is a frame. \n3. There are 16,265 scenes and each scene has around 247-248 frames. \n4. Each element (which is a frame in this case) can be identified by a unique timestamp.","aea05ac0":"Hopefully this clarifies what the relation between EgoDataset and AgentDataset is and my first point!","2809a96d":"Each attribute represents follows: \n* image: image drawn by Rasterizer. As you saw on the top of this kernel. This is usually be the input image for CNN\n* target_positions: The \"Ego car\" or \"Agent (car\/cyclist\/pedestrian etc)\"'s future position. This is the value to predict in this competition (not for Ego car's, but for Agents).\n* target_yaws: The Ego car's future yaw, to represent heading direction.\n* target_availabilities: flag to represent this is valid or not. Only flag=1 is used for competition evaluation.\n* history_positions: Past positions\n* history_yaws: Past yaws\n* history_availabilities:\n* world_to_image: 3x3 transformation matrix to convert world-coordinate into pixel-coordinate.\n* track_id: Unique ID for each Agent. None for Ego car.\n* timestamp: timestamp for current frame.\n* centroid: current center position\n* yaw: current direction\n* extent: Ego car or Agent's size. The car is not represented as point, but should be cared as dot box to include size information on the map.","af5f59a9":"#### The aim of this notebook is twofold - \n1. To give some insights on EgoDataset and AgentDataset, which I believe would help one gain an intuition about the input data to the model. The data in this competitions is a bit complex and I found L5kit to be an amazing tool but a blackbox. So hope this clarifies a few things.\n2. Raise certain question about what is the 'image' in a frame of EgoDataset or AgentDataset.\n\nSo, let's begin.","455754f6":"To understand this visualization for scene 0 of AgentDataset, press next button one by one (Do not press \"play\" button). You will notice that the first 5 frames are for **agent_1**, the next 6 frames are for **agent_328** and so on. \n\nNote: Here the green car is the subject agent car and the blue cars could be other agent cars or the Ego car.","98213517":"# Understanding AgentDataset and EgoDataset","72beb5e4":"Coming up next: The input to the CNN: (batch_size, 25, 224, 224). What does this 25 (in_channel) denote?","0f1ff1d9":"### AgentDataset\n![image.png](attachment:image.png)\n\n1. Here the subject is the agent car (all the data is for the agent car)\n2. AgentDataset (for train.zarr) has a total length of 22,496,709 and each element again is a frame (so the data structure of each element is exactly the same as of EgoDataset). \n3. There are 16,265 scenes and each scene has one of the three-\n    *   less number of frames than the corresponding scene in EgoDataset.\n    *   equal number of frames than the corresponding scene in EgoDataset.\n    *   more number of frames than the corresponding scene in EgoDataset.\n    For instance, scene-0 has 35 frames, scene-1 has 63 frames, scene-1001 has 2118 frames.\n4. Each element can be identified by a unique (timestamp + track_id) ."}}