{"cell_type":{"8a79cd2f":"code","5b343e94":"code","897bb577":"code","64ee5a6d":"code","e34924d4":"code","97c9b3c5":"code","648723b0":"code","36adc645":"code","41a6f673":"code","1bac060a":"code","cd088a30":"code","7aa2a4e9":"code","2cc7b94c":"code","dce6c197":"code","38266d6e":"code","c0cd1866":"code","3e789083":"code","67a5ba8d":"code","7ac48075":"code","010e7b9c":"code","9fa9356d":"code","3a3d519a":"code","9bcb121b":"code","6b0e929a":"code","3bed3d03":"code","0b547d54":"code","4c1f2f87":"markdown","d2736925":"markdown","e9a6a44c":"markdown","ecd4a349":"markdown","bc3b2033":"markdown","2d7b445b":"markdown","d871c526":"markdown","8d77c305":"markdown","6b2cdab8":"markdown","c52c8918":"markdown","fd96ce70":"markdown","c829aaf9":"markdown","9db506d2":"markdown","80d340ec":"markdown","205982f5":"markdown","4bb97756":"markdown","0ce111f5":"markdown","fef3f5de":"markdown","c457dedc":"markdown","c14e4fc4":"markdown","2cd6d933":"markdown","f814551c":"markdown"},"source":{"8a79cd2f":"#Import required libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve","5b343e94":"df_player = pd.read_csv(\"..\/input\/nba17-18\/nba_extra.csv\", index_col=0)                            \ndf_player.head(3)","897bb577":"df_team_standing = pd.read_csv(\"..\/input\/nba-enhanced-stats\/2017-18_standings.csv\", index_col=0)\ndf_team_standing.head(5)","64ee5a6d":"#Team name abbreviations according to Wiki:\n#https:\/\/en.wikipedia.org\/wiki\/Wikipedia:WikiProject_National_Basketball_Association\/National_Basketball_Association_team_abbreviations\n\n# Some of the team name between the tables are different, convert them so that they can match each other.\ndf_player.replace(to_replace='BRK', value='BKN', inplace=True)\ndf_player.replace(to_replace='TOT', value='TOR', inplace=True)\ndf_player.replace(to_replace='CHO', value='CHA', inplace=True)\n\ndf_team_standing.replace(to_replace='SA', value='SAS', inplace=True)\ndf_team_standing.replace(to_replace='NO', value='NOP', inplace=True)\ndf_team_standing.replace(to_replace='NY', value='NYK', inplace=True)\ndf_team_standing.replace(to_replace='GS', value='GSW', inplace=True)","e34924d4":"# Capture the number of winning game for each team\ndf_team_num_of_winning = df_team_standing[['teamAbbr', 'gameWon']]\ndf_team_num_of_winning = df_team_num_of_winning.groupby('teamAbbr').max().sort_values(['gameWon'], ascending=False)\n\n# Rename dataframe index\ndf_team_num_of_winning.index.names = ['Tm']\n\n# Print out the top 5 teams have the most winning\ndf_team_num_of_winning.head(5)","97c9b3c5":"df_player['win'] = df_player['Tm'].map(df_team_num_of_winning['gameWon'].to_dict())\ndf_player.head(5)\n","648723b0":"df_player.isnull().sum()[df_player.isnull().sum() > 0]\n","36adc645":"df_player.fillna(0, inplace=True)","41a6f673":"le_pos = LabelEncoder()\nle_pos.fit(df_player['Pos'])\ndf_player['Pos'] = le_pos.transform(df_player['Pos'])\n\nle_team = LabelEncoder()\nle_team.fit(df_player['Tm'])\ndf_player['Tm'] = le_team.transform(df_player['Tm'])","1bac060a":"df_player_name = df_player['Player']\ndf_player = df_player.drop(columns=['Player'])","cd088a30":"df_player.head(5)","7aa2a4e9":"f, ax = plt.subplots(figsize =(10, 10)) \nsns.heatmap(df_player.corr(), ax = ax, cmap =\"YlGnBu\", linewidths = 0.1) ","2cc7b94c":"df_player.corr()['win'].sort_values(ascending=False).iloc[1:].head(10)","dce6c197":"selected_features_name = df_player.corr()['win'].sort_values(ascending=False).iloc[1:].head(10).index\ncorr_matrix_selected = df_player[selected_features_name]","38266d6e":"display(corr_matrix_selected.head(5))","c0cd1866":"# Training set features & outputs\nall_x = df_player.iloc[:, :-1]\nall_y = df_player.iloc[:, -1]\n\nf_test_selected = pd.DataFrame()\nf_test_score = pd.DataFrame()\n\n# Select data using f_test\ndef select_features(data_x, data_y, select_algo, select_size):\n    k_best_selector = SelectKBest(select_algo, k=select_size)\n\n    selected_features = k_best_selector.fit_transform(data_x, data_y)\n    selected_fratures_scores = k_best_selector.scores_\n\n    selected_features_name = []\n\n    for selected, feature in zip(k_best_selector.get_support(), data_x.columns):\n        if selected:\n            selected_features_name.append(feature)\n\n    selected_data = pd.DataFrame(selected_features, columns=selected_features_name)\n    selected_fratures_scores = pd.DataFrame([selected_fratures_scores], columns=data_x.columns, index=['scores'])\n    \n    return selected_data, selected_fratures_scores\n\n# Obtain selected data by f-test\nf_test_selected, f_test_score = select_features(all_x, all_y, f_regression, 10)\n\n# Obtain selected data by mutual info\nmutual_info_selected, mutual_info_score = select_features(all_x, all_y, mutual_info_regression, 10)\n\n# Print selected feature between two algorithm\nprint(\"By F-test\")\nprint(f_test_selected.columns)\nprint(\"Bu mutual information\")\nprint(mutual_info_selected.columns)","3e789083":"display(f_test_selected.head(5).style.hide_index())\ndisplay(f_test_score.T.sort_values('scores', ascending=False).head(10))","67a5ba8d":"display(mutual_info_selected.head(5).style.hide_index())\ndisplay(mutual_info_score.T.sort_values('scores', ascending=False).head(10))","7ac48075":"train_test_ration = 0.8\n\n# Features of 4 different approaches\ntrain_x, test_x = train_test_split(all_x, random_state=1, train_size=train_test_ration)\ncorr_matrix_train_x, corr_matrix_test_x = train_test_split(corr_matrix_selected, random_state=1, train_size=train_test_ration)\nf_test_train_x, f_test_test_x = train_test_split(f_test_selected, random_state=1, train_size=train_test_ration)\nmutual_info_train_x, mutual_info_test_x = train_test_split(mutual_info_selected, random_state=1, train_size=train_test_ration)\n\n# Prediction target\ntrain_y, test_y = train_test_split(all_y, random_state=1, train_size=train_test_ration)","010e7b9c":"# Create linear regression object\nlr = LinearRegression(fit_intercept=True, normalize=True)\nlr_corr_matrix = LinearRegression(fit_intercept=True, normalize=True)\nlr_f_test = LinearRegression(fit_intercept=True, normalize=True)\nlr_mutual_info = LinearRegression(fit_intercept=True, normalize=True)\n\n# Train the model using the training sets\nlr.fit(train_x, train_y)\nlr_corr_matrix.fit(corr_matrix_train_x, train_y)\nlr_f_test.fit(f_test_train_x, train_y)\nlr_mutual_info.fit(mutual_info_train_x, train_y)","9fa9356d":"print('R2 score (All features):       %.3f' % lr.score(train_x, train_y))\nprint('R2 score (Correlation Matrix): %.3f' % lr_corr_matrix.score(corr_matrix_train_x, train_y))\nprint('R2 score (F-test):             %.3f' % lr_f_test.score(f_test_train_x, train_y))\nprint('R2 score (Mutual Information): %.3f' % lr_mutual_info.score(mutual_info_train_x, train_y))","3a3d519a":"# Prediction on training set to check the bias of the model\npred_y = lr.predict(train_x)\ncorr_matrix_pred_y = lr_corr_matrix.predict(corr_matrix_train_x)\nf_test_pred_y = lr_f_test.predict(f_test_train_x)\nmutual_info_pred_y = lr_mutual_info.predict(mutual_info_train_x)\n\nprint('Performance metrics over train data:')\nprint('(All Features)       Mean Absolute Error:', metrics.mean_absolute_error(train_y, pred_y))\nprint('(Correlation matrix) Mean Absolute Error:', metrics.mean_absolute_error(train_y, corr_matrix_pred_y))\nprint('(F-test)             Mean Absolute Error:', metrics.mean_absolute_error(train_y, f_test_pred_y))\nprint('(Mutual Information) Mean Absolute Error:', metrics.mean_absolute_error(train_y, mutual_info_pred_y))\nprint('')\n\n# Prediction on the test to see how it variance\npred_y = lr.predict(test_x)\ncorr_matrix_pred_y = lr_corr_matrix.predict(corr_matrix_test_x)\nf_test_pred_y = lr_f_test.predict(f_test_test_x)\nmutual_info_pred_y = lr_mutual_info.predict(mutual_info_test_x)\n\nprint('Performance metrics over test data:')\nprint('(All Features)       Mean Absolute Error:', metrics.mean_absolute_error(test_y, pred_y))\nprint('(Correlation matrix) Mean Absolute Error:', metrics.mean_absolute_error(test_y, corr_matrix_pred_y))\nprint('(F-test)             Mean Absolute Error:', metrics.mean_absolute_error(test_y, f_test_pred_y))\nprint('(Mutual Information) Mean Absolute Error:', metrics.mean_absolute_error(test_y, mutual_info_pred_y))","9bcb121b":"#Reference: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\nplot_learning_curve(lr, \"The learning curve\", train_x, train_y)","6b0e929a":"df_player_name = df_player_name.reset_index().drop(columns=['Rk'])\ndf_player_stat = df_player.reset_index().drop(columns=['Rk'])","3bed3d03":"player_index = df_player_name[df_player_name['Player'].str.contains(\"James Harden\")].index\nplayer_stat = df_player_stat.iloc[player_index.values,:]\n\npd.options.display.max_columns = None\npd.options.display.max_rows = None\ndisplay(player_stat)","0b547d54":"# Cut of James Harden free throw attempt by half from 727 to 350\nplayer_stat[player_stat.index == player_index.values]['FTA'] = 350\n\n# While keeping his FT% at 0.858, so that 350 * 0.858 = 300\nplayer_stat[player_stat.index == player_index.values]['FT'] = 300\nprint(\"If James Harden has only 350 free throw attempt, the number of game he would win: \", lr.predict(player_stat.iloc[:, :-1])[0])","4c1f2f87":"## Handle data in type of string\nIf the data type is a string, you often comes up a problem that you cannot process with any statistical model because those are mathematical models. So when we encounter strings, mapping it into a number would be a good idea so that we can pass data directly into mathematical equations.","d2736925":"# Data Preperation\nWe need a dataframe which contain player statistic against the number of winning game. First of all construct a dataframe with the data we have on hand.\n* Merge multiple dataframe into one show player stat against number of winning games\n* Create a common \"key\" column so that we can merge tables\n* Data cleaning: Make sure the index in the \"key\" column can match up accordingly\n* Data cleaning: Remove Null\/NaN data","e9a6a44c":"## Data cleaning\nCreate connection between table, make sure the mapping keys are unique and matchable.\nAnd convert the data if needed.","ecd4a349":"Also let's check out the learning curve","bc3b2033":"# Scope of Analysis\nAssuming I am a NBA player who didn't play very well in the league. I would like to know what aspect of my skill to improve in order to maximize the number of winning game in the year 2017 -2018. Thus, we will take into account the minutes per game (MPG), the points per game (POINTS) and the offensive\/defensive real plus-minus values (ORPM and DRPM) ... etc, against the number of winning game. And then analysis which part of my technique has the most CP-value in order to win the game. \n\nAfter we identify the most correlated aspect of winning a game, we then predict how much game we could win if we improve the three point percentage for let's say 1%. Or predict how many game can James Harden win if he has a lower free throw attempt.\n\n\n![http:\/\/pngimg.com\/uploads\/nba\/nba_PNG17.png](http:\/\/pngimg.com\/uploads\/nba\/nba_PNG17.png)\n\nLet's get started!","2d7b445b":"## (Approach 3 & 4) Feature selection by F-test & Mutual information\nHere we will also try two other feature selection algorithm called F-test and Mutual information. Both are selecing features base on the correlation between data and target.\n\nReference:\n[https:\/\/scikit-learn.org\/stable\/auto_examples\/feature_selection\/plot_f_test_vs_mi.html](https:\/\/scikit-learn.org\/stable\/auto_examples\/feature_selection\/plot_f_test_vs_mi.html)","d871c526":"# Conclusion\nFrom the result above we can see that the performance of the predicting model is pretty bad. Having a bad model can have many reason, I am going to list out a few potential reason.\n\n1. The feature selected has a low correlation with the target\n> If you go back and check the correlation matrix of the feature selected, you will notice that actually those feature are not quite correlated to win a game. Even through you may shot 3 point very well or getting a lot of steal, it doesn't mean that you will necessary win a game. In another point of view, NBA seems a fair game forcus on teamwork rather then individual skills.\n\n2. Overfitting or Underfitting\n> The model is underfitting. As we see before the learning curve, The training score and the cross validation score converge pretty well. Also from the mean absolute error analysis (MAE) analysis, we see that performance between the training data and test data are quite close. However they converge to a low score which means the model is suffer from high bias.\n\n3. Bad feature that destory the performance\n> Feature selection is a key for high accuracy model, selecting bad feature not only gain no benefit, but sometime may even hurt your model. Bad feature may confuse the system thus leading it to an incorrect prediction. However from the four appoaches I have done before, we can see that reducing features decrease the accuracy of the model. So I would say those features available are actually helping to predict the winning games a little bit, however those feature are not a game changer, we will need to identify other features highly correlated to our target.\n\nTo conclude, the data I have on hand didn't support predicting the target very well. One suggested step for future improvement is to identify more features that highly correclated. And interestingly, individual ability seems unrelated to win and loss. For example, Russell Westbrook we all know he has very good individual ability, but does he wins a lot? What about Stephen Curry, one of the best 3 point shooter and he did wins a lot. Is it really because of his individual ability? I would say winning a NBA seems value teemworks way more then individual ability.","8d77c305":"Here is the top 10 selected features by correlation matrix","6b2cdab8":"# Load data","c52c8918":"And let's see the mean absolute error over training set and test set data.","fd96ce70":"## Handle missing value\nData collected from the real world always is not ideal, some of the field may missing or in a format that not quite suitable for analysis. As you can see some data representing a percentage is a NULL value, the reason is that some player had never shot a field goal or three point during the entire season, resulting a (field goal \/ field goal attempt) = ( 0 \/ 0 ) = NaN. We can simply replace those value by ZERO for ease.","c829aaf9":"# Model Training\nNow the data is ready, let's see how good is it performing on predicting the winning games. As usual the data is splitted into training set and test set in a ration of 80:20. The linear regiression model will train against the training set and then verifiy against the test set, so we can understand the bias and the variance.","9db506d2":"## Merge data into a single dataframe for analysis\nBy mapping two table, we now have a dataframe contain \"player stat\" vs \"Number of winning game\"","80d340ec":"# Feature selection\n\nWhen we have large number of feature, often there is\/are redundant feature that doesn't help much in order to train the regression model. There are studies on how to select the best feature, there are many different approaches but it really depends on the real situation that your are facing, there isn't a one-model-fit-all things exist. Reducing feature help to prevent overfitting, reduce the time to train\/execute the model and sometime increse the accury of prediction. Althought, we don't actually have a lot of features here, but as an execrise I would like to try out a little bit and see the different.\n\nAnd I decided to apply four different approaches\n1. Feed all features to train the model\n2. Reduce the number of selected feature by: correlation matrix\n3. Reduce the number of selected feature by: F-test (Linear dependancy)\n4. Reduce the number of selected feature by: Mutual information (Non-linear dependancy)","205982f5":"# Finally we make some prediction\nThis part is just for fun. Since the model is not accurate, those result are just a reference. Hope you would find it interesting.\n\nJames Harden is known for getting many free throw by drawing fault. Some say it is a tactics\/technique, some say it is dirty and unsportsmanship. For this reason, I am interested to predict how many game can he win if I reduce his free throw attempt by half while keeping his free throw percentage.\n\n<img src=\"https:\/\/www.mercurynews.com\/wp-content\/uploads\/2018\/05\/harden2.jpg\" width=\"500\">","4bb97756":"Now we have the table ready to start analysis.\nLet's jump into the next stage!","0ce111f5":"And we drop the player name since it doesn't help in prediction","fef3f5de":"Show the R2 score over different approaches, emm...It seems the model doesn't perform quite well.","c457dedc":"# Thank you for reading!","c14e4fc4":"Here is the selected top 10 features by Mutual information and it's scores","2cd6d933":"Here is the top 10 selected features by F-test and it's scores","f814551c":"## (Approach 2) Feature selection by correclation matrix\nOne of the most easiest way to select feature is to calculate the correlation matrix. We will then select the top 10 correclated feature to train the linear regression model"}}