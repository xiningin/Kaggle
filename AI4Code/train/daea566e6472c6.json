{"cell_type":{"6112e37f":"code","f9248237":"code","eb463249":"code","0f97659f":"code","4b5066e8":"code","bb5c2131":"code","ddf87c6a":"code","798ff974":"code","94290aaf":"code","76b3128c":"code","4ba0ebc0":"code","e396ef94":"code","2873be72":"code","37b8abb3":"code","ce40d18b":"code","ed0f1e36":"code","7e91c517":"code","3531b8b8":"code","eea640ff":"code","818751c5":"code","7f803619":"code","1e08d03e":"code","c570bf71":"code","9cadc06c":"code","2777214b":"code","4ea0c696":"code","739813d8":"code","51cdd439":"code","c1c69c4d":"code","5512545d":"code","917493ba":"code","408e9a47":"code","ac16a230":"code","16f2c445":"code","85cd50cf":"code","0918eb55":"code","ad1115d1":"code","f02344ee":"code","22e14e8e":"markdown","f2e01db0":"markdown","e5f49b23":"markdown","2226846c":"markdown","23b7173d":"markdown","a50957f1":"markdown","7736f50c":"markdown","2f97a8c5":"markdown","e16efffa":"markdown","1b4a06dc":"markdown","c7605b41":"markdown","0a9250c7":"markdown","e7f2b068":"markdown","fb328371":"markdown","bacec287":"markdown","88d02c27":"markdown","08f79626":"markdown","a119810a":"markdown","1ae642be":"markdown"},"source":{"6112e37f":"import torchvision\nimport torchvision.datasets as dset\nfrom torchvision import transforms, models\nfrom torch.utils.data import DataLoader,Dataset\nimport matplotlib.pyplot as plt\nimport torchvision.utils\nimport numpy as np\nimport random\nfrom PIL import Image,ImageOps\nimport torch\nfrom torch.autograd import Variable  \nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport pandas as pd\nfrom skimage import io\nimport os\nimport IPython\nimport numbers\n%matplotlib inline","f9248237":"from PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True","eb463249":"class Config():  \n    image_dir = \"..\/input\/digixai-image-retrieval\/train_data\/\"\n    labels_dir = \"..\/input\/digixai-image-retrieval\/train_data\/label.txt\"\n    gallery_dir = \"..\/input\/digixalgoai\/test_data_B\/gallery\/\"\n    query_dir = \"..\/input\/digixalgoai\/test_data_B\/query\/\"\n    outfile = \"\/content\/drive\/My Drive\/log.txt\"\n    feature_file = \".\/features.pth\"\n    model = \".\/model.pth\"\n    loss = {\"gamma\":5,\"q_lambda\":1,\"normed\":True}\n    \n    feature_batch_size = 64            #batch size used while extracting features\n    train_batch_size = 128             #batch size used while training the network with extracted features\n    train_number_epochs = 30          #batch size used while extracting features\n    test_batch_size = 64\n    #some tranformations to be applied on the images\n    transformer = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])","0f97659f":"def imshow(img,text=None,should_save=False):\n    npimg = img.numpy()\n    plt.figure(figsize=(21,12))\n    plt.axis(\"off\")\n    if text:\n        plt.text(75, 8, text, style='italic',fontweight='bold',\n            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()    \n\ndef show_plot(iteration,loss):\n    plt.figure(figsize=(14,8))\n    plt.plot(iteration,loss)\n    plt.show()","4b5066e8":"class ResizeImage():\n    def __init__(self, size):\n      if isinstance(size, int):\n        self.size = (int(size), int(size))\n      else:\n        self.size = size\n    def __call__(self, img):\n      th, tw = self.size\n      return img.resize((th, tw))\n\n\nclass PlaceCrop(object):\n    \"\"\"Crops the given PIL.Image at the particular index.\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (w, h), a square crop (size, size) is\n            made.\n    \"\"\"\n\n    def __init__(self, size, start_x, start_y):\n        if isinstance(size, int):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.start_x = start_x\n        self.start_y = start_y\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL.Image): Image to be cropped.\n        Returns:\n            PIL.Image: Cropped image.\n        \"\"\"\n        th, tw = self.size\n        return img.crop((self.start_x, self.start_y, self.start_x + tw, self.start_y + th))\n\n\nclass ForceFlip(object):\n    \"\"\"Horizontally flip the given PIL.Image randomly with a probability of 0.5.\"\"\"\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL.Image): Image to be flipped.\n        Returns:\n            PIL.Image: Randomly flipped image.\n        \"\"\"\n        return img.transpose(Image.FLIP_LEFT_RIGHT)\n\ndef image_train(resize_size=256, crop_size=224):\n    \n  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                   std=[0.229, 0.224, 0.225])\n  return  transforms.Compose([\n        transforms.Resize(resize_size),\n        transforms.CenterCrop(crop_size),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        normalize\n    ])\n\ndef image_test(resize_size=256, crop_size=224):\n  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                   std=[0.229, 0.224, 0.225])\n  #ten crops for image when validation, input the data_transforms dictionary\n  start_first = 0\n  start_center = (resize_size - crop_size - 1) \/ 2\n  start_last = resize_size - crop_size - 1\n \n  return transforms.Compose([\n    ResizeImage(resize_size),\n    PlaceCrop(crop_size, start_center, start_center),\n    transforms.ToTensor(),\n    normalize\n  ])\n\ndef image_test_10crop(resize_size=256, crop_size=224):\n  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                   std=[0.229, 0.224, 0.225])\n  #ten crops for image when validation, input the data_transforms dictionary\n  start_first = 0\n  start_center = (resize_size - crop_size - 1) \/ 2\n  start_last = resize_size - crop_size - 1\n  data_transforms = {}\n  data_transforms['val0'] = transforms.Compose([\n      ResizeImage(resize_size),ForceFlip(),\n      PlaceCrop(crop_size, start_first, start_first),\n      transforms.ToTensor(),\n      normalize\n  ])\n  data_transforms['val1'] = transforms.Compose([\n      ResizeImage(resize_size),ForceFlip(),\n      PlaceCrop(crop_size, start_last, start_last),\n      transforms.ToTensor(),\n      normalize\n  ])\n  data_transforms['val2'] = transforms.Compose([\n      ResizeImage(resize_size),ForceFlip(),\n      PlaceCrop(crop_size, start_last, start_first),\n      transforms.ToTensor(),\n      normalize\n  ])\n  data_transforms['val3'] = transforms.Compose([\n      ResizeImage(resize_size),ForceFlip(),\n      PlaceCrop(crop_size, start_first, start_last),\n      transforms.ToTensor(),\n      normalize\n  ])\n  data_transforms['val4'] = transforms.Compose([\n      ResizeImage(resize_size),ForceFlip(),\n      PlaceCrop(crop_size, start_center, start_center),\n      transforms.ToTensor(),\n      normalize\n  ])\n  data_transforms['val5'] = transforms.Compose([\n      ResizeImage(resize_size),\n      PlaceCrop(crop_size, start_first, start_first),\n      transforms.ToTensor(),\n      normalize\n  ])\n  data_transforms['val6'] = transforms.Compose([\n    ResizeImage(resize_size),\n    PlaceCrop(crop_size, start_last, start_last),\n    transforms.ToTensor(),\n    normalize\n  ])\n  data_transforms['val7'] = transforms.Compose([\n    ResizeImage(resize_size),\n    PlaceCrop(crop_size, start_last, start_first),\n    transforms.ToTensor(),\n    normalize\n  ])\n  data_transforms['val8'] = transforms.Compose([\n    ResizeImage(resize_size),\n    PlaceCrop(crop_size, start_first, start_last),\n    transforms.ToTensor(),\n    normalize\n  ])\n  data_transforms['val9'] = transforms.Compose([\n    ResizeImage(resize_size),\n    PlaceCrop(crop_size, start_center, start_center),\n    transforms.ToTensor(),\n    normalize\n  ])\n  return data_transforms","bb5c2131":"\ndef d(u, v, hashbit):\n    \n    inner_product = torch.mm(u, v.t())\n    norm = torch.mm(u.pow(2).sum(dim=1, keepdim=True).pow(0.5), v.pow(2).sum(dim=1, keepdim=True).pow(0.5).t())\n    cos = inner_product \/ norm.clamp(min=0.00001)\n    \n    #formula 6\n    return (1 - cos.clamp(max=0.99999)) * hashbit \/ 2\n    \ndef pairwise_loss(u, v, label1, label2, hashbit = 48,gamma=1.0, q_lambda=1.0,normed=True):\n\n    label_ip = Variable(label1 == (label2.transpose(0,1))).float()\n    s = torch.clamp(label_ip, 0.0, 1.0)\n\n    one = torch.ones((u.size(0), hashbit)).cuda()\n\n    if (1 - s).sum() != 0 and s.sum() != 0:\n        # formula 2\n        positive_w = s * s.numel() \/ s.sum()\n        negative_w = (1 - s) * s.numel() \/ (1 - s).sum()\n        w = positive_w + negative_w\n\n    else:\n        # if either |S1|==0 or |S2|==0\n        w = 1\n\n    d_hi_hj = d(u, v, hashbit)\n\n    # formula 8\n    cauchy_loss = w * (s * torch.log(d_hi_hj \/ gamma) + torch.log(1 + gamma \/ d_hi_hj))\n\n    # formula 9\n    quantization_loss = torch.log(1 + d(u.abs(), one, hashbit) \/ gamma) + torch.log(1 + d(v.abs(), one, hashbit) \/ gamma)\n\n    # formula 7\n    loss = cauchy_loss.mean() + q_lambda * quantization_loss.mean()\n\n    return loss","ddf87c6a":"resnet_dict = {\"ResNet18\":models.resnet18, \"ResNet34\":models.resnet34, \"ResNet50\":models.resnet50, \"ResNet101\":models.resnet101, \"ResNet152\":models.resnet152} \nclass ResNetFc(nn.Module):\n  def __init__(self, name, hash_bit):\n    super(ResNetFc, self).__init__()\n    model_resnet = resnet_dict[name](pretrained=True)\n    self.conv1 = model_resnet.conv1\n    self.bn1 = model_resnet.bn1\n    self.relu = model_resnet.relu\n    self.maxpool = model_resnet.maxpool\n    self.layer1 = model_resnet.layer1\n    self.layer2 = model_resnet.layer2\n    self.layer3 = model_resnet.layer3\n    self.layer4 = model_resnet.layer4\n    self.avgpool = model_resnet.avgpool\n    \n    #convolution layers\n    self.feature_layers = nn.Sequential(self.conv1, self.bn1, self.relu, self.maxpool, \\\n                         self.layer1, self.layer2, self.layer3, self.layer4, self.avgpool)\n    \n    #fully connected layers\n    self.fc = nn.Sequential(nn.Linear(model_resnet.fc.in_features,256),\n                            nn.ReLU(),)\n    \n    #hashing layer outputs the hashbits\n    self.hash_layer = nn.Linear(256, hash_bit)\n    #initializing the layers\n    self.hash_layer.weight.data.normal_(0, 0.01)\n    self.hash_layer.bias.data.fill_(0.0)\n    \n    self.iter_num = 0\n    self.__in_features = hash_bit\n    self.step_size = 300\n    self.gamma = 0.005\n    self.power = 0.5\n    self.init_scale = 1.0\n    self.activation = nn.Tanh()\n    self.scale = self.init_scale\n\n    #passing through the whole network at once\n  def forward(self, x):\n    if self.training:\n        self.iter_num += 1\n\n    x = self.feature_layers(x)\n    x = x.view(x.size(0), -1)\n    x = self.fc(x)\n    y = self.hash_layer(x)\n    y = self.activation(y)\n    return y\n    \n    #passing only through the convolution layers to extract intermediate features\n  def forward_static(self,x):\n    x = self.feature_layers(x)\n    return x\n    \n    #passing the extracted features through the rest of the network for training\n  def forward_train(self,x):\n    x = x.view(x.size(0),-1)\n    x = self.fc(x)\n    y = self.hash_layer(x)\n    y = self.activation(y)\n    return y\n\n  def output_num(self):\n    return self.__in_features\n\n","798ff974":"class Encodings(Dataset):\n\n    def __init__(self, image_dir, path, transform=None):\n\n        imgs = pd.read_csv(path, header=None).iloc[:,0]\n        self.labels = pd.read_csv(path, header=None).iloc[:,1]\n        self.img_path = image_dir+imgs\n        #self.data =  pd.DataFrame(img_list)\n        # self.image_dir = image_dir\n        self.transform = transform\n        \n\n    def __len__(self):\n        return len(self.img_path)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = self.img_path.iloc[idx]\n        label = self.labels.iloc[idx]\n        image = Image.open(img_name).convert(\"RGB\")      \n\n        if self.transform:\n          image = self.transform(image)\n\n        sample = {'image':image, 'label':self.labels.iloc[idx]}\n\n        return sample","94290aaf":"#returens\n#encodings -- extracted intermediate features\n#labels-- the class to which the image belongs\n#indexes-- indices of the image in the data\n\ndef feature_extractor(encoding_dataloader, verbosity):\n\n    base_network.eval()\n\n    with torch.no_grad(): \n\n        for i_batch, sample in enumerate(encoding_dataloader):\n            \n            length = len(sample['label'])\n\n            if torch.cuda.is_available():\n                sample['image'] = sample['image'].cuda()\n                sample['label'] = sample['label'].cuda()\n\n            encoding = base_network.forward_static(sample['image'])\n            index = torch.Tensor(np.arange(i_batch*length,(i_batch+1)*length))\n\n            if not i_batch: \n                encodings = encoding\n                labels = sample['label']\n                indexes = index\n                continue\n\n            encodings = torch.cat([encodings, encoding], dim=0)\n            labels = torch.cat([labels,sample['label']],dim=0)\n            indexes = torch.cat([indexes,index],dim=0)\n\n            if verbosity: \n                for img in sample['label']:\n                    print(\"Image from label {} succesfully extracted\".format(img))\n\n        return encodings, labels, indexes","76b3128c":"hash_bit = 32 #Number of bits onto which each image is mapped\nbase_network = ResNetFc(name=\"ResNet50\",hash_bit=hash_bit).cuda()","4ba0ebc0":"image_dataset = Encodings(image_dir = Config.image_dir, path = Config.labels_dir, transform = Config.transformer)\nencoding_dataloader = DataLoader(image_dataset, shuffle=False, num_workers=0, batch_size=Config.train_batch_size)","e396ef94":"encodings, labels, indexes = feature_extractor(encoding_dataloader, verbosity=0)","2873be72":"encodings_dict = {\"encodings\":encodings, \"labels\":labels, \"indexes\":indexes}\ntorch.save(encodings_dict, Config.feature_file)","37b8abb3":"encodings_dict = torch.load(Config.feature_file)","ce40d18b":"class NetworkDataset(Dataset):\n    \n    def __init__(self,encodings_dict):\n        self.encodings_dict = encodings_dict    \n        # self.transform = transform\n        # self.should_invert = should_invert\n        \n    def __getitem__(self,index):\n\n        idx1 = int(random.choice(self.encodings_dict['indexes']))\n        label1 = self.encodings_dict['labels'][idx1]\n\n        # idx2 = int(random.choice(self.encodings_dict['indexes']))\n        # label2 = self.encodings_dict['labels'][idx2]\n\n\n        #we need to make sure approx 50% of images are in the same class\n        should_get_same_class = random.randint(0,1) \n\n        if should_get_same_class:\n            while True:\n                #keep looping till the same class image is found\n                idx2 = idx1 + random.randint(-10,10)                                 # as semantically similar images belong to \n                while (idx2>=len(self.encodings_dict[\"indexes\"]) or idx2<0):         # the same class and are stored together\n                    idx2 = idx1+random.randint(-10,10)                               # we just search in the nearby background of \n                                                                                     # the image 1.\n                label2 = self.encodings_dict['labels'][idx2] \n                if label1 == label2:\n                    break\n        else:\n            while True:\n                #keep looping till a different class image is found        \n                idx2 = int(random.choice(self.encodings_dict['indexes']))\n                label2 = self.encodings_dict['labels'][idx2] \n                if label1 != label2:\n                    break\n\n\n        img1 = self.encodings_dict['encodings'][idx1]\n        img2 = self.encodings_dict['encodings'][idx2]\n        \n        return img1, img2 , label1, label2\n    \n    def __len__(self):\n        return len(self.encodings_dict[\"indexes\"])","ed0f1e36":"def load_split_train_test(encodings_dict, valid_size = .1,batch_size=64):\n\n    train_data = NetworkDataset(encodings_dict = encodings_dict)\n\n    num_train = len(encodings_dict['indexes'])\n    indices = list(range(num_train))\n    split = int(np.floor(valid_size * num_train))\n    np.random.shuffle(indices)\n\n    from torch.utils.data.sampler import SubsetRandomSampler\n\n    train_idx, test_idx = indices[split:], indices[:split]\n    train_sampler = SubsetRandomSampler(train_idx)\n    test_sampler = SubsetRandomSampler(test_idx)\n    \n    trainloader = torch.utils.data.DataLoader(train_data,\n                sampler=train_sampler, batch_size=batch_size)\n    testloader = torch.utils.data.DataLoader(train_data,\n                sampler=test_sampler, batch_size=batch_size)\n    \n\n    return trainloader, testloader","7e91c517":"train_dataloader, valid_dataloader = load_split_train_test(encodings_dict, 0.05) # 5% data is for validation","3531b8b8":"print(base_network)","eea640ff":"optimizer = optim.AdamW([{\"params\":base_network.fc.parameters(), \"lr\":0.00001},{\"params\":base_network.hash_layer.parameters(), \"lr\":0.0001}])","818751c5":"counter = []\nloss_history = [] \niteration_number= 0","7f803619":"base_network.train()\n\nfor epoch in range(1,Config.train_number_epochs+1):\n\n    for i, data in enumerate(train_dataloader,1):\n\n        inputs1, inputs2 , label1, label2 = data\n        # inputs1, inputs2 , label1, label2 = inputs1.cuda(), inputs2.cuda() , label1.cuda(), label2.cuda()\n\n        label1.unsqueeze_(1)\n        label2.unsqueeze_(1)\n\n        inputs = torch.cat((inputs1, inputs2), dim=0)\n\n        optimizer.zero_grad()\n        outputs = base_network.forward_train(inputs)\n        similarity_loss = pairwise_loss(outputs.narrow(0,0,inputs1.size(0)),outputs.narrow(0,inputs1.size(0),inputs2.size(0)),label1,label2,hashbit = hash_bit,gamma=Config.loss[\"gamma\"],normed=Config.loss[\"normed\"],q_lambda=Config.loss[\"q_lambda\"])\n        similarity_loss.backward()\n        optimizer.step()\n\n\n        if i %10 == 0 :\n            iteration_number +=10\n            counter.append(iteration_number)\n            loss_history.append(similarity_loss.item()) \n\n    print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,similarity_loss.item()))","1e08d03e":"torch.save(base_network.state_dict(),Config.model)","c570bf71":"show_plot(counter[100:],loss_history[100:])","9cadc06c":"base_network.eval()\n\nvalid_loss = 0 \n\nwith torch.no_grad():\n\n    for data in valid_dataloader:\n\n        inputs1, inputs2 , label1, label2 = data\n        # inputs1, inputs2 , label1, label2 = inputs1.cuda(), inputs2.cuda() , label1.cuda(), label2.cuda()\n\n        label1.unsqueeze_(1)\n        label2.unsqueeze_(1)\n\n        inputs = torch.cat((inputs1, inputs2), dim=0)\n                           \n        outputs = base_network.forward_train(inputs)\n        similarity_loss = pairwise_loss(outputs.narrow(0,0,inputs1.size(0)),outputs.narrow(0,inputs1.size(0),inputs2.size(0)),label1,label2,hashbit = hash_bit,gamma=Config.loss[\"gamma\"],normed=Config.loss[\"normed\"],q_lambda=Config.loss[\"q_lambda\"])\n\n        valid_loss = valid_loss + similarity_loss.item()\n\n    print(\"loss = {}\".format(valid_loss\/len(valid_dataloader)))","2777214b":"hash_bit = 32\nbase_network = ResNetFc(name=\"ResNet50\",hash_bit=hash_bit).cuda()\nbase_network.load_state_dict(torch.load(Config.model))","4ea0c696":"class Encodings(Dataset):\n\n    def __init__(self, image_dir, transform=None):\n        \n        img_list = os.listdir(image_dir)\n        self.data =  pd.DataFrame(img_list)\n        self.image_dir = image_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.image_dir,\n                                self.data.iloc[idx, 0])\n        \n        image = Image.open(img_name).convert(\"RGB\")\n\n        if self.transform:\n          image = self.transform(image)\n\n        sample = {'image':image, 'name':self.data.iloc[idx,0]}\n\n        return sample","739813d8":"#passes the batch of images through the network and returns the output\n\ndef encoding_extractor(encoding_dataloader, verbosity):\n\n    base_network.eval()\n\n    with torch.no_grad(): \n\n        for i_batch, sample in enumerate(encoding_dataloader):\n\n            sample['image'] = sample['image'].cuda()\n\n            encoding = base_network.forward(sample['image'])\n            \n            if not i_batch: \n                encodings = encoding\n                names = sample['name']\n                continue\n\n            encodings = torch.cat([encodings, encoding], dim=0)\n            names.extend(sample['name'])\n\n            if verbosity: \n                    print(\"{} succesfully extracted\".format(i_batch))\n\n    return encodings, names","51cdd439":"def Euclidean_distance(verbosity):\n    result=[]\n    gallery = gallery_dict['encodings']\n\n    for query in query_dict['encodings']:\n      i=0\n      while(i<gallery.shape[0]):\n        matrix = torch.sum((gallery[i:i+256]-query).square(), dim=1)\n        if not i: dissimilarity_matrix = matrix\n        else: dissimilarity_matrix = torch.cat([dissimilarity_matrix, matrix], dim=0)\n        i = i+256\n      top_10 = dissimilarity_matrix.argsort(dim=0)[:10].cpu()\n      if verbosity: print(top_10)\n      result.append(top_10)\n    return result\n\ndef Hamming_distance(verbosity):\n    result=[]\n    gallery = gallery_dict['encodings']\n\n    for query in query_dict['encodings']:\n      i=0\n      while(i<gallery.shape[0]):\n        matrix = torch.matmul(gallery[i:i+1024], query.view(-1,1)) * (-1.0)\n        if not i: similarity_matrix = matrix\n        else: similarity_matrix = torch.cat([similarity_matrix, matrix], dim=0)\n        i = i+1024\n      top_10 = similarity_matrix.argsort(dim=0)[:10].view(-1,).cpu()\n      if verbosity: print(top_10)\n      result.append(top_10)\n    return result","c1c69c4d":"query_encoder = Encodings(Config.query_dir,\n                            transform=image_test(),)","5512545d":"query_dataloader = DataLoader(query_encoder,\n                                shuffle=False,\n                                num_workers=8, \n                                batch_size=Config.test_batch_size)","917493ba":"gallery_encoder = Encodings(Config.gallery_dir,\n                                transform=image_test())","408e9a47":"gallery_dataloader = DataLoader(gallery_encoder,\n                                    shuffle=False,\n                                    num_workers=8, \n                                    batch_size=Config.test_batch_size)","ac16a230":"query_encodings, query_names = encoding_extractor(query_dataloader, verbosity=0)\nquery_dict = {'encodings':query_encodings, 'names':query_names}","16f2c445":"gallery_encodings, gallery_names = encoding_extractor(gallery_dataloader, verbosity=0)\ngallery_dict = {'encodings':gallery_encodings, 'names':gallery_names}","85cd50cf":"result = Euclidean_distance(verbosity=0)","0918eb55":"output = pd.DataFrame()\n\ngallery_names = pd.Series(gallery_dict['names'])\n\nfor img_num in range(len(result)):\n    indexes = result[img_num].tolist()\n    retrieved = gallery_names[indexes].to_list()\n    output[query_dict['names'][img_num]]=retrieved\n    \noutput = output.transpose()\noutput.reset_index(inplace=True)","ad1115d1":"output.to_csv('.\/submission.csv',header=None,index=None)","f02344ee":"result = pd.read_csv('.\/submission.csv',header=None)\n\nnum = 189     # choose any integer between [0,41573]\n\nquery_img = Image.open(Config.query_dir+result.iloc[num,0])\nquery_img = Config.transformer(query_img).unsqueeze(0)\n\nimg_list = query_img\nfor img in result.iloc[num,1:]:\n    image = Image.open(Config.gallery_dir+img)\n    image = Config.transformer(image).unsqueeze(0)\n    img_list = torch.cat([img_list,image])\n\nimshow(torchvision.utils.make_grid(img_list, nrow=img_list.shape[0]))","22e14e8e":"## Validation\n\nChecking validation dataset loss value.","f2e01db0":"## Output","e5f49b23":"## Helper functions","2226846c":"The dataset class defined for the purpose of extracting features as mentioned earlier.","23b7173d":"Here we define two helper functions. `imshow` is for displaying images while `show_plot` is for plotting the loss function.","a50957f1":"## Preprocesss\n\nPreprocessing is an important step which can greatly influence the performance of the model. Various preprocessing techniques like cropping, rotating, flipping and other data augmentation techniques come under this. \n\nSome preprocessing techniques are applied before training, while some are used at time of testing like test10crop.","7736f50c":"## Extracting Intermediate Features","2f97a8c5":"Add train data to your notebook using this dataset https:\/\/www.kaggle.com\/varenyambakshi\/digixai-image-retrieval.","e16efffa":"# **Image Retrieval Using DCH**","1b4a06dc":"Accurate image retrieval is a core technology for shopping by picture taking, and also becomes a hotspot in the academia and industry. Here, we take a digital device image dataset in a real snap-to-shop scenario, provided in Huawei DIGIX Global AI Challenge. In a Content Based Image Retrieval (CBIR) System, the task is to retrieve similar images from a large database given a query image. The usual procedure is to extract some useful features from the query image, and retrieve images which have similar set of features. For this purpose, a suitable similarity measure is chosen, and images with high similarity scores are retrieved. Naturally the choice of these features play a very important role in the success of this system, and high level features are required to reduce the semantic gap.\n\nDeep Hashing methods are most commonly used to implement content based image retrieval. If you do not know about image retrieval Check out this [paper](https:\/\/arxiv.org\/pdf\/2006.05627.pdf).\n\nOut of all the Deep Hashing methods we shall implement the DEEP CAUCHY HASHING for content based image retrieval. Here is the [paper](http:\/\/ise.thss.tsinghua.edu.cn\/~mlong\/doc\/deep-cauchy-hashing-cvpr18.pdf).\n\nIf you find the notebook insightful please do upvote it.","c7605b41":"## Setup","0a9250c7":"## Loss\n\nHere we define the loss function of DCH. We basically pass two batches of images encoded in hashbits where some images are semantically (content-wise) similar while others are different. The more the distance between hashbits of similar images, the more heavily loss function punishes and vice-versa. To know about the maths you can refer the [paper](http:\/\/ise.thss.tsinghua.edu.cn\/~mlong\/doc\/deep-cauchy-hashing-cvpr18.pdf).  \n\nWe also write mention the equation number in the code in case you are following the paper.","e7f2b068":"## Testing\n\nLets run our code on the test dataset. The test dataset comprises of \n\n1. Gallery images\n2. Query images\n\nFor every query image we have to find the gallery images which are similar to the query images. So first we shall pass both the query as well as gallery images through the network and then comparing the hash bits find the similar images.","fb328371":"Lets start by importing necessary packages. Also make sure that you have added Google Cloud Services to your notebook (if not go to the \"Add-ons\" section and attach it to your notebook) and are using GPU.","bacec287":"## Train from Extracted ","88d02c27":"Add test data to your notebook using this dataset https:\/\/www.kaggle.com\/varenyambakshi\/digixalgoai.","08f79626":"Here we define two types of distances namely : Euclidean distance and Hamming distance. \n\n* Euclidean distance: gives slightly better results but takes more time\n* Hamming distance: gives slightly poorer results but takes less time","a119810a":"## Network\n\nAlthough there are many models available, we shall be using RESNET50. We will have broadly three types of layers namely \n1. Convolution layers\n1. Fully connected layers \n1. Hashing layer\n\nInstead of training the whole network at once we can use transfered learing and train few of the last layers. To further speed up training we can extract the intermediate feaures so that we do not have to pass the images everytime through the whole network. Instead we can just train on the extracted features.","1ae642be":"## Creating Dataset\n\nWe shall now create a function to perform train test split on the data. First load the extracted intermediate features.\n"}}