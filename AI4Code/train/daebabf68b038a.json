{"cell_type":{"f6255dbf":"code","c7ed7147":"code","99ce616e":"code","bb5c9a3f":"code","726917c9":"code","105e813a":"code","65e5a7d8":"code","fb2452e7":"code","a5a0bd06":"code","9bbfb356":"code","f97fa4cc":"code","addba488":"code","ba9b5113":"code","342df286":"code","d6754fae":"code","183c54a3":"code","0942c1a8":"code","3c1d0051":"code","c2cfb242":"code","3c7f7fbb":"code","c7be60c0":"code","d31dc132":"code","a8d0e1e2":"code","ad487e52":"code","d5205aac":"code","79cb366b":"code","499b131b":"code","95b87bb4":"code","04bcb79d":"code","2283cef8":"code","42da54a6":"code","5bb57adf":"code","ed08f5a4":"code","2d4f6e1a":"code","7bbe7023":"code","4a3f1d7f":"code","6f4a2ab7":"code","09e42ec0":"code","02c2af46":"code","39034080":"code","17a9de84":"code","a29e3031":"code","5711a223":"code","15cb7381":"code","cfb14fde":"code","1e4d549f":"code","b883bbc4":"code","9aaa85e3":"code","ddcbcaf6":"code","9aa2f181":"code","21f1d755":"code","4b538fc0":"code","350f96c0":"code","c1be33b5":"code","1fcf43f0":"code","fc4d11a8":"code","081bd391":"code","6ef073a5":"code","5c1e40ab":"code","6a7edebe":"code","1d625a34":"code","65364447":"code","455eb715":"code","92b9226f":"code","e0e29617":"code","5da8632e":"code","ec4fee2e":"code","0590de5e":"code","e50a39d6":"code","7a28f1a4":"code","6defb086":"code","254bc48e":"code","ac771e61":"code","951ec90d":"code","0596d506":"code","3bf6b6e2":"code","9dd25590":"code","326bb048":"code","ece837c8":"code","d78feb34":"code","81916be3":"code","a3819ad5":"markdown","d1241a4a":"markdown","2cb8ee8c":"markdown","63f2824a":"markdown","95bbc372":"markdown","dbc9f85a":"markdown","826b73f1":"markdown","cb67fda1":"markdown","94af38f5":"markdown","c7bfd445":"markdown","0d0195c3":"markdown","6548dab2":"markdown","aaf54872":"markdown","f1c0c68e":"markdown","c63c042b":"markdown","0a9ffe00":"markdown","2e840231":"markdown","fd9efb22":"markdown","48442fde":"markdown","4205d447":"markdown","30e9f51c":"markdown","dde88007":"markdown","632e7c43":"markdown","d71169c0":"markdown","d0780ef2":"markdown","536cd525":"markdown","ca8cb29c":"markdown","f50af742":"markdown","38426deb":"markdown","cd1d3093":"markdown","6b5fb15b":"markdown","4794a589":"markdown","606dc3b3":"markdown","4907ba37":"markdown","d9fa5d24":"markdown","cbb1bae5":"markdown","1e108fa6":"markdown","1abf038e":"markdown","65c06969":"markdown","405830fe":"markdown","e5f4f4f8":"markdown","6d112a66":"markdown","d5d93a8a":"markdown","0bf9f556":"markdown","3d2302ce":"markdown","10792b5a":"markdown","27cf6598":"markdown","8a97aa76":"markdown","608e51f0":"markdown","6dd37e33":"markdown","0e4bff68":"markdown","ec1601ec":"markdown","dd88daa0":"markdown","3356f301":"markdown","cad59cd8":"markdown","f24f70ca":"markdown","8b3760e6":"markdown","9a738fd7":"markdown","f93f8ad9":"markdown","d37dd211":"markdown","e2ce804c":"markdown","86c2c910":"markdown","6889dda7":"markdown","a763ba88":"markdown","ee60d043":"markdown","75a5c3e6":"markdown","87920080":"markdown","e92c4a72":"markdown","4d4c5fb7":"markdown","6c306b84":"markdown","86799916":"markdown","957f2ff9":"markdown","113d8e7e":"markdown","e80f0331":"markdown","76a1f86c":"markdown","d7053fd6":"markdown","8d5f4f77":"markdown","42c499e9":"markdown","15f7538c":"markdown","7cdccb22":"markdown","3857247a":"markdown","98147161":"markdown","e9e8756f":"markdown","9cb2f6aa":"markdown","0f3b6262":"markdown","db7b43bd":"markdown","60da800c":"markdown","ca8830de":"markdown","cddce5a3":"markdown","36e91ae8":"markdown","6e89c4c1":"markdown","4745783d":"markdown","febaa6cd":"markdown","0b331b21":"markdown","0bdce0fd":"markdown","ee212518":"markdown","ffd5c29a":"markdown","4d1862a0":"markdown","221827bb":"markdown","74ffcc5f":"markdown","2a585544":"markdown","ff5c3d52":"markdown","7c124b8b":"markdown","1e3257a3":"markdown","34951ba5":"markdown","594106e6":"markdown","5b95c187":"markdown","409c7c57":"markdown","44b2bd60":"markdown","72b94a3d":"markdown","9ed0846f":"markdown","f6c8e07a":"markdown","a6c1364e":"markdown","059434d6":"markdown","84bc8e60":"markdown","fe918845":"markdown","5585ba6b":"markdown","d223b37e":"markdown","a97d186d":"markdown","1bfc86e3":"markdown","34197565":"markdown","abbd644e":"markdown","c2dd34e4":"markdown","6e5b6e2e":"markdown"},"source":{"f6255dbf":"# Third party\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Otras liberr\u00edas auxiliares\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn import tree\n\nfrom imblearn import FunctionSampler\nfrom imblearn.pipeline import make_pipeline\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sb\nimport plotly as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport graphviz \nimport matplotlib\nimport matplotlib.pyplot as pypl\nfrom matplotlib.pyplot import figure\n\n# Local application\nimport miner_a_de_datos_an_lisis_exploratorio_utilidad as utils","c7ed7147":"seed = 27912","99ce616e":"filepath = \"..\/input\/pima-indians-diabetes-database\/diabetes.csv\"\n\nindex = None\ntarget = \"Outcome\"\n\ndata = utils.load_data(filepath, index, target)\n","bb5c9a3f":"data.sample(5, random_state=seed)","726917c9":"(X, y) = utils.divide_dataset(data, target=\"Outcome\")","105e813a":"X.sample(5, random_state=seed)","65e5a7d8":"y.sample(5, random_state=seed)","fb2452e7":"train_size = 0.75\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","a5a0bd06":"X_train.sample(5, random_state=seed)","9bbfb356":"X_test.sample(5, random_state=seed)","f97fa4cc":"y_train.sample(5, random_state=seed)","addba488":"y_test.sample(5, random_state=seed)","ba9b5113":"data_train = utils.join_dataset(X_train, y_train)\n\ndata_test = utils.join_dataset(X_test, y_test)","342df286":"data_train.sample(5, random_state=seed)","d6754fae":"data_test.sample(5, random_state=seed)","183c54a3":"data_train.shape","0942c1a8":"data_train.info(memory_usage=False)\n","3c1d0051":"y.cat.categories","c2cfb242":"utils.plot_histogram(data_train)","3c7f7fbb":"circular1 = px.pie(data_train, values='Outcome', names='Insulin')\ncircular1.show()","c7be60c0":"circular1 = px.pie(data_train, values='Outcome', names='SkinThickness')\ncircular1.show()","d31dc132":"eliminadas = ['Insulin','SkinThickness']","a8d0e1e2":"utils.plot_barplot(data_train) ","ad487e52":"utils.plot_barplot(data)","d5205aac":"cajas2 = go.Figure()\n\nfor columna in X_train:\n    cajas2.add_trace(go.Box(y=X_train[columna].values, name=X_train[columna].name))\n\ncajas2.show()","79cb366b":"cajas2 = go.Figure()\n\nfor columna in X_train:\n    if columna not in eliminadas:\n        cajas2.add_trace(go.Box(y=X_train[columna].values, name=X_train[columna].name))\n\ncajas2.show()","499b131b":"fig = px.violin(data_train, y=\"Glucose\")\nfig.show()","95b87bb4":"utils.plot_pairplot(data_train, target=\"Outcome\")","04bcb79d":"import plotly.express as px\npx.imshow(data_train.corr(method=\"pearson\"))","2283cef8":"dispersion = px.scatter(data_train, x=\"Age\",y=\"Pregnancies\",color=\"Outcome\",trendline=\"ols\")\ndispersion.show()","42da54a6":"contorno = px.density_contour(data_train, x=\"Age\", y=\"Pregnancies\")\ncontorno.show()","5bb57adf":"dispersion2 = px.scatter(data_train, x=\"DiabetesPedigreeFunction\",y=\"Pregnancies\",color=\"Outcome\",trendline=\"ols\")\ndispersion2.show()","ed08f5a4":"contorno2 = px.density_contour(data_train, x=\"DiabetesPedigreeFunction\", y=\"Pregnancies\")\ncontorno2.show()","2d4f6e1a":"eliminadas = ['Insulin','SkinThickness']\n\ncopiadatos = X_train.copy()\ncopiadatos = copiadatos.drop(eliminadas, axis=1)\ncopiadatos.sample(5)","7bbe7023":"class EliminarVariables():\n    \n    def __init__(self, columnas):\n        self.columnas=columnas\n    \n    def fit(self, x, y=None):\n        return self\n    \n    def transform(self, x, y=None):\n        return x.drop(self.columnas,axis=1,inplace=False)\n        ","4a3f1d7f":"variables = ['Glucose','BloodPressure','BMI']\n\nImputador = ColumnTransformer([('imp1',SimpleImputer(missing_values=0,strategy=\"mean\"), variables)])","6f4a2ab7":"def outlier_rejection(X, y):\n    model = IsolationForest(max_samples=100,\n                            contamination=0.4,\n                            random_state=seed)\n    model.fit(X)\n    y_pred = model.predict(X)\n    return X[y_pred == 1], y[y_pred == 1]\n\n\nEliminarOutliers = FunctionSampler(func=outlier_rejection)","09e42ec0":"Discretizador = KBinsDiscretizer(n_bins=3, strategy=\"uniform\")","02c2af46":"ZeroR = DummyClassifier(strategy=\"most_frequent\")","39034080":"ArbolDecision = DecisionTreeClassifier(random_state=seed)","17a9de84":"eliminadas = ['Insulin','SkinThickness']\n\nPipeline1 = make_pipeline(EliminarVariables(eliminadas),Imputador,EliminarOutliers, ArbolDecision)\nPipeline2 = make_pipeline(Discretizador, ArbolDecision)\nPipeline3 = make_pipeline(EliminarVariables(eliminadas),Imputador,EliminarOutliers, Discretizador, ArbolDecision)","a29e3031":"utils.evaluate(ZeroR,\n               X_train, X_test,\n               y_train, y_test)","5711a223":"utils.evaluate(ArbolDecision,\n               X_train, X_test,\n               y_train, y_test)","15cb7381":"utils.evaluate(Pipeline1,\n               X_train, X_test,\n               y_train, y_test)","cfb14fde":"utils.evaluate(Pipeline2,\n               X_train, X_test,\n               y_train, y_test)","1e4d549f":"utils.evaluate(Pipeline3,\n               X_train, X_test,\n               y_train, y_test)","b883bbc4":"filepath = \"..\/input\/breast-cancer-wisconsin-data\/data.csv\"\n\nindex = \"id\"\ntarget = \"diagnosis\"\n\n\ndata = utils.load_data(filepath, index, target)","9aaa85e3":"data.sample(5, random_state=seed)","ddcbcaf6":"data.info(memory_usage=False)","9aa2f181":"data = data.drop(data.columns[[31]], axis='columns')\ndata.info(memory_usage=False)","21f1d755":"(X, y) = utils.divide_dataset(data, target=\"diagnosis\")","4b538fc0":"X.sample(5, random_state=seed)","350f96c0":"y.sample(5, random_state=seed)","c1be33b5":"train_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","1fcf43f0":"X_train.sample(5, random_state=seed)","fc4d11a8":"y_train.sample(5, random_state=seed)","081bd391":"X_test.sample(5, random_state=seed)","6ef073a5":"y_test.sample(5, random_state=seed)","5c1e40ab":"data_train = utils.join_dataset(X_train, y_train)\ndata_test = utils.join_dataset(X_test, y_test)","6a7edebe":"data_train.sample(5, random_state=seed)","1d625a34":"data_test.sample(5, random_state=seed)","65364447":"data_train.info(memory_usage=False)","455eb715":"utils.plot_barplot(data_train)","92b9226f":"figure(figsize=(20,15))\n\nsb.heatmap(data_train.corr(), annot=True)","e0e29617":"borrado = ['perimeter_mean', 'area_mean', 'perimeter_se', 'area_se', 'perimeter_worst', 'area_worst', \n           'area_worst', 'radius_worst', 'texture_worst', 'concave points_mean', 'compactness_mean', \n           'concave points_worst', 'compactness_worst', 'concavity_worst']\n\ndata_train.drop(borrado, axis='columns', inplace=True)","5da8632e":"figure(figsize=(16,13))\n\nsb.heatmap(data_train.corr(), annot=True)","ec4fee2e":"(X_train2, y_train2) = utils.divide_dataset(data_train, target=\"diagnosis\")\n\nX_train2.sample(5, random_state=seed)","0590de5e":"y_train2.sample(5, random_state=seed)","e50a39d6":"# Estandarizamos los datos\ndata_est = (X_train2 - X_train2.mean()) \/ (X_train2.std()) \n\n# Convertimos los datos para que puedan ser representados en la gr\u00e1fica\ndata_grafica = pd.concat([y_train2, data_est], axis = 1)\ndata_grafica = pd.melt(data_grafica, id_vars = \"diagnosis\",\n                     var_name = \"Variables\",\n                     value_name = \"Estandarizaci\u00f3n\")\n\n# Definimos el tama\u00f1o y el tipo de gr\u00e1fica\npypl.figure(figsize = (25,10))\nsb.boxplot(x=\"Variables\", y=\"Estandarizaci\u00f3n\", hue=\"diagnosis\", data=data_grafica)\n\n# Rotamos el nombre de las variables para que no se solapen\npypl.xticks(rotation = 90)","7a28f1a4":"sb.catplot(x=\"Variables\", y=\"Estandarizaci\u00f3n\", hue=\"diagnosis\", data=data_grafica, height=10, aspect=5\/2, kind=\"swarm\")\npypl.xticks(rotation = 90)","6defb086":"utils.plot_histogram(data_train)","254bc48e":"class EliminarVariables():\n\n    def __init__(self, columnas):\n        self.columnas=columnas\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, x, y=None):\n        return x.drop(self.columnas, axis='columns', inplace=False)","ac771e61":"def outlier_rejection(X, y):\n\n    model = IsolationForest(max_samples=100,\n                            contamination=0.4,\n                            random_state=seed)\n    model.fit(X)\n    y_pred = model.predict(X)\n    return X[y_pred == 1], y[y_pred == 1]\n\nelimOutliers = FunctionSampler(func=outlier_rejection)","951ec90d":"discretizer = KBinsDiscretizer(n_bins=2, strategy=\"kmeans\")","0596d506":"zero_r_model = DummyClassifier(strategy=\"most_frequent\")\n\nutils.evaluate(zero_r_model,\n               X_train, X_test,\n               y_train, y_test)","3bf6b6e2":"tree_model = DecisionTreeClassifier(random_state=seed,\n                                    criterion='entropy',\n                                    min_samples_leaf = 5)\n\nutils.evaluate(tree_model,\n               X_train, X_test,\n               y_train, y_test)","9dd25590":"dot_data = tree.export_graphviz(tree_model, out_file=None, \n                         feature_names=list(X_train),  \n                         class_names=[\"B\", \"M\"],  \n                         filled=True, rounded=True,  \n                         special_characters=True)\n\ngraphviz.Source(dot_data)","326bb048":"sinOutliers_tree_model = make_pipeline(EliminarVariables(borrado),\n                                       elimOutliers,\n                                       tree_model)\n\nutils.evaluate(sinOutliers_tree_model,\n               X_train, X_test,\n               y_train, y_test)","ece837c8":"dot_data = tree.export_graphviz(tree_model, out_file=None, \n                         feature_names=list(X_train2),  \n                         class_names=[\"B\", \"M\"],  \n                         filled=True, rounded=True,  \n                         special_characters=True)\n\ngraphviz.Source(dot_data)","d78feb34":"discretize_tree_model = make_pipeline(discretizer, \n                                      tree_model)\n\nutils.evaluate(discretize_tree_model,\n               X_train, X_test,\n               y_train, y_test)","81916be3":"discretize_tree_model = make_pipeline(EliminarVariables(borrado),\n                                      elimOutliers, \n                                      discretizer, \n                                      tree_model)\n\nutils.evaluate(discretize_tree_model,\n               X_train, X_test,\n               y_train, y_test)","a3819ad5":"Y comprobamos que los 4 conjuntos resultantes son correctos:","d1241a4a":"# Pr\u00e1ctica 1: An\u00e1lisis exploratorio de datos, preprocesamiento y validaci\u00f3n de modelos de clasificaci\u00f3n\\*\n\n### Miner\u00eda de Datos: Curso acad\u00e9mico 2020-2021\n\n### Profesorado:\n\n* Juan Carlos Alfaro Jim\u00e9nez\n* Jos\u00e9 Antonio G\u00e1mez Mart\u00edn\n\n\\* Adaptado de las pr\u00e1cticas de Jacinto Arias Mart\u00ednez y Enrique Gonz\u00e1lez Rodrigo\n\n\n### Grupo H:\n\n* Alejandro Fern\u00e1ndez Arjona\n* Pablo Torrijos Arenas","2cb8ee8c":"An\u00e1lisis univariado (involucra una sola variable):\n* Histogramas para las variables num\u00e9ricas\n* Diagramas circulares para observar informaci\u00f3n de `Insulin` y `SkinThickness`\n* Diagramas de barras para la variable clase `Outcome`\n* Diagramas de cajas\n* Diagramas de viol\u00edn\n","63f2824a":"Vamos a usar primero el algoritmo Zero-R, aunque no tenga mucha precisi\u00f3n, nos puede dar una idea para la precisi\u00f3n que debemos conseguir con otros modelos. Para usar el algoritmo Zero-R, recurrimos al estimador `DummyClassifier` de `scikit-learn`:","95bbc372":"### 3.2.2 An\u00e1lisis multivariado","dbc9f85a":"Vamos a realizar un diagrama de cajas para observar las variables predictoras y sus cuartiles. Los diagramas de cajas sirven tambi\u00e9n para encontrar los outliers r\u00e1pidamente. Usamos el conjunto X_train ya que la variable clase no nos interesa analizarla en este caso, siempre toma valores 0 o 1.","826b73f1":"### 3.2.2 An\u00e1lisis univariado\n\nAhora, vamos a realizar un an\u00e1lisis univariado de las variables. Primero vamos a comprobar si nuestros datos tienen *outliers*. Para ello, nos har\u00e1 falta dividir nuestro nuevo `data_train` en `X_train2` e `y_train2`, del mismo modo que hicimos anteriormente, y comprobar que est\u00e1n bien divididos.","cb67fda1":"Podemos realizar otro diagrama de barras con el conjunto de datos original (data) en lugar de usar el conjunto de datos de entrenamiento (data_train), para comprobar si hab\u00edamos estratificado correctamente:","94af38f5":"Como hemos dicho antes, el conjunto de datos `Diabetes` tiene 768 instancias y 9 variables. Vamos a ver esta informaci\u00f3n sobre el conjunto de entrenamiento con el atributo shape.","c7bfd445":"### \u00c1rbol de decisi\u00f3n con los transformadores de limpieza","0d0195c3":"En esta pr\u00e1ctica hemos trabajado algunos de los aspectos m\u00e1s importantes del proceso *KDD* (*Knowledge Discovery from Data*):\n\n* Almacenamiento y carga de datos\n* An\u00e1lisis exploratorio de datos\n* Preprocesamiento de datos\n* Validaci\u00f3n de modelos de clasificaci\u00f3n\n\nPara la visualizaci\u00f3n de los datos, adem\u00e1s de las librer\u00edas `pandas` y `plotly`, hemos usado algunas librer\u00edas auxiliares, como `seaborn` y `graphviz`. Para los algoritmos de clasificaci\u00f3n hemos usado `scikit-learn`.\n\nHemos realizado los estudios sobre las base de datos `pima_diabetes` y `wisconsin`:\n\n- `pima_diabetes`: https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database\n- `wisconsin`: https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data\n\n---\n\nVamos a empezar realizando nuestro an\u00e1lisis para la base de datos `pima_diabetes`.","6548dab2":"De nuevo, comprobamos que se haya separado correctamente. Comenzamos con las variables predictoras:","aaf54872":"##\u00a02. Acceso y almacenamiento de datos","f1c0c68e":"Tenemos que darnos cuenta de que el conjunto de datos `diabetes` no tiene una variable identificadora, por lo que definimos como identificador `None` y el m\u00e9todo crear\u00e1 una variable Id autom\u00e1ticamente. Definimos como variable objetivo `Outcome`.\n\nUna vez hemos cargado el conjunto de datos es fundamental comprobar que el proceso ha funcionado sin problemas, y que las variables y los valores est\u00e1n dentro de lo esperado. Para ello, podemos escoger una instancia al azar o mostrar las primeras instancias del conjunto de datos.\n\nPodemos usar la funci\u00f3n `sample` para obtener una muestra aleatoria de `n` instancias del conjunto de datos, ya que con el m\u00e9todo `head` obtendr\u00edamos una muestra muy sesgada:","c63c042b":"Como hab\u00edamos dicho, exist\u00eda una correlaci\u00f3n peque\u00f1a, y aqu\u00ed podemos apreciar que hay mucha dispersi\u00f3n entre las variables. Esto se debe a que las mujeres no tienen una edad fija a la que quedarse embarazadas, ni un n\u00famero fijo de embarazos, sino que cada caso es muy diferente, aunque exista cierta correlaci\u00f3n.","0a9ffe00":"La variable `Pregnancies` sigue una distribuci\u00f3n normal con asimetr\u00eda positiva, y la mayor\u00eda de casos se encuentran entre 0 y 10 embarazos. No parece muy l\u00f3gico que haya tantos casos de mujeres con m\u00e1s de 4 casos, ya que en mi opini\u00f3n la gran mayor\u00eda de mujeres suelen tener entre 0 y 4 embarazos, no es lo normal que de 768 mujeres 38 de ellas hayan pasado por 8 embarazos. Esos datos los tendremos en cuenta aunque no parezcan l\u00f3gicos, ya que tampoco podemos considerarlos como outlayers ni como ruido. Lo que s\u00ed podemos considerar como outlayers son las mujeres que hayan tenido m\u00e1s de 12 embarazos, llegando incluso a 17. Estos valores los podemos eliminar.\n\nLas variables `Glucose`, `BloodPressure`, `BMI` y `DiabetesPedigreeFunction` muestran una distribuci\u00f3n con tendencia central (teniendo la \u00faltima de ellas una asimetr\u00eda positiva), es decir, tambi\u00e9n siguen una distribuci\u00f3n normal. Tambi\u00e9n debemos destacar que hay algunos casos cuyo nivel de glucosa, \u00edndice de masa corporal o presi\u00f3n sangu\u00ednea es 0, lo cual significa que se trata de valores perdidos y debemos imputarlos.  En cuanto a `BMI` y `DiabetesPedigreeFunction` tambi\u00e9n podemos destacar que hay algunos outlayers que debemos eliminar. Por ejemplo, en cuanto al \u00edndice de masa corporal, aunque lo m\u00e1s com\u00fan son los valores entre 20 y 40, existen casos con mayor IMC, pero no tanto como 60 o 68 como se ve en la gr\u00e1fica, ya que la obesidad extrema comprende desde 40 hasta 55 de IMC. En cuanto a la funci\u00f3n de pedigree de diabetes, tambi\u00e9n hay unos cuantos valores outlayers que eliminaremos m\u00e1s adelante. \n\nComo se puede observar, la variable `Insulina` parece seguir una distribuci\u00f3n exponencial, tieniendo la mayor\u00eda de los casos valor `0`. Sin embargo, si nos paramos a analizar un poco el significado de esta gr\u00e1fica, nos damos cuenta de que todos esos datos con valor de insulina igual a cero son err\u00f3neos. Una persona no puede generar cero de insulina, morir\u00eda, por lo que se trata de valores perdidos. En las variables que he comentado antes vamos a tratar de imputar los valores perdidos, sin embargo, en este caso, esos errores suponen el 49.8% de la variable, por lo que estar\u00edamos tratando con unos datos muy sesgados. En este caso, vamos a eliminar la variable y a no tenerla en cuenta para nuestro problema.\n\nPor \u00faltimo, la variable `SkinThickness` (distribuci\u00f3n normal), que mide el grosor de la piel, tambi\u00e9n tiene una gran cantidad de datos perdidos: suponen el 32.3% del total. Adem\u00e1s, no parece normal que el grosor de la piel est\u00e9 concentrado entre 15 y 40 mil\u00edmetros, ya que lo normal es entre 0.5mm y 4.0mm. En cualquier caso, esa enorme cantidad de datos perdidos es motivo suficiente para eliminar la variable y no tenerla en cuenta, al igual que hemos comentado respecto a la variable `Insulina`.\n\nPor \u00faltimo, la variable `Age` presenta de nuevo una distribuci\u00f3n normal, y en este caso no parece haber valores perdidos ni outlayers, ya que todas las edades se encuentran entre 20 y 80 a\u00f1os.","2e840231":"Podemos observar que la clase 0 (No diabetes) tiene el 65.1% de los casos, y la clase 1 (S\u00ed diabetes) el resto de los casos, es decir, 34.9%. Esto quiere decir que el problema no est\u00e1 `balanceado`.\n","fd9efb22":"# Breast Cancer Wisconsin (Diagnostic) Data Set","48442fde":"## 2. Acceso y almacenamiento de datos","4205d447":"Aunque los registros con valor mayor que cero no se aprecien bien, se ve claramente que casi la mitad de las instancias toman valor cero, por lo que, como ya hemos dicho, no debemos tener en cuenta esta variable predictora.","30e9f51c":"Podemos crear ahora un gr\u00e1fico de dispersi\u00f3n entre `Age` y `Pregnancies`, para comprobar la relaci\u00f3n que hemos comentado que exist\u00eda entre esas 2 variables. Separamos con colores seg\u00fan el `Outcome` sea 0 o 1.","dde88007":"## 5.1 Algoritmos de clasificaci\u00f3n","632e7c43":"Como era de esperar, el modelo *Zero-R* obtiene malos resultados, solo predice la clase mayoritaria (0). Si la clase estuviera balanceada, obtendr\u00edamos una precisi\u00f3n todav\u00eda peor, del 50%. Vamos a probar otros modelos para ver cu\u00e1nto mejora la precisi\u00f3n.","d71169c0":"An\u00e1lisis multivariado (involucra varias variables):\n* Matriz de gr\u00e1ficos de nube de puntos\n* Mapa de calor\n* Gr\u00e1ficos de dispersi\u00f3n\n* Diagramas de  densidad de contorno","d0780ef2":"Podemos observar, al igual que en el diagrama de cajas, que la mayor\u00eda de los registros se encuentran en el intervalo [100-150]. Adem\u00e1s, vemos como hay esos registros con valor 0 que deberemos imputar. ","536cd525":"Ahora comenzamos con la tarea m\u00e1s importante de toda la pr\u00e1ctica, el preprocesamiento de los datos. Vamos a transdormar los datos crudos en informaci\u00f3n m\u00e1s accesible para los algoritmos de aprendizaje. Vamos a realizar una limpieza y una discretizaci\u00f3n del conjunto de datos, todo ello dentro de un pipeline, para evitar fugas de datos.","ca8cb29c":"Para empezar, vamos a hacer un an\u00e1lisis univariado para identificar ruido y outliers en las variables de nuestro conjunto de datos.","f50af742":"Para facilitar el an\u00e1lisis exploratorio de datos, juntamos de nuevo las variables predictores con la variable clase.","38426deb":"Podemos ver c\u00f3mo contamos con *outliers* en todas las variables, por lo que los tendremos que eliminar posteriormente en el *pipeline*.\n\nAdem\u00e1s, en esta gr\u00e1fica podemos ver c\u00f3mo variables como `radius_mean` o `concavity_mean` pueden ser muy buenas para la clasificaci\u00f3n ya que est\u00e1n muy diferenciadas entre `B` y `M`. Todo lo contrario pasa con otras variables como `texture_se` o `smoothness_se`, que ser\u00e1 dif\u00edcil que sean \u00fatiles para nuestro \u00e1rbol de clasificaci\u00f3n.\n\n---\n\nAhora vamos a representar cada uno de los puntos de nuestro conjunto de test. En *plotly* podr\u00edamos hacer algo parecido usando:\n\n> px.strip(data_grafica, x=\"Variables\", y=\"Estandarizaci\u00f3n\", color=\"diagnosis\")\n\nSin embargo, tambi\u00e9n lo vamos a realizar con *Seaborn* ya que se ve mucho mejor.","cd1d3093":"## 4. Preprocesamiento de datos\n\n---\n\n### 4.1. Eliminar variables\n\nPara eliminar las variables vamos a crear esta funci\u00f3n simple (`EliminarVariables`) con `fit` y `transform` para poderlo usar en el *pipeline*. Simplemente tenemos que pasar por par\u00e1metro las variables que queremos borrar.","6b5fb15b":"Como podemos ver, solo aparecen las variables que dejamos en el an\u00e1lisis exploratorio, por lo que el *pipeline* parece haber funcionado correctamente. Este \u00e1rbol es pr\u00e1cticamente del mismo tama\u00f1o que el anterior, ya que como en nuestra base de datos tenemos pocas instancias, el pasar de 30 a 17 variables predictoras no hace que disminuya el tama\u00f1o de \u00e9ste ya que se queda antes sin instancias que sin variables. Con una base de datos m\u00e1s grande, seguramente s\u00ed que podr\u00edamos conseguir un modelo m\u00e1s compacto que sobreajuste menos.\n\n___\n\n### 5.3. Algoritmo CART (Classification and Regression Trees) con discretizaci\u00f3n\n\n---\n\n### 5.3.1. Algoritmo CART discretizando, pero sin eliminar variables y outliers.\n\nAhora vamos a comprobar el rendimiento del \u00e1rbol de decisi\u00f3n cuando le aplicamos el discretizador por `kmeans` con 2 *bins*. ","4794a589":"Como era evidente, los \u00e1rboles de decisi\u00f3n obtienen mejores resultados que Zero_R, que tiene un 65.1% de precisi\u00f3n, lo cual es l\u00f3gico teniendo en cuenta que ese es el porcentaje de ceros que hay en la variable clase; tanto en el conjunto de datos original, como en el conjunto de entrenamiento (gracias a que hemos estratificado correctamente). No importa que discreticemos o no, ni que usemos o no los transformadores de limpieza, todos los modelos con \u00e1rbol de clasificaci\u00f3n superan al algoritmo Zero_R.\n\nUsando simplemente \u00e1rboles de decisi\u00f3n, obtenemos una tasa de acierto del 66.67%, lo cual supone una mejora de m\u00e1s de 1.5% respecto al Zero_R.\n\nSi usamos tambi\u00e9n los transformadores de limpieza, obtenemos una precisi\u00f3n del 67.2%, que indica una mejora del 0.5% aproximadamente respecto a no usar esos transformadores.\n\nSi usamos \u00e1rboles de decisi\u00f3n y discretizamos el conjunto de datos (al igual que se hac\u00eda en la libreta de `iris`), se obtiene una tasa de acierto de 68.22%, mejorando en un 1.5% a su versi\u00f3n sin discretizar.\n\nY por \u00faltimo, si con \u00e1rboles de decisi\u00f3n usamos los transformadores de limpieza y discretizamos el conjunto de datos, conseguimos la mayor precisi\u00f3n de todas, 71.35%.\n\nEstos resultados pueden variar mucho seg\u00fan la semilla que elijamos (por la aleatoriedad) y porque estas base de datos con las que estamos trabajando son muy peque\u00f1as, y fallar o acertar la predicci\u00f3n en un par de registros puede variar mucho los resultados. Para obtener resultados m\u00e1s fiables, podr\u00edamos ejecutar estos mismos algoritmos con 100 semillas distintas (por ejemplo), y devolver como medida de precisi\u00f3n la media de todas esas pruebas. Otra opci\u00f3n ser\u00eda realizar una validaci\u00f3n cruzada, para evitar resultados demasiado buenos o demasiado malos.","606dc3b3":"Antes de comenzar el preprocesamiento, vamos a analizar las variables y sus relaciones, mediante gr\u00e1ficos y estad\u00edsticos. Usamos las libre\u00edas `Pandas`, `Plotly` y `Seaborn`.","4907ba37":"## 3.2 Visualizaci\u00f3n de las variables","d9fa5d24":"En el algoritmo *Zero-R* siempre se predecir\u00e1 la clase mayoritaria, en este caso B (62,81% en los datos de entrenamiento). La tasa de acierto es por tanto un n\u00famero muy cercano a dicho porcentaje (0,62573), ya que 0,6281 ser\u00eda dicha tasa si utiliz\u00e1semos el conjunto de datos de entrenamiento como test. Este rendimento es muy malo, ovbiamente, ya que tenemos que predecir si el c\u00e1ncer de mama es benigno o maligno, y para todos los casos dir\u00edamos que es benigno.\n\n---\n\n### 5.2. Algoritmo *CART* (*Classification and Regression Trees*)\n\n---\n\n### 5.2.1. Algoritmo *CART* sin eliminar variables y *outliers*.\n\nAhora vamos a probar los modelos basados en \u00e1rboles de clasificaci\u00f3n sin discretizar.\n\nPrimero creamos nuestro clasificador `DecisionTreeClassifier`, que usaremos para todas las dem\u00e1s modelos de \u00e1rboles de clasificaci\u00f3n. Como hiperpar\u00e1metros incluiremos la semilla, que garanzita que los resultados sean reproducibles; vamos a establecer como criterio la entrop\u00eda en lugar de Gini, ya que es el que m\u00e1s hemos usado en asignaturas anteriores y en la parte de teor\u00eda de Miner\u00eda de Datos; y establecemos a 4 el m\u00ednimo de hojas para no realizar un sobreajuste demasiado grande en las hojas del \u00e1rbol.\n\nAhora, vamos a comprobar el rendimiento del clasificador usando la base de datos original:","cbb1bae5":"Y podemos ver c\u00f3mo se crea una \u00faltima columna sin ning\u00fan dato ni nombre. Esto es debido a que en el fichero .csv la l\u00ednea con el nombre de las variables acaba con una coma, por lo que Pandas detecta una variable m\u00e1s sin ning\u00fan nombre especificado. Para arreglarlo, simplemente borramos esa columna, aunque tambi\u00e9n podr\u00edamos haber modificado el archivo .csv.","1e108fa6":"Podemos ver que en este caso el porcentaje de acierto ha bajado de un 92,398% a un 91,913%. Este resultado es m\u00e1s extra\u00f1o que los anteriores ya que para la discretizaci\u00f3n s\u00ed que es especialmente \u00fatil la eliminaci\u00f3n de outliers que no alteren artificialmente los intervalos, por lo que podemos presuponer que esta peque\u00f1a reducci\u00f3n del *accuracy* viene dada simplemente por tener mala suerte con estos datos en concreto o con la semilla escogida en los casos que se use la aleatorizaci\u00f3n.","1abf038e":"Vamos a usar ahora un \u00e1rbol de decisi\u00f3n, usamos el estimador `DecisionTreeClassifier` de `scikit-learn`. Usamos la misma semilla, como siempre, para que las pruebas sean reprodubibles.","65c06969":"Y volvemos a comprobar que los conjuntos creados son correctos:","405830fe":"Podemos ver c\u00f3mo efectivamente variables como `radius_mean`, `radius_se`, `radius_worst` o `concavity_mean` crean una divisi\u00f3n casi perfecta entre los casos Benignos y Malignos, mientras que otras como `texture_se`, `smoothness_se`, `fractal_dimension_mean` o `symmetry_se` est\u00e1n totalmente mezcladas.\n\nEl que haya variables que puedan crear una buena divisi\u00f3n por s\u00ed mismas nos dice que el modelo que generemos finalmente probablemente tenga un gran porcentaje de acierto, ya que simplemente usando esa clase se conseguir\u00eda un resultado decente.\n\n---\n\nAdem\u00e1s, vamos a observar el histograma de las variables:","e5f4f4f8":"##\u00a04.2 Discretizaci\u00f3n","6d112a66":"Tal y como se puede observar, el conjunto de datos de entrenamiento est\u00e1 formado por 576 casos y 9 variables (8 variables predictoras y 1 variable clase, Outcome).\n\nPara conocer el tipo de las variables usamos `info`:","d5d93a8a":"Es muy \u00fatil disponer del conjunto de datos separado dos subconjuntos, uno con las variables predictoras (`X`) y otro con la variable objetivo (`y`). Se puede utilizar el siguiente fragmento de c\u00f3digo para dividirlo: ","0bf9f556":"### 3.2.1 An\u00e1lisis univariado","3d2302ce":"Ahora es el momento de entrenar y validar nuestros clasificadores. Para ello, vamos a usar una matriz de confusi\u00f3n y tasa de acierto.\n\n### Zero_R","10792b5a":"Podemos ver c\u00f3mo no es excesivamente grande, y utiliza varias variables como `permieter_worst` o `concave points_worst` que nosotros hemos eliminado en el an\u00e1lisis exploratorio de datos, por lo que si no hemos realizado bien el proceso de selecci\u00f3n de variables, el resultado al borrarlas podr\u00eda verse muy afectado.\n\n---\n\n### 5.2.2. Algoritmo CART eliminando variables y outliers.\n\nAhora vamos a realizar un *pipeline* realizando la eliminaci\u00f3n de las variables que ten\u00edan una gran correlaci\u00f3n en el an\u00e1lisis exploratorio, y de los outliers que tambi\u00e9n encontramos en dicho an\u00e1lisis antes de evaluar el \u00e1rbol de clasificaci\u00f3n.","27cf6598":"Aleatorizamos las instancias del conjunto de datos (`shuffle=True`, valor por defecto aunque no lo especifiquemos) para evitar que eliminar todas las instancias de alguna clase en conjuntos de datos que est\u00e9n ordenados por la variable clase. No es nuestro caso, ya que la variable `Outcome` toma valores 0 y 1 alternadamente a lo largo de los 768 registros, pero no est\u00e1 mal que aleatoricemos los datos igualmente.\n\nMediante la semilla, con `random_state`, conseguimos las mismas particiones de training y test cada vez que ejecutemos el algoritmo, para conseguir la reproducibilidad de los experimentos.\n\nAl igual que en el caso que se nos dio de `iris`, hemos aplicado un *holdout* estratificado (`stratify=y`), para conservar la proporci\u00f3n de ejemplos de cada clase durante la revisi\u00f3n. Es muy importante que lo hagamos con esta base de datos, ya que al ser un problema desbalanceado, podr\u00edamos eliminar mucha informaci\u00f3n si no lo hacemos.\n\nDe nuevo, vamos a asegurarnos de que el conjunto de datos se ha dividido correctamente en entrenamiento y prueba. Comenzamos con las variables predictoras del conjunto de datos de entrenamiento:","8a97aa76":"---\n\nAhora vamos a realizar los mismos pasos, pero para la base de datos `Wisconsin`.","608e51f0":"Como se puede apreciar, no existe pr\u00e1cticamente correlaci\u00f3n entre ningun par de variables de nuestro problema. La \u00fanica pareja que supera el umbral 0.5 es la correlaci\u00f3n entre edad y n\u00famero de embarazos. Tiene sentido que a mayor edad, m\u00e1s n\u00famero de embarazos se tengan, pero realmente un valor de correlaci\u00f3n de 0.558 no es una correlaci\u00f3n muy fuerte.","6dd37e33":"Comenzamos cargando el conjunto de datos `diabetes`:","0e4bff68":"## 3. An\u00e1lisis exploratorio de datos","ec1601ec":"## 5.2 Evaluaci\u00f3n de modelos","dd88daa0":"Y continuamos con la variable clase:","3356f301":"Como podemos ver, ahora ya contamos con las 31 columnas que tenemos que tener.\n\nAhora, vamos a dividir nuestra base de datos en los conjuntos de variables predictoras (`X`), y variables predictivas (`y`).","cad59cd8":"Queremos imputar los datos de las variables `Glucose`, `BloodPressure` y `BMI`, como ya hemos dicho antes. Para ello necesitamos usar ColumnTransformer, al cual debemos indicarle qu\u00e9 tipo de imputador queremos usar (por ejemplo, SimpleImputer con la media), y las variables que queremos modificar.","f24f70ca":"Vamos a obtener una muestra aleatoria de ambos conjuntos:","8b3760e6":"Podemos destacar que `Glucose`, `BloodPressure` y `BMI` son variables sim\u00e9tricas, ya que la mediana se encuentra en el centro del rect\u00e1ngulo. Respecto a esas 3 variables, tambi\u00e9n podemos comprobar lo que dijimos antes, que tienen algunos datos perdidos que toman valor 0. `Pregnancies` y `DiabetesPedigreeFunction` tambi\u00e9n tienen registros con valor 0, pero como ya hemos dicho antes, en estas variables es completamente normal.\n\nEn cuanto a `Pregnancies`, salen valores muy altos, ya que por ejemplo seg\u00fan este diagrama la mediana se encuentra en 3, y existen m\u00faltiples registros con valor por encima de 10 embarazos.\n\nPor \u00faltimo, se pueden ver outlayers en todas las variables, excepto en la de `Glucose`","9a738fd7":"Como parecen correctos, para facilitar el an\u00e1lisis exploratorio posterior, vamos a juntar tanto X_train e y_train, como X_test e y_test:","f93f8ad9":"Una vez divididos, y comprobado que la divisi\u00f3n se ha realizado correctamente, vamos a representar un gr\u00e1fico de cajas en el que podremos ver si contamos con outliers, adem\u00e1s de la distribuci\u00f3n de cada variable para la clase. Para ello, primero tenemos que estandarizar los datos, creando `data_est`.\n\nUna versi\u00f3n usando *plotly* en lugar de *seaborn* ser\u00eda:\n> px.box(data_grafica, x=\"Variables\", y=\"Estandarizaci\u00f3n\", color=\"diagnosis\")\n\nSin embargo, como en ocasiones *plotly* no funciona o relantiza mucho *Kaggle*, adem\u00e1s de que en este caso se ven m\u00e1s claro los datos con *Seaborn*, nos quedaremos con esta \u00faltima librer\u00eda. ","d37dd211":"Efectivamente, en este caso existe una dispersi\u00f3n todav\u00eda mayor, debido a que es completamente indeferente el n\u00famero de embarazos que una mujer haya tenido, con su funci\u00f3n de pedigr\u00ed de diabetes.","e2ce804c":"Vamos a realizar ahora un mapa de calor (heatmap) para ver mejor la correlaci\u00f3n que existe entre las distintas variables predictoras de nuestra base de datos:","86c2c910":"Vamos a almacenar en una lista las dos variables a eliminar, para futuros usos.","6889dda7":"El primer conjunto de datos que usaremos es `diabetes`. Es original del Instituto Nacional de Diabetes y Enfermedades Digestivas y del ri\u00f1\u00f3n. El objetivo es intentar predecir si un paciente tiene o no diabetes, bas\u00e1ndonos en ciertas variables predictores. Se trata de 768 instancias, mujeres de entre 20 y 81 a\u00f1os, un extracto muy peque\u00f1o de la colecci\u00f3n de datos original.\n\nLa variable objetivo es `Outcome`, y estos son los posibles Outcomes:\n\n* `0`: Predicci\u00f3n de que el paciente no tiene diabetes.\n* `1`: Predicci\u00f3n de que el paciente tiene diabetes.\n\nLas distintas variables predictoras son las siguientes:\n\n* `Pregnancies`: N\u00famero de embarazos.\n* `Glucose`: Concentraci\u00f3n de glucosa en plasma (tras 2 horas de un test de tolerancia a glucosa oral).\n* `BloodPressure`: Presi\u00f3n arterial diast\u00f3lica (en mil\u00edmetros de mercurio).\n* `SkinThickness`: Espesor del pliegue cut\u00e1neo del tr\u00edceps (en mil\u00edmetros).\n* `Insulin`: Insulina en suero 2-horas.\n* `BMI`: \u00cdndice de masa corporal (kg \/ m^2).\n* `DiabetesPedigree`: Funci\u00f3n de pedigr\u00ed de diabetes.\n* `Age`: Edad (en a\u00f1os).\n\nEl objetivo ser\u00eda clasificar una nueva instancia (cuyo `Outcome` es desconocido) en funci\u00f3n de sus propiedades.","a763ba88":"Comprobamos que la carga se ha realizado correctamente, mediante un muestreo de 5 instancias:","ee60d043":"En el ejemplo de `iris` era evidente que dividir en 3 secciones era una buena opci\u00f3n viendo las gr\u00e1ficas, por que las 3 clases estaban muy diferenciadas, pero en el caso de `Diabetes`, no se puede extraer una informaci\u00f3n clara de las gr\u00e1ficas que hemos visto, por lo que usamos el mismo discretizador que us\u00e1bamos en `iris`: discretizaci\u00f3n uniforme, dividiendo en 3 intervalos de igual anchura.","75a5c3e6":"Al igual que antes, comprobamos que el conjunto de datos se haya separado correctamente:","87920080":"## 3.1 Descripci\u00f3n del conjunto de datos","e92c4a72":"### Eliminar outliers","4d4c5fb7":"Para crear un *pipeline*, vamos a usar la funci\u00f3n `make_pipeline` de `scikit-learn`. Esta toma como par\u00e1metros la lista de transformadores a aplicar al conjunto de datos y, al final de este, el estimador a utilizar.\n\nLos transformadores de limpieza que hemos son:\n* Eliminar las variables `Insulin` y `SkinThickness`.\n* Imputar valores perdidos de `Glucose`, `BloodPressure` y `BMI`.\n* Eliminar outliers de todas las variables que los tengan.\n\nVamos a evaluar 5 modelos distintos para comprobar como afectan los distintas transformadores a los resultados:\n* Usando el algoritmo `ZeroR`.\n* Usando el algoritmo `DecisionTreeClassifier`.\n* Pipeline usando `DecisionTreeClassifier` y los transformadores de limpieza.\n* Pipeline usando `DecisionTreeClassifier` y `KBinsDiscretizer`.\n* Pipeline usando `DecisionTreeClassifier`, los transformadores de limpieza y `KBinsDiscretizer`.","6c306b84":"Vemos como la tasa de aciertos ha disminuido del 93,567% al 90,058%. No es una diferencia muy grande teniendo en cuenta que los conjuntos de entrenamiento y test no son muy grandes, por lo que el resultado se puede ver bastante afectado dependiendo de la semilla que hayamos definido. A\u00fan as\u00ed, dicho resultado tiene sentido ya que los \u00e1rboles de clasificaci\u00f3n con variables num\u00e9ricas no se ven afectados por el hecho de tener variables muy correlacionadas entre s\u00ed, ni les castiga excesivamente la presencia de algunos outliers.\n\nAs\u00ed, con poco que eliminemos alguna variable que sea un algo mejor clasificando que las que dejamos, o que por lo que sea viene mejor para los datos de test que tenemos, ya reduciremos sensiblemente la tasa de acierto.\n\n---\n\nAl igual que antes, tambi\u00e9n vamos a representar el \u00e1rbol:","86799916":"Y ahora dividimos nuestros datos en los conjuntos de entrenamiento y prueba para `X` e `y`, con unos porcentajes de 70% entrenamiento y 30% prueba. Adem\u00e1s, al hacerlo los datos se aleatorizan, evitando as\u00ed problemas con bases de datos ordenadas.","957f2ff9":"##\u00a05. Algoritmos de clasificaci\u00f3n y evaluaci\u00f3n de modelos","113d8e7e":"Por \u00faltimo, finalizamos con la variable objetivo del conjunto de datos de entrenamiento:","e80f0331":"Sin embargo, queremos eliminar las variables dentro del pipeline, al igual que la normalizaci\u00f3n. Para ello, tenemos que crear una clase que implemente los m\u00e9todos fit( ) y transform( ), y el constructor debe recibir como par\u00e1metro las columnas a eliminar.","76a1f86c":"### \u00c1rbol de decisi\u00f3n","d7053fd6":"### \u00c1rbol de decisi\u00f3n discretizando el conjunto de datos","8d5f4f77":"Ahora, vamos a cargar los datos de la base de datos desde el fichero .csv, y definimos el nombre de la columna de id y de la clase.","42c499e9":"Vamos a usar los mismos *imports* y la misma semilla que en el an\u00e1lisis exploratorio de la base de datos `pima_diabetes`, por lo que no debemos realizar nada en este apartado.","15f7538c":"### 3.1 Descripci\u00f3n del conjunto de datos\n\nPrimero vamos a ver el tama\u00f1o de nuestro problema, conociendo el tama\u00f1o los conjuntos de datos que hemos creado, y los tipos de variables que \u00e9stos tienen.","7cdccb22":"Aunque hemos calculado el n\u00famero de valores perdidos manualmente, vamos a ver ahora esa cantidad con un gr\u00e1fico circular. Empezamos con `Insuline`:","3857247a":"En este caso, casi un tercio de los datos son datos err\u00f3neos, por lo que tambi\u00e9n tenemos que eliminar esta variable.","98147161":"Vamos a realizar ahora un diagrama de viol\u00edn para `Glucose` por ejemplo. Es similar a un diagrama de cajas pero a\u00f1adiendo densidad a ambos lados del diagrama:","e9e8756f":"Podemos crear ahora otro gr\u00e1fico de dispersi\u00f3n y otro de densidad de contorno entre dos variables que tengan correlaci\u00f3n casi nula (0), para ver la diferencia respecto a estos dos diagramas. Por ejemplo, `Pregnancies` y `DiabetesPedigreeFunction`.\n","9cb2f6aa":"De este an\u00e1lisis de correlaci\u00f3n podemos obtener conclusiones valiosas (poniendo el umbral entre muy correlacionadas y no en 0,9):\n* Por un lado, las variables `radius_mean`, `perimeter_mean` y `area_mean` est\u00e1n muy correlacionadas, lo cual tiene mucho sentido viendo que tanto el per\u00edmetro como el \u00e1rea son funciones matem\u00e1ticas basadas en la multiplicaci\u00f3n del radio por un n\u00famero. Lo mismo pasa entre `radius_se`, `perimeter_se` y `area_se`; y entre `radius_worst`, `perimeter_worst` y `area_worst`. Por l\u00f3gica, nos vamos a quedar con los valores del radio ya que los otros est\u00e1n basados en \u00e9l. Por tanto, nos quedamos solo con `radius_mean`, `radius_sd` y `radius_worst`.\n* Adem\u00e1s, la media del radio `radius_mean` est\u00e1 muy correlacionada con su peor valor `radius_worst`, por lo que podemos eliminar este \u00faltimo.\n* Con `texture_mean` y `texture_worst` pasa exactamente lo mismo, por lo que tambi\u00e9n eliminaremos `texture_worst`.\n* `concavity_mean`, `concave points_mean`, `compactness_mean` est\u00e1n muy relacionadas, por lo que nos quedamos con `concavity_mean`, y con `concavity_worst`, `concave points_worst` y `compactness_worst` ocurre lo mismo, qued\u00e1ndonos con `concavity_worst`.\n* Pero adem\u00e1s, `concavity_mean` y `concavity_worst` tambi\u00e9n est\u00e1n muy correlacionadas, por lo que eliminamos esta \u00faltima.\n\nPor ello, vamos a eliminar las variables comentadas anteriormente de `data_train`, simplificando as\u00ed la base de datos perdiendo la m\u00ednima informaci\u00f3n posible. \n\nEsto lo hacemos para continuar la visualizaci\u00f3n de las variables solo con las que nos interesan, aunque posteriormente hagamos el borrado dentro del *pipeline* ya que no usaremos `data_train` si no `X_train` e `y_train`.","0f3b6262":"Cargamos las librer\u00edas que vamos a usar posteriormente:","db7b43bd":"Podemos ver c\u00f3mo hemos obtenido un porcentaje de acierto del 92,398%, algo menor que el 93,567% que obten\u00edamos sin discretizar. Con esta variaci\u00f3n tan peque\u00f1a no podemos obtener ninguna conclusi\u00f3n sobre cu\u00e1l es mejor, aunque es un buen valor teniendo en cuenta que los \u00e1rboles de clasificaci\u00f3n trabajan especialmente bien con variables continuas.\n\n___\n\n### 5.3.2. Algoritmo *CART* discretizando y eliminando variables y *outliers*.\n\nPor \u00faltimo, vamos a aplicar la eliminaci\u00f3n de variables y *outliers* junto con la discretizaci\u00f3n.","60da800c":"Podemos ver que tenemos 398 instancias, en las cuales contamos con 30 variables predictoras num\u00e9ricas, y una variable clase categ\u00f3rica (la variable clase). Adem\u00e1s, esta variable clase tiene 2 estados posibles, `B` y `M`, habiendo bastantes m\u00e1s casos de `B` que de `M` por lo que nuestra muestra no est\u00e1 balanceada.\n\nEn cuanto a las variables predictoras, se dividen en 3 conjuntos atendiendo a la nomenclatura de las variables:\n* Medias: Las 10 primeras variables son medias de distintos par\u00e1metros de las c\u00e9lulas.\n* Desviaci\u00f3n t\u00edpica: Las 10 siguientes son las desviaciones t\u00edpicas de dichos par\u00e1metros.\n* Peor: Y las 10 \u00faltimas son el peor de los casos para cada variable, de entre las c\u00e9lulas observadas.","ca8830de":"Es decir, nuestra variable clase es bivariada (tiene 2 posibles valores).","cddce5a3":"Como se puede observar, se matiene el mismo porcentaje de ceros y unos en ambos casos, por lo que la estratificaci\u00f3n es correcta.","36e91ae8":"### 3.2 Visualizaci\u00f3n de las variables\n\n---\n\n### 3.2.1 An\u00e1lisis multivariado\n\nPrimero vamos a comprobar la correlaci\u00f3n de las variables predictoras realizando un an\u00e1lisis multivariado, ya que como tenemos una gran cantidad de variables (30), si podemos deber\u00edamos intentar reducir ese n\u00famero antes de iniciar una an\u00e1lisis univariado.\n\nPodr\u00edamos usar esta funci\u00f3n de `plotly` para hacerlo, pero se ve m\u00e1s claro con `Seaborn`.\n> px.imshow(data_train.corr(method=\"pearson\"))","6e89c4c1":"Adem\u00e1s, fijamos una semilla para que los experimentos sean reproducibles (hemos usado la misma que se us\u00f3 en la libreta del estudio de `iris`:","4745783d":"Hemos realizado ahora un an\u00e1lisis multivariado para tratar de obtener mejores conclusiones determinando la potencia discriminativa de los atributos, viendo las relaciones entre ellos respecto a la informaci\u00f3n que nos den sobre la variable objetivo `Outcome`.","febaa6cd":"Y prueba:","0b331b21":"Para la limpieza de datos en nuestro problema vamos a realizar los siguientes pasos:\n* Eliminar las variables `Insuline` y `SkinThickness` por tener m\u00e1s de un 20% de datos perdidos.\n* Imputar datos perdidos de las variables `Glucose`, `BloodPressure` y `BMI`.\n* Eliminar *outliers*\n\n### Eliminar variables\n\nPara comenzar con la limpieza de los datos, lo primero que vamos a hacer es eliminar las variables `Insuline` y `SkinThickness`, como hemos explicado en el an\u00e1lisis exploratorio. Podemos eliminar directamente las variables con la funci\u00f3n drop de pandas:","0bdce0fd":"Para eliminar outliers vamos a usar la funci\u00f3n que se nos proporcion\u00f3 en pr\u00e1cticas. Creamos un IsolationForest para detectar los outliers, y con un FunctionSampler la incluiremos en el pipeline. Es importante que el valor `random_state` lo igualemos a nuestra semilla, para permitir que los experimentos se puedan reproducir.","ee212518":"###\u00a0*Pipeline*","ffd5c29a":"Este gr\u00e1fico resultaba muy \u00fatil en problemas como el de base de datos `iris`, ya que al tener pocas variables y pocos registros se pod\u00edan extraer r\u00e1pidamente algunas conclusiones. Sin embargo, en este problema es dif\u00edcil sacar conclusiones viendo este pairplot debido a que hay 8 x 8 (64) gr\u00e1ficas y no se aprecian muy bien. Podemos ver algunos outliers en casi todos los diagramas.","4d1862a0":"### Algoritmo *Zero-R*","221827bb":"Repetimos para `SkinThickness`:","74ffcc5f":"## 4.1 Limpieza de datos","2a585544":"Si bien podr\u00edamos comenzar con el an\u00e1lisis exploratorio, vamos a dividir primero nuestro conjunto de datos en dos:\n\n* Una muestra de entrenamiento (vamos a usar 75%)\n* Una muestra de prueba (el 25% restante)\n\nDe este modo, podemos dejar el conjunto de prueba a modo de instancias no observadas para asegurarnos que los resultados de validaci\u00f3n han sido estimados de manera honesta (y no optimista). De hecho, si utilizamos el mismo conjunto de datos para aprender y validar un modelo, observaremos un resultado inusual y es que, conforme m\u00e1s sobreajustado est\u00e1 el modelo, menor es el error cometido. Por el contrario, si usamos un conjunto de entrenamiento muy peque\u00f1o (50% training, 50% test, por ejemplo), no estar\u00edamos ajustando lo suficiente (*underfitting*)y podemos obtener resultados peores de los que deber\u00edamos.\n\nPara realizar un *holdout* podemos utilizar el m\u00e9todo `train_test_split` de `scikit-learn`:\n\nPodr\u00edamos realizar una validaci\u00f3n cruzada con k=5 por ejemplo (carpetas de unas 154 o 153 instancias) para reducir la aleatoriedad y usar cada registro como test una vez, pero lo dejamos para la siguiente pr\u00e1ctica, para esta realizaremos un *holdout*.","ff5c3d52":"### 4.3. Discretizaci\u00f3n\n\n___\n\nVamos a utilizar un discretizador por `kmeans = 2`, ya que como hemos visto en el an\u00e1lisis de las variables, muchas de ellas como `radius_mean`, `radius_se`, `radius_worst`, `concavity_mean` o `concavity_worst` se pueden dividir casi perfectamente en dos partes, dejando a cada lado la clase mayoritaria. Adem\u00e1s, usamos `kmeans` porque como los datos est\u00e1n desbalanceados, si parti\u00e9semos por ejemplo por la media seguramente esa partici\u00f3n ser\u00eda peor. ","7c124b8b":"A simple vista en este diagrama podemos ver c\u00f3mo por culpa de todos los valores perdidos de la variable `Insulina`, \u00e9sta tiene su mediana en el valor 0, siendo los valores superiores a 326 los outliers, cuando no es para nada lo que ocurre en realidad. Como vamos a eliminar las variables `Insulina` y `SkinThickness`, podemos realizar otro diagrama de cajas con el resto de variables.","1e3257a3":"Podemos realizar un diagrama de densidad de contorno (o histograma en 2D) para comparar de otra manera estas 2 variables. Este gr\u00e1fico se utiliza cuando hay muchos puntos en un diagrama de dispersi\u00f3n, como el que acabamos de hacer.","34951ba5":"Y prueba:","594106e6":"Y comprobamos con el mapa de calor que el borrado se ha realizado correctamente, y qu eno nos dejamos ninguna variable con alta correlaci\u00f3n.","5b95c187":"## 5. Aprendizaje y evaluaci\u00f3n de modelos\n\n---\n\n### 5.1. Algoritmo Zero-R\n\nPrimero vamos a crear un clasificador `Zero-R`, que nos servir\u00e1 como *baseline* para poder comparar con los valores que obtengamos de nuestros \u00e1rboles de clasificaci\u00f3n.","409c7c57":"## 4. Preprocesamiento de datos","44b2bd60":"Podemos ver c\u00f3mo todas las variables siguen m\u00e1s o menos una distribuci\u00f3n normal. Sin embargo, algunas variables como `concavity_mean`, `radius_se`, `compactness_se` y `fractal_dimension_se` tienen una asimetr\u00eda positiva bastante destacable. Tambi\u00e9n podemos ver claramente los *outliers* en algunas variables como `concavity_se` o `radius_se`, entre otras.","72b94a3d":"Y al igual que antes, tambi\u00e9n parece que se han realizado las uniones correctamente.\n\n---\n\n## 3. An\u00e1lisis exploratorio de datos\n\nPrimero vamos a ver el tama\u00f1o de nuestro problema, conociendo el tama\u00f1o los conjuntos de datos que hemos creado, y los tipos de variables que \u00e9stos tienen.","9ed0846f":"# Pima Indians Diabetes Database","f6c8e07a":"Es decir, 6 variables predictoras del conjunto de datos (`Pregnancies`,`Glucose`,`BloodPressure`,`SkinThickness`,`Insulin`,`Age`) son num\u00e9ricas (continuas) del tipo `int64`, 2 variables predictoras num\u00e9ricas (continuas) del tipo `float64`, y la variable clase (`Outcome`) es categ\u00f3rica (`category`) o discreta, con los estados `0` y `1`, como vemos a continuaci\u00f3n:","a6c1364e":"En el muestreo anterior podemos ver c\u00f3mo hay una variable cuyo nombre es `Unnamed 32`. Lo volvemos a comprobar con data.info:","059434d6":"De esta forma nos hemos quedado solo con 17 variables, teniendo as\u00ed menos datos redundantes (lo cual es bastante malo para algunos algoritmos) ya que ahora cada una de las variables aporta bastante informaci\u00f3n propia.","84bc8e60":"### 4.2. Eliminaci\u00f3n de *outliers*\n\nPara eliminar los *outliers* vamos a usar una funci\u00f3n (`outlier_rejection`) que cree un `IsolationForest` para detectarlos, y despu\u00e9s pasarla por un `FunctionSampler` para poder incluirla en el *pipeline*.","fe918845":"Podemos ver c\u00f3mo obtenemos un *accuracy* del 93,567%, lo cual es una mejora muy grande con respecto a `Zero-R`. Tiene sentido, ya que como hemos visto en el an\u00e1lisis exploratorio de los datos hab\u00eda varias variables que pod\u00edan dividir bastante bien los datos dependiendo de la clase `B`o `M` obteniendo poco error.\n\n---\n\nTambi\u00e9n podemos representar el \u00e1rbol que se ha generado usando la librer\u00eda `graphviz`, que es el siguiente:","5585ba6b":"## 1. Preliminares","d223b37e":"##\u00a01. Preliminares","a97d186d":"### Inducci\u00f3n de \u00e1rboles de decisi\u00f3n","1bfc86e3":"### Imputar datos","34197565":"### \u00c1rbol de decisi\u00f3n discretizando el conjunto de datos y usando los transformadores de limpieza","abbd644e":"Para empezar, vamos a crear una matriz de gr\u00e1ficos del tipo nube puntos, al igual que se hizo en el estudio sobre `Iris`. Cada diagrama muestra la relaci\u00f3n entre pares de variables predictoras, coloreando cada registro seg\u00fan la clase a la que pertenezca (0 o 1).","c2dd34e4":"Hemos realizado distintos tipos de gr\u00e1ficas y de diagramas para comprender mejor las variables de nuestro problema y las capadidades de la librer\u00eda `plotly`, realizando tanto un an\u00e1lisis univariado, como uno multivariado.","6e5b6e2e":"Continuamos visualizando la variables clase:"}}