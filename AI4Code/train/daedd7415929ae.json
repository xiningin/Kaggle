{"cell_type":{"ee53fae3":"code","4872f071":"code","ed9df903":"code","1caaaa5b":"code","80a3e4ce":"code","5b3d44f5":"code","050f57c6":"code","d0d12236":"code","1c6d9ebf":"code","9f5c460f":"code","0ff04cef":"code","6b3f3ca4":"code","22676ff3":"code","e0459831":"code","2224393a":"code","1082db25":"code","d2eb4b40":"code","9fdd62ef":"code","44ae1a97":"code","ef179267":"code","a4aeea32":"code","8c781965":"code","83d40da5":"code","7d952bdf":"code","d32b5ef7":"code","c10643ca":"code","9d878b5b":"code","d82b5495":"code","a3bb804a":"code","57e6889a":"code","e369a162":"code","0800198b":"markdown","eb41b29d":"markdown","c0a68ec1":"markdown","5cdf5085":"markdown","e5595142":"markdown","a7081b18":"markdown","81745630":"markdown","40ea2ca9":"markdown","7d8d8064":"markdown","2ddcca97":"markdown","de52560c":"markdown","28e06deb":"markdown","b6c5081d":"markdown","64500a1c":"markdown","919cc93d":"markdown","149dc4d6":"markdown","27b38469":"markdown","dee1fba1":"markdown","a0dec936":"markdown","100ea31a":"markdown","a74286dd":"markdown","a6fd6d41":"markdown","b3edce6c":"markdown","41acd5ae":"markdown","b94b19b3":"markdown","424b0a96":"markdown","0d461fcb":"markdown","1e10b718":"markdown","943d3708":"markdown","3f386dcb":"markdown","2c3f8a6e":"markdown","d2d72f03":"markdown"},"source":{"ee53fae3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4872f071":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","ed9df903":"data = pd.read_csv('..\/input\/energy-efficiency-on-buildings\/Building Energy Efficiency.csv')","1caaaa5b":"data.head()","80a3e4ce":"data.shape","5b3d44f5":"data.isnull().sum()","050f57c6":"data.hist(bins=20, figsize=(20,15))\nplt.show()","d0d12236":"import plotly.express as px\nyprop = 'Surface Area'\nxprop = 'Cooling Load'\nh= None\npx.scatter(data, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")","1c6d9ebf":"import plotly.express as px\nyprop = 'Overall Height'\nxprop = 'Heating Load'\nh= None\npx.scatter(data, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")","9f5c460f":"import matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (12,7))\n## Plotting heatmap. # Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(data.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(data.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, );\nplt.title(\"Heatmap of all the Features of Train data set\", fontsize = 25);","0ff04cef":"from scipy.stats import randint as sp_randint\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor,AdaBoostRegressor,BaggingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import SVC\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import roc_auc_score","6b3f3ca4":"X = data[['Relative Compactness', 'Surface Area', 'Wall Area', 'Roof Area', 'Overall Height', 'Orientation', 'Glazing Area', 'Glazing Area Distribution']]\nY = data[['Heating Load', 'Cooling Load']]\nY1= data[['Heating Load']]\nY2= data[['Cooling Load']]","22676ff3":"X_train, X_test, y1_train, y1_test, y2_train, y2_test = train_test_split(X, Y1, Y2, test_size=0.33, random_state = 20)\n\nMinMax = MinMaxScaler(feature_range= (0,1))\nX_train = MinMax.fit_transform(X_train)\nX_test = MinMax.transform(X_test)","e0459831":"Acc = pd.DataFrame(index=None, columns=['model','train_Heating','test_Heating','train_Cooling','test_Cooling'])","2224393a":"regressors = [['SVR',SVR()],\n              \n              ['DecisionTreeRegressor',DecisionTreeRegressor()],\n              ['KNeighborsRegressor', KNeighborsRegressor()],\n              ['RandomForestRegressor', RandomForestRegressor()],\n              ['MLPRegressor',MLPRegressor()],\n              ['AdaBoostRegressor',AdaBoostRegressor()],\n              ['GradientBoostingRegressor',GradientBoostingRegressor()]]","1082db25":"for mod in regressors:\n    name = mod[0]\n    model = mod[1]\n    \n    model.fit(X_train,y1_train)\n    actr1 = r2_score(y1_train, model.predict(X_train))\n    acte1 = r2_score(y1_test, model.predict(X_test))\n    \n    model.fit(X_train,y2_train)\n    actr2 = r2_score(y2_train, model.predict(X_train))\n    acte2 = r2_score(y2_test, model.predict(X_test))\n    \n    Acc = Acc.append(pd.Series({'model':name, 'train_Heating':actr1,'test_Heating':acte1,'train_Cooling':actr2,'test_Cooling':acte2}),ignore_index=True )\nAcc.sort_values(by='test_Cooling')","d2eb4b40":"DTR = DecisionTreeRegressor()\nparam_grid = {\"criterion\": [\"mse\", \"mae\"],\"min_samples_split\": [14, 15, 16, 17],\n              \"max_depth\": [5, 6, 7],\"min_samples_leaf\": [4, 5, 6],\"max_leaf_nodes\": [29, 30, 31, 32],}\n\ngrid_cv_DTR = GridSearchCV(DTR, param_grid, cv=5)\n\ngrid_cv_DTR.fit(X_train,y2_train)\nprint(\"R-Squared::{}\".format(grid_cv_DTR.best_score_))\nprint(\"Best Hyperparameters::\\n{}\".format(grid_cv_DTR.best_params_))","9fdd62ef":"DTR = DecisionTreeRegressor(criterion= 'mse', max_depth= 6, max_leaf_nodes= 30, min_samples_leaf= 5, min_samples_split= 17)\n\nDTR.fit(X_train,y1_train)\nprint(\"R-Squared on train dataset={}\".format(DTR.score(X_test,y1_test)))\n\nDTR.fit(X_train,y2_train)   \nprint(\"R-Squaredon test dataset={}\".format(DTR.score(X_test,y2_test)))","44ae1a97":"from sklearn.model_selection import GridSearchCV\nparam_grid = [{'n_estimators': [350, 400, 450], 'max_features': [1, 2], 'max_depth': [85, 90, 95]}]\n\nRFR = RandomForestRegressor(n_jobs=-1)\ngrid_search_RFR = GridSearchCV(RFR, param_grid, cv=10, scoring='neg_mean_squared_error')\ngrid_search_RFR.fit(X_train, y2_train)\n\nprint(\"R-Squared::{}\".format(grid_search_RFR.best_score_))\nprint(\"Best Hyperparameters::\\n{}\".format(grid_search_RFR.best_params_))","ef179267":"RFR = RandomForestRegressor(n_estimators = 450, max_features = 1, max_depth= 90, bootstrap= True)\n\nRFR.fit(X_train,y1_train)\nprint(\"R-Squared on train dataset={}\".format(RFR.score(X_test,y1_test)))\n\nRFR.fit(X_train,y2_train)   \nprint(\"R-Squaredon test dataset={}\".format(RFR.score(X_test,y2_test)))","a4aeea32":"param_grid = [{\"learning_rate\": [0.01, 0.02, 0.1], \"n_estimators\":[150, 200, 250], \"max_depth\": [4, 5, 6], \n \"min_samples_split\":[1, 2, 3], \"min_samples_leaf\":[2, 3], \"subsample\":[1.0, 2.0]}]\n\nGBR = GradientBoostingRegressor()\ngrid_search_GBR = GridSearchCV(GBR, param_grid, cv=10, scoring='neg_mean_squared_error')\ngrid_search_GBR.fit(X_train, y2_train)\n\nprint(\"R-Squared::{}\".format(grid_search_GBR.best_score_))\nprint(\"Best Hyperparameters::\\n{}\".format(grid_search_GBR.best_params_))","8c781965":"GBR = GradientBoostingRegressor(learning_rate=0.1,n_estimators=250, max_depth=5, min_samples_split=3, min_samples_leaf=2, subsample=1.0)\n\nGBR.fit(X_train,y1_train)\nprint(\"R-Squared on train dataset={}\".format(GBR.score(X_test,y1_test)))\n\nGBR.fit(X_train,y2_train)   \nprint(\"R-Squaredon test dataset={}\".format(GBR.score(X_test,y2_test)))","83d40da5":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostRegressor\n\nmodel_CBR = CatBoostRegressor()\nparameters = {'depth':[8, 10],'iterations':[10000],'learning_rate':[0.02,0.03],\n              'border_count':[5],'random_state': [42, 45]}\n\ngrid = GridSearchCV(estimator=model_CBR, param_grid = parameters, cv = 2, n_jobs=-1)\ngrid.fit(X_train, y2_train)\nprint(\" Results from Grid Search \" )\nprint(\"\\n The best estimator across ALL searched params:\\n\", grid.best_estimator_)\nprint(\"\\n The best score across ALL searched params:\\n\", grid.best_score_)\nprint(\"\\n The best parameters across ALL searched params:\\n\", grid.best_params_)","7d952bdf":"MLPR = MLPRegressor(hidden_layer_sizes = [180,100,20],activation ='relu', solver='lbfgs',max_iter = 10000,random_state = 0)\nMLPR.fit(X_train,y1_train)\nprint(\"R-Squared on train dataset={}\".format(MLPR.score(X_test,y1_test)))\n\nMLPR.fit(X_train,y2_train)   \nprint(\"R-Squaredon test dataset={}\".format(MLPR.score(X_test,y2_test)))","d32b5ef7":"Acc1 = pd.DataFrame(index=None, columns=['model','train_Heating','test_Heating','train_Cooling','test_Cooling'])","c10643ca":"regressors1 = [['DecisionTreeRegressor',DecisionTreeRegressor(criterion= 'mse', max_depth= 6, max_leaf_nodes= 30, min_samples_leaf= 5, min_samples_split= 17)],\n              ['RandomForestRegressor', RandomForestRegressor(n_estimators = 450, max_features = 1, max_depth= 90, bootstrap= True)],\n              ['MLPRegressor',MLPRegressor(hidden_layer_sizes = [180,100,20],activation ='relu', solver='lbfgs',max_iter = 10000,random_state = 0)],\n              ['GradientBoostingRegressor',GradientBoostingRegressor(learning_rate=0.1,n_estimators=250, max_depth=5, min_samples_split=2, min_samples_leaf=3, subsample=1.0)]]","9d878b5b":"for mod in regressors1:\n    name = mod[0]\n    model = mod[1]\n    \n    model.fit(X_train,y1_train)\n    actr1 = r2_score(y1_train, model.predict(X_train))\n    acte1 = r2_score(y1_test, model.predict(X_test))\n    \n    model.fit(X_train,y2_train)\n    actr2 = r2_score(y2_train, model.predict(X_train))\n    acte2 = r2_score(y2_test, model.predict(X_test))\n    \n    Acc1 = Acc1.append(pd.Series({'model':name, 'train_Heating':actr1,'test_Heating':acte1,'train_Cooling':actr2,'test_Cooling':acte2}),ignore_index=True )\nAcc1.sort_values(by='test_Cooling')","d82b5495":"model = CatBoostRegressor(border_count= 5, depth= 8, iterations= 10000, learning_rate= 0.02, random_state= 45)\n\nmodel.fit(X_train,y1_train)\nactr1 = r2_score(y1_train, model.predict(X_train))\nacte1 = r2_score(y1_test, model.predict(X_test))\ny1_pred = model.predict(X_test)\n\nmodel.fit(X_train,y2_train)\nactr2 = r2_score(y2_train, model.predict(X_train))\nacte2 = r2_score(y2_test, model.predict(X_test))\ny2_pred = model.predict(X_test)","a3bb804a":"print(\"CatBoostRegressor: R-Squared on train dataset={}\".format(actr1))\nprint(\"CatBoostRegressor: R-Squared on test dataset={}\".format(acte1))\nprint(\"CatBoostRegressor: R-Squared on train dataset={}\".format(actr2))\nprint(\"CatBoostRegressor: R-Squared on test dataset={}\".format(acte2))","57e6889a":"x_ax = range(len(y1_test))\nplt.figure(figsize=(20,10))\nplt.subplot(2,1,1)\nplt.plot(x_ax, y1_test, label=\"Actual Heating\")\nplt.plot(x_ax, y1_pred, label=\"Predicted Heating\")\nplt.title(\"Heating test and predicted data\")\nplt.xlabel('X-axis')\nplt.ylabel('Heating load (kW)')\nplt.legend(loc='best',fancybox=True, shadow=True)\nplt.grid(True)\n\nplt.subplot(2,1,2)\nplt.plot(x_ax, y2_test, label=\"Actual Cooling\")\nplt.plot(x_ax, y2_pred, label=\"Predicted Cooling\")\nplt.title(\"Coolong test and predicted data\")\nplt.xlabel('X-axis')\nplt.ylabel('Cooling load (kW)')\nplt.legend(loc='best',fancybox=True, shadow=True)\nplt.grid(True)\n\nplt.show()","e369a162":"def AAD(y1_test, y1_pred):\n    AAD =[]\n    for i in range(len(y1_pred)):\n        AAD.append((y1_pred[i] - y1_test.values[i])\/y1_test.values[i]*100)\n    return AAD\n\nx_ax = range(len(y1_test))\nplt.figure(figsize=(20,10))\nplt.subplot(2,1,1)\nplt.plot(x_ax, AAD(y1_test, y1_pred), label=\"Relative deviation obtained on Heating load\")\nplt.title(\"Heating load\")\nplt.xlabel('X-axis')\nplt.ylabel('Error (%)')\nplt.legend(loc='best',fancybox=True, shadow=True)\nplt.grid(True)\n\nplt.subplot(2,1,2)\nplt.plot(x_ax, AAD(y2_test, y2_pred), label=\"Relative deviation obtained on Cooling load\")\nplt.title(\"Cooling load\")\nplt.xlabel('X-axis')\nplt.ylabel('Error (%)')\nplt.legend(loc='best',fancybox=True, shadow=True)\nplt.grid(True)\n\nplt.show()","0800198b":"# A. Data import","eb41b29d":"# B. Tune Random Forests Parameters\n\nRandom forest is an ensemble tool which takes a subset of observations and a subset of variables to build a decision trees. It builds multiple such decision tree and amalgamate them together to get a more accurate and stable prediction. We generally see a random forest as a black box which takes in input and gives out predictions, without worrying too much about what calculations are going on the back end. This black box itself have a few levers we can play with. Each of these levers have some effect on either the performance of the model or the resource \u2013 time balance. Parameters in random forest are either to increase the predictive power of the model or to make it easier to train the model. \n\nThere are primarily 3 features which can be tuned to improve the predictive power of the model :\n\n+ Max_features: These are the maximum number of features Random Forest is allowed to try in individual tree. Increasing max_features generally improves the performance of the model as at each node now we have a higher number of options to be considered. However, this is not necessarily true as this decreases the diversity of individual tree which is the USP of random forest. But, for sure, you decrease the speed of algorithm by increasing the max_features. Hence, you need to strike the right balance and choose the optimal max_features.\n\n+ n_estimators : This is the number of trees you want to build before taking the maximum voting or averages of predictions. Higher number of trees give you better performance but makes your code slower. You should choose as high value as your processor can handle because this makes your predictions stronger and more stable.\n\n+ min_sample_leaf : If you have built a decision tree before, you can appreciate the importance of minimum sample leaf size. Leaf is the end node of a decision tree. A smaller leaf makes the model more prone to capturing noise in train data. Generally I prefer a minimum leaf size of more than 50. However, you should try multiple leaf sizes to find the most optimum for your use case.\n\nThere are a few attributes which have a direct impact on model training speed. In this Notebook, we just mention about the n_jobs parameter. This parameter tells the engine how many processors is it allowed to use. A value of \u201c-1\u201d means there is no restriction whereas a value of \u201c1\u201d means it can only use one processor.","c0a68ec1":"As observed in the fitting calculation section, we will try to tuning model parameters using the training data set of Cooling load (or y2_train). ","5cdf5085":"Set variables & target functions.","e5595142":"Now, let's try to select some Regressors to check its performance.","a7081b18":"# D. CatBoostRegressor\n\nCatBoost is a recently open-sourced machine learning algorithm from Yandex. It can work with diverse data types to help solve a wide range of problems that businesses face today. To top it up, it provides best-in-class accuracy.\n\nIt yields state-of-the-art results without extensive data training typically required by other machine learning methods, and\nProvides powerful out-of-the-box support for the more descriptive data formats that accompany many business problems. \u201cCatBoost\u201d name comes from two words \u201cCategory\u201d and \u201cBoosting\u201d. \u201cBoost\u201d comes from gradient boosting machine learning algorithm as this library is based on gradient boosting library. Gradient boosting is a powerful machine learning algorithm that is widely applied to multiple types of business challenges like fraud detection, recommendation items, forecasting and it performs well also. It can also return very good result with relatively less data, unlike DL models that need to learn from a massive amount of data.\n\nCatBoost provides state of the art results and it is competitive with any leading machine learning algorithm on the performance front. Handling Categorical features automatically: We can use CatBoost without any explicit pre-processing to convert categories into numbers. CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features.\n\nIt also reduces the need for extensive hyper-parameter tuning and lower the chances of overfitting also which leads to more generalized models. Although, CatBoost has multiple parameters to tune and it contains parameters like the number of trees, learning rate, regularization, tree depth, fold size, bagging temperature and others.\n \n+ iterations: The maximum number of trees that can be built when solving machine learning problems. Fewer may be used.\n+ learning_rate: used for reducing the gradient step. It affects the overall time of training: the smaller the value, the more iterations are required for training.\n+ depth: Depth of the tree. Can be any integer up to 32. Good values in the range 1 - 10.\n\nParameter Search is usually known as grid search, basically looking for parameter values that score the best. An important thing to remember is that we can't use the test set to tune parameters, otherwise we'll overfit to the test set.\n\nPreventing Overfitting: CatBoost provides a nice facility to prevent overfitting. If you set iterations to be high, the classifier will use many trees to build the final classifier and you risk overfitting. If you set use_best_model=True and eval_metric='Accuracy' when initialising and then set eval_set to be a validation set then CatBoost won't use all the iterations, it will return the iteration that gives the best accuracy on the eval set. This is similar to early stopping used in neural networks. If you are having problems with overfitting, it would be a good idea to try this. I didn't see any improvement on this dataset though, probably because there are so many train points it is more difficult to overfit on.","81745630":"<a href=\"https:\/\/ibb.co\/tc0YwXc\"><img src=\"https:\/\/i.ibb.co\/3WxS5zW\/Energy-Efficiency-Buildings.jpg\" alt=\"Energy-Efficiency-Buildings\" border=\"0\"><\/a>","40ea2ca9":"# 4. Models parameters tuning","7d8d8064":"Create a DataFrame to store computation results obtained with different models.","2ddcca97":"# 3. CONCLUSION\n\nIn this Notebook, building energy performance has been investigated using different models to predict Heating and Cooling loads\u2022 We tried to learn how to tune different parameters in the models and obtained a very good prediction result (>99.5% on both Heating and Cooling loads, compared to the experimental data set). Some observations will be shown in the graphs bellow.","de52560c":"Yes, the accuracy of the model has been improved.","28e06deb":"Now, it's time to summary all our BEST models","b6c5081d":"Not easy to find out a clear correlation, we tried to check the correlation matrix.","64500a1c":"Wow ! we are happy with this improvement, then.","919cc93d":"# Introduction\n\nBuildings energy consumption is put away around 40% of total energy use. Predicting heating and cooling loads of a building in the initial phase of the design to find out optimal solutions amongst different designs is very important, as well as in the operating phase after the building has been finished for efficient energy. In this Notebook, diferent models were applied for predicting heating and cooling loads of a building based on a dataset for building energy performance. \n\nInput variables are: \n+ relative compactness, \n+ roof area, \n+ overall height, \n+ surface area, \n+ glazing are a, \n+ wall area,\n+ glazing area distribution of a building, \n+ orientation.\n\nOutput variables: \n+ heating loads, and \n+ cooling loads of the building. \n\nThe model was trained and validated on 33% on the data set, and the accuracy for the prediction test was 99.9% and 99.6% respectively.","149dc4d6":"Splitting the dataset into Training and Test set. Feature scaling or data normalization is a method used to normalize the range of independent variables or features of data. So when the values vary a lot in an independent variable, we use feature scaling so that all the values remain in the comparable range.","27b38469":"# Variable(s) Information:\n\n+ Relative Compactness\n+ Surface Area - m\u00b2\n+ Wall Area - m\u00b2\n+ Roof Area - m\u00b2\n+ Overall Height - m\n+ Orientation - 2:North, 3:East, 4:South, 5:West\n+ Glazing Area - 0%, 10%, 25%, 40% (of floor area)\n+ Glazing Area Distribution (Variance) - 1:Uniform, 2:North, 3:East, 4:South, 5:West\n+ Heating Load - kWh\n+ Cooling Load - kWh","dee1fba1":"And try to check the correlation between variables.","a0dec936":"Now, check the distribution of different variables.","100ea31a":"# 1. Introduction\n\nIn a building, thermal energy involves two measures of cooling load (CL), and heating load (HL) and these measures are regulated by heating ventilation and air conditioning (HVAC) system. The HVAC system is designed to compute the HL and CL of the space and thereby, provide a desirable indoor air condition. In this sense, some studies have focused on evaluating comfortable, yet energy-saving spaces. Required cooling and heating capacities are estimated mainly according to the basic factors, including building properties, its utilization, and climate conditions. \n\nMore sustainable consumption of energy can be ensured by a proper examination of the energy performance of buildings (EPB) and optimal designing of the HVAC system. Although many countries take such measures, there is still a high level of energy consumption, and it is projected to increase globally. According to the items mentioned above, many engineers have tried to develop different predictive and evaluative tools the primary aim in order to produce an optimal approximation of building energy consumption.","a74286dd":"# 3. Fitting - modeling","a6fd6d41":"Importing necessary Libraries ...","b3edce6c":"Wow, very impressive with the GradientBoostingRegressor.","41acd5ae":"# 2. Data Preparation\n\nIn this Notebook, we used the dataset taken from https:\/\/cml.ics.uci.edu\/, based on research by Tsanas and Xifara.","b94b19b3":"OK, it's not bad. Let's begin to process the data for fitting & modeling.","424b0a96":"# E. And suprising MLPRegressor","0d461fcb":"Good, I'm lucky since not have to process the NaN data.","1e10b718":"# A. Decision Tree Regressor parameters turning\n\nDecision Tree algorithm has become one of the most used machine learning algorithm both in competitions like Kaggle as well as in business environment. Decision Tree can be used both in classification and regression problem. The model is based on decision rules extracted from the training data. In regression problem, the model uses the value instead of class and mean squared error is used to for a decision accuracy. Decision tree model is not good in generalization and sensitive to the changes in training data. A small change in a training dataset may effect the model predictive accuracy.\n\n# Parameters\n\n+ max_features: The number of randomly chosen features from which to pick the best feature to split on a given tree node. It can be an integer or one of the two following methods (auto : square root of the total number of predictors. max : number of predictors.)\n+ max_leaf_nodes: The maximum number of leaf nodes a tree in the forest can have, an integer between 1 and 1e9, inclusive.\n+ max_depth: The maximum depth for growing each tree, an integer between 1 and 100, inclusive.\n+ min_samples_leaf: The minimum number of samples each branch must have after splitting a node, an integer between 1 and 1e6, inclusive. A split that causes fewer remaining samples is discarded.","943d3708":"# B. Spliting the data in X and Y","3f386dcb":"# C. Gradient Boosting Regression - Hyperparameter Tuning\n\nWhat we will do now is make an instance of the GradientBoostingRegressor. We will create our grid with the various values for the hyperparameters. We will then take this grid and place it inside GridSearchCV function so that we can prepare to run our model. There are some arguments that need to be set inside the GridSearchCV function such as estimator, grid, cv, etc.\n\nWith this tuning we can see that the mean squared error is lower than with the baseline model. We can now move to the final step of taking these hyperparameter settings and see how they do on the dataset. There are several hyperparameters we need to tune, and they are as follows:\n\n+ Learning rate: The learning rate is the weight that each tree has on the final prediction.\n+ Number of estimators: The number of estimators is show many trees to create. The more trees the more likely to overfit.\n+ Min samples split: The minimum number of samples required to split an internal node\n+ Max depth: Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n+ Subsample: Subsample is the proportion of the sample to use.","2c3f8a6e":"Boosting machine learning algorithms are highly used because they give better accuracy over simple ones. Performance of these algorithms depends on hyperparameters. An optimal set of parameters can help to achieve higher accuracy. Finding hyperparameters manually is tedious and computationally expensive. Therefore, automation of hyperparameters tuning is important. RandomSearch, GridSearchCV, and Bayesian optimization are generally used to optimize hyperparameters.\n\nIn this Notebook, we calculate the best parameters for the model using \u201cGridSearchCV\u201d.","d2d72f03":"Just improved a little bit accuracy ..."}}