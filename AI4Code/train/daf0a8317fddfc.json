{"cell_type":{"92ffbb64":"code","4cc2e822":"code","05ef3464":"code","b9a7e7d6":"code","7e0bb376":"code","e56b7e25":"code","2a08ba93":"code","8be80e30":"code","00468a37":"code","51cafff9":"code","2abda277":"code","ba670e47":"code","d5950cb7":"code","428652d9":"code","57f9ff8c":"code","c2cd9a53":"code","b841c250":"code","c94bfd6e":"markdown","e76f5462":"markdown","53edcdb5":"markdown"},"source":{"92ffbb64":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4cc2e822":"dataset = pd.read_csv(\"\/kaggle\/input\/avocado-prices\/avocado.csv\")[0:5000]\ndataset.head()","05ef3464":"dataset.info()","b9a7e7d6":"dataset.describe().T","7e0bb376":"train = dataset.loc[:,[\"AveragePrice\"]].values\ntrain","e56b7e25":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range = (0, 1))\ntrain_scaled = scaler.fit_transform(train)\ntrain_scaled","2a08ba93":"plt.figure(figsize=(15, 5));\nplt.plot(train_scaled);\nplt.show()","8be80e30":"dataset.shape[0]","00468a37":"# Creating a data structure with 250 timesteps and 1 output\nX_train = []\ny_train = []\ntimesteps = 250\n\nfor i in range(timesteps,dataset.shape[0]):\n    X_train.append(train_scaled[i-timesteps:i,0])\n    y_train.append(train_scaled[i,0])\n    \nprint(\"X_train length: \", len(X_train))\nprint(\"y_train length: \", len(y_train))","51cafff9":"X_train, y_train = np.array(X_train), np.array(y_train)","2abda277":"# Reshaping\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nlen(X_train[0])","ba670e47":"y_train[0:10]","d5950cb7":"X_train.shape","428652d9":"# Importing the Keras libraries and packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import SimpleRNN\nfrom keras.layers import Dropout\n\nregressor = Sequential() # initialising the RNN\n\n# adding the first RNN layer and some Droupout regularisation\nregressor.add(SimpleRNN(units=50, activation=\"tanh\", return_sequences=True, input_shape=(X_train.shape[1],1)))\n# (X_train.shape[1],1) => output: (250,1)\nregressor.add(Dropout(0.2))\n\n# adding the second RNN layer and some Droupout regularisation\nregressor.add(SimpleRNN(units=50, activation=\"tanh\", return_sequences=True))\nregressor.add(Dropout(0.2))\n\n# adding the third RNN layer and some Droupout regularisation\nregressor.add(SimpleRNN(units=50, activation=\"tanh\", return_sequences=True))\nregressor.add(Dropout(0.2))\n\n# adding a fourth RNN layer and some Dropout regularisation\nregressor.add(SimpleRNN(units=50))\nregressor.add(Dropout(0.2))\n\n# adding the output layer\nregressor.add(Dense(units = 1))\n\n# Compiling the RNN\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the RNN to the Training set\nregressor.fit(X_train, y_train, epochs = 20, batch_size = 132)","57f9ff8c":"dataset_test = pd.read_csv(\"\/kaggle\/input\/avocado-prices\/avocado.csv\")[5000:10000]\ndataset_test.head()","c2cd9a53":"test = dataset_test.loc[:,[\"AveragePrice\"]].values\ntest","b841c250":"# Getting the predicted stock price of 2017\ndataset_total = pd.concat((dataset['AveragePrice'], dataset_test['AveragePrice']), axis = 0)\ninputs = dataset_total[len(dataset_total) - len(dataset_test) - timesteps:].values.reshape(-1,1)\ninputs = scaler.transform(inputs)  # min max scaler\ninputs","c94bfd6e":"# Create RNN Model","e76f5462":"# Loading & Visualization the Dataset","53edcdb5":"# Recurrent Neural Network (RNN) Tutorial\n\nIn order to understand Recurrent Neural Networks, let's review the working principle of a feedforward network. Briefly, we can say that a structure that produces output by applying a number of mathematical operations to the information coming to the neurons on the layers.\n\nThe information received in the feedforward working structure is only processed forward. In this structure, roughly, an output value is obtained by passing the input data through the network. An error is obtained by comparing the obtained output value with the correct values. The weight values on the network are changed depending on the error and in this way, a model that can output the most accurate result is created.\n\n![](https:\/\/pyimagesearch.com\/wp-content\/uploads\/2016\/08\/simple_neural_network_header.jpg)\n\nIn the training of a Feedforward network, the error should be sufficiently reduced. Thus, a structure that will output appropriate to the inpute is created by regenerating the weights going to the neurons.\n\nFor example, consider a feedforward network trained to categorize objects on a photo. The given photo can be used for both training and testing, even in a random order. It does not have to be connected with the previous or next photograph. In other words, there is no concept depending on time or order, the only input that it is interested in is the current example.\n\n![](https:\/\/miro.medium.com\/proxy\/1*6xj691fPWf3S-mWUCbxSJg.jpeg)\n\nIn recurrent structures, the result is drawn not only by the current inputa but also from other inputs. As can be seen in the figure, besides the input data at time t in RNN, the hidden layer results from the moment t-1 are the input of the hidden layer at the moment t. The decision made for the input at t-1 affects the decision to be made at time t. In other words, inputs in these networks produce output by combining current and previous information.\n\nRecurrent structures are separated from feedforward structures because they use their output as input in the next process. We can say that recurrent networks have a memory. The reason for adding memory to a network is that the input set coming in a certain order has a meaning for the output. Feedforward networks are not sufficient for such data sets.\n\nAt this point, RNNs come into play. Recurrent networks are used to understand the structure of the data that comes in a certain order, such as text, speech, various sensors based on time or statistical data.\n\n![](https:\/\/files.knime.com\/sites\/default\/files\/fig_1_3.png)\n\nAs seen in the diagram, the result from the hidden layer in the process loop of RNN both produces output and is written to the content units. In this way, each new input is processed with content units produced by processing previous inputs. If there is a correlation between the data memorized at different times, this is called \u201clong term\u201d addiction. RNN is a network that can calculate the relationship between these long-term dependencies.\n\nIn the behavior, speech and thoughts of people, the information that was previously in memory, like this structure, is processed in a hidden layer by looping with new data. The mathematical formula used in this process is as follows.\n\n<img src=\"https:\/\/i.stack.imgur.com\/WPRfd.png\" width=\"300\"\/>\n\nht is the result of the hidden layer at t. The xt input is multiplied by the W weight. Then at time t-1, h (t-1) value kept in content unit is multiplied by U weight and summed with Wxt. W and U values are the weights between the input and the layer. Here, the weight matrix takes values according to which of the previous and current data has more or less effect on the result. The error resulting from these operations is calculated and new weight values are rearranged with backpropagation. The backprop process continues until the error is sufficiently minimized. Sum of Wxt + Uh (t-1) is put into activation function like sigmoid, tanh. Thus, very large or very small values are placed in a logical range. In this way, non-linearity is also provided.\n\n## An Example Recurrent Neural Network\nLet's assume that a cook makes 3 different dishes in order. If she made pizza on the 1st day, sushi on the 2nd day is making waffles on the 3rd day. If we need to guess what to do next day, first we need to understand what kind of problem we are facing and use a method suitable for this problem. The method to be applied here is the recurrent neural network method, since the food comes in a sequence, that is, the meal made the day before will have an effect on the next day. In this way, even if there is no new information with the information we have, we can even guess the meal to be made in the next few weeks.\n\n![](https:\/\/elham-khanche.github.io\/blog\/assets\/img\/RNN\/Slide4.png)\n\n## Backpropagation Trough Time(BPTT)\nWe can say that the purpose of recurrent networks is to classify sequential inputs correctly. We use the error's backprop and gradient descent to do these operations. Backprop is done in feedforward networks by distributing the error in the probe output to the weights of the backward error. By using this derivative, the learning coefficient is arranged in a gradient descent and weights are arranged to reduce the error.\n\nThe method used for RNN is the backprop application for all of a series of time sequential calculations known as BPTT. Artificial networks use a number of functions nested as f (h (g (x))). When a time-dependent variable is added here, the derivative operation can be resolved with the chain rule.\n\n![](https:\/\/miro.medium.com\/max\/936\/1*5hp-_tV2gQe80WJSJz9d6w.png)\n\nThe working logic of the above RNN is explained. In the figure above, a recurrent network structure with 5 inputs is shown. E denotes the error that occurs here. For example, in the process we do while performing backpropagation for E3, the derivative is used according to the weight of w. To solve this derivative, we use the chain rule and the product of several derivatives.\n\n## Truncated BPTT\nTruncated BPTT, on the other hand, approaches BPTT since BPTT will be very costly for forward and backward transactions for a long row of data. The downside to this is that the gradient cut is calculated, so the network cannot learn \u201clong term\u201d dependencies as well as full BPTT.\n\n## Vanishing\/Exploding Gradient\nGradient is a value that allows us to adjust all weights. However, in long interconnected networks, the effect of the error decreases considerably and the gradient may start to disappear. This makes it impossible to find the right result. Since all layers and time-dependent steps are interconnected, their derivatives are at risk of extinction or flying, i.e. excessive rise.\n\n![](https:\/\/miro.medium.com\/max\/728\/1*TIAsX_WBxLDF_S2jFpnOJw.png)\n\nGradient exploding, that is, excessive growth, will cause the network to produce huge values and will remove it from the correct result. Putting a threshold for this is cutting simple gradients with high value is one of the simple and effective ways. It is a much more difficult problem for the gradients to disappear with excessive size. It is not clear where and when should be stopped.\n\nFortunately, there are several solutions to solve this problem. Selecting appropriate initial values for W will reduce the extinction effect. Another solution is to use ReLU instead of sigmoid and tanh activation functions. The derivative of the ReLU function is 0 or 1. For this reason, such a problem will not be included. Another method is the LSTM method designed to solve this problem."}}