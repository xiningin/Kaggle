{"cell_type":{"152fd18a":"code","f7fee136":"code","f732bbe9":"code","0844ba37":"code","af3e45cc":"code","9c67bd6f":"code","3c5f611b":"code","a2db880b":"markdown","0a4e8c62":"markdown","662b21a2":"markdown"},"source":{"152fd18a":"# Importing libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\n\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import preprocessing\nfrom itertools import product\n\nprint (\"Import done\")","f7fee136":"# load data\n\ndf = pd.read_csv(\"..\/input\/30days-kfolds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if 'cont' in col]\ndf_test = df_test[useful_features]\n\nprint (\"Data loaded\")","f732bbe9":"# set the rest of posible values for hyperparameters and a grid of combinations of them\n\n\nparam_names = [\"n_estimators\",\"max_depth\",\"learning_rate\",\"subsample\",\"reg_alpha\"]\nparam_values = [\n    [500],                    # \"n_estimators\"        [1000,750, 500, 250, 50],\n    [3, 7, 13],                # \"max_depth\"         [3, 5, 7, 11, 13],\n    [1, 0.01,0.005],           # \"learning_rate\"   [1, 0.5, 0.01,0.005],\n    [.5,.7,1.0],               # \"subsample\"       [.5,.6,.7,.8,.9,1.0],\n    [30.0,40.0]                # \"reg_alpha\"       [1.0,20.,30.0,40.0,50.0] \n    ]\n\nparam_combinations=list(product(*param_values))   # creates a list of all possible combinations\nparam_grid = dict(zip(param_names, param_values))\ntotal_combinations = len(param_combinations)\nprint(\"Number of combinations:\" ,total_combinations)\nparam_grid","0844ba37":"num_iteration = 0 \nbest_score = np.inf  # set max error\nbest_params = {}\n\n#iterate every combination of params\nfor param_combination in param_combinations:   \n    num_iteration += 1 \n    # choose the hyperparameters for this iteration\n    actual_params=dict(zip(param_names, param_combination))  \n    print(\"Round\",num_iteration,\" \/ \",total_combinations)  \n\n    start_time = time()\n    final_predictions = []\n    scores=[]\n    print(\"Fold: \",end=\" \")\n\n    # iterate in every 5 folds wich data divided\n    for fold in range(5):     \n        \n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain.target\n        yvalid = xvalid.target\n        \n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n\n        # categorical data -> ordinal encoder        \n        ordinal_encoder = preprocessing.OrdinalEncoder()\n        xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n        xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n        xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    \n        # standarization numerical cols\n        scaler = preprocessing.StandardScaler()\n        xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n        xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n        xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n\n        # choose model with the hyperparameters of this round\n        model = XGBRegressor(random_state=fold, \n                            n_jobs=-1,               # use m\u00e1x number of CPUs\n                            tree_method='gpu_hist',  # use GPU\n                            eval_metric='rmse',\n                            **actual_params)         # add the actual hyperparameters\n        \n        # fit model and error calculation\n        model.fit(xtrain, ytrain)\n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)        \n        scores.append(rmse)\n        \n        print(fold,end =\" \") \n\n    time_elapsed = time() - start_time  \n    mean_scores = np.mean(scores)\n\n    print (\"\\n\\nMean MSE: \",mean_scores)\n    \n    if (mean_scores < best_score):\n        best_score = mean_scores\n        best_params = actual_params.copy()\n\n    print (\"Best MSE: \",best_score)\n    print (\"Actual params: \",actual_params)      \n    print (\"Best params:   \",best_params)  \n\n    print((\"Time elapsed %.2f min, estimated remaining time %.2f min\\n\")% \n            (time_elapsed\/60, ((len(param_combinations)-num_iteration)*time_elapsed\/60 )))\n\n","af3e45cc":"final_predictions = []\nscores=[]\nfor fold in range(5):     \n        \n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n        \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    # categorical data -> ordinal encoder        \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    \n    # standarization numerical cols\n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n\n    model = XGBRegressor(random_state=fold, \n                            n_jobs=-1,               # use m\u00e1x number of CPUs\n                            tree_method='gpu_hist',  # use GPU\n                            eval_metric='rmse',\n                            **best_params)         # add the actual hyperparameters\n\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold,rmse)\n    scores.append(rmse)\n    \n\nprint(\"MSE final: \", np.mean(scores)) ","9c67bd6f":"preds = np.mean(np.column_stack(final_predictions), axis=1)","3c5f611b":"sample_submission.target = preds\nsample_submission.to_csv(\"submission.csv\", index=False)","a2db880b":"## Train and predict with best hyperparameters","0a4e8c62":"# Hyperparameter optimization  - Grid search \nImplemented for learning purpose with no optimization libraries.\nTurn on GPU.","662b21a2":"## Train the model and search for best hyperparameters"}}