{"cell_type":{"9e0f29ff":"code","0b9c0632":"code","c8c1e540":"code","3c72e1fe":"code","4677a245":"code","a3a75a8b":"code","d591a74a":"code","0a03e16c":"code","ffcb1cab":"code","cb67c993":"code","ef8e270e":"code","ab9dc940":"code","53edddbc":"code","39b67247":"code","9590f41d":"code","6e9cde12":"code","7ed6d457":"code","d81b927a":"code","868587e9":"markdown"},"source":{"9e0f29ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0b9c0632":"import matplotlib.pyplot as plt","c8c1e540":"# Import numpy and pandas\nimport numpy as np\nimport pandas as pd\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv('..\/input\/gapminder.csv')\ndf.columns\n\n\n\n\n","3c72e1fe":"# Create arrays for features and target variable\ny = df.life.values\nX_fertility= df.fertility.values\nX= df[['population','fertility','GDP']].values\n\ny=y.reshape(-1,1)\nX_fertility=X_fertility.reshape(-1,1)\nX= X.reshape(-1,3)\nprint(y.shape,X_fertility.shape,X.shape)","4677a245":"# Import LinearRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Create the regressor: reg\nreg = LinearRegression()\n\n# Create the prediction space\nprediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)\n\n\n# Fit the model to the data\nreg.fit(X_fertility,y)\n\n# Compute predictions over the prediction space: y_pred\ny_pred = reg.predict(prediction_space)\n\n# Print R^2 \nprint(reg.score(X_fertility, y))\n\n# Plot regression line\nplt.scatter(x=df.fertility.values,y=df.life.values,color='blue')\nplt.plot(prediction_space, y_pred, color='black', linewidth=3)\nplt.show()","a3a75a8b":"# Import necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size =.3, random_state=42)\n\n# Create the regressor: reg_all\nreg_all = LinearRegression()\n\n# Fit the regressor to the training data\nreg_all.fit(X_train,y_train)\n\n# Predict on the test data: y_pred\ny_pred = reg_all.predict(X_test)\n\n# Compute and print R^2 and RMSE\nprint(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test,y_pred))\nprint(\"Root Mean Squared Error: {}\".format(rmse))\n","d591a74a":"from sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(reg_all, X, y, cv=5)\nprint(cv_scores)\nprint(np.mean(cv_scores))","0a03e16c":"# Perform 3-fold CV\ncvscores_3 = cross_val_score(reg,X,y,cv=3)\nprint(np.mean(cvscores_3))\n\n# Perform 10-fold CV\ncvscores_10 = cross_val_score(reg,X,y,cv=10)\nprint(np.mean(cvscores_10))\n","ffcb1cab":"%timeit cvscores_3","cb67c993":"%timeit cvscores_10","ef8e270e":"# Import Lasso\nfrom sklearn.linear_model import Lasso\n\n# Instantiate a lasso regressor: lasso\nlasso = Lasso(alpha=0.4,normalize=True)\nX=df.drop(['life','Region'],axis=1).values\ny=df.life.values\nX=X.reshape(-1,8)\n\n\n# Fit the regressor to the data\nlasso.fit(X,y)\n\n# Compute and print the coefficients\nlasso_coef = lasso.fit(X,y).coef_\nprint(lasso_coef)\ndf_columns= df.drop(['life','Region'],axis=1).columns\n","ab9dc940":"plt.plot(range(len(df_columns)), lasso_coef)\nplt.xticks(range(len(df_columns)), df_columns.values, rotation=60)\nplt.margins(0.02)\nplt.show()","53edddbc":"def display_plot(cv_scores, cv_scores_std):\n    fig = plt.figure()\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(alpha_space, cv_scores)\n\n    std_error = cv_scores_std \/ np.sqrt(10)\n\n    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n    ax.set_ylabel('CV Score +\/- Std Error')\n    ax.set_xlabel('Alpha')\n    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n    ax.set_xscale('log')\n    plt.show()","39b67247":"# Import necessary modules\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n# Setup the array of alphas and lists to store scores\nalpha_space = np.logspace(-4, 0, 50)\nridge_scores = []\nridge_scores_std = []\n\n# Create a ridge regressor: ridge\nridge = Ridge(normalize=True)\n\n# Compute scores over range of alphas\nfor alpha in alpha_space:\n\n    # Specify the alpha value to use: ridge.alpha\n    ridge.alpha = alpha\n    \n    # Perform 10-fold CV: ridge_cv_scores\n    ridge_cv_scores = cross_val_score(ridge,X,y,cv=10)\n    \n    # Append the mean of ridge_cv_scores to ridge_scores\n    ridge_scores.append(np.mean(ridge_cv_scores))\n    \n    # Append the std of ridge_cv_scores to ridge_scores_std\n    ridge_scores_std.append(np.std(ridge_cv_scores))\n\n# Display the plot\ndisplay_plot(ridge_scores, ridge_scores_std)\n","9590f41d":"# Import necessary modules\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n\n# Create the hyperparameter grid\nl1_space = np.linspace(0, 1, 30)\nparam_grid = {'l1_ratio': l1_space}\n\n# Instantiate the ElasticNet regressor: elastic_net\nelastic_net = ElasticNet()\n\n# Setup the GridSearchCV object: gm_cv\ngm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n\n# Fit it to the training data\ngm_cv.fit(X_train,y_train)\n\n# Predict on the test set and compute metrics\ny_pred = gm_cv.predict(X_test)\nr2 = gm_cv.score(X_test, y_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\nprint(\"Tuned ElasticNet R squared: {}\".format(r2))\nprint(\"Tuned ElasticNet MSE: {}\".format(mse))\n","6e9cde12":"df.boxplot( 'life','Region', rot=60)\n\n# Show the plot\nplt.show()","7ed6d457":"df_region = pd.get_dummies(df,drop_first=True)\n\n# Print the new columns of df_region\nprint(df_region.columns)","d81b927a":"from sklearn.linear_model import Ridge\n\n# Instantiate a ridge regressor: ridge\nridge = Ridge(alpha=0.5,normalize=True)\n\n# Perform 5-fold cross-validation: ridge_cv\nridge_cv = cross_val_score(ridge,X,y,cv=5)\n\n# Print the cross-validated scores\nprint(ridge_cv)","868587e9":"it seems like 'child_mortality' is the most important feature when predicting life expectancy."}}