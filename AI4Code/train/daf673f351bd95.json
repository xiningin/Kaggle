{"cell_type":{"1df67f86":"code","8df9334a":"code","447f6e7d":"code","940716b6":"code","fbe2ce94":"code","e4a50612":"code","a160dfb9":"code","ba34725e":"code","0b190fa4":"code","27990754":"code","6a8bf05a":"code","0b2c7c5c":"code","b1606492":"code","e127ff43":"code","a437459e":"code","c102d6c3":"code","b82f52e9":"code","d9cd9655":"code","940df6e8":"code","416dcfc9":"code","c29f3d5a":"code","be4437e5":"code","8b5f03e2":"code","6e293802":"code","7423d2f9":"code","4bb84ce8":"code","4e6208df":"markdown","937dae76":"markdown","e0f99a0d":"markdown","2180ae5e":"markdown","efd4a3d1":"markdown","b1f8755f":"markdown","7e5c979c":"markdown","8005f04b":"markdown","5348b6be":"markdown","3551152c":"markdown","3267bb43":"markdown","1f4ffa6d":"markdown","190e28c0":"markdown","1ab678ae":"markdown"},"source":{"1df67f86":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8df9334a":"!pip install wurlitzer -q\nfrom wurlitzer import sys_pipes","447f6e7d":"traincsv = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntraincsv.drop(['Id'], axis=1, inplace=True)","940716b6":"drop_col_list = []\n\nfor col in traincsv.columns:\n    if traincsv[col].isna().sum() > 900:\n        drop_col_list.append(col)\n        \nprint(f'{drop_col_list} is\/are going to be drop')\ntraincsv.drop(drop_col_list, axis=1, inplace=True)","fbe2ce94":"sns.pairplot(traincsv, y_vars='SalePrice' ,kind='hist', height=5, dropna=False)","e4a50612":"index = traincsv.loc[traincsv['LotArea']>40000].index\ntraincsv.drop(index, axis=0, inplace=True)","a160dfb9":"sns.kdeplot(data=traincsv, x='SalePrice')","ba34725e":"using_col_list = list(traincsv.columns)\nusing_col_list.remove('SalePrice')\n# for col in unusing_col_list:\n#     using_col_list.remove(col)\n    \nprint(len(using_col_list))","0b190fa4":"year_col = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt','YrSold']","27990754":"traincsv = traincsv.astype({'YearBuilt':'str', 'YearRemodAdd':'str', 'GarageYrBlt':'str', 'YrSold':'str'})","6a8bf05a":"label_mean, label_std = traincsv['SalePrice'].mean(), traincsv['SalePrice'].std()\ntraincsv['SalePrice'] = (traincsv['SalePrice']-label_mean)\/label_std","0b2c7c5c":"from sklearn.model_selection import train_test_split\ndftrain, dfval = train_test_split(traincsv, test_size=0.2)","b1606492":"!pip install tensorflow_decision_forests -q","e127ff43":"import tensorflow_decision_forests as tfdf\nprint(f'Tensorflow Decision Forests Version : {tfdf.__version__}')","a437459e":"feature_list = []\nfor col in using_col_list:\n    if dftrain[col].dtype != 'object' and col not in year_col:\n        if col != 'SalePrice':\n            feature_list.append(tfdf.keras.FeatureUsage(name=col,\n                                                        semantic=tfdf.keras.FeatureSemantic.NUMERICAL))\n    else:\n        feature_list.append(tfdf.keras.FeatureUsage(name=col, \n                                                    semantic=tfdf.keras.FeatureSemantic.CATEGORICAL))","c102d6c3":"trainds = tfdf.keras.pd_dataframe_to_tf_dataset(dftrain, label='SalePrice',task = tfdf.keras.Task.REGRESSION)\nvalds = tfdf.keras.pd_dataframe_to_tf_dataset(dfval, label='SalePrice',task = tfdf.keras.Task.REGRESSION)","b82f52e9":"model = tfdf.keras.RandomForestModel(features = feature_list, task = tfdf.keras.Task.REGRESSION,\n                                    exclude_non_specified_features=True,\n                                    num_trees=1000, max_depth=16,\n                                    split_axis=\"SPARSE_OBLIQUE\",categorical_algorithm=\"RANDOM\",\n                                    missing_value_policy='RANDOM_LOCAL_IMPUTATION',\n                                    sparse_oblique_normalization='STANDARD_DEVIATION',\n                                    compute_oob_variable_importances=True,\n                                    winner_take_all=False)\nmodel.compile(metrics=[\"Accuracy\"])","d9cd9655":"with sys_pipes():\n    model.fit(trainds)","940df6e8":"inspector = model.make_inspector()\nprint(f\"Available variable importances:\")\nfor importance in inspector.variable_importances().keys():\n    print(\"\\t\", importance)","416dcfc9":"inspector.variable_importances()[\"MEAN_INCREASE_IN_RMSE\"]","c29f3d5a":"forecast = model.predict(valds)","be4437e5":"real = dfval['SalePrice']\nplt.plot(real, real, marker='.', label='Base Line')\nplt.scatter(real, forecast, marker='.', color='r', label='Predicted')\nplt.legend()\nplt.show()","8b5f03e2":"testcsv = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nID = testcsv.pop('Id')\ntestcsv = testcsv[using_col_list]\ntestcsv = testcsv.astype({'YearBuilt':'str', 'YearRemodAdd':'str', 'GarageYrBlt':'str', 'YrSold':'str'})\ntestcsv.head(1)","6e293802":"testds = tfdf.keras.pd_dataframe_to_tf_dataset(testcsv, task = tfdf.keras.Task.REGRESSION)","7423d2f9":"test_vale = model.predict(testds)\ntest_vale = test_vale*label_std + label_mean\n\nsub = pd.DataFrame([i for i in test_vale],index=ID, columns=['SalePrice'])\nsub.head(1)","4bb84ce8":"sub.to_csv(\"predictions.csv\", mode='w')","4e6208df":"- using_col_list is features' list for training. If you want to delete some columns, just make unusing columns' list and remove them from using_col_list.","937dae76":"- LotArea has some unnormal values, So drop","e0f99a0d":"** IMPORTANT **\n<bar> In this part(feature_list), You can choose trainable columns. If you have some columns that you dont want to use input features, just don't add them into feature_list.<\/bar>\n\n- Making feature_list by using tfdf.keras.FeatureUsage.\n- If feature has numcerical nature, set semantic NUMERICAL. \n- Else feautre has categorical nature, set semantic CATEGORICAL","2180ae5e":"- Normalizing label is useful for model. And Save Label_mean and Label_std for test or val dataset.","efd4a3d1":"- Check relations each columns and label column('SalePrice') with hist. If you want to check graphs bigger, Just double click that!","b1f8755f":"- Year features are more likely category then numeric, So making list for them.","7e5c979c":"- Change Data type numeric to string.","8005f04b":"#### Dealing DataFrame Part\n- wurlitzer, sys_pipes are for training","5348b6be":"## Using TensorFlow RandomForestModel\n#### Beneift of RandomForestModel with TensorFlow \n1. We can deal some data that include NA\/Null values without preprocessing.\n2. We don't need to do any preporcessing for making train dataset.\n3. Easy to Build model","3551152c":"GET 0.146 SCORE","3267bb43":"#### Build Model\n** PARAMETERS INFORMATION **\n1. features - Specify the list and semantic of the input features of the model. If not specified, all the available features will be used. If specified and if exclude_non_specified_features=True, only the features in features will be used by the model. \n2. categorical_algorithm - RANDOM: Best splits among a set of random candidate. Find the a categorical split of the form \"value \\in mask\" using a random search. This solution can be seen as an approximation of the CART algorithm. This method is a strong alternative to CART. This algorithm is inspired from section \"5.1 Categorical Variables\" of \"Random Forest\", 2001. \n3. compute_oob_variable_importances - If true, compute the Out-of-bag feature importance (then available in the summary and model inspector). Note that the OOB feature importance can be expensive to compute. \n4. missing_value_policy - RANDOM_LOCAL_IMPUTATION: Missing attribute values are imputed from randomly sampled values from the training examples in the current node. This method was proposed by Clinic et al. in \"Random Survival Forests\" (https:\/\/projecteuclid.org\/download\/pdfview_1\/euclid.aoas\/1223908043). \n5. sparse_oblique_normalization - For sparse oblique splits i.e. split_axis=SPARSE_OBLIQUE. Normalization applied on the features, before applying the sparse oblique projections. STANDARD_DEVIATION: Normalize the feature by the estimated standard deviation on the entire train dataset. Also known as Z-Score normalization.\n6. split_axis - SPARSE_OBLIQUE: Sparse oblique splits (i.e. splits one a small number of features) from \"Sparse Projection Oblique Random Forests\", Tomita et al., 2020.","1f4ffa6d":"Set Dataset as REGRESSION by using task. If you want to classifer model, Just set task to CATEOGICAL.","190e28c0":"- I'm going to drop columns that have NA Values more than 900.","1ab678ae":"#### Tensor Part"}}