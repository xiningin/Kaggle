{"cell_type":{"bc869e34":"code","60aac2a3":"code","43b43555":"code","d1af9595":"code","0b518c1f":"code","681822e1":"code","1fa3345f":"code","65a2e71c":"code","22e84eab":"code","39997ef6":"code","b0b03b6f":"code","2fdea630":"code","af5c3649":"code","a8aee130":"code","574814f4":"code","b582a066":"code","c5e4af0c":"code","c89212ce":"code","1a4f5860":"code","cd9416a8":"code","bf53c723":"code","cb66a3f5":"code","8dd8f92a":"code","89a2b21f":"code","6c9bb978":"code","044a9fdb":"code","b12d290f":"code","029afa30":"code","03284111":"code","ad0c52c4":"code","6caa9ce9":"code","dd5fa9d9":"code","f7b25e66":"code","112b646b":"code","1f902c6f":"code","93c74904":"code","b18eadd0":"code","105972e3":"code","f73bff7f":"code","1903bf60":"code","a33136b0":"code","1ab74543":"code","955638c2":"code","97b53d3a":"code","635cd1fe":"code","9c7e67a6":"code","84784ecd":"code","f3a7da12":"markdown","4b7238d0":"markdown","84b204ca":"markdown","a9b19003":"markdown","80014a24":"markdown","a291a7ed":"markdown","fe49528f":"markdown","1155f4fa":"markdown","fc041a94":"markdown","01eb9f92":"markdown"},"source":{"bc869e34":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","60aac2a3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","43b43555":"df_daily = pd.read_csv(\"\/kaggle\/input\/bike-share-daily-data\/bike_sharing_daily.csv\")\ndf_hourly = pd.read_csv(\"\/kaggle\/input\/bike-share-daily-data\/bike_sharing_hourly.csv\")","d1af9595":"df = [df_daily,df_hourly]\nfor d in df:\n    print (d.isnull().any().sum())","0b518c1f":"def fixing_datatypes(df):\n    # Fixing the datatypes \n    df['dteday'] = df['dteday'].astype('datetime64')\n    df.loc[:,'season':'mnth'] = df.loc[:,'season':'mnth'].astype('category')\n    df[['holiday','workingday']] = df[['holiday','workingday']].astype('bool')\n    df[['weekday','weathersit']] = df[['weekday','weathersit']].astype('category')\n\n    \n      \n    return df","681822e1":"df_daily = fixing_datatypes(df_daily)\ndf_hourly = fixing_datatypes(df_hourly)\n\ndf_hourly['hr'] = df_hourly['hr'].astype('category')","1fa3345f":"# set the index to datetime\ndf_daily = df_daily.set_index('dteday')","65a2e71c":"df_daily[\"cnt\"].plot(figsize = (40,10))\ndf_daily_resample = df_daily.resample(rule = \"M\").mean().ffill()\ndf_daily_resample[\"cnt\"].plot(figsize = (40,10))","22e84eab":"df_daily_resample[\"cnt\"].resample(\"M\").mean().plot.bar(figsize = (40,10))","39997ef6":"from statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(df_daily_resample[\"cnt\"], model = \"multiplicative\")\nfig = result.plot()","b0b03b6f":"df_daily.head()","2fdea630":"sns.boxplot(df_daily['mnth'], df_daily['cnt'])","af5c3649":"df_daily[df_daily.cnt == df_daily.cnt.min()]","a8aee130":"df_daily[df_daily.cnt == df_daily.cnt.max()]","574814f4":"from sklearn.cluster import KMeans","b582a066":"\ndf_daily = df_daily.reset_index()\ndf_daily.head()","c5e4af0c":"# subset of the daily data for the k-means anomaly detection test\ndf = df_daily[[\"cnt\", \"season\", 'yr', 'mnth', 'holiday', 'weekday', 'workingday',\n       'weathersit', 'atemp', 'hum', 'windspeed']]","c89212ce":"# check the correlation of features of this subset \nsns.heatmap(abs(df.corr()), annot = True)","1a4f5860":"from sklearn.preprocessing import StandardScaler\nScaler = StandardScaler()\nnp_scaled = Scaler.fit_transform(df)\ndf = pd.DataFrame(np_scaled)\ndf.head()","cd9416a8":"from sklearn.decomposition import PCA\n# reduce to 2 importants features\npca = PCA(n_components=2)\ndf = pca.fit_transform(df)\n# standardize these 2 new features\nmin_max_scaler = StandardScaler()\nnp_scaled = min_max_scaler.fit_transform(df)\ndf = pd.DataFrame(np_scaled)","bf53c723":"df.head()","cb66a3f5":"# calculate with different number of centroids to see the loss plot (elbow method)\nn_cluster = range(1, 20)\nkmeans = [KMeans(n_clusters=i).fit(df) for i in n_cluster]\nscores = [kmeans[i].score(df) for i in range(len(kmeans))]\nfig, ax = plt.subplots()\nax.plot(n_cluster, scores)\nplt.show()","8dd8f92a":"SelectedKey = 3","89a2b21f":"df_daily['cluster'] = kmeans[SelectedKey].predict(df)\ndf_daily['principal_feature1'] = df[0]\ndf_daily['principal_feature2'] = df[1]\ndf_daily['cluster'].value_counts().plot.bar()","6c9bb978":"df_daily.head()","044a9fdb":"#plot the different clusters with the 2 main features\n\nsns.scatterplot(df_daily['principal_feature1'], df_daily['principal_feature2'], hue=df_daily[\"cluster\"], data = df_daily, style = df_daily[\"cluster\"])\n","b12d290f":"# return Series of distance between each point and his distance with the closest centroid\ndef getDistanceByPoint(data, model):\n    distance = pd.Series()\n    for i in range(0,len(data)):\n        Xa = np.array(data.loc[i])\n        Xb = model.cluster_centers_[model.labels_[i]-1]\n        distance.set_value(i, np.linalg.norm(Xa-Xb))\n    return distance","029afa30":"outliers_fraction = 0.1","03284111":"# get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly\ndistance = getDistanceByPoint(df, kmeans[SelectedKey])\nnumber_of_outliers = int(outliers_fraction*len(distance))\nthreshold = distance.nlargest(number_of_outliers).min() #Return the first n rows ordered by columns in descending order.\n# anomaly21 contain the anomaly result of method 2.1 Cluster (0:normal, 1:anomaly) \ndf_daily['anomaly21'] = (distance >= threshold).astype(int)","ad0c52c4":"fig, ax = plt.subplots(figsize=(10,10))\nsns.scatterplot(df_daily['principal_feature1'], df_daily['principal_feature2'], hue=df_daily['anomaly21'], data = df_daily, style = df_daily[\"cluster\"], ax = ax)\nplt.show()","6caa9ce9":"# set the index to datetime\ndf_daily = df_daily.set_index('dteday')","dd5fa9d9":"df_daily[\"cnt\"].plot(figsize = (20,10))\nplt.scatter (df_daily.index[df_daily['anomaly21'] == 1], df_daily[\"cnt\"][df_daily['anomaly21'] == 1], c = \"red\")","f7b25e66":"anomalies = df_daily[df_daily['anomaly21'] == 1]\nanomalies.loc['2012-10-1':'2012-12-31'][anomalies[\"cnt\"] < 500]","112b646b":"sns.barplot(x = df_daily[\"season\"] , y = df_daily[\"atemp\"], hue = df_daily[\"anomaly21\"])","1f902c6f":"sns.pairplot(df_daily[[ 'anomaly21', 'atemp', 'casual', 'hum']] , hue = \"anomaly21\")","93c74904":"df_daily = df_daily.reset_index()","b18eadd0":"# subset of the daily data for the k-means anomaly detection test\ndf = df_daily[[\"cnt\", \"season\", 'yr', 'mnth', 'holiday', 'weekday', 'workingday',\n       'weathersit', 'atemp', 'hum', 'windspeed']]\nscaler = StandardScaler()\nnp_scaled = scaler.fit_transform(df)\ndf = pd.DataFrame(np_scaled)","105972e3":"from sklearn.ensemble import IsolationForest\nmodel =  IsolationForest(contamination = outliers_fraction)\nmodel.fit(df)\ndf_daily['anomaly_isolation'] = pd.Series(model.predict(df))\ndf_daily['anomaly_isolation'] = df_daily['anomaly_isolation'].map( {1: 0, -1: 1} )\nprint(df_daily['anomaly_isolation'].value_counts())","f73bff7f":" # set the index to datetime\ndf_daily = df_daily.set_index('dteday')\ndf_daily[\"cnt\"].plot(figsize = (20,10))\nplt.scatter (df_daily.index[df_daily['anomaly_isolation'] == 1], df_daily[\"cnt\"][df_daily['anomaly_isolation'] == 1], c = \"red\")","1903bf60":"df_daily = df_daily.reset_index()\n# subset of the daily data for the k-means anomaly detection test\ndf = df_daily[[\"cnt\", \"season\", 'yr', 'mnth', 'holiday', 'weekday', 'workingday',\n       'weathersit', 'atemp', 'hum', 'windspeed']]\nscaler = StandardScaler()\nnp_scaled = scaler.fit_transform(df)\ndf = pd.DataFrame(np_scaled)\ndf.shape","a33136b0":"# important parameters and train\/test size\nprediction_time = 1 \ntestdatasize = 100\nunroll_length = 22\ntestdatacut = testdatasize + unroll_length  + 1\n\n#train data\nx_train = df[0:-prediction_time-testdatacut].as_matrix()\ny_train = df[prediction_time:-testdatacut  ][0].as_matrix()\n\n# test data\nx_test = df[0-testdatacut:-prediction_time].as_matrix()\ny_test = df[prediction_time-testdatacut:  ][0].as_matrix()\n\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","1ab74543":"def unroll(data,sequence_length):\n    result = []\n    for index in range(len(data) - sequence_length):\n        result.append(data[index: index + sequence_length])\n    return np.asarray(result)\n\n# adapt the datasets for the sequence data shape\nx_train = unroll(x_train,unroll_length)\nx_test  = unroll(x_test,unroll_length)\ny_train = y_train[-x_train.shape[0]:]\ny_test  = y_test[-x_test.shape[0]:]\n\n# see the shape\nprint(\"x_train\", x_train.shape)\nprint(\"y_train\", y_train.shape)\nprint(\"x_test\", x_test.shape)\nprint(\"y_test\", y_test.shape)","955638c2":"from keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import Sequential\nmodel = Sequential()\n\nmodel.add(LSTM(input_dim=x_train.shape[-1],output_dim=unroll_length,return_sequences=True))\nmodel.add(Dropout(0.2))\n\n\nmodel.add(LSTM(100,return_sequences=False))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=1))\nmodel.add(Activation('linear'))\n\nmodel.compile(loss='mse', optimizer='rmsprop')\nmodel.summary()","97b53d3a":"model.fit(x_train,y_train,batch_size=5,nb_epoch=30,validation_split=0.1)","635cd1fe":"\ndiff=[]\nratio=[]\np = model.predict(x_test)\n\nfor u in range(len(y_test)):\n    pr = p[u][0]\n    ratio.append((y_test[u]\/pr)-1)\n    diff.append(abs(y_test[u]- pr))\nfig, axs = plt.subplots()\naxs.plot(p,color='red', label='prediction')\naxs.plot(y_test,color='blue', label='y_test')\nplt.legend(loc='upper right')\nplt.show()","9c7e67a6":"diff = pd.Series(diff)\nnumber_of_outliers = int(outliers_fraction*len(diff))\nthreshold = diff.nlargest(number_of_outliers).min()\n# data with anomaly label (test data part)\ntest = (diff >= threshold).astype(int)\n# the training data part where we didn't predict anything (overfitting possible): no anomaly\ncomplement = pd.Series(0, index=np.arange(len(df)-testdatasize))\n# # add the data to the main\ndf_daily['anomalyLSTM'] = complement.append(test, ignore_index='True')\nprint(df_daily['anomalyLSTM'].value_counts())","84784ecd":"# set the index to datetime\ndf_daily = df_daily.set_index('dteday')\ndf_daily[\"cnt\"].plot(figsize = (20,10))\nplt.scatter (df_daily.index[df_daily['anomalyLSTM'] == 1], df_daily[\"cnt\"][df_daily['anomalyLSTM'] == 1], c = \"red\")","f3a7da12":"### ETS","4b7238d0":"### Features\n","84b204ca":"# Anomaly Detection\n\n### Cluster based anomaly detection (K-mean)\n\n* points far away from cluster centroids might be considered anomalies","a9b19003":"### Missing Values","80014a24":"* 29 oct 2012 shows as anomaly as found previously","a291a7ed":"# LSTM","fe49528f":"## Extremes and Boxplots","1155f4fa":"### model","fc041a94":"# Isolation Forest\n\n* Isolation Forest, like any tree ensemble method, is built on the basis of decision trees. In these trees, partitions are created by first randomly selecting a feature and then selecting a random split value between the minimum and maximum value of the selected feature.\n* outliers are less frequent than regular observations and are different from them in terms of values (they lie further away from the regular observations in the feature space). That is why by using such random partitioning they should be identified closer to the root of the tree (shorter average path length, i.e., the number of edges an observation must pass in the tree going from the root to the terminal node), with fewer splits necessary.","01eb9f92":"# df_daily - Target \"cnt\" Analysis"}}