{"cell_type":{"38d824fa":"code","6f0152f2":"code","b2b6b2fb":"code","1483a073":"code","37fa6cd3":"code","e6064f81":"code","9f19bec1":"code","e15dbded":"code","9d65082e":"code","534e1010":"code","92fb7487":"code","931323f0":"code","44b3d8fa":"code","bbfa84e9":"code","eeb72f67":"code","abeb3737":"code","d247747d":"code","be6da49a":"code","e803f484":"code","03b7eb91":"code","47c9d227":"code","44c88a81":"code","11897d1a":"code","5e9056c2":"code","eb709f8e":"code","35c48609":"code","cd271806":"code","1e32dc6f":"code","29bb39ad":"code","772b1c73":"code","c030f29d":"code","fb0d6ee6":"code","51076361":"code","2d0ecfc0":"code","089a27ea":"code","52b896cd":"code","05f9788d":"code","93d9f6ea":"code","8d5776be":"code","83491317":"code","938da2e8":"markdown","53bb52e7":"markdown","a629c45d":"markdown","b5117ff5":"markdown","fee39cd4":"markdown","e7d327e6":"markdown","035f8ae3":"markdown","e3bbefdf":"markdown","e25a40a4":"markdown","df65693c":"markdown","92b8bf46":"markdown","2e3dbbd5":"markdown","62ecdd3c":"markdown"},"source":{"38d824fa":"import numpy as np\nimport pandas as pd\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nimport gc\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.applications.imagenet_utils import _obtain_input_shape\nimport tensorflow as tf\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D","6f0152f2":"img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]","b2b6b2fb":"train_df = pd.read_csv(\"..\/input\/tgs-salt-identification-challenge\/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"..\/input\/tgs-salt-identification-challenge\/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","1483a073":"train_df[\"images\"] = [np.array(load_img(\"..\/input\/tgs-salt-identification-challenge\/train\/images\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","37fa6cd3":"train_df[\"masks\"] = [np.array(load_img(\"..\/input\/tgs-salt-identification-challenge\/train\/masks\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","e6064f81":"train_df[\"coverage\"] = train_df.masks.map(np.sum) \/ pow(img_size_ori, 2)","9f19bec1":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","e15dbded":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images \/ grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(train_df.index[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    ax = axs[int(i \/ grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","9d65082e":"ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)","534e1010":"tmp_img = np.zeros((img_size_target, img_size_target), dtype=train_df.images.loc[ids_train[10]].dtype)\ntmp_img[:img_size_ori, :img_size_ori] = train_df.images.loc[ids_train[10]]\nfix, axs = plt.subplots(1, 2, figsize=(15,5))\naxs[0].imshow(tmp_img, cmap=\"Greys\")\naxs[0].set_title(\"Original image\")\naxs[1].imshow(x_train[10].squeeze(), cmap=\"Greys\")\naxs[1].set_title(\"Scaled image\")","92fb7487":"def conv_block_simple(prevlayer, filters, prefix, strides=(1, 1)):\n    conv = Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", strides=strides, name=prefix + \"_conv\")(prevlayer)\n    conv = BatchNormalization(name=prefix + \"_bn\")(conv)\n    conv = Activation('relu', name=prefix + \"_activation\")(conv)\n    return conv\n\ndef conv_block_simple_no_bn(prevlayer, filters, prefix, strides=(1, 1)):\n    conv = Conv2D(filters, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\", strides=strides, name=prefix + \"_conv\")(prevlayer)\n    conv = Activation('relu', name=prefix + \"_activation\")(conv)\n    return conv","931323f0":"K.clear_session()\ndef get_unet_resnet(input_shape):\n    resnet_base = ResNet50(input_shape=input_shape, include_top=False)\n\n    for l in resnet_base.layers:\n        l.trainable = True\n    conv1 = resnet_base.get_layer(\"activation_1\").output\n    conv2 = resnet_base.get_layer(\"activation_10\").output\n    conv3 = resnet_base.get_layer(\"activation_22\").output\n    conv4 = resnet_base.get_layer(\"activation_40\").output\n    conv5 = resnet_base.get_layer(\"activation_49\").output\n\n    up6 = concatenate([UpSampling2D()(conv5), conv4], axis=-1)\n    conv6 = conv_block_simple(up6, 256, \"conv6_1\")\n    conv6 = conv_block_simple(conv6, 256, \"conv6_2\")\n\n    up7 = concatenate([UpSampling2D()(conv6), conv3], axis=-1)\n    conv7 = conv_block_simple(up7, 192, \"conv7_1\")\n    conv7 = conv_block_simple(conv7, 192, \"conv7_2\")\n\n    up8 = concatenate([UpSampling2D()(conv7), conv2], axis=-1)\n    conv8 = conv_block_simple(up8, 128, \"conv8_1\")\n    conv8 = conv_block_simple(conv8, 128, \"conv8_2\")\n\n    up9 = concatenate([UpSampling2D()(conv8), conv1], axis=-1)\n    conv9 = conv_block_simple(up9, 64, \"conv9_1\")\n    conv9 = conv_block_simple(conv9, 64, \"conv9_2\")\n\n    up10 = UpSampling2D()(conv9)\n    conv10 = conv_block_simple(up10, 32, \"conv10_1\")\n    conv10 = conv_block_simple(conv10, 32, \"conv10_2\")\n    conv10 = SpatialDropout2D(0.2)(conv10)\n    x = Conv2D(1, (1, 1), activation=\"sigmoid\", name=\"prediction\")(conv10)\n    model = Model(resnet_base.input, x)\n    return model","44b3d8fa":"import warnings\n\nfrom keras.applications.imagenet_utils import _obtain_input_shape\nfrom keras.layers import Input\nfrom keras import layers\nfrom keras.layers import Dense\nfrom keras.layers import Activation\nfrom keras.layers import Flatten\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import AveragePooling2D\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers import BatchNormalization\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import get_source_inputs\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\n\nWEIGHTS_PATH = 'https:\/\/github.com\/fchollet\/deep-learning-models\/releases\/download\/v0.2\/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\nWEIGHTS_PATH_NO_TOP = 'https:\/\/github.com\/fchollet\/deep-learning-models\/releases\/download\/v0.2\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n\ndef identity_block(input_tensor, kernel_size, filters, stage, block):\n    \"\"\"The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'keras.., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    \"\"\"\n    filters1, filters2, filters3 = filters\n    if K.image_data_format() == 'channels_last':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters2, kernel_size,\n               padding='same', name=conv_name_base + '2b')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n\n    x = layers.add([x, input_tensor])\n    x = Activation('relu')(x)\n    return x\n\n\ndef conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n    \"\"\"A block that has a conv layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'keras.., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    Note that from stage 3, the first conv layer at main path is with strides=(2,2)\n    And the shortcut should have strides=(2,2) as well\n    \"\"\"\n    filters1, filters2, filters3 = filters\n    if K.image_data_format() == 'channels_last':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = Conv2D(filters1, (1, 1), strides=strides,\n               name=conv_name_base + '2a')(input_tensor)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters2, kernel_size, padding='same',\n               name=conv_name_base + '2b')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n\n    shortcut = Conv2D(filters3, (1, 1), strides=strides,\n                      name=conv_name_base + '1')(input_tensor)\n    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n\n    x = layers.add([x, shortcut])\n    x = Activation('relu')(x)\n    return x","bbfa84e9":"def ResNet50(include_top=True, weights='imagenet',\n             input_tensor=None, input_shape=None,\n             pooling=None,\n             classes=1000):\n    if weights not in {'imagenet', None}:\n        raise ValueError('The `weights` argument should be either '\n                         '`None` (random initialization) or `imagenet` '\n                         '(pre-training on ImageNet).')\n\n    if weights == 'imagenet' and include_top and classes != 1000:\n        raise ValueError('If using `weights` as imagenet with `include_top`'\n                         ' as true, `classes` should be 1000')\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n    if K.image_data_format() == 'channels_last':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n\n    x = Conv2D(64, (7, 7), strides=(2, 2), padding='same', name='conv1')(img_input)\n    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n\n    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n\n#     x = AveragePooling2D((7, 7), name='avg_pool')(x)\n\n#     if include_top:\n#         x = Flatten()(x)\n#         x = Dense(classes, activation='softmax', name='fc1000')(x)\n#     else:\n#         if pooling == 'avg':\n#             x = GlobalAveragePooling2D()(x)\n#         elif pooling == 'max':\n#             x = GlobalMaxPooling2D()(x)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name='resnet50')\n\n    # load weights\n    if weights == 'imagenet':\n        if include_top:\n            weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels.h5',\n                                    WEIGHTS_PATH,\n                                    cache_subdir='models',\n                                    md5_hash='a7b3fe01876f51b976af0dea6bc144eb')\n        else:\n            weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                                    WEIGHTS_PATH_NO_TOP,\n                                    cache_subdir='models',\n                                    md5_hash='a268eb855778b3df3c7506639542a6af')\n        model.load_weights(weights_path,by_name=True)\n    return model","eeb72f67":"def relu6(x):\n    return K.relu(x, max_value=6)","abeb3737":"# model = UNet((img_size_target,img_size_target,1),start_ch=16,depth=5,batchnorm=True)\n# model = get_unet_mobilenet(input_shape=(128,128,1))\nmodel = get_unet_resnet(input_shape=(img_size_target,img_size_target,3))","d247747d":"def get_iou_vector(A, B):\n    batch_size = A.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        t, p = A[batch]>0, B[batch]>0        \n        intersection = np.logical_and(t, p)\n        union = np.logical_or(t, p)\n        iou = (np.sum(intersection > 0) + 1e-10 )\/ (np.sum(union > 0) + 1e-10)\n        thresholds = np.arange(0.5, 1, 0.05)\n        s = []\n        for thresh in thresholds:\n            s.append(iou > thresh)\n        metric.append(np.mean(s))\n\n    return np.mean(metric)\n\ndef my_iou_metric(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred>0.5], tf.float64)\n\ndef my_iou_metric_2(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred >0], tf.float64)","be6da49a":"from keras.losses import binary_crossentropy\nfrom keras import backend as K\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) \/ (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\ndef bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n\ndef weighted_bce_loss(y_true, y_pred, weight):\n    epsilon = 1e-7\n    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n    logit_y_pred = K.log(y_pred \/ (1. - y_pred))\n    loss = weight * (logit_y_pred * (1. - y_true) + \n                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n    return K.sum(loss) \/ K.sum(weight)\n\ndef weighted_dice_loss(y_true, y_pred, weight):\n    smooth = 1.\n    w, m1, m2 = weight, y_true, y_pred\n    intersection = (m1 * m2)\n    score = (2. * K.sum(w * intersection) + smooth) \/ (K.sum(w * m1) + K.sum(w * m2) + smooth)\n    loss = 1. - K.sum(score)\n    return loss\n\ndef weighted_bce_dice_loss(y_true, y_pred):\n    y_true = K.cast(y_true, 'float32')\n    y_pred = K.cast(y_pred, 'float32')\n    # if we want to get same size of output, kernel size must be odd\n    averaged_mask = K.pool2d(\n            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n    weight = K.ones_like(averaged_mask)\n    w0 = K.sum(weight)\n    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n    w1 = K.sum(weight)\n    weight *= (w0 \/ w1)\n    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n    return loss","e803f484":"import gc\ngc.enable()","03b7eb91":"model.compile(loss=bce_dice_loss, optimizer='adam', metrics=[my_iou_metric])","47c9d227":"x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)","44c88a81":"fig, axs = plt.subplots(2, 10, figsize=(15,3))\nfor i in range(10):\n    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n    axs[1][i].imshow(x_train[int(len(x_train)\/2 + i)].squeeze(), cmap=\"Greys\")\n    axs[1][i].imshow(y_train[int(len(y_train)\/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\nfig.suptitle(\"Top row: original images, bottom row: augmented images\")","11897d1a":"x_train = np.repeat(x_train,3,axis=3)\nx_valid = np.repeat(x_valid,3,axis=3)","5e9056c2":"model_checkpoint = ModelCheckpoint(\".\/keras.model\",monitor='val_my_iou_metric', \n                                   mode = 'max', save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=4, min_lr=0.00001, verbose=1)\n\nepochs = 100\nbatch_size = 32\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[model_checkpoint, reduce_lr],shuffle=True,verbose=2)","eb709f8e":"del ids_train, x_train, y_train, cov_train,depth_train,depths_df,depth_test\ngc.collect()","35c48609":"plt.plot(history.history['my_iou_metric'][1:])\nplt.plot(history.history['val_my_iou_metric'][1:])\nplt.title('model loss')\nplt.ylabel('val_my_iou_metric')\nplt.xlabel('epoch')\nplt.legend(['train','Validation'], loc='upper left')\nplt.show()","cd271806":"# model = load_model(\".\/keras.model\")\n# Load best model\nmodel.load_weights('.\/keras.model')","1e32dc6f":"def predict_result(model,x_test,img_size_target): # predict both orginal and reflect x\n    x_test_reflect =  np.array([np.fliplr(x) for x in x_test])\n    preds_test1 = model.predict([x_test]).reshape(-1, img_size_target, img_size_target)\n    preds_test2_refect = model.predict([x_test_reflect]).reshape(-1, img_size_target, img_size_target)\n    preds_test2 = np.array([ np.fliplr(x) for x in preds_test2_refect] )\n    preds_avg = (preds_test1 +preds_test2)\/2\n    return preds_avg","29bb39ad":"preds_valid = predict_result(model,x_valid,img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\ny_valid_ori = np.array([downsample(x) for x in y_valid])","772b1c73":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images \/ grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i \/ grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","c030f29d":"# src: https:\/\/www.kaggle.com\/aglotero\/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection \/ union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp \/ (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","fb0d6ee6":"## Scoring for last model\nthresholds = np.linspace(0.3, 0.7, 31)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","51076361":"threshold_best_index = np.argmax(ious) \niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]\n\nplt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","2d0ecfc0":"threshold_best","089a27ea":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images \/ grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i \/ grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","52b896cd":"del x_valid, y_valid,preds_valid,y_valid_ori,train_df\ngc.collect()","05f9788d":"def rle_encode(im):\n    pixels = im.flatten(order = 'F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\nbatch_size = 500\npreds_test = []\ni = 0\nwhile i < test_df.shape[0]:\n    print('Images Processed:',i)\n    index_val = test_df.index[i:i+batch_size]\n    depth_val = test_df.z[i:i+batch_size]\n    x_test = np.array([upsample(np.array(load_img(\"..\/input\/tgs-salt-identification-challenge\/test\/images\/{}.png\".format(idx), grayscale=True))) \/ 255 for idx in (index_val)]).reshape(-1, img_size_target, img_size_target, 1)\n    x_test = np.repeat(x_test,3,axis=3)\n    preds_test_temp = predict_result(model,x_test,img_size_target)\n    if i==0:\n        preds_test = preds_test_temp\n    else:\n        preds_test = np.concatenate([preds_test,preds_test_temp],axis=0)\n#     print(preds_test.shape)\n    i += batch_size","93d9f6ea":"del x_test,preds_test_temp\ngc.collect()","8d5776be":"import time\nt1 = time.time()\npred_dict = {idx: rle_encode(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}\nt2 = time.time()\n\nprint(f\"Usedtime = {t2-t1} s\")","83491317":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('resnet50_pretrained_submission.csv')","938da2e8":"# Training","53bb52e7":"# Build model","a629c45d":"# Show some example images","b5117ff5":"In this Kernel I show you how to use pre-trained Resnet50 with image size 128. \n\nThanks to everyone for sharing their insights. \nPlease check out the following kernels for more ideas:\n\nJack (Jiaxin) Shao: https:\/\/www.kaggle.com\/shaojiaxin\/u-net-with-simple-resnet-blocks-v2-new-loss\n\nBruno G. do Amaral: https:\/\/www.kaggle.com\/bguberfain\/elastic-transform-for-data-augmentation\n\nPeter H\u00f6nigschmid: https:\/\/www.kaggle.com\/phoenigs\/u-net-dropout-augmentation-stratification\n\nJuan C EsquivelTGS: https:\/\/www.kaggle.com\/jcesquiveld\/tgs-vanilla-u-net-with-simple-augmentation\n\nNPHard: https:\/\/www.kaggle.com\/meaninglesslives\/apply-crf-unet-resnet\n\nPS - You can easily get around 0.82 LB Score with some modifications. Happy Kaggling :-)","fee39cd4":"# Params and helpers","e7d327e6":"# Another sanity check with adjusted threshold\nAgain some sample images with the adjusted threshold.","035f8ae3":"# Calculating the salt coverage and salt coverage classes\nCounting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\nPlotting the distribution of coverages and coverage classes, and the class against the raw coverage.","e3bbefdf":"# Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions.","e25a40a4":"# Create train\/validation split stratified by salt coverage\nUsing the salt coverage as a stratification criterion. Also show an image to check for correct upsampling.","df65693c":"# Scoring\nScore the model and do a threshold optimization by the best IoU.","92b8bf46":"# Submission\nLoad, predict and submit the test image predictions.","2e3dbbd5":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255.","62ecdd3c":"# Loading of training\/testing ids and depths\nReading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train."}}