{"cell_type":{"3408d3f0":"code","7322782c":"code","0ded40bb":"code","5acef491":"code","eeef649d":"code","f9b6d15f":"code","9196bcea":"code","e17b1d38":"code","44ed5e13":"code","109f1755":"code","c8acaaff":"code","2415d42b":"code","9d04f037":"code","ae3eba81":"code","0dfd9b6a":"code","2305e4fe":"code","61955dd8":"code","037525a6":"code","24a3b560":"code","dc11a323":"code","49f8c10c":"code","e3e36f7c":"code","f5ae18f9":"code","02938bd3":"code","b1af71b1":"code","f9268bc0":"code","6afa06f9":"code","101491f5":"code","18ecc7b0":"code","a65d094b":"code","f1ece703":"code","78aa77ae":"code","bb25533f":"code","2d7d4cba":"code","0d840568":"code","6427764c":"code","659bdda4":"code","ea737f9a":"code","792f0175":"code","375ff411":"code","80811435":"code","685d8377":"code","c3440508":"code","5c7ac4bb":"code","f14a9359":"code","aac6fea5":"code","508e0cde":"code","f9575a17":"code","022bf666":"code","30c75e5c":"code","4100ca96":"code","02da2384":"code","55c0a72c":"code","c0a376af":"code","70206272":"code","fac20c98":"code","a2997b58":"code","ea2789c6":"code","a8ec707a":"code","729ed0bd":"code","2ce9ce87":"code","12a562fe":"code","eb0340d5":"code","b4396395":"code","a1ebc2ee":"code","67469871":"code","2f291157":"code","1f4aed86":"code","8dd0ea03":"code","da96314d":"code","c75dbb02":"code","965ce77e":"code","34cd4df5":"code","10e82c19":"code","9c016fb8":"code","cd64bd17":"code","6fe9dbd0":"code","37ed103c":"code","86a42c3b":"code","a81ff7f1":"code","3ac1d7ba":"code","e627bbe9":"code","5321dcdc":"code","7d7585d9":"code","7925f734":"code","1167c8f4":"code","749d97da":"code","4a220c1e":"code","ed884a0f":"code","e8b86384":"code","99062782":"code","2613b9d2":"code","c682e956":"code","796bdb3f":"code","f5ec8c94":"code","bf8651c9":"code","0a0338f8":"code","64975188":"code","eb9b74fa":"code","2576fc9e":"code","e99dcebb":"code","2ac8ced6":"code","b3a5e84c":"code","8b9d57b2":"code","cbcdefed":"code","6b351e1f":"code","50e667e0":"code","bbfa3eff":"code","efe59456":"code","0b538bda":"code","c522ba1a":"code","e71982d6":"code","b9f5fa88":"code","a7b03685":"code","0b8ad8e6":"code","5e417acc":"code","50622612":"code","77a8601e":"code","e8393962":"code","da4cef54":"code","7338bd42":"code","c24bcce8":"code","f9de9ac8":"code","b639c702":"code","1e8a793c":"code","dee51773":"code","84add643":"code","96e782da":"code","95de8476":"code","f92e9f5a":"code","66e38e58":"code","a9f0661e":"code","e277edc2":"code","014cbe0e":"code","3ec5453c":"code","70d57bea":"code","0e381818":"code","8c2873ca":"code","f9acf36f":"code","ff9af9ac":"code","0bcac4c4":"code","7134c6a3":"code","ad2dd173":"code","9cad709a":"code","8bf7dfe7":"code","6864cf59":"code","c1b04c6e":"code","c40c9927":"code","c8669e53":"code","eb319802":"code","1d7eaa12":"code","ad1215c2":"code","f51dc421":"code","744007b5":"code","08377f20":"code","0fc50846":"code","a4aaea0e":"code","055e7bb5":"code","f10d3ea3":"code","bdae01d9":"code","e983d723":"code","85aecb62":"code","5fc53061":"code","9ddc8ba7":"code","3e3e48de":"code","c2ee547b":"code","fc57e1aa":"code","1bea2b38":"code","ad78b79a":"code","544ba898":"code","425d4f1a":"code","293b11a5":"code","37a92f81":"code","ade8b85b":"code","fa981eb8":"code","2796c4e0":"code","b158cacc":"code","6f579983":"code","d3d3cd33":"code","ba700190":"code","66e01ea4":"code","03d32632":"code","403c7504":"code","cb337256":"code","e610ac79":"code","8f2637b7":"code","cd279872":"code","4dfe64b0":"code","a67de776":"code","2dba70d7":"code","72af776e":"code","550a3e00":"code","f4fce02c":"code","4f899fc0":"code","d8f5007d":"code","59fb6d39":"code","c2f795e1":"code","716ccc8a":"code","91a5cc58":"code","12f226d6":"code","ed0378e1":"code","52b92260":"code","edb42b9c":"code","a64e2735":"markdown","f21f278d":"markdown","d3aac91a":"markdown","7336b2c5":"markdown","397792f8":"markdown","31e20c8a":"markdown","e0e3a45f":"markdown","7fdd42b4":"markdown","fe54b86d":"markdown","3f4e5ef9":"markdown","dc4fefc8":"markdown","19686949":"markdown","566ea100":"markdown","0a737c48":"markdown","114808a6":"markdown","6ad32bdc":"markdown","582b51b3":"markdown","f4301299":"markdown","0d677601":"markdown","a14427da":"markdown","799074f0":"markdown","dc2eb834":"markdown","e2451b0a":"markdown","309d54ec":"markdown","326427ae":"markdown","013231c2":"markdown","100441dd":"markdown","cff037c7":"markdown","0046e624":"markdown","1a534747":"markdown","fe1e3678":"markdown","9c8b8ab5":"markdown","781c857a":"markdown","7f9ac3a7":"markdown","3b0a51b8":"markdown","43401fe6":"markdown","e71d8be7":"markdown","b0a594ee":"markdown","a15fcfb5":"markdown","9de5d881":"markdown","904b002d":"markdown","c24b1483":"markdown","e2db9db7":"markdown","cd9f057a":"markdown","33150787":"markdown","69e9d8d4":"markdown","2b71cac3":"markdown","7038e881":"markdown","bc8e1830":"markdown","edfb6a5f":"markdown","44bb691c":"markdown","18ba451a":"markdown","57c03318":"markdown","0aa16d4d":"markdown","f8749da0":"markdown","1335bdae":"markdown","e97d76b1":"markdown","00fc48ba":"markdown","0bafff80":"markdown","e313508b":"markdown","d3121625":"markdown","f9e69e29":"markdown","bcdb0045":"markdown","11987221":"markdown","3494bc3e":"markdown","c05e49f6":"markdown","51f8535a":"markdown","eeeb15a4":"markdown","6cfd1c7c":"markdown","4311abd9":"markdown","e57a7947":"markdown","0b48e385":"markdown","abe830e1":"markdown","fb27e2e7":"markdown","bc34bd17":"markdown","aa22fea0":"markdown","766b973a":"markdown","de8b128f":"markdown","0ed2f9af":"markdown","f97425ec":"markdown","aefe7369":"markdown","383400d4":"markdown","000dad1e":"markdown","77dd8741":"markdown","7947b711":"markdown","cb18fb59":"markdown","ad1a55c0":"markdown","45d02a95":"markdown","f911b553":"markdown","51746d1b":"markdown","fecf7133":"markdown","8a05b36b":"markdown","163aaf4b":"markdown","a1a9d50d":"markdown","ce3727d0":"markdown","3e4138b9":"markdown","0aa0c5de":"markdown","c882bdba":"markdown","a0a3b1d7":"markdown","cb76ffc9":"markdown","69eab8d1":"markdown","1f56f8ae":"markdown","ba634204":"markdown"},"source":{"3408d3f0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","7322782c":"# Setting option to display all the columns in dataset\npd.set_option('display.max_columns', 500)","0ded40bb":"# Setting option to display 300 rows in dataset\npd.set_option('display.max_rows', 300)","5acef491":"# Setting plot style to ggplot\nplt.style.use('ggplot')","eeef649d":"# Import required libaries for Ridge, Lasso and GridSearchCV\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn import linear_model\nfrom sklearn.model_selection import GridSearchCV","f9b6d15f":"df = pd.read_csv('..\/input\/house-price-prediction\/train.csv')","9196bcea":"df.shape","e17b1d38":"df.head()","44ed5e13":"df.describe()","109f1755":"df.info()","c8acaaff":"import missingno as mno","2415d42b":"mno.bar(df[df.columns[df.isna().any()]], sort=\"ascending\", color='purple')","9d04f037":"# Getting total number of NULL values and percentage of the columns\n# null_columns = df.columns[df.isna().any()]\nnull_value_count = df[df.columns[df.isna().any()]].isna().sum().sort_values(ascending=False)\nnull_percentage = (df[df.columns[df.isna().any()]].isna().sum() * 100 \/ df.shape[0]).sort_values(ascending=False)","ae3eba81":"null_data = pd.concat([null_value_count, null_percentage], axis=1, keys=['Count', 'Percentage'])","0dfd9b6a":"# Columns with NULL values and % of NULLs are populated in descending order\nnull_data","2305e4fe":"null_data[ null_data['Percentage'] > 15].index","61955dd8":"# Dropping these columns from the dataframe `df`\ndf.drop(columns=null_data[ null_data['Percentage'] > 15].index, inplace=True)","037525a6":"# Checking Shape\ndf.shape","24a3b560":"null_data = null_data[null_data['Percentage'] < 15]\nnull_data","dc11a323":"plt.figure(figsize=[20, 60])\n\nfor i, var in enumerate(null_data.index,start=1):\n    plt.subplot(5,3,i)\n    sns.countplot(df[var])\n    plt.title(f\"Countplot for {var}\")","49f8c10c":"# Impute \"GarageType\" with 'None' as it has a meaningful value\ndf['GarageType'].fillna('None', inplace=True)","e3e36f7c":"# Impute \"GarageYrBlt\" with Median value\ndf['GarageYrBlt'].fillna(df['GarageYrBlt'].median(), inplace=True)","f5ae18f9":"# Impute \"GarageFinish\" with 'None' as it has a meaningful value\ndf['GarageFinish'].fillna('None', inplace=True)","02938bd3":"# Impute \"GarageQual\" with 'None' as it has a meaningful value\ndf['GarageQual'].fillna('None', inplace=True)","b1af71b1":"# Impute \"GarageQual\" with 'None' as it has a meaningful value\ndf['GarageCond'].fillna('None', inplace=True)","f9268bc0":"# Impute \"BsmtExposure\" with 'None' as it has a meaningful value\ndf['BsmtExposure'].fillna('None', inplace=True)","6afa06f9":"# Impute \"BsmtFinType2\" with 'None' as it has a meaningful value\ndf['BsmtFinType2'].fillna('None', inplace=True)","101491f5":"# Impute \"BsmtFinType1\" with 'None' as it has a meaningful value\ndf['BsmtFinType1'].fillna('None', inplace=True)","18ecc7b0":"# Impute \"BsmtCond\" with 'None' as it has a meaningful value\ndf['BsmtCond'].fillna('None', inplace=True)","a65d094b":"# Impute \"BsmtQual\" with 'None' as it has a meaningful value\ndf['BsmtQual'].fillna('None', inplace=True)","f1ece703":"# Impute \"MasVnrArea\" with Median value\ndf['MasVnrArea'].fillna(df['MasVnrArea'].median(), inplace=True)","78aa77ae":"# Impute \"MasVnrType\" with Mode value\ndf['MasVnrType'].fillna(df['MasVnrType'].mode()[0], inplace=True)","bb25533f":"# Impute \"Electrical\" with Mode value\nval = df['Electrical'].mode()[0]\ndf['Electrical'].fillna(val, inplace=True)","2d7d4cba":"null_value_count = df[df.columns[df.isna().any()]].isna().sum().sort_values(ascending=False)\nnull_percentage = (df[df.columns[df.isna().any()]].isna().sum() * 100 \/ df.shape[0]).sort_values(ascending=False)\nnull_data = pd.concat([null_value_count, null_percentage], axis=1, keys=['Count', 'Percentage'])\nnull_data","0d840568":"# Dropping \"Street\" as it doesn't contain much variance. Most values are assigned to \"Pave\"\nsns.countplot(df['Street'])\ndf.drop(columns='Street', inplace=True)","6427764c":"# Dropping \"Condition2\" as it doesn't contain much variance. Most values are assigned to \"Norm\"\nsns.countplot(df['Condition2'])\ndf.drop(columns='Condition2', inplace=True)","659bdda4":"# Dropping \"RoofMatl\" as it doesn't contain much variance. Most values are assigned to \"CompShg\"\nsns.countplot(df['RoofMatl'])\ndf.drop(columns='RoofMatl', inplace=True)","ea737f9a":"# Dropping \"Heating\" as it doesn't contain much variance. Most values are assigned to \"GasA\"\nsns.countplot(df['Heating'])\ndf.drop(columns='Heating', inplace=True)","792f0175":"# Dropping \"LowQualFinSF\" as it doesn't contain much variance. Most values are assigned to \"0\"\nsns.countplot(df['LowQualFinSF'])\ndf.drop(columns='LowQualFinSF', inplace=True)","375ff411":"# Dropping \"3SsnPorch\" as it doesn't contain much variance. Most values are assigned to \"0\"\nsns.countplot(df['3SsnPorch'])\ndf.drop(columns='3SsnPorch', inplace=True)","80811435":"# Dropping \"ScreenPorch\" as it doesn't contain much variance. Most values are assigned to \"0\"\nsns.countplot(df['ScreenPorch'])\ndf.drop(columns='ScreenPorch', inplace=True)","685d8377":"# Dropping \"PoolArea\" as it doesn't contain much variance. Most values are assigned to \"0\"\nsns.countplot(df['PoolArea'])\ndf.drop(columns='PoolArea', inplace=True)","c3440508":"# Dropping \"MiscVal\" as it doesn't contain much variance. Most values are assigned to \"0\"\nsns.countplot(df['MiscVal'])\ndf.drop(columns='MiscVal', inplace=True)","5c7ac4bb":"# Dropping \"Utilities\" as it doesn't contain much variance. Most values are assigned to \"AllPub\"\nsns.countplot(df['Utilities'])\ndf.drop(columns='Utilities', inplace=True)","f14a9359":"# Dropping \"Id\" columns as it's not important\ndf.drop(columns='Id', inplace=True)","aac6fea5":"df.shape","508e0cde":"df.head()","f9575a17":"CurrentYear = 2021","022bf666":"df['Age_Built_Years'] = CurrentYear - df['YearBuilt']","30c75e5c":"df['Age_RemodAdd_Years'] = CurrentYear - df['YearRemodAdd']","4100ca96":"df['Age_GarageYrBlt_Years'] = CurrentYear - df['GarageYrBlt']","02da2384":"df['Age_YrSold_Years'] = CurrentYear - df['YrSold']","55c0a72c":"df.drop(columns=['YearBuilt','YearRemodAdd','GarageYrBlt','YrSold'], inplace=True)","c0a376af":"df.shape","70206272":"df.info()","fac20c98":"# Getting categorical variables\ncat_var = df.select_dtypes(include='object').columns\ncat_var","a2997b58":"# Getting numerical variables\nnum_var = df.select_dtypes(exclude='object').columns\nnum_var","ea2789c6":"plt.figure(figsize=[20,15])\nsns.displot(df['SalePrice'], aspect=2)\nplt.show()","a8ec707a":"df['SalePrice'].skew()","729ed0bd":"df['SalePrice'].kurtosis()","2ce9ce87":"# Plotting distribution of a log transformed \"SalePrice\" column\nplt.figure(figsize=[20,15])\nsns.displot(np.log(df['SalePrice']), aspect=2)\nplt.show()","12a562fe":"# Create Log Transformed \"SalePrice\" column\ndf['Transformed_SalePrice'] = np.log(df['SalePrice'])","eb0340d5":"# Plotting distribution of a \"Transformed_SalePrice\" column\nplt.figure(figsize=[20,15])\nsns.displot(df['Transformed_SalePrice'], aspect=2)\nplt.show()","b4396395":"# Checking if the newly added field is present in the dataset\ndf.head()","a1ebc2ee":"df.shape","67469871":"var = 'MasVnrType'\nsns.boxplot(x=var, y='SalePrice', data=df, width=1)\nplt.title(f\"Comparison of '{var}' with 'SalePrice' \")\nplt.show()","2f291157":"var = 'BsmtQual'\nsns.boxplot(x=var, y='SalePrice', data=df, width=1)\nplt.title(f\"Comparison of '{var}' with 'SalePrice' \")\nplt.show()","1f4aed86":"var = 'ExterQual'\nsns.boxplot(x=var, y='SalePrice', data=df, width=1)\nplt.title(f\"Comparison of '{var}' with 'SalePrice' \")\nplt.show()","8dd0ea03":"var = 'CentralAir'\nsns.boxplot(x=var, y='SalePrice', data=df, width=1)\nplt.title(f\"Comparison of '{var}' with 'SalePrice' \")\nplt.show()","da96314d":"var = 'SaleCondition'\nsns.boxplot(x=var, y='SalePrice', data=df, width=1)\nplt.title(f\"Comparison of '{var}' with 'SalePrice' \")\nplt.show()","c75dbb02":"plt.figure(figsize=[30,10])\nvar = 'Age_Built_Years'\nsns.boxplot(x=var, y='SalePrice', data=df, width=1)\nplt.title(f\"Comparison of '{var}' with 'SalePrice' \")\nplt.show()","965ce77e":"plt.figure(figsize=[30,20])\ncorr = df.drop(columns='Transformed_SalePrice', axis=1)\ncorr = corr.corr()\nmask=np.triu(np.ones_like(corr)) # Masking upper half of the triangle\nsns.heatmap(corr, cmap='RdYlGn', annot=True, fmt='.2f', square=True, mask=mask)\nplt.show()","34cd4df5":"plt.figure(figsize=[10,6])\n\nk = 10 # number of variables for a heatmap\ncols = corr.nlargest(k,'SalePrice')['SalePrice'].index\ncorrmatrix = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1)\nhm = sns.heatmap(corrmatrix, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size':10},\n                yticklabels=cols.values, xticklabels=cols.values, cmap='RdYlGn')\nplt.show()","10e82c19":"columns = ['SalePrice','OverallQual','GrLivArea','GarageCars','TotalBsmtSF','FullBath','MasVnrArea']\nsns.pairplot(df[columns], size=3)\nplt.show()","9c016fb8":"# Getting categorical variables\ncat_var = df.select_dtypes(include='object').columns\nprint(len(cat_var))","cd64bd17":"df_categorical = df.select_dtypes(include='object')","6fe9dbd0":"df_categorical.columns.shape","37ed103c":"# One Hot Encoding on categorical columns\ndf_dummies = pd.get_dummies(df_categorical, drop_first=True)","86a42c3b":"df_dummies.head()","a81ff7f1":"df.drop(list(df_categorical.columns), axis=1, inplace=True)","3ac1d7ba":"df.shape","e627bbe9":"df = pd.concat([df, df_dummies], axis=1)","5321dcdc":"df.shape","7d7585d9":"df.head()","7925f734":"X = df.drop(['SalePrice','Transformed_SalePrice'], axis=1)\ny = df['Transformed_SalePrice']","1167c8f4":"X.head()","749d97da":"y.head()","4a220c1e":"# Import library from sklearn from train, test & split\nfrom sklearn.model_selection import train_test_split","ed884a0f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","e8b86384":"print(X_train.shape)\nprint(y_train.shape)","99062782":"print(X_test.shape)\nprint(y_test.shape)","2613b9d2":"df.info(verbose=True, null_counts=True)","c682e956":"# Getting numerical variables\nnum_var = X_train.select_dtypes(include=['int64', 'float64']).columns\nprint(len(num_var))\nnum_var","796bdb3f":"# Import library for MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Instantiate an object of MinMaxScaler\nsc = MinMaxScaler()\n\n# Perform fit and transform on the train dataset\nX_train[num_var] = sc.fit_transform(X_train[num_var])\n\n# Perform only transform on the test dataset\nX_test[num_var] = sc.transform(X_test[num_var])","f5ec8c94":"print(X_train.shape)\nprint(X_test.shape)","bf8651c9":"# Import library for Linear Regression\nfrom sklearn.linear_model import LinearRegression","0a0338f8":"# Base Model\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","64975188":"# Checking co-efficients for all features\ncoeff = pd.DataFrame(regressor.coef_, X.columns, columns=['Coefficients'])\ncoeff","eb9b74fa":"# Checking the Intercept\nregressor.intercept_","2576fc9e":"# Making predictions on test data using the model\ny_pred = regressor.predict(X_test)","e99dcebb":"# Showing actual and predicted values side by side\ndf_result = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf_result","2ac8ced6":"X_train.shape","b3a5e84c":"X_test.shape","8b9d57b2":"# Import r2_score and mean squared error from library\nfrom sklearn.metrics import r2_score, mean_squared_error","cbcdefed":"y_pred_train = regressor.predict(X_train)\ny_pred_test = regressor.predict(X_test)\n\nmetric = []\nr2_train_lr = r2_score(y_train, y_pred_train)\nprint(f\"Train r2 score is : {r2_train_lr}\")\nmetric.append(r2_train_lr)\n\nr2_test_lr = r2_score(y_test, y_pred_test)\nprint(f\"Test r2 score is : {r2_test_lr}\")\nmetric.append(r2_test_lr)\n\nrss1_lr = np.sum(np.square(y_train - y_pred_train))\nprint(f\"Train RSS score is : {rss1_lr}\")\nmetric.append(rss1_lr)\n\nrss2_lr = np.sum(np.square(y_test - y_pred_test))\nprint(f\"Test RSS score is : {rss2_lr}\")\nmetric.append(rss2_lr)\n\nmse_train_lr = mean_squared_error(y_train, y_pred_train)\nprint(f\"Train MSE score is : {mse_train_lr}\")\nmetric.append(mse_train_lr**0.5)\n\nmse_test_lr = mean_squared_error(y_test, y_pred_test)\nprint(f\"Test MSE score is : {mse_test_lr}\")\nmetric.append(mse_test_lr**0.5)","6b351e1f":"# Importing RFE from library\nfrom sklearn.feature_selection import RFE","50e667e0":"X_train.columns","bbfa3eff":"# Instantiating a LinearRegression object\nlm = LinearRegression()\nlm.fit(X_train,y_train)\n\n# RFE\nrfe = RFE(lm,50)\n# Running RFE\nrfe = rfe.fit(X_train,y_train)","efe59456":"# Checking list of selections and ranking of each variable by RFE\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","0b538bda":"# Looking at the 50 variables selected by RFE\ncol = X_train.columns[rfe.support_]\ncol","c522ba1a":"# Looking at the variables which were NOT selected by RFE\nX_train.columns[~rfe.support_]","e71982d6":"# Creating a dataframe \"X_train_rfe\" with variables selected by RFE\nX_train_rfe = X_train[col]","b9f5fa88":"X_train_rfe.shape","a7b03685":"# Importing required statsmodels library\nimport statsmodels.api as sm\n\n# From statsmodels importing variance_inflation_factor\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","0b8ad8e6":"# Functions to build model using statsmodels and check VIF\ndef build_model(X,y):\n    X = sm.add_constant(X) #Adding the constant\n    lm = sm.OLS(y,X).fit() # fitting the model\n    print(lm.summary()) # model summary\n    return X\n    \ndef checkVIF(X):\n    vif = pd.DataFrame()\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return(vif)","5e417acc":"X_train_new = build_model(X_train_rfe,y_train)","50622612":"HighVIF = checkVIF(X_train_new)","77a8601e":"HighVIF[ HighVIF['VIF'] >=5 ]","e8393962":"highVIF_columns = list(HighVIF[ HighVIF['VIF'] >=5 ].Features.values)","da4cef54":"len(highVIF_columns)","7338bd42":"highVIF_columns.remove('const')","c24bcce8":"len(highVIF_columns)","f9de9ac8":"highVIF_columns","b639c702":"X_train_rfe.shape","1e8a793c":"X_train_rfe.drop(highVIF_columns, axis=1, inplace=True)","dee51773":"X_train_rfe.shape","84add643":"X_train_rfe.head()","96e782da":"# 1. Add a constant\nX_train_lm = sm.add_constant(X_train_rfe)\n\n# 2. Create model\nlr = sm.OLS(y_train,X_train_lm)\n\n# 3. Fit the model\nlm = lr.fit()\n\n# 4. View the parameters\nlm.params","95de8476":"lm.summary()","f92e9f5a":"print(X_train_rfe.shape)\nprint(X_test.shape)","66e38e58":"names = X_train_rfe.columns","a9f0661e":"# We will create a new dataframe \"X_test_new\" by removing the columns dropped by the train data set\nX_test_new = X_test[names]","e277edc2":"X_test_new.shape","014cbe0e":"# Add a constant\nX_test_new = sm.add_constant(X_test_new)","3ec5453c":"X_test_new.shape","70d57bea":"y_pred_train = lm.predict(X_train_lm)\ny_pred_test = lm.predict(X_test_new)\n\nmetric_lr = []\nr2_train_lr = r2_score(y_train, y_pred_train)\nprint(f\"Train r2 score is : {r2_train_lr}\")\nmetric_lr.append(r2_train_lr)\n\nr2_test_lr = r2_score(y_test, y_pred_test)\nprint(f\"Test r2 score is : {r2_test_lr}\")\nmetric_lr.append(r2_test_lr)\n\nrss1_lr = np.sum(np.square(y_train - y_pred_train))\nprint(f\"Train RSS score is : {rss1_lr}\")\nmetric_lr.append(rss1_lr)\n\nrss2_lr = np.sum(np.square(y_test - y_pred_test))\nprint(f\"Test RSS score is : {rss2_lr}\")\nmetric_lr.append(rss2_lr)\n\nmse_train_lr = mean_squared_error(y_train, y_pred_train)\nprint(f\"Train MSE score is : {mse_train_lr}\")\nmetric_lr.append(mse_train_lr**0.5)\n\nmse_test_lr = mean_squared_error(y_test, y_pred_test)\nprint(f\"Test MSE score is : {mse_test_lr}\")\nmetric_lr.append(mse_test_lr**0.5)","0e381818":"# Plotting graph b\/w actual and predicted values with train data\n\nfig = plt.figure()\n# sns.scatterplot(y_test,y_test_pred)\nsns.regplot(x=y_train,y=y_pred_train, scatter_kws = {\"color\": 'blue',\"s\": 20}, line_kws = {\"color\": 'green', \"lw\": 3}, marker='*')\nfig.suptitle('y_train vs y_train_pred', fontsize = 20)              \nplt.xlabel('y_train', fontsize = 18)                          \nplt.ylabel('y_train_pred', fontsize = 16)\nplt.show()","8c2873ca":"# Plotting graph b\/w actual and predicted values with test data\n\nfig = plt.figure()\n# sns.scatterplot(y_test,y_test_pred)\nsns.regplot(x=y_test,y=y_pred_test, scatter_kws = {\"color\": 'blue',\"s\": 50}, line_kws = {\"color\": 'green', \"lw\": 3}, marker='+')\nfig.suptitle('y_test vs y_test_pred', fontsize = 20)              \nplt.xlabel('y_test', fontsize = 18)                          \nplt.ylabel('y_test_pred', fontsize = 16)\nplt.show()","f9acf36f":"res = y_train - y_pred_train\nfig = plt.figure()\nsns.distplot(res,bins=20)\nfig.suptitle('Error Terms',fontsize=20)\nplt.xlabel('Errors',fontsize=15)\nplt.show()","ff9af9ac":"res = y_test - y_pred_test\nfig = plt.figure()\nsns.distplot(res,bins=20)\nfig.suptitle('Error Terms',fontsize=20)\nplt.xlabel('Errors',fontsize=15)\nplt.show()","0bcac4c4":"df_linear = pd.DataFrame(index=X_train_rfe.columns)\ndf_linear.rows = X_train_rfe.columns\ndf_linear['Linear'] = lm.params\ndf_linear","7134c6a3":"# List of 32 features\nnames = X_train_rfe.columns\nprint(len(names))\nprint(list(names))","ad2dd173":"# Import required libaries for Ridge, Lasso and GridSearchCV\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn import linear_model\nfrom sklearn.model_selection import GridSearchCV","9cad709a":"# Get only 32 features for both X_train and X_test\nX_train = X_train[names]\nX_test = X_test[names]","8bf7dfe7":"print(X_train.shape)\nprint(X_test.shape)","6864cf59":"# list of alphas to tune\n\nparams = { 'alpha' : [0.0001,0.001,0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,\n                       0.7,0.8,0.9,1.0,2.0,3.0,4.0,5.0,6.0,7.0,\n                      8.0,9.0,10.0,20,50,100,500,1000]}","c1b04c6e":"# Applying lasso regression with 5 fold cross validation\n\nlasso = Lasso()\nfolds = 5\nmodel_cv = GridSearchCV(estimator=lasso,\n                       param_grid=params,\n                       scoring='neg_mean_absolute_error',\n                       cv=folds,\n                       return_train_score=True,\n                       verbose=1)\nmodel_cv.fit(X_train, y_train)","c40c9927":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","c8669e53":"cv_results.shape","eb319802":"# Plotting train scores with alpha\n\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'], color='green')\nplt.xlabel('alpha')\nplt.ylabel(\"Negative Mean Absolute Error\")\n\nplt.title(\"Neg MAE and Alphas\")\nplt.show()","1d7eaa12":"# Plotting testing scores with alpha\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'], color='orange')\nplt.xlabel('alpha')\nplt.ylabel(\"Negative Mean Absolute Error\")\n\nplt.title(\"Neg MAE and Alphas\")\nplt.show()","ad1215c2":"optimalvalue_lasso = model_cv.best_params_['alpha']\noptimalvalue_lasso","f51dc421":"alpha = optimalvalue_lasso\nlasso = Lasso(alpha=alpha)\nlasso.fit(X_train, y_train)","744007b5":"lasso.coef_","08377f20":"df_lasso = pd.DataFrame(index=X_train.columns)\ndf_lasso.rows = X_train.columns\ndf_lasso['Lasso'] = lasso.coef_\ndf_lasso","0fc50846":"y_pred_train = lasso.predict(X_train)\ny_pred_test = lasso.predict(X_test)\n\nmetric_l = []\nr2_train_lr = r2_score(y_train, y_pred_train)\nprint(f\"Train r2 score is : {r2_train_lr}\")\nmetric_l.append(r2_train_lr)\n\nr2_test_lr = r2_score(y_test, y_pred_test)\nprint(f\"Test r2 score is : {r2_test_lr}\")\nmetric_l.append(r2_test_lr)\n\nrss1_lr = np.sum(np.square(y_train - y_pred_train))\nprint(f\"Train RSS score is : {rss1_lr}\")\nmetric_l.append(rss1_lr)\n\nrss2_lr = np.sum(np.square(y_test - y_pred_test))\nprint(f\"Test RSS score is : {rss2_lr}\")\nmetric_l.append(rss2_lr)\n\nmse_train_lr = mean_squared_error(y_train, y_pred_train)\nprint(f\"Train MSE score is : {mse_train_lr}\")\nmetric_l.append(mse_train_lr**0.5)\n\nmse_test_lr = mean_squared_error(y_test, y_pred_test)\nprint(f\"Test MSE score is : {mse_test_lr}\")\nmetric_l.append(mse_test_lr**0.5)","a4aaea0e":"# Import required libaries for Ridge, Lasso and GridSearchCV\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn import linear_model\nfrom sklearn.model_selection import GridSearchCV","055e7bb5":"# Get only 32 features for both X_train and X_test\nX_train = X_train[names]\nX_test = X_test[names]","f10d3ea3":"print(X_train.shape)\nprint(X_test.shape)","bdae01d9":"# list of alphas to tune\n\nparams = { 'alpha' : [0.0001,0.001,0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,\n                       0.7,0.8,0.9,1.0,2.0,3.0,4.0,5.0,6.0,7.0,\n                      8.0,9.0,10.0,20,50,100,500,1000]}","e983d723":"# Applying lasso regression with 5 fold cross validation\n\nridge = Ridge()\nfolds = 5\nmodel_cv = GridSearchCV(estimator=ridge,\n                       param_grid=params,\n                       scoring='neg_mean_absolute_error',\n                       cv=folds,\n                       return_train_score=True,\n                       verbose=1)\nmodel_cv.fit(X_train, y_train)","85aecb62":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","5fc53061":"cv_results.shape","9ddc8ba7":"# Plotting train scores with alpha\n\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'], color='green')\nplt.xlabel('alpha')\nplt.ylabel(\"Negative Mean Absolute Error\")\n\nplt.title(\"Neg MAE and Alphas\")\nplt.show()","3e3e48de":"# Plotting testing scores with alpha\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'], color='orange')\nplt.xlabel('alpha')\nplt.ylabel(\"Negative Mean Absolute Error\")\n\nplt.title(\"Neg MAE and Alphas\")\nplt.show()","c2ee547b":"optimalvalue_ridge = model_cv.best_params_['alpha']\noptimalvalue_ridge","fc57e1aa":"alpha = optimalvalue_ridge\nridge = Ridge(alpha=alpha)\nridge.fit(X_train, y_train)","1bea2b38":"ridge.coef_","ad78b79a":"df_ridge = pd.DataFrame(index=X_train.columns)\ndf_ridge.rows = X_train.columns\ndf_ridge['Ridge'] = ridge.coef_\ndf_ridge","544ba898":"y_pred_train = ridge.predict(X_train)\ny_pred_test = ridge.predict(X_test)\n\nmetric_r = []\nr2_train_lr = r2_score(y_train, y_pred_train)\nprint(f\"Train r2 score is : {r2_train_lr}\")\nmetric_r.append(r2_train_lr)\n\nr2_test_lr = r2_score(y_test, y_pred_test)\nprint(f\"Test r2 score is : {r2_test_lr}\")\nmetric_r.append(r2_test_lr)\n\nrss1_lr = np.sum(np.square(y_train - y_pred_train))\nprint(f\"Train RSS score is : {rss1_lr}\")\nmetric_r.append(rss1_lr)\n\nrss2_lr = np.sum(np.square(y_test - y_pred_test))\nprint(f\"Test RSS score is : {rss2_lr}\")\nmetric_r.append(rss2_lr)\n\nmse_train_lr = mean_squared_error(y_train, y_pred_train)\nprint(f\"Train MSE score is : {mse_train_lr}\")\nmetric_r.append(mse_train_lr**0.5)\n\nmse_test_lr = mean_squared_error(y_test, y_pred_test)\nprint(f\"Test MSE score is : {mse_test_lr}\")\nmetric_r.append(mse_test_lr**0.5)","425d4f1a":"comparison = pd.DataFrame(index=X_train.columns)\ncomparison.rows = X_train.columns\n\ncomparison['Linear'] = lm.params\ncomparison['Ridge'] = ridge.coef_\ncomparison['Lasso'] = lasso.coef_","293b11a5":"comparison.sort_values(by='Lasso', ascending=False)","37a92f81":"# Creating a table which contain all the metrics\n\nlr_table = {'Metric': ['R2 Score (Train)','R2 Score (Test)','RSS (Train)','RSS (Test)',\n                       'MSE (Train)','MSE (Test)'], \n        'Linear Regression': metric_lr\n        }\n\nlr_metric = pd.DataFrame(lr_table ,columns = ['Metric', 'Linear Regression'] )\nrg_metric = pd.Series(metric_r, name = 'Ridge Regression')\nls_metric = pd.Series(metric_l, name = 'Lasso Regression')\n\nfinal_metric = pd.concat([lr_metric, rg_metric, ls_metric], axis = 1)\n\nfinal_metric","ade8b85b":"print(f\"The optimal value for Ridge Regression is : {optimalvalue_ridge}\")\nprint(f\"The optimal value for Lasso Regression is : {optimalvalue_lasso}\")","fa981eb8":"# Doubling Lasso and Ridge Regression's alpha values\noptimalvalue_ridge *= 2\noptimalvalue_lasso *= 2\nprint(f\"Doubled alpha values of Ridge is {optimalvalue_ridge} and Lasso is {optimalvalue_lasso}\")","2796c4e0":"alpha = optimalvalue_lasso\nlasso = Lasso(alpha=alpha)\nlasso.fit(X_train, y_train)","b158cacc":"lasso.coef_","6f579983":"df_lasso = pd.DataFrame(index=X_train.columns)\ndf_lasso.rows = X_train.columns\ndf_lasso['Lasso'] = lasso.coef_\ndf_lasso","d3d3cd33":"y_pred_train = lasso.predict(X_train)\ny_pred_test = lasso.predict(X_test)\n\nmetric_double_l = []\nr2_train_lr = r2_score(y_train, y_pred_train)\nprint(f\"Train r2 score is : {r2_train_lr}\")\nmetric_double_l.append(r2_train_lr)\n\nr2_test_lr = r2_score(y_test, y_pred_test)\nprint(f\"Test r2 score is : {r2_test_lr}\")\nmetric_double_l.append(r2_test_lr)\n\nrss1_lr = np.sum(np.square(y_train - y_pred_train))\nprint(f\"Train RSS score is : {rss1_lr}\")\nmetric_double_l.append(rss1_lr)\n\nrss2_lr = np.sum(np.square(y_test - y_pred_test))\nprint(f\"Test RSS score is : {rss2_lr}\")\nmetric_double_l.append(rss2_lr)\n\nmse_train_lr = mean_squared_error(y_train, y_pred_train)\nprint(f\"Train MSE score is : {mse_train_lr}\")\nmetric_double_l.append(mse_train_lr**0.5)\n\nmse_test_lr = mean_squared_error(y_test, y_pred_test)\nprint(f\"Test MSE score is : {mse_test_lr}\")\nmetric_double_l.append(mse_test_lr**0.5)","ba700190":"alpha = optimalvalue_ridge\nridge = Ridge(alpha=alpha)\nridge.fit(X_train, y_train)","66e01ea4":"ridge.coef_","03d32632":"df_ridge = pd.DataFrame(index=X_train.columns)\ndf_ridge.rows = X_train.columns\ndf_ridge['Ridge'] = ridge.coef_\ndf_ridge","403c7504":"y_pred_train = ridge.predict(X_train)\ny_pred_test = ridge.predict(X_test)\n\nmetric_double_r = []\nr2_train_lr = r2_score(y_train, y_pred_train)\nprint(f\"Train r2 score is : {r2_train_lr}\")\nmetric_double_r.append(r2_train_lr)\n\nr2_test_lr = r2_score(y_test, y_pred_test)\nprint(f\"Test r2 score is : {r2_test_lr}\")\nmetric_double_r.append(r2_test_lr)\n\nrss1_lr = np.sum(np.square(y_train - y_pred_train))\nprint(f\"Train RSS score is : {rss1_lr}\")\nmetric_double_r.append(rss1_lr)\n\nrss2_lr = np.sum(np.square(y_test - y_pred_test))\nprint(f\"Test RSS score is : {rss2_lr}\")\nmetric_double_r.append(rss2_lr)\n\nmse_train_lr = mean_squared_error(y_train, y_pred_train)\nprint(f\"Train MSE score is : {mse_train_lr}\")\nmetric_double_r.append(mse_train_lr**0.5)\n\nmse_test_lr = mean_squared_error(y_test, y_pred_test)\nprint(f\"Test MSE score is : {mse_test_lr}\")\nmetric_double_r.append(mse_test_lr**0.5)","cb337256":"comparison['Ridge_Double'] = ridge.coef_\ncomparison['Lasso_Double'] = lasso.coef_","e610ac79":"comparison.sort_values(by='Lasso', ascending=False)","8f2637b7":"rg_metric = pd.Series(metric_double_r, name = 'Double Ridge Regression')\nls_metric = pd.Series(metric_double_l, name = 'Double Lasso Regression')\n\nfinal_metric = pd.concat([final_metric, rg_metric, ls_metric], axis = 1)\n\nfinal_metric","cd279872":"final_metric","4dfe64b0":"comparison.sort_values(by='Lasso',ascending=False).head()","a67de776":"# Looking at the current top 5 important predictor variables in Lasso model\ncomparison.sort_values(by='Lasso',ascending=False).Lasso.head(5)","2dba70d7":"# Creating a list to hold the current top 5 important predictor variables\ntop5_names = list(comparison['Lasso'].sort_values(ascending=False).head(5).index)\ntop5_names","72af776e":"# Drop the top 5 important predictor variables from X_train and X_test\nX_train = X_train.drop(top5_names, axis=1)\nX_test = X_test.drop(top5_names, axis=1)","550a3e00":"print(X_train.shape)\nprint(X_test.shape)","f4fce02c":"# list of alphas to tune\n\nparams = { 'alpha' : [0.0001,0.001,0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,\n                       0.7,0.8,0.9,1.0,2.0,3.0,4.0,5.0,6.0,7.0,\n                      8.0,9.0,10.0,20,50,100,500,1000]}","4f899fc0":"# Applying lasso regression with 5 fold cross validation\n\nlasso = Lasso()\nfolds = 5\nmodel_cv = GridSearchCV(estimator=lasso,\n                       param_grid=params,\n                       scoring='neg_mean_absolute_error',\n                       cv=folds,\n                       return_train_score=True,\n                       verbose=1)\nmodel_cv.fit(X_train, y_train)","d8f5007d":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","59fb6d39":"cv_results.shape","c2f795e1":"# Plotting train scores with alpha\n\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'], color='green')\nplt.xlabel('alpha')\nplt.ylabel(\"Negative Mean Absolute Error\")\n\nplt.title(\"Neg MAE and Alphas\")\nplt.show()","716ccc8a":"# Plotting testing scores with alpha\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'], color='orange')\nplt.xlabel('alpha')\nplt.ylabel(\"Negative Mean Absolute Error\")\n\nplt.title(\"Neg MAE and Alphas\")\nplt.show()","91a5cc58":"optimalvalue_lasso = model_cv.best_params_['alpha']\noptimalvalue_lasso","12f226d6":"alpha = optimalvalue_lasso\nlasso = Lasso(alpha=alpha)\nlasso.fit(X_train, y_train)","ed0378e1":"lasso.coef_","52b92260":"df_lasso = pd.DataFrame(index=X_train.columns)\ndf_lasso.rows = X_train.columns\ndf_lasso['Lasso'] = lasso.coef_","edb42b9c":"df_lasso.sort_values(by='Lasso', ascending=False).head(5)","a64e2735":"As the house age increases, we can see that the median SalePrice drops but there are few cases where the SalePrice goes up as well","f21f278d":"## Merge `df` and `df_dummies` dataframes","d3aac91a":"Out of 214 features, we will go ahead and select 50 initially","7336b2c5":"### Ridge Regression Model Evaluation","397792f8":"From the pairplot, we can see few observations:\n- `TotalBasementSF`, `GrLivingArea` & `OverallQual` are linearly correlated with SalePrice\n- `TotalBasementSF` and `GrLivingArea` have a positive correlation\n- Distribution of `MasVnrArea` is skewed to the right","31e20c8a":"### Getting the optimal value of lambda","e0e3a45f":"### Checking kurtosis value of `SalePrice`","7fdd42b4":"### Lasso features and their co-efficients","fe54b86d":"## Question 1\n\nWhat is the optimal value of alpha for ridge and lasso regression? What will be the changes in the model if you choose double the value of alpha for both ridge and lasso? What will be the most important predictor variables after the change is implemented?","3f4e5ef9":"## Question 4\n\nHow can you make sure that a model is robust and generalisable? What are the implications of the same for the accuracy of the model and why?","dc4fefc8":"## Getting the optimal value of lambda","19686949":"## Handling Year columns","566ea100":"# Housing Price Prediction","0a737c48":"**Observations**\n\nAfter doubling the optimal lambda values for both Ridge and Lasso Regression, we don't see any significant changes in both `metrics` and the `features`.\nThere are few, very minor variations here and there but overall very similar.","114808a6":"## Model 2: Building model using 50 features","6ad32bdc":"## Build final Ridge Regression model","582b51b3":"### Checking skew value of `SalePrice`","f4301299":"## Building base model","0d677601":"### Build Ridge Regression model","a14427da":"Countplot to check the occurence of most frequent values in the columns","799074f0":"## Checking the distribution of the target variable `SalePrice`","dc2eb834":"## Lasso Regression Model Evaluation","e2451b0a":"## Applying MinMax scaling on numerical features (excluding dummified columns)","309d54ec":"### Build final Lasso Regression model","326427ae":"## Model 1: Building model using RFE","013231c2":"# Comparison of co-efficients after Regularization","100441dd":"Houses with Central Air conditioning have a higher median price compared to the houses that don't have Central Air conditioning","cff037c7":"## Checking size of the dataset","0046e624":"# Data Preparation","1a534747":"As Basement quality increases from Fair to Excellent, we see a corresponding increase in SalePrice","fe1e3678":"## Question 2\n\nYou have determined the optimal value of lambda for ridge and lasso regression during the assignment. Now, which one will you choose to apply and why?","9c8b8ab5":"## Split data into train and test sets","781c857a":"As Exterior quality increases from Fair to Excellent, we see a corresponding increase in SalePrice","7f9ac3a7":"**Note**\n- kurtosis measures the tail-heaviness of the distribution\n- For a normal distribution, kurtosis value is 3\n- As kurtosis value increase, the tail heaviness also increases and vice-versa","3b0a51b8":"From this list, we will be picking the top 10 most correlated variables with SalePrice","43401fe6":"As we can see the train score is good, but test score is really underwhelming. This is the same behavior for Mean Squared Error as well.\nThis means that the model is overfitting.\n\n**Approach** <br>\nAs the columns are high, we will use RFE (Recursive Feature Elimination) approach to select 50 columns first and remove columns with high VIF","e71d8be7":"Houses that are partially completed have a higher median Saleprice compared to other categories. This might be because partially completed houses are usually new houses under construction.","b0a594ee":"## Checking the summary statistics of the dataset","a15fcfb5":"The rule of thumb for checking skewness:\n\n- If the skewness is between -0.5 and 0.5, the data are fairly symmetrical\n- If the skewness is between -1 and \u2013 0.5 or between 0.5 and 1, the data are moderately skewed\n- If the skewness is less than -1 or greater than 1, the data are highly skewed","9de5d881":"Now the data follows more or less a `normal distribution`","904b002d":"**Observations**\n- There are 32 significant features in the model\n- The features are sorted in the order of decreasing significance of the co-efficients (i.e. The most significant feature is at the top, followed by the next significant one and so on...)\n- The co-efficients are very close for Ridge and Lasso Regression models post regularization with slight variance to the model created by Linear Regression\n- However, the resultant selection of significant feature order still remains the same","c24b1483":"**Note**\n- Looking at the distribution, we can see that the distribution is skewed towards the right (i.e. outliers on data with high Sales Price). This indicates the presence of outliers.\n- In such cases, we cannot cap the values as Linear Regression models are interpolated and not extrapolated (i.e. Model will make wrong predictions in case of high Sales Prices).","e2db9db7":"We have 4 year columns. With this, we will create Age columns that track the time in year and remove these columns.\n<br> Age will be calculated by subtracting the Year from the current Year for each column.\n- YearBuilt\n- YearRemodAdd\n- GarageYrBlt\n- YrSold","cd9f057a":"**Answer**\n\nA robust model has low variance. This means that an unprecendented change in one or more features does not significantly alter the value of the predicted variable.\nSimilarly, a generalizable model has reduced model complexity. As the number of features increase in the model, it becomes more complex which usually leads to low bias but high variance.\nA generalizable model has just enough features that it has as much low variance as possible.\n\nThis can be observed from the Bias-Variance tradeoff visual shown below.\n\nThe OLS (Ordinary least squares) regression model is very sensitive to outliers and they induce high variance. To reduce this, we can go ahead with regularization (Ridge\/Lasso) which include a penalty term in the cost function of the model.\nThis penalty term will move the coefficents of the model towards 0 and thus it reduces model complexity (as feature addition is heavily discouraged). This reduces overfitting in the model.\n\nSo regularization gets us high variance with a small trade-off in bias. Thus it helps us build a model which is robust and generalizable.\nA robust and generalizable model will have a good, consistent train as well as test accuracy.","33150787":"### Looking at the top 5 important predictor variables in Lasso model","69e9d8d4":"## Question 3\n\nAfter building the model, you realised that the five most important predictor variables in the lasso model are not available in the incoming data. <br>\nYou will now have to create another model excluding the five most important predictor variables. Which are the five most important predictor variables now?","2b71cac3":"Based on this, we can see that using regular Linear Regression:\n- Residuals are centered around 0 and normally distributed which satisfies linear regression assumptions\n- Both train and test predictions are linear in nature compared to the actual values\n- Train r2 score is : 0.8537384198703117\n- Test r2 score is : 0.8348252098171571 \n\nWe will move ahead with `Lasso` and `Ridge` Regression to perform regulaization which helps models perform well with unseen data while identifying necessary underlying patterns in it.\n- In both Ridge and Lasso regression, which both allow some bias to get a significant decrease in variance, thereby pushing the model coefficients towards 0. \n- In Lasso, some of these coefficients become 0, thus resulting in model selection and, hence, easier interpretation, particularly when the number of coefficients is very large.","7038e881":"Getting features that have VIF over 5","bc8e1830":"## Getting the optimal value of lambda","edfb6a5f":"### Getting the new top 5 important predictor variables via Lasso Regression","44bb691c":"### Creating a correlation matrix and heatmap","18ba451a":"## Checking first few rows","57c03318":"### Ridge features and their co-efficients","0aa16d4d":"We can see that:\n- GrLivArea and TotaRmsAvbGrad are highly correlated at 0.83\n- TotalBsmtSF and 1stFlrSF are highly correlated at 0.82\n- GarageArea and GarageCars are highly correlated at 0.88\n- OverallQual and SalePrice are highly correlated at o.79","f8749da0":"#### Checking the data in the columns having NULL values < 15%","1335bdae":"**Objectives**\n\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia.\nThe company is looking at prospective properties to buy to enter the market. We are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\n\nThe company wants to know the following things about the prospective properties:\n1. Which variables are significant in predicting the price of a house, and\n2. How well those variables describe the price of a house.\n\nAlso, we need to determine the optimal value of lambda for ridge and lasso regression.\n\n**Business Goal**\n\nWe are required to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market.","e97d76b1":"As skewness is more than 1, in our case, we conclude that the column `SalePrice` is highly skewed","00fc48ba":"### Comparison of metrics after Regularization","0bafff80":"To handle this, we will perform `Log Transformation` on \"SalePrice\" column. This will transform the variable and make it as normally distributed as possible. Basically it reduces the skewness in the data.","e313508b":"We can see 19 columns having NULL values in the order of most NULLs to least NULLs","d3121625":"We will now proceed with dropping all Year columns","f9e69e29":"#### For these columns, we will perform imputation using the below approach:\n- If the column is `categorical`, we will use mode() to replace the missing values\n- If the column is `numerical`, we will use median() to replace the missing values\n- If the column value `NA` has a meaningful value (For e.g. `GarageType` = NA refers to \"No Garage\"). We will replace these values with `None`","bcdb0045":"# Importing data","11987221":"## Dropping the original categorical columns from the dataframe","3494bc3e":"After manual inspection of each column, we find that all columns are mapped to the right data types","c05e49f6":"**Observations**\n\nBelow are the reasons why I will prefer Lasso over Ridge Regression:\n- The values of R2 Score, RSS and MSE for Lasso Regression are slightly better than Ridge Regression in this particular model\n- In Lasso Regression, we can push the model co-efficients to actual zero value. This means that the features that have co-efficent value of 0 can be removed from the model. This results in feature selection\n- Model complexity also reduces because we can remove features with zero co-efficients","51f8535a":"---","eeeb15a4":"### Comparison of co-efficients after Regularization","6cfd1c7c":"## Create OHE (One-Hot Encoding) for categorical columns","4311abd9":"## Exploratory Data Analysis","e57a7947":"`Stone` Masonry Veneer Type has higher median Sales Price compared to other Veneer Types","0b48e385":"Running the NULL value check again to see if there are any null values left over","abe830e1":"## Check the data type of all the columns","fb27e2e7":"## Checking columns which have NULL values","bc34bd17":"# Modeling using Linear Regression","aa22fea0":"We will be dropping the columns based on:\n1. If the variance in the column is very less, then it provides little scope for the model to learn. We will be dropping these columns\n2. Few columns are not important such as `Id` as they don't provide any meaningful insights. We will remove these as well","766b973a":"## Final Model\nWe will be building the Final model with 32 features","de8b128f":"**Observations**\n- The R2 Score, RSS and MSE are all very close for Linear Regression, Ridge and Lasso\n- Lasso has better scores by a very slight margin compared to Ridge and Linear Regression","0ed2f9af":"### Lasso features and their co-efficients","f97425ec":"In our case as the `kurtosis` value is more than ~6.5, distribution tail is heavier","aefe7369":"### Residual Analysis of Train","383400d4":"## Divide data into `X` and `y` for building the model","000dad1e":"Checking the shape of the dataset after dropping above columns","77dd8741":"### Lasso Regression Model Evaluation","7947b711":"## Build final Lasso Regression model","cb18fb59":"## Dropping unimportant columns","ad1a55c0":"# Lasso Regression","45d02a95":"### Build Lasso Regression model","f911b553":"### Residual Analysis of Test","51746d1b":"## Checking the data types and nullability of columns","fecf7133":"### Linear Regression features and coefficients","8a05b36b":"## Linear Regression Final Model Evaluation","163aaf4b":"# Data Understanding, Exploration & Cleaning","a1a9d50d":"# Comparison of metrics after Regularization","ce3727d0":"# Ridge Regression","3e4138b9":"### Handling `SalePrice` high skewness and kurtosis","0aa0c5de":"## Lasso features and their co-efficients","c882bdba":"## Deleting all columns with high VIF","a0a3b1d7":"### Dropping NULLs for columns above 15%","cb76ffc9":"## Ridge features and their co-efficients","69eab8d1":"# Subjective Questions","1f56f8ae":"## Ridge Regression Model Evaluation","ba634204":"# Importing required libraries"}}