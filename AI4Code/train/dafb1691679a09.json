{"cell_type":{"3fdf1122":"code","e38c61ee":"code","d0bf9aa1":"code","c8804687":"code","401b1e7e":"code","c20e7530":"code","c2ea6236":"code","a97f3899":"markdown","f5d07746":"markdown"},"source":{"3fdf1122":"import gc\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold","e38c61ee":"# Original DB\ntrain_df_orig = pd.read_csv(\"..\/input\/train.csv\")\ntest_df_orig = pd.read_csv(\"..\/input\/test.csv\")\ntarget = train_df_orig['target']\norig_features = [c for c in train_df_orig.columns if c not in ['ID_code', 'target']]\n\n# DF TOTAL \ndf_total = pd.concat([train_df_orig[orig_features], test_df_orig[orig_features]], axis=0)\ndf_total = df_total.reset_index(drop=True)\n\n# DF sorted by 'var_68'\ndf_total['target'] = np.concatenate([train_df_orig.target.values,np.full(len(test_df_orig),np.nan)])\nsorted_total_df = df_total.sort_values('var_68', ascending=False).copy()\nsorted_total_df.head()","d0bf9aa1":"size_fil = 8\nsize_fil_med = int(size_fil\/2)\ntarget_s_filter = sorted_total_df.target.reset_index(drop=True).values\ntarget_s_filter_end = target_s_filter.copy()\nfor i in np.arange(size_fil_med,len(target_s_filter)-size_fil_med):\n    vec = np.concatenate([target_s_filter[(i-size_fil_med):i],target_s_filter[(i+1):(i+1+size_fil_med)]])\n    vec = vec[~np.isnan(vec)]\n    if len(vec)==0:\n        target_s_filter_end[i] = 0.0\n    else:\n        target_s_filter_end[i] = np.mean(vec)\ntarget_s_filter_end[np.isnan(target_s_filter_end)] = 0.0","c8804687":"sns.set(rc={'figure.figsize':(24,12)})\nline=sns.lineplot(data=pd.DataFrame({'MAF of Target':target_s_filter_end[16000:16400],\n                                     'Target':np.array(target_s_filter)[16000:16400]}))","401b1e7e":"# Include new feature\nsorted_total_df['target_filter'] = target_s_filter_end\ntotal_df_orig_reorder = sorted_total_df.sort_index()\ntrain_df = total_df_orig_reorder.iloc[:len(train_df_orig)]\ntest_df = total_df_orig_reorder.iloc[len(train_df_orig):]\ntrain_df.head()","c20e7530":"# Remove var_68\nfeatures = [c for c in train_df.columns if c not in ['target', 'var_68']]\n\nparam = {\n    'bagging_freq': 5,          \n    'bagging_fraction': 0.30,   \n    'boost_from_average':'false',   \n    'boost': 'gbdt',\n    'feature_fraction': 0.03368,   \n    'learning_rate': 0.01,      \n    'max_depth': -1,                \n    'metric':'auc',\n    'min_data_in_leaf': 80,     \n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 4,           \n    'tree_learner': 'serial',   \n    'objective': 'binary',      \n    'verbosity': 1\n}\n\nnfolds = 5\n\nfolds = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=31415)\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nAUC_l = []\niter_l = []\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 500000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, \n                    early_stopping_rounds = 250)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n    AUC_l.append(roc_auc_score(target[val_idx], oof[val_idx]))\n    iter_l.append(clf.best_iteration)\n    predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"Width Filter {} CV score:{:<8.5f} CV_stats:[{:<8.5f}, {:<8.5f} ({:<8.5f}), {:<8.5f}]\".format(size_fil,roc_auc_score(target, oof),np.min(AUC_l),np.mean(AUC_l), np.std(AUC_l), np.max(AUC_l)))\nscore = roc_auc_score(target, oof)\nbest_iter = np.mean(iter_l)\nbest_score_final = score","c2ea6236":"# Submission\nsub_df = pd.DataFrame({\"ID_code\":test_df_orig[\"ID_code\"].values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv('submission.csv.gz', index=False, compression='gzip')\nprint(pd.read_csv('submission.csv.gz').head())","a97f3899":"Applying a Moving Window Average Filter (MWAF) to 'target' orderec by 'var_68' with a window's width=8 (best CV).  I used different window's width (between 2 and 50).\n\nnp.mean(target[i-4, i-3, i-2, i-1, i+1, i+2, i+3, i+4])\n\nThe target at position 'i' is excluded to avoid overfitting.","f5d07746":"Hypothesis: if 'var_68' is a 'DateTime' feature related to 'target', some specific days-hours could have a high density of transactions.\n\nThe idea is to order the dataset (train+test) by 'var_68' and create a new feature by applying to 'target' a Moving Window Average (MWA) for each position. (Note: the original value of 'target' in each position is excluded in the average in order to avoid overfitting).\n\nI used similar idea in previous competitions with good results (Bosch, Rossmann, ...) but, however, this new feature did not improve my CV.\n\nThis notebook is a simple example for experimenting. For example: MWA can be applied to other features and with other functions...\n\nIt is this feature really useful? \nIs there someone trying something similar? \nSome combination or approximation?"}}