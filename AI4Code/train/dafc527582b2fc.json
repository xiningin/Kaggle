{"cell_type":{"ee8be721":"code","d670756b":"code","6ad3b39f":"code","9bc9f21d":"code","9c2a93b6":"code","69d72965":"code","2b436e11":"code","30960b9e":"code","a434b759":"code","f4ceae6c":"code","5669bbf4":"code","448fe3d0":"code","98b3e8a5":"markdown","f9a9cef2":"markdown","87c55182":"markdown","6a0b6bec":"markdown","e262dedd":"markdown","26ba4bbf":"markdown","d03671b0":"markdown","f903d65c":"markdown","c2c0b265":"markdown","3e653de2":"markdown","354933e8":"markdown","0795b6fc":"markdown"},"source":{"ee8be721":"import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# setting seeds for fixed pseudo-random numbers in repeated runs\nnp.random.seed(7)\ntf.random.set_seed(7)","d670756b":"# Content layers\ncontent_layers = ['block5_conv2'] \n\n# Style layers\nstyle_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n\nnum_content_layers = len(content_layers)\nnum_style_layers = len(style_layers)","6ad3b39f":"def get_model():\n  # Load our model. We load pretrained VGG, trained on imagenet data (weights=\u2019imagenet\u2019)\n    vgg19 = keras.applications.vgg19.VGG19(include_top = False, weights = 'imagenet')\n    vgg19.trainable = False\n    \n  # Get output layers corresponding to style and content layers \n    content_outputs = [vgg19.get_layer(name).output for name in content_layers]\n    style_outputs = [vgg19.get_layer(name).output for name in style_layers]\n    \n  # Build model \n    return keras.models.Model(inputs = [vgg19.input], outputs = [content_outputs, style_outputs])","9bc9f21d":"def preprocess_image(image):\n    '''\n    Takes an input of any size and resizes it to 500 x 500 along with applying the preprocessing erquired by vgg19\n    '''\n    image = keras.preprocessing.image.img_to_array(image)\n    image = tf.image.resize(image, [500, 500])\n    image = tf.expand_dims(image, axis=0)\n    image = keras.applications.vgg19.preprocess_input(image)\n    return image","9c2a93b6":"def deprocess_image(processed_image):\n    '''\n    Takes the output_image from vgg19 and removes the processing applied by vgg19.preprocess_input\n    '''\n    x = processed_image.numpy().copy()\n    x = np.squeeze(x, 0)\n    \n    # perform the inverse of the preprocessing step\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    x = x[:, :, ::-1]  # Convert back to RGB from BGR\n\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x","69d72965":"trial_image = plt.imread(\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/d\/d7\/Green_Sea_Turtle_grazing_seagrass.jpg\/1200px-Green_Sea_Turtle_grazing_seagrass.jpg\", format='jpg')\nplt.figure(figsize=(18,6))\n\nplt.subplot(1,3,1)\nplt.imshow(trial_image)\nplt.axis('off')\n\nplt.subplot(1,3,2)\n# preprocessed image is of shape (1, 500, 500, 3)\n# preprocess_image(trial_image)[0] removes the first axis\nplt.imshow(preprocess_image(trial_image)[0])\nplt.axis('off')\n\nplt.subplot(1,3,3)\nplt.imshow(deprocess_image(preprocess_image(trial_image)))\nplt.axis('off')\n\nplt.show()","2b436e11":"def content_loss_func(all_content_maps, all_noise_content_maps):\n    '''\n    Takes in the feature-maps from content-layers; iterates over each of them; and at each iteration calculates the Euclidean Distnce\n    \n    all_content_maps: output of the content_layers when content_image is passed through the model\n    \n    all_noise_content_maps: output of the content_layers when noise is passed through the model\n    \n    returns content_loss\n    '''\n    \n    content_loss = 0.\n    content_weight = 1\/num_content_layers\n    \n#     use for loop just in case we try more than one content layer\n#     we use [0] becuase without that, content_maps is of dtype list, which is unsupported\n    for content_maps, noise_content_maps in zip(all_content_maps, all_noise_content_maps):\n        content_loss += content_weight * tf.reduce_mean(tf.square(content_maps[0] - noise_content_maps[0]))\n        \n    return content_loss","30960b9e":"def gram_matrix(maps):\n    '''\n    Takes in feature-maps of a particular layer and computes the gram-matrix for that set of fmaps.\n    \n    '''\n    channels = tf.shape(maps)[-1]\n    maps = tf.reshape(maps, [-1, channels])\n    gram_matrix = tf.matmul(a = maps, b = maps, transpose_a = True)\n    return gram_matrix\/tf.cast(maps.shape[0], tf.float32)\n\ndef style_loss_func(all_style_maps, all_noise_style_maps):\n    '''\n    Takes in the feature-maps from style-layers; and iterates over each of them.\n    At each iteration it grabs the fmaps of that layer; computes the gram-matrices for the style_image fmaps and the noise fmaps;\n    and computes the Euclidean Distance as the style loss.\n    \n    all_style_maps: output of the style_layers when content_image is passed through the model\n    \n    all_noise_content_maps: output of the style_layers when noise is passed through the model\n    \n    returns style_loss\n    '''\n    style_loss = 0.\n    style_weight = 1\/num_style_layers\n    \n    for style_maps, noise_style_maps in zip(all_style_maps, all_noise_style_maps):\n        style_gram = gram_matrix(style_maps)\n        noise_gram = gram_matrix(noise_style_maps)\n        style_loss += style_weight * tf.reduce_mean(tf.square(style_gram - noise_gram))\n    return style_loss","a434b759":"def get_gradients(model, all_content_maps, all_style_maps, noise):\n    '''\n    Computes the gradients for style transfer.\n    \n    model: The neural network which computes the feature-maps\n    \n    all_content_maps: pre-computed feature-maps outputed by content_layers of the model\n    \n    all_style_maps: pre-computed feature-maps outputed by style_layers of the model\n    \n    returns gradients, total_loss, content_loss, style_loss\n    '''\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(noise)\n        \n        alpha, beta = 1e4, 1e-3\n    \n        noise_maps = model(noise)\n        all_noise_content_maps = noise_maps[0]\n        all_noise_style_maps = noise_maps[1]\n\n        content_loss = content_loss_func(all_content_maps, all_noise_content_maps)\n        style_loss = style_loss_func(all_style_maps, all_noise_style_maps)\n        \n        total_loss = alpha*content_loss + beta*style_loss\n        \n    return tape.gradient(total_loss, noise), total_loss, content_loss, style_loss","f4ceae6c":"def style_transfer(content_image, style_image, epochs = 2000):\n    '''\n    Inputs: content_image, style_image, #epochs\n    \n    Outputs: A new image with the content of the content_image and the style of the style_image\n    '''\n    \n    model = get_model()\n    for layer in model.layers:\n        layer.trainable = False\n    \n    noise = tf.Variable(preprocess_image(tf.random.uniform(shape=content_image.shape)))\n    content_image = preprocess_image(content_image)\n    style_image = preprocess_image(style_image)\n    \n    plt.figure(figsize=(18,6))\n    for i, image in enumerate([content_image, style_image, noise]):\n        plt.subplot(1,3, i+1)\n        plt.imshow(deprocess_image(image))\n        plt.axis('off')\n    plt.show()\n    \n    content_output = model(content_image)\n    all_content_maps = content_output[0]\n    \n    style_output = model(style_image)\n    all_style_maps = style_output[1]\n    \n    optimizer = keras.optimizers.Adam(lr=10.0)\n    \n    best_loss, best_image = float('inf'), None\n    \n    norm_means = np.array([103.939, 116.779, 123.68])\n    min_vals = -norm_means\n    max_vals = 255 - norm_means   \n    \n    image_list = []\n    \n    for epoch in range(epochs):\n        gradients, total_loss, content_loss, style_loss = get_gradients(model, all_content_maps, all_style_maps, noise)\n        optimizer.apply_gradients([(gradients, noise)])\n        clipped = tf.clip_by_value(noise, min_vals, max_vals)\n        noise.assign(clipped)\n        \n        if total_loss < best_loss:\n            best_loss = total_loss\n            best_image = deprocess_image(noise)\n            image_list.append(best_image)\n            \n        if epoch % 100 == 0:\n            print(\"\\nEpoch {}\/{}\".format(epoch, epochs))\n            print('Total_loss', total_loss.numpy())\n            print('Content_loss:', content_loss.numpy())\n            print('Style_loss:', style_loss.numpy())\n        \n    plt.figure(figsize=(10,10))\n    plt.imshow(best_image)\n    plt.axis('off')\n    plt.show()\n            \n    return (best_loss, best_image, image_list)","5669bbf4":"# make sure to set the correct format\ncontent_image = plt.imread(\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/d\/d7\/Green_Sea_Turtle_grazing_seagrass.jpg\/1200px-Green_Sea_Turtle_grazing_seagrass.jpg\", format='jpg')\nstyle_image = plt.imread(\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/a\/a5\/Tsunami_by_hokusai_19th_century.jpg\", format='jpg')","448fe3d0":"loss, image, image_list = style_transfer(content_image, style_image)\n\n#if you are unable to scroll the output, collapse the output and then expand it again.","98b3e8a5":"# Run","f9a9cef2":"# Content Loss\n\nCalculating the content loss is simple: Alter the noise image so as to reduce the Euclidean distance between the feature-maps generated by the noise and the content image.\n\nThis is how we do that:\nAssuming we are using multiple layers for calculating the content loss\n* Iterate over the layers.\n\n* Say we use 4 layers, then we iterate over them and in the first run grab the 1st layer which we are using for content loss.\n* Calculate the Euclidean distance between the feature maps.\n* Add that to the net content loss.\n* Grab the second layer which we are using for calculating the content loss.\n* Calculate the Euclidean distance between its feature maps.\n* Add that to the net content loss.\n\nand so on till all layers are used up. Note that we divide the loss at each iteration by the number of layers we use.","87c55182":"# Image Processing Functions\n\n* Neural nets expect a constant input-size\n* The `deprocess_image` function is needed as `vgg19.preprocess_input` method normalises the input and changes rgb to bgr. So we have to de-process to get the output back into rgb form.","6a0b6bec":"# Importing Base Model\n\nBefore you proceed, I highly recommend you read the [Style Transfer](https:\/\/arxiv.org\/pdf\/1508.06576.pdf) research paper or watch [Andrew Ng's explanation](https:\/\/www.youtube.com\/watch?v=R39tWYYKNcI&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=38) of it on youtube. If you did both then Kudos to you :)\n\n* In accordance to the reserach paper, I'll import the `VGG19 model` provided by keras which is pretrained on ImageNet dataset.\n\n* Note that we have to `drop the upper fully connected layers` as we are not interested in predictions. We are after the feature maps residing in the convolution layers.\n\n* Also, `freeze the weights` as we don't want the neural net to learn anything. We are just altering the noisy input in accordance to the content and style images.\n\n* In a CNN, the `earlier Conv-layers` detect low level features like vertical and horizontal edges and Conv-layers at the end of the Neural Network detect high level features like faces, or presence of a particular object or features.\n\n* Therefore, we use the `end layer\/layers` for content loss and the earlier layers for style.","e262dedd":"```\nNow watch carefully:\n\narray([[[ 0,  6],\n        [ 3,  9]],\n\n       [[ 1,  7],\n        [ 4, 10]],\n\n       [[ 2,  8],\n        [ 5, 11]]])\n        \nIf I ask you to find the correlation between channel-1 (the 0,3,6,9 one) and channel-2.\nYou would do the following: \n\n    [[0 x 1,  6 x 7],    =   [[0,  42],\n     [3 x 4,  9 x 10]]       [12, 90]]\n     \n     sum = 0 + 42 + 12 + 90 = 144\n     \n     \nThis is exactly what is happening here when row1 multiplies col2:\n\narray([[ 0,  3,  6,  9],      array([[0,  1,  2],         array([[126, 144, 162],\n       [ 1,  4,  7, 10],   x         [3,  4,  5],    =           [144, 166, 188],\n       [ 2,  5,  8, 11]])            [6,  7,  8],                [162, 188, 214]])\n                                     [9, 10, 11]])\n```","26ba4bbf":"# Loss Functions\n* I use the term `all_` when I'm using data from many layers.\n\n* For example `all_feature_maps` is set of feature-maps from layers 1,2,3 and so on. It will have a group of fmaps from layer-1, another group of fmaps from layer-2.\n\n* Writing `feature_maps` means the fmaps are of particular layer alone.\n\n**Example:** `for content_maps, noise_content_maps in zip(all_content_maps, all_noise_content_maps):`\n\nHere I iterate over the fmaps from different layers and at a particular iteration grab the fmaps of that particular layer alone.","d03671b0":"# Final Function","f903d65c":"# Style Loss\n\n* Style loss is reduced by reducing the difference between the Gram-Matrices of feature maps.\n\n* Note that (and this is very important) the output of a Conv-layer is a bunch of feature-maps. What we do is that we find the `correlation` between these feature-maps and store that as the `gram-matrix`.\n\n* Say a Conv-Layer outputs 7 feature-maps. Then we can imagine the gram-matrix as a table in which the x-axis is labelled: fmaps 0,1,2,3,4,5,6 and y-axis is also labelled fmaps 0,1,2,3,4,5,6. The entries in the table will be the correlation between those feature maps i.e entry 1,3 in the table will be correlation between fmap 1 and fmap 3. (Of course, the gram-matrix will be symmetric with mirrored data acoss the diagonal).\n\n* `Correlation` is calculated by element-wise multiplying matrices and summing the values of the result.\n\n* The `calculation of the gram-matrix` is tricky and very hard to understand by reading the code and it is further complicated by the image notation being in channels last notation used in libraries, but I've tried my best to explain it down below with an example.\n\n* The Style_loss function iterates across the layers.\n\n* At a particular, it calcualtes the gra matrix using the fmaps of the style_image and a seperate gram_matrix using the fmaps of the noise.\n\n* Then it computes the `euclidean distance between the gram-matrices`.\n\n* Same is done for each layer and the loss is collected in a net_loss.","c2c0b265":"**An example to show how the Gram-matrix is being calculated in the above cell.**\n\n```\narr = np.arange(12).reshape(2, 2,3)\narr = tf.constant(arr)\n\nThis is how you must have imagined it with. Three (Two by Two matrices), right?\n\narray([[[ 0,  1],\n        [ 2,  3]],\n\n       [[ 4,  5],\n        [ 6,  7]],\n\n       [[ 8,  9],\n        [10, 11]]])\n\n\nBut below is how it actually looks:\n\n<tf.Tensor: shape=(2, 2, 3), dtype=int64, numpy=\narray([[[ 0,  1,  2],\n        [ 3,  4,  5]],\n\n       [[ 6,  7,  8],\n        [ 9, 10, 11]]])>\n\nNote that these are two (two by three) matrices and not three (two by two) matrices which is the actual concept of an rgb image.\nSurprised? Angry? Ya me too. But it is what is is.\n\nMatrix notation works like this: (5 x 4 x 3 x 2) means you have five (4 x 3 x 2) matrices each of which a group of four (3 x 2) matrices.\n\nIf we look at it with the earlier perspective, in our head we can view the data being distributed as follows:\n\narray([[[ 0,  6],\n        [ 3,  9]],\n\n       [[ 1,  7],\n        [ 4, 10]],\n\n       [[ 2,  8],\n        [ 5, 11]]])\n```","3e653de2":"# Neural Style Transfer using Tensorflow : Detailed Implementation\n![Styled Image](https:\/\/github.com\/Cossak\/Neural-Style-Transfer-using-Tensorflow-Detailed-Implementation\/blob\/main\/output%20image.png?raw=true)","354933e8":"# Gradient Function","0795b6fc":"```\narr = tf.reshape(arr, [-1, 3])\n<tf.Tensor: shape=(4, 3), dtype=int64, numpy=\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11]])>\n\nThis reshaping step makes no sense for the time being but you will see the subtlety in a minute.\n\n\ntf.matmul(a = arr, b = arr, transpose_a=True) which gives:\n\n<tf.Tensor: shape=(3, 3), dtype=int64, numpy=\narray([[126, 144, 162],\n       [144, 166, 188],\n       [162, 188, 214]])>\n       \nUnder the hood what it did is the following:\n\narray([[ 0,  3,  6,  9],    array([[0,  1,  2],\n       [ 1,  4,  7, 10],           [3,  4,  5],\n       [ 2,  5,  8, 11]])          [6,  7,  8],\n                                   [9, 10, 11]])\n```"}}