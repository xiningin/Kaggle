{"cell_type":{"e4e42957":"code","88a35d9b":"code","86c7fb93":"code","740fd8cf":"code","3cff6a41":"code","2b42e0be":"code","508c1f79":"code","0b5c70af":"code","5b86c4d0":"code","2fb1b053":"code","94cef86f":"code","8f50c6ed":"code","ac732e5e":"code","6fd4606c":"code","2f851c72":"code","3e42d123":"code","82b2f3f5":"code","e659d119":"code","abce6f9c":"code","4a69cedd":"code","9128e005":"code","2305cf2c":"code","4e18ae83":"code","8ad97332":"code","75753439":"code","5ac1dec2":"code","599c1b73":"code","c1338646":"code","8621ad9c":"code","ccc03fe1":"code","8e67ca93":"code","142fe160":"code","539b4306":"code","c20dba9a":"code","851cef92":"code","7dba5635":"code","f3d3163b":"code","4f9f13af":"code","e8161963":"code","b4224ab7":"code","fcdac2c3":"code","6af8991b":"code","318cc5dd":"code","40a036b3":"code","5d3262e5":"code","5874fd93":"code","3e3e6095":"code","0e74131f":"code","c0b3a36c":"code","d68f82f9":"code","6b4b4fdf":"code","b47461aa":"code","d99becb6":"code","637aefe1":"code","6aaf6e8f":"code","b3b24015":"code","aebfa724":"code","1b4ed865":"code","e4aa77ea":"code","f5a8a8b7":"code","891fb3a8":"code","35d71b16":"code","a3a1a6b9":"code","64584be9":"code","0e960422":"code","a5e618c0":"code","1795cc69":"code","03c2af22":"code","b55920e2":"code","efa3d4ca":"code","d68bdb89":"code","13510343":"code","3607284d":"code","890c5201":"code","3f49cb62":"code","e8e8e2dd":"code","f836689d":"code","cdee2efb":"code","0f8e6506":"code","3172bcb5":"code","96914746":"code","983a3495":"code","a276340c":"code","93e0a948":"code","4a50ce58":"code","8913ea4f":"code","4ea54816":"code","2df28aa7":"code","71c2db62":"code","d43d3292":"code","63688e11":"code","5fdd2e12":"code","017dac9e":"code","444aa359":"code","56f3e21a":"code","935b21bf":"code","eeb71bf3":"code","d3b4ff50":"code","58246a35":"code","bf9c8ae1":"code","7393625c":"code","06d0dc99":"code","dfbae35c":"code","344c9445":"code","12f07e0e":"code","79fa0b04":"code","a3a57f89":"code","bb8486e0":"code","dce13b49":"code","633f89a7":"code","c5ad7b2d":"code","604dc6ef":"code","da700571":"code","f3133099":"code","4a301c5e":"code","c5eaba85":"code","5b4f3448":"code","3c2b0d69":"code","9798b8b5":"code","949ab9d0":"code","8797f6c8":"code","ec003b1a":"code","0a1860f5":"code","fd4f93a4":"code","4b9470e9":"code","5229479d":"code","3b39c160":"code","11e58700":"code","dbce4e47":"code","ff62e8f8":"code","1889e8b5":"code","eee83af8":"code","73aa7a20":"code","ba5995bc":"markdown","b146fbf8":"markdown","4ab2f948":"markdown","9671e1ad":"markdown","c58ea499":"markdown","58052beb":"markdown","ca53ee24":"markdown","fa1c967f":"markdown","4ffa6669":"markdown","47468923":"markdown","d15a37b9":"markdown","e311e686":"markdown","d00f678e":"markdown","f17593f3":"markdown","a1281ebb":"markdown","28ea3ae3":"markdown","6ed38c6f":"markdown","4fe0889e":"markdown","041253a1":"markdown","adc2c86a":"markdown","ba107a0d":"markdown","92fe5d9d":"markdown","69173fa6":"markdown","5ebd3727":"markdown","10d8bce5":"markdown","afc4aaca":"markdown","a6f7be3c":"markdown","1fb6a4b1":"markdown","122d5311":"markdown","c5532e2f":"markdown","a8a183e8":"markdown","f32f1bec":"markdown","899807b8":"markdown","c38eef38":"markdown","d7fdf170":"markdown","0c184648":"markdown","522cadbc":"markdown","e8f1ec58":"markdown","a6443ce1":"markdown","8563588c":"markdown","b80c120b":"markdown","3492c1f5":"markdown","6b449058":"markdown","b314f83e":"markdown","7eba2d1d":"markdown","b7634759":"markdown","61ee45ca":"markdown","77e6c16b":"markdown","22ac03f2":"markdown","5137f091":"markdown","51464504":"markdown","c9c3bea7":"markdown","7dfecce5":"markdown","fc0b6838":"markdown","6d8ce310":"markdown","1af85c8d":"markdown","637fcbf1":"markdown","52bd2646":"markdown","e9d4e4ef":"markdown","24f4bb0a":"markdown","12186953":"markdown","6da1d2d5":"markdown","93e44f99":"markdown","687b96e1":"markdown","9c21c4bf":"markdown","ab63565f":"markdown","59ae6ace":"markdown","140a95e2":"markdown","983607c3":"markdown","ef01a36e":"markdown","10763bfc":"markdown","de0880ae":"markdown","bef52467":"markdown","078717f1":"markdown","3dd387a9":"markdown","7947a954":"markdown","2aff507a":"markdown","3f0a4d23":"markdown","11332cbc":"markdown","71b9ad1f":"markdown","c0e73880":"markdown","5628b1bf":"markdown","d82703da":"markdown","ddc529c9":"markdown","baab1437":"markdown","8b8180ce":"markdown","6e5443bf":"markdown","cd34d241":"markdown","d81bf56b":"markdown","e9967e1e":"markdown","eab35287":"markdown","9bdc6fcf":"markdown","1b130074":"markdown","0fd3b449":"markdown","6cd94bd1":"markdown","34dfdf39":"markdown","da7fc1c4":"markdown","78a508f2":"markdown","a77c7b33":"markdown","3840f390":"markdown","c84505c4":"markdown","d4e9fb65":"markdown","e6c57dc3":"markdown","7a8b8c19":"markdown","59cfedfb":"markdown","ca3c1a12":"markdown","d7f1566e":"markdown","4a7f504f":"markdown","1ee3b0ac":"markdown","1ea9eac1":"markdown","29880365":"markdown","a2cd1169":"markdown","e4fbf5cc":"markdown","4e3a7c04":"markdown","0f3033dd":"markdown","94c25414":"markdown","0f4833bc":"markdown","43fffb1f":"markdown","66c14d86":"markdown","ea6af66d":"markdown","5770e89d":"markdown","68c56d64":"markdown","6b37e662":"markdown","c47b15a1":"markdown","1663d8ea":"markdown","2878caba":"markdown","3b1c0743":"markdown","a2b690a6":"markdown","dc97efff":"markdown","352fb8d4":"markdown","43b52e58":"markdown","13da4c90":"markdown"},"source":{"e4e42957":"#import some necessary libraries\nimport datetime\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\nimport time\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew, kurtosis, boxcox #for some statistics\nfrom scipy.special import boxcox1p, inv_boxcox\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\n#print(check_output([\"ls\", \"-rlt\", \"..\/StackedRegression\"]).decode(\"utf8\")) #check the files available in the directory\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory\n\nStartTime = datetime.datetime.now()","88a35d9b":"class MyTimer():\n    # usage:\n    #with MyTimer():                            \n    #    rf.fit(X_train, y_train)\n    \n    def __init__(self):\n        self.start = time.time()\n    def __enter__(self):\n        return self\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        end = time.time()\n        runtime = end - self.start\n        msg = 'The function took {time} seconds to complete'\n        print(msg.format(time=runtime))","86c7fb93":"#train = pd.read_csv('input\/train.csv')\n#test = pd.read_csv('input\/test.csv')\n\n# default competition\ncompetition = 'SR' # StackedRegression\n\na = !find ..\/ -name home-data-for-ml-course\n#if (competition == 'SRP_2'): # Stacked Regressions Part 2\nif (len(a) > 0):\n    competition = 'SR'\n    train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')#,index_col='Id')\n    test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')#,index_col='Id')\nelse:\n    competition = 'SRP_2'\n    train = pd.read_csv('..\/input\/train.csv')\n    test = pd.read_csv('..\/input\/test.csv')","740fd8cf":"##display the first five rows of the train dataset.\ntrain.head(5)","3cff6a41":"##display the first five rows of the test dataset.\ntest.head(5)\n","2b42e0be":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","508c1f79":"fig, ax = plt.subplots()\n#ax.scatter(x = train['GrLivArea'], y = train['SalePrice']\nax.scatter(x = train['GrLivArea'], y = np.log(train['SalePrice']))\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\n\n#m, b = np.polyfit(train['GrLivArea'], train['SalePrice'], 1)\nm, b = np.polyfit(train['GrLivArea'], np.log(train['SalePrice']), 1)\n#m = slope, b=intercept\nplt.plot(train['GrLivArea'], m*train['GrLivArea'] + b)\n\nplt.show()\n","0b5c70af":"train.shape[1]\n#a = int(np.sqrt(train.shape[1]))\na = 4\nb = int(train.shape[1]\/4)\nr = int(train.shape[1]\/a)\nc = int(train.shape[1]\/b)\ni = 0\nfig, ax = plt.subplots(nrows=r, ncols=c, figsize=(15, 60))\nfor row in ax:\n    for col in row:\n        #print(train.columns[i])\n        #print(train[train.columns[i]].dtype)\n        #col.plot()\n        #ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\n        try:\n            #col.scatter(x = train[train.columns[i]], y = train['SalePrice'])\n            col.scatter(x = train[train.columns[i]], y = np.log(train['SalePrice']))\n            col.title.set_text(train.columns[i])\n            #col.title(train.columns[i])\n        except:\n            temp=1\n        #except Exception as e:\n        #    print(e.message, e.args)\n        finally:\n            temp=1\n        i = i + 1\n        \nplt.show()","5b86c4d0":"# these don't help, leave them in the dataset\n#train = train.drop(train[(train['LotFrontage']>300) & (train['SalePrice']<400000)].index)\n#train = train.drop(train[(train['LotArea']>100000) & (train['SalePrice']<400000)].index)\n#train = train.drop(train[(train['BsmtFinSF1']>4000) & (train['SalePrice']<250000)].index)\n#train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<250000)].index)\n\n#Deleting outliers\ntrain = train.drop(train[(train['OverallQual']>9) & (train['SalePrice']<220000)].index)\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","2fb1b053":"#Check the graphic again\nfig, ax = plt.subplots()\n#ax.scatter(train['GrLivArea'], train['SalePrice'])\nax.scatter(train['GrLivArea'], np.log(train['SalePrice']))\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\n#m, b = np.polyfit(train['GrLivArea'], train['SalePrice'], 1)\nm, b = np.polyfit(train['GrLivArea'], np.log(train['SalePrice']), 1)\n#m = slope, b=intercept\nplt.plot(train['GrLivArea'], m*train['GrLivArea'] + b)\nplt.show()","94cef86f":"# linear \nx_data = train['GrLivArea']\ny_data = train['SalePrice']\nlog_y_data = np.log(train['SalePrice'])\n\ncurve_fit = np.polyfit(x_data, log_y_data, 1)\nprint(curve_fit)\ny = np.exp(1.11618915e+01) * np.exp(5.70762509e-04*x_data)\nplt.plot(x_data, y_data, \"o\")\nplt.scatter(x_data, y,c='red')","8f50c6ed":"# linear with log y\ny = np.exp(1.11618915e+01) * np.exp(5.70762509e-04*x_data)\nplt.plot(x_data, np.log(y_data), \"o\")\nplt.scatter(x_data, np.log(y),c='red')","ac732e5e":"# polynomial\nx_data = train['GrLivArea']\ny_data = train['SalePrice']\nlog_y_data = np.log(train['SalePrice'])\ncurve_fit_gla = np.polyfit(x_data, y_data, 2)\ny = curve_fit_gla[2] + curve_fit_gla[1]*x_data + curve_fit_gla[0]*x_data**2","6fd4606c":"plt.plot(x_data, y_data, \"o\")\nplt.scatter(x_data, y,c='red')","2f851c72":"# polynomial with log y\nplt.plot(x_data, np.log(y_data), \"o\")\nplt.scatter(x_data, np.log(y),c='red')","3e42d123":"sns.distplot(train['SalePrice'] , fit=norm)\n\n_skew = skew(train['SalePrice'])\n_kurtosis = kurtosis(train['SalePrice'])\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}, skew = {:.2f} kurtosis = {:.2f}\\n'.format(mu, sigma, _skew, _kurtosis))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n","82b2f3f5":"#We try the numpy function log1p which  applies log(1+x) to all elements of the column\n\n# option 1 - original\n#train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n# Option 2: use box-cox transform - this performs worse than the log(1+x), reverting back...now it seems to be scoring better. Maybe my earlier evaluation was incorrect\n# try different alpha values  between 0 and 1\nlam_l = 0.35\n#train[\"SalePrice\"] = boxcox1p(train[\"SalePrice\"], lam_l) # put this back in later, for final testing. remove for now as it confuses the evaluation scores between test and submit\n\n# Option 3: boxcox letting the algorithm select lmbda based on least-likelihood calculation\n#train[\"SalePrice\"], lam_l = boxcox(x=train[\"SalePrice\"], lmbda=None)\n\n# option 4 - compare to log1p => score is same\ntrain[\"SalePrice\"] = np.log(train[\"SalePrice\"])","e659d119":"x = np.linspace(0, 20)\ny1 = np.log(x)\ny2 = np.log1p(x)\ny3 = boxcox1p(x, 0.35)\ny4 = boxcox1p(x, 0.10)\ny5 = boxcox1p(x, 0.50)\nplt.plot(x,y1,label='log') \nplt.plot(x,y2,label='log1p') \nplt.plot(x,y3,label='boxcox .35') \nplt.plot(x,y4,label='boxcox .10') \nplt.plot(x,y5,label='boxcox .50') \nplt.legend()\nplt.show()\n\nx = np.linspace(0, 100000)\ny1 = np.log(x)\ny2 = np.log1p(x)\ny3 = boxcox1p(x, 0.35)\ny4 = boxcox1p(x, 0.10)\ny5 = boxcox1p(x, 0.50)\nplt.plot(x,y1,label='log') \nplt.plot(x,y2,label='log1p') \nplt.plot(x,y3,label='boxcox .35') \nplt.plot(x,y4,label='boxcox .10') \nplt.plot(x,y5,label='boxcox .50') \nplt.legend()\nplt.show()","abce6f9c":"# to revert back use y = inv_boxcox(x, lam_l) => train[\"SalePrice\"] = inv_boxcox(train[\"SalePrice\"], lam_l)\n# think the underlying formula is this: # pred_y = np.power((y_box * lambda_) + 1, 1 \/ lambda_) - 1\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n_skew = skew(train['SalePrice'])\n_kurtosis = kurtosis(train['SalePrice'])\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}, skew = {:.2f} kurtosis = {:.2f}\\n'.format(mu, sigma, _skew, _kurtosis))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n","4a69cedd":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_sales_prices = all_data['SalePrice'].astype(float)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","9128e005":"all_data.head()","2305cf2c":"def add_gla(row, p):\n    return (p[2] + p[1]*row.GrLivArea + p[0]*(row.GrLivArea**2))\n\n#all_data['GrLivAreaPoly'] = all_data.apply(lambda row: add_gla(row,curve_fit_gla), axis=1)\n#all_data[['GrLivAreaPoly','GrLivArea']].head()\n","4e18ae83":"all_data.GrLivArea[:ntrain]","8ad97332":"correlation_matrix = np.corrcoef(all_data.GrLivArea[:ntrain], y_train)\ncorrelation_xy = correlation_matrix[0,1]\nr_squared = correlation_xy**2\n\nprint(r_squared)","75753439":"a='''\ncorrelation_matrix = np.corrcoef(all_data.GrLivAreaPoly[:ntrain], y_train)\ncorrelation_xy = correlation_matrix[0,1]\nr_squared = correlation_xy**2\n\nprint(r_squared)'''","5ac1dec2":"for i in range(1,11,1):\n    j = i\/10\n    correlation_matrix = np.corrcoef(all_data.GrLivArea[:ntrain]**j, y_train)\n    correlation_xy = correlation_matrix[0,1]\n    r_squared = correlation_xy**2\n\n    print(j, r_squared)","599c1b73":"def add_gla2(row, p):\n    return (row.GrLivArea**p)\n\nall_data['GrLivAreaRoot'] = all_data.apply(lambda row: add_gla2(row,0.3), axis=1)","c1338646":"all_data.head()","8621ad9c":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","ccc03fe1":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","8e67ca93":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = train.corr(method='kendall')\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","142fe160":"print (corrmat['SalePrice'].sort_values(ascending=False)[:5], '\\n')\nprint (corrmat['SalePrice'].sort_values(ascending=False)[-5:])","539b4306":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","c20dba9a":"all_data.dtypes","851cef92":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","7dba5635":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","f3d3163b":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")","4f9f13af":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","e8161963":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","b4224ab7":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","fcdac2c3":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","6af8991b":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","318cc5dd":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","40a036b3":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","5d3262e5":"all_data['MSZoning'].value_counts()","5874fd93":"# this one may be a bit dangerous, maybe try to get zone from neighborhood most common value, similar to LotFrontage previously\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","3e3e6095":"all_data['Utilities'].value_counts()","0e74131f":"all_data = all_data.drop(['Utilities'], axis=1)","c0b3a36c":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","d68f82f9":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","6b4b4fdf":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","b47461aa":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","d99becb6":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","637aefe1":"\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\n","6aaf6e8f":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","b3b24015":"all_data['MSSubClass'].value_counts()\n","aebfa724":"all_data['OverallCond'].value_counts()","1b4ed865":"all_data.shape","e4aa77ea":"import datetime\nYr = all_data['YrSold'].min()\nMo = all_data['MoSold'].min()\nt = datetime.datetime(Yr, Mo, 1, 0, 0)\n\ndef calculateYrMo (row):   \n    return int((datetime.datetime(row.YrSold,row.MoSold,1) - t).total_seconds())","f5a8a8b7":"# either way will work\n#all_data['YrMoSold'] = all_data.apply(lambda row: int((datetime.datetime(row.YrSold,row.MoSold,1) - t).total_seconds()), axis=1)\n\nall_data['YrMoSold'] = all_data.apply(lambda row: calculateYrMo(row), axis=1)","891fb3a8":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\n","35d71b16":"from sklearn.preprocessing import LabelEncoder\n#cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n#        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n#        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n#        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n#        'YrSold', 'MoSold', 'YrMoSold')\n\n# Edit: Dropping PoolQC for missing values => makes the model worse, reverting\n#all_data = all_data.drop(['PoolQC'], axis=1)\n\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold', 'YrMoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))\n","a3a1a6b9":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n","64584be9":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n","0e960422":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\n\nlam_f = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam_f)\n    \n    #all_data[skewed_features] = np.log1p(all_data[skewed_features])","a5e618c0":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","1795cc69":"# to choose number of components, look at this chart. Reference: https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.09-principal-component-analysis.html\nfrom sklearn.decomposition import PCA\n\npca = PCA().fit(all_data)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')","03c2af22":"#do PCA\/StandardScaler+clip here or before the skew boxcox1p transform\n\nn_components = 50\npca = PCA(n_components=n_components)\nall_data_pca = pca.fit_transform(all_data)","b55920e2":"print(all_data.shape)\nprint(all_data_pca.shape)","efa3d4ca":"weights = np.round(pca.components_, 3) \nev = np.round(pca.explained_variance_ratio_, 3)\nprint('explained variance ratio',ev)\npca_wt = pd.DataFrame(weights)#, columns=all_data.columns)\npca_wt.head()","d68bdb89":"pca_wt.shape ","13510343":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = pd.DataFrame(all_data_pca).corr(method='kendall')\nplt.subplots(figsize=(12,9))\nplt.title(\"Kendall's Correlation Matrix PCA applied\", fontsize=16)\nsns.heatmap(corrmat, vmax=0.9, square=True)\n\n\n#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = train.corr(method='kendall')\nplt.subplots(figsize=(12,9))\nplt.title(\"Kendall's Correlation Matrix Initial Train Set\", fontsize=16)\nsns.heatmap(corrmat, vmax=0.9, square=True);\n","3607284d":"train_orig = train.copy()\ntrain_orig['SalePrice'] = y_train\ncorrmat = train_orig.corr(method='kendall')\nprint (corrmat['SalePrice'].sort_values(ascending=False)[:5], '\\n')\nprint (corrmat['SalePrice'].sort_values(ascending=False)[-5:])","890c5201":"a=''' not working now\ncorrelations = corrmat[\"SalePrice\"].sort_values(ascending=False)\nfeatures = correlations.index[0:10]\n\nsns.pairplot(train[features], size = 2.5)\nplt.show();'''","3f49cb62":"print(type(all_data))\nprint(type(pd.DataFrame(all_data_pca)))","e8e8e2dd":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nrc = RobustScaler()\n\nuse_pca = 0 # using PCA currently hurts the score\nuse_normalization = 0 # using StandardScaler doesn't work, try RobustScaler now\n\nif (use_pca == 1):\n    all_data_pca = pd.DataFrame(all_data_pca)\n    train = all_data_pca[:ntrain]\n    test = all_data_pca[ntrain:]\n    all_data_pca.head()\nelif (use_normalization == 1):\n    train = all_data[:ntrain]\n    test = all_data[ntrain:]\n    scaled_features = sc.fit_transform(train.values)\n    train = pd.DataFrame(scaled_features, index=train.index, columns=train.columns)\n    scaled_features = sc.transform(test.values)\n    test = pd.DataFrame(scaled_features, index=test.index, columns=test.columns)   \nelif (use_normalization == 2):\n    train = all_data[:ntrain]\n    test = all_data[ntrain:]\n    scaled_features = rc.fit_transform(train.values)\n    train = pd.DataFrame(scaled_features, index=train.index, columns=train.columns)\n    scaled_features = rc.transform(test.values)\n    test = pd.DataFrame(scaled_features, index=test.index, columns=test.columns)  \nelse:\n    # back to original splits (from train.csv and test.csv)\n    train = all_data[:ntrain]\n    test = all_data[ntrain:]\n","f836689d":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = train.corr(method='kendall')\nplt.subplots(figsize=(24,18))\nsns.heatmap(corrmat, vmax=0.9, square=True)","cdee2efb":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = test.corr(method='kendall')\nplt.subplots(figsize=(24,18))\nsns.heatmap(corrmat, vmax=0.9, square=True)","0f8e6506":"train.hist(bins=20, figsize=(30,20))\nplt.show()","3172bcb5":"test.hist(bins=20, figsize=(30,20))\nplt.show()","96914746":"train.describe()","983a3495":"test.describe()","a276340c":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n#from sklearn.metrics import mean_squared_log_error\n# to run locally: conda install -c anaconda py-xgboost\nimport xgboost as xgb\nimport lightgbm as lgb\n","93e0a948":"#Validation function\n# train.values and y_train are both log scaled so just need to take the square of the delta between them to calculate the error, then take the sqrt to get rmsle\n# but for now y_train is boxcox1p(), not log(). Use this to convert back: inv_boxcox(y_train, lam_l)\nn_folds=5 # was 5 => better score but twice as slow now\n\ndef rmsle_cv(model):\n    print(\"running rmsle_cv code\")\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) # was 42\n    # other scores: https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf)) # also r2\n    print(\"raw rmse scores for each fold:\", rmse)\n    return(rmse)\n\n# used for another competition\ndef mae_cv(model):\n    print(\"running mae_cv code\")\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) # was 42\n    # other scores: https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    mae = -cross_val_score(model, train.values, y_train, scoring=\"neg_mean_absolute_error\", cv = kf) # also r2\n    print(\"raw mae scores for each fold:\", mae)\n    return(mae)","4a50ce58":"print(y_train.mean())\n#print(inv_boxcox(y_train, lam_l).mean())","8913ea4f":"def runGSCV(num_trials, features, y_values):\n    non_nested_scores = np.zeros(num_trials) # INCREASES BIAS\n    nested_scores = np.zeros(num_trials)\n    # Loop for each trial\n    for i in range(num_trials):\n        print(\"Running GridSearchCV:\")\n        with MyTimer():    \n            #grid_result = gsc.fit(train, y_train)  \n            grid_result = gsc.fit(features, y_values)  \n        non_nested_scores[i] = grid_result.best_score_\n        if (competition == 'SR'):\n            print(\"Best mae %f using %s\" % ( -grid_result.best_score_, grid_result.best_params_))\n        else:\n            print(\"Best rmse %f using %s\" % ( np.sqrt(-grid_result.best_score_), grid_result.best_params_))\n        \n        # nested\/non-nested cross validation: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_nested_cross_validation_iris.html\n        with MyTimer():    \n            #nested_score = cross_val_score(gsc, X=train, y=y_train, cv=outer_cv, verbose=0).mean() \n            nested_score = cross_val_score(gsc, X=features, y=y_values, cv=outer_cv, verbose=0).mean() \n            # source code for cross_val_score is here: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/sklearn\/model_selection\/_validation.py#L137\n        if (competition == 'SR'):\n            print(\"nested mae score from KFold %0.3f\" % -nested_score)\n        else:\n            print(\"nested rmse score from KFold %0.3f\" % np.sqrt(-nested_score))\n        \n        nested_scores[i] = nested_score\n        print('grid_result',grid_result)\n        if (competition == 'SR'):\n            print(\"mean scores: r2(%0.3f) mae(%0.3f) nmse(%0.3f)\" % (grid_result.cv_results_['mean_test_r2'].mean(), -grid_result.cv_results_['mean_test_mae'].mean(),  np.sqrt(-grid_result.cv_results_['mean_test_nmse'].mean()) ))\n        else:\n            print(\"mean scores: r2(%0.3f) nmse(%0.3f) nmsle(%0.3f)\" % (grid_result.cv_results_['mean_test_r2'].mean(), np.sqrt(-grid_result.cv_results_['mean_test_nmse'].mean()), grid_result.cv_results_['mean_test_nmsle'].mean()))\n    return grid_result\n","4ea54816":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1)) # was 1","2df28aa7":"# initialize the algorithm for the GridSearchCV function\nENet = ElasticNet()\ntuningENet = 0 # takes 2 minutes to complete\n\nif (tuningENet == 1):\n    # use this when tuning\n    param_grid={\n        'alpha':[0.005,0.01,0.05,0.1],\n        'l1_ratio':[.6,.65,.7,.75,.8,.85,.9],\n        'fit_intercept':[True], # ,False\n        'normalize':[False], # True,\n        'max_iter':range(50,500,50),\n        'selection':['random'] # 'cyclic',\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'alpha':[0.05],\n        'l1_ratio':[.85],\n        'fit_intercept':[True],\n        'normalize':[False],\n        'max_iter':[100],\n        'selection':['random']\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error', # not working for ENet,. fix this locally\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=ENet,\n    param_grid=param_grid,\n    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)","71c2db62":"#ENet = make_pipeline(StandardScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nENet = make_pipeline(StandardScaler(), ElasticNet(**grid_result.best_params_, random_state=3))\n","d43d3292":"# initialize the algorithm for the GridSearchCV function\nKRR = KernelRidge()\ntuningKRR = 0 # this took 40 mins, 20 per iteration\n\nif (tuningKRR == 1):\n    # use this when tuning\n    param_grid={\n        'alpha':[2.0,2.2,2.4,2.6], \n        'kernel':['polynomial'], #for entire list see: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.pairwise.kernel_metrics.html#sklearn.metrics.pairwise.kernel_metrics\n        'gamma':[0.0001,0.001,0.01,0.1],\n        'degree':[1,2,3,4,5,6], \n        'coef0':[0.1,0.3,0.5,1.0,2.0]\n    }\n\nelse:\n    # use this when not tuning\n    # nmse: Best mae 583416973.611280 using {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n    # mae: Best mae 15805.764347 using {'alpha': 2.0, 'coef0': 0.1, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n    param_grid={\n        'alpha':[2.2], \n        'kernel':['polynomial'], # 'linear', 'rbf'\n        'gamma':[0.001],\n        'degree':[5], \n        'coef0':[0.5]\n    }\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=KRR,\n    param_grid=param_grid,\n    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)","63688e11":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nKRR = KernelRidge(**grid_result.best_params_)","5fdd2e12":"# initialize the algorithm for the GridSearchCV function\nrf = RandomForestRegressor()\ntuningRF = 0 # this took 2 hours last time, 1 hour per iteration\n\nif (tuningRF == 1):\n    # use this when tuning\n    param_grid={\n        'max_depth':[3,4,5],\n        'max_features':[None,'sqrt','log2'], \n        # 'max_features': range(50,401,50),\n        # 'max_features': [50,100], # can be list or range or other\n        'n_estimators':range(25,100,25), \n        #'class_weight':[None,'balanced'],  \n        'min_samples_leaf':range(5,15,5), \n        'min_samples_split':range(10,30,10), \n        'criterion':['mse', 'mae'] \n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'max_depth':[5],\n        'max_features':[None], # max_features is None is default and works here, removing 'sqrt','log2'\n        # 'max_features': range(50,401,50),\n        # 'max_features': [50,100], # can be list or range or other\n        'n_estimators': [50], # number of trees selecting 100, removing range(50,126,25)\n        #'class_weight':[None], # None was selected, removing 'balanced'\n        'min_samples_leaf': [5], #selecting 10, removing range 10,40,10)\n        'min_samples_split': [10], # selecting 20, removing range(20,80,10),\n        'criterion':['mse'] # remove gini as it is never selected\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=rf,\n    param_grid=param_grid,\n    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='mae' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)","017dac9e":"#RF = make_pipeline(StandardScaler(), RandomForestRegressor(max_depth=3,n_estimators=500))\nRF = make_pipeline(StandardScaler(), RandomForestRegressor(**grid_result.best_params_)) # slightly better than default, but still not good","444aa359":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state=5) # was 5","56f3e21a":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel\n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state=7, nthread = -1) # was random_state=7, cannot set to None \n\n","935b21bf":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","eeb71bf3":"if (competition == 'SR'):\n    score = mae_cv(lasso)\n    print(\"\\nLasso mae_cv score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nelse:\n    score = rmsle_cv(lasso)\n    print(\"\\nLasso rmsle_cv score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","d3b4ff50":"if (competition == 'SR'):\n    score = mae_cv(ENet)\n    print(\"\\ENet mae_cv score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nelse:\n    score = rmsle_cv(ENet)\n    print(\"\\ENet rmsle_cv score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","58246a35":"if (competition == 'SR'):\n    score = mae_cv(RF)\n    print(\"\\nRF mae_cv score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nelse:\n    score = rmsle_cv(RF)\n    print(\"\\nRF rmsle_cv score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","bf9c8ae1":"if (competition == 'SR'):\n    score = mae_cv(KRR)\n    print(\"\\nKRR mae_cv score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nelse:\n    score = rmsle_cv(KRR)\n    print(\"\\nKRR rmsle_cv score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","7393625c":"if (competition == 'SR'):\n    score = mae_cv(GBoost)\n    print(\"\\nGBoost mae_cv score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nelse:\n    score = rmsle_cv(GBoost)\n    print(\"\\nGBoost rmsle_cv score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","06d0dc99":"if (competition == 'SR'):\n    score = mae_cv(model_xgb)\n    print(\"\\nXGB mae_cv score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nelse:\n    score = rmsle_cv(model_xgb)\n    print(\"\\nXGB rmsle_cv score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","dfbae35c":"if (competition == 'SR'):\n    score = mae_cv(model_lgb)\n    print(\"\\nLGB mae_cv score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nelse:\n    score = rmsle_cv(model_lgb)\n    print(\"\\nLGB rmsle_cv score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","344c9445":"print(train.values)\n","12f07e0e":"print(y_train)","79fa0b04":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","a3a57f89":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso)) # maybe add RF here\n\nif (competition == 'SR'):\n    score = mae_cv(averaged_models)\n    print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nelse:\n    score = rmsle_cv(averaged_models)\n    print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","bb8486e0":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=10): # increasing this value should give a more accurate prediction, averaged over n_fold iterations\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156) # was 156\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models): # for each model passed in\n            for train_index, holdout_index in kfold.split(X, y): # create train,test for the number of folds\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index]) # fit the model for this fold\n                y_pred = instance.predict(X[holdout_index]) # predict values for this fold\n                out_of_fold_predictions[holdout_index, i] = y_pred # think we either use all of these values as features later, or the mean value?\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new and only feature\n        print(\"out_of_fold_predictions\", out_of_fold_predictions)\n        self.meta_model_.fit(out_of_fold_predictions, y) # need to see out_of_fold_predictions feature set\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","dce13b49":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR), # adding RF here did not help\n                                                 meta_model = lasso)\n\nif (competition == 'SR'):\n    score = mae_cv(stacked_averaged_models)\n    print(\"Stacking Averaged models score mean and std: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nelse:\n    score = rmsle_cv(stacked_averaged_models)\n    print(\"Stacking Averaged models score mean and std: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","633f89a7":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","c5ad7b2d":"def mae(y, y_pred):\n    return mean_absolute_error(y,y_pred)","604dc6ef":"type(train)","da700571":"test.values","f3133099":"averaged_models.fit(train.values, y_train)\naveraged_train_pred = averaged_models.predict(train.values)\naveraged_pred = np.exp(averaged_models.predict(test.values))\n#averaged_pred = np.expm1(averaged_models.predict(test.values))\n#averaged_pred = inv_boxcox(averaged_models.predict(test.values), lam_l)\n\nif (competition == 'SR'):\n    print(mae(y_train, averaged_train_pred))\nelse:\n    print(rmsle(y_train, averaged_train_pred))","4a301c5e":"averaged_models.predict(test.values)","c5eaba85":"averaged_train_pred","5b4f3448":"averaged_pred","3c2b0d69":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.exp(stacked_averaged_models.predict(test.values))\n#stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\n#stacked_pred = inv_boxcox(stacked_averaged_models.predict(test.values), lam_l)\n\nif (competition == 'SR'):\n    print(mae(y_train, stacked_train_pred))\nelse:\n    print(rmsle(y_train, stacked_train_pred))","9798b8b5":"print(y_train)\nprint(stacked_train_pred)\nprint(stacked_pred)","949ab9d0":"# values are not normalized\ntrain[train.columns].mean().head()","8797f6c8":"# are train and test normalized? between -1 and 1\nmodel_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.exp(model_xgb.predict(test))\n#xgb_pred = np.expm1(model_xgb.predict(test))\n#xgb_pred = inv_boxcox(model_xgb.predict(test), lam_l)\n\nif (competition == 'SR'):\n    print(mae(y_train, xgb_train_pred))\nelse:\n    print(rmsle(y_train, xgb_train_pred))","ec003b1a":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.exp(model_lgb.predict(test.values))\n#lgb_pred = np.expm1(model_lgb.predict(test.values))\n#lgb_pred = inv_boxcox(model_lgb.predict(test.values), lam_l)\n\nif (competition == 'SR'):\n    print(mae(y_train, lgb_train_pred))\nelse:\n    print(rmsle(y_train, lgb_train_pred))","0a1860f5":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\n\n\nstk = 0.70\nxgb = 0.15\nlgb = 0.15\n\n# can add averaged_pred here\n\nif (competition == 'SR'):\n    print(mae(y_train,stacked_train_pred*stk + xgb_train_pred*xgb + lgb_train_pred*lgb ))\nelse:\n    print(rmsle(y_train,stacked_train_pred*stk + xgb_train_pred*xgb + lgb_train_pred*lgb ))","fd4f93a4":"ensemble = stacked_pred*stk + xgb_pred*xgb + lgb_pred*lgb  # if using averaged_pred, need to add averaged_pred here","4b9470e9":"print(lgb_pred)\nprint(xgb_pred)\nprint(stacked_pred)\nprint(averaged_pred)\nprint(ensemble)","5229479d":"print(y_train,stacked_train_pred * stk + xgb_train_pred * xgb + lgb_train_pred * lgb) # if using averaged_pred, need to add averaged_pred here\n\nprint(y_train,stacked_train_pred)","3b39c160":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","11e58700":"sub.head()","dbce4e47":"!pwd","ff62e8f8":"ls -arlt","1889e8b5":"!head -10 submission.csv","eee83af8":"print(\"Start: \", StartTime)\nprint(\"End: \", datetime.datetime.now())","73aa7a20":"from mlens.ensemble import SuperLearner\nensemble = SuperLearner(random_state=1, verbose=2)","ba5995bc":"![Faron](http:\/\/i.imgur.com\/QBuDOjs.jpg)\n\n(Image taken from [Faron](https:\/\/www.kaggle.com\/getting-started\/18153#post103381))","b146fbf8":"We just average four models here **ENet, GBoost,  KRR and lasso**.  Of course we could easily add more models in the mix. ","4ab2f948":"Compare the r-squared values for different functions","9671e1ad":"**Box Cox Transformation of (highly) skewed features**","c58ea499":"- **MSZoning (The general zoning classification)** :  'RL' is by far  the most common value.  So we can fill in missing values with 'RL'\n","58052beb":"To make the two approaches comparable (by using the same number of models) , we just average **Enet, KRR and Gboost**, then we add **lasso as meta-model**.","ca53ee24":"This fit looks like it may be better, will add a column to the dataset using this equation after it is merged (all_data)\n\ny = curve_fit_gla[2] + curve_fit_gla[1]*x_data + curve_fit_gla[0]*x_data**2\n\nx_data = train['GrLivArea']\n\n","fa1c967f":"- **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. \n","4ffa6669":"On this gif, the base models are algorithms 0, 1, 2 and the meta-model is algorithm 3. The entire training dataset is \nA+B (target variable y known) that we can split into train part (A) and holdout part (B). And the test dataset is C. \n\nB1 (which is the prediction from the holdout part)  is the new feature used to train the meta-model 3 and C1 (which\nis the prediction  from the test dataset) is the meta-feature on which the final prediction is done. ","47468923":"Is there any remaining missing value ? ","d15a37b9":"##Stacking  models","e311e686":"Edit: pca.components_ is the matrix you can use to calculate the inverse of the PCA analysis, i.e. go back to the original dataset\nreference: https:\/\/stats.stackexchange.com\/a\/143949","d00f678e":"It remains no missing value.\n","f17593f3":"I recommend plotting the cumulative sum of eigenvalues (assuming they are in descending order). If you divide each value by the total sum of eigenvalues prior to plotting, then your plot will show the fraction of total variance retained vs. number of eigenvalues. The plot will then provide a good indication of when you hit the point of diminishing returns (i.e., little variance is gained by retaining additional eigenvalues).","a1281ebb":"- **MiscFeature** : data description says NA means \"no misc feature\"\n","28ea3ae3":"We use the **cross_val_score** function of Sklearn. However this function has not a shuffle attribut, we add then one line of code,  in order to shuffle the dataset  prior to cross-validation","6ed38c6f":"Do some PCA for the dataset, to remove some of the collinearity. Not sure if this will have any effect as collinearity is usually not detrimental to many or most algorithms","4fe0889e":"add the previous averaged models here","041253a1":"**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.","adc2c86a":"- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2** : For all these categorical basement-related features, NaN means that there is no  basement.\n","ba107a0d":"our column count went from 216 to n_component value","92fe5d9d":"use the 0.3 value as the improvement below this value is minimal","69173fa6":"- **Elastic Net Regression** :\n\nagain made robust to outliers","5ebd3727":"- **Fence** : data description says NA means \"no fence\"","10d8bce5":"Testing the new PCA dataset for analysis - RMSE looks works after PCA is applied, need to look at the Kaggle score later and see if it correlates, could be a mistake to use PCA on categorical dummy data. However, XGB is better with PCA n=50 option. Maybe use a heavier weight for that portion, or use all_data_pca only on that model...","afc4aaca":"The skew seems now corrected and the data appears more normally distributed. \n\nEdit: Both distributions have a positive kurtosis, which means there is a steep dropoff in the curve from the center, or the tails have few points. Skewness is close to 0 now, so that metric is closer to a normal distribution","a6f7be3c":"As you can see, the PCA analysis did its job, the features show little correlation now","1fb6a4b1":"##Base models","122d5311":"plot the poly fit data","c5532e2f":"- **Gradient Boosting Regression** :\n\nWith **huber**  loss that makes it robust to outliers\n    ","a8a183e8":"**Averaged base models class**","f32f1bec":"Cross Correlation chart to view collinearity within the features. Kendall's seems appropriate as the Output Variable is numerical and much of the input is categorical. Here is a chart to guide you to which method to use based on your dataset.\n\n![image.png](attachment:image.png)","899807b8":"![kaz](http:\/\/5047-presscdn.pagely.netdna-cdn.com\/wp-content\/uploads\/2017\/06\/image5.gif)\n\nGif taken from [KazAnova's interview](http:\/\/blog.kaggle.com\/2017\/06\/15\/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova\/)","c38eef38":"My equation has a worse correlation... will need to investigate this later. For now, i will simplify an just use a direct function...","d7fdf170":"Look at some correlation values in a list format","0c184648":"**Label Encoding some categorical variables that may contain information in their ordering set** ","522cadbc":"\nList of possible scoring values:  \nRegression  \n\n\u2018explained_variance\u2019 metrics.explained_variance_score  \n\u2018max_error\u2019 metrics.max_error  \n\u2018neg_mean_absolute_error\u2019 metrics.mean_absolute_error  \n\u2018neg_mean_squared_error\u2019 metrics.mean_squared_error  \n\u2018neg_root_mean_squared_error\u2019 metrics.mean_squared_error  \n\u2018neg_mean_squared_log_error\u2019 metrics.mean_squared_log_error  \n\u2018neg_median_absolute_error\u2019 metrics.median_absolute_error  \n\u2018r2\u2019 metrics.r2_score  \n\u2018neg_mean_poisson_deviance\u2019 metrics.mean_poisson_deviance  \n\u2018neg_mean_gamma_deviance\u2019 metrics.mean_gamma_deviance  ","e8f1ec58":"- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement","a6443ce1":"- **Kernel Ridge Regression** :","8563588c":"- **Functional** : data description says NA means typical","b80c120b":"- **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n","3492c1f5":"Here is the difference between BoxCox and Log values, the difference is substantial at the value of a house","6b449058":"We will be using Ensembling methods in this notebook. I will only create one dataset for all models to use, but you can create multiple datasets, each to be used by a different set of models, depending on your use case.\n![image.png](attachment:image.png)","b314f83e":"- **Random Forest Regressor** :","7eba2d1d":"###Final Training and Prediction","b7634759":"We get again a better score by adding a meta learner","61ee45ca":"#Data Processing","77e6c16b":"More filtering of data to try","22ac03f2":"## Ensembling StackedRegressor, XGBoost and LightGBM","5137f091":"- **LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can **fill in missing values by the median LotFrontage of the neighborhood**.","51464504":"- **Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n","c9c3bea7":"**Skewed features**","7dfecce5":"Objective: Predict prices for test (test.csv) dataset based on model build from train (train.csv) dataset\n\nEvaluation metric: 'The RMSE between log of Saleprice and log of prediction'. Need to convert salesprice to log value first. However seems that BoxCox does a better job here. For my testing E will remove BoxCox, but may want to put it back for the final submissions. Maybe one with boxcox1p() and one with log().\n\nOriginal competition (explains the evaluation metric): https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/overview\/evaluation My submission on that site is 33 out of 38000, top .08%\n\nOriginal notebook source: https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n\nreference for good ideas: https:\/\/towardsdatascience.com\/tricks-i-used-to-succeed-on-a-famous-kaggle-competition-adaa16c09f22\n\noriginal default score was .11543, new best score is 0.11353\n\nStacked and Ensembled Regressions to predict House Prices\nHow to Kaggle: https:\/\/www.youtube.com\/watch?v=GJBOMWpLpTQ\n\nDonald S\nJune 2020\n\nNeed to submit every 2 months as the leaderboard will rollover after this 2 month period\n","fc0b6838":"- **MSSubClass** : Na most likely means No building class. We can replace missing values with None\n","6d8ce310":"- **KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent)  for the missing value in KitchenQual.\n","1af85c8d":"Another way to combine multiple models:\n\nThe function cross_val_predict is appropriate for:\nVisualization of predictions obtained from different models.\n\nModel blending: When predictions of one supervised estimator are used to train another estimator in ensemble methods.","637fcbf1":"Seems that RMSLE does not correlate to a good score\n\n![image.png](attachment:image.png)","52bd2646":"**LightGBM:**","e9d4e4ef":"- **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None","24f4bb0a":"**Tansformation of the target variable**\n \nDefault is to use log1p as this is included in the evaluation metric, rmsle \nEdit: also trying box-cox transform.\nThe Box-Cox transform is given by:\n<pre>\ny = (x**lmbda - 1) \/ lmbda,  for lmbda > 0  \n    log(x),                  for lmbda = 0","12186953":"- **LightGBM** :","6da1d2d5":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers.\nTherefore, we can safely delete them.\n\nEdit: Think this deletion is safe to do as there are only 2 points and they seem to be abnormal, possibly data error","93e44f99":"[Documentation][1] for the Ames Housing Data indicates that there are outliers present in the training data\n[1]: http:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/Decock\/DataDocumentation.txt","687b96e1":"**Define a cross validation strategy**","9c21c4bf":"Getting the new train and test sets. ","ab63565f":"Let's explore these outliers\n","59ae6ace":"Now for r squared calculations we need to limit the comparison to only the train data since the test data does not have a Sales Price to compare to","140a95e2":"**StackedRegressor:**","983607c3":"* save this inv function for later, may need it:\n<pre>\nimport cmath\nd = (b**2) - (4*a*c)\nprint(-b-cmath.sqrt(d))\/(2*a)\nprint(-b+cmath.sqrt(d))\/(2*a)\ndef add_gla3(row, p):\n    return (p[2] + p[1]*row.GrLivArea + p[0]*(row.GrLivArea**2)) <-- change this function\nall_data['GrLivAreaPoly2'] = all_data.apply(lambda row: add_gla3(row,curve_fit_gla), axis=1)","ef01a36e":"In this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model. \n\nThe procedure, for the training part, may be described as follows:\n\n\n1. Split the total training set into two disjoint sets (here **train** and .**holdout** )\n\n2. Train several base models on the first part (**train**)\n\n3. Test these base models on the second part (**holdout**)\n\n4. Use the predictions from 3)  (called  out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs  to train a higher level learner called **meta-model**.\n\nThe first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration,  we train every base model on 4 folds and predict on the remaining fold (holdout fold). \n\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as \nnew feature to train our meta-model in the step 4.\n\nFor the prediction part , We average the predictions of  all base models on the test data  and used them as **meta-features**  on which, the final prediction is done with the meta-model.\n","10763bfc":"-  **LASSO  Regression**  : \n\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's  **Robustscaler()**  method on pipeline, also want to compare to StandardScaler() => RobustScaler() is slightly better","de0880ae":"##Outliers","bef52467":"let's first  concatenate the train and test data in the same dataframe","078717f1":"This model needs improvement, will run cross validate on it","3dd387a9":" We choose number of eigenvalues to calculate based on previous chart, 20 looks like a good number, the chart starts to roll off around 15 and almost hits max a 20. We can also try 30,40 and 50 to squeeze a little more variance out...","7947a954":"We use the scipy  function boxcox1p which computes the Box-Cox transformation of **\\\\(1 + x\\\\)**. \n\nNote that setting \\\\( \\lambda = 0 \\\\) is equivalent to log1p used above for the target variable.  \n\nSee [this page][1] for more details on Box Cox Transformation as well as [the scipy function's page][2]\n[1]: http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html\n[2]: https:\/\/docs.scipy.org\/doc\/scipy-0.19.0\/reference\/generated\/scipy.special.boxcox1p.html","2aff507a":"The target variable is right skewed.  As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.\n","3f0a4d23":"##Features engineering","11332cbc":"Check predictions, are they one same scale as SalePrice in Train dataset?","71b9ad1f":"**Transforming some numerical variables that are really categorical**\n\nEdit: I will add a new column year and date as ordinal, since there is some order to the values it may be useful","c0e73880":"Then, add this column from previous investigation to the dataset y = curve_fit_gla[2] + curve_fit_gla[1]x_data + curve_fit_gla[0]x_data**2","5628b1bf":"**Data Correlation**\n","d82703da":"* check and compare the new columns","ddc529c9":"- **XGBoost** :","baab1437":"- **SaleType** : Fill in again with most frequent which is \"WD\"","8b8180ce":"- **PoolQC** : data description says NA means \"No  Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general. ","6e5443bf":"We begin with this simple approach of averaging base models.  We build a new **class**  to extend scikit-learn with our model and also to leverage encapsulation and code reuse ([inheritance][1]) \n\n\n  [1]: https:\/\/en.wikipedia.org\/wiki\/Inheritance_(object-oriented_programming)","cd34d241":"Now we look at a polynomial fit","d81bf56b":"Edit: Look for fliers in other columns","e9967e1e":"**Averaged base models score**","eab35287":"###Less simple Stacking : Adding a Meta-model","9bdc6fcf":"<pre>\nTypical flow of model building: Use GridSearch for determining Best parameters => **best_estimator_ \n\ngood basic resource for models: \nhttps:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\nhttps:\/\/scikit-learn.org\/stable\/modules\/grid_search.html\n\nA search consists of:\nan estimator (regressor or classifier such as sklearn.svm.SVC());  \na parameter space;  \na method for searching or sampling candidates;  \na cross-validation scheme; and  \na score function.  ****\n","1b130074":"![image.png](attachment:image.png)","0fd3b449":"**Create File for Submission to Kaggle**","6cd94bd1":"**Getting dummy categorical features**","34dfdf39":"###More features engeneering","da7fc1c4":"Kfold is useful for thorough testing of a model, will give a more accurate score based on remove some data test on the remaining and change the data removed each time. See image below for details:","78a508f2":"###Note : \n Outliers removal is note always safe.  We decided to delete these two as they are very huge and  really  bad ( extremely large areas for very low  prices). \n\nThere are probably others outliers in the training data.   However, removing all them  may affect badly our models if ever there were also  outliers  in the test data. That's why , instead of removing them all, we will just manage to make some of our  models robust on them. You can refer to  the modelling part of this notebook for that. \n\nEdit: todo - look for other outliers in all columns. Maybe better to use StandardScaler() and np.clip","a77c7b33":"<pre>\nfor final model:\n    1) change back to BoxCox\n    2) remove random_state = 3, 5 and maybe all...\n\nIssue with submit screen data not matching run\/edit\/save version screen\nTrust the submit screen over the edit\/save version screen. This seems to be not what is actually submitted.\nwrong: https:\/\/www.kaggle.com\/donaldst\/stacked-regressions-part-2\/edit\/run\/37370884\ncorrect: https:\/\/www.kaggle.com\/donaldst\/stacked-regressions-part-2?scriptVersionId=37373730\n\n 1) #400  0.11543   Default notebook\n 2) #400  4.19420   BoxCox on Sale Price (forgot to revert back)\n 5) #400  0.11543   Back to default\n 6) #288  0.11513   Added YrMo Cat\/Ord column\n 8) #270  0.11482   Use BoxCox1p([SalePrice], lam) and revert using inv_boxcox\n10) #270  0.11631   use BoxCox([SalePrice]) which uses default lambda minimizing least-likelihood\n13)       0.11482   revert back to BoxCox1p\n14)       0.11485   change lam from .15 to .05 (better skew, similar kurtosis\n15) #263  0.11465   change lam to .20\n16)       0.11492   change lam to .25\n17) #259  0.11461   change lam to .30\n18)       0.11474   change lam to .40 (too far)\n19) #217  0.11395   change lam to .35 (stick with this for now)\n20)       0.11479   drop PoolQC column for missing values (column was accurate by replacing the missing with None)\n22) #217  0.11546   put PoolQC back, drop MiscFeature\n23)       0.11395*  revert, no dropping other columns\n25) #223  0.14229   add PCA(n=50) with no other adjustments\n27) #223  0.17279   PCA(n=20)\n28)       0.14292   revert to n=50 (noticed the score seems to change slightly even with the same settings\n29)       0.15168   change proportions of the models to use the model with the lowest rmsle (XGBoost) more, from .70, 15, .15 to .15, .70, .15 => score is worse, decreasing rmsle is not an accurate method to get a better score\n30)       0.11445 change max_depth of model_xgb from 3 to 2 (weaker learner) => increases the rmsle and has worse score\n31)       0.11410   try max_depth=4 => still worse, looks like 3 is the best number\n32)       0.11395   revert to ~ #23\n33) #211  0.11356   change from n_folds=5 to n_folds=10 (rmsle_cv and StackingAveragedModels) for more thorough testing (takes 15 mins to run now)\n34)       0.11356   use default scoring in cross_val_score function, not neg_mean_squared_error => same score, is this line of code not used?\n35) revert to #33   stacked=.70, xgb=.15, lgb=.15\n38)       0.11409   stacked=.98, xgb=.01, lgb=.01 => worse\n39) #216  0.11363   stacked=.70, xgb=.05, lgb=.25 => also worse\n41)       0.11375   stacked=.70, xgb=.25, lgb=.05 => also worse, seems ensembling is better...\n42)       0.11364   stacked=.60, xgb=.15, lgb=.25 => worse\n43)       0.11366   stacked=.60, xgb=.20, lgb=.20 => worse, looks like the kernel already hsa the optimal mix\n44)       0.11486   filter the other 4 columns => worse, seems this information is useful...\n45)       0.11622   put 2 back, hmm. even worse now...\n46)       0.11464   try other 2 instead  - ready to submit, just wait for tomorrow\n47)       0.11825   remove all filtering, reverting as any other combinations are worse, the original user must have already checked this\n49)  #176 0.11356   revert to #33 (think a few scores drpped off already, my score moved up even though it didn't improve from #33)\n50)  #176 0.11353   try StandardScaler() insteaf of RobustScaler(), 2x => slight improvement\n52)       0.11356   StandardScaler on Lasso only, see which one benefits most from the change to StandardScaler\n54)       0.11353   StandardScaler on ENet only, keep this for now as the score is the same\n56)       0.11488   revert back to np.log1p and np.expm1() as the metric for evaluation is the log of the prediction vs the log of the actual... => doesn't like this as much as BoxCox! very strange, maybe the metric changed? Why does BoxCox perform better?\n57)       0.11489   try np.log() and exp(), should give the same value, but may be faster or slower...\n58)       0.11488   revert back to BoxCox, using lambda=0. This will cause BoxCox to use log(x) instead of x**lambda - 1\n60)       0.11419   lam_l = 0.30 => worse\n61)       0.11408   lam_l = 0.40 => worse\n63)       0.11353   revert to #54, filter for OverallQual => same score, keep the filter in, fix bug (la)\n64)       0.11532   changed random states + 1, found a random_state that was worse for our model (Only GB and XGB changed their RMSLE values)\n65)       0.11632   transform and fit\/scale the train and transform test data don't think this is being done currently => worse\n66)       0.11629   try RobustScaler => no better, remove\n67)       0.11353   revert to #63\n68)       0.11353   try to add random forest regressor, can add it to average, stacked or separate\n69)       0.11376   adding RF(max_depth=3,n_estimators=500) as is to StackingAveragedModels() => not bad, now tune it\n71)       0.11382   RF with tuning => worse with tuning?\n72)       0.11379   more RF tuning => still worse\n74)       0.11383   RF remove StandardScaler() => worse, keep it if using RF\n75) revert to #68, not using RF at all\n76) standardize gridSearchCV tuning with a function\n77)       0.11353   n_folds=5 for rmsle_cv, leave the other one in as it helps a little.\n78)       0.11361   optimize ENet => no improvement...\n80)       0.11455   setting all random_state=None, think this will help with improving final private score... Also think this is messing up my  cv\/optimization process as I may be only improving on my score on the seed I chose, not on improving a general model score. However, since i can only see my score 10 times a day and therefore cannot optimize for score, I will revert back to set the random_state values that affect my score to keep the   comparison constant, one at a time to see which one(s) are actually affecting my score  \n81)       0.11392   random_state=42 => better score that 0.11455, keep this one\n82)       0.11441   random_state=1 => a little better, get rid of it for now, but consider putting back\n84)       0.11467   remove random_state=1 and set random_state=3 => also worse\n85)       0.11471   remove random_state=3 and set random_state=5 => worse\n86)       0.11421   remove random_state=5 and set random_state=7 => slightly better\n87)       0.11413   remove random_state=7 and set random_state=156 => slightly better\n88)       0.11385   now combine the ones that show improvement, 42+1+7+156 => score is good, but still not as good as having all random_state values... \n*89) add 3 back in, even though by itself it had a worse score, but perhaps there is some unknown interaction\nwaiting to submit\n4)    12035.60396   This other competition must be using straight absolute price error, no log involved\n5)    12107.31293   start from #88 revert from boxcox() to log(), will add 3 in next\n6)    12127.38729   add random_state=3 back in => worse, remove it\n7)    12154.78844   remove random_state=3 and add random_state=5 back in => also worse\n8)    12155.05135   add both 3 and 5 for sanity check, since we changed competitions, both should be even worse together... => yep, remove them\n9)    12114.40123   revert to #5, remove 3 and 5 => a little worse, why? rerun #9 and resubmit at #10\n10)   12144.94765   rerun #9 and see if score changes even on same code => yep, the score changes...\n11)   12155.05135   add random_state=3 and 5 back, want to see if the scores are more consistent that way...\n12)   12155.05135   repeat, => now the results are consistent. \n13)   12199.69976   add code for exploring GrLivArea, want to change the data to polynomial, and other transforms, based on the scatter plot => worse now\n14) **12146.05959   keep the GLARoot, remove the GrLivAreaPoly as it has worse correlation than the original... => better now\n31) added positive modifications after reverting to #14\n\ntry ENet with no scaling\nuse refit=mse for KRR => seems better, so investigate the best match for algorithm to metrics\ntry ENet now, refit=mse => slightly better\n\n?)optimize KRR\n? add averaged_pred to final model\n\n\n??) need to check if the test data is similar enough to the train data first glance looks similar enough\n??) still need to optimize GBoost, KRR and \"meta\" Lasso, along with xgb and lgb\n\n *** benchmark score (with boxcox, rmsle, all random_states are set to a number, no None values)\n * temporary changes needed for iteratively improving my model, may need to undo when I am finished\n ** benchmark score to beat after temporary changes applied\n\ndone:   \n    try pca on features => no improvement in score. will drop this for now\n    change ratios of models to find which is best predictor (gives best score), then work on improving that model first\n    change scale factor of 3 models => score does not correlate to rmsle, try to find a better metric\n    try StandardScaler and np.clip on features, currently see RobustScaler() being used but i already screened fliers so StandardScaler should be adequate\n    more filtering of fliers\n\nto do:\n    need to investigate why using BoxCox is (slightly) better than the evaluation metric further...  \n    use all_data_pca only on models that show improvement\n    add RandomForest[Regressor]\n    use GridsearchCV to find optimal parameters, however, need to find the best way to evaluate the scores. RMSLE doesn't seem to match well...\n    \n    ideas from other kernels:\n    \n    The inverse transformation has been done on the final_prediction with the same log object which was created for transformation. This is saved as saleprice.\n    saleprice=logy_netrf.inverse_transform(final_prediction)\n    Now RMLSE is calculated as below\n    np.sqrt(mean_squared_error(np.log(submission['SalePrice'].values), np.log(saleprice)))\n\n    however, RF model is chosen, as the error variance is lowest.\n    \n    I have also referred to evaluation metric, which has been described as 'The RMSE between log of Saleprice and log of prediction'.\n    \n    add new features for non-linear features (OverallQual, etc) add a new column with log or square of feature\n    \n    # feature engineering a new feature \"TotalSF\"\n    all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n    all_data['YrBltAndRemod'] = all_data['YearBuilt'] + all_data['YearRemodAdd']\n    all_data['Total_sqr_footage'] = (all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'])\n    all_data['Total_Bathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) + all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))\n    all_data['Total_porch_sf'] = (all_data['OpenPorchSF'] +                         \n    all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF'])\n    all_data['haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    \n    tune min_samples_leaf and min_samples_split for GradientBoostingRegressor!\n    \n    \n    predictions are bad at each end of the scale:\n\n    \n    ![image.png](attachment:image.png) \n    \n    to fix this, use:\n    q1 = final_sub['SalePrice'].quantile(0.0025)\n    q2 = final_sub['SalePrice'].quantile(0.0045)\n    q3 = final_sub['SalePrice'].quantile(0.99)\n\n    final_sub['SalePrice'] = final_sub['SalePrice'].apply(lambda x: x if x > q1 else x*0.79)\n    final_sub['SalePrice'] = final_sub['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n    final_sub['SalePrice'] = final_sub['SalePrice'].apply(lambda x: x if x < q3 else x*1.0)\n    \n    add linear regression from here: https:\/\/github.com\/chouhbik\/Kaggle-House-Prices\/blob\/master\/Kaggle-house-prices-KamalChouhbi.ipynb\n    \n    found an interesting here: http:\/\/ml-ensemble.com\/\n    \n    from mlens.ensemble import SuperLearner \n    ensemble = SuperLearner()\n    ensemble.add(estimators)\n    ensemble.add_meta(meta_estimator)\n    ensemble.fit(X, y).predict(X)\n    \npossibly use this instead of my own stack\/ensemble code:\n\nfrom mlens.ensemble import SuperLearner\nensemble = SuperLearner(random_state=1, verbose=2)\n\nexample stored in hard drive at SampleNotebooks\/EnsembleModels\/ML-Ensemble.ipynb\n    \n\n","3840f390":"Compare the r-squared values","c84505c4":"**Ensemble prediction:**\n\nwhen deciding which models to include in an ensemble:  \n    fewer are better  \n    more diverse are better  ","d4e9fb65":"We add **XGBoost and LightGBM** to the **StackedRegressor** defined previously. ","e6c57dc3":"- **FireplaceQu** : data description says NA means \"no fireplace\"","7a8b8c19":"##Target Variable","59cfedfb":"Now lets use a log y scale","ca3c1a12":"Let's see how these base models perform on the data by evaluating the  cross-validation rmsle error","d7f1566e":"**Stacking averaged Models Class**","4a7f504f":"**Stacking Averaged models Score**","1ee3b0ac":"Comments from the original author:\n\nThis competition is very important to me as  it helped me to begin my journey on Kaggle few months ago. I've read  some great notebooks here. To name a few:\n\n1. [Comprehensive data exploration with Python][1] by **Pedro Marcelino**  : Great and very motivational data analysis\n\n2. [A study on Regression applied to the Ames dataset][2] by **Julien Cohen-Solal**  : Thorough features engeneering and deep dive into linear regression analysis  but really easy to follow for beginners.\n\n3. [Regularized Linear Models][3] by **Alexandru Papiu**  : Great Starter kernel on modelling and Cross-validation\n\nI can't recommend enough every beginner to go carefully through these kernels (and of course through many others great kernels) and get their first insights in data science and kaggle competitions.\n\nAfter that (and some basic pratices) you should be more confident to go through [this great script][7] by **Human Analog**  who did an impressive work on features engeneering. \n\nAs the dataset is particularly handy, I  decided few days ago to get back in this competition and apply things I learnt so far, especially stacking models. For that purpose, we build two stacking classes  ( the simplest approach and a less simple one). \n\nAs these classes are written for general purpose, you can easily adapt them and\/or extend them for your regression problems. \nThe overall approach is  hopefully concise and easy to follow.. \n\nThe features engeneering is rather parsimonious (at least compared to some others great scripts) . It is pretty much :\n\n- **Imputing missing values**  by proceeding sequentially through the data\n\n- **Transforming** some numerical variables that seem really categorical\n\n- **Label Encoding** some categorical variables that may contain information in their ordering set\n\n-  [**Box Cox Transformation**][4] of skewed features (instead of log-transformation) : This gave me a **slightly better result** both on leaderboard and cross-validation.\n\n- ** Getting dummy variables** for categorical features. \n\nThen we choose many base models (mostly sklearn based models + sklearn API of  DMLC's [XGBoost][5] and Microsoft's [LightGBM][6]), cross-validate them on the data before stacking\/ensembling them. The key here is to make the (linear) models robust to outliers. This improved the result both on LB and cross-validation. \n\n  [1]: https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n  [2]:https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\n  [3]: https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models\n  [4]: http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html\n  [5]: https:\/\/github.com\/dmlc\/xgboost\n [6]: https:\/\/github.com\/Microsoft\/LightGBM\n [7]: https:\/\/www.kaggle.com\/humananalog\/xgboost-lasso\n\nTo my surprise, this does well on LB ( 0.11420 and top 4% the last time I tested it : **July 2, 2017** )\n\n","1ea9eac1":"Another look at the feature to output correlations","29880365":"- **Utilities** : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\"  and 2 NA . Since the house with 'NoSewa' is in the training set, **this feature won't help in predictive modelling**. We can then safely  remove it.\n","a2cd1169":"###Base models scores","e4fbf5cc":"- **Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n","4e3a7c04":"###Simplest Stacking approach : Averaging base models","0f3033dd":"#Modelling","94c25414":"Wow ! It seems even the simplest stacking approach really improve the score . This encourages \nus to go further and explore a less simple stacking approch. ","0f4833bc":"**Adding one more important feature**","43fffb1f":"Want to add ordinal or int column for Year and Month, this is the function to perform that task","66c14d86":"Since area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","ea6af66d":"**XGBoost:**","5770e89d":"We impute them  by proceeding sequentially  through features with missing values ","68c56d64":"- **Alley** : data description says NA means \"no alley access\"","6b37e662":"###Investigate Missing Data","c47b15a1":"![image.png](attachment:image.png)","1663d8ea":"The shape values are the number of columns in the PCA x the number of original columns","2878caba":"We first define a rmsle evaluation function ","3b1c0743":"**Import libraries**","a2b690a6":"Need to look at the y_log relationship since that is what we will be predicting in the model (convert back later)","dc97efff":"###Imputing missing values ","352fb8d4":"This chart does not look linear, or at least the line is not matching the data across the entire x axis. Looks like a drop off for High GrLivArea, seems home buyers are not willing to pay a corresponding amount extra for the large living area, looking for a \"volume discount\" maybe...\nLets look at this closer, first fit a line, next try a polynomial fit to compare","43b52e58":"replace cross_val_score() with cross_validate()\n# reference: https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\n\n>>> from sklearn.metrics import make_scorer\n>>> scoring = {'prec_macro': 'precision_macro',\n...            'rec_macro': make_scorer(recall_score, average='macro')}\n>>> scores = cross_validate(clf, X, y, scoring=scoring,\n...                         cv=5, return_train_score=True)\n>>> sorted(scores.keys())\n['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro',\n 'train_prec_macro', 'train_rec_macro']\n>>> scores['train_rec_macro']\narray([0.97..., 0.97..., 0.99..., 0.98..., 0.98...])","13da4c90":"Compare Train to Test data, verify the distributions look similar, maybe add probablity plots per feature with train and test on same chart"}}