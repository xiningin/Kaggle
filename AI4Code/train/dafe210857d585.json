{"cell_type":{"213629f5":"code","862f9a56":"code","2623bba8":"code","260ee25b":"code","3430b23a":"code","c1e25956":"code","c6730284":"code","56f24b08":"code","e969d25c":"code","330671cc":"code","e65ef91c":"code","e49b97e7":"code","73759695":"code","3854bd4a":"code","716bba82":"code","58a79fdd":"code","14a24763":"code","3852417e":"code","a38bc99c":"code","328fb489":"code","114c3d3e":"code","faf6f24b":"code","e1b3202b":"markdown","94ad789f":"markdown","77007fbb":"markdown","5f1cb1f4":"markdown","1927fe39":"markdown","8d4fc2b6":"markdown","21dcf83b":"markdown","f33bb7c0":"markdown","afb9750f":"markdown","b5114287":"markdown","2c589d16":"markdown","5efeb1b6":"markdown","de8012a0":"markdown","71cbda91":"markdown","9e07fcc4":"markdown","96e4fb5c":"markdown","b38d72c6":"markdown","3ae39dc4":"markdown","5b2fb482":"markdown","68a45afe":"markdown"},"source":{"213629f5":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","862f9a56":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import (GridSearchCV,\n                                     StratifiedKFold)\n\nfrom sklearn.ensemble import (RandomForestClassifier, \n                              AdaBoostClassifier, \n                              GradientBoostingClassifier, \n                              ExtraTreesClassifier, \n                              VotingClassifier)\n\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nSEED = 17","2623bba8":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nPassengerId = test.PassengerId\ntrain_len = len(train)\ndf = pd.concat([train, test])\n\ntrain.head(3)","260ee25b":"df.info()","3430b23a":"# filling in age and fare\ndf['Age'] = df['Age'].fillna(train['Age'].median())\ndf['Fare'] = df['Fare'].fillna(train['Fare'].median())","c1e25956":"g = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0)], color=\"Red\", shade = True)\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1)], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","c6730284":"g = sns.kdeplot(train[\"Fare\"][(train[\"Survived\"] == 0)], color=\"Red\", shade = True)\ng = sns.kdeplot(train[\"Fare\"][(train[\"Survived\"] == 1)], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Fare\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","56f24b08":"df['Age_cat'] = (df['Age']\/\/15)*15\ndf['Fare_cat'] = pd.qcut(df['Fare'], 5)","e969d25c":"df['Family_size'] = df['SibSp'] + df['Parch']\n\ng = sns.factorplot(x=\"Family_size\",y=\"Survived\", data=df, kind=\"bar\", size=6)\ng.despine(left=True)\ng = g.set_ylabels(\"Survival probability\")","330671cc":"df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.') #extracting title from name\n\ng = sns.countplot(x=\"Title\",data=df)\ng = plt.setp(g.get_xticklabels(), rotation=45) ","e65ef91c":"df['Title'].replace({'Mlle': 'Miss', 'Mme': 'Mrs', 'Ms': 'Miss'}, inplace = True)\ndf['Title'].replace(['Don', 'Rev', 'Dr', 'Major', 'Lady', 'Sir', 'Col', 'Capt', \n                     'the Countess', 'Jonkheer', 'Dona'], 'Rare', inplace = True)","e49b97e7":"df['Surname'] = df['Name'].apply(lambda x: str.split(x, ',')[0])\n\ndf['Relatives_survival'] = 0.5 #base probablity, will remain if we find no useful info from surname and ticket\n\nfor grp_index, grp_data in df[['Survived','Name', 'Surname', 'Fare', 'Ticket', 'PassengerId', \n                            'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Surname', 'Fare']):\n    \n    if len(grp_data) > 1:  # More then one person with the same surname\n        for index, row in grp_data.iterrows():\n            surv_max = grp_data.drop(index)['Survived'].max() #finding whether there are survived relatives\n            surv_min = grp_data.drop(index)['Survived'].min() #finding whether there are non-survived relatives\n            passID = row['PassengerId']\n            \n            if surv_max == 1:\n                df.loc[df['PassengerId'] == passID, 'Relatives_survival'] = 1 #if survived relatives found, set 1\n            elif surv_min == 0:\n                df.loc[df['PassengerId'] == passID, 'Relatives_survival'] = 0 #if not set 0\n\nfor grp_index, grp_data in df.groupby('Ticket'):\n    \n    if len(grp_data) > 1: # More then one person with the same ticket\n        for index, row in grp_data.iterrows(): # repeat the same as for surname, if survived relative not found\n            if (row['Relatives_survival'] == 0) or (row['Relatives_survival'] == 0.5):\n                surv_max = grp_data.drop(index)['Survived'].max()\n                surv_min = grp_data.drop(index)['Survived'].min()\n                passID = row['PassengerId']\n                \n                if surv_max == 1:\n                    df.loc[df['PassengerId'] == passID, 'Relatives_survival'] = 1\n                elif surv_min == 0:\n                    df.loc[df['PassengerId'] == passID, 'Relatives_survival'] = 0","73759695":"df['Age_cat'] = LabelEncoder().fit_transform(df['Age_cat'])\ndf['Fare_cat'] = LabelEncoder().fit_transform(df['Fare_cat'])\ndf['Title'] = LabelEncoder().fit_transform(df['Title'])\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'])\ndf.drop(['PassengerId', 'Age', 'Fare', 'Name', 'SibSp', 'Parch', \n         'Ticket', 'Cabin', 'Title', 'Embarked', 'Surname'], axis = 1, inplace = True)","3854bd4a":"df.head(3)","716bba82":"train = df[:train_len]\nx_train = train.drop('Survived', axis=1)\ny_train = train.Survived.astype(int)\ntest = df[train_len:].drop(\"Survived\",axis=1)","58a79fdd":"gbc_params = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] }\n\n\nrfc_params = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\nsvc_params = {'kernel': ['rbf'], \n              'gamma': [ 0.001, 0.01, 0.1, 1],\n              'C': [1, 10, 50, 100,200, 300, 1000]}\n\next_params = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\nada_params = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\nskf = StratifiedKFold(n_splits=5)","14a24763":"gbc = GradientBoostingClassifier(random_state=SEED)\nrfc = RandomForestClassifier(random_state=SEED)\nsvc = SVC(probability=True, random_state=SEED)\next = ExtraTreesClassifier(random_state=SEED)\ndtc = DecisionTreeClassifier(random_state=SEED)\nada = AdaBoostClassifier(dtc, random_state=SEED)","3852417e":"base_models_params = [[gbc, gbc_params],\n                      [rfc, rfc_params],\n                      [svc, svc_params],\n                      [ext, ext_params],\n                      [ada, ada_params]]","a38bc99c":"best_models = []\nfor model, params in base_models_params:\n    gridsearch = GridSearchCV(model, params, cv=skf, n_jobs=-1, verbose=1, scoring='accuracy').fit(\n        x_train, y_train)\n    best_models.append(gridsearch.best_estimator_)","328fb489":"voting_clf_hard = VotingClassifier([(str(model.__class__).split('.')[-1][:-2], model) for model in best_models], \n                                  voting='hard', verbose=1, n_jobs=-1)\nvoting_clf_soft = VotingClassifier([(str(model.__class__).split('.')[-1][:-2], model) for model in best_models], \n                              voting='soft', verbose=1, n_jobs=-1)\n\n\nvoting_clf_hard.fit(x_train, y_train)\nvoting_clf_soft.fit(x_train, y_train)\n\nprediction_hard = voting_clf_hard.predict(test)\nprediction_soft = voting_clf_soft.predict(test)","114c3d3e":"def write_to_submission_file(predictions, PassengerID, out_file='Submission.csv', \n                             columns=['PassengerID', 'Survived']):\n    predicted_df = pd.DataFrame(np.array([PassengerId, predictions]).T, columns=columns).astype(int)\n    predicted_df.to_csv(out_file, index=False)\n    return predicted_df","faf6f24b":"write_to_submission_file(prediction_hard, PassengerId, out_file='Submission_hard.csv')\nwrite_to_submission_file(prediction_soft, PassengerId, out_file='Submission_soft.csv');","e1b3202b":"Now I define models and search for the best params.","94ad789f":"As we may see, there are differences in the features between survived and non-survived people, and the differences are more obvious for various parts of the distribution. Therefore I replaced the features by the binned ones.","77007fbb":"### Labeling and unnecessary features deletion ###\n\nNow we shall make all our features numeric and drop the unnecessary ones.","5f1cb1f4":"### Libraries ###","1927fe39":"# Modeling #\nLet's create params for gridsearch first.","8d4fc2b6":"In the beginning, I should say that the Embarked feature logically seems irrelative, as it refers neither to a person's charachteristics, nor to his location at the board. Besides, empirically it doesn't improve the prediction accuracy. Therefor I just ignore it hereafter.","21dcf83b":"### Age, Fare and Cabin ###","f33bb7c0":"# Introduction #\nThis notebook contains little text as the code is supposed to be clear (I hope so) even for beginners. As it's my first public kernel, any suggestions are very welcome. Should you find the work useful, please vote.","afb9750f":"We see from the chart, that large families have less survival probability","b5114287":"### Family size ###","2c589d16":"We've got missing Age and Fare values. Let's just fill them with the median age. Besides there is little data regarding Cabin, 295 out of 130 entries are filled. Therefore it might be better to just drop this feature.","5efeb1b6":"## Name ##\n\nWe've got another useful feature left - name, and we should take the maximum advantage from it.\n\n### Title ###\nFirstly, let's extract titles.","de8012a0":"Now let's have a look at age and fare distribution.","71cbda91":"Here I combine the models using Voting. I submitted both hard and soft voting, but the hard onw turned to give better score.","9e07fcc4":"As we see, there is most titles are rare, it should be more useful to group them. Note, that Mlle, Mme and Ms might be equaled to Miss and Mrs.","96e4fb5c":"# Data preparation #\n\n## Loading the data ##","b38d72c6":"## Feature analysis and engineering ##","3ae39dc4":"Based on SibSp and Parch features we can easily add another one - family size.","5b2fb482":"# Submission #","68a45afe":"### Survived relatives ###\n\nSecondly, we might assume, that if there are families on board and one family member survived, the probability of other relatives to have survived are higher. We might consider this information from surnames and ticket numbers."}}