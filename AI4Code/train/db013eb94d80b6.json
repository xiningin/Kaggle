{"cell_type":{"5e1e2fb7":"code","2d338ff7":"code","d814debe":"code","0caba5f1":"code","23086fcf":"code","3af1a196":"code","966c53da":"code","318fc0e1":"code","12bbea27":"code","d6c648d8":"code","95f4410b":"code","6cfc115a":"code","3ba7d537":"markdown","7bf0b499":"markdown","f9a71a14":"markdown","ffd7a849":"markdown","25185416":"markdown","3217e858":"markdown","9dbc76c1":"markdown","78c8810a":"markdown","de068934":"markdown","e158fd19":"markdown","ddb0cddc":"markdown","a1d67a4b":"markdown","3bd00d82":"markdown","a879532a":"markdown","f4e7be8f":"markdown","dbcf9d70":"markdown","4f7c5177":"markdown","6fa06fbd":"markdown"},"source":{"5e1e2fb7":"%%time\n!pip install tensorflow==1.15\n!pip install bert-serving-server==1.10.0\n!pip install bert-serving-client==1.10.0\n\n!cp \/kaggle\/input\/biobert-pretrained \/kaggle\/working -r\n%mv \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/model.ckpt-1000000.index \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/bert_model.ckpt.index\n%mv \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/model.ckpt-1000000.data-00000-of-00001 \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/bert_model.ckpt.data-00000-of-00001\n%mv \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/model.ckpt-1000000.meta \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/bert_model.ckpt.meta\n\n!pip install transformers\n!pip install sentence-transformers\n!pip install rake-nltk\n\nprint('installation done')","2d338ff7":"import subprocess\nimport pickle as pkl\nimport pandas as pd\nimport numpy as np \nfrom sentence_transformers import SentenceTransformer\nfrom transformers import BertTokenizer, BertModel\nfrom ipywidgets import interact, widgets # this is what makes the dataframe interactive\nfrom scipy.spatial.distance import cdist\nfrom scipy.spatial.distance import jensenshannon\nfrom IPython.display import HTML, display\nimport matplotlib.pyplot as plt\nfrom os import path\nfrom PIL import Image\nfrom textblob import TextBlob\nimport pyLDAvis.gensim\nimport pyLDAvis\nimport gensim\nimport spacy\nimport os\nfrom scipy import spatial\n\nplt.style.use(\"dark_background\")\n","d814debe":"df = pkl.load(open('..\/input\/bertbiobertdataframe\/BERT-BioBERT-dataframe.pkl', \"rb\"))","0caba5f1":"meta_df=pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv')\nmeta_df = meta_df.dropna(subset=['url'])","23086fcf":"from collections import Counter\nmeta_df = meta_df.dropna(subset=['journal'])\njournals=meta_df['journal'].tolist()\n\ncount = Counter(journals)\nfreq=count.most_common(10)\n\npaper_count=pd.DataFrame(freq,columns=['journals','number of papers'])\npaper_count.sort_values('number of papers', ascending=False).set_index('journals')[:20].sort_values('number of papers', ascending=True).plot(kind='barh')","3af1a196":"import seaborn as sns\n\nheadline_length=df['title'].str.len()\nsns.distplot(headline_length)\nplt.show()\nheadline_length=df['abstract'].str.len()\nsns.distplot(headline_length)\nplt.show()\nheadline_length=df['body_text'].str.len()\nsns.distplot(headline_length)\nplt.show()","966c53da":"df['polarity'] = df['abstract'].map(lambda text: TextBlob(text).sentiment.polarity)\ndf['abstract_len'] = df['abstract'].astype(str).apply(len)\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\ndf['polarity'].iplot(kind='hist',bins=50,xTitle='polarity',linecolor='black',yTitle='count',title='Sentiment Polarity Distribution')\n","318fc0e1":"#Bert\nbert_client = SentenceTransformer('bert-base-nli-max-tokens')\n\n#Biobert\nbio_bert_command = 'bert-serving-start -model_dir \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed -max_seq_len=None -max_batch_size=32 -num_worker=2'\nprocess = subprocess.Popen(bio_bert_command.split(), stdout=subprocess.PIPE)\nfrom bert_serving.client import BertClient\nbiobert_client = BertClient(ignore_all_checks=True)","12bbea27":"bio_vectors = np.array(df.biobert_vector.tolist())\nbert_vectors = np.array(df.bert_vector.tolist())","d6c648d8":"from scipy import spatial\ndef score(model_vectors,model_encode,size):\n    score = []\n    for i in range(size):\n        result = 1 - spatial.distance.cosine(model_vectors[i],model_encode)\n        score.append(result)\n    return score","95f4410b":"def avrage (model1_score,model2_score):\n    l=[sum(n) for n in zip(*[model1_score,model2_score])]\n    final_score = [x * 0.5 for x in l]\n    return final_score ","6cfc115a":"default_question = 'Neonates and pregnant women'\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('max_colwidth', 180)\n\nresults=[]\ntotal_docs=df.shape[0]\n@interact\ndef search_articles(\n    query=default_question,\n    num_results=[10, 25, 100],show_scores=[False, True],score_type=['Cosine']):\n\n    bio_encode =biobert_client.encode([query])\n    bert_encode = bert_client.encode([query])\n   \n    if score_type is 'Cosine':\n        bert_score=score(bert_vectors,bert_encode,total_docs)\n        bio_score=score(bio_vectors,bio_encode,total_docs)\n        f_score=avrage(bert_score,bio_score)\n        df[\"score\"] = f_score\n        select_cols = ['title', 'abstract', 'authors', 'score','url']\n        results = df[select_cols].sort_values(by=['score'], ascending=False).head(num_results)\n        results = results.dropna(subset=['title'])\n        \n#     print(\"results : {}\".format(results[20703\t]))\n    if (len(results.index) == 0):\n        print('NO RESULTS')\n        \n        return None\n    else:\n        \n\n        top_row = results.iloc[0]\n\n        print('TOP RESULT OUT OF ' + str(total_docs) + ' DOCS FOR QUESTION:\\n' + query + '\\n')\n        print('TITLE: ' + str(top_row['title']) + '\\n')\n        print('ABSTRACT: ' + top_row['abstract'] + '\\n')\n        #print('PREDICTED TOPIC: ' + topic_list[int(top_row['best_topic'].replace('topic_', ''))])\n\n        print('\\nAUTHORS: ' + str(top_row['authors']))\n\n        select_cols.remove('authors')\n        \n        return results[select_cols]","3ba7d537":"### 2.Distribution of length for Both Body and Abstract","7bf0b499":"# Methodology for mixer\n*  we use the output vectors from [our notebook](https:\/\/www.kaggle.com\/fatma98\/biobert-bert-encoding), at which we make BERT and BioBERT vectorization model, and here we will avraging the score of BERT model based on BERT ecoding on the paper body_txt and BioBERT model  based on BioBERT encoding on papers title and abstract.","f9a71a14":"# Data load","ffd7a849":"# Methodology for LDA's risk factor tester\n* Here we used LDA model to ask queries about our task -Risk Factors-.\n* Each query is fitted on our main kernel and Pkl files are uploaded here for fast use.","25185416":"# Models","3217e858":"# Imports & Installation","9dbc76c1":"# Cons\n* Running code is very slow for the first time for new dataset.","78c8810a":"# Notebook purpose:\n* At this notebook you can get the nearest articles relevant to your questions about COVID_19.\n* You can get nearest article for each risk factor.\n","de068934":"# Pros. \n1. You can try many models:  Biobert, Bert and LDA.\n   Then by avraging the scores together -as in the main kernel-, you can get more accurate output.\n2. The body is used on some methods and the abstract with the title on others, so as to get as much information as possible when mixing the methods.\n3. The output of all the methods is saved as pkl files, so the user can use it quickly.\n4.  Very interactive GUI.\n5. Visualization of data.\n","e158fd19":"### 3.Visualisation of The distribution of abstract sentiment polarity score","ddb0cddc":"## Cosine function to get models Score:","a1d67a4b":"## Get avrage Score:","3bd00d82":"# Mixer GUI","a879532a":"# Data visualization","f4e7be8f":"***","dbcf9d70":"***","4f7c5177":"1.The following plot show you the top ten journals participated to share COVID_19 papers","6fa06fbd":"### Resources:\n* [BioBERT_BERT Encoding notebook](https:\/\/www.kaggle.com\/jdparsons\/biobert-corex-topic-search)\n* [Topic Modeling notebook](https:\/\/www.kaggle.com\/danielwolffram\/topic-modeling-finding-related-articles)\n* [BERT Word Embeddings Tutorial](https:\/\/l.facebook.com\/l.php?u=https%3A%2F%2Fmccormickml.com%2F2019%2F05%2F14%2FBERT-word-embeddings-tutorial%2F%3Ffbclid%3DIwAR0Tszxw2niNjbWOYvm9K3NV6syx4kP2AsbFvttIUArZxn0sJ_zGEOIaEF4&h=AT3C0KI7RcUlmwdtb-YKUvyBzhdXo9zIjTM3dwBreUm3XmyVyepLMqwTKnzbj_rmoH_FJa1x64is1L11hGHfDHInnidkbHzimnZyh3Zx4Z4vJQueXowbNHBWLnrkq-zo5hyXGw)"}}