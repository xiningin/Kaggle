{"cell_type":{"754cd626":"code","7e1d9573":"code","896f6d00":"code","8cfb6f3b":"code","6cbacd37":"code","d963a55a":"code","1338805b":"code","7351d314":"code","3fa0aa37":"code","6201ce91":"code","870871f6":"code","4238695f":"code","3d641ad3":"code","e9f84827":"code","f14fce82":"code","819e8f57":"code","198f33f8":"code","9f1e3830":"code","4946a19b":"code","46798a80":"code","86f11ace":"code","b308f681":"code","f91b4c9e":"code","adaaaf92":"code","4448d4f4":"code","ba3684ad":"code","9fa7af08":"code","6a261ebd":"code","b69b0b0f":"code","c047b967":"code","5675df72":"code","caa5f4ad":"code","41df3452":"markdown","9c9916e4":"markdown","db0ff259":"markdown","11f29b96":"markdown","41aa7ad7":"markdown","a931947e":"markdown","13ea12b0":"markdown","0fb3e657":"markdown","f67d478e":"markdown","db4e4681":"markdown"},"source":{"754cd626":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7e1d9573":"data = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")","896f6d00":"data.head()","8cfb6f3b":"data.tail()","6cbacd37":"data.info()","d963a55a":"data[\"class\"].value_counts()","1338805b":"data.describe().T","7351d314":"sns.countplot(x=\"class\", data=data, palette=\"PRGn\")\nplt.title(\"Class\")\nplt.show()","3fa0aa37":"# Converting the class variable type from object to integer\ndata[\"class\"] = [1 if each == 'Abnormal' else 0 for each in data[\"class\"]]","6201ce91":"sns.countplot(x=\"class\", data=data, palette=\"PRGn\")\nplt.title(\"Class\")\nplt.show()","870871f6":"sns.heatmap(data.corr(), annot=True, cmap=\"YlGn\", linewidths=0.5, fmt= '.1f')\nplt.show()","4238695f":"plt.figure(figsize=(6,6))\nsns.scatterplot(\"pelvic_incidence\" , y=\"sacral_slope\" , hue=\"class\" , palette=\"RdYlGn\" , data=data);","3d641ad3":"data.boxplot(figsize=(8,6),column='degree_spondylolisthesis',by ='class')\nplt.show()","e9f84827":"data.boxplot(figsize=(8,6),column='pelvic_incidence',by ='class')\nplt.show()","f14fce82":"sns.pairplot(data, hue=\"class\", palette=\"RdYlGn\")\nplt.show()","819e8f57":"x_data = data.drop([\"class\"],axis=1)\ny = data[\"class\"].values","198f33f8":"# Normalization\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nx","9f1e3830":"# Train - Test Split\nfrom sklearn.model_selection import train_test_split\nx_train ,x_test ,y_train ,y_test = train_test_split(x,y,test_size=0.33,random_state=56)","4946a19b":"y_train.size","46798a80":"y_test.size","86f11ace":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)","b308f681":"# Fit\nknn.fit(x_train,y_train)","f91b4c9e":"# Predict\nprediction = knn.predict(x_test) ","adaaaf92":"prediction","4448d4f4":"# prediction Countplot\nsns.countplot(x=prediction, palette=\"summer\")\nplt.title(\"Prediction\")\nplt.show()","ba3684ad":"# y_test Countplot\nsns.countplot(x=y_test, palette=\"summer\")\nplt.title(\"y_test\")\nplt.show()","9fa7af08":"print(\" K = {}, KNN Score: {} \".format(3,knn.score(x_test,y_test)))","6a261ebd":"# Finding the best k value\nneig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train,y_train)\n    prediction_ = knn.predict(x_test)\n    train_accuracy.append(knn.score(x_train, y_train))\n    test_accuracy.append(knn.score(x_test, y_test))\n    \nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","b69b0b0f":"# Plot\nplt.figure(figsize=[10,8])\nplt.plot(neig, test_accuracy, color=\"green\", label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, color=\"orange\", label = 'Training Accuracy')\nplt.legend()\nplt.title('K-Value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()","c047b967":"from sklearn.metrics import confusion_matrix\ncf_matrix = confusion_matrix(y_test,prediction_)\ncf_matrix","5675df72":"sns.heatmap(cf_matrix, annot=True, fmt='.0f', cmap=\"Greens\")\nplt.show()","caa5f4ad":"sns.heatmap(cf_matrix\/np.sum(cf_matrix), annot=True, \n            fmt='.2%', cmap='Greens_r')\nplt.show()","41df3452":"## Prepare Data","9c9916e4":"## KNN Model ","db0ff259":"# Introduction\n\nIn this notebook, we will make a classification using the K-nearest neighbors(KNN) algorithm and the biomechanical features of orthopedic patients.\n\n## \ud83d\udccb **Biomechanical Features** \n\nEach patient is represented in the data set by six biomechanical attributes derived from the shape and orientation of the pelvis and lumbar spine.\n* pelvic incidence\n* pelvic tilt\n* lumbar lordosis angle\n* sacral slope\n* pelvic radius\n* grade of spondylolisthesis","11f29b96":"* From here we can see that some of the normal values are classified as abnormal.","41aa7ad7":"# Understanding Data","a931947e":"## How does KNN work?\n\nThe KNN working can be explained on the basis of the below algorithm:\n\n1. Step: Select the number K of the neighbors\n1. Step: Calculate the Euclidean distance of K number of neighbors\n1. Step: Take the K nearest neighbors as per the calculated Euclidean distance.\n1. Step: Among these K neighbors, count the number of the data points in each category.\n1. Step: Assign the new data points to that category for which the number of the neighbor is maximum.\n1. Step: Our model is ready.\n\n![image.png](attachment:image.png)\n","13ea12b0":"## Why do we need a KNN Algorithm?\n\nSuppose there are two categories, i.e., Category A and Category B, and we have a new data point x1, so this data point will lie in which of these categories. To solve this type of problem, we need a KNN algorithm. With the help of KNN, we can easily identify the category or class of a particular dataset. Consider the below diagram:\n\n![image.png](attachment:image.png)","0fb3e657":"# K-Nearest Neighbors(KNN)\n\n![image.png](attachment:image.png)\n\n* K-Nearest Neighbours is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n* K-NN algorithm assumes the similarity between the new case\/data and available cases and put the new case into the category that is most similar to the available categories.\n\n","f67d478e":"# Prediction Using KNN","db4e4681":"# Visualization"}}