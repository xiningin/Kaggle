{"cell_type":{"77ea44d9":"code","b44ade93":"code","ca921fd9":"code","4c26c101":"code","bd4d2bb0":"code","3e16a14f":"code","25503e89":"code","1c2f23ac":"code","74b9396a":"code","31fc6acf":"code","2ea3e275":"code","e0b9c376":"code","2049723f":"code","a98e1984":"code","d2a47f15":"code","2566e791":"code","529bed9e":"code","017d945b":"code","a3b092ff":"code","e91e19ef":"code","9a518d16":"code","d5d9ad72":"code","4d4303ce":"code","306828d0":"code","ff78baae":"code","bb05104e":"code","88fa47e9":"code","5ff4ee22":"markdown","f42883b9":"markdown","6ba89b7b":"markdown","5702a8ec":"markdown","e4fd2a2a":"markdown","86ff695b":"markdown"},"source":{"77ea44d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b44ade93":"!git clone https:\/\/github.com\/parth1620\/Project-NST.git","ca921fd9":"import tensorflow as tf\nfrom tensorflow.keras.applications import vgg19\nimport PIL","4c26c101":"vgg=vgg19.VGG19(include_top=False,weights='imagenet')\nvgg.trainable=False","bd4d2bb0":"vgg.summary()","3e16a14f":"#Set all training parameters as false as we want to keep layer weights freezed\nfor parameter in vgg.layers:\n  print(parameter.name)\n  parameter.trainable=False\nparameter.name","25503e89":"image=PIL.Image.open('.\/Project-NST\/content10.jpg').convert('RGB')\nprint(image.size[0])\nif max(image.size)>500:\n    size=max_size\nelse:\n  size=max(image.size)\nprint(size)","1c2f23ac":"import PIL\nimport numpy as np\n#Now writing an image pre-processor function\n\ndef pre_processor(path):\n  image=PIL.Image.open(fp=path)\n  image=image.convert('RGB')\n  image=tf.keras.preprocessing.image.img_to_array(image)\n  input_image=tf.keras.applications.vgg19.preprocess_input(image)\n  input_image=np.expand_dims(input_image,axis=0)\n  return input_image","74b9396a":"content_p=pre_processor(path='.\/Project-NST\/content10.jpg')\nstyle_p=pre_processor(path='.\/Project-NST\/style12.jpg')\n\n#Moving content and style tensor to GPU\n\n\nprint(\"Content image shape \",content_p.shape)\nprint(\"Style image shape \",style_p.shape)\n\nprint(\"Content Image Data Type: \",content_p.dtype)\nprint(\"Style Image Data Type: \",style_p.dtype)","31fc6acf":"denormalizer=[103.939,116.779,123.68]\ndef decompresser(image):\n  image[0,:,:,0]+=denormalizer[0]\n  image[0,:,:,1]+=denormalizer[1]\n  image[0,:,:,2]+=denormalizer[2]\n  \n  img=np.clip(image,0,255).astype('int32')\n  img=np.squeeze(img,axis=0)\n\n  return img","2ea3e275":"content_d=decompresser(content_p)\nstyle_d=decompresser(style_p)\n\nprint(\"Shape of Decompressed content \",content_d.shape)\nprint(\"Shape of Decompressed_style \",style_d.shape)","e0b9c376":"import matplotlib.pyplot as plt\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nax1.imshow(content_d)\nax2.imshow(style_d)","2049723f":"style_layers={\n      'block1_conv1',\n      'block2_conv1',\n      'block3_conv1',\n      'block4_conv1',\n             #content feature\n      'block5_conv1'\n  }","a98e1984":"#Now we will find the features for content and style images and here is the function for that\ndef feature_extractor(content_image,style_image,model=vgg):\n  Features_style={}\n  Features_content={}\n  style_models=[tf.keras.Model(inputs=model.inputs,\n                                outputs=model.get_layer(layer).output) for layer in style_layers]\n  con_model=tf.keras.Model(inputs=model.input,outputs=model.get_layer('block4_conv2').output)\n  i=0\n  for style_model in style_models:\n    style_f=style_model(style_image)\n    Features_style[i]=style_f\n    i=i+1\n  con_f=con_model(content_image)\n\n  return Features_style,con_f","d2a47f15":"#Now see what content and style feature we get\nstyle_f,content_f=feature_extractor(content_p,style_p)","2566e791":"#Now finding the gram matrix\ndef gram_matrix(tensor):\n  b,h,w,c=tensor.shape\n  image=tf.reshape(tensor=tensor,shape=(h*w*b,c))\n  gram=tf.matmul(image,image,transpose_a=True)\n  return gram\/tf.cast(h*w*b,dtype=tf.float32)\n\n","529bed9e":"style_g={layer: gram_matrix(style_f[layer]) for layer in style_f}","017d945b":"style_g","a3b092ff":"#Now we will define two types of loss content and Loss function \n#Here content layer are 4_2 so loss will be according to that .\ndef content_loss(target_conv4_2,content_conv4_2):\n  loss=tf.reduce_mean((content_conv4_2-target_conv4_2)**2)\n  return loss","e91e19ef":"#Style weights are concerned with all other layers except 4_2 here target image \nstyle_weights=[\n     1.0,#'block1_conv1\n    0.75, #'block2_conv1\n    0.2, #'block3_conv1\n    0.2,    #'block4_conv1\n    0.2  #'block5_conv1\n]","9a518d16":"def style_loss(style_weights,target_features,style_gram):\n  loss=0.0\n  for layer in range(len(style_weights)):\n    target_f=target_features[layer]\n    target_g=gram_matrix(target_f)\n    style_g=style_gram[layer]\n    b,c,h,w=target_f.shape\n    layer_loss=style_weights[layer]*tf.reduce_mean((target_g-style_g)**2)\n    loss=loss+layer_loss\/(c*h*w)\n  return loss","d5d9ad72":"target=tf.Variable(content_p,trainable=True)\ntarget_s_feature,target_features=feature_extractor(target,target,vgg)\nprint(\"Content Loss : \",content_loss(target_conv4_2=target_features,content_conv4_2=content_f))\nprint(\"Style Loss : \",style_loss(style_weights,target_s_feature,style_g))","4d4303ce":"optimizer=tf.optimizers.Adam(learning_rate=2.1)\n\nalpha=1 #Content Reconstruction weight\nbeta=1e5\n\nepochs=4000\n\nsteps=500","306828d0":"def total_loss(s_loss,c_loss):\n  total_loss=alpha*c_loss+beta*s_loss\n  return total_loss","ff78baae":"results=[]\nfor i in range(epochs):\n  with tf.GradientTape() as tape:\n    target_s_feature,target_feature=feature_extractor(target,target,vgg)\n    c_loss=content_loss(target_conv4_2=target_feature,content_conv4_2=content_f)\n    s_loss=style_loss(style_weights,target_s_feature,style_g)\n    t_loss=total_loss(s_loss,c_loss)\n  grads=tape.gradient(t_loss,target)\n  optimizer.apply_gradients([(grads,target)])\n\n  if i%steps==0:\n    print(\"Total loss at epoch \",i,\" is :\",t_loss)\n    results.append(decompresser(target.numpy()))","bb05104e":"plt.figure(figsize=(20,20))\nfor i in range(len(results)):\n  plt.subplot(4,2,i+1)\n  plt.imshow(results[i])\nplt.show()","88fa47e9":"target_copy=decompresser(target.numpy())\ncontent_copy=decompresser(content_p)\n\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,20))\nax1.imshow(target_copy)\nax2.imshow(content_copy)","5ff4ee22":"# Loading VGG-19 Model","f42883b9":"* # Defining The Feature Extractor","6ba89b7b":"# Defining Style Loss Style loss=style_weights_for_layer*Squared Mean of Difference between style and Target Gram Matrix","5702a8ec":"# Training The Model With Adam Optimizer","e4fd2a2a":"# Preprocessing Image for VGG-19 Model","86ff695b":"# Defining Content Loss: Squared Mean(Content_layerloss-Target Layer Loss)"}}