{"cell_type":{"a1a6a1d4":"code","b9823325":"code","114c6c1e":"code","3353e1de":"code","591c85d7":"code","04da60f6":"code","6b1dc60f":"code","89db2e9b":"code","adae8ded":"code","7195b81f":"code","aa6b2488":"code","3b204407":"code","d107f233":"code","eba2f06a":"code","8fe1df9a":"code","4275118f":"code","c4d85f37":"code","cbab596d":"code","23d763a8":"code","d131fb33":"code","d74168be":"code","ca35d8f1":"code","5949581d":"code","ce21905a":"code","81798142":"code","fdab881f":"code","a10a01ff":"code","c8d6de93":"code","22100a66":"code","76bab5f8":"code","885eae23":"code","e77083b0":"code","4ba322e4":"code","babd3bdb":"code","d048d0fb":"code","a1eb5192":"code","528dc650":"code","df9a243e":"code","033aedc9":"code","0303c2ef":"code","65e2e328":"code","c7935d50":"code","6b08efd5":"code","a3e243bb":"code","58f85473":"code","b56b3840":"code","00078117":"code","0f8d0e70":"code","771a1747":"code","9b27992a":"code","8ebb21ba":"code","89f33188":"code","d205ab86":"code","04b4d188":"code","20f6a27f":"code","be5feec4":"code","81ce9535":"code","925e6c90":"code","016dade0":"code","e1c901b9":"code","301f9a07":"code","02f96ef5":"code","1979cfd6":"code","685e950f":"code","489877d1":"code","f4a90174":"code","6819f3a6":"code","ca6fdc40":"code","834c87a3":"code","e189f58a":"code","dd1edab3":"code","95632ffd":"code","fa12c08b":"code","85bc4c88":"code","fc5a94b3":"code","7ad1c351":"code","9c7556e3":"code","4af0a67c":"code","dca7ca42":"code","647a8811":"code","e617336f":"code","9606adc5":"code","2e9f17fa":"code","8c22086f":"code","d7d0a3c8":"code","6b888bb5":"code","389ad8c2":"code","1da60b72":"code","3b32d6ad":"code","c7814efc":"code","c615c721":"code","51c215bb":"code","a9a4a6f9":"code","e11e471a":"code","6f6cb3a5":"markdown","e129ac60":"markdown","073cdca5":"markdown","877b67fd":"markdown","46f3660d":"markdown","3b3c6540":"markdown","f2941110":"markdown","6237448d":"markdown","ac3f6c7e":"markdown","cf56985d":"markdown","ad851195":"markdown","dea8253f":"markdown","b7685280":"markdown","6257fa95":"markdown","a9899c3b":"markdown","94b52000":"markdown","fdc6e106":"markdown","dafc52aa":"markdown","18366fb5":"markdown","1556476b":"markdown","576deaf6":"markdown","a9de1d42":"markdown","4caba9fa":"markdown","30c51e1d":"markdown","111d500e":"markdown","daa115d9":"markdown","d7325fc9":"markdown","4d88d695":"markdown","1d3cd320":"markdown","5a6027d9":"markdown","d48845de":"markdown","67c96717":"markdown","19ce2f04":"markdown","238f63c3":"markdown","16b91e94":"markdown","5b295e8a":"markdown","7cafe410":"markdown","45551144":"markdown","51064811":"markdown","911b6172":"markdown","2a9c5b0b":"markdown","a1fc9f9f":"markdown","a42b737b":"markdown","c3606d76":"markdown","271c4092":"markdown","59b1128b":"markdown","a7387e65":"markdown","ca1550c0":"markdown","c170614e":"markdown","3c0879f4":"markdown","ffbe1809":"markdown","83abfee5":"markdown","0ec65782":"markdown","d2fe89cd":"markdown","793d7f69":"markdown","d24df25a":"markdown","4efad1b3":"markdown","01ea0faf":"markdown","fc339414":"markdown","f2a8cb45":"markdown","5a540aa7":"markdown","d82cea73":"markdown","93472f53":"markdown","542d45fb":"markdown","31278eb9":"markdown","1638b1bc":"markdown","b46520a0":"markdown","2563245a":"markdown","522566f6":"markdown","ff4b21e4":"markdown","6fee0359":"markdown","ef1b7cc3":"markdown","f5b2d201":"markdown","320e7069":"markdown","f5206ecb":"markdown","53dec0bd":"markdown","fdf7c013":"markdown","7ccc7818":"markdown","ed6d2bb5":"markdown","f1b6f3df":"markdown","d326909e":"markdown","22f1ee4c":"markdown","8cde9e92":"markdown","cf9843fe":"markdown","786c19ce":"markdown","fe032267":"markdown","f2e15e77":"markdown","14b46201":"markdown","a1f265e8":"markdown","7c23775c":"markdown","2eca961f":"markdown"},"source":{"a1a6a1d4":"# Python libraries\n# Classic,data manipulation and linear algebra\nimport pandas as pd\nimport numpy as np\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport squarify\n\n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix,  roc_curve, precision_recall_curve, accuracy_score, roc_auc_score\nimport lightgbm as lgbm\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_curve,auc\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_predict\nfrom yellowbrick.classifier import DiscriminationThreshold\n\n# Stats\nimport scipy.stats as ss\nfrom scipy import interp\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\n# Time\nfrom contextlib import contextmanager\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\n#ignore warning messages \nimport warnings\nwarnings.filterwarnings('ignore') ","b9823325":"data = pd.read_csv('..\/input\/diabetes.csv')","114c6c1e":"display(data.info(),data.head())","3353e1de":"# 2 datasets\nD = data[(data['Outcome'] != 0)]\nH = data[(data['Outcome'] == 0)]\n\n#------------COUNT-----------------------\ndef target_count():\n    trace = go.Bar( x = data['Outcome'].value_counts().values.tolist(), \n                    y = ['healthy','diabetic' ], \n                    orientation = 'h', \n                    text=data['Outcome'].value_counts().values.tolist(), \n                    textfont=dict(size=15),\n                    textposition = 'auto',\n                    opacity = 0.8,marker=dict(\n                    color=['lightskyblue', 'gold'],\n                    line=dict(color='#000000',width=1.5)))\n\n    layout = dict(title =  'Count of Outcome variable')\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)\n\n#------------PERCENTAGE-------------------\ndef target_percent():\n    trace = go.Pie(labels = ['healthy','diabetic'], values = data['Outcome'].value_counts(), \n                   textfont=dict(size=15), opacity = 0.8,\n                   marker=dict(colors=['lightskyblue', 'gold'], \n                               line=dict(color='#000000', width=1.5)))\n\n\n    layout = dict(title =  'Distribution of Outcome variable')\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)","591c85d7":"target_count()\ntarget_percent()","04da60f6":"data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)","6b1dc60f":"# Define missing plot to detect all missing values in dataset\ndef missing_plot(dataset, key) :\n    null_feat = pd.DataFrame(len(dataset[key]) - dataset.isnull().sum(), columns = ['Count'])\n    percentage_null = pd.DataFrame((len(dataset[key]) - (len(dataset[key]) - dataset.isnull().sum()))\/len(dataset[key])*100, columns = ['Count'])\n    percentage_null = percentage_null.round(2)\n\n    trace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, text = percentage_null['Count'],  textposition = 'auto',marker=dict(color = '#7EC0EE',\n            line=dict(color='#000000',width=1.5)))\n\n    layout = dict(title =  \"Missing Values (count & %)\")\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)\n    ","89db2e9b":"# Plotting \nmissing_plot(data, 'Outcome')","adae8ded":"plt.style.use('ggplot') # Using ggplot2 style visuals \n\nf, ax = plt.subplots(figsize=(11, 15))\n\nax.set_facecolor('#fafafa')\nax.set(xlim=(-.05, 200))\nplt.ylabel('Variables')\nplt.title(\"Overview Data Set\")\nax = sns.boxplot(data = data, \n  orient = 'h', \n  palette = 'Set2')","7195b81f":"def correlation_plot():\n    #correlation\n    correlation = data.corr()\n    #tick labels\n    matrix_cols = correlation.columns.tolist()\n    #convert to array\n    corr_array  = np.array(correlation)\n    trace = go.Heatmap(z = corr_array,\n                       x = matrix_cols,\n                       y = matrix_cols,\n                       colorscale='Viridis',\n                       colorbar   = dict() ,\n                      )\n    layout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                            #autosize = False,\n                            #height  = 1400,\n                            #width   = 1600,\n                            margin  = dict(r = 0 ,l = 100,\n                                           t = 0,b = 100,\n                                         ),\n                            yaxis   = dict(tickfont = dict(size = 9)),\n                            xaxis   = dict(tickfont = dict(size = 9)),\n                           )\n                      )\n    fig = go.Figure(data = [trace],layout = layout)\n    py.iplot(fig)","aa6b2488":"correlation_plot()","3b204407":"def median_target(var):   \n    temp = data[data[var].notnull()]\n    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n    return temp","d107f233":"def plot_distribution(data_select, size_bin) :  \n    # 2 datasets\n    tmp1 = D[data_select]\n    tmp2 = H[data_select]\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['diabetic', 'healthy']\n    colors = ['#FFD700', '#7EC0EE']\n\n    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = size_bin, curve_type='kde')\n    \n    fig['layout'].update(title = data_select)\n\n    py.iplot(fig, filename = 'Density plot')","eba2f06a":"plot_distribution('Insulin', 0)","8fe1df9a":"median_target('Insulin')","4275118f":"data.loc[(data['Outcome'] == 0 ) & (data['Insulin'].isnull()), 'Insulin'] = 102.5\ndata.loc[(data['Outcome'] == 1 ) & (data['Insulin'].isnull()), 'Insulin'] = 169.5","c4d85f37":"plot_distribution('Glucose', 0)","cbab596d":"median_target('Glucose')","23d763a8":"data.loc[(data['Outcome'] == 0 ) & (data['Glucose'].isnull()), 'Glucose'] = 107\ndata.loc[(data['Outcome'] == 1 ) & (data['Glucose'].isnull()), 'Glucose'] = 140","d131fb33":"plot_distribution('SkinThickness', 10)","d74168be":"median_target('SkinThickness')","ca35d8f1":"data.loc[(data['Outcome'] == 0 ) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 27\ndata.loc[(data['Outcome'] == 1 ) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 32","5949581d":"plot_distribution('BloodPressure', 5)","ce21905a":"median_target('BloodPressure')","81798142":"data.loc[(data['Outcome'] == 0 ) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 70\ndata.loc[(data['Outcome'] == 1 ) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 74.5","fdab881f":"plot_distribution('BMI', 0)","a10a01ff":"median_target('BMI')","c8d6de93":"data.loc[(data['Outcome'] == 0 ) & (data['BMI'].isnull()), 'BMI'] = 30.1\ndata.loc[(data['Outcome'] == 1 ) & (data['BMI'].isnull()), 'BMI'] = 34.3","22100a66":"#plot distribution \nplot_distribution('Age', 0)\nplot_distribution('Pregnancies', 0)\nplot_distribution('DiabetesPedigreeFunction', 0)","76bab5f8":"missing_plot(data, 'Outcome')","885eae23":"def plot_feat1_feat2(feat1, feat2) :  \n    D = data[(data['Outcome'] != 0)]\n    H = data[(data['Outcome'] == 0)]\n    trace0 = go.Scatter(\n        x = D[feat1],\n        y = D[feat2],\n        name = 'diabetic',\n        mode = 'markers', \n        marker = dict(color = '#FFD700',\n            line = dict(\n                width = 1)))\n\n    trace1 = go.Scatter(\n        x = H[feat1],\n        y = H[feat2],\n        name = 'healthy',\n        mode = 'markers',\n        marker = dict(color = '#7EC0EE',\n            line = dict(\n                width = 1)))\n\n    layout = dict(title = feat1 +\" \"+\"vs\"+\" \"+ feat2,\n                  yaxis = dict(title = feat2,zeroline = False),\n                  xaxis = dict(title = feat1, zeroline = False)\n                 )\n\n    plots = [trace0, trace1]\n\n    fig = dict(data = plots, layout=layout)\n    py.iplot(fig)","e77083b0":"def barplot(var_select, sub) :\n    tmp1 = data[(data['Outcome'] != 0)]\n    tmp2 = data[(data['Outcome'] == 0)]\n    tmp3 = pd.DataFrame(pd.crosstab(data[var_select],data['Outcome']), )\n    tmp3['% diabetic'] = tmp3[1] \/ (tmp3[1] + tmp3[0]) * 100\n\n    color=['lightskyblue','gold' ]\n    trace1 = go.Bar(\n        x=tmp1[var_select].value_counts().keys().tolist(),\n        y=tmp1[var_select].value_counts().values.tolist(),\n        text=tmp1[var_select].value_counts().values.tolist(),\n        textposition = 'auto',\n        name='diabetic',opacity = 0.8, marker=dict(\n        color='gold',\n        line=dict(color='#000000',width=1)))\n\n    \n    trace2 = go.Bar(\n        x=tmp2[var_select].value_counts().keys().tolist(),\n        y=tmp2[var_select].value_counts().values.tolist(),\n        text=tmp2[var_select].value_counts().values.tolist(),\n        textposition = 'auto',\n        name='healthy', opacity = 0.8, marker=dict(\n        color='lightskyblue',\n        line=dict(color='#000000',width=1)))\n    \n    trace3 =  go.Scatter(   \n        x=tmp3.index,\n        y=tmp3['% diabetic'],\n        yaxis = 'y2',\n        name='% diabetic', opacity = 0.6, marker=dict(\n        color='black',\n        line=dict(color='#000000',width=0.5\n        )))\n\n    layout = dict(title =  str(var_select)+' '+(sub),\n              xaxis=dict(), \n              yaxis=dict(title= 'Count'), \n              yaxis2=dict(range= [-0, 75], \n                          overlaying= 'y', \n                          anchor= 'x', \n                          side= 'right',\n                          zeroline=False,\n                          showgrid= False, \n                          title= '% diabetic'\n                         ))\n\n    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n    py.iplot(fig)","4ba322e4":"# Define pie plot to visualize each variable repartition vs target modalities : Survived or Died (train)\ndef plot_pie(var_select, sub) :\n    D = data[(data['Outcome'] != 0)]\n    H = data[(data['Outcome'] == 0)]\n    \n    col =['Silver', 'mediumturquoise','#CF5C36','lightblue','magenta', '#FF5D73','#F2D7EE','mediumturquoise']\n    \n    trace1 = go.Pie(values  = D[var_select].value_counts().values.tolist(),\n                    labels  = D[var_select].value_counts().keys().tolist(),\n                    textfont=dict(size=15), opacity = 0.8,\n                    hole = 0.5, \n                    hoverinfo = \"label+percent+name\",\n                    domain  = dict(x = [.0,.48]),\n                    name    = \"Diabetic\",\n                    marker  = dict(colors = col, line = dict(width = 1.5)))\n    trace2 = go.Pie(values  = H[var_select].value_counts().values.tolist(),\n                    labels  = H[var_select].value_counts().keys().tolist(),\n                    textfont=dict(size=15), opacity = 0.8,\n                    hole = 0.5,\n                    hoverinfo = \"label+percent+name\",\n                    marker  = dict(line = dict(width = 1.5)),\n                    domain  = dict(x = [.52,1]),\n                    name    = \"Healthy\" )\n\n    layout = go.Layout(dict(title = var_select + \" distribution by target <br>\"+(sub),\n                            annotations = [ dict(text = \"Diabetic\"+\" : \"+\"268\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .22, y = -0.1),\n                                            dict(text = \"Healthy\"+\" : \"+\"500\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .8,y = -.1)]))\n                                          \n\n    fig  = go.Figure(data = [trace1,trace2],layout = layout)\n    py.iplot(fig)","babd3bdb":"plot_feat1_feat2('Glucose','Age')","d048d0fb":"palette ={0 : 'lightblue', 1 : 'gold'}\nedgecolor = 'black'\n\nfig = plt.figure(figsize=(12,8))\n\nax1 = sns.scatterplot(x = data['Glucose'], y = data['Age'], hue = \"Outcome\",\n                    data = data, palette = palette, edgecolor=edgecolor)\n\nplt.annotate('N1', size=25, color='black', xy=(80, 30), xytext=(60, 35),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\nplt.plot([50, 120], [30, 30], linewidth=2, color = 'red')\nplt.plot([120, 120], [20, 30], linewidth=2, color = 'red')\nplt.plot([50, 120], [20, 20], linewidth=2, color = 'red')\nplt.plot([50, 50], [20, 30], linewidth=2, color = 'red')\nplt.title('Glucose vs Age')\nplt.show()","a1eb5192":"data.loc[:,'N1']=0\ndata.loc[(data['Age']<=30) & (data['Glucose']<=120),'N1']=1","528dc650":"barplot('N1', ':Glucose <= 120 and Age <= 30')","df9a243e":"plot_pie('N1', '(Glucose <= 120 and Age <= 30)')","033aedc9":"data.loc[:,'N2']=0\ndata.loc[(data['BMI']<=30),'N2']=1","0303c2ef":"barplot('N2', ': BMI <= 30')","65e2e328":"plot_pie('N2', 'BMI <= 30')","c7935d50":"plot_feat1_feat2('Pregnancies','Age')","6b08efd5":"palette ={0 : 'lightblue', 1 : 'gold'}\nedgecolor = 'black'\n\nfig = plt.figure(figsize=(12,8))\n\nax1 = sns.scatterplot(x = data['Pregnancies'], y = data['Age'], hue = \"Outcome\",\n                    data = data, palette = palette, edgecolor=edgecolor)\n\nplt.annotate('N3', size=25, color='black', xy=(6, 25), xytext=(10, 25),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\nplt.plot([0, 6], [30, 30], linewidth=2, color = 'red')\nplt.plot([6, 6], [20, 30], linewidth=2, color = 'red')\nplt.plot([0, 6], [20, 20], linewidth=2, color = 'red')\nplt.plot([0, 0], [20, 30], linewidth=2, color = 'red')\nplt.title('Pregnancies vs Age')\nplt.show()","a3e243bb":"data.loc[:,'N3']=0\ndata.loc[(data['Age']<=30) & (data['Pregnancies']<=6),'N3']=1","58f85473":"barplot('N3', ': Age <= 30 and Pregnancies <= 6')","b56b3840":"plot_pie('N3', 'Age <= 30 and Pregnancies <= 6')","00078117":"plot_feat1_feat2('Glucose','BloodPressure')","0f8d0e70":"palette ={0 : 'lightblue', 1 : 'gold'}\nedgecolor = 'black'\n\nfig = plt.figure(figsize=(12,8))\n\nax1 = sns.scatterplot(x = data['Glucose'], y = data['BloodPressure'], hue = \"Outcome\",\n                    data = data, palette = palette, edgecolor=edgecolor)\n\nplt.annotate('N4', size=25, color='black', xy=(70, 80), xytext=(50, 110),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\nplt.plot([40, 105], [80, 80], linewidth=2, color = 'red')\nplt.plot([40, 40], [20, 80], linewidth=2, color = 'red')\nplt.plot([40, 105], [20, 20], linewidth=2, color = 'red')\nplt.plot([105, 105], [20, 80], linewidth=2, color = 'red')\nplt.title('Glucose vs BloodPressure')\nplt.show()","771a1747":"data.loc[:,'N4']=0\ndata.loc[(data['Glucose']<=105) & (data['BloodPressure']<=80),'N4']=1","9b27992a":"barplot('N4', ': Glucose <= 105 and BloodPressure <= 80')","8ebb21ba":"plot_pie('N4', 'Glucose <= 105 and BloodPressure <= 80')","89f33188":"data.loc[:,'N5']=0\ndata.loc[(data['SkinThickness']<=20) ,'N5']=1","d205ab86":"barplot('N5', ':SkinThickness <= 20')","04b4d188":"plot_pie('N5', 'SkinThickness <= 20')","20f6a27f":"plot_feat1_feat2('SkinThickness','BMI')","be5feec4":"data.loc[:,'N6']=0\ndata.loc[(data['BMI']<30) & (data['SkinThickness']<=20),'N6']=1","81ce9535":"palette ={0 : 'lightblue', 1 : 'gold'}\nedgecolor = 'black'\n\nfig = plt.figure(figsize=(12,8))\n\nax1 = sns.scatterplot(x = data['SkinThickness'], y = data['BMI'], hue = \"Outcome\",\n                    data = data, palette = palette, edgecolor=edgecolor)\n\nplt.annotate('N6', size=25, color='black', xy=(20, 20), xytext=(50, 25),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\nplt.plot([0, 20], [30, 30], linewidth=2, color = 'red')\nplt.plot([0, 0], [16, 30], linewidth=2, color = 'red')\nplt.plot([0, 20], [16, 16], linewidth=2, color = 'red')\nplt.plot([20, 20], [16, 30], linewidth=2, color = 'red')\nplt.title('SkinThickness vs BMI')\nplt.show()","925e6c90":"barplot('N6', ': BMI < 30 and SkinThickness <= 20')","016dade0":"plot_pie('N6', 'BMI < 30 and SkinThickness <= 20')","e1c901b9":"plot_feat1_feat2('Glucose','BMI')","301f9a07":"palette ={0 : 'lightblue', 1 : 'gold'}\nedgecolor = 'black'\n\nfig = plt.figure(figsize=(12,8))\n\nax1 = sns.scatterplot(x = data['Glucose'], y = data['BMI'], hue = \"Outcome\",\n                    data = data, palette = palette, edgecolor=edgecolor)\n\nplt.annotate('N7', size=25, color='black', xy=(70, 35), xytext=(40, 60),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\nplt.plot([105, 105], [16, 30], linewidth=2, color = 'red')\nplt.plot([40, 40], [16, 30], linewidth=2, color = 'red')\nplt.plot([40, 105], [16, 16], linewidth=2, color = 'red')\nplt.plot([40, 105], [30, 30], linewidth=2, color = 'red')\nplt.title('Glucose vs BMI')\nplt.show()","02f96ef5":"data.loc[:,'N7']=0\ndata.loc[(data['Glucose']<=105) & (data['BMI']<=30),'N7']=1","1979cfd6":"barplot('N7', ': Glucose <= 105 and BMI <= 30')","685e950f":"plot_pie('N7', 'Glucose <= 105 and BMI <= 30')","489877d1":"plot_distribution('Insulin', 0)","f4a90174":"data.loc[:,'N9']=0\ndata.loc[(data['Insulin']<200),'N9']=1","6819f3a6":"barplot('N9', ': Insulin < 200')","ca6fdc40":"plot_pie('N9', 'Insulin < 200')","834c87a3":"data.loc[:,'N10']=0\ndata.loc[(data['BloodPressure']<80),'N10']=1","e189f58a":"barplot('N10', ': BloodPressure < 80')","dd1edab3":"plot_pie('N10', 'BloodPressure < 80')","95632ffd":"plot_distribution('Pregnancies', 0)","fa12c08b":"data.loc[:,'N11']=0\ndata.loc[(data['Pregnancies']<4) & (data['Pregnancies']!=0) ,'N11']=1","85bc4c88":"barplot('N11', ': Pregnancies > 0 and < 4')","fc5a94b3":"plot_pie('N11', 'Pregnancies > 0 and < 4')","7ad1c351":"data['N0'] = data['BMI'] * data['SkinThickness']\n\ndata['N8'] =  data['Pregnancies'] \/ data['Age']\n\ndata['N13'] = data['Glucose'] \/ data['DiabetesPedigreeFunction']\n\ndata['N12'] = data['Age'] * data['DiabetesPedigreeFunction']\n\ndata['N14'] = data['Age'] \/ data['Insulin']\n","9c7556e3":"D = data[(data['Outcome'] != 0)]\nH = data[(data['Outcome'] == 0)]","4af0a67c":"plot_distribution('N0', 0)","dca7ca42":"data.loc[:,'N15']=0\ndata.loc[(data['N0']<1034) ,'N15']=1","647a8811":"barplot('N15', ': N0 < 1034')","e617336f":"plot_pie('N15', 'N0 < 1034')","9606adc5":"target_col = [\"Outcome\"]\ncat_cols   = data.nunique()[data.nunique() < 12].keys().tolist()\ncat_cols   = [x for x in cat_cols ]\n#numerical columns\nnum_cols   = [x for x in data.columns if x not in cat_cols + target_col]\n#Binary columns with 2 values\nbin_cols   = data.nunique()[data.nunique() == 2].keys().tolist()\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n\n#Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    data[i] = le.fit_transform(data[i])\n    \n#Duplicating columns for multi value columns\ndata = pd.get_dummies(data = data,columns = multi_cols )\n\n#Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(data[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\n\n#dropping original values merging scaled values for numerical columns\ndf_data_og = data.copy()\ndata = data.drop(columns = num_cols,axis = 1)\ndata = data.merge(scaled,left_index=True,right_index=True,how = \"left\")","2e9f17fa":"def correlation_plot():\n    #correlation\n    correlation = data.corr()\n    #tick labels\n    matrix_cols = correlation.columns.tolist()\n    #convert to array\n    corr_array  = np.array(correlation)\n    trace = go.Heatmap(z = corr_array,\n                       x = matrix_cols,\n                       y = matrix_cols,\n                       colorscale='Viridis',\n                       colorbar   = dict() ,\n                      )\n    layout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                            #autosize = False,\n                            #height  = 1400,\n                            #width   = 1600,\n                            margin  = dict(r = 0 ,l = 100,\n                                           t = 0,b = 100,\n                                         ),\n                            yaxis   = dict(tickfont = dict(size = 9)),\n                            xaxis   = dict(tickfont = dict(size = 9)),\n                           )\n                      )\n    fig = go.Figure(data = [trace],layout = layout)\n    py.iplot(fig)\n","8c22086f":"correlation_plot()","d7d0a3c8":"# Def X and Y\nX = data.drop('Outcome', 1)\ny = data['Outcome']","6b888bb5":"def model_performance(model, subtitle) :   \n    #Kfold\n    cv = KFold(n_splits=5,shuffle=False, random_state = 42)\n    y_real = []\n    y_proba = []\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0,1,100)\n    i = 1\n    \n    for train,test in cv.split(X,y):\n        model.fit(X.iloc[train], y.iloc[train])\n        pred_proba = model.predict_proba(X.iloc[test])\n        precision, recall, _ = precision_recall_curve(y.iloc[test], pred_proba[:,1])\n        y_real.append(y.iloc[test])\n        y_proba.append(pred_proba[:,1])\n        fpr, tpr, t = roc_curve(y[test], pred_proba[:, 1])\n        tprs.append(interp(mean_fpr, fpr, tpr))\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc) \n    \n    # Confusion matrix\n    y_pred = cross_val_predict(model, X, y, cv=5)\n    conf_matrix = confusion_matrix(y, y_pred)\n    trace1 = go.Heatmap(z = conf_matrix  ,x = [\"0 (pred)\",\"1 (pred)\"],\n                        y = [\"0 (true)\",\"1 (true)\"],xgap = 2, ygap = 2, \n                        colorscale = 'Viridis', showscale  = False)\n    \n    #Show metrics\n    tp = conf_matrix[1,1]\n    fn = conf_matrix[1,0]\n    fp = conf_matrix[0,1]\n    tn = conf_matrix[0,0]\n    Accuracy  =  ((tp+tn)\/(tp+tn+fp+fn))\n    Precision =  (tp\/(tp+fp))\n    Recall    =  (tp\/(tp+fn))\n    F1_score  =  (2*(((tp\/(tp+fp))*(tp\/(tp+fn)))\/((tp\/(tp+fp))+(tp\/(tp+fn)))))\n\n    show_metrics = pd.DataFrame(data=[[Accuracy , Precision, Recall, F1_score]])\n    show_metrics = show_metrics.T\n\n    colors = ['gold', 'lightgreen', 'lightcoral', 'lightskyblue']\n    trace2 = go.Bar(x = (show_metrics[0].values), \n                    y = ['Accuracy', 'Precision', 'Recall', 'F1_score'], text = np.round_(show_metrics[0].values,4),\n                    textposition = 'auto', textfont=dict(color='black'),\n                    orientation = 'h', opacity = 1, marker=dict(\n            color=colors,\n            line=dict(color='#000000',width=1.5)))\n\n    #Roc curve\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_auc = auc(mean_fpr, mean_tpr)\n\n    trace3 = go.Scatter(x=mean_fpr, y=mean_tpr,\n                        name = \"Roc : \" ,\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2), fill='tozeroy')\n    trace4 = go.Scatter(x = [0,1],y = [0,1],\n                        line = dict(color = ('black'),width = 1.5,\n                        dash = 'dot'))\n    \n    #Precision - recall curve\n    y_real = y\n    y_proba = np.concatenate(y_proba)\n    precision, recall, _ = precision_recall_curve(y_real, y_proba)\n\n    trace5 = go.Scatter(x = recall, y = precision,\n                        name = \"Precision\" + str(precision),\n                        line = dict(color = ('lightcoral'),width = 2), fill='tozeroy')\n    \n    mean_auc=round(mean_auc,3)\n    #Subplots\n    fig = tls.make_subplots(rows=2, cols=2, print_grid=False,\n                          specs=[[{}, {}], \n                                 [{}, {}]],\n                          subplot_titles=('Confusion Matrix',\n                                          'Metrics',\n                                          'ROC curve'+\" \"+ '('+ str(mean_auc)+')',\n                                          'Precision - Recall curve',\n                                          ))\n    #Trace and layout\n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    fig.append_trace(trace3,2,1)\n    fig.append_trace(trace4,2,1)\n    fig.append_trace(trace5,2,2)\n    \n    fig['layout'].update(showlegend = False, title = '<b>Model performance report (5 folds)<\/b><br>'+subtitle,\n                        autosize = False, height = 830, width = 830,\n                        plot_bgcolor = 'black',\n                        paper_bgcolor = 'black',\n                        margin = dict(b = 195), font=dict(color='white'))\n    fig[\"layout\"][\"xaxis1\"].update(color = 'white')\n    fig[\"layout\"][\"yaxis1\"].update(color = 'white')\n    fig[\"layout\"][\"xaxis2\"].update((dict(range=[0, 1], color = 'white')))\n    fig[\"layout\"][\"yaxis2\"].update(color = 'white')\n    fig[\"layout\"][\"xaxis3\"].update(dict(title = \"false positive rate\"), color = 'white')\n    fig[\"layout\"][\"yaxis3\"].update(dict(title = \"true positive rate\"),color = 'white')\n    fig[\"layout\"][\"xaxis4\"].update(dict(title = \"recall\"), range = [0,1.05],color = 'white')\n    fig[\"layout\"][\"yaxis4\"].update(dict(title = \"precision\"), range = [0,1.05],color = 'white')\n    for i in fig['layout']['annotations']:\n        i['font'] = titlefont=dict(color='white', size = 14)\n    py.iplot(fig)","389ad8c2":"def scores_table(model, subtitle):\n    scores = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n    res = []\n    for sc in scores:\n        scores = cross_val_score(model, X, y, cv = 5, scoring = sc)\n        res.append(scores)\n    df = pd.DataFrame(res).T\n    df.loc['mean'] = df.mean()\n    df.loc['std'] = df.std()\n    df= df.rename(columns={0: 'accuracy', 1:'precision', 2:'recall',3:'f1',4:'roc_auc'})\n\n    trace = go.Table(\n        header=dict(values=['<b>Fold', '<b>Accuracy', '<b>Precision', '<b>Recall', '<b>F1 score', '<b>Roc auc'],\n                    line = dict(color='#7D7F80'),\n                    fill = dict(color='#a1c3d1'),\n                    align = ['center'],\n                    font = dict(size = 15)),\n        cells=dict(values=[('1','2','3','4','5','mean', 'std'),\n                           np.round(df['accuracy'],3),\n                           np.round(df['precision'],3),\n                           np.round(df['recall'],3),\n                           np.round(df['f1'],3),\n                           np.round(df['roc_auc'],3)],\n                   line = dict(color='#7D7F80'),\n                   fill = dict(color='#EDFAFF'),\n                   align = ['center'], font = dict(size = 15)))\n\n    layout = dict(width=800, height=400, title = '<b>Cross Validation - 5 folds<\/b><br>'+subtitle, font = dict(size = 15))\n    fig = dict(data=[trace], layout=layout)\n\n    py.iplot(fig, filename = 'styled_table')","1da60b72":"random_state=42\n\nfit_params = {\"early_stopping_rounds\" : 100, \n             \"eval_metric\" : 'auc', \n             \"eval_set\" : [(X,y)],\n             'eval_names': ['valid'],\n             'verbose': 0,\n             'categorical_feature': 'auto'}\n\nparam_test = {'learning_rate' : [0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4],\n              'n_estimators' : [100, 200, 300, 400, 500, 600, 800, 1000, 1500, 2000],\n              'num_leaves': sp_randint(6, 50), \n              'min_child_samples': sp_randint(100, 500), \n              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n              'subsample': sp_uniform(loc=0.2, scale=0.8), \n              'max_depth': [-1, 1, 2, 3, 4, 5, 6, 7],\n              'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n              'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n              'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n\n#number of combinations\nn_iter = 300\n\n#intialize lgbm and lunch the search\nlgbm_clf = lgbm.LGBMClassifier(random_state=random_state, silent=True, metric='None', n_jobs=4)\ngrid_search = RandomizedSearchCV(\n    estimator=lgbm_clf, param_distributions=param_test, \n    n_iter=n_iter,\n    scoring='accuracy',\n    cv=5,\n    refit=True,\n    random_state=random_state,\n    verbose=True)\n\ngrid_search.fit(X, y, **fit_params)\nopt_parameters =  grid_search.best_params_\nlgbm_clf = lgbm.LGBMClassifier(**opt_parameters)","3b32d6ad":"model_performance(lgbm_clf, 'LightGBM')\nscores_table(lgbm_clf, 'LightGBM')","c7814efc":"visualizer = DiscriminationThreshold(lgbm_clf)\n\nvisualizer.fit(X, y)  \nvisualizer.poof() ","c615c721":"knn_clf = KNeighborsClassifier()\n\nvoting_clf = VotingClassifier(estimators=[ \n    ('lgbm_clf', lgbm_clf),\n    ('knn', KNeighborsClassifier())], voting='soft', weights = [1,1])\n\nparams = {\n      'knn__n_neighbors': np.arange(1,30)\n      }\n      \ngrid = GridSearchCV(estimator=voting_clf, param_grid=params, cv=5)\n\ngrid.fit(X,y)\n\nprint(\"Best Score:\" + str(grid.best_score_))\nprint(\"Best Parameters: \" + str(grid.best_params_))","51c215bb":"knn_clf = KNeighborsClassifier(n_neighbors = 25)\n\nvoting_clf = VotingClassifier (\n        estimators = [('knn', knn_clf), ('lgbm', lgbm_clf)],\n                     voting='soft', weights = [1,1])","a9a4a6f9":"model_performance(voting_clf, 'LightGBM & KNN')\nscores_table(voting_clf, 'LightGBM & KNN')","e11e471a":"visualizer = DiscriminationThreshold(voting_clf)\n\nvisualizer.fit(X, y)  \nvisualizer.poof()  ","6f6cb3a5":"Loading dataset with pandas (pd)","e129ac60":"* **Pregnancies and Age**","073cdca5":"* **Pregnancies**","877b67fd":"* ** SkinThickness** : Triceps skin fold thickness (mm)","46f3660d":"* **Glucose** : Plasma glucose concentration a 2 hours in an oral glucose tolerance test","3b3c6540":"- <a href='#1'>1. Load libraries and read the data<\/a>  \n\n    - <a href='#1.1'>1.1. Load libraries<\/a> \n    - <a href='#1.2'>1.2. Read the data<\/a> \n    \n- <a href='#2'>2. Overview<\/a> \n\n    - <a href='#2.1'>2.1. Head<\/a> \n    - <a href='#2.2'>2.2. Target<\/a> \n    - <a href='#2.3'>2.3. Missing values<\/a> \n    \n- <a href='#3'>3. Replace missing values and EDA<\/a>\n\n    - <a href='#3.1'>3.1. Insulin<\/a> \n    - <a href='#3.2'>3.2. Glucose<\/a> \n    - <a href='#3.3'>3.3. SkinThickness<\/a>\n    - <a href='#3.4'>3.4. BloodPressure<\/a>\n    - <a href='#3.5'>3.5. BMI<\/a>\n    \n- <a href='#4'>4. New features (16) and EDA<\/a>\n\n- <a href='#5'>5. Prepare dataset<\/a> \n    - <a href='#5.1'>5.1. StandardScaler and LabelEncoder<\/a> \n    - <a href='#5.2'>5.2. Correlation Matrix<\/a>\n    - <a href='#5.3'>5.3. X and y<\/a>\n    - <a href='#5.4'>5.4. Model Performance<\/a>\n\t- <a href='#5.4'>5.5. Scores Table<\/a>\n    \n- <a href='#6'>6.Machine Learning<\/a> \n\n    - <a href='#6.1'>6.1. RandomSearch + LightGBM - Accuracy = 89.8%<\/a> \n    - <a href='#6.2'>6.2. LightGBM - Discrimination Threshold<\/a>\n    - <a href='#6.3'>6.3. GridSearch + LightGBM & KNN- Accuracy = 90.6%<\/a>\n    - <a href='#6.4'>6.4. LightGBM & KNN - Discrimination Threshold<\/a>\n    \n- <a href='#7'>7. Credits<\/a> ","f2941110":"Missing values : \n* Insulin = 48.7% - 374\n* SkinThickness = 29.56% - 227\n* BloodPressure = 4.56% - 35\n* BMI = 1.43% - 11\n* Glucose = 0.65% - 5","6237448d":"# <a id='6'>6. Machine Learning<\/a> ","ac3f6c7e":"# <a id='4'>4. New features (16) and EDA<\/a> ","cf56985d":"Healthy persons are concentrate with an blood pressure <= 80 and glucose <= 105","ad851195":"## <a id='6.4'>6.4. LightGBM & KNN  - Discrimination Threshold<\/a> ","dea8253f":"A **correlation matrix** is a table showing correlation coefficients between sets of variables. Each random variable (Xi) in the table is correlated with each of the other values in the table (Xj). This allows you to see which pairs have the highest correlation.","b7685280":"To replace missing values, we'll use median by target (Outcome)","6257fa95":"## <a id='2.1'>2.1. Head<\/a> ","a9899c3b":"# What is diabetes ? \nAcccording to NIH, \"**Diabetes** is a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat. Insulin, a hormone made by the pancreas, helps glucose from food get into your cells to be used for energy. Sometimes your body doesn\u2019t make enough\u2014or any\u2014insulin or doesn\u2019t use insulin well. Glucose then stays in your blood and doesn\u2019t reach your cells.\n\nOver time, having too much glucose in your blood can cause health problems. Although diabetes has no cure, you can take steps to manage your diabetes and stay healthy.\n\nSometimes people call diabetes \u201ca touch of sugar\u201d or \u201cborderline diabetes.\u201d These terms suggest that someone doesn\u2019t really have diabetes or has a less serious case, but every case of diabetes is serious.\n\n**What are the different types of diabetes?**\nThe most common types of diabetes are type 1, type 2, and gestational diabetes.\n\n**Type 1 diabetes**\nIf you have type 1 diabetes, your body does not make insulin. Your immune system attacks and destroys the cells in your pancreas that make insulin. Type 1 diabetes is usually diagnosed in children and young adults, although it can appear at any age. People with type 1 diabetes need to take insulin every day to stay alive.\n\n**Type 2 diabetes**\nIf you have type 2 diabetes, your body does not make or use insulin well. You can develop type 2 diabetes at any age, even during childhood. However, this type of diabetes occurs most often in middle-aged and older people. Type 2 is the most common type of diabetes.\n\n**Gestational diabetes**\nGestational diabetes develops in some women when they are pregnant. Most of the time, this type of diabetes goes away after the baby is born. However, if you\u2019ve had gestational diabetes, you have a greater chance of developing type 2 diabetes later in life. Sometimes diabetes diagnosed during pregnancy is actually type 2 diabetes.\n\n**Other types of diabetes**\nLess common types include monogenic diabetes, which is an inherited form of diabetes, and cystic fibrosis-related diabetes .\"","94b52000":"* **SkinThickness and BMI**","fdc6e106":"* **BloodPressure**","dafc52aa":"## Thank you all ! Merci \u00e0 tous ! :)","18366fb5":"Healthy persons are concentrate with an age <= 30 and glucose <= 120","1556476b":"![](http:\/\/image.noelshack.com\/fichiers\/2019\/06\/1\/1549295827-0-0000.png)","576deaf6":"27 for a healthy person and 32 for a diabetic person","a9de1d42":"* **LightGBM : Hyperparameters ** :\n\n    * learning_rate : This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates\n    * n_estimators : number of trees (or rounds)\n    * num_leaves : number of leaves in full tree, default: 31\n    * min_child_samples : minimal number of data in one leaf. Can be used to deal with over-fitting\n    * min_child_weight : minimal sum hessian in one leaf.\n    * subsample : randomly select part of data without resampling\n    * max_depth : It describes the maximum depth of tree. This parameter is used to handle model overfitting.\n    * colsample_bytree : LightGBM will randomly select part of features on each iteration if colsample_bytree smaller than 1.0. For example, if you set it to 0.8, LightGBM will select 80% of features before training each tree\n    * reg_alpha : regularization\n    * reg_lambda : regularization\n    \n    * early_stopping_rounds : This parameter can help you speed up your analysis. Model will stop training if one metric of one validation data doesn\u2019t improve in last early_stopping_round rounds. This will reduce excessive iterations","4caba9fa":"Now, we can compute correlation matrix ","30c51e1d":"## <a id='3.3'>3.3. SkinThickness<\/a> ","111d500e":"* **Insulin** : 2-Hour serum insulin (mu U\/ml)","daa115d9":"## <a id='6.1'>6.1. RandomSearch + LightGBM - Accuracy = 89.8%<\/a> ","d7325fc9":"* **SkinThickness**","4d88d695":"Insulin's medians by the target are really different ! 102.5 for a healthy person and 169.5 for a diabetic person","1d3cd320":"# <a id='2'>2. Overview<\/a> ","5a6027d9":"The datasets consist of several medical predictor (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.","d48845de":"Checking data head and info","67c96717":"**To fill these Nan values the data distribution needs to be understood against the target**. ","19ce2f04":"We can complete model performance report with a table contain all results by fold","238f63c3":"107 for a healthy person and 140 for a diabetic person","16b91e94":"![](http:\/\/image.noelshack.com\/fichiers\/2019\/06\/1\/1549285917-0000000000000000000.png)\n\n** LightGBM** is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n* Faster training speed and higher efficiency.\n* Lower memory usage.\n* Better accuracy.\n* Support of parallel and GPU learning.\n* Capable of handling large-scale data.","5b295e8a":"* ** Did you watch Inception ? ** Here is the same! It's not a dream in a dream but a new feature extract from a new feature","7cafe410":"## <a id='1.1'>1.1. Load libraries<\/a> ","45551144":"* ** BloodPressure** : Diastolic blood pressure (mm Hg)","51064811":"Bellow we define a stylized report with Plotly","911b6172":"## <a id='6.3'>6.3. GridSearch + LightGBM & KNN- Accuracy = 90.6%<\/a> ","2a9c5b0b":"The above graph shows that the data is unbalanced. The number of non-diabetic is 268 the number of diabetic patients is 500","a1fc9f9f":"## <a id='5.5'>5.5. Scores Tables<\/a> ","a42b737b":"![](http:\/\/image.noelshack.com\/fichiers\/2019\/06\/1\/1549276143-0-0-0.png)","c3606d76":"* **BMI **","271c4092":"Here, we define 3 plots functions","59b1128b":"## <a id='2.2'>2.2 Target<\/a> ","a7387e65":"We define X and y :","ca1550c0":"## <a id='3.2'>3.2. Glucose<\/a> ","c170614e":"* **Glucose and BMI**","3c0879f4":"* ** StandardScaler** :\n\nStandardize features by removing the mean and scaling to unit variance : \n\n![](http:\/\/image.noelshack.com\/fichiers\/2019\/06\/1\/1549282952-0-0-0-0-0-0.png)\n\nCentering and scaling happen independently on each feature by computing the relevant statistics on the samples in the set. Mean and standard deviation are then stored to be used on later data using the transform method.\n\nStandardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n\n* ** LabelEncoder** : Encode labels with value between 0 and n_classes-1.\n\nBellow we encode the data to feed properly to our algorithm","ffbe1809":"Loading the libraries","83abfee5":"With GridSearch CV we search the best \"n_neighbors\" to optimize accuracy of Voting Classifier","0ec65782":"To measure the performance of a model, we need several elements :\n\nThis part is essential\n\n* **Confusion matrix** : also known as the error matrix, allows visualization of the performance of an algorithm :\n\n    * true positive (TP) : Diabetic correctly identified as diabetic\n    * true negative (TN) : Healthy correctly identified as healthy\n    * false positive (FP) : Healthy incorrectly identified as diabetic\n    * false negative (FN) : Diabetic incorrectly identified as healthy\n\n![](https:\/\/image.noelshack.com\/fichiers\/2018\/20\/5\/1526651914-cs-heezweaa5hp7.jpg)\n\n* **Metrics ** :\n\n    * Accuracy : (TP +TN) \/ (TP + TN + FP +FN)\n    * Precision : TP \/ (TP + FP)\n    * Recall : TP \/ (TP + FN)\n    * F1 score : 2 x ((Precision x Recall) \/ (Precision + Recall))\n\n* **Roc Curve** : The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n\n![](http:\/\/image.noelshack.com\/fichiers\/2019\/06\/1\/1549284841-0-0-0-0-0-0-0-0-0-0.png)\n\n* **Precision Recall Curve** :  shows the tradeoff between precision and recall for different threshold","d2fe89cd":"We obtain a really good result but we can beat 90% with adding a KNeighborsClassifier to LightGBM (Voting Classifier)\n\n* **KNeighborsClassifier** : KNeighborsClassifier implements learning based on the k nearest neighbors of each query point, where  k is an integer value specified by the user.\n\n* **VotingClassifier** : VotingClassifier is a meta-classifier for combining similar or conceptually different machine learning classifiers for classification via majority or plurality voting","793d7f69":"To find the best hyperparameters, we'll use Random Search CV.\n\nRandom search is a technique where random combinations of the hyperparameters are used to find the best solution for the built model. \nGenerally RS is more faster and accurate than GridSearchCV who calculate all possible combinations. With Random Grid we specify the number of combinations that we want","d24df25a":"* **Age** : Age (years)\n* **DiabetesPedigreeFunction** : Diabetes pedigree function\n* **Pregnancies** : Number of times pregnant","4efad1b3":"# <a id='5'>5. Prepare dataset<\/a> ","01ea0faf":"What's target's distribution ? ","fc339414":"## <a id='6.2'>6.2. LightGBM - Discrimination Threshold<\/a> ","f2a8cb45":"* **BMI** : Body mass index (weight in kg\/(height in m)^2)","5a540aa7":"Healthy persons are concentrate with a BMI < 30 and skin thickness <= 20","d82cea73":"Hello All !!\n\nThis notebook is a guide to end to end a complete study in machine learning with different concepts like :\n\n* Completing missing values (most important part)\n* Exploratory data analysis\n* Creating new features (to increase accuracy)\n* Encoding features\n* Using LightGBM and optimize hyperparameters\n* Adding a KNN to LGBM to beat 90% accuracy (voting classifier)\n\n# If you like this notebook, feel free to upvote","93472f53":"# <a id='7'>7. Credits<\/a> ","542d45fb":"## <a id='5.1'>5.1. StandardScaler and LabelEncoder<\/a> ","31278eb9":"* **Others**","1638b1bc":"## <a id='5.4'>5.4. Model Performance<\/a> ","b46520a0":"According to wikipedia \"The body mass index (BMI) or Quetelet index is a value derived from the mass (weight) and height of an individual. The BMI is defined as the body mass divided by the square of the body height, and is universally expressed in units of kg\/m2, resulting from mass in kilograms and height in metres.\"\n\n30 kg\/m\u00b2 is the limit to obesity","2563245a":"# Who is Pima Indians ?\n\n\"The Pima (or Akimel O'odham, also spelled Akimel O'otham, \"River People\", formerly known as Pima) are a group of Native Americans living in an area consisting of what is now central and southern Arizona. The majority population of the surviving two bands of the Akimel O'odham are based in two reservations: the Keli Akimel O'otham on the Gila River Indian Community (GRIC) and the On'k Akimel O'odham on the Salt River Pima-Maricopa Indian Community (SRPMIC).\" Wikipedia\n\n![](http:\/\/image.noelshack.com\/fichiers\/2019\/06\/1\/1549305367-0-0-0-0-pima.jpg)","522566f6":"## <a id='1.2'>1.2. Read data<\/a> ","ff4b21e4":"## <a id='3.5'>3.5. BMI<\/a> ","6fee0359":"## <a id='3.4'>3.4. BloodPressure<\/a> ","ef1b7cc3":"* **Glucose and BloodPressure**","f5b2d201":"* **Discrimination Threshold** :\nA visualization of precision, recall, f1 score, and queue rate with respect to the discrimination threshold of a binary classifier. The discrimination threshold is the probability or score at which the positive class is chosen over the negative class","320e7069":"# <a id='1'>1. Load libraries and read the data<\/a> ","f5206ecb":"## <a id='2.3'>2.3. Missing values<\/a> ","53dec0bd":"A **correlation matrix** is a table showing correlation coefficients between sets of variables. Each random variable (Xi) in the table is correlated with each of the other values in the table (Xj). This allows you to see which pairs have the highest correlation.","fdf7c013":"## <a id='3.1'>3.1. Insulin<\/a> ","7ccc7818":"With n_neighbors = 25, the accuracy increase to 90.625 ! Bellow the model performance report ","ed6d2bb5":"All features are complete ! \nNow, we can create new features","f1b6f3df":"OK, all missing values are encoded with NaN value","d326909e":"## <a id='5.2'>5.2. Correlation Matrix<\/a> ","22f1ee4c":"## <a id='5.3'>5.3. X and y<\/a> ","8cde9e92":"\nBelow, you can see the accuracy of LGBM with replacement of the NaN values by the variable's mean (same results with the median)","cf9843fe":"* **Glucose and Age**","786c19ce":"* **Insulin**","fe032267":"Now, we can look at where are missing values : ","f2e15e77":"To train and test our algorithm we'll use cross validation K-Fold\n\n![](http:\/\/image.noelshack.com\/fichiers\/2019\/06\/1\/1549288372-00001.png)\n\nIn k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k \u2212 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once.","14b46201":"Credits  : \n* https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n* https:\/\/en.wikipedia.org\/wiki\/Body_mass_index\n* http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/EnsembleVoteClassifier\/\n* https:\/\/www.news-medical.net\/health\/What-is-Diabetes.aspx\n* https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html\n* http:\/\/ogrisel.github.io\/scikit-learn.org\/sklearn-tutorial\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\n* https:\/\/www.scikit-yb.org\/en\/latest\/api\/classifier\/threshold.html\n* https:\/\/www.analyticsindiamag.com\/why-is-random-search-better-than-grid-search-for-machine-learning\/\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html\n* https:\/\/scikit-learn.org\/stable\/modules\/preprocessing_targets.html#preprocessing-targets\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n* https:\/\/twitter.com\/bearda24\n* https:\/\/www.slideshare.net\/DhianaDevaRocha\/qcon-rio-machine-learning-for-everyone\n* https:\/\/medium.com\/@sebastiannorena\/some-model-tuning-methods-bfef3e6544f0\n* https:\/\/www.niddk.nih.gov\/health-information\/diabetes\/overview\/what-is-diabetes\n* https:\/\/en.wikipedia.org\/wiki\/Pima_people","a1f265e8":"# <a id='3'>3. Replace missing values and EDA<\/a> ","7c23775c":"We saw on data.head() that some features contain 0, it doesn't make sense here and this indicates missing value\nBelow we replace 0 value by NaN :","2eca961f":"----------\n**Pima Indians Diabetes - EDA & Prediction (0.906)**\n=====================================\n\n* **Accuracy - 5 Folds - LightGBM : 89.8%**\n* **Accuracy - 5 Folds - LightGBM & KNN : 90.6%**\n\n\n***Vincent Lugat***\n\n*February 2019*\n\n----------"}}