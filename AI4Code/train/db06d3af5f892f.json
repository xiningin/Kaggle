{"cell_type":{"a1f10157":"code","0cc1d561":"code","1a5d7e54":"code","3aca651d":"code","421e56dc":"code","420c60ce":"code","b9c88957":"code","79a862f7":"code","8c4c7410":"code","3a12d418":"code","28585c9b":"code","1f18aa38":"code","6e2a0d30":"code","4c1df760":"code","d0858c89":"code","3366949e":"code","6c0cc658":"code","592cb419":"code","c63144b8":"code","3758f4e6":"code","21b630f3":"code","7f057dc2":"code","9734952b":"code","a3cd0030":"code","78fc89de":"code","faf5dc4f":"code","f8dd7d9b":"code","b6bbb7ea":"code","33f86c9c":"code","fc4924b8":"code","67b28c42":"code","4b337bc1":"code","9a9e3fd3":"code","500403b8":"code","45a4eba3":"code","16f3164c":"code","adfa6019":"code","c8c44e39":"code","ce9013b4":"code","808b02c4":"code","f15f5481":"code","423d0075":"code","ada7726c":"code","b9cfa11d":"code","d35ce468":"code","05eeae84":"code","141c3c6f":"code","679da783":"code","45289b10":"code","3215ab34":"markdown","b21a3cfc":"markdown","87d227bf":"markdown","c8682103":"markdown"},"source":{"a1f10157":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0cc1d561":"train_data=pd.read_csv('..\/input\/train.csv')\ntest_data=pd.read_csv('..\/input\/test.csv')\nsub_sample=pd.read_csv('..\/input\/sample_submission.csv')","1a5d7e54":"train_data.head(10)","3aca651d":"train_data.shape","421e56dc":"test_data.head()","420c60ce":"test_data.shape","b9c88957":"sub_sample.head()","79a862f7":"Y_train = train_data[\"label\"]\nY_train = np.array(Y_train, np.uint8)","8c4c7410":"X_train = train_data.drop(labels = [\"label\"],axis = 1) \nX_train = np.array(X_train)\nX_test=np.array(test_data)","3a12d418":"print(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nY_train","28585c9b":"# free memory space\ndel train_data","1f18aa38":"#Convert train datset to (num_images, img_rows, img_cols) format \ntrain_images = X_train.reshape(X_train.shape[0], 28, 28)","6e2a0d30":"def plot_images(images, classes):\n    assert len(images) == len(classes) == 9\n    \n    # Create figure with 3x3 sub-plots.\n    fig, axes = plt.subplots(3, 3,figsize=(28,28),sharex=True)\n    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n   \n    for i, ax in enumerate(axes.flat):\n        # Plot image.\n        \n        ax.imshow(images[i], cmap=plt.get_cmap('gray'))    \n        xlabel = \"the number is: {0}\".format(classes[i])\n    \n        # Show the classes as the label on the x-axis.\n        ax.set_xlabel(xlabel)\n        ax.xaxis.label.set_size(28)\n        # Remove ticks from the plot.\n        ax.set_xticks([])\n        ax.set_yticks([])\n    \n    # Ensure the plot is shown correctly with multiple plots\n    # in a single Notebook cell.\n    \n    plt.show()","4c1df760":"random_numbers = [randint(0, len(train_images)) for p in range(0,9)]\nimages_to_show = [train_images[i] for i in random_numbers]\nclasses_to_show = [Y_train[i] for i in random_numbers]\nprint(\"Images to show: {0}\".format(len(images_to_show)))\nprint(\"Classes to show: {0}\".format(len(classes_to_show)))\n#plot the images\nplot_images(images_to_show, classes_to_show)","d0858c89":"from keras.utils.np_utils import to_categorical\n\nY_train= to_categorical(Y_train)\n","3366949e":"Y_train","6c0cc658":"Y_train.shape","592cb419":"#Splitting the train_images into the Training set and validation set\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, Y_train, Y_val= train_test_split(X_train, Y_train,\n               test_size=0.1, random_state=42,stratify=Y_train)","c63144b8":"print(X_train.shape)\nprint(Y_train.shape)\nprint(X_val.shape)\nprint(Y_val.shape)\nprint(X_test.shape)","3758f4e6":"X_train = X_train.astype('float32')\/255\nX_val=X_val.astype('float32')\/255\nX_test = X_test.astype('float32')\/255","21b630f3":"from keras.models import Sequential\nfrom keras.layers import Dense ,Dropout\nfrom keras.optimizers import SGD , RMSprop,Adam\nfrom keras import regularizers\nfrom keras.callbacks import LearningRateScheduler,ReduceLROnPlateau,ModelCheckpoint,EarlyStopping","7f057dc2":"def lr_schedule(epoch):\n    lrate = 0.001\n    if epoch > 50:\n        lrate = 0.0003\n    if epoch > 75:\n        lrate = 0.00003\n    elif epoch > 100:\n        lrate = 0.000003       \n    return lrate","9734952b":"lr_scheduler=LearningRateScheduler(lr_schedule)","a3cd0030":"#we can reduce the LR by half if the accuracy is not improved after 3 epochs.using the following code\nreduceOnPlateau = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=5, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001, mode='auto')","78fc89de":"#Save the model after every decrease in val_loss \ncheckpoint = ModelCheckpoint(filepath='bestmodel.hdf5', verbose=0,monitor='val_loss',save_best_only=True,save_weights_only=False)","faf5dc4f":"#Stop training when a monitored quantity has stopped improving.\nearlyStopping=EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')","f8dd7d9b":"callbacks_list = [lr_scheduler,checkpoint]","b6bbb7ea":"model = Sequential()\nmodel.add(Dense(128, activation='relu', input_shape=(784,),kernel_regularizer=regularizers.l2(0.01)))\n#model.add(Dropout(0.2))\nmodel.add(Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n#model.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n#model.add(Dropout(0.2))\nmodel.add(Dense(10, activation='softmax'))","33f86c9c":"model.summary()","fc4924b8":"keras.utils.plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='LR', expand_nested=False, dpi=96)","67b28c42":"sgd = SGD(lr=0.000002, decay=1e-6, momentum=0.9)\nrmsprop = RMSprop(lr=0.001 ,decay=1e-4)\nadam= Adam(lr=0.0003 ,decay=1e-4)\n\nmodel.compile(optimizer=rmsprop,\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])","4b337bc1":"H1= model.fit(X_train, Y_train, batch_size = 256, epochs = 100,callbacks=callbacks_list, \n              validation_data = (X_val, Y_val), verbose = 1)","9a9e3fd3":"plt.figure(0)\nplt.plot(H1.history['acc'],'r')\nplt.plot(H1.history['val_acc'],'g')\nplt.xticks(np.arange(0, 101, 5.0))\nplt.rcParams['figure.figsize'] = (14, 8)\nplt.xlabel(\"Num of Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Training Accuracy vs Validation Accuracy\")\nplt.legend(['train','validation'])","500403b8":"plt.figure(1)\nplt.plot(H1.history['loss'],'r')\nplt.plot(H1.history['val_loss'],'g')\nplt.xticks(np.arange(0, 101, 5.0))\nplt.rcParams['figure.figsize'] = (14, 8)\nplt.xlabel(\"Num of Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss vs Validation Loss\")\nplt.legend(['train','validation'])","45a4eba3":"score = model.evaluate(X_val, Y_val, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","16f3164c":"from sklearn.metrics import classification_report\n\npreds = model.predict_classes(X_val)\ny_lable = [y.argmax() for y in Y_val]\nprint(classification_report(y_lable,preds))\npreds1 = model.predict_classes(X_train)\nytr_lable = [y.argmax() for y in Y_train]\nprint(classification_report(ytr_lable,preds1))","adfa6019":"# predict results\nTest_perdect = model.predict(X_test)\n\n# select the indix with the maximum probability\nTest_perdect = np.argmax(Test_perdect,axis = 1)\n\nTest_perdect = pd.Series(Test_perdect,name=\"Label\")\n\nsubmission1 = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),Test_perdect],axis = 1)\n\nsubmission1.to_csv(\"submission1.csv\",index=False)","c8c44e39":"keras.backend.clear_session() ## clear the previous model. ","ce9013b4":"model2 = Sequential()\nmodel2.add(Dense(256, activation='relu', input_shape=(784,)))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(128, activation='relu'))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(64, activation='relu'))\nmodel2.add(Dropout(0.3))\nmodel2.add(Dense(10, activation='softmax'))","808b02c4":"model2.summary()","f15f5481":"#Save the model after every decrease in val_loss \ncheckpoint = ModelCheckpoint(filepath='bestmodel2.hdf5', verbose=0,monitor='val_loss',save_best_only=True,save_weights_only=False)","423d0075":"callbacks_list = [reduceOnPlateau,checkpoint]","ada7726c":"adam= Adam(lr=0.001 ,decay=1e-4)\nmodel2.compile(optimizer=adam,\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])","b9cfa11d":"H2= model2.fit(X_train, Y_train, batch_size = 256, epochs = 100, callbacks=callbacks_list, \n              validation_data = (X_val, Y_val), verbose = 1)","d35ce468":"plt.figure(0)\nplt.plot(H2.history['acc'],'r')\nplt.plot(H2.history['val_acc'],'g')\nplt.xticks(np.arange(0, 101, 5.0))\nplt.rcParams['figure.figsize'] = (14, 8)\nplt.xlabel(\"Num of Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Training Accuracy vs Validation Accuracy\")\nplt.legend(['train','validation'])","05eeae84":"plt.figure(1)\nplt.plot(H1.history['loss'],'r')\nplt.plot(H1.history['val_loss'],'g')\nplt.xticks(np.arange(0, 101, 5.0))\nplt.rcParams['figure.figsize'] = (14, 8)\nplt.xlabel(\"Num of Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss vs Validation Loss\")\nplt.legend(['train','validation'])","141c3c6f":"score = model2.evaluate(X_val, Y_val, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","679da783":"preds = model2.predict_classes(X_val)\ny_lable = [y.argmax() for y in Y_val]\nprint(classification_report(y_lable,preds))\npreds1 = model2.predict_classes(X_train)\nytr_lable = [y.argmax() for y in Y_train]\nprint(classification_report(ytr_lable,preds1))","45289b10":"# predict results\nTest_perd = model2.predict(X_test)\n\n# select the indix with the maximum probability\nTest_perd = np.argmax(Test_perd,axis = 1)\n\nTest_perd = pd.Series(Test_perd,name=\"Label\")\n\nsubmission2 = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),Test_perd],axis = 1)\n\nsubmission2.to_csv(\"submission2.csv\",index=False)\n\n","3215ab34":"## Define Model by keras","b21a3cfc":"* ## One Hot encoding \n\n**Encode labels to one hot vectors (ex : 4 ---> [0,0,0,0,1,0,0,0,0,0]   ,    9 ---> [0,0,0,0,0,0,0,0,0,1])**","87d227bf":"## Data Visualization","c8682103":"## Using Dropout"}}