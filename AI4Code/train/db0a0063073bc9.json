{"cell_type":{"a49fe727":"code","4e224a45":"code","9c6dca3e":"code","98da71ef":"code","6848052b":"code","830aac99":"code","d5cb31e2":"code","696539ef":"code","da1522a9":"code","44df9ea1":"code","04b35d54":"code","9e491e5a":"code","e011ec93":"code","5c8180ce":"code","f38329e0":"code","e1db053b":"code","c025ab79":"code","ce050157":"code","173a7a77":"code","e1880206":"code","b57f2645":"code","1cc30d2c":"code","afd216f7":"code","6d000e68":"code","198aa664":"code","1dd96701":"code","20e2b6b6":"code","c8704ee2":"code","8e2d4e6a":"code","de28617d":"code","3b7817a7":"code","9c7b1e6b":"code","eaff0608":"code","b5847d1a":"code","01e32a66":"code","3633b620":"code","2a37e209":"code","13219e1c":"code","c4559ef1":"code","932aa397":"code","c02f1445":"code","d8845d2e":"code","f550b4a4":"code","2ccb53ec":"code","672ba7ae":"code","47f52108":"code","797b3d99":"code","84d7c58c":"code","09fb90e1":"code","4ad1d529":"markdown","92ad4fc3":"markdown","701e6b44":"markdown","2c3ee327":"markdown","56ea37f9":"markdown","8581539c":"markdown","6103eae5":"markdown","b83d5a7f":"markdown","f28aad9e":"markdown","84d55b90":"markdown","32f1034a":"markdown","2ec9aabc":"markdown","ec97d12d":"markdown","a8cc8fba":"markdown","b6822e2c":"markdown"},"source":{"a49fe727":"import pandas as pd\nimport json\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom nltk.stem import PorterStemmer\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler","4e224a45":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9c6dca3e":"reviews = []\nthreshold = 1e6 #1 million reviews\nwith open('..\/input\/yelp-dataset\/yelp_academic_dataset_review.json') as fl:\n    for i, line in enumerate(fl):\n        reviews.append(json.loads(line)) #use 'loads' since line is string\n        if i + 1 >= threshold:\n            break\ndf_reviews = pd.DataFrame(reviews)\n\n# print the first five rows\ndf_reviews.head()","98da71ef":"df_reviews = df_reviews[['text', 'stars']]\ndf_reviews.head()","6848052b":"df_reviews.isna().mean()","830aac99":"stars_count = df_reviews['stars'].value_counts()\nstars_count = stars_count.sort_index()\n\nfig = plt.figure(figsize=(10, 6))\nax = sns.barplot(stars_count.index, stars_count.values)\nplt.title(\"Ratings Distribution\",fontsize = 20)\nplt.ylabel('Number of Reviews', fontsize = 12)\nplt.xlabel('Number of Stars', fontsize = 12);","d5cb31e2":"df_reviews=df_reviews[df_reviews.stars != 3]\n\npd.set_option('mode.chained_assignment', None)\ndf_reviews[\"labels\"] = df_reviews[\"stars\"].apply(lambda x: 1 if x < 3  else 0) # positive as 0 and negative as 1\ndf_reviews = df_reviews.drop(\"stars\",axis=1)\n\ndf_reviews.head()","696539ef":"label_count = df_reviews['labels'].value_counts()\nlabel_count = label_count.sort_index()\n\nfig = plt.figure(figsize=(6, 6))\nax = sns.barplot(label_count.index, label_count.values)\nplt.title(\"Class Distribution\",fontsize = 20)\nplt.ylabel('Number of Reviews', fontsize = 12)\nplt.xlabel('Sentiment', fontsize = 12);","da1522a9":"#baseline\nprint(\"{:.2f}\".format(df_reviews[df_reviews.labels == 0].shape[0]\/df_reviews.shape[0]))","44df9ea1":"train, test = train_test_split(df_reviews, test_size = 0.3, stratify = df_reviews['labels'], random_state = 42)","04b35d54":"punct = set(string.punctuation)","9e491e5a":"def text_prep(text):\n    #clean text\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    \n    #remove non-letters and lower case\n    text = re.sub('[^a-z\\s]', '', text.lower())\n    \n    #remove punctuation        \n    punc_removed = [char for char in text if char not in punct]\n    punc_removed = ''.join(punc_removed)\n    \n    return [word for word in punc_removed.split()]","e011ec93":"start_time = time.time()\ncv= CountVectorizer(binary=True, analyzer = text_prep, min_df = 10, max_df = 0.95)\ncv.fit_transform(train['text'].values)\ntrain_feature_set=cv.transform(train['text'].values)\ntest_feature_set=cv.transform(test['text'].values)\nprint(\"Time takes to convert text input into feature vector: \", round((time.time() - start_time)\/60, 2), \" mins\")","5c8180ce":"train_feature_set.shape[1]","f38329e0":"cv.vocabulary_['tasty']","e1db053b":"list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(23451)]","c025ab79":"y_train = train['labels'].values\ny_test = test['labels'].values","ce050157":"start_time = time.time()\nlr = LogisticRegression(solver = 'liblinear', random_state = 42, max_iter=1000)\nlr.fit(train_feature_set,y_train)\ny_pred = lr.predict(test_feature_set)\nprint(\"Time takes to train model and make predictions: \", round((time.time() - start_time)\/60, 2), \" mins\")","173a7a77":"print(\"Accuracy: \",round(metrics.accuracy_score(y_test,y_pred),3))\nprint(\"F1: \",round(metrics.f1_score(y_test, y_pred),3))","e1880206":"disp = metrics.plot_confusion_matrix(lr, test_feature_set, y_test,\n                                 display_labels=['Class 0', 'Class 1'],\n                                 cmap=plt.cm.Blues,\n                                 normalize='true')\ndisp.ax_.set_title('Logistic Regression Confusion matrix, with normalization');","b57f2645":"feature_importance = lr.coef_[0][:10]\nfor i,v in enumerate(feature_importance):\n    print('Feature: ', list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(i)], 'Score: ', v)","1cc30d2c":"feature_importance = lr.coef_[0]\nsorted_idx = np.argsort(feature_importance)","afd216f7":"top_10_pos_w = [list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(w)] for w in sorted_idx[range(-1,-11, -1)]]\nprint(top_10_pos_w)","6d000e68":"fig = plt.figure(figsize=(10, 6))\nax = sns.barplot(top_10_pos_w, feature_importance[sorted_idx[range(-1,-11, -1)]])\nplt.title(\"Most Important Words Used for Negative Sentiment\",fontsize = 20)\nx_locs,x_labels = plt.xticks()\nplt.setp(x_labels, rotation = 40)\nplt.ylabel('Feature Importance', fontsize = 12)\nplt.xlabel('Word', fontsize = 12);","198aa664":"sub_poi = train.loc[train.text.str.contains('poisoning')]\nround(sub_poi.labels.mean(),3)","1dd96701":"top_10_neg_w = [list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(w)] for w in sorted_idx[:10]]\nprint(top_10_neg_w)","20e2b6b6":"fig = plt.figure(figsize=(10, 6))\nax = sns.barplot(top_10_neg_w, feature_importance[sorted_idx[:10]])\nplt.title(\"Most Important Words Used for Positive Sentiment\",fontsize = 20)\nx_locs,x_labels = plt.xticks()\nplt.setp(x_labels, rotation = 40)\nplt.ylabel('Feature Importance', fontsize = 12)\nplt.xlabel('Word', fontsize = 12);","c8704ee2":"test_review = cv.transform([\"I did not enjoy the food or the service\"])\nlr.predict_proba(test_review)","8e2d4e6a":"pred_proba_df = pd.DataFrame(lr.predict_proba(test_feature_set))\nthreshold_list = [0.3,0.4,0.45,0.5]\nfor i in threshold_list:\n    print ('\\n******** For i = {} ******'.format(i))\n    Y_test_pred = pred_proba_df.applymap(lambda x: 1 if x>i else 0)\n    test_f1 = round(metrics.f1_score(y_test, Y_test_pred.loc[:,1].values),3)\n    print('F1: {}'.format(test_f1))","de28617d":"sw = set(stopwords.words(\"english\"))\nps = PorterStemmer()","3b7817a7":"def text_prep_stop_stem(text):\n    #clean text\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    \n    #remove non-letters and lower case\n    text = re.sub('[^a-z\\s]', '', text.lower())\n    \n    #remove punctuation        \n    punc_removed = [char for char in text if char not in punct]\n    punc_removed = ''.join(punc_removed)\n    \n    #stem and remove stop words\n    return [ps.stem(word) for word in punc_removed.split() if not word in sw]\n    #return [word for word in punc_removed.split() if not word in sw]","9c7b1e6b":"start_time = time.time()\ncv= CountVectorizer(binary=True, analyzer = text_prep_stop_stem, min_df = 10, max_df = 0.95)\ncv.fit_transform(train['text'].values)\ntrain_feature_set=cv.transform(train['text'].values)\ntest_feature_set=cv.transform(test['text'].values)\nprint(\"Time takes to convert text input into feature vector: \", round((time.time() - start_time)\/60, 2), \" mins\")","eaff0608":"train_feature_set.shape[1]","b5847d1a":"start_time = time.time()\nlr = LogisticRegression(solver = 'liblinear', random_state = 42, max_iter=1000)\nlr.fit(train_feature_set,y_train)\ny_pred = lr.predict(test_feature_set)\nprint(\"Time takes to train model and make predictions: \", round((time.time() - start_time)\/60, 2), \" mins\")\nprint(\"Accuracy: \",round(metrics.accuracy_score(y_test,y_pred),3))\nprint(\"F1: \",round(metrics.f1_score(y_test, y_pred),3))","01e32a66":"undersample = RandomUnderSampler(sampling_strategy='majority')\nX_under, y_under = undersample.fit_resample(train_feature_set,y_train)\nlr = LogisticRegression(solver = 'liblinear', random_state = 42, max_iter=1000)\nlr.fit(X_under,y_under)\ny_pred = lr.predict(test_feature_set)\nprint(\"Time takes to train model and make predictions: \", round((time.time() - start_time)\/60, 2), \" mins\")\nprint(\"Accuracy: \",round(metrics.accuracy_score(y_test,y_pred),3))\nprint(\"F1: \",round(metrics.f1_score(y_test, y_pred),3))","3633b620":"oversample = RandomOverSampler(sampling_strategy='minority')\nX_over, y_over = oversample.fit_resample(train_feature_set,y_train)\nlr = LogisticRegression(solver = 'liblinear', random_state = 42, max_iter=1000)\nlr.fit(X_over,y_over)\ny_pred = lr.predict(test_feature_set)\nprint(\"Time takes to train model and make predictions: \", round((time.time() - start_time)\/60, 2), \" mins\")\nprint(\"Accuracy: \",round(metrics.accuracy_score(y_test,y_pred),3))\nprint(\"F1: \",round(metrics.f1_score(y_test, y_pred),3))","2a37e209":"start_time = time.time()\ntfidf_v=TfidfVectorizer(use_idf=True, analyzer = text_prep, min_df = 10, max_df = 0.95)\ntfidf_v.fit_transform(train['text'].values)\ntrain_feature_set=tfidf_v.transform(train['text'].values)\ntest_feature_set=tfidf_v.transform(test['text'].values)\nprint(\"Time takes to convert text input into feature vector: \", round((time.time() - start_time)\/60, 2), \" mins\")","13219e1c":"start_time = time.time()\nlr = LogisticRegression(solver = 'liblinear', random_state = 42, max_iter=1000)\nlr.fit(train_feature_set,y_train)\ny_pred = lr.predict(test_feature_set)\nprint(\"Time takes to train model and make predictions: \", round((time.time() - start_time)\/60, 2), \" mins\")\nprint(\"Accuracy: \",round(metrics.accuracy_score(y_test,y_pred),3))\nprint(\"F1: \",round(metrics.f1_score(y_test, y_pred),3))","c4559ef1":"start_time = time.time()\ncv = CountVectorizer(binary=True, min_df = 10, max_df = 0.95, ngram_range=(1,2))\ncv.fit_transform(train['text'].values)\ntrain_feature_set=cv.transform(train['text'].values)\ntest_feature_set=cv.transform(test['text'].values)\nprint(\"Time takes to convert text input into feature vector: \", round((time.time() - start_time)\/60, 2), \" mins\")","932aa397":"train_feature_set.shape[1]","c02f1445":"cv.vocabulary_","d8845d2e":"start_time = time.time()\nlr = LogisticRegression(solver = 'liblinear', random_state = 42, max_iter=1000)\nlr.fit(train_feature_set,y_train)\ny_pred = lr.predict(test_feature_set)\nprint(\"Time takes to train model and make predictions: \", round((time.time() - start_time)\/60, 2), \" mins\")\nprint(\"Accuracy: \",round(metrics.accuracy_score(y_test,y_pred),3))\nprint(\"F1: \",round(metrics.f1_score(y_test, y_pred),3))","f550b4a4":"feature_importance = lr.coef_[0][:10]\nfor i,v in enumerate(feature_importance):\n    print('Feature: ', list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(i)], 'Score: ', v)","2ccb53ec":"feature_importance = lr.coef_[0]\nsorted_idx = np.argsort(feature_importance)\ntop_10_pos_w = [list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(w)] for w in sorted_idx[range(-1,-11, -1)]]\nprint(top_10_pos_w)","672ba7ae":"fig = plt.figure(figsize=(10, 6))\nax = sns.barplot(top_10_pos_w, feature_importance[sorted_idx[range(-1,-11, -1)]])\nplt.title(\"Most Important Words Used for Negative Sentiment\",fontsize = 20)\nx_locs,x_labels = plt.xticks()\nplt.setp(x_labels, rotation = 40)\nplt.ylabel('Feature Importance', fontsize = 12)\nplt.xlabel('Word', fontsize = 12);","47f52108":"top_10_neg_w = [list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(w)] for w in sorted_idx[:10]]\nprint(top_10_neg_w)","797b3d99":"fig = plt.figure(figsize=(10, 6))\nax = sns.barplot(top_10_neg_w, feature_importance[sorted_idx[:10]])\nplt.title(\"Most Important Words Used for Positive Sentiment\",fontsize = 20)\nx_locs,x_labels = plt.xticks()\nplt.setp(x_labels, rotation = 40)\nplt.ylabel('Feature Importance', fontsize = 12)\nplt.xlabel('Word', fontsize = 12);","84d7c58c":"lr.classes_ #negative class first, positive class next","09fb90e1":"test_review = cv.transform([\"I did not enjoy the food or the service\"])\nlr.predict_proba(test_review)","4ad1d529":"Treat 4,5 as positive sentiment and 1,2 as negative sentiment. Could treat 3 as neutral so it would be multi-class classification but for a simple binary classification we can take it out.","92ad4fc3":"Trying TFIDF Vectorizer:","701e6b44":"No missing values","2c3ee327":"Top words for the positive class (negative sentiment):","56ea37f9":"stars distribution:","8581539c":"binary feature representation, no stopwords removal or stemming, all unigrams","6103eae5":"Try undersampling\/oversampling","b83d5a7f":"Focus on *stars* and *text* only:","f28aad9e":"Top words for the negative class (positive sentiment):","84d55b90":"train test split","32f1034a":"Stopwords removal and stemming:","2ec9aabc":"Sentiment analysis using logistic regression","ec97d12d":"Import libraries:","a8cc8fba":"Class distribution:","b6822e2c":"Unigram and bigram:"}}