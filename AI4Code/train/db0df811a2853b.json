{"cell_type":{"d143098a":"code","1573761f":"code","9b778241":"code","98defbc5":"code","fef29b50":"code","cbc9eb32":"code","c1d1c606":"code","1f93e89b":"code","f3c584c1":"code","553c1dea":"code","7b6bac3b":"code","80970535":"code","13a40132":"code","0a45bd2d":"code","98c20848":"code","8aaa9d7a":"code","900abadf":"code","c208dbe2":"code","785fe199":"code","8d074553":"code","43e9dd73":"code","6609dd74":"code","38936996":"code","2e40eb8e":"code","a40e99c2":"code","826eae9a":"code","0d7d263a":"code","88223117":"code","5fdbb8e8":"code","95d11ed9":"markdown","d68bfa49":"markdown","8ebf415a":"markdown","2e884536":"markdown","4a932f44":"markdown","62bd96eb":"markdown","99026a0b":"markdown","8cd9c465":"markdown","88e3be45":"markdown","3549bc31":"markdown","1354b841":"markdown","c09af6e5":"markdown","080c284f":"markdown","510700a9":"markdown","34eeac15":"markdown","9421d53e":"markdown","0705d29d":"markdown","01f03317":"markdown","702194ee":"markdown","b975e601":"markdown","f3dbd8a5":"markdown","07bf6311":"markdown","93719594":"markdown","c6e7f6af":"markdown","3bf08dc9":"markdown","1d582802":"markdown","079e4aaa":"markdown"},"source":{"d143098a":"#Predicting OR function using McCulloch Pitts Neuron:\ndef McCullochPittsNeuron(x1 , x2):\n    theta = 1\n    g = x1 + x2\n    if g >= theta:\n        f_of_g = 1\n    elif g<= theta:\n        f_of_g = 0\n    y = f_of_g\n    return y","1573761f":"print(McCullochPittsNeuron(0,0) , McCullochPittsNeuron(1,0) , McCullochPittsNeuron(2,2))","9b778241":"import numpy as np\nimport math\nimport random\n\ndef perceptron(input_vector):\n    weights = np.random.normal(0 , 1 , len(input_vector))\n    bias = random.random()\n    output_vector = []\n    for i in range(len(input_vector)):\n        output = input_vector[i] * weights[i]\n        output_vector.append(output)      \n    output_vector.append(-1 * bias)\n    return sum(output_vector)\n","98defbc5":"perceptron([1,2,3])","fef29b50":"def perceptron_with_sigmoidActivation(input_vector):\n    weights = np.random.normal(0 , 1 , len(input_vector))\n    bias = random.random()\n    output_vector = []\n    for i in range(len(input_vector)):\n        output = input_vector[i] * weights[i]\n        output_vector.append(output)      \n        \n    exponent_term = -(math.exp(bias + sum(output_vector)))\n    denominator = 1+exponent_term\n    return 1\/denominator\n","cbc9eb32":"perceptron_with_sigmoidActivation([1,2,3])","c1d1c606":"#Creating a neural network using only python\n\nimport numpy as np\nX = [[1 , 2 , 3 , 2.5] , \n    [2 , 5 , -1 , 2] , \n    [-1.5 , 2.7 , 3.3 , -0.8]]\n\nX = np.array(X)\nclass denseLayer():\n    def __init__(self , n_inputs , n_neurons):\n        self.weights = 0.1*np.random.randn(n_inputs , n_neurons)\n        self.biases = np.zeros((1 , n_neurons))\n        \n    def reluActivation(self , inputs):\n        self.activatedOutput = np.maximum(0 , inputs)\n        return self.activatedOutput\n        \n    def forward(self , inputs):\n        self.output = np.dot(inputs , self.weights) + self.biases\n        return self.output\n        \n        \nlayer1 = denseLayer(4 , 5)\nlayer2 = denseLayer(5 , 2)\n\nunactivated_layer1 = layer1.forward(X)\nactivated_layer1 = layer1.reluActivation(unactivated_layer1)\nunactivated_layer2 = layer2.forward(activated_layer1)\nactivated_layer2 = layer2.reluActivation(unactivated_layer2)\nprint(activated_layer2)","1f93e89b":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","f3c584c1":"class SimpleClassifier(nn.Module):\n\n    def __init__(self, num_inputs, num_hidden, num_outputs):\n        super().__init__()\n        # Initialize the modules we need to build the network\n        self.linear1 = nn.Linear(num_inputs, num_hidden)\n        self.act_fn = nn.Tanh()\n        self.linear2 = nn.Linear(num_hidden, num_outputs)\n\n\n    def forward(self, x):\n        # Perform the calculation of the model to determine the prediction\n        x = self.linear1(x)\n        x = self.act_fn(x)\n        x = self.linear2(x)\n        return x","553c1dea":"model = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1)\n# Printing a module shows all its submodules\nprint(model)","7b6bac3b":"for name, param in model.named_parameters():\n    print(\"Parameter %s, shape %s\" % (name, str(param.shape)))","80970535":"import torch.utils.data as data","13a40132":"class XORDataset(data.Dataset):\n\n    def __init__(self, size, std=0.1):\n        \"\"\"\n        Inputs:\n            size - Number of data points we want to generate\n            std - Standard deviation of the noise (see generate_continuous_xor function)\n        \"\"\"\n        super().__init__()\n        self.size = size\n        self.std = std\n        self.generate_continuous_xor()\n\n\n    def generate_continuous_xor(self):\n        # Each data point in the XOR dataset has two variables, x and y, that can be either 0 or 1\n        # The label is their XOR combination, i.e. 1 if only x or only y is 1 while the other is 0.\n        # If x=y, the label is 0.\n        data = torch.randint(low=0, high=2, size=(self.size, 2), dtype=torch.float32)\n        label = (data.sum(dim=1) == 1).to(torch.long)\n        # To make it slightly more challenging, we add a bit of gaussian noise to the data points.\n        data += self.std * torch.randn(data.shape)\n\n        self.data = data\n        self.label = label\n\n\n    def __len__(self):\n        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n        return self.size\n\n\n    def __getitem__(self, idx):\n        # Return the idx-th data point of the dataset\n        # If we have multiple things to return (data point and label), we can return them as tuple\n        data_point = self.data[idx]\n        data_label = self.label[idx]\n        return data_point, data_label","0a45bd2d":"dataset = XORDataset(size=200)\nprint(\"Size of dataset:\", len(dataset))\nprint(\"Data point 0:\", dataset[0])","98c20848":"import matplotlib.pyplot as plt\ndef visualize_samples(data, label):\n    if isinstance(data, torch.Tensor):\n        data = data.cpu().numpy()\n    if isinstance(label, torch.Tensor):\n        label = label.cpu().numpy()\n    data_0 = data[label == 0]\n    data_1 = data[label == 1]\n\n    plt.figure(figsize=(4,4))\n    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n    plt.title(\"Dataset samples\")\n    plt.ylabel(r\"$x_2$\")\n    plt.xlabel(r\"$x_1$\")\n    plt.legend()","8aaa9d7a":"visualize_samples(dataset.data, dataset.label)\nplt.show()","900abadf":"data_loader = data.DataLoader(dataset, batch_size=10, shuffle=True)","c208dbe2":"# next(iter(...)) catches the first batch of the data loader\n# If shuffle is True, this will return a different batch every time we run this cell\n# For iterating over the whole dataset, we can simple use \"for batch in data_loader: ...\"\ndata_inputs, data_labels = next(iter(data_loader))\n\n# The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the\n# dimensions of the data point returned from the dataset class\nprint(\"Data inputs\", data_inputs.shape, \"\\n\", data_inputs)\nprint(\"Data labels\", data_labels.shape, \"\\n\", data_labels)","785fe199":"loss_module = nn.BCEWithLogitsLoss()","8d074553":"# Input to the optimizer are the parameters of the model: model.parameters()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)","43e9dd73":"train_dataset = XORDataset(size=1000)\ntrain_data_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True)","6609dd74":"visualize_samples(train_dataset.data, train_dataset.label)\nplt.show()","38936996":"from tqdm.notebook import tqdm\ndef train_model(model, optimizer, data_loader, loss_module, num_epochs=100):\n    # Set model to train mode\n    model.train()\n\n    # Training loop\n    for epoch in tqdm(range(num_epochs)):\n        for data_inputs, data_labels in data_loader:\n\n            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n            # data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n\n            ## Step 2: Run the model on the input data\n            preds = model(data_inputs)\n            preds = preds.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n\n            ## Step 3: Calculate the loss\n            loss = loss_module(preds, data_labels.float())\n\n            ## Step 4: Perform backpropagation\n            # Before calculating the gradients, we need to ensure that they are all zero.\n            # The gradients would not be overwritten, but actually added to the existing ones.\n            optimizer.zero_grad()\n            # Perform backpropagation\n            loss.backward()\n\n            ## Step 5: Update the parameters\n            optimizer.step()\n","2e40eb8e":"train_model(model, optimizer, train_data_loader, loss_module)","a40e99c2":"test_dataset = XORDataset(size=500)\n# drop_last -> Don't drop the last batch although it is smaller than 128\ntest_data_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False)","826eae9a":"visualize_samples(test_dataset.data, test_dataset.label)\nplt.show()","0d7d263a":"def eval_model(model, data_loader):\n    model.eval() # Set model to eval mode\n    true_preds, num_preds = 0., 0.\n\n    with torch.no_grad(): # Deactivate gradients for the following code\n        for data_inputs, data_labels in data_loader:\n\n            # Determine prediction of model on dev set\n            #data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n            preds = model(data_inputs)\n            preds = preds.squeeze(dim=1)\n            preds = torch.sigmoid(preds) # Sigmoid to map predictions between 0 and 1\n            pred_labels = (preds >= 0.5).long() # Binarize predictions to 0 and 1\n\n            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n            true_preds += (pred_labels == data_labels).sum()\n            num_preds += data_labels.shape[0]\n\n    acc = true_preds \/ num_preds\n    print(\"Accuracy of the model: %4.2f%%\" % (100.0*acc))","88223117":"eval_model(model, test_data_loader)","5fdbb8e8":"device = 'cpu'\nfrom matplotlib.colors import to_rgba\n@torch.no_grad() # Decorator, same effect as \"with torch.no_grad(): ...\" over the whole function.\ndef visualize_classification(model, data, label):\n    if isinstance(data, torch.Tensor):\n        data = data.cpu().numpy()\n    if isinstance(label, torch.Tensor):\n        label = label.cpu().numpy()\n    data_0 = data[label == 0]\n    data_1 = data[label == 1]\n\n    plt.figure(figsize=(4,4))\n    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n    plt.title(\"Dataset samples\")\n    plt.ylabel(r\"$x_2$\")\n    plt.xlabel(r\"$x_1$\")\n    plt.legend()\n\n    # Let's make use of a lot of operations we have learned above\n    model.to(device)\n    c0 = torch.Tensor(to_rgba(\"C0\")).to(device)\n    c1 = torch.Tensor(to_rgba(\"C1\")).to(device)\n    x1 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n    x2 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n    xx1, xx2 = torch.meshgrid(x1, x2) # Meshgrid function as in numpy\n    model_inputs = torch.stack([xx1, xx2], dim=-1)\n    preds = model(model_inputs)\n    preds = torch.sigmoid(preds)\n    output_image = preds * c0[None,None] + (1 - preds) * c1[None,None] # Specifying \"None\" in a dimension creates a new one\n    output_image = output_image.cpu().numpy() # Convert to numpy array. This only works for tensors on CPU, hence first push to CPU\n    plt.imshow(output_image, origin='upper', extent=(-0.5, 1.5, -0.5, 1.5))\n    plt.grid(False)\n\nvisualize_classification(model, dataset.data, dataset.label)\nplt.show()","95d11ed9":"![OR_function_MPM.png](attachment:d1554fe5-7675-467d-811d-5bfa13ec9047.png)\n\n<p> It can also work for more than 2 inputs <\/p>\n<p> Simillaryly, McCulloch Pitts Neuron can be used to represent linearly separable functions <\/p>","d68bfa49":"<h2> Optimization <\/h2>\n\n\nAfter defining the model and the dataset, it is time to prepare the optimiiiizion of the model. During training we will perform the following steps:\n<ul>\n    <li>Get a batch from the data loader<\/li>\n    <li>Obtain the predictions from the model for the batch<\/li>\n    <li>Calculate the loss based on the difference between predictions and labels<\/li>\n    <li>Backpropagation: calculate the gradients for every parameter with respect to the loss<\/li>\n    <li>Update the parameters of the model in the direction of the gradients<\/li><\/ul>","8ebf415a":"<h2> Evaluation <\/h2>\nOnce we have trained a model, it is time to evaluate it on a held-out test set. As our dataset consist of randomly generated data points, we need to first create a test set with a corresponding data loader.","2e884536":"<h1> INTRODUCTION TO DEEP LEARNING WITH PYTORCH <\/h1>\n\n<h3> Learning to classify XOR dataset with Pytorch and Neural Networks <\/h3>\n<p><b> author: Abhranta Panigrahi <\/b><\/p>","4a932f44":"moving $ \\theta $ to the other side of the inequality: \n\\begin{align*}\n y = f(g(x)) = 1 ,if: \\sum_{i=1}^{n} w_i * x_i - \\theta \\ge 0 \\\\\n y = f(g(x)) = 0 ,if: \\sum_{i=1}^{n} w_i * x_i - \\theta < 0\\\\\n\\end{align*}\n\n<h3>This term $\\theta$ is called as \"BIAS\".<\/h3>","62bd96eb":"Now, we can write a small training function. Remember our five steps: load a batch, obtain the predictions, calculate the loss, backpropagate, and update.","99026a0b":"If we trained our model correctly, we should see a score close to 100% accuracy. However, this is only possible because of our simple task, and unfortunately, we usually don\u2019t get such high scores on test sets of more complex tasks.\n<h2> Visualization <\/h2>\n\nTo visualize what our model has learned, we can perform a prediction for every data point in a range of [-0.5 , 1.5] , and visualize the predicted class as in the sample figure at the beginning of this section. This shows where the model has created decision boundaries, and which points would be classified as , and which as 1 . We therefore get a background image out of blue (class 0) and orange (class 1). The spots where the model is uncertain we will see a blurry overlap. The specific code is less relevant compared to the output figure which should hopefully show us a clear separation of classes:","8cd9c465":"The data package defines two classes which are the standard interface for handling data in PyTorch: data.Dataset, and data.DataLoader. The dataset class provides an uniform interface to access the training\/test data, while the data loader makes sure to efficiently load and stack the data points from the dataset into batches during training.","88e3be45":"<h2> The Dataset Class <\/h2>\n<ul><li>The dataset class summarizes the basic functionality of a dataset in a natural way.<\/li>\n    <li>To define a dataset in PyTorch, we simply specify two functions:<ul>\n        <li>__getitem__<\/li>\n        <li> __len__<\/li><\/ul>\n    <li>The get-item function has to return the -th data point in the dataset, while the len function returns the size of the dataset<\/li>\n    <\/ul>\n","3549bc31":"<h2> Loss Modules <\/h2>\n<p>We can calculate the loss for a batch by simply performing a few tensor operations as those are automatically added to the computation graph. For instance, for binary classification, we can use Binary Cross Entropy (BCE).<\/p>\n\n<p> Pytorch provides all these losses, so we just have to implement these losses. <\/p>\n<p> Everyone is adised to read more about these loss functions, as this is one of the most crucial part of a learning algorithm. <\/p>\n    \n    ","1354b841":"AI, deep learning, machine learning, data science, analytics.... these are all pretty confusing right ????\nLet's take a look at how they look like ?\n\n\n![AI_venn_diagram.png](attachment:251ce44b-bc4e-41ea-bcfa-c12ac7795342.png)","c09af6e5":"<p> One important thing to understand is that the basic goal of machine learningis to create a function that can map between any two spaces. In other words, we try to find a relation between the features and the label of any data <\/p>\n<p> For findig this function various different techniques are used, like- Boosting, bagging, etc. Simillarly, Deep Learning also anothing but another technique to predict the function to map the imput space to the output space. <\/p>\n<p> It was created to mimic the working of the human brain. It is not accurate in it's depiction of the human brain as it was developed at a time when we didn't know how the human brain works. But, they have astounding learning capabilities which has been one of the biggest reasons for the populariation of AI.<\/p>","080c284f":"<ul><li>Each linear layer has a weight matrix of the shape [output, input], and a bias of the shape [output]<\/li>\n    <li> The tanh activation function does not have any parameters.<\/li><\/ul>\n    ","510700a9":"<h2> The Dataloader Class <\/h2>\nThe class torch.utils.data.DataLoader represents a Python iterable over a dataset with support for automatic batching, multi-process data loading and many more features. The data loader communicates with the dataset using the function __getitem__, and stacks its outputs as tensors over the first dimension to form a batch. In contrast to the dataset class, we usually don\u2019t have to define our own data loader class, but can just create an object of it with the dataset as input. Additionally, we can configure our data loader with the following input arguments (only a selection, see full list here):\n\n<ul>\n    <li>batch_size: Number of samples to stack per batch<\/li>\n\n<li>shuffle: If True, the data is returned in a random order. This is important during training for introducing stochasticity.<\/li>\n\n<li>num_workers: Number of subprocesses to use for data loading. The default, 0, means that the data will be loaded in the main process which can slow down training for datasets where loading a data point takes a considerable amount of time (e.g. large images). More workers are recommended for those, but can cause issues on Windows computers. For tiny datasets as ours, 0 workers are usually faster.<\/li>\n\n<li>drop_last: If True, the last batch is dropped in case it is smaller than the specified batch size. This occurs when the dataset size is not a multiple of the batch size. Only potentially helpful during training to keep a consistent batch size.<\/li>","34eeac15":"<h2> PYTORCH <\/h2>\nThere is a lot to learn in pytorch but you guys need to learn it by yourselves. We will go through the basics of it.\n\nHere we are going to classify an XOR toy dataset using a small neural network.","9421d53e":"<h2> Stochastic Gradient Descent<\/h2>\n\nFor updating the parameters, PyTorch provides the package <b>torch.optim<\/b> that has most popular optimizers implemented. We will use the simplest of them: torch.optim.SGD. \n\nStochastic Gradient Descent updates parameters by multiplying the gradients with a small constant, called learning rate, and subtracting those from the parameters (hence minimizing the loss). Therefore, we slowly move towards the direction of minimizing the loss. A good default value of the learning rate for a small network as ours is 0.1.","0705d29d":"The neural network we are going to build is:\n\n<img src = \"https:\/\/uvadlc-notebooks.readthedocs.io\/en\/latest\/_images\/small_neural_network.svg\">","01f03317":"<h2> The data <\/h2>\n<p>PyTorch also provides a few functionalities to load the training and test data efficiently, summarized in the package <b>torch.utils.data<\/b>.<\/p>","702194ee":"So, we calculate this loss and then we backpropagate which another fancy name for chain rule of differentiation. After calculating the gradient of the loss with respect to all the weights and biases of the network, we change the parameters via an optimization method called as \"GRADIENT DESCENT\".\n\nP.S. We don't have time to discuss about those in details, but do feel free to read about them online ar ask ny of us, We will provide you with the resources. (it has a lot of maths though :3)","b975e601":"<h2> McCulloch-Pitts Neuron <\/h2>\n\n![mcCulloch_Pitts.png](attachment:cc49d0ba-4f69-47a1-b066-f3926244a2d0.png)\n\n<p>The first artificial neuron was the Threshold Logic Unit (TLU), or Linear Threshold Unit, first proposed by Warren McCulloch and Walter Pitts in 1943. The model was specifically targeted as a computational model of the \"nerve net\" in the brain.<\/p>\n<h3> FEATURES <\/h3>\n<ul><li>the neuron takes binary inputs<\/li>\n    <li>'g' aggregates the inputs<\/li>\n    <li>'f'takes a decision based on the aggregation (g) <\/li>\n    <\/ul>\n    \n    \n<h3> Learning Algorithm <\/h3>\n<p> Whenever excitatory inputs are given to the neuron, it fires.<\/p>\n\n\\begin{align*}\n g(x_1 , x_2 , x_3 , ........., x_n) = g(x) = \\sum_{i=1}^{n} x_i \\\\\n y = f(g(x)) = 1 ,if g(x) \\ge \\theta\\\\\n y = f(g(x)) = 0 ,if g(x) \\le \\theta\\\\\n\\end{align*}","f3dbd8a5":"<h2>Training <\/h2>\nFinally, we are ready to train our model. As a first step, we create a slightly larger dataset and specify a data loader with a larger batch size.","07bf6311":"<h2> Loss Function <\/h2>\n<p> So how wold our network know when it is doing a mistake and when it is not ? <\/p>\n<p> For that we use a loss function <\/p>\n<h3> Squared Error Loss <\/h3> <br>\n$ L(w) = \\sum_{i=1}^{n} (pred_i - actual_i)^2 $","93719594":"<h3> ACTIVATION FUNCTION <\/h3>\n<p> Now we are just a few steps away from being fully equipped with the basics of neural network <\/p>\n<p> The reason why neural networks are so powerful is that they can represent highly non linear data. This power comes from the activation functions. <\/p>\nWhen neural networks are used withou any non-linear activation function, they can only prform linear transformations. This severely reduces their ability to represent real world data. <\/p>\n\n$ y = w*x + b $ is a linear transformation.\nSo, instead of doing a linear transformation, we apply a function to the output to change it into a non linear transformation.\n$ y = f(w*x + b) $\n\nThese functions can be anything like sigmoid, tanh, ReLU, etc.\n\nSIGMOID:\n<img src = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/88\/Logistic-curve.svg\/1200px-Logistic-curve.svg.png\">\n\nRELU:\n<img src = \"https:\/\/miro.medium.com\/max\/1400\/1*DfMRHwxY1gyyDmrIAd-gjQ.png\">\n\nTANH:\n<img src = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/87\/Hyperbolic_Tangent.svg\/2560px-Hyperbolic_Tangent.svg.png\">\n","c6e7f6af":"<p> Till now we have learnt about basics of Exploratory Data Analysis, Machine learning, Calssification, Regression. We have implemented various machine learning algorithms using scikit learn. Now, it is time to come face to face with another bu word and really powerful technique - \"DEEP LEARNING\". <\/p>","3bf08dc9":"The optimizer provides two useful functions: optimizer.step(), and optimizer.zero_grad(). The step function updates the parameters based on the gradients as explained above. The function optimizer.zero_grad() sets the gradients of all parameters to zero. While this function seems less relevant at first, it is a crucial pre-step before performing backpropagation. If we would call the backward function on the loss while the parameter gradients are non-zero from the previous batch, the new gradients would actually be added to the previous ones instead of overwriting them. This is done because a parameter might occur multiple times in a computation graph, and we need to sum the gradients in this case instead of replacing them. Hence, remember to call optimizer.zero_grad() before calculating the gradients of a batch.","1d582802":"<h2> PERCEPTRON <\/h2>\n<p> After McCulloch Pitts model, the next model of an artificial neuron to become popular was the \"Perceptron\".<\/p>\n<p> The best thing about this model was that it sounded like a transformer and in real life, it added something called as <b>\"WEIGHT\"<\/b> to the connection.<\/p>\n<img src = \"https:\/\/datascientest.com\/wp-content\/uploads\/2021\/03\/perceptron-formule.png\">\n\n<h3> FEATURES<\/h3>\n<ul><li> Introduction of numerical inputs <\/li>\n    <li> Mechanism of learning those weights <\/li>\n    <li> Inputs are no longer limited to boolean va,ues\/ <\/li><\/ul>\n    \n<h3> WORKING FORMULA<\/h3>\n\\begin{align*}\n g(x) = \\sum_{i=1}^{n} w_i * x_i \\\\\n y = f(g(x)) = 1 ,if: \\sum_{i=1}^{n} w_i * x_i \\ge \\theta\\\\\n y = f(g(x)) = 0 ,if: \\sum_{i=1}^{n} w_i * x_i < \\theta\\\\\n\\end{align*}\n","079e4aaa":"We build a network of these neuroin to create an \"ARTIFICIAL NEURAL NETWORK\".\n<img src = \"https:\/\/www.tibco.com\/sites\/tibco\/files\/media_entity\/2021-05\/neutral-network-diagram.svg\">\n<p> Let's discuss about how they learn step by step. <\/p>"}}