{"cell_type":{"0ecc2f91":"code","941f652c":"code","5223d91f":"code","79a56ff0":"code","af016bf8":"code","3f6dda41":"code","f5af576e":"code","72f22def":"code","5f2a49b2":"code","dea9c6b8":"code","547d6703":"code","34af7061":"code","5870d102":"code","ce4e808b":"code","93d18cfa":"code","718fd270":"code","00a38846":"code","8dc1219e":"code","ecc130ed":"code","a232b7b4":"code","67ee52e0":"code","fb01eb49":"code","a64f89de":"markdown","caf03f21":"markdown","9f3ca515":"markdown","a84713d1":"markdown"},"source":{"0ecc2f91":"import pandas as pd\nimport numpy as np\nfrom sklearn import *\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n(market_train, news_train) = env.get_training_data()","941f652c":"def data_prep(market_train,news_train):\n    market_train.time = market_train.time.dt.date\n    news_train.time = news_train.time.dt.hour\n    news_train.sourceTimestamp= news_train.sourceTimestamp.dt.hour\n    news_train.firstCreated = news_train.firstCreated.dt.date\n    news_train['assetCodesLen'] = news_train['assetCodes'].map(lambda x: len(eval(x)))\n    news_train['assetCodes'] = news_train['assetCodes'].map(lambda x: list(eval(x))[0])\n    kcol = ['firstCreated', 'assetCodes']\n    news_train = news_train.groupby(kcol, as_index=False).mean()\n    market_train = pd.merge(market_train, news_train, how='left', left_on=['time', 'assetCode'], \n                            right_on=['firstCreated', 'assetCodes'])\n    lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n    market_train['assetCodeT'] = market_train['assetCode'].map(lbl)\n    \n    \n    market_train = market_train.dropna(axis=0)\n    \n    return market_train","5223d91f":"market_train = data_prep(market_train,news_train)","79a56ff0":"market_train.head()","af016bf8":"from datetime import datetime, date","3f6dda41":"# Remove noisy data from financial crisis of 2008 and before\n# Why? Because we have reason to believe this time is significantly different from how it acts now.\nmarket_train = market_train.loc[market_train['time_x']>=date(2009, 1, 1)]","f5af576e":"# The target is binary\nup = market_train.returnsOpenNextMktres10 >= 0","72f22def":"fcol = [c for c in market_train if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'audiences', \n                                             'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider', \n                                             'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]","5f2a49b2":"# We still need the returns for model tuning\nX = market_train[fcol].values\nup = up.values\nr = market_train.returnsOpenNextMktres10.values","dea9c6b8":"# Scaling of X values\n# It is good to keep these scaling values for later\nmins = np.min(X, axis=0)\nmaxs = np.max(X, axis=0)\nrng = maxs - mins\nX = 1 - ((maxs - X) \/ rng)","547d6703":"# Sanity check\nassert X.shape[0] == up.shape[0] == r.shape[0]","34af7061":"X_train, X_test, up_train, up_test, r_train, r_test\\\n= model_selection.train_test_split(X, up, r, test_size=0.25, random_state=99)","5870d102":"from xgboost import XGBClassifier\nimport time","ce4e808b":"xgb_up = XGBClassifier(n_jobs=4,n_estimators=250,max_depth=8,eta=0.1)","93d18cfa":"t = time.time()\nprint('Fitting Up')\nxgb_up.fit(X_train,up_train)\nprint(f'Done, time = {time.time() - t}')","718fd270":"from sklearn.metrics import accuracy_score\naccuracy_score(xgb_up.predict(X_test),up_test)","00a38846":"days = env.get_prediction_days()","8dc1219e":"n_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    t = time.time()\n    market_obs_df = data_prep(market_obs_df, news_obs_df)\n    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n    X_live = market_obs_df[fcol].values\n    X_live = 1 - ((maxs - X_live) \/ rng)\n    prep_time += time.time() - t\n    \n    t = time.time()\n    lp = xgb_up.predict_proba(X_live)\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    confidence = 2* lp[:,1] -1\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t","ecc130ed":"env.write_submission_file()","a232b7b4":"total = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')","67ee52e0":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom xgboost import plot_importance","fb01eb49":"plt.figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\nplt.bar(range(len(xgb_up.feature_importances_)), xgb_up.feature_importances_)\nplt.xticks(range(len(xgb_up.feature_importances_)), fcol, rotation='vertical');","a64f89de":"For good measure, we can check what XGBoost bases its decisions on","caf03f21":"# XGBoost Baseline\n\nThis notebook rephrases the challenge of predicting stock returns as the challenge of predicting whether a stock will go up. The evaluation  asks you to predict a confidence value between -1 and 1. The predicted confidence value gets then multiplied with the actual return. If your confidence is in the wrong direction (ie. you predict positive values while returns are actually negative), you loose on the metric. If your direction is right however, you want your confidence be as large as possible.\n\nStocks can only go up or down, if the stock is not going up, it must go down (at least a little bit). So if we know our model confidence in the stock going up, then our new confidence is:\n$$\\hat{y}=up-(1-up)=2*up-1$$\n\nWe are left with a \"simple\" binary classification problem, for which there are a number of good tool, here we use XGBoost, but pick your poison.","9f3ca515":"A side effect of treating this as a binary task is that we can use a simpler metric to judge our models","a84713d1":"<p>The notebook is based on:  https:\/\/www.kaggle.com\/jannesklaas\/lb-0-6-xgboost-baseline  <\/p>\n<p>Only difference:  xgb_up = XGBClassifier(n_jobs=4,n_estimators=200,max_depth=8,eta=0.1) <\/p>\n<p> His original model: xgb_up = XGBClassifier(n_jobs=4,n_estimators=100) <\/p>\n<p> Please knidly vote  if you like the parameters <\/p>\n    "}}