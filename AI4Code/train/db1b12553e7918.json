{"cell_type":{"4163fee1":"code","ac4bb5a8":"code","3d966efd":"code","2ced70da":"code","1c0bc5ad":"code","aa5bd4a7":"code","c362bab3":"code","fc3e18ea":"code","de93a3f2":"code","bafdd2dc":"code","8ff79ed2":"code","c12c52fa":"code","10d01c9d":"code","26464ddf":"code","60dfa328":"code","079edde8":"code","4e6ac8ca":"code","64e89162":"code","440590e1":"code","5d568114":"code","8d994f01":"code","8be8de53":"code","d53d4c19":"code","edbfa108":"code","14ad13c2":"code","e265fc46":"code","daa5e039":"code","4266bb36":"code","530084ba":"code","a5134b56":"code","c5b9fddf":"code","96e602c0":"code","85c04bcd":"code","acf6562f":"code","2eb0f5c7":"code","d2a7fa5c":"code","c1439e55":"markdown","a7e6c3fa":"markdown","9337f57d":"markdown","7489cc76":"markdown","e8080d33":"markdown","8a9dbe90":"markdown","cfc1afa5":"markdown","9105cf49":"markdown","51a48016":"markdown","4d2e0b5a":"markdown","858cb9c2":"markdown","08b773c6":"markdown","b1c6c4ab":"markdown","d5136771":"markdown","10121577":"markdown","e3f84119":"markdown","cdd201e3":"markdown"},"source":{"4163fee1":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nfrom tqdm.notebook import tqdm_notebook\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score\nfrom lightgbm import LGBMClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')","ac4bb5a8":"# Load files into a pandas dataframe\ntrain = pd.read_csv('..\/input\/autoinland-vehicle-insurance-claim-challenge\/Train.csv')\ntest = pd.read_csv('..\/input\/autoinland-vehicle-insurance-claim-challenge\/Test.csv')\nss = pd.read_csv('..\/input\/autoinland-vehicle-insurance-claim-challenge\/SampleSubmission.csv')","3d966efd":"# Preview the first five rows of the train set\ntrain.head()","2ced70da":"# Preview the first five rows of the test set\ntest.head()","1c0bc5ad":"# Preview the first five rows of the sample submission file\nss.head()","aa5bd4a7":"# Check the shape of the train and test sets\nprint(f'The shape of the train set is: {train.shape}\\nThe shape of the test set is: {test.shape}')","c362bab3":"# Check if there any missing values in train set\nax = train.isna().sum().sort_values().plot(kind = 'barh', figsize = (10, 7))\nplt.title('Percentage of Missing Values Per Column in Train Set', fontdict={'size':15})\nfor p in ax.patches:\n    percentage ='{:,.0f}%'.format((p.get_width()\/train.shape[0])*100)\n    width, height =p.get_width(),p.get_height()\n    x=p.get_x()+width+0.02\n    y=p.get_y()+height\/2\n    ax.annotate(percentage,(x,y))","fc3e18ea":"# Check if there missing values in test set\nax = test.isna().sum().sort_values().plot(kind = 'barh', figsize = (10, 7))\nplt.title('Percentage of Missing Values Per Column in Test Set', fontdict={'size':15})\n\nfor p in ax.patches:\n    percentage ='{:,.1f}%'.format((p.get_width()\/test.shape[0])*100)\n    width, height =p.get_width(),p.get_height()\n    x=p.get_x()+width+0.02\n    y=p.get_y()+height\/2\n    ax.annotate(percentage,(x,y))","de93a3f2":"# Combine train and test set\nntrain = train.shape[0] # to be used to split train and test set from the combined dataframe\n\nall_data = pd.concat((train, test)).reset_index(drop=True)\nprint(f'The shape of the combined dataframe is: {all_data.shape}')","bafdd2dc":"# Preview the last five rows of the combined dataframe\nall_data.tail()","8ff79ed2":"# Check the column names and datatypes\nall_data.info()","c12c52fa":"# Change each column to its appriopriate datatype\ndate_cols = [col for col in all_data.columns if 'Date' in col]\nnum_cols = ['Age', 'No_Pol']\ncat_cols = [col for col in all_data.columns if col not in date_cols+num_cols+['ID', 'target']]\n\nfor col in all_data.columns:\n  if col in date_cols:\n    all_data[col] = pd.to_datetime(all_data[col])\n  elif col in cat_cols:\n    all_data[col] = all_data[col].astype('category')\n\n# Confirm whether the changes have been applied successfully\nall_data.info()","10d01c9d":"sns.countplot(train.target)\nplt.title('Target Distribution', fontdict={'size':14});","26464ddf":"# Gender distribution \nax = all_data.Gender.value_counts().sort_values().plot(kind = 'barh', figsize=(10,7))\nplt.title('Gender Distribution', fontdict={'size': 15})\nfor p in ax.patches:\n  percentage ='{:,.1f}%'.format((p.get_width()\/all_data.shape[0])*100)\n  width, height =p.get_width(),p.get_height()\n  x=p.get_x()+width+0.02\n  y=p.get_y()+height\/2\n  ax.annotate(percentage,(x,y))","60dfa328":"mapper = {'Entity':'Other', 'Joint Gender':'Other', 'NOT STATED':'Other', 'NO GENDER': 'Other', 'SEX':\"Other\"}\nall_data.Gender = all_data.Gender.replace(mapper)\n\n# Confirm mappings\nall_data.Gender.value_counts()","079edde8":"# Check unique values for each categorical column\nfor col in cat_cols:\n  print(col)\n  print(all_data[col].unique(), '\\n')","4e6ac8ca":"# Fill in missing values\n# For cat cols and date cols fill in with mode and for num cols fill in with 9999\nfor col in all_data.columns:\n  if col in date_cols+cat_cols:\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n  elif col in num_cols:\n    all_data[col] = all_data[col].fillna(all_data[col].fillna(9999))\n\n# Confirm that there aren't any missing values\nall_data[all_data.columns.difference(['target'])].isna().sum()","64e89162":"# Extract date features from the date columns\nfor col in date_cols:\n  for date_feature in ['year', 'month', 'day']:\n    all_data[col+date_feature] = getattr(all_data[col].dt, date_feature)\n\nall_data.head()","440590e1":"# Encode categorical features\nall_data = pd.get_dummies(data = all_data, columns = cat_cols)\nall_data.head()","5d568114":"# import re\n# all_data = all_data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))","8d994f01":"main_cols = all_data.columns.difference(date_cols+['ID', 'target'])","8be8de53":"# s = all_data[main_cols].select_dtypes(include='object').columns\n# all_data[s] = all_data[s].astype(\"float\")\n# s = all_data[main_cols].select_dtypes(include='string').columns\n# all_data[s] = all_data[s].astype(\"float\")","d53d4c19":"# Separate train and test data from the combined dataframe\ntrain_df = all_data[:ntrain]\ntest_df = all_data[ntrain:]\n\n# Check the shapes of the split dataset\ntrain_df.shape, test_df.shape","edbfa108":"train_df.columns","14ad13c2":"# Select main columns to be used in training\n\nX = train_df[main_cols]\ny = train_df.target\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42)\n\n# Train model","e265fc46":"!pip install lightautoml","daa5e039":"## LightAutoML\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.dataset.roles import DatetimeRole\nfrom lightautoml.tasks import Task\nfrom lightautoml.utils.profiler import Profiler","4266bb36":"N_THREADS = 8  # threads cnt for lgbm and linear models\nJOB = 8\nN_FOLDS = 5  # folds cnt for AutoML\nRANDOM_STATE = 42  # fixed random state for various reasons\nTEST_SIZE = 0.2  # Test size for metric check\nTIMEOUT = 6000  # Time in seconds for automl run\n\nnp.random.seed(RANDOM_STATE)","530084ba":"from sklearn.metrics import roc_auc_score\ndef AUC_metric(y_true, y_pred, **kwargs):\n    return roc_auc_score(y_true, (y_pred > 0.5).astype(int), **kwargs)\n\n\ntask = Task(\"binary\", metric=roc_auc_score)\n\nroles = {\n    \"target\": \"target\",\n}\n\ntrain_df_1, test_df_1 = train_test_split(train_df.drop(['First Transaction Date', 'Policy Start Date', 'ID', 'Policy End Date'],axis=1), test_size=0.20)","a5134b56":"%%time \nautoml = TabularUtilizedAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       general_params = {'use_algos': [['catboost', 'lgb', 'lgb_tuned']]},\n                       reader_params = {'n_jobs': JOB})\n                       \noof_pred = automl.fit_predict(train_df_1, roles = roles)","c5b9fddf":"pred = automl.predict(test_df_1)\nprediction = (pred.data[:, 0] > 0.3).astype(int)","96e602c0":"print(\"AUC score :\", roc_auc_score(test_df_1[\"target\"], prediction))","85c04bcd":"# Make prediction on the test set\ntest_df = test_df[main_cols]\npred = automl.predict(test_df)\npredictions = (pred.data[:, 0] > 0.3).astype(int)\n\n# Create a submission file\nsub_file = ss.copy()\nsub_file.predictions = predictions\n\n# Check the distribution of your predictions\nsns.countplot(sub_file.predictions);","acf6562f":"ss[\"target\"] = predictions.astype(\"int\")","2eb0f5c7":"ss.to_csv(\"submission.csv\",index=False)","d2a7fa5c":"ss.head()","c1439e55":"# AutoInland Vehicle Insurance Claim Challenge StarterNotebook - Python\n\nThis is a simple starter notebook to get started with the AutoInland Vehicle Insurance Claim Challenge on Zindi.\n\nThis notebook covers:\n- Loading the data\n- Simple EDA and an example of feature enginnering\n- Data preprocessing and data wrangling\n- Creating a simple model\n- Making a submission\n- Some tips for improving your score","a7e6c3fa":"### Feature Engineering\n#### Alot of features can be extracted from dates\nTips:\n - Quarter, Start of Year, month?\n - Is it a weekend, weekday?\n - Is it a holiday\n - Duration between different periods, e.g start and end of a policy\n - What features can be derived from the age column\n - Be creative \ud83d\ude09","9337f57d":"### Read files","7489cc76":"### Training and making predictions\nTips:\n- Is lgbm the best model for this challenge?\n- Parameter tuning\n - Grid search, random search, perhaps bayesian search works better...\n\n","e8080d33":"### Distribution of the Gender column","8a9dbe90":"### Importing libraries","cfc1afa5":"### Filling in missing values\n#### Missing values can be filled using different strategies\nTips:\n - Mean\n - Max\n - Min\n - [sklearn SimpleImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html)\n - Others... do more reasearch","9105cf49":"###More Tips\n- Thorough EDA and domain knowledge sourcing\n- Re-group Categorical features \n- More Feature Engineering \n- Dataset balancing - oversampling, undersampling, SMOTE...\n- Ensembling of models \n- Cross-validation: Group folds, Stratified...","51a48016":"This notebook is from Zindi Competition [AutoInland Vehicle Insurance Claim Challenge](https:\/\/zindi.africa\/competitions\/autoinland-vehicle-insurance-claim-challenge)","4d2e0b5a":"## Combine train and test set for easy preprocessing ","858cb9c2":"### Some basic EDA","08b773c6":"# ******************* GOOD LUCK!!! ***************************","b1c6c4ab":"### Number of unique values per categorical column","d5136771":"### Making predictions of the test set and creating a submission file","10121577":"### Distribution of the target variable","e3f84119":"#### Try different strategies of dealing with categorical variables\nTips:\n - One hot encoding\n - Label encoding\n - Target encoding\n - Reduce the number of unique values...","cdd201e3":"#### [More on F1 Score](https:\/\/en.wikipedia.org\/wiki\/F-score)"}}