{"cell_type":{"61559489":"code","62ce8b61":"code","76cd2983":"code","fbbd2998":"code","66f47f77":"code","404bc78d":"code","78c40617":"code","23df8002":"code","b35c9815":"code","4ef97e46":"code","808e5999":"code","b2891f69":"code","06faffd5":"code","d1560d2f":"code","85a1d6ac":"code","8a2a71c3":"code","f62d1401":"code","b43544d1":"code","eb82cfee":"code","065a710d":"code","926fb848":"code","283b69ef":"code","718d8970":"code","89a071ee":"code","2c7ef06a":"code","1ca71975":"code","46611aef":"markdown","aef5d8a1":"markdown","b6f2f27a":"markdown","770ed590":"markdown","84805a60":"markdown","1b62b228":"markdown","413a75de":"markdown","1f29e47c":"markdown","5745e794":"markdown","0bc0d11f":"markdown","e0ddbb9d":"markdown","97f103e3":"markdown","b1d1c198":"markdown","733b3228":"markdown","ddc567fb":"markdown","e9f0b879":"markdown","f0b66ae6":"markdown","d9d2ae43":"markdown","d143d782":"markdown","83ecb19f":"markdown","b36dfd91":"markdown","bf44465f":"markdown","4343a4b6":"markdown"},"source":{"61559489":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport re\nimport time\nfrom datetime import datetime\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport requests\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('Setup Complete!')","62ce8b61":"no_pages = 2\n\ndef get_data(pageNo):  \n    headers = {\"User-Agent\":\"Mozilla\/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko\/20100101 Firefox\/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text\/html,application\/xhtml+xml,application\/xml;q=0.9,*\/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n\n    r = requests.get('https:\/\/www.amazon.com\/gp\/bestsellers\/books\/ref=zg_bs_pg_'+str(pageNo)+'?ie=UTF8&pg='+str(pageNo), headers=headers)#, proxies=proxies)\n    content = r.content\n    soup = BeautifulSoup(content)\n    #print(soup)\n\n    alls = []\n    for d in soup.findAll('div', attrs={'class':'a-section a-spacing-none aok-relative'}):\n        #print(d)\n        name = d.find('span', attrs={'class':'zg-text-center-align'})\n        n = name.find_all('img', alt=True)\n        #print(n[0]['alt'])\n        author = d.find('a', attrs={'class':'a-size-small a-link-child'})\n        rating = d.find('span', attrs={'class':'a-icon-alt'})\n        users_rated = d.find('a', attrs={'class':'a-size-small a-link-normal'})\n        price = d.find('span', attrs={'class':'p13n-sc-price'})\n\n        all1=[]\n\n        if name is not None:\n            #print(n[0]['alt'])\n            all1.append(n[0]['alt'])\n        else:\n            all1.append(\"unknown-product\")\n\n        if author is not None:\n            #print(author.text)\n            all1.append(author.text)\n        elif author is None:\n            author = d.find('span', attrs={'class':'a-size-small a-color-base'})\n            if author is not None:\n                all1.append(author.text)\n            else:    \n                all1.append('0')\n\n        if rating is not None:\n            #print(rating.text)\n            all1.append(rating.text)\n        else:\n            all1.append('-1')\n\n        if users_rated is not None:\n            #print(price.text)\n            all1.append(users_rated.text)\n        else:\n            all1.append('0')     \n\n        if price is not None:\n            #print(price.text)\n            all1.append(price.text)\n        else:\n            all1.append('0')\n        alls.append(all1)    \n    return alls","76cd2983":"results = []\nfor i in range(1, no_pages+1):\n    results.append(get_data(i))\nflatten = lambda l: [item for sublist in l for item in sublist]\ndf = pd.DataFrame(flatten(results),columns=['Book Name','Author','Rating','Customers_Rated', 'Price'])\ndf.to_csv('amazon_products.csv', index=False, encoding='utf-8')","fbbd2998":"df = pd.read_csv(\"amazon_products.csv\")","66f47f77":"df.shape","404bc78d":"df.head()","78c40617":"df.tail()","23df8002":"df.info()","b35c9815":"df['Rating'] = df['Rating'].apply(lambda x: x.split()[0])\ndf['Rating'] = pd.to_numeric(df['Rating'])\ndf[\"Price\"] = df[\"Price\"].str.replace('$', '')\ndf[\"Price\"] = df[\"Price\"].str.replace(',', '')\ndf['Price'] = df['Price'].apply(lambda x: x.split('.')[0])\ndf['Price'] = df['Price'].astype(int)\ndf[\"Customers_Rated\"] = df[\"Customers_Rated\"].str.replace(',', '')\ndf['Customers_Rated'] = pd.to_numeric(df['Customers_Rated'], errors='ignore')\ndf.head()","4ef97e46":"df.dtypes","808e5999":"## Replace the zero values in the DataFrame to NaN.\ndf.replace(str(0), np.nan, inplace=True)\ndf.replace(0, np.nan, inplace=True)","b2891f69":"## Counting the Number of NaNs in the DataFrame\ncount_nan = len(df) - df.count()\ncount_nan","06faffd5":"## Let's drop these NaNs.\ndf = df.dropna()","d1560d2f":"data = df.sort_values([\"Price\"], axis=0, ascending=False)[:15]\ndata","85a1d6ac":"from bokeh.models import ColumnDataSource\nfrom bokeh.transform import dodge\nimport math\nfrom bokeh.io import curdoc\ncurdoc().clear()\nfrom bokeh.io import push_notebook, show, output_notebook\nfrom bokeh.layouts import row\nfrom bokeh.plotting import figure\nfrom bokeh.transform import factor_cmap\nfrom bokeh.models import Legend\noutput_notebook()","8a2a71c3":"p = figure(x_range=data.iloc[:,1], plot_width=800, plot_height=550, title=\"Authors Highest Priced Book\", toolbar_location=None, tools=\"\")\n\np.vbar(x=data.iloc[:,1], top=data.iloc[:,4], width=0.9)\n\np.xgrid.grid_line_color = None\np.y_range.start = 0\np.xaxis.major_label_orientation = math.pi\/2","f62d1401":"show(p)","b43544d1":"data = df[df['Customers_Rated'] > 1000]\ndata = data.sort_values(['Rating'],axis=0, ascending=False)[:15]\ndata","eb82cfee":"p = figure(x_range=data.iloc[:,0], plot_width=800, plot_height=600, title=\"Top Rated Books with more than 1000 Customers Rating\", toolbar_location=None, tools=\"\")\n\np.vbar(x=data.iloc[:,0], top=data.iloc[:,2], width=0.9)\n\np.xgrid.grid_line_color = None\np.y_range.start = 0\np.xaxis.major_label_orientation = math.pi\/2\nshow(p)","065a710d":"p = figure(x_range=data.iloc[:,1], plot_width=800, plot_height=600, title=\"Top Rated Books with more than 1000 Customers Rating\", toolbar_location=None, tools=\"\")\n\np.vbar(x=data.iloc[:,1], top=data.iloc[:,2], width=0.9)\n\np.xgrid.grid_line_color = None\np.y_range.start = 0\np.xaxis.major_label_orientation = math.pi\/2\nshow(p)","926fb848":"data = df.sort_values([\"Customers_Rated\"], axis=0, ascending=False)[:20]","283b69ef":"data","718d8970":"from bokeh.transform import factor_cmap\nfrom bokeh.models import Legend\nfrom bokeh.palettes import Dark2_5 as palette\nimport itertools\nfrom bokeh.palettes import d3\n#colors has a list of colors which can be used in plots\ncolors = itertools.cycle(palette)\n\npalette = d3['Category20'][20]","89a071ee":"index_cmap = factor_cmap('Author', palette=palette,\n                         factors=data[\"Author\"])","2c7ef06a":"p = figure(plot_width=700, plot_height=700, title = \"Top Authors: Rating vs. Customers Rated\")\np.scatter('Rating','Customers_Rated',source=data,fill_alpha=0.6, fill_color=index_cmap,size=20,legend='Author')\np.xaxis.axis_label = 'RATING'\np.yaxis.axis_label = 'CUSTOMERS RATED'\np.legend.location = 'top_left'","1ca71975":"show(p)","46611aef":"<center><img src=\"https:\/\/images.unsplash.com\/photo-1637275769153-b5fb9e5647f1?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=870&q=80\" alt=\"Paris\" class=\"center\"><\/center>","aef5d8a1":"### Scraping the Amazon Best Selling Books\n\n**This URL that you are going to scrape is the following: https:\/\/www.amazon.com\/gp\/bestsellers\/books\/) The page argument can be modified to access data for each page. Hence, to access all the pages you will need to loop through all the pages to get the necessary dataset, but first, you need to find out the number of pages from the website.**\n\n**To connect to the URL and fetch the HTML content following things are required:**\n\n**Define a get_data function which will input the page numbers as an argument,\nDefine a user-agent which will help in bypassing the detection as a scraper,\nSpecify the URL to requests.get and pass the user-agent header as an argument,\nExtract the content from requests.get,\nScrape the specified page and assign it to soup variable,\nNext and the important step is to identify the parent tag under which all the data you need will reside. The data that you are going to extract is:**\n\n* Book Name\n* Author\n* Rating\n* Customers Rated\n* Price\n* The below image shows where the parent tag is located, and when you hover over it, all the required elements are highlighted.\n\n\n<center><img src = \"https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1587503052\/web4_whuq12.png\"><\/center>\n\n\n","b6f2f27a":"<center><img src=\"https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1587503050\/web2_d2bbrf.png\" alt=\"Paris\" class=\"center\"><\/center>\n\n\n\n**Enough of theory, right? So, let's install beautiful soup and start learning about its features and capabilities using Python.**\n\n**As a first step, you need to install the Beautiful Soup library using your terminal or jupyter lab. The best way to install beautiful soup is via pip, so make sure you have the pip module already installed.**\n\n\n`!pip3 install beautifulsoup4`","770ed590":"# <center>Data Visualization<\/center>","84805a60":"# <center>Asalamu Aalikum, Warahmato Allah wabarakatoh<\/center>","1b62b228":"**Let's do some preprocessing on the ratings, customers_rated, and price column.**\n* Since you know the ratings are out of 5, you can keep only the rating and remove the extra part from it.\n* From the customers_rated column, remove the comma.\n* From the price column, remove the rupees symbol, comma, and split it by dot.\n* Finally, convert all the three columns into integer or float.","413a75de":"### Let's verify the data types of the DataFrame.","1f29e47c":"# <center>Web Scraping<\/center>","5745e794":"**The above graph is a scatter plot of Authors who bagged customer rating vs. actual rating. The following conclusions can be made after looking at the above plot.**\n\nHands down Matt Haig's book The Midnight Library: A Novel is the best selling book since the rating, and the number of customers rated are both in sync.\n# <center>Conclusion<\/center>\n##  Congratulations on finishing the tutorial.\ud83c\udf89","0bc0d11f":"**From the above graph, you can observe that the top three highest-priced books are by the author `Koyohau Gotouge`, `Paul McCartney` and `J.K.Rowling`.\n**\n### Top Rated Books and Authors wrt Customers Rated\n \n**Let's find out which authors have the top-rated books and which books of those authors are top rated. However, while finding this out, you would filter out those authors in which less than 1000 customers rated.**","e0ddbb9d":"### Authors Highest Priced Book\n**Let's find out which all authors had the highest-priced book. You will visualize the results for such the top 20 authors.**","97f103e3":"**The below code cell will perform the following functions:**\n\n* Call the get_data function inside a for loop.\n* The for loop will iterate over this function starting from 1 till the number of pages+1.\n* Since the output will be a nested list, you would first flatten the list and then pass it to the DataFrame.\n* Finally, save the dataframe as a CSV file.","b1d1c198":"## <center style = \"font-family:Arial\">Learn what web scraping \ud83d\udd78 is and how it can be achieved with the help of Python's \ud83d\udc0d beautiful soup library. Learn by using Amazon website data.<\/center>","733b3228":"**The above graph shows the top 10 authors in descending order who have the highest rated books with more than 1000 customer ratings, which are Sadhguru, Bill Martin Jr. and Eric Carle.**\n\n**Most Customer Rated Authors and Books\nWhile you have already seen the top-rated books and top-rated authors, it would still be more convincing and credible to conclude the best author and the book based on the number of customers who rated for that book.**\n\n**So, let's quickly find that out.**","ddc567fb":"**Similar to the parent tag, you need to find the attributes for book name, author, rating, customers rated, and price. You will have to go to the webpage you would like to scrape, select the attribute and right-click on it, and select inspect element. This will help you in finding out the specific information fields you need an extract from the sheer HTML web page, as shown in the figure below:**\n\n<center><img src = \"https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1587503050\/web5_muvlzk.png\"><\/center>","e9f0b879":"# <center>Data Cleaning<\/center>","f0b66ae6":"**From the above output, you can observe that there is a total of six books that do not have an Author Name, while one book does not have a price associated with it. These pieces of information are crucial for an author who wants to sell his or her books and should not neglect to put such information.**","d9d2ae43":"# <center>Import Libraries<\/center>\n\n#### Let's import the required packages which you will use to scrape the data from the website and visualize it with the help of seaborn, matplotlib, and bokeh.","d143d782":"# <center> Reading CSV File<\/center>\n**Now let's load the CSV file you created and save in the above cell. Again, this is an optional step; you could even use the dataframe df directly and ignore the below step.**","83ecb19f":"<center>This tutorial was a basic introduction to web scraping with beautiful soup and how you can make sense out of the information extracted from the web by visualizing it using the bokeh plotting library. A good exercise to take a step forward in learning web scraping with beautiful soup is to scrape data from some other websites and see how you can get insights from it.<\/center>","b36dfd91":"**Note that some author names are not registered with Amazon, so you need to apply extra find for those authors. In the below cell code, you would find nested if-else conditions for author names, which are to extract the author\/publication names.**","bf44465f":"**The shape of the dataframe reveals that there are 100 rows and 5 columns in your CSV file.**\n\n**Let's print the first 5 rows of the dataset.**","4343a4b6":"**In the time when the internet is rich with so much data, and apparently, data has become the new oil, web scraping has become even more important and practical to use in various applications. Web scraping deals with extracting or scraping the information from the website. Web scraping is also sometimes referred to as web harvesting or web data extraction. Copying text from a website and pasting it to your local system is also web scraping. However, it is a manual task. Generally, web scraping deals with extracting data automatically with the help of web crawlers. Web crawlers are scripts that connect to the world wide web using the HTTP protocol and allows you to fetch data in an automated manner.**\n\n**Whether you are a data scientist, engineer, or anybody who analyzes vast amounts of datasets, the ability to scrape data from the web is a useful skill to have. Let's say you find data from the web, and there is no direct way to download it, web scraping using Python is a skill you can use to extract the data into a useful form that can then be imported and used in various ways.**\n\n**Some of the practical applications of web scraping could be:**\n\n**Gathering resume of candidates with a specific skill,\nExtracting tweets from twitter with specific hashtags,\nLead generation in marketing,\nScraping product details and reviews from e-commerce websites.\nApart from the above use-cases, web scraping is widely used in natural language processing for extracting text from the websites for training a deep learning model.**\n\n**Potential Challenges of Web Scraping\nOne of the challenges you would come across while scraping information from websites is the various structures of websites. Meaning, the templates of websites will differ and will be unique; hence, generalizing across websites could be a challenge.**\n\n**Another challenge could be longevity. Since the web developers keep updating their websites, you cannot certainly rely on one scraper for too long. Even though the modifications might be minor, but they still might create a hindrance for you while fetching the data.**\n\n**Hence, to address the above challenges, there could be various possible solutions. One would be to follow continuous integration & development (CI\/CD) and constant maintenance as the website modifications would be dynamic.**\n\n**Another more realistic approach is to use Application Programming Interfaces (APIs) offered by various websites & platforms. For example, Facebook and twitter provide you API's specially designed for developers who want to experiment with their data or would like extract information to let's say related to all friends & mutual friends and draw a connection graph of it. The format of the data when using APIs is different from usual web scraping i.e., JSON or XML, while in standard web scraping, you mainly deal with data in HTML format.**\n\n**What is Beautiful Soup?\nBeautiful Soup is a pure Python library for extracting structured data from a website. It allows you to parse data from HTML and XML files. It acts as a helper module and interacts with HTML in a similar and better way as to how you would interact with a web page using other available developer tools.**\n\n**It usually saves programmers hours or days of work since it works with your favorite parsers like lxml and html5lib to provide organic Python ways of navigating, searching, and modifying the parse tree.**\n\n**Another powerful and useful feature of beautiful soup is its intelligence to convert the documents being fetched to Unicode and outgoing documents to UTF-8. As a developer, you do not have to take care of that unless the document intrinsic doesn't specify an encoding or Beautiful Soup is unable to detect one.**\n\n**It is also considered to be faster when compared to other general parsing or scraping techniques.**\n\n**Types of Parsers**\n"}}