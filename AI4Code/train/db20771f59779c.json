{"cell_type":{"3405d596":"code","2466e9f0":"code","fa17a38c":"code","e331ce06":"code","d788b8ee":"code","8e3563ce":"code","0d1f981f":"code","e83ca781":"code","f941099e":"code","01833c07":"code","11f2006b":"code","23ab382c":"code","d406ef70":"code","c6b4508a":"code","3e58c479":"code","638e4cf4":"code","6b9ea40f":"code","ba986d37":"code","80d4e381":"code","ba2585d3":"markdown","24e317fc":"markdown","7e5a9661":"markdown","49e668e5":"markdown","8caafa84":"markdown","5ac41194":"markdown","78857bbe":"markdown"},"source":{"3405d596":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2466e9f0":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","fa17a38c":"dataset = pd.read_csv(\"..\/input\/question-cpe-swu2019\/question_cpe_swu2019.csv\")\ndataset","e331ce06":"Text_in_dataset = []\nClass_in_dataset = []\nfor row in dataset.to_dict('records'):\n    Text_in_dataset.append(row['question'])\n    Class_in_dataset.append(row['class'])\n#-------------------------------------------#\n# print('Text: ',Text_in_dataset)    \n# print('Class: ',Class_in_dataset)","d788b8ee":"!pip install --upgrade pythainlp","8e3563ce":"from pythainlp.tokenize import word_tokenize\nfrom pythainlp.util import dict_trie","0d1f981f":"## custom dictionary\nwords ={'\u0e21\u0e34\u0e14','\u0e40\u0e17\u0e2d\u0e21','\u0e20\u0e32\u0e04','\u0e24\u0e14\u0e39','\u0e40\u0e21\u0e37\u0e48\u0e2d','\u0e44\u0e2b\u0e23\u0e48','\u0e17\u0e30\u0e40\u0e1a\u0e35\u0e22\u0e19','\u0e21\u0e28\u0e27','\u0e27\u0e31\u0e19','\u0e22\u0e31\u0e07','\u0e44\u0e07','\u0e40\u0e01\u0e34\u0e19','\u0e01\u0e33\u0e2b\u0e19\u0e14','\u0e17\u0e38\u0e19','\u0e01\u0e32\u0e23','\u0e28\u0e36\u0e01\u0e29\u0e32','\u0e2d\u0e22\u0e32\u0e01','\u0e40\u0e17\u0e48\u0e32','\u0e04\u0e48\u0e32','\u0e2a\u0e21\u0e38\u0e14','\u0e27\u0e34\u0e28\u0e27\u0e01\u0e23\u0e23\u0e21\u0e28\u0e32\u0e2a\u0e15\u0e23\u0e4c','\u0e2a\u0e33\u0e19\u0e31\u0e01','\u0e08\u0e31\u0e14','\u0e15\u0e31\u0e49\u0e07','\u0e02\u0e32\u0e14','\u0e40\u0e23\u0e35\u0e22\u0e19',\n        '\u0e04\u0e33\u0e16\u0e32\u0e21','\u0e2b\u0e23\u0e37\u0e2d','\u0e01\u0e34\u0e19','\u0e0b\u0e31\u0e21\u0e40\u0e21\u0e2d\u0e23\u0e4c', '\u0e15\u0e2d\u0e19','\u0e27\u0e34\u0e0a\u0e32','\u0e1b\u0e23\u0e31\u0e1a','\u0e2d\u0e30\u0e44\u0e23','\u0e18\u0e19\u0e1a\u0e38\u0e23\u0e35','\u0e21\u0e2b\u0e32\u0e27\u0e34\u0e17\u0e22\u0e32\u0e25\u0e31\u0e22','\u0e19\u0e40\u0e23\u0e28\u0e27\u0e23','\u0e1a\u0e39\u0e23\u0e1e\u0e32','\u0e21\u0e2b\u0e34\u0e14\u0e25','\u0e2a\u0e2d\u0e1a','\u0e01\u0e25\u0e32\u0e07','\u0e44\u0e2b\u0e19','\u0e08\u0e30','\u0e40\u0e14\u0e37\u0e2d\u0e19','\u0e44\u0e21\u0e48','\u0e44\u0e14\u0e49','\u0e08\u0e48\u0e32\u0e22','\u0e15\u0e49\u0e2d\u0e07','\u0e40\u0e1e\u0e34\u0e48\u0e21','\u0e25\u0e14','\u0e23\u0e32\u0e22',\n        '\u0e02\u0e2d','\u0e16\u0e49\u0e32','\u0e2b\u0e32\u0e01','\u0e25\u0e48\u0e32','\u0e0a\u0e49\u0e32','\u0e17\u0e33','\u0e17\u0e31\u0e19','\u0e15\u0e34\u0e14\u0e15\u0e48\u0e2d','\u0e2b\u0e19\u0e31\u0e07\u0e2a\u0e37\u0e2d','\u0e2b\u0e2d\u0e1e\u0e31\u0e01','\u0e04\u0e37\u0e19','\u0e44\u0e1b','\u0e17\u0e35\u0e48','\u0e42\u0e14\u0e19','\u0e15\u0e31\u0e14','\u0e2b\u0e49\u0e2d\u0e07','\u0e04\u0e37\u0e2d','\u0e08\u0e2d\u0e07','\u0e04\u0e13\u0e30','\u0e28\u0e39\u0e19\u0e22\u0e4c','\u0e02\u0e2d\u0e07','\u0e1c\u0e34\u0e14','\u0e16\u0e2d\u0e19','\u0e2b\u0e21\u0e32','\u0e40\u0e01\u0e23\u0e14','\u0e42\u0e1b\u0e23','\u0e14\u0e35','\u0e04\u0e23\u0e31\u0e1a','\u0e2a\u0e27\u0e31\u0e2a\u0e14\u0e35',\n        '\u0e04\u0e48\u0e30','\u0e04\u0e23\u0e31\u0e1a','\u0e1a\u0e49\u0e32\u0e07','\u0e04\u0e2d\u0e21\u0e1e\u0e34\u0e27\u0e40\u0e15\u0e2d\u0e23\u0e4c','\u0e27\u0e34\u0e28\u0e27\u0e30','\u0e01\u0e33\u0e25\u0e31\u0e07','\u0e2d\u0e32\u0e08\u0e32\u0e23\u0e22\u0e4c','\u0e23\u0e32\u0e21','\u0e40\u0e17\u0e04\u0e42\u0e19\u0e42\u0e25\u0e22\u0e35','\u0e1e\u0e23\u0e30\u0e08\u0e2d\u0e21\u0e40\u0e01\u0e25\u0e49\u0e32','\u0e25\u0e32\u0e14\u0e01\u0e23\u0e30\u0e1a\u0e31\u0e07','\u0e1e\u0e23\u0e30\u0e19\u0e04\u0e23\u0e40\u0e2b\u0e19\u0e37\u0e2d','\u0e2d\u0e07\u0e04\u0e4c','\u0e19\u0e2d\u0e01','\u0e2d\u0e07\u0e04\u0e23\u0e31\u0e01\u0e29\u0e4c','\u0e27\u0e34\u0e17\u0e22\u0e4c','\u0e40\u0e27\u0e47\u0e1a\u0e44\u0e0b\u0e15\u0e4c','\u0e21\u0e35','\u0e44\u0e2b\u0e21','\u0e19\u0e27\u0e31\u0e15\u0e01\u0e23\u0e23\u0e21','\u0e1e\u0e31\u0e01',\n        '\u0e17\u0e23\u0e32\u0e1a','\u0e2d\u0e22\u0e39\u0e48','\u0e18\u0e23\u0e23\u0e21\u0e28\u0e32\u0e2a\u0e15\u0e23\u0e4c','\u0e28\u0e34\u0e25\u0e1b\u0e32\u0e01\u0e23','\u0e08\u0e38\u0e2c\u0e32','\u0e23\u0e31\u0e07\u0e2a\u0e34\u0e15', '\u0e42\u0e04\u0e23\u0e07\u0e01\u0e32\u0e23','\u0e27\u0e34\u0e17\u0e22\u0e32\u0e01\u0e32\u0e23','\u0e41\u0e15\u0e01', '\u0e15\u0e48\u0e32\u0e07', '\u0e01\u0e32\u0e23\u0e1a\u0e49\u0e32\u0e19','\u0e02\u0e49\u0e2d\u0e2a\u0e2d\u0e1a','\u0e19\u0e35\u0e49','\u0e01\u0e31\u0e1a','\u0e04\u0e33\u0e23\u0e49\u0e2d\u0e07','\u0e2b\u0e34\u0e27','\u0e02\u0e49\u0e32\u0e27','\u0e15\u0e34\u0e14','\u0e25\u0e2d\u0e22','\u0e2b\u0e19\u0e48\u0e2d\u0e22','\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e44\u0e23','\u0e16\u0e32\u0e21','\u0e17\u0e31\u0e48\u0e27\u0e44\u0e1b',\n        '\u0e40\u0e1b\u0e47\u0e19','\u0e2d\u0e32\u0e01\u0e32\u0e28','\u0e1b\u0e48\u0e27\u0e22','\u0e43\u0e1a','\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21','\u0e1a\u0e31\u0e19\u0e17\u0e36\u0e01','\u0e04\u0e39\u0e48\u0e21\u0e37\u0e2d','\u0e19\u0e34\u0e2a\u0e34\u0e15'}\ncustom_dictionary_trie = dict_trie(words)","e83ca781":"#\"Word Tokenize\"\nList_Text_token = []\nprint(\"##-----Word Tokenize-----##\")\nfor t in Text_in_dataset :\n    token = word_tokenize(t, engine=\"newmm\" , custom_dict=custom_dictionary_trie)\n    print(\"\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e17\u0e35\u0e48\u0e16\u0e39\u0e01\u0e15\u0e31\u0e14\u0e04\u0e33\u0e41\u0e25\u0e49\u0e27:\",token)\n    print(\"\u0e08\u0e33\u0e19\u0e27\u0e19\u0e04\u0e33:\",len(token))\n    List_Text_token.append(token)\n    print(\"-----------------\")\nprint(\"\u0e08\u0e33\u0e19\u0e27\u0e19\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14: \",len(List_Text_token))\n    ","f941099e":"## Limit  word \/text\n#\u0e40\u0e15\u0e34\u0e21\u0e43\u0e2b\u0e49\u0e04\u0e23\u0e1a 10 \u0e04\u0e33\nmaxseq = 10\nText_preparation = []\nfor s in List_Text_token :\n    n = maxseq - len(s)\n    if n > 0:\n        Text_preparation.append(s+([\"<EOS>\"]*n))#\u0e40\u0e15\u0e34\u0e21\u0e43\u0e2b\u0e49\u0e04\u0e23\u0e1a \u0e15\u0e32\u0e21\u0e17\u0e35\u0e48 Limit \u0e04\u0e33\n    elif n < 0:\n        Text_preparation.append(s[0:maxseq]) #\u0e15\u0e31\u0e14\u0e43\u0e2b\u0e49\u0e40\u0e2b\u0e25\u0e37\u0e2d Limit \u0e04\u0e33\n    else:\n        Text_preparation.append(s)\n# print('dataset_preparation:',Text_preparation)","01833c07":"from gensim.models.keyedvectors import KeyedVectors","11f2006b":"model_WordVec = KeyedVectors.load_word2vec_format('..\/input\/word-vectors-th-from-fasttext\/cc.th.300.vec',binary=False)","23ab382c":"XX = np.zeros((len(Text_preparation), 10, 300))\nfor i in range(len(Text_preparation)):\n    for j in range(len(Text_preparation[i])):\n        if Text_preparation[i][j] in model_WordVec.index_to_key:\n            XX[i,j] = model_WordVec.word_vec(Text_preparation[i][j])\n        else :\n            pass","d406ef70":"# Splitting the data train\/test\nX_train = XX[:700]\nY_train = Class_in_dataset[:700]\nX_test  = XX[700:]\nY_test  = Class_in_dataset[700:]\n# validation\nX_val = X_train[-70:]\nY_val = Y_train[-70:]\nX_train = X_train[:-70]\nY_train = Y_train[:-70]\n#-----------------------#\nprint(f\"\u0e08\u0e33\u0e19\u0e27\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 X_train\/X_val\/X_test: {len(X_train)}\/{len(X_val)}\/{len(X_test)}\")\nprint(f\"\u0e08\u0e33\u0e19\u0e27\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 Y_train\/Y_val\/Y_test: {len(Y_train)}\/{len(Y_val)}\/{len(Y_test)}\")","c6b4508a":"# list to numpy array\nY_test = np.array(Y_test)\nY_val  = np.array(Y_val)\nY_train = np.array(Y_train)","3e58c479":"from tensorflow import keras\nfrom tensorflow.keras import layers","638e4cf4":"model = keras.Sequential([\n        layers.Bidirectional(layers.GRU(15),input_shape=(10,300)),\n        layers.Dense(7, activation='softmax'),\n])","6b9ea40f":"model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy',]\n)\nmodel.summary()","ba986d37":"history = model.fit( X_train,Y_train ,\n                     validation_data = (X_val,Y_val),\n                     epochs = 50,\n                     batch_size = 16,)","80d4e381":"results = model.evaluate(X_test, Y_test)\nprint(\"Loss is\", results[0])\nprint(\"Accuracy score is\", results[1])","ba2585d3":"# Train #","24e317fc":"# Add Optimizer, Loss, and Metric #","7e5a9661":"# Evaluate #","49e668e5":"# Preprocessing #","8caafa84":"# Dataset #","5ac41194":"# Define Model #","78857bbe":"**Word2Vec**"}}