{"cell_type":{"bba8c485":"code","7df6dcda":"code","a37d0ecd":"code","ff8fa669":"code","cdfd2c0f":"code","e282049f":"code","6b9734a1":"code","b4d4cdf5":"code","675f3c06":"code","40ee39f6":"code","ea676fc5":"code","803ce9e9":"code","db2f632b":"code","4787a131":"markdown","0277bfd7":"markdown","d6662fdc":"markdown","3e46da25":"markdown","27b92636":"markdown","8635ee4f":"markdown","b16d1fd4":"markdown","359bb56a":"markdown","ff1547c8":"markdown","dd4a95d3":"markdown","9611fa69":"markdown"},"source":{"bba8c485":"!pip install -q transformers","7df6dcda":"from transformers import pipeline\nunmask = pipeline('fill-mask')","a37d0ecd":"unmask.tokenizer.mask_token","ff8fa669":"predictions = unmask('Elon Musk is the founder of <mask>')\nfor prediction in predictions:\n    print(prediction['sequence'].strip('<s>').strip('<\/s>'), end='\\t--- ')\n    print(f\"{round(100*prediction['score'],2)}% confidence\")","cdfd2c0f":"roberta_unmask = pipeline('fill-mask', model='roberta-base')\npredictions = roberta_unmask('Elon Musk is the founder of <mask>')\nfor prediction in predictions:\n    print(prediction['sequence'].strip('<s>').strip('<\/s>'), end='\\t--- ')\n    print(prediction['score'])","e282049f":"from transformers import AutoModelForMaskedLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained('roberta-base')\nmodel = AutoModelForMaskedLM.from_pretrained('roberta-base')\n\nsequence = f\"The world will end in {tokenizer.mask_token}\" # \"The world will end in <mask>\"\n\ninput_seq = tokenizer.encode(sequence, return_tensors='pt') # tensor([[0, 133, 232, 40, 253, 11, 50264, 2]])\nmask_token_index = torch.where(input_seq == tokenizer.mask_token_id)[1] # (tensor([0]), tensor([6])) - we only want the the 2nd dimension\n\ntoken_logits = model(input_seq).logits\nmasked_token_logits = token_logits[0, mask_token_index, :]\n\ntop_5_tokens = torch.topk(masked_token_logits, 5, dim=1).indices[0].tolist()\n\n# print('sequence:', sequence)\n# print('input_seq:', input_seq)\n# print('mask_token_index:', mask_token_index)\n# print('token_logits:', token_logits)\n# print('masked_token_logits:', masked_token_logits)\n# print('top_5_tokens:', top_5_tokens)","6b9734a1":"for token in top_5_tokens:\n    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))","b4d4cdf5":"from transformers import TFAutoModelForMaskedLM, AutoTokenizer\nimport tensorflow as tf\n\ntokenizer = AutoTokenizer.from_pretrained('roberta-base')\nmodel = TFAutoModelForMaskedLM.from_pretrained('roberta-base')\n\nsequence = f\"The world will end in {tokenizer.mask_token}\" # \"The world will end in <mask>\"\n\ninput_seq = tokenizer.encode(sequence, return_tensors='tf') # tensor([[0, 133, 232, 40, 253, 11, 50264, 2]])\nmask_token_index = tf.where(input_seq == tokenizer.mask_token_id)[0, 1] # (tensor([0]), tensor([6])) - we only want the the 2nd dimension\n\ntoken_logits = model(input_seq)[0]\nmasked_token_logits = token_logits[0, mask_token_index, :]\n\ntop_5_tokens = tf.math.top_k(masked_token_logits, 5).indices.numpy()\n\n# print('sequence:', sequence)\n# print('input_seq:', input_seq)\n# print('mask_token_index:', mask_token_index)\n# print('token_logits:', token_logits)\n# print('masked_token_logits:', masked_token_logits)\n# print('top_5_tokens:', top_5_tokens)","675f3c06":"for token in top_5_tokens:\n    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))","40ee39f6":"!wget --quiet https:\/\/raw.githubusercontent.com\/huggingface\/transformers\/master\/examples\/language-modeling\/run_mlm.py\n!pip install --quiet datasets\n!pip install --quiet git+https:\/\/github.com\/huggingface\/transformers","ea676fc5":"# Clear GPU memory (sometimes needed on Kaggle\/Colab)\nfrom numba import cuda\ncuda.select_device(0)\ncuda.close()\ncuda.select_device(0)","803ce9e9":"!python '.\/run_mlm.py' \\\n--model_name_or_path 'roberta-base' \\\n--dataset_name 'wikitext' \\\n--dataset_config_name 'wikitext-2-raw-v1' \\\n--do_train \\\n--do_eval \\\n--report_to none \\\n--max_train_samples 500 \\\n--max_val_samples 500 \\\n--output_dir '.\/test-mlm'","db2f632b":"model = AutoModelForMaskedLM.from_pretrained('.\/test-mlm\/')\ntokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n\nunmask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n\npredictions = unmask('Elon Musk is the founder of <mask>')\nfor prediction in predictions:\n    print(prediction['sequence'].strip('<s>').strip('<\/s>'), end='\\t--- ')\n    print(f\"{round(100*prediction['score'],2)}% confidence\")","4787a131":"We can now passed a string with a masked word to ```unmask()``` and it'll return an array of predictions. The mask for this tokenizer is described by `unmask.tokenizer.mask_token`. We can type it in manually, or use an `f\"string\"`","0277bfd7":"We can now load in this model by passing in the `output_dir` to `from_pretrained()`","d6662fdc":"Out of the box, the model fairs pretty well. From the top 5 results returend, we see that we've got relatively high confidence for the correct answers, and significantly lower scores for the incorrect ones. The default model used by this pipeline task is `distilroberta-base`. From its info [page](https:\/\/huggingface.co\/distilroberta-base) we see that it's a distilled version of the `roberta-base` model. We can use the parent model directly, by passsing it as an argument when creating the pipeline. HuggingFace offers muliple [models](https:\/\/huggingface.co\/models), each finetuned for a different task","3e46da25":"# Setting up our own workflow\nThe `pipeline()` method works well if you don't need a lot of customisation. But there willl be times when you want more control of over the process, we can instantiate, train and use our own model and tokenzier. The HuggingFace [docs](https:\/\/huggingface.co\/transformers\/task_summary.html#masked-language-modeling) give us a concise way of doing this.\n\n1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a DistilBERT model and loads it with the weights stored in the checkpoint.\n2. Define a sequence with a masked token, placing the `tokenizer.mask_token` instead of a word.\n3. Encode that sequence into a list of IDs and find the position of the masked token in that list.\n4. Retrieve the predictions at the index of the mask token: this tensor has the same size as the vocabulary, and the values are the scores attributed to each token. The model gives higher score to tokens it deems probable in that context.\n5. Retrieve the top 5 tokens using the PyTorch `topk` or TensorFlow `top_k` methods.\n6. Replace the mask token by the tokens and print the results\n","27b92636":"### PyTorch","8635ee4f":"### Tensorflow","b16d1fd4":"# Introduction\n\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. \n\n\nThe model we'll look at in this notebook were trained using a masked language modeling (MLM) objective. It was introduced in this [paper](https:\/\/arxiv.org\/abs\/1907.11692) and first released in this [repository](https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/roberta).","359bb56a":"# Installation\nWe'll be using the [Transformers](https:\/\/huggingface.co\/transformers\/) library by HuggingTorch throughout this notebook.\nIt provides a simple interface to use NLP models with both, PyTorch and Tensorflow\n\nTo get started, use ```pip``` to install the package ```transformers```","ff1547c8":"# Quick Start\n\nThe easiest way to use a pretrained model with HuggingFace is to use ```pipeline()```. This gives us access to a wide variety of NLP [tasks](https:\/\/huggingface.co\/transformers\/main_classes\/pipelines.html#transformers.pipeline). The one we're interested in is ```fill-mask```","dd4a95d3":"*Be sure to use an accelerator when training, this can take a long time*\n\nIn the cell below we're fine tuning `roberta-base` on the `wikitext` dataset. Since this isn't an interactive shell, and we don't want to upload the resulting weights and biases anywhere, we pass in `none` for the `report_to` flag. Even with an accelerator, this can still take a couple of minutes, so we limit training\/validation samples.\nTake note of the `outpu_dir` we'll need it later.","9611fa69":"# Fine tuning the model\nMost of the models on HuggingFace are meant to be fine-tuned for specific tasks. To save valuable time, HuggingFace offers a `Trainer` that'll fine tune our model to a dataset. All we need to do is provide it with a config. You can still [train directly](https:\/\/huggingface.co\/transformers\/training.html#fine-tuning-in-native-pytorch) through PyTorch or Tensorflow if you want, but there's very little benifit to doing that.\n\nTo make things even more easier, HuggingFace offers [scripts](https:\/\/github.com\/huggingface\/transformers\/tree\/master\/examples\/language-modeling) that can be run to generate the model. For fine tuning our model, we'll use `run_mlm.py`. This script requires [version 4.5.0](https:\/\/github.com\/huggingface\/transformers\/blob\/3f48b2bc3e5b555a06492f1e7b999ff29bb6058a\/examples\/language-modeling\/run_mlm.py#L51) which, at the time of writing, hasn't been released. So we'll need to install it from the master. It also requires the `datasets` package to import datasets using only their name"}}