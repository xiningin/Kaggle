{"cell_type":{"3cff1d84":"code","42fdf0f1":"code","26393acd":"code","166c7f64":"code","41bc7429":"code","3b8d171b":"code","a4ef28a9":"code","a9bd4631":"code","782df7a1":"code","e99fc760":"code","eb09edb8":"code","cc371f6c":"code","86dcfb3e":"code","e2ecfd9e":"code","8f88ebaa":"code","d593a353":"code","6e8c76ac":"code","dcccd95b":"code","36cec390":"code","31d0aa6e":"code","0506ccef":"code","30cb2afe":"code","fe2675b5":"code","92f9243b":"code","cd0693e6":"code","b4d5f209":"code","7acb65e6":"code","f38f7638":"code","c13bfb6c":"code","9ed69096":"code","9fd6a3ce":"code","9f84dbe4":"code","a1ccf45b":"code","6e91ae65":"code","ef846bea":"code","9d50f228":"code","9021a262":"code","056def4d":"code","a2f7774e":"code","f8f3daab":"code","6bb7a398":"code","958b9080":"code","a504d995":"code","32d8f8f4":"code","248557da":"code","e37701b6":"code","941dc90a":"code","b8e1b4c8":"code","83e5623c":"code","ca4d0a2e":"code","5c2f06a5":"code","bdb6aa3c":"code","1e2e2bb6":"code","ad366bbf":"code","dc6f99cd":"code","214d6e4b":"code","44e17109":"code","0f52c1db":"code","0914583a":"code","6159d40b":"code","cd0676f8":"code","908576a5":"code","a0fbc268":"code","dbd21cb0":"code","363bb64f":"code","3c0a4681":"code","874fddf9":"code","77745c94":"code","0d19c4b1":"code","8253bf40":"code","14094bc4":"code","69439943":"code","c7517105":"code","d9eed693":"code","810a7166":"code","2832cd02":"code","9bf2a106":"code","084df3f3":"code","4feedca2":"code","cb4880ef":"code","7cc6e76f":"code","9a899c4a":"code","a8516ff0":"code","834468c4":"code","8898ffdd":"markdown","73f098b4":"markdown","29bbf9f9":"markdown","2a886e24":"markdown","1523ede6":"markdown","1360379e":"markdown","ebde5e7e":"markdown","6dc9ffbe":"markdown","2256eac7":"markdown","26062459":"markdown","4fe2a1f5":"markdown","dcb65b92":"markdown","cd7af99d":"markdown","e474d0d2":"markdown","b1419c9a":"markdown","3ff858b4":"markdown","166c3c7b":"markdown","2e6f3d4d":"markdown","9295afdb":"markdown","5b7194cb":"markdown","80cf338d":"markdown","0775afaa":"markdown","212cdaa2":"markdown","0e9b7064":"markdown","a8810c11":"markdown","58070261":"markdown","125c15fc":"markdown","6786ad55":"markdown","4b28b62f":"markdown","20ef8241":"markdown","5af4603e":"markdown","36818cf7":"markdown","56e0345d":"markdown","33c6a30e":"markdown","3961de78":"markdown","80c7f61f":"markdown","31ef6f45":"markdown","000482f9":"markdown","d566db1a":"markdown","01d14efc":"markdown","e8be086a":"markdown","5e3824e2":"markdown","0a7dc84a":"markdown","5eca3761":"markdown","f884687e":"markdown","f01ca57d":"markdown","cbf87ee4":"markdown","ccdcd86f":"markdown","992073f9":"markdown","6fa4cf7a":"markdown","426201ad":"markdown","258216f7":"markdown","a931ee96":"markdown","dccc8c30":"markdown","4b0fb50c":"markdown","68561b58":"markdown","2400d4a0":"markdown","26178d9a":"markdown","445190bc":"markdown","31828d63":"markdown","f13efe56":"markdown","bef8c49a":"markdown","f83c6ff9":"markdown","fd6f8511":"markdown","f347e2b2":"markdown"},"source":{"3cff1d84":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\n\nfrom pykalman import KalmanFilter\nfrom scipy import signal","42fdf0f1":"df_train = pd.read_csv('..\/input\/liverpool-ion-switching\/train.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels': np.uint8})\ndf_test = pd.read_csv('..\/input\/liverpool-ion-switching\/test.csv', dtype={'time': np.float32, 'signal': np.float32})\n\nprint('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() \/ 1024**2))\nprint('Training Set Batches = {}'.format(int(len(df_train) \/ 500000)))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() \/ 1024**2))\nprint('Test Set Batches = {}'.format(int(len(df_test) \/ 500000)))","26393acd":"BATCH_SIZE = 500000\n\nfor i in range(10):\n    df_train.loc[i * BATCH_SIZE:((i + 1) * BATCH_SIZE) - 1, 'batch'] = i\n    \nfor i in range(4):\n    df_test.loc[i * BATCH_SIZE:((i + 1) * BATCH_SIZE) - 1, 'batch'] = i\n    \ndf_train['batch'] = df_train['batch'].astype(np.uint8)\ndf_test['batch'] = df_test['batch'].astype(np.uint8)","166c7f64":"fig = plt.figure(figsize=(15, 7))\nsns.barplot(x=df_train['open_channels'].value_counts().index, y=df_train['open_channels'].value_counts().values)\n\nplt.xlabel('Open Ion Channels', size=15, labelpad=20)\nplt.ylabel('Value Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.title('Open Ion Channels Value Counts in Training Set', size=15)\n\nplt.show()","41bc7429":"training_batches = df_train.groupby('batch')\n\nfig, axes = plt.subplots(ncols=2, nrows=5, figsize=(20, 20), dpi=100)\nfor i, batch in training_batches:\n    ax = plt.subplot(5, 2, i + 1) \n    \n    sns.barplot(x=batch['open_channels'].value_counts().index, y=batch['open_channels'].value_counts().values)\n    \n    plt.xlabel('')\n    plt.ylabel('')\n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    ax.set_title(f'Batch {i} ({batch.index.min()}-{batch.index.max()}) Target Distribution', size=15)\n    \nplt.tight_layout()\nplt.show()","3b8d171b":"def remove_ramp(X, constant=3):\n    r = np.arange(len(X))        \n    return X - ((r * constant) \/ len(X))","a4ef28a9":"def remove_sine(X, constant=5):\n    s = np.arange(len(X))        \n    return X - (constant * (np.sin(np.pi * s \/ len(X))))","a9bd4631":"df_train['signal_processed'] = df_train['signal'].copy()\ndf_test['signal_processed'] = df_test['signal'].copy()","782df7a1":"def report_training_batch(df, feature, batch):\n    \n    print(f'Training Batch {batch} - Unique Open Channel Values = {df[df[\"batch\"] == batch][\"open_channels\"].unique()}')\n    signal_openchannel_corr = np.corrcoef(df[df['batch'] == batch][feature], df[df['batch'] == batch]['open_channels'])[0][1]\n    print(f'Training Batch {batch} - Correlation between Signal and Open Channels = {signal_openchannel_corr:.4}')\n    print(f'Training Batch {batch} - Signal Mean = {df[df[\"batch\"] == batch][feature].mean():.4} and Open Channels Mean = {df[df[\"batch\"] == batch][\"open_channels\"].mean():.4}')\n    print(f'Training Batch {batch} - Signal Std = {df[df[\"batch\"] == batch][feature].std():.4} and Open Channels Std = {df[df[\"batch\"] == batch][\"open_channels\"].std():.4}')\n    print(f'Training Batch {batch} - Open Channels Range:')\n    for value in df[df['batch'] == batch]['open_channels'].unique():\n        print(f'                   Open Channels {value} - Min = {df.query(\"batch == @batch and open_channels == @value\")[feature].min():.6} and Max = {df.query(\"batch == @batch and open_channels == @value\")[feature].max():.6}')\n    \n    fig = plt.figure(figsize=(16, 6), dpi=100)\n    df[df['batch'] == batch].set_index('time')[feature].plot(label='Signal')\n    df[df['batch'] == batch].set_index('time')['open_channels'].plot(label='Open Channels')\n        \n    plt.xlabel('Time', size=15)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    plt.legend()\n    title = f'Training Batch {batch} ({df[df[\"batch\"] == batch].index.min()}-{df[df[\"batch\"] == batch].index.max()})'\n    plt.title(title, size=15)\n    \n    plt.show()\n    \ndef report_test_batch(df, feature, batch):\n    \n    print(f'Test Batch {batch} - Signal Mean = {df[df[\"batch\"] == batch][feature].mean():.4}')\n    print(f'Test Batch {batch} - Signal Std = {df[df[\"batch\"] == batch][feature].std():.4}')\n    \n    fig = plt.figure(figsize=(16, 6), dpi=100)\n    df[df['batch'] == batch].set_index('time')[feature].plot(label='Signal')\n        \n    plt.xlabel('Time', size=15)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    plt.legend()\n    title = f'Test Batch {batch} ({df[df[\"batch\"] == batch].index.min()}-{df[df[\"batch\"] == batch].index.max()})'\n    plt.title(title, size=15)\n    \n    plt.show()\n    ","e99fc760":"report_training_batch(df_train, 'signal', 0)","eb09edb8":"outlier_idx = df_train.query('signal > 0 and batch == 0').index\nbatch0_target0_mean = df_train.drop(outlier_idx).query('batch == 0 and open_channels == 0')['signal'].mean()\ndf_train.loc[outlier_idx, 'signal_processed'] = batch0_target0_mean","cc371f6c":"report_training_batch(df_train, 'signal_processed', 0)","86dcfb3e":"report_training_batch(df_train, 'signal', 1)","e2ecfd9e":"batch1_slice = df_train.loc[df_train.query('time <= 60.0000 and batch == 1').index, 'signal']\ndf_train.loc[df_train.query('time <= 60.0000 and batch == 1').index, 'signal_processed'] = remove_ramp(batch1_slice)","8f88ebaa":"report_training_batch(df_train, 'signal_processed', 1)","d593a353":"report_training_batch(df_train, 'signal', 2)","6e8c76ac":"report_training_batch(df_train.loc[df_train.query('batch == 2 and 143 < time <= 144').index], 'signal', 2)","dcccd95b":"report_training_batch(df_train, 'signal', 3)","36cec390":"report_training_batch(df_train.loc[df_train.query('batch == 3 and 190 < time < 191').index], 'signal', 3)","31d0aa6e":"report_training_batch(df_train, 'signal', 4)","0506ccef":"report_training_batch(df_train.loc[df_train.query('batch == 4 and 239 < time <= 240').index], 'signal', 4)","30cb2afe":"report_training_batch(df_train, 'signal', 5)","fe2675b5":"report_training_batch(df_train.loc[df_train.query('batch == 5 and 295 < time <= 296').index], 'signal', 5)","92f9243b":"report_training_batch(df_train, 'signal', 6)","cd0693e6":"batch6 = df_train.loc[df_train.query('batch == 6').index]['signal']\ndf_train.loc[df_train.query('batch == 6').index, 'signal_processed'] = remove_sine(batch6)","b4d5f209":"report_training_batch(df_train, 'signal_processed', 6)","7acb65e6":"report_training_batch(df_train, 'signal', 7)","f38f7638":"batch7 = df_train.loc[df_train.query('batch == 7').index]['signal']\ndf_train.loc[df_train.query('batch == 7').index, 'signal_processed'] = remove_sine(batch7)","c13bfb6c":"report_training_batch(df_train, 'signal_processed', 7)","9ed69096":"df_train['is_filtered'] = 0\ndf_train['is_filtered'] = df_train['is_filtered'].astype(np.uint8)\nbatch7_outlier_idx = pd.Int64Index(range(3641000, 3829000))\ndf_train.loc[batch7_outlier_idx, 'is_filtered'] = 1","9fd6a3ce":"report_training_batch(df_train.drop(batch7_outlier_idx), 'signal_processed', 7)","9f84dbe4":"open_channels0_mean = df_train[((df_train['batch'] == 3) | (df_train['batch'] == 7)) & (df_train['is_filtered'] == 0) & (df_train['open_channels'] == 0)]['signal_processed'].mean()\nopen_channels1_mean = df_train[((df_train['batch'] == 3) | (df_train['batch'] == 7)) & (df_train['is_filtered'] == 0) & (df_train['open_channels'] == 1)]['signal_processed'].mean()\nopen_channels2_mean = df_train[((df_train['batch'] == 3) | (df_train['batch'] == 7)) & (df_train['is_filtered'] == 0) & (df_train['open_channels'] == 2)]['signal_processed'].mean()\nopen_channels3_mean = df_train[((df_train['batch'] == 3) | (df_train['batch'] == 7)) & (df_train['is_filtered'] == 0) & (df_train['open_channels'] == 3)]['signal_processed'].mean()\n\ndf_train.loc[(df_train['is_filtered'] == 1) & (df_train['open_channels'] == 0), 'signal_processed'] = open_channels0_mean\ndf_train.loc[(df_train['is_filtered'] == 1) & (df_train['open_channels'] == 1), 'signal_processed'] = open_channels1_mean\ndf_train.loc[(df_train['is_filtered'] == 1) & (df_train['open_channels'] == 2), 'signal_processed'] = open_channels2_mean\ndf_train.loc[(df_train['is_filtered'] == 1) & (df_train['open_channels'] == 3), 'signal_processed'] = open_channels3_mean\n\nbatch7_filtered_part = df_train.loc[df_train['is_filtered'] == 1, 'signal_processed']\ndf_train.loc[df_train['is_filtered'] == 1, 'signal_processed'] = batch7_filtered_part + np.random.normal(0, 0.3, size=len(batch7_filtered_part)) ","a1ccf45b":"report_training_batch(df_train, 'signal_processed', 7)","6e91ae65":"report_training_batch(df_train, 'signal', 8)","ef846bea":"batch8 = df_train.loc[df_train.query('batch == 8').index]['signal']\ndf_train.loc[df_train.query('batch == 8').index, 'signal_processed'] = remove_sine(batch8)","9d50f228":"report_training_batch(df_train, 'signal_processed', 8)","9021a262":"report_training_batch(df_train.loc[df_train.query('batch == 8 and 430 < time <= 431').index], 'signal_processed', 8)","056def4d":"report_training_batch(df_train, 'signal', 9)","a2f7774e":"batch9 = df_train.loc[df_train.query('batch == 9').index]['signal']\ndf_train.loc[df_train.query('batch == 9').index, 'signal_processed'] = remove_sine(batch9)","f8f3daab":"report_training_batch(df_train, 'signal_processed', 9)","6bb7a398":"report_training_batch(df_train.loc[df_train.query('batch == 9 and 470 < time <= 471').index], 'signal_processed', 9)","958b9080":"report_test_batch(df_test, 'signal', 0)","a504d995":"batch0_1 = df_test.loc[:100000 - 1, 'signal']\ndf_test.loc[:100000 - 1, 'signal_processed'] = remove_ramp(batch0_1)","32d8f8f4":"report_test_batch(df_test.loc[:100000 - 1], 'signal_processed', 0)","248557da":"batch0_2 = df_test.loc[100000:200000 - 1, 'signal']\ndf_test.loc[100000:200000 - 1, 'signal_processed'] = remove_ramp(batch0_2)","e37701b6":"report_test_batch(df_test.loc[100000:200000 - 1], 'signal_processed', 0)","941dc90a":"report_test_batch(df_test.loc[200000:300000 - 1], 'signal', 0)","b8e1b4c8":"report_test_batch(df_test.loc[300000:400000 - 1], 'signal', 0)","83e5623c":"batch0_5 = df_test.loc[400000:500000 - 1, 'signal']\ndf_test.loc[400000:500000 - 1, 'signal_processed'] = remove_ramp(batch0_5)","ca4d0a2e":"report_test_batch(df_test.loc[400000:500000 - 1], 'signal_processed', 0)","5c2f06a5":"report_test_batch(df_test, 'signal_processed', 0)","bdb6aa3c":"report_test_batch(df_test, 'signal', 1)","1e2e2bb6":"report_test_batch(df_test.loc[500000:600000 - 1], 'signal_processed', 1)","ad366bbf":"batch1_2 = df_test.loc[600000:700000 - 1, 'signal']\ndf_test.loc[600000:700000 - 1, 'signal_processed'] = remove_ramp(batch1_2)","dc6f99cd":"report_test_batch(df_test.loc[600000:700000 - 1], 'signal_processed', 1)","214d6e4b":"batch1_3 = df_test.loc[700000:800000 - 1, 'signal']\ndf_test.loc[700000:800000 - 1, 'signal_processed'] = remove_ramp(batch1_3)","44e17109":"report_test_batch(df_test.loc[700000:800000 - 1], 'signal_processed', 1)","0f52c1db":"batch1_4 = df_test.loc[800000:900000 - 1, 'signal']\ndf_test.loc[800000:900000 - 1, 'signal_processed'] = remove_ramp(batch1_4)","0914583a":"report_test_batch(df_test.loc[800000:900000 - 1], 'signal_processed', 1)","6159d40b":"report_test_batch(df_test.loc[900000:1000000 - 1], 'signal_processed', 1)","cd0676f8":"report_test_batch(df_test, 'signal_processed', 1)","908576a5":"report_test_batch(df_test, 'signal', 2)","a0fbc268":"batch2 = df_test.loc[df_test.query('batch == 2').index]['signal']\ndf_test.loc[df_test.query('batch == 2').index, 'signal_processed'] = remove_sine(batch2)","dbd21cb0":"report_test_batch(df_test, 'signal_processed', 2)","363bb64f":"report_test_batch(df_test.loc[df_test.query('batch == 2 and 624 < time <= 625').index], 'signal_processed', 2)","3c0a4681":"report_test_batch(df_test, 'signal', 3)","874fddf9":"report_test_batch(df_test.loc[df_test.query('batch == 3 and 681 < time <= 682').index], 'signal_processed', 3)","77745c94":"# model 0\nmodel0_trn_idx = df_train.query('batch == 0 or batch == 1').index\nmodel0_tst_idx = df_test.query('batch == 0 and (500 < time <= 510)').index\n\ndf_test.loc[model0_tst_idx, 'model'] = 0\ndf_train.loc[model0_trn_idx, 'model'] = 0\n\n# model 1\nmodel1_trn_idx = df_train.query('batch == 2 or batch == 6').index\nmodel1_tst_idx = df_test.query('batch == 0 and (540 < time <= 550)').index\n\ndf_train.loc[model1_trn_idx, 'model'] = 1\ndf_test.loc[model1_tst_idx, 'model'] = 1\n\n# model 1.5\nmodel15_tst_idx = df_test.query('(batch == 0 and (530 < time <= 540)) or (batch == 1 and (580 < time <= 590)) or batch == 2 or batch == 3').index\ndf_test.loc[model15_tst_idx, 'model'] = 1.5\n\n# model 2\nmodel2_trn_idx = df_train.query('batch == 3 or batch == 7').index\nmodel2_tst_idx = df_test.query('(batch == 0 and (510 < time <= 520)) or (batch == 1 and (590 < time <= 600))').index\n\ndf_train.loc[model2_trn_idx, 'model'] = 2\ndf_test.loc[model2_tst_idx, 'model'] = 2\n\n# model 3\nmodel3_trn_idx = df_train.query('batch == 5 or batch == 8').index\nmodel3_tst_idx = df_test.query('(batch == 0 and (520 < time <= 530)) or (batch == 1 and (560 < time <= 570))').index\n\ndf_train.loc[model3_trn_idx, 'model'] = 3\ndf_test.loc[model3_tst_idx, 'model'] = 3\n\n# model 4\nmodel4_trn_idx = df_train.query('batch == 4 or batch == 9').index\nmodel4_tst_idx = df_test.query('(batch == 1 and (550 < time <= 560)) or (batch == 1 and (570 < time <= 580))').index\n\ndf_train.loc[model4_trn_idx, 'model'] = 4\ndf_test.loc[model4_tst_idx, 'model'] = 4\n\nfor model in [0, 1, 1.5, 2, 3, 4]:\n    print(f'\\n---------- Model {model} ----------\\n')\n    for batch in df_train[df_train['model'] == model]['batch'].unique():\n        model_signal_mean = df_train[(df_train['model'] == model) & (df_train['batch'] == batch)]['signal_processed'].mean()\n        model_signal_std = df_train[(df_train['model'] == model) & (df_train['batch'] == batch)]['signal_processed'].std()\n        model_signal_min = df_train[(df_train['model'] == model) & (df_train['batch'] == batch)]['signal_processed'].min()\n        model_signal_max = df_train[(df_train['model'] == model) & (df_train['batch'] == batch)]['signal_processed'].max()\n        print(f'Training Set Model {model} Batch {batch} signal_processed mean = {model_signal_mean:.4}, std = {model_signal_std:.4}, range = {model_signal_min:.4} - {model_signal_max:.4}')\n\n    for batch in df_test[df_test['model'] == model]['batch'].unique():\n        model_signal_mean = df_test[(df_test['model'] == model) & (df_test['batch'] == batch)]['signal_processed'].mean()\n        model_signal_std = df_test[(df_test['model'] == model) & (df_test['batch'] == batch)]['signal_processed'].std()\n        model_signal_min = df_test[(df_test['model'] == model) & (df_test['batch'] == batch)]['signal_processed'].min()\n        model_signal_max = df_test[(df_test['model'] == model) & (df_test['batch'] == batch)]['signal_processed'].max()\n        print(f'Test Set Model {model} Batch {batch} signal_processed mean = {model_signal_mean:.4}, std = {model_signal_std:.4}, range = {model_signal_min:.4} - {model_signal_max:.4}')\n        \nprint('\\n---------- Training Set Model Value Counts ----------\\n')\nprint(df_train['model'].value_counts())\nprint('\\n---------- Test Set Model Value Counts ----------\\n')\nprint(df_test['model'].value_counts())","0d19c4b1":"fig, axes = plt.subplots(nrows=2, figsize=(20, 14), dpi=100)\n\ndf_train.set_index('time')['signal_processed'].plot(label='Signal', ax=axes[0])\nfor batch in np.arange(0, 550, 50):\n    axes[0].axvline(batch, color='r', linestyle='--', lw=2)\n    \ndf_test.set_index('time')['signal_processed'].plot(label='Signal', ax=axes[1])\n\nfor batch in np.arange(500, 600, 10):\n    axes[1].axvline(batch, color='r', linestyle='--', lw=2)\nfor batch in np.arange(600, 700, 50):\n    axes[1].axvline(batch, color='r', linestyle='--', lw=2)\n    \naxes[1].axvline(560, color='y', linestyle='dotted', lw=8)\n\nfor i in range(2):    \n    for batch in np.arange(0, 550, 50):\n        axes[i].axvline(batch, color='r', linestyle='--', lw=2)\n        \n    axes[i].set_xlabel('')\n    axes[i].tick_params(axis='x', labelsize=15)\n    axes[i].tick_params(axis='y', labelsize=15)\n    axes[i].legend()\n    \naxes[0].set_title('Training Set Batches', size=18, pad=18)\naxes[1].set_title('Public\/Private Test Set Batches and Sub-batches', size=18, pad=18)\n\nplt.show()","8253bf40":"SHIFT_CONSTANT = np.exp(1)\n\ndf_train.loc[df_train['model'] == 4, 'signal_processed'] += SHIFT_CONSTANT\ndf_test.loc[df_test['model'] == 4, 'signal_processed'] += SHIFT_CONSTANT","14094bc4":"fig, axes = plt.subplots(nrows=2, figsize=(20, 14), dpi=100)\n\ndf_train.set_index('time')['signal_processed'].plot(label='Signal', ax=axes[0])\nfor batch in np.arange(0, 550, 50):\n    axes[0].axvline(batch, color='r', linestyle='--', lw=2)\n    \ndf_test.set_index('time')['signal_processed'].plot(label='Signal', ax=axes[1])\n\nfor batch in np.arange(500, 600, 10):\n    axes[1].axvline(batch, color='r', linestyle='--', lw=2)\nfor batch in np.arange(600, 700, 50):\n    axes[1].axvline(batch, color='r', linestyle='--', lw=2)\n    \naxes[1].axvline(560, color='y', linestyle='dotted', lw=8)\n\nfor i in range(2):    \n    for batch in np.arange(0, 550, 50):\n        axes[i].axvline(batch, color='r', linestyle='--', lw=2)\n        \n    axes[i].set_xlabel('')\n    axes[i].tick_params(axis='x', labelsize=15)\n    axes[i].tick_params(axis='y', labelsize=15)\n    axes[i].legend()\n    \naxes[0].set_title('Training Set Batches without Ghost Drift', size=18, pad=18)\naxes[1].set_title('Public\/Private Test Set Batches and Sub-batches without Ghost Drift', size=18, pad=18)\n\nplt.show()","69439943":"df_train['signal_processed_denoised'] = df_train['signal_processed'].copy(deep=True)\ndf_test['signal_processed_denoised'] = df_test['signal_processed'].copy(deep=True)\n\n# Clean parts of training set signal\nsignalA = df_train[df_train['batch'] != 7]['signal_processed_denoised'].values\nchannelsA = df_train[df_train['batch'] != 7]['open_channels'].values\n\n# Replacing a hidden outlier\nsignal1 = signalA[:1000000]\nchannels1 = channelsA[:1000000]\nmedian = np.median(signal1[channels1 == 0])\ncondition = (signal1 > -1) & (channels1 == 0)\nsignal1[condition] = median\nsignalA[:1000000] = signal1\n\n# Batch 7 first clean part and second clean part separated\nsignalB_good1 = df_train.loc[3_500_000:3_642_932 - 1]['signal_processed_denoised'].values\nsignalB_good2 = df_train.loc[3_822_753 + 1:4_000_000 - 1]['signal_processed_denoised'].values\nchannelsB_good1 = df_train.loc[3_500_000:3_642_932 - 1]['open_channels'].values\nchannelsB_good2 = df_train.loc[3_822_753 + 1:4_000_000 - 1]['open_channels'].values\n\n# Test set signal and Bidirectional Viterbi predictions\nsignalC = df_test['signal_processed_denoised'].values\nchannelsC = pd.read_csv('..\/input\/ion-switching-0945-predictions\/gbdt_blend_submission.csv')['open_channels'].astype(np.uint8)","c7517105":"label = np.arange(len(signalA))\nchannel_list = np.arange(11)\nn_list = np.empty(11)\nmean_list = np.empty(11)\nstd_list = np.empty(11)\nstderr_list = np.empty(11)\n\nfig, axes = plt.subplots(ncols=2, figsize=(25, 8), dpi=100)\n\nfor i in range(11):\n    x = label[channelsA == i]\n    y = signalA[channelsA == i]\n    n_list[i] = np.size(y)\n    mean_list[i] = np.mean(y)\n    std_list[i] = np.std(y)\n    \n    axes[0].plot(x, y, '.', markersize=0.5, alpha=0.02)    \n    axes[0].tick_params(axis='x', labelsize=15)\n    axes[0].tick_params(axis='y', labelsize=15)\n    axes[0].set_title('Training Set Signal Processed Open Channels', size=18, pad=18)\n    \nstderr_list = std_list \/ np.sqrt(n_list)\nsample_weight = 1 \/ stderr_list\nchannel_list = channel_list.reshape(-1, 1)\n\nlr = LinearRegression()\nlr.fit(channel_list, mean_list, sample_weight=sample_weight)\nmean_predictA = lr.predict(channel_list)\n\nx = np.linspace(-0.5, 10.5, 5)\ny = lr.predict(x.reshape(-1, 1))\naxes[1].plot(x, y, label='Predicted Means')\naxes[1].plot(channel_list, mean_list, '.', markersize=8, label='Original Means')\naxes[1].legend()\n\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\naxes[1].set_title('Training Set Signal Processed Means', size=18, pad=18)\n\nprint('Predicted Means of signalA (Training Clean Signal):\\n', mean_predictA)\nplt.show()","d9eed693":"def remove_target_mean(signal, target, means):\n    signal_out = signal.copy()\n    for i in range(11):\n        signal_out[target == i] -= means[i]\n    return signal_out\n\nsig_A = remove_target_mean(signalA, channelsA, mean_predictA)\nsig_B1 = remove_target_mean(signalB_good1, channelsB_good1, mean_predictA)\nsig_B2 = remove_target_mean(signalB_good2, channelsB_good2, mean_predictA)\nsig_C = remove_target_mean(signalC, channelsC, mean_predictA)","810a7166":"fig, axes = plt.subplots(nrows=3, figsize=(25, 20), dpi=100)\naxes[0].plot(sig_A, linewidth=1)\naxes[1].plot(np.hstack((sig_B1, sig_B2)), linewidth=1)\naxes[2].plot(sig_C, linewidth=1)\n\nfor i in range(3):\n    axes[i].tick_params(axis='x', labelsize=12)\n    axes[i].tick_params(axis='y', labelsize=12)\naxes[0].set_title('Training Set (Without Batch 7) Signal Processed After Means are Subtracted', size=18, pad=18)\naxes[1].set_title('Training Set Batch 7 (Without Noisy Part) Signal Processed After Means are Subtracted', size=18, pad=18)\naxes[2].set_title('Test Set Signal Processed After Means are Subtracted', size=18, pad=18)\n\nplt.show()","2832cd02":"def bandstop(x, samplerate=1000000, fp=np.array([4925, 5075]), fs=np.array([4800, 5200])):    \n    fn = samplerate \/ 2\n    wp = fp \/ fn\n    ws = fs \/ fn\n    gpass = 1\n    gstop = 10.0\n\n    N, Wn = signal.buttord(wp, ws, gpass, gstop)\n    b, a = signal.butter(N, Wn, 'bandstop')\n    y = signal.filtfilt(b, a, x)\n    return y\n\ndef bandpass(x, samplerate=1000000, fp=np.array([4925, 5075]), fs=np.array([4800, 5200])):\n    fn = samplerate \/ 2\n    wp = fp \/ fn\n    ws = fs \/ fn\n    gpass = 1\n    gstop = 10.0\n\n    N, Wn = signal.buttord(wp, ws, gpass, gstop)\n    b, a = signal.butter(N, Wn, \"bandpass\")\n    y = signal.filtfilt(b, a, x)\n    return y","9bf2a106":"train_normalized_signals = np.split(sig_A, 9)\ntrain_original_signals = np.split(signalA, 9)\ntrain_filtered_signals = []\ntrain_supervised_noise = []\n\n# Denoising training batches except Batch 7\nfor batch, original_signal in enumerate(train_original_signals):\n    \n    normalized_signal = train_normalized_signals[batch]    \n    filtered_signal = bandstop(normalized_signal)\n    noise = bandpass(normalized_signal)\n    \n    if batch >= 7:\n        batch += 1\n    \n    plt.figure(figsize=(25, 5))\n    plt.title(f'Open Channels Denormalized - Training Batch {batch}', size=18, pad=18)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    plt.plot(normalized_signal, label='Original Signal', linewidth=0.5, alpha=0.5)\n    plt.plot(filtered_signal, label = 'Filtered Signal', linewidth=0.5, alpha=0.5)\n    plt.show()\n    \n    clean_signal = original_signal - noise\n    plt.figure(figsize=(25, 5))\n    plt.title(f'Signal Space - Training Batch {batch}', size=18, pad=18)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    plt.plot(original_signal, linewidth=0.5, alpha=0.5)\n    plt.plot(clean_signal, linewidth=0.5, alpha=0.5)\n    plt.show()\n\n    plt.figure(figsize=(25, 5))\n    plt.title(f'Signal Space - Training Batch {batch}', size=18, pad=18)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    plt.plot(original_signal, linewidth=0.5, alpha=0.5)\n    plt.twinx()\n    plt.plot(filtered_signal, linewidth=0.5, alpha=0.5, c='orange')\n    plt.show()\n   \n    train_filtered_signals.append(clean_signal)\n    train_supervised_noise.append(noise)\n    \n# Denoising Batch 7 Part 1\nbatch7_filtered_signal1 = bandstop(sig_B1)\nbatch7_noise1 = bandpass(sig_B1)\n\nplt.figure(figsize=(25, 5))\nplt.title(f'Open Channels Denormalized - Training Batch 7 Part 1', size=18, pad=18)\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.plot(sig_B1, label='Batch 7 Normalized Signal Part 1', linewidth=0.5, alpha=0.5)\nplt.plot(batch7_filtered_signal1, label = 'Batch 7 Filtered Signal Part 1', linewidth=0.5, alpha=0.5)\nplt.show()\n    \nbatch7_clean_signal1 = signalB_good1 - batch7_noise1\nplt.figure(figsize=(25, 5))\nplt.title(f'Batch 7 Signal Space - Part 1', size=18, pad=18)\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.plot(signalB_good1, linewidth=0.5, alpha=0.5)\nplt.plot(batch7_filtered_signal1, linewidth=0.5, alpha=0.5)\nplt.show()\n\nplt.figure(figsize=(25, 5))\nplt.title(f'Batch 7 Signal Space - Part 1', size=18, pad=18)\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.plot(signalB_good1, linewidth=0.5, alpha=0.5)\nplt.twinx()\nplt.plot(batch7_filtered_signal1, linewidth=0.5, alpha=0.5, c='orange')\nplt.show()\n\n# Denoising Batch 7 Part 2\nbatch7_filtered_signal2 = bandstop(sig_B2)\nbatch7_noise2 = bandpass(sig_B2)\n\nplt.figure(figsize=(25, 5))\nplt.title(f'Open Channels Denormalized - Training Batch 7 Part 2', size=18, pad=18)\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.plot(sig_B2, label='Batch 7 Normalized Signal Part 2', linewidth=0.5, alpha=0.5)\nplt.plot(batch7_filtered_signal2, label = 'Batch 7 Filtered Signal Part 2', linewidth=0.5, alpha=0.5)\nplt.show()\n    \nbatch7_clean_signal2 = signalB_good2 - batch7_noise2\nplt.figure(figsize=(25, 5))\nplt.title(f'Batch 7 Signal Space - Part 2', size=18, pad=18)\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.plot(signalB_good2, linewidth=0.5, alpha=0.5)\nplt.plot(batch7_filtered_signal2, linewidth=0.5, alpha=0.5)\nplt.show()\n\nplt.figure(figsize=(25, 5))\nplt.title(f'Batch 7 Signal Space - Part 2', size=18, pad=18)\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.plot(signalB_good2, linewidth=0.5, alpha=0.5)\nplt.twinx()\nplt.plot(batch7_filtered_signal2, linewidth=0.5, alpha=0.5, c='orange')\nplt.show()\n    \ndf_train.loc[df_train['batch'] == 0, 'signal_processed_denoised'] = train_filtered_signals[0]\ndf_train.loc[df_train['batch'] == 1, 'signal_processed_denoised'] = train_filtered_signals[1]\ndf_train.loc[df_train['batch'] == 2, 'signal_processed_denoised'] = train_filtered_signals[2]\ndf_train.loc[df_train['batch'] == 3, 'signal_processed_denoised'] = train_filtered_signals[3]\ndf_train.loc[df_train['batch'] == 4, 'signal_processed_denoised'] = train_filtered_signals[4]\ndf_train.loc[df_train['batch'] == 5, 'signal_processed_denoised'] = train_filtered_signals[5]\ndf_train.loc[df_train['batch'] == 6, 'signal_processed_denoised'] = train_filtered_signals[6]\ndf_train.loc[df_train['batch'] == 6, 'signal_processed_denoised'] = train_filtered_signals[6]\ndf_train.loc[3500000:3642932 - 1, 'signal_processed_denoised'] = batch7_clean_signal1\ndf_train.loc[3822753 + 1:4000_000 - 1, 'signal_processed_denoised'] = batch7_clean_signal2\ndf_train.loc[df_train['batch'] == 8, 'signal_processed_denoised'] = train_filtered_signals[7]\ndf_train.loc[df_train['batch'] == 9, 'signal_processed_denoised'] = train_filtered_signals[8]","084df3f3":"test_normalized_signals1 = np.split(sig_C[:1000000], 10)\ntest_original_signals1 = np.split(signalC[:1000000], 10)\ntest_normalized_signal2 = sig_C[1000000:]\ntest_original_signal2 = signalC[1000000:]\ntest_filtered_signals = []\ntest_supervised_noise = []\n\n# Denoising test set sub batches part by part\nfor sub_batch, original_signal in enumerate(test_original_signals1):\n    \n    normalized_signal = test_normalized_signals1[sub_batch]    \n    filtered_signal = bandstop(normalized_signal)\n    noise = bandpass(normalized_signal)\n        \n    plt.figure(figsize=(25, 5))\n    plt.title(f'Open Channels Denormalized - Test Sub-Batch {sub_batch}', size=18, pad=18)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    plt.plot(normalized_signal, label='Original Signal', linewidth=0.5, alpha=0.5)\n    plt.plot(filtered_signal, label = 'Filtered Signal', linewidth=0.5, alpha=0.5)\n    plt.show()\n    \n    clean_signal = original_signal - noise\n    plt.figure(figsize=(25, 5))\n    plt.title(f'Signal Space - Test Sub-Batch {sub_batch}', size=18, pad=18)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    plt.plot(original_signal, linewidth=0.5, alpha=0.5)\n    plt.plot(clean_signal, linewidth=0.5, alpha=0.5)\n    plt.show()\n\n    plt.figure(figsize=(25, 5))\n    plt.title(f'Signal Space - Test Sub-Batch {sub_batch}', size=18, pad=18)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    plt.plot(original_signal, linewidth=0.5, alpha=0.5)\n    plt.twinx()\n    plt.plot(filtered_signal, linewidth=0.5, alpha=0.5, c='orange')\n    plt.show()\n   \n    test_filtered_signals.append(clean_signal)\n    test_supervised_noise.append(noise)\n        \n# Denoising test set second half\ntest_filtered_signal2 = bandstop(test_normalized_signal2)\ntest_noise2 = bandpass(test_normalized_signal2)\n\nplt.figure(figsize=(25, 5))\nplt.title(f'Open Channels Denormalized - Test Batch 2 & 3 (Second Half)', size=18, pad=18)\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.plot(test_normalized_signal2, label='Original Signal', linewidth=0.5, alpha=0.5)\nplt.plot(test_filtered_signal2, label = 'Filtered Signal', linewidth=0.5, alpha=0.5)\nplt.show()\n\ntest_clean_signal2 = test_original_signal2 - test_noise2\nplt.figure(figsize=(25, 5))\nplt.title(f'Signal Space - Test Batch 2 & 3 (Second Half)', size=18, pad=18)\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.plot(test_original_signal2, linewidth=0.5, alpha=0.5)\nplt.plot(test_clean_signal2, linewidth=0.5, alpha=0.5)\nplt.show()\n\nplt.figure(figsize=(25, 5))\nplt.title(f'Signal Space - Test Batch 2 & 3 (Second Half)', size=18, pad=18)\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.plot(test_original_signal2, linewidth=0.5, alpha=0.5)\nplt.twinx()\nplt.plot(test_filtered_signal2, linewidth=0.5, alpha=0.5, c='orange')\nplt.show()\n\ndf_test.loc[0:100000 - 1, 'signal_processed_denoised'] = test_filtered_signals[0]\ndf_test.loc[100000:200000 - 1, 'signal_processed_denoised'] = test_filtered_signals[1]\ndf_test.loc[200000:300000 - 1, 'signal_processed_denoised'] = test_filtered_signals[2]\ndf_test.loc[300000:400000 - 1, 'signal_processed_denoised'] = test_filtered_signals[3]\ndf_test.loc[400000:500000 - 1, 'signal_processed_denoised'] = test_filtered_signals[4]\ndf_test.loc[500000:600000 - 1, 'signal_processed_denoised'] = test_filtered_signals[5]\ndf_test.loc[600000:700000 - 1, 'signal_processed_denoised'] = test_filtered_signals[6]\ndf_test.loc[700000:800000 - 1, 'signal_processed_denoised'] = test_filtered_signals[7]\ndf_test.loc[800000:900000 - 1, 'signal_processed_denoised'] = test_filtered_signals[8]\ndf_test.loc[900000:1000000 - 1, 'signal_processed_denoised'] = test_filtered_signals[9]\ndf_test.loc[1000000:, 'signal_processed_denoised'] = test_clean_signal2","4feedca2":"fig, axes = plt.subplots(nrows=2, figsize=(20, 14), dpi=100)\n\ndf_train.set_index('time')['signal_processed'].plot(label='Signal', ax=axes[0], alpha=0.5)\ndf_train.set_index('time')['signal_processed_denoised'].plot(label='Signal Denoised', ax=axes[0], alpha=0.5)\nfor batch in np.arange(0, 550, 50):\n    axes[0].axvline(batch, color='r', linestyle='--', lw=2)\n    \ndf_test.set_index('time')['signal_processed'].plot(label='Signal', ax=axes[1], alpha=0.5)\ndf_test.set_index('time')['signal_processed_denoised'].plot(label='Signal Denoised', ax=axes[1], alpha=0.5)\n\nfor batch in np.arange(500, 600, 10):\n    axes[1].axvline(batch, color='r', linestyle='--', lw=2)\nfor batch in np.arange(600, 700, 50):\n    axes[1].axvline(batch, color='r', linestyle='--', lw=2)\n    \naxes[1].axvline(560, color='y', linestyle='dotted', lw=8)\n\nfor i in range(2):    \n    for batch in np.arange(0, 550, 50):\n        axes[i].axvline(batch, color='r', linestyle='--', lw=2)\n        \n    axes[i].set_xlabel('Time', size=15)\n    axes[i].tick_params(axis='x', labelsize=12)\n    axes[i].tick_params(axis='y', labelsize=12)\n    axes[i].legend()\n    \naxes[0].set_title('Training Set Batches Processed and Denoised', size=18, pad=18)\naxes[1].set_title('Public\/Private Test Set Batches and Sub-batches Processed and Denoised', size=18, pad=18)\n\nplt.show()","cb4880ef":"scaler = MinMaxScaler()\n\nfor i, batch in enumerate(df_train.groupby('batch')):\n    time = df_train.loc[(df_train['batch'] == i), 'time'].values.reshape(-1, 1)\n    df_train.loc[(df_train['batch'] == i), 'time_scaled'] = scaler.fit_transform(time)\n\nfor i, batch in enumerate(df_test.groupby('batch')):\n    time = df_test.loc[(df_test['batch'] == i), 'time'].values.reshape(-1, 1)\n    df_test.loc[(df_test['batch'] == i), 'time_scaled'] = scaler.fit_transform(time)\n    \ndf_train['time_scaled'] = df_train['time_scaled'].astype(np.float32)\ndf_test['time_scaled'] = df_test['time_scaled'].astype(np.float32)","7cc6e76f":"def kalman(signal, signal_covariance):\n    \n    kf = KalmanFilter(initial_state_mean=signal[0], \n                      initial_state_covariance=signal_covariance,\n                      observation_covariance=signal_covariance, \n                      transition_covariance=0.1,\n                      transition_matrices=1)\n    \n    pred_state, state_cov = kf.smooth(signal)\n    return pred_state","9a899c4a":"%%time\n\n# filter model 0\nprint(f'\\n---------- Model 0 ----------\\n')\n\nbatch0_corr = np.corrcoef(df_train[df_train['batch'] == 0]['signal_processed'], df_train[df_train['batch'] == 0]['open_channels'])[0][1]\ndf_train.loc[df_train['batch'] == 0, 'signal_processed_kalman'] = kalman(df_train[df_train['batch'] == 0]['signal_processed'].values, 0.6)\nfiltered_batch0_corr = np.corrcoef(df_train[df_train['batch'] == 0]['signal_processed_kalman'], df_train[df_train['batch'] == 0]['open_channels'])[0][1]\nprint(f'Training Batch 0 - Correlation between Signal and Open Channels increased from {batch0_corr:.6} to {filtered_batch0_corr:.6} (Covariance: {0.6})')\n\nbatch1_corr = np.corrcoef(df_train[df_train['batch'] == 1]['signal_processed'], df_train[df_train['batch'] == 1]['open_channels'])[0][1]\ndf_train.loc[df_train['batch'] == 1, 'signal_processed_kalman'] = kalman(df_train[df_train['batch'] == 1]['signal_processed'].values, 0.45)\nfiltered_batch1_corr = np.corrcoef(df_train[df_train['batch'] == 1]['signal_processed_kalman'], df_train[df_train['batch'] == 1]['open_channels'])[0][1]\nprint(f'Training Batch 1 - Correlation between Signal and Open Channels increased from {batch1_corr:.6} to {filtered_batch1_corr:.6} (Covariance: {0.45})')\n\nprint(f'Test Batch 0 Sub-batch 0 (Mean Model 0 Covariance: {0.525})')\ndf_test.loc[df_test.query('batch == 0 and (500 < time <= 510)').index, 'signal_processed_kalman'] = kalman(df_test.loc[df_test.query('batch == 0 and (500 < time <= 510)').index, 'signal_processed'].values, 0.525)\n\n# filter model 1\nprint(f'\\n---------- Model 1 ----------\\n')\n\nbatch2_corr = np.corrcoef(df_train[df_train['batch'] == 2]['signal_processed'], df_train[df_train['batch'] == 2]['open_channels'])[0][1]\ndf_train.loc[df_train['batch'] == 2, 'signal_processed_kalman'] = kalman(df_train[df_train['batch'] == 2]['signal_processed'].values, 0.03)\nfiltered_batch2_corr = np.corrcoef(df_train[df_train['batch'] == 2]['signal_processed_kalman'], df_train[df_train['batch'] == 2]['open_channels'])[0][1]\nprint(f'Training Batch 2 - Correlation between Signal and Open Channels increased from {batch2_corr:.6} to {filtered_batch2_corr:.6} (Covariance: {0.03})')\n\nbatch6_corr = np.corrcoef(df_train[df_train['batch'] == 6]['signal_processed'], df_train[df_train['batch'] == 6]['open_channels'])[0][1]\ndf_train.loc[df_train['batch'] == 6, 'signal_processed_kalman'] = kalman(df_train[df_train['batch'] == 6]['signal_processed'].values, 0.03)\nfiltered_batch6_corr = np.corrcoef(df_train[df_train['batch'] == 6]['signal_processed_kalman'], df_train[df_train['batch'] == 6]['open_channels'])[0][1]\nprint(f'Training Batch 6 - Correlation between Signal and Open Channels increased from {batch6_corr:.6} to {filtered_batch6_corr:.6} (Covariance: {0.03})')\n\nprint(f'Test Batch 0 Sub-batch 4 (Mean Model 1 Covariance: {0.03})')\ndf_test.loc[df_test.query('batch == 0 and (540 < time <= 550)').index, 'signal_processed_kalman'] = kalman(df_test.loc[df_test.query('batch == 0 and (540 < time <= 550)').index, 'signal_processed'].values, 0.03)\n\n# filter model 1.5\nprint(f'\\n---------- Model 1.5 ----------\\n')\n\nprint(f'Test Batch 0 Sub-batch 3 (Covariance: {0.1})')\ndf_test.loc[df_test.query('batch == 0 and (530 < time <= 540)').index, 'signal_processed_kalman'] = kalman(df_test.loc[df_test.query('batch == 0 and (530 < time <= 540)').index, 'signal_processed'].values, 0.1)\n\nprint(f'Test Batch 1 Sub-batch 3 (Covariance: {0.1})')\ndf_test.loc[df_test.query('batch == 1 and (580 < time <= 590)').index, 'signal_processed_kalman'] = kalman(df_test.loc[df_test.query('batch == 1 and (580 < time <= 590)').index, 'signal_processed'].values, 0.1)\n\nprint(f'Test Batch 2 (Covariance: {0.1})')\ndf_test.loc[df_test.query('batch == 2').index, 'signal_processed_kalman'] = kalman(df_test.loc[df_test.query('batch == 2').index, 'signal_processed'].values, 0.1)\nprint(f'Test Batch 3 (Covariance: {0.1})')\ndf_test.loc[df_test.query('batch == 3').index, 'signal_processed_kalman'] = kalman(df_test.loc[df_test.query('batch == 3').index, 'signal_processed'].values, 0.1)\n\n# filter model 2\nprint(f'\\n---------- Model 2 ----------\\n')\n\nbatch3_corr = np.corrcoef(df_train[df_train['batch'] == 3]['signal_processed'], df_train[df_train['batch'] == 3]['open_channels'])[0][1]\ndf_train.loc[df_train['batch'] == 3, 'signal_processed_kalman'] = kalman(df_train[df_train['batch'] == 3]['signal_processed'].values, 0.01)\nfiltered_batch3_corr = np.corrcoef(df_train[df_train['batch'] == 3]['signal_processed_kalman'], df_train[df_train['batch'] == 3]['open_channels'])[0][1]\nprint(f'Training Batch 3 - Correlation between Signal and Open Channels increased from {batch3_corr:.6} to {filtered_batch3_corr:.6} (Covariance: {0.01})')\n\nbatch7_corr = np.corrcoef(df_train[(df_train['batch'] == 7) & (df_train['is_filtered'] != 1)]['signal_processed'], df_train[(df_train['batch'] == 7) & (df_train['is_filtered'] != 1)]['open_channels'])[0][1]\ndf_train.loc[(df_train['batch'] == 7) & (df_train['is_filtered'] != 1), 'signal_processed_kalman'] = kalman(df_train[(df_train['batch'] == 7) & (df_train['is_filtered'] != 1)]['signal_processed'].values, 0.01)\nfiltered_batch7_corr = np.corrcoef(df_train[(df_train['batch'] == 7) & (df_train['is_filtered'] != 1)]['signal_processed_kalman'], df_train[(df_train['batch'] == 7) & (df_train['is_filtered'] != 1)]['open_channels'])[0][1]\nprint(f'Training Batch 7 - Correlation between Signal and Open Channels increased from {batch7_corr:.6} to {filtered_batch7_corr:.6} (Covariance: {0.01})')\n\nprint(f'Test Batch 0 Sub-batch 1 (Mean Model 2 Covariance: {0.01})')\ndf_test.loc[df_test.query('batch == 0 and (510 < time <= 520)').index, 'signal_processed_kalman'] = kalman(df_test.loc[df_test.query('batch == 0 and (510 < time <= 520)').index, 'signal_processed'].values, 0.01)\nprint(f'Test Batch 1 Sub-batch 4 (Mean Model 2 Covariance: {0.01})')\ndf_test.loc[df_test.query('batch == 1 and (590 < time <= 600)').index, 'signal_processed_kalman'] = kalman(df_test.loc[df_test.query('batch == 1 and (590 < time <= 600)').index, 'signal_processed'].values, 0.01)\n\n# filter model 3\nprint(f'\\n---------- Model 3 ----------\\n')\n\nbatch5_corr = np.corrcoef(df_train[df_train['batch'] == 5]['signal_processed'], df_train[df_train['batch'] == 5]['open_channels'])[0][1]\ndf_train.loc[df_train['batch'] == 5, 'signal_processed_kalman'] = kalman(df_train[df_train['batch'] == 5]['signal_processed'].values, 0.005)\nfiltered_batch5_corr = np.corrcoef(df_train[df_train['batch'] == 5]['signal_processed_kalman'], df_train[df_train['batch'] == 5]['open_channels'])[0][1]\nprint(f'Training Batch 5 - Correlation between Signal and Open Channels increased from {batch5_corr:.6} to {filtered_batch5_corr:.6} (Covariance: {0.005})')\n\nbatch8_corr = np.corrcoef(df_train[df_train['batch'] == 8]['signal_processed'], df_train[df_train['batch'] == 8]['open_channels'])[0][1]\ndf_train.loc[df_train['batch'] == 8, 'signal_processed_kalman'] = kalman(df_train[df_train['batch'] == 8]['signal_processed'].values, 0.005)\nfiltered_batch8_corr = np.corrcoef(df_train[df_train['batch'] == 8]['signal_processed_kalman'], df_train[df_train['batch'] == 8]['open_channels'])[0][1]\nprint(f'Training Batch 8 - Correlation between Signal and Open Channels increased from {batch8_corr:.6} to {filtered_batch8_corr:.6} (Covariance: {0.005})')\n\nprint(f'Test Batch 0 Sub-batch 2 - (Mean Model 3 Covariance: {0.005})')\ndf_test.loc[df_test.query('batch == 0 and (520 < time <= 530)').index, 'signal_processed_kalman'] = kalman(df_test.loc[df_test.query('batch == 0 and (520 < time <= 530)').index, 'signal_processed'].values, 0.005)\nprint(f'Test Batch 1 Sub-batch 1 - (Mean Model 3 Covariance: {0.005})')\ndf_test.loc[df_test.query('batch == 1 and (560 < time <= 570)').index, 'signal_processed_kalman'] = kalman(df_test.loc[df_test.query('batch == 1 and (560 < time <= 570)').index, 'signal_processed'].values, 0.005)\n\n# filter model 4\nprint(f'\\n---------- Model 4 ----------\\n')\n\nbatch4_corr = np.corrcoef(df_train[df_train['batch'] == 4]['signal_processed'], df_train[df_train['batch'] == 4]['open_channels'])[0][1]\ndf_train.loc[df_train['batch'] == 4, 'signal_processed_kalman'] = kalman(df_train[df_train['batch'] == 4]['signal_processed'].values, 0.005)\nfiltered_batch4_corr = np.corrcoef(df_train[df_train['batch'] == 4]['signal_processed_kalman'], df_train[df_train['batch'] == 4]['open_channels'])[0][1]\nprint(f'Training Batch 4 - Correlation between Signal and Open Channels increased from {batch4_corr:.6} to {filtered_batch4_corr:.6} (Covariance: {0.005})')\n\nbatch9_corr = np.corrcoef(df_train[df_train['batch'] == 9]['signal_processed'], df_train[df_train['batch'] == 9]['open_channels'])[0][1]\ndf_train.loc[df_train['batch'] == 9, 'signal_processed_kalman'] = kalman(df_train[df_train['batch'] == 9]['signal_processed'].values, 0.005)\nfiltered_batch9_corr = np.corrcoef(df_train[df_train['batch'] == 9]['signal_processed_kalman'], df_train[df_train['batch'] == 9]['open_channels'])[0][1]\nprint(f'Training Batch 9 - Correlation between Signal and Open Channels increased from {batch9_corr:.6} to {filtered_batch9_corr:.6} (Covariance: {0.005})')\n\nprint(f'Test Batch 1 Sub-batch 0 - (Mean Model 4 Covariance: {0.005})')\ndf_test.loc[df_test.query('batch == 1 and (550 < time <= 560)').index, 'signal_processed_kalman'] = kalman(df_test.loc[df_test.query('batch == 1 and (550 < time <= 560)').index, 'signal_processed'].values, 0.005)\nprint(f'Test Batch 1 Sub-batch 2 - (Mean Model 4 Covariance: {0.005})')\ndf_test.loc[df_test.query('batch == 1 and (570 < time <= 580)').index, 'signal_processed_kalman'] = kalman(df_test.loc[df_test.query('batch == 1 and (570 < time <= 580)').index, 'signal_processed'].values, 0.005)\n","a8516ff0":"fig, axes = plt.subplots(nrows=2, figsize=(20, 14), dpi=100)\n\ndf_train.set_index('time')['signal_processed'].plot(label='Signal', ax=axes[0], alpha=0.4)\ndf_train.set_index('time')['signal_processed_kalman'].plot(label='Signal Kalman Filtered', ax=axes[0], alpha=0.8)\nfor batch in np.arange(0, 550, 50):\n    axes[0].axvline(batch, color='r', linestyle='--', lw=2)\n    \ndf_test.set_index('time')['signal_processed'].plot(label='Signal', ax=axes[1], alpha=0.4)\ndf_test.set_index('time')['signal_processed_kalman'].plot(label='Signal Kalman Filtered', ax=axes[1], alpha=0.8)\n\nfor batch in np.arange(500, 600, 10):\n    axes[1].axvline(batch, color='r', linestyle='--', lw=2)\nfor batch in np.arange(600, 700, 50):\n    axes[1].axvline(batch, color='r', linestyle='--', lw=2)\n    \naxes[1].axvline(560, color='y', linestyle='dotted', lw=8)\n\nfor i in range(2):    \n    for batch in np.arange(0, 550, 50):\n        axes[i].axvline(batch, color='r', linestyle='--', lw=2)\n        \n    axes[i].set_xlabel('Time', size=15)\n    axes[i].tick_params(axis='x', labelsize=12)\n    axes[i].tick_params(axis='y', labelsize=12)\n    axes[i].legend()\n    \naxes[0].set_title('Training Set Batches Raw\/Filtered', size=18, pad=18)\naxes[1].set_title('Public\/Private Test Set Batches and Sub-batches Raw\/Filtered', size=18, pad=18)\n\nplt.show()","834468c4":"df_train.to_pickle('train.pkl')\ndf_test.to_pickle('test.pkl')\n\nprint('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() \/ 1024**2))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() \/ 1024**2))","8898ffdd":"50X zoom is applied to `signal_processed` between **681-682** to see levels clearly. The assumptions of levels are based on entire **Test Batch 2**, not the slice.\n\n### Test Batch 3 Levels\n\nThere are **3** or **4** levels in **Test Batch 3**:\n* Values between **-4** and **-2** are more likely to be **0** `open_channels` \n* Values closer to **-2** are more likely to be **1** `open_channels` \n* Values closer to **-1** are more likely to be **2** `open_channels` \n* Values greater than **-1** are more likely to be **3** `open_channels`","73f098b4":"Another approach is replacing the outliers with the mean `signal_processed` calculated on **Training Batch 3** and **Training Batch 7** (except the noisy part) for every `open_channels`. Using raw means might be prone to overfitting so adding a small random noise is a better option.","29bbf9f9":"### **2.4. Sine Transformation**\nSine transformation function is defined as $\\mathcal{S}(X) = X + (5 \\times \\sin(s \\times \\pi \\div N))$\n\n* $X$ is the given `signal` vector with size $N$\n* $s$ is a straight line with size $N$\n* $5$ is the constant multiplied with $sin$\n\nSine transformation can be seen on several batches. Instead of **10** second time frames, this transformation is applied to entire batches. It can be seen on **Training Batch 6**, **Training Batch 7**, **Training Batch 8**, **Training Batch 9**, **Test Batch 2**.\n\nThis transformation looks very unnatural because it looks like a seasonal trend. It messes up the ranges of `open_channels` and it has to be removed in order to make the distribution smoother. Sine removal function is basically the same function above with subtraction instead of addition.","2a886e24":"### **2.2. Decimal Noise**\n`signal` is written with **4** decimal places in dataset. Random noise might be applied to decimal places of `signal`. Rounding `signal` could be useful in some models.","1523ede6":"### **3.1. Training Batch 0**\n\n**Training Batch 0** is the first batch of training set. It has a very simple distribution. There are lots of concurrent spikes in `signal` and `open_channels` which indicate the increases in target. `open_channels` and `signal` are correlated with each other because of this reason.\n\n`open_channels` range is not accurate because there are couple outliers that are greater than **0** in `signal`. Either they can be removed or replaced with max `signal` value of its class (**0**).","1360379e":"All outliers in **Training Batch 0** are **0** `open_channels`. Mean signal for **0** `open_channels` is calculated for **Training Batch 0** after dropping those samples, and `signal` of those samples are replaced with it. After dealing with the outliers, distribution looks more natural and correlation increased by **0.0006**.\n\n### Training Batch 0 Levels\n\nThere are **2** levels in **Training Batch 0**:\n* **0** `open_channels` are between **-3.8506** and **-0.8545**\n* **1** `open_channels` are between **-2.4036** and **-0.4163**","ebde5e7e":"Removing the effects of sine transformation from `signal` cleans the entire **Training Batch 8**. Cleaning **Training Batch 8** increased the correlation from **0.6202** to **0.9742**.","6dc9ffbe":"### **4.2. Ghost Drift**\nEven though `signal_processed` is smooth for every batch, there is an anomaly on model 4's distribution. Model 4's signal is negatively shifted. That distribution has different means than other groups for every `open_channels` value. It can be shifted back to normal by adding `np.exp(1)` to `signal_processed`.","2256eac7":"50X zoom is applied to `signal` between **470-471** to see the levels clearly but it still very hard separate levels from each other. The boundaries of levels are from whole **Training Batch 9**, not from the slice.\n\n### Training Batch 9 Levels\n\nThere are **10** levels in **Training Batch 9**:\n* **1** `open_channels` are between **-4.99713** and **-3.1768**\n* **2** `open_channels` are between **-4.2639** and **-1.45995**\n* **3** `open_channels` are between **-3.4975** and **-0.473576**\n* **4** `open_channels` are between **-2.04313** and **1.04354**\n* **5** `open_channels` are between **-0.895961** and **2.26227**\n* **6** `open_channels` are between **0.251721** and **3.80713**\n* **7** `open_channels` are between **1.29101** and **4.95493**\n* **8** `open_channels` are between **2.68158** and **6.20222**\n* **9** `open_channels` are between **4.02257** and **7.29556**\n* **10** `open_channels` are between **5.02223** and **8.51555**","26062459":"### **2.10. Training Batch 9**\nIn **Training Batch 9**, `open_channels` is not correlated with `signal` because the entire batch is transformed with sine function. If the effects of that transformation are ignored, **Training Batch 9** has the same distribution with **Training Batch 4**. Besides the sine transformation, there is no outlier in this batch.","4fe2a1f5":"### **3.3. Training Batch 2**\nIn **Training Batch 2**, `open_channels` is extremely correlated with `signal`. There are very few **0** `open_channels` in **Training Batch 2**, that's why instead of concurrent spikes, there are vertical white lines.\n\nDescriptive statistics are accurate because there are no outliers, trends or anomalies in **Training Batch 2**.","dcb65b92":"### **3.7. Training Batch 6**\nIn **Training Batch 6**, `open_channels` is not correlated with `signal` because the entire batch is transformed with sine function. If the effects of that transformation are ignored, **Training Batch 6** has the same distribution with **Training Batch 2**. There are very few **0** `open_channels` in **Training Batch 6** that's why instead of concurrent spikes, there are vertical white lines. Those concurrent vertical white lines are not effected by sine transformation so it is safe to remove it. Besides the sine transformation, there is no outlier in this batch.","cd7af99d":"### **2.10. Test Batch 0**\n**Test Batch 0** is the first batch of test set and 5\/6 of public leaderboard. It is very different from training batches. Every **100,000** samples (**10** seconds) have different distributions. Distribution change in every precise **10** seconds doesn't look very natural. This could be the random noise applied by the organizers. Every **100,000** samples have to be analyzed and cleaned separately for **Test Batch 0**.","e474d0d2":"Fourth **100,000** samples of **Test Batch 0** might have outliers. The distribution is unique and it doesn't look like any batch from training set, but it could have the same levels with **Training Batch 3** and **Training Batch 7** or **Training Batch 0** and **Training Batch 1**.\n\n### Test Batch 0 (4th 100,000) Levels\n\nThere are **3** or **4** levels in **Test Batch 0** fourth **100,000** samples:\n* Values between **-3** and **-2** are more likely to be **0** `open_channels` \n* Values closer to **-2** are more likely to be **1** `open_channels` \n* Values closer to **-1** are more likely to be **2** `open_channels` or they are outliers\n* Values greater than **-1** are more likely to be **3** `open_channels` or they are outliers\n","b1419c9a":"### **3.6. Training Batch 5**\nIn **Training Batch 5**, `open_channels` is extremely correlated with `signal` and there are **6** unique values in it. It is not hard to detect levels compared to previous example since there are few of them.\n\nDescriptive statistics are accurate because there are no outliers, trends or anomalies in **Training Batch 5**","3ff858b4":"Fourth **100,000** samples of **Test Batch 1** have ramp transformation. After the ramp is removed, the distribution looks very similar to **Test Batch 0** fourth **100,000** samples. The distribution is unique and it doesn't look like any batch from training set, but it could have the same levels with **Training Batch 3** and **Training Batch 7**.\n\n### Test Batch 1 (4th 100,000) Levels\n\nThere are **4** levels in **Test Batch 0** fourth **100,000** samples:\n* Values between **-3** and **-2** are more likely to be **0** `open_channels` \n* Values closer to **-2** are more likely to be **1** `open_channels` \n* Values closer to **-1** are more likely to be **2** `open_channels` \n* Values greater than **-1** are more likely to be **3** `open_channels` ","166c3c7b":"### **1.2. Batch Target Distribution**\nBatches have very different target distributions as seen below. Every class from **0** to **10** doesn't exist in every batch. However, every batch is almost identical with another batch in terms of target distribution. Those similar batch pairs might be from the same sample.\n\n* **Training Batch 0** and **Training Batch 1** have the same class distribution. They both have extremely high number of **0**  `open_channels` and low number of **1** `open_channels`.\n* **Training Batch 2** and **Training Batch 6** have the same class distribution. They both have high number of **1** `open_channels` and low number of **0** `open_channels`.\n* **Training Batch 3** and **Training Batch 7** have the same class distribution. They both almost have the same number of **0**, **1**, **2**, **3** `open_channels`.\n* **Training Batch 5** and **Training Batch 8** have the same class distribution. They both almost have the same number of **0**, **1**, **2**, **3**, **4**, **5** `open_channels`.\n* **Training Batch 4** and **Training Batch 9** have the same class distribution. They both almost have the same number of **1**, **2**, **3**, **4**, **5**, **6**, **7**, **8**, **9**, **10** `open_channels`. The only difference between those two batches is there are 2 **0** `open_channels` in **Training Batch 4** which doesn't exist in **Training Batch 9**.","2e6f3d4d":"First **100,000** samples of **Test Batch 0** have ramp transformation. After the ramp is removed, levels can be seen clearly. The distribution looks very similar to **Training Batch 0** and **Training Batch 1**.\n\n### Test Batch 0 (1st 100,000) Levels\n\nThere are **2** levels in **Test Batch 0** first **100,000** samples:\n* Values between **-3.25** and **-2** are more likely to be **0** `open_channels` \n* Spikes greater than **-1** are more likely to be **1** `open_channels`","9295afdb":"### **2.8. Training Batch 7**\n\nIn **Training Batch 7**, `open_channels` is not correlated with `signal` because the entire batch is transformed with sine function. Another problem is, middle part has higher standard deviation than other parts. If the effects of sine transformation is ignored and middle part is scaled, **Training Batch 7** has the same distribution with **Training Batch 3**. There are **4** unique `open_channels` in **Training Batch 7** just like **Training Batch 3**, and their `open_channels` ranges are very similar.","5b7194cb":"First **100,000** samples of **Test Batch 1** doesn't have noise. The distribution looks very similar to **Training Batch 4** and **Training Batch 9**. Those two batches have at least **9** levels so it is very hard to detect them in test set.\n\n### Test Batch 1 (1st 100,000) Levels\n\nThere are **10** levels in **Test Batch 0** first **100,000** samples:\n* Values closer to **-5** are more likely to be **0** `open_channels` \n* Values closer to **-4** are more likely to be **1** `open_channels`\n* Values closer to **-3** are more likely to be **2** `open_channels`\n* Values closer to **-2** are more likely to be **3** `open_channels`\n* Values closer to **-1** are more likely to be **4** `open_channels`\n* Values closer to **0** are more likely to be **5** `open_channels`\n* Values closer to **2** are more likely to be **6** `open_channels`\n* Values closer to **4** are more likely to be **7** `open_channels`\n* Values closer to **5** are more likely to be **8** `open_channels`\n* Values closer to **6** are more likely to be **9** `open_channels`\n* Values greater than **7** are more likely to be **10** `open_channels`","80cf338d":"### **5.1. Time Scaled**\nIn some batches, `signal` is affected by `time`. That effect is clearly visible on **Batch 6**, **Batch 7**, **Batch 8**, **Batch 9** in training set, and **Batch 2** in test set, but their `signal` is cleaned. `time` can't be used as a predictor by itself because of the covariance shift, but it can be scaled on `batch` level. The values of `time` will be between **0** and **1** when it is scaled.","0775afaa":"### **5.2. Kalman Filter**\nKalman filter is applied to `signal_processed` in order to reduce the noise, but it is applied separately to every different signal. For training set, it is applied to every batch separately, and for test set, it is applied to every sub batch separately.\n\nEvery `signal_covariance` is tuned based on the filtered `signal_processed` and `open_channels` correlation. Mean of covariances returning the highest correlation in training set, are used for the test set signals.","212cdaa2":"### **2.12. Test Batch 2**\n**Test Batch 2** is transformed with sine function. If the effects of that transformation are ignored, **Test Batch 2** has the same distribution **Test Batch 0**'s and **Test Batch 1**'s fourth **100,000** samples.","0e9b7064":"50X zoom is applied to `signal` between **190-191** to see the levels clearly. The boundaries of levels are from whole **Training Batch 3**, not from the slice.\n\n### Training Batch 3 Levels\n\nThere are **4** levels in **Training Batch 3**:\n* **0** `open_channels` are between **-3.7073** and **-1.7493**\n* **1** `open_channels` are between **-2.7763** and **-0.3116**\n* **2** `open_channels` are between **-1.5271** and **0.9555**\n* **3** `open_channels` are between **-0.2341** and **2.2404**","a8810c11":"Third **100,000** samples of **Test Batch 1** have ramp transformation. After the ramp is removed, the distribution looks very similar to **Training Batch 4** and **Training Batch 9**. Those two batches have at least **9** levels so it is very hard to detect them in test set.\n\n### Test Batch 1 (3rd 100,000) Levels\n\nThere are **10** levels in **Test Batch 0** third **100,000** samples:\n* Values closer to **-4** are more likely to be **0** `open_channels` \n* Values closer to **-3** are more likely to be **1** `open_channels`\n* Values closer to **-2** are more likely to be **2** `open_channels`\n* Values closer to **-1** are more likely to be **3** `open_channels`\n* Values closer to **1** are more likely to be **4** `open_channels`\n* Values closer to **2** are more likely to be **5** `open_channels`\n* Values closer to **3** are more likely to be **6** `open_channels`\n* Values closer to **4** are more likely to be **7** `open_channels`\n* Values closer to **5** are more likely to be **8** `open_channels`\n* Values closer to **6** are more likely to be **9** `open_channels`\n* Values greater than **7** are more likely to be **10** `open_channels`","58070261":"Removing the effects of sine transformation from `signal` cleans the entire **Training Batch 6**. Cleaning **Training Batch 6** increased the correlation from **0.319** to **0.9062**.\n\n### Training Batch 6 Levels\n\nThere are **2** levels in **Training Batch 6**:\n* **0** `open_channels` are between **-3.90221** and **-1.69979**\n* **1** `open_channels` are between **-2.60893** and **-0.285418**","125c15fc":"### **1.1. Global Target Distribution**\nClasses are not balanced in training set. Most of the time, when the `open_channels` increases, its value count decreases, but not necessarily. Higher `open_channels` values are less common. **0** is the most common class and **10** is the least common class.","6786ad55":"## **5. Scaling and Filtering**","4b28b62f":"`report_training_batch` and `report_test_batch` helper functions are made for visualizing the `signal` and `open_channels` on `time`. They also print descriptive statistics such as:\n* unique `open_channels` values\n* correlation between `signal` and `open_channels`\n* `signal` and `open_channels` mean\n* `signal` and `open_channels` std\n* `signal` range of every `open_channels` value","20ef8241":"After all of the ramp transformations are removed from **Test Batch 1**, entire distribution looks cleaner. Every **10** second time frames are similar to different batches from training set. There might be outliers in fourth **100,000** samples of **Test Batch 0**, but it is hard to separate them from another level.","5af4603e":"### **4.3. Periodic Noise**\n\nFirst step of removing the periodic noise is calculating `signal_processed` mean for every `open_channels` value. In order to do that, entire **Batch 7** has to be removed from training set. Even though **Batch 7** looks clean, it can't be used for `signal_processed` mean calculation because the middle part was already replaced with mean values + random noise and it doesn't reflect the real data. \n\nTraining set `signal_processed` is separated into two parts `signalA` (every batch except **Batch 7**) and `signalB` (**Batch 7**). Clean parts of **Batch 7** can be used but it makes it harder to split the `signal_processed` into equal pieces because there are less than 500000 samples in it. For the sake of simplicity, **Batch 7** is omitted and it will be denoised separately.\n\nFinally, `signalC` is basically test set `signal_processed` and `channelsC` is test set predictions done by a high scoring model.","36818cf7":"Dropping the outliers from **Training Batch 7** increased the correlation from **0.8247** to **0.9583** and `open_channels` ranges are almost back to normal.\n\n### Training Batch 7 Levels\n\nThere are **4** levels in **Training Batch 7**:\n* **0** `open_channels` are between **-3.80866** and **-1.59495**\n* **1** `open_channels` are between **-2.86314** and **0.146036**\n* **2** `open_channels` are between **-1.75331** and **1.01144**\n* **3** `open_channels` are between **-0.343348** and **-2.17848**","56e0345d":"After the means are subtracted from every group, `signal_processed` is normalized and becomes flat. Test set doesn't have the labels so predictions of a strong model used for grouping.\n\nBatches can still be identified by their lengths but `signal_processed` is on the same level for every batch. Training set **Batch 7** and test set have some spikes because it is harder normalize them.","33c6a30e":"### **4.1. Model Classification**\n\nAfter `signal` is processed, the distribution becomes smooth and easier to predict. Every batch pair in training set have different unique `open_channels` values, `signal_processed` mean, std and ranges. There are **5** different distribution types in training and and **6** in test set. One extra distribution doesn't exist in training set so **5** or **6** different models have to be created for those different distributions.\n\nOne extra distribution in test set is something between 0-1 and 0-1-2-3 distributions. It could be both 0-1, 0-1-2 and 0-1-2-3, but it is hard to tell without `open_channels`. It is classified as **Model 1.5**. It can be predicted with **Model 0**, **Model 2** or resampled **Model 2** without 3 `open_channels`.\n\n| Model                        | Training      | Test                                                                                |\n|------------------------------|---------------|-------------------------------------------------------------------------------------|\n| 0-1 with 0s              | Batch 0 and 1 | Batch 0 1st 100,000 samples         |\n| 0-1 with 1s               | Batch 2 and 6 | Batch 0 5th 100,000 samples                                                       |\n| 0-1-2-3 or 0-1-2 |               | **Batch 0 4th 100,000 samples, Batch 1 4th 100,000 samples, Batch 2 and Batch 3** |\n| 0-1-2-3           | Batch 3 and 7 | Batch 0 2nd 100,000 samples and Batch 1 5th 100,000 samples                    |\n| 0-1-2-3-4-5                  | Batch 5 and 8 | Batch 0 3rd 100,000 samples and Batch 1 2nd 100,000 samples                    |\n| 0-1-2-3-4-5-6-7-8-9-10       | Batch 4 and 9 | Batch 1 1st and 3rd 100,000 samples                                             |","3961de78":"Removing the ramp transformation from first **100,000** samples of **Training Batch 1** increased the correlation from **0.3353** to **0.6902**.\n\n### Training Batch 1 Levels\n\nThere are **2** levels in **Training Batch 1**:\n* **0** `open_channels` are between **-3.9021** and **-1.53369**\n* **1** `open_channels` are between **-2.45985** and **-0.45438**","80c7f61f":"## **0. Introduction**\nCompetition data has only three features `signal`, `time` and `open_channels`. `open_channels` is the target and `signal` is the only predictor. It is a very small dataset in terms of features and samples.\n\n> **IMPORTANT**: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.\n\nThis is a very important detail to consider. Every **500,000** samples are coming from different batches. Data analysis and feature engineering should be done on batches independently.\n\nCompetition test data is split by 3\/7. First **30%** is the public test set and the other **70%** is the private test set. It means the public leaderboard scores are based on entire **Test Batch 0** and first 1\/5 of **Test Batch 1**.","31ef6f45":"## **4. Models**","000482f9":"### **2.9. Training Batch 8**\nIn **Training Batch 8**, `open_channels` is not correlated with `signal` because the entire batch is transformed with sine function. If the effects of that transformation are ignored, **Training Batch 8** has the same distribution with **Training Batch 5**. Besides the sine transformation, there is no outlier in this batch.","d566db1a":"50X zoom is applied to `signal` between **143-144** to see the levels clearly. The boundaries of levels are from whole **Training Batch 2**, not from the slice.\n\n### Training Batch 2 Levels\n\nThere are **2** levels in **Training Batch 2**:\n* **0** `open_channels` are between **-3.9107** and **-1.4957**\n* **1** `open_channels` are between **-2.7586** and **-0.3525**","01d14efc":"Fifth **100,000** samples of **Test Batch 0** have ramp transformation. After the ramp is removed, levels can be seen clearly. The distribution looks very similar to **Training Batch 2** and **Training Batch 6** with more frequent vertical white lines.\n\n### Test Batch 0 (5th 100,000) Levels\n\nThere are **2** levels in **Test Batch 0** last **100,000** Samples:\n* Values smaller than **-2** are more likely to be **0** `open_channels` \n* Values greater than **-2** are more likely to be **1** `open_channels`","e8be086a":"Removing the effects of sine transformation from `signal` doesn't clean the entire **Training Batch 7**. Removing sine transformation from **Training Batch 7** increased the correlation from **0.5126** to **0.8247**. **Training Batch 7** levels are not clear yet because of the outliers at the middle section.","5e3824e2":"It is not possible to filter every outlier by using `signal` values in **Training Batch 7**. Even though some of the samples are outliers, they can't be seen at first glance. It is better to filter entire middle part in which the outliers occur. Those samples are not dropped yet because models could perform better if filtered samples are assigned with lower sample weights. \n\n`is_filtered` feature is created for marking the filtered samples. The entire middle part between the first and the last outlier in **Training Batch 7** is filtered.","0a7dc84a":"Means of `signal_processed` for different `open_channels` have to be subtracted from itself in order to make the signal flat. Otherwise, FFT can't isolate periodicity without the normalization. It is easy to find means of different groups when batches are aligned. Means of `signal_processed` for every `open_channels` value for training set is listed below in an order. Those values can also be used for test set `signal_processed` normalization.","5eca3761":"### **2.3. Ramp Transformation**\nRamp transformation function is defined as $\\mathcal{R}(X) = X + ((r \\times 3) \\div N)$\n\n* $X$ is the given `signal` vector with size $N$\n* $r$ is a straight line with size $N$\n* $3$ is the slope of straight line, it is a constant because all of the ramp transformations are using $3$ for slope in entire dataset\n\nRamp transformation can be seen on several batches. Instead of entire batches, this transformation is applied to **10** second time frames. It can be seen on **Training Batch 1 first 100,000 samples**, **Test Batch 0 first, second and fifth 100,000 samples**, **Test Batch 1 second third and forth 100,000 samples**.\n\nThis transformation looks very unnatural because it always starts from and ends at multiples of 10. That's why it has to be removed in order to make the distribution smoother. Ramp removal function is basically the same function above with subtraction instead of addition.","f884687e":"Second **100,000** samples of **Test Batch 1** have ramp transformation. After the ramp is removed, levels can be seen clearly. The distribution looks very similar to **Training Batch 5** and **Training Batch 8**.\n\n### Test Batch 1 (2nd 100,000) Levels\n\nThere are **6** levels in **Test Batch 0** second **100,000** samples:\n* Values smaller than **-3** are more likely to be **0** `open_channels` \n* Values closer to **-2** are more likely to be **1** `open_channels`\n* Values closer to **-0.5** are more likely to be **2** `open_channels`\n* Values closer to **0.5** are more likely to be **3** `open_channels`\n* Values between **2** and **3** are more likely to be **4** `open_channels`\n* Values greater than **4** are more likely to be **5** `open_channels`","f01ca57d":"## **6. Conclusion**","cbf87ee4":"Second **100,000** samples of **Test Batch 0** have ramp transformation. After the ramp is removed, levels can be seen clearly. The distribution looks very similar to **Training Batch 3** and **Training Batch 7**.\n\n### Test Batch 0 (2nd 100,000) Levels\n\nThere are **4** levels in **Test Batch 0** second **100,000** samples:\n* Values closer to **-3** are more likely to be **0** `open_channels` \n* Values closer to **-2** are more likely to be **1** `open_channels`\n* Values between to **-1** are more likely to be **2** `open_channels`\n* Values greater than **1** are more likely to be **3** `open_channels`","ccdcd86f":"After all of the ramp transformations are removed from **Test Batch 0**, entire distribution looks cleaner. Every **10** second time frames are similar to different batches from training set. There might be outliers in fourth **100,000** samples of **Test Batch 0**, but it is hard to separate them from another level.","992073f9":"### **3.5. Training Batch 4**\nIn **Training Batch 4**, `open_channels` is extremely correlated with `signal` and there are **11** unique values in it. It is the only batch with every unique `open_channels` value. Based on the examples, there has to be **11** `signal` and `open_channels` levels in **Training Batch 4**, but it is very hard to detect them without zooming. Extremely high correlation might be a proof of those **11** levels.\n\nDescriptive statistics are accurate because there are no outliers, trends or anomalies in **Training Batch 4**","6fa4cf7a":"## **1. Target (Ion Channels)**\n> Many diseases, including cancer, are believed to have a contributing factor in common. **Ion channels** are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction.\n\n`open_channels` is the open ion channels at the given time (**0.0001 seconds**). There are **11** classes (from 0 to 10) to predict. Unique `open_channels` values are different in every batch so the rules learned from a batch may not generalize to another batch. This can be both regression or classification problem. However, regression models have worked better so far.","426201ad":"50X zoom is applied to `signal` between **239-240** to see the levels clearly but it still very hard to separate levels from each other. The boundaries of levels are from whole **Training Batch 4**, not from the slice.\n\n### Training Batch 4 Levels\n\nThere are **11** levels in **Training Batch 4**:\n* **0** `open_channels` are between **-5.7965** and **-5.7481**\n* **1** `open_channels` are between **-5.3438** and **-3.3162**\n* **2** `open_channels` are between **-4.6254** and **-1.6356**\n* **3** `open_channels` are between **-3.1726** and **-0.3517**\n* **4** `open_channels` are between **-2.1492** and **0.9949**\n* **5** `open_channels` are between **-0.9996** and **2.44**\n* **6** `open_channels` are between **0.1303** and **3.7926**\n* **7** `open_channels` are between **1.348** and **4.9947**\n* **8** `open_channels` are between **2.4193** and **6.0338**\n* **9** `open_channels` are between **3.9629** and **7.1794**\n* **10** `open_channels` are between **5.3285** and **8.6131**","258216f7":"Removing the effects of sine transformation from `signal` cleans the entire **Training Batch 9**. Cleaning **Training Batch 9** increased the correlation from **0.7469** to **0.9739**.","a931ee96":"Third **100,000** samples of **Test Batch 0** doesn't have noise. The distribution looks very similar to **Training Batch 5** and **Training Batch 8**. Those two batches have at least **6** levels in them so it is very hard to detect them in test set without `open_channels`.\n\n### Test Batch 0 (3rd 100,000) Levels\n\nThere are **6** levels in **Test Batch 0** third **100,000** samples:\n* Values closer to **-3** are more likely to be **0** `open_channels` \n* Values closer to **-2** are more likely to be **1** `open_channels`\n* Values closer to **-1** are more likely to be **2** `open_channels`\n* Values closer to **1** are more likely to be **3** `open_channels`\n* Values closer to **2** are more likely to be **4** `open_channels`\n* Values greater than **3** are more likely to be **5** `open_channels`","dccc8c30":"### **2.1. Outliers**\nThere are very few outliers in the entire dataset. They can be seen on **Training Batch 0**, **Training Batch 7**, **Test Batch 2** and **Test Batch 3**. Those outliers could be dangerous since lots of them are in private test set. It is very hard to predict class thresholds because of the outliers, so they have to be dealt with.","4b0fb50c":"Fifth **100,000** samples of **Test Batch 1**  doesn't have noise. The distribution looks very similar to **Training Batch 5** and **Training Batch 8**.\n\n### Test Batch 1 (5th 100,000) Levels\n\nThere are **6** levels in **Test Batch 1** fifth **100,000** samples:\n* Values smaller than **-3** are more likely to be **0** `open_channels` \n* Values closer to **-2** are more likely to be **1** `open_channels`\n* Values closer to **-1** are more likely to be **2** `open_channels`\n* Values closer to **0** are more likely to be **3** `open_channels`\n* Values closer to **1** are more likely to be **4** `open_channels`\n* Values greater than **1.5** are more likely to be **5** `open_channels`","68561b58":"50X zoom is applied to `signal_processed` between **430-431** to see the levels clearly. The boundaries of levels are from whole **Training Batch 8**, not from the slice.\n\n### Training Batch 8 Levels\n\nThere are **6** levels in **Training Batch 8**:\n* **0** `open_channels` are between **-3.65724** and **-1.6207**\n* **1** `open_channels` are between **-2.55889** and **-0.412766**\n* **2** `open_channels` are between **-1.45358** and **0.921311**\n* **3** `open_channels` are between **-0.236957** and **2.196**\n* **4** `open_channels` are between **0.875905** and **3.4344**\n* **5** `open_channels` are between **2.22102** and **4.6432**","2400d4a0":"## **3. Batches**\nEvery batch is unique, and have its own trends, characteristics and anomalies even though they have similar target distribution. They have to be analyzed separately. Necessary preprocessing steps can be decided more accurately that way. Filtering, normalizing and scaling should be done on batch level.\n\nEvery batch have one thing in common. If the noises are ignored, `signal` and `open_channels` have same number of levels in every batch of training set. There are concurrent spikes in `signal` and `open_channels`. That's why correlation between those two features is very high.\n\nAfter the `signal` is cleaned, it becomes easier to count levels in batches. By counting the levels in a batch, the lower and upper bounds can be predicted easily. Predicted `open_channels` values can be clipped according to lower and upper bounds of the current batch. This postprocessing operation is going to help labeling the classes more accurately.\n\n`signal_processed` feature is created for keeping both raw and processed `signal`. All changes made during the preprocessing will be written to `signal_processed`.","26178d9a":"### **2.13. Test Batch 3**\n**Test Batch 3** is the last batch of test set. It doesn't have noise. **Test Batch 3** has the same distribution with **Test Batch 0**'s and **Test Batch 1**'s fourth **100,000** samples, and entire **Test Batch 2**.","445190bc":"Removing the effects of sine transformation from `signal` cleans the entire **Test Batch 2**. The distribution is unique and it doesn't look like any batch from training set, but it could have the same levels with **Training Batch 3** and **Training Batch 7**. There might be outliers in **Test Batch 2**, but it is hard to separate them from another level.","31828d63":"50X zoom is applied to `signal_processed` between **624-625** to see levels clearly. The assumptions of levels are based on entire **Test Batch 2**, not the slice.\n\n\n### Test Batch 2 Levels\n\nThere are **3** or **4** levels in **Test Batch 2**:\n* Values between **-4** and **-2** are more likely to be **0** `open_channels` \n* Values closer to **-2** are more likely to be **1** `open_channels` \n* Values closer to **-1** are more likely to be **2** `open_channels` \n* Values greater than **-1** are more likely to be **3** `open_channels`","f13efe56":"### **2.11. Test Batch 1**\n**Test Batch 1** is the second batch of test set and its first **100,000** samples are the last 1\/6 of public leaderboard. It is similar to **Test Batch 0** in terms of different distributions. Every **100,000** samples have to be analyzed and cleaned separately for **Test Batch 1**.","bef8c49a":"## **2. Noise Types**","f83c6ff9":"50X zoom is applied to `signal` between **295-296** to see the levels clearly. The boundaries of levels are from whole **Training Batch 5**, not from the slice.\n\n### Training Batch 5 Levels\n\nThere are **6** levels in **Training Batch 5**:\n* **0** `open_channels` are between **-3.8174** and **-1.7285**\n* **1** `open_channels` are between **-2.8902** and **-0.3566**\n* **2** `open_channels` are between **-1.5576** and **0.9042**\n* **3** `open_channels` are between **-0.2754** and **2.2316**\n* **4** `open_channels` are between **0.9769** and **3.4705**\n* **5** `open_channels` are between **2.1698** and **4.7929**","fd6f8511":"### **3.4. Training Batch 3**\nIn **Training Batch 3**, `open_channels` is extremely correlated with `signal`. Instead of **2**, there are **4** levels in **Training Batch 3** unlike previous batches. Those levels represent every **4** unique `open_channels` values.\n\nDescriptive statistics are accurate because there are no outliers, trends or anomalies in **Training Batch 3**.","f347e2b2":"### **3.2. Training Batch 1**\n**Training Batch 1** has a very simple distribution. There are lots of concurrent spikes in `signal` and `open_channels` which indicate the increases in target. \n\n`open_channels` and `signal` are not as highly correlated with each other as in **Training Batch 0** because there is a ramp at the beginning. `open_channels` range, `signal` mean and std are not accurate because of that ramp. Besides the ramp, there is no outlier in this batch."}}