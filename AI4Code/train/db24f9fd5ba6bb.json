{"cell_type":{"0beac127":"code","f5640edd":"code","d0bb1688":"code","ecec39b0":"code","3490cb0e":"code","f1fac733":"code","14d4b564":"code","c5a4e28a":"code","4f38977e":"code","5e4c9256":"code","9060a537":"code","0d18969c":"code","fc02cd3f":"code","efc044cd":"code","b419c9b8":"code","6337d7d2":"code","c1edb4f6":"code","c596193c":"code","d020cd67":"code","c90f676b":"code","286ab312":"code","eebb9d29":"code","52496e3b":"code","0f795fad":"code","f9d9cf95":"code","83b06a28":"code","47002182":"code","fe54ef69":"code","b0ebbd92":"code","f3135fa9":"code","db71c524":"code","a82e8791":"code","7f861be0":"code","333ca5a7":"code","a2751478":"code","7e623786":"code","383fac6c":"code","d2a09763":"code","24a332b1":"code","85fe31e3":"code","2e8a0d46":"code","a717914e":"code","39a33ea9":"code","450879b4":"code","28cae641":"code","332fbe80":"code","f3abb1e5":"code","7ea685c2":"code","55a2c3e1":"code","bf678b09":"code","2ba7c90d":"code","4e7cb126":"code","c232f078":"code","38d2d32a":"code","8c434097":"code","854d417a":"code","f3480dc3":"code","f2874342":"code","93649ca7":"code","b6426824":"code","246e418d":"code","e844df63":"code","b570d968":"code","1e152d15":"code","36c25fa8":"code","0210c18e":"code","695dba41":"code","3393b150":"code","870a8a5d":"code","3be2684e":"code","e54d7849":"code","7cd890ff":"code","573b4185":"code","c4026262":"code","cc540960":"code","16c0988e":"code","21031763":"code","94b9f517":"code","36e86517":"code","91ae36b7":"code","1f9b07ac":"code","51437688":"code","0312edb8":"code","7f5eb8c4":"code","2b9eef3d":"code","79404273":"code","963ba2ab":"code","d340a922":"code","32060f78":"code","19359fde":"code","9226d41c":"code","8830eef4":"code","4065caff":"code","ba5ec5f1":"code","9f9d2a18":"code","f414658c":"code","a38e4038":"code","9f8e510c":"code","4925b8c4":"code","f4deb92a":"code","e911a192":"code","c979c0e6":"code","d4534fb5":"code","2ecb24aa":"code","0e40765f":"code","65969b6c":"code","72614cba":"code","3cf2ff2f":"code","d1579fb5":"code","edbe3a9e":"code","11ddb7f5":"code","fda617cf":"code","95891fc8":"code","369b404c":"code","2d9906e7":"code","7528412d":"code","d80c5c87":"code","8dae0abc":"code","b995f794":"code","2b2896c7":"code","a1ae66b2":"code","42b89f43":"code","f8454fc6":"code","8dd5bc25":"code","c1d5467d":"code","842a1224":"code","d874e388":"code","47607aaa":"code","2533058e":"code","d8670f5c":"code","f1a35097":"code","76bd7b0e":"code","6120e2aa":"code","4458a33d":"code","ae6f7081":"code","1e72514d":"code","1ec42731":"code","d6794202":"code","5bebf922":"code","ea72c58f":"code","a45fe2c5":"code","04d2d77d":"code","41ff293a":"code","f75ecbd2":"code","a7df702f":"code","2a12a0ab":"code","7deb3379":"code","0dcd80be":"code","e6f0bd46":"code","c19acbe6":"code","9873e213":"code","5fbea39b":"code","090558fe":"code","80f22c04":"code","18ea58bb":"code","0f982414":"code","4327e5d7":"code","25bfa7db":"code","bb15b900":"code","b26375a1":"code","50421c02":"code","8b6314da":"code","9800955e":"code","902da2e4":"code","c354e654":"markdown","0f39851a":"markdown","bbda816c":"markdown","d41be42e":"markdown","fd1106f7":"markdown","ecdaefb7":"markdown","7e7e086e":"markdown","abf20c00":"markdown","0c4415b2":"markdown","52a61574":"markdown","750be3ab":"markdown","f96c78e5":"markdown","7cfa61d0":"markdown","0ce1fb9c":"markdown","0c1fb480":"markdown","07e7375d":"markdown","80fba57f":"markdown","b1b798df":"markdown","37fe90cb":"markdown","aea6e962":"markdown","e45824eb":"markdown","c3af6440":"markdown","efb5c652":"markdown","bc9f336e":"markdown","38dc3d1d":"markdown","3bd8f95b":"markdown","f824be6f":"markdown","bcb014a1":"markdown","456c7cea":"markdown","deb2036b":"markdown","5641f928":"markdown","512392ba":"markdown","ab9899ba":"markdown","d76a3508":"markdown","f8590521":"markdown","a004517a":"markdown","2baf5cda":"markdown","ef6a5115":"markdown","2b2a9c05":"markdown","c459e69f":"markdown","d658891c":"markdown","b24c6184":"markdown","b072cbda":"markdown","b93f6fd2":"markdown","48a37367":"markdown","270d6cce":"markdown","ef893110":"markdown","f35e1410":"markdown","24769dda":"markdown","427df0cb":"markdown","c5bd5973":"markdown","69ecd9b4":"markdown","7f5360ed":"markdown","7e833177":"markdown","e4e6c098":"markdown","4a47c892":"markdown","70206fcd":"markdown","89ffadf1":"markdown","85aeeed9":"markdown","07837db0":"markdown","0baa660e":"markdown","eecb9388":"markdown","8c3dd8ae":"markdown","f45b601b":"markdown","e1366522":"markdown","f950e336":"markdown","7b9f0f6a":"markdown","6055729e":"markdown","db19f44a":"markdown","90623fda":"markdown","fa1297f7":"markdown","9f918225":"markdown","d23db61c":"markdown","cb241610":"markdown","cb091ff7":"markdown","1f65e445":"markdown","313f43ff":"markdown","dc890f28":"markdown","2f60c5e0":"markdown","383382a2":"markdown","5b033ed7":"markdown","faa57ce6":"markdown","11207a80":"markdown","78b963a3":"markdown","bb44c716":"markdown","230db32e":"markdown","dc24eef3":"markdown","bc24e5f1":"markdown","d325c85f":"markdown","66686ec5":"markdown","b14d620d":"markdown","befdc5cf":"markdown","fbb0d34e":"markdown","19c458bd":"markdown","c75a2e48":"markdown","4b0498c5":"markdown","0e36f6c9":"markdown","bdd21915":"markdown","571343f9":"markdown","f4bcd917":"markdown","7d51c12e":"markdown","d0df01de":"markdown","7135bcf0":"markdown","64ae5858":"markdown","88280456":"markdown","fe5417e3":"markdown","a4190236":"markdown","0ee233ff":"markdown","67fb6d06":"markdown","54f36b17":"markdown","67e8176b":"markdown","7629c2ad":"markdown","b1df6587":"markdown","eec9658d":"markdown","f3f9071b":"markdown","ea572e2e":"markdown","2710b6f2":"markdown","50853c33":"markdown","64bf9398":"markdown","69b14e90":"markdown","bd9e44ae":"markdown","ea99dab8":"markdown","db4324b3":"markdown","20d397e1":"markdown","be7e3c8e":"markdown","5f02042e":"markdown","89543454":"markdown","6b9b1b65":"markdown","58e12937":"markdown","016bd770":"markdown","2990b5bb":"markdown","c379199d":"markdown","fc8ba85f":"markdown","db08a217":"markdown","a08aa65e":"markdown","0f8480e5":"markdown","3337f876":"markdown","7342155f":"markdown","dcf82144":"markdown","4b23a5bd":"markdown","ffa7dbd8":"markdown","512346b4":"markdown","fc62d045":"markdown","726258d0":"markdown","215a3229":"markdown","c8eb6ace":"markdown","59c7669e":"markdown","91968bc8":"markdown","c4e0c68f":"markdown","61dfaa7b":"markdown","cfb31d5f":"markdown","d80e3f11":"markdown","6e888c4f":"markdown","0347fcc0":"markdown","092aef66":"markdown","26f534b4":"markdown","d0b111d2":"markdown","c94266cc":"markdown","d2dee6c2":"markdown","a14ed7ce":"markdown","af00b1bf":"markdown","ea8d4739":"markdown","e0eb6f13":"markdown","ca2f477f":"markdown","4568fdab":"markdown","54b91fc0":"markdown","40d1f2f0":"markdown","25ee5225":"markdown","e7040c2c":"markdown","b6d4a17f":"markdown","a5e1899c":"markdown","fe2fdcb4":"markdown","5091e34d":"markdown","f1cc7728":"markdown","f125d82e":"markdown","f9fd750e":"markdown"},"source":{"0beac127":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory``````````\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n            \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f5640edd":"# import libraries\npd.options.display.max_columns=None\npd.options.display.max_rows=85\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nimport xgboost as xgb\nfrom xgboost import plot_tree, plot_importance\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import learning_curve","d0bb1688":"# read training and testing CSV data to dataframes train & test respectively\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","ecec39b0":"# inspect the shape of the train & test dataframes\nprint(train.shape)\nprint(test.shape)","3490cb0e":"# inspect train\ntrain","f1fac733":"# dataframe information\ntrain.info()","14d4b564":"# statistics of train\ntrain.describe()","c5a4e28a":"# converting 'int' datatype to 'object' datatype for train data\ntrain[\"MSSubClass\"] = train[\"MSSubClass\"].astype(str)\ntrain[\"OverallQual\"] = train[\"OverallQual\"].astype(str)\ntrain[\"OverallCond\"] = train[\"OverallCond\"].astype(str)","4f38977e":"# converting 'int' datatype to 'object' datatype for test data\ntest[\"MSSubClass\"] = test[\"MSSubClass\"].astype(str)\ntest[\"OverallQual\"] = test[\"OverallQual\"].astype(str)\ntest[\"OverallCond\"] = test[\"OverallCond\"].astype(str)","5e4c9256":"# categorical columns include \"object\" datatypes\ncat_cols = train.select_dtypes(include=\"object\").columns\ncat_cols","9060a537":"# numerical columns exclude \"object\" datatypes\nnum_cols = train.select_dtypes(exclude=\"object\").columns\nnum_cols","0d18969c":"# SalePrice column\ntrain[\"SalePrice\"]","fc02cd3f":"# SalePrice histogram\nplt.figure(figsize=(10, 8))\nsns.histplot(train[\"SalePrice\"], kde=True)","efc044cd":"# missing value in train data\nmissing_values = train.isna().sum().sort_values(ascending=False)\nmissing_values","b419c9b8":"# visualising null values in train\nsns.heatmap(train.isnull(), cbar=False)","6337d7d2":"# missing values in numerical features of train\n(train.select_dtypes(exclude='object')).isnull().sum().sort_values(ascending=False).head(10)","c1edb4f6":"# getting unique values and their counts in LotFrontage column\ntrain[\"LotFrontage\"].value_counts()","c596193c":"#plot feature LotFrontage vs target SalePrice\nplt.figure(figsize=(8,4))\nax=sns.regplot(y=\"LotFrontage\", x=\"SalePrice\", data=train)\nplt.setp(ax.get_xticklabels(), rotation=90)\nplt.show()","d020cd67":"# impute mean for missing values\ntrain[\"LotFrontage\"].fillna(train[\"LotFrontage\"].mean(), inplace=True)\n","c90f676b":"# impute missing values in test data\ntest[\"LotFrontage\"].fillna(train[\"LotFrontage\"].mean(), inplace=True)","286ab312":"train[[\"GarageYrBlt\", \"YearBuilt\"]]","eebb9d29":"# plotting GarageYrBlt and YearBuilt vs SalePrice\nplt.figure(figsize=(8,4))\nsns.scatterplot(x=\"GarageYrBlt\", y=\"SalePrice\", data=train, color = 'r', alpha=0.4)\nsns.scatterplot(x=\"YearBuilt\", y=\"SalePrice\", data=train, color='b', alpha=0.4)\nplt.legend(labels = [\"GarageYrBlt\", \"YearBuilt\"])","52496e3b":"# comparing if the row values are same for the features\ncompare = np.where(train[\"GarageYrBlt\"] == train[\"YearBuilt\"], True, False)\ncomparison = np.array(compare)\nnp.unique(comparison, return_counts=True)","0f795fad":"# some of the null value rows in GarageYrBlt\ntrain.loc[train[\"GarageYrBlt\"].isnull()].head()","f9d9cf95":"# impute mean for missing values\ntrain[\"GarageYrBlt\"].fillna(0, inplace=True)","83b06a28":"# impute missing values in test data\ntest[\"GarageYrBlt\"].fillna(0, inplace=True)","47002182":"# missing values in categorical features of train\n(train.select_dtypes(include='object')).isnull().sum().sort_values(ascending=False).head(20)","fe54ef69":"train[\"PoolQC\"].value_counts()","b0ebbd92":"# get unique PoolArea value counts\ntrain[\"PoolArea\"].value_counts()","f3135fa9":"# drop PoolArea & PoolQC columns\ntrain.drop(columns=[\"PoolArea\", \"PoolQC\"], axis=1, inplace=True)\n","db71c524":"# drop PoolArea & PoolQC columns in test data\ntest.drop(columns=[\"PoolArea\", \"PoolQC\"], axis=1, inplace=True)\n","a82e8791":"# getting unique values and their counts in MiscFeature column\ntrain[\"MiscFeature\"].value_counts()","7f861be0":"# drop MiscFeature column\ntrain.drop(columns=[\"MiscFeature\"], axis=1, inplace=True)\n","333ca5a7":"# drop MiscFeature column in test data\ntest.drop(columns=[\"MiscFeature\"], axis=1, inplace=True)\n","a2751478":"# getting unique value count for Alley\ntrain[\"Alley\"].value_counts()","7e623786":"# impute 'NA' for null values in Alley\ntrain[\"Alley\"].fillna('NA', inplace=True)","383fac6c":"# impute null values for Alley in test data\ntest[\"Alley\"].fillna('NA', inplace=True)","d2a09763":"# getting counts of unique values in Fence\ntrain[\"Fence\"].value_counts()","24a332b1":"# impute 'NA' for null values in Fence\ntrain[\"Fence\"].fillna('NA', inplace=True)","85fe31e3":"# impute null values for Fence in test data\ntest[\"Fence\"].fillna('NA', inplace=True)","2e8a0d46":"# getting counts of unique values in FireplaceQu\ntrain[\"FireplaceQu\"].value_counts()","a717914e":"# getting counts of unique values in Fireplaces\ntrain[\"Fireplaces\"].value_counts()","39a33ea9":"# impute 'NA' for missing values in FireplaceQu\ntrain[\"FireplaceQu\"].fillna(\"NA\", inplace=True)","450879b4":"# impute for missing values in FireplaceQu in test data\ntest[\"FireplaceQu\"].fillna(\"NA\", inplace=True)","28cae641":"# imputing 'NA' for missing values\ntrain[\"GarageType\"].fillna('NA', inplace=True)\ntrain[\"GarageFinish\"].fillna('NA', inplace=True)\ntrain[\"GarageQual\"].fillna('NA', inplace=True)\ntrain[\"GarageCond\"].fillna('NA', inplace=True)\n","332fbe80":"# imputing 'NA' for missing values in test data\ntest[\"GarageType\"].fillna('NA', inplace=True)\ntest[\"GarageFinish\"].fillna('NA', inplace=True)\ntest[\"GarageQual\"].fillna('NA', inplace=True)\ntest[\"GarageCond\"].fillna('NA', inplace=True)\n","f3abb1e5":"# getting uniuqe value counts for BsmtExposure\ntrain[\"BsmtExposure\"].value_counts()","7ea685c2":"# getting uniuq value counts for BsmtFinType2\ntrain[\"BsmtFinType2\"].value_counts()","55a2c3e1":"train.loc[train[\"BsmtExposure\"].isnull()].head()","bf678b09":"# filling 'NA' for missing values\ntrain[\"BsmtExposure\"].fillna('NA', inplace=True)\ntrain[\"BsmtFinType2\"].fillna('NA', inplace=True)","2ba7c90d":"# filling 'NA' for missing values in test data\ntest[\"BsmtExposure\"].fillna('NA', inplace=True)\ntest[\"BsmtFinType2\"].fillna('NA', inplace=True)","4e7cb126":"# unique value counts for BsmtQual\ntrain[\"BsmtQual\"].value_counts()","c232f078":"# unique value counts for BsmtCond\ntrain[\"BsmtCond\"].value_counts()","38d2d32a":"# unique value counts for BsmtFinType1\ntrain[\"BsmtFinType1\"].value_counts()","8c434097":"train.loc[train[\"BsmtQual\"].isnull()].head()","854d417a":"# impute 'NA' for null values\ntrain[\"BsmtQual\"].fillna('NA', inplace=True)\ntrain[\"BsmtCond\"].fillna('NA', inplace=True)\ntrain[\"BsmtFinType1\"].fillna('NA', inplace=True)","f3480dc3":"# impute 'NA' for null values in test data\ntest[\"BsmtQual\"].fillna('NA', inplace=True)\ntest[\"BsmtCond\"].fillna('NA', inplace=True)\ntest[\"BsmtFinType1\"].fillna('NA', inplace=True)","f2874342":"# null values in train\ntrain.isnull().sum().sort_values(ascending=False).head(5)","93649ca7":"# getting unique value count for MasVnrArea\ntrain[\"MasVnrArea\"].value_counts()","b6426824":"# getting unique value count for MasVnrType\ntrain[\"MasVnrType\"].value_counts()","246e418d":"# impute most frequent values for MasVnrArea & MasVnrType\ntrain[\"MasVnrArea\"].fillna(0, inplace=True)\ntrain[\"MasVnrType\"].fillna(train[\"MasVnrType\"].mode()[0], inplace=True)","e844df63":"# impute most frequent values for MasVnrArea & MasVnrType in test data\ntest[\"MasVnrArea\"].fillna(0, inplace=True)\ntest[\"MasVnrType\"].fillna(train[\"MasVnrType\"].mode()[0], inplace=True)","b570d968":"# getting unique value counts for Electrical\ntrain[\"Electrical\"].value_counts()","1e152d15":"# impute most frequent category for null value\ntrain[\"Electrical\"].fillna(train[\"Electrical\"].mode()[0], inplace=True)","36c25fa8":"# impute most frequent category for null value in test data\ntest[\"Electrical\"].fillna(train[\"Electrical\"].mode()[0], inplace=True)","0210c18e":"# check for null values in train\nsns.heatmap(train.isnull(), cbar=False)","695dba41":"# checking null values in test data\ntest.isnull().sum()","3393b150":"# impute mean for numerical columns and mode for categorical columns in test data\ntest[\"MSZoning\"].fillna(test[\"MSZoning\"].mode()[0], inplace=True)\ntest[\"Exterior1st\"].fillna(test[\"Exterior1st\"].mode()[0], inplace=True)\ntest[\"Exterior2nd\"].fillna(test[\"Exterior2nd\"].mode()[0], inplace=True)\ntest[\"BsmtFinSF1\"].fillna(test[\"BsmtFinSF1\"].mean(), inplace=True)\ntest[\"TotalBsmtSF\"].fillna(test[\"TotalBsmtSF\"].mean(), inplace=True)\ntest[\"KitchenQual\"].fillna(test[\"KitchenQual\"].mode()[0], inplace=True)\ntest[\"Functional\"].fillna(test[\"Functional\"].mode()[0], inplace=True)\ntest[\"GarageCars\"].fillna(test[\"GarageCars\"].mean(), inplace=True)\ntest[\"SaleType\"].fillna(test[\"SaleType\"].mode()[0], inplace=True)","870a8a5d":"# check the shape of train and test after data cleaning\nprint(train.shape)\nprint(test.shape)","3be2684e":"# numerical features in train\ntrain.select_dtypes(exclude='object')","e54d7849":"sns.pairplot(train, diag_kind='kde', vars = [\"SalePrice\", 'LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd',\n       'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n       '1stFlrSF', '2ndFlrSF'], corner=True)","7cd890ff":"sns.pairplot(train, diag_kind='kde', vars = [\"SalePrice\", 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',\n       'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr',\n       'TotRmsAbvGrd', 'Fireplaces'], corner=True)","573b4185":"sns.pairplot(train, diag_kind='kde', vars = [\"SalePrice\", 'GarageYrBlt', 'GarageCars', 'GarageArea',\n       'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n       'ScreenPorch', 'MiscVal', 'MoSold', 'YrSold'], corner=True)","c4026262":"# correlation matrix of train data\ntrain.corr()","cc540960":"# visualising correaltion using a heatmap\nplt.figure(figsize=(12,12))\nsns.heatmap(train.corr(), cmap=\"rocket\")","16c0988e":"# calculating correaltion with SalePrice from max to min\nnum_cols_corr = train.corrwith(train[\"SalePrice\"]).sort_values(ascending=False)\nnum_cols_corr","21031763":"# filtering columns with < 0.3 correlation with target\nnumcols_to_drop = num_cols_corr[num_cols_corr < 0.3].index\nnumcols_to_drop","94b9f517":"# drop columns with weak correlation from train data\ntrain.drop(numcols_to_drop, axis=1, inplace=True)\n","36e86517":"# drop columns with weak correlation from test data\ntest.drop(numcols_to_drop, axis=1, inplace=True)\n","91ae36b7":"train.head()","1f9b07ac":"# drop 1stFlrSF, 2ndFlrSF from train\ntrain.drop(columns=[\"1stFlrSF\", \"2ndFlrSF\"], axis=1, inplace=True)\n","51437688":"# drop 1stFlrSF, 2ndFlrSF from test data\ntest.drop(columns=[\"1stFlrSF\", \"2ndFlrSF\"], axis=1, inplace=True)\n","0312edb8":"# drop GarageArea from train\ntrain.drop(columns=[\"GarageArea\"], axis=1, inplace=True)\n","7f5eb8c4":"# drop GarageArea from test data\ntest.drop(columns=[\"GarageArea\"], axis=1, inplace=True)\n","2b9eef3d":"# columns dropped earlier due to low correlation with SalePrice\nnumcols_to_drop","79404273":"# drop BsmtFinType2 from train\ntrain.drop(columns=[\"BsmtFinType2\"], axis=1, inplace=True)\n","963ba2ab":"# drop BsmtFinType2 from test data\ntest.drop(columns=[\"BsmtFinType2\"], axis=1, inplace=True)\n","d340a922":"# categorical features in train\ntrain.select_dtypes(include='object')","32060f78":"# plotting categorical columns vs Saleprice\nfig, ax = plt.subplots(11,4, figsize=(30, 70))\nfor col, subplot in zip(train.select_dtypes(include='object'), ax.flatten()):\n    sns.boxplot(x=train[col], y=\"SalePrice\", data=train, ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)","19359fde":"# unique value count of categories in Street\ntrain[\"Street\"].value_counts()","9226d41c":"# drop Street column from train\ntrain.drop(columns=[\"Street\"], axis=1, inplace=True)\n","8830eef4":"# drop Street column from test data\ntest.drop(columns=[\"Street\"], axis=1, inplace=True)\n","4065caff":"# LandContour, LandSlope columns in train\ntrain[[\"LandContour\", \"LandSlope\"]].head(20)","ba5ec5f1":"# getting unique value counts of LandContour, LandSlope\nprint(train[\"LandContour\"].value_counts())\nprint(train[\"LandSlope\"].value_counts())","9f9d2a18":"# plotting LandContour, LandSlope vs SalePrice\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\nsns.histplot(ax=axes[0], x=\"LandContour\", y=\"SalePrice\", data=train)\nsns.histplot(ax=axes[1], x=\"LandSlope\", y=\"SalePrice\", data=train)","f414658c":"# drop LandContour from train\ntrain.drop(columns=[\"LandContour\"], axis=1, inplace=True)\ntrain.shape","a38e4038":"# drop LandContour from test data\ntest.drop(columns=[\"LandContour\"], axis=1, inplace=True)\n","9f8e510c":"# getting value count of categories in utilities\ntrain[\"Utilities\"].value_counts()","4925b8c4":"# drop Utilities column from train\ntrain.drop(columns=[\"Utilities\"], axis=1, inplace=True)\n","f4deb92a":"# drop Utilities column from test\ntest.drop(columns=[\"Utilities\"], axis=1, inplace=True)\n","e911a192":"# count of unique values in Condition1, Condition2\nprint(train[\"Condition1\"].value_counts())\nprint(train[\"Condition2\"].value_counts())","c979c0e6":"# drop Condition2 from train\ntrain.drop(columns=[\"Condition2\"], axis=1, inplace=True)\n","d4534fb5":"# drop Condition2 from test\ntest.drop(columns=[\"Condition2\"], axis=1, inplace=True)\n","2ecb24aa":"# plot the category count for RoofStyle, RoofMatl\nfig, axes = plt.subplots(1, 2, figsize=(12,4))\nsns.countplot(ax=axes[0], x=\"RoofStyle\", data=train)\nsns.countplot(ax=axes[1], x=\"RoofMatl\", data=train)\nax=plt.xticks(rotation=90)","0e40765f":"print(train[\"RoofStyle\"].value_counts())\nprint(train[\"RoofMatl\"].value_counts())","65969b6c":"# drop RoofMatl from train\ntrain.drop(columns=[\"RoofMatl\"], axis=1, inplace=True)\n","72614cba":"# drop RoofMatl from test\ntest.drop(columns=[\"RoofMatl\"], axis=1, inplace=True)\n","3cf2ff2f":"# getting value counts in categories of Exterior1st, Exterior2nd\nprint(train[\"Exterior1st\"].value_counts())\nprint(train[\"Exterior2nd\"].value_counts())","d1579fb5":"# observe the 2 features in dataframe\ntrain[[\"Exterior1st\", \"Exterior2nd\"]].head(20)","edbe3a9e":"# comparing the values for all datapoints across the entire dataframe\ncompare = np.where(train[\"Exterior2nd\"] == train[\"Exterior1st\"], True, False)\ncomparison = np.array(compare)\nnp.unique(comparison, return_counts=True)","11ddb7f5":"# plotting Exterior1st, Exterior2nd vs SalePrice\nfig, axes = plt.subplots(1, 2, figsize=(24, 4))\nsns.boxplot(ax=axes[0], x=\"Exterior1st\", y=\"SalePrice\", data=train)\nsns.boxplot(ax=axes[1], x=\"Exterior2nd\", y=\"SalePrice\", data=train)","fda617cf":"# category counts in ExterQual, ExterCond\nprint(train[\"ExterQual\"].value_counts())\nprint(train[\"ExterCond\"].value_counts())","95891fc8":"fig, axes = plt.subplots(1, 2, figsize=(14,6))\nsns.boxplot(ax=axes[0], x=\"ExterQual\", y=\"SalePrice\", data=train)\nsns.boxplot(ax=axes[1], x=\"ExterCond\", y=\"SalePrice\", data=train)","369b404c":"# getting value count for categories in BsmtQual, BsmtCond\nprint(train[\"BsmtQual\"].value_counts())\nprint(train[\"BsmtCond\"].value_counts())","2d9906e7":"train[\"Heating\"].value_counts()","7528412d":"# drop Heating from train\ntrain.drop(columns=[\"Heating\"], axis=1, inplace=True)\n","d80c5c87":"# drop Heating from test\ntest.drop(columns=[\"Heating\"], axis=1, inplace=True)\n","8dae0abc":"# category count for HeatingQC\ntrain[\"HeatingQC\"].value_counts()","b995f794":"# plot HeatingQC vs SalePrice\nplt.figure(figsize=(8,4))\nsns.boxplot(x=\"HeatingQC\", y=\"SalePrice\", data=train)","2b2896c7":"# getting value count for CentralAir\ntrain[\"CentralAir\"].value_counts()","a1ae66b2":"print(train[\"GarageQual\"].value_counts())\nprint(train[\"GarageCond\"].value_counts())","42b89f43":"fig, axes = plt.subplots(1, 2, figsize=(14,6))\nsns.stripplot(ax=axes[0], x=\"GarageQual\", y=\"SalePrice\", data=train)\nsns.stripplot(ax=axes[1], x=\"GarageCond\", y=\"SalePrice\", data=train)","f8454fc6":"# drop GarageCond from train\ntrain.drop(columns=[\"GarageCond\"], axis=1, inplace=True)\n","8dd5bc25":"# drop GarageCond from test\ntest.drop(columns=[\"GarageCond\"], axis=1, inplace=True)\n","c1d5467d":"train[\"PavedDrive\"].value_counts()","842a1224":"train[\"SaleType\"].value_counts()","d874e388":"train[\"SaleCondition\"].value_counts()","47607aaa":"# check the shape of train and test after  feature selection\nprint(train.shape)\nprint(test.shape)","2533058e":"# skewness of target, Saleprice\nplt.figure(figsize=(8,6))\nsns.histplot(train[\"SalePrice\"])","d8670f5c":"# skewness of numerical columns in train data\ntrain.hist(bins=20, figsize=(20, 10))\nplt.show()","f1a35097":"# calculate skewness of numerical columns and sort values from maximun to minimum\nskew = (train.select_dtypes(exclude='object').skew(axis=0).sort_values(ascending=False))\nskew","76bd7b0e":"# columns in train data with skewness > 1\nskew_train = skew[skew > 1].index\n# only skewed features\nskew_cols = skew_train.drop([\"SalePrice\"])","6120e2aa":"# columns to be normalised\nskew_cols","4458a33d":"# applying log transformation to features in train data\nfor cols in skew_cols:\n    train[cols] = np.log1p(train[cols])\n    \ntrain","ae6f7081":"# applying log transformation to features in test data\nfor cols in skew_cols:\n    test[cols] = np.log1p(test[cols])\n","1e72514d":"# applying log transformation to target\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ntrain[\"SalePrice\"]","1ec42731":"# visualising normalised train data\ntrain.hist(bins=20, figsize=(20, 10))\nplt.show()","d6794202":"# visualising normalised target\nsns.histplot(train[\"SalePrice\"])","5bebf922":"X = train.drop([\"SalePrice\"], axis=1)\ny = train[\"SalePrice\"]\nprint(\"Shape of X:\", X.shape)\nprint(\"Shape of y:\", y.shape)","ea72c58f":"# filter the categorical columns from X\ncat_cols = X.select_dtypes(include=\"object\").columns\ncat_cols","a45fe2c5":"# train test split - split X, y into X_train, X_val, y_train, y_val\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.30, random_state=42)\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of y_train:\", y_train.shape)\nprint(\"Shape of X_val:\", X_val.shape)\nprint(\"Shape of y_val:\", y_val.shape)","04d2d77d":"# define column transformer pipeline\ncategorical_transformer = Pipeline(steps=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n\ncol_transform = ColumnTransformer(\n    transformers=[(\"categorical\", categorical_transformer,cat_cols)], remainder='passthrough'\n)","41ff293a":"# fit-transform column transformer on the training set X_train and based on the parameters, transform the validation set X_val\nX_train_encoded = col_transform.fit_transform(X_train)\nX_val_encoded = col_transform.transform(X_val)\nprint(\"Shape of X_train after encoding:\", X_train_encoded.shape)\nprint(\"Shape of X_val after encoding:\", X_val_encoded.shape)","f75ecbd2":"# apply column transformer and transform the test data\ntest_encoded = col_transform.transform(test)\nprint(\"Shape of test data after encoding:\", test_encoded.shape)","a7df702f":"# this code will get the original feature names along with their categories\nonehot_cols = col_transform.named_transformers_[\"categorical\"].named_steps[\"onehot\"].get_feature_names(cat_cols)","2a12a0ab":"# append the numerical columns with the encoded columns\noriginal_feature_names = list(onehot_cols) + list(train.select_dtypes(exclude=\"object\").columns.drop([\"SalePrice\"]))\noriginal_feature_names","7deb3379":"# fit-transform StandardScale on the training set X_train and based on the parameters, transform the validation set X_val\nscaler = StandardScaler(with_mean=False)\nX_train_scaled = scaler.fit_transform(X_train_encoded)\nX_val_scaled = scaler.transform(X_val_encoded)","0dcd80be":"# apply StandardScaler and standardise the test data\ntest_scaled = scaler.transform(test_encoded)","e6f0bd46":"# create an instance of the class XGBoost regressor\nxgb = xgb.XGBRegressor()","c19acbe6":"# define the parameter grid for a XGBoost model to use for Gridsearchcv\nparams = { 'max_depth': [3,6,10],\n           'learning_rate': [0.01, 0.05, 0.1],\n           'n_estimators': [500, 1000],\n           'colsample_bytree': [0.3, 0.7],\n           'alpha' : [0.1, 0.2, 0.3]}","9873e213":"# perform a gridsearch for the combination of parameters in 'params' grid for the regressor to find the model of least negative mean squared error\nxg_reg = GridSearchCV(estimator=xgb, \n                   param_grid=params,\n                   scoring='neg_mean_squared_error',\n                   cv=3,\n                   verbose=0)","5fbea39b":"# fit and train our X_train and y_train datasets on the gridsearch to get the best model\nxg_reg.fit(X_train_scaled, y_train)\n# use the best model from the gridsearch on X_test data to predict y values, y_predict\ny_predict = xg_reg.predict(X_val_scaled)","090558fe":"# calculate the mean absolute error and root mean squared error\nmae = mean_absolute_error(y_val, y_predict)\nprint(\"Mean Absolute error in log scale, mae:\", mae)\nmse = mean_squared_error(y_val, y_predict)\nrmse = np.sqrt(mse)\nprint(\"Root Mean Squared Error in log scale, rmse:\", rmse)","80f22c04":"# get the best parameter combination and the lowest root mean squared error(RMSE) for the best model\nprint(\"Best parameters:\", xg_reg.best_params_)\nprint(\"Lowest RMSE: \", (-xg_reg.best_score_)**(1\/2.0))","18ea58bb":"# best estimator from gridsearch is our final model\nprint(\"Best Estimator: \", xg_reg.best_estimator_)\nmodel = xg_reg.best_estimator_","0f982414":"# match the original feature names to the feature names of the model\nmodel.get_booster().feature_names = original_feature_names","4327e5d7":"# plot the decision tree in the model with features, feature values for each split and output leaf nodes\nfig, ax = plt.subplots(figsize=(20, 20))\nplot_tree(model, num_trees=30, ax=ax)\nplt.show()","25bfa7db":"# plot the important features selected by the model in order of their importances\nplt.figure(figsize = (20, 20))\nplot_importance(model, max_num_features=40, importance_type='weight')\nplt.rcParams[\"figure.figsize\"] = (20, 20)","bb15b900":"# plot the actual SalePrice values vs predicted SalePrice values\nplt.figure(figsize = (10, 4))\nax = sns.scatterplot(y=y_val, x=y_predict, color='crimson')\np1 = max(max(y_predict), max(y_val))\np2 = min(min(y_predict), min(y_val))\nplt.plot([p1, p2], [p1, p2], 'b-')\nax.set(ylabel = \"True values for SalePrice\", xlabel = \"Predicted values for SalePrice\", title =\"True SalePrice vs Predicted SalePrice \")","b26375a1":"# plot the error values vs the predicted values of target\nplt.figure(figsize = (10, 4))\nax = sns.residplot(x=y_predict, y = (y_val - y_predict), lowess=True, line_kws={'color': 'red'})\nax.set(xlabel = \"Predicted values\", ylabel = \"Residuals\", title =\"Residual plot\")","50421c02":"train_sizes, train_scores, validation_scores = learning_curve(model, X_train_scaled, y_train,\n                                            train_sizes=[1, 100, 300, 500], cv =3, scoring = 'neg_root_mean_squared_error')\ntrain_scores_mean = -train_scores.mean(axis = 1)\nvalidation_scores_mean = -validation_scores.mean(axis = 1)\n\nplt.plot(train_sizes, train_scores_mean, label = 'Training error')\nplt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n\nplt.ylabel('log RMSE')\nplt.xlabel('Training set size')\nplt.title('Learning curves')\nplt.legend()\nplt.ylim(0,0.6)","8b6314da":"# predict target and inverse the log transformation to get values on original scale\ntest_target = np.exp(model.predict(test_scaled))\ntest_target","9800955e":"# define a dataframe with target as SalePrice column, use Id columns from sample submission index\ndf_out = pd.DataFrame(test_target, columns=[\"SalePrice\"])\ndf_out[\"Id\"] = sample_submission[\"Id\"]\n\n# dataframe for submission\nsubmission = df_out[[\"Id\", \"SalePrice\"]]\nsubmission","902da2e4":"# convert to csv format for submission\nsubmission.to_csv(\"House_price_submission\", index=False)","c354e654":"So far we have visually analysed the features and their relationships. To make better decisions on selecting the best features, let's look at the statistics","0f39851a":"#### [Go to Index](#Index)","bbda816c":"***","d41be42e":"Let's plot all the categorical variables against our target *SalePrice* and analyse their relationship. The **variance** on the *SalePrice* for each unique category of a cloumn is shown as a box plot. The plots will give a lot of insights into the data. This will be our initial step in deciding which variables are important in predicting the target and which variables would not be crucial","fd1106f7":"We have all 5 categories of heating quality from Excellent to poor, as shown in the plot greatly correlates with the target. So this feature is important\n","ecdaefb7":"***","7e7e086e":"Correlation values are statistical readings of relationships among numerical features<br> Correlation matrix gives the relationship between each variable with respect to every other variable in the dataset, the correlation can be:\n- **positive** - when a variable responds to change in other variable in a positive way\n- **negative** - when a variable responds to change in other variable in a negative way\n- **zero** - no relationship between the 2 variables","abf20c00":"3 more Categorical features having missing values, let's analyse __MasVnrType & MasVnrArea__ together since they both represent the same property & both have 8 missing values\n\nMasVnrType: Masonry veneer type\n\t\n       BrkCmn\tBrick Common\n       BrkFace\tBrick Face\n       CBlock\tCinder Block\n       None\tNone\n       Stone\tStone\n\t\nMasVnrArea: Masonry veneer area in square feet","0c4415b2":"#### [Go to Index](#Index)","52a61574":"Let's observe the satistical details for __all numerical features__ like count, minimun value, maximum value, mean value, standard deviation etc. <br> This will help us to understand the data more statistically <br> For example, <br> In the entire dataset which is the oldest year of construction and latest year of construction of house?<br>Also, observe the columns __MSSubClass, OverallQual__ & __OverallCond__, could you spot anything different in these columns?","750be3ab":"Most datasets for predicitons have both *Numerical* and *Categorical* data<br>Features like __LotFrontage, LotArea__ have numerical data giving details of area in square feet<br>Features like __Lotshape, Utilities__ denote categories like Reg(Regular)\/AllPub(All public Utilities (E,G,W,& S))","f96c78e5":"To confirm the above points, let's do a thorough statistical exploration of the columns before acting upon the features<br>\nWhat to lookout for in categorical columns?\n\n- As we go through the dataframe, observe the values for each column. If we find suspicious distribution of values, analyse them statistically \n- We can observe more than 1 feature explaining same\/similar properties of the house,  we could analyse and drop one of them\n- If a column has same category mostly, analyse the variance of the column. Drop if the column has a very low variance","7cfa61d0":"## 7.2 Plotting Decision trees","0ce1fb9c":"Decision trees will give us insights on how the gradient boosting model works and the split decisions made at each feature node till reaching a leaf node","0c1fb480":"#### - Heating column seems to have same category for most datapoint, let's analyse\nHeating: Type of heating\n\t\t\n       Floor\tFloor Furnace\n       GasA\tGas forced warm air furnace\n       GasW\tGas hot water or steam heat\n       Grav\tGravity furnace\t\n       OthW\tHot water or steam heat other than gas\n       Wall\tWall furnace","07e7375d":"The category count is significant, we cannot drop any of the feature here, let's retain both","80fba57f":"Separate the target *SalePrice* from the train data, the **features** are stored in the variable **X** and the **target** in **y**","b1b798df":"#### [Go to Index](#Index)","37fe90cb":"Let's see **the list of Numerical and Categorical features** in the dataset","aea6e962":"#### 3.3.3 Irrelevant Features \n- Since we dropped columns having very low correlation, the features that are related to these dropped columns would become irrelevant or have no contribution to the training data, we can identify those features and remove them form train data","e45824eb":"Missing values may be *NA-No fireplce*. we have another feature __Fireplaces__ that shows the number of fireplaces in the house, let's analyse this feature","c3af6440":"More on learning curves here (https:\/\/hshan0103.medium.com\/understanding-bias-variance-trade-off-from-learning-curve-a64b4223bb02)","efb5c652":"## 3.1 Pairwise Correlation","bc9f336e":"#### 5.5 Exterior2nd is the exterior covering of house, if more than 1 material is used other than in Exterior1st, let's analyse them\nExterior1st: Exterior covering on house\n\n       AsbShng\tAsbestos Shingles\n       AsphShn\tAsphalt Shingles\n       BrkComm\tBrick Common\n       BrkFace\tBrick Face\n       CBlock\tCinder Block\n       CemntBd\tCement Board\n       HdBoard\tHard Board\n       ImStucc\tImitation Stucco\n       MetalSd\tMetal Siding\n       Other\tOther\n       Plywood\tPlywood\n       PreCast\tPreCast\t\n       Stone\tStone\n       Stucco\tStucco\n       VinylSd\tVinyl Siding\n       Wd Sdng\tWood Siding\n       WdShing\tWood Shingles\n\t\nExterior2nd: Exterior covering on house (if more than one material)\n\n       AsbShng\tAsbestos Shingles\n       AsphShn\tAsphalt Shingles\n       BrkComm\tBrick Common\n       BrkFace\tBrick Face\n       CBlock\tCinder Block\n       CemntBd\tCement Board\n       HdBoard\tHard Board\n       ImStucc\tImitation Stucco\n       MetalSd\tMetal Siding\n       Other\tOther\n       Plywood\tPlywood\n       PreCast\tPreCast\n       Stone\tStone\n       Stucco\tStucco\n       VinylSd\tVinyl Siding\n       Wd Sdng\tWood Siding\n       WdShing\tWood Shingles","38dc3d1d":"## 3. Data analysis & Feature selection - Numerical","3bd8f95b":"Let's impute the most frequent category for the missing value","f824be6f":"The values are same for 1089 data points(75% of data)<br>so we cannot drop the *GarageYrBlt* feature, for 25% of houses, Garage built year is different.<br> Let's locate the null values in the dataframe and investigate further how to impute those missing 81 values","bcb014a1":"**Train** has 1460 data observations, 80 features + 1 target(SalePrice column)<br> **Test** has 1459 data observations, 80 features<br> We have to train a model using the given train data and predict SalePrice for the test data<br> The **EDA findings from train data will be used for preparing and pre-processing the test data**","456c7cea":"The data on which our model is to be trained has all the different ranges of values and different metrics in data such as area in square feet, years, ratings from 1 to 10 etc., to get a uniformity in values, we need to **scale the data**. **StandardScaler** is one method of standardisation of data","deb2036b":"There are missing values in both categorical and numerical features, each of them need separate treatment for dealing with missing values<br> First, let's see the missing values in __Numerical features__","5641f928":"***","512392ba":"Our test data is already cleaned and pre-processed, let's fit our trained model on test data to predict target","ab9899ba":"We have successfully imputed missing values in numerical & categorical features! **No null values** in the train dataset now<br>Along the way we analysed the related features and removed 3 redundant features!!","d76a3508":"Now, it's clear that **PoolArea is 0** for same 1453 datapoints, that means 1453 houses in data **don't have pool**<br>Clearly *PoolArea* & *PoolQC* have **very low variance\/nearly zero variance**, they won't help in improving the model performance, so we can drop *PoolArea & PoolQC*","f8590521":"Now, it's time for **datacleaning**<br>There are many ways to deal with null values:\n- drop the rows with null values(we're not choosing this method here since information will be lost if rows are dropped)\n- **drop** the entire column with null values\n   - if the data is missing at random, \n   - if column becomes redundant & \n   - if we don't have a reason to have that column for training   \n- **impute** missing values\n   - if the data is not missing at random, \n   - if we can find why the values are missing & \n   - we have a reason to keep the column for training\n\nBefore deciding on the method(dropping or imputing), list out all the Null\/missing values from the entire dataset","a004517a":"___","2baf5cda":"In Train data, we have correlation from 0.74 to 0.04 in positive value and from -0.01 to -0.13 in negative value. Any value above 0.3 can be considered as strong correlation in positive and negative side. We don't have strong negative correlation here, so let's consider only positive correlation values<br> Let's **retain the features with threshold > 0.3**  and drop features with threshold < 0.3","ef6a5115":"**GarageType, GarageFinish, GarageQual, GarageCond** all describing the properties of Garage, have 81 values missing<br>The 81 missing values are from **same rows** (we confirmed this when analysing the **GarageYrBlt** column)","2b2a9c05":"By analysing at the count values in categories, the 1179 missing values beong to the missing category *NA - No Fence*, let's impute 'NA'","c459e69f":"BsmtFinSF2: Type 2 finished square feet\n\nBsmtFinType2: Rating of basement finished area (if multiple types)\n\n       GLQ\tGood Living Quarters\n       ALQ\tAverage Living Quarters\n       BLQ\tBelow Average Living Quarters\t\n       Rec\tAverage Rec Room\n       LwQ\tLow Quality\n       Unf\tUnfinshed\n       NA\tNo Basement\n\n","d658891c":"*LandSlope* has simplified categories compared to *LandContour*, so let's drop *LandContour*","b24c6184":"Clearly *Utilities* won't help to improve the performance of the model, it has **very low variance** in data, since all the houses fall into 1 category(well, except for only 1), so drop *Utilities* feature","b072cbda":"#### *Insights from Learning Curves:*\n\n**Total error in prediction = Bias + variance + Irreducible error**\n - Irreducible error cannot be minimised, this is the error made by the algorithm\n - We can reduce the error due to bias and variance\n - **Bias**: Error showing how far the predicted values are from actual value\n   - In our model, bias is low(the training error curve shows low error)\n - **Variance**: Error when the model is subjected to a new set of data\n   - In our model, variance is high (the gap between training and validation error curves)\n - Our model has **low bias, high variance**. It suffers from **overfitting**\n - When the 2 curves converge, variance error will reduce. The curves are starting to converge, we can provide more training data to reduce the gap\n - We can improve our model by\n   - **increasing data** for training, so that the model learns from the data and becomes generalised\n   - decreasing the complexity of data, by **reducing some of the features**","b93f6fd2":"After log transformation, the data has been approximated to a normal distribution","48a37367":"In *MasVnrType*, \"CBlock(Cinder Block)\" category is missing, however without statistics confirmation, we cannot impute 'CBlock' for the 8 missing values, so we impute most frequent values in both columns:\n- Impute **0** for missing values in *MasVnrArea*\n- Impute **mode**('None' category) for *MasVnrType*","270d6cce":"#### - PavedDrive feature has same category *'Y'* in visual check, let's analyse the variance\nPavedDrive: Paved driveway\n\n       Y\tPaved \n       P\tPartial Pavement\n       N\tDirt\/Gravel","ef893110":"#### [Go to Index](#Index)","f35e1410":"## Index\n\n#### Exploratory Data Analysis & Feature Selection\n\n   1. [Inspect data](#1.-Inspect-Data)\n   \n   2. [Handle missing values](#2.-Handle-missing-values)\n    \n        2.1 [Missing values in Numerical features](#2.1-Missing-values-in-Numerical-features)<br>\n        2.2 [Missing values in Categorical features](#2.2-Missing-values-in-Categorical-features)\n        \n   3. [Data analysis & Feature selection - Numerical](#3.-Data-analysis-&-Feature-selection---Numerical)   \n          \n        3.1 [Pairwise Correlation](#3.1-Pairwise-Correlation)        \n        3.2 [Correlation matrix & Heatmap](#3.2-Correlation-matrix-&-Heatmap)        \n        3.3 [Feature Analysis based on Correlation](#3.3.-Feature-Analysis-based-on-Correlation)\n    \n   4. [Data analysis & Feature selection - Categorical](#4.-Data-analysis-&-Feature-selection---Categorical)\n   \n#### Data Pre-Processing\n    \n   5. [Skewness and Normalisation](#5.-Skewness-and-Normalisation)\n    \n   6. [Training set-Validation set split, Pre-processing steps: OneHotEncoding, Standardisation](#6.-Training-set-Validation-set-split,-Pre-processing-steps:-OneHotEncoding,-Standardisation)\n    \n#### Model Training, Evaluation &  Validation\n    \n   7. [Extreme Gradient Boosting(XGB) Regressor](#7.-Extreme-Gradient-Boosting(XGB)-Regressor)\n    \n        7.1 [Hyperparameter Tuning](#7.1-Hyperparameter-Tuning)        \n        7.2 [Plotting Decision trees](#7.2-Plotting-Decision-trees)        \n        7.3 [Feature importance](#7.3-Feature-importance)        \n        7.4 [Predicted vs Actual plot](#7.4-Predicted-vs-Actual-plot)       \n        7.5 [Residual plot](#7.5-Residual-plot)       \n        7.6 [Learning Curve-Bias\/variance Tradeoff](#7.6-Learning-Curve-Bias\/variance-Tradeoff)\n        \n#### Target Prediction\n        \n   8. [Predict target for Test Data](#8.-Predict-target-for-Test-Data)<br><br>  \n   \n                \n                   \n   References:\n   \n   - [Inspiration from Gabriel Atkin's ML process pipeline video](https:\/\/www.youtube.com\/watch?v=zwYHloLXH0c&t=5885s)   \n   - [Kaggle notebook by Abu Bakar](https:\/\/www.kaggle.com\/bakar31\/eda-house-price-prediction)           \n   - [My earlier attempt of this problem](https:\/\/www.kaggle.com\/iravad\/house-price-advanced-regression-techniques-lasso)\n    \n     \n    ","24769dda":"#### - The related column *Heating QC: Heating quality and condition* - even if the type of heating is same throughout, the quality of heating may vary and it may be correlated with target. Let's analyse\nHeatingQC: Heating quality and condition\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tAverage\/Typical\n       Fa\tFair\n       Po\tPoor\n","427df0cb":"## 7. Extreme Gradient Boosting(XGB) Regressor","c5bd5973":"There are 690 counts of zero fireplaces, that matches with our 690 missing values in *FirelaceQu*, so impute them as 'NA - No Fireplace'","69ecd9b4":"## 7.3 Feature importance","7f5360ed":"__BsmtCond, BsmtFinType1, BsmtQual__ features have 37 missing values, let's see if the missing values are from the same rows<br>\nBsmtQual: Evaluates the height of the basement\n\n       Ex\tExcellent (100+ inches)\t\n       Gd\tGood (90-99 inches)\n       TA\tTypical (80-89 inches)\n       Fa\tFair (70-79 inches)\n       Po\tPoor (<70 inches\n       NA\tNo Basement\n\t\t\nBsmtCond: Evaluates the general condition of the basement\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical - slight dampness allowed\n       Fa\tFair - dampness or some cracking or settling\n       Po\tPoor - Severe cracking, settling, or wetness\n       NA\tNo Basement\n \nBsmtFinType1: Rating of basement finished area\n\n       GLQ\tGood Living Quarters\n       ALQ\tAverage Living Quarters\n       BLQ\tBelow Average Living Quarters\t\n       Rec\tAverage Rec Room\n       LwQ\tLow Quality\n       Unf\tUnfinshed\n       NA\tNo Basement","7e833177":"***","e4e6c098":"let's convert them into **Categorical datatypes**","4a47c892":"Below features have high correlation with target as observed from the heatmap:\n* OverallQual\n* GeLivArea\n* GarageCArs\n* GArageArea\n* TotalBcmtSF\n* 1stFlrSF\n* YearBuilt\n* YearRemodAdd etc.","70206fcd":"As we analysed the data, the distribution plots in the pairplot show that target variable *SalePrice* and some features are **highly skewed**. Data should follow normal distribution for the model to successfully predict target, because regression based models assume **normally distributed data for good predcition results**","89ffadf1":"Now, we've managed the missing values for Numerical Features. Let's analyse the missing values in __Categorical features__","85aeeed9":"#### [Go to Index](#Index)","07837db0":"In the above step we are predicting y_predict by applying our trained model on X_val_scaled, validation data. By comparing the predicted values y_predict to the actual value y_val, we can calculate the error<br>**Root Mean Squared Error**, rmse is the error metric used to evaluate the model. The error should be the least possible for our prediction to be accurate","0baa660e":"***","eecb9388":"___","8c3dd8ae":"The feature *BsmtFinSF2*(Type 2 finished square feet) has been dropped now, so the feature explaining about the rating of BsmtFinSF2 will become redundant, so let's drop the feature *BsmtFinType2*","f45b601b":"#### [Go to Index](#Index)","e1366522":"Feature importance plot shows the **most important features** used for prediction sorted in order of their importances<br>\nTotalBsmtSF(Total Basement area) is the top feature determining the target value, followed by GrLivArea(area above the ground), BsmtFinSF1(Type 1 finished area) and others","f950e336":"The residuals or error is the difference between the actual and predicted SalePrice. The plot has residuals along the y-axis and it's corresponding predcited SalePrice along the x-axis\n- The points above the 0 mid-line in y-axis(positive residuals) represent low prediction compared to actual SalePrice, the points below the 0 mid-line in y-axis(negative residuals) represent high prediction compared to actual SalePrice\n- The residual points on the 0 mid-line shows zero error\n- Most points clustering around the zero line shows error is lower\n- The plot should not have specific cluster or clear pattern for the model to be good, if the plot has a clear pattern\/shape, then there is room for improvement in a model. Plot **shape** of our model is **random** and our linear assumption hold true \n- The red line along the zero error line shows that the model has predicted well for most cases except for low range value in SalePrice","7b9f0f6a":"__LotFrontage__ has 259 null values<br>LotFrontage: Linear feet of street connected to property","6055729e":"All these features have a category 'NA' for 'No garage', the missing 81 values belong to 'NA'<br>Impute missing values as 'NA'","db19f44a":"#### - OverallQual & OverallCond are different properties. On a visual inspection of the data, the ratings are different in the 2 columns for each datapoint. So no action is needed\nOverallQual: Rates the overall material and finish of the house\n\n       10\tVery Excellent\n       9\tExcellent\n       8\tVery Good\n       7\tGood\n       6\tAbove Average\n       5\tAverage\n       4\tBelow Average\n       3\tFair\n       2\tPoor\n       1\tVery Poor\n\t\nOverallCond: Rates the overall condition of the house\n\n       10\tVery Excellent\n       9\tExcellent\n       8\tVery Good\n       7\tGood\n       6\tAbove Average\t\n       5\tAverage\n       4\tBelow Average\t\n       3\tFair\n       2\tPoor\n       1\tVery Poor","90623fda":"*MiscFeature* has 54 datapoints in 4 categories, we suspect missing 1406 datapoints may belong to the 'NA-None' category<br>Even if we impute the 1406(96%) datapoints with 'NA', the overall **variance** for the column will be **very low** & it will not contribute to the performance of the model. So drop the column","fa1297f7":"#### - Similar\/identical categorical features\n*LandContour & LandSlope* **both explain flatness** of the property in 2 different ways","9f918225":"We have completed the pre-processing of training and validation datasets. Now, we are ready to train the data on our model.<br>We are training the data on an XGBoost model. **Extreme Gradient(XG) Boosting** is an improvisation of an **ensemble algorithm** called Gradient Boosting and is known for great accuracy and speed","d23db61c":"The values look same except for few, many data points overlap<br>Statistics can help to get the real numbers!\n","cb241610":"**FireplaceQu** has 690 missing values<br>\nFireplaceQu: Fireplace quality\n\n       Ex\tExcellent - Exceptional Masonry Fireplace\n       Gd\tGood - Masonry Fireplace in main level\n       TA\tAverage - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n       Fa\tFair - Prefabricated Fireplace in basement\n       Po\tPoor - Ben Franklin Stove\n       NA\tNo Fireplace\n","cb091ff7":"***","1f65e445":"## 2.2 Missing values in Categorical features","313f43ff":"The missing 38 values are from **same rows** and category *NA - No Basement* is missing from both columns<br> Impute the missing values as 'NA'","dc890f28":"We start inspecting the Training data, let's take a visual look at the features and Target and let's try to understand the given set of properties of a house. We may not understand the data in-depth for now, that's okay. Let's just get to know about the data first. We will do a full exploratory analysis later to get into the technicalities of each feature","2f60c5e0":"The missing 37 values are *NA-No Basement*, they are from same rows as 'NA' is missing from above 3 features","383382a2":"Until now, we have analysed only the numeric features and their correlation using heatmap & correlation matrix. The dataset has numerous categorical features. Let's analyse the categorical data, select relevant features & drop redundant features using statistics","5b033ed7":"Exploring __PoolQC__ feature:","faa57ce6":"GarageType: Garage location\n\t\t\n       2Types\tMore than one type of garage\n       Attchd\tAttached to home\n       Basment\tBasement Garage\n       BuiltIn\tBuilt-In (Garage part of house - typically has room above garage)\n       CarPort\tCar Port\n       Detchd\tDetached from home\n       NA\tNo Garage\n\t\t\nGarageFinish: Interior finish of the garage\n\n       Fin\tFinished\n       RFn\tRough Finished\t\n       Unf\tUnfinished\n       NA\tNo Garage\n\t\t\n\nGarageQual: Garage quality\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical\/Average\n       Fa\tFair\n       Po\tPoor\n       NA\tNo Garage\n\t\t\nGarageCond: Garage condition\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical\/Average\n       Fa\tFair\n       Po\tPoor\n       NA\tNo Garage","11207a80":"## 2.1 Missing values in Numerical features","78b963a3":"Why do you think *PoolQC* has 1453 null values? Investigate...<br>PoolQC: Pool quality\n\t\t\n       Ex\tExcellent\n       Gd\tGood\n       TA\tAverage\/Typical\n       Fa\tFair\n       NA\tNo Pool","bb44c716":"Let's observe this feature's relationship with target","230db32e":"As we can see here, most of the columns have skewed to the right side. **Right skewed data** model can be good in predicting only lower value ranges but predcition will be poor in higher value ranges due to less data in higher value range. The data has to be normalised before training it on an algorithm","dc24eef3":"__BsmtExposure, BsmtFinType2__ both features have 38 missing values, let's see if the missing values are from the same rows<br>\nBsmtExposure: Refers to walkout or garden level walls\n\n       Gd\tGood Exposure\n       Av\tAverage Exposure (split levels or foyers typically score average or above)\t\n       Mn\tMimimum Exposure\n       No\tNo Exposure\n       NA\tNo Basement\n       \n BsmtFinType2: Rating of basement finished area (if multiple types)\n\n       GLQ\tGood Living Quarters\n       ALQ\tAverage Living Quarters\n       BLQ\tBelow Average Living Quarters\t\n       Rec\tAverage Rec Room\n       LwQ\tLow Quality\n       Unf\tUnfinshed\n       NA\tNo Basement","bc24e5f1":"## 5. Skewness and Normalisation","d325c85f":"## 7.1 Hyperparameter Tuning","66686ec5":"**Test data has null values in more columns** than train data, we have to clean the test data by **imputing mean** for numerical columns and **mode** for categorical columns. We cannot drop the columns with missing values, since it will lead to shape mismatch later during the model training stage","b14d620d":"In the above heatmap, lighter the hue, more the correlation & darker the hue, lesser the correlation between variables along X & Y axis<br>We can see the highly correlated features with target (*SalePrice* in Y-axis bottom)as lighter colors in the bottom of the plot","befdc5cf":"#### - Utilities appears to have same category value for many datapoints, let's check it's variance\nUtilities: Type of utilities available\n\t\t\n       AllPub\tAll public Utilities (E,G,W,& S)\t\n       NoSewr\tElectricity, Gas, and Water (Septic Tank)\n       NoSeWa\tElectricity and Gas Only\n       ELO\tElectricity only\t\n\t","fbb0d34e":"#### - Street appears to have same category *Paved* for most of the datapoints, let's verify it's variance by analysing the unique category counts\nStreet: Type of road access to property\n\n       Grvl\tGravel\t\n       Pave\tPaved","19c458bd":"Checking for missing values in train again","c75a2e48":"The model should be trained on a set of data and **validated on another new set of data**. Here we split the features and target in the ratio of 70% training data and 30% validation data. A validation dataset is needed to validate the results of a trained model. We will train a model using X_train, y_train and use our model on X_val to predict y values","4b0498c5":" - A value of 1 is correlation with itself, so *SalePrice* has 1\n - We have values starting from 0.79 for *OverallQual* to 0.04 for *3SsnPorch* and then we have negative correaltion for some features\n - Correlation **above 0.3 can be taken as high positive correlation and below -0.3 as high negative correlation**\n - Negatively correlation also affects target prediction, so consider both +ve, -ve highly correlated features\n - Also, if the correlation between 2 features are **similar**, we can drop one of the features as similarly correlated features will add to the complexity of the model and don't necessarily improve the performance","0e36f6c9":"**GarageYrBlt** is the next numerical feature with 81 missing values<br>GarageYrBlt: Year garage was built <br>Let's analyse if the year the garage was built is the same as the *YearBuilt*(the year the house was built). <br> Garages *might* have been built along with the house at the same time, let's look into the statistcs<br>If they're the same, we can drop the *GarageYrBuilt* feature since it may be regarded as duplicate information","bdd21915":"Exploring the relationships between features is crucial in data analysis in order to select the best set of features. A pairplot helps to analyse each numerical feature in relation to every other numerical feature in a dataset<br> Since we have a lot of numerical features in train data, a single pairplot with all numerical columns becomes complex and takes a long time to execute. Let's plot 3 separate pairplots for better visualisation","571343f9":"#### - Other 2 similar features that explain proximity are:\n\nCondition1: Proximity to various conditions\n\t\n       Artery\tAdjacent to arterial street\n       Feedr\tAdjacent to feeder street\t\n       Norm\tNormal\t\n       RRNn\tWithin 200' of North-South Railroad\n       RRAn\tAdjacent to North-South Railroad\n       PosN\tNear positive off-site feature--park, greenbelt, etc.\n       PosA\tAdjacent to postive off-site feature\n       RRNe\tWithin 200' of East-West Railroad\n       RRAe\tAdjacent to East-West Railroad\n\t\nCondition2: Proximity to various conditions (if more than one is present)\n\t\t\n       Artery\tAdjacent to arterial street\n       Feedr\tAdjacent to feeder street\t\n       Norm\tNormal\t\n       RRNn\tWithin 200' of North-South Railroad\n       RRAn\tAdjacent to North-South Railroad\n       PosN\tNear positive off-site feature--park, greenbelt, etc.\n       PosA\tAdjacent to postive off-site feature\n       RRNe\tWithin 200' of East-West Railroad\n       RRAe\tAdjacent to East-West Railroad","f4bcd917":"Looking in the dataframe, we have null values in all columns related to Garage(GarageType, GarageYrBlt, GarageFinish, GarageCars, GarageArea, GarageQual, GarageCond) for those 81 rows. That implies **those houses don't have a garage, so we cannot impute mean value here for *GarageYrBlt*, impute '0' for the null values**","7d51c12e":"Drop **Condition2** since almost all(1445) houses have 'Norm' category and this column has low variance","d0df01de":"**GrLivArea, TotRmsAbvGrd and BedroomAbvGr** have strong positive correlation with SalePrice","7135bcf0":"We import the library requirements here","64ae5858":"Let's take a look at the **target**","88280456":"#### Variance is the measure of spread between the unique values\/categories in a column. If more than 95% of the values in the column belong to a specific category then we say the column has low variance. The 5% of values in other categories cannot help in prediction and the column is considered to be a constant. The column will not improve the model performance in predicting the target, so we can drop the column","fe5417e3":"#### - When going through the *CentralAir*, it looks like, it has the same category *Y* throughout. If the category is same for all the datapoints, then we can drop the column. Let's analyse for counts\nCentralAir: Central air conditioning\n\n       N\tNo\n       Y\tYes","a4190236":"Let's explore the features based on their correlation values<br>\n\n#### 3.3.1 Analysing features having high correlation(>0.3) with target\n- GrLivArea: Above grade (ground) living area square feet\n- 1stFlrSF: First Floor square feet\n- 2ndFlrSF: Second floor square feet\n\n    - __GrLivArea__ has 0.708624 correlation with *SalePrice*, when we analyse this feature in the dataframe, this shows the *living area above ground* and this column is the sum of __1stFlrSF & 2ndFlrSF__, so we can simplify our training data by dropping the 1stFlrSF & 2ndFlrSF which are already included in *GrLivArea*","0ee233ff":"We have extracted only the categorical column names after encoding, to get all the column names from the training data, append the numerical columns to the list","67fb6d06":"The diagonal histograms in the pairplot show the distributions of individual columns along the axes and each scatter plot shows the relationship between columns along the x and y axes. For example,\n  - We can see strong positive linear relationship between columns **SalePrice and 1stFlrSF(1st floor area)** where the plot is a upward diagonal. \n  - But we can infer that **LotArea and YrBuilt** don't have a linear relationship. \n  - Pairplot can tell us about very few datapoints that are far away from the clusters, they are outliers that have extreme values deviating from the normal distribution. \n  - There can be **univariate**(that deviate from other datapoints in an individual feature) and **bivariate outliers**(that have abnormal readings\/values when 2 features are taken in account)<br>\n\nSimilar insights can be derived from the below pairplots too\n","54f36b17":"LandContour: Flatness of the property\n\n       Lvl\tNear Flat\/Level\t\n       Bnk\tBanked - Quick and significant rise from street grade to building\n       HLS\tHillside - Significant slope from side to side\n       Low\tDepression\n\t\t\nLandSlope: Slope of property\n\t\t\n       Gtl\tGentle slope\n       Mod\tModerate Slope\t\n       Sev\tSevere Slope","67e8176b":"## 4. Data analysis & Feature selection - Categorical","7629c2ad":"Here is a guide to XGBoost Decision Tree and feature importance visualisations(https:\/\/machinelearningmastery.com\/visualize-gradient-boosting-decision-trees-xgboost-python\/)","b1df6587":"Next column with more null values is __MiscFeature__, let's investigate the reason<br>\nMiscFeature: Miscellaneous feature not covered in other categories\n\t\t\n       Elev\tElevator\n       Gar2\t2nd Garage (if not described in garage section)\n       Othr\tOther\n       Shed\tShed (over 100 SF)\n       TenC\tTennis Court\n       NA\tNone","eec9658d":"## 2. Handle missing values","f3f9071b":"## 1. Inspect Data","ea572e2e":"***","2710b6f2":"We're having more null values in these columns:<br> **PoolQC          1453<br>\nMiscFeature     1406<br>\nAlley           1369<br>\nFence           1179**<br><br>They are **more than 80% of missing values** in each of the above columns, if missing values are more than 30% of total data we can drop the columns assuming that they don't contribute to overall performance. Imputing them with mean\/median\/mode without proper analysis will lead to bias in the model<br>Let's **analyse the reason** for the large percentage of missing data in certain features. Sometimes, missing values are not because of entry error\/at random, they may be missing intentionally because the data is not applicable\/intended to be in any of the categories","50853c33":"*SaleType or SaleCondition* columns have no abnormality in their category values, so no change is needed here","64bf9398":"## 7.5 Residual plot","69b14e90":"In *RoofMatl* 98% of datapoints fall under 1 category *CompShg*, so this feature won't be of importance in prediciting the target<br>Since it's **variance is less**, let's drop RoofMatl","bd9e44ae":"## 7.6 Learning Curve-Bias\/variance Tradeoff\n\nLearning curves can give us insights on how to further improve our model","ea99dab8":"***","db4324b3":"The __YearBuilt__ - *Original construction date*, statistics shows min value of original construction as 1872 and max values as 2010!<br> Please note that __MSSubClass, OverallQual__ & __OverallCond__ have **numerical data** but on a closer inspection of min, 25%, 50%, 75% & max values, they are **Categorical variables** indicating categories & **ratings** as numbers! We can confirm this by looking at the data description of these features ","20d397e1":"A **Correlation Matrix** gives the statistical relationship between every column to every other other column in a dataframe. The diagonal values are 1 as they are correlation values of column with itself. We can also see how each column is related to target. The correlation matrix can be **visualised using a heatmap**","be7e3c8e":"#### [Go to Index](#Index)","5f02042e":"**Skewness value** of < -1 and > 1 is considered that the data is highly skewed<br>\nFeatures with skewness >1 are positively skewed and features with skewness < -1 are negatively skewed. we don't have highly negatively skewed data in train dataset<br>**Filter** features having highly positively skewed data, **skewness > 1**<br>Drop *SalePrice* from the list, we'll deal with target skew later","89543454":"## 8. Predict target for Test Data","6b9b1b65":"Next feature with missing values is **Fence**, 1179 values are missing, let's analyse:<br>\nFence: Fence quality\n\t\t\n       GdPrv\tGood Privacy\n       MnPrv\tMinimum Privacy\n       GdWo\tGood Wood\n       MnWw\tMinimum Wood\/Wire\n       NA\tNo Fence","58e12937":"#### 3.3.2 Analysing features with similar correlation values\n- GarageCars: Size of garage in car capacity\n\n- GarageArea: Size of garage in square feet\n\n    - __GarageArea & GarageCars__ both explain Garage size in different metrics, so we can drop *GarageArea* & retain *GarageCars* which has high correlation with *Saleprice* compared to *GarageArea*","016bd770":"99.6% of rows have *paved* category, only 0.4% of total data has *Grvl* category, this column has **very low variance** and will not help in predicting the target, we can drop this feature ","2990b5bb":"Both are entirely different, *ExterQual* reflects the quality of exterior whereas *ExterCond* reflects the present condition of the exterior irrespective of quality, let's retain these features for training","c379199d":"**House Price prediction:**<br> I'm attempting this competition a second time here, my first attempt was 4 months ago, with a public score of 0.16806, when I didn't analyse the data extensively and feature selection was based on assumptions. My goal this time is to get better at my **EXPLORATORY DATA ANALYSIS-EDA** and overall process and not just to improvise the score. We will be analysing data extensively based on statistics and not on assumptions<br><br>\n**Update:** EDA and Feature Engineered data in this notebook coupled with a basic Linear Regression model got an improvement in the score(rmse: 0.14285) compared to my previous notebook attempt using Hyperparameter tuned Lasso Regression(rmse: 0.16806), I think that implies deeper Data Analysis alone can greatly improve the performance without the need for using complex models for training. May be that's why we hear lot of Data Scientists stressing on the importance of preparing and working on the data rather than fancy models.<br><br>Please upvote if you find the notebook useful","fc8ba85f":"#### - Let's analyse RoofStyle & RoofMatl both explaining about roof structure, they seem to have same values when we observe in the dataframe:\n\nRoofStyle: Type of roof\n\n       Flat\tFlat\n       Gable\tGable\n       Gambrel\tGabrel (Barn)\n       Hip\tHip\n       Mansard\tMansard\n       Shed\tShed\n\t\t\nRoofMatl: Roof material\n\n       ClyTile\tClay or Tile\n       CompShg\tStandard (Composite) Shingle\n       Membran\tMembrane\n       Metal\tMetal\n       Roll\tRoll\n       Tar&Grv\tGravel & Tar\n       WdShake\tWood Shakes\n       WdShngl\tWood Shingles","db08a217":"***","a08aa65e":"MSSubClass: Identifies the type of dwelling involved in the sale.\t\n\n        20\t1-STORY 1946 & NEWER ALL STYLES\n        30\t1-STORY 1945 & OLDER\n        40\t1-STORY W\/FINISHED ATTIC ALL AGES\n        45\t1-1\/2 STORY - UNFINISHED ALL AGES\n        50\t1-1\/2 STORY FINISHED ALL AGES\n        60\t2-STORY 1946 & NEWER\n        70\t2-STORY 1945 & OLDER\n        75\t2-1\/2 STORY ALL AGES\n        80\tSPLIT OR MULTI-LEVEL\n        85\tSPLIT FOYER\n        90\tDUPLEX - ALL STYLES AND AGES\n       120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n       150\t1-1\/2 STORY PUD - ALL AGES\n       160\t2-STORY PUD - 1946 & NEWER\n       180\tPUD - MULTILEVEL - INCL SPLIT LEV\/FOYER\n       190\t2 FAMILY CONVERSION - ALL STYLES AND AGES\n       \n OverallQual: Rates the overall material and finish of the house\n\n       10\tVery Excellent\n       9\tExcellent\n       8\tVery Good\n       7\tGood\n       6\tAbove Average\n       5\tAverage\n       4\tBelow Average\n       3\tFair\n       2\tPoor\n       1\tVery Poor\n\t\nOverallCond: Rates the overall condition of the house\n\n       10\tVery Excellent\n       9\tExcellent\n       8\tVery Good\n       7\tGood\n       6\tAbove Average\t\n       5\tAverage\n       4\tBelow Average\t\n       3\tFair\n       2\tPoor\n       1\tVery Poor","0f8480e5":"We have 95(6.5% of total data) houses without central air conditioning. The variance is high enough to retain this feature in our train data","3337f876":"The plot shows a good fit of the predicted values to the actual values along the regression line<br>Overall, the values hold together without much devation form the line showing the **error in prediction is very less**. However, the error is more in lower value range and higher value range of SalePrice","7342155f":"#### [Go to Index](#Index)","dcf82144":"PoolArea: Pool area in square feet <br>is a feature related to *PoolQC*, inspect ","4b23a5bd":"The SalePrice for maximum number of houses in the data is in between the price range 100000 and 200000. The data is **skewed** with more houses in the price range upto $ 300000 and on the right side of the histogram, there are very less number of houses with more SalePrice","ffa7dbd8":"Looking at the datatypes, non-null count of all the features, we will roughly have an idea about the columns and their missing values in general","512346b4":"Next feature to have more missing values is **Alley**, it has 1369 null values<br>\nAlley: Type of alley access to property\n\n       Grvl\tGravel\n       Pave\tPaved\n       NA \tNo alley access","fc62d045":"Hyper parameter tuning is process of choosing the right combination of parameters for an estimator from a given set of parameters, to get the best prediction in a model. Here we're using Grid Search Cross validation for tuning method","726258d0":"#### [Go to Index](#Index)","215a3229":"It's always good to analyse for any missing values in a dataset and clean the dataset before training our model","c8eb6ace":"98% of houses in the data use same type of heating(GasA), so it's **variance negligible**, drop the *Heating* feature","59c7669e":"Only 91% of data have category *'Y'*, so the column cannot be dropped","91968bc8":"#### - Finally let's analyse *SaleType* or *SaleCondition* features\nSaleType: Type of sale\n\t\t\n       WD \tWarranty Deed - Conventional\n       CWD\tWarranty Deed - Cash\n       VWD\tWarranty Deed - VA Loan\n       New\tHome just constructed and sold\n       COD\tCourt Officer Deed\/Estate\n       Con\tContract 15% Down payment regular terms\n       ConLw\tContract Low Down payment and low interest\n       ConLI\tContract Low Interest\n       ConLD\tContract Low Down\n       Oth\tOther\n       \nSaleCondition: Condition of sale\n\n       Normal\tNormal Sale\n       Abnorml\tAbnormal Sale -  trade, foreclosure, short sale\n       AdjLand\tAdjoining Land Purchase\n       Alloca\tAllocation - two linked properties with separate deeds, typically condo with a garage unit\t\n       Family\tSale between family members\n       Partial\tHome was not completed when last assessed (associated with New Homes)\n","c4e0c68f":"One hot encoding method will increase the number of columns since it encodes and transforms each category in a column into a new column. It also change the names of the categorical features into numericals, we need our **original column names** for evaluating our model, so get the original feature names form encoded data. after encoding, the column names have their respective category","61dfaa7b":"***","cfb31d5f":"Machine learning algorithms can only take numerical inputs, so we need to **encode the categories** in categorical columns into numerical values<br> **One hot encoding** is one of the methods to encode categories. We will build a pipeline below using **ColumnTransformer** to do this task, where it will encode the categorical columns but will 'passthrough' the numerical columns without any change","d80e3f11":"## 3.2 Correlation matrix & Heatmap","6e888c4f":"Though visually it seems both columns have similar categories and similar correlation with target, 15% of values differ for both columns, only 85% have the same material used for both exterior covering of the house<br>So we cannot  drop *Exterior2nd*<\/mark>","0347fcc0":"*LotFrontage* has a linear relationship with *SalePrice*<br>let's impute the missing values using the mean value of the column","092aef66":"***","26f534b4":"#### - GarageQual & GarageCond on visual inspection have same categories, let's explore them together\nGarageQual: Garage quality\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical\/Average\n       Fa\tFair\n       Po\tPoor\n       NA\tNo Garage\n\t\t\nGarageCond: Garage condition\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical\/Average\n       Fa\tFair\n       Po\tPoor\n       NA\tNo Garage","d0b111d2":"Let's look at the statistics of correlation value of each variable vs target<br>\nCalculate the **correlation of every numerical feature against SalePrice** and sort them from max to min values","c94266cc":"#### 5.6 ExterQual & ExterCond seem to explain about the quality and condition of the exterior, are they same?\nExterQual: Evaluates the quality of the material on the exterior \n\t\t\n       Ex\tExcellent\n       Gd\tGood\n       TA\tAverage\/Typical\n       Fa\tFair\n       Po\tPoor\n\t\t\nExterCond: Evaluates the present condition of the material on the exterior\n\t\t\n       Ex\tExcellent\n       Gd\tGood\n       TA\tAverage\/Typical\n       Fa\tFair\n       Po\tPoor\n\t\t","d2dee6c2":"For numerical variables, we will **impute** missing values with the mean\/median of the column data (depending on the type of value column<br> for example, for area values we can impute using mean and for age\/year values we can impute using median values)","a14ed7ce":"We have analysed all the features(both Numerical & Categorical) and based on the statistics, we dropped some features that are redundant, duplicate and those don't contribute in improving the performance of the model. The number of features have been **reduced from 80 to 50**, a **simplified model** will help in better prediciton of target","af00b1bf":"Now that we took care of all missing values in the train data, let's first analyse Numerical features by selecting relevant features & dropping redundant features using statistics.<br> **Correlation matrix & heatmap** can be used to explore numerical features and their correlation values","ea8d4739":"#### - BsmtQual & BsmtCond explain the height and general condition of the basement respectively\nBsmtQual: Evaluates the height of the basement\n\n       Ex\tExcellent (100+ inches)\t\n       Gd\tGood (90-99 inches)\n       TA\tTypical (80-89 inches)\n       Fa\tFair (70-79 inches)\n       Po\tPoor (<70 inches\n       NA\tNo Basement\n\t\t\nBsmtCond: Evaluates the general condition of the basement\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical - slight dampness allowed\n       Fa\tFair - dampness or some cracking or settling\n       Po\tPoor - Severe cracking, settling, or wetness\n       NA\tNo Basement","e0eb6f13":"*Alley* has only 2 categories with 91 values<br> Missing 1369 values contribute to 93.7% of total data(threshold for low variance is 95%), so cannot be considered low variance & we cannot drop the column<br>Missing values must belong to 3rd category *NA - No alley access*<br>Let's impute 'NA' for missing values","ca2f477f":"#### - BldgType & HouseStyle are entirely different features, as we observe from the descriptions and datapoints, so they don't need any action\n\nBldgType: Type of dwelling\n\t\t\n       1Fam\tSingle-family Detached\t\n       2FmCon\tTwo-family Conversion; originally built as one-family dwelling\n       Duplx\tDuplex\n       TwnhsE\tTownhouse End Unit\n       TwnhsI\tTownhouse Inside Unit\n\t\nHouseStyle: Style of dwelling\n\t\n       1Story\tOne story\n       1.5Fin\tOne and one-half story: 2nd level finished\n       1.5Unf\tOne and one-half story: 2nd level unfinished\n       2Story\tTwo story\n       2.5Fin\tTwo and one-half story: 2nd level finished\n       2.5Unf\tTwo and one-half story: 2nd level unfinished\n       SFoyer\tSplit Foyer\n       SLvl\tSplit Level\n\t","4568fdab":"## 7.4 Predicted vs Actual plot","54b91fc0":"Our goal is to predict the SalePrice of a house by training our model on a given set of data.<br> In this dataset, we have 81 columns and 1460 rows.<br> SalePrice column is the **target\/dependant variable** whose value, the model should be able to predict <br> Remaining 80 columns are the **features\/independent variables** whose values will affect the value of the target. <br> This is a regression problem since the prediciton variable is a continuous variable (Price value of a house)\n","40d1f2f0":"Last feature with missing value is __Electrical__, with 1 missing value\nElectrical: Electrical system\n\n       SBrkr\tStandard Circuit Breakers & Romex\n       FuseA\tFuse Box over 60 AMP and all Romex wiring (Average)\t\n       FuseF\t60 AMP Fuse Box and mostly Romex wiring (Fair)\n       FuseP\t60 AMP Fuse Box and mostly knob & tube wiring (poor)\n       Mix\tMixed\n\t\t","25ee5225":"**GarageArea** will be an important feature in determining *SalePrice*","e7040c2c":"Some of the insights from the above plots:\n- **Utilities** column has only 1 category *AllPub* affecting the *SalePrice*, the entire column could be redundant\n- **Exterior1st & Exterior2nd** columns have very similar correlation with *SalePrice* for almost all categories, they may be duplicates\n- Mean value of *SalePrice* for all categories in **Functional** column is almost same\n","b6d4a17f":"Here is a complete reference to XGBoost Classification & Regression models (https:\/\/medium.com\/sfu-cspmp\/xgboost-a-deep-dive-into-boosting-f06c9c41349)","a5e1899c":"*GarageQual* represents the Garage quality but *GarageCond* represents the condition of Garage. Except for few datapoints, 2 **columns are very similar in the category count and their correlation with *SalePrice***, let's drop *GarageCond* ","fe2fdcb4":"## 6. Training set-Validation set split, Pre-processing steps: OneHotEncoding, Standardisation","5091e34d":"Both clearly explain different properties, let's observe the category counts","f1cc7728":"Then we read in the Train, Test and sample_submission datasets","f125d82e":"## 3.3. Feature Analysis based on Correlation","f9fd750e":"#### Skewness refers to the asymmetrical\/distorted distribution of data along the mean. A normal distribution resembles a bell curve with data equally distributed on either side of the mean value of the data.  Skewed data has most of the values either to the right or left of the mean. Log Transformation is one of the methods used to approximate skewed data to normal distrbution"}}