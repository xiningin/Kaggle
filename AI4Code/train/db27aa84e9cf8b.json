{"cell_type":{"4d8bed98":"code","1624ef61":"code","52c99bde":"code","fe51cb88":"code","12e3ea40":"code","3d28fbd8":"code","0c870944":"code","eb570831":"code","0053ffc3":"code","e2738874":"code","27d571b8":"code","b2eafe57":"code","d52db3af":"code","cb95936f":"code","abe0fe19":"code","eb1bd7b4":"code","cf60eb17":"code","ea6836f5":"code","d250cd93":"code","32fa1a5b":"code","ea0ecb88":"code","d9b655af":"code","15ae5853":"code","9eb7e941":"code","6aaa0e90":"markdown","10a586a2":"markdown","9aaf4933":"markdown","e79d820b":"markdown","e8eac72b":"markdown","91b69713":"markdown","07b95fb0":"markdown","155283aa":"markdown","9cc720a5":"markdown","44479418":"markdown","ddb17aa3":"markdown","d1188b7a":"markdown","3163a58f":"markdown","30d975d7":"markdown","fa922a82":"markdown","1067dac7":"markdown","1f2a4587":"markdown","b17b7b60":"markdown","749f8452":"markdown"},"source":{"4d8bed98":"import numpy as np\nimport pandas as pd\nimport torch\nimport torchvision\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport time\nimport copy\nimport shutil \nimport zipfile\nfrom torchvision import transforms, models\nfrom tqdm import tqdm","1624ef61":"class MyRemoveBackground:\n    \"\"\"Remove images background.\n    \"\"\"\n    \n    def __init__(self):\n        pass\n\n    def __call__(self, in_img):\n        \n        # Convert PIL image to numpy array\n        in_img = np.array(in_img)\n        \n        # Get the height and width from OpenCV image\n        height, width = in_img.shape[:2]\n        \n        # Create a mask holder\n        mask = np.zeros([height, width], np.uint8)\n\n        # Grab Cut the object\n        bgdModel = np.zeros((1, 65),np.float64)\n        fgdModel = np.zeros((1, 65),np.float64)\n\n        # Hard Coding the Rect The object must lie within this rect.\n        rect = (15, 15, width-30, height-30)\n        cv2.grabCut(in_img, mask, rect, bgdModel, fgdModel, 10, cv2.GC_INIT_WITH_RECT)\n        mask = np.where((mask==2)|(mask==0), 0, 1).astype('uint8')\n        out_img = in_img * mask[:, :, np.newaxis]\n\n        # Get the background\n        background = in_img - out_img\n\n        # Change all pixels in the background that are not black to white\n        background[np.where((background > [0, 0, 0]).all(axis = 2))] = [255, 255, 255]\n\n        #Add the background and the image\n        out_img = background + out_img\n\n        return transforms.functional.to_pil_image(out_img)\n    \n\nclass ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path","52c99bde":"def unzip_data(zip_file, destination_dir):\n    \"\"\"Extract pictures from zip file.\n    \"\"\"\n    print('Data extraction started...', end='')\n    with zipfile.ZipFile(zip_file, 'r') as zip_obj:\n        zip_obj.extractall(destination_dir)\n    print('done.')\n    print(f'Files unzipped to \\'{destination_dir}\\'\\n')\n\ndef remove_background(image_roots):\n    \"\"\"Remove picture background.\n       This function use MyRemoveBackground class.\n    \"\"\"\n    remove_photo_background = MyRemoveBackground()\n\n    print('Backgrounds removing started...')\n    for path in image_roots:\n        files = os.listdir(path)\n        files = list(filter(lambda x: x.endswith('.jpg'), files))\n        \n        print(f'{len(files)} pictures was found in {path}', end='')\n        for i, file in enumerate(files):\n            img_original = cv2.imread(path + file)\n            img_cleaned = remove_photo_background(img_original)\n            img_cleaned = np.array(img_cleaned)\n            cv2.imwrite(path + file, img_cleaned)\n            if i % 20 == 0:\n                print('\\n{:>3d}\/{:>3d}'.format(i, len(files)), end='')\n            print('.', end='')\n        print()\n    print('Backgrounds removing is complete.\\n')\n    \ndef make_extra_images(image_roots):\n    \"\"\"Function will make extra pictures with horizontal and vertical reflection.\n    \"\"\"\n\n    print('Extra pictures generation started...', end='')\n    prefix_names = ['_090', '_180', '_270']\n\n    for path in image_roots:\n        files = os.listdir(path)\n        files = list(filter(lambda x: x.endswith('.jpg') and '_' not in x, files))\n\n        for i, file in enumerate(files):\n            img = cv2.imread(path + file)\n            # Make extra pictures: flip each of originals photo to 90, 180 and 270 degrees\n            for i, angle in enumerate([cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_180, cv2.ROTATE_90_COUNTERCLOCKWISE]):\n                img = cv2.rotate(img, angle)\n                img_name = path + file[:file.find('.')] + prefix_names[i] + file[file.find('.'):]\n                if not os.path.exists(img_name):\n                    cv2.imwrite(img_name, img)\n    print('done.')\n\n    for path in image_roots:\n        files = os.listdir(path)\n        files = list(filter(lambda x: x.endswith('.jpg'), files))\n        print(f'{len(files)} pictures added to \\'{path}\\'')\n    print()\n    \n\ndef make_train_valid_data():\n    \"\"\"Split train pictures to train and valid groups.\n    \"\"\"\n    \n    print('Copy pictures to Train and Valid groups started...', end='')\n    for dir_name in [train_dir, valid_dir]:\n        for class_name in class_names:\n            os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n\n    for class_name in class_names:\n        src_dir = os.path.join(data_root, 'train', class_name)\n        files = os.listdir(src_dir)\n        files = list(filter(lambda x: x.endswith('.jpg'), files))\n        \n        for i, file_name in enumerate(files):\n            if i % 6 != 0:\n                dst_dir = os.path.join(train_dir, class_name) \n            else:\n                dst_dir = os.path.join(valid_dir, class_name)\n            shutil.copy(os.path.join(src_dir, file_name), os.path.join(dst_dir, file_name))\n    print('done.')\n\n    # Generate mini report\n    for dir_name in [train_dir, valid_dir]:\n        for class_name in class_names:\n            dst_dir = os.path.join(dir_name, class_name)\n            files = os.listdir(dst_dir)\n            files = list(filter(lambda x: x.endswith('.jpg'), files))\n            print(f'{len(files)} pictures copied to \\'{dst_dir}\\'')\n    print()\n    \n\ndef make_test_data():\n    \"\"\"Copy test pictures to test group.\n    \"\"\"  \n\n    print('Copy pictures to Test group started...', end='')\n    src_dir = os.path.join(data_root, 'test')\n    dst_dir = os.path.join(test_dir, 'unknown')\n    shutil.copytree(src_dir, dst_dir)\n    files = os.listdir(dst_dir)\n    files = list(filter(lambda x: x.endswith('.jpg'), files))\n    print('done.')\n    print(f'{len(files)} pictures copied to \\'{dst_dir}\\'\\n')","fe51cb88":"# Clear output directory\n!rm * --recursive         \n\n# Let's define some variables\nclass_names = ['cleaned', 'dirty']\ntrain_dir = 'train'\nvalid_dir = 'valid'\ntest_dir = 'test'\n\n# Extract images (Kaggle enviropment)\ndata_root = '\/kaggle\/working\/plates\/'\nunzip_data(zip_file='..\/input\/plates.zip', destination_dir='\/kaggle\/working\/')\n\n# Remove images background \nremove_background(image_roots=[os.path.join(data_root, train_dir, 'cleaned\/'),\n                               os.path.join(data_root, train_dir, 'dirty\/'),\n                               os.path.join(data_root, 'test\/')])\n\n# Create extra images for training models\nmake_extra_images(image_roots=[os.path.join(data_root, train_dir, 'cleaned\/'),\n                               os.path.join(data_root, train_dir, 'dirty\/')])\n\n# Copy images to train, valid and test dir\nmake_train_valid_data()\nmake_test_data()\n","12e3ea40":"# Transformations methods\nimage_transforms = { \n    'train': transforms.Compose([\n        transforms.RandomPerspective(distortion_scale=0.2, p=0.1, interpolation=3, fill=255),\n        transforms.RandomChoice([transforms.CenterCrop(180),\n                                 transforms.CenterCrop(160),\n                                 transforms.CenterCrop(140),\n                                 transforms.CenterCrop(120),\n                                 transforms.Compose([transforms.CenterCrop(280),\n                                                     transforms.Grayscale(3),\n                                                     ]),\n                                 transforms.Compose([transforms.CenterCrop(200),\n                                                     transforms.Grayscale(3),\n                                                     ]),\n                                 ]),\n        transforms.Resize((224, 224)),\n        transforms.ColorJitter(hue=(0.1, 0.2)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ]),\n    'valid': transforms.Compose([\n        transforms.RandomPerspective(distortion_scale=0.2, p=0.1, interpolation=3, fill=255),\n        transforms.RandomChoice([transforms.CenterCrop(180),\n                                 transforms.CenterCrop(160),\n                                 transforms.CenterCrop(140),\n                                 transforms.CenterCrop(120),\n                                 transforms.Compose([transforms.CenterCrop(280),\n                                                     transforms.Grayscale(3),\n                                                     ]),\n                                 transforms.Compose([transforms.CenterCrop(200),\n                                                     transforms.Grayscale(3),\n                                                     ]),\n                                 ]),\n        transforms.Resize((224, 224)),\n        transforms.ColorJitter(hue=(0.1, 0.2)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ]),}\n\n# Datasets\ndataset = {\n    'train': torchvision.datasets.ImageFolder(root=train_dir, transform=image_transforms['train']),\n    'valid': torchvision.datasets.ImageFolder(root=valid_dir, transform=image_transforms['valid']),\n    'test': ImageFolderWithPaths('\/kaggle\/working\/test', transform=None),\n}\n \n# Dataloaders\nbatch_size = 4\ntrain_dataloader = torch.utils.data.DataLoader(dataset['train'],\n                                               batch_size=batch_size,\n                                               shuffle=True,\n                                               num_workers=batch_size)\n\nvalid_dataloader = torch.utils.data.DataLoader(dataset['valid'],\n                                               batch_size=batch_size,\n                                               shuffle=True,\n                                               num_workers=batch_size)\n\ntest_dataloader  = torch.utils.data.DataLoader(dataset['test'],\n                                               batch_size=1,\n                                               shuffle=False,\n                                               num_workers=0)\n\n# Mini report of loaded data\nprint('Train, Valid and Test datasets are loaded:\\n')\nprint('{:<7s}{:>10s}{:>10s}'.format('Dataset', 'Batches', 'Pictures')), print('-' * 28)\nprint('{:<7s}{:>10d}{:>10d}'.format('Train', len(train_dataloader), len(dataset['train'])))\nprint('{:<7s}{:>10d}{:>10d}'.format('Valid', len(valid_dataloader), len(dataset['valid'])))\nprint('{:<7s}{:>10d}{:>10d}'.format('Test',  len(test_dataloader),  len(dataset['test'])))\n                            ","3d28fbd8":"%matplotlib inline\nplt.rcParams['figure.figsize'] = (14, 10)\n\nimage_iter = iter(train_dataloader)\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\nfor i in range(3):\n    X_batch, y_batch = next(image_iter)\n    for j in range(0, len(X_batch)):\n        plt.subplot(3, 4, i*batch_size + j + 1)\n        plt.title(class_names[y_batch[j].item()])\n        plt.imshow(X_batch[j].permute(1, 2, 0).numpy() * std + mean)\n  ","0c870944":"class MyResNet18(torch.nn.Module):\n    def __init__(self):\n        super(MyResNet18, self).__init__()\n        self.net = models.resnet18(pretrained=True)\n        \n        # Disable grad for all conv layers\n        for param in self.net.parameters():\n            param.requires_grad = False                \n        \n        # Create some additional layers for ResNet model\n        fc_inputs = self.net.fc.in_features\n        self.net.fc = torch.nn.Sequential(\n            torch.nn.Linear(fc_inputs, 256),\n            torch.nn.ReLU(),\n            torch.nn.Linear(256, 128),\n            torch.nn.Sigmoid(),\n            torch.nn.Linear(128, 2),\n            # nn.LogSoftmax(dim=1) # For using NLLLoss()\n        )  \n    def forward(self, x):\n        x = self.net(x)\n        return x\n\n    \nclass MyResNet50(torch.nn.Module):\n    def __init__(self):\n        super(MyResNet50, self).__init__()\n        self.net = models.resnet50(pretrained=True)\n        \n        # Disable grad for all conv layers\n        for param in self.net.parameters():\n            param.requires_grad = False                \n        \n        # Create some additional layers for ResNet model\n        fc_inputs = self.net.fc.in_features\n        self.net.fc = torch.nn.Sequential(\n            torch.nn.Linear(fc_inputs, 256),\n            torch.nn.ReLU(),\n            torch.nn.Linear(256, 128),\n            torch.nn.Sigmoid(),\n            # torch.nn.Dropout(0.2),\n            torch.nn.Linear(128, 2),\n            # torch.nn.LogSoftmax(dim=1) # For using NLLLoss()\n        )  \n    def forward(self, x):\n        x = self.net(x)\n        return x\n\n    \nclass MyResNet152(torch.nn.Module):\n    def __init__(self):\n        super(MyResNet152, self).__init__()\n        self.net = models.resnet152(pretrained=True)\n        \n        # Disable grad for all conv layers\n        for param in self.net.parameters():\n            param.requires_grad = False                \n        \n        # Create some additional layers for ResNet model\n        fc_inputs = self.net.fc.in_features\n        self.net.fc = torch.nn.Sequential(\n            torch.nn.Linear(fc_inputs, 256),\n            torch.nn.ReLU(),\n            torch.nn.Linear(256, 2),\n            # nn.LogSoftmax(dim=1) # For using NLLLoss()\n        )  \n    def forward(self, x):\n        x = self.net(x)\n        return x\n    \n\nclass MyMobilenet(torch.nn.Module):\n    def __init__(self):\n        super(MyMobilenet, self).__init__()\n        self.net = models.mobilenet_v2(pretrained=True)\n        \n        # Disable grad for all conv layers\n        for param in self.net.parameters():\n            param.requires_grad = False                \n        \n        # Create some additional layers for ResNet model\n        fc_inputs = self.net.classifier[1].in_features\n        self.net.classifier[1] = torch.nn.Sequential(\n            torch.nn.Linear(fc_inputs, 256),\n            torch.nn.ReLU(),\n            # torch.nn.Dropout(0.2),\n            torch.nn.Linear(256, 2),\n            # torch.nn.LogSoftmax(dim=1) # For using NLLLoss()\n        )  \n    def forward(self, x):\n        x = self.net(x)\n        return x    \n\n\nclass MyAlexNet(torch.nn.Module):\n    def __init__(self):\n        super(MyAlexNet, self).__init__()\n        self.net = models.alexnet(pretrained=True)\n        \n        # Disable grad for all conv layers\n        for param in self.net.parameters():\n            param.requires_grad = False                \n        \n        # Create some additional layers for ResNet model\n        fc_inputs = self.net.classifier[6].in_features\n        self.net.classifier[6] = torch.nn.Sequential(\n            torch.nn.Linear(fc_inputs, 2),\n            # torch.nn.ReLU(),\n            # torch.nn.Linear(256, 2),\n            # torch.nn.LogSoftmax(dim=1) # For using NLLLoss()\n        )  \n    def forward(self, x):\n        x = self.net(x)\n        return x  ","eb570831":"def train_model(model, loss, optimizer, scheduler, num_epochs):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    loss_hist = {'train':[], 'valid':[]}\n    accuracy_hist = {'train':[], 'valid':[]}\n    \n    print('{:<7s}|{:^20s}|{:^20s}|'.format('', 'Train', 'Valid'))\n    print('{:<7s}|{:>10s}{:>10s}|{:>10s}{:>10s}|'.format('Epoch', 'Loss', 'Acc', 'Loss', 'Acc'))\n    print('-' * 50)\n    for epoch in range(num_epochs):\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                dataloader = train_dataloader\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                dataloader = valid_dataloader\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.\n            running_acc = 0.\n\n            # Iterate over data.\n            for inputs, labels in dataloader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                # Forward and backward\n                with torch.set_grad_enabled(phase == 'train'):\n                    preds = model(inputs)\n                    loss_value = loss(preds, labels)\n                    preds_class = preds.argmax(dim=1)\n\n                    # Backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss_value.backward()\n                        optimizer.step()\n\n                # Statistics\n                running_loss += loss_value.item()\n                # Running_acc += (preds_class == labels.data).float().mean()\n                running_acc += (preds_class == labels.data).float().mean().data.cpu().numpy()                \n\n\n            epoch_loss = running_loss \/ len(dataloader)\n            epoch_acc = running_acc \/ len(dataloader)\n            if phase == 'train':\n                print('{:>3d}\/{:>3d}|{:>10.4f}{:>10.4f}|'.format(epoch + 1, num_epochs, epoch_loss, epoch_acc), end='')\n            else:\n                print('{:>10.4f}{:>10.4f}|'.format(epoch_loss, epoch_acc))\n\n            loss_hist[phase].append(epoch_loss)\n            accuracy_hist[phase].append(epoch_acc)\n\n    return model, loss_hist, accuracy_hist","0053ffc3":"def train_MyModel(model, epoch_num):\n    print('\\n' + model.__class__.__name__ + ' training with {} epochs started...\\n'.format(epoch_num))\n \n    loss = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), amsgrad=True, lr=1.0e-3)\n\n    # Decay LR by a factor of 0.1 every 7 epochs\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n    model, losses, accuracies = train_model(model, loss, optimizer, scheduler, num_epochs=epoch_num);\n    print('\\nModel training finished.')    \n    \n    return model, losses, accuracies\n","e2738874":"import random\nseed = 77\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\n# model = MyResNet18()\nmodel = MyResNet50()\n# model = MyResNet152()\n# model = MyMobilenet()\n# model = MyAlexNet()\n\n# Run model training\nmodel, losses, accuracies = train_MyModel(model, epoch_num=30)\n\n# Switch model to prediction mode\nmodel.eval()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","27d571b8":"# Model accuracies visualization\nplt.rcParams['figure.figsize'] = (14, 7)\nfor experiment_id in accuracies.keys():\n    plt.plot(accuracies[experiment_id], label=experiment_id)\nplt.legend(loc='upper left')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch num', fontsize=15)\nplt.ylabel('Accuracy value', fontsize=15);\nplt.grid(linestyle='--', linewidth=0.5, color='.7')","b2eafe57":"# Model loss-function visualization\nplt.rcParams['figure.figsize'] = (14, 7)\nfor experiment_id in losses.keys():\n    plt.plot(losses[experiment_id], label=experiment_id)\nplt.legend(loc='upper left')\nplt.title('Model Loss')\nplt.xlabel('Epoch num', fontsize=15)\nplt.ylabel('Loss function value', fontsize=15)\nplt.grid(linestyle='--', linewidth=0.5, color='.7')","d52db3af":"transform_image = {\n    'to_tensor_and_normalize': transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])\n}\n\n# List of transformation methods\ntransforms_list = { \n    'original': transforms.Compose([\n        transforms.Resize((224, 224)),\n    ]),   \n#     'crop_220': transforms.Compose([\n#         transforms.CenterCrop(220),\n#         transforms.Resize((224, 224)),\n#     ]), \n#     'crop_200': transforms.Compose([\n#         transforms.CenterCrop(200),\n#         transforms.Resize((224, 224)),\n#     ]),    \n    'crop_180': transforms.Compose([\n        transforms.CenterCrop(180),\n        transforms.Resize((224, 224)),\n    ]),    \n    'crop_160': transforms.Compose([\n        transforms.CenterCrop(160),\n        transforms.Resize((224, 224)),\n    ]),   \n    'crop_140': transforms.Compose([\n        transforms.CenterCrop(140),\n        transforms.Resize((224, 224)),\n    ]),   \n#     'crop_120': transforms.Compose([\n#         transforms.CenterCrop(120),\n#         transforms.Resize((224, 224)),\n#     ]),    \n    'gray_280': transforms.Compose([\n        transforms.Grayscale(3),\n        transforms.CenterCrop(280),\n        transforms.Resize((224, 224)),\n    ]),\n    'gray_200': transforms.Compose([\n        transforms.Grayscale(3),\n        transforms.CenterCrop(200),\n        transforms.Resize((224, 224)),\n    ]),\n    'r_crop_180_1': transforms.Compose([\n        transforms.RandomCrop(180),\n        transforms.Resize((224, 224)),\n    ]),\n    'r_crop_180_2': transforms.Compose([\n        transforms.RandomCrop(180),\n        transforms.Resize((224, 224)),\n    ]),\n    'r_crop_180_3': transforms.Compose([\n        transforms.Grayscale(3),\n        transforms.RandomCrop(180),\n        transforms.Resize((224, 224)),\n    ]),        \n}\n\n# Test Dataset\ndataset['test'] = ImageFolderWithPaths('\/kaggle\/working\/test', transform=None)\n\n# Test Dataloaders\ntest_dataloader  = torch.utils.data.DataLoader(dataset['test'],\n                                               batch_size=1,\n                                               shuffle=False,\n                                               num_workers=0)","cb95936f":"img_id = 50\n\nimg_original = test_dataloader.dataset[img_id][0]\n\nimg_id = test_dataloader.dataset[img_id][2]\nimg_id = img_id.replace('\/kaggle\/working\/test\/unknown\/', '')\nimg_id = img_id.replace('.jpg', '')\n\nlabels = {}\nlabels['id'] = img_id\n\n# Make prediction for each method in tranformation_list\nfor i, method in enumerate(transforms_list):\n    img_transformed = transforms_list[method](img_original)\n    tensor = transform_image['to_tensor_and_normalize'](img_transformed)\n    tensor = tensor.to(device)\n    tensor = tensor.unsqueeze(0)\n\n    with torch.set_grad_enabled(False):\n        preds = model(tensor)\n        \n    label = torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy()[0]\n    labels[method] = label\n\n# Vizualization \nplt.rcParams['figure.figsize'] = (15, 10)\nfor i, method in enumerate(transforms_list):\n    img_transformed = transforms_list[method](img_original)\n    plt.subplot(3, 5, i + 1)\n    plt.title(img_id +': ['+ method + ', ' + str(round(labels[method],4))+']')\n    plt.imshow(img_transformed); \n\n# Print prediction's results    \nprint('{:3s}{:15s}{:7s}'.format('N', 'Method', 'Percent')), print('-' * 25)\nfor i, method in enumerate([x for x in labels if x != 'id']):\n    print('{:<3d}{:<15s}{:>7.4f}'.format(i+1, method, labels[method]))\nprint('-' * 25)","abe0fe19":"random.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\ndata = []\nfor img_original, labels, img_id in tqdm(test_dataloader.dataset):\n    labels = {}\n    labels['id'] = img_id\n    probs = np.array([])\n\n    for i, method in enumerate(transforms_list):\n        img_transformed = transforms_list[method](img_original)\n        tensor = transform_image['to_tensor_and_normalize'](img_transformed)\n        tensor = tensor.to(device)\n        tensor = tensor.unsqueeze(0)\n\n        with torch.set_grad_enabled(False):\n            preds = model(tensor)\n\n        label = torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy()[0]\n        labels[method] = label\n\n    data.append(labels)","eb1bd7b4":"# Create DataFrame\ndf = pd.DataFrame(data)\n\ndf['id'] = df['id'].str.replace('\/kaggle\/working\/test\/unknown\/', '')\ndf['id'] = df['id'].str.replace('.jpg', '')\n\ndf.set_index('id', inplace=True)\ndf.head()","cf60eb17":"# Probability distribution visualization\nplt.rcParams['figure.figsize'] = (15, 10)\nfor i, col_name in enumerate(df.columns):\n    plt.subplot(3, 5, i + 1)\n    plt.title(col_name)\n    plt.hist(x=[df[col_name]], bins=100, histtype='bar');\n","ea6836f5":"# Data is skewed.So let's make some modifications to fix it\ndf['original'] = df['original']**(0.6)\n# df['crop_220'] = df['crop_220']**(2.0)\n# df['crop_200'] = df['crop_200']**(2.0)\ndf['crop_180'] = df['crop_180']**(2.5)\ndf['crop_160'] = df['crop_160']**(2.5)\ndf['crop_140'] = df['crop_140']**(2.5)\n# df['crop_120'] = df['crop_120']**(2.0)\ndf['gray_280'] = df['gray_280']**(2.0)\ndf['gray_200'] = df['gray_200']**(2.5)\ndf['r_crop_180_1'] = df['r_crop_180_1']**(2.0)\ndf['r_crop_180_2'] = df['r_crop_180_2']**(2.0)\ndf['r_crop_180_3'] = df['r_crop_180_3']**(2.0)","d250cd93":"# And a middle of each prediction method to zero\nfor col_name in df.columns:\n    gap = 0.05\n    plates_min = 999\n\n    # Serch a middle between 0.4 and 0.7\n    for i in range(40, 70):\n        plates_num = df[(df[col_name] > i\/100) & (df[col_name] < i\/100 + gap)][col_name].count()\n        if plates_min > plates_num:\n            plates_min = plates_num\n            middle = i\/100\n\n    # Shift a middle to zero\n    df[col_name] = df[col_name] - middle + gap\/2","32fa1a5b":"# Let's see what we've got\nplt.rcParams['figure.figsize'] = (15, 10)\nfor i, col_name in enumerate(df.columns):\n    plt.subplot(3, 5, i + 1)\n    plt.title(col_name)\n    plt.hist(x=[df[col_name]], bins=100, histtype='bar');","ea0ecb88":"# Prepare submission file\ndf['mean'] = df.mean(axis=1)\ndf['label'] = df['mean'].map(lambda x: 'cleaned' if x < 0 else 'dirty')\ndf.head()","d9b655af":"# Remove extra columns\ndf.drop(df.columns[:-1], axis='columns', inplace=True)\ndf.head()","15ae5853":"# Export predictions to *.csv file\n# file_name = 'submission_' + model.__class__.__name__ + '.csv'\nfile_name = 'submission.csv'\ndf.to_csv(file_name)","9eb7e941":"!rm -rf train valid test","6aaa0e90":"# **Submission**","10a586a2":"## Ok. Let's start :)","9aaf4933":"## Run traning\nOk! We are ready for Model Training!\nChoose one of them and start","e79d820b":"## Prepare some Classes and Functions for pictures transformations","e8eac72b":"# **Prediction**","91b69713":"# **Trainig**","07b95fb0":"## Create a Test transformation methods\nNow we are ready for predictions\n","155283aa":"## Model Training Algorithm","9cc720a5":"## Load some images from batches\nIt's just for check what we have in datasets","44479418":"## Data preparation","ddb17aa3":"## Export submission file","d1188b7a":"# **Create models**","3163a58f":"# **Classes and Functions** ","30d975d7":"## Load data into Datasets\n* Create Train, Valid transformation methods (*Test transformations methods will be defined later*)\n* Create Train, Valid and Test datasets\n* Create Train, Valid and Test dataloaders","fa922a82":"## Several models for training that can be used","1067dac7":"## Let's predict the status for one chosen plate","1f2a4587":"# **Unzip data**","b17b7b60":"## Make predictions for all plates","749f8452":"## Model traning function"}}