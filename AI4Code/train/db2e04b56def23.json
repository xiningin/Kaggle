{"cell_type":{"f6b51c00":"code","eb557677":"code","49bbfea3":"code","21a92d3c":"code","772acc2c":"code","745c44b9":"code","51f262c0":"code","b6eb8b1e":"code","19157575":"code","1ac5bed0":"code","2235b102":"code","89b4a61b":"code","e3963bee":"code","cf8bbe5e":"code","c59956ed":"code","54622c98":"code","364a1970":"code","eadeee7e":"code","34c345dd":"code","38c08120":"code","e1d8f0fa":"code","781c3d0b":"code","7ae2f125":"code","24636f79":"code","2e2cfad4":"code","5b3f8dfe":"code","de0a864f":"code","fe71b08d":"code","80a2c301":"code","483d1e42":"code","fdee58bb":"code","07767df4":"code","f02db316":"code","742b1b7f":"code","223c0ef5":"code","a094853c":"code","8312941f":"code","afcc2795":"code","5814251b":"code","7660b702":"code","4130369f":"code","b0a1f8ac":"code","976831bc":"code","595643b0":"code","0c60242a":"code","96fa5c93":"code","d03fcba5":"code","e0785509":"markdown","41320367":"markdown","8de14c1e":"markdown","2c09a579":"markdown","4ed5993f":"markdown","0731ddf1":"markdown","68523f57":"markdown","e69ecbb5":"markdown"},"source":{"f6b51c00":"import numpy as np, pandas as pd\npd.options.display.max_columns = 100\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","eb557677":"def plot_history(history, epochs):\n    sns.set()\n    fig, ax = plt.subplots(1,1, figsize=(12, 4))\n    ax.plot(range(epochs), history.history['loss'], label=\"train\")\n    ax.plot(range(epochs), history.history['val_loss'], label=\"val\")\n    ax.legend()\n    ax.set_title(\"RMSE\")\n    #ax[0].axis((30, 200, 90, 95))","49bbfea3":"base_dir = \"..\/input\/beyond-analysis\/\"","21a92d3c":"# Load train and test sets\ntrain = pd.read_csv(base_dir + \"train.csv\")\ntest = pd.read_csv(base_dir + \"test.csv\")","772acc2c":"def unique_len(series):\n    return len(series.unique())\n\ndef deviation(series, mean):\n    return np.square(series.mean()-mean)","745c44b9":"def calc_customer_stats(df):\n    entry_mean = df['ENTRY'].mean()\n    revenue_mean = df['REVENUE'].mean()\n    winnings_1 = df['WINNINGS_1'].mean()\n    discount_mean = df['DISCOUNT'].mean()\n    deposit_mean = df['DEPOSIT'].mean()\n    deposit_numer_mean = df['DEPOSIT_NUMBER'].mean()\n    deposit_2_mean = df['DEPOSIT_2'].mean()\n    deposit_trials_mean = df['DEPOSIT_TRAILS'].mean()\n    entry_number_mean = df['ENTRY_NUMBER'].mean()\n    winning_number_mean = df['WINNINGS_NUMBER'].mean()\n    agg_dict = {\n        \"ENTRY\": [\"mean\",\"sum\", np.std, \"max\", \"min\", unique_len, lambda x: deviation(x, entry_mean)],\n        \"REVENUE\": [\"mean\",\"sum\", np.std, \"max\", \"min\", unique_len, lambda x: deviation(x, revenue_mean)],\n        \"WINNINGS_1\": [\"mean\",\"sum\", np.std, \"max\", \"min\", unique_len, lambda x: deviation(x, winnings_1)],\n        \"WINNINGS_2\": [\"mean\",\"sum\", np.std],\n        \"DISCOUNT\": [\"mean\",\"sum\", np.std, \"max\", \"min\", unique_len, lambda x: deviation(x, discount_mean)],\n        \"DEPOSIT\": [\"mean\",\"sum\", np.std, \"max\", \"min\", unique_len, lambda x: deviation(x, deposit_mean)],\n        \"DEPOSIT_NUMBER\": [\"mean\",\"sum\", np.std, \"max\", \"min\", unique_len, lambda x: deviation(x, deposit_numer_mean)],\n        \"DEPOSIT_2\": [\"mean\",\"sum\", np.std, \"max\", \"min\", unique_len, lambda x: deviation(x, deposit_2_mean)],\n        \"WITHDRAW\": [\"mean\",\"sum\", np.std],\n        \"WITHDRAW_NUMBER\": [\"mean\",\"sum\", np.std],\n        \"DEPOSIT_TRAILS\": [\"mean\",\"sum\", np.std, \"max\", \"min\", unique_len, lambda x: deviation(x, deposit_trials_mean)],\n        \"ENTRY_NUMBER\": [\"mean\",\"sum\", np.std, \"max\", \"min\", unique_len, lambda x: deviation(x, entry_number_mean)],\n        \"WINNINGS_NUMBER\": [\"mean\",\"sum\", np.std, \"max\", \"min\", unique_len, lambda x: deviation(x, winning_number_mean)],\n        \"PRACTICE_ENTRY\": [\"mean\",\"sum\", unique_len],\n        \"PRACTICE_WINNINGS\": [\"mean\",\"sum\", unique_len],\n        \"PRACTICE_ENTRY_NUMBER\": [\"mean\",\"sum\", unique_len],\n        \"PRACTICE_WINNINGS_NUMBER\": [\"mean\",\"sum\", unique_len]\n    }\n    \n    df_grp = df.groupby(\"UNIQUE_IDENTIFIER\").agg(agg_dict)\n    df_grp.columns = [\"_\".join(col) for col in df_grp.columns]\n    df_grp = df_grp.reset_index()\n    return df_grp","51f262c0":"# preprocess train data\ntrain_ = calc_customer_stats(train)\ntemp = train.groupby(\"UNIQUE_IDENTIFIER\")[[\"Y1\",\"Y2\"]].agg(lambda x: x.unique()[0]).reset_index()\ntrain_ = train_.merge(temp, on=\"UNIQUE_IDENTIFIER\", how=\"left\")","b6eb8b1e":"# Preprocess test data\ntest_ = calc_customer_stats(test)","19157575":"# Fill NaN values which are zero std\ntrain_ = train_.fillna(value=0)\ntest_ = test_.fillna(value=0)","1ac5bed0":"train_","2235b102":"print(train_.shape, test_.shape)","89b4a61b":"X = train_.drop(['UNIQUE_IDENTIFIER', 'Y1', 'Y2'], axis=1)\ny = train_[[\"Y1\", \"Y2\"]]\nX_test = test_.drop([\"UNIQUE_IDENTIFIER\"], axis=1)","e3963bee":"# Update to the latest version\n!pip install tensorflow","cf8bbe5e":"!pip install -q -U keras-tuner","c59956ed":"import keras_tuner as kt","54622c98":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras import backend as K\n\ndef model_builder(hp):\n  model = keras.Sequential()\n  model.add(keras.layers.Dense(16, input_shape=(X.shape[1], ), activation='relu'))\n    \n  # Tune the number of units in the first Dense layer\n  # Choose an optimal value between 32-512\n  hp_units = hp.Int('units1', min_value=32, max_value=512, step=32)\n  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n    \n  # Tune the dropout values\n  # Choose an optimal value between 0.1-0.5\n  hp_units = hp.Float('dropout1', min_value=0.1, max_value=0.5, step=0.01)\n  model.add(keras.layers.Dropout(hp_units))\n\n  hp_units = hp.Int('units2', min_value=32, max_value=512, step=32)\n  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n\n  hp_units = hp.Float('dropout2', min_value=0.1, max_value=0.5, step=0.01)\n  model.add(keras.layers.Dropout(hp_units))\n\n  model.add(keras.layers.Dense(2))\n\n  # Tune the learning rate for the optimizer\n  # Choose an optimal value from 0.01, 0.001, or 0.0001\n  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n    \n  # Custom loss function\n  def root_mean_squared_error(y_true, y_pred):\n      return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\n  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=root_mean_squared_error)\n\n  return model","364a1970":"tuner = kt.Hyperband(model_builder,\n                     objective=kt.Objective(\"val_loss\", direction=\"min\"),\n                     max_epochs=20,\n                     factor=3,\n                     directory='.\/',\n                     project_name='kt_tuner')","eadeee7e":"stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)","34c345dd":"tuner.search(X, y, epochs=100, validation_split=0.2, callbacks=[stop_early])","38c08120":"# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n# Build the model with the optimal hyperparameters and train it on the data for 100 epochs\nmodel = tuner.hypermodel.build(best_hps)\nhistory = model.fit(X, y, epochs=100, validation_split=0.2, verbose=1)","e1d8f0fa":"plot_history(history, 100)","781c3d0b":"val_acc_per_epoch = history.history['val_loss']\nbest_epoch = val_acc_per_epoch.index(min(val_acc_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))","7ae2f125":"hypermodel = tuner.hypermodel.build(best_hps)\n\n# Retrain the model\nhypermodel.fit(X, y, epochs=best_epoch, validation_split=0.2)","24636f79":"y_pred = hypermodel.predict(X_test)","2e2cfad4":"test_[\"Y1_NN\"] = y_pred[:,0]\ntest_[\"Y2_NN\"] = y_pred[:,1]\ntest_[[\"UNIQUE_IDENTIFIER\",\"Y1_NN\",\"Y2_NN\"]].to_csv(\"submission_NN.csv\", index=False)","5b3f8dfe":"import lightgbm as lgb\nfrom lightgbm import plot_importance\nfrom sklearn.model_selection import KFold\nimport warnings\nwarnings.filterwarnings('ignore')\nseed = 21\nnp.random.seed(seed)","de0a864f":"# preprocess train data\ntrain_ = calc_customer_stats(train)\ntemp = train.groupby(\"UNIQUE_IDENTIFIER\")[[\"STATUS_CHECK\", \"CATEGORY_1\",\"CATEGORY_2\",\"Y1\",\"Y2\"]].agg(lambda x: x.unique()[0]).reset_index()\ntrain_ = train_.merge(temp, on=\"UNIQUE_IDENTIFIER\", how=\"left\")","fe71b08d":"# Preprocess test data\ntest_ = calc_customer_stats(test)\ntemp = test.groupby(\"UNIQUE_IDENTIFIER\")[[\"STATUS_CHECK\", \"CATEGORY_1\", \"CATEGORY_2\"]].agg(lambda x: x.unique()[0]).reset_index()\ntest_ = test_.merge(temp, on=\"UNIQUE_IDENTIFIER\", how=\"left\")","80a2c301":"# Fill NaN values which are zero std\ntrain_ = train_.fillna(value=0)\ntest_ = test_.fillna(value=0)","483d1e42":"X = train_.drop(['UNIQUE_IDENTIFIER', 'Y1', 'Y2'], axis=1)\ny = train_[[\"Y1\", \"Y2\"]]\nX_test = test_.drop([\"UNIQUE_IDENTIFIER\"], axis=1)","fdee58bb":"print(X.shape, X_test.shape)","07767df4":"from sklearn.preprocessing import LabelEncoder\nle1 = LabelEncoder()\nle2 = LabelEncoder()\nX_test['CATEGORY_1'] = le1.fit_transform(X_test['CATEGORY_1'])\nX_test['CATEGORY_2'] = le2.fit_transform(X_test['CATEGORY_2'])\nX['CATEGORY_1'] = le1.transform(X['CATEGORY_1'])\nX['CATEGORY_2'] = le2.transform(X['CATEGORY_2'])","f02db316":"FOLDS = 5\nlgb_params1 = {'bagging_fraction': 0.7446132365786178,\n               'bagging_freq': int(94.57044996418587),\n               'feature_fraction': 0.9012097906532421,\n               'feature_fraction_bynode': 0.2724293678054648,\n               'lambda_l1': 6.697079462213692,\n               'lambda_l2': 0.05709330859845584,\n               'learning_rate': 0.17588487226531224,\n               'min_data_in_leaf': int(154.15270440199714),\n               'min_sum_hessian_in_leaf': 58.37985742493366,\n               'num_leaves': int(622.1019844986934),\n        'seed': seed,\n        'feature_fraction_seed': seed,\n        'bagging_seed': seed,\n        'drop_seed': seed,\n        'data_random_seed': seed,\n        'objective': 'rmse',\n        'boosting': 'gbdt',\n        'verbosity': -1,\n        'n_jobs': -1\n}\n\nlgb_params2 = {'bagging_fraction': 0.8251097530764318,\n               'bagging_freq': int(30.596065567048885),\n               'feature_fraction': 0.49998711057091194,\n               'feature_fraction_bynode': 0.8507173745725533,\n               'lambda_l1': 7.555927148134436,\n               'lambda_l2': 7.988171568347725,\n               'learning_rate': 0.054902068760180736,\n               'min_data_in_leaf': int(797.5794978618343),\n               'min_sum_hessian_in_leaf': 18.06814076686615,\n               'num_leaves': int(529.4188648232011),\n        'seed': seed,\n        'feature_fraction_seed': seed,\n        'bagging_seed': seed,\n        'drop_seed': seed,\n        'data_random_seed': seed,\n        'objective': 'rmse',\n        'boosting': 'gbdt',\n        'verbosity': -1,\n        'n_jobs': -1}","742b1b7f":"# Prediction arrays\nval_pred1 = np.zeros(X.shape[0])\ntest_pred1 = np.zeros(X_test.shape[0])\nval_pred2 = np.zeros(X.shape[0])\ntest_pred2 = np.zeros(X_test.shape[0])\n\n# K-fold cross valiation\nkfold = KFold(n_splits=FOLDS)\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n    print(\"#\"*40)\n    print(\"Training Fold - \", fold+1)\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_val, y_val     = X.iloc[val_idx]  , y.iloc[val_idx]\n\n    # create lgb datasets\n    train_data1 = lgb.Dataset(X_train, y_train.iloc[:,0], categorical_feature = [\"STATUS_CHECK\", \"CATEGORY_1\", \"CATEGORY_2\"])\n    val_data1   = lgb.Dataset(X_val,   y_val.iloc[:,0],   categorical_feature = [\"STATUS_CHECK\", \"CATEGORY_1\", \"CATEGORY_2\"])\n    train_data2 = lgb.Dataset(X_train, y_train.iloc[:,1], categorical_feature = [\"STATUS_CHECK\", \"CATEGORY_1\", \"CATEGORY_2\"])\n    val_data2   = lgb.Dataset(X_val,   y_val.iloc[:,1],   categorical_feature = [\"STATUS_CHECK\", \"CATEGORY_1\", \"CATEGORY_2\"])\n\n    # Train model\n    print(\"#\"*20)\n    print(\"Training for Y1\")\n    model1 = lgb.train(params = lgb_params1,\n                     train_set = train_data1,\n                     valid_sets = [train_data1, val_data1],\n                     num_boost_round = 10000,\n                     valid_names=[\"training\", \"validation\"],\n                     early_stopping_rounds = 1000,\n                     # callbacks=[neptune_callback],\n                     verbose_eval = 200)\n    print(\"#\"*20)\n    print(\"Training for Y2\")\n    model2 = lgb.train(params = lgb_params2,\n                     train_set = train_data2,\n                     valid_sets = [train_data2, val_data2],\n                     num_boost_round = 10000,\n                     valid_names=[\"training\", \"validation\"],\n                     early_stopping_rounds = 1000,\n                     # callbacks=[neptune_callback],\n                     verbose_eval = 200)\n\n    # Plot feature importance\n    plot_importance(model1, max_num_features=25, figsize= (10, 6), title=\"For Y1\")\n    plot_importance(model2, max_num_features=25, figsize= (10, 6), title=\"For Y2\")\n    plt.show()\n    \n    # Predict validation set\n    val_pred1[val_idx] = model1.predict(X_val)\n    val_pred2[val_idx] = model2.predict(X_val)\n    \n    # Predict test set\n    test_pred1 += model1.predict(X_test) \/ FOLDS\n    test_pred2 += model2.predict(X_test) \/ FOLDS","223c0ef5":"from sklearn.metrics import mean_squared_error as mse\nval_rmse = np.mean([np.sqrt(mse(train_[\"Y1\"], val_pred1)), np.sqrt(mse(train_[\"Y2\"], val_pred2))])\nprint(\"OOF score:\", val_rmse)","a094853c":"train_pred_LGB = pd.DataFrame({\"Y1_LGB\":val_pred1, \"Y2_LGB\":val_pred2})","8312941f":"test_[\"Y1_LGB\"] = test_pred1\ntest_[\"Y2_LGB\"] = test_pred2\ntest_[[\"UNIQUE_IDENTIFIER\", \"Y1_LGB\", \"Y2_LGB\"]].to_csv(\"submission_LGB.csv\", index=False)","afcc2795":"FOLDS = 5","5814251b":"print(X.shape, X_test.shape)","7660b702":"from catboost import Pool, CatBoostRegressor","4130369f":"# Prediction arrays\nval_pred1 = np.zeros(X.shape[0])\ntest_pred1 = np.zeros(X_test.shape[0])\nval_pred2 = np.zeros(X.shape[0])\ntest_pred2 = np.zeros(X_test.shape[0])\n\n# K-fold cross valiation\nkfold = KFold(n_splits=FOLDS)\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n    print(\"#\"*60)\n    print(\"Training Fold - \", fold+1)\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_val, y_val     = X.iloc[val_idx]  , y.iloc[val_idx]\n\n    # create lgb datasets\n    train_data1 = Pool(X_train, y_train.iloc[:,0], cat_features = [\"STATUS_CHECK\", \"CATEGORY_1\", \"CATEGORY_2\"])\n    val_data1   = Pool(X_val,   y_val.iloc[:,0],   cat_features = [\"STATUS_CHECK\", \"CATEGORY_1\", \"CATEGORY_2\"])\n    train_data2 = Pool(X_train, y_train.iloc[:,1], cat_features = [\"STATUS_CHECK\", \"CATEGORY_1\", \"CATEGORY_2\"])\n    val_data2   = Pool(X_val,   y_val.iloc[:,1],   cat_features = [\"STATUS_CHECK\", \"CATEGORY_1\", \"CATEGORY_2\"])\n\n    # Train model\n    print(\"#\"*20)\n    print(\"Training for Y1\")\n    model1 = CatBoostRegressor(iterations=2000, \n                              depth=3, \n                              learning_rate=0.1, \n                              loss_function='RMSE',\n                              eval_metric=\"RMSE\",\n                              use_best_model = True,\n                              early_stopping_rounds = 1000)\n    model1.fit(train_data1, eval_set=val_data1,verbose_eval=200)\n    print(\"#\"*20)\n    print(\"Training for Y2\")\n    model2 = CatBoostRegressor(iterations=7000, \n                              depth=4, \n                              learning_rate=0.05, \n                              loss_function='RMSE',\n                              eval_metric=\"RMSE\",\n                              use_best_model = True,\n                              early_stopping_rounds = 1000)\n    model2.fit(train_data2, eval_set=val_data2,verbose_eval=200)\n\n    \n    # Predict validation set\n    val_pred1[val_idx] = model1.predict(X_val)\n    val_pred2[val_idx] = model2.predict(X_val)\n    \n    # Predict test set\n    test_pred1 += model1.predict(X_test) \/ FOLDS\n    test_pred2 += model2.predict(X_test) \/ FOLDS","b0a1f8ac":"from sklearn.metrics import mean_squared_error as mse\nval_rmse = np.mean([np.sqrt(mse(train_[\"Y1\"], val_pred1)), np.sqrt(mse(train_[\"Y2\"], val_pred2))])\nprint(\"OOF score:\", val_rmse)","976831bc":"train_pred_CAT = pd.DataFrame({\"Y1_CAT\":val_pred1, \"Y2_CAT\":val_pred2})","595643b0":"test_[\"Y1_CAT\"] = test_pred1\ntest_[\"Y2_CAT\"] = test_pred2\ntest_[[\"UNIQUE_IDENTIFIER\", \"Y1_CAT\", \"Y2_CAT\"]].to_csv(\"submission_CAT.csv\", index=False)","0c60242a":"# All three models\nsub_NN = pd.read_csv(\".\/submission_NN.csv\")\nsub_LGB = pd.read_csv(\".\/submission_LGB.csv\")\nsub_CAT = pd.read_csv(\".\/submission_CAT.csv\")\ntest_[\"Y1\"] = (sub_NN.iloc[:,1] + sub_LGB.iloc[:,1] + sub_CAT.iloc[:,1])\/3\ntest_[\"Y2\"] = (sub_NN.iloc[:,2] + sub_LGB.iloc[:,2]+ sub_CAT.iloc[:,2])\/3\ntest_[[\"UNIQUE_IDENTIFIER\", \"Y1\", \"Y2\"]].to_csv(\"submission.csv\", index=False)","96fa5c93":"# Only boosting models\nsub_LGB = pd.read_csv(\".\/submission_LGB.csv\")\nsub_CAT = pd.read_csv(\".\/submission_CAT.csv\")\ntest_[\"Y1\"] = (sub_LGB.iloc[:,1] + sub_CAT.iloc[:,1])\/2\ntest_[\"Y2\"] = (sub_LGB.iloc[:,2]+ sub_CAT.iloc[:,2])\/2\ntest_[[\"UNIQUE_IDENTIFIER\", \"Y1\", \"Y2\"]].to_csv(\"submission_boost.csv\", index=False)","d03fcba5":"# from sklearn.ensemble import RandomForestRegressor\n\n# X = pd.concat([train_pred_CAT, train_pred_LGB], axis=1)\n# X_test = test_[[\"Y1_CAT\", \"Y1_LGB\", \"Y2_CAT\", \"Y2_LGB\"]]\n\n# print(X.shape, X_test.shape)\n\n# # Prediction arrays\n# val_pred1 = np.zeros(X.shape[0])\n# test_pred1 = np.zeros(X_test.shape[0])\n# val_pred2 = np.zeros(X.shape[0])\n# test_pred2 = np.zeros(X_test.shape[0])\n\n# # K-fold cross valiation\n# kfold = KFold(n_splits=FOLDS)\n# for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n#     print(\"#\"*60)\n#     print(\"Training Fold - \", fold+1)\n#     X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n#     X_val, y_val     = X.iloc[val_idx]  , y.iloc[val_idx]\n\n#     # create training datasets\n#     y_train1 = y_train.iloc[:,0]\n#     y_val1   = y_val.iloc[:,0]\n#     y_train2 = y_train.iloc[:,1]\n#     y_val2   = y_val.iloc[:,1]\n\n#     # Train model\n#     print(\"#\"*20)\n#     print(\"Training for Y1\")\n#     rfr1 = RandomForestRegressor(n_estimators = 1000,\n#                             max_depth = 3,\n#                            oob_score = True,\n#                            n_jobs = -1,\n#                            random_state = seed,\n#                            max_samples = 0.6)\n#     rfr1.fit(X_train, y_train1)\n#     print(\"#\"*20)\n#     print(\"Training for Y2\")\n#     rfr2 = RandomForestRegressor(n_estimators = 2000,\n#                             max_depth = 4,\n#                            oob_score = True,\n#                            n_jobs = -1,\n#                            random_state = seed,\n#                            max_samples = 0.6)\n#     rfr2.fit(X_train, y_train2)\n\n#     # Predict validation set\n#     val_pred1[val_idx] = rfr1.predict(X_val)\n#     val_pred2[val_idx] = rfr2.predict(X_val)\n    \n#     # Predict test set\n#     test_pred1 += rfr1.predict(X_test) \/ FOLDS\n#     test_pred2 += rfr2.predict(X_test) \/ FOLDS\n\n# from sklearn.metrics import mean_squared_error as mse\n# val_rmse = np.mean([np.sqrt(mse(train_[\"Y1\"], val_pred1)), np.sqrt(mse(train_[\"Y2\"], val_pred2))])\n# print(\"OOF score:\", val_rmse)\n\n# test_[\"Y1_RFR\"] = test_pred1\n# test_[\"Y2_RFR\"] = test_pred2\n# test_[[\"UNIQUE_IDENTIFIER\", \"Y1_RFR\", \"Y2_RFR\"]].to_csv(\"submission_RFR.csv\", index=False)","e0785509":"## LightGBM","41320367":"## DNN","8de14c1e":"If using catboost then go down from here...","2c09a579":"# Ensemble","4ed5993f":"# Model","0731ddf1":"If using boosting then go down from here to LightGBM...","68523f57":"## Random Forest\nA Failed attempt!","e69ecbb5":"## CatBoost"}}