{"cell_type":{"e6cb6f2d":"code","63372109":"code","3a13f331":"code","2cf574f8":"code","4694dbb2":"code","6bed1ceb":"code","c4b82d42":"code","2b46e3ae":"code","caa513bd":"code","bc229df9":"code","102361e7":"code","3b8902da":"code","c512f797":"code","565b6b54":"code","9d2bd3af":"code","da4fa742":"code","51dafbab":"code","1a137822":"code","4fe1cc76":"code","711235ae":"markdown"},"source":{"e6cb6f2d":"import os\nimport time\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import BatchSampler, SequentialSampler\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR","63372109":"def seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","3a13f331":"train = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/train.json', lines=True)\ntest = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/test.json', lines=True)\nsample_df = pd.read_csv('\/kaggle\/input\/stanford-covid-vaccine\/sample_submission.csv')","2cf574f8":"cols = ['sequence', 'structure', 'predicted_loop_type']\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\ntoken2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\ndef preprocess_inputs(data):\n    '''\n    Credits goes to @xhlulu: https:\/\/www.kaggle.com\/xhlulu\/openvaccine-simple-gru-model\n    '''\n    return np.transpose(\n        np.array(\n            data[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )","4694dbb2":"class OpenVaccineDataset(Dataset):\n    def __init__(self, data, labels):\n        super(OpenVaccineDataset, self).__init__()\n        self.data = data\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return {\n            'x': torch.tensor(self.data[idx]),\n            'y': torch.tensor(self.labels[idx])\n        }","6bed1ceb":"max_features = None\nmax_features = max_features or len(token2int)\npred_len = 68\nEMBEDDING_DIM = 100\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nNUM_TARGETS = 5\nLR = 1e-3\nBATCH_SIZE = 32\nEPOCHS = 150","c4b82d42":"class SpatialDropout(nn.Dropout2d):\n    \n    def forward(self, x):\n        x = x.permute(0, 3, 2, 1)  \n        x = super(SpatialDropout, self).forward(x)  \n        x = x.permute(0, 3, 2, 1)  \n        return x\n    \nclass NeuralNet(nn.Module):\n    \n    def __init__(self, embed_size, num_targets):\n        super(NeuralNet, self).__init__()\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size * 3, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n        \n        self.linear_out = nn.Linear(LSTM_UNITS * 2, num_targets)\n\n    def forward(self, x):\n        h_embedding = self.embedding(x)\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_reshaped = torch.reshape(\n            h_embedding, \n            shape=(-1, h_embedding.shape[1],  h_embedding.shape[2] * h_embedding.shape[3])\n        )\n        \n        h_lstm1, _ = self.lstm1(h_reshaped)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        h_truncated = h_lstm2[:, :pred_len]\n        \n        return self.linear_out(h_truncated)","2b46e3ae":"def loss_fn(outputs, targets):\n    colwise_mse = torch.mean(torch.square(targets - outputs), dim=(0, 1))\n    loss = torch.mean(torch.sqrt(colwise_mse), dim=-1)\n    return loss","caa513bd":"def get_model_optimizer(model):\n    # Differential Learning Rate\n    def is_linear(name):\n        return \"linear\" in name\n    \n    optimizer_grouped_parameters = [\n       {'params': [param for name, param in model.named_parameters() if not is_linear(name)], 'lr': LR},\n       {'params': [param for name, param in model.named_parameters() if is_linear(name)], 'lr': LR*3} \n    ]\n    \n    optimizer = AdamW(\n        optimizer_grouped_parameters, lr=LR, weight_decay=1e-2\n    )\n    \n    return optimizer","bc229df9":"class AverageMeter(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches \/\/ 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '\/' + fmt.format(num_batches) + ']'\n\ndef accuracy(output, target, topk=(1,)):\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 \/ batch_size))\n        return res","102361e7":"def train_loop_fn(train_loader, model, optimizer, device, scheduler, epoch=None):\n    # Train\n    batch_time = AverageMeter('Time', ':6.3f')\n    losses = AverageMeter('Loss', ':2.4f')\n    progress = ProgressMeter(\n        len(train_loader),\n        [batch_time, losses],\n        prefix=\"[TRAIN] Epoch: [{}]\".format(epoch)\n    )\n    model.train()\n    end = time.time()\n    for i, data in enumerate(train_loader):\n        inputs = data['x']\n        targets = data['y']\n        inputs = inputs.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        losses.update(loss.item(), BATCH_SIZE)\n        scheduler.step()\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if i % 37 == 0 and i !=0:\n            progress.display(i)","3b8902da":"def _run():\n    print('Starting Training ... ')\n    \n    print('  Loading Data ... ')\n    train_data = preprocess_inputs(data=train)\n    train_labels = np.array(train[pred_cols].values.tolist()).transpose((0, 2, 1))\n    train_dataset = OpenVaccineDataset(\n        train_data,\n        train_labels\n    )\n    train_data_loader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        drop_last=False,\n        pin_memory=True,\n        num_workers=4\n    )\n    print('  Data Loading Completed ... ')\n    \n    print('  Loading Model Configurations ... ')\n    num_train_steps = int(len(train_dataset)) \/ BATCH_SIZE\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model = NeuralNet(\n        embed_size=EMBEDDING_DIM, \n        num_targets=NUM_TARGETS\n    )\n    model = model.to(device)\n    optimizer = get_model_optimizer(model)\n    scheduler = CosineAnnealingLR(optimizer, T_max=num_train_steps*EPOCHS)\n    print('  Model Configuration Completed ... ')\n    \n    print('Training Started ... ')\n    for epoch in range(EPOCHS):\n        train_loop_fn(\n            train_data_loader,\n            model,\n            optimizer,\n            device,\n            scheduler,\n            epoch\n        )\n        if epoch == EPOCHS-1:\n            print('  Saving Model ...')\n            torch.save(model.state_dict(), 'model.bin')\n            print('  Model Saved ...')\n    \n    print('Training Completed.')","c512f797":"if __name__ == \"__main__\":\n    _run()","565b6b54":"class OpenVaccineTestDataset(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return {\n            'x': self.data[idx]\n        }","9d2bd3af":"def test_loop_fn(test_loader, model, device):\n    model.eval()\n    end = time.time()\n    preds = []\n    for i, data in tqdm(enumerate(test_loader),total=len(test_loader)):\n        inputs = data['x']\n        inputs = inputs.to(device, dtype=torch.long)\n        outputs = model(inputs)\n        preds.append(outputs.detach().cpu().numpy())\n    return preds","da4fa742":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\npred_len = 107\nmodel_short = NeuralNet(embed_size=EMBEDDING_DIM, num_targets=NUM_TARGETS).to(device)\nmodel_short.load_state_dict(\n    torch.load('model.bin')\n)\n\npred_len = 130\nmodel_long = NeuralNet(embed_size=EMBEDDING_DIM, num_targets=NUM_TARGETS).to(device)\nmodel_long.load_state_dict(\n    torch.load('model.bin')\n)\n\npublic_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\n\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)\n\npublic_dataset = OpenVaccineTestDataset(public_inputs)\nprivate_dataset = OpenVaccineTestDataset(private_inputs)\n\npublic_data_loader = DataLoader(\n    public_dataset,\n    shuffle=False,\n    batch_size=16,\n    pin_memory=False,\n    drop_last=False,\n    num_workers=0\n)\nprivate_data_loader = DataLoader(\n    private_dataset,\n    shuffle=False,\n    batch_size=16,\n    pin_memory=False,\n    drop_last=False,\n    num_workers=0\n)","51dafbab":"public_preds = np.vstack(test_loop_fn(public_data_loader, model_short, device))\nprivate_preds = np.vstack(test_loop_fn(private_data_loader, model_long, device))","1a137822":"preds_ls = []\n\nfor df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)","4fe1cc76":"submission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\nsubmission.to_csv('submission.csv', index=False)","711235ae":"### Predict on Test Set"}}