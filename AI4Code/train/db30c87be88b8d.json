{"cell_type":{"676730e4":"code","b05a606d":"code","fdfcb6e1":"code","e56b0b1e":"code","9068e868":"code","0edd4f39":"code","14387d59":"code","64d0dc86":"code","29bc2547":"code","1821bc86":"code","924539c2":"code","8d1a861a":"code","e15b2f49":"code","0deff440":"code","533883fc":"code","d890a0c1":"code","da0db19b":"code","a720ec33":"code","2ffe3da6":"code","750f6a4d":"code","7e7f64e4":"code","fbdc21c8":"code","c91fc4dc":"code","c8d03d04":"code","0b3c5da3":"code","12a883e8":"code","e5a4bbea":"code","e2f495a0":"code","155f32af":"code","5830b3a6":"code","06d1e4a6":"code","205a7fac":"code","b4cb2ebb":"code","39191e3e":"markdown","76a0b4ff":"markdown","89677390":"markdown","db6a8f84":"markdown","1dc574ca":"markdown","5eaf170b":"markdown","4a4eaa03":"markdown","ff2751b8":"markdown","3dde315a":"markdown","76510d5d":"markdown","b763cdc1":"markdown","a71f853f":"markdown","2c13dbc9":"markdown","71e2b6b4":"markdown","cc934a47":"markdown","2b05e474":"markdown","51398514":"markdown","064258bc":"markdown","abda7a61":"markdown","97fbfc20":"markdown","3fef5543":"markdown","c6b57d13":"markdown","594efde5":"markdown","39a5a8de":"markdown","b18cb2d6":"markdown","2f27e833":"markdown"},"source":{"676730e4":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.gridspec as gs\nimport numpy as np","b05a606d":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","fdfcb6e1":"print(\"Dataframe\")\nprint(train.head(2))\nprint(\"----------------------------------------------------\")\nprint(\"Info\")\nprint(train.info())\nprint(test.info())\nprint(\"----------------------------------------------------\")\nprint(\"Label Count\")\nprint(train[\"target\"].value_counts())\nprint(\"----------------------------------------------------\")\nprint(\"Missing Data\")\nprint(train.isnull().any().any())","e56b0b1e":"train_details = train.describe()\ntest_details = test.describe()","9068e868":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnew_feature1 = pd.DataFrame(scaler.fit_transform(train.iloc[:,2:]))\ntest_feature1 = pd.DataFrame(scaler.transform(test.iloc[:,1:]))\nnew_feature = new_feature1.describe()\ntest_feature = test_feature1.describe()\n\n\n\nfig = plt.figure(figsize=(18,6))\ng = gs.GridSpec(1,1,fig)\nax = fig.add_subplot(g[0,0])\n\n\n\n\nax.hist(train_details.loc[\"mean\",\"0\":], alpha = 0.5,color=\"magenta\", bins=80, label='train')\nax.hist(test_details.loc[\"mean\",\"0\":],color=\"darkblue\",bins=80, label='test')\nax.text(-0.2, 40, r\"train $\\mu={0:3.2f}$\".format(train_details.loc[\"mean\",\"0\":].mean()))\nax.text(-0.2, 35, r\"train $\\sigma={0:3.2f}$\".format(train_details.loc[\"std\",\"0\":].mean()))\nax.text(-0.2, 30, r\"test $\\mu={0:3.2f}$\".format(test_details.loc[\"mean\",\"0\":].mean()))\nax.text(-0.2, 25, r\"test $\\sigma={0:3.2f}$\".format(test_details.loc[\"std\",\"0\":].mean()))\n\nax.legend(loc=0)\nsns.kdeplot(train_details.loc[\"mean\",\"0\":],color=\"magenta\" , ax=ax, legend =False)\nsns.kdeplot(test_details.loc[\"mean\",\"0\":],color=\"darkblue\" , ax=ax, legend =False)\nax.set_title(\"Mean for Testing\/ Training set\")\n","0edd4f39":"from sklearn.linear_model import LogisticRegression\n\ndata =pd.concat([ test_feature1 , new_feature1.iloc[:,:]])\n\nlr = LogisticRegression(solver='liblinear')\n\nlr.fit(new_feature1.iloc[:,:], train.iloc[:,1])\ncoeff = pd.DataFrame(lr.coef_)\nmean = coeff.T.mean()\nstd = coeff.T.std()\ncoeff_50 = coeff.T.sort_values(by=0,ascending = False).iloc[:50,:]\ncol = coeff_50.index \ndata = data[col]\ndata = data.describe()\n\ndf_corr = new_feature1.iloc[:,:].apply(lambda x: x.corr(train.iloc[:,1]))\ndf_corr = df_corr.reset_index().sort_values(by=0,ascending = False).iloc[:50,:]\n\n\n\n\n\nfig = plt.figure(figsize=(25,25))\ng = gs.GridSpec(4,1,fig)\nax = fig.add_subplot(g[0,0])\nax2 = fig.add_subplot(g[1,0])\nax3 = fig.add_subplot(g[2,0])\nax4 = fig.add_subplot(g[3,0])\n\nax.set_title(\"Coefficient Of all Features\")\nax.plot(coeff.T,'ro' )\nax.text(0, 0.75, r\"$\\mu={0:3.2f}$\".format(mean[0]))\nax.text(0, 0.60, r\"$\\sigma={0:3.2f}$\".format(std[0]))\n\nax2.set_title(\"Coefficient Of top 50 Features\")\ncoeff_50.plot.bar(ax=ax2)\n\nax3.set_title(\"Mean of Top 50 Columns for both Dataset\")\ndata.loc[\"mean\",:].plot.bar(ax=ax3)\n\nax4.set_title(\"Correlation Of top 50 Features\")\ndf_corr.iloc[:,1].plot.bar(ax=ax4,color =\"Blue\")\n\n","14387d59":"plt.figure(figsize=(16,6))\n\nplt.title(\"Distribution of min values per row in the train and test set\")\nsns.distplot(new_feature1.min(axis=1),color=\"orange\", kde=True,bins=120, label='train')\nsns.distplot(test_feature1.min(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()\n\n","64d0dc86":"plt.figure(figsize=(16,6))\n\nplt.title(\"Distribution of Mean values per row in the train and test set\")\nsns.distplot(new_feature1.mean(axis=1),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test_feature1.mean(axis=1),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","29bc2547":"plt.figure(figsize=(16,6))\n\nplt.title(\"Distribution of std values per row in the train and test set\")\nsns.distplot(new_feature1.std(axis=1),color=\"orange\", kde=True,bins=120, label='train')\nsns.distplot(test_feature1.std(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","1821bc86":"features_90 = train[train[\"target\"] == 1]\nfeatures_902 = train[train[\"target\"] == 0]\nfeatures_90 = features_90.iloc[:90,:]\nfeatures_90_final = pd.concat([features_90, features_902])\nfeatures_90_final = pd.DataFrame(scaler.fit_transform(features_90_final.iloc[:,2:]))\n\n\nplt.figure(figsize=(16,6))\n\nplt.title(\"Distribution of Equal Dataset compare to Test Data\")\nsns.distplot(features_90_final.mean(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test_feature1.mean(axis=1),color=\"orange\", kde=True,bins=120, label='test')\n\nplt.legend()\nplt.show()\n\n","924539c2":"def scores(X ,y, model):\n    score = []\n    score2 = []\n    results = cross_val_score(model, X, y, cv = 3, scoring = 'roc_auc')\n    results2 = cross_val_score(model, X, y, cv = 3, scoring = 'accuracy')\n    score.append(results)\n    score2.append(results2)\n    score_df = pd.DataFrame(score).T\n    score_df.loc['mean'] = score_df.mean()\n    score_df.loc['std'] = score_df.std()\n    score_df= score_df.rename(columns={0:'roc_auc'})\n    print(score_df.iloc[-2:,:])\n    score_df2 = pd.DataFrame(score2).T\n    score_df2.loc['mean'] = score_df2.mean()\n    score_df2.loc['std'] = score_df2.std()\n    score_df2= score_df2.rename(columns={0:'acc'})\n    print(score_df2.iloc[-2:,:])","8d1a861a":"from sklearn.model_selection import GridSearchCV ,cross_val_score\nclf = LogisticRegression(max_iter=4000)\n\n\nparam_grid = [\n  {'class_weight' : ['balanced', None],'penalty': ['l1'], 'solver': ['liblinear', 'saga'],'C' : [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]},\n  {'class_weight' : ['balanced', None],'penalty': ['l2'], 'solver': ['newton-cg','lbfgs'],'C' : [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]},\n {'class_weight' : ['balanced', None],'penalty': ['l2','l1'], 'solver': ['saga'],'C' : [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]},\n ]\n\ngrid_search = GridSearchCV(clf, param_grid=param_grid, cv=10, scoring='roc_auc')\ngrid_search.fit(new_feature1, train.iloc[:,1])\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))","e15b2f49":"new_features_corr = new_feature1.corr()\nsns.heatmap(new_features_corr)","0deff440":"model = LogisticRegression(C=0.1,random_state=42,class_weight='balanced',penalty='l1',solver='liblinear')\nfrom sklearn.feature_selection import RFE\n\nrfr_grid =  [5,10,15,25, 50, 75, 100, 125, 150, 175, 200 ,225,250,275,300]\n\nfor feature_num in rfr_grid :\n    selector = RFE(model, feature_num, step=1)\n    selector = selector.fit(new_feature1, train.iloc[:,1])\n    new_features =  selector.transform(new_feature1)\n    print(\"Number of Features {}\".format(feature_num))\n    scores(new_feature1,train.iloc[:,1],selector)","533883fc":"selector = RFE(model,25, step=1)\nselector = selector.fit(new_feature1, train.iloc[:,1])\nnew_features =  selector.transform(new_feature1)\nbool_mask = pd.Series(selector.support_,name='bools') \nscores(new_feature1,train.iloc[:,1],selector)\ncol = []\nfor num, i  in enumerate(bool_mask):\n    if i ==True :\n        col.append(num)\n\nrfe_feature = new_feature1[col]\nrfe_test = test_feature1[col]\nprint(\"The columns that were used were {}\".format(col))","d890a0c1":"plt.figure(figsize=(16,6))\n\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(rfe_feature.mean(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(rfe_test.mean(axis=1),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","da0db19b":"rfe_feature[\"sum\"] = rfe_feature.sum(axis=1)\nrfe_feature[\"min\"] = rfe_feature.min(axis=1)\nrfe_feature[\"max\"] = rfe_feature.max(axis=1)\nrfe_feature[\"mean\"] = rfe_feature.mean(axis=1)\nrfe_feature[\"std\"] = rfe_feature.std(axis=1)\nrfe_feature[\"skew\"] = rfe_feature.skew(axis=1)\nrfe_feature[\"var\"] = rfe_feature[\"std\"]**2\n\nrfe_feature[\"target\"] = train.iloc[:,1]\n\n\n\nrfe_test[\"sum\"] = rfe_test.sum(axis=1)\nrfe_test[\"min\"] = rfe_test.min(axis=1)\nrfe_test[\"max\"] = rfe_test.max(axis=1)\nrfe_test[\"mean\"] = rfe_test.mean(axis=1)\nrfe_test[\"std\"] = rfe_test.std(axis=1)\nrfe_test[\"skew\"] = rfe_test.skew(axis=1)\nrfe_test[\"var\"] = rfe_test[\"std\"]**2\n\n\nrfe_feature_true = rfe_feature[rfe_feature[\"target\"] == 1]\nrfe_feature_false = rfe_feature[rfe_feature[\"target\"] == 0]\n\n","a720ec33":"fig = plt.figure(figsize=(21,35))\ng = gs.GridSpec(7,1,fig)\nax = fig.add_subplot(g[0,0])\nax1 = fig.add_subplot(g[1,0])\nax2 = fig.add_subplot(g[2,0])\nax3 = fig.add_subplot(g[3,0])\nax4= fig.add_subplot(g[4,0])\nax5 = fig.add_subplot(g[5,0])\nax6 = fig.add_subplot(g[6,0])\n\n\nax.set_title(\"SUM Distribuion\")\nsns.distplot(rfe_feature_true[\"sum\"],color=\"red\", kde=True,bins=120, label='True' ,ax=ax)\nsns.distplot(rfe_feature_false[\"sum\"],color=\"orange\", kde=True,bins=120, label='False',ax=ax)\n\nax1.set_title(\"MIN Distribuion\")\nsns.distplot(rfe_feature_true[\"min\"],color=\"red\", kde=True,bins=120, label='True' ,ax=ax1)\nsns.distplot(rfe_feature_false[\"min\"],color=\"orange\", kde=True,bins=120, label='False',ax=ax1)\n\nax2.set_title(\"MAX Distribuion\")\nsns.distplot(rfe_feature_true[\"max\"],color=\"red\", kde=True,bins=120, label='True' ,ax=ax2)\nsns.distplot(rfe_feature_false[\"max\"],color=\"orange\", kde=True,bins=120, label='False',ax=ax2)\n\nax3.set_title(\"MEAN Distribuion\")\nsns.distplot(rfe_feature_true[\"mean\"],color=\"red\", kde=True,bins=120, label='True' ,ax=ax3)\nsns.distplot(rfe_feature_false[\"mean\"],color=\"orange\", kde=True,bins=120, label='False',ax=ax3)\n\nax4.set_title(\"STD Distribuion\")\nsns.distplot(rfe_feature_true[\"std\"],color=\"red\", kde=True,bins=120, label='True' ,ax=ax4)\nsns.distplot(rfe_feature_false[\"std\"],color=\"orange\", kde=True,bins=120, label='False',ax=ax4)\n\nax5.set_title(\"SKEW Distribuion\")\nsns.distplot(rfe_feature_true[\"skew\"],color=\"red\", kde=True,bins=120, label='True' ,ax=ax5)\nsns.distplot(rfe_feature_false[\"skew\"],color=\"orange\", kde=True,bins=120, label='False',ax=ax5)\n\nax6.set_title(\"VAR Distribuion\")\nsns.distplot(rfe_feature_true[\"var\"],color=\"red\", kde=True,bins=120, label='True' ,ax=ax6)\nsns.distplot(rfe_feature_false[\"var\"],color=\"orange\", kde=True,bins=120, label='False',ax=ax6)\n\nax.legend(loc=\"best\")\n","2ffe3da6":"rfe_feature = rfe_feature.drop([\"target\"],axis=1)","750f6a4d":"model = LogisticRegression(C=0.1,random_state=42,class_weight='balanced',penalty='l1',solver='liblinear',tol=0.02,verbose=0)\nrfr_grid =  [23,24,25,26,27,28,29,30,31,32]\n\nfor feature_num in rfr_grid :\n    selector = RFE(model, feature_num, step=1)\n    selector = selector.fit(rfe_feature, train.iloc[:,1])\n    new_features =  selector.transform(rfe_feature)\n    print(\"Number of Features {}\".format(feature_num))\n    scores(new_features,train.iloc[:,1],selector)","7e7f64e4":"selector = RFE(model,23, step=1)\nselector = selector.fit(rfe_feature, train.iloc[:,1])\nnew_features =  selector.transform(rfe_feature)\nnew_test = selector.transform(rfe_test)\n\nbool_mask = pd.Series(selector.support_,name='bools') \nscores(new_features,train.iloc[:,1],selector)\nold_cols = [16, 33, 43, 63, 65, 73, 80, 82, 90, 91, 101, 108, 117, 127, 133, 134, 165, 189, 194, 199, 217, 226, 258, 295, 298,\"sum\",\"min\",\"max\",\"mean\",\"std\",\"skew\",\"var\"]\ncol = []\nfor num, i  in enumerate(bool_mask):\n \n    if i ==True :\n        col.append(old_cols[num])\n        \n        \n\n\nprint(\"The columns that were used were {}\".format(col))","fbdc21c8":"model = LogisticRegression(C=0.1,random_state=42,class_weight='balanced',penalty='l1',solver='liblinear')\nmodel = model.fit(new_features, train.iloc[:,1])\n","c91fc4dc":"prediction5 = model.predict_proba(new_test)\n\ntest[\"target\"]= prediction5[:,1]\nsubmission = test[[\"id\",\"target\"]]\nsubmission.to_csv(\"submission14.csv\",index=False)","c8d03d04":"rfe_feature2  =rfe_feature.drop([\"std\",\"var\"], axis = 1)\nrfe_test2  =rfe_test.drop([\"std\",\"var\"], axis = 1)","0b3c5da3":"\nmodel = LogisticRegression(C=0.1,random_state=42,class_weight='balanced',penalty='l1',solver='liblinear')\nrfr_grid =  [23,24,25,26,27,28,29,30,]\n\nfor feature_num in rfr_grid :\n    selector = RFE(model, feature_num, step=1)\n    selector = selector.fit(rfe_feature2, train.iloc[:,1])\n    new_features =  selector.transform(rfe_feature2)\n    print(\"Number of Features {}\".format(feature_num))\n    scores(new_features,train.iloc[:,1],selector)","12a883e8":"selector = RFE(model,23, step=1)\nselector = selector.fit(rfe_feature2, train.iloc[:,1])\nnew_features =  selector.transform(rfe_feature2)\nnew_test = selector.transform(rfe_test2)\nbool_mask = pd.Series(selector.support_,name='bools') \nscores(new_features,train.iloc[:,1],selector)\nold_cols = [16, 33, 43, 63, 65, 73, 80, 82, 90, 91, 101, 108, 117, 127, 133, 134, 165, 189, 194, 199, 217, 226, 258, 295, 298,\"sum\",\"min\",\"max\",\"mean\",\"skew\"]\ncol = []\nfor num, i  in enumerate(bool_mask):\n    if i ==True :\n        col.append(old_cols[num])\n\nprint(\"The columns that were used were {}\".format(col))","e5a4bbea":"model = LogisticRegression(C=0.1,random_state=42,class_weight='balanced',penalty='l1',solver='liblinear',tol=0.02,verbose=0)\nmodel = model.fit(rfe_feature2, train.iloc[:,1])\nprediction5 = model.predict_proba(rfe_test2)\n\ntest[\"target\"]= prediction5[:,1]\nsubmission = test[[\"id\",\"target\"]]\nsubmission.to_csv(\"submission15.csv\",index=False)","e2f495a0":"rfe_feature3  =rfe_feature.drop([\"std\",\"var\",\"skew\",\"min\",\"max\"], axis = 1)\nrfe_test3  = rfe_test.drop([\"std\",\"var\",\"skew\",\"min\",\"max\"], axis = 1)","155f32af":"model = LogisticRegression(C=0.1,random_state=42,class_weight='balanced',penalty='l1',solver='liblinear',tol=0.02,verbose=0)\nrfr_grid =  [23,24,25,26,27]\n\nfor feature_num in rfr_grid :\n    selector = RFE(model, feature_num, step=1)\n    selector = selector.fit(rfe_feature3, train.iloc[:,1])\n    new_features =  selector.transform(rfe_feature3)\n    print(\"Number of Features {}\".format(feature_num))\n    scores(new_features,train.iloc[:,1],selector)","5830b3a6":"selector = RFE(model,26, step=1)\nselector = selector.fit(rfe_feature3, train.iloc[:,1])\nnew_features =  selector.transform(rfe_feature3)\nnew_test = selector.transform(rfe_test3)\nbool_mask = pd.Series(selector.support_,name='bools') \nscores(new_features,train.iloc[:,1],selector)\nold_cols = [16, 33, 43, 63, 65, 73, 80, 82, 90, 91, 101, 108, 117, 127, 133, 134, 165, 189, 194, 199, 217, 226, 258, 295, 298,\"sum\",\"mean\"]\ncol = []\nfor num, i  in enumerate(bool_mask):\n    if i ==True :\n        col.append(old_cols[num])\n\nprint(\"The columns that were used were {}\".format(col))","06d1e4a6":"model = LogisticRegression(C=0.1,random_state=42,class_weight='balanced',penalty='l1',solver='liblinear')\nmodel = model.fit(rfe_feature3, train.iloc[:,1])\nprediction5 = model.predict_proba(rfe_test3)\n\ntest[\"target\"]= prediction5[:,1]\nsubmission = test[[\"id\",\"target\"]]\nsubmission.to_csv(\"submission16.csv\",index=False)","205a7fac":"from sklearn.ensemble import BaggingClassifier, VotingClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn import linear_model\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.linear_model import ElasticNet\n\nmodel_1= linear_model.SGDClassifier('log',eta0=1, max_iter=1000, tol=0.0001)\nmodel_2 = GaussianNB()\nmodel_3 = LogisticRegression( C = 0.1, class_weight ='balanced', penalty = 'l1', solver = 'liblinear')\nmodel_4 =  ElasticNet(alpha = 0.085, l1_ratio = 0.5)\ngrid  = [1,3,6,9,12,15,18,20,22,24,28]\n\n\n                          \nfor params in grid:\n    blg = BaggingClassifier(base_estimator=model_3,n_estimators = params).fit(rfe_feature3,train.iloc[:,1])\n    scores(rfe_feature3,train.iloc[:,1],blg)\n\n\nblg = BaggingClassifier(base_estimator=model_3,n_estimators = 24).fit(rfe_feature3,train.iloc[:,1])\nprediction5 = blg.predict_proba(rfe_test3)\n\n\ntest[\"target\"]= prediction5[:,1]\nsubmission = test[[\"id\",\"target\"]]\nsubmission.to_csv(\"submission17.csv\",index=False)","b4cb2ebb":"eclf = VotingClassifier(estimators=[('NB', model_2), ('LR', model_3), ('LIN', model_1)],\n                       voting='soft', weights=[1,2, 2])\n\neclf.fit(rfe_feature3,train.iloc[:,1])\nscores(rfe_feature3,train.iloc[:,1],eclf)\n\nprediction5 = eclf.predict_proba(rfe_test3)\n\n\ntest[\"target\"]= prediction5[:,1]\nsubmission = test[[\"id\",\"target\"]]\nsubmission.to_csv(\"submission20.csv\",index=False)\n","39191e3e":"The min KDE for the train dataset is slighly skewed right when compared to the test dataset.","76a0b4ff":"The report will now examine the train dataset vs the test dataset in greater depth.","89677390":"The mean's KDE for train and test dataset is slightly different with the test dataset having a taller distribution.","db6a8f84":"As you can see that each of new the values show differences between the target values. Therefore, I will recursively eliminate the features see how they improve the ROC score.\n","1dc574ca":"<h1> Optimising the Classifiers <\/h1>\nDue to the nature of the compeition, a binary classification competition, a Logestic Model was used to start the alogorithm exploration. A GridSearch was then performed on the Logistic Regression model to optimise the classifier.  ","5eaf170b":"The best parameters from the GridSearch were then used to make the first submission. ","4a4eaa03":"This showed a slight improvement however overfitting was still occuring. The next step was to try and combine a few models to reduce this overfitting.","ff2751b8":"<h1> Don't Overfit <\/h1>\n<br>\n<br>\nThis report is the process that was untaken to create the model that was entered into the Kaggle competition,\"Don't Overfit\".The premise of the competition is stated below : \n<br>\n<br>\n<blockquote>\"we have 20,000 rows of continuous variables, and a mere handful of training samples. Once again, we challenge you not to overfit. Do your best, model without overfitting, and add, perhaps, to your own legend.\"<\/blockquote>\n<br>\n<br>\nThe basic structure of this report will consist of :\n<ul>\n    <li> Exploratory Data Analysis<\/li>\n    <li> Exploring the Features Correlation<\/li>\n    <li> Creating a scoring function <\/li>\n    <li> Basic Modelling <\/li>\n    <li> Experimenting with the model - Feature selection <\/li>\n<\/ul>\n\n<h2> Exploratory Data Analysis<\/h2>","3dde315a":"This resulted in huge overfitting. From the initial graphs ,we can see that the std ditribution is more of a Gemma shape than a Gaussian one. This means that I will delete both the std and variance feature.","76510d5d":"The information below shows example records of the dataframe, a brief description of the dataframe , the balance of the binary classifications and any missing data.","b763cdc1":"After looking at the mean and std, a graph was made to examine the coefficient and correlation for each feature. ","a71f853f":"Again this resulted in high bias to the train dataset and therefore creating overfitting,so more of the features were dropped.","2c13dbc9":"<h1>Creating the Cross validation Scoring Function<\/h1>","71e2b6b4":"The two approaches that were used was the BaggingClassifier and  the VotingClassifier with the Votingclassifer offering better results on the test dataset. ","cc934a47":"With the reduced features we can see that the width of the distributions are similar, which can help us tune our model and so increase our ROC score on both the training and testing dataset. The first method was to add new features to the data. A quick evaluation on the postive and negative targets took place.","2b05e474":"Again, there are differences in the std KDE between the train and test dataset with the train test being skewed to the right. One factor that could explain this difference is the bias in the dataset. As there are many more positive targets than negatives ones, this could explain the differences in the train\/ test dataset if the test dataset had a 50-50% split. ","51398514":"<h1> Making an Emsemble <\/h1>","064258bc":"<h1> Feature Selection <\/h1> \n\nThere are a number of feature selection methods that could be used to select the features for our classifier. The Feature Selection methods that I used was : Recursive Feature Elimination.\n<br>\n<br>\nThis process obtains the importance of each feature and recursively drops the feature with the least coefficient or importance until it finds the selection with the best number of features.\n<br>\n<br>\nPCA was not used as the correlation between variables was not deemed as strong or even moderate. This lacking of strength between correlations can be seen in the heatmap graph below. \n","abda7a61":"An examination of the test and train  once again occured to see the difference between the two dataset. ","97fbfc20":"From an initial look, we can see the following information :\n<ul> \n    <li> Dataset contains Interval Data.<\/li>\n    <li> It has 300 features and only 250 records <\/li>\n    <li> There is an imbalance in the classification of 16\/25 <\/li>\n    <li> There is no missing Data<\/li>\n<\/ul>","3fef5543":"You can see that there has been some distortion to the train data, so even though it has the same mean and std as the train dataset, the scale is not the same. To fix this issue,the sklearn standard scaler was used to put both datasets on the same scale. As the data has a normal distributed within each feature, the scale will  distribute the data around 0, with a standard deviation of 1. \n<br> \n<br>\nTo do this, the mean and standard deviation are calculated for the feature and then the feature is scaled based on: x(i)\u2013mean(x) \/ stdev(x)","c6b57d13":"As we can see that the ROC score ranges from 0.703 to 0.808. However, we need to consider the problem of overfitting to the training model. Based on the fact that : \n<ul>\n    <li> The coefficient has a mean of -0.02 and std of 0.22<\/li>\n    <li> The differences between the train and test dataset will make overfitting likely<\/li>\n<\/ul>\na decicision was made to make the model simpler so that we try and reduce the bias and hopefully stop overfitting to our train dataset. These means we must reduce the number of features and then further tune the model so that we can make sure that we get the best bias-variance trade off that we can.","594efde5":"25 Variables were selected for the model. The variables the were selected can be seen below","39a5a8de":"Again, overfitting is occurring on the test data. The next step will be to do further research on how to avoid overfitting. I will investigate similar competitions and look at the process that the winners took to avoid overfitting with a high dimensional \/ low sample dataset.","b18cb2d6":"The graph below shows the Std and mean of the dataframe. ","2f27e833":"When the data that had a 50-50% split was compared to the test dataset, it showed a difference between the two dataset. \n<br>\n<br> \nThis shows the biggest problem that we face. If we reduce the variance in our training dataset then the model will overfit, as the training and test data has a big enough differences to predict high acc and roc scores on the training data but not on the test dataset.  "}}