{"cell_type":{"bff5da5e":"code","df636455":"code","7e07b07f":"code","cac753a9":"code","da8fe537":"code","8ed22cf6":"code","9205a622":"code","3f5b12e2":"code","37991f12":"code","ec4a0804":"code","6c67a5f3":"code","543f834c":"code","088784a3":"code","9cced0cd":"code","dc0ccfe7":"code","7de8d093":"markdown","16017968":"markdown","d62d3413":"markdown","d798a15a":"markdown","224786e9":"markdown","e7face69":"markdown","6c7b383e":"markdown","d804a7a4":"markdown","96e3ab76":"markdown","cc2dc283":"markdown","d14bc8bf":"markdown","9c269152":"markdown","3302295c":"markdown","d7ea15b4":"markdown","47005057":"markdown","742ad58b":"markdown","6a41f49a":"markdown","e5fdf7ed":"markdown","a7ad69b3":"markdown","d5492310":"markdown","abd84c57":"markdown","40579f91":"markdown","40fe7946":"markdown","debc9863":"markdown"},"source":{"bff5da5e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","df636455":"df=pd.read_csv('\/kaggle\/input\/titanic\/train_and_test2.csv')\ndf.head()","7e07b07f":"df.info()","cac753a9":"df.drop(['Passengerid','zero','zero.1','zero.2','zero.3','zero.4','zero.5','zero.6','zero.7','zero.8','zero.9','zero.10','zero.11','zero.12','zero.13','zero.14','zero.15','zero.16','zero.17','zero.18'],axis=1,inplace=True)\ndf.rename(columns={'2urvived':'Survived'},inplace=True) \ndf.head()","da8fe537":"df.dropna(inplace=True)","8ed22cf6":"plt.figure(figsize=(12,10))\n# we keep annot=True to make the values appear of df.corr() appear on the heatmap\nsns.heatmap(df.corr(),annot=True,cmap=plt.cm.plasma)","9205a622":"sns.pairplot(df)","3f5b12e2":"df.columns","37991f12":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nX=df.drop(['Survived'],axis=1)\nY=df['Survived']\nX_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.20,random_state=1)\n","ec4a0804":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(max_iter=300)\nlr.fit(X_train,y_train)\nyhat_lr=lr.predict(X_test)\nprint(\"Accuracy of Logistic Model is:\",accuracy_score(yhat_lr,y_test))","6c67a5f3":"from sklearn.metrics import accuracy_score,confusion_matrix\nax=confusion_matrix(yhat_lr,y_test)\nsns.heatmap(ax,annot=True,cmap=plt.cm.plasma)\nplt.xlabel('Predict')\nplt.ylabel('Actual')","543f834c":"from sklearn.neighbors import KNeighborsClassifier\nKN=KNeighborsClassifier(n_neighbors=5)\nKN.fit(X_train,y_train)\nyhat=KN.predict(X_test)\nprint(\"Accuracy of K-Nearest Neighbor Model is:\",accuracy_score(yhat,y_test))","088784a3":"from sklearn.tree import DecisionTreeClassifier\ntree=DecisionTreeClassifier(random_state=0)\ntree.fit(X_train,y_train)\nyhat=tree.predict(X_test)\nprint(\"Accuracy of Decision Tree Classifier Model is:\",accuracy_score(yhat,y_test))","9cced0cd":"from sklearn.svm import SVC\nsvm=SVC(kernel='linear')\nsvm.fit(X_train,y_train)\nyhat=svm.predict(X_test)\nprint(\"Accuracy of Support Vector Machine Model is:\",accuracy_score(yhat,y_test))","dc0ccfe7":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(n_estimators=200,criterion='entropy')\nrfc.fit(X_train,y_train)\nyhat=rfc.predict(X_test)\nprint(\"Accuracy of Random Forest Classifier Model is:\",accuracy_score(yhat,y_test))","7de8d093":"<h3 style=\"color:blue\">We see that the best model for the prediction is Support Vector Machine with a accuracy of 0.8396<\/h3>","16017968":"<center><h3>Thank you for reading my notebook and don't forget to upvote the notebook<\/h3><\/center>","d62d3413":"<h4>Remove unnecessary columns like zero.1,zero.2..... using pandas dropna() method<br>\nWe will rename '2urvived' column as 'Survived' using rename method<\/h4>","d798a15a":"**Heatmap of Co-Relation of various columns with each other\ndf.corr() shows the relation between various feature and we hence we can see the influence of the features on each other**","224786e9":"<h3>Let's visualize the data using confusion matrix<\/h3>\n<span style=\"color:red\"><b>For those who don't know about Confusion Matrix:<br><br> A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making.<\/b><\/span>","e7face69":"<center><h1>Importing Libraries<\/h1><\/center>","6c7b383e":"<h3>3) Decision Tree<\/h3>","d804a7a4":"**Let's see the various columns in df**","96e3ab76":"<h1>Importing Dataset<\/h1>","cc2dc283":"<h1>Best Model<\/h1>","d14bc8bf":"<h1>Data Preprocessing<\/h1>","9c269152":"<h3>5)Random Forest Classifier<\/h3>","3302295c":"**Numpy : Python Library used for working with arrays<br>\nMatplotlib : Used for data visualizations. ex: to plot the relations between various factors<br>\nSeaborn: used for data visualization like matplotlib<br>\nPandas: used for data analysis and manipulation<br>**","d7ea15b4":"<h3>4)Support Vector Machine(SVM) <\/h3>","47005057":"**Below we use pairplot method from seaborn library. It is used to plot graphs between the various features.**","742ad58b":"# Overview\n\nThis is the original data from Titanic competition plus some changes that I applied to it to be better suited for binary logistic regression:\n\nMerged the train and test data.\n\nRemoved the 'ticket' and 'cabin' attributes.\n\nMoved the 'Survived' attribute to the last column.\n\nAdded extra zero columns for categorical inputs to be better suited for One-Hot-Encoding.\n\nSubstituted the values of 'Sex' and 'Embarked' attributes with binary and categorical values respectively.\n\nFilled the missing values in 'Age' and 'Fare' attributes with the median of the data.","6a41f49a":"**We will split the data for training and testing using train_test_split**","e5fdf7ed":"<h1 style=\"text-align:center;color:blue\">Don't forget to Upvote if you like it.\ud83d\ude0a<\/h1>","a7ad69b3":"<h3>1)Logistic Regression<\/h3>","d5492310":"<h3>2) K-Nearest Neighbor<\/h3>","abd84c57":"# **Introduction**\n\n\nIn this kernal we will going through the whole process of creating a Machine Learning on the Titanic dataset. It provides us a glance over the fate of the passenger onboard the \"Unsinkable\" ship which sinked. The dataset categorizes the passanger based on their economic status, sex, age and their survival. In this kernel, we will be analyzing, cleaning and visulizing the data in different forms to obtain hidden insights. Also, we'll create different ML models and depending upon their accuracies, use the most suitable model for the prediction.","40579f91":"# **Building and Training the Model**","40fe7946":"**Remove the rows having null values**","debc9863":"<h1>Data Visualization<\/h1>"}}