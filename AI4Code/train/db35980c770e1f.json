{"cell_type":{"50b19d6b":"code","69972e34":"code","ee21a7a9":"code","b52489da":"code","febf2972":"code","99963dfb":"code","ab889b83":"code","98aac352":"code","e21f8528":"code","4164c998":"code","5ca68ac1":"code","25d9cbdf":"code","43a95d17":"code","f7680854":"code","7198a12f":"code","14a04fc9":"code","fb8b85eb":"code","4865aa37":"code","8cc8b2bb":"code","89123447":"code","e50c74fb":"code","28c9b802":"code","dbf7fd15":"code","d802923b":"code","29fdf7d1":"code","842737b0":"code","9829d04e":"code","459c4fdf":"code","bb014590":"code","34f3e578":"code","0b5b96fb":"code","d93e1443":"code","a5103b8b":"code","4aa9f27c":"code","3c8a712b":"code","d4f7398d":"markdown","0f28b931":"markdown","b7ea5ae5":"markdown","29770087":"markdown","4779c8d2":"markdown","6e3fad47":"markdown","f5ebffe6":"markdown","f87f4c21":"markdown","c4aa4a1b":"markdown","ce9eab13":"markdown","758ab26c":"markdown","728160ca":"markdown","2fb55a23":"markdown","c8689b95":"markdown","5a32884d":"markdown","ece60450":"markdown","21afc852":"markdown","e6cba24a":"markdown","d6abf9b3":"markdown","6172b503":"markdown","2beab0db":"markdown","786fba48":"markdown","5624d3b3":"markdown","e51c1220":"markdown","65c5d529":"markdown","f4200632":"markdown","938db850":"markdown"},"source":{"50b19d6b":"# Accuracy and Confusion matrices\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# A simple function to display the confusion matrix (accuracy, precision, recall ...)\ndef my_confusion_matrix(y_test, y_pred):\n    \n    #Create Confusion Matrix\n    # y_pred = DecTreeClf.predict(x_test)\n\n\n    from sklearn.metrics import classification_report, confusion_matrix\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))\n\n# A simple function to display just Accuracy\ndef my_accuracy(y_test,y_pred):\n    acc = accuracy_score(y_true=y_test, y_pred=y_pred)\n    print('Acc: {:.4f}'.format(acc))","69972e34":"## Function to split features and classes into train and test split\ndef split_data(features,classes,test_size = 0.25):\n    from sklearn.model_selection import train_test_split\n    import pandas as pd\n    # Split features in 75%,25% proportion for the model\n    x_train, x_test, y_train, y_test =  train_test_split(all_features,all_classes, test_size = 0.25, random_state = 0)\n    return x_train, x_test, y_train, y_test","ee21a7a9":"### DECISION TREE CLASSIFIER\ndef fun_decisiontree(x_train,y_train):\n    from sklearn import tree\n    # from sklearn.ensemble import RandomForestClassifier\n    import graphviz\n\n    DecTreeClf = tree.DecisionTreeClassifier()\n    DecTreeClf = DecTreeClf.fit(x_train,y_train)\n    return DecTreeClf\n\n### RANDOM FOREST CLASSIFIER\ndef fun_randomforest(x_train,y_train):\n    from sklearn.ensemble import RandomForestClassifier\n\n    # Create the random Forest Classifier and the corresponding K-Fold score\n    RandForestClf = RandomForestClassifier()\n    RandForestClf.fit(x_train,y_train)\n    return RandForestClf\n\n### SVM.SVC  CLASSIFIER    \ndef fun_SVM(x_train, y_train,C=1.0,kernel='linear'):    \n    from sklearn import svm, datasets\n\n    # Build the model and the corresponding predictions\n    C = 1.0\n    svc = svm.SVC(kernel='kernel',C=C).fit(x_train, y_train)\n    return svc","b52489da":"# Read the mammographic csv file and put it into a panda\ndef fun_read_csv_data():\n    import pandas as pd\n    columns_list = [\"BI-RADS\",\"Age\",\"Shape\",\"Margin\",\"Density\",\"Severity\"]\n    masses = pd.read_csv(\"..\/input\/mammograph-data-set\/mammographic_masses.data.txt\",na_values = \"?\", names = columns_list)\n    # masses = pd.read_csv(\"https:\/\/data.world\/uci\/mammographic-mass\/file\/mammographic_masses.data.csv\",na_values = \"?\", names = columns_list)\n    return masses\n    \n\nmasses = fun_read_csv_data()\n    \nmasses.head()\nmasses.describe() \n# import pandas as pd\n# columns_list = [\"BI-RADS\",\"Age\",\"Shape\",\"Margin\",\"Density\",\"Severity\"]\n# masses = pd.read_csv(\"..\/input\/mammograph-data-set\/mammographic_masses.data.txt\",na_values = \"?\", names = columns_list)\n# # masses = pd.read_csv(\"https:\/\/data.world\/uci\/mammographic-mass\/file\/mammographic_masses.data.csv\",na_values = \"?\", names = columns_list)\n\n# masses.head()\n# masses.describe()","febf2972":"# Check the Panda to see what's inside\n# And check out we've opened it properly\n\nfrom matplotlib import pyplot as plt\n# print (masses.describe())\nmasses.describe()\nprint (\"count : \" + str(masses.count()))\n\n\n","99963dfb":"# Create a panda that will hold \"Non Nana\" data (if we drop nan, outliers etc ...)\n# This will allow us to first check how many rows would be dropped\nmasses_nonan = masses.dropna()\nmasses_nonan.describe()\n","ab889b83":"# In this section we will create histograms plot for each paramaters\n# And we will put 2 bars for each\n# One with all the rows and one with all NAN values dropped\n# This will allow us to check whether NAN value are randomly distributed\n# -> i.e.: that simply dropping NaN will not create artificial bias\n\n# Plot age distribution\nplt.hist(masses[\"Age\"],color=\"blue\",label=\"Full file\")\nplt.hist(masses_nonan[\"Age\"],color=\"red\",label = \"with nan dropped\")\nplt.title(\"Age distribution\")\nplt.legend()\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Plot Shape distribution\nplt.hist(masses[\"Shape\"],color=\"blue\",label=\"Full file\")\nplt.hist(masses_nonan[\"Shape\"],color=\"red\",label = \"with nan dropped\")\nplt.title(\"Shape distribution\")\nplt.legend()\nplt.xlabel(\"Shape\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Plot Margin distribution\nplt.hist(masses[\"Margin\"],color=\"blue\",label=\"Full file\")\nplt.hist(masses_nonan[\"Margin\"],color=\"red\",label = \"with nan dropped\")\nplt.title(\"Margin distribution\")\nplt.legend()\nplt.xlabel(\"Margin\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Plot Density distribution\nplt.hist(masses[\"Density\"],color=\"blue\",label=\"Full file\")\nplt.hist(masses_nonan[\"Density\"],color=\"red\",label = \"with nan dropped\")\nplt.title(\"Density\")\nplt.legend()\nplt.xlabel(\"Density\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Plot Shape distribution\nplt.hist(masses[\"Severity\"],color=\"blue\",label=\"Full file\")\nplt.hist(masses_nonan[\"Severity\"],color=\"red\",label = \"with nan dropped\")\nplt.title(\"Severity distribution\")\n# plt.legend()\nplt.xlabel(\"Severity\")\nplt.ylabel(\"Count\")\nplt.show()","98aac352":"# Yep! It looks like the NaN are randomly distributed. It should be safe to drop them\n\n# Create a panda that will hold \"cleaned\" data (if we drop nan, outliers etc ...)\nmasses = masses.dropna()\nmasses.describe()","e21f8528":"def fun_convert_numpy(masses):\n\n    import numpy as np\n\n# # Convert the pandas into numpy for later on\n# # Unlike what it is being said, we'll keep it into a single numpy for now (we'll split features and labels later)\n    feature_names = [\"Age\",\"Shape\",\"Margin\",\"Density\"]\n\n    all_features = masses[feature_names].values\n    all_classes = masses['Severity'].values\n    return all_features,all_classes\n\nall_features, all_classes = fun_convert_numpy(masses)\n# print (masses)\nprint (all_features)\nprint (all_classes)","4164c998":"def fun_get_scaled_features(masses):\n    from sklearn.preprocessing import StandardScaler,MinMaxScaler\n    \n    all_features,all_classes = fun_convert_numpy(masses)\n    \n    # Scale features data using StandardScaler \n    # No need to scale the labels in all_classes as they are just 0 and 1's\n    scaler = StandardScaler()\n    all_features = scaler.fit_transform(all_features)\n    return all_features,all_classes\n\nall_features,all_classes = fun_get_scaled_features(masses)\nprint (all_features)\nprint (all_classes)\n","5ca68ac1":"def split_data(all_features,all_classes):\n    from sklearn.model_selection import train_test_split\n    import pandas as pd\n\n\n    # Split features in 75%,25% proportion for the model\n    x_train, x_test, y_train, y_test =  train_test_split(all_features,all_classes, test_size = 0.25, random_state = 0)\n    return x_train, x_test, y_train, y_test\n\nx_train, x_test, y_train, y_test = split_data(all_features,all_classes)","25d9cbdf":"from sklearn import tree\n# from sklearn.ensemble import RandomForestClassifier\nimport graphviz\n\nDecTreeClf = tree.DecisionTreeClassifier(random_state=1)\nDecTreeClf = DecTreeClf.fit(x_train,y_train)","43a95d17":"import graphviz\n\n# Save tree as dot file\ndot_data = tree.export_graphviz(DecTreeClf, out_file=None) \ngraph = graphviz.Source(dot_data)  \ngraph ","f7680854":"\n# Call the prediction function\ny_pred = DecTreeClf.predict(x_test)\n\n# Display the confusion matrix to check result (accuracy and so on)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n","7198a12f":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn import svm\n\n# Create K-Fold scores\nscores = cross_val_score(DecTreeClf, all_features, all_classes, cv=10)\n\n# Print the accuracy for each fold:\nprint(\"***SCORES :*** \")\nprint(scores)\nprint(\"*************\\n\")\n\n# And the mean accuracy of all 5 folds:\nprint(\"***MEAN:***\")\nprint(scores.mean())\nprint(\"*************\\n\")","14a04fc9":"from sklearn.ensemble import RandomForestClassifier\n\n# Create the random Forest Classifier and the corresponding K-Fold score\nRandForestClf = RandomForestClassifier()\nscores = cross_val_score(RandForestClf, all_features, all_classes, cv=10)\n\n# Print the accuracy for each fold:\n\nprint(\"***SCORES :*** \")\nprint(scores)\nprint(\"*************\\n\")\n\n# And the mean accuracy of all 5 folds:\nprint(\"***MEAN:***\")\nprint(scores.mean())\nprint(\"*************\\n\")\n","fb8b85eb":"from sklearn import svm, datasets\n\n# Build the model and the corresponding predictions\nC = 1.0\nsvc = svm.SVC(kernel='linear', C=C,random_state=1).fit(x_train, y_train)\ny_pred = svc.predict(x_test)\n\n","4865aa37":"y_pred = svc.predict(x_test)\n\n#Create Confusion Matrix\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","8cc8b2bb":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\ndef my_confusion_matrix(y_test, y_pred):\n    \n    #Create Confusion Matrix\n\n    from sklearn.metrics import classification_report, confusion_matrix\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))\n\ndef my_accuracy(y_test,y_pred):\n    acc = accuracy_score(y_true=y_test, y_pred=y_pred)\n    print('Acc: {:.4f}'.format(acc))","89123447":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef KNN_loop(x_train,y_train,x_test,MAX):\n    \n    # Loop MAX times\n    for i in range(MAX):\n        # Create the corresponding KNN Classifier \n        KNeighClf = KNeighborsClassifier(n_neighbors=i+1)\n        \n        # Fit and predict the model\n        KNN = KNeighClf.fit(x_train, y_train)\n        y_pred = KNN.predict(x_test)\n        \n        # Display the results\n        # my_confusion_matrix(y_test, y_pred)\n        print (\"Neighbors = \" + str(i+1))\n        my_accuracy(y_test,y_pred)\n\nKNN_loop(x_train,y_train,x_test,50)","e50c74fb":"# KNN with K=14 seems the best fit\n# Run it again with full results this time\nKNeighClf = KNeighborsClassifier(14)\nKNN = KNeighClf.fit(x_train, y_train)\n\ny_pred = KNN.predict(x_test)\n\nmy_confusion_matrix(y_test, y_pred)\nmy_accuracy(y_test,y_pred)","28c9b802":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import MinMaxScaler\n\n#Define Classifier\nMNomNBClf = MultinomialNB()\n\n# define min max scaler\nscaler = MinMaxScaler()\n# transform data\nscaled_train = scaler.fit_transform(x_train)\nscaled_test = scaler.fit_transform(x_test)\n\n# Train using the Classifier and training data\nMNomNBClf.fit(scaled_train, y_train)\ny_pred = MNomNBClf.predict(scaled_test)\n\n#Print Results\nmy_confusion_matrix(y_test, y_pred)\nmy_accuracy(y_test,y_pred)\n\n# print(scaled)\n# print(x_train)","dbf7fd15":"from sklearn import svm, datasets\n#USE SVM RBF\n# Build the model and the corresponding predictions\nC = 1.0\nsvc = svm.SVC(kernel='rbf', C=C).fit(x_train, y_train)\ny_pred = svc.predict(x_test)\n\n# Print Results\nmy_confusion_matrix(y_test, y_pred)\nmy_accuracy(y_test,y_pred)","d802923b":"from sklearn import svm, datasets\n#USE SVM Sigmoid\n# Build the model and the corresponding predictions\nC = 1.0\nsvc = svm.SVC(kernel='sigmoid', C=C).fit(x_train, y_train)\ny_pred = svc.predict(x_test)\n\n# Print Results\nmy_confusion_matrix(y_test, y_pred)\nmy_accuracy(y_test,y_pred)","29fdf7d1":"from sklearn import svm, datasets\n#USE SVM Poly\n# Build the model and the corresponding predictions\nC = 1.0\nsvc = svm.SVC(kernel='poly', C=C).fit(x_train, y_train)\ny_pred = svc.predict(x_test)\n\n# Print Results\nmy_confusion_matrix(y_test, y_pred)\nmy_accuracy(y_test,y_pred)","842737b0":"import statsmodels.api as sm\n\n# This is a simple fit using SM.ols\n# use summary for results\nest = sm.OLS(y_train, scaled_train).fit()\nest.summary()","9829d04e":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\ncv_scores = cross_val_score(clf, all_features, all_classes, cv=10)\ncv_scores.mean()","459c4fdf":"# The data will need to be rescaled for the Neural Network!\n# Create the scaler, change type to float32 and apply the scaler to the features\nscaler = MinMaxScaler()\nall_features.astype('float32')\nall_features = scaler.fit_transform(all_features)\n\n# As the labels are just 0 or 1 (for benignant or malignant tumors) there is no need to rescale them\nall_classes.astype('float32')\nall_classes = all_classes.reshape(-1,1)","bb014590":"from tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.model_selection import cross_val_score\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\n\n\n# This section will contain the hyperparameters\n# hyper_batch_size=1 # in case we want to try the SGD method\nhyper_epochs=100\nhyper_drop_out=0.25\nhyper_learning_rate = 0.025\nhyper_optimizer = keras.optimizers.Adam(learning_rate=hyper_learning_rate)\nhyper_verbose = 0\n\n#### This Function creates the topography of the model\ndef create_model():\n    model = Sequential()\n    \n    model.add(Dense(12,input_dim=4, kernel_initializer='normal', activation='relu'))\n    model.add(Dropout(hyper_drop_out))\n    \n    model.add(Dense(6, kernel_initializer='normal', activation='relu'))\n    model.add(Dropout(hyper_drop_out))\n    \n    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n    model.add(Dropout(hyper_drop_out))\n    \n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    opt = keras.optimizers.Adam(learning_rate=hyper_learning_rate)    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n\nestimator = KerasClassifier(build_fn=create_model,\n#                             batch_size=hyper_batch_size,\n                            epochs=hyper_epochs,\n                            verbose=hyper_verbose)\n\n\n\n# Now we can use scikit_learn's cross_val_score to evaluate this model identically to the others\ncv_scores = cross_val_score(estimator, all_features, all_classes, cv=10)\ncv_scores.mean()","34f3e578":"# Let's rebuild our data entirely from scratch\n# Much better for this since we'll be using one-hot-encoding for the first time\n# And it has been a long while since we've done this\n\n# Import section\nimport numpy as np \nimport pandas as pd\n# importing one hot encoder from sklearn \nfrom sklearn.preprocessing import OneHotEncoder \n\n\n# Let's start with fresh data\nfeature_names = [\"Age\",\"Shape\",\"Margin\",\"Density\"]\nall_features = masses[feature_names].values\nall_classes = masses['Severity'].values\n\n\n# The data will need to be rescaled for the Neural Network!\n# Create the scaler, change type to float32 and apply the scaler to the features\nscaler = MinMaxScaler()\nall_features.astype('float32')\nall_features = scaler.fit_transform(all_features)\n\n# As the labels are just 0 or 1 (for benignant or malignant tumors) there is no need to rescale them\nall_classes.astype('float32')\nall_classes = all_classes.reshape(-1,1)\n\n\n\n# creating one hot encoder object by default \n# We'll one hot encode Shape, Margin and Density\nOH_columns = [\"Shape\",\"Margin\",\"Density\"]\n\n# Create the one hot encoder object\nonehotencoder = OneHotEncoder() \n\nOH_all_features = np.delete(all_features, np.s_[:1], axis=1) \nage_feature = np.delete(all_features,np.s_[1:],axis=1)\n\n# print(type(OH_all_features))\n# print(type(age_feature))\n# print(OH_all_features.shape)\n\n\nOH_all_features = OneHotEncoder().fit_transform(OH_all_features).toarray()\nall_features = np.append(age_feature, OH_all_features, axis=1)\nprint(all_features.shape)\n\n","0b5b96fb":"from tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.model_selection import cross_val_score\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\n\n\n# This section will contain the hyperparameters\n# hyper_batch_size=1 # in case we want to try the SGD method\nhyper_epochs=50\nhyper_drop_out=0.25\nhyper_learning_rate = 0.025\nhyper_optimizer = keras.optimizers.Adam(learning_rate=hyper_learning_rate)\nhyper_verbose = 0\n\n#### This Function creates the topography of the model\ndef create_model_OHEC():\n    model2 = Sequential()\n    model2.add(Dense(12,input_dim=14, kernel_initializer='normal', activation='relu'))\n    model2.add(Dropout(hyper_drop_out))\n    model2.add(Dense(6, kernel_initializer='normal', activation='relu'))\n    model2.add(Dropout(hyper_drop_out))\n    \n#     model2.add(Dense(16, kernel_initializer='normal', activation='relu'))\n#     model2.add(Dropout(hyper_drop_out))\n#     model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n#     model.add(Dropout(hyper_drop_out))\n    model2.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    opt = keras.optimizers.Adam(learning_rate=hyper_learning_rate)    \n    model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model2\n\n\n\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n\n#Wrap our Keras model in an estimator compatible with scikit_learn\nestimator_OHEC = KerasClassifier(build_fn=create_model_OHEC,\n#                             batch_size=hyper_batch_size,\n                            epochs=hyper_epochs,\n                            verbose=hyper_verbose)\n\n\n\n# Now we can use scikit_learn's cross_val_score to evaluate this model identically to the others\ncv_scores = cross_val_score(estimator_OHEC, all_features, all_classes, cv=10)\ncv_scores.mean()","d93e1443":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1)\ncv_score = cross_val_score(xgb,all_features,all_classes,cv=5)\ncv_score.mean()","a5103b8b":"# Now, let's try this with Voting Classifier using sklearn methods\nfrom sklearn.ensemble import VotingClassifier\n\n\nvoting_clf = VotingClassifier(estimators = [('RandForestClf',RandForestClf),('svc',svc),('xgb',xgb) ], voting = 'soft') \nvoting_clf.fit(all_features,all_classes)","4aa9f27c":"cv = cross_val_score(voting_clf,all_features,all_classes,cv=5)\nprint(cv)\nprint(cv.mean())","3c8a712b":"voting_clf.fit(X_train_scaled,y_train)\ny_hat_base_vc = voting_clf.predict(X_test_scaled).astype(int)","d4f7398d":"Choosing K is tricky, so we can't discard KNN until we've tried different values of K. Write a for loop to run KNN with K values ranging from 1 to 50 and see if K makes a substantial difference. Make a note of the best performance you could get out of KNN.","0f28b931":"Now try a RandomForestClassifier instead. Does it perform better?","b7ea5ae5":"## Decision Trees\n\nBefore moving to K-Fold cross validation and random forests, start by creating a single train\/test split of our data. Set aside 75% for training, and 25% for testing.","29770087":"There are quite a few missing values in the data set. Before we just drop every row that's missing data, let's make sure we don't bias our data in doing so. Does there appear to be any sort of correlation to what sort of data has missing fields? If there were, we'd have to try and go back and fill that data in.","4779c8d2":"# Neural Network\n\nNow let's try it, classic neural network style :)","6e3fad47":"Evaluate whether the data needs cleaning; your model is only as good as the data it's given. Hint: use describe() on the dataframe.","f5ebffe6":"Measure the accuracy of the resulting decision tree model using your test data.","f87f4c21":"# Final Project\n\n## TO DO\nSort out the mess and put proper Comments on the notebook! The mess was due to the fact that, at the time, I had not yet taken back the habits of properly putting comments to make it more readable for others. \n\nI will update the notebook shortly in order to do that. Sorry for the mess!!\n\n\n## Acknowledgement and thank you:\n\nAcknowledgements and Thank you.\n\nThank you to the following people and organizations:\n\nTo google for the colab, playground, machine learning courses they provide for free (not just the crash course).\n\nTo Kaggle for providing these learning tools and environements as well as opportunities to practice\n\nAnd most of all a big thank you to \"Uncle Frank\". I followed his courses on skillshare and this guy is really the \"Batman\" of teaching Machine learning, especially to total newbies like me and the following notebook was created thanks to the \"Deep Neural Network\" course on Skillshare.\n\nSpeaking of which, this notebook was created using the \"skeleton\" Frank Kane provided in his courses. You can find it on sundog-education at this adress\nhttp:\/\/media.sundog-soft.com\/ml\/MLCourse.zip .\n\nand the related course here:\nhttps:\/\/www.skillshare.com\/classes\/Deep-Learning-and-Neural-Networks-with-Python\/45606211\/projects?via=logged-in-home-your-classes&autoPlay=1\n\nMany thanks for helping a newbie along Frank :)\n\n\nYou can find Frank Kane's courses here: 1) Skillshare : https:\/\/www.skillshare.com\/user\/fkane 2) Udemy : https:\/\/www.udemy.com\/user\/frank-kane-2\/ 3) Facebook : https:\/\/www.facebook.com\/groups\/142114343040195\/about\/ 4) Official sundog site : https:\/\/sundog-education.com\/\n\n\n\n## Predict whether a mammogram mass is benign or malignant\n\nWe'll be using the \"mammographic masses\" public dataset from the UCI repository (source: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Mammographic+Mass)\n\nThis data contains 961 instances of masses detected in mammograms, and contains the following attributes:\n\n\n   1. BI-RADS assessment: 1 to 5 (ordinal)  \n   2. Age: patient's age in years (integer)\n   3. Shape: mass shape: round=1 oval=2 lobular=3 irregular=4 (nominal)\n   4. Margin: mass margin: circumscribed=1 microlobulated=2 obscured=3 ill-defined=4 spiculated=5 (nominal)\n   5. Density: mass density high=1 iso=2 low=3 fat-containing=4 (ordinal)\n   6. Severity: benign=0 or malignant=1 (binominal)\n   \nBI-RADS is an assesment of how confident the severity classification is; it is not a \"predictive\" attribute and so we will discard it. The age, shape, margin, and density attributes are the features that we will build our model with, and \"severity\" is the classification we will attempt to predict based on those attributes.\n\nAlthough \"shape\" and \"margin\" are nominal data types, which sklearn typically doesn't deal with well, they are close enough to ordinal that we shouldn't just discard them. The \"shape\" for example is ordered increasingly from round to irregular.\n\nA lot of unnecessary anguish and surgery arises from false positives arising from mammogram results. If we can build a better way to interpret them through supervised machine learning, it could improve a lot of lives.\n\n## Your assignment\n\nApply several different supervised machine learning techniques to this data set, and see which one yields the highest accuracy as measured with K-Fold cross validation (K=10). Apply:\n\n* Decision tree\n* Random forest\n* KNN\n* Naive Bayes\n* SVM\n* And, as a bonus challenge, a neural network using Keras.\n\nThe data needs to be cleaned; many rows contain missing data, and there may be erroneous data identifiable as outliers as well.\n\nRemember some techniques such as SVM also require the input data to be normalized first.\n\nMany techniques also have \"hyperparameters\" that need to be tuned. Once you identify a promising approach, see if you can make it even better by tuning its hyperparameters.\n\nI was able to achieve over 80% accuracy - can you beat that?\n\nBelow I've set up an outline of a notebook for this project, with some guidance and hints. If you're up for a real challenge, try doing this project from scratch in a new, clean notebook!\n","c4aa4a1b":"Now create a DecisionTreeClassifier and fit it to your training data.","ce9eab13":"## KNN\nHow about K-Nearest-Neighbors? Hint: use neighbors.KNeighborsClassifier - it's a lot easier than implementing KNN from scratch like we did earlier in the course. Start with a K of 10. K is an example of a hyperparameter - a parameter on the model itself which may need to be tuned for best results on your particular data set.","758ab26c":"## Naive Bayes\n\nNow try naive_bayes.MultinomialNB. How does its accuracy stack up? Hint: you'll need to use MinMaxScaler to get the features in the range MultinomialNB requires.","728160ca":"Make sure you use the optional parmaters in read_csv to convert missing data (indicated by a ?) into NaN, and to add the appropriate column names (BI_RADS, age, shape, margin, density, and severity):","2fb55a23":"## Personal Functions Section\nThese are the functions that will be used in the coming code below\nLet's Start with accuracy and confusion matrices function\n\nlet's start with the \"general\" function. \nList:\n\n- Functions for accuracy and confusion matrices\n- ","c8689b95":"## Revisiting SVM\n\nsvm.SVC may perform differently with different kernels. The choice of kernel is an example of a \"hyperparamter.\" Try the rbf, sigmoid, and poly kernels and see what the best-performing kernel is. Do we have a new winner?","5a32884d":"Now instead of a single train\/test split, use K-Fold cross validation to get a better measure of your model's accuracy (K=10). Hint: use model_selection.cross_val_score","ece60450":"Next you'll need to convert the Pandas dataframes into numpy arrays that can be used by scikit_learn. Create an array that extracts only the feature data we want to work with (age, shape, margin, and density) and another array that contains the classes (severity). You'll also need an array of the feature name labels.","21afc852":"If the missing data seems randomly distributed, go ahead and drop rows with missing data. Hint: use dropna().","e6cba24a":"666\n\n## Functions for Model generation\nThe following will contain all the functions we will use to instantiate and create the models prior to running them\nNote the presence of the hyperparameters Section at the beginning-> it will be used as a convenience to have all of those in one single place to modify","d6abf9b3":"### Acknowledgement: Thanks to Ken Jee for pointing  skleaern Voting Classifiers out in his \"titanic for newbies\" notebook and youtube video\n(I did not saw it before oops!)\n\nhttps:\/\/www.kaggle.com\/kenjee\/titanic-project-example\nhttps:\/\/www.youtube.com\/watch?v=I3FBJdiExcg\n\nAlso, using XBG as per his \"titanic for newbies\" notebook","6172b503":"## Let's begin: prepare your data\n\nStart by importing the mammographic_masses.data.txt file into a Pandas dataframe (hint: use read_csv) and take a look at it.","2beab0db":"# Let's try neural network again but with One hot encoding this time\n\nWe'll one hot encore all the \"ordinal\" columns => i.e: Shape, margin and density","786fba48":"## Do we have a winner?\n\nWhich model, and which choice of hyperparameters, performed the best? Feel free to share your results!\n\nKNN Neighbors with K=14 is the winner here with Neural Network being a close second.\nOne hot encoding the ordinal values for the neural network did not had the intended effect and, so far, di slightly worse than without.","5624d3b3":"Some of our models require the input data to be normalized, so go ahead and normalize the attribute data. Hint: use preprocessing.StandardScaler().","e51c1220":"## Logistic Regression\n\nWe've tried all these fancy techniques, but fundamentally this is just a binary classification problem. Try Logisitic Regression, which is a simple way to tackling this sort of thing.\n\n\nNeural Networks\u00b6\nAs a bonus challenge, let's see if an artificial neural network can do even better. You can use Keras to set up a neural network with 1 binary output neuron and see how it performs. Don't be afraid to run a large number of epochs to train the model if necessary.","65c5d529":"### Acknowledgement: thanks to the good soul in the kaggle notebook here https:\/\/www.kaggle.com\/hamelg\/python-for-data-29-decision-trees\nIt showed me how to display the decision tree in a simple manner in Kaggle (I do still have problems with some librarie on kaggle)\n(I will need to learn how to change the param so it can better display on screen, but it will suffice for the moment :)\n\nDisplay the resulting decision tree.","f4200632":"## SVM\n\nNext try using svm.SVC with a linear kernel. How does it compare to the decision tree?","938db850":"# Build the Neural Network with One hot encoded ordinal features\n"}}