{"cell_type":{"dd11da80":"code","026e5099":"code","eb653773":"code","b2469173":"code","7c260656":"code","7f887d30":"code","a824ee24":"code","4f51b830":"code","537250d8":"code","2dad692a":"code","61d96644":"code","d6f2237f":"code","78516045":"code","0833357a":"code","ed8855aa":"code","f9d7c0e5":"code","f654bd22":"code","25bcdfa6":"code","8b122c94":"code","57e82203":"code","d826aaef":"code","7cd008eb":"code","93a966cf":"code","5a224929":"code","3cea0fcf":"code","67d8136e":"code","7356a2c5":"code","5c016dc3":"code","be956fd1":"code","41997383":"code","5b1873a2":"code","f757d7c2":"code","c57c6d8a":"code","cbc5156c":"code","9a9f1622":"code","b7eeb1f5":"markdown","61542ee1":"markdown","bc55b222":"markdown","20cfd150":"markdown","3cd944f6":"markdown","a934fd2d":"markdown","91699e9d":"markdown","71cb5ec1":"markdown","529e73c9":"markdown","f95d3f91":"markdown","4b7aae85":"markdown","d8b0b876":"markdown","830e7bb1":"markdown","2293f076":"markdown","df169ae2":"markdown","db7886c6":"markdown","9cad9033":"markdown","22c032a3":"markdown","b79aeade":"markdown","4b3df38a":"markdown"},"source":{"dd11da80":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","026e5099":"import pandas as pd\nimport numpy as np\nfrom IPython.display import Image,display\nfrom IPython.core.display import HTML\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score ,silhouette_samples\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.cm as cm\nfrom mpl_toolkits.mplot3d import Axes3D","eb653773":"Books = pd.read_csv('..\/input\/book-recommendation-dataset\/Books.csv',low_memory=False)\nBooks.head()","b2469173":"Users = pd.read_csv('..\/input\/book-recommendation-dataset\/Users.csv',low_memory=False)\nUsers.Age.fillna(value = Users.Age.mean(),inplace=True)\nUsers.head()","7c260656":"Ratings = pd.read_csv('..\/input\/book-recommendation-dataset\/Ratings.csv')\nRatings.head()","7f887d30":"Ratings.info()","a824ee24":"Books.info()","4f51b830":"Users.info()","537250d8":"Books=Books[~Books['Year-Of-Publication'].isin(['DK Publishing Inc', 'Gallimard'])]\nBooks['Year-Of-Publication'] = pd.to_numeric(Books['Year-Of-Publication'])\nBooks.shape","2dad692a":"sns.distplot(Books['Year-Of-Publication'],bins=100);","61d96644":"Books['Year-Of-Publication'].describe()","d6f2237f":"Books = Books[(Books['Year-Of-Publication']>=1950) & (Books['Year-Of-Publication']<=2016)]\nsns.distplot(Books['Year-Of-Publication'])","78516045":"sns.distplot(Users.Age)","0833357a":"Users.Age.describe()","ed8855aa":"Users = Users[(Users.Age>=15) & (Users.Age<=100)]\nsns.distplot(Users.Age)","f9d7c0e5":"print(\"shape before cleaning:\",Ratings.shape)\nRatings = Ratings[Ratings['ISBN'].isin(list(Books['ISBN'].unique()))]\nRatings = Ratings[Ratings['User-ID'].isin(list(Users['User-ID'].unique()))]\nprint(\"shape after cleaning:\",Ratings.shape)","f654bd22":"# Taking the mean of rating given by each user\nUser_rating_mean = Ratings.groupby('User-ID')['Book-Rating'].mean()\nuser_rating = Ratings.set_index('User-ID')\nuser_rating['mean_rating'] = User_rating_mean\nuser_rating.reset_index(inplace=True)\n# Keeping the books in which users \"likes\" the book\nuser_rating = user_rating[user_rating['Book-Rating'] > user_rating['mean_rating']]\n# Initializing a dummy variable for future use\nuser_rating['is_fav'] = 1\nprint(user_rating.shape)\nuser_rating.head()","25bcdfa6":"# Keeping the users who like more than 10 books and less than 100 books \nval = user_rating['User-ID'].value_counts()\nlist_to_keep = list(val[(val>10) & (val<100)].index)\nuser_rating = user_rating[user_rating['User-ID'].isin(list_to_keep)]\nuser_rating.shape","8b122c94":"user_rating.describe()","57e82203":"%%time\ndf = pd.pivot_table(user_rating,index='User-ID',columns='ISBN',values='is_fav')\ndf.fillna(value=0,inplace=True)\nprint(df.shape)\ndf.head(10)","d826aaef":"df.shape #(no. of users to be clustered,no. of books that they like from)","7cd008eb":"%%time\npca = PCA(n_components=3)\npca.fit(df)\npca_fit = pca.transform(df)","93a966cf":"pca_fit = pd.DataFrame(pca_fit,index=df.index)\npca_fit","5a224929":"Kmm = KMeans(n_clusters=3)\nplt.rcParams['figure.figsize'] = (16, 9)\nclusters = Kmm.fit_predict(pca_fit)\ncmhot = plt.get_cmap('brg')\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(pca_fit[0], pca_fit[2], pca_fit[1],c=clusters,cmap=cmhot)\nplt.title('Data points in 3D PCA axis', fontsize=20)\nplt.show()","3cea0fcf":"TSS = []\nfor i in range(2,26):\n    km = KMeans(n_clusters=i,random_state=0)\n    km.fit(pca_fit)\n    TSS.append(km.inertia_)\nplt.plot(range(2,26),TSS,'-')","67d8136e":"for n in [3,4,5,6,7,8]:\n    ax1 = plt.figure().gca()\n    ax1.set_xlim([-0.1, 1])\n    ax1.set_ylim([0, len(pca_fit) + (n + 1) * 10])\n    km = KMeans(n_clusters=n,random_state=0)\n    clusters = km.fit_predict(pca_fit)\n    silhouette_avg = silhouette_score(pca_fit, clusters)\n    print(\"For n_clusters =\", n,\n          \"The average silhouette_score is :\", silhouette_avg)\n    silhouette_values = silhouette_samples(pca_fit, clusters)\n    y_start = 10\n    for i in range(n):\n        ith_cluster = np.sort(silhouette_values[clusters==i])\n        cluster_size = ith_cluster.shape[0]\n        y_end = y_start + cluster_size \n        ax1.fill_betweenx(np.arange(y_start, y_end),\n                          0, ith_cluster)\n        ax1.text(-0.05, y_start + 0.5 * cluster_size, str(i))\n        y_start = y_end + 10\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n    ax1.set_yticks([])\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n),\n                 fontsize=14, fontweight='bold')\nplt.show()","7356a2c5":"Kmeans_final = KMeans(n_clusters=4,random_state=0).fit(pca_fit)\ndf['cluster'] = Kmeans_final.labels_\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(pca_fit[0], pca_fit[2], pca_fit[1],c=df['cluster'],cmap=cmhot)\nplt.title('Data points in 3D PCA axis', fontsize=20)\nplt.show()\n# Gettings the books for each cluster\ncl1_books = df[df.cluster == 0].mean()\ncl2_books = df[df.cluster == 1].mean()\ncl3_books = df[df.cluster == 2].mean()\ncl4_books = df[df.cluster == 3].mean()\n# Getting the users for each cluster\ncl1_users = df[df.cluster == 0].index\ncl2_users = df[df.cluster == 1].index\ncl3_users = df[df.cluster == 2].index\ncl4_users = df[df.cluster == 3].index","5c016dc3":"def cluster_books_des(Ser):\n    bks = pd.DataFrame(Ser).merge(Books,left_index=True,right_on='ISBN',how='left')\n    bks.rename(columns={0:'avg_score'},inplace=True)\n    bks.sort_values(by='avg_score',ascending=False,inplace=True)\n    print('Median Year:',int(bks['Year-Of-Publication'].median()))\n    print('\\nTop 5 Books\\n')\n    Top5_books = bks.index[:5]\n    for i,isbn in enumerate(Top5_books):\n        print(str(i+1)+'.',bks.loc[isbn]['Book-Title'])\n    Top5_authors = bks['Book-Author'].unique()[:5]\n    print('Top 5 Authors\\n')\n    for i,auth in enumerate(Top5_authors):\n        print(str(i+1)+'.',auth)\ncluster_books_des(cl1_books)","be956fd1":"def cluster_user_des(Ser):\n    cl_user = Users[Users['User-ID'].isin(list(Ser))]\n    print('Most Common Location:',cl_user['Location'].mode()[0])\n    print('\\nMean Age:',cl_user['Age'].mean())\n    sns.distplot(cl_user['Age'])\n    plt.yticks([])\ncluster_user_des(cl1_users)","41997383":"cluster_books_des(cl2_books.drop('cluster'))","5b1873a2":"cluster_user_des(cl2_users)","f757d7c2":"cluster_books_des(cl3_books.drop('cluster'))","c57c6d8a":"cluster_user_des(cl3_users)","cbc5156c":"cluster_books_des(cl4_books.drop('cluster'))","9a9f1622":"cluster_user_des(cl4_users)","b7eeb1f5":"# Analysing Clusters\n## Cluster 1<br>\n## Books and Authors","61542ee1":"## Creating crosstab for each user and each book","bc55b222":"# Choosing value for K <br>\n## 1. The Elbow Method","20cfd150":"**Seems like K = 4 provides the best Clustering**","3cd944f6":"Looks like this cluster belongs to the Potterheads!!","a934fd2d":"# Reading Data","91699e9d":"### Using PCA for dimension reduction ","71cb5ec1":"## Cluster 4","529e73c9":"## Cluster 3","f95d3f91":"# Applying Clustering","4b7aae85":"Cleaning the Publication year from the books table\n","d8b0b876":"#  Collaborative Filtering using K-Means","830e7bb1":"Cleaning the invalid ISBN and invalid Users from the Ratings Table","2293f076":"**We didn't get a clear elbow so we have to try another method** <br>\n## 2. Silhouette Analysis","df169ae2":"## Definition of liking a book\nWe want to get the information whether or not a person likes that book for our filtering. <br>Criteria for that will be : If a person rates a book more than his\/her average rating then he\/she likes the book.","db7886c6":"#### Analyzing demographics of Books and Users","9cad9033":"## Users","22c032a3":"We can see some invalid ages in that!","b79aeade":"We can see some invalid years in that!","4b3df38a":"## Cluster 2"}}