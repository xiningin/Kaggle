{"cell_type":{"7ff09670":"code","ae057217":"code","55503921":"code","ed31083e":"code","7dbee4ef":"code","e9aa74be":"code","9d2a5efd":"code","3277958b":"code","e81c062a":"code","28910b8d":"code","afdc0281":"code","30d4e9c3":"code","99eb946d":"code","bf4a550b":"code","fc82c5ce":"code","4d98c0d7":"code","a51e4a47":"code","fdad0e30":"code","2d3dd729":"code","e0093e67":"code","2f4cbdec":"code","b3c9d4aa":"code","b4eda9e7":"code","cd0548fb":"code","8ec15027":"code","4b748fd6":"code","3b604ec3":"code","ddbce44a":"code","fc2d01d9":"code","77da74b8":"code","118b7613":"markdown","258d730e":"markdown","22c420f8":"markdown","93f34d39":"markdown","cdbbf847":"markdown"},"source":{"7ff09670":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n!pip install split-folders","ae057217":"import splitfolders\nsplitfolders.ratio('..\/input\/face-mask-detection\/Dataset', output=\".\/\", seed=1337, ratio=(.9, .1)) ","55503921":"train_data_generator = keras.preprocessing.image.ImageDataGenerator(\n    horizontal_flip = True, vertical_flip = True, zoom_range = 0.1,\n    shear_range = 0.1, width_shift_range = 0.2, height_shift_range = 0.2, rotation_range = 90,\n)\ntest_data_generator = keras.preprocessing.image.ImageDataGenerator()","ed31083e":"train_data = train_data_generator.flow_from_directory(\".\/train\", target_size = (128, 128), batch_size = 1, shuffle = True)\ntest_data = test_data_generator.flow_from_directory(\".\/val\", target_size = (128,128), batch_size = 1, shuffle = True)","7dbee4ef":"labels =train_data.class_indices\nlabels","e9aa74be":"def get_array_from_datagen(train_generator):\n  x=[]\n  y=[]\n  train_generator.reset()\n  for i in range(train_generator.__len__()):\n    a,b=train_generator.next()\n    x.append(a)\n    y.append(b)\n  x=np.array(x, dtype = np.float32)\n  y=np.array(y, dtype = np.float32)\n  print(x.shape)\n  print(y.shape)\n  return x,y\n\nX_train, y_train = get_array_from_datagen(train_data)\nX_test, y_test = get_array_from_datagen(test_data)","9d2a5efd":"X_train = X_train.reshape(-1, 128, 128, 3)\nX_test = X_test.reshape(-1, 128, 128, 3)\ny_train = y_train.reshape(-1, 3)\ny_test = y_test.reshape(-1, 3)","3277958b":"import gc\nfrom keras import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\nfrom keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n\ninput_shape = (128, 128, 3)\nclass_num = len(labels)\n\ndef cnn1():\n    return Sequential([\n        Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same',input_shape=input_shape),\n        BatchNormalization(),\n        MaxPooling2D(pool_size=(2, 2),strides=2),\n        Dropout(0.3),\n        Conv2D(64, kernel_size=(5, 5), activation='relu'),\n        MaxPooling2D(pool_size=(2, 2),strides=2),\n        Dropout(0.3),\n        Flatten(),\n        Dense(512, activation='relu'),\n        Dropout(0.3),\n        Dense(64, activation='relu'),\n        Dense(class_num, activation='softmax')\n    ])\n\ndef cnn2():\n    return Sequential([\n        Conv2D(64, kernel_size=(3, 3), activation='relu',input_shape=input_shape),\n        BatchNormalization(),\n        MaxPooling2D(pool_size=(2, 2),strides=2),\n        Dropout(0.3),\n        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n        Dropout(0.3),\n        MaxPooling2D(pool_size=(2, 2),strides=2),\n        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n        Dropout(0.3),\n        MaxPooling2D(pool_size=(2, 2),strides=2),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dense(class_num, activation='softmax')\n    ])\n\ndef cnn3():\n    return Sequential([\n        Conv2D(128, kernel_size=(3, 3), activation='relu',padding='same',input_shape=input_shape),\n        BatchNormalization(),\n        Conv2D(128, kernel_size=(3, 3), activation='relu',padding='same'),\n        Dropout(0.3),\n        MaxPooling2D(pool_size=(2, 2),strides=2),\n        Conv2D(128, kernel_size=(3, 3), activation='relu'),\n        Dropout(0.3),\n        Conv2D(128, kernel_size=(3, 3), activation='relu'),\n        Dropout(0.3),\n        MaxPooling2D(pool_size=(2, 2),strides=2),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dense(class_num, activation='softmax')\n    ])","e81c062a":"learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(\n    monitor = \"val_accuracy\",\n    factor = 0.5,\n    patience = 3,\n    verbose = 0,\n    min_lr = 0.00001\n)\nearly_stopping = keras.callbacks.EarlyStopping(patience=5, verbose=1)","28910b8d":"import matplotlib.pyplot as plt\ndef plot_history(history):\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","afdc0281":"model = cnn1()\nmodel.compile(optimizer=\"Adam\", loss=[\"categorical_crossentropy\"], metrics = [\"accuracy\"])\nhistory = model.fit(X_train, y_train ,epochs=25, validation_data=(X_test, y_test),\n                    verbose=2, callbacks = [learning_rate_reduction, early_stopping])\ngc.collect()","30d4e9c3":"plot_history(history)","99eb946d":"model = cnn2()\nmodel.compile(optimizer=\"Adam\", loss=[\"categorical_crossentropy\"], metrics = [\"accuracy\"])\nhistory = model.fit(X_train, y_train ,epochs=25, validation_data=(X_test, y_test),\n                    verbose=2, callbacks = [learning_rate_reduction, early_stopping])\ngc.collect()","bf4a550b":"plot_history(history)","fc82c5ce":"model = cnn3()\nmodel.compile(optimizer=\"Adam\", loss=[\"categorical_crossentropy\"], metrics = [\"accuracy\"])\nhistory = model.fit(X_train, y_train ,epochs=25, validation_data=(X_test, y_test),\n                    verbose=2, callbacks = [learning_rate_reduction, early_stopping])\ngc.collect()","4d98c0d7":"plot_history(history)","a51e4a47":"base_model = tf.keras.applications.EfficientNetB0(include_top=False)","fdad0e30":"model = keras.Sequential([\n    base_model,\n    keras.layers.GlobalAveragePooling2D(),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(3, activation='softmax')\n])\nmodel.compile(\n    optimizer=\"Adam\",\n    loss='categorical_crossentropy',\n    metrics=[\"accuracy\"]\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    epochs=25,\n    callbacks = [learning_rate_reduction, early_stopping]\n)","2d3dd729":"plot_history(history)","e0093e67":"base_model = tf.keras.applications.ResNet50(include_top=False)","2f4cbdec":"model = keras.Sequential([\n    base_model,\n    keras.layers.GlobalAveragePooling2D(),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(3, activation='softmax')\n])\nmodel.compile(\n    optimizer=\"Adam\",\n    loss='categorical_crossentropy',\n    metrics=[\"accuracy\"]\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    epochs=25,\n    callbacks = [learning_rate_reduction, early_stopping]\n)","b3c9d4aa":"plot_history(history)","b4eda9e7":"base_model = tf.keras.applications.EfficientNetB0(include_top=False)\nmodel = keras.Sequential([\n    base_model,\n    keras.layers.GlobalAveragePooling2D(),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(3, activation='softmax')\n])\nmodel.compile(\n    optimizer=\"Adam\",\n    loss='categorical_crossentropy',\n    metrics=[\"accuracy\"]\n)\nmodel.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    epochs=14,\n    callbacks = [learning_rate_reduction],\n    verbose = 0\n)","cd0548fb":"from sklearn.metrics import classification_report\nprint(classification_report(y_test.argmax(axis = 1), model.predict(X_test).argmax(axis = 1)))","8ec15027":"import os\nfrom shutil import copyfile\nos.mkdir(\".\/test_data\")\nos.mkdir(\".\/test_data\/with_mask\")\nos.mkdir(\".\/test_data\/mask_weared_incorrect\")\nos.mkdir(\".\/test_data\/without_mask\")","4b748fd6":"paths = []\nnames = []\nk = 0\npath = \".\/val\/mask_weared_incorrect\"\nfor i in os.listdir(path):\n    paths.append(path + \"\/\" + i)\n    names.append(\"mask_weared_incorrect\")\n    copyfile(path + \"\/\" + i, f\"test_data\/mask_weared_incorrect\/{i}\")\n    k += 1\n    if k == 8:\n        break\nk = 0\npath = \".\/val\/with_mask\"\nfor i in os.listdir(path):\n    paths.append(path + \"\/\" + i)\n    names.append(\"with_mask\")\n    copyfile(path + \"\/\" + i, f\"test_data\/with_mask\/{i}\")\n    k += 1\n    if k == 8:\n        break\npath = \".\/val\/without_mask\"\nk = 0\nfor i in os.listdir(path):\n    paths.append(path + \"\/\" + i)\n    names.append(\"without_mask\")\n    copyfile(path + \"\/\" + i,  f\"test_data\/without_mask\/{i}\")\n    k += 1\n    if k == 9:\n        break","3b604ec3":"test_data_2 = test_data_generator.flow_from_directory(\".\/test_data\", target_size = (128,128), batch_size = 1, shuffle = False)","ddbce44a":"print(labels)\nlabels_2 = {}\nfor k,v in labels.items():\n    labels_2[v] = k\nlabels_2","fc2d01d9":"predictions = model.predict(test_data_2).argmax(axis = 1)\npred_label = []\nfor i in predictions:\n    pred_label.append(labels_2[i])\npred_label","77da74b8":"import os\nfig, axes = plt.subplots(nrows=5,\n                         ncols=5,\n                         figsize=(20, 20),\n                        subplot_kw={'xticks': [], 'yticks': []})\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(paths[i]))\n    ax.set_title(f\"PREDICTION:{pred_label[i]}\")\n    ax.set_xlabel(f\"DATA : {names[i]}\")\n#plt.tight_layout()\nplt.show()","118b7613":"Okay let's look some predictions on validation dataset","258d730e":"Accuricies :\n\nCNN ->  0.8911\n\nResNet50 -> 0.9833\n\nEfficientNet -> 0.9933","22c420f8":"First we will split images to train\/test data. And we don't want to apply augmentation to test data. Because of that we will manually split data and will give to ImageDataGenerator","93f34d39":"ImageDataGenerator does not really assign values to an array, it just hold pointers. Because of that every learning step CPU perform reading operations. This very slows learning speed. We will store data in numpy array type.","cdbbf847":"Best Model is EfficientNet, we will check recall, precision and f1-score values"}}