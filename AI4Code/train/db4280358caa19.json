{"cell_type":{"875656dc":"code","ddb6478f":"code","b75afce2":"code","a2ae4f80":"code","395dd3a4":"code","2eb62128":"code","e388b324":"code","62ef266d":"code","4a20e32f":"code","defdcad4":"code","530ae279":"code","e06ad706":"code","86814258":"code","ebe561df":"code","8570be16":"code","79004636":"code","5375332d":"code","435ef786":"code","fd47510c":"code","33d3c9c9":"code","56c91047":"code","6d81c0e9":"code","b92449d4":"code","cebb9317":"code","3fba4d86":"code","a9b4d60d":"code","748c2c79":"code","9e2a48a1":"code","6f4471b7":"code","6961570c":"code","e4ea4ae8":"code","295ff086":"code","6846a47f":"markdown","4858eeac":"markdown","df364d65":"markdown","0a077fb3":"markdown","ec854a93":"markdown","80ae4b2b":"markdown","c686e12d":"markdown","c5c91e19":"markdown"},"source":{"875656dc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow\nimport tensorflow.keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom kerastuner.tuners import RandomSearch","ddb6478f":"# Importing the data set\ndataset = pd.read_csv(\"\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv\")\ndataset.head()","b75afce2":"X = dataset.iloc[:,3:13]\ny = dataset.iloc[:,13]","a2ae4f80":"X.head()","395dd3a4":"y.head()","2eb62128":"dataset.isnull().sum()","e388b324":"geography= pd.get_dummies(X['Geography'],drop_first = True)\ngender= pd.get_dummies(X['Gender'],drop_first = True)","62ef266d":"geography,gender","4a20e32f":"## Concatenate the Data Frames\n\nX=pd.concat([X,geography,gender],axis=1)\n\n## Drop Unnecessary columns\nX=X.drop(['Geography','Gender'],axis=1)","defdcad4":"X","530ae279":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n","e06ad706":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","86814258":"X_train","ebe561df":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","8570be16":"classifier.summary()","79004636":"X_train.shape","5375332d":"# Fitting the ANN to the Training set\nmodel_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100)","435ef786":"model_history.history.keys()","fd47510c":"# list all data in history\n\nprint(model_history.history.keys())\n# summarize history for accuracy\nplt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","33d3c9c9":"\n# summarize history for loss\nplt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","56c91047":"# Part 3 - Making the predictions and evaluating the model\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)","6d81c0e9":"y_pred","b92449d4":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)","cebb9317":"cm","3fba4d86":"# Calculate the Accuracy\nfrom sklearn.metrics import accuracy_score\nscore=accuracy_score(y_pred,y_test)","a9b4d60d":"score","748c2c79":"X_train.shape","9e2a48a1":"def build_model(hp):\n    model = keras.Sequential()\n    for i in range(hp.Int('num_layers', 2, 30)):\n        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n                                            min_value=32,\n                                            max_value=600,\n                                            step=64),\n                               activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(\n        optimizer=keras.optimizers.Adam(\n            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n        loss='binary_crossentropy',\n        metrics=['accuracy'])\n    return model","6f4471b7":"\ntuner = RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5,\n    executions_per_trial=3,\n    directory='project1',\n    project_name='Churn')","6961570c":"tuner.search_space_summary()","e4ea4ae8":"tuner.search(X_train, y_train,\n             epochs=5,\n             validation_data=(X_test, y_test))","295ff086":"tuner.results_summary()\n","6846a47f":"## Initialising the ANN","4858eeac":"## Model creation","df364d65":"## Summary","0a077fb3":"## Keras Tuner","ec854a93":"## Dataset","80ae4b2b":"## Libraries","c686e12d":"## Analysis","c5c91e19":"## Conclusion\nFirst we created our own model and we got 85% accuracy. Our purpose was to get top 10 layers based on it's accuracy so we can use only best layers to train our model. The reason I trained manualy first model is just to show you the difference between simple model creation and by using keras tuner what we can achieve. The min value,max value,learning rate,step size is depends on you and it's experimental thing. So from the above hyperperameter tuning we can extand our score to 0.864 (However we can improve that as well by doing more experiments) and we can use that during actual model creation instead of wasting time to train and check again and again.\n\n## Please upvote this notebook if you find it useful.It really motivate to make new notebook."}}