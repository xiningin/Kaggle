{"cell_type":{"4199a3a7":"code","589f3032":"code","8ef4135f":"code","9bfd37cb":"code","7d4793d7":"code","bd0695aa":"code","fcb042d7":"code","a92c4cf7":"code","7e358c61":"code","1522e399":"code","adb7a23a":"code","91099440":"code","eb56c404":"code","b233e069":"code","ce36ccc6":"code","b24f20ec":"code","e0ac8494":"code","d946668a":"code","3fc54693":"code","65db0aec":"code","1b2a379d":"code","bea41d82":"code","aaaefd68":"code","d5ad8720":"code","6eb7a88c":"code","325ad90d":"code","09170bb9":"code","d8320073":"code","b6ad33a7":"code","8a03dfac":"code","be6b8f3f":"code","e64eaea2":"code","31b636cd":"code","54a076fb":"code","b37a246b":"code","8c0619fb":"code","68ff73ac":"code","787d31d8":"code","d288c99f":"code","defb41ad":"code","86d571e6":"code","dd1db0f0":"code","194bc1fd":"code","c74f7b61":"code","cb6aef26":"code","a2b0a2f0":"code","87d980df":"markdown","32b184a3":"markdown","a6c84317":"markdown","4af38d77":"markdown","93f0d2c3":"markdown","3a6b97d6":"markdown","8a8e3018":"markdown","b4cb83ce":"markdown","e3886aea":"markdown","2fc86dfd":"markdown"},"source":{"4199a3a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","589f3032":"import matplotlib.pyplot as plt\nimport seaborn as sns","8ef4135f":"plt.figure(figsize=(20,10))\nimage = plt.imread('..\/input\/svmachines\/Supportvectormachines.jpg')\nplt.imshow(image)","9bfd37cb":"plt.figure(figsize=(20,10))\nimage = plt.imread('..\/input\/usegaarea\/categor.jpg')\nplt.imshow(image)","7d4793d7":"df=pd.read_csv(\"..\/input\/breast-cancer\/cell_samples.csv\")\ndf.head()","bd0695aa":"\nax = df[df['Class'] == 4][0:50].plot(kind='scatter', x='Clump', y='UnifSize', color='DarkBlue', label='malignant');\ndf[df['Class'] == 2][0:50].plot(kind='scatter', x='Clump', y='UnifSize', color='Yellow', label='benign', ax=ax,figsize=(20,10));\nplt.show() #Here is the visualization of our target column","fcb042d7":"df.isnull().sum()\n#There is no any missing data in our data set","a92c4cf7":"df.drop([\"ID\",\"BareNuc\"],axis=1,inplace=True)\n#We get rid of the columns that are not useful","7e358c61":"df.head()","1522e399":"df.corr()[\"Class\"].sort_values()\n#Here we can see the correlation between the target and the othewr columns","adb7a23a":"plt.figure(figsize=(20,10))\nsns.heatmap(df.corr(),cmap=\"viridis\",annot=True)\n#here is a heatmap of correlations, we see that UnifSize and UnifShape has a the highest postive correlation with the target column","91099440":"sns.set_style(\"whitegrid\")\nplt.figure(figsize=(20,10))\nsns.countplot(x=\"UnifSize\",data=df)","eb56c404":"sns.set_style(\"whitegrid\")\nplt.figure(figsize=(20,10))\nsns.countplot(x=\"UnifShape\",data=df)","b233e069":"from sklearn.model_selection import train_test_split","ce36ccc6":"X=df.drop(\"Class\",axis=1)#We assign all coluns as features except from the target columns\ny=df[\"Class\"]","b24f20ec":"X.shape","e0ac8494":"y.shape","d946668a":"X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.3) \n#Here we split %30 of our data as test set and %70 as train set","3fc54693":"print(X_train.shape)\nprint(\"***********\")\nprint(X_test.shape)\nprint(\"***********\")\nprint(y_train.shape)\nprint(\"***********\")\nprint(y_test.shape)","65db0aec":"from sklearn.svm import SVC","1b2a379d":"model=SVC()","bea41d82":"model.fit(X_train, y_train)","aaaefd68":"predictions=model.predict(X_test)","d5ad8720":"from sklearn.metrics import classification_report, confusion_matrix","6eb7a88c":"print(classification_report(y_test,predictions))\nprint(\"*****************************************\")\nprint(confusion_matrix(y_test,predictions))","325ad90d":"from sklearn.linear_model import LogisticRegression","09170bb9":"model2=LogisticRegression()","d8320073":"model2.fit(X_train, y_train)","b6ad33a7":"predictions2=model2.predict(X_test)","8a03dfac":"print(classification_report(y_test,predictions2))\nprint(\"*****************************************\")\nprint(confusion_matrix(y_test,predictions2))","be6b8f3f":"from sklearn.tree import DecisionTreeClassifier","e64eaea2":"model3= DecisionTreeClassifier()","31b636cd":"model3.fit(X_train,y_train)","54a076fb":"predictions3=model3.predict(X_test)","b37a246b":"print(classification_report(y_test,predictions3))\nprint(\"*****************************************\")\nprint(confusion_matrix(y_test,predictions3))","8c0619fb":"from sklearn.neighbors import KNeighborsClassifier","68ff73ac":"error_rate=list()\n#here we iterate meny different k values and plot their error rates \n#and discover which one is better than others and has the lowest error rate\nfor i in range(1,40):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    prediction_i=knn.predict(X_test)\n    error_rate.append(np.mean(prediction_i != y_test))","787d31d8":"# Now we will plot the prediction error rates of different k values\nplt.figure(figsize=(15,10))\nplt.plot(range(1,40),error_rate, color=\"blue\", linestyle=\"--\",marker=\"o\",markerfacecolor=\"red\",markersize=10)\nplt.title(\"Error Rate vs K Value\")\nplt.xlabel=\"K Value\"\nplt.ylabel(\"Error Rate\")","d288c99f":"knn=KNeighborsClassifier(n_neighbors=5)","defb41ad":"knn.fit(X_train,y_train)","86d571e6":"predictions4=knn.predict(X_test)","dd1db0f0":"print(classification_report(y_test,predictions4))\nprint(\"*****************************************\")\nprint(confusion_matrix(y_test,predictions4))","194bc1fd":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()","c74f7b61":"rfc.fit(X_train, y_train)","cb6aef26":"predictions5=rfc.predict(X_test)","a2b0a2f0":"print(classification_report(y_test,predictions5))\nprint(\"*****************************************\")\nprint(confusion_matrix(y_test,predictions5))","87d980df":"<font color=\"blue\">\n    We have almost the same results with Logistic regression in this dataset.","32b184a3":"<font color=\"blue\">\n    We have almost the same results with Random Forest Classifier in this dataset.","a6c84317":"# Support Vector Machines:","4af38d77":"<font color=\"blue\">\n    We have almost the same results with Decision trees in this dataset.","93f0d2c3":"From the figure above , we see that k=3,5,9,11,13,15-15 gives the least error. We can choose any of them.","3a6b97d6":"<font color=\"blue\">\n    Here we can compare these results with other classification ML models as follows:","8a8e3018":"<font color=\"blue\">\n    Now our data is ready for the algorithm.","b4cb83ce":"<font color=\"blue\">\n*They are supervised machine learning models with associated algorithms that analyze data and recognize patterns used for classification and regression analysis\n\n*This notebook will focus for classification\n\n*SVM training algorithm builds a model that assigns new examples or at least test data points into one category or the other making it a non-probabilistic binary linear classifier\n\n*The separate categories are divided by a clear gap as wide as possible and new examples are predicted to belong to a category based on which side of the gap they fall on\n\n*The algorith draw a separating hyperplane between the two classes. But this separating line should be the best option. The algorithm chooses a hyperplane that maximizes the margin between the classes which touch the end points of the both classes, which are also known as the support vectors.\n\n*This separation can also be extended to nonlinear data","e3886aea":"<font color=\"blue\">\n    Here we evaluate the performance of our model","2fc86dfd":"<font color=\"blue\">\n    We have almost the same results with K Nearest Neigbors in this dataset."}}