{"cell_type":{"8a565707":"code","4a531cef":"code","5eedfe53":"code","85b3d487":"code","10ab242b":"code","8aa59dae":"code","f60769e6":"code","1f090278":"code","f811759f":"code","baa354cf":"code","c0393e78":"code","13568de1":"code","bd69b672":"code","e5d6e073":"code","aa2c099b":"code","76f5e2aa":"code","e9dae14d":"code","d236ecae":"markdown","d59b9602":"markdown","4c78fcbf":"markdown","c3e5cb60":"markdown","24b3e821":"markdown","b0c98d49":"markdown","8f00f686":"markdown","1b27c8fb":"markdown","f1c865b2":"markdown","37646a04":"markdown","498f053b":"markdown","44c3712c":"markdown","45d94dd9":"markdown","563a314e":"markdown"},"source":{"8a565707":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = '.\/'\nDATA_DIR = \"..\/input\/ventilator-pressure-prediction\/\"\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","4a531cef":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    competition='ventilator'\n    apex=True\n    print_freq=1000\n    num_workers=4\n    model_name='rnn'\n    scheduler='CosineAnnealingWarmRestarts' # ['linear', 'cosine', 'ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n    batch_scheduler=False\n    #num_warmup_steps=100 # ['linear', 'cosine']\n    #num_cycles=0.5 # 'cosine'\n    factor=0.995 # ReduceLROnPlateau\n    patience=7 # ReduceLROnPlateau\n    eps=1e-6 # ReduceLROnPlateau\n    T_max=50 # CosineAnnealingLR\n    T_0=50 # CosineAnnealingWarmRestarts\n    epochs=200\n    max_grad_norm=1000\n    gradient_accumulation_steps=1\n    hidden_size=512\n    lr=1e-3\n    min_lr=1e-5\n    weight_decay=1e-6\n    batch_size=256\n    n_fold=5\n    trn_fold=[0]\n    cate_seq_cols=[]\n    cont_seq_cols=['R', 'C', 'time_step', 'u_in', 'u_out']\n    train=True\n    inference=True\n    debug=False\n\nif CFG.debug:\n    CFG.epochs = 2\n    CFG.trn_fold=[0]\n    \n","5eedfe53":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport sys\nimport json\nimport math\nimport random\nfrom time import time\nfrom datetime import datetime\nfrom collections import Counter, defaultdict\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nfrom tqdm.auto import tqdm\nimport category_encoders as ce\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\nfrom torch.cuda.amp import GradScaler\nfrom torch.cuda.amp import autocast\n\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#if CFG.apex:\n#    from apex import amp\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","85b3d487":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_trues, y_preds):\n    score = mean_absolute_error(y_trues, y_preds)\n    return score\n\n\ndef init_logger(log_file=OUTPUT_DIR+'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything()\n\ndef decorate(s: str, decoration=None):\n    if decoration is None:\n        decoration = '\u2605' * 20\n\n    return ' '.join([decoration, str(s), decoration])\n\nclass Timer:\n    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' ', verbose=0):\n\n        if prefix: format_str = str(prefix) + sep + format_str\n        if suffix: format_str = format_str + sep + str(suffix)\n        self.format_str = format_str\n        self.logger = logger\n        self.start = None\n        self.end = None\n        self.verbose = verbose\n\n    @property\n    def duration(self):\n        if self.end is None:\n            return 0\n        return self.end - self.start\n\n    def __enter__(self):\n        self.start = time()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.end = time()\n        if self.verbose is None:\n            return\n        out_str = self.format_str.format(self.duration)\n        if self.logger:\n            self.logger.info(out_str)\n        else:\n            print(out_str)","10ab242b":"# ====================================================\n# Data Loading\n# ====================================================\ntrain = pd.read_csv(DATA_DIR + 'train.csv')\nif CFG.debug:\n    train = train[:80*5000]\ntest = pd.read_csv(DATA_DIR + 'test.csv')\nsub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n\ndisplay(train.head())\ndisplay(test.head())\ndisplay(sub.head())","8aa59dae":"class AbstractBaseBlock:\n    def fit(self, input_df: pd.DataFrame, y=None):\n        return self.transform(input_df)\n\n    def transform(self, input_df: pd.DataFrame) -> pd.DataFrame:\n        raise NotImplementedError()\n\n\nclass AddMultiplyingDividing(AbstractBaseBlock):\n    def transform(self, input_df):\n        input_df['area'] = input_df['time_step'] * input_df['u_in']\n        input_df['area'] = input_df.groupby('breath_id')['area'].cumsum()\n        input_df['cross'] = input_df['u_in']*input_df['u_out']\n        input_df['cross2'] = input_df['time_step']*input_df['u_out']\n        input_df['u_in_cumsum'] = (input_df['u_in']).groupby(input_df['breath_id']).cumsum()\n        input_df['one'] = 1\n        input_df['count'] = (input_df['one']).groupby(input_df['breath_id']).cumsum()\n        input_df['u_in_cummean'] = input_df['u_in_cumsum'] \/ input_df['count']\n        input_df = input_df.merge(\n            input_df[input_df[\"u_out\"]==0].groupby('breath_id')['u_in'].agg([\"mean\", \"std\", \"max\"]).add_prefix(\"u_out0_\").reset_index(),\n            on=\"breath_id\"\n        )\n        input_df = input_df.merge(\n            input_df[input_df[\"u_out\"]==1].groupby('breath_id')['u_in'].agg([\"mean\", \"std\", \"max\"]).add_prefix(\"u_out1_\").reset_index(),\n            on=\"breath_id\"\n        )\n\n        output_df = pd.DataFrame(\n            {\n                \"area\": input_df['area'],\n                #\"cross\": input_df['cross'],\n                #\"cross2\": input_df['cross2'],\n                \"u_in_cumsum\": input_df['u_in_cumsum'],\n                \"u_in_cummean\": input_df['u_in_cummean'],\n                \"u_out0_mean\": input_df['u_out0_mean'],\n                \"u_out0_max\": input_df['u_out0_max'],\n                \"u_out0_max\": input_df['u_out0_std'],\n                \"u_out1_mean\": input_df['u_out1_mean'],\n                \"u_out1_max\": input_df['u_out1_max'],\n                \"u_out1_max\": input_df['u_out1_std'],\n            }\n        )\n        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n        return output_df\n\n\nclass RCDummry(AbstractBaseBlock):\n    def transform(self, input_df):\n        input_df['R_dummy'] = input_df['R'].astype(str)\n        input_df['C_dummy'] = input_df['C'].astype(str)\n        #input_df['RC_dummy'] = input_df['R_dummy'] + input_df['C_dummy']\n        output_df = pd.get_dummies(input_df[[\"R_dummy\", \"C_dummy\"]])\n        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n        return output_df\n\n\nclass AddBreathTimeAndUInTime(AbstractBaseBlock):\n    def transform(self, input_df):\n        output_df = pd.DataFrame(\n            {\n                \"breath_time\": input_df['time_step'] - input_df['time_step'].shift(1),\n                \"u_in_time\": input_df['u_in'] - input_df['u_in'].shift(1)\n            }\n        )\n        output_df.loc[input_df['time_step'] == 0, 'breath_time'] = output_df['breath_time'].mean()\n        output_df.loc[input_df['time_step'] == 0, 'u_in_time'] = output_df['u_in_time'].mean()\n        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n        return output_df\n\nclass LagFeatures(AbstractBaseBlock):\n    def transform(self, input_df):\n        output_df = pd.DataFrame(\n            {\n                \"u_in_lag1\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(1).fillna(0),\n                \"u_in_lag2\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(2).fillna(0),\n                \"u_in_lag3\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(3).fillna(0),\n                \"u_in_lag4\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(4).fillna(0),\n                \"u_in_lag-1\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-1).fillna(0),\n                \"u_in_lag-2\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-2).fillna(0),\n                \"u_in_lag-3\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-3).fillna(0),\n                \"u_in_lag-4\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-4).fillna(0),\n                \"u_out_lag1\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(1).fillna(0),\n                \"u_out_lag2\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(2).fillna(0),\n                \"u_out_lag3\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(3).fillna(0),\n                \"u_out_lag4\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(4).fillna(0),\n                #\"u_out_lag-1\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-1).fillna(0),\n                #\"u_out_lag-2\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-2).fillna(0),\n                #\"u_out_lag-3\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-3).fillna(0),\n                #\"u_out_lag-4\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-4).fillna(0),\n            }\n        )\n        output_df[\"u_in_lag1_diff\"] = output_df[\"u_in_lag1\"] - input_df[\"u_in\"]\n        output_df[\"u_in_lag2_diff\"] = output_df[\"u_in_lag2\"] - input_df[\"u_in\"]\n        output_df[\"u_in_lag3_diff\"] = output_df[\"u_in_lag3\"] - input_df[\"u_in\"]\n        output_df[\"u_in_lag4_diff\"] = output_df[\"u_in_lag4\"] - input_df[\"u_in\"]\n\n        output_df[\"u_in_rolling_mean2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).mean()[\"u_in\"].reset_index(drop=True)\n        output_df[\"u_in_rolling_mean4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).mean()[\"u_in\"].reset_index(drop=True)\n        output_df[\"u_in_rolling_mean10\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(10).mean()[\"u_in\"].reset_index(drop=True)\n        if not CFG.debug:\n            output_df[\"u_in_rolling_max2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).max()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_max4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).max()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_max10\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(10).max()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_min2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).min()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_min4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).min()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_min10\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(10).min()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_std2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).std()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_std4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).std()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_std10\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(10).std()[\"u_in\"].reset_index(drop=True)\n        for col in output_df.columns:\n            output_df[col] = output_df[col].fillna(output_df[col].mean())\n        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n        return output_df\n","f60769e6":"feature_blocks = [\n    AddMultiplyingDividing(),\n    AddBreathTimeAndUInTime(),\n    RCDummry(),\n    LagFeatures()\n]","1f090278":"def run_blocks(input_df, blocks, y=None, test=False):\n    out_df = pd.DataFrame()\n\n    print(decorate('start run blocks...'))\n\n    with Timer(prefix='run test={}'.format(test)):\n        for block in feature_blocks:\n            with Timer(prefix='out_df shape: {} \\t- {}'.format(out_df.shape, str(block))):\n                if not test:\n                    out_i = block.fit(input_df.copy(), y=y)\n                else:\n                    out_i = block.transform(input_df.copy())\n\n            assert len(input_df) == len(out_i), block\n            name = block.__class__.__name__\n            out_df = pd.concat([out_df, out_i.add_suffix(f'@{name}')], axis=1)\n    print(f\"out_df shape: {out_df.shape}\")\n\n    return pd.concat([input_df, out_df], axis=1)\n\ntrain = run_blocks(train, blocks=feature_blocks)\ntest = run_blocks(test, blocks=feature_blocks, test=True)\nCFG.cont_seq_cols = list(set(CFG.cont_seq_cols))\ndisplay(train.head())\ndisplay(test.head())\n","f811759f":"train_col_order = [\"u_out\"] + train.columns.drop(\"u_out\").tolist()\ntest_col_order = [\"u_out\"] + test.columns.drop(\"u_out\").tolist()\ntrain = train[train_col_order]\ntest = test[test_col_order]\nscaler = RobustScaler()\nscaler_targets = [col for col in CFG.cont_seq_cols if col != \"u_out\"]\nprint(f\"Apply Standerd Scaler these columns: {scaler_targets}\")\nfor scaler_target in tqdm(scaler_targets):\n    scaler.fit(train.loc[:,[scaler_target]])\n    train.loc[:,[scaler_target]] = scaler.transform(train.loc[:,[scaler_target]])\n    test.loc[:,[scaler_target]] = scaler.transform(test.loc[:,[scaler_target]])\ndisplay(train.head())\ndisplay(test.head())\n","baa354cf":"print(set(train.drop([\"id\", \"breath_id\", \"pressure\"], axis=1).columns) - set(CFG.cont_seq_cols))\nprint(train.drop([\"id\", \"breath_id\", \"pressure\"], axis=1).shape)\nprint(len(CFG.cont_seq_cols))\n\nX = np.float32(train.drop([\"id\", \"breath_id\", \"pressure\"], axis=1)).reshape(-1, 80, len(CFG.cont_seq_cols))\ny = np.float32(train[\"pressure\"]).reshape(-1, 80, 1)\nX_test = np.float32(test.drop([\"id\", \"breath_id\"], axis=1)).reshape(-1, 80, len(CFG.cont_seq_cols))\n","c0393e78":"class L1Loss_masked(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, preds, y, u_out):\n\n        mask = 1 - u_out\n        mae = torch.abs(mask * (y - preds))\n        mae = torch.sum(mae) \/ torch.sum(mask)\n\n        return mae","13568de1":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.hidden_size = cfg.hidden_size\n        self.seq_emb = nn.Sequential(\n            nn.Linear(len(cfg.cont_seq_cols), self.hidden_size),\n            nn.LayerNorm(self.hidden_size),\n            nn.GELU(),\n            #nn.Dropout(0.1),\n        )\n        self.lstm1 = nn.LSTM(self.hidden_size, self.hidden_size\/\/2, dropout=0.1, batch_first=True, bidirectional=True)\n        self.lstm2 = nn.LSTM(self.hidden_size\/\/2 * 2, self.hidden_size\/\/4, dropout=0.1, batch_first=True, bidirectional=True)\n        self.lstm3 = nn.LSTM(self.hidden_size\/\/4 * 2, self.hidden_size\/\/8, dropout=0.1, batch_first=True, bidirectional=True)\n        self.head = nn.Sequential(\n            # nn.Linear(self.hidden_size\/\/8 * 2, self.hidden_size\/\/8 * 2),\n            nn.LayerNorm(self.hidden_size\/\/8 * 2),\n            nn.GELU(),\n            #nn.Dropout(0.),\n            nn.Linear(self.hidden_size\/\/8 * 2, 1),\n        )\n        for n, m in self.named_modules():\n            if isinstance(m, nn.LSTM):\n                print(f'init {m}')\n                for param in m.parameters():\n                    if len(param.shape) >= 2:\n                        nn.init.orthogonal_(param.data)\n                    else:\n                        nn.init.normal_(param.data)\n            elif isinstance(m, nn.GRU):\n                print(f\"init {m}\")\n                for param in m.parameters():\n                    if len(param.shape) >= 2:\n                        init.orthogonal_(param.data)\n                    else:\n                        init.normal_(param.data)\n\n    def forward(self, cont_seq_x):\n        bs = cont_seq_x.size(0)\n        seq_emb = self.seq_emb(cont_seq_x)\n        seq_emb, _ = self.lstm1(seq_emb)\n        seq_emb, _ = self.lstm2(seq_emb)\n        seq_emb, _ = self.lstm3(seq_emb)\n        output = self.head(seq_emb)#.view(bs, -1)\n        return output\nprint(CustomModel(CFG))","bd69b672":"# ====================================================\n# helper function\n# ====================================================\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\nscaler = GradScaler()\n\ndef train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    model.train()\n    losses = AverageMeter()\n    start = end = time()\n    for step, (inputs, y) in enumerate(train_loader):\n        inputs, y = inputs.to(device), y.to(device)\n        batch_size = inputs.size(0)\n        with autocast():\n            pred = model(inputs)\n            loss = criterion(pred, y, inputs[:,:,0].reshape(-1,80,1))\n        losses.update(loss.item(), batch_size)\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_steps\n        if CFG.apex:\n            scaler.scale(loss).backward()\n        else:\n            loss.backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            if CFG.apex:\n                scaler.step(optimizer)\n            else:\n                optimizer.step()\n            optimizer.zero_grad()\n            lr = 0\n            if CFG.batch_scheduler:\n                scheduler.step()\n                lr = scheduler.get_lr()[0]\n        if CFG.apex:\n            scaler.update()\n        end = time()\n    return losses.avg\n\n\ndef valid_fn(valid_loader, model, criterion, device):\n    model.eval()\n    preds = []\n    losses = AverageMeter()\n    start = end = time()\n    for step, (inputs, y) in enumerate(valid_loader):\n        inputs, y = inputs.to(device), y.to(device)\n        batch_size = inputs.size(0)\n        with torch.no_grad():\n            pred = model(inputs)\n        loss = criterion(pred, y, inputs[:,:,0].reshape(-1,80,1))\n        losses.update(loss.item(), batch_size)\n        preds.append(pred.view(-1).detach().cpu().numpy())\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_steps\n        end = time()\n    preds = np.concatenate(preds)\n    return losses.avg, preds\n\n\ndef inference_fn(test_loader, model, device):\n    model.eval()\n    model.to(device)\n    preds = []\n    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n    for step, (cont_seq_x) in tk0:\n        cont_seq_x = cont_seq_x.to(device)\n        with torch.no_grad():\n            pred = model(cont_seq_x)\n        preds.append(pred.view(-1).detach().cpu().numpy())\n    preds = np.concatenate(preds)\n    return preds","e5d6e073":"train[\"breath_id\"].unique()","aa2c099b":"# ====================================================\n# train loop\n# ====================================================\ndef train_loop(folds, fold, trn_idx, val_idx):\n\n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================\n    #trn_idx = folds[folds['fold'] != fold].index\n    #val_idx = folds[folds['fold'] == fold].index\n    \n    train_folds = X[trn_idx]\n    valid_folds = X[val_idx]\n    groups = train[\"breath_id\"].unique()[val_idx]\n    oof_folds = train[train[\"breath_id\"].isin(groups)].reset_index(drop=True)\n    y_train = y[trn_idx]\n    y_true = y[val_idx]\n\n    # train_dataset = TrainDataset(train_folds)\n    # valid_dataset = TrainDataset(valid_folds)\n    train_dataset = torch.utils.data.TensorDataset(\n        torch.from_numpy(train_folds),\n        torch.from_numpy(y_train)\n    )\n    valid_dataset = torch.utils.data.TensorDataset(\n        torch.from_numpy(valid_folds),\n        torch.from_numpy(y_true)\n    )\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=True,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=False,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    model = CustomModel(CFG)\n    model.to(device)\n\n    optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n    #optimizer = torch.optim.Adam(model.parameters(), lr=0.0008, eps=1e-08)\n    num_train_steps = int(len(train_folds) \/ CFG.batch_size * CFG.epochs)\n    \n    def get_scheduler(optimizer):\n        if CFG.scheduler=='linear':\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps\n            )\n        elif CFG.scheduler=='cosine':\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=CFG.num_cycles\n            )\n        elif CFG.scheduler=='ReduceLROnPlateau':\n            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n        elif CFG.scheduler=='CosineAnnealingLR':\n            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n        return scheduler\n\n    scheduler = get_scheduler(optimizer)\n\n    # ====================================================\n    # apex\n    # ====================================================\n    #if CFG.apex:\n    #    model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = L1Loss_masked()\n\n    best_score = np.inf\n\n    avg_losses = []\n    avg_val_losses = []\n    for epoch in range(CFG.epochs):\n\n        start_time = time()\n\n        # train\n        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n        #avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, None, device)\n        avg_losses.append(avg_loss)\n        \n        # eval\n        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n        avg_val_losses.append(avg_val_loss)\n        \n        if isinstance(scheduler, ReduceLROnPlateau):\n            scheduler.step(avg_val_loss)\n        elif isinstance(scheduler, CosineAnnealingLR):\n            scheduler.step()\n        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n            scheduler.step()\n\n        # scoring\n        score = avg_val_loss #get_score(y_true[non_expiratory_phase_val_idx], preds[non_expiratory_phase_val_idx])\n\n        elapsed = time() - start_time\n\n        best_notice = \"\"\n        if score < best_score:\n            best_notice = \"Best Score\"\n            best_score = score\n            # LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {score:.4f} Model')\n            torch.save({'model': model.state_dict(),\n                        'preds': preds},\n                        OUTPUT_DIR+f\"fold{fold}_best.pth\")\n    \n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s, lr: {optimizer.param_groups[0][\"lr\"]:.5f}, MAE Score: {score:.4f}, {best_notice}')\n\n    plt.figure(figsize=(14,6))\n    plt.plot(avg_losses, label=\"Train Loss\")\n    plt.plot(avg_val_losses, label=\"Train Loss\")\n    plt.title(f\"Fold {fold + 1} - Best score {best_score:.4f}\", size=18)\n    plt.show()\n\n    preds = torch.load(OUTPUT_DIR+f\"fold{fold}_best.pth\", map_location=torch.device('cpu'))['preds']\n    oof_folds['preds'] = preds.flatten()\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return oof_folds","76f5e2aa":"# ====================================================\n# main\n# ====================================================\ndef main():\n    \n    \"\"\"\n    Prepare: 1.train 2.test\n    \"\"\"\n    \n    def get_result(result_df):\n        preds = result_df['preds'].values\n        labels = result_df['pressure'].values\n        non_expiratory_phase_val_idx = result_df[result_df['u_out'] == 0].index # The expiratory phase is not scored\n        score = get_score(labels[non_expiratory_phase_val_idx], preds[non_expiratory_phase_val_idx])\n        LOGGER.info(f'Score (without expiratory phase): {score:<.4f}')\n    \n    if CFG.train:\n        # train \n        oof_df = pd.DataFrame()\n        kfold = KFold(n_splits=CFG.n_fold, random_state=42, shuffle=True)\n        for fold, (trn_idx, val_idx) in enumerate(kfold.split(X=X, y=y)):\n            if fold in CFG.trn_fold:\n                _oof_df = train_loop(X, fold, trn_idx, val_idx)\n                oof_df = pd.concat([oof_df, _oof_df])\n                LOGGER.info(f\"========== fold: {fold} result ==========\")\n                get_result(_oof_df)\n        # CV result\n        LOGGER.info(f\"========== CV ==========\")\n        get_result(oof_df)\n        # save result\n        oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)\n    for i, breath_id in enumerate(oof_df[\"breath_id\"].unique()):\n        oof_df[oof_df[\"breath_id\"]==breath_id].plot(x=\"time_step\", y=[\"preds\", \"pressure\", \"u_out\"], figsize=(16, 5))\n        plt.show()\n        if i == 10:\n            break\n    \n    if CFG.inference:\n        test_loader = torch.utils.data.DataLoader(X_test, batch_size=512, shuffle=False, pin_memory=True)\n        #test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size * 2, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)\n        for fold in CFG.trn_fold:\n            model = CustomModel(CFG)\n            path = OUTPUT_DIR+f\"fold{fold}_best.pth\"\n            state = torch.load(path, map_location=torch.device('cpu'))\n            model.load_state_dict(state['model'])\n            predictions = inference_fn(test_loader, model, device)\n            test[f'fold{fold}'] = predictions\n            del state, predictions; gc.collect()\n            torch.cuda.empty_cache()\n        # submission\n        test['pressure'] = test[[f'fold{fold}' for fold in CFG.trn_fold]].mean(1)\n        test[['id', 'pressure']+[f'fold{fold}' for fold in CFG.trn_fold]].to_csv(OUTPUT_DIR+'raw_submission.csv', index=False)\n        test[['id', 'pressure']].to_csv(OUTPUT_DIR+'submission.csv', index=False)\n","e9dae14d":"if __name__ == '__main__':\n    main()","d236ecae":"# data loading","d59b9602":"# Configuration","4c78fcbf":"# Loss","c3e5cb60":"# Train Loop","24b3e821":"# normalization","b0c98d49":"# Model","8f00f686":"# Can pytorch scores reach keras?\nI like [ynakama](https:\/\/www.kaggle.com\/yasufuminakama)'s notebook and often use it as a baseline.  \nI used his [baseline](https:\/\/www.kaggle.com\/yasufuminakama\/ventilator-pressure-lstm-starter) again.  \nHowever, the same features and close model architecture will not result in the same score as a keras-based model using tpu.  \nSome people in [this discussion](https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction\/discussion\/274176) have scored about 0.16x using Pytorch, including ynakama, so my abilities and ideas are lacking.  \nI'm going to share some of the ideas I've been working on for pytorch, so please give me some ideas if you don't mind.  \n\n## my experience (My best Score is Version 47)\n(There was a period of time when some of the experiments were not carefully managed, so the detailed hyperparameters are omitted.). \n- CosineAnnealingWarmRestarts scored the best, while ReduceLROnPlateau, CosineAnnealingLR, and scheduler scored the worst.  \n- I got a better score when I didn't set the scheduler than when I did set it poorly.  \n- I tried lstm layers from 1 to 5, and I got good scores with 3 to 5 layers.  \n- The features were copied from a good public notebook to keep up with the tpu keras model. \n- I took out and put in the RELU and LayerNorm layers, but the score didn't change much.  \n- The training data is reshape for each breath_id as in the other notebooks. This allows us to train in a batch processing manner, improving scores and reducing training time for me.  \n  \nIf you find this notebook useful, it would be great if you could upvote it along with his [notebook](https:\/\/www.kaggle.com\/yasufuminakama\/ventilator-pressure-lstm-starter) and these references.\n  \n  \n## references  \nhttps:\/\/www.kaggle.com\/yasufuminakama\/ventilator-pressure-lstm-starter\nhttps:\/\/www.kaggle.com\/mistag\/optuna-optimized-keras-base-model  \nhttps:\/\/www.kaggle.com\/tenffe\/finetune-of-tensorflow-bidirectional-lstm  \nhttps:\/\/www.kaggle.com\/kensit\/improvement-base-on-tensor-bidirect-lstm-0-173  \nhttps:\/\/www.kaggle.com\/hamzaghanmi\/tensorflow-bi-lstm-with-tpu  \nhttps:\/\/www.kaggle.com\/lukaszborecki\/ventilator-ts  ","1b27c8fb":"# reshape","f1c865b2":"# Utils","37646a04":"# Main","498f053b":"# End","44c3712c":"# helper function","45d94dd9":"# import","563a314e":"# create features"}}