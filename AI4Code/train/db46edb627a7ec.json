{"cell_type":{"afb87180":"code","ff3ccad5":"code","c3dddb13":"code","11c9c81a":"code","aa1d9774":"code","b44f63a6":"code","9c12f2a6":"code","ce09e3e2":"code","101a1358":"code","96705220":"code","8f6cac4a":"code","1dee8c08":"code","d92a5b6e":"code","0d3704bf":"code","b7689b9b":"code","cb451ee5":"code","e5b0f853":"code","81e68eb2":"code","f7c08325":"markdown","c06f57a2":"markdown","ab442fe2":"markdown","46a94169":"markdown","1203ea48":"markdown","da8a55b1":"markdown","bcffe116":"markdown","2d8a8baa":"markdown","af7f1230":"markdown","6f9206f4":"markdown","ac9287c2":"markdown","86ebb67a":"markdown","d803e913":"markdown","6ccf67a1":"markdown","435221eb":"markdown","3aac0bb4":"markdown","b40ed15f":"markdown","d38df511":"markdown"},"source":{"afb87180":"import os\nimport numpy as np\nfrom skimage.transform import rescale, resize\n\nimport tensorflow as tf\nfrom tqdm.keras import TqdmCallback\nprint(tf.__version__)\n\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","ff3ccad5":"## We'll always work with images in range [0, 1]\ndef get_noisy_img(img, sig=30):\n    \"\"\"Task 1: Removing white noise\"\"\"\n    sigma = sig \/ 255.\n    noise = np.random.normal(scale=sigma, size=img.shape)\n    img_noisy = np.clip(img + noise, 0, 1).astype(np.float32)\n    return img_noisy\n\ndef get_inpainted_img(img, mask_size=0.25):\n    \"\"\"Task 2: Inpaint rectangular zero mask\"\"\"\n    mask = np.ones_like(img)[:, :, :1]\n    sx, sy = int(mask_size * img.shape[0]), int(mask_size * img.shape[1])\n    x = np.random.randint(0, img.shape[0] - sx)\n    y = np.random.randint(0, img.shape[1] - sy)\n    mask[x:x + sx, y:y + sy] = 0\n    img_noisy = img * mask\n    return img_noisy, mask","c3dddb13":"from skimage import data\nfrom matplotlib import pyplot as plt \n\nimg = data.chelsea().astype(np.float32) \/ 255.\n\n_, axis = plt.subplots(1, 3, figsize=(16, 5))\naxis[0].imshow(img); axis[0].set_title(\"Input $x_0$\")\naxis[1].imshow(get_noisy_img(img)); axis[1].set_title(\"Denoising task\")\naxis[2].imshow(get_inpainted_img(img)[0]); axis[2].set_title(\"Inpainting task\")\nfor ax in axis:\n    ax.set_axis_off()\nplt.show()","11c9c81a":"def dip_workflow(x0,\n                 x_true, \n                 f, \n                 f_input_shape, \n                 z_std=0.1,\n                 loss_mask=None,\n                 num_iters=5000,\n                 init_lr=0.01,\n                 save_filepath=None):\n    \"\"\"Deep Image prior workflow\n    Args:\n        * x0: input image\n        * x_true: Ground-truth image, only used for metrics comparison\n        * f: Neural network to use as a prior\n        * f_input_shape: Shape (excluding batch size) of inputs to f\n        * loss_mask: if not None, a binary mask with the same shape as x0,\n            which is applied to both x and x0 before applying the loss.\n            Used for instance in the inpainting task.\n        * num_iters: Number of training iterations\n        * init_lr: Initial learning rate for Adam optimizer\n        * If True, will save the best model in the given filepath\n    \"\"\"\n    # Sample input z\n    shape = (1,) + f_input_shape\n    z = tf.constant(np.random.uniform(size=shape).astype(np.float32) * z_std, name='net_input')\n\n    # Training Loss\n    def loss_fn(x_true, x):\n        del x_true\n        nonlocal x0, loss_mask\n        if loss_mask is None:\n            return tf.keras.losses.MSE(x, x0)\n        else:\n            return tf.keras.losses.MSE(x * loss_mask, x0 * loss_mask)\n        \n    # Output\/log information\n    # Diff between generated image and true ground-truth\n    # as mean squared error and psnr (peak signal to noise ratio)\n    def mse_to_gt(x_true, x):\n        return tf.reduce_mean(tf.losses.mean_squared_error(x, x_true))\n    \n    def psnr_to_gt(x_true, x, maxv=1.):\n        mse = tf.reduce_mean(tf.losses.mean_squared_error(x, x_true))\n        psnr_ = 10. * tf.math.log(maxv** 2 \/mse) \/ tf.math.log(10.)\n        return psnr_\n    \n    # Optimization\n    opt = tf.keras.optimizers.Adam(learning_rate=init_lr)\n    f.compile(optimizer=opt, loss=loss_fn, metrics=[mse_to_gt, psnr_to_gt])\n    # Saving best model\n    callbacks = ()\n    if save_filepath is not None:\n        callbacks = create_saving_callback(save_filepath)\n    \n    # Training\n    history = f.fit(z, \n                    x_true[None, ...], \n                    epochs=num_iters,\n                    steps_per_epoch=1, \n                    verbose=0, \n                    callbacks=callbacks+(TqdmCallback(verbose=1),))\n    \n    # Display results with gridspec\n    x = f.predict(z)[0]\n    fig = plt.figure(figsize=(10, 12), constrained_layout=True)\n    gs = fig.add_gridspec(3, 2)\n    axes = [fig.add_subplot(gs[0, :]),\n            fig.add_subplot(gs[1, 0]),\n            fig.add_subplot(gs[1, 1]),\n            fig.add_subplot(gs[2, 0]),\n            fig.add_subplot(gs[2, 1])]\n    for ax in axes[1:]:\n        ax.set_axis_off()\n        \n    for key in history.history.keys():\n        axes[0].plot(range(num_iters), history.history[key], label=key)\n    axes[0].set_yscale('log')\n    axes[0].legend()\n    axes[0].set_title(\"Training dynamics\")\n    axes[1].imshow(x0); axes[1].set_title('Input image')\n    axes[2].imshow(x_true); axes[2].set_title('Ground-truth')\n    axes[3].imshow(x); axes[3].set_title(f'Last output (PSNR = {psnr_to_gt(x_true, x):.2f})')\n    if save_filepath is not None and os.path.exists(save_filepath):\n        f.load_weights(save_filepath)\n        x_opt = f.predict(z)[0]\n        axes[4].imshow(x_opt); axes[4].set_axis_off()\n        axes[4].set_title(f'Best model output (PSNR = {psnr_to_gt(x_true, x):.2f})')\n    plt.show()\n    return x","aa1d9774":"class GaussianNoiseWithDecay(tf.keras.layers.GaussianNoise):\n    \n    def __init__(self, stddev, decayrate=0.99999, decaysteps=1, **kwargs):\n        super(GaussianNoiseWithDecay, self).__init__(stddev, **kwargs)\n        self.num_calls = 0\n        self.decayrate = decayrate\n        self.decaysteps = decaysteps\n        \n        \n    def call(self, inputs, training=None):\n        def noised():\n            self.num_calls += 1\n            stddev = self.stddev * self.decayrate ** (self.num_calls \/\/ self.decaysteps)\n            return inputs + tf.keras.backend.random_normal(\n                shape=tf.python.ops.array_ops.shape(inputs),\n                mean=0.,\n                stddev=stddev,\n                dtype=inputs.dtype)\n\n        return tf.keras.backend.in_train_phase(noised, inputs, training=training)","b44f63a6":"def create_saving_callback(filepath):\n    return (tf.keras.callbacks.ModelCheckpoint(\n        filepath=filepath, \n        monitor='loss',\n        verbose=0, \n        save_best_only=True,\n        mode='min'),)","9c12f2a6":"def deep_image_prior(input_shape,\n                     noise_reg=None,\n                     layers=(128, 128, 128, 128, 128),\n                     kernel_size_down=3,\n                     kernel_size_up=3,\n                     skip=(0, 4, 4, 4, 4)):\n    def norm_and_active(x):\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.LeakyReLU()(x)\n        return x\n    \n    model = tf.keras.models.Sequential(name=\"Deep Image Prior\")\n    inputs = tf.keras.Input(shape=input_shape)\n    \n    ## Inputs\n    x = inputs\n    if noise_reg is not None:\n        x = GaussianNoiseWithDecay(**noise_reg)(x)\n    \n    ## Downsampling layers\n    down_layers = []\n    for i, (num_filters, do_skip) in enumerate(zip(layers, skip)):\n        if do_skip > 0:\n            down_layers.append(norm_and_active(tf.keras.layers.Conv2D(\n                filters=do_skip, kernel_size=1, strides=1, name=f\"conv_skip_depth_{i}\")(x)))\n        for j, strides in enumerate([2, 1]):\n            x = norm_and_active(tf.keras.layers.Conv2D(\n                num_filters, kernel_size_down, strides=strides, padding='same',\n                name=f\"conv_down_{j + 1}_depth_{i}\")(x))\n        \n    ## Upsampling\n    for i, (num_filters, do_skip) in enumerate(zip(layers[::-1], skip[::-1])):\n        x = tf.keras.layers.UpSampling2D(interpolation='bilinear', name=f\"upsample_depth_{i}\")(x)\n        if do_skip:\n            x = tf.keras.layers.Concatenate(axis=-1)([x, down_layers.pop()])\n        for j, kernel_size in enumerate([kernel_size_up, 1]):\n            x = norm_and_active(tf.keras.layers.Conv2D(\n                num_filters, kernel_size, strides=1, padding='same',\n                name=f\"conv_up_{j + 1}_depth_{i}\")(x))\n            \n    ## Last conv\n    x = tf.keras.layers.Conv2D(filters=3, kernel_size=1, strides=1, name=\"conv_out\")(x)\n    x = tf.keras.layers.Activation('sigmoid')(x)\n    return tf.keras.Model(inputs=inputs, outputs=x, name=\"deep_image_prior\")\n\n\ndef display_dip_model(input_shape=(256, 256, 3)):\n    model = deep_image_prior(input_shape)\n    model.build(input_shape)\n    print(model.summary())\ndisplay_dip_model()","ce09e3e2":"def deep_decoder(input_shape,\n                 noise_reg=None,\n                 layers=(128, 128, 128, 128, 128),\n                 kernel_size=1,\n                 bn_before_act=False,\n                 upsample_first=True):\n    \"\"\"Deep Decoder.\n       Takes as inputs a 4D Tensor (batch, width, height, channels)\"\"\"\n    ## Configure\n    model = tf.keras.models.Sequential(name=\"Deep Decoder\")\n    inputs = tf.keras.Input(shape=input_shape)\n    \n    ## Inputs\n    x = inputs\n    if noise_reg is not None:\n        x = GaussianNoiseWithDecay(**noise_reg)(x)\n                                               \n        \n    ## Deep Decoder\n    for i, num_filters in enumerate(layers):       \n        # Upsample (first)\n        if upsample_first and i != 0:\n            x = tf.keras.layers.UpSampling2D(interpolation='bilinear')(x)\n\n        # Conv     \n        if kernel_size > 1:\n            x = tf.keras.layers.ZeroPadding2D(int((kernel_size - 1) \/ 2))(x)\n        x = tf.keras.layers.Conv2D(num_filters, kernel_size, strides=1, padding='valid', use_bias=False)(x)\n\n        # Upsample (second)\n        if not upsample_first and i != len(num_channels_up) - 1:\n            x = tf.keras.layers.UpSampling2D(interpolation='bilinear')(x)\n\n        # Batch Norm + activation\n        if bn_before_act: \n            x = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n        x = tf.keras.layers.ReLU()(x)\n        if not bn_before_act: \n            x = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n                \n    # Final convolution\n    x = tf.keras.layers.Conv2D(filters=3, kernel_size=1, strides=1, padding='valid', use_bias=False)(x)\n    x = tf.keras.layers.Activation('sigmoid')(x)\n    return tf.keras.Model(inputs=inputs, outputs=x, name=\"deep_decoder\")\n\n\ndef display_deep_decoder_model(input_shape=(8, 8, 3)):\n    model = deep_decoder(input_shape)\n    model.build(input_shape)\n    print(model.summary())\ndisplay_deep_decoder_model()","101a1358":"x_true = resize(data.chelsea().astype(np.float32) \/ 255., (512, 512))\nx0 = get_noisy_img(x_true)\n_, axes = plt.subplots(1, 2, figsize=(10, 4))\naxes[0].imshow(x0); axes[0].set_axis_off()\naxes[0].set_title(\"Input noisy image\")\naxes[1].imshow(x_true); axes[1].set_axis_off()\naxes[1].set_title(\"Ground-truth image\")\nplt.show()","96705220":"input_shape = x0.shape\nnoise_reg = {'stddev': 1.\/ 30., 'decayrate': 1.0, 'decaysteps': 100000}\n\nmodel = deep_image_prior(input_shape, noise_reg=noise_reg)\nx = dip_workflow(x0, x_true, model, input_shape, num_iters=2000, save_filepath='best_dip_denoising.hdf5')","8f6cac4a":"layers = (128,) * 6\nupsample_factor = 2**(len(layers) - 1)\ninput_shape = (x0.shape[0] \/\/ upsample_factor, \n               x0.shape[1] \/\/ upsample_factor,\n               x0.shape[2])\nnoise_reg = None\nmodel = deep_decoder(input_shape, layers=layers, noise_reg=noise_reg)\n\nx = dip_workflow(x0, x_true, model, input_shape, num_iters=2000, \n                 save_filepath='best_deepdecoder_denoising.hdf5')","1dee8c08":"!wget https:\/\/raw.githubusercontent.com\/ameroyer\/tf_deep_decoder\/master\/test_data\/mask.png\nfrom skimage import io\nmask = io.imread(\"mask.png\")\nmask = resize(mask, (512, 512))[:, :, None]\n\nx_true = resize(data.chelsea().astype(np.float32) \/ 255., (512, 512))\nx0 = x_true * mask\n_, axes = plt.subplots(1, 3, figsize=(12, 4))\naxes[0].imshow(x0); axes[0].set_axis_off()\naxes[0].set_title(\"Input deteriorated image\")\naxes[1].imshow(x_true); axes[1].set_axis_off()\naxes[1].set_title(\"Ground-truth image\")\naxes[2].imshow(mask[:, :, 0], cmap=\"gray_r\"); axes[2].set_axis_off()\naxes[2].set_title(\"Mask\")\nplt.show()","d92a5b6e":"input_shape = x0.shape\nnoise_reg = {'stddev': 1.\/ 30., 'decayrate': 1.0, 'decaysteps': 100000}\n\nmodel = deep_image_prior(input_shape, noise_reg=noise_reg)\nx = dip_workflow(x0, x_true, model, input_shape, num_iters=5000,\n                 loss_mask=mask, save_filepath='best_dip_textinpainting.hdf5')","0d3704bf":"layers = (128,) * 6\nupsample_factor = 2**(len(layers) - 1)\ninput_shape = (x0.shape[0] \/\/ upsample_factor, \n               x0.shape[1] \/\/ upsample_factor,\n               x0.shape[2])\nnoise_reg = None\nmodel = deep_decoder(input_shape, layers=layers, noise_reg=noise_reg)\n\nx = dip_workflow(x0, x_true, model, input_shape, num_iters=4000, \n                 loss_mask=mask, save_filepath='best_deepdecoder_textinpainting.hdf5')","b7689b9b":"x_true = resize(data.chelsea().astype(np.float32) \/ 255., (512, 512))\nx0, mask = get_inpainted_img(x_true, 0.2)\n\n_, axes = plt.subplots(1, 3, figsize=(12, 4))\naxes[0].imshow(x0); axes[0].set_axis_off()\naxes[0].set_title(\"Input noisy image\")\naxes[1].imshow(x_true); axes[1].set_axis_off()\naxes[1].set_title(\"Ground-truth image\")\naxes[2].imshow(mask[:, :, 0], cmap=\"gray_r\"); axes[2].set_axis_off()\naxes[2].set_title(\"Mask\")\nplt.show()","cb451ee5":"input_shape = x0.shape\nnoise_reg = None\n\nlayers = (16, 32, 64, 128, 128, 128)\nmodel = deep_image_prior(input_shape, layers=layers, skip=(0,) * len(layers),\n                         kernel_size_up=5, noise_reg=noise_reg)\n\nx = dip_workflow(x0, x_true, model, input_shape, num_iters=5000,\n                 loss_mask=mask, save_filepath='best_dip_holeinpainting.hdf5')","e5b0f853":"layers = (128,) * 6\nupsample_factor = 2**(len(layers) - 1)\ninput_shape = (x0.shape[0] \/\/ upsample_factor, \n               x0.shape[1] \/\/ upsample_factor,\n               x0.shape[2])\nnoise_reg = None\nmodel = deep_decoder(input_shape, layers=layers, noise_reg=noise_reg)\n\nx = dip_workflow(x0, x_true, model, input_shape, num_iters=8000, \n                 loss_mask=mask, save_filepath='best_deepdecoder_holeinpainting.hdf5')","81e68eb2":"x_true = resize(data.chelsea().astype(np.float32) \/ 255., (512, 512))\nx0 = x_true\n\nlayers = (128,) * 6\nupsample_factor = 2**(len(layers) - 1)\ninput_shape = (x_true.shape[0] \/\/ upsample_factor, \n               x_true.shape[1] \/\/ upsample_factor,\n               x_true.shape[2])\nnoise_reg = None\nmodel = deep_decoder(input_shape, layers=layers, noise_reg=noise_reg)\n\nnum_img_params = x_true.size\nnum_model_params = model.count_params() + input_shape[0] * input_shape[1] * input_shape[2]\nprint(f\"Original image: {num_img_params} parameters\")\nprint(f\"Deep Decoder (including input z): {num_model_params} parameters\")\nprint(f\"Compression rate: {num_img_params \/ num_model_params}\")\n\nx = dip_workflow(x0, x_true, model, input_shape, num_iters=2000, \n                 save_filepath='best_deepdecoder_compression.hdf5')","f7c08325":"### Deep Image Prior results","c06f57a2":"\nThis notebook discusses\/summarizes the [Deep Image Prior](https:\/\/ameroyer.github.io\/image%20analsys\/deep_image_prior\/) and a closely related follow-up work, [the deep decoder](https:\/\/arxiv.org\/abs\/1810.03982), in `Tensorflow 2`\/`keras`. This is also an updated and simplified version of a `tf v1` code repository I uploaded some time ago [on github](https:\/\/github.com\/ameroyer\/tf_deep_decoder).","ab442fe2":"## Neural networks as regularizers ?\n\nIntuitively, the regularizer $R$ is task-independent and should encourage a realistic, natural-looking generated image. For instance, the total variation norm (TV) tends to favor images with uniform regions. \n\nThe main idea of the paper is to use a neural network for the regularization term, rather than an handcrafted prior, with the following high-level idea:\n$$\n\\begin{align}\nR(x) &= 0\\ \\mbox{if}\\ \\exists \\theta\\ \\mbox{s.t.}\\ x = f_{\\theta}(z)\\\\\nR(x) &= + \\infty,\\ \\mbox{otherwise}\n\\end{align}\n$$\n\nWhat this regularizer says is that intuitively, our target $x^{\\ast}$ is the \"optimal\" output (with respect to loss function $E$) that can be generated by the function $f$ from some fixed input $z$.\n\n\n\n### Workflow\nThe workflow of the model can thus be summarized as follows\n\\begin{align}\n\\mbox{Randomly initialize a neural network $f_{\\theta}$ and an input $z$}\\\\\n\\mbox{Train parameters }\\theta^\\ast = \\arg\\min_{\\theta} E(f_{\\theta}(z),x_0) \\\\\n\\mbox{Output }x^\\ast = f_{\\theta^\\ast}(z)\n\\end{align}\n\n","46a94169":"### Motivation\nOne could wonder  why this is a good choice for a prior at all. In fact, $f$, being instantiated as a neural network, *could* be powerful enough that any image $x$ can be generated from some random $z$ for a certain choice of parameters $\u03b8$, which would make the prior useless.\n\nFirst this is of course very dependent on the chosen architecture.\nSecond this is also motivated by experiments presented in the original paper: The authors analyze their proposed architecture on a simple reconstruction task and observe that the training objective usually descends much faster on natural looking images rather than noisy inputs (random white noise, same image + random noise, or same image with pixels permuted). This suggest that the network converges faster to structured signal like natural images rather than random outputs. This motivates its use as a prior, under a constrained number of iterations.\n\n\n![Figure](https:\/\/ameroyer.github.io\/images\/posts\/dip_toyexp.png)\n\n**Figure:** Learning curves for the reconstruction task using: a natural image, the same plus i.i.d. noise, the same but randomly scrambled, and white noise. (*source:* original Deep Image Prior paper)","1203ea48":"### Deep Decoder results","da8a55b1":"# Deep Image Prior\n\n\nStandard inverse problem in Computer Vision (denoising, inpainting, super-resolution) can usually be phrased as a two loss terms minimization objective:\n\n\\begin{align}\nx^\\ast = \\arg\\min_x E(x, x_0) + R(x)\n\\end{align}\n\nwhere $x_0$ is the input image, $E$ is a task-specific term and $R$ is a regularizer. \nFor instance in the denoising case, $x_0$ would be the input noisy image, $E$ could be the L2-loss and a classical choice for $R$ is the total variation norm, TV.","bcffe116":"### Deep Decoder\n\nThe [Deep Decoder](https:\/\/arxiv.org\/abs\/1810.03982) is a follow-up work which proposes to use a much simpler, under-parametrized, architecture as a prior for these reverse tasks. In particular the architecture is non-convolutional (kernel size = 1). The deep decoder architecture combines standard blocks include linear combination of channels (convolutions ), ReLU, batch-normalization and upscaling. \n\nAs a result, it's often easier and faster to train than the Deep Image Prior. Because of its low number of parameters, the original paper also explore using the Deep Decoder as an image compression scheme (see last section of the notebook).","2d8a8baa":"# Experiments\n\n## Denoising","af7f1230":"### Deep Decoder results","6f9206f4":"## Inpainting (large hole)\n\nHere, the hole width and height are set to be 20% of the original image's width\/height.\n\n**Note:** This is by far the most challenging task (lack of context, results depends a lot on the inpainted region)","ac9287c2":"### Deep Image Prior results","86ebb67a":"# Network Architectures\n\n### Deep Image Prior\nThe architecture used in experiments is well detailled in the [supplemental material](https:\/\/box.skoltech.ru\/index.php\/s\/ib52BOoV58ztuPM#pdfviewer). The authors mainly experimented with \"UNet\" style network, i.e. encoder-decoder architecture with a bottleneck and shortcut\/skip connections across the encoder and decoded between feature maps of the same spatial dimensions.\n\nSome additional observations:\n  * Simpler models (less shortcut\/skip paths) are usually better priors\n  * Leaky ReLu activations\n  * For upsampling, bilinear upsampling +  conv 1x1 performed better than using strided transposed convolutions (see also [this article](https:\/\/distill.pub\/2016\/deconv-checkerboard\/) on the topic). But no impact on downsampling operations.\n  \n  \nUsing `keras`, we can implement the proposed default architecture as follows:","d803e913":"# Deep Decoder for Compression\n\nOne interesting property of deep decoders is that they have a **low count of parameters** (for instance, in the previous example, roughly 68,864 trainable parameters). This is much lower than the number of parameters in the Deep Image Prior (2M) but also much much lower than the number of pixels in the image ($512 \\times 512 \\times 3 = 7.8\\text{M}$).\n\nTherefore, the deep decoder can be used as a compression scheme by training the model with $x_0$ being the image to compress, and using the model itself as a compressed representation, from which the image can be generated. There is no guarantee that the model perfectly reconstructs the image, which makes it a **lossy compression** scheme. Finally, the compression rate is given directly by the number of parameters in the model (including the input vector $z$).","6ccf67a1":"### Deep Image Prior results","435221eb":"### Deep Decoder results","3aac0bb4":"\n**Note 2:** Another small optimization is to track the loss during training and return the image at the lowest loss point, rather than the last generated one.","b40ed15f":"\n**Note 1:** In the [supplemental material](https:\/\/box.skoltech.ru\/index.php\/s\/ib52BOoV58ztuPM#pdfviewer), the authors also mention they sometimes use \"noise-based\" regularization, which consists in adding some small random noise to the input $z$ at different training iterations, with a (optionally) decaying variance.\n","d38df511":"## Inpainting (small, e.g. text)"}}