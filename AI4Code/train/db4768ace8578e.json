{"cell_type":{"1b2beb92":"code","f3aa7ef0":"code","6740667f":"code","2e96b9bf":"code","9c9e9741":"code","f5afe769":"code","1cf33f9b":"code","c5df8512":"code","44728553":"code","a84d1d79":"code","0e158903":"code","c6610577":"code","318ead3f":"code","6c8e71fc":"code","fe6b5e96":"code","ca41d105":"code","79280363":"markdown","39639b54":"markdown","8131ade0":"markdown","ad8c1dc4":"markdown","e3ad7cd1":"markdown","3a9db98d":"markdown","c5dee5a2":"markdown"},"source":{"1b2beb92":"from IPython.display import HTML\nHTML('<center><iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/bn8rVBuIcFg?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","f3aa7ef0":"# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport gc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport matplotlib.pyplot as plt","6740667f":"submission0 = pd.read_csv('\/kaggle\/input\/m5-final-models\/submission_LSTM.csv') \nsubmission1 = pd.read_csv('\/kaggle\/input\/m5-final-models\/submission_XGBoost.csv') \nsubmission2 = pd.read_csv('\/kaggle\/input\/m5-final-models\/submission_LGBM.csv')\nsubmission3 = pd.read_csv('\/kaggle\/input\/m5-final-models\/submission_prophet.csv')\nsubmission4 = pd.read_csv('\/kaggle\/input\/m5-final-models\/submission_SARIMAX.csv')","2e96b9bf":"calendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nprices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nvalidation = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv') \nsample_sub = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')","9c9e9741":"def perfect_sub(): # returns a perfect submition just to be sure that we are validating on the right window of values\n    submission = OperateBaseModels(submission0,submission1, a=0, b=1)\n    diference = validation.merge(submission, how='right')\n    shift= 56\n    perfect_submission = diference[diference.columns[-shift:-shift+28]]\n\n\n    #\n    col = { 'd_'+str(1914+i):'F'+str(i+1) for i in range(28)}\n    perfect_submission = perfect_submission.rename(columns=col)\n    perfect_submission.insert(loc=0, column='id', value= submission0['id']) \n    perfect_submission = perfect_submission.fillna(0)\n    return perfect_submission\n\n\ndef OperateBaseModels(ModelA,ModelB, a,b):\n    submissionMerge = pd.merge(ModelA, ModelB, on='id', how='left', suffixes=('_x', '_y'))#.mean(level=0)\n    submissionMerge\n    submission = pd.DataFrame()\n    submission.insert(loc=0, column='id', value= ModelB['id']) \n\n    for j in range(28):\n        i =j+1\n        \n        submission.insert(loc=i, column='F'+str(i), value= ((submissionMerge[submissionMerge.columns[i]]*a)+(submissionMerge[submissionMerge.columns[i+28]])*b)\/(a+b)) \n     \n\n    return submission","f5afe769":"from typing import Union\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm_notebook as tqdm\n","1cf33f9b":"class WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight \/ lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score \/ scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            all_scores.append(lv_scores.sum())\n\n        return np.mean(all_scores)\ndf_train_full =  pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv\")\ndf_calendar = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/calendar.csv\")\ndf_prices = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sell_prices.csv\")\n\ndf_train = df_train_full.iloc[:, :-28]\ndf_valid = df_train_full.iloc[:, -28:]\nevaluator = WRMSSEEvaluator(df_train, df_valid, df_calendar, df_prices)\n\n\ndef WRMSSEE(sub): # Add a function to take the standard submission format \n    val_preds = sample_sub[['id']].merge(sub, on = 'id')  #Order values like the submission example\n    \n    val_preds.columns = ['id'] + list(df_valid.columns)  # Rename columns\n    valid_preds = val_preds.iloc[:30490, -28:] #Take just validation data from the submition dataframe\n    return evaluator.score(valid_preds)","c5df8512":"def function(solution):\n    solution = np.abs(solution )\n    submission = OperateBaseModels(submission0,submission1, a=solution[0], b=solution[1])\n    submission = OperateBaseModels(submission,submission2, a=1, b=solution[2]) \n    submission = OperateBaseModels(submission,submission3, a=1, b=solution[3]) \n    submission = OperateBaseModels(submission,submission4, a=1, b=solution[4])\n                                   \n                                   \n    #submission = OperateBaseModels(submission,submission2, a=1, b=solution[2]) #I just added this for fun to see how much it affects to have more models into consideration \n    #submission = OperateBaseModels(submission,submission3, a=1, b=solution[3]) #00\n    #submission = OperateBaseModels(submission,submission4, a=1, b=solution[4])#00\n    #submission = OperateBaseModels(submission,submission5, a=1, b=solution[5])#00\n    #submission = OperateBaseModels(submission,submission6, a=1, b=solution[3])\n    #submission = OperateBaseModels(submission,submission7, a=1, b=solution[4])\n    #submission = OperateBaseModels(submission,submission8, a=1, b=solution[8])#00\n    #submission = OperateBaseModels(submission,submission9, a=1, b=solution[5])\n    #submission = OperateBaseModels(submission,submission10, a=1, b=solution[10])#00\n\n    #Mean_error = np.mean(np.mean(np.abs(perfect_sub._get_numeric_data()-submission._get_numeric_data())))\n    error = WRMSSEE(submission) #+Mean_error\/2\n    return error ","44728553":"perfect_sub = perfect_sub()","a84d1d79":"from scipy.optimize import OptimizeResult\nfrom scipy.optimize import minimize\nhist = []\ndef custmin(fun, x0, args=(), maxfev=None, stepsize=0.1,  # Then we optimize the coeficients to minimize error\n        maxiter=500, callback=None, **options):\n    bestx = x0\n    besty = fun(x0)\n    funcalls = 1\n    niter = 0\n    improved = True\n    stop = False\n\n    while improved and not stop and niter < maxiter:\n        improved = False\n        niter += 1\n        print('Iteration number',niter,'WRMSSEE',besty, 'using ', str(np.abs(np.array(bestx)) ))\n        hist.append(besty)\n        for dim in range(np.size(x0)):\n            for s in [bestx[dim] - stepsize, bestx[dim] + stepsize]:\n                testx = np.copy(bestx)\n                testx[dim] = s\n                testy = fun(testx, *args)\n                funcalls += 1\n                if testy < besty:\n                    besty = testy\n                    bestx = testx\n                    improved = True\n            if callback is not None:\n                callback(bestx)\n            if maxfev is not None and funcalls >= maxfev:\n                stop = True\n                break\n\n    return OptimizeResult(fun=besty, x=bestx, nit=niter,\n                          nfev=funcalls, success=(niter > 1))\nx0 = np.random.rand(5)\nres = minimize(function, x0, method=custmin, options=dict(stepsize=0.05))","0e158903":"Plot_solution =np.abs(res.x)\n# [9.35946038e-04, 1.00239378e+01, 1.67410230e-02, 4.01652749e+00,   1.16401096e-02, 9.17120136e-01]\nsns.set()\nplt.plot(hist)\nplt.ylabel('WRMSSEE')\nplt.xlabel('Iteration')\n\nPlot_solution","c6610577":"plt.figure()\n\n\nax = sns.barplot( x=pd.DataFrame(Plot_solution).index,y=0, data=pd.DataFrame(Plot_solution));\nax.set(xlabel='Model', ylabel='Coeficients form weighted average ');\nax.set_xticklabels(['DNN Embedings','XGBoost','LGBM','SARIMAX','Prophet'], rotation=30);","318ead3f":"solution = Plot_solution\n\nsolution = np.abs(solution )\nsubmission = OperateBaseModels(submission0,submission1, a=solution[0], b=solution[1])\nsubmission = OperateBaseModels(submission,submission2, a=1, b=solution[2]) \nsubmission = OperateBaseModels(submission,submission3, a=1, b=solution[3]) \nsubmission = OperateBaseModels(submission,submission4, a=1, b=solution[4])\n\n\n#submission = OperateBaseModels(submission,submission2, a=1, b=solution[2]) #I just added this for fun to see how much it affects to have more models into consideration \n#submission = OperateBaseModels(submission,submission3, a=1, b=solution[3]) #00\n#submission = OperateBaseModels(submission,submission4, a=1, b=solution[4])#00\n#submission = OperateBaseModels(submission,submission5, a=1, b=solution[5])#00\n#submission = OperateBaseModels(submission,submission6, a=1, b=solution[3])\n#submission = OperateBaseModels(submission,submission7, a=1, b=solution[4])\n#submission = OperateBaseModels(submission,submission8, a=1, b=solution[8])#00\n#submission = OperateBaseModels(submission,submission9, a=1, b=solution[5])\n#submission = OperateBaseModels(submission,submission10, a=1, b=solution[10])#00","6c8e71fc":"WRMSSEE(submission)","fe6b5e96":"#error\n#perfect_sub\ndiference = validation.merge(submission,how='left')                                      \nerror = np.mean(np.mean(np.abs(perfect_sub._get_numeric_data()-submission._get_numeric_data())))","ca41d105":"submission.to_csv(\"submission.csv\", index=False)\nsubmission","79280363":"# Introduction","39639b54":"Welcome to the \"M5 Forecasting - Accuracy\" competition! In this competition, contestants are challenged to forecast future sales at Walmart based on heirarchical sales in the states of California, Texas, and Wisconsin. Forecasting sales, revenue, and stock prices is a classic application of machine learning in economics, and it is important because it allows investors to make guided decisions based on forecasts made by algorithms.\n\nIn this kernel, I will briefly explain the structure of dataset. Then, I will visualize the dataset using Matplotlib and Plotly. And finally, I will demonstrate how this problem can be approached with a variety of forecasting algorithms.","8131ade0":"### Import libraries","ad8c1dc4":"## Preparing the ground","e3ad7cd1":"To get started, here is an excellent video about how to approach time series forecasting:","3a9db98d":"# EDA\nNow, I will try to visualize the sales data and gain some insights from it.","c5dee5a2":"## The dataset\nThe dataset consists of five .csv files.\n\n* calendar.csv - Contains the dates on which products are sold. The dates are in a yyyy\/dd\/mm format.\n\n* sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913].\n\n* submission.csv - Demonstrates the correct format for submission to the competition.\n\n* sell_prices.csv - Contains information about the price of the products sold per store and date.\n\n* sales_train_evaluation.csv - Available one month before the competition deadline. It will include sales for [d_1 - d_1941].\n\nIn this competition, we need to forecast the sales for [d_1942 - d_1969]. These rows form the evaluation set. The rows [d_1914 - d_1941] form the validation set, and the remaining rows form the training set. Now, since we understand the dataset and know what to predict, let us visualize the dataset."}}