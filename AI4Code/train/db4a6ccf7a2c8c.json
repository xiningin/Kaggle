{"cell_type":{"a383ea28":"code","dd7c5454":"code","645d5876":"code","2a351423":"code","eed31155":"code","c6ff16e7":"code","a59a138f":"code","20ab7d8b":"code","2aea0eed":"code","b22f3e5f":"code","1978f818":"code","b6c8fac5":"code","35baca29":"code","4fcd64b3":"code","a6d986ab":"code","04938394":"code","a521f261":"code","1aa23c3b":"code","a5b64fea":"code","67490429":"code","23ebd8b1":"code","57ffbcaf":"code","b9fbf7b9":"code","f5050204":"code","1ada1cac":"code","1e56bce1":"code","bf424f1b":"code","46ad8428":"code","704ab479":"code","58a150e3":"code","69611b45":"code","8f171b91":"code","bbcea8b1":"code","8ae1d4d2":"code","1ce03dee":"code","a5fd5a59":"code","bc20ddd9":"code","9e9006c0":"code","a86a06c5":"code","fc276494":"code","62c8f509":"code","0e41b7e6":"code","1f81184c":"code","6a15f167":"code","2acbbaf6":"code","244c97db":"code","41dedf1b":"code","a957ba2d":"code","baa9f9ec":"code","b3c4e269":"code","e0ca20e1":"code","a28b157b":"code","69e99984":"code","d01e4d26":"code","8a6c23ae":"code","94242ec3":"markdown","891be172":"markdown","525cc4e0":"markdown","a14ee893":"markdown","c14be92c":"markdown","0d6661f8":"markdown","179ac04f":"markdown","7faac703":"markdown","bd148dab":"markdown","f97fa9c7":"markdown","99dd2320":"markdown","6b61c4fb":"markdown","b136fbf6":"markdown"},"source":{"a383ea28":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","dd7c5454":"# uploading CSV files \ntrain=pd.read_csv('\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv')\ntest=pd.read_csv('\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Test Data.csv')\nprediction=pd.read_csv('\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Sample Prediction Dataset.csv')","645d5876":"# checking all the columns\ntest.columns","2a351423":"# copying target for test to maintain index while deleting any rows during data cleaning or\ntest['Risk_Flag']=prediction['risk_flag']","eed31155":"train.head(5)","c6ff16e7":"# there is no missing values in the both train and test we can check it by following code\ntrain.isna().sum()","a59a138f":"# there is no missing values in the both train and test we can check it by following code\ntest.isna().sum()","20ab7d8b":"# drop id column for both test and train data\ntrain.drop('Id',axis=1,inplace=True)\ntest.drop(columns='ID',inplace=True)","2aea0eed":"# appending the data(train and test can be used separately in the model but will cause unneccessary codes separetly while \n# cleaning and exploring data. I used it so that data cleaning and visualization can be easy)\ndata=train.append(test)\ndata","b22f3e5f":"# removing duplicates\ndata.drop_duplicates(inplace=True)","1978f818":"# number of rows left\nprint(data.shape)","b6c8fac5":"# to check percentage of duplicate data i.e. 80% of data was duplicated\nprint((280000-53693)\/280000)","35baca29":"# checking number of data for Default and Loyal customer\nplt.hist(data['Risk_Flag'])","4fcd64b3":"# percentage of default_loan\ndata['Risk_Flag'].value_counts(normalize=True)","a6d986ab":"# checking pattern between Risk and Marital Status\ndata.groupby(['Married\/Single']).count()","04938394":"# checking pattern between State and risk\ndata.groupby(['STATE','Risk_Flag']).count().sort_values(by='Risk_Flag',ascending=False)","a521f261":"sns.set(rc={'figure.figsize':(10,7)})\nsns.countplot(data=data,y='STATE')","1aa23c3b":"# checking if there is any difference in data distribution for credit risk\nsns.catplot(data=data, kind=\"violin\", x=\"Risk_Flag\", y=\"Income\")","a5b64fea":"sns.catplot( data=data, kind='violin',x=\"Risk_Flag\", y=\"Age\")","67490429":"# Using label encoder to encode categorical value\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()","23ebd8b1":"# encoding Marital Status because we have seen it has a pattern with Risk\ndata[\"Married\/Single\"] = label_encoder.fit_transform(data[\"Married\/Single\"])","57ffbcaf":"# checking correlation and deleting features with low correlation\ndata.corr()","b9fbf7b9":"# since column 'CURRENT_JOB_YRS' has very low correlation with risk we are dropping it\ndata.drop(columns='CURRENT_JOB_YRS',inplace=True)","f5050204":"# deleting rest categorical features\ndata.drop(columns=['STATE','CITY','House_Ownership','Car_Ownership','Profession'], inplace=True)","1ada1cac":"# splitting data into feature (X) and target(y)\nX=data.iloc[:,0:-1]\ny=data['Risk_Flag']","1e56bce1":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)","bf424f1b":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()","46ad8428":"# normalising data using MinMax and creating DF\nX_train_sc=pd.DataFrame(scaler.fit_transform(X_train),columns=X_train.columns,index=X_train.index)\nX_test_sc=pd.DataFrame(scaler.transform(X_test),columns=X_test.columns,index=X_test.index)","704ab479":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\ndef dtree_grid_search(X,y,nfolds):\n    #create a dictionary of all values we want to test\n    param_grid = { 'criterion':['gini','entropy'],'max_depth': np.arange(3, 15),'min_samples_split':[5,10,30,50],'min_samples_leaf':[10,25,50]}\n    # decision tree model\n    dtree_model=DecisionTreeClassifier()\n    #use gridsearch to test all values\n    dtree_gscv = GridSearchCV(dtree_model, param_grid, cv=nfolds)\n    #fit model to data\n    dtree_gscv.fit(X, y)\n    return dtree_gscv.best_params_","58a150e3":"# finding best param\ndtree_grid_search(X_train_sc,y_train,5)\n","69611b45":"tree = DecisionTreeClassifier(\n    criterion='gini',\n    max_depth = 3 ,\n    min_samples_split=5, \n    min_samples_leaf=25\n)","8f171b91":"tree.fit(X_train_sc,y_train)","bbcea8b1":"from sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\naccuracy_score(y_train, tree.predict(X_train_sc))","8ae1d4d2":"accuracy_score(y_test, tree.predict(X_test_sc))","1ce03dee":"plot_confusion_matrix(tree,\n                      X_test_sc,\n                      y_test,\n                      values_format='d',\n                      display_labels=['loyal','default'])","a5fd5a59":"from imblearn.combine import SMOTETomek\nfrom collections import Counter\nsm=SMOTETomek(0.95)\nX_train_sm,y_train_sm=sm.fit_resample(X_train_sc,y_train)\nprint('number of classes before fit {}'.format(Counter(y_train)))\nprint('number of classes after fit {}'.format(Counter(y_train_sm)))","bc20ddd9":"# finding best param\ndtree_grid_search(X_train_sm,y_train_sm,5)","9e9006c0":"tree_2 = DecisionTreeClassifier(\n    criterion='entropy',\n    max_depth = 14 ,\n    min_samples_split=5, \n    min_samples_leaf=10\n)","a86a06c5":"tree_2.fit(X_train_sm,y_train_sm)","fc276494":"accuracy_score(y_train_sm, tree_2.predict(X_train_sm))","62c8f509":"accuracy_score(y_test, tree_2.predict(X_test_sc))","0e41b7e6":"plot_confusion_matrix(tree_2,\n                      X_test_sc,\n                      y_test,\n                      values_format='d',\n                      display_labels=['loyal','default'])","1f81184c":"plot_confusion_matrix(tree_2,\n                      X_train_sm,\n                      y_train_sm,\n                      values_format='d',\n                      display_labels=['loyal','default'])","6a15f167":"from sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import GridSearchCV","2acbbaf6":"knn=KNeighborsClassifier()","244c97db":"grid_param={\n    'n_neighbors':[3,5,8,12,15,18,20],\n    'weights':['uniform','distance'],\n    'metric':['euclidean','manhattan']\n}\ngs= GridSearchCV(\n    KNeighborsClassifier(),\n    grid_param,\n    verbose=1,\n    cv=3,\n    n_jobs=-1\n    )\ngs_results=gs.fit(X_train_sm,y_train_sm)","41dedf1b":"gs_results.best_score_","a957ba2d":"gs_results.best_estimator_","baa9f9ec":"gs_results.best_params_","b3c4e269":"knn2 = KNeighborsClassifier(metric='manhattan', n_neighbors=3, weights='distance')","e0ca20e1":"knn2.fit(X_train_sm,y_train_sm)","a28b157b":"accuracy_score(y_train_sm, knn2.predict(X_train_sm))","69e99984":"accuracy_score(y_test, tree_2.predict(X_test_sc))","d01e4d26":"plot_confusion_matrix(knn2,\n                      X_train_sm,\n                      y_train_sm,\n                      values_format='d',\n                      display_labels=['loyal','default'])","8a6c23ae":"plot_confusion_matrix(knn2,\n                      X_test_sc,\n                      y_test,\n                      values_format='d',\n                      display_labels=['loyal','default'])","94242ec3":"# Normalising Data","891be172":"here we can see that approximately 90% customers who has defaulted their credit are single so we will consider this feature by encoding it .","525cc4e0":"# Conclusion","a14ee893":"# Splitting the data ","c14be92c":"Decision Tree Classifier is giving us very large amount of false negative and accuracy is decreasing as we are tyring to reduce the false negative but there is no huge overfitting. In other hand while using KNN we are getting huge overfitting but its giving more accurate prediction of default than Decision Tree.","0d6661f8":"# KNN Model","179ac04f":"In this project we are going to make a model to predict credit card default using dataset given on Kaggle. I am going to use various Machine Learning Models for this data in different Notebooks and then we will compare the Models. I am using Decision Tree Classifier, Logistic Regression, KNN, XGBoost etc... In case of data unbalance, I am going to use SMOTETomek , oversampling or undersampling. ","7faac703":"We have three csv files which contains train, test and target values of test data in prediction csv file. The data has five numerical features (Income, Age, Experience, Current job years, current house years) and six categorical features (Marital status, house ownership,car ownership, profession, city, state)","bd148dab":"# Data Exploration","f97fa9c7":"# using SMOTETomek to balace data","99dd2320":"# Introduction","6b61c4fb":"Above we can see that Maharashtra state has highest number of credit default case. But since the number people with credit card is also more from Maharashtra we cannot find any relation.","b136fbf6":"# Building Model Using Decision Tree Classifier"}}