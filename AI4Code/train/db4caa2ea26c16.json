{"cell_type":{"ad298e86":"code","e369700f":"code","be3134d8":"code","bd43b8f9":"code","a98ab5bd":"code","3e31ca29":"code","3cd55fef":"code","048a8eb0":"code","a7e19ba4":"code","ec4c65b7":"code","fb4307a8":"code","e5518002":"code","07795d05":"code","6421efb6":"code","69c4ca40":"code","9e9b58c1":"code","37fff6f7":"code","b6038735":"code","3a648a8f":"code","7d9e563d":"code","2eff6755":"code","c332aad9":"code","c602a346":"code","6c354b16":"code","a22b0a75":"code","e5a2a584":"code","a383569a":"code","1d215b50":"code","f4d1fac0":"markdown","a179bf77":"markdown","f6db8a03":"markdown","87b2435a":"markdown","b80524c3":"markdown","5c42ab1b":"markdown","ece2a839":"markdown","e19d84e2":"markdown","5a01f2b7":"markdown","4b94e47c":"markdown","cbf1056c":"markdown","2b3d1958":"markdown","64ff6387":"markdown","203b9339":"markdown","635697e7":"markdown"},"source":{"ad298e86":"import os\nimport sys\nimport tarfile\nimport time\n\n\nsource = 'http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/aclImdb_v1.tar.gz'\ntarget = 'aclImdb_v1.tar.gz'\n\n\ndef reporthook(count, block_size, total_size):\n    global start_time\n    if count == 0:\n        start_time = time.time()\n        return\n    duration = time.time() - start_time\n    progress_size = int(count * block_size)\n    speed = progress_size \/ (1024.**2 * duration)\n    percent = count * block_size * 100. \/ total_size\n    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB\/s | %d sec elapsed\" %\n                    (percent, progress_size \/ (1024.**2), speed, duration))\n    sys.stdout.flush()\n\n\nif not os.path.isdir('aclImdb') and not os.path.isfile('aclImdb_v1.tar.gz'):\n    \n    if (sys.version_info < (3, 0)):\n        import urllib\n        urllib.urlretrieve(source, target, reporthook)\n    \n    else:\n        import urllib.request\n        urllib.request.urlretrieve(source, target, reporthook)","e369700f":"if not os.path.isdir('aclImdb'):\n\n    with tarfile.open(target, 'r:gz') as tar:\n        tar.extractall()","be3134d8":"conda install -c conda-forge pyprind","bd43b8f9":"import pyprind\nimport pandas as pd\nimport os\n\n# change the `basepath` to the directory of the\n# unzipped movie dataset\n\nbasepath = 'aclImdb'\n\nlabels = {'pos': 1, 'neg': 0}\npbar = pyprind.ProgBar(50000)\ndf = pd.DataFrame()\nfor s in ('test', 'train'):\n    for l in ('pos', 'neg'):\n        path = os.path.join(basepath, s, l)\n        for file in sorted(os.listdir(path)):\n            with open(os.path.join(path, file), \n                      'r', encoding='utf-8') as infile:\n                txt = infile.read()\n            df = df.append([[txt, labels[l]]], \n                           ignore_index=True)\n            pbar.update()\ndf.columns = ['review', 'sentiment']","a98ab5bd":"import numpy as np\n\nnp.random.seed(0)\ndf = df.reindex(np.random.permutation(df.index))","3e31ca29":"df.to_csv('movie_data.csv', index=False, encoding='utf-8')","3cd55fef":"import pandas as pd\n\ndf = pd.read_csv('movie_data.csv', encoding='utf-8')\ndf.head(3)","048a8eb0":"df.shape","a7e19ba4":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer()\ndocs = np.array([\n    'The sun is shining',\n    'The weather is sweet',\n    'The sun is shining, the weather is sweet, and one and one is two'\n])\nbag = count.fit_transform(docs)","ec4c65b7":"print(count.vocabulary_)","fb4307a8":"print(bag.toarray()) #printing the feature vector that we've just created","e5518002":"np.set_printoptions(precision=2)","07795d05":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf = TfidfTransformer(use_idf=True, \n                         norm='l2', \n                         smooth_idf=True)\nprint(tfidf.fit_transform(count.fit_transform(docs))\n      .toarray())","6421efb6":"df.loc[0, 'review'][-50:]","69c4ca40":"import re\ndef preprocessor(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n    \n    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n    \n    return text","9e9b58c1":"preprocessor(df.loc[0, 'review'][-50:])","37fff6f7":"preprocessor(\"<\/a>This :) is :( a test :-)!\")","b6038735":"df['review'] = df['review'].apply(preprocessor)","3a648a8f":"from nltk.stem.porter import PorterStemmer\n\nporter = PorterStemmer()\n\ndef tokenizer(text):\n    return text.split()\n\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]","7d9e563d":"tokenizer('runners like running and thus they run')","2eff6755":"tokenizer_porter('runners like running and thus they run')","c332aad9":"import nltk\nnltk.download('stopwords')","c602a346":"from nltk.corpus import stopwords\n\nstop = stopwords.words('english')\n[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]","6c354b16":"X_train = df.loc[:25000, 'review'].values\ny_train = df.loc[:25000, 'sentiment'].values\nX_test = df.loc[25000:, 'review'].values\ny_test = df.loc[25000:, 'sentiment'].values","a22b0a75":"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\n\ntfidf = TfidfVectorizer(strip_accents=None,\n                        lowercase=False,\n                        preprocessor=None)\n\nparam_grid = [{'vect__ngram_range': [(1, 1)],\n               'vect__stop_words': [stop, None],\n               'vect__tokenizer': [tokenizer, tokenizer_porter],\n               'clf__penalty': ['l1', 'l2'],\n               'clf__C': [1.0, 10.0, 100.0]},\n              {'vect__ngram_range': [(1, 1)],\n               'vect__stop_words': [stop, None],\n               'vect__tokenizer': [tokenizer, tokenizer_porter],\n               'vect__use_idf':[False],\n               'vect__norm':[None],\n               'clf__penalty': ['l1', 'l2'],\n               'clf__C': [1.0, 10.0, 100.0]},\n              ]\n\nlr_tfidf = Pipeline([('vect', tfidf),\n                     ('clf', LogisticRegression(random_state=0))])\n\ngs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n                           scoring='accuracy',\n                           cv=5,\n                           verbose=1,\n                           n_jobs=-1)","e5a2a584":"gs_lr_tfidf.fit(X_train, y_train)","a383569a":"print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\nprint('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)","1d215b50":"clf = gs_lr_tfidf.best_estimator_\nprint('Test Accuracy: %.3f' % clf.score(X_test, y_test))","f4d1fac0":"### Preprocessing the movie dataset into more convenient format","a179bf77":"We have to remove this unnecessary html tags and punctuation. But we shall not delete the emoticons.","f6db8a03":"Saving the assembled data as CSV file:","87b2435a":"<strong>The results reveal that our machine learning model can predict whether a movie\nreview is positive or negative with 90 percent accuracy.<\/strong>","b80524c3":"## Assessing word relevancy via term frequency-inverse document frequency","5c42ab1b":"Shuffling the DataFrame:","ece2a839":"## Cleaning the data","e19d84e2":"# Applying Machine Learning To Sentiment Analysis","5a01f2b7":"## Bag-of-words Model","4b94e47c":"This is a machine-learning based sentiment analysis from the IMDb data. I have followed all the instructions of the book <strong>Python Machine Learning by Sebastian Raschka<\/strong>. The dataset has been downloaded from <a href=\"http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/\">this link<\/a>. You can run the following code to download the dataset into your OS.","cbf1056c":"Strip HTML and punctuation to speed up the GridSearch later:","2b3d1958":"## Preparing the IMDb movie review data for text processing","64ff6387":"## Training a logistic regression model for document classification","203b9339":"## Processing documents into tokens","635697e7":"Scikit-learn implements the TfidfTransformer, that takes the raw term frequencies from CountVectorizer as input and transforms them into tf-idfs:"}}