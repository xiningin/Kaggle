{"cell_type":{"43ea61d7":"code","0c574fa4":"code","640fb137":"code","b27759dc":"code","94c81c30":"code","a7728af1":"code","f8f26340":"code","c9650aae":"code","79f2efb0":"code","39379ae5":"code","559155ac":"code","cf3bfce3":"code","76bba5a1":"code","8feae159":"code","9a3cf86c":"code","426b707c":"code","4b3a3828":"code","4e123c98":"code","4a067a0b":"code","234c5c2c":"code","08aea3c7":"code","53d3ee86":"code","518efc22":"code","2977d76c":"code","971ff6d1":"code","f7f4ddf4":"code","dce1103d":"code","33ab6221":"code","334e0982":"code","b8c76829":"markdown","0ad94a86":"markdown","03c05ee1":"markdown","43544436":"markdown","fb09875b":"markdown","c4d7edba":"markdown","a3971643":"markdown","8eb683bf":"markdown","e5076020":"markdown","b58496e2":"markdown","193cf5c0":"markdown","f5a89eb3":"markdown"},"source":{"43ea61d7":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","0c574fa4":"df = pd.read_csv(r\"..\/input\/nlp-getting-started\/train.csv\")\ndf.head()","640fb137":"df.shape","b27759dc":"text = list(df['text'])\n#text","94c81c30":"#lower\nlist1 = []\nfor doc in text:\n    doc = doc.lower()\n    list1.append(doc)\ntext = list1\n\n#print(text)","a7728af1":"#word tokenize\nlist1 = []\nfor doc in text:\n    doc = word_tokenize(doc)\n    list1.append(doc)\ntext = list1\n\n#print(text)","f8f26340":"#remove punctuations\npunc = '''!()-[]{};:'\"\\, <>.\/?@#$%^&*_~'''\n\nlist2 = []\n\nfor doc in text:\n    list1 = []\n    for words in doc:\n        for char in words:\n            if char in punc:\n                words = words.replace(char,\"\")\n        if len(words) > 0:\n            list1.append(words)\n    list2.append(list1)\n   \ntext = list2\n#print(text)","c9650aae":"#remove stopwords\nlist2 = []\nfor doc in text:\n    list1 = []\n    for words in doc:\n        if words not in stopwords.words('english'):\n            list1.append(words)\n    list2.append(list1)\n    \ntext = list2\n#print(text)","79f2efb0":"#lemmatization\nwordnet = WordNetLemmatizer()\n    \ntext_lemmatize = []\n\nfor doc in text:\n    list1 = []\n    for word in doc:\n        list1.append(wordnet.lemmatize(word))\n\n    text_lemmatize.append(list1)\n\ntext = text_lemmatize\n#print(text)","39379ae5":"list1 = []\nfor doc in text:\n    string = ' '.join(doc)\n    list1.append(string)\n\ntext = list1\n#print(text)","559155ac":"#saving cleaned data in a pickle file\n# with open(\"cleaned_text\",\"wb\") as f:\n#     pickle.dump(text, f)","cf3bfce3":"#saving cleaned data in a pickle file for test data\n# with open(\"test_cleaned_text\",\"wb\") as f:\n#     pickle.dump(text, f)","76bba5a1":"#loading text from train\n# import pickle\n\n# with open(\"cleaned_text\",\"rb\") as f:\n#     text = pickle.load(f)","8feae159":"#loading text from test\n# import pickle\n\n# with open(\"test_cleaned_text\",\"rb\") as f:\n#     text = pickle.load(f)","9a3cf86c":"#text","426b707c":"Y = list(df['target'])","4b3a3828":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(text, Y, test_size=0.0001, random_state=0)","4e123c98":"print(len(X_train),\" \",len(X_test))","4a067a0b":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", decode_error=\"ignore\")\ntfidf_vectorizer.fit(X_train)","234c5c2c":"from sklearn.svm import SVC\n\ncls = SVC()\ncls.fit(tfidf_vectorizer.transform(X_train), y_train)","08aea3c7":"from sklearn.metrics import classification_report, accuracy_score\n\ny_pred = cls.predict(tfidf_vectorizer.transform(X_test))\nprint(accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","53d3ee86":"val = [\"Amazing day\",\"disaster earthquake\"]\nans = cls.predict(tfidf_vectorizer.transform(val))\nans","518efc22":"df2 = pd.read_csv(r\"..\/input\/nlp-getting-started\/test.csv\")\ndf2.head()","2977d76c":"text = list(df2['text'])\n#text","971ff6d1":"#text","f7f4ddf4":"ans = cls.predict(tfidf_vectorizer.transform(text))\nans","dce1103d":"submission = pd.DataFrame({'id': df2['id'], 'target': ans })","33ab6221":"submission.head()","334e0982":"submission.to_csv('my_submission_svm', index=False)","b8c76829":"### Clean test data from above code ","0ad94a86":"### Submission ","03c05ee1":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Import Libraries<\/a><\/span><\/li><li><span><a href=\"#Data-Cleaning-for-both-train-and-test-data\" data-toc-modified-id=\"Data-Cleaning-for-both-train-and-test-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Data Cleaning for both train and test data<\/a><\/span><\/li><li><span><a href=\"#Saving-Cleaned-data\" data-toc-modified-id=\"Saving-Cleaned-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Saving Cleaned data<\/a><\/span><\/li><li><span><a href=\"#Loading-Cleaned-data\" data-toc-modified-id=\"Loading-Cleaned-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Loading Cleaned data<\/a><\/span><\/li><li><span><a href=\"#Feature-Vectorization-using-TFIDF\" data-toc-modified-id=\"Feature-Vectorization-using-TFIDF-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Feature Vectorization using TFIDF<\/a><\/span><\/li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Model<\/a><\/span><\/li><li><span><a href=\"#Test-Data\" data-toc-modified-id=\"Test-Data-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Test Data<\/a><\/span><\/li><li><span><a href=\"#Clean-test-data-from-above-code\" data-toc-modified-id=\"Clean-test-data-from-above-code-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Clean test data from above code<\/a><\/span><\/li><li><span><a href=\"#Submission\" data-toc-modified-id=\"Submission-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Submission<\/a><\/span><\/li><li><span><a href=\"#Accuracy(SVC):-80.263%\" data-toc-modified-id=\"Accuracy(SVC):-80.263%-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Accuracy(SVC): 80.263%<\/a><\/span><\/li><\/ul><\/div>","43544436":"### Data Cleaning for both train and test data","fb09875b":"### Saving Cleaned data ","c4d7edba":"**This notebook is to share my knowledge to the beginners as to how to go about writing code in a competition and I have particularly used SVC to solve the problem and got a decent accuracy and I am currently working on a different approach to get a higher score.**","a3971643":"**Please upvote the notebook if you liked it.**","8eb683bf":"### Test Data ","e5076020":"### Model ","b58496e2":"### Import Libraries ","193cf5c0":"### Feature Vectorization using TFIDF ","f5a89eb3":"### Loading Cleaned data "}}