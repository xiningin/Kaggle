{"cell_type":{"90e161a2":"code","f862027a":"code","dc986d15":"code","0e27ec94":"code","d396c0df":"code","26410477":"code","c76cbc0d":"code","9301357f":"code","376707f4":"code","b8263095":"code","e3fba36a":"code","b83dee9d":"code","8206b130":"code","24dfc2fc":"code","0b1e9c65":"markdown","58d0682e":"markdown"},"source":{"90e161a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f862027a":"!curl -O http:\/\/www.robots.ox.ac.uk\/~vgg\/data\/pets\/data\/images.tar.gz\n!curl -O http:\/\/www.robots.ox.ac.uk\/~vgg\/data\/pets\/data\/annotations.tar.gz\n!tar -xf images.tar.gz\n!tar -xf annotations.tar.gz","dc986d15":"import os \ninput_dir='images\/'\ntarget_dir='annotations\/trimaps\/'\nimg_size=(160,160)\nnum_classes=4\nbatch_size=32\n\ninput_img_paths  =sorted([ os.path.join(input_dir,fname) for fname in os.listdir(input_dir) if fname.endswith('.jpg')])\ntarget_img_paths =sorted([ os.path.join(target_dir,fname) for fname in os.listdir(target_dir) if fname.endswith('.png') and not fname.startswith('.')]) \n\nprint(\"Number of samples:\", len(input_img_paths))\n\nfor input_dir,target_dir in zip(input_img_paths[:10],target_img_paths[:10]):\n    print(input_dir,'|',target_dir)","0e27ec94":"from IPython.display import Image , display\nfrom tensorflow.keras.preprocessing.image import load_img\nimport PIL\nfrom PIL import ImageOps","d396c0df":"display(Image(filename=input_img_paths[10]))\n","26410477":"img=PIL.ImageOps.autocontrast(load_img(target_img_paths[10]))\ndisplay(img)","c76cbc0d":"from tensorflow import keras","9301357f":"class OxfordPets(keras.utils.Sequence):\n    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n\n    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.input_img_paths = input_img_paths\n        self.target_img_paths = target_img_paths\n\n    def __len__(self):\n        return len(self.target_img_paths) \/\/ self.batch_size\n\n    def __getitem__(self, idx):\n        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n        i = idx * self.batch_size\n        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n        x = np.zeros((batch_size,) + self.img_size + (3,), dtype=\"float32\")\n        for j, path in enumerate(batch_input_img_paths):\n            img = load_img(path, target_size=self.img_size)\n            x[j] = img\n        y = np.zeros((batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n        for j, path in enumerate(batch_target_img_paths):\n            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n            y[j] = np.expand_dims(img, 2)\n        return x, y","376707f4":"from tensorflow.keras import layers","b8263095":"def get_model(img_size, num_classes):\n    inputs = keras.Input(shape=img_size + (3,))\n\n    ### [First half of the network: downsampling inputs] ###\n\n    # Entry block\n    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    previous_block_activation = x  # Set aside residual\n\n    # Blocks 1, 2, 3 are identical apart from the feature depth.\n    for filters in [64, 128, 256]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n        # Project residual\n        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n            previous_block_activation\n        )\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    ### [Second half of the network: upsampling inputs] ###\n\n    for filters in [256, 128, 64, 32]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.UpSampling2D(2)(x)\n\n        # Project residual\n        residual = layers.UpSampling2D(2)(previous_block_activation)\n        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model\n\n\n# Free up RAM in case the model definition cells were run multiple times\nkeras.backend.clear_session()\n\n# Build model\nmodel = get_model(img_size, num_classes)\nmodel.summary()\n        ","e3fba36a":"import random\n\nval_samples = 1000\nrandom.Random(1337).shuffle(input_img_paths)\nrandom.Random(1337).shuffle(target_img_paths)\ntrain_input_img_paths = input_img_paths[:-val_samples]\ntrain_target_img_paths = target_img_paths[:-val_samples]\nval_input_img_paths = input_img_paths[-val_samples:]\nval_target_img_paths = target_img_paths[-val_samples:]\n\n# Instantiate data Sequences for each split\ntrain_gen = OxfordPets(\n    batch_size, img_size, train_input_img_paths, train_target_img_paths\n)\nval_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)","b83dee9d":"model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n\n","8206b130":"callbacks = [\n    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.h5\", save_best_only=True)]\n\nepochs = 15\nmodel.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)","24dfc2fc":"\nval_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)\nval_preds = model.predict(val_gen)\n\n\ndef display_mask(i):\n    \"\"\"Quick utility to display a model's prediction.\"\"\"\n    mask = np.argmax(val_preds[i], axis=-1)\n    mask = np.expand_dims(mask, axis=-1)\n    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n    display(img)\n\n\n# Display results for validation image #10\ni = 10\n\n# Display input image\ndisplay(Image(filename=val_input_img_paths[i]))\n\n# Display ground-truth target mask\nimg = PIL.ImageOps.autocontrast(load_img(val_target_img_paths[i]))\ndisplay(img)\n\n# Display mask predicted by our model\ndisplay_mask(i)  # Note that the model only sees inputs at 150x150.","0b1e9c65":"# Input images and target segmentation masks","58d0682e":"What does one input image and corresponding segmentation mask look like?"}}