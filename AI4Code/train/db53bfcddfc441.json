{"cell_type":{"ef6cc936":"code","fc03e0d1":"code","4a5da1a8":"code","5e38619c":"code","2885667d":"code","5c5b58be":"code","3b3c18f3":"code","589d24bc":"code","e3334823":"code","22b1cdd0":"code","14c90910":"code","c62bf6d9":"code","33fdbaab":"code","98a656df":"markdown","2ab8fc05":"markdown","4f46ad7e":"markdown","62e16b1c":"markdown","bc38604e":"markdown","e544a603":"markdown","976a51c3":"markdown","0e417e6b":"markdown","361af48b":"markdown","aabd4387":"markdown"},"source":{"ef6cc936":"from IPython.display import Image\nImage(\"..\/input\/Dance_Robots_Comic.jpg\")","fc03e0d1":"import numpy as np\nimport pandas as pd\nimport keras as K\nimport random\nimport sqlite3\nimport cv2\nimport os\n\nfrom skimage.color import rgb2gray, gray2rgb\nfrom skimage.transform import resize\nfrom skimage.io import imread, imshow\nimport matplotlib.pyplot as plt\n\nfrom keras.layers import Input, Dropout, Dense, concatenate, Embedding\nfrom keras.layers import Flatten, Activation\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.utils import np_utils\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.layers import LSTM, CuDNNGRU, CuDNNLSTM\nfrom keras.layers import MaxPooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n\nimport warnings\nwarnings.filterwarnings('ignore')","4a5da1a8":"Dance_Data = np.load('..\/input\/Encoded_Dancer.npy')\nDance_Data.shape","5e38619c":"TRAIN_SIZE = Dance_Data.shape[0]\nINPUT_SIZE = Dance_Data.shape[1]\nSEQUENCE_LENGTH = 70\nX_train = np.zeros((TRAIN_SIZE-SEQUENCE_LENGTH, SEQUENCE_LENGTH, INPUT_SIZE), dtype='float32')\nY_train = np.zeros((TRAIN_SIZE-SEQUENCE_LENGTH, INPUT_SIZE), dtype='float32')\nfor i in range(0, TRAIN_SIZE-SEQUENCE_LENGTH, 1 ): \n    X_train[i] = Dance_Data[i:i + SEQUENCE_LENGTH]\n    Y_train[i] = Dance_Data[i + SEQUENCE_LENGTH]\n\nprint(X_train.shape)\nprint(Y_train.shape)","2885667d":"def get_model():\n    inp = Input(shape=(SEQUENCE_LENGTH, INPUT_SIZE))\n    x = CuDNNLSTM(512, return_sequences=True,)(inp)\n    x = CuDNNLSTM(256, return_sequences=True,)(x)\n    x = CuDNNLSTM(512, return_sequences=True,)(x)\n    x = CuDNNLSTM(256, return_sequences=True,)(x)\n    x = CuDNNLSTM(512, return_sequences=True,)(x)\n    x = CuDNNLSTM(1024,)(x)\n    x = Dense(512, activation=\"elu\")(x)\n    x = Dense(256, activation=\"elu\")(x)\n    outp = Dense(INPUT_SIZE, activation='sigmoid')(x)\n    \n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='mse',\n                  optimizer=Adam(lr=0.0002),\n                  metrics=['accuracy'],\n                 )\n\n    return model\n\nmodel = get_model()\n\nmodel.summary()","5c5b58be":"filepath=\"Ai_Dance_RNN_Model.hdf5\"\n\ncheckpoint = ModelCheckpoint(filepath,\n                             monitor='loss',\n                             verbose=1,\n                             save_best_only=True,\n                             mode='min')\n\nearly = EarlyStopping(monitor=\"loss\",\n                      mode=\"min\",\n                      patience=3,\n                     restore_best_weights=True)","3b3c18f3":"model_callbacks = [checkpoint, early]\nmodel.fit(X_train, Y_train,\n          batch_size=64,\n          epochs=60,\n          verbose=2,\n          callbacks = model_callbacks)","589d24bc":"model.save(filepath)\nmodel.save_weights('Ai_Dance_RNN_Weights.hdf5')","e3334823":"%%time\nDANCE_LENGTH  = 6000\nLOOPBREAKER = 4\n\nx = np.random.randint(0, X_train.shape[0]-1)\npattern = X_train[x]\noutp = np.zeros((DANCE_LENGTH, INPUT_SIZE), dtype='float32')\nfor t in range(DANCE_LENGTH):\n#   if t % 500 == 0:\n#     print(\"%\"+str((t\/DANCE_LENGTH)*100)+\" done\")\n  \n    x = np.reshape(pattern, (1, pattern.shape[0], pattern.shape[1]))\n    pred = model.predict(x, verbose=0)\n    result = pred[0]\n    outp[t] = result\n    new_pattern = np.zeros((SEQUENCE_LENGTH, INPUT_SIZE), dtype='float32') \n    new_pattern[0:SEQUENCE_LENGTH-1] = pattern[1:SEQUENCE_LENGTH]\n    new_pattern[-1] = result\n    pattern = np.copy(new_pattern)\n    ####loopbreaker####\n    if t % LOOPBREAKER == 1:\n        pattern[np.random.randint(0, SEQUENCE_LENGTH-10)] = Y_train[np.random.randint(0, Y_train.shape[0]-1)]","22b1cdd0":"Decoder = load_model('..\/input\/Dancer_Decoder_Model.hdf5')\nDecoder.load_weights('..\/input\/Dancer_Decoder_Weights.hdf5') ","14c90910":"Dance_Output = Decoder.predict(outp)\nDance_Output.shape","c62bf6d9":"IMG_HEIGHT = Dance_Output[0].shape[0]\nIMG_WIDTH = Dance_Output[0].shape[1]\n\nfor row in Dance_Output[0:10]:\n    imshow(row.reshape(64,96))\n    plt.show()\n","33fdbaab":"video = cv2.VideoWriter('AI_Dance_Video.avi', cv2.VideoWriter_fourcc(*\"XVID\"), 20.0, (IMG_WIDTH, IMG_HEIGHT),False)\n\nfor img in Dance_Output:\n    img = resize(img, (IMG_HEIGHT,IMG_WIDTH), mode='constant', preserve_range=True)\n    img = img * 255\n    img = img.astype('uint8')\n    video.write(img)\n    cv2.waitKey(50)\n    \nvideo.release()","98a656df":"(This is part 3 of 3 of my How to Teach an AI to Dance. I originally made 3 separate notebooks for this task before compiling them into one later. The complete assembled notebook of all 3 parts can be found here: https:\/\/www.kaggle.com\/valkling\/how-to-teach-an-ai-to-dance)\n\n# AI Dance Part 3: Train AI w\/ RNNs\n\nIf you have read any of my text generating notebooks or know text generating AIs this next part will be familiar with you. If not, here is one of my related notebooks:\n\nThe Pythonic Python Script for Making Monty Python Scripts: https:\/\/www.kaggle.com\/valkling\/pythonicpythonscript4makingmontypythonscripts\n\nFor the dancing AI, the technique is pretty much the same. We will use our compressed pictures to make n length sequences as input that the model will use to predict the n+1 frame in the sequence. The differences are:\n\n- The input\/outputs will not be in one-hot encoding but rather an array of floats between 0 and 1\n\n- We will need a larger brain for our model to make it work.\n\n- We will need to decode the results after to turn them into a usable video.","2ab8fc05":"## Train RNN Model","4f46ad7e":"## Callbacks","62e16b1c":"## Create the RNN Model\n\nThe model is simply 6 LSTM layers stacked on top of each other. While text data only needs around 2-4 LSTM layers to work, the dance data benifits from a few more as the result is not categorical this time and a large brain allows for more \"creativity\"(variation) on the AIs part. (Note: CuDNNLSTM layers are just LSTM layers that automatically optimize for the GPU. They run a lot faster than standard LSTM layers at the cost of customization options)","bc38604e":"## Generate New Computer Generated Dances\n\nThis block generates new dance sequences in the style of the video of DANCE_LENGTH size in frames. It takes a random seed pattern from the training set, predicts the next frame, adds it to the end of the pattern and drops the first frame of the pattern and predicts on the new pattern and so forth. The default DANCE_LENGTH of 6000 frames is 5 minutes of video at 20 FPS.\n\nPretty much the AI will try to accurately duplicate the Dance video but inevitably makes errors, and those errors compound, but is still trained well enough that it ends up making similar, but not quite the same, dances.\n\nThe LOOPBREAKER is used to add noise to the prediction pattern, replacing a random frame in the pattern with a random frame in the Dance_Data after every LOOPBREAKER frames. This noise can be used to force the AI to change up what it is doing. This can stop undertrained models from looping or overtrained models from duplication the training data too closely. Setting it too low, on the other hand, can cause the results to distort more. It is worth playing around with this setting and is a quick and dirty way to adjust the dance output post training.","e544a603":"## Save Video","976a51c3":"## Part 3 Results\n\nThe results of the video are surprisingly crisp. Even small things like the swish of the skirt or swoop of the hair are caught in the video. Like in the youtube video, these results are pretty overfit and the computer is mostly duplicating the dances. However, there are some interesting variations and deformations in the video. The dancer will sometimes shrink and expand its arms or compress into a blob and reform. Playing with the model or the loopbreaker can lead to some interesting results.\n\n### Possible Improvements\n\n- The RNN model could use more and varied dances to train on. A cheap way to do this is just take more frames from the video in part 1. There is also a 5 hour version of these dancing silhouettes. (However, only 3 hours are usable)\n\n- I don't think that the model needs any more layers, it is large enough as is, but readjusting the shape might make it more efficient. On text data RNNs, I tried using 1D convolution layers with mixed results. (It speeds up the training time a lot but the model is more prone to looping) Might work here though.","0e417e6b":"## Output the Dance\n\nBefore we can save the video, we need to decode the frames back into images using the decoder we made in part 2.","361af48b":"## Create Compressed Dance Sequences\n\nOur model will look at the last 70 frames and attemp to predict the 71st. As such, sur X variable will be an array of 70 (compressed) frames in sequence and our Y variable will be the 71st frame. This block chops our Dance_Data into such sequences of frames.","aabd4387":"## Read in Data\n\nWhen processing this type of model on text data, each character is expressed in one hot arrays between ~50-100, (depending on the unique characters in the text to consider). Our data is in 128 numpy arrays, so it is not that much more load on our model to consider our compressed images over single characters of a text document."}}