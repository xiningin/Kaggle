{"cell_type":{"0aeed125":"code","10521a4d":"code","8cc515c1":"code","fb29737b":"code","9814f702":"code","c451e5db":"code","db109d6f":"code","c7984589":"code","2a00b971":"code","b2c0d698":"code","5231b826":"code","03f98548":"code","d41ff341":"code","b0499321":"code","f7383d12":"code","edb0ed7e":"code","98621717":"code","1ce8c0ca":"code","0f9b0f1b":"code","f19581dd":"code","ff8356e8":"code","815d10e1":"code","035b24e2":"code","973b9de6":"code","f472d527":"code","1cd8f061":"code","d904483d":"code","bdcb96b7":"code","a3732edb":"code","3bb1292e":"code","20dee0dd":"code","97504f50":"code","20553044":"code","3d003550":"code","31452af5":"code","2d4d3238":"code","c0b525fb":"code","48d7b52d":"code","5ea73403":"code","043a7a3c":"code","b9e00561":"code","aee10330":"code","a5031be2":"code","3cc2b1aa":"code","caf7741f":"code","af2fba3b":"code","822cb228":"code","eb8ae78f":"code","f9a6146b":"code","44aac837":"code","1a060e0e":"code","ffda3896":"code","bdb44bdb":"code","cca08bcf":"code","32828e63":"code","aa554beb":"code","075bc00f":"code","d0b1d707":"code","4d1a7244":"code","8f5c653e":"code","dbb8b6c8":"code","af05d436":"code","29c72f99":"code","ec799baa":"code","13609905":"code","a9a90be2":"code","750e2278":"code","c9b3ca49":"code","33c15f1d":"code","4a363352":"code","c7433067":"code","dd68cb2a":"code","ef04b160":"code","231fcf0e":"code","99de3ec8":"code","c46b4612":"code","9806bbb9":"code","2dc3322a":"code","f8dc5cdc":"code","202d1407":"code","ad571d02":"code","fa51e771":"code","e69c2ab2":"code","56d5ae3c":"code","e9b64341":"code","09c0c050":"code","7d3a4543":"code","2d50a74f":"code","1b6a1fd8":"code","20a250de":"code","5dcf277d":"code","1739720a":"code","13d78626":"code","fb552d5a":"code","91541a00":"code","ca9c625e":"code","64f91c92":"code","8bf566bf":"code","e02ee3a1":"code","024d2d1b":"code","297970e6":"code","e90721f6":"code","f196bb19":"code","dbf596fa":"code","7fb36101":"code","817012f0":"code","645b647a":"code","e576df45":"code","70960f64":"code","72b29f29":"code","efeeaf1f":"code","6501e243":"code","ee66e6ea":"code","f55fd4c6":"code","00cf5fe5":"code","f9b5c303":"code","601f2516":"code","93c5b1c5":"code","77ff4735":"code","33cd2910":"code","8ad5f8fa":"code","308658b7":"code","5b96f85d":"code","8d8103c2":"code","7ebe623f":"code","1d795c74":"code","3b9f7343":"code","fda0865c":"code","3772bdf5":"code","008c1477":"code","fbb23b49":"code","cb8c87b8":"code","c6ca64f1":"code","c403c394":"code","74fbcec4":"code","457908f1":"code","c9ff6ad3":"markdown","358fa0ff":"markdown","9db98ba7":"markdown","9c69bff4":"markdown","e7ee52b4":"markdown","c4a22eaf":"markdown","bf628bcf":"markdown","5195aca0":"markdown","2845e8c4":"markdown","ca14d666":"markdown","002deb67":"markdown","68cf2eb0":"markdown","45b690b6":"markdown","51fa23d7":"markdown","3c0960cd":"markdown","772a6d2b":"markdown","20d7c478":"markdown","8fbe28ad":"markdown","b914d692":"markdown","d4974950":"markdown","d3cdc493":"markdown","c22fc16d":"markdown","1bf4f67a":"markdown","9d90f297":"markdown","a5121119":"markdown","9ce0b832":"markdown","e2174a77":"markdown","88bd79cb":"markdown","708cb4ff":"markdown","b1737595":"markdown","5afd5241":"markdown","ea3b83a7":"markdown","38d03f10":"markdown","a3349e88":"markdown","88fb1857":"markdown","6ca0eb7b":"markdown","d4d22c1b":"markdown","78c6e3ae":"markdown","2455bcbc":"markdown","7cc94377":"markdown","5d2f4762":"markdown","ae58f8f6":"markdown","e6b43d2a":"markdown","8ba0776d":"markdown","983b38e8":"markdown","7e31a4ac":"markdown","ffb14fbd":"markdown","f05ba552":"markdown","40f08e54":"markdown","87bbd32a":"markdown","28b33aa8":"markdown"},"source":{"0aeed125":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","10521a4d":"pd.set_option('display.max_columns', None)","8cc515c1":"import numpy as np # linear algebra\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nimport plotly_express as px \n\nfrom sklearn.metrics import accuracy_score, confusion_matrix,r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder,PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom tensorflow import keras\nimport tensorflow as tf\n%matplotlib inline","fb29737b":"import random\nrandom.seed(42)\nnp.random.seed(40)\n# tf.random.set_seed(10)","9814f702":"data = pd.read_csv('\/kaggle\/input\/productivity-prediction-of-garment-employees\/garments_worker_productivity.csv')\ndata.describe()","c451e5db":"# Looking at the sample dataset\ndata.head()","db109d6f":"# Looking at the features types\ndata.info()","c7984589":"data.shape","2a00b971":"# Looking at the columns\ndata.columns","b2c0d698":"# Looking at the null values in each feature\ndata.isnull().sum()","5231b826":"# Percentage of null values in the dataset\ndata.isnull().sum() \/ data.shape[0] * 100","03f98548":"# Separate categorical and numerical data for simplicity in analysis\ncategory = data.select_dtypes(include='object')\nnumerical = data.select_dtypes(exclude='object')","d41ff341":"for col in category.columns:\n    print(f\"{col}\")\n    print(category[col].unique())\n    print()","b0499321":"for n in numerical.columns:\n    print(n)","f7383d12":"data['date'] = pd.to_datetime(data['date'])\n#Setting date column as index\ndata.set_index('date', drop = False, inplace=True)\ndata.head()","edb0ed7e":"# Department Feature has space present in one of the values which needs to be treated\nprint('Unique Vaues in Department before cleaning:')\nprint(data.department.unique())\ndata['department'] = data.department.str.strip()\n# category.loc[:,'department'] = category.loc[:,'department'].str.strip() \nprint('Unique Vaues in Department afer cleaning:')\nprint(data.department.unique())","98621717":"category.department.value_counts()","1ce8c0ca":"# fixing the error in the name\ncategory['department']=category['department'].replace(['sweing'],['sewing']) ","0f9b0f1b":"# data.wip.fillna( int(data.wip.mean()), inplace = True)\ndata['wip'].interpolate(method='time',inplace=True)\ndata.wip","f19581dd":"data['month'] = data['date'].dt.month_name() \n# data['day'] = data['date'].dt.day_name()\ndata.head()","ff8356e8":"categorical = [ 'quarter', 'department', 'day', 'team', 'no_of_style_change','month']","815d10e1":"for i in range(len(categorical)):\n    print(categorical[i])\n    data[categorical[i]].value_counts().plot(kind='pie', autopct=\"%.2f\")\n    plt.show()","035b24e2":"data.day.value_counts().plot(kind='bar')","973b9de6":"data.quarter.value_counts().plot(kind='bar')","f472d527":"data.department.value_counts().plot(kind='bar')","1cd8f061":"data.team.value_counts().plot(kind='bar')","d904483d":"data.no_of_style_change.value_counts().plot(kind='bar')","bdcb96b7":"data.month.value_counts().plot(kind='bar')","a3732edb":"#  numerical.iloc[:, 0]\nnumerical.columns","3bb1292e":"for i in range(len(numerical.columns)):\n    print(numerical.columns[i])\n    numerical.iloc[:, i].plot(kind='hist')\n    plt.show()","20dee0dd":"numerical.boxplot(column=['wip', 'over_time'],figsize=(8,7))","97504f50":"numerical.boxplot(column=['incentive'],figsize=(8,7))","20553044":"numerical.boxplot(column = [ 'smv'],figsize=(8,7))","3d003550":"numerical.boxplot(column = [ 'idle_time', 'idle_men',  'no_of_workers'] ,figsize=(8,7))","31452af5":"numerical.boxplot(column = [ 'targeted_productivity', 'actual_productivity'],figsize=(8,7))","2d4d3238":"# Team feature is a categorical variable and we need to treat it likewise so that it can be consumed by Model\nteam_count = data.team.value_counts()\nplt.figure(figsize=(10,5))\nsns.barplot(team_count.index, team_count.values, alpha=0.8)\nplt.title('Team Numbers')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Teams', fontsize=12)\nplt.show()","c0b525fb":"fig, ax = plt.subplots(ncols=2, figsize=(12,5))\ndata.groupby(['department']).agg({'team':['sum']}).plot(ax=ax[0], kind = 'bar')\nax[0].set_title('total number of teams for each department')\ndata.groupby(['department']).agg({'team':['mean']}).plot(ax=ax[1], kind = 'bar')\nax[1].set_title('average teams for each department')","48d7b52d":"fig, ax = plt.subplots(ncols=2, figsize=(12,5))\ndata.groupby(['department']).agg({'incentive':['sum']}).plot(ax=ax[0], kind = 'bar')\nax[0].set_title('total incentives for each department')\ndata.groupby(['department']).agg({'incentive':['mean']}).plot(ax=ax[1], kind = 'bar')\nax[1].set_title('average incentives for each department')","5ea73403":"fig, ax = plt.subplots(ncols=2, figsize=(12,5))\ndata.groupby(['department']).agg({'idle_men':['sum']}).plot(ax=ax[0], kind = 'bar')\nax[0].set_title('total idle_men for each department')\ndata.groupby(['department']).agg({'idle_men':['mean']}).plot(ax=ax[1], kind = 'bar')\nax[1].set_title('average idle men for each department')","043a7a3c":"fig, ax = plt.subplots(ncols=2,figsize=(12,5))\ndata.groupby(['department']).agg({'idle_time':['sum']}).plot(ax=ax[0], kind = 'bar')\nax[0].set_title('total idle time for each department')\ndata.groupby(['department']).agg({'idle_time':['mean']}).plot(ax=ax[1], kind = 'bar')\nax[1].set_title('average idle time for each department')","b9e00561":"fig, ax = plt.subplots(ncols=2,figsize=(12,5))\ndata.groupby(['department']).agg({'wip':['sum']}).plot(ax=ax[0], kind = 'bar')\nax[0].set_title('total work in progress for each department')\ndata.groupby(['department']).agg({'wip':['mean']}).plot(ax=ax[1], kind = 'bar')\nax[1].set_title('average work in progress for each department')","aee10330":"fig, ax = plt.subplots(ncols=2,figsize=(12,7))\ndata.groupby(['department']).agg({'targeted_productivity':['mean']}).plot(ax=ax[0], kind = 'bar')\nax[0].set_title('Average Target Productivity for each department')\ndata.groupby(['department']).agg({'actual_productivity':['mean']}).plot(ax=ax[1], kind = 'bar')\nax[1].set_title('Average Actual Productivity for each department')","a5031be2":"teams_agg =      data.groupby('team').agg(actual = ('actual_productivity','mean'), incentives = ('incentive', 'mean'))\n# teams_agg.plot(kind='line', title=\"Incentives vs Actual Productivity\").legend(bbox_to_anchor= (1.02, 1));\n# define the figure and subplots\nfig, ax = plt.subplots(2,figsize=(12,5))\nax[0].bar(teams_agg.index,teams_agg['actual'], color='orange')\nax[1].bar(teams_agg.index,teams_agg['incentives'],color='green');\n\n# set chart titles\nax[0].set_title('Average Productivity')\nax[1].set_title('Number of Incentives')\n\n# auto-adjust the plots layout\nfig.tight_layout()","3cc2b1aa":"teams_agg =      data.groupby('team').agg(actual = ('actual_productivity','mean'), target = ('targeted_productivity', 'mean'))\n# teams_agg.plot(kind='line', title=\"Incentives vs Actual Productivity\").legend(bbox_to_anchor= (1.02, 1));\n# define the figure and subplots\nfig, ax = plt.subplots(2,figsize=(12,5))\nax[0].bar(teams_agg.index,teams_agg['actual'], color='orange')\nax[1].bar(teams_agg.index,teams_agg['target'],color='green');\n\n# set chart titles\nax[0].set_title('Average Productivity')\nax[1].set_title('Targeted Productivity')\n\n# auto-adjust the plots layout\nfig.tight_layout()","caf7741f":"teams_agg =      data.groupby('team').agg(actual = ('actual_productivity','mean'), time = ('idle_time', 'mean'), men = ('idle_men','mean'))\n# define the figure and subplots\nfig, ax = plt.subplots(3,figsize=(12,5))\nax[0].bar(teams_agg.index,teams_agg['actual'], color='orange')\nax[1].bar(teams_agg.index,teams_agg['time'],color='green');\nax[2].bar(teams_agg.index,teams_agg['men'],color='blue');\n\n# set chart titles\nax[0].set_title('Average Productivity')\nax[1].set_title('Average Idle time per team')\nax[2].set_title('Average Idle men per team')\n\n# auto-adjust the plots layout\nfig.tight_layout()","af2fba3b":"data.head()","822cb228":"myfig, myax = plt.subplots(nrows = 12, figsize=(18, 120))\nfor i in range(12):\n    data[data['team']==i+1]['actual_productivity'].plot.line(title = 'Team '+str(i+1)+' productivity in first quarter',ax = myax[i])\n#     plt.title('Team 2 productivity in first quarter')\nplt.show()","eb8ae78f":"over_time_avg = []\nfor i in data['no_of_workers'].unique():\n    over_time_avg.append(data['over_time'][data['no_of_workers'] == i].mean())\nsns.lineplot(x=data['no_of_workers'].unique(), y=over_time_avg)\nplt.title(\"Overtime Vs Team size\")\nplt.xlabel('Team Size')\nplt.ylabel('Overtime')\nplt.show()","f9a6146b":"over_time_avg = []\nfor i in data['no_of_workers'].unique():\n    over_time_avg.append(data['actual_productivity'][data['no_of_workers'] == i].mean())\nsns.lineplot(x=data['no_of_workers'].unique(), y=over_time_avg)\nplt.title(\"Productivity Vs Team size\")\nplt.xlabel('Team Size')\nplt.ylabel('Productivity')\nplt.show()","44aac837":"over_time_avg = []\nfor i in data['no_of_workers'].unique():\n    over_time_avg.append(data['idle_men'][data['no_of_workers'] == i].mean())\nsns.lineplot(x=data['no_of_workers'].unique(), y=over_time_avg)\nplt.title(\"Productivity Vs Team size\")\nplt.xlabel('Team Size')\nplt.ylabel('Productivity')\nplt.show()","1a060e0e":"numerical.columns","ffda3896":"fig, ax = plt.subplots(1,1,figsize = (10,5))\nsns.distplot(data['smv'], bins = 5, ax = ax)\nplt.show()","bdb44bdb":"fig, ax = plt.subplots(1,1,figsize = (10,5))\nsns.distplot(data['no_of_style_change'], bins = 10, ax = ax)\nplt.show()","cca08bcf":"fig, ax = plt.subplots(1,1,figsize = (10,5))\nsns.distplot(data['no_of_workers'], bins = 10, ax = ax)\nplt.show()","32828e63":"fig, ax = plt.subplots(1,1,figsize = (10,5))\nsns.distplot(data['targeted_productivity'], bins = 10, ax = ax)\nplt.show()","aa554beb":"fig, ax = plt.subplots(1,1,figsize = (10,5))\nsns.distplot(data['actual_productivity'], bins = 10, ax = ax)\nplt.show()","075bc00f":"fig_dims = (12,7)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.histplot(data=data[['actual_productivity']],ax=ax)\nplt.show()","d0b1d707":"# import matplotlib.pyplot as plt\nfig_dims = (12,7)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.histplot(data=data[['targeted_productivity', 'actual_productivity']],ax=ax)\nplt.show()","4d1a7244":"teams_agg =      data.groupby('team').agg(actual = ('actual_productivity','mean'), target = ('targeted_productivity', 'mean'))\nteams_agg.plot(kind='line', title=\"Target vs Actual Productivity\").legend(bbox_to_anchor= (1.02, 1));","8f5c653e":"fig, ax = plt.subplots(figsize = (10,8))\nsns.heatmap(numerical.corr(), annot = True, ax = ax,  cmap = 'coolwarm')","dbb8b6c8":"fig, ax = plt.subplots(figsize = (8,7))\nsns.scatterplot(x=data['smv'], y=data['no_of_workers'])","af05d436":"data['no_of_style_change'].value_counts()","29c72f99":"data[['quarter', 'department', 'day', 'team','no_of_style_change','month']]","ec799baa":"data['team']=data['team'].astype(str)\ndata['no_of_style_change']=data['no_of_style_change'].astype(str)\n# ['quarter', 'department', 'day', 'team','no_of_style_change','month']\ndata=pd.get_dummies(data)\n# Dropping Date column as it is no longer needed\ndata.drop(columns = 'date',inplace = True)\ndata.head()","13609905":"# data.actual_productivity = data.actual_productivity * 100\n# data.targeted_productivity = data.targeted_productivity * 100\n# data.describe()","a9a90be2":"from sklearn.preprocessing import MinMaxScaler\ncols_to_scale  = ['smv', 'wip', 'over_time', 'incentive', 'no_of_workers', 'idle_time', 'idle_men']\nmin_max_scaler = MinMaxScaler()\ndata[cols_to_scale] = min_max_scaler.fit_transform(data[cols_to_scale])\ndata.head()","750e2278":"data.describe()","c9b3ca49":"data.info()","33c15f1d":"# for i in data:\n#     data[i] = data[i].astype(int)\n# data.info()","4a363352":"# from sklearn.feature_selection import RFE\n# from sklearn.tree import DecisionTreeClassifier\n# # define the method\n# rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=15)\nX, y = data.drop(['actual_productivity'], axis=1), data['actual_productivity']\n# fit the model\n# rfe.fit(X, y)\n# # transform the data\n# X = rfe.transform(X)","c7433067":"# X, y = data.drop(['actual_productivity'], axis=1), data['actual_productivity']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nprint(X_train.shape)","dd68cb2a":"input_shape = 39","ef04b160":"from sklearn.linear_model import Ridge\nfrom sklearn.feature_selection import RFE\n# Creating an estimator\nridge = Ridge()\n# Creating RFE object \nrfe = RFE(estimator = ridge, n_features_to_select = input_shape, verbose = 1)\n# Fitting the training data into our model\nrfe.fit(X_train, y_train)\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","231fcf0e":"# Let us look at the columns which have been supported by the RFE\nRFE_ridge_support_columns = X_train.columns[rfe.support_]\nRFE_ridge_support_columns","99de3ec8":"# Preparing a new dataset containing only the RFE support columns data\nX_train = X_train[RFE_ridge_support_columns]\nX_train.head()\n# Preparing a new dataset containing only the RFE support columns data\nX_test = X_test[RFE_ridge_support_columns]\nX_test.head()","c46b4612":"from sklearn import metrics\nmodels_metrics = pd.DataFrame(columns = [ 'models','mae','mse', 'rmse' ,'mape', 'R2'])\ndef evaluate_model(model,Y_actual,Y_Predicted, df): \n    mape = np.mean(np.abs((Y_actual - Y_Predicted)\/Y_actual))*100\n    mae=metrics.mean_absolute_error(Y_actual, Y_Predicted)\n    mse=metrics.mean_squared_error(Y_actual, Y_Predicted)\n    rmse=np.sqrt(metrics.mean_squared_error(Y_actual, Y_Predicted))\n    r2 = metrics.r2_score(Y_actual, Y_Predicted)\n    df2 = {'models':model,'mae':mae,'mse':mse, 'rmse':rmse, 'mape':mape, 'R2': r2}\n    df = df.append(df2, ignore_index = True)\n    return df","9806bbb9":"from sklearn.linear_model import Ridge, Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor","2dc3322a":"# Building a model\nmodel_linear = LinearRegression()\nmodel_linear.fit(X_train,y_train)\n#Prediction using test set \ny_linear_pred = model_linear.predict(X_test)\nmodels_metrics = evaluate_model('Linear Regression', y_test, y_linear_pred, models_metrics)","f8dc5cdc":"plt.scatter(y_test, y_linear_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for Linear Regression Model\")\nplt.show()","202d1407":"# Building a model\nmodel_lasso = Lasso()\nmodel_lasso.fit(X_train,y_train)\n#Prediction using test set \ny_lasso_pred = model_lasso.predict(X_test)\nmodels_metrics = evaluate_model('Lasso Regression', y_test, y_lasso_pred, models_metrics)","ad571d02":"plt.scatter(y_test, y_lasso_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for Lasso Regression Model\")\nplt.show()","fa51e771":"# Building a model\nmodel_ridge = Ridge()\nmodel_ridge.fit(X_train,y_train)\n#Prediction using test set \ny_ridge_pred = model_ridge.predict(X_test)\n# Checking with metrics\nmodels_metrics = evaluate_model('Ridge Regression', y_test, y_ridge_pred, models_metrics)","e69c2ab2":"plt.scatter(y_test, y_ridge_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for Ridge Regression Model\")\nplt.show()","56d5ae3c":"models_metrics","e9b64341":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n# help(PolynomialFeatures)\n# help(Pipeline)","09c0c050":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n# Doing a polynomial regression: Comparing linear, quadratic and cubic fits\n# Pipeline helps you associate two models or objects to be built sequentially with each other, \n# in this case, the objects are PolynomialFeatures() and LinearRegression()\n\npipeline = Pipeline([('poly_features', PolynomialFeatures(degree=2)),\n                     ('model', LinearRegression())])\npipeline.fit(X_train, y_train)\ny_pred_degree2 = pipeline.predict(X_test)\nmodels_metrics = evaluate_model('Polynomial Degree 2', y_test, y_pred_degree2, models_metrics)\n    \npipeline = Pipeline([('poly_features', PolynomialFeatures(degree=3)),\n                     ('model', LinearRegression())])\npipeline.fit(X_train, y_train)\ny_pred_degree3 = pipeline.predict(X_test)\nmodels_metrics = evaluate_model('Polynomial Degree 3', y_test, y_pred_degree3, models_metrics)\n\npipeline = Pipeline([('poly_features', PolynomialFeatures(degree=4)),\n                     ('model', LinearRegression())])\npipeline.fit(X_train, y_train)\ny_pred_degree4 = pipeline.predict(X_test)\nmodels_metrics = evaluate_model('Polynomial Degree 4', y_test, y_pred_degree4, models_metrics)","7d3a4543":"plt.scatter(y_test, y_pred_degree3)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for Polynomial Regression Model\")\nplt.show()","2d50a74f":"from sklearn.linear_model import PoissonRegressor\npipeline_poisson = Pipeline([('model', PoissonRegressor())])\npipeline_poisson.fit(X_train, y_train)\ny_pred_poisson = pipeline_poisson.predict(X_test)\nmodels_metrics = evaluate_model('Poisson Regression', y_test, y_pred_poisson, models_metrics)\nmodels_metrics","1b6a1fd8":"plt.scatter(y_test, y_pred_poisson)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for Poisson Regression Model\")\nplt.show()","20a250de":"model_dt = DecisionTreeRegressor(random_state = 0)\nmodel_dt.fit(X_train,y_train)\n#Prediction using test set \ny_dt_pred = model_dt.predict(X_test)\n# Checking with metrics\nmodels_metrics = evaluate_model('Decision Tree Regression', y_test, y_dt_pred, models_metrics)","5dcf277d":"plt.scatter(y_test, y_dt_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for Decision Tree Regression Model\")\nplt.show()","1739720a":"# Building a model\nmodel_rf = RandomForestRegressor(n_estimators = 100 ,  random_state = 10)\nmodel_rf.fit(X_train,y_train)\n#Prediction using test set \ny_rf_pred = model_rf.predict(X_test)\n# Checking with metrics\nmodels_metrics = evaluate_model('Random Forest Regression', y_test, y_rf_pred, models_metrics)","13d78626":"plt.scatter(y_test, y_rf_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for Random Forest Regression Model\")\nplt.show()","fb552d5a":"model_svr = SVR(C=25)\nmodel_svr.fit(X_train, y_train)\ny_svr_pred = model_svr.predict(X_test)\n# model_svr.score(x2_test, y2_test)\nmodels_metrics = evaluate_model('Support Vector Regression', y_test, y_svr_pred, models_metrics)","91541a00":"plt.scatter(y_test, y_svr_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for SVR Regression Model\")\nplt.show()","ca9c625e":"model_kn = KNeighborsRegressor()\nmodel_kn.fit(X_train, y_train)\ny_kn_pred = model_kn.predict(X_test)\nmodels_metrics = evaluate_model('KNeighbour Regression def', y_test, y_kn_pred, models_metrics)","64f91c92":"model_kn = KNeighborsRegressor(n_neighbors=3)\nmodel_kn.fit(X_train, y_train)\ny_kn_pred = model_kn.predict(X_test)\nmodels_metrics = evaluate_model('KNeighbour Regression 3', y_test, y_kn_pred, models_metrics)","8bf566bf":"plt.scatter(y_test, y_kn_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for KNeighbors Regression Model\")\nplt.show()","e02ee3a1":"import xgboost as xgb\nxgbr = xgb.XGBRegressor(verbosity = 0)\nxgbr.fit(X_train, y_train)\ny_xgb_pred = xgbr.predict(X_test)\nmodels_metrics = evaluate_model('XGBoost', y_test, y_xgb_pred, models_metrics)","024d2d1b":"plt.scatter(y_test, y_xgb_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for XGBoost Model\")\nplt.show()","297970e6":"from sklearn.ensemble import GradientBoostingRegressor\nmodel_gbr = GradientBoostingRegressor(alpha=0.9,learning_rate=0.05, max_depth=2, min_samples_leaf=5, min_samples_split=2, n_estimators=100, random_state=30)\n\nmodel_gbr.fit(X_train, y_train)\ny_gbr_pred = model_gbr.predict(X_test)\nmodels_metrics = evaluate_model('Gradient Boost', y_test, y_gbr_pred, models_metrics)","e90721f6":"plt.scatter(y_test, y_gbr_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for Gradient Boost Model\")\nplt.show()","f196bb19":"model = keras.Sequential([\n                          keras.layers.Dense(input_shape, input_shape=(input_shape,), activation='linear'),\n                          keras.layers.Dense(1,activation='linear')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='mean_squared_error'\n)\nmodel.fit(X_train, y_train, epochs=100)","dbf596fa":"model.evaluate(X_test, y_test)","7fb36101":"y_test = np.array(y_test).reshape(-1,1)\ny_test.shape","817012f0":"y_ann_pred = model.predict( X_test )\nmodels_metrics = evaluate_model('ANN', y_test, y_ann_pred, models_metrics)","645b647a":"plt.scatter(y_test, y_ann_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for ANN Model\")\nplt.show()","e576df45":"model2 = keras.Sequential([\n                          keras.layers.Dense(input_shape, input_shape=(input_shape,), activation='linear'),\n                          keras.layers.Dense(64, activation='linear'),\n                          keras.layers.Dense(32, activation='linear'),\n                          keras.layers.Dense(1,activation='linear')\n])\n\nmodel2.compile(\n    optimizer='adam',\n    loss='mean_squared_error'\n)\nmodel2.fit(X_train, y_train, epochs=100)\nmodel2.evaluate(X_test, y_test)\ny_ann2_pred = model2.predict( X_test )\nmodels_metrics = evaluate_model('DNN', y_test, y_ann2_pred, models_metrics)","70960f64":"plt.scatter(y_test, y_ann2_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for ANN Model\")\nplt.show()","72b29f29":"models_metrics","efeeaf1f":"pip install neupy","6501e243":"X_train.describe()","ee66e6ea":"from neupy import algorithms\ngrnn_model = algorithms.GRNN(std=0.1, verbose=False)\ngrnn_model.train(X_train, y_train)\ny_grnn_pred = grnn_model.predict(X_test)\n# models_metrics = evaluate_model('GRNN', y_test, np.nan_to_num(y_grnn_pred), models_metrics)\nmodels_metrics = evaluate_model('GRNN', y_test, y_grnn_pred, models_metrics)\n# mse = np.mean((np.nan_to_num(y_predicted) - np.array(y_test).reshape(-1,1)) ** 2)\n# mse","f55fd4c6":"from neupy import algorithms\ngrnn_model = algorithms.GRNN(std=0.08, verbose=False)\ngrnn_model.train(X_train, y_train)\ny_grnn_pred = grnn_model.predict(X_test)\n# models_metrics = evaluate_model('GRNN2', y_test, np.nan_to_num(y_grnn_pred), models_metrics)\nmodels_metrics = evaluate_model('GRNN2', y_test, y_grnn_pred, models_metrics)","00cf5fe5":"plt.scatter(y_test, y_grnn_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for GRNN Model\")\nplt.show()","f9b5c303":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(grnn_model, X_train, y_train, cv = 10, scoring= 'neg_mean_absolute_error')\nprint(scores)\n# # models_metrics\n\n# # # list of alphas to tune\n# params = {'std': [ 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 1.0]}\n\n# # # Creating the Ridge estimator\n# # # ridge = Ridge()\n# grnn_model = algorithms.GRNN( std = 0.08, verbose=False)\n\n# folds = 5\n\n# # cross validation - creating cross validation object, passing parameters\n# model_cv = GridSearchCV(estimator = grnn_model, \n#                         param_grid = params, \n#                         scoring= 'neg_mean_absolute_error', \n#                         cv = folds, \n#                         return_train_score=True,\n#                         verbose = 1)\n\n# # Fitting the model on the training dataset\n# model_cv.fit(X_train, y_train)\n\n# # Preparing the datatframe containing the results\n\n# cv_results = pd.DataFrame(model_cv.cv_results_)\n\n# # Inspecting the results\n# cv_results.head()","601f2516":"\nfrom sklearn.model_selection import cross_val_predict\npredicted = cross_val_predict(grnn_model, X_train, y_train, cv=10)\n\nfig, ax = plt.subplots()\nax.scatter(y_train, predicted, edgecolors=(0, 0, 0))\nax.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"k--\", lw=4)\nax.set_xlabel(\"Measured\")\nax.set_ylabel(\"Predicted\")\nplt.show()","93c5b1c5":"# pip install keras\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import backend as K\n\nclass RBFLayer(Layer):\n    def __init__(self, units, gamma, **kwargs):\n        super(RBFLayer, self).__init__(**kwargs)\n        self.units = units\n        self.gamma = K.cast_to_floatx(gamma)\n\n    def build(self, input_shape):\n        self.mu = self.add_weight(name='mu',\n                                  shape=(int(input_shape[1]), self.units),\n                                  initializer='uniform',\n                                  trainable=True)\n        super(RBFLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        diff = K.expand_dims(inputs) - self.mu\n        l2 = K.sum(K.pow(diff, 4), axis=1)\n        res = K.exp(-1 * self.gamma * l2)\n        return res\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.units)","77ff4735":"tf.random.set_seed(10)\n# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n# K.set_session(sess)","33cd2910":"from tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.losses import binary_crossentropy\n\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(input_shape,1)))\nmodel.add(RBFLayer(10, 0.5))\nmodel.add(Dense(1, activation='sigmoid', name='foo'))\n\nmodel.compile(optimizer='rmsprop', loss=binary_crossentropy)\nmodel.fit(X_train, y_train, batch_size=256, epochs=3)","8ad5f8fa":"model.evaluate(X_test, y_test)","308658b7":"y_rbfnn_pred = model.predict(X_test)\nmodels_metrics = evaluate_model('RBFNN', y_test, np.nan_to_num(y_rbfnn_pred), models_metrics)","5b96f85d":"plt.scatter(y_test, y_rbfnn_pred)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nplt.xlabel(\"Actual Productivity\")\nplt.ylabel(\"Predicted Productivity\")\nplt.title(\"Actual vs Predicted for RBFNN Model\")\nplt.show()","8d8103c2":"models_metrics","7ebe623f":"final_metrics =  models_metrics.drop([3, 4, 10,14,16], axis=0)\nfinal_metrics ","1d795c74":"final_metrics['models'][5] = 'Polynomial Regression'\nfinal_metrics['models'][11] = 'KNeighbour Regression'\nfinal_metrics['models'][17] = 'GRNN'\nfinal_metrics","3b9f7343":"# models_metrics.mae.value_counts().plot(kind='bar')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplot_data = final_metrics#.drop(3) #Remove 3rd index since metrics is in different scale\nplt.figure(figsize=(10, 5))\nsns.set_theme(style=\"whitegrid\")\nax = sns.barplot(x=\"models\", y=\"mae\", data=plot_data)\nplt.xticks(rotation=90)","fda0865c":"plt.figure(figsize=(10, 5))\nsns.set_theme(style=\"whitegrid\")\nax = sns.barplot(x=\"models\", y=\"mse\", data=plot_data)\nplt.xticks(rotation=90)","3772bdf5":"plt.figure(figsize=(10, 5))\nsns.set_theme(style=\"whitegrid\")\nax = sns.barplot(x=\"models\", y=\"rmse\", data=plot_data)\nplt.xticks(rotation=90)","008c1477":"plt.figure(figsize=(10, 5))\nsns.set_theme(style=\"whitegrid\")\nax = sns.barplot(x=\"models\", y=\"mape\", data=plot_data)\nplt.xticks(rotation=90)","fbb23b49":"plt.figure(figsize=(10, 5))\nsns.set_theme(style=\"whitegrid\")\nax = sns.barplot(x=\"models\", y=\"R2\", data=plot_data)\nplt.xticks(rotation=90)","cb8c87b8":"# print(final_metrics.sort_values('mae'))\npd.options.display.width = 100\n# final_metrics.sort_values('mae')\nprint(final_metrics.sort_values('mae')[[\"models\",\"mae\"]])","c6ca64f1":"print(final_metrics.sort_values('mse')[[\"models\",\"mse\"]])","c403c394":"print(final_metrics.sort_values('rmse')[[\"models\",\"rmse\"]])","74fbcec4":"print(final_metrics.sort_values('mape')[[\"models\",\"mape\"]])","457908f1":"print(final_metrics.sort_values('R2',ascending=False)[[\"models\",\"R2\"]])","c9ff6ad3":"### Time Series","358fa0ff":"#### ANN with more hidden layers","9db98ba7":"### DATA FEATURING","9c69bff4":"### Ridge Model","e7ee52b4":"#### Simple ANN","c4a22eaf":"WIP: Or products in progress for each team, can be seen here","bf628bcf":"* From the above heatmap we can see that there is very high correlation between smv and no_of_workers.\n* over_time & smv as well as over_time and no_of)workers are correlated","5195aca0":"Only wip feature has null values with appx 42.47%","2845e8c4":"#### Department wise ","ca14d666":"## DATA Features","002deb67":"### Linear Regression Model","68cf2eb0":"### Polynomial Regression","45b690b6":"### Histograms","51fa23d7":"### KNeighbors Regression","3c0960cd":"## Bivariate Analysis","772a6d2b":"## EDA","20d7c478":"### One Hot Encoding\nPerforming One Hot encoding on the categorical data","8fbe28ad":"### Univariate Analysis","b914d692":"### Poisson Regression","d4974950":"In the above plot, one can see the number of teams for each department","d3cdc493":"One can see Department wise incentives given","c22fc16d":"### Decision Tree","1bf4f67a":"## GRNN Model","9d90f297":"### Lasso Model","a5121119":"Formula implemented:  \n* x: single input tensor (a vector, as we are only defining RBF for a flat record)\n* \u03bc  is a learned weight vector (e.g. one weight per  xi  in  x )\n* \u03b3  is a user-specified tuning parameter\n* the formula for a single RBF neuron activation is: exp(\u2212\u03b3\u2211(x\u2212\u03bc)2)","9ce0b832":"## DATA Preparation","e2174a77":"### Building ANN Models","88bd79cb":"### Random Forest","708cb4ff":"### SVR Models","b1737595":"### Pie Charts","5afd5241":"### Team wise","ea3b83a7":"### Boosting Algorithms","38d03f10":"### Scaling","a3349e88":"### Training & Splitting","88fb1857":"### Data Correlation","6ca0eb7b":"### RBFNN Model","d4d22c1b":"### DATA CLEANING","78c6e3ae":"We can look at the team wise incentives given. One can see that the Sewing teams have been given more incentives","2455bcbc":"### Actual Vs Target","7cc94377":"#### XG Boost","5d2f4762":"### Feature Selection","ae58f8f6":"One can see that the targeted productivity is capped at 0.8 while actual productivity is over 1","e6b43d2a":"Now we need to process the data such that it can be consumed by the Models","8ba0776d":"### Bar Plots","983b38e8":"# Model Building","7e31a4ac":"#### Gradient Boosting","ffb14fbd":"### Metrics","f05ba552":"## Attribute Information:\n\n* 01 date : Date in MM-DD-YYYY\n* 02 day : Day of the Week\n* 03 quarter : A portion of the month. A month was divided into four quarters\n* 04 department : Associated department with the instance\n* 05 team_no : Associated team number with the instance\n* 06 no_of_workers : Number of workers in each team\n* 07 no_of_style_change : Number of changes in the style of a particular product\n* 08 targeted_productivity : Targeted productivity set by the Authority for each team for each day.\n* 09 smv : Standard Minute Value, it is the allocated time for a task\n* 10 wip : Work in progress. Includes the number of unfinished items for products\n* 11 over_time : Represents the amount of overtime by each team in minutes\n* 12 incentive : Represents the amount of financial incentive (in BDT) that enables or motivates a particular course of action.\n* 13 idle_time : The amount of time when the production was interrupted due to several reasons\n* 14 idle_men : The number of workers who were idle due to production interruption\n* 15 actual_productivity : The actual % of productivity that was delivered by the workers. It ranges from 0-1.","40f08e54":"Let us look at few features which have numeric values but are in fact categorical","87bbd32a":"Let us look at the make up of the data against each categorical features","28b33aa8":"#### Boxplots for Numerical data"}}