{"cell_type":{"c7cbaca1":"code","e183a36f":"code","d4c4f952":"code","3f46fbe3":"code","ba7f819e":"code","0bbc9743":"code","73064c09":"code","ca116878":"code","6af0a2ef":"code","96670abe":"code","2eb9fe30":"code","7a88cb5e":"code","cf5ddc8f":"code","5c490559":"code","07b0bfc0":"code","bade9c86":"code","73ec9376":"code","e247326d":"code","5afc0540":"code","2348bffd":"code","424f6c97":"code","1507e106":"code","545cdcdb":"code","358f6fbb":"code","80c01997":"code","e521f132":"code","8e2db518":"code","c4e2d37a":"code","ef953c75":"code","049653a4":"code","f261f3b9":"code","b17b5b03":"code","d9288589":"code","200ac43f":"markdown","a0abca8e":"markdown"},"source":{"c7cbaca1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nfrom sklearn import preprocessing\nimport numpy as np\nimport lightgbm as lgb\nimport gc\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e183a36f":"df_train=pd.read_csv('\/kaggle\/input\/widsdatathon2022\/train.csv')\n\ndf_test=pd.read_csv('\/kaggle\/input\/widsdatathon2022\/test.csv')","d4c4f952":"len(df_train)","3f46fbe3":"len(df_test)","ba7f819e":"df_train.isnull().mean()","0bbc9743":"df_train['site_eui'].describe()","73064c09":"df_train['Year_Factor'].value_counts()","ca116878":"df_test['Year_Factor'].value_counts()","6af0a2ef":"df_test['Year_Factor'].value_counts()","96670abe":"df_test.isnull().mean()","2eb9fe30":"df_train","7a88cb5e":"df_test","cf5ddc8f":"def entrena_lgb(data,test,features,categorical,target):\n\n    kfold=GroupKFold(n_splits=6)\n\n\n    i=1\n\n    r=[]\n    \n    pred_test=np.zeros(len(test))\n\n    importancias=pd.DataFrame()\n\n    importancias['variable']=features\n    \n    \n    cat_ind=[features.index(x) for x in categorical if x in features]\n    \n    dict_cat={}\n    \n    categorical_numerical = data[categorical].dropna().select_dtypes(include=np.number).columns.tolist()\n    \n    categorical_transform=[x for x in categorical if x not in categorical_numerical]\n    \n    for l in categorical_transform:\n        le = preprocessing.LabelEncoder()\n        le.fit(list(data[l].dropna())+list(test[l].dropna()))\n\n        dict_cat[l]=le\n\n        data.loc[~data[l].isnull(),l]=le.transform(data.loc[~data[l].isnull(),l])\n        test.loc[~test[l].isnull(),l]=le.transform(test.loc[~test[l].isnull(),l])\n        \n        \n\n    for train_index,test_index in kfold.split(data,data[target],data['Year_Factor']):\n\n        lgb_data_train = lgb.Dataset(data.loc[train_index,features].values,data.loc[train_index,target].values)\n        lgb_data_eval = lgb.Dataset(data.loc[test_index,features].values,data.loc[test_index,target].values, reference=lgb_data_train)\n\n        params = {\n            'task': 'train',\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': { 'rmse'},\n            \"max_depth\":-1,\n            \"num_leaves\":32,\n            'learning_rate': 0.1,\n        \"min_child_samples\": 100,\n            'feature_fraction': 0.9,\n         \"bagging_freq\":1,\n            'bagging_fraction': 0.9,\n            \"lambda_l1\":10,\n            \"lambda_l2\":10,\n           # \"scale_pos_weight\":30,\n            'min_data_per_group':500,\n\n            'verbose': 1    \n        }\n\n\n\n\n        modelo = lgb.train(params,lgb_data_train,num_boost_round=13100,valid_sets=lgb_data_eval,early_stopping_rounds=50,verbose_eval=25,categorical_feature=cat_ind)\n\n        importancias['gain_'+str(i)]=modelo.feature_importance(importance_type=\"gain\")\n\n\n        data.loc[test_index,'estimator']=modelo.predict(data.loc[test_index,features].values, num_iteration=modelo.best_iteration)\n        \n        pred_test=pred_test+modelo.predict(test[features].values, num_iteration=modelo.best_iteration)\n\n        print (\"Fold_\"+str(i))\n        a= (mean_squared_error(data.loc[test_index,target],data.loc[test_index,'estimator']))**0.5\n        r.append(a)\n        print (a)\n        print (\"\")\n\n        i=i+1\n        \n    for l in categorical_transform:\n\n            data.loc[~data[l].isnull(),l]=dict_cat[l].inverse_transform(data.loc[~data[l].isnull(),l].astype(int))\n            \n            test.loc[~test[l].isnull(),l]=dict_cat[l].inverse_transform(test.loc[~test[l].isnull(),l].astype(int))\n            \n    importancias[\"gain_avg\"]=importancias[[\"gain_1\",\"gain_2\",\"gain_3\",\"gain_4\",\"gain_5\"]].mean(axis=1)\n    importancias=importancias.sort_values(\"gain_avg\",ascending=False).reset_index(drop=True)\n    \n    pred_test=(pred_test\/6)\n    \n    \n    oof=(mean_squared_error(data[target],data['estimator']))**0.5\n    \n    print (oof)\n    print (\"mean: \"+str(np.mean(np.array(r))))\n    print (\"std: \"+str(np.std(np.array(r))))\n    \n    dict_resultados={}\n    \n    dict_resultados['importancias']=importancias\n    \n    dict_resultados['predicciones']=pred_test\n    \n    \n    \n    return dict_resultados","5c490559":"no_usar=['site_eui','id']\n\ntarget='site_eui'\n\ncategorical=['Year_Factor','State_Factor','building_class','facility_type']\n\nfeatures=[x for x in df_train.columns if x not in no_usar]\n\n","07b0bfc0":"dict_resultados=entrena_lgb(data=df_train,test=df_test,features=features,categorical=categorical,target=target)","bade9c86":"\ndict_resultados['importancias']","73ec9376":"dict_resultados['importancias']['variable'].tolist()","e247326d":"temp=dict_resultados['importancias']\n\nfeatures_selected=temp['variable'].tolist()[0:4]\n\n","5afc0540":"dict_resultados_2=entrena_lgb(data=df_train,test=df_test,features=features_selected,categorical=categorical,target=target)","2348bffd":"dict_resultados_2['importancias']","424f6c97":"df_train.groupby([ 'State_Factor', 'building_class', 'facility_type',\n       'floor_area', 'year_built', 'energy_star_rating', 'ELEVATION']).size()","1507e106":"df_train.groupby([ 'State_Factor', 'building_class', 'facility_type',\n       'floor_area', 'year_built', 'energy_star_rating', 'ELEVATION']).size().value_counts()","545cdcdb":"df_train.groupby([ 'State_Factor', 'building_class', 'facility_type',\n       'floor_area', 'year_built',  'ELEVATION']).size().value_counts()","358f6fbb":"\nfeatures_selected_2=list(features_selected)\n\nfeatures_selected_2.extend(['State_Factor'])","80c01997":"dict_resultados_3=entrena_lgb(data=df_train,test=df_test,features=features_selected_2,categorical=categorical,target=target)","e521f132":"features_selected_2","8e2db518":"\n\nvariables=['facility_type',\n 'energy_star_rating',\n 'year_built',\n# 'floor_area',\n 'State_Factor']\n\ndf_train['combination_variables']=df_train[variables].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n\ndf_test['combination_variables']=df_test[variables].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)","c4e2d37a":"features_selected_3=list(features_selected_2)\n\nfeatures_selected_3.extend(['combination_variables'])\n\ncategorical.extend(['combination_variables'])","ef953c75":"dict_resultados_4=entrena_lgb(data=df_train,test=df_test,features=features_selected_3,categorical=categorical,target=target)","049653a4":"dict_resultados_4['importancias']","f261f3b9":"df_test['site_eui']=(dict_resultados_2['predicciones'].copy()+dict_resultados_3['predicciones'].copy())\/2","b17b5b03":"df_test[['id','site_eui']]","d9288589":"df_test[['id','site_eui']].to_csv('submission.csv',index=False)","200ac43f":"# Starter Baseline","a0abca8e":"The aim of this notebook is to show simple techniques to create a baseline for this competition.\n\nThis notebook has a lot to improve, so it is a good starting poinr for a lot of participants."}}