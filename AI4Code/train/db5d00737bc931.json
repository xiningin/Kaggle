{"cell_type":{"ab9763b2":"code","0d6a1f10":"code","99773198":"code","f32dab67":"code","b7ce4287":"code","cee0b8c5":"code","d53adaf5":"code","3aebd5d9":"code","006bf545":"code","7108696f":"code","28382499":"code","454e6f2c":"code","fb316134":"code","0b863706":"code","0e106752":"code","1e859eb8":"code","aa080508":"code","fb60dd96":"code","4f8bd150":"code","1eb25013":"code","d3b37e4a":"code","9872272b":"code","e53f3e8a":"code","c4969c3b":"code","6e76d4cb":"code","df77ba18":"code","0584c75b":"code","da95ddd8":"code","e156f2bd":"code","33764898":"code","35b4c528":"code","370a15bd":"code","39c2636b":"code","46129b95":"code","1451ea95":"code","72e8b538":"code","528e68d4":"code","3971c117":"code","eae178bc":"code","3d5a0930":"code","6a44b556":"code","507e03bf":"code","37f1b166":"code","5ae9a19c":"code","7427da28":"code","689d9fb2":"markdown","e16c3bf5":"markdown","87608650":"markdown","041f1a81":"markdown","08cd6e83":"markdown","cd3dc70b":"markdown","28ee8421":"markdown","3c88d750":"markdown","16b86b43":"markdown","f75ecfe6":"markdown"},"source":{"ab9763b2":"!mkdir .\/model\n!mkdir .\/token","0d6a1f10":"!pip install --upgrade pip","99773198":"%%capture\n! pip install jax==0.2.25\n! pip install jaxlib==0.1.74+cuda11 -f https:\/\/storage.googleapis.com\/jax-releases\/jax_releases.html\n! pip install git+https:\/\/github.com\/huggingface\/transformers.git\n! pip install git+https:\/\/github.com\/deepmind\/optax.git\n! pip install --upgrade -q git+https:\/\/github.com\/google\/flax.git #pip install flax\n! pip install seqeval\n! conda install -y -c conda-forge datasets\n! conda install -y importlib-metadata","f32dab67":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"","b7ce4287":"import jax, flax, tensorflow\nfor _m in (jax, flax, tensorflow):\n    print(f'{_m.__name__}: {_m.__version__}')\n# jax.devices()\njax.lib.xla_client._xla.is_optimized_build() ","cee0b8c5":"import numpy as np\nimport datasets\nfrom datasets import load_dataset, load_metric, Dataset\nimport pandas as pd\nfrom jax import lax, random, numpy as jnp\n# import random\nfrom typing import Tuple\nfrom pathlib import Path\nfrom collections import defaultdict\n# import flax\nimport optax\nimport json\nimport datetime\nimport pickle\nfrom itertools import chain\nfrom tqdm.auto import tqdm\nfrom typing import Callable\n# import jax\n# import jax.numpy as np #jnp\nfrom flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\nfrom flax.training import train_state\nfrom flax import traverse_util\nfrom jax.experimental.maps import xmap","d53adaf5":"max_length = 1024\nstride = 128\nmin_tokens = 6\n\n# TRAINING HYPERPARAMS\nBS = 1\nGRAD_ACC = 8\nLR = 5e-5\nWD = 0.01\nWARMUP = 0.1\nN_EPOCHS = 10","3aebd5d9":"#read train data\ntrain = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\ntrain.head(1)","006bf545":"# check unique classes\nclasses = train.discourse_type.unique().tolist()\nclasses","7108696f":"tags = defaultdict()\n\nfor i, c in enumerate(classes):\n    tags[f'B-{c}'] = i\n    tags[f'I-{c}'] = i + len(classes)\ntags[f'O'] = len(classes) * 2\ntags[f'Special'] = -100\n    \nl2i = dict(tags)\n\ni2l = defaultdict()\nfor k, v in l2i.items(): \n    i2l[v] = k\ni2l[-100] = 'Special'\n\ni2l = dict(i2l)\n\nN_LABELS = len(i2l) - 1 # not accounting for -100","28382499":"model_checkpoint = 'google\/bigbird-roberta-base' ","454e6f2c":"from transformers import AutoTokenizer, BigBirdTokenizerFast, BigBirdTokenizer\ntokenizer = BigBirdTokenizerFast.from_pretrained(model_checkpoint)\ntokenizer.tokenize(\"The weather is fine today. \")","fb316134":"# Not sure if this is needed, but in case we create a span with certain class without starting token of that class,\n# let's convert the first token to be the starting token.\n\ne = [0,7,7,7,1,1,8,8,8,9,9,9,14,4,4,4]\n\ndef fix_beginnings(labels):\n    for i in range(1,len(labels)):\n        curr_lab = labels[i]\n        prev_lab = labels[i-1]\n        if curr_lab in range(7,14):\n            if prev_lab != curr_lab and prev_lab != curr_lab - 7:\n                labels[i] = curr_lab -7\n    return labels\n\nfix_beginnings(e)","0b863706":"with open('\/kaggle\/input\/feedback-pickles\/bigbird_tokenizer.pickle', 'rb') as handle:\n    tokenized_datasets = pickle.load(handle)","0e106752":"tokenized_datasets","1e859eb8":"# The test was created by the 0.1 split of the data which is our validation\/evaluation dataset.\ntrain_dataset = tokenized_datasets[\"train\"]\neval_dataset = tokenized_datasets[\"test\"]","aa080508":"from transformers import FlaxAutoModelForTokenClassification, AutoConfig, FlaxBigBirdForTokenClassification #, is_tensorboard_available\n# from transformers.file_utils import get_full_repo_name\n# from transformers.utils import check_min_version\n# from transformers.utils.versions import require_version\n\nnum_labels = 15\nseed = 0\n\nconfig = AutoConfig.from_pretrained(model_checkpoint, num_labels=num_labels) #, dtype=jnp.dtype(\"bfloat16\"))\nmodel = FlaxAutoModelForTokenClassification.from_pretrained(model_checkpoint, config=config, seed=seed)","fb60dd96":"num_train_epochs = 15\nlearning_rate = 1e-5","4f8bd150":"total_batch_size = 4 #per_device_batch_size * jax.local_device_count()\nprint(\"The overall batch size (both for training and eval) is\", total_batch_size)","1eb25013":"# eval_dataset","d3b37e4a":"num_train_steps = len(train_dataset) \/\/ total_batch_size * num_train_epochs\nstep_per_epoch = len(train_dataset) \/\/ total_batch_size\ntotal_steps = num_train_steps * num_train_epochs\nlearning_rate_function = optax.cosine_onecycle_schedule(transition_steps=num_train_steps, peak_value=learning_rate, pct_start=0.01, )\nprint(\"The number of train steps (all the epochs) is\", num_train_steps)","9872272b":"class TrainState(train_state.TrainState):\n    logits_function: Callable = flax.struct.field(pytree_node=False)\n    loss_function: Callable = flax.struct.field(pytree_node=False)","e53f3e8a":"def decay_mask_fn(params):\n    flat_params = traverse_util.flatten_dict(params)\n    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}\n    return traverse_util.unflatten_dict(flat_mask)","c4969c3b":"def adamw(weight_decay):\n    #return optax.adamw(learning_rate=learning_rate_function, b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay, mask=decay_mask_fn)\n    return optax.lamb(learning_rate=0.000001, b1=0.9, b2=0.999, eps=1e-06, eps_root=0.0, weight_decay=weight_decay, mask=decay_mask_fn) # wd=0.0, mask=decay_mask_fn","6e76d4cb":"adamw = adamw(1e-2)","df77ba18":"def eval_fn(logits):\n    return logits.argmax(-1)","0584c75b":"@jax.jit\n\ndef cross_entropy_loss(logits, labels):\n    xentropy = optax.softmax_cross_entropy(logits, onehot(labels, num_classes=15)) #num_labels))\n    return jnp.mean(xentropy)","da95ddd8":"metric = load_metric(\"seqeval\")","e156f2bd":"# @jax.jit\ndef get_labels(y_pred, y_true):\n    true_predictions = [\n            [i2l[p] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(y_pred, y_true) #zip(predictions, labels)\n        ]\n    true_labels = [\n            [i2l[l] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(y_pred, y_true) #zip(predictions, labels)\n        ]\n    return true_predictions, true_labels","33764898":"# #  @jax.jit\ndef compute_metrics():\n    results = metric.compute()\n    if return_entity_level_metrics:\n        # Unpack nested dictionaries\n        final_results = {}\n        for key, value in results.items():\n            if isinstance(value, dict):\n                for n, v in value.items():\n                    final_results[f\"{key}_{n}\"] = v\n            else:\n                final_results[key] = value\n        return final_results\n    else:\n        return {\n            \"precision\": results[\"overall_precision\"],\n            \"recall\": results[\"overall_recall\"],\n            \"f1\": results[\"overall_f1\"],\n            \"accuracy\": results[\"overall_accuracy\"],\n        }","35b4c528":"# import jax\n# import jax.numpy as np\n\n# key = jax.random.PRNGKey(0)\n\nprint('JAX is running on', jax.lib.xla_bridge.get_backend().platform)","370a15bd":"rng = jax.random.PRNGKey(seed)\ndropout_rngs = jax.random.split(rng, jax.local_device_count())","39c2636b":"state = TrainState.create(\n    apply_fn=model.__call__,\n    params=model.params,\n    tx=adamw,\n    logits_function=eval_fn, #lambda logits: logits.argmax(-1), #eval_function,\n    loss_function=cross_entropy_loss,\n)","46129b95":"def train_data_collator(rng, dataset, batch_size):\n    \"\"\"Returns shuffled batches of size `batch_size` from truncated `train dataset`, sharded over all local devices.\"\"\"\n    \"\"\"\u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043f\u0435\u0440\u0435\u0442\u0430\u0441\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u0430\u043a\u0435\u0442\u044b \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c `batch_size` \u0438\u0437 \u0443\u0441\u0435\u0447\u0435\u043d\u043d\u043e\u0433\u043e `\u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e\u0435\u0437\u0434\u0430`, \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u0435 \u043d\u0430 \u0432\u0441\u0435 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u0430.\"\"\"\n    steps_per_epoch = len(dataset) \/\/ batch_size\n    perms = jax.random.permutation(rng, len(dataset))\n    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n    perms = perms.reshape((steps_per_epoch, batch_size))\n\n    for perm in perms:\n        batch = dataset[perm]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n\n        yield batch","1451ea95":"def eval_data_collator(dataset, batch_size):\n    \"\"\"Returns batches of size `batch_size` from `eval dataset`, sharded over all local devices.\"\"\"\n    for i in range(len(dataset) \/\/ batch_size):\n        batch = dataset[i * batch_size : (i + 1) * batch_size]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n\n        yield batch","72e8b538":"# token classification\n@jax.jit\ndef train_step(state, batch, dropout_rng):\n    targets = batch.pop(\"labels\")\n    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n    def loss_function(params):\n#         print(params)\n        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n        loss = state.loss_function(logits, targets)\n        return loss\n\n    grad_function = jax.value_and_grad(loss_function)\n    loss, grad = grad_function(state.params)\n    grad = jax.lax.pmean(grad, \"batch\")\n    new_state = state.apply_gradients(grads=grad)\n    metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_function(state.step)}, axis_name=\"batch\")\n    return new_state, metrics, new_dropout_rng","528e68d4":"parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,), backend='gpu')","3971c117":"# token classification\n@jax.jit\ndef eval_step(state, batch):\n    logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n    return state.logits_function(logits)","eae178bc":"parallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\", backend='gpu')","3d5a0930":"# import jax\n# import jax.numpy as np\n\n# key = jax.random.PRNGKey(0)\n\n# print('JAX is running on', jax.lib.xla_bridge.get_backend().platform)","6a44b556":"state = flax.jax_utils.replicate(state)","507e03bf":"return_entity_level_metrics=False\neval_steps = 404\ntotal_eval_steps = 4040\neval_stp = 404\nbest = 0","37f1b166":"# import jax\n# import jax.numpy as np\n\n# key = jax.random.PRNGKey(0)\n\nprint('JAX is running on', jax.lib.xla_bridge.get_backend().platform)","5ae9a19c":"from flax.jax_utils import replicate, unreplicate\nfrom itertools import chain\nimport time\n\ntrain_time = 0\nepochs = tqdm(range(num_train_epochs), desc=f\"Epoch ... (1\/{num_train_epochs})\", position=0)\nfor epoch in epochs:\n\n    train_start = time.time()\n    train_metrics = []\n\n    # Create sampling rng\n    rng, input_rng = jax.random.split(rng)\n\n    # train\n    for step, batch in enumerate(tqdm(train_data_collator(input_rng, train_dataset, total_batch_size),total=step_per_epoch,desc=\"Training...\",position=1,)):\n        state, train_metric, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n        train_metrics.append(train_metric)\n\n        cur_step = (epoch * step_per_epoch) + (step + 1)\n        if cur_step % step_per_epoch == 0 and cur_step > 0:\n            # Save metrics\n            train_metric = unreplicate(train_metric)\n            train_time += time.time() - train_start\n            \n            epochs.write(\n                f\"Step... ({cur_step}\/{total_steps} | Training Loss: {train_metric['loss']}, Learning Rate: {train_metric['learning_rate']})\"\n            )\n\n            train_metrics = []\n        \n        # if cur_step % eval_steps == 0 and cur_step > 0:\n\n            eval_metrics = {}\n            # evaluate\n            for batch in tqdm(\n                eval_data_collator(eval_dataset, total_batch_size),\n                total=len(eval_dataset) \/\/ total_batch_size,\n                desc=\"Evaluating ...\",\n                position=2,\n            ):\n                labels = batch.pop(\"labels\")\n                predictions = parallel_eval_step(state, batch)\n                predictions = np.array([pred for pred in chain(*predictions)])\n                labels = np.array([label for label in chain(*labels)])\n                labels[np.array(chain(*batch[\"attention_mask\"])) == 0] = -100\n                preds, refs = get_labels(predictions, labels)\n                metric.add_batch(\n                    predictions=preds,\n                    references=refs,\n                )\n\n            # # evaluate also on leftover examples (not divisible by batch_size)\n            # num_leftover_samples = len(eval_dataset) % total_batch_size\n\n            # # make sure leftover batch is evaluated on one device\n            # if num_leftover_samples > 0 and jax.process_index() == 0:\n            #     # take leftover samples\n            #     batch = eval_dataset[-num_leftover_samples:]\n            #     batch = {k: np.array(v) for k, v in batch.items()}\n\n            #     labels = batch.pop(\"labels\")\n            #     predictions = eval_step(unreplicate(state), batch)\n            #     labels = np.array(labels)\n            #     labels[np.array(batch[\"attention_mask\"]) == 0] = -100\n            #     preds, refs = get_labels(predictions, labels)\n            #     metric.add_batch(\n            #         predictions=preds,\n            #         references=refs,\n            #     )\n            \n            eval_metrics = compute_metrics()\n            epochs.write(\n                f\"Step... ({eval_steps}\/{total_eval_steps} | Val Loss: prc: {eval_metrics['precision']}, rec: {eval_metrics['recall']}, f1: {eval_metrics['f1']}, acc: {eval_metrics['accuracy']})\"\n            )\n            # print(eval_metrics)\n            eval_steps = eval_steps+eval_stp\n        if (cur_step % step_per_epoch == 0 and cur_step > 0) or (cur_step == total_steps):\n            \n            # save checkpoint after each epoch and push checkpoint to the hub\n            if jax.process_index() == 0:\n                params = jax.device_get(unreplicate(state.params))\n            if best < float(eval_metrics['f1']):\n                model.save_pretrained('.\/model', params=params)\n                tokenizer.save_pretrained('.\/token')\n                best = float(eval_metrics['f1'])\n                \n    epochs.desc = f\"Epoch ... {epoch + 1}\/{num_train_epochs}\"","7427da28":"# from IPython.display import FileLink\n# FileLink(r'.\/model\/config.json')","689d9fb2":"# JAX & FLAX for Token Classification","e16c3bf5":"# Dataset","87608650":"# Install dependencies","041f1a81":"## Create the initial train state","08cd6e83":"##### Data processing taken from here: [https:\/\/www.kaggle.com\/thedrcat\/feedback-prize-huggingface-baseline-training](http:\/\/)\n##### The code was based on the following works: [https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface](http:\/\/)\n#####                               [https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/flax\/token-classification\/run_flax_ner.py](http:\/\/)","cd3dc70b":"# Training and evaluation loop","28ee8421":"There are 8 cores in TPUv3-8, so the effective `batch_size = 8 * per_device_batch_size`","3c88d750":"# Model\nWe have a regression problem at hand so the model just needs to output 1 number.","16b86b43":"> #### This is the basic transformer training code for token classification. The advantage of using JAX and FLAX is the speed of learning - it takes 30 minutes for one epoch. To improve the result, you can modify the model to accept additional data.","f75ecfe6":"## Loss and eval functions\nThe standard loss function for regression problems is the MSE loss. The book by Bishop has an additional 0.5 term, but we're skipping in that without loss of generality. That term just scales the loss by a constant factor and doesn't have an impact on the gradients (other than scaling)."}}