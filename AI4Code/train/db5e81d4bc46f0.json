{"cell_type":{"55fafdc8":"code","db6d6438":"code","dacec938":"code","58dc467d":"code","c4f15b36":"code","ac5c923f":"code","ce25238c":"code","6572c900":"code","334fdb08":"code","88321017":"code","4681e05d":"code","4b76a5f6":"code","734bff0d":"code","f3994ad3":"code","d17d2cae":"code","188dbc2d":"markdown","2824de5c":"markdown","bba09274":"markdown","c8d2abb6":"markdown","22467711":"markdown","bd610339":"markdown"},"source":{"55fafdc8":"# importing the libraries \nimport pandas as pd\nimport numpy as np\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.stem import PorterStemmer\nimport nltk.tokenize as tokenize\nimport re\nimport string\nfrom collections import defaultdict","db6d6438":"data = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")","dacec938":"data.head()","58dc467d":"# We can see that the data consisits of 50000 reviews , with equal number of positve and negative reviews\nprint(data.shape)\ndata.sentiment.value_counts()","c4f15b36":"data.isnull().sum()","ac5c923f":"def process_review(review):\n  \n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    \n    review = review.lower()\n \n    review_tokens = tokenize.wordpunct_tokenize(review)\n    \n\n    review_clean = []\n    for word in review_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n            word not in string.punctuation):  # remove punctuation\n            stem_word = stemmer.stem(word)  # stemming word\n            review_clean.append(stem_word)\n\n    return review_clean","ce25238c":"def count_reviews(reviews, sentiment):\n \n    vocab_c = defaultdict(int)\n    for y, review in zip(sentiment, reviews):\n        for word in process_review(review):\n            pair = (word,y)\n\n            if pair in vocab_c:\n                vocab_c[pair] += 1\n\n            else:\n                vocab_c[pair] = 1\n\n    return vocab_c","6572c900":"freqs = count_reviews(data.review[:35000], data.sentiment[:35000])","334fdb08":"def train_naive_bayes(freqs, train_x, train_y):\n\n    loglikelihood = {}\n    logprior = 0\n\n\n    vocab = set([pair[0] for pair in freqs.keys()])\n    V = len(vocab)\n\n    N_pos = N_neg = 0\n    for pair in freqs.keys():\n        if pair[1] == 'positive':\n\n            N_pos += freqs[pair]\n\n        else:\n\n            N_neg += freqs[pair]\n\n\n    logprior = np.log ( (train_y == 'positive').sum() \/ (train_y == 'negative').sum())\n\n    for word in vocab:\n        freq_pos = freqs[(word,'positive')]\n        freq_neg = freqs[(word,'negative')]\n\n        p_w_pos = freq_pos + 1 \/ (N_pos + V)\n        p_w_neg = freq_neg + 1 \/ (N_neg + V)\n\n        loglikelihood[word] = np.log(p_w_pos \/ p_w_neg)\n\n\n    return logprior, loglikelihood","88321017":"logprior, loglikelihood = train_naive_bayes(freqs,data.review[:35000], data.sentiment[:35000])\nprint(logprior)\nprint(len(loglikelihood))","4681e05d":"def naive_bayes_predict(review, logprior, loglikelihood):\n\n    word_l = process_review(review)\n\n    p = 0\n\n    p += logprior\n\n    for word in word_l:\n\n        if word in loglikelihood:\n            p += loglikelihood[word]\n\n\n    return p","4b76a5f6":"p = naive_bayes_predict(data.review[42090], logprior, loglikelihood)\nprint(data.review[42090])\nprint(p)\nprint(data.sentiment[42090])","734bff0d":"def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n \n    accuracy = 0  \n\n    y_hats = []\n    for review in test_x:\n        if naive_bayes_predict(review, logprior, loglikelihood) > 0:\n            y_hat_i = 1\n        else:\n            y_hat_i = 0\n\n        y_hats.append(y_hat_i)    \n\n    accuracy = (((test_y[0] == np.array(y_hats))).sum()) \/ len(test_y[0])\n\n\n    return accuracy","f3994ad3":"Accuracy = test_naive_bayes(data.review[35000:50000], pd.factorize(data.sentiment[35000:50000]), logprior, loglikelihood)","d17d2cae":"print(Accuracy)","188dbc2d":"Now we predict whether the sentiment is  postive (p >0) or negative (p<0) . p is more of posterior probability of review being postive given the word divided by the posterior probability of review being negative given the word . If the posterior probability of review being positive is more than the negative , the fraction becomes more than 1 , and log(fraction) would be greater than 0 (p>0","2824de5c":"There are a couple of steps for training the naive bayes classifer.\n* We find the  prior probability - in simple terms if we had picked a review on random from the group of reviews with no specific info , what is probability if the sentiment is positive vs negative.So probabilty of review being positive is number of positive reviews in data ,divided by total reviews\n$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$\n* We find the likelhood of words i.e.  \n$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\$$\n\nwhere  $P(W_{pos})$ is the probability of word given the review is positve \n$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\} $$ (with laplace smoothing)","bba09274":"Naive bayes is an algorithm that could be used for sentiment analysis. It takes very short time to train and the basic assumption is that the words in the sentence are independent of eachother.\n\n","c8d2abb6":"Then we create a function , where we store the counts of the words for the particular classes . The freq dict. created will be later used in the training.","22467711":"We get to an accuracy of 82% , not at all bad for a simple classifier","bd610339":"First thing we need to do is process the input , so we can make useful inputs to the model-\n* Lowercase the reviews.\n* remove all the stopwords ( common words lke the ,a etc) and punctuations from the reviews.\n* stem the words , we dont want to count dance , danced , dancing as different words , so when we stem it , the output becomes danc for all the three, and is only taken once in the vocab building.\n\n"}}