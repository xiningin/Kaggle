{"cell_type":{"03b6e46b":"code","e233a764":"code","a998ada3":"code","1fba6f6f":"code","7699c04e":"code","c0af9352":"code","1e3ca2fe":"code","fd8941dd":"code","dfb53501":"code","00809d2d":"code","24f8741d":"code","d58ce8b1":"code","07a5942e":"code","c2d606c1":"code","d7e5ed1e":"code","ce695c34":"code","3456dd40":"code","c690b6e6":"code","2c099d76":"code","a7cf7e7b":"code","412a9749":"markdown","c798baf7":"markdown","3df66c57":"markdown","f01f217d":"markdown","6e30d3a3":"markdown","9f3f92ac":"markdown","b8aebeed":"markdown","d38b71ea":"markdown","468dff84":"markdown","3d112bba":"markdown","3d28b6b1":"markdown","a2b56cfa":"markdown","19b009a5":"markdown","5aad5d14":"markdown","ae5c04b2":"markdown","d6949495":"markdown","be0abaa2":"markdown","d0b379c1":"markdown","3f759133":"markdown"},"source":{"03b6e46b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e233a764":"# Importing libraries\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nimport tensorflow as tf\nfrom tensorflow import keras","a998ada3":"df = pd.read_csv('..\/input\/social-network-ads\/Social_Network_Ads.csv')\ndf.info()\ndf.describe()","1fba6f6f":"sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot = True)\nplt.show()","7699c04e":"plt.figure(figsize=(25,10))\ndf.corr()['Purchased'].sort_values(ascending = False).plot(kind='bar')\nplt.show()","c0af9352":"age_bins = range(15,75,5)\nmedians_by_age_group = df.groupby([\"Purchased\",pd.cut(df['Age'], age_bins)]).median()\nmedians_by_age_group\n\nmedians_by_age_group.index = medians_by_age_group.index.set_names(['Purchased', 'Age_group'])\nmedians_by_age_group.reset_index(inplace=True)\n\nfig, ax = plt.subplots(figsize=(12,5))\nsns.barplot(x='Age_group', y='EstimatedSalary',hue=\"Purchased\", data=medians_by_age_group,\n            palette=['#cc66ff','#0066ff'],\n            alpha=0.7,edgecolor='k',\n            ax=ax)\nax.set_title('Median estimated salary of customers based on Age who purchased or not')\nax.set_xlabel('Age group')\nplt.show()","1e3ca2fe":"plt.figure(figsize = (12,8))\nplt.grid(True)\nax = sns.countplot(x='Purchased', data=df, palette='Spectral_r')\nfor p in ax.patches:\n        ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+0.5))","fd8941dd":"X=df.iloc[:,0:-1]\ny=df.iloc[:,-1]\n\nscaler = MinMaxScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.3,random_state=2,stratify=y)","dfb53501":"model=SVC()\nmodel.fit(X_train,y_train)\nprint(f' Training Accuracy is:- {model.score(X_train,y_train)}')\nf'Test Accuracy is:- {model.score(X_test,y_test)}'","00809d2d":"k = range(1,20)\ntrainingAccuracy = []\ntestAccuracy=[]\nfor i in k:\n    knn = KNeighborsClassifier(n_neighbors=i,n_jobs=15,p=1,weights='distance')\n    knn.fit(X_train,y_train)\n    trainingacc = knn.score(X_train,y_train)\n    trainingAccuracy.append(trainingacc)\n    testAccuracy.append(knn.score(X_test,y_test))\n\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10,4))\nplt.xlabel(\"value of K\")\nplt.ylabel(\"Accuracy of test and training\")\nplt.title(\"Select best value of k\")\nplt.plot(k,trainingAccuracy)\nplt.plot(k,testAccuracy)\n  #axes[0].legend(['loss','val_loss'])\naxes.legend([\"Training Accurracy\",\"Test Accuracy\"])\nprint(\"\\n Best Test accuracy is:- \",max(testAccuracy))","24f8741d":"from sklearn.neighbors import KNeighborsClassifier\nknn_model=KNeighborsClassifier(n_jobs=15,n_neighbors=3,p=2,weights='uniform')\nknn_model.fit(X_train,y_train)\nprint(f' Training Accuracy {knn_model.score(X_train,y_train)}')\nf' Testing Accuracy {knn_model.score(X_test,y_test)}'","d58ce8b1":"from matplotlib.colors import ListedColormap\nX_set, y_set = scaler.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\nplt.contourf(X1, X2, knn_model.predict(scaler.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('K-NN (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","07a5942e":"from matplotlib.colors import ListedColormap\nX_set, y_set = scaler.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\nplt.contourf(X1, X2, knn_model.predict(scaler.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('K-NN (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","c2d606c1":"k_range = list(range(1,50))\nweight_options = [\"uniform\", \"distance\"]\npe=[1,2]\n\nparam_grid = dict(n_neighbors = k_range, weights = weight_options,p=pe)\nknn = KNeighborsClassifier()\nknngrid = GridSearchCV(knn, param_grid, cv = 10, scoring = 'accuracy',n_jobs=15)\nknngrid.fit(X_train,y_train)\n\nprint (\"Best score on 10 folds split Data on Train split is :- \",knngrid.best_score_)\nprint (\"\\n Best Param:- \",knngrid.best_params_)\nprint (\"\\n Best KNN Metric:- \", knngrid.best_estimator_)\n\nprint(f' \\n Training Accuracy {knngrid.score(X_train,y_train)}')\nf'Test Accuracy {knngrid.score(X_test,y_test)}'","d7e5ed1e":"y_predicted = knngrid.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_predicted)\nimport seaborn as sn\nplt.figure(figsize = (10,7))\nsn.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","ce695c34":"from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n\nlog_model.fit(X_train, y_train)\nprint(f' Training Accuracy {log_model.score(X_train,y_train)}')\nf'Test Accuracy {log_model.score(X_test,y_test)}'","3456dd40":"folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4)\nparam_grid = [\n        {\n            'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n            'solver' : ['lbfgs', 'sgd', 'adam'],            \n        }\n       ]\nclf = GridSearchCV(MLPClassifier(), param_grid, cv=folds,\n                           scoring='accuracy',n_jobs=-1,verbose = 1,\n)\nclf.fit(X_train, y_train)\nprint(f' Training Accuracy {clf.score(X_train,y_train)}')\nf'Test Accuracy {clf.score(X_test,y_test)}'","c690b6e6":"cv_results = pd.DataFrame(clf.cv_results_)\ncv_results[cv_results.rank_test_score<5]\n#print the optimum value of hyperparameters\nprint('Best hyperparameters: ', clf.best_params_)","2c099d76":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('accuracy')>= 0.95):\n      print(\"\\nReached 97% accuracy so cancelling training!\")\n      self.model.stop_training = True\ncallbacks = myCallback()\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(800, activation=tf.nn.relu),\n  tf.keras.layers.Dense(256, activation=tf.nn.relu),\n  tf.keras.layers.Dense(256, activation=tf.nn.relu),\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=500,batch_size=32, callbacks=[callbacks])","a7cf7e7b":"print(\"Accuracy on test data is \",model.evaluate(X_test, y_test))\nprint(\"Predicted value of ytest[4] is \",np.argmax(y_predicted[4]))\nprint(\"Actual Predicted value is:- \",y_test[4])","412a9749":"### <b><u>Best params used for MLPClassifier<\/u><\/b>","c798baf7":"### <b><u>Model 02:- KNN<\/u><\/b> ","3df66c57":"### <u><b>Model 01:- Support Vector machine(SVM)<\/b><\/u>","f01f217d":"### Checking the Purchased power in different Age groups ","6e30d3a3":"## Section02- EDA ","9f3f92ac":"### <b><u>Conclusion:-<\/u><\/b>\n* KNN proves to be the Best model with good accuracy on test among all the models used above.","b8aebeed":"## Section 03:- Scaling and Splitting the Dataset in to X_train and X_test.","d38b71ea":"#### <b><u>Checking confusion matrix for the above Hyperparameter tuned model<\/u><\/b>","468dff84":"### Checking the correlation","3d112bba":"### <b><u> Neural network with Dense layer and defining the Stop criteria on reachign the Accuracy threshold on Train data <\/u><\/b>","3d28b6b1":"### Checking the Correlation on the Target variable using barchart","a2b56cfa":"## Section 01:- Reading Data and Checking the Meta information","19b009a5":"## Section 04:- Model Building","5aad5d14":"#### <b><u>KNN With Hyper parameter Tuning<\/u><\/b>","ae5c04b2":"#### <b><u> Visualising the Classification and Misclassification of Train and Test Prediction <\/u><\/b>","d6949495":"### Checking the target Variable using CountPlot","be0abaa2":"### <b><u> Modal 03:- Logistic Regression <\/u><\/b>","d0b379c1":"### <b><u>Model 04:- Neural Network<\/u><\/b>","3f759133":"#### <b><u>With the help of above chart inputing the values for KNN<\/u><\/b>"}}