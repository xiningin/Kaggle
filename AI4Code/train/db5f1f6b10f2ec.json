{"cell_type":{"5522e37e":"code","81df79a2":"code","be0fe206":"code","1d3deffa":"code","1929a3e2":"code","4cad17a7":"code","26d69901":"code","ff5504d4":"code","752d0f22":"code","a8c624bc":"code","3bbf6b30":"code","1e057c71":"code","f1a9d523":"code","4fd5adf3":"code","f80b1923":"code","d7eb668d":"code","4a9fa388":"code","310f3638":"code","4811b884":"code","614ee870":"code","d8901896":"code","82ad36b4":"code","86b4bf7f":"code","1be64a7d":"code","264b2e0b":"code","e45806a0":"code","4ab6f70e":"code","7795dd3f":"code","3f36c4ee":"code","260ba0f1":"code","901436d7":"code","6de0689e":"code","28cf292f":"code","34ca6586":"code","0a036db5":"code","fc9b8df5":"code","b10b6dc0":"code","9324e5b2":"code","4db06452":"code","85ee5880":"code","88dd1b53":"code","4f7bee71":"code","be287318":"code","f3792e79":"code","75704fe4":"code","7133bbe3":"code","fab5c719":"code","821ca1ec":"code","48b073c0":"code","71d0c43b":"code","29362a4d":"code","c5ba49e3":"code","3e3b7d39":"markdown","212a5b09":"markdown","d890c89a":"markdown","e8ad28c3":"markdown","b6cc5032":"markdown","8e35290f":"markdown","3afd4a73":"markdown","fe07351f":"markdown","ed933422":"markdown","39552477":"markdown","871caada":"markdown","c255c69d":"markdown","e0aa8f8a":"markdown","eef65f44":"markdown","c2dd5314":"markdown","37cfe3b5":"markdown","8f68b385":"markdown","f268e76b":"markdown","dc1f9a18":"markdown","f1b902b3":"markdown","8ee619d5":"markdown","464934f3":"markdown","5a46c344":"markdown","75d35a52":"markdown","6d65f35e":"markdown","8a6818fa":"markdown","085d14e7":"markdown","270fc4f7":"markdown","4d21e6f4":"markdown","baccd70c":"markdown","bd5998cf":"markdown","208efa63":"markdown","1f3676f6":"markdown","03e11aaf":"markdown","64aea7f4":"markdown","808a463b":"markdown","3daaf780":"markdown","77cbac78":"markdown","ab263811":"markdown","f45dd599":"markdown","9fd37d8a":"markdown","1a069682":"markdown","7b296523":"markdown","a4854f6b":"markdown","8ba2c614":"markdown","b3ff9064":"markdown","f98f4f9e":"markdown","985ea268":"markdown"},"source":{"5522e37e":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\nimport seaborn as sns","81df79a2":"test_labels = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv')\nprint(test_labels.shape)\ntest_labels.head(2)","be0fe206":"test_labels = test_labels[(test_labels[['toxic','severe_toxic', 'obscene', 'threat', \n                                        'insult', 'identity_hate']] != -1).all(axis=1)]\nprint(test_labels.shape)\ntest_labels.head(2)","1d3deffa":"df_test = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv')\nprint(df_test.shape)\ndf_test.head(2)","1929a3e2":"# merge with an inner join\ntest = pd.merge(test_labels, df_test, on='id', how='inner')\nprint(test.shape)\ntest.head(2)","4cad17a7":"train = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ntrain.head(2)","26d69901":"# check the number of records\nprint('The dataset contains', train.shape[0], 'records and', train.shape[1], 'columns.')","ff5504d4":"# check that there are no missing values in either training set\nprint('The dataset has', train.isna().sum().sum(), 'missing values.')","752d0f22":"# check if there are any duplicates\nprint('The dataset has', train.duplicated().sum(), 'duplicates.')","a8c624bc":"train['comment_text'][4]","3bbf6b30":"train['comment_text'][13]","1e057c71":"train['comment_text'][1392]","f1a9d523":"# creating a list of column names\ncolumns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]","4fd5adf3":"# to_frame() converts series to DataFrame\nfrequency = train[columns].sum().to_frame().rename(columns={0: 'count'}).sort_values('count')\nfrequency.plot.barh(y='count', title='Count of Comments', figsize=(8, 5));","f80b1923":"train.groupby(columns).size().sort_values(ascending=False).reset_index()\\\n                      .rename(columns={0: 'count'}).head(15)","d7eb668d":"fig, ax = plt.subplots(figsize=(10, 6))\nfig.suptitle('Correlation Matrix')\nsns.heatmap(train[columns].corr(), annot=True, cmap=\"YlGnBu\", linewidths=.5, ax=ax);","4a9fa388":"from matplotlib_venn import venn2\nfrom matplotlib_venn import venn3\nfrom matplotlib_venn import venn3_circles\nfrom matplotlib_venn import venn2_circles","310f3638":"# build combinations\na = train[(train['toxic']==1) & (train['insult']==0) & (train['obscene']==0)].shape[0]\nb = train[(train['toxic']==0) & (train['insult']==1) & (train['obscene']==0)].shape[0]\nc = train[(train['toxic']==0) & (train['insult']==0) & (train['obscene']==1)].shape[0]\n\nab = train[(train['toxic']==1) & (train['insult']==1) & (train['obscene']==0)].shape[0]\nac = train[(train['toxic']==1) & (train['insult']==0) & (train['obscene']==1)].shape[0]\nbc = train[(train['toxic']==0) & (train['insult']==1) & (train['obscene']==1)].shape[0]\n\nabc = train[(train['toxic']==1) & (train['insult']==1) & (train['obscene']==1)].shape[0]\n\n# plot venn diagrams\nplt.figure(figsize=(8, 8))\nplt.title(\"Venn Diagram for 'toxic', 'insult' and 'obscene' comments\")\n\nv=venn3(subsets=(a, b, c, ab, ac, bc, abc), \n        set_labels=('toxic', 'insult', 'obscene'))\n\nvc=venn3_circles(subsets=(a, b, c, ab, ac, bc, abc),\n                linestyle='dashed', linewidth=1, color=\"grey\")\nvc[1].set_lw(8.0)\nvc[1].set_ls('dotted')\nvc[1].set_color('skyblue')\n\nplt.show();","4811b884":"# build combinations\na   = train[(train['toxic']==1) & (train['severe_toxic']==0)].shape[0]\nb   = train[(train['toxic']==0) & (train['severe_toxic']==1)].shape[0]\nab = train[(train['toxic']==1) & (train['severe_toxic']==1)].shape[0]\n\n# plot venn diagrams\nplt.figure(figsize=(8, 8))\nplt.title(\"Venn Diagram for 'toxic' and 'severe_toxic' comments\")\nv=venn2(subsets=(a, b, ab), set_labels=('toxic', 'severe_toxic'))\n\nc=venn2_circles(subsets=(a, b, ab),\n                linestyle='dashed', linewidth=1, color=\"grey\")\nc[1].set_lw(8.0)\nc[1].set_ls('dotted')\nc[1].set_color('skyblue')\n\nplt.show();","614ee870":"# import necessary libraries\nfrom wordcloud import WordCloud\nfrom collections import Counter\n\nimport re\nimport string\n\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')","d8901896":"# define an empty dictionary\nword_counter = {}\n\n# writing a clean_text function\ndef clean_text(text):\n    text = re.sub('[{}]'.format(string.punctuation), ' ', text.lower())\n    return ' '.join([word for word in text.split() if word not in (stop)])","82ad36b4":"# iterating through all columns in the dataset...\nfor col in columns:    \n    text = Counter()        \n    \n    # ... applying the clean-function to each column's comments and ...\n    train[train[col] == 1]['comment_text'].apply(lambda t: text.update(clean_text(t).split()))\n    \n    # ... combining all to one dataframe\n    word_counter[col] = pd.DataFrame.from_dict(text, orient='index')\\\n                                        .rename(columns={0: 'count'})\\\n                                        .sort_values('count', ascending=False)","86b4bf7f":"# iterating through new df word_counter and creating a WordCloud for each column\nfor col in word_counter:    \n    wc_list = word_counter[col]\n    \n    wordcloud = WordCloud(background_color='white', max_words=150, max_font_size=100, random_state=4)\\\n                          .generate_from_frequencies(wc_list.to_dict()['count'])\n\n    fig = plt.figure(figsize=(10, 8))\n    plt.title('\\n' + col + '\\n', fontsize=20, fontweight='bold')\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.axis('off')\n    plt.show()","1be64a7d":"# importing libraries\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import text, sequence","264b2e0b":"X_train = train[\"comment_text\"].values\nX_test  = test[\"comment_text\"].values\n\ny_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\ny_test  = test[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values","e45806a0":"# tokenizing the data\ntokenizer = Tokenizer(num_words=20000)\ntokenizer.fit_on_texts(list(X_train))\n\n# turning the tokenized text into sequences\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test  = tokenizer.texts_to_sequences(X_test)\n\n# padding the sequences\nX_train = sequence.pad_sequences(X_train, maxlen=200)\nX_test  = sequence.pad_sequences(X_test,  maxlen=200)\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape: ', X_test.shape)","4ab6f70e":"from keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.models import Model, Input, Sequential\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, SpatialDropout1D, Activation\nfrom keras.layers import Conv1D, Bidirectional, GlobalMaxPool1D, MaxPooling1D, BatchNormalization\nfrom keras.optimizers import Adam","7795dd3f":"# number of unique words we want to use (or: number of rows in incoming embedding vector)\nmax_features = 20000 \n\n# max number of words in a comment to use (or: number of columns in incoming embedding vector)\nmax_len = 200 \n\n# dimension of the embedding variable (or: number of rows in output of embedding vector)\nembedding_dims = 128","3f36c4ee":"# instantiate NN model\nbase_model = Sequential()\n\n# add embedding layer \nbase_model.add(Embedding(input_dim=max_features, input_length=max_len,\n                         output_dim=embedding_dims))\n\n# add pooling layer \n# ... which will extract features from the embeddings of all words in the comment\nbase_model.add(GlobalMaxPool1D())\n\n# add dense layer to produce an output dimension of 50 and apply relu activation\nbase_model.add(Dense(50, activation='relu'))\n\n# set the regularizing dropout layer to drop out 30% of the nodes\nbase_model.add(Dropout(0.3))\n\n# finally add a dense layer\n# ... which projects output into six units and squash it with sigmoid activation\nbase_model.add(Dense(6, activation='sigmoid'))","260ba0f1":"base_model.compile(loss='binary_crossentropy',\n                   optimizer=Adam(0.01), metrics=['accuracy'])\n\n# check the model with all our layers\nbase_model.summary()","901436d7":"base_hist = base_model.fit(X_train, y_train, batch_size=32, \n                           epochs=3, validation_split=0.1)","6de0689e":"# evaluate the algorithm on the test dataset\nbase_test_loss, base_test_auc = base_model.evaluate(X_test, y_test, batch_size=32)\nprint('Test Loss:    ', base_test_loss)\nprint('Test Accuracy:', base_test_auc)","28cf292f":"# instantiate CNN model\ncnn_model = Sequential()\n\n# add embedding layer \ncnn_model.add(Embedding(input_dim=max_features, input_length=max_len,\n                        output_dim=embedding_dims))\n \n# set the dropout layer to drop out 50% of the nodes\ncnn_model.add(SpatialDropout1D(0.5))\n\n# add convolutional layer that has ...\n# ... 100 filters with a kernel size of 4 so that each convolution will consider a window of 4 word embeddings\ncnn_model.add(Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'))\n\n# add normalization layer\ncnn_model.add(BatchNormalization())\n\n# add pooling layer \ncnn_model.add(GlobalMaxPool1D())\n\n# set the dropout layer to drop out 50% of the nodes\ncnn_model.add(Dropout(0.5))\n\n# add dense layer to produce an output dimension of 50 and using relu activation\ncnn_model.add(Dense(50, activation='relu'))\n\n# finally add a dense layer\ncnn_model.add(Dense(6, activation='sigmoid'))","34ca6586":"cnn_model.compile(loss='binary_crossentropy',\n                  optimizer=Adam(0.01),\n                  metrics=['accuracy'])\n\ncnn_model.summary()","0a036db5":"cnn_hist = cnn_model.fit(X_train, y_train, batch_size=32, \n                         epochs=3, validation_split=0.1)","fc9b8df5":"cnn_test_loss, cnn_test_auc = cnn_model.evaluate(X_test, y_test, batch_size=32)\nprint('Test Loss:    ', cnn_test_loss)\nprint('Test Accuracy:', cnn_test_auc)","b10b6dc0":"# instantiate RNN model\nrnn_model = Sequential()\n\n# add embedding layer \nrnn_model.add(Embedding(input_dim=max_features, input_length=max_len,\n                        output_dim=embedding_dims))\n\n# set the dropout layer to drop out 50% of the nodes\nrnn_model.add(SpatialDropout1D(0.5))\n\n# add bidirectional layer and pass in an LSTM()\nrnn_model.add(Bidirectional(LSTM(25, return_sequences=True)))\n\n# add normalization layer\nrnn_model.add(BatchNormalization())\n\n# add pooling layer \nrnn_model.add(GlobalMaxPool1D())\n\n# set the dropout layer to drop out 50% of the nodes\nrnn_model.add(Dropout(0.5))\n\n# add dense layer to produce an output dimension of 50 and using relu activation\nrnn_model.add(Dense(50, activation='relu'))\n\n# finally add a dense layer\nrnn_model.add(Dense(6, activation='sigmoid'))","9324e5b2":"rnn_model.compile(loss='binary_crossentropy',\n                  optimizer=Adam(0.01),\n                  metrics=['accuracy'])\n\nrnn_model.summary()","4db06452":"rnn_hist = rnn_model.fit(X_train, y_train, batch_size=32, \n                          epochs=3, validation_split=0.1)","85ee5880":"rnn_test_loss, rnn_test_auc = rnn_model.evaluate(X_test, y_test, batch_size=32)\nprint('Test Loss:    ', rnn_test_loss)\nprint('Test Accuracy:', rnn_test_auc)","88dd1b53":"# load the glove840B embedding\n\nembeddings_index = dict()\nf = open('..\/input\/glove840b300dtxt\/glove.840B.300d.txt')\n\nfor line in f:\n    # Note: use split(' ') instead of split() if you get an error\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Loaded %s word vectors.' % len(embeddings_index))","4f7bee71":"# create a weight matrix\nembedding_matrix = np.zeros((len(tokenizer.word_index)+1, 300))\n\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","be287318":"# instantiate pretrained glove model\nglove_model = Sequential()\n\n# add embedding layer \nglove_model.add(Embedding(input_dim =embedding_matrix.shape[0], input_length=max_len,\n                          output_dim=embedding_matrix.shape[1], \n                          weights=[embedding_matrix], trainable=False))\n \n# set the dropout layer to drop out 50% of the nodes\nglove_model.add(SpatialDropout1D(0.5))\n\n# add convolutional layer that has ...\n# ... 100 filters with a kernel size of 4 so that each convolution will consider a window of 4 word embeddings\nglove_model.add(Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'))\n\n# add normalization layer\nglove_model.add(BatchNormalization())\n\n# add pooling layer \nglove_model.add(GlobalMaxPool1D())\n\n# set the dropout layer to drop out 50% of the nodes\nglove_model.add(Dropout(0.5))\n\n# add dense layer to produce an output dimension of 50 and using relu activation\nglove_model.add(Dense(50, activation='relu'))\n\n# finally add a dense layer\nglove_model.add(Dense(6, activation='sigmoid'))","f3792e79":"glove_model.compile(loss='binary_crossentropy',\n                    optimizer=Adam(0.01),\n                    metrics=['accuracy'])\n\nglove_model.summary()","75704fe4":"glove_hist = glove_model.fit(X_train, y_train, batch_size=32, \n                             epochs=3, validation_split=0.1)","7133bbe3":"glove_test_loss, glove_test_auc = glove_model.evaluate(X_test, y_test, batch_size=32)\nprint('Test Loss:    ', glove_test_loss)\nprint('Test Accuracy:', glove_test_auc)","fab5c719":"# instantiate pretrained glove model\nglove_2_model = Sequential()\n\n# add embedding layer \nglove_2_model.add(Embedding(input_dim =embedding_matrix.shape[0], input_length=max_len,\n                          output_dim=embedding_matrix.shape[1], \n                          weights=[embedding_matrix], trainable=False))\n\n# set the dropout layer to drop out 50% of the nodes\nglove_2_model.add(SpatialDropout1D(0.5))\n\n# add bidirectional layer and pass in an LSTM()\nglove_2_model.add(Bidirectional(LSTM(25, return_sequences=True)))\n\n# add normalization layer\nglove_2_model.add(BatchNormalization())\n\n# add pooling layer \nglove_2_model.add(GlobalMaxPool1D())\n\n# set the dropout layer to drop out 50% of the nodes\nglove_2_model.add(Dropout(0.5))\n\n# add dense layer to produce an output dimension of 50 and using relu activation\nglove_2_model.add(Dense(50, activation='relu'))\n\n# finally add a dense layer\nglove_2_model.add(Dense(6, activation='sigmoid'))","821ca1ec":"glove_2_model.compile(loss='binary_crossentropy',\n                    optimizer=Adam(0.01),\n                    metrics=['accuracy'])\n\nglove_2_model.summary()","48b073c0":"glove_2_hist = glove_2_model.fit(X_train, y_train, batch_size=32, \n                                 epochs=3, validation_split=0.1)","71d0c43b":"glove_2_test_loss, glove_2_test_auc = glove_2_model.evaluate(X_test, y_test, batch_size=32)\nprint('Test Loss:    ', glove_2_test_loss)\nprint('Test Accuracy:', glove_2_test_auc)","29362a4d":"# concat all training, validation and testing accuracy scores\naccuracy_nn = ['Plain NN', \n               np.mean(base_hist.history['acc']), \n               np.mean(base_hist.history['val_acc']), \n               base_test_auc]\n\naccuracy_cnn = ['CNN', \n                np.mean(cnn_hist.history['acc']), \n                np.mean(cnn_hist.history['val_acc']), \n                cnn_test_auc]\n\naccuracy_rnn = ['RNN', \n                np.mean(rnn_hist.history['acc']), \n                np.mean(rnn_hist.history['val_acc']), \n                rnn_test_auc]\n\naccuracy_glove = ['Glove CNN', \n                  np.mean(glove_hist.history['acc']), \n                  np.mean(glove_hist.history['val_acc']), \n                  glove_test_auc]\n\naccuracy_glove_2 = ['Glove RNN', \n                    np.mean(glove_2_hist.history['acc']), \n                    np.mean(glove_2_hist.history['val_acc']), \n                    glove_2_test_auc]\n\n# create dataframe\ncomparison = pd.DataFrame([accuracy_nn])\n# append all other scores\ncomparison = comparison.append([accuracy_cnn, accuracy_rnn, accuracy_glove, accuracy_glove_2])","c5ba49e3":"# beautify the new dataframe\ncomparison.columns = ['Algorithm', 'Training Accuracy', 'Validation Accuracy', 'Testing Accuracy']\ncomparison.set_index(['Algorithm'], inplace=True)\ncomparison","3e3b7d39":"## 1. Obtaining and Viewing the Data\n<a id='1. Obtaining and Viewing the Data' >","212a5b09":"All resources used in this project are listed below.\n\n**General Explanations**:\n- https:\/\/www.quora.com\/How-can-we-use-Neural-Network-for-text-classification\n- https:\/\/medium.com\/ravenprotocol\/everything-you-need-to-know-about-neural-networks-6fcc7a15cb4\n- https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/\n- https:\/\/keras.io\/models\/sequential\/\n- https:\/\/medium.com\/jatana\/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f\n- https:\/\/medium.com\/jatana\/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f\n\n**Plain Neural Networks**:\n- https:\/\/www.depends-on-the-definition.com\/classify-toxic-comments-on-wikipedia\/\n\n**Batch Normalization**:\n- https:\/\/machinelearningmastery.com\/how-to-accelerate-learning-of-deep-neural-networks-with-batch-normalization\/\n- https:\/\/keras.io\/layers\/normalization\/\n\n**Convolutional Neural Networks**:\n- http:\/\/www.wildml.com\/2015\/11\/understanding-convolutional-neural-networks-for-nlp\/#more-348\n- https:\/\/www.researchgate.net\/publication\/323444293_Convolutional_Neural_Networks_for_Toxic_Comment_Classification\n\n**Recurrent Neural Networks**:\n- http:\/\/www.wildml.com\/2015\/09\/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns\/\n- https:\/\/www.kaggle.com\/gaussmake1994\/rnn-over-fasttext-embeddings-baseline-0-48-lb\n\n**Pretrained Word Embeddings**:\n- https:\/\/jovianlin.io\/embeddings-in-keras\/\n- https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/\n- https:\/\/www.kaggle.com\/vbookshelf\/keras-cnn-glove-early-stopping-0-048-lb\n\n**Toxic Comment Classification Challenge**:\n- https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/data\n- https:\/\/iarjset.com\/wp-content\/uploads\/2018\/10\/IARJSET.2018.597.pdf\n- https:\/\/www.kaggle.com\/joydeep29\/embedding-lstm-cnn-with-rmsprop-loss-0-07\n- https:\/\/medium.com\/@zake7749\/top-1-solution-to-toxic-comment-classification-challenge-ea28dbe75054\n- https:\/\/medium.com\/@nehabhangale\/toxic-comment-classification-models-comparison-and-selection-6c02add9d39f\n- https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/discussion\/52557","d890c89a":"It's time to wrap up everything we did:","e8ad28c3":"What does it mean to talk in an \"insulting\" or \"obscene\" or \"threatening\" way? <br> Word Clouds are a great visualization tool that helps to shed light upon these - so far hidden - comments.","b6cc5032":"The three types of comments that occur most often are:\n- toxic comments\n- obscene comments\n- insulting comments.\n\nLet's dive deeper and have a look at the number of frequent comment combinations:","8e35290f":"### 2.1. Bar Chart\n<a id='2.1. Bar Chart' >","3afd4a73":"### 2.4. Word Clouds\n<a id='2.4. Word Clouds' >","fe07351f":"**1. Test Data**","ed933422":"*Back to: <a href='#Table of contents'> Table of contents <\/a>*\n### 4.4. CNN with Pre-Trained GloVe Embedding\n<a id='4.4. CNN with Pre-Trained GloVe Embedding' >","39552477":"This algorithm did not improve the RNN's accuracy.","871caada":"So we have a column `id`, the comment itself in `comment_text` and six different labels qualifying the comment as `toxic`, `severe_toxic`, `obscene`, a `threat`, an `insult`, or `identity_hate`. Note that a comment can be qualified with several labels - or even with none of them!","c255c69d":"*Back to: <a href='#Table of contents'> Table of contents <\/a>*\n## 3. Preprocessing the Data\n<a id='3. Preprocessing the Data' >","e0aa8f8a":"## APPENDIX","eef65f44":"*Back to: <a href='#Table of contents'> Table of contents <\/a>*\n## 4. Evaluating Different Neural Networks\n<a id='4. Evaluating Different Neural Networks' >","c2dd5314":"Finally, we train the network with some parameters:\n\n- The **batch size** is the number of training examples in one forward\/backward pass. In general, larger batch sizes result in faster progress in training, but don't always converge as quickly. Smaller batch sizes train slower, but can converge faster. And the higher the batch size, the more memory space you\u2019ll need.\n\n- The training **epochs**\u200a are the number of times that the model is exposed to the training dataset. One epoch equals one forward pass and one backward pass of all the training examples. In general, the models improve with more epochs of training, to a point. They'll start to plateau in accuracy as they converge.","37cfe3b5":"**2. Training Data:**","8f68b385":"- Non-toxic comments, i.e. comments which have no label, are clearly ahead with 143346 comments. \n- \"Toxic\" comments then occur in different combinations within the first 15 ranks. The same holds for \"obscene\" comments as well as for \"insult\" comments. \n- Interestingly, the number of comments for each combination drops exponentially. \n\nThis leads to looking at a correlation matrix.","f268e76b":"Well, the validation accuracy is 0.02% worse than in our basic neural network.\n\nAs a side note: I first ran this algorithm **without batch normalization** and had a training accuracy of 0.9736 and a validation accuracy of 0.9748. Now, **with batch normalization**, both training and validation accuracy have increased slightly - to 0.9807 and 0.9805 respectively!","dc1f9a18":"*Back to: <a href='#Table of contents'> Table of contents <\/a>*\n## 5. Conclusions\n<a id='5. Conclusions' >","f1b902b3":"Recurrent Neural Networks (RNNs) are popular models that have shown great promise in many NLP tasks. The idea behind RNNs is to make use of sequential information. In a traditional neural network we assume that all inputs (and outputs) are independent of each other. But for many tasks, that\u2019s a very bad idea. If you want to predict the next word in a sentence, you had better know which words came before it. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being dependent on the previous computations. <br> Another way to think about RNNs is that they have a \u201cmemory\u201d which captures information about what has been calculated so far. In theory, RNNs can make use of information in arbitrarily long sequences, but in practice they are limited to looking back only a few steps.\n\n**Bidirectional RNNs** are based on the idea that the output at a given point in time may not only depend on the previous elements in the sequence, but also future elements. For example, to predict a missing word in a sequence you will want to look at the context on both the left and the right. <br> Bidirectional RNNs are quite simple. They are just two RNNs stacked on top of each other with a twist: half of the neurons start at the beginning of the data and work towards the end one step at a time, while the other half start at the end of the data and work towards the beginning at the same pace.\n\nInspiration for the setup of this RNN can be found __[here](https:\/\/medium.com\/@nehabhangale\/toxic-comment-classification-models-comparison-and-selection-6c02add9d39f)__.","8ee619d5":"*Back to: <a href='#Table of contents'> Table of contents <\/a>*\n### 4.2. Convolutional Neural Network (CNN)\n<a id='4.2. Convolutional Neural Network (CNN)' >","464934f3":"The *test comments* and the *test labels* - both in different datasets - differ in their length, as we had to drop a substantial amount of rows in the first one. Let's merge both datasets with an inner join to keep only the records with a proper label.","5a46c344":"### 2.3. Venn Diagrams\n<a id='2.3. Venn Diagrams' >","75d35a52":"The description states *\"value of -1 indicates it was not used for scoring\"*, so we will drop all of those rows.","6d65f35e":"*Back to: <a href='#Table of contents'> Table of contents <\/a>*\n## 2. Data Visualization\n<a id='2. Data Visualization' >","8a6818fa":"- When we look at the **training accuracy**, the **plain neural network** performs best.\n- When we check the **validation accuracy**, it's also the **plain neural network** that stands out.\n- But when we check how well the models classify unseen data - and therefore check the **testing accuracy** - it's the **pre-trained RNN** that does this job with the highest precision.\n\nIn this common challenge, it really came down to the decimal places. The winner reached an accuracy score of 0.9877 for a pre-trained embedding - although I'm not certain whether this is a training or validation accuracy. They also used a semi-supervised learning technique called Pseudo-Labelling!\n\nI think I came quite close to these numbers, but some minor tweaks could certainly be done to improve the third decimal place even more, such as trying different dropout patterns or numbers of nodes in certain layers. **It's not the sky that's the limit - it's the runtime!**","085d14e7":"Convolutional neural networks (CNN's) recently proved to be very effective at document classification, namely because they are able to pick out salient features (e.g. tokens or sequences of tokens) in a way that is invariant to their position within the input sequences. Simply put, a convolution is a sliding window function applied to a matrix. To set up a CNN, we have to add a convolutional layer:\n\n- A **convolutional layer**  consists of a set of \u201cfilters\u201d. These filters only take in a subset of the input data at a given time, but are applied across the full input by sweeping over it. The operations performed here are still linear, but they are generally followed by a non-linear activation function.\n\nFurthermore, I took a piece of advice I found __[here.](https:\/\/medium.com\/@zake7749\/top-1-solution-to-toxic-comment-classification-challenge-ea28dbe75054)__: *\"As many competitors pointed out, dropout and batch-normalization are the keys to prevent overfitting and dropout did a better job. By applying the dropout on the word embedding directly and behind the pooling does great regularization both on train set and test set. \"*\n\n- A **batch normalization layer** normalizes the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1. It will be added after the activation function between a convolutional and a max-pooling layer. __[This website](https:\/\/machinelearningmastery.com\/how-to-accelerate-learning-of-deep-neural-networks-with-batch-normalization\/)__ provides great advice for implementing such a layer.\n\nInspiration for the setup of this CNN can be be found __[here](https:\/\/machinelearningmastery.com\/best-practices-document-classification-deep-learning\/)__ and __[here](https:\/\/www.kaggle.com\/joydeep29\/embedding-lstm-cnn-with-rmsprop-loss-0-07)__.","270fc4f7":"## Table of contents\n<a id='Table of contents'><\/a>\n\n### <a href='#1. Obtaining and Viewing the Data'> 1. Obtaining and Viewing the Data <\/a>\n\n### <a href='#2. Data Visualization'> 2. Data Visualization <\/a>\n\n* <a href='#2.1. Bar Chart'> 2.1. Bar Chart <\/a>\n* <a href='#2.2. Correlation Matrix'> 2.2. Correlation Matrix <\/a>\n* <a href='#2.3. Venn Diagrams'> 2.3. Venn Diagrams <\/a>\n* <a href='#2.4. Word Clouds'> 2.4. Word Clouds <\/a>\n\n### <a href='#3. Preprocessing the Data'> 3. Preprocessing the Data <\/a>\n\n### <a href='#4. Evaluating Different Neural Networks'> 4. Evaluating Different Neural Networks <\/a>\n\n* <a href='#4.1. Baseline Neural Network'> 4.1. Baseline Neural Network <\/a>\n* <a href='#4.2. Convolutional Neural Network (CNN)'> 4.2. Convolutional Neural Network (CNN) <\/a>\n* <a href='#4.3. Recurrent Neural Network (RNN)'> 4.3. Recurrent Neural Network (RNN) <\/a>\n* <a href='#4.4. CNN with Pre-Trained GloVe Embedding'> 4.4. CNN with Pre-Trained GloVe Embedding <\/a>\n* <a href='#4.5. RNN with Pre-Trained GloVe Embedding'> 4.5. RNN with Pre-Trained GloVe Embedding <\/a>\n\n### <a href='#5. Conclusions'>5. Conclusions <\/a>","4d21e6f4":"Making use of weights from a Word2Vec model that has been trained for a very long time on a massive amount of text data might be a good way to improve the performance of text classification. With deep learning, more data is almost always the single best way that to improve a model's performance - and the embedded word vectors created by a Word2Vec model are no exception. For this reason, it's a good idea to load one of the top-tier, industry-standard models that has been open sourced for this exact purpose. \n\nLet's now explore the other way to add an embedding layer to our neural network by using a **pre-trained embedding layer**. We'll go with the GloVe method, first with a CNN and eventually an RNN.","baccd70c":"Next, we compile the network:\n\n- The **loss function** computes the error for a single training example. The cost function is the average of the loss functions of the entire training set. Our choice here is `categorical_crossentropy`, i.e. a multi-class logarithmic loss.\n- When we train neural networks, we usually use Gradient Descent to optimize the weights. At each iteration, we use back-propagation to calculate the derivative of the loss function with respect to each weight and subtract it from that weight. The **learning rate** determines how quickly or how slowly we want to update our weight values. It should be just high enough to take an acceptable time to converge, and just low enough to be capable of finding the local minima.\n- The **optimizer** is a search technique, which is used to update weights in the model. Our choice here is Adaptive Moment Estimation (`Adam`) which uses adaptive learning rates.\n- Performance **metrics** are used to measure the performance of the neural network. Accuracy, loss, validation accuracy, validation loss, mean absolute error, precision, recall and f1 score are some performance metrics. Our choice here is `accuracy`.","bd5998cf":"To make myself familiar with the data, it's always practical to perform some data visualizations. The first question that comes to my mind is: <br> **What type of comments occur most frequently?** Let's have a look!","208efa63":"*Back to: <a href='#Table of contents'> Table of contents <\/a>*\n### 4.5. RNN with Pre-Trained GloVe Embedding\n<a id='4.5. RNN with Pre-Trained GloVe Embedding' >","1f3676f6":"### 4.1.  Baseline Neural Network\n<a id='4.1. Baseline Neural Network' >","03e11aaf":"The correlation matrix illuminates interesting relationships :\n- \"Toxic\" comments are clearly correlated with both \"obscene\" and \"insult\" comments.\n- Interestingly, \"toxic\" and \"severe_toxic\" are only weakly correlated.\n- \"Obscene\" comments and \"insult\" comments are also highly correlated, which makes perfect sense.\n\nNext, let's use Venn diagrams to visualize these overlaps.","64aea7f4":"A pretrained embedding requires the arguments `weights=embedding_matrix` as well as `trainable=False` to freeze the weights.","808a463b":"First we import the necessary **Keras libraries** ...","3daaf780":"Let's explore some comments:","77cbac78":"As expected from a pretrained model with frozen weights, we only have a few trainable parameters left!","ab263811":"Seems that the accuracy is pretty decent for a basic attempt!","f45dd599":"Now, we have to preprocess our text data. **But how do we represent words to a deep learning algorithm?**\n\n1. We could, for instance, use a \u201cbag of words\u201d representation: think of a **one-hot vector** where there is one dimension for every word. If there are 13 million English words, this vector would be a 13 million-dimensional vector, with a \"hot code\" 1 for a given word. Yes, this is not a great representation of words.\n\n2. Instead, there are many **Word Embedding** models which map words to vectors (where the size of the vector is much smaller, like 300). These vectors use similarity metrics to reveal meaningful \"semantic relationships\" between certain words. Furthermore, the size of word embeddings is a tunable parameter, which can help overcome the \"Curse of Dimensionality\".\n\nWord Embeddings in a nutshell:\n\n- Word embeddings provide a dense representation of words and their relative meanings.\n\n- They are an improvement over the sparse representations used in simpler \"bag of word\" model representations.\n\n- Word embeddings can be learned from text data and reused among projects. They can also be learned as part of fitting a neural network onto text data.\n\n- There are two different ways to feed Word Embeddings into a neural network: train your own embedding layer or use a pre-trained embedding (like GloVe).\n    \nTo convert our text to vectors, we use Keras's convenient preprocessing tools to **tokenize** each example, convert it to a **sequence**, and then **pad the sequences** so they're all the same length.","9fd37d8a":"... and then set some **model parameters**.","1a069682":"### 2.2. Correlation Matrix\n<a id='2.2. Correlation Matrix' >","7b296523":"## Identify and Classify Toxic Online Comments\n\n### The dataset\n\nFor my fourth project, I chose the __[Toxic Comment Classification dataset on Kaggle](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge#description)__, where I was challenged to build a model that\u2019s capable of detecting different types of toxicity in online comments such as threats, obscenity, insults, and identity-based hate.\n\nGoogle's Conversation AI team is working on tools to help improve online conversations as the threat of abuse and harassment online lead many people to stop expressing themselves and give up on seeking different opinions. To better their current models in detecting toxic comments, they set up a competition on Kaggle hoping to help online discussions become more productive and respectful.\n\n### The Deep Learning Approach\n\nOne of the widely used Natural Language Processing (NLP) tasks in different business problems is text classification, whose goal is to automatically classify a text document into one or more predefined categories. It\u2019s an example of a supervised machine learning algorithm, since a labelled dataset is used for training a classifier.\n\nHere, the requirement is to build a multi-headed model that is capable of detecting different types of toxicity like threats, obscenity, insults and identity-based hate to improve Jigsaw\/Google's current models. \n\nIn **multi-headed classification**, data can belong to more than one label simultaneously. For example, in our case a comment may be toxic, obscene, and insulting at the same time. It may also happen that the comment is non-toxic and hence does not belong to any of the six labels.\n\nDeep learning is a subset of machine learning that uses a model of computing that's very much inspired by the structure of the brain. Hence, we call this model a neural network. In recent times, deep learning has transformed the fields of Natural Language Processing (NLP), where **deep neural networks** have achieved state-of-the-art performance.","a4854f6b":"Deep learning models - or neural networks - have achieved state-of-the-art results in text classification problems. Here we work through different neural networks to compare their results.\n\nWe'll start with **three deep learning models that use their own trained embedding layer**, and then try **two algorithms that use pre-trained embeddings**.","8ba2c614":"Let's start with instantiating a classic densely connected neural network to create a strong baseline. It's composed of several layers:\n\n- The **input layer**\u200a is the first layer in the neural network. It takes input values and passes them on to the next layer without applying any operations to them. We begin by defining an input layer that accepts a list of words with a dimension of 200 (= max words in one comment).\n\n- Next, we pass our vectors to an **embedding layer**, where we project the words onto a defined vector space depending on the distance of the surrounding words in a sentence. Embedding allows us to reduce model size, and most importantly the huge dimensions we have to deal with. \n\n- The output of the embedding layer is just a list of the coordinates of the words in this vector space. We need to define the size of this vector space and the number of unique words we are using. It's important to know that the embedding size is a parameter that one can tune and experiment with.\n\n- A **pooling layer** effectively downsamples the output of the prior layer, reducing the number of operations required for all following layers, but still passing on the valid information from the previous layer.\n\n- **Dense layers (or fully connected layers)** are simply a linear operation in which every input is connected to every output by a weight, generally followed by a non-linear activation function.\n\n- **Regularization\u200a layers** are used to overcome the over-fitting problem. In regularization we either penalise our loss term by adding a L1 (LASSO) or an L2 (Ridge) norm on the weight vector. Alternatively, we can apply a dropout layer where individual nodes are dropped out of the net, so that a reduced network is left.\n\n- Finally, the **output layer**\u200a is the last layer in the network and receives its input from the last hidden layer. With this layer we can get the desired number of values in a desired range. In our network we have 6 neurons in the output layer.\n\n- **Activation functions** are used to introduce non-linearity to neural networks. They squash the values into a smaller range. There are many activation functions used in the deep learning industry such as ReLU, Sigmoid or TanH.\n\nInspiration for the setup of this NN can be found __[here.](https:\/\/www.depends-on-the-definition.com\/classify-toxic-comments-on-wikipedia\/)__","b3ff9064":"*Back to: <a href='#Table of contents'> Table of contents <\/a>*\n### 4.3. Recurrent Neural Network (RNN)\n<a id='4.3. Recurrent Neural Network (RNN)' >","f98f4f9e":"The data consist of many Wikipedia comments which have been labelled by humans according to their relative toxicity. It includes the following:\n\n- train.csv - the training set, contains comments with their binary labels\n- test.csv - the test set, predicts toxicity probabilities for these comments.\n- sample_submission.csv - the submission sample with the correct format\n- test_labels.csv - labels for the test data (value of -1 indicates it was not used for scoring)\n\nWe'll start by loading in our training and testing data.","985ea268":"We face a slight increase in validation accuracy!"}}