{"cell_type":{"a28cac9d":"code","e1fdca52":"code","b820a6da":"code","181e8539":"code","29dd70f4":"code","e3e97c71":"code","ced305d8":"code","499c9ed4":"code","a5901046":"code","8acb7534":"code","14fdc8fa":"code","326e5ce5":"code","c3f91f0f":"code","0c108fad":"code","f0dc2d5d":"code","5a50dd82":"code","143b8463":"code","c8b1fca5":"code","c2a29d27":"code","cd837ad2":"code","468da4dd":"code","5668881d":"code","5a80571e":"code","509436bc":"code","d071044a":"code","15a7dff8":"code","decc8837":"code","254db4d0":"code","1667d28e":"code","73044c72":"code","43129aee":"code","a0b2b5ad":"code","6e21e0b9":"code","f4081284":"code","40c9aba1":"code","fd7bee61":"code","d32a4da3":"code","0db10de5":"code","39947915":"code","1921477d":"code","fece980e":"code","b027faf2":"code","19c17e15":"code","05e8a315":"code","18e9be9b":"code","23b1c779":"code","fde74204":"code","82613384":"code","06607b5b":"code","ae74dd22":"code","172e9f50":"code","f67fc872":"code","12d58ff9":"code","1b2b35f9":"code","610f51df":"code","b73a467a":"code","b7312fe5":"code","c8ac7fdd":"code","1b8800f9":"code","b474b491":"code","a1550c10":"code","864170c1":"code","4658bb95":"code","57c23191":"code","571116e3":"code","4406351f":"code","86c66a05":"code","d32ee7d6":"code","88919158":"code","c6c10ce7":"code","97e61a46":"code","d4d720de":"code","41ed56d7":"code","a3f20d0b":"code","64f7c8e1":"code","3900c904":"code","92c537ae":"code","c2056f6b":"code","56bf3b63":"code","5f6e830e":"code","51a19fcb":"code","4b6f19f5":"code","325b92c6":"code","14280acb":"code","93011bae":"code","9f10324a":"code","d0915ac6":"code","9a9545b9":"code","4160f2fb":"code","b1053246":"code","3e412a2a":"code","08febb5d":"code","1df1cf90":"code","9d726095":"code","79d12cf2":"code","5a94cc0f":"code","fe012131":"code","8b17d1ad":"code","acc8f825":"code","3ac3998d":"code","ed6f8875":"code","414e4d20":"code","508b760b":"code","3224a5ac":"code","2ac027f6":"code","50c79847":"code","ad5daf51":"code","ef78ceed":"code","dee1efbb":"code","28196a21":"code","b92b3b54":"code","c500df2a":"code","2a2508cf":"code","321d61cb":"code","191bde7d":"code","b2a192e8":"code","901aae91":"code","885b61a4":"code","0affc037":"code","d379171d":"code","82f99998":"code","8e35c361":"code","f805defc":"code","56eb05f2":"code","6f16c450":"code","c3ac40a7":"code","413d91c9":"code","ce3c060e":"code","a8d79c85":"code","941d0d1f":"code","a9be287c":"code","7983ad14":"code","15005631":"code","8a1ce597":"code","7a75b2d9":"code","8ba19f2c":"code","159e36e3":"code","219691da":"code","6dea7fae":"code","aba8ce95":"code","735ee680":"code","3ad8dc4e":"code","64843bd1":"code","4ad0f2d6":"code","bc03c1e1":"code","99e9954d":"code","7baa0ccf":"code","f6d4d422":"code","b032c824":"code","b9ef1b82":"code","43139779":"code","74c6a23f":"code","4e44166c":"code","f647c69b":"code","20f3ed5a":"code","859773f5":"code","e689b69a":"code","26dcfd47":"code","e28ff28f":"code","1e0a63ac":"code","fbae2dce":"code","1beeac22":"code","b867dd78":"code","c5a1b690":"code","5c1c3617":"code","f418a923":"code","99befef7":"code","c311491d":"code","361bdb13":"code","e2b8ad38":"code","b5ba7f43":"code","fc7224b0":"code","e5ac70f0":"code","29f07e6b":"code","01415aa0":"code","e32f809b":"code","6fee0034":"code","507424bc":"code","fb348f58":"code","4b76125c":"code","e7c7c1eb":"code","8b21dae2":"code","7fdde7e2":"code","cb039428":"code","832ff67c":"code","e35d6312":"code","378a866c":"code","6198b57c":"code","91c167df":"code","67c5cc14":"code","1ce81ba8":"code","9b17796e":"code","25c3eb96":"code","8ae5a6f2":"code","8e2571eb":"code","a374757a":"code","2c4c93de":"code","6f44ebb6":"code","85b4ccdc":"code","26d8a503":"code","6646b5f6":"code","ad43f471":"code","27090f37":"code","5a9c8183":"code","330391e3":"code","a573627e":"code","fc82c744":"code","74b4ea18":"code","fc435723":"code","3482a2ab":"code","f9baf69f":"code","819db992":"code","e0d419a4":"code","badd35db":"code","4d45d92a":"code","b3ba0f0b":"code","4a432820":"code","b58f961a":"code","b936f30a":"code","3fa893d8":"code","a9767ff7":"code","1f762352":"code","9affd81f":"code","7ec89b13":"code","7dc6b465":"code","b983528b":"code","4c9bfc93":"code","ef238686":"code","ab53d75f":"markdown","0f92d018":"markdown","1814cf83":"markdown","8c5b6124":"markdown","df3895fe":"markdown","d531b44f":"markdown","24786743":"markdown","06ee8dff":"markdown","779f5737":"markdown","885e2830":"markdown","dff2af66":"markdown","6137ac34":"markdown","cd716ba0":"markdown","69694c4e":"markdown","4cb508cb":"markdown","6f1a514a":"markdown","9b4e7deb":"markdown","2b0b4fc9":"markdown","a0e888cb":"markdown","8255c25b":"markdown","f8c97ad1":"markdown","e352d8d9":"markdown","ac0668b4":"markdown","18a14543":"markdown","4c35ba36":"markdown","d474ef6a":"markdown","87ffa011":"markdown","be9bbd00":"markdown","65bec7fa":"markdown","fe021a00":"markdown","606ca593":"markdown","942a971f":"markdown","b1e987d4":"markdown","3db5d551":"markdown","47245a92":"markdown","9ca51275":"markdown","93283e66":"markdown","51078ab5":"markdown","30b5c85d":"markdown","2196cfe9":"markdown","e5e47667":"markdown","08a7e5a8":"markdown","9a37197f":"markdown","c7520820":"markdown","3792463f":"markdown","dc2ad9bc":"markdown","1a8fa411":"markdown","174f9e04":"markdown","b40596e2":"markdown","002796df":"markdown","c07649b4":"markdown","0b864bf3":"markdown","6bae8e7b":"markdown","f0ee40f7":"markdown","61bc0155":"markdown","6f7a57f0":"markdown","ebeb0167":"markdown","3fad3474":"markdown","99ff4a36":"markdown","28ec61e7":"markdown","dc9657f5":"markdown","9ac60d32":"markdown","294cf0ac":"markdown","50ab2c83":"markdown","48944cba":"markdown","7ad15f69":"markdown","76973f7e":"markdown","0d9cb0a1":"markdown","5f57a7cb":"markdown","eeb534fb":"markdown","fd313338":"markdown","2eb532c2":"markdown","30f1cf63":"markdown","0a9419d7":"markdown","6959ffb7":"markdown","3b6530de":"markdown","66d919bc":"markdown","e842fe63":"markdown","c733a3a9":"markdown","112931d4":"markdown","5af7ff03":"markdown","51f4226e":"markdown","786ce905":"markdown","77c551b3":"markdown","18916444":"markdown","23b7fdeb":"markdown","5576bc0b":"markdown","6fcd3cae":"markdown","f0d6ba62":"markdown","a97f6000":"markdown","901dc3db":"markdown","968b1ac8":"markdown","34bce264":"markdown","a2583140":"markdown","10d2f48d":"markdown","1988b3f9":"markdown","ee990914":"markdown","6905ef8f":"markdown","092666fa":"markdown","3c57be89":"markdown","832c1985":"markdown","1b51532d":"markdown","8e624530":"markdown","c15c076e":"markdown"},"source":{"a28cac9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e1fdca52":"## Import Libraies\n\n## import all main libraries automatically with pyforest\n# !pip install pyforest\n# !pip install pycaret[full]\n\n# import pyforest\n\n## main libraries\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.ticker as mticker\n\n# !pip install squarify\nimport squarify as sq\n\nimport scipy.stats as stats\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport datetime as dt\nfrom datetime import datetime\n# from pyclustertend import hopkins\n\n## pre-processing\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.compose import make_column_transformer, ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.impute import SimpleImputer, KNNImputer\n\n## feature Selection\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression\n\n## scaling\nfrom sklearn.preprocessing import scale \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler\n\n## regression\/prediction\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n## ann\nfrom sklearn.neural_network import MLPRegressor\n\n## classification\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree \nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier, plot_importance\n\n## metrics\nfrom sklearn.metrics import plot_confusion_matrix, r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import make_scorer, precision_score, precision_recall_curve, plot_precision_recall_curve \nfrom sklearn.metrics import plot_roc_curve, roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score\nfrom sklearn.metrics import silhouette_samples,silhouette_score, average_precision_score\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom sklearn.metrics import auc\n\n## model selection\nfrom sklearn import model_selection\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold, KFold, cross_val_predict, train_test_split\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score, cross_validate\n\n## MLearning\nfrom sklearn.pipeline import make_pipeline, Pipeline\nimport optuna\nfrom sklearn.naive_bayes import GaussianNB\n\n## clevers\n# !pip install -U pandas-profiling --user\nimport pandas_profiling\nfrom pandas_profiling.report.presentation.flavours.html.templates import create_html_assets\n\nimport ipywidgets\nfrom ipywidgets import interact\nimport missingno as msno \n# !pip install wordcloud\nfrom wordcloud import WordCloud\n\n# !pip install termcolor\nimport colorama\nfrom colorama import Fore, Style  # makes strings colored\nfrom termcolor import colored\nfrom termcolor import cprint\n# grey red green yellow blue magenta cyan white (on_grey ..)\n# bold dark underline blink reverse concealed\n# cprint(\"Have a first look to:\",\"blue\",\"on_grey\", attrs=['bold'])\n\n## plotly and cufflinks\nimport plotly \nimport plotly.express as px\nimport cufflinks as cf\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n## Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.warn(\"this will not show\")\n\n## Figure&Display options\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option('max_colwidth',200)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.3f' % x)","b820a6da":"## Some Useful User-Defined-Functions\n\n###############################################################################\n\ndef missing_values(df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values[missing_values['Missing_Number']>0]\n\n###############################################################################\n\ndef first_looking(df):\n    print(colored(\"Shape:\", 'yellow', attrs=['bold']), df.shape,'\\n', \n          colored('*'*100, 'red', attrs=['bold']),\n          colored(\"\\nInfo:\\n\",'yellow', attrs=['bold']), sep='')\n    print(df.info(), '\\n', \n          colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"Number of Uniques:\\n\", 'yellow', attrs=['bold']), df.nunique(),'\\n',\n          colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"Missing Values:\\n\", 'yellow', attrs=['bold']), missing_values(df),'\\n', \n          colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"All Columns:\", 'yellow', attrs=['bold']), *list(df.columns), sep='\\n- ') \n    print(colored('*'*100, 'red', attrs=['bold']), sep='')\n\n    df.columns= df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n    \n    print(colored(\"Columns after rename:\", 'yellow', attrs=['bold']), *list(df.columns), sep='\\n- ')\n    print(colored('*'*100, 'red', attrs=['bold']), sep='')\n    \n###############################################################################\n## To view summary information about the columns\n\ndef summary(column):\n    print(colored(\"Column: \",'yellow', attrs=['bold']), column)\n    print(colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"Missing values: \", 'yellow', attrs=['bold']), df[column].isnull().sum())\n    print(colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"Missing values(%): \", 'yellow', attrs=['bold']), round(df[column].isnull().sum()\/df.shape[0]*100, 2))\n    print(colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"Unique values: \", 'yellow', attrs=['bold']), df[column].nunique())\n    print(colored('*'*100, 'red', attrs=['bold']), sep='')\n    print(colored(\"Value counts: \\n\", 'yellow', attrs=['bold']), df[column].value_counts(dropna = False), sep='')\n    print(colored('*'*100, 'red', attrs=['bold']), sep='')\n    \n###############################################################################\n                    \ndef multicolinearity_control(df):                    \n    df_temp = df.corr()\n    count = 'Done'\n    feature =[]\n    collinear= []\n    for col in df_temp.columns:\n        for i in df_temp.index:\n            if abs(df_temp[col][i] > .8 and df_temp[col][i] < 1):\n                    feature.append(col)\n                    collinear.append(i)\n                    cprint(f\"multicolinearity alert in between {col} - {i}\", \"red\", attrs=[\"bold\"])\n    else:\n        cprint(f\"There is NO multicollinearity between the features.\", \"blue\", attrs=[\"bold\"])                     \n                    \n###############################################################################\n\ndef duplicate_values(df):\n    print(colored(\"Duplicate check...\", 'yellow', attrs=['bold']), sep='')\n    duplicate_values = df.duplicated(subset=None, keep='first').sum()\n    if duplicate_values > 0:\n        df.drop_duplicates(keep='first', inplace=True)\n        print(duplicate_values, colored(\" Duplicates were dropped!\"),'\\n',\n              colored('*'*100, 'red', attrs=['bold']), sep='')\n    else:\n        print(colored(\"There are no duplicates\"),'\\n',\n              colored('*'*100, 'red', attrs=['bold']), sep='')     \n\n###############################################################################\n        \ndef drop_columns(df, drop_columns):\n    if drop_columns !=[]:\n        df.drop(drop_columns, axis=1, inplace=True)\n        print(drop_columns, 'were dropped')\n    else:\n        print(colored('Missing value control...', 'yellow', attrs=['bold']),'\\n',\n              colored('If there is a missing value above the limit you have given, the relevant columns are dropped and an information is given.'), sep='')\n\n###############################################################################\n\ndef drop_null(df, limit):\n    for i in df.isnull().sum().index:\n        if (df.isnull().sum()[i]\/df.shape[0]*100)>limit:\n            print(df.isnull().sum()[i], 'percent of', i ,'were null and dropped')\n            df.drop(i, axis=1, inplace=True)\n    print(colored('Last shape after missing value control:', 'yellow', attrs=['bold']), df.shape, '\\n', \n          colored('*'*100, 'red', attrs=['bold']), sep='')\n\n###############################################################################\n\ndef shape_control():\n    print('df.shape:', df.shape)\n    print('X.shape:', X.shape)\n    print('y.shape:', y.shape)\n    print('X_train.shape:', X_train.shape)\n    print('y_train.shape:', y_train.shape)\n    print('X_test.shape:', X_test.shape)\n    print('y_test.shape:', y_test.shape)\n\n###############################################################################  \n\n## show values in bar graphic\ndef show_values_on_bars(axs):\n    def _show_on_single_plot(ax):        \n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() \/ 2\n            _y = p.get_y() + p.get_height()\n            value = '{:.2f}'.format(p.get_height())\n            ax.text(_x, _y, value, ha=\"center\") \n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\n        \n############################################################################### \n\n'''This function detects the best z-score for outlier detection in the specified column.'''\n\ndef outlier_zscore(df, col, min_z=1, max_z = 5, step = 0.05, print_list = False):\n    z_scores = stats.zscore(df[col].dropna())\n    threshold_list = []\n    \n    for threshold in np.arange(min_z, max_z, step):\n        threshold_list.append((threshold, len(np.where(z_scores > threshold)[0])))\n    \n    df_outlier = pd.DataFrame(threshold_list, columns = ['threshold', 'outlier_count'])\n    df_outlier['pct'] = (df_outlier.outlier_count - df_outlier.outlier_count.shift(-1))\/df_outlier.outlier_count*100\n    df_outlier['pct'] = df_outlier['pct'].apply(lambda x : x-100 if x == 100 else x)\n    best_treshold = round(df_outlier.iloc[df_outlier.pct.argmax(), 0],2)\n    IQR_coef = round((best_treshold - 0.675) \/ 1.35, 2)\n    outlier_limit = int(df[col].dropna().mean() + (df[col].dropna().std()) * df_outlier.iloc[df_outlier.pct.argmax(), 0])\n    num_outlier = df_outlier.iloc[df_outlier.pct.argmax(), 1]\n    percentile_threshold = stats.percentileofscore(df[col].dropna(), outlier_limit)\n    plt.plot(df_outlier.threshold, df_outlier.outlier_count)\n    plt.vlines(best_treshold, 0, df_outlier.outlier_count.max(), colors=\"r\", ls = \":\")\n    plt.annotate(\"Zscore : {}\\nIQR_coef : {}\\nValue : {}\\nNum_outlier : {}\\nPercentile : {}\".format(best_treshold,\n                                                                          IQR_coef,\n                                                                          outlier_limit,\n                                                                          num_outlier,     \n                                                                          (np.round(percentile_threshold, 3), \n                                                                           np.round(100-percentile_threshold, 3))),\n                                                                          (best_treshold, df_outlier.outlier_count.max()\/2))\n    plt.show()\n    if print_list:\n        print(df_outlier)\n    return (plt, df_outlier, best_treshold, IQR_coef, outlier_limit, num_outlier, percentile_threshold)\n\n############################################################################### \n\n'''This function plots histogram, boxplot and z-score\/outlier graphs for the specified column.'''\n\ndef outlier_inspect(df, col, min_z = 1, max_z = 5, step = 0.05, max_hist = None, bins = 50):\n    fig = plt.figure(figsize=(20, 6))\n    fig.suptitle(col, fontsize=16)\n    plt.subplot(1,3,1)\n    if max_hist == None:\n        sns.distplot(df[col], kde=False, bins = 50)\n    else :\n        sns.distplot(df[df[col]<=max_hist][col], kde=False, bins = 50)\n    plt.subplot(1,3,2)\n    sns.boxplot(df[col])\n    plt.subplot(1,3,3)\n    z_score_inspect = outlier_zscore(df, col, min_z = min_z, max_z = max_z, step = step)\n    plt.show()\n    \n############################################################################### \n\n\"\"\"This function gives max\/min threshold, number of data, number of outlier and plots its boxplot,\naccording to the tree type and the entered z-score value for the relevant column.\"\"\"\n\ndef num_outliers(df, col, whis = 1.5):\n    q1 = df.groupby(\"class\")[col].quantile(0.25)\n    q3 = df.groupby(\"class\")[col].quantile(0.75)\n    iqr = q3 - q1\n    print(\"Column_name :\", col)\n    print(\"whis :\", whis)\n    print(\"-------------------------------------------\")\n    for i in np.sort(df['class'].unique()):\n        min_threshold = q1.loc[i] - whis*iqr.loc[i]\n        max_threshold = q3.loc[i] + whis*iqr.loc[i]\n        print(\"min_threshold:\", min_threshold, \"\\nmax_threshold:\", max_threshold)\n        num_outliers = len(df[df[\"class\"]==i][col][(df[col]<min_threshold) | (df[col]>max_threshold)])\n        print(f\"Num_of_values for {i} :\", len(df[df[\"class\"]==i]))\n        print(f\"Num_of_outliers for {i} :\", num_outliers)\n        print(\"-------------------------------------------\")\n    return sns.boxplot(y = df[col], x = df[\"class\"], whis=whis)\n\n############################################################################### \n\n\"\"\"This function assigns the NaN-value first and then drop related rows, according to the tree type and the entered\nwhis value and plots the boxplot for the relevant column. \"\"\"\n\ndef remove_outliers(df, col, whis=1.5):\n    q1 = df.groupby(\"class\")[col].quantile(0.25)\n    q3 = df.groupby(\"class\")[col].quantile(0.75)\n    iqr = q3 - q1\n    for i in np.sort(df['class'].unique()):\n        min_threshold = q1.loc[i] - whis*iqr.loc[i]\n        max_threshold = q3.loc[i] + whis*iqr.loc[i]\n        df.loc[((df[\"class\"]==i) & ((df[col]<min_threshold) | (df[col]>max_threshold))), col] = np.nan\n    return sns.boxplot(y = df[col], x = df[\"class\"], whis=whis)\n\n############################################################################### ","181e8539":"# df0 = pd.read_csv('creditcard.csv')\ndf0=pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf = df0.copy()\ndf.head(3) ","29dd70f4":"df.tail(3)","e3e97c71":"df.sample(3)","ced305d8":"first_looking(df)\nduplicate_values(df)\ndrop_columns(df, [])\ndrop_null(df, 90)\n# df.describe().T","499c9ed4":"df.describe().T","a5901046":"cprint('Have a first look to \"class\" column',\"green\",\"on_red\", attrs=[\"bold\"])\nprint(\"-----\"*10)\nsummary('class')","8acb7534":"cprint('Mean values according to the \"class\" column','green', 'on_red')\ndf.groupby('class').mean()","14fdc8fa":"cprint('Descriptive statistics according to the \"class==0, Non-Fraudulent\"',\"green\",\"on_red\", attrs=[\"bold\"])\ndf[df['class'] == 1].describe().T.style.background_gradient(subset = ['mean','min','50%', 'max'], cmap = 'RdPu')","326e5ce5":"cprint('Descriptive statistics according to the \"class==1, Fraudulent\"',\"green\",\"on_red\", attrs=[\"bold\"])\ndf[df['class'] == 0].describe().T.style.background_gradient(subset = ['mean','min','50%', 'max'], cmap = 'RdPu')","c3f91f0f":"fig = px.pie(df, values = df['class'].value_counts(), \n             names = (df['class'].value_counts()).index, \n             title = '\"class\" Column Distribution')\nfig.show()","0c108fad":"y = df['class']\nprint(f'Percentage of class-1: % {round(y.value_counts(normalize=True)[1]*100,2)} --> \\\n({y.value_counts()[1]} observations for class-1)\\nPercentage of class-0: % {round(y.value_counts(normalize=True)[0]*100,2)} --> ({y.value_counts()[0]} observations for class-0)')","f0dc2d5d":"cprint(\"Have a First Look to 'time' Column\",\"green\",\"on_red\", attrs=[\"bold\"])\nprint(\"-----\"*10)\nsummary('time')","5a50dd82":"cprint(\"Describe Values of Column 'time'\",\"green\",\"on_red\", attrs=[\"bold\"])\ndf.time.describe()","143b8463":"fig = px.box(df, x = df['time'], color = df['class'], title = 'Boxplot Of Column \"time\"')\nfig.show()","c8b1fca5":"cprint(\"Have a First Look to 'amount' Column\",\"green\",\"on_red\", attrs=[\"bold\"])\nprint(\"-----\"*10)\nsummary('time')","c2a29d27":"cprint(\"Describe Values of Column 'amount'\",\"green\",\"on_red\", attrs=[\"bold\"])\ndf.amount.describe()","cd837ad2":"fig = px.box(df, x = df['amount'], color = df['class'], title = 'Boxplot Of Column \"amount\"')\nfig.show()","468da4dd":"def transaction_transformer(price):\n    if price <= 5.600:\n        return 'Q-1'\n    if 5.600 < price <= 22.700:\n        return 'Q-2'\n    if 22.700 < price <= 75.510:\n        return 'Q-3'\n    else:\n        return 'Q-4'","5668881d":"df['transaction_class'] = df['amount'].apply(transaction_transformer)","5a80571e":"fig = px.pie(df, values = df[df['class']==0]['transaction_class'].value_counts(), \n             names = df[df['class']==0]['transaction_class'].value_counts().index, \n             title = 'transaction_class & class==0 Distribution')\nfig.show()","509436bc":"fig = px.pie(df, values = df[df['class']==1]['transaction_class'].value_counts(), \n             names = df[df['class']==1]['transaction_class'].value_counts().index, \n             title = 'transaction_class & class==1 Distribution')\nfig.show()","d071044a":"print('Number of fraudulent transactions for Q-1      : ', df[(df['class']==1) & (df['transaction_class']=='Q-1')]['class'].sum())\nprint('Total amount of fraudulent transactions for Q-1: ', df[(df['class']==1) & (df['transaction_class']=='Q-1')]['amount'].sum())\nprint('Mean amount of fraudulent transactions for Q-1 : ', df[(df['class']==1) & (df['transaction_class']=='Q-1')]['amount'].mean())","15a7dff8":"print('Number of fraudulent transactions for Q-2      : ', df[(df['class']==1) & (df['transaction_class']=='Q-2')]['class'].sum())\nprint('Total amount of fraudulent transactions for Q-2: ', df[(df['class']==1) & (df['transaction_class']=='Q-2')]['amount'].sum())\nprint('Mean amount of fraudulent transactions for Q-2 : ', df[(df['class']==1) & (df['transaction_class']=='Q-2')]['amount'].mean())","decc8837":"print('Number of fraudulent transactions for Q-3      : ', df[(df['class']==1) & (df['transaction_class']=='Q-3')]['class'].sum())\nprint('Total amount of fraudulent transactions for Q-3: ', df[(df['class']==1) & (df['transaction_class']=='Q-3')]['amount'].sum())\nprint('Mean amount of fraudulent transactions for Q-3 : ', df[(df['class']==1) & (df['transaction_class']=='Q-3')]['amount'].mean())","254db4d0":"print('Number of fraudulent transactions for Q-4      : ', df[(df['class']==1) & (df['transaction_class']=='Q-4')]['class'].sum())\nprint('Total amount of fraudulent transactions for Q-4: ', df[(df['class']==1) & (df['transaction_class']=='Q-4')]['amount'].sum())\nprint('Mean amount of fraudulent transactions for Q-4 : ', df[(df['class']==1) & (df['transaction_class']=='Q-4')]['amount'].mean())","1667d28e":"df.drop(['transaction_class'], axis = 1, inplace = True)","73044c72":"cprint(\"Heatmap of df\",'green', 'on_red', attrs=[\"bold\"])\nprint(\"-----\"*10)\nplt.figure(figsize = (17, 17))\nsns.heatmap (df.corr(), annot = True, fmt = '.2f', vmin = -1, vmax = 1)\nplt.xticks(rotation = 45);","43129aee":"multicolinearity_control(df)","a0b2b5ad":"df.corr()['class'].sort_values().drop('class').iplot(kind = 'barh', title = 'Correlation Between the Columns');","6e21e0b9":"cprint(\"Missing Values\",\"green\",\"on_red\", attrs=[\"bold\"])\nmissing_values(df)","f4081284":"cprint(\"Boxplots\",\"green\",\"on_red\", attrs=[\"bold\"])\n%matplotlib inline\nindex = 0\nplt.figure(figsize=(20,20))\nfor feature in df.columns[2:29]:\n    index += 1\n    plt.subplot(7,4,index)\n    sns.boxplot(x=feature, data=df, whis=1.5)","40c9aba1":"cprint(\"Boxplots\",\"green\",\"on_red\", attrs=[\"bold\"])\nindex = 0\nplt.figure(figsize=(20,20))\nfor feature in df.columns[:30]:\n    index += 1\n    plt.subplot(8,4,index)\n    sns.boxplot(y = feature, x = \"class\", data = df, whis=3)","fd7bee61":"from scipy.stats import zscore\nfrom scipy import stats\nfrom numpy import percentile","d32a4da3":"cprint(\"Outlier Inspection\",\"green\",\"on_red\", attrs=[\"bold\"])\nfor col in df.columns[:30]:\n    outlier_inspect(df, col)","0db10de5":"IQR_coef = 3\nz_score = round(0.675 + IQR_coef*1.35, 2)\nz_score","39947915":"z_score = 3\niqr_coef = round((z_score - 0.675) \/ 1.35, 2)\niqr_coef","1921477d":"df_out = df.copy()","fece980e":"cprint(\"Shape of new df before outlier check\",'green', 'on_red', attrs=[\"bold\"])\ndf_out.shape","b027faf2":"cprint(\"Value counts of 'class' column before outlier check\",'green', 'on_red', attrs=[\"bold\"])\ndf_out['class'].value_counts(dropna=False)","19c17e15":"cprint(\"Missing value control before outlier check\",'green', 'on_red', attrs=[\"bold\"])\nmissing_values(df_out)","05e8a315":"for col in df_out.columns[:30]:\n    num_outliers(df_out, col, whis=3)","18e9be9b":"for col in df_out.columns[1:30]:\n    remove_outliers(df_out, col, whis=3)","23b1c779":"cprint(\"Missing value control after outlier check\",'green', 'on_red', attrs=[\"bold\"])\nmissing_values(df_out)","fde74204":"df_out.dropna(inplace=True)","82613384":"cprint(\"Missing value control after outlier check\",'green', 'on_red', attrs=[\"bold\"])\nmissing_values(df_out)","06607b5b":"cprint(\"Shape of new df after outlier check\",'green', 'on_red', attrs=[\"bold\"])\ndf_out.shape","ae74dd22":"cprint(\"Value counts of 'class' column after outlier check\",'green', 'on_red', attrs=[\"bold\"])\ndf_out['class'].value_counts(dropna=False)","172e9f50":"def eval(model, X_train, X_test):\n    y_pred = model.predict(X_test)\n    y_pred_train = model.predict(X_train)\n    \n    print(confusion_matrix(y_test, y_pred))\n    print(\"Test_Set\")\n    print(classification_report(y_test,y_pred))\n    print(\"Train_Set\")\n    print(classification_report(y_train,y_pred_train))\n    print(\"---\"*20)\n    plot_confusion_matrix(model, X_test, y_test, cmap=\"plasma\")","f67fc872":"def train_val(y_train, y_train_pred, y_test, y_pred):\n    \n    scores = {\"train_set\": {\"Accuracy\" : accuracy_score(y_train, y_train_pred),\n                            \"Precision\" : precision_score(y_train, y_train_pred),\n                            \"Recall\" : recall_score(y_train, y_train_pred),                          \n                            \"f1\" : f1_score(y_train, y_train_pred),\n                            \"roc_auc\" : roc_auc_score(y_train, y_train_pred),\n                            \"recall_auc\" : auc(recall, precision)},\n    \n              \"test_set\": {\"Accuracy\" : accuracy_score(y_test, y_pred),\n                           \"Precision\" : precision_score(y_test, y_pred),\n                           \"Recall\" : recall_score(y_test, y_pred),                          \n                           \"f1\" : f1_score(y_test, y_pred),\n                           \"roc_auc\" : roc_auc_score(y_test, y_pred),\n                           \"recall_auc\" : auc(recall, precision)}}\n    \n    return pd.DataFrame(scores)","12d58ff9":"df_ml = df_out.copy()","1b2b35f9":"scaler = StandardScaler()\n\ndf_ml[\"amount\"] = scaler.fit_transform(df_ml[\"amount\"].values.reshape(-1,1))\ndf_ml[\"time\"] = scaler.fit_transform(df_ml[\"time\"].values.reshape(-1,1))","610f51df":"X = df_ml.drop(['class'], axis = 1)\ny = df_ml['class']","b73a467a":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2, random_state = 42)","b7312fe5":"print('X_train.shape : ', X_train.shape)\nprint('X_test.shape  : ', X_test.shape)","c8ac7fdd":"cprint('y_train.value_counts','green', 'on_red')\ny_train.value_counts()","1b8800f9":"cprint('y_test.value_counts','green', 'on_red')\ny_test.value_counts()","b474b491":"LogReg_model = LogisticRegression(solver='liblinear', class_weight = 'balanced', random_state = 42)\nLogReg_model.fit(X_train, y_train)\ny_pred = LogReg_model.predict(X_test)\ny_train_pred = LogReg_model.predict(X_train)","a1550c10":"LogReg_model_f1 = f1_score(y_test, y_pred)\nLogReg_model_acc = accuracy_score(y_test, y_pred)\nLogReg_model_recall = recall_score(y_test, y_pred)\nLogReg_model_auc = roc_auc_score(y_test, y_pred)\nLogReg_model_pre = precision_score(y_test, y_pred)\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\nLogReg_model_recall_auc = auc(recall, precision)","864170c1":"print(\"LogReg_Model\")\nprint (\"------------------\")\neval(LogReg_model, X_train, X_test)","4658bb95":"cprint('LogReg_model Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","57c23191":"from yellowbrick.classifier import ClassPredictionError\nvisualizer = ClassPredictionError(LogReg_model)\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n# Draw visualization\nvisualizer.poof();","571116e3":"cprint('Cross Validation scores for Logistic Regression with default parameters','green', 'on_red')\n\nLogReg_cv = LogisticRegression(solver='liblinear', class_weight = 'balanced', random_state = 42)\nLogReg_cv_scores = cross_validate(LogReg_cv, X_train, y_train, \n                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\nLogReg_cv_scores = pd.DataFrame(LogReg_cv_scores, index = range(1, 11))\nLogReg_cv_scores.mean()[2:]","4406351f":"param_grid = { \"class_weight\" : [\"balanced\", None],\n              'penalty': [\"l1\",\"l2\"],\n              'solver' : ['saga','lbfgs'],\n              }","86c66a05":"LogReg_grid = LogisticRegression(class_weight = 'balanced', random_state = 42)\nLogReg_grid_model = GridSearchCV(LogReg_grid, param_grid, scoring = \"f1\", verbose = 2, n_jobs = -1).fit(X_train, y_train)","d32ee7d6":"print(colored('\\033[1mBest Estimators of GridSearchCV for Logistic Regression Model:\\033[0m', 'blue'), colored(LogReg_grid_model.best_estimator_, 'red'))","88919158":"print(colored('\\033[1mBest Parameters of GridSearchCV for Logistic Regression Model:\\033[0m', 'blue'), colored(LogReg_grid_model.best_params_, 'red'))","c6c10ce7":"LogReg_tuned = LogisticRegression(class_weight = 'balanced',\n                                  penalty = 'l1', \n                                  solver = 'saga',\n                                  random_state = 42).fit(X_train, y_train)","97e61a46":"y_pred = LogReg_tuned.predict(X_test)\ny_train_pred = LogReg_tuned.predict(X_train)\n\n\nLogReg_tuned_f1 = f1_score(y_test, y_pred)\nLogReg_tuned_acc = accuracy_score(y_test, y_pred)\nLogReg_tuned_recall = recall_score(y_test, y_pred)\nLogReg_tuned_auc = roc_auc_score(y_test, y_pred)\nLogReg_tuned_pre = precision_score(y_test, y_pred)\nLogReg_tuned_recall_auc = auc(recall, precision)\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\nLogReg_tuned_recall_auc = auc(recall, precision)","d4d720de":"print(\"LogReg_tuned\")\nprint (\"------------------\")\neval(LogReg_tuned, X_train, X_test)","41ed56d7":"cprint('Logistic Regression_tuned Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","a3f20d0b":"from yellowbrick.classifier import ClassPredictionError\nvisualizer = ClassPredictionError(LogReg_tuned)\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n# Draw visualization\nvisualizer.poof();","64f7c8e1":"cprint('roc_curve','green', 'on_red')\nplot_roc_curve(LogReg_tuned, X_test, y_test);","3900c904":"cprint('precision_recall_curve','green', 'on_red')\nplot_precision_recall_curve(LogReg_tuned, X_test, y_test);","92c537ae":"cprint('LogReg_tuned Predictions','green', 'on_red')\nLogReg_Pred = {\"Actual\": y_test, \"LogReg_Pred\":y_pred}\nLogReg_Pred = pd.DataFrame.from_dict(LogReg_Pred)\nLogReg_Pred.sample(5)","c2056f6b":"pd.crosstab(LogReg_Pred['Actual'], LogReg_Pred['LogReg_Pred'])","56bf3b63":"cprint('Predictions','green', 'on_red')\nModel_Preds = LogReg_Pred\nModel_Preds.sample(5)","5f6e830e":"# logistic_regression = pickle.dump(LogReg_tuned, open('logistic_Regression_Model(tuned)', 'wb'))","51a19fcb":"RF_model = RandomForestClassifier(class_weight = \"balanced\", random_state = 42)\nRF_model.fit(X_train, y_train)\ny_pred = RF_model.predict(X_test)\ny_train_pred = RF_model.predict(X_train)","4b6f19f5":"RF_model_f1 = f1_score(y_test, y_pred)\nRF_model_acc = accuracy_score(y_test, y_pred)\nRF_model_recall = recall_score(y_test, y_pred)\nRF_model_auc = roc_auc_score(y_test, y_pred)\nRF_model_pre = precision_score(y_test, y_pred)\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\nRF_model_recall_auc = auc(recall, precision)","325b92c6":"print(\"RF_Model\")\nprint (\"------------------\")\neval(RF_model, X_train, X_test)","14280acb":"cprint('RF_model Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","93011bae":"from yellowbrick.classifier import ClassPredictionError\nvisualizer = ClassPredictionError(RF_model)\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n# Draw visualization\nvisualizer.poof();","9f10324a":"cprint('Random Forest Feature Importance','green', 'on_red')\nRF_feature_imp = pd.DataFrame(index=X.columns, data = RF_model.feature_importances_, columns = ['Importance']).sort_values(\"Importance\", ascending = False)\nRF_feature_imp","d0915ac6":"fig = px.bar(RF_feature_imp.sort_values('Importance', ascending = False), x = RF_feature_imp.sort_values('Importance', \n             ascending = False).index, y = 'Importance', title = \"Feature Importance\", \n             labels = dict(x = \"Features\", y =\"Feature_Importance\"))\nfig.show()","9a9545b9":"cprint('Cross Validation scores for Random Forest with default parameters','green', 'on_red')\nRF_cv = RandomForestClassifier(class_weight = \"balanced\", random_state = 42)\nRF_cv_scores = cross_validate(RF_cv, X_train, y_train, \n                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\nRF_cv_scores = pd.DataFrame(RF_cv_scores, index = range(1, 11))\nRF_cv_scores.mean()[2:]","4160f2fb":"param_grid = {'n_estimators' : [50, 100, 300],\n              'max_features' : [2, 3, 4],\n              'max_depth' : [3, 5, 7],\n              'min_samples_split' : [2, 5, 8]}","b1053246":"RF_grid = RandomForestClassifier(class_weight = 'balanced', random_state = 42)\nRF_grid_model = GridSearchCV(estimator = RF_grid, \n                             param_grid = param_grid, \n                             scoring = \"recall\", \n                             n_jobs = -1, verbose = 2)\nRF_grid_model.fit(X_train, y_train)","3e412a2a":"print(colored('\\033[1mBest Estimators of GridSearchCV for Random Forest Model:\\033[0m', 'blue'), colored(RF_grid_model.best_estimator_, 'red'))","08febb5d":"print(colored('\\033[1mBest Parameters of GridSearchCV for Random Forest Model:\\033[0m', 'blue'), colored(RF_grid_model.best_params_, 'red'))","1df1cf90":"RF_tuned = RandomForestClassifier(class_weight = 'balanced',\n                                  max_depth = 5,\n                                  max_features = 2,\n                                  min_samples_split = 5,\n                                  n_estimators = 50,\n                                  random_state = 42).fit(X_train, y_train)","9d726095":"y_pred = RF_tuned.predict(X_test)\ny_train_pred = RF_tuned.predict(X_train)\n\nRF_tuned_f1 = f1_score(y_test, y_pred)\nRF_tuned_acc = accuracy_score(y_test, y_pred)\nRF_tuned_recall = recall_score(y_test, y_pred)\nRF_tuned_auc = roc_auc_score(y_test, y_pred)\nRF_tuned_pre = precision_score(y_test, y_pred)\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\nRF_tuned_recall_auc = auc(recall, precision)","79d12cf2":"print(\"RF_tuned\")\nprint (\"------------------\")\neval(RF_tuned, X_train, X_test)","5a94cc0f":"cprint('RF_tuned Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","fe012131":"from yellowbrick.classifier import ClassPredictionError\nvisualizer = ClassPredictionError(RF_tuned)\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n# Draw visualization\nvisualizer.poof();","8b17d1ad":"cprint('roc_curve','green', 'on_red')\nplot_roc_curve(RF_tuned, X_test, y_test);","acc8f825":"cprint('precision_recall_curve','green', 'on_red')\nplot_precision_recall_curve(RF_tuned, X_test, y_test);","3ac3998d":"cprint('RF_tuned Predictions','green', 'on_red')\nRF_Pred = {\"Actual\": y_test, \"RF_Pred\":y_pred}\nRF_Pred = pd.DataFrame.from_dict(RF_Pred)\nRF_Pred.sample(5)","ed6f8875":"pd.crosstab(RF_Pred['Actual'], RF_Pred['RF_Pred'])","414e4d20":"cprint('Predictions','green', 'on_red')\nRF_Pred.drop(\"Actual\", axis = 1, inplace = True)\nModel_Preds = pd.merge(Model_Preds, RF_Pred, left_index = True, right_index = True)\nModel_Preds.sample(5)","508b760b":"# random_forest_classifier = pickle.dump(RF_tuned, open('random_forest_model(tuned)', 'wb'))","3224a5ac":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline","2ac027f6":"df_smote = df_out.copy()","50c79847":"scaler = StandardScaler()\n\ndf_smote[\"amount\"] = scaler.fit_transform(df_ml[\"amount\"].values.reshape(-1,1))\ndf_smote[\"time\"] = scaler.fit_transform(df_ml[\"time\"].values.reshape(-1,1))","ad5daf51":"X = df_smote.drop(['class'], axis = 1)\ny = df_smote['class']","ef78ceed":"over = SMOTE(sampling_strategy = {1: 10000})\nunder = RandomUnderSampler(sampling_strategy = {0: 10000})\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps = steps)\nX, y = pipeline.fit_resample(X, y)","dee1efbb":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2, random_state = 42)","28196a21":"print('X_train.shape : ', X_train.shape)\nprint('X_test.shape  : ', X_test.shape)","b92b3b54":"cprint('y_train.value_counts','green', 'on_red')\ny_train.value_counts()","c500df2a":"cprint('y_test.value_counts','green', 'on_red')\ny_test.value_counts()","2a2508cf":"LogReg_smote_model = LogisticRegression(solver = 'liblinear', class_weight = 'balanced', random_state = 42)\nLogReg_smote_model.fit(X_train, y_train)\ny_pred = LogReg_smote_model.predict(X_test)\ny_train_pred = LogReg_smote_model.predict(X_train)","321d61cb":"LogReg_smote_model_f1 = f1_score(y_test, y_pred)\nLogReg_smote_model_acc = accuracy_score(y_test, y_pred)\nLogReg_smote_model_recall = recall_score(y_test, y_pred)\nLogReg_smote_model_auc = roc_auc_score(y_test, y_pred)\nLogReg_smote_model_pre = precision_score(y_test, y_pred)\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\nLogReg_smote_model_recall_auc = auc(recall, precision)","191bde7d":"print(\"LogRegSmote_Model\")\nprint (\"------------------\")\neval(LogReg_smote_model, X_train, X_test)","b2a192e8":"cprint('LogRegSmote_model Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","901aae91":"from yellowbrick.classifier import ClassPredictionError\nvisualizer = ClassPredictionError(LogReg_smote_model)\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n# Draw visualization\nvisualizer.poof();","885b61a4":"cprint('Cross Validation scores for Logistic Regression with default parameters(SMOTE)','green', 'on_red')\nLogRegSmote_cv = LogisticRegression(solver = 'liblinear', class_weight = 'balanced', random_state = 42)\nLogRegSmote_cv_scores = cross_validate(LogRegSmote_cv, X_train, y_train, \n                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\nLogRegSmote_cv_scores = pd.DataFrame(LogRegSmote_cv_scores, index = range(1, 11))\nLogRegSmote_cv_scores.mean()[2:]","0affc037":"param_grid = { \"class_weight\" : [\"balanced\", None],\n              'penalty': [\"l1\",\"l2\"],\n              'solver' : ['saga','lbfgs'],\n              }","d379171d":"LogRegSmote_grid = LogisticRegression(class_weight = 'balanced', random_state = 42)\nLogRegSmote_grid_model = GridSearchCV(LogRegSmote_grid, param_grid, scoring = \"f1\", verbose = 2, n_jobs = -1).fit(X_train, y_train)","82f99998":"print(colored('\\033[1mBest Estimators of GridSearchCV for Logistic Regression Model:\\033[0m', 'blue'), colored(LogRegSmote_grid_model.best_estimator_, 'red'))","8e35c361":"print(colored('\\033[1mBest Parameters of GridSearchCV for Logistic Regression Model:\\033[0m', 'blue'), colored(LogRegSmote_grid_model.best_params_, 'red'))","f805defc":"# LogRegSmote_tuned = LogisticRegression(class_weight = 'balanced',\n#                                        penalty = 'l2', \n#                                        solver = 'lbfgs',\n#                                        random_state = 42).fit(X_train, y_train)","56eb05f2":"LogRegSmote_tuned = LogisticRegression(class_weight = 'balanced',\n                                       solver = 'liblinear',\n                                       random_state = 42).fit(X_train, y_train)","6f16c450":"y_pred = LogRegSmote_tuned.predict(X_test)\ny_train_pred = LogRegSmote_tuned.predict(X_train)\n\nLogRegSmote_tuned_f1 = f1_score(y_test, y_pred)\nLogRegSmote_tuned_acc = accuracy_score(y_test, y_pred)\nLogRegSmote_tuned_recall = recall_score(y_test, y_pred)\nLogRegSmote_tuned_auc = roc_auc_score(y_test, y_pred)\nLogRegSmote_tuned_pre = precision_score(y_test, y_pred)\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\nLogRegSmote_tuned_recall_auc = auc(recall, precision)","c3ac40a7":"print(\"LogRegSmote_tuned\")\nprint (\"------------------\")\neval(LogRegSmote_tuned, X_train, X_test)","413d91c9":"cprint('LogRegSmote_tuned Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","ce3c060e":"from yellowbrick.classifier import ClassPredictionError\nvisualizer = ClassPredictionError(LogRegSmote_tuned)\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n# Draw visualization\nvisualizer.poof();","a8d79c85":"cprint('roc_curve','green', 'on_red')\nplot_roc_curve(LogRegSmote_tuned, X_test, y_test);","941d0d1f":"cprint('precision_recall_curve','green', 'on_red')\nplot_precision_recall_curve(LogRegSmote_tuned, X_test, y_test);","a9be287c":"cprint('LogRegSmote_tuned Predictions','green', 'on_red')\nLogRegSmote_Pred = {\"Actual\": y_test, \"LogRegSmote_Pred\":y_pred}\nLogRegSmote_Pred = pd.DataFrame.from_dict(LogRegSmote_Pred)\nLogRegSmote_Pred.sample(5)","7983ad14":"pd.crosstab(LogRegSmote_Pred['Actual'], LogRegSmote_Pred['LogRegSmote_Pred'])","15005631":"cprint('Predictions','green', 'on_red')\nLogRegSmote_Pred.drop(\"Actual\", axis = 1, inplace = True)\nModel_Preds = pd.merge(Model_Preds, LogRegSmote_Pred, left_index = True, right_index = True)\nModel_Preds.sample(5)","8a1ce597":"# logistic_regression_smote = pickle.dump(LogRegSmote_tuned, open('logistic_regression_smote(tuned)', 'wb'))","7a75b2d9":"RF_smote_model = RandomForestClassifier(class_weight = \"balanced\", random_state = 42)\nRF_smote_model.fit(X_train, y_train)\ny_pred = RF_smote_model.predict(X_test)\ny_train_pred = RF_smote_model.predict(X_train)","8ba19f2c":"RF_smote_model_f1 = f1_score(y_test, y_pred)\nRF_smote_model_acc = accuracy_score(y_test, y_pred)\nRF_smote_model_recall = recall_score(y_test, y_pred)\nRF_smote_model_auc = roc_auc_score(y_test, y_pred)\nRF_smote_model_pre = precision_score(y_test, y_pred)\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\nRF_smote_model_recall_auc = auc(recall, precision)","159e36e3":"print(\"RF_smote_model\")\nprint (\"------------------\")\neval(RF_smote_model, X_train, X_test)","219691da":"cprint('RF_smote_model Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","6dea7fae":"from yellowbrick.classifier import ClassPredictionError\nvisualizer = ClassPredictionError(RF_smote_model)\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n# Draw visualization\nvisualizer.poof();","aba8ce95":"cprint('Cross Validation scores for Random Forest with default parameters(SMOTE)','green', 'on_red')\nRF_smote_cv = RandomForestClassifier(class_weight = \"balanced\", random_state = 42)\nRF_smote_cv_scores = cross_validate(RF_smote_cv, X_train, y_train, \n                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\nRF_smote_cv_scores = pd.DataFrame(RF_smote_cv_scores, index = range(1, 11))\nRF_smote_cv_scores.mean()[2:]","735ee680":"param_grid = {'n_estimators' : [50, 100, 300],\n              'max_features' : [2, 3, 4],\n              'max_depth' : [3, 5, 7],\n              'min_samples_split' : [2, 5, 8]}","3ad8dc4e":"RF_smote_grid = RandomForestClassifier(class_weight = 'balanced', random_state = 42)\nRF_smote_grid_model = GridSearchCV(estimator = RF_grid, \n                             param_grid = param_grid, \n                             scoring = \"recall\", \n                             n_jobs = -1, verbose = 2)\nRF_smote_grid_model.fit(X_train, y_train)","64843bd1":"print(colored('\\033[1mBest Estimators of GridSearchCV for Random Forest Smote Model:\\033[0m', 'blue'), colored(RF_smote_grid_model.best_estimator_, 'red'))","4ad0f2d6":"print(colored('\\033[1mBest Parameters of GridSearchCV for Random Forest Smote Model:\\033[0m', 'blue'), colored(RF_smote_grid_model.best_params_, 'red'))","bc03c1e1":"RFSmote_tuned = RandomForestClassifier(class_weight = 'balanced',\n                                       max_depth = 7,\n                                       max_features = 4,\n                                       min_samples_split = 5,\n                                       n_estimators = 100,\n                                       random_state = 42).fit(X_train, y_train)","99e9954d":"y_pred = RFSmote_tuned.predict(X_test)\ny_train_pred = RFSmote_tuned.predict(X_train)\n\nRFSmote_tuned_f1 = f1_score(y_test, y_pred)\nRFSmote_tuned_acc = accuracy_score(y_test, y_pred)\nRFSmote_tuned_recall = recall_score(y_test, y_pred)\nRFSmote_tuned_auc = roc_auc_score(y_test, y_pred)\nRFSmote_tuned_pre = precision_score(y_test, y_pred)\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\nRFSmote_tuned_recall_auc = auc(recall, precision)","7baa0ccf":"print(\"RFSmote_tuned\")\nprint (\"------------------\")\neval(RFSmote_tuned, X_train, X_test)","f6d4d422":"cprint('RFSmote_tuned Scores','green', 'on_red')\ntrain_val(y_train, y_train_pred, y_test, y_pred)","b032c824":"from yellowbrick.classifier import ClassPredictionError\nvisualizer = ClassPredictionError(RFSmote_tuned)\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n# Draw visualization\nvisualizer.poof();","b9ef1b82":"cprint('roc_curve','green', 'on_red')\nplot_roc_curve(RFSmote_tuned, X_test, y_test);","43139779":"cprint('precision_recall_curve','green', 'on_red')\nplot_precision_recall_curve(RFSmote_tuned, X_test, y_test);","74c6a23f":"cprint('RFSmote_tuned Predictions','green', 'on_red')\nRFSmote_Pred = {\"Actual\": y_test, \"RFSmote_Pred\":y_pred}\nRFSmote_Pred = pd.DataFrame.from_dict(RFSmote_Pred)\nRFSmote_Pred.sample(5)","4e44166c":"pd.crosstab(RFSmote_Pred['Actual'], RFSmote_Pred['RFSmote_Pred'])","f647c69b":"cprint('Predictions','green', 'on_red')\nRFSmote_Pred.drop(\"Actual\", axis = 1, inplace = True)\nModel_Preds = pd.merge(Model_Preds, RFSmote_Pred, left_index = True, right_index = True)\nModel_Preds.sample(10)","20f3ed5a":"# random_forest_smote = pickle.dump(RFSmote_tuned, open('random_forest_smote(tuned)', 'wb'))","859773f5":"df_dl = df_out.copy()","e689b69a":"scaler = StandardScaler()\n\ndf_dl[\"amount\"] = scaler.fit_transform(df_dl[\"amount\"].values.reshape(-1,1))\ndf_dl[\"time\"] = scaler.fit_transform(df_dl[\"time\"].values.reshape(-1,1))","26dcfd47":"X = df_dl.drop(['class'], axis = 1)\ny = df_dl['class']","e28ff28f":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2, random_state = 42)","1e0a63ac":"print('X_train.shape : ', X_train.shape)\nprint('X_test.shape  : ', X_test.shape)","fbae2dce":"cprint('y_train.value_counts','green', 'on_red')\nprint(\"-----\"*10)\ny_train.value_counts()","1beeac22":"cprint('y_test.value_counts','green', 'on_red')\nprint(\"-----\"*10)\ny_test.value_counts()","b867dd78":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam, SGD, Adamax, RMSprop,Adadelta\nfrom tensorflow.keras.layers import Dropout\nfrom sklearn.utils import class_weight\nfrom sklearn.utils import compute_class_weight\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import plot_roc_curve, roc_auc_score, roc_curve, average_precision_score\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.model_selection import GridSearchCV","c5a1b690":"dl_model = Sequential()\n\ndl_model.add(Dense(32, activation = \"relu\"))\ndl_model.add(Dropout(0.25))\ndl_model.add(Dense(16, activation = \"relu\"))\ndl_model.add(Dropout(0.25))\ndl_model.add(Dense(1, activation = \"sigmoid\"))","5c1c3617":"optimizer = Adam(lr = 0.001)\ndl_model.compile(loss = 'binary_crossentropy',\n                 optimizer = optimizer,\n                 metrics = [\"Recall\"])","f418a923":"train_classes = y_train\nclass_weights = compute_class_weight(\n                                        class_weight = \"balanced\",\n                                        classes = np.unique(train_classes),\n                                        y = train_classes                                                    \n                                    )\nclass_weights = dict(zip(np.unique(train_classes), class_weights)),\nprint(\"class_weights : \", class_weights)","99befef7":"early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"auto\", verbose = 1, patience = 20, restore_best_weights = True)","c311491d":"dl_model.fit(x = X_train, y = y_train, \n             validation_split = 0.1, \n             batch_size = 64, \n             epochs = 500, \n             verbose = 0, \n             callbacks = [early_stop], \n             class_weight = {0: 0.500766887790496, 1: 326.4929328621908})","361bdb13":"dl_model.summary()","e2b8ad38":"cprint('DL_model loss_df','green', 'on_red')\nloss_df = pd.DataFrame(dl_model.history.history)\nloss_df.head(1)","b5ba7f43":"cprint('DL_model loss_df','green', 'on_red')\nloss_df.plot();","fc7224b0":"cprint('DL_model Evaluation(train_set)','green', 'on_red')\nprint(\"-----\"*10)\ndl_model.evaluate(X_train, y_train)","e5ac70f0":"cprint('DL_model Evaluation(test_set)','green', 'on_red')\nprint(\"-----\"*10)\ndl_model.evaluate(X_test, y_test)","29f07e6b":"cprint('DL_model loss & accuracy','green', 'on_red')\nprint(\"-----\"*10)\nloss, accuracy = dl_model.evaluate(X_test, y_test, verbose=0)\nprint(\"loss     : \", loss)\nprint(\"accuracy : \", accuracy)","01415aa0":"cprint('Confusion_Matrix & Classification_Report','green', 'on_red')\nprint(\"-----\"*10)\ny_pred = (dl_model.predict(X_test) > 0.5).astype(\"int32\")\n#y_pred = model.predict_classes(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","e32f809b":"y_pred_proba = dl_model.predict(X_test)\n\nDL_Acc = accuracy_score(y_test, y_pred)\nDL_AP = average_precision_score(y_test, y_pred_proba)\nDL_f1 = f1_score(y_test, y_pred)\nDL_rec = recall_score(y_test, y_pred)\nDL_roc_auc = roc_auc_score(y_test, y_pred_proba)\nDL_pre = precision_score(y_test, y_pred)\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\nDL_recall_auc = auc(recall, precision)","6fee0034":"cprint('DL_model Scores','green', 'on_red')\nprint(\"-----\"*10)\nprint(\"DL_Acc           :\", DL_Acc)\nprint(\"DL_AP            :\", DL_AP)\nprint(\"DL_f1            :\", DL_f1)\nprint(\"DL_rec           :\", DL_rec)\nprint(\"DL_roc_auc       :\", DL_roc_auc)\nprint(\"DL_pre           :\", DL_pre)\nprint(\"DL_recall_auc    :\", DL_recall_auc)","507424bc":"def build_classifier(optimizer):\n    classifier = Sequential()\n    classifier.add(Dense(units = 32, activation = 'relu'))\n    classifier.add(Dropout(0.25))\n    classifier.add(Dense(units = 16, activation = 'relu'))\n    classifier.add(Dropout(0.25))\n    classifier.add(Dense(units = 1, activation = 'sigmoid'))\n    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = [\"Recall\"])\n    return classifier","fb348f58":"print(\"class_weights : \", class_weights)","4b76125c":"early_stop = EarlyStopping(monitor = \"loss\", mode = \"auto\", verbose = 1, patience = 20, restore_best_weights=True)","e7c7c1eb":"classifier = KerasClassifier(build_fn = build_classifier, epochs = 20)\nparameters = {'batch_size': [32, 64],\n              'optimizer': ['adam', 'rmsprop', \"SGD\", \"adagrad\", \"adadelta\"]}\ngrid_model = GridSearchCV(estimator = classifier,\n                          param_grid = parameters,\n                          scoring = 'recall',\n                          cv = 10,\n                          n_jobs = -1,\n                          verbose = 0)\ngrid_model.fit(X_train, y_train, callbacks = [early_stop], class_weight = {0: 0.500766887790496, 1: 326.4929328621908})","8b21dae2":"print(\"grid_model.best_score_ :\", grid_model.best_score_)","7fdde7e2":"print(\"grid_model.best_params_ :\", grid_model.best_params_)","cb039428":"y_test_pred = (grid_model.predict(X_test) > 0.5).astype(\"int32\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(classification_report(y_test, y_test_pred))","832ff67c":"dl_tuned = Sequential()\n\ndl_tuned.add(Dense(32, activation = \"relu\"))\ndl_tuned.add(Dropout(0.25))\ndl_tuned.add(Dense(16, activation = \"relu\"))\ndl_tuned.add(Dropout(0.25))\ndl_tuned.add(Dense(1, activation = \"sigmoid\"))","e35d6312":"optimizer = SGD(lr = 0.001)\ndl_tuned.compile(loss = 'binary_crossentropy',\n                 optimizer = optimizer,\n                 metrics = [\"Recall\"])","378a866c":"train_classes = y_train\nclass_weights = compute_class_weight(\n                                        class_weight = \"balanced\",\n                                        classes = np.unique(train_classes),\n                                        y = train_classes                                                    \n                                    )\nclass_weights = dict(zip(np.unique(train_classes), class_weights)),\nclass_weights","6198b57c":"early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"auto\", verbose = 1, patience = 20, restore_best_weights = True)","91c167df":"dl_tuned.fit(x = X_train, y = y_train, \n             validation_split = 0.1, \n             batch_size = 64, \n             epochs = 500, \n             verbose = 0, \n             callbacks = [early_stop], \n             class_weight = {0: 0.500766887790496, 1: 326.4929328621908})","67c5cc14":"dl_tuned.summary()","1ce81ba8":"cprint('DL_tuned loss_df','green', 'on_red')\nloss_df = pd.DataFrame(dl_tuned.history.history)\nloss_df.head(1)","9b17796e":"cprint('DL_tuned loss_df','green', 'on_red')\nloss_df.plot();","25c3eb96":"cprint('DL_tuned Evaluation(train_set)','green', 'on_red')\nprint(\"-----\"*10)\ndl_tuned.evaluate(X_train, y_train)","8ae5a6f2":"cprint('DL_tuned Evaluation(test_set)','green', 'on_red')\nprint(\"-----\"*10)\ndl_tuned.evaluate(X_test, y_test)","8e2571eb":"cprint('DL_tuned loss & accuracy','green', 'on_red')\nprint(\"-----\"*10)\nloss, accuracy = dl_tuned.evaluate(X_test, y_test, verbose=0)\nprint(\"loss     : \", loss)\nprint(\"accuracy : \", accuracy)","a374757a":"cprint('Confusion_Matrix & Classification_Report','green', 'on_red')\ny_pred = (dl_tuned.predict(X_test) > 0.5).astype(\"int32\")\n#y_pred = model.predict_classes(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","2c4c93de":"y_pred_proba = dl_tuned.predict(X_test)\n\nDLTuned_Acc = accuracy_score(y_test, y_pred)\nDLTuned_AP = average_precision_score(y_test, y_pred_proba)\nDLTuned_f1 = f1_score(y_test, y_pred)\nDLTuned_rec = recall_score(y_test, y_pred)\nDLTuned_roc_auc = roc_auc_score(y_test, y_pred_proba)\nDLTuned_pre = precision_score(y_test, y_pred)\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\nDLTuned_recall_auc = auc(recall, precision)","6f44ebb6":"cprint('DL_model Scores','green', 'on_red')\nprint(\"-----\"*10)\nprint(\"DL_Tuned_Acc           :\", DLTuned_Acc)\nprint(\"DL_Tuned_AP            :\", DLTuned_AP)\nprint(\"DL_Tuned_f1            :\", DLTuned_f1)\nprint(\"DL_Tuned_rec           :\", DLTuned_rec)\nprint(\"DL_Tuned_roc_auc       :\", DLTuned_roc_auc)\nprint(\"DL_Tuned_pre           :\", DLTuned_pre)\nprint(\"DL_Tuned_recall_auc    :\", DLTuned_recall_auc)","85b4ccdc":"fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Keras Model_Weighted')\nplt.show()","26d8a503":"cprint('optimal_threshold for roc_curve','green', 'on_red')\nprint(\"-----\"*10)\nfp_rate, tp_rate, thresholds = roc_curve(y_test, y_pred_proba)\noptimal_idx = np.argmax(tp_rate - fp_rate)\noptimal_threshold = thresholds[optimal_idx]\noptimal_threshold","6646b5f6":"y_pred_proba = dl_tuned.predict(X_test)\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(precision, recall)\nplt.xlabel('precision')\nplt.ylabel('recall')\nplt.title('Precision Recall Curve')\nplt.show()","ad43f471":"cprint('optimal_threshold for precision_recall_curve','green', 'on_red')\nprint(\"-----\"*10)\nprecisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)\noptimal_idx = np.argmax((2 * precisions * recalls) \/ (precisions + recalls))  \noptimal_threshold = thresholds[optimal_idx]\noptimal_threshold","27090f37":"# dl_tuned.save('neural_network(tuned).h5')","5a9c8183":"# from tensorflow.keras.models import load_model","330391e3":"# neural_network = load_model('neural_network(tuned).h5')","a573627e":"Actual = pd.DataFrame(y_test)\nActual.head()","fc82c744":"y_pred_proba[:5]","74b4ea18":"y_pred = (dl_model.predict(X_test) > 0.5).astype(\"int32\")\ny_pred[:5]","fc435723":"cprint('DL_tuned Predictions','green', 'on_red')\nDL_pred = pd.DataFrame(y_pred, index = Actual.index)\nDL_pred = pd.concat([Actual, DL_pred], axis=1)\nDL_pred.rename(columns = {'class':'Actual', 0:'DL_Pred'}, inplace = True)\nDL_pred.head()","3482a2ab":"DL_pred.sample(10)","f9baf69f":"pd.crosstab(DL_pred['Actual'], DL_pred['DL_Pred'])","819db992":"compare = pd.DataFrame({\"Model\": [\"LogReg_Model\", \"LogReg_Tuned\", \"RF_Model\", \"RF_Tuned\", \"LogRegSmote_Model\",\n                                  \"LogRegSmote_Tuned\", \"RFSmote_Model\", \"RFSmote_Tuned\", \"DL_Model\", \"DL_Tuned\"],\n                        \n                        \"Accuracy_Score\": [LogReg_model_acc, LogReg_tuned_acc, RF_model_acc, RF_tuned_acc, LogReg_smote_model_acc,\n                                  LogRegSmote_tuned_acc, RF_smote_model_acc, RFSmote_tuned_acc, DL_Acc, DLTuned_Acc],\n                        \n                        \"F1_Score\": [LogReg_model_f1, LogReg_tuned_f1, RF_model_f1, RF_tuned_f1, \n                                  LogReg_smote_model_f1, LogRegSmote_tuned_f1, RF_smote_model_f1, RFSmote_tuned_f1, DL_f1,\n                                  DLTuned_f1],\n                        \n                        \"Precision_Score\": [LogReg_model_pre, LogReg_tuned_pre, RF_model_pre, RF_tuned_pre, LogReg_smote_model_pre,\n                                  LogRegSmote_tuned_pre, RF_smote_model_pre, RFSmote_tuned_pre, DL_AP, DLTuned_AP],\n                                                 \n                        \"Recall_Score\": [LogReg_model_recall, LogReg_tuned_recall, RF_model_recall, RF_tuned_recall, \n                                  LogReg_smote_model_recall, LogRegSmote_tuned_recall, RF_smote_model_recall, RFSmote_tuned_recall,\n                                  DL_rec, DLTuned_rec],\n                        \n                        \"Roc_Auc_Score\": [LogReg_model_auc, LogReg_tuned_auc, RF_model_auc, RF_tuned_auc, LogReg_smote_model_auc,\n                                  LogRegSmote_tuned_auc, RF_smote_model_auc, RFSmote_tuned_auc, DL_roc_auc, DLTuned_roc_auc],\n                       \n                        \"Recall_Auc_Score\": [LogReg_model_recall_auc, LogReg_tuned_recall_auc, RF_model_recall_auc, \n                                  RF_tuned_recall_auc, LogReg_smote_model_recall_auc, LogRegSmote_tuned_recall_auc, \n                                  RF_smote_model_recall_auc, RFSmote_tuned_recall_auc, DL_recall_auc, DLTuned_recall_auc]})\n  \n    \ncompare = compare.sort_values(by=\"Recall_Score\", ascending=True)\nfig = px.bar(compare, x = \"Recall_Score\", y = \"Model\", title = \"Recall_Score\")\nfig.show()\n\ncompare = compare.sort_values(by=\"F1_Score\", ascending=True)\nfig = px.bar(compare, x = \"F1_Score\", y = \"Model\", title = \"F1_Score\")\nfig.show()\n\ncompare = compare.sort_values(by=\"Roc_Auc_Score\", ascending=True)\nfig = px.bar(compare, x = \"Roc_Auc_Score\", y = \"Model\", title = \"Roc_Auc_Score\")\nfig.show()\n\ncompare = compare.sort_values(by=\"Recall_Auc_Score\", ascending=True)\nfig = px.bar(compare, x = \"Recall_Auc_Score\", y = \"Model\", title = \"Recall_Auc_Score\")\nfig.show()\n\ncompare = compare.sort_values(by=\"Precision_Score\", ascending=True)\nfig = px.bar(compare, x = \"Precision_Score\", y = \"Model\", title = \"Precision_Score\")\nfig.show()\n\ncompare = compare.sort_values(by=\"Accuracy_Score\", ascending=True)\nfig = px.bar(compare, x = \"Accuracy_Score\", y = \"Model\", title = \"Accuracy_Score\")\nfig.show()","e0d419a4":"compare.sort_values(by=\"Recall_Score\", ascending=False)","badd35db":"df_out.corr()['class'].sort_values().drop('class').iplot(kind = 'barh', title = 'Correlation Between the Columns');","4d45d92a":"RFSmote_feature_imp = pd.DataFrame(index=X.columns, data = RF_smote_model.feature_importances_, columns = ['Importance']).sort_values(\"Importance\", ascending = False)\n\nfig = px.bar(RFSmote_feature_imp.sort_values('Importance', ascending = False), x = RFSmote_feature_imp.sort_values('Importance', \n             ascending = False).index, y = 'Importance', title = \"RFSmote Feature_Importance\", \n             labels = dict(x = \"Features\", y =\"Importance\"))\nfig.show()","b3ba0f0b":"RFSmote_tuned_feature_imp = pd.DataFrame(index=X.columns, data = RFSmote_tuned.feature_importances_, columns = ['Importance']).sort_values(\"Importance\", ascending = False)\n\nfig = px.bar(RFSmote_tuned_feature_imp.sort_values('Importance', ascending = False), x = RFSmote_tuned_feature_imp.sort_values('Importance', \n             ascending = False).index, y = 'Importance', title = \"RFSmote_tuned Feature_Importance\", \n             labels = dict(x = \"Features\", y =\"Importance\"))\nfig.show()","4a432820":"df_deploy = df_out[['v2', 'v3', 'v4', 'v7', 'v10', 'v11', 'v12', 'v14', 'v16', 'v17', 'class']].copy()\ndf_deploy.head(1)","b58f961a":"X = df_deploy.drop(['class'], axis = 1)\ny = df_deploy['class']","b936f30a":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\n\nover = SMOTE(sampling_strategy = {1: 10000})\nunder = RandomUnderSampler(sampling_strategy = {0: 10000})\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps = steps)\nX, y = pipeline.fit_resample(X, y)","3fa893d8":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2, random_state = 42)","a9767ff7":"print('X_train.shape : ', X_train.shape)\nprint('X_test.shape  : ', X_test.shape)","1f762352":"cprint('y_train.value_counts','green', 'on_red')\ny_train.value_counts()","9affd81f":"cprint('y_test.value_counts','green', 'on_red')\ny_test.value_counts()","7ec89b13":"models = []\n\nmodels.append((\"LogReg_Deploy\", LogisticRegression(class_weight = 'balanced',\n                                                   penalty = 'l2', \n                                                   solver = 'lbfgs',\n                                                   random_state = 42)))\nmodels.append((\"RandomForest_Deploy\", RandomForestClassifier(class_weight = 'balanced',\n                                                             max_depth = 7,\n                                                             max_features = 4,\n                                                             min_samples_split = 2,\n                                                             n_estimators = 50,\n                                                             random_state = 42)))\nmodels.append((\"KNN_trial\", KNeighborsClassifier()))\nmodels.append((\"SVC_trial\", SVC(class_weight=\"balanced\", random_state=42)))\nmodels.append((\"DTC_trial\", DecisionTreeClassifier(class_weight=\"balanced\", random_state=42)))\nmodels.append((\"ADA_trial\", AdaBoostClassifier(random_state=42)))\nmodels.append((\"GBC_trial\", GradientBoostingClassifier(random_state=42)))\nmodels.append((\"XGB_trial\", XGBClassifier(random_state=42, verbosity = 0)))\nmodels.append((\"LGB_trial\", LGBMClassifier(random_state=42)))\n\n# evaluate each model in turn\n\nresults = []\nnames = []\nf1_scores = []\nrecall_scores = []\nroc_auc_scores = []\n\nfor name, model in models:\n        \n    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=\"recall\")\n\n    results.append(cv_results)\n    names.append(name)\n\n    print(f\"{name} MODEL: {round(cv_results.mean(), 4)}\")\n        \n    y_pred = model.fit(X_train, y_train).predict(X_test)\n\n    f1_scores.append(f1_score(y_test, y_pred))\n    recall_scores.append(recall_score(y_test, y_pred))\n    roc_auc_scores.append(roc_auc_score(y_test, y_pred))    \n\nresult_df = pd.DataFrame(results, columns=[i for i in range(1, 11)], index=names).T\nresult_df.iplot(kind=\"box\", boxpoints=\"all\", title=\"CV Results\")\n\ncompare = pd.DataFrame({\"F1\": f1_scores,\n                        \"Recall\": recall_scores,\n                        \"ROC AUC\": roc_auc_scores                        \n                       }, index=names)\n\nfor score in compare.columns:\n    compare[score].sort_values().iplot(kind=\"barh\", title=f\"{score} Score\")\n    \ncompare","7dc6b465":"LogReg_Deploy = LogisticRegression(class_weight = 'balanced',\n                                         penalty = 'l2', \n                                         solver = 'lbfgs',\n                                         random_state = 42).fit(X_train, y_train)\n\ny_pred = LogReg_Deploy.predict(X_test)\ny_train_pred = LogReg_Deploy.predict(X_train)\n\nLogReg_Deploy_f1 = f1_score(y_test, y_pred)\nLogReg_Deploy_acc = accuracy_score(y_test, y_pred)\nLogReg_Deploy_recall = recall_score(y_test, y_pred)\nLogReg_Deploy_auc = roc_auc_score(y_test, y_pred)\nLogReg_Deploy_pre = precision_score(y_test, y_pred)\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\nLogReg_Deploy_recall_auc = auc(recall, precision)\n\nprint(\"LogReg_Deploy\")\nprint (\"------------------\")\neval(LogReg_Deploy, X_train, X_test)","b983528b":"RandomForest_Deploy = RandomForestClassifier(class_weight = 'balanced',\n                                       max_depth = 7,\n                                       max_features = 4,\n                                       min_samples_split = 2,\n                                       n_estimators = 50,\n                                       random_state = 42).fit(X_train, y_train)\n\ny_pred = RandomForest_Deploy.predict(X_test)\ny_train_pred = RandomForest_Deploy.predict(X_train)\n\nRandomForest_Deploy_f1 = f1_score(y_test, y_pred)\nRandomForest_Deploy_acc = accuracy_score(y_test, y_pred)\nRandomForest_Deploy_recall = recall_score(y_test, y_pred)\nRandomForest_Deploy_auc = roc_auc_score(y_test, y_pred)\nRandomForest_Deploy_pre = precision_score(y_test, y_pred)\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\nRandomForest_Deploy_recall_auc = auc(recall, precision)\n\nprint(\"RandomForest_Deploy\")\nprint (\"------------------\")\neval(RandomForest_Deploy, X_train, X_test)","4c9bfc93":"# logistic_regression = pickle.dump(LogReg_Deploy, open('logistic_regression_model', 'wb'))","ef238686":"# random_forest_classifier = pickle.dump(RandomForest_Deploy, open('random_forest_model', 'wb'))","ab53d75f":"<a id=\"3\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">3 - ANALYSIS<p>","0f92d018":"<a id=\"6.3.4\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.3.4 Random Forest Classifier GridSearchCV<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","1814cf83":"<a id=\"4.3\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.3 Data Cleaning<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","8c5b6124":"- 'remove_outliers' function assigns the NaN-value first and then drop related rows, according to the class and the entered\nwhis value and plots the boxplot for the relevant column.","df3895fe":"<a id=\"9\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">9 - FINAL MODEL<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \nFor the final model, I will first look at the correlation values of the df_out columns and the feature importance values of the Random Forest Classifier default and tuned models created with SMOTE data above. ","d531b44f":"![](https:\/\/cdn.dnaindia.com\/sites\/default\/files\/styles\/full\/public\/2017\/04\/04\/562375-cyberfraud-040417.jpg)\n\n**Image credit:** [dnaindia](https:\/\/www.dnaindia.com\/mumbai\/report-goregaon-man-duped-of-rs-30k-in-credit-card-fraud-2380446)","24786743":"<a id=\"2.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">2.1 User Defined Functions<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**We have defined some useful user defined functions.**","06ee8dff":"<a id=\"1.4\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.4 Project Structure & Tasks<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n#### 1. Exploratory Data Analysis & Data Cleaning\n\n- Import Modules, Load Data & Data Review\n- Exploratory Data Analysis\n- Data Cleaning\n    \n#### 2. Data Preprocessing\n\n- Scaling\n- Train - Test Split\n\n#### 3. Model Building\n\n- Logistic Regression without SMOTE\n- Apply SMOTE\n- Logistic Regression with SMOTE\n- Random Forest Classifier with SMOTE\n- Neural Network\n\n#### 4. Model Deployement\n\n- Save and Export the Model as .pkl\n- Save and Export Variables as .pkl ","779f5737":"**Final Model with Random Forest Classifier (With Feature Importance and SMOTE)**","885e2830":"- 'num_outliers' function gives max\/min threshold, number of data, number of outlier and plots its boxplot,\naccording to the tree type and the entered z-score value for the relevant column.\n- We can see the number of outliers column by column below.","dff2af66":"The scores of the other models I ran with the default parameters are very good.\nHowever, I'll go with Logistic Regression and Random Forest Classifier for model deployment. ","6137ac34":"<a id=\"1.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.1 Context<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\nFraud that involves cell phones, insurance claims, tax return claims, credit card transactions, government procurement etc. represent significant problems for governments and businesses and specialized analysis techniques for discovering fraud using them are required. These methods exist in the areas of Knowledge Discovery in Databases (KDD), Data Mining, Machine Learning and Statistics. They offer applicable and successful solutions in different areas of electronic fraud crimes.\n\nIn general, the primary reason to use data analytics techniques is to tackle fraud since many internal control systems have serious weaknesses. For example, the currently prevailing approach employed by many law enforcement agencies to detect companies involved in potential cases of fraud consists in receiving circumstantial evidence or complaints from whistleblowers. As a result, a large number of fraud cases remain undetected and unprosecuted. In order to effectively test, detect, validate, correct error and monitor control systems against fraudulent activities, businesses entities and organizations rely on specialized data analytics techniques such as data mining, data matching, sounds like function, Regression analysis, Clustering analysis and Gap. Techniques used for fraud detection fall into two primary classes: statistical techniques and artificial intelligence.\n\nhttps:\/\/en.wikipedia.org\/wiki\/Data_analysis_techniques_for_fraud_detection","cd716ba0":"<a id=\"5.3.3\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.3.3 Feature Importance for Random Forest Model<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","69694c4e":"- As seen above, there are fraudulent transactions in every quarter. \n- Q-1 has the highest number of transactions with 213 transactions.\n- Q-4 has the highest amount with 56014.34 amount.","4cb508cb":"<a id=\"5.2.2\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.2.2 Evaluating Model Performance<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","6f1a514a":"<a id=\"6.1.2\"><\/a>\n#### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.1.2 Applying SMOTE and Train - Test Split<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","9b4e7deb":"<a id=\"6\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">6 - MODEL BUILDING WITH SMOTE<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \nThis dataset is severely **unbalanced** (most of the transactions are non-fraud). So the algorithms are much more likely to classify new observations to the majority class and high accuracy won't tell us anything. To address the problem of imbalanced dataset we can use undersampling and oversampling data approach techniques. Oversampling increases the number of minority class members in the training set. The advantage of oversampling is that no information from the original training set is lost unlike in undersampling, as all observations from the minority and majority classes are kept. On the other hand, it is prone to overfitting. \n\nThere is a type of oversampling called **[SMOTE](https:\/\/www.geeksforgeeks.org\/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python\/)** (Synthetic Minority Oversampling Technique), which we are going to use to make our dataset balanced. It creates synthetic points from the minority class.","2b0b4fc9":"<a id=\"6.3\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">6.3 Random Forest Classifier with SMOTE<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","a0e888cb":"<a id=\"6.1\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">6.1 Data Pre - Processing<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","8255c25b":"<a id=\"5.3.5\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.3.5 Random Forest Classifier GridSearchCV<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","f8c97ad1":"<a id=\"5.1\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">5.1 Data Pre - Processing<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","e352d8d9":"<a id=\"7.1.2\"><\/a>\n#### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7.1.2 Train - Test Split<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","ac0668b4":"<a id=\"5.3\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">5.3 Random Forest Classifier without SMOTE<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","18a14543":"**The dataset is too large. Let's take a look at how the distribution of class=1 and class=0 by quartiles is shaped.**","4c35ba36":"<a id=\"7.7\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7.7 Neural Network GridSearchCV<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","d474ef6a":"<a id=\"6.2.2\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.2.2 Evaluating Model Performance<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","87ffa011":"---\n---\n","be9bbd00":"**Final Model with Logistic Regression (With Feature Importance and SMOTE)**","65bec7fa":"<a id=\"7.9.1\"><\/a>\n#### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7.9.1 Loading Model and Scaler<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","fe021a00":"![image.png](attachment:image.png)","606ca593":"<a id=\"7.4\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7.4 Compile Model<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","942a971f":"- 'class' column has binary type values.\n- We have an imbalanced data. Class frequencies of the target variable are quite imbalanced.\n- Almost 0.17 % of the transactions are fraudulent(473).\n- Almost 99.83 % of the transactions are non-fraudulent(283253).","b1e987d4":"<a id=\"7.6\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7.6 Evaluating Model Performance<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","3db5d551":"<a id=\"7.2\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7.2 Import Libraries<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","47245a92":"<a id=\"6.3.5\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.3.5 Random Forest Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","9ca51275":"<a id=\"6.2.1\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.2.1 Model Building with Logistic Regression<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","93283e66":"What about the outliers?","51078ab5":"***According to the basic examinations on the dataset;***\n\n- We have a classification problem.\n- We are going to make classification on the target variable \"class\".\n- And we will build a model to get the best classification on the \"class\" column.\n- Because of that we are going to look at the balance of \"class\" column.\n- The dataset has 31 columns and 283726 observations after dropping of duplicated observations.\n- All columns contain numerical values. \n- There seems to be no missing value. ","30b5c85d":"<a id=\"4\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">4 - DATA CLEANING & EXPLORATORY DATA ANALYSIS (EDA)<p>\n\n**Exploratory Data Analysis is an initial process of analysis, in which we can summarize characteristics of data such as pattern, trends, outliers, and hypothesis testing using descriptive statistics and visualization.**","2196cfe9":"<a id=\"5.2.6\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.2.6 Prediction<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","e5e47667":"<a id=\"5.3.6\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.3.6 Random Forest Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","08a7e5a8":"<a id=\"1\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">1 - DATA<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where it has **492 frauds** out of **284,807** transactions. The dataset is **highly unbalanced**, the positive class (frauds) account for 0.172% of all transactions.\n\n**For a better understanding and more information, please refer to https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud**","9a37197f":"<a id=\"5.3.1\"><\/a>\n#### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.3.1 Model Building with Random Forest Classifier<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","c7520820":"<a id=\"8\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">8 - THE COMPARISON OF MODELS<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","3792463f":"<a id=\"6.1.1\"><\/a>\n#### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.1.1 Scaling<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","dc2ad9bc":"<a id=\"4.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.2 Examination of Features and Data Insights<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**In the given dataset, feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. So, we can divide data into two groups and compare their characteristics. Here, we can find the average of both the groups using groupby() and mean() function.**   ","1a8fa411":"- There seems to be no missing value.","174f9e04":"# WELCOME!","b40596e2":"<a id=\"6.2.4\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.2.4 Logistic Regression GridSearchCV<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","002796df":"Let's look the correlation and multicollinearity among the features.","c07649b4":"**'amount' Column**\n\n- Amount: This feature is the transaction Amount, can be used for example-dependant cost-senstive learning.","0b864bf3":"<a id=\"7.3\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7.3 Define Model<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","6bae8e7b":"<a id=\"10.1\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">10.1 Save and Export the Model as .pkl<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \nI save the models for deployment below. ","f0ee40f7":"Based on the above values, I create the data for my final model with the following columns. ","61bc0155":"<a id=\"6.2\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">6.2 Logistic Regression with SMOTE<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","6f7a57f0":"<a id=\"5.2.5\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.2.5 Logistic Regression ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","ebeb0167":"<a id=\"7.9\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7.9 Prediction<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","3fad3474":"Check Missing Values and Outliers","99ff4a36":"<a id=\"4.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.1 A General Look at the Data<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","28ec61e7":"<a id=\"6.2.6\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.2.6 Prediction<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","dc9657f5":"<a id=\"1.3\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.3 What the Problem is<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\nThe aim of this study is to predict whether a credit card transaction is fraudulent. Of course, this is not easy to do.\nFirst of all, we need to analyze and recognize the data well in order to draw a roadmap and choose the correct arguments to use. Accordingly, we can examine the frequency distributions of variables. We can observe variable correlations and want to explore multicollinearity. We can show the distribution of the target variable's classes over other variables. \nAlso, it is useful to take missing values and outliers.\n\nAfter these procedures, we can move on to the model building stage by doing the basic data pre-processing we are familiar with. ","9ac60d32":"<a id=\"10.2\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">10.2 Example of An Application Code for Deployment<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \nBelow is an example of an app.py. With this app, I deployed my model via streamlit for real life predictions.","294cf0ac":"**'time' Column**\n\n- Time: This feature is contains the seconds elapsed between each transaction and the first transaction in the dataset.","50ab2c83":"<a id=\"5.3.4\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.3.4 Random Forest Classifier Cross Validation<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","48944cba":"<a id=\"toc\"><\/a>\n\n## <h3 style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\" class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tablist\" aria-controls=\"home\">TABLE OF CONTENTS<\/h3>\n\n* [1 - DATA](#1)\n    * [1.1 Context](#1.1)\n    * [1.2 About the Features](#1.2) \n    * [1.3 What the Problem is](#1.3) \n    * [1.4 Project Structure & Tasks](#1.4) \n* [2 - LIBRARIES NEEDED IN THE STUDY](#2)\n    * [2.1 User Defined Functions](#2.1)\n* [3 - ANALYSIS](#3)\n    * [3.1 Loading & Reading the Data](#3)\n* [4 - EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION](#4)\n    * [4.1 A General Look at the Data](#4.1)\n    * [4.2 Examination of Features and Data Insights](#4.2)\n    * [4.3 Data Cleaning](#4.3)\n* [5 - MODEL BUILDING WITHOUT SMOTE](#5)\n    * [5.1 Data Pre - Processing](#5.1)          \n        * [5.1.1 Scalling](#5.1.1)\n        * [5.1.2 Train - Test Split](#5.1.2)\n    * [5.2 Logistic Regression without SMOTE](#5.2)\n        * [5.2.1 Model Building](#5.2.1)\n        * [5.2.2 Evaluating Model Performance](#5.2.2)\n        * [5.2.3 Logistic Regression Cross Validation](#5.2.3)\n        * [5.2.4 Logistic Regression GridSearchCV](#5.2.4)\n        * [5.2.5 Logistic Regression ROC (Receiver Operating Curve) & AUC (Area Under Curve)](#5.2.5)            \n        * [5.2.6 Prediction](#5.2.6)        \n    * [5.3 Random Forest Classifier without SMOTE](#5.3)\n        * [5.3.1 Model Building](#5.3.1)\n        * [5.3.2 Evaluating Model Performance](#5.3.2)\n        * [5.3.3 Feature Importance for Random Forest Model](#5.3.3) \n        * [5.3.4 Random Forest Classifier Cross Validation](#5.3.4)\n        * [5.3.5 Random Forest Classifier GridSearchCV](#5.3.5)\n        * [5.3.6 Random Forest Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#5.3.6) \n        * [5.3.7 Prediction](#5.3.7) \n* [6 - MODEL BUILDING WITH SMOTE](#6)\n    * [6.1 Data Pre - Processing](#6.1)          \n        * [6.1.1 Scalling](#6.1.1)\n        * [6.1.2 Train - Test Split](#6.1.2)\n    * [6.2 Logistic Regression with SMOTE](#6.2)\n        * [6.2.1 Model Building](#6.2.1)\n        * [6.2.2 Evaluating Model Performance](#6.2.2)\n        * [6.2.3 Logistic Regression Cross Validation](#6.2.3)\n        * [6.2.4 Logistic Regression GridSearchCV](#6.2.4)\n        * [6.2.5 Logistic Regression ROC (Receiver Operating Curve) & AUC (Area Under Curve)](#6.2.5)            \n        * [6.2.6 Prediction](#6.2.6)        \n    * [6.3 Random Forest Classifier with SMOTE](#6.3)\n        * [6.3.1 Model Building](#6.3.1)\n        * [6.3.2 Evaluating Model Performance](#6.3.2)\n        * [6.3.3 Random Forest Classifier Cross Validation](#6.3.3)\n        * [6.3.4 Random Forest Classifier GridSearchCV](#6.3.4)\n        * [6.3.5 Random Forest Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#6.3.5)     \n        * [6.3.6 Prediction](#6.3.6) \n* [7 - NEURAL NETWORK](#7)\n    * [7.1 Data Pre - Processing](#7.1)          \n        * [7.1.1 Scalling](#7.1.1)\n        * [7.1.2 Train - Test Split](#7.1.2)\n    * [7.2 Import Libraries](#7.2)\n    * [7.3 Define Model](#7.3)\n    * [7.4 Compile Model](#7.4)\n    * [7.5 Fit Model](#7.5)\n    * [7.6 Evaluating Model Performance](#7.6)\n    * [7.7 Neural Network GridSearchCV](#7.7)\n    * [7.8 Neural Network ROC (Receiver Operating Curve) & AUC (Area Under Curve)](#7.8)  \n    * [7.9 Prediction](#7.9)  \n* [8 - THE COMPARISON OF MODELS](#8)   \n* [9 - FINAL MODEL](#9)\n* [10 - MODEL DEPLOYMENT](#10)\n    * [10.1 Save and Export the Model as .pkl](#10.1)\n    * [10.2 Example of An Application Code for Deployment](#9.2)  \n* [11 - CONCLUSION](#11)\n","7ad15f69":"Let's first load the required creditcard dataset using pandas's \"read_csv\" function.","76973f7e":"<a id=\"1.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.2 About The Features<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, it is not provided the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n**Feature Information:**\n\n**Time**: This feature is contains the seconds elapsed between each transaction and the first transaction in the dataset. \n\n**Amount**:  This feature is the transaction Amount, can be used for example-dependant cost-senstive learning. \n\n**Class**: This feature is the target variable and it takes value 1 in case of fraud and 0 otherwise.","0d9cb0a1":"<a id=\"6.2.3\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.2.3 Logistic Regression Cross Validation<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","5f57a7cb":"***Based on the examinations made above,***\n\n- There is no multicollinearity problem among the features.\n- We have weak level correlation between the features and the target column.\n- Also there is weak level correlation between the columns.","eeb534fb":"<a id=\"7.1.1\"><\/a>\n#### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7.1.1 Scalling<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","fd313338":"<a id=\"7\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7 - NEURAL NETWORK<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\nIn the final step, we will make classification with Neural Network which is a Deep Learning algorithm. \n\nNeural networks are a series of algorithms that mimic the operations of a human brain to recognize relationships between vast amounts of data. They are used in a variety of applications in financial services, from forecasting and marketing research to fraud detection and risk assessment.\n\nA neural network contains layers of interconnected nodes. Each node is a perceptron and is similar to a multiple linear regression. The perceptron feeds the signal produced by a multiple linear regression into an activation function that may be nonlinear.\n\nIn a multi-layered perceptron (MLP), perceptrons are arranged in interconnected layers. The input layer collects input patterns. The output layer has classifications or output signals to which input patterns may map. \n\nHidden layers fine-tune the input weightings until the neural network\u2019s margin of error is minimal. It is hypothesized that hidden layers extrapolate salient features in the input data that have predictive power regarding the outputs.\n\nWe will discover **[how to create](https:\/\/towardsdatascience.com\/building-our-first-neural-network-in-keras-bdc8abbc17f5)** your deep learning neural network model in Python using **[Keras](https:\/\/keras.io\/about\/)**. Keras is a powerful and easy-to-use free open source Python library for developing and evaluating deep learning models.\n\n- The steps we are going to cover for this algorithm are as follows:\n\n   *i. Import Libraries*\n   \n   *ii. Define Model*\n    \n   *iii. Compile Model*\n   \n   *iv. Fit Model*\n   \n   *v. Prediction and Model Evaluating*\n   \n   *vi. Plot Precision and Recall Curve*","2eb532c2":"<a id=\"5.2.3\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.2.3 Logistic Regression Cross Validation<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","30f1cf63":"<a id=\"6.3.6\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.3.6 Prediction<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","0a9419d7":"'class' Column-Target Column","6959ffb7":"<a id=\"11\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">11 - CONCLUSION<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","3b6530de":"<a id=\"3.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">3.1 Loading & Reading the Data<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**How to read and assign the dataset as df: You can [Visit Here](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_csv.html) (You can define it as what you want instead of df)**","66d919bc":"![image.png](attachment:ca064de1-97de-401a-b50e-13cb74d83c2e.png)","e842fe63":"<a id=\"10\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">10 - MODEL DEPLOYMENT<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \nYou cooked the food in the kitchen and moved on to the serving stage. The question is how do you showcase your work to others? Model Deployment helps you showcase your work to the world and make better decisions with it. But, deploying a model can get a little tricky at times. Before deploying the model, many things such as data storage, preprocessing, model building and monitoring need to be studied.\n\nDeployment of machine learning models, means making your models available to your other business systems. By deploying models, other systems can send data to them and get their predictions, which are in turn populated back into the company systems. Through machine learning model deployment, can begin to take full advantage of the model you built.\n\nData science is concerned with how to build machine learning models, which algorithm is more predictive, how to design features, and what variables to use to make the models more accurate. However, how these models are actually used is often neglected. And yet this is the most important step in the machine learning pipline. Only when a model is fully integrated with the business systems, real values \u200b\u200bcan be extract from its predictions.","c733a3a9":"I obtained the best parameters with GridSearch above for Logistic Regression and Random Forest Classifier. I want to get a final score by running these models with best parameters and other models with default parameters. ","112931d4":"<a id=\"5\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">5 - MODEL BUILDING WITHOUT SMOTE<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n---------\nThis dataset is severely unbalanced (most of the transactions are non-fraud). \n    \nWe will make class prediction with three different algorithms. \n    \nIt is important that we can evaluate the effectiveness of SMOTE. For this reason, implement the Logistic Regression and Random Forest Classifier algorithms in two different ways, with SMOTE applied and without.","5af7ff03":"![image.png](attachment:cf1df657-cb8d-4236-8c06-87237dc82d3f.png)","51f4226e":"<a id=\"7.1\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7.1 Data Pre - Processing<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","786ce905":"# <h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">FRAUD DETECTION<\/h1>","77c551b3":" Features from V1 to V28 are the principal components obtained with PCA.","18916444":"<a id=\"7.5\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7.5 Fit Model<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","23b7fdeb":"<a id=\"7.8\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7.8 Neural Network ROC (Receiver Operating Curve) & AUC (Area Under Curve)<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","5576bc0b":"**All 'v' Columns**","6fcd3cae":"<a id=\"5.2.4\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.2.4 Logistic Regression GridSearchCV<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","f0d6ba62":"<a id=\"6.3.3\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.3.3 Random Forest Classifier Cross Validation<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","a97f6000":"<a id=\"5.2\"><\/a>\n### <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">5.2 Logistic Regression without SMOTE<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","901dc3db":"Welcome to \"***Fraud Detection ***\"study.\n\nIn this study without knowing what the column names are, we will only be interested in their values. \n\nWe will implement ***Logistic Regression, Random Forest, Neural Network*** algorithms and ***SMOTE*** technique. Also visualize performances of the models using ***Seaborn, Matplotlib, Plotly*** and ***Yellowbrick*** in a variety of ways.","968b1ac8":"<a id=\"2\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">2 - LIBRARIES NEEDED IN THE STUDY<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","34bce264":"- 'outlier_zscore' function detects the best z-score for outlier detection in the specified column.\n- 'outlier_inspect' function plots histogram, boxplot and z-score\/outlier graphs for the specified column.","a2583140":"The datasets contains transactions made by credit cards in September 2013 by european cardholders. A study is requested from us to predict which transaction is fraudulent or not by using this data.\n\nFirst of all, to observe the structure of the data, outliers, missing values and features that affect the target variable, we used exploratory data analysis and data visualization techniques.\n\nThen, we performed data pre-processing operations to increase the recall score of algorithms.** \n\nWe built models to predict transaction is fraudulent or not. We trained our models with train set, tested the success of models with test set. \n\nIn this study, we made modelling with ***Logistic Regression***, ***Random Forest Classifier*** and ***Neural Network***.\n\nWe used scikit-learn ***Confusion Metrics*** module for accuracy calculation and the ***Yellowbrick*** module for model selection and visualization.\n\nImplementing SMOTE Technique on our imbalanced dataset helped us to balanced the data.\n\nAfter comparison between models, we found that RandomForestClassifier (Befor and After) SMOTE gave us the best result.\n\nI chose the 10 most effective feature according to the correlation, and feature importance results. With these 10 feature and target feature, I create a new dataframe. With this dataframe, I made modelling with Logistic Regression and Random Forest Classifier with SMOTE.\n\nI save  Logistic Regression and Random Forest Classifier as a final model. Then with these final models, I deployed my study via streamlit.\n\nWe have come to the end of the study. I hope it was useful. I would be very happy if you send your constructive and educational comments about the kernel.\n\nPlease don't forget to upvote if you liked!\n\nYou can find an instruction about how to deploy a model at last part of the kernel given below.\nhttps:\/\/www.kaggle.com\/azizozmen\/employee-churn-attrition-predict-eda-multi-algosHow ","10d2f48d":"<a id=\"5.2.1\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.2.1 Model Building with Logistic Regression<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","1988b3f9":"<a id=\"5.3.7\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.3.7 Prediction<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","ee990914":"<a id=\"6.3.1\"><\/a>\n#### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.3.1 Model Building with Random Forest Classifier<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","6905ef8f":"<a id=\"5.3.2\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.3.2 Evaluating Model Performance<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","092666fa":"<a id=\"6.3.2\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.3.2 Evaluating Model Performance<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","3c57be89":"Let's create a new df before outlier operation.","832c1985":"<a id=\"5.1.1\"><\/a>\n#### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.1.1 Scaling<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","1b51532d":"<a id=\"5.1.2\"><\/a>\n#### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.1.2 Train - Test Split<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","8e624530":"<a id=\"6.2.5\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.2.5 Logistic Regression ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","c15c076e":"**Evaluation**\n\n1. There is no missing value in this data set.\n2. When each feature is examined, although it is seen that there are outliers in the boxplots, it is evaluated that this dataset does not actually have an outlier.\n3. However, since it was considered that the data with a z-score value above 4.73 (IQR_coef = 3) according to the class type would adversely affect the model, so I decided to assign the Nan-Value to these data first and then drop them."}}