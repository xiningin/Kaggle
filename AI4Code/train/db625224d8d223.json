{"cell_type":{"61dbee8a":"code","29c44338":"code","53877e71":"code","07a15c7c":"code","bc3949ac":"code","24cbfce8":"code","19f2a9a3":"code","834fb3e9":"code","adda5fae":"code","3d7e44ba":"code","823d36d9":"code","8d54cc8c":"code","4dd3ad78":"code","d037ca62":"code","968fe6ad":"code","0958f252":"code","61a11032":"code","a1267f25":"code","ef77f9a2":"markdown","efd7eca2":"markdown","7b6c26d5":"markdown","9cfbd08d":"markdown","18f3dce9":"markdown","dab998dd":"markdown","b79937f1":"markdown"},"source":{"61dbee8a":"##################################################\n## Credit card fraud Deduction\n##################################################\n#!\/usr\/bin\/python\n__author__ = 'Midhunkumar S'\n__maintainer__ = 'Kaggle'\n##################################################\n\n\"\"\"Note: This kernal only for explaining scaling and Imbalanced dataset \nand i am not going to do any hyperparameter tuning or create model\"\"\"","29c44338":"# Importing Libraries\nimport numpy as np\nimport pandas as pd\npd.options.display.max_columns \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolors = ['#AADDCC','#00CCCC','#CC6677','#DDFFAA','#77AACC']\nsns.set_theme(palette=colors, style='darkgrid', \n        rc={'axes.facecolor':'whitesmoke', 'figure.facecolor':'whitesmoke'})\nimport warnings\nwarnings.filterwarnings(\"ignore\")","53877e71":"# Importing data\ndf = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","07a15c7c":"# Checking basic information about columns\ndf.info()","bc3949ac":"df.shape","24cbfce8":"# Checking null Values\ndf.isnull().sum()","19f2a9a3":"df.head(10)","834fb3e9":"df.describe()","adda5fae":"print(f'Total Non-Fraud Transaction{round(df[df.Class == 0].count().max()\/len(df)*100,4)} %')\nprint(f'Total Fraud Transaction{round(df[df.Class == 1].count().max()\/len(df)*100,4)} %')\n","3d7e44ba":"sns.histplot(df.Class, color= colors[1])","823d36d9":"# Ploting time and Amount to check if its normally ditributed\namount_val = df['Amount'].values\ntime_val = df['Time'].values\nfig , ax = plt.subplots(1,2,figsize= (18,4))\nsns.distplot(amount_val,ax= ax[0],color= colors[1])\nax[0].set_title(\"Amount Distribution \")\nsns.distplot(time_val,ax= ax[1],color= colors[-1])\nax[1].set_title(\"Time Distribution \")\nplt.show()","8d54cc8c":"# Scaling\n\nfrom sklearn.preprocessing import RobustScaler\n\nScaler = RobustScaler()\n\ndf['Amount_Scaled'] = Scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['Time_Scaled'] = Scaler.fit_transform(df['Time'].values.reshape(-1,1))","4dd3ad78":"Amount = df['Amount_Scaled']\nTime = df['Time_Scaled']\n\ndf.drop(['Time','Amount'], axis= 1, inplace= True)\ndf.drop(['Amount_Scaled','Time_Scaled'], axis= 1, inplace= True)\n\ndf.insert(0,'Amount',Amount)\ndf.insert(1,'Time',Time)\n","d037ca62":"df.head()","968fe6ad":"sns.countplot(data=df, x=\"Class\", hue=\"Class\")\nplt.title(\"Orginal dataset\")\nplt.show()","0958f252":"# Random OverSampling, SMOTE, ADASYN\n# implearn  - Imbalanced-Learn python library \n\nX = df.copy()\ny = X.pop(\"Class\")\n\nfrom imblearn.over_sampling import RandomOverSampler,SMOTE,ADASYN\n\noversample = RandomOverSampler(sampling_strategy= 0.4) # We can also use \"minority\" to give 1:1 ratio\nsmote = SMOTE(sampling_strategy=0.8, k_neighbors= 5)# Note: we need to mention K as its uses KNN algorthiom \nadasyn = ADASYN(sampling_strategy= \"minority\" ,n_neighbors= 5) ","61a11032":"# Fiting the RandomoverSample ,SMOTE , ADASYN\nX_OVER, y_OVER = oversample.fit_resample(X,y)\nX_SMOTE, y_SMOTE = smote.fit_resample(X,y)\nX_ADASYN, y_ADASYN = adasyn.fit_resample(X,y)","a1267f25":"fig , ax = plt.subplots(2,2,figsize= (13,13))\nsns.countplot(y,ax= ax[0,0])\nax[0,0].set_title(\"Original Dataset \")\nsns.countplot(y_OVER,ax= ax[0,1])\nax[0,1].set_title(\"Random OverSample Dataset\")\nsns.countplot(y_SMOTE,ax= ax[1,0])\nax[1,0].set_title(\"SMOTE OverSmaple Dataset\")\nsns.countplot(y_ADASYN,ax= ax[1,1])\nax[1,1].set_title(\"ADASYN OverSample Dataset\")\nplt.show()","ef77f9a2":"### Working on Imbalanced data set","efd7eca2":"#### 1. Random Over Sampling:\nRandom oversampling is the simplest oversampling technique to balance the imbalanced nature of the dataset. It balances the data by replicating the minority class samples. This does not cause any loss of information, but the dataset is prone to overfitting as the same information is copied.\n\n#### 2. SMOTE\nTo Avoid Overfitting from Random OverSampling we can use SMOTE,  first it \nselects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b.\n\n#### 3.ADASYN\nThe major difference between SMOTE and ADASYN is the difference in the generation of synthetic sample points for minority data points. In ADASYN, we consider a density distribution r\u2093 which thereby decides the number of synthetic samples to be generated for a particular point, whereas in SMOTE, there is a uniform weight for all minority points.","7b6c26d5":"#### OverSampling:\n> `I am going to use OverSampling techniques today as sometimes if we remove observations from the majority class we might lose some important information`\n\nBelow are some Most used OverSampling Techniques.\n1. Random Over Sampling\n2. Smote\n3. ADASYN","9cfbd08d":"#### what is imbalanced dataset?\nImbalanced data sets are a special case for classification problems where the class distribution is not uniform among the classes. one Class being majority in the dataset. which will cause biased prediction.\n\nTo overcome this problem, we use two sampling techniques.\n\n1. UnderSampling \n2. OverSampling\n\n#### What is UnderSampling?\nUndersampling techniques remove examples from the training dataset that belong to the majority class in order to better balance the class distribution, By reducing the ratio between 1:100 to 1:2 or even 1:1.\n\n#### What is OverSampling?\nOverSampling direct opposite to UnderSampling, which adds new examples or copies and adds exiting examples to the dataset to increase the minority class ratio. ex- 100:1 to 100:80 or 100:100 ","18f3dce9":"`Note:`\n> 1. From above results we can see no null Values.\n> 2. As thses features derived from PCA we assume its should already scaled except Amount and Time.\n> 3. And Most importent thing is dataset higly Imbalanced \ud83e\udd7a  As Non-Fraud (99.83%) of the time, while Fraud transactions occurs (0.17%)  \n> 4. To avoid overfitting we need to get Amount,time scaled as Models use gradient descent as an optimization technique require data to be scaled.","dab998dd":"# `EDA`","b79937f1":"## Scaling\nThere are several scaling methods available. Below most known ones.\n### 1. StandardScaler\nStandardize features by removing the mean and scaling to unit variance\nThe standard score of a sample x is calculated as:\n\n> `z = (x - u) \/ s`\n\n### 2. MinMaxScaler\nThis estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\nThe transformation is given by:\n\n> `X_std = (X - X.min(axis=0)) \/ (X.max(axis=0) - X.min(axis=0))`\n\n>` X_scaled = X_std * (max - min) + min`\n\nwhere min, max = feature_range.\n\n### 3. MaxAbsScaler\nScale each feature by its maximum absolute value.\nThis estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift\/center the data, and thus does not destroy any sparsity.\n### 4. RobustScaler\nThis Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n\n> **`I am going to choose Robust Scaler as we have the advantage that Robust scaler will take care of outliers`**"}}