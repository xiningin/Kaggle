{"cell_type":{"27950a45":"code","1c5b2126":"code","199b4742":"code","0dcec30c":"code","debe2ad3":"code","0f27f771":"code","c48ef555":"code","99c32377":"code","eb0a26aa":"code","4eb3fb66":"code","88fff6bb":"code","dce6706d":"code","02704bb9":"code","304f7f5f":"code","6cc1c483":"code","2e590df3":"code","f094f5fa":"code","7409c77f":"code","bd2fb86b":"code","040e8fd1":"code","6330fef9":"code","ec978aab":"code","3823e65d":"code","19a9def0":"code","f34d41d6":"code","011e0042":"code","ee3048fd":"code","af8f121a":"code","aac32b63":"code","8a331e5f":"code","b204a56d":"code","db7628ac":"code","c4c725f0":"code","a43e1045":"code","5bb609a8":"code","712fc0f2":"code","118dee64":"code","7d93c693":"code","3fcdf9f9":"code","97aac2a0":"markdown","8ef896af":"markdown","335749d4":"markdown","77c5bbeb":"markdown","9597c493":"markdown","d69cb88d":"markdown","4fe6ba7c":"markdown","c5465c0d":"markdown"},"source":{"27950a45":"#the basics\nimport os, re, math, string, pandas as pd, numpy as np, seaborn as sns\n\n#graphing\nimport matplotlib.pyplot as plt\n\n#deep learning\nimport tensorflow as tf\n\n#nlp\nfrom wordcloud import STOPWORDS\n\n#scaling\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler","1c5b2126":"#choose batch size\nBATCH_SIZE = 16\n\n#how many epochs?\nEPOCHS = 2\n\n#clean Tweets?\nCLEAN_TWEETS = False\n\n#use meta data?\nUSE_META = True\n\n#add dense layer?\nADD_DENSE = False\nDENSE_DIM = 64\n\n#add dropout?\nADD_DROPOUT = True\nDROPOUT = .2\n\n#train BERT base model? \nTRAIN_BASE = True","199b4742":"#get data\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\n#peek at train\ntrain.head()","0dcec30c":"#save ID\ntest_id = test['id']\n\n#drop from train and test\ncolumns = {'id', 'location'}\ntrain = train.drop(columns = columns)\ntest = test.drop(columns = columns)\n\n#fill missing with unknown\ntrain['keyword'] = train['keyword'].fillna('unknown')\ntest['keyword'] = test['keyword'].fillna('unknown')\n\n#add keyword to tweets\ntrain['text'] = train['text'] + ' ' + train['keyword']\ntest['text'] = test['text'] + ' ' + test['keyword']\n\n#drop fkeyword rom train and test\ncolumns = {'keyword'}\ntrain = train.drop(columns = columns)\ntest = test.drop(columns = columns)","debe2ad3":"#a handy set indeed\ncontractions = { \n\"ain't\": \"am not \/ are not \/ is not \/ has not \/ have not\",\n\"aren't\": \"are not \/ am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had \/ he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall \/ he will\",\n\"he'll've\": \"he shall have \/ he will have\",\n\"he's\": \"he has \/ he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has \/ how is \/ how does\",\n\"I'd\": \"I had \/ I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall \/ I will\",\n\"I'll've\": \"I shall have \/ I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had \/ it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall \/ it will\",\n\"it'll've\": \"it shall have \/ it will have\",\n\"it's\": \"it has \/ it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had \/ she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall \/ she will\",\n\"she'll've\": \"she shall have \/ she will have\",\n\"she's\": \"she has \/ she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as \/ so is\",\n\"that'd\": \"that would \/ that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has \/ that is\",\n\"there'd\": \"there had \/ there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has \/ there is\",\n\"they'd\": \"they had \/ they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall \/ they will\",\n\"they'll've\": \"they shall have \/ they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had \/ we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall \/ what will\",\n\"what'll've\": \"what shall have \/ what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has \/ what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has \/ when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has \/ where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall \/ who will\",\n\"who'll've\": \"who shall have \/ who will have\",\n\"who's\": \"who has \/ who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has \/ why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had \/ you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall \/ you will\",\n\"you'll've\": \"you shall have \/ you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\ncontractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n\n#define function to expand contractions and showcase\ndef expand_contractions(s, contractions = contractions):\n    def replace(match):\n        return contractions[match.group(0)]\n    return contractions_re.sub(replace, s)\n\nexpand_contractions(\"can't stop won't stop\")","0f27f771":"#this stips tweet related characters\ndef strip_all_entities(x):\n    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \",x).split())\n\n#check functionality\nstrip_all_entities('@shawn Titanic #tragedy could have been prevented Economic Times: Telegraph.co.uk Titanic tragedy could have been prevented... http:\/\/bet.ly\/tuN2wx')","c48ef555":"#this removes stopwords\ndef remove_stopwords(x):\n    return ' '.join([i for i in x.split() if i not in STOPWORDS])","99c32377":"#apply to Tweets\nif CLEAN_TWEETS:\n    for df in [train, test]:\n        df['text'] = df['text'].apply(expand_contractions)\n        df['text'] = df['text'].apply(strip_all_entities)\n        df['text'] = df['text'].apply(remove_stopwords)","eb0a26aa":"def average_word_length(x):\n    x = x.split()\n    return np.mean([len(i) for i in x])","4eb3fb66":"for df in [train, test]:\n    df['word count'] = df['text'].apply(lambda x: len(x.split()))\n    df['character count'] = df['text'].apply(lambda x: len(x))\n    df['average word length'] = df['text'].apply(average_word_length)\n    df['unique word count'] = df['text'].apply(lambda x: len(set(x.split())))\n    df['stopword count'] = df['text'].apply(lambda x: len([i for i in x.lower().split() if i in STOPWORDS]))\n    #df['punctuation count'] = df['text'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]))\n    df['stopword ratio'] = df['stopword count'] \/ df['word count']\n    df['url count'] = df['text'].apply(lambda x: len([i for i in x.lower().split() if 'http' in i or 'https' in i]))","88fff6bb":"meta_train = StandardScaler().fit_transform(train.iloc[:, 2:])\nmeta_test = StandardScaler().fit_transform(test.iloc[:, 1:])\n\n#meta_train = MinMaxScaler().fit_transform(train.iloc[:, 2:])\n#meta_test = MinMaxScaler().fit_transform(test.iloc[:, 1:])","dce6706d":"#BERT\nfrom transformers import BertTokenizer\nTOKENIZER = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nenc = TOKENIZER.encode(\"Encode me!\")\ndec = TOKENIZER.decode(enc)\nprint(\"Encode: \" + str(enc))\nprint(\"Decode: \" + str(dec))","02704bb9":"#RoBERTa\nfrom transformers import RobertaTokenizer\nTOKENIZER = RobertaTokenizer.from_pretrained(\"roberta-base\")\nenc = TOKENIZER.encode(\"Encode me!\")\ndec = TOKENIZER.decode(enc)\nprint(\"Encode: \" + str(enc))\nprint(\"Decode: \" + str(dec))","304f7f5f":"#AlBERTa\nfrom transformers import AlbertTokenizer\nTOKENIZER = AlbertTokenizer.from_pretrained(\"albert-base-v1\")\nenc = TOKENIZER.encode(\"Encode me!\")\ndec = TOKENIZER.decode(enc)\nprint(\"Encode: \" + str(enc))\nprint(\"Decode: \" + str(dec))","6cc1c483":"#BART\nfrom transformers import BartTokenizer\nTOKENIZER = BartTokenizer.from_pretrained('facebook\/bart-large')\nenc = TOKENIZER.encode(\"Encode me!\")\ndec = TOKENIZER.decode(enc)\nprint(\"Encode: \" + str(enc))\nprint(\"Decode: \" + str(dec))","2e590df3":"#ELECTRA and BERT have identical tokenizers\nfrom transformers import BertTokenizer\nTOKENIZER = BertTokenizer.from_pretrained('google\/electra-base-generator')\nenc = TOKENIZER.encode(\"Encode me!\")\ndec = TOKENIZER.decode(enc)\nprint(\"Encode: \" + str(enc))\nprint(\"Decode: \" + str(dec))","f094f5fa":"import tensorflow as tf\nfrom transformers import TFBertModel, BertModel","7409c77f":"#get BERT layer\nbert_base = TFBertModel.from_pretrained('bert-base-uncased')\n#bert_base = BertModel.from_pretrained('bert-base-uncased')          #to use with PyTorch\n\n#select BERT tokenizer\nTOKENIZER = BertTokenizer.from_pretrained(\"bert-base-uncased\")","bd2fb86b":"def bert_encode(data,maximum_len) :\n    input_ids = []\n    attention_masks = []\n  \n\n    for i in range(len(data.text)):\n        encoded = TOKENIZER.encode_plus(data.text[i],\n                                        add_special_tokens=True,\n                                        max_length=maximum_len,\n                                        pad_to_max_length=True,\n                                        return_attention_mask=True)\n      \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n        \n    return np.array(input_ids),np.array(attention_masks)","040e8fd1":"def build_model(model_layer, learning_rate, use_meta = USE_META, add_dense = ADD_DENSE,\n               dense_dim = DENSE_DIM, add_dropout = ADD_DROPOUT, dropout = DROPOUT):\n    \n    #define inputs\n    input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n    meta_input = tf.keras.Input(shape = (meta_train.shape[1], ))\n    \n    #insert BERT layer\n    transformer_layer = model_layer([input_ids,attention_masks])\n    \n    #choose only last hidden-state\n    output = transformer_layer[1]\n    \n    #add meta data\n    if use_meta:\n        output = tf.keras.layers.Concatenate()([output, meta_input])\n    \n    #add dense relu layer\n    if add_dense:\n        print(\"Training with additional dense layer...\")\n        output = tf.keras.layers.Dense(dense_dim,activation='relu')(output)\n    \n    #add dropout\n    if add_dropout:\n        print(\"Training with dropout...\")\n        output = tf.keras.layers.Dropout(dropout)(output)\n    \n    #add final node for binary classification\n    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n    \n    #assemble and compile\n    if use_meta:\n        print(\"Training with meta-data...\")\n        model = tf.keras.models.Model(inputs = [input_ids,attention_masks, meta_input],outputs = output)\n    \n    else:\n        print(\"Training without meta-data...\")\n        model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n\n    model.compile(tf.keras.optimizers.Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","6330fef9":"#define conveient training function to visualize learning curves\ndef plot_learning_curves(history): \n    fig, ax = plt.subplots(1, 2, figsize = (20, 10))\n\n    ax[0].plot(history.history['accuracy'], color = '#171820')\n    ax[0].plot(history.history['val_accuracy'], color = '#fdc029')\n\n    ax[1].plot(history.history['loss'], color='#171820')\n    ax[1].plot(history.history['val_loss'], color = '#fdc029')\n\n    ax[0].legend(['train', 'validation'], loc = 'upper left')\n    ax[1].legend(['train', 'validation'], loc = 'upper left')\n\n    fig.suptitle(\"Model Learning Curves\", fontsize=14)\n\n    ax[0].set_ylabel('Accuracy')\n    ax[0].set_xlabel('Epoch')\n    ax[1].set_ylabel('Loss')\n    ax[1].set_xlabel('Epoch')\n\n    return plt.show()","ec978aab":"if TRAIN_BASE:\n    #get our inputs\n    print('Encoding Tweets...')\n    train_input_ids,train_attention_masks = bert_encode(train,60)\n    test_input_ids,test_attention_masks = bert_encode(test,60)\n    print('Tweets encoded')\n    print('')\n\n    #debugging step\n    print('Train length:', len(train_input_ids))\n    print('Test length:', len(test_input_ids))","3823e65d":"#and build and view parameters\nBERT_base = build_model(bert_base, learning_rate = 1e-5)\nBERT_base.summary()","19a9def0":"checkpoint = tf.keras.callbacks.ModelCheckpoint('base_model.h5', monitor='val_loss', save_best_only = True, save_weights_only = True)","f34d41d6":"#train BERT\nif TRAIN_BASE:\n    if USE_META:\n        history = BERT_base.fit([train_input_ids,train_attention_masks, meta_train], train.target, validation_split = .2, epochs = EPOCHS, callbacks = [checkpoint], batch_size = BATCH_SIZE)\n    \n    else:\n        history = BERT_base.fit([train_input_ids,train_attention_masks], train.target, validation_split = .2, epochs = EPOCHS, callbacks = [checkpoint], batch_size = BATCH_SIZE)   ","011e0042":"if TRAIN_BASE:\n    plot_learning_curves(history)","ee3048fd":"#load model with best losses\nif TRAIN_BASE:\n    BERT_base.load_weights('base_model.h5') ","af8f121a":"#predict with BERT\nif TRAIN_BASE:\n    if USE_META:\n        preds_base = BERT_base.predict([test_input_ids,test_attention_masks,meta_test])\n\n    else:\n        preds_base = BERT_base.predict([test_input_ids,test_attention_masks])","aac32b63":"#save as dataframe\nif TRAIN_BASE:\n    submission_base = pd.DataFrame()\n    submission_base['id'] = test_id\n    submission_base['prob'] = preds_base\n    submission_base['target'] = np.round(submission_base['prob']).astype(int)\n    submission_base.head(10)","8a331e5f":"if TRAIN_BASE:\n    submission_base = submission_base[['id', 'target']]\n\n    #save to disk\n    submission_base.to_csv('submission_bert_base.csv', index = False)\n    print('Submission saved')","b204a56d":"#get BERT layer\nbert_large = TFBertModel.from_pretrained('bert-large-uncased')\n#bert_base = BertModel.from_pretrained('bert-large-uncased')          #to use with PyTorch\n\n#select BERT tokenizer\nTOKENIZER = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n\n#get our inputs\ntrain_input_ids,train_attention_masks = bert_encode(train,60)\ntest_input_ids,test_attention_masks = bert_encode(test,60)\n\n#debugging step\nprint('Train length:', len(train_input_ids))\nprint('Test length:', len(test_input_ids))\n\n#and build and view parameters\nBERT_large = build_model(bert_large, learning_rate = 1e-5)\nBERT_large.summary()","db7628ac":"checkpoint = tf.keras.callbacks.ModelCheckpoint('large_model.h5', monitor='val_loss', save_best_only = True, save_weights_only = True)","c4c725f0":"#train BERT\nif USE_META:\n    history = BERT_large.fit([train_input_ids,train_attention_masks, meta_train], train.target, validation_split = .2, epochs = EPOCHS, callbacks = [checkpoint], batch_size = BATCH_SIZE)\n    \nelse:\n    history = BERT_large.fit([train_input_ids,train_attention_masks], train.target, validation_split = .2, epochs = EPOCHS, callbacks = [checkpoint], batch_size = BATCH_SIZE)","a43e1045":"plot_learning_curves(history)","5bb609a8":"#load model with best losses\nBERT_large.load_weights('large_model.h5') ","712fc0f2":"#predict with BERT\nif USE_META:\n    preds_large = BERT_large.predict([test_input_ids,test_attention_masks,meta_test])\n\nelse:\n    preds_large = BERT_large.predict([test_input_ids,test_attention_masks])  ","118dee64":"#save as dataframe\nsubmission_large = pd.DataFrame()\nsubmission_large['id'] = test_id\nsubmission_large['prob'] = preds_large\nsubmission_large['target'] = np.round(submission_large['prob']).astype(int)\nsubmission_large.head(10)","7d93c693":"submission_large = submission_large[['id', 'target']]\n\n#save to disk\nsubmission_large.to_csv('submission_bert_large.csv', index = False)\nprint('Submission saved')","3fcdf9f9":"#create blended submission\nsubmission_final = pd.DataFrame()\nsubmission_final['id'] = test_id\nif TRAIN_BASE:\n    submission_final['base prob'] = preds_base\nsubmission_final['large prob'] = preds_large\n#submission_final['blended prob'] = .5 * preds_base + .5 * preds_large\nsubmission_final.head(10)","97aac2a0":"# I. Meta Features\n\n**We can also include meta-features to feed to our BERT model to see if it improves performance:**","8ef896af":"# Exploring NLP with HuggingFace Transformers\n\n**\ud83e\udd17 Transformers is [\"On a mission to solve NLP, one commit at a time\"](https:\/\/huggingface.co\/), so I figured I would make a notebook to explore their library to demonstrate how powerful it is. In this notebook, I use BERT to classify texts that are about natural disastors. I have another notebook where I use a GPT-2 with HuggingFace to generate text, which you can find [here](https:\/\/www.kaggle.com\/tuckerarrants\/text-generation-with-huggingface-gpt2)**","335749d4":"# 0. Preprocessing\n\n**We will add this section to experiment with text processing and how cleaning text affects our model's performance. However, these mdoels were originally trained without preprocessing to learn the context of as many kinds of sentences as possible, so carrying out standard NLP text cleaning could actually hurt our model's performance**","77c5bbeb":"**Tweak here for quick notebook changes:**","9597c493":"# II. Different Tokenizers\n\n**We can quickly explore how the different transformer models in HuggingFace tokenize text:**","d69cb88d":"## 2. Large\n\n**Just a beefier version of BERT; you can read more about it [here](https:\/\/huggingface.co\/bert-large-uncased)**","4fe6ba7c":"# IV. Final Submission\n\n**Here we can experiment with blending the predictions of different transformer models with different weights if we want to, but seeing as the BERT base and large are similarly structured, blending their predictions probably won't benefit us too much:**","c5465c0d":"# III. BERT\n\n## 1. Base\n\n**Here we will use the TensorFlow BERT base model, which you can read more about [here](https:\/\/huggingface.co\/bert-base-uncased). You can also implement these transformers with PyTorch just as easily:**"}}