{"cell_type":{"6e1f253c":"code","ef9c2a2b":"code","072fb4b4":"code","963f9405":"code","0284ac5f":"code","f49f5959":"code","fff2e026":"code","9218e0cc":"code","2d301690":"code","5eac650b":"code","eebdd471":"code","bb60493d":"code","f6504a09":"code","ea54a8cc":"code","f5094ed4":"code","b0cac6b1":"code","7db6817e":"code","02e986b2":"code","28797489":"code","753e4ccd":"code","fb3bff90":"code","8da8b1d3":"code","7f196082":"code","628d6b96":"code","7d235f89":"code","431b7677":"code","2c9f9dd2":"code","e01cd8d3":"code","55104f4f":"code","5226187f":"code","934a0fc9":"code","da4b8ce8":"markdown","edc3042b":"markdown","a64b41b2":"markdown","ff3d781b":"markdown","65e7d81b":"markdown","dfaf19d1":"markdown","332ca869":"markdown","ae2a91ab":"markdown","d4155167":"markdown"},"source":{"6e1f253c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ef9c2a2b":"#### Import Dependencies\n%matplotlib inline\n#### Start Python Imports\nimport math, time, random, datetime\n#### Data Manipulation\nimport numpy as np\nimport pandas as pd\n#### Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n#### Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n#### Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n##### Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')\n","072fb4b4":"import pandas as pd \ntrain = pd.read_csv(\"..\/input\/allstate-claims-severity\/train.csv\")\ntest = pd.read_csv(\"..\/input\/allstate-claims-severity\/test.csv\")\n\n#Print all rows and columns. Dont hide any\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","963f9405":"#Understand the data\nprint(train.shape)\nprint(test.shape)","0284ac5f":"# Save the test id for later submission \n# and drop the id from train and test. As id is unique for all rows and don't carry any information \ntest_id = test[\"id\"]\ntest.drop(\"id\", axis = 1, inplace = True)\ntrain.drop(\"id\", axis = 1, inplace = True)\nprint(train.loss.describe())\nprint(\"\")\nprint(\"The loss\/target skewness : \",train.loss.skew())","f49f5959":"# since the skewness of loss is about 3, which is higher. Hence it should be corrected. \n#I have choosen to go with log1p\n\n#Before skew correction \nsns.violinplot(data=train,y=\"loss\")  \nplt.show()\n\ntrain[\"loss\"] = np.log1p(train[\"loss\"])\n\n#visualize the transformed column\nsns.violinplot(data=train,y=\"loss\")  \nplt.show()","fff2e026":"train.loss.describe()","9218e0cc":"data = train.copy()\nvalid = test.copy()\ndel train\ndel test","2d301690":"s = (data.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(len(object_cols))\nprint(object_cols)","5eac650b":"#combined the train and test so that get dummies can be applied on all at the same time. \n## But at the same time elimination due to correlation should be considered only of from train data. \nmergedata = data.append(valid)","eebdd471":"print(mergedata.shape)\nprint(valid.shape)\nprint(len(data))\nprint(len(valid))","bb60493d":"#missingno.matrix(valid, figsize = (30,12))\n# no missings..","f6504a09":"#remove the one of the pair columns which have higher abs correlation \ndata_corr = data.corr()\nlistOfFeatures = [i for i in data_corr]\nsetOfDroppedFeatures = set() \nfor i in range(len(listOfFeatures)) :\n    for j in range(i+1,len(listOfFeatures)): #Avoid repetitions \n        feature1=listOfFeatures[i]\n        feature2=listOfFeatures[j]\n        if abs(data_corr[feature1][feature2]) > 0.8: #If the correlation between the features is > 0.8\n            setOfDroppedFeatures.add(feature1) ","ea54a8cc":"mergedata = mergedata.drop(setOfDroppedFeatures, axis=1)","f5094ed4":"s = (data.dtypes != 'object')\nnon_object_cols = list(s[s].index)\nnon_object_cols = [ col for col in non_object_cols if col not in setOfDroppedFeatures ]\nprint(len(non_object_cols))","b0cac6b1":"nonCorrelatedWithOutput = [column for column in non_object_cols if abs(data[column].corr(data[\"loss\"])) < 0.025]\nprint(len(nonCorrelatedWithOutput))\nnonCorrelatedWithOutput","7db6817e":"mergedata = mergedata.drop(nonCorrelatedWithOutput, axis=1)","02e986b2":"mergedata.shape","28797489":"# #uncomment if want to see the visualizatio of distribution (takes time)\n\n# #names of all the columns\n# cols = data.columns\n\n# #Plot count plot for all attributes in a 29x4 grid\n# n_cols = 4\n# n_rows = 29\n# for i in range(n_rows):\n#     fg,ax = plt.subplots(nrows=1,ncols=n_cols,sharey=True,figsize=(12, 8))\n#     for j in range(n_cols):\n#         sns.countplot(x=cols[i*n_cols+j], data=data, ax=ax[j])\n","753e4ccd":"# cols taken from the visualization plots.\n# to-do: find out generic method to do the same. \nunbalanced_cols = ['cat15','cat18','cat20','cat21','cat22','cat33','cat35','cat48','cat56','cat58',\n                   'cat59','cat60','cat62','cat63','cat64','cat67','cat68','cat69','cat70','cat77']","fb3bff90":"mergedata = mergedata.drop(unbalanced_cols, axis=1)\nprint(mergedata.shape)","8da8b1d3":"mergedata_ohe = pd.get_dummies(mergedata)\nmergedata_ohe.shape","7f196082":"# seperate the train and test from mergedata\ntrainLen = len(data)\nprint(\"train len : \", trainLen)\ntrain_ohe = mergedata_ohe.iloc[:trainLen]\ntest_ohe = mergedata_ohe.iloc[trainLen:]\n\ndel mergedata_ohe\ndel data\ndel valid\n\nprint(train_ohe.shape)\nprint(test_ohe.shape)","628d6b96":"print(test_ohe.loss.isnull().sum())\n# We should remove the extra added loss column to the test data\ntest_ohe = test_ohe.drop(\"loss\", axis = 1)","7d235f89":"#Build and fit the model\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import Lasso, Ridge\nimport xgboost as xgb # XGBoost implementation\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0007, random_state=1))\nridge = make_pipeline(RobustScaler(), Ridge(alpha =20, random_state=42))","431b7677":"X_train = train_ohe.drop(\"loss\", axis = 1)\nY_train = train_ohe[\"loss\"]\nprint(X_train.shape)\nprint(Y_train.shape)\ndel train_ohe","2c9f9dd2":"## lasso \n# lasso.fit(X_train, y_train)\n# test_y_log = lasso.predict(test_ohe)\n# test_y = np.exp(1)**test_y_log","e01cd8d3":"from sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, train_size=.90,random_state=2016)\n\n\nRANDOM_STATE = 2016\n#'seed': RANDOM_STATE,\nparams = {\n    'min_child_weight': 1,\n    'eta': 0.03,\n    'colsample_bytree': 0.5,\n    'max_depth': 12,\n    'subsample': 0.8,\n    'alpha': 1,\n    'seed': RANDOM_STATE,\n    'gamma': 1.5,\n    'silent': 1,\n    'verbose_eval': True,\n    'nthread':7,\n    'base_score':7.76\n}\n\n\n\nxgtrain = xgb.DMatrix(x_train, label=y_train)\nxgval = xgb.DMatrix(x_valid, label=y_valid)\n\nwatchlist = [ (xgtrain,'train'),(xgval,'eval')]\n\nmodel = xgb.train(params, xgtrain,300, watchlist,verbose_eval=True)\n\ndel xgtrain\ndel xgval\nxgtest = xgb.DMatrix(test_ohe)\ntest_y_log = model.predict(xgtest)\ntest_y = np.exp(1)**test_y_log","55104f4f":"del X_train\ndel y_train","5226187f":"submission = pd.DataFrame()\nsubmission[\"id\"] = test_id\nprint(test_y.shape)\nprint(test_id.shape)\n\nsubmission[\"loss\"] = test_y\nsubmission.to_csv(\"submission_svm_l.csv\", index=False)","934a0fc9":"# score is : 1126.77","da4b8ce8":"### Now one hot encoding!!","edc3042b":"#### there is no missing value issue with data ","a64b41b2":"### Non correlated cols are removed...","ff3d781b":"### before doing one hot encoding using get dummies, 1] we need to remove highly mutual correlated features(one of the pair), 2] also the features with less correlation with the target.                                                  3] Then we need to remove the data that isn't properly distrubuted. ","65e7d81b":"### *** let's try ML now","dfaf19d1":"![](http:\/\/)### By observing the distrubutions(very uneven), mey be we should drop some of the features. ","332ca869":"### Done with the encoding and EDA so far. Will have to check with different  alternatives : 1] with out dropping unbalanced data. 2] without the skew fixing  ","ae2a91ab":"##### Now the correlation with the target ","d4155167":"#### All the categorical features should be one hot encoded.... "}}