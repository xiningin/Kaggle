{"cell_type":{"07ace612":"code","5c993f7d":"code","8f51ecaf":"code","6bc07b3d":"code","11ab13cd":"code","ce9c730f":"code","46cc069e":"code","92e1bcaf":"code","b8154b9e":"code","0d45700a":"code","460a5e06":"code","e084ec65":"code","8934ba19":"code","2e6b7223":"code","ce3f2b87":"code","123d7ee1":"code","ebe3f3b2":"code","9461ac0c":"code","ca0f9214":"code","fda46664":"code","2c47c84e":"code","8708ad79":"code","ae345060":"code","88db3c64":"code","85fe0c0e":"code","5fac03ff":"code","9f9320ab":"code","778c18bc":"code","daedb25a":"code","100515bb":"code","a66142c7":"code","b2bd4214":"code","d30e58ec":"code","148cadd4":"code","34e75065":"code","380bdcd3":"code","6eaea1a3":"code","49cabfbb":"code","99eeec6b":"code","7c21209e":"code","ae38eef5":"code","cbc66c95":"code","e377a366":"code","45d7d258":"code","241a9f9b":"markdown","bee27d66":"markdown","d3cfeeb8":"markdown","81b885c4":"markdown","8494f9bb":"markdown","13c0afd7":"markdown","d6840ccf":"markdown","4d17713f":"markdown","be153bd8":"markdown","476ff415":"markdown","4f7e2d39":"markdown","5c8f37a5":"markdown","f2a238b8":"markdown","1f6d8c96":"markdown"},"source":{"07ace612":"# Installing git-lfs and setting up \n# git repo to push model and tokenizer to the hub\n\n!apt-get install git-lfs\n!git init\n!git lfs install","5c993f7d":"pip install datasets seqeval transformers==4.12.5","8f51ecaf":"from datasets import load_dataset","6bc07b3d":"raw_dataset = load_dataset(\"conll2003\")","11ab13cd":"raw_dataset","ce9c730f":"print(raw_dataset[\"train\"][0][\"tokens\"])\nprint(raw_dataset[\"train\"][0][\"ner_tags\"])","46cc069e":"dataset_feature = raw_dataset[\"train\"].features\nprint(dataset_feature)","92e1bcaf":"print(dataset_feature[\"ner_tags\"])","b8154b9e":"ner_labels = dataset_feature[\"ner_tags\"].feature.names\nprint(ner_labels)    # all NER labels","0d45700a":"# labels and respective ids are required to \n# support inference API on huggingFace website\n\nid2label = {str(i): label for i, label in enumerate(ner_labels)}\nlabel2id = {value: key for key, value in id2label.items()}","460a5e06":"model_checkpoint = \"bert-base-cased\"","e084ec65":"from transformers import AutoTokenizer","8934ba19":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","2e6b7223":"sample_input = tokenizer(raw_dataset[\"train\"][0][\"tokens\"], is_split_into_words=True)","ce3f2b87":"print(sample_input.tokens())\nprint(sample_input.word_ids())","123d7ee1":"print(raw_dataset[\"train\"][0][\"ner_tags\"])","ebe3f3b2":"def align_labels_and_tokens(word_ids, labels):\n    \n    \"\"\" Aligns tokens and their respective labels\n    \n    Args:\n        word_ids (list): word ids of tokens after subword tokenization.\n        labels (list): original labels irrespective of subword tokenization.\n        \n    Returns:\n        updated_labels (list): labels aligned with respective tokens. \n    \n    \"\"\"\n    \n    updated_labels = []\n    current_word = None\n    for word_id in word_ids:\n        if word_id != current_word:\n            current_word = word_id\n            updated_labels.append(-100 if word_id is None else labels[word_id])   \n        elif word_id is None:\n            updated_labels.append(-100)\n        else:\n            label = labels[word_id]\n            # B-XXX to I-XXX for subwords (Inner entities)\n            if label % 2 == 1:\n                label+=1\n            updated_labels.append(label)\n    return updated_labels\n","9461ac0c":"sample_labels = raw_dataset[\"train\"][0][\"ner_tags\"]\nprint(sample_labels)\nprint(align_labels_and_tokens(sample_input.word_ids(), sample_labels))","ca0f9214":"def tokenize_and_align_labels(dataset):\n    \n    \"\"\" Performs tokenization and aligns all tokens and labels\n        in the dataset.\n    \n    Args:\n        dataset (DatasetDict): dataset containing tokens and labels.\n    \n    Returns:\n        tokenized_data (dict): contains input_ids, attention_mask, token_type_ids, labels\n        \n    \"\"\"\n    \n    tokenized_data = tokenizer(dataset[\"tokens\"], truncation=True, is_split_into_words=True)\n    all_labels = dataset[\"ner_tags\"]\n    updated_labels = []\n    for i, labels in enumerate(all_labels):\n        updated_labels.append(align_labels_and_tokens(tokenized_data.word_ids(i), labels))\n    tokenized_data[\"labels\"] = updated_labels\n    return tokenized_data\n","fda46664":"tokenize_and_align_labels(raw_dataset['train'][:5])","2c47c84e":"tokenized_dataset = raw_dataset.map(\n    tokenize_and_align_labels,\n    batched = True,\n    remove_columns = raw_dataset[\"train\"].column_names\n)","8708ad79":"from transformers import DataCollatorForTokenClassification","ae345060":"# Data collator pads inputs and labels\ndata_collator = DataCollatorForTokenClassification(\n    tokenizer = tokenizer\n)","88db3c64":"from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    model_checkpoint, \n    num_labels=len(ner_labels),\n    id2label = id2label,\n    label2id = label2id\n)","85fe0c0e":"# Specifying training arguments\n\nbatch_size = 32\n\nargs = TrainingArguments(\n    model_checkpoint,\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    weight_decay=0.01\n)","5fac03ff":"from datasets import load_metric\nimport numpy as np","9f9320ab":"# Loading helper function to compute metrics\nmetric = load_metric(\"seqeval\")","778c18bc":"def compute_metrics(p):\n    \n    \"\"\" Computes and returns metrics during training.\n    \n    Args:\n        p (tuple): tuple containing predictions, labels as lists.\n    \n    Returns:\n        dict: Dictionary containing precision, recall, f1 score, accuracy. \n    \n    \"\"\"\n    \n    predictions, labels = p \n    predictions = np.argmax(predictions, axis=2)\n    true_predictions = [\n        [ner_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [ner_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    metrics = metric.compute(predictions=true_predictions, references=true_labels)\n    \n    return {\n        \"precision\": metrics[\"overall_precision\"],\n        \"recall\": metrics[\"overall_recall\"],\n        \"f1\": metrics[\"overall_f1\"],\n        \"accuracy\": metrics[\"overall_accuracy\"]\n    }\n","daedb25a":"trainer = Trainer(\n    model,\n    args,\n    train_dataset = tokenized_dataset[\"train\"],\n    eval_dataset = tokenized_dataset[\"validation\"],\n    data_collator = data_collator,\n    tokenizer = tokenizer,\n    compute_metrics = compute_metrics\n)","100515bb":"trainer.train()    # Fine-tunes model on downstream task","a66142c7":"from huggingface_hub import notebook_login\n\nnotebook_login()    # Authorizes huggingface repo","b2bd4214":"repo_name = \"bert-finetuned-ner\"\nmodel.save_pretrained(repo_name, push_to_hub=True)\ntokenizer.push_to_hub(repo_name)","d30e58ec":"import pandas as pd\nimport matplotlib\nfrom IPython.display import display\n\nmatplotlib.style.use('fivethirtyeight') ","148cadd4":"predictions, labels, _ = trainer.predict(tokenized_dataset[\"test\"])\npredictions = np.argmax(predictions, axis=2)\n\ntrue_predictions = [\n    [ner_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n    for prediction, label in zip(predictions, labels)\n]\ntrue_labels = [\n    [ner_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n    for prediction, label in zip(predictions, labels)\n]\n\nresults = metric.compute(predictions=true_predictions, references=true_labels)","34e75065":"results_df = pd.DataFrame({\"LOC\": results[\"LOC\"], \"MISC\": results[\"MISC\"], \"ORG\": results[\"ORG\"], \"PER\": results[\"PER\"]}).drop(\"number\", axis=0)","380bdcd3":"display(results_df)","6eaea1a3":"results_df.plot(kind=\"bar\", rot=0, figsize=(12,8))\nmatplotlib.pyplot.title(\"Performance on Test data\")\nmatplotlib.pyplot.show()","49cabfbb":"from transformers import pipeline\n\n# Loading the pipeline from hub\n# Pipeline handles the preprocessing and post processing steps\nmodel_checkpoint = \"balamurugan1603\/bert-finetuned-ner\"\nnamedEntityRecogniser = pipeline(\n    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n)","99eeec6b":"text = \"Mark Zuckerberg is one of the founders of Facebook, a company from the United States\"","7c21209e":"sample_output = namedEntityRecogniser([text])","ae38eef5":"print(sample_output)","cbc66c95":"from spacy import displacy","e377a366":"def visualize(pipeline_output, texts):\n    \n    \"\"\" Visualizes text and their Named entities.\n    \n    Args:\n        pipeline_output (list): Output of the pipeline.\n        texts (list): List containing original text.\n    \n    Returns:\n        Nothing\n        \n    \"\"\"\n    \n    for i in range(len(sample_output)):\n        entities = []\n        for ents in sample_output[i]:\n            entities.append({\"end\": ents[\"end\"], \"label\": ents[\"entity_group\"], \"start\": ents[\"start\"]})\n        displacy.render({\n            \"ents\": entities,\n            \"text\": texts[i]\n        }, style=\"ent\", manual=True)\n        ","45d7d258":"visualize(sample_output, [text])","241a9f9b":"# **Loading Dataset :**","bee27d66":"# **Data :**\n**CoNLL-2003 :** The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. **Link :** https:\/\/huggingface.co\/datasets\/conll2003","d3cfeeb8":"This tokenizer is different from normal tokenizer since it uses **subword tokenization** technique ( Splits single word into multiple tokens ). <br>Hence **word_ids** are used to keep track of the words ( sub-words of same word have same word_id ).","81b885c4":"# **Named Entity Recognition using Transfer Learning**","8494f9bb":"![image.png](attachment:677f09f8-ff09-44cd-b9d3-7b25432d64e9.png)\n\nBy : [Balamurugan P](https:\/\/www.linkedin.com\/in\/bala-murugan-62073b212\/)","13c0afd7":"After tokenization, There is a mismatch between tokens and labels. \n<br>The following function rectifies it.","d6840ccf":"# **Inference :**","4d17713f":"# **Importing model :**","be153bd8":"**O** - word doesn\u2019t correspond to any entity.<br>\n**B-PER\/I-PER** - word corresponds to the beginning of \/ is inside a person entity.<br>\n**B-ORG\/I-ORG** - word corresponds to the beginning of \/ is inside an organization entity.<br>\n**B-LOC\/I-LOC** - word corresponds to the beginning of \/ is inside a location entity.<br>\n**B-MISC\/I-MISC** - word corresponds to the beginning of \/ is inside a miscellaneous entity.<br>","476ff415":"# **Fine-tuning :**","4f7e2d39":"# **Evaluating the model :**","5c8f37a5":"# **Importing Tokenizer :**","f2a238b8":"# **Pushing model to hub :**","1f6d8c96":"# **Overview :**\nI have **Fine tuned BERT using HuggingFace transformers** to perform **Named Entity Recognition** on Text data. BERT is a state-of-the-art model with **attention mechanism** as underlying architecture trained with masked-language-modelling and next-sentence-prediction objectives, used for various tasks including Question answering systems, Text Summarization etc... which can also perform token classification tasks such as NER with great performance. "}}