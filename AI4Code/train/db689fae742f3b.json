{"cell_type":{"b59391b1":"code","1b62d361":"code","1a797e9e":"code","adc32b3b":"code","65f8ddb1":"code","2ec70d1a":"code","4e73c9c0":"code","b5b33548":"code","75b15c05":"code","1d21f463":"code","c48c34bd":"code","08dff2ec":"code","0ab4b7d9":"code","9e6c3839":"code","62ca239f":"code","983e7aaa":"code","5375edc9":"code","7066ab4e":"code","76944c36":"code","6f2ad9d0":"code","09b9d83c":"code","1e8a0ebd":"code","11ec1f68":"code","d919cd77":"code","706c57fe":"code","1629d4f5":"code","39a01940":"code","b32852b1":"code","a08fcae8":"code","7c44dbdc":"code","75b291f5":"code","c040d305":"code","6e22f5c0":"code","178e327a":"code","36309eb7":"code","be7f38fa":"code","866b7e51":"code","183469cd":"code","c94db1f7":"code","d9bb0f12":"code","158b8941":"code","335ac54b":"code","3dc15d23":"code","64d96686":"code","e87abd87":"code","88ca55ff":"code","924d8efb":"markdown","a7396695":"markdown","9ecb740a":"markdown","610c4637":"markdown","d9d53513":"markdown","51926b68":"markdown","e73d170b":"markdown","8c0abfa4":"markdown","eec61546":"markdown","77151ccf":"markdown","94a06b57":"markdown","049fc06f":"markdown","f280fb09":"markdown","f8b4ad85":"markdown","90963a4d":"markdown","656a3804":"markdown","68c58017":"markdown","db75f10a":"markdown","bad25b38":"markdown","22d0db3c":"markdown","8d281c71":"markdown","1f906376":"markdown","5c5f29fb":"markdown","833132c0":"markdown","0545c6bb":"markdown","c83378d6":"markdown"},"source":{"b59391b1":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1b62d361":"df = pd.read_csv(\"..\/input\/xAPI-Edu-Data\/xAPI-Edu-Data.csv\")\ndf","1a797e9e":"df.info()","adc32b3b":"df.describe()","65f8ddb1":"df.columns","2ec70d1a":"df.isnull().sum()","4e73c9c0":"df.rename(index=str, columns={'gender':'Gender', \n                              'NationalITy':'Nationality',\n                              'raisedhands':'RaisedHands',\n                              'VisITedResources':'VisitedResources'},\n                               inplace=True)\ndf.columns","b5b33548":"# Exploring nationalities\nnationality = sns.countplot(x='Nationality', data=df ,palette='coolwarm')\nnationality.set(xlabel='Nationality', ylabel='Count', title='Nationality')\nplt.setp(nationality.get_xticklabels(), rotation=60)\nplt.show()","75b15c05":"gender = sns.countplot(x='Class', hue='Gender', data=df, palette='coolwarm')\ngender.set(xlabel='Class', ylabel='Count', title='Gender comparison')\nplt.show()","1d21f463":"for i in range(1,17):\n    print(df.iloc[:,i].value_counts())\n    print(\"*\"*20)","c48c34bd":"print(\"Class Unique Values : \", df[\"Class\"].unique())\nprint(\"Topic Unique Values : \", df[\"Topic\"].unique())\nprint(\"StudentAbsenceDays Unique Values : \", df[\"StudentAbsenceDays\"].unique())\nprint(\"ParentschoolSatisfaction Unique Values : \", df[\"ParentschoolSatisfaction\"].unique())\nprint(\"Relation Unique Values : \", df[\"Relation\"].unique())\nprint(\"SectionID Unique Values : \", df[\"SectionID\"].unique())\nprint(\"Gender Unique Values : \", df[\"Gender\"].unique())","08dff2ec":"y=df.Class.values\nx=df.drop(\"Class\",axis=1)\nnp.unique(y)","0ab4b7d9":"#x = pd.get_dummies(data = x, columns = ['Class'] , prefix = ['Class'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['StudentAbsenceDays'] , prefix = ['StudentAbsenceDays'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['ParentschoolSatisfaction'] , prefix = ['ParentschoolSatisfaction'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['Relation'] , prefix = ['Relation'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['SectionID'] , prefix = ['SectionID'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['Gender'] , prefix = ['Gender'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['PlaceofBirth'] , prefix = ['PlaceofBirth'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['Semester'] , prefix = ['Semester'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['ParentAnsweringSurvey'] , prefix = ['ParentAnsweringSurvey'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['GradeID'] , prefix = ['GradeID'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['StageID'] , prefix = ['StageID'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['Topic'] , prefix = ['Topic'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['Nationality'] , prefix = ['Nationality'] , drop_first = False)\nx.head(3)","9e6c3839":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=52)","62ca239f":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n#Naive Bayes\nnb =  GaussianNB()\nnb.fit(x_train, y_train)\ny_pred=nb.predict(x_test)\n\nprint(\"Accuracy of naive bayees algorithm: \",nb.score(x_test,y_test))","983e7aaa":"from sklearn.neighbors import KNeighborsClassifier\n\nerror_rate = []\nfor i in range(1,51):    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    pred_i = knn.predict(x_test)\n    error_rate.append(np.mean(pred_i != y_test))\n\nplt.figure(figsize=(8,4))\nplt.plot(range(1,51),error_rate,color='darkred', marker='o',markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","5375edc9":"knn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(x_train,y_train)\ny_pred=knn.predict(x_test)\n\nprint('KNN (K=9) accuracy is: ',knn.score(x_test,y_test))","7066ab4e":"from sklearn.linear_model import SGDClassifier\nsgd =  SGDClassifier(loss='modified_huber', shuffle=True,random_state=101)\nsgd.fit(x_train, y_train)\ny_pred=sgd.predict(x_test)\nprint(sgd.score(x_test,y_test))\nprint('Classification report: \\n',classification_report(y_test,y_pred))","76944c36":"from sklearn.svm import SVC\n\nsvm=SVC(random_state=1)\nsvm.fit(x_train,y_train)\n\n#accuracy\nprint(\"accuracy of svm algorithm: \",svm.score(x_test,y_test))","6f2ad9d0":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\nprint(\"Accuracy score for Decision Tree Classification: \" ,dt.score(x_test,y_test))","09b9d83c":"from sklearn.ensemble import RandomForestClassifier\nresults = []\nn_estimator_options = [20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\nfor trees in n_estimator_options:\n    model = RandomForestClassifier(trees, oob_score=True, n_jobs=-1, random_state=101)\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = np.mean(y_test==y_pred)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, n_estimator_options).plot(color=\"darkred\",marker=\"o\")","1e8a0ebd":"results = []\nmax_features_options = ['auto',None,'sqrt',0.95,0.75,0.5,0.25,0.10]\nfor trees in max_features_options:\n    model = RandomForestClassifier(n_estimators=75, oob_score=True, n_jobs=-1, random_state=101, max_features = trees)\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = np.mean(y_test==y_pred)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, max_features_options).plot(kind=\"bar\",color=\"darkred\",ylim=(0.7,0.9))","11ec1f68":"results = []\nmin_samples_leaf_options = [5,10,15,20,25,30,35,40,45,50]\nfor trees in min_samples_leaf_options:\n    model = RandomForestClassifier(n_estimators=75, oob_score=True, n_jobs=-1, random_state=101, max_features = 0.5, min_samples_leaf = trees)\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = np.mean(y_test==y_pred)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, min_samples_leaf_options).plot(color=\"darkred\",marker=\"o\")","d919cd77":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=75, oob_score=True, n_jobs=-1, random_state=101, max_features = 0.5 , min_samples_leaf = 50)\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\nprint(rf.score(x_test,y_test))\n\nrf_cm = confusion_matrix(y_test,y_pred)\nprint('Confusion matrix: \\n',rf_cm)\n\nprint('Classification report: \\n',classification_report(y_test,y_pred))","706c57fe":"y=df.Relation.values\nx=df.drop(\"Relation\",axis=1)\nnp.unique(y)","1629d4f5":"x = pd.get_dummies(data = x, columns = ['Class'] , prefix = ['Class'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['StudentAbsenceDays'] , prefix = ['StudentAbsenceDays'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['ParentschoolSatisfaction'] , prefix = ['ParentschoolSatisfaction'] , drop_first = False)\n#x = pd.get_dummies(data = x, columns = ['Relation'] , prefix = ['Relation'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['SectionID'] , prefix = ['SectionID'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['Gender'] , prefix = ['Gender'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['PlaceofBirth'] , prefix = ['PlaceofBirth'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['Semester'] , prefix = ['Semester'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['ParentAnsweringSurvey'] , prefix = ['ParentAnsweringSurvey'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['GradeID'] , prefix = ['GradeID'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['StageID'] , prefix = ['StageID'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['Topic'] , prefix = ['Topic'] , drop_first = False)\nx = pd.get_dummies(data = x, columns = ['Nationality'] , prefix = ['Nationality'] , drop_first = False)\nx.head()","39a01940":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=52)","b32852b1":"#Naive Bayes\nnb =  GaussianNB()\nnb.fit(x_train, y_train)\ny_pred=nb.predict(x_test)\ngaussian_accuracy = nb.score(x_test,y_test)\nprint(\"Accuracy of naive bayees algorithm: \",gaussian_accuracy)","a08fcae8":"error_rate = []\nfor i in range(1,100):    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    pred_i = knn.predict(x_test)\n    error_rate.append(np.mean(pred_i != y_test))\n\nplt.figure(figsize=(8,4))\nplt.plot(range(1,100),error_rate,color='darkred', marker='o',markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","7c44dbdc":"knn = KNeighborsClassifier(n_neighbors=98)\nknn.fit(x_train,y_train)\ny_pred=knn.predict(x_test)\nknn_acc = knn.score(x_test,y_test)\nprint('KNN (K=98) accuracy is: ',knn_acc)","75b291f5":"svm=SVC(random_state=1)\nsvm.fit(x_train,y_train)\nsvm_pred = svm.score(x_test,y_test)\n#accuracy\nprint(\"accuracy of svm algorithm: \",svm_pred)","c040d305":"dt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ndt_acc = dt.score(x_test,y_test)\n#print('Classification report: \\n',classification_report(y_test,y_pred))\nprint(\"Accuracy score for Decision Tree Classification: \" , dt_acc)","6e22f5c0":"from sklearn.linear_model import SGDClassifier\nsgd =  SGDClassifier(loss='modified_huber', shuffle=True,random_state=101)\nsgd.fit(x_train, y_train)\ny_pred=sgd.predict(x_test)\nsgd_acc = sgd.score(x_test,y_test)\nprint(sgd_acc)\nprint('Classification report: \\n',classification_report(y_test,y_pred))","178e327a":"from sklearn.ensemble import RandomForestClassifier\nresults = []\nn_estimator_options = [20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\nfor trees in n_estimator_options:\n    model = RandomForestClassifier(trees, oob_score=True, n_jobs=-1, random_state=101)\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = np.mean(y_test==y_pred)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, n_estimator_options).plot(color=\"darkred\",marker=\"o\")","36309eb7":"results = []\nmax_features_options = ['auto',None,'sqrt',0.95,0.75,0.5,0.25,0.10]\nfor trees in max_features_options:\n    model = RandomForestClassifier(n_estimators=30, oob_score=True, n_jobs=-1, random_state=101, max_features = trees)\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = np.mean(y_test==y_pred)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, max_features_options).plot(kind=\"bar\",color=\"darkred\",ylim=(0.7,0.9))","be7f38fa":"results = []\nmin_samples_leaf_options = [5,10,15,20,25,30,35,40,45,50]\nfor trees in min_samples_leaf_options:\n    model = RandomForestClassifier(n_estimators=30, oob_score=True, n_jobs=-1, random_state=101, max_features = None, min_samples_leaf = trees)\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = np.mean(y_test==y_pred)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, min_samples_leaf_options).plot(color=\"darkred\",marker=\"o\")","866b7e51":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=30, oob_score=True, n_jobs=-1, random_state=101, max_features = None , min_samples_leaf = 25)\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\nrf_acc = rf.score(x_test,y_test)\nprint(rf_acc)\n\nrf_cm = confusion_matrix(y_test,y_pred)\nprint('Confusion matrix: \\n',rf_cm)\n\nprint('Classification report: \\n',classification_report(y_test,y_pred))","183469cd":"from xgboost import XGBClassifier\nresults = []\nmax_depth = [x for x in range(1,10)]\nfor depth in max_depth:\n    xgb = XGBClassifier(max_depth= depth ,n_estimators= 300 , objective=\"binary:logistic\")\n    xgb.fit(x_train, y_train)\n    y_pred_xgb = xgb.predict(x_test)\n    accuracy = np.mean(y_test==y_pred_xgb)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, max_depth).plot(color=\"darkred\",marker=\"o\")","c94db1f7":"results = []\nn_estimators = [x for x in range(100,500,20)]\nfor estimators in n_estimators:\n    xgb = XGBClassifier(max_depth=7,n_estimators= estimators , objective=\"binary:logistic\")\n    xgb.fit(x_train, y_train)\n    y_pred_xgb = xgb.predict(x_test)\n    accuracy = np.mean(y_test==y_pred_xgb)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, n_estimators).plot(color=\"darkred\",marker=\"o\")","d9bb0f12":"xgb_model = XGBClassifier(max_depth=7,n_estimators=220, objective=\"binary:logistic\")\nxgb_model.fit(x_train, y_train)\ny_pred_xgb = xgb_model.predict(x_test)\n\nprint('Classification report: \\n',classification_report(y_test,y_pred_xgb))\n#predictions = [round(value) for value in y_pred_xgb]\n#accuracy_xgb = accuracy_score(y_test, predictions)\n\nxgb_x_test = x_test\nxgb_cm = confusion_matrix(y_test,y_pred_xgb)\nxgb_cm","158b8941":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n#fit\nlr=LogisticRegression()\nlr.fit(x_train,y_train)\nlog_acc = lr.score(x_test,y_test)\n\n#confision matrix\nlr_cm = confusion_matrix(y_test,y_pred)\nlog_reg_x_test = x_test\n#accuracy\nprint(\"test accuracy is {}\", log_acc)\nlr_cm","335ac54b":"# visualize with seaborn library\nsns.heatmap(xgb_cm,annot=True,fmt=\"d\") \nplt.show()","3dc15d23":"# print the first 10 predicted probabilities of class membership\nxgb_model.predict_proba(xgb_x_test)[0:10]","64d96686":"arr = xgb_model.predict_proba(xgb_x_test)\n\n# Creating pandas dataframe from numpy array\npred_val = pd.DataFrame({'Predicted_Father': arr[:, 0], 'Predicted_Mum': arr[:, 1]})\npred_val","e87abd87":"import sklearn.metrics\n\nproba_test = xgb_model.predict_proba(xgb_x_test)\n\nproba_xgb_test = proba_test[:,1]\n\ny_test2 = np.where(y_test == 'Mum',1,0)\nfpr, tpr, threshold = sklearn.metrics.roc_curve(y_test2, proba_xgb_test)\nroc_auc = sklearn.metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.4f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","88ca55ff":"pred_val.to_csv(r'\\predicted_values.csv')","924d8efb":"One Hot Encoder ","a7396695":"# Result","9ecb740a":"# Random Forest Classifier","610c4637":"# Logistic Regression","d9d53513":"# Support Vector Machine","51926b68":"# Stochastic Gradient Descent","e73d170b":"Attributes\n\n1 Gender - student's gender (nominal: 'Male' or 'Female\u2019)\n\n2 Nationality- student's nationality (nominal:\u2019 Kuwait\u2019,\u2019 Lebanon\u2019,\u2019 Egypt\u2019,\u2019 SaudiArabia\u2019,\u2019 USA\u2019,\u2019 Jordan\u2019,\u2019 Venezuela\u2019,\u2019 Iran\u2019,\u2019 Tunis\u2019,\u2019 Morocco\u2019,\u2019 Syria\u2019,\u2019 Palestine\u2019,\u2019 Iraq\u2019,\u2019 Lybia\u2019)\n\n3 Place of birth- student's Place of birth (nominal:\u2019 Kuwait\u2019,\u2019 Lebanon\u2019,\u2019 Egypt\u2019,\u2019 SaudiArabia\u2019,\u2019 USA\u2019,\u2019 Jordan\u2019,\u2019 Venezuela\u2019,\u2019 Iran\u2019,\u2019 Tunis\u2019,\u2019 Morocco\u2019,\u2019 Syria\u2019,\u2019 Palestine\u2019,\u2019 Iraq\u2019,\u2019 Lybia\u2019)\n\n4 Educational Stages- educational level student belongs (nominal: \u2018lowerlevel\u2019,\u2019MiddleSchool\u2019,\u2019HighSchool\u2019)\n\n5 Grade Levels- grade student belongs (nominal: \u2018G-01\u2019, \u2018G-02\u2019, \u2018G-03\u2019, \u2018G-04\u2019, \u2018G-05\u2019, \u2018G-06\u2019, \u2018G-07\u2019, \u2018G-08\u2019, \u2018G-09\u2019, \u2018G-10\u2019, \u2018G-11\u2019, \u2018G-12 \u2018)\n\n6 Section ID- classroom student belongs (nominal:\u2019A\u2019,\u2019B\u2019,\u2019C\u2019)\n\n7 Topic- course topic (nominal:\u2019 English\u2019,\u2019 Spanish\u2019, \u2018French\u2019,\u2019 Arabic\u2019,\u2019 IT\u2019,\u2019 Math\u2019,\u2019 Chemistry\u2019, \u2018Biology\u2019, \u2018Science\u2019,\u2019 History\u2019,\u2019 Quran\u2019,\u2019 Geology\u2019)\n\n8 Semester- school year semester (nominal:\u2019 First\u2019,\u2019 Second\u2019)\n\n9 Parent responsible for student (nominal:\u2019mom\u2019,\u2019father\u2019)\n\n10 Raised hand- how many times the student raises his\/her hand on classroom (numeric:0-100)\n\n11- Visited resources- how many times the student visits a course content(numeric:0-100)\n\n12 Viewing announcements-how many times the student checks the new announcements(numeric:0-100)\n\n13 Discussion groups- how many times the student participate on discussion groups (numeric:0-100)\n\n14 Parent Answering Survey- parent answered the surveys which are provided from school or not (nominal:\u2019Yes\u2019,\u2019No\u2019)\n\n15 Parent School Satisfaction- the Degree of parent satisfaction from school(nominal:\u2019Yes\u2019,\u2019No\u2019)\n\n16 Student Absence Days-the number of absence days for each student (nominal: above-7, under-7)","8c0abfa4":"One Hot Encoder","eec61546":"# Random Forest","77151ccf":"Split","94a06b57":"# Naive Bayes","049fc06f":"# Decision Tree","f280fb09":"Best accurary score --->>> xgboost\n\nWe select this model.","f8b4ad85":"# KNN","90963a4d":"# Stochastic Gradient Descent","656a3804":"# XGBoost","68c58017":"# Naive Bayes","db75f10a":"# ML For Relation Attribute","bad25b38":"# Support Vector Machine","22d0db3c":"# EDA\n","8d281c71":"# ML for Class Attribute","1f906376":"This column contains 3 unique values, so we cannot apply logistic regression method.","5c5f29fb":"# Missing Values","833132c0":"# Split","0545c6bb":"# Decision Tree","c83378d6":"# KNN"}}