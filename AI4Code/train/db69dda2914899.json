{"cell_type":{"f9e68805":"code","7bc61265":"code","7eebbf62":"code","72044e7e":"code","38eb2f48":"code","721774fa":"code","d0ba0c68":"code","7359690e":"code","305027d6":"code","25f70afc":"code","f29a963a":"code","0f9ff0f9":"code","da47e426":"code","c4349857":"code","74e88259":"code","60925f61":"code","1ce04869":"code","27a64af8":"code","70905469":"code","ff29a35f":"code","eb7f7d59":"code","2d97754d":"code","ba59af19":"code","6b0b5243":"code","c34f026c":"code","278f7b7d":"code","1aaa4d54":"code","f849d979":"code","9a53b7e1":"code","925d1687":"code","3d467e34":"code","ae0af359":"code","1db7bc4c":"markdown","25c87935":"markdown","10518c63":"markdown","0dfb8583":"markdown","9de38675":"markdown","f6a9e255":"markdown","8bbf917b":"markdown","5a2f0871":"markdown","98631d41":"markdown","716e9dfe":"markdown","8236b3c8":"markdown","bd1335ba":"markdown","53e36a32":"markdown","de1e5cc8":"markdown","38793c49":"markdown","7262ec31":"markdown","0e5a833b":"markdown","0d744616":"markdown","6f2e4dd7":"markdown","b88e0b11":"markdown","668f6a32":"markdown","382bf408":"markdown","35fdfd25":"markdown","78f9af07":"markdown","e4c85a03":"markdown","4014ad61":"markdown","0d08d97d":"markdown","af92ce6f":"markdown","1589e0ae":"markdown","80647e18":"markdown","16b0cc8c":"markdown","aaaf1be9":"markdown","5f5a65cf":"markdown","b5338fb8":"markdown","8b965cba":"markdown","408861e4":"markdown","38d40eb9":"markdown","9e47ba99":"markdown","beeeeb19":"markdown","b99baee5":"markdown","5329905d":"markdown"},"source":{"f9e68805":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Plot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Feature Selection\nfrom sklearn.feature_selection import SelectPercentile, f_classif\n\n# Modelling\nfrom sklearn.model_selection import train_test_split\n\n# Regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBRegressor\n\n# Classification\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\n# Cross-Validation\nfrom sklearn.model_selection import StratifiedKFold","7bc61265":"submission_data = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\ntrain_data = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')","7eebbf62":"train_data.head()","72044e7e":"train_data.describe()","38eb2f48":"train_data.columns\n# It appears there is a target column.","721774fa":"# Missing values\nmissing_values_train = train_data.isna().any().sum()\nprint('Missing values in train data: {0}'.format(missing_values_train[missing_values_train > 0]))\n\nmissing_values_test = test_data.isna().any().sum()\nprint('Missing values in test data: {0}'.format(missing_values_test[missing_values_test > 0]))","d0ba0c68":"# Duplicates\nduplicates_train = train_data.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates_train))\n\nduplicates_test = test_data.duplicated().sum()\nprint('Duplicates in test data: {0}'.format(duplicates_test))","7359690e":"categorical_train = train_data.nunique().sort_values(ascending=True)\nprint('Categorical variables in train data: \\n{0}'.format(categorical_train))\n\ncategorical_test = test_data.nunique().sort_values(ascending=True)\nprint('Categorical variables in train data: \\n{0}'.format(categorical_test))\n# No categorical variable other than the `target`.","305027d6":"fig, ax = plt.subplots(1, 2)\nsns.heatmap(train_data.corr(), ax=ax[0])\nsns.heatmap(test_data.corr(), ax=ax[1])\nfig.set_figheight(8)\nfig.set_figwidth(16)\nfig.show()\n# Variables are not correlated","25f70afc":"# Get train data without the target and ids\ndata = train_data.iloc[:, 1:-1].copy()\n# Get the target\ny = train_data.target.copy()\n\n# It takes time to handle all of the data.\n# So, I am using a smaller portion of the data\n# while debugging\/testing.\n#data = train_data.iloc[0:50000, 1:-1].copy()\n#y = train_data.target[0:50000].copy()","f29a963a":"# Select features\nprint(f\"Data shape before selection: {data.shape}\")\nFeatureSelection = SelectPercentile(score_func=f_classif, percentile=20)\nselected = FeatureSelection.fit_transform(data, y)\nprint(f\"Data shape after selection: {selected.shape}\")\n\n# Get the list of the selected features\nselected_features = np.where(FeatureSelection.get_support())\nprint(f\"Selected Features: {selected_features}\") \n\nselected_features = [f'f{feature}' for feature in selected_features[0]]","0f9ff0f9":"# Since I am not sure about my selected features.\n# Sometimes it is better not even using them.\ndef get_X(use_selected_features=True):\n    if use_selected_features:\n        return data[selected_features]\n    return data","da47e426":"# Break data into two pieces or normalize\n# Gaussian Naive Bayes and Logistic Regression works with normalized data\ndef split_data(X, y, normalize=False):\n    if normalize:\n        scaler = StandardScaler()\n        normalized = scaler.fit_transform(X.copy())\n        return train_test_split(normalized, y, random_state=1)\n    return train_test_split(X, y, random_state=1)","c4349857":"def run_regression_algorithm(X, y, model, n, text, early_stopping_rounds = None):\n    # It actually takes hours to run all those models, \n    # so instead of running them everytime I will directly\n    # give the outputs from my previous runs.\n    return\n\n    # Split data\n    train_X, val_X, train_y, val_y = split_data(X, y)\n    # Fit model\n    if early_stopping_rounds: # For XGBRegressor\n        model.fit(train_X, train_y, early_stopping_rounds=early_stopping_rounds,\n                  eval_set=[(val_X, val_y)], verbose=False)\n    else:\n        model.fit(train_X, train_y)\n    # Make predictions\n    predictions = model.predict(val_X)\n    # Get AUC\n    auc = roc_auc_score(val_y, predictions)\n    # Print the error\n    print('{0}{1}AUC:  {2}'.format(text, n, auc))","74e88259":"# Compare with different values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    run_regression_algorithm(get_X(), y,\n                             DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0),\n                             '{0}  \\t\\t '.format(max_leaf_nodes), 'Max leaf nodes: ')","60925f61":"model = RandomForestRegressor(random_state=1)\nrun_regression_algorithm(get_X(), y, model, '', '')","1ce04869":"# Compare with different values of n_estimators\nfor n_estimators in range(100, 1000, 100):\n    run_regression_algorithm(get_X(), y,\n                             XGBRegressor(n_estimators=n_estimators, learning_rate=0.05, n_jobs=4),\n                             '{0}  \\t\\t '.format(n_estimators), 'N estimators: ', 5)","27a64af8":"# Cross-validation, https:\/\/www.kaggle.com\/hamzaghanmi\/make-it-simple\/notebook\ndef run_regression_algoritm_with_cross_validation(X, y, model, early_stopping_rounds = None):\n    # It actually takes hours to run all those models, \n    # so instead of running them everytime I will directly\n    # give the outputs from my previous runs.\n    return\n\n    fold = 1\n    skf = StratifiedKFold(n_splits=15, random_state=48, shuffle=True)\n    for train_idx, test_idx in skf.split(X, y):\n        train_X, val_X = X.iloc[train_idx], X.iloc[test_idx]\n        train_y, val_y = y.iloc[train_idx], y.iloc[test_idx]\n\n        # Fit model\n        if early_stopping_rounds: # For XGBRegressor\n            model.fit(train_X, train_y, early_stopping_rounds=early_stopping_rounds,\n                      eval_set=[(val_X, val_y)], verbose=False)\n        else:\n            model.fit(train_X, train_y)\n        # Make predictions\n        predictions = model.predict(val_X)\n        # Get AUC\n        auc = roc_auc_score(val_y, predictions)\n        # Print the AUC\n        print(\"Fold: %d  \\t\\t AUC:  %f\" %(fold, auc))\n        fold += 1","70905469":"model = DecisionTreeRegressor(max_leaf_nodes=500, random_state=0)\nrun_regression_algoritm_with_cross_validation(get_X(), y, model)","ff29a35f":"model = RandomForestRegressor(random_state=1)\nrun_regression_algoritm_with_cross_validation(get_X(), y, model)","eb7f7d59":"model = XGBRegressor(n_estimators=500, learning_rate=0.05, n_jobs=4)\nrun_regression_algoritm_with_cross_validation(get_X(), y, model, 5)","2d97754d":"def run_classification_algoritm(X, y, model, normalize):\n    # It actually takes hours to run all those models, \n    # so instead of running them everytime I will directly\n    # give the outputs from my previous runs.\n    return\n    \n    # Split data\n    train_X, val_X, train_y, val_y = split_data(X, y, normalize)\n    # Fit model\n    model.fit(train_X, train_y)\n    # Make predictions\n    predictions = model.predict(val_X)\n    # Get the accuracy score\n    score = accuracy_score(val_y, predictions)\n    # Print the accuracy\n    print(\"Accuracy score:  %f\" %(score))","ba59af19":"# Credits to https:\/\/www.kaggle.com\/markosthabit\/tbs-november-naive-bayes\n# https:\/\/iq.opengenus.org\/gaussian-naive-bayes\/\n#     Gaussian Naive Bayes is a variant of Naive Bayes that follows\n#     Gaussian normal distribution and supports continuous data.\n# So, normalize the data and actually try both ways to get the difference.  \nmodel = GaussianNB()\nrun_classification_algoritm(get_X(), y, model, True)\nrun_classification_algoritm(get_X(), y, model, False)","6b0b5243":"# Credits to https:\/\/www.kaggle.com\/sugamkhetrapal\/tps-nov-2021-1-06-xgboost\/notebook\nmodel = XGBClassifier(max_depth=1, subsample=0.5, colsample_bytree=0.5, eval_metric='error', use_label_encoder=False, random_state=1)\nrun_classification_algoritm(get_X(), y, model, False)","c34f026c":"# Credits to https:\/\/www.kaggle.com\/hamzaghanmi\/make-it-simple\n# https:\/\/kambria.io\/blog\/logistic-regression-for-machine-learning\/\nmodel = LogisticRegression(solver='liblinear')\nrun_classification_algoritm(get_X(False), y, model, True)\nrun_classification_algoritm(get_X(False), y, model, False)","278f7b7d":"# Cross-validation, https:\/\/www.kaggle.com\/hamzaghanmi\/make-it-simple\/notebook\ndef run_classification_algoritm_with_cross_validation(X, y, model, normalize):\n    # It actually takes hours to run all those models, \n    # so instead of running them everytime I will directly\n    # give the outputs from my previous runs.\n    return\n\n    X = np.array(X)\n    # Apply standard scaler\n    if normalize:\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n\n    fold = 1\n    skf = StratifiedKFold(n_splits=15, random_state=48, shuffle=True)\n    for train_idx, test_idx in skf.split(X, y):\n        train_X, val_X = X[train_idx], X[test_idx]\n        train_y, val_y = y.iloc[train_idx], y.iloc[test_idx]\n\n        # Fit the model\n        model.fit(train_X, train_y)\n        # Make predictions\n        predictions = model.predict_proba(val_X)[:,1]\n        # Get AUC\n        auc = roc_auc_score(val_y, predictions)\n        # Print the AUC\n        print(\"Fold: %d  \\t\\t AUC:  %f\" %(fold, auc))\n        fold += 1","1aaa4d54":"model = GaussianNB()\nrun_classification_algoritm_with_cross_validation(get_X(), y, model, True)","f849d979":"model = XGBClassifier(max_depth=1, subsample=0.5, colsample_bytree=0.5, eval_metric='error', use_label_encoder=False, random_state=1)\nrun_classification_algoritm_with_cross_validation(get_X(), y, model, False)","9a53b7e1":"model = LogisticRegression(solver='liblinear')\nrun_classification_algoritm_with_cross_validation(get_X(False), y, model, True)","925d1687":"# Create X, do not use features for now.\nX = get_X(False)\n\n# Create test X, drop ids.\n# For now create it without selected features\n# It gives better results this way.\ntest_X = test_data.iloc[:, 1:]\n\n# Apply standard scaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ntest_X = scaler.transform(test_X)\n\n# Create the model\nmodel = LogisticRegression(solver='liblinear')\n\n# Cross-validation, https:\/\/www.kaggle.com\/hamzaghanmi\/make-it-simple\/notebook\nfold = 1\ntest_predictions = np.zeros(test_X.shape[0])\nskf = StratifiedKFold(n_splits=15, random_state=48, shuffle=True)\nfor train_idx, test_idx in skf.split(X, y):\n    train_X, val_X = X[train_idx], X[test_idx]\n    train_y, val_y = y.iloc[train_idx], y.iloc[test_idx]\n\n    # Fit the model\n    model.fit(train_X, train_y)\n    # Make predictions\n    predictions = model.predict_proba(val_X)[:,1]\n    # Get AUC\n    auc = roc_auc_score(val_y, predictions)\n    # Print the error\n    print(\"Fold: %d  \\t\\t AUC:  %f\" %(fold, auc))\n\n    # Make predictions, use probability\n    test_predictions += model.predict_proba(test_X)[:,1] \/ skf.n_splits\n    fold += 1","3d467e34":"# Run the code to save predictions in the format used for competition scoring\noutput = pd.DataFrame({'id': test_data.id, 'target': test_predictions})\noutput.to_csv('submission.csv', index=False)","ae0af359":"output","1db7bc4c":"I am actually planing to make a feature analysis but for now keep it simple.\n\nCredits to https:\/\/www.kaggle.com\/markosthabit\/tbs-november-naive-bayes","25c87935":"```\nN estimators: 100  \t\t AUC:  0.71377\nN estimators: 200  \t\t AUC:  0.71857\nN estimators: 300  \t\t AUC:  0.71919\nN estimators: 400  \t\t AUC:  0.71921\nN estimators: 500  \t\t AUC:  0.71921\nN estimators: 600  \t\t AUC:  0.71921\nN estimators: 700  \t\t AUC:  0.71921\nN estimators: 800  \t\t AUC:  0.71921\nN estimators: 900  \t\t AUC:  0.71921\n```","10518c63":"# Basic Data Check\n\nCredits to https:\/\/www.kaggle.com\/raahulsaxena\/tps-nov-21-data-check-feature-analysis\n\nAlthough this data doesn't contain any missing, duplicated, categorical variables etc. This notebook helped me a lot to understand the basics of data check.","0dfb8583":"```\nAUC:  0.70127\n```","9de38675":"### XGBoost XGBClassifier","f6a9e255":"```\nAccuracy score:  0.676300\nAccuracy score:  0.676300\n```","8bbf917b":"### XGBoost XGBClassifier","5a2f0871":"### Random Forest Regressor","98631d41":"### Logistic Regression","716e9dfe":"# Modelling","8236b3c8":"## Classification with Cross-Validation","bd1335ba":"### Decision Tree Regressor","53e36a32":"### Decision Tree Regressor","de1e5cc8":"```\nFold: 1  \t\t AUC:  0.698372\nFold: 2  \t\t AUC:  0.699659\nFold: 3  \t\t AUC:  0.701081\nFold: 4  \t\t AUC:  0.705221\nFold: 5  \t\t AUC:  0.703162\nFold: 6  \t\t AUC:  0.702732\nFold: 7  \t\t AUC:  0.698388\nFold: 8  \t\t AUC:  0.704422\nFold: 9  \t\t AUC:  0.702508\nFold: 10 \t\t AUC:  0.699164\nFold: 11 \t\t AUC:  0.703322\nFold: 12 \t\t AUC:  0.699437\nFold: 13 \t\t AUC:  0.698628\nFold: 14 \t\t AUC:  0.703577\nFold: 15 \t\t AUC:  0.697842\n```","38793c49":"### Gaussian Naive Bayes","7262ec31":"```\nFold: 1  \t\t AUC:  0.710758\nFold: 2  \t\t AUC:  0.711012\nFold: 3  \t\t AUC:  0.710687\nFold: 4  \t\t AUC:  0.717570\nFold: 5  \t\t AUC:  0.714278\nFold: 6  \t\t AUC:  0.715372\nFold: 7  \t\t AUC:  0.712060\nFold: 8  \t\t AUC:  0.717004\nFold: 9  \t\t AUC:  0.716060\nFold: 10 \t\t AUC:  0.714406\nFold: 11 \t\t AUC:  0.716232\nFold: 12 \t\t AUC:  0.710764\nFold: 13 \t\t AUC:  0.711267\nFold: 14 \t\t AUC:  0.715551\nFold: 15 \t\t AUC:  0.710048\n```","0e5a833b":"# Explore Data","0d744616":"```\nMax leaf nodes: 5           AUC:  0.58807\nMax leaf nodes: 50          AUC:  0.62875\nMax leaf nodes: 500         AUC:  0.64594\nMax leaf nodes: 5000        AUC:  0.63100\n```","6f2e4dd7":"### Random Forest Regressor","b88e0b11":"## Handle duplicates","668f6a32":"# Submission","382bf408":"## Regression","35fdfd25":"```\nFold: 1  \t\t AUC:  0.748223\nFold: 2  \t\t AUC:  0.747558\nFold: 3  \t\t AUC:  0.744476\nFold: 4  \t\t AUC:  0.751129\nFold: 5  \t\t AUC:  0.750191\nFold: 6  \t\t AUC:  0.750874\nFold: 7  \t\t AUC:  0.746259\nFold: 8  \t\t AUC:  0.750019\nFold: 9  \t\t AUC:  0.751094\nFold: 10 \t\t AUC:  0.749236\nFold: 11 \t\t AUC:  0.750499\nFold: 12 \t\t AUC:  0.746850\nFold: 13 \t\t AUC:  0.747999\nFold: 14 \t\t AUC:  0.753662\nFold: 15 \t\t AUC:  0.747682\n```","78f9af07":"# Importing Librabies and Loading datasets","e4c85a03":"## Categorical variables","4014ad61":"### Gaussian Naive Bayes","0d08d97d":"```\nFold: 1  \t\t AUC:  0.709428\nFold: 2  \t\t AUC:  0.709001\nFold: 3  \t\t AUC:  0.710660\nFold: 4  \t\t AUC:  0.714958\nFold: 5  \t\t AUC:  0.712237\nFold: 6  \t\t AUC:  0.715526\nFold: 7  \t\t AUC:  0.708318\nFold: 8  \t\t AUC:  0.714901\nFold: 9  \t\t AUC:  0.714512\nFold: 10 \t\t AUC:  0.711692\nFold: 11 \t\t AUC:  0.715017\nFold: 12 \t\t AUC:  0.708338\nFold: 13 \t\t AUC:  0.710486\nFold: 14 \t\t AUC:  0.713657\nFold: 15 \t\t AUC:  0.712419\n```","af92ce6f":"## Classification","1589e0ae":"# Features","80647e18":"```\nFold: 1  \t\t AUC:  0.648110\nFold: 2  \t\t AUC:  0.645001\nFold: 3  \t\t AUC:  0.646716\nFold: 4  \t\t AUC:  0.651258\nFold: 5  \t\t AUC:  0.650062\nFold: 6  \t\t AUC:  0.650459\nFold: 7  \t\t AUC:  0.646524\nFold: 8  \t\t AUC:  0.652755\nFold: 9  \t\t AUC:  0.652514\nFold: 10 \t\t AUC:  0.647052\nFold: 11 \t\t AUC:  0.650992\nFold: 12 \t\t AUC:  0.646726\nFold: 13 \t\t AUC:  0.644246\nFold: 14 \t\t AUC:  0.648659\nFold: 15 \t\t AUC:  0.645532\n```","16b0cc8c":"## Handle missing values","aaaf1be9":"### XGBoost XGBRegressor","5f5a65cf":"I am not sure which modelling approach will give the best results. So, why not try many of them?\n\nAlso, different models will generate different predictions. Since, we are also allowed to submit probabilities, both predictions which are binary (0, 1) in classification models and probabilities in regression models should be okay to submit. It is also possible using `predict_proba` instead of `predict` in classification models.\n\n**I am trying what I have learned so far, so please comment if I am doing something wrong or weird :).**\n\n> It actually takes hours to run all those models, so instead of running them everytime I will directly give the outputs from my previous runs.","b5338fb8":"## Correlated variables","8b965cba":"## Final model\n\nSo far so good simplest solution gives the best results. So I have picked the `LogisticRegression` as the final model.\n\nCredits to https:\/\/www.kaggle.com\/hamzaghanmi\/make-it-simple\/notebook","408861e4":"```\nAccuracy score:  0.672873\n```","38d40eb9":"```\nFold: 1  \t\t AUC:  0.715399\nFold: 2  \t\t AUC:  0.716105\nFold: 3  \t\t AUC:  0.714442\nFold: 4  \t\t AUC:  0.723253\nFold: 5  \t\t AUC:  0.719537\nFold: 6  \t\t AUC:  0.720576\nFold: 7  \t\t AUC:  0.716693\nFold: 8  \t\t AUC:  0.722241\nFold: 9  \t\t AUC:  0.720729\nFold: 10 \t\t AUC:  0.719843\nFold: 11 \t\t AUC:  0.721102\nFold: 12 \t\t AUC:  0.716606\nFold: 13 \t\t AUC:  0.718105\nFold: 14 \t\t AUC:  0.721361\nFold: 15 \t\t AUC:  0.716609\n```","9e47ba99":"### Logistic Regression","beeeeb19":"```\nAccuracy score:  0.736847\nAccuracy score:  0.736813\n```","b99baee5":"### XGBoost XGBRegressor","5329905d":"## Regression with Cross-Validation"}}