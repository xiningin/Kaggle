{"cell_type":{"6933e7de":"code","6b175342":"code","491a2e18":"code","d4161929":"code","71f808ae":"code","2105506c":"code","dc000fe0":"code","523afba7":"code","e584029f":"code","fa32d82b":"code","ed534b2f":"code","3f2a8027":"code","ada11e44":"code","2d124170":"code","8c3c7e8a":"code","c8336692":"code","543c86be":"code","bd6429e7":"code","50f8f429":"code","86409dca":"code","feb1fd1b":"code","a59f7d9c":"code","4e11a33c":"code","fe2f6506":"code","8c6b1d7a":"code","2ef6d571":"code","d29b564f":"code","907d1979":"code","ded72577":"code","9832907f":"code","440e401f":"code","3389ef56":"code","3fae4a28":"code","b69e4a75":"code","5e8a5b0b":"code","adf8cba3":"code","954d6d58":"code","38e7e2bb":"code","cc232df5":"code","d39905bb":"code","1653d378":"code","53d85996":"code","d01dfb04":"code","03fc8b19":"code","33bd6a26":"code","da69c13c":"code","3c6094dd":"code","220f391c":"markdown","48bfca0e":"markdown","26dc5eef":"markdown","f2527fec":"markdown","3a8238b1":"markdown","d76b88b4":"markdown","ac2b1b72":"markdown","fb095ad7":"markdown","db615f74":"markdown","bc632b5c":"markdown","36a0c6d6":"markdown","9b129ba8":"markdown","1c28bc1b":"markdown"},"source":{"6933e7de":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport nltk\nimport datetime\n\n\ntrain_df = pd.read_csv(\"..\/input\/sales_train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nsubmission_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\nitems_df = pd.read_csv(\"..\/input\/items.csv\")\nitem_categories_df = pd.read_csv(\"..\/input\/item_categories.csv\")\nshops_df = pd.read_csv(\"..\/input\/shops.csv\")\n\nprint(\"Shape of train data : {}, Shape of test data : {}\".format(train_df.shape, test_df.shape))","6b175342":"[a for a in train_df.columns if a not in test_df]","491a2e18":"train_df.head()","d4161929":"test_df.head()","71f808ae":"items_df.head()","2105506c":"items_df.describe()","dc000fe0":"feature_count = 25\nitems_df['item_name_length'] = items_df['item_name'].map(lambda x : len(x)) #Length of each item_name(including punctuation in the item_name)\nitems_df['item_name_word_count'] = items_df['item_name'].map(lambda x : len(x.split(' '))) #Number of words\/group of characters seperated by a whitespace\ntfidf = sklearn.feature_extraction.text.TfidfVectorizer(max_features=feature_count) #tfidf = term frequency inverse document frequency\nitems_df_item_name_text_features = pd.DataFrame(tfidf.fit_transform(items_df['item_name']).toarray())\nprint(\"Shape of items_df_item_name_text_features : {}\".format(items_df_item_name_text_features.shape))\ncols = items_df_item_name_text_features.columns\nfor idx in range(feature_count):\n    items_df['item_name_tfidf_' + str(idx)] = items_df_item_name_text_features[cols[idx]]\nitems_df.head()","523afba7":"item_categories_df.head()","e584029f":"item_categories_df.describe()","fa32d82b":"feature_count = 25\nitem_categories_df['item_categories_name_length'] = item_categories_df['item_category_name'].map(lambda x : len(x)) #Length of each item_category_name(including punctuation in the item_category_name)\nitem_categories_df['item_categories_name_word_count'] = item_categories_df['item_category_name'].map(lambda x : len(x.split(' '))) #Number of words\/group of characters seperated by a whitespace\ntfidf = sklearn.feature_extraction.text.TfidfVectorizer(max_features=feature_count) #tfidf = term frequency inverse document frequency\nitem_categories_df_item_category_name_text_features = pd.DataFrame(tfidf.fit_transform(item_categories_df['item_category_name']).toarray())\ncols = item_categories_df_item_category_name_text_features.columns\nfor idx in range(feature_count):\n    item_categories_df['item_category_name_tfidf_' + str(idx)] = item_categories_df_item_category_name_text_features[cols[idx]]\nitem_categories_df.head()","ed534b2f":"shops_df.tail()","3f2a8027":"shops_df.describe()","ada11e44":"feature_count = 25\nshops_df['shop_name_length'] = shops_df['shop_name'].map(lambda x : len(x)) #Length of each shop_name(including punctuation in the shop_name)\nshops_df['shop_name_word_count'] = shops_df['shop_name'].map(lambda x : len(x.split(' '))) #Number of words\/group of characters seperated by a whitespace\ntfidf = sklearn.feature_extraction.text.TfidfVectorizer(max_features=feature_count) #tfidf = term frequency inverse document frequency\nshops_df_shop_name_text_features = pd.DataFrame(tfidf.fit_transform(shops_df['shop_name']).toarray())\ncols = shops_df_shop_name_text_features.columns\nfor idx in range(feature_count):\n    shops_df['shop_name_tfidf_' + str(idx)] = shops_df_shop_name_text_features[cols[idx]]\nshops_df.head()","2d124170":"train_df.head()","8c3c7e8a":"#turn data into monthly data\ntrain_df['date'] = pd.to_datetime(train_df['date'], format='%d.%m.%Y')\ntrain_df['month'] = train_df['date'].dt.month\ntrain_df['year'] = train_df['date'].dt.year\ntrain_df = train_df.drop(['date', 'item_price'], axis=1)\ntrain_df = train_df.groupby([c for c in train_df.columns if c not in ['item_cnt_day']], as_index=False)[['item_cnt_day']].sum()\ntrain_df = train_df.rename(columns={'item_cnt_day':'item_cnt_month'})\n\n#Monthly mean\nshop_item_monthly_mean = train_df[['shop_id', 'item_id', 'item_cnt_month']].groupby(['shop_id', 'item_id'], as_index=False)[['item_cnt_month']].mean()\nshop_item_monthly_mean = shop_item_monthly_mean.rename(columns={'item_cnt_month':'item_cnt_month_mean'})\n\n#Add Mean Features\ntrain_df = pd.merge(train_df, shop_item_monthly_mean, how='left', on=['shop_id', 'item_id'])\ntrain_df.head()","c8336692":"#Last Month : Oct 2015\nshop_item_prev_month = train_df[train_df['date_block_num'] == 33][['shop_id', 'item_id', 'item_cnt_month']]\nshop_item_prev_month = shop_item_prev_month.rename(columns={'item_cnt_month':'item_cnt_prev_month'})\nshop_item_prev_month.head()","543c86be":"#Add the above previous month features\ntrain_df = pd.merge(train_df, shop_item_prev_month, how='left', on=['shop_id', 'item_id'])\ntrain_df.head()","bd6429e7":"np.where(pd.isnull(train_df))","50f8f429":"train_df = train_df.fillna(0.)\ntrain_df.head()","86409dca":"#Add Item, Category and Shop features\ntrain_df = pd.merge(train_df, items_df, how='left', on='item_id')\ntrain_df.head()","feb1fd1b":"train_df = pd.merge(train_df, item_categories_df, how='left', on=['item_category_id'])\ntrain_df.head()","a59f7d9c":"train_df = pd.merge(train_df, shops_df, how='left', on=['shop_id'])\ntrain_df.head()","4e11a33c":"#Manipulate test data\ntest_df['month'] = 11\ntest_df['year'] = 2015\ntest_df['date_block_num'] = 34\ntest_df.head()","fe2f6506":"#Add mean features\nshop_item_monthly_mean.head()","8c6b1d7a":"len(test_df)","2ef6d571":"len(train_df)","d29b564f":"test_df = pd.merge(test_df, shop_item_monthly_mean, how='left', on=['shop_id', 'item_id'])\nprint(len(test_df))","907d1979":"test_df.head()","ded72577":"5320 in train_df.item_id.values","9832907f":"5233 in train_df.item_id.values","440e401f":"#Add previous month features\ntest_df = pd.merge(test_df, shop_item_prev_month, how='left', on=['shop_id', 'item_id'])\ntest_df.head()","3389ef56":"#Items features\ntest_df = pd.merge(test_df, items_df, how='left', on='item_id')\ntest_df.head()","3fae4a28":"#Item Category features\ntest_df = pd.merge(test_df, item_categories_df, how='left', on='item_category_id')\n#Shops features\ntest_df = pd.merge(test_df, shops_df, how='left', on='shop_id')\ntest_df = test_df.fillna(0.)\ntest_df['item_cnt_month'] = 0.\ntest_df.head()","b69e4a75":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ntrain_test_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\nprint(\"train_df.shape = {}, test_df.shape = {}, train_test_df.shape = {}\".format(train_df.shape, test_df.shape, train_test_df.shape))\nstores_hm = train_test_df.pivot_table(index='shop_id', columns='item_category_id', values='item_cnt_month', aggfunc='count', fill_value=0)\nprint(\"stores_hm.shape = {}\".format(stores_hm.shape))\nstores_hm.tail()","5e8a5b0b":"#Heatmap of \"item_cnt_month\" in \"shop_id vs item_category_id\" in \"train_test_df\"\nfig, ax = plt.subplots(figsize=(15,15))\nsns.heatmap(stores_hm, ax=ax, cbar=False)","adf8cba3":"#Heatmap of \"item_cnt_month\" in \"shop_id vs item_category_id\" in \"train_df\"\nstores_hm = train_df.pivot_table(index='shop_id', columns='item_category_id', values='item_cnt_month', aggfunc='count', fill_value=0)\n_, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(stores_hm, ax=ax, cbar=False)","954d6d58":"#Heatmap of \"item_cnt_month\" in \"shop_id vs item_category_id\" in \"test_df\"\nstores_hm = test_df.pivot_table(index='shop_id', columns='item_category_id', values='item_cnt_month', aggfunc='count', fill_value=0)\n_, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(stores_hm, ax=ax, cbar=False)","38e7e2bb":"for c in ['shop_name', 'item_category_name', 'item_name']:\n    le = sklearn.preprocessing.LabelEncoder()\n    le.fit(list(train_df[c].unique()) + list(test_df[c].unique()))\n    train_df[c] = le.transform(train_df[c].astype(str))\n    test_df[c] = le.transform(test_df[c].astype(str))\n    print(c)","cc232df5":"train_df.head()","d39905bb":"print(train_df['shop_id'], train_df['shop_name'])\nprint('*'*80)\ntest_df.head","1653d378":"feature_list = [c for c in train_df.columns if c not in 'item_cnt_month']\n#Validation hold out month is 33\nx1 = train_df[train_df['date_block_num'] < 33]\ny1 = np.log1p(x1['item_cnt_month'].clip(0., 20.))\nx1 = x1[feature_list]\nx2 = train_df[train_df['date_block_num'] == 33]\ny2 = np.log1p(x2['item_cnt_month'].clip(0., 20.))\nx2 = x2[feature_list]\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators=25, random_state=42, max_depth=15, n_jobs=-1)#use n_estimators=25, max_depth=15, random_state=42(for consistent results)\nrf.fit(x1, y1)\nprint(\"RMSE on Validation hold out month 33: {}\".format(np.sqrt(sklearn.metrics.mean_squared_error(y2.clip(0., 20.), rf.predict(x2).clip(0., 20.)))))\n\n#Full train\nrf.fit(train_df[feature_list], train_df['item_cnt_month'].clip(0., 20.))\nprint(\"Accuracy on training data without considering variable importances:{}\".format(round(rf.score(train_df[feature_list], train_df['item_cnt_month'].clip(0., 20.))*100, 2)))\n\n#predict\ntest_df['item_cnt_month'] = rf.predict(test_df[feature_list]).clip(0., 20.)\n\n#create submission file\ntest_df[['ID', 'item_cnt_month']].to_csv('submission.csv', index=False)","53d85996":"from sklearn.tree import export_graphviz\nimport pydot\nfrom IPython.display import Image","d01dfb04":"#doesn't print. May be because its too large. Try with a smaller image.\n'''tree = rf.estimators_[3]\nexport_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n(graph, ) = pydot.graph_from_dot_file('tree.dot')\ngraph.write_png('tree.png')\nImage(filename='tree.png')'''","03fc8b19":"#To see the decision tree in action, visualizing with a smaller one for illustrative purposes\nrf_small = RandomForestRegressor(n_estimators=2, random_state=42, max_depth=3, n_jobs=-1)\nrf_small.fit(train_df[feature_list], train_df['item_cnt_month'].clip(0., 20.))\nsmall_tree = rf_small.estimators_[1]\nexport_graphviz(small_tree, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\ngraph.write_png('small_tree.png')\nImage(filename='small_tree.png')","33bd6a26":"# Get numerical feature importances\nimportances = list(rf.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","da69c13c":"# Set the style\nplt.style.use('fivethirtyeight')\n\n#set size\nfig = plt.figure(figsize=(25, 5))\nax = fig.add_subplot(111)\n\n# list of x locations for plotting\nx_values = list(range(len(importances)))\n\n# Make a bar chart\nax.bar(x_values, importances, orientation = 'vertical')\n\n# Tick labels for x axis\nplt.xticks(x_values, feature_list, rotation='vertical', fontsize=12)\n\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');","3c6094dd":"rf_most_important = RandomForestRegressor(n_estimators=25, random_state=42, max_depth=15, n_jobs=-1)\n\n# Extract the ten most important features\nimportant_features = ['item_cnt_month_mean', 'date_block_num', 'item_cnt_prev_month', 'item_id', 'item_name', 'item_name_length', 'year', 'item_name_word_count', 'item_name_tfidf_21', 'item_category_name_tfidf_6']\n\n#Full train\nrf_most_important.fit(train_df[important_features], train_df['item_cnt_month'].clip(0., 20.))\nprint(\"Accuracy on training data considering variable importances:{}\".format(round(rf_most_important.score(train_df[important_features], train_df['item_cnt_month'].clip(0., 20.))*100, 2)))\n\n#predict\ntest_df['item_cnt_month'] = rf_most_important.predict(test_df[important_features]).clip(0., 20.)\n\n#create submission file\ntest_df[['ID', 'item_cnt_month']].to_csv('submission_variable_importance.csv', index=False)","220f391c":"**Text features for shops_df**","48bfca0e":"**Simple bar plot of the feature importances to illustrate the disparities in the relative significance of the variables.**","26dc5eef":"**Visualize**","f2527fec":"**Label Encoding**","3a8238b1":" **Train & Predict Models**","d76b88b4":"**For the available data, new random forest with only ten most important variables works better than the one which considers all features.**","ac2b1b72":"Author: Jatin Singh Bhati\n\n**Prediction of Sales using Random Forest** \n\n**Loading Libraries and Data**","fb095ad7":"**New random forest with only ten most important variables**","db615f74":"**shop_id and labels for shop_name are the same and hence redundant. Similarly in the case of item_name and item_category_name. One in each redundant pair can be dropped.**\n\n**These are not dropped for the time being, since variable importance takes care of it later in this notebook.**","bc632b5c":"**Text features for items_df**","36a0c6d6":"**Visualization of a single decision tree**","9b129ba8":"**Variable Importances**\n\n**In order to quantify the usefulness of all the variables in the entire random forest, we can look at the relative importances of the variables.**","1c28bc1b":"**Text features for item_categories_df**"}}