{"cell_type":{"d75b7a5b":"code","c72f630d":"code","1e34b766":"code","86aa3376":"markdown","40dc91ce":"markdown"},"source":{"d75b7a5b":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA","c72f630d":"def pca_custom(X, k):\n    n, p = np.shape(X)\n    # make Z with mean zero\n    Z = X - np.mean(X, axis=0)\n    # calculate covariance maxtrix\n    covZ = np.cov(Z.T)\n    # calculate eigenvalues and eigenvectors\n    eigValue, eigVector = np.linalg.eig(covZ)\n    # sort eigenvalues in descending order\n    index = np.argsort(-eigValue)\n    \n    # select k principle components\n    if k > p:\n        print (\"k must lower than input data dimension\uff01\")\n        return 1\n    else:\n        # select k eigenvectors with largest eigenvalues\n        selectVector = eigVector[:, index[:k]]\n        T = np.matmul(Z, selectVector)\n    return T, selectVector","1e34b766":"use_sklearn = True\n\n# generate data\n# x = np.random.randn(100, 20)\n\ny = np.random.randn(20, 1)\nx = np.matmul(y, [[1.3, -0.5]])\n\n# set k\nk = 2\n# PCA\npcaX, selectVector = pca_custom(x, k)\n\nprint('PCA transformation matrix: ')\nprint(selectVector)\n\nif use_sklearn:\n    z = x - np.mean(x, axis=0)\n    pcaResults = PCA(n_components=k).fit(z)\n    print ('PCA transformation matrix from sklearn:')\n    print(pcaResults.components_.T)\n    newX = np.matmul(z, pcaResults.components_.T)\n\nprint('original data')\nplt.plot(x[:, 0], x[:, 1], 'ok')\nplt.show()\n\nprint('After PCA')\nplt.plot(pcaX[:, 0], pcaX[:, 1], 'or')\nplt.show()\n\nif use_sklearn:\n    print('After PCA from sklearn')\n    plt.plot(newX[:, 0], newX[:, 1], 'ob')\n    plt.show()","86aa3376":"# Principal component analysis\nwiki: https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis\n\nPrincipal component analysis (PCA) is a statistical procedure that uses an orthogonal linear transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. \n\nThis transformation is defined in such a way that the first principal component has the largest possible variance, and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n\n<img src=\"https:\/\/s2.ax1x.com\/2019\/10\/06\/ugWAC6.png\" alt=\"drawing\" width=\"700\"\/>\n\n## Method\nObservation set $X$ contains $N$ samples and each sample $x_n \\in R^p$.   Transformation matrix $W$ contains $p$ column vectors, each vector $w_k \\in R^p$.\nPrinciple component score\n$$s_{nk} = x_n w_k$$\n\nIn order to maximize variance, the first weight vector $w_1$ thus has to satisfy\n$$w_1 = \\arg\\max_{w_1} \\sum_n s_{nk}^2=\\arg\\max_{w_1} W^T X^T X W,$$\n$$s.t. ||W||=1$$\n\nSince $w_1$ has been defined to be a unit vector, it equivalently also satisfies\n$$w_1 = \\arg\\max_{w_1} \\frac{W^T X^T X W}{W^T W}$$\n\nA standard result for a positive semidefinite matrix such as $X^T X$ is that the maximum possible value is the largest eigenvalue of the matrix, which occurs when $w_1$ is the corresponding eigenvector.\n\nThe full principal components decomposition of X can therefore be given as\n$$T = XW$$\nwhere W is a p-by-p matrix of weights whose columns are the eigenvectors of $X^T X$.\n\nPrincipal component analysis is also a technique for dimension reduction.\u200aIt combines our input variables in a specific way, then we can drop the \u201cleast important\u201d variables while still retaining the most valuable parts of all of the variables.\n\n## Algorithm\n1. Take the matrix of independent variables $X$ and, for each column, subtract the mean of that column from each entry and obtain $Z$. (This ensures that each column has a mean of zero.)\n2. Take the matrix $Z$, transpose it, and multiply the transposed matrix by $Z$. The resulting matrix is the covariance matrix $Z^T Z$.\n3. Calculate the eigenvectors and their corresponding eigenvalues of $Z^{T}Z$. \n4. Take the eigenvalues $\u03bb_1, \u03bb_2, \u2026, \u03bb_p$ and sort them from largest to smallest. In doing so, sort the eigenvectors in $W$ accordingly to obtain $W^*$.\n5. Calculate $Z^* = ZW^*$. This new matrix, $Z^*$, is a centered\/standardized version of $X$ but now each observation is a combination of the original variables, where the weights are determined by the eigenvector.","40dc91ce":"# PlayGround\nhttp:\/\/setosa.io\/ev\/principal-component-analysis\/\n# Code\nhttps:\/\/github.com\/csuldw\/MachineLearning\/tree\/master\/PCA"}}