{"cell_type":{"85c9c9aa":"code","87c60eb2":"code","9546847b":"code","493f51be":"code","9274c2a3":"code","bb48bf2b":"code","c583d42f":"code","4d091b3f":"code","da99a6b9":"code","ba538307":"markdown","1d5fbc91":"markdown","48ec7d72":"markdown","8ae5b7ef":"markdown","167c436a":"markdown","2a2ae841":"markdown"},"source":{"85c9c9aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","87c60eb2":"## \u8a13\u7df4\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\ntrain = pd.read_csv(\"..\/input\/ykc-cup-1st\/train.csv\")\ntrain.head()\n\n## \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\ntest = pd.read_csv(\"..\/input\/ykc-cup-1st\/test.csv\")\ntest.head()\n\n## submission\u30d5\u30a1\u30a4\u30eb\u8aad\u307f\u8fbc\u307f\nsample_submission = pd.read_csv(\"..\/input\/ykc-cup-1st\/sample_submission.csv\")\nsample_submission.head()\n\ntrain[\"area1\"] = train[\"area_name\"].apply(lambda x : x.split(\" \")[0])\ntrain[\"area2\"] = train[\"area_name\"].apply(lambda x : x.split(\" \")[1])\ntest[\"area1\"] = test[\"area_name\"].apply(lambda x : x.split(\" \")[0])\ntest[\"area2\"] = test[\"area_name\"].apply(lambda x : x.split(\" \")[1])\n\ndef count_encoding(x_train, x_test, col, suffix = \"_count\", pre_concat = True):\n    if pre_concat: ##\u3082\u3057\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3082\u7528\u3044\u3066count\u3059\u308b\u306a\u3089\n        dic = pd.concat([x_train[col], x_test[col]]).value_counts().to_dict()\n    else:\n        dic = x_train[col].value_counts().to_dict()\n    x_train[col + suffix] = x_train[col].map(dic)\n    x_test[col + suffix] = x_test[col].map(dic)\n    return x_train, x_test\n\n## count\n# train[\"genre_area\"] = train[\"genre_name\"] + train[\"area_name\"]\n# test[\"genre_area\"] = test[\"genre_name\"] + test[\"area_name\"]\nfor c in [\"genre_name\", \"area_name\", \"area1\", \"area2\"]:\n    train, test = count_encoding(train, test, c)\n# train = train.drop([\"genre_area\"], axis = 1)\n# test = test.drop([\"genre_area\"], axis = 1)\n\n\ndef get_distance_feature(store):\n    d = np.sqrt(np.sum((store[[\"latitude\", \"longitude\"]].values[None, :, :] - store[[\"latitude\", \"longitude\"]].values[:, None, :]) ** 2, axis = 2))\n\n    for t in [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 5, 7]:\n        store[f\"rate_under{t}\"] = np.mean(d < t, axis = 1)\n\n    store[\"mean\"] = np.mean(d, axis = 1)\n    for q in [0.1, 1, 3, 5, 10, 25, 50, 75, 90]:\n        store[f\"quantile{q}\"] = np.percentile(d, q, axis = 1)\n\n    store = store.drop([\"longitude\", \"latitude\"], axis = 1)\n    return store\n\n\ndf = pd.concat([train, test])\n\n## \u8fd1\u304f\u306e\u304a\u5e97\nstore = df.groupby([\"store_id\"]).apply(lambda x : x.iloc[0][[\"latitude\", \"longitude\"]])\nstore = get_distance_feature(store)\nstore = store.add_prefix(\"distance_feature_\")\ntrain = train.merge(store, on = \"store_id\", how = \"left\")\ntest = test.merge(store, on = \"store_id\", how = \"left\")\n\n## \u8fd1\u304f\u306e\u304a\u5e97\uff08\u540c\u3058\u30b8\u30e3\u30f3\u30eb\uff09\nstore = df.groupby([\"store_id\"]).apply(lambda x : x.iloc[0][[\"latitude\", \"longitude\", \"genre_name\"]])\n\nstore_parts = []\nfor g in store[\"genre_name\"].unique()[::-1]:\n    store_part = store[store[\"genre_name\"] == g][[\"latitude\", \"longitude\"]]\n    store_parts.append(store_part)\n\nstore = pd.concat(store_parts)\nstore = store.add_prefix(\"distance_feature_same_genre_\")\ntrain = train.merge(store, on = \"store_id\", how = \"left\")\ntest = test.merge(store, on = \"store_id\", how = \"left\")\n\n## area size \u3068 area density\n# for a in [\"area_name\", \"area1\", \"area2\"]:\nfor a in [\"area_name\", \"area1\"]:\n# for a in [\"area_name\"]:\n    area = df.groupby([a])[[\"latitude\", \"longitude\"]].agg(\"std\").add_suffix(\"_std\")\n    area[f\"{a}_size1\"] = np.sqrt(area[\"latitude_std\"] ** 2 + area[\"longitude_std\"] ** 2)\n    area_max = df.groupby([a])[[\"latitude\", \"longitude\"]].agg(\"max\")\n    area_min = df.groupby([a])[[\"latitude\", \"longitude\"]].agg(\"min\")\n\n    area_diff = area_max - area_min\n    area[f\"{a}_size2\"] = np.sqrt((area_diff[\"latitude\"] ** 2 + area_diff[\"longitude\"] ** 2))\n    area = area.drop([\"latitude_std\", \"longitude_std\"], axis = 1)\n    train = train.merge(area, on = a, how = \"left\")\n    test = test.merge(area, on = a, how = \"left\")\n\n    train[f\"{a}_density1\"] = train[f\"{a}_count\"] \/ (train[f\"{a}_size1\"] + 1e-8)\n    train[f\"{a}_density2\"] = train[f\"{a}_count\"] \/ (train[f\"{a}_size2\"] + 1e-8)\n    test[f\"{a}_density1\"] = test[f\"{a}_count\"] \/ (test[f\"{a}_size1\"] + 1e-8)\n    test[f\"{a}_density2\"] = test[f\"{a}_count\"] \/ (test[f\"{a}_size2\"] + 1e-8)","9546847b":"import numpy as np\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport pandas as pd\nfrom tqdm import tqdm\n\ndef target_encoding(x_train, x_test, label_train, cols, suffix = \"_te\", num_fold = 5, smooth_param = 0.1, stratified = False):\n    # target encoding\u3092\u884c\u3046\u95a2\u6570\n\n    # input:\n    # smooth_param : smoothing\u306e\u5f37\u3055\u3092\u6c7a\u3081\u308b\uff0e \u3042\u308b\u30ab\u30c6\u30b4\u30ea\u306e\u30b5\u30f3\u30d7\u30eb\u304c\u5c11\u306a\u3044\u3068\u304d\u306b\uff0c\u305d\u308c\u3092\u5168\u4f53\u306e\u30e9\u30d9\u30eb\u306e\u5e73\u5747\u5024\u3092prior\u3068\u3057\u3066\u5747\u3059\n    \n    x_train[\"label\"] = label_train\n    label_average = np.average(label_train) ## \u5168\u4f53\u306e\u5e73\u5747\u5024\uff0e smoothing\u306b\u4f7f\u3046\n    n_reg = len(x_train) * smooth_param\n    \n    for col in cols:\n        if num_fold > 1:\n            if stratified:\n                kfold = StratifiedKFold(num_fold, shuffle=True, random_state=42)\n            else:\n                kfold = KFold(num_fold, shuffle=True, random_state=42)\n            x_train[col + \"_te\"] = -999\n            for k_fold, (agg_inds, not_agg_inds) in enumerate(kfold.split(x_train, x_train[col].astype(\"str\"))):\n                print(f\"{col}, {k_fold+1} \/ {num_fold}\")\n                dic = x_train.iloc[agg_inds].groupby(col)[\"label\"].agg(lambda x : (np.sum(x) + n_reg * label_average)\/(len(x) + n_reg)).to_dict()\n                x_train.loc[not_agg_inds, col + \"_te\"] = x_train.loc[not_agg_inds, col].map(dic)\n            dic = x_train.groupby(col)[\"label\"].agg(\"mean\").to_dict()\n        else:\n            dic = x_train.groupby(col)[\"label\"].agg(\"mean\").to_dict()\n            x_train[col + \"_te\"] = x_train[col].map(dic)\n        x_test[col + \"_te\"] = x_test[col].map(dic)#.fillna(label_average)\n        \n    x_train = x_train.drop(\"label\", axis = 1)\n    return x_train, x_test\n\ncat = [\"store_id\", \"genre_name\", \"area_name\", \"day_of_week\", \"area1\", \"area2\"]\ntrain, test = target_encoding(train, test, train[\"log_visitors\"], cat, num_fold = 1)","493f51be":"target = \"log_visitors\" ## \u4e88\u6e2c\u5bfe\u8c61\ndrops = [\"id\", target] + cat\nfeatures = list(train.columns.drop(drops)) ## \u4e88\u6e2c\u306b\u4f7f\u7528\u3059\u308b\u7279\u5fb4\u91cf\u306e\u540d\u524d\nn_split = 3 ## cross validation\u306efold\u6570\nprint(\"num_features : \", len(features))","9274c2a3":"from sklearn.model_selection import StratifiedKFold\n\n## cross validation\u3092\u884c\u3044\uff0c\u5404fold\u3067\u8a13\u7df4\u3057\u305f\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u306e\u5e73\u5747\u5024\u3092submit\u3059\u308b\npreds_test = []\nkfold = StratifiedKFold(n_splits=n_split, shuffle = True, random_state=42)\nrmses = []\nfi = 0\nfor i_fold, (train_idx, valid_idx) in enumerate(kfold.split(train, train[\"store_id\"])):\n    print(f\"--------fold {i_fold}-------\")\n    \n    ## train data\n    x_tr = train.loc[train_idx, features]\n    y_tr = train.loc[train_idx, target]\n\n    ## valid data\n    x_va = train.loc[valid_idx, features]\n    y_va = train.loc[valid_idx, target]\n\n    ## train LGBM model\n    model = LGBMRegressor(subsample = 0.8, subsample_freq=1, colsample_bytree=0.8, learning_rate=0.01, n_estimators=10000,\n                          min_split_gain=1e-4, min_child_weight=1e-4, reg_alpha=1, reg_lambda=1)\n#     model = LGBMRegressor()\n    model.fit(x_tr, y_tr, eval_set = [(x_tr.values, y_tr), (x_va.values, y_va)],\n              early_stopping_rounds = 500, verbose = 100)\n    fi += model.booster_.feature_importance(\"gain\")\n    ## evaluate on valid\n    pred_val = model.predict(x_va)\n    rmse = np.sqrt(np.mean((pred_val - y_va) ** 2))\n    corr = np.corrcoef(pred_val, y_va)[0,1]\n    print(\"rmse : \", rmse)\n    print(\"corr : \", corr)\n    \n    ## pred on test\n    pred_test = model.predict(test[features].values)\n    preds_test.append(pred_test)\n    \n    rmses.append(rmse)\n    ","bb48bf2b":"print(np.mean(rmses))","c583d42f":"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfeature_importance = pd.DataFrame({\"importance\" : fi, \"name\" : features})\nplt.figure(figsize = (10, 16))\nsns.barplot(data = feature_importance.sort_values(by = \"importance\", ascending = False).head(30), x = \"importance\", y = \"name\")\nplt.show()","4d091b3f":"list(feature_importance.sort_values(by = \"importance\", ascending = False).tail(10)[\"name\"])","da99a6b9":"## 5fold\u3067\u8a13\u7df4\u3057\u305f\u305d\u308c\u305e\u308c\u306e\u4e88\u6e2c\u5024\u3092\u5143\u306b\uff0c\u305d\u308c\u3089\u306e\u5e73\u5747\u3092\u8a08\u7b97\u3057\u3066\u6700\u7d42\u7684\u306a\u4e88\u6e2c\u5024\u3068\u3059\u308b\uff0e\nfinal_pred = np.mean(np.vstack(preds_test), axis = 0)\ntest[\"log_visitors\"] = final_pred\nsample_submission = sample_submission.drop([\"log_visitors\"], axis = 1)\nsample_submission = sample_submission.merge(test[[\"log_visitors\", \"id\"]], on = \"id\")\nsample_submission.to_csv(\"submission.csv\", index = False)","ba538307":"## \u91cd\u8981\u306a\u9023\u7d9a\u5024\u7279\u5fb4\u91cf\u540c\u58eb\u306e\u5272\u308a\u7b97\uff0c\u5f15\u304d\u7b97\ndef div(df, c1, c2):\n    df[c1 + \"_!div!_\" + c2] = df[c1] \/ (df[c2] + 1e-2)\n    return df\ndef diff(df, c1, c2):\n    df[c1 + \"_!diff!_\" + c2] = df[c1] - df[c2]\n    return df\nfor df in [train, test]:\n    df = div(df, \"store_id_te\", \"genre_name_te\")\n    df = div(df, \"store_id_te\", \"area_name_te\")\n    df = div(df, \"store_id_te\", \"day_of_week_te\")\n    \n    df = diff(df, \"store_id_te\", \"genre_name_te\")\n    df = diff(df, \"store_id_te\", \"area_name_te\")\n    df = diff(df, \"store_id_te\", \"day_of_week_te\")","1d5fbc91":"## Submission","48ec7d72":"for c in train.columns:\n    print(c, len(train[c].unique()))","8ae5b7ef":"## Target encoding","167c436a":"## load","2a2ae841":"## Train, evaluate, and pred"}}