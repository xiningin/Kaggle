{"cell_type":{"d1ce5079":"code","b246238c":"code","5c503e24":"code","df06bd84":"code","65bd3720":"code","f197daae":"code","ee3852a7":"code","31662c3f":"code","bc39a3d8":"code","61828300":"code","cc6aa97f":"code","a087bfbf":"code","a702cae6":"code","a8a94b8f":"code","53c804c3":"code","b73b322e":"code","1d5baddc":"code","5030de25":"code","a0cd6a6d":"code","c157fd43":"code","0282bef8":"code","073510a5":"code","e23aadb1":"code","3accbe6b":"code","77b7535e":"code","a21e32cf":"code","b7404e92":"code","34ab4d8d":"code","6fca3590":"code","4bd4ee07":"code","c3d4fa63":"code","ac88694e":"code","70eccd39":"code","a1b39164":"markdown","e887d507":"markdown","e0ca6d63":"markdown","fde3dd30":"markdown","951f6f10":"markdown","902da056":"markdown","dd728bac":"markdown","355ea898":"markdown","11caab3b":"markdown","7b43e749":"markdown","9b7b43cd":"markdown"},"source":{"d1ce5079":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b246238c":"import numpy as np \nimport pandas as pd\nimport random\nfrom collections import Counter\nimport pprint\nimport time\nimport tensorflow as tf  # deep learning library. Tensors are just multi-dimensional arrays\nimport sys\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","5c503e24":"train = pd.read_csv(\"..\/input\/train.csv\")","df06bd84":"train.head()","65bd3720":"train.tail()","f197daae":"train.isnull().sum().sum()","ee3852a7":"print('Shapes')\nprint(\"TRAIN :\",train.shape)\n","31662c3f":"print(len(train[train[\"target\"]==0]), \" :  sincere questions\")\nprint(len(train[train[\"target\"]==1]), \" :  insincere questions\")","bc39a3d8":"SAMPLE_SIZE = 80000\ndf_0 = train[train[\"target\"]==0].sample(SAMPLE_SIZE, random_state = 101)\n    # filter out class 1\ndf_1 = train[train[\"target\"]==1].sample(SAMPLE_SIZE, random_state = 101)\n\ndf = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n   # shuffle\ndf= shuffle(df)\ndf_data, df_test = train_test_split(df, test_size=0.2)\n\n","61828300":"dictio = {0:\"sincere question  \",1:\"insincere question\"}\ndef pretty_print_text_and_label(i):\n    print(dictio[df_data.iloc[i][2]] + \"\\t:\\t\" + train.iloc[i][1][:80] + \"...\")","cc6aa97f":"print(\"type \\t : \\t question\\n\")\n# choose  a random spam set to analyse\n# random.randrange(start, stop, step)\npretty_print_text_and_label(random.randrange(0,4572))\npretty_print_text_and_label(random.randrange(0,4572,4))\npretty_print_text_and_label(random.randrange(0,4572,50))\npretty_print_text_and_label(random.randrange(0,4572,100))\npretty_print_text_and_label(random.randrange(0,4572,200))\npretty_print_text_and_label(random.randrange(0,4572,500))\npretty_print_text_and_label(random.randrange(0,4572,800))\npretty_print_text_and_label(random.randrange(0,4572,1000))","a087bfbf":"Sincere_counts = Counter()\nInsencere_counts = Counter()\nTotal_counts = Counter()\npp = pprint.PrettyPrinter(indent=4)","a702cae6":"for i in range(len(df_data)):  #range(len(train)):\n    if(df_data.iloc[i][2] == 0):\n        for word in df_data.iloc[i][1].split(\" \"):\n            Sincere_counts[word] += 1\n            Total_counts[word] += 1\n    else:\n        for word in df_data.iloc[i][1].split(\" \"):\n            Insencere_counts[word] += 1\n            Total_counts[word] += 1","a8a94b8f":"print(\"the most used word in insencere questions\")\npp.pprint(Insencere_counts.most_common()[0:20])","53c804c3":"print(\"the most used word in sencere questions\")\npp.pprint(Sincere_counts.most_common()[0:20])","b73b322e":"print(\"the most used word in all questions\")\npp.pprint(Sincere_counts.most_common()[0:20])","1d5baddc":"vocab = set(Total_counts.keys())\nvocab_size = len(vocab)\nprint(vocab_size)","5030de25":"vocab_vector = np.zeros((1, 200)) # np.zeros((1, vocab_size))\npp.pprint(vocab_vector.shape)\npp.pprint(vocab_vector)","a0cd6a6d":"word_column_dict = {}\np=0\nfor word,count in list(Total_counts.most_common()[0:200]):\n    # {key: value} is {word: column}\n    word_column_dict[word] = p\n    p+=1","c157fd43":"def update_input_layer(text):\n    \n    global vocab_vector\n    \n    # clear out previous state, reset the vector to be all 0s\n    vocab_vector *= 0\n    for word in text.split(\" \"):\n        if word in word_column_dict:\n            vocab_vector[0][word_column_dict[word]] += 1\n        \n    return vocab_vector.tolist()[0]\n\n# example \nprint(update_input_layer(\"the the\"))","0282bef8":"X_train = [update_input_layer(df_data.iloc[i][1]) for i in range(len(df_data))]","073510a5":"y_train = [df_data.iloc[i][2] for i in range(len(df_data))]","e23aadb1":"from keras import Sequential\nfrom keras.layers import Dense\nmodel = Sequential()  # a basic feed-forward model\n#model.add(tf.keras.layers.Flatten())  # takes our 28x28 and makes it 1x784\n\nmodel.add(Dense(128,input_dim=200, activation=tf.nn.relu))  # a simple fully-connected layer, 128 units, relu activation\nmodel.add(Dense(1, activation=tf.nn.sigmoid))  # our output layer. 10 units for 10 classes. Softmax for probability distribution\n\nmodel.compile(optimizer='adam',  # Good default optimizer to start with\n              loss='binary_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n              metrics=['accuracy'])  # what to track","3accbe6b":"model.summary()","77b7535e":"history = model.fit(np.asarray(X_train), np.asarray(y_train), epochs = 100)","a21e32cf":"# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","b7404e92":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","34ab4d8d":"df_test.head()","6fca3590":"X_test = [update_input_layer(df_test.iloc[i][1]) for i in range(len(df_test))]","4bd4ee07":"predictions = model.predict(np.asarray(X_test))","c3d4fa63":"k=0\nfor i in range(len(df_test)):\n    if(predictions[i][0]>0.5 and df_test.iloc[i,2]==1):\n        k+=1\n    elif(predictions[i][0]<0.5 and df_test.iloc[i,2]==0):\n        k+=1\n  \n","ac88694e":"print(k\/len(df_test)*100, \"% of testing data are well predicted\")  ","70eccd39":"print(\"type \\t : \\t question\\n\")\nfor d in range(5):\n    r = random.randrange(0,len(df_test))\n    s = \"sincere question  :\"\n    q = \"===> predicted ===> sincere question \"\n    if df_test.iloc[r,2]==1:\n        s = \"insincere question  :\"\n    if predictions[r][0]>0.5 : \n        q = \"===> predicted   ===> insincere question \"\n        \n    print(s+df_test.iloc[r,1]+q)\n","a1b39164":"**Build the SpamClassificationNeuralNetwork**","e887d507":"\nWe can see that from all our dataset, we have a total of 13874 unique words. Use this to build up our vocabulary vector containing columns of all these words.\n\nBecause, 139026, can be a large size in memory (a matrix of size 139026 by 8000), we will take in to account just the 100 most used words","e0ca6d63":"![](http:\/\/www.marketing-professionnel.fr\/wp-content\/uploads\/2011\/03\/quora-marketing.png)\n","fde3dd30":"Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out **insincere questions** -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.\nIn this competition,we  will develop models that identify and flag insincere questions. ","951f6f10":"Now, let's create a dictionary that allows us to look at every word in our vocabulary and map it to the vocab_vector column.","902da056":"**Quora**  is a question-and-answer website where questions are asked, answered, edited, and organized by its community of users in the form of opinions.","dd728bac":"Neural Networks only understand numbers hence we have to find a way to represent our text inputs in a way it can understand","355ea898":"**Transform Text into Numbers**\n","11caab3b":"**Importing libraries : **","7b43e749":"**Data loading :**","9b7b43cd":"**Cheking NaN values**"}}