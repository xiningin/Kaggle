{"cell_type":{"1392352d":"code","f54ffaf4":"code","42788196":"code","ec9ee04e":"code","0315f274":"code","636700c5":"code","59f966fb":"code","a1871e02":"code","859e66ce":"code","d6b73b0b":"code","5d7ee76c":"code","6618cb4a":"code","ff006ff8":"code","fcac9aac":"code","c73cec88":"code","4f86d20a":"code","1d46fcb9":"code","eb433f17":"code","443c19bc":"code","40d872b0":"code","7364d4b5":"markdown","468a04db":"markdown","3a429923":"markdown"},"source":{"1392352d":"# importing system libraries\n\nfrom os import walk\nfrom string import punctuation\nfrom random import shuffle\nfrom collections import Counter\n\n# importing additional libraries\n\nimport pandas as pd\nimport sklearn as sk\nimport nltk","f54ffaf4":"# Read the whole data from the Enron Dataset into a variable allData.\n\npathwalk = walk(r\"\/kaggle\/input\/enron-spam\/\")\n\nallHamData, allSpamData = [], []\nfor root, dr, file in pathwalk:\n    if 'ham' in str(file):\n        for obj in file:\n            with open(root + '\/' + obj, encoding='latin1') as ip:\n                allHamData.append(\" \".join(ip.readlines()))\n                \n    elif 'spam' in str(file):\n        for obj in file:\n            with open(root + '\/' + obj, encoding='latin1') as ip:\n                allSpamData.append(\" \".join(ip.readlines()))","42788196":"# remove all redundent data\n\nallHamData = list(set(allHamData))\nallSpamData = list(set(allSpamData))","ec9ee04e":"# storing it in a dataframe\n\nhamPlusSpamData = allHamData + allSpamData\nlabels = [\"ham\"]*len(allHamData) + [\"spam\"]*len(allSpamData)\n\nraw_df = pd.DataFrame({\"email\": hamPlusSpamData, \n                       \"label\": labels})","0315f274":"# checking how it looks\n\nraw_df.sample(5)","636700c5":"# get an overview of the data\n\nraw_df.label.hist(bins=3)","59f966fb":"# creating a preprocessing function\n# to tokenize and lemmatize the data using NLTK library\n\ndef preprocess(data):\n    # tokenization\n    tokens = nltk.word_tokenize(data)\n    tokens = [w.lower() for w in tokens if w.isalpha()]\n\n    # finding uncommon words\n    cnt = Counter(tokens)\n    uncommons = cnt.most_common()[:-int(len(cnt)*0.1):-1]\n    \n    # listing stopwords from NLTK\n    stops = set(nltk.corpus.stopwords.words('english'))\n\n    # removing stop words and uncommon words\n    tokens = [w for w in tokens if (w not in stops and w not in uncommons)]\n\n    # lemmatization\n    lemmatizer = nltk.WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(w, pos='a') for w in tokens]\n\n    return tokens","a1871e02":"# pre-processing the emails\n# using word_tokenize() and WordNetLemmatizer()\n\nnltk_processed_df = pd.DataFrame()\nnltk_processed_df['email'] = [preprocess(e) for e in raw_df.email]","859e66ce":"# label encoding the labels\n\nlabel_encoder = sk.preprocessing.LabelEncoder()\nnltk_processed_df['label'] = label_encoder.fit_transform(raw_df.label)","d6b73b0b":"# checking how the processed data looks like\n\nnltk_processed_df.sample(5)","5d7ee76c":"# converting categorical email data to numerical data using Counters\n\nX, y = nltk_processed_df.email, nltk_processed_df.label\nX_featurized = [Counter(i) for i in X]","6618cb4a":"# getting the data ready for NaiveBayesClassifier \n# randomizing using shuffle\n# manually splitting into test and train data\n\nallDataProcessed = [(X_featurized[i], y[i]) for i in range(len(X))]\n\nshuffle(allDataProcessed)\n\ntrainData, testData = allDataProcessed[:int(len(allDataProcessed)*0.7)], allDataProcessed[int(len(allDataProcessed)*0.7):]","ff006ff8":"# Training the model\n\nmodel_nltkNaiveBayes = nltk.classify.NaiveBayesClassifier.train(trainData)","fcac9aac":"# Testing the model\n\ntesting_accuracy = nltk.classify.accuracy(model_nltkNaiveBayes, testData)\nprint(\"Accuracy with NLTK's Naive Bayes classifier is:\", testing_accuracy)","c73cec88":"# Vectorize the features using CountVectorizer\n\ncv_vec = sk.feature_extraction.text.CountVectorizer(tokenizer = nltk.word_tokenize, \n                                                    stop_words = nltk.corpus.stopwords.words(\"english\"))\n\ncv_X = cv_vec.fit_transform(raw_df.email)","4f86d20a":"# Vectorize the features using TfidfVectorizer\n\ntfidf_vec = sk.feature_extraction.text.TfidfVectorizer(tokenizer = nltk.word_tokenize, \n                                                    stop_words = nltk.corpus.stopwords.words(\"english\"))\n\ntdidf_X = cv_vec.fit_transform(raw_df.email)","1d46fcb9":"# label encode the labels using LabelEncoder\n\nlabel_encoder = sk.preprocessing.LabelEncoder()\ny = label_encoder.fit_transform(raw_df.label)","eb433f17":"# loading the MultinomialNB model\n\nfrom sklearn.naive_bayes import MultinomialNB\nmodel_sklearn_mnb = MultinomialNB()","443c19bc":"# getting cross validation score on count-vectorized features\n# getting cross validation score on tfidf processed features\n\ncv_score = sk.model_selection.cross_validate(model_sklearn_mnb, cv_X, y)\n\ntfidf_score = sk.model_selection.cross_validate(model_sklearn_mnb, tdidf_X, y)","40d872b0":"# checking the scores by putting them into a dataframe first\n\nsklearn_scores = pd.DataFrame([cv_score, tfidf_score], index=['CountVetorizer', 'TfidfVectorizer'])\nsklearn_scores = sklearn_scores.applymap(lambda x: x.mean())\n\nsklearn_scores","7364d4b5":"# NLTK's Naive Bayes Classifier\n### with custom text pre-processor using NLTK's word_tokenize and WordNetLemmatizer","468a04db":"# Spam Email Classification Model","3a429923":"# Scikit-learn's Multinomial Naive Bayes Classifier\n### with CountVectorizer and TfidfVectorizer comparison"}}