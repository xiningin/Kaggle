{"cell_type":{"dbfd54af":"code","6a56601f":"code","196677bc":"code","380942d3":"code","fdabe1b5":"code","62c6faa1":"code","7bb1a617":"code","b0761ce1":"code","1a4b7d43":"code","45686345":"code","d00d6e3d":"code","d21a1ffc":"code","f5ad9812":"code","8678a8ab":"code","2847c602":"code","f0b3fdf6":"code","6c4afddf":"code","160cc06a":"code","39b62c16":"code","6df6b33c":"code","007dc1a3":"code","39a4207a":"code","7513f45b":"code","98613076":"code","6e23d959":"code","e3c62f6d":"code","34892544":"code","eac1ebac":"code","24bd9311":"code","c94047e9":"code","5ac916f9":"code","f5176f4d":"code","37d77ec8":"code","8f8dbaef":"markdown","dd8005d7":"markdown","b17fe65b":"markdown","5869b61c":"markdown","acfc222c":"markdown","db04e832":"markdown","3a51ad9d":"markdown","805684de":"markdown","7c537349":"markdown","9d53279c":"markdown","1f6fa6b5":"markdown","560323f3":"markdown","45b036b6":"markdown","2a5a8500":"markdown","99ab0ce6":"markdown","1acc1ec5":"markdown"},"source":{"dbfd54af":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","6a56601f":"df = pd.read_csv('..\/input\/house-price-tehran-iran\/housePrice.csv')","196677bc":"df.head()","380942d3":"df['CategoricalAddress'] = df['Address']\nAddress = pd.unique(df['Address'])\nAddress = Address.tolist()\nAddressDict = {}\nfor i in Address:\n    AddressDict[i] = Address.index(i)\n\ndf = df.replace({'Address':AddressDict})\ndf.head()","fdabe1b5":"sns.heatmap(data=df.corr())","62c6faa1":"X = df.drop(['Price','Price(USD)','Area','CategoricalAddress'], axis=1)\ny = df['Price(USD)']","7bb1a617":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\ntrain_RMSE_list = []\ntest_RMSE_list = []\n\nfor d in range(1,10):\n    \n    polynomial_converter = PolynomialFeatures(degree=d , include_bias= False)\n    polynomial_features = polynomial_converter.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(polynomial_features, y, test_size=0.3, random_state=101)\n    polymodel = LinearRegression()\n    polymodel.fit(X_train, y_train)\n    \n    y_train_pred = polymodel.predict(X_train)\n    y_test_pred = polymodel.predict(X_test)\n    \n    train_RMSE = np.sqrt(mean_squared_error(y_train,y_train_pred))\n    test_RMSE = np.sqrt(mean_squared_error(y_test,y_test_pred))\n    \n    train_RMSE_list.append(train_RMSE)\n    test_RMSE_list.append(test_RMSE)","b0761ce1":"train_RMSE_list","1a4b7d43":"test_RMSE_list","45686345":"plt.plot(range(1,6), train_RMSE_list[:5], label= 'Train RMSE')\nplt.plot(range(1,6), test_RMSE_list[:5], label= 'Test RMSE')\n\nplt.xlabel('polynomian Degree')\nplt.ylabel('RMSE')\nplt.legend()","d00d6e3d":"polynomial_converter = PolynomialFeatures(degree=2 , include_bias= False)\npolynomial_features = polynomial_converter.fit_transform(X)\npolynomial_features.shape","d21a1ffc":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(polynomial_features, y, test_size=0.3, random_state=101)","f5ad9812":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)","8678a8ab":"X_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","2847c602":"X_train","f0b3fdf6":"X_test","6c4afddf":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","160cc06a":"X_train","39b62c16":"X_test","6df6b33c":"from sklearn.linear_model import LassoCV","007dc1a3":"lasso_cv_model = LassoCV(eps=0.01, n_alphas=50, cv=5)\nlasso_cv_model.fit(X_train,y_train)\nlasso_cv_model.alpha_","39a4207a":"from sklearn.metrics import mean_squared_error,mean_absolute_error\ny_pred_lasso = lasso_cv_model.predict(X_test)\nMAE = mean_absolute_error(y_test,y_pred_lasso)\nMSE = mean_squared_error(y_test, y_pred_lasso)\nRMSE = np.sqrt(MSE)","7513f45b":"pd.DataFrame({'Ridge Metrics':[MAE,MSE,RMSE]},index=['MAE','MSE','RMSE'])","98613076":"lasso_cv_model.coef_","6e23d959":"from sklearn.linear_model import RidgeCV\nridge_cv_model = RidgeCV(alphas = (0.1,1.0,10), scoring='neg_mean_absolute_error')\nridge_cv_model.fit(X_train,y_train)\nridge_cv_model.alpha_","e3c62f6d":"from sklearn.metrics import mean_squared_error,mean_absolute_error\ny_pred_ridge = ridge_cv_model.predict(X_test)\nMAE = mean_absolute_error(y_test,y_pred_ridge)\nMSE = mean_squared_error(y_test, y_pred_ridge)\nRMSE = np.sqrt(MSE)","34892544":"pd.DataFrame({'Ridge Metrics':[MAE,MSE,RMSE]},index=['MAE','MSE','RMSE'])","eac1ebac":"ridge_cv_model.coef_","24bd9311":"from sklearn.linear_model import ElasticNetCV","c94047e9":"elastic_model = ElasticNetCV(l1_ratio=[.1, .5, .7,.9, .95, .99, 1], cv=5, max_iter=100000)\nelastic_model.fit(X_train, y_train)","5ac916f9":"y_pred_elastic = elastic_model.predict(X_test)\nMAE = mean_absolute_error(y_test,y_pred_elastic)\nMSE = mean_squared_error(y_test, y_pred_elastic)\nRMSE = np.sqrt(MSE)","f5176f4d":"pd.DataFrame({'Ridge Metrics':[MAE,MSE,RMSE]},index=['MAE','MSE','RMSE'])","37d77ec8":"elastic_model.coef_","8f8dbaef":"### Regularization can be done by three way:\n- L1 -> Lasso Regression\n- L2 -> Ridge Regression\n- Combining L1 and L2 -> Elastic Net","dd8005d7":"# Feature Scaling","b17fe65b":"# Split the data to train and test","5869b61c":"# L2: Ridge Regression","acfc222c":"# L1: Lasso Regression ","db04e832":"# Regularization","3a51ad9d":"### feature scaling has two benefits:\n- can lead to great increases in performance\n- Absolutely necessery for some models\n\n#### scaling the data can takes place with two ways\n\n- Standardization\n- Normalization\n","805684de":"# Combines L1 and L2: Elastic Net","7c537349":"## this plot can helps us to find best degree","9d53279c":"# Now it's better to Adjusting Model Parameters","1f6fa6b5":"# Determine the fetures and lables","560323f3":"### but i want to use the crosse validation\n##### cross validation is a more advanced set of methods of spliting\n##### data into training and testing sets\n\n- by cross validation we can **fit 100 percent of data** \n  and **test 100 percent of data**.","45b036b6":"#### I want to regular the data because:\n\n- Minimizing model complexity\n- Penalizing the loss function\n- Reducing model overfiting","2a5a8500":"### 1- Standardization","99ab0ce6":"# Polynomial Feature","1acc1ec5":"### 2- Normalization"}}