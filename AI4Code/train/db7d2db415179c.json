{"cell_type":{"bfb2af3e":"code","1be74ada":"code","2167a563":"code","c6d708f2":"code","a59059a2":"code","d56e7407":"code","bb497d74":"code","24a1d27c":"code","cec5435e":"code","2b542d58":"code","58a8502d":"code","89e2b3a7":"code","8d45f514":"code","14da9872":"code","cc9ca057":"code","687abf66":"code","a7a066f6":"code","0d8003ab":"code","07a81f73":"code","0c82770b":"code","741c9088":"code","8378ef53":"code","4453f3e4":"code","93d3d69c":"code","1c92a317":"code","9d4cb21b":"code","38e354d5":"code","e9957945":"code","1aeee59a":"code","0ea46f6a":"code","f53b6d23":"code","74c051f7":"code","b9d68ad4":"code","2bf52bc9":"code","21f840e4":"code","8357faa1":"code","5666a5df":"code","d9a4695e":"code","881281b4":"code","0e084812":"code","74ccb5aa":"code","812da7a0":"code","10b5eb71":"code","dc52d7df":"code","7a3e1eef":"code","2ca029f4":"code","27c917f5":"code","43564a73":"code","678e23c8":"code","e9b0f693":"code","d133f6ab":"code","314800ae":"code","6cd68508":"code","0605ddb2":"code","746a4335":"code","28ce1830":"code","2c9306aa":"code","ef7a83ab":"code","1360fafc":"code","866eb182":"code","331c639e":"code","17afbbd8":"code","e38efa0b":"code","3e4b2a3e":"code","9fa148f1":"code","56caa216":"code","136841a4":"code","76177458":"code","c34e3529":"code","b6a6f924":"code","52c3ff25":"code","de14fce7":"code","253d1d4e":"code","12f7e5ea":"code","e4813bca":"code","ff3b4a20":"code","9f085405":"code","1f4d50f9":"code","8e455434":"markdown","55d0b4e6":"markdown","0fac4340":"markdown","40b47dda":"markdown","2d050c9d":"markdown","f9c11ee1":"markdown","dcbc989f":"markdown","825dfc1d":"markdown","f43319d9":"markdown","86896383":"markdown","91cd9cb8":"markdown","d424907c":"markdown","92274005":"markdown","5b79a5ce":"markdown","9389a034":"markdown","6cde0b57":"markdown","bcc92864":"markdown","2d4764c8":"markdown","5c588972":"markdown","74714108":"markdown","a316461c":"markdown","29aa939b":"markdown","4716b6b3":"markdown","5f053fae":"markdown","f1bf0c8c":"markdown","e7565fc3":"markdown","4422948b":"markdown","ebfa0653":"markdown","39d1cfef":"markdown","b8ef018d":"markdown","7590626f":"markdown","89f05509":"markdown","bb727320":"markdown","db531e50":"markdown","2db537c1":"markdown","6763f4a8":"markdown","e987fb68":"markdown","5e79fdc5":"markdown","cd0db5ce":"markdown","8fbc60cc":"markdown","777b5aae":"markdown","6b0c848d":"markdown","02d4d13d":"markdown"},"source":{"bfb2af3e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1be74ada":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visiualization\nimport seaborn as sns # data visisualization like distribytion chart, matrix plot, heat maps\nimport sklearn # scikit library for machine learning\n\n###########","2167a563":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing,metrics,manifold\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,cross_val_predict\nfrom imblearn.over_sampling import ADASYN,SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nimport collections\nimport matplotlib.patches as mpatches\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\nfrom sklearn.preprocessing import RobustScaler\nimport xgboost\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD,SparsePCA\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\nfrom collections import Counter\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","c6d708f2":"train = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')\ntest = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv')\nsample_submission=pd.read_csv('..\/input\/quora-insincere-questions-classification\/sample_submission.csv')","a59059a2":"print(train.columns)\nprint(train.info)\nprint(train.head())\nprint(train.describe())","d56e7407":"train['question_text'].str.len().hist() #number of characters present in each sentence","bb497d74":"train['question_text'].str.split().map(lambda x: len(x)).hist()","24a1d27c":"train['question_text'].str.split().apply(lambda x : [len(i) for i in x]). map(lambda x: np.mean(x)).hist() ","cec5435e":"insincere=train[train['target']==1]\n\nsincere=train[train['target']==0]\n\nprint(\"First 10 samples of insincere questions\\n\".format(),insincere[:10])\n\nprint(\"First 10 samples of sincere \\n\".format(),sincere[:10])","2b542d58":"count=train['target'].value_counts()\n\nprint('Total Counts of both sets'.format(),count)\n\nprint(\"==============\")\n\n#Creating a function to plot the counts using matplotlib\ndef plot_counts(count_good,count_bad):\n    plt.rcParams['figure.figsize']=(6,6)\n    plt.bar(0,count_good,width=0.6,\n            label='Sincere Questions',\n            color='Indigo')\n    plt.legend()\n    plt.bar(2,count_bad,width=0.6,\n            label='Insincere Questions',\n            color='Red')\n    plt.legend()\n    plt.ylabel('Count of Questions')\n    plt.xlabel('Types of Questions')\n    plt.show()","58a8502d":"count_good=train[train['target']==0]\ncount_bad=train[train['target']==1]\nplot_counts(len(count_good),len(count_bad))","89e2b3a7":"#Function for checking word length\ndef cal_len(data):\n    return len(data)\n\n#Create generic plotter with Seaborn\ndef plot_count(count_ones,count_zeros,title_1,title_2,subtitle):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(count_zeros,ax=ax1,color='Blue')\n    ax1.set_title(title_1)\n    sns.distplot(count_ones,ax=ax2,color='Red')\n    ax2.set_title(title_2)\n    fig.suptitle(subtitle)\n    plt.show()   ","8d45f514":"count_sincere=count_good['question_text'].str.split().apply(lambda z:cal_len(z))\ncount_insincere=count_bad['question_text'].str.split().apply(lambda z:cal_len(z))\nprint(\"Sincere Questions:\" + str(count_sincere))\nprint(\"Insincere Questions:\" + str(count_insincere))\nplot_count(count_sincere,count_insincere,\"Sincere Questions\",\n           \"Insincere Questions\",\"Questions Analysis\")","14da9872":"#We will be using the \"generic_plotter\" function.\n\ncount_sincere_punctuations=count_good['question_text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\n\ncount_insincere_punctuations=count_bad['question_text'].apply(lambda z:len([c for c in str(z) if c in string.punctuation]))\n\nplot_count(count_sincere_punctuations,count_insincere_punctuations,\"Sincere Questions Punctuations\",\"Insincere Questions Punctuations\",\"Questions Punctuation Analysis\")","cc9ca057":"def plot_count_1(count_ones,count_zeros,title_1,title_2,subtitle):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(count_zeros,ax=ax1,color='Blue')\n    ax1.set_title(title_1)\n    sns.distplot(count_ones,ax=ax2,color='Orange')\n    ax2.set_title(title_2)\n    fig.suptitle(subtitle)\n    plt.show()    ","687abf66":"stops=set(stopwords.words('english'))\n\ncount_sincere_stops=count_good['question_text'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\n\ncount_insincere_stops=count_bad['question_text'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\n\nplot_count_1(count_sincere_stops,count_insincere_stops,\"Sincere Questions Stopwords\",\"Insincere Questions Stopwords\",\"Questions Stopwords Analysis\")","a7a066f6":"def display_cloud(data,color,figsize):\n    plt.subplots(figsize=figsize)\n    wc = WordCloud(stopwords=STOPWORDS,\n                   background_color=\"white\", \n                   contour_width=2, \n                   contour_color=color,\n                   max_words=2000, \n                   max_font_size=256,\n                   random_state=42)\n    wc.generate(' '.join(data))\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()","0d8003ab":"display_cloud(train['question_text'],\n              color='red',\n              figsize=(5,5)) #WordCloud for the insincere questions","07a81f73":"display_cloud(sincere['question_text'],\n              color='red',\n              figsize=(5,5)) #WordCloud for the sincere questions","0c82770b":"display_cloud(insincere['question_text'],\n              color='red',\n              figsize=(5,5)) #WordCloud for the insincere questions","741c9088":"import nltk\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))","8378ef53":"#create the corpus.\n\ncorpus=[]\nquestion_text = train['question_text'].str.split()\nquestion_text= question_text.values.tolist()\ncorpus=[word for i in question_text for word in i]","4453f3e4":"from collections import defaultdict\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1","93d3d69c":"counter= Counter(corpus)\nmost = counter.most_common()\n\nx, y= [], []\nfor word,count in most[:40]:\n    if (word not in stop):\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x)","1c92a317":"#Ngram exploration\nfrom nltk.util import ngrams","9d4cb21b":"def get_top_ngram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) \n                  for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:10]","38e354d5":"insincere_label=train[train['target']== 1]['question_text']\nprint(insincere_label.head())  #top insincere questions\n\n\nsincere_label=train[train['target']== 0]['question_text']\nprint(sincere_label.head())  #top sincere questions","e9957945":"bigram_insincere=get_top_ngram(insincere_label,n=2)\nx,y=map(list,zip(*bigram_insincere))\nsns.barplot(x=y,y=x)","1aeee59a":"bigram_sincere=get_top_ngram(sincere_label,n=2)\nx,y=map(list,zip(*bigram_sincere))\nsns.barplot(x=y,y=x)","0ea46f6a":"trigram_insincere=get_top_ngram(insincere_label,n=3)\nx,y=map(list,zip(*trigram_insincere))\nsns.barplot(x=y,y=x)","f53b6d23":"trigram_sincere=get_top_ngram(sincere_label,n=3)\nx,y=map(list,zip(*trigram_sincere))\nsns.barplot(x=y,y=x)","74c051f7":"pentagram_insincere=get_top_ngram(insincere_label,n=5)\nx,y=map(list,zip(*pentagram_insincere))\nsns.barplot(x=y,y=x)","b9d68ad4":"pentagram_sincere=get_top_ngram(sincere_label,n=5)\nx,y=map(list,zip(*pentagram_sincere))\nsns.barplot(x=y,y=x)","2bf52bc9":"%%time\nimport re\n#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data","21f840e4":"train['question_text']=train['question_text'].apply(lambda z: remove_punctuations(z))","8357faa1":"#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data","5666a5df":"train['question_text']=train['question_text'].apply(lambda z: remove_html(z))","d9a4695e":"#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data","881281b4":"train['question_text']=train['question_text'].apply(lambda z: remove_url(z))","0e084812":"#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data","74ccb5aa":"train['question_text']=train['question_text'].apply(lambda z: remove_emoji(z))","812da7a0":"train['question_text'][:5]","10b5eb71":"#Lemmatize the dataset\nfrom nltk.stem import WordNetLemmatizer\n\n\ndef lemma_traincorpus(data):\n    lemmatizer=WordNetLemmatizer()\n    out_data=\"\"\n    for words in data:\n        out_data+= lemmatizer.lemmatize(words)\n    return out_data\n\ntrain['question_text']=train['question_text'].apply(lambda z: lemma_traincorpus(z))\n\ntrain['question_text'].sample()","dc52d7df":"from nltk.stem import *\n\ndef stem_traincorpus(data):\n    stemmer = PorterStemmer()\n    out_data=\"\"\n    for words in data:\n        out_data+= stemmer.stem(words)\n    return out_data\n\ntrain['question_text']=train['question_text'].apply(lambda z: stem_traincorpus(z))\n\ntrain['question_text'].sample()","7a3e1eef":"%%time\ntfidf_vect=TfidfVectorizer(stop_words='english',\n                           ngram_range=(1,3))\n\ntrain_tfidf=tfidf_vect.fit_transform(train['question_text'].values.tolist())\n\ntrain_tfidf.shape","2ca029f4":"print(train_tfidf)","27c917f5":"from collections import Counter\nfrom tqdm import tqdm\nfrom scipy.sparse import csr_matrix\nimport math\nimport operator\nfrom sklearn.preprocessing import normalize\nimport numpy as np ","43564a73":"#Count Vectorization\nimport matplotlib\nimport matplotlib.pyplot as plt\n\ndef vectorize(data):\n    cv=CountVectorizer()\n    fit_data_cv=cv.fit_transform(data)\n    return fit_data_cv,cv","678e23c8":"#Tfidf vectorization from sklearn\ndef tfidf(data):\n    tfidfv=TfidfVectorizer()\n    fit_data_tfidf=tfidfv.fit_transform(data)\n    return fit_data_cv,tfidfv","e9b0f693":"def dimen_reduc_plot(test_data,\n                     test_label,\n                     option):\n    tsvd= TruncatedSVD(n_components=2,\n                       algorithm=\"randomized\",\n                       random_state=42)\n    tsne=TSNE(n_components=2,\n              random_state=42) #not recommended instead use PCA\n    pca=SparsePCA(n_components=2,\n                  random_state=42)\n    if(option==1):\n        tsvd_result=tsvd.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        \n        sns.scatterplot(x=tsvd_result[:,0],\n                        y=tsvd_result[:,1],\n                        hue=test_label        )\n        \n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(tsvd_result[:,0],\n                    tsvd_result[:,1],\n                    c=test_label,\n                    cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',\n                                 label='Insincere Questions')\n        color_orange=mpatches.Patch(color='orange',\n                                    label='Sincere Questions')\n        plt.legend(handles=[color_orange,\n                            color_red])\n        plt.title(\"TSVD\")\n        plt.show()\n    if(option==2):\n        tsne_result=tsne.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        sns.scatterplot(x=tsne_result[:,0],\n                        y=tsne_result[:,1],\n                        hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=tsne_result[:,0],\n                    y=tsne_result[:,1],\n                    c=test_label,\n                    cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',\n                                 label='Insincere Questions')\n        color_orange=mpatches.Patch(color='orange',\n                                    label='Sincere Questions')\n        plt.legend(handles=[color_orange,\n                            color_red])\n        plt.title(\"PCA\")\n        plt.show() \n    if(option==3):\n        pca_result=pca.fit_transform(test_data.toarray())\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        sns.scatterplot(x=pca_result[:,0],\n                        y=pca_result[:,1],\n                        hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=pca_result[:,0],\n                    y=pca_result[:,1],\n                    c=test_label,\n                    cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='Insincere Questions')\n        color_orange=mpatches.Patch(color='orange',\n                                    label='Sincere Questions')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"TSNE\")\n        plt.show()","d133f6ab":"train_data=train       \n\ndata_vect=train_data['question_text'].values\n\ndata_vect_good=count_good['question_text'].values\n\ntarget_vect=train_data['target'].values\n\ntarget_data_vect_good=train[train['target']==0].values\n\ndata_vect_bad=count_bad['question_text'].values\n\ntarget_data_vect_bad=train[train['target']==1].values\n\ntrain_data_cv,cv= vectorize(data_vect)\n\nreal_review_train_data_cv,cv=vectorize(data_vect_good)\n\nprint(train_data.head())\n\ndimen_reduc_plot(train_data_cv,target_vect,1)\n\ndimen_reduc_plot(real_review_train_data_cv,target_data_vect_good,1)\n\ndimen_reduc_plot(real_review_train_data_cv,target_data_vect_bad,1)\n\n# dimen_reduc_plot(train_data_cv,target_vect,3)\n# dimen_reduc_plot(real_review_train_data_cv,target_data_vect_good,3)\n# dimen_reduc_plot(train_data_cv,target_vect,2)\n# dimen_reduc_plot(real_review_train_data_cv,target_data_vect_good,2)","314800ae":"#TSNE visualization on first 1000 samples\ntrain_data=train[:1000]       \n\ndata_vect=train_data['question_text'].values\n\ndata_vect_good=count_good['question_text'].values\n\ntarget_vect=train_data['target'].values\n\ntarget_data_vect_good=train[train['target']==1].values\n\ndata_vect_bad=count_bad['question_text'].values\n\ntarget_data_vect_bad=train[train['target']==1].values\n\ntrain_data_cv,cv= vectorize(data_vect)\n\nreal_review_train_data_cv,cv=vectorize(data_vect_good)\n\ndimen_reduc_plot(train_data_cv,target_vect,3)","6cd68508":"check_df=list(train['question_text'].str.split())","0605ddb2":"%%time\n## Load word2vec algorithm from gensim\nfrom gensim.models import Word2Vec,KeyedVectors","746a4335":"model=Word2Vec(check_df,min_count=1)\nword_li=list(model.wv.vocab)\nprint(word_li[:5])","28ce1830":"#View the Tensor\nprint(model)\nprint(model['questions'])","2c9306aa":"##save the modeled words produced from Word2Vec\nmodel.save('word2vec_model.bin')\nloaded_model=KeyedVectors.load('word2vec_model.bin')\nprint(loaded_model)","ef7a83ab":"#View the Embedding Word Vector\nplt.plot(model['questions'])\nplt.show()","1360fafc":"train.sample()","866eb182":"#Measure Cosine distance\ndistance=model.similarity('questions','insincere')\nprint(distance)","331c639e":"# PCA transform in 2D for visualization of embedded words\nfrom matplotlib import pyplot\n\npca = PCA(n_components=2)\n\ntransformation_model=loaded_model[loaded_model.wv.vocab]\n\nresult = pca.fit_transform(transformation_model[:50])","17afbbd8":"# create a scatter plot of the projection\n\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(loaded_model.wv.vocab)\nfor i, word in enumerate(words[:50]):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()","e38efa0b":"#Using Google News Embeddings For our corpus\ngoogle_news_embed='..\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin'\ngoogle_loaded_model=KeyedVectors.load_word2vec_format(google_news_embed,binary=True)\nprint(google_loaded_model)","3e4b2a3e":"#Visualize the Word Vectors\nplt.plot(google_loaded_model['questions'])\nplt.plot(google_loaded_model['insincere'])\nplt.show()","9fa148f1":"from matplotlib import pyplot","56caa216":"pca = PCA(n_components=2)\n\ntransformation_model=google_loaded_model[google_loaded_model.wv.vocab]\n\nresult = pca.fit_transform(transformation_model[:50])\n\n# create a scatter plot of the projection\npyplot.scatter(result[:, 0], result[:, 1])\n\nwords = list(google_loaded_model.wv.vocab)\n\nfor i, word in enumerate(words[:50]):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()","136841a4":"from gensim.scripts.glove2word2vec import glove2word2vec\n\nglove_file='..\/input\/nlpword2vecembeddingspretrained\/glove.6B.50d.txt'\nword2vec_output_file = 'glove.6B.100d.txt.word2vec'\nglove_loaded=glove2word2vec(glove_file, word2vec_output_file)\nprint(glove_loaded)","76177458":"import matplotlib.pyplot as plt","c34e3529":"glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\nplt.plot(glove_model['questions'])\nplt.plot(glove_model['insincere'])\nplt.show()","b6a6f924":"# PCA transform in 2D for visualization of glove embedded words\nfrom matplotlib import pyplot\npca = PCA(n_components=2)\n\ntransformation_model=glove_model[glove_model.wv.vocab]\nresult = pca.fit_transform(transformation_model[:50])\n\n# create a scatter plot of the projection\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(glove_model.wv.vocab)\nfor i, word in enumerate(words[:50]):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()","52c3ff25":"from gensim.models import Word2Vec,KeyedVectors\n\nfasttext_file=\"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\"\nprint(fasttext_file)","de14fce7":"fasttext_model = KeyedVectors.load_word2vec_format(fasttext_file, binary=False)\nplt.plot(fasttext_model['questions'])\nplt.plot(fasttext_model['insincere'])\nplt.show()","253d1d4e":"# PCA transform in 2D for visualization of glove embedded words\nfrom matplotlib import pyplot\npca = PCA(n_components=2)\n\ntransformation_model=fasttext_model[fasttext_model.wv.vocab]\nresult = pca.fit_transform(transformation_model[:50])\n\n# create a scatter plot of the projection\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(fasttext_model.wv.vocab)\nfor i, word in enumerate(words[:50]):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()","12f7e5ea":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer","e4813bca":"#Creating Embedding Matrix\nmaxlen=1000\nmax_features=5000\nembed_size=300\n\ntrain_sample=train['question_text']","ff3b4a20":"#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_sample))\ntrain_sample=tokenizer.texts_to_sequences(train_sample)","9f085405":"#Pad the sequence- To allow same length for all vectorized words\ntrain_sample=pad_sequences(train_sample,maxlen=maxlen)\n\nEMBEDDING_FILE = '..\/input\/wikinews300d1msubwordvec\/wiki-news-300d-1M-subword.vec'","1f4d50f9":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\nplt.plot(embedding_matrix[20])\nplt.show()","8e455434":"## Inference From Exploratory Analysis\n\nThe following can be inferred from the data:\n\nThe dataset is imbalanced.\nThe dataset contains redundant words and html syntaxes.\nPunctuations\/stopwords are present in a equal distribution in the dataset.\nThis tells us that we have to do lots of cleaning!","55d0b4e6":"Removing punctuations","0fac4340":"# Visualization of most occuring words through word cloud","40b47dda":"## TSNE visualization","2d050c9d":"## N-grams","f9c11ee1":"### Average word length in each sentence","dcbc989f":"Bigram sincere questions","825dfc1d":"# Data Cleaning\n\nGetting rid of \n1. HTML codes\n2. URLs\n3. Emojis\n4. Stopwords\n5. Punctuations\n6. Expanding Abbreviations","f43319d9":"Stemming","86896383":"## Analyse the count of words in each segment- both positive and negative reviews","91cd9cb8":"## Analyse Stopwords","d424907c":"# Transforming corpus","92274005":"Removing URL","5b79a5ce":"# Dimension reduction of the embedding vectors","9389a034":"Pentagram sincere questions","6cde0b57":"   \n# Working with Glove Embeddings","bcc92864":"## Measuring insincere and sincere questions","2d4764c8":"## Frequency count of sincere and insincere question texts:","5c588972":"Lemmantization","74714108":"### Number of words appearing in each line.","a316461c":"## Visualizing the Vector Space","29aa939b":"## Using Fasttext","4716b6b3":"## TF-IDF","5f053fae":"Pentagram insincere questions","f1bf0c8c":"## Count Punctuations\/Stopwords\/Codes and other semantic datatypes","e7565fc3":"## Trigrams","4422948b":"# Vectorization and Embeddings","ebfa0653":"Removing Emojis","39d1cfef":"## Building corpus","b8ef018d":"## Compeletion of static embeddings","7590626f":"Trigram sincere questions","89f05509":"Trigram insincere questions","bb727320":"## Using Keras","db531e50":"# Semantic Embeddings","2db537c1":"# 1. Data import","6763f4a8":"## Pentagram","e987fb68":"   # 2.Descriptives","5e79fdc5":"## Exploratory data analysis","cd0db5ce":"Bigram Insincere Questions","8fbc60cc":"## occurrences of each word in a list of tuples","777b5aae":"## PCA transform in 2D for visualization of google news embedded words","6b0c848d":"## bigrams","02d4d13d":"## Dimensionality reduction"}}