{"cell_type":{"c196cc36":"code","1f6e96eb":"code","54b3032e":"code","b5bb8c70":"code","9225fe3c":"code","82fe777a":"code","72fa72f3":"code","45c49dfd":"code","478871fb":"code","75fb241f":"code","fa50cd0b":"code","a6b3bc5c":"code","e4dd4131":"code","55061efb":"code","a0fdb347":"code","65f9f084":"code","2c9a1642":"code","7a40362e":"code","090f0949":"code","dc88b95f":"code","9bedbbff":"code","62084951":"code","032451fd":"code","75ab10c2":"code","6b51ae1e":"code","4823f56a":"markdown","64a90020":"markdown","6fb900ef":"markdown","b04a0a1e":"markdown"},"source":{"c196cc36":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport matplotlib\nimport seaborn as sns\n\n#Preprocessing\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n#Model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.metrics import accuracy_score","1f6e96eb":"#import the data and shape\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nprint(train.shape,test.shape)\n","54b3032e":"#add extra one columns\ntrain['kfold']=-1\n\nkfold = model_selection.KFold(n_splits=15, shuffle= True, random_state = 42)\n\nfor fold, (train_indicies, valid_indicies) in enumerate(kfold.split(X=train)):\n    print(fold,train_indicies,valid_indicies)\n    ","b5bb8c70":"for fold, (train_indicies, valid_indicies) in enumerate(kfold.split(X=train)):\n    train.loc[valid_indicies,'kfold'] = fold","9225fe3c":"train.describe(include = \"all\")\n","82fe777a":"print(train.kfold.value_counts())","72fa72f3":"print(train.info())","45c49dfd":"#using isnull() method\ntrain.isnull().sum()","478871fb":"#using sns heatmap \nsns.heatmap(train.isnull(), cbar = False)","75fb241f":"train.corr()","fa50cd0b":"train.drop_duplicates()","a6b3bc5c":"num_cols  = [col for col in train.columns if train[col].dtype in ['int64', 'float64']]\nnum_cols.remove(\"target\")\nnum_cols.remove(\"id\")\nnum_cols.remove(\"kfold\")\nnum_cols","e4dd4131":"#Now plot a boxplot to identify the outliers in our numerical features.\nsns.boxplot(data = train[num_cols], orient = 'h', palette = 'Set3', linewidth = 2.5 )\nplt.title(\"Numerical Features Box Plot\")","55061efb":"sns.boxplot(x = train[\"target\"], orient = 'h', linewidth = 2.5 )\nplt.title(\"Target Column Box Plot\")","a0fdb347":"#Remove the outliers\ndef removeoutliers(df=None, columns=None):\n    for column in columns:\n        Q1 = df[column].quantile(0.25)\n        Q3 = df[column].quantile(0.75)\n        IQR = Q3 - Q1\n        floor, ceil = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n        df[column] = df[column].clip(floor, ceil)\n        print(f\"The columnn: {column}, has been treated for outliers.\\n\")\n    return df\nout_cols = [\"cont0\",\"cont6\",\"cont8\",\"target\"]\ntrain = removeoutliers(train,out_cols)","65f9f084":"#Now plot a boxplot to identify the outliers in our numerical features.\nsns.boxplot(data = train[num_cols], orient = 'h', palette = 'Set3', linewidth = 2.5 )\nplt.title(\"Numerical Features Box Plot\")","2c9a1642":"sns.boxplot(x = train[\"target\"], orient = 'h', linewidth = 2.5 )\nplt.title(\"Target Column Box Plot\")","7a40362e":"# Select categorical columns \ncat_cols = [cname for cname in train.columns if\n                    train[cname].dtype == \"object\"]\ncat_cols","090f0949":"useful_features = cat_cols + num_cols\nuseful_features","dc88b95f":"test = test[useful_features]\ntest","9bedbbff":"xtrain= None\nxvalid= None\nxtest= None\nytrain = None\nyvalid = None\n\nfor fold in range(10):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #ordinal encode categorical colums and standardscaler is applied (mean0,sd=1)\n    ordinal_encoder = OrdinalEncoder()\n    \n    xtrain[cat_cols] = ordinal_encoder.fit_transform(xtrain[cat_cols])\n    xvalid[cat_cols] = ordinal_encoder.transform(xvalid[cat_cols])\n    xtest[cat_cols] = ordinal_encoder.transform(xtest[cat_cols])\n    \n    scaler = preprocessing.StandardScaler()\n    xtrain[num_cols] = scaler.fit_transform(xtrain[num_cols])\n    xvalid[num_cols] = scaler.transform(xvalid[num_cols])\n    xtest[num_cols] = scaler.transform(xtest[num_cols])","62084951":"# import packages for hyperparameters tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nspace={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n        'gamma': hp.uniform ('gamma', 1,9),\n        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'n_estimators': 180,\n        'seed': 0\n    }","032451fd":"def objective(space):\n    clf=XGBRegressor(\n                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n                    colsample_bytree=int(space['colsample_bytree']))\n    \n    # evaluation = [( xvalid, yvalid)]\n    \n    clf.fit(xtrain,ytrain, eval_metric=\"auc\",\n            early_stopping_rounds=10,verbose=False)\n    \n\n    pred = clf.predict(xvalid)\n    accuracy = accuracy_score(yvalid, pred>0.5)\n    print (\"SCORE:\", accuracy)\n    return {'loss': -accuracy, 'status': STATUS_OK }","75ab10c2":"#Model hyperparameter of XGboostRegressor\nxgb_params = {\n        'learning_rate': 0.03628302216953097,\n        'subsample': 0.7875490025178,\n        'colsample_bytree': 0.11807135201147,\n        'max_depth': 3,\n        'booster': 'gbtree', \n        'reg_lambda': 0.0008746338866473539,\n        'reg_alpha': 23.13181079976304,\n        'random_state':40,\n        'n_estimators':10000\n        \n        \n    }\n\n#store the final_prediction data and score\nfinal_predictions = []\nscore= []\n    \nmodel= XGBRegressor(**xgb_params,\n                       tree_method='gpu_hist',\n                       predictor='gpu_predictor',\n                       gpu_id=0)\nmodel.fit(xtrain,ytrain,early_stopping_rounds=300,eval_set=[(xvalid,yvalid)],verbose=2000)\npreds_valid = model.predict(xvalid)\n    \n    #Training model apply the test data and predict the output\ntest_pre = model.predict(xtest)\nfinal_predictions.append(test_pre)\n    \n    #Rootmeansquared output\nrms = mean_squared_error(yvalid,preds_valid,squared=False)\n    \nscore.append(rms)\n    #way of output is display\nprint(f\"fold:{fold},rmse:{rms}\")\n\n#mean of repeation of fold data and identify the  mean and standard deviation \nprint(np.mean(score),np.std(score))","6b51ae1e":"#prediction of data\npreds = np.mean(np.column_stack(final_predictions),axis=1)\nprint(preds)\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsample_submission.target = preds\nsample_submission.to_csv(\"submission.csv\",index=False)\nprint(\"success\")","4823f56a":"The available hyperopt optimization algorithms are -\n\n* **hp.choice(label, options)** \u2014 Returns one of the options, which should be a list or tuple.\n\n1. **hp.randint(label, upper)** \u2014 Returns a random integer between the range [0, upper).\n\n* **hp.uniform(label, low, high)** \u2014 Returns a value uniformly between low and high.\n\n* **hp.quniform(label, low, high, q)** \u2014 Returns a value round(uniform(low, high) \/ q) * q, i.e it rounds the decimal values and returns an integer.\n\n* **hp.normal(label, mean, std)** \u2014 Returns a real value that\u2019s normally-distributed with mean and standard deviation sigma.","64a90020":"parts of Optimization Process \n\nThe optimization process consists of 4 parts which are as follows-\n\n1. Initialize domain space\nThe domain space is the input values over which we want to search.\n\n2. Define objective function\nThe objective function can be any function which returns a real value that we want to minimize. In this case, we want to minimize the validation error of a machine learning model with respect to the hyperparameters. If the real value is accuracy, then we want to maximize it. Then the function should return the negative of that metric.\n\n3. Optimization algorithm\nIt is the method used to construct the surrogate objective function and choose the next values to evaluate.\n\n4. Results\nResults are score or value pairs that the algorithm uses to build the model.","6fb900ef":"**Bayesian Optimization with HYPEROPT **\n\n* Bayesian optimization is optimization or finding the best parameter for a machine learning or deep learning algorithm.\n\n* Optimization is the process of finding a minimum of cost function , that determines an overall better performance of a model on both train-set and test-set.\n\n* In this process, we train the model with various possible range of parameters until a best fit model is obtained.\n\n* Hyperparameter tuning helps in determining the optimal tuned parameters and return the best fit model, which is the best practice to follow while building an ML or DL model.\n\n*  In this section, we discuss one of the most accurate and successful hyperparameter tuning method, which is Bayesian Optimization with HYPEROPT.\n\n* Please see my kernel Bayesian Optimization using HYPEROPT, for more information on the optimization process using HYPEROPT.\n\nSo, we will start with HYPEROPT.\n","b04a0a1e":"Store numerical features"}}