{"cell_type":{"ea268cc6":"code","d946a81e":"code","6bd2275f":"code","af9fdf68":"code","30fb3bb4":"code","e1e2cb80":"code","e1edfa3a":"code","c8331d14":"code","5424ac9a":"code","0a195688":"code","cec954da":"code","a9a3e315":"code","6ba439cf":"code","0d5950f1":"code","4d19e114":"code","746bb72f":"code","f28c5893":"code","b385d5f9":"code","91414f27":"code","f9e9583c":"code","be9d1ff5":"code","915f52d7":"code","d6847d17":"code","c692a1b4":"code","5c3114ae":"code","2dc774b6":"code","30d74427":"code","7f1df13c":"code","481d7dc2":"code","43f48de3":"code","af806b29":"code","91b5649b":"code","5917a8ce":"code","18dbc17c":"code","c580f1da":"code","439857f8":"code","45a597ed":"markdown","4082004f":"markdown","801a920f":"markdown","1ba6a59f":"markdown","7f6e1914":"markdown","dce82b6c":"markdown","86ccb5c9":"markdown","4eb2a3c9":"markdown","e4e6773d":"markdown","76535f23":"markdown","e45a8b3e":"markdown","a61fd6a8":"markdown","fd0d61d2":"markdown","5c729f9f":"markdown","4eda9e6c":"markdown","a5782770":"markdown","322953d0":"markdown","c52eacd7":"markdown","69b022c2":"markdown","e3901bfe":"markdown"},"source":{"ea268cc6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport statsmodels.api as lm\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor","d946a81e":"df = pd.read_csv('..\/input\/insurance\/insurance.csv')\ndf.head()","6bd2275f":"df.shape","af9fdf68":"df.isnull().sum()","30fb3bb4":"df.describe()","e1e2cb80":"sns.set_palette('Set2')\nsns.set_style(\"whitegrid\")","e1edfa3a":"df['sex'].value_counts(normalize=True)","c8331d14":"sns.set(rc = {'figure.figsize':(14,8)})\nsns.set_palette('Set2')\nsns.set_style(\"whitegrid\")\nsns.histplot(x='age', stat='count', hue='sex', data=df, multiple='dodge', bins=6, shrink=0.8)\nplt.xlabel('age')\nplt.ylabel('count')\nplt.show()","5424ac9a":"df['smoker'].value_counts(normalize=True)","0a195688":"df[df['sex']=='male']['smoker'].value_counts(normalize=True)","cec954da":"df[df['sex']=='female']['smoker'].value_counts(normalize=True)","a9a3e315":"sns.histplot(x='smoker', stat='count', hue='sex', data=df, multiple='dodge', shrink=0.8)\nplt.xlabel('smoker')\nplt.ylabel('count')\nplt.show()","6ba439cf":"# Encoding categorical variables so we can visualise correlations of every variable with charges\ndf_copy = df.copy()\nle_sex = LabelEncoder()\nle_smoker = LabelEncoder()\nle_region = LabelEncoder()\ndf_copy['sex'] = le_sex.fit_transform(df['sex'])\ndf_copy['smoker'] = le_smoker.fit_transform(df['smoker'])\ndf_copy['region'] = le_region.fit_transform(df['region'])\ndf_copy.head()","0d5950f1":"sns.set(rc = {'figure.figsize':(12,7)})\nsns.set_palette('Set2')\nsns.set_style(\"whitegrid\")\ncorrelations_df = df_copy.corr()\nsns.heatmap(correlations_df, cmap=sns.cubehelix_palette(as_cmap=True))\nplt.title('Correlations between the features')\nplt.show()","4d19e114":"sns.displot(df['charges'], kde=True, height=6,aspect=1.5)\nplt.show()","746bb72f":"sns.displot(data=df, col='smoker', x='charges', kde=True, height=6, aspect=1.3)\nplt.show()","f28c5893":"sns.set(rc = {'figure.figsize':(15,8)})\nsns.set_palette('Set2')\nsns.set_style(\"whitegrid\")\nsns.boxplot(x='charges', y='sex', hue='smoker', data=df, orient='h')\nplt.title('charges for both sexes depending on smoker var')\nplt.show()","b385d5f9":"sns.lmplot(x='age', y='charges', data=df, hue='smoker', height=6, aspect=2)\nplt.title('charges vs age for smokers and none smokers')\nplt.show()","91414f27":"X = df_copy.iloc[:,:-1].values\ny = df_copy.iloc[:,-1].values","f9e9583c":"ohe_region = OneHotEncoder(drop='first')\nnew = ohe_region.fit_transform(X[:,-1].reshape(-1,1)).toarray()\nX = np.delete(X,np.s_[-1],1)\nX = np.hstack((X, new))\nX[:5,:]","be9d1ff5":"# feature scaling\nsc_X = StandardScaler()\nX = sc_X.fit_transform(X)","915f52d7":"# splitting the data into train set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=338\/1339, random_state=0)","d6847d17":"lin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)","c692a1b4":"def evaluate_perf(regressor, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, print_res=True):\n    y_pred_train, y_pred_test = regressor.predict(X_train), regressor.predict(X_test)\n    mse_train = mean_squared_error(y_train, y_pred_train)\n    mse_test = mean_squared_error(y_test, y_pred_test)\n    rmse_train, rmse_test = np.sqrt(mse_train), np.sqrt(mse_test)\n    r2_train, r2_test = r2_score(y_train, y_pred_train), r2_score(y_test, y_pred_test)\n    if print_res:\n        print(f'''On training set:\\nMSE: {mse_train}    RMSE: {rmse_train}    r2: {r2_train}\\n\n=====================================\\nOn test set:\\nMSE: {mse_test}    RMSE: {rmse_test}    r2: {r2_test}''')\n    return mse_train, rmse_train, r2_train, mse_test, rmse_test, r2_test","5c3114ae":"_ ,_ ,_ ,mse, rmse, r2 = evaluate_perf(lin_reg)","2dc774b6":"results_df = pd.DataFrame([['Linear Regression', mse, rmse, r2]], columns=['Model', 'MSE', 'RMSE', 'R^2'])\nresults_df","30d74427":"degree = [2,3,4]\nresults = []\nfor d in degree:\n    poly_feat = PolynomialFeatures(degree=d)\n    X_poly = poly_feat.fit_transform(X)\n    Xp_train, Xp_test, yp_train, yp_test = train_test_split(X_poly, y, test_size=338\/1339, random_state=0)\n    poly_reg = LinearRegression()\n    poly_reg.fit(Xp_train, yp_train)\n    mse_train, rmse_train, r2_train, mse_test, rmse_test, r2_test = evaluate_perf(poly_reg, Xp_train, yp_train, Xp_test, yp_test, print_res=False)\n    results.append([d, mse_train, rmse_train, r2_train, mse_test, rmse_test, r2_test])\npoly_results_df = pd.DataFrame(results, columns=['degree', 'mse_train', 'rmse_train', 'r2_train', 'mse_test', 'rmse_test', 'r2_test'])\npoly_results_df","7f1df13c":"results_df_2 = pd.DataFrame([['Polynomial Regression', *poly_results_df[poly_results_df['degree']==2].iloc[:,4:].values[0]]], columns=['Model', 'MSE', 'RMSE', 'R^2'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","481d7dc2":"sv_reg = SVR(kernel='rbf', C=17000)\nsv_reg.fit(X_train, y_train)\n_ ,_ ,_ ,mse, rmse, r2 = evaluate_perf(sv_reg)","43f48de3":"results_df_3 = pd.DataFrame([['SVR', mse, rmse, r2]], columns=['Model', 'MSE', 'RMSE', 'R^2'])\nresults_df = results_df.append(results_df_3, ignore_index=True)\nresults_df","af806b29":"n_estimators = [10, 30, 100, 300, 1000, 3000]\nmax_leaf_nodes = [10, 30, 50, 75, 100]\nresults = []\nfor est in n_estimators:\n    for n_nodes in max_leaf_nodes:\n        for_reg = RandomForestRegressor(n_estimators=est, max_leaf_nodes=n_nodes, random_state=0)\n        for_reg.fit(X_train, y_train)\n        mse_train, rmse_train, r2_train, mse_test, rmse_test, r2_test = evaluate_perf(for_reg, print_res=False)\n        results.append([est, n_nodes, mse_train, rmse_train, r2_train, mse_test, rmse_test, r2_test])\nfor_results_df = pd.DataFrame(results, columns=['n_estimators', 'n_nodes', 'mse_train', 'rmse_train', 'r2_train', 'mse_test', 'rmse_test', 'r2_test'])\nfor_results_df","91b5649b":"max_r2 = max(for_results_df['r2_test'])\nfor_results_df[for_results_df['r2_test']==max_r2]","5917a8ce":"min_rmse = min(for_results_df['rmse_test'])\nfor_results_df[for_results_df['rmse_test']==min_rmse]","18dbc17c":"results_df_4 = pd.DataFrame([['Random Forest', *for_results_df[for_results_df['r2_test']==max_r2].iloc[:,5:].values[0]]], columns=['Model', 'MSE', 'RMSE', 'R^2'])\nresults_df = results_df.append(results_df_4, ignore_index=True)\nresults_df","c580f1da":"sns.set(rc = {'figure.figsize':(10,6)})\nsns.set_palette('Set2')\nsns.set_style(\"whitegrid\")\nsns.barplot(x='R^2', y='Model', data=results_df)\nplt.title('R^2 for each model')\nplt.show()","439857f8":"sns.barplot(x='RMSE', y='Model', data=results_df)\nplt.title('RMSE for each model')\nplt.show()","45a597ed":"***\n# Polynomial Regression","4082004f":"now the three last column represents the region.\n> `[0 , 0, 0]` ==> northeast.<br>\n`[1 , 0, 0]` ==> northwest.<br>\n`[0 , 1, 0]` ==> southeast.<br>\n`[0 , 0, 1]` ==> southwest.","801a920f":"We see that the dataset is well distributed when it comes to age and gender (male: 51%, female: 49%).","1ba6a59f":"***\n# Support Vector Regression:","7f6e1914":"The plot shows that the vast majority (80%) are none-smokers with a slight outnumbering for female.<br>\n>82% of females are none-smokers.<br>\n76% of males are none-smokers.","dce82b6c":"This conludes it that random forest is the best model we have got with an `rmse = 4022.73`.","86ccb5c9":"We conclude that smokers spend more on treatment.<br>\nPS: none smokers make 80% of the dataset as we have seen before.","4eb2a3c9":"Our best performing random forest model has `n_estimators = 100` & `max_leaf_nodes = 30`. ","e4e6773d":"***\n# Linear Regression:","76535f23":"We notice a strong correlation between `charges` and `smoker` feature. We also see a slight connection between `charges` and `age`.","e45a8b3e":"now let's see how charges are distributed ","a61fd6a8":"We see that charges increases with aging which is expected and totally normal. But notice the huge difference between the mean of charges for smokers and none smokers. Point is, it's high time you quit smoking! If you never smoked then well done! You saved your health and a ton of money.\n***\n# Data Preprocessing:\nWe will work with `df_copy` as it already have the categorical data encoded. We are going to add a small modification that is `OneHotEncoder` on the `region` column.","fd0d61d2":"# Medical Insurance Cost (EDA\/ machine learning models):\nIn this notebook I'm going to explore Medical Cost Personal Dataset. Since I'm a beginner, I will apply some new things I learnt in EDA from the enriching notebook of [Dandelion](https:\/\/www.kaggle.com\/hely333) about the same dataset. I will also be building different machine learning models to predict the charges for every patient. Let's get into it!\n***\n### Import Libraries:","5c729f9f":"We see that linear regression has the worst value of `r^2 = 0.79`.<br>\nThe three other models have a very similar r^2 values with **random forest** slightly ahead (`r^2 = 0.89`).<br>\nLet's see if the results are the same based on the square roots of the mean square error.","4eda9e6c":"This boxplot supports the last observation that none smokers spend less on treatment in general. Moreover, it shows that the average of charges for male smokers is greater than that of female smokers. That may be due to the larger number of male smokers.","a5782770":"We will choose `degree = 2` as higher degrees leads to overfitting the training set.","322953d0":"### Data:\n* age: age of primary beneficiary\n* sex: insurance contractor gender, female, male\n* bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg \/ m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n* children: Number of children covered by health insurance \/ Number of dependents\n* smoker: Smoking\n* region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n* charges: Individual medical costs billed by health insurance","c52eacd7":"### Import Dataset:","69b022c2":"***\n# Data Analysis:","e3901bfe":"***\n# Random Forest:"}}