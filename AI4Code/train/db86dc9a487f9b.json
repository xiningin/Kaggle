{"cell_type":{"83ad9be9":"code","7c6b119e":"code","292bc135":"code","bdb56f78":"code","0a3479bb":"code","92742132":"code","d0e9b8b0":"code","296f2444":"code","c4fad50a":"code","cc97670b":"code","f0f21e6a":"code","048111dc":"code","cea321ea":"code","ba275ec2":"code","109eb1b4":"code","84f89fee":"code","2478d304":"code","ad5a1568":"code","719bfcca":"code","5749393b":"code","095321d5":"code","ed9b1d96":"code","e783f88a":"code","6ab06a62":"code","822f078a":"code","e549c99e":"code","805eb16e":"code","ba02db8d":"code","1a2b607b":"code","fefd537d":"code","88d828df":"code","e6795aeb":"code","47249335":"code","27cfc09c":"code","5fe42194":"code","eeceb79e":"code","5911ca7c":"code","11935fe8":"code","a5c97f51":"code","51161d64":"code","96c5f013":"code","69e0c23c":"code","9a23aaf5":"code","b0657f06":"code","e1c6d138":"code","1a3228e7":"code","083a236f":"code","fa6b4487":"code","bdca666c":"code","cbb89964":"code","e5c0b09d":"code","d9e261bd":"code","25366f06":"code","c0764898":"code","de07edff":"code","84c19244":"code","01c6db95":"code","9426d14f":"code","fbebada1":"code","df93858a":"code","d14ab493":"code","9bcb6eea":"code","b8d4c3cb":"code","65eb9e10":"code","0d081f0f":"code","1a8c5305":"code","b983ffb8":"code","4a083b8d":"code","2dccd22c":"code","71a92fec":"code","f67f4093":"code","67345aa2":"code","4cc694a1":"code","4df70898":"code","257df29e":"code","e6fb658c":"code","56561afa":"code","89d3c690":"code","3e93ad06":"code","6a63fcd1":"code","c3f2e3e8":"code","61c5f8b6":"code","8ad95d6f":"code","4d69d99e":"code","9436c517":"code","87f8c35a":"code","4d5d50d9":"code","5db95693":"code","d8e0f7ae":"code","06597379":"code","cb35994b":"code","15667687":"code","e208d8b3":"code","163ebcab":"code","75b75892":"code","1e8a0023":"code","d6c5dd10":"code","2943d4d1":"code","294f4251":"code","2a0a049f":"code","23f06557":"code","247473f1":"code","3b2dccb0":"code","f635442c":"code","de9abca1":"code","6cf2e629":"code","6ee9b356":"code","96272487":"code","e48c49c5":"code","403e3a8a":"code","116c283b":"code","cc88f4dc":"code","cc6543d3":"code","2631d6cf":"code","8cf4e8c1":"code","c2291778":"code","1e2f97f7":"code","f0fe3c26":"code","4d9f0fbb":"code","c0607480":"code","52181b84":"code","0df28740":"code","b14a9383":"code","519c3684":"code","a0278672":"code","a06f136e":"code","1b160377":"code","f26b05f8":"markdown","1c346bed":"markdown","fbb2fa18":"markdown","595a5a57":"markdown","5a2e06b7":"markdown","4815a82c":"markdown","7c3161f1":"markdown","4ac0e653":"markdown","2a0cb57a":"markdown","a10bd995":"markdown","8106b3ca":"markdown","45a8c912":"markdown","a81f0b50":"markdown","d18320f6":"markdown","300cc3ff":"markdown","8cb21ec9":"markdown","9bd6b95b":"markdown"},"source":{"83ad9be9":"#-------importing EDA Libraries----------\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as mn\nimport pandas_profiling\n#-------- Suppressing Warnings------------\nimport warnings\nwarnings.filterwarnings('ignore')\n#-------- Machine Learning Libraries------\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn import metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","7c6b119e":"churn_data = pd.read_csv(\"..\/input\/telecom-churn-data-sets\/churn_data.csv\")","292bc135":"churn_data.head()","bdb56f78":"churn_data.info()","0a3479bb":"churn_data.describe()","92742132":"churn_data.shape","d0e9b8b0":"customer_data = pd.read_csv('..\/input\/telecom-churn-data-sets\/customer_data.csv')","296f2444":"customer_data.head()","c4fad50a":"customer_data.info()","cc97670b":"customer_data.shape","f0f21e6a":"internet_data = pd.read_csv('..\/input\/telecom-churn-data-sets\/internet_data.csv')","048111dc":"internet_data.head()","cea321ea":"internet_data.info()","ba275ec2":"internet_data.shape","109eb1b4":"# Merge all three datasets together\ndf = pd.merge(churn_data,customer_data,how = 'inner', on = 'customerID')","84f89fee":"telecom_data = pd.merge(df,internet_data, how = 'inner' , on = 'customerID')","2478d304":"telecom_data.head()","ad5a1568":"telecom_data.info()","719bfcca":"telecom_data.describe()","5749393b":"telecom_data.isnull().sum()","095321d5":"# No missing data\nmn.bar(telecom_data)","ed9b1d96":"telecom_data.profile_report()","e783f88a":"# We will take care of the variables with 'Yes' or 'No'\nvar = ['PhoneService','PaperlessBilling','Churn','Partner','Dependents']\ndef binary_map(x):\n     return x.map({'Yes': 1, 'No':0})\n\ntelecom_data[var] = telecom_data[var].apply(binary_map)    ","6ab06a62":"telecom_data.head()","822f078a":"var1 = ['Contract','gender','PaymentMethod','InternetService']\ndummy_var = pd.get_dummies(telecom_data[var1],drop_first=True)","e549c99e":"dummy_var","805eb16e":"# Dropping the columns after dummy encoding them\ntelecom_data.drop(var1,axis = 1, inplace = True)","ba02db8d":"# Dummy encoding some other categorical variables\nmultiple_lines = pd.get_dummies(telecom_data['MultipleLines'],prefix = 'Multiple_lines')\n# Dropping the large column name\nmultiple_lines = multiple_lines.drop(['Multiple_lines_No phone service'],1)\n\nonline_security = pd.get_dummies(telecom_data['OnlineSecurity'],prefix = 'Online_Security')\n# Dropping the large column name\nonline_security = online_security.drop(['Online_Security_No internet service'],1)\n    \n    \nonline_backup = pd.get_dummies(telecom_data['OnlineBackup'],prefix = 'Online_Backup')\n# Dropping the large column name\nonline_backup = online_backup.drop(['Online_Backup_No internet service'],1)\n    \n\ndevice_protection = pd.get_dummies(telecom_data['DeviceProtection'],prefix = 'Device_Protection')\n# Dropping the large column name\ndevice_protection = device_protection.drop(['Device_Protection_No internet service'],1)\n\ntech_support = pd.get_dummies(telecom_data['TechSupport'],prefix = 'Tech_Support')\n# Dropping the large column name\ntech_support = tech_support.drop(['Tech_Support_No internet service'],1)\n\nstreaming_tv = pd.get_dummies(telecom_data['StreamingTV'],prefix = 'Streaming_TV')\n# Dropping the large column name\nstreaming_tv = streaming_tv.drop(['Streaming_TV_No internet service'],1)\n\n\nstreaming_movies = pd.get_dummies(telecom_data['StreamingMovies'], prefix = 'Streaming_Movies')\n# Dropping the large column name\nstreaming_movies = streaming_movies.drop(['Streaming_Movies_No internet service'],1)\n","1a2b607b":"# Concatenating the dummy variables with telecom data\ntelecom_data = pd.concat([dummy_var,telecom_data,multiple_lines,online_security,online_backup,device_protection,\n                          tech_support,streaming_tv,streaming_movies],axis = 1)\n# Droping the original categorical columns\ntelecom_data.drop(['MultipleLines','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport',\n                         'StreamingTV','StreamingMovies'],axis = 1,inplace = True)","fefd537d":"telecom_data.head()","88d828df":"telecom_data['TotalCharges']","e6795aeb":"telecom_data.info()","47249335":"# Coverting the dtype of TotalCharges variable\ntelecom_data['TotalCharges'] = pd.to_numeric(telecom_data['TotalCharges'], errors = 'coerce')","27cfc09c":"# There are null values in 'TotalCharges' variable\ntelecom_data['TotalCharges'].isnull().sum()","5fe42194":"telecom_data = telecom_data[~np.isnan(telecom_data['TotalCharges'])]","eeceb79e":"# All the rows with nan values have been removed from the dataset\ntelecom_data['TotalCharges'].isnull().sum()","5911ca7c":"X = telecom_data.drop(['customerID','Churn'],axis =1)","11935fe8":"y = telecom_data['Churn']","a5c97f51":"X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7, test_size=0.3, random_state=100)","51161d64":"var2 = ['tenure','MonthlyCharges','TotalCharges']\nscaler =  StandardScaler()\nX_train[var2] = scaler.fit_transform(X_train[var2])","96c5f013":"X_train[['tenure','MonthlyCharges','TotalCharges']]","69e0c23c":"X_train.head()","9a23aaf5":"# Let's see the correlation matrix \nplt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(telecom_data.corr(),annot = True)\nplt.show()","b0657f06":"X_test = X_test.drop(['Multiple_lines_No','Online_Security_No','Online_Backup_No','Device_Protection_No','Tech_Support_No',\n                       'Streaming_TV_No','Streaming_Movies_No'], 1)\nX_train = X_train.drop(['Multiple_lines_No','Online_Security_No','Online_Backup_No','Device_Protection_No','Tech_Support_No',\n                       'Streaming_TV_No','Streaming_Movies_No'],1)","e1c6d138":"# Checking correlation after dropping the dummy variables\nplt.figure(figsize = (20,10))\nsns.heatmap(X_train.corr(),annot = True)\nplt.show()","1a3228e7":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","083a236f":"logreg = LogisticRegression()\nrfe = RFE(logreg,15)\nrfe = rfe.fit(X_train,y_train)","fa6b4487":"rfe.support_","bdca666c":"rfe.ranking_","cbb89964":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","e5c0b09d":"col = X_train.columns[rfe.support_]","d9e261bd":"col","25366f06":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","c0764898":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","de07edff":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","84c19244":"y_train_pred_final = pd.DataFrame({'Churn':y_train.values, 'Churn_Prob':y_train_pred})\ny_train_pred_final['CustID'] = y_train.index\ny_train_pred_final.head()","01c6db95":"y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","9426d14f":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nprint(confusion)","fbebada1":"# Predicted     not_churn    churn\n# Actual\n# not_churn        3275      360\n# churn            574       713  ","df93858a":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))","d14ab493":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","9bcb6eea":"col = col.drop('MonthlyCharges', 1)\ncol","b8d4c3cb":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","65eb9e10":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","0d081f0f":"y_train_pred[:10]","1a8c5305":"y_train_pred_final['Churn_Prob'] = y_train_pred","b983ffb8":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","4a083b8d":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))","2dccd22c":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","71a92fec":"# Let's drop TotalCharges since it has a high VIF\ncol = col.drop('TotalCharges')\ncol","f67f4093":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","67345aa2":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","4cc694a1":"y_train_pred[:10]","4df70898":"y_train_pred_final['Churn_Prob'] = y_train_pred","257df29e":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","e6fb658c":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))","56561afa":"# Lets check VIF again\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","89d3c690":"# Let's take a look at the confusion matrix again \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nconfusion","3e93ad06":"# Actual\/Predicted     not_churn    churn\n        # not_churn        3278      357\n        # churn            597       690  ","6a63fcd1":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","c3f2e3e8":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","61c5f8b6":"# Let us calculate specificity\nTN \/ float(TN+FP)","8ad95d6f":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","4d69d99e":"# positive predictive value \nprint (TP \/ float(TP+FP))","9436c517":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","87f8c35a":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","4d5d50d9":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Churn, y_train_pred_final.Churn_Prob, drop_intermediate = False )","5db95693":"draw_roc(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","d8e0f7ae":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","06597379":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","cb35994b":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","15667687":"y_train_pred_final['final_predicted'] = y_train_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.3 else 0)\n\ny_train_pred_final.head()","e208d8b3":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.final_predicted)","163ebcab":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.final_predicted )\nconfusion2","75b75892":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","1e8a0023":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","d6c5dd10":"# Let us calculate specificity\nTN \/ float(TN+FP)","2943d4d1":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","294f4251":"# Positive predictive value \nprint (TP \/ float(TP+FP))","2a0a049f":"# Precision\nconfusion[1,1]\/(confusion[0,1]+confusion[1,1])","23f06557":"# Recall\nconfusion[1,1]\/(confusion[1,0]+confusion[1,1])","247473f1":"from sklearn.metrics import precision_recall_curve","3b2dccb0":"y_train_pred_final.Churn, y_train_pred_final.predicted","f635442c":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","de9abca1":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","6cf2e629":"X_test[['tenure','MonthlyCharges','TotalCharges']] = scaler.transform(X_test[['tenure','MonthlyCharges','TotalCharges']])","6ee9b356":"X_test = X_test[col]\nX_test.head()","96272487":"X_test_sm = sm.add_constant(X_test)","e48c49c5":"y_test_pred = res.predict(X_test_sm)","403e3a8a":"y_test_pred[:10]","116c283b":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","cc88f4dc":"# Let's see the head\ny_pred_1.head()","cc6543d3":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","2631d6cf":"# Putting CustID to index\ny_test_df['CustID'] = y_test_df.index","8cf4e8c1":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","c2291778":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","1e2f97f7":"y_pred_final.head()","f0fe3c26":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Churn_Prob'})","4d9f0fbb":"# Rearranging the columns\ny_pred_final =  y_pred_final.reindex(columns=['CustID','Churn','Churn_Prob'])","c0607480":"y_pred_final.head()","52181b84":"y_pred_final['final_predicted'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.42 else 0)","0df28740":"y_pred_final.head()","b14a9383":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Churn, y_pred_final.final_predicted)","519c3684":"confusion2 = metrics.confusion_matrix(y_pred_final.Churn, y_pred_final.final_predicted )\nconfusion2","a0278672":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","a06f136e":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","1b160377":"# Let us calculate specificity\nTN \/ float(TN+FP)","f26b05f8":"### Making predictions on test set","1c346bed":"### Model Building","fbb2fa18":"### Creating dummy encoding","595a5a57":"### Precision and recall trade-off","5a2e06b7":"### Train-Test Split","4815a82c":"### Checking VIF","7c3161f1":"### Creating a new coulmn 'predicted' with 1 if prob > 0.5 else 0","4ac0e653":"### Plotting the ROC curve\n\nAn ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","2a0cb57a":"### Accessing the model with statsmodel","a10bd995":"### Metrics beyond simple accuracy","8106b3ca":"### Precision and recall","45a8c912":"### Feature Scaling","a81f0b50":"### Looking at the correlation","d18320f6":"### Findinf the optimal cutt-off point","300cc3ff":"### From the above curve we can see that 0.3 is the optimal point","8cb21ec9":"### Feature selection using RFE","9bd6b95b":"### Dropping the highly correlated dummy variables"}}