{"cell_type":{"989316c5":"code","7d3ec885":"code","c9eee058":"code","b39b718b":"code","2bfbb3c8":"code","b93e7235":"code","9d463d75":"code","15266001":"code","1d00d51f":"code","d7e94c2b":"code","d52d25fd":"code","c585c869":"code","35dcefc7":"code","0a0435ad":"code","b936a115":"code","82b159e3":"code","c580092c":"code","52ccdee3":"code","f11d3eea":"code","9f1aabdd":"code","fbd16fee":"code","c8eecbec":"code","927697c7":"code","bbdceeb1":"markdown","a351ce31":"markdown","9855384d":"markdown","311b1999":"markdown","2330ab01":"markdown","43ebbd0d":"markdown","b66b6f20":"markdown","91221348":"markdown","454c5ff5":"markdown","01d871c3":"markdown","24c31ecb":"markdown","99f905db":"markdown"},"source":{"989316c5":"class GradientDescents:\n  \n  '''\n  Gradient Descent From Scratch\n  '''\n\n  import random\n\n  def progress_tracker(self, step: int, cost_function: float) -> None:\n    '''\n    The function allows you to track online progress\n\n    :param step: current step\n    :param cost_function: the value of the cost function at the moment\n\n    '''\n    from IPython.display import clear_output\n    clear_output(wait=True)\n    print('Step: {}'.format(step))\n    print('Loss: {:.2f}'.format(cost_function))\n\n  def mse_function(self, y_true: list, y_pred: list) -> float:\n    '''\n    Function that calculates MSE\n\n    :param y_true: the y values we know from the actual data\n    :param y_pred: the y values we got at the moment\n\n    :return mse: MSE value\n    '''\n    # Number of values to compare with actual y\n    n = len(y_true)\n    # Starting with 0\n    pre_mse = 0\n    for index, value in enumerate(y_true):\n      pre_mse += (value - y_pred[index])**2\n    mse = pre_mse\/n\n    return mse\n\n  def gradient_descent_multi(self, X_true: list, y_true: list, \\\n                              weights: list = None, max_steps: int = 10000, \\\n                              learning_rate: float = 0.003, \\\n                              save_steps: int = 0) -> dict:\n    '''\n    Gradient descent for multiple variables\n\n    :param X_true: actual attributes\n    :param y_true: actual results\n    :param weights: starting weights, if we don't want to start training from random\n    :param learning_rate: learning rate\n    :param max_steps: maximum number of steps at which the algorithm will stop\n    :param save_steps: if 0, only last step will be saved\n                       If not 0, every #save_steps will be saved\n    \n    :return {\n      :return weights: regression weights\n      :return mse: MSE\n      :return steps: # of Steps\n      :return mse_list: list of MSEs if save_steps > 0\n      :return weights_list: list of weigtht lists if save_steps > 0\n    }\n    '''\n\n    # Code for data with only one atribute\n    if (type(X_true[0])==int) or (type(X_true[0])==float):\n      for i, x in enumerate(X_true):\n        X_true[i]=[x,1]\n    elif (type(X_true[0])==list) and (len(X_true[0])==1):\n      for i, x in enumerate(X_true):\n        X_true[i].append(1)\n\n    # Initialize random weights\n    if weights == None:\n      weights = [self.random.random() for f in X_true[0]]\n\n    if save_steps > 0:\n      mse_list = []\n      weights_list = []\n    \n    # MSE of the previous state\n    mse_prev = 0\n    mse = 999\n\n    # Nubmer of experiments we've got\n    n = len(X_true)\n\n    step = 0\n    while (step <= max_steps) and (abs(mse_prev-mse)>1e-5):\n      # Calculate gradients\n      gradients = []\n      for wi, w_value in enumerate(weights):\n        current_gradient=0\n        for yi, y_t_val in enumerate(y_true):\n          current_gradient += -2*(y_t_val - sum([w*x for w,x in \\\n                                                 zip(weights,X_true[yi])]))* X_true[yi][wi]\n        current_gradient = current_gradient\/n\n        gradients.append(current_gradient)\n\n      # Change weights\n      for gi, gr_value in enumerate(gradients):\n        weights[gi] = weights[gi] - learning_rate*gr_value\n\n      # Calculate y_pred\n      y_pred = []\n      for X_current in X_true:\n        y_pred.append(sum([w*x for w,x in zip(weights,X_current)]))\n      \n      step +=1\n      mse_prev = mse\n      mse = self.mse_function(y_true, y_pred)\n      self.progress_tracker(step, mse)\n\n      if save_steps > 0:\n        if step % save_steps == 0:\n          mse_list.append(mse)\n          weights_list.append(weights)\n\n    if save_steps > 0:\n      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1, \\\n                      'mse_list': mse_list, 'weights_list': weights_list}\n    else:\n      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1}\n\n    return return_dict","7d3ec885":"new_grad = GradientDescents()","c9eee058":"import random","b39b718b":"# Loading Boston Dataset\nfrom sklearn.datasets import load_boston\nX, y = load_boston(return_X_y=True)\nX_true = []\nfor i in X:\n  x_s_list = [f for f in i]\n  # x_s_list.append(1)\n  X_true.append(x_s_list)\ny_true = [f for f in y]\ndel X, y","2bfbb3c8":"learning_rate = 0.0000003\nmax_steps = 50000\nstep = 0\n\n# Momentum\ngamma = 0.5\n\n# For the purity of the choice of the algorithm, I will start the weights from 1\nweights = [1] * len(X_true[0])\n\n# Number of elements in row of X\nn = len(X_true[0])\n\n# Gradients\n# Current gradient\ngradient = []\n# Previous move\nv_t_previous = [0] * len(X_true[0])\n\nall_mses_algo1 = []\n\nwhile step < max_steps:\n  # For experiment we will equal seed with step\n  random.seed(step)\n  # Get random row\n  index = random.randint(0, n-1)\n  # X & y for current step\n  X_current = X_true[index]\n  y_current = y_true[index]\n  gradient = []\n  # Calculate current gradient\n  for x_i in X_current:\n    current_gradient = -2*(y_current - sum([w*x for w,x in \\\n                                          zip(weights,X_current)]))*x_i\n    gradient.append(current_gradient)\n  \n  # Apply momentum for previous step\n  momentum_v_t_previous = [f*gamma for f in v_t_previous]\n  # Applying step for gradient\n  step_gradient = [f*learning_rate for f in gradient]\n  # New delta to move weights\n  v_t = [a+b for a,b in zip(momentum_v_t_previous,step_gradient)]\n  v_t_previous = v_t\n\n  # Move weights\n  for vti, vti_value in enumerate(v_t):\n    weights[vti] = weights[vti] - vti_value\n\n  y_pred = sum([w*x for w,x in zip(weights,X_current)])\n  mse = new_grad.mse_function([y_pred], [y_current])\n\n  step += 1\n\n  new_grad.progress_tracker(step, mse)\n\n  # Check on progress\n  y_pred_algo_1 = []\n  for X_current in X_true:\n    y_pred_algo_1.append(sum([w*x for w,x in zip(weights,X_current)]))\n\n  mse_algo_1 = new_grad.mse_function(y_pred_algo_1, y_true)\n\n\n  all_mses_algo1.append(mse_algo_1)","b93e7235":"learning_rate = 0.0000003\nmax_steps = 50000\nstep = 0\n\n# Momentum\ngamma = 0.9\n\n# For the purity of the choice of the algorithm, I will start the weights from 1\nweights = [1] * len(X_true[0])\n\n# Number of elements in row of X\nn = len(X_true[0])\n\n# Gradients\n# Current gradient\ngradient = []\n# Previous move\nv_t_previous = [0] * len(X_true[0])\nall_mses_algo2 = []\nwhile step < max_steps:\n  # For experiment we will equal seed with step\n  random.seed(step)\n  # Get random row\n  index = random.randint(0, n-1)\n  # X \u0438 y for current step\n  X_current = X_true[index]\n  y_current = y_true[index]\n  gradient = []\n  # Calculating current gradient\n  for x_i in X_current:\n    current_gradient = -2*(y_current - sum([w*x for w,x in \\\n                                          zip(weights,X_current)]))*x_i\n    gradient.append(current_gradient)\n  \n  # Apply momentum for previous step\n  momentum_v_t_previous = [f*gamma for f in v_t_previous]\n  # Applying step for gradient\n  step_gradient = [f*(1-gamma) for f in gradient]\n  # New delta to move weights\n  v_t = [a+b for a,b in zip(momentum_v_t_previous,step_gradient)]\n  # \u0412 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0439 \u0441\u0434\u0432\u0438\u0433 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u043d\u043e\u0432\u044b\u0439\n  v_t_previous = v_t\n\n  # Make weights move\n  for vti, vti_value in enumerate(v_t):\n    weights[vti] = weights[vti] - vti_value * learning_rate\n\n  y_pred = sum([w*x for w,x in zip(weights,X_current)])\n  mse = new_grad.mse_function([y_pred], [y_current])\n\n  step += 1\n\n  new_grad.progress_tracker(step, mse)\n\n  # Follow the progress\n  y_pred_algo_2 = []\n  for X_current in X_true:\n    y_pred_algo_2.append(sum([w*x for w,x in zip(weights,X_current)]))\n\n  mse_algo_2 = new_grad.mse_function(y_pred_algo_2, y_true)\n\n  all_mses_algo2.append(mse_algo_2)\n","9d463d75":"import matplotlib.pyplot as plt","15266001":"steps = [i+1 for i, f in enumerate(all_mses_algo2)]","1d00d51f":"fig = plt.figure(figsize=(15, 5))\nax = fig.add_subplot()\nax.plot(steps, all_mses_algo1, color='#ff6361', \\\n        label='First model')\nplt.plot(steps, all_mses_algo2, color='#ffa600', \\\n         label='Second model')\nplt.title('Models Comparison All')\nax.legend()\nplt.show()","d7e94c2b":"fig = plt.figure(figsize=(15, 5))\nax = fig.add_subplot()\nax.plot(steps[:500], all_mses_algo1[:500], color='#ff6361', \\\n        label='First model')\nplt.plot(steps[:500], all_mses_algo2[:500], color='#ffa600', \\\n         label='Second model')\nplt.title('Models Comparison Head')\nax.legend()\nplt.show()","d52d25fd":"fig = plt.figure(figsize=(15, 5))\nax = fig.add_subplot()\nax.plot(steps[-2000:], all_mses_algo1[-2000:], color='#ff6361', \\\n        label='First model')\nplt.plot(steps[-2000:], all_mses_algo2[-2000:], color='#ffa600', \\\n         label='Second model')\nplt.title('Models Comparison Tail')\nax.legend()\nplt.show()","c585c869":"class GradientDescents:\n  \n  '''\n  Gradient descent from scratch\n  '''\n\n  import random\n\n  def progress_tracker(self, step: int, cost_function: float) -> None:\n    '''\n    The function allows you to track online progress\n\n    :param step: current step\n    :param cost_function: the value of the cost function at the moment\n\n    '''\n    from IPython.display import clear_output\n    clear_output(wait=True)\n    print('Step: {}'.format(step))\n    print('Loss: {:.2f}'.format(cost_function))\n\n  def mse_function(self, y_true: list, y_pred: list) -> float:\n    '''\n    Function that calculates MSE\n\n    :param y_true: the y values we know from the actual data\n    :param y_pred: the y values we got at the moment\n\n    :return mse: MSE value\n    '''\n    # Number of values \u200b\u200bto compare\n    n = len(y_true)\n    # Starting with 0\n    pre_mse = 0\n    for index, value in enumerate(y_true):\n      pre_mse += (value - y_pred[index])**2\n    mse = pre_mse\/n\n    return mse\n\n  def gradient_descent_multi(self, X_true: list, y_true: list, \\\n                              weights: list = None, max_steps: int = 10000, \\\n                              learning_rate: float = 0.003, \\\n                              save_steps: int = 0) -> dict:\n    '''\n    Gradient descent for multiple variables\n\n    :param X_true: actual attributes\n    :param y_true: actual results\n    :param weights: starting weights, if we don't want to start training from random\n    :param learning_rate: learning rate\n    :param max_steps: maximum number of steps at which the algorithm will stop\n    :param save_steps: if 0, only last step will be saved\n                       If not 0, every #save_steps will be saved\n    \n    :return {\n      :return weights: regression weights\n      :return mse: MSE\n      :return steps: # of Steps\n      :return mse_list: list of MSEs if save_steps > 0\n      :return weights_list: list of weigtht lists if save_steps > 0\n    }\n    '''\n\n    # Code for data with one attribute\n    if (type(X_true[0])==int) or (type(X_true[0])==float):\n      for i, x in enumerate(X_true):\n        X_true[i]=[x,1]\n    elif (type(X_true[0])==list) and (len(X_true[0])==1):\n      for i, x in enumerate(X_true):\n        X_true[i].append(1)\n\n    # Initizlize random weights\n    if weights == None:\n      weights = [self.random.random() for f in X_true[0]]\n\n    if save_steps > 0:\n      mse_list = []\n      weights_list = []\n    \n    # previous step MSE\n    mse_prev = 0\n    mse = 999\n\n    # Number of experiments we got     \n    n = len(X_true)\n\n    step = 0\n    while (step <= max_steps) and (abs(mse_prev-mse)>1e-5):\n      # Count gratients\n      gradients = []\n      for wi, w_value in enumerate(weights):\n        current_gradient=0\n        for yi, y_t_val in enumerate(y_true):\n          current_gradient += -2*(y_t_val - sum([w*x for w,x in \\\n                                                 zip(weights,X_true[yi])]))* X_true[yi][wi]\n        current_gradient = current_gradient\/n\n        gradients.append(current_gradient)\n\n      # Move weights\n      for gi, gr_value in enumerate(gradients):\n        weights[gi] = weights[gi] - learning_rate*gr_value\n\n      # Calculate y_pred\n      y_pred = []\n      for X_current in X_true:\n        y_pred.append(sum([w*x for w,x in zip(weights,X_current)]))\n      \n      step +=1\n      mse_prev = mse\n      mse = self.mse_function(y_true, y_pred)\n      self.progress_tracker(step, mse)\n\n      if save_steps > 0:\n        if step % save_steps == 0:\n          mse_list.append(mse)\n          weights_list.append(weights)\n\n    if save_steps > 0:\n      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1, \\\n                      'mse_list': mse_list, 'weights_list': weights_list}\n    else:\n      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1}\n\n    return return_dict\n\n  def momentum_gradient_descent(self, X_true: list, y_true: list, \\\n                                weights: list = None, max_steps: int = 10000, \\\n                                learning_rate: float = 0.003, gamma: float = 0.9, \\\n                                save_steps: int = 0) -> dict:\n    '''\n    Gradient descent with acceleration\n\n    : param X_true: actual attributes\n    : param y_true: actual results\n    : param weights: starting weights, if we don't want to start training from random\n    : param learning_rate: learning rate\n    : param max_steps: maximum number of steps at which the algorithm will stop\n    : param save_steps: if 0, only the last step will be saved\n                        if the value is nonzero,\n                        every i-th step will be saved\n\n\n    : return {\n      : return weights: regression weights\n      : return mse: MSE value\n      : return steps: number of steps\n      : return mse_list: MSE value during training if save_steps> 0\n      : return weights_list: weights as we learn if save_steps> 0\n    }\n                      \n    '''\n    # Code for data with one attribute\n    if (type(X_true[0])==int) or (type(X_true[0])==float):\n      for i, x in enumerate(X_true):\n        X_true[i]=[x,1]\n    elif (type(X_true[0])==list) and (len(X_true[0])==1):\n      for i, x in enumerate(X_true):\n        X_true[i].append(1)\n\n\n    if save_steps > 0:\n      mse_list = []\n      weights_list = []\n\n    step = 0\n    mse_prev = 999999999\n\n\n    # Random weights\n    if weights == None:\n      weights = [self.random.random() for f in X_true[0]]\n\n    # Amount of elemets in row X\n    n = len(X_true[0])\n\n    # Gradients\n    # Current gradient\n    gradient = []\n    # Previous move\n    v_t_previous = [0] * len(X_true[0])\n    \n    # TO DO: Change it to tain-validation datasets\n    while step < max_steps:\n\n      # We take a random number to select data\n      index = self.random.randint(0, n-1)\n      # X & y for current step\n      X_current = X_true[index]\n      y_current = y_true[index]\n      gradient = []\n      # Calculate current gradient\n      for x_i in X_current:\n        current_gradient = -2*(y_current - sum([w*x for w,x in \\\n                                              zip(weights,X_current)]))*x_i\n        gradient.append(current_gradient)\n      \n      # Apply momentum\n      momentum_v_t_previous = [f*gamma for f in v_t_previous]\n      # Apply step to a gradient\n      step_gradient = [f*learning_rate for f in gradient]\n      # Get a new move\n      v_t = [a+b for a,b in zip(momentum_v_t_previous,step_gradient)]\n      v_t_previous = v_t\n\n      # Make a move to weights\n      for vti, vti_value in enumerate(v_t):\n        weights[vti] = weights[vti] - vti_value\n\n      y_pred = sum([w*x for w,x in zip(weights,X_current)])\n\n      y_pred_algo = []\n      for X_current in X_true:\n        y_pred_algo.append(sum([w*x for w,x in zip(weights,X_current)]))\n\n      \n      mse = self.mse_function(y_pred_algo, y_true)\n\n      if mse < mse_prev:\n        # Saving only best results\n        final_weights = weights\n\n      mse_prev = mse\n\n      step += 1\n\n      self.progress_tracker(step, mse)\n\n      if save_steps > 0:\n        if step % save_steps == 0:\n          mse_list.append(mse)\n          weights_list.append(weights)\n\n    if save_steps > 0:\n      return_dict = {'weights': final_weights, 'mse':mse, 'steps': step-1, \\\n                      'mse_list': mse_list, 'weights_list': weights_list}\n    else:\n      return_dict = {'weights': final_weights, 'mse':mse, 'steps': step-1}\n\n    return return_dict","35dcefc7":"# Load Boston Dataset\nfrom sklearn.datasets import load_boston\nX, y = load_boston(return_X_y=True)\nX_true = []\nfor i in X:\n  x_s_list = [f for f in i]\n  # x_s_list.append(1)\n  X_true.append(x_s_list)\ny_true = [f for f in y]\ndel X, y","0a0435ad":"msgd = GradientDescents()","b936a115":"gd = msgd.momentum_gradient_descent(X_true, y_true, learning_rate=0.000003, max_steps=1000)","82b159e3":"y_pred_algo_1 = []\nfor X_current in X_true:\n  y_pred_algo_1.append(sum([w*x for w,x in zip(gd['weights'],X_current)]))\n\nmse_algo_1 = msgd.mse_function(y_pred_algo_1, y_true)","c580092c":"gd_next = msgd.momentum_gradient_descent(X_true, y_true, learning_rate=0.000003, max_steps=10000,\\\n                                         weights = gd['weights'])","52ccdee3":"y_pred_algo_2 = []\nfor X_current in X_true:\n  y_pred_algo_2.append(sum([w*x for w,x in zip(gd_next['weights'],X_current)]))\n\nmse_algo_2 = msgd.mse_function(y_pred_algo_2, y_true)","f11d3eea":"mse_algo_1","9f1aabdd":"mse_algo_2","fbd16fee":"gd_next = msgd.momentum_gradient_descent(X_true, y_true, learning_rate=0.000003, max_steps=50000,\\\n                                         weights = gd_next['weights'])","c8eecbec":"y_pred_algo_3 = []\nfor X_current in X_true:\n  y_pred_algo_3.append(sum([w*x for w,x in zip(gd_next['weights'],X_current)]))\n\nmse_algo_3 = msgd.mse_function(y_pred_algo_3, y_true)","927697c7":"mse_algo_3","bbdceeb1":"I will implement both and see where the MSE is less after the same number of steps","a351ce31":"Tests show that the first model is more efficient. We will put it in the class","9855384d":"## Theory\nStochastic Gradient Descent with Momentum is built on Stochastic Gradient Descent. It is fed with a random vector from known data in the training dataset.<br> All the work on calculating the gradient is happening one line at a time. Then we move on to the next random line.<br> It takes a long time, so to speed everything up, we use momentum","311b1999":"# Test on Boston Dataset","2330ab01":"We take old class from [multivariant gradient descent](https:\/\/www.kaggle.com\/konstantinsuspitsyn\/multivariate-gradient-descent) and we keep progress_tracker & mse_function","43ebbd0d":"Realisation\n1. [An overview of gradient descent optimization algorithms](https:\/\/arxiv.org\/pdf\/1609.04747.pdf):<br>\n$v_t = \\gamma * v_{t-1} + \\alpha \\nabla_\\theta f(\\theta)$<br>\n$\\theta = \\theta - v_t$","b66b6f20":"# Stochastic gradient descent with momentum from scratch","91221348":"This is a series on Jupiter Notebooks:\n1. [Gradient descent for formulas like y = a * x + b](https:\/\/www.kaggle.com\/konstantinsuspitsyn\/gradient-descent-for-formulas-like-y-a-x-b)\n2. [Multivariate Gradient Descent](https:\/\/www.kaggle.com\/konstantinsuspitsyn\/multivariate-gradient-descent)\n3. Stochastic gradient descent with momentum\n4. [ADAM](https:\/\/www.kaggle.com\/konstantinsuspitsyn\/adam-from-scratch)","454c5ff5":"# Final Class","01d871c3":"Very interesting, but I found 2 kinds of formulas in different articles: <br>\n1. [An overview of gradient descent optimization algorithms](https:\/\/arxiv.org\/pdf\/1609.04747.pdf): <br>\n$ v_t = \\gamma * v_ {t-1} + \\alpha \\nabla_ \\theta f (\\theta) $ <br>\n$ \\theta = \\theta - v_t $\n2. [An Improved Analysis of Stochastic Gradient Descentwith Momentum](https:\/\/arxiv.org\/pdf\/2007.07989.pdf): <br>\n$ v_t = \\gamma * v_ {t-1} + (1- \\gamma) \\nabla_ \\theta f (\\theta) $ <br>\n$ \\theta = \\theta - \\alpha v_t $ <br>\nWhere: <br> $ \\nabla_ \\theta f (\\theta) $ - partial derivative with respect to $ \\theta $ <br>\n$ \\gamma $ - acceleration weight. $ \\gamma \\in [0,1) $ <br>\n$ \\alpha $ - learning rate (shift step)","24c31ecb":"Of course, it would be more correct to calculate MSE on a test dataset, but we see that the result improves as we train","99f905db":"Try second algorithm<br>\n2. [An Improved Analysis of Stochastic Gradient Descentwith Momentum](https:\/\/arxiv.org\/pdf\/2007.07989.pdf):<br>\n$v_t = \\gamma * v_{t-1} + (1-\\gamma) \\nabla_\\theta f(\\theta)$<br>\n$\\theta = \\theta - \\alpha v_t$<br>"}}