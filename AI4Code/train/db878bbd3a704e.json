{"cell_type":{"01379f2e":"code","8e8596f9":"code","a30ff9a3":"code","541113b8":"code","27064178":"code","7f377489":"code","7ee89daf":"code","cfea52c7":"code","72be119c":"code","7c25145b":"code","8b33b4a0":"code","8d1674eb":"code","bfe5ddf5":"code","64554b54":"code","efa72c57":"code","6ae18fc5":"code","519455f0":"code","fa6b1100":"code","142d8711":"code","78e21fc3":"code","fbe7abb9":"code","2295428e":"code","9eedc423":"code","217a593e":"code","9380af3d":"code","9ab63a0e":"code","57154ff7":"code","451e1c17":"code","2461ee0f":"code","6cb361c5":"code","96bb9b44":"code","307fe9e1":"code","7ec6e614":"code","d9a3a389":"code","51a79c9b":"code","6157385c":"markdown","d106e563":"markdown","0819eebe":"markdown","eaf4c254":"markdown","75816dc3":"markdown","21e3552e":"markdown","3b4f945a":"markdown","28edca36":"markdown","55a2c5c8":"markdown","3cc0cff3":"markdown","cf620737":"markdown","a73b210e":"markdown","f12bcdb2":"markdown","ec73ed76":"markdown","b0729007":"markdown","911588a3":"markdown","3c3c4202":"markdown","bb6651e6":"markdown","51819472":"markdown","660f47bc":"markdown","6028c7fb":"markdown","ad8a685b":"markdown","24839a51":"markdown","6ad33384":"markdown","9fbc893f":"markdown","78117b84":"markdown"},"source":{"01379f2e":"import numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8e8596f9":"print(os.listdir(\"..\/input\/\"))","a30ff9a3":"app_train = pd.read_csv('..\/input\/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","541113b8":"app_test = pd.read_csv('..\/input\/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","27064178":"app_train['TARGET'].value_counts()","7f377489":"app_train['TARGET'].astype(int).plot.hist();","7ee89daf":"def missing_values_table(df):\n    \n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        \n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        return mis_val_table_ren_columns","cfea52c7":"missing_values = missing_values_table(app_train)\nmissing_values.head(20)","72be119c":"app_train.dtypes.value_counts()","7c25145b":"app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","8b33b4a0":"le = LabelEncoder()\nle_count = 0\n\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        \n        if len(list(app_train[col].unique())) <= 2:\n            \n            le.fit(app_train[col])\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","8d1674eb":"app_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","bfe5ddf5":"train_labels = app_train['TARGET']\n\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","64554b54":"correlations = app_train.corr()['TARGET'].sort_values()\n\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","efa72c57":"app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","6ae18fc5":"plt.style.use('fivethirtyeight')\nplt.hist(app_train['DAYS_BIRTH'] \/ 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","519455f0":"plt.figure(figsize = (10, 8))\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] \/ 365, label = 'target == 0')\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] \/ 365, label = 'target == 1')\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","fa6b1100":"age_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] \/ 365\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","142d8711":"age_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","78e21fc3":"plt.figure(figsize = (8, 8))\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","fbe7abb9":"ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","2295428e":"plt.figure(figsize = (8, 6))\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","9eedc423":"plt.figure(figsize = (10, 12))\n\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    plt.subplot(3, 1, i + 1)\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","217a593e":"plot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\nplot_data = plot_data.dropna().loc[:100000, :]\n\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\ngrid.map_upper(plt.scatter, alpha = 0.2)\ngrid.map_diag(sns.kdeplot)\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","9380af3d":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \nfeatures = list(train.columns)\ntest = app_test.copy()\n\nimputer = Imputer(strategy = 'median')\nscaler = MinMaxScaler(feature_range = (0, 1))\n\nimputer.fit(train)\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","9ab63a0e":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(C = 0.0001)\nlog_reg.fit(train, train_labels)","57154ff7":"log_reg_pred = log_reg.predict_proba(test)[:, 1]","451e1c17":"submit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","2461ee0f":"submit.to_csv('log_reg_baseline.csv', index = False)","6cb361c5":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","96bb9b44":"random_forest.fit(train, train_labels)\n\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\npredictions = random_forest.predict_proba(test)[:, 1]","307fe9e1":"submit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\nsubmit.to_csv('random_forest_baseline.csv', index = False)","7ec6e614":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n \n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    labels = features['TARGET']\n    \n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        cat_indices = 'auto'\n    \n    elif encoding == 'le':\n        label_encoder = LabelEncoder()\n        \n        cat_indices = []\n        \n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                cat_indices.append(i)\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    feature_names = list(features.columns)\n    \n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    feature_importance_values = np.zeros(len(feature_names))\n    test_predictions = np.zeros(test_features.shape[0])  \n    out_of_fold = np.zeros(features.shape[0])\n    \n    valid_scores = []\n    train_scores = []\n    \n    for train_indices, valid_indices in k_fold.split(features):\n        \n        train_features, train_labels = features[train_indices], labels[train_indices]\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        best_iteration = model.best_iteration_\n        \n        feature_importance_values += model.feature_importances_ \/ k_fold.n_splits\n        \n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] \/ k_fold.n_splits\n        \n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","d9a3a389":"submission, fi, metrics = model(app_train, app_test)\nprint('Baseline metrics')\nprint(metrics)","51a79c9b":"submission.to_csv('baseline_lgb.csv', index = False)","6157385c":"### Exterior Sources\n\nThe 3 variables with the strongest negative correlations with the target are `EXT_SOURCE_1`, `EXT_SOURCE_2`, and `EXT_SOURCE_3`.","d106e563":"All three `EXT_SOURCE` featureshave negative correlations with the target, indicating that as the value of the `EXT_SOURCE` increases, the client is more likely to repay the loan. We can also see that `DAYS_BIRTH` is positively correlated with `EXT_SOURCE_1` indicating that maybe one of the factors in this score is the client age.","0819eebe":"This model should score 0.678 when submitted.","eaf4c254":"## Pairs Plot\n\nAs a final exploratory plot, we can make a pairs plot of the `EXT_SOURCE` variables and the `DAYS_BIRTH` variable. ","75816dc3":"## Imports\n\nWe are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`. ","21e3552e":"In this plot, the red indicates loans that were not repaid and the blue are loans that are paid. We can see the different relationships within the data. There does appear to be a moderate positive linear relationship between the `EXT_SOURCE_1` and the `DAYS_BIRTH` (or equivalently `YEARS_BIRTH`), indicating that this feature may take into account the age of the client. ","3b4f945a":"# Home Credit Default Risk Competition\n\nIn this notebook, we will take an initial look at the Home Credit default risk machine learning competition currently hosted on Kaggle. The objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan. This is a standard supervised classification task:\n\n\n\n# Data\n\nThe data is provided by [Home Credit](http:\/\/www.homecredit.net\/about-us.aspx), a service dedicated to provided lines of credit (loans) to the unbanked population. Predicting whether or not a client will repay a loan or have difficulty is a critical business need, and Home Credit is hosting this competition on Kaggle to see what sort of models the machine learning community can develop to help them in this task. \n\n","28edca36":"`EXT_SOURCE_3` displays the greatest difference between the values of the target. We can clearly see that this feature has some relationship to the likelihood of an applicant to repay a loan. The relationship is not very strong, but these variables will still be useful for a machine learning model to predict whether or not an applicant will repay a loan on time.","55a2c5c8":"When it comes time to build our machine learning models, we will have to fill in these missing values (imputation). ","3cc0cff3":"## Examine the Distribution of the Target Column\n\nThe target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. ","cf620737":"### Label Encoding and One-Hot Encoding\nFor label encoding, we use the Scikit-Learn `LabelEncoder` and for one-hot encoding, the pandas `get_dummies(df)` function.","a73b210e":"The test set is considerably smaller and lacks a `TARGET` column. ","f12bcdb2":"## Read in Data \n\nFirst, we can list all the available data files. There are a total of 9 files: 1 main file for training (with target) 1 main file for testing (without the target), 1 example submission file, and 6 other files containing additional information about each loan. ","ec73ed76":"The logistic regression baseline score around 0.671 when submitted.","b0729007":"## Examine Missing Values\n\nNext we can check the number and percentage of missing values in each column. ","911588a3":"# Baseline\n\nTo get a baseline, we will use all of the features after encoding the categorical variables. We will preprocess the data by filling in the missing values (imputation) and normalizing the range of the features (feature scaling). ","3c3c4202":"This submission should score about 0.735 on the leaderboard. We will certainly best that in future work! ","bb6651e6":"## Column Types\n\nLet's look at the number of columns of each data type. `int64` and `float64` are numeric variables. `object` columns contain strings and are categorical features.","51819472":"### Effect of Age on Repayment","660f47bc":"# Light Gradient Boosting Machine\n\nThe Gradient Boosting Machine is currently the leading model for learning on structured datasets. ","6028c7fb":"There are far more loans that were repaid on time than loans that were not repaid. ","ad8a685b":"## Encoding Categorical Variables\n\n A machine learning model unfortunately cannot deal with categorical variables.\n\n* Label encoding: assign each unique category in a categorical variable with an integer. No new columns are created. \n* One-hot encoding: create a new column for each unique category in a categorical variable. Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns. \n\nIn this competition, we will use Label Encoding for any categorical variables with only 2 categories and One-Hot Encoding for any categorical variables with more than 2 categories. ","24839a51":"# Exploratory Data Analysis\n\nExploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. \nThe goal of EDA is to learn what our data can tell us. ","6ad33384":"The training data has 307511 observations (each one a separate loan) and 122 features (variables) including the `TARGET` (the label we want to predict).","9fbc893f":"### Correlations\n\nlet's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the `.corr` dataframe method.","78117b84":"## Improved Model: Random Forest\n\nThe Random Forest is a much more powerful model especially when we use hundreds of trees. We will use 100 trees in the random forest."}}