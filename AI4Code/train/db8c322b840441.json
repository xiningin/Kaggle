{"cell_type":{"4b3b74aa":"code","f9aa3ff4":"code","2785c65a":"code","6105dc73":"code","f4514274":"code","993a3358":"code","85661f48":"code","910f833b":"code","c98d3f8a":"code","c7930648":"code","0ad658ac":"code","d652fa52":"code","7278a981":"code","dcf0acde":"code","31408a9c":"code","15e9c244":"code","5f25a123":"code","2c701d2f":"code","3e09304d":"code","a63cd77a":"code","f4eab222":"code","0cd1bfad":"code","e2c3f0ea":"code","9fe3919b":"code","ce052249":"code","ff919d4b":"code","433d2938":"code","1cd536d5":"code","114fcdba":"code","5ea08ae9":"code","c8d008f6":"code","3fc8e4d6":"code","821f0f8b":"code","26426480":"code","e43da5ab":"code","955b30a5":"code","8761d8fc":"code","3265a37f":"code","fc13d861":"code","205a5a5f":"code","8a194b6c":"code","646a61c7":"code","61ae60b5":"code","0a38f54f":"code","de86157b":"code","2e28c0ab":"code","c123c386":"code","73d666aa":"code","b930f799":"code","c7ddc587":"code","49f9c571":"code","69585809":"code","2719f874":"code","3b60950e":"markdown","ccec4efd":"markdown","180e017e":"markdown","4897fa38":"markdown","81154941":"markdown","adb7e951":"markdown","e23ceb35":"markdown","17017171":"markdown","f31e7e67":"markdown","bb31001d":"markdown","f602d9fa":"markdown","78a38bc2":"markdown","f3b9d334":"markdown","f95c772a":"markdown","074f8909":"markdown","5848e126":"markdown","2b5b7059":"markdown","225030f8":"markdown","05369a6c":"markdown","58e5c845":"markdown","40703593":"markdown","f65030c8":"markdown","2b967e3e":"markdown","160a31cb":"markdown","58905f9f":"markdown","8539a3b4":"markdown","6674ebc3":"markdown","3eb291ba":"markdown","484fc284":"markdown","79e7da56":"markdown","7ce2a8da":"markdown","efee54bd":"markdown"},"source":{"4b3b74aa":"'''Ignore deprecation and future, and user warnings.'''\nimport warnings as wrn\nwrn.filterwarnings('ignore', category = DeprecationWarning) \nwrn.filterwarnings('ignore', category = FutureWarning) \nwrn.filterwarnings('ignore', category = UserWarning) \n\n'''Import basic modules.'''\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n'''Customize visualization\nSeaborn and matplotlib visualization.'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n%matplotlib inline\n\n'''Plotly visualization .'''\nimport plotly.offline as py\nfrom plotly.offline import iplot, init_notebook_mode\nimport plotly.graph_objs as go\ninit_notebook_mode(connected = True) # Required to use plotly offline in jupyter notebook\n\n'''Display markdown formatted output like bold, italic bold etc.'''\nfrom IPython.display import Markdown\ndef bold(string):\n    display(Markdown(string))","f9aa3ff4":"'''Read in export and import data from CSV file'''\ndf_train = pd.read_csv('..\/input\/bigquery-geotab-intersection-congestion\/train.csv')\ndf_test = pd.read_csv('..\/input\/bigquery-geotab-intersection-congestion\/test.csv')","2785c65a":"'''Train and Test data at a glance.'''\nbold('**Preview of Train Data:**')\ndisplay(df_train.head())\nbold('**Preview of Test Data:**')\ndisplay(df_test.head())","6105dc73":"'''Dimension of train and test data'''\nbold('**Shape of our train and test data**')\nprint('Dimension of train:',df_train.shape) \nprint('Dimension of test:',df_test.shape)","f4514274":"'''Funtion for Variable Description'''\ndef description(df):\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.iloc[0].values\n    summary['Second Value'] = df.iloc[1].values\n    summary['Third Value'] = df.iloc[2].values\n    return summary","993a3358":"bold('**Variable Description of  train Data:**')\ndisplay(description(df_train))\nbold('**Variable Description of  test Data:**')\ndisplay(description(df_test))","85661f48":"'''Visulization of IntersectionID'''\nplt.figure(figsize=(15,6))\ndf_train.IntersectionId.value_counts()[:50].plot(kind='bar', color = 'teal')\nplt.xlabel(\"Intersection Number\", fontsize=18)\nplt.ylabel(\"Count\", fontsize=18)\nplt.title(\"TOP 50 most commmon IntersectionID's \", fontsize=22)\nplt.show()","910f833b":"'''Visulization of Entry\/Exit StreetNames'''\nplt.figure(figsize=(15,6))\ndf_train['EntryStreetName'].value_counts()[:50].plot(kind='bar', color = 'darkred')\nplt.xlabel(\"Entry Street Names\", fontsize=18)\nplt.ylabel(\"Count\", fontsize=18)\nplt.title(\"TOP 50 most Entry Street Names \", fontsize=22)\nplt.show()\n\nplt.figure(figsize=(15,6))\ndf_train['ExitStreetName'].value_counts()[:50].plot(kind='bar', color = 'darkgreen')\nplt.xlabel(\"Exit Street Names\", fontsize=18)\nplt.ylabel(\"Count\", fontsize=18)\nplt.title(\"TOP 50 most Exit Street Names \", fontsize=22)\nplt.show()","c98d3f8a":"'''Visulization of Path'''\nplt.figure(figsize=(15,6))\ndf_train.Path.value_counts()[:50].plot(kind='bar', color = 'teal')\nplt.xlabel(\"Path\", fontsize=18)\nplt.ylabel(\"Count\", fontsize=18)\nplt.title(\"TOP 50 most commmon Paths\", fontsize=22)\nplt.show()","c7930648":"sns.set_style(\"dark\")\nfig, ax = plt.subplots(2,1, figsize=[15, 12])\n\nsns.countplot(data = df_train, x = 'EntryHeading', ax = ax[0], palette = 'YlOrRd_r')\nax[0].set_title('Count plot of Entry Heading', fontsize = 22)\nax[0].set_xlabel('Entry Heady', fontsize = 18)\n\nsns.countplot(data = df_train, x = 'ExitHeading', ax = ax[1], palette = 'YlGnBu')\nax[1].set_title('Count plot of Exit Heading', fontsize = 22)\nax[1].set_xlabel('Exit Heady', fontsize = 18)\n\nplt.subplots_adjust(hspace = 0.3)\nplt.show()","0ad658ac":"total = len(df_train)\nplt.figure(figsize=(15,6))\nsns.set_style(\"white\")\n\nax = sns.countplot(x = \"City\", data = df_train, palette = 'Dark2')\nax.set_title(\"City Count Distribution\", fontsize=20)\nax.set_ylabel(\"Count\",fontsize= 17)\nax.set_xlabel(\"City Names\", fontsize=17)\nsizes=[]\nfor p in ax.patches:\n    height = p.get_height()\n    sizes.append(height)\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=14) \nax.set_ylim(0, max(sizes) * 1.15)\n\nplt.show()","d652fa52":"fig, ax = plt.subplots(2,1, figsize=[15, 12])\n\nsns.countplot(data = df_train[df_train['Weekend']==0], x = 'Hour', hue = 'City', ax = ax[0], palette = 'Dark2')\nax[0].legend()\nax[0].set_title('Count Distribution of Hour by Week days ', fontsize = 22)\nax[0].set_xlabel('Hour', fontsize = 18)\n\nsns.countplot(data = df_train[df_train['Weekend']==1], x = 'Hour', hue = 'City', ax = ax[1], palette = 'Dark2')\nax[1].legend()\nax[1].set_title('Count Distribution of Hour by Weekend days', fontsize = 22)\nax[1].set_xlabel('Hour', fontsize = 18)\n\nplt.subplots_adjust(hspace = 0.3)\nplt.show()","7278a981":"plt.figure(figsize=(15,6))\nax = sns.countplot(x = \"Month\", data = df_train, hue = 'City',palette = 'Dark2')\nax.set_title(\"Month Distribution by cities\", fontsize=20)\nax.set_ylabel(\"Count\",fontsize= 17)\nax.set_xlabel(\"Month\", fontsize=17)\nplt.show()","dcf0acde":"'''Visualition of Map Plot of Atlanta city'''\nimport mplleaflet\nplt.figure(figsize=(10,10))\nmap1 = df_train[df_train['City']=='Atlanta'].groupby(['Latitude', 'Longitude'])['RowId'].count().reset_index()\nplt.scatter(map1['Longitude'], map1['Latitude'], alpha=0.5)\n\nmplleaflet.display()","31408a9c":"'''Visualition of Map Plot of Boston city'''\nplt.figure(figsize=(10,10))\nmap2 = df_train[df_train['City']=='Boston'].groupby(['Latitude', 'Longitude'])['RowId'].count().reset_index()\nplt.scatter(map2['Longitude'], map2['Latitude'], alpha=0.5)\n\nmplleaflet.display()","15e9c244":"'''Visualition of Map Plot of Philadelphia city'''\nplt.figure(figsize=(10,10))\nmap3 = df_train[df_train['City']=='Philadelphia'].groupby(['Latitude', 'Longitude'])['RowId'].count().reset_index()\nplt.scatter(map3['Longitude'], map3['Latitude'], alpha=0.5)\n\nmplleaflet.display()","5f25a123":"'''Visualition of Map Plot of Chicago city'''\nplt.figure(figsize=[10, 10])\nmap4 = df_train[df_train['City']=='Chicago'].groupby(['Latitude', 'Longitude'])['RowId'].count().reset_index()\nsns.kdeplot(map4['Longitude'], map4['Latitude'])\n\nmplleaflet.display()","2c701d2f":"fig, ax = plt.subplots(nrows=2, ncols=2)\nsns.set_style(\"whitegrid\")\n\ndf_train[df_train['City']=='Atlanta'].groupby('Hour')['TotalTimeStopped_p80'].mean().plot(\n    ax=ax[0,0],title=\"Atlanda's Total Stoppage Time in Hours\", color='r', figsize=(18,15))\n\ndf_train[df_train['City']=='Boston'].groupby('Hour')['TotalTimeStopped_p80'].mean().plot(\n    ax=ax[0,1],title=\"Boston's Total Stoppage Time in Hours\", color='r', figsize=(18,15))\n\n\ndf_train[df_train['City']=='Chicago'].groupby('Hour')['TotalTimeStopped_p80'].mean().plot(\n    ax=ax[1,0],title=\"Chicago's Total Stoppage Time in Hours\", color='r', figsize=(18,15))\n\n\ndf_train[df_train['City']=='Philadelphia'].groupby('Hour')['TotalTimeStopped_p80'].mean().plot(\n    ax=ax[1,1],title=\"Philadelphia's Total Stoppage Time in Hours\", color='r', figsize=(18,15))\n\nplt.show()","3e09304d":"df_train['Intersection'] = df_train['IntersectionId'].astype(str) + df_train['City']\ndf_test['Intersection'] = df_test['IntersectionId'].astype(str) + df_test['City']\nprint(df_train['Intersection'].sample(6).values)","a63cd77a":"from sklearn.preprocessing import LabelEncoder \nle = LabelEncoder()\nle.fit(pd.concat([df_train['Intersection'], df_test['Intersection']]).drop_duplicates().values)\ndf_train['Intersection'] = le.transform(df_train['Intersection'])\ndf_test['Intersection'] = le.transform(df_test['Intersection'])\nprint(df_train['Intersection'].sample(6).values)","f4eab222":"# Reference: https:\/\/www.kaggle.com\/bgmello\/how-one-percentile-affect-the-others\n\n'''Let's use the following road types: Street, Avenue, Road, Boulevard, Broad and Drive'''\nroad_encoding = {\n    'Road': 1,\n    'Street': 2,\n    'Avenue': 2,\n    'Drive': 3,\n    'Broad': 3,\n    'Boulevard': 4\n}","0cd1bfad":"def encode(x):\n    if pd.isna(x):\n        return 0\n    for road in road_encoding.keys():\n        if road in x:\n            return road_encoding[road]\n        \n    return 0","e2c3f0ea":"df_train['EntryTypeStreet'] = df_train['EntryStreetName'].apply(encode)\ndf_train['ExitTypeStreet'] = df_train['ExitStreetName'].apply(encode)\ndf_test['EntryTypeStreet'] = df_test['EntryStreetName'].apply(encode)\ndf_test['ExitTypeStreent'] = df_test['ExitStreetName'].apply(encode)\nprint(df_train['EntryTypeStreet'].sample(10).values)","9fe3919b":"df_train[\"same_street_exact\"] = (df_train[\"EntryStreetName\"] ==  df_train[\"ExitStreetName\"]).astype(int)\ndf_test[\"same_street_exact\"] = (df_test[\"EntryStreetName\"] ==  df_test[\"ExitStreetName\"]).astype(int)","ce052249":"'''Defineing the directions'''\ndirections = {\n    'N': 0,\n    'NE': 1\/4,\n    'E': 1\/2,\n    'SE': 3\/4,\n    'S': 1,\n    'SW': 5\/4,\n    'W': 3\/2,\n    'NW': 7\/4\n}","ff919d4b":"df_train['EntryHeading'] = df_train['EntryHeading'].map(directions)\ndf_train['ExitHeading'] = df_train['ExitHeading'].map(directions)\n\ndf_test['EntryHeading'] = df_test['EntryHeading'].map(directions)\ndf_test['ExitHeading'] = df_test['ExitHeading'].map(directions)\n\ndf_train['diffHeading'] = df_train['EntryHeading']- df_train['ExitHeading']  \ndf_test['diffHeading'] = df_test['EntryHeading']- df_test['ExitHeading']\n\ndisplay(df_train[['ExitHeading','EntryHeading','diffHeading']].drop_duplicates().head(5))","433d2938":"#def cyclical_encode(data, col, max_val):\n#    data[col + '_sin'] = np.sin(2 * np.pi * data[col]\/max_val)\n#    data[col + '_cos'] = np.cos(2 * np.pi * data[col]\/max_val)\n#    return data\n\n#df_train = cyclical_encode(df_train, 'Hour', 24)\n#df_test = cyclical_encode(df_test, 'Hour', 24) ","1cd536d5":"#'''One Hot Ecoding of month feature'''\n#df_train = pd.concat([df_train, pd.get_dummies(df_train['Month'], prefix='Month', drop_first = False)], axis=1)\n#df_test = pd.concat([df_test, pd.get_dummies(df_test['Month'], prefix='Month', drop_first = False)], axis=1)","114fcdba":"'''One Hot Ecoding of City feature'''\ndf_train = pd.concat([df_train, pd.get_dummies(df_train['City'], drop_first = False)], axis=1)\ndf_test = pd.concat([df_test, pd.get_dummies(df_test['City'], drop_first = False)], axis=1)","5ea08ae9":"\"\"\"Adding temperature (\u00b0F) of each city by month\"\"\"\n# Reference: https:\/\/www.kaggle.com\/dcaichara\/feature-engineering-and-lightgbm\nmonthly_avg = {'Atlanta1': 43.0, 'Atlanta5': 68.5, 'Atlanta6': 76.0, 'Atlanta7': 78.0, 'Atlanta8': 78.0, 'Atlanta9': 72.5,\n              'Atlanta10': 62.0, 'Atlanta11': 52.5, 'Atlanta12': 45.0, 'Boston1': 29.5, 'Boston5': 58.5, 'Boston6': 68.0,\n              'Boston7': 74.0, 'Boston8': 73.0, 'Boston9': 65.5, 'Boston10': 54.5,'Boston11': 45.0, 'Boston12': 35.0,\n              'Chicago1': 27.0, 'Chicago5': 59.5, 'Chicago6': 70.0, 'Chicago7': 76.0, 'Chicago8': 75.5, 'Chicago9': 68.0,\n              'Chicago10': 56.0,  'Chicago11': 44.5, 'Chicago12': 32.0, 'Philadelphia1': 34.5, 'Philadelphia5': 66.0,\n              'Philadelphia6': 75.5, 'Philadelphia7': 80.5, 'Philadelphia8': 78.5, 'Philadelphia9': 71.5, 'Philadelphia10': 59.5,\n              'Philadelphia11': 49.0, 'Philadelphia12': 40.0}\n# Concatenating the city and month into one variable\ndf_train['city_month'] = df_train[\"City\"] + df_train[\"Month\"].astype(str)\ndf_test['city_month'] = df_test[\"City\"] + df_test[\"Month\"].astype(str)\n\n# Creating a new column by mapping the city_month variable to it's corresponding average monthly temperature\ndf_train[\"average_temp\"] = df_train['city_month'].map(monthly_avg)\ndf_test[\"average_temp\"] = df_test['city_month'].map(monthly_avg)","c8d008f6":"\"\"\"Adding rainfall (inches) of each city by month\"\"\"\nmonthly_rainfall = {'Atlanta1': 5.02, 'Atlanta5': 3.95, 'Atlanta6': 3.63, 'Atlanta7': 5.12, 'Atlanta8': 3.67, 'Atlanta9': 4.09,\n              'Atlanta10': 3.11, 'Atlanta11': 4.10, 'Atlanta12': 3.82, 'Boston1': 3.92, 'Boston5': 3.24, 'Boston6': 3.22,\n              'Boston7': 3.06, 'Boston8': 3.37, 'Boston9': 3.47, 'Boston10': 3.79,'Boston11': 3.98, 'Boston12': 3.73,\n              'Chicago1': 1.75, 'Chicago5': 3.38, 'Chicago6': 3.63, 'Chicago7': 3.51, 'Chicago8': 4.62, 'Chicago9': 3.27,\n              'Chicago10': 2.71,  'Chicago11': 3.01, 'Chicago12': 2.43, 'Philadelphia1': 3.52, 'Philadelphia5': 3.88,\n              'Philadelphia6': 3.29, 'Philadelphia7': 4.39, 'Philadelphia8': 3.82, 'Philadelphia9':3.88 , 'Philadelphia10': 2.75,\n              'Philadelphia11': 3.16, 'Philadelphia12': 3.31}\n# Creating a new column by mapping the city_month variable to it's corresponding average monthly rainfall\ndf_train[\"average_rainfall\"] = df_train['city_month'].map(monthly_rainfall)\ndf_test[\"average_rainfall\"] = df_test['city_month'].map(monthly_rainfall)","3fc8e4d6":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nlat_long = ['Latitude', 'Longitude']\nfor col in lat_long:\n    df_train[col] = (scaler.fit_transform(df_train[col].values.reshape(-1, 1)))\n    df_test[col] = (scaler.fit_transform(df_test[col].values.reshape(-1, 1)))","821f0f8b":"\"\"\"Let's see the columns of data\"\"\"\ndf_train.columns.values","26426480":"\"\"\"Let's drop the unwanted variables from test and train dataset\"\"\"\ndf_train.drop(['RowId', 'IntersectionId', 'EntryStreetName', 'ExitStreetName', 'Path', 'city_month', 'City'], axis=1, inplace=True)\ndf_test.drop(['RowId', 'IntersectionId', 'EntryStreetName', 'ExitStreetName', 'Path', 'city_month', 'City'], axis=1, inplace=True)","e43da5ab":"\"\"\"Let\u2019s make a correlation matrix heatmap for the data set.\"\"\"\nplt.figure(figsize=(18,14))\nsns.heatmap(df_train.corr(),vmin=-1, vmax=1, center=0,\n            square=True, cmap = sns.diverging_palette(20, 220, n=200))\nplt.show()","955b30a5":"\"\"\"Let's look at our final train and test data for modelling.\"\"\"\nbold('**Updated train data for modelling:**')\ndisplay(df_train.head(3))\nbold('**Updated test data for modelling:**')\ndisplay(df_test.head(3))","8761d8fc":"'''Function to reduce the DF size'''\n# source: https:\/\/www.kaggle.com\/kernels\/scriptcontent\/3684066\/download\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","3265a37f":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","fc13d861":"'''Seting X and Y'''\ntarget_var = df_train.iloc[:, 7:22]\nX_train = df_train.drop(target_var,axis = 1)\n\ny1_train = df_train[\"TotalTimeStopped_p20\"]\ny2_train = df_train[\"TotalTimeStopped_p50\"]\ny3_train = df_train[\"TotalTimeStopped_p80\"]\ny4_train = df_train[\"DistanceToFirstStop_p20\"]\ny5_train = df_train[\"DistanceToFirstStop_p50\"]\ny6_train = df_train[\"DistanceToFirstStop_p80\"]\n\nX_test = df_test","205a5a5f":"\"\"\"Let's have a final look at our data\"\"\"\nbold('**Data Dimension for Model Building:**')\nprint('Input matrix dimension:', X_train.shape)\nprint('Output vector dimension:',y1_train.shape)\nprint('Test data dimension:', X_test.shape)","8a194b6c":"description(X_train)","646a61c7":"\"\"\"pecifying categorical features\"\"\"\ncat_feat = ['Hour', 'Weekend','Month', 'same_street_exact', 'Intersection',\n       'Atlanta', 'Boston', 'Chicago', 'Philadelphia', 'EntryTypeStreet', 'ExitTypeStreet']","61ae60b5":"all_preds ={0:[],1:[],2:[],3:[],4:[],5:[]}\nall_target = [y1_train, y2_train, y3_train, y4_train, y5_train, y6_train]","0a38f54f":"# Reference: \n# https:\/\/medium.com\/analytics-vidhya\/hyperparameters-optimization-for-lightgbm-catboost-and-xgboost-regressors-using-bayesian-6e7c495947a9\n# https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html#for-faster-speed\n\n'''Importing Libraries'''\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\n\ndtrain = lgb.Dataset(data=X_train, label=y1_train)\n\n'''Define objective function'''\ndef hyp_lgbm(num_leaves, feature_fraction, bagging_fraction, max_depth, min_split_gain, min_child_weight, lambda_l1, lambda_l2):\n      \n        params = {'application':'regression','num_iterations': 400,\n                  'learning_rate':0.01,\n                  'metric':'rmse'} # Default parameters\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['min_split_gain'] = min_split_gain\n        params['min_child_weight'] = min_child_weight\n        params['lambda_l1'] = lambda_l1\n        params['lambda_l2'] = lambda_l2\n        \n        cv_results = lgb.cv(params, dtrain, nfold=5, seed=44, categorical_feature=cat_feat, stratified=False,\n                            verbose_eval =None)\n#         print(cv_results)\n        return -np.min(cv_results['rmse-mean'])","de86157b":"''' Define search space of hyperparameters'''\npds = {'num_leaves': (100, 230),\n          'feature_fraction': (0.1, 0.5),\n          'bagging_fraction': (0.8, 1),\n          'lambda_l1': (0,3),\n          'lambda_l2': (0,5),\n          'max_depth': (8, 19),\n          'min_split_gain': (0.001, 0.1),\n          'min_child_weight': (1, 20)\n        }","2e28c0ab":"'''Define a surrogate model of the objective function and call it.'''\noptimizer = BayesianOptimization(hyp_lgbm,pds,random_state=44)\n                                  \n# Optimize\noptimizer.maximize(init_points=5, n_iter=12)","c123c386":"'''Best parameters after optimization'''\noptimizer.max","73d666aa":"p = optimizer.max['params']\nparam = {'num_leaves': int(round(p['num_leaves'])),\n         'feature_fraction': p['feature_fraction'],\n         'bagging_fraction': p['bagging_fraction'],\n         'max_depth': int(round(p['max_depth'])),\n         'lambda_l1': p['lambda_l1'],\n         'lambda_l2':p['lambda_l2'],\n         'min_split_gain': p['min_split_gain'],\n         'min_child_weight': p['min_child_weight'],\n         'learing_rate':0.05,\n         'objective': 'regression',\n         'boosting_type': 'gbdt',\n         'verbose': 1,\n         'seed': 44,\n         'metric': 'rmse'\n        }\nparam","b930f799":"'''Instantiate the models with optimized hyperparameters.'''\ntrain = X_train \ntest = X_test \nfrom sklearn.model_selection import train_test_split\n\nfor i in range(len(all_preds)):\n    print('Training and predicting for target {}'.format(i+1))\n    X_train,X_test,y_train,y_test=train_test_split(train,all_target[i], test_size=0.2, random_state=31)\n    xg_train = lgb.Dataset(X_train,\n                           label = y_train\n                           )\n    xg_valid = lgb.Dataset(X_test,\n                           label = y_test\n                           )\n    clf = lgb.train(param, xg_train, 10000, valid_sets = [xg_valid],categorical_feature=cat_feat,\n                         verbose_eval=100, early_stopping_rounds = 200)\n    all_preds[i] = clf.predict(test, num_iteration=clf.best_iteration)","c7ddc587":"submission = pd.read_csv('..\/input\/bigquery-geotab-intersection-congestion\/sample_submission.csv')\n#submission.head()","49f9c571":"dt = pd.DataFrame(all_preds).stack()\ndt = pd.DataFrame(dt)\nsubmission['Target'] = dt[0].values","69585809":"submission.head()","2719f874":"submission.to_csv('lgbm2_submission.csv', index=False)","3b60950e":"# 2. Variable Description, Identification, and Correction","ccec4efd":"## 3.1 Intersection ID","180e017e":"## 3.7 Total Time Stopped\nIn this section, we are going analysis the total time stopped on the intersections in different cities.","4897fa38":"# Work in Progress: Stay Tuned\n# <font color='red'>Give me your feedback and if you find my kernel helpful please UPVOTE will be appreciated.<\/font>\n## --Thank You for Reading","81154941":"## 3.4 Entry\/Exit Heading","adb7e951":"**Finding:**\n* we can notice that during the eveving from 15:00 to 17:00 stoppage time has significantly increased. ","e23ceb35":"# 3. Exploratory Data Analysis","17017171":"**Findings:**\n*In the city count distribution:*  \n   * Philadelphia have 45.29% of the total entries followed by Boston(21.23%), Atlanta(17.89%), Chicago(15.59%).\n\n*In the  Distribution of Hour by Week days:*\n   * Between 08:00 to 17:00, is the rush hour in all cities, but for Philadelphia, it is 08:00 to 24:00.\n   \n*In the  Distribution of Hour by Weeked days:*\n   * All the cities are by far the most common in all hours during the weekend, only on 05:00 to 7:00. \n\n*In the Distribution of month by cities:*\n   * It's seem that we have only six month of data only.","f31e7e67":"* North Broad Street is the most Entry and Exit Street Names.","bb31001d":"## <span style='color:darkgreen;background:yellow'>4.1. Intersection ID \n\nMaking a new columns of IntersectionId with city name.","f602d9fa":"* The most common path is North Broad Street_N_North Broad Street_N \n![](https:\/\/www.google.com\/maps\/vt\/data=MWTnQbznJmgQc1dK9zCSLTWU57GXsKLQZdCtJkcTtx32-iDaKYZkXEOKPnBlEbEwZQBQFWHr9f4dvV5rZwSBFGBTbKTtm8_0deCBMUj2viNEif81m4boySADz5YLfdkwgqpz_U9TSBsZbNa9lyUowfNhDFoa65PDHXL13kK8mIK_4K6ZN7l5z5P-2PklT5-64QKwoOIuyYaAk2I1xcMbtmD7m3WqGgiPTQo0ZY9i0Qhk-y1qcksSqQ)","78a38bc2":"## 6.1 Retrain and Predict Using Optimized Hyperparameters","f3b9d334":"# Objective\n* Exploratory Data Analysis (EDA) to analyzing data sets to summarize their main characteristics, often with visual methods.\n* Extensive feature engineering and modeling.","f95c772a":"## 3.2 Entry\/Exit Street Names","074f8909":"## <span style='color:darkgreen;background:yellow'>4.2. Encoding Street Names \nWe are encode the street name according to its road type. ","5848e126":"## <span style='color:darkgreen;background:yellow'>4.4. Encoding Cyclic or Time Features\nEncoding cyclical continuous features - 24-hour time. Some data is inherently cyclical. Time is a rich example of this: minutes, hours, seconds, day of week, week of month, month, season, and so on all follow cycles.\n\nReference:\n* https:\/\/www.kaggle.com\/vikassingh1996\/handling-categorical-variables-encoding-modeling\n* https:\/\/www.kaggle.com\/avanwyk\/encoding-cyclical-features-for-deep-learning","2b5b7059":"## <span style='color:darkgreen;background:yellow'>4.5 standardizing of lat-long","225030f8":"## <span style='color:darkgreen;background:yellow'>4.3. Encoding Cordinal Direction \nTurn Direction: \n\nThe cardinal directions can be expressed using the equation: $$ \\frac{\\theta}{\\pi} $$\n\nWhere $\\theta$ is the angle between the direction we want to encode and the north compass direction, measured clockwise.\n\n\nReference: \n* https:\/\/www.kaggle.com\/danofer\/baseline-feature-engineering-geotab-69-5-lb\n* This is an important feature, as shown by janlauge here : https:\/\/www.kaggle.com\/janlauge\/intersection-congestion-eda\n\n* We can fill in this code in python (e.g. based on: https:\/\/www.analytics-link.com\/single-post\/2018\/08\/21\/Calculating-the-compass-direction-between-two-points-in-Python , https:\/\/rosettacode.org\/wiki\/Angle_difference_between_two_bearings#Python , https:\/\/gist.github.com\/RobertSudwarts\/acf8df23a16afdb5837f )","05369a6c":"## 3.5 City and Time features\n\nI will analyse city and time feature together to get clear picture about the data and its distribution.","58e5c845":"**Findings:**\n\nI got the idea from @jpmille\n\nIntersection as a collection of related traffic paths through a common point. In this dataset, the 'Path' feature lists  the unique paths at each interection. \n\nIn the below picture : \n* Here the intersection of West Dauphin Street, Germantown Avenue, North 7th Street. (see in Total Time Stopped section)\n* It's a 6 leg intersection: it can have up to 6 paths plus any u-turns.\n\n<img src= 'https:\/\/i.ibb.co\/rH898sn\/Screenshot-94-LI.jpg' width=\"500px\">\n\n* Day\/Night and weather affect traffics. We can Add temperature and rainfall of the cities.\n* Urban locations: likely if a street is numbered but Rural locations are not, may affect the traffic.\n* The intersection is surrounded by businesses: may affect the traffic during weeks and weekend day.","40703593":"# 6. Model Building & Evaluation","f65030c8":"## 3.3 Path","2b967e3e":"## <span style='color:darkgreen;background:yellow'>4.5. Encoding City + Temperature (\u00b0F) + Rainfall (inches) \n\nI am adding temperature and rainfall because these variables may affect the taffic stoppage time in cities. This kernel will help you to understand how other variables are affecting the traffic. https:\/\/www.kaggle.com\/jpmiller\/eda-to-break-through-rmse-68\n\nData is collected from https:\/\/www.ncdc.noaa.gov\/ and http:\/\/www.rssweather.com OR just google it.","160a31cb":"# About This Kernel\nThis verbosity tries to explain everything I could know. Once you get through the notebook, you can find this useful and straightforward. I attempted to explain things as simple as possible. We are going to learn about the data and get ideas for feature engineering and modeling.","58905f9f":"* We can observed that in general Entry and Exit Heading is exactly the same.\n* Most of vehicel are heading toward East and West.","8539a3b4":"## Explation of Features:\nExplation given by: @pcjimmmy, @sohier, @anshuls235, @awray3 , @panosc \n\n**Intersection :** An intersection is an at-grade junction where two or more roads or streets meet or cross. Intersections may be classified by number of road segments, traffic controls, and\/or lane design.\n\n**EntryStreetName :** It is the street name where the traffic or vehicle entering the intersection. \n\n**ExitStreentName :** It is the street name where the traffic or vehicel exiting the intersection.\n* We have the missing values in the Entry and Exit StreetName only.\n\n**EntryHeading and ExitHeading :** Those are the directions the vehicle was traveling when entering\/exiting the intersection. For example, you could see North\/North if a truck just drove straight through without turning.\n\n**TotalTimeStopped_p20,_p40,_p50,_p60,_p80 :**  Let's focus on one row, say the one with RowId 2255742, 2255748. This row has the following percentiles for the Total Time Stopped feature:\n\np20: 0\n\np40: 0\n\np50: 0\n\np60: 0\n\np80: 13\n\nOf all the cars driving through the intersection, the first 60% didn't have to wait at all. The next 20% had to wait for 13 seconds and not sure about the next 20% as the data is not given for that.\n\n**DistanceToFirstStop:** DistanceToFirstStop is how far before the intersection the truck stopped for the first time.\n\n**TimeFromFirstStop:** TimeFromFirstStop is how long it took from that point to cross the intersection.","6674ebc3":"# Competition Info\nThe dataset for this competition includes aggregate stopped vehicle information and intersection wait times. Your task is to predict congestion, based on an aggregate measure of stopping distance and waiting times, at intersections in 4 major US cities: Atlanta, Boston, Chicago & Philadelphia.\n\n![](http:\/\/media1.giphy.com\/media\/SI7JdaPix700qX6qCm\/source.gif)","3eb291ba":"# 1. Importing Packages and Collecting Data\n","484fc284":"# 4. Feature Engineering\n","79e7da56":"## <span style='color:darkgreen;background:yellow'>4.6 Droping the variables ","7ce2a8da":"## 3.6 Latitude\/Longitude","efee54bd":"# 5. Seting X and Y"}}