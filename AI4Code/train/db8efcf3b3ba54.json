{"cell_type":{"256878be":"code","265865ca":"code","9fd1b43f":"code","768df2a7":"code","5e68ed67":"code","f2068334":"code","90adcf9e":"code","cf386d4f":"code","126263dd":"code","dffc2ada":"code","c0fb0c40":"code","32567c0a":"code","cb4b6d7a":"code","6500443d":"code","a4147448":"code","0ac489a4":"code","d18ee64a":"code","259e64d4":"code","f68171a8":"code","af911c7e":"code","6b5abe31":"code","bb51d19a":"code","555bf512":"code","2d805f5a":"code","2ebf3470":"code","4c89b205":"code","365c2944":"code","b11cbbe5":"code","88d0922f":"code","554ab412":"code","01482717":"markdown","e30af4f5":"markdown","851fc7cc":"markdown","66cfb8fc":"markdown","082e3245":"markdown","43c835a2":"markdown","043fe131":"markdown","da281339":"markdown","46dd24e8":"markdown","032aada9":"markdown","ed65b3f2":"markdown","1a70e952":"markdown","6cf958d5":"markdown","0a1f685b":"markdown","efac16ff":"markdown","6f375f8b":"markdown","10cf6f40":"markdown"},"source":{"256878be":"# Loading the dependencies for the model\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nimport math","265865ca":"# Loading the iris data into X, y. Each dataset will have its own way of doing this.\niris = load_iris()\nX = iris.data\ny = iris.target\ny_labels = iris.target_names\nX_labels = iris.feature_names","9fd1b43f":"#4 features, 150 samples, 3 target values to predict\nprint(\"Column names - \",X_labels)\nprint(\"Shape of X - \",X.shape)\nprint(\"Target values - \",y_labels)","768df2a7":"#Separating test and train data for model evaluation\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)","5e68ed67":"#Instantiate the classifier and fit it to training data\nnb = GaussianNB()\nnb.fit(X_train, y_train)","f2068334":"#Evaluation of the model using test data\nnb.score(X_test, y_test)","90adcf9e":"# Model's prediction when sepal_len = 5.8, sepal_wid = 2.2, petal_len = 4.2, petal_wid = 0.4\nnb.predict([[5.8,2.2,4.2,0.4]])","cf386d4f":"#Putting all the data into a dataframe\ndf = pd.DataFrame(X, columns=X_labels)\ndf['class'] = y\ndf.head()","126263dd":"#Calculating class priors (simply the probability of class labels)\npriors = np.unique(y, return_counts=True)[1] \/ len(y)\npriors","dffc2ada":"#Class wise mean for each feature\nclasswise_means = df.groupby(['class']).mean().reset_index()\nclasswise_means","c0fb0c40":"#Class wise mean for each feature\nclasswise_std = df.groupby(['class']).std().reset_index()\nclasswise_std","32567c0a":"# Gaussian probability function vectorized to work on multidimensional arrays with broadcasting\ndef pdf(x, mean, sd):\n    return (1 \/ np.sqrt(2*np.pi*sd**2)) * np.exp(-1*((x-mean)**2\/(2*sd**2)))\n\nvectorized_pdf = np.vectorize(pdf)","cb4b6d7a":"#Converting everything to numpy arrays\nmean_array = np.array(classwise_means.iloc[:,1:])\nstd_array = np.array(classwise_std.iloc[:,1:])\n\n#Sample is the feature vector, whose label needs to be predicted\nsample = [5.1, 3.5, 1.4, 0.2]","6500443d":"#Element wise application of the pdf(x,mean,std) function on broadcasted arrays\nelementwise_pdf = vectorized_pdf(sample, mean_array, std_array)\nelementwise_pdf","a4147448":"#Rowwise product of the above matrix\nrowwise_product = np.product(elementwise_pdf,axis=1)\nrowwise_product","0ac489a4":"#Elementwise multiplication with corresponding class probabilities (priors)\nmultiply_priors = np.multiply(priors,rowwise_product)\nmultiply_priors","d18ee64a":"#Argmax to predict class label\nprediction = np.argmax(multiply_priors)\nprediction","259e64d4":"from sklearn.feature_extraction.text import CountVectorizer","f68171a8":"#Defining the training data\ndocuments = ['quite happy with it', \n             'bad device', \n             'great job with the features', \n             'bad experience',\n             'horrible device',\n             'very happy with the product']\n\nclasses = ['positive','negative','positive','negative','negative','positive']","af911c7e":"#Fitting the count vectorizer to get word representation for each sentence\ncnt = CountVectorizer()\ncnt_matrix = cnt.fit_transform(documents).todense()\ncnt_matrix.shape","6b5abe31":"#Creating a dataframe to calculate summary statistics later\ntraining_data = pd.DataFrame(cnt_matrix,\n                                index = documents, \n                                columns=cnt.get_feature_names())\ntraining_data['Class'] = classes\ntraining_data","bb51d19a":"#Calculating label probability (priors)\nlabel_proba = np.unique(classes, return_counts=True)[1]\/len(classes)\nlabel_proba","555bf512":"#Calculating word frequency for each word in training data (label-wise)\n\n## 0 = negative, 1 = positive\nNc = training_data.groupby(['Class']).sum().reset_index('Class').drop('Class',axis=1)\nNc","2d805f5a":"# Calculating N which is the total number of word occurances associated with each label\nN = np.sum(np.array(Nc),axis=1)\nN","2ebf3470":"#Calculating length of vocabulary\nd = len(cnt.get_feature_names())\nd","4c89b205":"#Setting value of the laplace smoothing factor alpha\na = 1","365c2944":"#Getting theta for a single input word\ndef get_thetas(word):\n    try:\n        return (np.array(Nc[word]) + a) \/ (N+d)\n    except:\n        return (np.array([0]*len(Nc)) + a) \/ (N+d)\n    \nget_thetas('happy')","b11cbbe5":"#The sentence for which label needs to be predicted (word tokenized from start)\nsample = ['happy', 'with', 'the', 'product']","88d0922f":"# Calculating product of the probabilities for each word (label-wise)\nprobability_product = np.product(np.array([list(get_thetas(i)) for i in sample]), axis=0)\nprobability_product","554ab412":"#Prediction of label using argmax after multiplying the thetas with label probabilities (priors)\nnp.argmax(np.multiply(label_proba, probability_product))","01482717":"### 1.3 Curve fitting\n\nWhen working with different algorithms, its important to undertand where a particular algorithm will work and where it may not. For this, I have always found a 2 dimensional example of decision boundaries quite useful from an intuitive point of view, since the core 'nature' of the classifier retains itself in higher dimensional spaces as well. The objective is to separate the blue from the red points. The decision boundary with the confidence is plotted. \n\nAs seen below (image from sklearn documentation), the naive bayes classifier is capable of fitting smooth continous decision boundaries, but fails when the data needs a high degree polynomial.\n\n<img src='http:\/\/akshaysehgal.com\/img\/NBclassifier.png' width=700>","e30af4f5":"The value 1 corresponds to versicolor, as seen by the y_labels (0 represents setosa). This is a straightforward sklearn workflow and should be quite familiar if you have implemented any model with sklearn before.","851fc7cc":"The 1 here represents 'positive' label. \n\nQuite clearly, the first thing that comes to mind is that if we replace the count vectorizer with tfidf vectorizer, the words that are not so important at representing a label will get lesser weights, while improving the weights for words which actually represent the labels. In this case, words like device, experience etc each contribute to either positive or negative labels respectively. \n\nSimilarly, removing stopwords and lemmatizing words will have even more impact on the accuracy of the model.","66cfb8fc":"## 1. What is the Naive Bayes Algorithm?\n\n### 1.1 Overview\n\nNaive Bayes is a supervised classification algorithm which belongs to the family of \"Probabilistic Classifiers\". As the name suggests, <u>it is uses Bayes' theorem at its core with a 'naive' assumption<\/u>.\n\nThe algorithm is widely used for simple classification problems and works well with text data in a bag of words form. Infact, it was most popularized by its use as a spam email classifier by google. To this date, its widely used as a benchmarking model by data scientists in hackatons on kaggle.\n\nBefore we get into the crux of the algorithm, its important to know what bayes rule is.","082e3245":"> We have looked at how naive bayes algorithm fundamentally works, how to quickly implement it and how to implement is from scratch (both the gaussian and multinomial classifiers). My above attempt at an article should have given the reader enough intuition and background to be able to master naive bayes and other similar probabilistic classifiers. I had originally written this on [my website](http:\/\/akshaysehgal.com\/) so do feel free to check it out here.","43c835a2":"### Gaussian Naive Bayes\n\nWhen your features are continous in nature, you assume that they are conditionally independent and that the data associated with each class is normally distributed. To calculate the $P(x_i \\mid y)$ values, we segment the data by the class, compute mean, variance and then derive the probability distribution as below.\n\n$$P(x_i \\mid y) = {1 \\over \\sqrt{2 \\pi \\sigma^2_y}}exp \\Bigg(-{(x_i - \\mu_y)^2 \\over 2 \\sigma^2_y} \\Bigg)$$\n\n\nThe first step is to calculate the summary statistics. For gaussian classifier, we need the class-wise mean and standard deviation and we need the class priors, which is nothing but the class probability in the entire data. With these 3, we should be able to predict which class a particular sample belongs to. Following is a diagram which shows the vectorized computation of the gaussian classifier in a graphical representation.\n\n\n<img src=\"http:\/\/akshaysehgal.com\/img\/vectorized_gaussian.png\" alt=\"Drawing\" style=\"width: 600px;\"\/>","043fe131":"<b>Now let's say someone tells you that B has occured. The area outside the circle that represents B is now meaningless since the new sample space is now B. This means, we need to revise the probabilities that have been assiged to the spaces inside B.<\/b>\n\nNow someone asks you, what is the probability that A occurs, given that B has occured. Since B has occured and the new sample space is now B, the area that was initially represented by $P(A \\cap B)$ is now the $P(A \\mid B)$. But the probability now has to be recalculated.\n\nWhat is the probability of this new section? Well, we can just use simple ratios for solving this.\n\n$$ P (A \\mid B) = {P(A \\cap B) \\over P(B)} = {0.3 \\over (0.3+0.2)}$$\n\nNow, the whole exercise we did above can also be done for when A has occured. Meaning that we can use symmetry to show - \n\n$$ P (B \\mid A) = {P(A \\cap B) \\over P(A)}$$\n\nUsing the two symmetric equations and replacing $P(A \\cap B)$, we can write the famous equation - \n\n$$ P (A \\mid B) = {P (B \\mid A) \\times P(A) \\over P(B)}$$","da281339":"The 0 here means first class 'setosa'.\n\nFew interesting notes based on this calculation -\n1. The prediction here is directly made using the priors matrix, classwise mean matrix and classwise standard deviation matrix.\n2. Training the algorithm with new data simply means updating these 3 matrices and re-running calculations to predict the labels.","46dd24e8":"---\n## 2. How to implement the Naive Bayes algorithm?\n\nLets do a quick implementation of Naive Bayes. This is how you would actually implement it when working with data. The focus will be on the algorithm rather than data preprocessing for now. For this example, I will select the iris data which is available as part of the sklearn datasets API.","032aada9":"Follow me:\n- [Linkedin](https:\/\/www.linkedin.com\/in\/akshay-sehgal-a5071458\/)\n- [Stackoverflow](https:\/\/stackoverflow.com\/users\/4755954\/akshay-sehgal)\n- [Github](https:\/\/github.com\/Akshaysehgal2005)\n- [Website](http:\/\/akshaysehgal.com\/)","ed65b3f2":"> Author: Akshay Sehgal (www.akshaysehgal.com)\n\n> Author Notes: A lot of folks have joined Kaggle during this lockdown to learn Machine Learning and I believe a good start to it is by truely understanding how some of the basic algorithms work behind the scenes. In this notebook I try to give a background on how arguably the most impactful algorithm in ML space works. Instead of using sklearn directly, I try to look into how it can be implemented (in a vectorized manner) in numpy itself.","1a70e952":"---\n## 3. What's happening behind the scenes?\n\nThis section we are going to implement the different classifiers from scratch, making sure we actually show what's happening behind the scene. Also, since I am not a big fan of for loops, I will try to give a vectorized implementation of the model, as much as possible.","6cf958d5":"## 4. Where can I read more?\n\n- https:\/\/www.youtube.com\/watch?v=j9WZyLZCBzs&list=PLUl4u3cNGP61MdtwGTqZA0MreSaDybji8\n- https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html\n- https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier\n- https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html\n- https:\/\/machinelearningmastery.com\/naive-bayes-for-machine-learning\/","0a1f685b":"### 1.3 The Naive Assumption\n\nNow, let's see an 'expanded' version of this definition which is more relevant to data science. We want to know what is the probability of $y$ given that $x_1, x_2, ... , x_n$ have occured (where $x_i$ are different features and y is the class label we want to predict). This is given by - \n\n$$P(y \\mid x_1, x_2, ... , x_n) = {P(x_1, x_2, ... , x_n \\mid y) \\times P(y) \\over P(x_1, x_2, ... , x_n)}$$\n\nHere comes the <u>Naive assumption of conditional independence<\/u>. The assumption is that all the features $x_1, x_2 ..., x_n$ are independent. This means that $P(A,B) = P(A).P(B)$. Applying this to the above equation we get - \n\n$$P(y \\mid x_1, x_2, ... , x_n) = {P(x_1 \\mid y).P(x_2 \\mid y)... P(x_n \\mid y).P(y) \\over P(x_1).P(x_2)...P(x_n)}$$\n\nUsing proper convention - \n\n$$P(y \\mid x_1, x_2, ... , x_n) = {P(y)\\prod_{i=1}^{n} P(x_i \\mid y) \\over \\prod_{i=1}^{n} P(x_i)}$$\n\nSince the denominator is constant, we can write the equation as -\n\n$$P(y \\mid x_1, x_2, ... , x_n) \\propto P(y)\\prod_{i=1}^{n} P(x_i \\mid y)$$\n\nIn order to turn this into a classifier, we need to pick up the one with the max probability. This can be expressed as -\n\n$$y = argmax_y(P(y)\\prod_{i=1}^{n} P(x_i \\mid y))$$\n\nThe last step is known as Maximum A Posteriori decision rule.","efac16ff":"### 1.4 Different Classifiers\n\nNow, when you actually plan on implementing this, you will notice there are multiple classifiers available that can be used for a naive bayes model. The point of these classifiers is quite simple. The above equation calculates $P(x_i \\mid y)$ in a straight forward way for features with discrete values, but what if the features are continous variables. Clearly, this will need you to 'assume' the nature of the distribution since unlike discrete valued features, where you could just sum up the number of times a discrete value occured along with the specific y class label. In this case we can use classifiers such as GaussianNB.\n\nSimply put, by choosing different classifiers, you get to choose the assumptions regarding the nature of distributions of $P(x_i \\mid y)$. More about these classifiers in later sections.","6f375f8b":"### 1.2 The Bayes' Theorem\n\nYou can get a great intuitive sense about conditional probability from lectures of Prof John Tsitsiklis (MITOpenCourseware). \n\n>\"You know something about the world, and based on what you know, you setup a probability model and you write down probabilities about the different outcomes. Then someone gives you some new information, which changes your beliefs and thus changes the probabilities of your outcomes.\"\n\nIn a simple sense, bayes' theorem talks about the 'new' probabilities of an experiment given that an event has occured.\n\nLet's say you have a sample space the probabilities for different events are given as below - \n\n<img src=\"http:\/\/akshaysehgal.com\/img\/img.jpg\" alt=\"Drawing\" style=\"width: 400px;\"\/>\n\n\n","10cf6f40":"### Multinomial classifier\n\n\nMultinomial classifier is a bit more complex but is the usual choice for text based data. Here you assume that your data is distributed multinomially. Multinomial distribution is a generalized case of binomial, bernoulli and categorical distributions depending on the number of trials (n) and the number of outcomes (k). If there are only 2 possible outcomes (k=2) and multiple trials (n>1) then your multinomial distribution becomes a binomial distribution.\n\nLets say you have a few sentences with labels - \n\n- \"Great phone\" - Positive\n- \"Didn't like the device\" - Negative\n- \"Wonderful interface\" - Positive\n- \"Horrible experience\" - Negative\n\nLets say you are asked to classify the sentence \"Great smartphone\". What you are trying to do is to just find the probability of the words \"Great\" and \"Smartphone\" to occur with the label Positive\/Negative. Based on the product of the probabilities of the 2 words (remember the 'naive' assumption) you calculate which has a higher product of probabilities and then predict that as the corresponding label. This is done by simply calculating $N_w \/ N$ where $N_w$ is the number of times the word occurs with the label, and N is the total number of words that occur <u>with the given label<\/u>.\n\n\nHere comes the issue while doing this. If the word isn't used in the traning data (like in this case \"Smartphone\"), the value of the product of probabilities will become 0. How do we solve this? We use a techinque called additive smoothing (laplace smoothing) which just adds some stuff to the numerator and denominator. $P(x_i \\mid y)$ is the probability $\\theta_i$ of $i$ appearing in a sample belonging to $y$.\n\n$$ \\hat\\theta_i = {N_w + \\alpha \\over N + \\alpha d} $$\n\nHere $\\alpha$ is set to 1 incase of laplace smoothing, while $\\alpha$ is set > 1 for Lidstone smoothing. $N_w$ is the number of times the word occurs with that label, N is the number of words in vocabulary which occur with that label, and d is the total size of the vocabulary.\n\n\nNote that for the implementation I will only show the case where we use count vectorizer but for improving the model, it is important to leverage various techiniques such as stopword removal, lemmitization, tfidf vectorizer etc.\nWe will start with creating a small text dataset with 2 labels."}}