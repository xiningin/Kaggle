{"cell_type":{"b57c6486":"code","5e2dbaf4":"code","1a3d0544":"code","7dc9faf6":"code","0b9e5fb6":"code","30cdfc9e":"code","646d36de":"code","f2cdc1dc":"code","cf97411d":"code","3b9eb22f":"code","b47faeb9":"code","b9dbddc7":"code","a270fe69":"code","b94de741":"code","e8ba428a":"code","29fcc1a1":"code","64c34a38":"code","2040f3a1":"code","a5d3c140":"code","4ddf7367":"code","b5f078d7":"code","6b7159f8":"code","41f9f247":"code","bed02820":"code","0253c8f9":"code","d4e1bcec":"code","b418df90":"code","4856d886":"code","7cc96b7b":"code","6ac319fe":"code","2f9d9e7c":"code","8e96b10d":"code","7cbfc082":"code","b2887906":"code","0b178187":"code","7c86c67b":"code","7ea6ffa0":"code","a4b962f6":"code","a54caf41":"code","9637da45":"code","0450e60a":"code","101a2108":"markdown","d5fe49f5":"markdown","248cf7db":"markdown","51cb7281":"markdown","8a6c2993":"markdown","08375397":"markdown","6ae9ce2b":"markdown","951c328b":"markdown","68af1731":"markdown","f14a5430":"markdown","720aa3aa":"markdown","fce8904f":"markdown","0eeec349":"markdown","d0b13d17":"markdown","95acf944":"markdown","264e20e6":"markdown","40be2997":"markdown","f08157ad":"markdown","25cbd763":"markdown","ccbc5e25":"markdown","c9e82e15":"markdown","b7f09ba8":"markdown","d4f39678":"markdown","4807531a":"markdown","3831e28a":"markdown","a7d1f2a4":"markdown","7d123d93":"markdown","980ff746":"markdown","2c38b067":"markdown","fbfa4e0c":"markdown","991acbbc":"markdown","4141a8cb":"markdown","6a76ead8":"markdown","8fff2ea9":"markdown","d1da880e":"markdown","50c650dd":"markdown","dd05ce38":"markdown","897c4d3d":"markdown","bae573e1":"markdown","1ded294c":"markdown","81cc743a":"markdown","973f42c7":"markdown","1363b658":"markdown","5a6bdcc9":"markdown","0760048a":"markdown","3a19e692":"markdown","2b4b8553":"markdown","c180ab76":"markdown","d060654c":"markdown","8f9b3464":"markdown","cbb91a29":"markdown","e6d10cff":"markdown","4de4603f":"markdown","d8871fa8":"markdown","1dc4418e":"markdown","07567378":"markdown","2c5d04dc":"markdown","538d3013":"markdown","e9905b77":"markdown","e4363dbe":"markdown","4d063201":"markdown","f31bb148":"markdown","6d434d5d":"markdown","6e86a81d":"markdown","04c49c33":"markdown","38753ef7":"markdown","45894db1":"markdown"},"source":{"b57c6486":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder,StandardScaler,PowerTransformer, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_predict, train_test_split,GridSearchCV,cross_val_score\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge,ElasticNet\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\n\n\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.feature_selection import SelectKBest,SelectPercentile,f_classif,f_regression,mutual_info_regression,mutual_info_classif,SelectFromModel,RFE\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\n\nimport optuna\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')","5e2dbaf4":"pd.set_option('max_columns',100)\npd.set_option('max_rows',900)\n\npd.set_option('max_colwidth',200)\n\ndf = pd.read_csv('..\/input\/car-price-prediction\/CarPrice_Assignment.csv')\ndf.head()","1a3d0544":"df.info()","7dc9faf6":"df.duplicated().sum()","0b9e5fb6":"def missing (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values\n\nmissing(df)","30cdfc9e":"df.nunique()","646d36de":"df1= df.copy()","f2cdc1dc":"df1['CarName'].sample(5)","cf97411d":"df1['CarName'].unique()","3b9eb22f":"df1['model'] = [x.split()[0] for x in df1['CarName']]\ndf1['model'] = df1['model'].replace({'maxda': 'Mazda','mazda': 'Mazda', \n                                     'nissan': 'Nissan', \n                                     'porcshce': 'Porsche','porsche':'Porsche', \n                                     'toyouta': 'Toyota', 'toyota':'Toyota',\n                            'vokswagen': 'Volkswagen', 'vw': 'Volkswagen', 'volkswagen':'Volkswagen'})\n","b47faeb9":"df1= df1.drop(['car_ID','CarName'], axis=1)","b9dbddc7":"print (f' We have {df1.shape[0]} instances with the {df1.shape[1]-1} features and 1 output variable')","a270fe69":"numerical= df1.drop(['price'], axis=1).select_dtypes('number').columns\n\ncategorical = df1.select_dtypes('object').columns\n\nprint(f'Numerical Columns:  {df1[numerical].columns}')\nprint('\\n')\nprint(f'Categorical Columns: {df1[categorical].columns}')","b94de741":"df1['price'].describe()","e8ba428a":"print( f\"Skewness: {df1['price'].skew()}\")","29fcc1a1":"df1['price'].iplot(kind='hist')","64c34a38":"df1[numerical].describe()","2040f3a1":"df1[numerical].iplot(kind='hist');","a5d3c140":"df1[numerical].iplot(kind='histogram',subplots=True,bins=50)\n","4ddf7367":"skew_limit = 0.75 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models. \nskew_vals = df1[numerical].skew()\nskew_cols= skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols","b5f078d7":"df1[skew_cols.index].iplot(kind='hist');","6b7159f8":"df1[skew_cols.index].iplot(kind='histogram',subplots=True,bins=50)\n","41f9f247":"df_try = df1.copy()\n\nfor col in skew_cols.index.values:\n    df_try[col] = df_try[col].apply(np.log1p)\n\nprint(df_try[skew_cols.index].skew())\nprint()\n\ndf_try[skew_cols.index].iplot(kind='histogram',subplots=True,bins=50);","bed02820":"df_trans = df1[skew_cols.index].copy()\npt = PowerTransformer(method='yeo-johnson')\ntrans= pt.fit_transform(df_trans)\ndf_trans = pd.DataFrame(trans, columns =skew_cols.index )\nprint(df_trans.skew())\nprint()\ndf_trans.iplot(kind='histogram',subplots=True,bins=50);","0253c8f9":"numerical1= df1.select_dtypes('number').columns\n\n\nmatrix = np.triu(df1[numerical1].corr())\nfig, ax = plt.subplots(figsize=(14,10)) \nsns.heatmap (df1[numerical1].corr(), annot=True, fmt= '.2f', vmin=-1, vmax=1, center=0, cmap='coolwarm',mask=matrix, ax=ax);","d4e1bcec":"df1 = df1.drop('citympg',axis=1)","b418df90":"df1[categorical].head()","4856d886":"print(df1.groupby('fueltype')['price'].mean().sort_values())\nprint()\ndf1.groupby('fueltype')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","7cc96b7b":"print(df1.groupby('aspiration')['price'].mean().sort_values())\nprint()\ndf1.groupby('aspiration')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","6ac319fe":"print(df1.groupby('carbody')['price'].mean().sort_values())\nprint()\ndf1.groupby('carbody')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","2f9d9e7c":"print(df1.groupby('drivewheel')['price'].mean().sort_values())\nprint()\ndf1.groupby('drivewheel')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","8e96b10d":"print(df1.groupby('enginelocation')['price'].mean().sort_values())\nprint()\ndf1.groupby('enginelocation')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","7cbfc082":"print(df1.groupby('enginetype')['price'].mean().sort_values())\nprint()\ndf1.groupby('enginetype')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","b2887906":"print(df1.groupby('fuelsystem')['price'].mean().sort_values())\nprint()\ndf1.groupby('fuelsystem')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","0b178187":"print(df1.groupby('model')['price'].mean().sort_values())\nprint()\ndf1.groupby('model')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","7c86c67b":"df2 = pd.get_dummies(df1, columns=categorical, drop_first=True)\ndf2.head()","7ea6ffa0":"X= df2.drop('price', axis=1)\ny= df2['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = LinearRegression()\n\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint (f'model : {model} and  rmse score is : {np.sqrt(mean_squared_error(y_test, y_pred))}, r2 score is {r2_score(y_test, y_pred)}')\n\n\n","a4b962f6":"rmse_test =[]\nr2_test =[]\nmodel_names =[]\n\nnumerical2= df2.drop(['price'], axis=1).select_dtypes('number').columns\n\nX= df2.drop('price', axis=1)\ny= df2['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\ns = StandardScaler()\np= PowerTransformer(method='yeo-johnson', standardize=True)\n\nrr = Ridge()\nlas = Lasso()\nel= ElasticNet()\nknn = KNeighborsRegressor()\n\nmodels = [rr,las,el,knn]\n\nfor model in models:\n    ct = make_column_transformer((s,numerical2),(p,skew_cols.index),remainder='passthrough')  \n    pipe = make_pipeline(ct, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    rmse_test.append(round(np.sqrt(mean_squared_error(y_test, y_pred)),2))\n    r2_test.append(round(r2_score(y_test, y_pred),2))\n    print (f'model : {model} and  rmse score is : {round(np.sqrt(mean_squared_error(y_test, y_pred)),2)}, r2 score is {round(r2_score(y_test, y_pred),2)}')\n\nmodel_names = ['Ridge','Lasso','ElasticNet','KNeighbors']\nresult_df = pd.DataFrame({'RMSE':rmse_test,'R2_Test':r2_test}, index=model_names)\nresult_df","a54caf41":"rmse_test =[]\nr2_test =[]\nmodel_names =[]\n\n\n\nX= df2.drop('price', axis=1)\ny= df2['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\nrf = RandomForestRegressor(random_state=42)\ngb = GradientBoostingRegressor(random_state=42)\net= ExtraTreesRegressor(random_state=42)\nxgb = XGBRegressor(random_state=42)\n\nmodels = [rf,gb,et,xgb]\n\nfor model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    rmse_test.append(round(np.sqrt(mean_squared_error(y_test, y_pred)),2))\n    r2_test.append(round(r2_score(y_test, y_pred),2))\n    print (f'model : {model} and  rmse score is : {round(np.sqrt(mean_squared_error(y_test, y_pred)),2)}, r2 score is {round(r2_score(y_test, y_pred),4)}')\n\nmodel_names = ['RandomForest','GradientBoost','ExtraTree','XGB']\nresult_df = pd.DataFrame({'RMSE':rmse_test,'R2_Test':r2_test}, index=model_names)\nresult_df","9637da45":"X= df2.drop('price', axis=1)\ny= df2['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nrf = RandomForestRegressor(n_estimators= 220, random_state=42 )\n\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nprint (f' rmse score is : {round(np.sqrt(mean_squared_error(y_test, y_pred)),4)}, r2 score is {round(r2_score(y_test, y_pred),4)}')\n\n\n","0450e60a":"importances = rf.feature_importances_\nfeature_names = [f'feature {i}' for i in range(X.shape[1])]\n\n# what are scores for the features\nfor i in range(len(rf.feature_importances_)):\n    if rf.feature_importances_[i] >0.001:\n        print(f'{X_train.columns[i]} : {round(rf.feature_importances_[i],3)}')\n\nprint()\n\nplt.bar([X_train.columns[i] for i in range(len(rf.feature_importances_))], rf.feature_importances_)\nplt.xticks(rotation=90)\nplt.rcParams[\"figure.figsize\"] = (24,12)\nplt.show()","101a2108":"### With Power Transformer","d5fe49f5":"- There is no zero variance variable. \n- Car ID column is repetition of the index. So I'll drop it.\n- Carname has 147 different entity. I'll check it. And try to find a way to reduce the variance.\n- Other than that there is no problem.","248cf7db":"<a id=\"3\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Target Variable<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","51cb7281":"- There is a quite difference based on the engine location. Rear engine location almost 3 times expensive than front ones.","8a6c2993":"- Based on the Random Forest Regressor:\n   - **enginesize**\n   - **curbweight**\n   - **highway mpg**\n   - **horse power**\n- have biggest importance scores.\n","08375397":"<a id=\"5\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Categorical Features<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","6ae9ce2b":"<a id=\"12\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>Conclusion<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","951c328b":"### Drivewheel & Price","68af1731":"- Since this is a beginner friendly notebook, let's see several options in the practice without touching our main dataset.\n- First I'll show np.log method.\n- Then I'll use transformation methods.","f14a5430":"-Before moving forward, I'll handle the 'carname'","720aa3aa":"- Now we are talking.\n- Random forest, without any tuning got .94 R2 and lowest RMSE.\n- XGBoost also did a good job without any optimization \/ tuning.","fce8904f":"### Fuel Type & Price","0eeec349":"- With hyperparameter tuning we got a lift. \n- RMSE (from 1984.44 to 1975.8483)\n- R2 (from .9432 to .9437)","d0b13d17":"- It is important to note that Random Forest Regressor gave importance score bigger than 0 to 16 features.\n- Model used 16 out of 63 features to get best prediction.\n- For [For deatiled discussion on the Feature Selection-The Most Common Methods to Know](https:\/\/www.kaggle.com\/kaanboke\/feature-selection-the-most-common-methods-to-know)","95acf944":"<a id=\"13\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>References & Further Reading<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\n\n[Machine Learning - Beginner &Intermediate-Friendly BOOKS](https:\/\/www.kaggle.com\/general\/255972)","264e20e6":"- Based on the model, Porsche, Buick and Jaguar are the most expensive ones.\n- Chevroletis the least expensive model.","40be2997":"<a id=\"4\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Numerical Features<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","f08157ad":"<a id=\"8\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Ridge &  Lasso  &  Elasticnet  &  KNN with Scaler and Transformer<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","25cbd763":"- Let's make a copy of the dataset and start to work on it.","ccbc5e25":"- Overall data types seems ok. ","c9e82e15":"- No missing values and no duplicates. Hurray!!!","b7f09ba8":"### Get the Dummies","d4f39678":"#### By the way, when you like the topic, you can show it by supporting \ud83d\udc4d\n\n####  **Feel free to leave a comment**. \n\n#### All the best \ud83e\udd18","4807531a":"- Diesel cars are more expensive than cars with gas.","3831e28a":"### Model & Price","a7d1f2a4":"<a id=\"10\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Best Model with the Hyperparameter Tuning<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","7d123d93":"- During the modelling process, we can use power transformer.","980ff746":"- We have 9 numerical features which have more than .5 correlation with the price variable.\n- Which is a good sign for the prediction capability of the model, but still we need to see in the practice.\n- From the threshold .9 perspective: Highwaympg and citympg has .97 correlation. We can drop one of them to avoid multicollinearity problems for the linear models.\n- I have observed several highly correlated features below the .9 level.","2c38b067":"### Aspiration & Price","fbfa4e0c":"- I'll use only the brands\/make not the models.\n- I have seen several typos, I'll handle those.","991acbbc":"<a id=\"1\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>What Problem We Have and Which Metric to Use?<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","4141a8cb":"### Fuel system & Price","6a76ead8":"<a id=\"7\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Baseline Model<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","8fff2ea9":"![](https:\/\/media.giphy.com\/media\/3ov9jWu7BuHufyLs7m\/giphy.gif?cid=ecf05e47q8liqtbxy73738g13h2ofqf9nm9q82lm3py081io&rid=giphy.gif&ct=g)","d1da880e":"gif credit: https:\/\/giphy.com\/gifs\/girl-life-car-3ov9jWu7BuHufyLs7m","50c650dd":"- I'll use linear regression model as a base model\n- And then I will use Ridge, Lasso, Elasticnet, KNeighborsRegressor and Support Vector MAchine Regressor\n- And then i will use ensemble models, like Randomforest, Gradient Boosting and Extra Trees\n-  Finally I will look at the XGBoost Regresson.\n- And after evaluating the algorithm, we will select our best model.\n- Let's start.","dd05ce38":"### Engine type & Price","897c4d3d":"- Let's see the numerical features","bae573e1":"- Let's import the libraries","1ded294c":"- Several features have gaussian-normal like ditsribution.\n- I have also observed skewness.\n- I'll look those in details.","81cc743a":"<a id=\"2\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Exploratory Data Analysis<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","973f42c7":"DATA DICTONARY\t\t\t\t\t\t\n\t\t\t\t\t\t\n1\t**Car_ID**: \t\t\tUnique id of each observation \t\t\n2\t**Symboling**:  \t\t\tIts assigned insurance risk rating, A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe. \t\t\n3\t**carCompany**: \t\t\tName of car company \t\t\n4\t**fueltype**:\t\t\tCar fuel type i.e gas or diesel \t\t\n5\t**aspiration**:\t\t\tAspiration used in a car \t\t\n6\t**doornumber**:\t\t\tNumber of doors in a car\t\t\n7\t**carbody**:\t\t\tbody of car \t\t\n8\t**drivewheel**:\t\t\ttype of drive wheel\t\t\n9\t**enginelocation**:\t\t\tLocation of car engine\t\t\n10\t**wheelbase**:\t\t\tWeelbase of car (\t\t\n11\t**carlength**:\t\t\tLength of car \t\t\n12\t**carwidth**:\t\t\tWidth of car \t\t\n13\t**carheight**:\t\t\theight of car\t\t\n14\t**curbweight**:\t\t\tThe weight of a car without occupants or baggage. \t\t\n15\t**enginetype**:\t\t\tType of engine. \t\t\n16\t**cylindernumber**:\t\t\tcylinder placed in the car \t\t\n17\t**enginesize**:\t\t\tSize of car \t\t\n18\t**fuelsystem**:\t\t\tFuel system of car \t\t\n19\t**boreratio**:\t\t\tBoreratio of car \t\t\n20\t**stroke**:\t\t\tStroke or volume inside the engine \t\t\n21\t**compressionratio**:\t\t\tcompression ratio of car \t\t\n22\t**horsepower**:\t\t\tHorsepower \t\t\n23\t**peakrpm**:\t\t\tcar peak rpm \t\n24\t**citympg**:\t\t\tMileage in city \t\t\n25\t**highwaympg**:\t\t\tMileage on highway \t\t\n26\t**price**: \t\t\tPrice of car \t\t\n\nReference: https:\/\/www.kaggle.com\/hellbuoy\/car-price-prediction","1363b658":"<a id=\"9\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Random Forest& Gradient Boosting & Extra Trees & XGBoost<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","5a6bdcc9":"- By using standard scaler and power transformer for the skewness;\n- For linear models we got .92 for the R2 and\n- 2307.47 RMSE which are better scores compare to the baseline model.","0760048a":"#### Hi all.  \ud83d\ude4b\n\n#### We continue our **Beginner-Intermediate Friendly Machine Learning series**, which would help anyone who wants to learn or refresh the basics of ML.\n\n#### What we have covered: \n\n#### [Beginner Friendly Detailed Explained EDAs \u2013 For anyone at the beginnings of DS\/ML journey](https:\/\/www.kaggle.com\/general\/253911#1393015) \u2714\ufe0f\n\n#### [BIAS & VARIANCE TRADEOFF](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-bias-variance-tradeoff) \u2714\ufe0f\n\n#### [LINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-linear-algorithms)  \u2714\ufe0f\n\n#### [NONLINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/nonlinear-algorithms)  \u2714\ufe0f\n\n#### [The Most Used Methods to Deal with MISSING VALUES](https:\/\/www.kaggle.com\/kaanboke\/the-most-used-methods-to-deal-with-missing-values)  \u2714\ufe0f\n\n#### [Beginner Friendly End to End ML Project- Classification with Imbalanced Data](https:\/\/www.kaggle.com\/kaanboke\/beginner-friendly-end-to-end-ml-project-enjoy)  \u2714\ufe0f\n\n#### [How to Prevent the Data Leakage ?](https:\/\/www.kaggle.com\/kaanboke\/how-to-prevent-the-data-leakage) \u2714\ufe0f\n\n#### [The Most Common EVALUATION METRICS- A Gentle Intro](https:\/\/www.kaggle.com\/kaanboke\/the-most-common-evaluation-metrics-a-gentle-intro) \u2714\ufe0f\n\n#### [Feature Selection-The Most Common Methods to Know](https:\/\/www.kaggle.com\/kaanboke\/feature-selection-the-most-common-methods-to-know) \u2714\ufe0f\n\n\n#### In this notebook we will  implement **End to End Prediction Model** by using different ML algorithms**\n#### Enjoy \ud83e\udd18","3a19e692":"- Based on the price, there are differences among the carbody.\n- While Wagon cars the leats expensive ones, hardtop and the convertibles are the most expensive ones.","2b4b8553":"### Carbody & Price","c180ab76":"<a id=\"11\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Feature Importance<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","d060654c":"<a id=\"0\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Data<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","8f9b3464":"- Our dataset has 7 different engine types and price changes amongs them significantly.","cbb91a29":"- Let's drop the 'citympg'","e6d10cff":"- Baseline Model,in our case, Linear Regression model, without and scaling and transformation did a quite a good job.","4de4603f":"<a id=\"toc\"><\/a>\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n    \n* [Data](#0)\n* [What Problem We Have and Which Metric to Use?](#1)\n\n* [Exploratory Data Analysis](#2)\n    * [Target Variable](#3)\n    * [Numerical Features](#4)\n    * [Categorical Features](#5)    \n    \n* [Model Selection](#6)    \n    * [Baseline Model](#7)\n    * [Models with Ridge & Lasso & ElasticNet and KNN](#8)\n    * [Models with Random Forest & Extra Trees & Gradient Boosting & XGBoost](#9)    \n    * [Best Model with Hyperparameter Tuning](#10)\n    * [Feature Importance](#11)    \n\n\n* [Conclusion](#12)\n\n* [References & Further Reading](#13)\n","d8871fa8":"gif credit: https:\/\/giphy.com\/","1dc4418e":"- Even though target variable has right skewness, I will not make any transformation on it.\n","07567378":"### With np.log","2c5d04dc":"# Car Price Prediction Data","538d3013":"![](https:\/\/media.giphy.com\/media\/3jVT4U5bilspG\/giphy.gif?cid=ecf05e47ijcazulbfateoqyazwbckrtfakr1olt4krmdycsd&rid=giphy.gif&ct=g)","e9905b77":"- Let's observe the correlation among the numerical features\n- And also observe the correlation with the target variable","e4363dbe":"- We have developed model to predict car price problem.\n\n- First, we  made the detailed exploratory analysis.\n- We have decided which metric to use.\n- We analyzed both target and features in detail.\n- We transform categorical variables into numeric so we can use them in the model.\n- We transform numerical variables to reduce skewness and get close to normal  distribution.\n- We use pipeline to avoid data leakage.\n- We looked at the results of the each model and selected the best one for the problem in hand.\n- We made hyperparameter tuning of the best model see the improvement\n- We looked at the feature importance.\n\n\n\n- After this point it is up to you to develop and improve the models.  **Enjoy** \ud83e\udd18","4d063201":"<a id=\"6\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>MODEL SELECTION<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","f31bb148":"### Engine location & Price","6d434d5d":"- Our dataset has 8 different fuel system and price changes amongs them significantly.","6e86a81d":"- Rear wheel drive cars are the most expensive ones.  Front wheel cars the least expensive ones.","04c49c33":"Turbo aspiration is more expensive than standard aspiration","38753ef7":"- Based on the data and data dictionary, We have prediction \/ regression problem.\n- We wil make prediction on the target variable **PRICE**\n- And we will build a model to get best prediction on the price variable.\n- For that we will use RMSE(Root Mean Squared Error) and R2\n- [For the detailed info about the evaluation metrics](https:\/\/www.kaggle.com\/kaanboke\/the-most-common-evaluation-metrics-a-gentle-intro)","45894db1":"- Let's drop the 'model' and 'carid' columns"}}