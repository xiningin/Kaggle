{"cell_type":{"ea2ae626":"code","42655da8":"code","d764684a":"code","a1932de5":"code","067a25de":"code","9ad337f7":"code","e47ba42d":"code","b59744d3":"code","2c82f252":"code","cba63544":"code","10477491":"code","e9c8ab38":"code","3884626d":"code","c64729a4":"code","a21f9eb0":"code","8eae426b":"code","b2e2b2de":"code","0d49bcd9":"code","a528dc31":"code","7d5f3f77":"code","af755a03":"code","af6e8ea7":"code","4be9a9c4":"code","6b5df94d":"code","83c83469":"code","96d929c8":"code","03d6b728":"code","d9590714":"code","f6b10b0d":"code","7ccef513":"code","7c5466f8":"code","2bcfcfad":"code","6a6bc992":"code","7c913cb4":"code","6dfe87a1":"code","2ec4f6f3":"code","29f50233":"code","d2bdc350":"code","0920e994":"code","f8a661f7":"code","27f8a51b":"code","93ccdea2":"code","fb3b91bb":"code","a3cd8461":"code","30c1992f":"code","beb09a9b":"code","7d096e56":"code","64e2fede":"code","239bb97f":"code","8da7366f":"code","efc67b4d":"code","60b6d780":"code","69c92678":"code","550d52e3":"code","6af5aaae":"code","2bb1f1e6":"code","a67ecbe8":"code","e7ddba16":"code","6c8bcf4e":"code","11f913c4":"code","ecd086ad":"code","2d3db634":"code","458fceb1":"code","9b85a938":"code","09c997f3":"code","b3ee20d1":"code","2314d482":"code","4fc3b197":"code","74a996a1":"code","3e666a8d":"code","6e391289":"code","43bf0ba2":"code","e2f1aea0":"code","2f7c5943":"code","ca4aabf7":"code","7ae3c1e1":"code","af6e8fef":"code","836f382e":"code","444310f4":"code","3c8da411":"code","768e6bd8":"code","f6dbb8c2":"code","27c69459":"code","e1613230":"code","bd826e1f":"code","f70a2c1b":"code","2dacd79d":"code","66db3a5f":"code","e52d5277":"code","d019c06a":"code","53fc7928":"code","dda35d2a":"code","4d188e1f":"code","5722ce03":"code","7e96e5c4":"code","9561ba09":"code","9123625d":"code","edd8c5be":"code","b78b75d4":"code","82401b98":"code","641728c5":"code","3929b624":"code","4172a039":"code","4a95cb60":"code","23699be0":"code","150755ac":"code","a41b766c":"code","3959dc39":"code","859724da":"code","53135ca0":"code","a20a5d38":"code","b2535cec":"code","85ae9f3e":"code","17c8f784":"code","d3527a25":"code","a630cd22":"code","e8f68fab":"code","5ae84e29":"code","3a873b1d":"code","b2f34966":"code","58bc789f":"code","ba8cd1f5":"code","b1b81bd3":"code","3085e182":"code","e482ccf5":"code","3ee0eb72":"code","dcd87343":"code","1cb14889":"code","e415b1c7":"code","5b67324d":"code","70d271cf":"code","3c7a1540":"code","e2637640":"code","bec93f80":"code","a08d1965":"code","2dfbfcce":"code","e45048cd":"code","17158b0e":"code","bbfb76a3":"code","8f9c10c4":"code","a36ce8d9":"markdown","35845f96":"markdown","1711611a":"markdown","add772ef":"markdown","4c905a43":"markdown","1e3a6b57":"markdown","5b3d2d39":"markdown","92abcf87":"markdown","51dd0e78":"markdown","c4af5e27":"markdown","c0642e0a":"markdown","c8887988":"markdown","a66ec84a":"markdown","c21cdf8a":"markdown","5635ebd0":"markdown","9c1552f3":"markdown","478a46af":"markdown","82d52779":"markdown","8d1413b7":"markdown","99b7a26a":"markdown","ca719d33":"markdown","cfbe38b9":"markdown","68fe8546":"markdown","16aea495":"markdown","f05137d2":"markdown","1f17baea":"markdown","7089f186":"markdown","93bca420":"markdown","0bed6b11":"markdown","c826d5a4":"markdown","f5694e06":"markdown","3c3d0143":"markdown","9de5dc05":"markdown","fcd910e2":"markdown","5894f654":"markdown","73a8cf91":"markdown","adc4dcfd":"markdown","404bd818":"markdown","98b6d034":"markdown","3360ca66":"markdown","6859f50f":"markdown","fdf04d27":"markdown","ec12ee53":"markdown","afa5651c":"markdown","1e4a296a":"markdown","ff149fda":"markdown","51b679b8":"markdown","a2616677":"markdown","8e35814a":"markdown","38036680":"markdown","dbdb151a":"markdown","50d7b444":"markdown","97559294":"markdown","ef09e452":"markdown","a8fe677a":"markdown","05575966":"markdown","b4b9e9a0":"markdown","451e8509":"markdown","8045807f":"markdown","c20f0d44":"markdown","ea3e8b57":"markdown","bd939ed2":"markdown"},"source":{"ea2ae626":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import f1_score\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nimport warnings\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","42655da8":"train = pd.read_csv('\/kaggle\/input\/janatahack-machine-learning-for-banking\/train_fNxu4vz.csv')\ntest = pd.read_csv('\/kaggle\/input\/janatahack-machine-learning-for-banking\/test_fjtUOL8.csv')\nsample = pd.read_csv('\/kaggle\/input\/janatahack-machine-learning-for-banking\/sample_submission_HSqiq1Q.csv')","d764684a":"print(train.shape)\nprint(test.shape)\nprint(sample.shape)","a1932de5":"train.columns","067a25de":"test.columns","9ad337f7":"categorical_columns = train.select_dtypes(exclude=['int', 'float']).columns\ncategorical_columns","e47ba42d":"numerical_columns = train.select_dtypes(include=['int', 'float']).columns\nnumerical_columns","b59744d3":"print(train['Interest_Rate'].value_counts())\nsns.countplot(train['Interest_Rate'])","2c82f252":"train.isna().sum()","cba63544":"test.isna().sum()","10477491":"train.isna().sum()\/train.shape[0]","e9c8ab38":"test.isna().sum()\/test.shape[0]","3884626d":"train['Interest_Rate'] = train['Interest_Rate'].astype(int)","c64729a4":"print(train['Months_Since_Deliquency'].skew())\nprint(train['Months_Since_Deliquency'].kurtosis())\nsns.distplot(train['Months_Since_Deliquency'])","a21f9eb0":"df = train.append(test)","8eae426b":"df.shape","b2e2b2de":"\ndf.plot(kind = 'box',figsize=(20,5))","0d49bcd9":"df.columns","a528dc31":"df['Loan_Amount_Requested'] #Loan applied by the borrower","7d5f3f77":"df['Loan_Amount_Requested'].isna().sum()","af755a03":"df['Loan_Amount_Requested'].dtypes","af6e8ea7":"df['Loan_Amount_Requested'] = df['Loan_Amount_Requested'].str.replace(',', '').astype(int)","4be9a9c4":"print(df['Loan_Amount_Requested'].skew())\nprint(df['Loan_Amount_Requested'].kurtosis())\nsns.distplot(df['Loan_Amount_Requested'])","6b5df94d":"print(df['Loan_Amount_Requested'].min())\nprint(df['Loan_Amount_Requested'].max())","83c83469":"fig, ax = plt.subplots(figsize=(5,5))\nplt.suptitle('')\ndf.boxplot(column=['Loan_Amount_Requested'], by='Interest_Rate', ax=ax)","96d929c8":"pd.cut(df['Loan_Amount_Requested'],bins = 3)","03d6b728":"df['Loan_label'] = pd.cut(x=df['Loan_Amount_Requested'], bins= 3, labels=['Low','Medium','High'], right=True)\n\nprint(df['Loan_Amount_Requested'])\n\nprint(df['Loan_label'].unique())","d9590714":"dic = {'Low':1,'Medium':2,'High':3}\ndf['Loan_label'] = df['Loan_label'].map(dic)","f6b10b0d":"print(df['Loan_Amount_Requested'].corr(df['Interest_Rate']))\nprint(df['Loan_label'].corr(df['Interest_Rate']))","7ccef513":"df['Length_Employed']","7c5466f8":"df['Length_Employed'].value_counts()","2bcfcfad":"df['Length_Employed'] = df['Length_Employed'].replace('10+ years','10 years')\ndf['Length_Employed'] = df['Length_Employed'].replace('< 1 year','0 years')","6a6bc992":"df.head()","7c913cb4":"plt.figure(figsize=(10,5))\nsns.countplot(df['Length_Employed'])","6dfe87a1":"df['Length_Employed'].fillna(df['Length_Employed'].mode()[0],inplace = True)","2ec4f6f3":"df['Length_Employed'].isna().sum()","29f50233":"plt.figure(figsize=(10,5))\nsns.countplot(df['Length_Employed'])","d2bdc350":"df[['A','B']] = df['Length_Employed'].str.split(\" \",expand = True)","0920e994":"df['Length_Employed'] = df['A']\ndel df['A']\ndel df['B']","f8a661f7":"df['Length_Employed'] = df['Length_Employed'].astype(int)","27f8a51b":"df['Home_Owner'].value_counts()","93ccdea2":"df.groupby('Home_Owner')['Interest_Rate'].unique()","fb3b91bb":"pd.crosstab(train['Home_Owner'],train['Interest_Rate'])","a3cd8461":"df['Home_Owner'].isna().sum()\/df.shape[0]","30c1992f":"sns.countplot(df['Home_Owner'])","beb09a9b":"df['Home_Owner'].fillna(df['Home_Owner'].fillna(df['Home_Owner'].mode()[0]),inplace = True)","7d096e56":"sns.countplot(df['Home_Owner'])","64e2fede":"print(df['Annual_Income'].skew())\nprint(df['Annual_Income'].kurtosis())\nsns.distplot(df['Annual_Income'])","239bb97f":"df['Annual_Income'].isna().sum()\/df['Annual_Income'].shape[0]","8da7366f":"print(df['Annual_Income'].min())\nprint(df['Annual_Income'].max())","efc67b4d":"df['Annual_Income'].fillna(df['Annual_Income'].median(),inplace = True)","60b6d780":"df['Annual_Income']","69c92678":"num = [200000,500000,100000]\n\nprint(type(num[0]))\n\n\ndf['Income_label'] = pd.cut(x=df['Annual_Income'], bins= 3, labels=['Low','Medium','High'], right=True)\n\nprint(df['Annual_Income'])\n\nprint(df['Income_label'].unique())","550d52e3":"dic = {'Low':1,'Medium':2,'High':3}\ndf['Income_label'] = df['Income_label'].map(dic)","6af5aaae":"df['Income_Verified'].value_counts()","2bb1f1e6":"df['Income_Verified'].isna().sum()","a67ecbe8":"sns.countplot(df['Income_Verified'])","e7ddba16":"df['Purpose_Of_Loan'].isna().sum()","6c8bcf4e":"plt.figure(figsize=(25,8))\nsns.countplot(df['Purpose_Of_Loan'])","11f913c4":"df['Debt_To_Income'].isna().sum()","ecd086ad":"print(df['Debt_To_Income'].skew())\nprint(df['Debt_To_Income'].kurtosis())\nsns.distplot(df.Debt_To_Income)","2d3db634":"df['Inquiries_Last_6Mo'].isna().sum()","458fceb1":"sns.countplot(df['Inquiries_Last_6Mo'])","9b85a938":"df['Months_Since_Deliquency']","09c997f3":"deli = []\nfor i in df['Months_Since_Deliquency']:\n    if pd.isnull(i) == True:\n        deli.append(0)\n    else:\n        deli.append(1)\ndf['Deliquency'] = deli","b3ee20d1":"df['Deliquency'].value_counts()","2314d482":"df.drop('Months_Since_Deliquency',axis = 1,inplace = True)","4fc3b197":"df['Number_Open_Accounts'].describe()","74a996a1":"df['Number_Open_Accounts'].dtype","3e666a8d":"plt.figure(figsize = (20,5))\nsns.countplot(df['Number_Open_Accounts'])","6e391289":"df['Number_Open_Accounts'].isna().sum()","43bf0ba2":"df['Total_Accounts'].dtype","e2f1aea0":"plt.figure(figsize = (30,5))\nsns.countplot(df['Total_Accounts'])","2f7c5943":"df['Total_Accounts'].isna().sum()","ca4aabf7":"print(df['Total_Accounts'].corr(df['Interest_Rate']))","7ae3c1e1":"df['Gender'].value_counts()","af6e8fef":"df['Gender'].isna().sum()","836f382e":"sns.countplot(df['Gender'])","444310f4":"df['Gender'].isna().sum()","3c8da411":"df[\"Number_Invalid_Acc\"] = df[\"Total_Accounts\"] - df[\"Number_Open_Accounts\"]\ndf[\"Number_Years_To_Repay_Debt\"] = df[\"Loan_Amount_Requested\"]\/df[\"Annual_Income\"]\n","768e6bd8":"df = df[['Loan_ID','Loan_Amount_Requested','Loan_label','Length_Employed','Home_Owner','Annual_Income','Income_label','Income_Verified','Purpose_Of_Loan','Debt_To_Income','Inquiries_Last_6Mo','Number_Open_Accounts','Total_Accounts','Deliquency','Gender','Number_Invalid_Acc','Number_Years_To_Repay_Debt','Interest_Rate']]","f6dbb8c2":"df.head()","27c69459":"trains = df[df['Interest_Rate'].isna() == False] \ntests = df[df['Interest_Rate'].isna() == True]","e1613230":"trains['Interest_Rate'] = trains['Interest_Rate'].astype(int)","bd826e1f":"trains","f70a2c1b":"trains.groupby('Length_Employed')['Loan_Amount_Requested'].agg(['count','min','max','mean','median','std'])","2dacd79d":"trains.groupby('Home_Owner')['Loan_Amount_Requested'].agg(['count','min','max','mean','median','std'])","66db3a5f":"plt.figure(figsize=(12,6))\nsns.scatterplot(x=trains['Loan_Amount_Requested'],y=trains['Annual_Income'])","e52d5277":"trains.groupby('Purpose_Of_Loan')['Loan_Amount_Requested'].agg(['count','min','max','mean','median','std'])","d019c06a":"trains.groupby('Interest_Rate')['Loan_Amount_Requested'].agg(['count','min','max','mean','median','std'])","53fc7928":"trains.groupby('Interest_Rate')['Annual_Income'].agg(['count','min','max','mean','median','std'])","dda35d2a":"df_new = trains.append(tests)","4d188e1f":"trains.drop('Loan_ID',axis =1,inplace = True )\ntests.drop('Loan_ID',axis =1,inplace = True )","5722ce03":"X_train, Y = trains.drop([\"Interest_Rate\"], axis=1).values, trains[\"Interest_Rate\"].astype(int).values\nX_test = tests.values\n\nX_train.shape, Y.shape, X_test.shape","7e96e5c4":"trains.head()","9561ba09":"kfold, scores = KFold(n_splits=5, shuffle=True, random_state=0), list()\nfor train, test in kfold.split(X_train):\n    x_train, x_test = X_train[train], X_train[test]\n    y_train, y_test = Y[train], Y[test]\n    \n    model = CatBoostClassifier(random_state=27, max_depth=4, n_estimators=1000, verbose=500)\n    model.fit(x_train, y_train, cat_features=[1,2,3,5,6,7,12,13])\n    preds = model.predict(x_test)\n    score = f1_score(y_test, preds, average=\"weighted\")\n    scores.append(score)\n    print(score)\nprint(\"Average: \", sum(scores)\/len(scores))","9123625d":"model = CatBoostClassifier(random_state=27, n_estimators=1000, max_depth=4, verbose=500)\nmodel.fit(X_train, Y, cat_features=[1,2,3,5,6,7,12,13])\npreds1 = model.predict(X_test)","edd8c5be":"feat_imp = pd.Series(model.feature_importances_, index=trains.drop([\"Interest_Rate\"], axis=1).columns)\nfeat_imp.nlargest(30).plot(kind='barh', figsize=(8,10))","b78b75d4":"sample['Interest_Rate'] = preds1\nsample.to_csv('Solution_with_Cat.csv',index=False)","82401b98":"cat_columns = ['Home_Owner','Income_Verified','Purpose_Of_Loan','Gender']","641728c5":"from sklearn.preprocessing import LabelEncoder \n  \nle = LabelEncoder() \n\nfor columns in cat_columns:\n    df_new[columns]= le.fit_transform(df_new[columns]) ","3929b624":"df_new.head()","4172a039":"col = ['Loan_label','Length_Employed','Home_Owner','Income_label','Income_Verified','Purpose_Of_Loan','Inquiries_Last_6Mo','Deliquency','Gender']","4a95cb60":"df_new = pd.get_dummies(df_new)","23699be0":"train_df = df_new[df_new['Interest_Rate'].isna() == False] \ntest_df = df_new[df_new['Interest_Rate'].isna() == True]","150755ac":"train_df","a41b766c":"test_df","3959dc39":"train_df.drop('Loan_ID',axis = 1,inplace = True)\ntest_df.drop('Loan_ID',axis = 1,inplace = True)","859724da":"train_df['Interest_Rate'] = train_df['Interest_Rate'].astype(int)","53135ca0":"test_df.drop('Interest_Rate',axis = 1,inplace = True)","a20a5d38":"import h2o\nh2o.init()\n\ntrain1 = h2o.H2OFrame(train_df)\ntest1 = h2o.H2OFrame(test_df)\ntrain1.columns\ny = 'Interest_Rate'\nx = train1.col_names\nx.remove(y)\ntrain1['Interest_Rate'] = train1['Interest_Rate'].asfactor()\ntrain1['Interest_Rate'].levels()\nfrom h2o.automl import H2OAutoML\naml = H2OAutoML(max_models = 30, max_runtime_secs=1000, seed = 1)\naml.train(x = x, y = y, training_frame = train1)\n","b2535cec":"preds = aml.predict(test1)\npreds\nans=h2o.as_list(preds) \n\nsample['Interest_Rate'] = ans['predict']\nsample.to_csv('Solution_with_autoML.csv',index=False)","85ae9f3e":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV, train_test_split,RandomizedSearchCV\nfrom math import sqrt\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n#%matplotlib inline \nimport os\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score","17c8f784":"#Model\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\nY = train_df['Interest_Rate']\nX = train_df.drop('Interest_Rate',axis = 1)\nX1 = pd.get_dummies(X)\nX_test = test_df","d3527a25":"Y = Y-1","a630cd22":"Y.value_counts()","e8f68fab":"evals_result = {}\nfeature_imp = pd.DataFrame()\nfeatures = [feat for feat in X1.columns]\nfolds = StratifiedKFold(n_splits=3, shuffle=False, random_state =8736)\nparam = {\n    'boost_from_average':'false',\n    'boosting_type': 'gbdt',\n    'feature_fraction': 0.54,\n    'learning_rate': 0.005,\n    'max_depth': -1,  \n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 16.0,\n    'num_leaves': 40,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'multiclass',\n    'num_class': 3,\n    'verbosity': 1,\n    \"n_jobs\":-1,\n    \"metric\" : \"multi_logloss\",\n}\n\npredictions = np.zeros((len(X1),3))\npredictions_test = np.zeros((len(X_test),3))","5ae84e29":"X","3a873b1d":"X_test","b2f34966":"for fold_, (train_idx,val_idx) in enumerate(folds.split(X1.values,Y.values)):\n    print(\"Fold {}\".format(fold_+1))\n    d_train = lgb.Dataset(X1.iloc[train_idx][features], label=Y.iloc[train_idx])\n    d_val = lgb.Dataset(X1.iloc[val_idx][features],label=Y.iloc[val_idx])\n    num_round = 1000000\n    clf = lgb.train(param,d_train,num_round,valid_sets=[d_train,d_val],verbose_eval=1000, early_stopping_rounds=5000,evals_result=evals_result)\n    oof = clf.predict(X1.iloc[val_idx][features],num_iteration=clf.best_iteration)\n    fold_imp = pd.DataFrame()\n    fold_imp[\"Feature\"] = features\n    fold_imp[\"importance\"] = clf.feature_importance()\n    fold_imp[\"fold\"] = fold_ +1\n    feat_imp_df = pd.concat([feature_imp,fold_imp], axis=0)\n    predictions += clf.predict(X1, num_iteration=clf.best_iteration)\n    predictions_test += clf.predict(X_test, num_iteration=clf.best_iteration)\n    pred_lab = pd.DataFrame([np.argmax(pr) for pr in predictions])\n    oof_lab = pd.DataFrame([np.argmax(pr) for pr in oof])\n    acc_score = accuracy_score(Y,pred_lab)\n    oof_acc = accuracy_score(Y.iloc[val_idx],oof_lab)\n    print(\"OOF Accuracy {} and Training Accuracy {}\".format(oof_acc,acc_score))","58bc789f":"prediction_test_lab = pd.DataFrame([np.argmax(pr) for pr in predictions_test])\nprediction_test_lab = prediction_test_lab+1","ba8cd1f5":"prediction_test_lab","b1b81bd3":"test = list(df[df[\"Interest_Rate\"].isnull()][\"Loan_ID\"])\nsub = pd.DataFrame({\"Loan_ID\":test,\"Interest_Rate\":prediction_test_lab[0]})\n","3085e182":"train['Interest_Rate'].value_counts()","e482ccf5":"sub['Interest_Rate'].value_counts()","3ee0eb72":"train_df","dcd87343":"test_df","1cb14889":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.feature_selection import SelectFwe, f_regression\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tpot.builtins import OneHotEncoder, StackingEstimator","e415b1c7":"def extra_tree(Xtrain,Ytrain,Xtest):\n    extra = ExtraTreesClassifier()\n    extra.fit(Xtrain, Ytrain) \n    extra_prediction = extra.predict(Xtest)\n    return extra_prediction","5b67324d":"def Xg_boost(Xtrain,Ytrain,Xtest):\n    xg = XGBClassifier(loss='exponential', learning_rate=0.05, n_estimators=1000, subsample=1.0, criterion='friedman_mse', \n                                  min_samples_split=2, \n                                  min_samples_leaf=5, min_weight_fraction_leaf=0.0, max_depth=10, min_impurity_decrease=0.0, \n                                  min_impurity_split=None, \n                                  init=None, random_state=None, max_features=None, verbose=1, max_leaf_nodes=None, warm_start=False, \n                                  presort='deprecated', \n                                  validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n    xg.fit(Xtrain, Ytrain) \n    xg_prediction = xg.predict(Xtest)\n    return xg_prediction","70d271cf":"def LGBM(Xtrain,Ytrain,Xtest):\n    lgbm = LGBMClassifier(boosting_type='gbdt', num_leaves=40,\n                            max_depth=5, learning_rate=0.05, n_estimators=1000, subsample_for_bin=200, objective='binary', \n                            min_split_gain=0.0, min_child_weight=0.001, min_child_samples=10,\n                            subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0,\n                            reg_lambda=0.0, random_state=None, n_jobs=1, silent=True, importance_type='split')\n    #lgbm = LGBMClassifier(n_estimators= 500)\n    lgbm.fit(X_train, Y_train)\n    lgbm_preds = lgbm.predict(X_test)\n    return lgbm_preds","3c7a1540":"Y_train = train_df['Interest_Rate']\nX_train = train_df.drop('Interest_Rate',axis = 1)\nX_test = test_df","e2637640":"pred_xg = Xg_boost(X_train,Y_train,X_test)\npred_et = extra_tree(X_train,Y_train,X_test)\npred_l = LGBM(X_train,Y_train,X_test)","bec93f80":"sample['Interest_Rate'] = pred_xg\nprint(sample['Interest_Rate'].unique())\nsample.to_csv('XG.csv',index = False)","a08d1965":"sample['Interest_Rate'] = pred_et\nprint(sample['Interest_Rate'].unique())\nsample.to_csv('ET.csv',index = False)","2dfbfcce":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0).fit(X_train, Y_train)\nans = clf.predict(X_test)","e45048cd":"sample['Interest_Rate'] = ans\nprint(sample['Interest_Rate'].unique())\nsample.to_csv('LR.csv',index = False)","17158b0e":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=10).fit(X_train, Y_train)\nprediction_of_rf = rf.predict(X_test)\nsample['Interest_Rate'] = prediction_of_rf\nprint(sample['Interest_Rate'].unique())\nsample.to_csv('RF.csv',index = False)","bbfb76a3":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X_train,Y_train)\n\n# Predicted class\nnri = neigh.predict(X_test)","8f9c10c4":"sample['Interest_Rate'] = nri\nprint(sample['Interest_Rate'].unique())\nsample.to_csv('KNN.csv',index = False)","a36ce8d9":"Average Loan_Amount_Requested is maximum for people with Home_Owner status as Mortgage.","35845f96":"# Problem Statement","1711611a":"# Annual_Income","add772ef":"# # *Model* Building","4c905a43":"# kurtosis","1e3a6b57":"![image.png](attachment:image.png)","5b3d2d39":"h20 - AutoML","92abcf87":"Annual Income has lot of outliers","51dd0e78":"So, when is the skewness too much?\n\nThe rule of thumb seems to be:\n\nIf the skewness is between -0.5 and 0.5, the data are fairly symmetrical.\n\nIf the skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed), the data are moderately skewed.\n\nIf the skewness is less than -1(negatively skewed) or greater than 1(positively skewed), the data are highly skewed.\n\n","c4af5e27":"# Loading Data","c0642e0a":"conclusion - It is a balanced Data set","c8887988":"Planning to remove Months_Since_Deliquency since it has 50% null value","a66ec84a":"# # **try - Different models[[](http:\/\/)](http:\/\/)","c21cdf8a":"# EDA and Data Vizualization","5635ebd0":"Planning to create a new column on the basis of loan amount","9c1552f3":"# Insights from Data","478a46af":"Idea is to convert\n\n10+ years to 10 and < 1 years to 0 ","82d52779":"categorical_columns","8d1413b7":"# gender","99b7a26a":"One hot Encoding","ca719d33":"# Modeling without label encoding","cfbe38b9":"# # Modelling with Label and one hot encoding ","68fe8546":"# Months_Since_Deliquency","16aea495":"Range of values","f05137d2":"# Income_Verified","1f17baea":"# Lets explore each variable and try to fill the missing values","7089f186":"**Please Upvote if you like it**","93bca420":"Loan_Amount_Requested is in object type while it should be numerical value \n","0bed6b11":"Learn more about kurtosis at my [blog ](http:\/\/www.kaggle.com\/learn-forum\/154867)","c826d5a4":"# Data Description","f5694e06":"# The data is ready for modelling","3c3d0143":"15% are null","9de5dc05":"# Purpose_Of_Loan","fcd910e2":"Pearson's correlation for continuous pair variables, as we know.\n\nspearman's correlation for ordinal pair of variables or at least one of the ordinal variables.\n\nchi-square or Cramer's V test for categorical pair of variables, give you association.\n\nPerform ANOVA and ANCOVA to see the significance of continuous variables across the categories of categorical variables.","5894f654":"It is in categorical type, we can change into range ","73a8cf91":"lightGBM, XGBoost, CatBoost, Extra Tree Clasifier can give good results","adc4dcfd":"Points at the top of the plot that although the Annual_Income is high, the Loan_Amount_Requested is low.\nWe can also see a few Annual_Incomes that are more than 1000000.","404bd818":"1. Average loan amount requested is highest for small business followed by debt_consolidation.\n2. Average income requested is also high for House loans.","98b6d034":"Postively skewed\n\nDo not follow a normal distribution curve\n\nhas less outlier as kurtosis is negative and no tail","3360ca66":"Baseline: Kurtosis value of 0\n\nData that follow a normal distribution perfectly have a kurtosis value of 0. Normally distributed data establishes the baseline for kurtosis. Sample kurtosis that significantly deviates from 0 may indicate that the data are not normally distributed.\n\nPositive kurtosis:\n\nA distribution with a positive kurtosis value indicates that the distribution has heavier tails and a sharper peak than the normal distribution. For example, data that follow a t distribution have a positive kurtosis value.\n\nNegative kurtosis:\n\nA distribution with a negative kurtosis value indicates that the distribution has lighter tails and a flatter peak than the normal distribution. For example, data that follow a beta distribution with first and second shape parameters equal to 2 have a negative kurtosis value.","6859f50f":"Creating new features","fdf04d27":"# Number_Open_Accounts","ec12ee53":"# Total_Accounts","afa5651c":"# Label encoding","1e4a296a":"The mean Annual_Income for Interest Rate Category 1 is the highest. And it is almost the same for Interest Rate categories 2 and 3.","ff149fda":"# It is a classification model","51b679b8":"![image.png](attachment:image.png)","a2616677":"![image.png](attachment:image.png)","8e35814a":"Percentage of null values ","38036680":"No null values","dbdb151a":"# Completed EDA on all parameters","50d7b444":"# Length_Employed","97559294":"A negative kurtosis means that your distribution is flatter than a normal curve with the same mean and standard deviation.","ef09e452":"1. Average Loan Amount Requested is maximum for people with 10+ years of experience.\n2. Mean Loan_Amount_Requested is minimum for people with <1 year of being employed.","a8fe677a":"Clearly, there is a distinction between the average loan amount requested for the type of interest rates.\nAverage Loan Amount seems to be high for Interest Rate category 3 and there is not much between the loan amount for categories 1 and 2.","05575966":"# Inquiries_Last_6Mo","b4b9e9a0":"> Have you ever wondered how lenders use various factors such as credit score, annual income, the loan amount approved, tenure, debt-to-income ratio etc. and select your interest rates? \n> \n> The process, defined as \u2018risk-based pricing\u2019, uses a sophisticated algorithm that leverages different determining factors of a loan applicant. Selection of significant factors will help develop a prediction algorithm which can estimate loan interest rates based on clients\u2019 information. On one hand, knowing the factors will help consumers and borrowers to increase their credit worthiness and place themselves in a better position to negotiate for getting a lower interest rate. On the other hand, this will help lending companies to get an immediate fixed interest rate estimation based on clients information. Here, your goal is to use a training dataset to predict the loan rate category (1 \/ 2 \/ 3) that will be assigned to each loan in our test set.\n> \n> You can use any combination of the features in the dataset to make your loan rate category predictions. Some features will be easier to use than others.","451e8509":"# Home_Owner","8045807f":"handling missing values and Remove years","c20f0d44":"numerical_columns","ea3e8b57":"Replace all null values with mode","bd939ed2":"# Concating train and test data into single Dataframe for preprocessing"}}