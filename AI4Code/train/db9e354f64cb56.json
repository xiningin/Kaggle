{"cell_type":{"ac333638":"code","a421f902":"code","17b0640e":"code","44e30c21":"code","e5222d41":"code","5d47db44":"code","526ef6d2":"code","8362c4c6":"code","189cc283":"code","2b867174":"code","09eba2b4":"markdown","c4c4c5b8":"markdown","24c6ff28":"markdown","bf17eb6a":"markdown","582d141e":"markdown"},"source":{"ac333638":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport gc\nimport datetime\nimport numpy as np\nnp.random.seed(42)\nimport pandas as pd\nimport tensorflow as tf\ntf.random.set_seed(42)\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom scipy.optimize import minimize\nfrom tqdm.notebook import tqdm\nfrom time import time\n\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE","a421f902":"MIXED_PRECISION = False\nXLA_ACCELERATE = True\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","17b0640e":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\nss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\nss_2 = ss.copy()\nss_3 = ss.copy()\nss_blend = ss.copy()\n\ncols = [c for c in ss.columns.values if c != 'sig_id']","44e30c21":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ndef log_loss_metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']","e5222d41":"top_feats = [  0,   1,   2,   3,   5,   6,   8,   9,  10,  11,  12,  14,  15,\n        16,  18,  19,  20,  21,  23,  24,  25,  27,  28,  29,  30,  31,\n        32,  33,  34,  35,  36,  37,  39,  40,  41,  42,  44,  45,  46,\n        48,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n        63,  64,  65,  66,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n        78,  79,  80,  81,  82,  83,  84,  86,  87,  88,  89,  90,  92,\n        93,  94,  95,  96,  97,  99, 100, 101, 103, 104, 105, 106, 107,\n       108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n       121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134,\n       135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n       149, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 163, 164,\n       165, 166, 167, 168, 169, 170, 172, 173, 175, 176, 177, 178, 180,\n       181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 195,\n       197, 198, 199, 202, 203, 205, 206, 208, 209, 210, 211, 212, 213,\n       214, 215, 218, 219, 220, 221, 222, 224, 225, 227, 228, 229, 230,\n       231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245,\n       246, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260,\n       261, 263, 265, 266, 268, 270, 271, 272, 273, 275, 276, 277, 279,\n       282, 283, 286, 287, 288, 289, 290, 294, 295, 296, 297, 299, 300,\n       301, 302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 315,\n       316, 317, 320, 321, 322, 324, 325, 326, 327, 328, 329, 330, 331,\n       332, 333, 334, 335, 338, 339, 340, 341, 343, 344, 345, 346, 347,\n       349, 350, 351, 352, 353, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 368, 369, 370, 371, 372, 374, 375, 376, 377,\n       378, 379, 380, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n       392, 393, 394, 395, 397, 398, 399, 400, 401, 403, 405, 406, 407,\n       408, 410, 411, 412, 413, 414, 415, 417, 418, 419, 420, 421, 422,\n       423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n       436, 437, 438, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n       452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n       466, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 479, 482,\n       483, 485, 486, 487, 488, 489, 491, 492, 494, 495, 496, 500, 501,\n       502, 503, 505, 506, 507, 509, 510, 511, 512, 513, 514, 516, 517,\n       518, 519, 521, 523, 525, 526, 527, 528, 529, 530, 531, 532, 533,\n       534, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547,\n       549, 550, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563,\n       564, 565, 566, 567, 569, 570, 571, 572, 573, 574, 575, 577, 580,\n       581, 582, 583, 586, 587, 590, 591, 592, 593, 595, 596, 597, 598,\n       599, 600, 601, 602, 603, 605, 607, 608, 609, 611, 612, 613, 614,\n       615, 616, 617, 619, 622, 623, 625, 627, 630, 631, 632, 633, 634,\n       635, 637, 638, 639, 642, 643, 644, 645, 646, 647, 649, 650, 651,\n       652, 654, 655, 658, 659, 660, 661, 662, 663, 664, 666, 667, 668,\n       669, 670, 672, 674, 675, 676, 677, 678, 680, 681, 682, 684, 685,\n       686, 687, 688, 689, 691, 692, 694, 695, 696, 697, 699, 700, 701,\n       702, 703, 704, 705, 707, 708, 709, 711, 712, 713, 714, 715, 716,\n       717, 723, 725, 727, 728, 729, 730, 731, 732, 734, 736, 737, 738,\n       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n       752, 753, 754, 755, 756, 758, 759, 760, 761, 762, 763, 764, 765,\n       766, 767, 769, 770, 771, 772, 774, 775, 780, 781, 782, 783, 784,\n       785, 787, 788, 790, 793, 795, 797, 799, 800, 801, 805, 808, 809,\n       811, 812, 813, 816, 819, 820, 821, 822, 823, 825, 826, 827, 829,\n       831, 832, 833, 834, 835, 837, 838, 839, 840, 841, 842, 844, 845,\n       846, 847, 848, 850, 851, 852, 854, 855, 856, 858, 860, 861, 862,\n       864, 867, 868, 870, 871, 873, 874]\n\nprint(len(top_feats))","5d47db44":"def create_model(num_columns, hidden_units, dropout_rate, learning_rate):\n        \n    inp1 = tf.keras.layers.Input(shape = (num_columns, ))\n    x1 = tf.keras.layers.BatchNormalization()(inp1)\n    \n    for i, units in enumerate(hidden_units[0]):\n        x1 = tfa.layers.WeightNormalization(tf.keras.layers.Dense(units, activation = 'elu'))(x1)\n        x1 = tf.keras.layers.Dropout(dropout_rate[0])(x1)\n        x1 = tf.keras.layers.BatchNormalization()(x1)\n        \n    inp2 = tf.keras.layers.Input(shape = (num_columns, ))\n    x2 = tf.keras.layers.BatchNormalization()(inp2)\n    \n    for i, units in enumerate(hidden_units[1]):\n        x2 = tfa.layers.WeightNormalization(tf.keras.layers.Dense(units, activation = 'elu'))(x2)\n        x2 = tf.keras.layers.Dropout(dropout_rate[1])(x2)\n        x2 = tf.keras.layers.BatchNormalization()(x2)\n        \n    inp3 = tf.keras.layers.Input(shape = (num_columns, ))\n    x3 = tf.keras.layers.BatchNormalization()(inp3)\n    \n    for i, units in enumerate(hidden_units[2]):\n        x3 = tfa.layers.WeightNormalization(tf.keras.layers.Dense(units, activation = 'elu'))(x3)\n        x3 = tf.keras.layers.Dropout(dropout_rate[2])(x3)\n        x3 = tf.keras.layers.BatchNormalization()(x3)\n        \n    x = tf.keras.layers.Concatenate()([x1, x2, x3])\n    x = tf.keras.layers.Dropout(dropout_rate[3])(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    for units in hidden_units[3]:\n        \n        x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(units, activation = 'elu'))(x)\n        x = tf.keras.layers.Dropout(dropout_rate[4])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        \n    out = tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation = 'sigmoid'))(x)\n    \n    model = tf.keras.models.Model(inputs = [inp1, inp2, inp3], outputs = out)\n    \n    model.compile(optimizer = tfa.optimizers.Lookahead(tf.optimizers.Adam(learning_rate), sync_period = 10), \n                  loss = 'binary_crossentropy')\n    \n    return model","526ef6d2":"hidden_units = [[2048, 512, 2048], \n                [512, 1024, 512], \n                [512, 1024, 2048, 1024, 512], \n                [1024, 1024]]\n\ndropout_rate = [0.4, 0.3, 0.45, 0.3, 0.4]\n\nsize = int(np.ceil(0.8 * len(train.columns.values)))\n\nmodel = create_model(size, hidden_units, dropout_rate, 1e-3)\ntf.keras.utils.plot_model(model, show_shapes = False, show_layer_names= True,\n                          rankdir = 'TB', expand_nested = False, dpi = 96)","8362c4c6":"hidden_units = [[2048, 512, 2048], \n                [512, 1024, 512], \n                [512, 1024, 2048, 1024, 512], \n                [1024, 1024]]\n\ndropout_rate = [0.4, 0.3, 0.45, 0.3, 0.4]\n\nsize = int(np.ceil(0.8 * len(top_feats)))\n\nres = train_targets.copy()\nss.loc[:, train_targets.columns] = 0\nres.loc[:, train_targets.columns] = 0\n\nN_STARTS = 3\n\nfor seed in range(N_STARTS):\n    \n    split_cols = []\n    for _ in range(len(hidden_units) - 1):\n        split_cols.append(np.random.choice(top_feats, size))\n        \n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits = 5, random_state = seed, shuffle = True).split(train_targets, train_targets)):\n        \n        start_time = time()\n        \n        x_tr = [train.values[tr][:, split_cols[0]], train.values[tr][:, split_cols[1]], train.values[tr][:, split_cols[2]]]\n        x_val = [train.values[te][:, split_cols[0]], train.values[te][:, split_cols[1]], train.values[te][:, split_cols[2]]]\n        y_tr, y_val = train_targets.astype(float).values[tr], train_targets.astype(float).values[te]\n        x_tt = [test_features.values[:, split_cols[0]], test_features.values[:, split_cols[1]], test_features.values[:, split_cols[2]]]\n        \n        model = create_model(size, hidden_units, dropout_rate, 1e-3)\n        rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 3, verbose = 0, \n                                min_delta = 1e-4, min_lr = 1e-5, mode = 'min')\n        ckp = ModelCheckpoint(f'split_nn.hdf5', monitor = 'val_loss', verbose = 0, \n                              save_best_only = True, save_weights_only = True, mode = 'min')\n        es = EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, patience = 10, mode = 'min', \n                           baseline = None, restore_best_weights = True, verbose = 0)\n        history = model.fit(x_tr, y_tr, validation_data = (x_val, y_val), \n                            epochs = 100, batch_size = 128, callbacks = [rlr, ckp, es], verbose = 0)\n        hist = pd.DataFrame(history.history)\n        model.load_weights(f'split_nn.hdf5')\n        ss.loc[:, train_targets.columns] += model.predict(x_tt, batch_size = 128)\n        res.loc[te, train_targets.columns] += model.predict(x_val, batch_size = 128)\n        print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] Split NN: Seed {seed}, Fold {n}:', hist['val_loss'].min())\n        \n        K.clear_session()\n        del model, history, hist\n        x = gc.collect()\n        \nss.loc[:, train_targets.columns] \/= ((n + 1) * N_STARTS)\nres.loc[:, train_targets.columns] \/= N_STARTS","189cc283":"print(f'Split NN OOF Metric: {log_loss_metric(train_targets, res)}')\nres.loc[train['cp_type'] == 1, train_targets.columns] = 0\nss.loc[test['cp_type'] == 1, train_targets.columns] = 0\nprint(f'Split NN OOF Metric with postprocessing: {log_loss_metric(train_targets, res)}')","2b867174":"ss.to_csv('submission.csv', index = False)","09eba2b4":"# Submission","c4c4c5b8":"# Data Preparation","24c6ff28":"# Train Model\n\nFor each split, we use 80\\% of top features as input.","bf17eb6a":"# Split Neural Network\n\n[Split Neural Network (SplitNN)][1] is a kind of distributed learning approach. It uses multiple splits of NN to train with different portions of features and then aggregated by another NN. It is very similar to the stacking ensemble of multiple NNs. Let's see if this approach works.\n\n![SplitNN.png](attachment:SplitNN.png)\n\n[1]:https:\/\/arxiv.org\/pdf\/1812.00564.pdf","582d141e":"# Model Functions"}}