{"cell_type":{"baf6d0b3":"code","ec95633b":"code","9d553c5f":"code","68059e20":"code","50f6f3e3":"code","dd5260e6":"code","b6fb2441":"code","2017ce34":"code","bb20c455":"code","61cc43d9":"code","601e51c3":"code","6a597df1":"code","63168649":"markdown","86152990":"markdown","d56ef21e":"markdown","08d76dcb":"markdown","596f97f2":"markdown","2ab3ecc5":"markdown","2c99e864":"markdown","3fa50f8c":"markdown","702adb30":"markdown","a331f2cf":"markdown","64af0200":"markdown","bb885b7f":"markdown","5acb39d4":"markdown","43e61977":"markdown","23bea3c1":"markdown","7c830f8a":"markdown","476a26a8":"markdown","8169e36d":"markdown","6fa1fd7b":"markdown","b7dd3b56":"markdown","376cb432":"markdown","5c281269":"markdown","ddf1daee":"markdown","f985365e":"markdown","9cbea379":"markdown","2d2d1832":"markdown","1b8ab51a":"markdown","eba796fc":"markdown","daa2bb22":"markdown"},"source":{"baf6d0b3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom PIL import Image \nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout","ec95633b":"# List to store image data\ndata = []\n# List to store image labels (classes)\nlabels = []\n# Number of classes\nclasses = 43\n# Current path of the dataset\ncurrent_path = '..\/input\/german-traffic-signs-1\/gtsrb-preprocessed'\n\n\n# Iterates between 0 and 42 (43 classes)\nfor i in range(classes):\n    \n    # Path of each image\n    path = os.path.join(current_path, 'train', str(i))\n    images = os.listdir(path)\n    \n    # Iterates between each image\n    for a in images:\n        # Try to load the images\n        try:\n            # Open the image\n            image = Image.open(path + '\/' + a)\n            # Resizes the image to 30x30\n            image = image.resize((30, 30))\n            # Turns the image into an array\n            image = np.array(image)\n            # Append the image to \"data\" list\n            data.append(image)\n            # Append the label to \"labels\" list\n            labels.append(i)\n        # If it doesn't work, shows an error message\n        except:\n            print('Error loading images!')\n\n# Turns lists into array\ndata = np.array(data)\nlabels = np.array(labels)","9d553c5f":"print('DATA SHAPE: ', data.shape)\nprint('LABELS SHAPE', labels.shape)","68059e20":"# 20% to train\nX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.2, random_state=42)\n\nprint(X_train.shape,'|', X_test.shape,'|',y_train.shape,'|',y_test.shape)\n\n# Use \"to_categorical\" method to convert the labels present in y_train and y_test into one-hot encoding\ny_train = to_categorical(y_train, 43)\ny_test = to_categorical(y_test, 43)","50f6f3e3":"model = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=X_train.shape[1:]))\nmodel.add(Conv2D(32, kernel_size=(5, 5), activation='relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPool2D(2, 2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(43, activation='softmax'))","dd5260e6":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","b6fb2441":"# 15 epochs\nhistory = model.fit(X_train, y_train, batch_size=64, epochs=15, validation_data=(X_test, y_test))","2017ce34":"# Figure size\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\n# Plot train and validation accuracy\nplt.plot(history.history['accuracy'], label='train accuracy')\nplt.plot(history.history['val_accuracy'], label='validation accuracy')\nplt.title('Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\n\nplt.subplot(1, 2, 2)\n# Plot loss and validation loss\nplt.plot(history.history['loss'], label='train loss')\nplt.plot(history.history['val_loss'], label='validation loss')\nplt.title('Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","bb20c455":"y_test = pd.read_csv('..\/input\/test-data\/test.csv')\ny_test.head()","61cc43d9":"# Target\nlabels = y_test['ClassId'].values\n# Test data path\ncurrent_path = '..\/input\/german-traffic-signs-1\/gtsrb-preprocessed\/'\n# Images path\nimgs = current_path + y_test['Path'].values\n\n# Store image data\ndata = []\n\n\nfor img in imgs:\n    # Open image\n    image = Image.open(img)\n    # Resize to 30x30\n    image = image.resize((30, 30))\n    # Append in \"data\" list\n    data.append(np.array(image))\n    \n# Convert \"data\" list to array\nX_test = np.array(data)\n\n# Make predictions\npreds = model.predict_classes(X_test)\n\n# Evaluate model\nprint('ACCURACY: {} %'.format(round(accuracy_score(labels, preds) * 100, 3)))","601e51c3":"model.save('traffic_classifier.h5')","6a597df1":"# Libraries\nimport tkinter as tk\nfrom tkinter import filedialog\nfrom tkinter import *\nfrom PIL import ImageTk, Image\nimport numpy\nfrom keras.models import load_model\n\n# Load your model\nmodel = load_model('traffic_classifier.h5') # Path to your model\n\n# Dictionary to label all traffic signs class.\nclasses = { 1:'Speed limit (20km\/h)',\n            2:'Speed limit (30km\/h)', \n            3:'Speed limit (50km\/h)', \n            4:'Speed limit (60km\/h)', \n            5:'Speed limit (70km\/h)', \n            6:'Speed limit (80km\/h)', \n            7:'End of speed limit (80km\/h)', \n            8:'Speed limit (100km\/h)', \n            9:'Speed limit (120km\/h)', \n            10:'No passing', \n            11:'No passing veh over 3.5 tons', \n            12:'Right-of-way at intersection', \n            13:'Priority road', \n            14:'Yield', \n            15:'Stop', \n            16:'No vehicles', \n            17:'Veh > 3.5 tons prohibited', \n            18:'No entry', \n            19:'General caution', \n            20:'Dangerous curve left', \n            21:'Dangerous curve right', \n            22:'Double curve', \n            23:'Bumpy road', \n            24:'Slippery road', \n            25:'Road narrows on the right', \n            26:'Road work', \n            27:'Traffic signals', \n            28:'Pedestrians', \n            29:'Children crossing', \n            30:'Bicycles crossing', \n            31:'Beware of ice\/snow',\n            32:'Wild animals crossing', \n            33:'End speed + passing limits', \n            34:'Turn right ahead', \n            35:'Turn left ahead', \n            36:'Ahead only', \n            37:'Go straight or right', \n            38:'Go straight or left', \n            39:'Keep right', \n            40:'Keep left', \n            41:'Roundabout mandatory', \n            42:'End of no passing', \n            43:'End no passing veh > 3.5 tons' }\n\n# Initialise GUI\ntop=tk.Tk()\n# Window dimensions (800x600)\ntop.geometry('800x600')\n# Window title\ntop.title('Traffic sign classification')\n# Window background color\ntop.configure(background='#CDCDCD')\n# Window label\nlabel=Label(top,background='#CDCDCD', font=('arial',15,'bold'))\n# Sign image\nsign_image = Label(top)\n\n\n# Function to classify image\ndef classify(file_path):\n    global label_packed\n    # Open the image file path\n    image = Image.open(file_path)\n    # Resize the image\n    image = image.resize((30,30))\n    # Inserts a new axis that will appear at the axis position in the expanded array shape\n    image = numpy.expand_dims(image, axis=0)\n    # Convert to numpy array\n    image = numpy.array(image)\n    # Make prediction\n    pred = model.predict_classes([image])[0]\n    sign = classes[pred+1]\n    print(sign)\n    label.configure(foreground='#011638', text=sign) \n    \n# Function to show the \"classify\" button\ndef show_classify_button(file_path):\n    # Create the button\n    classify_b=Button(top,text=\"Classify Image\",command=lambda: classify(file_path),padx=10,pady=5)\n    # Configure button colors\n    classify_b.configure(background='#364156', foreground='white',font=('arial',10,'bold'))\n    # Configure button place (location)\n    classify_b.place(relx=0.79,rely=0.46)\n    \n# Function to upload image\ndef upload_image():\n    try:\n        # Path of the image\n        file_path=filedialog.askopenfilename()\n        # Open file path\n        uploaded=Image.open(file_path)\n        uploaded.thumbnail(((top.winfo_width()\/2.25),(top.winfo_height()\/2.25)))\n        im=ImageTk.PhotoImage(uploaded)\n        sign_image.configure(image=im)\n        sign_image.image=im\n        label.configure(text='')\n        show_classify_button(file_path)\n    except:\n        pass\n    \n# Create \"Upload\" button\nupload=Button(top,text=\"Upload an image\",command=upload_image,padx=10,pady=5)\n# \"Upload\" button colors and font\nupload.configure(background='#364156', foreground='white',font=('arial',10,'bold'))\n# Button location\nupload.pack(side=BOTTOM,pady=50)\nsign_image.pack(side=BOTTOM,expand=True)\nlabel.pack(side=BOTTOM,expand=True)\n# Window title text\nheading = Label(top, text=\"Know Your Traffic Sign\",pady=20, font=('arial',20,'bold'))\n# Window colors\nheading.configure(background='#CDCDCD',foreground='#364156')\nheading.pack()\ntop.mainloop()","63168649":"# Libraries","86152990":"Approximately 96% accuracy in the test. It is a great result, but it can be improved if we use a pre-trained CNN, but that is a topic for the next notebook.","d56ef21e":"- [Python Project on Traffic Signs Recognition with 95% Accuracy using CNN & Keras](https:\/\/data-flair.training\/blogs\/python-project-traffic-signs-recognition\/)","08d76dcb":"# Save model","596f97f2":"# References","2ab3ecc5":"To use the interface, copy and paste the code below into a text editor or IDE (Visual Studio Code, Spyder, Pycharm, Sublime Text etc.) and save the file with the name **GUI.py** (it is important that the file extension is **.py**), then run the code.","2c99e864":"### Evaluate","3fa50f8c":"In this extra section I am providing the code to build a **graphical user interface (GUI)** for the project.\nAccording to Wikipedia: \n> The graphical user interface (GUI) is a form of user interface that allows users to interact with electronic devices through graphical icons and audio indicator such as primary notation, instead of text-based user interfaces, typed command labels or text navigation. GUIs were introduced in reaction to the perceived steep learning curve of command-line interfaces (CLIs), which require commands to be typed on a computer keyboard. \n\nWe will use the TKinter library and if you are not familiar, I recommend checking the [Tkinter documentation](https:\/\/docs.python.org\/3\/library\/tkinter.html) or looking for some tutorials online.","702adb30":"# Description","a331f2cf":"### Instructions","64af0200":"### Project","bb885b7f":"![traffic sign gui](https:\/\/d2h0cx97tjks2p.cloudfront.net\/blogs\/wp-content\/uploads\/sites\/2\/2019\/12\/graphical-user-interface-project-in-python.png)","5acb39d4":"### Compile the model","43e61977":"The dataset contains more than 50,000 images of different traffic signs. It is further classified into 43 different classes. The dataset is quite varying, some of the classes have many images while some classes have few images. ","23bea3c1":"# Considerations","7c830f8a":"# Load data","476a26a8":"### Read test data (.csv)","8169e36d":"We will build a deep neural network model that can classify traffic signs present in the image into different categories. With this model, we are able to read and understand traffic signs which are a very important task for all autonomous vehicles.","6fa1fd7b":"### Code","b7dd3b56":"# Program interface","376cb432":"The architecture of our model is:\n\n- **2** Conv2D layer (filter=32, kernel_size=(5,5), activation=\u201drelu\u201d)\n- MaxPool2D layer ( pool_size=(2,2))\n- Dropout layer (rate=0.25)\n- **2** Conv2D layer (filter=64, kernel_size=(3,3), activation=\u201drelu\u201d)\n- MaxPool2D layer ( pool_size=(2,2))\n- Dropout layer (rate=0.25)\n- Flatten layer to squeeze the layers into 1 dimension\n- Dense Fully connected layer (256 nodes, activation=\u201drelu\u201d)\n- Dropout layer (rate=0.5)\n- Dense layer (43 nodes, activation=\u201dsoftmax\u201d)","5c281269":"# Plot results","ddf1daee":"### Train and validate the model","f985365e":"### Dataset","9cbea379":"# Split the data","2d2d1832":"# Test model with test dataset","1b8ab51a":"# Modeling","eba796fc":"### Output","daa2bb22":"Our dataset contains a **test-data** folder and in a **test.csv** file, we have the details related to the image path and their respective class labels."}}