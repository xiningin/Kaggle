{"cell_type":{"020a0831":"code","628929dc":"code","264576e0":"code","c03abf19":"code","3421cf24":"code","7903776c":"code","22a5c7d7":"code","a1f3ef49":"code","859d5a6f":"markdown","7b4dbd71":"markdown","fb86bc07":"markdown","a9766a24":"markdown","4a6434fa":"markdown","70d6c087":"markdown","17f7b2ba":"markdown","3919fa86":"markdown","f668d189":"markdown","9c3d5cae":"markdown"},"source":{"020a0831":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport numpy as np","628929dc":"df = pd.read_csv('..\/input\/wineuci\/Wine.csv')\n\ndf.columns = [  'name',\n                'alcohol',\n             \t'malicAcid',\n             \t'ash',\n            \t'ashalcalinity',\n             \t'magnesium',\n            \t'totalPhenols',\n             \t'flavanoids',\n             \t'nonFlavanoidPhenols',\n             \t'proanthocyanins',\n            \t'colorIntensity',\n             \t'hue',\n             \t'od280_od315',\n             \t'proline'\n                ]","264576e0":"X = df.drop(['name','alcohol'],axis=1)\ny = df['name'] - 1 # shifted y to range 0 to 1\nlabels = ['Wine 1', 'Wine 2', 'Wine 3'] # dataset does not supply the names of the labels","c03abf19":"X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=1) # random_state=1 for same results every time.\ninput_shape = [X_train.shape[1]]","3421cf24":"model = keras.Sequential([\n    # input layer\n    layers.BatchNormalization(input_shape=input_shape),\n    # hidden layer 1\n    layers.Dense(units=256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(rate=0.4),\n    # hidden layer 2\n    layers.Dense(units=128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(rate=0.4),\n    # hidden layer 3\n    layers.Dense(units=64, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(rate=0.4),\n    # hidden layer 4\n    layers.Dense(units=32, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(rate=0.4),\n    layers.Dense(units=3, activation='softmax')\n])\n\nmodel.compile(\n    optimizer=Adam(learning_rate=0.0001),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)","7903776c":"history = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=512,\n    epochs=700,\n)\nprint(\"\")","22a5c7d7":"### Loss Graph\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Learning Curve: Loss over Epochs\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend(['Training Loss', 'Validation Loss'])\n\n### Accuracy Graph\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Learning Curve: Accuracy over Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend(['Training Accuracy', 'Validation Accuracy'])","a1f3ef49":"y_actual = y_train.to_numpy()\ny_pred = model.predict(X_train, verbose=0)\ny_pred = np.argmax(y_pred, axis=-1)\n\nprint(\"On {} samples of untrained(test) dataset:\".format(len(y_pred)))\nprint(\"Prediction:\")\nprint(y_pred)\nprint(\"Actual:\")\nprint(y_actual)\n\n### Classification Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_actual,y_pred, target_names=labels))\n\n### Confusion Matrix Graph\ncm = confusion_matrix(y_true=y_actual, y_pred=y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\ndisp.plot()\nplt.title('Confusion Matrix')","859d5a6f":"### Results of Predictions","7b4dbd71":"## **1. Import modules**","fb86bc07":"### My Evaluation:\n\n- I tweaked the neural network through trial and error to consistently get low loss values for both the training and validation data for training.\n- I also made sure that the accuracy for both the training and validation data are above 0.9\n- Specifically, I increased the dropout rate and added batch normalization to try and reduce overfitting. I learned this from the Deep Learning course here on Kaggle.\n- Although the learning curve for loss and accuracy show that there are signs of overfitting as the training data does better than the validation data, it's miniscule enough to not affect the precision of the model.","a9766a24":"## **5. Evaluation of Model's Performance**\n### Results of Training\nLearning Curve showing Loss and Accuracy during training.","4a6434fa":"## **4. Training**","70d6c087":"### Defining features and labels","17f7b2ba":"## **2. Data Preparation**\n### Import data and adding headers","3919fa86":"### Splitting X and y and defining input shape","f668d189":"Carlo Antonio T. Taleon BSCS-2A | 2020-2021 | Written January 2021\n\n# **Wine Classification using Tensorflow with Keras**\n- This is my final project for the 1st semester of Introduction to Artificial Intelligence class at WVSU-CICT.\n- It's also the first notebook I'm writing on Kaggle. :)\n- This project classifies Wine using the [Wine Data Set on UCI](https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine).","9c3d5cae":"## **3. The Model**\n### Defining and compiling the model"}}