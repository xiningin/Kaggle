{"cell_type":{"7d726d63":"code","d4ac652b":"code","ae5608e8":"code","c0042731":"code","12cb6fc0":"code","87ae2cc5":"code","ae7dec44":"code","a78d6381":"code","ab502857":"code","863a6484":"code","6dabec8e":"code","0cbb68b6":"code","56954309":"code","0ce142eb":"code","b5d9b240":"code","32a44b2f":"code","8b0cc84e":"code","7fb2b246":"code","60c145f1":"code","9809873d":"code","ffe965fa":"code","5c6bb27a":"code","6863d8e2":"code","e20308a9":"code","27b6995c":"code","2d054f05":"code","fe33f644":"code","2c6a4ecd":"code","99c80add":"code","95096ee5":"code","316bd8da":"code","384207e8":"code","af744a98":"code","4fabdefe":"code","24f2ef61":"code","eea5cda2":"code","bf835c8d":"code","285228ed":"code","635e9542":"code","57cb9670":"code","479696ed":"code","6aabb03f":"code","e59e1141":"code","d4f2d0e8":"code","c367bd46":"code","97e1598c":"markdown","b48870ee":"markdown","e58f7029":"markdown","fe428d77":"markdown","3b23a363":"markdown","7b1e83e5":"markdown","f944f039":"markdown","13d19cb3":"markdown","3fabf457":"markdown","0be3e9dc":"markdown","d3aa370a":"markdown","67016b46":"markdown","bfd3ca11":"markdown","e19d1699":"markdown","7e5f8c6e":"markdown","8ee30265":"markdown","4807302d":"markdown","e178c46b":"markdown","962e0d6c":"markdown","7d507124":"markdown","2826a197":"markdown","38daee80":"markdown","4b22abb1":"markdown","aa125057":"markdown","e7061b34":"markdown","ac9262d1":"markdown","78015e0b":"markdown","e5a5586a":"markdown","c268d815":"markdown","c97a1bf8":"markdown","fc6705f3":"markdown","3982e0a6":"markdown","793eb483":"markdown","4b9430a7":"markdown","2fe22f31":"markdown","33589bd1":"markdown","909536ba":"markdown","bf99dd2b":"markdown","daefee1d":"markdown","00e8e8bf":"markdown"},"source":{"7d726d63":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv\nfrom keras.utils import to_categorical\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import metrics\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom scipy.spatial.distance import cdist\nimport matplotlib.pyplot as plt\nfrom pandas_profiling import ProfileReport\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.preprocessing import LabelBinarizer\nimport datetime\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,LSTM\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\n\nimport xgboost\n\nfrom sklearn.metrics import mean_absolute_error\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","d4ac652b":"data = pd.read_csv(\"\/kaggle\/input\/red-electrica-espaola-consumo-y-precios\/2020-03-26-15-33_chronograf_data.csv\")\nN = int(data.shape[0]\/2)\n","ae5608e8":"data","c0042731":"demandas = data.iloc[:N]\nprecios = data.iloc[N:]\ntimestamps = pd.to_datetime(data._time[:N])\ndf = pd.DataFrame(columns = ['precio','demanda','year','month','day','weekday','weekdayLabel','hour'], index = timestamps)\ndf.year = df.index.year\ndf.month = df.index.month\ndf.day = df.index.day\ndf.weekday = df.index.dayofweek\ndf.hour = df.index.hour\ndf.weekdayLabel = df.weekday.map(lambda num: ['Lunes','Martes','Miercoles','Jueves','Viernes','S\u00e1bado','Domingo'][num])\ndf.precio = precios._value.values\ndf.demanda = demandas._value.values","12cb6fc0":"df","87ae2cc5":"weeks = 3\ndays =7\n\nfor week in range(weeks):\n    fig, axs = plt.subplots(2, days, figsize=(30,5))\n    for day in range(days):\n        axs[0,day].plot(df.hour[:24],df.precio[week*168+day*24:week*168+day*24+24])\n        axs[1,day].plot(df.hour[:24],df.demanda[week*168+day*24:week*168+day*24+24], 'tab:orange')\n\n\n        axs[0,day].set( ylabel='Precio')\n        axs[1,day].set(xlabel=df.weekdayLabel[day*24] , ylabel='Demanda')\n\n# Hide x labels and tick labels for top plots and y ticks for right plots.","ae7dec44":"df","a78d6381":"df[['demanda','precio']]","ab502857":"precio_demanda_dias = df[['demanda','precio']].values.reshape(int(N\/24),48)\n","863a6484":"# k means determine k\ndistortions = []\nK = range(1,10)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k).fit(precio_demanda_dias)\n    kmeanModel.fit(precio_demanda_dias)\n    distortions.append(sum(np.min(cdist(precio_demanda_dias, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) \/ precio_demanda_dias.shape[0])\n\n# Plot the elbow\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k for Prices')\nplt.show()\n","6dabec8e":"K = 3","0cbb68b6":"precio_demanda_dias = df[['demanda','precio']].values.reshape(int(N\/24),48)\nprecio_demanda_ts = df[['demanda','precio']].values.reshape(int(N\/24),24,2)\ndemandas = precio_demanda_ts[:,:,0]\nprecios = precio_demanda_ts[:,:,1]","56954309":"\n\ncolumns = [\"precio\"+str(hour) if hour < 24 else \"demanda\"+str(hour-24)  for hour in range(48)] +['2017','2018','2019']+[str(i) for i in range(12)]+ [str(i) for i in range(31)] + ['Lunes','Martes','Miercoles','Jueves','Viernes','S\u00e1bado','Domingo']+['maxPrice','maxDemanda','minPrice','minDemanda','meanPrice','meanDemanda','weekDayLabel','fridaySaturdaySunday','label',]\n\n\nmaxPrice = precios.max(axis=1).reshape(-1,1)\nmaxDemanda = demandas.max(axis=1).reshape(-1,1)\nminPrecio = precios.min(axis=1).reshape(-1,1)\nminDemanda = demandas.min(axis=1).reshape(-1,1)\nmeanPrecio = precios.mean(axis=1).reshape(-1,1)\nmeanDemanda = demandas.mean(axis=1).reshape(-1,1)\nyear = df.year[::24].values.reshape(-1,1)\nyear = to_categorical(year)[:,-3:]\nmonth = df.month[::24].values.reshape(-1,1)\nmonth = to_categorical(month)[:,-12:]\nday = df.day[::24].values.reshape(-1,1)\nday = to_categorical(day)[:,-31:]\nweekDay = df.weekday[::24].values.reshape(-1,1)\nweekday = to_categorical(weekDay)\nweekDayLabel = df.weekdayLabel[::24].values.reshape(-1,1)\nfridaySaturdaySunday = df.weekday[::24].map( lambda day: 1 if day in [4,5,6] else 0).values.reshape(-1,1)\nclusterer = KMeans(n_clusters=K, random_state=10)\nlabels = clusterer.fit_predict(precio_demanda_dias)\nlabels = np.array([str(label) for label in labels]).reshape([-1,1])\nprecios_demanda_etiquetadas_valores = np.hstack([precio_demanda_dias,year,month,day,weekday,maxPrice,maxDemanda,minPrecio,minDemanda,meanPrecio,meanDemanda,weekDayLabel,fridaySaturdaySunday,labels])\nprecios_demanda_atributos_etiquetadas = pd.DataFrame(precios_demanda_etiquetadas_valores, columns = columns)","0ce142eb":"precios_demanda_atributos_etiquetadas","b5d9b240":"seleccion_de_estudio_automatico = precios_demanda_atributos_etiquetadas.iloc[:,-9:]","32a44b2f":"seleccion_de_estudio_automatico","8b0cc84e":"#profile = ProfileReport(seleccion_de_estudio_automatico, title='Pandas Profiling Report', html={'style':{'full_width':True}})\n#profile.to_widgets()\n#profile.to_file(output_file=\"output.html\")","7fb2b246":"tc = pd.crosstab(index=seleccion_de_estudio_automatico.weekDayLabel,\n            columns=seleccion_de_estudio_automatico.label, margins=True)\ndia_por_etiqueta = tc.values[-1,:-1]\netiqueta_por_dia = tc.values[:-1,-1:]\ntc_percentaje_particion_dia = tc.values[:-1,:-1]*100\/dia_por_etiqueta\ntc_percentaje_dia_particion = tc.values[:-1,:-1]*100\/etiqueta_por_dia\ndistribucion_porcentaje_particion = dia_por_etiqueta*100\/tc.values[-1,-1]\ndistribucion_porcentaje_dia = etiqueta_por_dia*100\/tc.values[-1,-1]\ndistribucion_si_aleatorio = etiqueta_por_dia * dia_por_etiqueta\/ tc.values[-1,-1]","60c145f1":"tc","9809873d":"tc_percentaje_particion_dia","ffe965fa":"tc_percentaje_dia_particion","5c6bb27a":"distribucion_porcentaje_dia","6863d8e2":"distribucion_si_aleatorio","e20308a9":"pd.crosstab(index=seleccion_de_estudio_automatico.fridaySaturdaySunday,\n            columns=seleccion_de_estudio_automatico.label, margins=True)","27b6995c":"pd.crosstab(index=pd.cut(seleccion_de_estudio_automatico.meanPrice, 5\n                        ,labels=['muy bajo','bajo','medio','alto','muy alto']),\n            columns=seleccion_de_estudio_automatico.fridaySaturdaySunday, margins=True)\n","2d054f05":"pd.crosstab(index=pd.cut(seleccion_de_estudio_automatico.meanDemanda, 5\n                        ,labels=['muy bajo','bajo','medio','alto','muy alto']),\n            columns=seleccion_de_estudio_automatico.label, margins=True)","fe33f644":"pd.crosstab(index=pd.cut(seleccion_de_estudio_automatico.meanDemanda, 5\n                        ,labels=['muy bajo','bajo','medio','alto','muy alto']),\n            columns=seleccion_de_estudio_automatico.fridaySaturdaySunday, margins=True)","2c6a4ecd":"pd.crosstab(index=pd.cut(seleccion_de_estudio_automatico.maxPrice, 5,labels=['muy bajo','bajo','medio','alto','muy alto']\n                        ),\n            columns=seleccion_de_estudio_automatico.weekDayLabel, margins=True)","99c80add":"pd.crosstab(index=pd.cut(seleccion_de_estudio_automatico.maxPrice, 5,labels=['muy bajo','bajo','medio','alto','muy alto']\n                        ),\n            columns=seleccion_de_estudio_automatico.fridaySaturdaySunday, margins=True)","95096ee5":"pd.crosstab(index=pd.cut(seleccion_de_estudio_automatico.maxDemanda, 5,labels=['muy bajo','bajo','medio','alto','muy alto']\n                        ),\n            columns=seleccion_de_estudio_automatico.weekDayLabel, margins=True)","316bd8da":"pd.crosstab(index=pd.cut(seleccion_de_estudio_automatico.maxDemanda, 5,labels=['muy bajo','bajo','medio','alto','muy alto']\n                        ),\n            columns=seleccion_de_estudio_automatico.fridaySaturdaySunday, margins=True)","384207e8":"pd.crosstab(index=pd.cut(seleccion_de_estudio_automatico.meanPrice, 5,labels=['muy bajo','bajo','medio','alto','muy alto']\n                        ),\n            columns=pd.cut(seleccion_de_estudio_automatico.meanDemanda, 5,labels=['muy bajo','bajo','medio','alto','muy alto']), margins=True)","af744a98":"#Parametros\nhorizon = 32\nforecast = 24      \nstart_test ='2018-09-01'","4fabdefe":"df","24f2ef61":"\nprecio = df.precio.values.reshape(-1,1)\ndemanda = df.demanda.values.reshape(-1,1)\nprecio_diff = df.precio.pct_change().values.reshape(-1,1)\ndemanda_diff =  df.demanda.pct_change().values.reshape(-1,1)\nhours = to_categorical(df.hour)\ncolumns = ['precio','demanda','precio_diff','demanda_diff']+['hour'+str(i) for i in range(24)]\nX = pd.DataFrame(np.hstack([precio,demanda,precio_diff,demanda_diff,hours]),columns = columns)\nX.index = data._time[:N].map( lambda t: pd.to_datetime(t))\nX = X[1:] #Dropnan\n\n\n#Reescalamos las variables continuas y volvemos a montar el conjunto de entrenamiento.\noriginal_train = X[X.index<pd.to_datetime(start_test).tz_localize('Europe\/Madrid')]\noriginal_test = X[X.index>pd.to_datetime(start_test).tz_localize('Europe\/Madrid')]\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\nscaler = scaler.fit(original_train.iloc[:,:4])\n\n\nX = pd.DataFrame(np.hstack([scaler.transform(X.iloc[:,:4]),hours[1:]]),columns = columns)\nX.index = data._time[1:N].map( lambda t: pd.to_datetime(t))","eea5cda2":"price_index = 0\n\ndef dataset_slidding_window_supervised(X_dataset,horizon,forecast,output):\n    num_features = X_dataset.shape[1]\n    num_samples = X_dataset.shape[0] - horizon - forecast\n    X = np.zeros((num_samples,horizon,X_dataset.shape[1]))    \n    Y = np.zeros((num_samples,forecast))\n    for i in range(num_samples):\n        subset = np.array(X_dataset.iloc[i:i+horizon,:num_features])\n        X[i,:,:] = subset\n        subset = np.array(X_dataset.iloc[i+horizon:i+horizon+forecast,output])\n        Y[i,:] = subset\n    return X,Y\n\nX,Y = dataset_slidding_window_supervised(X,horizon,forecast,price_index)\n\n","bf835c8d":"print(original_test.index[0],original_test.index[30*24+1])","285228ed":"\nX_train = X[:original_train.shape[0]]\nY_train = Y[:original_train.shape[0]]\nX_test = X[original_test.shape[0]:][:30*24+1]  #(Cogemos un mes completo hacia adelante) que corresponde con septiembre.\nY_test = Y[original_test.shape[0]:][:30*24+1]\n","635e9542":"batch_size = 516\n\n# design network\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(X.shape[1], X.shape[2]),return_sequences = False))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(25))\nmodel.add(Dense(Y.shape[1]))\n\nmodel.compile(loss='mae', optimizer='adam')\nearly_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\ncheckpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n# fit network\nhistory = model.fit(X_train, Y_train, epochs=2000, batch_size=batch_size, validation_data=(X_test, Y_test), verbose=2, shuffle=True, callbacks = [early_stopping,checkpoint])\n# plot history\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n","57cb9670":"scaler.min_, scaler.scale_ = scaler.min_[0], scaler.scale_[0]","479696ed":"prediction = model.predict(X_test)\n\nY_rescaled = []\nY_pred_rescaled = []\nfor i in range(30): \n    Y_i = scaler.inverse_transform(Y_test[i*24].reshape(-1,1))\n    y_i_pred = scaler.inverse_transform(prediction[i*24].reshape(-1,1))\n    Y_rescaled.append(Y_i)\n    Y_pred_rescaled.append(y_i_pred)\n    fig, ax = plt.subplots()\n    line1, = ax.plot(Y_i, linewidth=2,\n                     label='Valor real')\n\n\n    line2, = ax.plot(y_i_pred,'--',\n                     label='Prediccion',color='red')\n\n    ax.legend(loc='lower left')\n    plt.title(str(original_test.index[i*24])+' Hasta '+str(original_test.index[i*24+24]))\n    plt.show()\n","6aabb03f":"np.array(Y_rescaled).shape\nnp.array(Y_pred_rescaled).shape","e59e1141":"errors = []\nfor i in range(30):\n    errors.append(mean_absolute_error(Y_rescaled[i],Y_pred_rescaled[i]))\n","d4f2d0e8":"plt.plot(errors)","c367bd46":"np.mean(errors)","97e1598c":"Se puede observar que los d\u00edas cuya demanda m\u00e1xima es elevada es m\u00e1s frecuente en los d\u00edas laborables.","b48870ee":"No hay diferencia de precio medio significativo los Viernes,Sabados y Domingo.","e58f7029":"El error absoluto medio de las predicciones tomadas a las 11 del d\u00eda anterior durante todo septiembre es:","fe428d77":"Vamos a estudiar las propiedades estad\u00edsticas de cada partici\u00f3n para conocer un poco mejor el problema.","3b23a363":"# **Categorizaci\u00f3n de las particiones**","7b1e83e5":"Vamos a ver la forma que tienen los 30 primeros d\u00edas","f944f039":"EL 8.28% de los datos del Lunes pertenecen al cluster 0.","13d19cb3":"Este es nuestro conjunto de datos:\nContiene 52512 muestras, correspondientes a 52512\/2 valores correspondientes al precio y 52512\/2 valores correspondientes a la demanda en tramos horarios apilados verticalmente.","3fabf457":"Esta es la distribuci\u00f3n de frecuencias de los d\u00edas de la semana por partici\u00f3n","0be3e9dc":"Mirando el estad\u00edstico r de Pearson:\n\n* El precio m\u00e1ximo tiene una fuerte correlaci\u00f3n con el precio medio y en menor medida con el precio m\u00ednimo.\n* La demanda m\u00e1xima tiene una fuerte correlaci\u00f3n con la demanda media y en menor medida con la demanda m\u00ednima.\n* Los estad\u00edsticos del precio apenas tienen correlaci\u00f3n con los estad\u00edsticos de la demanda.\n* El precio m\u00e1ximo y la demanda m\u00ednima no tienen apenas correlaci\u00f3n.\n* El precio m\u00e1ximo tiene una fuerte correlaci\u00f3n con el precio medio.\n\nCon la m\u00e1trix de Cram\u00e8r's V haciendo uso del estad\u00edstico ch\u00ed cuadrado\nse podr\u00edan estudiar la asociaci\u00f3n existente entre las particiones y el d\u00eda de la semana al que pertenece, por ejemplo.","d3aa370a":"# Agrupamiento","67016b46":"Ajustamos el escalador anteriormente usado para los valores reales y sus predicciones.","bfd3ca11":"Estos datos es la distribuci\u00f3n de frecuencias esperada si la asignaci\u00f3n de los d\u00edas  y particiones fuera aleatoria.","e19d1699":"Se puede observar mejor descargando este fichero HTML que se encuentra en los datos de salida de este cuaderno interactivo.","7e5f8c6e":"No nos interesan demasiadas particiones, un punto adecuado suele ser en el que se observa una estabilizaci\u00f3n de la distorsi\u00f3n. K=3 parece un buen candidato.","8ee30265":"# **Introducci\u00f3n**\n\nLos datos est\u00e1n extraidos desde la api de REE, han sido almacenados en InfluxDB, una base de datos para series temporales a trav\u00e9s del siguiente script: [https:\/\/github.com\/frapercan\/REE-INFLUXDB](https:\/\/github.com\/frapercan\/REE-INFLUXDB).\n\nHemos seleccionado el intervalo de tiempo con el que vamos a trabajar: 2017,2018,2019, y los datos correspondientes a la demanda real y al Precio Voluntario al Peque\u00f1o Consumidor muestradas por horas.\n\n\n\n\n\n\n","4807302d":"Todo este estudio nos da se\u00f1ales de que vamos a poder obtener una predicci\u00f3n m\u00e1s o menos ajustada, no estamos frente a muestras aleatorias.","e178c46b":"Aplicamos el m\u00e9todo de Elbow para seleccionar un n\u00famero adecuado de grupos **K**","962e0d6c":"* El 2.83 de los datos de la partici\u00f3n 0 son del tipo Domingo (Si la distribuci\u00f3n fuera independiente deber\u00eda estar en el 65%, asimismo el 8% de los datos de los Domingos est\u00e1n en la partici\u00f3n 0 (si la distribuci\u00f3n fuese aleatoria estar\u00eda en el 14%)\n* El 54.5% de los datos de la particion 0 son del tipo Domingo ( Si la distribuci\u00f3n fuese aleatoria deber\u00eda estar en el 37.8% ), asimismo el 91% de los datos de los Domingos est\u00e1n en la particion 2 (si la distribuci\u00f3n fuese aleatoria estar\u00eda en el 14%)\n\nPodemos afirmar que hay una relaci\u00f3n clara entre las particiones y los d\u00edas de la semana.","7d507124":"Hay dependencia lineal entre el precio y la demanda media.","2826a197":"Creamos una funcion que recibe un conjunto de entrenamiento y devuelve los datos como un problema supervisado (X,Y),\nCreando ventanas correderas sin saltos con un horizonte especifico,\nla variable de indice \"output\" que servira como etiqueta con la longitud de la predicci\u00f3n \"forecast\"","38daee80":"El precio parece que se encuentra mayoritariamente comprendido en los precios medios y altos de nuestra discretizaci\u00f3n.\nNo hay diferencias significativas de precio dependiendo del d\u00eda de la semana que sea.","4b22abb1":"# **Exploraci\u00f3n del precio por dias.**","aa125057":"los datos se han agrupado en franjas de la semana concretas, se puede observar como los datos del fin de semana y los viernes siguen una distribuci\u00f3n distinta dentro de cada grupo.","e7061b34":"Vamos a utilizar una LSTM como problema de regresi\u00f3n con series temporales sobre los datos de la demanda y el precio, adem\u00e1s de sus diferencias con respecto a la hora anterior y escalados en un rango de -1 y 1. La hora de la muestra est\u00e1 codificada en 24, ceros y unos violando el teorema central del l\u00edmite.\n\nComenzamos dando forma al dataset, necesitamos un conjunto X muestras de entrenamiento,X_test ser\u00e1n las muestras que utilizaremos para la validaci\u00f3n y Y corresponde con X desplazado un d\u00eda en el tiempo, y Y_test, el conjunto de validaci\u00f3n ser\u00e1 Septiembre de 2019 entero. \n\nNuestro horizonte\/ventana de datos, es decir la longitud de la secuencia de entrada ser\u00e1 de 32 horas, sin embargo la longitud de la predicci\u00f3n ser\u00e1 de 24 horas. ","ac9262d1":"El 2.83% de los datos de la partici\u00f3n 0, corresponde a los Lunes.","78015e0b":"El error de validaci\u00f3n es m\u00e1s peque\u00f1o que el de entrenamiento, esto es debido a que el conjunto de validaci\u00f3n es muy peque\u00f1o y se ha ajustado mejor ","e5a5586a":"Transformamos las muestras en conjuntos de 24 horas","c268d815":"Vamos a generar un informe autom\u00e1tico utilizando todos los datos menos las 24 muestras correspondiente al d\u00eda recogido.","c97a1bf8":"# **Prediccion a 24 horas.**","fc6705f3":"Ajustamos el escalador anteriormente usado para los valores reales y sus predicciones. Quitando el resto de caracteristicas.","3982e0a6":"La demanda disminuye signifcativamente los Viernes sabados y domingo. Probablemente tambien los d\u00edas festivos aunque no est\u00e1n contemplados.","793eb483":"Ahora estudiamos la correspondencia entre el precio medio y los grupos en base al precio.","4b9430a7":"estos datos son los porcentajes de distribuci\u00f3n de cada particion por cada d\u00eda.","2fe22f31":"Creamos un dataframe que contenga la informaci\u00f3n que nos va a hacer falta durante esta fase de nuestro estudio. \n\nVamos a incluir los datos estad\u00edsticos b\u00e1sicos del precio y la demanda, tambien datos sobre la hora del d\u00eda, el d\u00eda de la semana, el mes y el a\u00f1o.","33589bd1":"# Procedemos a visualizar **tablas de contingencia**: ","909536ba":"* Primero de todo analizamos la distribuci\u00f3n de los grupos creados con respecto a los d\u00edas de la semana.","bf99dd2b":"# **Inicializaci\u00f3n del conjunto de datos:**\n\nCargamos el fichero CSV en nuestro contenedor de datos, a\u00f1adimos algunos atributos extraidos de la marca de tiempo como por ejemplo el d\u00eda de la semana.\nBorramos los atributos que no nos vayan a hacer falta.[](http:\/\/)","daefee1d":"Vamos a particionar los datos, para extraer informaci\u00f3n relevante sobre el conjunto de datos. De esa forma podremos hacernos una idea de que car\u00e1cteristicas de \u00e9l nos interesan para realizar nuestra predicci\u00f3n.","00e8e8bf":"\n\nEl precio y la demanda van de la mano, se han creado h\u00e1bitos de consumo en\u00e9rgetico que se repiten de forma periodica y se han establecido leyes para regularlos.\n\nSe observan las horas Valles, y las horas punta que corresponden con las horas de menor y mayor consumo energ\u00e9tico. "}}