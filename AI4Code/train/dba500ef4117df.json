{"cell_type":{"f9eab3da":"code","6ebf50fc":"code","5ebdb706":"code","664b5610":"code","de1da7f6":"code","beb7c19d":"code","278afb0c":"code","4c046cb6":"code","29d94b90":"code","eac1f0a3":"code","1e268ed1":"code","04318fc8":"code","74e2948f":"code","7f1542dc":"code","c2d8bf29":"code","05ed30f0":"code","78151adf":"code","41594709":"code","badd3b77":"code","254633c8":"code","78a00496":"markdown","ad2c2e1f":"markdown","b81e1ace":"markdown","1d810422":"markdown","af9739a9":"markdown","ca287291":"markdown","e2dd448d":"markdown","59d7023a":"markdown","7ec7ffda":"markdown","eb055785":"markdown","68d8d96b":"markdown","f0f6eab2":"markdown","518547a0":"markdown","fce1ce72":"markdown"},"source":{"f9eab3da":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","6ebf50fc":"data = pd.read_csv(\"..\/input\/winequality-red.csv\")\ndata.head()","5ebdb706":"data.info()","664b5610":"data.describe()","de1da7f6":"from sklearn.preprocessing import LabelEncoder\n\nbins = (2, 5, 8)\ngroup_names = ['bad', 'good']\n\ndata['quality'] = pd.cut(data[\"quality\"], bins = bins, labels = group_names)\n\nlabel_quality = LabelEncoder()\n\ndata['quality'] = label_quality.fit_transform(data['quality'].astype(str))\ndata['quality'].value_counts()","beb7c19d":"sns.countplot(data['quality'])\nplt.show()","278afb0c":"# Pearson correlation\nplt.subplots(figsize=(15, 9))\ncor = data.corr()\nsns.heatmap(cor, annot=True, linewidths=.5)\nplt.show()","4c046cb6":"#correlated features with quality\ncor_feat = abs(cor[\"quality\"])\n\n#relevant features\nrel_feat = cor_feat[cor_feat>0.2]\nrel_feat","29d94b90":"# correlation btw relevant features\ndata[[\"volatile acidity\",\"citric acid\"]].corr()","eac1f0a3":"data[[\"volatile acidity\",\"sulphates\"]].corr()","1e268ed1":"data[[\"volatile acidity\",\"alcohol\"]].corr()","04318fc8":"X = data.drop(\"quality\", axis=1)\ny = data[\"quality\"]","74e2948f":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2, random_state=10)","7f1542dc":"#Backward Elimination\nimport statsmodels.api as sm\ncols = list(X_train.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X_train[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y_train,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\n\nselected_features_BE = cols\nselected_features_BE","c2d8bf29":"from sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\n\nmodel = LinearRegression()\n\nrfe = RFE(model, 8)\nX_rfe = rfe.fit_transform(X_train, y_train)\n\nmodel.fit(X_rfe, y_train)\nprint(rfe.support_)\nprint(rfe.ranking_)","05ed30f0":"#no of features\nnof_list=np.arange(1,11)            \nhigh_score=0\n#Variable to store the optimum features\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    model = LinearRegression()\n    rfe = RFE(model,nof_list[n])\n    X_train_rfe = rfe.fit_transform(X_train,y_train)\n    X_test_rfe = rfe.transform(X_test)\n    model.fit(X_train_rfe,y_train)\n    score = model.score(X_test_rfe,y_test)\n    score_list.append(score)\n    if(score>high_score):\n        high_score = score\n        nof = nof_list[n]\n\nprint(\"Optimum number of features: %d\" %nof)\nprint(\"Score with %d features: %f\" % (nof, high_score))","78151adf":"cols = list(X_train.columns)\nmodel = LinearRegression()\n\n#Initializing RFE model\nrfe = RFE(model, 7)             \n\n#Transforming data using RFE\nX_rfe = rfe.fit_transform(X_train,y_train)  \n\n#Fitting the data to model\nmodel.fit(X_rfe,y_train)              \ntemp = pd.Series(rfe.support_,index = cols)\nselected_features_rfe = temp[temp==True].index\nprint(selected_features_rfe)\n","41594709":"from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n\nreg = LassoCV()\nreg.fit(X_train, y_train)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X_test,y_test))\ncoef = pd.Series(reg.coef_, index = X.columns)","badd3b77":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","254633c8":"imp_coef = coef.sort_values()\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")\nplt.show()","78a00496":"* Embedded methods are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration.\n* Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold.","ad2c2e1f":"### Import packages","b81e1ace":"Final relevant features are:\nvolatile acidity, sulphates, alcohol            ","1d810422":"As the optimum number of features is 7. We now feed 10 as number of features to RFE and get the final set of features given by RFE method","af9739a9":"###  Embedded Method:\n","ca287291":"### Import data","e2dd448d":"* The model is built after selecting the features.\n* The filtering here is done using correlation matrix and it is most commonly done using Pearson correlation.","59d7023a":"* A wrapper method needs one machine learning algorithm and uses its performance as evaluation criteria.\n* There are different wrapper methods such as Backward Elimination, Forward Selection, Bidirectional Elimination and RFE.","7ec7ffda":"**For Classification :**\n[Follow this link](https:\/\/www.kaggle.com\/alokevil\/red-wine-classification)","eb055785":"### Feature Selection (wrapper method)","68d8d96b":"We do that by using loop starting with 1 feature and going up to 11. We then take the one for which the accuracy is highest.","f0f6eab2":" ### Feature Selection (filter method)","518547a0":"**Backward Elimination : **\n* we feed all the possible features to the model at first\n* The performance metric used here to evaluate feature performance is pvalue. ","fce1ce72":"**Recursive feature elimination (RFE) : **\n\n* The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. \n* First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. \n* Then, the least important features are pruned from current set of features."}}