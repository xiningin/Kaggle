{"cell_type":{"f462fc11":"code","b5092ea9":"code","a7d1f3fa":"code","83f9dcec":"code","1e8dbaf8":"code","69dd2037":"code","4e4963ab":"code","bdd6ef9f":"code","fd617714":"code","20f970de":"code","cf1da20b":"code","91d30634":"code","8e611e07":"code","c00bdf29":"code","42be9ff3":"code","62c0b64e":"code","af525799":"code","a037642d":"code","c3c0d211":"code","f3415c34":"markdown","16066261":"markdown","7dde5fad":"markdown","a6bdbe53":"markdown","7e5f5d0f":"markdown","40a61684":"markdown","7d39dd94":"markdown","3517e965":"markdown","2c90ce48":"markdown"},"source":{"f462fc11":"import numpy as np \nimport pandas as pd \nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection, feature_extraction, linear_model\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVC\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier","b5092ea9":"# Reading in data\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","a7d1f3fa":"# Viewing number of disastrous and non disastrous tweets\ntrain['target'].value_counts()","83f9dcec":"def lowercase_text(text):\n    text = text.lower()\n    return text\n\ntrain['text'] = train['text'].apply(lambda x: lowercase_text(x))\ntest['text'] = test['text'].apply(lambda x: lowercase_text(x))","1e8dbaf8":"train['text'].head()","69dd2037":"# Removing punctuation, html tags, symbols, numbers, etc.\ndef remove_noise(text):\n    # Dealing with Punctuation\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","4e4963ab":"# Calling remove_noise function in order to remove noise\ntrain['text'] = train['text'].apply(lambda x: remove_noise(x))\ntest['text'] = test['text'].apply(lambda x: remove_noise(x))","bdd6ef9f":"train['text'].head(8)","fd617714":"!pip install nlppreprocess\nfrom nlppreprocess import NLP\n\nnlp = NLP()\n\ntrain['text'] = train['text'].apply(nlp.process)\ntest['text'] = test['text'].apply(nlp.process)  ","20f970de":"train['text'].head(8)","cf1da20b":"stemmer = SnowballStemmer(\"english\")\n\ndef stemming(text):\n    text = [stemmer.stem(word) for word in text.split()]\n    return ' '.join(text)\n\ntrain['text'] = train['text'].apply(stemming)\ntest['text'] = test['text'].apply(stemming)","91d30634":"# Using CountVectorizer to change the teweets to vectors\ncount_vectorizer = CountVectorizer(analyzer='word', binary=True)\ncount_vectorizer.fit(train['text'])\n\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test['text'])","8e611e07":"y = train['target']","c00bdf29":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import  RandomForestClassifier\n# Creating a simple MultinomialNB model\nmodel1 = MultinomialNB(alpha=1)\n\nscores = model_selection.cross_val_score(model, train_vectors, y, cv=3, scoring=\"f1\")\nscores","42be9ff3":"# Training model with train_vectors and target variable\nmodel.fit(train_vectors, y)","62c0b64e":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","af525799":"# Predicting model with the test data that was vectorized (test_vectors)\nsample_submission['target'] = model.predict(test_vectors)","a037642d":"# Viewing submission\nsample_submission.head()","c3c0d211":"# Submission\nsample_submission.to_csv(\"submission.csv\", index=False)","f3415c34":"> ### Bag of Words:\n\nBag of words basically takes every single unique word that appears in the dataset and turns all of them into a feature. Sentences that contain the word are marked with a 1.\n\n\n**For Example:**\n![](https:\/\/miro.medium.com\/max\/2424\/1*yYt2gduO0KpYPX0ewT4BHA.png)","16066261":"### Inspecting Target Feature","7dde5fad":"### Lowercase Text\n\nWe lowercase every word so the model doesn't treat an uppercased version different from the lowercased version\n\n**For example:**\n- EARTH and earth are supposed to be identical\n","a6bdbe53":"### Removing Text Noise\n\nWe create a prebuilt function and call it in order to remove punctuation, html tags, urls, numbers, etc.","7e5f5d0f":"<a id='3'><\/a>\n## Text Preprocessing\n","40a61684":"### Stemming\n\nWe will use a stemmer to stem each word so the word will be transformed to its root word","7d39dd94":"### Text PreProcessing Method:\n- **LOWERCASE:** Make all words **UPPERCASE** or **lowercase**\n- **STOPWORDS:** Remove stopwords - words that commonly come up and don't really have much meaning. For Ex: \"I\"\n- **TOKENIZATION:** Basically splitting strings into numbers  we want\n- **NOISE:** Removing Punctuation, Numbers, etc.\n\n### OPTIONAL:\n- **STEMMING:** Transforming words to their base root by cutting off the end of the word. (e.g. 'Running', 'Runner' = 'Run). Some words may be cut off too much (e.g. 'Spotify' -> 'Spotif') but it doesn't make a big difference\n- **LEMMATIZATION:** Similar to stemming, except instead of cutting off the end of the word it basically counts similar words as one lemma. (e.g. 'Great', 'Good' = 'Good') Note: We don't include this step because we already used stemming so it isn't necessary.\n\nRemember, you do need to use all of these steps. An extensive text preprocessing method may lead to worse results.","3517e965":"### Removing Stopwords\n\nWe will be using the NLP library in order to remove the stopwords","2c90ce48":"For the first sentence \"This is the first document\" notice how there is a 1 under each word that is included in the sentence. This is similar to how hot-encoding works."}}