{"cell_type":{"9cabf60c":"code","ecc68874":"code","e2719f15":"code","586e3d87":"code","84c11526":"code","9189c395":"code","370abb2b":"code","fe4648e6":"code","06672e78":"code","88024311":"code","206cf477":"code","15655712":"code","194aaa38":"code","f6b30f05":"code","b7310db8":"code","b5a39e88":"code","4d90e671":"code","4ff330e5":"code","3bef8fa7":"code","035bb446":"code","1fa28e76":"code","385bffff":"code","ece42b57":"code","0ebf8501":"code","327d345e":"code","de782e4b":"code","3d12e8fc":"code","56c0054a":"code","b524854a":"code","2f2b6250":"code","9c9695f4":"code","d58b38f2":"code","c00cab8c":"code","ed7b6727":"code","c962194a":"code","561ec5b0":"code","9f183238":"code","30fec635":"code","227cdf0a":"code","ab51ae6d":"code","678b9eb7":"markdown","8ebe5e46":"markdown","97aa11b0":"markdown","54b346af":"markdown","30084226":"markdown","1b6c04b4":"markdown","4e2db2f4":"markdown","a5843ff0":"markdown","d643c073":"markdown","5e7bfbfc":"markdown","562df1ea":"markdown","59edba39":"markdown","05f03fb3":"markdown","2535f795":"markdown","92a9afdb":"markdown","a1760ce4":"markdown","768ee189":"markdown","456ab6ff":"markdown","11c761ef":"markdown","ae679453":"markdown","6168d375":"markdown","ea55d146":"markdown","3eca6a8b":"markdown","3ee8150d":"markdown","0dce5b43":"markdown","592aae58":"markdown","c482c53e":"markdown","ac9031b2":"markdown","68d46e1b":"markdown","230be216":"markdown","9f5745c9":"markdown","ce4a8d57":"markdown","911fb867":"markdown","ab34d293":"markdown","396f386b":"markdown","080c5293":"markdown","68608769":"markdown","033775b6":"markdown"},"source":{"9cabf60c":"#Let's import necessary dependencies \nimport pandas as pd\nimport warnings\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline ","ecc68874":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score","e2719f15":"warnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 1000)\n\n#Read data for analysis\ndata=pd.read_csv('..\/input\/covtype.csv')","586e3d87":"print('Data Dimension:')\nprint('Number of Records:', data.shape[0])\nprint('Number of Features:', data.shape[1])","84c11526":"#Names of columns\nprint('Feature Names')\nprint(data.columns)","9189c395":"#A huge list!\nprint(data.info())","370abb2b":"\nplt.figure(figsize=(6,4))\nsns.countplot(y=data.dtypes ,data=data)\nplt.xlabel(\"Data Type Count\")\nplt.ylabel(\"Data types\")\n","fe4648e6":"#Let's check for missing values once again\ndata.isnull().sum()","06672e78":"data.describe()","88024311":"print('Skewness of the below features:')\nprint(data.skew())","206cf477":"skew=data.skew()\nskew_df=pd.DataFrame(skew,index=None,columns=['Skewness'])\nplt.figure(figsize=(15,7))\nsns.barplot(x=skew_df.index,y='Skewness',data=skew_df)\nplt.xticks(rotation=90)","15655712":"class_dist=data.groupby('Cover_Type').size()\nclass_label=pd.DataFrame(class_dist,columns=['Size'])\nplt.figure(figsize=(8,6))\nsns.barplot(x=class_label.index,y='Size',data=class_label)","194aaa38":"for i,number in enumerate(class_dist):\n    percent=(number\/class_dist.sum())*100\n    print('Cover_Type',class_dist.index[i])\n    print('%.2f'% percent,'%')","f6b30f05":"data.head()","b7310db8":"cont_data=data.loc[:,'Elevation':'Horizontal_Distance_To_Fire_Points']\n\nbinary_data=data.loc[:,'Wilderness_Area1':'Soil_Type40']\n\nWilderness_data=data.loc[:,'Wilderness_Area1': 'Wilderness_Area4']\n\nSoil_data=data.loc[:,'Soil_Type1':'Soil_Type40']","b5a39e88":"#Iterate via columns of data having only binary features\nfor col in binary_data:\n    count=binary_data[col].value_counts()\n    print(col,count)","4d90e671":"print('Soil Type',' Occurence_count')\nfor col in binary_data:\n    count=binary_data[col].value_counts()[1] #considering all one's among 1 and 0's in each soil type\n    if count < 1000:\n        print(col,count)","4ff330e5":"# data_num = data.select_dtypes([np.int, np.float]) #If you need to select only numeric features. \n#Here we already have all numeric Data.\n\nfor i, col in enumerate(cont_data.columns):\n    plt.figure(i)\n    sns.distplot(cont_data[col])","3bef8fa7":"# %%time\ndata['Cover_Type']=data['Cover_Type'].astype('category') #To convert target class into category\n\nfor i, col in enumerate(cont_data.columns):\n    plt.figure(i,figsize=(8,4))\n    sns.boxplot(x=data['Cover_Type'], y=col, data=data, palette=\"coolwarm\")","035bb446":"%%time\nfor i, col in enumerate(binary_data.columns):\n    plt.figure(i,figsize=(6,4))\n    sns.countplot(x=col, hue=data['Cover_Type'] ,data=data, palette=\"rainbow\")","1fa28e76":"%%time\n#If someone can help me with function to reverse one hot coding, please let me know in comment. I know this is not the robust way.\ndef rev_code(row):\n    for c in Soil_data.columns:\n        if row[c]==1:\n            return c  \n\ndata['Soil_Type']=Soil_data.apply(rev_code, axis=1) #Time consuming","385bffff":"%%time\ndef rev_code(row):\n    for c in Wilderness_data.columns:\n        if row[c]==1:\n            return c  \n\ndata['Wilderness_Type']=Wilderness_data.apply(rev_code, axis=1) #Time consuming","ece42b57":"%%time\nplt.figure(figsize=(16,8))\nsns.countplot(x='Wilderness_Type', hue='Cover_Type',data=data, palette=\"rainbow\")\nplt.xticks(rotation=90)","0ebf8501":"%%time\nplt.figure(figsize=(16,8))\nsns.countplot(x='Soil_Type', hue='Cover_Type',data=data, palette=\"rainbow\")\nplt.xticks(rotation=90)","327d345e":"soil_counts = []\nfor num in range(1,41):\n    col = ('Soil_Type' + str(num))\n    this_soil = data[col].groupby(data['Cover_Type'])\n    totals = []\n    for value in this_soil.sum():\n        totals.append(value)\n    total_sum = sum(totals)\n    soil_counts.append(total_sum)\n    print(\"Total Trees in Soil Type {0}: {1}\".format(num, total_sum))\n    percentages = [ (total*100 \/ total_sum) for total in totals]\n    print(\"{0}\\n\".format(percentages))\nprint(\"Number of trees in each soil type:\\n{0}\".format(soil_counts))","de782e4b":"plt.figure(figsize=(15,8))\nsns.heatmap(cont_data.corr(),cmap='magma',linecolor='white',linewidths=1,annot=True)","3d12e8fc":"g = sns.PairGrid(cont_data)\ng.map(plt.scatter)","56c0054a":"# %%time\n# g = sns.PairGrid(cont_data)\n# g.map_diag(plt.hist)\n# g.map_upper(sns.kdeplot)\n# g.map_lower(sns.kdeplot)","b524854a":"X=data.loc[:,'Elevation':'Soil_Type40']\ny=data['Cover_Type']","2f2b6250":"#Features to be removed before the model\nrem=['Hillshade_3pm','Soil_Type7','Soil_Type8','Soil_Type14','Soil_Type15',\n     'Soil_Type21','Soil_Type25','Soil_Type28','Soil_Type36','Soil_Type37']","9c9695f4":"#Remove the unwanted features\nX.drop(rem, axis=1, inplace=True)","d58b38f2":"#Splitting the data into  train and test\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=101)","c00cab8c":"%%time\n#Setup arrays to store training and test accuracies\nneighbors = np.arange(1,7)\ntrain_accuracy =np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    #Fit the model\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n    \n    #Compute accuracy on the test set\n    test_accuracy[i] = knn.score(X_test, y_test) ","ed7b6727":"#Generate plot\nplt.figure(figsize=(10,6))\nplt.title('k-NN Varying number of neighbors')\nplt.plot(neighbors, test_accuracy, label='Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label='Training accuracy')\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\n#plt.show()","c962194a":"#Setup a knn classifier with k neighbors\nknn = KNeighborsClassifier(n_neighbors=5) #Using Eucledian distance","561ec5b0":"#Fit the model\nknn.fit(X_train,y_train)","9f183238":"#Get accuracy. Note: In case of classification algorithms score method represents accuracy.\nAccuracy=knn.score(X_test,y_test)\nprint('KNN Accuracy:',Accuracy)","30fec635":"import scipy.stats as ss\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import zscore\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns","227cdf0a":"%%time\nMLA = []\nZ = [LinearSVC() , DecisionTreeClassifier() , LogisticRegression() , GaussianNB() ,RandomForestClassifier() , \n     GradientBoostingClassifier()]\nX = [\"LinearSVC\" , \"DecisionTreeClassifier\" , \"LogisticRegression\" , \"GaussianNB\" ,\"RandomForestClassifier\" , \n     \"GradientBoostingClassifier\"]\n\nfor i in range(0,len(Z)):\n    model = Z[i]\n    model.fit( X_train , y_train )\n    pred = model.predict(X_test)\n    MLA.append(accuracy_score(pred , y_test))","ab51ae6d":"d = { \"Algorithm\" : X, \"Accuracy\" : MLA }\n\ndfm = pd.DataFrame(d)\ndfm","678b9eb7":"**Looks like we got many binary independent features. Good!**\n**Now let us understand the data type of each features**","8ebe5e46":"**Skewness**\n>The skewness for a normal distribution is zero, and any symmetric data should have a skewness near zero. \n>Negative values for the skewness indicate data that are skewed left and positive values for the skewness indicate data that are skewed right. \n>By skewed left, it means that the left tail is long relative to the right tail. Similarly, skewed right means that the right tail is long relative to the left tail.","97aa11b0":"> Yup! It's done. Looks like we have a desired single Soil_Type and Wilderness_Type feature. Let's now use count plot against our Target Cover_Type","54b346af":"> But I'm interested in percentwise distribution of each class. Let's check","30084226":"**Not bad. KNN works great here. Lazy learner is doing a good work at differentiating a CoverType. **","1b6c04b4":"**#Inferences:**\n1.  Cover_Type 1 and 2 i.e **Spruce\/Fir** and **Lodgepole Pine** seems to dominate the area. \n2.  Also the Cover_Type 4 i.e **Cottonwood\/Willow** is minimal compare to the rest","4e2db2f4":"\n**I want to see the number of  values counts within each features, mainly for the Binary types**","a5843ff0":" **Explore Data Dimension and count of values without any sneak peek in Data**","d643c073":"*  Plots looks cool right? What's Even more cool you know?\n*  The insights. Let's figure out very general insights\n*  There are couple of features which shows not much of variance with respect to classes\n*  And features such as 'Elevation', 'slope' and 'horizontal distance to road_ways does a good job","5e7bfbfc":"**#Inferences:**\n> **This tells me lots of valuable insights. Mostly regarding the soil types. Wanna know? Ok, let me not hide it from you**.\n* It's Just that, there are some of the Soil types which consists of very few counts.  \n* Statistically speaking, for half a million records, balance number per soil type (total 40 in number) is 581012\/40 = 14.5k\n* Whereas, here we see a different figure. I know that data need not be balanced all the times. But may be we can get rid of really small size features. Isn't it?\n* Let me list down those along with there size. I'm displaying the Soil type having less than 1000 occurence size","562df1ea":"**Did we check the co-relation??**\n * > No we didn't. This is something that I usually check first. No, problem. it's never too late.\n * > Let's better vizualise it via heatmap. All in one!","59edba39":"* > This gives us the relation and its shape with respect to other features. Various inferences can be drwan out.\n* > Pairgrid plot is just awesome. And it's even more awesome when it's combined with KDE clusters. \n* > But for considerably heavy data, its time consuming. Be aware before running the below plot.","05f03fb3":"* >Couple of features are have a good amount of co-relation. Guess which one? I'll tell you.\n* >  Hillshade_9am ~ Hillshade_3pm and Aspect ~ Hillshade_3pm","2535f795":"> Neighbor value = 5 yeilds the best result. Let's go by that for now. ","92a9afdb":"* ** I have tried various Classification algorithms out of which KNN served the best.**\n* ** Algorithms such as RandomForest and DecisionTree are doing a decent job here. So please explore.**","a1760ce4":"> Let's visualize the change in accuracies with respect to train and test data at different neighbors ","768ee189":"**Data Modelling**","456ab6ff":"> I'll do the same for Wilderness Area","11c761ef":">Nice! Now, Let's convert the whole data into few Mini datasets. I'll make use of it in plots\n* cont_data - Data without binary features i.e continuous features\n* binary_Data - Data having all binary features [Wilderness Areas + Soil Types]\n* wilderness_Data - Binary Wilderness Areas\n* Soil_Data - Binary Soil Types","ae679453":"*  **Try to surpass these accuracies. **\n*  **My objective was to 'Get to know' the Forest Cover Type Dataset for which I tried to articulate it step by step.**\n*  **If you liked it, please let me know with a upvote, It serves a Motivation. Good Luck and Thank you for spending your time here! :)**","6168d375":"> Let's do something similar for our binary features. This time we'll use countplot.","ea55d146":"* > So the plot does justice to the distribution which each class but I want to have a single feature having Soil_Type corresponding to each row. \n* > Let's see if I can do it.  This will help me to visualize it better, instead of counting 0's and 1's in each one hot coded Soil types.","3eca6a8b":"* I know this will make more sense in a visual such as bar graph right? I'm excited to see it too. But let's infer more from the numbers as of now. \n* We'll do plottings once we start with Bivariate and Multivariate analysis. \n* We'll see if we need to really drop the above soil types. \n* We can only confirm on it if it is not aligned (give any relation) to our target variable i.e Cover_Type. So, please wait, do not conclude. Climax is yet to come :D","3ee8150d":"**Data_Dictionary**\n\n1. Elevation = Elevation in meters.\n2. Aspect = Aspect in degrees azimuth.\n3. Slope = Slope in degrees.\n4. Horizontal_Distance_To_Hydrology = Horizontal distance to nearest surface water features.\n5. Vertical_Distance_To_Hydrology = Vertical distance to nearest surface water features.\n6. Horizontal_Distance_To_Roadways = Horizontal distance to nearest roadway.\n7. Hillshade_9am = Hill shade index at 9am, summer solstice. Value out of 255.\n8. Hillshade_Noon = Hill shade index at noon, summer solstice. Value out of 255.\n9. Hillshade_3pm = Hill shade index at 3pm, summer solstice. Value out of 255.\n10. Horizontal_Distance_To_Fire_Point = sHorizontal distance to nearest wildfire ignition points.\n11. Wilderness_Area1 = Rawah Wilderness Area\n12. Wilderness_Area2 = Neota Wilderness Area\n13. Wilderness_Area3 = Comanche Peak Wilderness Area\n14. Wilderness_Area4 = Cache la Poudre Wilderness Area\n\n**Soil_Type1 to Soil_Type40 [Total 40 Types]**\n\n**Cover_TypeForest Cover Type designation. Integer value between 1 and 7, with the following key:**\n1. Spruce\/Fir\n2.  Lodgepole Pine\n3.  Ponderosa Pine\n4.  Cottonwood\/Willow\n5.  Aspen\n6.  Douglas-fir\n7.  Krummholz","0dce5b43":"1. **So we have complete Numeric Data, Even Better!!**\n2. **Also there doesn't seem to be any missing value. Good work at Data Collection**","592aae58":"**How about the class balance? We'll see**","c482c53e":"**Let's get started with plots based EDA (Exploratory Data Analysis) **\n*  Fun begins here, am I right?\n* Data Distribution of features via Histograms. Although I love box plots more than histograms, we'll use boxplot to check distribution with respect to categorical variable. In our case that is Cover_Type, having 7 different category of classes.","ac9031b2":"**#Inferences:**\n> Some of the Variables are heavily skewed hence need to be corrected or transformed  on a later stage. ","68d46e1b":"* > Here, First i want to check the shape of continous features with respect to the target class. Hence I'll use the continuous_data (cont_data) and plot a boxplot against target. \n* > You can also look at violinplot here, It's visually appealing. ","230be216":"**I'll put the accuracies obtained by various other classification techniques. Try to enhance it more via Cross Validation may be.**\n**Let me know in comment if you manage to raise your accuracies. **\n> **I'm gonna do it too. Those are my next steps, such as CV, more insights, Feature engineering etc. I'll roll out updates part by part**","9f5745c9":"* >Above two plots tells  us the count of trees in each class considering Wilderness and Soil Type.\n* >Soil_Type plot is not very clear since it's  too vast. So let's go by the number. We'll see how many and what type of Cover_Type we have under each soil Type","ce4a8d57":">Let us take a step to remove the features with low Std deviation as demonstrated earlier. \n>Also I'll remove one of the co-related variable","911fb867":"**We forgot to check the Data distribution for each feature. Spend some good time here. Lot's of inferences I believe**","ab34d293":"* > There's lot of scope for Data Viz. as far this dataset is concerned. My objective was a surface walkthrough the dataset. I would roll out new versions on this part by part. \n* >Let's now wind it up by Data Modelling. Another Excitement, right?","396f386b":"* > The above plots more or less tells us about the skewness that we saw earlier. Let's dig down into Bivariate and Multivariate Analysis\n* > Let's check for distribution with respect to our target. This is where magic happens!","080c5293":"**Oh common let us check the data atleast, enough with size and dimension**","68608769":"**#Inferences:**\n1. Few of the features looks skewed, we'll see those later.\n2. No missing Values (We say this for the third time :p)\n3. Wilderness Area and Soil Type are one hot coded.\n4. Scales are different over the whole data, hence might need to scale for some required algorithms.","033775b6":"* X = Input or independent variables\n* y=  Target variable ('Cover_Type')"}}