{"cell_type":{"2bee4cc2":"code","c6565aa5":"code","21c694d4":"code","12151df4":"code","ce71fe02":"code","1c3ff5a6":"code","b3ce0a2a":"code","9208d6fc":"code","d8a2c837":"code","91bba5eb":"code","4773fdb2":"code","784902ff":"code","cecb2901":"code","d086f927":"code","738c7c30":"code","8e883918":"code","0c56e653":"code","a28e8396":"code","873d2bdf":"code","32d7188b":"code","589869a2":"code","ab6f7f76":"code","3b581206":"code","830d556b":"code","c187724b":"code","2e8bb905":"code","77f8389c":"code","7fd03088":"code","81190a85":"code","0a6e99cb":"code","d4b6c2fa":"code","439bd7b5":"code","f47c4882":"code","99d1ccc5":"code","97feebe9":"code","44d522e8":"code","ee80cfa0":"code","3c729bcb":"code","d386d572":"code","a8856b91":"code","b17bd579":"code","b1a38d0e":"code","882ee10b":"code","b80b56d0":"code","e5210df2":"code","e6133e42":"code","cfc4019f":"code","e3ac3580":"code","7c32e4c4":"code","3101276b":"code","f365f6d2":"code","6edfb6e1":"code","6f3d9ca0":"code","f37f7752":"code","b26e1f79":"code","ab207d7a":"code","d680d846":"code","fcb19334":"code","68302b35":"code","262d2ae2":"code","0583ec55":"code","bc01b8a6":"code","70a30191":"code","8e3c7bbd":"code","38cc8b9d":"markdown","adf03d7f":"markdown","09567b23":"markdown","52390a44":"markdown","151c4718":"markdown","96bec8fb":"markdown","d73dbe38":"markdown","5d9b64ef":"markdown","a44177f9":"markdown","3575d6a7":"markdown","00918cb6":"markdown","b102117f":"markdown","0e3c7f11":"markdown","e85fb0b8":"markdown","014952cf":"markdown","84fa06fc":"markdown","be6e30f1":"markdown","567758cc":"markdown","eb533c47":"markdown","ff092d7e":"markdown","bc412372":"markdown","fce17453":"markdown","e44d782e":"markdown","5d34304e":"markdown","47fcb6d2":"markdown","2f5bc49b":"markdown","d1610897":"markdown","6acc8d52":"markdown","2a74f71e":"markdown","e36ed1b1":"markdown","e92ca266":"markdown","0f115eae":"markdown","2e3cb98e":"markdown","7bb857f2":"markdown","1b081163":"markdown","be044ba3":"markdown","ee33472a":"markdown","930ba96f":"markdown","07862b4e":"markdown","f3d89ced":"markdown","4eb2a84f":"markdown","a2d06c6b":"markdown","f11acc2b":"markdown","ae1eff77":"markdown","0515c19b":"markdown","f46661e6":"markdown","9dd5d550":"markdown","74252ef0":"markdown","2500dbdd":"markdown","9211b4fb":"markdown","21bf315d":"markdown","93764bac":"markdown","ea0b4e84":"markdown","b5061e99":"markdown","92b80a72":"markdown","1da20d98":"markdown","dc4f5301":"markdown"},"source":{"2bee4cc2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6565aa5":"import pandas as pd \nimport numpy as np\n\nimport seaborn as sns\nsns.set_style(\"dark\")\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom time import time\n\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","21c694d4":"df_train = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_features.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/lish-moa\/test_features.csv\")\ntrain_scored = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_targets_scored.csv\")\ntrain_non_scored = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv\")","12151df4":"features = df_train\n","ce71fe02":"features.info()","1c3ff5a6":"#quality check\n#1.  check the datsets\ndf_train.head()\n","b3ce0a2a":"# test\ndf_test.head()","9208d6fc":"# scored\ntrain_scored.head()","d8a2c837":"# non scored\ntrain_non_scored.head()","91bba5eb":"# check For missing values \ndf_train.isnull().sum().sum()","4773fdb2":"# check For missing values \ndf_test.isnull().sum().sum()","784902ff":"# check for target sparsity\nscored = train_scored.drop(columns = [\"sig_id\"] , axis = 1)\n# non zero target varaibles\nprint((scored.to_numpy()).sum()\/(scored.shape[0]*scored.shape[1])*100 , \"%\")","cecb2901":"non_scored = train_non_scored.drop(columns = [\"sig_id\"] , axis = 1)\n# non zero target_nonscored varaibles\nprint((non_scored.to_numpy()).sum()\/(non_scored.shape[0]*non_scored.shape[1])*100 , \"%\")","d086f927":"# list the columns \n# list(features)\n# get all the gene features and cell features\ncommon  = ['sig_id',\n 'cp_type',\n 'cp_time',\n 'cp_dose']\n\n\ngenes = list(filter(lambda x : \"g-\" in x  , list(features)))\n\ncells = list(filter(lambda x : \"c-\" in x  , list(features)))\n\n\n","738c7c30":"plt.figure(figsize=(6,6))\nax = sns.countplot(features[\"cp_type\"] , palette=\"Set2\")\nax.set_title(\"Treatment Type\")\n\n\nplt.show()","8e883918":"plt.figure(figsize=(6,6))\nax = sns.countplot(features[\"cp_dose\"] , palette=\"Set2\")\nax.set_title(\"Treatment Dose\")\n\n\nplt.show()","0c56e653":"plt.figure(figsize=(6,6))\nax = sns.countplot(features[\"cp_time\"] , palette=\"Set2\")\nax.set_title(\"Treatment time\")\n\n\nplt.show()","a28e8396":"fig, axs = plt.subplots(ncols=2 , nrows = 2 , figsize=(9, 9))\nsns.distplot(features['g-0'] ,color=\"b\", kde_kws={\"shade\": True}, ax=axs[0][0] )\nsns.distplot(features['g-1'] ,color=\"r\", kde_kws={\"shade\": True}, ax=axs[0][1] )\nsns.distplot(features['g-2'], color=\"g\", kde_kws={\"shade\": True}, ax=axs[1][0] )\nsns.distplot(features['g-3'] ,color=\"y\", kde_kws={\"shade\": True}, ax=axs[1][1] )\nplt.show()","873d2bdf":"# some stats plot for genes\nfig, axs = plt.subplots(ncols=2 , nrows = 2 , figsize=(13,13))\nsns.distplot(features[genes].max(axis =1) ,color=\"b\",hist=False, kde_kws={\"shade\": True}, ax=axs[0][0] ).set(title = 'max')\nsns.distplot(features[genes].min(axis =1) ,color=\"r\",hist=False, kde_kws={\"shade\": True}, ax=axs[0][1] ).set(title = 'min')\nsns.distplot(features[genes].mean(axis =1), color=\"g\",hist=False, kde_kws={\"shade\": True}, ax=axs[1][0] ).set(title = 'mean')\nsns.distplot(features[genes].std(axis =1) ,color=\"y\",hist=False, kde_kws={\"shade\": True}, ax=axs[1][1] ).set(title = 'sd')\nplt.show()","32d7188b":"fig, axs = plt.subplots(ncols=2 , nrows = 2 , figsize=(9, 9))\nsns.distplot(features['c-0'] ,color=\"b\", kde_kws={\"shade\": True}, ax=axs[0][0] )\nsns.distplot(features['c-1'] ,color=\"r\", kde_kws={\"shade\": True}, ax=axs[0][1] )\nsns.distplot(features['c-2'], color=\"g\", kde_kws={\"shade\": True}, ax=axs[1][0] )\nsns.distplot(features['c-3'] ,color=\"y\", kde_kws={\"shade\": True}, ax=axs[1][1] )\nplt.show()","589869a2":"fig, axs = plt.subplots(ncols=2 , nrows = 2 , figsize=(13,13))\nsns.distplot(features[cells].max(axis =1) ,color=\"b\",hist=False, kde_kws={\"shade\": True}, ax=axs[0][0] ).set(title = 'max')\nsns.distplot(features[cells].min(axis =1) ,color=\"r\",hist=False, kde_kws={\"shade\": True}, ax=axs[0][1] ).set(title = 'min')\nsns.distplot(features[cells].mean(axis =1), color=\"g\",hist=False, kde_kws={\"shade\": True}, ax=axs[1][0] ).set(title = 'mean')\nsns.distplot(features[cells].std(axis =1) ,color=\"y\",hist=False, kde_kws={\"shade\": True}, ax=axs[1][1] ).set(title = 'sd')\nplt.show()","ab6f7f76":"target  = train_scored.drop(['sig_id'] , axis =1)\n\nfig, ax = plt.subplots(figsize=(9,9))\nax = sns.countplot(target.sum(axis =1), palette=\"Set2\")\ntotal = float(len(target))\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.4f}%'.format((height\/total)*100),\n            ha=\"center\") \n\nplt.show()","3b581206":"## counts per target class- \nsns.kdeplot(target.sum() , shade = True , color = \"b\")","830d556b":"top_targets = pd.Series(target.sum()).sort_values(ascending=False)[:5]\nbottom_targets = pd.Series(target.sum()).sort_values()[:5]\nfig, axs = plt.subplots(figsize=(9,9) , nrows=2)\nsns.barplot(top_targets.values , top_targets.index , ax = axs[0] ).set(title = \"Top five targets\")\nsns.barplot(bottom_targets.values , bottom_targets.index, ax = axs[1] ).set(title = \"bottom five targets\")\nplt.show()","c187724b":"cols = pd.DataFrame({'value': [1 for i in list(target) ]} , index = [i.split('_')[-1] for i in list(target)] )\ncols_top_5 = cols.groupby(level=0).sum().sort_values(by = 'value' , ascending = False)[:5]","2e8bb905":"fig, ax = plt.subplots(figsize=(9,9))\n\nsns.barplot(x = cols_top_5.value , y = cols_top_5.index , palette=\"Set2\" , orient='h')\n\n\nfor p in ax.patches:\n    width = p.get_width()\n    plt.text(8+p.get_width(), p.get_y()+0.55*p.get_height(),\n             '{:1.4f}%'.format((width \/206 )*100), # total 206 columns\n             ha='center', va='center')\n\nplt.show()","77f8389c":"print(\"Top five suffixes constitue for about \", list(cols_top_5.sum()\/cols.sum().values)[0]*100 , \"%\")","7fd03088":"\ng  = sns.FacetGrid(features, col=\"cp_type\" )\ng.map(sns.countplot , 'cp_time'  )\nplt.show()\n\n# sns.countplot(x = features['cp_time']  )\n","81190a85":"g  = sns.FacetGrid(features, col=\"cp_type\" )\ng.map(sns.countplot , 'cp_dose'  )\nplt.show()","0a6e99cb":"g  = sns.FacetGrid(features, col=\"cp_dose\" )\ng.map(sns.countplot , 'cp_time'  )\nplt.show()","d4b6c2fa":"# g_mean and  c_mean and g_mean for analysis.\nfeatures['c_mean'] = features[cells].mean(axis =1)\nfeatures['g_mean'] = features[genes].mean(axis =1)\n\n","439bd7b5":"fig, axs = plt.subplots(figsize=(16,16) , nrows=2 , ncols =3)\nplt.subplot(231)\nfor i in features.cp_type.unique():\n    sns.distplot(features[features['cp_type']==i]['g_mean'],label=i, hist=False, kde_kws={\"shade\": True})\nplt.title(f\"g_mean based on cp_type\")\nplt.legend()\n\nplt.subplot(232)\nfor i in features.cp_time.unique():\n    sns.distplot(features[features['cp_time']==i]['g_mean'],label=i, hist=False, kde_kws={\"shade\": True})\nplt.title(f\"g_mean based on cp_time\")\nplt.legend()\n\nplt.subplot(233)\nfor i in features.cp_dose.unique():\n    sns.distplot(features[features['cp_dose']==i]['g_mean'],label=i, hist=False, kde_kws={\"shade\": True})\nplt.title(f\"g_mean based on cp_dose\")\nplt.legend()\n\nplt.subplot(234)\nsns.boxplot( x = features['cp_type'] , y = features['g_mean'] )\nplt.title(f\"g_mean based on cp_type\")\nplt.legend()\n\nplt.subplot(235)\nsns.boxplot( x = features['cp_time'] , y = features['g_mean'] )\nplt.title(f\"g_mean based on cp_time\")\nplt.legend()\n\nplt.subplot(236)\nsns.boxplot( x = features['cp_dose'] , y = features['g_mean'] )\nplt.title(f\"g_mean based on cp_dose\")\nplt.legend()\n\nplt.show()\n\n","f47c4882":"fig, axs = plt.subplots(figsize=(16,16) , nrows=2 , ncols =3)\nplt.subplot(231)\nfor i in features.cp_type.unique():\n    sns.distplot(features[features['cp_type']==i]['c_mean'],label=i, hist=False, kde_kws={\"shade\": True})\nplt.title(f\"c_mean based on cp_type\")\nplt.legend()\n\nplt.subplot(232)\nfor i in features.cp_time.unique():\n    sns.distplot(features[features['cp_time']==i]['c_mean'],label=i, hist=False, kde_kws={\"shade\": True})\nplt.title(f\"c_mean based on cp_time\")\nplt.legend()\n\nplt.subplot(233)\nfor i in features.cp_dose.unique():\n    sns.distplot(features[features['cp_dose']==i]['c_mean'],label=i, hist=False, kde_kws={\"shade\": True})\nplt.title(f\"c_mean based on cp_dose\")\nplt.legend()\n\nplt.subplot(234)\nsns.boxplot( x = features['cp_type'] , y = features['c_mean'] )\nplt.title(f\"c_mean based on cp_type\")\nplt.legend()\n\nplt.subplot(235)\nsns.boxplot( x = features['cp_time'] , y = features['c_mean'] )\nplt.title(f\"c_mean based on cp_time\")\nplt.legend()\n\nplt.subplot(236)\nsns.boxplot( x = features['cp_dose'] , y = features['c_mean'] )\nplt.title(f\"c_mean based on cp_dose\")\nplt.legend()\n\nplt.show()\n\n","99d1ccc5":"feat_target  = pd.merge(features , train_scored , how = \"inner\" , on = ['sig_id','sig_id'])\ntarget_cols = list(target)\nfeat_target[\"target_sum\"] = feat_target[target_cols].sum(axis =1)\nfeat_target.drop(\"sig_id\" , axis = 1, inplace = True)\n","97feebe9":"fig,ax = plt.subplots(figsize=(16,9))\nplt.subplot(131)\nsns.countplot(x = 'target_sum' , hue= 'cp_type', data = feat_target)\nplt.subplot(132)\nsns.countplot(x = 'target_sum' , hue= 'cp_time', data = feat_target)\nplt.subplot(133)\nsns.countplot(x = 'target_sum' , hue= 'cp_dose', data = feat_target)\n\nplt.show()","44d522e8":"fig,ax = plt.subplots(figsize=(16,9))\nplt.subplot(121)\nsns.barplot(x = 'target_sum' , y= 'c_mean', data = feat_target)\nplt.subplot(122)\nsns.barplot(x = 'target_sum' , y= 'g_mean', data = feat_target)\n\nplt.show()","ee80cfa0":"corr = features[genes[:99]].corr() # taking only first 99 genes other wise its a mess\nf, ax = plt.subplots(figsize=(45, 45))\n# Add diverging colormap from red to blue\ncmap = sns.diverging_palette(250, 10, as_cmap=True)\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# plot the heatmap\nsns.heatmap(corr,  mask = mask,\n        xticklabels=corr.columns,\n        yticklabels=corr.columns , cmap=cmap)\nplt.show()","3c729bcb":"corr = features[cells].corr()\nf, ax = plt.subplots(figsize=(45, 45))\n# Add diverging colormap from red to blue\ncmap = sns.diverging_palette(250, 10, as_cmap=True)\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# plot the heatmap\nsns.heatmap(corr,  mask = mask,\n        xticklabels=corr.columns,\n        yticklabels=corr.columns , cmap=cmap)\nplt.show()","d386d572":"corr = target.corr()\nf, ax = plt.subplots(figsize=(45, 45))\n# Add diverging colormap from red to blue\ncmap = sns.diverging_palette(250, 10, as_cmap=True)\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# plot the heatmap\nsns.heatmap(corr,  mask = mask,\n        xticklabels=corr.columns,\n        yticklabels=corr.columns , cmap=cmap)\nplt.show()","a8856b91":"kot = corr[corr>=.5]\nplt.figure(figsize=(12,8))\nsns.heatmap(kot, cmap=\"Reds\" )\nplt.show()","b17bd579":"# pca analysis for genes \n# a bit of data cleaning \n# get a train test consolidated DF\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\ntrain = df_train.drop(['c_mean', 'g_mean'] , axis=1)\ntrain['type'] = 'train'\ntest = df_test\ntest['type'] = 'test'\nX = train.append(test)\n\n# lets label encode cp_type , cp_dose and cp_time\n# X = pd.get_dummies(columns = ['cp_type' , 'cp_dose', 'cp_time'], drop_first =True , data = X)\nnumeric_cols = genes+cells\nX[numeric_cols] = StandardScaler().fit_transform(X[numeric_cols])","b1a38d0e":"pca_genes = PCA(n_components=5)\npca_gene_data = pca_genes.fit_transform(X[genes])\nprincipal_genes = pd.DataFrame(data = pca_gene_data\n             , columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'])","882ee10b":"print('Explained variation per principal component: {}'.format(pca_genes.explained_variance_ratio_))","b80b56d0":"fig,ax = plt.subplots(figsize=(9, 9))\nsns.barplot(x =['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5'], y = pca_genes.explained_variance_ratio_*100  )\nsns.lineplot(x =['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5'], y = pca_genes.explained_variance_ratio_*100, color =\"r\")\nplt.show()","e5210df2":"pca_gene = PCA(n_components=2)\npca_gene_data = pca_gene.fit_transform(X[genes])\ninter_pc_gene = pd.DataFrame(data = pca_gene_data\n             , columns = ['PC1', 'PC2'])\nX['PC1_gene'] = inter_pc_gene['PC1']\nX['PC2_gene'] = inter_pc_gene['PC2']","e6133e42":"fig, ax = plt.subplots(figsize=(9,16))\nplt.subplot(311)\nsns.scatterplot(\n    x=\"PC1_gene\", y=\"PC2_gene\",\n    hue=\"cp_type\",\n    style = \"cp_type\",\n    data=X,\n    legend=\"full\",\n)\nplt.subplot(312)\nsns.scatterplot(\n    x=\"PC1_gene\", y=\"PC2_gene\",\n    hue=\"cp_time\",\n    style = \"cp_time\",\n    data=X,\n    legend=\"full\",\n)\nplt.subplot(313)\nsns.scatterplot(\n    x=\"PC1_gene\", y=\"PC2_gene\",\n    hue=\"cp_dose\",\n    style = \"cp_dose\",\n    data=X,\n    legend=\"full\",\n)\nplt.show()","cfc4019f":"pca_cell = PCA(n_components=5)\npca_cell_data = pca_cell.fit_transform(X[cells])\nprincipal_cell = pd.DataFrame(data = pca_cell_data\n             , columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'])","e3ac3580":"principal_cell.head()","7c32e4c4":"print('Explained variation per principal component: {}'.format(pca_cell.explained_variance_ratio_))","3101276b":"fig,ax = plt.subplots(figsize=(9, 9))\nsns.barplot(x =['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5'], y = pca_cell.explained_variance_ratio_*100  )\nsns.lineplot(x =['PCA1', 'PCA2', 'PCA3', 'PCA4', 'PCA5'], y = pca_cell.explained_variance_ratio_*100, color =\"r\")\nplt.show()","f365f6d2":"pca_cell = PCA(n_components=2)\npca_cell_data = pca_cell.fit_transform(X[cells])\ninter_pc_cell = pd.DataFrame(data = pca_cell_data\n             , columns = ['PC1', 'PC2'])\nX['PC1_cell'] = inter_pc_cell['PC1']\nX['PC2_cell'] = inter_pc_cell['PC2']","6edfb6e1":"fig, ax = plt.subplots(figsize=(9,16))\nplt.subplot(311)\nsns.scatterplot(\n    x=\"PC1_cell\", y=\"PC2_cell\",\n    hue=\"cp_type\",\n    style = \"cp_type\",\n    data=X,\n    legend=\"full\",\n)\nplt.subplot(312)\nsns.scatterplot(\n    x=\"PC1_cell\", y=\"PC2_cell\",\n    hue=\"cp_time\",\n    style = \"cp_time\",\n    data=X,\n    legend=\"full\",\n)\nplt.subplot(313)\nsns.scatterplot(\n    x=\"PC1_cell\", y=\"PC2_cell\",\n    hue=\"cp_dose\",\n    style = \"cp_dose\",\n    data=X,\n    legend=\"full\",\n)\nplt.show()","6f3d9ca0":"X = pd.get_dummies(columns = ['cp_type' , 'cp_dose', 'cp_time'], drop_first =True , data = X) # dummification is important here\n# features_final = transformed_genes + ['PC1_cell', 'PC2_cell', 'cp_type_trt_cp', 'cp_dose_D2', 'cp_time_48', 'cp_time_72']\nfeatures_final = ['PC1_gene', 'PC2_gene','PC1_cell', 'PC2_cell', 'cp_type_trt_cp', 'cp_dose_D2', 'cp_time_48', 'cp_time_72']","f37f7752":"X_train = X[X['type']  == 'train'][features_final]\nY_train = target\nX_test = X[X['type']  == 'test'][features_final]","b26e1f79":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom category_encoders import CountEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import log_loss\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.multioutput import MultiOutputClassifier\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","ab207d7a":"x = X_train.to_numpy()\ny = Y_train.to_numpy()\nx_test = X_test.to_numpy()","d680d846":"classifier = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist'))\n# classifier = MultiOutputClassifier(XGBClassifier())\n\nclf = Pipeline([\n                ('classify', classifier)\n               ])\n","fcb19334":"\n\n\nparams = {'classify__estimator__colsample_bytree': 0.6522,\n          'classify__estimator__gamma': 3.6975,\n          'classify__estimator__learning_rate': 0.0503,\n          'classify__estimator__max_delta_step': 2.0706,\n          'classify__estimator__max_depth': 10,\n          'classify__estimator__min_child_weight': 31.5800,\n          'classify__estimator__n_estimators': 166,\n          'classify__estimator__subsample': 0.8639\n         }\n\n_ = clf.set_params(**params)","68302b35":"SEED = 42\nNFOLDS = 10 #increase folds\nDATA_DIR = '\/kaggle\/input\/lish-moa\/'\nnp.random.seed(SEED)","262d2ae2":"oof_preds = np.zeros(y.shape)\ntest_preds = np.zeros((test.shape[0], y.shape[1]))\noof_losses = []\nkf = KFold(n_splits=NFOLDS)\nfor fn, (trn_idx, val_idx) in enumerate(kf.split(x, y)):\n    print('Starting fold: ', fn)\n    X_train, X_val = x[trn_idx], x[val_idx]\n    y_train, y_val = y[trn_idx], y[val_idx]\n    \n    # drop where cp_type==ctl_vehicle (baseline)\n    ctl_mask = X_train[:,-4]==0\n    X_train = X_train[~ctl_mask,:]\n    y_train = y_train[~ctl_mask]\n    \n    clf.fit(X_train, y_train)\n    val_preds = clf.predict_proba(X_val) # list of preds per class\n    val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n    oof_preds[val_idx] = val_preds\n    \n    loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n    oof_losses.append(loss)\n    preds = clf.predict_proba(x_test)\n    preds = np.array(preds)[:,:,1].T # take the positive class\n    test_preds += preds \/ NFOLDS\n    \nprint(oof_losses)\nprint('Mean OOF loss across folds', np.mean(oof_losses))\nprint('STD OOF loss across folds', np.std(oof_losses))","0583ec55":"# set control train preds to 0\ncontrol_mask = X[X['type'] =='train']['cp_type_trt_cp'] ==0\noof_preds[control_mask] = 0\n\nprint('OOF log loss: ', log_loss(np.ravel(y), np.ravel(oof_preds)))","bc01b8a6":"# set control test preds to 0\ncontrol_mask = X[X['type'] =='test']['cp_type_trt_cp'] == 0\ntest_preds[control_mask] = 0","70a30191":"# create the submission file\nsub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\nsub.iloc[:,1:] = test_preds\nsub.to_csv('submission.csv', index=False)","8e3c7bbd":"sub.head()","38cc8b9d":"# Bivariate analysis","adf03d7f":"we can see that D1(low dose) is administed more for 48 hr group the rest looks preety simmilar","09567b23":"> plotting Highest(>=0.5) correlation matrix for target","52390a44":"way bigger improvement than the prevoius 0.25","151c4718":"Its mostly red but some cells have blues throught the row in cells","96bec8fb":"* Dim1(PCA1) explains about 22% of varience and the rest are way less about approx 5%\n* we can restrict the PCA to 2 Dimensions as the rest are same any way and PCA 1 has clear domination.","d73dbe38":"***Maybe these suffixes contain some hidden meaning which could help us treat them in diffrent ways to get better results. There definately a pattern and maybe domain knowledge could help us exploit the above finding***","5d9b64ef":"# About MoA:\n> ***In pharmacology, the term mechanism of action (MOA) refers to the specific biochemical interaction through which a drug substance produces its pharmacological effect. A mechanism of action usually includes mention of the specific molecular targets to which the drug binds, such as an enzyme or receptor.***\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/ec\/Mechanism_of_action_for_beta_blockers.png)","a44177f9":" let\u2019s look at the densities for the first 4 gene features as an example:\n\n","3575d6a7":"***PCA for cell columns***","00918cb6":"# Improved  baseline XGBOOST ","b102117f":"# PCA","0e3c7f11":"1. The means are pretty nicely distributed around zero \n2. standard deviations chiefly between 0.5 and 1.5.\n3. The min and max are a nice mirror image of each other. \n4. There are notable increases around the range of positive\/negative 9 - 10(min and max respectively)","e85fb0b8":"This is a multi-label classification problem. Drugs can have multiple MoA annotations which describe binary responses from different cell types in different ways. The evaluation metric is the mean columnwise log loss.\n\nThe data comes in the shape of train and test files. In contrast to other competitions, here we have two separate files for the training predictors (train_features.csv) and the targets (train_targets_scored.csv). Each row corresponds to a specific treatment. In addition, we are also given an optional set of MoA targets (train_targets_nonscored.csv) that we don\u2019t need to predict, but can use for context analysis.\n\n","014952cf":"## **Taget variable**\n","84fa06fc":"# Gene expression feautures","be6e30f1":"1. About 39% have no Moa annotations. it was expexcted to be a sparse data and since 40% are completly empty this verifies our assumptions.\n2. For more than 1 MoA annotation, we see a tail that extends down to 7 simultaneous MoAs (for 0.03% of cases)\n3. majority of the cases have single annotation about 52% cases","567758cc":"Modeling lets apply what we have learned from the EDA using xgboost classifier and sklearn)","eb533c47":"# **Lets inspect the features dataframe.**","ff092d7e":"# Multivariate analysis and Heatmaps","bc412372":"Stats plots for cell features","fce17453":"ctrl_vehicle is as expected way less than  trt_cp as expected","e44d782e":"Max seems concentrated on the other hand min is mostly concentracted but for some points negative tails (-10) seems to rise. there is a bit of bump in mean arount -10 as well.","5d34304e":"**Interesting!!! , For cells the negative tails are of-course in effect but for genes only 7 positive has a negative g_mean which is worth instigating further!!!**","47fcb6d2":"# Cell feautures","2f5bc49b":"***cell vs common vars analysis***","d1610897":"Given the notable amount of correlations in the gene and, especially, the cell features, gives us hope to reduce the dimensions of data.Hence PCA could come in handy here.\n\n","6acc8d52":"let\u2019s look at the densities for the first 4 gene features as an example:","2a74f71e":"***Target vs g_mean and  c_mean***","e36ed1b1":"1. There are two cases with only one anotation \n2. The top target seems to be around 800 (nfkb-inhibitor) also terms like inhibitor antagonist seem to reapeat with diffrent suffixes.we can dig into that and try to find patterns between inhibitors etc...","e92ca266":"***Take the input***","0f115eae":"## **lets dig into target variables.**","2e3cb98e":"***gene features vs common features (cp_type , cp_dose, cp_duration )***","7bb857f2":"Insights","1b081163":"> Missing values train","be044ba3":"Baseline XG boost can increase performance by optimizing it","ee33472a":"All the targets are binary columns, indicating whether a certain cell type responds to the drug, or not. Some target classes also measure the type of response; e.g. there is an adrenergic_receptor_agonist class and an adrenergic_receptor_antagonist class. Those should probably not be active for the same sample row.\n\nOur challenge is a multi-label classification problem, and as such the rows (i.e. drug samples) can have multiple MoA\u2019s (i.e. more than one target class can be active). Let\u2019s first look at the distribution of how many target classes can be active at once.\n\n","930ba96f":"Most of the observations are under 200 \n","07862b4e":"1. as expected ctrl_vehicle doesnt have any target annotation\n2. most of the obs have one target annotations.\n","f3d89ced":"# Lets check some visualizations","4eb2a84f":"Comparing common variables(cp_type , cp_dose and cp_time)","a2d06c6b":"**Univariate plots**","f11acc2b":"****Target vs commom vars****","ae1eff77":"get required columns in lists for future refrence","0515c19b":"approx 86% varience is explained by PC1 rest are as low as 1% a clear dominance of PC1","f46661e6":"A big difference. Almost an order of magnitude more sparse than the already sparse scoring targets.","9dd5d550":"The vast majority of treatments are compund treatments (\u201ctrt_cp\u201d), Controls have no MoAs.","74252ef0":"Less than half a percent non-zero.A very imbalanced data . check for non scored.\n","2500dbdd":"# MODELING ","9211b4fb":"lets see the distribution of targets based on thier prefixes seprated  by \"_\"","21bf315d":"Target correlation heatmap","93764bac":"distributions looks normal. which is good we won't need a transformation . There is still some skew but nothing major.","ea0b4e84":"These also seem normal.  there are spikes in -10 area maybe they are potential outliers? <br>\nhas a longer negative tail in comparison to genes data.","b5061e99":"***Gene vs common vars analysis***","92b80a72":"PCA for gene columns","1da20d98":"treatment dose has 2 values D1 and D2 (high vs low). <br>\ntreatment time is distributed under 24,48,72 ","dc4f5301":"as seen in univariate analysis for cells and genes cells have negative tail and genes have a positive tail. \n\nThe distribution on the basis of common vars are simmilar with slight diffrence in negative(extreme) values for cells and positive (extreme) values for genes"}}