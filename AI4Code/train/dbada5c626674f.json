{"cell_type":{"0650298f":"code","3667370c":"code","aea0c357":"code","3724e0f8":"code","fbc357c3":"code","064df2cf":"code","5afe6750":"code","53fd2ebe":"code","7ebe3566":"code","30825f02":"code","899413dc":"code","30232a67":"code","22e138a0":"code","0190dc0b":"code","381e97b9":"code","6c41a83e":"code","01d3dd19":"code","e523b6da":"code","3296cf54":"code","b9dc8f51":"code","226a9a43":"code","c7c08100":"markdown","6c922d50":"markdown","307a8e28":"markdown","0937f57c":"markdown","380c364c":"markdown","1a42bb70":"markdown","905e4ed6":"markdown","b4612576":"markdown","43212967":"markdown","128d7c97":"markdown","b78dac23":"markdown","2c195135":"markdown","02b96b13":"markdown","ae62e74b":"markdown"},"source":{"0650298f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport math\n%matplotlib inline","3667370c":"df2014 = pd.read_csv('..\/input\/la-liga-match-data\/laliga2014.csv', encoding= 'unicode_escape')\ndf2015 = pd.read_csv('..\/input\/la-liga-match-data\/laliga2015.csv', encoding= 'unicode_escape')\ndf2016 = pd.read_csv('..\/input\/la-liga-match-data\/laliga2016.csv', encoding= 'unicode_escape')\ndf2017 = pd.read_csv('..\/input\/la-liga-match-data\/laliga2017.csv', encoding= 'unicode_escape')\ndf2018 = pd.read_csv('..\/input\/la-liga-match-data\/laliga2018.csv', encoding= 'unicode_escape')\ndf2019 = pd.read_csv('..\/input\/la-liga-match-data\/laliga2019.csv', encoding= 'unicode_escape')\ndf2020 = pd.read_csv('..\/input\/la-liga-match-data\/laliga2020.csv', encoding= 'unicode_escape')","aea0c357":"# adding goals data for both teams\n\ndef feature(df):\n    h_sc=[]\n    a_sc = []\n    h_co = []\n    a_co = []\n    for i in range(0, len(df), 1):\n        score = df['Score'][i]\n        score = score.split('-')\n        h_sc.append(score[0])\n        a_co.append(score[0])\n        a_sc.append(score[1])\n        h_co.append(score[1])\n    df['Home Team Goals Scored'] = h_sc\n    df['Away Team Goals Scored'] = a_sc\n    df['Home Team Goals Conceeded'] = h_co\n    df['Away Team Goals Conceeded'] = a_co\n    return df","3724e0f8":"df2014 = feature(df2014)\ndf2014['year'] = 2014\ndf2015 = feature(df2015)\ndf2015['year'] = 2015\ndf2016 = feature(df2016)\ndf2016['year'] = 2016\ndf2017 = feature(df2017)\ndf2017['year'] = 2017\ndf2018 = feature(df2018)\ndf2018['year'] = 2018\ndf2019 = feature(df2019)\ndf2019['year'] = 2019\ndf2020 = feature(df2020)\ndf2020['year'] = 2020","fbc357c3":"final_data = pd.concat([df2014,df2015,df2016,df2017, df2018, df2019, df2020])\nfinal_data.reset_index(inplace=True)\nfinal_data.drop('index', axis=1, inplace=True)\ndf1 = final_data","064df2cf":"home_points  = []\naway_points = []\nfor i in range(0, len(final_data['Away Team'])):\n    if(final_data['Home Team Goals Scored'][i]>final_data['Away Team Goals Scored'][i]):\n        home_points.append(3)\n        away_points.append(0)\n    elif(final_data['Home Team Goals Scored'][i]<final_data['Away Team Goals Scored'][i]):\n        away_points.append(3)\n        home_points.append(0)\n    else:\n        away_points.append(1)\n        home_points.append(1)","5afe6750":"final_data['Home Points'] = home_points\nfinal_data['Away Points'] = away_points","53fd2ebe":"X = final_data[['Match Excitement', 'Home Team Rating', 'Away Team Rating',\n       'Home Team Possession %', 'Away Team Possession %',\n       'Home Team Off Target Shots', 'Home Team On Target Shots',\n       'Home Team Total Shots', 'Home Team Blocked Shots', 'Home Team Corners',\n       'Home Team Throw Ins', 'Home Team Pass Success %',\n       'Home Team Aerials Won', 'Home Team Clearances', 'Home Team Fouls',\n       'Home Team Yellow Cards', 'Home Team Second Yellow Cards',\n       'Home Team Red Cards', 'Away Team Off Target Shots',\n       'Away Team On Target Shots', 'Away Team Total Shots',\n       'Away Team Blocked Shots', 'Away Team Corners', 'Away Team Throw Ins',\n       'Away Team Pass Success %', 'Away Team Aerials Won',\n       'Away Team Clearances', 'Away Team Fouls', 'Away Team Yellow Cards',\n       'Away Team Second Yellow Cards', 'Away Team Red Cards', 'year']]\ny = final_data['Home Points']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)","7ebe3566":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\naccuracies = []\n\n# Code for finding the best value of n_neighbors\n# for i in range(0,1000,1):\n#     if(i!=0):\n#         model = KNeighborsClassifier(n_neighbors=i) # 103 is perfect\n#         model.fit(X_train, y_train)\n#         y_pred = model.predict(X_test)\n#     from sklearn import metrics\n# #     print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n#     if(i == 0):\n#         accuracies.append(0)\n#     else:\n#         accuracies.append(metrics.accuracy_score(y_test, y_pred))\n        \nmodel = KNeighborsClassifier(n_neighbors=103) # 103 is perfect\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint('Accuracy : ',metrics.accuracy_score(y_test, y_pred))\nprint('Confusion Matrix : ')\nprint(confusion_matrix(y_test, y_pred))","30825f02":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(max_depth=6)\nmodel.fit(X_train,y_train)\npred = model.predict(X_test)\nprint('Accuracy : ',metrics.accuracy_score(y_test, pred))\nprint('Confusion Matrix : ')\nprint(confusion_matrix(y_test, pred))","899413dc":"print('Training Accuracy : ',model.score(X_train, y_train))","30232a67":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 250, criterion = 'entropy', random_state = 42)\nclassifier.fit(X_train, y_train)\npredictions = classifier.predict(X_test)\nprint(f'Accuracy : {metrics.accuracy_score(y_test, predictions):.3f}')\nprint(confusion_matrix(y_test, predictions))","22e138a0":"print('Training Accuracy : ',classifier.score(X_train, y_train))","0190dc0b":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators' : [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n    'criterion' : ['entropy'],\n    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n#     'max_features': ['auto', 'sqrt'],\n#     'min_samples_leaf': [1, 2, 4],\n#     'min_samples_split': [2, 5, 10]   \n}\n\n\ngrid = GridSearchCV(RandomForestClassifier(), param_grid, refit = True, verbose = 1,n_jobs=-1)\ngrid.fit(X_train, y_train) \n\nprint(grid.best_params_) \ngrid_predictions = grid.predict(X_test)\n\nprint(f'Accuracy : {metrics.accuracy_score(y_test, grid_predictions):.3f}')","381e97b9":"grid.best_params_","6c41a83e":"from sklearn.ensemble import GradientBoostingClassifier\ngb=GradientBoostingClassifier(n_estimators=250,learning_rate=0.1)\ngb.fit(X_train,y_train)\npred = gb.predict(X_test)\nprint(f'Accuracy : {metrics.accuracy_score(y_test, pred):.3f}')","01d3dd19":"print('Training Accuracy : ',gb.score(X_train, y_train))","e523b6da":"import xgboost as xb\n!ignore warnings\n\nmodel = xb.XGBClassifier()\nmodel.fit(X_train,y_train)\npred = model.predict(X_test)\n\nprint(f'Accuracy : {metrics.accuracy_score(y_test, pred):.3f}')","3296cf54":"print('Training Accuracy : ',model.score(X_train, y_train))","b9dc8f51":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators' : range(1,1000,100),\n    'learning_rate' : [ 0.1,0.01, 0.001,0.0001]\n}\n\ngrid = GridSearchCV(GradientBoostingClassifier(), param_grid, refit = True, verbose = 3,n_jobs=-1)\ngrid.fit(X_train, y_train) \n\nprint(grid.best_params_) \ngrid_predictions = grid.predict(X_test)\n\nprint(f'Accuracy : {metrics.accuracy_score(y_test, grid_predictions):.3f}')","226a9a43":"grid.best_params_","c7c08100":"# KNN Classification","6c922d50":"## Conclusion\n\nBest Classifier for this particular data is Gradient Boosting Classifier.\n\nThe three methods are similar, with a significant amount of overlap. \nIn a nutshell:\n\n* A decision tree is a simple, decision making-diagram.\n* Random forests are a large number of trees, combined (using averages or \"majority rules\") at the end of the process.\n* Gradient boosting machines also combine decision trees, but start the combining process at the beginning, instead of at the end","307a8e28":"# Preparing Data","0937f57c":"# Data loading","380c364c":"# Import Statements","1a42bb70":"# Decision Tree","905e4ed6":"# Match Winning Stats","b4612576":"## Random Forest Vs Decision Tree\n\nRandom Forest is suitable for situations when we have a large dataset, and interpretability is not a major concern.\nDecision trees are much easier to interpret and understand. Since a random forest combines multiple decision trees, it becomes more difficult to interpret. ","43212967":"# Grid CV","128d7c97":"# Feature Engineering","b78dac23":"# XG Boost","2c195135":"# Random Forest Classifier","02b96b13":"# Grid Search CV","ae62e74b":"# Gradient Boosting Classifier"}}