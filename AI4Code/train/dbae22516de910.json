{"cell_type":{"2cac7ae6":"code","1ff022ec":"code","20262da8":"code","679e3de5":"code","df5d6bc3":"code","20132074":"code","02b3192a":"code","9dce1284":"code","8981aed2":"code","7e78c463":"code","1541ac90":"code","45e72391":"code","7e27b2df":"code","04dd2153":"code","7bb5ec39":"code","070a351c":"code","81f5e4f6":"code","bbb43175":"code","bf9b52cc":"code","9b070e20":"code","584eb14d":"code","d75f880a":"code","7165d113":"code","6b660959":"code","f170c811":"code","8e8e28ed":"code","eb8aa195":"code","ce34efe6":"code","d5bf47e7":"code","34bfa848":"code","668ccb31":"code","84716c43":"code","13e9b5c1":"code","5ccd62d3":"code","993ac8a7":"code","02a203cd":"code","4b67e7b0":"code","c96932ac":"code","f68eb77e":"code","162971e0":"code","6a46d6ba":"code","f54ffbcc":"code","6769446d":"code","4d8f18e4":"code","462a3077":"code","f28a87c7":"code","f49a101e":"code","d86dcdb7":"code","e4bbaa1e":"code","c173d7d8":"code","b8de156b":"code","7a2dc893":"code","13f55f01":"code","3b579d7f":"code","c7bcb4aa":"code","8bcbaee1":"code","4b11fdfb":"code","30d6accd":"code","892b5eac":"code","3d6fcd9f":"code","662ae0d0":"code","c7912f87":"code","ff5db713":"code","bece26a6":"code","9cf03fd1":"code","a999634a":"code","5707c5b2":"code","99f5cf45":"code","e864e553":"code","e09e348b":"code","e275ecaa":"code","3ead277f":"code","d33b8e07":"code","596d1ea9":"code","f814d4c7":"code","fbf964e3":"code","0b208800":"code","e80ce5a3":"code","9b349ef6":"markdown","0adff441":"markdown","7b593425":"markdown","b18ee6cb":"markdown","e7618a41":"markdown","7d08d357":"markdown","b0714965":"markdown","b8e991aa":"markdown","4be2fd97":"markdown"},"source":{"2cac7ae6":"!pip freeze > requirements.txt.txt","1ff022ec":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgb\nimport catboost\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\n\nfrom tqdm import tqdm\nfrom itertools import product\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\n\nimport sys\nimport os\nimport gc\nfrom glob import glob\nimport pickle\nimport json\nimport subprocess\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, RepeatedStratifiedKFold\n","20262da8":"DF_CEILING_VALUE = 20000.0\nSHOP_CEILING_VALUE = 999999.9\nITEM_CEILING_VALUE = 999999.9","679e3de5":"PATH = '..\/input\/competitive-data-science-predict-future-sales\/'\n\ndf = pd.read_csv(PATH + 'sales_train.csv')\ndf_test = pd.read_csv(PATH + 'test.csv')\nsample = pd.read_csv(PATH + 'sample_submission.csv')\nitems = pd.read_csv(PATH + 'items.csv')\nshops = pd.read_csv(PATH + 'shops.csv')\nitem_cats = pd.read_csv(PATH + 'item_categories.csv')","df5d6bc3":"data_files_names = [\"df\",\"df_test\",\"sample\",\"items\",\"shops\",\"item_cats\"]\ndata_files = [df,df_test,sample,items,shops,item_cats]","20132074":"df_test = df_test.drop('ID', axis=1)","02b3192a":"df","9dce1284":"df.loc[df['item_cnt_day'] < 0.0, 'item_cnt_day'] = 0.0","8981aed2":"df[df['item_cnt_day'] > 100.0]","7e78c463":"df.loc[df['item_cnt_day'] > 100.0, 'item_cnt_day'] = 1.0\nlen(df[df['item_cnt_day']>10.0])","1541ac90":"df[df['item_cnt_day'] > 10.0]","45e72391":"df.loc[df['shop_id'] == 57, 'shop_id'] = 0\ndf.loc[df['shop_id'] == 58, 'shop_id'] = 1\ndf.loc[df['shop_id'] == 11, 'shop_id'] = 10\ndf.loc[df['shop_id'] == 40, 'shop_id'] = 39\n\ndf_test.loc[df_test['shop_id'] == 57, 'shop_id'] = 0\ndf_test.loc[df_test['shop_id'] == 58, 'shop_id'] = 1\ndf_test.loc[df_test['shop_id'] == 11, 'shop_id'] = 10\ndf_test.loc[df_test['shop_id'] == 40, 'shop_id'] = 39","7e27b2df":"df['shop_item_id'] = df['shop_id'].astype('str').str.zfill(2) +  df['item_id'].astype('str').str.zfill(5)\ndf['item_category_id'] = pd.merge(df, items, on='item_id',how='left')['item_category_id']\ndf","04dd2153":"df = df.groupby(['date_block_num', 'shop_id','item_id','shop_item_id', 'item_category_id'], as_index=False\n        ).agg({'item_cnt_day':'sum'}\n        ).rename(columns={'item_cnt_day':'mon_shop_item_cnt'})\ndf","7bb5ec39":"# Add shop_item_id to df_test\ndf_test['shop_item_id'] = df_test['shop_id'].astype('str').str.zfill(2) + df_test['item_id'].astype('str').str.zfill(5)\ndf_test['item_category_id'] = pd.merge(df_test, items, on='item_id',how='left')['item_category_id']\ndf_test","070a351c":"count = 0\ndf_ids = df['shop_item_id'].unique()\nrepeat_count = 0\nfor one_id in df_test['shop_item_id'].sample(1000):\n    if one_id in df_ids:\n        count += 1\n    repeat_count += 1\n\n    if repeat_count > 1000:\n        break\nprint(count \/ repeat_count)","81f5e4f6":"plt.figure(figsize=(12,6))\nplt.hist(df['mon_shop_item_cnt'])","bbb43175":"df.loc[df['mon_shop_item_cnt'] > 100.0, 'mon_shop_item_cnt'] = 101.0","bf9b52cc":"print(len(df.loc[df['mon_shop_item_cnt'] > 100.0]), len(df.loc[df['mon_shop_item_cnt'] > 101.0]))","9b070e20":"len(np.unique(np.concatenate([df['shop_item_id'], df_test['shop_item_id']])))","584eb14d":"# Outer -> 520915\ntransition = pd.DataFrame(np.unique(np.concatenate([df['shop_item_id'], df_test['shop_item_id']])), columns=['shop_item_id'])\nfor i in range(34):\n    transition = pd.merge(transition, df[df['date_block_num']==i].drop(['date_block_num', 'shop_id', 'item_id', 'item_category_id'], axis=1).rename(columns={'mon_shop_item_cnt': i}), on='shop_item_id', how='left')\ntransition = transition.fillna(0)\ntransition","d75f880a":"plt.figure(figsize=(12,6))\nplt.bar(transition.loc[:, 0:].columns,transition.loc[:, 0:].sum())","7165d113":"plt.figure(figsize=(12,6))\nplt.hist(transition.loc[:, 0:].T.sum())","6b660959":"# I think these data is too big but not invalid\ntransition[transition.loc[:, 0:].T.sum() >= 500.0]","f170c811":"# transition_mean = transition.loc[:, 0:].mean().mean()\n# transition_std = transition.loc[:, 0:].std().std()\ntransition_max = transition.loc[:, 0:].max().max()\ntransition_max","8e8e28ed":"std_transition = transition.copy()\nstd_transition.loc[:, 0:] = (std_transition.loc[:, 0:]) \/ transition_max\nstd_transition","eb8aa195":"shops.loc[shops['shop_id'] == 57, 'shop_id'] = 0\nshops.loc[shops['shop_id'] == 58, 'shop_id'] = 1\nshops.loc[shops['shop_id'] == 11, 'shop_id'] = 10\nshops.loc[shops['shop_id'] == 40, 'shop_id'] = 39\nshops","ce34efe6":"shop_df = df.groupby(['date_block_num', 'shop_id'], as_index=False\n        ).agg({'mon_shop_item_cnt':'sum'}\n        )\nshop_transition = pd.DataFrame(shops['shop_id'].unique(), columns=['shop_id'])\n\nfor i in range(34):\n    shop_transition = pd.merge(shop_transition, shop_df[shop_df['date_block_num']==i].drop('date_block_num', axis=1).rename(columns={'mon_shop_item_cnt': i}), on='shop_id', how='left')\nshop_transition = shop_transition.fillna(0)\n\nshop_transition_max = shop_transition.loc[:, 0:].max().max()\nshop_transition.loc[:, 0:] = (shop_transition.loc[:, 0:]) \/ shop_transition_max\n\nshop_transition","d5bf47e7":"shop_feature = transition.loc[:, ['shop_item_id']].copy()\nshop_feature['shop_id'] = shop_feature['shop_item_id'].str[:2].astype(int)\nshop_feature","34bfa848":"shop_feature = pd.merge(shop_feature, shop_transition, on='shop_id', how='left')\nshop_feature = shop_feature.drop('shop_id', axis=1)\nshop_feature","668ccb31":"items","84716c43":"item_df = df.groupby(['date_block_num', 'item_id'], as_index=False\n        ).agg({'mon_shop_item_cnt':'sum'})\nitem_transition = pd.DataFrame(items['item_id'].unique(), columns=['item_id'])\n\nfor i in range(34):\n    item_transition = pd.merge(item_transition, item_df[item_df['date_block_num']==i].drop('date_block_num', axis=1).rename(columns={'mon_shop_item_cnt': i}), on='item_id', how='left')\nitem_transition = item_transition.fillna(0)\n\nitem_transition_max = item_transition.loc[:, 0:].max().max()\nitem_transition.loc[:, 0:] = (item_transition.loc[:, 0:]) \/ item_transition_max\n\nitem_transition","13e9b5c1":"item_feature = transition.loc[:, ['shop_item_id']].copy()\nitem_feature['item_id'] = item_feature['shop_item_id'].str[2:].astype(int)\nitem_feature = pd.merge(item_feature, item_transition, on='item_id', how='left')\nitem_feature = item_feature.drop('item_id', axis=1)\nitem_feature","5ccd62d3":"cats_df = df.groupby(['date_block_num', 'item_category_id'], as_index=False\n        ).agg({'mon_shop_item_cnt':'sum'})\ncats_transition = pd.DataFrame(cats_df['item_category_id'].unique(), columns=['item_category_id'])\nfor i in range(34):\n    cats_transition = pd.merge(cats_transition, cats_df[cats_df['date_block_num']==i].drop('date_block_num', axis=1).rename(columns={'mon_shop_item_cnt': i}), on='item_category_id', how='left')\ncats_transition = cats_transition.fillna(0)\n\ncats_transition_max = cats_transition.loc[:, 0:].max().max()\ncats_transition.loc[:, 0:] = (cats_transition.loc[:, 0:]) \/ cats_transition_max\n\ncats_transition","993ac8a7":"cats_feature = transition.loc[:, ['shop_item_id']].copy()\ncats_feature['item_id'] = cats_feature['shop_item_id'].str[2:].astype(int)\ncats_feature['item_category_id'] = pd.merge(cats_feature, items, on='item_id',how='left')['item_category_id']\ncats_feature = pd.merge(cats_feature, cats_transition, on='item_category_id', how='left')\ncats_feature = cats_feature.drop(['item_id','item_category_id'], axis=1)\ncats_feature","02a203cd":"print(shop_feature.loc[:, 0:].mean().mean(), item_feature.loc[:, 0:].mean().mean())","4b67e7b0":"shop_feature.loc[:, 0:] += np.random.normal(0, shop_feature.loc[:, 0:].mean().mean() * 0.025, shop_feature.loc[:, 0:].shape)\nitem_feature.loc[:, 0:] += np.random.normal(0, item_feature.loc[:, 0:].mean().mean() * 0.025, item_feature.loc[:, 0:].shape)\ncats_feature.loc[:, 0:] += np.random.normal(0, cats_feature.loc[:, 0:].mean().mean() * 0.025, cats_feature.loc[:, 0:].shape)","c96932ac":"shop_feature","f68eb77e":"features = [std_transition, shop_feature, item_feature, cats_feature]","162971e0":"del shop_feature, item_feature, cats_feature\ngc.collect()","6a46d6ba":"# I adjust transition data but not apply.\nfor i in range(len(features)):\n    month_means = features[i].loc[:, 0:].mean()\n    plt.figure(figsize=(12,6))\n    plt.bar(features[i].loc[:, 0:].columns,month_means)\n\n    x = np.array(features[i].loc[:, 0:].columns)\n    x = x.reshape(-1, 1)\n    y = np.array(month_means)\n    y = y.reshape(-1, 1)\n\n    # Regression to predict 34 month amm (around month mean)\n    mm_reg_model = LinearRegression()\n    mm_reg_model.fit(x,y)\n\n    plt.figure(figsize=(12,6))\n    plt.bar(features[i].loc[:, 0:].columns,month_means)\n\n    x = np.concatenate([x, [[34]]], axis=0)\n    y = mm_reg_model.predict(x)\n    plt.plot(x, y, linewidth=4, color=\"red\", marker=\"o\")\n\n    mm_ratio = np.array(y \/ y[34])\n    mm_ratio = mm_ratio[:34]\n    mm_ratio = mm_ratio.reshape(len(mm_ratio))\n\n    plt.figure(figsize=(12,6))\n    plt.bar(features[i].loc[:, 0:].columns, month_means \/ mm_ratio)\n\n\n    features[i].loc[:, 0:] = features[i].loc[:, 0:] \/ mm_ratio","f54ffbcc":"# I adjust transition data but not apply.\nfor i in range(len(features)):\n    count_zero = (features[0].loc[:, 0:] == 0).sum()\n    plt.figure(figsize=(12,6))\n    plt.bar(features[i].loc[:, 0:].columns,count_zero)","6769446d":"pre_ave = 0.3","4d8f18e4":"std_pre_ave = (pre_ave \/ transition_max)\nprint(std_pre_ave, pre_ave, transition_max)","462a3077":"# Generate index\nindex = std_transition.index\n\ntrain_index = []\nval_index = []\ntest_index = []\n\ntrain_index = index","f28a87c7":"# Get colum which don't has any sales inofrmation for fillna.\nstd_transition.loc[:20000, 0:].T.sum().sort_values()","f49a101e":"std_transition.loc[18252]","d86dcdb7":"df_test","e4bbaa1e":"sub_index = pd.DataFrame(std_transition.index, columns=['index'])\nsub_index['shop_item_id'] = std_transition['shop_item_id']\nsub_index = pd.merge(df_test, sub_index, on='shop_item_id',how='left')\nprint(sub_index.isna().sum())\nsub_index = sub_index.fillna(18252)\nsub_index = sub_index['index']\nsub_index","c173d7d8":"val_index = train_index\nval_index","b8de156b":"class WindowGenerator():\n    def __init__(self, start_month, last_target_month, input_width, train_index=train_index, val_index=val_index, test_index=test_index, sub_index=sub_index, features=features):\n        self.start_month = start_month\n        self.last_target_month = last_target_month\n        \n        # Work out the window parameters.\n        self.input_width = input_width\n        # It includes y value\n        self.total_width = input_width + 1\n        \n        # Store the raw data.\n        self.train_index = train_index\n        self.val_index = val_index\n        self.test_index = test_index\n        self.sub_index = sub_index\n        \n        # Fixed\n        self.features = features\n        self.features_number = len(features)\n        \n        self.repeat_number = self.last_target_month - self.start_month - self.input_width\n    def __repr__(self):\n        return '\\n'.join([\n            f'Start month: {self.start_month}',\n            f'Input window size: {self.input_width}',\n            f'last_target_month: {self.last_target_month}',\n            \n            f'repeat_number: {self.repeat_number}',\n            \n            f'features_number: {self.features_number}',\n        ])\n    \n    def generate_window(self, features_lists, index_i, start_point):\n        one_window = []\n        for window_i in range(self.input_width):\n            one_features = []\n            for feature_i in range(self.features_number):\n                # Array start zero, so I plus 1 to start point.\n                one_features.append(features_lists[feature_i][index_i][start_point + 1 + window_i])\n            one_window.append(one_features)\n\n        return one_window\n    \n    def generate_list(self, index):\n        x_data = []\n        y_data = []\n        \n        index_number = len(index)\n        features_lists = [feature.loc[index].drop('shop_item_id', axis=1).to_numpy().tolist() for feature in features]\n        \n        reduce = 0\n        \n        # (batch, time, features)\n        for index_i in range(index_number):\n            for time_i in range(self.repeat_number):\n                start_point = self.start_month + time_i\n                one_y = features_lists[0][index_i][start_point + self.total_width]\n                \n#                 # Data whose target is 0 is very big, so I reduce harf. Even if after that, y mean is 0.2, so I should extract more.\n#                 if self.repeat_number > 2:\n#                     if one_y == 0:\n#                         if np.random.rand() < 0.5:\n#                             continue\n                \n                one_window = self.generate_window(features_lists, index_i, start_point)\n                \n                x_data.append(one_window)\n                y_data.append(one_y)\n\n        return x_data, y_data\n    def generate_sub_x(self):\n        features_lists = [feature.loc[self.sub_index].drop('shop_item_id', axis=1).to_numpy().tolist() for feature in features]\n        index_number = len(self.sub_index)\n        \n        # start_point + self.total_width = 34\n        start_point = 34 - self.total_width\n        \n        sub_x = []\n        # (batch, time, features)\n        for index_i in range(index_number):\n             sub_x.append(self.generate_window(features_lists, index_i, start_point))\n                \n        return sub_x\n    \n    def make_dataset(self, index):\n        x_data, y_data = self.generate_list(index)\n        return tf.Dataset.from_tensor_slices((x_data, y_data))\n\n    def train_list(self):\n        return self.generate_list(self.train_index)\n\n    def val_list(self):\n        return self.generate_list(self.val_index)\n\n    def test_list(self):\n        return self.generate_list(self.test_index)\n    \n    \n    @property\n    def train(self):\n        return self.make_dataset(self.train_index)\n\n    @property\n    def val(self):\n        return self.make_dataset(self.val_index)\n\n    @property\n    def test(self):\n        return self.make_dataset(self.test_index)\n","7a2dc893":"input_width = 6\n\ntrain_window = WindowGenerator(\n    start_month = 20 - (input_width + 1),\n    last_target_month = 32,\n    input_width = input_width,\n)\ntrain_window","13f55f01":"val_window = WindowGenerator(\n    start_month = 33 - (input_width + 1),\n    last_target_month = 33,\n    input_width = input_width,\n)\nval_window","3b579d7f":"%%time\nx_train, y_train = train_window.train_list()","c7bcb4aa":"%%time\nx_val, y_val = val_window.val_list()","8bcbaee1":"adj_x_train = np.array(x_train)\nadj_y_train = np.array(y_train)\nadj_y_train = adj_y_train.clip(0, 20.0 \/ transition_max)\nprint(adj_y_train.mean(), adj_y_train.std(), adj_y_train.max())","4b11fdfb":"plt.figure(figsize=(12,6))\nplt.hist(np.sqrt(adj_y_train))","30d6accd":"# One time delete 0.01%\nover_index = np.where(adj_y_train < std_pre_ave)[0]\nprint(over_index)\nprint(len(adj_y_train), len(over_index))\ndrop_rate = 0.035\ndrop_number = int(len(over_index) * drop_rate) if int(len(over_index) * drop_rate) > 1.0 else 1\nprint(drop_number)\n\nadj_area = 0.00005","892b5eac":"%%time\nfor i in range(100):\n    over_index = np.where(adj_y_train < std_pre_ave)[0]\n    np.random.shuffle(over_index)\n    drop_index = over_index[:drop_number]\n    drop_number = int(drop_number * 0.95)\n    \n    adj_x_train = np.delete(adj_x_train, drop_index, 0)\n    adj_y_train = np.delete(adj_y_train, drop_index)\n    if adj_y_train.mean() + adj_area > std_pre_ave:\n        break\nprint(f'Stop i:{i}')\nprint(adj_y_train.mean(), adj_y_train.std(), adj_y_train.max())","3d6fcd9f":"adj_x_val = np.array(x_val)\nadj_y_val = np.array(y_val)\nadj_y_val = adj_y_val.clip(0, 20.0 \/ transition_max)\nprint(adj_y_val.mean(), adj_y_val.std(), adj_y_val.max())","662ae0d0":"plt.figure(figsize=(12,6))\nplt.hist(np.sqrt(adj_y_val))","c7912f87":"# One time delete 0.01%\nover_index = np.where(adj_y_val < std_pre_ave)[0]\nprint(over_index)\nprint(len(adj_y_val), len(over_index))\ndrop_rate = 0.02\ndrop_number = int(len(over_index) * drop_rate) if int(len(over_index) * drop_rate) > 1.0 else 1\nprint(drop_number)\n\nadj_area = 0.00005","ff5db713":"%%time\nfor i in range(100):\n    over_index = np.where(adj_y_val < std_pre_ave)[0]\n    np.random.shuffle(over_index)\n    drop_index = over_index[:drop_number]\n    drop_number = int(drop_number * 0.95)\n    \n    adj_x_val = np.delete(adj_x_val, drop_index, 0)\n    adj_y_val = np.delete(adj_y_val, drop_index)\n    if adj_y_val.mean() + adj_area > std_pre_ave:\n        break\nprint(f'Stop i:{i}')\nprint(adj_y_val.mean(), adj_y_val.std(), adj_y_val.max())","bece26a6":"plt.figure(figsize=(12,6))\nplt.hist(np.sqrt(adj_y_val))","9cf03fd1":"del x_train, y_train, x_val, y_val","a999634a":"def flatten_x(x):\n    flat_row = []\n    for one_row in x:\n        temp_flat_row = []\n        for feature_set in one_row:\n            for one_feature in feature_set:\n                temp_flat_row.append(one_feature)\n        flat_row.append(temp_flat_row)\n        \n    return flat_row","5707c5b2":"adj_x_train = flatten_x(adj_x_train)\nadj_x_val = flatten_x(adj_x_val)","99f5cf45":"models = []","e864e553":"lgb_train = lgb.Dataset(adj_x_train, adj_y_train, free_raw_data=False)\nlgb_val = lgb.Dataset(adj_x_val, adj_y_val, reference=lgb_train, free_raw_data=False)\n\n\nlgbm_params = {\n    'objective': 'mse',\n    'metric': 'rmse',\n    \"num_leaves\": 500,\n    'is_unbalance':True,\n    'boosting':'gbdt',\n    \"learning_rate\": 0.01,\n    'num_boost_round': 10000,\n    'early_stopping_rounds':200\n}\nmodel = lgb.train(lgbm_params,\n                  lgb_train,\n                  valid_names=['train', 'valid'],\n                  valid_sets=[lgb_train, lgb_val],\n                  early_stopping_rounds=20,\n                  verbose_eval=100)\n\nmodel.save_model(f'.\/lgb_model')\nmodels.append(model)\n","e09e348b":"model = catboost.CatBoostRegressor(\n    iterations=700,\n    learning_rate=0.02,\n    depth=12,\n    eval_metric='RMSE',\n    random_seed = 23,\n    bagging_temperature = 0.2,\n    od_type='Iter',\n    metric_period = 75,\n    od_wait=100\n)\n\nmodel.fit(\n    adj_x_train,\n    adj_y_train, \n    eval_set=(adj_x_val,adj_y_val),\n)\n\nmodel.save_model(f'.\/cat_model')\nmodels.append(model)","e275ecaa":"model = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42,\n)\nmodel.fit(\n    adj_x_train,\n    adj_y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(adj_x_val, adj_y_val)], \n    verbose=True, \n    early_stopping_rounds = 10\n)\nmodel.save_model(f'.\/xgb_model')\nmodels.append(model)","3ead277f":"# Gradient boosting models\nsub_x = train_window.generate_sub_x()\nsub_x = flatten_x(sub_x)\n\npreds = np.zeros(len(sub_x))\nfor model in models:\n    preds = preds + model.predict(sub_x)\npreds = preds \/ len(models)","d33b8e07":"# preds = model.predict(sub_x)\npreds = preds * transition_max\npreds = preds.clip(0.0, 20.0)","596d1ea9":"print(preds.mean(), preds.std(), preds.max())","f814d4c7":"plt.figure(figsize=(12,6))\nplt.hist(preds,bins=50)","fbf964e3":"plt.figure(figsize=(12,6))\nplt.hist(preds[(preds>0.0) & (preds<1.0)],bins=50)","0b208800":"sample['item_cnt_month'] = preds\nsample.to_csv('submission.csv', index=False)","e80ce5a3":"sample.head(50)","9b349ef6":"# 6. EDA","0adff441":"From this result, I can understand that the frequency of data wasn't changed. Only sales amount is decreasing.\n\n<br \/>\n\n# 7. Data Leakgages & Metrics Optimization\nI write about that at 1.Most important ingenuity.\nAfter leaned about Data Leakages, I want to know features of submission data. Then, from Metics Optimization lectures, I knew submitting by single value can find out mean of submission data, because it used RMSE. Thus, I estimate average is 0.3, so I define variable here.","7b593425":"# 8. Validation\nThis is time seriese dataset. Thus, I used only before 32 months data as trained data. Also, I used target is 33 month dataset as validation.","b18ee6cb":"# 11. Ensemble\nI did hyperparameter tuning. I ensemble three types of gradient boost models.","e7618a41":"# 5. Mean Encording\nI learned add noise will improve dataset quority of mean encording data. Shop and item etc feature are one type of mean encording, so I add noise.\n\n50% data of submit data don't has past sales information. Thus, shop and item features are important. However, the data which has only item and shop data is too big, so I add small noise.","7d08d357":"# 9. Generate Dataset\nBy making class, I can generate dataset very easy.","b0714965":"# 10. Adjust Dataset\nAfter generating dataset from above class by separate train and val by 10. Vlidation method, I eliminate some data less than estimated 34 months average.","b8e991aa":"# 4. Feature Preprocessing","4be2fd97":"![justin-lim-JKjBsuKpatU-unsplash.jpg](attachment:ee30e7dd-81c9-4872-8bbc-963fa234ec4c.jpg)\nPhoto by <a href=\"https:\/\/unsplash.com\/@justinlim?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Justin Lim<\/a> on <a href=\"https:\/\/unsplash.com\/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash<\/a>\n  \n<br \/>\n\nI'm taking How to Win a Data Science Competition: Learn from Top Kagglers course. It was very wonderful and this is that course's final project.\n\n# Flow\n1. Most important ingenuity\n2. Result\n3. Requirements.txt\n4. Feature Preprocessing\n5. Mean Encording\n6. EDA\n7. Data Leakgages & Metrics Optimization\n8. Generate Dataset\n9. Validation\n10. Adjust Dataset\n11. Ensembles\n\n<br \/>\n\n## 1. Most important ingenuity\nThe features which I found and I think are very important are three.\n\nFirstly, I will show the most important ingenuity of my model. It is aligning the train and validation average to the estimated 34 months' mean.\n\nI think important data is sales_train.csv because these data become both input and target value.\n\nFirst, I check the relationship to test.csv which we should predict and submit has a lot of columns that don't have any past months' sales information from sales_train.csv: approximately 45%. From that, the information of other data such as items, shops, and especially item_categories information (after I add categories transition data, model greatly improved than other data).\n\nNext, I found the amount of sales is decreasing from the first month to last month. The below graph shows the mean item count per month for each month 0~33.\n\nThe mean is decreasing, so I want to eliminate the descent because I generate a data set of the transition of each month's sales information to predict the next month. For example, I add 6 months of sales data of the same shop, the same item, the same shop, the same item, and the same category. Thus, this previous information is very important, because the training target may become one years ago data.\n\nThus, I used LinearRegression from scikit-learn, and I adjusted the data by this program.\n\n![Screen Shot 2021-12-09 at 0.43.59.png](attachment:cacbd132-e2d2-4a9b-8a92-046ee3b622f9.png)\n\n```python\nmonth_means = features[i].loc[:, 0:].mean()\nplt.figure(figsize=(12,6))\nplt.bar(features[i].loc[:, 0:].columns,month_means)\n\nx = np.array(features[i].loc[:, 0:].columns)\nx = x.reshape(-1, 1)\ny = np.array(month_means)\ny = y.reshape(-1, 1)\n\n# Regression to predict 34 month amm (around month mean)\nmm_reg_model = LinearRegression()\nmm_reg_model.fit(x,y)\n\nplt.figure(figsize=(12,6))\nplt.bar(features[i].loc[:, 0:].columns,month_means)\n\nx = np.concatenate([x, [[34]]], axis=0)\ny = mm_reg_model.predict(x)\nplt.plot(x, y, linewidth=4, color=\"red\", marker=\"o\")\n\nmm_ratio = np.array(y \/ y[34])\nmm_ratio = mm_ratio[:34]\nmm_ratio = mm_ratio.reshape(len(mm_ratio))\n\nplt.figure(figsize=(12,6))\nplt.bar(features[i].loc[:, 0:].columns, month_means \/ mm_ratio)\n\nfeatures[i].loc[:, 0:] = features[i].loc[:, 0:] \/ mm_ratio\n```\n\n![Screen Shot 2021-12-09 at 0.44.10.png](attachment:d422941e-bcce-4efc-870e-41dcb168bbf3.png)\n\nI set 34 months mean prediction which was predicted by linear regression as 1 and aligned columns for each month.\n\nHowever, a model trained by using that data becomes worse. I think that reason is this information on those changes is important, so I took other approaches.\n\nThat approach is estimating 34 months' mean.\n\nAfter taking the lessons, Metrics optimization, I submit single values predictions many times, to estimate 34 months' correct mean, because I know they used RMSE, so the value which can get the best score is close to the mean of target data. The below list is result.\n\n### **Single value scores**\n\n- 0.2     : 1.220329 and 1.206208.\n- 0.25   : 1.217912 and 1.203698.\n- 0.3     : 1.217545 and 1.203262.\n- 0.35   : 1.219230 and 1.204903.\n- 0.4     : 1.222959 and 1.208611.\n- 0.5     : 1.23646\n- 0.6     : 1.245039 and 1.235736.\n\nThe above data is the result, from that I can estimate the mean close to 0.3. Thus, I adjust the train and validation data to close to 0.3.\n\n```python\nadj_x_train = np.array(x_train)\nadj_y_train = np.array(y_train)\nadj_y_train = adj_y_train.clip(0, 20.0 \/ transition_max)\nprint(adj_y_train.mean(), adj_y_train.std(), adj_y_train.max())\nplt.figure(figsize=(12,6))\nplt.hist(np.sqrt(adj_y_train))\n# One time delete 0.01%\nover_index = np.where(adj_y_train < std_pre_ave)[0]\nprint(over_index)\nprint(len(adj_y_train), len(over_index))\ndrop_rate = 0.035\ndrop_number = int(len(over_index) * drop_rate) if int(len(over_index) * drop_rate) > 1.0 else 1\nprint(drop_number)\n\nadj_area = 0.00005\n%%time\nfor i in range(100):\n    over_index = np.where(adj_y_train < std_pre_ave)[0]\n    np.random.shuffle(over_index)\n    drop_index = over_index[:drop_number]\n    drop_number = int(drop_number * 0.95)\n    \n    adj_x_train = np.delete(adj_x_train, drop_index, 0)\n    adj_y_train = np.delete(adj_y_train, drop_index)\n    if adj_y_train.mean() + adj_area > std_pre_ave:\n        break\nprint(f'Stop i:{i}')\nprint(adj_y_train.mean(), adj_y_train.std(), adj_y_train.max())\n```\n\n\n# 2. Result\nThis process of eliminating some columns to close an accurate dataset was good I think.\n\nVersion 2 is non adjusted and LB score 1.00716\nVersion 3 is adjusted but LB socre becomes worse 1.00718\n\nVersion 4 is adjusted more and LB score is 1.00637.\n\nIt improved very little, so it may be meaningless.\n\n# 3. Requirements.txt\nMyApp\nsklearn: 0.23.2\nLightGBM: 3.2.1\nCatBoost: 1.0.0"}}