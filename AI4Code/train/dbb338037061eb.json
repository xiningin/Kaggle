{"cell_type":{"9f374cb7":"code","27009814":"code","15f855bd":"code","4598eb05":"code","c656109a":"code","b062fedd":"code","d671e4c8":"code","6ec20ea4":"code","632f9159":"code","c2619303":"code","8fa64e3a":"code","e559b841":"code","693fd8a4":"code","fb640eae":"code","ea1dbab1":"code","64cb0ef5":"code","64c27273":"code","c62b0d4e":"code","b31b752a":"code","605be5a3":"code","2969f35b":"code","ba7408fc":"code","f1593eb2":"code","576dd16e":"code","aa57a4fe":"code","33856644":"code","7db591db":"code","d8067379":"code","bb824c2e":"code","6b0ae843":"code","b15a7505":"code","22caa978":"code","d5b09e1b":"code","3a6ef80a":"code","c6019b71":"code","854406b9":"markdown","1152c060":"markdown","69acb735":"markdown","038928e7":"markdown","1145617c":"markdown","27a1a67f":"markdown","2ec354c0":"markdown"},"source":{"9f374cb7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns' , 200)\npd.set_option('display.max_rows' , 200)\npd.set_option('display.width' , 1000)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","27009814":"df_house = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_house.head()","15f855bd":"# size of dataset\ndf_house.shape","4598eb05":"df_house.describe()","c656109a":"#check datatype of column\ndf_house.info()","b062fedd":"#check for null value\ndf_null =round(df_house.isnull().sum()*100\/len(df_house),2)\ndf_null= df_null[df_null.values >0]","d671e4c8":"#drop the columns having more than 50% null data\ndf_house.drop(columns = list(df_null[df_null.values >50].index),inplace = True ,axis =1)","6ec20ea4":"#Computing null values \ncol = list(set(df_null.index)-set(df_null[df_null.values >50].index))\ndf_house_null = df_house[col]\ndf_house_null.head()","632f9159":"# amount of null values in percentage in df_house_null\nround(df_house_null.isnull().sum()*100\/len(df_house),2)","c2619303":"# lets replace Null values of num col to median\nnum_col = list(set(df_house_null._get_numeric_data()))\nfor i in num_col:\n    df_house[i].fillna(df_house[i].median(),inplace = True)\ndf_house[num_col].isnull().sum()","8fa64e3a":"# lets replace Null values of cat col to mode\ncat_col = list(set(df_house_null.columns) - set(num_col))\nfor i in cat_col:\n    df_house[i].fillna(df_house[i].mode()[0],inplace = True)\ndf_house[cat_col].isnull().sum()","e559b841":"#lets check high correlation columns\ndf_corr=df_house.corr()\ndf_corr = df_corr.where(np.triu(np.ones(df_corr.shape),k=1).astype(np.bool)).unstack().reset_index()\ndf_corr = df_corr.sort_values(by = 0 ,ascending = False)\ndf_corr.dropna(inplace=True)\ndf_corr = df_corr[df_corr['level_0']=='SalePrice']\n#taking top 30 high correlated data\ndf_corr = df_corr.head(30)\ndf_corr","693fd8a4":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nnul_col = df_house._get_numeric_data()\ncat_col = list(set(df_house.columns)-set(nul_col))\nfor colName in cat_col:\n    le.fit(df_house[colName].drop_duplicates())\n    df_house[colName] = le.transform(df_house[colName])","fb640eae":"from sklearn.model_selection import train_test_split\ndf_train,df_test = train_test_split(df_house,test_size = .80,random_state = 100)","ea1dbab1":"# after label encoding check datatype\ndf_train.info()","64cb0ef5":"y_train = df_train.pop('SalePrice')\nX_train = df_train","64c27273":"X_train_sm = sm.add_constant(X_train)\nlr = sm.OLS(y_train,X_train_sm.astype(float))\nlr_model = lr.fit()\nlr_model.summary()\np_values_df = pd.DataFrame()\np_values_df['Features'] = X_train.columns\np_values_df['P_Value'] = [round(lr_model.pvalues[i],2) for i in X_train.columns] ","c62b0d4e":"#find the inverse variance Factor\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\ndef vif_func(X_train,vif):\n    vif['Features'] = X_train.columns\n    vif['VIF'] = [variance_inflation_factor(X_train.astype(float).values,i) for i in range(X_train.astype(float).shape[1])]\n    vif['VIF'] = round(vif['VIF'],2)\n    vif = vif.sort_values(by = 'VIF',ascending = False)\n    return vif\nvif = pd.DataFrame()\nvif_func(X_train,vif)","b31b752a":"# concat p_value and VIF dataframe\np_vif_df = pd.DataFrame()\np_vif_df = vif.merge(p_values_df , how = 'inner')\n# drop for constance variable\np_vif_df.drop(index = 0, axis = 0)","605be5a3":"# function to improvise model\ncol_name = []\np_vif_df = p_vif_df[p_vif_df['P_Value']<.05]\np_vif_df .head()","2969f35b":"# drop col having VIF < 5 \np_vif_df = p_vif_df[p_vif_df['VIF']<5]\nFinal_col = list(p_vif_df['Features'].value_counts().index)\nFinal_col","ba7408fc":"# build model using final columns\nX_train_sm = X_train_sm[Final_col]\nlr = sm.OLS(y_train,X_train_sm)\nlr_model = lr.fit()\nlr_model.summary()","f1593eb2":"# Check for Y_train_pred\ny_train_pred = lr_model.predict(X_train_sm)\n# calculating residual\nres_train = y_train - y_train_pred\n# plot distribution of residual\nsns.distplot(res_train)\n# lets check efficency of model of train sample\nfrom sklearn.metrics import r2_score\nr2_score(y_true = y_train,y_pred = y_train_pred )","576dd16e":"sns.regplot(x=y_train, y=y_train_pred)\nplt.title('Predicted Points Vs. Actual Points', fontdict={'fontsize': 20})\nplt.xlabel('Actual Points', fontdict={'fontsize': 15})\nplt.ylabel('Predicted Points', fontdict={'fontsize': 15})\nplt.show()","aa57a4fe":"#defining target and dependent dataset in test model\ny_test = df_test.pop('SalePrice')\nX_test = df_test","33856644":"\n# update with columns which we used in  X_train\nX_test_sm = sm.add_constant(X_test)\nX_test_sm = X_test_sm[Final_col]\nX_test_sm.head()","7db591db":"# Check for Y_est_pred\n\ny_test_pred = lr_model.predict(X_test_sm)\n# calculating residual\nres_test = y_test - y_test_pred\n# plot distribution of residual\nsns.distplot(res_test)\n# lets check efficency of model of train sample\nfrom sklearn.metrics import r2_score\nr2_score(y_true = y_test,y_pred = y_test_pred )","d8067379":"df_house_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_house_test.head()","bb824c2e":"df_house_test.info()","6b0ae843":"#defining target and dependent dataset in test model\nX_house_test = df_house_test","b15a7505":"\n# update with columns which we used in  X_train\nX_house_test_sm = sm.add_constant(X_house_test)\nX_house_test_sm = X_house_test_sm[Final_col]\nX_house_test_sm.head()","22caa978":"# Check for Y_est_pred\n\ny_house_test_pred = lr_model.predict(X_house_test_sm)\n","d5b09e1b":"output = pd.DataFrame({'Id': X_house_test_sm.index, \n                       'SalePrice': y_house_test_pred})\noutput.shape","3a6ef80a":"output.dropna(inplace = True)","c6019b71":"output.to_csv('submission.csv', index=False)\nprint('Your submission was successfully saved!')","854406b9":"#### Observation:We can see for baove odel we have Rsquare .874 and adjusted R squrae .874\n1. MasVnrArea : Masonry veneer area in square feet is having coef value 195.6622\n2. Id : Type of roof is having coef value 96.7571\n3. LotArea : Lot size in square feet is having coef value  1.3115\n4. Fireplaces: Number of fireplaces is having coef value  6.784e+04\n5. RoofMatl: Roof material is having coef value  2.31e+04\n","1152c060":"### Splitting the data into train and test dataset","69acb735":"#### now we need to follow reverse method approch to find best fit model\n1. If V factor and p value is above 0.5 drop that column\n2. If Vfactor and P value same than first drop column having high P value than see whether its affecting V factor","038928e7":"SalePrice = 17.470 * MasVnrArea+ Id * 66.1091 + LotArea *1.6484 + BsmtFullBath * 2.689e+04+ Fireplaces *4.321e+04+ RoofMatl *4.252e+04","1145617c":"### We can see that the equation for best fitted line is:","27a1a67f":"#### Final columns are \n['ScreenPorch', 'WoodDeckSF', 'BsmtFullBath', 'MasVnrArea']","2ec354c0":"def model_improvise(p_vif_df,y_train,X_train):\n    # check for highest p value as data is already sorted accross Pvalue\n    for i in range(p_vif_df.shape[0]):\n        if p_vif_df['P_Value'] > 0.05:\n            col_name.append(p_vif_df['Features'])\n    X_train.drop(columns = col_name,axis = 1 , inplace = True)\nmodel_improvise(p_vif_df,y_train,X_train)\nX_train.columns"}}