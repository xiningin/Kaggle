{"cell_type":{"f2628a40":"code","c1b6930f":"code","40def89b":"code","a50c005d":"code","839ed9b9":"code","b582428e":"code","2a66ea80":"code","4c51a6ec":"code","fde6c949":"code","10fa2f82":"code","9e45adc4":"code","48fd7d27":"code","508cf30f":"code","cad9c4be":"code","2f2a574a":"code","138ce294":"code","f72c8153":"code","4a21d7f9":"code","c43bdf37":"code","2e6e54dd":"code","7c23f2e4":"code","51319b86":"code","12b2f237":"code","cd403d80":"code","8e845d4e":"code","f6589938":"code","e99a3fda":"code","d0f40795":"code","c9328819":"code","eab57a75":"code","238eb438":"code","9508d0eb":"code","9fbc46de":"code","6231da8c":"code","bf4bab1d":"code","f0c934dc":"code","b3edd714":"code","24732e7e":"code","f4c48414":"code","f712489c":"code","dbc46254":"code","35c23a83":"code","eaf06df9":"code","d6cf05ea":"code","16c9fa7a":"code","ef0e5142":"code","7b7887ef":"code","467f0a0d":"code","3079c530":"code","0a3f7536":"code","9ff881fd":"code","53082719":"code","1d88fa0c":"code","35389475":"code","5eb6bb54":"code","fecef992":"code","c1878840":"code","2752ae5b":"code","a5b05c4f":"code","f29be4bf":"code","2eef775e":"code","0fee78b5":"code","810b58ec":"code","a2e77393":"code","aef6c5aa":"code","73907687":"code","77025d46":"code","6ae8f3b6":"code","f1e2a776":"code","c327a65a":"code","4724a650":"code","0b64f1e8":"code","9ed3a2a7":"code","2b30e1d7":"code","2834304c":"code","90e4faf7":"code","d3f744a0":"code","b9d87cf4":"code","ce337c1e":"code","91b3cb06":"code","4a7a15cd":"code","989c124a":"code","bb41c738":"code","00f89e9e":"code","0f6bf0f4":"code","219e3356":"code","7fa1479d":"code","e5a5b838":"code","541045d5":"code","781e1017":"code","750776b5":"code","0ab73714":"code","559e1aba":"code","d1c8f10a":"code","c85c3d95":"code","94ec1376":"code","a598e5df":"code","4954ea9a":"code","7775db3b":"code","281cf112":"code","2ad8057b":"code","2ad54f6a":"code","66e6e092":"code","f45bc32e":"code","728cf112":"code","926a7363":"code","31e4b394":"code","949eeaaf":"code","581a31cd":"code","e3e525b8":"code","f49ac5fd":"code","d4ab0b2d":"code","dac30505":"code","2d03431d":"code","aa083fe1":"code","57483709":"code","b94f0505":"code","a147cf3a":"code","bb509a6a":"code","c44e4f9d":"code","5c536a4d":"code","11973211":"code","cfb91684":"code","311bd06b":"code","586da863":"code","d40c966b":"code","9f30b8c0":"code","bdcb6979":"code","da63aced":"code","964bcbb0":"code","c06e940f":"code","9d0fa547":"code","7f219886":"code","c5200512":"code","9b70a6fd":"code","25e9b72d":"code","6833118a":"code","e79987e2":"code","ceefcd35":"code","e441d873":"code","debbaf0d":"code","a098d982":"code","8ab2199e":"code","3edbeb33":"code","95788b6f":"code","ebf5555a":"code","d7bdebeb":"code","487a0699":"code","ae3f4e89":"code","e5523c89":"code","820a0515":"code","5ce270fb":"markdown","50e83acb":"markdown","d18f700a":"markdown","38c2d55c":"markdown","55008164":"markdown","eea0c29b":"markdown","5de7f524":"markdown","369d0a43":"markdown","8bb4ecf1":"markdown","c9f2bf1e":"markdown","21b7fc52":"markdown","2a1ad9b3":"markdown","b96b567e":"markdown","06870ed6":"markdown","793fc27e":"markdown","29755189":"markdown","4cac5cc3":"markdown","8fcc4f1e":"markdown","0f65b499":"markdown","0a4d4e93":"markdown","6b9ece36":"markdown","d00d1ae1":"markdown","d1e6e7ca":"markdown","b1e48f53":"markdown","16bccc35":"markdown","3df06084":"markdown","c79aede8":"markdown","eb227f04":"markdown","a47ca0c1":"markdown","c5c844c6":"markdown","982f5b2a":"markdown","4291c11e":"markdown","30a70310":"markdown","21d5da8f":"markdown","962cbc16":"markdown","d8f62b06":"markdown","5e24419a":"markdown","676b27b7":"markdown","22c95a88":"markdown","422c8a4d":"markdown","3d0bf530":"markdown","c9f84f8c":"markdown","983a29e3":"markdown","fa8ca62a":"markdown","0a3fec66":"markdown","17c2dc6e":"markdown","33e878a0":"markdown","baa27557":"markdown","58c38942":"markdown","da6a8605":"markdown","55ff6a82":"markdown","3acef8ce":"markdown","ea4c9d68":"markdown","a4b58dac":"markdown","b2fcac91":"markdown","e9be026b":"markdown","b11bc31d":"markdown","50a3e380":"markdown","70c4b937":"markdown","352b1abc":"markdown","2119b41c":"markdown","ebd550e8":"markdown","e63c024e":"markdown","d5c0c224":"markdown","ebe7e946":"markdown","9d1f7a6e":"markdown","f84d415c":"markdown","a367fdb7":"markdown","b51d15ec":"markdown","e495fe15":"markdown","7f8ed040":"markdown","c8ba9a52":"markdown","da7c3ca2":"markdown","bc295265":"markdown","25541c58":"markdown","8a7929f1":"markdown","11e10daf":"markdown","ce32ca48":"markdown","f473884f":"markdown","1954020e":"markdown","c0a56e81":"markdown","99e31907":"markdown","5ce316ef":"markdown","96d3ade5":"markdown","4cd55c82":"markdown","c0bf8958":"markdown","d7fa1f21":"markdown","a01bd1fd":"markdown","256a7e04":"markdown","c96100cf":"markdown","3b9d9bb9":"markdown","d7997e7f":"markdown","283769a9":"markdown","2af20530":"markdown","def71038":"markdown","f8835502":"markdown","d5a282c6":"markdown","e68eec13":"markdown","171921d2":"markdown","6970457c":"markdown","689e3ffb":"markdown","2d179861":"markdown","cc032cea":"markdown","00b8a18f":"markdown"},"source":{"f2628a40":"import os\nimport datetime\n\nimport IPython\nimport IPython.display\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\n\nmpl.rcParams['figure.figsize'] = (8, 6)\nmpl.rcParams['axes.grid'] = False","c1b6930f":"CSV_FILE_NAME_OUTPUT_MOY = '..\/input\/french-financial-news\/FrenchNewsDayConcat.csv'\n\ndf = pd.read_csv(CSV_FILE_NAME_OUTPUT_MOY)\n","40def89b":"df","a50c005d":"date_time = pd.to_datetime(df.pop('Date'), format='%Y.%m.%d %H:%M:%S')\n","839ed9b9":"date_time","b582428e":"df.head()","2a66ea80":"#On supprime les collones inutiles\ndf.pop('Nbr Day')\ndf.pop('NbrNewsJour')","4c51a6ec":"df.head()","fde6c949":"plot_cols = ['Open','Mean sent text','Day Sent Vader Text','Volume']\nplot_features = df[plot_cols]\nplot_features.index = date_time\n_ = plot_features.plot(subplots=True)\n\nplot_features = df[plot_cols][:48]\nplot_features.index = date_time[:48]\n_ = plot_features.plot(subplots=True)","10fa2f82":"df.describe().transpose()","9e45adc4":"df['Volume'].plot()","48fd7d27":"# Il y a des donn\u00e9s manquantes dans les Volumes, on remplace les 0 par la moyenne\nVolume = df['Volume']\n\nmeanVol = Volume.mean()\n\nbad_Volume = Volume == 0.0\nVolume[bad_Volume] = meanVol\n","508cf30f":"df.describe().transpose()","cad9c4be":"plt.hist2d(df['High'], df['Low'], bins=(50, 50), vmax=10)\nplt.colorbar()\nplt.xlabel('High')\nplt.ylabel('Low')","2f2a574a":"########################################################\n# Pour faciliter la convergence du model, il faut absolument d\u00e9finir une seul variable qui contien l'info du cours.\n# Actuelement cette info est dans Open \/ Close \/ High \/ Low.\n# Il faut calculer les diff\u00e9rences comme \"Open_Close_Var\" et \"Amplitude\" pour \"Adj Close\" et autre, ex Open-Min ...\n# A d\u00e9terminer pour permetre de r\u00e9cuperer un max d'info avec une seul variable de cours.\n########################################################","138ce294":"# Convert to amplitude.\ndf['Amplitude'] = df['High'] - df['Low']\n\ndf['Amplitude']","f72c8153":"df['Open_Close_Var'] = df['Close'] - df['Open']\n\ndf['Open_Close_Var']","4a21d7f9":"plt.hist2d(df['Amplitude'], df['Open_Close_Var'], bins=(50, 50), vmax=5)\nplt.colorbar()\nplt.xlabel('Amplitude')\nplt.ylabel('Open_Close_Var')\nax = plt.gca()\nax.axis('tight')","c43bdf37":"df['Open_Low_Var'] = df['Open'] - df['Low']\n","2e6e54dd":"df['Close_High_Var'] = df['Close'] - df['High']\n","7c23f2e4":"#df['Close_AdjClose'] = df['Close'] - df['Adj Close']\n#df['Close_AdjClose']","51319b86":"\n#Suppresion des variables de cours redondantes\ndf.pop('High')\ndf.pop('Low')\ndf.pop('Close')\n\n#pour le cac40 AdjClose est inutile car indentique \u00e0 Close\ndf.pop('Adj Close')","12b2f237":"df.describe().transpose()","cd403d80":"'''\n#####################\n## Ajout de features  -> non utilis\u00e9 car rend difficile la convergence du model\n#####################\n\n#Moyenne et d\u00e9riv\u00e9\nfor column in df:\n    df[column+'_mean10'] = df[column].rolling(window=10,min_periods=0).mean()\n    df[column+'_diff'] = df[column].diff()\n    df[column+'_diff'][0] = df[column+'_diff'][1]\n    \nplt.plot(np.array(df['Mean sent text']))\nplt.plot(np.array(df['Mean sent text_diff']))\n'''","8e845d4e":"df\n","f6589938":"timestamp_s = date_time.map(datetime.datetime.timestamp)","e99a3fda":"timestamp_s","d0f40795":"week = 7*24*60*60     # 5 jours ouvr\u00e9s par semaines\nmonth = 30.4167*24*60*60\nyear = 365.2425*24*60*60    # 254 jours ouvr\u00e9s par ann\u00e9s\nyear10 = 365.2425*24*60*60*10    # 254 jours ouvr\u00e9s par ann\u00e9s\n\n\ndf['week sin'] = np.sin(timestamp_s * (2 * np.pi \/ week))\ndf['week cos'] = np.cos(timestamp_s * (2 * np.pi \/ week))\n\ndf['month sin'] = np.sin(timestamp_s * (2 * np.pi \/ month))\ndf['month cos'] = np.cos(timestamp_s * (2 * np.pi \/ month))\n\ndf['Year sin'] = np.sin(timestamp_s * (2 * np.pi \/ year))\ndf['Year cos'] = np.cos(timestamp_s * (2 * np.pi \/ year))\n\ndf['10 Year sin'] = np.sin(timestamp_s * (2 * np.pi \/ year10))\ndf['10 Year cos'] = np.cos(timestamp_s * (2 * np.pi \/ year10))","c9328819":"plt.plot(np.array(df['week sin'])[:20])\nplt.plot(np.array(df['week cos'])[:20])\nplt.xlabel('Time [Day]')\nplt.title('Time of week signal')","eab57a75":"plt.plot(np.array(df['month sin'])[:100])\nplt.plot(np.array(df['month cos'])[:100])\nplt.xlabel('Time [Day]')\nplt.title('Time of month signal')","238eb438":"plt.plot(np.array(df['Year sin'])[:365])\nplt.plot(np.array(df['Year cos'])[:365])\nplt.xlabel('Time [Day]')\nplt.title('Time of Year signal')","9508d0eb":"plt.plot(np.array(df['10 Year sin'])[:365])\nplt.plot(np.array(df['10 Year cos'])[:365])\nplt.xlabel('Time [Day]')\nplt.title('Time of 10 Year signal')","9fbc46de":"#Limitation de la taille de memoire GPU utilis\u00e9, sur RTX3090, 18Go\n'''\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n  try:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=18000)])   #18 Go\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Virtual devices must be set before GPUs have been initialized\n    print(e)\n'''","6231da8c":"fft = tf.signal.rfft(df['Open'])\nf_per_dataset = np.arange(0, len(fft))\n\nn_samples_h = len(df['Open'])\nday_per_year = 365.2524\nyears_per_dataset = n_samples_h\/(day_per_year)\n\nf_per_year = f_per_dataset\/years_per_dataset\nplt.step(f_per_year, np.abs(fft))\nplt.xscale('log')\nplt.ylim(0, 150000)\nplt.xlim([0.1, max(plt.xlim())])\nplt.xticks([1, 12, 52.1429], labels=['1\/Year', '1\/mounth', '1\/week'])\n_ = plt.xlabel('Frequency (log scale)')","bf4bab1d":"fft = tf.signal.rfft(df['month sin'])\nf_per_dataset = np.arange(0, len(fft))\n\nn_samples_h = len(df['month sin'])\nday_per_year = 365.2524\nyears_per_dataset = n_samples_h\/(day_per_year)\n\nf_per_year = f_per_dataset\/years_per_dataset\nplt.step(f_per_year, np.abs(fft))\nplt.xscale('log')\nplt.ylim(0, 150)\nplt.xlim([0.1, max(plt.xlim())])\nplt.xticks([1, 12, 52.1429], labels=['1\/Year', '1\/mounth', '1\/week'])\n_ = plt.xlabel('Frequency (log scale)')","f0c934dc":"fft = tf.signal.rfft(df['Day Sent Vader Text URL'])\nf_per_dataset = np.arange(0, len(fft))\n\nn_samples_h = len(df['Mean sent text'])\nday_per_year = 365.2524\nyears_per_dataset = n_samples_h\/(day_per_year)\n\nf_per_year = f_per_dataset\/years_per_dataset\nplt.step(f_per_year, np.abs(fft))\nplt.xscale('log')\nplt.ylim(0, 20)\nplt.xlim([0.1, max(plt.xlim())])\nplt.xticks([1, 12, 52.1429], labels=['1\/Year', '1\/mounth', '1\/week'])\n_ = plt.xlabel('Frequency (log scale)')","b3edd714":"#check dataset\ndf.describe().transpose()","24732e7e":"column_indices = {name: i for i, name in enumerate(df.columns)}\n\nn = len(df)\ntrain_df = df[0:int(n*0.7)]\nval_df = df[int(n*0.7):int(n*0.9)]\ntest_df = df[int(n*0.9):]\n\nnum_features = df.shape[1]","f4c48414":"n","f712489c":"train_df","dbc46254":"val_df","35c23a83":"test_df","eaf06df9":"num_features","d6cf05ea":"#On v\u00e9rifie la d\u00e9coupe du dataset\nplt.plot(train_df['Open'],label='train_df')\nplt.plot(val_df['Open'],label='val_df')\nplt.plot(test_df['Open'],label='test_df')\nplt.legend()\nplt.xlabel('Time [Day]')\nplt.title('Data set split check')","16c9fa7a":"#Affichage des donn\u00e9s de test\n","ef0e5142":"plt.figure(figsize=(16, 4))\nplt.plot(test_df['Open'],label='test_df')\nplt.legend()\nplt.xlabel('Time [Day]')\nplt.title('Data set split check')","7b7887ef":"train_mean = train_df.mean()\ntrain_std = train_df.std()\n\ntrain_df = (train_df - train_mean) \/ train_std\nval_df = (val_df - train_mean) \/ train_std\ntest_df = (test_df - train_mean) \/ train_std","467f0a0d":"train_df","3079c530":"#On v\u00e9rifie la d\u00e9coupe du dataset\nplt.figure(figsize=(15, 6))\nplt.plot(train_df['Volume'],label='train_df')\nplt.plot(val_df['Volume'],label='val_df')\nplt.plot(test_df['Volume'],label='test_df')\nplt.legend()\nplt.xlabel('Time [Day]')\nplt.title('Data set split check')","0a3f7536":"#On v\u00e9rifie la d\u00e9coupe du dataset\nplt.figure(figsize=(15, 6))\nplt.plot(train_df['Open'],label='train_df')\nplt.plot(val_df['Open'],label='val_df')\nplt.plot(test_df['Open'],label='test_df')\nplt.legend()\nplt.xlabel('Time [Day]')\nplt.title('Data set split check')","9ff881fd":"df_std = (df - train_mean) \/ train_std\ndf_std = df_std.melt(var_name='Column', value_name='Normalized')\nplt.figure(figsize=(12, 6))\nax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n_ = ax.set_xticklabels(df.keys(), rotation=90)","53082719":"class WindowGenerator():\n  def __init__(self, input_width, label_width, shift,\n               train_df=train_df, val_df=val_df, test_df=test_df,\n               label_columns=None):\n    # Store the raw data.\n    self.train_df = train_df\n    self.val_df = val_df\n    self.test_df = test_df\n\n    # Work out the label column indices.\n    self.label_columns = label_columns\n    if label_columns is not None:\n      self.label_columns_indices = {name: i for i, name in\n                                    enumerate(label_columns)}\n    self.column_indices = {name: i for i, name in\n                           enumerate(train_df.columns)}\n\n    # Work out the window parameters.\n    self.input_width = input_width\n    self.label_width = label_width\n    self.shift = shift\n\n    self.total_window_size = input_width + shift\n\n    self.input_slice = slice(0, input_width)\n    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n    self.label_start = self.total_window_size - self.label_width\n    self.labels_slice = slice(self.label_start, None)\n    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n  def __repr__(self):\n    return '\\n'.join([\n        f'Total window size: {self.total_window_size}',\n        f'Input indices: {self.input_indices}',\n        f'Label indices: {self.label_indices}',\n        f'Label column name(s): {self.label_columns}'])","1d88fa0c":"w1 = WindowGenerator(input_width=24, label_width=1, shift=24,\n                     label_columns=['Open'])\nw1","35389475":"w1.label_columns_indices","5eb6bb54":"w1.column_indices","fecef992":"w1.input_width","c1878840":"w1.total_window_size","2752ae5b":"w1.label_indices","a5b05c4f":"w2 = WindowGenerator(input_width=6, label_width=1, shift=1,\n                     label_columns=['Open'])\nw2","f29be4bf":"w2 = WindowGenerator(input_width=60, label_width=1, shift=1,\n                     label_columns=['Open'])\nw2","2eef775e":"def split_window(self, features):\n  inputs = features[:, self.input_slice, :]\n  labels = features[:, self.labels_slice, :]\n  if self.label_columns is not None:\n    labels = tf.stack(\n        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n        axis=-1)\n\n  # Slicing doesn't preserve static shape information, so set the shapes\n  # manually. This way the `tf.data.Datasets` are easier to inspect.\n  inputs.set_shape([None, self.input_width, None])\n  labels.set_shape([None, self.label_width, None])\n\n  return inputs, labels\n\nWindowGenerator.split_window = split_window","0fee78b5":"print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","810b58ec":"#######################################################################################\n####            ATTENTION, ICI ON UTILISE train_df AU LIEU DE test_df !!!!!\n#######################################################################################\n\n# Stack three slices, the length of the total window:\n\"\"\"\nexample_window = tf.stack([np.array(train_df[:w2.total_window_size]),\n                           np.array(train_df[100:100+w2.total_window_size]),\n                           np.array(train_df[200:200+w2.total_window_size])])\n\"\"\"\n\nexample_window = tf.stack([np.array(test_df[:w2.total_window_size]),\n                           np.array(test_df[10:10+w2.total_window_size]),\n                           np.array(test_df[20:20+w2.total_window_size])])\n\n\n\nexample_inputs, example_labels = w2.split_window(example_window)\n\nprint('All shapes are: (batch, time, features)')\nprint(f'Window shape: {example_window.shape}')\nprint(f'Inputs shape: {example_inputs.shape}')\nprint(f'labels shape: {example_labels.shape}')","a2e77393":"w2.example = example_inputs, example_labels","aef6c5aa":"def plot(self, model=None, plot_col='Open', max_subplots=3):\n  inputs, labels = self.example\n  plt.figure(figsize=(15, 10))\n  plot_col_index = self.column_indices[plot_col]\n  max_n = min(max_subplots, len(inputs))\n  for n in range(max_n):\n    plt.subplot(3, 1, n+1)\n    plt.ylabel(f'{plot_col} [normed]')\n    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n             label='Inputs', marker='.', zorder=-10)\n\n    if self.label_columns:\n      label_col_index = self.label_columns_indices.get(plot_col, None)\n    else:\n      label_col_index = plot_col_index\n\n    if label_col_index is None:\n      continue\n\n    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n    if model is not None:\n      predictions = model(inputs)\n      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                  marker='X', edgecolors='k', label='Predictions',\n                  c='#ff7f0e', s=64)\n\n    if n == 0:\n      plt.legend()\n\n  plt.xlabel('Time [day]')\n\nWindowGenerator.plot = plot","73907687":"w2.plot()","77025d46":"w2.plot(plot_col='Volume')","6ae8f3b6":"def make_dataset(self, data):\n  data = np.array(data, dtype=np.float32)\n  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n      data=data,\n      targets=None,\n      sequence_length=self.total_window_size,\n      sequence_stride=1,\n      shuffle=True,\n      batch_size=32,)   #batch_size=32\n\n  ds = ds.map(self.split_window)\n\n  return ds\n\nWindowGenerator.make_dataset = make_dataset","f1e2a776":"@property\ndef train(self):\n  return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n  return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n  return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n  #result = getattr(self, '_example', None)\n  #########################################\n  result = next(iter(self.test))\n  if result is None:\n    # No example batch was found, so get one from the `.train` dataset\n    print(\" #### No example batch was found, so get one from the `.train` dataset ####\")\n    #########################################\n    #result = next(iter(self.train))\n    result = next(iter(self.test))\n    # And cache it for next time\n    self._example = result\n  return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example","c327a65a":"# Each element is an (inputs, label) pair\n#w2.train.element_spec\n#############################################\nw2.test.element_spec","4724a650":"w2.plot()","0b64f1e8":"#for example_inputs, example_labels in w2.train.take(1):\n#############################################\nfor example_inputs, example_labels in w2.test.take(1):\n  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n  print(f'Labels shape (batch, time, features): {example_labels.shape}')","9ed3a2a7":"w2.plot()","2b30e1d7":"w2.plot()","2834304c":"single_step_window = WindowGenerator(\n    input_width=1, label_width=1, shift=1,\n    label_columns=['Open'])\nsingle_step_window","90e4faf7":"#for example_inputs, example_labels in single_step_window.train.take(1):\n#############################################\nfor example_inputs, example_labels in single_step_window.test.take(1):\n  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n  print(f'Labels shape (batch, time, features): {example_labels.shape}')","d3f744a0":"#Creat a funtion to plot training loss\ndef plotLoss():\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    mean_absolute_error = history.history['mean_absolute_error']\n\n    epochs = range(1, len(loss) + 1)\n\n    plt.plot(epochs, loss, label='Training loss')\n    plt.plot(epochs, val_loss, label='Validation loss')\n    plt.plot(epochs, mean_absolute_error, label='mean_absolute_error')\n    plt.title('Training and validation loss')\n    plt.legend()\n\n    plt.show()","b9d87cf4":"class Baseline(tf.keras.Model):\n  def __init__(self, label_index=None):\n    super().__init__()\n    self.label_index = label_index\n\n  def call(self, inputs):\n    if self.label_index is None:\n      return inputs\n    result = inputs[:, :, self.label_index]\n    return result[:, :, tf.newaxis]","ce337c1e":"baseline = Baseline(label_index=column_indices['Open'])\n\nbaseline.compile(loss=tf.losses.MeanSquaredError(),\n                 metrics=[tf.metrics.MeanAbsoluteError()])\n\nval_performance = {}\nperformance = {}\nval_performance['Baseline'] = baseline.evaluate(single_step_window.val)\nperformance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0)","91b3cb06":"wide_window = WindowGenerator(\n    input_width=40, label_width=40, shift=1,\n    label_columns=['Open'])\n\nwide_window","4a7a15cd":"print('Input shape:', wide_window.example[0].shape)\nprint('Output shape:', baseline(wide_window.example[0]).shape)","989c124a":"wide_window.plot(baseline)","bb41c738":"linear = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=1)\n])","00f89e9e":"print('Input shape:', single_step_window.example[0].shape)\nprint('Output shape:', linear(single_step_window.example[0]).shape)","0f6bf0f4":"# To gain computation time y limited the MAX_EPOCHS.\n# You can try biger value but take care to overfiting :p\nMAX_EPOCHS = 1000 #4000\n\ndef compile_and_fit(model, window, patience=4000):  #40\n  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n  model.compile(loss=tf.losses.MeanSquaredError(),\n                optimizer=tf.optimizers.Adam(),  #learning_rate=0.001\n                metrics=[tf.metrics.MeanAbsoluteError()])\n\n  history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n  return history","219e3356":"MAX_EPOCHS = 1000\nhistory = compile_and_fit(linear, single_step_window)\n\nIPython.display.clear_output()\n\nval_performance['Linear'] = linear.evaluate(single_step_window.val)\nperformance['Linear'] = linear.evaluate(single_step_window.test, verbose=0)","7fa1479d":"plotLoss()","e5a5b838":"print('Input shape:', wide_window.example[0].shape)\nprint('Output shape:', baseline(wide_window.example[0]).shape)","541045d5":"wide_window.plot(linear)","781e1017":"linear.summary()  ###","750776b5":"plt.bar(x = range(len(train_df.columns)),\n        height=linear.layers[0].kernel[:,0].numpy())\naxis = plt.gca()\naxis.set_xticks(range(len(train_df.columns)))\n_ = axis.set_xticklabels(train_df.columns, rotation=90)","0ab73714":"from tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\n\n\ndense = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=10),\n    tf.keras.layers.Dense(units=10),\n    tf.keras.layers.Dense(units=5),\n    tf.keras.layers.Dense(units=1),\n])\n","559e1aba":"MAX_EPOCHS = 200\nhistory = compile_and_fit(dense, single_step_window)\n\nIPython.display.clear_output()\n\nval_performance['Dense'] = dense.evaluate(single_step_window.val)\nperformance['Dense'] = dense.evaluate(single_step_window.test, verbose=0)","d1c8f10a":"plotLoss()","c85c3d95":"wide_window.plot(dense)","94ec1376":"dense.summary()","a598e5df":"CONV_WIDTH = 10\nconv_window = WindowGenerator(\n    input_width=CONV_WIDTH,\n    label_width=1,\n    shift=1,\n    label_columns=['Open'])\n\nconv_window","4954ea9a":"conv_window.plot()\nplt.title(\"Given 10 days inputs, predict on day into the future.\")","7775db3b":"multi_step_dense = tf.keras.Sequential([\n    # Shape: (time, features) => (time*features)\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=1),\n    # Add back the time dimension.\n    # Shape: (outputs) => (1, outputs)\n    tf.keras.layers.Reshape([1, -1]),\n])","281cf112":"print('Input shape:', conv_window.example[0].shape)\nprint('Output shape:', multi_step_dense(conv_window.example[0]).shape)","2ad8057b":"MAX_EPOCHS = 300\nhistory = compile_and_fit(multi_step_dense, conv_window)\n\nIPython.display.clear_output()\nval_performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.val)\nperformance['Multi step dense'] = multi_step_dense.evaluate(conv_window.test, verbose=0)","2ad54f6a":"plotLoss()","66e6e092":"multi_step_dense.summary()","f45bc32e":"conv_window.plot(multi_step_dense)","728cf112":"print('Input shape:', wide_window.example[0].shape)\ntry:\n  print('Output shape:', multi_step_dense(wide_window.example[0]).shape)\nexcept Exception as e:\n  print(f'\\n{type(e).__name__}:{e}')","926a7363":"conv_model = tf.keras.Sequential([\n    tf.keras.layers.Conv1D(filters=8,\n                           kernel_size=(CONV_WIDTH,)),\n    tf.keras.layers.Dense(units=1),\n])","31e4b394":"print(\"Conv model on `conv_window`\")\nprint('Input shape:', conv_window.example[0].shape)\nprint('Output shape:', conv_model(conv_window.example[0]).shape)","949eeaaf":"MAX_EPOCHS = 50\nhistory = compile_and_fit(conv_model, conv_window)\n\nIPython.display.clear_output()\nval_performance['Conv'] = conv_model.evaluate(conv_window.val)\nperformance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)","581a31cd":"plotLoss()","e3e525b8":"conv_window.plot(conv_model)","f49ac5fd":"conv_model.summary()","d4ab0b2d":"wide_window = WindowGenerator(\n    input_width=60, label_width=60, shift=1,\n    label_columns=['Open'])\n\nwide_window","dac30505":"print(\"Wide window\")\nprint('Input shape:', wide_window.example[0].shape)\nprint('Labels shape:', wide_window.example[1].shape)\nprint('Output shape:', conv_model(wide_window.example[0]).shape)","2d03431d":"LABEL_WIDTH = 20\nINPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1)\nwide_conv_window = WindowGenerator(\n    input_width=INPUT_WIDTH,\n    label_width=LABEL_WIDTH,\n    shift=1,\n    label_columns=['Open'])\n\nwide_conv_window","aa083fe1":"print(\"Wide conv window\")\nprint('Input shape:', wide_conv_window.example[0].shape)\nprint('Labels shape:', wide_conv_window.example[1].shape)\nprint('Output shape:', conv_model(wide_conv_window.example[0]).shape)","57483709":"wide_conv_window.plot(conv_model)","b94f0505":"lstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] => [batch, time, lstm_units]\n    tf.keras.layers.LSTM(1, return_sequences=True),\n    # Shape => [batch, time, features]\n    tf.keras.layers.Dense(units=5),\n    tf.keras.layers.Dense(units=1)\n])","a147cf3a":"print('Input shape:', wide_window.example[0].shape)\nprint('Output shape:', lstm_model(wide_window.example[0]).shape)","bb509a6a":"MAX_EPOCHS = 250\nhistory = compile_and_fit(lstm_model, wide_window)\n\nIPython.display.clear_output()\nval_performance['LSTM'] = lstm_model.evaluate(wide_window.val)\nperformance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)","c44e4f9d":"plotLoss()","5c536a4d":"wide_window.plot(lstm_model)","11973211":"lstm_model.summary()","cfb91684":"x = np.arange(len(performance))\nwidth = 0.3\nmetric_name = 'mean_absolute_error'\nmetric_index = lstm_model.metrics_names.index('mean_absolute_error')\nval_mae = [v[metric_index] for v in val_performance.values()]\ntest_mae = [v[metric_index] for v in performance.values()]\n\nplt.ylabel('mean_absolute_error [Open, normalized]')\nplt.bar(x - 0.17, val_mae, width, label='Validation')\nplt.bar(x + 0.17, test_mae, width, label='Test')\nplt.xticks(ticks=x, labels=performance.keys(),\n           rotation=45)\n_ = plt.legend()","311bd06b":"for name, value in performance.items():\n  print(f'{name:12s}: {value[1]:0.4f}')","586da863":"single_step_window = WindowGenerator(\n    # `WindowGenerator` returns all features as labels if you \n    # don't set the `label_columns` argument.\n    input_width=1, label_width=1, shift=1)\n\nwide_window = WindowGenerator(\n    input_width=24, label_width=24, shift=1)\n\nfor example_inputs, example_labels in wide_window.train.take(1):\n  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n  print(f'Labels shape (batch, time, features): {example_labels.shape}')","d40c966b":"baseline = Baseline()\nbaseline.compile(loss=tf.losses.MeanSquaredError(),\n                 metrics=[tf.metrics.MeanAbsoluteError()])","9f30b8c0":"val_performance = {}\nperformance = {}\nval_performance['Baseline'] = baseline.evaluate(wide_window.val)\nperformance['Baseline'] = baseline.evaluate(wide_window.test, verbose=0)\n","bdcb6979":"dense = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=10),\n    tf.keras.layers.Dense(units=10),\n    tf.keras.layers.Dense(units=5),\n    tf.keras.layers.Dense(units=1),\n    tf.keras.layers.Dense(units=num_features)\n])","da63aced":"MAX_EPOCHS = 50\n\nhistory = compile_and_fit(dense, single_step_window)\n\nIPython.display.clear_output()\nval_performance['Dense'] = dense.evaluate(single_step_window.val)\nperformance['Dense'] = dense.evaluate(single_step_window.test, verbose=0)\n\nplotLoss()","964bcbb0":"%%time\nwide_window = WindowGenerator(\n    input_width=24, label_width=24, shift=1)\n\nlstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] => [batch, time, lstm_units]\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    # Shape => [batch, time, features]\n    tf.keras.layers.Dense(units=num_features)\n])\n\nMAX_EPOCHS = 20\n\nhistory = compile_and_fit(lstm_model, wide_window)\n\nIPython.display.clear_output()\nval_performance['LSTM'] = lstm_model.evaluate( wide_window.val)\nperformance['LSTM'] = lstm_model.evaluate( wide_window.test, verbose=0)\n\nplotLoss()\n\nprint()","c06e940f":"class ResidualWrapper(tf.keras.Model):\n  def __init__(self, model):\n    super().__init__()\n    self.model = model\n\n  def call(self, inputs, *args, **kwargs):\n    delta = self.model(inputs, *args, **kwargs)\n\n    # The prediction for each timestep is the input\n    # from the previous time step plus the delta\n    # calculated by the model.\n    return inputs + delta","9d0fa547":"%%time\nresidual_lstm = ResidualWrapper(\n    tf.keras.Sequential([\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    tf.keras.layers.Dense(\n        num_features,\n        # The predicted deltas should start small\n        # So initialize the output layer with zeros\n        kernel_initializer=tf.initializers.zeros)\n]))\n\nMAX_EPOCHS = 20\nhistory = compile_and_fit(residual_lstm, wide_window)\n\nIPython.display.clear_output()\nval_performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.val)\nperformance['Residual LSTM'] = residual_lstm.evaluate(wide_window.test, verbose=0)\n\nplotLoss()\n\nprint()","7f219886":"x = np.arange(len(performance))\nwidth = 0.3\n\nmetric_name = 'mean_absolute_error'\nmetric_index = lstm_model.metrics_names.index('mean_absolute_error')\nval_mae = [v[metric_index] for v in val_performance.values()]\ntest_mae = [v[metric_index] for v in performance.values()]\n\nplt.bar(x - 0.17, val_mae, width, label='Validation')\nplt.bar(x + 0.17, test_mae, width, label='Test')\nplt.xticks(ticks=x, labels=performance.keys(),\n           rotation=45)\nplt.ylabel('MAE (average over all outputs)')\n_ = plt.legend()","c5200512":"for name, value in performance.items():\n  print(f'{name:15s}: {value[1]:0.4f}')","9b70a6fd":"OUT_STEPS = 5\nIN_WIDTH = 10\n\nmulti_window = WindowGenerator(input_width=IN_WIDTH,\n                               label_width=OUT_STEPS,\n                               shift=OUT_STEPS)\n\nmulti_window.plot()\nmulti_window","25e9b72d":"class MultiStepLastBaseline(tf.keras.Model):\n  def call(self, inputs):\n    return tf.tile(inputs[:, -1:, :], [1, OUT_STEPS, 1])\n\nlast_baseline = MultiStepLastBaseline()\nlast_baseline.compile(loss=tf.losses.MeanSquaredError(),\n                      metrics=[tf.metrics.MeanAbsoluteError()])\n\nmulti_val_performance = {}\nmulti_performance = {}\n\nmulti_val_performance['Last'] = last_baseline.evaluate(multi_window.val)\nmulti_performance['Last'] = last_baseline.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(last_baseline)","6833118a":"class RepeatBaseline(tf.keras.Model):\n  def call(self, inputs):\n    return inputs\n\nrepeat_baseline = RepeatBaseline()\nrepeat_baseline.compile(loss=tf.losses.MeanSquaredError(),\n                        metrics=[tf.metrics.MeanAbsoluteError()])\n\nif OUT_STEPS == IN_WIDTH:\n    multi_val_performance['Repeat'] = repeat_baseline.evaluate(multi_window.val)\n    multi_performance['Repeat'] = repeat_baseline.evaluate(multi_window.test, verbose=0)\n\n    multi_window.plot(repeat_baseline)","e79987e2":"multi_linear_model = tf.keras.Sequential([\n    # Take the last time-step.\n    # Shape [batch, time, features] => [batch, 1, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n    # Shape => [batch, 1, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n","ceefcd35":"MAX_EPOCHS = 300\n\nhistory = compile_and_fit(multi_linear_model, multi_window)\n\nIPython.display.clear_output()\n\nplotLoss()\n\nmulti_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val)\nmulti_performance['Linear'] = multi_linear_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_linear_model)\n\nmulti_linear_model.summary()","e441d873":"multi_dense_model = tf.keras.Sequential([\n    # Take the last time step.\n    # Shape [batch, time, features] => [batch, 1, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n    # Shape => [batch, 1, dense_units]\n    tf.keras.layers.Dense(20),\n    # Shape => [batch, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nMAX_EPOCHS = 300\nhistory = compile_and_fit(multi_dense_model, multi_window)\n\nIPython.display.clear_output()\n\nplotLoss()\n\nmulti_val_performance['Dense'] = multi_dense_model.evaluate(multi_window.val)\nmulti_performance['Dense'] = multi_dense_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_dense_model)\n\nmulti_dense_model.summary()","debbaf0d":"CONV_WIDTH = 3\nmulti_conv_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n    # Shape => [batch, 1, conv_units]\n    tf.keras.layers.Conv1D(64, kernel_size=(CONV_WIDTH)),\n    # Shape => [batch, 1,  out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\n'''\nmulti_conv_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\n    # Shape => [batch, 1, conv_units]\n    tf.keras.layers.Conv1D(1024, activation='relu', kernel_size=(CONV_WIDTH)),\n    # Shape => [batch, 1,  out_steps*features]\n    tf.keras.layers.Dense(2048, activation='relu'),\n    tf.keras.layers.Dense(2048, activation='relu'),\n    tf.keras.layers.Dense(2048, activation='relu'),\n    tf.keras.layers.Dense(2048, activation='relu'),\n    tf.keras.layers.Dense(512, activation='relu'),  \n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n'''\nMAX_EPOCHS = 100\nhistory = compile_and_fit(multi_conv_model, multi_window)\n\nIPython.display.clear_output()\n\nplotLoss()\n\nmulti_val_performance['Conv'] = multi_conv_model.evaluate(multi_window.val)\nmulti_performance['Conv'] = multi_conv_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_conv_model)\n\nmulti_conv_model.summary()","a098d982":"multi_lstm_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units]\n    # Adding more `lstm_units` just overfits more quickly.\n    tf.keras.layers.LSTM(64, return_sequences=False),\n    # Shape => [batch, out_steps*features]\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros),\n    # Shape => [batch, out_steps, features]\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\nMAX_EPOCHS = 30\nhistory = compile_and_fit(multi_lstm_model, multi_window)\n\nIPython.display.clear_output()\n\nplotLoss()\n\nmulti_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val)\nmulti_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_lstm_model)\n\nmulti_lstm_model.summary()","8ab2199e":"class FeedBack(tf.keras.Model):\n  def __init__(self, units, out_steps):\n    super().__init__()\n    self.out_steps = out_steps\n    self.units = units\n    self.lstm_cell = tf.keras.layers.LSTMCell(units)\n    # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n    self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n    self.dense = tf.keras.layers.Dense(num_features)","3edbeb33":"feedback_model = FeedBack(units=32, out_steps=OUT_STEPS)","95788b6f":"def warmup(self, inputs):\n  # inputs.shape => (batch, time, features)\n  # x.shape => (batch, lstm_units)\n  x, *state = self.lstm_rnn(inputs)\n\n  # predictions.shape => (batch, features)\n  prediction = self.dense(x)\n  return prediction, state\n\nFeedBack.warmup = warmup","ebf5555a":"prediction, state = feedback_model.warmup(multi_window.example[0])\nprediction.shape","d7bdebeb":"def call(self, inputs, training=None):\n  # Use a TensorArray to capture dynamically unrolled outputs.\n  predictions = []\n  # Initialize the lstm state\n  prediction, state = self.warmup(inputs)\n\n  # Insert the first prediction\n  predictions.append(prediction)\n\n  # Run the rest of the prediction steps\n  for n in range(1, self.out_steps):\n    # Use the last prediction as input.\n    x = prediction\n    # Execute one lstm step.\n    x, state = self.lstm_cell(x, states=state,\n                              training=training)\n    # Convert the lstm output to a prediction.\n    prediction = self.dense(x)\n    # Add the prediction to the output\n    predictions.append(prediction)\n\n  # predictions.shape => (time, batch, features)\n  predictions = tf.stack(predictions)\n  # predictions.shape => (batch, time, features)\n  predictions = tf.transpose(predictions, [1, 0, 2])\n  return predictions\n\nFeedBack.call = call","487a0699":"print('Output shape (batch, time, features): ', feedback_model(multi_window.example[0]).shape)","ae3f4e89":"MAX_EPOCHS = 50\nhistory = compile_and_fit(feedback_model, multi_window)\n\nIPython.display.clear_output()\n\nplotLoss()\n\nmulti_val_performance['AR LSTM'] = feedback_model.evaluate(multi_window.val)\nmulti_performance['AR LSTM'] = feedback_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(feedback_model)\n\nfeedback_model.summary()","e5523c89":"x = np.arange(len(multi_performance))\nwidth = 0.3\n\n\nmetric_name = 'mean_absolute_error'\nmetric_index = lstm_model.metrics_names.index('mean_absolute_error')\nval_mae = [v[metric_index] for v in multi_val_performance.values()]\ntest_mae = [v[metric_index] for v in multi_performance.values()]\n\nplt.bar(x - 0.17, val_mae, width, label='Validation')\nplt.bar(x + 0.17, test_mae, width, label='Test')\nplt.xticks(ticks=x, labels=multi_performance.keys(),\n           rotation=45)\nplt.ylabel(f'MAE (average over all times and outputs)')\n_ = plt.legend()","820a0515":"for name, value in multi_performance.items():\n  print(f'{name:8s}: {value[1]:0.4f}')","5ce270fb":"This expanded window can be passed directly to the same `baseline` model without any code changes. This is possible because the inputs and labels have the same number of timesteps, and the baseline just forwards the input to the output:\n\n  ![One prediction 1h into the future, ever hour.](images\/last_window.png)","50e83acb":"<a id=\"residual\"><\/a>\n\n#### Advanced: Residual connections\n\nThe `Baseline` model from earlier took advantage of the fact that the sequence doesn't change drastically from time step to time step. Every model trained in this tutorial so far was randomly initialized, and then had to learn that the output is a a small change from the previous time step.\n\nWhile you can get around this issue with careful initialization, it's  simpler to build this into the model structure.\n\nIt's common in time series analysis to build models that instead of predicting the next value, predict how the value will change in the next timestep.\nSimilarly, \"Residual networks\" or \"ResNets\" in deep learning refer to architectures where each layer adds to the model's accumulating result.\n\nThat is how you take advantage of the knowledge that the change should be small.\n\n![A model with a residual connection](images\/residual.png)\n\nEssentially this initializes the model to match the `Baseline`. For this task it helps models converge faster, with slightly better performance.","d18f700a":"A recurrent model can learn to use a long history of inputs, if it's relevant to the predictions the model is making. Here the model will accumulate internal state for 24h, before making a single prediction for the next 24h.\n\nIn this single-shot format, the LSTM only needs to produce an output at the last time step, so set `return_sequences=False`.\n\n![The lstm accumulates state over the input window, and makes a single prediction for the next 24h](images\/multistep_lstm.png)\n","38c2d55c":"Note: Stacking a python list like this only works with eager-execution, using `Model.compile(..., run_eagerly=True)` for training, or with a fixed length output. For a dynamic output length you would need to use a `tf.TensorArray` instead of a python list, and `tf.range` instead of the python `range`.","55008164":"Like the `baseline` model, the linear model can be called on batches of wide windows. Used this way the model makes a set of independent predictions on consecutive time steps. The `time` axis acts like another `batch` axis. There are no interactions between the predictions at each time step.\n\n![A single step prediction](images\/wide_window.png)","eea0c29b":"A convolutional model makes predictions based on a fixed-width history, which may lead to better performance than the dense model since it can see how things are changing over time:\n\n![A convolutional model sees how things change over time](images\/multistep_conv.png)","5de7f524":"### Advanced: Autoregressive model\n\nThe above models all predict the entire output sequence in a single step.\n\nIn some cases it may be helpful for the model to decompose this prediction into individual time steps. Then each model's output can be fed back into itself at each step and predictions can be made conditioned on the previous one, like in the classic [Generating Sequences With Recurrent Neural Networks](https:\/\/arxiv.org\/abs\/1308.0850).\n\nOne clear advantage to this style of model is that it can be set up to produce output with a varying length.\n\nYou could take any of the single-step multi-output models trained in the first half of this tutorial and run  in an autoregressive feedback loop, but here you'll focus on building a model that's been explicitly trained to do that.\n\n![Feedback a model's output to its input](images\/multistep_autoregressive.png)\n","369d0a43":"Test run this model on the example inputs:","8bb4ecf1":"The convolutional models in the next section fix this problem.","c9f2bf1e":"Try it out:","21b7fc52":"You could train a `dense` model on a multiple-input-step window by adding a `layers.Flatten` as the first layer of the model:","2a1ad9b3":"## Data windowing\n\nThe models in this tutorial will make a set of predictions based on a window of consecutive samples from the data. \n\nThe main features of the input windows are:\n\n* The width (number of time steps) of the input and label windows\n* The time offset between them.\n* Which features are used as inputs, labels, or both. \n\nThis tutorial builds a variety of models (including Linear, DNN, CNN and RNN models), and uses them for both:\n\n* *Single-output*, and *multi-output* predictions.\n* *Single-time-step* and *multi-time-step* predictions.\n\nThis section focuses on implementing the data windowing so that it can be reused for all of those models.\n","b96b567e":"But this will be easier for the model to interpret if you convert the MIN \/ MAX columns to AMPLITUDE:","06870ed6":"With the `RNN`'s state, and an initial prediction you can now continue iterating the model feeding the predictions at each step back as the input.\n\nThe simplest approach to collecting the output predictions is to use a python list, and `tf.stack` after the loop.","793fc27e":"#### Linear\n\nA simple linear model based on the last input time step does better than either baseline, but is underpowered. The model needs to predict `OUTPUT_STEPS` time steps, from a single input time step with a linear projection. It can only capture a low-dimensional slice of the behavior, likely based mainly on the time of day and time of year.\n\n![Predct all timesteps from the last time-step](images\/multistep_dense.png)","29755189":"Iterating over a `Dataset` yields concrete batches:","4cac5cc3":"### 4. Create `tf.data.Dataset`s","8fcc4f1e":"#### RNN","0f65b499":"This gives the model access to the most important frequency features. In this case you knew ahead of time which frequencies were important. \n\nIf you didn't know, you can determine which frequencies are important using an `fft`. To check our assumptions, here is the `tf.signal.rfft` of the temperature over time. Note the obvious peaks at frequencies near `1\/year` and `1\/day`: ","0a4d4e93":"### Baseline\n\nBefore building a trainable model it would be good to have a performance baseline as a point for comparison with the later more complicated models.\n\nThis first task is to predict temperature 1h in the future given the current value of all features. The current values include the current temperature. \n\nSo start with a model that just returns the current temperature as the prediction, predicting \"No change\". This is a reasonable baseline since temperature changes slowly. Of course, this baseline will work less well if you make a prediction further in the future.\n\n![Send the input to the output](images\/baseline.png)","6b9ece36":"There are clearly diminishing returns as a function of model complexity on this problem.","d00d1ae1":"### 3. Plot\n\nHere is a plot method that allows a simple visualization of the split window:","d1e6e7ca":"This tutorial trains many models, so package the training procedure into a function:","b1e48f53":"With this dataset typically each of the models does slightly better than the one before it.","16bccc35":"### Linear model\n\nThe simplest **trainable** model you can apply to this task is to insert linear transformation between the input and output. In this case the output from a time step only depends on that step:\n\n![A single step prediction](images\/narrow_window.png)\n\nA `layers.Dense` with no `activation` set is a linear model. The layer only transforms the last axis of the data from `(batch, time, inputs)` to `(batch, time, units)`, it is applied independently to every item across the `batch` and `time` axes.","3df06084":"Now you can plot the model's predictions on a wider window. Note the 3 input time steps before the first prediction. Every prediction here is based on the 3 preceding timesteps:","c79aede8":"Note above that the `features` axis of the labels now has the same depth as the inputs, instead of 1.","eb227f04":"The `baseline`, `linear` and `dense` models handled each time step independently. Here the model will take multiple time steps as input to produce a single output.\n\nCreate a `WindowGenerator` that will produce batches of the 3h of inputs and, 1h of labels:","a47ca0c1":"### Performance","c5c844c6":"This plot aligns inputs, labels, and (later) predictions based on the time that the item refers to:","982f5b2a":"Plotting the baseline model's predictions you can see that it is simply the labels, shifted right by 1h.","4291c11e":"Now peek at the distribution of the features. Some features do have long tails, but there are no obvious errors like the `-9999` wind velocity value.","30a70310":"### Split the data","21d5da8f":"## Data error correction : \n####    -> Volume min value = 0 , it's an error ?  \n####       Not so easy to correct, we can try to remplace this 0 by the mean of the curve :\n\n\n","962cbc16":"### Normalize the data\n\nIt is important to scale features before training a neural network. Normalization is a common way of doing this scaling. Subtract the mean and divide by the standard deviation of each feature.","d8f62b06":"# FrenchNews : CAC40 prediction with deep learning and news sentiment analysis\n##### Go to the link below for the original tutorial file modified for stocks prediction.\nhttps:\/\/www.tensorflow.org\/tutorials\/structured_data\/time_series\n\nNote : I\u2019v correct the this tutorial to make it plot the validation data and not the train data.\n\nSome models don't converge, i think it's because there is a lot of nose in the stocks market...\n\n","5e24419a":"The `window` object creates `tf.data.Datasets` from the training, validation, and test sets, allowing you to easily iterate over batches of data.\n","676b27b7":"The time in seconds is not a useful model input. The stock market could maybe have year, month and week periodicity... \n\nThere are many ways you could deal with periodicity.\n\nA simple approach to convert it to a usable signal is to use `sin` and `cos` to convert the time to clear \"Time of day\" and \"Time of year\" signals:","22c95a88":"#### Baseline\n\nThe same baseline model can be used here, but this time repeating all features instead of selecting a specific `label_index`.","422c8a4d":"Train and evaluate it on the ` conv_window` and it should give performance similar to the `multi_step_dense` model.","3d0bf530":"Finally this `make_dataset` method will take a time series `DataFrame` and convert it to a `tf.data.Dataset` of `(input_window, label_window)` pairs using the `preprocessing.timeseries_dataset_from_array` function.","c9f84f8c":"Next look at the statistics of the dataset:","983a29e3":"Sometimes the model doesn't even place the most weight on the input `T (degC)`. This is one of the risks of random initialization. ","fa8ca62a":"#### Time","0a3fec66":"### Single-shot models\n\nOne high level approach to this problem is use a \"single-shot\" model, where the model makes the entire sequence prediction in a single step.\n\nThis can be implemented efficiently as a `layers.Dense` with `OUT_STEPS*features` output units. The model just needs to reshape that output to the required `(OUTPUT_STEPS, features)`.","17c2dc6e":"### Multi-step dense\n\nA single-time-step model has no context for the current values of its inputs. It can't see how the input features are changing over time. To address this issue the model needs access to multiple time steps when making predictions:\n\n![Three time steps are used for each prediction.](images\/conv_window.png)\n","33e878a0":"### 1. Indexes and offsets\n\nStart by creating the `WindowGenerator` class. The `__init__` method includes all the necessary logic for the input and label indices.\n\nIt also takes the train, eval, and test dataframes as input. These will be converted to `tf.data.Dataset`s of windows later.","baa27557":"The rest of this section defines a `WindowGenerator` class. This class can:\n\n1. Handle the indexes and offsets as shown in the diagrams above.\n1. Split windows of features into a `(features, labels)` pairs.\n2. Plot the content of the resulting windows.\n3. Efficiently generate batches of these windows from the training, evaluation, and test data, using `tf.data.Dataset`s.","58c38942":"Run it on an example batch to see that the model produces outputs with the expected shape:","da6a8605":"## Next steps\n\nThis tutorial was a quick introduction to time series forecasting using TensorFlow.\n\n* For further understanding, see:\n  * Chapter 15 of [Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/), 2nd Edition \n  * Chapter 6 of [Deep Learning with Python](https:\/\/www.manning.com\/books\/deep-learning-with-python).\n  * Lesson 8 of [Udacity's intro to TensorFlow for deep learning](https:\/\/www.udacity.com\/course\/intro-to-tensorflow-for-deep-learning--ud187), and the [exercise notebooks](https:\/\/github.com\/tensorflow\/examples\/tree\/master\/courses\/udacity_intro_to_tensorflow_for_deep_learning) \n* Also remember that you can implement any [classical time series model](https:\/\/otexts.com\/fpp2\/index.html) in TensorFlow, this tutorial just focuses on TensorFlow's built-in functionality.","55ff6a82":"An important constructor argument for all keras RNN layers is the `return_sequences` argument. This setting can configure the layer in one of two ways.\n\n1. If `False`, the default, the layer only returns the output of the final timestep, giving the model time to warm up its internal state before making a single prediction: \n\n![An lstm warming up and making a single prediction](images\/lstm_1_window.png)\n\n2. If `True` the layer returns an output for each input. This is useful for:\n  * Stacking RNN layers. \n  * Training a model on multiple timesteps simultaneously.\n\n![An lstm making a prediction after every timestep](images\/lstm_many_window.png)","3acef8ce":"\n#### Dense","ea4c9d68":"#### RNN\n\nThis tutorial only builds an autoregressive RNN model, but this pattern could be applied to any model that was designed to output a single timestep.\n\nThe model will have the same basic form as the single-step `LSTM` models: An `LSTM` followed by a `layers.Dense` that converts the `LSTM` outputs to model predictions.\n\nA `layers.LSTM` is a `layers.LSTMCell` wrapped in the higher level `layers.RNN` that manages the state and sequence results for you (See [Keras RNNs](https:\/\/www.tensorflow.org\/guide\/keras\/rnn) for details).\n\nIn this case the model has to manually manage the inputs for each step so it uses `layers.LSTMCell` directly for the lower level, single time step interface.","a4b58dac":"The above performances are averaged across all model outputs.","b2fcac91":"The main down-side of this approach is that the resulting model can only be executed on input windows of exactly this shape. ","e9be026b":"The gains achieved going from a dense model to convolutional and recurrent models are only a few percent (if any), and the autoregressive model performed clearly worse. So these more complex approaches may not be worth while on **this** problem, but there was no way to know without trying, and these models could be helpful for **your** problem.","b11bc31d":"We'll use a `(70%, 20%, 10%)` split for the training, validation, and test sets. Note the data is **not** being randomly shuffled before splitting. This is for two reasons.\n\n1. It ensures that chopping the data into windows of consecutive samples is still possible.\n2. It ensures that the validation\/test results are more realistic, being evaluated on data collected after the model was trained.","50a3e380":"That printed some performance metrics, but those don't give you a feeling for how well the model is doing.\n\nThe `WindowGenerator` has a plot method, but the plots won't be very interesting with only a single sample. So, create a wider `WindowGenerator` that generates windows 24h of consecutive inputs and labels at a time. \n\nThe `wide_window` doesn't change the way the model operates. The model still makes predictions 1h into the future based on a single input time step. Here the `time` axis acts like the `batch` axis: Each prediction is made independently with no interaction between time steps.","70c4b937":"The first method this model needs is a `warmup` method to initialize its internal state based on the inputs. Once trained this state will capture the relevant parts of the input history. This is equivalent to the single-step `LSTM` model from earlier:","352b1abc":"The mean and standard deviation should only be computed using the training data so that the models have no access to the values in the validation and test sets.\n\nIt's also arguable that the model shouldn't have access to future values in the training set when training, and that this normalization should be done using moving averages. That's not the focus of this tutorial, and the validation and test sets ensure that you get (somewhat) honest metrics. So in the interest of simplicity this tutorial uses a simple average.","2119b41c":"### Feature engineering\n\nBefore diving in to build a model it's important to understand your data, and be sure that you're passing the model appropriately formatted data.","ebd550e8":"With `return_sequences=True` the model can be trained on 24h of data at a time.\n\nNote: This will give a pessimistic view of the model's performance. On the first timestep the model has no access to previous steps, and so can't do any better than the simple `linear` and `dense` models shown earlier.","e63c024e":"Here is the plot of its example predictions on the `wide_window`, note how in many cases the prediction is clearly better than just returning the input temperature, but in a few cases it's worse:","d5c0c224":"Here is the overall performance for these multi-output models.","ebe7e946":"A simple baseline for this task is to repeat the last input time step for the required number of output timesteps:\n\n![Repeat the last input, for each output step](images\/multistep_last.png)","9d1f7a6e":"Note that the output is shorter than the input. To make training or plotting work, you need the labels, and prediction to have the same length. So build a `WindowGenerator` to produce wide windows with a few extra input time steps so the label and prediction lengths match: ","f84d415c":"Now the `WindowGenerator` object gives you access to the `tf.data.Dataset` objects, so you can easily iterate over the data.\n\nThe `Dataset.element_spec` property tells you the structure, `dtypes` and shapes of the dataset elements.","a367fdb7":"Since this task is to predict 24h given 24h another simple approach is to repeat the previous day, assuming tomorrow will be similar:\n\n![Repeat the previous day](images\/multistep_repeat.png)","b51d15ec":"Note that the `Window`'s `shift` parameter is relative to the end of the two windows.\n","e495fe15":"One advantage to linear models is that they're relatively simple to  interpret.\nYou can pull out the layer's weights, and see the weight assigned to each input:","7f8ed040":"### Recurrent neural network\n\nA Recurrent Neural Network (RNN) is a type of neural network well-suited to time series data. RNNs process a time series step-by-step, maintaining an internal state from time-step to time-step.\n\nFor more details, read the [text generation tutorial](https:\/\/www.tensorflow.org\/tutorials\/text\/text_generation) or the [RNN guide](https:\/\/www.tensorflow.org\/guide\/keras\/rnn). \n\nIn this tutorial, you will use an RNN layer called Long Short Term Memory ([LSTM](https:\/\/www.tensorflow.org\/versions\/r2.0\/api_docs\/python\/tf\/keras\/layers\/LSTM)).","c8ba9a52":"Here is code to create the 2 windows shown in the diagrams at the start of this section:","da7c3ca2":"#### RNN\n","bc295265":"Typically data in TensorFlow is packed into arrays where the outermost index is across examples (the \"batch\" dimension). The middle indices are the \"time\" or \"space\" (width, height) dimension(s). The innermost indices are the features.\n\nThe code above took a batch of 3, 7-timestep windows, with 19 features at each time step. It split them into a batch of 6-timestep, 19 feature inputs, and a 1-timestep 1-feature label. The label only has one feature because the `WindowGenerator` was initialized with `label_columns=['T (degC)']`. Initially this tutorial will build models that predict single output labels.","25541c58":"## Setup","8a7929f1":"The `WindowGenerator` object holds training, validation and test data. Add properties for accessing them as `tf.data.Datasets` using the above `make_dataset` method. Also add a standard example batch for easy access and plotting:","11e10daf":"### Convolution neural network\n \nA convolution layer (`layers.Conv1D`) also takes multiple time steps as input to each prediction.","ce32ca48":"You can plot the other columns, but the example window `w2` configuration only has labels for the `T (degC)` column.","f473884f":"Similarly the `Date Time` column is very useful, but not in this string form. Start by converting it to seconds:","1954020e":"Here is the evolution of a few features over time. ","c0a56e81":"This approach can be used in conjunction with any model discussed in this tutorial. \n\nHere it is being applied to the LSTM model, note the use of the `tf.initializers.zeros` to ensure that the initial predicted changes are small, and don't overpower the residual connection. There are no symmetry-breaking concerns for the gradients here, since the `zeros` are only used on the last layer.","99e31907":"#### CNN","5ce316ef":"### Performance","96d3ade5":"### Dense\n\nBefore applying models that actually operate on multiple time-steps, it's worth checking the performance of deeper, more powerful, single input step models.\n\nHere's a model similar to the `linear` model, except it stacks several a few `Dense` layers between the input and the output: ","4cd55c82":"The metrics for the multi-output models in the first half of this tutorial show the performance averaged across all output features. These performances similar but also averaged across output timesteps. ","c0bf8958":"##  dataset import\n\n","d7fa1f21":"This method returns a single time-step prediction, and the internal state of the LSTM:","a01bd1fd":"The difference between this `conv_model` and the `multi_step_dense` model is that the `conv_model` can be run on inputs of any length. The convolutional layer is applied to a sliding window of inputs:\n\n![Executing a convolutional model on a sequence](images\/wide_conv_window.png)\n\nIf you run it on wider input, it produces wider output:","256a7e04":"## Multi-step models\n\nBoth the single-output and multiple-output models in the previous sections made **single time step predictions**, 1h into the future.\n\nThis section looks at how to expand these models to make **multiple time step predictions**.\n\nIn a multi-step prediction, the model needs to learn to predict a range of future values. Thus, unlike a single step model, where only a single future point is predicted, a multi-step model predicts a sequence of the future values.\n\nThere are two rough approaches to this:\n\n1. Single shot predictions where the entire time series is predicted at once.\n2. Autoregressive predictions where the model only makes single step predictions and its output is fed back as its input.\n\nIn this section all the models will predict **all the features across all output time steps**.\n","c96100cf":"Depending on the task and type of model you may want to generate a variety of data windows. Here are some examples:\n\n1. For example, to make a single prediction 24h into the future, given 24h of history you might define a window like this:\n\n  ![One prediction 24h into the future.](images\/raw_window_24h.png)\n\n2. A model that makes a prediction 1h into the future, given 6h of history would need a window like this:\n\n  ![One prediction 1h into the future.](images\/raw_window_1h.png)","3b9d9bb9":"## Single step models\n\nThe simplest model you can build on this sort of data is one that predicts a single feature's value, 1 timestep (1h) in the future based only on the current conditions.\n\nSo start by building models to predict the `T (degC)` value 1h into the future.\n\n![Predict the next time step](images\/narrow_window.png)\n\nConfigure a `WindowGenerator` object to produce these single-step `(input, label)` pairs:","d7997e7f":"Train the model and evaluate its performance:","283769a9":"### Multi-output models\n\nThe models so far all predicted a single output feature, `T (degC)`, for a single time step.\n\nAll of these models can be converted to predict multiple features just by changing the number of units in the output layer and adjusting the training windows to include all features in the `labels`.\n","2af20530":"Let's take a glance at the data. Here are the first few rows:","def71038":"### Baselines","f8835502":"In the above plots of three examples the single step model is run over the course of 24h. This deserves some explanation:\n\n* The blue \"Inputs\" line shows the input temperature at each time step. The model recieves all features, this plot only shows the temperature.\n* The green \"Labels\" dots show the target prediction value. These dots are shown at the prediction time, not the input time. That is why the range of labels is shifted 1 step relative to the inputs.\n* The orange \"Predictions\" crosses are the model's prediction's for each output time step. If the model were predicting perfectly the predictions would land directly on the \"labels\".","d5a282c6":"### 2. Split\nGiven a list consecutive inputs, the `split_window` method will convert them to a window of inputs and a window of labels.\n\nThe example `w2`, above, will be split like this:\n\n![The initial window is all consecutive samples, this splits it into an (inputs, labels) pairs](images\/split_window.png)\n\nThis diagram doesn't show the `features` axis of the data, but this `split_window` function also handles the `label_columns` so it can be used for both the single output and multi-output examples.","e68eec13":"Now train the model:","171921d2":"For the multi-step model, the training data again consists of hourly samples. However, here, the models will learn to predict 24h of the future, given 24h of the past.\n\nHere is a `Window` object that generates these slices from the dataset:","6970457c":"Instantiate and evaluate this model:","689e3ffb":"#### Dense\n\nAdding a `layers.Dense` between the input and output gives the linear model more power, but is still only based on a single input timestep.","2d179861":"Below is the **same** model as `multi_step_dense`, re-written with a convolution. \n\nNote the changes:\n* The `layers.Flatten` and the first `layers.Dense` are replaced by a `layers.Conv1D`.\n* The `layers.Reshape` is no longer necessary since the convolution keeps the time axis in its output.","cc032cea":"#### Performance","00b8a18f":"### Inspect and cleanup"}}