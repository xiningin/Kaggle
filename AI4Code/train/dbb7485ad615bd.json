{"cell_type":{"1617255a":"code","20ff47a5":"code","dc085d9a":"code","82785a74":"code","f45c4378":"code","e2afe869":"code","bd267177":"code","6a2a7f4c":"code","deb69d5c":"code","c2a7f2e3":"code","20f08aac":"code","a3f29c1a":"code","94113848":"code","bfd41076":"code","047eedfe":"code","a16e6696":"code","7b3aa581":"code","bc3732f0":"code","049c27e3":"code","44c5b584":"code","c9ba7c93":"code","4e723d26":"code","938a40d3":"code","9cdfc446":"code","74c7da83":"code","1f7a3901":"code","4642aae1":"code","86b724cd":"code","de95fbea":"code","405a4602":"code","7e47beff":"code","a89e4130":"code","82df598b":"code","e33f9f8c":"code","155d7a48":"markdown","98562738":"markdown","c8c9c8bd":"markdown","bc4949b7":"markdown","77912443":"markdown","b224a7bb":"markdown","d55f1c39":"markdown","395f7ac2":"markdown","a238e7e1":"markdown","f3f007fc":"markdown","93001837":"markdown","b90995bc":"markdown","2b5c421b":"markdown","cfa50f29":"markdown","35781cdc":"markdown","ffeab13a":"markdown","c4968a12":"markdown","d74c86db":"markdown"},"source":{"1617255a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom category_encoders import MEstimateEncoder\nimport optuna","20ff47a5":"def read_data(data_dir):\n    train = pd.read_csv(os.path.join(data_dir, 'train.csv'), index_col='id')\n    test = pd.read_csv(os.path.join(data_dir, 'test.csv'), index_col='id')\n    sample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'), index_col='id')\n    return train, test, sample_submission","dc085d9a":"DAT_DIR = '..\/input\/tabular-playground-series-feb-2021'\ntrain, test, sample_submission = read_data(DAT_DIR)","82785a74":"disc_features = [train[c].dtype == 'object' for c in train.drop('target', axis=1).columns]\ncat_cols = train.select_dtypes(include='object')\ncat_cols","f45c4378":"for c in cat_cols:\n    enc = LabelEncoder()\n    train[c] = enc.fit_transform(train[c])\n    test[c] = enc.transform(test[c])","e2afe869":"X_train = train.drop('target', axis=1)\ny_train = train.target","bd267177":"xgb_params = {'max_depth': 7, \n              'learning_rate': 0.002368706913117573, \n              'n_estimators': 3842, \n              'min_child_weight': 4, \n              'colsample_bytree': 0.6612496396706031, \n              'subsample': 0.6060764549240347, \n              'reg_alpha': 0.18899174723187226, \n              'reg_lambda': 30.33470416661318}","6a2a7f4c":"def score_dataset(X, y, model=XGBRegressor(), cv_folds=2):\n    # Label encoding for categoricals\n    #\n    # Label encoding is good for XGBoost and RandomForest, but one-hot\n    # would be better for models like Lasso or Ridge. The `cat.codes`\n    # attribute holds the category levels.\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(\n        model, X, y, cv=cv_folds, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","deb69d5c":"def make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=disc_features) \n    mi_scores = pd.Series(mi_scores, name='MI Scores', index=X.columns)    \n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","c2a7f2e3":"%%time\n\nmi_scores = make_mi_scores(X_train, y_train, discrete_features = disc_features)","20f08aac":"mi_scores","a3f29c1a":"def create_pairwise_product(X):\n    X2 = pd.DataFrame()\n    for idx1, c1 in enumerate(X.columns):\n        for idx2, c2 in enumerate(X.columns):\n            if idx1 >= idx2: continue\n                \n            new_var = pd.Series(X[c1] * X[c2], name=f'{c1}-{c2}', index=X.index)\n            X2 = pd.concat([X2, new_var], axis=1)\n                \n    return X2","94113848":"pw_cols = ['cont8', 'cat1', 'cont0', 'cat9']\nX_train_pw = create_pairwise_product(X_train[pw_cols])\nX_train2 = X_train.join(X_train_pw)","bfd41076":"score = score_dataset(X_train2, y_train, model=XGBRegressor(**xgb_params), cv_folds=2)\nprint(f'RMSE: = {score:.4f}')","047eedfe":"pw_cols = [c for c in X_train.columns if mi_scores[c] > 0.005]\nX_train_pw = create_pairwise_product(X_train[pw_cols])\nX_train2 = X_train.join(X_train_pw)","a16e6696":"score = score_dataset(X_train2, y_train, model=XGBRegressor(**xgb_params), cv_folds=2)\nprint(f'RMSE: = {score:.4f}')","7b3aa581":"# Create PCA\n\npca = PCA()\nX_train_pca = pca.fit_transform(X_train)","bc3732f0":"component_names = [f'PC{i+1}' for i in range(X_train_pca.shape[1])]\nX_train_pca = pd.DataFrame(X_train_pca, columns=component_names)","049c27e3":"X_train_pca","44c5b584":"loadings = pd.DataFrame(pca.components_.T, columns=component_names, index=X_train.columns)","c9ba7c93":"loadings","4e723d26":"def plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1,2)\n    n = pca.n_components_\n    grid = np.arange(1, n+1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(xlabel='Component', title='% Explained Variance', ylim=(0.0, 1.0))\n    \n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0,cv], 'o-')\n    axs[1].set(xlabel='Component', title='% Cumulative Variance', ylim=(0.0, 1.0))\n    \n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","938a40d3":"_ = plot_variance(pca)","9cdfc446":"X_train2 = X_train.join(X_train_pca)","74c7da83":"score = score_dataset(X_train2, y_train, model=XGBRegressor(**xgb_params), cv_folds=2)\nprint(f'RMSE: = {score:.4f}')","1f7a3901":"X_train_cat_cnts = list(map(lambda c: X_train[c].value_counts(), cat_cols))","4642aae1":"X_train_cat_cnts = pd.DataFrame(X_train_cat_cnts).T","86b724cd":"X_train_cat_cnts","de95fbea":"# Encoding split\nX_train_enc = X_train.sample(frac=0.2, random_state=0)\ny_train_enc = y_train.loc[X_train_enc.index]\n\n# Training split\nX_train_res = X_train.drop(index=X_train_enc.index)\ny_train_res = y_train.drop(X_train_enc.index)","405a4602":"print(f'X_train_enc.shape = {X_train_enc.shape}, y_train_enc = {y_train_enc.shape}')\nprint(f'X_train_res.shape = {X_train_res.shape}, y_train_res = {y_train_res.shape}')","7e47beff":"enc = MEstimateEncoder(cols=['cat6', 'cat7', 'cat8', 'cat9'], m=1.0)\nenc.fit(X_train_enc, y_train_enc)\nX_train2 = enc.transform(X_train_res, y_train_res)","a89e4130":"X_train2","82df598b":"xgb_params = {'max_depth': 7, \n              'learning_rate': 0.002368706913117573, \n              'n_estimators': 3842, \n              'min_child_weight': 4, \n              'colsample_bytree': 0.6612496396706031, \n              'subsample': 0.6060764549240347, \n              'reg_alpha': 0.18899174723187226, \n              'reg_lambda': 30.33470416661318}","e33f9f8c":"score = score_dataset(X_train2, y_train_res, model=XGBRegressor(**xgb_params), cv_folds=2)\nprint(f'RMSE: = {score:.4f}')","155d7a48":"## EDA","98562738":"## Conclusions","c8c9c8bd":"We have tried a few common feature engineering approaches but none of them seem useful. In the next steps, we will survey the existing work to collect new ideas.","bc4949b7":"We also re-introduce utility functions to make model assessment easier.","77912443":"Next, we will try PCA transformation approach.","b224a7bb":"We now confirm that adding new PCA variables do not help.","d55f1c39":"The first 2 PCA components explain 80% of total variances. Next, let's check their MI scores.","395f7ac2":"Pairwise product variables do not seem to make much difference.","a238e7e1":"# Kaggle Tabular Feb Challenge - Feature Engineering","f3f007fc":"It seems the top 4 variables, i.e., cont8, cat1, cont0, and cat9, are more significant than others. Let us try creating products of these variables. ","93001837":"## Data Import","b90995bc":"It does not seem to make a difference. Now, let's try adding a little more variables.","2b5c421b":"It seems variables cat9 has many levels, followed by cat6, cat7, and cat8. Let's use them as candicates for target encoding. To avoid overfitting, we split the training data into encoding and rest.","cfa50f29":"### Target Encoding","35781cdc":"### PCA Transformation","ffeab13a":"In this notebook, I will explore various feature engineering techniques to improve the model performance.","c4968a12":"First let's examine the variable's influnce on the target.","d74c86db":"### Create pairwise product features"}}