{"cell_type":{"a64f0aed":"code","4cd0cbf3":"code","9af6bc91":"code","b4193017":"code","fb797ba4":"code","979490d5":"code","0fc4e2d7":"code","976e190f":"code","7e1ced42":"code","7543e796":"code","b8cca29d":"code","7ebcbc8b":"code","fcc3d437":"code","dd67f9b6":"code","aa499f6a":"code","0a2ff81e":"code","9126577d":"code","5d1886cc":"markdown","8f278aec":"markdown","37abec7c":"markdown","79a746e2":"markdown","75933962":"markdown","e89f0b2d":"markdown","47c5659e":"markdown","a855bd3b":"markdown","6cdfb37e":"markdown","8e281a42":"markdown","4c94919b":"markdown"},"source":{"a64f0aed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4cd0cbf3":"import pandas as pd\nAdmission_Predict = pd.read_csv(\"..\/input\/admission-prediction\/Admission_Predict.csv\")","9af6bc91":"df=pd.read_csv(\"..\/input\/admission-prediction\/Admission_Predict.csv\")\ndf.head()","b4193017":"df['Chance of admit class']=df['Chance of Admit '].apply(lambda x:1 if x>0.80 else 0)\ndf.head()","fb797ba4":"## Shape of data\nprint(' Shape of Data \\n Rows :',df.shape[0],', Columns : ',df.shape[1])","979490d5":"## Checking for null values\nmissing_values = df.isnull().sum() * 100\/len(df)\nmissing_values_df = pd.DataFrame({'Column_name':df.columns,'Missing_percent':missing_values})\nmissing_values_df","0fc4e2d7":"#iNFORMATION ABOUT THE DATA we can observe data doesnot have any Null value\ndf.info()","976e190f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\nimport seaborn as sns\nimport warnings\nfrom scipy import stats\nwarnings.filterwarnings('ignore')\n# lets see the distribution for the target variable\nprint('Skewness of chance of admit : ',df['Chance of Admit '].skew())\nplt.figure(figsize = (10,5))\nsns.distplot(df['Chance of Admit '],kde = True,color = 'g',fit = stats.norm)\nplt.show()","7e1ced42":"fig, ax = plt.subplots(figsize=(15,6))\nrs = stats.probplot(df['Chance of Admit '],plot = ax)\nplt.show()","7543e796":"from sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","b8cca29d":"features = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP',\n       'LOR ', 'CGPA', 'Research']\nX = df[features]\ny = df['Chance of Admit ']","7ebcbc8b":"X = df[features]\ny = df['Chance of Admit ']\ntrainX ,testX , trainY, testY = train_test_split(X, y, train_size = 0.7,random_state = 5) ","fcc3d437":"lin_reg = LinearRegression()\npredY = lin_reg.fit(trainX,trainY).predict(testX)\nm1 = r2_score(testY,predY)\nprint('Accuracy\/RSquared : ',r2_score(testY, predY))\nprint('Root Mean Squared Error : ',np.sqrt(mean_squared_error(testY,predY)))","dd67f9b6":"dec_tree = DecisionTreeRegressor()\npredY = dec_tree.fit(trainX,trainY).predict(testX)\nm2 = dec_tree.score(testX,testY)\nprint('Accuracy : ',dec_tree.score(testX, testY))\nprint('Root Mean Squared Error : ',np.sqrt(mean_squared_error(testY,predY)))\n\n","aa499f6a":"corr = df.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","0a2ff81e":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.model_selection import GridSearchCV,train_test_split \nfrom sklearn.metrics import mean_absolute_error\nrf_model = RandomForestRegressor(n_estimators = 100,random_state = 42)\nrf_model.fit(trainX,trainY)\nprint('Mean absolute error for RF model: %0.4f' %mean_absolute_error(testY,rf_model.predict(testX)))","9126577d":"feature_importance = pd.DataFrame(sorted(zip(rf_model.feature_importances_, X.columns)), columns=['value','Feature'])\nplt.figure(figsize=(10, 6)) \nsns.barplot(x=\"value\", y=\"Feature\", data=feature_importance.sort_values(by=\"value\", ascending=False)) \n","5d1886cc":"Observations: We can see df[chance of admit] lies between -1 to 1 . Its value is :-0.35. We dont Require any transformations on the data.As it is kind of symmetric.","8f278aec":"Linear Regression","37abec7c":"I split the data into two different sets, one for the independent features \u2014 x, and one for the dependent variable \u2014 y (which is the last column). Next I split the dataset x into two separate sets \u2014 xTrain and xTest. Similarly, I splitted the dataset y into two sets as well \u2014 yTrain and yTest. The training set has 75% of data while test set has 25% of it.","79a746e2":"Decision Tree Regressor","75933962":"The mean absolute error for the random forest model is 0.0421.\nThe final part was to plot the class wise feature importance. Clearly CGPA is the most important criteria for graduate admission followed by GRE and TOEFL score.","e89f0b2d":"Random Forest","47c5659e":"n_estimators is the number of trees to be used in the forest. Since Random Forest is an ensemble method comprising of creating multiple decision trees, this parameter is used to control the number of trees to be used in the process.","a855bd3b":"As a quick summary, I used random forest algorithm to visualize the importance of each features for graduate admission. This could be of great help for students preparing for their higher studies.","6cdfb37e":"This one will tell us about the data in best fit line. Where we can notice in the end data is kind of far from the line.","8e281a42":"As per data we can observe few changes in the above data.\nGRE Score: 340 Total\nTOEFL Score: 120 Total\nUniversity Rating: Out of 5\nCGPA: Out of 10\nChance of Admit: if chance of admit>0.8 then Admission will be Completed else Admission Cannot be processed.\nTo look around at it we will create another column \"Chance of admit class\" where it will be categorical value 0 or 1.","4c94919b":"Out of these two methods we can assume that accuracy level is better with Linear Regression."}}