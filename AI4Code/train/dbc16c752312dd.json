{"cell_type":{"88cfc6a2":"code","bbaf9bfa":"code","80397c95":"code","7019e061":"code","5efa3a9e":"code","98df7d22":"code","5108f1a9":"code","f7f0890f":"code","28cdfdc3":"code","ee6a3498":"code","9ee94e5d":"code","87c21a50":"code","79fa1763":"code","e11a6bab":"code","5f512640":"code","df69acbd":"code","5ef9e4c7":"code","8ba38bff":"code","e946a1e4":"code","65eea2cd":"code","e215e8d9":"code","26821aef":"code","635fce72":"code","bc269df2":"code","dfc504d4":"code","64e3dbea":"code","0638366b":"markdown","36dedd08":"markdown","fc6abefd":"markdown","c4723043":"markdown","b94b98bf":"markdown","a00a4804":"markdown","9c8fe413":"markdown","a186fbde":"markdown","57fdf439":"markdown","f0a8ebfa":"markdown","20eea7a1":"markdown","c32da1e2":"markdown","cbd11f13":"markdown","5d235853":"markdown","e6b247f2":"markdown","8124bea0":"markdown","5817119a":"markdown","7efa257a":"markdown","6364af2e":"markdown","a9a4dc1b":"markdown"},"source":{"88cfc6a2":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso, ElasticNet\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","bbaf9bfa":"mtcars=pd.read_csv('..\/input\/mtcars\/mtcars.csv')\nmtcars.head()","80397c95":"mtcars1=mtcars.iloc[:,1:]\nmtcars1.head()","7019e061":"mtcars1.info()","5efa3a9e":"mtcars1.describe()","98df7d22":"mtcar4=mtcars1.transform(lambda x:x**0.5)\nX_wo1=mtcar4.drop(['mpg'],axis=1)\nY_wo1=mtcar4['mpg'].values\nX_const_wo1=sm.add_constant(X_wo1)\nmodel_wo1=sm.OLS(Y_wo1,X_const_wo1).fit()\nmodel_wo1.summary()\n","5108f1a9":"X=mtcars1.drop(['mpg'],axis=1)\nY=mtcars1.mpg\nvif_sqrt=[variance_inflation_factor(X_const_wo1.values,i) for i in range(X_const_wo1.shape[1])]\npd.DataFrame({'vif':vif_sqrt[1:]},index=X.columns).T","f7f0890f":"X_wo1=X_wo1[['drat','vs']]\nx_train1,x_test1,y_train1,y_test1=train_test_split(X_wo1,Y_wo1,test_size=0.3,random_state=1)\nlin_reg_log=LinearRegression()\nlin_reg_log.fit(x_train1,y_train1)\nprint('R^2 for train:',lin_reg_log.score(x_train1,y_train1))\nprint('R^2 for test:',lin_reg_log.score(x_test1,y_test1))","28cdfdc3":"mtcar5=mtcars1[['disp','hp','drat','wt','qsec','mpg']]\nmt_log=mtcar5.transform(lambda x:np.log(x))\nmt_log[['cyl','vs','am','gear','carb']]=mtcars1[['cyl','vs','am','gear','carb']]\nmt_log.head()\nX_wo1=mt_log.drop(['mpg'],axis=1)\nY_wo1=mt_log['mpg'].values\n\nX_const_wo1=sm.add_constant(X_wo1)\nmodel_wo1=sm.OLS(Y_wo1,X_const_wo1).fit()\nmodel_wo1.summary()\n\nvif_sqrt=[variance_inflation_factor(X_const_wo1.values,i) for i in range(X_const_wo1.shape[1])]\nvif_pd=pd.DataFrame({'vif':vif_sqrt[1:]},index=X.columns).T\nprint(vif_pd)\n\nX_wo1=X_wo1[['hp','gear','am','vs']]\nx_train1,x_test1,y_train1,y_test1=train_test_split(X_wo1,Y_wo1,test_size=0.3,random_state=1)\nlin_reg_log=LinearRegression()\nlin_reg_log.fit(x_train1,y_train1)\nprint()\n\nprint('R^2 for train:',lin_reg_log.score(x_train1,y_train1))\nprint('R^2 for test:',lin_reg_log.score(x_test1,y_test1))","ee6a3498":"mtcar5=mtcars1[['disp','hp','drat','wt','qsec','mpg']]\nmt_log=mtcar5.transform(lambda x:1\/x)\nmt_log[['cyl','vs','am','gear','carb']]=mtcars1[['cyl','vs','am','gear','carb']]\nmt_log.head()\nX_wo1=mt_log.drop(['mpg'],axis=1)\nY_wo1=mt_log['mpg'].values\nX_const_wo1=sm.add_constant(X_wo1)\nmodel_wo1=sm.OLS(Y_wo1,X_const_wo1).fit()\nmodel_wo1.summary()\nvif_sqrt=[variance_inflation_factor(X_const_wo1.values,i) for i in range(X_const_wo1.shape[1])]\nvif_pd=pd.DataFrame({'vif':vif_sqrt[1:]},index=X_wo1.columns).T\nprint(vif_pd)\n\nX_wo1=X_wo1[['drat','carb','vs']]\nx_train1,x_test1,y_train1,y_test1=train_test_split(X_wo1,Y_wo1,test_size=0.3,random_state=1)\nlin_reg_log=LinearRegression()\nlin_reg_log.fit(x_train1,y_train1)\nprint()\n\nprint('R^2 for train:',lin_reg_log.score(x_train1,y_train1))\nprint('R^2 for test:',lin_reg_log.score(x_test1,y_test1))","9ee94e5d":"cor=mtcars1.corr()\ncor1=cor['mpg']\nfeatu=cor1[abs(cor1)>0.5][1:]# [1:] remove mpg from list\n\nmult_cor=mtcars1[['cyl', 'disp', 'hp', 'drat', 'wt', 'vs', 'am', 'carb','mpg']].corr()\ncor_max=max(abs(featu.values))\nfinal=featu[abs(featu.values)==cor_max]\nfinal","87c21a50":"X2=mtcars1[['wt']]\nY2=mtcars1.mpg\nx_train,x_test,y_train,y_test=train_test_split(X2,Y2,test_size=0.3,random_state=1)\nlin1=LinearRegression()\nlin1.fit(x_train,y_train)\nprint('R2 for train:',lin1.score(x_train,y_train))\nprint('R2 for test:',lin1.score(x_test,y_test))","79fa1763":"X=mtcars1.drop(['mpg'],axis=1)\nY=mtcars1.mpg\nmodel=sm.OLS(Y,X).fit()\nmodel.pvalues","e11a6bab":"lin=LinearRegression()\ncols=list(X.columns)\nselect_feat=[]\n\nwhile(len(cols)>0):\n    p=[]\n    X1=X[cols]\n    model1=sm.OLS(Y,X1).fit()\n    p=pd.Series(model1.pvalues,index=X1.columns)\n    pmax=max(p)\n    if(pmax>0.05):\n        feature_with_p_max = p.idxmax()\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselect_feat=cols\nselect_feat","5f512640":"X2=mtcars1[['wt', 'qsec', 'am']]\nY2=mtcars1.mpg\nx_train,x_test,y_train,y_test=train_test_split(X2,Y2,test_size=0.3,random_state=1)\nlin1=LinearRegression()\nlin1.fit(x_train,y_train)\nprint('R2 for train:',lin1.score(x_train,y_train))\nprint('R2 for test:',lin1.score(x_test,y_test))","df69acbd":"lin=LinearRegression()\nX.columns\nhighsc=0\nnof=0\nsupport_score=[]\nnoflist=np.arange(1,11)\nfor n in noflist:\n    x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,random_state=1)\n    rfe=RFE(lin,n)\n    X_train_rfe=rfe.fit_transform(x_train,y_train)\n    X_test_rfe=rfe.transform(x_test)\n    lin.fit(X_train_rfe,y_train)\n    score=lin.score(X_test_rfe,y_test)\n    if(score>highsc):\n        highsc=score\n        nof=n\n        support_score=rfe.support_\n        \ntemp=pd.Series(support_score,index=X.columns)\nprint('No of optimum features:',n)\nprint('SCore for optimum features:',highsc)\nprint('Features Selected:\\n')\ntemp[temp==True].index","5ef9e4c7":"X2=mtcars1[['drat', 'wt', 'gear', 'carb']]\nY2=mtcars1.mpg\nx_train,x_test,y_train,y_test=train_test_split(X2,Y2,test_size=0.3,random_state=1)\nlin1=LinearRegression()\nlin1.fit(x_train,y_train)\nprint('R2 for train:',lin1.score(x_train,y_train))\nprint('R2 for test:',lin1.score(x_test,y_test))","8ba38bff":"vif=[variance_inflation_factor(X.values, j) for j in range(X.shape[1])]\nvif_pd=pd.Series(vif,index=X.columns)\nvif_pd","e946a1e4":"def calculate_vif(x):\n    output=pd.DataFrame()\n    vif=[variance_inflation_factor(x.values, j) for j in range(x.shape[1])]\n    cols = x.shape[1]\n    thresh=5.0\n    for i in range(cols):\n        print('Iteration:',i)\n        a=np.argmax(vif)\n        print('Max vif found at:',a)\n        if(vif[a]>thresh):\n            if i==0:\n                output=x.drop(x.columns[a],axis=1)\n                vif=[variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n            else:\n                output=output.drop(output.columns[a],axis=1)\n                vif=[variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n        else:\n            break\n    return output.columns\ncalculate_vif(X).values","65eea2cd":"X2=mtcars1[['disp', 'vs', 'am']]\nY2=mtcars1.mpg\nx_train,x_test,y_train,y_test=train_test_split(X2,Y2,test_size=0.3,random_state=1)\nlin1=LinearRegression()\nlin1.fit(x_train,y_train)\nprint('R2 for train:',lin1.score(x_train,y_train))\nprint('R2 for test:',lin1.score(x_test,y_test))","e215e8d9":"X=mtcars1.drop(['mpg'],axis=1)\nY=mtcars1.mpg\n\nreg = LassoCV()\nreg.fit(X, Y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,Y))\ncoef = pd.Series(reg.coef_, index = X.columns)\n\n","26821aef":"coeff=coef.sort_values()\ncoeff.plot(kind='bar')\nplt.show()\n","635fce72":"X2=mtcars1[['disp', 'hp']]\nY2=mtcars1.mpg\nx_train,x_test,y_train,y_test=train_test_split(X2,Y2,test_size=0.3,random_state=1)\nlin1=LinearRegression()\nlin1.fit(x_train,y_train)\nprint('R2 for train:',lin1.score(x_train,y_train))\nprint('R2 for test:',lin1.score(x_test,y_test))","bc269df2":"reg1=ElasticNet()\nreg1.fit(X, Y)\nprint(\"Best alpha using built-in ElasticNet: %f\" % reg1.alpha)\nprint(\"Best score using built-in ElasticNet: %f\" %reg1.score(X,Y))\ncoef_elastic = pd.Series(reg1.coef_, index = X.columns)","dfc504d4":"coeff=coef_elastic.sort_values()\ncoeff.plot(kind='bar')\nplt.show()","64e3dbea":"X2=mtcars1[['wt','carb','cyl','disp', 'hp','qsec']]\nY2=mtcars1.mpg\nx_train,x_test,y_train,y_test=train_test_split(X2,Y2,test_size=0.3,random_state=1)\nlin1=LinearRegression()\nlin1.fit(x_train,y_train)\nprint('R2 for train:',lin1.score(x_train,y_train))\nprint('R2 for test:',lin1.score(x_test,y_test))","0638366b":"## Feature Selection","36dedd08":"### Inverse","fc6abefd":"* Using Recursive Feature Selection we get 'drat', 'wt', 'gear', 'carb' as most relevant features ","c4723043":"### ElasticNet","b94b98bf":"### SQRT","a00a4804":"### ----------------------------------------------------------------------------------------------------------------------------------------------------------------","9c8fe413":"### Wrapper Methods","a186fbde":"## Transformation","57fdf439":"### LASSO","f0a8ebfa":"### Conclusion","20eea7a1":"* As you see all are correlated to each other. So we need to select which one has highest correlation with target variable\n* Using Pearson Correlation we get 'wt' as most relevant feature","c32da1e2":"* Using Pearson Correlation method we get R^2 as 0.66. However it also has overfitting problem\n* Using Feature Selection on basis of vif we get R^2 as 0.60\n* Using Recursive Feature Selection we get R^2 as 0.70. We get overfitting using this methodd\n* Using Backward Elimination we get R^2 as 0.75\n* Using LASSO we get get R^2 as 0.60\n* Using ElasticNet we get get R^2 as 0.72","cbd11f13":"* By Tansformation we get that log transformation gave us best R^2 for train: 0.7969614608648559\n* SQRT Transformation get R^2 for train: 0.5788558777828829\n* Inverse Transformation get R^2 for train: 0.6410026585549855\n* Thus Log gave us best result","5d235853":"### Backward elimination","e6b247f2":"### Pearson Correlation","8124bea0":"* Using backward elimination we get 'wt', 'qsec', 'am' as most relevant features ","5817119a":"### Recursive Feature Selection","7efa257a":"### LOG","6364af2e":"* Using Feature Selection on basis of vif we get 'disp', 'vs', 'am' as most relevant features ","a9a4dc1b":"### Feature Selection on basis of vif"}}