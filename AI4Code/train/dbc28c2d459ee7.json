{"cell_type":{"1ee2f342":"code","bd2cef65":"code","c330fbea":"code","f64b3828":"code","49c40758":"code","04f22c8e":"code","35b216b9":"code","1389e4ae":"code","cf9efe81":"code","3fa244a0":"code","f16b6475":"code","e308ae5d":"code","c9ef83cc":"code","4d3f5808":"code","6dcd6bea":"code","ccb41f8c":"code","161d1e81":"markdown","a055eec2":"markdown","ad85c706":"markdown","becbace3":"markdown","24918240":"markdown","9c6d387c":"markdown","ff5b3e39":"markdown","e6a8ca4c":"markdown","31198330":"markdown"},"source":{"1ee2f342":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom scipy.stats import skew\nfrom math import sqrt\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\npd.set_option('display.max_columns', 300)","bd2cef65":"df_train = pd.read_csv('..\/input\/train.csv', index_col=\"Id\")\ndf_test = pd.read_csv('..\/input\/test.csv', index_col=\"Id\")\nprint('Columnas de entrenamiento : '+str(len(df_train.columns)))\nprint('Columnas de test : '+str(len(df_test.columns)))\ny_train = df_train.SalePrice\n#Eliminamos la columma del precio en el entrenamiento\n#df_train = df_train.drop(columns='SalePrice')\nprint('Columnas de entrenamiento : '+str(len(df_train.columns)))\n\nprint('Columnas de entrenamiento num: '+str(len(df_train._get_numeric_data().columns)))\nprint('Columnas de entrenamiento obj: '+str(len(df_train.select_dtypes(include='object').columns)))\n\n#check the decoration\n\ndf_test.columns\n\n#descriptive statistics summary\n#df_train['SalePrice'].describe()\n#histogram\n#sns.distplot(df_train['SalePrice']);","c330fbea":"df_train.shape , df_test.shape","f64b3828":"#Visualizacion de los primeros datos. \n\ndf_train.head()","49c40758":"#descriptive statistics summary\ndf_train['SalePrice'].describe()\n","04f22c8e":"#histogram\nsns.distplot(df_train['SalePrice']);","35b216b9":"df_train.skew(), df_train.kurt()","1389e4ae":"numeric_features = df_train.select_dtypes(include=[np.number])\nnumeric_features.columns\ncorrelation = numeric_features.corr()\nprint(correlation['SalePrice'].sort_values(ascending = False),'\\n')","cf9efe81":"\nf , ax = plt.subplots(figsize = (14,12))\n\nplt.title('Correlation of Numeric Features with Sale Price',y=1,size=16)\n\nsns.heatmap(correlation,square = True,  vmax=0.8)","3fa244a0":"   \nsns.set()\ncolumns = ['GrLivArea', 'GarageCars','GarageArea','TotalBsmtSF' ,'1stFlrSF' ,'FullBath','TotRmsAbvGrd','YearBuilt']\nsns.pairplot(df_train[columns],size = 2 ,kind ='scatter',diag_kind='kde')\nplt.show()","f16b6475":"df_train_v2=df_train[columns]\ncolumns2 = ['GrLivArea', 'GarageCars','GarageArea','TotalBsmtSF' ,'1stFlrSF' ,'FullBath','TotRmsAbvGrd','YearBuilt']\n\ndf_test_v2=df_test[columns2]\n#df_train_v2=df_test[columns]\n#df_train_v2\ntotal = df_train_v2.isnull().sum().sort_values(ascending=False)\npercent = (df_train_v2.isnull().sum()\/df_train_v2.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1,join='outer', keys=['Total Missing Count', '% of Total Observations'])\nmissing_data.index.name =' Numeric Feature'\n\nmissing_data.head(20)","e308ae5d":"columns = ['GrLivArea', 'GarageCars','GarageArea','TotalBsmtSF' ,'1stFlrSF' ,'FullBath','TotRmsAbvGrd','YearBuilt']\ndf_train_v2=df_train_v2[columns]\ncolumns2 = ['GrLivArea', 'GarageCars','GarageArea','TotalBsmtSF' ,'1stFlrSF' ,'FullBath','TotRmsAbvGrd','YearBuilt']\n\ndf_test_v2=df_test_v2[columns2]\ndef cat_imputation(column, value):\n    df_train_v2.loc[df_train_v2[column].isnull(),column] = value\ndef cat_imputation_test(column, value):\n    df_test_v2.loc[df_test_v2[column].isnull(),column] = value    \n    \n#cat_imputation('MasVnrArea', 0.0)\n#cat_imputation_test('MasVnrArea', 0.0)\ncat_imputation_test('GarageCars', 0.0)\ncat_imputation_test('GarageArea', 0.0)\ncat_imputation_test('TotalBsmtSF', 0.0)\n","c9ef83cc":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\ndf_train=df_train_v2\ndf_test=df_test_v2\n","4d3f5808":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df_train.values)\n    rmse= np.sqrt(-cross_val_score(model, df_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","6dcd6bea":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n#GB_model = ENet.fit(df_train, y_train)\n# Defining two rmse_cv functions\ndef rmse_cv2(model):\n    rmse = np.sqrt(-cross_val_score(model, df_train, y_train, scoring=\"neg_mean_squared_error\", cv = 10))\n    return(rmse)\n#model_elastic = make_pipeline(RobustScaler(), ElasticNet(alpha = 0.0005))\nmodel_elastic = Ridge(alpha = 5)\n#cv_elastic = rmse_cv2(model_elastic).mean()\n\nmodel_elastic.fit(df_train, y_train)\nkrr_pred = model_elastic.predict(df_test)\n#krr_pred = model_elastic.predict(df_test)\n\n\n\n# Setting up competition submission\nsub = pd.DataFrame()\n#sub['Id'] = df_test.index\nsub['Id'] = df_test.index\nsub['SalePrice'] = krr_pred\nsub.to_csv('submission.csv',index=False)\nprint(\"Entrenameinto concluido\")\nkrr_pred","ccb41f8c":"## Getting our SalePrice estimation\n#Final_labels = (np.exp(GB_model.predict(df_test))) \n## Saving to CSV\n#pd.DataFrame({'Id': df_test.index, 'SalePrice': Final_labels}).to_csv('submission.csv', index =False)\n#print(\"Fichero creado\")","161d1e81":"Como \"clar\u00edsimamente\" se ve, hay unas variables con una correlaci\u00f3n mayor que otras. Esto nos podr\u00eda hacer descartar algunas variables para simplificar.  Podr\u00edamos simplificar en el estudio de esta variables, por ejemplo realizando un \"zoom\" de este Map para las variables seleccionadas por ejemplo pero no lo haremos en este Kelnel.\nVamos a hacer una selecci\u00f3n directa de las variables con m\u00e1s correlaci\u00f3n y lanzar un comando para obtner unos bontios plots de f\u00e1cil interpretaci\u00f3n :)\n","a055eec2":"## 2.1 Data cleaning\n\nUna vez seleccionada la informaci\u00f3n, queremos descartar que haya informaci\u00f3n incorrecta. Uno de los puntos importantes es ue hacer con los valores vac\u00edos. Visualicemos cuantos hay, y, en este caso, quitaremos LotFrontage de la nuestra muestra. En MasVnrArea, pondremos un 0.","ad85c706":"Dos ideas estan claras. Tenemos un gran n\u00famero de variables,  y algunas no son muy faciles de interpretar. Por otro lado  el SalePrice no tiene una distribuci\u00f3n normal.\n\nDe skew y kurt hablamos en otro momento, pero lo introducimos ya por ser importante.\n","becbace3":"# 1 Introducci\u00f3n  \/Introduction\n\nI'm going to write this kernel in Spanish. I'll promise write the next one in English.\n\nTeniendo poca experiencia en este mundo, lo que pretendo con este kernel es tener un primer modelo de referencia donde cada paso sea simple. Profundizar en cada unos de los pasos ser\u00e1 ya un es fuerzo m\u00e1s o menos importante, pero ser consciente de que se est\u00e1 haciendo en cada paso deber\u00eda quedar claro aqui.\n\nPartiremos como siempre en la competiciones del data set que nos proporcionan. Hablaremos de las tarea de an\u00e1lisis de datos, de data cleaning, seleccion y transformaci\u00f3n. \u00bfQu\u00e9 es importante en cad una de esas tareas?  Est\u00e1 claro que analizar es necesario, porque sin conocer los datos no llegaremos a ningun sitio. Pero \u00bfhasta cuando \"limpiar datos\"? \u00bfQu\u00e1 datos seleccionar? \u00bfPor qu\u00e9 trasformar si supone un esfuerzo?\n\nRespecto al modelado tambi\u00e9n simplificaremos al m\u00e1ximo. Pero realizaremos m\u00e1s de un modelado,  no tanto para conseguir buenos resultados, sino para poder comparar y  detectar tambien si los puntos anteriores nos han beneficiado. Intentaremos ver la diferenia entre una parametrizaci\u00f3n y otra de algun modelo.\n\nEvidentemente entrenaremos nuestro model  y veremos resultados. \n\nComentar que este kernel lo realizar\u00e9 en espa\u00f1ol. Espero comentarios sobre si este hecho ayuda as algunos compa\u00f1eros a introducirse en kaggle. \n\nEmpezamos.\n\n\n\n# 2 An\u00e1lisis del data set\n\nEste paso, aunque parece sencillo es el primer obst\u00e1culo a superar por alguien que se inicia en Data Sience. Entiendo que hay varias razones como pueden ser las siguientes:\n\n-El data set es un conjunto de datos \"impuesto\". Puede ser que no conozcamos nada acerca de esos datos. Evidentemente, en la descripci\u00f3n de los datos y mediante una primera aproximaci\u00f3n podemos familiarizarnos con ellso, pero dependiendo de las tem\u00e1tica nos puede costar m\u00e1s o menos conseguir esa familiarizaci\u00f3n.\n\n-Se requieren conocimientos estad\u00edsticos m\u00e1s o menos importantes para realizar un an\u00e1lisis m\u00e1s o menos profundo. Todos esos conocimientos se pueden aprender si nos los tenemos, evidentemente, pero en algunos casos podemos perdernos al revisar los kernels de compa\u00f1eros m\u00e1s avanzados.\n\n-Los conocimientos de python y las librer\u00edas utilizadas para el an\u00e1lisis, si no son muy avanzados, puede complicar tambien este an\u00e1lsis. Python es sencillo, pero los inicios siempre son complicados.\n\n-Intuici\u00f3n. En algunos casos la intuici\u00f3n participa en ese an\u00e1lisis de datos, pero cuando empecemos en el mundo del Data Science preferimos tomar decisiones claras.  Quiz\u00e1 en un momento m\u00e1s avanzado podemos incluso realizar cosas m\u00e1s simples que lo que intentemos en un momento inicial.\n\nDicho esto, vamos a realizar un an\u00e1lisis de datos m\u00ednimo. Aun podr\u00edamos realizar una an\u00e1lisis menor, simplemente contando los datos y visualizando los valores de manera r\u00e1pida.\n\nCompo paso necesario anterior, realizaremos la carga de datos. Lo que ser\u00e1 obtener los datos de los ficheros proporcionados para que ya sean usables en Python desde un data frame por ejemplo\n\n## 2.1 Data Loading y primera aproximaci\u00f3n a los datos","24918240":"# 6 Modelling\nHemos escogido 3 modelos.\n-Lasso Regression\n-Elastic Net Regression\n-Gradient boosting Regression\n\nNo vamos a profundizar en la caracter\u00edscas, virtudes y desventajas de cada una. De momento nos limitaremos a trabajar con estos 3 modelos.\n\n## 6.1 Imports\n","9c6d387c":"> ## 6.3 Resultados","ff5b3e39":"If you found any mistakes or have any tips please be free to comment.","e6a8ca4c":"## 6.2 Funci\u00f3n de validacion","31198330":"## 2.1 Relaci\u00f3n entre datos para selecci\u00f3n\nVamos a revisar la correlaci\u00f3n entre las variables y el SalePrice. N\u00famericamente y en un bonito gr\u00e1fico. Vamos a limitarnos en  este caso a variables num\u00e9ricas. Como ejemplo ya nos servir\u00e1."}}