{"cell_type":{"4cd51cdc":"code","3b794023":"code","0c43d79d":"code","b03daaab":"code","74ecfbc4":"code","edcc2b2b":"code","8f2718e0":"code","dc9223e5":"code","7f1e8cdc":"code","04c0cc84":"code","fb82112b":"code","00e2bcd9":"code","a5ad4b82":"code","017330ce":"code","e13f7fb7":"markdown","656c4f74":"markdown","adba519a":"markdown","f870ac87":"markdown","a4669076":"markdown","52062107":"markdown","8670d1e2":"markdown"},"source":{"4cd51cdc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b794023":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","0c43d79d":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]\ntrain_df","b03daaab":"train_df = train_df.drop(['PassengerId','Cabin','Ticket'], axis=1)\ntest_df = test_df.drop(['PassengerId','Cabin','Ticket'], axis=1)\n\n#complete missing age with median\ntrain_df['Age'].fillna(train_df['Age'].median(), inplace = True)\ntest_df['Age'].fillna(test_df['Age'].median(), inplace = True)\n\n#complete embarked with mode\ntrain_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)\n\ntest_df['Fare'].fillna(test_df['Fare'].mode()[0], inplace = True)\n\ntrain_df","74ecfbc4":"train_df['Sex'] = train_df['Sex'].map({'male':0,'female':1})\ntest_df['Sex'] = test_df['Sex'].map({'male':0,'female':1})\n\ntrain_df","edcc2b2b":"for dataset in [train_df,test_df]:\n    \n    # Creating a categorical variable to tell if the passenger is alone\n    dataset['IsAlone'] = ''\n    dataset['IsAlone'].loc[((dataset['SibSp'] + dataset['Parch']) > 0)] = 1\n    dataset['IsAlone'].loc[((dataset['SibSp'] + dataset['Parch']) == 0)] = 0\n    \n    \n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    # take only top 10 titles\n    title_names = (dataset['Title'].value_counts() < 10) #this will create a true false series with title name as index\n\n    #apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n    print(dataset['Title'].value_counts())\n    \n    dataset.drop(['Name','SibSp','Parch'],axis=1,inplace=True)\n\ntrain_df.head()","8f2718e0":"#define x and y variables for dummy features original\ntrain_dummy = pd.get_dummies(train_df,drop_first=True)\ntest_dummy = pd.get_dummies(test_df,drop_first=True)\n\ntrain_dummy","dc9223e5":"X_final = train_dummy.drop(['Survived'],axis=1).values # for original features\ntarget = train_dummy['Survived'].values\nX_final","7f1e8cdc":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_final = sc.fit_transform(X_final)\nprint(X_final.shape,'\\n',X_final)","04c0cc84":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\ndef euclidean_distance(x1, x2):\n    return np.sqrt(np.sum((x1 - x2)**2))\n\nclass MYKMeans:\n    def __init__(self,k=5,iters=1000,plot_steps=False):\n        self.k=k\n        self.iters=iters\n        self.plot_steps = plot_steps\n\n        # clusters are essentially a group of points.. for k clusters, we will have k group of points so let's make that\n        self.n_clusters = [[] for _ in range(self.k)] # empty list of lists [[],[],[]]\n\n        # We need to have a list of centroids for every cluster. For k clusters, we'll have k centroids in list\n        self.centroids = [] \n\n    def predict(self,X):\n        # the meat of the code!\n        # So, how do you predict??\n        self.X=X\n        self.n_samples, self.n_features = X.shape # (1000,10) means 10000 samples with 10 feature each\n\n        # Step 2: It is to chose random centroids!\n        rand_idx = np.random.choice(self.n_samples, self.k, replace=False) # [34,54]\n        self.centroids = [self.X[i] for i in rand_idx] # centroids = [X[34],X[54]], which is a list of size n_features, just a sample\n\n        # Let's go through Optimizing clusters!\n        for _ in range(self.iters):\n            # first step to optimization is to create new clusters!\n            # create clusters by assigning them to their nearest centroid\n            self.clusters = self.create_clusters(self.centroids) # abstract and send centroids!\n            if self.plot_steps:\n                self.plot()\n\n            # calculate new centroids from the clusters!\n            # we do that by taking mean \n            old_centroids = self.centroids\n            self.centroids = self.update_centroids(self.clusters) # again clusters is just a list of lists. \n\n            if self.plot_steps:\n                self.plot()\n                \n            # check if clusters have changed, if yes, we are done else train again!\n            if self.is_converged(old_centroids, self.centroids):\n                break\n            # Classify samples as the index of their clusters\n\n        return self.get_cluster_labels(self.clusters)\n\n    def create_clusters(self, centroids):\n        clusters = [[] for _ in range(self.k)]\n\n        for idx, sample in enumerate(self.X): # for every sample\n            # distance of the current sample to each centroid\n            distances = [euclidean_distance(sample, centroid) for centroid in centroids] # remember centroid is also a sample, that we chose, we find dist between these 2 samples for every centroid\n            closest_index = np.argmin(distances) # min dist of this sample with every centroid\n\n            # once you have the cluster you want to assign to, put that sample idx in a cluster\n            clusters[closest_index].append(idx)\n        return clusters\n\n    def update_centroids(self, clusters):\n        # assign mean value of clusters to centroids\n        # make empty list of zeroes\n        centroids = np.zeros((self.k, self.n_features))\n\n        for cluster_idx, cluster in enumerate(clusters):\n            cluster_mean = np.mean(self.X[cluster], axis=0) # since cluster has the idx of samples, we do X[cluster] to get actual sample\n            centroids[cluster_idx] = cluster_mean\n\n        return centroids\n\n    def get_cluster_labels(self, clusters):\n        # each sample will get the label of the cluster it was assigned to\n        labels = np.empty(self.n_samples)\n\n        for cluster_idx, cluster in enumerate(clusters):\n          for sample_index in cluster:\n            labels[sample_index] = cluster_idx\n        return labels\n\n    def is_converged(self,old , new):\n        # distances between each old and new centroids, fol all centroids\n        distances = [euclidean_distance(old[i], new[i]) for i in range(self.k)]\n        return sum(distances) == 0\n  \n    def plot(self):\n        fig, ax = plt.subplots(figsize=(12, 8))\n\n        for i, index in enumerate(self.clusters):\n          point = self.X[index].T\n          ax.scatter(*point,linewidth=3)\n\n        for point in self.centroids:\n          ax.scatter(*point, marker=\"x\", color='black', linewidth=6)\n\n        plt.show()\n\n# X,y = make_blobs(centers = 3,n_samples = 1000, n_features = 3, shuffle = False)\nkmeans_clf = MYKMeans(k=2)","fb82112b":"preds = kmeans_clf.predict(X_final)\npreds","00e2bcd9":"accurate = 0\nfor i,val in enumerate(train_dummy['Survived']):\n    if val==preds[i]:\n        accurate+=1\nprint(accurate)\nprint(\"Accuracy = \",accurate\/len(preds))","a5ad4b82":"from sklearn.cluster import KMeans\nkmeans_sk = KMeans(n_clusters=2, random_state=0).fit(X_final)\nkmeans_sk.labels_","017330ce":"accurate = 0\nfor i,val in enumerate(train_dummy['Survived']):\n    if val==kmeans_sk.labels_[i]:\n        accurate+=1\nprint(accurate)\nprint(\"Accuracy = \",accurate\/len(kmeans_sk.labels_))","e13f7fb7":"# How do we build a custom KMeans in NUMPY?\n","656c4f74":"## This was a basic Custom K means Clustering Algo in Numpy vs the one in Sklearn!\n\n\n#### Please visit my website wherein I am doing 100 Projects in Deep Learning.\n---\n### [AI Unquote](https:\/\/www.aiunquote.com)\n### Follow me on LinkedIn where I post DS related Stuff everyday!!  [Alaap Dhall](https:\/\/www.linkedin.com\/in\/alaapdhall\/)\n***\n\n<br>\n\n**Thank you**","adba519a":"### let's do Feature Engineering and add IsAlone and Title instead of Name,SibSp, and Parch.","f870ac87":"### First understand what a kmeans is:\nhere's a cool link for your reference!!\nhttps:\/\/www.youtube.com\/watch?v=4b5d3muPQmA\n\nNow we know that the main function is to\n1. Get centroids randimly\n2. Create Clusters\n3. Update Centroids\n4. Repeat step 2 and 3 until converged.\n\nGreat!! Now how do we code?\nHere is a step by step Guidline with code!!\n","a4669076":"## Step 1:\nCreate Empty Clusters and Centroids\n```\n    self.n_clusters = [[] for _ in range(K)], where k is no of clusters!\n```\n\n## Step 2:\nChoose random data points to be your centroids!\nUse numpy to chose k random points.\n```\n    rand_idx = np.random.choice(self.n_samples, self.k, replace=False) # [34,54]\n    self.centroids = [self.X[i] for i in rand_idx]\n    \n```\n\n## Step 3:\nOnce we have random centroids decided, we will find all the points nearest to those cemtroids and put them in a list of clusters\n1. In order to find clusters, go through every sample, and for each sample calculate its difference with all the centroids.\n2. Centroid with minimum dist to that sample will be assigned to the sample\n```\n    for sample in X: # for every sample\n        distances = [euclidean_distance(sample, centroid) for centroid in centroids] \n        closest_index = np.argmin(distances) \n```\n\n## Step 4:\nOnce we have created a cluster of points, we will update our centroids to the mean of current clusters, and then find new clusters with new centroids until we converge.\n\n```\n    for cluster_idx, cluster in enumerate(clusters):\n        cluster_mean = np.mean(X[cluster]) \n        centroids[cluster_idx] = cluster_mean\n```\n\n## Final:\nCheck if the clusters have converged!","52062107":"- 78.4 vs 78.7\n- looks like we are close to the original classifier from sklearn!\n- we will improve on it more in later versions!","8670d1e2":"## WAIT!!\n\n\n# Looking for 100 Data Science Interview Questions?\n## I've got you: *https:\/\/www.linkedin.com\/posts\/alaapdhall_50-of-100-data-science-interview-questions-activity-6716618160969269248-bX5W*\n\n## If you're Interested in Deep Learning with PyTorch, visit https:\/\/www.aiunquote.com for 100 project in Deep Learning Series!!\n\n---"}}