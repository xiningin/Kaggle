{"cell_type":{"4eee8648":"code","77586a58":"code","ecd00e2d":"code","8b851150":"code","373f58ef":"code","22481531":"code","948a55e3":"code","56f39b0f":"code","327b5e14":"code","bc81ca1e":"code","1051b636":"code","d9e9cc5e":"code","ee6a6c60":"code","ec1c377b":"code","1fd24d37":"code","fb4cd871":"code","707c6af1":"code","104bb487":"code","16f048fe":"code","c321198d":"code","5f434237":"code","5073ec9c":"code","bc252f67":"code","2b30531d":"markdown","34ced614":"markdown","dfb43dfc":"markdown","eed21e69":"markdown","0d95ad76":"markdown","f248406d":"markdown","f3a07382":"markdown","44ee3794":"markdown","85f651cc":"markdown","17b37947":"markdown"},"source":{"4eee8648":"#! pip install git+https:\/\/www.github.com\/keras-team\/keras-contrib.git\n! wget https:\/\/github.com\/jganzabal\/santander_kaggle_solutions_tests\/raw\/master\/santander_helper.py","77586a58":"#Import base libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport warnings\nfrom tqdm import tqdm_notebook as tqdm\nimport random\nfrom scipy import interp\nimport statsmodels.api as sm\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nplt.style.use('ggplot')\n%matplotlib inline\n#Import sklearn libraries\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import KFold,cross_val_predict,cross_validate,cross_val_score,train_test_split\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\n","ecd00e2d":"#Help functions\n\ndef plot_2d_space(X, y, label='Classes'):\n    plt.figure(figsize=(8,8))\n    markers = ['o', 's']\n    for l, m in zip(np.unique(y), markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1]\n            , label=l, marker=m\n            )\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()\n#ROC\/AUC\ndef plot_roc_curve(fpr, tpr):  \n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","8b851150":"# GET INDICIES OF REAL TEST DATA FOR FE\n#######################\n# TAKE FROM YAG320'S KERNEL\n# https:\/\/www.kaggle.com\/yag320\/list-of-fake-samples-and-public-private-lb-split\n\ntest_path = '..\/input\/test.csv'\ntrain_path = '..\/input\/train.csv'\n\ndf_test = pd.read_csv(test_path)\ndf_test.drop(['ID_code'], axis=1, inplace=True)\ndf_test = df_test.values\n\nunique_samples = []\nunique_count = np.zeros_like(df_test)\nfor feature in range(df_test.shape[1]):\n    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n    unique_count[index_[count_ == 1], feature] += 1\n\n# Samples which have unique values are real the others are fake\nreal_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\nprint('Found',len(real_samples_indexes),'real test')\nprint('Found',len(synthetic_samples_indexes),'fake test')\n\n###################\n\nd = {}\nfor i in range(200): d['var_'+str(i)] = 'float32'\nd['target'] = 'uint8'\nd['ID_code'] = 'object'\n\ntrain = pd.read_csv('..\/input\/train.csv', dtype=d)\ntest = pd.read_csv('..\/input\/test.csv', dtype=d)\n\nprint('Loaded',len(train),'rows of train')\nprint('Loaded',len(test),'rows of test')\nprint('Found',len(real_samples_indexes),'real test')\nprint('Found',len(synthetic_samples_indexes),'fake test')\n\n###################\n\nd = {}\nfor i in range(200): d['var_'+str(i)] = 'float32'\nd['target'] = 'uint8'\nd['ID_code'] = 'object'\n\ntrain = pd.read_csv(train_path, dtype=d)\ntest = pd.read_csv(test_path, dtype=d)\n\nprint('Loaded',len(train),'rows of train')\nprint('Loaded',len(test),'rows of test')","373f58ef":"# FREQUENCY ENCODE\ndef encode_FE(df,col,test):\n    cv = df[col].value_counts()\n    nm = col+'_FE'\n    df[nm] = df[col].map(cv)\n    test[nm] = test[col].map(cv)\n    test[nm].fillna(0,inplace=True)\n    if cv.max()<=255:\n        df[nm] = df[nm].astype('uint8')\n        test[nm] = test[nm].astype('uint8')\n    else:\n        df[nm] = df[nm].astype('uint16')\n        test[nm] = test[nm].astype('uint16')        \n    return\n\ntest['target'] = -1\ncomb = pd.concat([train,test.loc[real_samples_indexes]],axis=0,sort=True)\nfor i in range(200): \n    encode_FE(comb,'var_'+str(i),test)\ntrain = comb[:len(train)]; del comb\nprint('Added 200 new magic features!')","22481531":"del df_test, real_samples_indexes, synthetic_samples_indexes, unique_count, unique_samples, d","948a55e3":"#Create DataFrames\n\n# Load data with counts saved in the previous cells\ndf_train_data = train.drop(columns=['ID_code'])\ndf_train_X = df_train_data.drop(columns=['target'])\n\nreverse_columns = True\nif reverse_columns:\n    reverse_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 11, 15, 16, 18, 19, 22, 24, 25, 26,\n                    27, 29, 32, 35, 37, 40, 41, 47, 48, 49, 51, 52, 53, 55, 60, 61,\n                    62, 65, 66, 67, 69, 70, 71, 74, 78, 79, 82, 84, 89, 90, 91, 94,\n                    95, 96, 97, 99, 103, 105, 106, 110, 111, 112, 118, 119, 125, 128,\n                    130, 133, 134, 135, 137, 138, 140, 144, 145, 147, 151, 155, 157,\n                    159, 161, 162, 163, 164, 167, 168, 170, 171, 173, 175, 176, 179,\n                    180, 181, 184, 185, 187, 189, 190, 191, 195, 196, 199,\n                    ]\n\n    for j in reverse_list:\n        df_train_X[f'var_{j}'] *= -1","56f39b0f":"#Define X,y\ny = df_train_data['target'].values\nX = df_train_X.values\n\nprint(df_train_X.shape)","327b5e14":"#Check the distribution of target classes\ndf_train_data.groupby(by=\"target\")[\"target\"].count().plot(kind=\"bar\",title=\"Distribution Of Classes\")","bc81ca1e":"# Normalize data\nrs = RobustScaler()\nX_scaled_rs = rs.fit_transform(X)","1051b636":"#Let`s plot a 2d scatter plot to see the distribution of 2 random dimensions\nn = random.randrange(start=0,stop=198)\nprint(n)\nplot_2d_space(X_scaled_rs[:,n:n+2], y[:], 'Imbalanced dataset')","d9e9cc5e":"from sklearn.metrics import accuracy_score,classification_report\nfrom sklearn.metrics import confusion_matrix,roc_curve, auc,roc_auc_score\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import KFold,cross_validate,cross_val_predict,cross_val_score,StratifiedKFold\nimport seaborn as sns\n\nExtraTreeModel = ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n                                   max_depth=None, max_features='auto', max_leaf_nodes=None,\n                                   min_impurity_decrease=0.0, min_impurity_split=None,\n                                   min_samples_leaf=1, min_samples_split=2,\n                                   min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n                                   oob_score=False, random_state=None, verbose=0, warm_start=False)","ee6a6c60":"#Train with cross validation\ncv = StratifiedKFold(n_splits=5)\n\ntprs = []\naucs = []\nmean_fpr = np.linspace(0, 1, 100)\n\ni = 0\nfor k_train, k_test in tqdm(cv.split(X_scaled_rs, y)):\n    probas_ = ExtraTreeModel.fit(X_scaled_rs[k_train], y[k_train]).predict_proba(X_scaled_rs[k_test])\n    # Compute ROC curve and area the curve\n    fpr, tpr, thresholds = roc_curve(y[k_test], probas_[:, 1])\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    tprs[-1][0] = 0.0\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n\n    i += 1\nplt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n         label='Chance', alpha=.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nplt.plot(mean_fpr, mean_tpr, color='b',\n         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n         lw=2, alpha=.8)\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\nplt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                 label=r'$\\pm$ 1 std. dev.')\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","ec1c377b":"#Import NB model\nfrom sklearn.naive_bayes import GaussianNB\n\ndef create_NB_Model():\n    model = GaussianNB()\n    return model\n\ndef fit_NB_Model(Model,X,y,cv_splits=5): \n    cv = KFold(n_splits=cv_splits)\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    \n    for train,test in cv.split(X, y):\n        probas_ = Model.fit(X[train], y[train]).predict_proba(X[test])\n        # Compute ROC curve and area the curve\n        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n        tprs.append(interp(mean_fpr, fpr, tpr))\n        tprs[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        \n    mean_tpr = np.mean(tprs, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    return Model,mean_auc\n\ndef get_NB_results(X,y,cv_splits=5):\n    model_aucs  = []\n    models = {}\n    for k in tqdm(range(0,int(X.shape[1]\/2))):\n        Model = create_NB_Model()\n        X_train = X[:,[k,k+200]]#.reshape(-1,1)\n        model,auc = fit_NB_Model(Model,X_train,y,cv_splits=cv_splits)\n        #Append model\n        models[\"Model_\"+str(k)] =model\n        model_aucs.append(auc)\n        print(str(auc)+str(\" AUC Average for Model_\")+str(k))\n        #Fit each Model\n    return models,np.array(model_aucs)\n\ndef make_NB_predictions(X_train,y_train,cv,X_test):\n    #Create a new array to store results\n    train_predictions = np.zeros_like(X_train)\n    test_predictions = np.zeros_like(X_test)\n    #Fit model\n    models,model_aucs = get_NB_results(X_train,y_train,cv_splits=cv)\n    #Iterate over the X\n    for k in range(0,int(X_train.shape[1]\/2)):\n        X_train_pred = X_train[:,[k,k+200]]#.reshape(-1,1)\n        train_probas_ = models.get(\"Model_\"+str(k)).predict_proba(X_train_pred)[:, 1]\n        train_predictions[:, k] = train_probas_\n        \n    for j in range(0,int(X_test.shape[1]\/2)):\n        X_test_pred = X_test[:,[j,j+200]]#.reshape(-1,1)\n        test_probas_ = models.get(\"Model_\"+str(j)).predict_proba(X_test_pred)[:, 1]\n        test_predictions[:, j] = test_probas_\n        \n    return test_predictions,train_predictions\n\ndef save_submit_file(predictions, filename, test_filename=test_path, index_column='ID_code', target_column = 'target'):\n    df_test_submit = pd.read_csv(test_filename).set_index(index_column)\n    df_test_submit[target_column] = predictions\n    df_test_submit[[target_column]].to_csv(filename)\n    return ","1fd24d37":"_,train_predictions = make_NB_predictions(X_train = X_scaled_rs,y_train = y,cv = 5,X_test=X_scaled_rs)","fb4cd871":"from sklearn.linear_model import LogisticRegression\nensemble_model = LogisticRegression()\nensemble_model.fit(X = train_predictions, y = y)","707c6af1":"fpr, tpr, thresholds = roc_curve(y, ensemble_model.predict_proba(train_predictions)[:, 1])\nplot_roc_curve(fpr, tpr)","104bb487":"# Normalize test data\ndf_test = test.drop(columns=['ID_code', 'target'])\nX_test = df_test.values\nX_test_scaled_rs = rs.fit_transform(X_test)","16f048fe":"test_predictions,train_predictions = make_NB_predictions(X_train = X_scaled_rs,y_train = y,cv = 5,X_test=X_test_scaled_rs)","c321198d":"int(X.shape[1]\/2)","5f434237":"ensemble_model.fit(X = train_predictions, y = y)\npredictions = ensemble_model.predict(test_predictions)\n","5073ec9c":"save_submit_file(predictions, \"submission_rev4.csv\")","bc252f67":"from IPython.display import FileLink\nFileLink('submission_rev4.csv')","2b30531d":"# Notebook inspired in the bellow solutions discussed\n* [CNN, Independence, Counts Magic [0.92174 private]](https:\/\/www.kaggle.com\/jganzabal\/cnn-independence-counts-magic-0-92174-private)\n* [List of Fake Samples and Public\/Private LB split](https:\/\/www.kaggle.com\/yag320\/list-of-fake-samples-and-public-private-lb-split)\n* [200 Magical Models - Santander - [0.920]](https:\/\/www.kaggle.com\/cdeotte\/200-magical-models-santander-0-920\/comments)","34ced614":"Maybe we can use the package https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/api.html#module-imblearn.under_sampling","dfb43dfc":"#### According with the discussions related with this competition, the creator include syntetic samples accros all the data\n\n> # Divide fake from real:\n\nThis is taken from https:\/\/www.kaggle.com\/yag320\/list-of-fake-samples-and-public-private-lb-split","eed21e69":"# Download and import packages\nDownload keras-contrib for CyclicLR and santander_helper for auc metric and custom Datagenerator","0d95ad76":"# Load and prepare data","f248406d":"# Define a Model","f3a07382":"Obs. Test some resample techinique before the next steps","44ee3794":"# Add counts to each of the 200 vars\nThis is taken from https:\/\/www.kaggle.com\/cdeotte\/200-magical-models-santander-0-920\/comments, a must read kernel from @cdeotte","85f651cc":"# Make predictions","17b37947":"# Create 200 models and ensemble results"}}