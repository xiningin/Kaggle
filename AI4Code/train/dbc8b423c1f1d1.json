{"cell_type":{"b0868ab0":"code","335e615e":"code","e4abe40f":"code","a4f8610e":"code","3d7d7c3a":"code","3b032959":"code","da6d11e3":"code","c0af3a0b":"code","77737b43":"code","bed1609a":"code","176ce9ac":"code","a304878a":"code","36d3b625":"code","f12484d0":"code","cc810681":"code","b357c58f":"code","ce5b84b3":"code","2ecee007":"code","986aa036":"code","121673f7":"code","9488cbfd":"code","5f755379":"code","9dda636a":"code","927ee56f":"code","e8d7d1ce":"code","4893898f":"code","6b184c39":"code","de3a8dfe":"code","a3b5b0c7":"code","2b772292":"code","590ce50c":"code","082385f2":"code","b7062ad6":"code","850a2f2c":"code","c9e3f8e5":"code","45c12567":"code","ba7a6fb9":"code","81d257fd":"code","5504c520":"code","0773cd29":"code","d8b81da2":"code","141e4349":"code","22b25f62":"code","c9163387":"markdown","8cd57fef":"markdown","2b38b57c":"markdown","717b50f3":"markdown","d42d01df":"markdown","54dbd344":"markdown","3280f206":"markdown","cc5dad07":"markdown","a0ecf20c":"markdown","b2f077ac":"markdown"},"source":{"b0868ab0":"#Installation of required libraries\nimport numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\nimport warnings\nwarnings.simplefilter(action = \"ignore\")\n\nimport plotly.offline as py \npy.init_notebook_mode(connected=True)                  \nimport plotly.graph_objs as go                         \nimport plotly.tools as tls                             \nfrom collections import Counter                        \nimport plotly.figure_factory as ff\n\nimport warnings\nimport missingno as msno\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(\"ignore\", category=ConvergenceWarning)\nwarnings.filterwarnings(\"ignore\")","335e615e":"#Reading the dataset\ndf = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.head()","e4abe40f":"# The size of the data set was examined. It consists of 768 observation units and 9 variables.\ndf.shape","a4f8610e":"#Feature information\ndf.info()","3d7d7c3a":"# Descriptive statistics of the data set accessed.\ndf.describe([0.10,0.25,0.50,0.75,0.90,0.95,0.99]).T","3b032959":"# The distribution of the Outcome variable was examined.\ndf.Outcome.value_counts()\n\ntrace0 = go.Bar(\n            x = df[df[\"Outcome\"]== 0][\"Outcome\"].value_counts().index.values,\n            y = df[df[\"Outcome\"]== 0][\"Outcome\"].value_counts().values,\n            name='Not Diabetes')\n\ntrace1 = go.Bar(\n            x = df[df[\"Outcome\"]== 1][\"Outcome\"].value_counts().index.values,\n            y = df[df[\"Outcome\"]== 1][\"Outcome\"].value_counts().values,\n            name='Diabetes')\n\n\ndata = [trace0, trace1]\nlayout = go.Layout(yaxis=dict(title='Count'),\n                   xaxis=dict(title='Outcome Variable'),title='Outcome Variable Distribution')\n\nfig = go.Figure(data=data, layout=layout)\nfig.data[0].marker.line.width = 4\nfig.data[0].marker.line.color = \"black\"\nfig.data[1].marker.line.width = 4\nfig.data[1].marker.line.color = \"black\"\npy.iplot(fig, filename='grouped-bar')","da6d11e3":"# The histagram of the Age variable was reached.\ndf_diabetes = df.loc[df[\"Outcome\"] == 1]['Age'].values.tolist()\ndf_notdiabetes = df.loc[df[\"Outcome\"] == 0]['Age'].values.tolist()\ndf_age = df['Age'].values.tolist()\n\n#First plot\ntrace0 = go.Histogram(x=df_diabetes, histnorm='probability', name=\"Diabetic\")\n#Second plot\ntrace1 = go.Histogram(x=df_notdiabetes, histnorm='probability', name=\"Not Diabetic\")\n#Third plot\ntrace2 = go.Histogram(x=df_age, histnorm='probability', name=\"Overall Age\")\n\n#Creating the grid\nfig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]], \n                        subplot_titles=('Diabetic','Not Diabetic', 'General Distribuition'))\n\n#setting the figs\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\n\nfig['layout'].update(showlegend=True, title='Age Distribuition', bargap=0.05)\npy.iplot(fig)","c0af3a0b":"figure_row = 4\nfigure_column = 2 \nflag = 0\nfig = tls.make_subplots(rows=figure_row, cols=figure_column)\nfor column in range(1,figure_column+1):\n    for row in range(1,figure_row+1):\n        trace = go.Histogram(x=df[df.columns[flag]].values.tolist(), histnorm='probability', name=df.columns[flag])\n        fig.append_trace(trace, row, column)\n        flag = flag+1\npy.iplot(fig)","77737b43":"# Histogram and density graphs of all variables were accessed.\nfig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.distplot(df.Age, bins = 20, ax=ax[0,0] , color=\"red\") \nsns.distplot(df.Pregnancies, bins = 20, ax=ax[0,1], color=\"red\") \nsns.distplot(df.Glucose, bins = 20, ax=ax[1,0], color=\"red\") \nsns.distplot(df.BloodPressure, bins = 20, ax=ax[1,1], color=\"red\") \nsns.distplot(df.SkinThickness, bins = 20, ax=ax[2,0], color=\"red\")\nsns.distplot(df.Insulin, bins = 20, ax=ax[2,1], color=\"red\")\nsns.distplot(df.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0], color=\"red\") \nsns.distplot(df.BMI, bins = 20, ax=ax[3,1],color=\"red\") ","bed1609a":"# Access to the correlation of the data set was provided. What kind of relationship is examined between the variables. \n# If the correlation value is> 0, there is a positive correlation. While the value of one variable increases, the value of the other variable also increases.\n# Correlation = 0 means no correlation.\n# If the correlation is <0, there is a negative correlation. While one variable increases, the other variable decreases. \n# When the correlations are examined, there are 2 variables that act as a positive correlation to the Salary dependent variable.\n# These variables are Glucose. As these increase, Outcome variable increases.\ndf.corr()","176ce9ac":"# Correlation matrix graph of the data set\nf, ax = plt.subplots(figsize= [20,15])\nsns.heatmap(df.corr(), annot=True, fmt=\".2f\", ax=ax, cmap = \"magma\" )\nax.set_title(\"Correlation Matrix\", fontsize=20)\nplt.show()","a304878a":"df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)","36d3b625":"# Now we observe how many missing data are.\ndf.isnull().sum()","f12484d0":"# Have been visualized using the missingno library for the visualization of missing observations.\n# Plotting \nimport missingno as msno\nmsno.bar(df);","cc810681":"# The missing values will be filled with the median values of each variable.\n\ndef median_target(var):   \n    temp = df[df[var].notnull()]\n    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n    return temp","b357c58f":"# The values to be given for incomplete observations are given the median value of people who are not sick and the median values of people who are sick.\ncolumns = df.columns\ncolumns = columns.drop(\"Outcome\")\nfor i in columns:\n    median_target(i)\n    df.loc[(df['Outcome'] == 0 ) & (df[i].isnull()), i] = median_target(i)[i][0]\n    df.loc[(df['Outcome'] == 1 ) & (df[i].isnull()), i] = median_target(i)[i][1]","ce5b84b3":"# Missing values were filled.\ndf.isnull().sum()","2ecee007":"msno.bar(df);","986aa036":"# In the data set, there were asked whether there were any outlier observations compared to the 25% and 75% quarters.\n# It was found to be an outlier observation.\nfor feature in df:\n    \n    Q1 = df[feature].quantile(0.25)\n    Q3 = df[feature].quantile(0.75)\n    IQR = Q3-Q1\n    lower = Q1- 1.5*IQR\n    upper = Q3 + 1.5*IQR\n    \n    if df[(df[feature] > upper)].any(axis=None):\n        print(feature,\"yes\")\n    else:\n        print(feature, \"no\")","121673f7":"figure_row = 4\nfigure_column = 2 \nflag = 0\nfig = tls.make_subplots(rows=figure_row, cols=figure_column)\nfor column in range(1,figure_column+1):\n    for row in range(1,figure_row+1):\n        trace = go.Box(x = df[df[\"Outcome\"]== 1][df.columns[flag]],\n                       name = df.columns[flag])\n        fig.append_trace(trace, row, column)\n        flag = flag+1\n\nfig['layout'].update(showlegend=True, title='Data Distribuition - Diabetic', bargap=0.05)\npy.iplot(fig)","9488cbfd":"figure_row = 4\nfigure_column = 2 \nflag = 0\nfig = tls.make_subplots(rows=figure_row, cols=figure_column)\nfor column in range(1,figure_column+1):\n    for row in range(1,figure_row+1):\n        trace = go.Box(x = df[df[\"Outcome\"]== 0][df.columns[flag]],\n                       name = df.columns[flag])\n        fig.append_trace(trace, row, column)\n        flag = flag+1\n\nfig['layout'].update(showlegend=True, title='Data Distribuition - Not Diabetic', bargap=0.05)\npy.iplot(fig)","5f755379":"#We conduct a stand alone observation review for the Insulin variable\n#We suppress contradictory values\nQ1 = df.Insulin.quantile(0.25)\nQ3 = df.Insulin.quantile(0.75)\nIQR = Q3-Q1\nlower = Q1 - 1.5*IQR\nupper = Q3 + 1.5*IQR\ndf.loc[df[\"Insulin\"] > upper,\"Insulin\"] = upper","9dda636a":"fig = tls.make_subplots(1, 1)\ntrace1 = go.Box(x = df[\"Insulin\"])\nfig.append_trace(trace1, 1, 1)\npy.iplot(fig)","927ee56f":"# We determine outliers between all variables with the LOF method\nfrom sklearn.neighbors import LocalOutlierFactor\nlof =LocalOutlierFactor(n_neighbors= 10)\nlof.fit_predict(df)","e8d7d1ce":"df_scores = lof.negative_outlier_factor_\nnp.sort(df_scores)[0:30]","4893898f":"#We choose the threshold value according to lof scores\nthreshold = np.sort(df_scores)[7]\nthreshold","6b184c39":"#We delete those that are higher than the threshold\noutlier = df_scores > threshold\ndf = df[outlier]","de3a8dfe":"# The size of the data set was examined.\ndf.shape","a3b5b0c7":"# According to BMI, some ranges were determined and categorical variables were assigned.\nNewBMI = pd.Series([\"Underweight\", \"Normal\", \"Overweight\", \"Obesity 1\", \"Obesity 2\", \"Obesity 3\"], dtype = \"category\")\ndf[\"NewBMI\"] = NewBMI\ndf.loc[df[\"BMI\"] < 18.5, \"NewBMI\"] = NewBMI[0]\ndf.loc[(df[\"BMI\"] > 18.5) & (df[\"BMI\"] <= 24.9), \"NewBMI\"] = NewBMI[1]\ndf.loc[(df[\"BMI\"] > 24.9) & (df[\"BMI\"] <= 29.9), \"NewBMI\"] = NewBMI[2]\ndf.loc[(df[\"BMI\"] > 29.9) & (df[\"BMI\"] <= 34.9), \"NewBMI\"] = NewBMI[3]\ndf.loc[(df[\"BMI\"] > 34.9) & (df[\"BMI\"] <= 39.9), \"NewBMI\"] = NewBMI[4]\ndf.loc[df[\"BMI\"] > 39.9 ,\"NewBMI\"] = NewBMI[5]","2b772292":"# A categorical variable creation process is performed according to the insulin value.\ndef set_insulin(row):\n    if row[\"Insulin\"] >= 16 and row[\"Insulin\"] <= 166:\n        return \"Normal\"\n    else:\n        return \"Abnormal\"","590ce50c":"# The operation performed was added to the dataframe.\ndf = df.assign(NewInsulinScore=df.apply(set_insulin, axis=1))\n\ndf.head()","082385f2":"# Some intervals were determined according to the glucose variable and these were assigned categorical variables.\nNewGlucose = pd.Series([\"Low\", \"Normal\", \"Overweight\", \"Secret\", \"High\"], dtype = \"category\")\ndf[\"NewGlucose\"] = NewGlucose\ndf.loc[df[\"Glucose\"] <= 70, \"NewGlucose\"] = NewGlucose[0]\ndf.loc[(df[\"Glucose\"] > 70) & (df[\"Glucose\"] <= 99), \"NewGlucose\"] = NewGlucose[1]\ndf.loc[(df[\"Glucose\"] > 99) & (df[\"Glucose\"] <= 126), \"NewGlucose\"] = NewGlucose[2]\ndf.loc[df[\"Glucose\"] > 126 ,\"NewGlucose\"] = NewGlucose[3]","b7062ad6":"df.head()","850a2f2c":"# Here, by making One Hot Encoding transformation, categorical variables were converted into numerical values. It is also protected from the Dummy variable trap.\ndf = pd.get_dummies(df, columns =[\"NewBMI\",\"NewInsulinScore\", \"NewGlucose\"], drop_first = True)\ndf.head()","c9e3f8e5":"categorical_df = df[['NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight','NewBMI_Underweight',\n                     'NewInsulinScore_Normal','NewGlucose_Low','NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret']]\ncategorical_df.head()","45c12567":"y = df[\"Outcome\"]\nX = df.drop([\"Outcome\",'NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight','NewBMI_Underweight',\n                     'NewInsulinScore_Normal','NewGlucose_Low','NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret'], axis = 1)\ncols = X.columns\nindex = X.index\nX.head()","ba7a6fb9":"# The variables in the data set are an effective factor in increasing the performance of the models by standardization.  \n# There are multiple standardization methods. These are methods such as\" Normalize\",\" MinMax\",\" Robust\" and \"Scale\".\nfrom sklearn.preprocessing import RobustScaler\ntransformer = RobustScaler().fit(X)\nX = transformer.transform(X)\nX = pd.DataFrame(X, columns = cols, index = index)","81d257fd":"X = pd.concat([X,categorical_df], axis = 1)\nX.head()","5504c520":"y.head()","0773cd29":"# Validation scores of all base models\n\nmodels = []\nmodels.append(('LR', LogisticRegression(random_state = 12345)))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier(random_state = 12345)))\nmodels.append(('RF', RandomForestClassifier(random_state = 12345)))\nmodels.append(('SVM', SVC(gamma='auto', random_state = 12345)))\nmodels.append(('XGB', GradientBoostingClassifier(random_state = 12345)))\nmodels.append((\"LightGBM\", LGBMClassifier(random_state = 12345)))\n\n# evaluate each model in turn\nresults = []\nnames = []","d8b81da2":"for name, model in models:\n    \n        kfold = KFold(n_splits = 10, random_state = 12345)\n        cv_results = cross_val_score(model, X, y, cv = 10, scoring= \"accuracy\")\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,10))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","141e4349":"#########################\n##### RANDOM FOREST #####\n#####      LGBM     #####\n#####      XGB      #####\n#########################\n\nrf_params = {\"n_estimators\" :[100,200,500,1000], \n             \"max_features\": [3,5,7], \n             \"min_samples_split\": [2,5,10,30],\n             \"max_depth\": [3,5,8,None]}\n\nlgbm_params = {\"learning_rate\": [0.01, 0.03, 0.05, 0.1, 0.5],\n               \"n_estimators\": [500, 1000, 1500],\n               \"max_depth\":[3,5,8]}\n\nxgb_params = {\"learning_rate\": [0.01, 0.1, 0.2, 1],\n              \"min_samples_split\": np.linspace(0.1, 0.5, 10),\n              \"max_depth\":[3,5,8],\n              \"subsample\":[0.5, 0.9, 1.0],\n              \"n_estimators\": [100,1000]}\n\nrf_model = RandomForestClassifier(random_state = 357)\nlgbm = LGBMClassifier(random_state = 357)\nxgb = GradientBoostingClassifier(random_state = 357)\n\nrf_cv_model = GridSearchCV(rf_model, rf_params, cv = 10, n_jobs = -1,verbose = 1).fit(X, y)\nlgbm_cv_model = GridSearchCV(lgbm, lgbm_params,  cv = 10, n_jobs = -1, verbose = 1).fit(X, y)\nxgb_cv_model  = GridSearchCV(xgb,xgb_params, cv = 10, n_jobs = -1, verbose = 1).fit(X, y)\n\nrf_tuned = RandomForestClassifier(**rf_cv_model.best_params_).fit(X,y)\nlgbm_tuned = LGBMClassifier(**lgbm_cv_model.best_params_).fit(X,y)\nxgb_tuned = GradientBoostingClassifier(**xgb_cv_model.best_params_).fit(X,y)","22b25f62":"tuned = [rf_tuned,lgbm_tuned,xgb_tuned]\nprint(\"Best Model Parameters and Scores:\")\nfor i in tuned:\n    score = cross_val_score(i, X, y, cv = 10).mean()\n    print(i,\"score:\",score)","c9163387":"<a id='7'><\/a><center> <h1 style=\"background-color:orange; color:white\" ><br>One Hot Encoding<br><\/h1>\n<b>Categorical variables in the data set should be converted into numerical values. For this reason, these transformation processes are performed with Label Encoding and One Hot Encoding method.<\/b><\/center>","8cd57fef":"<a id='9'><\/a><center> <h1 style=\"background-color:orange; color:white\" ><br>Model Tuning and Result<br><\/h1>","2b38b57c":"<!---------------------------------------------->\n<!-----------------ANA BA\u015eLIK------------------->\n<!---------------------------------------------->\n<center> <h1 style=\"background-color:orange; color:white\"><br>Diabetes Prediction using Machine Learning<br><\/h1><\/center>\n\n\n<!---------------------------------------------->\n<!------------------A\u00c7IKLAMA-------------------->\n<!---------------------------------------------->\n<br>\nDiabetes mellitus, commonly known as diabetes, is a metabolic disease that causes high blood sugar. The hormone insulin moves sugar from the blood into your cells to be stored or used for energy. With diabetes, your body either doesn\u2019t make enough insulin or can\u2019t effectively use the insulin it does make. Untreated high blood sugar from diabetes can damage your nerves, eyes, kidneys, and other organs.There are a few different types of diabetes:<br \/><br \/>\n\n<b>-<\/b> Type 1 diabetes is an autoimmune disease. The immune system attacks and destroys cells in the pancreas, where insulin is made. It\u2019s unclear what causes this attack. About 10 percent of people with diabetes have this type.<br \/>\n<b>-<\/b> Type 2 diabetes occurs when your body becomes resistant to insulin, and sugar builds up in your blood.<br \/>\n<b>-<\/b> Prediabetes occurs when your blood sugar is higher than normal, but it\u2019s not high enough for a diagnosis of type 2 diabetes.<br \/>\n<b>-<\/b> Gestational diabetes is high blood sugar during pregnancy. Insulin-blocking hormones produced by the placenta cause this type of diabetes.<br \/><br \/>\nA rare condition called diabetes insipidus is not related to diabetes mellitus, although it has a similar name. It\u2019s a different condition in which your kidneys remove too much fluid from your body.<br \/>\n<\/br>\n\n\n<!---------------------------------------------->\n<!------------VER\u0130 SET\u0130 HAKKINDA B\u0130LG\u0130---------->\n<!---------------------------------------------->\n\n<center> <h1 style=\"background-color:orange; color:white\" ><br>Details about the dataset<br><\/h1><\/center>\n<br>\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, BMI, insulin level, age etc.\n<br><br>\n\n\n<!---------------------------------------------->\n<!-----VER\u0130 SET\u0130 DE\u011e\u0130\u015eKENLER\u0130 HAKKINDA B\u0130LG\u0130---->\n<!---------------------------------------------->\n\n<p>\n- <b>Pregnancies:<\/b> Number of times pregnant<br \/>\n- <b>Glucose:<\/b> Plasma glucose concentration a 2 hours in an oral glucose tolerance test<br \/>\n- <b>BloodPressure:<\/b> Diastolic blood pressure (mm Hg)<br \/>\n- <b>SkinThickness:<\/b> Triceps skin fold thickness (mm)<br \/>\n- <b>Insulin:<\/b> 2-Hour serum insulin (mu U\/ml)<br \/>\n- <b>BMI:<\/b> Body mass index (weight in kg\/(height in m)^2)<br \/>\n- <b>DiabetesPedigreeFunction:<\/b> Diabetes pedigree function<br \/>\n- <b>Age:<\/b> Age (years)<br \/>\n- <b>Outcome:<\/b> Class variable (0 or 1)<br \/>\n<\/p>\n\n<!---------------------------------------------->\n<!------------------G\u00d6RSEL---------------------->\n<!---------------------------------------------->\n\n<center><img src=\"http:\/\/i.hizliresim.com\/WzLcvu.jpg\" style=\"width:30%;height:10%;\"><\/center>\n   \n\n<!---------------------------------------------->\n<!-------------------TABLO---------------------->\n<!---------------------------------------------->\n<br>    \n<center>    \n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action\" style=\"background-color:orange; color:white\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"><br>Notebook Content!<br><\/h3>  \n  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\" style=\"color:orange\">Exploratory Data Analysis<span        class=\"badge badge-primary badge-pill\" style=\"background-color:orange; color:white\">1<\/span><\/a>\n  \n  <a id=\"section2\" class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\" aria-controls=\"messages\" style=\"color:orange\">Data Preprocessing<span class=\"badge badge-primary badge-pill\" style=\"background-color:orange; color:white\">2<\/span><\/a>\n  \n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"settings\" style=\"color:orange\">Missing Observation Analysis<span class=\"badge badge-primary badge-pill\" style=\"background-color:orange; color:white\">3<\/span><\/a>\n  \n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"settings\" style=\"color:orange\">Outlier Observation Analysis<span class=\"badge badge-primary badge-pill\" style=\"background-color:orange; color:white\">4<\/span><\/a>\n  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#5\" role=\"tab\" aria-controls=\"settings\" style=\"color:orange\">Local Outlier Factor (LOF)<span class=\"badge badge-primary badge-pill\" style=\"background-color:orange; color:white\">5<\/span><\/a> \n  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#6\" role=\"tab\" aria-controls=\"settings\" style=\"color:orange\">Feature Engineering<span class=\"badge badge-primary badge-pill\" style=\"background-color:orange; color:white\">6<\/span><\/a>\n\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#7\" role=\"tab\" aria-controls=\"settings\" style=\"color:orange\">One Hot Encoding<span class=\"badge badge-primary badge-pill\" style=\"background-color:orange; color:white\">6<\/span><\/a> \n  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#8\" role=\"tab\" aria-controls=\"settings\" style=\"color:orange\">Base Model<span class=\"badge badge-primary badge-pill\" style=\"background-color:orange; color:white\">7<\/span><\/a> \n  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#9\" role=\"tab\" aria-controls=\"settings\" style=\"color:orange\">Model Tuning and Result<span class=\"badge badge-primary badge-pill\" style=\"background-color:orange; color:white\">8<\/span><\/a>\n  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#10\" role=\"tab\" aria-controls=\"settings\" style=\"color:orange\">Reporting Diabetes Prediction Using Machine Learning<span class=\"badge badge-primary badge-pill\" style=\"background-color:orange; color:white\">13<\/span><\/a> \n\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#final\" role=\"tab\" aria-controls=\"settings\" style=\"color:black\">The model created as a result of XGBoost hyperparameter optimization became the model with the lowest Cross Validation Score value. (0.90)<span class=\"badge badge-primary badge-pill\" style=\"background-color:orange; color:white\">Result<\/span><\/a> <\/center>","717b50f3":"<a id='5'><\/a><center> <h1 style=\"background-color:orange; color:white\" ><br>Local Outlier Factor (LOF)<br><\/h1>","d42d01df":"<a id='2'><\/a><center> <h1 style=\"background-color:orange; color:white\" ><br>Data Preprocessing<br><\/h1><\/center>\n<a id='3'><\/a>\n<center> <h1 style=\"background-color:orange; color:white\" ><br>Missing Observation Analysis<br><\/h1>\n<b>\nWe saw that some properties contain 0, it doesn't make sense here. Because it is impossible for these values to be 0 for a human. Therefore, the value 0 is actually missing data. If we change the values from 0 to NaN values, it can be observed how many missing data are.<\/b><\/center>","54dbd344":"<a id='9'><\/a><center> <h1 style=\"background-color:orange; color:white\" ><br>Report<br><\/h1><\/center>\nThe aim of this study was to create classification models for the diabetes data set and to predict whether a person is sick by establishing models and to obtain maximum validation scores in the established models. The work done is as follows:\n\n<b>1)<\/b> Diabetes Data Set read.<br \/>\n\n<b>2)<\/b> With Exploratory Data Analysis; The data set's structural data were checked. The types of variables in the dataset were examined. Size information of the dataset was accessed. The 0 values in the data set are missing values. Primarily these 0 values were replaced with NaN values. Descriptive statistics of the data set were examined.<br \/>\n\n<b>3)<\/b> Data Preprocessing section; df for: The NaN values missing observations were filled with the median values of whether each variable was sick or not. The outliers were determined by LOF and dropped. The X variables were standardized with the rubost method..<br \/>\n\n<b>4)<\/b> During Model Building; Logistic Regression, KNN, SVM, CART, Random Forests, XGBoost, LightGBM like using machine learning models Cross Validation Score were calculated. Later Random Forests, XGBoost, LightGBM hyperparameter optimizations optimized to increase Cross Validation value.<br \/>\n\n<b>5)<\/b> Result; The model created as a result of XGBoost hyperparameter optimization became the model with the lowest Cross Validation Score value. (0.90)<br \/>","3280f206":"<a id='4'><\/a><center> <h1 style=\"background-color:orange; color:white\" ><br>Outlier Observation Analysis<br><\/h1>","cc5dad07":"<a id='6'><\/a><center> <h1 style=\"background-color:orange; color:white\" ><br>Feature Engineering<br><\/h1>\n<b>Creating new variables is important for models. But you need to create a logical new variable. For this data set, some new variables were created according to BMI, Insulin and glucose variables.<\/b><\/center>","a0ecf20c":"<a id='1'><\/a><center> <h1 style=\"background-color:orange; color:white\" ><br>Exploratory Data Analysis<br><\/h1>","b2f077ac":"<a id='8'><\/a><center> <h1 style=\"background-color:orange; color:white\" ><br>Base Models<br><\/h1>"}}