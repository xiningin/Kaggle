{"cell_type":{"f9c28254":"code","d051e71c":"code","72960454":"markdown","65264cf8":"markdown","8c19147f":"markdown"},"source":{"f9c28254":"MODEL_NAME = \"..\/input\/huggingface-roberta-variants\/roberta-base\/roberta-base\"\nEPOCHS = 1","d051e71c":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n\ndef load_dfs():\n    train_csv = '..\/input\/commonlitreadabilityprize\/train.csv'\n    test_csv = '..\/input\/commonlitreadabilityprize\/test.csv'\n    df_train = pd.read_csv(train_csv)[[\"excerpt\", \"target\"]].rename(columns={\"target\": \"label\", \"excerpt\": \"text\"})\n    df_test = pd.read_csv(test_csv)[[\"id\", \"excerpt\"]].rename(columns={ \"excerpt\": \"text\"})\n    return df_train, df_test\n\ndef rmse(y_true, y_pred): return np.sqrt(((y_true - y_pred) ** 2).mean().item())\n    \ndef compute_metrics(pred_results):\n    y_pred = pred_results.predictions.squeeze()\n    y_true = pred_results.label_ids\n    return {\"rmse\": rmse(y_true, y_pred)}\n\ndef submit(trainer, ds_test):\n    sample_sub_csv = '..\/input\/commonlitreadabilityprize\/sample_submission.csv'\n    pred_csv = '\/kaggle\/working\/submission.csv'\n    pred_results = trainer.predict(ds_test)\n    y_pred = pred_results.predictions.squeeze()\n    df_res = pd.read_csv(sample_sub_csv)\n    df_res['target'] = y_pred.tolist()\n    df_res.to_csv(pred_csv, index=False)\n\ndef tokenize(tokenizer, df_train, df_val, df_test):    \n    train_tokenized = tokenizer(df_train['text'].tolist(), padding=\"max_length\", truncation=True, max_length=512)\n    val_tokenized = tokenizer(df_val['text'].tolist(), padding=\"max_length\", truncation=True, max_length=512)\n    test_tokenized = tokenizer(df_test['text'].tolist(), padding=\"max_length\", truncation=True, max_length=512)\n    train_tokenized['label'] = df_train['label'].tolist()\n    val_tokenized['label'] = df_val['label'].tolist()\n    ds_train = [dict(zip(train_tokenized,t)) for t in zip(*train_tokenized.values())]\n    ds_val = [dict(zip(val_tokenized,t)) for t in zip(*val_tokenized.values())]\n    ds_test = [dict(zip(test_tokenized,t)) for t in zip(*test_tokenized.values())]\n    return ds_train, ds_val, ds_test\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1)\ndf_base, df_test = load_dfs()\ndf_train, df_val = train_test_split(df_base, test_size=0.066)\nds_train, ds_val, ds_test = tokenize(tokenizer, df_train, df_val, df_test)\nargs = TrainingArguments(\"\/kaggle\/working\/model\/\", num_train_epochs=EPOCHS, \n                         evaluation_strategy=\"steps\", eval_steps=100, report_to=\"none\")\ntrainer = Trainer(model=model, args=args, train_dataset=ds_train, eval_dataset=ds_val, \n                  compute_metrics=compute_metrics)\ntrainer.train()\nsubmit(trainer, ds_test)","72960454":"## \ud83e\udd17\ud83e\udd17 Thanks for reading this notebook! Remember to upvote if you found it useful, and stay tuned for the next deliveries! \ud83e\udd17\ud83e\udd17","65264cf8":"We had imported [Huggingface Roberta Variants](https:\/\/www.kaggle.com\/sauravmaheshkar\/huggingface-roberta-variants) instead of BERT this time.\nAnd we are using 1 epoch because this is what gave better results (compared against 3).\n","8c19147f":"# 2- Learning \ud83e\udd17  - Out-of-the-box RoBERTa [LB: 0.53]\n\nHi, and welcome! This is the second kernel of the series `Learning \ud83e\udd17`, a personal project I'm currently working on. I am an experienced data scientist diving into the hugging face transformers library and this series or kernels is a \"working diary\", as I do it. The approach I'm taking is the following: \n1. Explore various out-of-the-box models, without digging into their technical details. \n2. After that, I'll start going over the best ranked public kernels, understand their ideas, and reproduce them by myself. \n\nYou are invited to follow me in this journey. In this short kernel  we fine-tune an out-of-the-box cased RoBERTa, with just the minimal set up required for it to run in this competition, obtaining a leaderboard score of `0.53`. \n\n\nThis is an ongoing project, so expect more notebooks to be added to the series soon. Actually, we are currently working on the following ones:\n\n\n1. [Learning \ud83e\udd17  - Out-of-the-box BERT [LB: 0.577]](https:\/\/www.kaggle.com\/julian3833\/1-learning-out-of-the-box-bert-lb-0-577)\n2. [Learning \ud83e\udd17 - Out-of-the-box RoBERTa [LB: 0.53]](https:\/\/www.kaggle.com\/julian3833\/2-learning-out-of-the-box-roberta-lb-0-53) (this notebook)\n3. [Learning \ud83e\udd17 - Out-of-the-box Electra [LB: 0.58]](https:\/\/www.kaggle.com\/julian3833\/3-learning-out-of-the-box-electra-lb\/) \n4. _Learning \ud83e\udd17 - Minimal fine tuning (WIP)_\n5. _Learning \ud83e\udd17 - Preprocessing (WIP)_\n6. _Learning \ud83e\udd17 - Reviewing public kernels (WIP)_\n7. _Learning \ud83e\udd17 - Intra-domain pre training RoBERTa (WIP)_\n\n\n\n## This notebook\n\nThe code below is just a copy of the code in [1- Learning \ud83e\udd17  - Out-of-the-box BERT [LB: 0.577]](https:\/\/www.kaggle.com\/julian3833\/1-learning-out-of-the-box-bert-lb-0-577) with just 2 changes, which are the following ones. Refer to that notebook for a more detailed description of the process and a more verbose, commented code."}}