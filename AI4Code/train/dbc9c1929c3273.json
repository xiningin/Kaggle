{"cell_type":{"1b5396e7":"code","ec55903d":"code","972a0cc9":"code","22d5077c":"code","66b83def":"code","c99a938e":"code","91d162d5":"code","a1db308a":"code","8871683a":"code","2598a490":"code","3abc4f75":"code","fa276d99":"code","bef08332":"code","49eb8fe6":"code","5e11266e":"code","30644650":"code","23373e6f":"code","36a33f25":"code","d70395a8":"code","607c3742":"code","879e57ce":"code","e07ad8d8":"code","b43f7a77":"markdown","aef8f5a4":"markdown","27291607":"markdown","b87af39e":"markdown","4b845de6":"markdown","1183dead":"markdown","0eefc72d":"markdown","a7ce91fa":"markdown","31e7c752":"markdown","ebf0f4da":"markdown","d7d89c45":"markdown","89d482d2":"markdown","52b6c913":"markdown"},"source":{"1b5396e7":"import numpy as np \nimport pandas as pd\npd.set_option('display.max_rows', 800)\npd.set_option('display.max_columns', 500)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# import all libraries and dependencies for machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV \n\nimport warnings\nwarnings.filterwarnings('ignore')","ec55903d":"df = pd.read_csv(\"..\/input\/melbourne-housing-market\/Melbourne_housing_FULL.csv\")\ndf.head()","972a0cc9":"df.info()","22d5077c":"df.describe()  ","66b83def":"df.head()","c99a938e":"num_col = df.select_dtypes(include=np.number).columns\nprint(\"Numerical columns: \\n\",num_col)\n\ncat_col = df.select_dtypes(exclude=np.number).columns\nprint(\"Categorical columns: \\n\",cat_col)","91d162d5":"df.drop([\"Address\",\"Date\",\"Postcode\",\"Lattitude\",\"Longtitude\",\"Date\"], axis=1,inplace=True)\ndf.dropna(inplace=True)\n\n# Import label encoder \nfrom sklearn import preprocessing \n  \n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \n# Encode labels in column . \n\ndf['Suburb']= label_encoder.fit_transform(df['Suburb'])\ndf['Type']= label_encoder.fit_transform(df['Type'])\ndf['Method']= label_encoder.fit_transform(df['Method'])\ndf['SellerG']= label_encoder.fit_transform(df['SellerG'])\ndf['Regionname']= label_encoder.fit_transform(df['Regionname'])\ndf['CouncilArea']= label_encoder.fit_transform(df['CouncilArea'])\n  \ndf.head()\n","a1db308a":"# Let's check the distribution of y variable\nplt.figure(figsize=(10,10), dpi= 80)\nsns.boxplot(df['Price'])\nplt.title('Price')\nplt.show()","8871683a":"plt.figure(figsize=(8,8))\nplt.title('Price Distribution Plot')\nsns.distplot(df['Price'])","2598a490":"num_col = df.select_dtypes(include=np.number).columns\nprint(\"Numerical columns: \\n\",num_col)\n\ncat_col = df.select_dtypes(exclude=np.number).columns\nprint(\"Categorical columns: \\n\",cat_col)","3abc4f75":"# Let's check the multicollinearity of features by checking the correlation matric\n\nplt.figure(figsize=(15,15))\np=sns.heatmap(df[num_col].corr(), annot=True,cmap='RdYlGn',center=0)","fa276d99":"# Train test split\nX = df.drop(['Price'], axis = 1)\ny = df['Price']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=500)","bef08332":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","49eb8fe6":"gbr = GradientBoostingRegressor(learning_rate = 0.05, random_state = 100)\ngbr.fit(X_train,y_train)\ny_pred = gbr.predict(X_test)\n\nprint(\"r2 score : \",r2_score(y_test,y_pred))\nprint(\"MAPE     : \",mean_absolute_percentage_error(y_test,y_pred))","5e11266e":"gbr = GradientBoostingRegressor(learning_rate = 0.1, random_state = 100)\ngbr.fit(X_train,y_train)\ny_pred = gbr.predict(X_test)\n\nprint(\"r2 score : \",r2_score(y_test,y_pred))\nprint(\"MAPE     : \",mean_absolute_percentage_error(y_test,y_pred))","30644650":"gbr = GradientBoostingRegressor(random_state = 100)\n\n# defining parameter range \nparam_grid={'n_estimators':[100,200,300], \n            'learning_rate': [0.2,0.3,0.5],\n            'max_depth':[2,3,5], \n            'min_samples_leaf':[1,3,5]}   \n  \ngrid = GridSearchCV(gbr, param_grid, refit = True, verbose = 3, n_jobs = -1) \n  \n# fitting the model for grid search \ngrid.fit(X_train, y_train)","23373e6f":"# Best parameter after hyper parameter tuning \nprint(grid.best_params_) \n  \n# Moel Parameters \nprint(grid.best_estimator_)","36a33f25":"# Prediction using best parameters\ngrid_predictions = grid.predict(X_test) \n  \nprint(\"r2 score : \",r2_score(y_test,grid_predictions))\nprint(\"MAPE     : \",mean_absolute_percentage_error(y_test,grid_predictions))","d70395a8":"#You can still decrease the mape by trying out different values for estimators ,learning depth and other factors,\n#but be mindful that trying out of more values means it will lead to pressure on your RAM and the process will take a lot of time\n#Maybe hours as well and your computer might get hanged in between, so do it only if you have powerful gpu and good ram.","607c3742":"!pip install autoviz","879e57ce":"import pandas as pd \nfrom autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()","e07ad8d8":"df = AV.AutoViz(filename=\"..\/input\/melbourne-housing-market\/Melbourne_housing_FULL.csv\",sep=',', depVar='Price', dfte=df, header=0, verbose=2, \n                 lowess=False, chart_format='svg', max_rows_analyzed=150000, max_cols_analyzed=30)","b43f7a77":"## Gradient Boosting Regression\n\n- learning_rate = 0.1","aef8f5a4":"# Gradient Boosting Regression","27291607":"## 3. Understanding the data","b87af39e":"## Grid Search\n","4b845de6":"## 6. Model Building","1183dead":"## 2. Load Data","0eefc72d":"## 5. Exploratory Data Analysis","a7ce91fa":"## Gradient Boosting Regression\n\n- learning_rate = 0.05","31e7c752":"## 1. Import Libraries","ebf0f4da":"## 4. Data Pre-processing","d7d89c45":"**Summary :**\n\nMAPE is quite higher, so let's try to tune the parameter again","89d482d2":"**Summary :**\n\nMAPE has improved as compared to the earlier model, let's try to tune the parameter using gridsearch","52b6c913":"1. Convert Business Problem to Data Science Problem\n2. Load Data\n3. Understand the Data\n4. Data Preprocessing\n5. Exploratory Data Analysis\n6. Model Building\n7. Predictions and Evaluations\n8. Hyperparameter Tuning"}}