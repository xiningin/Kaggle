{"cell_type":{"2a3aac5a":"code","56be805c":"code","3fd7515c":"code","730430c0":"code","5442465c":"code","ded10b85":"code","74e1c181":"code","74f3cb7d":"code","99502f40":"code","a8f6806e":"code","96f33705":"code","4580bb35":"code","209d5754":"code","aca28598":"code","ba52bc81":"code","b5addd6a":"code","aa8222b0":"code","ab4f0d87":"code","0237d458":"code","65d7afcf":"code","8cfd44ea":"code","73ef9754":"code","92ecf736":"code","49a31db7":"code","39d1e9f5":"code","5002b42a":"code","cba0e768":"code","eb662fe5":"code","f927ca6a":"code","e27b832e":"code","b2dfd50e":"code","a0a57495":"code","277e6eaa":"code","1bfa9d17":"code","237b9986":"code","cdb9b487":"code","648f7d78":"code","800b95fe":"code","5de53d7e":"code","3a38610e":"code","535202e6":"code","2005a092":"code","8ef9e6e3":"code","c954801f":"code","be383fd5":"code","ed2d6cd7":"code","8e037751":"code","bd7cb95d":"code","05f76123":"code","d70a0ebc":"code","502d0d63":"code","0226f736":"code","01e9e0c8":"code","a6011287":"code","47bdbb9f":"code","0a77873a":"code","f06174ca":"code","988b3ed8":"code","9e01b442":"code","da4b2d81":"code","f80eea65":"code","890463f9":"code","857701ee":"code","e3efe64a":"code","fb181158":"code","96dd2a66":"code","624c8a1e":"code","f23ca328":"code","5e55d927":"code","eb307218":"code","89675ccb":"code","086915a4":"code","4871944b":"code","274dadc9":"code","1ac9e22a":"code","6bc0c726":"code","d19bd5eb":"code","8a2dbc7a":"code","9da4ca53":"code","f3a78e75":"code","77bb7673":"code","01c2a807":"code","1ddde6a8":"code","58288976":"code","f0e1e39e":"code","56335602":"code","52129ddc":"code","d6113e62":"code","7bf65b0c":"code","d7a46498":"code","035e4d71":"code","6a1b883e":"code","92ec2a9d":"code","b609c44e":"code","b8467677":"code","04813c5f":"code","19303cd4":"code","0b591a5b":"code","dfaa988d":"markdown","4a999f48":"markdown","79400b82":"markdown","54c2df29":"markdown","fcfba2f9":"markdown","c92beb45":"markdown","31185d3a":"markdown","de98fff4":"markdown","ff6c46f8":"markdown","b3de5786":"markdown","15bc322e":"markdown","183840af":"markdown","13e1af9c":"markdown","1bae8d3c":"markdown","98785936":"markdown","c633f7ce":"markdown","91829b92":"markdown","85b10924":"markdown","6e412886":"markdown","ccc3f43d":"markdown","8830e6bc":"markdown","17fb9f58":"markdown","f02cb9f6":"markdown","5b5f521f":"markdown","93bd6a2b":"markdown","3a2dcb40":"markdown","740d298a":"markdown","84a7d024":"markdown","84ad50b2":"markdown","294b8122":"markdown","9158b81c":"markdown","2ea69a7f":"markdown","f3f18f23":"markdown","6e5c4877":"markdown","9c416a50":"markdown","bc1ee424":"markdown","2850dd1b":"markdown","d21fff3d":"markdown","9eef76d7":"markdown","64a1ff07":"markdown","dd3ace08":"markdown","ccf8b3f1":"markdown","ed6fcd65":"markdown","89247141":"markdown","45ca046c":"markdown"},"source":{"2a3aac5a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# use the inline backend to generate the plots within the browser\n%matplotlib inline\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nmpl.style.use('ggplot')  # optional: for ggplot-like style\n\n# check for latest version of Matplotlib\nprint('Matplotlib version: ', mpl.__version__) # >= 2.0.0\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","56be805c":"missing_values = [\"n\/a\", \"na\", \"--\",\" \"]\ntrain= pd.read_csv('..\/input\/housepricesadvanced-regression-techniques\/train (1).csv',na_values = missing_values)\ntest= pd.read_csv('..\/input\/housepricesadvanced-regression-techniques\/test (1).csv',na_values = missing_values)\ntrain.shape","3fd7515c":"train.columns","730430c0":"train['SalePrice'].describe()","5442465c":"sns.distplot(train['SalePrice']);","ded10b85":"scatter_data=pd.concat([train['SalePrice'], train['GrLivArea']], axis=1)\nscatter_data.head()","74e1c181":"scatter_data.plot(kind='scatter', x='GrLivArea', y='SalePrice', figsize=(10, 6), color='darkblue')\nplt.title('sale price vs grlivarea')\nplt.xlabel('grlivarea')\nplt.ylabel('sale price ')\n\nplt.show()","74f3cb7d":"scatter_data=pd.concat([train['SalePrice'], train['TotalBsmtSF']], axis=1)\nscatter_data.head()","99502f40":"scatter_data.plot(kind='scatter', x='TotalBsmtSF', y='SalePrice', figsize=(10, 6), color='darkblue')\n\nplt.title('sale price vs grlivarea')\nplt.xlabel('TotalBsmtSF')\nplt.ylabel('sale price ')\n\nplt.show()","a8f6806e":"var = 'OverallQual'\ndata_box = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data_box)\nfig.axis(ymin=0, ymax=800000);","96f33705":"var = 'YearBuilt'\ndata_box = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(18, 12))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data_box)\nfig.axis(ymin=0, ymax=800000);","4580bb35":"corrmat=train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","209d5754":"k = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm=train[cols].corr()\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True,annot=True, square=True, fmt='.2f', annot_kws={'size': 10})\n","aca28598":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)\nplt.show();","ba52bc81":"#saleprice_scaled = StandardScaler().fit_transform(data['SalePrice'][:,np.newaxis])","b5addd6a":"#saleprice_scaled2 = np.delete(saleprice_scaled, np.where(\n#    (saleprice_scaled >=3))[0], axis=0)\n#saleprice_scaled2[saleprice_scaled2>=3]","aa8222b0":"#bivariate analysis saleprice\/grlivarea\nvar = 'GrLivArea'\ndata1 = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata1.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","ab4f0d87":"train.sort_values(by = 'GrLivArea', ascending = False)[:2]\n","0237d458":"train= train.drop(train[train['Id'] == 1298].index)\ntrain= train.drop(train[train['Id'] == 523].index)","65d7afcf":"sns.distplot(train['SalePrice'],fit= norm);\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","8cfd44ea":"#applying log transformation\n#train['SalePrice'] = np.log1p(train['SalePrice'])\n#transformed histogram and normal probability plot\nsns.distplot(train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","73ef9754":"#histogram and normal probability plot\n#sns.distplot(data['GrLivArea'], fit=norm);\n#fig = plt.figure()\n#res = stats.probplot(data['GrLivArea'], plot=plt)","92ecf736":"data=pd.concat((train, test))\ny= train.SalePrice.values\ndata.drop(columns=['SalePrice'], inplace= True)\ndata.shape","49a31db7":"data.shape","39d1e9f5":"y= train.SalePrice.values\ny","5002b42a":"#applying log transformation\n#data['GrLivArea'] = np.log(data['GrLivArea'])\n#transformed histogram and normal probability plot\nsns.distplot(data['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(data['GrLivArea'], plot=plt)","cba0e768":"#histogram and normal probability plot\nsns.distplot(data['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(data['TotalBsmtSF'], plot=plt)","eb662fe5":"total = data.isnull().sum().sort_values(ascending=False)\npercent = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","f927ca6a":"data[\"PoolQC\"] =data[\"PoolQC\"].fillna(\"None\")\ndata[\"MiscFeature\"] = data[\"MiscFeature\"].fillna(\"None\")\ndata[\"Alley\"] = data[\"Alley\"].fillna(\"None\")\ndata[\"Fence\"]= data[\"Fence\"].fillna(\"None\")\ndata[\"FireplaceQu\"]= data[\"FireplaceQu\"].fillna(\"None\")\n\n\n\n","e27b832e":"data.columns\ndata.shape","b2dfd50e":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ndata[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","a0a57495":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    data[col] =data[col].fillna('None')","277e6eaa":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    data[col] = data[col].fillna(0)","1bfa9d17":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    data[col] = data[col].fillna(0)","237b9986":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    data[col] = data[col].fillna('None')","cdb9b487":"data[\"MasVnrType\"] = data[\"MasVnrType\"].fillna(\"None\")\ndata[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(0)","648f7d78":"data['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])","800b95fe":"data = data.drop(['Utilities'], axis=1)\n","5de53d7e":"data.shape","3a38610e":"data[\"Functional\"] =data[\"Functional\"].fillna(\"Typ\")","535202e6":"data['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])","2005a092":"data['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])","8ef9e6e3":"data['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])","c954801f":"data['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])","be383fd5":"data['MSSubClass'] = data['MSSubClass'].fillna(\"None\")","ed2d6cd7":"data_na = (data.isnull().sum() \/ len(data)) * 100\ndata_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :data_na})\nmissing_data.head()","8e037751":"#MSSubClass=The building class\ndata['MSSubClass'] = data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ndata['OverallCond'] = data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ndata['YrSold'] = data['YrSold'].astype(str)\ndata['MoSold'] = data['MoSold'].astype(str)\n","bd7cb95d":"#mapper = {'Ex': 4, 'Gd': 3, 'TA':2, 'Fa': 1}\n#data['KitchenQual'] = data['KitchenQual'].map(mapper)\n","05f76123":"print('Shape all_data: {}'.format(data.shape))","d70a0ebc":"from sklearn.preprocessing import LabelEncoder\n#cols = ( 'LandSlope','LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', \n   #     'YrSold', 'MoSold')\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(data[c].values)) \n    data[c] = lbl.transform(list(data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(data.shape))","502d0d63":"c=data.dtypes\nc.head(20)","0226f736":"data.drop(columns='Id', inplace= True)\ndata.shape","01e9e0c8":"# Adding total sqfootage feature \ndata['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\ndata.shape","a6011287":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nnumeric_feats = data.dtypes[data.dtypes != \"object\"].index\n# Check the skew of all numerical features\nskewed_feats = data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n","47bdbb9f":"skewness.head(59)","0a77873a":"skew(data['Alley'])","f06174ca":"skewness.index","988b3ed8":"data_X=data","9e01b442":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to  log transform\".format(skewness.shape[0]))\n\n#from scipy.special import boxcox1p\nskewed_features = skewness.index\nskewed_features\ndata[skewed_features]=np.log1p(data[skewed_features])\n\n#lam = 0.15\n#for feat in skewed_features:\n #    data[feat] = boxcox1p(data[feat], lam)","da4b2d81":"data = pd.get_dummies(data)\nprint(data.shape)\n\n\n    ","f80eea65":"train_size= train.shape[0]\ntrain_size","890463f9":"X_train= data[:train_size]\ny_train1= y[:train_size]\ny_train = train.SalePrice.values\nX_test=data[train_size:]\n","857701ee":"y","e3efe64a":"y_train1","fb181158":"y_train","96dd2a66":"train_size","624c8a1e":"X_train.shape","f23ca328":"y.shape","5e55d927":"data.shape","eb307218":"y_train.shape","89675ccb":"from sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X_train, y_train)\nreg.score(X_train, y_train)\n","086915a4":"from sklearn.model_selection import cross_val_score\nprint(cross_val_score(reg, X_train, y_train, cv=3))","4871944b":"from sklearn.linear_model import Lasso \nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\n","274dadc9":"X_test.columns","1ac9e22a":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","6bc0c726":"y_train","d19bd5eb":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","8a2dbc7a":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","9da4ca53":"print(score)","f3a78e75":"y_pred=reg.predict(X_train)\nres= y_train-y_pred\nsns.distplot(res, fit= norm)\nres=np.sort(res)\nres","77bb7673":"import statsmodels.api as sm\nsm.qqplot(res,line='45',fit=True,dist=stats.norm)\nres\n","01c2a807":"residuals = stats.probplot(res, plot=plt)","1ddde6a8":"sns.distplot(y_pred, fit= norm)","58288976":"fig=sm.qqplot(y_pred,line='45',fit=True,dist=stats.norm)","f0e1e39e":"y_train_transf=np.log1p(y_train)\nsns.distplot(y_train_transf, fit= norm)","56335602":"fig=sm.qqplot(y_train_transf,line='45',fit=True,dist=stats.norm)","52129ddc":"reg = LinearRegression().fit(X_train, y_train_transf)\nreg.score(X_train, y_train_transf)","d6113e62":"y_pred=reg.predict(X_train)\nres= y_train_transf-y_pred\nsns.distplot(res, fit= norm)\n","7bf65b0c":"fig=sm.qqplot(res,line='45',fit=True,dist=stats.norm)","d7a46498":"##scatter_data=pd.concat(res,y_train)\n#scatterdata\nplt.scatter(y_pred, res)\nplt.xlabel(\"Sale Price\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals Vs SalePrice(Fitted values)\")\nplt.show()","035e4d71":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfeats=data_X.dtypes[data_X.dtypes != \"object\"].index\nfeats\ndata_numeric= data_X[feats]\n\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = data_numeric.columns\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(data_numeric.values, i)\n                          for i in range(len(data_numeric.columns))]\nvif_data.sort_values(by='VIF',ascending=False)\n","6a1b883e":"data_numeric.loc[data_numeric['YearRemodAdd']==data_numeric['YearBuilt'],'YearRemodAdd']=0\ndata_numeric[['YearBuilt','YearRemodAdd']]\n","92ec2a9d":"# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(data_numeric.values, i)\n                          for i in range(len(data_numeric.columns))]\nvif_data.sort_values(by='VIF',ascending=False)","b609c44e":"\ntrain[['GrLivArea','1stFlrSF','2ndFlrSF']].head(20)","b8467677":"data_numeric.drop(['1stFlrSF','2ndFlrSF'], axis=1, inplace=True)","04813c5f":"data_numeric.drop('GrLivArea', axis=1, inplace=True)\n#data_numeric.drop('TotalSF', axis=1, inplace=True)","19303cd4":"vif_data = pd.DataFrame()\nvif_data[\"feature\"] = data_numeric.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(data_numeric.values, i)\n                          for i in range(len(data_numeric.columns))]\nvif_data.sort_values(by='VIF',ascending=False)","0b591a5b":"data_numeric.drop('YearBuilt', axis=1, inplace=True)\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = data_numeric.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(data_numeric.values, i)\n                          for i in range(len(data_numeric.columns))]\nvif_data.sort_values(by='VIF',ascending=False)","dfaa988d":"At this point , we are ready with the train and test data sets. ","4a999f48":"**Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","79400b82":"**Functional** : data description says NA means typical","54c2df29":"**Loading data and defining missing values: **\n\nThe pandas library read_csv  by default searches for the following missing value types:\n \u2018\u2019, \u2018#N\/A\u2019, \u2018#N\/A N\/A\u2019, \u2018#NA\u2019, \u2018-1.#IND\u2019, \u2018-1.#QNAN\u2019, \u2018-NaN\u2019, \u2018-nan\u2019, \u20181.#IND\u2019, \u20181.#QNAN\u2019, \u2018<NA>\u2019, \u2018N\/A\u2019, \u2018NA\u2019, \u2018NULL\u2019, \u2018NaN\u2019, \u2018n\/a\u2019, \u2018nan\u2019, \u2018null\u2019\n There is an additional argument **\"na_values= \"** that can be passed so as to check custom na type values. for e.g. a space character \" \" etc.","fcfba2f9":"According to Central Limit Theorem, the distribution should be Normal. But we can see that there are soe visible deviations from the expected behaviour. Let us consider some tranformations on respnse variable 'SalePrice'","c92beb45":"from sklearn.preprocessing import OrdinalEncoder\nord_fields=['MSSubClass','ExterQual','LotShape','BsmtQual','BsmtCond','BsmtExposure',\n            'BsmtFinType1', 'BsmtFinType2','HeatingQC','Functional',\n            'FireplaceQu','KitchenQual', 'GarageFinish','GarageQual','GarageCond','PoolQC','Fence']\norders=[ ['20','30','40','45','50','60','70','75','80','85', '90','120','150','160','180',\n    '190'],  ['Fa','TA','Gd','Ex'], ['Reg','IR1' ,'IR2','IR3'], \n ['None','Fa','TA','Gd','Ex'], ['None','Po','Fa','TA','Gd','Ex'],\n ['None','No','Mn','Av','Gd'],['None','Unf','LwQ', 'Rec','BLQ','ALQ' , \n'GLQ' ], ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ],\n ['Po','Fa','TA','Gd','Ex'],  ['Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],\n ['None','Po','Fa','TA','Gd','Ex'],  ['Fa','TA','Gd','Ex'],\n ['None','Unf','RFn','Fin'],  ['None','Po','Fa','TA','Gd','Ex'],\n['None','Po','Fa','TA','Gd','Ex'],\n ['None','Fa','Gd','Ex'], ['None','MnWw','GdWo','MnPrv','GdPrv'] ]\nfor i in range(len(orders)):     \n    ord_en=OrdinalEncoder(categories = {0:orders[i]}) \n    data.loc[:,ord_fields[i]]=ord_en.fit_transform(data.loc[:,ord_fields[i]].values.reshape(-1,1))\n","31185d3a":"If we implement this for all such variables where, label encoding is appropriate than onehot encoding, [in this reference by mitra mirshafiee ](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/comments)","de98fff4":"Let us save the data as 'data_X' before transformations and other manipulation tasks. This data_X will be used later so as to check the linear regression assumptions, for which its good to have the non-transformed data.","ff6c46f8":"**GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)","b3de5786":"Let's onfirm if there are still any missing values:","15bc322e":"Some categorical variables are represented in numerical variables. So lets transform them.","183840af":"**SaleType** : Fill in again with most frequent which is \"WD\"","13e1af9c":"**BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 :** For all these categorical basement-related features, NaN means that there is no basement.","1bae8d3c":"For the columns \"GarageType\",\"GarageFinish\",'GarageQual', 'GarageCond' missing value implies the feature not being available. Hence considered as None\". Will be later encoded as cetegorical using onehot encoding.","98785936":"**Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string","c633f7ce":"We can drop the column Id so as to ignore it during modelling","91829b92":"We can go with the label encoding for the other features that do not require ordinal encoding.\nFeatures: ","85b10924":"**MSZoning (The general zoning classification)** : 'RL' is by far the most common value. So we can fill in missing values with 'RL'","6e412886":"'**All models are wrong, but some are useful**' - George E.P. Box, British Statistician\n\nThis notebook is on House prices prediction using linear regression. \nHere in this notebook, I have started off taking some parts  from the amazing notebooks by  [Comprehensive data analysis by Pedro](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) and [Stacked Regressions by Serigne](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)\n\n**Focus of the notebook**\nThe notebook reproduces some of the results from above mentioned notebooks. But takes a differente standpoint regarding the assumptions of linear regression. The focus of this notebook is to illustrate the assumptions of linear regression in a methodological way.\n\n**Assumptions of linear regression:**\n1. Linearity- \n2. Normality\n3. Homoscedasticity\n4. Independence( No Multicollinearity)\n\n\nSome notebooks consider the transformations well ahead of the modelling. For e.g. [in  Notebook by Serigne](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard?rvi=1) and also [Notebook by Pedro](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) . However, in this notebook, tranformation have not been considered aprior to modelling to prevent the misconception that can arise to reader on assumptions of Linear regression.\n\nInfact when I googled, came to know that there are already lot of intersting discussions on this very topic about the normality assumption in linear regression. For e.g. [ in this stackexchange discussion ](https:\/\/stats.stackexchange.com\/questions\/12262\/what-if-residuals-are-normally-distributed-but-y-is-not?newreg=be2fc14894c247fd95e8abdbb16f5af0).\nAlso the website [by Robert Nau of Duke University](https:\/\/people.duke.edu\/~rnau\/testing.htm) helped me understand some key concepts.\n\nThe conclusions (based on my understanding) are as follows:\n1. The normality assumption in linear regression is for the residuals and not for the response\/predictor variable. \n2. The normality assumption of residuals enables  one to calculate p-values and make statistical inferences. Because the p-value, confidence intervals are based on this assumption that residuals are normally distributed.\n3. If someone doesn't want to make any statistical inferences after linear regression and the residuals are not normally distributed, it is perfectly valid.\n\nAs an additional point,even if the distribution of predictor variables is non-normal , Central limit theorem guarantees that the resulting  distribution will be normal i.e. the response variable which is being modelled as linear combination of predictor variables(could be Non-normal) will turn out to be normal.\nNow here is the catch. What if the resulting distribution of response variable y(SalePrice here) doesn't follow normal distribution?  Let us go ahead with the data analysis and check what happens.","ccc3f43d":"**Probplot**: Now there is the other version of illustrating the same point . That is by calling the probplot from ScikitLearn.","8830e6bc":"**Q-Q plot:** Let us go ahead and call the Q-Q plot from statsmodels.api. ","17fb9f58":"**MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.","f02cb9f6":"We can observe that the features 'YearBuilt' and 'YearRemododAdd' are strongly correlated. Hence we can redefine the 'YearRemodAdd' to a new variable such that the variable captures, whether remodelling has been done or not. In the dataset, If  a house is not remodelled the 'YearBuit'= 'YearRemmodAdd'. So we use that condition to extract the infomation on whether remodelling was done or not on a partiuclar house.","5b5f521f":"What are the predictor variables we  have in our dataset?\n[Ames dataset](http:\/\/jse.amstat.org\/v19n3\/decock.pdf) has great details on how the data has been configured and other interesting details. Intrested reader may want to look into the documentation of the data set. As a word of caution, some of the links in the document mentioned above seems inactive. \nBut this link provides nice [Definition of data set variables](https:\/\/cran.r-project.org\/web\/packages\/AmesHousing\/AmesHousing.pdf)","93bd6a2b":"**Homoscedasticity :**\n From the above plot, of Residuals vs Fitted values, it appears that there is no strong sense of variation in residuals w.r.t. fitted values","3a2dcb40":"**Utilities** : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.","740d298a":"Now that data is ready, we can go ahead with the modelling. Note that we have not done any transformations on the data like box-cox transformation to normalize the skewness of the distributions in different variables. One can attempt these tranformations  if not satified with the model performance.","84a7d024":"There is some improvement, still the VIFs are way too high as far as the standard acceptable values of VIF are concerned. One can make suitable analysis to overcome this problem in their further study. That is it from my end.\nIn summary, we have accomplished the task of how diagnoise whether a model is suitable for a given data set. Specifically, by appealing to the first principles of linear regression i.e. linearity, normality of residuals, homoscedasticity and no multi collinearity. Thanks for reading.","84ad50b2":"It is assumed that  similar neighborhoods will be similar fromtage. Hence the median of frontage corresponding to a neighborhood has been used to imputed the missing value.","294b8122":"The distribution of residuals is comparitively more normal than the previous un-transformed version. But not close enough to be called as standard normal distribution. So let's check for the other assumptions of linear regression proceeding to **\"homoscedasticity\"**.\n","9158b81c":"**MSSubClass** : Na most likely means No building class. We can replace missing values with None","2ea69a7f":"The variable **'GrLivArea'** has vey high VIF. Possible that it is corrrleating with **'1st Flr SF'** and '**2nd FLr SF**'. When we tabulate a shown below table, it is clear that GRLivArea is nothing but sum of '1stFlrSF' '2ndFlrSF'. So we can safely remove the variable 'GrLivArea' from our data set. Also its  Total SF = BSmtSF+ 1stFlrSF+ 2ndFlrSF, we can remove that variable as well.","f3f18f23":"To understand how sales price changes with respect to predictor variables. Out of many variables we can choose to illustrate how ","6e5c4877":"**Linear regression 1st assumption**:  It can be observed that the residuals are closely alligned with the normal distribution . But is it enough to assuume that normal distribution is satified? ","9c416a50":"**KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.","bc1ee424":"We can unmerge the data into train and test as per the size of original train and test data set that was imported in first place.","2850dd1b":"Let us proceed with the one hot encoding for all categorical features. pd.get_dummies , by default  creates one hot encoding for all variables of the class 'object'.","d21fff3d":"Converting ordinal variables to preserve the order of the value. In other words, the quantitative values of variables increase corresponding to the variables' qualitative interpretation. For example for the variable, Kitchen quality, the label encoding can be custommised as shown below as suggested by [Koji ](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/comments)","9eef76d7":"'SalePrice ' is the response variable we want to predict.Let's look at its statistical summary","64a1ff07":"We can observe that, after the transfromation, the distribution of saleprice approaches closer to normal distribution, How does this affect the prediction, Let's see.","dd3ace08":"The variable 'year built' is one geric variable which could encapsulate the  information about many other varibales. FOr e.g. it is seen to correlate with the sq.feet area (1stflrsf ) . Now it can be seen that it is correlating with pool quality as well. So may be it can lead us to the understandong that the older houses may have low pool quality. So lets drop the variable 'year built' and see how our VIF table changes.","ccf8b3f1":"**Residual plot:**  Since we are done with the modelling part, let us plot the residuals and check if they are normally distributed or not","ed6fcd65":"**Missing value treatment**\n\n**PoolQC** : data description says NA means \"No Pool\"\n\n**MiscFeature**  : data description says NA means \"no misc feature\"\n\n**Alley** : data description says NA means \"no alley access\"\n\n**FireplaceQu** : data description says NA means \"no fireplace\"\n\n**Fence** : data description says NA means \"no fence\"\n","89247141":"The plots suggest some kind of non -linear behaviour of normalized residuals. So there is an indication that the normality assumption of Linear regression hasn't been satisfied well. Let us cplot the distribution of the response variable \"SalePrice\". ","45ca046c":"**BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement"}}