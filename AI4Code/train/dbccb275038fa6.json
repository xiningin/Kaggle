{"cell_type":{"b8cb7e0b":"code","c3fc6649":"code","714bb204":"code","0a5c88b5":"code","a0cdd5e5":"code","854cb33b":"code","d3aaa658":"code","394dc96a":"code","4271436c":"code","1043e544":"code","e1505eaa":"code","72ba20b2":"code","b1e4c273":"code","e7887e37":"markdown","521b5e98":"markdown","ef5d3abd":"markdown","dc2abb4e":"markdown"},"source":{"b8cb7e0b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","c3fc6649":"train = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/train.parquet')\ntest = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/test.parquet')\nsub = pd.read_csv('..\/input\/kaggle-pog-series-s01e01\/sample_submission.csv')","714bb204":"cols_to_drop=['publishedAt', 'trending_date']\ntrain=train.drop(cols_to_drop,axis=1)\ntrain.columns","0a5c88b5":"import shap\nimport random\nimport lightgbm as lgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\n\nSEED = 99\nrandom.seed(SEED)\nnp.random.seed(SEED)","a0cdd5e5":"trainmodel = train.copy()\n\n# read the \"object\" columns and use labelEncoder to transform to numeric\nfor col in trainmodel.columns[trainmodel.dtypes == 'object']:\n    le = LabelEncoder()\n    trainmodel[col] = trainmodel[col].astype(str)\n    le.fit(trainmodel[col])\n    trainmodel[col] = le.transform(trainmodel[col])","854cb33b":"X = trainmodel.drop(['target','video_id'], axis = 1)\ny = trainmodel['target']","d3aaa658":"lgb_params = {\n                    'objective':'mae',\n                    'metric':'mae',\n                    'n_jobs':-1,\n                    'learning_rate':0.005,\n                    'num_leaves': 20,\n                    'max_depth':-1,\n                    'subsample':0.9,\n                    'n_estimators':2500,\n                    'seed': SEED,\n                    'early_stopping_rounds':100, \n                }","394dc96a":"# choose the number of folds, and create a variable to store the mae values and the iteration values.\nK = 5\nfolds = KFold(K, shuffle = True, random_state = SEED)\nbest_scorecv= 0\nbest_iteration=0\n\n# Separate data in folds, create train and validation dataframes, train the model and cauculate the mean AUC.\nfor fold , (train_index,test_index) in enumerate(folds.split(X, y)):\n    print('Fold:',fold+1)\n          \n    X_traincv, X_testcv = X.iloc[train_index], X.iloc[test_index]\n    y_traincv, y_testcv = y.iloc[train_index], y.iloc[test_index]\n    \n    train_data = lgb.Dataset(X_traincv, y_traincv)\n    val_data   = lgb.Dataset(X_testcv, y_testcv)\n    \n    LGBM = lgb.train(lgb_params, train_data, valid_sets=[train_data,val_data], verbose_eval=250)\n    best_scorecv += LGBM.best_score['valid_1']['mae']\n    best_iteration += LGBM.best_iteration\n\nbest_scorecv \/= K\nbest_iteration \/= K\nprint('\\n Mean mae score:', best_scorecv)\nprint('\\n Mean best iteration:', best_iteration)","4271436c":"lgb_params = {\n                    'objective':'mae',\n                    'metric':'mae',\n                    'n_jobs':-1,\n                    'learning_rate':0.05,\n                    'num_leaves': 20,\n                    'max_depth':-1,\n                    'subsample':0.9,\n                    'n_estimators':round(best_iteration),\n                    'seed': SEED,\n                    'early_stopping_rounds':None, \n                }\n\ntrain_data_final = lgb.Dataset(X, y)\nLGBM = lgb.train(lgb_params, train_data)","1043e544":"print(LGBM)","e1505eaa":"# telling wich model to use\nexplainer = shap.TreeExplainer(LGBM)\n# Calculating the Shap values of X features\nshap_values = explainer.shap_values(X)","72ba20b2":"shap.summary_plot(shap_values[1], X, plot_type=\"bar\")","b1e4c273":"shap.summary_plot(shap_values[1], X)","e7887e37":"ValueError: num_boost_round should be greater than zero.\n\nAfter the errors above I couldn't make the Shap values ","521b5e98":"#In the original code the metric was auc. Since in this challenge it's mae, I changed metric and objective. And I got the error which I have no clue how to fix it. Any tip is welcome. ","ef5d3abd":"#I know that dropping the 2 features below is not correct. Anyway, I 'll take a shot to see what happens.","dc2abb4e":"The script was made by  rossinEndrew https:\/\/www.kaggle.com\/endrewrossin\/fast-lightgbm-model-to-detect-exam-result-w-shap\/notebook\n\nThough I could only embarassed his code."}}