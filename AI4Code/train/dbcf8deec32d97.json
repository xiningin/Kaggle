{"cell_type":{"b74f6870":"code","73c0f90d":"code","99e84949":"code","4885b178":"code","95fc9833":"code","ebc60c15":"code","fd88aca7":"code","ab516b29":"code","ba9affd3":"code","2bd9ecff":"code","87f4da25":"code","e5f217e7":"code","66e9029d":"code","17023a0c":"code","3a96bb82":"code","07c9e1c7":"code","f2f91d1a":"code","2e3f81a9":"code","5c44aef9":"code","d2abb082":"code","ed950800":"code","3de6929c":"code","1bfc6226":"code","d535b544":"code","0ef200a7":"code","8bc41294":"code","f108e320":"code","68a16575":"code","2cd8354a":"code","a5809b05":"code","ada6ee94":"code","63eab0ac":"code","158a17f0":"code","45220416":"code","6f1464df":"code","ed7bee5f":"code","32cb65ae":"code","f2c47734":"code","3597664d":"code","d3d47051":"code","a095bca0":"code","de915a13":"code","513c2966":"code","6cf037d2":"code","ce51e786":"code","4984aacc":"code","2b224a0d":"code","93ab5211":"code","78c173b6":"code","c9297b79":"code","119bc313":"code","9acb3dc4":"code","a666b30a":"code","2ffe53bb":"code","6d1a6ce3":"code","de000533":"code","6322b66b":"code","5f402c80":"markdown","57c8f962":"markdown","d779549c":"markdown","399b81dd":"markdown","475de426":"markdown","1ad67bea":"markdown","65b76439":"markdown","7d9ae2de":"markdown","9d26df48":"markdown","86f2effe":"markdown","087a92f2":"markdown","eb7f2fab":"markdown","e10781a8":"markdown","e212a6cd":"markdown","80ab8684":"markdown","228b9b5c":"markdown","b1e8098e":"markdown","031ebd60":"markdown","334d8d0d":"markdown","233341c4":"markdown","cf262221":"markdown","245b4980":"markdown","3f657983":"markdown","27a1f926":"markdown"},"source":{"b74f6870":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport math\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import GridSearchCV","73c0f90d":"def root_mean_squared_log_error(y_valid, y_preds):\n    \"\"\"Calculate root mean squared error of log(y_true) and log(y_pred)\"\"\"\n    if len(y_preds)!=len(y_valid): return 'error_mismatch'\n    y_preds_new = [math.log(x) for x in y_preds]\n    y_valid_new = [math.log(x) for x in y_valid]\n    return mean_squared_error(y_valid_new, y_preds_new, squared=False)","99e84949":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\npd.set_option('display.max_columns', None)\ntrain_data.head()","4885b178":"train_data.shape","95fc9833":"print(train_data.columns[train_data.isna().any()].unique())\nlen(train_data.columns[train_data.isna().any()].unique())","ebc60c15":"#correlation matrix\ncorrmat = train_data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","fd88aca7":"features = [x for x in train_data.columns if x not in ['SalePrice']]\nX = train_data.drop(['SalePrice'], axis=1)\nY = train_data['SalePrice']","ab516b29":"X_train, X_valid, y_train, y_valid = train_test_split(X, Y, random_state=42)\n\nnumerical_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\n\ncategorical_cols = [cname for cname in X_train.columns if\n                    X_train[cname].nunique() < 13 and \n                    X_train[cname].dtype == \"object\"]\n\n\nnumerical_transformer = SimpleImputer(strategy='constant')\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n])","ba9affd3":"from sklearn.ensemble import RandomForestRegressor\n\nrandom_model = RandomForestRegressor(random_state=42, n_estimators=1000)\n\nrandom_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('random_model', random_model)\n                     ])\n\nrandom_clf.fit(X_train, y_train)\n\nrandom_clf.fit(X_train, y_train)\n\nrandom_preds = random_clf.predict(X_valid)\n\nprint('RMSLE:', root_mean_squared_log_error(y_valid, random_preds))","2bd9ecff":"from xgboost import XGBRegressor\n\nxgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.01, random_state=42)\n\nxgb_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('xgb_model', xgb_model)\n                     ])\n\nxgb_clf.fit(X_train, y_train, xgb_model__verbose=False)\n\nxgb_clf.fit(X_train, y_train)\n\nxgb_preds = xgb_clf.predict(X_valid)\n\nprint('RMSLE:', root_mean_squared_log_error(y_valid, xgb_preds))","87f4da25":"from sklearn.ensemble import AdaBoostRegressor\n\nada_model = AdaBoostRegressor(random_state=42, learning_rate=0.01, n_estimators=1000)\n\nada_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('xgb_model', ada_model)\n                     ])\n\nada_clf.fit(X_train, y_train)\n\nada_clf.fit(X_train, y_train)\n\nada_preds = ada_clf.predict(X_valid)\n\nprint('RMSLE:', root_mean_squared_log_error(y_valid, ada_preds))","e5f217e7":"train_data['OverallQual'].isnull().sum()","66e9029d":"plt.figure(figsize=(15, 10))\nplt.title(\"OverallQual vs SalePrice\")\nsns.barplot(x='OverallQual', y='SalePrice', data=train_data)\nplt.show()","17023a0c":"train_data['OverallCond'].isnull().sum()","3a96bb82":"plt.figure(figsize=(15, 10))\nplt.title(\"OverallCond vs SalePrice\")\nsns.barplot(x='OverallCond', y='SalePrice', data=train_data)\nplt.show()","07c9e1c7":"train_data['YearBuilt'].isnull().sum()","f2f91d1a":"plt.figure(figsize=(35, 10))\nplt.title(\"Year Built vs SalePrice\")\nsns.barplot(x='YearBuilt', y='SalePrice', data=train_data)\nplt.show()","2e3f81a9":"train_data['YearRemodAdd'].isnull().sum()","5c44aef9":"plt.figure(figsize=(35, 10))\nplt.title(\"Year Remod Add vs SalePrice\")\nsns.barplot(x='YearRemodAdd', y='SalePrice', data=train_data)\nplt.show()","d2abb082":"train_data['TotalBsmtSF'].isnull().sum()","ed950800":"plt.figure(figsize=(35, 10))\nplt.title(\"TotalBsmtSF vs SalePrice\")\nsns.regplot(x='TotalBsmtSF', y='SalePrice', data=train_data)\nplt.show()","3de6929c":"plt.figure(figsize=(35, 10))\nplt.title(\"Distribution of TotalBsmtSF\")\nsns.boxplot(train_data['TotalBsmtSF'])\nplt.show()","1bfc6226":"Q1 = train_data['TotalBsmtSF'].quantile(0.25)\nQ3 = train_data['TotalBsmtSF'].quantile(0.75)\nIQR = Q3 - Q1\n\noutliers = train_data.loc[(train_data['TotalBsmtSF'] > (Q3 + 1.75 * IQR)) | (train_data['TotalBsmtSF'] < (Q1 - 1.75 * IQR)), 'TotalBsmtSF']\n\nprint(\"Percent of Outliers: \", outliers.count() \/ train_data['TotalBsmtSF'].count() * 100)","d535b544":"train_data.drop(train_data.loc[(train_data['TotalBsmtSF'] > (Q3 + 1.75 * IQR)) | (train_data['TotalBsmtSF'] < (Q1 - 1.75 * IQR))].index, inplace=True)\n\ntrain_data.shape","0ef200a7":"plt.figure(figsize=(35, 10))\nplt.title(\"TotalBsmtSF vs SalePrice\")\nsns.regplot(x='TotalBsmtSF', y='SalePrice', data=train_data)\nplt.show()","8bc41294":"train_data['1stFlrSF'].isnull().sum()","f108e320":"plt.figure(figsize=(35, 10))\nplt.title(\"1stFlrSF vs SalePrice\")\nsns.regplot(x='1stFlrSF', y='SalePrice', data=train_data)\nplt.show()","68a16575":"plt.figure(figsize=(35, 10))\nplt.title(\"Distribution of 1stFlrSF\")\nsns.boxplot(train_data['1stFlrSF'])\nplt.show()","2cd8354a":"Q1 = train_data['1stFlrSF'].quantile(0.25)\nQ3 = train_data['1stFlrSF'].quantile(0.75)\nIQR = Q3 - Q1\n\noutliers = train_data.loc[(train_data['1stFlrSF'] > (Q3 + 1.75 * IQR)) | (train_data['1stFlrSF'] < (Q1 - 1.75 * IQR)), '1stFlrSF']\n\nprint(\"Percent of Outliers: \", outliers.count() \/ train_data['1stFlrSF'].count() * 100)","a5809b05":"train_data.drop(train_data.loc[(train_data['1stFlrSF'] > (Q3 + 1.75 * IQR)) | (train_data['1stFlrSF'] < (Q1 - 1.75 * IQR))].index, inplace=True)\n\ntrain_data.shape","ada6ee94":"plt.figure(figsize=(35, 10))\nplt.title(\"1stFlrSF vs SalePrice\")\nsns.regplot(x='1stFlrSF', y='SalePrice', data=train_data)\nplt.show()","63eab0ac":"train_data['GrLivArea'].isnull().sum()","158a17f0":"plt.figure(figsize=(35, 10))\nplt.title(\"GrLivArea vs SalePrice\")\nsns.regplot(x='GrLivArea', y='SalePrice', data=train_data)\nplt.show()","45220416":"plt.figure(figsize=(35, 10))\nplt.title(\"Distribution of GrLivArea\")\nsns.boxplot(train_data['GrLivArea'])\nplt.show()","6f1464df":"Q1 = train_data['GrLivArea'].quantile(0.25)\nQ3 = train_data['GrLivArea'].quantile(0.75)\nIQR = Q3 - Q1\n\noutliers = train_data.loc[(train_data['GrLivArea'] > (Q3 + 1.75 * IQR)) | (train_data['GrLivArea'] < (Q1 - 1.75 * IQR)), 'GrLivArea']\n\nprint(\"Percent of Outliers: \", outliers.count() \/ train_data['GrLivArea'].count() * 100)","ed7bee5f":"train_data.drop(train_data.loc[(train_data['GrLivArea'] > (Q3 + 1.75 * IQR)) | (train_data['GrLivArea'] < (Q1 - 1.75 * IQR))].index, inplace=True)\n\ntrain_data.shape","32cb65ae":"plt.figure(figsize=(35, 10))\nplt.title(\"GrLivArea vs SalePrice\")\nsns.regplot(x='GrLivArea', y='SalePrice', data=train_data)\nplt.show()","f2c47734":"train_data['FullBath'].isnull().sum()","3597664d":"plt.figure(figsize=(15, 10))\nplt.title(\"FullBath vs SalePrice\")\nsns.barplot(x='FullBath', y='SalePrice', data=train_data)\nplt.show()","d3d47051":"train_data['TotRmsAbvGrd'].isnull().sum()","a095bca0":"plt.figure(figsize=(15, 10))\nplt.title(\"TotRmsAbvGrd vs SalePrice\")\nsns.barplot(x='TotRmsAbvGrd', y='SalePrice', data=train_data)\nplt.show()","de915a13":"train_data['GarageCars'].isnull().sum()","513c2966":"plt.figure(figsize=(15, 10))\nplt.title(\"Garage Cars vs SalePrice\")\nsns.barplot(x='GarageCars', y='SalePrice', data=train_data)\nplt.show()","6cf037d2":"train_data['GarageArea'].isnull().sum()","ce51e786":"plt.figure(figsize=(15, 10))\nplt.title(\"Garage Area vs SalePrice\")\nsns.regplot(x='GarageArea', y='SalePrice', data=train_data)\nplt.show()","4984aacc":"plt.figure(figsize=(35, 10))\nplt.title(\"Distribution of GarageArea\")\nsns.boxplot(train_data['GarageArea'])\nplt.show()","2b224a0d":"Q1 = train_data['GarageArea'].quantile(0.25)\nQ3 = train_data['GarageArea'].quantile(0.75)\nIQR = Q3 - Q1\n\noutliers = train_data.loc[(train_data['GarageArea'] > (Q3 + 1.75 * IQR)) | (train_data['GarageArea'] < (Q1 - 1.75 * IQR)), 'GarageArea']\n\nprint(\"Percent of Outliers: \", outliers.count() \/ train_data['GarageArea'].count() * 100)","93ab5211":"train_data.drop(train_data.loc[(train_data['GarageArea'] > (Q3 + 1.75 * IQR)) | (train_data['GarageArea'] < (Q1 - 1.75 * IQR))].index, inplace=True)\n\ntrain_data.shape","78c173b6":"plt.figure(figsize=(35, 10))\nplt.title(\"Garage Area vs SalePrice\")\nsns.regplot(x='GarageArea', y='SalePrice', data=train_data)\nplt.show()","c9297b79":"'''params = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n\ngrid = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('xgb_model', GridSearchCV(xgb_model, param_grid=params, n_jobs=4, cv=5, verbose=3 ))\n                     ])\n\ngrid.fit(X_train, y_train)\nprint('\\n All results:')\nprint(grid.cv_results_)\nprint('\\n Best estimator:')\nprint(grid.best_estimator_)\nprint('\\n Best score:')\nprint(grid.best_score_ * 2 - 1)\nprint('\\n Best parameters:')\nprint(grid.best_params_)'''","119bc313":"hp_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0.5, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.02, max_delta_step=0, max_depth=4,\n             min_child_weight=1, monotone_constraints='()',\n             n_estimators=1000, n_jobs=0, num_parallel_tree=1, random_state=42,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\nhp_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('hp_model', hp_model)\n                     ])\n\nhp_clf.fit(X_train, y_train, hp_model__verbose=False)\n\nhp_preds = hp_clf.predict(X_valid)\n\nprint('RMSLE:', root_mean_squared_log_error(y_valid, hp_preds))","9acb3dc4":"X = train_data.drop(['SalePrice'], axis=1)\ny = train_data.SalePrice","a666b30a":"X.columns.to_list()","2ffe53bb":"X_feat_eng = X.copy()\n\nX_feat_eng['years_since_update'] = X_feat_eng['YearRemodAdd'] - X_feat_eng['YearBuilt']\nX_feat_eng['geometry'] = X_feat_eng['LotArea'] \/ X_feat_eng['LotFrontage']\nX_feat_eng['land_topology'] = X_feat_eng['LandSlope'] + '_' + X_feat_eng['LandContour']\nX_feat_eng['value_proposition'] = X_feat_eng['YearBuilt'] * X_feat_eng['OverallQual']\nX_feat_eng['finished_basement'] = X_feat_eng['BsmtFinSF1'] > 0\nX_feat_eng['garage_value'] = X_feat_eng['YearBuilt'] * X_feat_eng['GarageCars']\nX_feat_eng['misc_value'] = X_feat_eng['Fireplaces'] + X_feat_eng['OverallQual']  \nX_feat_eng = X_feat_eng.drop(columns=['GarageCars'])\nX_feat_eng['Test'] = X_feat_eng['Street'] + '_' + X_feat_eng['Alley']\n\nfeature_numerical_cols = [cname for cname in X_feat_eng.columns if \n                X_feat_eng[cname].dtype in ['int64', 'float64']]\n\nfeature_categorical_cols = [cname for cname in X_feat_eng.columns if X_feat_eng[cname].nunique() < 50 \n                            and X_feat_eng[cname].dtype in ['object', 'bool']]\n\nfeature_numerical_transformer = SimpleImputer(strategy='constant')\n\nfeature_categorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\nfeature_preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', feature_numerical_transformer, feature_numerical_cols),\n        ('cat', feature_categorical_transformer, feature_categorical_cols)\n])\n\nfeature_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0.0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.02, max_delta_step=0, max_depth=4,\n             min_child_weight=0.0, monotone_constraints='()',\n             n_estimators=1250, n_jobs=0, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\nfeature_clf = Pipeline(steps=[('feature_preprocessor', feature_preprocessor),\n                      ('feature_model', feature_model)\n                     ])\n\nfeature_X_train, feature_X_valid, feature_y_train, feature_y_valid = train_test_split(X_feat_eng, y, random_state=0)\n\nfeature_clf.fit(feature_X_train, feature_y_train, feature_model__verbose=False) \nfeature_preds = feature_clf.predict(feature_X_valid)\n\nprint('RMSLE:', root_mean_squared_log_error(feature_y_valid, feature_preds))","6d1a6ce3":"X_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","de000533":"X_test['years_since_update'] = X_test['YearRemodAdd'] - X_test['YearBuilt']\nX_test['geometry'] = X_test['LotArea'] \/ X_test['LotFrontage']\nX_test['land_topology'] = X_test['LandSlope'] + '_' + X_test['LandContour']\nX_test['value_proposition'] = X_test['YearBuilt'] * X_test['OverallQual']\nX_test['finished_basement'] = X_test['BsmtFinSF1'] > 0\nX_test['garage_value'] = X_test['YearBuilt'] * X_test['GarageCars']\nX_test['misc_value'] = X_test['Fireplaces'] + X_test['OverallQual']\nX_test['Test'] = X_test['Street'] + '_' + X_test['Alley']\n\nX_test = X_test.drop(columns=['GarageCars'])\n\nfeature_clf.fit(X_feat_eng, y, feature_model__verbose=False)","6322b66b":"preds = feature_clf.predict(X_test)\noutput = pd.DataFrame({'Id': X_test.Id,\n                       'SalePrice': preds})\noutput.to_csv('submission.csv', index=False)","5f402c80":"So TotalBsmtSF is a quantitative variable and contains outliers. Outliers are always bad for predictions, and since there are only 14 outliers in this case, we will just remove all of them.","57c8f962":"# Take a look at our training set\n\nI always believe before we do any data analysis, we should take a look at our data.","d779549c":"**GrLivArea**","399b81dd":"The below method calculated the root mean squared log error that the submission score will be calculated based on. Since it is not a standard score metric in sklearn, we will write this one by ourselves. ","475de426":"**YearBuilt**","1ad67bea":"From the above codes, we can see that our train data has 1460 entries with 81 variables. Also, 19 of these variables contain missing values, and the exact columns that have missing values are also outputed using train_data.columns[train_data.isna().any()].unique(). \n\nThen, the heatmap tells us the correlations between different variables. Heatmap with the correlation matrix is honestly a very powerful way for us to quickly understand the relationship between data. If we look at the SalePrice row, we can clearly see that it is highly correlated with variables like OverallQual, GriLivArea, and etc. These are also useful information we should take into consideration. ","65b76439":"# Final Data Modification\n\nFinally, let's create some more parameters and make the predictions","7d9ae2de":"So TotalBsmtSF is a quantitative variable and contains outliers. Outliers are always bad for predictions, and since there are only 14 outliers in this case, we will just remove all of them.","9d26df48":"**OverallQual**","86f2effe":"**FullBath**","087a92f2":"**YearRemodAdd**","eb7f2fab":"# Prediction","e10781a8":"**GarageArea**","e212a6cd":"**OverallCond**","80ab8684":"So TotalBsmtSF is a quantitative variable and contains outliers. Outliers are always bad for predictions, and since there are only 14 outliers in this case, we will just remove all of them.","228b9b5c":"**Garage Cars**","b1e8098e":"**TotalBsmtSF**","031ebd60":"**1stFlrSF**","334d8d0d":"**TotRmsAbvGrd**","233341c4":"# Introduction\n\nHello everyone! This is my second Kaggle Notebook. In this one I will discuss how I approached the House Price Prediction challenge and analyzed this dataset. So first, let's us load the data and import all the libraries we will use.","cf262221":"Since XGB Regressor has the best performance, we will use this model in later sections.","245b4980":"# Feature Engineering \n\nSo now we have decided on which model to use. Let's determine the parameters we want to feed to our model. So from the correlation matrix, we can see that OverallQual, OverallCond, YearBuilt, YearRemodAdd, TotalBsmtSF, 1stFlrSF, GrLivArea, FullBath, TotRmsAbvGrd, GarageCars, and GarageArea are parameters that are highly correlated(>0.5) with the SalePrice variable.\n\nLet's see the distributions of these variables.","3f657983":"# Model Selection\n\nBefore we explore further, let's just select the model we are going to use. In this section, we will make some quick implementations of different machine learning models and choose one with the best performance. The models will simply be trained using all features except SalePrice column and be scored using root mean squared log error. ","27a1f926":"# Hyperparameter Tuning\n\nIn this section, we will choose the best parameters for our XGB Regressor. "}}