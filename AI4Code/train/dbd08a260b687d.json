{"cell_type":{"d1a84353":"code","9dacbe68":"code","7c40e955":"code","defec238":"code","f78e358b":"code","d007de55":"code","408432f8":"code","72ff32df":"code","7eb2407b":"code","6f5e13e9":"code","4be332af":"code","a7107db6":"code","c13b475f":"code","b24c8308":"code","4f62ff41":"code","472e855c":"code","5a6f0ca1":"code","be91d81d":"code","b6cc54c9":"code","7220c0b0":"code","0630a61b":"code","7a34628c":"code","9e54e44e":"code","fd8aa860":"code","edfaa6eb":"code","9003f555":"markdown","f1d043fd":"markdown","4298e0c9":"markdown","3e636b35":"markdown","00b97713":"markdown"},"source":{"d1a84353":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9dacbe68":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","7c40e955":"pd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)","defec238":"#Exploratory data analysis\nprint(train.shape)","f78e358b":"#Try considering location\/keyword?","d007de55":"print(train.head())","408432f8":"import nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer","72ff32df":"def process_tweet(tweet):\n    tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n    tweet2 = re.sub('https?:\\\/\\\/.*[\\r\\n]*', '', tweet2)\n    tweet2 = re.sub(r'#', '', tweet2)\n    #tweet2 = correct_spellings(tweet2)\n    tokenizer = TweetTokenizer(preserve_case = False, strip_handles = True, reduce_len = True)\n    tweet_tokens = tokenizer.tokenize(tweet2)\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords.words('english') and word not in string.punctuation):\n            tweets_clean.append(word)\n    stemmer = PorterStemmer()\n    tweets_stem = []\n    for word in tweets_clean:\n        stem_word = stemmer.stem(word)\n        tweets_stem.append(stem_word)\n    return tweets_stem","7eb2407b":"def build_freqs(tweets, ys):\n    yslist = np.squeeze(ys).tolist()\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        if (tweet == 'na'):\n            continue\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n    return freqs","6f5e13e9":"#Make NAs a readable format\ntrain = train.fillna(\"na\")\ntest = test.fillna(\"na\")","4be332af":"X_train = train.values\ny_train = train['target'].values\nX_test = test.values","a7107db6":"#from sklearn.model_selection import train_test_split\n#X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full)","c13b475f":"#Get frequency table\ntext_freqs = build_freqs(X_train[:, 3], y_train)\nkeyword_freqs = build_freqs(X_train[:, 1], y_train)\nlocation_freqs = build_freqs(X_train[:, 2], y_train)\nprint(f'type(keyword_freqs) = {type(keyword_freqs)}')\nprint(f'len(keyword_freqs) = {len(keyword_freqs)}')\nprint(f'type(location_freqs) = {type(location_freqs)}')\nprint(f'len(location_freqs) = {len(location_freqs)}')","b24c8308":"def extract_features(tweet, freqs):\n    tweet_stems = process_tweet(tweet)\n    x = np.zeros((1,2))\n    for i in tweet_stems:\n        x[0,0] += freqs.get((i, 1.0), 0)\n        x[0,1] += freqs.get((i, 0.0), 0)\n    assert(x.shape == (1,2))\n    return x","4f62ff41":"X_train_log = np.zeros((len(X_train), 6))\nfor i in range(len(X_train)):\n    X_train_log[i, 0:2] = extract_features(X_train[i, 3], text_freqs)\n    X_train_log[i, 2:4] = extract_features(X_train[i, 1], keyword_freqs)\n    X_train_log[i, 4:6] = extract_features(X_train[i, 2], location_freqs)\ny_train_log = y_train","472e855c":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train_log, y_train_log)","5a6f0ca1":"#Validation","be91d81d":"#X_val_log = np.zeros((len(X_val), 6))\n#for i in range(len(X_val)):\n#    X_val_log[i, 0:2] = extract_features(X_val[i, 3], text_freqs)\n#    X_val_log[i, 2:4] = extract_features(X_val[i, 1], keyword_freqs)\n#    X_val_log[i, 4:6] = extract_features(X_val[i, 2], location_freqs)\n#y_val_log = y_val","b6cc54c9":"#Check accuracay on validation data\nfrom sklearn.metrics import f1_score\n#pred_val = lr.predict(X_val_log)\n#print(f1_score(y_val_log, pred_val))","7220c0b0":"print(f1_score(y_train_log, lr.predict(X_train_log)))","0630a61b":"print(lr.coef_, lr.intercept_)","7a34628c":"def predict_tweet(tweet, freqs):\n    x = extract_features(tweet, freqs)\n    log_prob = lr.intercept_[0] + np.dot(lr.coef_[0], x[0])\n    return np.exp(log_prob) \/ (np.exp(log_prob) + 1)","9e54e44e":"#process test set\nX_test_log = np.zeros((len(X_test), 6))\nfor i in range(len(X_test)):\n    X_test_log[i, 0:2] = extract_features(X_test[i, 3], text_freqs)\n    X_test_log[i, 2:4] = extract_features(X_test[i, 1], keyword_freqs)\n    X_test_log[i, 4:6] = extract_features(X_test[i, 2], location_freqs)\npred = lr.predict(X_test_log)\npredictions = pd.DataFrame()\npredictions['id'] = test['id']\npredictions['target'] = pred\nprint(predictions.head())","fd8aa860":"print(predictions['target'].value_counts())","edfaa6eb":"#Submit predictions\npredictions.to_csv('submissions.csv', index=False)","9003f555":"This is a pretty bad model -- although the f1 scores are pretty similar for training and validation, so there is not much overfitting.","f1d043fd":"There are many na entries in the keyword and location columns, we change this into a format that can be used by the process_tweet function.","4298e0c9":"Now that we have used the training data to form a frequency table, we obtain, for each tweet\/keyword\/location, the number of words corresponding to positive or negative results.","3e636b35":"We use logistic regression to predict whether a tweet corresponds to disaster.\nThe features used are: number of positive words, number of negative words, number of positive keywords, number of negative keywords, number of positive locations, number of negative locations.","00b97713":"The two functions below clean tweets into arrays of stem words. Then we count, for each stem word, how many times it appears with a positive (disaster) tweet and how many times it appears with a negative tweet. The idea is that if it appears more often with a disaster tweet, then the inclusion of that word makes the tweet more likely to be a disaster tweet."}}