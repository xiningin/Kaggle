{"cell_type":{"5c31232c":"code","ce530284":"code","adfb45fa":"code","ec590caf":"code","f9ac8e2a":"code","622bed3f":"code","ea7dcac1":"code","7293fd41":"code","b75e9f8b":"code","109d744a":"code","9794254c":"code","25e01994":"code","f612860d":"code","601137a8":"code","f87221f6":"code","101ce118":"code","a4f2bfc7":"code","c9273971":"code","df6e9f6a":"code","06481bc2":"code","9ca0f3b6":"code","402a7a55":"code","874ef1d0":"code","ca083850":"code","cb528cad":"code","008126f7":"code","bdd1516c":"code","f814a263":"code","de56dddd":"code","1c7653dd":"code","81aa6fd8":"code","a2b43200":"code","87d3a3e8":"code","ad69ba25":"code","9fac09ed":"code","fd249293":"code","c04a43c1":"code","42224aef":"code","d042affe":"code","68cb619e":"code","8bae54cf":"code","89857e7d":"code","5c626c38":"markdown","c2062ebc":"markdown","fc4d8821":"markdown","898fc8e9":"markdown","ede12ed0":"markdown","e286841a":"markdown","07918059":"markdown","16b99939":"markdown","d9264e4c":"markdown","5a3fa12c":"markdown","19a2738b":"markdown","82e1cce7":"markdown","9811d30b":"markdown","4d89ca6d":"markdown","46731e7d":"markdown","d80e320f":"markdown","a429f287":"markdown","ea5a9b9a":"markdown","ddaaee6a":"markdown","487b58ad":"markdown","770a0bee":"markdown","11988e0e":"markdown","78d1a989":"markdown","45b46af8":"markdown","92bfb2bf":"markdown","7424a915":"markdown"},"source":{"5c31232c":"# Imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Classfiers\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom keras.wrappers.scikit_learn import KerasClassifier\n# Regressors\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n#Feature selection\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import RFE\n#Preprocess\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom sklearn.pipeline import Pipeline\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n     \n    \nsns.set(rc={'figure.figsize':(11.7,8.27)})","ce530284":"# Load the Train Dataset an show some columns\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain.head()","adfb45fa":"train.info()","ec590caf":"train.isnull().sum(axis = 0)","f9ac8e2a":"train.drop('Cabin',axis=1,inplace=True)","622bed3f":"train.Embarked.value_counts()","ea7dcac1":"train.Embarked.fillna(train.Embarked.mode()[0], inplace=True)","7293fd41":"train.Survived.value_counts(normalize=True) * 100","b75e9f8b":"ax = sns.countplot(x=\"Survived\", hue=\"Sex\", data=train)\nax.set_title(\"Survived x Sex\")","109d744a":"#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( train, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , train['Age'].max()))\na.add_legend()","9794254c":"ax = sns.countplot(x=\"Survived\", hue=\"Pclass\", data=train)\nax.set_title(\"Survived x Pclass\")","25e01994":"ax = sns.countplot(x=\"Survived\", hue=\"Embarked\", data=train)\nax.set_title(\"Survived x Embarked\")","f612860d":"#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(train, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()","601137a8":"#plot distributions of passangers who survived or not by Fare\na = sns.FacetGrid( train, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Fare', shade= True )\na.set(xlim=(0 , train['Fare'].max()))\na.add_legend()","f87221f6":"train.Fare = train.Fare.astype(int)\n\ntrain['FareBins'] = pd.qcut(train.Fare,4,labels=[1,2,3,4])\ntrain['FareBins'] = train['FareBins'].astype(int)","101ce118":"ax = sns.countplot(x=\"Survived\", hue=\"FareBins\", data=train)\nax.set_title(\"Survived x FareBins\")","a4f2bfc7":"train['FamilySize'] = train ['SibSp'] + train['Parch'] + 1\ntrain['IsAlone'] = 1 #initialize to yes\/1 is alone\ntrain.loc[train['FamilySize'] > 1,'IsAlone'] = 0\n# drop the other columns\ntrain.drop(columns=[\"SibSp\",\"Parch\"],inplace=True)","c9273971":"train.drop(columns=[\"Ticket\"],inplace=True)","df6e9f6a":"ax = sns.countplot(x=\"Survived\", hue=\"FamilySize\", data=train)\nax.set_title(\"Survived x FamilySize\")","06481bc2":"embarked_label  = {\"Q\":1,\"C\":2,\"S\":3}\ntrain.Embarked = train.Embarked.replace(embarked_label)\ntrain.Embarked = train.Embarked.astype(int)","9ca0f3b6":"train.loc[train['Sex'] == 'male','Sex'] = 1\ntrain.loc[train['Sex'] == 'female','Sex'] = 0\ntrain.Sex = train.Sex.astype(int)","402a7a55":"# extract the titles\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ntrain['Title'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n# replace titles\ntrain['Title'] = train['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr','Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain['Title'] = train['Title'].replace('Mlle', 'Miss')\ntrain['Title'] = train['Title'].replace('Ms', 'Miss')\ntrain['Title'] = train['Title'].replace('Mme', 'Mrs')\ntrain['Title'] =train['Title'].astype('category')\ntrain.Title = train.Title.replace(titles)","874ef1d0":"#Drop the column name\ntrain.drop(columns=[\"Name\"],inplace=True)","ca083850":"#Configure the train \nage_guess_columns = ['Sex','Pclass','FamilySize','Title']\nage_train = pd.get_dummies(train.dropna())\ny_age_train = age_train['Age']\nX_age_train = age_train[age_guess_columns]","cb528cad":"# Lets see if our regressor method makes sense\nfrom sklearn.dummy import DummyRegressor\ndummy = DummyRegressor(strategy=\"median\")\n# Fit the model\ndummy.fit(X_age_train,y_age_train)\nkfold = KFold()\ncv_results = cross_val_score(dummy, X_age_train, y_age_train, cv=kfold,scoring='neg_mean_squared_error')\nprint(f\"Kfold get {cv_results.mean()*-1} mse\")","008126f7":"# Fit the model\nforest = RandomForestRegressor()\nforest.fit(X_age_train,y_age_train)\nkfold = KFold()\ncv_results = cross_val_score(forest, X_age_train, y_age_train, cv=kfold,scoring='neg_mean_squared_error')\nprint(f\"Kfold get {cv_results.mean()*-1} mse\")","bdd1516c":"age_guess = train[train.Age.isna()]\ny_age_guess= age_train['Age']\nX_age_guess = age_guess[age_guess_columns]","f814a263":"train.loc[train.Age.isna(),'Age'] =  forest.predict(train[train.Age.isna()][age_guess_columns])","de56dddd":"train['AgeBins'] = pd.cut(train.Age,4,labels=[1,2,3,4])\ntrain['AgeBins'] = train['AgeBins'].astype(int)","1c7653dd":"ax = sns.countplot(x=\"Survived\", hue=\"AgeBins\", data=train)\nax.set_title(\"Survived x AgeBins\")","81aa6fd8":"sns.heatmap(train.corr(),annot=True)","a2b43200":"X_columns = ['Sex','Pclass','FamilySize','IsAlone','Embarked','Title','AgeBins','FareBins','Age'] # 'Fare',\ny_column = 'Survived'","87d3a3e8":"#Train and test\nX_train = train[X_columns]\ny_train = train[y_column]\n\n\n#Scale \nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = pd.DataFrame(scaler.transform(X_train),columns=X_train.columns)","ad69ba25":"from sklearn.model_selection import GridSearchCV\nRF_model = RandomForestClassifier()\nRF_parameters = {'criterion':['gini', 'entropy'],'max_depth':[5,10,20],'max_features':['auto','sqrt','log2'],'max_leaf_nodes':[None,10,15,20],'n_estimators':[50,100,200]}\nRF_clf = GridSearchCV(RF_model, RF_parameters)\nRF_clf.fit(X_train_scaled, y_train)\nprint(RF_clf.best_params_)","9fac09ed":"XB_parameters = {\"learning_rate\":[0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\"max_depth\": [ 3, 4, 5, 6, 8, 10, 12, 15],\"min_child_weight\" : [ 1, 3, 5, 7 ],\"gamma\": [ 0.0, 0.1, 0.2 , 0.3, 0.4 ], \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] }\nXB_model = XGBClassifier()\nXB_clf = GridSearchCV(XB_model, XB_parameters)\nXB_clf.fit(X_train_scaled, y_train)\nprint(XB_clf.best_params_)","fd249293":"from sklearn.feature_selection import RFE\nrfe_selector = RFE(estimator=RandomForestClassifier(**RF_clf.best_params_), n_features_to_select=7, step=10, verbose=5)\nrfe_selector.fit(train[X_columns], train[y_column])\nrfe_support = rfe_selector.get_support()\nrfe_feature = train[X_columns].loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')\nbest_columns = rfe_feature\nprint(best_columns)","c04a43c1":"from imblearn.over_sampling import SMOTE\ntrain.Survived.value_counts(normalize=True)\nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(X_train_scaled, y_train)\n\nX_train_resampled = X_res\ny_train_resampled = y_res\ny_train_resampled.value_counts()","42224aef":"#Classfier selection\nkfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\nmodels = []\nmodels.append(('Random Forest', RandomForestClassifier(**RF_clf.best_params_)))\nmodels.append(('XBR',XGBClassifier(**XB_clf.best_params_)))\n#Variaveis para verificar o melhor\nresults = []\nnames = []\ncv_results= []\nfor name, model in models:\n  cf_result =  cross_val_score(model, X_train_resampled, y_train_resampled, cv=kfold,scoring='accuracy')\n  cv_results.append(cf_result)\n  names.append(name)\n  msg = f\"Classfier: {name},   cross val:{cf_result.mean().round(4)} \" \n  print(msg)","d042affe":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ntest.head()","68cb619e":"\ntest.drop('Cabin',axis=1,inplace=True)\ntest.Embarked.fillna(test.Embarked.mode()[0], inplace=True)\ntest.Fare = test.Fare.fillna(test.Fare.mean())\ntest.Fare = test.Fare.astype(int)\n\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\ntest['IsAlone'] = 1 #initialize to yes\/1 is alone\ntest.loc[test['FamilySize'] > 1,'IsAlone'] = 0\n# drop the other columns\ntest.drop(columns=[\"SibSp\",\"Parch\"],inplace=True)\ntest.drop(columns=[\"Ticket\"],inplace=True)\n\ntest.loc[test['Sex'] == 'male','Sex'] = 1\ntest.loc[test['Sex'] == 'female','Sex'] = 0\ntest.Sex = test.Sex.astype(int)\n\ntest['Title'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n# replace titles\ntest['Title'] = test['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr','Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest['Title'] = test['Title'].replace('Mlle', 'Miss')\ntest['Title'] = test['Title'].replace('Ms', 'Miss')\ntest['Title'] = test['Title'].replace('Mme', 'Mrs')\ntest['Title'] =test['Title'].astype('category')\ntest.Title = test.Title.replace(titles)\n#Drop the column name\ntest.drop(columns=[\"Name\"],inplace=True)\n\ntest.Embarked = test.Embarked.replace(embarked_label)\n#test.Embarked = test.astype(int)\n\n\n# Age fill with regressor\ntest.loc[test.Age.isna(),'Age'] =  forest.predict(test[test.Age.isna()][age_guess_columns])\n\ntest['AgeBins'] = pd.cut(test.Age,4,labels=[1,2,3,4])\ntest['AgeBins'] = test['AgeBins'].astype(int)\n\n\ntest.Fare = test.Fare.astype(int)\n\ntest['FareBins'] = pd.qcut(test.Fare,4,labels=[1,2,3,4])\ntest['FareBins'] = test['FareBins'].astype(int)\n\n\nX_test = pd.get_dummies(test)\n\n#Scaler\nscalerTest = StandardScaler()\nscalerTest.fit(X_test)\nX_test_scaled = pd.DataFrame(scalerTest.transform(X_test),columns=X_test.columns)","8bae54cf":"model = XGBClassifier(**XB_clf.best_params_)\nmodel.fit(X_train_scaled,y_train)\nX_test['Survived'] = model.predict(X_test_scaled[X_train_scaled.columns])","89857e7d":"X_test[['PassengerId','Survived']].to_csv('EduardoRenzTitanic.csv',index=False)","5c626c38":"## Predict the test\n\nLets prepare the data and predict","c2062ebc":"## Data Exploration\n\nNow let's do some exeploration in our dataset\n\nWe ha ve 11 columns**","fc4d8821":"## Prepare the data\n\nNow we will prepare the data for a model","898fc8e9":"Also Ticket is not important","ede12ed0":"Let's Join the Parch and Sibsp Together and determine if the passenger is alone or not","e286841a":"# Feature Selection\n\nNow with the best params, we will use feature selection technique with our model, using RFE.","07918059":"Age is a more complicated issue to resolve, since there are a lot of missing values, we need to do some extra exploration before filling theses missing values.","16b99939":"# Model tuning\n\nLets's find what is the best hyper parameters o use in our models with Grid Search","d9264e4c":"# Import the Libs","5a3fa12c":"## Get missing ages by a model\n\nLets guess ages with a regressor model","19a2738b":"Here we count the amount of null values in our dataset.\n\nAs we can see, there are 3 columns with null values (Age, Cabin and Embarked).","82e1cce7":"Tuning of XBoost Classifier","9811d30b":"Extract te title names to a new column","4d89ca6d":"The mode of this column is 'S', lets fill the null values with this mode","46731e7d":"People who embarked on S have less chance to survive","d80e320f":"Lets change Fare to in, since there no significancy on cents of fare.\nAlso lets separate into bins","a429f287":"Alone pelople have a less chance to survive than pelople with family on board.","ea5a9b9a":"We will assume the Cabin data is not important for this competition, so we can drop it.","ddaaee6a":"### Oversampling\n\nLets oversample the train data with imblearn SMOTE technique \n\nimblearn it's a library to work with unbalanced datasets.\nin this case the lib create new data using clustering.\n\nSee [this link](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html) for more details.\n\n","487b58ad":"Lets Get Best Params of Random Forest","770a0bee":"Lets switch sex string by binary\n\n1 = male\n0 = female","11988e0e":"We have only 2 missing values on Embarked column, let's see a little more about this column.","78d1a989":"With XBBoost","45b46af8":"38% of people in this dataset died.","92bfb2bf":"## Data Engineering","7424a915":"Create Age Bins"}}