{"cell_type":{"61fb26e2":"code","0b4c56dd":"code","2ad3aac7":"code","6f7a8a73":"code","16299a6a":"code","364be252":"code","36e926f6":"code","030205c6":"code","048aaf7a":"code","e9b84056":"code","c4072fe8":"code","f661f846":"code","a53b90a3":"code","d5bf995d":"code","8cecee91":"code","5007a631":"code","6c25ca5b":"code","f80b037e":"code","339fda84":"code","ef633c39":"code","03ee563a":"code","3700357c":"code","4ce8e961":"code","f6165838":"code","a5616e85":"markdown","937ae5db":"markdown"},"source":{"61fb26e2":"pip install neattext","0b4c56dd":"import re \nimport nltk\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport neattext.functions as nfx\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.text import one_hot\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dense, LSTM, Dropout\nfrom keras.callbacks import Callback, EarlyStopping\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","2ad3aac7":"train_df = pd.read_csv('..\/input\/emotions-dataset-for-nlp\/train.txt',header=None, sep=';', names=['Text','Emotion'])\nvalid_df = pd.read_csv('..\/input\/emotions-dataset-for-nlp\/val.txt',header=None, sep=';', names=['Text','Emotion'])\ntest_df = pd.read_csv('..\/input\/emotions-dataset-for-nlp\/test.txt',header=None, sep=';', names=['Text','Emotion'])","6f7a8a73":"train_df.head()","16299a6a":"train_df.isnull().sum()","364be252":"counts = train_df['Emotion'].value_counts()\nsns.barplot(x=counts.index, y=counts)\nplt.xlabel('Emotion')\nplt.ylabel('Count')","36e926f6":"train_df['length'] = train_df['Text'].apply(len)\ntrain_df.head()","030205c6":"train_df['length'].plot(bins=100, kind='hist')","048aaf7a":"train_df['length'].describe()","e9b84056":"train_df[train_df['length']==300]['Text']","c4072fe8":"length_text = sns.FacetGrid(data=train_df, col='Emotion', col_wrap=3)\nlength_text.map(plt.hist, 'length', bins=20, color='b')","f661f846":"lb = LabelEncoder()\ntrain_df['Label'] = lb.fit_transform(train_df['Emotion'])\nvalid_df['Label'] = lb.fit_transform(valid_df['Emotion'])\ntest_df['Label'] = lb.fit_transform(test_df['Emotion'])","a53b90a3":"vocab_size = 1000\nlen_sentence=150","d5bf995d":"from tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nnltk.download('stopwords')\nstopwords = set(nltk.corpus.stopwords.words('english'))","8cecee91":"def text_prepare(data, column):\n    print(data.shape)\n    stemmer = PorterStemmer()\n    corpus = []\n    \n    for text in data[column]:\n        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n        \n        text = text.lower()\n        text = text.split()\n        \n        text = [stemmer.stem(word) for word in text if word not in stopwords]\n        text = \" \".join(text)\n        \n        corpus.append(text)\n    one_hot_word = [one_hot(input_text=word, n=vocab_size) for word in corpus]\n    embeddec_doc = pad_sequences(sequences=one_hot_word,\n                              maxlen=len_sentence,\n                              padding=\"pre\")\n    print(data.shape)\n    return embeddec_doc","5007a631":"X_train=text_prepare(train_df, \"Text\")\nX_valid =text_prepare(valid_df, \"Text\")\nX_test=text_prepare(test_df, \"Text\")","6c25ca5b":"X_train.shape","f80b037e":"y_train = train_df['Label']\ny_valid = valid_df['Label']\ny_test = test_df['Label']","339fda84":"enc = OneHotEncoder()\n\n# y_train\ny_train = np.array(y_train)\ny_train = enc.fit_transform(y_train.reshape(-1,1)).toarray()\n\ny_test\ny_test = np.array(y_test)\ny_test = enc.fit_transform(y_test.reshape(-1,1)).toarray()\n\n# y_valid\ny_valid = np.array(y_valid)\ny_valid = enc.fit_transform(y_valid.reshape(-1,1)).toarray()","ef633c39":"model = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=150, input_length=len_sentence))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(200))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(120, activation=\"relu\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(6, activation=\"softmax\"))","03ee563a":"model.compile(optimizer=\"Adam\", loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","3700357c":"my_callbacks  = [EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=5,\n                              mode='auto')]","4ce8e961":"hist = model.fit(X_train, y_train, epochs = 100, batch_size = 32, validation_data=(X_valid, y_valid),verbose = 1, callbacks=my_callbacks)","f6165838":"fig, ax = plt.subplots(1, 2, figsize=(10, 3))\nax = ax.ravel()\n\nfor i, met in enumerate(['accuracy', 'loss']):\n    ax[i].plot(hist.history[met])\n    ax[i].plot(hist.history['val_' + met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs')\n    ax[i].set_ylabel(met)\n    ax[i].legend(['train', 'val']) ","a5616e85":"# Import Libraries","937ae5db":"# Import Dataset"}}