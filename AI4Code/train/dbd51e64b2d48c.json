{"cell_type":{"0cc60b9b":"code","547029f5":"code","e5a5f58f":"code","911ea665":"code","e5f96750":"code","81707bfd":"code","60173a6d":"code","ba43a7cd":"code","ddd201b1":"code","0f71d571":"code","418d7320":"code","9592bd61":"code","2cef3599":"code","56ed2da3":"code","2ee989f2":"code","18ce0202":"code","179758fd":"code","38e7b28f":"code","d2f93784":"code","ae4f9b78":"code","f9f93711":"code","f97a4787":"code","d5c77e0d":"markdown","f48142dc":"markdown","04310968":"markdown","47e366a4":"markdown","9e841382":"markdown","b94503e7":"markdown","6d766ab4":"markdown","fc2a30df":"markdown","d297101f":"markdown","30423337":"markdown","6a58ac24":"markdown","8f491f14":"markdown","b0c58862":"markdown","fcb9991e":"markdown","c77652b8":"markdown","0f8cc7e8":"markdown","254ab11b":"markdown","f7ebb196":"markdown","10387265":"markdown","2d94a19e":"markdown","6c8f6d68":"markdown","478a7f47":"markdown"},"source":{"0cc60b9b":"%matplotlib inline\nimport os\nimport time\nimport codecs\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, skewnorm\nimport numpy.polynomial.polynomial as poly","547029f5":"'''\nTime when the event occurred. Times are reported in milliseconds since the epoch (1970-01-01T00:00:00.000Z),\nand do not include leap seconds. In certain output formats, the date is formatted for readability; Long Integer\n\nupdated: Time when the event was most recently updated. Times are reported in milliseconds since the epoch.\nIn certain output formats, the date is formatted for readability; Long Integer\n'''\n\n\ndef time_converter(epoch):\n    \"\"\"\n    This function coverts a given millisecond time to GMT+0.0 time\n    Since I'm in GMT+5.0 h zone, I am applying the required correction as well\n    \"\"\"\n    # taken from https:\/\/stackoverflow.com\/a\/12400556\/3212945\n    # 18000000 is correction for GMT+5\n    _tt = time.ctime((epoch-18000000)\/1000)\n    day, month, mmonth = _tt.split()[0], _tt.split()[1], _tt.split()[2]\n\n    return day, month, mmonth\n\n\ndef get_average_by(df, group_by, get_average_of, re_index):\n    \"\"\"\n    Gives the mean of parameter speciifed in a new dataset\n    \n    PARAMETERS:\n    -----------\n                df: dataset to be used\n          group_by: column name in the dataset by which grouping is required\n    get_average_of: column name in the dataset for which mean calculation is required\n          re_index: same as group_by\n    \"\"\"\n    # reindexing idea taken from https:\/\/stackoverflow.com\/a\/30010004\/3212945\n    _temp = df.groupby([group_by]).agg(Mean=(get_average_of, 'mean')).reindex(re_index).reset_index()\n    return _temp\n\n\ndef plot_hist_normdist(x, nbins, plot_hatch, plot_label):\n    \"\"\"\n    Plots histogram with normal and skewnormal distriubtions overplotted on the data\n    \n    PARAMETERS:\n    -----------\n             x: Data for which histogram is required\n         nbins: Number of bins for histogram\n    plot_hatch: Hatch style for matplotlib histogram\n    plot_label: Label to display in plot legend\n     plot_list: A string list containing the word \n    \"\"\"\n    h3, l3 = [], []\n\n    x_new = np.arange(x.min(), x.max(), 0.01)\n    plt.hist(x, bins=nbins, histtype='step', hatch=plot_hatch, label=plot_label)\n    h1, l1 = plt.gca().get_legend_handles_labels()\n    plt.xlabel(plot_label)\n    plt.ylabel('Counts')\n    plt.twinx()\n    plt.plot(x_new, norm.pdf(x_new, *norm.fit(x)), label='Normal distribution')\n    plt.plot(x_new, skewnorm.pdf(x_new, *skewnorm.fit(x)), label='Skewnormal distribution')\n    plt.ylabel('PDF for various normal distributions')\n    plt.gca().set_ylim(bottom=0)\n    h2, l2 = plt.gca().get_legend_handles_labels()\n    for i, j in zip(h1, l1):\n        h3.append(i)\n        l3.append(j)\n    for i, j in zip(h2, l2):\n        h3.append(i)\n        l3.append(j)\n    plt.legend(handles=h3, labels=l3, loc='best')\n    plt.xlabel(plot_label)\n    \n\ndef axis_cheating(plt, x):\n    \"\"\"\n    This function changes labels of plots forcibly\n    \n    PARAMETERS:\n    -----------\n        plt: matplotlib.pypot handle\n          x: list for ticks\n    \"\"\"\n    try:\n        x = list(x)\n        _t = [i for i in x]\n        _t.insert(0, '-1')\n        _t.insert(len(_t), '0')\n        plt.gca().set_xticklabels(_t)\n    except ValueError:\n        x = list(x)\n        _t = [i for i in x]\n        plt.gca().set_xticklabels(_t)\n\n\ndef plot_means(x, y, xlab, xticks):\n    \"\"\"\n    This function plots the mean values for given datasets\n    \n    PARAMETERS:\n    -----------\n         x: first dataset\/dataseries\n         y: second dataset\/dataseries\n      xlab: xlabel for the plot\n    xticks: this parameter is used to correct the xlabels using axis_cheating function\n    \"\"\"\n\n    if xlab == 'Days':\n        lab = 'Daily'\n    elif xlab == 'Months':\n        lab = 'Monthly'\n\n    plt.plot(x['Mean'], 'r-o', label='{} mean of EQ magnitude for 1990'.format(lab))\n    plt.xlabel(xlab)\n    plt.ylabel('Mean magnitude of EQ')\n\n    if xlab=='Months':\n        plt.gca().set_xticks(np.arange(0, 12, 1))\n    \n    axis_cheating(plt, xticks)\n    \n    h1, l1 = plt.gca().get_legend_handles_labels()\n\n    plt.twinx()\n    \n    plt.plot(y['Mean'], 'g-o', label='{} mean of EQ significance for 1990'.format(lab))\n    plt.ylabel('Mean significance of EQ')\n    \n    h2, l2 = plt.gca().get_legend_handles_labels()\n    h1, h2, l1, l2 = h1[0], h2[0], l1[0], l2[0]\n    \n    plt.legend([h1, h2], [l1, l2])\n    plt.grid('on')\n    plt.tight_layout()\n    \n\ndef plot_boxenplots(df, to_x, to_hue, mag_sig, xlab):\n    \"\"\"\n    This function plots sns.boxenplots for given dataset and hues\n    \n    PARAMETERS:\n    -----------\n         df: dataframe\n       to_x: categorical data for x-axis\n     to_hue: cateogrical data for dividing the data further into packets\n    mag_sig: list of string(s) containing column names from df\n    \"\"\"\n\n    for i in mag_sig:\n        plt.figure(figsize=(40,7), dpi=300)\n        f = sns.boxenplot(data=df[[i, to_x, to_hue]], x=to_x, y=i, hue=to_hue, orient='v')\n        if to_x == 'day':\n            f.invert_xaxis()\n        plt.xlabel(xlab)\n        if i == 'mag':\n            plt.ylabel('Magnitude of EQ')\n            f.legend_.set_title(to_hue)\n        else:\n            plt.ylabel('Significance of EQ')\n            f.legend_.set_title(to_hue)\n        plt.tight_layout()","e5a5f58f":"#file_path = os.getcwd() + '\/'\n#file_names = [f for f in os.listdir(os.curdir) if f.startswith('1990') and f.endswith('.csv')]\n\nfile_path = '..\/input\/earthquakes-monthly-usgs-updated-monthly\/'\n\nday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n\nmonth_names = ['jan', 'feb', 'mar', 'apr', 'may', 'jun',\n               'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n\ndf_temp = pd.DataFrame()\n\nfor i, v in enumerate(range(1, 13, 1)):\n    print(file_path + '1990_{}.csv'.format(i+1))\n    temp = pd.read_csv(file_path + '1990_{}.csv'.format(i+1))\n    df_temp = df_temp.append(temp)\n\n# this will be used in a few plots, trust me\nmag_sig = ['mag', 'sig']","911ea665":"df_temp.isna().sum()","e5f96750":"df_temp2 = df_temp[['mag', 'time', 'updated', 'sig', 'type', 'longitude', 'latitude']]","81707bfd":"day, month_na, month_nu = [], [], []\n\nfor i in df_temp2['time']:\n    _d, _m, __m = time_converter(i)\n    day.append(_d)\n    month_na.append(_m)\n    month_nu.append(__m)\n    \nday_df = pd.DataFrame(day)\nday_df.columns = ['day']\n\nmonth_names_df = pd.DataFrame(month_na)\nmonth_names_df.columns = ['month_names']\n\nmonth_numbers_df = pd.DataFrame(month_nu)\nmonth_numbers_df.columns = ['month_numbers']\n\ndf_temp2 = pd.concat([df_temp2.reset_index().drop('index', axis=1), day_df, month_names_df, month_numbers_df], axis=1, sort=False)","60173a6d":"df_temp2.head()","ba43a7cd":"# just to check the time-day correspondance is correct or not\n# it should be a wednesday, according to the dataframe\ntime_converter(633829215390)\n# the result can also be checked from https:\/\/www.epochconverter.com\/","ddd201b1":"plt.figure(figsize=(15,6))\nplt.subplot(211)\nf = sns.histplot(data=df_temp2, x='day', hue='type', multiple='dodge')\nf.set_yscale('log')\nf.set_xlabel('Days')\nplt.tight_layout()\n\nplt.subplot(212)\nf = sns.histplot(data=df_temp2, x='month_names', hue='type', multiple='dodge')\nf.set_xticklabels([i.capitalize() for i in month_names])\nf.set_xlabel('Months')\nf.set_yscale('log')","0f71d571":"# me being lazy\nx, y = df_temp2['mag'], df_temp2['sig']\n\nplt.figure(figsize=(16, 8))\n\nplt.subplot(221)\nplot_hist_normdist(x=x, nbins=64, plot_hatch='\/', plot_label='Magnitude of EQ')\n\nplt.subplot(223)\npd.plotting.boxplot(x, vert=False, figsize=(12, 4))\n_ = plt.xlabel('Magnitude of EQ')\n\nplt.subplot(222)\nplot_hist_normdist(x=y, nbins=64, plot_hatch='\\\\', plot_label='Significance of EQ')\n\nplt.subplot(224)\npd.plotting.boxplot(y, vert=False, figsize=(12, 4))\n_ = plt.xlabel('Significance of EQ')\n\nplt.tight_layout()","418d7320":"for i in ['mag', 'sig']:\n    x, y =  round(df_temp2[i].mean() - df_temp2[i].min(), 4), round(df_temp2[i].mean() - df_temp2[i].max(), 4)\n    \n    print('The difference between mean and min value of {} parameter is {}'.format(i, x))\n    print('The difference between mean and min value of {} parameter is {}\\n'.format(i, y))","9592bd61":"df_temp2[['mag', 'day']].groupby('day').mean().reindex(day_names).T","2cef3599":"df_temp2[['sig', 'day']].groupby('day').mean().reindex(day_names).T","56ed2da3":"_mn = [i.capitalize() for i in month_names]\ndf_temp2[['mag', 'month_names']].groupby('month_names').mean().reindex(_mn).T","2ee989f2":"df_temp2[['sig', 'month_names']].groupby('month_names').mean().reindex(_mn).T","18ce0202":"x1 = get_average_by(df=df_temp2, group_by='day', get_average_of='mag', re_index=day_names)\ny1 = get_average_by(df=df_temp2, group_by='day', get_average_of='sig', re_index=day_names)\nx2 = get_average_by(df=df_temp2, group_by='month_names', get_average_of='mag', re_index=_mn)\ny2 = get_average_by(df=df_temp2, group_by='month_names', get_average_of='sig', re_index=_mn)","179758fd":"plt.figure(figsize=(6, 8))\nplt.subplot(211)\nplot_means(x1, y1, 'Days', day_names)\nplt.subplot(212)\nplot_means(x2, y2, 'Months', _mn)","38e7b28f":"plot_boxenplots(df_temp2, 'day', 'type', mag_sig, 'Days')","d2f93784":"plot_boxenplots(df_temp2, 'month_names', 'type', mag_sig, 'Months')","ae4f9b78":"plot_boxenplots(df_temp2, 'type', 'day', mag_sig, 'EQ types')","f9f93711":"plot_boxenplots(df_temp2, 'type', 'month_names', mag_sig, 'Months')","f97a4787":"px.scatter_geo(df_temp2, 'latitude', 'longitude',\n               hover_data=['latitude','longitude', 'time', 'mag', 'sig'], color='type')","d5c77e0d":"### CUSTOM FUNCTIONS","f48142dc":"### CHECKING NAs","04310968":"### LIBRARY IMPORTS","47e366a4":"### Let's first see what events have caused how much detections per day and per month for the year 1990.","9e841382":"Now, let's plot these averages.","b94503e7":"This is all for now :)","6d766ab4":"THAT'S A LOT OF SKEWNESS.","fc2a30df":"### GEO PLOT,","d297101f":"### READ THE FILES FOR YEAR 1990\n\nI am adding `month_names`, `month_numbers` to the dataset. These two will be used to categorize the dataset.","30423337":"Let's add days to the dataset as well using `time_converter` function.","6a58ac24":"## Visualisation, FINALLY !!","8f491f14":"* Both the **magnitude** and **significance** data is highly skewed and contains a LOT of outliers\n* Both dataset show bimodal shape (they have to peaks) and do not fit either normal\/skewnormal curves","b0c58862":"### Let's check the mean\/max\/min distance for both these datasets.","fcb9991e":"Let's get some averages","c77652b8":"### BOXEN PLOTS, A LOT OF THEM","0f8cc7e8":"# Time series analysis of earthquake data provided by [Itokiana RAFIDINARIVO](https:\/\/www.kaggle.com\/itokianarafidinarivo)\n\n\n## Disclaimer\nThis is the first of two (or three) notebooks on EQ dataset intended for a time series analysis.\n\n## A note for reader\nThis is my second attempt at a time series analysis. The first one was performed on [Surface Solar Radiation Dataset](https:\/\/www.kaggle.com\/saurabhshahane\/surface-solar-radiation-dataset) by [Saurabh Shahane](https:\/\/www.kaggle.com\/saurabhshahane) and can be found [here](https:\/\/www.kaggle.com\/syedalimohsinbukhari\/surface-solar-radiation-time-series-analysis). So, if there was something you wanted to see but did not find, kindly let me know. It will be of great help to me. Suggestions, corrections, and constructive feedbacks are all welcomed.\n\n## Plan for future notebook(s)\nThis notebook only covers the year 1990 as an exploratory dataset. Future notebook(s) with the complete data set will include 1990 year as well.\n\n## Caveat in the code\nThis code is using `ctime`, which can make the dataset shift depending upon what time zone you're in. For my local PC, the averages were different than what are shown in **DAILY\/MONTHLY MEANS OF THE MAG\/SIG** here on kaggle. I tried several times, but I could not get my local system and kaggle data to get synchronized. If you know how to resolve this issue, kindly let me know.","254ab11b":"* The EQ\/quarry blast\/explosion, although numerous in numbers having large outliers have lower means compared to the sparsely occuring nuclear explosions, even lower than rock burst events.\n* The monthly\/daily average of EQ\/QB\/E events is almost same (also seen in the distribution plot earlier), however the mean values for NE is not evenly distributed.","f7ebb196":"* The number of **EQ, quarry blast** events are quite even across all days\/months except **Sunday** for QB.\n* The number of events for **Explosion** are also even except for **Sunday** and the month of **February, March**, and **December**.\n* Rock burst have minimal contribution in the weekly data and are mostly seen in the month of **Decmber** only.\n*  **Sunday\/Monday** and **January\/February\/March** have been quite friendly in the world. No `nuclear explosions` :D","10387265":"We'll not take the rows with any 0\/NA value.\n\nAlso, there are two major cateogrical columns\n* magType: magnitude type of earthquake event\n*    type: type of event from which the reading was recorded\n\nAs the dataset already contain two similar-to-one-another columns (`mag`, `sig`) I will only keep `type` column for my use, and also to reduce redundancy.","2d94a19e":"Now let's see the distribution of `mag` and `sig` values on `daily` and `montly` basis, divided by `type` of the event.","6c8f6d68":"### DAILY\/MONTHLY MEANS OF THE MAG\/SIG","478a7f47":"### Now let's focus a bit on the magnitude of EQs and their significance"}}