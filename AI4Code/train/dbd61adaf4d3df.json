{"cell_type":{"34aeb927":"code","f8b60e83":"code","4a5f9bf6":"code","0f955d44":"code","06a93f7d":"code","c258a7d9":"code","0b6a5227":"code","e4611eb6":"code","90d08a6d":"code","a80a1b69":"code","37fe9efb":"code","aac40e5e":"code","dd596b8f":"code","e83f8a53":"code","cf5078d0":"code","c71b631b":"code","896be131":"code","6848cd9b":"code","3e0890de":"code","db0862c7":"code","c04eb2dd":"code","3e1218c0":"code","97d80492":"code","af31d0bd":"code","90d2647e":"code","e57c6157":"code","9ba4c6b9":"code","20ab737f":"code","ed65ff86":"code","c57adcae":"code","a8f307a2":"code","446cc42f":"code","b6506f57":"markdown","b2d45b58":"markdown","18ada1fd":"markdown","9f773f08":"markdown","fe803dc2":"markdown"},"source":{"34aeb927":"import keras\nfrom keras.datasets import fashion_mnist \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans ","f8b60e83":"(X_train,y_train), (X_test,y_test) = fashion_mnist.load_data()","4a5f9bf6":"#Sanity check\nX_train.shape #60000 images of dimensions 28 x 28","0f955d44":"#Reshapeing X to a 2D array for PCA and then k-means\nX = X_train.reshape(-1,X_train.shape[1]*X_train.shape[2]) #We will only be using X for clustering\nX.shape","06a93f7d":"y = y_train","c258a7d9":"#Sanity check\nprint (\"The shape of X is \" + str(X.shape))\nprint (\"The shape of y is \" + str(y.shape)) #We will be using y only to check our clustering ","0b6a5227":"#Visualise an image \nn= 2 #Enter Index here to View the image \nplt.imshow(X[n].reshape(X_train.shape[1], X_train.shape[2]), cmap = plt.cm.binary)\nplt.show()\ny[n]","e4611eb6":"# To perform PCA we must first change the mean to 0 and variance to 1 for X using StandardScalar\nClus_dataSet = StandardScaler().fit_transform(X) #(mean = 0 and variance = 1)","90d08a6d":"from sklearn.decomposition import PCA\n# Make an instance of the Model\nvariance = 0.98 #The higher the explained variance the more accurate the model will remain\npca = PCA(variance)","a80a1b69":"#fit the data according to our PCA instance\npca.fit(Clus_dataSet)","37fe9efb":"print(\"Number of components before PCA  = \" + str(X.shape[1]))\nprint(\"Number of components after PCA 0.98 = \" + str(pca.n_components_)) #dimension reduced from 784","aac40e5e":"#Transform our data according to our PCA instance\nClus_dataSet = pca.transform(Clus_dataSet)","dd596b8f":"print(\"Dimension of our data after PCA  = \" + str(Clus_dataSet.shape)) ","e83f8a53":"#To visualise the data inversed from PCA\napproximation = pca.inverse_transform(Clus_dataSet)\nprint(\"Dimension of our data after inverse transforming the PCA  = \" + str(approximation.shape))","cf5078d0":"#image reconstruction using the less dimensioned data\nplt.figure(figsize=(8,4));\n\nn = 500 #index value, change to view different data\n\n# Original Image\nplt.subplot(1, 2, 1);\nplt.imshow(X[n].reshape(X_train.shape[1], X_train.shape[2]),\n              cmap = plt.cm.gray,);\nplt.xlabel(str(X.shape[1])+' components', fontsize = 14)\nplt.title('Original Image', fontsize = 20);\n\n# 196 principal components\nplt.subplot(1, 2, 2);\nplt.imshow(approximation[n].reshape(X_train.shape[1], X_train.shape[2]),\n              cmap = plt.cm.gray,);\nplt.xlabel(str(Clus_dataSet.shape[1]) +' components', fontsize = 14)\nplt.title(str(variance * 100) + '% of Variance Retained', fontsize = 20);\n\nprint(y_train[n])","c71b631b":"#THIS CODE TAKES A LONG TIME TO RUN, IT IS TO FIND THE n_init VALUE.\n# #We will use k = 10, not the best choice but for simplicity as the INDEX has 10 values\n# #to check for best n_init with k = 10\n# inertia = []\n# for k in range(5, 100):\n#     kmeans = KMeans(init = \"k-means++\",n_clusters=10, n_init = k,random_state=1).fit(Clus_dataSet)\n#     inertia.append(np.sqrt(kmeans.inertia_))\n\n# plt.plot(range(5, 100), inertia, marker='s');\n# plt.xlabel('$k$')\n# plt.ylabel('$J(C_k)$');","896be131":"#n_clusters = 10 because INDEX has 10 values. Not the best value but a simple logic.\n#The value of n_init at 35 yields good results so we will use it. For confirmation us the above code.\nk_means = KMeans(init = \"k-means++\", n_clusters = 10, n_init = 35)","6848cd9b":"#fit the data to our k_means model\nk_means.fit(Clus_dataSet)","3e0890de":"k_means_labels = k_means.labels_ #List of labels of each dataset\nprint(\"The list of labels of the clusters are \" + str(np.unique(k_means_labels)))","db0862c7":"G = len(np.unique(k_means_labels)) #Number of labels\n\n#2D matrix  for an array of indexes of the given label\ncluster_index= [[] for i in range(G)]\nfor i, label in enumerate(k_means_labels,0):\n    for n in range(G):\n        if label == n:\n            cluster_index[n].append(i)\n        else:\n            continue        ","c04eb2dd":"#Visualisation for clusters = clust\nplt.figure(figsize=(20,20));\nclust = 8 #enter label number to visualise\nnum = 100 #num of data to visualize from the cluster\nfor i in range(1,num): \n    plt.subplot(10, 10, i); #(Number of rows, Number of column per row, item number)\n    plt.imshow(X[cluster_index[clust][i+500]].reshape(X_train.shape[1], X_train.shape[2]), cmap = plt.cm.binary);\n    \nplt.show()","3e1218c0":"Y_clust = [[] for i in range(G)]\nfor n in range(G):\n    Y_clust[n] = y[cluster_index[n]] #Y_clust[0] contains array of \"correct\" category from y_train for the cluster_index[0]\n    assert(len(Y_clust[n]) == len(cluster_index[n])) #dimension confirmation\n","97d80492":"#counts the number of each category in each cluster\ndef counter(cluster):\n    unique, counts = np.unique(cluster, return_counts=True)\n    label_index = dict(zip(unique, counts))\n    return label_index","af31d0bd":"label_count= [[] for i in range(G)]\nfor n in range(G):\n    label_count[n] = counter(Y_clust[n])\n\nlabel_count[1] #Number of items of a certain category in cluster 1","90d2647e":"class_names = {0:'T-shirt\/top', 1:'Trouser',2: 'Pullover',3: 'Dress',4: 'Coat',5:\n               'Sandal',6: 'Shirt', 7:'Sneaker',8:  'Bag',9: 'Ankle boot'} #Dictionary of class names\n\n#A function to plot a bar graph for visualising the number of items of certain category in a cluster\ndef plotter(label_dict):\n    plt.bar(range(len(label_dict)), list(label_dict.values()), align='center')\n    a = []\n    for i in [*label_dict]: a.append(class_names[i])\n    plt.xticks(range(len(label_dict)), list(a), rotation=45, rotation_mode='anchor')","e57c6157":"#Bar graph with the number of items of different categories clustered in it\nplt.figure(figsize=(20,20))\nfor i in range (1,11):\n    plt.subplot(5, 2, i)\n    plotter(label_count[i-1]) \n    plt.title(\"Cluster\" + str(i-1))","9ba4c6b9":"k_means_cluster_centers = k_means.cluster_centers_ #numpy array of cluster centers\nk_means_cluster_centers.shape #comes from 10 clusters and 420 features ","20ab737f":"#cluster visualisation\nmy_members = (k_means_labels == 3) #Enter different Cluster number to view its 3D plot\nmy_members.shape\nfig = plt.figure(figsize=(15, 10))\nax = fig.add_subplot(1,1,1,projection='3d')\n#Clus_dataSet.shape\n#Clus_dataSet[my_members,300].shape\nax.plot(Clus_dataSet[my_members, 0], Clus_dataSet[my_members,1],Clus_dataSet[my_members,2], 'w', markerfacecolor=\"blue\", marker='.',markersize=10)","ed65ff86":"#install these if you haven't\n#!pip install chart_studio \n#!pip install plotly","c57adcae":"import plotly as py\nimport plotly.graph_objs as go\nimport plotly.express as px","a8f307a2":"#3D Plotly Visualisation of Clusters using go\n\nlayout = go.Layout(\n    title='<b>Cluster Visualisation<\/b>',\n    yaxis=dict(\n        title='<i>Y<\/i>'\n    ),\n    xaxis=dict(\n        title='<i>X<\/i>'\n    )\n)\n\ncolors = ['red','green' ,'blue','purple','magenta','yellow','cyan','maroon','teal','black']\ntrace = [ go.Scatter3d() for _ in range(11)]\nfor i in range(0,10):\n    my_members = (k_means_labels == i)\n    index = [h for h, g in enumerate(my_members) if g]\n    trace[i] = go.Scatter3d(\n            x=Clus_dataSet[my_members, 0],\n            y=Clus_dataSet[my_members, 1],\n            z=Clus_dataSet[my_members, 2],\n            mode='markers',\n            marker = dict(size = 2,color = colors[i]),\n            hovertext=index,\n            name='Cluster'+str(i),\n   \n            )\n\nfig = go.Figure(data=[trace[0],trace[1],trace[2],trace[3],trace[4],trace[5],trace[6],trace[7],trace[8],trace[9]], layout=layout)\n    \npy.offline.iplot(fig)\n","446cc42f":"#If you hover over the points in the above plots you get an index value\nn = 56180 #Use that value here to visualise the selected data\nplt.imshow(X[n].reshape(28, 28), cmap = plt.cm.binary)\nplt.show()\n","b6506f57":"The KMeans class has many parameters that can be used, but we will be using these three:\n<ul>\n    <li> <b>init<\/b>: Initialization method of the centroids. <\/li>\n    <ul>\n        <li> Value will be: \"k-means++\" <\/li>\n        <li> k-means++: Selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.<\/li>\n    <\/ul>\n    <li> <b>n_clusters<\/b>: The number of clusters to form as well as the number of centroids to generate. <\/li>\n    <ul> <li> Value will be: 10 ( we have 10 classes according to INDEX)<\/li> <\/ul>\n    <li> <b>n_init<\/b>: Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. <\/li>\n    <ul> <li> Value will be: Depend on our inertia results <\/li> <\/ul>\n<\/ul>\n\nInitialize KMeans with these parameters, where the output parameter is called <b>k_means<\/b>.","b2d45b58":"# PCA (Principle Component Analysis)","18ada1fd":"For Cluster 9 you can its mostly Ankle Boots with a few other items","9f773f08":"## K-MEANS ++","fe803dc2":"**INDEX of the dataset**\nWill be used to check our clusters\n* 0 is Tshirt\n* 1 is Trouser \n* 2 is Pullover\n* 3 is Dress\n* 4 is Coat\n* 5 is Sandal\n* 6 is Shirt\n* 7 is Sneaker\n* 8 is Bag\n* 9 is Ankle Boot"}}