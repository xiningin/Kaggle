{"cell_type":{"ce26b65d":"code","986bb6fb":"code","eedae7b9":"code","0df0a360":"code","86482c7a":"code","77cd29a2":"code","6e190a43":"code","495e3ece":"code","a8ff901d":"code","756fd5b3":"code","9081eaa9":"code","b9962ded":"code","42e5aa54":"code","19504b2c":"code","c9d07641":"code","ead85449":"code","76357048":"code","219b92d2":"code","95857958":"code","7ec74fed":"code","e916a3a8":"code","4ebe7176":"code","7a5b85d9":"code","11f3f099":"code","7e73d912":"code","5936ab38":"code","df0af20b":"code","64530b48":"code","b037fd34":"code","024ecc42":"code","5d8c9e6f":"code","825c26e2":"code","12b78cff":"code","23666830":"code","a94540b0":"code","b8adefa2":"code","ad35243c":"code","848bdb00":"code","c7436fab":"code","af4648f4":"code","a3b78d3a":"code","00322a70":"code","7a383068":"code","0b18f6e2":"code","b90fddc7":"code","f5d854be":"code","1bbb99a5":"code","ec4ef1b5":"code","976a9fb5":"code","47219f38":"code","0f0f6704":"code","05025272":"code","f3853ead":"code","b3c28fd4":"code","5281fc34":"code","2eb1c2c2":"code","6f0b414e":"code","c1ece328":"code","28d9ebc2":"code","a2208fc0":"code","ba10483d":"code","e024f0a9":"code","42f46ab0":"code","fc604ae2":"code","f8ab60be":"code","7f74b002":"code","d3132390":"code","d7cb1b7b":"code","3cda157f":"code","89c19e9f":"code","cf989acb":"code","0d90295e":"code","c12eb784":"code","aec97462":"code","5908d795":"code","b65f2f8d":"code","bf2727f8":"code","25168447":"code","e3ae8f78":"code","c06c6a84":"code","f5c4d3e9":"code","f4fd7f51":"code","898218cc":"code","00fe034c":"code","a48bd07b":"code","692ecbea":"code","8e60cdca":"code","96a88526":"code","85bda766":"markdown","63b0489c":"markdown","6383480e":"markdown","38ab13a8":"markdown","f79197f5":"markdown","3f123876":"markdown","f72db7db":"markdown","10268238":"markdown","b9f57bcd":"markdown","e68dd574":"markdown","2e90559b":"markdown","e120d4e3":"markdown","362e125c":"markdown","34716edf":"markdown","67c727d8":"markdown","0ac1fd0d":"markdown","8bbac538":"markdown","47f19ae7":"markdown","0aef2043":"markdown","3f5d0be2":"markdown","9cc822a0":"markdown","84eae068":"markdown","b65de775":"markdown","a55ccaee":"markdown","3e08fbf2":"markdown"},"source":{"ce26b65d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","986bb6fb":"import matplotlib.pyplot as plt\nimport seaborn as sns","eedae7b9":"plt.figure(figsize=(15,18))\nim=plt.imread(\"..\/input\/king-county-map\/king county.jpg\")\nplt.imshow(im) \n#This is the actual map of the county","0df0a360":"df=pd.read_csv(\"..\/input\/housesalesprediction\/kc_house_data.csv\")\ndf.head()","86482c7a":"df.isnull().sum()\n# Here we see that there is no missing data in this data set","77cd29a2":"df.describe().transpose()\n#Here we get overall statistical description of our data set","6e190a43":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(10,8))\nsns.distplot(df[\"price\"])","495e3ece":"fig, ax = plt.subplots(2)  \nsns.countplot(df[\"bedrooms\"],  ax=ax[0])\nsns.countplot(df[\"floors\"],  ax=ax[1])\n\n","a8ff901d":"df.corr()","756fd5b3":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),linewidths=0.5,annot=True)","9081eaa9":"df.corr()[\"price\"].sort_values(ascending=False)\n#Here we can clearly see that there is positive high correlation between house prices and sqft_living(Square footage of the apartments interior living space)","b9962ded":"plt.figure(figsize=(15,10))\nsns.scatterplot(x=\"sqft_living\", y=\"price\", data=df,color=\"red\")\n#Here we visualize the relation between house prices and the square of the living area","42e5aa54":"plt.figure(figsize=(15,10))\nsns.scatterplot(x=\"long\", y=\"price\", data=df, color=\"red\")","19504b2c":"plt.figure(figsize=(15,10))\nsns.scatterplot(x=\"lat\",y=\"price\", data=df, color=\"green\")","c9d07641":"df.plot(x=\"long\",y=\"lat\",c=\"price\", kind=\"scatter\",alpha=0.5,figsize=(20,15), cmap=plt.get_cmap(\"jet\"), colorbar=True, s=df[\"grade\"])\n#here we visualize the longitude and latitude and get the actual ara of the county and their relation with the price\n#We can easily see that the prices between 47.7 and 47.5 latitude has the highes prices","ead85449":"plt.figure(figsize=(15,20))\nim=plt.imread(\"..\/input\/king-county-map\/king county.jpg\")\nplt.imshow(im) \n#This is the actual map of the county and it correlates with our langitude and latitude plot above","76357048":"plt.figure(figsize=(15,10))\nsns.scatterplot(x=\"long\",y=\"lat\",data=df, hue=\"price\",palette=\"rocket\",alpha=0.8)","219b92d2":"df.describe()[\"price\"]\n#Here we the overall statistical information about house pricess and the outliers begins from 3.2 millon dolars","95857958":"df[df[\"price\"]>3000000]\n#Here we can see that there are only 40 houses that have higher than 3 million dolar house price","7ec74fed":"df_without_outliers=df.sort_values(\"price\",ascending=False).iloc[46:]\ndf_without_outliers.sort_values(\"price\",ascending=False).head()\n#Now we have created a new data frame that has house prices lower than 3 million dolars","e916a3a8":"plt.figure(figsize=(15,10))\nsns.scatterplot(x=\"long\",y=\"lat\",data=df_without_outliers, hue=\"price\",palette=\"rocket\",alpha=0.8)","4ebe7176":"df_without_outliers.plot(x=\"long\",y=\"lat\",c=\"price\", kind=\"scatter\",alpha=0.5,figsize=(20,15), cmap=plt.get_cmap(\"jet\"), colorbar=True)\n","7a5b85d9":"plt.figure(figsize=(14,6))\nsns.boxplot(x=\"waterfront\", y=\"price\", data=df)","11f3f099":"df.head()","7e73d912":"df.drop(\"id\",axis=1, inplace=True)\ndf.head(3) # Now we dropped the id column from the data frame","5936ab38":"df[\"date\"]=pd.to_datetime(df[\"date\"])\ndf.head(3) #here we have changed the structure of the date column in order use it better and make some feature engineering","df0af20b":"#Here we will create two new columns by feature enginnering in order to analyze data according time properties\ndf[\"year\"]=df[\"date\"].apply(lambda date: date.year)\ndf[\"month\"]=df[\"date\"].apply(lambda date: date. month)","64530b48":"df[[\"year\",\"month\"]].head()\n#Here we have added our new feature that were hidden in the date column","b037fd34":"df.groupby(\"month\").mean()[\"price\"]\n#here we cna see the average price per month","024ecc42":"fig, ax = plt.subplots(1,2) \ndf.groupby(\"month\").mean()[\"price\"].plot(ax=ax[0], figsize=(15,6),c=\"red\")\ndf.groupby(\"year\").mean()[\"price\"].plot(ax=ax[1], figsize=(15,6),c=\"red\")\n#In the plot below, we see that the prices tends to become higher from march to july\n#The housing prices rise up from 2014 to 2015","5d8c9e6f":"df.drop(\"date\", axis=1, inplace=True)\n#There is not to do with the date column and we get all the useful data via feature engineering","825c26e2":"df.head()","12b78cff":"df[\"zipcode\"].value_counts()\n# we need to drop zipcode column because ml algorithm will treat this as continues value and then cause wrong predcitions\n# We can not make them dummy variables because there 70 ifferent zip codes","23666830":"df.drop(\"zipcode\", axis=1, inplace=True)\ndf.head()","a94540b0":"def renovation(feature):\n    if feature > 0:\n        feature=1\n    return feature\n#here we create a function that will assign 1 for those that are renovated and 0 those that are not renovated\n        ","b8adefa2":"df[\"yr_renovated\"]=df[\"yr_renovated\"].apply(renovation)","ad35243c":"df[\"yr_renovated\"]. value_counts(). head(70)\n#Now we have just two class as 0 for non-renovated ones and 1 for renovated ones","848bdb00":"#Here we assign features  to the X and price to the y \nX=df.drop(\"price\",axis=1).values\ny=df[\"price\"].values","c7436fab":"X","af4648f4":"y","a3b78d3a":"from sklearn.model_selection import train_test_split","00322a70":"X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3)\n#Here we split our data as train and test set","7a383068":"X_train.shape","0b18f6e2":"y_train.shape","b90fddc7":"X_test.shape","f5d854be":"y_test.shape","1bbb99a5":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n","ec4ef1b5":"model=Sequential() #here we get an insance of our model\nmodel.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel.add(Dense(1)) # here we add a dthe fina layer with 1 neurons because we have one output, that is the house price\n","976a9fb5":"model.compile(optimizer=\"adam\", loss=\"mse\")\n#Here assign adam optimizer as our optimizer and mean squared error as our loss function for our deep learning model","47219f38":"model.fit(x= X_train, y= y_train, batch_size=128, epochs=300, validation_data=(X_test, y_test))\n#Here we fit our model into the training X and y set with batch_size 128 and 300 epaochs, and we use also test dataset as validation","0f0f6704":"pd.DataFrame(model.history.history)\n#Here we can see losses in both our loss function and validation loss in the test data ","05025272":"\npd.DataFrame(model.history.history).plot(figsize=(15,10))\n#There happens a decrease in both our training and validation loss to a certain point and become stable after 60.th epoch\n#Moreover there is no overfitting because both lines goes with a perfect harmony","f3853ead":"from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score","b3c28fd4":"predictions=model.predict(X_test) #here the trained algorithm makes predictions","5281fc34":"print(\"The absolute mean error :\",mean_absolute_error(y_test, predictions))\nprint(\"The squared mean error :\",mean_squared_error(y_test, predictions))\nprint(\"The squared mean error :\",np.sqrt(mean_squared_error(y_test, predictions)))\n","2eb1c2c2":"print(\"The mean of the real data: \",df[\"price\"].mean())\nprint(\"The absolute mean error :\",mean_absolute_error(y_test, predictions))","6f0b414e":"print(\"The Variance Score :\", explained_variance_score(y_test, predictions))\n#The variance shows how many percent that our model can explain,so our model can explain %58 procent accurately","c1ece328":"sns.distplot((y_test-predictions),color=\"red\")","28d9ebc2":"plt.figure(figsize=(18,8))\nplt.scatter(y_test, predictions)\nplt.scatter(y_test,y_test,color=\"red\")\n#In this we can see that our model predict lower and normal house prices very good, but the oulier not good\n#The outlier affects negatively the performance of our model","a2208fc0":"from sklearn.preprocessing import StandardScaler","ba10483d":"scaler=StandardScaler()","e024f0a9":"scaler.fit(X_train)","42f46ab0":"scaler.transform(X_train)","fc604ae2":"scaler.transform(X_test)","f8ab60be":"model2=Sequential() #here we get an insance of our model\nmodel2.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel2.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel2.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel2.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel2.add(Dense(1)) # here we add a dthe fina layer with 1 neurons because we have one output, that is the house price","7f74b002":"model2.compile(optimizer=\"adam\", loss=\"mse\")\n#Here assign adam optimizer as our optimizer and mean squared error as our loss function for our deep learning model","d3132390":"model2.fit(x= X_train, y= y_train, batch_size=64, epochs=300, validation_data=(X_test, y_test))","d7cb1b7b":"pd.DataFrame(model2.history.history)","3cda157f":"pd.DataFrame(model2.history.history).plot(figsize=(15,8))","89c19e9f":"predictions2=model2.predict(X_test)","cf989acb":"print(\"The absolute mean error :\",mean_absolute_error(y_test, predictions2))\nprint(\"The squared mean error :\",mean_squared_error(y_test, predictions2))\nprint(\"The squared mean error :\",np.sqrt(mean_squared_error(y_test, predictions2)))\n","0d90295e":"print(\"The mean of the real data: \",df[\"price\"].mean())\nprint(\"The absolute mean error :\",mean_absolute_error(y_test, predictions2))","c12eb784":"print(\"The Variance Score :\", explained_variance_score(y_test, predictions2))\n#The variance shows how many percent that our model can explain,so our model can explain %58 procent accurately","aec97462":"\nplt.figure(figsize=(10,15))\nplt.scatter(y_test,predictions2)\nplt.scatter(y_test,predictions, color=\"green\")\nplt.scatter(y_test,y_test, color=\"red\")","5908d795":"X=df.drop(\"price\", axis=1)\ny=df[\"price\"]","b65f2f8d":"X_train, X_test, y_train, y_test= train_test_split(X,y)","bf2727f8":"from sklearn.linear_model import LinearRegression","25168447":"model3=LinearRegression()","e3ae8f78":"model3.fit(X_train,y_train)","c06c6a84":"predictions3=model3.predict(X_test)","f5c4d3e9":"print(\"Predictions of Linear Regression:\",mean_absolute_error(y_test,predictions3))","f4fd7f51":"\nplt.figure(figsize=(10,15))\nplt.scatter(y_test,predictions3, color=\"green\")\nplt.scatter(y_test,y_test, color=\"red\")\n","898218cc":"model4=Sequential() #here we get an insance of our model\nmodel4.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel4.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel4.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel4.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel4.add(Dense(1)) # here we add a dthe fina layer with 1 neurons because we have one output, that is the house price","00fe034c":"model4.compile(optimizer=\"rmsprop\",loss=\"mse\")","a48bd07b":"model4.fit(x= X_train, y= y_train, batch_size=64, epochs=300, validation_data=(X_test, y_test))","692ecbea":"predictions4=model4.predict(X_test)","8e60cdca":"print(mean_absolute_error(y_test,predictions4))","96a88526":"plt.figure(figsize=(10,15))\nplt.scatter(y_test,predictions4, color=\"green\")\nplt.scatter(y_test,y_test, color=\"red\")\n","85bda766":"Now we can see better the price distribution according to the latitude and longitude","63b0489c":"Now our model decreased mean error from 149 000 dolar to 135 000.","6383480e":"Now we rescaled our features and it is ready for ML algorithm","38ab13a8":"> There is 149 000 dolar error and it means %20 procent error that our model makes","f79197f5":"We changed the optimizer from adams to rmsprop, but predictions are worse than before.\n\nThe next step is to drop the outliers and train the model again","3f123876":"The next step is to chech whether our data shapes in train and test set comply with each other","f72db7db":"# 1. Exploratory Data Analysis:","10268238":"Below we will make some feature engineering for yr_renovated column because majority of the houses are not renovated","b9f57bcd":"# 5. Retraining Our Model","e68dd574":"In order to get better distribution, we can drop some outliers","2e90559b":"From this plot above, we understanda that the longitude between -122.0 and -122.4 has the most expensive prices, and the lontitude -121.4 has the lowest house prices","e120d4e3":"Linear Regression performs better than our deep learning model","362e125c":"we can just drop the id column because it has nor a special mening for predicting house prices","34716edf":"#Therefore, I will just create a new data frame without these outliers and create geographical maps again","67c727d8":"The boxplot above shows that the houses near waterfront have higher house prices","0ac1fd0d":"# 3. Splitting Data and Training the Algorithm:","8bbac538":"Because  the predictions of our model is not good enough, we will standardize our features and retrain the model","47f19ae7":"Now we will use linear regression:","0aef2043":"Our target is house prices, we can also special correlation of prices with the other features","3f5d0be2":"From this plot above, we understand that the latitude between 47.5 and 47.7 has the most expensive house prices, and the latitudes between 47.2 and 47.4 has the lowest house prices","9cc822a0":"# 2. Feature Engineering:","84eae068":"The variance also increased from %58 to %65","b65de775":"# 4. Predicting and Evaluation of the Model's Performance","a55ccaee":"Now our data is ready for machine learning algorithm","3e08fbf2":"In the distribution plot above, we see that the house prices are mainy distributed between 0 and 1 million dolar, and there some extreme outliers that we can just skip in order to prevent their influence over our deep learning model"}}