{"cell_type":{"954c1914":"code","076fb86c":"code","76d7389f":"code","84c561eb":"code","e21154d3":"code","34c77985":"code","cb031648":"code","41d379f8":"code","4c8fe286":"code","57ea3a22":"code","1a2943ac":"code","d1a6b032":"code","620a7b92":"code","cd9cb9ff":"code","e2e71968":"code","1d7a2167":"code","093fa4d2":"code","302628e3":"code","da2cd51a":"code","e6e413ce":"code","eb4e2aea":"code","14af9f60":"code","849b1a9c":"code","65c1131d":"code","7bedbde8":"code","5407c0a6":"code","112901eb":"code","1980949e":"code","2894689a":"code","79203ac6":"code","493b53e9":"code","e0035d0f":"code","b343bcd7":"code","fa8e561f":"code","f7c22564":"code","464aab63":"code","9c3cef2c":"code","56545294":"code","dc8c6447":"code","37b13168":"code","865156a1":"code","e163fbc3":"code","d55bda69":"code","0726c0d1":"code","38b3723b":"code","e703461d":"code","1f10c1ad":"code","b677938c":"code","d4a1dc26":"code","257c9812":"code","6a7b84f3":"code","d9e22cbe":"code","6ebc828e":"code","1ead4cfd":"code","58e45de2":"code","76f204ed":"code","9741ac32":"code","c8774992":"code","2c2097bb":"code","118a48e2":"code","cb554633":"code","02655ca0":"code","5deafad9":"code","6b56329f":"code","d5c60322":"code","23c782ca":"code","e60631a0":"code","51f95f35":"code","32f37e3e":"code","0424312c":"code","85d89a3c":"code","98b1bfd3":"code","2c189c71":"code","290f57f4":"code","17925531":"code","f9d1b71d":"code","1ca536e8":"code","bb2e9f2f":"code","eff32088":"code","1b7b1f3b":"code","77b9282c":"code","f6524673":"code","191499ec":"code","0dadecf7":"code","8626be65":"code","9df49737":"code","f8c8c5e5":"code","643336d7":"code","31b71b32":"code","ce63f508":"code","9ebf8869":"code","777a7144":"code","5b7271a4":"markdown","1d151731":"markdown","32ec2944":"markdown","df55bcf9":"markdown","52c0d283":"markdown","c8125716":"markdown","cb913b21":"markdown","94fffd51":"markdown","1739d3aa":"markdown","5f5ba86e":"markdown","0f6fab70":"markdown","2274bc4e":"markdown","811df850":"markdown","c676ef0c":"markdown","467e6c75":"markdown","487f2405":"markdown","e1267347":"markdown","d8e556ae":"markdown","6776b5bf":"markdown","c5494d75":"markdown","bfa344d1":"markdown","87e8156f":"markdown","492c37c9":"markdown","79fe70db":"markdown","2d0976a9":"markdown","52665e49":"markdown","e5ff60b4":"markdown","057d8b78":"markdown","0df591bc":"markdown","2f8e780f":"markdown","3745e02c":"markdown","7d1f02bd":"markdown","3e38d022":"markdown","80f360de":"markdown","39fde902":"markdown","a6b92c7f":"markdown","80fa59b2":"markdown","4d4675bb":"markdown","c8483c8f":"markdown","08a25ab4":"markdown","67c1b2ea":"markdown","e349835a":"markdown","3bb30957":"markdown","ee0fadda":"markdown","4942dd72":"markdown","53e23ad8":"markdown","56f218a8":"markdown","1134a1ed":"markdown","cf7776c7":"markdown","1e85d581":"markdown","04c87401":"markdown","adc21560":"markdown","fec413ed":"markdown","f6bdf096":"markdown","d7791568":"markdown","4f2edea3":"markdown"},"source":{"954c1914":"import pandas as pd\nimport numpy as np\nimport json\nfrom pandas.io.json import json_normalize\n\n\nwith open(\"..\/input\/data.dat\") as json_file:\n    json_data = json.load(json_file)\n    houses=pd.DataFrame(json_data)","076fb86c":"#The loaded JSON data has been saved in a Python dictionary. \n#Using json_normalize, flattening the \"data\" dictionary into a table and saving it in a DataFrame \"df\".\ndf=json_normalize(json_data['houses'])\nprint(df.head(5))","76d7389f":"df['date'].value_counts()","84c561eb":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n#plotting bar graph to audit \"date\"\ndf['date'].value_counts().plot(kind=\"line\",figsize=(15,5))\nplt.show()\n\n# there is a different date formate for \"23052014T000000\" replacing it with \"20140523T000000\"\ndf['date'].replace(['23052014T000000'],['20140523T000000'],inplace=True) \n\n# there is a date value with 20140631T000000, where as June month does not contain 31st and hence considering it has Irregularities. changing date 20140631T000000 to 20140701T000000  \ndf['date'].replace(['20140631T000000'],['20140701T000000'],inplace=True)\n","e21154d3":"#'address' is split according to \",\" and stored into 'address_list'\ndf['address_list'] = df['address'].str.split(', ')\n#from 'address_list', col[0] is asigned to street,col[1] is asigned to city\n#col[2] is asigned to statezip and col[3] is asigned to country\ndf['street'] = df['address_list'].apply(lambda col: col[0])\ndf['city']=df['address_list'].apply(lambda col: col[1])\ndf['statezip']=df['address_list'].apply(lambda col: col[2])\ndf['country']=df['address_list'].apply(lambda col: col[3])\ndf.head(3)","34c77985":"#Dropping 'address' column and also dummy list created as 'address_list'\ndf.drop('address',axis=1, inplace=True)\ndf.drop('address_list', axis=1, inplace = True)\ndf.head(3)","cb031648":"#\"room\" attribute values are extracted into 'bathrooms' and 'bedrooms',and \"room\" attribute is dropped once its values are extracted\ndf['bathrooms'] = df.rooms.str.extract('Number of bathrooms: (\\d.\\d+)', expand = True)\ndf['bedrooms'] = df.rooms.str.extract('Number of bedrooms: (\\d+)', expand = True)\ndf.drop('rooms', axis=1, inplace = True)\ndf.head(3)\n","41d379f8":"#splitting the values of 'area.sqft_living\/sqft_lot' according to \"=\"\ndf['area.sqft_living\/sqft_lot_list'] = df['area.sqft_living\/sqft_lot'].str.split('=')\n# col[1] has the values of sqft_living and sqft_lot, hence storing it in 'area.sqft_living\/sqft_lot_list_list1'\ndf['area.sqft_living\/sqft_lot_list_list1'] = df['area.sqft_living\/sqft_lot_list'].apply(lambda col: col[1])\ndf['area.sqft_living\/sqft_lot_list_list2'] = df['area.sqft_living\/sqft_lot_list_list1'].str.split('\\ ')\ndf['sqft_living']=df['area.sqft_living\/sqft_lot_list_list2'].apply(lambda col: col[0])\ndf['sqft_lot']=df['area.sqft_living\/sqft_lot_list_list2'].apply(lambda col: col[1])\n#dropping all dummy list used to store while splitting values of 'area.sqft_living\/sqft_lot'\ndf.drop('area.sqft_living\/sqft_lot_list_list1',axis=1, inplace=True)\ndf.drop('area.sqft_living\/sqft_lot_list_list2', axis=1, inplace = True)\ndf.drop('area.sqft_living\/sqft_lot_list', axis=1, inplace = True)\ndf.drop('area.sqft_living\/sqft_lot', axis=1, inplace = True)\n\n#renaming the columns from \"area.sqft_above\" to \"sqft_above\" and \"area.sqft_basement\" to \"sqft_basement\"\ndf.rename(index=str, columns={\"area.sqft_above\": \"sqft_above\", \"area.sqft_basement\": \"sqft_basement\"},inplace=True)\ndf.head(3)","4c8fe286":"#right stripping the value of 'sqft_living'\ndf['sqft_living'] = df['sqft_living'].map(lambda x: x.rstrip('\\\\'))\ndf.head(2)","57ea3a22":"#Trying find the Irregularities in sqft_living by applying, sqft_basement + sqft_above = sqft_living\ndf['sqft_temp'] =  df[['sqft_basement', 'sqft_above']].sum(axis=1)\n\n#comparing the temp values with 'sqft_living'\ndf[df['sqft_temp'] != df['sqft_living']].index\n#print(df.iloc[[4338]])\n\n#df.ix[4338, 'sqft_living'] = df.ix[4338,['sqft_basement', 'sqft_above']].sum()\n\n#print(df.iloc[[4338]])\n#print(df.iloc[[4339]])\n\n#df.ix[4339, 'sqft_living'] = df.ix[4339,['sqft_basement', 'sqft_above']].sum()\n\n#print(df.iloc[[4339]])","1a2943ac":"#changing the datetime format.\ndf['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%dT%H:%m:%s')","d1a6b032":"#Changing the data types\ndf[['bedrooms', 'bathrooms']] = df[['bedrooms', 'bathrooms']].astype(float)\ndf[['sqft_lot','sqft_living']] = df[['sqft_lot','sqft_living']].astype(np.int64)\ndf['price'] = df['price'].apply(np.int64)\ndf.info()","620a7b92":"#rearranging the dataframe according to the requirement\ndf = df[['date', 'price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', \n         'condition', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'street', 'city', 'statezip', \n         'country']]","cd9cb9ff":"df.head()","e2e71968":" # 'O' for Objects\n#df.describe(include=['O'])\ndf.describe(include=\"all\")","1d7a2167":"df['city'].value_counts()","093fa4d2":"#City\n# there is a Lexical error in city \"sammamish\" which is replaced with the average value of city named \"Sammamish\"\ndf['city'].replace(['sammamish'],['Sammamish'],inplace=True) \n\n# there is a Lexical error in city \"Samamish\" which is replaced with the average value of city named \"Sammamish\"\ndf['city'].replace(['Samamish'],['Sammamish'],inplace=True) \n\n# there is a Lexical error in city \"Seaattle\" which is replaced with the average value of city named \"Seattle\"\ndf['city'].replace(['Seaattle'],['Seattle'],inplace=True) \n\n# there is a Lexical error in city \"Seatle\" which is replaced with the average value of city named \"Seattle\"\ndf['city'].replace(['Seatle'],['Seattle'],inplace=True) \n \n# there is a Lexical error in city \"seattle\" which is replaced with the average value of city named \"Seattle\"\ndf['city'].replace(['seattle'],['Seattle'],inplace=True)\n\n# there is a Lexical error in city \"Issaguah\" which is replaced with the average value of city named \"Issaquah\"\ndf['city'].replace(['Issaguah'],['Issaquah'],inplace=True)\n\n# there is a Lexical error in city \"Woodenville\" which is replaced with the average value of city named \"Woodinville\"\ndf['city'].replace(['Woodenville'],['Woodinville'],inplace=True)\n \n# there is a Lexical error in city \"redmond\" which is replaced with the average value of city named \"Redmond\"\ndf['city'].replace(['redmond'],['Redmond'],inplace=True)\n\n# there is a Lexical error in city \"Redmund\" which is replaced with the average value of city named \"Redmond\"\ndf['city'].replace(['Redmund'],['Redmond'],inplace=True)\n\n# there is a Lexical error in city \"Redmund\" which is replaced with the average value of city named \"Redmond\"\ndf['city'].replace(['Redmonde'],['Redmond'],inplace=True)\n\n# there is a Lexical error in city \"auburn\" which is replaced with the average value of city named \"Auburn\"\ndf['city'].replace(['auburn'],['Auburn'],inplace=True)\n\n# there is a Lexical error in city \"Auburnt\" which is replaced with the average value of city named \"Auburn\"\ndf['city'].replace(['Auburnt'],['Auburn'],inplace=True)\n\n# there is a Lexical error in city \"Sureline\" which is replaced with the average value of city named \"Shoreline \"\ndf['city'].replace(['Sureline'],['Shoreline'],inplace=True)\n\n# there is a Lexical error in city \"Bellvue\" which is replaced with the average value of city named \"Bellevue \"\ndf['city'].replace(['Bellvue'],['Bellevue'],inplace=True)\n\n# there is a Lexical error in city \"Belleview\" which is replaced with the average value of city named \"Bellevue \"\ndf['city'].replace(['Belleview'],['Bellevue'],inplace=True)\n\n# there is a Lexical error in city \"Snogualmie\" which is replaced with the average value of city named \"Snoqualmie\"\ndf['city'].replace(['Snogualmie'],['Snoqualmie'],inplace=True)\n\n# there is a Lexical error in city \"Coronation\" which is replaced with the average value of city named \"Carnation\"\ndf['city'].replace(['Coronation'],['Carnation'],inplace=True)\n\n# there is a Lexical error in city \"Kirklund\" which is replaced with the average value of city named \"Kirkland\"\ndf['city'].replace(['Kirklund'],['Kirkland'],inplace=True)\n\n#The above changes can aslo be done as show in below code,\n#df.city.replace({\"sammamish\":\"Sammamish\", \"Samamish\": \"Sammamish\", \"Seaattle\":\"Seattle\", \"Seatle\":\"Seattle\",\n#\"seattle\":\"Seattle\", \"Issaguah\":\"Issaquah\"}, inplace=True) \ndf.city.value_counts()","302628e3":"df['bathrooms'].value_counts()","da2cd51a":"#Bathroom\n# there is a abnormal data value in \"bathrooms\", \"1.70\" which is replaced with the average value\"1.75\"\ndf['bathrooms'].replace([1.70],[1.75],inplace=True) \n\n# there is a lexical error in \"bathrooms\", \"1.05\" which is replaced with the value \"1.50\"\ndf['bathrooms'].replace([1.05],[1.50],inplace=True) \n\n# there is a abnormal data value in \"bathrooms\", \"2.55\" which is replaced with the nearest value of bathrooms named \"2.50\"\ndf['bathrooms'].replace([2.55],[2.50],inplace=True) \n\n# there is a abnormal data value in \"bathrooms\", \"2.30\" which is replaced with the average value of bathrooms named \"2.25\"\ndf['bathrooms'].replace([2.30],[2.25],inplace=True) \n\n# there is a lexical error in \"bathrooms\", \"2.57\" which is replaced with the average value of bathrooms named \"2.75\"\ndf['bathrooms'].replace([2.57],[2.75],inplace=True) \n\ndf['bathrooms'].value_counts()","e6e413ce":"df[df.duplicated(keep=False)]","eb4e2aea":"#dropping the row which are duplictaes\ndf.drop_duplicates(keep=\"first\", inplace=True)","14af9f60":"df.info()","849b1a9c":"df.isnull().sum()","65c1131d":"# unique values of 'yr_renovated'\ndf['yr_renovated'].unique()","7bedbde8":"#creating dummy dataframe to use it for predicting the null values for yr_renovated by using mean\ndf_impute= df.copy()\ndf_impute.head(5)\n","5407c0a6":"#converting the yr_renovated column, from float64 to int64 and replacing all NaN to '0'\ndf_impute['yr_renovated'] = np.nan_to_num(df_impute['yr_renovated']).astype(np.int64)","112901eb":"#For linear regression we use sklearn (built in python library) and import linear regression from it.\nfrom sklearn.linear_model import LinearRegression\n#Initializing Linear Regression to a variable reg\nreg = LinearRegression()\n#we know that 'yr_renovated' are to be predicted , hence we set labels (output) as 'yr_renovated' column\nlabels = df_impute['yr_renovated']\n#Converting dates to 1\u2019s and 0\u2019s so that it doesn\u2019t influence our data much\n#We use 0 for houses which are new that is built after 2014.\nconv_dates = [1 if values == 2014 else 0 for values in df_impute.date]\ndf_impute['date'] = conv_dates\ntrain1 = df_impute.drop(['city','street','country','statezip','price'],axis=1)","1980949e":"#We again import another dependency to split our data into train and test\nfrom sklearn.cross_validation import train_test_split","2894689a":"#train data is set to 90% and 10% of the data to be my test data , and randomized the splitting of data by using random_state.\nX_train, X_test, y_train, y_test = train_test_split(train1,labels,test_size = 0.10, random_state=2)","79203ac6":"reg.fit(X_train,y_train)","493b53e9":"reg.score(X_test,y_test)","e0035d0f":"df[\"yr_renovated\"].fillna(df.groupby([\"yr_built\",\"condition\"])[\"yr_renovated\"].transform(\"mean\"), inplace=True)\ndf.yr_renovated.describe()","b343bcd7":"df.isnull().sum()","fa8e561f":"df['yr_renovated'].fillna(0, inplace=True)","f7c22564":"df_impute = df.copy()","464aab63":"#we know that 'yr_renovated' are to be predicted , hence we set labels (output) as 'yr_renovated' column\nlabels = df_impute['yr_renovated']\n#Converting dates to 1\u2019s and 0\u2019s so that it doesn\u2019t influence our data much\n#We use 0 for houses which are new that is built after 2014.\nconv_dates = [1 if values == 2014 else 0 for values in df_impute.date]\ndf_impute['date'] = conv_dates\ntrain1 = df_impute.drop(['city','street','country','statezip','price'],axis=1)","9c3cef2c":"reg.fit(X_train,y_train)","56545294":"reg.score(X_test,y_test)","dc8c6447":"df_imptd1 = df.copy()","37b13168":"#we know that 'yr_renovated' are to be predicted , hence we set labels (output) as 'yr_renovated' column\nlabels = df_imptd1['yr_renovated']\n#Converting dates to 1\u2019s and 0\u2019s so that it doesn\u2019t influence our data much\n#We use 0 for houses which are new that is built after 2014.\nconv_dates = [1 if values == 2014 else 0 for values in df_imptd1.date]\ndf_imptd1['date'] = conv_dates\ntrain1 = df_imptd1.drop(['city','street','country','statezip','price'],axis=1)","865156a1":"#train data is set to 90% and 10% of the data to be my test data , and randomized the splitting of data by using random_state.\nX_train, X_test, y_train, y_test = train_test_split(train1,labels,test_size = 0.10, random_state=2)","e163fbc3":"reg.fit(X_train,y_train)","d55bda69":"reg.score(X_test,y_test)","0726c0d1":"df['yr_renovated'] = df_impute['yr_renovated'].astype(np.int64)","38b3723b":"df.price.value_counts()","e703461d":"#checking how many values are < 1\ndf_impute[df_impute['price'] < 1] ","1f10c1ad":"df[[\"price\",\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\"sqft_above\",\"yr_built\",\"sqft_living\",\"sqft_lot\"]].describe()","b677938c":"#replacing all the 0.0 to NaN\ndf['price'] = df['price'].replace(0.0, np.nan)","d4a1dc26":"#replacing all NaN to mean values of price \ndf[\"price\"].fillna(df_impute.groupby([\"bedrooms\",\"bathrooms\",\"city\",\"statezip\"])[\"price\"].transform(\"mean\"), inplace=True)\ndf.price.describe()","257c9812":"df.isnull().sum()","6a7b84f3":"#dropping all NaN values from price\ndf.dropna(subset=['price'],axis=0,inplace=True)","d9e22cbe":"df_final = df.copy()","6ebc828e":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndf.boxplot(figsize=(20,10))","1ead4cfd":"bp = df.boxplot(column='price',figsize=(10,15))","58e45de2":"# We can see a bunch of price above 0.5, then something around 2.5, the outliers are:\ndf[df['price'] > 2.0] ","76f204ed":"# plotting baoxplot to check outliers price vs bedrooms\nbp = df.boxplot(column='price', by = 'bedrooms',figsize=(15,10))","9741ac32":"df[df['price'] > 2.0] ","c8774992":"#creating a dummy dataframe to check on price vs date of property sold.\ndf_dummy = df\n#df_dummy['date'] = pd.to_datetime(df_dummy['date'])","2c2097bb":"# plot price vs date\n\n#df_dummy['Year'] = df_dummy['date'].dt.year\n#df_dummy.boxplot(column='price', by='Year', figsize=(10,10))\n\n#df.set_index(['date',df.date.dt.year])['price'].unstack().boxplot()","118a48e2":"df[(df['price'] > 0.5) & (df['price'] > 1.0) & (df['price'] > 2.5)].describe()","cb554633":"# sqft_lot\ndf.boxplot(column='sqft_lot', figsize=(10,10))","02655ca0":"df[(df['sqft_lot'] > 400000) & (df['sqft_lot'] > 600000)] ","5deafad9":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\nimport seaborn as sns\nfrom matplotlib import rcParams\n\n%matplotlib inline \n%pylab inline ","6b56329f":"sns.pairplot(data=df, x_vars=['bathrooms','bedrooms','sqft_living','sqft_lot','sqft_above','waterfront'], y_vars=[\"price\"])","d5c60322":"df.describe()","23c782ca":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols","e60631a0":"m = ols('price ~ sqft_living',df).fit()\nprint (m.summary())","51f95f35":"m = ols('price ~ sqft_living + bedrooms + view + condition',df).fit()\nprint (m.summary())","32f37e3e":"sns.jointplot('sqft_living','price', data=df, size=10, alpha=.5)","0424312c":"sns.jointplot('sqft_living','price', data=df, xlim=(500,3500), ylim=(100000,1000000), size=10, alpha=.5)","85d89a3c":"df[\"statezip\"].nunique()\n","98b1bfd3":"df['statezip'].value_counts()","2c189c71":"df.groupby('statezip')['price'].mean()","290f57f4":"zip_WA_98103 = df['statezip'] == \"WA 98103\"  # True if zip is 98103","17925531":"sns.jointplot('sqft_living','price', data=df[zip_WA_98103], size=10, alpha=.5)","f9d1b71d":"df.describe()","1ca536e8":"import seaborn as sns\nimport mpl_toolkits\n\n#\ndf['bedrooms'].value_counts().plot(kind='bar')\nplt.title('Number of Bedrooms')\nplt.xlabel('Bedrooms')\nplt.ylabel('Count')\nsns.despine\n","bb2e9f2f":"#PRIcE Vs SQFT_LIVING\nplt.scatter(df.price,df.sqft_living)\nplt.title(\"Price Vs Square feet\")\nplt.xlabel('Square feet')\nplt.ylabel('Price')\nplt.show()","eff32088":"#PRICE Vs BEDROOMS\nplt.scatter(df.bedrooms,df.price)\nplt.title(\"Bedroom and Price\")\nplt.xlabel(\"Bedrooms\")\nplt.ylabel(\"Price\")\nplt.show()\nsns.despine()","1b7b1f3b":"#Total sqft including basement vs price and waterfront vs price\nplt.scatter((df['sqft_living']+df['sqft_basement']),df['price'])\nplt.title(\"sqft_living and sqft_basement vs Price\")\nplt.xlabel(\"sqft_living and sqft_basement\")\nplt.ylabel(\"Price\")\nplt.show()","77b9282c":"plt.scatter(df.waterfront,df.price)\nplt.title(\"Waterfront Vs Price (0 = No Waterfront )\")\nplt.xlabel(\"waterfront\")\nplt.ylabel(\"Price\")\nplt.show()","f6524673":"plt.scatter(df.floors,df.price)\nplt.title(\"floors Vs Price\")\nplt.xlabel(\"floors\")\nplt.ylabel(\"Price\")\nplt.show()","191499ec":"plt.scatter(df.condition,df.price)\nplt.title(\"condition Vs Price\")\nplt.xlabel(\"condition\")\nplt.ylabel(\"Price\")\nplt.show()","0dadecf7":"#For linear regression we use sklearn (built in python library) and import linear regression from it.\nfrom sklearn.linear_model import LinearRegression\n#Initializing Linear Regression to a variable reg\nreg = LinearRegression()\n#we know that 'yr_renovated' are to be predicted , hence we set labels (output) as 'yr_renovated' column\nlabels = df['price']\n#Converting dates to 1\u2019s and 0\u2019s so that it doesn\u2019t influence our data much\n#We use 0 for houses which are new that is built after 2014.\nconv_dates = [1 if values == 2014 else 0 for values in df.date]\ndf['date'] = conv_dates\ntrain1 = df.drop(['city','street','country','statezip','price'],axis=1)\n","8626be65":"#We again import another dependency to split our data into train and test\nfrom sklearn.cross_validation import train_test_split","9df49737":"#train data is set to 90% and 10% of the data to be my test data , and randomized the splitting of data by using random_state.\nX_train, X_test, y_train, y_test = train_test_split(train1,labels,test_size = 0.10, random_state=2)","f8c8c5e5":"map(pd.np.shape,[X_train, X_test, y_train, y_test])","643336d7":"reg.fit(X_train,y_train)","31b71b32":"reg.score(X_test,y_test)\n","ce63f508":"df_final.info()","9ebf8869":"df_final.head()","777a7144":"filename = 'output.csv'\ndf_final.to_csv(filename, encoding='utf-8', index=False)","5b7271a4":"### Examining and loading the data into a Pandas DataFrame","1d151731":"From the above data, we see that there are 4371 null values in \"yr_renovated\"","32ec2944":"##### There are two ways that we are trying to replace the null values of \"yr_renovated\"\n* Replacing all NaN with zero \n* Replacing NaN with mean","df55bcf9":"price of 26590000.0 and 12899000.0 looks very high compared to other, that's a weird value too and price of 7800.0 and 84350.0 \nlooks very low compared to other values.\n* Price also depends on the other conditions of the house\n* Lets check the price according to \"price\",\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\"sqft_above\",\"yr_built\",\"sqft_living\",\"sqft_lot\"\n\n###### Let's investigate outliers by each attribute ","52c0d283":"From the above data, when we observe the unique values, Some of the city names were Lexical errors.","c8125716":"We have train data , test data and labels for both. Fitting our train and test data into linear regression model.","cb913b21":"##### Predicting the yr_renovated without any changes with the full data","94fffd51":"As we can see from all the above representation that many factors are affecting the prices of the house , like square feet which increases the price of the house and even location influencing the prices of the house.\nCreating a model to which would predict the price of the house based upon the other factors such as square feet , water front etc .","1739d3aa":"It\u2019s important to look at the shape of the data \u2013 and to double check if the data is reasonable. Corrupted data is not uncommon,so will run two checks\n* first, use df.describe() to look at all the variables in our analysis. \n* Second, plot histograms of the variables that the analysis is targeting using plt.pyplot.hist().","5f5ba86e":"Something worth considering is that different neighborhoods can vary greatly in average house price. Some nice neighborhoods are very expensive and some other (also nice!) neighborhoods can be quite affordable. It might be good to look at average house price by zipcode since we have that in our dataset.","0f6fab70":"There are 4601 x 18 records and there are many missing values in \"yr_renovated\"","2274bc4e":"##### Predicting the yr_renovated when NaN is changed with mean values with the full data","811df850":"Lets look at \"continuous\" features. Intuitively that will be sqft_living but could possibly be 'bathrooms','bedrooms','sqft_living','sqft_lot','sqft_above','waterfront'. Lets take a look at these with some plots using seaborn.","c676ef0c":"From the above figure we can see that more the living area , more the price though data is concentrated towards a particular price zone , but from the figure we can see that the data points seem to be in linear direction.We can also see some irregularities that the house with the highest square feet was sold for very less , maybe there is another factor or probably the data must be wrong. ","467e6c75":"We can see a bunch of sqft_lot above 1074218, then something around 600000, look at the outliers:","487f2405":"## Multivariate linear regression","e1267347":"##### checking how many values are < 1","d8e556ae":"###### We are trying to replace the zero values of \"price\"\nReplacing zero with mean","6776b5bf":"'nan' doesn't occur in counts() \n\n*  checking how many 'NaN' values are there\n*  checking how many 0\n*  checking how many < 1","c5494d75":"### Investigating Duplicates","bfa344d1":"##### Splitting the \"address\" values to => \"street\",\"city\", \"statezip\", \"country\"","87e8156f":"In our multivariate regression output above, we learn that by using additional independent variables, such as the number of bedrooms, we can provide a model that fits the data better, as the R-squared for this regression has increased to 0.222. This means that we went from being able to explain about 0.202(20.2%) of the variation in the model to 0.222(22.2%) with the addition of a few more independent variables. ","492c37c9":"We can see more factors affecting the price","79fe70db":"The first thing to notice is that 'price' has many outliers.\nHowever, plotting all data together might not be right because of the different ranges of attributes. Therefore, we look at one attribute at a time instead.","2d0976a9":"From the above graph, we can summarize that for a standard three bedroom house, the number of outliers imply that the owners paid a premium price for their respective property, which would have either been at a waterfront or the properties' per sqft_living value would have been higher than standard rates. Figuratively, the sandard rates are below 0.5. Under close observation, above the value of 1.0, there are few properties which may have higher sqft_living value, the values above 2.5 might be of those properties which may be near water front \n\nTo verify the above properties whose sqft_living or their location is at a waterfront, we further plot refined graph on waterfront, sqft_living, bedrooms, bathrooms, view, condition. ","52665e49":"It looks like there are 77 different statezip code in the given data. Lets see how many house sales there were in each.","e5ff60b4":"Trying to find the average house sale price in each zipcode ...","057d8b78":"Having the regression summary output is important for checking the accuracy of the regression model and data to be used for estimation and prediction \u2013 but visualizing the regression is an important step to take to communicate the results of the regression in a more digestible format.\n\nThis section will rely entirely on Seaborn (sns) , which has an incredibly simple and intuitive function for graphing regression lines with scatterplots. I chose to create a jointplot for square footage and price that shows the regression line as well as distribution plots for each variable.","0df591bc":"The increase of price with sqft_living space is pretty clear and the \"Pearson r value\" is 0.45 indicating a reasonable correlation. However, the data distributions show a big concentration of values in the lower left of the plot. That makes sense, most houses are between 300 and 3000 sqft and a few hundred thousand dollars. We can eliminate the very expensive and very large houses and take another look at the data.\n\nIf we set the size (xlim) from 500 to 3500sqft and the price (ylim) from 100,000 to $1,000,000 the data still shows the trend but it looks very scattered.","2f8e780f":"We can see that \"lot\" size is not well correlated to price but the data for living space is reasonable. Visually the best feature to use looks like sqft_living as we expected.","3745e02c":"###### checking how many 0 in price","7d1f02bd":"##### References:\n* https:\/\/stackoverflow.com\/\n* https:\/\/www.coursera.org\/learn\/ml-regression\/lecture\/G12Qp\/a-case-study-in-predicting-house-prices\n* https:\/\/medium.com\/towards-data-science\/create-a-model-to-predict-house-prices-using-python-d34fe8fad88f\n* https:\/\/www.datacamp.com\/courses\/cleaning-data-in-python\n* http:\/\/www.developintelligence.com\/blog\/2017\/08\/data-cleaning-pandas-python\/\n* http:\/\/uwescience.github.io\/DSSG2015-predicting-permanent-housing\/images\/DSSG2015-PPH-final-presentation.pdf\n","3e38d022":"# Parsing the property sales data stored in \u201cdata.dat\u201d\nThe real estate markets, like those in Sydney and Melbourne, present an interesting opportunity for data analysts to analyze and predict where property prices are moving towards.  Prediction of property prices is becoming increasingly important and beneficial. Property prices are a good indicator of both the overall market condition and the economic health of a country. Considering the data provided, we are wrangling a large set of property sales records stored in an unknown format and with unknown data quality issues.","80f360de":"From the above data, when we observe the unique values, Some of the \"bathrooms\" values were Irregular.","39fde902":"price of minimum 7.800000e+03 with sqft_living 370.000000, zero bathroom and zero bedroom with condition as 1 which is really low comparing to other property's conditions, 2.659000e+07 with sqft_living 13540.000000 looks very high compared to other, that's a weird value too and maximum price of 2.659000e+07 and 7.800000e+03 looks very low compared to other values.\nPrice also depends on the other conditions of the house","a6b92c7f":"# Working with Regression Model\n\nLooking at the bedroom columns , the dataset has a house where the house has 9 bedrooms , seems to be a massive house and would be interesting to know more about it as we progress.\n\nMaximum square feet is 13,540 where as the minimum is 370. we can see that the data is distributed.\nSimilarly , we can infer so many things by just looking at the describe function.\n\nNow , we are going to see some visualization and also going to see how and what can we infer from visualization.\n\nLet\u2019s see which is most common bedroom number. Let\u2019s look at this problem from a builder\u2019s perspective, sometimes it\u2019s important for a builder to see which is the highest selling house type which enables the builder to make house based on that. For example, in India , for a good locality a builder opts to make houses which are more than 3 bedrooms which attracts the higher middle class and upper class section of the society.\nLet\u2019s see how this pans out for this data!\n","80fa59b2":"We can see the home prices vary from $0 to $2.659000e+07 with living space from 370.000000sqft to 13540sqft. Lots of variety!","4d4675bb":"Using the \"selectors\" above we can look at plots of price vs sqft_living in that zipcode.","c8483c8f":"As we can see from the visualization 3 bedroom houses are most commonly sold followed by 4 bedroom. So how is it useful ? For a builder having this data , He can make a new building with more 3 and 4 bedroom\u2019s to attract more buyers.\nSo now we know that 3 and 4 bedroom\u2019s are highest selling. But at which locality ?\n#### How common factors are affecting the price of the houses ?\nWe saw the common locations and now we\u2019re going to see few common factors affecting the prices of the house and if so ? then by how much ?\nLet us start with , If price is getting affecting by living area of the house or not ?","08a25ab4":"From the above value counts of \"date\", we found that there are two dates with different date format when compared to other date formats.","67c1b2ea":"After fitting our data to the model we can check the score of our data ie , prediction. in this case the prediction is 58%","e349835a":"###  Investigating Lexical errors for all the columes in dataframe","3bb30957":"When we produce a linear regression summary with OLS with only two variables this will be the formula that we use:\n\nReg = ols(\u2018Dependent variable ~ independent variable(s), dataframe).fit()\n\nprint(Reg.summary())\n\nWhen we look at housing prices and square footage for houses, we print out the following summary report:","ee0fadda":"The zipcode that look the most interesting to me is WA 98103. WA 98103 has the most house sale values, 148, with an average sale price of $5.603248e+05. \n","4942dd72":"From the above observation, we see that there are 248 count of \"0.0\" in price\n","53e23ad8":"We can see a bunch of price above 0.5, then something around 1.0 and then something around 2.5","56f218a8":"### Investigating the missing values ","1134a1ed":"We can see a bunch of sqft_lot above 400000, then something around 600000, look at the outliers:","cf7776c7":"### Using boxpot to detect outliers","1e85d581":"The 98103 zipcode has a distribution that looks similar to the complete dataset.","04c87401":"When we print the summary of the OLS regression, all relevant information can be easily found, including R-squared, t-statistics, standard error, and the coefficients of correlation. Looking at the output, it\u2019s clear that there is an extremely significant relationship between square footage and housing prices since there is an extremely high t-value of 144.920, and a P>|t| of 0%\u2013which essentially means that this relationship has a near-zero chance of being due to statistical variation or chance.\n\nThis relationship also has a decent magnitude \u2013 for every additional 100 square-feet a house ","adc21560":"Quick takeaways: We are working with a dataset that contains 4600 observations, mean price is approximately $5.648432e+05,median price is approximately $4.710000e+05, and the average house\u2019s area is 2139.346957 ft2","fec413ed":"From the above data, we found that there is only one row which is repeating twice.","f6bdf096":"# Auditing and cleansing the loaded data\nIn this task, we are inspecting and auditind the data to identify the data problems, and then fix the problems. Different generic and major data problems could be found in the data might include:\n\n* Lexical errors, e.g., typos and spelling mistakes\n* Irregularities, e.g., abnormal data values and data formats\n* Violations of the Integrity constraint.\n* Outliers\n* Duplications\n* Missing values\n* Inconsistency, e.g., inhomogeneity in values and types in representing the same data","d7791568":"##### Predicting the yr_renovated when changed from NaN to zero, with the full data","4f2edea3":"##### Floors vs Price and condition vs Price"}}