{"cell_type":{"c38a019b":"code","6468f59a":"code","029198a2":"code","c38c1f56":"code","7eb604df":"code","87f29bf0":"code","f5866a1d":"code","36bcddb5":"code","dfaf77fe":"code","838fa06f":"code","c753e8e6":"code","9ed4c018":"code","856d5819":"code","54713928":"markdown","e5cc7547":"markdown","c9d83e4f":"markdown","89db9802":"markdown","909fef3e":"markdown","d5a93773":"markdown","1ccee2c4":"markdown","d8f22d85":"markdown","0cab42bc":"markdown","ec773b4a":"markdown"},"source":{"c38a019b":"'''\nTODO\n- Get the data (https:\/\/www.kaggle.com\/madhavmalhotra\/heart-disease-prediction-simplified)\n- Sigmoid function\n- Feedforward function: weighted input, activation\n- Cost function: round activations to prediction, Mean Squared Error\n- Compute gradient function: dpred, dsigmoid, dz\n- Gradient descent function: Take theta and subtract alpha * gradient 100 times\n- Graph cost over time\n'''","6468f59a":"import numpy as np","029198a2":"X = np.load(\"..\/input\/heart-disease-prediction-simplified\/X.npy\")\ny = np.load(\"..\/input\/heart-disease-prediction-simplified\/y.npy\")","c38c1f56":"print(X.shape)\nprint(y.shape)","7eb604df":"print(\"male  age  has_high_BP  systolic_BP  diastolic_BP\")\nprint(X)","87f29bf0":"print(y)","f5866a1d":"def sigmoid(x):\n    return 1 \/ ( 1 + np.exp(-x) )\n\ndef sigmoid_prime(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\nsigmoid(10)","36bcddb5":"def feedforward(X,theta):\n    z = np.dot(X, theta)       #Weighted input\n    a = sigmoid(z)             #Activation\n    pred = np.round(a)         #Round to either 0 or 1\n    return pred\n    \nX_temp = X[0]                #Select the first row\nnum_features = X_temp.shape[-1] #Num columns = num features\nprint(\"The number of input features is: \" + str(num_features))\n\ntheta = np.random.randn(num_features)\nprint(theta)\n\npred = feedforward(X_temp, theta)\nprint(pred)","dfaf77fe":"def cost_function(X,y,theta):\n    pred = feedforward(X,theta)\n    m = y.shape[0]\n    \n    diff_squared = (pred - y) ** 2\n    avg_per_example = np.sum(diff_squared) \/ m\n    return avg_per_example\n    \ncost = cost_function(X,y,theta)\nprint(cost)","838fa06f":"def gradient_icky(pred,X,y):\n    m = y.shape[0]                                          #i goes from 0 up to but not including m\n    n = X.shape[-1]                                         #j goes from 0 up to but not including n\n    \n    cost_by_pred = 2 * (pred - y)                           #This returns an m x 1 vector\n    sigmoid_out_by_sigmoid_in = sigmoid_prime(X)            #This returns a m x n vector\n    \n    gradient = np.zeros(n)                                  #Create an array of zeros as placeholders\n    for j in range(0,n):                                    #go through all the input features\n        summation_result = 0\n        for i in range(0,m):                                #go through all the examples\n            current_example = cost_by_pred[i] * sigmoid_out_by_sigmoid_in[i,j] * X[i,j]\n            summation_result = summation_result + current_example\n        \n        avg = summation_result \/ m\n        gradient[j] = avg\n    \n    return gradient\n    \ngradient = gradient_icky(pred,X,y)\nprint(\"Note the output is a n x 1 vector:\", gradient)","c753e8e6":"def gradient_vectorised(pred,X,y):\n    m = y.shape[0]\n    \n    cost_by_pred = 2 * (pred - y)                           #This returns an m x 1 vector\n    sigmoid_out_by_sigmoid_in = sigmoid_prime(X)            #This returns a m x n vector\n    X_sum = np.sum(X, axis=0)                               #X was m x n. Now it's 1 x n\n    #print(f\"X: {X.shape}. X_sum: {X_sum.shape}\")\n    \n    gradient = np.dot(cost_by_pred, sigmoid_out_by_sigmoid_in) * X_sum\n    return gradient \/ m\n    \ngradient_vectorised(pred,X,y)","9ed4c018":"def gradient_descent(X,y,theta,learning_rate,num_iters):\n    cost_history = []\n    for i in range(num_iters):\n        print(\".\", end=\"\")\n        pred = feedforward(X,theta)\n        cost = cost_function(X,y,theta)                   #This is just to monitor cost over iterations\n        cost_history.append(cost)\n        \n        gradient = gradient_vectorised(pred,X,y)          #This is the gradient descent part\n        theta = theta - learning_rate * gradient\n    \n    return theta, cost_history\n    \n        \nlearning_rate = 0.1\nnum_iters = 100\ntheta = np.random.randn(5)\ntheta, cost_history = gradient_descent(X,y,theta,learning_rate,num_iters)\nprint(cost_history)","856d5819":"import matplotlib\n\nx = range(num_iters)\ny = cost_history\nmatplotlib.pyplot.plot(x, y)","54713928":"![clue 1](https:\/\/i.imgur.com\/7tao8P3.png)\n![clue 2](https:\/\/i.imgur.com\/dfFK9nZ.png)","e5cc7547":"![cost equation](https:\/\/i.imgur.com\/JYtFeIe.png)","c9d83e4f":"## Step 4: Write Gradient Descent and Gradient Computation function","89db9802":"This notebook is accompanied by a video tutorial. If you'd like to see me explaining this code, see [here](https:\/\/youtu.be\/gWmtODw0J_Q) :-)\n\n---","909fef3e":"![feedforward equations](https:\/\/i.imgur.com\/vTDYEyR.png)","d5a93773":"![Gradient descent equation](https:\/\/i.imgur.com\/r3418Te.png)\n![Gradient computation](https:\/\/i.imgur.com\/Pe8YH01.png)","1ccee2c4":"## Step 3: Write Feedforward\/Cost functions\nFeedforward: Input to output","d8f22d85":"## Step 1: Get Data","0cab42bc":"## Step 2: Write Helper Functions","ec773b4a":"![sigmoid derivative](https:\/\/hausetutorials.netlify.app\/posts\/2019-12-01-neural-networks-deriving-the-sigmoid-derivative\/sigmoid.jpg)"}}