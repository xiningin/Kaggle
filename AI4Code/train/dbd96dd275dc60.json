{"cell_type":{"868a0a52":"code","9b4e4f4a":"code","f5fdd6f8":"code","6e34ae6e":"code","b87492c6":"code","3f260ebc":"code","3f896038":"code","83e8c3a0":"code","b5b3f927":"code","05d7fb70":"code","289b7781":"code","9fc38dff":"code","862b1964":"code","1e205f69":"code","0c809621":"code","80f32272":"code","def2eeda":"code","13c04d5f":"code","160320dc":"code","e55ae9a0":"code","99d11099":"code","f421e72e":"code","11ca180b":"code","732f7c7b":"code","66c5b712":"code","4df11ca9":"code","ce8e9e23":"code","247cb621":"code","a68627fa":"code","9fe818e3":"code","4361ba31":"code","b7e8506b":"code","e1973c2d":"code","87d7fef1":"code","a38f6f06":"code","cce94603":"code","2fb2ed12":"code","8469e222":"code","df1af968":"code","7b7b5c43":"code","a22bcbdd":"code","20b42a72":"code","d84b6326":"code","d6057a12":"code","574f799c":"code","aeb9f3e3":"code","3b2c56e9":"code","ae91def7":"code","05352d2a":"code","6e71fd08":"code","b381f34b":"code","5339d39d":"code","8fd40d74":"code","b6d743a4":"code","c9983d2b":"code","f10e492b":"code","9e3740c0":"code","99f10841":"code","8a98a46f":"code","2960a3a2":"code","753ec646":"code","61db5a18":"code","1a412eea":"code","8466a416":"code","6f9a4e1c":"code","610abb87":"code","c28a28d2":"code","500e59a7":"code","32662ba8":"code","e380a696":"code","c96970ac":"code","0a2dbf0f":"code","3abbae46":"markdown","d6ad2016":"markdown","8f1b6695":"markdown","85be397d":"markdown","2c984d89":"markdown","37c179eb":"markdown","7a892208":"markdown","6ad53c1d":"markdown","adf6c94e":"markdown","fab2a884":"markdown","58ce54a6":"markdown","6c597096":"markdown","d99ae321":"markdown","17d9f9d6":"markdown","18599c64":"markdown","ef254f65":"markdown","16739a43":"markdown","ca4272a2":"markdown","e85db418":"markdown","1e241c54":"markdown","6af0ab8e":"markdown","ad1a2c40":"markdown","16d2ca06":"markdown","37e787fc":"markdown","32ba6649":"markdown","53b1c58a":"markdown","957b20ea":"markdown"},"source":{"868a0a52":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor","9b4e4f4a":"# import Data (training and validation sets)\ndf = pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/TrainAndValid.csv\", low_memory=False)","f5fdd6f8":"df.info()","6e34ae6e":"df.isna().sum()","b87492c6":"fig, ax =  plt.subplots()\nax.scatter(df[\"saledate\"][:1000], df.SalePrice[:1000])","3f260ebc":"df.SalePrice.plot.hist();","3f896038":"# Import data but this time parae dates\n\ndf=pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/TrainAndValid.csv\", low_memory=False, parse_dates=[\"saledate\"] )","83e8c3a0":"df.saledate.dtype","b5b3f927":"df.saledate[:1000]","05d7fb70":"fig, ax= plt.subplots()\n\nax.scatter(df.saledate[:1000], df.SalePrice[:1000])","289b7781":"df.head()","9fc38dff":"df.head().T","862b1964":"df.saledate.head(20)","1e205f69":"df.sort_values(by=[\"saledate\"], inplace=True, ascending=True)\ndf.saledate.head(20\n                )","0c809621":"df_temp= df.copy()","80f32272":"df_temp[\"saleYear\"]= df_temp.saledate.dt.year\ndf_temp[\"saleMonth\"]= df_temp.saledate.dt.month\ndf_temp[\"saleDay\"]= df_temp.saledate.dt.day\ndf_temp[\"saleDayOfWeek\"]= df_temp.saledate.dt.dayofweek\ndf_temp[\"saleDayOfYear\"]= df_temp.saledate.dt.dayofyear","def2eeda":"df_temp.head().T","13c04d5f":"# Now we have enriched our DF with Dattime deatures, we can remove saledate\n\ndf_temp.drop(\"saledate\", axis=1, inplace=True)","160320dc":"# check values of different dataset columns\ndf_temp.state.value_counts()","e55ae9a0":"df_temp.head().T","99d11099":"pd.api.types.is_string_dtype(df_temp[\"UsageBand\"])","f421e72e":"# to find out which columns contain strings\n\nfor labels, content in df_temp.items():\n    if pd.api.types.is_string_dtype(content):\n        print(labels)","11ca180b":"# This will turn all of the s tring value into category values\n\nfor label, content in df_temp.items():\n    if pd.api.types.is_string_dtype(content):\n        df_temp[label]= content.astype(\"category\").cat.as_ordered()","732f7c7b":"df_temp.info()","66c5b712":"df_temp.state.cat.categories","4df11ca9":"df_temp.state.cat.codes","ce8e9e23":"# CHeck null percentage\ndf_temp.isnull().sum()\/len(df_temp)","247cb621":"#Export current tmp csv\n\n#df_temp.to_csv(\"data\/train_tmp.csv\", index=False)","a68627fa":"# Import reprocessed data\n\n#df_temp= pd.read_csv(\"data\/train_tmp.csv\", low_memory=False)","9fe818e3":"df_temp.isna().sum()","4361ba31":"for label, content in df_temp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        print(label)","b7e8506b":"# check for which numeric columns have null values\n\nfor label,content in df_temp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","e1973c2d":"#Fill numeric rows with median\n\nfor label,content in df_temp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            #Add a binary columns which tells us if the data was missing or not\n            df_temp[label+\"_is_missing\"]= pd.isnull(content)\n            #Fill missing numeric values with median\n            df_temp[label] = content.fillna(content.median())","87d7fef1":"for label,content in df_temp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(labels)","a38f6f06":"df_temp.head().T","cce94603":"# Lets fill all missing categorical values\nfor label, content in df_temp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        print(label)","2fb2ed12":"#Turn categorical values into numbers and fill missing\n# +1 is done to convert -1 to 0. pd.Categories fills empty vales with Code -1. As we do not want any negative data in our \n#evaluation we are going to add 1 to it so that it becomes 0\nfor label, content in df_temp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        #Add binary column to indicate wether sampple has missing values\n        df_temp[label+\"is_missing\"]= pd.isnull(content)\n        #Turn categories into number and add+1\n        df_temp[label] =pd.Categorical(content).codes +1","8469e222":"pd.Categorical(df_temp[\"state\"]).codes +1\n","df1af968":"df_temp.info()","7b7b5c43":"df_temp.head().T","a22bcbdd":"df_temp.isna().sum()[:20]","20b42a72":"%%time\n# Instantiate model \n\n#model= RandomForestRegressor(n_jobs=-1,\n#                            random_state=42)\n\n#model.fit(df_temp.drop(\"SalePrice\", axis=1), df_temp[\"SalePrice\"])","d84b6326":"#score the model\n#model.score(df_temp.drop(\"SalePrice\", axis=1), df_temp[\"SalePrice\"])","d6057a12":"## splitting data into train and validation sets\ndf_temp.saleYear.value_counts()","574f799c":"#Split data into training and validation\ndf_val=df_temp[df_temp.saleYear==2012]\ndf_train=df_temp[df_temp.saleYear!=2012]\n\nlen(df_val), len(df_train)","aeb9f3e3":"#Split data into X and y\nX_train, y_train =df_train.drop(\"SalePrice\", axis=1), df_train.SalePrice\n","3b2c56e9":"X_valid, y_valid = df_val.drop(\"SalePrice\", axis=1), df_val.SalePrice","ae91def7":"X_train.shape , y_train.shape, X_valid.shape, y_valid.shape","05352d2a":"#create an evaluation function so that we can use this functionality multiple times over different params\nfrom sklearn.metrics import mean_absolute_error, mean_squared_log_error, r2_score\n\ndef rmsle(y_test, y_preds):\n    \"\"\"\n    Calcs rmsle between ppredictions and true labels.\n    \"\"\"\n    return np.sqrt(mean_squared_log_error(y_test, y_preds))\n\n#Create function to evaluate model on a few diff levels\n\ndef show_scores(model):\n    train_preds= model.predict(X_train)\n    val_preds = model.predict(X_valid)\n    scores={\"Training MAE\": mean_absolute_error(y_train, train_preds),\n           \"Valid MAE\": mean_absolute_error(y_valid, val_preds),\n           \"Training RMSLE\": rmsle(y_train, train_preds),\n           \"Valid RMSLE\": rmsle(y_valid, val_preds),\n           \"Training R2\": r2_score(y_train, train_preds),\n           \"Valid R2\": r2_score(y_valid, val_preds)}\n    return scores","6e71fd08":"#this takes far too long for experimenting\n#%%time\n#model= RandomForestRegressor(n_jobs=-1,\n#                            random_state=42)\n\n#model.fit(X_train)","b381f34b":"model= RandomForestRegressor(n_jobs=-1, random_state=42,\n                            max_samples=10000)\n\n","5339d39d":"%%time\n#Cutting down on max_samples to see how much it imporoves training time\nmodel.fit(X_train, y_train)","8fd40d74":"%%time\nshow_scores(model)","b6d743a4":"# HyperParameters tuning with RandomizedSerarchCV","c9983d2b":"%%time\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Different Random forest regressor hyper params\nrf_grid={\"n_estimators\":np.arange(10, 100, 10),\n        \"max_depth\":[None, 3, 5, 10],\n        \"min_samples_split\":np.arange(2,10,2),\n        \"min_samples_leaf\": np.arange(1,20,2),\n        \"max_features\":[0.5,1,\"sqrt\",\"auto\"],\n        \"max_samples\":[10000]}\n\n#Intantiate Randomized search CV model\nrs_model= RandomizedSearchCV(RandomForestRegressor(n_jobs=-1,\n                                                  random_state=42),\n                                                  param_distributions=rf_grid,\n                                                  n_iter=5,\n                                                  cv=5,\n                                                  verbose=True)\n#Fit the randomizedSearchCV model\nrs_model.fit(X_train, y_train)","f10e492b":"#Finding the best model params\nrs_model.best_params_","9e3740c0":"#Evaluate the randomized search models(only trained on 10000 examples)\nshow_scores(rs_model)","99f10841":"%%time\n# Most ideal parameters:\nideal_model=RandomForestRegressor(n_estimators=40,\n                                 min_samples_leaf=1,\n                                 min_samples_split=14,\n                                 max_features=0.5,\n                                 n_jobs=-1,\n                                 max_samples=None,\n                                 random_state=42)\n\nideal_model.fit(X_train, y_train)","8a98a46f":"show_scores(ideal_model)","2960a3a2":"#import test data\ndf_test = pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/Test.csv\", low_memory=False, parse_dates=[\"saledate\"])\ndf_test.shape","753ec646":"def preprocess_data(df):\n    \"\"\"\n    Performs transformations on df and returns transformed df.\n    \"\"\"\n    df[\"saleYear\"] = df.saledate.dt.year\n    df[\"saleMonth\"] = df.saledate.dt.month\n    df[\"saleDay\"] = df.saledate.dt.day\n    df[\"saleDayOfWeek\"] = df.saledate.dt.dayofweek\n    df[\"saleDayOfYear\"] = df.saledate.dt.dayofyear\n    \n    df.drop(\"saledate\", axis=1, inplace=True)\n    \n    # Fill the numeric rows with median\n    for label, content in df.items():\n        if pd.api.types.is_numeric_dtype(content):\n            if pd.isnull(content).sum():\n                # Add a binary column which tells us if the data was missing or not\n                df[label+\"_is_missing\"] = pd.isnull(content)\n                # Fill missing numeric values with median\n                df[label] = content.fillna(content.median())\n    \n        # Filled categorical missing data and turn categories into numbers\n        if not pd.api.types.is_numeric_dtype(content):\n            df[label+\"_is_missing\"] = pd.isnull(content)\n            # We add +1 to the category code because pandas encodes missing categories as -1\n            df[label] = pd.Categorical(content).codes+1\n    \n    return df","61db5a18":"#Process test data\ndf_test= preprocess_data(df_test)\ndf_test.head()","1a412eea":"df_test.head()","8466a416":"#manually adjust df_test to have auctioneerID_is_missing columns\n\ndf_test[\"auctioneerID_is_missing\"]=False\ndf_test.head()","6f9a4e1c":"test_preds= ideal_model.predict(df_test)","610abb87":"len(test_preds)","c28a28d2":"df_preds= pd.DataFrame()\ndf_preds[\"SalesID\"] = df_test[\"SalesID\"]\ndf_preds[\"SalesPrice\"]= test_preds\ndf_preds","500e59a7":"#Save the predictions as per the competition format and check out the results\n#df_preds.to_csv(\"data\/bluebook_for_bulldozer_test_predictions.csv\", index=False)","32662ba8":"len(ideal_model.feature_importances_)","e380a696":"len(X_train.columns)","c96970ac":"# Helper function for plotting feature importance\n\n\ndef plot_features(columns, importances, n=20):\n    df = (pd.DataFrame({\"features\": columns,\n                        \"feature_importances\": importances})\n          .sort_values(\"feature_importances\", ascending=False)\n          .reset_index(drop=True))\n    \n    # Plot the dataframe\n    fig, ax = plt.subplots()\n    ax.barh(df[\"features\"][:n], df[\"feature_importances\"][:20])\n    ax.set_ylabel(\"Features\")\n    ax.set_xlabel(\"Feature importance\")\n    ax.invert_yaxis()","0a2dbf0f":"plot_features(X_train.columns, ideal_model.feature_importances_)","3abbae46":"## Feature Importance\n\nwhich diff attributes of the data were most important when it comes to predicting target variables(SalesPrice)","d6ad2016":"# Make preds on test data","8f1b6695":"# Now that all of our data is numeric, as well as dataframe has no missing values, we should be able to biuld a ML model","85be397d":"**Question:** WHy the above value, does not hold water\/true\/reliable?\nBecause we have done our scoring\/evaluation on the same datset on which we trianed our model","2c984d89":"## Save df_temp to a new csv ","37c179eb":"Finally now our test df has same features as training df, we can make preds","7a892208":"### Parsing Dates\nWhen we work with timeseries data, we want to enrich the time and Date coponent\nas much a possible\n\nWe can do that by telling pandas which of our columns has dates in it using 'pars date' parameter","6ad53c1d":"### Building an evalluation function\n","adf6c94e":"## Fill missing values\n\n### Fill numeric missing values first","fab2a884":"# Convert string to categories\nOne way we can turn all of our data into numbers is by converting them into pandas catgories.\n\nWe can check the different datatypes compatible with pandas here: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/general_utility_functions.html#data-types-related-functionality","58ce54a6":"# Thanks to pandas categories() we now have a way to access all of our data in form of numbers. But we still have a bunch of missing data","6c597096":"# Change max samples value","d99ae321":"# Sort DataFrame by saledate\u00b6\nWhen working with time series data, it's a good idea to sort it by date.","17d9f9d6":"### test_preds= ideal_model.predict(df_test)\n\nThis will not work as it has not been manipulated, filtered or cleaned","18599c64":"# The 'Valid RMSLE' value is what we are looking for and it is around  0.2452416398953833, which is very close and puts our code in top 30 of the submissions","ef254f65":"### Add datetime param for 'saledate' column","16739a43":"# Make predictions over test dataset","ca4272a2":"## Preprocessing the data(getting the test dataset in the same form of our training dataset)","e85db418":"def plot_features(columns, importances, n=20):\n    df = (pd.DataFrame({\"features\": columns,\n                        \"feature_importances\": importances})\n          .sort_values(\"feature_importances\", ascending=False)\n          .reset_index(drop=True))\n    \n    # Plot the dataframe\n    fig, ax = plt.subplots()\n    ax.barh(df[\"features\"][:n], df[\"feature_importances\"][:20])\n    ax.set_ylabel(\"Features\")\n    ax.set_xlabel(\"Feature importance\")\n    ax.invert_yaxis()","1e241c54":"# 5. Modelling\n\nWe've done enough EDA (we could always do more) but let's start to do some model-driven EDA","6af0ab8e":"# Format preds into same format Kaggle has asked\n","ad1a2c40":"# we can find how the columns differ using sets\nset(X_train.columns) - set(df_test.columns)","16d2ca06":"## Testing our model on a subset (to tune hyperparams)","37e787fc":"**Question to finish: Why knowing the feature importances of a trained machine learning model is helpful?**\n\nFinal challenge\/extension: What other machine learning models could you try on our dataset? Hint: https:\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/index.html check out the regression section of this map, or try to look at something like CatBoost.ai or XGBooost.ai.","32ba6649":"# Make a copy of the original dataframe\n\nWe make a copy of the original dataframe so when we manipulate the copy, we've still got our original data.\n","53b1c58a":"# \ud83d\ude9c Predicting the Sale Price of Bulldozers using Machine Learning\nIn this notebook, we're going to go through an example machine learning project with the goal of predicting the sale price of bulldozers.\n\n### 1. Problem defition\nHow well can we predict the future sale price of a bulldozer, given its characteristics and previous examples of how much similar bulldozers have been sold for?\n\n### 2. Data\nThe data is downloaded from the Kaggle Bluebook for Bulldozers competition: https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/data\n\nThere are 3 main datasets:\n\n* Train.csv is the training set, which contains data through the end of 2011.\n* Valid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Our score on this set is used to create the public leaderboard.\n* Test.csv is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n\n\n### 3. Evaluation\nThe evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.\n\nFor more on the evaluation of this project check: https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/overview\/evaluation\n\nNote: The goal for most regression evaluation metrics is to minimize the error. For example, our goal for this project will be to build a machine learning model which minimises RMSLE.\n\n### 4. Features\nKaggle provides a data dictionary detailing all of the features of the dataset. You can view this data dictionary on Google Sheets: https:\/\/docs.google.com\/spreadsheets\/d\/18ly-bLR8sbDJLITkWG7ozKm8l3RyieQ2Fpgix-beSYI\/edit?usp=sharing","957b20ea":"## Train a model with the best HyperParams\n**Note** These were found after a 100 iterations of RandomizedSearchCV"}}