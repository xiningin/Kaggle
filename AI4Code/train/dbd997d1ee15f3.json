{"cell_type":{"20f2d996":"code","522a00e0":"code","ac893f18":"code","03ca6a5d":"code","8a983580":"code","08cfd095":"code","c5757e18":"code","4a4e3352":"code","415cf680":"code","c54f783a":"code","36417290":"code","1e2b37ed":"code","8045e0a5":"code","6424f4e1":"code","ef72aeb3":"code","a44a0326":"code","b87c20cd":"code","5b854073":"code","26b5ac70":"code","0869fcd5":"markdown","4590f087":"markdown","a851e575":"markdown"},"source":{"20f2d996":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\nimport os\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","522a00e0":"SEED = 42\ndef seed_everything(seed):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\nseed_everything(SEED)","ac893f18":"## \u8a13\u7df4\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\ntrain = pd.read_csv(\"..\/input\/ykc-cup-1st\/train.csv\")\ntrain.head()\n\n## \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\ntest = pd.read_csv(\"..\/input\/ykc-cup-1st\/test.csv\")\ntest.head()\n\n## submission\u30d5\u30a1\u30a4\u30eb\u8aad\u307f\u8fbc\u307f\nsample_submission = pd.read_csv(\"..\/input\/ykc-cup-1st\/sample_submission.csv\")\nsample_submission.head()\n\ntrain[\"area1\"] = train[\"area_name\"].apply(lambda x : x.split(\" \")[0])\ntrain[\"area2\"] = train[\"area_name\"].apply(lambda x : x.split(\" \")[1])\ntest[\"area1\"] = test[\"area_name\"].apply(lambda x : x.split(\" \")[0])\ntest[\"area2\"] = test[\"area_name\"].apply(lambda x : x.split(\" \")[1])\n\ndef count_encoding(x_train, x_test, col, suffix = \"_count\", pre_concat = True):\n    if pre_concat: ##\u3082\u3057\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3082\u7528\u3044\u3066count\u3059\u308b\u306a\u3089\n        dic = pd.concat([x_train[col], x_test[col]]).value_counts().to_dict()\n    else:\n        dic = x_train[col].value_counts().to_dict()\n    x_train[col + suffix] = x_train[col].map(dic)\n    x_test[col + suffix] = x_test[col].map(dic)\n    return x_train, x_test\n\n## count\n# train[\"genre_area\"] = train[\"genre_name\"] + train[\"area_name\"]\n# test[\"genre_area\"] = test[\"genre_name\"] + test[\"area_name\"]\nfor c in [\"genre_name\", \"area_name\", \"area1\", \"area2\"]:\n    train, test = count_encoding(train, test, c)\n# train = train.drop([\"genre_area\"], axis = 1)\n# test = test.drop([\"genre_area\"], axis = 1)\n\n\ndef get_distance_feature(store):\n    d = np.sqrt(np.sum((store[[\"latitude\", \"longitude\"]].values[None, :, :] - store[[\"latitude\", \"longitude\"]].values[:, None, :]) ** 2, axis = 2))\n\n    for t in [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 5, 7]:\n        store[f\"rate_under{t}\"] = np.mean(d < t, axis = 1)\n\n    store[\"mean\"] = np.mean(d, axis = 1)\n    for q in [0.1, 1, 3, 5, 10, 25, 50, 75, 90]:\n        store[f\"quantile{q}\"] = np.percentile(d, q, axis = 1)\n\n    store = store.drop([\"longitude\", \"latitude\"], axis = 1)\n    return store\n\n\ndf = pd.concat([train, test])\n\n## \u8fd1\u304f\u306e\u304a\u5e97\nstore = df.groupby([\"store_id\"]).apply(lambda x : x.iloc[0][[\"latitude\", \"longitude\"]])\nstore = get_distance_feature(store)\nstore = store.add_prefix(\"distance_feature_\")\ntrain = train.merge(store, on = \"store_id\", how = \"left\")\ntest = test.merge(store, on = \"store_id\", how = \"left\")\n\n## \u8fd1\u304f\u306e\u304a\u5e97\uff08\u540c\u3058\u30b8\u30e3\u30f3\u30eb\uff09\nstore = df.groupby([\"store_id\"]).apply(lambda x : x.iloc[0][[\"latitude\", \"longitude\", \"genre_name\"]])\n\nstore_parts = []\nfor g in store[\"genre_name\"].unique()[::-1]:\n    store_part = store[store[\"genre_name\"] == g][[\"latitude\", \"longitude\"]]\n    store_parts.append(store_part)\n\nstore = pd.concat(store_parts)\nstore = store.add_prefix(\"distance_feature_same_genre_\")\ntrain = train.merge(store, on = \"store_id\", how = \"left\")\ntest = test.merge(store, on = \"store_id\", how = \"left\")\n\n## area size \u3068 area density\n# for a in [\"area_name\", \"area1\", \"area2\"]:\nfor a in [\"area_name\", \"area1\"]:\n# for a in [\"area_name\"]:\n    area = df.groupby([a])[[\"latitude\", \"longitude\"]].agg(\"std\").add_suffix(\"_std\")\n    area[f\"{a}_size1\"] = np.sqrt(area[\"latitude_std\"] ** 2 + area[\"longitude_std\"] ** 2)\n    area_max = df.groupby([a])[[\"latitude\", \"longitude\"]].agg(\"max\")\n    area_min = df.groupby([a])[[\"latitude\", \"longitude\"]].agg(\"min\")\n\n    area_diff = area_max - area_min\n    area[f\"{a}_size2\"] = np.sqrt((area_diff[\"latitude\"] ** 2 + area_diff[\"longitude\"] ** 2))\n    area = area.drop([\"latitude_std\", \"longitude_std\"], axis = 1)\n    train = train.merge(area, on = a, how = \"left\")\n    test = test.merge(area, on = a, how = \"left\")\n\n    train[f\"{a}_density1\"] = train[f\"{a}_count\"] \/ (train[f\"{a}_size1\"] + 1e-8)\n    train[f\"{a}_density2\"] = train[f\"{a}_count\"] \/ (train[f\"{a}_size2\"] + 1e-8)\n    test[f\"{a}_density1\"] = test[f\"{a}_count\"] \/ (test[f\"{a}_size1\"] + 1e-8)\n    test[f\"{a}_density2\"] = test[f\"{a}_count\"] \/ (test[f\"{a}_size2\"] + 1e-8)","03ca6a5d":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, QuantileTransformer, MinMaxScaler\n\ndef preprocess_continuous(x_train, x_test, continuous, params):\n    ss_cands = {\n    \"ss\" : StandardScaler(),\n    \"rank\" : QuantileTransformer(n_quantiles=len(x_train)\/\/10, output_distribution=\"normal\"),\n    \"mm\" : MinMaxScaler()}\n\n    x_train[continuous], x_test[continuous] = log_transform(x_train[continuous], x_test[continuous], skew_threshold = 1, n_max_repeat = 10)\n    \n    if params[\"normalize\"] != None:\n#         normalize_cols = [col for col in continuous if len(np.unique(x_train[col])) > 2] ## binary\u306fnormalize\u306e\u5bfe\u8c61\u306b\u3057\u306a\u3044\n        normalize_cols = continuous\n        print(normalize_cols)\n        \n        ss = ss_cands[params[\"normalize\"]]\n        ss.fit(x_train[normalize_cols])\n        x_train[normalize_cols] = ss.transform(x_train[normalize_cols])\n        x_test[normalize_cols] = ss.transform(x_test[normalize_cols])\n    \n    x_train = x_train.fillna(0)\n    x_test = x_test.fillna(0)\n    \n    return x_train, x_test\n\ndef preprocess_categorical(x_train, x_test, categoricals):\n    \n    ## thresholding\n    threshold = len(x_train) \/\/ 1e4\n    for col in categoricals:\n        dic = (x_train[col].value_counts() < threshold).to_dict()\n        x_train[col] = np.where(x_train[col].map(dic) == False, x_train[col], \"nan\")\n        x_test[col] = np.where(x_test[col].map(dic) == False, x_test[col], \"nan\")\n\n    most_appear_each_categories = {}\n    for col in categoricals:\n        x_train.loc[:,col] = x_train.loc[:, col].astype(str).fillna(\"nan\")\n        most_appear_each_categories[col] = list(x_train[col].value_counts().index)[0]\n        cands = np.unique(x_train[col])\n        if \"nan\" in cands:\n            x_test[col] = np.where(x_test[col].isin(cands), x_test[col].astype(str), \"nan\")\n        else:\n            x_test[col] = np.where(x_test[col].isin(cands), x_test[col].astype(str), most_appear_each_categories[col])\n\n    num_classes = 0\n    print(\"col, num_classes\")\n    for col in categoricals:\n        print(col, num_classes)\n        le = LabelEncoder()\n        le.fit(x_train[col].values)\n        x_train[col] = le.transform(x_train[col]).astype(np.uint32) + num_classes\n        x_test[col] = le.transform(x_test[col]).astype(np.uint32) + num_classes\n        num_classes += len(le.classes_)\n\n    return x_train, x_test, num_classes\n\n\ndef log_transform(x, x_test, skew_threshold = 1, n_max_repeat = 5):\n    \n    for k in range(n_max_repeat):\n        skewness = np.nanmean(((x - np.nanmean(x)) \/ (np.nanstd(x) + 1e-8)) ** 3)\n#         print(skewness)\n        if skewness > skew_threshold:    \n            minval = np.nanmin(x)\n            x = np.log1p(x - minval)\n            x_test = np.log1p(x_test - minval)\n        elif skewness < -skew_threshold:\n            maxval = np.nanmax(x)\n            x = np.log1p(maxval - x)\n            x_test = np.log1p(maxval - x_test)\n        else:\n            break\n    return x, x_test\n\nimport pandas as pd\nimport numpy as np\n\ndef get_feature_importance(gbms, columns):\n    feature_importance = pd.DataFrame()\n    feature_importance[\"name\"] = columns\n    feature_importance[\"importance\"] = 0\n    for k, gbm in enumerate(gbms):\n        feature_importance[\"importance\"] += gbm.feature_importance(\"gain\").astype(np.float32)\n    feature_importance = feature_importance.sort_values(by = \"importance\", ascending = False)\n    return feature_importance\n\ndef get_col_type(x_train):\n    continuous = []\n    categorical = []\n    for col in x_train.columns:\n        if \"object\" == str(x_train[col].dtype):\n            categorical.append(col)\n        else:\n            continuous.append(col)\n    return categorical, continuous\n","8a983580":"import tensorflow as tf\nclass MixupGenerator(tf.keras.utils.Sequence):\n    def __init__(self, tr_x, tr_y, batch_size = 32, training = True, mix_alpha = 0.1, mix_end = 50):\n        self.tr_x = tr_x\n        self.tr_y = tr_y\n        self.length = len(tr_x[0])\n        self.training = training\n        self.batch_size = batch_size\n        self.batch_num = self.length\/\/self.batch_size\n        self.rand_order = np.random.permutation(self.length)\n        self.alpha = mix_alpha\n        self.epoch = 0\n        self.mix_end = mix_end\n        self.verbose_print = True\n        \n    def __getitem__(self, idx):\n        if idx == 0:\n            self.on_epoch_start()\n\n        start_pos = self.batch_size * idx\n        end_pos = start_pos + self.batch_size\n        if end_pos > self.length:\n            end_pos = self.length\n            self.rand_order = np.random.permutation(self.length)\n            \n        x1 = [t[self.rand_order[start_pos : end_pos]] for t in self.tr_x]\n        y1 = [t[self.rand_order[start_pos : end_pos]] for t in self.tr_y]\n        if self.training == False or self.alpha == 0:\n            return x1 + [np.ones([len(x1[0]), 1, 1])], y1\n\n        ## mixup\n#         idx2 = np.random.randint(0, self.length, self.batch_size)\n        lam = np.random.beta(self.alpha, self.alpha, len(x1[0]))\n        lam[lam < 0.5] = 1 - lam[lam < 0.5]\n\n        def reshape_lam(lam, x):\n            return lam.reshape([-1] + [1] * (x.ndim - 1))\n        \n        def make_mixup_target(y, lam):\n            y_roll = np.roll(y, -1, axis = 0)\n            return y * reshape_lam(lam, y) + y_roll * (1 - reshape_lam(lam, y))\n            \n        for k in range(len(y1)):\n            y1[k] = make_mixup_target(y1[k], lam)\n            \n        if self.verbose_print:\n            print(lam.shape, lam[:10])\n            self.verbose_print = False\n        \n        return x1 + [lam.reshape(-1, 1, 1)], y1\n    \n    \n    def __len__(self):\n        return self.batch_num\n    def on_epoch_start(self):\n        self.epoch += 1\n        if self.training:\n            if self.epoch == self.mix_end:\n                self.training = False\n\n                print(self.epoch)\n                print(\"mix_end\")","08cfd095":"class ManifoldMixup(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(ManifoldMixup, self).__init__(**kwargs)\n        self.on = True\n    def call(self, inputs, training=None):\n        if training is None:\n            training = K.learning_phase()\n        x, lam = inputs[0], inputs[1]\n        if (training) and self.on:\n            print(\"mix\")\n            x_roll = tf.roll(x, shift=-1, axis = 0)\n            return tf.add(tf.multiply(x, lam), tf.multiply(x_roll, 1 - lam))\n        else:\n            return x\n\n    def compute_output_shape(self, input_shape):\n        return input_shape","c5757e18":"from tensorflow import keras\nfrom tensorflow.keras import backend as K\n    \ndef get_model(params):\n    n_emb = params[\"n_emb\"]#8\n    n_units = params[\"n_units\"]#[64, 32]\n    p_dropout = params[\"p_drop\"]#0.3\n    lr = params[\"lr\"]#1e-4\n    decay = params[\"decay\"]#1e-5\n    activation = params[\"activation\"]#mish\n    with_linear = params[\"with_linear\"]#True\n    with_fm = params[\"with_fm\"]#True\n    with_deep = params[\"with_deep\"]#True\n    num_continuous = params[\"num_continuous\"]\n    num_categorical = params[\"num_categorical\"]\n    num_classes = params[\"num_classes\"]\n\n    def return_linear(inp_con, inp_cat, num_classes):\n        con_linear = keras.layers.Dense(1, activation=None)(inp_con)\n        cat_linear = keras.layers.Embedding(num_classes, 1)(inp_cat)\n        cat_linear = keras.layers.Flatten()(cat_linear)\n        linear = keras.layers.Concatenate(axis = 1)([con_linear, cat_linear])\n        reduce_sum = keras.layers.Lambda(lambda x: keras.backend.sum(x, axis = 1, keepdims=True))\n        linear = reduce_sum(linear)\n        return linear\n    \n    def return_fm(emb):\n        fm = FM()(emb)\n        return fm\n\n    def return_deep(emb, n_units, activation, p_dropout):\n        deep = keras.layers.Flatten()(emb)\n        for k in range(len(n_units)):\n            deep = keras.layers.Dense(n_units[k], activation=None)(deep)\n            deep = keras.layers.BatchNormalization()(deep)\n            deep = keras.layers.Activation(activation)(deep)\n            deep = keras.layers.Dropout(rate = p_dropout)(deep)\n        return deep\n\n    ## input and embed\n    inp_con_raw = keras.layers.Input(shape=(num_continuous,), name = \"continuous_input\")\n    inp_con = keras.layers.Reshape((num_continuous, 1))(inp_con_raw)\n    con_emb = keras.layers.Dense(n_emb, activation=None)(inp_con)\n    \n    inp_cat_raw = keras.layers.Input(shape=(num_categorical,), name = \"categorical_input\")\n    cat_emb = keras.layers.Embedding(num_classes, n_emb)(inp_cat_raw)\n    \n    emb = keras.layers.Concatenate(axis = 1)([con_emb, cat_emb])\n    inp_lam = keras.layers.Input([1, 1])\n    emb = ManifoldMixup()([emb, inp_lam])\n    \n    ## each component\n    outs = []\n    if with_linear:\n        outs += [return_linear(inp_con_raw, inp_cat_raw, num_classes)]\n    if with_fm:\n        outs += [return_fm(emb)]\n    if with_deep:\n        outs += [return_deep(emb, n_units, activation, p_dropout)]\n            \n    ## output\n    if len(outs) > 1:\n        x = keras.layers.Concatenate()(outs)\n    else:\n        x = outs[0]\n#     out = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n\n#     x = keras.layers.Dense(16, activation=\"relu\")(x)\n    out = keras.layers.Dense(1, activation=None)(x)\n\n    \n    model = keras.models.Model(inputs = [inp_con_raw, inp_cat_raw, inp_lam], outputs = [out])\n#     model.compile(loss=[keras.losses.binary_crossentropy], optimizer=RAdam(learning_rate=lr, decay = decay))\n    return model\n\n\nclass FM(keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def call(self, x):\n        ps = keras.backend.pow(keras.backend.sum(x, 1), 2)\n        sp = keras.backend.sum(keras.backend.pow(x, 2), 1)\n        fm = keras.layers.Subtract()([ps, sp])\n        fm = keras.backend.sum(x, 1)\n        return fm\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[2])\n\n\nimport tensorflow as tf\nclass Mish(tf.keras.layers.Layer):\n\n    def __init__(self, **kwargs):\n        super(Mish, self).__init__(**kwargs)\n        self.supports_masking = True\n\n    def call(self, inputs):\n        return inputs * K.tanh(K.softplus(inputs))\n\n    def get_config(self):\n        base_config = super(Mish, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\ndef mish(x):\n\treturn tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n \nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.layers import Activation\nget_custom_objects().update({'mish': Activation(mish)})","4a4e3352":"!pip install --no-warn-conflicts -q tensorflow-addons\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, ReduceLROnPlateau, LearningRateScheduler\ndef trainNN(model, params, x_train, y_train, tr_inds, val_inds, continuous, categorical, fold = 0, verbose = 1):\n    \n#     opt = keras.optimizers.Adam(learning_rate=params[\"lr\"], decay = params[\"decay\"])\n    opt = tfa.optimizers.AdamW(learning_rate=params[\"lr\"], weight_decay = params[\"decay\"])\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(loss=[keras.losses.mse], optimizer=opt)\n    \n    cb_schedule = ReduceLROnPlateau(monitor='val_loss', factor=params[\"lr_reduce_factor\"], patience=params[\"lr_reduce_patience\"], verbose=1, min_lr=params[\"lr_min\"])\n    cb_es=EarlyStopping(monitor='val_loss',patience=10)\n    cb_save = ModelCheckpoint(f'model{fold}.hdf5', save_best_only=True, monitor='val_loss', mode='min', restore_best_weights=True)\n    callbacks = [cb_schedule, cb_es, cb_save]\n    \n    \n    tr_gen = MixupGenerator([x_train.loc[tr_inds, continuous].values, x_train.loc[tr_inds, categorical].values], [y_train[tr_inds].values], batch_size = params[\"batch_size\"], training = True, mix_end = params[\"epochs\"] - 5, mix_alpha = params[\"mix_alpha\"])\n    va_gen = MixupGenerator([x_train.loc[val_inds, continuous].values, x_train.loc[val_inds, categorical].values], [y_train[val_inds].values], batch_size = params[\"batch_size\"], training = False)\n    \n    model.fit(tr_gen, \n              epochs=params[\"epochs\"],\n              callbacks = callbacks,\n          validation_data=va_gen, verbose = verbose)\n    loss_history = model.history.history[\"val_loss\"]\n    \n#     ## sgd\n#     if params[\"epochs_sgd\"] > 0:\n#         model.compile(loss=[keras.losses.mse], optimizer=keras.optimizers.SGD(lr=params[\"lr_sgd\"], decay = params[\"decay_sgd\"], momentum = 0.9, nesterov = True))\n#         model.fit([x_train.loc[tr_inds, continuous], x_train.loc[tr_inds, categorical]], y_train[tr_inds], \n#                   batch_size=params[\"batch_size_sgd\"], epochs=params[\"epochs_sgd\"],\n#               callbacks = callbacks,\n#               validation_data=([x_train.loc[val_inds, continuous], x_train.loc[val_inds, categorical]], y_train[val_inds]), verbose = verbose)\n#         loss_history += model.history.history[\"val_loss\"]\n    model.load_weights(f\"model{fold}.hdf5\")\n    return model, loss_history","415cf680":"import numpy as np\ndef calc_score(y_test, y_pred):\n    score = {}\n    score[\"rmse\"] = np.sqrt(np.mean((y_test - y_pred)**2))\n    score[\"mse\"] = np.mean((y_test - y_pred)**2)\n    score[\"corr\"] = np.corrcoef(y_test, y_pred)[0,1]\n    return score","c54f783a":"categorical, continuous = get_col_type(train)\ncategorical.remove(\"id\")\ncontinuous.remove(\"log_visitors\")\nprint(categorical)","36417290":"params = {\n    \"normalize\" : \"ss\", \n    \"n_emb\" : 4,\n    \"n_units\" : [128, 64, 16],\n    \"p_drop\" : 0.5,\n    \"activation\" : \"mish\",\n    \"with_linear\" : True,\n    \"with_fm\" : True,\n    \"with_deep\" : True,\n    \"lr\" : 5e-3,\n    \"lr_min\" : 5e-3,\n    \"lr_reduce_factor\" : 0.8,\n    \"lr_reduce_patience\" : 5,\n    \"mix_alpha\" : 0.2,\n    \"decay\" : 1e-3,\n    \"batch_size\" : 128,\n    \"epochs\" : 15,\n    \"lr_sgd\" : 1e-5,\n    \"decay_sgd\" : 1e-2,\n    \"batch_size_sgd\" : 32,\n    \"epochs_sgd\" : 0}","1e2b37ed":"\ndrops = []\ntrain, test = preprocess_continuous(train, test, continuous, params) ## normalize\u3068\u304b\ntrain, test, num_classes = preprocess_categorical(train, test, categorical) ## label encoding\u3068\u304b\ntrain.head()","8045e0a5":"## nn\u306e\u305f\u3081\u306b\u6e96\u5099\nparams[\"num_continuous\"] = len(continuous)\nparams[\"num_categorical\"] = len(categorical)\nparams[\"num_classes\"] = num_classes","6424f4e1":"target = \"log_visitors\" ## \u4e88\u6e2c\u5bfe\u8c61\ndrops = [\"id\", target]\nfeatures = list(train.columns.drop(drops)) ## \u4e88\u6e2c\u306b\u4f7f\u7528\u3059\u308b\u7279\u5fb4\u91cf\u306e\u540d\u524d\nn_split = 5 ## cross validation\u306efold\u6570\nx_train = train[features]\nx_test = test[features]\ny_train = train[target]\n# y_train_mean = np.mean(y_train)\n# y_train = y_train - y_train_mean\n\n# kfold = GroupKFold(5)\nmodels = []\nscores = []\nloss_histories = []\nfor k in range(20):\n    model = get_model(params)\n    model, loss_history = trainNN(model, params, x_train, y_train, np.arange(len(x_train)), np.arange(len(x_train)), continuous, categorical, fold = k, verbose = 1)\n    models.append(model)\n    loss_histories.append(loss_history)","ef72aeb3":"score_average = pd.DataFrame(scores).mean().to_dict()\nprint(score_average)","a44a0326":"{'rmse': 0.3226358329530318, 'mse': 0.1041633176597084, 'corr': 0.6482009184727325}\n{'rmse': 0.32079737067250724, 'mse': 0.10296978872413694, 'corr': 0.6519500141961425}\n{'rmse': 0.3203280668121673, 'mse': 0.1026900536725764, 'corr': 0.6517946837546061}\n{'rmse': 0.3193463436920432, 'mse': 0.10205253913878383, 'corr': 0.6538121299218247}","b87c20cd":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfor loss_history in loss_histories:\n    plt.plot(loss_history)\nprint(params[\"epochs\"])\nplt.xlabel(\"epochs\")\nplt.ylabel(\"val_loss\")\nplt.title(\"learning curve\")","5b854073":"y_preds = 0\nfor model in models:\n    y_pred = model.predict([x_test[continuous], x_test[categorical], np.ones([len(x_test), 1, 1])])[:,0]\n    y_preds += y_pred\nfinal_pred = y_preds\/len(models)","26b5ac70":"sample_submission[\"log_visitors\"] = final_pred\nsample_submission.to_csv(\"submission.csv\", index = False)","0869fcd5":"## NN","4590f087":"## load","a851e575":"## Submission"}}