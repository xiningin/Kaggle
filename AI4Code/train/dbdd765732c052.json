{"cell_type":{"770f5d71":"code","0c98edac":"code","848192a9":"code","bc9cdbab":"code","bf6f6c4d":"code","c5d48d07":"code","8424d633":"code","23c20920":"code","e268b486":"code","1ceb8aed":"code","0bc19d44":"code","b990304e":"code","7dcc450a":"code","396168d2":"code","5cc26c07":"code","f40dae31":"code","6c9975e7":"code","f90ce9c9":"code","56d03400":"code","3d69f1db":"code","3c1c260f":"code","360cda13":"code","16d4d7ce":"code","85762952":"code","fc6af605":"code","337b88c2":"code","c61adc6f":"code","cf42e530":"code","9753856d":"code","f574087a":"code","3a8cd173":"code","de84df93":"code","d61dac76":"code","a831e1bc":"code","98b69e61":"code","49c271d3":"code","b135adbb":"code","8107fb1f":"code","d7b8239b":"code","1d0f945b":"code","3d9e05a7":"code","f5b449b0":"code","1ecd094d":"code","fc5dc3f1":"code","91fecb0d":"code","b62054d1":"code","837a8d28":"code","d989b88e":"code","c9e78fde":"code","56b019a6":"code","ba4d8e8d":"code","de643b6d":"code","a7327e00":"code","ba85bc09":"code","5debcc25":"code","aa3d6006":"code","7a94d37c":"code","f0898591":"code","0e400ecf":"code","6fb935f4":"code","d3e7668c":"code","58f45dc0":"code","b73c3482":"code","2e064cf1":"code","9e0908c6":"code","3688969d":"code","ee2a2125":"code","b7aea246":"code","c3a6b138":"code","0e40a686":"code","da3e8919":"code","6a5963dd":"code","d0615a3d":"code","862a9b5c":"code","b9b87c01":"code","42a08e63":"code","3efdf40b":"code","b71824b3":"code","609ee704":"code","ff57d398":"code","68b11629":"code","a6b6be19":"code","b5f4c202":"code","e75855fe":"code","53ab0dce":"code","4884acc1":"code","125e4482":"code","b9fae5ee":"code","18111d31":"code","aa57de96":"code","b51a6190":"code","de277090":"code","d0cfc0a9":"code","b14a65ca":"code","9ffdffef":"code","da9cf7b0":"code","a65d544f":"code","5d2ae495":"code","02687dbb":"code","fa4ddea5":"code","3bd0d4a4":"code","7ca12ef8":"code","cb63795a":"code","f9c92dc3":"code","68da4647":"code","0dbcbe0e":"code","4cc713b9":"code","f7b4fe78":"code","fa8101d4":"code","d30d5c27":"code","2e18ab82":"code","eef8a2d2":"code","77920dd1":"code","e446d495":"code","406f722e":"code","24b5d22e":"code","726838bd":"code","e6f76621":"code","d8cd4baa":"code","766dedac":"code","c45cfb05":"code","01bc7738":"code","316aefdd":"code","9fd67afc":"code","0c68d3cb":"code","8f963314":"code","cc051dbf":"code","cc92fb5d":"code","416f2d90":"code","32bdb5d9":"code","5d07f171":"code","c4767d2e":"code","a502041a":"code","dea96c25":"code","7dc8137a":"code","aaffdd1f":"code","6ec47a90":"code","de54962d":"code","e5c174a3":"code","88de4dcc":"code","49200780":"code","303cabb0":"code","da73628a":"code","3bcfb11d":"code","3b7bb5d8":"code","e7776763":"code","9cf0af33":"code","6730cd22":"code","b763f79d":"code","2819d184":"code","4c3c9b7d":"code","47ec2eb5":"code","53bd9026":"code","c73e0c52":"code","699aa2e2":"code","1e592e26":"code","0c06e995":"code","05a46827":"code","ead4afb2":"code","caa60069":"code","528b15ef":"code","73a858fe":"code","a123d8c4":"code","f250be70":"code","3f0486cd":"code","06978634":"code","f48a82bc":"code","f97b1761":"code","835895f8":"code","2f47cc03":"code","14ef468f":"code","e1e901dc":"code","16eaf21b":"code","b34b980a":"code","93b2a129":"code","2bc8bd58":"code","b62ce19b":"code","4a5ddd64":"code","1f221955":"code","f15332bc":"code","da43672b":"code","a78a8301":"code","b8131975":"code","c2e77c94":"code","dd8cf9d5":"markdown","9bef780a":"markdown","f4f47751":"markdown","99e606f5":"markdown","0d23db4c":"markdown","b0a8252c":"markdown","8e2c939b":"markdown","11b10f0d":"markdown","280cfb8f":"markdown","9b5d53d0":"markdown","c3183f2f":"markdown","f5205aff":"markdown","1d62ed9b":"markdown","42858227":"markdown","a5e9a907":"markdown","e53aa1f1":"markdown","d76d98f1":"markdown","08f5c427":"markdown","dede0502":"markdown","48432b94":"markdown","cfca9180":"markdown","16533169":"markdown","3354daa7":"markdown","a0102627":"markdown","ea0c1f91":"markdown","28590c37":"markdown","91d97bba":"markdown","a429ff71":"markdown","4e11f845":"markdown","3341e77a":"markdown","6e74fafe":"markdown","d889492a":"markdown","cb2193e2":"markdown","e79e7540":"markdown","e7a67f1a":"markdown","0bc2c866":"markdown","a1b03768":"markdown","30df70c2":"markdown","7214d29c":"markdown","9e72118c":"markdown","eadbf9ce":"markdown","6c09e534":"markdown","2567b0af":"markdown","b54a7a05":"markdown","9a039d71":"markdown","aff0a9f0":"markdown","ec660f46":"markdown","24c12d52":"markdown","3cd476f6":"markdown","88e71610":"markdown","f2ca3a29":"markdown","cc4f1667":"markdown","e62116d6":"markdown","6d311e45":"markdown","644c2235":"markdown","d8a42748":"markdown","d47340d5":"markdown","928f2e18":"markdown","a979d921":"markdown","b1529738":"markdown","d7f1e7b6":"markdown","26ded586":"markdown","fa950e08":"markdown","c0b27035":"markdown","fcec6258":"markdown","1124e663":"markdown","6b5ad2da":"markdown","631d2ff2":"markdown","1cab60c5":"markdown","1add901e":"markdown","c6be98a2":"markdown","b838a624":"markdown","af64a560":"markdown","83a44522":"markdown","a1a87987":"markdown","609b4bd8":"markdown","42c6a0c9":"markdown","73402343":"markdown","46295fe9":"markdown","a638310a":"markdown","79810637":"markdown","3e5859a3":"markdown","663ff6da":"markdown","5ee1ea34":"markdown","7eb77b9c":"markdown","994ebf97":"markdown","ab960ec9":"markdown","c82effcc":"markdown","f7b0e42e":"markdown","4011d062":"markdown","081398a7":"markdown","f490f692":"markdown","1565fa0e":"markdown","81fab0a6":"markdown","61af83a4":"markdown","94389ea7":"markdown","04523003":"markdown","2f433e2c":"markdown","eef5e2c1":"markdown"},"source":{"770f5d71":"!pip install pandarallel\n!pip install langdetect\n!pip install glove_python\n!pip install pyjarowinkler","0c98edac":"# Basics\nimport gc\nimport re\nimport os\nimport json\nimport heapq\nimport pickle\nimport string\nimport random\nimport logging\nimport datetime\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom copy import deepcopy\nfrom unicodedata import normalize\n\n# Multiprocessing, compute acceleration\nimport numba\nfrom numba import prange\nfrom pandarallel import pandarallel\nfrom multiprocessing import Pool, cpu_count\n\n# NLP\nimport nltk\nimport spacy\nfrom langdetect import detect\nfrom glove import Corpus, Glove\nfrom gensim.utils import deaccent\n# nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom gensim.models import CoherenceModel, TfidfModel\nfrom gensim.models import LdaModel, LdaMulticore\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.corpora import Dictionary\nfrom pyjarowinkler import distance\n\n# Graphs\nimport networkx as nx\n\n# Data Science\nimport umap\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Visuals\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\nimport pyLDAvis\nimport pyLDAvis.gensim\nfrom wordcloud import WordCloud, STOPWORDS\n\n%matplotlib inline\noutput_notebook()","848192a9":"# Set figure size for matplotlib\nplt.rcParams[\"figure.figsize\"] = (10, 10)\npd.options.display.max_colwidth=160","bc9cdbab":"# Install language models\n!python -m spacy download en\n!python -m spacy download fr\n!python -m spacy download es\n!python -m spacy download it\n!python -m spacy download de\n!python -m spacy download pt","bf6f6c4d":"class DataHandler(object):\n    \"\"\"\n    Class that is used to load all the data from the CORD-19 dataset\n    \"\"\"\n    def __init__(self, datapath):\n        self.datapath = datapath\n\n    def read_metadata(self, metadata_fileneme):\n        \"\"\"\n        Read metadata csv\n        \"\"\"\n        return pd.read_csv(self.datapath + metadata_fileneme)\n\n    def read_papers(self, filelist):\n        \"\"\"\n        Read json files from list of files\n        \"\"\"\n        list_json_data = []\n\n        for filename in filelist:\n            file = json.load(open(filename, \"rb\"))\n            list_json_data.append(file)\n\n        return list_json_data\n\n    def read_files_paths(self, folderpath, fileext=\".json\"):\n        \"\"\"\n        Get paths to all the files inside a folder recursively. Set an extensions if needed\n        \"\"\"\n        fls = [\n            os.path.join(root, fn)\n            for root, dirs, files in os.walk(Path(folderpath))\n            for fn in files\n            if Path(fn).suffix == fileext\n        ]\n\n        return fls\n\n    def parse_authors(self, authors):\n        \"\"\"\n        Parse authors field\n        \"\"\"\n        authors_ls = []\n        names_ls = []\n        affiliations_ls = []\n        emails_ls = []\n\n        for author in authors:\n\n            author_text = []\n\n            # Parse name\n            middle_name = \" \".join(author[\"middle\"])\n            full_name = (\n                \" \".join([author[\"first\"], middle_name, author[\"last\"]])\n                if author[\"middle\"]\n                else \" \".join([author[\"first\"], author[\"last\"]])\n            )\n            author_text.append(full_name)\n            names_ls.append(full_name)\n\n            # Parse affiliation\n            affiliation = author[\"affiliation\"]\n            if affiliation:\n                affiliation_text = []\n                laboratory = affiliation[\"laboratory\"]\n                institution = affiliation[\"institution\"]\n                location = affiliation[\"location\"]\n                if laboratory:\n                    affiliation_text.append(laboratory)\n                if institution:\n                    affiliation_text.append(institution)\n                if location:\n                    affiliation_text.append(\" \".join(list(location.values())))\n\n                affiliation_text = \", \".join(affiliation_text)\n                author_text.append(f\"({affiliation_text})\")\n                affiliations_ls.append(affiliation_text)\n\n            # Parse email\n            email = author[\"email\"]\n            if email:\n                author_text.append(f\"[{email}]\")\n                emails_ls.append(email)\n\n            # Concat info\n            author_text = \", \".join(author_text)\n            authors_ls.append(author_text)\n\n        return {\n            \"authors_full\": \"; \".join(authors_ls),\n            \"names\": \"; \".join(names_ls),\n            \"emails\": \"; \".join(emails_ls),\n            \"affiliations\": \"; \".join(affiliations_ls),\n        }\n\n    def parse_abstract_body(self, body_text, raw_mode=False):\n        \"\"\"\n        Parse abstract and body fields\n        \"\"\"\n        entries_ls = []\n\n        for entry in body_text:\n            # Get section and its text\n            text = entry[\"text\"]\n            section = entry[\"section\"]\n\n            if raw_mode != True:\n                entries_ls.append(section)\n                entries_ls.append(text)\n            else:\n                entries_ls.append({\"text\": text, \"section\": section})\n\n        if raw_mode != True:\n            return \"\\n\\n\".join(entries_ls)\n        else:\n            return entries_ls\n\n    def parse_bib(self, bibs):\n        \"\"\"\n        Parse bibliography field\n        \"\"\"\n        if type(bibs) == dict:\n            bibs = list(bibs.values())\n        bibs = deepcopy(bibs)\n\n        bibs_clean = []\n        for bib in bibs:\n            title = bib[\"title\"]\n            authors = bib[\"authors\"]\n            year = bib[\"year\"]\n            venue = bib[\"venue\"]\n            bibs_clean.append(f\"{title} [{year}, {venue}]\")\n\n        return \"; \".join(bibs_clean)\n\n    def build_df(self, subfolder_papers, no_full_text=False):\n        \"\"\"\n        Build a final Dataframe\n        \"\"\"\n        list_json = self.read_files_paths(self.datapath + subfolder_papers)\n        list_json_data = self.read_papers(list_json)\n\n        raw_data = {\n            \"paper_id\": [],\n            \"title\": [],\n            \"authors\": [],\n            \"authors_names\": [],\n            \"authors_affiliations\": [],\n            \"authors_emails\": [],\n            \"abstract\": [],\n            \"text\": [],\n            \"bibliography\": [],\n            \"raw_bibliography\": [],\n        }\n        for file in tqdm(list_json_data):\n            \n            raw_data[\"paper_id\"].append(file[\"paper_id\"])\n            raw_data[\"title\"].append(file[\"metadata\"][\"title\"])\n\n            authors = self.parse_authors(file[\"metadata\"][\"authors\"])\n            raw_data[\"authors\"].append(authors[\"authors_full\"])\n            raw_data[\"authors_names\"].append(authors[\"names\"])\n            raw_data[\"authors_affiliations\"].append(authors[\"affiliations\"])\n            raw_data[\"authors_emails\"].append(authors[\"emails\"])\n            \n            if \"abstract\" in file:\n                raw_data[\"abstract\"].append(self.parse_abstract_body(file[\"abstract\"]))\n            else:\n                raw_data[\"abstract\"].append(\"\")\n            if no_full_text == True:\n                raw_data[\"text\"].append(\"\")\n            else:\n                raw_data[\"text\"].append(self.parse_abstract_body(file[\"body_text\"]))\n            raw_data[\"bibliography\"].append(self.parse_bib(file[\"bib_entries\"]))\n            raw_data[\"raw_bibliography\"].append(file[\"bib_entries\"])\n\n        df_data = pd.DataFrame(raw_data)\n\n        return df_data","c5d48d07":"data_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_file_path = \"metadata.csv\"","8424d633":"datahandler = DataHandler(data_path)","23c20920":"df_meta = datahandler.read_metadata(metadata_file_path)","e268b486":"df_biorxiv = datahandler.build_df('biorxiv_medrxiv')\ndf_biorxiv['subset'] = \"biorxiv\"","1ceb8aed":"# df_comm = datahandler.build_df('comm_use_subset')\n# df_comm['subset'] = \"comm\"","0bc19d44":"df_noncomm = datahandler.build_df('noncomm_use_subset')\ndf_noncomm['subset'] = \"noncomm\"","b990304e":"# df_custom = datahandler.build_df('custom_license')\n# df_custom['subset'] = \"custom_license\"","7dcc450a":"columns_to_keep = [\n    \"paper_id\",\n#     \"source_x\",\n#     \"journal\",\n#     \"doi\",\n#     \"pmcid\",\n#     \"pubmed_id\",\n    \"title_x\",\n    \"authors_x\",\n#     \"authors_names\",\n#     \"authors_affiliations\",\n#     \"authors_emails\",\n    \"abstract_x\",\n    \"publish_time\",\n    \"text\",\n    \"bibliography\",\n    \"raw_bibliography\",\n#     \"license\",\n#     \"Microsoft Academic Paper ID\",\n#     \"WHO #Covidence\",\n    \"subset\"\n]","396168d2":"# Declare the datasets you want to merge\ndatasets_to_read = [\n    df_biorxiv,\n    df_noncomm\n]","5cc26c07":"# Concat dfs generated from jsons, drop duplicates according to paper_id and abstract\ndf_all = pd.concat(datasets_to_read)\ndf_all = df_all.drop_duplicates(subset=['paper_id', 'abstract',]).reset_index(drop=True)","f40dae31":"# Merge raw data with metadata\ndf_merged = pd.merge(df_meta, df_all, left_on='sha', right_on='paper_id', how='inner')[columns_to_keep]\n# Rename columns\ndf_merged = df_merged.rename(columns={'source_x': 'source', 'title_x': 'title', 'authors_x': 'authors', 'abstract_x': 'abstract',})\n# Replace empty string fields with nans\ndf_merged = df_merged.replace(r'^\\s*$', np.nan, regex=True)","6c9975e7":"# Drop duplicates again and reset index\ndf_merged.drop_duplicates(subset='abstract', inplace=True)\ndf_merged.reset_index(drop=True, inplace=True)","f90ce9c9":"# Create column with all text data\ndf_merged['title_abstract_text'] = df_merged['title'].astype(str) + \"\\n\\n\" + df_merged['abstract'].astype(str) + \"\\n\\n\" + df_merged['text'].astype(str)","56d03400":"# Check for nans\ndf_merged.loc[:, df_merged.isnull().any()].columns","3d69f1db":"df_merged['authors'].fillna(\"\", inplace=True)\ndf_merged['abstract'].fillna(\"\", inplace=True)","3c1c260f":"df_merged.shape","360cda13":"class Preprocessor(object):\n    \"\"\"\n    Class that is used to clean raw str\n    \"\"\"\n    def __init__(self, force_deaccent, force_ascii, min_token_len, stop_list_custom, disabled_components, default_language):\n        \n        self.force_deaccent = force_deaccent\n        self.force_ascii = force_ascii\n        self.default_language = default_language\n        self.min_token_len = min_token_len\n        self.nlp_list = {\n            \"en\": spacy.load(\"en\", disable=disabled_components),\n            \"fr\": spacy.load(\"fr\", disable=disabled_components),\n            \"es\": spacy.load(\"es\", disable=disabled_components),\n            \"it\": spacy.load(\"it\", disable=disabled_components),\n            \"de\": spacy.load(\"de\", disable=disabled_components),\n            \"pt\": spacy.load(\"pt\", disable=disabled_components),\n            }\n        self.stop_list_all = self.init_stopwords(stop_list_custom)\n\n    def init_stopwords(self, stop_list_custom):\n        \"\"\"\n        Get list of stopwords from nltk, spacy and used custom list \n        \"\"\"\n        # Get nltk stopwords\n        stop_list_nltk = list(set(stopwords.words('english'))) \\\n        + list(set(stopwords.words('french'))) \\\n        + list(set(stopwords.words('spanish'))) \\\n        + list(set(stopwords.words('italian'))) \\\n        + list(set(stopwords.words('german'))) \\\n        + list(set(stopwords.words('portuguese')))\n        # Get spacy stopwords\n        stop_list_spacy = list(spacy.lang.en.stop_words.STOP_WORDS) \\\n        + list(spacy.lang.fr.stop_words.STOP_WORDS) \\\n        + list(spacy.lang.es.stop_words.STOP_WORDS) \\\n        + list(spacy.lang.it.stop_words.STOP_WORDS) \\\n        + list(spacy.lang.de.stop_words.STOP_WORDS) \\\n        + list(spacy.lang.pt.stop_words.STOP_WORDS) \\\n        \n        return stop_list_nltk+stop_list_spacy+stop_list_custom\n    \n    def detect_language(self, text):\n        \"\"\"\n        Detect language with langdetect\n        \"\"\"\n        try:\n            lang = detect(text)\n        except:\n            lang = \"unknown\"\n        return lang \n\n    def preprocess(self, text):\n        \"\"\"\n        Main function to preprocess the text\n        \"\"\"\n        lang = self.detect_language(text)\n            \n        if lang in [\"en\", \"fr\", \"es\", \"it\", \"de\", \"pt\"]:\n            nlp = self.nlp_list[lang]\n        else:\n            nlp = self.nlp_list[self.default_language]\n            \n        # Delete some punctuation before preprocessing BUT not all of it because some can be involved in n-grams (e.g. \"-\")\n        text=re.sub(r'[!\"#$%&\\'()*+,.\/:;<=>?@\\[\\\\\\]^_`{|}~]',r' ',text) \n        \n        # Apply spacy to the text\n        doc = nlp(text)\n        # Lemmatization, remotion of noise (stopwords, digit, puntuaction and singol characters)\n        tokens = [\n            token.lemma_ for token in doc if\n            token.lemma_ != '-PRON-'\n            and not token.is_punct\n            and not token.is_digit\n            and not token.like_num\n            and not token.like_url\n            and not token.like_email\n            and len(token.lemma_) >= self.min_token_len and len(token.text) >= self.min_token_len\n            and token.lemma_.lower() not in self.stop_list_all and token.text.lower() not in self.stop_list_all\n        ]\n        \n        # Recreation of the text\n        text = \" \".join(tokens)\n\n        # Remove accents, normalize to ascii\n        if self.force_ascii:\n            text = normalize('NFD', text).encode('ascii', 'ignore').decode('UTF-8')\n        \n        if self.force_deaccent:\n            text = deaccent(text)\n    \n        # Remove double spaces\n        text=re.sub(r'\\s+',r' ',text)\n        \n        # Set as lowercase\n        text = text.lower().strip()\n\n        return text","16d4d7ce":"preprocessor = Preprocessor(\n    force_ascii=False,\n    force_deaccent=True,\n    min_token_len=1,\n    stop_list_custom=[\n        'positives', 'true', 'false', 'tp', 'fp' 'cc_nc', 'q_q', 'r', 'b', 'p', 'q', 'h', 'cc', 'doi', 'medrxiv', 'fig', 'org', 'tb'\n    ],\n    disabled_components=['parser', 'ner'],\n    default_language='en'\n)","85762952":"pandarallel.initialize(nb_workers=cpu_count(), progress_bar=True)","fc6af605":"df_merged[\"title_abstract_text_preprocessed\"] = df_merged.parallel_apply(\n        lambda x: preprocessor.preprocess(x[\"title_abstract_text\"]), axis=1\n    )","337b88c2":"def find_num_topics(dictionary, corpus, docs, end, start=2, step=2):\n    \"\"\"\n    Train multiple LDA models in an indicated range of number of topics  \n    \"\"\"\n    coherence_values = []\n    model_list = []\n    for num_topics in tqdm(range(start, end, step)):\n        model = LdaModel(\n            corpus=corpus,\n            num_topics=num_topics,\n            id2word=id2word,\n            update_every=1,\n            eval_every = 100,\n            random_state=100,\n            chunksize=2000,\n            passes=4,\n            iterations=100,\n            per_word_topics=True,\n        )\n\n        model_list.append(model)\n        coherencemodel = CoherenceModel(\n            model=model, texts=docs, dictionary=dictionary, coherence=\"c_v\"\n        )\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values","c61adc6f":"def plot_coherence_score(num_topic, coherence_score):\n    \"\"\"\n    Plot coherence scores for LDA model\n    \"\"\"\n    p = figure(plot_width=400, plot_height=400)\n\n    # Add both a line and circles on the same plot\n    p.line(num_topic, coherence_score, line_width=2)\n    p.circle(num_topic, coherence_score, fill_color=\"white\", size=8)\n    p.xaxis.axis_label = \"Number Of Topics\"\n    p.yaxis.axis_label = \"Coherence Score\"\n\n    show(p)","cf42e530":"def visualize_topics(lda_model, corpus, id2word):\n    \"\"\"\n    Generate a visual dashboard of LDA topics\n    \"\"\"\n    pyLDAvis.enable_notebook()\n    \n    return pyLDAvis.gensim.prepare(lda_model, corpus, id2word)","9753856d":"def print_topics(lda_model):\n    \"\"\"\n    Print topics made of keywords found in LDA model\n    \"\"\"\n    columns_name = []\n    pd_dict = {}\n\n    num_topic = lda_model.num_topics\n    for i in range(num_topic):\n        columns_name.append(((f\"Topic_{i+1}\", \"Word\")))\n        words, weight = zip(*lda_model.show_topic(i))\n        pd_dict[f\"topic{i+1}\"] = list(words)\n\n    df = pd.DataFrame(pd_dict)\n\n    return df","f574087a":"# Split to tokens\ndocs_tokens = [doc.split() for doc in list(df_merged.title_abstract_text_preprocessed)]","3a8cd173":"# Infer bigram model\nbigram_mod = Phraser(Phrases(docs_tokens, min_count=10, threshold=10))","de84df93":"# Save model if needed\n# bigram_mod.save('.\/data\/bigram_mod')","d61dac76":"# Get bigrams\ndocs_bigram_tokens = [bigram_mod[doc] for doc in docs_tokens]","a831e1bc":"# Create Dictionary\nid2word = Dictionary(docs_bigram_tokens)\nid2word.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)","98b69e61":"# Save model if needed\n# id2word.save('.\/data\/id2word')","49c271d3":"# Term Document Frequency\nbow_corpus = [id2word.doc2bow(doc) for doc in docs_bigram_tokens]","b135adbb":"# TF-IDF Document Frequency\ntfidf = TfidfModel(bow_corpus)\ntfidf_corpus = tfidf[bow_corpus]","8107fb1f":"start_topic, end_topic, step = 10, 18, 2\n\n# Train LDA models\nmodel_list, coherence_values = find_num_topics(\n    dictionary=id2word,\n    corpus=bow_corpus,\n    docs=docs_bigram_tokens, \n    start=start_topic, \n    end=end_topic,\n    step=step\n    )","d7b8239b":"plot_coherence_score(range(start_topic, end_topic, step), coherence_values)","1d0f945b":"bow_lda_model = model_list[1]","3d9e05a7":"# Compute Perplexity\nprint('Perplexity: ', bow_lda_model.log_perplexity(bow_corpus))","f5b449b0":"# visualize_topics(bow_lda_model, bow_corpus, id2word)","1ecd094d":"print_topics(bow_lda_model)","fc5dc3f1":"# Save model if needed\n# bow_lda_model.save('bow_lda_model')","91fecb0d":"start_topic, end_topic, step = 10, 18, 2\n\ntfidf_model_list, tfidf_coherence_values = find_num_topics(\n    dictionary=id2word,\n    corpus=tfidf_corpus,\n    docs=docs_bigram_tokens,\n    start=start_topic,\n    end=end_topic,\n    step=step\n    )","b62054d1":"plot_coherence_score(range(start_topic, end_topic, step), tfidf_coherence_values)","837a8d28":"tfidf_lda_model = tfidf_model_list[0]","d989b88e":"# Compute Perplexity\nprint('Perplexity: ', tfidf_lda_model.log_perplexity(tfidf_corpus))  ","c9e78fde":"# visualize_topics(tfidf_lda_model, tfidf_corpus, id2word)","56b019a6":"print_topics(tfidf_lda_model).style.background_gradient(cmap='viridis')","ba4d8e8d":"# Save model if needed\n# tfidf_lda_model.save('tfidf_lda_model')","de643b6d":"lda_model = bow_lda_model","a7327e00":"def topic_all_documents(lda_model, corpus, texts):\n    \"\"\"\n    Obtain topics for all the documents\n    \"\"\"\n    # Init output\n    documents_topic_df = pd.DataFrame()\n    columns_name = [\"Document\", \"Dominant Topic\", \"Topic Score\", \"Word List\"]\n    columns_name.extend([f\"Topic_{i+1}\" for i in range(lda_model.num_topics)])\n    columns_name.append(\"Text\")\n\n    words_topic = []\n    for i in range(lda_model.num_topics):\n        x, _ = zip(*lda_model.show_topic(i))\n        words_topic.append(list(x))\n\n    for document_indx, topic_score in enumerate(lda_model.get_document_topics(corpus)):\n        dominant_topic = sorted(topic_score, key=lambda x: x[1], reverse=True)[0]\n\n        row_score = np.zeros(lda_model.num_topics)\n        index, score = zip(*topic_score)\n        row_score[list(index)] = score\n        row_score = np.around(row_score, 4)\n\n        documents_topic_df = documents_topic_df.append(\n            pd.concat(\n                [\n                    pd.Series(\n                        [\n                            int(document_indx),\n                            dominant_topic[0] + 1,\n                            round(dominant_topic[1], 4),\n                            words_topic[dominant_topic[0]],\n                        ]\n                    ),\n                    pd.Series(row_score),\n                ],\n                ignore_index=True,\n            ),\n            ignore_index=True,\n        )\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    documents_topic_df = pd.concat([documents_topic_df, contents], axis=1)\n\n    documents_topic_df.columns = columns_name\n    documents_topic_df[\"Dominant Topic\"] = pd.to_numeric(\n        documents_topic_df[\"Dominant Topic\"]\n    )\n    documents_topic_df[\"Document\"] = pd.to_numeric(documents_topic_df[\"Document\"])\n\n    return documents_topic_df","ba85bc09":"%%time\ndocuments_topic_df = topic_all_documents(\n    lda_model=lda_model, corpus=bow_corpus, texts=docs_bigram_tokens\n)","5debcc25":"documents_topic_df.head(5)","aa3d6006":"most_representative_df = pd.DataFrame()\ndomiant_topic_df = documents_topic_df.groupby(\"Dominant Topic\")\n\nfor i, grp in domiant_topic_df:\n    most_representative_df = pd.concat(\n        [\n            most_representative_df,\n            grp.sort_values([\"Topic Score\"], ascending=False).head(1),\n        ],\n        axis=0,\n    )\n\nmost_representative_df.reset_index(drop=True, inplace=True)\n\nmost_representative_df = most_representative_df.iloc[:, 0:4]\n\nmost_representative_df.columns = [\n    \"Document\",\n    \"Topic_Num\",\n    \"Best Topic Score\",\n    \"Word List\",\n]","7a94d37c":"most_representative_df.style.background_gradient(cmap='viridis')","f0898591":"def topic_distribution(lda_model, documents_topic_df):\n    \"\"\"\n    \n    \"\"\"\n    topic_avg = documents_topic_df.groupby(['Dominant Topic'])[\"Topic Score\"].mean()\n    topic_count = documents_topic_df.groupby(['Dominant Topic'])['Document'].count()\n\n    topic_df = pd.DataFrame()\n    topic_df['Average'] = topic_avg\n    topic_df['Count'] = topic_count\n    topic_df = topic_df.reset_index()\n    \n    topic_df = topic_df.fillna(0)\n\n    topic_df.plot.bar(x='Dominant Topic', y='Count', rot=0)\n\n    return topic_df","0e400ecf":"topic_distibution_res = topic_distribution(lda_model, documents_topic_df)","6fb935f4":"topic_distibution_res.style.background_gradient(cmap='viridis')","d3e7668c":"cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  \n\ncloud = WordCloud(prefer_horizontal=1.0)\n\ntopics = lda_model.show_topics(formatted=False)\n\nfig, axes = plt.subplots(4, 2, figsize=(10,10), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n    plt.gca().axis('off')\n\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","58f45dc0":"def plot_tsne(lda_model):    \n    \"\"\"\n    Plot Documents Clusters based on topics scors\n    \"\"\"\n    topic_score = documents_topic_df.iloc[:,4: 4 + lda_model.num_topics]\n    \n    topic_num = np.array(documents_topic_df.iloc[:,1]).astype(int)\n\n    # tSNE Dimension Reduction\n    tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n    tsne_lda = tsne_model.fit_transform(topic_score)\n\n    # Plot the Topic Clusters using Bokeh\n    output_notebook()\n    n_topics = lda_model.num_topics\n    mycolors = np.array([color for name, color in mcolors.CSS4_COLORS.items()])\n    \n    plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n                  plot_width=900, plot_height=700)\n    plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num + 5])\n    show(plot)","b73c3482":"plot_tsne(lda_model)","2e064cf1":"def prediction_unseen_doc(lda_model, doc, threshold=0.1):\n    \"\"\"\n    Get the most representative topic of a new documment\n    \"\"\"\n    doc_preprocessed = doc.split()\n    doc_tokens = bigram_mod[doc_preprocessed]\n    bow_tokens = id2word.doc2bow(doc_tokens)\n\n    rows = []\n    for i, score in sorted(\n        lda_model.get_document_topics(bow_tokens), key=lambda x: x[1], reverse=True\n    ):\n        if score > threshold:\n            words, _ = zip(*lda_model.show_topic(i))\n            rows.append([f\"Topic_{i+1}\", score, \"; \".join(words)])\n            break\n\n    return pd.DataFrame(rows, columns=[\"Topic\", \"Score\", \"Words\"])","9e0908c6":"def document_same_topic(df_topic, documents_topic_df, df_merged):\n    \"\"\"\n    Obtain documents that have the same topic as df_topic\n    \"\"\"\n\n    for index, row in df_topic.iterrows():\n        topic = int(row[\"Topic\"].split(\"_\")[-1])\n\n        doc_same_topic = list(\n            documents_topic_df[documents_topic_df[\"Dominant Topic\"] == topic][\n                \"Document\"\n            ]\n        )\n\n        doc_detail = df_merged.loc[doc_same_topic]\n\n    return doc_detail","3688969d":"# Define the queries\nQUERY1 = '''\nData on potential risks factors\nSmoking, pre-existing pulmonary disease\nCo-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities\nNeonates and pregnant women\nSocio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n'''\nQUERY2 = '''\nTransmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n'''\nQUERY3 = '''\nSeverity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n'''\nQUERY4 = '''\nSusceptibility of populations\n'''\nQUERY5 = '''\nPublic health mitigation measures that could be effective for control\n'''","ee2a2125":"# Preprocess the queries\nquery_1_preprocessed = preprocessor.preprocess(QUERY1)\nquery_2_preprocessed = preprocessor.preprocess(QUERY2)\nquery_3_preprocessed = preprocessor.preprocess(QUERY3)\nquery_4_preprocessed = preprocessor.preprocess(QUERY4)\nquery_5_preprocessed = preprocessor.preprocess(QUERY5)","b7aea246":"unseen_doc_q1 = prediction_unseen_doc(lda_model=lda_model,doc=query_1_preprocessed)\nunseen_doc_same_topic_q1 = document_same_topic(unseen_doc_q1, documents_topic_df, df_merged).head(10)","c3a6b138":"unseen_doc_q1.style.background_gradient(cmap='viridis')","0e40a686":"unseen_doc_same_topic_q1.head()","da3e8919":"unseen_doc_q2 = prediction_unseen_doc(lda_model=lda_model,doc=query_2_preprocessed)\nunseen_doc_same_topic_q2 = document_same_topic(unseen_doc_q2, documents_topic_df, df_merged).head(10)","6a5963dd":"unseen_doc_q2.style.background_gradient(cmap='viridis')","d0615a3d":"unseen_doc_same_topic_q2.head()","862a9b5c":"unseen_doc_q3 = prediction_unseen_doc(lda_model=lda_model,doc=query_3_preprocessed)\nunseen_doc_same_topic_q3 = document_same_topic(unseen_doc_q3, documents_topic_df, df_merged).head(10)","b9b87c01":"unseen_doc_q3.style.background_gradient(cmap='viridis')","42a08e63":"unseen_doc_same_topic_q3.head()","3efdf40b":"unseen_doc_q4 = prediction_unseen_doc(lda_model=lda_model,doc=query_4_preprocessed)\nunseen_doc_same_topic_q4 = document_same_topic(unseen_doc_q4, documents_topic_df, df_merged).head(10)","b71824b3":"unseen_doc_q4.style.background_gradient(cmap='viridis')","609ee704":"unseen_doc_same_topic_q4.head()","ff57d398":"unseen_doc_q5 = prediction_unseen_doc(lda_model=lda_model,doc=query_5_preprocessed)\nunseen_doc_same_topic_q5 = document_same_topic(unseen_doc_q5, documents_topic_df, df_merged).head(10)","68b11629":"unseen_doc_q5.style.background_gradient(cmap='viridis')","a6b6be19":"unseen_doc_same_topic_q5.head()","b5f4c202":"GLOVE_TRAIN_MODE = False","e75855fe":"# Create a Glove object which will use the corpus matrix created above lines to create embeddings\nglove = Glove(no_components=300, learning_rate=0.05)\nif GLOVE_TRAIN_MODE:\n    # Creating a corpus object\n    corpus = Corpus()\n\n    # Fit the corpus with a list of tokens\n    corpus.fit(docs_tokens, window=10)\n\n    # Fit glove embeddings and add dict to it\n    glove.fit(corpus.matrix, epochs=30, no_threads=30, verbose=True)\n    glove.add_dictionary(corpus.dictionary)\n    \n    # Save obtained model\n    glove.save('opencovid_glove_300d.model')","53ab0dce":"def most_similar(v, *ignore, n=1):\n    \"\"\"\n    Get most similar words using words embeddings and Glove vector matrix \n    \"\"\"\n    similar = []\n    for word, u in vectors.items():\n        if word in ignore:\n            continue\n        similarity = u.dot(v)\n        if len(similar) < n:\n            heapq.heappush(similar, (similarity, word))\n        else:\n            heapq.heappushpop(similar, (similarity, word))\n\n    return sorted(similar, reverse=True)","4884acc1":"def plot_wordsl(words, lines=False):\n    \"\"\"\n    Plot simple words relations with Glove embeddings\n    \"\"\"\n    BW = \"\\x1b[1;30;45m\"\n    EEND = \"\\x1b[0m\"\n\n    wwl = []\n    for ww in words:\n        if ww in vectors:\n            wwl.append(ww)\n        else:\n            print(\n                BW,\n                \"*** WARNING ***** the word \",\n                ww,\n                \"is not in the embedding vectors\",\n                EEND,\n            )\n\n    words = wwl\n    pca = PCA(n_components=2)\n    xys = pca.fit_transform([vectors[w] for w in words])\n\n    if lines:\n        for i in range(0, len(words), 2):\n            plt.plot(xys[i : i + 2, 0], xys[i : i + 2, 1])\n    else:\n        plt.scatter(*xys.T)\n\n    for word, xy in zip(words, xys):\n        plt.annotate(word, xy, fontsize=15)\n\n    return pca","125e4482":"# If load from saved copy\nglove = Glove.load('\/kaggle\/input\/assystem-opencovid19-helpers\/opencovid_glove_300d.model')","b9fae5ee":"# Check embeddings dimensions\nlen(glove.word_vectors[glove.dictionary['virus']])","18111d31":"# Check number of words contained in dict\nlen(glove.dictionary)","aa57de96":"# Put in a dictionary to easily get embeddings\nvectors = {word: glove.word_vectors[glove.dictionary[word]] for word in glove.dictionary.keys()}\nlen(vectors)","b51a6190":"print(vectors['virus'])","de277090":"# Test it 1!\nxxll = most_similar(vectors['covid19'], n=10)\nnear_w = [x[1] for x in xxll]\nnear_w","d0cfc0a9":"# Test it 2!\nxxll = most_similar(vectors['pneumonia'], n=10)\nnear_w = [x[1] for x in xxll]\nnear_w","b14a65ca":"plt.title('Words relations using Glove', fontsize=20)\nplot_wordsl(['pneumonia',\n 'severe',\n 'respiratory',\n 'bacterial',\n 'patient',\n 'acute',\n 'child',\n 'bronchiolitis',\n 'interstitial',\n 'cause'], lines=True)","9ffdffef":"@numba.jit(target=\"cpu\", nopython=True, parallel=True, fastmath=True)\ndef fast_cosine(u, v):\n    \"\"\"\n    Compute cosine distance between two matrices\n    \"\"\"\n    m = u.shape[0]\n    udotv = 0\n    u_norm = 0\n    v_norm = 0\n    for i in range(m):\n        if (np.isnan(u[i])) or (np.isnan(v[i])):\n            continue\n\n        udotv += u[i] * v[i]\n        u_norm += u[i] * u[i]\n        v_norm += v[i] * v[i]\n\n    u_norm = np.sqrt(u_norm)\n    v_norm = np.sqrt(v_norm)\n\n    if (u_norm == 0) or (v_norm == 0):\n        ratio = 0.0\n    else:\n        ratio = udotv \/ (u_norm * v_norm)\n    return ratio","da9cf7b0":"@numba.jit(target=\"cpu\", nopython=True, parallel=True, fastmath=True)\ndef fast_get_keyWordsMatching_compute(Xtf1, Xtf2):\n    \"\"\"\n    Compute multiplication element by element of vectors and return index of non negative value after multiplication\n    \"\"\"\n    pointwiseMultiplication = np.multiply(Xtf1, Xtf2)\n    index = pointwiseMultiplication.nonzero()[1]\n\n    return pointwiseMultiplication, index","a65d544f":"def fast_get_keyWordsMatching(Xtf1, Xtf2, tf):\n    \"\"\"\n    Retrieve keywords responsible of the matching score\n    \"\"\"\n    pointwiseMultiplication, index = fast_get_keyWordsMatching_compute(\n        Xtf1.toarray(), Xtf2.toarray()\n    )\n    cross = []\n    for i in index:\n        # take the word from the dictionary that corrisponds to the index\n        key = list(tf.vocabulary_.keys())[list(tf.vocabulary_.values()).index(i)]\n        # take the value of the multiplication\n        value = pointwiseMultiplication[0, i]\n        cross.append((key, value))\n\n    return sorted(cross, key=lambda x: -x[1])  # sort output","5d2ae495":"def get_matching(df_merged, xtfidf_papers, xtfidf_query):\n    \"\"\"\n    Compute scores for documents\n    \"\"\"\n    score_overall_papers = []\n    keywords_papers = []\n    for i in prange(df_merged.shape[0]):\n        score_overall_papers.append(\n            fast_cosine(xtfidf_papers[i].toarray()[0], xtfidf_query.toarray()[0])\n        )\n        keywords_papers.append(\n            fast_get_keyWordsMatching(xtfidf_papers[i], xtfidf_query, tfidf)\n        )\n\n    return score_overall_papers, keywords_papers","02687dbb":"# Query expansion 1 with Glove\nquery_1_expanded_l = []\nfor ww in query_1_preprocessed.split():\n    if ww not in ['neonates']:\n        xxll = most_similar(vectors[ww], n=3)\n        near_w = [x[1] for x in xxll]\n        query_1_expanded_l += near_w\nquery_1_expanded = \" \".join(query_1_expanded_l)","fa4ddea5":"query_1_preprocessed, query_1_expanded","3bd0d4a4":"# Query expansion 2 with Glove\nquery_2_expanded_l = []\nfor ww in query_2_preprocessed.split():\n    xxll = most_similar(vectors[ww], n=3)\n    near_w = [x[1] for x in xxll]\n    query_2_expanded_l += near_w\nquery_2_expanded = \" \".join(query_2_expanded_l)","7ca12ef8":"query_2_preprocessed, query_2_expanded","cb63795a":"# Query expansion 3 with Glove\nquery_3_expanded_l = []\nfor ww in query_3_preprocessed.split():\n    xxll = most_similar(vectors[ww], n=3)\n    near_w = [x[1] for x in xxll]\n    query_3_expanded_l += near_w\nquery_3_expanded = \" \".join(query_3_expanded_l)","f9c92dc3":"query_3_preprocessed, query_3_expanded","68da4647":"# Query expansion 4 with Glove\nquery_4_expanded_l = []\nfor ww in query_4_preprocessed.split():\n    xxll = most_similar(vectors[ww], n=3)\n    near_w = [x[1] for x in xxll]\n    query_4_expanded_l += near_w\nquery_4_expanded = \" \".join(query_4_expanded_l)","0dbcbe0e":"query_4_preprocessed, query_4_expanded","4cc713b9":"# Query expansion 5 with Glove\nquery_5_expanded_l = []\nfor ww in query_5_preprocessed.split():\n    xxll = most_similar(vectors[ww], n=3)\n    near_w = [x[1] for x in xxll]\n    query_5_expanded_l += near_w\nquery_5_expanded = \" \".join(query_5_expanded_l)","f7b4fe78":"query_5_preprocessed, query_5_expanded","fa8101d4":"# Initialize TfIdfVectorizer object\ntfidf = TfidfVectorizer(\n        ngram_range=(1, 1)\n)","d30d5c27":"# Fit TfIdf model\ntfidf.fit(df_merged[\"title_abstract_text_preprocessed\"])","2e18ab82":"# Get sparse matrices for papers \nxtfidf_papers = tfidf.transform(df_merged[\"title_abstract_text_preprocessed\"])","eef8a2d2":"# Get sparse matrices for queries\nxtfidf_query_1 = tfidf.transform([query_1_expanded])\nxtfidf_query_2 = tfidf.transform([query_2_expanded])\nxtfidf_query_3 = tfidf.transform([query_3_expanded])\nxtfidf_query_4 = tfidf.transform([query_4_expanded])\nxtfidf_query_5 = tfidf.transform([query_5_expanded])","77920dd1":"# Get matching scores and keywords that matched for queries\nscores_1, keywords_1 = get_matching(df_merged, xtfidf_papers, xtfidf_query_1)\nscores_2, keywords_2 = get_matching(df_merged, xtfidf_papers, xtfidf_query_2)\nscores_3, keywords_3 = get_matching(df_merged, xtfidf_papers, xtfidf_query_3)\nscores_4, keywords_4 = get_matching(df_merged, xtfidf_papers, xtfidf_query_4)\nscores_5, keywords_5 = get_matching(df_merged, xtfidf_papers, xtfidf_query_5)","e446d495":"df_merged_q1 = df_merged[['paper_id', 'title']].copy()\ndf_merged_q2 = df_merged[['paper_id', 'title']].copy()\ndf_merged_q3 = df_merged[['paper_id', 'title']].copy()\ndf_merged_q4 = df_merged[['paper_id', 'title']].copy()\ndf_merged_q5 = df_merged[['paper_id', 'title']].copy()","406f722e":"df_merged_q1['score'] = scores_1\ndf_merged_q2['score'] = scores_2\ndf_merged_q3['score'] = scores_3\ndf_merged_q4['score'] = scores_4\ndf_merged_q5['score'] = scores_5\ndf_merged_q1['keywords'] = [[x[0] for x in r][:2] for r in keywords_1]\ndf_merged_q2['keywords'] = [[x[0] for x in r][:2] for r in keywords_2]\ndf_merged_q3['keywords'] = [[x[0] for x in r][:2] for r in keywords_3]\ndf_merged_q4['keywords'] = [[x[0] for x in r][:2] for r in keywords_4]\ndf_merged_q5['keywords'] = [[x[0] for x in r][:2] for r in keywords_5]","24b5d22e":"df_merged_q1 = df_merged_q1.sort_values(by='score', ascending=False).reset_index(drop=True)\ndf_merged_q2 = df_merged_q2.sort_values(by='score', ascending=False).reset_index(drop=True)\ndf_merged_q3 = df_merged_q3.sort_values(by='score', ascending=False).reset_index(drop=True)\ndf_merged_q4 = df_merged_q4.sort_values(by='score', ascending=False).reset_index(drop=True)\ndf_merged_q5 = df_merged_q5.sort_values(by='score', ascending=False).reset_index(drop=True)","726838bd":"# Cut too long title for pretty printing\ndf_merged_q1['title'] = df_merged_q1['title'].apply(lambda x: \" \".join(x.split()[:10])+\"...\")\ndf_merged_q2['title'] = df_merged_q2['title'].apply(lambda x: \" \".join(x.split()[:10])+\"...\")\ndf_merged_q3['title'] = df_merged_q3['title'].apply(lambda x: \" \".join(x.split()[:10])+\"...\")\ndf_merged_q4['title'] = df_merged_q4['title'].apply(lambda x: \" \".join(x.split()[:10])+\"...\")\ndf_merged_q5['title'] = df_merged_q5['title'].apply(lambda x: \" \".join(x.split()[:10])+\"...\")","e6f76621":"df_merged_q1.head(10).style.background_gradient(cmap='viridis')","d8cd4baa":"df_merged_q2.head(10).style.background_gradient(cmap='viridis')","766dedac":"df_merged_q3.head(10).style.background_gradient(cmap='viridis')","c45cfb05":"df_merged_q4.head(10).style.background_gradient(cmap='viridis')","01bc7738":"df_merged_q5.head(10).style.background_gradient(cmap='viridis')","316aefdd":"QUERY_NEW = \"Air pollution, arthritis\"\nquery_new_preprocessed = preprocessor.preprocess(QUERY_NEW)","9fd67afc":"# Query expansion 4 with Glove\nquery_new_expanded_l = []\nfor ww in query_new_preprocessed.split():\n    xxll = most_similar(vectors[ww], n=3)\n    near_w = [x[1] for x in xxll]\n    query_new_expanded_l += near_w\nquery_new_expanded = \" \".join(query_new_expanded_l)","0c68d3cb":"query_new_preprocessed, query_new_expanded","8f963314":"xtfidf_query_new = tfidf.transform([query_new_expanded])","cc051dbf":"scores_new, keywords_new = get_matching(df_merged, xtfidf_papers, xtfidf_query_new)","cc92fb5d":"df_merged_qnew = df_merged[['paper_id', 'title']].copy()\ndf_merged_qnew['score'] = scores_new\ndf_merged_qnew['keywords'] = [[x[0] for x in r][:2] for r in keywords_new]","416f2d90":"df_merged_qnew = df_merged_qnew.sort_values(by='score', ascending=False).reset_index(drop=True)\ndf_merged_qnew['title'] = df_merged_qnew['title'].apply(lambda x: \" \".join(x.split()[:10])+\"...\")","32bdb5d9":"df_merged_qnew.head(10).style.background_gradient(cmap='viridis')","5d07f171":"def check_cite(df, dict_merged_title_bib, citations):\n    \"\"\"\n    Generate graph nodes and edges\n    \"\"\"\n    cite_frame = set()\n    for cite in df['bibliography']:\n        for dict_paper in dict_merged_title_bib:\n            processed_title = dict_paper['title']\n            if cite and processed_title:\n                try:\n                    if distance.get_jaro_distance(processed_title, cite, winkler=True, scaling=0.1) > 0.88:\n                        citations.add((df['paper_id'],dict_paper['paper_id']))\n                        cite_frame.add((df['paper_id'],dict_paper['paper_id']))\n                except:\n                    break\n    return cite_frame","c4767d2e":"def clean_for_graphs(text):\n    \"\"\"\n    Light prepossessing steps that are used to preprocess text for graph generation\n    \"\"\"\n    # convert string to upper case\n    text = text.lower()\n    # prepare regex for char filtering\n    re_print = re.compile(\"[^%s]\" % re.escape(string.printable))\n    # prepare translation table for removing punctuation\n    table = str.maketrans(\"\", \"\", string.punctuation)\n    # normalize unicode characters\n    text = normalize(\"NFD\", text).encode(\"ascii\", \"ignore\")\n    text = text.decode(\"UTF-8\")\n    # tokenize on white space\n    words = text.split()\n    # remove punctuation from each token\n    words = [w.translate(table) for w in words]\n    # remove non-printable chars form each token\n    words = [re_print.sub(\"\", w) for w in words]\n    new_text = \" \".join(words)\n\n    return new_text","a502041a":"GRAPH_TRAIN_MODE = False","dea96c25":"df_merged_title_bib =  df_merged[['paper_id','title','bibliography']].copy()\nif GRAPH_TRAIN_MODE:\n\n    pandarallel.initialize(nb_workers=cpu_count()-1, progress_bar=True)\n    \n    df_merged_title_bib['bibliography'] = df_merged_title_bib['bibliography'].apply(lambda x: x.split(';'))\n    \n    dict_merged_title_bib = df_merged_title_bib.to_dict('records')\n    df_merged_title_bib['title'] = df_merged_title_bib['title'].apply(clean_for_graphs)\n    df_merged_title_bib['bibliography'] = df_merged_title_bib['bibliography'].apply(lambda x: [clean_for_graphs(cite) for cite in x])\n    citations = set()\n    df_merged_title_bib['cite'] = df_merged_title_bib.parallel_apply(check_cite, args=(dict_merged_title_bib, citations), axis=1)","7dc8137a":"# Load from Assystem-OpenCovid-Helpers Dataset if needed\ndf_merged_title_bib = pd.read_pickle('\/kaggle\/input\/assystem-opencovid19-helpers\/opencovid_df_bib_graph_1000.pkl')","aaffdd1f":"# Get paper_id --> title matrix for prettyprinting\npaper_id_to_title = df_merged_title_bib[['paper_id', 'title']].groupby('paper_id')['title'].apply(list).to_dict()\npaper_id_to_title = {k:v[0] for k, v in paper_id_to_title.items()}","6ec47a90":"# Drop empty links\ncitations = list(set([list(i)[0] for i in list(df_merged_title_bib['cite']) if len(i)>0]))","de54962d":"# Create a Directed graph and add edges to it\nDG=nx.DiGraph()\nDG.add_edges_from(citations, color='red')","e5c174a3":"# Generate positions and insert to nodes attrs\npos = nx.random_layout(DG) \nfor n, p in pos.items():\n    DG.nodes[n]['pos'] = p.tolist()","88de4dcc":"edge_x = []\nedge_y = []\nfor edge in DG.edges():\n    x0, y0 = DG.nodes[edge[0]]['pos']\n    x1, y1 = DG.nodes[edge[1]]['pos']\n    edge_x.append(x0)\n    edge_x.append(x1)\n    edge_x.append(None)\n    edge_y.append(y0)\n    edge_y.append(y1)\n    edge_y.append(None)","49200780":"edge_trace = go.Scatter(\n    x=edge_x, y=edge_y,\n    line=dict(width=0.5, color='#888'),\n    hoverinfo='none',\n    mode='lines')\n\nnode_x = []\nnode_y = []\nfor node in DG.nodes():\n    x, y = DG.nodes[node]['pos']\n    node_x.append(x)\n    node_y.append(y)\n\nnode_trace = go.Scatter(\n    x=node_x, y=node_y,\n    mode='markers',\n    hoverinfo='text',\n    marker=dict(\n        showscale=True,\n        colorscale='Viridis',\n        reversescale=True,\n        color=[],\n        size=13,\n        colorbar=dict(\n            thickness=15,\n            title='Node Connections',\n            xanchor='left',\n            titleside='right'\n        ),\n        line_width=2))","303cabb0":"node_adjacencies = []\nnode_text = []\nfor node, adjacencies in dict(DG.degree()).items():\n    node_adjacencies.append(adjacencies)\n    node_text.append(f\"Paper ID: {node}<br>Title: {paper_id_to_title[node]}<br>Nb of connections: {str(adjacencies)}\")\n\nnode_trace.marker.color = node_adjacencies\nnode_trace.text = node_text","da73628a":"fig = go.Figure(data=[edge_trace, node_trace],\n                layout=go.Layout(\n                title='Network graph of a subset of COVID19 documents dataset (1000 samples)',\n                titlefont_size=10,\n                showlegend=False,\n                hovermode='closest',\n                margin=dict(b=20,l=5,r=5,t=40),\n                annotations=[ dict(\n                    text=\"COVID-19 Open Research Challenge\",\n                    showarrow=False,\n                    xref=\"paper\", yref=\"paper\",\n                    x=0.005, y=-0.002)],\n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n                )\nfig.show()","3bcfb11d":"dict_degree = dict(DG.degree)\n\nfig = plt.gcf()\n# fig.set_size_inches(20, 20)\npos = nx.layout.spring_layout(DG)\nnode_sizes = [v * 40 for v in dict_degree.values()]\nM = DG.number_of_edges()\nnodes = nx.draw_networkx_nodes(DG, pos, node_size=node_sizes, node_color='red')\nedges = nx.draw_networkx_edges(DG, pos, node_size=node_sizes, arrowstyle='->',\n                               edge_cmap=plt.cm.Blues)\n\nplt.show()\n# fig.savefig('citations_graph.png', dpi=100)","3b7bb5d8":"dict(list(dict(DG.in_degree).items())[:5])","e7776763":"sorted(DG.in_degree, key=lambda x: x[1], reverse=True)[:5]","9cf0af33":"dict(list(dict(DG.out_degree).items())[:5])","6730cd22":"sorted(DG.out_degree, key=lambda x: x[1], reverse=True)[:5]","b763f79d":"fig = plt.gcf()\n# fig.set_size_inches(24, 18)\npos = nx.layout.spring_layout(DG)\n\ndict_in_degree = dict(DG.in_degree)\ndict_out_degree = dict(DG.out_degree)\nnode_sizes_in = [v * 40 for v in dict_in_degree.values()]\nnode_sizes_out = [v * 40 for v in dict_out_degree.values()]\nM = DG.number_of_edges()\nnodes = nx.draw_networkx_nodes(DG, pos, node_size=node_sizes_in, node_color='red')\n\nnodes = nx.draw_networkx_nodes(DG, pos, node_size=node_sizes_out, node_color='blue')\nedges = nx.draw_networkx_edges(DG, pos, node_size=node_sizes, arrowstyle='->',\n                               edge_cmap=plt.cm.Blues)\n\nplt.show()\n# fig.savefig('citations_graph_in_out_degree.png', dpi=100)","2819d184":"fig = plt.gcf()\n# fig.set_size_inches(24, 18)\npos = nx.layout.spring_layout(DG)\n\ndict_in_degree = dict(DG.in_degree)\ndict_out_degree = dict(DG.out_degree)\nnode_sizes_in = [v * 40 for v in dict_in_degree.values()]\nnode_sizes_out = [v * 40 for v in dict_out_degree.values()]\nM = DG.number_of_edges()\n\nGcc = sorted(nx.weakly_connected_components(DG), key=len, reverse=True)\nG_connected = DG.subgraph(Gcc[0])\n\nnode_connected = G_connected.nodes\nedges_connected = G_connected.edges\n\nnode_unconnected =  DG.nodes - G_connected.nodes\nedges_unconnected = DG.edges - G_connected.edges\nG_unconnected=nx.DiGraph()\nG_unconnected.add_edges_from(edges_unconnected, color='red')\n\n\ndict_connected = dict(G_connected.degree)\ndict_unconnected = dict(G_unconnected.degree)\nnode_sizes_connected = [v * 40 for v in dict_connected.values()]\nnode_sizes_unconnected = [v * 40 for v in dict_unconnected.values()]\n\nnodes_connected_draw = nx.draw_networkx_nodes(G_connected, pos, node_color='red',node_size=node_sizes_connected )\nedges_connected_draw = nx.draw_networkx_edges(G_connected, pos, node_size=node_sizes_connected,\n                       arrowstyle='->',\n                        edge_color= 'red',\n                        edge_cmap=plt.cm.Blues\n                      )\n\nnodes_unconnected_draw = nx.draw_networkx_nodes(G_unconnected, pos, node_color='blue',node_size=node_sizes_unconnected)\nedges_unconnected_draw = nx.draw_networkx_edges(G_unconnected, pos, node_size=node_sizes_unconnected,\n                       arrowstyle='->',\n                       edge_color= 'blue',                        \n                       edge_cmap=plt.cm.Blues\n                      )\n\nplt.show()\n# fig.savefig('connected_components_graph.png', dpi=100)","4c3c9b7d":"list(nx.weakly_connected_components(DG))[:1]","47ec2eb5":"sorted(list(nx.weakly_connected_components(DG)), key=len, reverse=True)[:1]","53bd9026":"list(nx.strongly_connected_components(DG))[:5]","c73e0c52":"sorted(list(nx.strongly_connected_components(DG)), key=len, reverse=True)[:5]","699aa2e2":"def build_dict_node(df):\n    \"\"\"\n    Build a dictionary mapping each paper id to an index in \n    [0..num_of_paper - 1]\n    \"\"\"\n    dict_node = dict()\n    for i in range(len(df['paper_id'])):\n        dict_node[df['paper_id'].iloc[i]] = i\n    return dict_node","1e592e26":"sub_graph = ['09b322d7bbb2bec7d12d4fb16d8c397118e33dad',\n  '22c4892020511ff70f17fbaec468003b9287462e',\n  '253af824466307bf9415fd7419f6029dc9180939',\n  '8dbeac1f0bb7e99bc67744546f62dd4b199646cc',\n  '93478a74599203ababc690436e54bb3ceb780061',\n  'c715062c7f223c58fb278fb9470f084ec0b4f944']\ndf_merged_picked_6 = pd.read_pickle('\/kaggle\/input\/assystem-opencovid19-helpers\/df_merged_title_bib_picked_6.pkl')","0c06e995":"# The edges\nlist(DG.edges(sub_graph))","05a46827":"nodes_sub_graph = list(sub_graph)","ead4afb2":"df_sub_graph =  df_merged_picked_6[df_merged_picked_6['paper_id'].isin(nodes_sub_graph)]","caa60069":"dict_sub_graph = build_dict_node(df_sub_graph)","528b15ef":"dict_sub_graph","73a858fe":"abstract = df_sub_graph['abstract']\nabstract_list = abstract.to_list()","a123d8c4":"tfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(abstract_list)","f250be70":"ddsim_matrix = cosine_similarity(tfidf_matrix[:], tfidf_matrix)","3f0486cd":"list_edges_sub_graph = list(DG.edges(list(sub_graph)))","06978634":"def weight_edges(edges,matrix_tfidf,dict_node):\n    \"\"\"\n    Generate weight edges\n    \"\"\"\n    weight_edges = []\n    for edge in edges:\n        list_edge = list(edge)\n        edge_start, edge_end = list_edge[:]\n        weight_edge = matrix_tfidf[dict_node[edge_start]][dict_node[edge_end]]\n        list_edge.append(weight_edge)\n        weight_edges.append(tuple(list_edge))\n    return weight_edges","f48a82bc":"list_edges_weighted_sub_graph = weight_edges(list_edges_sub_graph,ddsim_matrix,dict_sub_graph)","f97b1761":"list_edges_weighted_sub_graph","835895f8":"directed_sub_graph = nx.DiGraph()","2f47cc03":"directed_sub_graph.add_weighted_edges_from(list_edges_weighted_sub_graph)","14ef468f":"fig = plt.gcf()\nfig.set_size_inches(15, 15)\npos = nx.spring_layout(directed_sub_graph)  # positions for all nodes\nlabels = nx.get_edge_attributes(directed_sub_graph,'weight')\ndict_degree_sub_graph = dict(directed_sub_graph.degree)\nnode_sizes_sub_graph = [v * 80 for v in dict_degree_sub_graph.values()]\nnx.draw_networkx_nodes(directed_sub_graph, pos\n                       ,label=sub_graph, \n                       node_size=node_sizes_sub_graph, \n                       node_color=\"red\")\nnx.draw_networkx_edges(directed_sub_graph, pos, \n                       edgelist=list_edges_weighted_sub_graph,\n                       width=1)\n\nnx.draw_networkx_edge_labels(directed_sub_graph, pos, edge_labels=labels)\n\n\nnx.draw_networkx_labels(directed_sub_graph, pos, edfont_size=8, font_family='sans-serif')\nplt.axis('off')\nplt.show()\n# nx.draw_networkx_edge_labels(test_graph,pos=pos)","e1e901dc":"scores = np.random.random((1,6))[0].tolist()\n\ndf_sub_graph_id = df_sub_graph[['paper_id']]\ndf_sub_graph_id['score'] = scores ","16eaf21b":"df_sub_graph_id","b34b980a":"nx.in_degree_centrality(directed_sub_graph)","93b2a129":"degree_centrality = [ v for v in nx.in_degree_centrality(directed_sub_graph).values() ]","2bc8bd58":"df_sub_graph_id['degree_centrality'] = degree_centrality","b62ce19b":"df_sub_graph_id.style.background_gradient(cmap='viridis')","4a5ddd64":"df_sub_graph_id.set_index(\"paper_id\", drop=True, inplace=True)","1f221955":"dict_score_sub_graph = df_sub_graph_id.to_dict(orient=\"index\")","f15332bc":"for key in dict_score_sub_graph.keys():\n    dict_score_sub_graph[key]['neighbor'] = {}\n    dict_score_sub_graph[key]['neighbor'] = dict(directed_sub_graph[key])","da43672b":"def dict_new_score_sub_graph(dict_score_sub_graph):\n    for paper in dict_score_sub_graph:\n        new_score = dict_score_sub_graph[paper]['score'] * dict_score_sub_graph[paper]['degree_centrality']\n        for neighbor in dict_score_sub_graph[paper]['neighbor']:\n            new_score = new_score + dict_score_sub_graph[paper]['neighbor'][neighbor]['weight'] * dict_score_sub_graph[neighbor]['score'] * dict_score_sub_graph[neighbor]['degree_centrality'] \n        dict_score_sub_graph[paper]['new_score'] = new_score\n        del dict_score_sub_graph[paper]['neighbor']\n    return dict_score_sub_graph","a78a8301":"dict_new_score = dict_new_score_sub_graph(dict_score_sub_graph)","b8131975":"df_sub_graph_new_score = pd.DataFrame.from_dict(dict_new_score, orient='index')","c2e77c94":"df_sub_graph_new_score.style.background_gradient(cmap='viridis')","dd8cf9d5":"--------------","9bef780a":"### 3.2.4 Merge data (biorxiv and noncomm only)","f4f47751":"Our trained Glove model allow us to get words similar semantically to our query","99e606f5":"Generate edges with weight from tf-idf matrix similarity. Weight in here will be presented for the similarity of 2 papers that have the citation.","0d23db4c":"And let's try to search for **a possible new risk factor**","b0a8252c":"* Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors","8e2c939b":"Generate tf-idf matrix similarity of documents","11b10f0d":"<a id=\"section-four\"><\/a>\n# 1.3 Task details (taken from task definition)\nWhat do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\nSpecifically, we want to know what the literature reports about:\n* Data on potential risks factors\n    * Smoking, pre-existing pulmonary disease\n    * Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities\n    * Neonates and pregnant women\n    * Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n* Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n* Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n* Susceptibility of populations\n* Public health mitigation measures that could be effective for control","280cfb8f":"When we'll have a new article coming up, we'll be able to detect its topic automatically. This could allow us to filter out the articles that don't speak of COVID19 risk factors or the opposite, get a list of articles that also speak about a specific one or all risk factors.","9b5d53d0":"Sort paper based on the out-degree","c3183f2f":"### 3.4.4 Get topics for all documents","f5205aff":"### 3.4.5 The most representative document for each topic","1d62ed9b":"* Data on potential risks factors\n    * Smoking, pre-existing pulmonary disease\n    * Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities\n    * Neonates and pregnant women\n    * Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.","42858227":"Sort list of strongly_connected_components based on the number of node included.","a5e9a907":"![Pr%C3%A9sentation1.jpg](attachment:Pr%C3%A9sentation1.jpg)\n\n","e53aa1f1":"<a id=\"section-zero\"><\/a>\n# Contents\n* [Contents](#section-zero)\n* [1. Introduction](#section-one)\n    * [1.1. About](#section-two)\n    * [1.2. Contributors](#section-three)\n    * [1.3. Task details](#section-four)\n    * [1.4. Dataset description](#section-five)\n* [2. Methodology](#section-six)\n    * [2.1 Data Loading](#section-seven)\n    * [2.2 Preprocessing](#section-eight)\n    * [2.3 Topic Modelling](#section-nine)\n    * [2.4 Search Engine](#section-ten)\n    * [2.5 Graphs](#section-eleven)\n* [3. Implementation](#section-twelve)\n    * [3.1. Packages Import](#section-thirteen)\n    * [3.2. Data Import](#section-fourteen)\n    * [3.3. Data Preprocess](#section-fifteen)\n    * [3.4. Data exploration with Topic Modelling](#section-sixteen)\n    * [3.5. Search Engine](#section-seventeen)\n    * [3.6. Graphs](#section-eighteen)\n* [Citations](#section-nineteen)","d76d98f1":"### 3.5.3 Search Engine TfIDF","08f5c427":"* Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups","dede0502":"### 3.3.2 Initiate Preprocessor class and preprocess all the text ","48432b94":"Clustering of papers\n\nOn this plot, we are presenting the clusters that have highest number of papers (red), and others nodes (blue) which stand seperately. The blue nodes are the papers where the scientists are trying to work on a new topic, rather than the hot topics. ","cfca9180":"* Data on potential risks factors\n    * Smoking, pre-existing pulmonary disease\n    * Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities\n    * Neonates and pregnant women\n    * Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.","16533169":"Helper functions","3354daa7":"### 3.4.3 LDA using TF-IDF","a0102627":"### 3.2.3 Import & process the data","ea0c1f91":"Let's hand pick some random articles from full dataset","28590c37":"### 3.4.2 LDA with BOW","91d97bba":"Read metadata","a429ff71":"Plot weighted graph","4e11f845":"### 3.3.1 Define Preprocessor class\nThe preprocessor class uses simple logics to preprocess raw text data. We are mainly using SpaCy for it with 6 most used languages. Here are some main steps implemented:\n* language detection;\n* special characters deletion;\n* tokenization, lemmatization, stopwords deletion, filtering according to token lenght and some other;\n* deaccent (optional);\n* force ascii (optional, more strict than deaccent).","3341e77a":"Now let's apply graph theory on adjusting the score of search engine by considering the relationship of papers. In this experiment, we only take a subgraph from the original one which include 6 documents to reduce time processing.","6e74fafe":"![tfidf.PNG](attachment:tfidf.PNG)","d889492a":"### 3.4.1 Generation of matrices and dictionary","cb2193e2":"------------------","e79e7540":"### 3.4.6 Topic Distribution","e7a67f1a":"# COVID-19 Risk Factors Analysis","0bc2c866":"Now let's define our queries about risk factors and preprocess them. They will be used in the Search engine as well.","a1b03768":"* Susceptibility of populations","30df70c2":"Read Comm Data (if needed)","7214d29c":"Degree of a paper, why it's important ?\n\nIn a directed graph, there are 2 features we care which are :\n- in_degree : in_degree of a node is number of arrow point in this node\n- out_degree : out_degree of a node is number of arrow point out from this node \n\nWhen thinking about graph of papers, in_degree number could be understood as the number of citations that one paper \"take from\" other papers. It could be used to evaluate the diversity of the scope, the topic, the abstract of one paper.\n\nOn the other hand, out_degree number could be understood as the number of citations that one paper \"give to\" other papers. It could be used to evaluate the reputation and the quality of one paper. The paper that has more citations will have more valuable informations to answer the questions.","9e72118c":"As we can see, even on this subset of data, the topics are well distinguished:","eadbf9ce":"Read Noncomm Data","6c09e534":"Create citations nodes and edges. Takes a lot of time as high O complexity. Define a number of paper you want to simulates or just import a set of already generated links from Assystem-OpenCovid19-Helpers in section 3.6.2.","2567b0af":"Now let's find a topic for each query and show documents that share the same topic","b54a7a05":"<a id=\"section-sixteen\"><\/a>\n# 3.4 Data exploration with Topic Modelling","9a039d71":"* Public health mitigation measures that could be effective for control","aff0a9f0":"* Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups","ec660f46":"### 3.4.9 Prediction on unseen documents","24c12d52":"Finally, we can recalculate the score of Search engine based on the relationship of Directed weighed graph. In this experiment, I take random score as the result of search engine which mention in the previous section to compare with the result after recalculating","3cd476f6":"### 3.5.1 Train a Glove model","88e71610":"-----------","f2ca3a29":"### 3.6.2 Generate graph with NetworkX","cc4f1667":"Sort list of weakly_connected_components based on the number of node included","e62116d6":"Helper functions","6d311e45":"Sort paper based on the in-degree","644c2235":"-----------","d8a42748":"Helper functions","d47340d5":"On this plot, the higher citations one node have, the bigger the node is.","928f2e18":"------------","a979d921":"### 3.2.1 Define Datahandler class","b1529738":"### UPVOTE \ud83d\udc4d IF YOU FIND OUR SOLUTIONS INTERESTING\/USEFUL AND DON'T HESITATE TO CONTACT US \ud83d\udd8a FOR ANY QUESTIONS  ","d7f1e7b6":"### 3.4.7 Word Clouds of Top Keywords","26ded586":"<a id=\"section-fifteen\"><\/a>\n# 3.3 Data Preprocess","fa950e08":"Helper functions","c0b27035":"Read Biorxiv Data","fcec6258":"<a id=\"section-eighteen\"><\/a>\n# 3.6 Graphs","1124e663":"<a id=\"section-twelve\"><\/a>\n# 3. Implementation","6b5ad2da":"-----------","631d2ff2":"Generate nodes and edges","1cab60c5":"Helper functions","1add901e":"<a id=\"section-fourteen\"><\/a>\n# 3.2 Data import","c6be98a2":"* Public health mitigation measures that could be effective for control","b838a624":"### 3.6.1 Data Preparation","af64a560":"# Degree centrality\n\nDegree centrality starts with the assumption that the paper with the most connections (edges) is the most important. Rather than returning a count it is the degree of the node divided by the total possible number of edges that the node could have. For the case of the directed graph the degree of the incoming vertices and outgoing vertices would likely be treated separately.\n\n$$Centrality_{degree}(v)=\\frac{degree_v}{   \\left| N \\right|    -1}$$\n\n\nWhere N is the the graph and v is the vertex (node) that we are measuring.\n\n* Item A Degree Centrality of 1 means the node is directly connected to every other node\n* Item A Degree Centrality of 0 means the node isn't connected to any other node in the network\n\n\nBy combining the importance of each paper node and the weight of connection between nodes in the graph. The new score will be calculated as follows :\n\n$$newScoreNode = DegreeCentrality * scoreNode + \\sum_{}^{NeighborOfNode} weightOfEdgesConnectedToNeighbor * ScoreOfNeighbor * DegreeCentralityOfNeighbor$$   \n","83a44522":"pyLDAvis could help us to easily vizualize the topics. We include a non-interactive version here. If needed an interactive version, please execute the cell below the plot.","a1a87987":"On this plot, we present the in_degree and out_degree of graph. The red nodes are the papers that take citations from others papers while the blue nodes are the papers that give citations to others papers. Some papers could give citations and take citations simultaneously.\n\nAgain,the higher the degree, the bigger nodes are.","609b4bd8":"### 3.6.4 Plot a directed version","42c6a0c9":"* Susceptibility of populations","73402343":"pyLDAvis could help us to easily vizualize the topics. We include a non-interactive version here. If needed an interactive version, please execute the cell below the plot.","46295fe9":"<a id=\"section-five\"><\/a>\n# 1.4 Dataset Description (taken from challenge definition)\nIn response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 44,000 scholarly articles, including over 29,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.\n\n**We are using the V4 of this dataset (updated on 2020-03-20).**","a638310a":"Train your Glove model if needed","79810637":"<a id=\"section-one\"><\/a>\n# 1. Introduction\n<a id=\"section-two\"><\/a>\n# 1.1 About\nThe goal of this notebook is to understand how risk factors could influence the flow of disease for COVID19 (or possibly trigger it) and thus help the research community in better understanding of COVID19. What WHO says about COVID and coronavirus ?\n\n> **Coronaviruses** are a large family of viruses which may cause illness in animals or humans.  In humans, several coronaviruses are known to cause **respiratory infections** ranging from the common cold to more severe diseases such as Middle East Respiratory Syndrome (MERS) and Severe Acute Respiratory Syndrome (SARS). The most recently discovered coronavirus causes coronavirus disease **COVID-19**. Source: WHO\n\n> **COVID-19** is the infectious disease caused by the most recently discovered coronavirus. This new virus and disease were unknown before the outbreak began in Wuhan, China, in December 2019. Source: WHO\n\nAnd what's about the risk factors ?\n>The risk depends on where you  are - and more specifically, whether there is a COVID-19 outbreak unfolding there.\n**For most people in most locations the risk of catching COVID-19 is still low.** However, there are now places around the world (cities or areas) where the disease is spreading. For people living in, or visiting, these areas the risk of catching COVID-19 is higher. Governments and health authorities are taking vigorous action every time a new case of COVID-19 is identified. Be sure to comply with any local restrictions on travel, movement or large gatherings. Cooperating with disease control efforts will reduce your risk of catching or spreading COVID-19.\nCOVID-19 outbreaks can be contained and transmission stopped, as has been shown in China and some other countries. Unfortunately, new outbreaks can emerge rapidly. It\u2019s important to be aware of the situation where you are or intend to go. WHO publishes daily updates on the COVID-19 situation worldwide.\n\n> **While we are still learning about how COVID-2019 affects people, older persons and persons with pre-existing medical conditions (such as high blood pressure, heart disease, lung disease, cancer or diabetes)  appear to develop serious illness more often than others.**\n\nSo, as we can see, we still don't know a lot about this coronavirus and we are learning something new each day. And it's here that our goal is placed - we should help the community to learn more about it. The number of literature speaking of COVID19 is increasing each day nowadays. With this increasing number, it gets more and more difficult for scientists to get valuable informations out of scientific papers within a short period of time. **By helping the scientific community, we are saving lives.** The scientists are exploring different aspects of the virus and they need a tool to give them a hand. We propose a simple, but yet robust and complete set of approaches (solutions) to resolve this issue. In this submission, we are showing:\n1. How to properly centralize loading and preprocessing of large volumes of data by making use of developed methods and multiprocessing compute\n2. How to explore large quantities of data using topic modelling for extracting relevant topics out of raw text, automate the search of these topics, their visualisation and interpretation\n3. How to make use of word embeddings as a robust NLP techniques that will allow us to get similar words based on their distributions but also complex semantic relations between them.\n4. How to use word emebeddings for Query expansion to power search engines.\n5. How to build simple yet powerful search engines to get a list of relevant articles according to user's query.\n6. How to concieve complex graph representations of data to search for strong and weak connections between them but also cluster them based on their importance.\n\n**Our code is fully reproducible so don't hesitate to execute on your machine and experiment!** Plus, we are open for any feedback and suggestions that we will add to this kernel to make it live longer \ud83d\ude00\n<a id=\"section-three\"><\/a>\n# 1.2 Contributors\n### 1.2.1 Team members\nAll the team members are submitting under **Assystem**'s \ud83c\udfe2 company name.\nThe team members are:\n* Ali KABBADJ (@alikabbadj)\n* Kien DANG Trung (@luffy2106)\n* Zakaria BOUHOUN (@zakaidev)\n* Aleksei IANCHERUK (@alekseiiancheruk)\n\n### 1.2.2 A brief Assystem group description\nAs an independent engineering group, Assystem has over 50 years of experience providing industrial infrastructures with engineering services and managing projects that are complex in size, technological content, and safety requirements. Assystem supports its clients in the design, construction supervision, commissioning and operation of their industrial infrastructures, ensuring that their facilities are safe and effective, in line with their budget and schedule.As a major player in digital transformation, Assystem also supports its clients in their **digital projects with digital engineering and industrial data science** solutions to meet their operational performance challenges. Assystem currently employ 5,700 people working in fourteen countries in Europe, the Middle East, and Asia.","3e5859a3":"<a id=\"section-thirteen\"><\/a>\n# 3.1 Packages import","663ff6da":"<a id=\"section-seventeen\"><\/a>\n# 3.5 Search Engine","5ee1ea34":"Let's visualize the results !","7eb77b9c":"Helper fuctions","994ebf97":"<a id=\"section-six\"><\/a>\n# 2. Methodology\nOur goal is not only to answer to the questions of the challenge about risk factors mentioned above but also to find some new possible risk factors and to link them to the existing ones. We normally distinguish different types of risk factors such as:\n* **Behavioural: relate to actions that a person choose to take**\n    * Smoking\n    * Alcohol\n    * Nutritional problems\n    * Low activity\n    * Unprotected sex\n* **Physiological: relate to a person's body and are often influenced by genetics**\n    * High blood pressure\n    * High Blood sugar\n* **Demographic: relate to all population**\n    * Age\n    * Gender\n    * Income\n    * Social group\n    * Religion\n* **Environmental: relate to physical settings**\n    * Air pollution\n    * Access to water\n* **Genetic: diseases triggered by genes issues**\n    * Cancer\n    * Arthritis\n    * Different types of chromosomal abnormalities\n    \nLet's see now what are the common risk factors across the world according to WHO (WHO, Numbers of the 10 leading global risks for mortality (death), 2004):\n![image2.jpg](attachment:image2.jpg)\n\n\nOur approach for discovering risk factors for COVID19 is represented below:\n![COVID1.svg](attachment:COVID1.svg)\n\nYou will find more details below.\n\n<a id=\"section-seven\"><\/a>\n# 2.1 Data Loading\nWe are using standard Python packages for reading metadata and parsing the articles files. The metadata file is used as a reference and after the extraction of raw articles we merge them with this metadata file. As the compute power (RAM specifically) of a build-in Kaggle notebook is not high, we are limiting ourselves to loading of only a subset of documents. to be specific, we load:\n* Biorxiv\n* Non-comm\n\nPros:\n* Minimal dependancies\n* Modular (to parse new field just add a new method and modify dataframe builder)\n\nSome basic operations as drop of duplicated and fillna are also performed.\n\n<a id=\"section-eight\"><\/a>\n# 2.2 Preprocessing\n![image3.png](attachment:image.png)\nAt this step we are defining a Preprocessor class that enables us to effectively preprocess any raw text. Before passing to preprocessing itself, we also detect language of the text. We are using parallel execution to speed up the process. The steps of preprocessing are:\n1. Delete special characters\n2. Tokenize\n3. Lemmatize\n4. Delete stopwords\n5. Delete numbers tokens, tokens of url, emails, etc.\n6. Filter too short tokens (optional)\n7. Force ascii (optional)\n8. Deaccent (optional)\n\nPros:\n* Multilingual (6 languages supported)\n* Adjustuble\nCons:\n* Dependant on language data\n* Takes a big amount of RAM to load all models\n* Execution time (parallel executions helps here however)\n\n<a id=\"section-nine\"><\/a>\n# 2.3 Topic Modelling\nTopic Modelling will help us not to get a number of topics that are discussed in the documents. There are multiple methods of doing this but for this application we preferred to use **Latent Dirichlet Allocation (LDA)**.\n\nLDA is an unsupervised machine-learning model that takes documents as input and gives topics as output.\n\nSome pros of LDA approach:\n* Unsupervised: no need of labelling data\n* Fast: pretty fast to execute if well parametrized\n* Easy to implement: bag of words are a standard in NLP so easy to implement, also multiple implementations already exist (we are using gensim)\n* Easy prediction: once ready, it's easy to predict a topic for a new document\n\nCommon cons of LDA are pretty well known:\n* Difficult to tune: the number of topics k is fixed and we should define it ourselves\/pick it after multiple tryings, plus other parameters need a lot of handpicking\n* Sometimes slow: with big-sized dictionnaires, model creation can take a long time\n* Bag of words: the structure of the sentences is not captured, only word-level information\n* Interpretation: the result of topic modelling is not 100% clear and needs to be interpreted by a human expert\n\nWe are using two variants of LDA: LDA BOW and LDA TFIDF. We are generating multiple LDA models with different number of topics as a main hyper-parameter to then choose the best model based on the coherence and perplexity scores. We are visualizing then T-SNE 2D plot with topics obtained as long as topic distribution. As a final one, we use our model to infer the questions about risk factors to get the documents also speaking of risk factors.\n\n<a id=\"section-ten\"><\/a>\n# 2.4 Search Engine\nIn this part, we decided not to use modern language models despite their SOTA results in many benchmarks but stay somewhat in between classic search engines and semantic ones. Our apporoach is to perform a so-called Query expansion. Using this appoarch we are taking user's query and using a semantic model to add to it some similar words that might help in  the search.\n\nWe are using Glove as a semantic model. The advantage of it compared to Word2vec is that GloVe does not rely just on local statistics, but incorporates global statistics (word co-occurrence) to obtain word vectors. The expanded user query is then passed to a simple TFIDF-based search engine that computes query-to-document cosine similarity.\n\nSome pros:\n* Easy to build\n* Not hungry on ressource as doesn't require a GPU\n* Gives robust and stable results\n* Approach often used in closed-domain industrial search engines\n\nSome cons:\n* No Question-Answering model for now (can be easily added)\n* Glove embeddings should be trained on a large volume of data\n\n<a id=\"section-eleven\"><\/a>\n# 2.5 Graphs\nIt could be interesting to simulate the relations between all the papers and for this we decided to also add a graph apporoach. We present the relationships of scientific papers by graphs. Each node in the graph presents one paper, one directed edge from one node to another node presents for a citation between 2 papers. \n\nIn the reality, if one paper have high number of citations, it means that this paper is valuable in term of scientific contribution and thus attracts more scientists and has the most valuable information.\n\nAlso, a clustering could be performed on graphs. In graph theory or in the reality of science, when objects are in the same cluster, they tend to share the same interests, the orientation they want to follow, try to solve the big issues of science. Simply speaking, a cluster which has more papers than the others will represent the theme that many scientists want to discover.\n\nThere are 2 kinds of clustering :\n1. Weak connected components (the citations in a cluster tend to branching or go to variaty rather than deeper). In this case, the scientists are interested in the same big theme but they still didn't dig into the same specific problem)\n2. Strong connected components (the citations tend to go deeper and cyclic rather than go variety). Ex : Paper 1 -> paper 3 -> paper 6 -> paper 1. The papers in this kind of cluster have very strong relationship, and more importantly, the information in this papers is treasured (because the flow of knowledge come back to this paper again and again).\n\nPros:\n* Advanced insights from the papers\n* Papers clustering\n* Detection of weak and strong connections\n\nCons:\n* Graph generation and simulation takes a lot of time","ab960ec9":"--------------","c82effcc":"### 3.6.3 Plot an undirected version with Plotly","f7b0e42e":"* Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors","4011d062":"![bow.PNG](attachment:bow.PNG)","081398a7":"Helper functions","f490f692":"Read Custom License Data (if needed)","1565fa0e":"Pick the model with the highest coherence score","81fab0a6":"Now we can recalculate the score of search engine to see the difference","61af83a4":"Let's try to plot words relations from Glove. If we have names of risk factors, Glove could helps us to identify words (new risk factors, related diseases) that ofter co-occur or have the same meening but were lost in a big quantity of papers. ","94389ea7":"### 3.4.8 T-SNE plot of document clusters","04523003":"Choose best model","2f433e2c":"### 3.5.2 Get words embeddings","eef5e2c1":"### 3.2.2 Set paths to data and initiate DataHandler class"}}