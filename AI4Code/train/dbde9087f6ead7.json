{"cell_type":{"be901339":"code","9040d500":"code","81c1e31a":"code","e3c25927":"code","398911d3":"code","8410cf9d":"code","a14f4b15":"code","ad10d03f":"code","c7d0746b":"code","6c8d4d51":"code","ae805760":"code","3e76276f":"code","0af98afc":"code","1b328dcc":"code","14db2b00":"code","1ba50c3a":"code","a5e67583":"code","9190f0b1":"code","73920a77":"code","4f08d15f":"code","796923bd":"code","ce7a910e":"code","c2879290":"code","d10b37b8":"code","99d21b05":"code","81f77b2a":"markdown","7c966c6d":"markdown","41d6a9fe":"markdown","8ada0369":"markdown"},"source":{"be901339":"import pandas as pd\nimport keras as keras\nimport numpy as np\nimport sklearn.model_selection as skm\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport timeit\nfinal_models = {\"keras1\":\"\", \"keras2\":\"\"}","9040d500":"def plot_y_true_vs_y_pred(x, y_true, y_pred, idx):\n    pred_data = {\"SNo\": list(idx), \"Y_true\": y_true, \"Y_pred\": y_pred}\n    x = list(x)\n    pred_data[\"X\"] = x\n    pred_data[\"Correct\"] = np.equal(pred_data[\"Y_true\"], pred_data[\"Y_pred\"] )\n    df_pred = pd.DataFrame(pred_data)\n    df_pred_summary = df_pred.groupby([\"Correct\"]).count()\n    print(df_pred_summary.head(10))\n    num_sample_rows = 50\n    #df_pred_fail = df_pred[df_pred[\"Y_true\"]!=df_pred[\"Y_pred\"]]\n    #df_pred_fail = df_pred_fail.head(num_sample_rows)\n    df_pred = df_pred[df_pred[\"Y_true\"]!=df_pred[\"Y_pred\"]]\n    #df_pred = df_pred.sample(num_sample_rows)\n    #df_pred = pd.concat([df_pred, df_pred_fail], axis=0)\n    i = 1\n    fig = plt.figure(figsize=(15,20))\n    fig.suptitle(\"True Value vs Predicted Value\", size=20)\n    for idx,row in df_pred.head(40).iterrows():\n        plt.subplot(7,7,i)\n        img = row[\"X\"].reshape(28,28)\n        img_class_true = row[\"Y_true\"]\n        img_class_pred = row[\"Y_pred\"]\n        plt.imshow(img, cmap=\"Greys\")\n        plt.title(str(row[\"SNo\"]) + \": \" + str(img_class_true) + \" Prediction:\" + str(img_class_pred))\n        plt.subplots_adjust(wspace=1.0, hspace=1.0)\n        i = i+1","81c1e31a":"def plot_bad_data(x, y):\n    data = {\"Y_true\": y}\n    x = list(x)\n    data[\"X\"] = x\n    df_data = pd.DataFrame(data)\n    i = 1\n    fig = plt.figure(figsize=(15,20))\n    fig.suptitle(\"Confusing Data\", size=20)\n    for idx,row in df_data.iterrows():\n        plt.subplot(7,7,i)\n        img = row[\"X\"].reshape(28,28)\n        img_class_true = row[\"Y_true\"]\n        plt.imshow(img, cmap=\"Greys\")\n        plt.title(str(img_class_true))\n        plt.subplots_adjust(wspace=1.0, hspace=1.0)\n        i = i+1","e3c25927":"from skimage.transform import resize\n\nthreshold_color = 100 \/ 255\nsize_img = 28\ndef find_left_edge(x):\n    edge_left = []\n    n_samples = x.shape[0]\n    for k in range(n_samples):\n        for j in range(size_img):\n            for i in range(size_img):\n                if (x[k, size_img*i+j] >= threshold_color):\n                    edge_left.append(j)\n                    break\n            if (len(edge_left) > k):\n                break\n    return edge_left\ndef find_right_edge(x):\n    edge_right = []\n    n_samples = x.shape[0]\n    for k in range(n_samples):\n        for j in range(size_img):\n            for i in range(size_img):\n                if (x[k, size_img*i+(size_img-1-j)] >= threshold_color):\n                    edge_right.append(size_img-1-j)\n                    break\n            if (len(edge_right) > k):\n                break\n    return edge_right\ndef find_top_edge(x):\n    edge_top = []\n    n_samples = x.shape[0]\n    for k in range(n_samples):\n        for i in range(size_img):\n            for j in range(size_img):\n                if (x[k, size_img*i+j] >= threshold_color):\n                    edge_top.append(i)\n                    break\n            if (len(edge_top) > k):\n                break\n    return edge_top\ndef find_bottom_edge(x):\n    edge_bottom = []\n    n_samples = x.shape[0]\n    for k in range(n_samples):\n        for i in range(size_img):\n            for j in range(size_img):\n                if (x[k, size_img*(size_img-1-i)+j] >= threshold_color):\n                    edge_bottom.append(size_img-1-i)\n                    break\n            if (len(edge_bottom) > k):\n                break\n    return edge_bottom\ndef stretch_image(x):\n    #get edges\n    edge_left = find_left_edge(x)\n    edge_right = find_right_edge(x)\n    edge_top = find_top_edge(x)\n    edge_bottom = find_bottom_edge(x)\n    \n    #cropping and resize\n    n_samples = x.shape[0]\n    x = x.reshape(n_samples, size_img, size_img)\n    for i in range(n_samples):      \n        x[i] = resize(x[i][edge_top[i]:edge_bottom[i]+1, edge_left[i]:edge_right[i]+1], (size_img, size_img))\n    x = x.reshape(n_samples, size_img ** 2)\n    return x","398911d3":"train_data = pd.read_csv(\"..\/input\/train.csv\")\nx_train_all = np.array(train_data.drop([\"label\"], axis=1))\nprint (x_train_all[0][x_train_all[0] > 0])\nx_train_all = x_train_all\/255\nx_train_all = stretch_image(x_train_all)\n#x_train_all [ x_train_all > 0] = 1\n\ny_train_all = np.array(train_data[\"label\"])\nidx_all = range(x_train_all.shape[0])\nif 1==2:\n    bad_data_idx = [28290, 16301,14101,15065, 6389,7764,28611,20954,2316, 37056, 37887, 36569, 40257]\n    plot_bad_data(x_train_all[bad_data_idx], y_train_all[bad_data_idx])\n    x_train_all = np.delete(x_train_all, bad_data_idx, axis=0)\n    y_train_all = np.delete(y_train_all, bad_data_idx)\n    idx_all = np.delete(idx_all, bad_data_idx)\ny_train_all = keras.utils.to_categorical(y_train_all, num_classes=10)\nx_train, x_valid, y_train, y_valid, idx_train, idx_valid = skm.train_test_split(x_train_all, y_train_all,idx_all,  test_size=0.2)\n\ntest_data = pd.read_csv(\"..\/input\/test.csv\")\nx_test = np.array(test_data)\nx_test = x_test\/255\nx_test = stretch_image(x_test)\n#x_test[x_test > 0]=1\n\n\n\n","8410cf9d":"datagen = keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n    featurewise_std_normalization=False,\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.1,                                               \n    horizontal_flip=False, shear_range=0.2)\ndatagen.fit(x_train.reshape(-1,28,28,1))\n\n","a14f4b15":"if 1==2:\n    augmented_image = []\n    augmented_image_labels = []\n    x_train = x_train.reshape(-1,28,28,1)\n    for num in range (0, x_train.shape[0]):\n        augmented_image.append(x_train[num])\n        augmented_image_labels.append(y_train[num])\n        if num<500:\n            augmented_image.append(keras.preprocessing.image.random_rotation(x_train[num], 15, row_axis=1, col_axis=1, channel_axis=2))\n            augmented_image_labels.append(y_train[num])\n\n        if num > 500 and num < 1000:\n            augmented_image.append(keras.preprocessing.image.random_shear(x_train[num], 0.2, row_axis=1, col_axis=1, channel_axis=2))\n            augmented_image_labels.append(y_train[num])\n\n        if num > 1000 and num < 1500:\n            augmented_image.append(keras.preprocessing.image.random_shift(x_train[num], 0, 0.2, row_axis=1, col_axis=1, channel_axis=2))\n            augmented_image_labels.append(y_train[num])\n\n    x_train = np.array(augmented_image).reshape(-1, 784)\n    y_train = np.array(augmented_image_labels)","ad10d03f":"print (test_data.head(1))\ns = pd.read_csv(\"..\/input\/sample_submission.csv\")\ns.head(1)\nprint(x_valid.shape[0]*4)\nprint(x_train.shape[0])","c7d0746b":"learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='acc', \n                                            patience=1, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.000001)","6c8d4d51":"def tensorflow_keras_model(x_train, y_train, x_valid, y_valid, num_classes,\\\n                                num_epochs, learning_rate):\n    \n    keras_model = keras.models.Sequential()\n    keras_model.add(keras.layers.Conv2D(filters=32,kernel_size=(6,6),strides=(1,1), \\\n                                        padding=\"same\",input_shape=(28,28,1)))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Conv2D(filters=64,kernel_size=(6,6),strides=(1,1), \\\n                                            padding=\"same\")) #,activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Flatten())\n    keras_model.add(keras.layers.Dense(units=1024)) #, activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Dense(units=num_classes))#, activation='softmax'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('softmax'))  \n    \n    #opt = keras.optimizers.Adam(lr=learning_rate)\n    opt = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n    keras_model.compile(optimizer=opt,loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    keras_model.fit(x_train.reshape(-1,28,28,1), y_train, epochs=num_epochs, callbacks=[learning_rate_reduction])\n    final_models[\"keras1\"] = keras_model\n    keras_model.save(\"Model_MNIST_Keras_Lenet.h5\")\n    cur_y_pred = keras_model.predict(x_valid.reshape(-1,28,28,1))\n    y_valid_argmax = np.argmax(y_valid, 1)\n    y_pred_argmax = np.argmax(cur_y_pred, 1)\n    y_correct = np.equal(y_valid_argmax, y_pred_argmax)\n    acc = y_correct.sum()\/y_pred_argmax.shape[0]\n    return cur_y_pred, acc, y_valid_argmax,y_pred_argmax ","ae805760":"def exec_tensorflow_keras_model():\n    num_epochs = 20 #50\n    learning_rate=0.000001\n    num_rows, num_features, num_classes = x_train.shape[0], x_train.shape[1], 10\n    print(num_rows, num_features, num_classes)\n    final_pred_base_model, acc,y_valid_argmax,y_pred_argmax  = \\\n        tensorflow_keras_model(x_train, y_train, x_valid, y_valid, num_classes, num_epochs, learning_rate)  \n\n    print(\"Num Epoch:\", num_epochs, \" Accuracy:\", acc) #, \\\n           #   \" Weights:\", \" \".join(list(final_w.astype(str).flatten())), \" Bias:\", final_b)\n    plot_y_true_vs_y_pred(x_valid, y_valid_argmax.reshape(len(y_valid)), y_pred_argmax.reshape(len(y_valid)))","3e76276f":"#timeit.timeit(exec_tensorflow_keras_model, number=1)","0af98afc":"#VGG -> 64->MAXPOOL->128->MAXPOOL->256->256->MAXPOOL->512->512->MAXPOOL-->512->512->MAXPOOL->FC4096->FC4096->FC1000","1b328dcc":"#.99154\ndef tensorflow_keras_model_2(x_train, y_train, x_valid, y_valid, num_classes,\\\n                                num_epochs, learning_rate):\n    keras_model = keras.models.Sequential()\n    keras_model.add(keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1), \\\n                                            padding=\"same\",activation='relu', input_shape=(28,28,1)))    #From 64->128->256 changing to 32->64->128\n    keras_model.add(keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1), \\\n                                            padding=\"same\")) #,activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu')) \n    keras_model.add(keras.layers.Conv2D(filters=32,kernel_size=(5,5),strides=(2,2), \\\n                                            padding=\"same\")) #,activation='relu'))\n    #keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1), \\\n                                        padding=\"same\",activation='relu',input_shape=(28,28,1)))\n    keras_model.add(keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1), \\\n                                            padding=\"same\")) #,activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.Conv2D(filters=64,kernel_size=(5,5),strides=(2,2), \\\n                                            padding=\"same\")) #,activation='relu'))\n    #keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.050))\n                    \n    \n    \n    keras_model.add(keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1), \\\n                                            padding=\"same\",activation='relu'))\n    keras_model.add(keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1), \\\n                                            padding=\"same\")) #,activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.Conv2D(filters=128,kernel_size=(5,5),strides=(2,2), \\\n                                            padding=\"same\")) #,activation='relu'))\n    #keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.075))\n    \n    keras_model.add(keras.layers.Flatten())\n    keras_model.add(keras.layers.Dense(units=1024)) # #  , activation='relu')) Chaging from 20148 to 1024\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.Dropout(rate=0.100))\n    \n    keras_model.add(keras.layers.Dense(units=128)) # #  , activation='relu'))  Changin from 256 tp 128\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.Dropout(rate=0.100))\n    \n    keras_model.add(keras.layers.Dense(units=num_classes))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('softmax'))  \n    \n    opt = keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)\n    #opt = keras.optimizers.Adam(lr=learning_rate)\n    keras_model.compile(optimizer=opt,loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    keras_model.fit_generator(datagen.flow(x_train.reshape(-1,28,28,1), y_train, batch_size=32),steps_per_epoch=len(x_train) \/ 32,  \\\n                              epochs=num_epochs, callbacks=[learning_rate_reduction])\n    final_models[\"keras2\"] = keras_model\n    keras_model.save(\"Model_MNIST_Keras_Resnet.h5\")\n   \n    cur_y_pred = keras_model.predict(x_valid.reshape(-1,28,28,1))\n    y_valid_argmax = np.argmax(y_valid, 1)\n    y_pred_argmax = np.argmax(cur_y_pred, 1)\n    y_correct = np.equal(y_valid_argmax, y_pred_argmax)\n    acc = y_correct.sum()\/y_pred_argmax.shape[0]\n    return cur_y_pred, acc, y_valid_argmax,y_pred_argmax , keras_model\n    ","14db2b00":"model2_list = []\nnum_model = 12\ndef exec_tensorflow_keras_model_2():\n    num_epochs = 30 #50  With 30 epochs acc was 99.667\n    learning_rate=0.00001\n    \n    num_rows, num_features, num_classes = x_train.shape[0], x_train.shape[1], 10\n    for imodel in range(num_model):\n        final_pred_base_model, acc,y_valid_argmax,y_pred_argmax, keras_model  = \\\n                    tensorflow_keras_model_2(x_train, y_train, x_valid, y_valid, num_classes, num_epochs, learning_rate)\n        model2_list.append(keras_model)\n\n        print(\"Num Epoch:\", num_epochs, \" Accuracy:\", acc) #, \\\n           #   \" Weights:\", \" \".join(list(final_w.astype(str).flatten())), \" Bias:\", final_b)\n    plot_y_true_vs_y_pred(x_valid, y_valid_argmax.reshape(len(y_valid)), y_pred_argmax.reshape(len(y_valid)), idx_valid)","1ba50c3a":"timeit.timeit(exec_tensorflow_keras_model_2, number=1)\n","a5e67583":"def tensorflow_keras_model_3(x_train, y_train, x_valid, y_valid, num_classes,\\\n                                num_epochs, learning_rate):\n    \n    keras_model = keras.models.Sequential()\n    keras_model.add(keras.layers.Conv2D(filters=6,kernel_size=(6,6),strides=(1,1), \\\n                                        padding=\"same\",input_shape=(28,28,1)))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Conv2D(filters=16,kernel_size=(6,6),strides=(1,1), \\\n                                            padding=\"same\")) #,activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Conv2D(filters=120,kernel_size=(6,6),strides=(1,1), \\\n                                            padding=\"same\")) #,activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Flatten())\n    keras_model.add(keras.layers.Dense(units=120)) #, activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Dense(units=120)) #, activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Dense(units=num_classes))#, activation='softmax'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('softmax'))  \n    \n    #opt = keras.optimizers.Adam(lr=learning_rate)\n    opt = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n    keras_model.compile(optimizer=opt,loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    keras_model.fit(x_train.reshape(-1,28,28,1), y_train, epochs=num_epochs, callbacks=[learning_rate_reduction])\n    final_models[\"keras1\"] = keras_model\n    keras_model.save(\"Model_MNIST_Keras_Lenet.h5\")\n    cur_y_pred = keras_model.predict(x_valid.reshape(-1,28,28,1))\n    y_valid_argmax = np.argmax(y_valid, 1)\n    y_pred_argmax = np.argmax(cur_y_pred, 1)\n    y_correct = np.equal(y_valid_argmax, y_pred_argmax)\n    acc = y_correct.sum()\/y_pred_argmax.shape[0]\n    return cur_y_pred, acc, y_valid_argmax,y_pred_argmax ","9190f0b1":"def exec_tensorflow_keras_model_3():\n    num_epochs = 30 #50\n    learning_rate=0.00001\n    num_rows, num_features, num_classes = x_train.shape[0], x_train.shape[1], 10\n    final_pred_base_model, acc,y_valid_argmax,y_pred_argmax  = \\\n        tensorflow_keras_model_3(x_train, y_train, x_valid, y_valid, num_classes, num_epochs, learning_rate)  \n\n    print(\"Num Epoch:\", num_epochs, \" Accuracy:\", acc) #, \\\n           #   \" Weights:\", \" \".join(list(final_w.astype(str).flatten())), \" Bias:\", final_b)\n    plot_y_true_vs_y_pred(x_valid, y_valid_argmax.reshape(len(y_valid)), y_pred_argmax.reshape(len(y_valid)), idx_valid)","73920a77":"#timeit.timeit(exec_tensorflow_keras_model_3, number=1)","4f08d15f":"#model1 = final_models[\"keras1\"]\nmodel2 = final_models[\"keras2\"]\n#y_pred_valid1 = model1.predict(x_valid.reshape(-1,28,28,1))\n#y_pred_valid2 = model2.predict(x_valid.reshape(-1,28,28,1))\nx_valid = x_valid.reshape(-1,28,28,1)\ny_pred_valid2 = np.zeros( (x_valid.shape[0],10) ) \nfor imodel in range(num_model): #len(model2_list)):\n    y_pred_valid2 = y_pred_valid2 + model2_list[imodel].predict(x_valid)\n    \ny_pred_valid = y_pred_valid2 #0.4*y_pred_valid1 + 0.6*y_pred_valid2\n\ny_pred_valid_final = np.argmax(y_pred_valid, axis=1)\ny_correct = np.equal(y_pred_valid_final, np.argmax(y_valid, axis=1))\nacc_final = y_correct.sum()\/y_valid.shape[0]\nprint(acc_final)","796923bd":"#y_pred_valid1 = model1.predict(x_test.reshape(-1,28,28,1))\n#y_pred_test2 = model2.predict(x_test.reshape(-1,28,28,1))\nx_test = x_test.reshape(-1,28,28,1)\ny_pred_test2 = np.zeros( (x_test.shape[0],10) ) \nfor imodel in range(num_model):\n    y_pred_test2 = y_pred_test2 + model2_list[imodel].predict(x_test)\n","ce7a910e":"y_pred_test = y_pred_test2 #0.4*y_pred_valid1 + 0.6*y_pred_valid2\ny_pred_test_final = np.argmax(y_pred_test, axis=1)\n","c2879290":"dictionary_data = {\"ImageId\":np.arange(1, x_test.shape[0]+1), \"Label\":y_pred_test_final}\ndf_final = pd.DataFrame(dictionary_data)\ndf_final.to_csv(\"submission.csv\", index=False)","d10b37b8":"#","99d21b05":"plot_y_true_vs_y_pred(x_valid, np.argmax(y_valid, 1).reshape(len(y_valid)), y_pred_valid_final.reshape(len(y_valid)), idx_valid)","81f77b2a":"## Load & Clean data\nLet us load train data and plot some points so as to understand our data better. We see that some data in training data set is incorrectly classified. We will remove these data points so as to avoid confusing the model. Let us then split the cleaned train data into train and validation data set. ","7c966c6d":"**LeNet Architecture**\nINPUT => CONV => RELU => POOL => CONV => RELU => POOL => FC => RELU => FC","41d6a9fe":"## Techniques used to improve accuracy\n### Batch Normalization\nIf configuration of one batch is very different than configuration of other batch, then converging the model will be very diificult as weights detemined for one batch would lead to very unsatisfactory results for other batch. Same concept can be applied to deep neural networks. Each batch in every hidden layer should ideally have similar set of data points. This is called batch normalization and Keras provides a very simple way of doing this as shown below. It is advised to add Batch Normalization before activation layer. \n### Dropout\nDropout is essentially discarding random data points in oder to avoid overfitting. With Batch Normalization, this may not be required, so we will check and confirm.\n","8ada0369":"\n## Augment data\nData augmentation refers to adding more data points in the train data so that model can learn better. For MNIST data some of the techniques used to create new data points that might help the model to train better are rotation, shear and shift. AKeras has a very nice API for facilitating this and that is what we will try to use."}}