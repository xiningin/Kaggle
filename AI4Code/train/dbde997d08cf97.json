{"cell_type":{"a22811a7":"code","0b1aec8d":"code","af616c8e":"code","f90c6e4f":"code","6bb66fcc":"code","22aea248":"code","70708971":"code","92d06bb8":"code","5d0cc350":"code","a4b6d154":"code","9b289159":"code","da895fc4":"code","cb946bca":"code","dc41a71e":"code","2865482b":"code","6a2ba928":"code","7725b271":"code","946ce2b3":"code","0e510745":"code","8947fc4e":"code","36e5073d":"code","106b7f05":"code","5a8f8b41":"code","5d315a44":"code","312fcc16":"code","f9451e27":"code","db73cfd5":"code","9fb0db2b":"code","64b459fb":"code","ab71fe8c":"code","9f4026a9":"code","522f73f8":"code","abc73eba":"code","ceda847e":"code","65d05116":"code","b558b8dd":"code","6773c714":"code","de783a72":"code","cac991e4":"code","cefeb239":"code","212b18e5":"code","8f100d77":"code","bd335525":"code","b9dbbb56":"code","c1ab4245":"code","81ab29e9":"code","18829375":"code","aec21c17":"code","88a690ae":"code","c62b08d4":"code","42216125":"code","88c401cd":"code","229116cf":"code","1bfcc726":"code","7fde45d6":"code","009efbb1":"code","7c00ec78":"code","217d35a9":"code","87304021":"markdown","d1d0db2f":"markdown","ffe57270":"markdown","b5b75ef8":"markdown","81c16c8f":"markdown","1a37a8f1":"markdown"},"source":{"a22811a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0b1aec8d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)  \nfrom sklearn import metrics, preprocessing, model_selection\nfrom sklearn.model_selection import train_test_split,cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt                                      # to plot graph\n%matplotlib inline\nimport xgboost as xgb\nimport lightgbm as lgb\nSEED = 1\n\n#To ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","af616c8e":"file = r'\/kaggle\/input\/analytics-vidhya-janatahack-customer-segmentation\/'\ntrain_df = pd.read_csv(file+'Train_aBjfeNk.csv')\ntest_df = pd.read_csv(file+'Test_LqhgPWU.csv')\nsub_df = pd.read_csv(file+'sample_submission_wyi0h0z.csv')","f90c6e4f":"train_df.head()","6bb66fcc":"test_df.head()","22aea248":"sub_df.head()","70708971":"print(train_df.shape, test_df.shape,sub_df.shape)","92d06bb8":"train_df['Segmentation'].value_counts()","5d0cc350":"train_df.isnull().sum()","a4b6d154":"test_df.isnull().sum()","9b289159":"missing_df = train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.loc[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\n\nind = np.arange(missing_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(8,6))\nrects = ax.barh(ind, missing_df.missing_count.values, color='blue')\nax.set_yticks(ind)\nax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_title(\"Number of missing values in each column\")\nplt.show()","da895fc4":"missingvalues_prop = (train_df.isnull().sum()\/len(train_df)).reset_index()\nmissingvalues_prop.columns = ['field','proportion']\nmissingvalues_prop = missingvalues_prop.sort_values(by = 'proportion', ascending = False)\n# print(missingvalues_prop)\nmissingvaluescols = missingvalues_prop[missingvalues_prop['proportion'] > 0.10].field.tolist()\nprint(missingvaluescols)","cb946bca":"# Normalise can be set to true to print the proportions instead of Numbers.\ntrain_df['Segmentation'].value_counts(normalize=True)","dc41a71e":"train_df['Segmentation'].value_counts().plot.bar(figsize=(4,4),title='Segmentation - Split for Train Dataset')\nplt.xlabel('ExtraTime')\nplt.ylabel('Count')","2865482b":"plt.figure(1)\nplt.subplot(221)\ntrain_df['Gender'].value_counts(normalize=True).plot.bar(figsize=(20,10), fontsize = 15.0)\nplt.title('Gender', fontweight=\"bold\", fontsize = 22.0)\nplt.ylabel('Count %', fontsize = 20.0)\n\n\nplt.subplot(222)\ntrain_df['Ever_Married'].value_counts(normalize=True).plot.bar(figsize=(20,10), fontsize = 15.0)\nplt.title('Ever_Married', fontweight=\"bold\",fontsize = 22.0)\nplt.ylabel('Count %', fontsize = 20.0)\n\nplt.subplot(223)\ntrain_df['Graduated'].value_counts(normalize=True).plot.bar(figsize=(20,10), fontsize = 15.0)\nplt.title('Graduated', fontweight=\"bold\", fontsize = 22.0)\nplt.ylabel('Count %', fontsize = 20.0)\n\nplt.subplot(224)\ntrain_df['Work_Experience'].value_counts(normalize=True).plot.bar(figsize=(20,10), fontsize = 15.0)\nplt.title('Work_Experience', fontweight=\"bold\", fontsize = 22.0)\nplt.ylabel('Count %', fontsize = 20.0)\nplt.tight_layout()","6a2ba928":"plt.figure(1)\nplt.subplot(221)\ntrain_df['Spending_Score'].value_counts(normalize=True).plot.bar(figsize=(20,10), fontsize = 15.0)\nplt.title('Spending_Score', fontweight=\"bold\", fontsize = 22.0)\nplt.ylabel('Count %', fontsize = 20.0)\n\n\nplt.subplot(222)\ntrain_df['Family_Size'].value_counts(normalize=True).plot.bar(figsize=(20,10), fontsize = 15.0)\nplt.title('Family_Size', fontweight=\"bold\",fontsize = 22.0)\nplt.ylabel('Count %', fontsize = 20.0)\n\nplt.subplot(223)\ntrain_df['Var_1'].value_counts(normalize=True).plot.bar(figsize=(20,10), fontsize = 15.0)\nplt.title('Var_1', fontweight=\"bold\", fontsize = 22.0)\nplt.ylabel('Count %', fontsize = 20.0)\n","7725b271":"plt.figure(1)\nplt.subplot(121)\nsns.distplot(train_df['Age'])\n\nplt.subplot(122)\ntrain_df['Age'].plot.box(figsize=(16,5))\n\nplt.show()","946ce2b3":"train_df.columns","0e510745":"plt.figure(1)\nplt.subplot(121)\nsns.distplot(np.log1p(train_df['Age']))\n\nplt.subplot(122)\nnp.log1p(train_df['Age']).plot.box(figsize=(16,5))\n\nplt.show()","8947fc4e":"Gender=pd.crosstab(train_df['Gender'],train_df['Segmentation'])\nEver_Married=pd.crosstab(train_df['Ever_Married'],train_df['Segmentation'])\nGraduated=pd.crosstab(train_df['Graduated'],train_df['Segmentation'])\nProfession=pd.crosstab(train_df['Profession'],train_df['Segmentation'])\n\n\n\nGender.div(Gender.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nEver_Married.div(Ever_Married.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nGraduated.div(Graduated.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nProfession.div(Profession.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nplt.tight_layout()\n","36e5073d":"Work_Experience=pd.crosstab(train_df['Work_Experience'],train_df['Segmentation'])\nEver_Married=pd.crosstab(train_df['Ever_Married'],train_df['Segmentation'])\nGraduated=pd.crosstab(train_df['Graduated'],train_df['Segmentation'])\nProfession=pd.crosstab(train_df['Profession'],train_df['Segmentation'])\n\n\n\nWork_Experience.div(Work_Experience.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nEver_Married.div(Ever_Married.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nGraduated.div(Graduated.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nProfession.div(Profession.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))","106b7f05":"# * join the datasets\ntrain_df['is_train']  = 1\ntest_df['Segmentation'] = -1\ntest_df['is_train'] = 0","5a8f8b41":"# Find common Ids in both train and test\ncommon_ids = np.intersect1d(train_df.ID, np.intersect1d(train_df.ID, test_df.ID))\nprint(\"Common ids in train and test are\",len(common_ids))\n\ncommon_ids_df = train_df[train_df['ID'].isin(common_ids)]\nprint(common_ids_df.head())\n\n#Remove leaked_ids\ntest_df_leak = test_df[test_df['ID'].isin(common_ids)]\nprint(test_df_leak.head())\n\nprint(test_df.shape, test_df_leak.shape)","5d315a44":"# separate non-leak ids\n#Remove leaked_ids\ntest_df_non_leak = test_df[~test_df['ID'].isin(common_ids)]\ntest_df_non_leak.head()","312fcc16":"test_df_leak_predictions = train_df[train_df['ID'].isin(common_ids)]\ntest_df_leak_predictions[['ID', 'Segmentation']].head()","f9451e27":"test_df_leak_predictions[['ID', 'Segmentation']].shape","db73cfd5":"full_df = train_df.append(test_df)","9fb0db2b":"full_df.head()","64b459fb":"full_df.dtypes","ab71fe8c":"full_df.isnull().sum()","9f4026a9":"# append train and test data\ntestcount = len(test_df)\ncount = len(full_df)-testcount\nprint(count)\n\ntrain = full_df[:count]\ntest = full_df[count:]\ntrain_df = train.copy()\ntest_df = test.copy()","522f73f8":"full_df.columns","abc73eba":"full_df.isnull().sum()","ceda847e":"cols = ['ID', 'Gender', 'Ever_Married', 'Age', 'Graduated', 'Profession',\n       'Work_Experience', 'Spending_Score', 'Family_Size', 'Var_1',\n        'is_train' ]\nfor col in cols:\n    if train_df[col].dtype==object:\n        print(col)\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n        train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n        test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))","65d05116":"X = train_df.drop(['Segmentation', 'is_train' ,'ID'],axis=1)\ny = train_df['Segmentation'].values\n\ntrain_X = X.copy()\ntrain_y = y.copy()\n\ntest_X = test_df.drop(['Segmentation', 'is_train' ,'ID'],axis=1)\nprint(X.shape, test_X.shape)","b558b8dd":"X.head()","6773c714":"test_X.head()","de783a72":"X.isnull().sum()","cac991e4":"params = {}\nparams['learning_rate'] = 0.01\nparams['n_estimators'] = 10000\nparams['objective'] = 'multiclass'\nparams['boosting_type'] = 'gbdt'","cefeb239":"feature_cols = X.columns","212b18e5":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                  stratify=y, \n                                                  random_state=1234, \n                                                  test_size=0.20, shuffle=True)","8f100d77":"cat_cols = ['Gender','Ever_Married', 'Graduated', 'Profession','Family_Size',\n            'Spending_Score','Var_1']\nlabel_col = 'Segmentation'","bd335525":"clf = lgb.LGBMClassifier(**params)\n    \nclf.fit(X_train, y_train, early_stopping_rounds=200,\n        eval_set=[(X_valid, y_valid)], \n        eval_metric='multi_error', verbose=False, categorical_feature=cat_cols)\n\neval_score = accuracy_score(y_valid, clf.predict(X_valid[feature_cols]))\n\nprint('Eval ACC: {}'.format(eval_score))","b9dbbb56":"best_iter = clf.best_iteration_\nparams['n_estimators'] = best_iter\nprint(params)","c1ab4245":"test_X.shape","81ab29e9":"preds = clf.predict(test_X[feature_cols])","18829375":"np.unique(preds, return_counts=True)","aec21c17":"sub_df['Segmentation'] = preds\n\nsub_df.head()","88a690ae":"plt.rcParams['figure.figsize'] = (12, 6)\nlgb.plot_importance(clf)\nplt.show()","c62b08d4":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\nname = \"baseline_lgb.csv\"\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = name):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n\n# create a link to download the dataframe\ncreate_download_link(sub_df)","42216125":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\nname = \"leaked.csv\"\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = name):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n\n# create a link to download the dataframe\ncreate_download_link(test_df_leak_predictions)","88c401cd":"from catboost import Pool, CatBoostClassifier","229116cf":"model = CatBoostClassifier(\n    iterations=10000,\n    learning_rate=0.005,\n    random_strength=0.1,\n    depth=8,\n    loss_function='MultiClass',\n    eval_metric='Accuracy',\n    leaf_estimation_method='Newton'\n)","1bfcc726":"train_pool = Pool(data=X_train, label=y_train)\ntest_pool = Pool(data=X_valid, label=y_valid) \nmodel.fit(train_pool,plot=True,eval_set=test_pool ,verbose=False)","7fde45d6":"cat_preds = model.predict(test_X)","009efbb1":"np.unique(cat_preds, return_counts=True)","7c00ec78":"sub_df['Segmentation'] = cat_preds\n\nsub_df.head()","217d35a9":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\nname = \"baseline_cb.csv\"\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = name):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n\n# create a link to download the dataframe\ncreate_download_link(sub_df)","87304021":"### Catboost","d1d0db2f":"### As I showed how the data leak output I Saved, I then manually merged the data leak ouptut from train dataset to the final test set predictions. I really think AV should have take care of the data leak.\n\n### The Public LB score : 0.9504 (Accuracy) Rank - 38","ffe57270":"### Feature Engineering","b5b75ef8":"#### Checking missing values","81c16c8f":"### Exploratory Data Analysis","1a37a8f1":"# Private LB score: 0.94927 (Accuracy) Rank - 19"}}