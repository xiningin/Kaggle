{"cell_type":{"b9af63b3":"code","55cd2c3f":"code","b03694d3":"code","161692ca":"code","549ddf30":"code","6ad1e3bd":"code","62f7f90d":"code","95feb8bc":"code","18d8619b":"code","7fc48927":"code","1b14ed64":"code","da403625":"code","d8c5a4b3":"code","9868e93c":"code","1cd736ac":"code","e52721f5":"code","b7913ea0":"code","44ee9d7f":"code","049ccc9d":"code","be9173b5":"code","0ed35748":"markdown","a9602b86":"markdown","51dac6d8":"markdown"},"source":{"b9af63b3":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom tensorflow.keras import layers,regularizers,Sequential,Model,backend,callbacks,optimizers,metrics,losses\nimport tensorflow as tf\nimport sys\nimport json\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","55cd2c3f":"# Import train data, drop sig_id, cp_type\n\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\nnon_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_features = train_features.drop(['sig_id','cp_type','cp_dose','cp_time'],axis=1)\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_scored = train_targets_scored.drop('sig_id',axis=1)\nlabels_train = train_targets_scored.values\n\n# Drop training data with ctl vehicle\n\ntrain_features = train_features.iloc[non_ctl_idx]\nlabels_train = labels_train[non_ctl_idx]\n\n# Import test data\n\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntest_features = test_features.drop(['sig_id','cp_dose','cp_time'],axis=1)\n\n# Import predictors from public kernel\n\njson_file_path = '..\/input\/t-test-pca-rfe-logistic-regression\/main_predictors.json'\n\nwith open(json_file_path, 'r') as j:\n    predictors = json.loads(j.read())\n    predictors = predictors['start_predictors']","b03694d3":"# Create g-mean, c-mean, genes_pca (2 components), cells_pca (all components)\n\ncs = train_features.columns.str.contains('c-')\ngs = train_features.columns.str.contains('g-')\n\ndef preprocessor(train,test):\n    \n    # PCA\n    \n    n_gs = 2 # No of PCA comps to include\n    n_cs = 100 # No of PCA comps to include\n    \n    pca_cs = PCA(n_components = n_cs)\n    pca_gs = PCA(n_components = n_gs)\n\n    train_pca_gs = pca_gs.fit_transform(train[:,gs])\n    train_pca_cs = pca_cs.fit_transform(train[:,cs])\n    test_pca_gs = pca_gs.transform(test[:,gs])\n    test_pca_cs = pca_cs.transform(test[:,cs])\n    \n    # c-mean, g-mean\n    \n    train_c_mean = train[:,cs].mean(axis=1)\n    test_c_mean = test[:,cs].mean(axis=1)\n    train_g_mean = train[:,gs].mean(axis=1)\n    test_g_mean = test[:,gs].mean(axis=1)\n    \n    # Append Features\n    \n    train = np.concatenate((train,train_pca_gs,train_pca_cs,train_c_mean[:,np.newaxis]\n                            ,train_g_mean[:,np.newaxis]),axis=1)\n    test = np.concatenate((test,test_pca_gs,test_pca_cs,test_c_mean[:,np.newaxis],\n                           test_g_mean[:,np.newaxis]),axis=1)\n    \n    # Scaler for numerical values\n\n    # Scale train data\n    scaler = preprocessing.StandardScaler()\n\n    train = scaler.fit_transform(train)\n\n    # Scale Test data\n    test = scaler.transform(test)\n    \n    return train, test\n","161692ca":"n_labels = train_targets_scored.shape[1]\nn_train = train_features.shape[0]\nn_test = test_features.shape[0]\n\n\n# Prediction Clipping Thresholds\n\np_min = 0.0005\np_max = 0.9995\n\n# OOF Evaluation Metric with clipping and no label smoothing\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))","549ddf30":"def build_model(n_features, n_features_2, n_labels, label_smoothing = 0.0005):    \n    input_1 = layers.Input(shape = (n_features,), name = 'Input1')\n    input_2 = layers.Input(shape = (n_features_2,), name = 'Input2')\n\n    head_1 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(512, activation=\"elu\"), \n        layers.BatchNormalization(),\n        layers.Dense(256, activation = \"elu\")\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = layers.Concatenate()([input_2, input_3])\n\n    head_2 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(512, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(512, \"elu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(256, \"elu\")\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_avg = layers.Average()([input_3, input_4]) \n\n    head_3 = Sequential([\n        layers.BatchNormalization(),\n        layers.Dense(256, kernel_initializer='lecun_normal', activation='selu'),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels, kernel_initializer='lecun_normal', activation='selu'),\n        layers.BatchNormalization(),\n        layers.Dense(n_labels, activation=\"sigmoid\")\n        ],name='Head3')\n\n    output = head_3(input_4_avg)\n\n\n    model = Model(inputs = [input_1, input_2], outputs = output)\n    model.compile(optimizer='adam', loss=losses.BinaryCrossentropy(label_smoothing=label_smoothing), metrics=logloss)\n    \n    return model","6ad1e3bd":"# Generate Seeds\n\nn_seeds = 7\nnp.random.seed(1)\nseeds = np.random.randint(0,100,size=n_seeds)\n\n# Training Loop\n\nn_folds = 10\ny_pred = np.zeros((n_test,n_labels))\noof = tf.constant(0.0)\nhists = []\nfor seed in seeds:\n    fold = 0\n    kf = KFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in kf.split(train_features):\n        X_train, X_test = preprocessor(train_features.iloc[train].values,\n                                       train_features.iloc[test].values)\n        _,data_test = preprocessor(train_features.iloc[train].values,\n                                   test_features.drop('cp_type',axis=1).values)\n        X_train_2 = train_features.iloc[train][predictors].values\n        X_test_2 = train_features.iloc[test][predictors].values\n        data_test_2 = test_features[predictors].values\n        y_train = labels_train[train]\n        y_test = labels_train[test]\n        n_features = X_train.shape[1]\n        n_features_2 = X_train_2.shape[1]\n\n        model = build_model(n_features, n_features_2, n_labels)\n        \n        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.1, patience=2, mode='min', min_lr=1E-5)\n        early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=10, mode='min',restore_best_weights=True)\n\n        hist = model.fit([X_train,X_train_2],y_train, batch_size=128, epochs=192,verbose=0,validation_data = ([X_test,X_test_2],y_test),\n                         callbacks=[reduce_lr, early_stopping])\n        hists.append(hist)\n        \n        # Save Model\n        model.save('TwoHeads_seed_'+str(seed)+'_fold_'+str(fold))\n\n        # OOF Score\n        y_val = model.predict([X_test,X_test_2])\n        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))\/(n_folds*n_seeds)\n\n        # Run prediction\n        y_pred += model.predict([data_test,data_test_2])\/(n_folds*n_seeds)\n\n        fold += 1","62f7f90d":"# Model Architecture\n\ntf.keras.utils.plot_model(model,show_shapes=True)","95feb8bc":"# Analysis of Training\n\ntf.print('OOF score is ',oof)\n\nplt.figure(figsize=(12,8))\n\nhist_trains = []\nhist_lens = []\nfor i in range(n_folds*n_seeds):\n    hist_train = (hists[i]).history['logloss']\n    hist_trains.append(hist_train)\n    hist_lens.append(len(hist_train))\nhist_train = []\nfor i in range(min(hist_lens)):\n    hist_train.append(np.mean([hist_trains[j][i] for j in range(n_folds*n_seeds)]))\n\nplt.plot(hist_train)\n\nhist_vals = []\nhist_lens = []\nfor i in range(n_folds*n_seeds):\n    hist_val = (hists[i]).history['val_logloss']\n    hist_vals.append(hist_val)\n    hist_lens.append(len(hist_val))\nhist_val = []\nfor i in range(min(hist_lens)):\n    hist_val.append(np.mean([hist_vals[j][i] for j in range(n_folds*n_seeds)]))\n\nplt.plot(hist_val)\n\nplt.yscale('log')\nplt.yticks(ticks=[1,1E-1,1E-2])\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])","18d8619b":"# Generate submission file, Clip Predictions\n\nsub = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\nsub.iloc[:,1:] = y_pred\n\n# Set ctl_vehicle to 0\nsub.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0\n\n# Save Submission\nsub.to_csv('sub2H.csv', index=False)","7fc48927":"import os\nimport gc\nimport pickle\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm\nfrom time import time","1b14ed64":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\nss_svc = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\nss_lr = ss_svc.copy()\n\ncols = [c for c in ss_svc.columns.values if c != 'sig_id']\n\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\n\nfrom sklearn.preprocessing import QuantileTransformer\n\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n    \ndef preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ndef log_loss_metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']    \n\nfrom sklearn.linear_model import Ridge\n\nscaler = StandardScaler()\nX = scaler.fit_transform(train.values)#)[:, top_feats])\nx_tt = scaler.transform(test_features.values)#[:, top_feats])\n\n# from sklearn.svm import SVC, SVR\n#from cuml.svm import SVC, SVR\n\nN_STARTS = 3\nN_SPLITS = 5\n\nres_svc = train_targets.copy()\nss_svc.loc[:, train_targets.columns] = 0\nres_svc.loc[:, train_targets.columns] = 0\n\nfor tar in tqdm(range(train_targets.shape[1])):\n\n    start_time = time()\n    targets = train_targets.values[:, tar]\n\n    if targets.sum() >= N_SPLITS:\n\n        for seed in range(N_STARTS):\n\n            skf = StratifiedKFold(n_splits = N_SPLITS, random_state = seed, shuffle = True)\n\n            for n, (tr, te) in enumerate(skf.split(targets, targets)):\n\n                x_tr, x_val = X[tr], X[te]\n                y_tr, y_val = targets[tr], targets[te]\n\n                model = Ridge(alpha = 0.0001)#SVC(C = 1, cache_size = 2000)\n                model.fit(x_tr, y_tr)\n                ss_svc.loc[:, train_targets.columns[tar]] += model.predict(x_tt) \/ (N_SPLITS * N_STARTS)\n                res_svc.loc[te, train_targets.columns[tar]] += model.predict(x_val) \/ N_STARTS\n\n    score = log_loss(train_targets.loc[:, train_targets.columns[tar]], res_svc.loc[:, train_targets.columns[tar]])\n    print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] SVM Target {tar}:', score)\n    \nprint(f'SVM OOF Metric: {log_loss_metric(train_targets, res_svc)}')\nres_svc.loc[train['cp_type'] == 1, train_targets.columns] = 0\nss_svc.loc[test['cp_type'] == 1, train_targets.columns] = 0\nprint(f'SVM OOF Metric with postprocessing: {log_loss_metric(train_targets, res_svc)}')","da403625":"!pip install --no-index --find-links \/kaggle\/input\/pytorchtabnet\/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","d8c5a4b3":"train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsubmission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\nremove_vehicle = True\n\nif remove_vehicle:\n    train_features = train.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n    train_targets_nonscored = train_targets_nonscored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\nelse:\n    train_features = train","9868e93c":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport numpy as np\nimport pandas as pd \n\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss\n#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score\n\ndef seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)\n\nMAX_EPOCH=250\ntabnet_params = dict(n_d=24, n_a=24, n_steps=1, gamma=1.3,\n                     lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                     mask_type='entmax',\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=5,\n                                           min_lr=1e-5,\n                                           factor=0.9,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=10, \n                     )\n\n\nfrom pytorch_tabnet.metrics import Metric\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nclass LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 \/ (1 + np.exp(-y_pred))\n        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n        return np.mean(-aux)\n    \n    \ndata_path = \"..\/input\/lish-moa\/\"","1cd736ac":"train = res_svc[cols].values\nX_test = ss_svc[cols].values\ntrain_targets_scored = pd.read_csv(data_path+'train_targets_scored.csv')\ntrain_targets_scored.drop(columns=[\"sig_id\"], inplace=True)\nsubmission = pd.read_csv(data_path+'sample_submission.csv')\n\nscores_auc_all= []\ntest_cv_preds = []\n\nNB_SPLITS = 10\nmskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=0, shuffle=True)\noof_preds = []\noof_targets = []\nscores = []\nscores_auc = []\nfor fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train, train_targets_scored)):\n    print(\"FOLDS : \", fold_nb)\n\n    ## model\n    X_train, y_train = train[train_idx, :], train_targets_scored.values[train_idx, :]\n    X_val, y_val = train[val_idx, :], train_targets_scored.values[val_idx, :]\n    model = TabNetRegressor(**tabnet_params)\n\n    model.fit(X_train=X_train,\n              y_train=y_train,\n              eval_set=[(X_val, y_val)],\n              eval_name = [\"val\"],\n              eval_metric = [\"logits_ll\"],\n              max_epochs=MAX_EPOCH,\n              patience=20, batch_size=1024, virtual_batch_size=128,\n              num_workers=1, drop_last=False,\n              # use binary cross entropy as this is not a regression problem\n              loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n\n    preds_val = model.predict(X_val)\n    # Apply sigmoid to the predictions\n    preds =  1 \/ (1 + np.exp(-preds_val))\n    score = np.min(model.history[\"val_logits_ll\"])\n#     name = cfg.save_name + f\"_fold{fold_nb}\"\n#     model.save_model(name)\n    ## save oof to compute the CV later\n    oof_preds.append(preds_val)\n    oof_targets.append(y_val)\n    scores.append(score)\n\n    # preds on test\n    preds_test = model.predict(X_test)\n    test_cv_preds.append(1 \/ (1 + np.exp(-preds_test)))\n\noof_preds_all = np.concatenate(oof_preds)\noof_targets_all = np.concatenate(oof_targets)\ntest_preds_all = np.stack(test_cv_preds)\n\naucs = []\nfor task_id in range(oof_preds_all.shape[1]):\n    aucs.append(roc_auc_score(y_true=oof_targets_all[:, task_id],\n                              y_score=oof_preds_all[:, task_id]))\nprint(f\"Overall AUC : {np.mean(aucs)}\")\nprint(f\"Average CV : {np.mean(scores)}\")","e52721f5":"\n\nall_feat = [col for col in submission.columns if col not in [\"sig_id\"]]\nsubmission[all_feat] = test_preds_all.mean(axis=0)\n# set control to 0\nsubmission.loc[test['cp_type']==1, submission.columns[1:]] = 0\nsubmission.to_csv('subSVM.csv', index=None)\nsubSVM = submission.copy()","b7913ea0":"train = oof\nX_test = y_pred\ntrain_targets_scored = pd.read_csv(data_path+'train_targets_scored.csv')\ntrain_targets_scored.drop(columns=[\"sig_id\"], inplace=True)\nsubmission = pd.read_csv(data_path+'sample_submission.csv')\n\nscores_auc_all= []\ntest_cv_preds = []\n\nNB_SPLITS = 10\nmskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=0, shuffle=True)\noof_preds = []\noof_targets = []\nscores = []\nscores_auc = []\nfor fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train, train_targets_scored)):\n    print(\"FOLDS : \", fold_nb)\n\n    ## model\n    X_train, y_train = train[train_idx, :], train_targets_scored.values[train_idx, :]\n    X_val, y_val = train[val_idx, :], train_targets_scored.values[val_idx, :]\n    model = TabNetRegressor(**tabnet_params)\n\n    model.fit(X_train=X_train,\n              y_train=y_train,\n              eval_set=[(X_val, y_val)],\n              eval_name = [\"val\"],\n              eval_metric = [\"logits_ll\"],\n              max_epochs=MAX_EPOCH,\n              patience=20, batch_size=1024, virtual_batch_size=128,\n              num_workers=1, drop_last=False,\n              # use binary cross entropy as this is not a regression problem\n              loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n\n    preds_val = model.predict(X_val)\n    # Apply sigmoid to the predictions\n    preds =  1 \/ (1 + np.exp(-preds_val))\n    score = np.min(model.history[\"val_logits_ll\"])\n#     name = cfg.save_name + f\"_fold{fold_nb}\"\n#     model.save_model(name)\n    ## save oof to compute the CV later\n    oof_preds.append(preds_val)\n    oof_targets.append(y_val)\n    scores.append(score)\n\n    # preds on test\n    preds_test = model.predict(X_test)\n    test_cv_preds.append(1 \/ (1 + np.exp(-preds_test)))\n\noof_preds_all = np.concatenate(oof_preds)\noof_targets_all = np.concatenate(oof_targets)\ntest_preds_all = np.stack(test_cv_preds)\n\naucs = []\nfor task_id in range(oof_preds_all.shape[1]):\n    aucs.append(roc_auc_score(y_true=oof_targets_all[:, task_id],\n                              y_score=oof_preds_all[:, task_id]))\nprint(f\"Overall AUC : {np.mean(aucs)}\")\nprint(f\"Average CV : {np.mean(scores)}\")","44ee9d7f":"all_feat = [col for col in submission.columns if col not in [\"sig_id\"]]\nsubmission[all_feat] = test_preds_all.mean(axis=0)\n# set control to 0\nsubmission.loc[test['cp_type']==1, submission.columns[1:]] = 0\nsubmission.to_csv('subTab2.csv', index=None)\nsubTab = submission.copy()","049ccc9d":"#blend","be9173b5":"submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\npredictions =[]\npredictions.append(subSVM)\npredictions.append(sub)\npredictions.append(subTab)\n\ncolumns = list(submission.columns)\ncolumns.remove('sig_id')\n\ny_pred = pd.DataFrame()\ny_pred['sig_id'] = predictions[0]['sig_id']\n\nfor column in columns:\n    column_data = []\n    for i in range(len(predictions)):\n        column_data.append(predictions[i][column])\n    y_pred[column] = np.mean(column_data, axis=0)\n\ny_pred.shape\n\ntest = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\n\npublic_ids = list(submission['sig_id'].values)\ntest_ids = list(test['sig_id'].values)\nprivate_ids = list(set(test_ids)-set(public_ids))\n\ncolumns = list(submission.columns)\ncolumns.remove('sig_id')\n\nsubmission = pd.DataFrame(index = public_ids + private_ids, columns=columns)\nsubmission.index.name = 'sig_id'\n\nsubmission[:] = 0\n\np_min = 0.0005\np_max = 0.9995\n\nsubmission.loc[y_pred.sig_id,:] = np.clip(y_pred[columns].values,p_min,p_max)\n\nsubmission.loc[test[test.cp_type == 'ctl_vehicle'].sig_id] = 0\n\nsubmission.to_csv('submission.csv', index=True)\n\nsubmission.head().T","0ed35748":"tabnet2","a9602b86":"# Multi Input ResNet Model\n\n\n## This notebook is a python\/tensorflow version of [this notebook](https:\/\/www.kaggle.com\/demetrypascal\/2heads-deep-resnets-pipeline-smoothing). Please upvote the original as well if you find this work useful.","51dac6d8":"Save oof and y_pred to feed into tabnet."}}