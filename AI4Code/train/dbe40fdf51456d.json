{"cell_type":{"7b7f2d8e":"code","1c8b6331":"code","6b70fc21":"code","8181232e":"code","efcd416a":"code","f7e9d9f7":"code","0ce4a12a":"code","62e41650":"code","ce68be8a":"markdown","fd2ca98d":"markdown","a8cd63bb":"markdown","6e944292":"markdown","49379c56":"markdown"},"source":{"7b7f2d8e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 12})\nfrom itertools import product\nnp.random.seed(10)","1c8b6331":"# install EconML\n!pip install -q econml\nimport econml","6b70fc21":"def make_dataset():\n    N_features   =    30\n    N_rows       = 10000\n\n    # create the irreducible error component\n    random_noise = lambda N_rows: np.random.uniform(-1, 1, size = N_rows)\n    \n    # create the control\n    X  = np.random.uniform(0, 1, size=(N_rows,1))\n    \n    # create the true treatment effect data\n    TE = np.array([true_relationship(x_i) for x_i in X])\n\n    # create the functions mapping the support of X to \u211d\n    support_size = 5\n    support = np.random.choice(np.arange(N_features), size=support_size, replace=False)\n    coefs_Y = np.random.uniform(0, 1, size=support_size)\n    coefs_T = np.random.uniform(0, 1, size=support_size)\n\n    # create the confounders\n    W =          np.random.normal( 0, 1, size=(N_rows, N_features))\n    # create the treatment feature within W\n    T =          np.dot( W[:, support], coefs_T) + random_noise(N_rows)\n    # create the outcome (target)\n    Y = (TE*T) + np.dot( W[:, support], coefs_Y) + random_noise(N_rows)\n    \n    return (Y, T, X, W)","8181232e":"# Treatment effect function\ndef true_relationship(x):\n    return np.exp(2*x[0])\n\nY, T, X, W = make_dataset()","efcd416a":"from econml.dml       import CausalForestDML\nfrom sklearn.ensemble import RandomForestRegressor\n\nnon_parametric_model = CausalForestDML(model_y=RandomForestRegressor(random_state=0),\n                                       model_t=RandomForestRegressor(random_state=0),\n                                       criterion='mse', n_estimators=1000,\n                                       min_impurity_decrease=0.001,random_state=0)","f7e9d9f7":"X_test = np.array(list(product(np.arange(0, 1, 0.01), repeat=1)))\n\nnon_parametric_model.fit(Y, T, X=X, W=W);\nnon_parametric_model_results = non_parametric_model.effect(X_test)\nlb, ub                       = non_parametric_model.effect_interval(X_test, alpha=0.01)\n\n# plot the reults\nplt.figure(figsize=(10,6))\n# double ML results plot\nplt.plot(X_test, non_parametric_model_results, label='Non-parametric double ML')\nplt.fill_between(X_test[:, 0], lb, ub, alpha=.4)\n# ground truth plot\nground_truth = np.array([true_relationship(x_i) for x_i in X_test])\nplt.plot(X_test, ground_truth, '--', label='True effect', c='black')\nplt.ylabel('Outcome (Effect)')\nplt.xlabel('Treatment (Cause)')\nplt.legend(loc='upper left')\nplt.show();","0ce4a12a":"# Treatment effect function\ndef true_relationship(x):\n    return np.sin(np.pi*x[0])\n\nY, T, X, W = make_dataset()","62e41650":"non_parametric_model.fit(Y, T, X=X, W=W);\nnon_parametric_model_results = non_parametric_model.effect(X_test)\nlb, ub                       = non_parametric_model.effect_interval(X_test, alpha=0.01)\n# plot the reults\nplt.figure(figsize=(10,6))\n# double ML results plot\nplt.plot(X_test, non_parametric_model_results, label='Non-parametric double ML')\nplt.fill_between(X_test[:, 0], lb, ub, alpha=.4)\n# ground truth plot\nground_truth = np.array([true_relationship(x_i) for x_i in X_test])\nplt.plot(X_test, ground_truth, '--', label='True effect', c='black')\nplt.ylabel('Outcome (Effect)')\nplt.xlabel('Treatment (Cause)')\nplt.legend(loc='lower center')\nplt.show();","ce68be8a":"### Example 2.\nThe ground truth relationship between the treatment feature and the outcome is given by \n$$ \\theta(x) = \\sin(\\pi x)$$","fd2ca98d":"As can be seen the results are more than satisfactory.\n\nIn a forthcoming notebook I shall apply double ML to some '*real-world*' data.\n\n# References, links, and related reading\n\nDouble ML\n* [Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, James Robins \"*Double\/debiased machine learning for treatment and structural parameters*\",  The Econometrics Journal, Volume 21, Pages C1-C68 (2018)](https:\/\/doi.org\/10.1111\/ectj.12097)\n* [Alexandre Belloni, Victor Chernozhukov, and Christian Hansen \"*Inference on Treatment Effects after Selection among High-Dimensional Controls*\", The Review of Economic Studies, Volume 81, Pages 608-650 (2014)](https:\/\/doi.org\/10.1093\/restud\/rdt044)\n\nCausal Forests\n* [Susan Athey and Guido W. Imbens \"*Recursive partitioning for heterogeneous causal effects*\", PNAS volume **113** pages 7353-7360 (2016)](https:\/\/www.pnas.org\/content\/pnas\/113\/27\/7353.full.pdf)\n* [Susan Athey, Guido W. Imbens, and Stefan Wager \"*Approximate residual balancing: debiased inference of average treatment effects in high dimensions*\",  Journal of the Royal Statistical Society Series B (Statistical Methodology) volume **80** pages 597-623 (2018)](https:\/\/doi.org\/10.1111\/rssb.12268)\n* [Stefan Wager and Susan Athey \"*Estimation and Inference of Heterogeneous Treatment Effects using Random Forests*\", Journal of the American Statistical Association\nvolume **113** pages 1228-1242 (2018)](https:\/\/doi.org\/10.1080\/01621459.2017.1319839)\n* [Susan Athey, Stefan Wager \"*Estimating Treatment Effects with Causal Forests: An Application*\", arXiv:1902.07409 (2019)](https:\/\/arxiv.org\/pdf\/1902.07409.pdf)\n\nPackages\n\n* [EconML](https:\/\/github.com\/Microsoft\/EconML) (GitHub)\n* [EconML documentation](https:\/\/econml.azurewebsites.net\/)\n* [Causal ML: A Python Package for Uplift Modeling and Causal Inference with ML](https:\/\/github.com\/uber\/causalml) (GitHub)\n* [DoubleML - Double Machine Learning in Python](https:\/\/github.com\/DoubleML\/doubleml-for-py) (GitHub). See also: [Philipp Bach, Victor Chernozhukov, Malte S. Kurz, and Martin Spindler \"*DoubleML - An Object-Oriented Implementation of Double Machine Learning in Python*\", arXiv:2104.03220 (2021)](https:\/\/arxiv.org\/pdf\/2104.03220.pdf)\n\nRelated reading\n\n* [Judea Peral \"*The Book of Why: The New Science of Cause and Effect*\", Basic Books (2018)](https:\/\/www.basicbooks.com\/titles\/judea-pearl\/the-book-of-why\/9780465097616\/)\n* [Eleanor Dillon, Jacob LaRiviere, Scott Lundberg, Jonathan Roth, and Vasilis Syrgkanis \"*Be careful when interpreting predictive models in search of causal insights*\"](https:\/\/shap.readthedocs.io\/en\/latest\/example_notebooks\/overviews\/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%C2%A0insights.html)\n","a8cd63bb":"# Two examples\n### The synthetic dataset\nWe create a synthetic dataset consisting of 30 features with 10k rows of data.\nThe synthetic data follows the the partially linear regression (PLR) model where we have the outcome given by\n$$ Y = T\\theta + g(X) + \\epsilon$$\nwhere $\\theta$ is the causal, or 'lift' parameter that we are interested in calculating. Also we have\n$$ T = m(X) + \\epsilon$$\nwhere $\\epsilon$ are the irreducible error contributions, and $g$ and $m$ are the nuisance  functions. The first equation is the main equation, whilst the second equation is dedicated to the confounding and is analogous to the [omitted-variable bias](https:\/\/en.wikipedia.org\/wiki\/Omitted-variable_bias).\n\nIn the synthetic data\n* Y is the target \n* T is the feature of interest\n* X are the control vectors $ \\sim \\mathcal{U} (0,1)$\n* W is a matrix of the confounders $ \\sim \\mathcal{N} (0,1)$","6e944292":"# Causality: Double ML examples using EconML and Causal Forests\n\nMuch of what we do on kaggle falls under the category of [predictive modelling](https:\/\/en.wikipedia.org\/wiki\/Predictive_modelling); we are given a \nsample of data with which to build a model, and assuming that the data is [stationary](https:\/\/en.wikipedia.org\/wiki\/Stationary_process) and that the data that the model was trained on, and the data that the model will be used on, have the same underlying distributions, we can use that model to then predict the outcome (*i.e.* the target) of new, previously unseen out-of-sample data. \n\nIt is very easy to obtain the correlations between features in ML, but [**correlation does not imply causation**](https:\/\/en.wikipedia.org\/wiki\/Correlation_does_not_imply_causation). (For some wonderful and amusing examples of where where things can go very wrong see the website [Spurious Correlations](https:\/\/tylervigen.com\/spurious-correlations)). Put simply, it is entirely possible that a correlation is the result of coincidence. The recent blog post [\"*Be careful when interpreting predictive models in search of causal insights*\"](https:\/\/shap.readthedocs.io\/en\/latest\/example_notebooks\/overviews\/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%C2%A0insights.html) is well worth reading.\n \nThere are situations where we wish to study [causality](https:\/\/en.wikipedia.org\/wiki\/Causality), or cause and effect.  The gold standard when it comes to studying cause and effect is the experiment. In the field of machine learning such experiments often take the form of [A\/B tests](https:\/\/en.wikipedia.org\/wiki\/A\/B_testing) where changes are compared and contrasted with the results of randomized control groups. However, there are numerous circumstances where this is impractical:\n\n* Historical non-experimental observational data: No more data can ever be obtained (an example is the Titanic)\n* Impossibility of performing experiments, for example in the economic or social sciences\n* We may wish to select a few potentially interesting treatments before actually performing possibly expensive and\/or time consuming A\/B tests\n\nFor the purposes of this notebook we shall define causality as the influence a single continuous treatment (i.e. feature) has on the outcome (i.e. the predictions) whilst keeping all of the other features, or \"confounders\" ($W$) constant. We shall adopt the nomenclature of the feature under investigation as the **treatment $T$**, and the effect is the **outcome** $Y$.\n\nWe shall assume that there is no **unobserved confounding** going on. There may well be a feature(s) that has a significant contribution to the target that was not included in the original dataset. Here we assume that all of the relevant features have been measured. Unobserved confounding is the bane of causality; if we do have unobserved confounders then there is little else to do other than go out and obtain data regarding these missing features for our dataset. \n\n## Double ML\nDouble ML is an ingenious development by [Victor Chernozhukov](https:\/\/www.mit.edu\/~vchern\/) and co-workers, of particular note is the publication [\"*Double\/debiased machine learning for treatment and structural parameters*\"](https:\/\/doi.org\/10.1111\/ectj.12097). The 'double' comes from the use of primary and auxiliary predictive models.\n\nThe overall process can be summarised as:\n\n*     Sample splitting: the data is divided into two equal sets\n*    Model the response based on the covariates for both splits\n*    Model the treatment based on the covariates for both splits\n* For both of the above models we shall be using the scikit-learn [`RandomForestRegressor`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html)\n*    Calculate the response and treatment residuals $ \\tilde{Y}, \\tilde{T}$\n* Finally fit the response residuals w.r.t. the treatment residuals using ML to obtain the overall treatment effect from\n\n$$ \\tilde{Y} = \\theta(X) \\cdot \\tilde{T} + \\epsilon $$\n\n## EconML\n[EconML](https:\/\/www.microsoft.com\/en-us\/research\/project\/econml\/) is a python package  developed by the ALICE team at Microsoft Research. In the following examples we shall be using the Causal Forests ([`CausalForestDML`](https:\/\/econml.azurewebsites.net\/_autosummary\/econml.dml.CausalForestDML.html)) routine, where the residuals are fitted using a non-parametric estimator.\n","49379c56":"### Example 1.\nThe ground truth relationship between the treatment feature and the outcome is given by \n$$  \\theta(x) = e^{2x}$$"}}