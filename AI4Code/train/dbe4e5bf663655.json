{"cell_type":{"bca534f0":"code","875d6d7e":"code","8c30ee89":"code","5535c0c7":"code","5b01d7f5":"code","a96520a0":"code","3a76cfdf":"code","326895cb":"code","75893e5d":"code","097a3b41":"code","d9e08716":"code","3991958c":"code","edb0380a":"code","1fc36ee4":"code","d5843338":"code","3b5b005f":"markdown","649a9128":"markdown","ea650525":"markdown","734c061a":"markdown","7a0c1c60":"markdown","abc4df93":"markdown"},"source":{"bca534f0":"import gc\nimport os\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\n\ntqdm.pandas()\n\nprint(os.listdir(\"..\/input\"))\n","875d6d7e":"TEXT_COL = 'comment_text'\nEMB_PATH = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\ntrain = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv', index_col='id')\ntest = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv', index_col='id')","8c30ee89":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(embed_dir=EMB_PATH):\n    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(open(embed_dir)))\n    return embedding_index\n\ndef build_embedding_matrix(word_index, embeddings_index, max_features, lower = True, verbose = True):\n    embedding_matrix = np.zeros((max_features, 300))\n    for word, i in tqdm(word_index.items(),disable = not verbose):\n        if lower:\n            word = word.lower()\n        if i >= max_features: continue\n        try:\n            embedding_vector = embeddings_index[word]\n        except:\n            embedding_vector = embeddings_index[\"unknown\"]\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n\ndef build_matrix(word_index, embeddings_index):\n    embedding_matrix = np.zeros((len(word_index) + 1,300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embeddings_index[word]\n        except:\n            embedding_matrix[i] = embeddings_index[\"unknown\"]\n    return embedding_matrix","5535c0c7":"maxlen = 220\nmax_features = 100000\nembed_size = 300\ntokenizer = Tokenizer(num_words=max_features, lower=True) #filters = ''\n#tokenizer = text.Tokenizer(num_words=max_features)\nprint('fitting tokenizer')\ntokenizer.fit_on_texts(list(train[TEXT_COL]) + list(test[TEXT_COL]))\nword_index = tokenizer.word_index\nX_train = tokenizer.texts_to_sequences(list(train[TEXT_COL]))\ntrain['target'] = train['target'].apply(lambda x: 1 if x > 0.5 else 0)\ny_train = train['target'].values\nX_test = tokenizer.texts_to_sequences(list(test[TEXT_COL]))\n\nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_test = pad_sequences(X_test, maxlen=maxlen)\n\n\ndel tokenizer\ngc.collect()","5b01d7f5":"embeddings_index = load_embeddings()","a96520a0":"embedding_matrix = build_matrix(word_index, embeddings_index)","3a76cfdf":"del embeddings_index\ngc.collect()","326895cb":"class Attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        \n        self.supports_masking = True\n\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n        \n        weight = torch.zeros(feature_dim, 1)\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n        \n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n        \n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim), \n            self.weight\n        ).view(-1, step_dim)\n        \n        if self.bias:\n            eij = eij + self.b\n            \n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n        \n        if mask is not None:\n            a = a * mask\n\n        a = a \/ torch.sum(a, 1, keepdim=True) + 1e-10\n\n        weighted_input = x * torch.unsqueeze(a, -1)\n        return torch.sum(weighted_input, 1)","75893e5d":"# Refactored based on reasonable remarks\n# of @ddanevskyi https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification\/discussion\/79911\nclass NeuralNet(nn.Module):\n    def __init__(self):\n        super(NeuralNet, self).__init__()\n        \n        hidden_size = 64\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        \n        self.embedding_dropout = nn.Dropout2d(0.2) \n        self.lstm = nn.GRU(embed_size, hidden_size, bidirectional=True, batch_first=True)\n        \n        self.lstm_attention = Attention(hidden_size*2, maxlen)\n        \n        self.out = nn.Linear(384, 1)\n\n        \n    def forward(self, x):\n        h_embedding = self.embedding(x)\n        h_embedding = self.embedding_dropout(h_embedding.transpose(1,2).unsqueeze(-1)).squeeze().transpose(1,2)\n\n        h_lstm, _ = self.lstm(h_embedding)\n        h_lstm_atten = self.lstm_attention(h_lstm)\n\n        avg_pool = torch.mean(h_lstm, 1)\n        max_pool, _ = torch.max(h_lstm, 1)\n        \n        conc = torch.cat((h_lstm_atten, avg_pool, max_pool), 1)\n        out = self.out(conc)\n        \n        return out\n    \ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","097a3b41":"# Stolen from https:\/\/github.com\/Bjarten\/early-stopping-pytorch\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')\n        self.val_loss_min = val_loss","d9e08716":"from sklearn.model_selection import KFold\nsplits = list(KFold(n_splits=5).split(X_train, y_train))","3991958c":"BATCH_SIZE = 2048\nNUM_EPOCHS = 100\n\ntrain_preds = np.zeros((len(X_train)))\ntest_preds = np.zeros((len(X_test)))\n\nx_test_cuda = torch.tensor(X_test, dtype=torch.long).cuda()\ntest = torch.utils.data.TensorDataset(x_test_cuda)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n\nfor i, (train_idx, valid_idx) in enumerate(splits):\n    x_train_fold = torch.tensor(X_train[train_idx], dtype=torch.long).cuda()\n    y_train_fold = torch.tensor(y_train[train_idx, np.newaxis], dtype=torch.float32).cuda()\n    x_val_fold = torch.tensor(X_train[valid_idx], dtype=torch.long).cuda()\n    y_val_fold = torch.tensor(y_train[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n    \n    model = NeuralNet()\n    model.cuda()\n    \n    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n    optimizer = torch.optim.Adam(model.parameters())\n    \n    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=BATCH_SIZE, shuffle=False)\n    \n    early_stopping = EarlyStopping(patience=3, verbose=True)\n    \n    print(f'Fold {i + 1}')\n    \n    for epoch in range(NUM_EPOCHS):\n        start_time = time.time()\n        \n        model.train()\n        avg_loss = 0.\n        for x_batch, y_batch in tqdm(train_loader, disable=True):\n            optimizer.zero_grad()\n            y_pred = model(x_batch)\n            loss = loss_fn(y_pred, y_batch)\n            loss.backward()\n            optimizer.step()\n            avg_loss += loss.item() \/ len(train_loader)\n        \n        model.eval()\n        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n        test_preds_fold = np.zeros(len(X_test))\n        avg_val_loss = 0.\n        for i, (x_batch, y_batch) in enumerate(valid_loader):\n            y_pred = model(x_batch).detach()\n            avg_val_loss += loss_fn(y_pred, y_batch).item() \/ len(valid_loader)\n            valid_preds_fold[i * BATCH_SIZE:(i+1) * BATCH_SIZE] = sigmoid(y_pred.cpu().numpy())[:, 0]\n        \n        elapsed_time = time.time() - start_time \n        print('Epoch {}\/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n            epoch + 1, NUM_EPOCHS, avg_loss, avg_val_loss, elapsed_time))\n        \n        early_stopping(avg_val_loss, model)\n        \n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break\n        \n        \n    # load the last checkpoint with the best model\n    model.load_state_dict(torch.load('checkpoint.pt'))\n    \n    for i, (x_batch,) in enumerate(test_loader):\n        y_pred = model(x_batch).detach()\n\n        test_preds_fold[i * BATCH_SIZE:(i+1) * BATCH_SIZE] = sigmoid(y_pred.cpu().numpy())[:, 0]\n\n    train_preds[valid_idx] = valid_preds_fold\n    test_preds += test_preds_fold \/ len(splits)    ","edb0380a":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train>0.5, train_preds)","1fc36ee4":"submission = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv', index_col='id')\nsubmission['prediction'] = test_preds\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.head()","d5843338":"submission.to_csv('submission.csv', index=False)","3b5b005f":"# Training","649a9128":"# Data preprocessing","ea650525":"Nothig particularly exciting. Pytorch implementation of a great starting kernel https:\/\/www.kaggle.com\/christofhenkel\/keras-baseline-lstm-attention-5-fold\n\nNotebook is based on https:\/\/www.kaggle.com\/hung96ad\/pytorch-starter","734c061a":"# Model","7a0c1c60":"# Dependencies","abc4df93":"# Submission"}}