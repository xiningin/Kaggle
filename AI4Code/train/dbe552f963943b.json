{"cell_type":{"24adf0f1":"code","573c264b":"code","c7b13314":"code","b952bbd1":"code","6ebd8158":"code","6f7e542e":"code","c05f7490":"code","e8f63a8e":"code","58ec648a":"code","b3de8486":"code","1eb7b919":"code","f4bd31b5":"code","4e9cea37":"code","6dfc619d":"code","3659f1d2":"code","664f2f83":"code","9b289725":"code","4478ea1d":"code","1028d20e":"code","c3c8a8ae":"code","4feafeef":"code","f2ec40ac":"code","9a2790d0":"code","34bf03f3":"code","203e945c":"code","a1087d6e":"markdown","9c29dc54":"markdown","02e3a1bd":"markdown","9bd0629a":"markdown","bcd2dcc0":"markdown","016748e7":"markdown","e067544f":"markdown","162228fb":"markdown","661d8338":"markdown","10692175":"markdown","f256d883":"markdown","5753189d":"markdown","8fc2946a":"markdown","40790341":"markdown","419db35b":"markdown","0a37bf2e":"markdown","fb145938":"markdown"},"source":{"24adf0f1":"#%reset\n#!pip install pyspellchecker\n#!pip install googletrans\nimport pandas as pd\nimport matplotlib as plt\nimport re\nimport numpy as np\n#from spellchecker import SpellChecker\nfrom nltk.stem import WordNetLemmatizer\n#from googletrans import Translator\nfrom textblob import Word\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nimport nltk\nimport gensim\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nimport gc\ngc.enable()\n!pip install emoji\nimport emoji\nfrom nltk.corpus import wordnet\nimport datetime\nimport time\nimport operator\nfrom textblob import TextBlob\nfrom tqdm import tqdm, trange\nfrom nltk.tokenize import TweetTokenizer\n\nfrom IPython.display import clear_output\nclear_output()","573c264b":"dtypesDict_tr = {\n'id'                            :         'int32',\n'target'                        :         'float16',\n'severe_toxicity'               :         'float16',\n'obscene'                       :         'float16',\n'identity_attack'               :         'float16',\n'insult'                        :         'float16',\n'threat'                        :         'float16',\n'asian'                         :         'float16',\n'atheist'                       :         'float16',\n'bisexual'                      :         'float16',\n'black'                         :         'float16',\n'buddhist'                      :         'float16',\n'christian'                     :         'float16',\n'female'                        :         'float16',\n'heterosexual'                  :         'float16',\n'hindu'                         :         'float16',\n'homosexual_gay_or_lesbian'     :         'float16',\n'intellectual_or_learning_disability':    'float16',\n'jewish'                        :         'float16',\n'latino'                        :         'float16',\n'male'                          :         'float16',\n'muslim'                        :         'float16',\n'other_disability'              :         'float16',\n'other_gender'                  :         'float16',\n'other_race_or_ethnicity'       :         'float16',\n'other_religion'                :         'float16',\n'other_sexual_orientation'      :         'float16',\n'physical_disability'           :         'float16',\n'psychiatric_or_mental_illness' :         'float16',\n'transgender'                   :         'float16',\n'white'                         :         'float16',\n'publication_id'                :         'int8',\n'parent_id'                     :         'float32',\n'article_id'                    :         'int32',\n'funny'                         :         'int8',\n'wow'                           :         'int8',\n'sad'                           :         'int8',\n'likes'                         :         'int16',\n'disagree'                      :         'int16',\n'sexual_explicit'               :         'float16',\n'identity_annotator_count'      :         'int16',\n'toxicity_annotator_count'      :         'int16'\n}","c7b13314":"train_data = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")","b952bbd1":"# kill all other columns except comment text\ncols_to_keep = ['comment_text','target']\ntrain_data = train_data.drop(train_data.columns.difference(cols_to_keep), axis=1)\ngc.collect()","6ebd8158":"def replace_contractions(text):\n  \n  \"\"\"\n  This functions check's whether a text contains contractions or not. \n  In case a contraction is found, the corrected value from the dictionary is \n  returned.\n  Example: \"I've\" towards \"I have\"\n  \"\"\"\n  \n  #replace words with \"'ve\" to \"have\"\n  matches = re.findall(r'\\b\\w+[\\'`\u00b4]ve\\b', text)\n  if len(matches) != 0:\n    text = re.sub(r'[\\'`\u00b4]ve\\b', \" have\", text)\n  \n  #replace words with \"'re\" to \"are\"\n  matches = re.findall(r'\\b\\w+[\\'`\u00b4]re\\b', text)\n  if len(matches) != 0:\n    text = re.sub(r'[\\'`\u00b4]re\\b', \" are\", text)\n  \n  #replace words with \"'ll\" to \"will\"\n  matches = re.findall(r'\\b\\w+[\\'`\u00b4]ll\\b', text)\n  if len(matches) != 0:\n    text = re.sub(r'[\\'`\u00b4]ll\\b', \" will\", text)\n  \n  #replace words with \"'m\" to \"am\"\n  matches = re.findall(r'\\b\\w+[\\'`\u00b4]m\\b', text)\n  if len(matches) != 0:\n    text = re.sub(r'[\\'`\u00b4]m\\b', \" am\", text)\n  \n  #replace words with \"'d\" to \"would\"\n  matches = re.findall(r'\\b\\w+[\\'`\u00b4]d\\b', text)\n  if len(matches) != 0:\n    text = re.sub(r'[\\'`\u00b4]d\\b', \" would\", text)\n  \n  #replace words with contraction according to the contraction_dict\n  matches = re.findall(r'\\b\\w+[\\'`\u00b4]\\w+\\b', text)\n  for x in matches:\n    if x in contraction_dict.keys():\n      text = text.replace(x, contraction_dict.get(x))\n  \n  # replace all \"'s\" by space\n  matches = re.findall(r'\\b\\w+[\\'`\u00b4]s\\b', text)\n  if len(matches) != 0:\n    text = re.sub(r'[\\'`\u00b4]s\\b', \" \", text)\n  return text\n\n# Dictionary of contractions coming out of the pre-investigation in the other kernel\ncontraction_dict = {\"Can't\":\"can not\", \"Didn't\":\"did not\", \"Doesn't\":\"does not\", \n                    \"Isn't\":\"is not\", \"Don't\":\"do not\", \"Aren't\":\"are not\", \"#\":\"\",\n                    \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\",\n                    \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n                    \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\",\n                    \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n                    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n                    \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\",\n                    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n                    \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\",\n                    \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n                    \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n                    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n                    \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n                    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n                    \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n                    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n                    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n                    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n                    \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\",\n                    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n                    \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n                    \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n                    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n                    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n                    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n                    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n                    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n                    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n                    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n                    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n                    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n                    \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n                    \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n                    \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\n                    \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n                    \"you're\": \"you are\", \"you've\": \"you have\", \"c'mon\":\"come on\",\n                    \"Don''t\":\"do not\", \"Haden't\":\"had not\", \"Grab'em\":\"grab them\", \"USA''s\":\"USA\",\n                    \"Pick'em\":\"pick them\", \"I'lll\":\"I will\", \"Tell'em\":\"tell them\", \"Y'all\":\"you all\",\n                    \"Wouldn't\":\"would not\", \"Shouldn't\":\"should not\", \"I'DVE\":\"I would have\",\n                    \"SHOOT'UM\":\"shoot them\", \"CANN'T\":\"can not\", \"COUD'VE\":\"could have\", \"Yo'ure\":\"you are\",\n                    \"LOCK'EM\":\"lock them\", \"G'night\":\"goodnight\", \"W'ell\":\"we will\", \"IT'D\":\"it would\",\n                    \"Couldn't\":\"could not\", \"LOCK'UM\":\"lock them\", \"WOULD'NT\":\"would not\", \"Cant't\":\"can not\",\n                    \"HADN'T\":\"had not\", \"It''s\":\"it is\", \"Don'ts\":\"do not\", \"Arn't\":\"are not\",\n                    \"We'll\":\"we will\", \"G'Night\":\"goodnight\", \"THAT'LL\":\"that will\", \"Dpn't\":\"do not\",\n                    \"Idon'tgetitatall\":\"I do not get it at all\", \"THEY'VE\":\"they have\", \"Le'ts\":\"let us\",\n                    \"SEND'EM\":\"send them\", \"AIN'T\":\"is not\", \"WE'D\":\"we would\", \"I'vemade\":\"I have made\",\n                    \"SHE'LL\":\"she will\", \"I'llbe\":\"I will be\", \"I'mma\":\"I am a\", \"Could'nt\":\"could not\",\n                    \"You'very\":\"you are very\", \"Light'em\":\"light them\", \"Con't\":\"can not\", \"I'\u039c\":\"I am\",\n                    \"Kick'em\":\"kick them\", \"Shoudn't\":\"should not\", \"That''s\":\"that is\",\n                    \"Didn't_work\":\"did not work\", \"You'rethinking\":\"you are thinking\", \"Dn't\":\"do not\",\n                    \"CON'T\":\"can not\", \"DON'T\":\"do not\", \"C'Mon\":\"come on\", \"You'res\":\"you are\",\n                    \"Amn't\":\"is not\", \"WE'RE\":\"we are\", \"Can't\":\"can not\", \"Kouldn't\":\"could not\",\n                    \"SHouldn't\":\"should not\", \"Does't\":\"does not\", \"COULD'VE\":\"could have\",\n                    \"TrumpIDin'tCare\":\"Trump did not care\", \"Iv'e\":\"I have\", \"Dose't\":\"does not\",\n                    \"DOESEN'T\":\"does not\", \"Give'em\":\"give them\", \"Won'tdo\":\"will not do\",\n                    \"They'l\":\"they will\", \"He''s\":\"he is\", \"I'veve\":\"I have\", \"Wern't\":\"were not\",\n                    \"Pay'um\":\"pay them\", \"She''l\":\"she will\", \"Y'know\":\"you know\", \"DIdn't\":\"did not\",\n                    \"O'bamacare\":\"Obamacare\", \"I'ma\":\"I am a\", \"Ma'am\":\"madam\", \"WASN'T\":\"was not\",\n                    \"Dont't\":\"do not\", \"Is't\":\"is not\", \"OU'RE\":\"you are\", \"YOU'RE\":\"you are\",\n                    \"Ther'es\":\"there is\", \"C'mooooooon\":\"come on\", \"They_didn't\":\"they did not\",\n                    \"Som'thin\":\"something\", \"Love'em\":\"love them\", \"You''re\":\"you are\", \"I'D\":\"I would\",\n                    \"HASN'T\":\"has not\", \"WOULD'VE\":\"would have\", \"WAsn't\":\"was not\", \"ARE'NT\":\"are not\",\n                    \"Dowsn't\":\"does not\", \"It'also\":\"it is also\", \"Geev'um\":\"give them\", \"Theyv'e\":\"they have\",\n                    \"Theyr'e\":\"they are\", \"Take'em\":\"take them\", \"Book'em\":\"book them\", \"Havn't\":\"have not\",\n                    \"DOES'NT\":\"does not\", \"Who''s\":\"who is\", \"WON't\":\"will not\", \"I'Il\":\"I will\",\n                    \"I'don\":\"I do not\", \"AREN'T\":\"are not\", \"Ev'rybody\":\"everybody\", \"Hold'um\":\"hold them\",\n                    \"WE'LL\":\"we will\", \"Cab't\":\"can not\", \"IJustDon'tThink\":\"I just do not think\",\n                    \"Wouldn'T\":\"would not\", \"U'r\":\"you are\", \"I''ve\":\"I have\", \"DONT'T\":\"do not\",\n                    \"G'morning\":\"good morning\", \"You'ld\":\"you would\", \"We''ll\":\"we will\", \"YOUR'E\":\"you are\",\n                    \"TrumpDoesn'tCare\":\"Trump does not care\", \"Wasn't\":\"was not\", \"You'all\":\"you all\",\n                    \"Y'ALL\":\"you all\", \"G'bye\":\"goodbye\", \"YOU'VE\":\"you have\", \"Does'nt\":\"does not\",\n                    \"Don'TCare\":\"do not care\",  \"Weren't\":\"were not\", \"Y'All\":\"you all\", \"They'lll\":\"they will\",\n                    \"You'reOnYourOwnCare\":\"you are on your own care\", \"I'veposted\":\"I have posted\",\n                    \"Run'em\":\"run them\", \"Vote'em\":\"vote them\", \"Would't\":\"would not\", \"I'l\":\"I will\",\n                    \"Ddn't\":\"did not\", \"I'mm\":\"I am\", \"Sshouldn't\":\"should not\", \"Your'e\":\"you are\",\n                    \"I'v\":\"I have\", \"We'really\":\"we are really\", \"DOESN'T\":\"does not\", \"DiDn't\":\"did not\",\n                    \"Needn't\":\"need not\", \"They'er\":\"they are\", \"Look'em\":\"look them\", \"I'v\u00c8\":\"I have\",\n                    \"Didn`t\":\"did not\", \"I'lll\":\"I will\", \"Wouldn't\":\"would not\", \"It`s\":\"it is\", \"What's\":\"what is\",\n                    \"ISN`T\":\"is not\", \"WE'RE\":\"we are\", \"Are'nt\":\"are not\", \"DOesn't\":\"does not\", \"I'M\":\"I am\",\n                    \"WON'T\":\"will not\", \"WEREN'T\":\"were not\", \"TrumpDon'tCareAct\":\"Trump do not care act\",\n                    \"HAVEN'T\":\"have not\", \"That''s\":\"that is\", \"Do'nt\":\"do not\"}","6f7e542e":"def replace_symbol_special(text,check_vocab=False, vocab=None): \n\n    ''' \n    This method can be used to replace dashes ('-') around and within the words using regex.\n    It only removes dashes for words which are not known to the vocabluary.\n    Next to that, common word separators like underscores ('_') and slashes ('\/') are replaced by spaces. \n    '''\n\n        \n    # replace all dashes and abostropes at the beginning of a word with a space\n    matches = re.findall(r\"\\s+(?:-|')\\w*\", text)\n    # if there is a match is in text\n    if len(matches) != 0:\n      # remove the dash from the match or better text\n      for match in matches:\n        text = re.sub(match, re.sub(r\"(?:-|')\", ' ', match), text)\n    \n    # replace all dashes and abostrophes at the end of a word with a space\n    # function works as above\n    matches = re.findall(r\"\\w*(?:-|')\\s+\", text)\n    if len(matches) != 0:\n      for match in matches:\n        text = re.sub(match, re.sub(r\"(?:-|')\", ' ', match), text)\n    \n    if check_vocab == True:\n      # replace dashes and abostrophes in the middle of the word only in case it is not known to a dictionary\n      # function works as above\n      matches = re.findall(r\"\\w*(?:-|')\\w*\", text)\n      if len(matches) != 0:\n        for match in matches:\n          #check if the word with dash in the middle in in the vocabluary\n          if match not in vocab.keys():\n            text = re.sub(match, re.sub(r\"(?:-|')\", ' ', match), text)\n    \n    #\n    text = re.sub(r'(?:_|\\\/)', ' ', text)\n    \n    text = re.sub(r' +', ' ', text)#-\n    return text\n  \n# Initially we consideredto remove the dash for words with this beginning. \n# However we found that it had almost no impact. Applying it to the total text, would kill correct spellings.\n# pre_suffix_dict = {'bi-':'bi', \t'co-':'co','re-':'re',\t'de-':'de','pre-':'pre',\t'sub-':'sub', 'un-':'un'}","c05f7490":"\n\ndef find_smilies(text):\n  \n  '''\n  For investigation only: Find most common keyboard typed smilies in the text.\n  '''\n  \n  #define a pattern to find typical keyboard smilies\n  pattern = r\"((?:3|<)?(?::|;|=|B)(?:-|'|'-)?(?:\\)|D|P|\\*|\\(|o|O|\\]|\\[|\\||\\\\|\\\/)\\s)\"\n  # Find the matches n the text\n  matches = re.findall(pattern, text)\n  # If the text contain matches print the text and the smilies found\n  if len(matches) != 0:\n    print(text, matches)\n    \n    \n\n    \ndef replace_smilies(text):\n  \n  '''\n  Simplyfied method to replace keyboard smilies with its very simple translation.\n  '''\n  \n  #Find and replace all happy smilies\n  matches = re.findall(r\"((?:<|O|o|@)?(?::|;|=|B)(?:-|'|'-)?(?:\\)|\\]))\", text)\n  if len(matches) != 0:\n    text = re.sub(r\"((?:<|O|o|@)?(?::|;|=|B)(?:-|'|'-)?(?:\\)|\\]))\", \" smile \", text)\n  \n  #Find and replace all laughing smilies\n  matches = re.findall(r\"((?:<)?(?::|;|=)(?:-|'|'-)?(?:d|D|P|p)\\b)\", text)\n  if len(matches) != 0:\n    text = re.sub(r\"((?:<)?(?::|;|=)(?:-|'|'-)?(?:d|D|P|p)\\b)\", \" smile \", text)\n  \n  #Find and replace all unhappy smilies\n  matches = re.findall(r\"((?:3|<)?(?::|;|=|8)(?:-|'|'-)?(?:\\(|\\[|\\||\\\\|\\\/))\", text)\n  if len(matches) != 0:\n    text = re.sub(r\"((?:3|<)?(?::|;|=|8)(?:-|'|'-)?(?:\\(|\\[|\\||\\\\|\\\/))\", \" unhappy \", text)\n  \n  #Find and replace all kissing smilies\n  matches = re.findall(r\"((?:<)?(?::|;|=)(?:-|'|'-)?(?:\\*))\", text)\n  if len(matches) != 0:\n    text = re.sub(r\"((?:<)?(?::|;|=)(?:-|'|'-)?(?:\\*))\", \" kiss \", text)\n  \n  #Find and replace all surprised smilies\n  matches = re.findall(r\"((?::|;|=)(?:-|'|'-)?(?:o|O)\\b)\", text)\n  if len(matches) != 0:\n    text = re.sub(r\"((?::|;|=)(?:-|'|'-)?(?:o|O)\\b)\", \" surprised \", text)\n    \n  #Find and replace all screaming smilies\n  matches = re.findall(r\"((?::|;|=)(?:-|'|'-)?(?:@)\\b)\", text)\n  if len(matches) != 0:\n    text = re.sub(r\"((?::|;|=)(?:-|'|'-)?(?:@)\\b)\", \" screaming \", text)\n    \n  #Find and replace all hearts\n  matches = re.findall(r\"\u2665|\u2764|<3|\u2765|\u2661\", text)\n  if len(matches) != 0:\n    text = re.sub(r\"(?:\u2665|\u2764|<3|\u2765|\u2661)\", \" love \", text)\n  \n  text = re.sub(' +', ' ',text)\n  return text","e8f63a8e":"def remove_stopwords(text, stop_words):\n  \n  ''' \n  Remove stopwords and multiple whitespaces around words\n  '''\n  \n  #Compile stopwords separated by | and stopped by word boundary \n  stopword_re = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b')\n  # Replace the stopwords by space\n  text = stopword_re.sub(' ', text)\n  #Replace double spaces by a single space\n  text = re.sub(' +', ' ',text)\n  return text","58ec648a":"def clean_text(text, scope='general'):\n  \n  '''\n  This function handles text cleaning from various symbols.\n  - it translates special font types into the standard text type of python.\n  - it removes all symbols except for dashes and abostrophes being handled by \n    \"replace_symbol_special\".\n  - it handles multi letter appearances like \"comiiii\" > \"comi\"\n  - typical unknown words like \"Trump\"\n  '''\n  \n  \n  \n  #compile all special symbols from the dictionary to one regex function\n  translate_regex = re.compile(r'(' + r'|'.join(translate_dictionary.keys()) + r')')\n  \n  # find all matches of special symbols in the text\n  matches = re.findall(translate_regex, text)\n  # if there is one or more matches\n  if len(matches) != 0:\n    for x in matches:\n      if x in translate_dictionary.keys():\n        #replace the symbol by its replacement item\n        text = re.sub(x, translate_dictionary.get(x), text)\n  \n  # find and remove all \"http\" links\n  matches = re.findall(r'http\\S+', text)\n  if len(matches) != 0:\n    text = re.sub(r'http\\S+', '', text)\n  \n  #remove all backslashes\n  matches = re.findall(r'\\\\', text)\n  if len(matches) != 0:\n    text = re.sub(r'\\\\', ' ', text)\n  \n  # compile all remaining special characters into one translate line and replace them by space\n  # the translate function is really fast thus here our preferred choice\n  text = text.translate(str.maketrans(''.join(puncts), len(''.join(puncts))*' '))  \n  \n  #find words where 4 repetitions of a letter goes in a row and reduce them to only one\n  #we are not correcting words with 2 or three identical letters in a row as this could destroy correct words\n  #first find repeating characters\n  matches = re.findall(r'(.)\\1{3,}', text)\n  # is some are found\n  if len(matches) != 0:\n    #for each match replace it with its first letter (x[0])\n    for x in matches:\n      character_re = re.compile(x + '{3,}')\n      matchesInside = re.findall(character_re, text)\n      if len(matchesInside) != 0:\n        for x in matchesInside:\n          text = re.sub(x, x[0], text)\n          \n  # hahaha s by one haha \n  matches = re.findall(r'\\b[h,a]{4,}\\b', text)\n  if len(matches) != 0:\n    text = re.sub(r'\\b[h,a]{4,}\\b', 'haha', text)\n  \n  # as we found many unknown word variations including 'Trump' we reduce thse  words just to Trump\n  # being represented in most word vectors\n  matches = re.findall(r'\\w*[Tt][Rr][uU][mM][pP]\\w*', text)\n  if len(matches) != 0:\n    for x in matches:\n      text = re.sub(x, 'Trump', text)\n      \n  #remove potential double spaces generated during processing        \n  text = re.sub(' +', ' ',text) \n  \n  # those symbols are not touched by this function ->see replace_contraction or replace_special_symbols\n  #keep = [\"'\", '-', '\u00b4']\n  \n  \n  return text\n\n\n\n\n\n# The dictionary was generated in the compare and investigation phase in the other notebook\ntranslate_dictionary = {'\\t': 't', '0': '0', '1': '1', '2': '2', '3': '3', '5': '5', '6': '6',\n                         '8': '8', '9': '9', 'd': 'd', 'e': 'e', 'h': 'h', 'm': 'm', 't': 't',\n                         '\u00b2': '2', '\u00b9': '1', '\u011d': 'g', '\u0153': 'ae', '\u015d': 's', '\u01e7': 'g', '\u0251': '\u0251',\n                         '\u0252': 'a', '\u0254': 'c', '\u0259': 'e', '\u025b': 'e', '\u0261': 'g', '\u0262': 'g', '\u026a': 'i',\n                         '\u0274': 'n', '\u0280': 'r', '\u028f': 'y', '\u0299': 'b', '\u029c': 'h', '\u029f': 'l', '\u02b0': 'h',\n                         '\u02b3': 'r', '\u02b7': 'w', '\u02b8': 'y', '\u02e2': '5', '\u035e': '-', '\u035f': '_', '\u0366': 'o',\n                         '\u0391': 'a', '\u0392': 'b', '\u0395': 'e', '\u039c': 'm', '\u039d': 'n', '\u039f': 'o', '\u03a4': 't',\n                         '\u03ad': 'e', '\u03af': 'i', '\u03b1': 'a', '\u03ba': 'k', '\u03c7': 'x', '\u0406': 'i', '\u0410': 'a',\n                         '\u0411': 'e', '\u0415': 'e', '\u0417': '#', '\u0418': 'n', '\u041a': 'k', '\u041c': 'm', '\u041d': 'h',\n                         '\u041e': 'o', '\u0420': 'p', '\u0421': 'c', '\u0423': 'y', '\u0425': 'x',  '\u0432': 'b',\n                         '\u043a': 'k', '\u043c': 'm', '\u043d': 'h', '\u044b': 'bi', '\u044c': 'b', '\u0451': 'e', '\u0459': 'jb',\n                         '\u0493': 'f', '\u04af': 'y', '\u051c': 'w', '\u0570': 'h', '\u05d0': 'n', '\u0be6': '0', '\u0c66': 'o',\n                         '\u0d66': 'o', '\u0ed0': 'o', '\u13a5': 'i', '\u13ab': 'j', '\u13e7': 'd', '\u1428': '-', '\u1438': '<',\n                         '\u1472': 'b', '\u1473': 'b', '\u15de': 'd', '\u1d00': 'a', '\u1d04': 'c', '\u1d05': 'n', '\u1d07': 'e',\n                         '\u1d0a': 'j', '\u1d0b': 'k', '\u1d0d': 'm', '\u1d0f': 'o', '\u1d11': 'o', '\u1d18': 'p', '\u1d1b': 't',\n                         '\u1d1c': 'u', '\u1d20': 'v', '\u1d21': 'w', '\u1d35': 'i', '\u1d37': 'k', '\u1d3a': 'n', '\u1d3c': 'o',\n                         '\u1d49': 'e', '\u1d52': 'o', '\u1d57': 't', '\u1d58': 'u', '\u1e83': 'w', '\u1f00': 'a', '\u1f08': 'a',\n                         '\u1f0c': 'a', '\u1f76': 'l', '\u1f7a': 'u', '\u2012': '-', '\u2081': '1', '\u2083': '3', '\u2084': '4',\n                         '\u210b': 'h', '\u2120': 'sm', '\u212f': 'e', '\u2134': 'c', '\u254c': '--', '\u2c8f': 'h', '\u2ca3': 'p',\n                         '\u4e0b': 'under', '\u4e0d': 'Do not', '\u4eba': 'people', '\u4f0e': 'trick', '\u4f1a': 'meeting',\n                         '\u4f5c': 'Make', '\u4f60': 'you', '\u514b': 'Gram', '\u5173': 'turn off', '\u522b': 'do not',\n                         '\u52a0': 'plus', '\u534e': 'China', '\u5356': 'Sell', '\u53bb': 'go with', '\u54e5': 'brother',\n                         '\u56ed': 'garden', '\u56fd': 'country', '\u5706': 'circle', '\u571f': 'soil', '\u5730': 'Ground',\n                         '\u574f': 'Bad', '\u5916': 'outer', '\u5927': 'Big', '\u5931': 'Lost', '\u5b50': 'child', '\u5c0f': 'small',\n                         '\u6210': 'to make', '\u6226': 'War', '\u6240': 'Place', '\u62ff': 'take', '\u6545': 'Therefore',\n                         '\u6587': 'Text', '\u660e': 'Bright', '\u662f': 'Yes', '\u6709': 'Have', '\u6b4c': 'song', \n                         '\u6b8a': 'special', '\u6cb9': 'oil', '\u6e29': 'temperature', '\u7279': 'special', \n                         '\u7344': 'prison', '\u7684': 'of', '\u7a0e': 'tax', '\u7cfb': 'system', '\u7fa4': 'group',\n                         '\u821e': 'dance', '\u82f1': 'English', '\u8521': 'Cai', '\u8bae': 'Discussion', '\u8c37': 'Valley',\n                         '\u8c46': 'beans', '\u90fd': 'All', '\u94b1': 'money', '\u964d': 'drop', '\u969c': 'barrier',\n                         '\u9a97': 'cheat', '\uc138': 'three', '\uc548': 'within', '\uc601': 'spirit', '\uc694': 'Yo',\n                          '\u037a': '', '\u039b': 'L', '\u039e': 'X', '\u03ac': 'a', '\u03ae': 'or', '\u03b9': 'j',\n                         '\u03be': 'X', '\u03c2': 's', '\u03c8': 't', '\u03cc': 'The', '\u03cd': 'gt;', '\u03ce': 'o',\n                         '\u03d6': 'e.g.', '\u0413': 'R', '\u0414': 'D', '\u0416': 'F', '\u041b': 'L', '\u041f': 'P', \n                         '\u0424': 'F', '\u0428': 'Sh', '\u0431': 'b', '\u043f': 'P', '\u0444': 'f', '\u0446': 'c', \n                         '\u0447': 'no', '\u0448': 'sh', '\u0449': 'u', '\u044d': 'uh', '\u044e': 'Yu', '\u0457': 'her',\n                         '\u045b': 'ht', '\u0541': 'Winter', '\u0561': 'a', '\u0564': 'd', '\u0565': 'e', '\u056b': 's',\n                         '\u0571': 'h', '\u0574': 'm', '\u0575': 'y', '\u0576': 'h', '\u057c': 'r', '\u057d': 'c', \n                         '\u0580': 'p', '\u0582': '\u00b3', '\u05d1': 'B', '\u05d3': 'D', '\u05d4': 'God', '\u05d5': 'and',\n                         '\u05d8': 'ninth', '\u05d9': 'J', '\u05da': 'D', '\u05db': 'about', '\u05dc': 'To', '\u05dd': 'From', \n                         '\u05de': 'M', '\u05df': 'Estate', '\u05e0': 'N', '\u05e1': 'S.', '\u05e2': 'P', '\u05e3': 'Jeff',\n                         '\u05e4': 'F', '\u05e6': 'C', '\u05e7': 'K.', '\u05e8': 'R.', '\u05e9': 'That', '\u05ea': 'A',\n                         '\u0621': 'Was', '\u0622': 'Ah', '\u0623': 'a', '\u0625': 'a', '\u0627': 'a', '\u0629': 'e', \n                         '\u062a': 'T', '\u062c': 'C', '\u062d': 'H', '\u062e': 'Huh', '\u062f': 'of the', '\u0631': 'T',\n                         '\u0632': 'Z', '\u0633': 'Q', '\u0634': 'Sh', '\u0635': 's', '\u0637': 'I', '\u0639': 'AS', '\u063a': 'G',\n                         '\u0641': 'F', '\u0642': 'S', '\u0643': 'K', '\u0644': 'to', '\u0645': 'M', '\u0646': 'N', '\u0647': 'e', \n                         '\u0648': 'And', '\u0649': 'I', '\u064a': 'Y', '\u0686': 'What', '\u06a9': 'K', '\u06cc': 'Y', \n                         '\u0915': 'A', '\u092e': 'M', '\u0930': 'And', '\u0a97': 'C', '\u0a9c': 'The same', \n                         '\u0aa4': 'I', '\u0ab0': 'I', '\u0b9c': 'SAD', '\u10da': 'L', '\u1e51': 'o', '\u1f10': 'e',\n                         '\u1f14': '\u00cb', '\u1f21': 'or', '\u1f31': '\u0131', '\u1f34': 'i', '\u1f40': 'The', '\u1f41': 'The',\n                         '\u1f50': '\u00ff', '\u1f70': 'a', '\u1f72': '.', '\u1f78': 'The', '\u1f7b': 'gt;', '\u1fb6': 'a', \n                         '\u1fc6': 'or', '\u1fd6': '\u0e01', '\u1fe6': 'I', '\u3046': 'U', '\u3055': 'The', '\u3063': 'What',\n                         '\u3064': 'One', '\u306a': 'The', '\u3088': 'The', '\u3089': 'Et al', '\u30a8': 'The', \n                         '\u30af': 'The', '\u30b5': 'The', '\u30b7': 'The', '\u30b8': 'The', '\u30b9': 'The',\n                         '\u30c1': 'The', '\u30c4': 'The', '\u30cb': 'D', '\u30cf': 'Ha', '\u30de': 'Ma', \n                         '\u30ea': 'The', '\u30eb': 'Le', '\u30ec': 'Les', '\u30ed': 'The', '\u30f3': 'The',\n                         '\u4e00': 'One', '\u4e0e': 'versus', '\u4e14': 'And', '\u4e3a': 'for', '\u4e70': 'buy',\n                         '\u4e86': 'Up', '\u4e9b': 'some', '\u4ed6': 'he', '\u4ee5': 'Take', '\u4eec': 'They',\n                         '\u4ef6': 'Items', '\u4f20': 'pass', '\u4f26': 'Lun', '\u4f46': 'but', '\u4fe1': 'letter',\n                         '\u5019': 'Waiting', '\u507d': 'Pseudo', '\u5168': 'all', '\u516c': 'public', '\u5176': 'its',\n                         '\u517b': 'support', '\u51ac': 'winter', '\u51f8': 'Convex', '\u51fb': 'hit', '\u5224': 'Judge',\n                         '\u5230': 'To', '\u53cb': 'Friend', '\u53ef': 'can', '\u5417': 'What?', '\u548c': 'with',\n                         '\u552f': 'only', '\u56e0': 'because', '\u5723': 'Holy', '\u5728': 'in', '\u57fa': 'base',\n                         '\u5802': 'Hall', '\u58eb': 'Shishi', '\u590d': 'complex', '\u591a': 'many', '\u5929': 'day',\n                         '\u597d': 'it is good', '\u5982': 'Such as', '\u5a5a': 'marriage', '\u5b69': 'child', \n                         '\u5ba0': 'Pet', '\u5bd3': 'Apartment', '\u5bf9': 'Correct', '\u5c41': 'fart', \n                         '\u5c48': 'Qu', '\u5de8': 'huge', '\u5df1': 'already', '\u5f0f': 'formula', '\u5f53': 'when',\n                         '\u5f7c': 'he', '\u5f92': 'only', '\u5f97': 'Got', '\u6012': 'angry', '\u602a': 'strange',\n                         '\u6050': 'fear', '\u60e7': 'fear', '\u60f3': 'miss you', '\u6124': 'anger', '\u6211': 'I',\n                         '\u6218': 'war', '\u6279': 'Batch', '\u628a': 'Put', '\u62c9': 'Pull', '\u62f7': 'Copy', \n                         '\u63a5': 'Connect', '\u64cd': 'Fuck', '\u6536': 'Receive', '\u653f': 'Politics', \n                         '\u6559': 'teach', '\u65a4': 'jin', '\u65af': 'S', '\u65b0': 'new', '\u65f6': 'Time', \n                         '\u666e': 'general', '\u66fe': 'Once', '\u672c': 'this', '\u6740': 'kill', '\u6781': 'pole',\n                         '\u67e5': 'check', '\u6817': 'chestnut', '\u682a': 'stock', '\u6837': 'kind', '\u68c0': 'Check',\n                         '\u6b22': 'Happy', '\u6b7b': 'dead', '\u6c49': 'Chinese', '\u6ca1': 'No', '\u6cbb': 'rule', \n                         '\u6cd5': 'law', '\u6d3b': 'live', '\u70b9': 'point', '\u71fb': 'Moth', '\u7269': 'object',\n                         '\u731c': 'guess', '\u7334': 'monkey', '\u7406': 'Rational', '\u751f': 'Health', '\u7528': 'use',\n                         '\u767d': 'White', '\u767e': 'hundred', '\u76f4': 'straight', '\u76f8': 'phase', '\u770b': 'Look',\n                         '\u7763': 'Supervisor', '\u77e5': 'know', '\u793e': 'Society', '\u795d': 'wish', '\u79ef': 'product',\n                         '\u7a23': 'Jesus', '\u7ecf': 'through', '\u7ed3': 'Knot', '\u7ed9': 'give', '\u7f8e': 'nice', \n                         '\u8036': 'Yay', '\u804a': 'chat', '\u80dc': 'Win', '\u81f3': 'to', '\u865a': 'Virtual', '\u88fd': 'Made', \n                         '\u8981': 'Want', '\u8ba4': 'recognize', '\u8ba8': 'discuss', '\u8ba9': 'Let', '\u8bc6': 'knowledge',\n                         '\u8bdd': 'words', '\u8bed': 'language', '\u8bf4': 'Say', '\u8c0a': 'friendship', \n                         '\u8c13': 'Predicate', '\u8c61': 'Elephant', '\u8d3a': 'He', '\u8d62': 'win', '\u8fce': 'welcome',\n                         '\u8fd8': 'also', '\u8fd9': 'This', '\u901a': 'through', '\u9244': 'iron', '\u95ee': 'ask', \n                         '\u963f': 'A', '\u9898': 'question', '\u989d': 'amount', '\u9b3c': 'ghost', '\u9e21': 'Chicken',\n                         '\uac00': 'end', '\uac08': 'Go', '\uac8c': 'to', '\uaca9': 'case', '\uacbd': 'circa', '\uad00': 'tube',\n                         '\uad6d': 'soup', '\uae08': 'gold', '\ub098': 'I', '\ub294': 'The', '\ub2c8': 'Nee', '\ub2e4': 'All',\n                         '\ub300': 'versus', '\ub3c4': 'Degree', '\ub41c': 'The', '\ub4dc': 'De', '\ub4e4': 'field', \n                         '\ub54c': 'time', '\ub7f0': 'Run', '\ub835': 'Hi', '\ub85d': 'rock', '\ub93c': 'Crown', \n                         '\ub9ac': 'Lee', '\ub9c8': 'hemp', '\ub9cc': 'just', '\ubc18': 'half', '\ubd84': 'minute', \n                         '\uc0ac': 'four', '\uc0c1': 'Prize', '\uc11c': 'book', '\uc11d': 'three', '\uc131': 'castle',\n                         '\uc2a4': 'The', '\uc2dc': 'city', '\uc54a': 'Not', '\uc57c': 'Hey', '\uc57d': 'about', \n                         '\uc5b4': 'uh', '\uc640': 'Wow', '\uc6a9': 'for', '\uc720': 'U', '\uc744': 'of', '\uc774': 'this',\n                         '\uc778': 'sign', '\uc798': 'well', '\uc81c': 'My', '\uc950': 'rat', '\uc9c0': 'G', '\ucd08': 'second',\n                         '\uce90': 'Can', '\ud0f1': 'Tang', '\ud2b8': 'The', '\ud2f0': 'tea', '\ud328': 'tile', '\ud488': 'Width', \n                         '\ud55c': 'One', '\ud569': 'synthesis', '\ud574': 'year', '\ud5c8': 'Huh', '\ud654': 'anger', '\ud669': 'sulfur',\n                         '\ud558': 'Ha', '\ufb01': 'be', '\uff10': '#', '\uff12': '#', '\uff18': '#', '\uff25': 'e', '\uff27': 'g',\n                         '\uff28': 'h', '\uff2d': 'm', '\uff2e': 'n', '\uff2f': 'O', '\uff33': 's', '\uff35': 'U', '\uff37': 'w',\n                         '\uff41': 'a', '\uff42': 'b', '\uff43': 'c', '\uff44': 'd', '\uff45': 'e', '\uff46': 'f', '\uff47': 'g',\n                         '\uff48': 'h', '\uff49': 'i', '\uff4b': 'k', '\uff4c': 'l', '\uff4d': 'm', '\uff4e': 'n', '\uff4f': 'o',\n                         '\uff52': 'r', '\uff53': 's', '\uff54': 't', '\uff55': 'u', '\uff56': 'v', '\uff57': 'w', '\uff59': 'y',\n                         '\ud835\udc00': 'a', '\ud835\udc02': 'c', '\ud835\udc03': 'd', '\ud835\udc05': 'f', '\ud835\udc07': 'h', '\ud835\udc0a': 'k', '\ud835\udc0d': 'n', \n                         '\ud835\udc0e': 'o', '\ud835\udc11': 'r', '\ud835\udc13': 't', '\ud835\udc14': 'u', '\ud835\udc18': 'y', '\ud835\udc19': 'z', '\ud835\udc1a': 'a',\n                         '\ud835\udc1b': 'b', '\ud835\udc1c': 'c', '\ud835\udc1d': 'd', '\ud835\udc1e': 'e', '\ud835\udc1f': 'f', '\ud835\udc20': 'g', '\ud835\udc21': 'h', \n                         '\ud835\udc22': 'i', '\ud835\udc23': 'j', '\ud835\udc25': 'i', '\ud835\udc26': 'm', '\ud835\udc27': 'n', '\ud835\udc28': 'o', '\ud835\udc29': 'p',\n                         '\ud835\udc2a': 'q', '\ud835\udc2b': 'r', '\ud835\udc2c': 's', '\ud835\udc2d': 't', '\ud835\udc2e': 'u', '\ud835\udc2f': 'v', '\ud835\udc30': 'w',\n                         '\ud835\udc31': 'x', '\ud835\udc32': 'y', '\ud835\udc33': 'z', '\ud835\udc65': 'x', '\ud835\udc66': 'y', '\ud835\udc67': 'z', '\ud835\udc69': 'b',\n                         '\ud835\udc6a': 'c', '\ud835\udc6b': 'd', '\ud835\udc6c': 'e', '\ud835\udc6d': 'f', '\ud835\udc6e': 'g', '\ud835\udc6f': 'h', '\ud835\udc70': 'i',\n                         '\ud835\udc71': 'j', '\ud835\udc72': 'k', '\ud835\udc73': 'l', '\ud835\udc74': 'm', '\ud835\udc75': 'n', '\ud835\udc76': '0', '\ud835\udc77': 'p',\n                         '\ud835\udc79': 'r', '\ud835\udc7a': 's', '\ud835\udc7b': 't', '\ud835\udc7e': 'w', '\ud835\udc80': 'y', '\ud835\udc81': 'z', '\ud835\udc82': 'a',\n                         '\ud835\udc83': 'b', '\ud835\udc84': 'c', '\ud835\udc85': 'd', '\ud835\udc86': 'e', '\ud835\udc87': 'f', '\ud835\udc88': 'g', '\ud835\udc89': 'h',\n                         '\ud835\udc8a': 'i', '\ud835\udc8b': 'j', '\ud835\udc8c': 'k', '\ud835\udc8d': 'l', '\ud835\udc8e': 'm', '\ud835\udc8f': 'n', '\ud835\udc90': 'o', \n                         '\ud835\udc91': 'p', '\ud835\udc92': 'q', '\ud835\udc93': 'r', '\ud835\udc94': 's', '\ud835\udc95': 't', '\ud835\udc96': 'u', '\ud835\udc97': 'v', \n                         '\ud835\udc98': 'w', '\ud835\udc99': 'x', '\ud835\udc9a': 'y', '\ud835\udc9b': 'z', '\ud835\udca9': 'n', '\ud835\udcb6': 'a', '\ud835\udcb8': 'c',\n                         '\ud835\udcbd': 'h', '\ud835\udcbe': 'i', '\ud835\udcc0': 'k', '\ud835\udcc1': 'l', '\ud835\udcc3': 'n', '\ud835\udcc5': 'p', '\ud835\udcc7': 'r',\n                         '\ud835\udcc8': 's', '\ud835\udcc9': 't', '\ud835\udcca': 'u', '\ud835\udccc': 'w', '\ud835\udcce': 'y', '\ud835\udcd2': 'c', '\ud835\udcec': 'c',\n                         '\ud835\udcee': 'e', '\ud835\udcf2': 'i', '\ud835\udcf4': 'k', '\ud835\udcf5': 'l', '\ud835\udcfb': 'r', '\ud835\udcfc': 's', '\ud835\udcfd': 't',\n                         '\ud835\udcff': 'v', '\ud835\udd74': 'j', '\ud835\udd78': 'm', '\ud835\udd7f': 'i', '\ud835\udd82': 'm', '\ud835\udd86': 'a', '\ud835\udd87': 'b',\n                         '\ud835\udd88': 'c', '\ud835\udd89': 'd', '\ud835\udd8a': 'e', '\ud835\udd8b': 'f', '\ud835\udd8c': 'g', '\ud835\udd8d': 'h', '\ud835\udd8e': 'i', \n                         '\ud835\udd92': 'm', '\ud835\udd93': 'n', '\ud835\udd95': 'p', '\ud835\udd97': 'r', '\ud835\udd98': 's', '\ud835\udd99': 't', '\ud835\udd9a': 'u',\n                         '\ud835\udd9b': 'v', '\ud835\udd9c': 'w', '\ud835\udd9e': 'n', '\ud835\udd9f': 'z', '\ud835\uddd5': 'b', '\ud835\uddd8': 'e', '\ud835\uddd9': 'f',\n                         '\ud835\uddde': 'k', '\ud835\udddf': 'l', '\ud835\udde0': 'm', '\ud835\udde2': 'o', '\ud835\udde4': 'q', '\ud835\udde6': 's', '\ud835\udde7': 't',\n                         '\ud835\uddea': 'w', '\ud835\udded': 'z', '\ud835\uddee': 'a', '\ud835\uddef': 'b', '\ud835\uddf0': 'c', '\ud835\uddf1': 'd', '\ud835\uddf2': 'e',\n                         '\ud835\uddf3': 'f', '\ud835\uddf4': 'g', '\ud835\uddf5': 'h', '\ud835\uddf6': 'i', '\ud835\uddf7': 'j', '\ud835\uddf8': 'k', '\ud835\uddf9': 'i',\n                         '\ud835\uddfa': 'm', '\ud835\uddfb': 'n', '\ud835\uddfc': 'o', '\ud835\uddfd': 'p', '\ud835\uddff': 'r', '\ud835\ude00': 's', '\ud835\ude01': 't',\n                         '\ud835\ude02': 'u', '\ud835\ude03': 'v', '\ud835\ude04': 'w', '\ud835\ude05': 'x', '\ud835\ude06': 'y', '\ud835\ude07': 'z', '\ud835\ude10': 'l',\n                         '\ud835\ude13': 'l', '\ud835\ude16': 'o', '\ud835\ude22': 'a', '\ud835\ude23': 'b', '\ud835\ude24': 'c', '\ud835\ude25': 'd', '\ud835\ude26': 'e',\n                         '\ud835\ude27': 'f', '\ud835\ude28': 'g', '\ud835\ude29': 'h', '\ud835\ude2a': 'i', '\ud835\ude2b': 'j', '\ud835\ude2c': 'k', '\ud835\ude2e': 'm',\n                         '\ud835\ude2f': 'n', '\ud835\ude30': 'o', '\ud835\ude31': 'p', '\ud835\ude32': 'q', '\ud835\ude33': 'r', '\ud835\ude34': 's', '\ud835\ude35': 't',\n                         '\ud835\ude36': 'u', '\ud835\ude37': 'v', '\ud835\ude38': 'w', '\ud835\ude39': 'x', '\ud835\ude3a': 'y', '\ud835\ude3c': 'a', '\ud835\ude3d': 'b',\n                         '\ud835\ude3e': 'c', '\ud835\ude3f': 'd', '\ud835\ude40': 'e', '\ud835\ude43': 'h', '\ud835\ude45': 'j', '\ud835\ude46': 'k', '\ud835\ude47': 'l', \n                         '\ud835\ude48': 'm', '\ud835\ude4a': 'o', '\ud835\ude4b': 'p', '\ud835\ude4d': 'r', '\ud835\ude4f': 't', '\ud835\ude52': 'w', '\ud835\ude54': 'y',\n                         '\ud835\ude56': 'a', '\ud835\ude57': 'b', '\ud835\ude58': 'c', '\ud835\ude59': 'd', '\ud835\ude5a': 'e', '\ud835\ude5b': 'f', '\ud835\ude5c': 'g',\n                         '\ud835\ude5d': 'h', '\ud835\ude5e': 'i', '\ud835\ude5f': 'j', '\ud835\ude60': 'k', '\ud835\ude62': 'm', '\ud835\ude63': 'n', '\ud835\ude64': 'o',\n                         '\ud835\ude65': 'p', '\ud835\ude67': 'r', '\ud835\ude68': 's', '\ud835\ude69': 't', '\ud835\ude6a': 'u', '\ud835\ude6b': 'v', '\ud835\ude6c': 'w',\n                         '\ud835\ude6d': 'x', '\ud835\ude6e': 'y', '\ud835\udfce': '0', '\ud835\udfcf': '1', '\ud835\udfd0': '2', '\ud835\udfd3': '5', '\ud835\udfd4': '6',\n                         '\ud835\udfd6': '8', '\ud835\udfec': '0', '\ud835\udfed': '1', '\ud835\udfee': '2', '\ud835\udfef': '3', '\ud835\udff0': '4', '\ud835\udff1': '5',\n                         '\ud835\udff2': '6', '\ud835\udff3': '7', '\ud835\udfd1':'3', '\ud835\udfd2':'4', '\ud835\udfd5':'7', '\ud835\udfd7':'9',\n                         '\ud83c\udde6': 'a', '\ud83c\udde9': 'd', '\ud83c\uddea': 'e', '\ud83c\uddec': 'g', '\ud83c\uddee': 'i', \n                         '\ud83c\uddf3': 'n', '\ud83c\uddf4': 'o', '\ud83c\uddf7': 'r', '\ud83c\uddf9': 't', '\ud83c\uddfc': 'w', '\ud83d\udd92': 'thumps up',\n                         '\u210f':'h', '\u02b2':'j', '\uff23':'c', '\u013a':'i', '\uff2a':'j', '\u0138':'k', '\uff30':'p'}\n\n\n\n\n\n\n# List was cerated in separate notebook investigating on word embedding. \n# These dictionary is used to remove unwanted characters from the text\npuncts =                 ['_','!', '?','\\x08', '\\n', '\\x0b', '\\r', '\\x10', '\\x13', '\\x1f', ' ', ' # ', '\"', '#', \n                         '# ', '$', '%', '&',  '(', ')', '*', '+', ',',  '\/', '.', ':', ';', '<',\n                         '=', '>', '@', '[', '\\\\', ']', '^', '`', '{', '|', '}', '~', '\\x7f', '\\x80',\n                         '\\x81', '\\x85', '\\x91', '\\x92', '\\x95', '\\x96', '\\x9c', '\\x9d', '\\x9f', '\\xa0', \n                         '\u00a1', '\u00a2\u0f3c', '\u00a3', '\u00a4', '\u00a5', '\u00a7', '\u00a8', '\u00a9', '\u00ab', '\u00ac', '\\xad', '\u00af', '\u00b0', '\u00b1', '\u00b3',\n                         '\u00b6', '\u00b7', '\u00b8', '\u00ba', '\u00bb', '\u00bc', '\u00bd', '\u00be', '\u00bf', '\u00d7', '\u00d8', '\u00f7', '\u00f8', '\u0184', '\u01bd',\n                         '\u01d4', '\u023b', '\u025c', '\u0269', '\u0283', '\u028c', '\u02bb', '\u02bc', '\u02c8', '\u02cc', '\u02d0', '\u02d9', '\u02da', '\u0301', '\u0304', '\u0305', \n                         '\u0307', '\u0308', '\u0323', '\u0328', '\u032f', '\u0331', '\u0332', '\u0336', '\u035c', '\u035d', '\u035e', '\u035f', '\u0361', '\u0366', '\u061f', '\u064e', '\u0650', '\u06a1', \n                         '\u06de', '\u06e9', '\u0701', '\u093e', '\u094d', '\u0abe', '\u0ac0', '\u0ac1', '\u0e4f', '\u0e4f\u032f\u0361', '\u0f3c', '\u0f3d', '\u1403', '\u1423', '\u1426', '\u1427',\n                         '\u144e', '\u146d', '\u146f', '\u14a7', '\u14c0', '\u14c2', '\u14c3', '\u14c7', '\u152d', '\u1d26', '\u1d28', '\u1d7b', '\u1f38', '\u1f39', '\u1f7c', \n                         '\u1fbd', '\u1fc3', '\\u2000', '\\u2001', '\\u2002', '\\u2003', '\\u2004', '\\u2005', '\\u2006', \n                         '\\u2007', '\\u2008', '\\u2009', '\\u200a', '\\u200b', '\\u200c', '\\u200d', '\\u200e',\n                         '\\u200f', '\u2010', '\u2011', '\u2012', '\u2013', '\u2014', '\u2015', '\u2016', '\u2018', '\u2019', '\u201a', '\u201b', '\u201c', '\u201d', '\u201e',\n                         '\u2020', '\u2021', '\u2022', '\u2023', '\u2026', '\\u2028', '\\u202a', '\\u202c', '\\u202d', '\\u202f', '\u2030',\n                         '\u2032', '\u2033', '\u2039', '\u203a', '\u203f', '\u2044', '\u204d\u0334\u031b\\u3000', '\u204e', '\u2074', '\u2082', '\u20ac', '\u20b5', '\u20bd', '\u2103', '\u2105',\n                         '\u2110', '\u2122', '\u212e', '\u2153', '\u2190', '\u2191', '\u2192', '\u2193', '\u21b3', '\u21b4', '\u21ba', '\u21cc', '\u21d2', '\u21e4', '\u2206', '\u220e',\n                         '\u220f', '\u2212', '\u2215', '\u2219', '\u221a', '\u221e', '\u2229', '\u2234', '\u2235', '\u223c', '\u2248', '\u2260', '\u2264', '\u2265', '\u2282', '\u2295',\n                         '\u2298', '\u22c5', '\u22c6', '\u2320', '\u238c', '\u23d6', '\u2500', '\u2501', '\u2503', '\u2508', '\u250a', '\u2517', '\u2523', '\u252b', '\u2533', '\u254c', '\u2550',\n                         '\u2551', '\u2554', '\u2557', '\u255a', '\u2563', '\u2566', '\u2569', '\u256a', '\u256d', '\u256d\u256e', '\u256e', '\u256f', '\u2570', '\u2571', '\u2572', '\u2580',\n                         '\u2582', '\u2583', '\u2584', '\u2585', '\u2586', '\u2587', '\u2588', '\u258a', '\u258b', '\u258f', '\u2591', '\u2592', '\u2593', '\u2594', '\u2595', \n                         '\u2599', '\u25a0', '\u25aa', '\u25ac', '\u25b0', '\u25b1', '\u25b2', '\u25b7', '\u25b8', '\u25ba', '\u25bc', '\u25be', '\u25c4', '\u25c7', '\u25cb',\n                         '\u25cf', '\u25d0', '\u25d4', '\u25d5', '\u25dd', '\u25de', '\u25e1', '\u25e6', '\u2605', '\u2606', '\u260f', '\u2610', '\u2612', '\u2619', '\u261b',\n                         '\u261c', '\u261e', '\u262d', '\u263b', '\u263c', '\u2666', '\u2669', '\u266a', '\u266b', '\u266c', '\u266d', '\u2672', '\u2686', '\u26ad', '\u26b2', '\u2700',\n                         '\u2713', '\u2718', '\u271e', '\u2727', '\u272c', '\u272d', '\u2730', '\u273e', '\u2746', '\u2767', '\u27a4', '\u27a5', '\u2800', '\u290f', '\u2981',\n                         '\u2a5b', '\u2b2d', '\u2b2f', '\\u3000', '\u3001', '\u3002', '\u300a', '\u300b', '\u300c', '\u300d', '\u3014', '\u30fb', '\u3138', '\u3153',\n                         '\u951f', '\ua725', '\\ue014', '\\ue600', '\\ue602', '\\ue607', '\\ue608', '\\ue613', '\\ue807',\n                         '\\uf005', '\\uf020', '\\uf04a', '\\uf04c', '\\uf070',  '\\uf202\\uf099', '\\uf203',\n                         '\\uf071\\uf03d\\uf031\\uf02f\\uf032\\uf028\\uf070\\uf02f\\uf032\\uf02d\\uf061\\uf029',\n                         '\\uf099', '\\uf09a', '\\uf0a7', '\\uf0b7', '\\uf0e0', '\\uf10a', '\\uf202', \n                         '\\uf203\\uf09a', '\\uf222', '\\uf222\\ue608', '\\uf410', '\\uf410\\ue600', '\\uf469', \n                         '\\uf469\\ue607', '\\uf818', '\ufd3e', '\ufd3e\u0361', '\ufd3f', '\ufdfb', '\\ufeff', '\uff01', '\uff05', '\uff07',\n                         '\uff08', '\uff09', '\uff0c', '\uff0d', '\uff0e', '\uff0f', '\uff1a', '\uff1e', '\uff1f', '\uff3c', '\uff5c', '\uffe6', '\ufffc', '\ufffd',\n                         '\ud835\udcbb', '\ud835\udd7e', '\ud835\udd84', '\ud835\udd90', '\ud835\udd91', '\ud835\udd94', '\ud835\udddc', '\ud835\ude0a', '\ud835\ude2d', '\ud835\ude44', '\ud835\ude61', '\ud835\udf48', '\ud83d\udd91', '\ud83d\udd92']\n\n ","b3de8486":"def clean_numbers(x):\n  \n  \"\"\"\n  The following function is used to format the numbers.\n  In the beginning \"th, st, nd, rd\" are removed\n  \"\"\"\n  \n  #remove \"th\" after a number\n  matches = re.findall(r'\\b\\d+\\s*th\\b', x)\n  if len(matches) != 0:\n    x = re.sub(r'\\s*th\\b', \" \", x)\n    \n  #remove \"rd\" after a number \n  matches = re.findall(r'\\b\\d+\\s*rd\\b', x)\n  if len(matches) != 0:\n    x = re.sub(r'\\s*rd\\b', \" \", x)\n  \n  #remove \"st\" after a number\n  matches = re.findall(r'\\b\\d+\\s*st\\b', x)\n  if len(matches) != 0:\n    x = re.sub(r'\\s*st\\b', \" \", x)\n    \n  #remove \"nd\" after a number\n  matches = re.findall(r'\\b\\d+\\s*nd\\b', x)\n  if len(matches) != 0:\n    x = re.sub(r'\\s*nd\\b', \" \", x)\n  \n  # replace standalone numbers higher than 10 by #\n  # this function does not touch numbers linked to words like \"G-20\"\n  matches = re.findall(r'^\\d+\\s+|\\s+\\d+\\s+|\\s+\\d+$', x)\n  if len(matches) != 0:\n    x = re.sub('^[0-9]{5,}\\s+|\\s+[0-9]{5,}\\s+|\\s+[0-9]{5,}$', ' ##### ', x)\n    x = re.sub('^[0-9]{4}\\s+|\\s+[0-9]{4}\\s+|\\s+[0-9]{4}$', ' #### ', x)\n    x = re.sub('^[0-9]{3}\\s+|\\s+[0-9]{3}\\s+|\\s+[0-9]{3}$', ' ### ', x)\n    x = re.sub('^[0-9]{2}\\s+|\\s+[0-9]{2}\\s+|\\s+[0-9]{2}$', ' ## ', x)\n    #we do include the range from 1 to 10 as all word-vectors include them\n    #x = re.sub('[0-9]{1}', '#', x)\n    \n  return x","1eb7b919":"def year_and_hour(text):\n  \"\"\"\n  This function is used to replace \"yr,yrs\" by year and \"hr,hrs\" by hour.\n  \"\"\"\n  \n  # Find matches for \"yr\", \"yrs\", \"hr\", \"hrs\"\n  matches_year = re.findall(r'\\b\\d+\\s*yr\\b', text)\n  matches_years = re.findall(r'\\b\\d+\\s*yrs\\b', text)\n  matches_hour = re.findall(r'\\b\\d+\\s*hr\\b', text)\n  matches_hours = re.findall(r'\\b\\d+\\s*hrs\\b', text)\n  \n  # replace all matches accordingly\n  if len(matches_year) != 0:\n    text = re.sub(r'\\b\\d+\\s*yr\\b', \"year\", text)\n  if len(matches_years) != 0:\n    text = re.sub(r'\\b\\d+\\s*yrs\\b', \"year\", text)\n  if len(matches_hour) != 0:\n    text = re.sub(r'\\b\\d+\\s*hr\\b', \"hour\", text)\n  if len(matches_hours) != 0:\n    text = re.sub(r'\\b\\d+\\s*hrs\\b', \"hour\", text)\n  return text","f4bd31b5":"def textBlobLemmatize(sentence):\n  \"\"\"\n  This function uses the Word lemmatizer function of the textBlob package.\n  \"\"\"  \n  #for each word in the text, replace the word by its lemmatized version\n  for x in sentence.split():\n    sentence = sentence.replace(x, Word(x).lemmatize())\n  return sentence","4e9cea37":"def build_vocab(df):\n  \n  '''Build a dictionary of words and its number of occurences from the data frame'''\n  \n  #initialize the tokenizer\n  tokenizer = TweetTokenizer()\n  \n  vocab = {}\n  for i, row in enumerate(df):\n      #tokenize the sentence \n      words = tokenizer.tokenize(row)\n      #for each word, check if it is in the dict otherwise add a new entry\n      for w in words:\n       \n        try:\n            vocab[w] += 1\n        except KeyError:\n            vocab[w] = 1\n  \n  return vocab","6dfc619d":"#https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-for-glove-part1-eda\ndef check_coverage(vocab,embeddings_index, print_oov_num=100):\n  '''\n  This function checks what part of the vocabluary and the text is covered by the embedding index.\n  It returns a list of tuples of unknown words and its occuring frequency.\n  '''\n  \n  a = {}\n  oov = {}\n  k = 0\n  i = 0\n\n  # for every word in vocab\n  for word in vocab:\n      # check if it can be found in the embedding\n      try:\n          # store the embedding index to a\n          a[word] = embeddings_index[word]\n          # count up by #of occurences in df\n          k += vocab[word]\n      except:\n          # if no embedding for word, add to oov\n          oov[word] = vocab[word]\n          # # count up by #of occurences in df\n          i += vocab[word]\n          pass\n  # calc percentage of #of found words by length of vocab\n  print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n  # devide number of found words by number of all words from df\n  print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n\n  # return unknown words sorted by number of occurences\n  sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n  print('Top unknown words are:', sorted_x[:print_oov_num])\n\n  #return dict of unknown words + occurences\n  return oov","3659f1d2":"def  load_embedding_vocab(path):\n  '''\n  Load the embeddings in the right format and return the vocab dictionary. \n  '''  \n  # Print starting info about the pre-processing\n  starttime = datetime.datetime.now().replace(microsecond=0)\n  print(\"Starttime: \", starttime)\n\n  def timediff(time):\n    return time - starttime\n  \n  EMBEDDING_FILE = path\n  def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n  embeddings_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(EMBEDDING_FILE)) \n    \n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Embedding model loaded and vocab returned. Time since start: \", timediff(time))\n  \n  #return the vocab\n  return embeddings_index","664f2f83":"def preprocessing_conv(df, calc_coverage=False, model_vocab=None, print_oov_num=100):\n  \n  \"\"\"\n  Function that combines the whole pre-processing process specifically for neural networks where less pre-processing is required compared to conventional methods.\n  This means we will not remove stopwords, lemmatize or remove typical punctuation.\n  \n  If 'calc_coverage'  == True --> please specify the embedding vocabulary 'model_vocab'\n  \n  \"\"\"\n  \n  if np.logical_and(calc_coverage == True, model_vocab == None):\n    print('Error: Please add a model vocablulary (model_vocab) to the variables or turn calc_coverage to \"False\" \\n')\n  \n  # Set parameters\n  #spell = SpellChecker() #  not used due to timing constraint\n  #transl = Translator() #  not used due to timing constraint\n  nltk.download('stopwords')\n  stop_words = set(stopwords.words('english'))\n  clear_output()\n  \n  # Print starting info about the pre-processing\n  starttime = datetime.datetime.now().replace(microsecond=0)\n  print('Dataset Length: ', len(df), \"Starttime: \", starttime)\n\n  def timediff(time):\n    return time - starttime\n  \n  # check the coverage of the original text using model_vocab\n  if calc_coverage == True: \n    #build a vocab from text\n    vocab = build_vocab(df.comment_text)\n    print('Vocabulary from dataset created. \\n')\n    check_coverage(vocab,model_vocab, print_oov_num=print_oov_num)\n\n  # First we convert all characters to lower characters\n  df.comment_text = df.comment_text.apply(lambda x: x.lower())\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Comments have been converted to lower case letters. Time since start: \", timediff(time) )\n\n  # The replace_contractions function is applied on the data frame\n  df.comment_text = df.comment_text.apply(lambda x: replace_contractions(x))\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Contractions have been replaced. Time since start: \", timediff(time))\n\n  # Replace keyboard smilies with text\n  df.comment_text = df.comment_text.apply(lambda x: replace_smilies(x))\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Smilies have been converted to text. Time since start: \", timediff(time))\n  \n  # Remove stopwords\n  df.comment_text = df.comment_text.apply(lambda x: remove_stopwords(x, stop_words))\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Stopwords have been removed. Time since start: \", timediff(time))\n\n  # Replace or remove special characters like - \/ _ according to rules\n  df.comment_text = df.comment_text.apply(lambda x: replace_symbol_special(x,check_vocab=calc_coverage, vocab=model_vocab))\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Special symbols have been processed. Time since start: \", timediff(time))\n\n  # The clean_text function is applied on the data frame\n  df.comment_text = df.comment_text.apply(lambda x: clean_text(x))\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"All signs have been removed. Time since start: \", timediff(time))\n\n  # The clean_numbers function is applied\n  df.comment_text = df.comment_text.apply(lambda x: clean_numbers(x))\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"All numbers have been replaced with ###. Time since start: \", timediff(time))\n\n  # Abbreviations are replaced by year and hour\n  df.comment_text = df.comment_text.apply(lambda x: year_and_hour(x))\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Yr and hr have been replaced by year and hour. Time since start: \", timediff(time))\n  \n  # Lemmatize the text\n  df.comment_text = df.comment_text.apply(lambda x: textBlobLemmatize(x))\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"All words have been lemmatized. Time since start: \", timediff(time))\n  \n  # *function takes too long against its benefit, moved to unused functions\n   # We apply the spellchecker function\n  #df.comment_text = df.comment_text.apply(lambda x: spellchecking(x, spell))\n  #time = datetime.datetime.now().replace(microsecond=0)\n  #print(\"Spellchecking has been performed. Time since start: \", timediff(time))\n  \n  # *function takes too long against its benefit, moved to unused functions\n  # We apply the translate function\n  #df.comment_text = df.comment_text.apply(lambda x: translator(x, transl))\n  #time = datetime.datetime.now().replace(microsecond=0)\n  #print(\"Words in foreign languages have been translated to english. Time since start: \", timediff(time))\n  \n  # check the coverage of the original text using model_vocab\n  if calc_coverage == True:\n    vocab = build_vocab(df.comment_text)\n    unknown = check_coverage(vocab, model_vocab, print_oov_num=print_oov_num)\n    \n  print('Pre-processing done. Time since start: ', timediff(time))\n  \n  return df","9b289725":"#Reload test data\n#train_data = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")\n","4478ea1d":"df = preprocessing_conv(test_data, calc_coverage=False, model_vocab=None, print_oov_num=100) #500.000","1028d20e":"#df = preprocessing_conv(train_data, calc_coverage=False, model_vocab=None, print_oov_num=100) #500.000","c3c8a8ae":"def preprocessing_NN(df, model_vocab, calc_coverage=True, print_oov_num=100):\n  \"\"\"\n  This function is only correcting words which are not out of the box known towards the embedding dictionary.\n  It is optimized using the nltk TweetTokenizer.\n\n  Function that combines the whole pre-processing process specifically for neural networks where less pre-processing is required compared to conventional methods.\n  This means we will not remove stopwords, lemmatize or remove typical punctuation.\n  \"\"\"\n  \n  # Set parameters\n  tokenizer = TweetTokenizer()\n  \n  # Print starting info about the pre-processing\n  starttime = datetime.datetime.now().replace(microsecond=0)\n  print('Dataset Length: ', len(df), \"Starttime: \", starttime)\n\n  def timediff(time):\n    return time - starttime\n  \n  # build a vocabulary from the text \n  vocab = build_vocab(df.comment_text)\n  print('Embedding vectors are loaded. \\n')\n  # check the coverage and receive a dictionary of unknown words\n  unknown = check_coverage(vocab,model_vocab, print_oov_num=print_oov_num)\n  # extract the list of unknown words\n  unknown = unknown.keys()\n  \n  ## Process the unknown words\n  # The replace_contractions function is applied on the data frame\n  corrected = [replace_contractions(x) for x in unknown]\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Contractions have been replaced. Time since start: \", timediff(time))\n\n  # Replace emojis with text\n  corrected = [emoji.demojize(x) for x in corrected]\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Emojis have been converted to text. Time since start: \", timediff(time))\n\n  # Replace keyboard smilies with text\n  corrected = [replace_smilies(x) for x in corrected]\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Smilies have been converted to text. Time since start: \", timediff(time))\n\n  # The clean_text function is applied on the data frame\n  corrected = [clean_text(x) for x in corrected]\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"All signs have been removed. Time since start: \", timediff(time))\n  \n  # The clean_numbers function is applied\n  corrected = [clean_numbers(x) for x in corrected]\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"All numbers have been replaced with ###. Time since start: \", timediff(time))\n  \n    # Replace or remove special characters like - \/ _ according to rules\n  corrected = [replace_symbol_special(x, check_vocab=True, vocab=model_vocab) for x in corrected]\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Special symbols have been processed. Time since start: \", timediff(time))\n\n  # Abbreviations are replaced by year and hour\n  corrected = [year_and_hour(x) for x in corrected]\n  time = datetime.datetime.now().replace(microsecond=0)\n  print(\"Yr and hr have been replaced by year and hour. Time since start: \", timediff(time))\n  \n  # *Takes too long\n  #Correct spelling mistakes\n  #corrected = [TextBlob(x).correct() for x in corrected]\n  #time = datetime.datetime.now().replace(microsecond=0)\n  #print(\"Yr and hr have been replaced by year and hour. Time since start: \", timediff(time))\n  \n  #create a dictionary from word and correction\n  dictionary = dict(zip(unknown, corrected))\n  keys = dictionary.keys()\n  \n  #remove all keys where unknown equals correction after processing\n  #create a new dict\n  dict_mispell = dict()\n  for key in dictionary.keys():\n    # if the correction differs from the unknown word add it to the new dict\n    if key != dictionary.get(key):\n      dict_mispell[key] = dictionary.get(key)\n  \n  time = datetime.datetime.now().replace(microsecond=0)\n  print('Correction dictionary of unknown words prepared. Time since start: ', timediff(time))\n  #print(dict_mispell, '\\n')\n  \n  def clean_mispell(text, dict_mispell):\n    '''Replaces the unknown words in the text by its corrections.'''\n    #tokenize the text with TweetTokenizer\n    words = tokenizer.tokenize(text)\n    for i, word in enumerate(words):\n      # if the word is among the misspellings\n      if word in dict_mispell.keys():\n        #replace it by the corrected word\n        words[i] = dict_mispell.get(word)\n    #merge text by space\n    text = ' '.join(words)\n    # remove all double spaces potentially appearing after pre-processing.\n    text  = re.sub(r' +', ' ', text)\n    return text\n      \n  \n  #tqdm.pandas()\n  df.comment_text = df.comment_text.apply(lambda x: clean_mispell(x, dict_mispell))\n  time = datetime.datetime.now().replace(microsecond=0)\n  print('Unknown words replaced excluding coverage check. Time since start: ', timediff(time))\n  \n  # print the final result\n  if calc_coverage == True: \n    vocab = build_vocab(df.comment_text)\n    unknown = check_coverage(vocab,model_vocab, print_oov_num=print_oov_num)\n    time = datetime.datetime.now().replace(microsecond=0)\n    print('Pre-processing done including coverage check. Time since start: ', timediff(time))\n  \n  return df","4feafeef":"model_vocab_fasttext = load_embedding_vocab('..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec')","f2ec40ac":"\nmodel_vocab_glove = load_embedding_vocab('..\/input\/glove840b300dtxt\/glove.840B.300d.txt')","9a2790d0":"test_data = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")\nprint('Fasttext \\n')\ndf = preprocessing_NN(test_data,model_vocab_fasttext, calc_coverage=True, print_oov_num=100)  #if you set calc_coverage=False it is even quicker, just does not show the coverage after processing\n\ntest_data = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")\nprint('\\n Glove \\n')\ndf = preprocessing_NN(test_data,model_vocab_glove, calc_coverage=True, print_oov_num=100)  #if you set calc_coverage=False it is even quicker, just does not show the coverage after processing","34bf03f3":"train_data = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\nprint('Fasttext \\n')\ndf = preprocessing_NN(train_data,model_vocab_fasttext, calc_coverage=True, print_oov_num=100)  #if you set calc_coverage=False it is even quicker, just does not show the coverage after processing\n\ntrain_data = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\nprint('\\n Glove \\n')\ndf = preprocessing_NN(train_data,model_vocab_glove, calc_coverage=True, print_oov_num=100)  #if you set calc_coverage=False it is even quicker, just does not show the coverage after processing","203e945c":"for i, x in enumerate(df.comment_text):\n  print(x)\n  if i==15:\n    break ","a1087d6e":"**Test Set Testing**","9c29dc54":"For the pre-processing for keras or better for models using word embeddings, we completely restructured our method.\n\nInitially, we used a very similar method, we designed for preprocessing_conv() where step by step the dataset is transform according to the methods. However, we found when checking the vocabulary - embedding coverage as well as in the process of comparing embeddings, that some embedding indexes know quite some symbols and special words and others not. We found that **every word embedding actually needs its own pre-processing.** \n\nTo avoid having several methods to serve the specific needs, we rewrote the initial function (now in **Unused or outdated methods**  preprocess_NN_all()) in a way that **only words which are unknown to the embedding indexes are being corrected** towards our designed methods.\nBy that we can keep all known symbols, special words which the specific embedding vocabulary knows including punctuations. Thus the aim is to **maximize the true embedding coverage** of the dataset **only tuning when needed.**\n\nNext to that we updated some out of methods making use of the embedding vocabulary.\n\nAs main input the function takes the dataset, the embedding vocabulary.\n1. Vocabulary from data set is build\n2. The coverage of the dataset vocabulary is compared to the embedding vocabulary\n3. Unknown words (oov = out of vocabulary) are identified\n4. The relevant pre-processings are applied towards the unknown words only \n5. A dictionary with unknown word and its correction is generated, words which could not get improved are ignored\n6. Unknown words in the text are replaced \n7. Dataset returned in a way that every word is separated by space.\n\nThus anyone who wants to use the **outcome of this pre-processing only needs to separate the words by space ( ' ' ).**\n\nTo tokenize the words in a good way we used the **TweetTokenizer,** which we found most powerful for our challenge. It keeps symbols for smilies together, however separates punctuation at the end of a sentence. Next to that it appeared to be extremely fast! We could reduce our proprocesing speed by more than 20 minutes on the full 1.8 million dataset, so that the method including coverage check takes about 16 minutes. This will be explained more in detail in the next part.\n","02e3a1bd":"- Download the relevant libraries\n- Connect to Google drive files\n- Load train and test set\n- Download relevant word embeddings without loading\n  (save RAM)\n  ","9bd0629a":"#### Test pre-processing neural networks","bcd2dcc0":"Pre-processing conventional is** designed to be used with Count Vectorizer or TFIDF** where we want to **reduce the words towards its core**. Therefore, we remove words which are very frequent and thus having less of a true meaning towards a potential model. and try to combine words with the same meaning.\n\nConsequently, we decided for this pre-processing to turn all words** to lower case as well as lemmatize** them to combine all words which are in the end the same, **remove stopwords, process numbers and most of the symbols.** Doing that we will decrease the high dimensional matrix generated by count vectorizer or TFIDF towards the relevant words. **Smilies are translated**.\n\nFor the removal of stopwords we are using the nltk library, for the lemmatizer the TextBlob Lemmatizer which works much faster and way more reliable then other lemmatizer or stemmer we tested.\n\nNext to that, we **tried to integrate spellchecker or translators.** Both appeared to be **very slow and not too effective.** Overall, the dataset seemed to be clean from misspellings as well as mainly written in English. Some Chinese or other words are translated in the 'Clean_text'-method. For that we used the python google translator (now in the unused functions) and integrated the outcome to the clean_text library.\n\nOptionally, the function can measure how much of the vocabulary is represented in a certain embedding vocabulary and print the unknown words. This can be used as a reflection or estimation. However, overall the function in itself is independent from a word embedding vocab.","016748e7":"We wrote different methods to pre-process the data. These methods are design so that they can e used for different kind of models using different vectorizer types.\nThe methods are optimized by using FastText and Glove Embedding Indexes.\n\n**Our method includes functions like:**\n  -** Replace contractions** by its long version such as *'I'm' --> 'I am'* or *'Let's' 'Let us'*\n  - **Replace special symbols** like dashes, underscores and slashes which are typically used to separate words. \n    It can keep words like dashes in words like* 'e-mail'* known to the embedding index while removing it for '*anti-Trump*' where the index has no entry\n  - **Replace** typical keyboard typed **smilies** into its simple translation such as *'-)' --> 'smile'*\n  - **Replace stopwords**\n  - **Clean the text from unknown characters** by either translating it or by removing it\n  - **Transform standalone numbers** for example *'5337' to '####'* to reduce noise\n  - **Replace year and hour** abbreviations to the full word\n  - TextBlob **lemmatizer**, which we found to work relatively stable compared to others\n  \n**Further methods are used to handle and compare the datasets and embeddings**\n  - **Build a vocabulary** from a dataset together with the word frequency\n  - **Load different embeddings and build a vocabulary** from GloVe, FastText or GoogleNews in its specific ways\n  - **Check the coverage** of embedding indexes against the dataset vocabulary, returning unknown words\n  \n  \n  \n  All these methods are combined in different ways to serve the specific requirements of a vectorizer and model combination.\n  This will be explained in detail for each pre-processing method.","e067544f":"# Super fast pre-processing and modeling ","162228fb":"### Pre-processing for neural networks with keras","661d8338":"#### Test pre-processing conventional","10692175":"### Used methods within the pre-processing\n\n","f256d883":"##Pre-processing\n\nAfter our EDA: https:\/\/www.kaggle.com\/s7anmerk\/lean-import-to-save-ram-and-eda\n\nIn the following part we will prepare the data for using it in various models. Therefore, we wrote different functions which we step by step improved during the learning phase. For getting more information how we got towards the functions and dictionaries, please check out the 'Compare embeddings' file.\n\n1. Load datasets\n2. Set up the methods to be used in the preprocessing\n\n*The core:*\n3. Preprocesssing for more conventional methods like LightGBM or Naive Bayes (preprocessing_conv() )\n4. **Prepprocessing for Neural Networks using word embeddings (very fast, only improves unknown words)**\n5. Testing !!!!","5753189d":"**Train Set Testing**","8fc2946a":"From the testing of the method, we can see that it works **extremely quick!** Using the Tweet Tokenizer as well as only correcting unknown words.\nComparing FastText and GloVe coverage we can see that both perform very similar reaching already **98% text coverage using TweetTokenizer** - while google news in out of the box much weaker (for more information see the 'Compare embeddings' notebook). After processing the unknown words, we can **increase the coverage by another 1 % to more than 99% text coverage.** \n\nLooking only towards the **vocabulary coverage we could increase by about 15%** on the trainset to up to 67% coverage. Considering that special names that are not known to the embedding, however also are no misspelling it is a very good score. Still **hughe improvements can be made finding a reliable function to split hashtags and links** such as *'theglobeandmail' to 'the globe and mail'*. The functions we tested were not satisfying, so that we decided to live with the small error.  \n\nAs explained already when describing the method, the outcome is again a dataframe with the **cleaned text where all the words or tokens are splitted by space** (the the printing below). It includes still all relevant punctuation, known contractions, symbols but also unknown words which will be ignored in the neural network pre-processing in the end.\n\nThe cleaned text is handed over to the model. Here it is important to **avoid any build in pre-processing!** Keras for example had build-in tokenizers, which are transforming the text to lower case or remove punctuation. This is not needed after this pre-processing as the embedding vectors can handle these differences. **Thus, to keep the full potential, we should only split by (' ').**","40790341":"#### Method","419db35b":"### Load libraries and datasets","0a37bf2e":"### Pre-processing conventional","fb145938":"#### Method"}}