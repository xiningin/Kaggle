{"cell_type":{"3b38f084":"code","0354115f":"code","52bf939e":"code","5e963c34":"code","209f78c7":"code","fad10169":"code","9af60972":"code","a034f1c3":"code","7daf83cd":"code","d02b66ae":"code","4094cfa6":"code","01167d21":"code","376a2b5b":"code","b76a90ac":"code","d59ab98a":"code","7f99b83d":"code","7d1f4e56":"code","353efba9":"code","d5d11e27":"code","d82c0d3b":"code","a0ceab07":"code","bb969419":"code","e238da3b":"code","5d14f2de":"code","861464b7":"code","a865144b":"code","92ba28f2":"code","7c2ea00a":"code","1ddfc31c":"code","d2598a8c":"code","d0aa9935":"code","7d0af855":"code","842ab429":"code","cea65cc2":"code","96025ac3":"code","d643daa5":"code","603a1e37":"code","b94be4b4":"code","ea24f571":"code","98135856":"code","7c9e8138":"code","38995bc0":"code","4952adf2":"code","4ae8b669":"markdown","7558e74e":"markdown","19b4a188":"markdown","3a5c08b0":"markdown","4637fece":"markdown","a0972d8e":"markdown","5624fc3e":"markdown"},"source":{"3b38f084":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd\nimport re\nimport os\nfrom collections import Counter\nimport numpy as np\n\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \nimport plotly.offline as offline\nimport plotly.graph_objs as go\nplotly.offline.init_notebook_mode(connected=True)\nfrom IPython.display import IFrame\nfrom IPython.core.display import display, HTML\n\n\nfrom textblob import TextBlob\nimport scattertext as st\nimport pyLDAvis.gensim\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nimport gensim\nfrom gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\nfrom gensim.models.wrappers import LdaMallet\nfrom gensim.corpora import Dictionary\neng_stopwords = set(stopwords.words(\"english\"))\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nlem = WordNetLemmatizer()\ntokenizer=ToktokTokenizer()\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport gc\ngc.collect()\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\n# Any results you write to the current directory are saved as output.","0354115f":"#Spacy\nimport string\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\npunctuations = string.punctuation\nfrom nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer() \nown_stop = ['is','the','are','a','be','he','what'] \nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n\n\n#nlp = spacy.load('en')\nnlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])\nparser = English()\n\n","52bf939e":"data = pd.read_csv(\"..\/input\/zomato.csv\",low_memory=True) \ndata_full = data.copy()\nimport gc\ngc.collect()","5e963c34":"data['review_word_count'] = data['reviews_list'].str.split().str.len()\nsns.boxplot(x=data['review_word_count'])","209f78c7":"data = data[data['review_word_count'] < 200]","fad10169":"sns.boxplot(x=data['review_word_count'])","9af60972":"data.duplicated().sum()\ndata.name = data.name.apply(lambda x: x.title())\ndata.rate.replace(('NEW','-'),np.nan,inplace =True)\ndata.rate = data.rate.astype('str')\ndata.rate = data.rate.apply(lambda x: x.replace('\/5','').strip())\ndata.rate = data.rate.astype('float')\ndata.dropna(how ='any', inplace = True)","a034f1c3":"def cleanText(inputString):\n\treview=re.sub(r\"http\\S+\",'' , inputString)\n\treview=re.sub(r'\\W',' ',review) # remove punchations \n\treview=review.lower()\n\treview=re.sub(r'\\s+[a-z]\\s+',' ',review) # remove single characters which have space in starting and end of the characters\n\treview=re.sub(r'^[a-z]\\s+',' ',review) # remnove single characters which have at starting position of the sentence \n\treview=re.sub(r'^[0-9 ]+',' ',review) # remnove single characters which have at starting position of the sentence \n\treview=re.sub(r'[^a-zA-Z0-9\\s]',' ',review) # remove extra spaces.\n\n\treview=re.sub(r'\\s+',' ',review) # remove extra spaces.\n\treview=[lemmatizer.lemmatize(word) for word in review.split()]\n\treview =' '.join(review)\n\treturn review\n\ndef rated_Clean(inputString):\n    review=re.sub(r'\\brated\\b','',inputString)\n    return review\n\n\n\n\n#Spacy Lemma # Own Stop words\n\ndef spacy_lemma_text(text):\n    doc = nlp(text)\n    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n    tokens = [tok for tok in tokens if tok not in spacy_stopwords and tok not in punctuations]\n    tokens = ' '.join(tokens)\n    return tokens","7daf83cd":"##Spacy Stemming\ndata['Clean_Review'] = data['reviews_list'].progress_apply(spacy_lemma_text)\ndata['Clean_Review'] = data['Clean_Review'].progress_apply(lambda x:cleanText(x))\ndata['Clean_Review'] = data['Clean_Review'].progress_apply(spacy_lemma_text)\n","d02b66ae":"gc.collect()","4094cfa6":"def detect_polarity(text):\n    return TextBlob(text).sentiment.polarity\n\ndata['texblo_polarity'] = data.Clean_Review.apply(detect_polarity)\ndata['sentiment_type']=''\ndata.loc[data.texblo_polarity >0.25,'sentiment_type']='POSITIVE'\ndata.loc[data.texblo_polarity==0,'sentiment_type']='NEUTRAL'\ndata.loc[data.texblo_polarity<0.25,'sentiment_type']='NEGATIVE'\n","01167d21":"data['texblo_polarity'].hist(bins=50)","376a2b5b":"x=data['sentiment_type'].value_counts()\nplt.figure(figsize=(9,7))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"# per sentiment\")\nplt.ylabel('# Review Sentiment Count', fontsize=12)\nplt.xlabel('Sentiment ', fontsize=12)\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","b76a90ac":"from tqdm import tqdm\ntqdm.pandas()\n","d59ab98a":"import scattertext as st\nnlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])\ndata['parsed'] = data.Clean_Review.progress_apply(nlp)","7f99b83d":"corpus = st.CorpusFromParsedDocuments(data,\n                             category_col='sentiment_type',\n                             parsed_col='parsed').build()","7d1f4e56":"html = st.produce_scattertext_explorer(corpus,\n           category='POSITIVE',                            \n          category_name='POSITIVE',\n          not_category_name='NEGATIVE',\n          width_in_pixels=600,\n          minimum_term_frequency=15,\n          term_significance = st.LogOddsRatioUninformativeDirichletPrior(),\n          )","353efba9":"filename = \"Postive-vs-Negative.html\"\nopen(filename, 'wb').write(html.encode('utf-8'))\nIFrame(src=filename, width = 1000, height=700)\n","d5d11e27":"#Using log scales\ngc.collect()","d82c0d3b":"html = st.produce_scattertext_explorer(corpus,\n                                       category='POSITIVE',\n                                       category_name='POSITIVE',\n                                       not_category_name='NEGATIVE',\n                                       minimum_term_frequency=15,\n                                       width_in_pixels=700,\n                                       transform=st.Scalers.log_scale_standardize)","a0ceab07":"filename = \"Postive-vs-Negative.html\"\nopen(filename, 'wb').write(html.encode('utf-8'))\nIFrame(src=filename, width = 1900, height=700)\n","bb969419":"#to seperate sentenses into words\ndef preprocess(comment):\n    \"\"\"\n    Function to build tokenized texts from input comment\n    \"\"\"\n    return gensim.utils.simple_preprocess(comment, deacc=True, min_len=3)\n\nall_text=data.Clean_Review.apply(lambda x: preprocess(x))\n","e238da3b":"bigram = gensim.models.Phrases(all_text)\n","5d14f2de":"def clean(word_list):\n    \"\"\"\n    Function to clean the pre-processed word lists \n    \n    Following transformations will be done\n    1) Stop words removal from the nltk stopword list\n    2) Bigram collation (Finding common bigrams and grouping them together using gensim.models.phrases)\n    3) Lemmatization (Converting word to its root form : babies --> baby ; children --> child)\n    \"\"\"\n    #remove stop words\n    clean_words = [w for w in word_list if not w in spacy_stopwords]\n    #collect bigrams\n    clean_words = bigram[clean_words]\n    #Lemmatize\n    clean_words=[lem.lemmatize(word, \"v\") for word in clean_words]\n    return(clean_words)    ","861464b7":"all_text=all_text.apply(lambda x:clean(x))\ndictionary = Dictionary(all_text)\ncorpus = [dictionary.doc2bow(text) for text in all_text]\n","a865144b":"#create the LDA model\nldamodel = LdaModel(corpus=corpus, num_topics=5, id2word=dictionary,alpha='auto',passes=25)","92ba28f2":"pyLDAvis.enable_notebook()\n","7c2ea00a":"#The size of the circle represents what % of the corpus it contains.\npyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n","1ddfc31c":"## Lets Do Some EDA !!!\ndata = data_full.copy()","d2598a8c":"data.rate.replace(('NEW','-'),np.nan,inplace =True)\ndata.rate = data.rate.astype('str')\ndata.rate = data.rate.apply(lambda x: x.replace('\/5','').strip())\ndata.rate = data.rate.astype('float')\n\ndata.dropna(how ='any', inplace = True)","d0aa9935":"data.rename(columns={'approx_cost(for two people)': 'Cost_For_Two', \\\n                     'listed_in(city)': 'Listed_Location','listed_in(type)': 'meal_type'}, inplace=True)\n","7d0af855":"labels = list(data.location.value_counts().index)\nvalues = list(data.location.value_counts().values)\n\nfig = {\n    \"data\":[\n        {\n            \"labels\" : labels,\n            \"values\" : values,\n            \"hoverinfo\" : 'label+percent',\n            \"domain\": {\"x\": [0, .9]},\n            \"hole\" : 0.6,\n            \"type\" : \"pie\",\n            \"rotation\":120,\n        },\n    ],\n    \"layout\": {\n        \"title\" : \"Zomato Bangalore\",\n        \"annotations\": [\n            {\n                \"font\": {\"size\":10},\n                \"showarrow\": True,\n                \"text\": \"Locations\",\n                \"x\":0.2,\n                \"y\":0.9,\n            },\n        ]\n    }\n}\n\niplot(fig)","842ab429":"data.Cost_For_Two = data['Cost_For_Two'].apply(lambda x: x.replace(',','').strip())\nf, ax = plt.subplots(1,1, figsize = (15, 4))\n\nax = sns.countplot(data['rate'])\n","cea65cc2":"data['Cost_For_Two'] = data['Cost_For_Two'].astype(int)\ndata['Cost_For_Two_log'] = np.log1p(data['Cost_For_Two'])\n","96025ac3":"fig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nplt.hist(data['Cost_For_Two']);\nplt.title('Distribution of Cost')","d643daa5":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(data['Cost_For_Two'], data['rate'])\nplt.title('Cost vs Rating');\n","603a1e37":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(data['Cost_For_Two_log'], data['votes'])\nplt.title('Cost(log) vs Votes');\n","b94be4b4":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(data['rate'], data['votes'])\nplt.title('Rating vs Votes');\n","ea24f571":"sns.countplot(data['online_order'])","98135856":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nsns.boxplot(x='book_table', y='Cost_For_Two', data=data.loc[data['book_table'].isin(data['book_table'].value_counts().head(10).index)]);\nplt.title('Book Table vs Cost');\n","7c9e8138":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nsns.boxplot(x='online_order', y='Cost_For_Two', data=data.loc[data['online_order'].isin(data['online_order'].value_counts().head(10).index)]);\nplt.title('Online vs Cost');\n","38995bc0":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nsns.boxplot(x='Cost_For_Two', y='rate', data=data.loc[data['Cost_For_Two'].isin(data['Cost_For_Two'].value_counts().head(10).index)]);\nplt.title('Cost vs Rating');\n","4952adf2":"\"\"\"needed_col = ['rate','votes','texblo_polarity']\ntemp_df=data_full[needed_col]\ncorr=temp_df.corr()\nplt.figure(figsize=(12,8))\nsns.heatmap(corr,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values, annot=True)\"\"\"","4ae8b669":"##Without pre-processing **","7558e74e":"## Scattertext -  visualizing linguistic variation between document categories in a language-independent way. In layman term - visualizations class-associated term frequencies.","19b4a188":"## Alphabetic order among equally frequent terms","3a5c08b0":"In this kernel, we will go through:\n* Scattertext -> Visualizing Empath topics and categories (Postive Review vs Negative)\n* Topic Modeling - LDA\n* EDA - Correlation between Customer Review vs Rating vs Votes\n\n\n\n","4637fece":"## Customer Review - Positive vs Negative ","a0972d8e":"**## Used 1000 rows to avoid run time error.","5624fc3e":"## TextBlob for Sentiment Analysis \n#Try Flair -character-level LSTM neural network which takes sequences of letters and words into account when predicting."}}