{"cell_type":{"517cd72a":"code","2aba85a0":"code","e872a131":"code","76138157":"code","c626a3de":"code","e78f6038":"code","d9c3fe72":"code","c1417be8":"code","1cd8500c":"code","74d46d79":"code","44fd700a":"code","b69b89b3":"code","10a5eeec":"code","2316a2c7":"code","acc04594":"code","0358b56a":"code","f3c4ee07":"code","67dd83b9":"code","0498f1f1":"code","19d48a1a":"code","629ebe07":"code","963446fd":"code","e647cf1c":"code","19dc0e0b":"code","a2387765":"code","c73e1465":"code","f1c2c46f":"code","909df71a":"code","bf931c26":"code","e6b31b0a":"code","b92de13c":"code","a60ac497":"code","c0ac7288":"code","a78f0fb8":"code","ea0b0723":"code","909f30c2":"code","15b3bf4e":"code","6a1e4f4b":"code","eb054ecd":"code","f62c91bf":"code","90e7156e":"code","18e98217":"code","44cf1214":"markdown","ef6e88bc":"markdown"},"source":{"517cd72a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2aba85a0":"# load important packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e872a131":"# data loading\ndata = pd.read_csv('\/kaggle\/input\/datasets_383055_741735_CarPrice_Assignment.csv')\ndata.head()","76138157":"# data info\ndata.info()","c626a3de":"# checking for null values\ndata.isna().sum()","e78f6038":"# getting all columns\ndata.columns","d9c3fe72":"def get_unique_values(dataset):\n    \"\"\" this function will return unique values from a dataset attribute wise\"\"\"\n    df = dataset.select_dtypes(include = np.object)\n    cols = list(df.columns)\n    for i in cols:\n        print('{}: {}'.format(i,df[i].unique()), '\\n')","c1417be8":"get_unique_values(data)","1cd8500c":"# drop unnecessary attributes from data\ndata_clean = data.copy(deep= True)\ndata_clean.drop(columns = ['car_ID','CarName'], axis = 1, inplace = True)\ndata_clean.columns","74d46d79":"# select float datatype data\ndata_clean.select_dtypes(include= [np.float, np.int64]).head()","44fd700a":"# select category data\ndata_clean.select_dtypes(include= np.object).head()","b69b89b3":"df_numeric = data_clean.select_dtypes(include= [np.float, np.int64])\ndf_cat = data_clean.select_dtypes(include= np.object)","10a5eeec":"# pairplot for better understanding of data\nsns.pairplot(df_numeric)\nplt.show()","2316a2c7":"df_cat.columns","acc04594":"# barplots for target variable importance on input attributes\nfor i in list(df_cat.columns):\n    plt.style.use('seaborn')\n    sns.barplot(x = i, y = 'price', data = data_clean,estimator= sum )\n    plt.show()","0358b56a":"data_clean.groupby('fueltype')['price'].sum().sort_values(ascending = False)","f3c4ee07":"# more pandas way to see the data group wise\nfor i in list(df_cat.columns):\n    print(pd.DataFrame(data_clean.groupby(i)['price'].sum().sort_values(ascending = False)), '\\n')","67dd83b9":"# boxplots for outliers\nfor i in list(df_numeric.columns):\n    plt.boxplot(i, data = df_numeric)\n    plt.title(i)\n    plt.show()","0498f1f1":"data_clean.head()","19d48a1a":"# create a final data with dummies\nfinal_data = pd.get_dummies(data_clean, columns= list(df_cat.columns),drop_first= True)\nfinal_data.columns","629ebe07":"\nprint('original data shape : {}'.format(data.shape))\nprint('final data shape : {}'.format(final_data.shape))","963446fd":"# scaling\nfrom sklearn.preprocessing import MinMaxScaler\nms = MinMaxScaler()\nfinal_data[list(df_numeric.columns)[:-1]] = ms.fit_transform(final_data[list(df_numeric.columns)[:-1]])","e647cf1c":"X_data = final_data.drop(columns= 'price')\ny_data = final_data['price']","19dc0e0b":"# array of input and target variable\nX = X_data.iloc[:].values\ny = y_data.iloc[:].values","a2387765":"# divide the data into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","c73e1465":"# import all machine learning packages from sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb","f1c2c46f":"# build models\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\ndt_model = DecisionTreeRegressor()\ndt_model.fit(X_train, y_train)\nrf_model = RandomForestRegressor()\nrf_model.fit(X_train,y_train)\nxg_model = xgb.XGBRegressor()\nxg_model.fit(X_train,y_train)","909df71a":"# train model evaluation\nprint('linear model score: {}'.format(lr_model.score(X_train, y_train)))\nprint('decison model score: {}'.format(dt_model.score(X_train, y_train)))\nprint('rf model score: {}'.format(rf_model.score(X_train, y_train)))\nprint('xg boost model score: {}'.format(xg_model.score(X_train, y_train)))","bf931c26":"# test model evaluation\nprint('linear model score: {}'.format(lr_model.score(X_test, y_test)))\nprint('decison model score: {}'.format(dt_model.score(X_test, y_test)))\nprint('rf model score: {}'.format(rf_model.score(X_test, y_test)))\nprint('xg boost model score: {}'.format(xg_model.score(X_test, y_test)))","e6b31b0a":"# getting predictions on all models \ny_pred_lr = lr_model.predict(X_test)\ny_pred_dt = dt_model.predict(X_test)\ny_pred_rf = rf_model.predict(X_test)\ny_pred_xg = xg_model.predict(X_test)","b92de13c":"# error metrics for all models\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nprint('linear reg metrics', '\\n')\nprint('mae score : {}'.format(mean_absolute_error(y_test, y_pred_lr)))\nprint('mse score : {}'.format(mean_squared_error(y_test, y_pred_lr)))\nprint('rmse score : {}'.format(np.sqrt(mean_squared_error(y_test, y_pred_lr))))\nprint('R2 score : {}'.format(r2_score(y_test, y_pred_lr)))","a60ac497":"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nprint('decision tree metrics', '\\n')\nprint('mae score : {}'.format(mean_absolute_error(y_test, y_pred_dt)))\nprint('mse score : {}'.format(mean_squared_error(y_test, y_pred_dt)))\nprint('rmse score : {}'.format(np.sqrt(mean_squared_error(y_test, y_pred_dt))))\nprint('R2 score : {}'.format(r2_score(y_test, y_pred_dt)))","c0ac7288":"\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nprint('randomforest metrics', '\\n')\nprint('mae score : {}'.format(mean_absolute_error(y_test, y_pred_rf)))\nprint('mse score : {}'.format(mean_squared_error(y_test, y_pred_rf)))\nprint('rmse score : {}'.format(np.sqrt(mean_squared_error(y_test, y_pred_rf))))\nprint('R2 score : {}'.format(r2_score(y_test, y_pred_rf)))","a78f0fb8":"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nprint('xgboost metrics', '\\n')\nprint('mae score : {}'.format(mean_absolute_error(y_test, y_pred_xg)))\nprint('mse score : {}'.format(mean_squared_error(y_test, y_pred_xg)))\nprint('rmse score : {}'.format(np.sqrt(mean_squared_error(y_test, y_pred_xg))))\nprint('R2 score : {}'.format(r2_score(y_test, y_pred_xg)))","ea0b0723":"#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(dt_model.feature_importances_, index=X_data.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","909f30c2":"feat_importances = pd.Series(rf_model.feature_importances_, index=X_data.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","15b3bf4e":"feat_importances = pd.Series(xg_model.feature_importances_, index=X_data.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","6a1e4f4b":"# Applying PCA (diamentionality reduction technique)\n# 95% of variance\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 0.95)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_","eb054ecd":"pca.explained_variance_","f62c91bf":"X_train.shape","90e7156e":"# Applying PCA\n# 99% of variance\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 0.99)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_","18e98217":"X_train.shape","44cf1214":"### Note: we saw diamentionality reduction from original training set after applying PCA","ef6e88bc":"### Note: The idea of feature importance and PCA was to give you an idea about it"}}