{"cell_type":{"cb72d644":"code","dfa9ef1a":"code","44cdb8e5":"code","bd187dac":"code","51de7ca2":"code","cf016e83":"code","cc6c75a1":"code","99658a63":"code","675abf7d":"code","891c3afb":"code","62906c6c":"code","a3fff26c":"code","07b1b3f3":"code","94fda349":"code","efa29b95":"code","b3c65076":"code","5e330ccb":"code","6c59e1ac":"code","38f11686":"code","e7161b26":"code","ce08a880":"code","175f5698":"code","241afc51":"code","a56758f3":"code","aebeb7a4":"code","c5c855c2":"code","c9fa365f":"code","2132abe1":"markdown","e5553964":"markdown","365769db":"markdown","b99bc43c":"markdown","313f7678":"markdown","734d292b":"markdown","c9341f86":"markdown","d6c1bb0b":"markdown","402d5234":"markdown","bb520650":"markdown","a455f182":"markdown","28cad70b":"markdown","7a5370e0":"markdown","f6186de6":"markdown","7f1ff175":"markdown"},"source":{"cb72d644":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt # ploting data and visualization \n\nfrom sklearn import datasets # inbuilt planar datasets\nfrom sklearn.linear_model import LogisticRegressionCV # Logistic Regression Classifier","dfa9ef1a":"def load_extra_datasets():  \n    N = 200\n    noisy_circles = datasets.make_circles(n_samples=N, factor=.5, noise=.3)\n    noisy_moons = datasets.make_moons(n_samples=N, noise=.2)\n    gaussian_quantiles = datasets.make_gaussian_quantiles(mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)\n    \n    return noisy_circles, noisy_moons, gaussian_quantiles","44cdb8e5":"noisy_circles, noisy_moons, gaussian_quantiles = load_extra_datasets()\n\nX, y = noisy_moons\nX, y = X.T, y.reshape(1, y.shape[0])\n\nplt.scatter(X[0, :], X[1, :], c=y, s=40, cmap=plt.cm.Spectral);","bd187dac":"# spiral dataset\n\n# N = 100 # number of points per class\n# D = 2 # dimensionality\n# K = 2 # number of classes\n# X = np.zeros((N*K,D)) # data matrix (each row = single example)\n# y = np.zeros(N*K, dtype='uint8') # class labels\n# for j in range(K):\n#   ix = range(N*j,N*(j+1))\n#   r = np.linspace(0.0,1,N) # radius\n#   t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n#   X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n#   y[ix] = j\n\n# X= X.T # every row represents features and every column represents different examples \n# y= y.reshape(1, 200) # a row vector representing different class labels\n\n# # lets visualize the data:\n# plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.copper)\n# plt.show()","51de7ca2":"print('Shape of X:', X.shape)\nprint('Shape of y:', y.shape)","cf016e83":"# Train the logistic regression classifier\nclf = LogisticRegressionCV()\nclf.fit(X.T, y.T)","cc6c75a1":"def plot_decision_boundary(model, X, y):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    Z = model(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)","99658a63":"# Plot the decision boundary for logistic regression\nplot_decision_boundary(lambda x: clf.predict(x), X, y)\nplt.title(\"Logistic Regression\")\n\n# Print accuracy\nLR_predictions = clf.predict(X.T)\nprint('accuracy of Logistic Regression model is:', sum(sum(LR_predictions == y))\/y.shape[1]*100)","675abf7d":"def layer_sizes(X, y):\n    \"\"\"\n    Arguments:\n    X -- input dataset of shape (input size, number of examples)\n    Y -- labels of shape (output size, number of examples)\n    \n    Returns:\n    n_x -- the size of the input layer\n    n_h -- the size of the hidden layer\n    n_y -- the size of the output layer\n    \"\"\"\n    \n    n_x = X.shape[0] # number of input features\n    n_h = 4 # number of nodes in the hidden layer\n    n_y = y.shape[0] # number of output nodes\n    \n    return (n_x, n_h, n_y)","891c3afb":"(n_x, n_h, n_y) = layer_sizes(X, y)\nprint(\"The size of the input layer is: n_x = \" + str(n_x))\nprint(\"The size of the hidden layer is: n_h = \" + str(n_h))\nprint(\"The size of the output layer is: n_y = \" + str(n_y))","62906c6c":"def initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    params -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"\n    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n    \n    W1 = np.random.randn(n_h, n_x) * 0.01\n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y, n_h) * 0.01\n    b2 = np.zeros((n_y, 1))\n    \n    assert (W1.shape == (n_h, n_x))\n    assert (b1.shape == (n_h, 1))\n    assert (W2.shape == (n_y, n_h))\n    assert (b2.shape == (n_y, 1))\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","a3fff26c":"parameters = initialize_parameters(n_x, n_h, n_y)\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","07b1b3f3":"def forward_propagation(X, parameters):\n    \"\"\"\n    Argument:\n    X -- input data of size (n_x, m)\n    parameters -- python dictionary containing your parameters (output of initialization function)\n    \n    Returns:\n    A2 -- The sigmoid output of the second activation\n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n    \"\"\"\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    Z1 = np.dot(W1, X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = 1\/(1+np.exp(-Z2))\n    \n    assert(A2.shape == (1, X.shape[1]))\n    \n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","94fda349":"A2, cache = forward_propagation(X, parameters)\n\n# Note: we use the mean here just to make sure that your output matches ours. \nprint(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))","efa29b95":"def compute_cost(A2, Y, parameters):\n    \"\"\"\n    Computes the cross-entropy cost given in equation (13)\n    \n    Arguments:\n    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n    [Note that the parameters argument is not used in this function, \n    but the auto-grader currently expects this parameter.\n    Future version of this notebook will fix both the notebook \n    and the auto-grader so that `parameters` is not needed.\n    For now, please include `parameters` in the function signature,\n    and also when invoking this function.]\n    \n    Returns:\n    cost -- cross-entropy cost given equation (13)\n    \n    \"\"\"\n    \n    m = Y.shape[1] # number of example\n    \n    cost = (-1\/m)*(np.dot(np.log(A2), Y.T) + np.dot(np.log(1-A2), Y.T))\n    \n    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n                                    # E.g., turns [[17]] into 17 \n    assert(isinstance(cost, float))\n    \n    return cost","b3c65076":"print(\"cost = \" + str(compute_cost(A2, y, parameters)))","5e330ccb":"def backward_propagation(parameters, cache, X, Y):\n    \"\"\"\n    Implement the backward propagation using the instructions above.\n    \n    Arguments:\n    parameters -- python dictionary containing our parameters \n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n    X -- input data of shape (2, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    \n    Returns:\n    grads -- python dictionary containing your gradients with respect to different parameters\n    \"\"\"\n    m = X.shape[1]\n    \n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    \n    A1 = cache['A1']\n    A2 = cache['A2']\n    \n    dZ2 = A2 - Y\n    dW2 = (1\/m)*(np.dot(dZ2, A1.T))\n    db2 = (1\/m)*(np.sum(dZ2, axis=1, keepdims=True))\n    dZ1 = np.dot(W2.T, dZ2)*(1-np.power(A1, 2))\n    dW1 = (1\/m)*(np.dot(dZ1, X.T))\n    db1 = (1\/m)*(np.sum(dZ1, axis=1, keepdims=True))\n    \n    \n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads","6c59e1ac":"grads = backward_propagation(parameters, cache, X, y)\nprint (\"dW1 = \"+ str(grads[\"dW1\"]))\nprint (\"db1 = \"+ str(grads[\"db1\"]))\nprint (\"dW2 = \"+ str(grads[\"dW2\"]))\nprint (\"db2 = \"+ str(grads[\"db2\"]))","38f11686":"def update_parameters(parameters, grads, learning_rate = 1.2):\n    \"\"\"\n    Updates parameters using the gradient descent update rule given above\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients \n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    b1 = parameters['b1']\n    b2 = parameters['b2']\n    \n    dW1 = grads['dW1']\n    db1 = grads['db1']\n    dW2 = grads['dW2']\n    db2 = grads['db2']\n    \n    W1 -= learning_rate*dW1\n    b1 -= learning_rate*db1\n    W2 -= learning_rate*dW2\n    b2 -= learning_rate*db2\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","e7161b26":"parameters = update_parameters(parameters, grads)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","ce08a880":"def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n    \"\"\"\n    Arguments:\n    X -- dataset of shape (2, number of examples)\n    Y -- labels of shape (1, number of examples)\n    n_h -- size of the hidden layer\n    num_iterations -- Number of iterations in gradient descent loop\n    print_cost -- if True, print the cost every 1000 iterations\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n    \n    parameters = initialize_parameters(n_x, n_h, n_y)\n    \n    for i in range(0, num_iterations):\n        A2, cache = forward_propagation(X, parameters)\n        \n        cost = compute_cost(A2, Y, parameters)\n        \n        grads = backward_propagation(parameters, cache, X, Y)\n        \n        parameters = update_parameters(parameters, grads)\n\n    if print_cost and i % 1000 == 0:\n        print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    return parameters","175f5698":"parameters = nn_model(X, y, 4, num_iterations=10000)\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","241afc51":"def predict(parameters, X):\n    \"\"\"\n    Using the learned parameters, predicts a class for each example in X\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (n_x, m)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 \/ blue: 1)\n    \"\"\"\n    \n    A2, cache = forward_propagation(X, parameters)\n    predictions = (A2>0.5)\n    \n    return predictions","a56758f3":"# Build a model with a n_h-dimensional hidden layer\nparameters = nn_model(X, y, n_h = 5, num_iterations = 10000)\n\n# Plot the decision boundary\nplot_decision_boundary(lambda x: predict(parameters, x.T), X, y)\nplt.title(\"Decision Boundary for hidden layer size \" + str(4))\nplt.show()","aebeb7a4":"# Print accuracy\npredictions = predict(parameters, X)\nprint('Accuracy: %.1f' %(sum(sum(y == predict(parameters, X)))\/y.shape[1]*100) + '%')","c5c855c2":"plt.figure(figsize=(16, 32))\nhidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\nfor i, n_h in enumerate(hidden_layer_sizes):\n    plt.subplot(5, 2, i+1)\n    plt.title('Hidden Layer of size %d' % n_h)\n    parameters = nn_model(X, y, n_h, num_iterations = 5000)\n    plot_decision_boundary(lambda x: predict(parameters, x.T), X, y)\n    predictions = predict(parameters, X)\n    accuracy = sum(sum(y == predict(parameters, X)))\/y.shape[1]*100\n    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))","c9fa365f":"# I hope you like this notebook\n# If you have any doubts or suggestions please comment below, looking forward to hear from you guys\n# If you have reached here please upvote this notebook, It will boost my confidence","2132abe1":"## Calculating the cost","e5553964":"### Nodes in different layers","365769db":"# Importing libraries","b99bc43c":"## Updating parameters","313f7678":"## Predicting the accuracy","734d292b":"# Shape of the dataset","c9341f86":"### Building Logistic Regression classifier and checking what its accuracy is","d6c1bb0b":"## One can either use imported datasets from sklearn or the one created below\n#### comment the other one \n### There are 4 type of datasets we have\n#### 1) noisy_circles\n#### 2) noisy_moons\n#### 3) gaussian_quantiles\n#### 4) spiral","402d5234":"###### Interpretation: The dataset is not linearly separable, so logistic regression doesn't perform well. Hopefully a neural network will do better. Let's try this now!","bb520650":"### Plot the decision boundary and check the performance of our classifier","a455f182":"# Lets create a Neural Network with One hidden layer and see how it performs","28cad70b":"## Backward propagation","7a5370e0":"## Creating the model using all the methods created above","f6186de6":"## Forward propagation using tanh activation function","7f1ff175":"## Initializing parameters"}}