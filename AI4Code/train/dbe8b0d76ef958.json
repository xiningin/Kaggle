{"cell_type":{"d413d720":"code","f66974ce":"code","52c480d1":"code","720dedc8":"code","e25a0434":"code","28763d7d":"code","234d255c":"code","293ab74f":"code","30ab281c":"code","291ef08d":"code","3c8d1756":"code","e450a970":"code","d41571db":"code","d9fedd5b":"code","99f74cda":"code","31879005":"code","93f9261b":"code","aa9bc12e":"code","bbba74d3":"code","5db40d0d":"code","df9cbdb2":"code","c6b13eaf":"markdown","ec041112":"markdown","dc696002":"markdown","06556652":"markdown","5213ecf7":"markdown","10c04fd2":"markdown","5b303cdd":"markdown","7c92a8c1":"markdown","9aa00c6b":"markdown","5cc2339e":"markdown","9b201b01":"markdown","11fe9e46":"markdown","8b2d7ace":"markdown"},"source":{"d413d720":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f66974ce":"# read train\ntrain = pd.read_csv(\"..\/input\/train.csv\")\nprint(train.shape)\ntrain.head()","52c480d1":"# read test\ntest = pd.read_csv(\"..\/input\/test.csv\")\nprint(test.shape)\ntest.head()","720dedc8":"# put labels into y_train variable \nY_train = train[\"label\"]\n\n# Drop label column\nX_train = train.drop(labels=[\"label\"], axis=1)","e25a0434":"# visualize number of digits classes\nplt.figure(figsize =(15,10))\ng = sns.countplot(Y_train, palette=\"icefire\")\nplt.title(\"Number of digit classes\")\nY_train.value_counts()","28763d7d":"# plot for the three number\nimg = X_train.iloc[9].as_matrix() # as_matrix: Converting to Matrix\nimg = img.reshape((28,28))\nplt.imshow(img, cmap='gray')\nplt.title(train.iloc[0,0])\nplt.axis(\"off\")\nplt.show()","234d255c":"# plot for the seven number\nimg = X_train.iloc[6].as_matrix()\nimg = img.reshape((28,28))\nplt.imshow(img, cmap='gray')\nplt.title(train.iloc[3,0])\nplt.axis(\"off\")\nplt.show()","293ab74f":"X_train = X_train \/ 255.0\ntest = test \/ 255.0\nprint(\"X_train shape: \", X_train.shape)\nprint(\"test shape: \", test.shape)","30ab281c":"# reshape\nX_train = X_train.values.reshape(-1,28,28,1)\nterst = test.values.reshape(-1,28,28,1)\nprint(\"x_train shape: \", X_train.shape)\nprint(\"test shape: \", test.shape)","291ef08d":"# Label encoding \nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding**\nY_train = to_categorical(Y_train, num_classes = 10)","3c8d1756":"# split the train and the validation set for the fittig\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.1, random_state=2)\nprint(\"X_train shape: \", X_train.shape)\nprint(\"X_test shape: \", X_test.shape)\nprint(\"Y_train shape: \", Y_train.shape)\nprint(\"Y_test shape: \", Y_test.shape)","e450a970":"# examples for the eight\nplt.imshow(X_train[0][:,:,0], cmap='gray')\nplt.show()","d41571db":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical  # convert to one-hot-encoding\nfrom keras.models import Sequential # Sequential: A structure with layers in it.\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\n#\nmodel.add(Conv2D(filters=8, kernel_size=(5,5), padding='Same', activation='relu', input_shape=(28,28,1)))\nmodel.add(MaxPool2D(pool_size=(2,2))) # Max pooling: Transfer the max values \u200b\u200bin our image to the pooling layer.\nmodel.add(Dropout(0.25)) \n#\nmodel.add(Conv2D(filters=16, kernel_size=(3,3), padding='Same', activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2))) \nmodel.add(Dropout(0.25))\n# fully connected\nmodel.add(Flatten())# flatting : Doing a straight vector by extending our matrix\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))  # softmax: It is a more generalized version of Sigmoid.","d9fedd5b":"# define the optimizer \noptimizer = Adam(lr=0.003, beta_1=0.9, beta_2=0.999)","99f74cda":"# loss: If the error is too many we update the weight, until the error is minimized\n# categorical_crossentropy : If the classification is more than 2, we use categorical_crossentropy.\nmodel.compile(optimizer = optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","31879005":"epochs = 100 # for better result increase the epochs\nbatch_size = 500","93f9261b":"# Data augmentation\ndatagen = ImageDataGenerator(\n    featurewise_center = False, # set input mean to 0 over the dataset\n    samplewise_center = False,  # set each sample mean to 0 \n    featurewise_std_normalization = False,  # divide inputs by std of the dataset\n    samplewise_std_normalization = False,   # divide each input by its std\n    zca_whitening = False,  # dimension reduction\n    rotation_range = 0.5,  # Randomly rotate images in the range 5 degrees\n    zoom_range = 0.5,   # Randomly zoom image 5%\n    width_shift_range = 0.5,  # Randomly shift images horizontally 5%\n    height_shift_range = 0.5,  # Randomly shift images vertically 5%\n    horizontal_flip = False, # Randomly flip images\n    vertical_flip = False) # Randomly flip images\n\ndatagen.fit(X_train)","aa9bc12e":"history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size),\n                             epochs = epochs, validation_data = (X_test, Y_test), steps_per_epoch = X_train.shape[0] \/\/ batch_size)","bbba74d3":"# Plot the loss and accuracy curves for training and validation\nplt.plot(history.history['val_loss'], color='r', label= \"validation loss\")\nplt.title(\"Test Loss\")\nplt.xlabel(\"Number of epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","5db40d0d":"# Plot the loss and accuracy curves for training and validation\nplt.plot(history.history['val_acc'], color='b', label= \"validation accuracy\")\nplt.title(\"Test Accuracy\")\nplt.xlabel(\"Number of epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","df9cbdb2":"#Confusion Matrix\n# Predict the values from the validation dataset\ny_pred = model.predict(X_test)\n# Convert predictions classes to one hot vectors\ny_pred_classes = np.argmax(y_pred, axis=1)\n# Convert validation observation to one hot vectors\ny_true = np.argmax(Y_test, axis=1)\n# Compute the confussion matrix\nconfusion_mtx = confusion_matrix(y_true, y_pred_classes)\n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(16,8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01, cmap=\"Blues\", linecolor=\"Green\", fmt='.1f', ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","c6b13eaf":"<a id=\"1\"><\/a> <br>\n## 2) Preparing Dataset","ec041112":"<a id=\"0\"><\/a> <br>\n## 1) Introduction\n* We will be working on this kernel Degit Recognizer data. We'll introduce 80% of the Degit Recognizer we have, and we will try to predict the remaining 20%.\n* In this Kernel we will do the Convolutional Neural Network (CNN) step by step.\n* Let's start by creating our libraries\n","dc696002":"<a id=\"4\"><\/a> <br>\n## 5) Convolutional Neural Network (CNN)\n* Cnn is generally used for image classification.\n* We will do step by step.\n","06556652":"<a id=\"8\"><\/a>\n### D) Data Augmentation\n* In order to prevent Over Fitting, we are making minor changes to our training data and through these changes we are reproducing imprint like img.\n","5213ecf7":"<a id=\"7\"><\/a>\n### C) Epochs and Batch Size","10c04fd2":"<a id=\"9\"><\/a>\n### E) Fit the Model\n","5b303cdd":"<a id=\"5\"><\/a> <br>\n### A) Define Optimizer\n---\n* Adam optimizer : Adaptive momentum. If we use Adam, learning_rate is not fixed. It updates Learning_rate and enables us to learn more quickly.","7c92a8c1":"<a id=\"6\"><\/a>\n### B) Compile Model\n","9aa00c6b":"<a id=\"11\"><\/a> <br>\n> # CONCLUSION \n* If you want a more detailed kernel. Check out DATAI TEAM's Convolutional Neural Network (CNN) Tutorial Kernel. https:\/\/www.kaggle.com\/kanncaa1\/convolutional-neural-network-cnn-tutorial\n---\n<br> **Thank you for your votes and comments.**                                                                                                                                             \n<br>**If you have any suggest, May you write for me, I will be happy to hear it.**","5cc2339e":"<a id=\"10\"><\/a>\n### F) Evaluate the Model\n* Test Loss visualization\n* Confusion matrix","9b201b01":"# Cihan Yatbaz\n###  10 \/ 12 \/ 2018\n---\n\n\n1.  [Introduction:](#0)\n2. [Preparing Dataset :](#1)\n3. [Normalization, Reshape and Label Encoding:](#2)\n4. [Train Test Split  :](#3)\n5. [Convolutional Neural Network (CNN)  :](#4)\n    1. [Define Optimizer  :](#5)\n    1. [Compile Model  :](#6)\n    1. [Epochs and Batch Size  :](#7)\n    1. [Data Augmentation  :](#8)\n    1. [Fit the model  :](#9)\n    1. [Evaluate the Model  :](#10)\n6. [CONCLUSION :](#11)","11fe9e46":"<a id=\"3\"><\/a> <br>\n## 4) Train Test Split\n* We'll introduce 80% of the Degit Recognizer we have, and we will try to predict the remaining 20%.","8b2d7ace":"<a id=\"2\"><\/a> <br>\n## 3) Normalization, Reshape and Label Encoding\n* Normalization\n    * Increases CNN's operating speed.\n* Reshape\n    * Keras works with 3D matrix does not work with 2D so it must be 3D.\n* Label Encoding\n    * Encode labels to one hot vectors\n        * 2 => [0,0,1,0,0,0,0,0,0,0]\n        * 4 => [0,0,0,0,1,0,0,0,0,0]\n---\n* The maximum value that a picture can take is 355, so we divide by 255. And we're normalizing x_train and y_train."}}