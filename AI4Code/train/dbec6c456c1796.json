{"cell_type":{"db217775":"code","e4308353":"code","ed181a31":"code","8780aece":"code","9e3f74c9":"code","4176f392":"code","427aa602":"code","b46dd0df":"code","b32c581b":"code","96f8db82":"code","4c40c21d":"code","f1a5ee75":"code","c6e0043b":"code","e7b8be7d":"code","8a0a1c58":"code","a2bc3712":"code","44410b16":"code","9b8a0985":"code","b66fb02d":"code","f0ca41c2":"code","f42ccf9c":"code","d9657b25":"code","609b73d4":"code","2fb268f7":"code","d00a1218":"code","57210210":"code","e29edc12":"code","9854ae77":"code","2086cbd6":"code","88420165":"code","2c29a5a7":"code","ba29c964":"code","7c2e0a4a":"code","d338ad22":"code","6554b2c8":"code","43f67e36":"code","17779f77":"code","e4526dc1":"code","381f273b":"code","9e316ade":"markdown","33e2c938":"markdown","96978807":"markdown","085fdfbd":"markdown","51bf332d":"markdown","8280d05e":"markdown","bfdde923":"markdown","aa648201":"markdown","58e78d02":"markdown","7999f11f":"markdown","28aab5ae":"markdown","644b5055":"markdown","3dbbc488":"markdown","5b9067da":"markdown","79ccc2d3":"markdown","76e54b0c":"markdown","0989428a":"markdown","080c946c":"markdown","16455488":"markdown","e269907e":"markdown","df2f185d":"markdown","8a1d3642":"markdown","6bdf66f2":"markdown","eca9c2ef":"markdown","2e602bf1":"markdown","92aa1ccd":"markdown","5bf1e259":"markdown","34994a9a":"markdown","e50e0fdc":"markdown","5631ffaf":"markdown","f68a552e":"markdown","d5fa4996":"markdown","1010a46f":"markdown"},"source":{"db217775":"# Import torch and other required modules\nimport torch\nimport numpy as np","e4308353":"# Example 1 - working (change this)\nli=[\n    [\n        [1,2,3],\n        [4,5,6],\n        [7,8,9]\n    ],\n    [\n        [10,11,12],\n        [13,14,15],\n        [16,17,18]\n    ],\n    [\n        [19,20,21],\n        [22,23,24],\n        [25,26,27]\n    ]\n]\ntorch.tensor(li)","ed181a31":"tu=tuple(li)\ntorch.tensor(tu)","8780aece":"arr=np.array(tu)\ntorch.tensor(arr)","9e3f74c9":"li1=[\n    [\n        [1,2],  # Here we have removed 3\n        [4,5,6],\n        [7,8,9]\n    ],\n    [\n        [10,11,12],\n        [13,15], #here we have removed 14\n        [16,17,18]\n    ],\n    [\n        [19,20,21],\n        [22,23,24],\n        [25,26,27]\n    ]\n]\ntorch.tensor(li1)","4176f392":"li2=[\n    [\n        [1,2],  # Here we have removed 3\n        [4,5,6],\n        [7,8,9]\n    ],\n    [\n        [10,11], # here we have removed 12\n        [13,14,15], \n        [16,17,18]\n    ],\n    [\n        [19,21], # Here we have removed 20\n        [22,23,24],\n        [25,26,27]\n    ]\n]\ntorch.tensor(li2)","427aa602":"li3=[\n    [\n        [1,2],  \n        [5,6],\n        [8,9]\n    ],\n    [\n        [10,11], \n        [13,15], \n        [16,18]\n    ],\n    [\n        [19,21], \n        [22,23],\n        [25,26]\n    ]\n]\ntorch.tensor(li3)","b46dd0df":"# A tensor with values 0 to 9\nx=torch.arange(10)\n\n# A tensor with multiples of 3 upto 30(including)\nx1=torch.arange(3,31,3) # Since the last value to be considered is 31-1=30\n\nx,x1","b32c581b":"y=torch.randn(5)\n\n# Creating a 2D Tensor\ny1=torch.randn((3,3))\n\n# Creating a 3D Tensor\ny2=torch.randn((2,3,4))\n\ny,y1,y2","96f8db82":"# 5 logarithmically spaced values between 2^1 and 2^10\nz=torch.logspace(1,10,5,base=2)\n\n# 10 logarithmically spaced values between 10^0 and 10^2\nz1=torch.logspace(0,2,steps=10,base=10)\n\n# 2 logarithmically spaced values netween 4^0 and 4^2\nz2=torch.logspace(0,2,2,4)\n\nz,z1,z2","4c40c21d":"# 5 linearly spaced numbers from 1 to 10\na=torch.linspace(1,10,5)\n\n# 5 linearly spaced numbers from 2 to 12\na1=torch.linspace(2,12,6)\n\na,a1","f1a5ee75":"type(x),type(y),type(z),type(a), type(torch.tensor([]))","c6e0043b":"v = torch.tensor([0., 0., 0.], requires_grad=True)\nv.backward(torch.tensor([0., 2., 3.]))\nv.grad","e7b8be7d":"# So here we will never get a Gradient which is zero the same can be extended to always positive gradients and \n# always greater than zero gradients\ndef f1(grad):\n    for i in range(grad.numel()):\n        if grad[i]==0:\n            grad[i]+=1\n\nv = torch.tensor([0., 0., 0.], requires_grad=True)\nh = v.register_hook(f1)\nv.backward(torch.tensor([0., 2., 3.]))\nv1=v.grad\nh.remove()\n\nv2= torch.tensor([0., 0., 0.], requires_grad=True)\nv2.backward(torch.tensor([0., 2., 3.]))\nv3=v2.grad\n\nv1,v3","8a0a1c58":"# So here Gradient is always greater than zero gradients\ndef f1(grad):\n    for i in range(grad.numel()):\n        if grad[i]<0:\n            grad[i]*=-1\n        elif grad[i]==0:\n            grad[i]+=1\n\nv = torch.tensor([0., 1., 0.], requires_grad=True)\nh = v.register_hook(f1)\nv.backward(torch.tensor([-5., 0., 3.]))\nv1=v.grad\nh.remove()\n\nv2= torch.tensor([0., 1., 0.], requires_grad=True)\nv2.backward(torch.tensor([-5., 0., 3.]))\nv3=v2.grad\n\nv1,v3","a2bc3712":"v=torch.tensor([3.,3.,3.,3.], requires_grad=True)\nlearning_rate=1\nh=v.register_hook(lambda grad:grad*learning_rate)\nv.backward(torch.tensor([1.,1.5,3.,6.]))\nv.grad","44410b16":"v=torch.tensor([3.,3.,3.,3.], requires_grad=True)\nlearning_rate=0.1\nh=v.register_hook(lambda grad:grad*learning_rate)\nv.backward(torch.tensor([1.,1.5,3.,6.]))\nv.grad","9b8a0985":"# Breaking The Function\nv=torch.tensor([3.,3.,3.,3.], requires_grad=True)\nlearning_rate=0.1\nh=v.register_hook(lambda x:x[0]*learning_rate)\nv.backward(torch.tensor([1.,1.5,3.,6.]))\nv.grad","b66fb02d":"s=torch.tensor(\n    [\n        [0,0,0],\n        [4,0,0],\n        [0,1,0]\n    ]\n)\ns.to_sparse()","f0ca41c2":"s.to_sparse(sparse_dim=1)","f42ccf9c":"s.to_sparse(3)","d9657b25":"s1=torch.tensor(\n    [\n        [\n            [0,2,0],\n            [4,0,0],\n            [0,0,0]\n        ],\n        [\n            [0,0,12],\n            [0,0,0],\n            [16,0,0]\n        ],\n        [\n            [0,0,0],\n            [0,0,0],\n            [0,0,0]\n        ]\n    ]\n)\n\ns1.to_sparse(3)","609b73d4":"t=torch.tensor([1,2,3])\nt1=torch.tensor([1,0,0])\nt2=torch.tensor([0,1,0])\nt3=torch.tensor([0,0,1])","2fb268f7":"(torch.Tensor.dot(t,t), # 1*1 + 2*2 + 3*3\n torch.Tensor.dot(t,t1), # 1*1 + 0*2 + 0*3\n torch.Tensor.dot(t,t2), # 0*1 + 1*2 + 0*3\n torch.Tensor.dot(t,t3)) # 0*1 + 0*2 + 1*3","d00a1218":"ten=torch.tensor([\n    [0,1],\n    [2,3]\n])\ntorch.Tensor.dot(ten,ten)","57210210":"ten_sor=torch.tensor([\n    [\n        [1,2,3],\n        [4,5,6],\n        [7,8,9]\n    ],\n    [\n        [10,11,12],\n        [13,14,15],\n        [16,17,18]\n    ],\n    [\n        [19,20,21],\n        [22,23,24],\n        [25,26,27]\n    ]\n])\nten_sor.shape","e29edc12":"(\n    torch.Tensor.flatten(ten_sor,start_dim=1,end_dim=2).shape,# 0th dimension will be untouched\n    torch.Tensor.flatten(ten_sor,start_dim=0,end_dim=1).shape # 2nd dimesion will be untouched\n)","9854ae77":"torch.Tensor.flatten(ten_sor,start_dim=1,end_dim=2) \n\n#same as torch.flatten(ten_sor,start_dim=-2,end_dim=-1) \n","2086cbd6":"torch.Tensor.flatten(ten_sor,start_dim=0,end_dim=1)\n\n#same as torch.flatten(ten_sor,start_dim=-3,end_dim=-2)","88420165":"# default #same as torch.flatten(ten_sor,start_dim=-3,end_dim=-1) or \n# same as torch.flatten(ten_sor,start_dim=0 ,end_dim=2)\ntorch.Tensor.flatten(ten_sor)","2c29a5a7":"torch.Tensor.flatten(ten_sor,start_dim=2,end_dim=0)","ba29c964":"torch.Tensor.flatten(ten_sor,start_dim=3)","7c2e0a4a":"te=torch.tensor([\n    [\n        [1,2,3],\n        [4,5,6],\n        [7,8,9]\n    ],\n    [\n        [10,11,12],\n        [13,14,15],\n        [16,17,18]\n    ],\n    [\n        [19,20,21],\n        [22,23,24],\n        [25,26,27]\n    ]\n])","d338ad22":"te.numel()","6554b2c8":"torch.arange(10).numel()","43f67e36":"torch.randn((10,101,8)).numel()","17779f77":"!pip install jovian --upgrade --quiet","e4526dc1":"import jovian","381f273b":"jovian.commit()","9e316ade":"Using register_hook() while doing Gradient Descent we can include the calculation of learning rate in the calcualtion of Gradient \nfor Backpropagation","33e2c938":"# Function 4\n## **torch.Tensor.flatten(input, start_dim=0, end_dim=-1)**\n\nThis function will flatten(reduce the dimensions) a continous range of dimensions in a tensor.\nIt will return the modified tensor.","96978807":"Dot product is useful in many places but usually input would have dimension greater than 1. So this function can be of use but rarely. Although the numpy's dot product function can be used with any two matrix till they are compactible for the dot product to occur.","085fdfbd":"## Conclusion\n\nSo we started from creating basic tensors using the PyTorch library and saw 5 functions which are important and are useful in many situations. These are the ones that caught my eye while reading the documentation and so I have tried to find out how exactly they work and in which situations we can use them.","51bf332d":"So in the following example, in the same tensor we have restricted the indicies to 1 D therefore only the row indicies of the non-zero elements are present in indicies.\n\nThe values will have the original column order i.e. the whole row is present in the values tensor where there is a non-zero element present. ","8280d05e":"**torch.arange(start,end,step)**\n\njust like the arange() function in numpy, is used to create tensor with values which starts from \"start\"(default being 0) and ends at \"end-1\" with the default step being 1.","bfdde923":"# Intoduction To PyTorch \n\n### Understanding torch.Tensor\n\nPyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab.\n\nIn this  notebook we will better understand one of the basic class of PyTorch, Tensor.\n\nThis Tensor class is present in the Module torch.\n\nSo in the next line we import the required Module which is torch.\n\nIn this notebook we will first briefly discuss various ways to create Tensors then we will disscus these 5 Interesting Funstions associated with the Class torch.Tensor.\n","aa648201":"So here also we can see there is not uniformity but li3 can be executed.","58e78d02":"numel() is a basic function which will be used in various places especially in for in loops.","7999f11f":"In the following example we see that we have a sparse matrix let's understand the attributes inside it\n\nInside the outer most tensor we have two tensors, indicies and values, alongwith size of the input and the number of non-zero \nelements in the input  tensor.\n\nSo Indicies Tensor has always 2 rows, unless mentioned otherwise in \"sparseDims\".\n\nThe first row has the row indicies of non-zero elements.\n\nTHe second row has the column indicies of non-zero elements","28aab5ae":"## Reference Links\n* Official documentation for `torch.Tensor`: https:\/\/pytorch.org\/docs\/stable\/tensors.html","644b5055":"We can have Problems with flatten function when start_dim comes after end_dim\n\nAs follows:","3dbbc488":"In the following example we can give maximum value of sparse_dim=3 i.e. the maximum dimension of the input","5b9067da":"**torch.logspace(start,end,steps=100,base=10.0)**\n\nReturns 1D tensor of \"steps\" number of values logarithmically spaced numbers with Base \"base\"  between $\\text{base}^\\text{start} $ and $\\text{base}^\\text{end} $","79ccc2d3":"### **torch.Tensor**\n\nTensor class in PyTorch is a Multidimensional matrix containing Homogenous elements. These are the building blocks of any program we will write using PyTorch.\n\nSo let's create Objects(Tensor) of this class then we will see the 5 Interesting Functions of the Class.","76e54b0c":"**torch.randn(num)** \n\nIt will return a tensors with \"num\" number of random values from a normal distribution(with mean=0 and variance=1 )\n\nAs we can see below we can also create N-Dimensional Tensor as well.","0989428a":"# Function 3\n## **torch.Tensor.dot(input, tensor)**\n\nComputes the dot product of two 1D tensors only.","080c946c":"We deal with sparse matrix in many areas of Machine Learning and Deep Learning a few are listed below:\n    \n    1) In OneHotEncoding\n    2) In data for BagOfWord model of a input Document.\n    3) Tf-Idf vectorizer    \n    4) some datasets while dealing with Recommender System. ","16455488":"Function will break if the arguements have out of range values","e269907e":"using torch.Tensor()  which is a constructor of the class.\nInside the constructor we can pass any list, tuple, numpy array to crearte a Tensor with the Same Data.","df2f185d":"**torch.linspace(start, end, steps=100)** \n\nReturns 1D tensor of length \"steps\" equally spaced points between \"start\" and \"end\".","8a1d3642":"The above tensor is a 3D tensor which has 3 dimensions 0,1,2 .\n\nAlso can be represented as -3 for 0, -2 for 1, -1 for 2\n\nstart_dim is the first dimension to flatten\n\nend_dim is the last dimension to flatten\n\nBoth these indicate the range of dimensions which will be left flattened by the function any other dimension will be remain untouched.\n\nOnly consecutive dimensions can be flattened.","6bdf66f2":"# Function 2\n## **torch.Tensor.to_sparse(sparseDims)**\n\nReturns a sparse copy of the tensor. We can optionally specify number of sparse dimensions using the \"sparseDims\" keyword.","eca9c2ef":"But here the Dimensions must be uniform across the Tensor. Otherwise we cannot build a Tensor.","2e602bf1":"There are some other functions that are also used to create tensors some of them are as follows:","92aa1ccd":"So sparse_dim cannot be greater than the dimension of the input tensor ","5bf1e259":"Flatten function is useful during processing of images with the help of Neural Networks (No CNN as CNN can take input as the whole image 2D\/3D tensor but Neural Network cannot take a 2D\/3D tensor but only 1D tensor)","34994a9a":"# Function 1\n## **torch.Tensor.register_hook(self, hook)**\n\nWhen this function is executed we will call the registered hook whenever a gradient with respect to the Tensor is computed.\n\nSo say we have declared a tensor on which we can call backward() i.e. require_grad is set to True but say we want to apply a function on the gradient and store the result of that function in the the \".grad\" variable then this function is used.\n\nAlso the function gives a handler which can be usedd to remove the hook by calling \n\ntorch.Tensor.register_hook(self, hook).remove()","e50e0fdc":"In the above example we see that the function\/Lambda function must not return only one value from the \"grad\" attribute of the \ntensor. Rather return the whole \"grad\" attribute or return nothing as shown in the above examples. ","5631ffaf":"## Let's look at some of the interesting Functions \n    1) torch.Tensor.register_hook(hook)\n    2) torch.Tensor.to_sparse(sparseDims)\n    3) torch.Tensor.dot(input, tensor)\n    4) torch.Tensor.flatten(input, start_dim=0, end_dim=-1) \n    5) torch.Tensor.numel(input)","f68a552e":"# Function 5\n## **torch.Tensor.numel(input)**\n\nReturns the total number of elements in the input tensor.","d5fa4996":"If we provide any tensors with dimension higher than 1 then the function will not accept.","1010a46f":"To confirm that indeed tensor created by **torch.tensor()** is the same as the tensors created by above functions:"}}