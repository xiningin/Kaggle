{"cell_type":{"cf515c34":"code","06cb5acb":"code","7c9b090a":"code","dbfb2bb4":"code","70b9d30f":"code","e90e8dce":"code","109cd60c":"code","d00a20a3":"code","5bb5bfd1":"code","e7e0bac2":"code","76dd3bee":"code","9d58604c":"code","2ae3d75c":"code","4cb47c5a":"code","46312818":"code","5dc832e6":"code","dd40b9c5":"code","8740efad":"code","4a283737":"code","8d9549e1":"code","b078b802":"code","1a683771":"code","bd1a298a":"code","8b183119":"code","0226fef3":"code","44159ce4":"code","972a4964":"code","3a90d48e":"code","c779fc34":"code","d16587a9":"code","69d4c31d":"code","c2f5458e":"code","16cb4cfe":"code","56cfc4bc":"code","a2440e42":"code","2d478691":"code","8edd2d5c":"code","0f2036e3":"code","22222293":"code","4bf9eacc":"code","d03e0192":"code","1fdcec61":"code","67be56e5":"code","f84256ee":"code","257675a5":"code","dccb29a3":"code","66814ef2":"code","8fc2d10b":"code","72eb90f7":"code","ad4142d8":"code","972c6b00":"code","d9a3b98a":"code","b2f10250":"code","226ffabd":"code","049d61df":"code","abed585c":"code","a7de4300":"code","48578b20":"code","48ed054b":"code","575e6985":"code","1edcb38c":"code","6c9fcd74":"code","f625d828":"code","09233820":"code","40445c11":"code","0f7b1e96":"code","8835e892":"code","14a734a4":"code","7bd9c436":"code","865d8325":"code","e282556a":"code","254cabe5":"code","b2d3d1a4":"code","926ed805":"code","f6018516":"code","bb547d38":"code","f90ec821":"code","09df0d50":"code","7a80e40e":"code","0df4d6d4":"code","d4595640":"code","2681d63e":"code","f7718d59":"code","f94fa6ea":"code","fb60dd66":"code","c514075a":"code","e5d44b06":"code","c43ef8b1":"code","abf814e5":"code","16d97392":"code","b3fb7f6d":"code","04cd9ccf":"code","bbd8d7a7":"code","0af01a0a":"code","ba30c0ec":"code","af067807":"code","1b05c6c6":"code","bf3508fa":"code","8954a1ea":"code","85f40464":"code","b64a2dfd":"code","ea2c4d71":"code","0f698c60":"code","ed4ca031":"markdown","133374c5":"markdown","349758b2":"markdown","6baca60a":"markdown","2b85e9f3":"markdown","90166344":"markdown","ef8c6035":"markdown","11857107":"markdown","03300177":"markdown","d2a2c7b9":"markdown","872605c1":"markdown","7336acc3":"markdown","83d05624":"markdown","da31c33f":"markdown","2e0d8441":"markdown","aead3e71":"markdown","f4b30428":"markdown","bea18441":"markdown","ff189733":"markdown","247b967c":"markdown","4c684721":"markdown","aa9aba9d":"markdown","363d1b0b":"markdown","af1e186c":"markdown","f47cdd32":"markdown","9ed92dff":"markdown","813b1e40":"markdown","29f236b2":"markdown","f3511279":"markdown","e9f6b306":"markdown","b8a89105":"markdown"},"source":{"cf515c34":"import numpy as np  \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","06cb5acb":"! pip install contractions","7c9b090a":"import re\nimport nltk\nimport gensim\nimport contractions\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom collections import Counter\nfrom random import choice\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Embedding, Activation, Dropout\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import LSTM\n\n\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","dbfb2bb4":"data_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndata_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ndata_subm = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","70b9d30f":"data_train.head(5)","e90e8dce":"data_test.head(5)","109cd60c":"print(f' :: Train datasets :: \\nNumber of Rows : {data_train.shape[0]}\\nNumber of Columns : {data_train.shape[1]}')\nprint(f' :: Test datasets  :: \\nNumber of Rows : {data_test.shape[0]}\\nNumber of Columns : {data_test.shape[1]}')","d00a20a3":"data_train.isnull().sum()","5bb5bfd1":"from missingno import matrix,bar, heatmap\nbar(data_train, figsize=(15,6), sort='descending',color='lightgreen')","e7e0bac2":"ax = sns.countplot(data=data_train, x='target')\nax.set_title('Count of target class')\nfor p in ax.patches:\n    ax.annotate(str(int(p.get_height())), (p.get_x()+0.3, p.get_height()*1.006))\n","76dd3bee":"fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(20,6))\nsns.histplot(data_train['text'].apply(lambda x: len(x)), kde=True, ax=ax1, color='r').set_title('Length of characters in text', fontsize=18, color='r')\nsns.histplot(data_train[data_train['target']==1]['text'].apply(lambda x: len(x)), kde=True, ax=ax2).set_title('Length of characters in Disaster text', color='b', fontsize=18)\nsns.histplot(data_train[data_train['target']==0]['text'].apply(lambda x: len(x)), kde=True, ax=ax3).set_title('Length of characters in Non Disaster text', color='b', fontsize=18)","9d58604c":"fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(20,6))\nsns.histplot(data_train['text'].apply(lambda x: len(x.split())), kde=True, ax=ax1, color='r').set_title('Count of words in text', fontsize=18, color='r')\nsns.histplot(data_train[data_train['target']==1]['text'].apply(lambda x: len(x.split())), kde=True, ax=ax2).set_title('Count of words in text in Disaster text', color='b', fontsize=18)\nsns.histplot(data_train[data_train['target']==0]['text'].apply(lambda x: len(x.split())), kde=True, ax=ax3).set_title('Count of words in text in Non Disaster text', color='b', fontsize=18)","2ae3d75c":"def plot_wordcloud(text, bg_color='salmon', cmap='rainbow'):\n    c = choice(['Paired','Set2','husl','Spectral','coolwarm'])\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(25,10))\n    wordcloud = WordCloud(width=3000, height=2000, background_color=bg_color, colormap=cmap,\n                     collocations=False, stopwords=STOPWORDS, random_state=51).generate(' '.join(text))\n    ax1.imshow(wordcloud)\n    ax1.axis('off')\n    labels = pd.Series(data=text).value_counts().index[:20]\n    data = pd.Series(data=text).value_counts()[:20]\n    sns.barplot(y=labels, x=data, ax=ax2, palette=c)","4cb47c5a":"def stemming(tokens):\n    porter_stemmer = PorterStemmer()\n    steammed_keywords = []\n    for token in tokens:\n        steammed_keywords.append(porter_stemmer.stem(token))\n    plot_wordcloud(steammed_keywords)","46312818":"def lemmatization(tokens, plot=1):\n    lemtz = WordNetLemmatizer()\n    lemmatize_keywords = []\n    for token in tokens:\n        lemmatize_keywords.append(lemtz.lemmatize(token, wordnet.VERB))\n    if plot == 1:\n        plot_wordcloud(lemmatize_keywords)\n    else:\n        return ' '.join(lemmatize_keywords)","5dc832e6":"key_data = data_train['keyword'].fillna('blank').apply(lambda x:re.sub('[^a-zA-Z]+','_', x))\nkeywords_list = []\nfor keyword in key_data:\n    if keyword != 'blank':\n        keywords_list.extend(keyword.split())","dd40b9c5":"lemmatization(keywords_list)","8740efad":"key_data = data_train[data_train['target'] == 0]['keyword'].fillna('blank').apply(lambda x:re.sub('[^a-zA-Z]+','_', x))\nkeywords_list = []\nfor keyword in key_data:\n    if keyword != 'blank':\n        keywords_list.extend(keyword.split())","4a283737":"lemmatization(keywords_list)","8d9549e1":"key_data = data_train[data_train['target'] == 1]['keyword'].fillna('blank').apply(lambda x:re.sub('[^a-zA-Z]+','_', x))\nkeywords_list = []\nfor keyword in key_data:\n    if keyword != 'blank':\n        keywords_list.extend(keyword.split())","b078b802":"lemmatization(keywords_list)","1a683771":"plt.figure(figsize=(20,6))\nlabels = data_train['location'].value_counts().index[:20]\ndata = data_train['location'].value_counts()[:20]\nax = sns.barplot(x=labels, y = data)\nfor p in ax.patches:\n    ax.annotate(str(int(p.get_height())), (p.get_x()+0.2, p.get_height()+0.5))\nax.set_title('Top 20 Country who tweets', fontsize=20)\nax.set_xticklabels(labels=labels, rotation=90);","bd1a298a":"plt.figure(figsize=(20,6))\nlabels = data_train[data_train['target']==1]['location'].value_counts().index[:20]\ndata = data_train[data_train['target']==1]['location'].value_counts()[:20]\nax = sns.barplot(x=labels, y = data)\nfor p in ax.patches:\n    ax.annotate(str(int(p.get_height())), (p.get_x()+0.2, p.get_height()+0.5))\nax.set_title('Top 20 Country who tweets Disaster', fontsize=20)\nax.set_xticklabels(labels=labels, rotation=90);","8b183119":"plt.figure(figsize=(20,6))\ndata = data_train[data_train['target']==0]['location'].value_counts()[:20]\nlabels = data_train[data_train['target']==0]['location'].value_counts().index[:20]\nax = sns.barplot(x=labels, y = data)\nfor p in ax.patches:\n    ax.annotate(str(int(p.get_height())), (p.get_x()+0.2, p.get_height()+0.5))\nax.set_title('Top 20 Country who tweets Non Disaster', fontsize=20)\nax.set_xticklabels(labels=labels, rotation=90);","0226fef3":"# pd.set_option('display.max_cotext_cleaningh', 200)\n# pd.set_option('display.max_rows', 500)\n# pd.reset_option('display.max_rows')\n# pd.reset_option('display.max_colwidth')","44159ce4":"df_train = data_train[['id','text','target']]\ndf_test = data_test[['id', 'text']]","972a4964":"def top_20(data, title='', axes=None):\n    labels = pd.Series(data=data).value_counts().index[:20]\n    data = pd.Series(data=data).value_counts()[:20]\n    ax = sns.barplot(x=labels, y=data, ax=axes)\n    for p in ax.patches:\n        ax.annotate(str(int(p.get_height())), (p.get_x()+0.2, p.get_height()+0.5))\n    ax.set_title(label='Top 20 '+ title, fontsize=20)\n    ax.set_xticklabels(labels=labels, rotation=90)","3a90d48e":"fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(18, 15))\n\n# person tagged\nhandles_tags = []\nfor tweet in df_train[df_train['target'] == 0]['text']:\n    handles_tags.extend(re.findall('@[a-zA-Z0-9_]{3,}',tweet))\n\ntop_20(handles_tags,'Twitter handler tagged - Non Disaster', ax1)\n\n#hashtag\nhash_tags = []\nfor tweet in df_train[df_train['target'] == 0]['text']:\n    hash_tags.extend([text.lower() for text in re.findall('#[a-zA-Z0-9]+',tweet)])\n\ntop_20(hash_tags, 'Hash tag used - Non Disaster', ax2)\n\nplt.tight_layout()","c779fc34":"fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(18, 15))\n\n# person tagged\nhandles_tags = []\nfor tweet in df_train[df_train['target'] == 1]['text']:\n    handles_tags.extend(re.findall('@[a-zA-Z0-9_]{3,}',tweet))\n\ntop_20(handles_tags,'Twitter handler tagged - Disaster', ax1)\n\n#hash tag\nhash_tags = []\nfor tweet in df_train[df_train['target'] == 1]['text']:\n    hash_tags.extend([text.lower() for text in re.findall('#[a-zA-Z0-9]+',tweet)])\n\ntop_20(hash_tags, 'Hash tag used - Disaster', ax2)\n\nplt.tight_layout()","d16587a9":"def text_cleaning(text):\n    text = text.lower()\n    text = re.sub(r'((https)|(http)):[a-zA-Z.\/0-9]+',' ',text) # removing links \n    text = re.sub('@[a-zA-Z0-9_]{3,}',' ',text) # removing tagged person\n    text = re.sub('#[a-zA-Z0-9]+',' ',text) # removing the hashtag\n    text = re.sub('<.*>', ' ', text) # removing the html tags\n    text = ' '.join([contractions.fix(t) for t in text.split()])\n    text = re.sub('[^a-zA-Z\\s]+',' ',text) # removing the punctuation\n    text = lemmatization(text.split(), 0) # normalize the text\n    text = re.sub('[a-z]*(amp)[a-z]*', ' ', text) # most repeating words\n    text = text.replace('via','')\n    text = ' '.join([t if len(t) > 2 else '' for t in text.split()]) # removing word of length 2 and less\n    text = re.sub('[ ]{2,}',' ',text) # removing the extra white space\n    text = ' '.join([t for t in text.split() if t not in STOPWORDS]) # removing the stopwords\n    return text.strip()","69d4c31d":"pd.options.display.max_colwidth = 200","c2f5458e":"pd.concat([df_train['text'], df_train['text'].apply(text_cleaning)], axis=1)","16cb4cfe":"df_train['Cleaned_Text'] = df_train['text'].apply(text_cleaning)\ndf_test['Cleaned_Text'] = df_test['text'].apply(text_cleaning)","56cfc4bc":"df_train = df_train[['id','text','Cleaned_Text', 'target']]\ndf_train.head()","a2440e42":"df_test = df_test[['id','text','Cleaned_Text']]\ndf_test.head()","2d478691":"disaster_text = []\nfor i in df_train[df_train['target'] == 1]['Cleaned_Text']:\n    disaster_text.extend(i.split())\n    \nplot_wordcloud(disaster_text)","8edd2d5c":"ndisaster_text = []\nfor i in df_train[df_train['target'] == 0]['Cleaned_Text']:\n    ndisaster_text.extend(i.split())\n\nplot_wordcloud(ndisaster_text)","0f2036e3":"cv = CountVectorizer(max_features=10000)\nX = cv.fit_transform(df_train['Cleaned_Text']).toarray()","22222293":"X.shape","4bf9eacc":"X_train, X_test, y_train, y_test = train_test_split(X, df_train['target'], test_size=0.2, random_state=41)","d03e0192":"logR = LogisticRegression()\nlogR.fit(X_train, y_train)","1fdcec61":"logRes = logR.predict(X_test)\nprint(f\"Logistic Regression Accuracy : {accuracy_score(y_test, logRes)*100:.2f}%\")","67be56e5":"print(classification_report(y_test, logRes))","f84256ee":"tfidf = TfidfVectorizer()\nX = tfidf.fit_transform(df_train['Cleaned_Text']).toarray()","257675a5":"X.shape","dccb29a3":"X_train, X_test, y_train, y_test = train_test_split(X, df_train['target'], test_size=0.2, random_state=41)","66814ef2":"logRT = LogisticRegression()\nlogRT.fit(X_train, y_train)","8fc2d10b":"logRes = logRT.predict(X_test)\nprint(f\"Logistic Regression Accuracy : {accuracy_score(y_test, logRes)*100:.2f}%\")","72eb90f7":"print(classification_report(y_test, logRes))","ad4142d8":"multi = MultinomialNB()\nmulti.fit(X_train, y_train)","972c6b00":"predM = multi.predict(X_test)","d9a3b98a":"print(f\"Multinomial Navie bayes Accuracy : {accuracy_score(predM, y_test)*100:.2f}%\")","b2f10250":"print(classification_report(predM, y_test))","226ffabd":"randF = RandomForestClassifier()\nrandF.fit(X_train, y_train)","049d61df":"predRF = randF.predict(X_test)\nprint(f\"Random Forest Accuracy : {accuracy_score(predRF, y_test)*100:.2f}%\")","abed585c":"print(classification_report(predRF, y_test))","a7de4300":"xgb = XGBClassifier()\nxgb.fit(X_train, y_train)","48578b20":"Predxgb = xgb.predict(X_test)\nprint(f\"Xgb Accuracy : {accuracy_score(Predxgb, y_test)*100:.2f}%\")","48ed054b":"print(classification_report(Predxgb, y_test))","575e6985":"text = df_train['Cleaned_Text'].tolist()\ny = df_train['target']","1edcb38c":"print(text[:20])","6c9fcd74":"token = Tokenizer()\ntoken.fit_on_texts(text)","f625d828":"vocab_size = len(token.word_index)+1\nvocab_size","09233820":"encoded_text = token.texts_to_sequences(text)\nprint(encoded_text[:5])","40445c11":"plt.hist([len(t) for t in encoded_text]);","0f7b1e96":"#to make the sequences of equal length\nmax_length = 20\nX = pad_sequences(encoded_text, maxlen=max_length, padding='post')","8835e892":"X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=0.2, stratify=y)","14a734a4":"vec_size = 300\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, vec_size, input_length = max_length))\n\nmodel.add(Conv1D(64, 8, activation= 'relu'))\nmodel.add(MaxPooling1D(2))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(GlobalMaxPooling1D())\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nadam = Adam(learning_rate=0.0001)\nmodel.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","7bd9c436":"hist = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))","865d8325":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20,5))\n\nax1.plot(hist.history['accuracy'])\nax1.plot(hist.history['val_accuracy'])\nax1.legend(['Training Accuracy','Validation Accuracy'])\nax1.set_title('Accuracy')\n\nax2.plot(hist.history['loss'])\nax2.plot(hist.history['val_loss'])\nax2.legend(['Training Loss','Validation Loss'])\nax2.set_title('Losses')","e282556a":"X = [text.split() for text in df_train['Cleaned_Text'].tolist()]\ny = df_train['target']","254cabe5":"dim = 20\nword2Vector = gensim.models.Word2Vec(sentences=X, vector_size=dim, window=15, min_count=1)","b2d3d1a4":"# vocab_size = len(word2Vector.wv.key_to_index)+1","926ed805":"word2Vector.wv['usa']","f6018516":"word2Vector.wv.most_similar('news')","bb547d38":"token = Tokenizer()\ntoken.fit_on_texts(X)\n\nX = token.texts_to_sequences(X)\nprint(X[:5])","f90ec821":"token.word_index","09df0d50":"maxlen = 20\nX = pad_sequences(X, maxlen=maxlen, padding='post')\nprint(X)","7a80e40e":"vocab_size = len(token.word_index)+1","0df4d6d4":"def get_weight_matrix(model, vocab):\n    weight_vectors = np.zeros((vocab_size, dim))\n    \n    for word, index in vocab.items():\n        weight_vectors[index] = model.wv[word]\n    return weight_vectors","d4595640":"embedding_vectors = get_weight_matrix(word2Vector, vocab = token.word_index)\nprint(embedding_vectors.shape)","2681d63e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","f7718d59":"model = Sequential()\nmodel.add(Embedding(vocab_size, output_dim = dim, weights = [embedding_vectors], input_length = maxlen, trainable=False))\nmodel.add(LSTM(units=128))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(optimizer='adam',loss='binary_crossentropy', metrics=['acc'])\nmodel.summary()","f94fa6ea":"hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)","fb60dd66":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20,5))\n\nax1.plot(hist.history['acc'])\nax1.plot(hist.history['val_acc'])\nax1.legend(['Training Accuracy','Validation Accuracy'])\nax1.set_title('Accuracy')\n\nax2.plot(hist.history['loss'])\nax2.plot(hist.history['val_loss'])\nax2.legend(['Training Loss','Validation Loss'])\nax2.set_title('Losses')","c514075a":"text = df_train['Cleaned_Text'].tolist()\ny = df_train['target'].values","e5d44b06":"token = Tokenizer()\ntoken.fit_on_texts(text)","c43ef8b1":"vocab_size = len(token.word_index)+1\nprint(vocab_size)","abf814e5":"encoded_text = token.texts_to_sequences(text)\nprint(encoded_text[:3])","16d97392":"max_len = 20\nX = pad_sequences(encoded_text, maxlen=max_len, padding='post')\nprint(X)","b3fb7f6d":"%%time\nglove_vectors = dict()\n\nwith open('..\/input\/glovetwitter27b100dtxt\/glove.twitter.27B.200d.txt') as file:\n    for line in file:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:])\n        glove_vectors[word] = vectors","04cd9ccf":"len(glove_vectors.keys())","bbd8d7a7":"glove_vectors['happy'].shape","0af01a0a":"word_vector_matrix = np.zeros((vocab_size, 200))\n\nmispelled_vocab = []\n\nfor word, index in token.word_index.items():\n    vector = glove_vectors.get(word, None)\n    if vector is not None:\n        word_vector_matrix[index] = vector\n    else:\n        mispelled_vocab.append(word)","ba30c0ec":"print(mispelled_vocab,'\\n\\n*** Number of Total Mispelled Words** :: ' ,len(mispelled_vocab))","af067807":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)","1b05c6c6":"vec_size = 200\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, vec_size, input_length = max_len, weights = [word_vector_matrix], trainable = False))\n\nmodel.add(Conv1D(64, 8, activation= 'relu'))\nmodel.add(MaxPooling1D(2))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(16, activation='relu'))\n\nmodel.add(GlobalMaxPooling1D())\n\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","bf3508fa":"hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)","8954a1ea":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20,5))\n\nax1.plot(hist.history['accuracy'])\nax1.plot(hist.history['val_accuracy'])\nax1.legend(['Training Accuracy','Validation Accuracy'])\nax1.set_title('Accuracy')\n\nax2.plot(hist.history['loss'])\nax2.plot(hist.history['val_loss'])\nax2.legend(['Training Loss','Validation Loss'])\nax2.set_title('Losses')","85f40464":"def encode_text(text):\n    token = Tokenizer()\n    token.fit_on_texts(text)\n    text = token.texts_to_sequences(text)\n    text = pad_sequences(text, maxlen=max_len, padding='post')\n    return text","b64a2dfd":"data_subm.head()","ea2c4d71":"data_subm['target'] = model.predict_classes(encode_text(df_test['Cleaned_Text'].tolist()))\ndata_subm.to_csv('Submission.csv',index=False)","0f698c60":"data_subm.head()","ed4ca031":"### Words Embedding (Glove Vectors)","133374c5":"## Keywords and Location EDA","349758b2":"### Multinomial Navie Bayes","6baca60a":"#### Disaster","2b85e9f3":"### Function for lemmatization","90166344":"#### WordCloud and Top20 word used in Keywords after Text Cleaning - Non Disaster","ef8c6035":"### Model Building","11857107":"## Text EDA","03300177":"#### Non Disaster","d2a2c7b9":"### Words Embedding (Word2Vec)","872605c1":"## Word Embedding","7336acc3":"### Text Cleaning","83d05624":"#### Non Disaster","da31c33f":"## Location","2e0d8441":"### Logistic Regression","aead3e71":"### Tf-IDF(Term frequency and inverse document frequency)","f4b30428":"## Loading the data","bea18441":"### Keywords","ff189733":"#### Disaster","247b967c":"### Non Disaster","4c684721":"#### WordCloud and Top20 word used in Keywords after Text Cleaning - Disaster","aa9aba9d":"### Xg boost","363d1b0b":"### Random Forest","af1e186c":"### Function to plot Wordcloud","f47cdd32":"## Importing Relevant Libraries","9ed92dff":"### WordCloud and Top20 word used in Keywords for Real Disaster","813b1e40":"## Exploratory Data Analysis","29f236b2":"## Bag of Words (Term Frequency)","f3511279":"### Function for stemming","e9f6b306":"### Disaster","b8a89105":"### WordCloud and Top20 word used in Keywords"}}