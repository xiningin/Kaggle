{"cell_type":{"c46a29dc":"code","97dfac32":"code","6bde8526":"code","666f7a6b":"code","9f69601e":"code","dd97a209":"code","cf2f7788":"code","ede5e369":"code","5ff7d572":"code","7bb7e56e":"code","d17cc302":"markdown","3d5dba5d":"markdown","0ec16ba7":"markdown","fc221a0b":"markdown","da877fa9":"markdown","642a3843":"markdown","1e7e1252":"markdown"},"source":{"c46a29dc":"### author P Jacquot\n\nimport numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","97dfac32":"class RBM:\n    def __init__(self, n_visible, n_hidden):\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        self.v = np.zeros(n_visible) ## visible units activation\n        self.h = np.zeros(n_hidden) ## hidden units activation\n        # # RBM weights & directly associated quantities\n        self.w = None\n        self.v_bias = None\n        self.h_bias = None\n    \n    def random_init_weights(self): ## weights randomly chosen between [-0.5, 0.5]\n        self.v_bias = np.random.random(self.n_visible)-0.5\n        self.h_bias = np.random.random(self.n_hidden) -0.5\n        self.w = np.random.random((self.n_visible, self.n_hidden)) -0.5\n        \n    def eval_energy(self):\n        neg_en =  np.dot(self.v, self.v_bias) + np.dot(self.h, self.h_bias) + self.v.T @ self.w @ self.h\n        return -neg_en\n        ","6bde8526":"rbm_ex = RBM(3,4)\nrbm_ex.random_init_weights()\nrbm_ex.v = np.random.randint(0,2,rbm_ex.n_visible)\nrbm_ex.h = np.random.randint(0,2,rbm_ex.n_hidden)\nprint(\"state : visible: \", rbm_ex.v, \"\\nhidden: \", rbm_ex.h)\nprint(\"\\nenergy: \", rbm_ex.eval_energy())","666f7a6b":"def sigmoid(x):\n    return 1.0\/(1+np.exp(-x))\n\ndef random_activ_from_proba(proba_to_be_one):\n    draw_uni = np.random.uniform() ##uniform draw in [0,1]\n    return 1 if (draw_uni < proba_to_be_one) else 0\n\ndef get_hidden_vec_from_visibles(rbm):\n    probas_h_one = sigmoid( rbm.h_bias + rbm.v.T @ rbm.w)\n    return np.array([random_activ_from_proba(pr) for pr in probas_h_one])\n\ndef get_visible_vec_from_hiddens(rbm):\n    probas_v_one = sigmoid( rbm.v_bias + rbm.w @ rbm.h)\n    return np.array([random_activ_from_proba(pr) for pr in probas_v_one])\n    \nget_hidden_vec_from_visibles(rbm_ex) ## an example","9f69601e":"def gibbs_sampling(rbm, nb_samples=1, nb_its_within_samples=1000, nb_its_presample=1000, print_every_nb_sample = 100):\n    it_idx= -nb_its_presample ##starting iterations before sampling\n    samples_v = []\n    samples_h = []\n    while len(samples_v) < nb_samples:\n        rbm.v = get_visible_vec_from_hiddens(rbm)\n        rbm.h = get_hidden_vec_from_visibles(rbm)\n        if it_idx>0 and it_idx % nb_its_within_samples ==0:\n            samples_v.append(np.copy(rbm.v))\n            samples_h.append(np.copy(rbm.h))\n            if len(samples_v)%  print_every_nb_sample ==0:\n                print(\"iteration \", it_idx, \", write new samples.\")\n        it_idx += 1\n    return samples_v, samples_h\n\nsamples_v, samples_h = gibbs_sampling(rbm_ex, nb_samples=10)\nprint(\"samples_v =\", samples_v)\nprint(\"samples_h =\", samples_h)","dd97a209":"from matplotlib import pyplot as plt\nfrom collections import Counter\n\n#def get_num_occurences_dict(samples_v,samples_h):\nnb_samples = 1000\nsamples_v, samples_h = gibbs_sampling(rbm_ex, nb_samples=nb_samples, nb_its_within_samples= 100)","cf2f7788":"samples_states = [str(np.concatenate((samples_v[k], samples_h[k]))) for k in range(nb_samples)]\n\nnb_occurences_states = Counter(samples_states)\nprint(nb_occurences_states)\n\nplt.bar(nb_occurences_states.keys(), nb_occurences_states.values(), width=2, color='g')","ede5e369":"import itertools\n\ndef compute_partition_function(rbm):\n    Z=0\n    for tuple_state in itertools.product([0,1],repeat=rbm.n_visible + rbm.n_hidden):\n        v = np.array(tuple_state)[:rbm.n_visible]\n        h = np.array(tuple_state)[rbm.n_visible:]\n        Z+= np.exp(np.dot(v, rbm.v_bias) + np.dot(h, rbm.h_bias) + v.T @ rbm.w @ h)\n    return Z\nZ = compute_partition_function(rbm_ex)\nprint(\"Z =\", Z)","5ff7d572":"def compute_proba_h_1(rbm, idx_unit_h, Z):\n    proba = 0\n    for tuple_state in itertools.product([0,1],repeat=rbm.n_visible + rbm.n_hidden-1):\n        v = np.array(tuple_state)[:rbm.n_visible]\n        h = np.concatenate((tuple_state[rbm.n_visible:rbm.n_visible+idx_unit_h], (1.0,) , tuple_state[rbm.n_visible+idx_unit_h:]))\n        proba += np.exp(np.dot(v, rbm.v_bias) + np.dot(h, rbm.h_bias) + v.T @ rbm.w @ h)\n    return proba\/Z\n\ndef compute_proba_v_1(rbm, idx_unit_v, Z):\n#     print(\"idx_unit_v\", idx_unit_v)\n    proba = 0\n    for tuple_state in itertools.product([0,1],repeat=rbm.n_visible + rbm.n_hidden-1):\n        v = np.concatenate((tuple_state[:idx_unit_v], (1.0,) , tuple_state[idx_unit_v:rbm.n_visible-1]))\n        h = np.array(tuple_state)[rbm.n_visible-1:]\n        proba += np.exp(np.dot(v, rbm.v_bias) + np.dot(h, rbm.h_bias) + v.T @ rbm.w @ h)\n    return proba\/Z\n\nprint(\"proba v1:\" ,compute_proba_v_1(rbm_ex, 1, Z))\n\nproba_hv = lambda i, j: compute_proba_v_1(rbm_ex, i, Z)*compute_proba_h_1(rbm_ex,j,Z)\nmatrix_hv_pr_expectation = np.zeros((rbm_ex.n_visible, rbm_ex.n_hidden), dtype=float)\nfor i in range(rbm_ex.n_visible):\n    for j in range(rbm_ex.n_hidden):\n        matrix_hv_pr_expectation[i,j] = proba_hv(i,j)\nprint(\"\\nmatrix of expected products =\\n\", matrix_hv_pr_expectation)","7bb7e56e":"def eval_proba_from_samples(idx_unit_to_one, dict_occurences_states, num_samples):\n    proba = 0\n    for state_str in nb_occurences_states:\n        state = np.fromstring(state_str[1:-1], sep=' ', dtype=int) ## state was given as a string\n        if state[idx_unit_to_one] ==1:\n            proba += nb_occurences_states[state_str]\/num_samples\n    return proba\n\nprint(\"eval proba v1:\" ,eval_proba_from_samples(1, nb_occurences_states, nb_samples))\n\neval_proba_hv = lambda i, j: eval_proba_from_samples(i, nb_occurences_states, nb_samples)*eval_proba_from_samples(rbm_ex.n_visible+j, nb_occurences_states, nb_samples)\neval_matrix_hv_pr_expectation = np.zeros((rbm_ex.n_visible, rbm_ex.n_hidden), dtype=float)\nfor i in range(rbm_ex.n_visible):\n    for j in range(rbm_ex.n_hidden):\n        eval_matrix_hv_pr_expectation[i,j] = eval_proba_hv(i,j)\nprint(\"\\nevaluation matrix of expected products =\\n\", eval_matrix_hv_pr_expectation)\n\nprint(\"\\ndistance between actual matrix and evaluation:\\n\", np.linalg.norm(np.subtract(matrix_hv_pr_expectation, eval_matrix_hv_pr_expectation), ord=1))","d17cc302":"Let is now evaluate the same matrix with the sampled states from Gibbs sampling, and compare the values:","3d5dba5d":"Let us know consider a small example of an RBM, a random state, and print its energy:","0ec16ba7":"Let us first define the class for the restricted boltzmann machine (RBM), and define energy function:\n$$E(v,h) = b_v^T v + b_h^T h  + h^T W v \\ ,$$\nwhere $b_v$ and $b_h$ are the visible and hidden bias, and $W$ is the interaction (visible-hidden) weight matrix of the RBM","fc221a0b":"Let us now sample a large batch to make statistics","da877fa9":"Now we will write the Gibbs sampling to evaluate the Boltzmann distribution:\n$$ \\mathbb{P}(v,h) = \\frac{1}{Z}  e^{-\\beta E(v,h)} $$\nwhere $\\beta$ is a coefficient referred to as inverse temperature (we take $\\beta=1$), and $Z$ is the partition function.\n\nTo do this, we use of the conditional probability rules (which are for instance proved in Hinton's paper):\n$$  \\mathbb{P}(h_j=1| {v}) = \\sigma \\left( c_j + \\sum\\limits_{i=1}^n v_i w_{ij} \\right)  \\text{   and   }     \\mathbb{P}(v_i=1| {h}) = \\sigma \\left( b_i + \\sum\\limits_{j=1}^m w_{ij} h_j \\right) \\ ,$$\nwhere $\\sigma(.) x \\mapsto \\frac{1}{1+e^{-x}}$ is the sigmoid function.","642a3843":"The sampler enables to evaluate expectation quantities (where the expectation is taken with respect to the Boltzmann distribution) such as: $$\\langle h_i v_j \\rangle := \\sum_{h,v \\in \\{0,1\\} } \\mathbb{P}(h_i=h) \\mathbb{P}(v_j=v) h v = \\mathbb{P}(h_i=1)\\mathbb{P}(v_j=1)  . $$\n\nFor our example in dimension 7, we have $2^7 = 128$ possible states, so that we can still compute the associated distribution in bruteforce, and compare the above expectation with the one given by our Gibbs sampler.\n","1e7e1252":" Let us know write the Gibbs sampling loop. We loop a few times (presample) before sampling to let the Markov process converge to the target distribution."}}