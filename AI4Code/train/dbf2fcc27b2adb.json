{"cell_type":{"9f9879f6":"code","31574386":"code","50d64881":"code","b1e1520b":"code","4eab2e68":"code","b3ee1705":"code","4db0efe6":"code","a196e25e":"code","4583df13":"code","a4304160":"code","fa51afb1":"code","ae975875":"code","f0cf51c9":"code","a2103c93":"code","80a07b85":"markdown","38fdc5b1":"markdown","7a46db25":"markdown","27ac8cb0":"markdown","c22511a1":"markdown","9e8de67b":"markdown","792e0698":"markdown","3779d7fd":"markdown","e507fd71":"markdown","390bcfb0":"markdown","308b9b02":"markdown","db28ab5a":"markdown","4297e0ca":"markdown","d4eff670":"markdown","7a95b110":"markdown","03001d83":"markdown","589289cb":"markdown","88c2f9f9":"markdown"},"source":{"9f9879f6":"import pandas as pd\nimport numpy as np","31574386":"train = pd.read_csv('..\/input\/train.csv', header=0, sep=',', quotechar='\"', usecols=['target', 'comment_text'])\ntrain['target_bool'] = np.where(train['target']>=0.5, 1, 0)\ntrain[train['target_bool'] ==1].describe()","50d64881":"#select a subset of the train dataset accodirng to the portion (between 0 and 1)\nportion = 1\nindices = np.random.permutation(train.shape[0])\nexplore_idx = indices[:np.int(portion * train.shape[0])]\ntrain_explore = train.iloc[explore_idx]\nprint(\"New shape is : \" + str(train_explore.shape))","b1e1520b":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport datetime\nimport string","4eab2e68":"# train_explore_clean = train_explore\n# lemmatizer = nltk.WordNetLemmatizer()\n# stop = stopwords.words(\"english\")\n# tokenizer = nltk.TreebankWordTokenizer()\n\n# #cleaning\n# tic = datetime.datetime.now()\n# train_explore_clean['comment_text_clean'] = train_explore_clean['comment_text'].str.replace('[0-9]','') ### remove numbers\n# tac = datetime.datetime.now()\n# time = tac - tic\n# print(\"remove number time\" + str(time))\n# tic = datetime.datetime.now()\n# train_explore_clean['comment_text_clean']=train_explore_clean['comment_text_clean'].apply(lambda x : x.lower()) ### to lower case\n# tac = datetime.datetime.now()\n# time = tac - tic\n# print(\"To lower time\" + str(time))\n# tic = datetime.datetime.now()\n# train_explore_clean['comment_text_token'] = train_explore_clean['comment_text_clean'].apply(lambda x : tokenizer.tokenize(x)) ### tokenize\n# tac = datetime.datetime.now()\n# time = tac - tic\n# print(\"Tokenize time\" + str(time))\n# tic = datetime.datetime.now()\n# train_explore_clean['comment_text_token_no_stop'] = train_explore_clean['comment_text_token'].apply(lambda x: [item for item in x if item not in stop]) ### remove stopr words\n# tac = datetime.datetime.now()\n# time = tac - tic\n# print(\"Remove stopword time\" + str(time))\n# tic = datetime.datetime.now()\n# train_explore_clean['comment_text_token_no_stop'] = train_explore_clean['comment_text_token_no_stop'].apply(lambda x : \" \".join(x))\n# tac = datetime.datetime.now()\n# time = tac - tic\n# print(\"Join time\" + str(time))\n\n\n\n# train_explore_clean.head()","b3ee1705":"train_explore_clean = train_explore\nlemmatizer = nltk.WordNetLemmatizer()\nstops = stopwords.words(\"english\")\ntokenizer = nltk.TreebankWordTokenizer()\n\ndef remove_words_in_string(sentence, words):\n    sentence = sentence\n    remove_list = words\n    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n    sentence = sentence.translate(translator)\n    word_list = sentence.split()\n    return ' '.join([i for i in word_list if i not in remove_list])\n\n# cleaning\n# \ntic = datetime.datetime.now()\ntrain_explore_clean['comment_text_clean'] = train_explore_clean['comment_text'].str.replace('[0-9]','') ### remove numbers\ntac = datetime.datetime.now()\ntime = tac - tic\nprint(\"remove number time\" + str(time))\ntic = datetime.datetime.now()\ntrain_explore_clean['comment_text_clean']=train_explore_clean['comment_text_clean'].apply(lambda x : x.lower()) ### to lower case\ntac = datetime.datetime.now()\ntime = tac - tic\nprint(\"To lower time\" + str(time))\ntic = datetime.datetime.now()\ntrain_explore_clean['comment_no_stop']=train_explore_clean['comment_text_clean'].apply(lambda sentence : remove_words_in_string(sentence, stops)) ### to lower case\ntac = datetime.datetime.now()\ntime = tac - tic\nprint(\"remove stop words time : \" + str(time))\n","4db0efe6":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n# all parameters not specified are set to their defaults\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.metrics import average_precision_score, roc_curve, roc_auc_score, classification_report\nfrom matplotlib import pyplot\nfrom sklearn.naive_bayes import MultinomialNB","a196e25e":"#parameters\nmax_df = 0.3\nmin_df = 0.003 \ntoxicity_threshold = 0.2\nn_gram = (1, 3)\n\n#processing\ntexts = train_explore_clean[train_explore_clean['target'] >toxicity_threshold ]['comment_no_stop']\ntfidf = TfidfVectorizer(min_df=np.int(min_df * texts.shape[0]), max_df=max_df, ngram_range=n_gram)\nfeatures = tfidf.fit_transform(texts)\nvectorizer = pd.DataFrame(\n    features.todense(),\n    columns=tfidf.get_feature_names()\n)\nprint( \"column names are \" + str(vectorizer.columns.values), \" Number of column is \" + str (vectorizer.columns.shape))","4583df13":"# df = pd.DataFrame(train_explore_clean[train_explore_clean['comment_text_token_no_stop'].str.contains(u'could care less')][['target','comment_text']])\nX = tfidf.transform(train_explore_clean['comment_no_stop'])\ny=  train_explore_clean['target_bool']\nxTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.05, random_state =42)","a4304160":"logisticRegr = LogisticRegression(C=2)\nlogisticRegr.fit(xTrain, yTrain)\npredictions = logisticRegr.predict_proba(xTest)\npredictions = pd.DataFrame(predictions)\n# cm = metrics.confusion_matrix(yTest, predictions)\n# print(cm)\n# average_precision = average_precision_score(yTest, predictions)\n# print('Average precision-recall score: {0:0.2f}'.format(\n#       average_precision))\nfpr, tpr, thresholds = roc_curve(yTest, predictions[1])\nauc = roc_auc_score(yTest, predictions[1])\nprint('AUC: %.3f' % auc)\n# plot no skill\npyplot.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\npyplot.plot(fpr, tpr, marker='.')\n# show the plot\npyplot.show()\nclassification_report(yTest, predictions[1]>0.5)","fa51afb1":"Naive = MultinomialNB()\nNaive.fit(xTrain,yTrain)\n\n#Predict the response for test dataset\npredictions = Naive.predict_proba(xTest)\npredictions = pd.DataFrame(predictions)\n\n\n# cm = metrics.confusion_matrix(yTest, predictions)\n# print(cm)\n# average_precision = average_precision_score(yTest, predictions)\n# print('Average precision-recall score: {0:0.2f}'.format(\n#       average_precision))\nfpr, tpr, thresholds = roc_curve(yTest, predictions[1])\nauc = roc_auc_score(yTest, predictions[1])\nprint('AUC: %.3f' % auc)\n# plot no skill\npyplot.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\npyplot.plot(fpr, tpr, marker='.')\n# show the plot\npyplot.show()\nclassification_report(yTest, predictions[1]>0.5)","ae975875":"test = pd.read_csv('..\/input\/test.csv', header=0, sep=',', quotechar='\"')\nlemmatizer = nltk.WordNetLemmatizer()\nstop = stopwords.words(\"english\")\ntokenizer = nltk.TreebankWordTokenizer()\n\n#cleaning\ntic = datetime.datetime.now()\ntest['comment_text_clean'] = test['comment_text'].str.replace('[0-9]','') ### remove numbers\ntac = datetime.datetime.now()\ntime = tac - tic\nprint(\"remove number time\" + str(time))\ntic = datetime.datetime.now()\ntest['comment_text_clean']=test['comment_text_clean'].apply(lambda x : x.lower()) ### to lower case\ntac = datetime.datetime.now()\ntime = tac - tic\nprint(\"To lower time\" + str(time))\ntic = datetime.datetime.now()\ntest['comment_no_stop']=test['comment_text_clean'].apply(lambda sentence : remove_words_in_string(sentence, stops)) ### to lower case\ntac = datetime.datetime.now()\ntime = tac - tic\nprint(\"remove stop words time : \" + str(time))\n","f0cf51c9":"test = tfidf.transform(test['comment_no_stop'])\nsubmission =  logisticRegr.predict_proba(test)\nsubmission = pd.DataFrame(submission)\nmy_submission = pd.read_csv('..\/input\/sample_submission.csv', header=0, sep=',', quotechar='\"')\nmy_submission = pd.merge(submission, my_submission, left_index=True, right_index=True)\nmy_submission=my_submission[['id', 1]].rename(columns= {1 : 'prediction'})\nmy_submission.to_csv(r'submission.csv', index=False)\n","a2103c93":"my_submission.head(10)","80a07b85":"### Submission","38fdc5b1":"## TFIDF on the toxic part only !!","7a46db25":"## Logistic regression","27ac8cb0":"## Splitting train\/dev set","c22511a1":"#### 1)In this version, stopwords are removed using tokeninsation and then search in list. Seems to be very inefficient (time consumming)**","9e8de67b":"### first we need to clean !","792e0698":"### Load packages","3779d7fd":"### Cleaning test set","e507fd71":"## Naives bayes","390bcfb0":"## For the unclean train_explore","308b9b02":"### small set of everything : train_explore","db28ab5a":"# Playing with TF.IDF","4297e0ca":"# Experimenting models","d4eff670":"### TFIDF (set with a threshold of toxicity)","7a95b110":"# Importing data","03001d83":"#### 2)Different process for removing stop words wichi is Three to four time faster (most likely because it does not tokenize and save in a new column)****","589289cb":"# Exploirng data","88c2f9f9":"# Submission"}}