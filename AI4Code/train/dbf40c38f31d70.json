{"cell_type":{"5d8ce672":"code","2d58d748":"code","0bdd73f1":"code","fdbefb5b":"code","74bae908":"code","f7a06d48":"code","a222202e":"code","12742dee":"code","7976423c":"code","232ff9c4":"code","182373bb":"code","ddd3c935":"code","c8cd6e34":"code","3e235ef7":"code","865f813f":"code","4f66eae2":"code","25b75d48":"code","f430cc03":"code","7e211253":"code","0dca6c2c":"code","4ffdfed6":"code","45ba538d":"code","7a163236":"code","b5232e0d":"code","8d0bf206":"code","68f56dd3":"code","73803067":"code","2c9ccdea":"code","047fc6d3":"code","f757f14a":"code","9fb3ad96":"code","ad9036e3":"code","786836df":"code","5af7d286":"code","8ff5c70d":"code","038a347c":"code","12d767b9":"code","2cc1d858":"code","ce3177c4":"code","b3bb4e7a":"code","7839e01d":"code","538701e5":"markdown","e33ff2b7":"markdown","4a64366a":"markdown","e3a3462e":"markdown","56cd34d6":"markdown","db457e41":"markdown","52b6eea4":"markdown","4d7d138f":"markdown","948c2cb1":"markdown","f2c06e83":"markdown","d2c4c60d":"markdown","6e874696":"markdown","f1479888":"markdown","23cf2c10":"markdown","07af25fb":"markdown","fd3b4909":"markdown","55b11b96":"markdown","68cf44a0":"markdown","cb089ab2":"markdown","1e239d30":"markdown","d564ad85":"markdown","99f4fae9":"markdown","82f2f503":"markdown","b992798c":"markdown","7c3e0620":"markdown","3d68b549":"markdown","7b30f86a":"markdown","12562a6e":"markdown","b08bcade":"markdown"},"source":{"5d8ce672":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2d58d748":"# Read the data\ndata = pd.read_csv(r\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","0bdd73f1":"data.shape","fdbefb5b":"data['TotalCharges'] = data[\"TotalCharges\"].replace(\" \",np.nan)","74bae908":"data.dropna(inplace = True)","f7a06d48":"data.isnull().sum()","a222202e":"data.nunique()","12742dee":"data.head()","7976423c":"# Variables and their unique values for analysis\nfor item in data.columns:\n    print(item,\" : \", data[item].unique())","232ff9c4":"data['MultipleLines'].replace(to_replace = 'No phone service',value = 'No',inplace = True)\n\nreplace_columns = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingMovies','StreamingTV']\nfor i in replace_columns:\n    data[i].replace(to_replace = 'No internet service',value = 'No',inplace = True)","182373bb":"data.nunique()","ddd3c935":"def plot_feature(feature):\n    ax = sns.countplot(x = feature,data = data)\n    total = len(data)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/total * 100),\n            ha=\"center\")     \n    plt.title(\"Customer Distribution by {}\".format(feature)) \n    plt.show()","c8cd6e34":"plot_feature('Churn')\nplot_feature('gender')\nplot_feature('Partner')\nplot_feature('SeniorCitizen')\nplot_feature('PhoneService')\nplot_feature('MultipleLines')\nplot_feature('InternetService')\nplot_feature('OnlineSecurity')\nplot_feature('Contract')","3e235ef7":"def plot_bar(d,var1,var2):\n    grp = d.groupby(var1)[var2].value_counts()\n    grp.unstack().plot(kind = 'bar')\n    plt.xlabel(var1)\n    plt.ylabel(\"Count of Churn Customers\")\n    plt.title(\"Churn Customer Distribution by {}\".format(var1)) ","865f813f":"plot_bar(data,'gender','Churn');\nplot_bar(data,'SeniorCitizen','Churn');\nplot_bar(data,'Partner','Churn');\nplot_bar(data,'Dependents','Churn');\nplot_bar(data,'PhoneService','Churn');\nplot_bar(data,'MultipleLines','Churn');\nplot_bar(data,'InternetService','Churn');\nplot_bar(data,'OnlineSecurity','Churn');\nplot_bar(data,'OnlineBackup','Churn');\nplot_bar(data,'DeviceProtection','Churn');\nplot_bar(data,'TechSupport','Churn');\nplot_bar(data,'StreamingTV','Churn');\nplot_bar(data,'StreamingMovies','Churn');\nplot_bar(data,'Contract','Churn');\nplot_bar(data,'PaperlessBilling','Churn');\nplot_bar(data,'PaymentMethod','Churn');","4f66eae2":"ax = sns.distplot(data['tenure'], hist=True, kde=False, \n             bins=int(200\/6), \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax.set_ylabel('Number of Customers')\nax.set_xlabel('Tenure in months')\nax.set_title('Distribution of Customers by tenure')","25b75d48":"def tenure_count(row):\n    if row['tenure'] <= 12 :\n        return 'tenure_0-12'\n    elif (row['tenure'] > 12 and row['tenure'] <= 24):\n        return 'tenure_12-24'\n    elif (row['tenure'] > 42 and row['tenure'] <= 36):\n        return 'tenure_24-36'\n    elif (row['tenure'] > 36 and row['tenure'] <= 48):\n        return 'tenure_36-48'\n    elif (row['tenure'] > 48 and row['tenure'] <= 60):\n        return 'tenure_48-60'\n    else:\n        return 'tenure_60+'\n    \ndata['grp_tenure'] = data.apply(tenure_count,axis = 1)\ndata['grp_tenure'].value_counts()","f430cc03":"plot_bar(data,'grp_tenure','Churn')","7e211253":"data['TotalCharges'] = data['TotalCharges'].astype(float)","0dca6c2c":"plt.scatter(x = data['MonthlyCharges'],y = data['TotalCharges'],c = 'green')\nplt.xlabel(\"Monthly Charges\")\nplt.ylabel(\"Total Charges Charges\")\nplt.title(\"Ralation between Monthly and Total Charges\")","4ffdfed6":"ax = sns.kdeplot(data[data['Churn'] == 'No']['MonthlyCharges'])\nax = sns.kdeplot(data[data['Churn'] == 'Yes']['MonthlyCharges'],color = 'Red')\nax.set_xlabel('Monthly Charges')\nax.set_ylabel('Density')\nax.set_title('Distribution of Monthly charges by churn')","45ba538d":"ax = sns.kdeplot(data[data['Churn'] == 'No']['TotalCharges'])\nax = sns.kdeplot(data[data['Churn'] == 'Yes']['TotalCharges'],color = 'Red')\nax.set_xlabel('Total Charges')\nax.set_ylabel('Density')\nax.set_title('Distribution of Total charges by churn')","7a163236":"data.drop(['customerID'],axis = 1,inplace = True)","b5232e0d":"objList = data.select_dtypes(include = \"object\").columns\nprint (objList)","8d0bf206":"#Label Encoding for object to numeric conversion\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfor feat in objList:\n    data[feat] = le.fit_transform(data[feat].astype(str))\n\nprint (data.info())","68f56dd3":"y = data['Churn']\ndata.drop(['Churn'],axis = 1, inplace = True)\nTrain_x = data","73803067":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX=sc.fit_transform(Train_x)","2c9ccdea":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25,random_state = 0)","047fc6d3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import *\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","f757f14a":"LR = LogisticRegression(solver = 'liblinear')\nLR.fit(X_train,y_train)\ny_pred_LR = LR.predict(X_test)\nLR_Score = accuracy_score(y_pred_LR,y_test)\nprint(\"Accuracy Using LR : \", LR_Score)","9fb3ad96":"#Weights of the Variables\npd.Series(LR.coef_[0],index = Train_x.columns.values)","ad9036e3":"RF = RandomForestClassifier(n_estimators=600,max_features=15,\n                            n_jobs = -1,random_state=0,\n                            min_samples_leaf=50,oob_score=True,\n                            max_leaf_nodes=30 )\nRF.fit(X_train,y_train)\ny_pred_RF = RF.predict(X_test)\nRF_Score = accuracy_score(y_pred_RF,y_test)\nprint(\"Accuracy Using RF  : \", RF_Score)","786836df":"imp_features = pd.Series(RF.feature_importances_,index = Train_x.columns.values)\nimp_features.sort_values()[-5:].plot(kind = 'bar')","5af7d286":"SVM = SVC(kernel='rbf',C =1) \nSVM.fit(X_train,y_train)\ny_pred_SVM = SVM.predict(X_test)\nSVM_Score = accuracy_score(y_pred_SVM,y_test)\nprint(\"Accuracy Using SVM  : \", SVM_Score)","8ff5c70d":"gaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred_GB = gaussian.predict(X_test)\nGB_Score = accuracy_score(y_pred_GB,y_test)\nprint(\"Accuracy Using Gaussian Algorithm : \", GB_Score)","038a347c":"XGB = XGBClassifier(n_estimators=100,learning_rate = 0.1,max_depth = 4)\nXGB.fit(X_train, y_train)\ny_pred_XGB = XGB.predict(X_test)\nXGB_Score = accuracy_score(y_test, y_pred_XGB)\nprint(\"Accuracy Using XGBoost : \", XGB_Score)","12d767b9":"lgbm = LGBMClassifier(n_estimators=100,learning_rate = 0.1,max_depth = 5)\nlgbm.fit(X_train, y_train)\ny_pred_LGBM = lgbm.predict(X_test)\nLGBM_Score = accuracy_score(y_test,y_pred_LGBM )\nprint(\"Accuracy Using LIGTH GBM Classifier : \", LGBM_Score)","2cc1d858":"labels = ['Churn', 'Not-Churn']\ncm = confusion_matrix(y_test, y_pred_LGBM)\nprint(cm)","ce3177c4":"ax= plt.subplot()\nsns.heatmap(cm,annot=True, ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['Not-Churn', 'Churn']); ax.yaxis.set_ticklabels(['Not-Churn', 'Churn']);","b3bb4e7a":"Results = pd.DataFrame({'Model': ['Logistic Regression','Gaussian Naive Bayes','SVM','Random Forest','XG_Boost','LightGBM'],\n                        'Accuracy Score' : [LR_Score,GB_Score,SVM_Score,RF_Score,XGB_Score,LGBM_Score]})","7839e01d":"Final_Results = Results.sort_values(by = 'Accuracy Score', ascending=False)\nFinal_Results = Final_Results.set_index('Model')\nprint(Final_Results)","538701e5":"Support Vector Machines","e33ff2b7":"1. Data consist of 7043 records with 21 features.\n2. It is observed that there are around 11 records in \"Total charges\" column having space values.Hence these are replaced by Nan values and later on removed as they are very less in number. ","4a64366a":"Reference : https:\/\/www.kaggle.com\/bandiatindra\/telecom-churn-prediction","e3a3462e":"from the above graph in red line, As monthlly charges increases, there is higher probability of Churn","56cd34d6":"XGBOOST","db457e41":"As monthly charge increases,Total Charge also increases.","52b6eea4":"Random Forest","4d7d138f":"From above, we can say that Features such as ( MultipleLines','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingMovies','StreamingTV') have multiple values as 'No Internet Service or No Phone Service' instead of 'NO'.\nHence these are replaced by 'No' for better analysis.","948c2cb1":"Aim : To predict the behaviour of customers in order to retain them.      \n     In short, Analyze the data and use machile learning techniques in order to prevent the loss of clients\/customers for      telecom industry \n\nDefinition : (Source : Investopedia)        \nThe churn rate, also known as the rate of attrition or customer churn, is the rate at which customers stop doing business with an entity. It is most commonly expressed as the percentage of service subscribers who discontinue their subscriptions within a given time period","f2c06e83":"Label Encoding: I used Label encoding.We can also use One hot encoding ","d2c4c60d":"from the above graph in red line, Churn Rate is Higher when Total charges are on lower side","6e874696":"# Title : Telecom Customer Churn","f1479888":"# Implement Machine Learning Algorithms","23cf2c10":"'nunique' is used to see how many unique values each feature has.\nIt will be easy to plot and analyze each feature based on unique values.","07af25fb":"LIGHTGBM Classifier","fd3b4909":"It seems Total Charges,Monthly charges are positively related to churn while Tenure,Phone Service,Contract are negatively related to Churn rate .All these are important factors while deciding churn rate based on their weights\n","55b11b96":"Logistic Regression","68cf44a0":"It is clear that customers having one or two month tenure(New Customers to Service) have higher Churn Rate ","cb089ab2":"Import Necessary Libraries","1e239d30":"Observations :\n    1. Distribution of Customers by Churn \n        73% - No and  27% - Yes\n    2. There are equal number of male and female customers(50-50%)\n    3. 48% customers have partners while 52% don't have\n    4. There are less number of senior citizens(around 16 %) while majority of the people are young(84%)\n    5. 10% people do not have Phone Service\n    6. Distribution of Customers by Multiple Lines \n        58% - No and 42% - Yes\n    7. Distribution of Customers by Internet Service \n        a. 34% - DSL \n        b. 44% - Fiber Optic Service\n        c. 22% - No Internet Service\n    8. Distribution of Customers by Online Security \n        71% - No and 29 % Yes\n    9. Distribution of Customers by Contract\n        a. 55% - Month to Month \n        b. 21% - One Year Contract\n        c. 24% - Two Year Contract","d564ad85":"1. There are large number of customers  with tenure greater than 60 months and also they have less Churn Rate as compared to count\n2. There are around 90% chances that customers with tenure less than 12 months will left the service.(High Churn)\n","99f4fae9":"Observations :\n    1. Churn rate is almost same for both male and female customers\n    2. There are high number of churn customers who are not senior citizens in number, but if we see there are less number         of Senior citizens.Hence when compared with total Senior citizen, Churn rate is higher for SeniorCitizen as compared to Young People\n    3. The Customers who don't have partners as well as no dependents have higher churn rate\n    4. Customers having Phone service have higher chances of Churn while those who don't phone seervice , they have minimal chances of churn\n    5. Customers with Internet Service type as \"Fiber Optic\" have higher chances of churn\n    6. Customers who don't have TechSupport have higher churn rate\n    7. Month Month Contract cutomers have a very high Churn Rate\n    8. Cutomers having Paperless billing have higher Churn Rate\n    9. Customers with Electronic Check have more churn than any other payment methods\n    10.MultipleLines, StreamingTV, StreamingMovies have not much effect on Churn","82f2f503":"Exploring Features with respect to Churn","b992798c":"Top five Important featues using Random Forest\n1. Contract\n2. Monthly Charges\n3. Tenure\n4. Total Charges\n5. Internet Service\n\nThese features have also high weightage by Logistic Regression. Hence,We can conclude that,these features should be considered while deciding Churn ","7c3e0620":"## Exploratory Data Analysis ","3d68b549":"Gaussian Naive Bayes","7b30f86a":"Converting Categorical values into Numerical","12562a6e":"Above results show Accuracy for LightGBM , Random Forest,LR and XGBoost is nearly same.                                \nAccuracy maybe further increased using one hot encoding technique rather than Label encoder that we used above.","b08bcade":"Exploring Individual Features"}}