{"cell_type":{"89fdf7df":"code","9554a672":"code","a3c43d47":"code","7c6d8fbe":"code","f2114de1":"code","d32969b7":"code","0115513a":"code","80bf1379":"code","c8dfa516":"code","3a5477c4":"code","5f14883e":"code","e6b0b95a":"code","ca6eb42a":"code","efbee31d":"code","273567e9":"code","49674463":"code","c0ae8f1a":"code","ff3ffab7":"code","0e62288e":"code","1a554c98":"code","da2b9d45":"markdown"},"source":{"89fdf7df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9554a672":"from sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n\n\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import SimpleRNN\nfrom keras.layers import Embedding\nfrom keras.layers import Bidirectional\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Dropout\n\nfrom keras.utils import to_categorical\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt","a3c43d47":"df = pd.read_csv(\"\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\", names=['comment', 'label'], header=0, encoding='utf-8')","7c6d8fbe":"df.head()","f2114de1":"document_lenghts = list(map(len, df.comment.values))\nprint(np.max(document_lenghts))\nprint(np.min(document_lenghts))\nprint('mean_size:',np.mean(document_lenghts))\nprint('median_size:',np.median(document_lenghts))","d32969b7":"df.label.value_counts()","0115513a":"dictionary_length = 1000\ninput_length = 100\n\ntokenizer = Tokenizer(num_words=dictionary_length)\ntokenizer.fit_on_texts(df.comment.values)","80bf1379":"post_seq = tokenizer.texts_to_sequences(df.comment.values)","c8dfa516":"print(len(post_seq))\nprint(post_seq[0])\nprint(len(post_seq[0]))","3a5477c4":"post_seq_padded = pad_sequences(post_seq, maxlen=input_length)","5f14883e":"print(len(post_seq_padded))\nprint(post_seq_padded[0])\nprint(len(post_seq_padded[0]))","e6b0b95a":"x_original = post_seq_padded\nx_original = np.array(x_original)\n\ny_original = df['label'].values\ny_original = 1*(y_original=='positive')\ny_original = np.array(y_original)\n\n\n\n\nx, y = shuffle(x_original, y_original, random_state=23)","ca6eb42a":"x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.3, random_state=23)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=23)","efbee31d":"print(\"train set:\", x_train.shape)\nprint(\"validation set:\", x_val.shape)\nprint(\"test set:\", x_test.shape)","273567e9":"model = Sequential()\nmodel.add(Embedding(dictionary_length, 2, input_length=input_length))\nmodel.add(Dense(32,activation=\"relu\"))\nmodel.add(Dense(32,activation=\"relu\"))\nmodel.add(Bidirectional(SimpleRNN(16, return_sequences=True)))\nmodel.add(Bidirectional(SimpleRNN(16, return_sequences=True)))\nmodel.add(Bidirectional(SimpleRNN(16, return_sequences=False)))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","49674463":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","c0ae8f1a":"history = model.fit(x=x_train, y=y_train, batch_size=256, verbose=1, epochs=5, validation_data=(x_val, y_val))","ff3ffab7":"history_dict = history.history\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.figure(figsize=(8,5))\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(8,5))\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim((0.5,1))\nplt.show()","0e62288e":"bad_comment = tokenizer.texts_to_sequences([\"Wow, this is the worst film I ever seen. This film is really bad\"])\ngood_comment = tokenizer.texts_to_sequences([\"Not so bad, it is not a masterpiece but I liked it\"])\n\nbad_comment = pad_sequences(bad_comment, maxlen=input_length)\ngood_comment = pad_sequences(good_comment, maxlen=input_length)\n\n#print(bad_comment)\nprint(model.predict(bad_comment))\nprint(model.predict(good_comment))","1a554c98":"e = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape)\nplt.figure(figsize=(12,9))\nplt.plot(weights[:,0], weights[:,1], 'bo')\nplt.title('Word embedding')\nplt.show()","da2b9d45":"The dataset is balanced"}}