{"cell_type":{"120dd71e":"code","d19308ce":"code","71ce4b94":"code","c3c78221":"code","483629e5":"code","c296d73c":"code","7d371bc0":"code","a4626301":"code","f483cde7":"code","f927a20b":"code","8c74ac84":"code","82836535":"code","dd1b933e":"code","8ec3d46d":"code","18b713fc":"code","6357ba50":"markdown","4b7541ec":"markdown","7086b711":"markdown","cd435a2e":"markdown","0facb627":"markdown","99950068":"markdown","e152c988":"markdown","d7b7ad80":"markdown","370d12cd":"markdown","18b35f6d":"markdown","e57df2e4":"markdown","fcc32544":"markdown","5232ac87":"markdown","b6fecf30":"markdown","9362526f":"markdown"},"source":{"120dd71e":"!cp -r ..\/input\/vittutorialillustrations\/* .\/ \n\n!pip install nb_black\n%load_ext nb_black","d19308ce":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 1.7\n!pip install timm","71ce4b94":"# Numpy, Pandas and MatPlot\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.style.use(\"ggplot\")\n\n# Who doesn't love PyTorch\nimport torch\nimport torch.nn as nn\n\n# For the Transformations\nimport torchvision.transforms as transforms\n\n# For the TPU to work\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl\n\nimport timm\n\n# Some Native Python Libraries\nimport gc\nimport os\nimport time\nimport random\nfrom datetime import datetime\n\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom sklearn import model_selection, metrics","c3c78221":"# For parallelization in TPUs\nos.environ[\"XLA_USE_BF16\"] = \"1\"\nos.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\"","483629e5":"def seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results\n    \n    Arguments:\n        seed {int} -- Number of the seed\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nseed_everything(1001)","c296d73c":"# general global variables\nDATA_PATH = \"..\/input\/cassava-leaf-disease-classification\"\nTRAIN_PATH = \"..\/input\/cassava-leaf-disease-classification\/train_images\/\"\nTEST_PATH = \"..\/input\/cassava-leaf-disease-classification\/test_images\/\"\nMODEL_PATH = (\n    \"..\/input\/vit-base-models-pretrained-pytorch\/jx_vit_base_p16_224-80ecf9dd.pth\"\n)\n\n# model specific global variables\n\n# THIS IS THE IMAGE SIZE IN PIXELS:\nIMG_SIZE = 224\n\nBATCH_SIZE = 16\n\nLR = 2e-05\n\nN_EPOCHS = 10","7d371bc0":"# Lets take a look at the data:\ndf = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n\n# Splitting the dataset into training and testsets:\ntrain_df, valid_df = model_selection.train_test_split(\n    df, test_size=0.1, random_state=42, stratify=df.label.values\n)\n\ndf.head()","a4626301":"df.label.value_counts().plot(kind=\"bar\")","f483cde7":"class CassavaDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Helper Class to create the pytorch dataset\n    \"\"\"\n\n    def __init__(self, df, data_path=DATA_PATH, mode=\"train\", transforms=None):\n        super().__init__()\n        self.df_data = df.values\n        self.data_path = data_path\n        self.transforms = transforms\n        self.mode = mode\n        self.data_dir = \"train_images\" if mode == \"train\" else \"test_images\"\n\n    def __len__(self):\n        return len(self.df_data)\n\n    def __getitem__(self, index):\n        img_name, label = self.df_data[index]\n        img_path = os.path.join(self.data_path, self.data_dir, img_name)\n        img = Image.open(img_path).convert(\"RGB\")\n\n        if self.transforms is not None:\n            image = self.transforms(img)\n\n        return image, label","f927a20b":"# Time to do some data augmentations.\n\ntransforms_train = transforms.Compose(\n    [\n        # Resizing the image to the previously stated size\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        # Randomly flipping the images horizontally with the probability of 30%\n        transforms.RandomHorizontalFlip(p=0.3),\n        # Randomly flipping the images vertically with the probability of 30%\n        transforms.RandomVerticalFlip(p=0.3),\n        # Randomly Rotating the images by 10 degrees\n        transforms.RandomRotation(10),\n        # Randomly 10 degrees worth of Affine\n        transforms.RandomAffine(10),\n        # Croppping the images to the stated\n        transforms.RandomResizedCrop(IMG_SIZE),\n        # Converting the image to tensor\n        transforms.ToTensor(),\n        # Normalizing\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ]\n)\n\ntransforms_valid = transforms.Compose(\n    [\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ]\n)","8c74ac84":"class ViTBase16(nn.Module):\n    def __init__(self, n_classes, pretrained=False):\n\n        super(ViTBase16, self).__init__()\n\n        self.model = timm.create_model(\"vit_base_patch16_224\", pretrained=False)\n\n        # I mean it is pretrained?!\n        if pretrained:\n            self.model.load_state_dict(torch.load(MODEL_PATH))\n\n        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n\n        # The Accuracy starts from Zero:\n        epoch_loss = 0.0\n        epoch_accuracy = 0.0\n\n        # This bad boy trains the model\n        self.model.train()\n        for i, (data, target) in enumerate(train_loader):\n\n            # Clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = self.forward(data)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # Calculate Accuracy\n            accuracy = (output.argmax(dim=1) == target).float().mean()\n            # update training loss and accuracy\n            epoch_loss += loss\n            epoch_accuracy += accuracy\n\n            # perform a single optimization step (parameter update)\n            if device.type == \"xla\":\n                xm.optimizer_step(optimizer)\n\n                if i % 20 == 0:\n                    xm.master_print(f\"\\tBATCH {i+1}\/{len(train_loader)} - LOSS: {loss}\")\n\n            else:\n                optimizer.step()\n\n        return epoch_loss \/ len(train_loader), epoch_accuracy \/ len(train_loader)\n\n    def validate_one_epoch(self, valid_loader, criterion, device):\n        # keep track of validation loss\n        valid_loss = 0.0\n        valid_accuracy = 0.0\n\n        ######################\n        # validate the model #\n        ######################\n        self.model.eval()\n        for data, target in valid_loader:\n            # move tensors to GPU if CUDA is available\n            if device.type == \"cuda\":\n                data, target = data.cuda(), target.cuda()\n            elif device.type == \"xla\":\n                data = data.to(device, dtype=torch.float32)\n                target = target.to(device, dtype=torch.int64)\n\n            with torch.no_grad():\n                # forward pass: compute predicted outputs by passing inputs to the model\n                output = self.model(data)\n                # calculate the batch loss\n                loss = criterion(output, target)\n                # Calculate Accuracy\n                accuracy = (output.argmax(dim=1) == target).float().mean()\n                # update average validation loss and accuracy\n                valid_loss += loss\n                valid_accuracy += accuracy\n\n        return valid_loss \/ len(valid_loader), valid_accuracy \/ len(valid_loader)","82836535":"def fit_tpu(\n    model, epochs, device, criterion, optimizer, train_loader, valid_loader=None\n):\n\n    valid_loss_min = np.Inf  # track change in validation loss\n\n    # keeping track of losses as it happen\n    train_losses = []\n    valid_losses = []\n    train_accs = []\n    valid_accs = []\n\n    for epoch in range(1, epochs + 1):\n        gc.collect()\n        para_train_loader = pl.ParallelLoader(train_loader, [device])\n\n        xm.master_print(f\"{'='*50}\")\n        xm.master_print(f\"EPOCH {epoch} - TRAINING...\")\n        train_loss, train_acc = model.train_one_epoch(\n            para_train_loader.per_device_loader(device), criterion, optimizer, device\n        )\n        xm.master_print(\n            f\"\\n\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss}, ACCURACY: {train_acc}\\n\"\n        )\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        gc.collect()\n\n        if valid_loader is not None:\n            gc.collect()\n            para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n            xm.master_print(f\"EPOCH {epoch} - VALIDATING...\")\n            valid_loss, valid_acc = model.validate_one_epoch(\n                para_valid_loader.per_device_loader(device), criterion, device\n            )\n            xm.master_print(f\"\\t[VALID] LOSS: {valid_loss}, ACCURACY: {valid_acc}\\n\")\n            valid_losses.append(valid_loss)\n            valid_accs.append(valid_acc)\n            gc.collect()\n\n            # save model if validation loss has decreased\n            if valid_loss <= valid_loss_min and epoch != 1:\n                xm.master_print(\n                    \"Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...\".format(\n                        valid_loss_min, valid_loss\n                    )\n                )\n            #                 xm.save(model.state_dict(), 'best_model.pth')\n\n            valid_loss_min = valid_loss\n\n    return {\n        \"train_loss\": train_losses,\n        \"valid_losses\": valid_losses,\n        \"train_acc\": train_accs,\n        \"valid_acc\": valid_accs,\n    }","dd1b933e":"model = ViTBase16(n_classes=5, pretrained=True)","8ec3d46d":"def _run():\n    train_dataset = CassavaDataset(train_df, transforms=transforms_train)\n    valid_dataset = CassavaDataset(valid_df, transforms=transforms_valid)\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=True,\n    )\n\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n        valid_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False,\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=8,\n    )\n\n    valid_loader = torch.utils.data.DataLoader(\n        dataset=valid_dataset,\n        batch_size=BATCH_SIZE,\n        sampler=valid_sampler,\n        drop_last=True,\n        num_workers=8,\n    )\n\n    criterion = nn.CrossEntropyLoss()\n    #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    device = xm.xla_device()\n    model.to(device)\n\n    lr = LR * xm.xrt_world_size()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    xm.master_print(f\"INITIALIZING TRAINING ON {xm.xrt_world_size()} TPU CORES\")\n    start_time = datetime.now()\n    xm.master_print(f\"Start Time: {start_time}\")\n\n    logs = fit_tpu(\n        model=model,\n        epochs=N_EPOCHS,\n        device=device,\n        criterion=criterion,\n        optimizer=optimizer,\n        train_loader=train_loader,\n        valid_loader=valid_loader,\n    )\n\n    xm.master_print(f\"Execution time: {datetime.now() - start_time}\")\n\n    xm.master_print(\"Saving Model\")\n    xm.save(\n        model.state_dict(), f'model_5e_{datetime.now().strftime(\"%Y%m%d-%H%M\")}.pth'\n    )","18b713fc":"# Start training processes\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type(\"torch.FloatTensor\")\n    a = _run()\n\n\n# _run()\nFLAGS = {}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method=\"fork\")","6357ba50":"<h1 style=\"font-size:32px; font-family:Tahoma;margin-right:50px\"> Fitting the Model to the TPU \ud83c\udfd7<\/h1>","4b7541ec":"<center><br><h1 style=\"font-size:50px; font-family:Tahoma;margin-top:50px;position:inherit;\">Vision Transformers are Easy<\/h1><\/center>\n<br>\n<center><img src=\"https:\/\/www.iita.org\/wp-content\/uploads\/2019\/07\/Cassava-Mosaic-Disease.jpg\" style=\"width:70%; border-radius:10px;\"><\/center>","7086b711":"<p style=\"font-size:22px; font-family:Tahoma;margin-right:50px\">Installing Pytorch-XLA inorder to access the TPU:<\/p>","cd435a2e":"<p style=\"font-size:22px; font-family:Tahoma;margin-right:50px\">\ud83d\udea8 Jargon Alert \ud83d\udea8<br><br>I know, TPU Parallelization sounds aweful but you can watch <a href=\"https:\/\/www.youtube.com\/watch?v=HgGyWS40g-g\">this video<\/a> about it. Watch till 1:04. It is by folks at TensorFlow so you need not watch it for a very long time.<br>(No one likes TensorFlow or atleast in the world I live in)<\/p>","0facb627":"<p style=\"font-size:22px; font-family:Tahoma;margin-right:50px\">Please give some due credit to <a href=\"https:\/\/www.kaggle.com\/abhinand05\">Abhinand<\/a> for helping make this Kernel a reality.<\/p>","99950068":"<p style=\"font-size:22px; font-family:Tahoma;margin-right:50px\">When we say that: <br>\"Hey let's seed this!\"<br>\nDL Developers aren't really thinking about gardening, or well, uh. It has to do maybe a little bit about the reproducebility of the model.<br><br>Check out this Medium Article:<a href=\"https:\/\/medium.com\/@ODSC\/properly-setting-the-random-seed-in-ml-experiments-not-as-simple-as-you-might-imagine-219969c84752\"><br><i>Properly Setting the Random Seed in ML Experiments. Not as Simple as You Might Imagine<\/i> \ud83d\udc35<\/a><br>to get a better idea.<\/p>","e152c988":"<p style=\"font-size:22px; font-family:Tahoma;margin-right:50px\">Importing some really important stuff in here. I hope you are still awake as you course through this notebook.<p>","d7b7ad80":"<p style=\"font-size:22px; font-family:Tahoma;margin-right:50px\">Grabbing the Model<\/p>","370d12cd":"<h1 style=\"font-size:32px; font-family:Tahoma;margin-right:50px\">Data Augmentation to go through:<\/h1>\n<p style=\"font-size:22px; font-family:Tahoma;margin-right:50px\">We go through some really particular Data Augmentations. I have to say that our dude <a href=\"https:\/\/www.kaggle.com\/abhinand05\">Abhinand<\/a> did an amazing job here. Kudos to him.<br><br>\nIf you want a better explanation head to the docs of <em style=\"background-color:#bfbfbf; font-family: 'Courier New', monospace;\">torchvision.transforms<\/em> <a href=\"https:\/\/pytorch.org\/docs\/stable\/torchvision\/transforms.html\">here<\/a>.<\/p>","18b35f6d":"<p style=\"font-size:22px; font-family:Tahoma;margin-right:50px\">Defining the CasavaDataset:<\/p>","e57df2e4":"<h1 style=\"font-size:32px; font-family:Tahoma;margin-right:50px\">Defining the Model<\/h1>","fcc32544":"<p style=\"font-size:22px; font-family:Tahoma;margin-right:50px\">We will now be visualizing on how\n<p style=\"font-size:22px; font-family:Tahoma;margin-right:50px\">Plotting to see how various labels are distributed:<\/p>","5232ac87":"<p style=\"font-size:22px; font-family:Tahoma;margin-right:50px\">Well folks, that would be it, I'll update this every once in a while to see if there's more scope for improvement. I will not be using this notebook to make submissions because parts of it are code that were adapted from other's works. Please give some due credit to <a href=\"https:\/\/www.kaggle.com\/abhinand05\">Abhinand<\/a> too. <br><br>Thanks for going through the notebook. An upvote would be appreciated \ud83d\ude0b<\/p>","b6fecf30":"<h1 style=\"font-size:32px; font-family:Tahoma;margin-right:50px\">Training the Model<\/h1>","9362526f":"<p style=\"font-size:22px; font-family:Tahoma;margin-right:50px\">Specifying necessary paths and other necessities:<p>"}}