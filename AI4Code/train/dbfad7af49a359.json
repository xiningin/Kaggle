{"cell_type":{"094f3d27":"code","806d90fd":"code","98aebd59":"code","1e3395a4":"code","83530705":"code","c6d8a524":"code","637a4b5b":"code","8654ab02":"code","f828e2fe":"code","2b4cedee":"code","117cecf0":"code","beba2935":"code","258564e5":"code","be9622a7":"code","5aa3da2d":"code","3806e592":"code","a900301f":"code","9f7bc145":"code","a9276ae0":"code","2f739c75":"code","efa6a1fa":"code","518c2374":"code","9a7d5f9e":"code","b8b990e8":"code","8b58f5a0":"code","bc9f6168":"markdown","ba23d756":"markdown","fa0b32f0":"markdown","b46cceaa":"markdown","52d7a7e5":"markdown","9a5071a6":"markdown","db112981":"markdown","001cd53c":"markdown","0aeb718e":"markdown","51f4a935":"markdown","801e9d42":"markdown","60234028":"markdown","8c9625b1":"markdown"},"source":{"094f3d27":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom yellowbrick.cluster import KElbowVisualizer\n\nimport os\n\n","806d90fd":"print(os.listdir(\"..\/input\"))","98aebd59":"dataset=pd.read_excel(\"..\/input\/dataset.xlsx\")\ndataset #this is our data ","1e3395a4":"dataset.isnull().sum() #Here we see if there are empty answers.","83530705":"dataset.describe().T # Statistical properties of numeric data.","c6d8a524":"dataset.hist(figsize=(10,10),layout=(3,1)); #Histograms of our numeric data.","637a4b5b":"#number of unique values for each column\nfor col in dataset:\n    print(len(dataset[col].unique()))","8654ab02":"#Here we seperate our structural data from numeric data.\nstructural1=dataset.iloc[:,0:5]\nstructural2=dataset.iloc[:,8:]\nnumeric_data=dataset.iloc[:,5:8]\nstructural_data=pd.concat([structural1,structural2],axis=1)\nstructural_data.head()#first 5 rows of our structural data","f828e2fe":"numeric_data.head()#first 5 rows of our numeric data","2b4cedee":"#here we convert our structural data into numeric data\nle=LabelEncoder()\n\nfor col in structural_data:\n    structural_data[col]=le.fit_transform(structural_data[col])\nstructural_data","117cecf0":"#Finally we merge our processed structural data with our numeric data.\nfinal_data=pd.concat([structural_data,numeric_data],axis=1)\nfinal_data","beba2935":"pca = PCA(n_components=2)\n\nreduced_data=pca.fit_transform(final_data)\n\nplt.scatter(reduced_data[:,0],reduced_data[:,1])\nplt.show()\n","258564e5":"kmeans=KMeans() #this is our clustering algorithm ->KMeans\n\n#We will determine our optimal number of clusters with elbow method from both our actual data and reduced data.\n\nvisualiser=KElbowVisualizer(kmeans,k=(2,10))\nvisualiser.fit(final_data)\nvisualiser.poof()\n\n","be9622a7":"kmeans=KMeans()\nvisualiser=KElbowVisualizer(kmeans,k=(2,10))\nvisualiser.fit(reduced_data)\nvisualiser.poof()","5aa3da2d":"algorithm=KMeans(n_clusters=4)#our algorithm\nk_fit=algorithm.fit(final_data)\n\nclusters=k_fit.labels_\nclusters #here are our clusters","3806e592":"plt.scatter(reduced_data[:,0],reduced_data[:,1],c=clusters,cmap=\"viridis\")#we visualise the results\nplt.show()","a900301f":"final_data[\"cluster\"]=clusters+1\nfinal_data.head() #first 5 values of final data","9f7bc145":"final_data[final_data[\"cluster\"]==1].head() #first 5 values of cluster 1","a9276ae0":"final_data[final_data[\"cluster\"]==1].describe().T # Statistical properties of cluster 1","2f739c75":"final_data[final_data[\"cluster\"]==2].head() #first 5 values of cluster 2","efa6a1fa":"final_data[final_data[\"cluster\"]==2].describe().T # Statistical properties of cluster 2","518c2374":"final_data[final_data[\"cluster\"]==3].head() #first 5 values of cluster 3","9a7d5f9e":"final_data[final_data[\"cluster\"]==3].describe().T # Statistical properties of cluster 3","b8b990e8":"final_data[final_data[\"cluster\"]==4].head() #first 5 values of cluster 4","8b58f5a0":"final_data[final_data[\"cluster\"]==4].describe().T # Statistical properties of cluster 4","bc9f6168":"## Step 3 - Model Building\nHere we build our clustering model and evaluate it. First we will choose the appropriate cluster number with elbow method.","ba23d756":"Now we will build our model and visualise it with our reduced data.","fa0b32f0":"# Introduction\nAim of this research is segmenting people into clusters according to their sense of entertainment, social behaviour and moral values with <br>\nK-Means algorithm and visualising the results.\n\n*Data which is used in this study is collected from GoogleDocs from real people. Their identities are not shared in order to keep their personal information safe.","b46cceaa":"## Step 3 - Data Preprocessing\n**Here we manipulate our data for our algorithm.**<br><br>\nMachine learning algorithms work with numeric data so we have to preprocess our structural data before using it.","52d7a7e5":"## Step 1 - Importing Libraries\n**Here we import libraries that we will use in our research.**<br>\nNumpy: For linear algebra and array operations<br>\nPandas: For structural data manipulation<br>\nMatplotlib: For data visualisation<br>\nSklearn: For machine machine learning algorithms,data preprocessing and dimensionality reduction<br>\nYellowbrick: For parametre analysis for our algorithm<br>\nos: For directory operations","9a5071a6":"As we can see from the graphs incline suddenly changes and distortion score is not very high when our number of clusters is 4. So we will build a algorithm with 4 clusters.","db112981":"Now let's add our results and have a final look.","001cd53c":"Now we read our dataset and have a look at it.","0aeb718e":"Now it's time to visualise the results.","51f4a935":"And we will apply PCA to our data in order to visualise it approximately.","801e9d42":"## Step 2 - First Look To Our Data\n**Here we look into our data and examine it's general attributes.**<br><br>\nThis is the name of our data file:","60234028":"# Prepearing the Environment and Data Preprocessing ","8c9625b1":"## Result\nAs we can see in the graph and samples at the clusters, approximately all participants are clustered with people who have tendency with them."}}