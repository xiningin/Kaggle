{"cell_type":{"5fbb2226":"code","08ce9875":"code","51cf847e":"code","9c145e5f":"code","ca52e60a":"code","f25ba7c9":"code","2481189c":"code","9c4a6729":"code","a74bdadf":"code","d838957a":"code","7f314b27":"code","7a150727":"code","8d0fa3af":"code","45e65615":"code","d88c25e4":"code","b031406f":"code","2544895f":"code","f474d54c":"code","fab4cf66":"code","cb735458":"code","621229dc":"code","ad4ee689":"code","f2a2c685":"code","586000b6":"code","2abfd956":"code","4e221afd":"code","2daadc7d":"code","3ce8ef23":"code","5eaac41d":"code","c1df12f4":"code","ea9a2f85":"code","68ff51a4":"code","95c77895":"code","7eba1c7a":"code","f702a18d":"code","59740767":"code","8b014bc8":"code","a81259be":"code","f1c65320":"code","0e2366f4":"code","10d878ee":"code","675cf896":"markdown","07b22672":"markdown","81e3db5b":"markdown","ea2aa1eb":"markdown","843fc9da":"markdown","d8a1eb91":"markdown","918c9c0f":"markdown","963bb567":"markdown","5eeeda6b":"markdown","7f3d8a9a":"markdown","af802ce2":"markdown","f972385a":"markdown","48214fb5":"markdown","92ac6e98":"markdown","aed2383e":"markdown","21962131":"markdown","c238030b":"markdown","426699c3":"markdown","b507ed7c":"markdown","2ab22dbf":"markdown","6e84967f":"markdown","71bf05bb":"markdown","af2f58f0":"markdown","f54bfc40":"markdown","3d58f945":"markdown","82f91116":"markdown","32bd9ff2":"markdown","f4c6fa72":"markdown","565326c5":"markdown","4024ce58":"markdown","e69d573e":"markdown","d6294b3c":"markdown","62e7c4eb":"markdown","c44d69e8":"markdown","3e116077":"markdown"},"source":{"5fbb2226":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image","08ce9875":"#Reading Users file:\nu_cols = ['User_ID', 'Age', 'Sex', 'Occupation', 'ZIP_Code']\nusers = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.user',\n                    sep='|', names=u_cols,encoding='latin-1')\n\n#Reading Ratings file:\nr_cols = ['User_ID', 'Movie_ID', 'Rating', 'Timestamp']\nratings = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.data',\n                      sep='\\t', names=r_cols,encoding='latin-1')","51cf847e":"users.shape","9c145e5f":"users.head()","ca52e60a":"ratings.shape","f25ba7c9":"ratings.head()","2481189c":"len(ratings['Movie_ID'].unique())","9c4a6729":"nb_users  = users['User_ID'].nunique()\nnb_movies = ratings['Movie_ID'].nunique()\n\nprint(\"There are %d unique users and %d unique movies; so we need to prepare \" \n      \"an matrix of size %d by %d.\" %(nb_users, nb_movies, nb_users, nb_movies))","a74bdadf":"ratings_matrix = ratings.pivot_table(index=['User_ID'],columns=['Movie_ID'],values='Rating').reset_index(drop=True)\nratings_matrix.fillna(0, inplace = True)\n\ndata_matrix = np.array(ratings_matrix)\nprint(data_matrix.shape)\nprint(data_matrix)","d838957a":"from sklearn.mixture import GaussianMixture\nfrom scipy.special import logsumexp\nimport itertools","7f314b27":"gmm_model = GaussianMixture(n_components=2, covariance_type='full', \n                            tol=0.001, reg_covar=1e-06, max_iter=100, \n                            n_init=1, init_params='kmeans', weights_init=None, \n                            means_init=None, precisions_init=None, random_state=42, \n                            warm_start=False, verbose=0, verbose_interval=10)\ngmm_model.fit(data_matrix)","7a150727":"print(gmm_model.means_.shape)\nprint(gmm_model.covariances_.shape)\nprint(gmm_model.weights_.shape)","8d0fa3af":"Image(\"..\/input\/input\/4.JPG\")","45e65615":"Image(\"..\/input\/input\/5.JPG\")","d88c25e4":"#Fill Missing Values i.e Recommend\ninver0 = np.linalg.inv(gmm_model.covariances_[0])\ninver1 = np.linalg.inv(gmm_model.covariances_[1])\ndeter0 = np.linalg.det(gmm_model.covariances_[0])\ndeter1 = np.linalg.det(gmm_model.covariances_[1])\n\nn = data_matrix.shape[0]\nd = data_matrix.shape[1]\nK = gmm_model.means_.shape[0]\nmean = gmm_model.means_\nvariance = gmm_model.covariances_\nweight = np.log(gmm_model.weights_)\ncalc = np.zeros((n, K))\nind = np.zeros((n, d))\nsoft = calc\nadd = np.zeros((n,))\ndim = np.zeros((n,))\nX_pred = ind\n    \nind = np.where(data_matrix != 0, 1, 0)            \ndim = np.sum(ind, axis=1)\n\nfor i in range(n):\n    for j in range(K):\n        res = data_matrix[i] - mean[j]\n        res = np.multiply(res, ind[i])\n        #Multivariate Gaussian\n        if j == 0:\n            A = (res.T @ inver0) @ res\n            C = (dim[i]\/2)*np.log(2*np.pi) + np.log(deter0 + 1e-16)\/2\n        else:\n            A = (res.T @ inver1) @ res\n            C = (dim[i]\/2)*np.log(2*np.pi) + np.log(deter1 + 1e-16)\/2\n        B = 2\n        calc[i, j] = weight[j] + (-A\/B) - C\n\nadd = logsumexp(calc, axis = 1)\n\n#Since the entire computation is done in log-domain to avoid Numerical instability\n#we need to bring it back in its original domain\nsoft = np.exp(np.subtract(np.transpose(calc), add))\n\nlg = np.sum(add)\n    \nX_calc = np.transpose(soft) @ gmm_model.means_\n\n#We will use predicted value if the entry is 0 in original rating matrix\ndata_matrix_pred_GMM = np.where(data_matrix == 0, X_calc, data_matrix)\n\nfor i in range(data_matrix_pred_GMM.shape[0]):\n    for j in range(data_matrix_pred_GMM.shape[1]):\n        data_matrix_pred_GMM[i, j] = round(data_matrix_pred_GMM[i, j])\n\n#For measuring the performance we have to use the predicted matrix\nfor i in range(X_calc.shape[0]):\n    for j in range(X_calc.shape[1]):\n        X_pred[i, j] = round(X_calc[i, j])","b031406f":"print(\"Original Rating Matrix: \\n\", data_matrix)","2544895f":"print(\"Rating Matrix After Applying GMM: \\n\", data_matrix_pred_GMM)","f474d54c":"ind_matrix = np.zeros((nb_users, nb_movies))\nind_matrix = np.where(data_matrix != 0, 1, 0)\n\nx = np.multiply(X_pred, ind_matrix)\nRMSE_GMM = np.sqrt(np.mean((x - data_matrix)**2))\nprint(\"RMSE of GMM Model is %f.\" %RMSE_GMM)","fab4cf66":"# Understanding Non-Negative Matrix Factorization(NMF)\nfrom sklearn.decomposition import NMF\nimport scipy.sparse as sparse\nfrom scipy.sparse.linalg import spsolve\nfrom numpy.linalg import solve\n\nX = np.array([[1, 2, 3], [5, 10, 15]])\nprint(\"X is:\\n\", X)\nmodel = NMF(n_components=2, init='random', random_state=42)\nW = model.fit_transform(X)\nH = model.components_\nprint(\"W is:\\n\", W)\nprint(\"H is:\\n\", H)\nprint(\"The Result of Matrix Multiplication of W and H is Same as X:\\n\", np.matmul(W, H))","cb735458":"Image(\"..\/input\/inputimage\/1.JPG\")","621229dc":"Image(\"..\/input\/inputimage\/2.JPG\")","ad4ee689":"Image(\"..\/input\/inputimage\/3.JPG\")","f2a2c685":"model = NMF(n_components=2, init='random', random_state=42)\nuser_vec = model.fit_transform(data_matrix)\nitem_vec = model.components_.T\n\ndef implicit_ALS(ratings, user_vec, item_vec, lambda_val, iteration, typ):                 \n    \n    ctr = 1\n\n    if typ == 'user':\n        while ctr <= iteration:\n            YTY = item_vec.T.dot(item_vec)\n            lambdaI = np.eye(YTY.shape[0]) * lambda_val\n\n            for u in range(user_vec.shape[0]):\n                user_vec[u, :] = solve((YTY + lambdaI), \n                                        ratings[u, :].dot(item_vec))\n            ctr += 1\n\n        return user_vec\n    \n    if typ == 'item':\n        while ctr <= iteration:\n            XTX = user_vec.T.dot(user_vec)\n            lambdaI = np.eye(XTX.shape[0]) * lambda_val\n            \n            for i in range(item_vec.shape[0]):\n                item_vec[i, :] = solve((XTX + lambdaI), \n                                        ratings[:, i].T.dot(user_vec))\n            ctr += 1\n        return item_vec\n        \n    \nuser_vec = implicit_ALS(data_matrix, user_vec, item_vec, lambda_val=0.2,\n                        iteration=20, typ='user')\nitem_vec = implicit_ALS(data_matrix, user_vec, item_vec, lambda_val=0.2,\n                        iteration=20, typ='item')\n\ndef predict_all():\n        \"\"\" Predict ratings for every user and item. \"\"\"\n        predictions = np.zeros((user_vec.shape[0], \n                                item_vec.shape[0]))\n        for u in range(user_vec.shape[0]):\n            for i in range(item_vec.shape[0]):\n                predictions[u, i] = predict(u, i)\n                \n        return predictions\ndef predict(u, i):\n    \"\"\" Single user and item prediction. \"\"\"\n    return user_vec[u, :].dot(item_vec[i, :].T)\n\npredict = predict_all()\n\n\ndata_matrix_pred_ALS = np.where(data_matrix == 0, predict, data_matrix)\n\nfor i in range(data_matrix_pred_ALS.shape[0]):\n    for j in range(data_matrix_pred_ALS.shape[1]):\n        data_matrix_pred_ALS[i, j] = round(data_matrix_pred_ALS[i, j])\n\n#For measuring the performance we have to use the predicted matrix\nX_pred = np.zeros((nb_users, nb_movies))\nfor i in range(predict.shape[0]):\n    for j in range(predict.shape[1]):\n        X_pred[i, j] = round(predict[i, j])","586000b6":"print(\"Original Rating Matrix: \\n\", data_matrix)","2abfd956":"print(\"Rating Matrix After Applying ALS: \\n\", data_matrix_pred_ALS)","4e221afd":"ind_matrix = np.zeros((nb_users, nb_movies))\nind_matrix = np.where(data_matrix != 0, 1, 0)\n\nx = np.multiply(X_pred, ind_matrix)\nRMSE_ALS = np.sqrt(np.mean((x - data_matrix)**2))\nprint(\"RMSE of ALS Model is %f.\" %RMSE_ALS)","2daadc7d":"import torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data\nfrom torch.autograd import Variable","3ce8ef23":"data_matrix_torch = torch.FloatTensor(data_matrix)","5eaac41d":"class SAE(nn.Module):\n    def __init__(self, ):\n        super(SAE, self).__init__()\n        self.fc1 = nn.Linear(nb_movies, 20)\n        self.fc2 = nn.Linear(20, 10)\n        self.fc3 = nn.Linear(10, 20)\n        self.fc4 = nn.Linear(20, nb_movies)\n        self.activation = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.activation(self.fc1(x))\n        x = self.activation(self.fc2(x))\n        x = self.activation(self.fc3(x))\n        x = self.fc4(x)\n        return x\n    \n    def prediction(self, x):\n        pred = self.forward(x)\n        return pred.detach().numpy()\n    \nsae = SAE()\ncriterion = nn.MSELoss()\noptimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)","c1df12f4":"nb_epoch = 200\nfor epoch in range(1, nb_epoch + 1):\n    train_loss = 0\n    s = 0\n    for id_user in range(nb_users):\n        input = Variable(data_matrix_torch[id_user]).unsqueeze(0)\n        target = input.clone()\n        if torch.sum(target.data > 0) > 0:\n            output = sae(input)\n            target.require_grad = False\n            output[target == 0] = 0\n            loss = criterion(output, target)\n            mean_corrector = nb_movies\/float(torch.sum(target.data > 0) + 1e-10)\n            loss.backward()\n            train_loss += np.sqrt(loss.data * mean_corrector)\n            s +=1\n            optimizer.step()\n    print('epoch: ' + str(epoch) + ' loss: ' + str((train_loss\/s).item()))","ea9a2f85":"predict_SAE = np.zeros((nb_users, nb_movies))\nfor id_user in range(nb_users):\n    input = Variable(data_matrix_torch[id_user]).unsqueeze(0)\n    predict_SAE[id_user] = sae.prediction(input)\n\n#We will use predicted value if the entry is 0 in original rating matrix\ndata_matrix_pred_SAE = np.where(data_matrix == 0, predict_SAE, data_matrix)\n\nfor i in range(data_matrix_pred_SAE.shape[0]):\n    for j in range(data_matrix_pred_SAE.shape[1]):\n        data_matrix_pred_SAE[i, j] = round(data_matrix_pred_SAE[i, j])\n\n#For measuring the performance we have to use the predicted matrix\nX_pred = np.zeros((nb_users, nb_movies))\nfor i in range(predict_SAE.shape[0]):\n    for j in range(predict_SAE.shape[1]):\n        X_pred[i, j] = round(predict_SAE[i, j])","68ff51a4":"print(\"Original Rating Matrix: \\n\", data_matrix)","95c77895":"print(\"Rating Matrix after Applying Stacked Auto-Encoder: \\n\", data_matrix_pred_SAE)","7eba1c7a":"ind_matrix = np.zeros((nb_users, nb_movies))\nind_matrix = np.where(data_matrix != 0, 1, 0)\n\nx = np.multiply(X_pred, ind_matrix)\nRMSE_SAE = np.sqrt(np.mean((x - data_matrix)**2))\nprint(\"RMSE of SAE Model is %f.\" %RMSE_SAE)","f702a18d":"m_cols = ['Movie_ID', 'Title', 'Release_Date', 'Video_Release_Date', 'IMDB_URL']\nmovies = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.item', sep='|',\n                     names=m_cols, usecols=range(5),encoding='latin-1')\nmovies.head(10)","59740767":"movie_id = movies[movies['Title'] == 'Richard III (1995)']['Movie_ID'].values.item()\nprint(\"Movie ID is:\", movie_id)","8b014bc8":"#Create an indicator matrix to ensure the movie was not rated previously\nind_matrix = np.zeros((nb_users, nb_movies))\nind_matrix = np.where(data_matrix == 0, 1, 0)\n\n#Multiply predicted rating matrix with this indicator matrix to consider only\n#the predicted ones\npred = np.multiply(data_matrix_pred_SAE, ind_matrix)\npred = pred[:, 9]\npred_df = pd.DataFrame(pred)\npred_df.columns = ['Rating']\npred_df = pred_df[pred_df['Rating'] >= 4]\npred_df = pred_df.head(5)\npred_df","a81259be":"user_id = [3, 4, 5, 6, 8]\nusers_recommend = users[users['User_ID'].isin(user_id)]\nusers_recommend","f1c65320":"users.tail()","0e2366f4":"#Create an indicator matrix to ensure the movie was not rated previously\nind_matrix = np.zeros((nb_users, nb_movies))\nind_matrix = np.where(data_matrix == 0, 1, 0)\n\n#Multiply predicted rating matrix with this indicator matrix to consider\n#only the predicted ones\npred = np.multiply(data_matrix_pred_SAE, ind_matrix)\npred = pred[939, :]\npred_df = pd.DataFrame(pred)\npred_df.columns = ['Rating']\npred_df = pred_df[pred_df['Rating'] >= 4]\npred_df = pred_df.head(5)\npred_df","10d878ee":"movie_id = [1, 2, 10, 11, 15]\nmovie_recommend = movies[movies['Movie_ID'].isin(movie_id)]\nmovie_recommend","675cf896":"#### Auto Encoder is a Unsupervised learning technique that creates a representation of actual data in a reduced dimension. We will use one Auto Encoder on top of another one to make it Stacked.","07b22672":"# Auto-Encoder","81e3db5b":"# Altenating Least Squares using Non-Negative Matrix Factorization","ea2aa1eb":"#### So there are 943 users.","843fc9da":"#### Import the libraries.","d8a1eb91":"#### Creating the architecture of Stacked AutoEncoder model.","918c9c0f":"#### So we can see the top 5 movies which will possibly be rated by User_ID 940 as 4 have their Movie_ID as 1, 2, 10, 11, 15 (add 1 to the index).","963bb567":"#### Whenever we browse any website to search any product we experience that the website offers some other products which we might not have explicitly searched...this is what \"Recommender System\" does. It tries to identify patterns in the searches and recommends based on that.\n\n#### Here I will show you 3 approaches to design a simple recommender system -\n\n### ***1. Gaussian Mixture Model and Expectation-Maximization Algorithm***\n\n### ***2. Altenating Least Squares using Non-Negative Matrix Factorization***\n\n### ***3. Stacked Auto-Encoder***","5eeeda6b":"#### Measure the performance: we will consider the entries from the original ratings matrix and see their predicted values and then compute RMSE.","7f3d8a9a":"#### Now we will implement ALS.","af802ce2":"#### The minimization of the Loss function yields below matrix form for the user_vector and item_vector.","f972385a":"#### Let's select User_ID as 940; so in the output predicted rating matrix the row index will be 939.","48214fb5":"#### Since we have reformatted the data into our desired format now we will proceed towards fitting the data into different models.","92ac6e98":"#### Measure the performance: we will consider the entries from the original ratings matrix and see their predicted values and then compute RMSE.","aed2383e":"#### Converting the data into Torch tensors.","21962131":"#### A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians. Bayesian Information Criterion (BIC) is used as a performance measure (lower the better) in Gaussian Mixture model.\n\n#### In the context of each user, we must sample a user type and then the rating profile from the Gaussian distribution associated with the type. We will use the Expectation Maximization (EM) algorithm to estimate such a mixture from a partially observed rating matrix. The EM algorithm proceeds by iteratively assigning (softly) users to types (E-step) and subsequently re-estimating the Gaussians associated with each type (M-step). Once we have the mixture, we can use it to predict values for all the missing entries in the data matrix.\n\n#### The Scikit-Learn Gaussian Mixture model uses EM algorithm.","c238030b":"#### Load the data.","426699c3":"#### Measure the performance: we will consider the entries from the original ratings matrix and see their predicted values and then compute RMSE.","b507ed7c":"#### But before we dig into the details of ALS we want to understand what is Non Negative Matrix Factorization(NMF). NMF is finding out two non-negative matrices (W, H) whose product approximates the non- negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction.","2ab22dbf":"#### So the GMM model assigned 943 users into 2 components and since there is 1682 movies (features); each feature has its own mean and variance for each component.\n\n#### If we use \"Predict\" method we will get 943 outputs that specifies the component label but since we have to predict what will be the rating of 1682 movies for each user we have to implement below logic.","6e84967f":"#### Predict using this Stacked Auto-Encoder.","71bf05bb":"#### I tried to showcase how a tiny Recommender System can be created using different techniques and briefly explained their basics.\n\n#### Please upvote if you like it!","af2f58f0":"#### The Loss function of ALS is as below where X_u is the user_vector, y_i is the item_vector which are created applying NMF on the rating matrix and r_ui is basically the rating. There are Regularization terms as well.\n\n#### One important point here is that to minimize this Loss function only user_id and item_id combinations for which there is a rating is considered (u,i belongs to S which is the non-zero rating matrix).","f54bfc40":"#### We will now select a particular user and recommend movies to that user.","3d58f945":"#### Since the RMSE of the Stacked Auto-Encoder model is comparatively less than GMM and ALS based models and we can see that the output rating matrix is completely filled up we will use this matrix to show how it can be used to *recommend*.\n\n#### We will first select a particular movie from the 'movies' dataset and using its 'Movie_ID' we will select the top 5 entries from the output rating matrix for which the rating is greater than\/equal to 4 and the movie was not already rated because if it is rated then the user has watched the movie and there is no point in recommending same movie to that user. We can use the index of these top 5 entries to fetch the user information.\n\n#### Alternatively, we will select a particular user and select top 5 movies which are not previously rated by the user and our recommender system predicted that the predicted rating will be greater than\/equal to 4 for them.","82f91116":"#### Train the Stacked Auto Encoder.","32bd9ff2":"#### Lets's select the movie as 'Richard III (1995)' and see how we can recommend it.","f4c6fa72":"#### Since the Movie_ID is 10, in our predicted rating matrix the columns index will be 9.","565326c5":"#### The entire computation will be done in log-domain to avoid Numerical instability; so the log of Posterior probability will be as below.","4024ce58":"# Gaussian Mixture Model and Expectation-Maximization Algorithm","e69d573e":"#### So there are 1682 unique movies that were rated.","d6294b3c":"#### Import the PyTorch modules.","62e7c4eb":"#### So we can see the top 5 users who will possibly rate 'Richard III (1995)' as 4 have their User_ID as 3, 4, 5, 6, 8 (add 1 to the index).","c44d69e8":"#### There are 100,000 ratings of those 943 users.","3e116077":"#### Build a GMM model using covariance_type='full' so that the covariance matrix become a square matrix and n_components=2."}}