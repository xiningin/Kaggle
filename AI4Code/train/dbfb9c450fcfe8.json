{"cell_type":{"44bbdef9":"code","b049e472":"code","bb00afaf":"code","ca6be0ba":"code","d128de3e":"code","60ef30eb":"code","f2796c13":"code","ab849c71":"code","9b15313a":"code","92aeb530":"code","684cf515":"code","e9bbf0d5":"code","90830d1a":"code","b8ad7983":"code","939bd442":"code","0c502f71":"code","01f1ca03":"code","9581800c":"code","d216d42c":"code","40c90773":"code","2de31295":"code","6bbac91d":"code","f99a9dc1":"code","28b67d28":"code","6e7f973c":"code","328274af":"code","b01f85b8":"code","04aaeb21":"code","443dea1f":"code","c392f1e2":"code","f4f29ded":"code","88c6cc4b":"code","22759c33":"code","cc951245":"code","670c099c":"code","4059789d":"code","7f088093":"code","3c91f450":"code","1349a900":"code","edbf04ed":"markdown","fc3f78a1":"markdown","c501cfd2":"markdown","ada0f862":"markdown","7e334bd4":"markdown","2b0736a3":"markdown","70d62a58":"markdown","babef629":"markdown","c8b771d5":"markdown","953bf77f":"markdown","f6ad9550":"markdown","19b0e7e1":"markdown","8c1e7123":"markdown","80eb86f5":"markdown","0bbcdadc":"markdown","de2267aa":"markdown","e3354775":"markdown","6eda8448":"markdown","95447c1f":"markdown","cc99e84b":"markdown","e70735ac":"markdown","cbc63749":"markdown","8b5ea840":"markdown","fb466433":"markdown","4e7a5bf7":"markdown","291a6400":"markdown","cd1671a7":"markdown","b7c0689b":"markdown","bbfc7edf":"markdown","8bd761a1":"markdown","b29da0c6":"markdown","388bd25a":"markdown","6d19f640":"markdown","c902a14e":"markdown","08677209":"markdown"},"source":{"44bbdef9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\n\nfrom sklearn import metrics\n\nfrom itertools import combinations\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt","b049e472":"df = pd.read_csv(\"\/kaggle\/input\/claim_history.csv\")\ndf.head()","bb00afaf":"from sklearn.model_selection import train_test_split\nfeatures = df[[\"CAR_TYPE\",\"OCCUPATION\",\"EDUCATION\"]]\nfeatures.head()","ca6be0ba":"#Split for training and testing data\nfeatures_train,features_test,labels_train, labels_test = train_test_split(features,df[\"CAR_USE\"],test_size = 0.3, random_state=27513,stratify = df[\"CAR_USE\"])","d128de3e":"cross_Table_Train = pd.crosstab(labels_train,columns =  [\"Count\"],margins=True,dropna=True)\ncross_Table_Train[\"Proportions\"] = (cross_Table_Train[\"Count\"]\/len(labels_train))*100\ncross_Table_Train","60ef30eb":"cross_Table_test = pd.crosstab(labels_test,columns =  [\"Count\"],margins=True,dropna=True)\ncross_Table_test[\"Proportions\"] = (cross_Table_test[\"Count\"]\/len(labels_test))*100\ncross_Table_test","f2796c13":"c=0\nprob_train = len(features_train)\/len(df[\"CAR_USE\"]) #Probability of the observation in Training set\nfor i in df[\"CAR_USE\"]:\n    if i ==\"Commercial\":\n        c+=1        #Probability of the observation being Commercial\n(prob_train*c\/len(df[\"CAR_USE\"]))\/(c\/len(df[\"CAR_USE\"]))   #Probability that observation is in the Training partition given that CAR_USE = Commercial\nprint(\"The probability that an observation is in the Training partition given that CAR_USE = Commercial is\",(prob_train*c\/10302)\/(c\/10302))","ab849c71":"count=0\nprob_test = len(features_test)\/len(df[\"CAR_USE\"]) #Probability of the observation in Testing set\nfor i in df[\"CAR_USE\"]:\n    if i ==\"Private\":\n        count+=1        #Probability of the observation being Private\n(prob_test*count\/10302)\/(count\/10302)   #Probability that observation is in the Testing partition given that CAR_USE = Private\n\nprint(\"The probability that an observation is in the Testing partition given that CAR_USE = Private is\",(prob_test*count\/10302)\/(count\/10302))","9b15313a":"features_train[\"Labels\"] = labels_train","92aeb530":"#Entropy of Root Node\ncnt = 0\nfor i in df[\"CAR_USE\"]:\n    if i == \"Commercial\":\n        cnt+=1\nproba_commercial = cnt\/len(df[\"CAR_USE\"])\n\nproba_private = (len(df[\"CAR_USE\"])-cnt)\/len(df[\"CAR_USE\"])\n\nans = -((proba_commercial * np.log2(proba_commercial) + proba_private * np.log2(proba_private)))\nprint(\"Entropy for root node is given as\",ans)","684cf515":"#All possible combinations for occupation\noccupation_column = df[\"OCCUPATION\"].unique()\noccupation_combinations = []\nfor i in range(1,math.ceil(len(occupation_column)\/2)):\n    occupation_combinations+=list(combinations(occupation_column,i))","e9bbf0d5":"#All possible combinations for car type\ncar_type_column = df[\"CAR_TYPE\"].unique()\ncar_type_combinations = []\n\nfor i in range(1,math.ceil(len(car_type_column)\/2)+1):\n    x = list(combinations(car_type_column,i))\n    if i == 3:\n        x = x[:10]\n    car_type_combinations.extend(x) ","90830d1a":"#All possible combinations for education\neducation_combinations = [(\"Below High School\",),(\"Below High School\",\"High School\",),(\"Below High School\",\"High School\",\"Bachelors\",),(\"Below High School\",\"High School\",\"Bachelors\",\"Masters\",)]","b8ad7983":"def EntropyIntervalSplit (\n   inData,          # input data frame (predictor in column 0 and target in column 1)\n   split):          # split value\n\n   #print(split)\n   dataTable = inData\n   dataTable['LE_Split'] = False\n   for k in dataTable.index:\n       if dataTable.iloc[:,0][k] in split:\n           dataTable['LE_Split'][k] = True\n   #print(dataTable['LE_Split'])\n   crossTable = pd.crosstab(index = dataTable['LE_Split'], columns = dataTable.iloc[:,1], margins = True, dropna = True)   \n   #print(crossTable)\n\n   nRows = crossTable.shape[0]\n   nColumns = crossTable.shape[1]\n   \n   tableEntropy = 0\n   for iRow in range(nRows-1):\n      rowEntropy = 0\n      for iColumn in range(nColumns):\n         proportion = crossTable.iloc[iRow,iColumn] \/ crossTable.iloc[iRow,(nColumns-1)]\n         if (proportion > 0):\n            rowEntropy -= proportion * np.log2(proportion)\n      #print('Row = ', iRow, 'Entropy =', rowEntropy)\n      #print(' ')\n      tableEntropy += rowEntropy *  crossTable.iloc[iRow,(nColumns-1)]\n   tableEntropy = tableEntropy \/  crossTable.iloc[(nRows-1),(nColumns-1)]\n  \n   return(tableEntropy)","939bd442":"def calculate_min_entropy(df,variable,combinations):\n    inData1 = df[[variable,\"Labels\"]]\n    entropies = []\n    for i in combinations:\n        EV = EntropyIntervalSplit(inData1, list(i))\n        entropies.append((EV,i))\n    return min(entropies)","0c502f71":"entropy_occupation = calculate_min_entropy(features_train,\"OCCUPATION\",occupation_combinations)\nentropy_occupation","01f1ca03":"entropy_cartype = calculate_min_entropy(features_train,\"CAR_TYPE\",car_type_combinations)\nentropy_cartype","9581800c":"entropy_education = calculate_min_entropy(features_train,\"EDUCATION\",education_combinations)\nentropy_education","d216d42c":"df_1_left = features_train[(features_train[\"OCCUPATION\"] == \"Blue Collar\") | (features_train[\"OCCUPATION\"] == \"Unknown\") | (features_train[\"OCCUPATION\"] == \"Student\")]\ndf_1_right =  features_train[(features_train[\"OCCUPATION\"] != \"Blue Collar\") & (features_train[\"OCCUPATION\"] != \"Unknown\") & (features_train[\"OCCUPATION\"] != \"Student\")]\nlen(df_1_right),len(df_1_left)","40c90773":"left_edu_entropy = calculate_min_entropy(df_1_left,\"EDUCATION\",education_combinations)\nleft_edu_entropy","2de31295":"left_ct_entropy = calculate_min_entropy(df_1_left,\"CAR_TYPE\",car_type_combinations)\nleft_ct_entropy","6bbac91d":"occupation_column = ['Blue Collar', 'Unknown', 'Student']\noccupation_combinations = []\nfor i in range(1,math.ceil(len(occupation_column)\/2)):\n    occupation_combinations+=list(combinations(occupation_column,i))\nleft_occupation_entropy = calculate_min_entropy(df_1_left,\"OCCUPATION\",occupation_combinations)\noccupation_combinations","f99a9dc1":"occupation_column = ['Professional', 'Manager', 'Clerical', 'Doctor','Lawyer','Home Maker']\noccupation_combinations = []\nfor i in range(1,math.ceil(len(occupation_column)\/2)):\n    occupation_combinations+=list(combinations(occupation_column,i))\nright_occupation_entropy = calculate_min_entropy(df_1_right,\"OCCUPATION\",occupation_combinations)\n\nright_edu_entropy = calculate_min_entropy(df_1_right,\"EDUCATION\",education_combinations)\nright_ct_entropy = calculate_min_entropy(df_1_right,\"CAR_TYPE\",car_type_combinations)\nright_ct_entropy , right_edu_entropy , right_occupation_entropy","28b67d28":"df_2_left_left = df_1_left[(features_train[\"EDUCATION\"] == \"Below High School\")]\ndf_2_left_right = df_1_left[(features_train[\"EDUCATION\"] != \"Below High School\")]","6e7f973c":"cnt = 0\nfor i in df_2_left_left[\"Labels\"]:\n    if i == \"Commercial\":\n        cnt+=1\nproba_commercial = cnt\/len(df_2_left_left[\"Labels\"])\nprint(\"Count of commercial and private is\",cnt,(len(df_2_left_left)-cnt),\"respectively and probability of the event\",proba_commercial)","328274af":"cnt = 0\nfor i in df_2_left_right[\"Labels\"]:\n    if i == \"Commercial\":\n        cnt+=1\nproba_commercial = cnt\/len(df_2_left_right[\"Labels\"])\nprint(\"Count of commercial and private is\",cnt,(len(df_2_left_right)-cnt),\"respectively and probability of the event\",proba_commercial)","b01f85b8":"df_2_right_left = df_1_right[(features_train[\"CAR_TYPE\"] == \"Minivan\") | (features_train[\"CAR_TYPE\"] == \"Sports Car\") | (features_train[\"CAR_TYPE\"] == \"SUV\")]\ndf_2_right_right = df_1_right[(features_train[\"CAR_TYPE\"] != \"Minivan\") & (features_train[\"CAR_TYPE\"] != \"Sports Car\") & (features_train[\"CAR_TYPE\"] != \"SUV\")]","04aaeb21":"cnt = 0\nfor i in df_2_right_left[\"Labels\"]:\n    if i == \"Commercial\":\n        cnt+=1\nproba_commercial = cnt\/len(df_2_right_left[\"Labels\"])\n1-proba_commercial\nprint(\"Count of commercial and private is\",cnt,(len(df_2_right_left)-cnt),\"respectively and probability of the event\",proba_commercial)","443dea1f":"cnt = 0\nfor i in df_2_right_right[\"Labels\"]:\n    if i == \"Commercial\":\n        cnt+=1\nproba_commercial = cnt\/len(df_2_right_right[\"Labels\"])\nproba_commercial\nprint(\"Count of commercial and private is\",cnt,(len(df_2_right_right)-cnt),\"respectively and probability of the event\",proba_commercial)","c392f1e2":"#Thresold probability of the event from training set\ncnt = 0\nfor i in features_train[\"Labels\"]:\n    if i == \"Commercial\":\n        cnt+=1\nthreshold = cnt\/len(features_train[\"Labels\"])\nprint(\"Threshold probability of an event is given as\",threshold)","f4f29ded":"predicted_probability=[]\nocc = [\"Blue Collar\",\"Student\",\"Unknown\"]\nedu = [\"Below High School\",]\ncartype = [\"Minivan\",\"SUV\",\"Sports Car\"]\nfor k in features_test.index:\n    if features_test.iloc[:,1][k] in occ:\n            if features_test.iloc[:,2][k] in edu:\n                predicted_probability.append(0.24647887323943662)  #Leftmost Leaf Node\n            else:\n                predicted_probability.append(0.8504761904761905)   #Right leaf from left subtree\n    else:\n            if features_test.iloc[:,0][k] in cartype:\n                predicted_probability.append(0.006151953245155337)  #Left leaf from right subtree\n            else:\n                predicted_probability.append(0.5464396284829721)   #Rightmost Leaf Node","88c6cc4b":"prediction = []\nfor i in range(0,len(labels_test)):\n    if predicted_probability[i] >= threshold :\n        prediction.append(\"Commercial\")\n    else:\n        prediction.append(\"Private\")","22759c33":"from sklearn.metrics import accuracy_score\nprint(\"Missclassification Rate\",1-accuracy_score(labels_test,prediction))","cc951245":"RASError = 0.0\nfor i in range (0,len(labels_test)):\n    if labels_test.iloc[i] == \"Commercial\":\n        RASError += (1-predicted_probability[i])**2\n    else:\n        RASError += (predicted_probability[i])**2\nRASError = math.sqrt(RASError\/len(labels_test))\nRASError","670c099c":"true_values = 1.0 * np.isin(labels_test, ['Commercial'])\nAUC = metrics.roc_auc_score(true_values, predicted_probability)\nAUC","4059789d":"print(\"Root Average Squared Error\",RASError)\nprint(\"Area Under the curve\",AUC)\nprint(\"Missclassification Rate\",1-accuracy_score(labels_test,prediction))","7f088093":"OneMinusSpecificity, Sensitivity, thresholds = metrics.roc_curve(labels_test, predicted_probability, pos_label = 'Commercial')","3c91f450":"OneMinusSpecificity = np.append([0], OneMinusSpecificity)\nSensitivity = np.append([0], Sensitivity)\n\nOneMinusSpecificity = np.append(OneMinusSpecificity, [1])\nSensitivity = np.append(Sensitivity, [1])","1349a900":"plt.figure(figsize=(6,6))\nplt.plot(OneMinusSpecificity, Sensitivity, marker = 'o',\n         color = 'orange', linestyle = 'solid', linewidth = 2, markersize = 6)\nplt.plot([0, 1], [0, 1], color = 'darkblue', linestyle = '--')\nplt.grid(True)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic Curve\")\nax = plt.gca()\nax.set_aspect('equal')\nplt.show()","edbf04ed":"Entropy for best split in Car Type","fc3f78a1":"Once we have the list of predicted probabilities, we use the threshold to compare and predict the class of the observation. If the predicted probability is greater than or equal to threshold, classify it as a event (Commercial), else non-event (Private). ","c501cfd2":"Give the frequency table of the target variable (CAR_USE) in Testing partition?","ada0f862":"Split the dataframes according to the given split. If the observation belongs to ('Blue Collar', 'Unknown', 'Student'), then it will be in the left dataframe, else it will be in the right dataframe. In this way, we create a parent node where we ask question about the occupation and split our data into two child nodes. Here, we are done with level 1.","7e334bd4":"From the numbers above, it can be observed that the predcitor that can be used at level 1 is\n### Occupation : Entropy Value 0.71488\nIt has the minimum entropy for the split ('Blue Collar', 'Unknown', 'Student') and the entropy value is 0.71488","2b0736a3":"### Entropy\nThe goal of building a Decision Tree is to subdivide the data by predictors in such a way that the target values for the observations in the terminal nodes are as similar as possible. Meaning, the terminal nodes must have maximum number of observations belonging to the same class. \nOne criteria to do that is to calculate the entropy value.\n\n#### *Entropy*\n  Entropy is the measure of the impurity of the split. Higher the entropy value, more impure is the split. Hence, the best split is the one which has lowest entropy value. The entropy values lie between 0 to 1. The entropy calculations are discussed over the link [here.][1]\n\n[1]: https:\/\/www.saedsayad.com\/decision_tree.htm","70d62a58":"### Train Test Split\nDivide our data into training and testing sets. Features are the independent varibles (predictors). Labels are the dependent variables (Target). ","babef629":"### Root Average Squared Error","c8b771d5":"## Machine Learning : Decision Tree for Classification\n\nIn this notebook we will implement a decision tree from scratch without using any inbuilt functions. The main goal here is to get the understanding of how decision tree calculations are done.","953bf77f":"### Possible combinations for each predictor [Occupation, Car_type, Education]","f6ad9550":"### Conclusion \nWe build a decision tree for classifying the data into two different classes based on three predictors. The decision tree has two levels, four leaf nodes at which the decision is made. We used entropy values to find the best split at each node. To test how the model is performing, we used various evaluation metrics like misclassification rate and area under the curve. By looking at those numbers, we can say that the model is performing decently.  ","19b0e7e1":"Now, we have reached the second level of the tree. We have four leaf nodes into which our data is splitted. Let \"Commercial\" be the event and \"Private\" be non-event. From the leaf node data, we will find the probability of event at that particular node. We do this by taking the proportion of count of \"Commercial\" observations to the total observations. We do this for each leaf node.\n\nOnce we have these probability, we will use it to make predictions.\n","8c1e7123":"### Entropy of Root node\nHere, we calculate the entropy of root node. To calculate the entropy of root node, we calculate the probability for Commercial and Private Car_Use in the given dataframe. By substituting these values in the entropy formula, we find the entropy of root node.   ","80eb86f5":"### Conditional Probability\nWhat is the probability that an observation is in the Training partition given that CAR_USE = Commercial?\n#### P ( Train | Car Use = Commercial )\n","0bbcdadc":"Few metrics to see how the model is behaving","de2267aa":"### Entropy Calculations for left split","e3354775":"### Area under the curve","6eda8448":"### Misclassification Rate\nTo see how well the model is behaving, calculate the misclassification rate. Misclassification rate is defined as percentage of the observations from the dataset misclassified. Lower this rate, more accurate the model is. We have our predictions for the observations in testing data and their original classes too. Use it to find the misclassification rate.","95447c1f":"### Decision Rules\nNow that we have all the numbers, we define decision rules by which the model will predict the class of an observation. We use the probability of each node and thresold probability to define the set of rules. First of all, get the list of predicted probabilty for each observation that we wish to classify. We use our testing dataset to do so.","cc99e84b":"It can be observed that the data is distributed in equal proportions in both training and testing sets. This stratification will help in testing the model that we will generate as it will have equal proportions of Commercial and Private observations.","e70735ac":"Entropy for best split in Occupation","cbc63749":"### Entropy calculations for right split","8b5ea840":"### Function to find minimum entropy split in each predictor\nIn the above function, we calculated the entropy for just one single split. Now the function below takes all such possible splits, calculate entropy of ech split and return the minimum entropy split. By this, we get the best split for the given predictor. We use this function for each predictor to get best entropies of each predictor. ","fb466433":"The best split with minimum entropy that we get is on Car type for the left node at level 1\n(0.7689481386570244, ('Minivan', 'SUV', 'Sports Car')).","4e7a5bf7":"### Entropy Calculation for a single split in the given predictor\nHere we define a function which returns the entropy of the given split. It takes the predictor and corresponding target values as the first input. The second input is the split on which we need to calculate the entropy on. For example we take occupation as the predictor and the split for it will be something like ('Blue Collar', 'Manager', 'Student'), i.e we will split our observations based on this tuple. Then after splitting, we calculate frequency table to get the proportion of target variables in each split. Using this, we can calculate the entropy which will be returned by this function.","291a6400":"### Frequency Table\nFrequency tables give the proportion of Commercial and Private car use in the given data. It will be required for entropy calculations discussed below. \nGive the frequency table of the target variable (CAR_USE) in training partition.","cd1671a7":"### Import numpy to speed up the mathematical calculations and pandas to manipulate the data using dataframes.","b7c0689b":"What is the probability that an observation is in the Test partition given that CAR_USE = Private?\n\n#### P ( Test | Car Use = Private )","bbfc7edf":"## Problem Statement\nBuild a decision tree model to predict the usage of car. The data is the claim_history.csv which has 10,302 observations. The analysis specifications are:\n\n### Target Variable  \n#### CAR_USE\nThe usage of a car. This variable has two categories which are Commercial and Private. The Commercial category is the Event value.\n\n### Nominal Predictor \n#### CAR_TYPE. \nThe type of a car. This variable has six categories which are Minivan, Panel Truck, Pickup, SUV, Sports Car, and Van.\n#### OCCUPATION. \nThe occupation of the car owner. This variable has nine categories which are Blue Collar, Clerical, Doctor, Home Maker, Lawyer, Manager, Professional, Student, and Unknown.\n\n### Ordinal Predictor\n#### EDUCATION. \nThe education level of the car owner. This variable has five ordered categories which are Below High School < High School < Bachelors < Masters < Doctors. \n\n### Analysis Specifications\n\nPartition. Specify the target variable as the stratum variable. Use stratified simple random sampling to put 70% of the records into the Training partition, and the remaining 30% of the records into the Test partition. The random state is 27513.\n\n### Decision Tree. \nThe maximum number of branches is two. The maximum depth is two. The split criterion is the *Entropy* metric.","8bd761a1":"As we have the probabilites of event for each node ready, we calculate the threshold against which it can be compared. For this example, we will calculate threshold by taking the proportion of target Event value in the training dataset.","b29da0c6":"### Finding best splits\n\n### Steps\n1.Start with the first predictor, select a value to split data in the node into two partitions.\n   Interval or Ordinal Predictors: {x: x < a} and {x: x \uf0b3 a} where a is a value\n   Nominal Predictor: {x: x belongs to A} and {x: x belongs to A} where A is a subset of values\n   \n2.Apply the Goodness of Split Criterion, calculate the impurity of the partitions, and evaluate the reduction of impurity from the parent node.\n\n3.Select the split partition that results in the largest reduction of impurity and labels it the \u201cbest\u201d split for the predictor.\n\n4.Repeat steps 1 to 3 for each remaining predictors.  Go to step 5 when \u201cbest\u201d splits are found for all predictors.\n\n5.Ranks all of the \u201cbest\u201d split of each predictor according to the reduction in impurity achieved by each split.\n\n6.Select the predictor and its split partition that most reduced impurity of the parent node.\n\n7.Assign observations to one of the two child nodes.\n\n8.Determine whether the child nodes are terminal based on the Stopping Criteria (Here it is when we reach at level 2)\n\n9.Repeat steps 1 to 8 for each non-terminal child nodes.\n\nTo find the best splits, we will have to take each predictor in consideration. For each predictor, we find all possible combinations that the predictor has. By doing this, we can frame proper question to be asked at each split. So, we will do this for the first level. We will take all three predictors in consideration and find the best split amongst all three. Then from those three best splits, we will select one split based on the minimum entropy value. This will be question we will ask at level 1.\n\nFor example, we have a predictor which has values A,B,C,D. The possible combinations for this predictor would be (A)(B)(C)(D)(A,B)(A,C)(A,D)(B,C)(B,D)(C,D). These tuples that we see are the possible questions that we can frame while splitting the data. It can be like, whether the predictor belongs to class A. If yes, the it will be splitted at the left node, or it will be belonging to B,C or D, which will end up at right node. According to this split, the entropy value will be calculated based upon the target variable proportions. If the entropy value is less, it can be considered as the best split.\n\nWe will try the above steps for each of the predictors and find the best split.","388bd25a":"### Load Dataset","6d19f640":"Entropy for best split in EDUCATION","c902a14e":"### Receiver Operating Characteristic","08677209":"Now we repeat the same process of entropy calculation on each sub-dataframe we created. We will find best split at each node again and get done with level 2."}}