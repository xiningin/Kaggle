{"cell_type":{"992efc62":"code","c1afb7eb":"code","7f950771":"code","3a623367":"code","828f559f":"code","a3a905a6":"code","85d72f24":"code","3f451471":"code","eee5c634":"code","25f888fe":"code","a60cf12c":"code","5c55ff3a":"code","2c068c16":"code","28d2a266":"code","763f72f8":"code","a1f7c256":"code","341b853e":"markdown","bcc3767b":"markdown","91835d3a":"markdown","c81b9b52":"markdown","edaa05c6":"markdown","36d6487f":"markdown","5d320af1":"markdown","06352cf9":"markdown","098609ab":"markdown","9bbb95c2":"markdown","b19c9bd2":"markdown","0e4ed447":"markdown","adae42c2":"markdown"},"source":{"992efc62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c1afb7eb":"# load data set\nx = np.load('..\/input\/Sign-language-digits-dataset\/X.npy')\ny = np.load('..\/input\/Sign-language-digits-dataset\/Y.npy')","7f950771":"img_size = 64\nplt.subplot(1,2,1)\nplt.imshow(x[700].reshape(img_size,img_size))\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.imshow(x[1500].reshape(img_size,img_size))\nplt.axis('off')","3a623367":"# As you can see, y (labels) are already one hot encoded\nprint(y.max())\nprint(y.min())\nprint(y.shape)\n\n# And x (features) are already scaled between 0 and 1\nprint(x.max())\nprint(x.min())\nprint(x.shape)","828f559f":"# Now,lets create x_train, y_train, x_test, y_test arrays\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n#reshape\nx_train = x_train.reshape(-1,64,64,1)\nx_test = x_test.reshape(-1,64,64,1)\n#print x_train and y_train shape\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","a3a905a6":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential # to create a cnn model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 8, kernel_size = (5,5), padding = 'Same', activation = 'relu', input_shape = (64,64,1)))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 16, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(Dropout(0.25))\n\n\n# fully connected\nmodel.add(Flatten())\n\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dense(10, activation = 'softmax'))","85d72f24":"model.summary()","3f451471":"# Define the optimizer\noptimizer = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)","eee5c634":"# Compile the model\nmodel.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","25f888fe":"history = model.fit(x_train,y_train,epochs=100,validation_data=(x_test,y_test))","a60cf12c":"plt.figure(figsize=(24,8))\n\nplt.subplot(1,2,1)\nplt.plot(history.history[\"val_acc\"], label=\"validation_accuracy\", c=\"red\", linewidth=4)\nplt.plot(history.history[\"acc\"], label=\"training_accuracy\", c=\"green\", linewidth=4)\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1,2,2)\nplt.plot(history.history[\"val_loss\"], label=\"validation_loss\", c=\"red\", linewidth=4)\nplt.plot(history.history[\"loss\"], label=\"training_loss\", c=\"green\", linewidth=4)\nplt.legend()\nplt.grid(True)\n\nplt.suptitle(\"ACC \/ LOSS\",fontsize=18)\n\nplt.show()\n","5c55ff3a":"print('Train accuracy of the model: ',history.history['acc'][-1])","2c068c16":"print('Train loss of the model: ',history.history['loss'][-1])","28d2a266":"print('Validation accuracy of the model: ',history.history['val_acc'][-1])","763f72f8":"print('Validation loss of the model: ',history.history['val_loss'][-1])","a1f7c256":"# confusion matrix\nimport seaborn as sns\n# Predict the values from the validation dataset\nY_pred = model.predict(x_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"BuPu\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","341b853e":"<a id=\"3\"><\/a> <br>\n## Train-Test Split Data","bcc3767b":"<a id=\"10\"><\/a>\n# Conclusion\n* If you like it, please upvote.\n* If you have any question, I will be appreciate to hear it.","91835d3a":"<a id=\"5\"><\/a> <br>\n### Create Model\n","c81b9b52":"<a id=\"8\"><\/a>\n### Fit the Model","edaa05c6":"# Orhan SERTKAYA","36d6487f":"<a id=\"6\"><\/a>\n### Define Optimizer   \n* Adam optimizer: Change the learning rate","5d320af1":"<a id=\"1\"><\/a> <br>\n# INTRODUCTION\n* In this kernel, we will be working on Sign Language Digits Dataset (Implementing with Keras).","06352cf9":"Content:\n* [Introduction](#1):\n* [Loading the Data Set](#2):\n* [Train-Test Split Data](#3):\n* [Convolutional Neural Network(Implementing with Keras)](#4):\n* [Create Model](#5):\n* [Define Optimizer](#6):\n* [Compile Model](#7):\n* [Fit the Model](#8):\n* [Evaluate the model](#9):\n* [Conclusion](#10):","098609ab":"<a id=\"7\"><\/a>\n### Compile Model\n* categorical crossentropy\n* We make binary cross entropy at previous parts and in machine learning tutorial\n* At this time we use categorical crossentropy. That means that we have multi class.\n* <a href=\"https:\/\/ibb.co\/jm1bpp\"><img src=\"https:\/\/preview.ibb.co\/nN3ZaU\/cce.jpg\" alt=\"cce\" border=\"0\"><\/a>","9bbb95c2":"<a id=\"4\"><\/a> <br>\n## Implementing with Keras","b19c9bd2":"# Convolutional Neural Networks (CNNs \/ ConvNets)","0e4ed447":"<a id=\"9\"><\/a>\n## Evaluate the model\n* Validation and Loss visualization\n* Confusion matrix","adae42c2":"<a id=\"2\"><\/a> <br>\n## Loading the Data Set\n* In this part we load and visualize the data."}}