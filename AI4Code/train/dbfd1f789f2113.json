{"cell_type":{"225c2700":"code","200b691f":"code","2effa492":"code","2824da50":"code","2d68b691":"code","68fd26b0":"code","f4b17e1b":"code","7542a1ee":"code","adbb5825":"code","258bd057":"code","adf2514d":"code","c3802957":"code","0fd87e53":"code","3268e0ef":"code","990b7b44":"code","bab82bd2":"code","bc238a98":"code","afc85520":"code","39c01e1d":"code","966f25ae":"code","bd2f6d96":"code","a4376fb5":"code","658691d9":"code","c6762aa5":"code","032ef02e":"code","609a71d2":"code","22557ed7":"code","6b09e044":"code","b913c394":"code","afa2e9cb":"code","756c8a08":"code","ea3d2dd8":"code","91a36742":"code","b8f407d4":"code","da534af5":"markdown","3b2a801d":"markdown","8508fc11":"markdown"},"source":{"225c2700":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","200b691f":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Normalizer\nfrom scipy.sparse import hstack\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom prettytable import PrettyTable \n\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nimport pickle\nfrom tqdm import tqdm\nimport os\nfrom collections import Counter","2effa492":"df = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\",encoding='latin1')\ndf.iloc[50:55]","2824da50":"top_loc = df['Location'].value_counts()\nlen(top_loc)","2d68b691":"clas = df['Sentiment'].value_counts()\nclas","68fd26b0":"#https:\/\/www.kaggle.com\/datatattle\/covid-19-tweets-eda-viz?scriptVersionId=43030869&cellId=57\n\n#Remove Urls and HTML links\ndef remove_urls(text):\n    url_remove = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_remove.sub(r'', text)\ndf['preprocessed_tweets']=df['OriginalTweet'].apply(lambda x:remove_urls(x))\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ndf['OriginalTweet']=df['preprocessed_tweets'].apply(lambda x:remove_html(x))","f4b17e1b":"# Lower casing\ndef lower(text):\n    low_text= text.lower()\n    return low_text\ndf['preprocessed_tweets']=df['OriginalTweet'].apply(lambda x:lower(x))\n\n\n# Number removal\ndef remove_num(text):\n    remove= re.sub(r'\\d+', '', text)\n    return remove\ndf['OriginalTweet']=df['preprocessed_tweets'].apply(lambda x:remove_num(x))","7542a1ee":"#Remove stopwords & Punctuations\nfrom nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\n\ndef punct_remove(text):\n    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n    return punct\ndf['preprocessed_tweets']=df['OriginalTweet'].apply(lambda x:punct_remove(x))\n\n\n\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndf['OriginalTweet']=df['preprocessed_tweets'].apply(lambda x:remove_stopwords(x))","adbb5825":"#Remove mentions and hashtags\ndef remove_mention(x):\n    text=re.sub(r'@\\w+','',x)\n    return text\ndf['preprocessed_tweets']=df['OriginalTweet'].apply(lambda x:remove_mention(x))\ndef remove_hash(x):\n    text=re.sub(r'#\\w+','',x)\n    return text\ndf['OriginalTweet']=df['preprocessed_tweets'].apply(lambda x:remove_hash(x))","258bd057":"#Remove extra white space left while removing stuff\ndef remove_space(text):\n    space_remove = re.sub(r\"\\s+\",\" \",text).strip()\n    return space_remove\ndf['preprocessed_tweets']=df['OriginalTweet'].apply(lambda x:remove_space(x))\n\ndf = df.drop(columns=['preprocessed_tweets'])","adf2514d":"df.head()","c3802957":"print(df['Location'].isnull().values.any())\nprint(\"number of nan values\",df['Location'].isnull().values.sum())","0fd87e53":"#https:\/\/stackoverflow.com\/questions\/60701329\/different-ways-to-pre-process-date-in-machine-learning-using-python\nimport datetime\npre_tweetat = []\norigin = datetime.datetime(2020,3,16)\nfor i in tqdm(df[\"TweetAt\"]):\n    days = (datetime.datetime.strptime(i, '%d-%m-%Y') - origin).days\n    pre_tweetat.append(days)\ndf[\"pre_tweetat\"] = pre_tweetat\n","3268e0ef":"df.tail(5)","990b7b44":"sentiment_pre = []\nfor i in tqdm(df[\"Sentiment\"]):\n    if i == 'Extremely Positive':\n        sentiment_pre.append(1)\n    elif i == 'Positive':\n        sentiment_pre.append(1)\n    elif i == 'Neutral':\n        sentiment_pre.append('N')\n    elif i == 'Negative':\n        sentiment_pre.append(0)\n    elif i == 'Extremely Negative':\n        sentiment_pre.append(0)    \ndf[\"sentiment_pre\"] = sentiment_pre\n\ndf = df[df.sentiment_pre != 'N']\ndf.sentiment_pre.value_counts()","bab82bd2":"df = df.drop([\"ScreenName\"],axis=1)\ndf = df.drop([\"UserName\"],axis=1)\ndf = df.drop([\"TweetAt\"],axis=1)\ndf = df.drop([\"Sentiment\"],axis=1)\ndf = df.drop([\"Location\"],axis=1)\n","bc238a98":"df","afc85520":"Y=df['sentiment_pre'].values\nY=Y.astype('int')                 #https:\/\/stackoverflow.com\/a\/45347800\nX=df.drop([\"sentiment_pre\"],axis=1)\nlen(Y)","39c01e1d":"#splitting the data and in this I will use RandomSearchCV therefore will only split into xtrain and xtest\n# train test split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,stratify=Y)\n#X_train, X_cv, Y_train, y_cv = train_test_split(X_train, Y_train, test_size=0.33, stratify=Y_train)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)\n#print(X_cv.shape,y_cv.shape)","966f25ae":"#Encoding the preprocessed_tweets column\nmodel_tweets = CountVectorizer(max_features=41157,ngram_range=(1,2),min_df=10)\nmodel_tweets.fit(X_train[\"OriginalTweet\"].values)\n\nX_train_tweet_bow = model_tweets.transform(X_train[\"OriginalTweet\"].values)\nX_test_tweet_bow = model_tweets.transform(X_test[\"OriginalTweet\"].values)\n\nprint(X_train_tweet_bow.shape, Y_train.shape)\nprint(X_test_tweet_bow.shape, Y_test.shape)\n","bd2f6d96":"#Encoding Numerical features : pre_tweetat\nmodel_time = Normalizer()\nmodel_time.fit(X_train[\"pre_tweetat\"].values.reshape(1,-1))\nmodel_time2 = Normalizer()\nmodel_time2.fit(X_test[\"pre_tweetat\"].values.reshape(1,-1))\n\nX_train_time = model_time.transform(X_train[\"pre_tweetat\"].values.reshape(1,-1)).T\nX_test_time = model_time2.transform(X_test[\"pre_tweetat\"].values.reshape(1,-1)).T\n\nprint(X_train_time.shape, Y_train.shape)\nprint(X_test_time.shape, Y_test.shape)","a4376fb5":"#Concatinating all the features\n\nX_tr = hstack((X_train_tweet_bow,X_train_time)).tocsr()\nX_te = hstack((X_test_tweet_bow,X_test_time)).tocsr()\n\nprint(X_tr.shape, Y_train.shape)\n\nprint(X_te.shape, Y_test.shape)","658691d9":"#Appling Multinomial NB: BOW featurization\n\ndef batch_predict(clf, data):\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs\n\n    y_data_pred = []\n    tr_loop = data.shape[0] - data.shape[0]%1000\n    # consider you X_tr shape is 49041, then your tr_loop will be 49041 - 49041%1000 = 49000\n    # in this for loop we will iterate unti the last 1000 multiplier\n    for i in range(0, tr_loop, 1000):\n        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])\n    # we will be predicting for the last data points\n    if data.shape[0]%1000 !=0:\n        y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])\n    \n    return y_data_pred","c6762aa5":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n\nnbayes = MultinomialNB(class_prior = [0.5,0.5],fit_prior=False)\nparameters = {'alpha':[0.00001,0.0005, 0.0001,0.005,0.001,0.05,0.01,0.1,0.5,1,5,10,50,100]}\nclf = GridSearchCV(nbayes,parameters,cv=10, scoring='roc_auc',return_train_score=True,n_jobs=-1)\nclf.fit(X_tr,Y_train)\n\nresults = pd.DataFrame.from_dict(clf.cv_results_)\nresults = results.sort_values([\"param_alpha\"])\n\ntrain_auc = results[\"mean_train_score\"]\ncv_auc = results[\"mean_test_score\"]\n\nalph = results[\"param_alpha\"]             #Reference from https:\/\/www.youtube.com\/watch?v=RMqT5kDtJhs&t=807s @12:48mins\nlog_alp=[]\nfor i in range(0,len(alph),1):\n    l=math.log10(alph[i])\n    log_alp.append(l)\nlog_alp.sort()\n\nplt.figure(figsize=(20,9))\n\nplt.plot(log_alp,train_auc,label='Train AUC')\nplt.plot(log_alp,cv_auc,label='CV AUC')\nplt.scatter(log_alp,train_auc,label='Train AUC Points')\nplt.scatter(log_alp,cv_auc,label='CV AUC Points')\nplt.legend()\nplt.xlabel(\"Log-Alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"Hyper parameter Vs AUC plot\")\n\nplt.grid()\nplt.show()\n","032ef02e":"#printing the results for better observation\nresults[[\"param_alpha\",\"mean_test_score\",\"mean_train_score\"]]","609a71d2":"best_alpha = 0.5\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\n\nmulti_nb = MultinomialNB(alpha=best_alpha,class_prior=[0.5,0.5])\nmulti_nb.fit(X_tr,Y_train)\n\ny_train_pred = batch_predict(multi_nb,X_tr)\ny_test_pred = batch_predict(multi_nb,X_te)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(Y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(Y_test, y_test_pred)\n\n\nplt.figure(figsize=(10,10))\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"Alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","22557ed7":"# we are writing our own function for predict, with defined thresould\n# we will pick a threshold that will give the least fpr\ndef find_best_threshold(threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    return t\n\ndef predict_with_best_t(proba, threshould):\n    predictions = []\n    for i in proba:\n        if i>=threshould:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","6b09e044":"best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\nprint('='*50)\nprint(\"Train confusion matrix\")\nprint('='*50)\ntrain_cm = confusion_matrix(Y_train, predict_with_best_t(y_train_pred, best_t))\nsns.heatmap(train_cm, annot=True,fmt=\"d\",cmap='Blues')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","b913c394":"print('='*50)\nprint(\"Test confusion matrix\")\nprint('='*50)\ntest_cm = confusion_matrix(Y_test, predict_with_best_t(y_test_pred, best_t))\nsns.heatmap(test_cm, annot=True,fmt=\"d\",cmap='Blues')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","afa2e9cb":"#Extracting True Positives for WordClouds\n\npred1 = predict_with_best_t(y_test_pred, best_t)\nfal_pos = []\nfor i in range(len(Y_test)):\n    if (Y_test[i]==1) and (pred1[i]==1):\n        fal_pos.append(i)\n\n\n#extracting the Tweets from X_test\npos_tweets = []\nfor i in tqdm(fal_pos):\n    pos_tweets.append(X_test['OriginalTweet'].values[i])\n    \n","756c8a08":"#https:\/\/www.geeksforgeeks.org\/generating-word-cloud-python\/\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport pandas as pd\n    \ncomment_words = ''\nstopwords = set(STOPWORDS)\n  \n# iterate through the csv file\nfor val in pos_tweets:\n       \n    # typecaste each val to string\n    val = str(val)\n  \n    # split the value\n    tokens = val.split()\n      \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 1200, height = 600,\n                background_color ='White',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n  \n# plot the WordCloud image                       \nplt.figure(figsize = (10, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n  \nplt.show()","ea3d2dd8":"#Extracting True Negatives for WordClouds\n\npred0 = predict_with_best_t(y_test_pred, best_t)\ntru_neg = []\nfor i in range(len(Y_test)):\n    if (Y_test[i]==0) and (pred0[i]==0):\n        tru_neg.append(i)\n\n\n#extracting the Tweets from X_test\nneg_tweets = []\nfor i in tqdm(tru_neg):\n    neg_tweets.append(X_test['OriginalTweet'].values[i])\n    \n","91a36742":"#https:\/\/www.geeksforgeeks.org\/generating-word-cloud-python\/\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport pandas as pd\n    \ncomment_words = ''\nstopwords = set(STOPWORDS)\n  \n# iterate through the csv file\nfor val in neg_tweets:\n       \n    # typecaste each val to string\n    val = str(val)\n  \n    # split the value\n    tokens = val.split()\n      \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 1200, height = 600,\n                background_color ='White',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n  \n# plot the WordCloud image                       \nplt.figure(figsize = (10, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n  \nplt.show()","b8f407d4":"#Extracting the top 20 Features of Postive and the Negative Class\n\n#https:\/\/imgur.com\/mWvE7gj\ntop_feat = multi_nb.feature_log_prob_\nsort_tf=np.argsort(top_feat)\n\n\nneg_feat = sort_tf[0]\nneg_feat = neg_feat[::-1][:20]         #top 20 negative features\npos_feat = sort_tf[1]\npos_feat = pos_feat[::-1][:20]         #top 20 positive features\n\n\nl = []\nl.extend(model_tweets.get_feature_names())\nl.append('pre_tweetat')\n\n\nprint(\"-\"*100)\nprint(\"Top 20 Positive features\")\nprint(\"-\"*100)\nfor i in pos_feat:\n    print(l[i])\n    \nprint(\"-\"*100)\nprint(\"Top 20 Negative features\")\nprint(\"-\"*100)\nfor j in neg_feat:\n    print(l[j])","da534af5":"Splitting the data into **X** and **Y**","3b2a801d":"Train Test Split","8508fc11":"**Encoding the Columns**"}}