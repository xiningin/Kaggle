{"cell_type":{"8ba81fd4":"code","e45fb9d0":"code","b764bca4":"code","6ce5e136":"code","6f4e6703":"code","c0aa18c7":"code","ff89e8e4":"code","1cfcc8f4":"markdown","d3533ab7":"markdown","1137f961":"markdown","e06bd03e":"markdown","458ea993":"markdown","a340ab71":"markdown","ae9088db":"markdown","597b4e78":"markdown","58d56fb7":"markdown"},"source":{"8ba81fd4":"import logging\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error, log_loss\nfrom typing import List, Tuple, Optional\n\nclass Logger:\n\n    def info(self, message: str):\n        print(f\"[{self.now_string()}]: {message}\")\n\n    def now_string(self: str):\n        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n\nlogger = Logger()    ","e45fb9d0":"class Data:\n\n    def __init__(self, x: np.ndarray, y: Optional[np.ndarray]):\n        self.values: np.array = x\n        self.target: Optional[np.array] = y\n        self.sorted_indexes: Optional[np.array] = None\n\n        # sort index for each feature\n        # note: necessary only for training\n        sorted_indexes = []\n        for feature_id in range(self.values.shape[1]):\n            sorted_indexes.append(np.argsort(self.values[:, feature_id]))\n        self.sorted_indexes = np.array(sorted_indexes).T\n\n\nclass Node:\n\n    def __init__(self, id: int, weight: float):\n        self.id: int = id\n\n        # note: necessary only for leaf node\n        self.weight: float = weight\n\n        # split information\n        self.feature_id: int = None\n        self.feature_value: float = None\n\n    def is_leaf(self) -> bool:\n        return self.feature_id is None\n\n\nclass TreeUtil:\n\n    @classmethod\n    def left_child_id(cls, id: int) -> int:\n        \"\"\"node id of left child\"\"\"\n        return id * 2 + 1\n\n    @classmethod\n    def right_child_id(cls, id: int) -> int:\n        \"\"\"node id of right child\"\"\"\n        return id * 2 + 2\n\n    @classmethod\n    def loss(cls, sum_grad: float, sum_hess: float) -> Optional[float]:\n        if np.isclose(sum_hess, 0.0, atol=1.e-8):\n            return None\n        return -0.5 * (sum_grad ** 2.0) \/ sum_hess\n\n    @classmethod\n    def weight(cls, sum_grad: float, sum_hess: float) -> Optional[float]:\n        if np.isclose(sum_hess, 0.0, atol=1.e-8):\n            return None\n        return -1.0 * sum_grad \/ sum_hess\n\n    @classmethod\n    def node_ids_depth(self, d: int) -> List[int]:\n        return list(range(2 ** d - 1, 2 ** (d + 1) - 1))\n\n\nclass Tree:\n\n    def __init__(self, params: dict):\n        self.params: dict = params\n        self.nodes: List[Node] = []\n\n        # parameters\n        self.max_depth: int = params.get(\"max_depth\")\n\n        # add initial node\n        node = Node(0, 0.0)\n        self.nodes.append(node)\n\n    def construct(self, data: Data, grad: np.ndarray, hess: np.ndarray):\n\n        # data\n        assert (data.sorted_indexes is not None)\n        n = len(data.values)\n        values = data.values\n        sorted_indexes = data.sorted_indexes\n\n        # node ids records belong to\n        node_ids_data = np.zeros(n, dtype=int)\n\n        # [comment with [] is important for understanding]\n        # [for each depth]\n        for depth in range(self.max_depth):\n\n            # node ids in the depth\n            node_ids_depth = TreeUtil.node_ids_depth(depth)\n\n            # [1. find best split] -------------------\n\n            # split information for each node\n            feature_ids, feature_values = [], []\n            left_weights, right_weights = [], []\n\n            # [for each node]\n            for node_id in node_ids_depth:\n\n                node = self.nodes[node_id]\n                # logger.debug(f\"{node_id}: find split -----\")\n\n                # [calculate sum grad and hess of records in the node]\n                sum_grad, sum_hess = 0.0, 0.0\n                for i in range(n):\n                    if node_ids_data[i] != node_id:\n                        continue\n                    sum_grad += grad[i]\n                    sum_hess += hess[i]\n\n                # [initialize best gain (set as all directed to left)]\n                best_gain, best_feature_id, best_feature_value = 0.0, 0, -np.inf\n                best_left_weight, best_right_weight = node.weight, 0.0\n\n                if sum_hess > 0:\n                    sum_loss = TreeUtil.loss(sum_grad, sum_hess)\n                else:\n                    sum_loss = 0.0\n\n                # logger.debug(f\"sum grad:{sum_grad} hess:{sum_hess} loss:{sum_loss}\")\n\n                # [for each feature]\n                for feature_id in range(data.values.shape[1]):\n                    prev_value = -np.inf\n                    \n                    # [have gradients\/hessian of left child records(value of record < value of split)]\n                    left_grad, left_hess = 0.0, 0.0\n\n                    sorted_index = sorted_indexes[:, feature_id]\n\n                    # [for each sorted record]\n                    for i in sorted_index:\n                        # skip if the record does not belong to the node\n                        # NOTE: this calculation is redundant and inefficient.\n                        if node_ids_data[i] != node_id:\n                            continue\n\n                        value = values[i, feature_id]\n\n                        # [evaluate split, if split can be made at the record]\n                        if value != prev_value and left_hess > 0 and (sum_hess - left_hess) > 0:\n                            \n                            # [calculate loss of the split using gradient and hessian]\n                            right_grad = sum_grad - left_grad\n                            right_hess = sum_hess - left_hess\n                            left_loss = TreeUtil.loss(left_grad, left_hess)\n                            right_loss = TreeUtil.loss(right_grad, right_hess)\n                            if left_loss is not None and right_loss is not None:\n                                gain = sum_loss - (left_loss + right_loss)\n                                # logger.debug(f\"'feature{feature_id} < {value}' \" +\n                                #       f\"lg:{left_grad:.3f} lh:{left_hess:.3f} rg:{right_grad:.3f} rh:{right_hess:.3f} \" +\n                                #       f\"ll:{left_loss:.3f} rl:{right_loss:.3f} gain:{gain:.3f}\")\n                                \n                                # [update if the gain is better than current best gain]\n                                if gain > best_gain:\n                                    best_gain = gain\n                                    best_feature_id = feature_id\n                                    best_feature_value = value\n                                    best_left_weight = TreeUtil.weight(left_grad, left_hess)\n                                    best_right_weight = TreeUtil.weight(right_grad, right_hess)\n\n                        prev_value = value\n                        left_grad += grad[i]\n                        left_hess += hess[i]\n\n                # logger.debug(f\"node_id:{node_id} split - 'feature{best_feature_id} < {best_feature_value}'\")\n                feature_ids.append(best_feature_id)\n                feature_values.append(best_feature_value)\n                left_weights.append(best_left_weight)\n                right_weights.append(best_right_weight)\n\n            # [2. update nodes and create new nodes] ----------\n            for i in range(len(node_ids_depth)):\n                node_id = node_ids_depth[i]\n                feature_id = feature_ids[i]\n                feature_value = feature_values[i]\n                left_weight = left_weights[i]\n                right_weight = right_weights[i]\n\n                # update current node\n                node = self.nodes[node_id]\n                node.feature_id = feature_id\n                node.feature_value = feature_value\n\n                # create new nodes\n                left_node = Node(TreeUtil.left_child_id(node_id), left_weight)\n                right_node = Node(TreeUtil.right_child_id(node_id), right_weight)\n                self.nodes += [left_node, right_node]\n\n            # [3. update node ids of records] ----------\n            for i in range(len(node_ids_data)):\n                # directed by split\n                node_id = node_ids_data[i]\n                node = self.nodes[node_id]\n                feature_id, feature_value = node.feature_id, node.feature_value\n\n                # update\n                is_left = values[i, feature_id] < feature_value\n                if is_left:\n                    next_node_id = TreeUtil.left_child_id(node_id)\n                else:\n                    next_node_id = TreeUtil.right_child_id(node_id)\n                node_ids_data[i] = next_node_id\n\n    def predict(self, x: np.ndarray) -> np.ndarray:\n        values = x\n\n        # node ids records belong to\n        node_ids_data = np.zeros(len(values), dtype=int)\n\n        for depth in range(self.max_depth):\n            for i in range(len(node_ids_data)):\n                # directed by split\n                node_id = node_ids_data[i]\n                node = self.nodes[node_id]\n                feature_id, feature_value = node.feature_id, node.feature_value\n\n                # update\n                if feature_id is None:\n                    next_node_id = node_id\n                elif values[i, feature_id] < feature_value:\n                    next_node_id = TreeUtil.left_child_id(node_id)\n                else:\n                    next_node_id = TreeUtil.right_child_id(node_id)\n                node_ids_data[i] = next_node_id\n\n        weights = np.array([self.nodes[node_id].weight for node_id in node_ids_data])\n\n        return weights\n\n    def dump(self) -> str:\n        \"\"\"dump tree information\"\"\"\n        ret = []\n        for depth in range(self.max_depth + 1):\n            node_ids_depth = TreeUtil.node_ids_depth(depth)\n            for node_id in node_ids_depth:\n                node = self.nodes[node_id]\n                if node.is_leaf():\n                    ret.append(f\"{node_id}:leaf={node.weight}\")\n                else:\n                    ret.append(\n                        f\"{node_id}:[f{node.feature_id}<{node.feature_value}] \" +\n                        f\"yes={TreeUtil.left_child_id(node_id)},no={TreeUtil.right_child_id(node_id)}\")\n        return \"\\n\".join(ret)","b764bca4":"class GBDTEstimator:\n\n    def __init__(self, params: dict):\n        self.params: dict = params\n        self.trees: List[Tree] = []\n\n        # parameters\n        self.n_round: int = params.get(\"n_round\")\n        self.eta: float = params.get(\"eta\")\n\n    def calc_grad(self, y_true: np.ndarray, y_pred: np.ndarray) \\\n            -> Tuple[np.ndarray, np.ndarray]:\n        pass\n\n    def fit(self, x: np.ndarray, y: np.ndarray):\n        data = Data(x, y)\n        self._fit(data)\n\n    def _fit(self, data: Data):\n        pred = np.zeros(len(data.values))\n        for round in range(self.n_round):\n            logger.info(f\"construct tree[{round}] --------------------\")\n            grad, hess = self.calc_grad(data.target, pred)\n            tree = Tree(self.params)\n            tree.construct(data, grad, hess)\n            self.trees.append(tree)\n            # NOTE: predict only last tree\n            pred += self._predict_last_tree(data)\n\n    def predict(self, x: np.ndarray) -> np.ndarray:\n        data = Data(x, None)\n        return self._predict(data)\n\n    def _predict(self, data: Data) -> np.ndarray:\n        pred = np.zeros(len(data.values))\n        for tree in self.trees:\n            pred += tree.predict(data.values) * self.eta\n        return pred\n\n    def _predict_last_tree(self, data: Data) -> np.ndarray:\n        assert(len(self.trees) > 0)\n        tree = self.trees[-1]\n        return tree.predict(data.values) * self.eta\n\n    def dump_model(self) -> str:\n        ret = []\n        for i, tree in enumerate(self.trees):\n            ret.append(f\"booster[{i}]\")\n            ret.append(tree.dump())\n        return \"\\n\".join(ret)\n\n\nclass GBDTRegressor(GBDTEstimator):\n\n    def calc_grad(self, y_true: np.ndarray, y_pred: np.ndarray) \\\n            -> Tuple[np.ndarray, np.ndarray]:\n        grad = y_pred - y_true\n        hess = np.ones(len(y_true))\n        return grad, hess\n\n\nclass GBDTClassifier(GBDTEstimator):\n\n    def calc_grad(self, y_true: np.ndarray, y_pred: np.ndarray) \\\n            -> Tuple[np.ndarray, np.ndarray]:\n        # (reference) regression_loss.h\n        y_pred_prob = 1.0 \/ (1.0 + np.exp(-y_pred))\n        eps = 1e-16\n        grad = y_pred_prob - y_true\n        hess = np.maximum(y_pred_prob * (1.0 - y_pred_prob), eps)\n        return grad, hess\n\n    def predict_proba(self, x: np.ndarray) -> np.ndarray:\n        # apply sigmoid\n        return 1.0 \/ (1.0 + np.exp(-self.predict(x)))\n","6ce5e136":"# setting parameters for experiment\ndata_rows = 1000\nparameter_type = \"base\"","6f4e6703":"data = pd.read_csv(\"..\/input\/otto-group-product-classification-challenge\/train.csv\")\ndata[\"target\"] = data[\"target\"].str[-1:].astype(int)\n# print(data[\"target\"].value_counts())\ndata[\"target\"] = np.where(data[\"target\"] > 5, 1, 0)\ndata = data.drop(\"id\", axis=1)\n\nrand = np.random.RandomState(seed=71)\nidx_all = rand.choice(len(data), data_rows * 2, replace=False)\nidx = rand.choice(data_rows * 2, data_rows, replace=False)\nmask_tr = np.isin(np.arange(data_rows * 2), idx)\nmask_va = ~mask_tr\nidx_tr = idx_all[mask_tr]\nidx_va = idx_all[mask_va]\n\ndata_tr = data.iloc[idx_tr]\ndata_va = data.iloc[idx_va]\n\nassert(data_tr.shape[0] == data_rows)\nassert(data_va.shape[0] == data_rows)\nassert(len(np.unique(np.concatenate([data_tr.index, data_va.index]))) == 2 * data_rows)\n\ndata_tr = data_tr.reset_index(drop=True)\ndata_va = data_va.reset_index(drop=True)","c0aa18c7":"%%time\nfrom sklearn.metrics import log_loss\nimport time\n\ntr_x = data_tr.drop(\"target\", axis=1).values\nva_x = data_va.drop(\"target\", axis=1).values\ntr_y = data_tr[\"target\"].values\nva_y = data_va[\"target\"].values\n\nif parameter_type == \"simple\":\n    params = {\"n_round\": 2, \"max_depth\": 2, \"eta\": 1.0}\nif parameter_type == \"base\":\n    params = {\"n_round\": 25, \"max_depth\": 5, \"eta\": 0.1}\n\nstart_time = time.time()\nmodel = GBDTClassifier(params)\nmodel.fit(tr_x, tr_y)\nend_time = time.time()\nlogger.info(f\"elapsed_time: {end_time - start_time:.2f} sec\")\nva_pred = model.predict_proba(va_x)\nscore = log_loss(va_y, va_pred)\nlogger.info(f\"logloss: {score}\")","ff89e8e4":"%%time\nimport xgboost as xgb\n\nsimple_params = {\"n_round\": 2, \"max_depth\": 2,\n                 \"objective\": \"binary:logistic\",\n                 \"base_score\": 0.5, \"random_state\": 71, \"seed\": 171,\n                 \"eta\": 1.0, \"alpha\": 0.0, \"lambda\": 0.0, \"tree_method\": \"exact\",\n                 \"colsample_bytree\": 1.0, \"subsample\": 1.0,\n                 \"gamma\": 0.0, \"min_child_weight\": 0.0, \"nthread\": 1, \"early_stopping_rounds\":100}\n\nbase_params = {\n    \"objective\": \"binary:logistic\",\n    \"eta\": 0.1,\n    \"gamma\": 0.0, \"alpha\": 0.0, \"lambda\": 1.0,\n    \"min_child_weight\": 1, \"max_depth\": 5,\n    \"subsample\": 0.8, \"colsample_bytree\": 0.8,\n    'silent': 1, 'random_state': 71,\n    \"n_round\": 1000, \"early_stopping_rounds\": 10,\n    \"nthread\": 1,\n}\n\n\nif parameter_type == \"simple\":\n    params = simple_params\nif parameter_type == \"base\":\n    params = base_params\n\ntr_x = data_tr.drop(\"target\", axis=1).values\nva_x = data_va.drop(\"target\", axis=1).values\ntr_y = data_tr[\"target\"].values\nva_y = data_va[\"target\"].values\ndtrain = xgb.DMatrix(tr_x, tr_y)\ndvalid = xgb.DMatrix(va_x, va_y)\n\nn_round = params.pop(\"n_round\")\nearly_stopping_rounds = params.pop(\"early_stopping_rounds\")\n\nwatchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n\nbst = xgb.train(params, dtrain, num_boost_round=n_round,\n                evals=watchlist, early_stopping_rounds=early_stopping_rounds)\nva_pred = bst.predict(dvalid)\nscore = log_loss(va_y, va_pred)\nprint(score)\n#bst.dump_model(\"model_otto.txt\")\n\n# n = 1000: 25 rounds, logloss:0.2439\n# n = 30000: 150 rounds, 13sec, logloss:0.1446","1cfcc8f4":"# GBDT Implementation","d3533ab7":"### Preparation","1137f961":"### Run simpleGBDT","e06bd03e":"### GBDTEstimator, GBDTRegressor, GBDTClassifier class\u3000","458ea993":"### Data, Node, TreeUtil, Tree class","a340ab71":"## Run","ae9088db":"### Data Preparation","597b4e78":"### Run xgboost for comparison","58d56fb7":"## 1. GBDT Implementation"}}