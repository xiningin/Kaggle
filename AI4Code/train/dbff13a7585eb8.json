{"cell_type":{"93bb62eb":"code","9743c464":"code","7c693dd7":"code","dc5eb77d":"code","83d2c0a5":"code","ebfb21f3":"code","13ebc25f":"code","a14a3c1e":"code","b33e020b":"code","621a5ee6":"code","c583a09f":"code","432339f9":"code","31065361":"code","8078c5b7":"code","57a2ca5f":"code","460c9ee4":"code","bd110cde":"code","ad7f7af7":"code","333d698a":"code","5cce3eaa":"code","a7e0dad8":"code","bf05f046":"code","de6c31b1":"code","52eac9f5":"code","5c0ebacc":"code","b05eccad":"code","7a299f6f":"code","7a1b7e3c":"code","5333823d":"code","71a5dd62":"code","4afc1c09":"code","8fd257d4":"code","25f3b7cd":"markdown","e384fcf3":"markdown","5d60fb03":"markdown","68d0bbfc":"markdown","bf9b41c3":"markdown","3e59ee61":"markdown","88a9abfc":"markdown","d1bdf9bd":"markdown","a4ae7e89":"markdown","7c9adef9":"markdown","0c25cf90":"markdown","c7394d43":"markdown","54c61d51":"markdown","882c58dc":"markdown","c191a0d5":"markdown","a4fb2a04":"markdown","7a35e717":"markdown","3ed9ddec":"markdown","b97ab2fc":"markdown","7c358762":"markdown","638d8d62":"markdown","43f35fcc":"markdown","bb507dae":"markdown"},"source":{"93bb62eb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler","9743c464":"train_df = pd.read_csv('\/kaggle\/input\/autompg-dataset\/auto-mpg.csv')","7c693dd7":"print(f'Training Shape: {train_df.shape}')","dc5eb77d":"print(train_df.head())\nprint(train_df.sample(5))","83d2c0a5":"display(train_df.info())","ebfb21f3":"train_df['horsepower'] = train_df['horsepower'].replace('?', np.NaN).astype('float64')","13ebc25f":"train_df_corr = train_df.corr().abs().unstack().sort_values(kind='quicksort', ascending=False).reset_index()\ntrain_df_corr.rename(columns={\"level_0\": \"Feature A\", \n                             \"level_1\": \"Feature B\", 0: 'Correlation Coefficient'}, inplace=True)\ntrain_df_corr[train_df_corr['Feature A'] == 'horsepower'].style.background_gradient(cmap='summer_r')","a14a3c1e":"train_df['horsepower'] = train_df.groupby(['displacement'], sort=False)['horsepower'].apply(lambda x: x.fillna(x.mean()))\ntrain_df['horsepower'] = train_df.groupby(['cylinders'], sort=False)['horsepower'].apply(lambda x: x.fillna(x.mean()))","b33e020b":"sns.set()\nplt.subplots(figsize=(10, 6))\nsns.distplot(train_df['horsepower'],bins=25)\nplt.show()","621a5ee6":"print(f'There are some missing values: {train_df.isna().any().any()}')","c583a09f":"g = sns.PairGrid(train_df.drop('car name',axis=1), hue='origin')\ng = g.map_diag(plt.hist, alpha=0.4)\ng = g.map_upper(sns.scatterplot)\ng = g.map_lower(sns.regplot)","432339f9":"org=train_df.copy()\norg['origin']=train_df.origin.map({1: 'US', 2: 'Asian',3:'European'})\norg['origin'].value_counts(normalize=True)","31065361":"sns.set()\nfig, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='origin', y=\"mpg\", data=org)\nplt.axhline(org.mpg.mean(),color='r',linestyle='dashed',linewidth=2)\nplt.show()","8078c5b7":"pd.crosstab(train_df['car name'],train_df['origin'])","57a2ca5f":"train_df['Brand'] = train_df['car name'].str.extract('([A-Za-z]+)\\s', expand=False)","460c9ee4":"train_df['Brand']= train_df['Brand'].replace(np.NaN, 'subaru')\ntrain_df['Brand']= train_df['Brand'].replace('chevroelt', 'chevrolet')\ntrain_df['Brand']= train_df['Brand'].replace('vw', 'volkswagen')\ntrain_df['Brand']= train_df['Brand'].replace('toyouta', 'toyota')\ntrain_df['Brand']= train_df['Brand'].replace('vokswagen', 'volkswagen')\ntrain_df['Brand']= train_df['Brand'].replace('maxda', 'mazda')\ntrain_df['Brand']= train_df['Brand'].replace('mazada', 'mazda')\ntrain_df['Brand']= train_df['Brand'].replace('chevy', 'chevrolet')","bd110cde":"train_df['Brand'].value_counts(normalize=True)","ad7f7af7":"fig, ax = plt.subplots(figsize=(16, 8))\nsns.countplot(train_df['Brand'])\nplt.xticks(rotation=60)\nplt.show()","333d698a":"le = LabelEncoder()\ntrain_df['Brand'] = le.fit_transform(train_df['Brand'])\ntrain_df.drop('car name', axis=1, inplace=True)\ntrain_df.sample(5)","5cce3eaa":"features=train_df.columns.tolist()\nfor feature in features:\n    print(f'{feature} Skewness: {train_df[feature].skew():.2f}, Kurtosis: {train_df[feature].kurtosis():.2f}')","a7e0dad8":"skew_cols=['cylinders','displacement','horsepower','weight']\ntrain_df[skew_cols]=np.log1p(train_df[skew_cols])\nfor feature in features:\n    print(f'{feature} skewness: {train_df[feature].skew():.2f}, Kurtosis: {train_df[feature].kurtosis():.2f}')","bf05f046":"from sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor\nimport xgboost as xgb\nimport lightgbm as lgb","de6c31b1":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_df.drop('mpg', axis=1))\n    rmse= np.sqrt(np.abs(cross_val_score(model, train_df.drop('mpg', axis=1).values, train_df['mpg'], scoring=\"neg_mean_squared_error\", cv = kf, n_jobs=-1)))\n    return(rmse)\n\ndef rtw_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_df.drop('mpg', axis=1))\n    rtw= cross_val_score(model, train_df.drop('mpg', axis=1).values, train_df['mpg'], scoring=\"r2\", cv = kf, n_jobs=-1)\n    return(rtw)","52eac9f5":"mods = [LinearRegression(),Ridge(),GradientBoostingRegressor(),\n      RandomForestRegressor(),BaggingRegressor(),\n      xgb.XGBRegressor(), lgb.LGBMRegressor()]\n\nmodel_df = pd.DataFrame({\n    'Model': [type(i).__name__ for i in mods],\n    'RMSE': [np.mean(rmsle_cv(i)) for i in mods],    \n    'Rmse Std': [np.std(rmsle_cv(i)) for i in mods],\n    'R2': [np.mean(rtw_cv(i)) for i in mods],\n    'R2 Std': [np.std(rmsle_cv(i)) for i in mods]})\ndisplay(model_df.sort_values(by='RMSE', ascending=True).reset_index(drop=True).style.background_gradient(cmap='summer_r'))","5c0ebacc":"X_train, X_test, y_train, y_test = train_test_split(train_df.drop('mpg', axis=1),train_df['mpg'], test_size= 0.33, random_state=42)","b05eccad":"linreg=LinearRegression()\nlinreg.fit(X_train,y_train)\ny_pred=linreg.predict(X_test)\n\nrmse=np.sqrt(mean_squared_error(y_test,y_pred))\nprint(rmse)\nprint(r2_score(y_test, y_pred))","7a299f6f":"ridge = Ridge()\nridge.fit(X_train,y_train)\ny_pred=ridge.predict(X_test)\n\nrmse=np.sqrt(mean_squared_error(y_test,y_pred))\nprint(rmse)\nprint(r2_score(y_test, y_pred))","7a1b7e3c":"bag_regressor = BaggingRegressor()\nbag_regressor.fit(X_train,y_train)\ny_predict = bag_regressor.predict(X_test)\nrmse_bgr = np.sqrt(mean_squared_error(y_test,y_predict))\n\nrmse=np.sqrt(mean_squared_error(y_test,y_predict))\nprint(rmse)\nprint(r2_score(y_test, y_predict))","5333823d":"gb_regressor = GradientBoostingRegressor()\ngb_regressor.fit(X_train,y_train)\ny_predict = gb_regressor.predict(X_test)\nrmse_bgr = np.sqrt(mean_squared_error(y_test,y_predict))\n\nrmse=np.sqrt(mean_squared_error(y_test,y_predict))\nprint(rmse)\nprint(r2_score(y_test, y_predict))\n\n\nfeature_imp = pd.DataFrame(sorted(zip(gb_regressor.feature_importances_,train_df.drop('mpg', axis=1).columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('Features')\nplt.tight_layout()\nplt.show()","71a5dd62":"rf_regressor = RandomForestRegressor()\nrf_regressor.fit(X_train,y_train)\ny_predict = rf_regressor.predict(X_test)\nrmse_bgr = np.sqrt(mean_squared_error(y_test,y_predict))\n\nrmse=np.sqrt(mean_squared_error(y_test,y_predict))\nprint(rmse)\nprint(r2_score(y_test, y_predict))\n\n\nfeature_imp = pd.DataFrame(sorted(zip(rf_regressor.feature_importances_,train_df.drop('mpg', axis=1).columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('Features')\nplt.tight_layout()\nplt.show()","4afc1c09":"xg_reg=xgb.XGBRegressor(booster='gbtree', objective='reg:squarederror')\n\nxg_reg.fit(X_train,y_train)\nxg_y_pred=xg_reg.predict(X_test)\nxg_rmse=np.sqrt(mean_squared_error(y_test,xg_y_pred))\nprint(xg_rmse)\nprint(r2_score(y_test,xg_y_pred))\n\nfeature_imp = pd.DataFrame(sorted(zip(xg_reg.feature_importances_,train_df.drop('mpg', axis=1).columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('Features')\nplt.tight_layout()\nplt.show()","8fd257d4":"lgb_reg = lgb.LGBMRegressor()\n\nlgb_reg.fit(X_train,y_train)\ny_predict=lgb_reg.predict(X_test)\nrmse=np.sqrt(mean_squared_error(y_test,y_predict))\nprint(rmse)\nprint(r2_score(y_test,y_predict))\n\nfeature_imp = pd.DataFrame(sorted(zip(lgb_reg.feature_importances_,train_df.drop('mpg', axis=1).columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('Features')\nplt.tight_layout()\nplt.show()","25f3b7cd":"**Our heatmap confirms there is some kind of linear relation between these features. Let's see if we can catch this trend with our regression models. We'll be there soon...**","e384fcf3":"**Without tuning hyper-paramaters our best option looks like GradientBoostingRegressor, but while in terms of being very close to GBR, XGBRegressor does little bit better on r2 score.**","5d60fb03":"# Data Processing & Feature Engineering","68d0bbfc":"* **Before we move on, I just wanted to take look at effect of origin on the vehicles mpg since our database is mostly US(Around 62%):**","bf9b41c3":"**From this boxplot we can easily see that US manufactured cars are least efficent in terms of mpg and they are almost below global average. It might mean nothing though, we need to move on:**","3e59ee61":"**Before we move on to build our regression models we might take something out of car names feature instead of just dropping them.**\n\n* **It looks like our database has the car brands in first words of our car name column, can we extract this out for searching relations between them?**","88a9abfc":"**Before building our models last thing I want to check is if our data distribution skewed.**","d1bdf9bd":"**It looks much better! It's time to transform these brands to model applicable. We going to categorize them by using sklearn's Label Encoder:** ","a4ae7e89":"**We are going to fill missing values with grouping more related columns.**","7c9adef9":"**I'm also going to do ta train test split to take a closer look on models and check feature importances on different models.**","0c25cf90":"plt.figure(figsize=(12,6))\nsns.heatmap(train_df.corr(),annot=True, linewidths=0.2,cmap='coolwarm', center=0)\nplt.xticks(rotation=45)\nplt.yticks(rotation=45)\nplt.show()","c7394d43":"**Our dataset has some typos and duplicate names for some brands, we must replace them before we proceed.**","54c61d51":"**Can we decrease our skewness with some log scaling? Well little bit better than before.**","882c58dc":"### Exploratory Data Analysis\n\n**Exploratory data analysis would be a great start for us. We need to get some insights before building our models.**","c191a0d5":"* **So let's begin with importing neccesary libraries:**","a4fb2a04":"**Before we replace missing values in horsepower let's check correlations between hp and other variables:**","7a35e717":"**I could say our model did ok with the small data we have!**\n\n**Thank you for checking out my work! I'm in my early days of this journey, there is lot to learn. Please let me know if I did some mistakes, I'm still trying to improve myself!**\n","3ed9ddec":"# Introduction\n\nWhen I started doing this analysis my main goal was getting experience. I'm still learning and trying to improve my skills, so there might be some areas can be improved. My main objectives on this project are:\n* **Explorating and visualising the data with pandas and seaborn packages**\n* **Building and tuning couple regression models to get some stable results with sklearn and xgboost packages**\n\n## Data Set Information:\n\nThis dataset is a slightly modified version of the dataset provided in the StatLib library. In line with the use by Ross Quinlan (1993) in predicting the attribute \"mpg\", 8 of the original instances were removed because they had unknown values for the \"mpg\" attribute. The original dataset is available in the file \"auto-mpg.data-original\".\n\n\"The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes.\" (Quinlan, 1993)\n\n\n## Attribute Information:\n\n1. mpg: continuous\n2. cylinders: multi-valued discrete\n3. displacement: continuous\n4. horsepower: continuous\n5. weight: continuous\n6. acceleration: continuous\n7. model year: multi-valued discrete\n8. origin: multi-valued discrete\n9. car name: string (unique for each instance)\n","b97ab2fc":" **Let's start with cross validation, then we take a closer look on these models**","7c358762":"**When we look at figure below we notice there is some degree of relation between some of the features by just looking it. Interesting...**","638d8d62":"# Modelling","43f35fcc":"**Seems better. Distribution is skewed but we'll deal with it later...**","bb507dae":"**When we take a look at basic statistics we can already see that horsepower column has datatype of object but it should be a float type. In order to convert this column first we need to replace missing value \"?\" in this data to \"NaN\".**"}}