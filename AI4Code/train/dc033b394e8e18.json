{"cell_type":{"0bb956dd":"code","234b3813":"code","c7cfdb9d":"code","cde9f40d":"code","5b28143d":"code","c3c67324":"code","3b89a789":"code","69d3154b":"code","b1b369d3":"code","3630e0f9":"code","94996dd4":"code","517a2d9d":"code","48c046dd":"code","640a241c":"code","104678ea":"code","ad6ce614":"code","98f0dee4":"code","3b5a8d48":"code","4db732fa":"code","2895810c":"code","68e55b26":"code","d5dce21f":"code","2e1bb6e2":"markdown","564999b4":"markdown","b039621e":"markdown","758be242":"markdown","18dc83b2":"markdown","328f203c":"markdown","3b62c616":"markdown","47c0ef5f":"markdown","cb69f13f":"markdown","9f3e7eb1":"markdown","0a2a9864":"markdown","2678e731":"markdown","e64b5039":"markdown"},"source":{"0bb956dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","234b3813":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import cross_validate   #Additional scklearn functions\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe","c7cfdb9d":"import warnings  \nwarnings.filterwarnings('ignore')\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\n\ntrain = pd.read_csv('..\/input\/bank-customers\/Churn Modeling.csv')\ntarget = 'Exited'\nIDcol = 'ID'","cde9f40d":"train","5b28143d":"train.shape","c3c67324":"train.drop(columns=['Surname', 'Geography'], axis=1, inplace=True)","3b89a789":"train['Gender']=train['Gender'].map({'Female':0, 'Male':1})","69d3154b":"train","b1b369d3":"train.info()","3630e0f9":"train.shape","94996dd4":"train.columns","517a2d9d":"features=['RowNumber', 'CustomerId', 'CreditScore', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary' ]","48c046dd":"y=train['Exited']\nX=train[features]","640a241c":"y.shape","104678ea":"X.shape","ad6ce614":"X.info","98f0dee4":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","3b5a8d48":"corrmat=train.corr()\ntop_corr_features=corrmat.index\nplt.figure(figsize=(20,20))\nfig=sns.heatmap(train[top_corr_features].corr(),annot=True,\n                cmap=\"Accent\")","4db732fa":"space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n        'gamma': hp.uniform ('gamma', 1,9),\n        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'n_estimators': 180,\n        'seed': 0\n    }","2895810c":"def objective(space):\n    clf=xgb.XGBClassifier(\n                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n                    colsample_bytree=int(space['colsample_bytree']))\n    \n    evaluation = [( X_train, y_train), ( X_test, y_test)]\n    \n    clf.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"auc\",\n            early_stopping_rounds=10,verbose=False)\n    \n\n    pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, pred>0.5)\n    print (\"SCORE:\", accuracy)\n    return {'loss': -accuracy, 'status': STATUS_OK }","68e55b26":"trials = Trials()\n\nbest_hyperparams = fmin(fn = objective,\n                        space = space,\n                        algo = tpe.suggest,\n                        max_evals = 100,\n                        trials = trials)","d5dce21f":"print(\"The best hyperparameters are : \",\"\\n\")\nprint(best_hyperparams)","2e1bb6e2":"**Importing the libraries**","564999b4":"**Results**","b039621e":"**Importing and reading of data**","758be242":"# XGBOOST PARAMETERS\n**GENERAL PARAMETERS:** **They define the overall functionality of XGBoost algorithm. These include the following parameters:**\n\n**a) booster [default=gbtree]**\n\n**b) silent [default=0]**\n\n**c) nthread [default to maximum number of threads available if not set]**\n\n**BOOSTER PARAMETERS:-** **Guide the individual booster (tree\/regression) at each step**\n\n**a)eta [default=0.3]**\n\n**b) min_child_weight [default=1]**\n\n**c) min_child_weight [default=1]**\n\n**d) max_leaf_nodes**\n\n**e) gamma [default=0]**\n\n**f) max_delta_step [default=0]**\n\n**g) subsample [default=1]**\n\n**h) colsample_bytree [default=1]**\n\n**i) colsample_bylevel [default=1]**\n\n**j) lambda [default=1]**\n\n**k) alpha [default=0]**\n\n**l) scale_pos_weight [default=1]**\n\n**LEARNING TASK PARAMETERS:-** **They Guide the optimization performed**\n\n**a) objective [default=reg:linear]**\n\n**b) eval_metric [ default according to objective ]**\n\n**c) seed [default=0]**","18dc83b2":"# HYPEROPT\n**HYPEROPT is a powerful python library that search through an hyperparameter space of values and find the best possible values that yield the minimum of the loss function.**\n\n**Bayesian Optimization technique uses Hyperopt to tune the model hyperparameters. Hyperopt is a Python library which is used to tune model hyperparameters.**","328f203c":"**Splitting into train and test set**","3b62c616":"**Correlation heatmap**","47c0ef5f":"**Optimization Algorithm**","cb69f13f":"**Define Objective Function**","9f3e7eb1":"# Hyperparameter Tuning with an example\n\n**Now let us perform the hyperparameter tuning on the dataset to understand the working**","0a2a9864":"**Initializing Domain space for range of values**","2678e731":"# Bayesian Optimisation with HYPEROPT\n\n**Optimization is the process of finding a minimum of cost function , that determines an overall better performance of a model on both train-set and test-set.**\n\n**Bayesian optimization is optimization or finding the best parameter for a machine learning or deep learning algorithm.**\n\n**Here, we train the model with various possible range of parameters until a best fit model is obtained.**\n\n**Hyperparameter tuning helps in determining the optimal tuned parameters and return the best fit model.**","e64b5039":"# GENERAL INTRODUCTION TO HYPERPARAMETERS:\n\n**Hyperparameters are certain weights that determine the learning process of an algorithm. \nXGBoost algorithm has become the ultimate weapon of many data scientist. It\u2019s a highly sophisticated algorithm, powerful enough to deal with all sorts of irregularities of data. It is a powerful machine learning algorithm especially where speed and accuracy are concerned. Building a model using XGBoost is easy. But, improving the model using XGBoost is difficult as it contains multiple parameters.**"}}