{"cell_type":{"f8a29e07":"code","3dc0f669":"code","013610b7":"code","d7c769f3":"code","a77e545a":"code","818da486":"code","f4f48c1f":"code","d6ab2799":"code","69d93f16":"code","94eef947":"code","db56b5db":"code","c184a850":"code","c1d6955b":"code","0a83ee10":"code","d11660fa":"code","2ae90db3":"code","d0167521":"code","9ec3efeb":"code","80cc72bb":"code","6aad122a":"code","ff0c96bc":"code","128ac7d8":"code","62a3842e":"code","900a64e9":"code","0ff3d97d":"code","a9aae992":"code","8634f2e2":"code","afae84b0":"code","c277f51c":"code","786dfc43":"code","2ebae63c":"code","59095ff5":"code","f6f54f15":"code","d0d7b1ec":"code","871cf598":"code","79e35df0":"code","b4dfdd3d":"code","4636151c":"code","24a11d69":"code","2127004a":"code","0190bd14":"code","5d431086":"code","64088f10":"code","e2cd89d1":"code","7a6971ef":"code","7864440d":"code","f8330a74":"code","523ac2e5":"code","4cf2e136":"code","6f36a80c":"code","d6614739":"code","911584f1":"code","793a9ed4":"code","7ade53a9":"code","3a49f6a1":"code","dc6f50cb":"code","5f54b3e2":"code","4f2cea8e":"code","7daa9ebb":"code","e54ef8b1":"code","69102441":"code","cd2cc405":"code","056eb147":"code","56b3a862":"code","14ff5be9":"code","ff952a77":"code","294c566c":"code","e476697b":"code","80c7d85c":"code","d249df16":"code","e629b4a4":"code","3bd89bef":"code","159b06d3":"code","ff2b1d99":"code","06a1d51f":"code","60eba4cb":"code","04805b0d":"code","6378e97b":"code","5b17a62e":"code","8b1c3474":"code","f6b30cbe":"code","ae64aff6":"code","b819addb":"code","3d726f6b":"code","99ebb920":"code","f36f307a":"markdown","1d44e1f0":"markdown","80806ff4":"markdown","a7af0bdc":"markdown","abdd6c47":"markdown","d32fe1b3":"markdown","c4a91f57":"markdown","34e846e8":"markdown","b4804457":"markdown","9b5c894c":"markdown","ff9bde0f":"markdown","e12ae3f0":"markdown","f9f7f5cf":"markdown","b92fb2d0":"markdown","576b4d85":"markdown","814384e8":"markdown","172e9e03":"markdown","54fae967":"markdown","e6802285":"markdown","fbd589e2":"markdown","31628d28":"markdown","cb8c421c":"markdown","6f2161e0":"markdown","0a1819c4":"markdown","f78cfc29":"markdown","db99887f":"markdown","3251e20f":"markdown","c905a87b":"markdown","a5a650e0":"markdown","10f34289":"markdown","7a99924a":"markdown","1e808cf1":"markdown","bf3e9170":"markdown","d9eee718":"markdown","d92c70eb":"markdown","2c3c159f":"markdown","b8565990":"markdown","d0a2158f":"markdown","5d098c12":"markdown","45cb348d":"markdown","71409f72":"markdown","08e3b031":"markdown","ffc15a8c":"markdown","c4288e51":"markdown","2f7d54fa":"markdown","56eb22bf":"markdown","ff22395e":"markdown","27fadc5b":"markdown","1cefbf83":"markdown","48d565cc":"markdown","43f48271":"markdown","749c2df3":"markdown","774c8491":"markdown","916b8ada":"markdown","28076e42":"markdown","ca353bd9":"markdown"},"source":{"f8a29e07":"#Data Manipulation Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re #regular expressions\n#Progress bar\nfrom tqdm import tqdm\nfrom datetime import datetime\n#Read Images\nimport os\nfrom skimage import io\n#from skimage import io #returning error ImportError: cannot import name 'io' so temporarily commented\nfrom PIL import Image\nimport cv2 # When open cv was used, there was an error in getting array from image. Using Pillow eliminated the error.\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Image copy\nfrom shutil import copyfile\nfrom random import seed\nfrom random import random\n\n\n#Model Pre-processing\n#from sklearn.model_selection import train_test_split\n\n#Modelling\nimport tensorflow as tf\nimport sys\nfrom matplotlib import pyplot\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.applications.vgg16 import VGG16\nfrom keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\nfrom keras.models import Model\nfrom keras.optimizers import SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import  r2_score,roc_auc_score,f1_score,recall_score,precision_score,classification_report, confusion_matrix,log_loss\nimport random","3dc0f669":"# Increase rows and columns visible on the notebook\npd.set_option('display.max_rows', 5000)\npd.set_option('display.max_columns', 50)\npd.set_option('max_colwidth', 100)\n\n# import required libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")","013610b7":"image_path = '..\/input\/sbtic-animal-classification\/SBTIC\/'\nos.listdir(image_path)","d7c769f3":"# How many images in directories\ncategories = ['test',  'train_zebras', 'train_elephants']\n\nfor category in categories:\n    full_image_path = image_path +  category + \"\/\" +category + \"\/\"\n    \n    print(category,len(os.listdir(full_image_path)))\n    ","a77e545a":"image_categories = []\nfile_names =[]\nimage_names = []\n# Loop across the directories having images.\nfor category in categories:        \n    full_image_path = image_path +  category + \"\/\" +category + \"\/\"\n    image_file_names = [os.path.join(full_image_path, f) for f in os.listdir(full_image_path)] # Retrieve the filenames from the all the  directories. OS package used.\n    excempt = full_image_path + '.DS_Store' #If using linux, hidden file .DS_Store is read which causes failure as not jpg. Remove it\n    if excempt in image_file_names:\n        image_file_names.remove(excempt)\n    for file in image_file_names:         # Read the labels and load them into an array\n        file_name = os.path.basename(file) ## Eliminate path from file name\n        image_categories.append(category)\n        file_names.append(file)\n        image_names.append(file_name)","818da486":"print(len(file_names))\nprint(len(image_names))\nprint(len(image_categories))","f4f48c1f":"# df = pd.DataFrame(file_names,image_names)\n# df_cat = pd.DataFrame(image_categories)\n\ndf = pd.DataFrame({'file_names': file_names, 'image_names': image_names,'image_categories':image_categories}, columns=['file_names', 'image_names','image_categories'])\n\n# result = pd.merge(df,df_cat, how='outer')","d6ab2799":"#Delete directory if it exists.\nimport shutil\n\ndef ignore_absent_file(func, path, exc_inf):\n    except_instance = exc_inf[1]\n    if isinstance(except_instance, FileNotFoundError):\n        return\n    raise except_instance\n\nshutil.rmtree('\/kaggle\/working\/SBTIC\/test', onerror=ignore_absent_file)","69d93f16":"# create directories\ndataset_home = 'SBTIC\/'\nsubdirs = ['train\/', 'validation\/']\nfor subdir in subdirs:\n    # create label subdirectories\n    labeldirs = ['train_elephants\/', 'train_zebras\/']\n    for labldir in labeldirs:\n        newdir = dataset_home + subdir + labldir\n        os.makedirs(newdir, exist_ok=True)","94eef947":"output_path = '\/kaggle\/working'\nos.listdir(output_path)","db56b5db":"# Copy files from input to output train and validaton directories and their corresponding class directories\nseed = 1\nval_ratio = 0.25\nfor index, row in df.iterrows():\n    if row['image_categories'] != 'test':\n        src = row['file_names']\n        if random() < val_ratio:\n            dst = '\/kaggle\/working\/SBTIC\/validation'+ '\/' + row['image_categories'] + '\/' +row['image_names']\n        else:\n            dst = '\/kaggle\/working\/SBTIC\/train'+ '\/' + row['image_categories'] + '\/' +row['image_names']\n        copyfile(src, dst)","c184a850":"# How many images in directories\ncategories = ['train_zebras', 'train_elephants']\noutput_path = dst = '\/kaggle\/working\/SBTIC\/'\nfor category in categories:\n    full_image_path = output_path +  'validation' + \"\/\" +category + \"\/\"\n    print(category,len(os.listdir(full_image_path)))\nfor category in categories:\n    full_image_path = output_path +  'train' + \"\/\" +category + \"\/\"\n    print(category,len(os.listdir(full_image_path)))","c1d6955b":"#Function to upload and if need be resize the training images\ndef upload_train_images(image_path, categories ,height, width):\n    images = []\n    labels = []\n    file_names =[]\n    # Loop across the directories having images.\n    for category in categories:\n        \n        # Append the  category directory into the main path\n        full_image_path = image_path +  category + \"\/\" +category + \"\/\"\n        # Retrieve the filenames from the all the three wheat directories. OS package used.\n        image_file_names = [os.path.join(full_image_path, f) for f in os.listdir(full_image_path)]\n        \n        #If using linux, hidden file .DS_Store is read which causes failure as not jpg. Remove it\n        excempt = full_image_path + '.DS_Store'\n        if excempt in image_file_names:\n            image_file_names.remove(excempt)\n            \n        # Read the images and load them into an array\n        for file in image_file_names[0:100]:         \n            image=io.imread(file) #io package from SKimage package\n            # Resize?\n            #image_from_array = Image.fromarray(image, 'RGB')\n            ##Resize image\n            #size_image = image_from_array.resize((height, width)) # no resize\n            #Append image into list\n            images.append(np.array(image))\n            # Label for each image as per directory\n            labels.append(category)\n            file_names.append(file)\n        \n    return images, labels, file_names\n\n## Invoke the function\n\n#Image resize parameters if needed. Not resizing in this case so code below just a boilerplate incase resizing needed\nheight = 256\nwidth = 256\n\ncategories = ['train_zebras', 'train_elephants'] \ntrain_images, train_categories, train_file_names  = upload_train_images('\/kaggle\/input\/sbtic-animal-classification\/SBTIC\/',categories,height,width)\n#Size and dimension of output image and labels\ntrain_images = np.array(train_images)\ntrain_categories = np.array(train_categories)\ntrain_file_names = np.array(train_file_names)\n\n#Check properties of uploaded images\nprint(\"Shape of training images is \" + str(train_images.shape))\nprint(\"Shape of training labels is \" + str(train_categories.shape))\nprint(\"Shape of training labels is \" + str(train_file_names.shape))","0a83ee10":"## Eliminate path from file name\n# use regular expressions to extract the name of image\nimage_names = []\nfor i in train_file_names:\n    fname = os.path.basename(i)\n    image_names.append(fname)\n\n#View images\nimage_names = np.array(image_names)\nprint(len(image_names))\nimage_names[0:5]\n","d11660fa":"import random\ndef show_train_images(images, train_categories, train_file_names,image_names,images_count):\n     for i in range(images_count):\n        \n        index = int(random.random() * len(images))\n        plt.axis('off')\n        plt.imshow(images[index])\n        plt.show()\n        \n        print(\"Size of this image is \" + str(images[index].shape))\n        print(\"Class of the image is \" + str(train_categories[index]))\n        print(\"Image path is \" + str(train_file_names[index]))        \n        print(\"Image name is \" + str(image_names[index]))   \n\n#Execute the function\nprint(\"Train images, sizes and class labels\")\nshow_train_images(train_images, train_categories,train_file_names,image_names, 10)","2ae90db3":"title = train_categories[1],image_names[1]\ntitle","d0167521":"# a function to show the image batch\ndef show_batch_train_images(images,train_categories,image_names):\n    plt.figure(figsize=(20,15))\n    for n in range(20):\n        ax = plt.subplot(5,5,n+1)\n        index = int(random.random() * len(images))\n        plt.imshow(images[index])\n        title = train_categories[index],image_names[index]\n        plt.title(title)\n#         plt.title(CLASS_NAMES[labels[n]==1][0].title())\n#         print(\"Size of this image is \" + str(images[index].shape))\n        plt.axis('off')\n\nshow_batch_train_images(train_images,train_categories,image_names)\nplt.show()","9ec3efeb":"#Categories of Images\npd.Series(train_categories).value_counts().reset_index().values.tolist()","80cc72bb":"# Plot chart\nsns.countplot(df.image_categories)\n# plt.show()\n# df.image_categories","6aad122a":"# define cnn model\ndef define_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(330, 330, 3)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.2))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.2))\n    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dropout(0.5))\n    model.add(Dense(2, activation='softmax'))\n    # compile model\n    \n    opt = SGD(lr=0.001, momentum=0.9)\n \n    #Compile the model\n    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n \n# plot diagnostic learning curves\ndef summarize_diagnostics(history):\n    # plot loss\n    plt.subplot(211)\n    plt.title('Cross Entropy Loss')\n    plt.plot(history.history['loss'], color='blue', label='train')\n    plt.plot(history.history['val_loss'], color='orange', label='test')\n    # plot accuracy\n    plt.subplot(212)\n    plt.title('Classification Accuracy')\n    plt.plot(history.history['accuracy'], color='blue', label='train')\n    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n    # save plot to file\n    filename = sys.argv[0].split('\/')[-1]\n    plt.savefig(filename + '_plot.png')\n    plt.close()\n    \n# run the test harness for evaluating a model\ndef run_test_harness():\n    # define model\n    print(\"Define Model\")\n    model = define_model()\n    # create data generator\n    print(\"Creating Image Data Generator\")\n    datagen = ImageDataGenerator(rescale=1.0\/255.0)\n    \n    # prepare iterators\n    print(\"Preparing iterators\")\n    train_it = datagen.flow_from_directory('\/kaggle\/working\/SBTIC\/train\/', class_mode='binary', batch_size=64, target_size=(330, 330))\n    test_it = datagen.flow_from_directory('\/kaggle\/working\/SBTIC\/validation\/', class_mode='binary', batch_size=64, target_size=(330, 330))\n    \n    # fit model\n    print(\"Fitting the model\")\n    history = model.fit_generator(train_it, steps_per_epoch=len(train_it),validation_data=test_it, validation_steps=len(test_it), epochs=5, verbose=1) #We 10 epochs before\n    \n    print(\"Testing the model\")\n    # evaluate model\n    _, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=1)\n    print('> %.3f' % (acc * 100.0))\n    # learning curves\n    summarize_diagnostics(history)\n    return(history)\n","ff0c96bc":"#Execute the Model\nmodel_history = run_test_harness()","128ac7d8":"# plot Loss and classification accuracy\n    plt.subplot(211)\n    plt.title('Cross Entropy Loss')\n    plt.plot(model_history.history['loss'], color='blue', label='train')\n    plt.plot(model_history.history['val_loss'], color='orange', label='test')\n    # plot accuracy\n    plt.subplot(212)\n    plt.title('Classification Accuracy')\n    plt.plot(model_history.history['accuracy'], color='blue', label='train')\n    plt.plot(model_history.history['val_accuracy'], color='orange', label='test')\n    plt.show()","62a3842e":"# Apply data augmentation on baseline model above.\n# Create cnn model\ndef define_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(330, 330, 3)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(2, activation='sigmoid'))\n    # compile model\n    opt = SGD(lr=0.001, momentum=0.9)\n    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n \n# plot diagnostic learning curves\ndef summarize_diagnostics(history):\n    # plot loss\n    pyplot.subplot(211)\n    pyplot.title('Cross Entropy Loss')\n    pyplot.plot(history.history['loss'], color='blue', label='train')\n    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n    # plot accuracy\n    pyplot.subplot(212)\n    pyplot.title('Classification Accuracy')\n    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n    # save plot to file\n    filename = sys.argv[0].split('\/')[-1]\n    pyplot.savefig(filename + '_plot.png')\n    pyplot.close()\n \n# run the test harness for evaluating a model\ndef run_test_harness():\n    # define model\n    model = define_model()\n    # create data generators\n    train_datagen = ImageDataGenerator(rescale=1.0\/255.0, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n    test_datagen = ImageDataGenerator(rescale=1.0\/255.0)\n    \n    # prepare iterators\n    train_it = train_datagen.flow_from_directory('\/kaggle\/working\/SBTIC\/train\/',class_mode='binary', batch_size=64, target_size=(330, 330))\n    test_it = test_datagen.flow_from_directory('\/kaggle\/working\/SBTIC\/validation\/',class_mode='binary', batch_size=64, target_size=(330, 330))\n    \n    # fit model\n    history = model.fit_generator(train_it, steps_per_epoch=len(train_it),validation_data=test_it, validation_steps=len(test_it), epochs=5, verbose=1) # Were 10 epochs earlier\n    # evaluate model\n    _, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=1)\n    print('> %.3f' % (acc * 100.0))\n    # learning curves\n    summarize_diagnostics(history)\n    return(history)\n","900a64e9":"da_model_history = run_test_harness()","0ff3d97d":"plt.subplot(211)\nplt.title('Cross Entropy Loss')\nplt.plot(da_model_history.history['loss'], color='blue', label='train')\nplt.plot(da_model_history.history['val_loss'], color='orange', label='test')\n# plot accuracy\nplt.subplot(212)\nplt.title('Classification Accuracy')\nplt.plot(da_model_history.history['accuracy'], color='blue', label='train')\nplt.plot(da_model_history.history['val_accuracy'], color='orange', label='test')\nplt.show()","a9aae992":" # Create cnn model\ndef vgg_model():\n    # load model\n    model = VGG16(weights='imagenet',include_top=False, input_shape=(330, 330, 3)) #weights='imagenet'. Crosscheck before and after\n    # mark loaded layers as not trainable\n    for layer in model.layers:layer.trainable = False\n    # add new classifier layers\n    flat1 = Flatten()(model.layers[-1].output)\n    class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n    output = Dense(2, activation='softmax')(class1)\n    # define new model\n    model = Model(inputs=model.inputs, outputs=output)\n    # compile model\n    opt = SGD(lr=0.001, momentum=0.9)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) #sparse_categorical_crossentropy\n    return model\n \n# plot diagnostic learning curves\ndef summarize_diagnostics(history):\n    # plot loss\n    pyplot.subplot(211)\n    pyplot.title('Cross Entropy Loss')\n    pyplot.plot(history.history['loss'], color='blue', label='train')\n    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n    # plot accuracy\n    pyplot.subplot(212)\n    pyplot.title('Classification Accuracy')\n    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n    # save plot to file\n    filename = sys.argv[0].split('\/')[-1]\n    pyplot.savefig(filename + '_plot.png')\n    pyplot.close()\n \n# run the test harness for evaluating a model\ndef run_test_harness():\n    # define model\n    model = vgg_model()\n    # create data generators\n    #train_datagen = ImageDataGenerator(rescale=1.0\/255.0, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n    train_datagen = ImageDataGenerator(rescale=1.\/255,rotation_range=40,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2,zoom_range=0.2,horizontal_flip=True,fill_mode='nearest')\n    test_datagen = ImageDataGenerator(rescale=1.0\/255.0)\n    # prepare iterators                   \n    train_it = train_datagen.flow_from_directory('\/kaggle\/working\/SBTIC\/train\/',class_mode='categorical', batch_size=64, target_size=(330, 330))\n    test_it = test_datagen.flow_from_directory('\/kaggle\/working\/SBTIC\/validation\/',class_mode='categorical', batch_size=64, target_size=(330, 330))\n    # fit model\n    history = model.fit_generator(train_it, steps_per_epoch=len(train_it),validation_data=test_it, validation_steps=len(test_it), epochs=1, verbose=1) #Were 50 epochs earlier\n    # evaluate model\n    _, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=1)\n    print('> %.3f' % (acc * 100.0))\n    # learning curves\n    summarize_diagnostics(history)\n    return(history)","8634f2e2":"vgg_model","afae84b0":"tl_model_history = run_test_harness()","c277f51c":"plt.subplot(211)\nplt.title('Cross Entropy Loss')\nplt.plot(tl_model_history.history['loss'], color='blue', label='train')\nplt.plot(tl_model_history.history['val_loss'], color='orange', label='test')\n# plot accuracy\nplt.subplot(212)\nplt.title('Classification Accuracy')\nplt.plot(tl_model_history.history['accuracy'], color='blue', label='train')\nplt.plot(tl_model_history.history['val_accuracy'], color='orange', label='test')\nplt.show()","786dfc43":"#Create a directory combining both train and validation dataset\ndataset_home = 'SBTIC\/'\nsubdirs = ['combined\/']\nfor subdir in subdirs:\n    # create label subdirectories\n    labeldirs = ['train_elephants\/', 'train_zebras\/']\n    for labldir in labeldirs:\n        newdir = dataset_home + subdir + labldir\n        os.makedirs(newdir, exist_ok=True)","2ebae63c":"output_path = '\/kaggle\/working\/SBTIC\/combined\/train_zebras'\nos.listdir(output_path)","59095ff5":"# Copy files from input to combined directory. \nseed = 1\nfor index, row in df.iterrows():\n    if row['image_categories'] != 'test':\n        src = row['file_names']\n        dst = '\/kaggle\/working\/SBTIC\/combined'+ '\/' + row['image_categories'] + '\/' +row['image_names']\n        copyfile(src, dst)","f6f54f15":"# How many images in directories\ncategories = ['train_zebras', 'train_elephants']\noutput_path = dst = '\/kaggle\/working\/SBTIC\/combined'\nfor category in categories:\n    full_image_path = output_path +   \"\/\" +category + \"\/\"\n    print(full_image_path)\n    print(category,len(os.listdir(full_image_path)))","d0d7b1ec":"# RUN the model on full dataset.\ndef run_final_model():\n# define model\n    model = vgg_model()\n    # create data generator\n    datagen = ImageDataGenerator(featurewise_center=True)\n    # specify imagenet mean values for centering\n    #datagen.mean = [123.68, 116.779, 103.939]\n    # prepare iterator\n    train_it = datagen.flow_from_directory('\/kaggle\/working\/SBTIC\/combined\/',class_mode='categorical', batch_size=64, target_size=(330, 330))\n    print(\"Fitting the model\")\n    # fit model\n    model.fit_generator(train_it, steps_per_epoch=len(train_it), epochs=1, verbose=0) #Were 11 epochs\n    # save model\n    model.save('marine.h5')\n    class_dictionary = train_it.class_indices\n    print(train_it.classes)\n    print(class_dictionary)\n    return(train_it)\n","871cf598":"# Excecute the model\ntrain_it = run_final_model()","79e35df0":"#Import test data from test path\nt_file_names =[]\nt_file_path =[]\ntest_image_path = '..\/input\/sbtic-animal-classification\/SBTIC\/test\/test\/'\ntest_image_file_names = [os.path.join(test_image_path, f) for f in os.listdir(test_image_path)] # Retrieve the filenames from the all the  directories. OS package used.\nfor tfile in test_image_file_names:         # Read the labels and load them into an array\n        FILE = os.path.basename(tfile) ## Eliminate path from file name\n        t_file_names.append(FILE)    \n        t_file_path.append(tfile)\nprint(len(t_file_names))\nprint(len(t_file_path))","b4dfdd3d":"t_file_names[1]\nt_file_path[1]","4636151c":"#Create Test Dataframe\ndf_test = pd.DataFrame({'t_file_names': t_file_names,'t_file_path':t_file_path}, columns=['t_file_names','t_file_path'])\ndf_test","24a11d69":"# make a prediction for a new image.\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.models import load_model\n \n# load and prepare the image\ndef load_image(filename):\n    # load the image\n    img = load_img(filename, target_size=(330, 330))\n    # convert to array\n    img = img_to_array(img)\n    # reshape into a single sample with 3 channels\n    img = img.reshape(1, 330, 330, 3)\n    # center pixel data\n    img = img.astype('float32')\n    img = img - [123.68, 116.779, 103.939]\n    return img\n \n# load an image and predict the sample image\ndef run_sample_prediction():\n    # load the image\n    img = load_image('..\/input\/sbtic-animal-classification\/SBTIC\/test\/test\/ASG001e15q_2.jpeg')\n    # load model\n    model = load_model('marine.h5')\n    # predict the class\n    y_predicted = model.predict(img) #oringinal\n    y_classes = y_predicted.argmax(axis=-1)\n    #y_classes = keras.np_utils.probas_to_classes(y_predicted)\n    print(\"Prediction\",y_predicted)\n    #print(\"class\",y_classes)\n    print(\"rint\",y_classes)\n    return(y_predicted,y_classes)\n ","2127004a":"#Check the Prediction_Result \ny_predicted,y_classes = run_prediction()","0190bd14":"df_test[0:3]","5d431086":"def run_test_prediction():\n    # load the image\n    test_images =[]\n    predictions =[]\n    for index, row in df_test[0:3].iterrows():\n        img = load_image(row['t_file_path'])\n        test_images.append(row['t_file_names'])\n        print(index)\n        print(test_images)\n        # load model\n        model = load_model('marine.h5')\n        # predict the class\n        y_predicted = model.predict(img) \n        predictions.append(y_predicted)\n        y_classes = y_predicted.argmax(axis=-1)\n        print(\"Prediction\",y_predicted)\n        print(\"rint\",y_classes)\n    return(y_predicted,y_classes,test_images,predictions)","64088f10":"##Full test set prediction\n\n,y_classes,test_images,predictions = run_test_prediction()","e2cd89d1":"column_names = []\nlabels = (train_it.class_indices)\ndict_labels = dict((v,k) for k,v in labels.items())\nfor key, value in dict_labels.items():\n    print(key, '->', value)\n    column_names.append(value)\ncolumn_names.insert( 0, 'FILE');\ncolumn_names\n","7a6971ef":"predictions","7864440d":"df_FILE = pd.DataFrame(test_images)\ndf_FILE\ndf_predicted = pd.DataFrame(np.concatenate(predictions))\ndf_predicted\nresult = pd.concat([df_FILE, df_predicted], axis=1)\nresult.columns =[column_names]\nresult\n#  df_predicted = pd.DataFrame(test_images,np.concatenate(predictions), columns =column_names)\n","f8330a74":"img = load_image('..\/input\/sbtic-animal-classification\/SBTIC\/test\/test\/ASG001e15q_2.jpeg')\nplt.imshow(img)\nplt.show()","523ac2e5":"labels = (train_generator.class_indices)\n# labels = dict((v,k) for k,v in labels.items())\n# predictions = [labels[k] for k in predicted_class_indices]","4cf2e136":"#Label encoding to change \nprint(np.unique(train_categories))\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ntrain_labels_enc = label_encoder.fit_transform(train_categories)\nprint(np.unique(train_labels_enc))\n# Elephant = 0, zebra =1","6f36a80c":"#Convert the predicted labels to categorical type\ntrain_labels_cat = to_categorical(train_labels_enc)\nprint(train_categories)\nprint(train_labels_enc)\nprint(train_labels_cat)\n##Display the categorical training labels\n#Elephant\nprint(train_labels_cat[0])\nprint(train_labels_cat[199])\n##\nprint(train_labels_cat[300])\nprint(train_labels_cat[399])","d6614739":"#Function to upload and if need be resize the training images\ndef upload_train_images(image_path, categories ,height, width):\n    images = []\n    labels = []\n    file_names =[]\n    # Loop across the directories having images.\n    for category in categories:\n        \n        # Append the  category directory into the main path\n        full_image_path = image_path +  category + \"\/\" +category + \"\/\"\n        # Retrieve the filenames from the all the three wheat directories. OS package used.\n        image_file_names = [os.path.join(full_image_path, f) for f in os.listdir(full_image_path)]\n        \n        #If using linux, hidden file .DS_Store is read which causes failure as not jpg. Remove it\n        excempt = full_image_path + '.DS_Store'\n        if excempt in image_file_names:\n            image_file_names.remove(excempt)\n            \n        # Read the images and load them into an array\n        for file in image_file_names[0:200]:         \n            image=io.imread(file) #io package from SKimage package\n            # Resize?\n            #image_from_array = Image.fromarray(image, 'RGB')\n            ##Resize image\n            #size_image = image_from_array.resize((height, width)) # no resize\n            #Append image into list\n            image = image.astype('float32')\/255\n            images.append(np.array(image))\n            # Label for each image as per directory\n            labels.append(category)\n            file_names.append(file)\n        \n    return images, labels, file_names\n\n## Invoke the function\n\n#Image resize parameters if needed. Not resizing in this case so code below just a boilerplate incase resizing needed\nheight = 256\nwidth = 256\n\ncategories = ['train_zebras', 'train_elephants'] \ntrain_images, train_categories, train_file_names  = upload_train_images('\/kaggle\/input\/sbtic-animal-classification\/SBTIC\/',categories,height,width)\n#Size and dimension of output image and labels\ntrain_images = np.array(train_images)\ntrain_categories = np.array(train_categories)\ntrain_file_names = np.array(train_file_names)\n\n#Check properties of uploaded images\nprint(\"Shape of training images is \" + str(train_images.shape))\nprint(\"Shape of training labels is \" + str(train_categories.shape))\nprint(\"Shape of training labels is \" + str(train_file_names.shape))","911584f1":"#Normalize the image pixels\ntrain_images = train_images.astype('float32')\/255","793a9ed4":"# Training to have 90% and validation 10%. High value of training taken so that we have ample training images. \n# The more the images, the better the model\nX_train,X_valid,Y_train,Y_valid = train_test_split(train_images,train_labels_cat,test_size = 0.1,random_state=None)\n\nprint(\"X Train count is \",len(X_train),\"Shape\",X_train.shape, \" and Y train count \",len(Y_train), \"Shape\", Y_train.shape )\nprint(\"X validation count is \",len(X_valid), \"Shape\",X_valid.shape,\" and Y validation count \", len(Y_valid), \"Shape\",Y_valid.shape)","7ade53a9":"#Define the CNN Model\n#Sequential API to add one layer at a time starting from the input.\nmodel = Sequential()\n# Convolution layer with 32 filters first Conv2D layer.  \n# Each filter transforms a part of the image using the kernel filter. The kernel filter matrix is applied on the whole image.\n# Relu activation function used to add non linearity to the network.\nmodel.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_train.shape[1:]))\n# Convolution layer with 64 filters second Conv2D layer \nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n# Max pooling applied. Reduces the size of the image by half. Is a downsampling filter which looks at the 2 neighboring pixels and picks the maximal value\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n# Drop applied as a regularization method, where a proportion of nodes in the layer are randomly ignored by setting their wieghts to zero for each training sample.\n# This drops randomly a proportion of the network and forces the network to learn features in a distributed way. This improves generalization and reduces overfitting.\nmodel.add(Dropout(rate=0.25))\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(rate=0.25))\n# Flatten to convert the final feature maps into a one single 1D vector. Needed so as to make use of fully connected layers after some convolutional\/maxpool layers.\n# It combines all the found local features of the previous convolutional layers.\nmodel.add(Flatten())\n#Dense layer applied to create a fully-connected artificial neural networks classifier.\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(rate=0.5))\n#Neural net outputs distribution of probability of each class.\nmodel.add(Dense(2, activation='softmax')) # 2 output classes\nmodel.summary()","3a49f6a1":"#Compilation of the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01), \n                    loss=tf.keras.losses.categorical_crossentropy, \n                    metrics = [tf.keras.metrics.categorical_accuracy])","dc6f50cb":"#Using ten epochs for the training and saving the accuracy for each epoch\nhistory = model.fit(X_train[1:10], Y_train[1:10], batch_size=32, epochs=12,\n                    validation_data=(X_valid, Y_valid)) #  #,validation_split = 0.2, callbacks=callbacks,\n\n#Class weight parameter specified for to rectify class imbalance ,class_weight=class_weights","5f54b3e2":"#Display of the accuracy and the loss values\nplt.figure(0)\nplt.plot(history.history['categorical_accuracy'], label='training accuracy')\nplt.plot(history.history['val_categorical_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history.history['loss'], label='training loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","4f2cea8e":"# Create dictionary and dataframe to hold results for various models\ndict = {'Model':['Baseline CNN' ,'Mobile Net V2', 'Data Augmentation'], \n        'AUC': [0,0,0],\n        'Log Loss':[0,0,0], \n        'F1 score':[0,0,0], \n        'Recall':[0,0,0], \n        'Precision':[0,0,0]} \ndf_results = pd.DataFrame(dict,columns = ['Model','Log Loss','AUC','F1 score','Recall','Precision'])\n\n\n# Function to calculate Results for each model\ndef model_results(model_type,y_test_data, y_prediction_data, y_test_class, y_pred_class):\n    \n    index_val = df_results[df_results['Model']==model_type].index\n    \n    #Asign scores to dataframe\n    df_results.loc[index_val,'AUC'] = roc_auc_score(y_test_data, y_prediction_data)\n    df_results.loc[index_val,'Log Loss'] = log_loss(Y_valid, y_prediction_data)\n    df_results.loc[index_val,'F1 score'] = f1_score(y_test_class, y_pred_class,average='weighted')\n    df_results.loc[index_val,'Recall'] = recall_score(y_test_class, y_pred_class,average='weighted')\n    df_results.loc[index_val,'Precision'] = precision_score(y_test_class, y_pred_class,average='weighted')\n\n    return(df_results)","7daa9ebb":"#Baseline Prediction\ny_prediction = model.predict(X_valid) # make predictions\n\n#Baseline Results\ndominant_y_valid=np.argmax(Y_valid, axis=1)\ndominant_y_predict=np.argmax(y_prediction, axis=1)\n\nmodel_results('Baseline CNN',Y_valid, y_prediction,dominant_y_valid,dominant_y_predict)","e54ef8b1":"#Confusion Matrix\nimport itertools\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=75) \n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        \n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nclass_names = range(3)\n# cm = confusion_matrix(rounded_Y_valid , rounded_Y_predict_trf)\ncm = confusion_matrix(dominant_y_valid , dominant_y_predict)\nplt.figure(2)\nplt.figure(figsize=(5,5))\nplot_confusion_matrix(cm, classes=class_names, title='Confusion matrix')","69102441":"# Create the base model from the pre-trained model MobileNet V2\nbase_model = tf.keras.applications.MobileNetV2(input_shape=X_train.shape[1:],\n                                               include_top=False,\n                                               weights='imagenet')","cd2cc405":"#To use weights in the pre-trained model\nbase_model.trainable = False \n\n#Define the pre-trained model\npretrained_model = tf.keras.Sequential([base_model,tf.keras.layers.GlobalAveragePooling2D(),tf.keras.layers.Dense(3, activation=\"softmax\")])\n\npretrained_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01), loss=tf.keras.losses.categorical_crossentropy, \n                         metrics = [tf.keras.metrics.categorical_accuracy])\n\npretrained_model.summary()","056eb147":"#Fit the pretrained model to the  data\nhistory_trf = pretrained_model.fit(X_train, Y_train, epochs=5,batch_size=32 , \n                validation_data=(X_valid, Y_valid), class_weight=class_weights)","56b3a862":"#Display of the accuracy and the loss values\nplt.figure(0)\nplt.plot(history_trf.history['categorical_accuracy'], label='training accuracy')\nplt.plot(history_trf.history['val_categorical_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history_trf.history['loss'], label='training loss')\nplt.plot(history_trf.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","14ff5be9":"#Mobile Net V2 Prediction\ny_prediction_trf = pretrained_model.predict(X_valid) # make predictions\n\n#Baseline Results\ndominant_y_valid=np.argmax(Y_valid, axis=1)\ndominant_y_predict=np.argmax(y_prediction_trf, axis=1)\n\nmodel_results('Mobile Net V2',Y_valid, y_prediction_trf,dominant_y_valid,dominant_y_predict)","ff952a77":"print(classification_report(dominant_y_valid , dominant_y_predict))","294c566c":"#Confusion Matrix\nimport itertools\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=75) \n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        \n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nclass_names = range(3)\n# cm = confusion_matrix(rounded_Y_valid , rounded_Y_predict_trf)\ncm = confusion_matrix(dominant_y_valid , dominant_y_predict)\nY_valid, y_predict_trf\nplt.figure(2)\nplt.figure(figsize=(5,5))\nplot_confusion_matrix(cm, classes=class_names, title='Mobile Net V2 Confusion matrix')","e476697b":"image_gen = ImageDataGenerator(\n    #featurewise_center=True,\n    #featurewise_std_normalization=True,\n    rescale=1.\/255,\n    rotation_range=15,\n    width_shift_range=.15,\n    height_shift_range=.15,\n    horizontal_flip=True)\n\n#training the image preprocessing\nimage_gen.fit(X_train, augment=True)","80c7d85c":"#Subject the model to training with pretrained model\nhistory_idg = pretrained_model.fit_generator(train_generator,\n                                   epochs = 10,\n                                   shuffle = False, \n                                   steps_per_epoch=3,\n                                   validation_steps=1,\n                                   validation_data=val_generator,\n                                   class_weight=class_weights)","d249df16":"#Display of the accuracy and the loss values\nplt.figure(0)\nplt.plot(history_idg.history['categorical_accuracy'], label='training accuracy')\n# plt.plot(history_idg.history['val_categorical_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history_idg.history['loss'], label='training loss')\n# plt.plot(history_idg.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","e629b4a4":"# Prediction\ny_prediction_idg = pretrained_model.predict(X_valid) # make predictions\n\nlogloss = log_loss(Y_valid, y_prediction_idg)\nlogloss","3bd89bef":"#Function to upload the test images\ndef upload_test_images(image_path, height, width):\n    test_images = []\n    test_image_paths = []\n        # Retrieve the filenames from the all the test directory\n    test_image_file_names = [os.path.join(image_path, f) for f in os.listdir(image_path)]\n        # Read the image pixels\n    for file in test_image_file_names:\n        test_image=io.imread(file)\n        # Append image into list\n        test_image_from_array = Image.fromarray(test_image, 'RGB')\n        #Resize image\n        test_size_image = test_image_from_array.resize((height, width))\n        #Append image into list\n        test_images.append(np.array(test_size_image))\n        test_image_paths.append(file)\n    return test_images,test_image_paths\n\n## Invoke the function\n#Image resize parameters\nheight = 256\nwidth = 256\ntest_images,test_image_paths = upload_test_images('\/kaggle\/input\/cgiar-computer-vision-for-crop-disease\/ICLR\/test\/test\/',height,width)\ntest_images = np.array(test_images)","159b06d3":"#Size and dimension of test image\nprint(\"Shape of test images is \" + str(test_images.shape))\n# Check image paths\ntest_image_paths[0:5]","ff2b1d99":"# use regular expressions to extract the name of image\nimage_names = []\nfor i in test_image_paths:\n#     name = i\n    i = re.sub(\"[^A-Z0-9]\", \"\", str(i))\n    i = i.replace(\"JPG\", \"\")\n    i = i.replace(\"PNG\", \"\")\n    i = i.replace(\"JPEG\", \"\")\n    i = i.replace(\"JFIF\", \"\")\n    i = i.replace(\"JFIF\", \"\")\n    image_names.append(i)\n\n#View images\nimage_names[0:5]","06a1d51f":"#Prediction for all images\ny_prediction = model.predict_proba(test_images) # make predictions\ny_prediction[400:500]","60eba4cb":"# Prediction for all images per test image\ntest_images = np.array(test_images)\npreds = []\nfor img in tqdm(test_images):\n    img = img[np.newaxis,:] # add a new dimension\n    prediction = pretrained_model.predict_proba(img) # make predictions predict_proba\n    preds.append(prediction) \npreds","04805b0d":"#healthwheat =0 stem_rust = 2 ,leaf_rst =1\n# create a dummy dataset\nhealthy_wheat = pd.Series(range(610), name=\"healthy_wheat\", dtype=np.float32)\nstem_rust = pd.Series(range(610), name=\"stem_rust\", dtype=np.float32)\nleaf_rust = pd.Series(range(610), name=\"leaf_rust\", dtype=np.float32)\nsubmission = pd.concat([healthy_wheat,stem_rust,leaf_rust], axis=1)\n\nfor i in range(0 ,len(preds)):\n    submission.loc[i] = preds[i]","6378e97b":"#Append the image names to the result output\nsubmission[\"ID\"] = image_names","5b17a62e":"submission.head(10)","8b1c3474":"cols = submission.columns.tolist()\ncols = cols[-1:] + cols[:-1]\nsubmission = submission[cols]","f6b30cbe":"submission.columns","ae64aff6":"submission[submission['ID'] == 'ICLRELRIT5']","b819addb":"submission['ID1'] = submission['ID']","3d726f6b":"submission['ID'] = submission['ID'].str[1:]","99ebb920":"# write to csv\nsubmission.to_csv(\"sub.csv\", index=False)","f36f307a":"b) Train The model","1d44e1f0":"Transfer learning results","80806ff4":"Count images and their corresponding class","a7af0bdc":"### Explore Images in Directories","abdd6c47":"## Modelling","d32fe1b3":"### Challenges\n\na) Huge data size of images. Took 15.5GB out of 16GB which caused kernel to crash.\n\nb) Mixed up images by Zindi hence had to reload the dataset.","c4a91f57":"#### c) Split the test and validation.\n\nThe validation set will be used to test overfitting in our model. The test images cannot be used as they do not have labels.**","34e846e8":"### 6.2 Image Data Augmentation\n\nWe will generate more image data using ImageDataGenerator. The Image data generator package artificially creates training images through different ways of processing or combination of multiple processing, such as random rotation, shifts, shear and flips, etc.","b4804457":"c) Fitting","9b5c894c":"### References\n\nhttps:\/\/zindi.africa\/competitions\/sbtic-animal-classification\/data","ff9bde0f":"Create Dataframe from predictions and Columns above","e12ae3f0":"Convert the encoded dependent values to categorical types. Reason is because ANN works best with categorical values","f9f7f5cf":"### c) Upload Training images upload","b92fb2d0":"Visualize the images distribution per label","576b4d85":"Create a combined directory with all the train and validation images used earlier for training. We will need to do final training on all images","814384e8":"Image Augmentation Results","172e9e03":"## 7.0 Subject the model to test data\n\na) Import the test data from test directory","54fae967":"#### Image Augmentation","e6802285":"Improvement on AUC represents degree or measure of separability. \nIt tells how much model is capable of distinguishing between classes. Higher the AUC, implies the model is  better at distinguishing between the wheat with step or leaf or is healthy\n\nLow Log Loss means a low uncertainty of your model.\n","fbd589e2":"Prediction for one image","31628d28":"## 5.0 Baseline Model","cb8c421c":"### Improvements\nClass weight\n\nTransfer learning\n\nMTL\n\nAdaptive images --sic\n\nOversampling\/downsampling\n\nHaving a validation set\n\nEnsemble in image classification\n\n\nTraining augmentations\n\nRandom resized crop preserving aspect with scale ~ uniform(0.5, 1) using nearest-neighbor interpolation\n\nRandom horizontal and vertical flip, and 90 degrees rotation\n\nNormalizing each image channel to N(0, 1)\n\nFor each channel: channel = channel * a + b, where a ~ N(1, 0.1), b ~ N(0, 0.1)\n\nTest-time augmentations\n\nHorizontal and vertical flip, and 90 degrees rotation\n\nhttps:\/\/www.kaggle.com\/c\/recursion-cellular-image-classification\/discussion\/110457\nrandom crop 384x384,\nrandom flip,\nrandom rotation multiple of 90 degree\n\nData augmentation\nhttps:\/\/www.kaggle.com\/c\/recursion-cellular-image-classification\/discussion\/110337\n\n\nhttps:\/\/www.hackerearth.com\/practice\/machine-learning\/advanced-techniques\/winning-tips-machine-learning-competitions-kazanova-current-kaggle-3\/tutorial\/\nImage classification: Here you can do scaling, resizing, removing noise (smoothening), annotating etc\n\nSTEPS\nload train and test datasets\nsetup train\/test image transforms\nsetup train\/test data loaders","6f2161e0":"b) Display batch images","0a1819c4":"Above shows that the data is balanced","f78cfc29":"Execute the model","db99887f":"Check main directory","3251e20f":"### Optimize and compile the model\n\nOPTIMIZER: ADAM applied to minimize the loss function.\n\nLOSS: categorical_crossentropy - multi-class log loss\n\nMetrics: Categorical accuracy as it's classification problem","c905a87b":"## 4.0 Images Pre-processing\n\nIn addition to images resizing done during importation, below preparation activities done before modelling.","a5a650e0":"### Transfer Learning : VGG 16","10f34289":"#### Confusion Matrix","7a99924a":"#### b) Normalization\n\nBenefits of normalization\n1. Reduce the effect of illumination's differences.\n2. CNN converges faster on [0..1] data than on [0..255].","1e808cf1":"### d) Categories of Training Images","bf3e9170":"#### b) Classification Report","d9eee718":"Map labels to prediction","d92c70eb":"### Conclusion\n\nActions\n\na) To upload the above submission on zindi so as to get the results of the test data.\n\nb) Optimize the combined data optimization and transfer learning model.\n\nc) Consider other transfer models e.g Resnet\n\nd) The stem rust and leaf rust conflicts in the model noted. Consider re-running the model with higher resolution with batch uploads.","2c3c159f":"Copy images into the combined directory","b8565990":"#### Mobile Net V2 Transfer Running Results\n#### a) AUC and Log Loss","d0a2158f":"### Baseline CNN Model. \n\n3 Layer CNN with 3 by 3 filter and relu activation function.","5d098c12":"Load Test Data","45cb348d":"Baseline output","71409f72":"#### a) Label Encoding. \n\nThe train labels are string variables of two types. These will be encoded to convert them to numerical variables","08e3b031":"Training.","ffc15a8c":"Baseline Model Accuracy","c4288e51":"## 6.0 Challenging the solution\n### 6.1 Transfer Learning : Model to use is MobileNetV2\n\nWith transfer learning, instead of starting the learning process from scratch, you start from patterns that have been learned when solving a different problem. This way you leverage previous learnings and avoid starting from scratch.\n\nMore about MobileNetV2 here  - > https:\/\/ai.googleblog.com\/2018\/04\/mobilenetv2-next-generation-of-on.html\n\na) Import the MobileNetV2 from keras","2f7d54fa":"Image name is part of full image URL as above. We will seperate the name from the image path as below","56eb22bf":"Predict whole test set","ff22395e":"## 2.0 Libraries Importation","27fadc5b":"### Define the CNN model\nConvolutional Neural Networks algorith was designed to map image data to an output variable hence is the best algorithm to use.\n\nThe benefit of using CNNs is their ability to develop an internal representation of a n-dimensional image. This allows the model to learn position and scale across different images, which is important when working with images.","1cefbf83":"### Train on whole dataset i.e both train and validation. To apply transfer learning and image augmentation model","48d565cc":"## Subject the Model to Test Data","43f48271":"Create a dataframe with images","749c2df3":"### c) Display sample training images\n\na) Individual images","774c8491":"Graph of accuracy and loss for training and validation","916b8ada":"Check Sample Predicted Image Visually","28076e42":"# 1.0 Standard Bank Tech Impact Challenge: Animal classification\n\n## Defining the question\n\n### Specifying the Question\n\nThe objective of the challenge is to create a machine learning model to accurately predict the likelihood that an image contains a zebra, as opposed to an elephant. \n\nChallenge: https:\/\/zindi.africa\/competitions\/sbtic-animal-classification\/data\n\n\n### Metric for success\n- Log loss\n\n### Understanding the context\n\nTotal dataset contains 18,000+ images of zebras and elephants, sampled from the Snapshot Serengeti collection of more than 6 million animals. The data was retrieved from the Data Repository for the University of Minnesota, https:\/\/doi.org\/10.13020\/D6T11K, under a creative commons license, from a study titled: Camera Trap Images used in \"Identifying Animal Species in Camera Trap Images using Deep Learning and Citizen Science\".*\n\n### Recording the experimental design\n\nCRISP- DM methodology will be applied. Below steps will be undertaken to create the classifer.\n\n- Business understanding - understanding the background\n- Data understanding \n- Exploratory data analysis\n- Feature engineering\n- Data modelling\n- Model interpretation\n\n### Data relevance\n","ca353bd9":"How many images in each of the directories"}}