{"cell_type":{"20f80e20":"code","733168ce":"code","e9afd9d2":"code","f6a63b8a":"code","85434181":"code","9cc93664":"code","583cc27c":"code","7b04c1e7":"code","db50c6c3":"code","08ad6b30":"code","d0ae2f8a":"code","1ac3a8e4":"code","82221461":"code","02bcfd1f":"code","d77e7aae":"code","c173d387":"code","40b468e9":"code","ca6137bb":"code","3fa617e8":"code","a89515be":"code","77c524d6":"code","67dc5887":"code","ffc26f2a":"code","c10b8f80":"code","f18729d5":"code","99919fbb":"code","eb79f106":"code","2996e2ec":"code","b8f9d1f3":"code","8d6bcbeb":"markdown","7f80cc71":"markdown","f42a18c9":"markdown","42f4ba47":"markdown","ad327e70":"markdown","88d8c68d":"markdown","10d8c163":"markdown","c8daf428":"markdown","d8f757d5":"markdown","7f02d142":"markdown","9d36971e":"markdown","6c20f580":"markdown","22717025":"markdown","ade44268":"markdown","f5204674":"markdown","d049b1e5":"markdown","8a9e4726":"markdown","bd88b055":"markdown","ad395646":"markdown","91d5fbfe":"markdown","47cf1c87":"markdown","d6e0aa55":"markdown","33a4f4a3":"markdown"},"source":{"20f80e20":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = [18, 8]","733168ce":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('hqFHAnkSP2U', width=800, height=450)","e9afd9d2":"rating_df = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/rating.csv', parse_dates=['timestamp'], dtype={'userId': 'uint32', 'movieId': 'uint32', 'rating': 'float32'})\nmovie_df = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/movie.csv', dtype={'movieId': 'uint32'})\n\nrating_df.shape, movie_df.shape","f6a63b8a":"rating_df['gave_rating_year'] = rating_df['timestamp'].dt.year\nrating_df['gave_rating_month'] = rating_df['timestamp'].dt.month_name().str[:3]\n\nrating_df.drop('timestamp', axis=1, inplace=True)","85434181":"rating_df['sentiment_analysis'] = rating_df['rating'].map({\n    0.5: 'Negative', 1.0: 'Negative', 1.5: 'Negative', 2.0: 'Negative', 2.5: 'Negative',\n    3.0: 'Neutral', 3.5: 'Neutral',\n    4.0: 'Positive', 4.5: 'Positive', 5.0: 'Positive'\n})","9cc93664":"rating_df.head(3)","583cc27c":"movie_df.head(3)","7b04c1e7":"final_df = pd.merge(rating_df, movie_df, on='movieId', how='inner')  # by default \u2018inner\u2019\n\nfinal_df.shape","db50c6c3":"final_df.head()","08ad6b30":"movie_genres = []\n\nfor genre in movie_df['genres']:\n    for movie in genre.split('|'):\n        movie_genres.append(movie)","d0ae2f8a":"genre_counts = pd.Series(movie_genres).value_counts()[:18]","1ac3a8e4":"from wordcloud import WordCloud\n\ngenres_cloud = WordCloud(width=800, height=400, background_color='white', colormap='magma')\ngenres_cloud.generate_from_frequencies(genre_counts)\n\nplt.imshow(genres_cloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","82221461":"top_7_genres = pd.Series(movie_genres).value_counts()[:7]\n\nsns.barplot(y=top_7_genres.index, x=top_7_genres.values, palette='magma').set_title(\n        'Top-7 Movie Genres', fontsize=14, weight='bold')\n\nplt.show()","02bcfd1f":"splot1 = sns.countplot(final_df['sentiment_analysis'])\n\nfor p in splot1.patches:\n                splot1.annotate(format(p.get_height() \/ final_df['rating'].shape[0] * 100, '.1f'),\n                                (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                               rotation=0, ha='center', va='bottom', xytext=(0, 10), textcoords='offset points')\n        \nplt.xlabel(None)\nplt.title('User Sentiment Analysis (%)', fontsize=20, weight='bold')\nplt.show()","d77e7aae":"def plot_progress_year(feature, title):\n    rating_progress = final_df.groupby(feature)['userId'].count()\n    \n    plt.plot(rating_progress, linestyle='-', marker='o', markersize=10)\n    plt.title(title, fontsize=16)\n    plt.xticks([1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015], rotation=30)\n    plt.ylim([0, 2100000])\n    plt.show()\n\nplot_progress_year('gave_rating_year', 'Progress rating count per year')","c173d387":"df_index = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nmonth_progress = final_df.groupby('gave_rating_month').userId.count().reindex(df_index)\n\nplt.plot(month_progress, linestyle='-', color='red', marker='o', markersize=10)\nplt.title('Progress rating count by month')\nplt.ylim([1250000, 2100000])\nplt.show()","40b468e9":"genres_str = movie_df['genres'].str.split('|').astype(str)","ca6137bb":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0)\ntfidf_matrix = tfidf.fit_transform(genres_str)\n\ntfidf_matrix.shape  # banyak karena n-gram (1,2)\n# tfidf.get_feature_names()","3fa617e8":"# Since we have used the TF-IDF Vectorizer, calculating the Dot Product will directly give us the Cosine Similarity Score\nfrom sklearn.metrics.pairwise import linear_kernel\n\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\ncosine_sim[:4, :4]","a89515be":"indices = pd.Series(movie_df.index, index=movie_df['title'])\n\n# Function that get movie recommendations based on the cosine similarity score of movie genres\ndef genre_recommendations(title, similarity=False):\n    \n    if similarity == False:\n        \n        idx = indices[title]\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        sim_scores = sim_scores[1:11] # you can change to 20 movies, even more\n    \n        movie_indices = [i[0] for i in sim_scores]\n    \n        return pd.DataFrame({'Movie': movie_df['title'].iloc[movie_indices].values})\n    \n    \n    elif similarity == True:\n        \n        idx = indices[title]\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        sim_scores = sim_scores[1:11]\n        \n        movie_indices = [i[0] for i in sim_scores]\n        similarity_ = [i[1] for i in sim_scores]\n        \n        return pd.DataFrame({'Movie': movie_df['title'].iloc[movie_indices].values,\n                             'Similarity': similarity_})","77c524d6":"genre_recommendations('Kung Fu Panda (2008)', similarity=True)","67dc5887":"genre_recommendations(\"Indiana Jones and the Temple of Doom (1984)\", similarity=True)","ffc26f2a":"movie_df['movie_release_year'] = movie_df['title'].str.extract(r'(?:\\((\\d{4})\\))?\\s*$', expand=False)","c10b8f80":"## option 2\n\ndef genre_recommendations_2(title, most_recent=False):\n    \n    if most_recent == False:\n        \n        idx = indices[title]\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        sim_scores = sim_scores[1:10] # you can change to 20 movies, even more\n    \n        movie_indices = [i[0] for i in sim_scores]\n    \n        return pd.DataFrame({'Movie': movie_df['title'].iloc[movie_indices].values})\n    \n    \n    elif most_recent == True:\n        \n        idx = indices[title]\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        sim_scores = sim_scores[1:14]\n        \n        movie_indices = [i[0] for i in sim_scores]\n        \n        most_recent_movie = pd.DataFrame({'Movie': movie_df['title'].iloc[movie_indices].values,\n                                          'release_year': movie_df['movie_release_year'].iloc[movie_indices].values})\n        \n        return most_recent_movie.sort_values('release_year', ascending=False).head(10)","f18729d5":"genre_recommendations_2('Green Hornet, The (2011)', most_recent=True)","99919fbb":"rating_mean = final_df.groupby('title')['rating'].mean().reset_index()\ntotal_rating = final_df.groupby('title')['rating'].count().reset_index()\n\ntotal_rating_mean = pd.merge(rating_mean, total_rating, on='title', how='inner')\ntotal_rating_mean.rename(columns={'rating_x': 'rating_mean',\n                                  'rating_y': 'total_rating'},\n                                  inplace=True)\n\nfinal_df2 = movie_df.merge(total_rating_mean, on='title', how='left').dropna()","eb79f106":"## option 3\n\ndef genre_recommendations_3(title, best_rating=False):\n    \n    if best_rating == False:  # sort by total rating\n        \n        idx = indices[title]\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        sim_scores = sim_scores[1:14]\n    \n        movie_indices = [i[0] for i in sim_scores]\n        \n        most_rating_movie = pd.DataFrame({'Movie': final_df2['title'].iloc[movie_indices].values,\n                                          'total_rating': final_df2['total_rating'].iloc[movie_indices].values})\n    \n        return most_rating_movie.sort_values('total_rating', ascending=False).head(10)\n    \n    \n    elif best_rating == True:  # sort by best rating\n        \n        idx = indices[title]\n        sim_scores = list(enumerate(cosine_sim[idx]))\n        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n        sim_scores = sim_scores[1:14]\n        \n        movie_indices = [i[0] for i in sim_scores]\n        \n        most_recent_movie = pd.DataFrame({'Movie': final_df2['title'].iloc[movie_indices].values,\n                                          'rating_mean': final_df2['rating_mean'].iloc[movie_indices].values})\n        \n        return most_recent_movie.sort_values('rating_mean', ascending=False).head(10)","2996e2ec":"genre_recommendations_3('Taken (2008)', best_rating=True)","b8f9d1f3":"genre_recommendations_3('Taken (2008)', best_rating=False)","8d6bcbeb":"# What's the problem in real world today?\n\n![too many choices](https:\/\/sitavriend.files.wordpress.com\/2017\/06\/551615-supermarket.jpg)\n*[Source](https:\/\/sitavriend.files.wordpress.com\/2017\/06\/551615-supermarket.jpg)*\n\n* Nowadays, people have **too many options** for selecting the items they actually want \/ need\n* Try to calculate how much time & energy we've wasted just to find a video, online goods (e-commerce), jobs, even friends what you really want all by yourself?\n* But if you have a friend who really **understands what you need**, also really **understands the product**, he \/ she can give excellent recommendation to you\n* This makes your life easier right?\n\n> ### We need a Recommendation System that can help us to find something what we really need","7f80cc71":"### Another options:\n\n* Sort by rating mean to recommend movies with a high rating\n* Sort by total rating","f42a18c9":"# Data Cleaning\n    \n* Both datasets don't have missing or duplicated values\n* Add new features using datetime for analysist & EDA","42f4ba47":"### Problem in this recommendation:\n* This system only recommend similar movie\n* But remember, a few years can make a big difference in quality, especially in sci-fi or animation movies\n* Maybe some user can think this is a bad recommendation system, because give them an old movies recommendation\n\n> ### Perhaps you like old movies like me, but some people doesn't like old movies, so it's depends on your users","ad327e70":"* We can sort by movie_release_year to give a better recommendation\n* Even system still recommend this old movies, but we can put it behind a new movie\n* In top-n recommender (only recommend 5 - 10 items per row), users only see newest movie ","88d8c68d":"* Now we have a pairwise cosine similarity matrix for all the movies in the dataset.\n* The next step is to write a function that returns the 10 most similar movies based on the cosine similarity score.","10d8c163":"# Make a Movie Recommendation\n* With 3 different solutions","c8daf428":"# Types of Recommendation Engines:\n\n## 1. Content-Based\n\n![Content-based](https:\/\/miro.medium.com\/max\/1000\/0*lhH61OnvbJFWrykk.png)\n[Image Source - Analytics Vidhya in Medium](https:\/\/medium.com\/analytics-vidhya\/content-based-recommender-systems-in-python-2b330e01eb80)\n\nThe Content-Based Recommender relies on the similarity of the items being recommended. The basic idea is that if you like an item, then you will also like a \u201csimilar\u201d item. It generally works well when it's easy to determine the context\/properties of each item.\n\n\nContent-Based method also solve **Cold Start problem**. If we're visiting an e-commerce \/ video platform for the first time, they won't know anything about us. So how can they give a great recommendation for us?\n\n### Basic solution:\n\n1. First, recommend the best selling products or some latest releases\n1. After we see \/ click on some product, they will pick products with similar content to recommend us","d8f757d5":"# Load Dataset with reduce memory technique\n\n* uint8\tUnsigned integer (0 to 255)\n* uint16\tUnsigned integer (0 to 65535)\n* uint32\tUnsigned integer (0 to 4294967295)\n* uint64\tUnsigned integer (0 to 18446744073709551615)\n\n> userId & movieId don't have a negative values, use uint (no negative value) can help us to reduce memory, since we use a large dataset","7f02d142":"# Merge","9d36971e":"* If you like old movies, maybe this dataset can give good recommendation for you.\n* But this might not be suitable for those of you who like new movies","6c20f580":"### Overall, here are the pros of using content-based recommendation:\n\n1. No need for data on other users, thus no cold-start or sparsity problems.\n1. Can recommend to users with unique tastes.\n1. Can recommend new & unpopular items.\n1. Can provide explanations for recommended items by listing content-features that caused an item to be recommended (in this case, movie genres)\n\n### However, there are some cons of using this approach:\n\n1. Finding the appropriate features is hard.\n1. Does not recommend items outside a user's content profile.\n1. Unable to exploit quality judgments of other users.","22717025":"# Study Case\n\n## 1. Netflix\n\n* Recommendations **increases** users engagement with the product and **decrease** subscription cancellations rates.\n* This allows them to invest more money on new content **(rather than do advertising)**, which viewers will continue to view, giving them a good ROI.\n* According to [their paper](https:\/\/dl.acm.org\/doi\/pdf\/10.1145\/2843948), this system saves the company around $1 billion each year.","ade44268":"### From total 138493 users\n* 50% users gave positive rating\n* 32.5% users are neutral \n* 17.6% users gave negative rating","f5204674":"## 2. Amazon\n\n* At Amazon.com, we use recommendation algorithms to personalize the online store for each customer.\n* The store radically changes based on customer interests.\n* For example: showing programming titles to a software engineer and baby toys to a new mother\n\n### Recommendation engines play a role not only in helping customers find more of what they need, but these systems also improve cart value.\n> ### If Amazon doesn\u2019t have to pay much more for shipping to send you two or three times as many products, their profit margins improve.","d049b1e5":"# Visualization","8a9e4726":"# Recommender Systems part 1\n    \n* Welcome to my kernel, this is the first notebook for my final project series after finished 3 month Data Science class in Purwadhika Jakarta.\n* I make it simple for presentation purpose","bd88b055":"> ### The problem of this method: it will always be limited to the same type of movie the user see in the past","ad395646":"* Many people watch movies at near the end of the year\n* Perhaps because they get a holiday, or a lot of good movies come out at that time","91d5fbfe":"## Why Recommendation systems are important and valuable tools for business?\n\n![Netflix](https:\/\/miro.medium.com\/max\/2000\/1*dMR3xmufnmKiw4crlisQUA.png)\n\n* 35 percent of what consumers purchase on Amazon\n* 40 percent of app installs on Google Play\n* 60 percent of what people watch on YouTube\n* 75 percent of what they watch on Netflix\n* Are come from their product recommendations systems!!","47cf1c87":"# Machine Learning model (TF-IDF vectorizer + Cosine Similarity)\n\n## TF-IDF\n\n![tfidf](https:\/\/miro.medium.com\/max\/1200\/1*V9ac4hLVyms79jl65Ym_Bw.jpeg)\nsource: [medium](https:\/\/medium.com\/@ted_mei\/demystify-tf-idf-in-indexing-and-ranking-5c3ae88c3fa0)\n\n* TF is simply the frequency of a word in a document\n* IDF is the inverse of the document frequency among the whole corpus of documents\n\nExample: Suppose we search for \u201cthe results of latest European Socccer games\u201d on Google. It is certain that \u201cthe\u201d will occur more frequently than \u201csoccer games\u201d but the relative importance of **soccer games** is higher than **the** search query point of view\n\n> ### In such cases, TF-IDF weighting negates the effect of high frequency words in determining the importance of an item (document).","d6e0aa55":"# Reference\n    \n* [James Le, \"The 4 Recommendation Engines That Can Predict Your Movie Tastes\"](https:\/\/medium.com\/@james_aka_yale\/the-4-recommendation-engines-that-can-predict-your-movie-tastes-bbec857b8223)\n* [Corinna Underwood, \"Use Cases of Recommendation Systems in Business\"](https:\/\/emerj.com\/ai-sector-overviews\/use-cases-recommendation-systems\/)","33a4f4a3":"## Cosine-Similarity\n\n* Cosine similarity takes the angle between two non-zero vectors and calculates the cosine of that angle, and this value is known as the similarity between the two vectors\n* This similarity score ranges from 0 to 1, with 0 being the lowest (the least similar) and 1 being the highest (the most similar).\n* If the angle between two vectors is 0\u00b0, then the similarity would be 1. Conversely, if the angle between two vectors is 90\u00b0, then the similarity would be 0. For two vectors with an angle greater than 90\u00b0, then we also consider those to be 0.\n\n![cosine](https:\/\/www.machinelearningplus.com\/wp-content\/uploads\/2018\/10\/3d_projection-865x922.png)\nsource: [machinelearningplus](https:\/\/www.machinelearningplus.com\/nlp\/cosine-similarity\/)\n\n> ### The smaller the angle, higher the cosine similarity\n\n1. As you can see, **Doc Dhoni_Small** and the main **Doc Dhoni** are oriented closer together in 3-D space, even though they are far apart by magnitiude.\n1. It\u2019s important to note that only the angle between the two vectors is considered, and not the magnitude of the vectors."}}