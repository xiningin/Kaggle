{"cell_type":{"d8493fca":"code","bde3c7fe":"code","01f26781":"code","a3e39a11":"code","65c2212e":"code","009edb88":"code","1be91d67":"code","21fa375c":"code","3a438680":"code","088f6ba2":"code","0c677e7e":"code","38b5952a":"code","f6c38d5b":"code","a81aac7d":"code","2ec2f917":"code","b1da1d6e":"code","cc427647":"code","c21580ba":"code","cba6bc1c":"code","fefc7af2":"code","4e10e279":"code","8da5e3ef":"code","6d6c0707":"code","24fd28bd":"markdown","fdc6a91f":"markdown","23f3b3e0":"markdown","3c86c84f":"markdown","a512074e":"markdown","bedd0c13":"markdown","a3d238d4":"markdown","4a363f4f":"markdown","5d6d53fe":"markdown","9266334f":"markdown","d7552c6b":"markdown","f0e34687":"markdown","7df9f2f9":"markdown","b166fd47":"markdown","99cb83d4":"markdown","15713a7c":"markdown","0938a0d4":"markdown","32db911c":"markdown","752f6863":"markdown"},"source":{"d8493fca":"import numpy as np \nimport pandas as pd","bde3c7fe":"reddata = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","01f26781":"reddata","a3e39a11":"df = reddata\ndf.describe().T","65c2212e":"df.isnull().sum()","009edb88":"reddata.head()","1be91d67":"gb = reddata.groupby('quality')\nprint(gb.first())","21fa375c":"print(reddata['quality'].head())","3a438680":"import csv\nimport random\nimport math\nimport operator","088f6ba2":"## only in order to do feature scaling we are using scikit-learn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler","0c677e7e":"sc1 = MinMaxScaler()\nsc2 = StandardScaler()\n\ndef loadDataset(filename, split, trainingSet=[] , testSet=[]):\n\twith open(filename, 'r') as csvfile:\n\t    lines = csv.reader(csvfile)\n\t    dataset = list(lines)\n\t    for x in dataset[1:]:\n\t        if float(x[-1]) >6.5:\n\t            x[-1] = 1\n\t        else: \n\t            x[-1] = 0\n\t    for x in range(1,len(dataset)-1):\n\t        for y in range(11):\n\t            dataset[x][y] = float(dataset[x][y])\n\t        if random.random() < split:\n\t            trainingSet.append(dataset[x])\n\t        else:\n\t            testSet.append(dataset[x])\n\n\ndef euclideanDistance(instance1, instance2, length):\n\tdistance = 0\n\tfor x in range(length):\n\t\tdistance += pow((instance1[x] - instance2[x]), 2)\n\treturn math.sqrt(distance)\n\ndef chebyshevDistance(instance1, instance2, length):\n\tdistance = []\n\tfor x in range(length):\n\t\t#distance += pow((instance1[x] - instance2[x]), 2)\n\t\tdistance.append(abs(instance1[x] - instance2[x]))\n\treturn max(distance)\n\ndef getNeighbors(trainingSet, testInstance, k):\n\tdistances = []\n\tlength = len(testInstance)-1\n\tfor x in range(len(trainingSet)):\n\t\tdist = euclideanDistance(testInstance, trainingSet[x], length)\n\t\tdistances.append((trainingSet[x], dist))\n\tdistances.sort(key=operator.itemgetter(1))\n\tneighbors = []\n\tfor x in range(k):\n\t\tneighbors.append(distances[x][0])\n\treturn neighbors\n\ndef getNeighborsWithchebyshev(trainingSet, testInstance, k):\n\tdistances = []\n\tlength = len(testInstance)-1\n\tfor x in range(len(trainingSet)):\n\t\tdist = chebyshevDistance(testInstance, trainingSet[x], length)\n\t\tdistances.append((trainingSet[x], dist))\n\tdistances.sort(key=operator.itemgetter(1))\n\tneighbors = []\n\tfor x in range(k):\n\t\tneighbors.append(distances[x][0])\n\treturn neighbors\n\ndef getResponse(neighbors):\n\tclassVotes = {}\n\tfor x in range(len(neighbors)):\n\t\tresponse = neighbors[x][-1]\n\t\tif response in classVotes:\n\t\t\tclassVotes[response] += 1\n\t\telse:\n\t\t\tclassVotes[response] = 1\n\tsortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n\treturn sortedVotes[0][0]\n\ndef getAccuracy(testSet, predictions):\n\tcorrect = 0\n\tfor x in range(len(testSet)):\n\t\tif testSet[x][-1] == predictions[x]:\n\t\t\tcorrect += 1\n\treturn (correct\/float(len(testSet))) * 100.0\n\t\ndef main():\n    # prepare data\n    trainingSet=[]\n    testSet=[]\n    split = 0.67\n    loadDataset('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv', split, trainingSet, testSet)\n    print('Train set: %d' % len(trainingSet))\n    print('Test set: %d' % len(testSet))\n    predictions=[]\n    k = 3\n    print(\"Here we have taken : K=3  \")\n    \n    ## ACCURACY BEFORE FEATURE SCALING \n    for x in range(len(testSet)):\n        neighbors = getNeighbors(trainingSet, testSet[x], k)\n        result = getResponse(neighbors)\n        predictions.append(result)\n        #print('> predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))\n    accuracy = getAccuracy(testSet, predictions)\n    print('Accuracy before feature scaling is done: ' + str(accuracy) + '%')\n    \n    ## ACCURACY AFTER FEATURE SCALING \n    \n    ## ACCURACY AFTER MinMax way of scaling was done\n    trainingSet=[]\n    testSet=[]\n    predictions=[]\n    loadDataset('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv', split, trainingSet, testSet)\n    trainingSet = sc1.fit_transform(trainingSet)\n    testSet = sc1.transform(testSet)\n    for x in range(len(testSet)):\n        neighbors = getNeighbors(trainingSet, testSet[x], k)\n        result = getResponse(neighbors)\n        predictions.append(result)\n        #print('> predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))\n    accuracy = getAccuracy(testSet, predictions)\n    print('Accuracy after Min-Max scaler was used for feature scaling: ' + str(accuracy) + '%')\n    \n    ## ACCURACY AFTER StandardScaler was used for scaling\n    trainingSet=[]\n    testSet=[]\n    predictions=[]\n    loadDataset('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv', split, trainingSet, testSet)\n    trainingSet = sc2.fit_transform(trainingSet)\n    testSet = sc2.transform(testSet)\n    for x in range(len(testSet)):\n        neighbors = getNeighbors(trainingSet, testSet[x], k)\n        result = getResponse(neighbors)\n        predictions.append(result)\n    accuracy = getAccuracy(testSet, predictions)\n    print('Accuracy after StandardScaler was used for feature scaling: ' + str(accuracy) + '%')\n   \n\n    ## ACCURACY after chebyshev distance is used\n    trainingSet=[]\n    testSet=[]\n    predictions=[]\n    loadDataset('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv', split, trainingSet, testSet)\n    trainingSet = sc2.fit_transform(trainingSet)\n    testSet = sc2.transform(testSet)\n    for x in range(len(testSet)):\n        neighbors = getNeighborsWithchebyshev(trainingSet, testSet[x], k)\n        result = getResponse(neighbors)\n        predictions.append(result)\n    accuracy = getAccuracy(testSet, predictions)\n    print('Accuracy after we use chebyshev distance formula: ' + str(accuracy) + '%')\n   \n    \nmain()","38b5952a":"def collectallaccuracy(k):\n   \n    trainingSet=[]\n    testSet=[]\n    split = 0.67\n    loadDataset('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv', split, trainingSet, testSet)\n    trainingSet = sc2.fit_transform(trainingSet)\n    testSet = sc2.transform(testSet)\n   \n    predictions=[]\n    for x in range(len(testSet)):\n        neighbors = getNeighbors(trainingSet, testSet[x], k)\n        result = getResponse(neighbors)\n        predictions.append(result)\n    accur = getAccuracy(testSet, predictions)\n    return accur","f6c38d5b":"Accuracies = []\nprint('Let us see when standardScaler is used for feature scaling and euclidean distance is used \\nthen for different k what is the respective accuracy we obtain')\nprint()\nprint('For different K its accuracy is : ')\n\nfor i in range(1, 21):\n        val = collectallaccuracy(i)\n        Accuracies.append(val)\n        print('K = '+str(i)+' Accuracy = '+str(val))","a81aac7d":"print(\"Max Accuracy we get is = \"+str(max(Accuracies))+\" at k = \"+ str(Accuracies.index(max(Accuracies))+1))","2ec2f917":"import matplotlib.pyplot as plt \nplt.figure(figsize=(12, 6))\nplt.plot(range(1, 21),  Accuracies, color='green', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Relationship between K and its respective accuracy')\nplt.xlabel('K Value')\nplt.ylabel('Accuracy')","b1da1d6e":"from sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport numpy as np \nimport pandas as pd","cc427647":"redwine = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndf = redwine.copy()","c21580ba":"df['quality'] = [1 if x>6.5 else 0 for x in df['quality']]\ny = df[\"quality\"]\nX = df.drop(\"quality\", axis=1)\nX = pd.DataFrame(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=40, stratify=y)","cba6bc1c":"knn_model = KNeighborsClassifier().fit(X_train, y_train)\ny_pred = knn_model.predict(X_test)\naccuracy_score(y_test, y_pred)","fefc7af2":"knn_cv = GridSearchCV(KNeighborsClassifier(), {\"n_neighbors\": np.arange(1,50)}, cv=10)\nknn_cv.fit(X_train, y_train)","4e10e279":"print(\"Best score is:\" + str(knn_cv.best_score_),\"and Best params is: \" + str(knn_cv.best_params_))","8da5e3ef":"knn = KNeighborsClassifier(2)\nknn_tuned = knn.fit(X_train, y_train)","6d6c0707":"knn_tuned.score(X_test, y_test)","24fd28bd":"<center><h1>ML project on KNN<\/h1> <\/center>","fdc6a91f":"### DETAILS ABOUT THE RED WINE QUALITY DATASET","23f3b3e0":"### Let us see the steps for implementing KNN\nSteps given are: \n1. <b>Handle Data:<\/b> Open the dataset from CSV and split into test\/train datasets.\n2. <b>Similarity:<\/b> Calculate the distance between two data instances.\n3. <b>Neighbors:<\/b> Locate k most similar data instances.\n4. <b>Response:<\/b> Generate a response from a set of data instances.\n5. <b>Accuracy:<\/b> Summarize the accuracy of predictions.\n6. <b>Main:<\/b> Tie it all together.","3c86c84f":"# KNN Algorithm - Finding Nearest Neighbors \n## Dataset used :Red Wine Quality","a512074e":">","bedd0c13":">","a3d238d4":"___","4a363f4f":"________________________________________________________________________________________________________________________________","5d6d53fe":"\n\n\n## PART B: Implementation using scikit-learn","9266334f":">","d7552c6b":"\n\n\n## PART A: Implementation from scratch","f0e34687":">","7df9f2f9":"<center><h4> Project By: Swati Tripathi<\/h4> <\/center>","b166fd47":">","99cb83d4":">","15713a7c":"> This notebook is divided into two parts A and B which shows two different ways of implementing KNN <br>\n<b>PART A<b>: Implementation from scratch<br>\n<b>PART B<b>: Implementation using scikit-learn <br> ","0938a0d4":">","32db911c":"\n>","752f6863":"________________________________________________________________________________________________________________________________"}}