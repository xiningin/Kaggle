{"cell_type":{"4f6c70cd":"code","77f2684e":"code","6d13a8d5":"code","60de0859":"code","7498ccce":"code","d239e61e":"code","2bbd27f4":"code","c4ae5525":"code","2ef530b4":"code","1f64dddc":"code","51d52358":"code","9dc47471":"code","eeb57054":"code","81121fd3":"code","c97f56f2":"code","e62a09d6":"code","eead0b92":"code","419bc8cc":"code","8ab70561":"code","a27e8204":"code","3a524ce4":"code","8b430e09":"code","9dbb929f":"code","c2f42638":"code","8e20b3a4":"code","02d455f2":"code","1fd78212":"code","baf4deb4":"code","ce8a3fe5":"code","66a9c870":"code","111de37d":"code","b0848db1":"code","50f2e349":"code","a75e22ac":"code","b90c4bfd":"code","95d0fa80":"code","6c7f8556":"code","34f73f29":"code","6e072fd4":"code","b42f9863":"markdown","6a14da93":"markdown","8f6ed512":"markdown","e1e8d8a1":"markdown","146da70f":"markdown","c5fd631a":"markdown","82a1217d":"markdown","07be05ac":"markdown","b7826f06":"markdown","356e3049":"markdown","921d9142":"markdown","5926caab":"markdown","fb6fe233":"markdown","3ace9368":"markdown","c7060e4d":"markdown","6748a942":"markdown","dde5f68a":"markdown","982c73f5":"markdown","400d66af":"markdown","5209814e":"markdown","1ae5694e":"markdown","00f3392b":"markdown","2c45ed63":"markdown","91b1038b":"markdown","4d389c91":"markdown","7163cfed":"markdown","aefde5de":"markdown","13f1b958":"markdown","d8d7b802":"markdown","c43999d1":"markdown","de6b9e83":"markdown","ceeea84b":"markdown","1fde727d":"markdown","236bdb80":"markdown","88c2a125":"markdown","31dc504d":"markdown","fce081ab":"markdown","0bb80eab":"markdown","e2b87466":"markdown","52d97d6e":"markdown","4689b647":"markdown"},"source":{"4f6c70cd":"# Import Pandas and NumPy\nimport numpy as np \nimport pandas as pd\n\n# Import Libraries for plotting\nimport matplotlib.pyplot as plt, seaborn as sns\n\n# ignors Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","77f2684e":"wine_df = pd.read_csv('..\/input\/red-wine-quality\/winequality-red.csv')","6d13a8d5":"print('Shape of general_data_set :',wine_df.shape)\nwine_df.head()","60de0859":"wine_df.info()","7498ccce":"wine_df.describe()","d239e61e":"wine_df.isnull().sum()","2bbd27f4":"feature_ = wine_df.columns[:-1] # Skip the target variable 'quality'. Later, we weill plot it separately\n\ndef get_percentile(feature, q_range):\n    \"\"\"This function is used to calculate the percentile for the passed input feature present\n    in wine_df dataframe. After calculation, the value is rounded upto the second decimal place and returned.\n    Args:----\n        feature: the feature(present in wine_df dataframe) for which the quantile needs to be calculated.\n        q_range: used to define which percentile value (example - 50 for calculating 50th percentile(median))\n    returns:----\n    dist: the quantile value\n    \"\"\"\n    dist = wine_df[feature].describe()[str(q_range) + '%']\n    return round(dist,2)\n\ndef render_counterplot():\n    fig=plt.figure(figsize=(18, 20))\n    for column, feature in enumerate(feature_):\n        fig.add_subplot(4, 3, column + 1)\n        \n        q1 = get_percentile(feature, 25)\n        q2 = get_percentile(feature, 50)\n        q3 = get_percentile(feature, 75)\n          \n        sns.histplot(data=wine_df, x=feature, kde=True, color = '#c4bc1a')\n        \n        plt.axvline(q1, linestyle='--', color='green', label='Q1')\n        plt.axvline(q2, color='red', label='Q2 (Median)')\n        plt.axvline(q3, linestyle='--',  color='black', label='Q3')\n        plt.legend()\n        \n    plt.show()\n    \nrender_counterplot()","c4ae5525":"# Initialize a figure for plotting the Distribution of target class 'quality'\nplt.figure(figsize = (18,6))\n\n# Generate a countplot for the passed feature_name\nax1 = sns.countplot(x='quality', data=wine_df, palette='Set2')        \nplt.xlabel('Wine Quality',fontsize  = 14)\nplt.ylabel('Count',fontsize  = 14)\n\n# Make twin axis\nax2=ax1.twinx()\n\n# Switch so count axis is on right, frequency on left\nax2.yaxis.tick_left()\nax1.yaxis.tick_right()\n\n# Also switch the labels over\nax1.yaxis.set_label_position('right')\nax2.yaxis.set_label_position('left')\n\nax2.set_ylabel('Frequency [%]')\n\nfor p in ax1.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax1.annotate('{:.1f}%'.format(100.*y\/len(wine_df)), (x.mean(), y), ha='center', va='bottom')","2ef530b4":"features_ = wine_df.columns.values[:-1]\n\nfig=plt.figure(figsize=(16, 26))\nfor column, feature in enumerate(features_):\n    if feature != \"quality\":\n        fig.add_subplot(5, 3, column + 1)\n        sns.boxplot(data=wine_df, x=\"quality\", y=feature, color=\"#8585f2\", palette=\"bright\")\n    \nplt.show()","1f64dddc":"# Import and initialize randomforest model\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state=1, max_depth=12)\n\n# prepare training data for this model\nx = wine_df.drop(['quality'] , axis = 1)\n\n# Train Model\nrf.fit(x, wine_df.quality)\n\n# store importance and feature names\nfeatures = wine_df.columns    # store feature names\nimportances = rf.feature_importances_    # get feature importance score from randomforest model using .feature_importance_ attribute\nsorted_index = np.argsort(importances)[:]    # sort importance in descending order\n\n# Plot Feature Importance\nplt.figure(figsize=(18,6))\nplt.title('Feature Importances', fontsize= 14)\nplt.barh(range(len(sorted_index)), importances[sorted_index], color='green', align='center')\nplt.yticks(range(len(sorted_index)), [features[i] for i in sorted_index]) # set x-axis ticks as feature names\nplt.xlabel('Feature Importance', fontsize= 12)\nplt.show()\n","51d52358":"wine_df.drop(['free sulfur dioxide','citric acid'],axis=1,inplace=True)","9dc47471":"# Input Features\nX = wine_df.drop('quality', axis = 1)\n\n# Target Variable\ny = wine_df['quality']","eeb57054":"from sklearn.model_selection import train_test_split \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,stratify = y, random_state=42)\nprint(y_test)","81121fd3":"from sklearn.preprocessing import StandardScaler\n\n# Initialize StandardScaler\nscaler = StandardScaler()\n# standardize all columns of training and testing data\nX_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])\nX_test[X_train.columns] = scaler.transform(X_test[X_train.columns])\nprint(y_test)","c97f56f2":"# Save the training data in a separate df which will be used for finding coorelation\ndf_corr = X_train.copy()","e62a09d6":"# plot a heatmap of correlation in training data\nplt.figure(figsize = (18,12))\n\n# seaborn heatmap\nsns.heatmap(df_corr.corr(),cbar=True, annot=True) # annot=True shows value in each file of heatmap, cbar shows the colour scale\nplt.show()","eead0b92":"# model building....","419bc8cc":"X_train.head()","8ab70561":"# Import Synthetic minority over-sampling from imblearn libraray\nfrom imblearn.over_sampling import SMOTE","a27e8204":"smte = SMOTE(random_state=42)\nX_train_os, y_train_os = smte.fit_resample(X_train,y_train)  # Fit SMOTE with training data and re-sample the data to get balanced data.\n\nprint('Training data classes :5\\'s   6\\'s   7\\'s   4\\'s  8\\'s 3\\'s')\nprint('Before over-sampling   {}   {}   {}   {}   {}   {}'.format(list(y_train.value_counts().values)[0],list(y_train.value_counts().values)[1],list(y_train.value_counts().values)[2],list(y_train.value_counts().values)[3],list(y_train.value_counts().values)[4],list(y_train.value_counts().values)[5]))\nprint('After over-sampling    {}   {}   {}   {}  {}  {}\\n'.format(list(y_train_os.value_counts().values)[0],list(y_train_os.value_counts().values)[1],list(y_train_os.value_counts().values)[2],list(y_train_os.value_counts().values)[3],list(y_train_os.value_counts().values)[4],list(y_train_os.value_counts().values)[5]))\n\nprint('After Over-sampling {} synthetic records were added to the training data and now there are equal proportion of records from each class'.format(len(X_train_os)-len(X_train)))","3a524ce4":"print(y_train_os)\n# print(y_test)","8b430e09":"# Encode the target variable\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_train_os = le.fit_transform(y_train_os)\ny_test = le.transform(y_test)","9dbb929f":"# # Encode the target variable\n# from sklearn import preprocessing\n# le = preprocessing.LabelEncoder()\n# le.fit(y_test)\n# le.fit(y_train_os)\n# print(y_test)\n# print(y_train_os)\n# y_train_os = le.fit_transform(y_train_os)\n# y_test = le.transform(y_test)\n# # print(y_test)","c2f42638":"print(y_train_os .shape)\nprint(y_test.shape)","8e20b3a4":"# y_test=np.array(y_test).reshape(len(y_test),1)\n# y_train_os=np.array(y_train_os).reshape(len(y_train_os),1)\ny_train_os = pd.DataFrame(y_train_os.reshape(len(y_train_os),1))\ny_test = pd.DataFrame(y_test.reshape(len(y_test),1))","02d455f2":"import tensorflow as tf                     # use to_categorical function present in tensorflow library\ny_train_os = tf.keras.utils.to_categorical(y_train_os, 6)\ny_test = tf.keras.utils.to_categorical(y_test, 6)\n","1fd78212":"# Import Necessary Libraries\nfrom tensorflow.keras.models import Sequential\n\nfrom tensorflow.keras.layers import Input, Dense\nfrom keras.layers import Dropout","baf4deb4":"# Inputs (These inputs resemble an AND gate)\nx1 = np.array([[0,0], [0,1], [1,0], [1,1]], \"uint8\")\n\n# Target\ny1  = np.array([[0], [0], [0], [1]], \"uint8\")","ce8a3fe5":"# Define sequential model\nmodel = Sequential()\nmodel.add(Dense(10, input_dim= 2,kernel_initializer ='normal', activation= 'tanh'))\n# Add output layer, here we just need to add 1 output layer, with 2 dimensional input and sigmoid activation function\nmodel.add(Dense(units=1, activation='sigmoid',input_dim=2))\n\n# Compile model and define the loss function, optimizers, and metrics for prediction\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","66a9c870":"# Print model summary\nmodel.summary()","111de37d":"model.fit(x1, y1, epochs=500)","b0848db1":"print('Weights :',model.layers[0].get_weights()[0])\nprint('Bias    :',model.layers[0].get_weights()[1]) ","50f2e349":"# For 0 Condition of AND gate (0,0)\nprint('For 0 Condition, Model prediction :',model.predict(np.array([[0,0]], \"uint8\"))) # give the test data in an array format\n\n# For 1 Condition of AND gate (1,1)\nprint('For 1 Condition, Model prediction :',model.predict(np.array([[1,1]], \"uint8\")))","a75e22ac":"# Initialize a sequential model\nmodel2=Sequential()\n\n# add input layer with 9 neurons because we have 9 input features\nmodel2.add(tf.keras.layers.Input(shape = 9,))\n\n# add first hidden layer\nmodel2.add(tf.keras.layers.Dense(32,activation='relu'))\n\nmodel2.add(tf.keras.layers.Dense(64,activation='relu'))\nmodel2.add(tf.keras.layers.Dropout(0.3))\n\nmodel2.add(tf.keras.layers.Dense(64,activation='relu'))\nmodel2.add(tf.keras.layers.Dropout(0.3))\n\n# add output layer with 6 neurons as there are 6 categories in the output feature\nmodel2.add(tf.keras.layers.Dense(6,activation='softmax'))","b90c4bfd":"model2.summary()","95d0fa80":"model2.compile(optimizer='adam',loss='categorical_crossentropy',metrics= ['accuracy'])","6c7f8556":"model2.fit(X_train_os,y_train_os,batch_size=50,epochs=400, verbose=1)","34f73f29":"y_pred = model2.predict(X_test)\nprint(y_pred)","6e072fd4":"\ny_test = np.argmax(y_test,axis=1)\n# print(y_test)","b42f9863":"Split Features and Target","6a14da93":"#### Compile the prepared model  \n\nCompile defines the loss function, the optimizer and the performance metrics.","8f6ed512":"\nAccuracy is the proportion of correct model prediction.\n> **Accuracy  : 91.30 %**","e1e8d8a1":"## 5. Model Building  \nWe will be using  Artificial Neural Network (ANN) Model for this project. ","146da70f":"\n*     So for a sample of 100 red wines, model wil correctly predict the wine quality for 91 wines.\n*     there were 6 classes in the target variable, out of which 3 classes had a very few records (support signifies the number of records in testing data corresponding to each class).\n*     Even after over-sampling model performance for classes 0,1 was not good.\n*     For class 5, even though it was minority class, model performance was decent.\n","c5fd631a":"Standardize input columns using **standard scaler**","82a1217d":"overveiw of dataset","07be05ac":"No major pattern observed in bivariate analysis\n- The distribution of 'sulphates', 'alcohol' and 'citric acid' tend to increase with increasing wine quality. So, we can say that they are positively related.\n- The distribution of 'volatile acidity', 'density' and 'ph' tend to decrease with increasing wine quality. So, we can say that they are negatively related.","b7826f06":"**Let's use Random Forest Model to calculate Importance of Each Feature**","356e3049":"here we can see all values are fine..\n","921d9142":"\n \n Red wine is a type of wine made from dark-colored grape varieties. The actual color of the wine can range from intense violet,\n typical of young wines, through to brick red for mature wines and brown for older red wines.\n >so point is that  we need to predict the quality of wine with respect to different parameters such as alcohol, acidity, density, pH, etc using an Artificial Neural Network (ANN).","5926caab":"seprating the traning ans testing data by using train_test_split","fb6fe233":"**Perform training data over-sampling**","3ace9368":"We will be using matplotlib and seaborn data visualization libraries.\nSeaborn is a library library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.","c7060e4d":"\n# Outline\n1. Import Datasets\n2. Missing Value's Imputation\n3. Exploratory Data Analysis \n4. Feature Engineering\n5. Model Building\n6. Model Evaluation","6748a942":"# Let's check how out model looks","dde5f68a":"**This Dataset contains 11 different features which affect the quality of red wine.**","982c73f5":"In univariate analysis, it was observed that a input features like 'chlorides', 'residual sugar', 'total sulphur dioxide', 'sulphates' are skewed due to the presence of outliers.","400d66af":"\n    - Alcohol content greatly affects the wine quality. Sulphates, volatile acidity and sulphur dioxide are also important for wine quality prediction.\n    - As the importance of citric acid and free sulphur dioxide is very low, we can avoid them in model building.\n\n","5209814e":"Drop 'free sulfur dioxide','citric acid' because of thier lower importance","1ae5694e":" # **Red Wine Quality**\n ","00f3392b":"# **Model Evaluation  and prediction**","2c45ed63":"# Key take-aways from EDA :  \n\n- Based on the univariate analysis of feature, it was observed that a few Features are highly skewed which implies that extreme outliers are present.\n- Based on the Bi-variate analysis, it was observed that a Few features were positively or negatively correlated with the target variable. \n- The target variable 'quality' is highly imbalanced which can affect machine learning model's performance.","91b1038b":"Train the ANN Model","4d389c91":"#### Lets have a look at the Input variables (based on physicochemical tests):  \n\n- fixed acidity : non-volatile acids that do not evaporate readily\n\n- volatile acidity : are high acetic acid in wine which leads to an unpleasant vinegar taste\n\n- citric acid : acts as a preservative, adds freshness and flavor to wines\n\n- residual sugar : amount of sugar remaining after fermentation stops\n\n- chlorides : the amount of salt in the wine\n\n- free sulfur dioxide : prevents microbial growth\n\n- total sulfur dioxide : amount of free + bound forms of SO2\n\n- density : density of wine (affects sweetness)\n\n- pH :  is a scale used to specify the acidity or basicity of wine\n\n- sulphates : sulphate content\n\n- alcohol : alcohol content\n\n- quality - quality (score between 0 and 10, Output variable (based on sensory data))","7163cfed":"**Define the ANN Model for the wine_df data**","aefde5de":"* Let's check the distribution of each Input Features using Histplot\n\n> histplot is used to Plot univariate or bivariate histograms to show distributions of a variable.A histogram is a classic visualization tool that represents the distribution of one or more variables by counting the number of observations that fall within disrete bins.*","13f1b958":"* here we can see description of dataset features which affectinng the wind quality..","d8d7b802":"* **There are outliers present in a few features like 'residual sugar', 'total sulfur dioxide' as their max value is much higher compared to the 75th percentile value.**","c43999d1":"A slight multi collinearity is observed in a few features. But, the level of collinearity is not very high. So we will not drop any feature.","de6b9e83":"- There is a huge Class imbalance in the target variable 'quality'.  \n- Out of 6 Categories, category 5 and 6 together have around 82.5% of the records.","ceeea84b":" # 3. Exploratory Data Analysis","1fde727d":"Model Training Finished!","236bdb80":"# 2.Missing Value's Imputation","88c2a125":"# 1.importing dataset \n\n  with help of padas library","31dc504d":"**Here we can see  No Missng values present.it means all values in dataset are proper placed**","fce081ab":"Since we are using categorical cross-entropy for traing ANN model later in this project.","0bb80eab":"Keep stratify = y, because we want stratified training and testing data. It means that all 6 categories of target variable will be present in equal ratio in both training and testing data.","e2b87466":"Convert y_train_os and y_test into binary matrix representation.","52d97d6e":"![image.png](attachment:e40cd904-7e0c-4829-a3cd-2331be3aaf6c.png)!","4689b647":"# 4. feature engineering"}}