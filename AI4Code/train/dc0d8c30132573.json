{"cell_type":{"d5b9162a":"code","8e626f55":"code","1e3fe652":"code","0c4c0f97":"code","6ed8cbf9":"code","39811f55":"code","d6b21963":"code","640e5278":"code","625c1180":"code","a560e03c":"code","77aef775":"code","3ea7c6f0":"code","759a6360":"code","d5e53704":"code","772cd4e3":"code","7e3ae618":"code","22bded97":"code","c231d49c":"code","e03358fd":"code","5e6fa7c3":"code","7bedd846":"markdown","eaa6889d":"markdown","01e28674":"markdown","c64716ac":"markdown","f3aea8c5":"markdown","316f7791":"markdown","25ae19d1":"markdown","6476362d":"markdown","cdab52c9":"markdown","c5653613":"markdown","b7b16ee2":"markdown","b8ba7c96":"markdown","0b4fcf65":"markdown","3401ce8e":"markdown","68f51979":"markdown","1c46dfd1":"markdown","672f4282":"markdown","5b4c7c30":"markdown","cc25ef5e":"markdown","4f30891a":"markdown","06a24e4a":"markdown","5341d109":"markdown"},"source":{"d5b9162a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","8e626f55":"from sklearn.preprocessing import LabelEncoder,MinMaxScaler,StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier ,RandomForestClassifier ,GradientBoostingClassifier\nfrom xgboost import XGBClassifier \nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.metrics import roc_auc_score ,mean_squared_error,accuracy_score,classification_report,roc_curve,confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.stats.mstats import winsorize\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\npd.set_option('display.max_columns',None)\nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six","1e3fe652":"# accessing to the folder where the file is stored\npath = '..\/input\/banking-project-term-deposit\/preprocessed_data.csv'\n\n# Load the dataframe\ndataframe = pd.read_csv(path)\n\nprint('Shape of the data is: ',dataframe.shape)\n\ndataframe.head()\n\n","0c4c0f97":"# Predictors\nX = dataframe.iloc[:,:-1]\n\n# Target\ny = dataframe.iloc[:,-1]\n\n# Dividing the data into train and test subsets\nx_train,x_val,y_train,y_val = train_test_split(X,y,test_size=0.2,random_state=42)\n","6ed8cbf9":"# run Logistic Regression model\nmodel = LogisticRegression()\n# fitting the model\nmodel.fit(x_train, y_train)\n# predicting the values\ny_scores = model.predict(x_val)\n\n","39811f55":"\n# getting the auc roc curve\nauc = roc_auc_score(y_val, y_scores)\nprint('Classification Report:')\nprint(classification_report(y_val,y_scores))\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_val, y_scores)\nprint('ROC_AUC_SCORE is',roc_auc_score(y_val, y_scores))\n    \n#fpr, tpr, _ = roc_curve(y_test, predictions[:,1])\n    \nplt.plot(false_positive_rate, true_positive_rate)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC curve')\nplt.show()","d6b21963":"# Run Decision Tree Classifier\nmodel = DecisionTreeClassifier()\n\nmodel.fit(x_train, y_train)\ny_scores = model.predict(x_val)\nauc = roc_auc_score(y_val, y_scores)\nprint('Classification Report:')\nprint(classification_report(y_val,y_scores))\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_val, y_scores)\nprint('ROC_AUC_SCORE is',roc_auc_score(y_val, y_scores))\n    \n#fpr, tpr, _ = roc_curve(y_test, predictions[:,1])\n    \nplt.plot(false_positive_rate, true_positive_rate)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC curve')\nplt.show()","640e5278":"from sklearn import tree\nfrom sklearn.tree import export_graphviz # display the tree within a Jupyter notebook\nfrom IPython.display import SVG\nfrom graphviz import Source\nfrom IPython.display import display\nfrom ipywidgets import interactive, IntSlider, FloatSlider, interact\nimport ipywidgets\nfrom IPython.display import Image\nfrom subprocess import call\nimport matplotlib.image as mpimg","625c1180":"@interact\ndef plot_tree(crit=[\"gini\", \"entropy\"],\n              split=[\"best\", \"random\"],\n              depth=IntSlider(min=1,max=30,value=2, continuous_update=False),\n              min_split=IntSlider(min=2,max=5,value=2, continuous_update=False),\n              min_leaf=IntSlider(min=1,max=5,value=1, continuous_update=False)):\n    \n    estimator = DecisionTreeClassifier(random_state=0,\n                                       criterion=crit,\n                                       splitter = split,\n                                       max_depth = depth,\n                                       min_samples_split=min_split,\n                                       min_samples_leaf=min_leaf)\n    estimator.fit(x_train, y_train)\n    print('Decision Tree Training Accuracy: {:.3f}'.format(accuracy_score(y_train, estimator.predict(x_train))))\n    print('Decision Tree Test Accuracy: {:.3f}'.format(accuracy_score(y_val, estimator.predict(x_val))))\n\n    graph = Source(tree.export_graphviz(estimator,\n                                        out_file=None,\n                                        feature_names=x_train.columns,\n                                        class_names=['0', '1'],\n                                        filled = True))\n    \n    display(Image(data=graph.pipe(format='png')))\n    \n    return estimator\n","a560e03c":"# run Random Forrest Classifier\nmodel = RandomForestClassifier()\n\nmodel.fit(x_train, y_train)\ny_scores = model.predict(x_val)\nauc = roc_auc_score(y_val, y_scores)\nprint('Classification Report:')\nprint(classification_report(y_val,y_scores))\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_val, y_scores)\nprint('ROC_AUC_SCORE is',roc_auc_score(y_val, y_scores))\n    \n#fpr, tpr, _ = roc_curve(y_test, predictions[:,1])\n    \nplt.plot(false_positive_rate, true_positive_rate)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC curve')\nplt.show()","77aef775":"# Selecting 8 number of features\n#   selecting models\nmodels = LogisticRegression()\n#   using  rfe and selecting 8 features\nrfe = RFE(models,8)\n#   fitting the model\nrfe = rfe.fit(X,y)\n#   ranking features\nfeature_ranking = pd.Series(rfe.ranking_, index=X.columns)\nplt.show()\nprint('Features  to be selected for Logistic Regression model are:')\nprint(feature_ranking[feature_ranking.values==1].index.tolist())\nprint('===='*30)\n\n","3ea7c6f0":"# Selecting 8 number of features\n# Random Forrest classifier model\nmodels = RandomForestClassifier()\n#   using  rfe and selecting 8 features\nrfe = RFE(models,8)\n#   fitting the model\nrfe = rfe.fit(X,y)\n#   ranking features\nfeature_ranking = pd.Series(rfe.ranking_, index=X.columns)\nplt.show()\nprint('Features  to be selected for Random Forrest Classifier are:')\nprint(feature_ranking[feature_ranking.values==1].index.tolist())\nprint('===='*30)\n\n","759a6360":"# splitting the data into train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n# selecting the data\nrfc = RandomForestClassifier(random_state=42)\n# fitting the data\nrfc.fit(X_train, y_train)\n# predicting the data\ny_pred = rfc.predict(X_test)\n# feature importances\nrfc_importances = pd.Series(rfc.feature_importances_, index=X.columns).sort_values().tail(10)\n# plotting bar chart according to feature importance\nrfc_importances.plot(kind='bar')\nplt.show()","d5e53704":"# splitting the data\nx_train,x_val,y_train,y_val = train_test_split(X,y, test_size=0.3, random_state=42, stratify=y)\n# selecting the classifier\nrfc = RandomForestClassifier()\n# selecting the parameter\nparam_grid = { \n'max_features': ['auto', 'sqrt', 'log2'],\n'max_depth' : [4,5,6,7,8],\n'criterion' :['gini', 'entropy']\n             }\n# using grid search with respective parameters\ngrid_search_model = GridSearchCV(rfc, param_grid=param_grid)\n# fitting the model\ngrid_search_model.fit(x_train, y_train)\n# printing the best parameters\nprint('Best Parameters are:',grid_search_model.best_params_)","772cd4e3":"from sklearn.metrics import roc_auc_score,roc_curve,classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom imblearn.over_sampling import SMOTE\nfrom yellowbrick.classifier import roc_auc\n\n\n# A function to use smote\ndef grid_search_random_forrest_best(dataframe,target):\n    \n    # splitting the data\n    x_train,x_val,y_train,y_val = train_test_split(dataframe,target, test_size=0.3, random_state=42)\n    \n    # Applying Smote on train data for dealing with class imbalance\n    smote = SMOTE()\n    \n    X_sm, y_sm =  smote.fit_sample(x_train, y_train)\n    \n    rfc = RandomForestClassifier(n_estimators=11, max_features='auto', max_depth=8, criterion='entropy',random_state=42)\n    \n    rfc.fit(X_sm, y_sm)\n    y_pred = rfc.predict(x_val)\n    print(classification_report(y_val, y_pred))\n    print(confusion_matrix(y_val, y_pred))\n    visualizer = roc_auc(rfc,X_sm,y_sm,x_val,y_val)\n\n\ngrid_search_random_forrest_best(X,y)","7e3ae618":"grid_search_random_forrest_best(X[['age', 'job', 'education', 'month', 'day_of_week', 'duration', 'campaign', 'poutcome']],y)","22bded97":"from sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import VotingClassifier\n\n\n# splitting the data  \nx_train,x_val,y_train,y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n# using smote\nsmote = SMOTE()\nX_sm, y_sm =  smote.fit_sample(x_train, y_train)\n# models to use for ensembling  \nmodel1 = RandomForestClassifier()\nmodel3 = GradientBoostingClassifier()\nmodel2 = LogisticRegression()\n# fitting the model\nmodel = VotingClassifier(estimators=[('rf', model1), ('lr', model2), ('xgb',model3)], voting='soft')\nmodel.fit(X_sm,y_sm)\n# predicting balues and getting the metrics\ny_pred = model.predict(x_val)\n","c231d49c":"print(classification_report(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nvisualizer = roc_auc(model,X_sm,y_sm,x_val,y_val)","e03358fd":"# Preprocessed Test File\ntest = pd.read_csv('..\/input\/banking-project-term-deposit\/new_train.csv')\ntest.head()\n","5e6fa7c3":"smote = SMOTE()\n\nX_sm, y_sm =  smote.fit_sample(x_train, y_train)\n\n\nrfc = RandomForestClassifier()\n# selecting the parameter\nparam_grid = { \n'max_features': ['auto', 'sqrt', 'log2'],\n'max_depth' : [4,5,6,7,8],\n'criterion' :['gini', 'entropy']\n             }\n# using grid search with respective parameters\ngrid_search_model = GridSearchCV(rfc, param_grid=param_grid)\n\n# fitting the model\ngrid_search_model.fit(X_sm, y_sm)\n    \n# Predict on the preprocessed test file\ny_pred = grid_search.predict(test)\n    \n#prediction = pd.DataFrame(y_pred,columns=['y'])\n#submission = pd.concat([Id,prediction['y']],1)\n\n#submission.to_csv('submission.csv',index=False)","7bedd846":"## Data Loading and Cleaning\n\n","eaa6889d":"## Prediction on the test data\n\nIn the below task, we have performed a prediction on the test data. We have used Logistic Regression for this prediction. You can use the model of your choice that will give you the best metric score on the validation data. \n\nIn this task below, we will read the test file and store the `Id` column from the test file in a variable `Id`. This column would be of use to us while submission since we need to have an Id column in the submission file which is the same Id of the observations in the test data.\n\nWe have to perform the same preprocessing operations on the test data that we have performed on the train data. For demonstration purposes, we have preprocessed the test data and this preprocessed data is present in the csv file `test_preprocessed.csv`\n\nWe then make a prediction on the preprocessed test data using the Grid Search Logisitic regression model. And as the final step, we concatenate this prediction with the `Id` column and then convert this into a csv file which becomes the `submission.csv` ","01e28674":"## Feature Selection \n\nNow that we have applied vanilla models on our data, we now have a basic understanding of what our predictions look like. Let's now use feature selection methods for identifying the best set of features for each model.","c64716ac":"## Applying vanilla models on the data\n\nSince we have performed preprocessing on our data and also done with the EDA part, it is now time to apply vanilla machine learning models on the data and check their performance.","f3aea8c5":"# Problem Statement\n\n### Business Use Case\n\nThere has been a revenue decline for a Portuguese bank and they would like to know what actions to take. After investigation, they found out that the root cause is that their clients are not depositing as frequently as before. Knowing that term deposits allow banks to hold onto a deposit for a specific amount of time, so banks can invest in higher gain financial products to make a profit. In addition, banks also hold better chance to persuade term deposit clients into buying other products such as funds or insurance to further increase their revenues. As a result, the Portuguese bank would like to identify existing clients that have higher chance to subscribe for a term deposit and focus marketing efforts on such clients.\n\n### Data Science Problem Statement\n\nPredict if the client will subscribe to a term deposit based on the analysis of the marketing campaigns the bank performed.\n\n### Evaluation Metric\nWe will be using ROC-AUC for evaluation. \n\n### Objective of this template notebook\n\nThe main objective of this template is to take you through the entire working pipeline that you may follow while appraoching a Machine Learning problem.\n\nWe will be defining a task to be performed and write the code to solve the task.\n\n__The tasks performed below should serve as a good guide regarding the steps that you should go about a Machine Learning Problem. But kindly do not restrict yourself to only the tasks that have been performed in this notebook and feel free to bring your ideas,skills and strategies and implement them as well.__\n\n\n### Word of caution\n\nThis template is just an example of a data-science pipeline, every data science problem is unique and there are multiple ways to tackle them. Go through this template and try to leverage the information in this while solving your hackathon problems but you may not be able to use all the functions created here.","316f7791":"\n### Load the Preprocessed dataset\n\n- In this task, we'll load the dataframe in pandas, drop the unnecessary columns and display the top five rows of the dataset.","25ae19d1":"### Loading Data Modelling Libraries\n\nWe will use the popular scikit-learn library to develop our machine learning algorithms. In sklearn, algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the matplotlib and seaborn library. Below are common classes to load.","6476362d":"###  Using RFE for feature selection\nIn this task let's use Recursive Feature Elimination for selecting the best features. RFE is a wrapper method that uses the model to identify the best features. \n\n- For the below task, we have inputted 8 feature. You can change this value and input the number of features you want to retain for your model\n","cdab52c9":"#### GETTING THE METRICS TO CHECK OUR MODEL PERFORMANCE","c5653613":"#### FITTING THE MODEL AND PREDICTING THE VALUES","b7b16ee2":"## Ensembling\n\nEnsemble learning uses multiple machine learning models  to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. In the below task, we have used an ensemble of three models - `RandomForestClassifier()`, `GradientBoostingClassifier()`, `LogisticRegression()`. Feel free to modify this function as per your requirements and fit more models or change the parameters for every model.\n","b8ba7c96":"### Fit vanilla classification models\n\nSince we have label encoded our categorical variables, our data is now ready for applying machine learning algorithms. \n\nThere are many Classification algorithms are present in machine learning, which are used for different classification applications. Some of the main classification algorithms are as follows-\n- Logistic Regression\n- DecisionTree Classifier\n- RandomForest Classfier\n\nThe code we have written below internally splits the data into training data and validation data. It then fits the classification model on the train data and then makes a prediction on the validation data and outputs the scores for this prediction.","0b4fcf65":"###  Importing necessary libraries\n\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks.","3401ce8e":"### Applying the grid search function for random forest only on the best features obtained using Random Forest","68f51979":"# Grid-Search & Hyperparameter Tuning \n\nHyperparameters are function attributes that we have to specify for an algorithm. By now, you should be knowing that grid search is done to find out the best set of hyperparameters for your model.  ","1c46dfd1":"### Observations :\n\nWe can test the features obtained from both the feature selection techniques by inserting these features to the model and depending on which set of features perform better, we can retain them for the model. \n\n__The Feature Selection techniques can differ from problem to problem and the techniques applied for this problem may or may not work for the other problems. In those cases, feel free to try out other methods like PCA, SelectKBest(), SelectPercentile(), tSNE etc.__","672f4282":"### Applying the best parameters obtained using Grid Search on Random Forest model\n\nIn the task below, we fit a random forest model using the best parameters obtained using Grid Search. Since the target is imbalanced, we apply Synthetic Minority Oversampling (SMOTE) for undersampling and oversampling the majority and minority classes in the target respectively. \n\n__Kindly note that SMOTE should always be applied only on the training data and not on the validation and test data.__\n\nYou can try experimenting with and without SMOTE and check for the difference in recall. ","5b4c7c30":"# Understanding the dataset\n\n**Data Set Information**\n\nThe data is related to direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be subscribed ('yes') or not ('no') subscribed.\n\nThere are two datasets:\n`train.csv` with all examples (32950) and 21 inputs including the target feature, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]\n\n`test.csv` which is the test data that consists  of 8238 observations and 20 features without the target feature\n\nGoal:- The classification goal is to predict if the client will subscribe (yes\/no) a term deposit (variable y).\n\n**Features**\n\n|Feature|Feature_Type|Description|\n|-----|-----|-----|\n|age|numeric|age of a person|  \n|job |Categorical,nominal|type of job ('admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')|  \n|marital|categorical,nominal|marital status ('divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)|  \n|education|categorical,nominal| ('basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown') | \n|default|categorical,nominal| has credit in default? ('no','yes','unknown')|  \n|housing|categorical,nominal| has housing loan? ('no','yes','unknown')|  \n|loan|categorical,nominal| has personal loan? ('no','yes','unknown')|  \n|contact|categorical,nominal| contact communication type ('cellular','telephone')|  \n|month|categorical,ordinal| last contact month of year ('jan', 'feb', 'mar', ..., 'nov', 'dec')| \n|day_of_week|categorical,ordinal| last contact day of the week ('mon','tue','wed','thu','fri')|  \n|duration|numeric| last contact duration, in seconds . Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no')|\n|campaign|numeric|number of contacts performed during this campaign and for this client (includes last contact)|  \n|pdays|numeric| number of days that passed by after the client was last contacted from a previous campaign (999 means client was not previously contacted)|  \n|previous|numeric| number of contacts performed before this campaign and for this client|  \n|poutcome|categorical,nominal| outcome of the previous marketing campaign ('failure','nonexistent','success')|  \n\n**Target variable (desired output):**  \n\n|Feature|Feature_Type|Description|\n|-----|-----|-----|\n|y | binary| has the client subscribed a term deposit? ('yes','no')|","cc25ef5e":"### Grid Search for Random Forest\n\nIn the below task, we write a code that performs hyperparameter tuning for a random forest classifier. We have used the hyperparameters `max_features`, `max_depth` and `criterion` for this task. Feel free to play around with this function by introducing a few more hyperparameters and chaniging their values","4f30891a":"#### PREPARING THE TRAIN AND TEST DATA\n","06a24e4a":"### The above two steps are combined and run in a single cell for all the remaining models respectively","5341d109":"### Feature Selection using Random Forest\n\nRandom Forests are often used for feature selection in a data science workflow. This is because the tree based strategies that random forests use, rank the features based on how well they improve the purity of the node. The nodes having a very low impurity get split at the start of the tree while the nodes having a very high impurity get split towards the end of the tree. Hence by pruning the tree after desired amount of splits, we can create a subset of the most important features."}}