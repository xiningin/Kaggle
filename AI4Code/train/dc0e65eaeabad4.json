{"cell_type":{"ebea8c97":"code","4afc8838":"code","eb45cb7f":"code","b0bd743b":"code","fe62c61b":"code","389f03ec":"code","31d7a64b":"code","5a5ce5d5":"code","6b4d8b44":"code","f57e876a":"code","5aff4404":"code","547b3023":"markdown","1f9288cf":"markdown","a4ea52f5":"markdown","2bb49b34":"markdown","fa255da8":"markdown","7d7cd033":"markdown","5182fb4b":"markdown","6b87250e":"markdown"},"source":{"ebea8c97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4afc8838":"df = pd.read_csv('..\/input\/men-s-shoe-pricese\/7004_1.csv')","eb45cb7f":"display(df)","b0bd743b":"df.describe()","fe62c61b":"df.info()","389f03ec":"df = df.drop(['count', 'flavors', 'isbn', 'vin', 'websiteIDs'], axis = 1) #removing complete NaN cols","31d7a64b":"df_temp = df.copy(deep=True)\ndf_temp.dropna(subset = [\"colors\"], inplace=True) #Data cleaning\n\nbag_colors = []\n\nfor index, row in df_temp.iterrows():\n    bag_colors.extend(row['colors'].split(',')) # Creating the bag of colors\n    \nbag_color_dict = {ii:bag_colors.count(ii) for ii in set(bag_colors)} # Calculating the frequency\nD = dict(sorted(bag_color_dict.items(), key=lambda item: item[1], reverse=True)[:10]) # Choosing the top 10 colors\n\n# Plotting it\nplt.figure(figsize=(15, 3))\nplt.bar(range(len(D)), list(D.values()), align='center', width=0.8)\nplt.xticks(range(len(D)), list(D.keys()))\nplt.show() ","5a5ce5d5":"df_temp = df.copy(deep=True)\ndf_temp.dropna(subset = [\"manufacturer\"], inplace=True) #Data cleaning\n\nbag_colors = []\n\nfor index, row in df_temp.iterrows():\n    bag_colors.extend(row['manufacturer'].split(',')) # Creating the bag of manufacturer\n    \nbag_color_dict = {ii:bag_colors.count(ii) for ii in set(bag_colors)} # Calculating the frequency\nD = dict(sorted(bag_color_dict.items(), key=lambda item: item[1], reverse=True)[:10]) # Choosing the top 10 manufacturer\n\n# Plotting it\nplt.figure(figsize=(15, 3))\nplt.bar(range(len(D)), list(D.values()), align='center', width=0.8, color=\"orange\")\nplt.xticks(range(len(D)), list(D.keys()))\nplt.show() ","6b4d8b44":"df_temp = df.copy(deep=True)\ndf_temp.dropna(subset = [\"prices.currency\"], inplace=True) #Data cleaning\n\nbag_currency = []\n\nfor index, row in df_temp.iterrows():\n    bag_currency.extend(row['prices.currency'].split(',')) # Creating the bag of currency\n\nbag_currency_dict = {ii:bag_currency.count(ii) for ii in set(bag_currency)} # Calculating the frequency\nD = dict(sorted(bag_currency_dict.items(), key=lambda item: item[1], reverse=True)[:3]) # Choosing the top 10 colors\n\n# Plotting it\nplt.figure(figsize=(15, 3))\nplt.bar(range(len(D)), list(D.values()), align='center', width=0.8, color=\"green\")\nplt.xticks(range(len(D)), list(D.keys()))\nplt.show() ","f57e876a":"df_temp = df.copy(deep=True)\ndf_temp.dropna(subset = [\"prices.returnPolicy\"], inplace=True) #Data cleaning\n\nbag_returnPolicy = []\n\nfor index, row in df_temp.iterrows():\n#     print(row['prices.returnPolicy'])\n    bag_returnPolicy.extend(row['prices.returnPolicy'].split('!')) # Creating the bag of currency\n\nbag_returnPolicy_dict = {ii:bag_returnPolicy.count(ii) for ii in set(bag_returnPolicy)} # Calculating the frequency\nD = dict(sorted(bag_returnPolicy_dict.items(), key=lambda item: item[1], reverse=True)[:10]) # Choosing the top 10 colors\n\n# Plotting it\nplt.figure(figsize=(30, 3))\nplt.bar(range(len(D)), list(D.values()), align='center', width=0.8, color=\"green\")\nplt.xticks(range(len(D)), list(D.keys()), rotation='vertical')\nplt.show()","5aff4404":"df_temp = df.copy(deep=True)\ndf_temp.dropna(subset = [\"weight\"], inplace=True) #Data cleaning\n\nbag_weight = []\n\nfor index, row in df_temp.iterrows():\n    rw=0\n    if( row['weight'].find(\"lbs\") !=-1 ):\n        rw = float(row['weight'].split(\"lbs\")[0].strip()) * 453.592\n\n    elif( row['weight'].find(\"Kg\") != -1 ):\n        rw = float(row['weight'].split(\"Kg\")[0].strip()) * 1000\n        \n    elif( row['weight'].find(\"g\") != -1 ):\n        rw = float(row['weight'].split(\"g\")[0].strip()) * 1\n        \n    elif( row['weight'].find(\"pounds\") != -1 ):\n        rw = float(row['weight'].split(\"pounds\")[0].strip()) * 453.592\n        \n    elif( row['weight'].find(\"ounces\") != -1 ):\n        rw = float(row['weight'].split(\"ounces\")[0].strip()) * 28.3495\n        \n    bag_weight.append(rw) # Creating the bag of currency\n\nbag_weight = {ii:bag_weight.count(ii) for ii in set(bag_weight)} # Calculating the frequency\nD = dict(sorted(bag_weight.items(), key=lambda item: item[1], reverse=True)) # Choosing the top 10 colors\n\n# Plotting it\nplt.figure(figsize=(30, 3))\nplt.hist(bag_weight, bins=250)\nplt.ylabel('Count')\nplt.xlabel('Weight of shoes in grams')\nplt.show()","547b3023":"## 5. Weight of shoes","1f9288cf":"# Challenge of the month of Janaury 2022.\nThis is a submission for AISC COTM Jan 2021, regarding EDA\n\nChallenge description:\n\n```\nDescription\nBackground\nExploratory Data Analysis is very crucial and an unavoidable task for any AI-enabled application to come to life. This process is a thorough examination meant to uncover the trends, patterns, and relationships that are not readily apparent in any dataset. A data scientist cannot draw reliable conclusions from a massive quantity of data by simply collecting the dataset. Instead, they have to look at it carefully and methodically through an analytical lens. The insights that are uncovered from data analysis may eventually lead to the selection of an appropriate predictive model in the modelling stage.\n\n\nThis month's challenge is to perform exploratory data analysis on a dataset with a massive quantity to discover trends, patterns and meaningful insights that can later be used to create a prediction model.\n\n\nDo you have what it takes to win this challenge? Attempt now and stand a chance to win surprise giveaways!\n\n\nHere's how you can participate in the challenge:\n\nStep 1: Attend the following sessions or watch the session recording to understand how you could approach the challenge:\n\n1. EDA Part 1 - conducted on Monday, 27 December 2021\n2. EDA Part 2 - conducted on Wednesday, 29 December 2021\n3. EDA Part 3 - to be conducted on 11 January 2022\n4. Advanced Python: Pandas with Matplotlib I and II - conducted on 13-14 May 2021\n\nStep 2: Collect \/ Acquire a dataset of your choice to perform an exploratory data analysis.\nNOTE: The minimum requirement of the dataset is: The size of the dataset should be at least 1200 rows and 6 features\n\nStep 3: Create a new Jupyter Notebook \/ Google Colab Notebook.\n\nStep 4: Read and analyse the dataset.\n\nStep 5: Perform an in-depth extensive EDA (Exploratory Data Analysis) on the dataset. The analysis should contain at least 5 different types of graphs\/charts.\n\nStep 6: Extract trends, patterns and relationships within the dataset. Create a Word Document \/ Google Doc to summarise your findings.\nNOTE:\n1. Present your findings and insights from the analysis in the document.\n2. Save the graphs \/ take screenshots of the plots to include them in the summary document.\n3. The page limit for the document is 3 pages.\n\nSubmission Details:\n\nUpload the notebook and document in Google Drive and submit the link.\n \nHow to Submit?\n\nSubmit the Google Drive link containing the Jupyter Notebook \/ Google Colab Notebook and the Word Document \/ Google Doc in the google form - https:\/\/forms.gle\/qoxNZBK32c5cuag98\n\nDeadline: January 31, 2021, 8:00 PM\n\nHave a great experience! All the best!\n```\n","a4ea52f5":"## 3. Currency used for prices of shoes","2bb49b34":"## Reading the data","fa255da8":"## Basic EDA tests","7d7cd033":"## 1. Colors of the shoes","5182fb4b":"## 2. Manufacturer of the shoes","6b87250e":"## 4. Return policy of shoes"}}