{"cell_type":{"eafb4d1b":"code","3cbc6df2":"code","f663c54b":"code","26dc771a":"code","942d1124":"code","a0597274":"code","e3b7bcd5":"code","6152fa17":"code","1ebe5338":"code","300fe33f":"code","aa67229d":"code","64fb3b57":"code","e35a11cf":"code","261a7b4f":"code","3828c531":"code","f0c72346":"code","80c38ee3":"code","2ae8e952":"code","1dfed15b":"code","e3bb4c4f":"code","86bbe1d6":"code","3bec8de8":"code","80b9bb16":"code","b152fcf6":"code","7920be60":"code","2c0b7353":"code","8123da36":"code","b3ffc911":"code","9622267a":"code","e2bb68ed":"markdown","bdeeef2d":"markdown","78f4e3ac":"markdown","ace5698c":"markdown","b75be6d9":"markdown","36d5099c":"markdown","c1752f20":"markdown","ff455524":"markdown","cabb0558":"markdown","519c2c64":"markdown","50b8d840":"markdown","743a28f1":"markdown","9bca52cc":"markdown","9e573dea":"markdown","d653f497":"markdown","60a6168f":"markdown","eed78eb1":"markdown","f2297e27":"markdown","bc80c7d3":"markdown","63277bbd":"markdown","3ad21f49":"markdown"},"source":{"eafb4d1b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import cluster\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb","3cbc6df2":"df = pd.read_csv('..\/input\/base-imoveis-123i\/base_123i.csv')\ndf.head()","f663c54b":"len(df)","26dc771a":"# A visual check of each column's value\ndef check_df_columns(df):\n    for col in df.columns:\n\n        print(f\" ----- {col} ----- \")\n        print(100 * (df[col].value_counts() \/ df.shape[0]))\n        \ncheck_df_columns(df)","942d1124":"# Print how many nulls on each column\ndef print_null_cols(df):\n    for col in df.columns:\n        nulls_value = df[col].isna().sum()\n        percentage = 100*(nulls_value \/ df.shape[0])\n        message = \"Column {} has {} nulls \/ {}% \".format(col, nulls_value, percentage)\n        print(message)\n        \nprint_null_cols(df)\ndf.dtypes","a0597274":"# Function to clean data\ndef clean_data(df):\n    \n    # remove weird data with values equal to -1\n    df = df[df.maximum_estimate != -1]\n    df = df[df.minimum_estimate != -1]\n    df = df[df.point_estimate != -1]\n    df = df[df.garages != -1]\n    df = df[df.rooms != -1]\n    df = df[df.useful_area != -1]\n    \n    # remove cities with low data\n    city_dominance = 100 * (df['city'].value_counts() \/ df.shape[0])\n    for a, b in zip(city_dominance.index, city_dominance):\n        print(a)\n        print(b)\n        if b < 1:\n            df = df[df.city != a]\n            \n    # lower case state\n    df['state'] = [x.lower() for x in df['state']]\n    \n    return df\n\ndf = clean_data(df)\n\ndf = df.reset_index(drop=True)","e3b7bcd5":"print(100 * (df['city'].value_counts() \/ df.shape[0]))","6152fa17":"len(df)","1ebe5338":"\"\"\"\nneighborhood_list = []\nfor latitude, longitude in tqdm_notebook(zip(df['latitude'], df['longitude']), total=len(df)):\n\n    #ex: https:\/\/maps.googleapis.com\/maps\/api\/geocode\/json?key=APIKEY&latlng=-23.56417040,-46.65790930\n    \n    try:\n        response = requests.get('https:\/\/maps.googleapis.com\/maps\/api\/geocode\/json?key=APIKEY&latlng={0},{1}'.format(latitude, longitude))\n        resp_json_payload = response.json()        \n        #type_component = resp_json_payload['results'][0]['address_components'][2]['types']\n        type_component = resp_json_payload['results'][0]['address_components']\n        \n        neighborhood = None\n        for i in type_component:            \n            list_component_type = i['types']            \n            if 'sublocality' in list_component_type:            \n                neighborhood = i['long_name']\n                break\n    except:\n        neighborhood = None\n        \n    neighborhood_list.append(neighborhood)\n\ndf['neighborhood'] = neighborhood_list\n\"\"\"","300fe33f":"# Create cluster using Kmeans\ndef find_cluster(df):\n\n    location = df.copy()\n\n    columns_to_keep = ['latitude', 'longitude']\n\n    for col in location:\n        if col not in columns_to_keep:\n            location = location.drop(col, 1)\n    \n    location['lng_parsed'] = pd.to_numeric(location['longitude'], errors='coerce')\n    location['lat_parsed'] = pd.to_numeric(location['latitude'], errors='coerce')\n    \n    location = location.drop('longitude', 1)\n    location = location.drop('latitude', 1)\n\n    # Remove Outliers do dataframe\n    location = location[((location.lat_parsed - location.lat_parsed.mean()) \/ location.lat_parsed.std()).abs() < 3]\n    location = location[((location.lng_parsed - location.lng_parsed.mean()) \/ location.lng_parsed.std()).abs() < 3]\n    location = location[np.abs(location.lat_parsed-location.lat_parsed.mean()) <= (3*location.lat_parsed.std())]\n    location = location[np.abs(location.lng_parsed-location.lng_parsed.mean()) <= (3*location.lng_parsed.std())]\n\n    for i in range(0, len(df)):\n        if i not in location.index:\n            df = df[df.index != i]\n\n    df = df.reset_index(drop=True)\n\n    x1 = location.lng_parsed\n    x2 = location.lat_parsed\n    \n    # Plot charts and execute Kmeans clustering\n    plt.figure(num=None, figsize=(8, 6), facecolor='w', edgecolor='k')\n    plt.title('Latitude x Longitude')\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n    plt.scatter(x1, x2)\n    plt.show()\n\n    \n    plt.figure(num=None, figsize=(8, 6), facecolor='w', edgecolor='k')\n    plt.scatter(location.lng_parsed, location.lat_parsed)\n    plt.title('Clustering neighborhoods')\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n\n    location_full = location\n\n    location_np = np.array(location)\n\n    # Execute Kmeans clustering\n    # S\u00e3o Paulo has approximately 100 neighborhoods\n    k = 100 # Define the value of k\n    kmeans = cluster.KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(location_np)\n\n    labels = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n\n    for i in range(k):\n        # select only data observations with cluster label == i\n        ds = location_np[np.where(labels==i)]\n        # plot the data observations\n        plt.plot(ds[:,0],ds[:,1],'o', markersize=6)\n        # plot the centroids\n        lines = plt.plot(centroids[i,0],centroids[i,1],'kx')\n        # make the centroid x's bigger\n        plt.setp(lines,ms=8.0)\n        plt.setp(lines,mew=3.0)\n    plt.show()\n\n    list_areas_kmeans = []\n    for f, b in zip(labels, location_full.index):\n        list_areas_kmeans.append(f)\n\n    return list_areas_kmeans, df, location","aa67229d":"list_areas_kmeans, df, location = find_cluster(df)\n\ndf['area_kmeans'] = list_areas_kmeans","64fb3b57":"df.head()","e35a11cf":"order_by_median = df.groupby(by=[\"area_kmeans\"])[\"point_estimate\"].median().sort_values(ascending=True).index\n\nimport matplotlib.ticker as ticker\n\nsns.set(rc={'figure.figsize':(17,5)})\nsns.set(palette=\"pastel\")\nsns.boxplot(x=\"area_kmeans\", y=\"point_estimate\", color=\"orange\", data=df, showfliers=False)\nsns.despine(offset=10, trim=True)\nax = plt.gca()\nax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%d'))\nax.xaxis.set_major_locator(ticker.MultipleLocator(base=5))\nplt.show()\n\nsns.set(rc={'figure.figsize':(17,5)})\nsns.set(font_scale=0.78)\nsns.boxplot(x=\"area_kmeans\", y=\"point_estimate\", palette=\"rainbow\", data=df, showfliers=False, order=order_by_median)\nsns.despine(offset=10, trim=True)\n#ax = plt.gca()\n#ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%d'))\n#ax.xaxis.set_major_locator(ticker.MultipleLocator(base=5))\nplt.show()","261a7b4f":"print(order_by_median[-5:].tolist())","3828c531":"latitude_list = []\nlongitude_list = []\nlabel_area_kmeans = []\nfor i in order_by_median[-5:]:\n    expensive_areas = df.loc[(df['area_kmeans'] == i)]\n    latitude_list.append(expensive_areas['latitude'][0:1])\n    longitude_list.append(expensive_areas['longitude'][0:1])\n    label_area_kmeans.append(i)\n    \nlatitude_list_less = []\nlongitude_list_less = []\nlabel_area_kmeans_less = []\nfor i in order_by_median[0:5]:\n    less_expensive_areas = df.loc[(df['area_kmeans'] == i)]\n    latitude_list_less.append(less_expensive_areas['latitude'][0:1])\n    longitude_list_less.append(less_expensive_areas['longitude'][0:1])\n    label_area_kmeans_less.append(i)","f0c72346":"import folium\nfrom folium import plugins\n\nheatmap = df.copy()\nheatmap['count'] = 1\nbase_heatmap = folium.Map(location=['-23.5713874', '-46.6522521'], zoom_start=12)\n\nfor a, b, c in zip(latitude_list, longitude_list, label_area_kmeans):    \n    folium.Marker((a, b), popup=c, icon=folium.Icon(color='red')).add_to(base_heatmap)\n    \nfor a, b, c in zip(latitude_list_less, longitude_list_less, label_area_kmeans_less):    \n    folium.Marker((a, b), popup=c, icon=folium.Icon(color='darkblue')).add_to(base_heatmap)\n\nplugins.HeatMap(data=heatmap[['latitude', 'longitude', 'count']].groupby(['latitude', 'longitude']).sum().reset_index().values.tolist(), radius=8, max_zoom=4).add_to(base_heatmap)\n\nbase_heatmap","80c38ee3":"# Remove unuseful columns\ndef drop_columns(df, list_columns):\n    df = df.drop(list_columns, axis=1)    \n    \n    return df\n\nlist_columns = ['address', 'tower_name', 'latitude', 'longitude', 'city', 'state']\ndf = drop_columns(df, list_columns)","2ae8e952":"# Convert some columns to dummies\ndef one_hot_encoder(df):\n    one_hot = pd.get_dummies(df['building_type'])\n    df = df.drop('building_type',axis = 1)\n    df = df.join(one_hot)\n    \n    return df\n\ndf = one_hot_encoder(df)","1dfed15b":"# Split data\/target\ndef get_data_target(df, target):\n    y = df[target]    \n    X = df.drop(target,axis = 1)\n    \n    return X, y","e3bb4c4f":"# Function to train the model\ndef train(model):\n\n    model = model\n\n    model.fit(X_train, y_train,\n            eval_set = [(X_train, y_train), (X_test, y_test)],\n            eval_metric = 'rmse',\n            early_stopping_rounds = 5,\n            verbose=True)\n\n    best_iteration = model.get_booster().best_ntree_limit\n    preds = model.predict(X_test, ntree_limit=best_iteration)\n    \n    return preds, model, best_iteration","86bbe1d6":"# Function to evaluate some metrics\ndef evaluate_metrics(preds, model, best_iteration):\n    rmse = np.sqrt(mean_squared_error(y_test, preds))\n    r2 = r2_score(y_test, preds, multioutput='variance_weighted')\n    \n    evals_result = model.evals_result()  \n    plt.rcParams[\"figure.figsize\"] = (8, 6)\n    xgb.plot_importance(model, max_num_features=None)\n    plt.show()\n\n    range_evals = np.arange(0, len(evals_result['validation_0']['rmse']))\n\n    val_0 = evals_result['validation_0']['rmse']\n    val_1 = evals_result['validation_1']['rmse']\n    plt.figure(num=None, figsize=(8, 6), facecolor='w', edgecolor='k')\n    plt.plot(range_evals, val_1, range_evals, val_0)\n    plt.ylabel('Validation error')\n    plt.xlabel('Iteration')\n    plt.show()\n    \n    print(\"Best iteration: %f\" % (int(best_iteration)))\n    print(\"RMSE: %f\" % (rmse))\n    print(\"R2: %f\" % (r2))\n    print('Observed value single sample: {}'.format(y_test[0:1].tolist()[0]))\n    print('Predicted value single sample: {}'.format(int(model.predict(X_test[0:1], ntree_limit=best_iteration)[0])))","3bec8de8":"# First model\ntarget = 'point_estimate'\nX, y = get_data_target(df, target)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nxg_reg = xgb.XGBRegressor()\n\npreds, model, best_iteration = train(xg_reg)\n\nevaluate_metrics(preds, model, best_iteration)","80b9bb16":"# Second model\nlist_columns = ['minimum_estimate', 'maximum_estimate']\ndf = drop_columns(df, list_columns)\n\ntarget = 'point_estimate'\nX, y = get_data_target(df, target)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nxg_reg = xgb.XGBRegressor()\n\npreds, model, best_iteration = train(xg_reg)\n\nevaluate_metrics(preds, model, best_iteration)","b152fcf6":"# Function to create new features\ndef feature_engineering(df):\n\n    mean_rooms_list = []\n    mean_garages_list = []\n    mean_useful_area_list = []    \n    std_rooms_list = []\n    std_garages_list = []\n    std_useful_area_list = []\n    \n    for i in tqdm_notebook(df.index):\n\n        rooms_df = df.loc[(df['rooms']) & ((df['area_kmeans'] ==  df['area_kmeans'][i]))]\n        mean_rooms = rooms_df['rooms'].mean()\n        mean_rooms_list.append(float(mean_rooms))\n        std_rooms = rooms_df['rooms'].std()\n        std_rooms_list.append(float(std_rooms))\n\n        garages_df = df.loc[(df['garages']) & ((df['area_kmeans'] ==  df['area_kmeans'][i]))]\n        mean_garages = garages_df['garages'].mean()\n        mean_garages_list.append(float(mean_garages))\n        std_garages = garages_df['garages'].std()\n        std_garages_list.append(float(std_garages))\n        \n        useful_area_df = df.loc[(df['useful_area']) & ((df['area_kmeans'] ==  df['area_kmeans'][i]))]\n        mean_useful_area = useful_area_df['useful_area'].mean()\n        mean_useful_area_list.append(float(mean_useful_area))\n        std_useful_area = useful_area_df['useful_area'].std()\n        std_useful_area_list.append(float(std_useful_area))\n\n    df['mean_rooms'] = mean_rooms_list\n    df['mean_garages'] = mean_garages_list\n    df['mean_useful_area'] = mean_useful_area_list\n    df['std_rooms'] = std_rooms_list\n    df['std_garages'] = std_garages_list\n    df['std_useful_area'] = std_useful_area_list    \n    \n    return df","7920be60":"df = feature_engineering(df)\ndf.head()","2c0b7353":"# Third model\ntarget = 'point_estimate'\nX, y = get_data_target(df, target)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nxg_reg = xgb.XGBRegressor()\n\npreds, model, best_iteration = train(xg_reg)\n\nevaluate_metrics(preds, model, best_iteration)","8123da36":"df = pd.read_csv('..\/input\/sao-paulo-real-estate-sale-rent-april-2019\/sao-paulo-properties-april-2019.csv')\ndf = df[df['Negotiation Type'] == 'sale']\ndf.head()","b3ffc911":"list_columns = ['Property Type', 'Latitude', 'Longitude', 'Negotiation Type']\ndf = drop_columns(df, list_columns)\n\n#df['District'] = df['District'].astype('category')\n\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndf['District'] = labelencoder.fit_transform(df['District'])\n\ntarget = 'Price'\nX, y = get_data_target(df, target)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nxg_reg = xgb.XGBRegressor()\n\npreds, model, best_iteration = train(xg_reg)\n\nevaluate_metrics(preds, model, best_iteration)","9622267a":"sample = pd.DataFrame({'Condo': 1300, 'Size': 100, 'Rooms': 2, 'Toilets': 2, 'Suites': 1, 'Parking': 1, 'Elevator': 1, 'Furnished': 0, 'Swimming Pool': 0, 'New': 0, 'District': 40}, index=[0])\n\nprint(model.predict(sample, ntree_limit=best_iteration))","e2bb68ed":"## Testando um dataset mais profundo\n\nCertamente um dos caminhos para melhorar o modelo de *valuation* de im\u00f3veis seria melhorar a base de dados. A base fornecida possui um n\u00famero limitado de *features* em rela\u00e7\u00e3o ao que \u00e9 poss\u00edvel encontrar dentro da l\u00f3gica do neg\u00f3cio.\n\n**Por exemplo:** Seria importante incluir mais caracter\u00edsticas em rela\u00e7\u00e3o aos im\u00f3veis, como quantidade de banheiros, su\u00edtes, elevadores, piscina, o bairro do im\u00f3vel, o valor do condom\u00ednio, se o im\u00f3vel \u00e9 novo, se o apartamento est\u00e1 sendo vendido mobiliado, etc.\n\nQuanto mais caracter\u00edsticas em rela\u00e7\u00e3o aos im\u00f3veis, melhor ser\u00e1 a performance do algoritmo, e v\u00e1rias caracter\u00edsticas podem ser facilmente obtidas atr\u00e1ves de webscraping ou em parcerias com outras empresas que possuem uma base de dados de im\u00f3veis.\n\nAlguns meses atr\u00e1s eu fiz o upload no Kaggle de um dataset com 13 mil im\u00f3veis localizados na cidade de S\u00e3o Paulo. Esses im\u00f3veis foram obtidos atrav\u00e9s do webscraping de diversos sites de im\u00f3veis do Brasil.\n\nApesar deste dataset ser menor em rela\u00e7\u00e3o ao dataset fornecido para o case, ele possui mais *features*.\n\nLogo abaixo eu rodei o mesmo modelo utilizado acima, mas agora utilizando os dados desta nova base de dados, que possui mais features em rela\u00e7\u00e3o a base fornecida para o case.","bdeeef2d":"Neste modelo anteior, como j\u00e1 era esperado, ocorreu um aumento do erro na m\u00e9trica RMSE e uma diminui\u00e7\u00e3o do R2. Contudo, nos livramos das vari\u00e1veis que estavam causando Data Leakage e temos um modelo mais confi\u00e1vel e generaliz\u00e1vel.\n\nPodemos tentar melhorar mais ainda o modelo utilizando algumas t\u00e9cnicas para aprimoramento das *features*. A fun\u00e7\u00e3o **feature_engineering** gera mais seis novas *features* no dataset para tentar ajudar o modelo a ter uma melhor performance.\n\nCalculamos a m\u00e9dia e o desvio padr\u00e3o dos valores de quartos, garagens e \u00e1rea \u00fatil para cada cluster.\n\n<b>Terceiro modelo:<\/b>","78f4e3ac":"# Modelo\n\nAbaixo \u00e9 explorado uma poss\u00edvel solu\u00e7\u00e3o para a precifica\u00e7\u00e3o de im\u00f3veis utilizando machine learning. Foram rodadas vers\u00f5es diferentes do modelo como forma de mostrar alternativas para melhorar os resultados encontrados.","ace5698c":"## Encontrando bairros atrav\u00e9s de Clustering\n\nConsiderando que obter as informa\u00e7\u00f5es sobre o bairro dos im\u00f3veis usando uma fonte externa (como o Google Maps API) em alguns casos n\u00e3o seja poss\u00edvel ou desej\u00e1vel, uma outra forma de contornar este problema usando os dados fornecidos pela base \u00e9 atrav\u00e9s de *Clustering*!\n\nPodemos clusterizar as informa\u00e7\u00f5es de Latitude e Longitude para estimar as sublocalidades. Na fun\u00e7\u00e3o abaixo utilizamos o algoritmo de **Kmeans** para prever os bairros dos im\u00f3veis nos dados.","b75be6d9":"Vemos que o processo de *feature engineering*, ou seja, gerar novas *features* derivadas das *features* j\u00e1 existentes, fornece um bom resultado. O modelo melhorou e essas novas *features* est\u00e3o com uma \"import\u00e2ncia\" alta.","36d5099c":"## Uma previs\u00e3o real\n\nS\u00e3o in\u00fameras as possibilidades de aplica\u00e7\u00e3o deste tipo de modelo para o setor imobili\u00e1rio. Conseguir saber com fundamento em dados o valor de um im\u00f3vel de acordo com suas caracter\u00edsticas \u00e9 de fato algo muito importante.\n\nAbaixo eu aplico o \u00faltimo modelo treinado em um im\u00f3vel da pr\u00f3pria Loft localizado no bairro Jardim Paulista, utilizando as caracter\u00edsticas apresentadas no site. Conseguimos assim obter uma previs\u00e3o de pre\u00e7o, que representa um valor te\u00f3rico dos im\u00f3veis com as mesmas caracter\u00edsticas na regi\u00e3o.","c1752f20":"A visualiza\u00e7\u00e3o dessas informa\u00e7\u00f5es pode ser bastante importante para uma empresa que deseja, por exemplo, expandir para outras cidades onde n\u00e3o se possui conhecimento t\u00e1cito sobre o local. Saber previamente qual a varia\u00e7\u00e3o de pre\u00e7o dos im\u00f3veis das subregi\u00f5es da cidade pode ser uma informa\u00e7\u00e3o crucial para o sucesso do neg\u00f3cio.","ff455524":"![title](https:\/\/i.imgur.com\/K7Tow8m.png)","cabb0558":"# Informa\u00e7\u00f5es estret\u00e9gicas\n\nPodemos utilizar as informa\u00e7\u00f5es da base para fazer an\u00e1lises sobre coisas interessantes em rela\u00e7\u00e3o a l\u00f3gica de n\u00e9gocio do setor imobili\u00e1rio. Responder algumas perguntas e chegar em algumas conclus\u00f5es importantes com o uso de dados.\n\n## Varia\u00e7\u00e3o de pre\u00e7os\n\nAbaixo podemos ver um gr\u00e1fico estilo Boxplot que mostra a varia\u00e7\u00e3o de pre\u00e7o dos im\u00f3veis (point_estimate) de acordo com as \u00e1reas encontradas pela clusteriza\u00e7\u00e3o. ","519c2c64":"## Google Maps API para aprimorar o dataset\n\n\u00c9 poss\u00edvel utilizar o Google Maps API para recuperar o Bairro de cada im\u00f3vel utilizando as informa\u00e7\u00f5es de **Latitude** e **Longitude** fornecidas no dataset. O c\u00f3digo abaixo realiza esta extra\u00e7\u00e3o e insere uma nova coluna no dataframe com o bairros.\n\n**Obs:** O c\u00f3digo abaixo n\u00e3o foi rodado devido a necessidade de utiliza\u00e7\u00e3o da API do Google. Para recuperar os bairros da base de dados seria necess\u00e1rio gastar algumas centenas de reais pelo uso da API e por isso fica aqui somente como sugest\u00e3o de que \u00e9 poss\u00edvel ser feito.","50b8d840":"<b>Quarto modelo:<\/b>","743a28f1":"## Treinamento e previs\u00f5es\n\nAbaixo rodamos diversas vers\u00f5es do modelo e analisamos os resultados.\n\n<b>Primeiro modelo:<\/b>","9bca52cc":"## Heatmap\n\nPodemos utilizar os dados de Latitude e Longitude, junto com as \u00e1reas encontradas pelo clustering, para criar um **mapa de calor** onde pode ser visualizado todos os im\u00f3veis da base de dados. Conseguimos identificar quais regi\u00f5es os im\u00f3veis da base est\u00e3o localizados e tamb\u00e9m quais os clusters que possuem a maior m\u00e9dia (marcador vermelho) e menor m\u00e9dia (marcador azul) de pre\u00e7o dos im\u00f3veis.","9e573dea":"# Explorando os dados","d653f497":"Os clusters abaixo s\u00e3o as \u00e1reas que possuem a maior m\u00e9dia de pre\u00e7o dos im\u00f3veis:","60a6168f":"A m\u00e9trica **RMSE**, que avalia a perfomance do modelo, foi muito melhor com esta nova base de dados. Isto mostra que obter mais *features* em rela\u00e7\u00e3o aos im\u00f3veis melhora o modelo de *valuation*.","eed78eb1":"Saber quais s\u00e3o as \u00e1reas de uma cidade que possuem as maiores ou menores variabilidades nos pre\u00e7os \u00e9 uma informa\u00e7\u00e3o bastante importante para empresas que atuam no setor imobili\u00e1rio. Estrat\u00e9gias de marketing, por exemplo, podem fazer uso dessa informa\u00e7\u00e3o para elaborar a\u00e7\u00f5es em regi\u00f5es espec\u00edficas da cidade. Equipes respons\u00e1veis pela aquisi\u00e7\u00e3o de novos im\u00f3veis tamb\u00e9m podem utilizar esta informa\u00e7\u00e3o para identificar as diferentes realidades das diversas regi\u00f5es da cidade.","f2297e27":"![title](https:\/\/i.imgur.com\/NsCStAG.png)\n<p style=\"text-align:center\">\n    Este \u00e9 um notebook de apresenta\u00e7\u00e3o da resolu\u00e7\u00e3o do business case de Data Science fornecido pela Loft.<br>\n    <span style=\"font-size:10px;\">Desenvolvido por Pedro Almeida.<\/span>\n<\/p>","bc80c7d3":"\n# Conclus\u00f5es e sugest\u00f5es\n\nEste presente estudo procurou explorar a base de dados fornecida pela Loft e extrair informa\u00e7\u00f5es importantes com implica\u00e7\u00f5es para o seu neg\u00f3cio.\n\nMesmo n\u00e3o possuindo informa\u00e7\u00f5es sobre os bairros onde os im\u00f3veis da base estavam localizados, uma informa\u00e7\u00e3o bastante importante no mercado imobili\u00e1rio, foi poss\u00edvel utilizar t\u00e9nicas de clusteriza\u00e7\u00e3o para obter uma aproxima\u00e7\u00e3o dessa informa\u00e7\u00e3o. Esses clusters se mostraram bastante importantes como *feature* (caracter\u00edstica) para o modelo desenvolvido.\n\nDescobrimos que mesmo a base possuindo pre\u00e7os m\u00e1ximos e m\u00ednimos estimados para os im\u00f3veis, estas informa\u00e7\u00f5es n\u00e3o s\u00e3o \u00fateis para o desenvolvimento de um modelo de machine learning. Porque a utiliza\u00e7\u00e3o dessas informa\u00e7\u00f5es leva ao problema de Data Leakage, onde um modelo aparenta bons resultados, mas n\u00e3o \u00e9 generaliz\u00e1vel para dados novos.\n\nVimos que realizar processos de *feature engineering*, onde novas *features* s\u00e3o criadas derivadas das *features* j\u00e1 existentes, \u00e9 bastante relevante para melhorar as previs\u00f5es do modelo.\n\nPor fim, apresentei uma sugest\u00e3o de outra estrutura de base de dados do setor imobili\u00e1rio, desenvolvida por mim alguns meses atr\u00e1s, que possui informa\u00e7\u00f5es mais detalhadas sobre im\u00f3veis na regi\u00e3o de S\u00e3o Paulo. Esta nova base se apresentou mais eficaz e mostrou melhores resultados quando aplicada ao modelo de *valuation* de im\u00f3veis.","63277bbd":"Analisando os resultados, fica claro que esta vers\u00e3o do modelo acima sofre de **Data Leakage**, pois as vari\u00e1veis *minimum_estimate* e *maximum_estimate* s\u00e3o altamente correlacionadas com o target utilizado -> *point_estimate*. Isto acontece quando os dados que estamos usando para treinar o algoritmo cont\u00e9m informa\u00e7\u00f5es do que n\u00f3s estamos tentando prever. No caso, \u00e9 poss\u00edvel descobrir o nosso target (point_estimate) simplesmente manipulando as outras vari\u00e1veis do modelo, sem nem mesmo precisar utilizar Machine Learning.\n\nPor mais que estejamos utilizando *early stopping* para evitar overfitting no modelo, s\u00e3o as proprias features que est\u00e3o causando este problema. Neste caso, \u00e9 importante ficar atento, pois, por mais que este modelo esteja aparentemente gerando excelentes previs\u00f5es com os dados de teste, o modelo n\u00e3o \u00e9 generalizavel para dados novos.\n\nLogo abaixo rodamos uma outra vers\u00e3o do modelo, agora retirando as features *minimum_estimate* e *maximum_estimate*.\n\n<b>Segundo modelo:<\/b>","3ad21f49":"Este tipo de an\u00e1lise pode ser \u00fatil tanto para a elabora\u00e7\u00e3o do pre\u00e7o final de venda do im\u00f3vel, quanto para a aquisi\u00e7\u00e3o de im\u00f3veis. O output de resultados como este, em conjunto com o modelo de neg\u00f3cio, pode acelerar ainda mais a compra instant\u00e2nea de im\u00f3veis."}}