{"cell_type":{"84b1a86e":"code","f29fe1f0":"code","0c1769ad":"code","cf1bc6c9":"code","af3775b0":"code","0ed3a8bc":"code","bb4ddab6":"code","3a0a475c":"markdown","d64436da":"markdown","cb80c52b":"markdown","9b0abf82":"markdown"},"source":{"84b1a86e":"!pip install --upgrade mxnet-cu100\n!pip install autogluon","f29fe1f0":"# Load the dataset\nfrom autogluon.tabular import TabularDataset, TabularPredictor\ntrain_data = TabularDataset('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest_data = TabularDataset('..\/input\/tabular-playground-series-jun-2021\/test.csv')","0c1769ad":"train_data.head(5)","cf1bc6c9":"test_data.head(5)","af3775b0":"# Fit AutoGluon on the data, using the 'target' column as the label.\n\nlabel = 'target'\nfit_args = {}\n\n# If you want to speed up training, exclude neural network models via:\n# fit_args['excluded_model_types'] = ['NN', 'FASTAI']\n\npredictor = TabularPredictor(label=label, eval_metric='log_loss').fit(train_data, time_limit = 60*60*8, presets='best_quality', **fit_args)","0ed3a8bc":"# Get prediction probabilites\nprobs=predictor.predict_proba(test_data, as_multiclass=True)\nprobs","bb4ddab6":"import pandas as pd\n\nsubmit = test_data[['id']]\nsubmit = pd.concat([submit, probs], axis=1)\nsubmit.to_csv('submission.csv',index=False)","3a0a475c":"Making predictions with the best model trained so far. ","d64436da":"### AutoGluon - AutoML framework\n\nAutoGluon is built upon the emphasis of ensembling over hyperparameter tuning. Typically, in order to improve model performance, we can either pursue hyperparameter tuning in order to find the best set of hyperparameters corresponding to data or we can pursue model ensembling - bagging, boosting and stacking.\n\nHowever, performing an exhaustive search among a large space of hyperparameters can be highly time-consuming. At the same time, if your training data changes, the best set of hyperparameters you found out may no longer be the best, and so you would have to find them again.\n\nThis is the reason why AutoGluon focuses on building highly stacked ensembles, believing that you can still achieve optimal model performances without tuning hyperparameters at all.\n\nTutorials: https:\/\/auto.gluon.ai\/dev\/tutorials\/tabular_prediction\/index.html\n\nGitHub: https:\/\/github.com\/awslabs\/autogluon\/","cb80c52b":"Some pointers to note about AutoGluon:\n1. You can specify the metric that you want to track. In our case, it is **log_loss** and can be specified in the <code>eval_metric<\/code> argument.\n2. You can specify which models to fit. Not specifying will iterate over all algorithms in the library.\n3. You can also specify which models to exclude. Models like Neural Networks may take relatively longer to train.\n4. It is very important to specify the time limits. Specifying a time limit of **8 hours** should be best since the Kaggle run-time limit is **9 hours** and the kernel shall take some time in making predictions beyond 8 hours of training.\n5. Models will run on CPU. **AutoGluon in currently not GPU-compatible**, so don't waste your GPU run-time keeping it on!\n    ","9b0abf82":"**In order to get best predictions, we need to train on 100% of data.** AutoGluon ensures that the model **predictions made later are with the best model trained in the fitting history**. \n\nIn order to confirm that, we can split the training data as 80\/20 and track performance of various fitted models. "}}