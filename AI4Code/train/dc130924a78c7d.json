{"cell_type":{"57d3b180":"code","7aab7192":"code","e5fed82b":"code","ac6506e5":"code","93c0ebff":"code","b00d88f7":"code","89a9f61d":"code","5972bb6b":"code","904ef8d2":"code","9198204c":"code","1a422c9d":"code","afa1273f":"code","89fa54e3":"code","e29a59ba":"code","3f7c1adb":"code","77fdc7a0":"code","3f003b05":"code","5cb07c1c":"markdown","4e9c99f1":"markdown","14a1766f":"markdown","83344f33":"markdown","12679bb6":"markdown","5d6faa4a":"markdown","cc2436ed":"markdown","ef40bc74":"markdown","b5a9420c":"markdown","3e34de16":"markdown","6e4cacf9":"markdown","f386b5d8":"markdown","cb184171":"markdown","2acd856a":"markdown","7de1820d":"markdown","f46270e9":"markdown","34984b7b":"markdown","52ca2064":"markdown","ea3e5c54":"markdown","53952e39":"markdown"},"source":{"57d3b180":"!pip install sweetviz\n\n# import numpy, matplotlib, etc.\nimport numpy as np\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport sweetviz as sw\nfrom math import sqrt\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.mode.chained_assignment = None  \n\n# sklearn imports\nfrom sklearn import metrics\nfrom sklearn import pipeline\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn import neural_network\nfrom sklearn import model_selection\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, RepeatedKFold, GridSearchCV\nfrom sklearn.model_selection import LeavePOut\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import compose\n\nfrom tqdm import tqdm\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7aab7192":"datacsv = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntestcsv = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_id = testcsv['Id'] # ID list to be added to a result vector\nreport = sw.analyze(datacsv)\nreport.show_notebook(layout='vertical')","e5fed82b":"def dropColumns(csv,columns):\n    for column in columns:\n        csv.drop(column,axis=1,inplace=True)\n\ndef setAsMeanVal(csv,columns):                  # Replacing the missing values with the mean.\n    for column in columns:\n        csv[column].fillna(csv[column].mean(),inplace = True) \n\ndef setAsMostOccur(csv,columns):                # Replacing the missing values with the most occurred number of times.\n    for column in columns:\n        csv[column].fillna(csv[column].mode()[0],inplace = True) \n\ndef setAsNone(csv,columns):                                      # Some of our features appear as NAN but actually are None\n    for column in columns:\n        csv[column].fillna('None',inplace=True)\n\n\n\n#Train        \ndropColumns(datacsv,['Id','Street','Utilities','Condition2','RoofMatl','Heating','PoolQC','Alley','BsmtHalfBath','3SsnPorch','LowQualFinSF','KitchenAbvGr','PoolArea','Fence','MiscFeature','MiscVal'])\nsetAsMeanVal(datacsv,['LotFrontage', 'MasVnrArea', 'GarageYrBlt'])\nsetAsMostOccur(datacsv,['MasVnrType', 'Electrical'])                                        \nsetAsNone(datacsv,['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond'])\n#Test\ndropColumns(testcsv,['Id','Street','Utilities','Condition2','RoofMatl','Heating','PoolQC','Alley','BsmtHalfBath','3SsnPorch','LowQualFinSF','KitchenAbvGr','PoolArea','Fence','MiscFeature','MiscVal'])\nsetAsMeanVal(testcsv,['LotFrontage', 'MasVnrArea', 'GarageYrBlt','BsmtFinSF1','BsmtUnfSF','TotalBsmtSF','GarageArea'])\nsetAsMostOccur(testcsv,['MasVnrType','Electrical','MSZoning','KitchenQual','BsmtFullBath','GarageCars'])\nsetAsNone(testcsv,['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond'])\n\n","ac6506e5":"cols = ['SalePrice','OverallQual','GrLivArea','TotalBsmtSF','GarageCars','GarageArea','1stFlrSF','YearBuilt']\nsns.pairplot(datacsv[cols],height=1.5)\nplt.show()","93c0ebff":"plt.figure(figsize=[10,5])\nsns.boxplot(x='OverallQual', y='SalePrice', data=datacsv)\nplt.show()\n\nplt.figure(figsize=(6,6))\nsns.regplot(x='YearBuilt', y='SalePrice', data=datacsv, line_kws={\"color\": \"red\"})\nplt.show()\n\nplt.figure(figsize=[10,5])\nsns.regplot(x='GrLivArea', y='SalePrice', data=datacsv, line_kws={\"color\": \"red\"})\nplt.show()","b00d88f7":"datacsv[\"TotalArea\"] = datacsv.GrLivArea + datacsv.TotalBsmtSF\ntestcsv[\"TotalArea\"] = testcsv.GrLivArea + testcsv.TotalBsmtSF # Same thing has to be done to the test!\nsetAsMeanVal(testcsv,['TotalArea'])\nsns.set_style(\"darkgrid\")\nsns.set_palette(\"dark\")\nsns.regplot(data=datacsv, x='TotalArea',y='SalePrice',line_kws={\"color\": \"red\"})\nplt.title('TotalArea VS SalePrice',fontsize=22)\nsns.despine()\nplt.show()","89a9f61d":"corr=datacsv.corr()\nsns.set(font_scale=1)\nplt.figure(figsize=(10, 13))\nsns.set_palette(\"dark\")\nabs(corr['SalePrice']).sort_values()[:-1].plot.barh()\nplt.gca().set_facecolor('#FFFFFF')\nplt.title('SalePrice Correlations',fontsize=20)\nplt.show()","5972bb6b":"corr = datacsv.corr()\ncorr = abs(corr['SalePrice']).sort_values()[:-1]\ntoDropList=corr[corr.values < 0.15].index\ndatacsv.drop(toDropList, axis=1, inplace=True)\ntestcsv.drop(toDropList, axis=1, inplace=True) # exact same variables will be dropped from the test, regardless of their relevancy, based on my analysis","904ef8d2":"report = sw.analyze(datacsv)\nreport.show_notebook(layout='vertical')","9198204c":"t = datacsv['SalePrice'].copy() #Pure Result expectaion vector\nx = datacsv.drop(['SalePrice'], axis=1) #Pure Data, without results\n\nnumerical_cols = x.select_dtypes(include=['int64', 'float64']).columns #list all variables that have numerical values\ncategorical_cols = x.select_dtypes(include=['object', 'bool']).columns #list all variables that have categorial values\nall_cols = categorical_cols.tolist() + numerical_cols.tolist() #list them together\n\nct_enc_std = ColumnTransformer([\n            (\"encoding\", OrdinalEncoder(), categorical_cols),\n            (\"standard\", MinMaxScaler(), numerical_cols)]) \nx_encoded = pd.DataFrame(ct_enc_std.fit_transform(x, t), columns=all_cols)\nselector = RFECV(SGDRegressor(random_state=73), cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=73)).fit(x_encoded, t) # regressor selects best features to keep\nx_less_features = x_encoded.loc[:, selector.support_]\nprint(x_less_features)\n","1a422c9d":"hyper_parameters = {'penalty': ('l2', 'l1', 'elasticnet'), 'alpha':[0.0001, 0.001, 0.01, 0.1], 'learning_rate':['constant'], 'eta0':[0.0001, 0.001, 0.01, 0.1]}\n\ngs_model1 = GridSearchCV(SGDRegressor(random_state=73), hyper_parameters).fit(x_encoded, np.log(t))\nprint('best parameters model 1: ', gs_model1.best_params_)\ngs_model2 = GridSearchCV(SGDRegressor(random_state=73), hyper_parameters).fit(x_less_features, np.log(t))\nprint('best parameters model 2: ', gs_model2.best_params_)","afa1273f":"model1 = SGDRegressor(alpha=gs_model1.best_params_['alpha'], learning_rate='constant', eta0=gs_model1.best_params_['eta0'], penalty=gs_model1.best_params_['penalty'])\nmodel2 = SGDRegressor(alpha=gs_model2.best_params_['alpha'], learning_rate='constant', eta0=gs_model2.best_params_['eta0'], penalty=gs_model2.best_params_['penalty'])","89fa54e3":"def find_generator_len(generator, use_pbar=True):\n    i = 0\n    if use_pbar:\n        pbar = tqdm(desc='Calculating Length', ncols=1000, bar_format='{desc}{bar:10}{r_bar}')\n    for a in generator:\n        i += 1\n        if use_pbar:\n            pbar.update()\n    if use_pbar:\n        pbar.close()\n    return i","e29a59ba":"# calculate score and loss from cv (KFold or LPO) and display graphs\ndef CVscoreLoss(x, t, model, k=None, p=None, show_score_loss_graphs=False, use_pbar=True):\n    scoresLosses_df = pd.DataFrame(columns=['fold_id', 'split', 'score', 'loss'])\n\n    if k is not None:\n        cv = KFold(n_splits=k, shuffle=True, random_state=73)\n    elif p is not None:\n        cv = LeavePOut(p)\n    else:\n        raise ValueError(\"Missing k or p to calculate Cross Validation\")\n\n    if use_pbar:\n        pbar = tqdm(desc='Calculating Models', total=find_generator_len(cv.split(x)))\n\n    for i, (train_ids, val_ids) in enumerate(cv.split(x)):\n        x_train = x.loc[train_ids] # Create train x and t\n        t_train = t.loc[train_ids]\n        x_val = x.loc[val_ids] # Create valid x and t\n        t_val = t.loc[val_ids]\n\n        model.fit(x_train, t_train) #train the model\n\n        y_train = model.predict(x_train) # Getting predictions\n        y_val = model.predict(x_val)\n        \n        y_train = np.exp(y_train)\n        y_val = np.exp(y_val)\n\n        scoresLosses_df.loc[len(scoresLosses_df)] = [i, 'train', model.score(x_train, t_train), mean_squared_error(t_train, np.log(y_train), squared = False)]\n        scoresLosses_df.loc[len(scoresLosses_df)] = [i, 'val', model.score(x_val, t_val), mean_squared_error(t_val, np.log(y_val), squared = False)]\n\n        if use_pbar:\n            pbar.update()\n\n    if use_pbar:\n        pbar.close()\n\n    val_scores_losses_df = scoresLosses_df[scoresLosses_df['split']=='val']\n    train_scores_losses_df = scoresLosses_df[scoresLosses_df['split']=='train']\n\n    mean_val_score = val_scores_losses_df['score'].mean()\n    mean_val_loss = val_scores_losses_df['loss'].mean()\n    mean_train_score = train_scores_losses_df['score'].mean()\n    mean_train_loss = train_scores_losses_df['loss'].mean()\n\n    if show_score_loss_graphs:\n        fig = px.line(scoresLosses_df, x='fold_id', y='score', color='split', title=f'Mean Val Score: {mean_val_score:.2f}, Mean Train Score: {mean_train_score:.2f}')\n        fig.show()\n        fig = px.line(scoresLosses_df, x='fold_id', y='loss', color='split', title=f'Mean Val Loss: {mean_val_loss:.2f}, Mean Train Loss: {mean_train_loss:.2f}')\n        fig.show()\n        \nprint(\"Model 1, k = 10 Figures: \")\nCVscoreLoss(x_less_features, np.log(t), model1, k=10, show_score_loss_graphs=True, use_pbar=True )\nprint(\"Model 2, k = 10 Figures: \")\nCVscoreLoss(x_less_features, np.log(t), model2, k=10, show_score_loss_graphs=True, use_pbar=True )\nprint(\"Model 1, k = 5 Figures: \")\nCVscoreLoss(x_less_features, np.log(t), model1, k=5,  show_score_loss_graphs=True, use_pbar=True )\nprint(\"Model 2, k = 5 Figures: \")\nCVscoreLoss(x_less_features, np.log(t), model2, k=5,  show_score_loss_graphs=True, use_pbar=True )","3f7c1adb":"print(\"Model 2, p = 1 Figures (takes a short while to load): \")\nCVscoreLoss(x_less_features, np.log(t), model2, p=1,  show_score_loss_graphs=True, use_pbar=False ) # CHANGE THIS TO TRUE FOR DEBUGGING PURPOSES. Settle on keeping this at false.","77fdc7a0":"x_col_list = x_less_features.columns.values.tolist() # List the columns we wish to keep\ntestcsv = testcsv[x_col_list] # Drop what we don't care about. Like me feelings\nnumerical_cols_test = testcsv.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols_test = testcsv.select_dtypes(include=['object', 'bool']).columns\nct_enc_std_test = ColumnTransformer([\n            (\"encoding\", OrdinalEncoder(), categorical_cols_test),\n            (\"standard\", MinMaxScaler(), numerical_cols_test)])\n \n\nx_encoded_test = pd.DataFrame(ct_enc_std_test.fit_transform(testcsv), columns=x_col_list) #activate the encoder and scaler for all cols\nprint(x_encoded_test)\n","3f003b05":"test_pred_model = model2.predict(x_encoded_test)\ntest_pred_model_exp = np.exp(test_pred_model)\nresult = pd.DataFrame()\nresult['Id'] = test_id\nresult['SalePrice'] = test_pred_model_exp\nresult.to_csv('submission.csv', index = False)\nprint(result)","5cb07c1c":"# **CHAPTER 2: LOOKING FOR CORRELATION! (you'll get tired of this word)**\n\nNew we would want to look at the strong correlations (for that we need to click on the \"Associations\" button at the top of the analysis.\n\"SalesPrice\" is the bottom row, and it would be the easiest to check column by column, and search for high correlations. Deeper the blue, the better.\nNamely we'll see the correlation between the House Price, the Overall Quality (which obviously seems to have a large significance), Ground Living Area, Basement Area, 1st floor area, Size of the Garage, and The year it was built.\n\nIt is expected these parameters on the surface would have a good correlation with the price, at least:","4e9c99f1":"# **CHAPTER! ... I LOST COUNT!**\n# **BIBLIOGRAPHY**\n(Or in my words: the part you shouldn't read)\n\n\nI'd like to thank google, my mother, god, and the inventor of coffee, whoever you may be.\n\n[Vivek Chowdhury's notebook](https:\/\/www.kaggle.com\/vivek468\/can-we-predict-house-prices-sweetvizeda)\n\n[shivam's notebook](https:\/\/www.kaggle.com\/shivam546\/house-pricing-dataset)\n\nAlmog's Notebook. I ONLY PEEKED! I **TOTALLY DIDN'T COPY PASTE!** <font size=\"0.7\">only a little<\/font>","14a1766f":"We can see that the overall Quality does seem to be a good qualifier for price (again, it seems that the houses with the score of 10 are somehow less reliable than the lower scores, **for the degree of error!**)\n\nFor the \"year built\" we see less of a clear correlation, but the trend does follow, we see that for older houses the degree of error is quite large, but for newer homes the degree of certainty is quite good!\n\nAnd best of all, in my opinion, is the living area, I think we should try to combine it with the underground area for a new shining \"total area\" variable, that could be a better predictor.","83344f33":"The total area variable seems to be a good discision, having it a 2nd best indicator for the house value.\nHowever, we would want to get rid of bad correlating variables.\nWe will set the bar at aprox 0.15 correlation rate, and drop whatever don't fir the criteria:","12679bb6":"\nSo now we know that the better model would have\n* alpha=0.001\n* eta0=0.01\n* learning_rate=constant\n* penalty=elasticnet\n\nHow do I know that?\nBecause I LISTENED DURING THE BEGGINING OF CHAPTER 3.\n\n# **Chapter 4: There is no chapter 4.**\n\nNo seriously, let me just apply the model, that's all.\nFirst step would be making a list of the features that we want.\nThen we just abra-cadabra the columns we don't like.\nWe'll print the data after we applied the encoding, and look at the data.","5d6faa4a":"If at first you don't read it clearly, its ok, let me analyze it for you!\n\n![what is that](https:\/\/media.istockphoto.com\/photos\/young-woman-with-bad-eyesight-using-laptop-wearing-glasses-picture-id1291768231?k=20&m=1291768231&s=612x612&w=0&h=amiivJJOKr8VezkIMeoibry5dtw0_Or1bd79Xz9sUwA=)\n\nIf at first you don't read it clearly, its ok, let me analyze it for you!\n\nFunnily enough, some conclusions can be made that aren't nesseccerily related to price:\nIn most cases, the Ground Level area functions as an upper bound for the underground space, that is to be expected (therefore, given **ONLY** the underground area, we can say with a high degree of confidence the above-ground area limit).\n\nThe overall quality of a house is ranked from 0 to 10, and can be a good indicator for the price RANGE, not as much for the house total price (also, houses that got the grade 9 have a more-stable price range than the 10 rank).\n\nGarage space isn't as good an indicator, but does have some correlation, it seems.\n\nYear built does correlate in a good degree with the price range, and with the area of the house! Turns out, the newer the house, the bigger the area, on average!\n\nConclusion: We'll draw the above-ground area paremeters, and the overall quality, and year built all VS sales price, these seem to be the most interesting:","cc2436ed":"The result?","ef40bc74":"if this seems excessive, its because I change my mind easily.\n\n![cart](https:\/\/c.tenor.com\/02CP9fEzkbkAAAAC\/target-shopping.gif)\n\nNice! We have a lot to cook with, now.\nNext step is just analyzing the raw data. We'll use sweetviz for that, they're awesome at doing that.","b5a9420c":"# House prices are difficult to predict. BUT LET'S TRY ANYWAY!\n\n![image.png](attachment:433a8fb7-4e36-4e18-b285-feb3fd0cda7c.png)\n\nTo start, we gotta import some stuff. This is the boring bit, just putting stuff from the shelf into out cart.\n\n\nLETS US START!\n\n# **Chapter 1: Preparing the ground!**\n","3e34de16":"Aaaaaand success!!!\nWho knew? with the exception of quite literally MANSION sized houses, the new data-set seems like a good predictor for the house price. (Also, seems like there's a singular anomality as a house in Iowa with a total of about 12,000 squared feet for the low low price of ~200,000 dollars, that's a catch if I've ever seen one.\n\n![minecraft house](https:\/\/i.imgur.com\/tZYCK30.png)\n\nAlright, next thing we'll do is observe the correlations with sale price, and rank them:","6e4cacf9":"Now thatNow that we dropped what we deeped unfit, we are left with less features we deem the best fit to predict.\nWe can redo the SweetViz and see the new correlation heatmap, allong with the completed data:","f386b5d8":"To finish our model, we need to find hyper-parameters (and narrow down our 59 parameters to a lot less).\nWe will start by narrowing down our best features:","cb184171":"Now that we have our two models, we need to check if the features are good enough for a good prediction, as well as deciding which model is better.","2acd856a":"After running our feature selector we can see that we have 37 features remaining (whereas we had many more before).\n\n# **CHAPTER 3: MODEL WORK.**\n(not the look-pretty-kind)\n\nBesides the fact that 37 and 73 (the random seed) are the best couple of numbers in nature...\n\n![Bazinga](https:\/\/i.imgur.com\/3f6892j.png)\n\n\nBUT ENOUGH NERDING OUT! Now we need to check each model to see which one fits better.\nFor that, let's find the best parameters for each model:","7de1820d":"Now that we know the best parameters for the feature selection we can use RFECV and use CV to choose the best number of features on our dataset. The default CV is 5. We will enter the Scikit-learn RepeatedKFold to repeat every KFold several times with different splits:","f46270e9":"![result](https:\/\/i.imgur.com\/g55AMaD.png)\n\n\nThe result seem to be some kind of a weird number, but ok, its whatever I guess.","34984b7b":"If you REALLY wanna see the progress, and also see it move. It's exciting, I know.","52ca2064":"Just by looking at it, it is clear that some data is either missing, incomplete, or redundant at some parts.\nWe'll take care of that, right now.\nTo clean the data, we will remove the following parameters:\n   1. We don't need \"ID\" parameters.\n   2. Street - only 2\/1460 are gravel, the rest are pavement. Besides less than 0.01 correlation. BYE!\n   3. Alley - Missing 94% data is too much to bear. Besides, 0,14 correlation?! BYE!\n   3. Utilities - only 2\/1460 have \"Electricity and Gas Only\", the rest have \"All public Utilities\", Besides less than 0.01 correlation, BYE!\n   4. Condition2 - only 8\/1460 are \"Adjacent to feeder street\", while the rest are just \"Normal\", and have a 0.1 correlation with price, BYE!\n   5. RoofMatl - nearly all roofs (98%) are \"Standard (Composite) Shingle\". Also, low correlation to price, BYE!\n   6. Heating - 98% are \"Gas forced warm air furnace\", also it has only 0.12 correlation with sale price.\n   7. PoolQc - more than 99% of the data is missing. We can't deduce anything if you don't have a pool, BYE!\n   8. BsmtHalfBath - 94% are 0, also it has only 0.01 correlation with sale price, BYE!\n   9. 3SsnPorch - 98% are 0, also it has only 0.04 correlation with sale price, BYE!\n   10. LowQualFinSF - 98% are 0, also it has less than 0.01 correlation with sale price, BYE!\n   11. KitchenAbvGr - has 0.14 correlation, and less than 1% have 2 kitchens. ALSO WHY WOULD YOU WANT 2 KITCHENS?! BYE!\n   12. PoolArea - More than 99% are 0. If you don't have a pool, you can't be cool. BYE!\n   13. Fence - about 81% is missing, that's too much in my opinion, even with these 0.19 correlation to sale price. OUT! BYE!\n   14. MiscFeature - 96% are None, also it has only 0.01 correlation with sale price. BYE!\n   15. MiscVal - More than 96% are 0, also it has only 0.02 correlation with sale price. If you can't show off anything nice, then BYCE! (I can't think of a rhyme)\n  \n![pool is cool](https:\/\/cdn.shopify.com\/s\/files\/1\/1259\/9857\/products\/chaos_pool_jig_1000_1_grande.png?v=1579731658) \n\nAlso, if some data has missing components, we'll fill it in. Some of the data has \"NA\" as \"none\", we have to fix that, too, as well as missing data:","ea3e5c54":"Leaving in the function the option to turn on or off the progress bar was a smart design choice that I can't take credit for, thats kind of brilliant.\n\n![statistics](https:\/\/i.redd.it\/4f71u8ti5hg31.jpg)\n\n\nAfter look at the two models, Model 2 seems like the more reliable one (smaller Loss, better Score).\nLets try to run Model 2 with p=1:","53952e39":"Does it look pretty?\nshouldn't be, I only care about seeing it has 37 columns, and that the data make sense (to some degree, you know)...\n\nOK, next thing is just preparing the model (I remind you, I prefer model 2), print, and viola."}}