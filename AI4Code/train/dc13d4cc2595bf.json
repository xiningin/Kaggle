{"cell_type":{"bdae58d4":"code","9f089b78":"code","d23d74c3":"code","01e77a92":"code","b39cf704":"code","75cfe2df":"code","4fee76e1":"code","b3b746d9":"markdown"},"source":{"bdae58d4":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport cv2","9f089b78":"import pandas as pd\nimport numpy as np\nimport torch\n\n\nINPUT_PATH = '\/kaggle\/input\/bengaliai-cv19'\ntest_df = pd.read_csv(INPUT_PATH + '\/test.csv')\nsubmission_df = pd.read_csv(INPUT_PATH + '\/sample_submission.csv')\n\nresults = []","d23d74c3":"import cv2\nfrom tqdm.auto import tqdm\n\nHEIGHT = 137\nWIDTH = 236\nSIZE = 128\n\ndef bbox(img):\n    rows = np.any(img, axis=1)\n    cols = np.any(img, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n    return rmin, rmax, cmin, cmax\n\ndef crop_resize(img0, size=SIZE, pad=16):\n    #crop a box around pixels large than the threshold \n    #some images contain line at the sides\n    ymin,ymax,xmin,xmax = bbox(img0[5:-5,5:-5] > 80)\n    #cropping may cut too much, so we need to add it back\n    xmin = xmin - 13 if (xmin > 13) else 0\n    ymin = ymin - 10 if (ymin > 10) else 0\n    xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n    ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n    img = img0[ymin:ymax,xmin:xmax]\n    #remove lo intensity pixels as noise\n    img[img < 28] = 0\n    lx, ly = xmax-xmin,ymax-ymin\n    l = max(lx,ly) + pad\n    #make sure that the aspect ratio is kept in rescaling\n    img = np.pad(img, [((l-ly)\/\/2,), ((l-lx)\/\/2,)], mode='constant')\n    return cv2.resize(img,(size,size))\n\ndef Resize(df,size=128):\n    resized = {} \n    df = df.set_index('image_id')\n    for i in tqdm(range(df.shape[0])):\n       # image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size))\n        image0 = 255 - df.loc[df.index[i]].values.reshape(137,236).astype(np.uint8)\n        #normalize each image by its max val\n        img = (image0*(255.0\/image0.max())).astype(np.uint8)\n        image = crop_resize(img)\n        resized[df.index[i]] = image.reshape(-1)\n    resized = pd.DataFrame(resized).T.reset_index()\n    resized.columns = resized.columns.astype(str)\n    resized.rename(columns={'index':'image_id'},inplace=True)\n    return resized","01e77a92":"from torch.utils.data import Dataset, DataLoader\n\nh, w = 128, 128\n# h, w = 137, 236\nclass GraphemeDataset(Dataset):\n    def __init__(self,df,_type='train'):\n        self.df = df\n        self.data = df.iloc[:, 1:].values\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self,idx):\n        name = self.df.iloc[idx,0]\n        image = self.data[idx, :].reshape(h, w).astype(np.float)\n        image = np.stack([image, image, image], axis=-1)\n        image = np.transpose(image, (2, 0, 1))\n        return image,name","b39cf704":"test = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/test.csv')\ntest_data = ['test_image_data_0.parquet','test_image_data_1.parquet','test_image_data_2.parquet','test_image_data_3.parquet']","75cfe2df":"%%time\n## Inference a little faster using @Iafoss and  @peters technique\nrow_id, target = [], []\nimgs = []\nfor fname in test_data:\n    print(fname)\n    data = pd.read_parquet(f'\/kaggle\/input\/bengaliai-cv19\/{fname}')\n    data = Resize(data)\n    test_image = GraphemeDataset(data)\n    dl = torch.utils.data.DataLoader(test_image, batch_size=128, num_workers=4, shuffle=False)\n    with torch.no_grad():\n        for x, y in tqdm(dl):\n            # x = x.unsqueeze(1).float().cpu()\n            x = x.squeeze(1).detach().cpu().numpy()\n            for idx in range(x.shape[0]):\n                cimg = np.transpose(x[idx, :, :, :], (1, 2, 0))\n                imgs.append(cimg)           ","4fee76e1":"fig, axs = plt.subplots(4, 3, figsize=(20,10))\n\nfor row in range(4):\n    for col in range(3):\n        idx = row*3 + col\n        img = imgs[idx]\n        axs[row, col].imshow(img, cmap='gray')\n        axs[row, col].axis('off')","b3b746d9":"A look at how the test images actually look"}}