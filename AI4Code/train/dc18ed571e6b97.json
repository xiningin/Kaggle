{"cell_type":{"e78595e4":"code","29741a59":"code","e489d864":"code","d0c71f12":"code","5b24f4b2":"code","76e2bab8":"code","ac102aed":"code","35798c5a":"code","0c0c4642":"code","65e6a54e":"code","541cc103":"code","6b418634":"code","afc401c8":"code","1b390434":"code","cc591323":"code","8b7f3be1":"code","00ade84b":"code","64bb60e0":"code","78ca04f5":"code","e842f887":"code","8e8bcb04":"code","dcb35839":"code","19ed0284":"code","2ff7e10c":"code","252334de":"code","8fd94004":"code","91fa1949":"code","ec566d90":"code","54d51dc1":"code","37e2b7c5":"code","7ae6f985":"code","5d1c64b2":"code","42fc26e0":"code","09f7f567":"code","6827713f":"code","bb361054":"code","491b6e40":"code","33ff2b09":"code","c8b322d1":"code","dff69295":"code","ea328850":"code","5ca406ef":"code","8c3d4e8f":"code","dd6e23c3":"code","3c2ec597":"code","c05d6511":"code","17f44996":"code","69c6f701":"code","5d006e05":"code","5e5d1f18":"code","252ade93":"code","59c472c8":"code","1bbcb4c2":"code","28d2b779":"code","981fe1d9":"code","01a90d78":"code","335f6dfc":"code","73c901a2":"code","9bb202fd":"code","7b1f9ebb":"code","01621911":"code","df152ba1":"code","01d4391b":"code","dba8a9b7":"code","c9d743c1":"code","84067800":"code","6df941f5":"markdown","dbea9870":"markdown","89894473":"markdown","f5b664b4":"markdown","1e206283":"markdown","ce44d405":"markdown","69ef0799":"markdown","33049519":"markdown","e5947e2d":"markdown","5aace9a6":"markdown","14f3aa28":"markdown","766584ea":"markdown","c228b8c9":"markdown","cfb9b548":"markdown","d36f3298":"markdown","69bdb6de":"markdown","18a27079":"markdown","a5dfb0ce":"markdown","e3249365":"markdown","4ea7b677":"markdown","493e4d3d":"markdown","23a1a4cf":"markdown","91c70145":"markdown","3040be12":"markdown","7f33ca9f":"markdown","fb689dc9":"markdown","2704e102":"markdown","18e3dcef":"markdown","c8c964a6":"markdown","ca25fd71":"markdown"},"source":{"e78595e4":"import numpy as np # linear algebra\nimport pandas as pd # data processing\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_rows', 150)\nimport os\nimport gc\ngc.enable()\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\nfrom scipy import stats\n%matplotlib inline\nfrom datetime import timedelta\nimport datetime as dt\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nimport warnings\nwarnings.filterwarnings('ignore')\nimport urllib        #for url stuff\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\nfrom IPython.display import display\nfrom tqdm import tqdm\nfrom collections import Counter\nimport ast\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom textblob import TextBlob\nimport scipy.stats as stats\n\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\nimport seaborn as sns #for making plots\nimport matplotlib.pyplot as plt # for plotting\nimport os  # for os commands\n\nimport gensim\nfrom gensim import corpora, models, similarities\nimport logging\nimport tempfile\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom collections import OrderedDict\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\n\n\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\noutput_notebook()","29741a59":"ROOT = '\/kaggle\/input\/nlp-getting-started\/'\n\n## load the data \ndf_train = pd.read_csv(ROOT+'train.csv')\ndf_test = pd.read_csv(ROOT+'test.csv')\ndf_sub = pd.read_csv(ROOT+'sample_submission.csv')","e489d864":"print(df_train.info())\nprint(df_test.info())\nprint(df_sub.info())","d0c71f12":"df_train.describe()","5b24f4b2":"df_train.head()","76e2bab8":"target = df_train['target']\nsns.set_style('whitegrid')\nsns.countplot(target)","ac102aed":"df_train[\"text\"].head()","35798c5a":"#To check the text content\ndf_train[\"text\"].tolist()[:5]","0c0c4642":"t = df_train[\"text\"].to_list()\nfor i in range(5):\n    print('Tweet Number '+str(i+1)+': '+t[i])","65e6a54e":"l = df_train[\"location\"].to_list()\nprint('There is '+ str(len(set(l)))+\" different loction\")","541cc103":"#Posting Location top 20\ndf_train['location'].value_counts().head(n=20)","6b418634":"# Graph of the number of tweets in each location\nlocation_count  = df_train['location'].value_counts()\nlocation_count = location_count[:10,]\nplt.figure(figsize=(10,5))\nsns.barplot(location_count.index, location_count.values, alpha=0.8)\nplt.title('Top 10 locations')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('location', fontsize=12)\nplt.show()","afc401c8":"location_count  = df_test['location'].value_counts()\nlocation_count = location_count[:10,]\nplt.figure(figsize=(10,5))\nsns.barplot(location_count.index, location_count.values, alpha=0.8)\nplt.title('Top 10 locations')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('location', fontsize=12)\nplt.show()","1b390434":"df = df_train[df_train['location'].notnull()]","cc591323":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndf['location'] = le.fit_transform(df.location.values)","8b7f3be1":"X = df['location']\ny = df['target']\n\n\n# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values.reshape(-1, 1))\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))","00ade84b":"f, (ax1) = plt.subplots(1, 1, figsize=(12,8))\n\nf.suptitle('Cluster (Dimensionality Reduction)', fontsize=14)\n\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='Not Fake')\nred_patch = mpatches.Patch(color='#AF0000', label='Fake')\n\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='Not Fake', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fake', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\nax1.grid(True)\n\nax1.legend(handles=[blue_patch, red_patch])\n\n\nplt.show()","64bb60e0":"df_train['keyword'].value_counts().head(n=20)","78ca04f5":"keyword_count  = df_train['keyword'].value_counts()\nkeyword_count = keyword_count[:10,]\nplt.figure(figsize=(10,5))\nsns.barplot(keyword_count.index, keyword_count.values, alpha=0.8)\nplt.title('Top 10 keywords')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('keyword', fontsize=12)\nplt.show()","e842f887":"df_test['keyword'].value_counts().head(n=20)","8e8bcb04":"keyword_count  = df_test['keyword'].value_counts()\nkeyword_count = keyword_count[:10,]\nplt.figure(figsize=(10,5))\nsns.barplot(keyword_count.index, keyword_count.values, alpha=0.8)\nplt.title('Top 10 keywords')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('keyword', fontsize=12)\nplt.show()","dcb35839":"keyword_train  = list(set(df_train['keyword']))\nkeyword_test  = list(set(df_test['keyword']))\n\nprint(len(keyword_train))\nprint(len(keyword_test))","19ed0284":"def intersection(lst1, lst2): \n    lst3 = [value for value in lst1 if value in lst2] \n    return lst3 \n\nlen(intersection(keyword_train, keyword_test))","2ff7e10c":"df = df_train[df_train['keyword'].notnull()]\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndf['keyword'] = le.fit_transform(df.keyword.values)","252334de":"X = df['keyword']\ny = df['target']\n\n\n# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values.reshape(-1, 1))\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))","8fd94004":"f, (ax1) = plt.subplots(1, 1, figsize=(12,8))\n# labels = ['Not Fake', 'Fake']\nf.suptitle('Clusters (Dimensionality Reduction)', fontsize=14)\n\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='Not Fake')\nred_patch = mpatches.Patch(color='#AF0000', label='Fake')\n\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='Not Fake', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fake', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\nax1.grid(True)\n\nax1.legend(handles=[blue_patch, red_patch])\n\n\nplt.show()","91fa1949":"len(set(df_train['text']))","ec566d90":"len(set(df_test['text']))","54d51dc1":"html_tags = ['<P>', '<\/P>', '<Table>', '<\/Table>', '<Tr>', '<\/Tr>', '<Ul>', '<Ol>', '<Dl>', '<\/Ul>', '<\/Ol>', \\\n             '<\/Dl>', '<Li>', '<Dd>', '<Dt>', '<\/Li>', '<\/Dd>', '<\/Dt>']\nr_buf = ['It', 'is', 'are', 'do', 'does', 'did', 'was', 'were', 'will', 'can', 'the', 'a', 'of', 'in', 'and', 'on', \\\n         'what', 'where', 'when', 'which'] + html_tags\n\ndef clean(x):\n    x = x.lower()\n    for r in r_buf:\n        x = x.replace(r, '')\n    x = re.sub(' +', ' ', x)\n    return x\n\nbin_question_tokens = ['it', 'is', 'are', 'do', 'does', 'did', 'was', 'were', 'will', 'can']\nstop_words = text.ENGLISH_STOP_WORDS.union([\"book\"])\n\ndef predict(json_data, annotated=False):\n    # Parse JSON data\n    candidates = json_data['long_answer_candidates']\n    candidates = [c for c in candidates if c['top_level'] == True]\n    doc_tokenized = json_data['document_text'].split(' ')\n    question = json_data['question_text']\n    question_s = question.split(' ') \n    if annotated:\n        ann = json_data['annotations'][0]\n\n    tfidf = TfidfVectorizer(ngram_range=(1,1), stop_words=stop_words)\n    tfidf.fit([json_data['document_text']])\n    q_tfidf = tfidf.transform([question]).todense()\n\n    # Find the nearest answer from candidates\n    distances = []\n    scores = []\n    i_ann = -1\n    for i, c in enumerate(candidates):\n        s, e = c['start_token'], c['end_token']\n        t = ' '.join(doc_tokenized[s:e])\n        distances.append(levenshtein_distance(clean(question), clean(t)))\n        \n        t_tfidf = tfidf.transform([t]).todense()\n        score = 1 - spatial.distance.cosine(q_tfidf, t_tfidf)\n        \n        scores.append(score)\n\n    # Format results\n    \n    ans = candidates[np.argmax(scores)]\n    if np.max(scores) < 0.2:\n        ans_long = '-1:-1'\n    else:\n        ans_long = str(ans['start_token']) + ':' + str(ans['end_token'])\n    if question_s[0] in bin_question_tokens:\n        ans_short = 'YES'\n    else:\n        ans_short = ''\n        \n    # Preparing data for debug\n    if annotated:\n        ann_long_text = ' '.join(doc_tokenized[ann['long_answer']['start_token']:ann['long_answer']['end_token']])\n        if ann['yes_no_answer'] == 'NONE':\n            if len(json_data['annotations'][0]['short_answers']) > 0:\n                ann_short_text = ' '.join(doc_tokenized[ann['short_answers'][0]['start_token']:ann['short_answers'][0]['end_token']])\n            else:\n                ann_short_text = ''\n        else:\n            ann_short_text = ann['yes_no_answer']\n    else:\n        ann_long_text = ''\n        ann_short_text = ''\n        \n    ans_long_text = ' '.join(doc_tokenized[ans['start_token']:ans['end_token']])\n    if len(ans_short) > 0 or ans_short == 'YES':\n        ans_short_text = ans_short\n    else:\n        ans_short_text = '' # Fix when short answers will work\n                    \n    return ans_long, ans_short, question, ann_long_text, ann_short_text, ans_long_text, ans_short","37e2b7c5":"reindexed_data = df_train['text'] ","7ae6f985":"def get_top_n_words(n_top_words, count_vectorizer, text_data):\n    '''\n    returns a tuple of the top n words in a sample and their \n    accompanying counts, given a CountVectorizer object and text sample\n    '''\n    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n    vectorized_total = np.sum(vectorized_headlines, axis=0)\n    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n    \n    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n    for i in range(n_top_words):\n        word_vectors[i,word_indices[0,i]] = 1\n\n    words = [word[0].encode('ascii',errors=\"ignore\").decode('utf-8',errors=\"ignore\") for \n             word in count_vectorizer.inverse_transform(word_vectors)]\n\n    return (words, word_values[0,:n_top_words].tolist()[0])","5d1c64b2":"count_vectorizer = CountVectorizer(stop_words='english')\nwords, word_values = get_top_n_words(n_top_words=25,\n                                     count_vectorizer=count_vectorizer, \n                                     text_data=reindexed_data)\n\nfig, ax = plt.subplots(figsize=(10,4))\nax.bar(range(len(words)), word_values);\nax.set_xticks(range(len(words)));\nax.set_xticklabels(words, rotation='vertical');\nax.set_title('Top words in headlines dataset (excluding stop words)');\nax.set_xlabel('Word');\nax.set_ylabel('Number of occurences');\nplt.show()","42fc26e0":"tagged_headlines = [TextBlob(reindexed_data[i]).pos_tags for i in range(reindexed_data.shape[0])]","09f7f567":"tagged_headlines_df = pd.DataFrame({'tags':tagged_headlines})\n\nword_counts = [] \npos_counts = {}\n\nfor headline in tagged_headlines_df[u'tags']:\n    word_counts.append(len(headline))\n    for tag in headline:\n        if tag[1] in pos_counts:\n            pos_counts[tag[1]] += 1\n        else:\n            pos_counts[tag[1]] = 1\n            \nprint('Total number of words: ', np.sum(word_counts))\nprint('Mean number of words per tweet: ', np.mean(word_counts))","6827713f":"y = stats.norm.pdf(np.linspace(0,14,50), np.mean(word_counts), np.std(word_counts))\n\nfig, ax = plt.subplots(figsize=(8,4))\nax.hist(word_counts, bins=range(1,14), density=True);\nax.plot(np.linspace(0,14,50), y, 'r--', linewidth=1);\nax.set_title('Headline word lengths');\nax.set_xticks(range(1,14));\nax.set_xlabel('Number of words');\nplt.show()","bb361054":"pos_sorted_types = sorted(pos_counts, key=pos_counts.__getitem__, reverse=True)\npos_sorted_counts = sorted(pos_counts.values(), reverse=True)\n\nfig, ax = plt.subplots(figsize=(14,4))\nax.bar(range(len(pos_counts)), pos_sorted_counts);\nax.set_xticks(range(len(pos_counts)));\nax.set_xticklabels(pos_sorted_types);\nax.set_title('Part-of-Speech Tagging for questions');\nax.set_xlabel('Type of Word');","491b6e40":"small_count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\nsmall_text_sample = reindexed_data.sample(n=500, random_state=0).values\n\nprint('Tweets before vectorization: {}'.format(small_text_sample[123]))\n\nsmall_document_term_matrix = small_count_vectorizer.fit_transform(small_text_sample)\n\nprint('Tweets after vectorization: \\n{}'.format(small_document_term_matrix[123]))","33ff2b09":"n_topics = 5","c8b322d1":"lsa_model = TruncatedSVD(n_components=n_topics)\nlsa_topic_matrix = lsa_model.fit_transform(small_document_term_matrix)","dff69295":"def get_keys(topic_matrix):\n    '''\n    returns an integer list of predicted topic \n    categories for a given topic matrix\n    '''\n    keys = topic_matrix.argmax(axis=1).tolist()\n    return keys\n\ndef keys_to_counts(keys):\n    '''\n    returns a tuple of topic categories and their \n    accompanying magnitudes for a given list of keys\n    '''\n    count_pairs = Counter(keys).items()\n    categories = [pair[0] for pair in count_pairs]\n    counts = [pair[1] for pair in count_pairs]\n    return (categories, counts)","ea328850":"lsa_keys = get_keys(lsa_topic_matrix)\nlsa_categories, lsa_counts = keys_to_counts(lsa_keys)","5ca406ef":"def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n    '''\n    returns a list of n_topic strings, where each string contains the n most common \n    words in a predicted category, in order\n    '''\n    top_word_indices = []\n    for topic in range(n_topics):\n        temp_vector_sum = 0\n        for i in range(len(keys)):\n            if keys[i] == topic:\n                temp_vector_sum += document_term_matrix[i]\n        temp_vector_sum = temp_vector_sum.toarray()\n        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n        top_word_indices.append(top_n_word_indices)   \n    top_words = []\n    for topic in top_word_indices:\n        topic_words = []\n        for index in topic:\n            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n            temp_word_vector[:,index] = 1\n            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n            topic_words.append(the_word.encode('ascii',errors=\"ignore\").decode('utf-8',errors=\"ignore\"))\n        top_words.append(\" \".join(topic_words))         \n    return top_words","8c3d4e8f":"top_n_words_lsa = get_top_n_words(10, lsa_keys, small_document_term_matrix, small_count_vectorizer)\n\nfor i in range(len(top_n_words_lsa)):\n    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])","dd6e23c3":"top_3_words = get_top_n_words(3, lsa_keys, small_document_term_matrix, small_count_vectorizer)\nlabels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lsa_categories]\n\nfig, ax = plt.subplots(figsize=(8,4))\nax.bar(lsa_categories, lsa_counts);\nax.set_xticks(lsa_categories);\nax.set_xticklabels(labels);\nax.set_ylabel('Number of tweets');\nax.set_title('LSA topic counts');\nplt.show()","3c2ec597":"tsne_lsa_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsne_lsa_vectors = tsne_lsa_model.fit_transform(lsa_topic_matrix)","c05d6511":"def get_mean_topic_vectors(keys, two_dim_vectors):\n    '''\n    returns a list of centroid vectors from each predicted topic category\n    '''\n    mean_topic_vectors = []\n    for t in range(n_topics):\n        articles_in_that_topic = []\n        for i in range(len(keys)):\n            if keys[i] == t:\n                articles_in_that_topic.append(two_dim_vectors[i])    \n        \n        articles_in_that_topic = np.vstack(articles_in_that_topic)\n        mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)\n        mean_topic_vectors.append(mean_article_in_that_topic)\n    return mean_topic_vectors","17f44996":"colormap = np.array([\n    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\" ])\ncolormap = colormap[:n_topics]","69c6f701":"top_3_words_lsa = get_top_n_words(3, lsa_keys, small_document_term_matrix, small_count_vectorizer)\nlsa_mean_topic_vectors = get_mean_topic_vectors(lsa_keys, tsne_lsa_vectors)\n\nplot = figure(title=\"t-SNE Clustering of {} LSA Topics\".format(n_topics), plot_width=700, plot_height=700)\nplot.scatter(x=tsne_lsa_vectors[:,0], y=tsne_lsa_vectors[:,1], color=colormap[lsa_keys])\n\nfor t in range(n_topics):\n    label = Label(x=lsa_mean_topic_vectors[t][0], y=lsa_mean_topic_vectors[t][1], \n                  text=top_3_words_lsa[t], text_color=colormap[t])\n    plot.add_layout(label)\n    \nshow(plot)","5d006e05":"lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', \n                                          random_state=0, verbose=0)\nlda_topic_matrix = lda_model.fit_transform(small_document_term_matrix)","5e5d1f18":"lda_keys = get_keys(lda_topic_matrix)\nlda_categories, lda_counts = keys_to_counts(lda_keys)","252ade93":"top_n_words_lda = get_top_n_words(10, lda_keys, small_document_term_matrix, small_count_vectorizer)\n\nfor i in range(len(top_n_words_lda)):\n    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])","59c472c8":"top_3_words = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)\nlabels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lda_categories]\n\nfig, ax = plt.subplots(figsize=(10,4))\nax.bar(lda_categories, lda_counts);\nax.set_xticks(lda_categories);\nax.set_xticklabels(labels);\nax.set_title('LDA topic counts');\nax.set_ylabel('Number of tweets');","1bbcb4c2":"tsne_lda_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsne_lda_vectors = tsne_lda_model.fit_transform(lda_topic_matrix)","28d2b779":"top_3_words_lda = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)\nlda_mean_topic_vectors = get_mean_topic_vectors(lda_keys, tsne_lda_vectors)\n\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), plot_width=600, plot_height=600)\nplot.scatter(x=tsne_lda_vectors[:,0], y=tsne_lda_vectors[:,1], color=colormap[lda_keys])\n\nfor t in range(n_topics):\n    label = Label(x=lda_mean_topic_vectors[t][0], y=lda_mean_topic_vectors[t][1], \n                  text=top_3_words_lda[t], text_color=colormap[t])\n    plot.add_layout(label)\n\nshow(plot)","981fe1d9":"# Preparing a corpus for analysis and checking the first 5 entries\ncorpus=[]\n\ncorpus = df_train['text'].to_list()\n\ncorpus[:5]","01a90d78":"TEMP_FOLDER = tempfile.gettempdir()\nprint('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","335f6dfc":"# removing common words and tokenizing\n# google-quest-challenge\nstoplist = stopwords.words('english') + list(punctuation) + list(\"([)]?\") + [\")?\"]\n\ntexts = [[word for word in str(document).lower().split() if word not in stoplist] for document in corpus]\n\ndictionary = corpora.Dictionary(texts)\ndictionary.save(os.path.join(TEMP_FOLDER, 'tweets.dict'))  # store the dictionary","73c901a2":"corpus = [dictionary.doc2bow(text) for text in texts]\ncorpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'tweets.mm'), corpus) ","9bb202fd":"tfidf = models.TfidfModel(corpus) # initialize a model","7b1f9ebb":"corpus_tfidf = tfidf[corpus]  # use the model to transform vectors","01621911":"total_topics = 10\n\nlda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\ncorpus_lda = lda[corpus_tfidf]","df152ba1":"# Show first n important word in the topics\n\nlda.show_topics(total_topics,5)","01d4391b":"data_lda = {i: OrderedDict(lda.show_topic(i,25)) for i in range(total_topics)}","dba8a9b7":"df_lda = pd.DataFrame(data_lda)\ndf_lda = df_lda.fillna(0).T\nprint(df_lda.shape)","c9d743c1":"df_lda","84067800":"import pyLDAvis.gensim\npyLDAvis.enable_notebook()\npanel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='tsne')\npanel","6df941f5":"* Next, we create a histogram of the headline word lengths and use speech fragment tagging to understand the types of words used in the entire community. This requires first converting all header strings to TextBlobs and calling the pos_tags method for each, giving a list of tagged words for each header.","dbea9870":"* This is a much better result.\n*  For this reason, LDA appears the more appropriate algorithm when we scale up the clustering process.","89894473":"* Thus we have training data, small_document_term_matrix and we can implement a clustering algorithm.\n* Our choice will be either Latent Semantic Analysis (LSA) or Latent Dirichlet Allocation (LDA). ","f5b664b4":"<a id = \"13\"><\/a>\n# Preprocessing\n\n* Now I'll apply a clustering algorithm to headlines to examine the topic and how it has evolved over time.\n\n* The only preprocessing step required in our case is feature construction. In practice, this simply means converting each string to a numerical vector.","1e206283":"# Introduction\n\nIt has been very difficult to know if the news published on social media in the last few years is true or false. In this analysis, we will determine whether any news on social media is true or false with a data set prepared from Twitter.\n\n<font color = \"blue\">\nContent:\n\n1. [Load and Check Data](#1)\n2. [Exploring Data](#2)\n    * [Tweet Location](#3)\n        - [In Test Data](#4)\n        - [Label Encoding location column](#5)\n    * [Keywords](#6)\n        - [In Test Data](#7)\n    * [Intersection](#8)\n    * [Text Analysis](#9)\n    \n3. [Topic Modeling](#10)\n    * [Latent Dirichlet Allocation (LDA)](#11)\n    * [Part-of-Speech Tagging for questions](#12)\n\n4. [Preprocessing](#13)\n    * [Latent Semantic Analysis (LSA)](#14)\n    * [Latent Dirichlet Allocation (LDA)](#15)\n    \n5. [Visualization of LDA](#16)\n    * [Creating a transformation](#17)\n    ","ce44d405":"<a id = \"8\"><\/a>\n## Intersection\n\n* **The intersection between the two datasets give us the same number of unique keywords, so the keywords used in both datasets with different occurrence**","69ef0799":"<a id = \"3\"><\/a>\n## Tweet Location","33049519":"<a id = \"11\"><\/a>\n## Latent Dirichlet Allocation (LDA)","e5947e2d":"* To accurately compare LSA with LDA, we use a size reduction technique called t-SNE that will serve to better illuminate the success of the clustering process.","5aace9a6":"<a id = \"16\"><\/a>\n# Visualization of LDA","14f3aa28":"<a id = \"12\"><\/a>\n## Part-of-Speech Tagging for questions","766584ea":"<a id = \"4\"><\/a>\n### In test data","c228b8c9":"<a id = \"2\"><\/a>\n# Exploring Data\n\n* Checking target distrubution and class imbalance","cfb9b548":"<a id = \"1\"><\/a>\n# Load and Check Data","d36f3298":"* We've turned our first small title example into a list of predicted subject categories, where each category is characterized by the most common words.","69bdb6de":"<a id = \"6\"><\/a>\n## Keywords","18a27079":"<a id = \"15\"><\/a>\n## Latent Dirichlet Allocation (LDA)\n\n* We'll repeat same process using LDA.","a5dfb0ce":"<a id = \"9\"><\/a>\n## Text analysis\n\n* I want to check if there is some duplicated tweets, it could be a retweet. If we know that a tweet is fake or not so the other duplicated tweets will get the same class.","e3249365":"* We project this matrix in two dimensions so that we can compare LDA and LSA accurately.","4ea7b677":"* This is a bit a of a failed result.\n* Let's move forward and try another clustering method.","493e4d3d":"<a id = \"5\"><\/a>\n### Label Encoding location column","23a1a4cf":"* **There is a little difference between keywords in train and test set, I will check the intersection between the keywords in train and test**","91c70145":"<a id = \"10\"><\/a>\n# Topic Modeling\n\n* Topic modeling is a type of statistical modeling for discovering the abstract \u201ctopics\u201d that occur in a collection of documents.\n\n* It's very important to do topic modeling and understand the content of each question and answer. The reason is that there is some question out of meaning and are not related to the category so they will get little answer and sometimes some blaming.","3040be12":"* **Blue dots represent real events**","7f33ca9f":"<a id = \"7\"><\/a>\n### In test data","fb689dc9":"* Now we have reduced these n_topics-dimensional vectors to two-dimensional representations, we can plot the clusters using Bokeh. ","2704e102":"<a id = \"17\"><\/a>\n## Creating a transformation\n\n* The transformations are standard Python objects, typically initialized by means of a training corpus\n\n* Different transformations may require different initialization parameters; in case of TfIdf, the \u201ctraining\u201d consists simply of going through the supplied corpus once and computing document frequencies of all its features. Training other models, such as LST OR LDA.","18e3dcef":"* We created a corpus of documents represented as a stream of vectors.\n* To continue, lets use that corpus, with the help of Gensim.\n\n*Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning. Gensim is implemented in Python and Cython. (WIKIPEDIA)* ","c8c964a6":"<a id = \"14\"><\/a>\n## Latent Semantic Analysis (LSA)\n\n* This is effectively just a truncated singular value decomposition of a document-term matrix, with only the r= n_topics largest singular values preserved.","ca25fd71":"* LDA\u2019s topics can be interpreted as probability distributions over words.\n* So i'll try 10 topics."}}