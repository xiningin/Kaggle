{"cell_type":{"d3ce9588":"code","adeea4a5":"code","165af716":"code","0ad12b79":"code","ea107ddd":"code","8779cd45":"code","7f2ca3b8":"code","a50cb950":"code","221f7f6f":"code","c2b9ae9c":"code","4e9566a2":"code","462bd61d":"code","f7465f42":"code","c7485b79":"code","b34b4422":"code","e1935284":"code","fa0d3f01":"code","ebae4b18":"code","32a3a8e3":"markdown","38ce6063":"markdown","6efe4112":"markdown","13539df1":"markdown","c78625e6":"markdown","e31747bf":"markdown","1d8b626b":"markdown","2069f386":"markdown","63ca194f":"markdown","bcea87f9":"markdown","c49ed6e8":"markdown","d7bb9a0f":"markdown","fecde890":"markdown","ae710c2a":"markdown","0c47a880":"markdown","bf1a842f":"markdown","a40bc811":"markdown","c67d705f":"markdown","84d9b5ad":"markdown","89f41b87":"markdown","57b6d59f":"markdown"},"source":{"d3ce9588":"import pandas as pd\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_features = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","adeea4a5":"X_train = train.iloc[:, 2:]\nX_train = X_train.drop(columns=[\"Name\",\"Parch\",\"Ticket\",\"Cabin\",\"Embarked\"])\ny_train = train.iloc[:, 1]\n\nX_test = test_features.iloc[:, 1:]\nX_test = X_test.drop(columns=[\"Name\",\"Parch\",\"Ticket\",\"Cabin\",\"Embarked\"])","165af716":"X_train.isnull().sum() # 177 Missing values for Age\nX_test.isnull().sum()  # 86  Missing values for Age, 1 for Fare","0ad12b79":"from sklearn.impute import SimpleImputer\nimport numpy as np\n# Replacing missing values for age with the mean value for age\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer = imputer.fit(np.array(X_train.iloc[:, 2]).reshape(-1,1))\nX_train.iloc[:, 2] = np.round(imputer.transform(np.array(X_train.iloc[:, 2]).reshape(-1,1)),0)\nX_test.iloc[:,2] = np.round(imputer.transform(np.array(X_test.iloc[:, 2]).reshape(-1,1)),0)\n\n# Assigning one missing Fare value in the test set by taking the mean of all other values\nX_test['Fare'].iloc[152] = np.mean(X_test['Fare'].dropna())","ea107ddd":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX_train['Sex'] = le.fit_transform(X_train['Sex'])\nX_test['Sex'] = le.fit_transform(X_test['Sex'])","8779cd45":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\nX_train = ct.fit_transform(X_train)\nX_test = ct.fit_transform(X_test)","7f2ca3b8":"X_train = pd.DataFrame(X_train,columns=[\"P1\",\"P2\",\"P3\",\"Sex\",\"Age\",\"SibSp\",\"Fare\"])\nX_test = pd.DataFrame(X_test,columns=[\"P1\",\"P2\",\"P3\",\"Sex\",\"Age\",\"SibSp\",\"Fare\"])","a50cb950":"X_train = X_train.drop(columns=[\"P1\"])\nX_test = X_test.drop(columns=[\"P1\"])","221f7f6f":"for i in range(len(X_train)):\n    if X_train[\"SibSp\"][i] == 0:\n        X_train[\"SibSp\"][i] = 0\n    else:\n        X_train[\"SibSp\"][i] = 1\n\nfor i in range(len(X_test)):\n    if X_test[\"SibSp\"][i] == 0:\n        X_test[\"SibSp\"][i] = 0\n    else:\n        X_test[\"SibSp\"][i] = 1","c2b9ae9c":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train[[\"Age\",\"Fare\"]] = sc.fit_transform(X_train[[\"Age\",\"Fare\"]])\nX_test[[\"Age\",\"Fare\"]] = sc.transform(X_test[[\"Age\",\"Fare\"]])","4e9566a2":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom xgboost import XGBClassifier\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\"),\n    SVC(kernel=\"rbf\"),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n\tAdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression(),\n    MLPClassifier(alpha=1, max_iter=1000),\n    GaussianProcessClassifier(),\n    XGBClassifier()]\n\nnames = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\", \"Random Forest\",\n         \"AdaBoost\", \"Gradient Boosting\", \"Naive Bayes\", \"LDA\", \"QDA\", \"Logistic Regression\",\n         \"Neural Net\",\"Gaussian Process\", \"XGBoost\"\n        ]","462bd61d":"from sklearn.model_selection import cross_val_score\n\n# Perform cross validation on all classifiers\n# Store results in a dictionary\n# Zip assigns a key-value or pairing from two different lists\nresults = {}\nfor name, clf in zip(names, classifiers):\n    scores = cross_val_score(clf, X_train, y_train, cv=10)\n    results[name] = scores","f7465f42":"df = pd.DataFrame(results)\nmeans = df.mean()*100\nstd = df.std()*100","c7485b79":"acc = pd.DataFrame(means,columns=['Mean Accuracy'])\nacc[\"std\"] = std\nacc = acc.sort_values([\"Mean Accuracy\"])","b34b4422":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.barplot(acc[\"Mean Accuracy\"],acc.index,xerr=acc[\"std\"], palette=\"Blues\")\nplt.xlim(0, 101)\nplt.title(\"Classifiers Accuracy\")\nplt.xlabel(\"Accuracy %\")\nfor i in np.arange(0,100,10):\n    plt.axvline(i,lw=0.5,c=\"black\")\nprint(\"The classifier with the highest accuracy is {}\".format(acc.index[-1]) + \" with an accuracy of {:.2f}%\".format(acc.iloc[-1,0]))","e1935284":"from sklearn.model_selection import GridSearchCV\nbest_classifier = GradientBoostingClassifier()\nparameters = {\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.1, 0.2],\n    \"criterion\": [\"friedman_mse\",  \"mae\"],\n    \"n_estimators\":[10],\n    \"max_depth\":[3,5,8],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"subsample\": [0.5, 0.55, 0.6]}\n#    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n#    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n\ngrid_search = GridSearchCV(estimator = best_classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nprint(\"Best Accuracy: {:.2f} %\".format(grid_search.best_score_*100))\nprint(\"Best Parameters:\", grid_search.best_params_)","fa0d3f01":"from sklearn.metrics import accuracy_score\nbest_classifier = GradientBoostingClassifier(**grid_search.best_params_)\nbest_classifier.fit(X_train, y_train)\ny_pred = best_classifier.predict(X_test)","ebae4b18":"testset = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nPassengerId = testset.iloc[:,0]\ndf = pd.DataFrame(PassengerId, columns=[\"PassengerId\"])\ndf[\"Survived\"] = y_pred\ndf.to_csv(\"output.csv\",index=False)","32a3a8e3":"# Storing the data in a dataframe that is sorted by accuracy","38ce6063":"### Replacing missing values","6efe4112":"# Dataset split and selecting some of the relevant columns (Pclass, age, sex, fare)","13539df1":"# Importing datasets","c78625e6":"### The focus of this notebook is mainly on trying many classifiers. I wanted to create a code template involving many classifiers, a robust evaluation of them, quick visualization of respective performance, hyperparameter optimization and evaluation using the best classifier. ","e31747bf":"# Output file","1d8b626b":"# Feature scaling","2069f386":"# Predicting using optimal hyperparameters for best classifiers","63ca194f":"# Hyperparameter tuning for the best classifier","bcea87f9":"# Plotting the accuracy of each classifier","c49ed6e8":"# Taking care of missing data","d7bb9a0f":"## Testing for missing values","fecde890":"# Classifiers","ae710c2a":"# **Encoding the gender category**\nCategorical variables must be converted to numerical values to include them in a machine learning model","0c47a880":"Setting SibSp to 0 if no siblings or spouse, 1 if there are","bf1a842f":"## Storing the data in a dataframe and calculating the mean accuracy and standard deviation for each classifier","a40bc811":"Convert back to DataFrame","c67d705f":"# Encoding the SibSp variable","84d9b5ad":"# Cross validation\n\nCross validation is used to measure the performance of the models, while controlling for the dependencies of the model's performance on a specific configuration of the training set.","89f41b87":"Drop a dummy variable","57b6d59f":"# Encoding the Pclass category"}}