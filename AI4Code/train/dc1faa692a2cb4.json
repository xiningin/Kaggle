{"cell_type":{"e31fb065":"code","e5c8953f":"code","8f0675f3":"code","6126bf81":"code","33a40647":"code","6ac1da4f":"code","346c54ae":"code","6a66f38b":"code","2e7e45f1":"code","6f152cc0":"code","96398d5a":"code","c30409a6":"code","69a299f9":"code","69a0662d":"code","e3813c1a":"code","61001669":"code","2b82798a":"code","955eb50e":"code","b076ceec":"code","80c937d3":"code","8f3becbd":"code","66b8bc66":"code","cfdf0ab0":"code","4580abed":"code","235c2be8":"code","c3d024b6":"code","7458ae76":"code","2a628677":"code","e4d0f167":"code","56fcb549":"code","3932e711":"code","bdfd90e9":"code","d16725b9":"code","58ee3eac":"code","d7f0b68c":"code","dfef798a":"code","c9a2d451":"code","b26e2b43":"code","5857631e":"code","d40ce1ed":"code","606746d6":"code","a1940ccc":"code","1cb64c21":"code","4d5dc02a":"code","07b5b69e":"code","8ba2076c":"code","2ba59323":"code","8026d019":"code","0b23cc22":"code","22611623":"code","b4a248b3":"code","1820114b":"code","916cb50c":"code","d06f1a4f":"markdown","e3f45814":"markdown","86b655d4":"markdown","a2cea13b":"markdown","ebdf2146":"markdown","75b77614":"markdown"},"source":{"e31fb065":"print(\"Read in libraries\")\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom sklearn import preprocessing\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom random import random\n\nimport datetime\nimport lightgbm as lgb","e5c8953f":"print(\"Print Directories\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8f0675f3":"train = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/submission.csv\")\nprint(\"read in files\")\ntest.head()","6126bf81":"train = train.append(test[test['Date']>'2020-03-31'])","33a40647":"#fix dates\nimport datetime \ntrain['Date'] = pd.to_datetime(train['Date'], format='%Y-%m-%d')\ntrain['day_dist'] = train['Date']-train['Date'].min()\ntrain['day_dist'] = train['day_dist'].dt.days\nprint(train['Date'].max())\n#print(val['Date'].max())\nprint(test['Date'].min())\nprint(test['Date'].max())\n#print(test['Date'].max()-test['Date'].min())\ncat_cols = train.dtypes[train.dtypes=='object'].keys()\ncat_cols\n\n#fix na\nfor cat_col in cat_cols:\n    train[cat_col].fillna('no_value', inplace = True)\n\n#make a place variable \ntrain['place'] = train['Province_State']+'_'+train['Country_Region']\n#vcheck = train[(train['Date']>='2020-03-12')]","6ac1da4f":"#get the cat columns\ncat_cols = train.dtypes[train.dtypes=='object'].keys()\ncat_cols\n\n#label the columns\nfor cat_col in ['place']:\n    #train[cat_col].fillna('no_value', inplace = True) #train[cat_col].value_counts().idxmax()\n    le = preprocessing.LabelEncoder()\n    le.fit(train[cat_col])\n    train[cat_col]=le.transform(train[cat_col])\n    \n#check train keys \ntrain.keys()\n\n#set columns were going to drop during our training stage\ndrop_cols = ['Id', 'ConfirmedCases','Date', 'ForecastId','Fatalities','day_dist', 'Province_State', 'Country_Region']","346c54ae":"#val = train[(train['Id']).isnull()==True]\n#train = train[(train['Id']).isnull()==False]\nval = train[(train['Date']>='2020-03-19')&(train['Id'].isnull()==False)]\n#test = train[(train['Date']>='2020-03-12')&(train['Id'].isnull()==True)]\n#train = train[(train['Date']<'2020-03-22')&(train['Id'].isnull()==False)]\n#val = train\nval","6a66f38b":"y_ft = train[\"Fatalities\"]\ny_val_ft = val[\"Fatalities\"]\n\ny_cc = train[\"ConfirmedCases\"]\ny_val_cc = val[\"ConfirmedCases\"]\n\n#train.drop(drop_cols, axis=1, inplace=True)\n#test.drop(drop_cols, axis=1, inplace=True)\n#val.drop(drop_cols, axis=1, inplace=True)\n","2e7e45f1":"y_val_ft","6f152cc0":"#define scoring functions\ndef rmsle (y_true, y_pred):\n    return np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2)))\n\ndef mape (y_true, y_pred):\n    return np.mean(np.abs(y_pred -y_true)*100\/(y_true+1))\n","96398d5a":"#set params for lgbt\nparams = {\n    \"objective\": \"regression\",\n    \"boosting\": 'gbdt', #\"gbdt\",\n    \"num_leaves\": 1280,\n    \"learning_rate\": 0.05,\n    \"feature_fraction\": 0.9, # 0.9,\n    \"reg_lambda\": 2,\n    \"metric\": \"rmse\",\n    'min_data_in_leaf':20\n}\n","c30409a6":"#get dates for iterating over \ndates = test['Date'].unique()\ndates","69a299f9":"#subset them for the relevant dates\ndates = dates[dates>'2020-03-31']\ndates","69a0662d":"len(dates)","e3813c1a":"train.isnull().sum()","61001669":"i = 0\nfold_n = 0\nfor date in dates:\n\n    fold_n = fold_n +1 \n    i = i+1\n    if i==1:\n        nrounds = 200\n    else:\n        nrounds = 100\n    print(i)\n    print(nrounds)\n    train['shift_1_cc'] = train.groupby(['place'])['ConfirmedCases'].shift(i)\n    train['shift_2_cc'] = train.groupby(['place'])['ConfirmedCases'].shift(i+1)\n    train['shift_3_cc'] = train.groupby(['place'])['ConfirmedCases'].shift(i+2)\n    train['shift_4_cc'] = train.groupby(['place'])['ConfirmedCases'].shift(i+3)\n    train['shift_5_cc'] = train.groupby(['place'])['ConfirmedCases'].shift(i+4)\n        \n    val2 = train[train['Date']==date]\n    train2 = train[(train['Date']<date)]\n    y_cc = train2[\"ConfirmedCases\"]\n    #y_val_cc = val2[\"ConfirmedCases\"]\n    \n    train2.drop(drop_cols, axis=1, inplace=True)\n    val2.drop(drop_cols, axis=1, inplace=True)\n    \n    #np.log1p(y)\n    #feature_importances = pd.DataFrame()\n    #feature_importances['feature'] = train.keys()\n    \n    #score = 0       \n    dtrain = lgb.Dataset(train2, label=y_cc)\n    dvalid = lgb.Dataset(val2, label=y_val_cc)\n\n    model = lgb.train(params, dtrain, nrounds, \n                            #valid_sets = [dtrain, dvalid],\n                            categorical_feature = ['place'], #'Province\/State', 'Country\/Region'\n                            verbose_eval=False)#, early_stopping_rounds=50)\n\n    y_pred = model.predict(val2,num_iteration=nrounds)  #model.best_iteration\n    #y_pred = np.expm1( y_pred)\n    #vcheck.loc[vcheck['Date']==date,'cc_predict'] = y_pred\n    test.loc[test['Date']==date,'ConfirmedCases'] = y_pred\n    train.loc[train['Date']==date,'ConfirmedCases'] = y_pred\n    #y_oof[valid_index] = y_pred\n\n    #rmsle_score = rmsle(y_val_cc, y_pred)\n    #mape_score = mape(y_val_cc, y_pred)\n    #score += rmsle_score\n    #print (f'fold: {date}, rmsle: {rmsle_score:.5f}' )\n    #print (f'fold: {date}, mape: {mape_score:.5f}' )\n","2b82798a":"#y_pred = model.predict(val2,num_iteration=nrounds) ","955eb50e":"test[test['Country_Region']=='Italy']","b076ceec":"y_pred.mean()","80c937d3":"i = 0\nfold_n = 0\nfor date in dates:\n\n    fold_n = fold_n +1 \n    i = i+1\n    if i==1:\n        nrounds = 200\n    else:\n        nrounds = 100\n    print(i)\n    print(nrounds)\n    \n    train['shift_1_cc'] = train.groupby(['place'])['Fatalities'].shift(i)\n    train['shift_2_cc'] = train.groupby(['place'])['Fatalities'].shift(i+1)\n    train['shift_3_cc'] = train.groupby(['place'])['Fatalities'].shift(i+2)\n    train['shift_4_cc'] = train.groupby(['place'])['Fatalities'].shift(i+3)\n    train['shift_5_cc'] = train.groupby(['place'])['Fatalities'].shift(i+4)\n        \n    val2 = train[train['Date']==date]\n    train2 = train[(train['Date']<date)]\n    y_ft = train2[\"Fatalities\"]\n    #y_val_cc = val2[\"ConfirmedCases\"]\n    \n    train2.drop(drop_cols, axis=1, inplace=True)\n    val2.drop(drop_cols, axis=1, inplace=True)\n    \n    #np.log1p(y)\n    #feature_importances = pd.DataFrame()\n    #feature_importances['feature'] = train.keys()\n    \n    #score = 0       \n    dtrain = lgb.Dataset(train2, label=y_ft)\n    dvalid = lgb.Dataset(val2, label=y_val_ft)\n\n    model = lgb.train(params, dtrain, nrounds, \n                            #valid_sets = [dtrain, dvalid],\n                            categorical_feature = ['place'], #'Province\/State', 'Country\/Region'\n                            verbose_eval=False)#, early_stopping_rounds=50)\n\n    y_pred = model.predict(val2,num_iteration=nrounds)  #model.best_iteration\n    #y_pred = np.expm1( y_pred)\n    #vcheck.loc[vcheck['Date']==date,'cc_predict'] = y_pred\n    test.loc[test['Date']==date,'Fatalities'] = y_pred\n    train.loc[train['Date']==date,'Fatalities'] = y_pred\n    #y_oof[valid_index] = y_pred\n\n    #rmsle_score = rmsle(y_val_cc, y_pred)\n    #mape_score = mape(y_val_cc, y_pred)\n    #score += rmsle_score\n    #print (f'fold: {date}, rmsle: {rmsle_score:.5f}' )\n    #print (f'fold: {date}, mape: {mape_score:.5f}' )\n","8f3becbd":"test[test['Country_Region']=='Italy']","66b8bc66":"print(len(test))","cfdf0ab0":"train_sub = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv\")","4580abed":"#train_sub.loc[(train_sub['Date']=='2020-03-24')&(train_sub['Country_Region']=='France')&(train_sub['Province_State']=='France'),'ConfirmedCases'] = 22654\n#train_sub.loc[(train_sub['Date']=='2020-03-24')&(train_sub['Country_Region']=='France')&(train_sub['Province_State']=='France'),'Fatalities'] = 1000","235c2be8":"test = pd.merge(test,train_sub[['Province_State','Country_Region','Date','ConfirmedCases','Fatalities']], on=['Province_State','Country_Region','Date'], how='left')","c3d024b6":"print(len(test))","7458ae76":"test.head()","2a628677":"test.loc[test['ConfirmedCases_x'].isnull()==True]","e4d0f167":"test.loc[test['ConfirmedCases_x'].isnull()==True, 'ConfirmedCases_x'] = test.loc[test['ConfirmedCases_x'].isnull()==True, 'ConfirmedCases_y']","56fcb549":"test.head()","3932e711":"test.loc[test['Fatalities_x'].isnull()==True, 'Fatalities_x'] = test.loc[test['Fatalities_x'].isnull()==True, 'Fatalities_y']","bdfd90e9":"dates","d16725b9":"#last_amount = test.loc[(test['Country_Region']=='Italy')&(test['Date']=='2020-03-24'),'ConfirmedCases_x']","58ee3eac":"#last_fat = test.loc[(test['Country_Region']=='Italy')&(test['Date']=='2020-03-24'),'Fatalities_x']","d7f0b68c":"#last_fat.values[0]","dfef798a":"#dates","c9a2d451":"#len(dates)","b26e2b43":"#30\/29\n","5857631e":"#i = 0\n#k = 35","d40ce1ed":"\n#for date in dates:\n#    k = k-1\n#    i = i + 1\n#    test.loc[(test['Country_Region']=='Italy')&(test['Date']==date),'ConfirmedCases_x'] =  last_amount.values[0]+i*(5000-(100*i))\n#    test.loc[(test['Country_Region']=='Italy')&(test['Date']==date),'Fatalities_x'] =  last_fat.values[0]+i*(800-(10*i))","606746d6":"#test.loc[(test['Country_Region']=='Italy')] #&(test['Date']==date),'ConfirmedCases_x' ","a1940ccc":"sub = test[['ForecastId', 'ConfirmedCases_x','Fatalities_x']]","1cb64c21":"sub.columns = ['ForecastId', 'ConfirmedCases', 'Fatalities']","4d5dc02a":"sub.head()","07b5b69e":"sub.loc[sub['ConfirmedCases']<0, 'ConfirmedCases'] = 0","8ba2076c":"sub.loc[sub['Fatalities']<0, 'Fatalities'] = 0","2ba59323":"sub['Fatalities'].describe()\nsub.dtypes","8026d019":"#rename submission columns \n#sub = sub.rename(columns={'ForecastId': 'newName1', 'ConfirmedCases': 'Fatalities'})","0b23cc22":"#sub.to_csv('submission.csv',index=False)","22611623":"print(\"Read in libraries\")\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom random import random\n\nprint(\"read in train file\")\ndf=pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv\",\n               usecols=['Province_State','Country_Region','Date','ConfirmedCases','Fatalities'])\n\nprint(\"fill blanks and add region for counting\")\ndf.fillna(' ',inplace=True)\ndf['Lat']=df['Province_State']+df['Country_Region']\ndf.drop('Province_State',axis=1,inplace=True)\ndf.drop('Country_Region',axis=1,inplace=True)\n\ncountries_list=df.Lat.unique()\ndf1=[]\nfor i in countries_list:\n    df1.append(df[df['Lat']==i])\nprint(\"we have \"+ str(len(df1))+\" regions in our dataset\")\n\n#read in test file \ntest=pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/test.csv\")\n\n#create the estimates assuming measurement error \nsubmit_confirmed=[]\nsubmit_fatal=[]\nfor i in df1:\n    # contrived dataset\n    data = i.ConfirmedCases.astype('int32').tolist()\n    # fit model\n    try:\n        #model = SARIMAX(data, order=(2,1,0), seasonal_order=(1,1,0,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\n        model = SARIMAX(data, order=(1,1,0), seasonal_order=(1,1,0,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\n        #model = SARIMAX(data, order=(1,1,0), seasonal_order=(0,1,0,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\n        #model = ARIMA(data, order=(3,1,2))\n        model_fit = model.fit(disp=False)\n        # make prediction\n        predicted = model_fit.predict(len(data), len(data)+34)\n        new=np.concatenate((np.array(data),np.array([int(num) for num in predicted])),axis=0)\n        submit_confirmed.extend(list(new[-43:]))\n    except:\n        submit_confirmed.extend(list(data[-10:-1]))\n        for j in range(34):\n            submit_confirmed.append(data[-1]*2)\n    \n    # contrived dataset\n    data = i.Fatalities.astype('int32').tolist()\n    # fit model\n    try:\n        #model = SARIMAX(data, order=(1,0,0), seasonal_order=(0,1,1,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\n        model = SARIMAX(data, order=(1,1,0), seasonal_order=(1,1,0,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\n        #model = ARIMA(data, order=(3,1,2))\n        model_fit = model.fit(disp=False)\n        # make prediction\n        predicted = model_fit.predict(len(data), len(data)+34)\n        new=np.concatenate((np.array(data),np.array([int(num) for num in predicted])),axis=0)\n        submit_fatal.extend(list(new[-43:]))\n    except:\n        submit_fatal.extend(list(data[-10:-1]))\n        for j in range(34):\n            submit_fatal.append(data[-1]*2)\n            \n#make the submission file \ndf_submit=pd.concat([pd.Series(np.arange(1,1+len(submit_confirmed))),pd.Series(submit_confirmed),pd.Series(submit_fatal)],axis=1)\ndf_submit=df_submit.fillna(method='pad').astype(int)\n\n#view submission file \ndf_submit.head()\n#df_submit.dtypes\n\n#join the submission file info to the test data set \n#rename the columns \ndf_submit.rename(columns={0: 'ForecastId', 1: 'ConfirmedCases',2: 'Fatalities',}, inplace=True)","b4a248b3":"sub.head()","1820114b":"df_submit.head()","916cb50c":"#rename \nsub.rename(columns={'ForecastId': 'ForecastId1', \"ConfirmedCases\": 'Confirmed_lgbt',\"Fatalities\": 'Fatalities_lgbt',}, inplace=True)\ndf_submit.rename(columns={'ForecastId': 'ForecastId2', \"ConfirmedCases\": 'Confirmed_arima',\"Fatalities\": 'Fatalities_arima',}, inplace=True)\ndf_submit.shape\n\n#combine \ndf_combine = pd.concat([sub, df_submit], axis=1, join='inner')\n\n#average \ncols = ['Confirmed_lgbt','Confirmed_arima']\ndf_combine['ConfirmedCases'] = df_combine[cols].astype(float).mean(axis=1)\n\ncols = ['Fatalities_lgbt','Fatalities_arima']\ndf_combine['Fatalities'] = df_combine[cols].astype(float).mean(axis=1)\n\n#drop\ndel df_combine['ForecastId2']\ndel df_combine['Confirmed_lgbt']\ndel df_combine['Fatalities_lgbt']\ndel df_combine['Confirmed_arima']\ndel df_combine['Fatalities_arima']\n\ndf_combine.head()\n\n#rename\ndf_combine.rename(columns={'ForecastId1': 'ForecastId', \"ConfirmedCases\": 'ConfirmedCases',\"Fatalities\": 'Fatalities',}, inplace=True)\n\n#make submission file\ndf_combine.to_csv('submission.csv',index=False)\n\n#make complete test file \ntest=pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/test.csv\")\ncomplete_test= pd.merge(test, df_combine, how=\"left\", on=\"ForecastId\")\ncomplete_test.to_csv('complete_test.csv',index=False)\n","d06f1a4f":"# Combine both into average","e3f45814":"At this part we append our testing data to the training data at the date where this is no overlap. Today the train set went up to March 30th.","86b655d4":"# Run ARIMA Influenza Baselines \nhttps:\/\/www.kaggle.com\/skeller\/arima-influenza-baselines","a2cea13b":"At this point you want to set your validation set to be the section of time that is in the overlap period with train","ebdf2146":"# Run Light GBM Model\nhttps:\/\/www.kaggle.com\/pietromarinelli\/8th-place-at-day-1-with-lgb-with-few-features","75b77614":"Another tricky part, set this to be the same date as the stacked test data set from the beginning. "}}