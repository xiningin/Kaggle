{"cell_type":{"0a78fcff":"code","58e4e40a":"code","f3fdc607":"code","179ec13d":"code","e40ed36f":"code","8b079c38":"code","d2d54425":"code","87a7269d":"code","a9eec085":"code","cc22a328":"code","9f663859":"code","9f4c03e2":"code","1ca5d5b1":"code","19bb0136":"code","214b05dc":"code","061d9ab5":"code","8254ac0f":"code","67299428":"code","96a71b59":"code","e2eb773b":"code","12e09c0f":"code","6d2b9a8e":"code","5ed5c442":"code","7c5b54e2":"code","17503104":"code","5331d0a4":"code","b7096a7f":"code","fd50b4a7":"code","9a0e553f":"code","683549c7":"code","a11bea91":"code","71779dac":"code","413f1fcc":"code","9c042959":"code","49186fa4":"code","f3cda55b":"code","bb8f49c3":"code","09923f62":"code","cf8c8528":"code","489660b8":"code","e08448e7":"code","1b77cbd9":"code","bdfbae95":"code","392981d8":"code","7c24b4a4":"code","1eab56f9":"code","b721dc86":"code","3bef51ca":"code","9ad23c36":"code","95d96e89":"code","ddbfc633":"code","f9e201cf":"code","eb29aaf5":"code","c3b3c206":"code","ada0bda9":"code","9e439037":"code","ccf5a929":"code","749bfa1e":"code","a02f7672":"code","a4179a1a":"code","57657d1c":"code","7ea4c821":"code","fb42825b":"code","3987fd15":"code","67544d14":"code","c63a8455":"code","27f5ee77":"code","d0b6144d":"code","9fdf38d9":"code","14b09cb4":"code","bc0626b3":"code","938746f6":"code","66ab976c":"code","f896c0b5":"code","e3a25370":"code","21721afe":"code","7c2bc36f":"code","3b9d0006":"code","5a34b62a":"code","5ec7d49b":"markdown","9c206cfb":"markdown","02af0e3f":"markdown","85f94aee":"markdown","b6a4c267":"markdown","aa1ba199":"markdown","93aa5645":"markdown","4245cede":"markdown","6dcda964":"markdown","7e9ee256":"markdown","b29081de":"markdown","28969fde":"markdown","fcda5809":"markdown","509313b0":"markdown","a908c5fe":"markdown","1dd8139f":"markdown","389b973e":"markdown","18ec2e6b":"markdown","e36d44d3":"markdown","c9ccc85c":"markdown","b3a9ccbb":"markdown","f3306ef1":"markdown","b8aafda9":"markdown","ac3914c9":"markdown","f3451e7a":"markdown","7e9704c7":"markdown","4a887ee7":"markdown","f32a9643":"markdown","99df2776":"markdown","437b9e19":"markdown","4f239497":"markdown"},"source":{"0a78fcff":"# Import libraries for math and data processing\nimport numpy as np\nimport pandas as pd\n\n# Import libraries for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Import libraries for feature engineering\nfrom sklearn import set_config\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, OrdinalEncoder, PowerTransformer, FunctionTransformer, QuantileTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score, learning_curve, RandomizedSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import IsolationForest, StackingRegressor, BaggingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge, Lasso, SGDRegressor\n\n# Visualize diagrams for sklearn objects\nset_config(display = 'diagram')\n\n# Import librarie for statistics\nimport scipy.stats as stats\nimport statsmodels.formula.api as smf\nfrom statsmodels.graphics.gofplots import qqplot\n\n# Import libraries for ML\nimport xgboost as xgb\nfrom xgboost import (XGBRegressor, cv as XGBcv)\nfrom lightgbm import (LGBMRegressor, cv as LGBMcv)\nfrom catboost import (CatBoostRegressor, cv as CBcv)\n\n\n# Set a globally used random seed for every method\nrand_seed = 123\n\n# Change to 0 to disable below graph functions\nenable_graphs = 1\n\n# Function used to plot correlation heatmap\ndef plot_corr_heatmap(df, mtd = 'pearson', size = (50,50), n_important = 10):\n    \n    if enable_graphs == 0:\n        print('Heatmap Graph disabled')\n        return\n    \n    important_cols = df.corr(method = mtd).nlargest(n_important, 'SalePrice')['SalePrice'].index\n    \n    fig, ax = plt.subplots(figsize = size)\n    sns.set(font_scale = 6)\n    sns.heatmap(\n                    data       = df[important_cols].corr(method = mtd), \n                    annot      = True, \n                    annot_kws  = {'fontsize' : 10},\n                    linewidths = 0.7, \n                    linecolor  = 'black', \n                    square     = True, \n                    ax         = ax, \n                    cbar       = False, \n                    fmt        = '.3f', \n                    cmap       = 'coolwarm', \n                    robust     = True\n                )\n    plt.show()\n    sns.reset_orig()\n    return \n\ndef plot_skewness(df, figsize = (30,8), title = ''):\n    \n    if enable_graphs == 0:\n        print('Skewness Graph disabled')\n        return\n    \n    ax = df.skew().plot(\n                            kind    = 'bar', \n                            figsize = figsize,\n                            xlabel  = 'Variables', \n                            ylabel  = 'Skewness',\n                            title   = title,\n                            grid    = False,\n                            rot     = 90\n                        )\n\n    for container in ax.containers:\n        ax.bar_label(container, fmt = '%.3f', padding = 5)\n        \n    return plt.show()\n\n# Function used to perform a normality check\ndef normality_test(df):\n    normality_test = pd.DataFrame(data = {'Feature' : df.columns, \n                                          'Shapiro Statistic': '', \n                                          'Shapiro p-value' : '', \n                                          'Anderson-Darling Statistic' : '', \n                                          'Anderson-Darling Critical Value (1%)' : ''\n                                         })\n\n    for feature in normality_test['Feature']:\n        normality_test['Shapiro Statistic'] = stats.shapiro(df[feature])[0]\n        normality_test['Shapiro p-value']   = stats.shapiro(df[feature])[1]\n        \n        normality_test['Anderson-Darling Statistic']           = stats.anderson(df[feature])[0]\n        normality_test['Anderson-Darling Critical Value (1%)'] = stats.anderson(df[feature])[1][4]\n\n    return normality_test\n\n# Function used to average predictions\ndef average_predictions(preds1, preds2, mtd = 'log'):\n    if mtd == 'log':\n        avrg = np.exp((np.log(preds1) + np.log(preds2))\/2)\n    else:\n        avrg = np.average([preds1, preds2])\n    \n    return avrg","58e4e40a":"# Load data in a DataFrame\ntrain_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","f3fdc607":"# Visualize some train data\ntrain_df","179ec13d":"# Visualize some test data\ntest_df","e40ed36f":"# Visualize the shape of the data\ntrain_df.shape, test_df.shape","8b079c38":"# Let's see the % of NaN values in each column of TRAIN DF (only those ones with missing values)\nmask = train_df.isna().mean().sort_values() > 0\nax = (train_df.loc[:, mask[mask == True].keys()].isna().mean().sort_values() * 100).plot(\n                                                                                          kind    = 'barh', \n                                                                                          figsize = (30,12),\n                                                                                          xlabel  = 'Variables', \n                                                                                          title   = '% of missing values in Train DF'\n                                                                                        )\n\nfor container in ax.containers:\n    ax.bar_label(container, fmt = '%.2f%%', padding = 5)","d2d54425":"# Let's see the % of NaN values in each column of TEST DF (only those ones with missing values)\nmask = test_df.isna().mean().sort_values() > 0\nax = (test_df.loc[:, mask[mask == True].keys()].isna().mean().sort_values() * 100).plot(\n                                                                                          kind    = 'barh', \n                                                                                          figsize = (30,15),\n                                                                                          xlabel  = 'Variables', \n                                                                                          title   = '% of missing values in Test DF',\n                                                                                          grid    = False\n                                                                                        )\nfor container in ax.containers:\n    ax.bar_label(container, fmt = '%.2f%%', padding = 5)","87a7269d":"# Features almost empty will be removed later\nmissing = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']","a9eec085":"# Categorical columns with missing values (descending order)\nnan_mask = train_df[train_df.columns[train_df.dtypes == object]].isna().sum().sort_values(ascending = False) > 0\ntrain_df.loc[:, nan_mask[nan_mask == True].keys()].dtypes == object","cc22a328":"# Let's see data types\ntrain_df.info()","9f663859":"# See some statistics\ntrain_df.describe(include = 'all')","9f4c03e2":"# Filter numerical columns and visualize histograms\nnum_mask = train_df.dtypes != object\nnum_cols = train_df.loc[:, num_mask[num_mask == True].keys()]\nnum_cols.hist(figsize = (30,15), layout = (4,10))\nplt.show()","1ca5d5b1":"normality_test(num_cols)","19bb0136":"plot_skewness(num_cols, title = 'Skewness of numerical features')","214b05dc":"# Let's see the outliers\nnum_cols.plot(\n                kind     = 'box',\n                subplots = True, \n                layout   = (4,10), \n                sharex   = False, \n                sharey   = False,\n                figsize  = (30,15),\n                grid     = True\n            )\n\nplt.show()","061d9ab5":"plot_corr_heatmap(num_cols, size = (12,12), n_important = 15)","8254ac0f":"plot_corr_heatmap(num_cols, size = (12,12), mtd = 'spearman', n_important = 15)","67299428":"most_corr_features = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath']\nsns.pairplot(train_df[most_corr_features])","96a71b59":"# Assignations\nX = train_df.drop(columns = ['Id', 'SalePrice'], axis = 1)\ny = train_df.loc[:, 'SalePrice'].to_frame()","e2eb773b":"# As we discussed above, let's assign this variables as categorical\nX = X.astype(dtype = {\n                        'YrSold'       : object, \n                        'YearBuilt'    : object, \n                        'YearRemodAdd' : object, \n                        'MoSold'       : object, \n                        'GarageYrBlt'  : object\n                     }\n            )","12e09c0f":"# Temporary DF used to seek relations\nareas = X.loc[:, [\n                    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n                    'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n                    'LowQualFinSF', 'GrLivArea', 'GarageArea', \n                    'OpenPorchSF', 'PoolArea'\n                 ]\n            ]","6d2b9a8e":"# There is a linear relationship between this features\n# 'BsmtFinSF1' + 'BsmtFinSF2' + 'BsmtUnfSF' = 'TotalBsmtSF'\nsns.scatterplot(\n                    x = np.sum(areas.loc[:, ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF']], axis = 1),\n                    y = areas.loc[:, 'TotalBsmtSF']\n                )","5ed5c442":"# Ensure this by adjusting an OLS model\nmodel = smf.ols(\"TotalBsmtSF ~ BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF\", data = areas).fit()\nprint(model.summary())","7c5b54e2":"# Same thing with this features. There is a linear relationship too\n# 'OpenPorchSF' + '1stFlrSF' + '2ndFlrSF' + 'PoolArea' = 'GrLivArea'\nsns.scatterplot(\n                    x = np.sum(areas.loc[:, ['OpenPorchSF', '1stFlrSF', '2ndFlrSF', 'PoolArea']], axis = 1),\n                    y = areas.loc[:, 'GrLivArea']\n                )","17503104":"# Add a column in order to check linearity later\nareas['TotalAreaAbvGr'] = np.sum(areas.loc[:, ['OpenPorchSF', '1stFlrSF', '2ndFlrSF', 'PoolArea']], axis = 1)\nareas['TotalAreaAbvGr']","5331d0a4":"# Check linearity\nmodel = smf.ols(\"GrLivArea ~ TotalAreaAbvGr\", data = areas).fit()\nprint(model.summary())","b7096a7f":"# Now, we check if htere is a relationship between this ones\nsns.scatterplot(\n                    x = np.sum(areas.loc[:, ['TotalBsmtSF', 'TotalAreaAbvGr']], axis = 1),\n                    y = areas.loc[:, 'GrLivArea']\n                )","fd50b4a7":"# It seems like there is\nmodel = smf.ols(\"GrLivArea ~ TotalAreaAbvGr + TotalBsmtSF\", data = areas).fit()\nprint(model.summary())","9a0e553f":"# Filter the numerical data with the most relevant features\nnumerical_features = X[most_corr_features[1:]]\nnumerical_features","683549c7":"# Filter the categorical features\ncategorical_features = X.select_dtypes(include = [object])\ncategorical_features","a11bea91":"# Modify this variable, leaving only the relevant features\nX = X.loc[:, list(categorical_features.columns) + list(numerical_features.columns)]\n\n# Drop almost empty columns\nX.drop(columns = missing, inplace = True)\n\n# See the result\nX","71779dac":"# First, detect the outliers with IsolationForest\niforest = IsolationForest(\n                            n_estimators  = 100,\n                            max_samples   = 1.0,\n                            contamination = 0.01, \n                            bootstrap     = True,\n                            n_jobs        = -1, \n                            random_state  = rand_seed,\n                            warm_start    = True\n                        )\n\n# Add an indicator column in the new data\nX['outliers'] = iforest.fit_predict(X[numerical_features.columns])","413f1fcc":"# See the detected outliers\nX[X['outliers'] == -1].select_dtypes(exclude = [object])","9c042959":"# Get the outliers indexes\noutliers_index = X[X['outliers'] == -1].index","49186fa4":"fig, ax = plt.subplots(nrows = 1, ncols = numerical_features.shape[1], sharex = True, sharey = False, figsize = (30, 5))\n\nfig.suptitle(t = 'Outliers detected by IsolationForest', fontsize = 14)\n\nfor i, col in enumerate(numerical_features):\n    ax[i].set_xlabel('Index')\n    ax[i].set_ylabel(col)\n    ax[i].set_label('Outlier')\n    ax[i].scatter(x = numerical_features.index, y = numerical_features[col], label = 'Inlier')\n    ax[i].scatter(x = outliers_index, y = numerical_features[col][outliers_index], c = 'red', label = 'Outlier')\n    ax[i].legend()\n\nplt.show()","f3cda55b":"# Filter the outliers\nX = X[X['outliers'] == 1].iloc[:, :-1]\nX","bb8f49c3":"# Get categorical columns names\ncateg_cols_names = X.columns[X.dtypes == object]\n\n# Categorical columns with missing values\ncateg_nan_cols = X[categ_cols_names].columns[X[categ_cols_names].isna().sum() > 0]\n\n# Get numerical columns names\nnum_cols_names = X.columns[X.dtypes != object]\n\n# Numerical columns with missing values\nnum_nan_cols = X[num_cols_names].columns[X[num_cols_names].isna().sum() > 0]","09923f62":"# Assign np.nan type to NaN values in categorical features\n# in order to ensure detectability in posterior methods\nX[categ_nan_cols] = X[categ_nan_cols].fillna(value = np.nan, axis = 1)\n\n# Same thing in numerical features\nX[num_nan_cols] = X[num_nan_cols].fillna(value = np.nan, axis = 1)","cf8c8528":"# Define pipeline for imputation of the numerical features\nnum_pipeline = Pipeline(steps = [\n                                    ('Simple Imputer', SimpleImputer(strategy = 'median')),\n                                    ('Robust Scaler', RobustScaler()),\n                                    ('Power Transformer', PowerTransformer())\n                                ]\n                        )\n\n# Define pipeline for imputation and encoding of the categorical features\ncateg_pipeline = Pipeline(\n                            steps = [\n                                        ('Categorical Imputer', SimpleImputer(strategy = 'most_frequent')),\n                                        ('One Hot Encoder', OneHotEncoder(drop = 'first'))\n                                    ]\n                         )\n\n# Transform specific columns\nct = ColumnTransformer(\n                        [\n                            ('Categorical Pipeline', categ_pipeline, categ_cols_names),\n                            ('Numerical Pipeline', num_pipeline, num_cols_names)\n                        ], \n                        remainder        = 'passthrough', \n                        sparse_threshold = 0,\n                        n_jobs           = -1\n                    )\n\n# Final pipeline with all transformations\npipe = Pipeline(steps = [\n                            ('Column Transformer', ct), \n                            ('PCA', PCA(n_components = 286, whiten = False, svd_solver = 'full', random_state = rand_seed)),\n                        ]\n               )","489660b8":"# Let's visualize and check the pipeline\npipe","e08448e7":"# Create a new DF with the transformed data\nXt = pd.DataFrame(data = pipe.fit_transform(X))\nXt","1b77cbd9":"# Let's see some statistics of the new data\nXt.describe()","bdfbae95":"normality_test(Xt)","392981d8":"# Let's see the worst case\nqqplot(Xt[Xt.skew().abs().idxmax()], fit = True, line = '45')\nplt.show()","7c24b4a4":"plot_skewness(Xt, title = 'Skewness of features once transformed')","1eab56f9":"Xt.iloc[:, :30].hist(figsize = (30,10), layout = (3,10))\nplt.show()","b721dc86":"# No missing values left\nprint('Missing values = ' + str(Xt.isna().sum().sum()))","3bef51ca":"# Let's see the outliers\nXt.plot(kind = 'box', figsize = (30,10), grid = True, rot = 90)\nplt.show()","9ad23c36":"# See the last case\nsns.scatterplot(x = Xt.index, y = Xt.iloc[:, -1])","95d96e89":"# Filter the most skewed features\nskew_mask = Xt.skew().abs() > 0.3\nskewed = Xt.loc[:, skew_mask[skew_mask == True].keys()]\nskewed","ddbfc633":"# Transform the skewed features in order they look more Gaussian-like\nfor col in skewed:\n    #Xt[col] = stats.yeojohnson(x = skewed[col], lmbda = None)[0]\n    Xt[col] = stats.yeojohnson(x = skewed[col], lmbda = stats.yeojohnson_normmax(skewed[col]))","f9e201cf":"qqplot(Xt[Xt.skew().abs().idxmax()], fit = True, line = '45')\nplt.show()","eb29aaf5":"plot_skewness(Xt, title = 'Skewness of features after all transformations')","c3b3c206":"sns.distplot(Xt[Xt.skew().abs().idxmax()], kde = False, fit = stats.norm)","ada0bda9":"# Filter the same indexes in the target variable\ny_ol = y.drop(labels = outliers_index)\ny_ol","9e439037":"# Aplly a log transform in order to reduce skewness. After modeling, we need to do inverse transform to get real results.\ny_ol = pd.DataFrame(data = np.log(y_ol), columns = ['SalePrice'])","ccf5a929":"sns.distplot(y_ol, kde = False, fit = stats.norm)","749bfa1e":"# The target variable now has a very little skewness\ny_ol.skew()","a02f7672":"# Let's pairplot against 4 less skewed variables\nplot_df = Xt.iloc[:, Xt.skew().abs().sort_values()[:4].index]\nplot_df.insert(loc = plot_df.shape[1], column = 'SalePrice', value = y_ol.values)\nsns.pairplot(plot_df)","a4179a1a":"# Assing train variables\nX_train = Xt\ny_train = y_ol","57657d1c":"# Folds for cross-validation\ncv_folds = 5","7ea4c821":"# Tunning\nrs = RandomizedSearchCV(\n                            estimator = XGBRegressor(\n                                                        objective   = 'reg:squarederror',\n                                                        eval_metric = 'rmse',\n                                                        booster     = \"gbtree\",\n                                                        base_score  = 0.5,\n                                                        tree_method = 'gpu_hist',\n                                                        n_jobs      = -1\n                                                    ),\n                            param_distributions = {\n                                                    'n_estimators'     : [10, 100, 650, 1000, 2000, 3500],\n                                                    'max_depth'        : [2, 4, 6, 8, 10, 12],\n                                                    'learning_rate'    : np.random.random(size = 10),\n                                                    'reg_lambda'       : np.random.uniform(0, 10, size = 5),\n                                                    'reg_alpha'        : np.random.uniform(0, 10, size = 5),\n                                                    'min_child_weight' : np.random.uniform(0, 10, size = 5),\n                                                    'max_delta_step'   : np.random.uniform(0, 10, size = 5),\n                                                    'subsample'        : [0.1, 0.25, 0.5, 0.75, 1],\n                                                    'tree_method'      : ['gpu_hist']\n                                                  },\n                            n_iter             = 100,\n                            scoring            = 'neg_root_mean_squared_error',\n                            n_jobs             = -1,\n                            cv                 = cv_folds,\n                            return_train_score = True,\n                            random_state       = rand_seed\n                        )\n\nres = rs.fit(X_train, y_train)","fb42825b":"cv_res = pd.DataFrame(res.cv_results_)\ncv_res","3987fd15":"res.best_estimator_","67544d14":"res.best_score_","c63a8455":"res.best_params_","27f5ee77":"res.best_index_","d0b6144d":"\"\"\"\n# Define the stacking\nestimators = [\n                ('XGBoost Regressor', xgbr),\n                ('LightBM Regressor', lgbmr)\n            ]\n\nsr = StackingRegressor(\n                        estimators      = estimators,\n                        final_estimator = cbr,\n                        cv              = cv_folds,\n                        n_jobs          = -1,\n                        verbose         = 0\n                    )\n\n# Perform the CV\nprint(str(cv_folds) + '-fold cross validation scores:\\n')\nfor clf, label in zip([xgbr, lgbmr, sr], ['XGBoost Regressor', 'LightBM Regressor', 'StackingRegressor']):\n    scores = cross_val_score(clf, X_train, y_train, cv = cv_folds, scoring = 'neg_root_mean_squared_error', n_jobs = -1)\n    print(\"RMSE Score: %0.2f (+\/- %0.2f) [%s]\" % (np.abs(scores.mean()), scores.std(), label))\n\"\"\"","9fdf38d9":"\"\"\"\ntrain_sizes_abs, train_scores, test_scores, fit_times, score_times = learning_curve(\n                                                                                        estimator    = xgbr, \n                                                                                        X            = X_train, \n                                                                                        y            = y_train.values.ravel(), \n                                                                                        cv           = cv_folds, \n                                                                                        scoring      = 'neg_root_mean_squared_error', \n                                                                                        n_jobs       = -1, \n                                                                                        return_times = True\n                                                                                    )\n\ntrain_scores = np.abs(train_scores)\ntest_scores = np.abs(test_scores)\n\n# Define some variables to use in the graphic\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\nfit_times_mean = np.mean(fit_times, axis=1)\nfit_times_std = np.std(fit_times, axis=1)\n\nfig, axes = plt.subplots(1, 1, figsize=(5, 5))\n\n# Plot learning curve\naxes.grid()\naxes.fill_between(train_sizes_abs, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha = 0.1, color=\"r\")\naxes.fill_between(train_sizes_abs, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha = 0.1, color=\"g\")\naxes.plot(train_sizes_abs, train_scores_mean, \"o-\", color = \"r\", label = \"Training score\")\naxes.plot(train_sizes_abs, test_scores_mean, \"o-\", color = \"g\", label = \"Cross-validation score\")\naxes.legend(loc = \"best\")\n\"\"\"","14b09cb4":"\"\"\"\n# Perform the CV for XGBoost Regressor\nprint(str(cv_folds) + '-fold cross validation scores:\\n')\nscores = cross_val_score(xgbr, X_train, y_train, cv = cv_folds, scoring = 'neg_root_mean_squared_error', n_jobs = -1)\nprint(\"RMSE Score: %0.2f (+\/- %0.2f) [%s]\" % (np.abs(scores.mean()), scores.std(), 'XGBoost Regressor'))\n\n# Perform the CV for LightBM Regressor\nprint(str(cv_folds) + '-fold cross validation scores:\\n')\nscores = cross_val_score(lgbmr, X_train, y_train, cv = cv_folds, scoring = 'neg_root_mean_squared_error', n_jobs = -1)\nprint(\"RMSE Score: %0.2f (+\/- %0.2f) [%s]\" % (np.abs(scores.mean()), scores.std(), 'LightBM Regressor'))\n\"\"\"","bc0626b3":"# Define the regressors\nxgbr = XGBRegressor(\n                        n_estimators     = 3500,\n                        max_depth        = 10,\n                        learning_rate    = 0.11365365322994458,\n                        reg_lambda       = 9.862718851483583,\n                        reg_alpha        = 0.9904247647422559,\n                        max_delta_step   = 0.7813366697492852,\n                        min_child_weight = 3.01544112210789,\n                        subsample        = 0.5,\n                        objective        = 'reg:squarederror',\n                        eval_metric      = 'rmse',\n                        booster          = \"gbtree\",\n                        base_score       = 0.5,\n                        tree_method      = 'gpu_hist',\n                        n_jobs           = -1,\n                        random_state     = rand_seed\n                   )\n\nlgbmr = LGBMRegressor(\n                        objective        = 'rmse', \n                        device_type      = 'gpu',\n                        gpu_use_dp       = True,\n                        n_estimators     = 3500,\n                        max_depth        = 10,\n                        learning_rate    = 0.11365365322994458,\n                        reg_lambda       = 9.862718851483583,\n                        reg_alpha        = 0.9904247647422559,\n                        max_delta_step   = 0.7813366697492852,\n                        min_child_weight = 3.01544112210789,\n                        subsample        = 0.5,\n                        n_jobs           = -1,\n                        random_state     = rand_seed\n                    )","938746f6":"xgbr.fit(X_train, y_train)","66ab976c":"lgbmr.fit(X_train, y_train)","f896c0b5":"averaged = average_predictions(xgbr.predict(X_train), lgbmr.predict(X_train))\n\nprint('XGBoost - RMSE on TRAIN = {:.6f}' . format(mean_squared_error(y_train, xgbr.predict(X_train), squared = False)))\nprint('LightBM - RMSE on TRAIN = {:.6f}' . format(mean_squared_error(y_train, lgbmr.predict(X_train), squared = False)))\nprint('AVERAGED - RMSE on TRAIN = {:.6f}' . format(mean_squared_error(y_train, averaged, squared = False)))","e3a25370":"print('XGBoost - R2 on TRAIN = {:.3f}' . format(xgbr.score(X_train, y_train)))\nprint('LightBM - R2 on TRAIN = {:.3f}' . format(lgbmr.score(X_train, y_train)))","21721afe":"# Filter the test dataset and drop the same columns as before\nX_test = test_df.loc[:, list(categorical_features.columns) + list(numerical_features.columns)]\nX_test.drop(columns = missing, inplace = True)\nX_test","7c2bc36f":"# Perform all transformations in the same way\nX_test = pd.DataFrame(data = pipe.fit_transform(X_test))\nX_test","3b9d0006":"# Make the predictions\npredictions = pd.DataFrame(data = {'Id' : test_df['Id'], 'SalePrice' : np.exp(average_predictions(xgbr.predict(X_test), lgbmr.predict(X_test)))})\npredictions","5a34b62a":"# Submit\npredictions.to_csv(\"submission.csv\", index = False)","5ec7d49b":"This is the heatmap of the Pearson's correlation of the numerical variables","9c206cfb":"This is one of the most important parts of the notebook. In this lines, we build the pipeline used to transform data.","02af0e3f":"Define the regressors","85f94aee":"As we can see below, there aren't any numerical features with normal distribution","b6a4c267":"This is the heatmap of the Spearman's correlation of the numerical variables","aa1ba199":"# 1. Libraries and data importation","93aa5645":"Now, try to fix the skewness of some features","4245cede":"Plot of the skewness degree of each numerical feature","6dcda964":"# 3. Separate features from target","7e9ee256":"Now, features are more Gaussian-like","b29081de":"Now, we search for relationships between some numerical features","28969fde":"\u00bfHow many features do we have with missing values? \u00bfHow many missing values do they have?","fcda5809":"Let's see the ouliers detected by IsolationForest","509313b0":"Here's a plot of the learning curve","a908c5fe":"These are the categorical features with missing values","1dd8139f":"Let's see the RMSE results for each case","389b973e":"Now, let's take a look at the interquartile ranges of the numerical features, in order to detect potential outliers","18ec2e6b":"#### Numerical features histograms\nAs we can see below, there are some features that have stepped distributions (YearBuilt, YearRemodAdd, GarageYrBlt, MoSold, YrSold). That's because they really are categorical features instead of numerical.","e36d44d3":"Now, we can plot again the histograms to compare it","c9ccc85c":"Now, les treat the target variable.","b3a9ccbb":"# 6. Predictions\nHere we make the predictions","f3306ef1":"# 4. Missing data handling, transformations and feature engineering\nIn this section, we'll manage the missing values and make some transformations in order to get quality data","b8aafda9":"Now, let's see some scatterplots of the most important relations","ac3914c9":"After the transformations, one of the worst cases left it's pretty Gaussian-like","f3451e7a":"Fit the regressors","7e9704c7":"Let's plot again the skewness of each feature, in order to check the transformations made","4a887ee7":"# 5. Machine Learning model for predictions\nIn this section we'll use a Stacking of XGBoost, LGBM and SGD regressors. Also we perform a cross validation check.","f32a9643":"# 2. Data visualization (EDA)\nLet's take a look at the dataset","99df2776":"Finally, plot again the skewness graph of the worst case","437b9e19":"Now, let's treat the outliers","4f239497":"Now, the R2 value"}}