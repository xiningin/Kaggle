{"cell_type":{"014aab14":"code","67fe8f12":"code","d7b3eaf5":"code","0dcbc5e1":"code","0c29746b":"code","8b5c602b":"code","727f5ec4":"code","cbe57737":"code","1511c8bc":"code","f65c5810":"code","4f927a0b":"code","6d55ff63":"code","faa929cf":"code","6e467712":"code","2ea2071a":"code","8674831e":"code","e0d3bfed":"code","8e0cf73d":"code","6640b9ad":"code","7f959c3c":"code","56b524d0":"code","ac6f8f50":"code","3b1fbe20":"code","216eab19":"code","4837cefb":"markdown","12b864e7":"markdown","4674142a":"markdown","a75caf70":"markdown","f19e9ce2":"markdown","c2afe8b0":"markdown","ede50c92":"markdown","0bd76c7d":"markdown","fad1e593":"markdown","62e7a675":"markdown","237d94f4":"markdown","339d7e0b":"markdown","6c482f8a":"markdown","e8937706":"markdown","312303a6":"markdown","1a9c2882":"markdown","88c8e187":"markdown","e80710df":"markdown","ec62f23b":"markdown","c89ed103":"markdown","da63f22b":"markdown","e364f6fd":"markdown"},"source":{"014aab14":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport matplotlib.animation as animation\nfrom IPython import display\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom sklearn.model_selection import train_test_split","67fe8f12":"plt.style.use('seaborn-whitegrid')\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)","d7b3eaf5":"mse = tf.keras.losses.MeanSquaredError()","0dcbc5e1":"def plot_train_results(data, error):\n    \"\"\" plot the train results based on the prediction and \n        mse for every epoch in a video \"\"\"\n    \n    fig = plt.figure(dpi=100)\n    ax = fig.add_subplot(1, 1, 1)\n    xs = x\n    ys = []\n\n    def animate(i, xs, ys):\n        ys = data[i]\n        ax.clear()\n        ax.plot(xs, ys, 'o', markersize=1.2, label='pred')\n        ax.plot(xs, np.sin(x), 'o', markersize=1.2, label='sin')\n        ax.set_title('epoch '+str(i).zfill(2)+' - error '+str(error.loc[i, 'error'].round(3)))\n        ax.set_ylim(-1.1,1.1)\n        ax.legend()\n    \n    ani = animation.FuncAnimation(fig, animate, fargs=(xs, ys), frames=epochs, interval=500)\n    video = ani.to_html5_video()\n    html = display.HTML(video)\n    display.display(html)\n    plt.close()","0c29746b":"x = tf.linspace(0.0, 2*np.pi, 1000)\nx = tf.reshape(x, (1000,1))","8b5c602b":"plt.figure(dpi=100)\nplt.plot(x, np.sin(x), 'o', markersize=0.5, label='sin')\nplt.xlabel(\"Input\")\nplt.ylabel(\"Output\")\nplt.title(\"Sine Function\")\nplt.legend()\nplt.show()","727f5ec4":"epochs = 50\nunits = 50\nbatch_size = 64","cbe57737":"data = pd.DataFrame(index=range(len(x)))\nerror = pd.DataFrame()","1511c8bc":"x_values = tf.linspace(-3.0, 3.0, 100)\n\nplt.figure(dpi=100)\nplt.plot(x_values, layers.Activation('relu')(x_values), label='relu')\nplt.plot(x_values, layers.Activation('elu')(x_values), label='elu')\nplt.plot(x_values, layers.Activation('selu')(x_values), label='selu')\nplt.plot(x_values, layers.Activation('swish')(x_values), label='swish')\nplt.plot(x_values, layers.Activation('linear')(x_values), label='linear')\nplt.xlim(-3, 3)\nplt.xlabel(\"Input\")\nplt.ylabel(\"Output\")\nplt.title(\"Activation Functions\")\nplt.legend()\nplt.show()","f65c5810":"model = keras.Sequential([\n    layers.Dense(units=units, input_shape=[1]),\n    layers.Dense(units=1, activation='linear')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\n\nfor epoch in range(epochs):\n    history = model.fit(\n        x, np.sin(x),\n        batch_size=batch_size,\n        epochs=1,\n        verbose=0\n    )\n    data[epoch]=model.predict(x)\n    error.loc[epoch, 'error'] = mse(np.sin(x), model.predict(x)).numpy()","4f927a0b":"plot_train_results(data, error)","6d55ff63":"w1, b1, w2, b2 = model.weights\n\nprint('Shape x:', x.shape)\nprint('Shape w1:', w1.shape)\nprint('Shape b1:', b1.shape)\nprint('Shape w1*x:', (w1*x).shape)\nprint('Shape w1*x+b1:', ((w1*x)+b1).shape)\nprint('Shape w2:', w2.shape)\nprint('Shape (w1*x+b1)*w2+b2:',(tf.matmul(((w1*x)+b1),w2)+b2).shape)\nprint('Shape model.predict(x):', model.predict(x).shape)","faa929cf":"np.array_equal(np.array(tf.matmul(tf.keras.activations.linear(((w1*x)+b1)), w2)+b2), model.predict(x))","6e467712":"model = keras.Sequential([\n    layers.Dense(units=units, input_shape=[1], activation='relu'),\n    layers.Dense(units=1, activation='linear')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\n\nfor epoch in range(epochs):\n    history = model.fit(\n        x, np.sin(x),\n        batch_size=batch_size,\n        epochs=1,\n        verbose=0\n    )\n    data[epoch]=model.predict(x)\n    error.loc[epoch, 'error'] = mse(np.sin(x), model.predict(x)).numpy()","2ea2071a":"plot_train_results(data, error)","8674831e":"model = keras.Sequential([\n    layers.Dense(units=units, input_shape=[1], activation='relu'),\n    layers.Dense(units=units, activation='relu'),\n    layers.Dense(units=1, activation='linear')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\n\nfor epoch in range(epochs):\n    history = model.fit(\n        x, np.sin(x),\n        batch_size=batch_size,\n        epochs=1,\n        verbose=0\n    )\n    data[epoch]=model.predict(x)\n    error.loc[epoch, 'error'] = mse(np.sin(x), model.predict(x)).numpy()","e0d3bfed":"plot_train_results(data, error)","8e0cf73d":"model = keras.Sequential([\n    layers.Dense(units=units, input_shape=[1], activation='relu'),\n    layers.Dense(units=units, activation='relu'),\n    layers.Dense(units=units, activation='relu'),\n    layers.Dense(units=1, activation='linear')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\n\nfor epoch in range(epochs):\n    history = model.fit(\n        x, np.sin(x),\n        batch_size=batch_size,\n        epochs=1,\n        verbose=0\n    )\n    data[epoch]=model.predict(x)\n    error.loc[epoch, 'error'] = mse(np.sin(x), model.predict(x)).numpy()","6640b9ad":"plot_train_results(data, error)","7f959c3c":"X_train, X_val, y_train, y_val = train_test_split(np.array(x), np.sin(x), test_size=0.33, random_state=2021)","56b524d0":"epochs = 200\n\nmodel = keras.Sequential([\n    layers.Dense(units=units, input_shape=[1], activation='relu'),\n    layers.Dense(units=1, activation='linear')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=batch_size,\n    epochs=epochs,\n    verbose=0\n)","ac6f8f50":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));","3b1fbe20":"epochs = 200\n\nmodel = keras.Sequential([\n    layers.Dense(units=units, input_shape=[1], activation='relu'),\n    layers.Dense(units=units, activation='relu'),\n    layers.Dense(units=units, activation='relu'),\n    layers.Dense(units=1, activation='linear')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=batch_size,\n    epochs=epochs,\n    verbose=0\n)","216eab19":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));","4837cefb":"The results are a little bit better than bevor but not satisfactory.","12b864e7":"# Functions <a name=\"Functions\"><\/a>\nWe define some helper functions.","4674142a":"To measure the quality of the predictions of the models we use the simple mean squared error function:","a75caf70":"# Input Data <a name=\"InputData\"><\/a>\nWe consider a simple sine function.","f19e9ce2":"## One Hidden Layer With Activation Function <a name=\"OneHiddenLayerExt\"><\/a>\nTo improve the model above and represent a non linear correlation we extend the model by an activation function in the first layer. ","c2afe8b0":"# Settings <a name=\"Settings\"><\/a>","ede50c92":"After every training step we write the predictions values into the data frame **data** and the value of the mse error into the data frame **error**:","0bd76c7d":"The predictions will be better now.","fad1e593":"## Three Hidden Layers <a name=\"AnalyseThreeHiddenLayers\"><\/a>\nNow we want to investigate the training process of the model defined in section 5.4:","62e7a675":"We define some plot settings which are equivalent to Kaggle course:","237d94f4":"As we can see we get a better representation and want to improve further.","339d7e0b":"To visualize we use the function **plot_train_results** which represent the training results in a video. In this notebook different activation funcions are considered:","6c482f8a":"# Analyse Training Process <a name=\"AnalyseTrainingProcesss\"><\/a>\nIn this section we focus on the training process. Therefor we split the data set into training and validation data. ","e8937706":"## Three Hidden Layers <a name=\"ThreeHiddenLayers\"><\/a>\nFinally we add a third hidden layer to the model above:","312303a6":"## One Hidden Layer <a name=\"OneHiddenLayer\"><\/a>\nWe consider a simple model with one hidden layer and no activation function with resprect to the first hidden layer:","1a9c2882":"After that we test if our calculation and the prediction are equl:","88c8e187":"# Libraries <a name=\"Libraries\"><\/a>","e80710df":"As we can see, the model delivers a linear representation which is no good approximation of the sine function. \n\nThe model consists of one hidden and one output layer. Because of this we have two weight variables **w1, w2** and bias variables **b1, b2**. We go through the neural network and calculate the prediction based on the current weights and bias variables. ","ec62f23b":"## One Hidden Layer <a name=\"AnalyseOneHiddenLayers\"><\/a>\nFirst we analyse the training process of the model defined in section 5.2:","c89ed103":"# Intro\nWelcome to this Deep Learning tutorial notebook which is an extension to the Kaggle course named [Intro to Deep Learning](https:\/\/www.kaggle.com\/learn\/intro-to-deep-learning). We consider simple neural network models and take a look on their underlying theory. Like in the Kaggle course we use the keras libraries.\n\n**Table of content:**\n1. [Libraries](#Libraries)\n2. [Settings](#Settings)\n3. [Functions](#Functions)\n4. [Input Data](#InputData)\n5. [Simple Models](#SimpleModels) <br>\n    5.1 [One Hidden Layer](#OneHiddenLayer)<br>\n    5.2 [One Hidden Layer With An Activation Function](#OneHiddenLayerExt)<br>\n    5.3 [Two Hidden Layers](#TwoHiddenLayers)<br>\n    5.4 [Three Hidden Layers](#ThreeHiddenLayers)<br>\n6. [Analyse Training Process](#AnalyseTrainingProcess) <br>\n    6.1 [One Hidden Layer](#AnalyseOneHiddenLayers) <br>\n    6.2 [Three Hidden Layers](#AnalyseThreeHiddenLayers) <br>\n\n<span style=\"color: royalblue;\">Please vote the notebook up if it helps you. Feel free to leave a comment above the notebook. Thank you. <\/span>","da63f22b":"# Simple Models <a name=\"SimpleModels\"><\/a>\nWe focus on a successive expansion of simple neural networks. Please feel free to play with the settings of models like the number of epochs, number of units of a hidden layer or the batch size. As described in the [Kaggle course](https:\/\/www.kaggle.com\/ryanholbrook\/overfitting-and-underfitting) we can define a model's capacity by the size and complexity of the patterns it is able to learn. We can increase the capacity of a network by making it wider (more units to existing layers) or by making it deeper (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.","e364f6fd":"## Two Hidden Layers <a name=\"TwoHiddenLayers\"><\/a>\nWe add a further hidden layer to the model above to improve the results."}}