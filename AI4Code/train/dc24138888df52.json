{"cell_type":{"468b4ffa":"code","47242c57":"code","fc3957ae":"code","c1209574":"code","b5a7f692":"code","d6a2f75e":"code","75258810":"code","ebba482b":"code","39bbf41d":"code","bfbbf51f":"code","4a5464e5":"code","d17812d6":"code","839b3840":"code","25155a59":"code","1bfd83e6":"code","cf4fdf98":"code","f2e8a731":"code","93871c40":"code","851378ab":"code","472987e7":"code","2bad9739":"code","e5ffb6c4":"code","0fd98139":"code","8bdc4aaa":"code","1ea34c96":"code","08fda3ca":"code","c8233070":"code","830750fd":"code","43a67500":"code","051330b5":"code","01542477":"code","04cb90c7":"code","5f844f62":"code","28df91d0":"code","42e4f56e":"code","8f8e6370":"code","bf19a257":"code","08c0f7bd":"code","312eaec4":"code","25aeeb95":"code","d881bc56":"code","daa7fa64":"code","b2a1a652":"code","ef450b24":"code","994cae98":"code","14eb59f7":"code","46a8e31e":"code","b8a41092":"code","1d540fa5":"code","bfdafc40":"code","e48cf81e":"code","0ff5ce00":"code","bfa9c284":"code","96e211c9":"code","0c8baf61":"code","af86bd82":"markdown","30622dc1":"markdown","7e46d2eb":"markdown","a227127c":"markdown","a9d85525":"markdown","3d7d3363":"markdown","b5212341":"markdown","4b533412":"markdown","4df89294":"markdown","97b54de5":"markdown","f01b92fa":"markdown","8e703cb3":"markdown","31bcf73f":"markdown","21ca6a63":"markdown","24f3f9de":"markdown","88ab7953":"markdown","53b814c2":"markdown","7b6cfcc1":"markdown","f8174dff":"markdown","ad420a59":"markdown","6a940e83":"markdown","a3b0b6a6":"markdown","08e7963c":"markdown","cebc4af1":"markdown","f5f31b3b":"markdown"},"source":{"468b4ffa":"#General Libraries\nimport numpy as np\nimport pandas as pd \n\n# Statistic & Machine Learning Libraries\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\n# Data Visualiztion Libraries\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\n\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","47242c57":"dataset = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","fc3957ae":"dataset.head()","c1209574":"df = dataset.copy()\n\ndf = df.dropna() # Missing observation is removed.\ndf.head()","b5a7f692":"df.count() # Removed 201 missing observations.","d6a2f75e":"df.info() ","75258810":"df.describe().T","ebba482b":"corr_data = df.corr() # It checks the correlation between the numerical values in the data.\nsns.clustermap(corr_data,annot= True,fmt = '.2f')\n# While annot shows the numerical values on the chart, fmt determines how many digits will be displayed after the comma.\n\nplt.title('Correlation Between Features')\nplt.show();","39bbf41d":"# Dependent variable\n\ndf[\"stroke\"].value_counts() \n\n#1 = Stroke, 0= No stroke","bfbbf51f":"y = df[\"stroke\"] #Dependent variable\nX = df.drop([\"stroke\"], axis=1) #Independent variable","4a5464e5":"X['smoking_status'] = X['smoking_status'].replace({'formerly smoked' or 'smokes':'smoked','never smoked' or 'Unknown':'non_smoking'})\n#4 Status downgraded to Status 2.\nX['smoking_status'] = [1 if i.strip() == 'smoked' else 0 for i in X.smoking_status]\nX['gender'] = [1 if i.strip() == 'Male' else 0 for i in X.gender]\nX['ever_married'] = [1 if i.strip() == 'Yes' else 0 for i in X.ever_married]\nX['Residence_type'] = [1 if i.strip() == 'Urban' else 0 for i in X.Residence_type]\n\nX = X.drop([\"work_type\"], axis=1)\nX = X.drop([\"id\"], axis=1)\n\nX.head()","d17812d6":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.2, \n                                                    random_state = 42)","839b3840":"loj = LogisticRegression(solver = \"liblinear\")\nloj_model = loj.fit(X_train,y_train)\nloj_model\n\ntestscore_lr =accuracy_score(y_test, loj_model.predict(X_test))\naccuracy_score(y_test, loj_model.predict(X_test)) ","25155a59":"crosscore_lr =cross_val_score(loj_model, X_test, y_test, cv = 10).mean() \ncross_val_score(loj_model, X_test, y_test, cv = 10).mean()","1bfd83e6":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb_model = nb.fit(X_train, y_train)\nnb_model\n\ny_pred = nb_model.predict(X_test)\ntestscore_nb =   accuracy_score(y_test, y_pred) \naccuracy_score(y_test, y_pred) ","cf4fdf98":"crosscore_nb=cross_val_score(nb_model, X_test, y_test, cv = 10).mean() \ncross_val_score(nb_model, X_test, y_test, cv = 10).mean() ","f2e8a731":"knn = KNeighborsClassifier()\nknn_model = knn.fit(X_train, y_train)\nknn_model\n\ny_pred = knn_model.predict(X_test)\n\ntestscore_knn =accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","93871c40":"knn_params = {\"n_neighbors\": np.arange(1,50)}\n\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, knn_params, cv=10)\nknn_cv.fit(X_train, y_train)","851378ab":"print(\"The best parameters: \" + str(knn_cv.best_params_)) ","472987e7":"knn = KNeighborsClassifier(14)\nknn_tuned = knn.fit(X_train, y_train)\nknn_tuned.score(X_test, y_test)","2bad9739":"y_pred = knn_tuned.predict(X_test)\ncrosscore_knn=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred) ","e5ffb6c4":"svm_model = SVC(kernel = \"linear\").fit(X_train, y_train)\ny_pred = svm_model.predict(X_test)\n\ntestscore_svm=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred) #test acc","0fd98139":"svc_params = {\"C\": np.arange(1,2)}\n\nsvc = SVC(kernel = \"linear\")\n\nsvc_cv_model = GridSearchCV(svc,svc_params, \n                            cv = 10, \n                            n_jobs = -1, \n                            verbose = 2 )\n\nsvc_cv_model.fit(X_train, y_train)\n\nprint(\"The best parameters: \" + str(svc_cv_model.best_params_))","8bdc4aaa":"svc_tuned = SVC(kernel = \"linear\", C = 1).fit(X_train, y_train)\n\ny_pred = svc_tuned.predict(X_test)\ncrosscore_svm=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","1ea34c96":"svc_model = SVC(kernel = \"rbf\").fit(X_train, y_train)\ny_pred = svc_model.predict(X_test)\ntestscore_nonsvm=accuracy_score(y_test, y_pred) \naccuracy_score(y_test, y_pred) ","08fda3ca":"svc_params = {\"C\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100],\n             \"gamma\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100]}\n\nsvc = SVC()\nsvc_cv_model = GridSearchCV(svc, svc_params, \n                         cv = 10, \n                         n_jobs = -1,\n                         verbose = 2)\n\nsvc_cv_model.fit(X_train, y_train)","c8233070":"print(\"The Best Parameters: \" + str(svc_cv_model.best_params_))","830750fd":"svc_tuned = SVC(C = 0.0001, gamma = 0.0001).fit(X_train, y_train)\ny_pred = svc_tuned.predict(X_test)\n\ncrosscore_nonsvm=accuracy_score(y_test, y_pred) #Tuning acc\naccuracy_score(y_test, y_pred) #Tuning acc","43a67500":"from sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler()\n\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","051330b5":"from sklearn.neural_network import MLPClassifier\nmlpc = MLPClassifier().fit(X_train_scaled, y_train)\n\ny_pred = mlpc.predict(X_test_scaled)\n\ntestscore_mlpc=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","01542477":"\n### Tuning YSA\n\nmlpc_params = {\"alpha\": [0.01, 0.02, 0.005],\n              \"hidden_layer_sizes\": [(100,100,100),\n                                     (100,100),\n                                     (5, 3)],\n              \"solver\" : [\"lbfgs\",\"adam\"],\n              \"activation\": [\"relu\",\"logistic\"]}\n\nmlpc = MLPClassifier()\nmlpc_cv_model = GridSearchCV(mlpc, mlpc_params, \n                         cv = 10, \n                         n_jobs = -1,\n                         verbose = 2)\n\nmlpc_cv_model.fit(X_train_scaled, y_train)","04cb90c7":"print(\"The Best Parameters: \" + str(mlpc_cv_model.best_params_))","5f844f62":"mlpc_tuned = MLPClassifier(activation = \"relu\", \n                           alpha = 0.1, \n                           hidden_layer_sizes = (5, 3),\n                          solver = \"sgd\")","28df91d0":"mlpc_tuned.fit(X_train_scaled, y_train)\ny_pred = mlpc_tuned.predict(X_test_scaled)\ncrosscore_mlpc=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","42e4f56e":"from sklearn.tree import DecisionTreeClassifier\ncart = DecisionTreeClassifier()\ncart_model = cart.fit(X_train, y_train)","8f8e6370":"y_pred = cart_model.predict(X_test)\ntestscore_cart=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","bf19a257":"\n#Model Tunning\n\ncart_grid = {\"max_depth\": range(1,10),\n            \"min_samples_split\" : list(range(2,50)) }\ncart = tree.DecisionTreeClassifier()\ncart_cv = GridSearchCV(cart, cart_grid, cv = 10, n_jobs = -1, verbose = 2)\ncart_cv_model = cart_cv.fit(X_train, y_train)\n\nprint(\"The Best Parameters \" + str(cart_cv_model.best_params_))","08c0f7bd":"cart = tree.DecisionTreeClassifier(max_depth = 5, min_samples_split = 19)\ncart_tuned = cart.fit(X_train, y_train)\n\ny_pred = cart_tuned.predict(X_test)\ncrosscore_cart=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","312eaec4":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier().fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\ntestscore_rf=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","25aeeb95":"\n#Model Tuniing\n\nrf_params = {\"max_depth\": [2,5,8],\n            \"max_features\": [2,5],\n            \"n_estimators\": [10,500],\n            \"min_samples_split\": [2,5]}\n\nrf_model = RandomForestClassifier()\n\nrf_cv_model = GridSearchCV(rf_model, \n                           rf_params, \n                           cv = 10, \n                           n_jobs = -1, \n                           verbose = 2) \n\nrf_cv_model.fit(X_train, y_train)\nprint(\"The Best Parameters: \" + str(rf_cv_model.best_params_))","d881bc56":"rf_tuned = RandomForestClassifier(max_depth = 5, \n                                  max_features = 2, \n                                  min_samples_split = 5,\n                                  n_estimators = 5)\n\nrf_tuned.fit(X_train, y_train)","daa7fa64":"y_pred = rf_tuned.predict(X_test)\ncrosscore_rf=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","b2a1a652":"from sklearn.ensemble import GradientBoostingClassifier\ngbm_model = GradientBoostingClassifier().fit(X_train, y_train)\n\ny_pred = gbm_model.predict(X_test)\ntestscore_gbm=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","ef450b24":"### Model Tuning\n\ngbm_params = {\"learning_rate\" : [0.001, 0.01],\n             \"n_estimators\": [100,500],\n             \"max_depth\": [3,10],\n             \"min_samples_split\": [2,10]}\n\ngbm = GradientBoostingClassifier()\n\ngbm_cv = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2)\ngbm_cv.fit(X_train, y_train)\n\nprint(\"The Best Parameters: \" + str(gbm_cv.best_params_))","994cae98":"gbm = GradientBoostingClassifier(learning_rate = 0.001, \n                                 max_depth = 3,\n                                min_samples_split = 2,\n                                n_estimators = 100)\n\ngbm_tuned =  gbm.fit(X_train,y_train)\ny_pred = gbm_tuned.predict(X_test)\ncrosscore_gbm=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","14eb59f7":"\nmodeller_name=['LR','NB','KNN', 'SVC', 'Non-SVC','ANN','CART', 'RF','GradientBoosting']\nmodel_Test=[testscore_lr,testscore_nb,testscore_knn,testscore_svm,testscore_nonsvm,testscore_mlpc,testscore_cart,testscore_rf,testscore_gbm]\nmodel_Croos=[crosscore_lr,crosscore_nb,crosscore_knn,crosscore_svm,crosscore_nonsvm,crosscore_mlpc,crosscore_cart,crosscore_rf,crosscore_gbm]\n#creating line1\n\nline1= go.Scatter(\n    x = modeller_name, # x axis\n    y = model_Test, # y axis\n    mode = \"markers\", #type of plot\n    name = \"Test Scores\", # name of the plots\n    marker = dict(color = 'rgba(167,150,55,0.8)'), #color + opacity\n    text = modeller_name # hover text\n)\n\n#cretaing line2\nline2= go.Scatter(\n    x = modeller_name,\n    y = model_Croos,\n    mode = 'lines+markers',\n    name = \"Cross Valid Scores\",\n    marker = dict(color = 'rgba(95,26,80,0.8)'),\n    text = modeller_name\n) \n\ndata = [line1,line2]\n\nlayout= dict(title= 'Comparison of Test & Cross Validation Scores',              \n             xaxis= dict(title= 'ML Methods',ticklen= 5,zeroline= False)\n)\nfig = dict(data = data, layout = layout)\niplot(fig)","46a8e31e":"dataset = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf=dataset[:750]\ndf = df.dropna()\ny = df[\"stroke\"] \nX = df.drop([\"stroke\"], axis=1)\n\nX['smoking_status'] = X['smoking_status'].replace({'formerly smoked' or 'smokes':'smoked','never smoked' or 'Unknown':'non_smoking'})\nX['smoking_status'] = [1 if i.strip() == 'smoked' else 0 for i in X.smoking_status]\nX['gender'] = [1 if i.strip() == 'Male' else 0 for i in X.gender]\nX['ever_married'] = [1 if i.strip() == 'Yes' else 0 for i in X.ever_married]\nX['Residence_type'] = [1 if i.strip() == 'Urban' else 0 for i in X.Residence_type]\n\nX = X.drop([\"work_type\"], axis=1)\nX = X.drop([\"id\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.2, \n                                                    random_state = 42)\n\n\n\nsvc_tuned = SVC(kernel = \"linear\", C = 1).fit(X_train, y_train)\n\ny_pred = svc_tuned.predict(X_test)\ncrosscore_svm=accuracy_score(y_test, y_pred)","b8a41092":"#Prediction function\n\ndef predict_stroke(patient_profile):\n    users_input = pd.DataFrame(patient_profile)\n    prediction = svc_tuned.predict(users_input)          \n    if prediction == 0:\n        print('Low Stroke Risk')\n    else:\n        print('High Stroke Risk')\n    return ","1d540fa5":"\nPatient1={'gender':[1],'age':[23],'hypertension':[0],'heart_disease':[0],'ever_married':[0],'residence_type':[1],\n        'avg_glucose_level':[100.00],'bmi':[22],'smoking_status':[0]}\n\n\nPatient2={'gender':[1],'age':[68],'hypertension':[1],'heart_disease':[1],'ever_married':[1],'residence_type':[1],\n        'avg_glucose_level':[221.67],'bmi':[28],'smoking_status':[1]}\n\n\nPatient3={'gender':[0],'age':[20],'hypertension':[0],'heart_disease':[0],'ever_married':[0],'residence_type':[1],\n        'avg_glucose_level':[87],'bmi':[19],'smoking_status':[0]}\n\nPatient4={'gender':[0],'age':[76],'hypertension':[1],'heart_disease':[1],'ever_married':[1],'residence_type':[1],\n        'avg_glucose_level':[155],'bmi':[31],'smoking_status':[1]}\n\nPatient5={'gender':[1],'age':[23],'hypertension':[1],'heart_disease':[1],'ever_married':[0],'residence_type':[0],\n        'avg_glucose_level':[167],'bmi':[31],'smoking_status':[0]}\n\n\nPatient6={'gender':[0],'age':[85],'hypertension':[0],'heart_disease':[0],'ever_married':[1],'residence_type':[0],\n        'avg_glucose_level':[90],'bmi':[21],'smoking_status':[0]}\n","bfdafc40":"predict_stroke(Patient1)","e48cf81e":"predict_stroke(Patient2)","0ff5ce00":"predict_stroke(Patient3)","bfa9c284":"predict_stroke(Patient4)","96e211c9":"predict_stroke(Patient5)","0c8baf61":"predict_stroke(Patient6)","af86bd82":"# My Social Media Accounts\n\n![image.png](attachment:d371a5fb-fb82-47a6-9fa0-085ac8349f73.png)\n\nLinkedin: https:\/\/www.linkedin.com\/in\/kahveciburak\/\n\nTwitter: https:\/\/twitter.com\/burakkahveci_\n\nMail: burakkahveci42@gmail.com","30622dc1":"<a id =\"20\"><\/a><br>\n## Stroke Risk Prediction of Patients","7e46d2eb":"<a id =\"11\"><\/a><br>\n### Support Vector Machine (SVM)","a227127c":"<a id =\"17\"><\/a><br>\n## Evaluation of Results","a9d85525":"<a id =\"9\"><\/a><br>\n### Naive Bayesian (NB)","3d7d3363":"<a id =\"16\"><\/a><br>\n### Gradient Boosting Machines (GBM)","b5212341":"<a id =\"14\"><\/a><br>\n### Classification And Regression Tree (CART)","4b533412":"<a id =\"4\"><\/a><br>\n## Data Preprocessing & Variables types adjustment","4df89294":"<a id =\"1\"><\/a><br>\n## Load and Review Dataset","97b54de5":"# Stroke Risk Prediction and Data Visualization with Machine Learning \n\n### Writer: Burak Kahveci - Bioengineer & Data Scientist \n\n## Introduction \n* A stroke is a serious life-threatening medical condition that happens when the blood supply to part of the brain is cut off.\n\n**Signs and symptoms of stroke include:**\n* **Trouble speaking and understanding what others are saying.** You may experience confusion, slur your words or have difficulty understanding speech.\n* **Problems seeing in one or both eyes.** You may suddenly have blurred or blackened vision in one or both eyes, or you may see double.\n* **Trouble walking.** You may stumble or lose your balance. You may also have sudden dizziness or a loss of coordination.\n\n![image.png](attachment:6abd3b14-0d7d-464b-8e03-6fede693c278.png)\n\n### Reference: Mayo Clinic","f01b92fa":"<a id =\"15\"><\/a><br>\n### Random Forest (RF)","8e703cb3":"<a id =\"19\"><\/a><br>\n## Artificial Patients Profiles for Prection Function","31bcf73f":"<a id =\"2\"><\/a><br>\n### Reading Dataset & Pre-examination","21ca6a63":"<a id =\"6\"><\/a><br>\n## Machine Learning Models","24f3f9de":"<a id =\"5\"><\/a><br>\n## Train & Test Split","88ab7953":"<a id =\"8\"><\/a><br>\n### Logistic Regression (LR)","53b814c2":"In this tutorial, we will use 9 techniques which are logistic regression, navie bayesian, k-nearest neigbors, support vector machine, Non-Linear SVM (Radial Basis Function), artificial neural networks, Classification And Regression Tree (CART), Random Forest (RF), and Gradient Boosting Machines (GBM)","7b6cfcc1":"<a id =\"3\"><\/a><br>\n### Basic Visualization","f8174dff":"<a id =\"10\"><\/a><br>\n### K-nearest neighbors (KNN)","ad420a59":"# Thank you! If you like this tutorial, you can upvote & follow my kaggle profile :)\n\n![image.png](attachment:67fea2e5-d831-44d6-9944-9fdf45d2d8a1.png)!","6a940e83":"## Content\n\n1. [Load and Review Dataset](#1)\n    * [Reading Dataset & Pre-examination](#2)\n    * [Basic Visualization](#3)\n2. [Data Preprocessing & Variables types adjustment](#4)\n3. [Train & Test Split](#5)\n4. [Machine Learning Models](#6)\n    * [General Informations](#7)\n    * [Logistic Regression (LR)](#8)\n    * [Naive Bayesian (NB)](#9)\n    * [K-nearest neighbors (KNN)](#10)\n    * [Support Vector Machine (SVM)](#11)\n    * [Non-Linear SVM (Radial Basis Function)](#12)\n    * [Artificial Neural Networks (ANN)](#13)\n    * [Classification And Regression Tree (CART)](#14)\n    * [Random Forest (RF)](#15)\n    * [Gradient Boosting Machines (GBM)](#16)\n5. [Evaluation of Results](#17)\n6. [Creation of Prediction Funtion](#18)\n7. [Artificial Patients Profiles for Prection Function](#19)\n8. [Stroke Risk Prediction of Patients](#20)","a3b0b6a6":"<a id =\"7\"><\/a><br>\n### General Informations","08e7963c":"<a id =\"13\"><\/a><br>\n### Artificial Neural Networks (ANN)","cebc4af1":"<a id =\"12\"><\/a><br>\n### Non-Linear SVM (Radial Basis Function)","f5f31b3b":"<a id =\"18\"><\/a><br>\n## Creation of Prediction Funtion"}}