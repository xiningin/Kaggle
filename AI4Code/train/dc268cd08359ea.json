{"cell_type":{"fdd5bcf0":"code","3622f1ad":"code","11ecb49b":"code","d3bea0a5":"code","af92a770":"code","81541924":"code","ff0ed8c3":"code","572aaf1a":"code","1a531e3f":"code","7649165b":"code","b8bc7640":"code","c0cc75e0":"code","56e777f1":"code","23928414":"code","c3999c2f":"code","670a7228":"code","98ee4b48":"markdown","842fcc12":"markdown","42207ed2":"markdown","7d90a722":"markdown","948fd48a":"markdown","27ae324f":"markdown"},"source":{"fdd5bcf0":"#import libraries\nimport pandas as pd\nimport re\nimport numpy as np\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS as stop\nfrom spacy.lemmatizer import Lemmatizer, Lookups\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer ","3622f1ad":"nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner', 'textcat'])","11ecb49b":"sents= pd.read_csv(\"..\/input\/harryporter-sentence\/harry.csv\",encoding=\"utf-8\")","d3bea0a5":"sents.head()","af92a770":"def preprocess_text_en(text_col: pd.Series,\n                    stem: bool = False,\n                    lem: bool = True,\n                    ) -> pd.Series:\n   \n    # Remove NAs from the dataset.\n    text_col = text_col.dropna()\n\n    # Lowercase.\n    text_col = text_col.str.lower()\n\n    # Remove non-alpha characters.\n    text_col = text_col.apply(lambda s: re.sub('[^a-zA-Z]+', ' ', s))\n\n    # Adjust spacing.\n    text_col = text_col.str.strip()\n    text_col = text_col.replace('', np.nan)\n    text_col = text_col.dropna().astype(str)\n\n    text_col = text_col.str.replace(r'\\s+', ' ')\n\n    # Remove stopwords.\n    text_col = text_col.apply(lambda desc: ' '.join(\n        [item for item in desc.split(' ') if item not in stop]))\n    text_col = text_col.replace('', np.nan)\n    text_col = text_col.dropna()\n\n    if lem:\n        text_col = pd.Series(nlp.pipe(text_col), index=text_col.index)\n        for ind, val in text_col.iteritems():\n            end_str = []\n            for item in val:\n                if item.lemma_ == '-PRON-':\n                    #print(val,item.lemma_,item)\n                    next\n                elif len(item.text) > 2 :\n                    #print(val,item.lemma_,item)\n                    end_str.append(item.lemma_)\n                else:\n                    end_str.append(item.text)\n\n            text_col[ind] = end_str\n\n        text_col = text_col.apply(\n            lambda desc: ' '.join([item for item in desc]))\n    elif stem:\n        ps = PorterStemmer()\n        text_col = text_col.apply(lambda desc: word_tokenize(desc))\n        text_col = text_col.apply(\n            lambda desc: [ps.stem(item) for item in desc])\n        text_col = text_col.apply(\n            lambda desc: ' '.join([item for item in desc]))\n        # TODO: include acronym replacement for stemming\n\n    return text_col","81541924":"def vectorize_text(text_col: pd.Series,\n                   key_bigrams: bool,\n                   vec_type: str = 'Tfidf',\n                   **kwargs):\n    #print(text_col)\n    text_raw = text_col\n\n    if vec_type == 'Tfidf':\n        tfidf_vec = TfidfVectorizer(**kwargs)\n        vectorized = tfidf_vec.fit_transform(text_raw)\n        vectorizer_obj = tfidf_vec\n        \n    return vectorized, vectorizer_obj","ff0ed8c3":"def vectorize_new_text(text_col: pd.Series,\n                         vectorizer_obj):\n   \n    # Get raw values from pandas series\n    text_raw = text_col\n\n    # Apply proper vectorization\n    vectorized = vectorizer_obj.transform(text_raw)\n\n    # Return vectorized object\n    return vectorized","572aaf1a":"from sklearn.metrics import pairwise_distances\ndef distance_calculation(new_nc_mat,\n                         old_nc_mat,\n                         metric: str ):\n    \n    if metric == 'jaccard':\n        old_nc_mat = old_nc_mat.toarray().astype(bool)\n        new_nc_mat = new_nc_mat.toarray().astype(bool)\n\n    similarity = 1 - pairwise_distances(new_nc_mat, old_nc_mat, metric=metric)\n    return similarity","1a531e3f":"def get_similar_matches(old: pd.Series,\n                        new: pd.Series,\n                        metric: str,\n                        num_matches: int\n                        ) -> pd.DataFrame:\n        key_cols=['id','text']\n        kwargs = {'min_df': 2}\n        kwargs['ngram_range'] = (1, 2)\n        if metric in ['cosine', 'jaccard']:\n            old_vec, vec_obj = vectorize_text(text_col=old,\n                                              vec_type='Tfidf',\n                                              key_bigrams=False,\n                                              **kwargs)\n            new_vec = vectorize_new_text(text_col=new,\n                                         vectorizer_obj=vec_obj)\n\n            similarity = distance_calculation(new_nc_mat=new_vec,\n                                              old_nc_mat=old_vec,\n                                              metric=metric)\n            similarity = pd.Series(similarity.flatten())\n       \n        top_matches = pd.DataFrame(old.reset_index())\n        top_matches.rename(\n            columns={'index': 'orig_index_old', 0: 'preprocessed_desc_old'},\n            inplace=True)\n        top_matches[metric] = similarity\n        \n        top_matches = top_matches.iloc[0:num_matches]\n        top_matches = top_matches.join(\n                        sents[key_cols],\n                        how='left',\n                        on='orig_index_old')\n       \n \n \n        return top_matches","7649165b":"def run_example(old: pd.Series,\n                new: pd.Series,\n                num_matches: int) -> pd.DataFrame:\n   \n    \n   \n    #cosine\n    cosine = get_similar_matches(old, new, 'cosine', num_matches)\n    #print(cosine)\n    # jaccard\n    jaccard = get_similar_matches(old, new, 'jaccard', num_matches)\n    #print(jaccard)\n    \n    distances = {'cosine': cosine,\n                'jaccard':jaccard}\n   \n    return distances","b8bc7640":"List=['1140']\nfor i in List:\n    \n    x = i\n    nc_inv = x\n    new_nc = sents[sents['id'] == int(nc_inv)]['text'][0:1]\n    old_ncs = sents[sents['id'] != int(nc_inv)]['text']\n    old_ncs = old_ncs.astype(str) \n    old_preprocessed = preprocess_text_en(text_col=old_ncs, stem=False, lem=True)\n    new_preprocessed = preprocess_text_en(text_col=new_nc, stem=False, lem=True)\n    example = run_example(old_preprocessed, new_preprocessed, 999)","c0cc75e0":"sent= sents[sents['id'] == int(1140)]['text'][0:1]\nprint(\"The reference sentence is:\\n\",sent)","56e777f1":"dfj=example['jaccard'].sort_values(by=['jaccard'],ascending=False)","23928414":"#Top 5 most simialr documents wrt referece document by Jaccard\ndfj['text'].head(5)","c3999c2f":"dfc=example['cosine'].sort_values(by=['cosine'],ascending=False)","670a7228":"#Top 5 most simialr documents wrt referece document by Cosine\ndfc['text'].head(5)","98ee4b48":"![](https:\/\/miro.medium.com\/max\/1400\/1*oF1QyMamN5jXCXfffSRrqA.png)","842fcc12":"![](https:\/\/miro.medium.com\/max\/533\/1*hub04IikybZIBkSEcEOtGA.png)","42207ed2":"![](https:\/\/studymachinelearning.com\/wp-content\/uploads\/2020\/04\/jaccard.png)","7d90a722":"# This notebook is about finding the closest document among various document. Its an unsupervised learning.","948fd48a":" We commonly spend a lot of time looking for a specific piece of information in a large document. And we commonly find if using CTRL + F. The proverbial Google-fu, the art of effectively searching for information on google is a valuable skill in a 21st-century workplace. All of humanity\u2019s knowledge is available to us, it is a matter of asking the right question, and knowing how to skim through results to find the relevant answer.\n Our brains perform a semantic search, where we review the results and find sentences that are similar to our search query. This is especially true in finance and legal professions as documents get lengthy and we have to resort to searching many keywords to find the right sentence or a passage. To this day, the cumulative human effort spent on discovery is staggering.","27ae324f":"The formula of Cosine and Jaccard"}}