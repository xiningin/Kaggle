{"cell_type":{"af846043":"code","6e67393a":"code","0ded105c":"code","484f848b":"code","6efd8a87":"code","2de5d17d":"code","0ad75f41":"code","1da86ace":"code","e63a33b2":"code","39f45752":"code","abd6da27":"code","9334e86d":"code","46ebef52":"code","3f76318c":"code","fb4cc1af":"code","35337901":"code","963716fa":"code","74e8b5ce":"code","42e3adb3":"code","a7fc5159":"code","1b390d65":"code","e3da5d7b":"code","f4fc8fd3":"code","1a0f06d3":"code","e608090a":"code","e437dedf":"markdown","34ddec57":"markdown","2243cfe3":"markdown","b5604d96":"markdown","8413f705":"markdown"},"source":{"af846043":"! mkdir JNLPBA","6e67393a":"! wget https:\/\/raw.githubusercontent.com\/allenai\/scibert\/master\/data\/ner\/JNLPBA\/train.txt -O JNLPBA\/train.raw","0ded105c":"! wget https:\/\/raw.githubusercontent.com\/allenai\/scibert\/master\/data\/ner\/JNLPBA\/test.txt -O JNLPBA\/test.raw","484f848b":"! wget https:\/\/raw.githubusercontent.com\/allenai\/scibert\/master\/data\/ner\/JNLPBA\/dev.txt -O JNLPBA\/dev.raw","6efd8a87":"! ls JNLPBA","2de5d17d":"! cat JNLPBA\/*.raw | cut -f 4 | sort | grep -v \"^$\" | uniq > JNLPBA\/labels.txt","0ad75f41":"! cat JNLPBA\/labels.txt","1da86ace":"! cat JNLPBA\/train.raw | cut -f 1,4 | tr '\\t' ' ' > JNLPBA\/train.txt.tmp\n! cat JNLPBA\/test.raw | cut  -f 1,4 | tr '\\t' ' ' > JNLPBA\/test.txt.tmp\n! cat JNLPBA\/dev.raw | cut -f 1,4 | tr '\\t' ' ' > JNLPBA\/dev.txt.tmp","e63a33b2":"! head JNLPBA\/train.txt.tmp","39f45752":"! head JNLPBA\/test.txt.tmp","abd6da27":"! head JNLPBA\/dev.txt.tmp","9334e86d":"# ! git clone https:\/\/github.com\/huggingface\/transformers\n! git clone https:\/\/github.com\/AMR-KELEG\/transformers.git","46ebef52":"! pip install transformers","3f76318c":"! cd transformers\/examples\/ner\/","fb4cc1af":"! wget \"https:\/\/raw.githubusercontent.com\/stefan-it\/fine-tuned-berts-seq\/master\/scripts\/preprocess.py\"","35337901":"%env MAX_LENGTH=128\n%env BERT_MODEL=mrm8488\/scibert_scivocab-finetuned-CORD19","963716fa":"! python preprocess.py \/kaggle\/working\/JNLPBA\/train.txt.tmp $BERT_MODEL $MAX_LENGTH > \/kaggle\/working\/JNLPBA\/train.txt\n! python preprocess.py \/kaggle\/working\/JNLPBA\/test.txt.tmp $BERT_MODEL $MAX_LENGTH > \/kaggle\/working\/JNLPBA\/test.txt\n! python preprocess.py \/kaggle\/working\/JNLPBA\/dev.txt.tmp $BERT_MODEL $MAX_LENGTH > \/kaggle\/working\/JNLPBA\/dev.txt","74e8b5ce":"! head \/kaggle\/working\/JNLPBA\/train.txt","42e3adb3":"%env OUTPUT_DIR=roberta\n%env BATCH_SIZE=32\n%env NUM_EPOCHS=2\n%env SAVE_STEPS=750\n%env SEED=1","a7fc5159":"! pip install -r \/kaggle\/working\/transformers\/examples\/requirements.txt","1b390d65":"! python \/kaggle\/working\/transformers\/examples\/ner\/run_ner.py --data_dir \/kaggle\/working\/JNLPBA\/ \\\n--model_type roberta \\\n--labels \/kaggle\/working\/JNLPBA\/labels.txt \\\n--model_name_or_path $BERT_MODEL \\\n--output_dir $OUTPUT_DIR \\\n--max_seq_length  $MAX_LENGTH \\\n--num_train_epochs $NUM_EPOCHS \\\n--per_gpu_train_batch_size $BATCH_SIZE \\\n--save_steps $SAVE_STEPS \\\n--seed $SEED \\\n--do_train \\\n--do_eval \\\n--do_predict \\\n--overwrite_output_dir \\\n--evaluate_during_training \\\n--logging_steps 4000","e3da5d7b":"from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForTokenClassification\n\nmodel = AutoModelForTokenClassification.from_pretrained('roberta')\ntokenizer = AutoTokenizer.from_pretrained('mrm8488\/scibert_scivocab-finetuned-CORD19')\n","f4fc8fd3":"import torch\n\nwith open('JNLPBA\/labels.txt', 'r') as f:\n    labels = [l.strip() for l in f.readlines()]\n\nlabel_id_to_label = {i:l for i, l in enumerate(labels)}\n\ndef tokenize(sample):\n    return tokenizer.encode(' '.join(['CLS', sample, 'SEP']))\n\ndef get_prediction(sample):\n    tokens = tokenize(sample)\n    attention_mask = [1] * len(tokens)\n    predictions = model.forward(input_ids=torch.LongTensor([tokens]),\n                                attention_mask=torch.LongTensor([attention_mask]))[0].argmax(axis=2).tolist()[0]\n    return [label_id_to_label[i] for i in predictions]","1a0f06d3":"sample = 'IL-2 gene expression and NF-kappa B activation through CD28 requires reactive oxygen production by 5-lipoxygenase.'\npredictions = get_prediction(sample)\nfor token, pred in zip(tokenizer.tokenize(sample), predictions):\n    print(token, pred)","e608090a":"print('''IL-2 B-DNA\ngene I-DNA\nexpression O\nand O\nNF-kappa B-protein\nB I-protein\nactivation O\nthrough O\nCD28 B-protein\nrequires O\nreactive O\noxygen O\nproduction O\nby O\n5-lipoxygenase B-protein\n. O''')","e437dedf":"## Download the JNLPBA dataset\n## TODO: Check the release dates of each dataset and investigate the entities in each dataset\n- https:\/\/www.aclweb.org\/anthology\/W04-1213.pdf\n","34ddec57":"## Conclusion\n- The sample shows that the model isn't generating good enough results.\n- Additionally, the way BERT is tokenizing the sample introduces a tricky problem of alligning the predictions to the tokens.","2243cfe3":"# Test the model for single sample","b5604d96":"# Bio Named Entity Recognition\n- Named Entity Recognition is the task of determining a set of entities in the sentence.\n- In other words, each word is classified to one of the predefined entities.\n- BERT models can be used to solve the NER task by adding a softmax layer after the last embedding layer.\nSince the embeddings generated from BERT already hold information about the realtion between the word and the other words of the sentence and thus are suitable for the NER task since the entities can hugely depend on the context.\n\n## TLDR\nThe model isn't good enough to be used till now. Is BERT actually good enough?","8413f705":"# Fine-tune the model\nhttps:\/\/huggingface.co\/mrm8488\/scibert_scivocab-finetuned-CORD19"}}