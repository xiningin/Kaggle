{"cell_type":{"dd207f93":"code","fcbdb664":"code","9c58adfd":"code","bb3cc2cb":"code","a368f26f":"code","6e2e79e5":"code","6180831a":"code","c0262c97":"code","bbb465ad":"code","6b340878":"code","2275361b":"code","2c2ae573":"code","40baca73":"code","715aabbb":"code","0a3c91fc":"code","737856cd":"code","ee2aa99c":"code","b6ed63fa":"code","954c4cd6":"code","dd223127":"code","8082a0b4":"code","650ee2d9":"code","0fc81947":"code","6df93ecc":"code","bf6f1d7a":"markdown"},"source":{"dd207f93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Visualisation Library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fcbdb664":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain.shape, test.shape","9c58adfd":"train=train.iloc[:,1:302]\ntrain.head()","bb3cc2cb":"sns.set_style('white')\nsns.set_color_codes(palette='deep')\nf,ax= plt.subplots(figsize = (8,7))\nsns.distplot(train['target'])","a368f26f":"features=train.iloc[:,1:302]\nfeatures.shape\nlabel=train.iloc[:,0]\nfeatures.head()\nlabel.head()\n","6e2e79e5":"plt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(features.columns)[1:28]):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(features[col])\n    plt.title(col)","6180831a":"plt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(features.columns)[28:56]):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(features[col])\n    plt.title(col)","c0262c97":"plt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(features.columns)[56:84]):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(features[col])\n    plt.title(col)","bbb465ad":"plt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(features.columns)[112:140]):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(features[col])\n    plt.title(col)","6b340878":"plt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(features.columns)[140:168]):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(features[col])\n    plt.title(col)","2275361b":"plt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(features.columns)[168:199]):\n    plt.subplot(7, 5, i + 1)\n    plt.hist(features[col])\n    plt.title(col)","2c2ae573":"a=[]\nfor f in features:\n    a.append(features.iloc[:,int(f)].isnull().sum())\nprint(a)","40baca73":"\n# for j in range(0,300):\n#     Q3 = features.iloc[:,j].quantile(0.75)\n#     Q1 = features.iloc[:,j].quantile(0.25) \n#     iqr=Q3-Q1\n#     upperBound=Q3 + 1.5*iqr\n#     lowerBound=Q1 - 1.5*iqr\n#     for i in range(0,250) :\n#         if (features.iloc[i,j] > upperBound ):\n#             print(features.iloc[i,j],upperBound)\n#             features[i,j]=features.iloc[:,j].mean()\n\n#         if (features.iloc[i,j] < lowerBound):\n#             print(features.iloc[i,j],lowerBound)\n#             features[i,j]=features.iloc[:,j].mean()\n\n\n  ","715aabbb":"corr = train.corr()\nc=corr.iloc[:,0].sort_values(ascending=False)\nc.iloc[120:135]","0a3c91fc":"from sklearn.model_selection import StratifiedKFold,RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nX_train = features.drop(['75','195'],axis=1)\nX=features.drop(['75','195'],axis=1)\n\ny_train = label\nX_test = test.drop(['id','75','195'], axis=1)\n\nscaler = StandardScaler()\nSX_train = scaler.fit_transform(X_train)\nSX_test = scaler.transform(X_test)","737856cd":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nlr = LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nmodel_lr=lr.fit(SX_train,y_train)\nscores=cross_val_score(lr, SX_train, y_train,cv=3)\nprint(\"Accuracy: %0.3f (std %0.3f)\" % (scores.mean(), scores.std() * 2))\n","ee2aa99c":"import xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBClassifier\nscaler = StandardScaler()\nSX_train = scaler.fit_transform(X_train)\nSX_test = scaler.transform(X_test)\ndtrain = xgb.DMatrix(SX_train, label=y_train)\nfolds = StratifiedKFold(n_splits=20, shuffle=True, random_state=42)\nparams = {'learning_rate':[0.1,0.01,0.02,0.03,0.2,0.3],\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [1,2,3, 4, 5]\n        }\nxgb = XGBClassifier(n_estimators=600, objective='binary:logistic')\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params,n_iter=5,scoring='roc_auc', n_jobs=4, \n                                   cv=folds.split(SX_train,y_train), \n                                   random_state=1001)\nrandom_search.fit(SX_train, y_train)\nprint('Best score: {}'.format(random_search.best_score_))\nprint('Best parameters: {}'.format(random_search.best_params_))","b6ed63fa":"import xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBClassifier\nfor x in X_train.columns:\n    print(x)\nprint(y_train)\nscaler = StandardScaler()\nXSX_train = scaler.fit_transform(X_train)\nXSX_test = scaler.transform(X_test)\ndtrain = xgb.DMatrix(SX_train, label=y_train)\nfolds = StratifiedKFold(n_splits=20, shuffle=True, random_state=42)\nparams = {'learning_rate':[0.1,0.01,0.02,0.03,0.2,0.3],\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [1,2,3, 4, 5]\n        }\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=400, max_depth=2,\n                        min_child_weight=10, gamma=1, subsample=0.8, colsample_bytree=0.6,\n                        objective='binary:logistic',  seed=27)\nmodel_xgb=xgb.fit(XSX_train,y_train)","954c4cd6":"fea_imp = pd.DataFrame({'imp': model_xgb.feature_importances_})\nprint(fea_imp.sort_values(by='imp',ascending=True))","dd223127":"import eli5\neli5.show_weights(model_xgb)","8082a0b4":"top_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(model_xgb).feature if 'BIAS' not in i]\nprint(top_features)\nXX_train = train[top_features]\nX_test=test.drop(['id'], axis=1)\nXX_test = X_test[top_features]\nscaler = StandardScaler()\nXSX_train = scaler.fit_transform(XX_train)\nXSX_test = scaler.transform(XX_test)\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=400, max_depth=2,\n                        min_child_weight=10, gamma=1, subsample=0.8, colsample_bytree=0.6,\n                        objective='binary:logistic',  seed=27)\nmodel_xgb=xgb.fit(XSX_train,y_train)\nscores=cross_val_score(model_xgb, XSX_train, y_train,cv=5)\nprint(\"Accuracy: %0.3f (std %0.3f)\" % (scores.mean(), scores.std() * 2))","650ee2d9":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\nLogisticRegression(solver='liblinear', max_iter=10000)\nfolds = StratifiedKFold(n_splits=20, shuffle=True, random_state=42)\nparameter_grid = {'class_weight' : ['balanced', None],\n                  'penalty' : ['l2', 'l1'],\n                  'C' : [0.001, 0.01, 0.08, 0.1, 0.15, 1.0, 10.0, 100.0],\n                 }\n\ngrid_search = GridSearchCV(lr, param_grid=parameter_grid, cv=folds, scoring='roc_auc')\ngrid_search.fit(XSX_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))","0fc81947":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nlr = LogisticRegression(class_weight=None, penalty='l1', C=0.08, solver='liblinear')\nmodel_lr=lr.fit(XSX_train,y_train)\nscores=cross_val_score(lr, XSX_train, y_train,cv=5)\nprint(\"Accuracy: %0.3f (std %0.3f)\" % (scores.mean(), scores.std() * 2))","6df93ecc":"pred=model_lr.predict(XSX_test)\nsub=pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub[\"target\"]=pred\nsub.to_csv(\"output\",index=False)","bf6f1d7a":"NO Dependencies could be seen through the correlation matrix now have to search for the normality of response variable"}}