{"cell_type":{"1971fdbb":"code","80bd596d":"code","57f7a366":"code","ba6f165c":"code","1fca1af9":"code","2d4d1b75":"code","67174a2c":"code","514bd73b":"code","456f8764":"code","b3bfe2a3":"code","dc675d16":"code","b6f779be":"code","7bbdf6ce":"code","95c21628":"code","57608daf":"code","612b8a73":"code","dfcdc537":"code","a36c2086":"code","bd89dac4":"code","827597cf":"code","ab1ec526":"code","b2713327":"code","9c5b30b1":"code","555a40b3":"code","0317c5ed":"code","4763b469":"code","ec6985b4":"code","4d7a3ffc":"code","cb5f8449":"code","55aa0c11":"code","c4290693":"code","97da2e74":"code","3ed18134":"code","50459d50":"markdown","4f67e519":"markdown","68f9a87f":"markdown","a0377222":"markdown"},"source":{"1971fdbb":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","80bd596d":"df=pd.read_csv(\"\/kaggle\/input\/cleaned-tayara-cars\/cleaned_tayara_cars2.0.csv\")","57f7a366":"print(\"nb of brands \"+str(df[\"brand\"].nunique()))\nprint(\"nb of fuel types \"+str(df[\"Fuel_Type\"].nunique()))\nprint(\"nb of cities \"+str(df[\"city\"].nunique()))\nprint(\"nb of car models \"+str(df[\"model\"].nunique()))","ba6f165c":"df.drop([\"price\"],axis=1,inplace=True)\ndf.drop([\"Unnamed: 0\"],axis=1,inplace=True)\ndf.drop([\"Kms_Driven\"],axis=1,inplace=True)\ndf.drop([\"year\"],axis=1,inplace=True)\ndf.head()","1fca1af9":"#encoding model frequency\nmodel_frequency  = df.groupby('model').size()\/len(df)\ndf.loc[:,'model_freq_encode'] = df['model'].map(model_frequency)\n\ndf.drop([\"model\"],axis=1,inplace=True)\n\ndf.head()","2d4d1b75":"#encoding brand frequency\nbrand_frequency  = df.groupby('brand').size()\/len(df)\ndf.loc[:,'brand_freq_encode'] = df['brand'].map(brand_frequency)\ndf.drop([\"brand\"],axis=1,inplace=True)\n\ndf.head()","67174a2c":"#encoding city frequency\ncity_frequency  = df.groupby('city').size()\/len(df)\ndf.loc[:,'city_freq_encode'] = df['city'].map(city_frequency)\ndf.drop([\"city\"],axis=1,inplace=True)\n\ndf.head()","514bd73b":"#encoding fuel frequency\nfuel_frequency  = df.groupby('Fuel_Type').size()\/len(df)\ndf.loc[:,'fuel_freq_encode'] = df['Fuel_Type'].map(fuel_frequency)\n\n\ndf.drop([\"Fuel_Type\"],axis=1,inplace=True)\ndf.head()\n","456f8764":"#encoding horsepow frequency\nfuel_frequency  = df.groupby('Horse_pow').size()\/len(df)\ndf.loc[:,'horse_freq_encode'] = df['Horse_pow'].map(fuel_frequency)\n\n\ndf.drop([\"Horse_pow\"],axis=1,inplace=True)\ndf.head()","b3bfe2a3":"df.columns.tolist()","dc675d16":"L= list(df.columns.values)\ndel L[1]","b6f779be":"X=df[L]\nX","7bbdf6ce":"y=df[['log_price']]\ny","95c21628":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)","57608daf":"#standardization scaler \nfrom sklearn.preprocessing import StandardScaler\ns_scaler = StandardScaler()\nX_train = s_scaler.fit_transform(X_train.astype(np.float))\nX_test = s_scaler.transform(X_test.astype(np.float))","612b8a73":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()  \nregressor.fit(X_train, y_train)\n\n#predicting the test set result\ny_pred = regressor.predict(X_test)\n\n\n#evaluate the results\nfrom sklearn import metrics\n\n\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))  \nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n","dfcdc537":"sns.distplot(y_test-y_pred)","a36c2086":"from sklearn.tree import DecisionTreeRegressor\n\ndtr = DecisionTreeRegressor()\n\ndtr.fit(X_train, y_train)            \ndtr_y_pred=dtr.predict(X_test)    \nprint('MAE:', metrics.mean_absolute_error(y_test, dtr_y_pred))  \nprint('MSE:', metrics.mean_squared_error(y_test, dtr_y_pred))  \nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dtr_y_pred)))","bd89dac4":"y_test=y_test.to_numpy()\ny_test=np.squeeze(y_test)\nsns.distplot(y_test-dtr_y_pred)\nplt.xlabel(\"Prediction Error\")\n_ = plt.ylabel(\"Count\")","827597cf":"from sklearn.ensemble import RandomForestRegressor\n#use the random grid to search for best hyperparameters\n#first create the best model to tune\nrf = RandomForestRegressor()","ab1ec526":"import numpy as np\n#HyperParameters for Randomized Search CV\n#note for self : Randomized searchCV is much faster than Grid SearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","b2713327":"random_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)\n\n","9c5b30b1":"from sklearn.model_selection import RandomizedSearchCV\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","555a40b3":"rf_random.fit(X_train,y_train.values.ravel())","0317c5ed":"rf_random.best_params_","4763b469":"predictions = rf_random.predict(X_test)","ec6985b4":"y_test=y_test.to_numpy()\ny_test=np.squeeze(y_test)\n\nsns.distplot(y_test-predictions)\nplt.xlabel(\"Prediction Error\")\n_ = plt.ylabel(\"Count\")","4d7a3ffc":"print('MAE:', metrics.mean_absolute_error(y_test, predictions))  \nprint('MSE:', metrics.mean_squared_error(y_test, predictions))  \nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))","cb5f8449":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\n","55aa0c11":"net = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                       colsample_bynode=1, colsample_bytree=1, gamma=0,\n                       importance_type='gain', learning_rate=0.08, max_delta_step=0,\n                       max_depth=7, min_child_weight=1, missing=None, n_estimators=100,\n                       n_jobs=1, nthread=None, random_state=0,\n                       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n                       silent=None, subsample=0.75, verbosity=1, objective='reg:squarederror')","c4290693":"net.fit(X_train, y_train)\nXGB_y_pred=net.predict(X_test)","97da2e74":"print('MAE:', metrics.mean_absolute_error(y_test, XGB_y_pred))  \nprint('MSE:', metrics.mean_squared_error(y_test, XGB_y_pred))  \nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, XGB_y_pred)))","3ed18134":"sns.distplot(y_test-XGB_y_pred)\nplt.xlabel(\"Prediction Error\")\n_ = plt.ylabel(\"Count\")","50459d50":"#  Linear Regression\n","4f67e519":"# XGBoost","68f9a87f":"# Decision Tree Regressor","a0377222":"# Random Forest Regressor"}}