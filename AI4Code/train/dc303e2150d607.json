{"cell_type":{"db43c966":"code","58a1c7a9":"code","4af36374":"code","b46daa4a":"code","b8c4a6f5":"code","ae357843":"code","e414986a":"code","4532256a":"code","a4d28dfc":"code","95c5ff41":"code","36627a02":"code","5334fe51":"code","0daa1a9c":"code","fefa949f":"code","c6356ba1":"code","3024d987":"code","7417bede":"code","bb280d97":"code","acc05d89":"code","b3b130ce":"code","f561ae28":"code","bd9a2c01":"code","2355b977":"code","e175db9a":"code","d3b63566":"code","2ccb623d":"code","fb71acd0":"code","526e52b8":"code","8343344f":"code","e332bd5b":"code","812d64bc":"code","7c70ffe3":"code","4e061443":"code","a2492133":"code","492d2fc2":"code","c8ad90b4":"code","eef4f59e":"code","4772900e":"code","62cce6a1":"code","87eea5ac":"code","836a7a67":"code","cfceceee":"code","e9ae8521":"code","705a85e0":"code","ab85dc3b":"code","1f076f29":"code","9e6a704a":"code","042ba7a1":"code","a5c2d407":"code","414f9e3b":"code","a750b8e9":"code","c1826816":"markdown","f18a2730":"markdown","d614c97e":"markdown","efbd85d6":"markdown","586b3236":"markdown","48b03ace":"markdown","7c80baae":"markdown","6d08e1f6":"markdown","feca828f":"markdown","33362ef0":"markdown","ce045d9a":"markdown","98114c94":"markdown","72067f38":"markdown","170ad38d":"markdown","229b3d85":"markdown","3a5d08ee":"markdown","458e9cff":"markdown","8affb74e":"markdown","d799dd5c":"markdown","5e4c20bd":"markdown","ed1c35fd":"markdown","2d11766e":"markdown","7092af8b":"markdown","02a284e7":"markdown","3be40207":"markdown","7e791353":"markdown","d0ca0cd3":"markdown","0f399993":"markdown","13f574b3":"markdown","c6ca5801":"markdown","6f36c2ff":"markdown","62551c54":"markdown","c022ae5f":"markdown","c79c8217":"markdown","e778cb06":"markdown","9e717be7":"markdown","80669591":"markdown","2c18f58d":"markdown","6d5c7d53":"markdown","e12f6a0e":"markdown"},"source":{"db43c966":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\n# install tweet-preprocessor to clean tweets\n# source: https:\/\/towardsdatascience.com\/twitter-sentiment-analysis-nlp-text-analytics-b7b296d71fce\n!pip install tweet-preprocessor\n\nimport re\nimport preprocessor as p\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","58a1c7a9":"# read the test and train datasets using Pandas\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","4af36374":"# my personal reusable function for detecting missing data\ndef missing_value_describe(data):\n    # check missing values in the data\n    missing_value_stats = (data.isnull().sum() \/ len(data)*100)\n    missing_value_col_count = sum(missing_value_stats > 0)\n    missing_value_stats = missing_value_stats.sort_values(ascending=False)[:missing_value_col_count]\n    print(\"Number of columns with missing values:\", missing_value_col_count)\n    if missing_value_col_count != 0:\n        # print out column names with missing value percentage\n        print(\"\\nMissing percentage (desceding):\")\n        print(missing_value_stats)\n    else:\n        print(\"No missing data!!!\")","b46daa4a":"train.info()\nmissing_value_describe(train)","b8c4a6f5":"test.info()\nmissing_value_describe(test)","ae357843":"# NLP related packages\nimport string\nimport spacy\nfrom nltk import word_tokenize\nfrom gensim.parsing.preprocessing import remove_stopwords\n!python -m spacy download en_core_web_sm\n\n# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\nnlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])","e414986a":"# drop id, keyword, and location columns for train and test datasets\ncols_to_drop = [\"id\", \"keyword\", \"location\"]\ntrain = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)\ntrain.head()","4532256a":"def remove_link(text):\n    return re.compile(r'https?:\/\/\\S+|www\\.\\S+').sub(r'',text)\n\ndef tweet_processing(text):\n    # see above markdown cell for explanation\n    sentence = p.clean(remove_link(text)).\\\n               strip().lower().translate(str.maketrans('', '', string.punctuation))\n    \n    # parse sentence with spacy lemmatization and remove stopwords\n    doc = remove_stopwords(\" \".join([token.lemma_ for token in nlp(sentence)]))\n    \n    # remove single letter number or letter\n    doc = \" \".join([word for word in doc.split() if len(word)>1])\n    \n    # remove -PRON- from spacy output\n    return \" \".join([word for word in doc.split() if word != \"-PRON-\"])","a4d28dfc":"train[\"text_processed\"] = train.text.apply(tweet_processing)\ntest[\"text_processed\"] = test.text.apply(tweet_processing)","95c5ff41":"train","36627a02":"test","5334fe51":"def helper(text):\n    if \"UTC]?\" in text:\n        return text\nprint(\"training data containing timestamp\")\nprint(train.text.apply(helper).value_counts())\n\nprint(\"\\ntesting data containing timestamp\")\nprint(test.text.apply(helper).value_counts())","0daa1a9c":"train.query(\"text == 'M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http:\/\/t.co\/zDtoyd8EbJ'\")","fefa949f":"train.query(\"text == 'M1.57 [01:11 UTC]?3km NNW of Lake Henshaw California. http:\/\/t.co\/f9KQksoSw3'\")","c6356ba1":"train = train.drop(7610).reset_index(drop=True) # drop 7610 and reset index","3024d987":"# \"utc\" and \"km\" might be important features; I manually added it back\ntrain.iloc[3053, 2] = \"utc km \" + train.iloc[3053, 2].split(\"utc3 \")[1] \ntrain.iloc[7140, 2] = \"utc km \" + train.iloc[7140, 2].split(\"utc5 \")[1]","7417bede":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","bb280d97":"# preprocessor to remove digits in string\ndef remove_numeric_string(text):\n    return re.sub(r'\\d+', '', text)\n\n# create bag of word representation of the text data\nbow_vectorizer = CountVectorizer(dtype=np.float32,\n                                 preprocessor=remove_numeric_string, # remove numeric term\n                                 strip_accents='unicode',\n                                 tokenizer=word_tokenize,\n                                 ngram_range=(1,1), # keep the unigram\n                                 analyzer='word', # feature should be made of word n-gram\n                                 min_df=10, # ignore terms appeared less than 10 times\n                                 max_df=0.75) # ignore terms appeared more than 75% of the tweets available\nbow_vector = bow_vectorizer.fit_transform(train['text_processed'])\nbow_features = pd.DataFrame(bow_vector.toarray(), \n             columns=sorted(bow_vectorizer.vocabulary_))\nbow_features","acc05d89":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt","b3b130ce":"# create index arrays for filtering rows based on the target\ndisaster_text_row_index = train.query(\"target == 1\").index\nnon_disaster_text_row_index = train.query(\"target == 0\").index","f561ae28":"# create a dictionary of term frequency\ndef create_term_frequency_count(term_features):\n    word_count = term_features.sum()\n    return dict(zip(word_count.index, word_count.values))\n\n# function for create wordcloud from term frequency dictionary\ndef create_word_cloud(category_count, title=\"\"):\n    wordcloud = WordCloud(background_color=\"white\",\n                          random_state=1, # reproducible wordcloud\n                          width=1600, \n                          height=800).generate_from_frequencies(category_count)\n    plt.figure(figsize=(10,10));\n    plt.imshow(wordcloud)\n    plt.title(label=title, fontdict={\"fontsize\": 30})\n    plt.axis(\"off\")\n    plt.show();","bd9a2c01":"# create term frequency dictionary and use it for plotting the word cloud\ncategory_count = create_term_frequency_count(bow_features.iloc[disaster_text_row_index])\ncreate_word_cloud(category_count, title=\"Disaster Tweet Wordcloud\")\n\n\ncategory_count = create_term_frequency_count(bow_features.iloc[non_disaster_text_row_index])\ncreate_word_cloud(category_count, title=\"Non-Disaster Tweet Wordcloud\")","2355b977":"# data\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\n\n# Classifiers\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom lightgbm.sklearn import LGBMClassifier\n\n# classifiers \/ models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    f1_score,\n    make_scorer,\n    plot_confusion_matrix,\n)\n\n# other\nfrom sklearn.model_selection import (\n    RandomizedSearchCV,\n    cross_val_score,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz\nfrom sklearn.metrics import f1_score, make_scorer, recall_score","e175db9a":"# 80-20 train valid split; only run it once\ntrain_df, valid_df = train_test_split(train, test_size=0.2, random_state=2021)\nX_train, y_train = train_df[\"text_processed\"], train_df[\"target\"]\nX_valid, y_valid = valid_df[\"text_processed\"], valid_df[\"target\"]","d3b63566":"# define preprocessor\ntext_features = \"text_preprocessed\"\ntarget_column = 'target'\n\n# function for storing the validation metric scores\ndef mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n    scores = cross_validate(model, X_train, y_train, \n                            return_train_score=True, **kwargs)\n    mean_scores = pd.DataFrame(scores).mean()\n    std_scores = pd.DataFrame(scores).std()\n    out_col = []\n    for i in range(len(mean_scores)):\n        out_col.append((f\"%0.3f (+\/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n    return pd.Series(data=out_col, index=mean_scores.index)","2ccb623d":"results_df = {}\nresults_df[\"DummyClassifier\"] = mean_std_cross_val_scores(DummyClassifier(strategy=\"stratified\", \n                                                                          random_state=2), \n                                                          X_train, y_train)\npd.DataFrame(results_df).T","fb71acd0":"# function to cross validate commonly used models with only change to the embedding function\ndef cross_validate_models(embedding_function, X_train, y_train):\n    pipe_lr = make_pipeline(embedding_function, LogisticRegression(max_iter=1000, random_state=2))\n    pipe_svc = make_pipeline(embedding_function, SVC(random_state=2))\n    pipe_knn = make_pipeline(embedding_function, KNeighborsClassifier())\n    pipe_rf = make_pipeline(embedding_function, RandomForestClassifier(random_state=2))\n    pipe_lgbm = make_pipeline(embedding_function, LGBMClassifier(random_state=2))\n    classifiers = {\n        'Logistic Regression' : pipe_lr,\n        \"SVC\": pipe_svc,\n        'KNN' : pipe_knn,\n        'Raindom Forest' : pipe_rf,\n        'LightGBM' : pipe_lgbm,\n    }\n\n    # cross validation score of the default model hyperparameter\n    results_df = {}\n    for (name, pipe) in classifiers.items():\n        print(\"cross validating\", name)\n        results_df[name] = mean_std_cross_val_scores(pipe, X_train, y_train)\n    return pd.DataFrame(results_df)\n\n# sort the model results colmuns by the validation score row with descending order\ndef display_model_results(model_results):\n    return model_results.sort_values(by=\"test_score\", ascending=False, axis=1)","526e52b8":"# cross validate and check the bag of words embedding performance\nbow_results = cross_validate_models(bow_vectorizer, X_train, y_train)\ndisplay_model_results(bow_results) # results are sorted by test_score row for each column","8343344f":"# create TF-IDF representation of the text data\ntfidf_vectorizer = TfidfVectorizer(dtype=np.float32,\n                                   preprocessor=remove_numeric_string, # remove numeric term\n                                   strip_accents='unicode',\n                                   tokenizer=word_tokenize,\n                                   ngram_range=(1,1), # keep the unigram\n                                   analyzer='word', # feature should be made of word n-gram\n                                   min_df=10, # ignore terms appeared less than 10 times\n                                   max_df=0.75, # ignore terms appeared more than 75% of the tweets available\n                                   sublinear_tf=True) \ntfidf_vector = tfidf_vectorizer.fit_transform(train['text_processed'])\ntfidf_features = pd.DataFrame(tfidf_vector.toarray(), \n             columns=sorted(tfidf_vectorizer.vocabulary_))\ntfidf_features","e332bd5b":"# create term frequency dictionary and use it for plotting the word cloud\ncategory_count = create_term_frequency_count(tfidf_features.iloc[disaster_text_row_index])\ncreate_word_cloud(category_count, title=\"Disaster Tweet Wordcloud\")\n\ncategory_count = create_term_frequency_count(tfidf_features.iloc[non_disaster_text_row_index])\ncreate_word_cloud(category_count, title=\"Non-Disaster Tweet Wordcloud\")","812d64bc":"# baseline model\nresults_df = {}\nresults_df[\"DummyClassifier\"] = mean_std_cross_val_scores(DummyClassifier(strategy=\"stratified\", \n                                                                          random_state=2), \n                                                          X_train, y_train)\npd.DataFrame(results_df).T","7c70ffe3":"# cross validate and check the TF-IDF embedding performance\ntfidf_results = cross_validate_models(tfidf_vectorizer, X_train, y_train)\ndisplay_model_results(tfidf_results) # results are sorted by test_score row for each column","4e061443":"import gensim\nfrom gensim.models import Word2Vec\nfrom collections import defaultdict","a2492133":"# source for creating sentence embedding from mean Word2Vec words embedding \n# http:\/\/nadbordrozd.github.io\/blog\/2016\/05\/20\/text-classification-with-word2vec\/\n# I made changes to the sklearn transformer to code to adapt the problem\nclass MeanEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        self.dim = next(iter(word2vec.items()))[1].shape[0]\n\n    def fit(self, X, y):\n        return self\n\n    def transform(self, X):\n        return np.array([\n            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n                    or [np.zeros(self.dim)], axis=0)\n            for words in X\n        ], dtype=object)","492d2fc2":"# create word embedding using Word2vec model and output the embedding size \nword2vec_model = Word2Vec(X_train.apply(word_tokenize), seed=1)\nword2vec_dict = dict(zip(word2vec_model.wv.key_to_index.keys(), \n         word2vec_model.wv.vectors))\nprint(word2vec_model)","c8ad90b4":"# cross validate and check the self-trained Word2Vec embedding performance\nword2vec_results = cross_validate_models(MeanEmbeddingVectorizer(word2vec_dict), \n                                         X_train.apply(word_tokenize), y_train)\ndisplay_model_results(word2vec_results) # results are sorted by test_score row for each column","eef4f59e":"# loading pretrained google news word2vec embedding 300D\nfrom gensim.models import KeyedVectors\nword2vec_pretrained = KeyedVectors.load_word2vec_format(\"..\/input\/googles-trained-word2vec-model-in-python\/GoogleNews-vectors-negative300.bin\",binary=True)\nword2vec_pretrained_dict = dict(zip(word2vec_pretrained.key_to_index.keys(), \n                                    word2vec_pretrained.vectors))","4772900e":"# use mean word embedding approach with pretrained Google news Word2Vec embedding\npretrained_word2vec_results = cross_validate_models(MeanEmbeddingVectorizer(word2vec_pretrained_dict), \n                                         X_train.apply(word_tokenize), y_train)\ndisplay_model_results(pretrained_word2vec_results) # results are sorted by test_score row for each column","62cce6a1":"# free RAM for about 4 GB\ndel word2vec_pretrained\ndel word2vec_pretrained_dict","87eea5ac":"import torch\nimport transformers","836a7a67":"# For DistilBERT:\nbert_model_class, bert_tokenizer_class, bert_pretrained_weights = (transformers.DistilBertModel, \n                                                                   transformers.DistilBertTokenizer, \n                                                                   'distilbert-base-uncased')\n# Load pretrained model\/tokenizer\nbert_tokenizer = bert_tokenizer_class.from_pretrained(bert_pretrained_weights)\nbert_model = bert_model_class.from_pretrained(bert_pretrained_weights)\n\n# use BERT tokenizer to preprocessing the sentence texts\ntokenized_sentences = X_train.apply((lambda x: bert_tokenizer.encode(x, add_special_tokens=True)))\n\n# pad the sentence for creating embedding\nmax_len = 0\nfor i in tokenized_sentences.values:\n    if len(i) > max_len:\n        max_len = len(i)\n\npadded_sentences = np.array([i + [0]*(max_len-len(i)) for i in tokenized_sentences.values])\npadded_sentences.shape","cfceceee":"# create attention mask to tell BERT to ignore the padding added when it's processing its input\nattention_mask = np.where(padded_sentences != 0, 1, 0)\nattention_mask.shape","e9ae8521":"# this will take a while and watch out for you memory usage\ninput_ids = torch.tensor(padded_sentences)  \nattention_mask = torch.tensor(attention_mask)\n\nwith torch.no_grad():\n    last_hidden_states = bert_model(input_ids, attention_mask=attention_mask)\nbert_features = last_hidden_states[0][:,0,:].numpy()\nbert_features.shape","705a85e0":"# use distillBERT embedding to cross validate the data\ndistill_bert_results = cross_validate_models(None, bert_features, y_train)\ndisplay_model_results(distill_bert_results) # results are sorted by test_score row for each column","ab85dc3b":"# For BERT\nbert_model_class, bert_tokenizer_class, bert_pretrained_weights = (transformers.BertModel, \n                                                                   transformers.BertTokenizer, \n                                                                   'bert-base-uncased')\n# Load pretrained model\/tokenizer\nbert_tokenizer = bert_tokenizer_class.from_pretrained(bert_pretrained_weights)\nbert_model = bert_model_class.from_pretrained(bert_pretrained_weights)\n\n# use BERT tokenizer to preprocessing the sentence texts\ntokenized_sentences = X_train.apply((lambda x: bert_tokenizer.encode(x, add_special_tokens=True)))\n\n# pad the sentence for creating embedding\nmax_len = 0\nfor i in tokenized_sentences.values:\n    if len(i) > max_len:\n        max_len = len(i)\n\npadded_sentences = np.array([i + [0]*(max_len-len(i)) for i in tokenized_sentences.values])\npadded_sentences.shape","1f076f29":"# create attention mask to tell BERT to ignore the padding added when it's processing its input\nattention_mask = np.where(padded_sentences != 0, 1, 0)\nattention_mask.shape","9e6a704a":"# this will take a while and watch out for you memory usage\ninput_ids = torch.tensor(padded_sentences)  \nattention_mask = torch.tensor(attention_mask)\n\nwith torch.no_grad():\n    last_hidden_states = bert_model(input_ids, attention_mask=attention_mask)\nbert_features = last_hidden_states[0][:,0,:].numpy()\nbert_features.shape","042ba7a1":"# use BERT-based embedding to cross validate the data\nbert_results = cross_validate_models(None, bert_features, y_train)\ndisplay_model_results(bert_results) # results are sorted by test_score row for each column","a5c2d407":"# load the pretrained Word2Vec model again since it's deleted for saving memory\nword2vec_pretrained = KeyedVectors.load_word2vec_format(\"..\/input\/googles-trained-word2vec-model-in-python\/GoogleNews-vectors-negative300.bin\",binary=True)\nword2vec_pretrained_dict = dict(zip(word2vec_pretrained.key_to_index.keys(), \n                                    word2vec_pretrained.vectors))","414f9e3b":"# current best model is SVC created using the Google News pretrained Word2Vec embedding\nbest_model = make_pipeline(MeanEmbeddingVectorizer(word2vec_pretrained_dict), SVC(random_state=2))\nbest_model.fit(X_train.apply(word_tokenize), y_train)\n\n# show classification report\nprint(classification_report(y_valid, best_model.predict(X_valid.apply(word_tokenize)), digits=3))\n\n# plot confusion matrix\nfig, ax = plt.subplots(figsize=(6, 6))\nplot_confusion_matrix(best_model, X_valid.apply(word_tokenize), y_valid,\n                      values_format=\"d\", cmap=plt.cm.Blues, ax=ax);","a750b8e9":"# free RAM for about 4 GB\ndel word2vec_pretrained\ndel word2vec_pretrained_dict","c1826816":"#### Comparing the TF-IDF results with the Bag of Words results with the default parameters for models and minimal setups for the vectorizers, you can see the Bag of Words representation's best model SVC has slight higher validation score (0.786) than the best model of the TF-IDF representation validation score of SVC model (0.782). ","f18a2730":"#### Using a smaller version of BERT (DistillBERT), the best model is Logistic regression with validation score of 0.799 using default parameters, which is slightly lower than the pretrained Google News Word2Vec embedding, but better than the bag of words, TF-IDF and self-trained Word2Vec model.","d614c97e":"## 2. Data Preprocessing","efbd85d6":"#### After using the best model with default parameter with the pretrained Google News Word2Vec embedding, the validation score for the disaster tweet classification task is 0.775, which is pretty close to the cross validation score of 0.804. The model generalize well. Looking at the confusion matrix, we can observe there are larger proportion of the class 1 being predicted as class 0. More hyperparameter tuning, feature selection, and other techniques can be applied improve the modeling outcome. \n\n#### Since this post is about comparing the embedding performances on sklearn model with default parameters, this is the end of this kernel. ","586b3236":"#### Let's drop row 7610 since it's a duplication of the row 7140; and remove the timestamp for row 3053 and 7140.","48b03ace":"## 3. Create Text Representations","7c80baae":"#### If you like the notebook, please comment and upvote!","6d08e1f6":"## 1. Read CSV files with Pandas","feca828f":"#### Apply the preprocessing function to the training and testing texts","33362ef0":"### 3.1 Bag of Words","ce045d9a":"#### Compare the self-trained Word2Vec results vs the pretrained Google Word2Vec embedding results, the pretrained embedding achieve better results with the best model of SVC having mean validation score of 0.804, which is higher than the best bag of words model (0.786), the best TF-IDF model (0.782), and the self-trained Word2Vec model (0.684).","98114c94":"#### Split the train dataset set to 80% train and 20% valid and prepare X and y data","72067f38":"#### After creating bag of words representation of the text data, we obtained 1120 word features that can be used for training classification models.","170ad38d":"#### This is what the testing data look like after the preprocessing","229b3d85":"#### Now, let's create text representations with the preprocessed text data","3a5d08ee":"#### After dropping the aforementioned columns, we only have the \"text\" and \"target\" columns left for further text preprocessing. \n#### Now, let's apply the following preprocessing steps:\n1. Lowercase the text: `.lower()`\n2. Replace extra whitespaces: `.replace('\\s\\s+', ' ')`\n3. Remove punctuations: `.translate(str.maketrans('', '', string.punctuation))`\n4. Handle other Tweet related hashtags and emojis\": `tweet-preprocessor.clean()`\n5. lemmatization: translate a given word to its dictionary form using `token.lemma_` from spaCy\n6. Remove stopwords: `gensim.parsing.preprocessing.remove_stopwords()`","458e9cff":"#### After running the DistillBERT model, we obtained 768 feature columns for training ML models","8affb74e":"![image.png](attachment:f9797465-dcf4-4778-91b8-ad8a9c897045.png)\n![image.png](attachment:a5a4a5c6-63df-4487-a163-b5838d47a535.png)\n\n### Dataset description and machine learning problem\n\nThis dataset contains Tweeter text data for training machine learning model to predict whether the Tweets are about real disasters or not.\n\nI will explore text representations using **bag of words**, **TF-IDF**, **Word2Vec (self-trained, pretrained)**, and **BERT (pretrained DistillBERT and BERT-based)** to create embeddings to see how will they perform with sklearn's traditional machine learning models  with their default parameters.","d799dd5c":"#### From the above two wordcloud generated by counting the term frequencies in two classes of text, we can see the disaster tweets have high frequency terms like \"kill\", \"suicide\", \"bomb\", and etc. And the non-disaster tweets have high frequency terms like \"like\", \"love\", \"video\", and etc.\n\n","5e4c20bd":"#### The default pretrained BERT-based model performs slightly worse than the DistillBERT model using sklearn model's default parameters. Currently, the best performing model is the SVC model trained on the Google News pretrained Word2Vec embedding with 300 dimensions with cross validation valid score of 0.804.","ed1c35fd":"#### Since this notebook is about comparing the text representations with the default model parameters, I will not tune the model or apply any feature selection or ensemble methods on the data and models. From the above output, we can observe that SVC (0.786), Logistic Regression (0.781), and LightGBM (0.765) are the top performing mode for the Bag of Word representation. Logistic regression has the smallest train and valid score gap and SVC has the second longest fit time followed after random forest.","2d11766e":"#### From the above result, we can observe that there are 33.86% of the data missing for the location column and there are 0.0797% of the data missing for the keyword column. Since we are interested in creating text representations using only the tweet texts, we can drop the two columns together with the id column for both the train and test datasets.","7092af8b":"## 4. Evaluate best model on validation set","02a284e7":"#### After some data exploration, I notice there are duplicates in the training dataset. Let me drop the duplicates and also handle the rows with UTC timestamp in it.","3be40207":"#### Find best ML models available based on the data we have","7e791353":"Bag of words representation of the sentence is the most straightforward one. All the unique words' word counts are recorded as features. For each sentence, the embedding is a list of numbers represented how many times the feature words appeared in the sentence. For example, let's say the entire text dataset has 6 unique words as features of [\"A\", \"B\", \"C\" ,\"D\", \"E\", \"F\"] and the 1st sentence has 3 words matched with the first 3 words in the word features with count of 1 for each word. Then, the bag of words embedding for the 1st sentence would be represented as [1, 1, 1, 0, 0, 0]. Below is a better visual example of bag of words:\n![image.png](attachment:aa342c9c-8972-4d06-8312-045e1e6d95ff.png)","d0ca0cd3":"### 3.2 TF-IDF","0f399993":"#### The output of the TF-IDF vectorizer looks pretty similar for the wordcloud visualization, but \"scream\" doesn't have high vector value in the Bag of Word representation.","13f574b3":"#### From the above result, we can observe that there are 33.27% of the data missing for the location column and there are 0.08% of the data missing for the keyword column.","c6ca5801":"BERT was originally pre-trained on the whole of the English Wikipedia and Brown Corpus and is fine-tuned on downstream natural language processing tasks like question and answering sentence pairs.\n\nI will use the HuggineFace pretrained DistillBERT and BERT models to create embeddings:\nhttps:\/\/huggingface.co\/bert-base-uncased","6f36c2ff":"#### Using dummy classifer as the baseline, we can see the baseline validation score is 0.510.\n#### To make the whole cross validating process reproducible, I will set the random state and put the models into a function and only allow me to pass the embedding function to create embedding.","62551c54":"#### Check missing data","c022ae5f":"#### This is what the training data look like after the preprocessing","c79c8217":"> Word2Vec uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between the vectors) indicates the level of semantic similarity between the words represented by those vectors.\n\n\nI used the Gensim's Word2Vec model to train my own Word2Vec model and also the pretrained Google News Word2Vec model to compare the results.\n\nFor Gensim Word2Vec documantion, see here: https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html","e778cb06":"#### Since Word2Vec models will only return word embedding, I will create sentence embedding by averaging the existing words' embedding for each sentence. The below two classes will act as the function to transform the word embeddings to sentence embedding for training models.","9e717be7":"#### Without using pretrained Word2Vec model, I trainde my own Word2Vec models using default parameters to create sentence embedding for the tweets. The performance is not as good as the Bag of Words and the TF-IDF representation. Next, I will load the Google Word2Vec pretrained word embedding to create sentence embdedding to compare the modeling results with default model parameters. ","80669591":"#### Let's visualize the TF-IDF representation with wordcloud for both the disaster texts and the non disaster texts.","2c18f58d":"### 3.4 BERT","6d5c7d53":"### 3.3 Word2Vec","e12f6a0e":"> TF-IDF stands for \"Term Frequency \u2014 Inverse Document Frequency\". It is a statistical technique that quantifies the importance of a word in a document based on how often it appears in that document and a given collection of documents (corpus).\n\n* **TF quantifies the word frequency**: if a word occurs frequently in a document, then it should be more important and relevant than other words that appear fewer times and we should give that word a high score (TF).\n* **IDF penalize the document frequency**: if a word appears many times in a document but also in too many other documents, it\u2019s probably not a relevant and meaningful word. Hence, we should assign a lower score to that word (IDF). The number of documents is fixed. Having higher number of documents containing a term will lower its IDF whereas having a lower number of documents containing a term will increase the value of IDF. \n\n![image.png](attachment:01402eab-98f9-4009-bbb1-feb011a1e606.png)"}}