{"cell_type":{"7034d8f9":"code","3e86384f":"code","db2224af":"code","6d9a24c8":"code","92ba6668":"code","507441fb":"code","8c560299":"code","9bddb8ad":"code","154689e5":"code","232df5de":"code","e46e5dea":"code","e24563f5":"code","501a6d1e":"code","45a81736":"code","fa5c728c":"code","91abc67b":"code","e693dad1":"code","54396b5b":"code","ee2facc2":"code","f6c85b96":"code","fbd32718":"code","7bac61ea":"code","598a115b":"code","e5519257":"code","1836ac54":"code","60e47727":"code","1dd04f22":"code","1baf3544":"code","2c9523f8":"code","bcad450e":"code","38016826":"code","7ea6edd0":"code","0a09a196":"code","72e4078d":"code","54ffaefc":"code","96afe9a1":"code","008299e7":"code","1ad51b7f":"code","aa72a7ae":"code","ddbb981e":"code","3d2c3f92":"code","40014242":"code","46b22684":"code","216cb588":"code","40eea876":"code","039f7fb5":"code","13ee29a9":"code","c856350d":"code","5aa1d39e":"code","3a2a9f4e":"code","a73dafd4":"code","c664695f":"code","55e94c68":"code","bad96c30":"code","1a78ea96":"code","5fd5242c":"code","a24fe17d":"code","0c1abe11":"code","043c6032":"code","93b0c9fe":"code","0e80cab5":"code","c4db76d8":"code","ffa25183":"code","ecabfc9c":"code","c4154c3a":"code","3c3cd4ec":"code","66ba9121":"code","1d130d3f":"code","33d57bfb":"code","2928c5be":"code","f6467c29":"code","b05f667b":"code","31f797da":"code","c6b2ed6b":"code","53f2bb27":"code","0d746bf2":"code","aa6ec900":"code","02ca62c4":"code","d996f254":"code","84130b1a":"code","bd7a94e5":"code","3037e375":"code","85784c28":"code","a7aa2071":"code","b38b2941":"code","bd2ce86f":"code","b3476027":"code","cb11d874":"code","734f3f7e":"code","d3205342":"markdown","d5970bd9":"markdown","0e5fbc04":"markdown","06cce786":"markdown","822933cc":"markdown","e63ef99f":"markdown","1d85df84":"markdown","6f415f44":"markdown","e8d89bc2":"markdown","8a2b3e40":"markdown","a99b1048":"markdown","97ce648f":"markdown","809ff4f8":"markdown","99ae3d0f":"markdown","8ba48ada":"markdown","1c28c72b":"markdown","bd78a2b7":"markdown","3d358c41":"markdown","fd5f86cb":"markdown","60e0e27d":"markdown","475b9f0e":"markdown","70fd7bcd":"markdown","a175b9fe":"markdown","8234e85e":"markdown","2e6a73b4":"markdown","59fa2120":"markdown","50d6b507":"markdown","feb0a3be":"markdown","25339f14":"markdown","44fbdb01":"markdown","a3c223cb":"markdown","e9a37b3b":"markdown","d671080d":"markdown","03ede7da":"markdown","601163e9":"markdown","06f5e016":"markdown","e4469854":"markdown","b551cefd":"markdown","d8af59e6":"markdown","44db32aa":"markdown","398d6f18":"markdown","be497939":"markdown","cf1b1288":"markdown","7abd87c9":"markdown","9ffade6b":"markdown","bb454bf8":"markdown","f9d4fe58":"markdown","ceb5a249":"markdown","9375b755":"markdown","1fa927e4":"markdown","166ec590":"markdown","16e72927":"markdown","1b26078f":"markdown","886b4b53":"markdown","e8fc5909":"markdown","9c997187":"markdown","6c99d494":"markdown","8b224b74":"markdown","ac1b7653":"markdown","cebc4514":"markdown"},"source":{"7034d8f9":"import pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns \nfrom scipy import stats\nfrom scipy.stats import norm,skew\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport xgboost as xgb\nimport catboost as catb\nimport operator\nimport time\nimport ast\nfrom collections import Counter\nimport itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3e86384f":"path = '..\/input\/'","db2224af":"train = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')","6d9a24c8":"print(str(train.shape) + ' | ' + str(test.shape))","92ba6668":"fig = plt.figure(figsize=(10, 8))\nplt.title(\"Distribution of NA\")\ntrain.isna().sum().sort_values(ascending=True).plot(kind='barh',colors='Blue', fontsize=12)","507441fb":"pd.set_option(\"display.max_columns\",100)\ntrain.head(3)","8c560299":"train.dropna().shape","9bddb8ad":"train[['revenue', 'budget', 'runtime']].describe()","154689e5":"train.loc[train['id'] == 16,'revenue'] = 192864          # Skinning\ntrain.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          \ntrain.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs\ntrain.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven\ntrain.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout \ntrain.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\ntrain.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood\ntrain.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\ntrain.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada\ntrain.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\ntrain.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\ntrain.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times\ntrain.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\ntrain.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \ntrain.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy \ntrain.loc[train['id'] == 1542,'budget'] = 1              # All at Once\ntrain.loc[train['id'] == 1542,'budget'] = 15800000       # Crocodile Dundee II\ntrain.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\ntrain.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit\ntrain.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon\ntrain.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\ntrain.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\ntrain.loc[train['id'] == 2491,'revenue'] = 6800000       # Never Talk to Strangers\ntrain.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\ntrain.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams\ntrain.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\ntrain.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture","232df5de":"test.loc[test['id'] == 3889,'budget'] = 15000000       # Colossal\ntest.loc[test['id'] == 6733,'budget'] = 5000000        # The Big Sick\ntest.loc[test['id'] == 3197,'budget'] = 8000000        # High-Rise\ntest.loc[test['id'] == 6683,'budget'] = 50000000       # The Pink Panther 2\ntest.loc[test['id'] == 5704,'budget'] = 4300000        # French Connection II\ntest.loc[test['id'] == 6109,'budget'] = 281756         # Dogtooth\ntest.loc[test['id'] == 7242,'budget'] = 10000000       # Addams Family Values\ntest.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\ntest.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage\ntest.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee","e46e5dea":"# TRAIN \n\ntrain.runtime[train.id == 391] = 86 #Il peor natagle de la meva vida\ntrain.runtime[train.id == 592] = 90 #\u0410 \u043f\u043e\u0443\u0442\u0440\u0443 \u043e\u043d\u0438 \u043f\u0440\u043e\u0441\u043d\u0443\u043b\u0438\u0441\u044c\ntrain.runtime[train.id == 925] = 95 #\u00bfQui\u00e9n mat\u00f3 a Bambi?\ntrain.runtime[train.id == 978] = 93 #La peggior settimana della mia vita\ntrain.runtime[train.id == 1256] = 92 #Cipolla Colt\ntrain.runtime[train.id == 1542] = 93 #\u0412\u0441\u0435 \u0438 \u0441\u0440\u0430\u0437\u0443\ntrain.runtime[train.id == 1875] = 86 #Vermist\ntrain.runtime[train.id == 2151] = 108 #Mechenosets\ntrain.runtime[train.id == 2499] = 108 #Na Igre 2. Novyy Uroven\ntrain.runtime[train.id == 2646] = 98 #\u540c\u684c\u7684\u59b3\ntrain.runtime[train.id == 2786] = 111 #Revelation\ntrain.runtime[train.id == 2866] = 96 #Tutto tutto niente niente\n\n# TEST\ntest.runtime[test.id == 4074] = 103 #Shikshanachya Aaicha Gho\ntest.runtime[test.id == 4222] = 93 #Street Knight\ntest.runtime[test.id == 4431] = 100 #\u041f\u043b\u044e\u0441 \u043e\u0434\u0438\u043d\ntest.runtime[test.id == 5520] = 86 #Glukhar v kino\ntest.runtime[test.id == 5845] = 83 #Frau M\u00fcller muss weg!\ntest.runtime[test.id == 5849] = 140 #Shabd\ntest.runtime[test.id == 6210] = 104 #Le dernier souffle\ntest.runtime[test.id == 6804] = 145 #Chaahat Ek Nasha..\ntest.runtime[test.id == 7321] = 87 #El truco del manco","e24563f5":"power_six = train.id[train.budget > 1000][train.revenue < 100]\n\nfor k in power_six :\n    train.loc[train['id'] == k,'revenue'] =  train.loc[train['id'] == k,'revenue'] * 1000000","501a6d1e":"def visualize_distribution(y):\n    sns.distplot(y,fit=norm)\n    mu,sigma=norm.fit(y)\n    plt.legend([\"Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f})\".format(mu,sigma)])\n    plt.title(\"Distribution of revenue\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n    \n    \ndef visualize_probplot(y):\n    stats.probplot(y,plot=plt)\n    plt.show()","45a81736":"visualize_distribution(test.budget)\nvisualize_probplot(test.budget)","fa5c728c":"train['budget'] = np.log1p(train['budget'])\ntest['budget'] = np.log1p(test['budget'])\n\ntrain['popularity'] = np.log1p(train['popularity'])\ntest['popularity'] = np.log1p(test['popularity'])","91abc67b":"visualize_distribution(train.budget)\nvisualize_probplot(train.budget)","e693dad1":"visualize_distribution(train.revenue)\nvisualize_probplot(train.revenue)","54396b5b":"train = train.drop(['imdb_id', 'poster_path'], axis = 1)\ntest = test.drop(['imdb_id', 'poster_path'], axis = 1)","ee2facc2":"train.loc[train[\"cast\"].notnull(),\"cast\"]=train.loc[train[\"cast\"].notnull(),\"cast\"].apply(lambda x : ast.literal_eval(x))\ntrain.loc[train[\"crew\"].notnull(),\"crew\"]=train.loc[train[\"crew\"].notnull(),\"crew\"].apply(lambda x : ast.literal_eval(x))\n\ntest.loc[test[\"cast\"].notnull(),\"cast\"]=test.loc[test[\"cast\"].notnull(),\"cast\"].apply(lambda x : ast.literal_eval(x))\ntest.loc[test[\"crew\"].notnull(),\"crew\"]=test.loc[test[\"crew\"].notnull(),\"crew\"].apply(lambda x : ast.literal_eval(x))","f6c85b96":"train.loc[train[\"cast\"].notnull(),\"cast\"]=train.loc[train[\"cast\"].notnull(),\"cast\"]\\\n.apply(lambda x : [y[\"name\"] for y in x if y[\"order\"]<6]) \n\ntest.loc[test[\"cast\"].notnull(),\"cast\"]=test.loc[test[\"cast\"].notnull(),\"cast\"]\\\n.apply(lambda x : [y[\"name\"] for y in x if y[\"order\"]<6]) ","fbd32718":"def get_DirProdExP(df):\n    df[\"Director\"]=[[] for i in range(df.shape[0])]\n    df[\"Producer\"]=[[] for i in range(df.shape[0])]\n    df[\"Executive Producer\"]=[[] for i in range(df.shape[0])]\n\n    df[\"Director\"]=df.loc[df[\"crew\"].notnull(),\"crew\"]\\\n    .apply(lambda x : [y[\"name\"] for y in x if y[\"job\"]==\"Director\"])\n\n    df[\"Producer\"]=df.loc[df[\"crew\"].notnull(),\"crew\"]\\\n    .apply(lambda x : [y[\"name\"] for y in x if y[\"job\"]==\"Producer\"])\n\n    df[\"Executive Producer\"]=df.loc[df[\"crew\"].notnull(),\"crew\"]\\\n    .apply(lambda x : [y[\"name\"] for y in x if y[\"job\"]==\"Executive Producer\"])\n    \n    return df","7bac61ea":"train = get_DirProdExP(train)\ntest = get_DirProdExP(test)","598a115b":"train.head(3)","e5519257":"train.describe()","1836ac54":"print ('budget: ' + str(sum(train['budget'].isna())) + ', popularity: ' + str(sum(train['popularity'].isna())) + \n      ', runtime: ' + str(sum(train['runtime'].isna())) + ', revenue: ' + str(sum(train['revenue'].isna())))","60e47727":"f = ['budget', 'popularity', 'runtime', 'revenue']\nsns.pairplot(train[f].dropna())","1dd04f22":"print(\"raw format:\", train['spoken_languages'].iloc[0])\n\ntrain['spoken_languages'] = train['spoken_languages'].apply(lambda x: list(map(lambda d: list(d.values())[0], ast.literal_eval(x)) if isinstance(x, str) else []))\ntest['spoken_languages'] = test['spoken_languages'].apply(lambda x: list(map(lambda d: list(d.values())[0], ast.literal_eval(x)) if isinstance(x, str) else []))\n\ntrain.head().spoken_languages","1baf3544":"train['nb_spoken_languages'] = train.spoken_languages.apply(len)\ntest['nb_spoken_languages'] = test.spoken_languages.apply(len)\n\ntrain['english_spoken'] = train.spoken_languages.apply(lambda x: 'en' in x)\ntest['english_spoken'] = test.spoken_languages.apply(lambda x: 'en' in x)","2c9523f8":"train['nb_spoken_languages'].value_counts()","bcad450e":"all_languages = pd.concat([train.original_language, test.original_language], axis=0).value_counts()\nall_languages[all_languages > 10]","38016826":"# Here are the main languages\nmain_languages = list(all_languages[all_languages>20].index)\n# Let's categorize them, and add a 'other' catergorie\ndict_language = dict(zip(main_languages, range(1, len(main_languages)+1)))\ndict_language['other'] = 0\n\n#keep only the languages that are on main_languages\ntrain.original_language = train.original_language.apply(lambda x: x if x in main_languages else 'other')\ntest.original_language = test.original_language.apply(lambda x: x if x in main_languages else 'other')\n\n#put languages to numeric according to the indexes of the dictionary\ntrain['language'] = train.original_language.apply(lambda x: dict_language[x])\ntest['language'] = test.original_language.apply(lambda x: dict_language[x])","7ea6edd0":"# Apply the same preprocessing on the string values\ntrain.genres = train.genres.apply(lambda x: list(map(lambda d: list(d.values())[1], ast.literal_eval(x)) if isinstance(x, str) else []))\ntest.genres = test.genres.apply(lambda x: list(map(lambda d: list(d.values())[1], ast.literal_eval(x)) if isinstance(x, str) else []))\n\ntrain.genres.head()","0a09a196":"plt.bar(train.genres.apply(len).value_counts().sort_index().keys(), train.genres.apply(len).value_counts().sort_index())","72e4078d":"for v in train[train.genres.apply(len)==7][['title', 'genres']].values:\n    print('film:', v[0], '\\ngenres:', *v[1], '\\n')","54ffaefc":"genres = Counter(itertools.chain.from_iterable(pd.concat((train.genres, test.genres), axis=0).values))\ngenres","96afe9a1":"%%time\ntemp_train = train[['id', 'genres']]\ntemp_test = test[['id', 'genres']]\n\nfor g in genres:\n    temp_train[g] = temp_train.genres.apply(lambda x: 1 if g in x else 0)\n    temp_test[g] = temp_test.genres.apply(lambda x: 1 if g in x else 0)\n    \nX_train = temp_train.drop(['genres', 'id'], axis=1).values\nX_test = temp_test.drop(['genres', 'id'], axis=1).values\n\n# Number of features we want for genres\nn_comp_genres = 3\n\n# Build the SVD pipeline\nsvd = make_pipeline(\n    TruncatedSVD(n_components=n_comp_genres),\n    Normalizer(norm='l2', copy=False)\n)\n\n# Here are our new features\nf_train = svd.fit_transform(X_train)\nf_test = svd.transform(X_test)","008299e7":"temp_train.head(3)","1ad51b7f":"my_genres = [g for g in genres if g != 'TV Movie']\nmy_genres","aa72a7ae":"train = pd.concat([train, temp_train.iloc[:,1:]], axis=1) \ntrain.drop(train.columns[-1],axis=1, inplace = True)\n\ntest = pd.concat([test, temp_test.iloc[:,1:]], axis=1) \ntest.drop(test.columns[-1], axis=1, inplace = True)\n","ddbb981e":"train.Keywords = train.Keywords.apply(lambda x: list(map(lambda d: list(d.values())[1], ast.literal_eval(x)) if isinstance(x, str) else []))\ntest.Keywords = test.Keywords.apply(lambda x: list(map(lambda d: list(d.values())[1], ast.literal_eval(x)) if isinstance(x, str) else []))","3d2c3f92":"train['nb_keywords'] = train.Keywords.apply(len)\ntest['nb_keywords'] = test.Keywords.apply(len)","40014242":"train.production_companies = train.production_companies.apply(lambda x: list(map(lambda d: list(d.values())[0], ast.literal_eval(x)) if isinstance(x, str) else []))\ntest.production_companies = test.production_companies.apply(lambda x: list(map(lambda d: list(d.values())[0], ast.literal_eval(x)) if isinstance(x, str) else []))","46b22684":"production_companies = Counter(itertools.chain.from_iterable(pd.concat((train.production_companies, test.production_companies), axis=0).values))\nprint(\"Number of different production companies:\", len(production_companies))","216cb588":"train['nb_production_companies'] = train.production_companies.apply(len)\ntest['nb_production_companies'] = test.production_companies.apply(len)","40eea876":"%%time\nprint('Applying SVD on production companies to create reduced features')\n\n# Factorizing all the little production companies into an 'other' variable\nbig_companies = [p for p in production_companies if production_companies[p] > 30]\ntrain.production_companies = train.production_companies.apply(lambda l: list(map(lambda x: x if x in big_companies else 'other', l)))\n\ntemp_train = train[['id', 'production_companies']]\ntemp_test = test[['id', 'production_companies']]\n\nfor p in big_companies + ['other']:\n    temp_train[p] = temp_train.production_companies.apply(lambda x: 1 if p in x else 0)\n    temp_test[p] = temp_test.production_companies.apply(lambda x: 1 if p in x else 0)\n    \nX_train = temp_train.drop(['production_companies', 'id'], axis=1).values\nX_test = temp_test.drop(['production_companies', 'id'], axis=1).values\n\n# Number of features we want for genres\nn_comp_production_companies = 3\n\n# Build the SVD pipeline\nsvd = make_pipeline(\n    TruncatedSVD(n_components=n_comp_production_companies),\n    Normalizer(norm='l2', copy=False)\n)\n\n# Here are our new features\nf_train = svd.fit_transform(X_train)\nf_test = svd.transform(X_test)\n\nfor i in range(n_comp_production_companies):\n    train['production_companies_reduced_{}'.format(i)] = f_train[:, i]\n    test['production_companies_reduced_{}'.format(i)] = f_test[:, i]","039f7fb5":"train[['production_companies_reduced_0', 'production_companies_reduced_1', 'production_companies_reduced_2']].head(3)","13ee29a9":"train.production_countries = train.production_countries.apply(lambda x: list(map(lambda d: list(d.values())[0], ast.literal_eval(x)) if isinstance(x, str) else []))\ntest.production_countries = test.production_countries.apply(lambda x: list(map(lambda d: list(d.values())[0], ast.literal_eval(x)) if isinstance(x, str) else []))","c856350d":"production_countries = Counter(itertools.chain.from_iterable(pd.concat((train.production_countries, test.production_countries), axis=0).values))\nprint(\"Number of different production companies:\", len(production_countries))","5aa1d39e":"%%time\nprint('Applying SVD on production countries to create reduced features')\n\n# Factorizing all the little production companies into an 'other' variable\nbig_countries = [p for p in production_countries if production_countries[p] > 30]\ntrain.production_countries = train.production_countries.apply(lambda l: list(map(lambda x: x if x in big_countries else 'other', l)))\n\ntemp_train = train[['id', 'production_countries']]\ntemp_test = test[['id', 'production_countries']]\n\nfor p in big_countries + ['other']:\n    temp_train[p] = temp_train.production_countries.apply(lambda x: 1 if p in x else 0)\n    temp_test[p] = temp_test.production_countries.apply(lambda x: 1 if p in x else 0)\n    \nX_train = temp_train.drop(['production_countries', 'id'], axis=1).values\nX_test = temp_test.drop(['production_countries', 'id'], axis=1).values\n\n# Number of features we want for genres\nn_comp_production_countries = 3\n\n# Build the SVD pipeline\nsvd = make_pipeline(\n    TruncatedSVD(n_components=n_comp_production_countries),\n    Normalizer(norm='l2', copy=False)\n)\n\n# Here are our new features\nf_train = svd.fit_transform(X_train)\nf_test = svd.transform(X_test)\n\nfor i in range(n_comp_production_countries):\n    train['production_countries_reduced_{}'.format(i)] = f_train[:, i]\n    test['production_countries_reduced_{}'.format(i)] = f_test[:, i]","3a2a9f4e":"train[['production_countries_reduced_0', 'production_countries_reduced_1', 'production_countries_reduced_2']].head(3)","a73dafd4":"test.loc[test.release_date.isna(), 'release_date'] = '05\/01\/00'","c664695f":"#Train\ntrain['release_date'] = pd.to_datetime(train['release_date'], format='%m\/%d\/%y')\ntrain['Year'] = train.release_date.dt.year\ntrain['Month'] = train.release_date.dt.month\ntrain['Day'] = train.release_date.dt.day\ntrain['dayofweek'] = train.release_date.dt.dayofweek \ntrain['quarter'] = train.release_date.dt.quarter   \n#Test\ntest['release_date'] = pd.to_datetime(test['release_date'], format='%m\/%d\/%y')\ntest['Year'] = test.release_date.dt.year\ntest['Month'] = test.release_date.dt.month\ntest['Day'] = test.release_date.dt.day\ntest['dayofweek'] = test.release_date.dt.dayofweek \ntest['quarter'] = test.release_date.dt.quarter  ","55e94c68":"dummies = pd.get_dummies(train['Month'] ,drop_first=True).rename(columns=lambda x: 'Month' + str(x))\ndummies2 = pd.get_dummies(test['Month'] ,drop_first=True).rename(columns=lambda x: 'Month' + str(int(x)))\ntrain = pd.concat([train, dummies], axis=1)\ntest = pd.concat([test, dummies2], axis = 1)","bad96c30":"ddow = pd.get_dummies(train['dayofweek'] ,drop_first=True).rename(columns=lambda x: 'dayofweek' + str(x))\nddow2 = pd.get_dummies(test['dayofweek'] ,drop_first=True).rename(columns=lambda x: 'dayofweek' + str(int(x)))\ntrain = pd.concat([train, ddow], axis=1)\ntest = pd.concat([test, ddow2], axis = 1)","1a78ea96":"print ('Train: ' + str(max(train.Year)) + ' Test: ' + str(max(test.Year)))","5fd5242c":"#Train\ntrain.loc[train['Year'] > 2018, 'Year'] = train.loc[train['Year'] > 2018, 'Year'].apply(lambda x: x - 100)\n#Test\ntest.loc[test['Year'] > 2018, 'Year'] = test.loc[test['Year'] > 2018, 'Year'].apply(lambda x: x - 100)","a24fe17d":"test.Year.describe()","0c1abe11":"data_plot = train[['revenue', 'Year']]\nmoney_Y = data_plot.groupby('Year')['revenue'].sum()\n\nmoney_Y.plot(figsize=(15,8))\nplt.xlabel(\"Year of release\")\nplt.ylabel(\"revenue\")\nplt.xticks(np.arange(1960,2015,5))\n\nplt.show()","043c6032":"f,ax = plt.subplots(figsize=(18, 10))\nplt.bar(train.Month, train.revenue, color = 'blue')\nplt.xlabel(\"Month of release\")\nplt.ylabel(\"revenue\")\nplt.show()","93b0c9fe":"f,ax = plt.subplots(figsize=(15, 10))\nplt.bar(train.dayofweek, train.revenue, color = 'blue')\nplt.xlabel(\"Dayofweek of release\")\nplt.ylabel(\"revenue\")\nplt.show()","0e80cab5":"def lazzy_feat(df):\n    \n    df['Ratiobudgetbypopularity'] = df['budget']\/df['popularity']\n    df['RatiopopularitybyYear'] = df['popularity']\/df['Year']\n    df['RatoioruntimebyYear'] = df['runtime']\/df['Year']\n    \n    \n    df['budget_runtime_ratio'] = df['budget']\/df['runtime'] \n    df['budget_Year_ratio'] = df['budget']\/df['Year']\n    \n    return df","c4db76d8":"train = lazzy_feat(train)\ntest = lazzy_feat(test)","ffa25183":"# NAs\n\ntrain['has_homepage'] = np.where(train['homepage'].isna(), 0, 1)\ntrain ['has_collection'] = np.where(train['belongs_to_collection'].isna(), 0, 1)\n\ntest['has_homepage'] = np.where(test['homepage'].isna(), 0, 1)\ntest ['has_collection'] = np.where(test['belongs_to_collection'].isna(), 0, 1)\n\ntrain['has_tagline'] = np.where (train['tagline'].isna(), 0, 1)\ntest['has_tagline'] = np.where (test['tagline'].isna(), 0, 1)\n\n#Fix Strange occurences\n\ntrain['title_different'] = np.where(train['original_title'] == train['title'], 0, 1)\ntest['title_different'] = np.where(test['original_title'] == test['title'], 0, 1)\n\ntrain['isReleased'] = np.where(train['status'] != 'Released', 0, 1)\ntest['isReleased'] = np.where(test['status'] != 'Released', 0, 1)\n","ecabfc9c":"features = ['budget', \n            'popularity', \n            'runtime', \n            'nb_spoken_languages', \n            'nb_production_companies',\n            'english_spoken', \n            'language',\n            'has_homepage', 'has_collection', 'isReleased', 'has_tagline', 'title_different',\n            'Day',\n            'quarter', 'Year',\n            'nb_keywords', \n            'Month2', 'Month3',  'Month4', 'Month5',  'Month6', 'Month7',\n            'Ratiobudgetbypopularity', 'RatiopopularitybyYear',\n            'RatoioruntimebyYear', 'budget_runtime_ratio', 'budget_Year_ratio',\n            'Month8', 'Month9',  'Month10', 'Month11', 'Month12']","c4154c3a":"features += [col for col in train.columns if 'dayofweek' in col and col != \"dayofweek\"]\nfeatures += my_genres\nfeatures += ['production_companies_reduced_{}'.format(i) for i in range(n_comp_production_companies)]\nfeatures += ['production_countries_reduced_{}'.format(i) for i in range(n_comp_production_countries)]\nX = train[features]\nX['revenue'] = train.revenue","3c3cd4ec":"X.columns","66ba9121":"cor_features = X[['revenue', 'budget',  'popularity', 'runtime', 'nb_spoken_languages', 'nb_production_companies',\n            'Day', 'quarter', 'Year','nb_keywords' ]]\nf,ax = plt.subplots(figsize=(20, 12))\nsns.heatmap(cor_features.corr(), annot=True, linewidths=.7, fmt= '.2f',ax=ax)\nplt.show()","1d130d3f":"X.columns","33d57bfb":"X = X.drop(['revenue'], axis = 1)\ny = train.revenue.apply(np.log1p)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12, shuffle=True)","2928c5be":"params = {'objective': 'reg:linear', \n          'eta': 0.01, \n          'max_depth': 6, \n          'min_child_weight': 3,\n          'subsample': 0.8,\n          'colsample_bytree': 0.8,\n          'colsample_bylevel': 0.50, \n          'gamma': 1.45, \n          'eval_metric': 'rmse', \n          'seed': 12, \n          'silent': True    \n}","f6467c29":"# create dataset for xgboost\nxgb_data = [(xgb.DMatrix(X_train, y_train), 'train'), (xgb.DMatrix(X_test, y_test), 'valid')]","b05f667b":"print('Starting training...')\n# train\nxgb_model = xgb.train(params, \n                  xgb.DMatrix(X_train, y_train),\n                  5000,  \n                  xgb_data, \n                  verbose_eval=200,\n                  early_stopping_rounds=200)","31f797da":"xgb_model_full = xgb.XGBRegressor(objective  = 'reg:linear', \n          eta = 0.01, \n          max_depth = 6,\n          min_child_weight = 3,\n          subsample = 0.8, \n          colsample_bytree = 0.8,\n          colsample_bylevel = 0.50, \n          gamma = 1.45, \n          eval_metric = 'rmse',\n          seed = 12, n_estimators = 2000)\n","c6b2ed6b":"xgb_model_full.fit (X.values, y)","53f2bb27":"catmodel = catb.CatBoostRegressor(iterations=10000, \n                                 learning_rate=0.01, \n                                 depth=5, \n                                 eval_metric='RMSE',\n                                 colsample_bylevel=0.7,\n                                 bagging_temperature = 0.2,\n                                 metric_period = None,\n                                 early_stopping_rounds=200,\n                                 random_seed=12)","0d746bf2":"ti=time.time()\ncatmodel.fit(X, y, \n             eval_set=(X_train, y_train), \n             verbose=500, \n             use_best_model=True)\n\nprint(\"Number of minutes of training of model_cal = {:.2f}\".format((time.time()-ti)\/60))\n\ncat_pred_train=catmodel.predict(X)\ncat_pred_train[cat_pred_train<0]=0","aa6ec900":"# Feature Importance CATB\nfea_imp = pd.DataFrame({'imp': catmodel.feature_importances_, 'col': X.columns})\nfea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\nfea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 12))\nplt.savefig('catboost_feature_importance.png')   ","02ca62c4":"fig, ax = plt.subplots(figsize=(20,12))\nxgb.plot_importance(xgb_model, max_num_features=30, height = 0.8, ax = ax)\nplt.title('XGBOOST Features (avg over folds)')\nplt.show()","d996f254":"train_pred = xgb_model.predict(xgb.DMatrix(X), ntree_limit=xgb_model.best_ntree_limit)\nplt.figure(figsize=(32,15))\nplt.plot(y[:500],label=\"Real\")\nplt.plot(train_pred[:500],label=\"train_pred\")\nplt.legend(fontsize=15)\nplt.title(\"Real and predicted revenue of first 500 entries of train set\",fontsize=24)\nplt.show()","84130b1a":"plt.figure(figsize=(32,15))\nplt.plot(y[:500],label=\"Real\")\nplt.plot(cat_pred_train[:500],label=\"train_pred\")\nplt.legend(fontsize=15)\nplt.title(\"Real and predicted revenue of first 500 entries of train set\",fontsize=24)\nplt.show()","bd7a94e5":"plt.figure(figsize=(35,18))\nplt.plot(y[:600],label=\"Real\", color = \"red\")\nplt.plot(xgb_model.predict(xgb.DMatrix(X), ntree_limit=xgb_model.best_ntree_limit)[:600],label=\"xgb\", color = \"blue\")\nplt.plot(cat_pred_train[:600],label=\"catb\", color = \"green\")\nplt.legend(fontsize=15)\nplt.title(\"Real and predicted revenue of first 500 entries of train set\",fontsize=24)\nplt.show()","3037e375":"X_test = test[features]\nxgb_pred = np.expm1(xgb_model.predict(xgb.DMatrix(X_test), ntree_limit=xgb_model.best_ntree_limit))\npd.DataFrame({'id': test.id, 'revenue': xgb_pred}).to_csv('xgbsubmission.csv', index=False)","85784c28":"xgb_pred[0]","a7aa2071":"xgb_pred_f = np.expm1(xgb_model_full.predict(X_test.values))\npd.DataFrame({'id': test.id, 'revenue': xgb_pred_f}).to_csv('xgbfullsubmission.csv', index=False)\nxgb_pred_f[0]","b38b2941":"X_test = test[features]\ncatb_pred = np.expm1(catmodel.predict(X_test.values))\npd.DataFrame({'id': test.id, 'revenue': catb_pred}).to_csv('catbsubmission.csv', index=False)","bd2ce86f":"catb_pred[0]","b3476027":"ens_pred = 0.3*xgb_pred_f + 0.7*catb_pred\npd.DataFrame({'id': test.id, 'revenue': ens_pred}).to_csv('enssubmission.csv', index=False)","cb11d874":"ens_pred[0]","734f3f7e":"pd.DataFrame({'id': test.id, 'revenue': ens_pred}).head()","d3205342":"Here is the distribution of the number of genres per movie. There are 3 films with 7 genres!","d5970bd9":"### Quantitative features","0e5fbc04":"#### XGB","06cce786":"# MDB Box Office Prediction ","822933cc":"---","e63ef99f":"We have to fix the NaT generated on test file","1d85df84":"Train revenue and budget","6f415f44":"https:\/\/kaggle.com\/c\/tmdb-box-office-prediction","e8d89bc2":"#### Feature Importance","8a2b3e40":"**After getting the prediction we must perform the inverse operation**","a99b1048":"### Production countries","97ce648f":"### Models","809ff4f8":"First lets look how many NaN are on these data","99ae3d0f":"#### Month Distribution","8ba48ada":"### Director, Producer, Executive Producer","1c28c72b":"Instead of creating 20 categorical features, one for each genre, let's reduce those categories in a smaller space thanks to **SVD**.","bd78a2b7":"Now we can create 2 additional features: the number of spoken languages, and wheter the english belongs to them.","3d358c41":"#### CATB","fd5f86cb":"#### Day of week Distribution","60e0e27d":"Test Budget","475b9f0e":"---","70fd7bcd":"Import data","a175b9fe":"### Keywords","8234e85e":"## Data overview & Feature Engineering","2e6a73b4":"### Ensemble","59fa2120":"![alt text](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/10300\/logos\/thumb76_76.png?t=2019-02-05-19-29-15)","50d6b507":"#### Fix Years","feb0a3be":"Frequency of all genres:","25339f14":"## If you liked dont forget to vote up the kernel and share your comments and feedback!\n\n\n![](https:\/\/cdn.shopify.com\/s\/files\/1\/1061\/1924\/products\/Smiling_Emoji_with_Smiling_Eyes_large.png?v=1480481060)","44fbdb01":"### Lazzy features","a3c223cb":"Let's look at numbers first!\nThe quantitative features that could be helpful are:\n\n* the budget\n* the popularity\n* the runtime\n* and the target : revenue","e9a37b3b":"Useless Features\n\nWe drop some of the features that are not useful (at first glance)\n\n* imdb_id : if we stick to the data that is provided, we don't need this id. Perhaps we could add some new external data with it later...\n* poster_path : a link to the poster picture (no need for now, if we want to use some ensemble techniques)","d671080d":"The spoken languages are contained in a list of dictionaries, represented by a string, let's symplify it.","03ede7da":"At first look, the budget and the revenue seem correlated!","601163e9":"XGB parameters","06f5e016":"Let\u00b4s fix the revenue for those films which revenue and budget are significantlly low","e4469854":"#### Dummy DayofWeek","b551cefd":"### Outliers??","d8af59e6":"It turns out that the log transformation is the one which makes the probability plot closer to linear: we thus apply it to Train and Test. We will keep the original revenue until we deploy model just for correlations studies","44db32aa":"### Features from NAs Has Homepage","398d6f18":"The max Year can't be 2068!!\n\n* After a paralell study of these cases we have found that all the dates > 2018 must start with 19XX\n\nLets fix it!!","be497939":"First we have discovered that some films have really strange values such as runtime which are films that has 0 minutes of runtime what is completely impossible and that info is public and been released before the movie so we can deploy it manually in our model :D","cf1b1288":"#### Dummy Month","7abd87c9":"### Production_companies ","9ffade6b":"We normalize the data for having a reduced spectre","bb454bf8":"We are working with a wide range of revenue values so we will need to aproximate it to a normal distribution in order that the data can be more moldable","f9d4fe58":"There is one film in witch 9 languages are spoken ! Or maybe it is the number of languages in which the film has been translated...\n\n**Original Language**\n\nLet's see what are the principal main original languages in both train and test data :","ceb5a249":"### Export","9375b755":"### Catboost","1fa927e4":"#### XGB FULL","166ec590":"#### Year Distribution","16e72927":"There are several outliers so let\u00b4s clean it!!","1b26078f":"### Release Date","886b4b53":"### CLEAN","e8fc5909":"### Movie genre\n\nLet's look at the different genres associated with the movies.","9c997187":"### Cast & Crew","6c99d494":"There are films which budget is 0, so we need to fix it, let's do it with the median that works fine avoiding outliers.","8b224b74":"Elements null for each column","ac1b7653":"### Language","cebc4514":"#### Heat Correlation Matrix"}}