{"cell_type":{"70b694b3":"code","4b604a9b":"code","16ed3a05":"code","0f5884be":"code","64af387d":"code","fb96fb02":"code","185271a3":"code","28dcdc7d":"code","d637c5ad":"code","2da0c113":"code","91b6423f":"code","40397c92":"code","237de25e":"code","e0fa39ef":"code","e3174963":"code","f4de2aff":"code","84c51717":"code","1344a8ea":"code","defaca08":"markdown","26689cd2":"markdown","1dbc08fb":"markdown","bb5d09c2":"markdown","3a3cb9b2":"markdown","877fa693":"markdown","7d14f10f":"markdown","2fda8c02":"markdown","940f12f4":"markdown","68adb410":"markdown","00fd6b88":"markdown","7370352a":"markdown","7bc4d293":"markdown","4fa807ad":"markdown","4ff89e72":"markdown","7c1063c2":"markdown","60c642b3":"markdown"},"source":{"70b694b3":"# import the necessary packages\nimport matplotlib.pyplot as plt \nimport tensorflow as tf\nfrom PIL import Image \nimport seaborn as sns\nimport pandas as pd \nimport numpy as np\nimport os ","4b604a9b":"img1 = \"..\/input\/cat-and-dog\/training_set\/training_set\/cats\/cat.1915.jpg\"\nimg2 = \"..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/dog.2881.jpg\"\nimg3 = \"..\/input\/flowers-recognition\/flowers\/flowers\/sunflower\/7791014076_07a897cb85_n.jpg\"\nimg4 = \"..\/input\/fruits\/fruits-360\/Training\/Banana\/221_100.jpg\"\nimgs = [img1, img2, img3, img4]","16ed3a05":"from tensorflow.keras.applications.inception_v3 import preprocess_input\n\ndef _load_image(img_path):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = preprocess_input(img)\n    return img \n\ndef _get_predictions(_model):\n    f, ax = plt.subplots(1, 4)\n    f.set_size_inches(80, 40)\n    for i in range(4):\n        ax[i].imshow(Image.open(imgs[i]).resize((200, 200), Image.ANTIALIAS))\n    plt.show()\n    \n    f, axes = plt.subplots(1, 4)\n    f.set_size_inches(80, 10)\n    for i,img_path in enumerate(imgs):\n        img = _load_image(img_path)\n        preds  = tf.keras.applications.imagenet_utils.decode_predictions(_model.predict(img), top=3)[0]\n        b = sns.barplot(y=[c[1] for c in preds], x=[c[2] for c in preds], ax=axes[i])\n        b.tick_params(labelsize=36)\n        f.tight_layout()","0f5884be":"from keras.applications.xception import Xception\nxception_weights = '..\/input\/xception\/xception_weights_tf_dim_ordering_tf_kernels.h5'\nxception_model = Xception(weights=xception_weights)\n_get_predictions(xception_model)","64af387d":"from tensorflow.keras.applications.vgg16 import preprocess_input\n\ndef _load_image(img_path):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = preprocess_input(img)\n    return img ","fb96fb02":"from keras.applications.vgg16 import VGG16\nvgg16_weights = '..\/input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\nvgg16_model = VGG16(weights=vgg16_weights)\n_get_predictions(vgg16_model)","185271a3":"def _load_image(img_path):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = tf.keras.applications.vgg19.preprocess_input(img)\n    return img","28dcdc7d":"from keras.applications.vgg19 import VGG19\nvgg19_weights = '..\/input\/vgg19\/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\nvgg19_model = VGG19(weights=vgg19_weights)\n_get_predictions(vgg19_model)","d637c5ad":"def _load_image(img_path):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = tf.keras.applications.resnet.preprocess_input(img)\n    return img ","2da0c113":"from keras.applications.resnet50 import ResNet50\nresnet_model = ResNet50(weights=\"imagenet\")\n_get_predictions(resnet_model)","91b6423f":"from tensorflow.keras.applications.xception import preprocess_input\n\ndef _load_image(img_path):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = preprocess_input(img)\n    return img","40397c92":"from keras.applications.xception import Xception\nxception_weights = '..\/input\/xception\/xception_weights_tf_dim_ordering_tf_kernels.h5'\nxception_model = Xception(weights=xception_weights)\n_get_predictions(xception_model)","237de25e":"def _load_image(img_path):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = tf.keras.applications.inception_resnet_v2.preprocess_input(img)\n    return img","e0fa39ef":"inceptionResNetV2_model = tf.keras.applications.InceptionResNetV2(weights=\"imagenet\")\n_get_predictions(inceptionResNetV2_model)","e3174963":"def _load_image(img_path):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = tf.keras.applications.resnet_v2.preprocess_input(img)\n    return img ","f4de2aff":"resnet50V2_model = tf.keras.applications.ResNet50V2(weights=\"imagenet\", classifier_activation=\"softmax\", input_shape=(224,224,3), pooling='max')\n_get_predictions(resnet50V2_model)","84c51717":"def _load_image(img_path):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = tf.keras.applications.efficientnet.preprocess_input(img)\n    return img ","1344a8ea":"resnet50V2_model = tf.keras.applications.EfficientNetB0(weights=\"imagenet\", classifier_activation=\"softmax\", input_shape=(224,224,3), pooling='max')\n_get_predictions(resnet50V2_model)","defaca08":"---","26689cd2":"## Deeper with Networks and Convolutions in the Network\n\n\u201cNetwork in Network\u201d Min Lin et al. Put forward the idea in 2014 that linear filters in convolutional neural networks can increase their learning abilities with nonlinear activation functions. It creates a new model by adding a multi-layer perceptron structure into its linear convolutional network.\n\n<img src=\"https:\/\/miro.medium.com\/max\/576\/0*x9LLKCJNjyAwcaEf.png\" \/>\n\nThe model in the article \"Network in Network (NIN)\" consists of the convolutional neural network and average pooling operations with 3 multi-layer sensor structures as follows.\n\n<img src=\"https:\/\/miro.medium.com\/max\/576\/0*OHXA0LZquvEWsCqq.png\" \/>\n\nThe model was successful in MNIST, CIFAR10, CIFAR100 and SVHN datasets. After applying data augmentation and dropout, it was observed that the performance increased for all data sets.\n\nHowever, Google's article \"Going Deeper with Convolutions\" was published in 2015 with the idea of \"Going Deeper with Convolutions\" under the name of GoogLeNet architecture, referring to the LeNet model as its starting point. Here, both a \"deeper\" and a \"wider\" model are designed. It is the first publication to use the width concept in convolutional neural network models.\n\nThey obtain the width element in the model with the modules they call \"inception\".\n\n<img src=\"https:\/\/miro.medium.com\/max\/814\/0*NqJO38nD7pjE7_8i.png\" \/>\n\n> \u201cYou mustn\u2019t be afraid to dream a little bigger darling!\u201d from Inception movie","1dbc08fb":"---","bb5d09c2":"## Inception Res-Net-v1\n\nThe most important difference of Inception Res-Net v1 from Inception v4 and Inception Res-Net v2 is that it has no parallel convolution layers and its output size is 35 \u00d7 35 \u00d7 256, not 35 \u00d7 35 \u00d7 384.\n\n<img src=\"https:\/\/miro.medium.com\/max\/497\/0*CmEvqdtom-IMgbzl.png\" \/>\n\n<img src=\"https:\/\/miro.medium.com\/max\/566\/0*wIN86TwJyy-tgDK_.png\" \/>","3a3cb9b2":"---","877fa693":"**Result 1:** Inception-Res-Net-v1 is a hybrid model of Inception and Res-Net and has the same computational load as Inception v3.\n\n**Result 2:** Inception-Res-Net-v2 is an inception hybrid model with significantly improved recognition performance and increased processing load.\n\n**Result 3:** Inception v4 is an architecture that has roughly the same performance as Inception-Res-Net-v2, but is designed with only inception modules without residual structure.","7d14f10f":"## INCEPTION v4-INCEPTION-RES-NET (Res-Netv1-v2)\n\nThe most different of the optimizations and innovations made in Deep Networks is the ResNet structure in which 'residual' connections are made. With the article \"Deep Residual Learning for Image Recognition\", He et al. Changed the size of the first convolution process to 1 \u00d7 1 and optimized the residual layer. ReLU is a structure based on taking a direct sum from the activation function output and transferring the previous activation value to the output even under conditions where learning stops at the convolution outputs.\n\n<img src=\"https:\/\/miro.medium.com\/max\/403\/0*cV05PkgAauQVsgXQ.png\" \/>\n\nPure inception v4 or Res-Net-Inception is suggested as its architectural structure. If we look at the architecture in detail, the relationship between them is shown above. Inception architecture is highly adjustable. This means that different versions of filters or layers can be used and does not completely degrade the quality of the trained network. The innovation in Inception v4 is to create starting blocks for mesh sizes. <br>\nBelow are the Stem, 4 \u00d7 InceptionA, 7 \u00d7 InceptionB, 3 \u00d7 InceptionC, Reduction A and Reduction B structures in the Inception v4 module, respectively.","2fda8c02":"## Inception v4\n\nSzegedy et al defends the ResNet theory with their article \"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\" published in 2016.\n\nThe block below is the Stem layer used after login in Inception v4 and Res-Net Inception (Res-Net-v1-v2) modules. While the input image is 299 \u00d7 299 \u00d7 3D, it ultimately produces an output of 35 \u00d7 35 \u00d7 384. In some convolution layers, there is a valid padded process with the values of the matrix indicated by (V). This process fulfills the task of reducing the previous size by the corresponding rate in the next step. If there is no (V) the output is the same size as the input.\n\n<img src=\"https:\/\/miro.medium.com\/max\/247\/0*z0CiXJkdaBBBRoLf.png\" \/>\n\n<img src=\"https:\/\/miro.medium.com\/max\/492\/0*6m8Wkws2R_Q7Ffq_.png\" \/>\n\n'N', 'm', 'k' and 'l' are filter bank size.\n\nThe Reduction-A module, which is used to reduce from 17 \u00d7 17 to 8 \u00d7 8, is as follows.\n\nAs a result of the Residual optimization performed in the Inception v4 module, the general network structure of Inception Res-Net v1 and v2 modules is described in the rest of the article.\n\n<img src=\"https:\/\/miro.medium.com\/max\/450\/0*Jw4jnGluM4qvntc8.png\" \/>\n\nThe difference between Inception v1 and v2 is that the 5 \u00d7 Inception-ResNet-A, 10 \u00d7 Inception-ResNet-B, 5 \u00d7 Inception-ResNet-C and Reduction-A, Reduction-B layers are made up of different designs.\nThe Stem layer is the same in Inception Res-Net v2 as in the Inception v4 module. In addition, the 17 \u00d7 17 dimension reduction step from Reduction-A 35 \u00d7 35 is the same in Inception v4, Inception Res-Net v1 and v2 architectures. However, Stem layer in Inception Res-Net v1 is as follows.","940f12f4":"---","68adb410":"## Inception Res-Net-v2\n\n<img src=\"https:\/\/miro.medium.com\/max\/497\/0*L69onCjMS6xgI653.png\" \/>\n\n<img src=\"https:\/\/miro.medium.com\/max\/326\/0*JYVy4a4KBQLycEVt.png\" \/>\n\nThe number of filters in the Reduction-A layer, which is also common in the 3 Inception structure, is as shown in the table below.\n\n<img src=\"https:\/\/miro.medium.com\/max\/497\/0*r_9R3REwhSfYTiCa.png\" \/>\n\nTo scale the combined Inception Res-Net modules, a structure like the following is used. This scaling job is used only for inception versions of Res-Net and serves to keep the final linear activation value at an appropriate level (usually 0.1).\n\nRes-Net is about going deep, and the Inception family is about going into expansion. Using the table, we can say that Inception Res-Net-v2 has produced the optimum result for going both deep and wide.\n\n<img src=\"https:\/\/miro.medium.com\/max\/367\/0*ZBw8lfFbeD1ll76U.png\" \/>","00fd6b88":"## Inception v1\n\nThe expansion effect in Inception modules is created by parallel execution of 1 \u00d7 1, 3 \u00d7 3, 5 \u00d7 5 filters and 3 \u00d7 3 maximum joint operation in the convolution layers. This structure is called \"Naive Inception Module\".\n\n<img src=\"https:\/\/miro.medium.com\/max\/415\/0*2gQI1OfGwTdAWd0Y.png\" \/>\n\nHowever, there is an important problem here: the transaction complexity, the size of the output and the number of parameters become too large due to parallel operations. To overcome this problem, 1 \u00d7 1 convolution layers are added before the parallel Naive Inception convolution layers to achieve size reduction. This is version1 of Inception Modules.\n\nFor example; Assuming that the input image size is 100 \u00d7 100 \u00d7 60: The size is reduced by 100 \u00d7 100 \u00d7 20 as a result of convolution with 20 1 \u00d7 1 size filters. This is called \"pooling of feature\".\n\n<img src=\"https:\/\/miro.medium.com\/max\/450\/0*PTdqcaFDEeJj-kXQ.png\" \/>\n\nIt is an average process for attributes. Because the tensor depth (number of channels) will be reduced. This is very similar to the process of reducing the width and height dimension in a normal maximum joint operation. In addition, the 1 \u00d7 1 convolution layers are followed by the ReLU process as the activation function.\n\n**The whole network consists of 9 inception modules and more than 100 layers in total.**\n\n\u201cAverage pooling\u201d is used instead of Fully Connected-FC layers. Thus, 7 \u00d7 7 \u00d7 1024 tensors are converted into a 1 \u00d7 1 \u00d7 1024 size vector. Thus, a great advantage is provided in terms of parameter cost.\n\n**GoogLeNet uses 5 million parameters and 12 times less parameters than AlexNet.**\n\nR-CNN (Region with Convolutional Neural Networks) method makes use of Inception v1 model in 'Object Recognition' model. The table below provides information on the layers of the entire model. The information in the columns are respectively: Type of operation in the layer, filter size \/ step shift size (patch size \/ stride), output size, depth filter size (depth), filter size for size reduction (reduce), calculated parameter. number (params) is shown as processing load (ops).\n\n<img src=\"https:\/\/miro.medium.com\/max\/814\/0*PH1dxF7ZrXpq4kn1.png\" \/>\n\n- With the average pooling layer, 5 \u00d7 5 filter size and 3 stride steps, outputs 4 \u00d7 4 \u00d7 512 (4a) and 4 \u00d7 4 \u00d7 528 (4b) are as shown in the steps in the table.\n- Dimension reduction is performed using 128 filters with a 1 \u00d7 1 convolution and the activation function is linear.\n- A vector with 1024 elements is created with a fully connected layer and the activation function is linear.\n- With a dropout layer, 40% of the outputs are deleted randomly.\n- A linear layer realizes multiple classification in a 1000-class problem with a differentiable softmax loss function.\n\n**With the Inception v2-v3 article published just 1 day after the original Resnet article, we can say that November 2015 is \"Deep Learning Spring\". :)**","7370352a":"## Inception v2\n\n1. When the Inception architecture is reconsidered for computer vision, Szegedy et al. (Google team) in the article \"Rethinking the Inception Architecture for Computer Vision\" emphasizes the optimization and more effective use of the convolutional network in some constraints made in the inception model.\n    - To avoid the bottleneck in the previous structure, the feed forward network classifier or regressors can be represented by an acyclic graph from the input layer. This information defines a clear direction to the flow. The dimensions of the tensor slowly decrease as you move from the entrance to the exit.\n    - It is also important that the width and depth are balanced within the mesh. For optimum performance, the width must be set accurately with the filter sizes within the modules that provide the depth. Increasing width and depth improves the quality of the network structure, while the parallel processing load increases rapidly.\n2. In the GoogLeNet network structure, before the 3 \u00d7 3 convolution layers, a significant size reduction was achieved with 1 \u00d7 1 convolution. At this stage, it means that by multiplying the convolution process with a larger filter, a faster training can be provided with more specific parameters. It also reduces computational and memory burden.\n\n<img src=\"https:\/\/miro.medium.com\/max\/193\/0*_uvrFtTfEfcfIQ6G.png\" \/>\n\n3. Convolution operations with large dimensions (5 \u00d7 5, 7 \u00d7 7 etc.) bring the calculation burden with it. For example, in the case of using the same size filter with 5 \u00d7 5 convolution operation and 3 \u00d7 3 convolution operation, it creates 25\/9 = 2.78 times more calculation load. If the shape is examined closely, each outlet forms a small fully connected network. This significantly benefits from size reduction and calculation burden. The 5 \u00d7 5 Convolution operation displacement with the mini network is shown in the above figure.\n\n    - For the typical case, the assumption should be approached as follows; n = a.m so the activation number is changed by the 'a' factor. Having a two-layer replacement for the 5 \u00d7 5 layer seems reasonable to achieve this expansion in two steps: increase the filter size by in both steps (a = 1). In this condition, it results in (9 + 9) \/ 25 times of account load savings as a result of the network structure. This multiplication provides 28% gain. In the figure below, a network structure that becomes a 2-layer 3 \u00d7 3 convolution instead of a 5 \u00d7 5 convolution is obtained.\n    \n<img src=\"https:\/\/miro.medium.com\/max\/284\/0*yNSjiDiQh-R9wMHT.png\" \/>\n\n- By performing spatial multiplication in asymmetric convolution, the 3 \u00d7 3 convolution is divided into 3 \u00d7 1 sub-layers and the output number is adjusted to 3. It provides 33% gain compared to using an equal number of filters and inputs. However, if two layers of 2 \u00d7 2 convolution were used instead of 3 \u00d7 3 convolution layers, only 11% processing load would be gained.\n\n<img src=\"https:\/\/miro.medium.com\/max\/123\/0*sKfzMnxHXuJLHeXH.png\" \/>\n\nCreating 3 \u00d7 1 convolutional sub-layers from 3 \u00d7 3 convolution layers: According to the theory, when the (n \u00d7 n) convolution process is expressed with (1 \u00d7 n) and (n \u00d7 1) sub-layers, the computational gain increases dramatically by n. In practice, while this approach does not give successful results in the first layers of the network structure (m \u00d7 m feature map is in the range of m = 12-20), extremely successful results are obtained when this approach is used.\nThe current structure of the Inception module becomes as follows.\n\n<img src= \"https:\/\/miro.medium.com\/max\/252\/0*sPHrd9A3gMxowD3i.png\" \/>\n\n4. Batch-normalization or dropout processes also contribute to increasing the performance. Of course, both processes have different features here.\n\n<img src=\"https:\/\/miro.medium.com\/max\/495\/0*gFQt7iOfZM2RJh_u.png\" \/>\n\n5. Reducing the size of the active grid structure; Convolutional networks traditionally use the commoning process to reduce the mesh size. Maximum or average pooling increases the activation size in network filters.\n\n<img src=\"https:\/\/miro.medium.com\/max\/450\/0*7ymv5dXhhwa1ynPD.png\" \/>\n\nFor example, starting with the number of k filters and d \u00d7 d mesh structure, the mesh structure becomes (d \/ 2) \u00d7 (d \/ 2) when the 2k filter number is reached. This means that the calculation load increases with 2 (d \/ 2) \u00b2 (k) \u00b2 operations. Probably, when the convolution steps change places instead of commoning, a 3 times more profitable calculation load occurs. The building on the right has 3 times the account load.\nWhen used for parallel convolution operations and the partnership shift step (stride) 2 in the following structure, the computational load is again reduced and the computational gain is increased. The Inception v2 structure is 42 layers deep. It achieves a more effective result than GoogLeNet and VGGNet with only 2.5 more account load. Network structure is as in the table below.\n\n<img src=\"https:\/\/miro.medium.com\/max\/497\/0*z0bNSnLtGa8KRXXM.png\" \/>","7bc4d293":"## Inception v3\n\nIt is also a module suggested in the article \"Rethinking the Inception Architecture for Computer Vision\". The only feature that distinguishes Inception v2 is not only convolution layers but also the addition of set normalization (batch-normalized) and fully connected (FC) layer as auxiliary classifier. When Inception v2 is edited this way, it is called Inception v3. The comparison between the two is as follows.\n\n<img src=\"https:\/\/miro.medium.com\/max\/497\/0*_QGz0t1PDJ1TZPuC.png\" \/>","4fa807ad":"---\n\n### Resources\n- [Kaggle Notebook | Shivam Bansal](https:\/\/www.kaggle.com\/shivamb\/cnn-architectures-vgg-resnet-inception-tl\/notebook#notebook-container)\n- [Pyimagesearch](https:\/\/www.pyimagesearch.com\/2017\/03\/20\/imagenet-vggnet-resnet-inception-xception-keras\/)\n- [Tensorflow Webpage](https:\/\/www.tensorflow.org\/)\n- [Keras Webpage](https:\/\/keras.io\/)\n- [INCEPTION & RES-NET](https:\/\/medium.com\/@ayyucekizrak\/deri%CC%87n-bi%CC%87r-kar%C5%9Fila%C5%9Ftirma-inception-res-net-versiyonlar%C4%B1-f5cfb83df131)\n- [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https:\/\/arxiv.org\/pdf\/1905.11946.pdf)\n- [Going deeper with convolutions](https:\/\/arxiv.org\/pdf\/1409.4842.pdf)","4ff89e72":"---","7c1063c2":"<img src=\"https:\/\/www.synapticiel.co\/wp-content\/uploads\/2019\/10\/technology-rendering-background-human-on-geometric-element_4ea4078c-b728-11e8-8eb6-2fb8491c2cf0.jpg\" \/>\n\nIn this notebook, I review 6 different articles and present a comparative summary. Thus, I aim to reduce your processing load and increase your success rate.\nOptimization is something like that! :)\n\n> \u201cNo one knows what the right algorithm is, but it gives us hope that if we can discover some crude approximation of whatever this algorithm is and implement it on a computer, that can help us make a lot of progress.\u201d Andrew Ng","60c642b3":"---"}}