{"cell_type":{"d857c042":"code","5bc83025":"code","4c2fc787":"code","ecee915a":"code","87ba8b23":"code","33941afd":"code","d5e102a0":"code","b878ac31":"code","13fddf7b":"code","93482c89":"code","2bfed2b6":"code","55db88c5":"code","29245488":"code","e5f7ca59":"code","995f9396":"code","3e48ce77":"code","015a8593":"code","1b65c560":"code","8aff448f":"code","e44b59aa":"code","e14a2ab6":"code","01643010":"code","71b4e4d5":"code","da3860c3":"code","29714c58":"code","2a446b03":"code","62030252":"code","ce31957a":"code","c83b9256":"code","42f4c1af":"code","d4b475d4":"code","2a69b6d5":"code","b063c6d9":"code","bd5e8b49":"code","b536ab36":"code","e66bffd7":"code","1ba92ad2":"code","b8cccb04":"code","83ed0bf2":"code","23d78788":"code","474c8eb8":"code","dc1e4fb5":"markdown","c6cfd23c":"markdown","f57981d6":"markdown","229c5242":"markdown","c71abd2e":"markdown"},"source":{"d857c042":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5bc83025":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import metrics","4c2fc787":"#import datafile\ndf = pd.read_csv(\"\/kaggle\/input\/vehicle-dataset-from-cardekho\/CAR DETAILS FROM CAR DEKHO.csv\")","ecee915a":"df.shape","87ba8b23":"df.head()","33941afd":"# Check for unique values\nprint(df[\"seller_type\"].unique())\nprint(df[\"fuel\"].unique())\nprint(df[\"transmission\"].unique())\nprint(df[\"owner\"].unique())","d5e102a0":"print(df[\"seller_type\"].value_counts())\nprint(df[\"fuel\"].value_counts())\nprint(df[\"transmission\"].value_counts())\nprint(df[\"owner\"].value_counts())","b878ac31":"#Removing unneccessary data\nfinal_dataset = df.drop(\"name\", axis = 1)\nfinal_dataset.drop(final_dataset[final_dataset['fuel']=='Electric'].index,axis=0,inplace=True)\nfinal_dataset.drop(final_dataset[final_dataset['owner']=='Test Drive Car'].index,axis=0,inplace=True)","13fddf7b":"final_dataset.shape","93482c89":"#Adding a new column car_Age & removing year column\nfinal_dataset[\"car_Age\"] = 2020 - final_dataset[\"year\"]\nfinal_dataset.drop([\"year\"],axis = 1, inplace=True)","2bfed2b6":"final_dataset['no_of_previous_owners'] = final_dataset['owner'].map({'First Owner':1,'Second Owner':2,'Third Owner':3,\"Fourth & Above\":4})","55db88c5":"final_dataset = final_dataset.dropna()\nfinal_dataset['no_of_previous_owners'] = final_dataset['no_of_previous_owners'].astype(int)\nfinal_dataset.drop(['owner'], axis=1, inplace=True)","29245488":"final_dataset.head()","e5f7ca59":"final_dataset.drop(final_dataset[final_dataset['seller_type']=='Trustmark Dealer'].index,axis=0,inplace=True)","995f9396":"#Encoding the categorical data\nfinal_dataset = pd.get_dummies(final_dataset, drop_first=True)\nfinal_dataset.shape","3e48ce77":"final_dataset.head()","015a8593":"# Looking for Correlation\n#Dependence of target with every variable (ranging from -1 to 1)\ncorr_matrix = final_dataset.corr()\ncorr_matrix['selling_price'].sort_values(ascending=False)","1b65c560":"#checking the correlation between variables\ncorrmat = final_dataset.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\ng = sns.heatmap(final_dataset[top_corr_features].corr(), annot=True, cmap=\"RdYlGn\")","8aff448f":"#Independent and dependent variable(s)\nX = final_dataset.drop(\"selling_price\", axis=1)      #final_dataset.iloc[:,1:]\ny = final_dataset[[\"selling_price\"]]                 #final_dataset.iloc[:,0]","e44b59aa":"#feature importance\n\nmodel = ExtraTreesRegressor()\nmodel.fit(X,y)","e14a2ab6":"#looking for most important features\n#plotting the graph of feature importance for better visualisation\nfeat_importance = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importance.nlargest(6).plot(kind='bar')","01643010":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)","71b4e4d5":"print(\"Length of traing data set is :{} and length of test set is :{}\".format(len(X_train), len(X_test)))","da3860c3":"#Randomized search CV for random forest\n\n#Number of treers in random forest\nn_estimators =[int(x) for x in np.linspace(100,1200,num = 12)]\n#Number of features to consider at every split\nmax_features =[\"auto\", \"sqrt\"]\n#Maximum number of levels in the tree \nmax_depth = [int(x) for x in np.linspace(5, 30,6)]\n#Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n#Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","29714c58":"#create the random forest grid\nrf_grid = {'n_estimators' : n_estimators,\n               'max_features' : max_features,\n               'max_depth' : max_depth,\n               'min_samples_split' : min_samples_split,\n               'min_samples_leaf' : min_samples_leaf}\nprint(rf_grid)","2a446b03":"#Randomized search CV for gradient boosting\n#Number of treers in random forest\nn_estimators =[int(x) for x in np.linspace(100,1200,num = 12)]\n#Learning rate\nlearning_rate = [0.01, 0.02, 0.05, 0.1, 0.2]\nsubsample = [0.05, 0.06, 0.08, 0.09, 0.1]\ncriterion = ['mse', 'rmse', 'friedman_mse']\n#Number of features to consider at every split\nmax_features =[\"auto\", \"sqrt\"]","62030252":"#creating gradient boosting grid\ngb_grid = {'n_estimators' : n_estimators,\n           'learning_rate' : learning_rate,\n           'subsample' : subsample,\n           'max_depth' : max_depth,\n           'max_features' : max_features}\nprint(gb_grid)","ce31957a":"#Use the random grid to search the best parameters\n#Create the base model to tune\nrf_model = RandomForestRegressor()","c83b9256":"final_rf_model = RandomizedSearchCV(estimator = rf_model, \n                                 param_distributions=rf_grid,\n                                 scoring='neg_mean_squared_error',\n                                 n_iter = 20,\n                                 cv = 5,\n                                 verbose = 2,\n                                 random_state = 42,\n                                 n_jobs =1)","42f4c1af":"final_rf_model.fit(X_train,y_train)","d4b475d4":"final_rf_model.best_params_","2a69b6d5":"y_pred = final_rf_model.predict(X_test)","b063c6d9":"sns.distplot(y_test['selling_price']-y_pred)","bd5e8b49":"print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","b536ab36":"from sklearn.ensemble import GradientBoostingRegressor\ngb_model = GradientBoostingRegressor()","e66bffd7":"final_gb_model = RandomizedSearchCV(estimator = gb_model, \n                                 param_distributions=gb_grid,\n                                 scoring='neg_mean_squared_error',\n                                 n_iter = 20,\n                                 cv = 5,\n                                 verbose = 2,\n                                 random_state = 42,\n                                 n_jobs =1)","1ba92ad2":"final_gb_model.fit(X_train, y_train)","b8cccb04":"final_gb_model.best_params_","83ed0bf2":"y_pred = final_gb_model.predict(X_test)","23d78788":"sns.distplot(y_test['selling_price']-y_pred)","474c8eb8":"print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","dc1e4fb5":"### Gradient Boosting","c6cfd23c":"## Conclusion\n### Gradient Boost gives better RMSE than Random Forest","f57981d6":"## Trying Different Models","229c5242":"## Creating hyperparameters for randomized search CV","c71abd2e":"### Random Forest"}}