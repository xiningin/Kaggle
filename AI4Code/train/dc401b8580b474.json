{"cell_type":{"ef336a22":"code","7689d5dc":"code","cac9a318":"code","e972d6f8":"code","07bd8db4":"code","2147c75a":"code","4f218e7e":"code","2d5df916":"code","a386c0f5":"code","bc415ff4":"code","19921a00":"code","e6b9363c":"code","81aff7f2":"code","a45f30ea":"code","d6a4508a":"markdown","7200e895":"markdown","f1a5eefb":"markdown","f69e6844":"markdown","c0dd51ed":"markdown","1ade7409":"markdown","de9ff207":"markdown","b70382f5":"markdown","15854914":"markdown","eccae603":"markdown","a441ceae":"markdown","95344ee6":"markdown","9d713ff3":"markdown","9e5d9795":"markdown","17d25317":"markdown","cd558806":"markdown","4d398516":"markdown","dae873a1":"markdown","6dd69efd":"markdown"},"source":{"ef336a22":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import ks_2samp\nfrom collections import OrderedDict\nfrom operator import itemgetter\n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split, KFold\nfrom sklearn.metrics import plot_confusion_matrix, recall_score, precision_score, plot_precision_recall_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.dummy import DummyClassifier\n\nfrom imblearn.over_sampling import SMOTE","7689d5dc":"df = pd.read_excel('..\/input\/covid19\/dataset.xlsx',unidecode='ascii')\ndf_positive = df[df['SARS-Cov-2 exam result']=='positive']\ndf_negative = df[df['SARS-Cov-2 exam result']=='negative']\n\ndf = df.drop(['Patient addmited to regular ward (1=yes, 0=no)', \n                  'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n                  'Patient addmited to intensive care unit (1=yes, 0=no)'],axis=1)\ndf = df.set_index('Patient ID')","cac9a318":"nan_analyze = df.isna().sum()\/len(df)\n\nprint(\"Quantidade de tuplas:\", len(df))\nprint(\"Percentual m\u00e9dio missing values:\", round(nan_analyze.mean()*100,1),\"%\")\n\nlabel_perc = []\nfor i in np.arange(0, len(df.columns), 10):\n    label_perc.append(str(i)+\"%\")\nplt.figure(figsize=[10,40])\n\nplt.yticks(np.arange(len(df.columns)), nan_analyze.index.values)\nplt.xticks(np.arange(0, 1.1, .1), label_perc)\n\nplt.ylim(0,len(df.columns))\n\nplt.barh(np.arange(len(df.columns)), nan_analyze)","e972d6f8":"df_filtered = df[~np.isnan(df['Hematocrit'])]\nnan_analyze_filtered = df_filtered.isna().sum()\/len(df_filtered)\n\nprint(\"Quantidade de tuplas:\", len(df_filtered))\nprint(\"Percentual m\u00e9dio missing values:\", round(nan_analyze_filtered.mean()*100,1),\"%\")\n\n\nlabel_perc = []\nfor i in np.arange(0, 110, 10):\n    label_perc.append(str(i)+\"%\")\nplt.figure(figsize=[10,40])\n\nplt.yticks(np.arange(len(df_filtered.columns)), nan_analyze_filtered.index.values)\nplt.xticks(np.arange(0, 1.1, .1), label_perc)\n\nplt.ylim(0,len(df_filtered.columns))\n\nplt.barh(np.arange(len(df_filtered.columns)), nan_analyze_filtered)","07bd8db4":"df_filtered = df_filtered[nan_analyze_filtered[nan_analyze_filtered<=.4].index.values]\n\nnan_analyze_filtered = df_filtered.isna().sum()\/len(df_filtered)\n\nprint(\"Quantidade de tuplas:\", len(df_filtered))\nprint(\"Percentual m\u00e9dio missing values:\", round(nan_analyze_filtered.mean()*100,1),\"%\")\n\nlabel_perc = []\nfor i in np.arange(0, 110, 10):\n    label_perc.append(str(i)+\"%\")\nplt.figure(figsize=[10,10])\n\nplt.yticks(np.arange(len(df_filtered.columns)), nan_analyze_filtered.index.values)\nplt.xticks(np.arange(0, 1.1, .1), label_perc)\n\nplt.ylim(0,len(df_filtered.columns))\n\nplt.barh(np.arange(len(df_filtered.columns)), nan_analyze_filtered)","2147c75a":"features = df_filtered.dtypes[df_filtered.dtypes=='float64'].index.values\n\nks_list = []\npvalue_list = []\nfeature_list = []\n\nfor feature in features:\n    \n    positive = df_positive[~np.isnan(df_positive[feature])]\n    negative = df_negative[~np.isnan(df_negative[feature])]\n    \n    if len(positive)*len(negative)>0:\n        ks, pvalue = ks_2samp(positive[feature], negative[feature])\n        ks_list.append(ks)\n        pvalue_list.append(pvalue)\n        feature_list.append(feature)\n        \ndf_ks = pd.DataFrame(data=zip(ks_list,pvalue_list),columns=['ks', 'pvalue'],index=feature_list)\ndf_ks = df_ks.sort_values(by='ks',ascending=True)\n\ndf_ks['ks']\nplt.figure(figsize=(8,15))\nplt.yticks(np.arange(len(df_ks)), df_ks.index.values)\nplt.title('Diferen\u00e7a entre positivo VS negativo')\nplt.barh(np.arange(len(df_ks)), df_ks['ks'])","4f218e7e":"df_treated = df_filtered\ncat_features = df_filtered.dtypes[df_filtered.dtypes == 'object'].index.values\n\nfor feature in cat_features:\n    df_treated[feature] = df_treated[feature].fillna(df_treated[feature].mode().values[0]) \n    \ndf_treated = df_treated.fillna(df_treated.median())\n\ndf_treated_dummies = pd.get_dummies(df_treated, drop_first=True, dtype='bool')\n\ncolumns = list(df_treated_dummies.drop(labels=['SARS-Cov-2 exam result_positive'],axis=1).columns.values)\ncolumns.append('SARS-Cov-2 exam result_positive')\n\ndf_treated_dummies = df_treated_dummies[columns]","2d5df916":"ax = df_treated['SARS-Cov-2 exam result'].value_counts().plot(kind='bar',\n                                    figsize=(14,8))\nax.set_xticklabels(['negative', 'positive'], rotation=0, fontsize=20)","a386c0f5":"y = df_treated_dummies['SARS-Cov-2 exam result_positive']\nx = df_treated_dummies.iloc[:,:-1]\nx['Random'] = np.random.rand(x.shape[0])\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n\nsmote = SMOTE()\nX_train_random, y_train_random = smote.fit_sample(X_train, y_train)\nX_train, y_train = smote.fit_sample(X_train.iloc[:, :-1], y_train)","bc415ff4":"kfold = KFold(n_splits=20, random_state=42)\n\nparam_grid = {\n    'min_samples_split':[2, 4, 6],\n    'min_samples_leaf':[2, 4, 6],\n    'n_estimators':[10, 30, 50],\n    'max_depth':[3, 5]\n    }\n\nclf_rf = RandomForestClassifier(random_state=42)\ngrid = GridSearchCV(estimator=clf_rf, param_grid=param_grid, cv=kfold, scoring='recall', n_jobs=-1)\ngrid.fit(X=X_train, y=y_train)\nclf_rf = grid.best_estimator_","19921a00":"clf_rf.fit(X_train_random, y_train_random)\n    \ncols = x.columns\nfeature_importance = pd.DataFrame(data=clf_rf.feature_importances_, index=cols, columns=['FI'])\nfeature_importance = feature_importance.sort_values(by='FI', ascending=True)\n    \nplt.figure(figsize=(8,10))\n\nplt.yticks(np.arange(len(feature_importance)), feature_importance.index.values)\nplt.barh(np.arange(len(feature_importance)), feature_importance['FI'])\n\nplt.show()","e6b9363c":"columns = list(feature_importance.sort_values(by='FI', ascending=False).index.values)\ncolumns_new = columns[:columns.index('Random')]\nprint(\"Quantidade de features:\", len(columns_new))\n\ny = df_treated_dummies['SARS-Cov-2 exam result_positive']\nx = df_treated_dummies[columns_new]\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n\nsmote = SMOTE()\nX_train, y_train = smote.fit_sample(X_train, y_train)","81aff7f2":"kfold = KFold(n_splits=20, random_state=42)\n\nparam_grid = {\n    'min_samples_split':[2, 4, 6],\n    'min_samples_leaf':[2, 4, 6],\n    'n_estimators':[10, 30, 50],\n    'max_depth':[3, 5]\n    }\n\nclf_rf = RandomForestClassifier(random_state=42)\ngrid = GridSearchCV(estimator=clf_rf, param_grid=param_grid, cv=kfold, scoring='recall', n_jobs=-1)\ngrid.fit(X=X_train, y=y_train)\nclf_rf = grid.best_estimator_","a45f30ea":"clf_rf.fit(X_train, y_train)\n\nplot_confusion_matrix(clf_rf, X_test, y_test, cmap=plt.cm.Blues, values_format='.00f')\n\nprint(\"Recall treino:\", recall_score(y_train, clf_rf.predict(X_train)))\n\nprint(\"Recall valida\u00e7\u00e3o:\", cross_val_score(clf_rf, X_train, y_train, cv=20, scoring='recall', n_jobs=-1).mean())\n\nclf_rf.fit(X_train, y_train)\nprint(\"Recall teste:\", recall_score(y_test, clf_rf.predict(X_test)))","d6a4508a":"## Carregamento dos dados e importa\u00e7\u00f5es","7200e895":"#### Descri\u00e7\u00e3o\n\nEsse notebook \u00e9 o processo de modelagem, utilizando o algoritmo RandomForest, principalmente com as features que s\u00e3o obtidas a partir de um hemograma. Algumas an\u00e1lises s\u00e3o desenvolvidas nesse notebook:\n\n* Missing value;\n* Diferen\u00e7a entre distribui\u00e7\u00f5es;\n* Data Cleasing;\n* Dataset balence;\n* Otimiza\u00e7\u00e3o de parametros;\n* Feature engineering;","f1a5eefb":"\u00c9 poss\u00edvel observar que as features do hemograma est\u00e3o com cerca de 90% de missing values, como o objetivo desse notebook consiste em analisar essas features, ser\u00e3o filtradas no dataframe as tuplas em que as features do hemograma est\u00e3o presentes.","f69e6844":"Um m\u00e9todo simples de feature engineering \u00e9 o treinamento do modelo otimizado anteriormente com uma feature com valores aleat\u00f3rios e comparar a import\u00e2ncia entre as features. A feature aleat\u00f3ria ser\u00e1 utilizada como corte para escolher as features importantes para o modelo.\n\nDevido a pequena quantidade de tuplas no dataset \u00e9 importante diminuir a quantidade de features, pois diminuiu a chance de overfitting.","c0dd51ed":"## Feature engineering","1ade7409":"Com o gr\u00e1fico \u00e9 poss\u00edvel observar a nova quantidade. Com essa nova quantidade de features ser\u00e3o desenvolvidos os passos anteriores para otimizar o modelo com esse novo conjunto de features.","de9ff207":"Ap\u00f3s a sele\u00e7\u00e3o das tuplas que possuem as features do exame de sangue, a quantidade de tuplas caiu cerca de 90% e a incid\u00eancia de missing values de algumas features ainda \u00e9 alta, algumas com mais de 80%, para a analise ser menos influenciada pela imputa\u00e7\u00e3o de uma alta quantidade de missing values, ser\u00e1 aplicado um filtro nas features que possuem menos de 40% de missing values.","b70382f5":"Para a imputa\u00e7\u00e3o de dados nos missing values, ser\u00e3o adotadas as seguintes estrat\u00e9gias:\n* Num\u00e9ricos: imputa\u00e7\u00e3o da mediana;\n* Categ\u00f3ricos: imputa\u00e7\u00e3o da moda;\n\nAl\u00e9m disso as features categ\u00f3ricas ser\u00e3o transformadas em colunas dummies.","15854914":"## Diferen\u00e7a entre distribui\u00e7\u00f5es","eccae603":"> # Modelagem utilizando dados de Hemograma","a441ceae":"## Dataset balence","95344ee6":"Devido ao desbalanceamento da classe target, \u00e9 dif\u00edcil para o modelo identificar os poss\u00edveis comportamentos entre as classes.\n\nUm dos m\u00e9todos utilizados para contornar essa condi\u00e7\u00e3o \u00e9 a imputa\u00e7\u00e3o de dados sint\u00e9ticos, que fazem uma pequena altera\u00e7\u00e3o no valor das features, sem modificar o comportamento geral do cluster formado pela classe do evento raro.\n\nUm problema de utilizar essa abordagem em bases com pequena volumetria de dados, \u00e9 que pode tend\u00eanciar o dataset a gerar overfitting.\n","9d713ff3":"## Conclus\u00e3o","9e5d9795":"## Data Cleasing","17d25317":"\u00c9 poss\u00edvel observar que as principais features do modelo s\u00e3o as do hemograma e o modelo, mesmo com um pequeno dataset para treino, possui uma performance melhor que o aleat\u00f3rio.\n\nAl\u00e9m disso o modelo pode ser melhorado com a modifica\u00e7\u00e3o do threshold do score para classificar, o default \u00e9 acima de 0.5, diminuindo esse valor \u00e9 poss\u00edvel diminuir a quantidade de Falsos Negativos, mas como efeito colateral ir\u00e1 aumentar o Falso Positivo.\n\nEsse modelo \u00e9 para um estudo entre as rela\u00e7\u00f5es das features obtidas em um simples hemograma e o Covid-19, n\u00e3o sendo poss\u00edvel utiliz\u00e1-lo na pr\u00e1tica devido \u00e0 baixa volumetria de dados para a valida\u00e7\u00e3o apropriada para uso cl\u00ednico.","cd558806":"## Otimiza\u00e7\u00e3o dos parametros","4d398516":"## Missing Value","dae873a1":"Para as features num\u00e9ricas cont\u00ednuas, pode ser feita uma an\u00e1lise se existe diferen\u00e7a estat\u00edstica entre as distribui\u00e7\u00f5es, quando comparado quem testou positivo contra negativo para o Covid-19.\n\nEssa an\u00e1lise ser\u00e1 feita utilizando estat\u00edstica Kolmogorov-Smirnov, mais conhecido como ks statistic, essa m\u00e9trica mostra o quanto uma distribui\u00e7\u00e3o \u00e9 distante da outra, isso ajuda a entender se uma vari\u00e1vel consegue responder bem ao modelo ou se a vari\u00e1vel n\u00e3o ser\u00e1 representativa.","6dd69efd":"Para modelos de classifica\u00e7\u00e3o bin\u00e1ria um dos grandes threshold \u00e9 a precis\u00e3o vs recall. Essa compara\u00e7\u00e3o \u00e9 exemplificada abaixo:\n* Precis\u00e3o: Falsos positivos s\u00e3o mais prejudiciais ao evento modelado;\n* Recall: Falsos negativos s\u00e3o mais prejudiciais ao evento modelado;\n\nAp\u00f3s consultar especialistas da \u00e1rea m\u00e9dica, foi poss\u00edvel entender que por conta da caracter\u00edstica do tratamento do Covid-19 ser sintom\u00e1tica. Falsos Positivos n\u00e3o s\u00e3o um problema t\u00e3o grande, pois ser\u00e3o tratados de forma sintom\u00e1tica, enquanto o Falso Negativo pode levar a pessoa ser liberada p\u00f3s medica\u00e7\u00e3o e contaminar outras pessoas.\n\nPor esse motivo, a m\u00e9trica utilizada para otimizar o modelo ser\u00e1 o recall."}}