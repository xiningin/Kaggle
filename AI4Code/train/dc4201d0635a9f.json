{"cell_type":{"11d6d77e":"code","bdb5c80a":"code","9de5fc63":"code","4d559358":"code","210cee0d":"code","11455d04":"code","0a69e8fa":"code","934893a8":"code","e7900d20":"code","038e0fd9":"code","e8f38201":"code","f9bf0958":"code","0f8357b1":"code","d0545b89":"code","f18cb0aa":"code","e8eed76e":"code","4ce959ee":"code","492ed379":"code","09998609":"code","b45a87b2":"code","3f6b1a3c":"code","1d22ec9e":"code","da949ce1":"code","1ab00c89":"code","2b4b2ffc":"code","f2d2e145":"code","d6b16922":"code","b32cef91":"code","e8ba2909":"code","9a6ef515":"code","bbce3f12":"code","1edb401c":"code","6d83969d":"code","e88b3200":"code","f18f433a":"code","54de5665":"code","8761fbf3":"code","bf938cf2":"code","e9a4b0fd":"code","a2e265b6":"code","707dee42":"code","6e764d7f":"markdown","bd164836":"markdown","f2c5fe6d":"markdown","05128082":"markdown","cbf17fec":"markdown","bf0a11f8":"markdown","94d85d09":"markdown","2777c2e2":"markdown","8dc7fb2c":"markdown","100f4d23":"markdown","48ce024a":"markdown","4d99e983":"markdown","1ae995c5":"markdown"},"source":{"11d6d77e":"!ls ..\/input","bdb5c80a":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport sys\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom time import time","9de5fc63":"# Remember to change directory path\ntrain = pd.read_csv(\"..\/input\/train.csv\", parse_dates=['first_active_month'])\ntest = pd.read_csv(\"..\/input\/test.csv\", parse_dates=['first_active_month'])\nprint(train.shape)\nprint(test.shape)","4d559358":"data = pd.concat([train,test])\nprint(data.head(5))","210cee0d":"data.dtypes","11455d04":"data.describe()","0a69e8fa":"target_col = \"target\"\n\nplt.figure(figsize=(8,6))\nplt.scatter(range(train.shape[0]), np.sort(train[target_col].values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('Loyalty Score', fontsize=12)\nplt.show()","934893a8":"plt.figure(figsize=(12,8))\nsns.distplot(train[target_col].values, bins=50, kde=False, color=\"red\")\nplt.title(\"Histogram of Loyalty score\")\nplt.xlabel('Loyalty score', fontsize=12)\nplt.show()","e7900d20":"(train['target']<-30).sum()","038e0fd9":"cnt_srs_1 = train['first_active_month'].dt.date.value_counts()\ncnt_srs_1 = cnt_srs_1.sort_index()\ncnt_srs_2 = test['first_active_month'].dt.date.value_counts()\ncnt_srs_2 = cnt_srs_2.sort_index()\n\nsns.set(rc={'figure.figsize':(14, 6)})\nsns.barplot(cnt_srs_1.index, cnt_srs_1.values, alpha = 0.5, color = 'green')\nsns.barplot(cnt_srs_2.index, cnt_srs_2.values, alpha = 0.5, color = 'red')\n#plt.bar(cnt_srs_1.index, cnt_srs_1.values, alpha = 0.5, color = 'green')\n#plt.bar(cnt_srs_2.index, cnt_srs_2.values, alpha = 0.5, color = 'red')\n\nplt.xticks(rotation = 'vertical')\n#plt.xlabel('First active month', fontsize=12)\n#plt.ylabel('Number of cards', fontsize=12)\n#plt.title(\"First active month count in train set\")\n\nplt.show()","e8f38201":"print(data.feature_1.unique())","f9bf0958":"# feature 1\nplt.figure(figsize=(16,8))\nsns.boxplot(x=\"feature_1\", y=data.target, data=data)\nplt.xticks(rotation='vertical')\nplt.xlabel('Feature 1', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Feature 1 distribution\")\nplt.show()","0f8357b1":"print(data.feature_2.unique())","d0545b89":"# feature 2\nplt.figure(figsize=(16,8))\nsns.boxplot(x=\"feature_2\", y=data.target, data=data)\nplt.xticks(rotation='vertical')\nplt.xlabel('Feature 2', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Feature 2 distribution\")\nplt.show()","f18cb0aa":"print(data.feature_3.unique())","e8eed76e":"# feature 3\nplt.figure(figsize=(16,8))\nsns.boxplot(x=\"feature_3\", y=data.target, data=data)\nplt.xticks(rotation='vertical')\nplt.xlabel('Feature 3', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Feature 3 distribution\")\nplt.show()","4ce959ee":"hist = pd.read_csv('..\/input\/historical_transactions.csv')","492ed379":"hist.head(5)","09998609":"hist.dtypes","b45a87b2":"# Number of historical transactiones for each card_id\ngdf = hist.groupby('card_id')\n#print(gdf.head(5))\n\ngdf = gdf['purchase_amount'].size().reset_index()\nprint(gdf.head(5))\n\ngdf.columns = ['card_id', 'num_hist_transactions']\ntrain = pd.merge(train, gdf, on='card_id', how='left')\ntest = pd.merge(test, gdf, on='card_id', how='left')\ndata = pd.merge(data, gdf, on='card_id', how='left')\nprint(data.head(5))","3f6b1a3c":"bins = [0, 10, 20, 30, 40, 50, 75, 100, 150, 200, 500, 10000]\ntrain['binned_num_hist_transactions'] = pd.cut(train['num_hist_transactions'], bins)\ncnt_srs = train.groupby(\"binned_num_hist_transactions\")['target'].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_num_hist_transactions\", y='target', data=train, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned_num_hist_transactions', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"binned_num_hist_transactions distribution\")\nplt.show()","1d22ec9e":"gdf = hist.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\ngdf.columns = [\"card_id\", \"sum_hist_trans\", \"mean_hist_trans\", \"std_hist_trans\", \"min_hist_trans\", \"max_hist_trans\"]\ntrain = pd.merge(train, gdf, on=\"card_id\", how=\"left\")\ntest = pd.merge(test, gdf, on=\"card_id\", how=\"left\")","da949ce1":"bins = np.percentile(train[\"sum_hist_trans\"], range(0,101,10))\ntrain['binned_sum_hist_trans'] = pd.cut(train['sum_hist_trans'], bins)\n#cnt_srs = train_df.groupby(\"binned_sum_hist_trans\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_sum_hist_trans\", y='target', data=train, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned_sum_hist_trans', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Sum of historical transaction value (Binned) distribution\")\nplt.show()","1ab00c89":"gdf = hist.groupby(\"card_id\")\ngdf = gdf[\"installments\"].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\ngdf.columns = [\"card_id\", \"sum_hist_installments\", \"mean_hist_installments\", \"std_hist_installments\", \n               \"min_hist_installments\", \"max_hist_installments\"]\ntrain = pd.merge(train, gdf, on=\"card_id\", how=\"left\")\ntest = pd.merge(test, gdf, on=\"card_id\", how=\"left\")","2b4b2ffc":"bins = np.percentile(train[\"sum_hist_installments\"], range(0,101,10))\ntrain['binned_sum_hist_installments'] = pd.cut(train['sum_hist_installments'], bins, duplicates = 'drop')\n#cnt_srs = train_df.groupby(\"binned_sum_hist_trans\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_sum_hist_installments\", y='target', data=train, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned_sum_hist_installments', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Sum of historical transaction installments (Binned) distribution\")\nplt.show()","f2d2e145":"new_trans = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")","d6b16922":"new_trans.head(5)","b32cef91":"new_trans.dtypes","e8ba2909":"gdf = new_trans.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].size().reset_index()\ngdf.columns = [\"card_id\", \"num_merch_transactions\"]\ntrain = pd.merge(train, gdf, on=\"card_id\", how=\"left\")\ntest = pd.merge(test, gdf, on=\"card_id\", how=\"left\")","9a6ef515":"bins = [0, 10, 20, 30, 40, 50, 75, 10000]\ntrain['binned_num_merch_transactions'] = pd.cut(train['num_merch_transactions'], bins)\ncnt_srs = train.groupby(\"binned_num_merch_transactions\")['target'].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_num_merch_transactions\", y='target', data=train, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned_num_merch_transactions', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Number of new merchants transaction (Binned) distribution\")\nplt.show()","bbce3f12":"gdf = new_trans.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\ngdf.columns = [\"card_id\", \"sum_merch_trans\", \"mean_merch_trans\", \"std_merch_trans\", \"min_merch_trans\", \"max_merch_trans\"]\ntrain = pd.merge(train, gdf, on=\"card_id\", how=\"left\")\ntest = pd.merge(test, gdf, on=\"card_id\", how=\"left\")","1edb401c":"bins = np.nanpercentile(train[\"sum_merch_trans\"], range(0,101,10))\ntrain['binned_sum_merch_trans'] = pd.cut(train['sum_merch_trans'], bins)\n#cnt_srs = train_df.groupby(\"binned_sum_hist_trans\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_sum_merch_trans\", y='target', data=train, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned sum of new merchant transactions', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Sum of New merchants transaction value (Binned) distribution\")\nplt.show()","6d83969d":"gdf = new_trans.groupby(\"card_id\")\ngdf = gdf[\"installments\"].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\ngdf.columns = [\"card_id\", \"sum_merch_installments\", \"mean_merch_installments\", \"std_merch_installments\", \n               \"min_merch_installments\", \"max_merch_installments\"]\ntrain = pd.merge(train, gdf, on=\"card_id\", how=\"left\")\ntest = pd.merge(test, gdf, on=\"card_id\", how=\"left\")","e88b3200":"bins = np.nanpercentile(train[\"sum_merch_installments\"], range(0,101,10))\ntrain['binned_sum_merch_installments'] = pd.cut(train['sum_merch_installments'], bins, duplicates = 'drop')\n#cnt_srs = train_df.groupby(\"binned_sum_hist_trans\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_sum_merch_installments\", y='target', data=train, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned sum of new merchant transactions installments', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Sum of New merchants transaction installments value (Binned) distribution\")\nplt.show()","f18f433a":"train[\"year\"] = train[\"first_active_month\"].dt.year\ntrain[\"month\"] = train[\"first_active_month\"].dt.month\ntest[\"month\"] = test[\"first_active_month\"].dt.month\ntest[\"year\"] = test[\"first_active_month\"].dt.year\n\n# data['year'] = data['first_active_month'].dt.year\n# data['month'] = data['first_active_month'].dt.month\n\ncols_to_use = [\"feature_1\", \"feature_2\", \"feature_3\", \"year\", \"month\", \n               \"num_hist_transactions\", \"sum_hist_trans\", \"mean_hist_trans\", \"std_hist_trans\", \n               \"min_hist_trans\", \"max_hist_trans\",\n               \"sum_hist_installments\", \"mean_hist_installments\", \"std_hist_installments\", \n               \"min_hist_installments\", \"max_hist_installments\",            \n               \"num_merch_transactions\", \"sum_merch_trans\", \"mean_merch_trans\", \"std_merch_trans\",\n               \"min_merch_trans\", \"max_merch_trans\",\n               \"sum_merch_installments\", \"mean_merch_installments\", \"std_merch_installments\",\n               \"min_merch_installments\", \"max_merch_installments\",\n              ]\n\n\ntrain_X = train[cols_to_use]\ntrain_y = train['target'].values\ntest_X = test[cols_to_use]\nprint(train_X.shape[0])\nprint(train_y.shape[0])","54de5665":"# checking minable view: get into consideration there are missing values \n# for indicators coming from new_merchants_transactions file.\nprint(train_X.head(5))\nprint(train_X.info(5))\nprint(train_X.isnull().sum())","8761fbf3":"print(train_y[:5])\nprint(np.info(train_y))\nprint(np.isnan(train_y).sum())","bf938cf2":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nimport warnings\nimport time\nimport sys\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\n\nparam = {'num_leaves': 100,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': 6,\n         'learning_rate': 0.005,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}","e9a4b0fd":"folds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(train[cols_to_use]))\npredictions = np.zeros(len(test[cols_to_use]))\nstart = time.time()\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train[cols_to_use].values, train['target'].values)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][cols_to_use], label=train['target'].iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][cols_to_use], label=train['target'].iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][cols_to_use], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = cols_to_use\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[cols_to_use], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, train['target'])**0.5))","a2e265b6":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","707dee42":"sub_df = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"submit.csv\", index=False)","6e764d7f":"### purchase_amount as indicator","bd164836":"Check out this 2207 input values that might be either outliers or erros. Take them into consideration.","f2c5fe6d":"Overview of the columns\nThe field descriptions are as follows:\n\n    - card_id - Card identifier\n    - month_lag - month lag to reference date\n    - purchase_date - Purchase date\n    - authorized_flag - 'Y' if approved, 'N' if denied\n    - category_3 - anonymized category\n    - installments - number of installments of purchase\n    - category_1 - anonymized category\n    - merchant_category_id - Merchant category identifier (anonymized )\n    - subsector_id - Merchant category group identifier (anonymized )\n    - merchant_id - Merchant identifier (anonymized)\n    - purchase_amount - Normalized purchase amount\n    - city_id - City identifier (anonymized )\n    - state_id - State identifier (anonymized )\n    - category_2 - anonymized category\n    \nNow let us make some features based on the historical transactions and merge them with train and test set.","05128082":"### installments as indicator","cbf17fec":"# Model tuning","bf0a11f8":"# Baseline: minable view","94d85d09":"## Historical transactions, aggregated by different indicators","2777c2e2":"# Exploring data: train, test files.","8dc7fb2c":"# Exploring data: historical_transactions file.","100f4d23":"# Exploring data: new_merchants_transactions file.","48ce024a":"### installments as indicator","4d99e983":"### purchase_amount as indicator","1ae995c5":"## Overview of the columns: historical_transactions file."}}