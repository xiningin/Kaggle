{"cell_type":{"9c5e3419":"code","1ad428d0":"code","407b60b8":"code","92bc35c8":"code","8658969c":"code","c5923069":"code","88b48f03":"code","aa7b7a8b":"code","8d41f811":"code","12e53bc6":"code","254b1447":"code","100bfcb1":"code","679fe7a9":"code","68446983":"code","e0a8fba0":"code","e36da1af":"code","521cebe2":"code","b7bda2dd":"code","a2f24583":"code","2746b650":"code","e0fb9278":"code","af0fe53f":"code","1deb8143":"code","00860257":"markdown","4fc0c70a":"markdown","ca5bfcfb":"markdown","61843d70":"markdown","1c3948f1":"markdown","9cc3c4b1":"markdown","23f50833":"markdown","6214f0c7":"markdown","007e4dfe":"markdown","c32384c1":"markdown","6f171af2":"markdown","fabf7226":"markdown","d79b157c":"markdown","1e826ea3":"markdown","68b9e894":"markdown","1e98508a":"markdown"},"source":{"9c5e3419":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","1ad428d0":"from fastai.vision import *","407b60b8":"import os\ncwd = os.getcwd()\nprint(cwd)\npath_img = Path('..\/input\/images\/images')","92bc35c8":"fnames = get_image_files(path_img)\nfnames[:5]","8658969c":"np.random.seed(2) #random seed for reproducibility\npat = r'\/([^\/]+)_\\d+.jpg$' #regex expression","c5923069":"sz = 128\nbs = 64","88b48f03":"data = ImageDataBunch.from_name_re('.', fnames, pat, ds_tfms=get_transforms(), size=sz, bs=bs\n                                  ).normalize(imagenet_stats)","aa7b7a8b":"data.show_batch(rows=3, figsize=(7,6))","8d41f811":"print(data.classes)\nlen(data.classes),data.c","12e53bc6":"from fastai.metrics import accuracy\nlearn = create_cnn(data, models.resnet50, metrics=accuracy)","254b1447":"learn.lr_find()","100bfcb1":"learn.recorder.plot()","679fe7a9":"learn.fit_one_cycle(4,max_lr=1e-2)","68446983":"learn.save('stage-1')","e0a8fba0":"learn.unfreeze()","e36da1af":"learn.lr_find()","521cebe2":"learn.recorder.plot()","b7bda2dd":"learn.fit_one_cycle(4, max_lr=slice(1e-6,1e-4))","a2f24583":"learn.save('stage-2')","2746b650":"interp = ClassificationInterpretation.from_learner(learn)\nlosses,idxs = interp.top_losses()\nlen(data.valid_ds)==len(losses)==len(idxs)","e0fb9278":"interp.plot_top_losses(9, figsize=(15,11))","af0fe53f":"interp.plot_confusion_matrix(figsize=(12,12), dpi=60)","1deb8143":"interp.most_confused(min_val=2)","00860257":"When having so many classes, it can be hard to interpret the confusion matrix. Fastai has the `interp.most_confused()` function to print out which classes the network was most confused by.","4fc0c70a":"We will utilize the learning rate finder to find the optimal learning rate for training. The idea behind this is that we will train the network while increasing the learning rate, and observe how the loss decreases. We choose the learning rate for when the loss decreases the fastest.","ca5bfcfb":"# Training: Resnet50\n\nWe will now start training a new model. We will take a convolutional neural network architecture (ResNet50) pretrained on the [ImageNet](http:\/\/www.image-net.org\/) dataset, an add a new layer to output the predictions for our given dataset. We will then train this last layer. The idea behind this is that the ResNet50 trained on the ImageNet dataset already has learnt to identify important features and does not need to be retrained from scratch. So we only train the last layer for our particular task.","61843d70":"Note that we will resize the images to `sz=128` and load our data into the neural network with a batch size of `bs=64`. We use default transformations for image augmentation to improve training and generalization. \n\nLet's look at some sample data:","1c3948f1":"We see that the optimal learning rate seems to be around `lr=1e-2` so that is what we will train the model with:","9cc3c4b1":"# Conclusions and Acknowledgements\nI hope this starter kernel helps you get started with the Oxford-IIIT Pet dataset and Fastai. Thanks to the [fast.ai](fast.ai) course and library!\n\n**If you enjoyed this kernel, please give it an upvote!**","23f50833":"We can also create a confusion matrix:","6214f0c7":"We will use a regex expression to extract the labels of the images and load into the `ImageDataBunch` class, which is the class for inputting image data into the neural network for the fastai library.","007e4dfe":"We can plot and see which images the neural network was most confused by:","c32384c1":"This is a very accurate model!","6f171af2":"# Looking at the data","fabf7226":"# Fine-tuning \nWe will now unfreeze the rest of the model and train the rest of the layers as well. We will use lower learning rate for the earlier layers of the model, since these layers are probably detecting simple features like lines or circles, and don't need to be adjusted much. However, the later layers are adjusted more, with higher learning rates. This technique is known as discriminative learning rates.\n\nWe will first determine the optimal learning rate again.","d79b157c":"# Introduction to the dataset\nThe Oxford-IIIT Pet Dataset is a 37 category pet dataset with roughly 200 images for each class created by the Visual Geometry Group at Oxford. The images have a large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation. Here we will use the fastai library to train a model for multi-class classification of cat and dog breeds. Future kernels will look at bounding-box detection and image segmentation with the fastai library.\n\nThis kernel was inspired by [lesson 1](https:\/\/github.com\/fastai\/course-v3\/blob\/master\/nbs\/dl1\/lesson1-pets.ipynb) of the [2019 version of fast.ai Part 1](https:\/\/course.fast.ai)","1e826ea3":"We have 37 classes as we expected.","68b9e894":"# Results\/Interpretation\n\nFastai provides some nice tools for checking our results:","1e98508a":"Let's import all the necessary libraries. This is very easy with fastai, as the computer vision functionality is part of the `fastai.vision` module:"}}