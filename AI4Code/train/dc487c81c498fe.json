{"cell_type":{"c513927b":"code","bb9b01a1":"code","0a936a59":"code","64be593c":"code","ed71f87a":"code","703eafb3":"code","808035c8":"code","47c99b22":"code","7a61ce7d":"code","4cb59ab4":"code","e023b0dc":"code","f58b6d4f":"code","6cee1496":"code","77898dbd":"code","a068b386":"code","748ced57":"code","56813094":"code","0e55381d":"code","ff321841":"code","5ba58064":"code","0f5aa9a7":"code","a4d7dd0b":"code","0d276fa9":"code","06cdf67a":"code","f846b09f":"code","36d6a9d0":"code","d78b9760":"code","15dc7ea1":"code","15a6f35a":"code","79111137":"code","8f83cdb6":"code","671382db":"code","3ce0ebca":"code","83d039be":"code","047e3d84":"code","8078246c":"code","34d33321":"code","0ad23615":"code","513fa3a7":"markdown","173d88e1":"markdown","33cda833":"markdown","a4b6e811":"markdown","573877b4":"markdown"},"source":{"c513927b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bb9b01a1":"!ls -lrtah \/kaggle\/input\/sign-language-digits-dataset","0a936a59":"!ls -lrtah \/kaggle\/input\/sign-language-digits-dataset\/Sign-language-digits-dataset","64be593c":"PATH = '\/kaggle\/input'\nDATASET = 'sign-language-digits-dataset'","ed71f87a":"X = np.load(f'{PATH}\/{DATASET}\/X.npy')\nY = np.load(f'{PATH}\/{DATASET}\/Y.npy')","703eafb3":"X.shape, Y.shape","808035c8":"X[1]","47c99b22":"Y[1]","7a61ce7d":"X = np.concatenate((X[204:409,:],\n                  X[822:1028,:],\n                  X[1649:1855,:],\n                  X[1443:1649,:],\n                  X[1236:1443,:],\n                  X[1855:2062,:],\n                  X[615:822,:],\n                  X[409:615,:],\n                  X[1028:1236,:],\n                  X[0:204,:]),axis = 0)","4cb59ab4":"X_3d = X.reshape(-1, 64, 64, 1)\nX_3d.shape","e023b0dc":"X_3d[0]","f58b6d4f":"import matplotlib.pyplot as plt","6cee1496":"plt.figure(figsize=(20,6))\n\nfor i,j in enumerate([0,205,411,617,823,1030,1237,1444,1650,1858]):\n    plt.subplot(2,5,i+1)\n    plt.subplots_adjust(top = 2, bottom = 1)\n    plt.imshow(X[j].reshape(64,64))\n    plt.title(np.argmax(Y[j]))\n    plt.axis('off')","77898dbd":"plt.imshow(X_3d[0, :, :, 0])\nY[0]","a068b386":"plt.matshow(X_3d[64, :, :, 0])\nplt.axis('off')\nY[786]","748ced57":"X_3d[1, 0, 0, 0]","56813094":"X_2d = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\nX_2d.shape","0e55381d":"X_2d[0]","ff321841":"plt.imshow(X_2d[501].reshape(X.shape[1], X.shape[2]))\nY[501]","5ba58064":"X.shape[1] * X.shape[2]","0f5aa9a7":"Y_2d = np.argmax(Y, axis=1).reshape(Y.shape[0], 1)","a4d7dd0b":"Y_2d.shape","0d276fa9":"Y_2d","06cdf67a":"def show_model_history(modelHistory, model_name):\n    history=pd.DataFrame()\n    history[\"Train Loss\"] = modelHistory.history['loss']\n    history[\"Validation Loss\"] = modelHistory.history['val_loss']\n    history[\"Train Accuracy\"] = modelHistory.history['accuracy']\n    history[\"Validation Accuracy\"] = modelHistory.history['val_accuracy']\n    \n    fig, axarr=plt.subplots(nrows=2, ncols=1 ,figsize=(12,8))\n    \n    axarr[0].set_title(\"History of Loss in Train and Validation Datasets\")\n    history[[\"Train Loss\", \"Validation Loss\"]].plot(ax=axarr[0])\n    \n    axarr[1].set_title(\"History of Accuracy in Train and Validation Datasets\")\n    history[[\"Train Accuracy\", \"Validation Accuracy\"]].plot(ax=axarr[1]) \n    \n    plt.suptitle(\" Convulutional Model {} Loss and Accuracy in Train and Validation Datasets\".format(model_name))\n    plt.show()","f846b09f":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam, RMSprop","36d6a9d0":"from sklearn.model_selection import train_test_split","d78b9760":"print(\"Original Shape : \", X.shape, Y.shape)\nprint(\"2D Shape       : \", X_2d.shape, Y_2d.shape)\nprint(\"3D Shape       : \", X_3d.shape, Y.shape)","15dc7ea1":"history","15a6f35a":"from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\nfrom keras.callbacks import EarlyStopping","79111137":"model_CNN_1 = None\n\nmodel_CNN_1 = Sequential()\n#1: Con - Pool - BN - D\nmodel_CNN_1.add(\n    Conv2D(\n        filters = 64,\n        kernel_size = (4, 4),\n        strides = (2, 2), # 2\n        padding = 'same',\n        activation = 'relu',\n        input_shape = X_3d[0].shape\n    )\n)\nmodel_CNN_1.add(\n    MaxPooling2D(\n        pool_size = (2, 2),\n        strides = (1, 1), # 1\n        padding = 'valid'\n    )\n)\nmodel_CNN_1.add(BatchNormalization())\nmodel_CNN_1.add(Dropout(0.20))\n#2: Con - Pool - BN - D\nmodel_CNN_1.add(\n    Conv2D(\n        filters = 96,\n        kernel_size = (3, 3),\n        strides = (1, 1), # 1\n        padding = 'valid',\n        activation = 'relu'\n    )\n)\nmodel_CNN_1.add(\n    MaxPooling2D(\n        pool_size = (2, 2),\n        strides = (1, 1), # 1\n        padding = 'valid'\n    )\n)\nmodel_CNN_1.add(BatchNormalization())\nmodel_CNN_1.add(Dropout(0.20))\n#3: Con - Pool - BN - D7\nmodel_CNN_1.add(\n    Conv2D(\n        filters = 32,\n        kernel_size = (3, 3),\n        strides = (2, 2), # 2\n        padding = 'same',\n        activation = 'relu'\n    )\n)\nmodel_CNN_1.add(\n    MaxPooling2D(\n        pool_size = (2, 2),\n        strides = (1, 1), # 1\n        padding = 'valid'\n    )\n)\nmodel_CNN_1.add(BatchNormalization())\nmodel_CNN_1.add(Dropout(0.20))\n#1 : ANN\nmodel_CNN_1.add(\n    Flatten()\n)\nmodel_CNN_1.add(\n    Dense(\n        units = 256,\n        activation = 'relu'\n    )\n)\nmodel_CNN_1.add(BatchNormalization())\n#3 : ANN\nmodel_CNN_1.add(\n    Dense(\n        units = 128,\n        activation = 'relu'\n    )\n)\nmodel_CNN_1.add(BatchNormalization())\n#4 : ANN\nmodel_CNN_1.add(\n    Dense(\n        units = 64,\n        activation = 'relu'\n    )\n)\nmodel_CNN_1.add(BatchNormalization())\n#5 : ANN\nmodel_CNN_1.add(\n    Dense(\n        units = 32,\n        activation = 'relu'\n    )\n)\nmodel_CNN_1.add(BatchNormalization())\n#N : ANN\nmodel_CNN_1.add(\n    Dense(\n        units = Y.shape[1],\n        activation = 'softmax'\n    )\n)\n\nmodel_CNN_1.summary()\n\n# Data\nX_train, X_test, Y_train, Y_test = train_test_split(X_3d, Y, stratify=Y, test_size=0.25, random_state=20)\n\n#optimizer = RMSprop(lr=0.001)\noptimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999)\n\nearlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose = 1) \n\nmodel_CNN_1.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics='accuracy')\n\nhistory_CNN_1 = model_CNN_1.fit(X_train, Y_train, epochs=100, validation_data=(X_test, Y_test), callbacks=[earlyStopping])","8f83cdb6":"show_model_history(history_CNN_1, \"CNN_1\")","671382db":"plt.imshow(X_3d[1111, :, :, 0])","3ce0ebca":"X_3d[1111].reshape(1, 64, 64, 1).shape","83d039be":"y_pred = model_CNN_1.predict(X_3d[1111].reshape(1, 64, 64, 1))\ny_pred","047e3d84":"np.argmax(y_pred)","8078246c":"import plotly.express as px","34d33321":"px.bar(x=range(10), y=list(y_pred[0]), title='Predicted Sign Value')","0ad23615":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix\n# Seaborn plotting\nimport seaborn as sns\n# Predict the values from the validation dataset\nY_pred = model_CNN_1.predict(X_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_test,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"BuPu\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","513fa3a7":"model_ANN_CC = None\n\nmodel_ANN_CC = Sequential()\n\nmodel_ANN_CC.add(\n    Dense(\n        64 * 64 * 2, activation='relu', input_shape=(X.shape[1] * X.shape[2], )\n    )\n)\nmodel_ANN_CC.add(\n    Dense(\n        64 * 64 * 2 * 2, activation='relu'\n    )\n)\nmodel_ANN_CC.add(\n    Dense(\n        64 * 64 * 2, activation='relu'\n    )\n)\nmodel_ANN_CC.add(\n    Dense(\n        64 * 64 \/\/ 2, activation='relu'\n    )\n)\nmodel_ANN_CC.add(\n    Dense(\n        64 * 64 \/\/ 4, activation='relu'\n    )\n)\nmodel_ANN_CC.add(\n    Dense(\n        64 * 64 \/\/ 16, activation='relu'\n    )\n)\nmodel_ANN_CC.add(\n    Dense(\n        64 * 64 \/\/ 64, activation='relu'\n    )\n)\nmodel_ANN_CC.add(\n    Dense(\n        64 * 64 \/\/ 64 \/\/ 4, activation='relu'\n    )\n)\nmodel_ANN_CC.add(\n    Dense(\n        10, activation='softmax'\n    )\n)\n\nmodel_ANN_CC.summary()\n\noptimizer = Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n\nmodel_ANN_CC.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics='accuracy')\n\nhistory = model_ANN_CC.fit(X_2d, Y, epochs = 20, validation_split=0.3)","173d88e1":"for x,y in enumerate(y_pred[0]):\n    print(x, y)","33cda833":"model_ANN_BC = None\n\nmodel_ANN_BC = Sequential()\n\nmodel_ANN_BC.add(\n    Dense(\n        64 * 64 * 2, activation='relu', kernel_initializer = 'uniform', input_shape=(X.shape[1] * X.shape[2], )\n    )\n)\nmodel_ANN_BC.add(\n    Dense(\n        64 * 64 * 2 * 2, kernel_initializer = 'uniform', activation='relu'\n    )\n)\nmodel_ANN_BC.add(\n    Dense(\n        64 * 64 * 2, kernel_initializer = 'uniform', activation='relu'\n    )\n)\nmodel_ANN_BC.add(\n    Dense(\n        64 * 64 \/\/ 2, kernel_initializer = 'uniform', activation='relu'\n    )\n)\nmodel_ANN_BC.add(\n    Dense(\n        64 * 64 \/\/ 4, kernel_initializer = 'uniform', activation='relu'\n    )\n)\nmodel_ANN_BC.add(\n    Dense(\n        64 * 64 \/\/ 16, kernel_initializer = 'uniform', activation='relu'\n    )\n)\nmodel_ANN_BC.add(\n    Dense(\n        64 * 64 \/\/ 64, kernel_initializer = 'uniform', activation='relu'\n    )\n)\nmodel.add(\n    Dense(\n        64 * 64 \/\/ 64 \/\/ 4, kernel_initializer = 'uniform', activation='relu'\n    )\n)\nmodel.add(\n    Dense(\n        1, kernel_initializer = 'uniform', activation='sigmoid'\n    )\n)\n\nmodel_ANN_BC.summary()\n\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n\nmodel_ANN_BC.compile(loss='binary_crossentropy', optimizer=optimizer, metrics='accuracy')\n\nhistory = model_ANN_BC.fit(X_2d, Y_2d, epochs = 100, validation_split=0.3)","a4b6e811":"Binary Output i.e. 1","573877b4":"Categorical Output i.e. 10"}}