{"cell_type":{"5d6bfbee":"code","bb45d182":"code","f9ebb18e":"code","66e98c65":"code","99baf90c":"code","4dc286bb":"code","2e03a12a":"code","a3a28f71":"code","37745938":"code","d8d38e19":"code","c7de814b":"code","f6e51f19":"code","4c825469":"code","be2ec6f1":"code","a4a6886d":"code","5d3161bc":"code","fcd5f1e7":"code","d173b6b0":"code","ec935f27":"code","7c0fcbf9":"code","19cb4785":"code","d51e11b1":"code","8196041a":"code","c0656b6b":"code","bc87a023":"code","d8c5fdae":"code","c9c543db":"code","6e2e49e1":"code","abb8aacc":"code","a3e70edb":"code","d40fbce3":"code","7bee2ca3":"code","ec787570":"code","4db7d3ff":"code","bce14e07":"code","fc70381a":"code","e80d954c":"code","f065c207":"code","1f14fab9":"code","49313954":"code","9c54e944":"code","bbe25df8":"code","99567199":"code","f41bb63d":"code","acff81b7":"code","b70786e5":"code","82210c52":"code","1302de82":"code","6e41537a":"code","7598d00b":"markdown","7539e474":"markdown","385d1e06":"markdown","33df52eb":"markdown","9ebcddba":"markdown","6b4196c4":"markdown","e0b5562c":"markdown","8878a7c1":"markdown","4010a229":"markdown","127c1b5c":"markdown","e8afe372":"markdown","1544a95d":"markdown","42650bd7":"markdown","e403460b":"markdown"},"source":{"5d6bfbee":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as smt\nfrom matplotlib import pyplot as plt\nfrom datetime import datetime as dt\n\n#reading data into dataframes\nbc = pd.read_csv(\"BTC-USD.csv\", index_col=0, parse_dates=True)\nusd = pd.read_csv(\"USD.csv\", index_col=0, parse_dates=True)","bb45d182":"#displaying bitcoin dataframe\nbc = bc.drop(bc.columns[[0,1,2,4]], axis=1)\nbc.tail()","f9ebb18e":"#displaying usd dataframe\nusd = usd.drop(usd.columns[[1,2,3,4,5]], axis=1)\nusd.head()","66e98c65":"print(bc.shape)\nprint(usd.shape)","99baf90c":"bc.describe()","4dc286bb":"usd.describe()","2e03a12a":"# Fancy way of joining two dataframes together\njoin = bc\nfor x in join.index.values:\n    for x2 in usd.index.values:\n        if x == x2:\n            itemindex = np.where(usd.index==x2)[0][0]\n            val = usd.at[x2, \"Price\"]\n            join.at[x, 'Price'] = val\n\n\njoin.head()","a3a28f71":"# Practical way of joining two dataframes together\njoin2=pd.merge(usd,bc, how='inner', left_index=True, right_index=True)\njoin2['Price'] = join2['Price_y']\njoin2 = join2[['Close','Volume','Price']]","37745938":"join2.head()","d8d38e19":"join2 = join2.sort_values(by='Date')","c7de814b":"join2.head()","f6e51f19":"join = join.dropna()\njoin.plot(subplots=True, figsize=(12,8))","4c825469":"join2.plot(subplots=True, figsize=(12,8))","be2ec6f1":"def plotcharts(y, title, lags=None, figsize=(12,8)):\n    # formatting the output of the graphs nicely\n    fig = plt.figure(figsize=figsize)\n    layout = (2,2)\n    ts_ax = plt.subplot2grid(layout, (0,0))\n    hist_ax = plt.subplot2grid(layout, (0,1))\n    acf_ax = plt.subplot2grid(layout, (1,0))\n    pacf_ax = plt.subplot2grid(layout, (1,1))\n    \n    # Plotting just the time series\n    y.plot(ax=ts_ax)\n    ts_ax.set_title(title, fontsize=14, fontweight=\"bold\")\n    # Plotting the distribution\n    y.plot(ax=hist_ax, kind=\"hist\", bins=25)\n    hist_ax.set_title(\"Histogram\")\n    # Using the statsmodel package to plot the autocorrelation\n    smt.graphics.tsa.plot_acf(y, lags=lags, ax=acf_ax)\n    smt.graphics.tsa.plot_pacf(y, lags=lags, ax=pacf_ax)\n    #[ax.set_xlim(0) for ax in [acf_ax, pacf_ax]]\n    plt.tight_layout()\n    return ts_ax, acf_ax, pacf_ax","a4a6886d":"series = join","5d3161bc":"num_var = len(series.iloc[1,:])\nfor i in range(0, num_var):\n    plotcharts(series.iloc[:,i].dropna(), title=series.columns[i], lags=48)","fcd5f1e7":"#log transformation\n# Converting the data to a logarithmic scale\nlog = pd.DataFrame(np.log(series))\n\n# Differencing the log values twice. \nlog_diff = log.diff().dropna()\nlog_diff = log_diff.diff().dropna()\nlog_diff.plot(subplots=True)\n\n# Normailizing Data for comparison\n#importing minmaxscaler \nfrom sklearn.preprocessing import MinMaxScaler\n\n#creating minmaxscaler object\n#Transform features by scaling each feature to a given range.\n#This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\nnorm = MinMaxScaler()\n\n# Now we're going to  normalize those log differences between 0 and 1 and take another look at those graphs.\njoin_norm = pd.DataFrame(norm.fit_transform(log_diff), columns=log_diff.columns)\njoin_norm","d173b6b0":"num_var = len(join_norm.iloc[1,:])\nfor i in range(0, num_var):\n    plotcharts(join_norm.iloc[:,i].dropna(), title=join_norm.columns[i], lags=48)","ec935f27":"num_var = len(join.iloc[1,:])\nfor i in range(0, num_var):\n    plotcharts(join.iloc[:,i].dropna(), title=join.columns[i], lags=48)","7c0fcbf9":"from statsmodels.tsa.stattools import adfuller\n\ndef ad_test(dataset):\n    dftest = adfuller(dataset, autolag= \"AIC\")\n    print(\"1. ADF : \", dftest[0])\n    print(\"2. P-Value : \", dftest[1])\n    print(\"3. Num Of Lags : \", dftest[2])\n    print(\"4. Num of Observations Used For ADF Regression and Critical Values Calculation : \", dftest[3])\n    print(\"5. Critical Values : \")\n    for key, val in dftest[4].items():\n        print(\"\\t\", key, \": \", val)\n        \nad_test(log_diff[\"Close\"])\nprint(\"--------------------------------------------\")\nad_test(log_diff[\"Volume\"])\nprint(\"--------------------------------------------\")\nad_test(log_diff[\"Price\"])\n","19cb4785":"# Splitting the dataset into train & test subsets\nn_obs = 10\ntrain, test = log_diff[:-n_obs], log_diff[-n_obs:]\nlog_diff.head()","d51e11b1":"# Fitting the VAR model\nfrom statsmodels.tsa.api import VAR\n\nmodel = VAR(log_diff)\nresults = model.fit(maxlags = 200, ic = 'aic')\nresults.summary()","8196041a":"lag_order = results.k_ar\npredicted = results.forecast(log_diff.values[-lag_order:],n_obs)\nforecast = pd.DataFrame(predicted, index = log_diff.index[-n_obs:], columns = log_diff.columns)\n\nlag_order","c0656b6b":"# Inverting the Differencing Transformation\ndef invert_transformation(df, df_forecast, second_diff):\n    for col in df.columns:\n        # Undo the 2nd Differencing\n        if second_diff:\n            df_forecast[str(col)] = (df[col].iloc[-1] - df[col].iloc[-2]) + df_forecast[str(col)].cumsum()\n        # Undo the 1st Differencing\n        df_forecast[str(col)] = df[col].iloc[-1] + df_forecast[str(col)].cumsum()\n\n    return df_forecast\n\nforecast_values = invert_transformation(train, forecast, second_diff=True)\nforecast_values","bc87a023":"# Actual vs Forecasted Plots\nfig, axes = plt.subplots(nrows = int(len(log_diff.columns)\/2), ncols = 3, dpi = 100, figsize = (10,10))\n\nfor i, (col,ax) in enumerate(zip(log_diff.columns, axes.flatten())):\n    forecast_values[col].plot(color = '#F4511E', legend = True, ax = ax).autoscale(axis =' x',tight = True)\n    test[col].plot(color = '#3949AB', legend = True, ax = ax)\n\n    ax.set_title(col + ' - Actual vs Forecast')\n    ax.xaxis.set_ticks_position('none')\n    ax.yaxis.set_ticks_position('none')\n\n    ax.spines[\"top\"].set_alpha(0)\n    ax.tick_params(labelsize = 6)\n\nplt.tight_layout()\nplt.show()\n\n# Red are predicted values and blue are actual values","d8c5fdae":"from sklearn.metrics import mean_squared_error\nfrom numpy import asarray as arr\nmse = mean_squared_error(test, forecast_values)\nprint(\"\\nMean Squared Error: \", mse)","c9c543db":"#importing xgboost and train_test_split\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n#displaying dataset\nlog_diff.head()","6e2e49e1":"# Splitting the dataset into train & test subsets\nX, y = log_diff.iloc[:,:], log_diff.iloc[:,:-2]\n\n# We have to put our data in a Dmatrix because that is the type of object that XGBoost uses\ndata_dmatrix = xgb.DMatrix(data=X,label=y)\n# 80% of the data is being used to train and 20 percent is being used to test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","abb8aacc":"y_test.head()\n","a3e70edb":"# Decide what is important in your model and investigate what parameters are most important to you\nxg_reg = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators = 200)","d40fbce3":"# xgboost uses regularization to ensure we are not overfitting the data\nxg_reg.fit(X_train,y_train)\n\n\n# This is the function where you would feed in new data to get predictions\npreds = xg_reg.predict(X_test)","7bee2ca3":"# Mean Squared Errors need to be compared in the same units.\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))","ec787570":"rmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))","4db7d3ff":"preds\ny_test['predictions'] = preds.tolist()","bce14e07":"y_test = y_test.sort_index()","fc70381a":"y_test","e80d954c":"vis = y_test.tail(50)","f065c207":"# Be careful using models and packages that you are not familiar with.\n# These results are extremely good but I would need to spend a lot more time looking at the theory in order to be able to present these results.\nvis.plot()","1f14fab9":"\njoin_diff = join.diff().dropna()\njoin_diff = join_diff.diff().dropna()\nX, y = join_diff.iloc[:,:], join_diff.iloc[:,:-2]\ndata_dmatrix2 = xgb.DMatrix(data=X,label=y)","49313954":"\n# 80% of the data is being used to train and 20 percent is being used to test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n# Decide what is important in your model and investigate what parameters are most important to you\nxg_reg = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators = 200)\n# xgboost uses regularization to ensure we are not overfitting the data\nxg_reg.fit(X_train,y_train)\n\npreds = xg_reg.predict(X_test)\n# Mean Squared Errors need to be compared in the same units.\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))\n\ny_test['predictions'] = preds.tolist()\nvis2 = y_test.tail(50)\nvis2.plot()","9c54e944":"data_dmatrix","bbe25df8":"#importing prophet and fitting model\nfrom fbprophet import Prophet\nfrom fbprophet.plot import plot_plotly, plot_components_plotly","99567199":"#functions to automate prophet charts for each series\n#within these created functions are the functions that come from fb prophet.\ndef fit_model(df):\n    m = Prophet(daily_seasonality=True)\n    m.fit(df)\n    future = m.make_future_dataframe(periods=365)\n    forecast = m.predict(future)\n    return m, forecast, future\n\ndef fb_plots(m, fore):\n    return plot_plotly(m, forecast)\n\ndef fb_subplots(m, fore):\n    return m.plot(forecast), m.plot_components(forecast)","f41bb63d":"bc = pd.read_csv(\"BTC-USD.csv\")\nbc_price = bc.drop(bc.columns[[1,2,3,5,6]], axis=1)\nbc_vol = bc.drop(bc.columns[[1,2,3,4,5]], axis=1)\n# Date must be labelled ds and \nbc_price.columns = [\"ds\",\"y\"]\nbc_vol.columns = [\"ds\",\"y\"]\nbc_price.head()","acff81b7":"model, forecast, future = fit_model(bc_price)","b70786e5":"future.tail()","82210c52":"forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","1302de82":"fb_plots(model, forecast)","6e41537a":"fb_subplots(model, forecast)","7598d00b":"Here we plot four essential graphs that provide us insight into the closing price of bitcoin over this series of time. The bottom two graphs are the graphs that allow you to look at the dependency structure. They're called the autocorrelation and partial autocorrection function graphs. Each bar on in the autocorrelation function graph captures the series itself and how it is correlated with it's own past. These sort of graphs are very useful for identifying the order of our model.","7539e474":"## Stationary Condition\n\nTime series data consists of primary four components:\n\n1. Trend \n2. Seasonality\n3. Cyclically\n4. Irregularity, sometimes referred to as the random component\n\n#### Trend\n\n- Trend is the increase or decrease in the series over a period of time, it persists over a long period of time.\n- Example: Population growth over the years can be seen as an upward trend.\n\n#### Seasonality\n\n- Regular pattern of up and down fluctuations.\n- It is a short-term variation occurring due to seasonal factors\n- Example: Sales of ice-cream increases during the summer season\n\n#### Cyclically\n\n- It is a medium-term variation caused by circumstances, which repeat in irregular intervals.\n- Example: 5 years of economic growth, followed by 2 years of economic recession, followed by 7 years of economic growth followed by 1 year of economic recession\n\n#### Irregularity\n\n- It refers to variations which occur due to unpredictable factors and also do not repeat in particular patterns.\n- Example: Variations caused by incidents like earthquake, floods, war, etc.\n\nIf the dataset presents all four time series components then the data is non-stationary, which means that typically these components will be present. If the data is not stationary then the time series forecasting will effective. Applying it on a dataset with each component present will not really perform well.\n\nSo how do we differentiate between these time series. \n\nStationary time series depends on:\n\n1. Mean \n2. Variance\n3. Co-variance\n\n## Normalization\n\nNormalized data within statistics often involves eliminating units of measurement from a dataset. As a result, this enables us to easily compare data with different scales and are measured from different sources.\n\nWhy is Normalized Data Important?\nWhen training a machine learning model, we aim to bring the data to a common scale and so the various features are less sensitive to each other. In this case, we can utilize data normalization as a method of transforming our data, which may be of different units or scales (bitcoin and usd). This allows our model to train using features that could lead to more accurate predictions.\n\n## Transformation\n\nApplying differencing or seasonal differencing log of the series should make the series stationary.\n\nSimply put, stationarity removes trends from the dataset which can be extremely intrusive to our models. Basically, stationarity makes our models perform and predict better.\n\n\n[Source](https:\/\/www.youtube.com\/watch?v=_vQ0W_qXMxk)","385d1e06":"The forecast is expecting bitcoin to continue rising in value. There has a been a new spike at the end of 2020 and start of 2021 that added to this prediction. ","33df52eb":"Here we are running a statistical test to determine how well the times series was transformed to be stationary. This is a test that outputs certain statistical patterns that we can use to judge whether each parameter is stationary.","9ebcddba":"[Source](https:\/\/www.youtube.com\/watch?v=_vQ0W_qXMxk&t=1157s)","6b4196c4":"# Models\n\nWe will be evaluating three methods for forecasting time series data.\n1. Vector Autoregressive (VAR) Model\n2. XGBoost Model\n3. Facebook Prophet\n\n#### Train-test Set Split\n\nWe now have to split the sample into training and validation sets. In time series we have to be careful with this because we cannot simply randomly select a training and testing set, because of the time dependence.\n\nOur dataset are given within daily intervals and so we can predict up to a particular number of days.\n\n## Vector Autoregressive (AR) Model\n\nVAR models (vector autoregressive models) are used for multivariate time series. The structure is that each variable is a linear function of past lags of itself and past lags of the other variables.\n\nAs an example suppose that we measure three different time series variables, denoted by x_{t,1} and x_{t,2}.\n\nThe vector autoregressive model of order 1, denoted as VAR(1), is as follows:\n\n\\begin{equation*}\nx_{t,1} = \\alpha_{1} + \\phi_{11} x_{t\u22121,1} + \\phi_{12}x_{t\u22121,2} + \\phi_{13}x_{t\u22121,3} + w_{t,1}\n\\end{equation*}\n\n\\begin{equation*}\nx_{t,2} = \\alpha_{2} + \\phi_{21} x_{t\u22121,1} + \\phi_{22}x_{t\u22121,2} + \\phi_{23}x_{t\u22121,3} + w_{t,2}\n\\end{equation*}\n\nEach variable is a linear function of the lag 1 values for all variables in the set.\n\nIn a VAR(2) model, the lag 2 values for all variables are added to the right sides of the equations, In the case of three x-variables (or time series) there would be six predictors on the right side of each equation, three lag 1 terms and three lag 2 terms.\n\nIn general, for a VAR(p) model, the first p lags of each variable in the system would be used as regression predictors for each variable.\n\nVAR models are a specific case of more general VARMA models. VARMA models for multivariate time series include the VAR structure above along with moving average terms for each variable. More generally yet, these are special cases of ARMAX models that allow for the addition of other predictors that are outside the multivariate set of principal interest.\n\n[Source](https:\/\/online.stat.psu.edu\/stat510\/lesson\/11\/11.2)","e0b5562c":"### Bitcoin Closing Price Forecast","8878a7c1":"## Facebook Prophet\n\nOpen-source library available [here](https:\/\/facebook.github.io\/prophet\/)\n\nFacebook prophet was created to work as a tool for most general time series predictions. It is easy to use and it teaches beginners machine learning in an intuitive way.\n\nFacebook also allows for those with domain knowledge to not be blocked when they get an answer and get some additional insight that may yield some value.\n\nFacebook prophet has two main functions auto.arima and the other is exponential smoothing. They both perform a model selection process so they're really trying to do a lot of work for you and take a lot of difficulty out of building the model, but sometimes you would get bad performing models, if you were to simply apply it to a dataset. As the results aren't always intuitive enough to make the forecast better.\n\nFacebook prophet is able to visualize significant features in the time series such as trends, outliers, seasonality, etc. Also, the forecasting method is robust enough to handle any missing values.\n\nSo typically with times series problems you would want to model the generative process of how this times series would be created. That becomes difficult to write, a generative model for a time series process is like at each state a new issue is going to happen and it is going to depend on the past in some way. Instead, facebook built a discriminative model, which is a simple decomposable time series model. It's a generalized additive model so each component is additive but the individual components may not be linear.\n\n\\begin{equation*}\ny(t) = \\text{piecewise_trend}(t) + \\text{seasonality}(t) + \\text{holiday_effects}(t) + \\text{noise}\n\\end{equation*}\n\nThe first component is a piecewise trend, that could either be a logistic trend or a linear trend, and that's going to account for basically how fast the time series is growing or declining.\n\nThe second component is seasonality, that is something that happens regularly in cycles. There is some holiday effects and noise reduction included.\n\n\nThe piecewise linear trend or the logistic trend  is developed by generating a bunch a candidate change points. These are point where the model is thinking it could potentially change its trajectory, and it's going to assume that most of time those changes are zero, but some of the time it's going to allow it to change. So the data is basically going to tell us when the time series has shifted its trajectory, which is a really nice feature. So prophet is learning from the data how to extrapolate locally from modeling from the past data.\n\n\n[How it works](https:\/\/facebook.github.io\/prophet\/docs\/quick_start.html#python-api)\n\n[Youtube Video](https:\/\/www.youtube.com\/watch?v=pOYAXv15r3A&feature=youtu.be)","4010a229":"After fitting the model, we see the results of the model. It provides the coefficient and other stats.","127c1b5c":"### Autocorrelation and Partial Autocorrelation\n\nThe coefficient of correlation between two values in a time series is called the autocorrelation function (ACF) For example the ACF for a time series  is given by:\n\n\\begin{equation*} \\mbox{Corr}(y_{t},y_{t-k}). \\end{equation*}\n\nThis value of k is the time gap being considered and is called the lag. A lag 1 autocorrelation (i.e., k = 1 in the above) is the correlation between values that are one time period apart. More generally, a lag k autocorrelation is the correlation between values that are k time periods apart.\n\nThe ACF is a way to measure the linear relationship between an observation at time t and the observations at previous times. If we assume an AR(k) model, then we may wish to only measure the association between  and  and filter out the linear influence of the random variables that lie in between (i.e., ), which requires a transformation on the time series. Then by calculating the correlation of the transformed time series we obtain the partial autocorrelation function (PACF).\n\nThe PACF is most useful for identifying the order of an autoregressive model. Specifically, sample partial autocorrelations that are significantly different from 0 indicate lagged terms of  that are useful predictors of . To help differentiate between ACF and PACF, think of them as analogues to  and partial  values as discussed previously.\n\nGraphical approaches to assessing the lag of an autoregressive model include looking at the ACF and PACF values versus the lag. In a plot of ACF versus the lag, if you see large ACF values and a non-random pattern, then likely the values are serially correlated. In a plot of PACF versus the lag, the pattern will usually appear random, but large PACF values at a given lag indicate this value as a possible choice for the order of an autoregressive model. It is important that the choice of the order makes sense. For example, suppose you have blood pressure readings for every day over the past two years. You may find that an AR(1) or AR(2) model is appropriate for modeling blood pressure. However, the PACF may indicate a large partial autocorrelation value at a lag of 17, but such a large order for an autoregressive model likely does not make much sense.","e8afe372":"Here, using a dickey-fuller test (another econometric technique) we have some useful information about our data. This package runs a dickey fuller test where it is looking for the optimal amount of lags for the model while testing for unit roots. The autolag=\"AIC\" parameter is doing the work here. For more on AICs [click here](https:\/\/www.statisticshowto.com\/akaikes-information-criterion\/).  Our pvalue is very low so that means that we can reject the null hypothesis that has a unit root. We have also identified a good number of lags to begin our analysis with. Our data should be ready to be feed into our models.","1544a95d":"By comparing the prices of bitcoin and usd visually, we can see a peak in bitcoin around 2017-2018 and a decline in the usd around the same time. There is a similar pattern toward the end of 2020 going into 2021. ","42650bd7":"## XGBoost\n\nXGBoost model use Machine Learning approaches, where we exclusively care about quality of prediction. XGBoost regressors can be used for time series forecast, even though they are not specifically meant for long term forecasts. \n\nXGBoost uses trees to help augment a linear regression.  It looks at the relationship between a variable in a certain year and certain lags. Years that have more similar coefficients are grouped together and allowed to vary differently than other years with similar relationships to their lags.\n\n#### [What is Boosting and how XGBoost operates?](https:\/\/www.datacamp.com\/community\/tutorials\/xgboost-in-python#what)\n\n#### [Video on XGBoost](https:\/\/www.youtube.com\/watch?v=4rikgkt4IcU)","e403460b":"# Forecasting Tutorial\nWe are going to be forecasting the price of Bitcoin ([More on Bitcoin here](https:\/\/www.youtube.com\/watch?v=HhOuvNDMlI0\/)). There are many different ways of forecasting and we have learned much of the foundational theory in our econometrics courses. Knowing what is going on behind the scenes is necessary for interpretation but, thanks to python, it is not necessary for implementation. In this notebook we will go through 3 very different ways of forecasting. \n\n1.   Vector autoregression (VAR)\n2.   Xgboost\n3. Facebook Prophet\n\n\n\n\n\n## Data Collection & Preprocessing\n\nSo the first thing that we are going to want to do is collect data. We will use Yahoo Bitcoin History, the data is free to download. Available [here](https:\/\/finance.yahoo.com\/quote\/BTC-USD\/history?p=BTC-USD&guccounter=1&guce_referrer=aHR0cHM6Ly90b3dhcmRzZGF0YXNjaWVuY2UuY29tL3ByZWRpY3RpbmctcHJpY2VzLW9mLWJpdGNvaW4td2l0aC1tYWNoaW5lLWxlYXJuaW5nLTNlODNiYjRkZDM1Zg&guce_referrer_sig=AQAAADw3s4Aaum1jPPN-7OIUDQmu6j4SiVGzs0U51SpDhE84Ec30JpqsegxOZYH5lnrNdcM-ZqTtFRI7VLXpikmV4GPDk1ZDnQeBQrtA4W96EdxGo0dwZbtcLWqtK3z3POErPkEb1VvQe0PF8fBlArbyg2HguzJ8pEXcT7ptl3tqR5iZ).\n\nWe are also using US Dollar Index Futures Historical Data from Investing.com. Available [here](https:\/\/www.investing.com\/currencies\/us-dollar-index-historical-data).\n\nBoth Datasets are available in the chat window. \n\nWe want to make sure that we are using enough historical data to make the model more accurate. We are using historical data for bitcoin and usd prices from Aug 2014 to Dec 2021."}}