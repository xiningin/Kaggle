{"cell_type":{"0412d7dc":"code","5f4fa1be":"code","7b5a4127":"code","aedde853":"code","5913fea1":"code","df71be33":"code","d5fb7f69":"code","282d6ded":"code","2fa1c918":"code","1e411d7a":"code","efb5a987":"code","8d5a0d13":"code","f2088dee":"markdown","981fb696":"markdown","d6907459":"markdown","be8cdba5":"markdown","ecda482a":"markdown","beb49f60":"markdown","11253e34":"markdown","a859883d":"markdown","a14566fa":"markdown","a9bd9693":"markdown","638e1d5c":"markdown","ea630fff":"markdown","16deeed2":"markdown","a8262db0":"markdown","3462fded":"markdown"},"source":{"0412d7dc":"from IPython.core.display import display, HTML\nimport glob\nfor filename in glob.glob('\/kaggle\/input\/cord-answers1\/*.txt'):\n    f = open(filename,\"r\")\n    ans = f.read()\n    display(HTML(ans))","5f4fa1be":"import numpy as np\nimport pandas as pd \nimport glob\nimport json\nimport math\n\nroot_dir = '\/kaggle\/input\/CORD-19-research-challenge' \ndf = pd.read_csv(f'{root_dir}\/metadata.csv') # Reading the metadata of the data set\nsha_abstract = df[['sha', 'abstract']] # Filtering out rows with both `SHA` and `Abstract`\nsha_abstract = sha_abstract.dropna().reset_index()[['sha', 'abstract']] # Separating `SHA` and `Abstract` columns into separate lists\n\ncorpus = list(sha_abstract.values[:,1])\nsha = list(sha_abstract.values[:,0])","7b5a4127":"fs = open('\/kaggle\/input\/stopwords\/stopwordlist.txt','r') # Source of stopwordlist: https:\/\/gist.github.com\/sebleier\/554280\nstopWordsList = fs.read().split(\" \")","aedde853":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nquery = ['coronavirus coronaviruses cov covid']\nqC = query+corpus\nfeaturesize = 512 # It was found that increasing the featuresize did not change the number of abstracts found\nvectorizer = TfidfVectorizer(stopWordsList,max_features=featuresize)\nX = vectorizer.fit_transform(qC).toarray()\n#print(vectorizer.get_feature_names())\n\ncovidCS = []\ncovid_SHA = []\nID = []\nfor i in range(len(corpus)):\n    cossim = cosine_similarity(X[0].reshape(1,-1),X[i+1].reshape(1,-1))\n    if cossim > 0.: \n        covidCS.append(cossim)\n        covid_SHA.append(sha[i])\n        ID.append(i)\nprint(\"{} out of {} abstracts contains your query for featuresize = {}\".format(len(covidCS),len(corpus),featuresize))\n","5913fea1":"# Each covid_SHA may have multiple SHAs. \n# Create a list of SHAs, with each element containing a list of SHAs pointing to the same article\nfull_sha = []\nfor sha in covid_SHA:\n    newsha = sha.split('; ')\n    full_sha.append(newsha)\n    \ndoc_paths = glob.glob(f'{root_dir}\/*\/*\/*\/*.json')\nroot_dir = '\/kaggle\/input\/CORD-19-research-challenge'\ndf = pd.read_csv(f'{root_dir}\/metadata.csv')\nsub_df = df[['sha', 'title', 'authors', 'url']]\ndef get_text(full_sha):\n    # Initialise the full_text array of articles\n    full_text=[]\n    for full_sha_num in full_sha: # for each list of SHA pointing to the same article\n        sha = full_sha_num[0]\n        document_path = [path for path in doc_paths if sha in path] # we find the document path for the first SHA in the list as all SHA points to the same article\n        with open(document_path[0]) as f:\n            file = json.load(f)\n            article_series = sub_df[sub_df['sha'].str.contains(sha, na=False)]\n            sha = article_series.values[0][0]\n            title = article_series.values[0][1]\n            authors = article_series.values[0][2]\n            url = article_series.values[0][3]\n            for text_part in file['body_text']:\n                text = text_part['text']\n                # remove citations from each paragraph\n                for citation in text_part['cite_spans']:\n                    text = text.replace(citation['text'], \"\")\n                full_text.append([sha, title, authors, url, text])\n    return full_text\nfull_text = get_text(full_sha)\nfull_text_df = pd.DataFrame(full_text, columns=['sha','title','authors', 'url', 'text'])\npd.set_option('display.max_colwidth', 50)\nfull_text_df.head()\nprint(\"No. of paragraphs: \",len(full_text_df))","df71be33":"import torch\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\n\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')\ntokenizer = BertTokenizer.from_pretrained('bert-large-cased-whole-word-masking-finetuned-squad')\nmodel.eval(); # set model in eval mode","d5fb7f69":"# Formatting functions to be used later\n\ndef highlight_paragraph(paragraph, token_start, token_end,num_seg_a):\n    para =\"\"\n    para = para + \" \".join(paragraph[num_seg_a:token_start])\n    para = para + \"[start]\"  + \" \".join(paragraph[token_start:token_end+1])+ \"[end]\"\n    para = para + \" \".join(paragraph[token_end+1:])\n    return para\n\ndef split_paragraph(paragraph, qns):\n    input_ids = tokenizer.encode(paragraph)\n    #print(len(input_ids))\n    qns_ids = tokenizer.encode(qns)\n    #print(len(qns_ids))\n    total_ids = qns_ids + input_ids\n    tokens = tokenizer.convert_ids_to_tokens(total_ids)\n    assert len(tokens) == len(qns_ids) + len(input_ids)\n    num_sections = math.ceil(len(tokens)\/512) \n    sections = []\n    while len(tokens) > 512: \n        one_section = tokens[:512]\n        one_sect_id = total_ids[:512]\n        found_end = False\n        i = 0\n        last_index = len(one_section)\n        while not found_end:\n            i += 1\n            if (one_section[last_index-i-1][-1] == \".\" and one_section[last_index-i][0].isupper()):\n                found_end = True               \n                sent_end = i\n                sections.append((one_section[:last_index-sent_end],one_sect_id[:last_index-sent_end]))\n                tokens = tokens[:len(qns_ids)+1]+tokens[last_index-sent_end:]\n                total_ids = total_ids[:len(qns_ids)+1]+tokens[last_index-sent_end:]\n    sections.append((tokens,total_ids))\n    return sections","282d6ded":"def askQuestion(taskno, paragraphs, titles, authors, urls):\n    task = taskno[0] # element 0 is the task topic where tf-idf is performed as first filter\n    question = taskno[1] # element 1 is the specific question\n    tP = [task]+paragraphs\n    vectorizer = TfidfVectorizer(stopWordsList,max_features=featuresize) \n    Y = vectorizer.fit_transform(tP).toarray() # vectorize task topic and every paragraph\n    taskCS = []\n    \n    counter_threshold = 0\n    # use cosine similarity to find top k paragraphs relevant to task topic\n    for i in range(len(paragraphs)):\n        cossim = cosine_similarity(Y[0].reshape(1,-1),Y[i+1].reshape(1,-1))\n        taskCS.append(cossim[0][0])\n        \n    ## Find top k similar paragraphs\n    k = 20\n\n    ranked = np.argsort(taskCS)[::-1][:k]\n    ranked_CS = [taskCS[i] for i in ranked]\n    ranked_para = [paragraphs[i] for i in ranked]\n    ranked_url = [urls[i] for i in ranked]\n    ranked_title = [titles[i] for i in ranked]\n    ranked_author = [authors[i] for i in ranked]\n    \n    sectionlist = []\n    answerlist=[]\n    urllist=[]\n    authorlist=[]\n    titlelist=[]\n    \n    for i in range(k):\n        answer_text = ranked_para[i]\n        sections = split_paragraph(answer_text,question)\n        \n        for s in range(len(sections)):\n            urllist.append(ranked_url[i])\n            authorlist.append(ranked_author[i])\n            titlelist.append(ranked_title[i])\n            \n            tokens, input_ids = sections[s]\n\n            sep_index = input_ids.index(tokenizer.sep_token_id) # Search the input_ids for the first instance of the `[SEP]` token.\n            num_seg_a = sep_index + 1 # The number of segment A tokens includes the [SEP] token istelf\n            num_seg_b = len(input_ids) - num_seg_a # The remainder are segment B.   \n            segment_ids = [0]*num_seg_a + [1]*num_seg_b # Construct the list of 0s and 1s.\n            assert len(segment_ids) == len(input_ids) # There should be a segment_id for every input token.\n    \n            start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n                                     token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n\n            answer_start = torch.argmax(start_scores) # Find the tokens with the highest `start` and `end` scores.\n            answer_end = torch.argmax(end_scores)   \n            answer = ' '.join(tokens[answer_start:answer_end+1]) # Combine the tokens in the answer and print it out.\n\n            para = highlight_paragraph(tokens,answer_start,answer_end,num_seg_a)\n            para = para.replace(' ##', '')\n            para = para.replace('[CLS]', '')\n            para = para.replace('[SEP]', '')\n            max_length = 10 # we limit answer lengths to < 10 words\n            if (answer_start != answer_end & answer_end - answer_start < max_length):  \n                answerlist.append(answer)\n                sectionlist.append(para)\n                \n            else:\n                sectionlist.append(\"Answer not found\")\n                answerlist.append(\"Answer not found\")\n    \n    new_df = pd.DataFrame(list(zip(answerlist,sectionlist,titlelist,authorlist,urllist)),columns=['Answers','Evidence','Title','Authors','URL'])\n    \n    new_df = new_df[~new_df['Answers'].str.match(\"Answer not found\")]\n    new_df = new_df.iloc[:,1:]\n    return new_df","2fa1c918":"paragraphs = list(full_text_df.values[:,4])\nSHAp = list(full_text_df.values[:,0])\ntitles = list(full_text_df.values[:,1])\nauthors = list(full_text_df.values[:,2])\nurls = list(full_text_df.values[:,3])\n\nfeaturesize = 1024\n\ntask1 = ['incubation', \"How long is the incubation period?\"]\ntask2 = ['movement strategies',\"Effectiveness of movement control strategies\"]\ntask3 = ['transmission asymptomatic',\"Prevalence of asymptomatic shedding and transmission\"]\ntask4 = ['transmission', 'mode of transmission']","1e411d7a":"#for taskno in [task1,task2,task3]:\ntaskno = task1\nanswers_df = askQuestion(taskno, paragraphs, titles, authors, urls)\n\nhtml_df = answers_df.to_html()\nhtml_df = html_df.replace('[start]', '<span style=\"color: blue;\"> ')\nhtml_df = html_df.replace('[end]', ' <\/span>')\nhtml_df = \"<h2>Question: \"+taskno[1]+\"<\/h2>\"+html_df\nhtml_df = \"<h2>Topics: \"+taskno[0]+\"<\/h2>\"+html_df\ndisplay(HTML(html_df))","efb5a987":"filtered = answers_df.loc[[7,9,11],['Evidence','Title','Authors', 'URL']]\nhtml_df = filtered.to_html()\nhtml_df = html_df.replace('[start]', '<span style=\"color: blue;\"> ')\nhtml_df = html_df.replace('[end]', ' <\/span>')\nhtml_df = \"<h2>Question: \"+taskno[1]+\"<\/h2>\"+html_df\nhtml_df = \"<h2>Topics: \"+taskno[0]+\"<\/h2>\"+html_df\ndisplay(HTML(html_df))\n\n# Save answers as txt file\n# with open(\"incubation_answers.txt\", \"w+\") as f:\n#     f.write(html_df)","8d5a0d13":"for filename in glob.glob('\/kaggle\/input\/cord-answers1\/*.txt'):\n    f = open(filename,\"r\")\n    ans = f.read()\n    display(HTML(ans))","f2088dee":"## Credits:\n\nThis work is part of a collaborative effort with @jingjielim, @strifonov, @atanasova, @darumen, @preslav.\n\nIt covers the first branch in our diagram of contribution shown below:\n\n![cord-logo-final](https:\/\/user-images.githubusercontent.com\/55004415\/79496823-b42fca00-7ff4-11ea-993f-cd5792955641.png)","981fb696":"We manually filter the 3 best answers for each question and present it below","d6907459":"# Results:","be8cdba5":"The same proceedure was performed for the rest of the tasks and results were saved to file. We present the rest of the results below: ","ecda482a":"### Extracting all paragraphs of all the relevant full-text to form our Q&A Database","beb49f60":"## Concluding Thoughts\n* Simple approach of using TF-IDF to sequentially filter the corpus of documents + using a pre-trained language model finetuned on Q&A tasks to highlight answers from the shortlisted paragraphs allowed us to answer some of the key questions with reasonable accuracy.\n* However, this approach was effective only to answer very specific questions (such as identifying the length of incubation periods, etc) and would not be very effective in answering broader questions such as \"What do we know about the relationship between the environment and the virus?\" (unless an article specifically reports that word-for-word of course). \n* In this approach, most of the Information Retrieval was performed in the TF-IDF step, which converts queries and documents into a simple Bag-of-Words representation. As opposed to current state-of-the-art language models that makes use of attention mechanisms to learn contextual representations, a simple Bag-of-Words representation has poor contextual representation.\n* For convenience, we used the BERT-finetuned-on-SQuAD model available from Huggingface transformers. This model was pretrained on corpora of texts from Wikipedia and fine-tuned on questions posed on Wikipedia articles (SQuAD). A possible improvement would be to use models that are pre-trained on more relevant domain-specific corpus such as BioBERT or SciBERT (which have been pre-trained on biomedical and scientific publications respectively), and\/or further fine-tuning on domain-specific Q&A datasets. One such example could be the PubMedQA Dataset (https:\/\/arxiv.org\/abs\/1909.06146). ","11253e34":"# Detailed Approach:","a859883d":"### Matching Top k = 20 paragraphs\nTo avoid searching through all 190k paragraphs in our Q&A database (it is likely that most will not contain the answer), we first use TF-IDF and match the paragraphs to the Task topic and sort them in decreasing cosine similarity in order to find the top k relevant paragraphs to a single task topic.\n\n### Using BERT+Q&A:\nFor each of the top k=20 paragraphs we will attempt to use BERT Q&A to find answers to the specific task question. Note that while we have used the task topics to shortlist the paragraphs based on relevancy, there is no guarantee that the shortlisted paragraphs will contain answers to our specific questions and thus answers would need to be manually inspected and assessed. The following was done to help filter 'good' answers:\n* We require answers to be between 1-10 words long\n* We modify our stop-word list to include to the stop list words that frequently occur but do not answer task questions and confuses the model","a14566fa":"## 1. Find articles relevant to Coronavirus and Covid-19 by matching abstracts using **TF-IDF**\n\n### Loading Data into DF\nWe load data from both summary csv file and all availiable text \/ abstracts from additional .json files. A quick scan through all the artcles in the dataset reveals that only a fraction in the dataset are pertaining to Coronaviruses. We thus propose the following:\n* Perform a query on all Abstracts to find matches of articles discussing Coronavirus and dump the rest. We make the assumption that if Coronavirus\/Covid is not mentioned in the abstract, the article is likely not about Coronavirus\/Covid.\n* After finding the relevant abstracts, we extract the body\/full-text of the article to perform Q&A subsequently. Since not all abstracts have a full-text, we will only look for abstracts with full-text via their SHA. ","a9bd9693":"### Import BERT from Huggingface Transformers\nBERT (Bidirectional Encoder Representations from Transformers), is a language model that has acheived state-of-the-art results in many natural language processing (NLP) tasks. It makes use of the attention mechanism to learn contextual relations between words in a text. We will not explain the details of the workings of the transformer, but refer the reader to good resources such as: http:\/\/jalammar.github.io\/illustrated-transformer\/ \n\nFor the purpose of our tasks, we will use the BERT large-uncased pre-trained model config 'bert-large-cased' that has been fine-tuned on the Stanford Question Answering Dataset (SQuaD). This model is available from Huggingface transformers (see https:\/\/huggingface.co\/transformers\/) using the PyTorch framework.","638e1d5c":"### Use TF-IDF to perform a Query search on all the abstracts to find relevant articles for Coronavirus\nTF-IDF (Term Frequency - Inverse Document Frequency) is a statistical measure that is often used to evaluate how important a word is to a document (here, the word is the query and the documents are the abstracts). The formula for computing the TF-IDF score of word $i$ in document $j$ is:\n\n$$ \\text{TF-IDF score} = TF(i,j)*IDF(i)\\text{, where:}$$\n\n$$TF(i,j) = \\frac{\\text{word i frequency in document j}}{\\text{total # words in document j}}$$\n\n$$IDF(i) = \\log_2\\left( \\frac{\\text{Total # of documents}}{\\text{# documents with word i}}\\right)$$\n\nIn addtition to accounting for the word importance to the document, we also make use of vectorization to convert the query and the individual abstracts into feature vectors which can be matched via their cosine similarity.\n\n* We will perform TF-IDF + vectorization on the query and all the abstract paragraphs individually\n    * Remove common 'stop words' since we only care about the topic. We use the list from: https:\/\/gist.github.com\/sebleier\/554280\n    * Select a suitable feature size (need to be large enough to capture enough vocab)    \n* Then compute cosine-similarity to compare the similarity between the query vector and all the abstract vectors\n* Record SHAs of all the **non-zero** cosine-similarity","ea630fff":"# CORD-19 Challenge\nIn this notebook we attempt to answer key scientific questions for COVID-19 using the COVID-19 Open Research Dataset (CORD-19) by using a combination of **Text-mining** and **Natural Language Processin**g (NLP) tools such as** TF-IDF** and **BERT (finetuned on Question Answering Tasks)**. The key steps in our approach are illustrated in the following pipeline:\n![cordpipeline](https:\/\/user-images.githubusercontent.com\/55004415\/79482903-7f654800-7fdf-11ea-95c9-a022a9ed2e3b.png)\n","16deeed2":"## 2. Find the top k paragraphs for each task topic and feed paragraphs into pre-trained BERT model to perform Q&A","a8262db0":"We list the Task topic and Task questions we hope to address and perform them one by one sequentially","3462fded":"---"}}