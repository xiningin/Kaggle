{"cell_type":{"bd92e9de":"code","e51866f7":"code","c15da80b":"code","19d0c9e3":"code","0f5186ec":"code","7f143dba":"code","c3b80e55":"code","0c6a7027":"code","9d31db59":"code","3e10c128":"code","4967163b":"code","b829cd66":"code","401f4d13":"code","a774b913":"code","0828729a":"code","24c16097":"code","0cada408":"code","be7607ec":"code","a2d128c2":"code","6b4e7945":"code","3ac50261":"code","acaf5782":"code","7ad782ff":"code","bf538282":"code","7adceac0":"code","273c9913":"code","60109d68":"code","659caa8b":"code","6b62a259":"code","47519c0b":"code","7410a24b":"code","79478814":"code","7f405fc0":"code","6a116446":"code","cd1c396e":"code","0c08aa42":"code","c962fd48":"code","e1079d69":"code","27e1272c":"code","b70f3132":"code","52789de4":"code","f182a360":"code","17140a93":"code","27ae2f57":"code","a449892d":"code","08d7bf6e":"code","9f9d0421":"code","0b76c7c7":"markdown","c03d6ace":"markdown","b86619b9":"markdown","b31f59ac":"markdown","ccf6ed61":"markdown","07a37d8b":"markdown","97f8ea14":"markdown","4eb63be9":"markdown","b7efe64a":"markdown","de9b8173":"markdown","62fa8970":"markdown","36d29d1c":"markdown","b384fc6a":"markdown","ea76a254":"markdown","008eea57":"markdown","eed616f7":"markdown","fe5925c6":"markdown","6b3c6c76":"markdown","6a9752e2":"markdown","757dd09b":"markdown","1ff96078":"markdown","ccef4d7c":"markdown","23200621":"markdown","680f089f":"markdown","2f30bfab":"markdown","d5bcfd9f":"markdown"},"source":{"bd92e9de":"!pip install -q transformers==2.0.0\n!pip install -q fastai==1.0.58","e51866f7":"from IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"..\/input\/riiid-answer-correctness-prediction-rapids\/custom.css\", \"r\").read()\n    return HTML(\"<style>\"+styles+\"<\/style>\")\ncss_styling()","c15da80b":"def notebook_styling():\n    styles = open(\"..\/input\/riiid-answer-correctness-prediction-rapids\/custom_rapids.css\", \"r\").read()\n    return HTML(\"<style>\"+styles+\"<\/style>\")\nnotebook_styling()","19d0c9e3":"class color:\n    '''S from Start & E from End.'''\n    S = '\\033[1m' + '\\033[93m'\n    E = '\\033[0m'","0f5186ec":"import numpy as np  # Linear Algebra\nimport pandas as pd # Data Manipulation\nfrom pathlib import Path\n\nimport os\n\nimport torch\nimport torch.optim\n\nimport random\n\n# fastai\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\n\n# Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Transformers\nfrom transformers import PreTrainedModel,PreTrainedTokenizer,PretrainedConfig\n\nfrom transformers import BertForSequenceClassification,BertTokenizer,BertConfig\nfrom transformers import RobertaForSequenceClassification,RobertaTokenizer,RobertaConfig\nfrom transformers import XLNetForSequenceClassification,XLNetTokenizer,XLNetConfig\nfrom transformers import XLMForSequenceClassification,XLMTokenizer,XLMConfig\nfrom transformers import DistilBertForSequenceClassification,DistilBertTokenizer,DistilBertConfig","7f143dba":"import fastai\nimport transformers\n\nprint(color.S+\"FastAI version:\"+color.E,fastai.__version__)\nprint(color.S+\"Transformers version:\"+color.E,transformers.__version__)","c3b80e55":"for dirname,_,filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname,filename))","0c6a7027":"DATA_ROOT = Path('..')\/\"\/kaggle\/input\/sentiment-analysis-on-movie-reviews\"\ntrain = pd.read_csv(DATA_ROOT\/'train.tsv.zip',sep='\\t')\ntest = pd.read_csv(DATA_ROOT\/'test.tsv.zip',sep='\\t')\nprint(color.S+ \"Train shape:\"+color.E,train.shape)\nprint(color.S+ \"Test shape:\"+color.E,test.shape)\ntrain.head()","9d31db59":"MODEL_CLASSES = {'bert':(BertForSequenceClassification,BertTokenizer,BertConfig),\n                'xlm':(XLMForSequenceClassification,XLMTokenizer,XLMConfig),\n                'xlnet':(XLNetForSequenceClassification,XLNetTokenizer,XLNetConfig),\n                'roberta':(RobertaForSequenceClassification,RobertaTokenizer,RobertaConfig),\n                'distilbert':(DistilBertForSequenceClassification,DistilBertTokenizer,DistilBertConfig)}","3e10c128":"# parameters\nseed = 42\nuse_fp16 = False   # Whether or not to use mixed precision training.\nbs = 16 # A DataBunch is a collection of PyTorch DataLoaders returned when you call the databunch function. It also defines how they are created from your training, validation, and optionally test LabelList instances.\nmodel_type = 'bert'\npretrained_model_name = 'bert-base-uncased'","4967163b":"model_class,tokenizer_class,config_class = MODEL_CLASSES[model_type]","b829cd66":"model_class.pretrained_model_archive_map.keys() # Note that, in latest version, it will throw an error, so you need to downgrade the version of transformer to 2.0.0","401f4d13":"def seed_all(seed):\n    random.seed(seed)  # Python Random Number\n    np.random.seed(seed) # Numpy random number\n    torch.manual_seed(seed) # Torch random number\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed) # GPU Vars\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nseed_all(seed)","a774b913":"class TransformersBaseTokenizer(BaseTokenizer):\n    def __init__(self,pretrained_tokenizer:PreTrainedTokenizer,model_type = 'bert',**kwargs):\n        self._pretrained_tokenizer = pretrained_tokenizer\n        self.max_seq_len = pretrained_tokenizer.max_len\n        self.model_type = model_type\n    \n    def __call__(self,*args,**kwargs):\n        return self\n    \n    def tokenizer(self,t):\n        CLS  = self._pretrained_tokenizer.cls_token\n        SEP = self._pretrained_tokenizer.sep_token\n        if self.model_type in ['roberta']:\n            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n            tokens = [CLS] + tokens + [SEP]\n        else:\n            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n            if self.model_type in ['xlnet']:\n                tokens = tokens + [SEP] +  [CLS]\n            else:\n                tokens = [CLS] + tokens + [SEP]\n                \n        return tokens","0828729a":"transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\ntransformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer,model_type = model_type)\nfastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer,pre_rules = [],post_rules = [])","24c16097":"class TransformersVocab(Vocab):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        super(TransformersVocab, self).__init__(itos = [])\n        self.tokenizer = tokenizer\n\n    def numericalize(self, t:Collection[str]) -> List[int]:\n        \"Convert a list of tokens `t` to their ids.\"\n        return self.tokenizer.convert_tokens_to_ids(t)\n        #return self.tokenizer.encode(t)\n\n    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        \"Convert a list of `nums` to their tokens.\"\n        nums = np.array(nums).tolist()\n        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n\n    def __getstate__(self):\n        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n\n    def __setstate__(self, state:dict):\n        self.itos = state['itos']\n        self.tokenizer = state['tokenizer']\n        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})","0cada408":"transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\nnumericalize_processor = NumericalizeProcessor(vocab = transformer_vocab)\n\ntokenize_processor = TokenizeProcessor(tokenizer = fastai_tokenizer,include_bos=False, include_eos=False)\ntransformer_processor = [tokenize_processor, numericalize_processor]","be7607ec":"pad_first = bool(model_type in ['xlnet'])\npad_idx = transformer_tokenizer.pad_token_id","a2d128c2":"tokens = transformer_tokenizer.tokenize('Salut c est moi, Hello it s me')\nprint(tokens)\nids = transformer_tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)\ntransformer_tokenizer.convert_ids_to_tokens(ids)","6b4e7945":"# Implementing DataBunch\n\ndatabunch = (TextList.from_df(train,cols = 'Phrase',processor = transformer_processor)\n            .split_by_rand_pct(0.1,seed = seed)\n            .label_from_df(cols = 'Sentiment')\n            .add_test(test)\n            .databunch(bs = bs,pad_first=pad_first,pad_idx = pad_idx))","3ac50261":"print('[CLS] token :', transformer_tokenizer.cls_token)\nprint('[SEP] token :', transformer_tokenizer.sep_token)\nprint('[PAD] token :', transformer_tokenizer.pad_token)\ndatabunch.show_batch()","acaf5782":"print('[CLS] id :', transformer_tokenizer.cls_token_id)\nprint('[SEP] id :', transformer_tokenizer.sep_token_id)\nprint('[PAD] id :', pad_idx)\ntest_one_batch = databunch.one_batch()[0]\nprint('Batch shape : ',test_one_batch.shape)\nprint(test_one_batch)","7ad782ff":"class CustomTransformerModel(nn.Module):\n    def __init__(self,transformer:PreTrainedModel):\n        super(CustomTransformerModel,self).__init__()\n        self.transformer = transformer\n    \n    def forward(self,input_ids,attention_mask = None):\n        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n        logits = self.transformer(input_ids,attention_mask = attention_mask)[0]\n        return logits","bf538282":"config = config_class.from_pretrained(pretrained_model_name)\nconfig.num_labels = 5\nconfig.use_bfloat16 = use_fp16\nprint(config)","7adceac0":"transformer_model = model_class.from_pretrained(pretrained_model_name,config = config)\ncustom_transformer_model = CustomTransformerModel(transformer= transformer_model)","273c9913":"from fastai.callbacks import *\nfrom transformers import AdamW\nfrom functools import partial\n\nCustomAdamW = partial(AdamW,correct_bias = False)\nlearner = Learner(databunch,custom_transformer_model,opt_func = CustomAdamW,metrics = [accuracy,error_rate])\n\nlearner.callbacks.append(ShowGraph(learner))\nif use_fp16: learner = learner.to_fp16()","60109d68":"print(learner.model)","659caa8b":"# For DistilBERT\n# list_layers = [learner.model.transformer.distilbert.embeddings,\n#                learner.model.transformer.distilbert.transformer.layer[0],\n#                learner.model.transformer.distilbert.transformer.layer[1],\n#                learner.model.transformer.distilbert.transformer.layer[2],\n#                learner.model.transformer.distilbert.transformer.layer[3],\n#                learner.model.transformer.distilbert.transformer.layer[4],\n#                learner.model.transformer.distilbert.transformer.layer[5],\n#                learner.model.transformer.pre_classifier]\n\n# For bert-base-uncasecased\nlist_layers = [learner.model.transformer.bert.embeddings,\n              learner.model.transformer.bert.encoder.layer[0],\n              learner.model.transformer.bert.encoder.layer[1],\n              learner.model.transformer.bert.encoder.layer[2],\n              learner.model.transformer.bert.encoder.layer[3],\n              learner.model.transformer.bert.encoder.layer[4],\n              learner.model.transformer.bert.encoder.layer[5],\n              learner.model.transformer.bert.encoder.layer[6],\n              learner.model.transformer.bert.encoder.layer[7],\n              learner.model.transformer.bert.encoder.layer[8],\n              learner.model.transformer.bert.encoder.layer[9],\n              learner.model.transformer.bert.encoder.layer[10],\n              learner.model.transformer.bert.encoder.layer[11],\n              learner.model.transformer.bert.pooler]\n\n# For xlnet-base-cased\n# list_layers = [learner.model.transformer.transformer.word_embedding,\n#               learner.model.transformer.transformer.layer[0],\n#               learner.model.transformer.transformer.layer[1],\n#               learner.model.transformer.transformer.layer[2],\n#               learner.model.transformer.transformer.layer[3],\n#               learner.model.transformer.transformer.layer[4],\n#               learner.model.transformer.transformer.layer[5],\n#               learner.model.transformer.transformer.layer[6],\n#               learner.model.transformer.transformer.layer[7],\n#               learner.model.transformer.transformer.layer[8],\n#               learner.model.transformer.transformer.layer[9],\n#               learner.model.transformer.transformer.layer[10],\n#               learner.model.transformer.transformer.layer[11],\n#               learner.model.transformer.sequence_summary]\n\n# For roberta-base\n# list_layers = [learner.model.transformer.roberta.embeddings,\n#               learner.model.transformer.roberta.encoder.layer[0],\n#               learner.model.transformer.roberta.encoder.layer[1],\n#               learner.model.transformer.roberta.encoder.layer[2],\n#               learner.model.transformer.roberta.encoder.layer[3],\n#               learner.model.transformer.roberta.encoder.layer[4],\n#               learner.model.transformer.roberta.encoder.layer[5],\n#               learner.model.transformer.roberta.encoder.layer[6],\n#               learner.model.transformer.roberta.encoder.layer[7],\n#               learner.model.transformer.roberta.encoder.layer[8],\n#               learner.model.transformer.roberta.encoder.layer[9],\n#               learner.model.transformer.roberta.encoder.layer[10],\n#               learner.model.transformer.roberta.encoder.layer[11],\n#               learner.model.transformer.roberta.pooler]","6b62a259":"learner.split(list_layers)\nnum_groups = len(learner.layer_groups)\nprint('Learner split in',num_groups,'groups')\nprint(learner.layer_groups)","47519c0b":"learner.save('untrain')\nseed_all(seed)\nlearner.load('untrain');","7410a24b":"learner.freeze_to(-1)  # Freeze uptil the last part (classifier)\nlearner.summary()","79478814":"learner.lr_find()\nlearner.recorder.plot(skip_end=10,suggestion=True)","7f405fc0":"learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7))\n#The momentum is the first beta in Adam (or the momentum in SGD\/RMSProp). When you pass along (0.95,0.85) it means going from 0.95\n#to 0.85 during the warmup then from 0.85 to 0.95 in the annealing, but it only changes the first beta in Adam, yes.","6a116446":"learner.save('first_cycle')           \nseed_all(seed)\nlearner.load('first_cycle');","cd1c396e":"learner.freeze_to(-2)     # Freeze uptil the last two parts (Transformer and classifier)","0c08aa42":"lr = 1e-5","c962fd48":"learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","e1079d69":"learner.save('second_cycle')\nseed_all(seed)\nlearner.load('second_cycle');","27e1272c":"learner.freeze_to(-3)   \nlearner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))\nlearner.save('third_cycle')\nseed_all(seed)\nlearner.load('third_cycle');","b70f3132":"learner.unfreeze() # Trainable embeddings, transformer and classifier\nlearner.fit_one_cycle(2, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9)) ","52789de4":"learner.predict(\"I am feeling happy today!!\")","f182a360":"learner.predict(\"Would you be with me, I need you\")","17140a93":"learner.export('transformer.pkl');","27ae2f57":"path = '\/kaggle\/working'\nexport_learner = load_learner(path, file = 'transformer.pkl')","a449892d":"export_learner.predict(\"Would you be with me, I need you\")","08d7bf6e":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in databunch.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]\n\ntest_preds = get_preds_as_nparray(DatasetType.Test)","9f9d0421":"sample_submission = pd.read_csv(DATA_ROOT \/ 'sampleSubmission.csv')\nsample_submission['Sentiment'] = np.argmax(test_preds,axis=1)\nsample_submission.to_csv(\"predictions.csv\", index=False)","0b76c7c7":"* Now, having selected the **BERT MODEL**, we need to get the name of the different bert models available, for that, the \n`pretrained_model_archive_map`, is used","c03d6ace":"## Conclusions: \ud83d\ude42\n\n* I tried to explain in the simplest way (by which I understood this thing), and added some colors to make it more engaging, however, if you have any idea or doubt which could make this notebook more engaging and informative, let me know, thanks a lot \n\n## References: \ud83d\udcda\n\n1. Biggest shoutout to the author of this notebook, which got me inspired to write it again in my own language [FastAI with Transformer](https:\/\/www.kaggle.com\/maroberti\/fastai-with-transformers-bert-roberta\/notebook)\n2. FastAI Documents [FastAI](https:\/\/docs.fast.ai\/text.html)","b86619b9":"## Data Pre-processing\n\n* Now, for a moment forget that we need to do something more. We simply know the fact that, computers cannot understand texts, they only understand numbers. So, how to convert the text into numbers?\n* Split the word into a list (referred to as tokenization)\n* Assign each word a number (known as Numericalization in fastai)\n\n* Now, for each of the transformer model, we need to preprocess the text to convert into number (by tokenizing and numericalization). So, for that we have the `tokenizer` class, which converts the text into appropriate input for the model\n* What does an already present `tokenizer` class do? \n<div class=\"alert alert-block alert-info\">\n    <ul>\n    <li>Tokenizing (splitting strings in sub-word token strings), converting tokens strings to ids and back, and encoding\/decoding (i.e., tokenizing and converting to integers).<\/li>\n\n    <li>Adding new tokens to the vocabulary in a way that is independent of the underlying structure (BPE, SentencePiece\u2026).<\/li>\n\n    <li>Managing special tokens (like mask, beginning-of-sentence, etc.): adding them, assigning them to attributes in the tokenizer for easy access and making sure they are not split during tokenization.<\/li>\n     <\/ul>\n<\/div>\n\n\n#### Custom Tokenizer\nIt is quite simple, just need to observe 3 things and then you are done..... :)\n\n> 1. The `BaseTokenizer` class, which takes an input a string, and gives an output a list, containing the words of the sentence\n> 2. The `Tokenizer` class, which takes an argument a BaseTokenizer, and outputs a list of tokens, along with the padding and **start** and **end** of sentence.\n\nLet us implement it, it is really very very easy!!!","b31f59ac":"## Learner: Custom Optimizer\/ Custom Metrics\n\n<div class=\"alert alert-block alert-success\">\n    <li> Hugging Face has implemented two custom optimizers: <code>BertAdam<\/code> and <code>OpenAIAdam<\/code>. Since, these has been implemented in pytorch, we would be able to integrate it with FastAI, and for using <code>BertAdam<\/code>, we need to follow a convention that, we need to set a attribute name <code>correct_bias=False<\/code><\/li>\n <\/div>\n\n\n","ccf6ed61":"<div class=\"alert alert-block alert-success\">\n    <li>Check batch and numericalizer :<\/li>\n <\/div>","07a37d8b":"## Setting up the DataBunch \u2705\n\n* So, the only thing that needs to be taken is, if the model requires the padding first, or padding last, and then you are good to train, let us see it how is it code?\n\n","97f8ea14":"## Custom processor\n\n* Now that we have our `custom tokenizer` and `numericalizer`, we can create the `custom processor`. Notice we are passing the `include_bos = False` and `include_eos = False` options. This is because fastai adds its own special tokens by default which interferes with the [CLS] and [SEP] tokens added by our custom tokenizer.","4eb63be9":"* Check groups :","b7efe64a":"## Train the Model and Experimentation \n\n\n<div class=\"alert alert-block alert-success\">\n    <li> So, this is new for me,but we would be using <b>Slanted Triangular Learning Rate<\/b>, <b>Discriminant Learning Rate<\/b><\/li>\n <\/div>\n","de9b8173":"## Saving the model","62fa8970":"## Predictions","36d29d1c":"* Now, for loading the classes (such as **tokenizer**,**config**,**model**), they all have a common method known as `from_pretrained(pretrained_model_name)`, and in our case, `pretrained_model_name` is a string (which is a shortcut name for the model). As, for an example, *roberta_base*.","b384fc6a":"## Fastai with HuggingFace \ud83e\udd17Transformers \n\n<center><img src = \"https:\/\/miro.medium.com\/max\/1400\/1*Aqcm4iX3AQNWx9Zb-z7o1Q.png\"><\/center>\n\n\n\n> <div class=\"alert alert-block alert-info\">\n>     <p>This is an introductory notebook to the transformers with HuggingFace and FastAI, I have tried to do it as much interactively as I can, however if you find any mistakes or ways in which it could be improved, please let me know. And if you find this notebook useful, let it be upvoted (it's free)<\/p>\n> <\/div>","ea76a254":"## The task in hand: Movie Reviews\n\nGiven an input text \ud83d\udd23 related to the movie, we need to classify the given text into one of the 5 classes \ud83c\udfc6, which are based on the rating of the movie.\n<div class=\"alert alert-block alert-info\">\n  \n <ul>\nThe classses:\n     <li>0 -> Negative          \u2705<\/li>\n<li>1 -> Somewhat Negative \u2705<\/li>\n<li>2 -> Neutral           \u2705<\/li>\n<li>3 -> Somewhat Positive \u2705<\/li>\n<li>4 -> Positive          \u2705<\/li>\n   <\/ul>\n<\/div>\n\n* The Data is present in the `DataFrame` file, which can be loaded using `Pandas` \ud83d\udc3c\n","008eea57":"<div class=\"alert alert-block alert-success\">\n    <li> We freeze all the layers, except the classifier one, and check the layers that are trainable<\/li>\n <\/div>\n\n\n**Experiment. 1** : Freezing uptil the last layer","eed616f7":"**Experiment. 2** : Freezing uptil the last second layer","fe5925c6":"## Creating prediction file and submitting it \ud83c\udf89\n\nLet us understand one problem here, when training and testing, the shuffling of the data would have been done, so the output would not be corresponding to the exact same statement that is rpesent in the test dataset. So, let us see how to handle it","6b3c6c76":"**Experiment. 4** : Freezing all the layers","6a9752e2":"\nMAIN TRANSFORMER CLASS\n<div class=\"alert alert-block alert-info\">\n<b>In transformers, each model architecture consists of three things:<\/b>\n<\/div>\n\n* **Configuration Class**: Contains the architecture of the model\n* **Model Class**: Contains the pretrained weights of the model\n* **Tokenizer Class**: Tokenizes and preprocess the data, to make it compatible with the model","757dd09b":"<div class=\"alert alert-block alert-success\">\n    <li> Now, for <code>MultiClass Classification<\/code> we need to modify the configuration class, (remember that, configuration is like the brain for the model!!, it has all the information about what layers to add in the model...)<\/li>\n <\/div>\n\n","1ff96078":"## CUSTOM MODEL\n<div class=\"alert alert-block alert-success\">\n    <li> Let us see, how can we make custom models of transformer to better meet our needs<\/li>\n <\/div>","ccef4d7c":"**Experiment. 3** : Freezing uptil the last third layer","23200621":"## Util Function\n\n* For getting reproducible results \ud83d\udcc9, we need to set the seed, so that every time, something random is done, it is done such that, every time, we get the same random number","680f089f":"* Just take care that, all the custom classes are defined earlier, as it needs to be present while predicting","2f30bfab":"<div class=\"alert alert-block alert-success\">\n    <li> Note here that we use slice to create separate learning rate for each group.<\/li>\n <\/div>","d5bcfd9f":"In this implementation, we need to take the following things into account:\n\n1. Since, this is not a RNN \u274c, we need to limit the sequence length to a particular number\n2. Most of the NLP models require special token at the start and the end of the sentence\n3. In some models such as RoBERTa, requires a space at the start of the string. So, for that we need to call the `add_prefix_space` equal to True\n\nSome of the few styles of input in various models:\n  \n    \u2687 bert :      [CLS] + tokens + [SEP] + padding\n    \u2687 xlm  :      [CLS] + tokens + [SEP] + padding\n    \u2687 distilbert: [CLS] + tokens + [SEP] + padding\n    \u2687 roberta:    [CLS] + prefix_space + tokens + [CLS] + [SEP]\n    \u2687 xlnet:      padding + tokens + [SEP] + [CLS]\n    \n\n* And for the padding part? We need to do nothing. It will be implemented itself in the `DataBunch`\n\n\n\n### CUSTOM NUMERICALIZER (For FastAI)\n\n* In `fastai`, `NumericalizeProcessor` object takes as `vocab` argument. \n\n\n<div class=\"alert alert-block alert-success\">\n    <li>Why are we doing this? \ud83e\udd14<\/li>\n    <li>To understand the implementation of Numericalization<\/li>\n<\/div>\n\n* Let us do it, it is quite easy, nothing more to do than converting into tokens and implementing the function to go from list of strings to list of numbers and vice versa \ud83e\udd29"}}