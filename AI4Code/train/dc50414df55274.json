{"cell_type":{"7e0f213e":"code","a1c5676c":"code","0f89c080":"code","745b60e3":"code","2d8556f6":"code","ad196909":"code","908acb3b":"code","a9534713":"code","af9fa4cb":"code","164b3237":"code","2448d8f8":"code","0a2f4d0a":"code","28d306f7":"code","1b5bd4eb":"code","c6cb7c06":"code","963a26f1":"code","ce51678e":"code","cef49a8a":"code","d2f1a5fa":"code","51eecdd4":"code","5ae51074":"code","71e7adf4":"code","0a21181b":"code","f2741f2b":"code","08d98c7e":"code","b66d466b":"code","756b3c22":"code","3f19c46d":"code","8ecb234c":"code","81e62dde":"code","45b0d1a9":"code","47f4bf24":"code","1a4d7715":"code","89c9e16c":"code","a629c0f6":"code","a4cb0c4b":"code","47c2f81d":"code","a0eb018a":"code","204731f7":"code","b3b0add7":"code","2bc5f9a3":"code","44d1a6e8":"markdown","e80e0f17":"markdown","9b4c372d":"markdown","29527219":"markdown","1a43e5e7":"markdown","883c7f1b":"markdown","32b57b7a":"markdown","56cc62e8":"markdown","f8e800e3":"markdown","77e22555":"markdown","20fcea72":"markdown","1300e09b":"markdown","c8c85229":"markdown","9000a61c":"markdown","50f12588":"markdown","9a787703":"markdown","980400dc":"markdown","e4b403fd":"markdown","aa160793":"markdown","a6b238d3":"markdown","d81bc2cf":"markdown","a898600a":"markdown","5658fd37":"markdown","6fc43e72":"markdown","864ea4dd":"markdown","a320c27e":"markdown","23615139":"markdown","f15b54c5":"markdown"},"source":{"7e0f213e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a1c5676c":"!pip3 install contractions\n","0f89c080":"print(os.listdir('..\/input'))","745b60e3":"import re\nimport nltk\nimport contractions\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\nfrom nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer() \nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","2d8556f6":"def check_null(data):\n    for i in data.columns:\n        print(i,\":\",data[str(i)].isna().sum())","ad196909":"data = pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndata.head()","908acb3b":"data.shape","a9534713":"data.info()","af9fa4cb":"check_null(data)","164b3237":"data['review'][0]","2448d8f8":"def clean_txt(txt):\n        ##html code\n        TAG_RE = re.compile(r'<[^>]+>') \n        txt = TAG_RE.sub('', txt.lower())\n        ##emojis\n        txt=txt.encode(\"ascii\",\"ignore\")\n        txt=txt.decode()\n        ##numbers removing\n        txt=''.join(i for i in txt if not i.isdigit())\n        ##punctuation\n        txt = re.sub(r'[^\\w\\s]', ' ', txt) \n        ##stopwords\n        txt = ' '.join([i for i in txt.split() if not i in STOPWORDS])\n        ##removing certain sized words\n        txt=' '.join([i for i in txt.split() if len(i)>2])\n        ##contractions\n        txt=contractions.fix(txt)\n        ##stemmers\n        ##txt= stemmer.stem(txt)  should stemming be performed or lemmatization and why?\n        ##lemmatizer\n        txt=lemmatizer.lemmatize(txt)\n        return txt\nclean_txt(data['review'][0])\n        ","0a2f4d0a":"data['Clean Text']=data['review'].apply(clean_txt)\ndata.head()","28d306f7":"sentiment = {'positive':0,'negative':1}\ndata['sentiment'] =  data['sentiment'].map(sentiment)\ndata.head()","1b5bd4eb":"X = data['Clean Text']\nY = data['sentiment']","c6cb7c06":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2, random_state=42)","963a26f1":"print(len(max(data['Clean Text'],key=len)))\nprint(len(min(data['Clean Text'],key=len)))","ce51678e":"vocab_size = 10000\nembedding_dim = 16\nmax_length = 21\ntrunc_type='post'\noov_tok = \"<OOV>\"\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(x_train)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(x_train)\npadded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(x_test)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)","cef49a8a":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","d2f1a5fa":"num_epochs = 5\nhistory = model.fit(padded, y_train , epochs=num_epochs, validation_data=(testing_padded, y_test))","51eecdd4":"from sklearn.metrics import classification_report \ny_pred = model.predict(testing_padded)\ny_pred = (y_pred > 0.6)\nprint(classification_report(y_test,y_pred))","5ae51074":"import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['Accuracy','Val Accuracy'])\nplt.show()","71e7adf4":"sentences = [ ]\nfor _,row in data.iterrows():\n    sentences+=row['Clean Text'].split()\n[sentences[:2]]","0a21181b":"num_features = 300  # Word vector dimensionality\nmin_word_count = 1 # Minimum word count\nnum_workers = 4     # Number of parallel threads\ncontext = 10        # Context window size\ndownsampling = 1e-3 # (0.001) Downsample setting for frequent words\n\n# Initializing the train model\nfrom gensim.models import word2vec\nprint(\"Training model....\")\nmodel = word2vec.Word2Vec([sentences],\n                          workers=num_workers,\n                          size=num_features,\n                          min_count=min_word_count,\n                          window=context,\n                          sample=downsampling)\n\nprint('Completed')\n# # To make the model memory efficient\nmodel.init_sims(replace=True)\n\n# # Saving the model for later use. Can be loaded using Word2Vec.load()\n# model_name = \"300features_40minwords_10context\"\n# model.save(model_name)","f2741f2b":"list(model.wv.vocab)","08d98c7e":"print (model['one'])","b66d466b":"print(model.wv.most_similar(\"okay\"))","756b3c22":"print(model.wv.most_similar(\"films\"))","3f19c46d":"# Function to average all word vectors in a paragraph\ndef featureVecMethod(words, model, num_features):\n    # Pre-initialising empty numpy array for speed\n    featureVec = np.zeros(num_features,dtype=\"float32\")\n    nwords = 0\n    \n    #Converting Index2Word which is a list to a set for better speed in the execution.\n    index2word_set = set(model.wv.index2word)\n    \n    for word in  words.split():\n        if word in index2word_set:\n            #print(\"Found Word\")\n            nwords = nwords + 1\n            featureVec = np.add(featureVec,model[word])\n    \n    # Dividing the result by number of words to get average\n    featureVec = np.divide(featureVec, nwords)\n    return featureVec\n\n","8ecb234c":"# Function for calculating the average feature vector\ndef getAvgFeatureVecs(reviews, model, num_features):\n    counter = 0\n    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n    for review in reviews:\n        # Printing a status message every 1000th review\n        if counter%1000 == 0:\n            print(\"Review %d of %d\"%(counter,len(reviews)))\n            \n        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n        counter = counter+1\n        \n    return reviewFeatureVecs\n","81e62dde":"trainVectors = getAvgFeatureVecs(x_train,model,num_features)\ntestVectors = getAvgFeatureVecs(x_test,model,num_features)","45b0d1a9":"from sklearn import tree\nfrom sklearn.metrics import classification_report\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(trainVectors,y_train)\nres = clf.predict(testVectors)\nprint(classification_report(y_test,res))","47f4bf24":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(n_estimators = 100)\n    \nforest = forest.fit(trainVectors, y_train)\nres = forest.predict(testVectors)\nprint(classification_report(y_test,res))","1a4d7715":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ny_pred = gnb.fit(trainVectors, y_train).predict(testVectors)\nprint(classification_report(y_test,y_pred))","89c9e16c":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\ny_pred = lr.fit(trainVectors, y_train).predict(testVectors)\nprint(classification_report(y_test,y_pred))","a629c0f6":"GLOVE_DIR='..\/input\/glove6b50dtxt'\nembeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","a4cb0c4b":"EMBEDDING_DIM = 50\nembedding_matrix = np.zeros((len(word_index) + 1, 50))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","47c2f81d":"embedding_layer = tf.keras.layers.Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=max_length,\n                            trainable=False)","a0eb018a":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],input_length=max_length,trainable=False),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","204731f7":"num_epochs = 15\nhistory = model.fit(padded, y_train , epochs=num_epochs, validation_data=(testing_padded, y_test))","b3b0add7":"from sklearn.metrics import classification_report \ny_pred = model.predict(testing_padded)\ny_pred = (y_pred > 0.6)\nprint(classification_report(y_test,y_pred))","2bc5f9a3":"import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['Accuracy','Val Accuracy'])\nplt.show()","44d1a6e8":"## **Converting sentences into tokens**","e80e0f17":"We have imported all the necessary files. Let's Load the Data.","9b4c372d":"## Training Time","29527219":"Again some issue right ? Do you think it is because of the embedding or something else ? Try fixing it. ","1a43e5e7":"Hmm, it seems like there is some issue with the training if you look at those graphs . Can you identify what it is and how to fix it ?\n\nIf yes, give it a go!","883c7f1b":"## **Word2Vec Embeddings**","32b57b7a":"Let's check how our model performed, shall we?","56cc62e8":"Want to see what a embedding vector looks like ?  Run the next cell.","f8e800e3":"As you can see , there are couple of HTML tags in the dataset. We will be getting rid of them , white spaces, some special characters, etc. I often find it hard to keep a count of things which need to be removed from the text , so shall we make a list ?\n\n**To Do Cleaning List**\n\n1) Remove HTML TAGS\n\n2) Remove emojis\n\n3) Remove numbers\n\n4) Remove Punctuation\n\n5) Remove Stopwords\n\n6) Removing words whose length is less than 2\n\n7) Fixing Contractions\n\n8) Stemming or Lemmatizing the words (Upto you which one should be performed)","77e22555":"## **Glove Embedding**","20fcea72":"## Training Time !!","1300e09b":"You will have to download the GloveEmbedding vectors for this if you have downloaded this file and want to run it on your local computer. \n[You can download it here.](https:\/\/nlp.stanford.edu\/data\/glove.6b.zip)\n\nIn this notebook, the Glove Embeddings have already been included.","c8c85229":"## Making the Embedding Matrix","9000a61c":"## **Splitting the Data**","50f12588":"# **That's all Folks! \ud83c\udf89 Hope you enjoyed this tutorial!**\n\nIf you learnt something new , don't forget to upvote this notebook ! If you found something in the notebook and would like to tell me , leave a comment in the notebook and I'll get back to you ASAP. \ud83d\udd96\ud83c\udffc\n\nTake Care ! \ud83d\ude0a","9a787703":"Okay , now our embedding is ready. Let's have a look at our embedding's vocabulary!","980400dc":"## **Checks**\n\nBefore going on with any Machine Learning or Deep Learning Tasks , there are always certain checks which should be made when it comes to the dataset.\nThese include :\n\n1) Checking the shape of the data\n\n2) Checking the type of data in each column (more than often Date type data is given the object tag as a data type)\n\n3) Checking presence of Null values\n\n","e4b403fd":"## Loading the Embedding","aa160793":"# **Natural Language Processing : Word Embeddings and More!\u2728**\n\nThis is the notebook I used in the Webinar conducted by the Data Science Community of SRM University. It is an implementation of various kinds of embeddings and their results on and after training. I have included the link to the ppt used to explain some of the concepts in the webinar(should you find them too hard to get). \n\n\ud83d\udccc [PPT Link](https:\/\/drive.google.com\/file\/d\/1C5DSz-WezR_onXYyIYUM3iU8UbuhVBcN\/view?usp=sharing) : Apart from Embeddings , it also discusses how you can make your Model stronger using RNNS & LSTMS with CNNs , etc , which was covered by my co-host [Harsh Sharma](https:\/\/www.linkedin.com\/in\/harshsharma27\/).\n\nAlright then , hope you have some fun ! \ud83d\ude04","a6b238d3":"## Perfomance Check ","d81bc2cf":"## **Label Encoding**\n\nThe sentiment need to be changed to numbers so that the machine can interpret them correctly.","a898600a":"## **The DL Model**\n\n\nThis is where the embedding layer will come into play. Our padded sequences will be feeded into the network and the network will assign each word in the padded sequence a vector. ","5658fd37":"Let's see whether this embedding knows it's neighbours. Run the next cell to find words in the vocabulary the embedding finds most relatable to the words we have given.","6fc43e72":"## DL Model \n\nThe only difference between this and the DL model we trained in the beginning , is that here the weights of the embedding layer are provided by us. \nThese weights are the ones which came from the Glove embedding.","864ea4dd":"## **Importing Libraries and Downloading Necessary Items**\n","a320c27e":"## **Making the Embedding Matrix \/ Layer**\n\nThe below two codes help make the embedding vector. Logic explained in PPT.","23615139":"## **Finding the Perfect Fit**\n\nTime to try our prepapred vectors with certain classification ML algorithms.","f15b54c5":"## **Text Cleaning**\n\nEvery Dataset has to go through cleaning. Let's check what kind of cleaning we will be performing in our case. "}}