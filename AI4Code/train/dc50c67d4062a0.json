{"cell_type":{"d5154ce9":"code","43955208":"code","ad3e87f9":"code","0a953a3d":"code","57279792":"code","5529c208":"code","1d184e3d":"code","387bf7c5":"code","d335e05d":"code","45711558":"code","a6896482":"code","f4e44f24":"code","0b0ba5e5":"code","f296cdee":"code","7fd26405":"code","909ef8b7":"code","71128183":"code","1a8b9a93":"code","de617764":"code","6559927e":"code","20fe2b2e":"code","05a1f1f0":"code","a762473e":"code","1478c7b1":"code","40b51116":"code","d7113368":"code","1bc895a6":"code","dbac4eb0":"code","2b5c0de6":"code","96516158":"code","58abe7fc":"code","d9f0c6f5":"code","fc281fb3":"code","887cfc42":"code","0452fe1b":"code","1d13e04d":"code","05d3a008":"code","beff1715":"code","ec5cbefe":"code","e71d5d45":"code","5accd577":"code","8eb0841f":"code","82b0ebe0":"code","e4457486":"code","1acd8c3d":"code","77db8ff7":"code","847a7fdf":"code","80c0fe7b":"code","ae2f289c":"code","cb3f4b1d":"code","e4e9be98":"code","d36c7705":"code","0c9ea5c4":"code","1fa1e23f":"code","54c5b9e9":"code","cde08895":"code","7ec45f35":"code","8dde0a05":"code","db679236":"code","2ac49206":"code","c7f1dee0":"code","dbf9738e":"code","a503d73f":"code","d0809bc7":"code","b8efe7a1":"code","bd18c520":"code","2c75177f":"code","db6577d8":"code","7132e34b":"code","160a54de":"code","850f906e":"code","717c1e07":"code","addbb64c":"code","f2a70b31":"code","b9f2f025":"code","1c0eb8e1":"code","7fdad12f":"code","b52b5073":"markdown","ae27869f":"markdown","8471f92b":"markdown","dcfed58b":"markdown","c1e03fa1":"markdown","4f35964d":"markdown","669f3946":"markdown","18f2e9ca":"markdown","ca39261b":"markdown","15de7704":"markdown","f7db2797":"markdown","1502139d":"markdown","c250a5e3":"markdown","3c74cdd6":"markdown","33f36f29":"markdown","6c89ab96":"markdown","89fd0810":"markdown"},"source":{"d5154ce9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (12, 8)\n\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sklearn.base as skb\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nimport sklearn.utils as sku\nimport sklearn.linear_model as sklm\nimport sklearn.neighbors as skn\nimport sklearn.ensemble as ske\nimport catboost as cb\nimport scipy.stats as sstats\nimport random\nseed = 12\nnp.random.seed(seed)\n\nfrom datetime import date","43955208":"!pip install pandas-profiling --quiet\nimport pandas_profiling as pp","ad3e87f9":"# important funtions\ndef datasetShape(df):\n    rows, cols = df.shape\n    print(\"The dataframe has\",rows,\"rows and\",cols,\"columns.\")\n    \n# select numerical and categorical features\ndef divideFeatures(df):\n    numerical_features = df.select_dtypes(include=[np.number])\n    categorical_features = df.select_dtypes(include=[np.object])\n    return numerical_features, categorical_features","0a953a3d":"base = '\/kaggle\/input\/of-genomes-and-genetics-hackerearth-ml\/'\ndata_file = base + \"train.csv\"\ndf = pd.read_csv(data_file)\ndf.head()","57279792":"data_file = base + \"test.csv\"\ndf_test = pd.read_csv(data_file)\ndf_test.head()","5529c208":"# set target feature\ntargetFeature='Genetic Disorder'\ntargetFeature2='Disorder Subclass'","1d184e3d":"# check dataset shape\ndatasetShape(df)","387bf7c5":"# remove ID from train data\ndf.drop(['Patient Id'], inplace=True, axis=1)","d335e05d":"# check for duplicates\nprint(df.shape)\ndf.drop_duplicates(inplace=True)\nprint(df.shape)","45711558":"df.info()","a6896482":"df_test.info()","f4e44f24":"# remove irrelevant columns\ndf.drop(['Patient First Name', 'Family Name', \"Father's name\", \"Father's age\", \"Mother's age\", 'Institute Name', 'Location of Institute', 'Status', 'Parental consent', 'Autopsy shows birth defect (if applicable)', 'Place of birth', 'No. of previous abortion'], axis=1, inplace=True)\ndf_test.drop(['Patient First Name', 'Family Name', \"Father's name\", \"Father's age\", \"Mother's age\", 'Institute Name', 'Location of Institute', 'Status', 'Parental consent', 'Autopsy shows birth defect (if applicable)', 'Place of birth', 'No. of previous abortion'], axis=1, inplace=True)\ndf.describe()","0b0ba5e5":"cont_features, cat_features = divideFeatures(df)\ncat_features.head()","f296cdee":"# check target feature distribution\ndf[targetFeature].hist()\nplt.show()","7fd26405":"# check target feature distribution\ndf[targetFeature2].hist()\nplt.show()","909ef8b7":"# boxplots of numerical features for outlier detection\n\nfig = plt.figure(figsize=(16,16))\nfor i in range(len(cont_features.columns)):\n    fig.add_subplot(4, 4, i+1)\n    sns.boxplot(y=cont_features.iloc[:,i])\nplt.tight_layout()\nplt.show()","71128183":"plt.figure(figsize=(32,32))\nsns.pairplot(df)\nplt.show()","1a8b9a93":"# correlation heatmap for all features\ncorr = df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask = mask, annot=True)\nplt.show()","de617764":"profile = pp.ProfileReport(df, title='Pandas Profiling Report', explorative=True)\nprofile.to_file(\"profile.html\")","6559927e":"profile.to_notebook_iframe()","20fe2b2e":"# remove all columns having no values\ndf.dropna(axis=1, how=\"all\", inplace=True)\ndf_test.dropna(axis=1, how=\"all\", inplace=True)\ndf.dropna(axis=0, how=\"all\", inplace=True)\n\n# drop rows where target features are not available\ndf.dropna(subset=['Genetic Disorder', 'Disorder Subclass'], how='any', inplace=True)\ndatasetShape(df)","05a1f1f0":"# drop single valued column\nkeep = [c for c in list(df) if df[c].nunique() > 1]\ndf = df[keep]\nkeep.remove('Disorder Subclass')\nkeep.remove('Genetic Disorder')\nkeep.insert(0, 'Patient Id')\ndf_test = df_test[keep]\ndatasetShape(df)","a762473e":"# plot missing values\n\ndef calc_missing(df):\n    missing = df.isna().sum().sort_values(ascending=False)\n    missing = missing[missing != 0]\n    missing_perc = missing\/df.shape[0]*100\n    return missing, missing_perc\n\nif df.isna().any().sum()>0:\n    missing, missing_perc = calc_missing(df)\n    missing.plot(kind='bar',figsize=(30,8))\n    plt.title('Missing Values')\n    plt.show()\nelse:\n    print(\"No Missing Values\")","1478c7b1":"def fillNan(df, col, value):\n    df[col].fillna(value, inplace=True)","40b51116":"# setting Assisted conception IVF\/ART missing values to Not available\nfillNan(df, 'Assisted conception IVF\/ART', 'Yes')\nfillNan(df_test, 'Assisted conception IVF\/ART', 'Yes')\ndf['Assisted conception IVF\/ART'].isna().any()","d7113368":"# setting H\/O radiation exposure (x-ray) missing values to -\nfillNan(df, 'H\/O radiation exposure (x-ray)', '-')\nfillNan(df_test, 'H\/O radiation exposure (x-ray)', '-')\ndf['H\/O radiation exposure (x-ray)'].isna().any()","1bc895a6":"# setting Respiratory Rate (breaths\/min) missing values to Normal (30-60)\nfillNan(df, 'Respiratory Rate (breaths\/min)', 'Normal (30-60)')\nfillNan(df_test, 'Respiratory Rate (breaths\/min)', 'Normal (30-60)')\ndf['Respiratory Rate (breaths\/min)'].isna().any()","dbac4eb0":"# setting Folic acid details (peri-conceptional) missing values to Yes\nfillNan(df, 'Folic acid details (peri-conceptional)', 'Yes')\nfillNan(df_test, 'Folic acid details (peri-conceptional)', 'Yes')\ndf['Folic acid details (peri-conceptional)'].isna().any()","2b5c0de6":"# setting H\/O serious maternal illness missing values to No\nfillNan(df, 'H\/O serious maternal illness', 'No')\nfillNan(df_test, 'H\/O serious maternal illness', 'No')\ndf['H\/O serious maternal illness'].isna().any()","96516158":"# setting Birth asphyxia missing values to Not available\nfillNan(df, 'Birth asphyxia', 'Not available')\nfillNan(df_test, 'Birth asphyxia', 'Not available')\ndf['Birth asphyxia'].isna().any()","58abe7fc":"# setting Birth defects missing values to Singular\nfillNan(df, 'Birth defects', 'Singular')\nfillNan(df_test, 'Birth defects', 'Singular')\ndf['Birth defects'].isna().any()","d9f0c6f5":"# setting Blood test result missing values to inconclusive\nfillNan(df, 'Blood test result', 'inconclusive')\nfillNan(df_test, 'Blood test result', 'inconclusive')\ndf['Blood test result'].isna().any()","fc281fb3":"# setting H\/O substance abuse missing values to -\nfillNan(df, 'H\/O substance abuse', '-')\nfillNan(df_test, 'H\/O substance abuse', '-')\ndf['H\/O substance abuse'].isna().any()","887cfc42":"# setting missing values to mean values\nfillNan(df, 'White Blood cell count (thousand per microliter)', df['White Blood cell count (thousand per microliter)'].mean())\nfillNan(df_test, 'White Blood cell count (thousand per microliter)', df['White Blood cell count (thousand per microliter)'].mean())\ndf['White Blood cell count (thousand per microliter)'].isna().any()","0452fe1b":"# setting History of anomalies in previous pregnancies missing values to No\nfillNan(df, 'History of anomalies in previous pregnancies', 'No')\nfillNan(df_test, 'History of anomalies in previous pregnancies', 'No')\ndf['History of anomalies in previous pregnancies'].isna().any()","1d13e04d":"# setting Inherited from father missing values to No\nfillNan(df, 'Inherited from father', 'No')\nfillNan(df_test, 'Inherited from father', 'No')\ndf['Inherited from father'].isna().any()","05d3a008":"# setting Gender missing values to Ambiguous\nfillNan(df, 'Gender', 'Ambiguous')\nfillNan(df_test, 'Gender', 'Ambiguous')\ndf['Gender'].isna().any()","beff1715":"# setting Follow-up missing values to Low\nfillNan(df, 'Follow-up', 'Low')\nfillNan(df_test, 'Follow-up', 'Low')\ndf['Follow-up'].isna().any()","ec5cbefe":"# setting Maternal gene missing values to No\nfillNan(df, 'Maternal gene', 'No')\nfillNan(df_test, 'Maternal gene', 'No')\ndf['Maternal gene'].isna().any()","e71d5d45":"# setting missing values to mean values\nfillNan(df, 'Patient Age', df['Patient Age'].mean())\nfillNan(df_test, 'Patient Age', df['Patient Age'].mean())\ndf['Patient Age'].isna().any()","5accd577":"# setting missing values to most occurring values\nfillNan(df, 'Symptom 1', df['Symptom 1'].mode()[0])\nfillNan(df_test, 'Symptom 1', df['Symptom 1'].mode()[0])\n\nfillNan(df, 'Symptom 2', df['Symptom 2'].mode()[0])\nfillNan(df_test, 'Symptom 2', df['Symptom 2'].mode()[0])\n\nfillNan(df, 'Symptom 3', df['Symptom 3'].mode()[0])\nfillNan(df_test, 'Symptom 3', df['Symptom 3'].mode()[0])\n\nfillNan(df, 'Symptom 4', df['Symptom 4'].mode()[0])\nfillNan(df_test, 'Symptom 4', df['Symptom 4'].mode()[0])\n\nfillNan(df, 'Symptom 5', df['Symptom 5'].mode()[0])\nfillNan(df_test, 'Symptom 5', df['Symptom 5'].mode()[0])\n            \nfillNan(df, 'Heart Rate (rates\/min', df['Heart Rate (rates\/min'].mode()[0])\nfillNan(df_test, 'Heart Rate (rates\/min', df['Heart Rate (rates\/min'].mode()[0])","8eb0841f":"print(\"Train Missing:\",df.isna().any().sum())\nprint(\"Test Missing:\",df_test.isna().any().sum())","82b0ebe0":"cont_features, cat_features = divideFeatures(df)\ncat_features","e4457486":"custom_feat = [\"Genes in mother's side\",\n 'Inherited from father',\n 'Maternal gene',\n 'Paternal gene',\n 'Respiratory Rate (breaths\/min)',\n 'Heart Rate (rates\/min',\n 'Follow-up',\n 'Gender',\n 'Birth asphyxia',\n 'Folic acid details (peri-conceptional)',\n 'H\/O serious maternal illness',\n 'H\/O radiation exposure (x-ray)',\n 'H\/O substance abuse',\n 'Assisted conception IVF\/ART',\n 'History of anomalies in previous pregnancies',\n 'Birth defects',\n 'Blood test result']","1acd8c3d":"# extract numerical and categorical for dummy and scaling later\nfor feat in custom_feat:\n    dummyVars = pd.get_dummies(df[feat], drop_first=True, prefix=feat+\"_\")\n    df = pd.concat([df, dummyVars], axis=1)\n    df.drop(feat, axis=1, inplace=True)\ndatasetShape(df)\n\ndf.head()","77db8ff7":"# extract numerical and categorical for dummy and scaling later\nfor feat in custom_feat:\n    dummyVars = pd.get_dummies(df_test[feat], drop_first=True, prefix=feat+\"_\")\n    df_test = pd.concat([df_test, dummyVars], axis=1)\n    df_test.drop(feat, axis=1, inplace=True)\ndatasetShape(df_test)\n\ndf_test.head()","847a7fdf":"# helper functions\n\ndef printScore(y_train, y_train_pred):\n    print(skm.f1_score(y_train, y_train_pred, average=\"macro\"))","80c0fe7b":"df_f1 = df.sample(frac=1, random_state=seed).reset_index(drop=True)\ndf_f2 = df.sample(frac=1, random_state=seed).reset_index(drop=True)","ae2f289c":"# remove Disorder Subclass from df1\ndf_f1.drop('Disorder Subclass', inplace=True, axis=1)\n\n# convert Genetic Disorder to one-hot\n# dummyVars = pd.get_dummies(df_f2['Genetic Disorder'], drop_first=True, prefix=\"GeneticDisorder_\")\n# df_f2 = pd.concat([df_f2, dummyVars], axis=1)\n# df_f2.drop('Genetic Disorder', axis=1, inplace=True)\n\n# convert Genetic Disorder to label-encoding\ngdle = skp.LabelEncoder()\ndf_f2['Genetic Disorder'] = gdle.fit_transform(df_f2['Genetic Disorder'])","cb3f4b1d":"# shuffle samples\ndf_f1_shuffle = df_f1.sample(frac=1, random_state=seed).reset_index(drop=True)\ndf_f2_shuffle = df_f2.sample(frac=1, random_state=seed).reset_index(drop=True)\n\n# separate target feature\ndf_f1_y = df_f1_shuffle.pop(targetFeature)\ndf_f1_X = df_f1_shuffle\n\n# transform the text label to integers\nf1_le = skp.LabelEncoder()\ndf_f1_y = f1_le.fit_transform(df_f1_y)\n# print(f1_le.classes_)\n\n# split into train dev and test\nX_f1_train, X_f1_test, y_f1_train, y_f1_test = skms.train_test_split(df_f1_X, df_f1_y, train_size=0.8, random_state=seed)\nprint(f\"Train set has {X_f1_train.shape[0]} records out of {len(df_f1_shuffle)} which is {round(X_f1_train.shape[0]\/len(df_f1_shuffle)*100)}%\")\nprint(f\"Test set has {X_f1_test.shape[0]} records out of {len(df_f1_shuffle)} which is {round(X_f1_test.shape[0]\/len(df_f1_shuffle)*100)}%\")\n\n# separate target feature\ndf_f2_y = df_f2_shuffle.pop(targetFeature2)\ndf_f2_X = df_f2_shuffle\n\n# transform the text label to integers\nf2_le = skp.LabelEncoder()\ndf_f2_y = f2_le.fit_transform(df_f2_y)\n# print(f2_le.classes_)\n\n# split into train dev and test\nX_f2_train, X_f2_test, y_f2_train, y_f2_test = skms.train_test_split(df_f2_X, df_f2_y, train_size=0.8, random_state=seed)\nprint(f\"Train set has {X_f2_train.shape[0]} records out of {len(df_f2_shuffle)} which is {round(X_f2_train.shape[0]\/len(df_f2_shuffle)*100)}%\")\nprint(f\"Test set has {X_f2_test.shape[0]} records out of {len(df_f2_shuffle)} which is {round(X_f2_test.shape[0]\/len(df_f2_shuffle)*100)}%\")","e4e9be98":"# reset index for X_train and X_test\nX_f1_train.reset_index(drop=True, inplace=True)\nX_f1_test.reset_index(drop=True, inplace=True)\nX_f1_train.index[:5]","d36c7705":"# reset index for X_train and X_test\nX_f2_train.reset_index(drop=True, inplace=True)\nX_f2_test.reset_index(drop=True, inplace=True)\nX_f2_train.index[:5]","0c9ea5c4":"# scaler = skp.RobustScaler()\n# scaler = skp.MinMaxScaler()\nscaler = skp.StandardScaler()\n\n# apply scaling to all numerical variables except dummy variables as they are already between 0 and 1\nX_f1_train[cont_features.columns] = pd.DataFrame(scaler.fit_transform(X_f1_train[cont_features.columns]), columns=cont_features.columns)\n\n# scale test data with transform()\nX_f1_test[cont_features.columns] = pd.DataFrame(scaler.transform(X_f1_test[cont_features.columns]), columns=cont_features.columns)\n\n# view sample data\nX_f1_train.describe()","1fa1e23f":"# scaler = skp.RobustScaler()\n# scaler = skp.MinMaxScaler()\nscaler = skp.StandardScaler()\n\n# apply scaling to all numerical variables except dummy variables as they are already between 0 and 1\nX_f2_train[cont_features.columns] = pd.DataFrame(scaler.fit_transform(X_f2_train[cont_features.columns]), columns=cont_features.columns)\n\n# scale test data with transform()\nX_f2_test[cont_features.columns] = pd.DataFrame(scaler.transform(X_f2_test[cont_features.columns]), columns=cont_features.columns)\n\n# view sample data\nX_f2_train.describe()","54c5b9e9":"class_weights_f1 = sku.class_weight.compute_class_weight('balanced', np.unique(y_f1_train), y_f1_train)\nclass_weights_f1 = dict(enumerate(class_weights_f1))\nclass_weights_f1","cde08895":"class_weights_f2 = sku.class_weight.compute_class_weight('balanced', np.unique(y_f2_train), y_f2_train)\nclass_weights_f2 = dict(enumerate(class_weights_f2))\nclass_weights_f2","7ec45f35":"sample_weights_f1 = sku.class_weight.compute_sample_weight('balanced', y_f1_train)\nsample_weights_f1","8dde0a05":"sample_weights_f2 = sku.class_weight.compute_sample_weight('balanced', y_f2_train)\nsample_weights_f2","db679236":"import catboost as cb\n\ncat_model_f1 = cb.CatBoostClassifier(verbose=0, iterations=70, \n#                                   eval_metric='F1', \n                                  class_weights=class_weights_f1, \n#                                   use_best_model=True\n                                 )\ncat_model_f1.fit(X_f1_train, y_f1_train, eval_set=(X_f1_test, y_f1_test))\nprint(cat_model_f1.best_score_)\n\ny_f1_train_pred = cat_model_f1.predict(X_f1_train)\ny_f1_test_pred = cat_model_f1.predict(X_f1_test)\nprint(skm.accuracy_score(y_f1_train, y_f1_train_pred))\nprint(skm.accuracy_score(y_f1_test, y_f1_test_pred))\nprintScore(y_f1_train, y_f1_train_pred)\nprintScore(y_f1_test, y_f1_test_pred)","2ac49206":"import catboost as cb\n\ncat_model_f2 = cb.CatBoostClassifier(verbose=0, iterations=80, \n#                                   eval_metric='F1', \n                                  class_weights=class_weights_f2, \n#                                   use_best_model=True\n                                 )\ncat_model_f2.fit(X_f2_train, y_f2_train, eval_set=(X_f2_test, y_f2_test))\nprint(cat_model_f2.best_score_)\n\ny_f2_train_pred = cat_model_f2.predict(X_f2_train)\ny_f2_test_pred = cat_model_f2.predict(X_f2_test)\nprint(skm.accuracy_score(y_f2_train, y_f2_train_pred))\nprint(skm.accuracy_score(y_f2_test, y_f2_test_pred))\nprintScore(y_f2_train, y_f2_train_pred)\nprintScore(y_f2_test, y_f2_test_pred)","c7f1dee0":"rf_model_f1 = ske.RandomForestClassifier(verbose=0, random_state=1, n_jobs=-1, class_weight='balanced_subsample',\n                                 n_estimators=100,max_depth=10, \n                                 min_samples_split = 5, min_samples_leaf = 3\n                                )\nrf_model_f1.fit(X_f1_train, y_f1_train)\n\n# predict\ny_f1_train_pred = rf_model_f1.predict(X_f1_train)\ny_f1_test_pred = rf_model_f1.predict(X_f1_test)\nprint(skm.accuracy_score(y_f1_train, y_f1_train_pred))\nprint(skm.accuracy_score(y_f1_test, y_f1_test_pred))\nprintScore(y_f1_train, y_f1_train_pred)\nprintScore(y_f1_test, y_f1_test_pred)","dbf9738e":"rf_model_f2 = ske.RandomForestClassifier(verbose=0, random_state=1, n_jobs=-1, class_weight='balanced_subsample',\n                                 n_estimators=300,max_depth=10, \n                                 min_samples_split = 10, min_samples_leaf = 5\n                                )\nrf_model_f2.fit(X_f2_train, y_f2_train)\n\n# predict\ny_f2_train_pred = rf_model_f2.predict(X_f2_train)\ny_f2_test_pred = rf_model_f2.predict(X_f2_test)\nprint(skm.accuracy_score(y_f2_train, y_f2_train_pred))\nprint(skm.accuracy_score(y_f2_test, y_f2_test_pred))\nprintScore(y_f2_train, y_f2_train_pred)\nprintScore(y_f2_test, y_f2_test_pred)","a503d73f":"import xgboost as xg","d0809bc7":"# # Grid used for parameter tuning\n# param_test1 = {\n#     'max_depth': np.arange(5, 12, 2),\n#     'learning_rate': np.arange(0.04, 0.07, 0.01)\n# }\n# xgb_cv1 = skms.GridSearchCV(estimator = xg.XGBClassifier(n_estimators=100, objective='macro', nthread=4, seed=seed), \n#                              param_grid = param_test1, scoring='f1', n_jobs=4, \n#                              cv=3, verbose=1)\n# xgb_cv1.fit(X_f1_train, y_f1_train)\n# print(xgb_cv1.best_params_, xgb_cv1.best_score_)\n# # max_depth = 10\n# # learning_rate = 0.04","b8efe7a1":"# # Grid used for parameter tuning\n# param_test2 = {\n#  'subsample': np.arange(0.5, 1, 0.1),\n#  'min_child_weight': range(1, 6, 1)\n# }\n# xgb_cv2 = skms.GridSearchCV(estimator = xg.XGBClassifier(n_estimators=500, max_depth = 10, \n#                                                      objective= 'multi:softprob', nthread=4, seed=seed), \n#                             param_grid = param_test2, scoring='f1', n_jobs=4,\n#                             cv=5, verbose=1)\n# xgb_cv2.fit(X_train_small, y_train_small)\n# print(xgb_cv2.best_params_, xgb_cv2.best_score_)\n# print(xgb_cv2.best_estimator_)\n# # subsample = 0.5\n# # min_child_weight = 2","bd18c520":"xgb_model_f1 = xg.XGBClassifier(objective ='multi:softprob', random_state=seed, scoring='f1', \n                             learning_rate=0.0001, subsample=0.5, n_jobs=-1, sample_weight=sample_weights_f1,\n                             n_estimators=100, max_depth = 8)\nxgb_model_f1.fit(X_f1_train, y_f1_train)\n\n# predict\ny_f1_train_pred = xgb_model_f1.predict(X_f1_train)\ny_f1_test_pred = xgb_model_f1.predict(X_f1_test)\nprint(skm.accuracy_score(y_f1_train, y_f1_train_pred))\nprint(skm.accuracy_score(y_f1_test, y_f1_test_pred))\nprintScore(y_f1_train, y_f1_train_pred)\nprintScore(y_f1_test, y_f1_test_pred)","2c75177f":"xgb_model_f2 = xg.XGBClassifier(objective ='multi:softprob', random_state=seed, scoring='f1', \n                             learning_rate=0.15, subsample=1, n_jobs=-1, sample_weight=sample_weights_f2,\n                             n_estimators=100, max_depth = 5)\nxgb_model_f2.fit(X_f2_train, y_f2_train)\n\n# predict\ny_f2_train_pred = xgb_model_f2.predict(X_f2_train)\ny_f2_test_pred = xgb_model_f2.predict(X_f2_test)\nprint(skm.accuracy_score(y_f2_train, y_f2_train_pred))\nprint(skm.accuracy_score(y_f2_test, y_f2_test_pred))\nprintScore(y_f2_train, y_f2_train_pred)\nprintScore(y_f2_test, y_f2_test_pred)","db6577d8":"import lightgbm as lgb\nlgb_model_f1 = lgb.LGBMClassifier(objective='multi', random_state=1, n_jobs=-1, \n                               class_weight=class_weights_f1,\n                               learning_rate=0.1, n_estimators=70)\nlgb_model_f1.fit(X_f1_train, y_f1_train)\n\n# predict\ny_f1_train_pred = lgb_model_f1.predict(X_f1_train)\ny_f1_test_pred = lgb_model_f1.predict(X_f1_test)\nprint(skm.accuracy_score(y_f1_train, y_f1_train_pred))\nprint(skm.accuracy_score(y_f1_test, y_f1_test_pred))\nprintScore(y_f1_train, y_f1_train_pred)\nprintScore(y_f1_test, y_f1_test_pred)","7132e34b":"import lightgbm as lgb\nlgb_model_f2 = lgb.LGBMClassifier(objective='multi', random_state=1, n_jobs=-1, \n#                                class_weight=class_weights_f2,\n                               learning_rate=0.08, n_estimators=100)\nlgb_model_f2.fit(X_f2_train, y_f2_train)\n\n# predict\ny_f2_train_pred = lgb_model_f2.predict(X_f2_train)\ny_f2_test_pred = lgb_model_f2.predict(X_f2_test)\nprint(skm.accuracy_score(y_f2_train, y_f2_train_pred))\nprint(skm.accuracy_score(y_f2_test, y_f2_test_pred))\nprintScore(y_f2_train, y_f2_train_pred)\nprintScore(y_f2_test, y_f2_test_pred)","160a54de":"# Generate Ensembles\n\ndef rmse_cv(model):\n    '''\n    Use this function to get quickly the rmse score over a cv\n    '''\n    rmse = np.sqrt(-skms.cross_val_score(model, X_train, y_train, \n                                         scoring=\"neg_mean_squared_error\", cv = 5, n_jobs=-1))\n    return rmse\n\nclass MixModel(skb.BaseEstimator, skb.RegressorMixin, skb.TransformerMixin):\n    '''\n    Here we will get a set of models as parameter already trained and \n    will calculate the mean of the predictions for using each model predictions\n    '''\n    def __init__(self, algs):\n        self.algs = algs\n\n    # Define clones of parameters models\n    def fit(self, X, y):\n        self.algs_ = [skb.clone(x) for x in self.algs]\n        \n        # Train cloned base models\n        for alg in self.algs_:\n            alg.fit(X, y)\n\n        return self\n    \n    # Average predictions of all cloned models\n    def predict(self, X):\n        predictions = np.column_stack([\n            stacked_model.predict(X) for stacked_model in self.algs_\n        ])\n        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=predictions)","850f906e":"mixed_model_f1 = MixModel(algs = [\n    cat_model_f1,\n    rf_model_f1,\n    xgb_model_f1,\n    lgb_model_f1\n])\n# score = rmse_cv(mixed_model)\n# print(\"\\nAveraged base algs score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nmixed_model_f1.fit(X_f1_train, y_f1_train)\n\n# predict\ny_f1_train_pred = mixed_model_f1.predict(X_f1_train)\ny_f1_test_pred = mixed_model_f1.predict(X_f1_test)\nprintScore(y_f1_train, y_f1_train_pred)\nprintScore(y_f1_test, y_f1_test_pred)","717c1e07":"mixed_model_f2 = MixModel(algs = [\n#     cat_model_f2,\n    rf_model_f2,\n#     xgb_model_f2,\n#     lgb_model_f2\n])\n# score = rmse_cv(mixed_model)\n# print(\"\\nAveraged base algs score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\nmixed_model_f2.fit(X_f2_train, y_f2_train)\n\n# predict\ny_f2_train_pred = mixed_model_f2.predict(X_f2_train)\ny_f2_test_pred = mixed_model_f2.predict(X_f2_test)\nprintScore(y_f2_train, y_f2_train_pred)\nprintScore(y_f2_test, y_f2_test_pred)","addbb64c":"# generate test results for targetFeature\ndef getTestResults():\n    df_final_f1 = df_f1.sample(frac=1, random_state=1).reset_index(drop=True)\n    test_cols_f1 = [x for x in df_final_f1.columns if targetFeature not in x]\n    df_final_test_f1 = df_test[test_cols_f1]\n    df_y_f1 = df_final_f1.pop(targetFeature)\n    df_X_f1 = df_final_f1\n    \n    df_y_f1 = f1_le.transform(df_y_f1)\n\n    scaler_f1 = skp.RobustScaler()\n#     scaler = skp.MinMaxScaler()\n#     scaler = skp.StandardScaler()\n\n    df_X_f1[cont_features.columns] = pd.DataFrame(scaler.fit_transform(df_X_f1[cont_features.columns]), columns=cont_features.columns)\n    df_final_test_f1[cont_features.columns] = pd.DataFrame(scaler.transform(df_final_test_f1[cont_features.columns]), columns=cont_features.columns)\n\n#     sample_weights_f1 = sku.class_weight.compute_sample_weight('balanced', df_y_f1)\n    \n    model_f1 = MixModel(algs = [\n#         cat_model_f1,\n#         rf_model_f1,\n        xgb_model_f1,\n#         lgb_model_f1\n    ])\n\n    model_f1.fit(df_X_f1, df_y_f1)\n\n    # predict\n    y_train_pred_f1 = model_f1.predict(df_X_f1)\n    y_test_pred_f1 = model_f1.predict(df_final_test_f1)\n    print(\"Accuracy Score for Train:\",skm.accuracy_score(df_y_f1, y_train_pred_f1))\n    printScore(df_y_f1, y_train_pred_f1)\n    return y_test_pred_f1\n\n# ML models\nresults = getTestResults()","f2a70b31":"submission = pd.DataFrame({\n    'Patient Id': df_test['Patient Id'],\n    targetFeature: f1_le.inverse_transform(results.ravel()),\n})\nprint(submission[targetFeature].value_counts())","b9f2f025":"# generate test results for targetFeature2\ndef getTestResults():\n    df_final_f2 = df_f2.sample(frac=1, random_state=1).reset_index(drop=True)\n    test_cols_f2 = [x for x in df_final_f2.columns if targetFeature2 not in x]\n    df_final_test_f2 = df_test[test_cols_f2]\n    df_y_f2 = df_final_f2.pop(targetFeature2)\n    df_X_f2 = df_final_f2\n    \n    df_y_f2 = f2_le.transform(df_y_f2)\n\n    scaler_f2 = skp.RobustScaler()\n#     scaler = skp.MinMaxScaler()\n#     scaler = skp.StandardScaler()\n\n    df_X_f2[cont_features.columns] = pd.DataFrame(scaler.fit_transform(df_X_f2[cont_features.columns]), columns=cont_features.columns)\n    df_final_test_f2[cont_features.columns] = pd.DataFrame(scaler.transform(df_final_test_f2[cont_features.columns]), columns=cont_features.columns)\n\n#     sample_weights_f2 = sku.class_weight.compute_sample_weight('balanced', df_y_f2)\n    \n    model_f2 = MixModel(algs = [\n#         cat_model_f2,\n        rf_model_f2,\n#         xgb_model_f2,\n#         lgb_model_f2\n    ])\n\n    model_f2.fit(df_X_f2, df_y_f2)\n\n    # predict\n    y_train_pred_f2 = model_f2.predict(df_X_f2)\n    y_test_pred_f2 = model_f2.predict(df_final_test_f2)\n    print(\"Accuracy Score for Train:\",skm.accuracy_score(df_y_f2, y_train_pred_f2))\n    printScore(df_y_f2, y_train_pred_f2)\n    return y_test_pred_f2\n\n# ML models\ndf_test[targetFeature] = results.ravel()\nresults2 = getTestResults()","1c0eb8e1":"submission[targetFeature2] = f2_le.inverse_transform(results2.ravel())\nprint(submission[targetFeature2].value_counts())","7fdad12f":"# generate submission file\nsubmission.to_csv('.\/submission_XGB_RF1.csv', index=False)","b52b5073":"### RandomForest","ae27869f":"# Step 1: Reading and Understanding the Data","8471f92b":"# Step 5: Test Evaluation & Submission","dcfed58b":"# Step 2: EDA","c1e03fa1":"### XGBoost","4f35964d":"# Step 4: Data Modelling\n\n### Split Train-Test Data","669f3946":"### LightGBM","18f2e9ca":"### Profiling for Whole Data","ca39261b":"# HackerEarth ML - Of Genomes And Genetics","15de7704":"### Handle Missing","f7db2797":"### Feature Scaling","1502139d":"## Model Building","c250a5e3":"### Univariate Analysis","3c74cdd6":"### CatBoost","33f36f29":"With XGB and RF, 33.87 LB is scored. You can clone and try with other models.","6c89ab96":"# Step 3: Data Preparation","89fd0810":"### One-hot Encoding"}}