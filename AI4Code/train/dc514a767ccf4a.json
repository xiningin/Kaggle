{"cell_type":{"81507d3b":"code","e749bd97":"code","e04cadf0":"code","f01e4a37":"code","6425a31f":"code","2bede8ff":"code","de2c7ab2":"code","cb41bffb":"code","a9199970":"code","31b87572":"code","93b0a3b8":"code","68cb7978":"code","4bc8a184":"code","1822f2a4":"code","60e1b537":"code","e9745384":"code","86467b9e":"code","94a76d82":"code","b836326d":"code","165c945b":"code","4c89b02e":"code","83055325":"code","e5e756b4":"code","390139d5":"code","81ad0c80":"code","2a0a8ae9":"code","ea38b03e":"code","669dca37":"code","7bed5cdf":"code","93f929f4":"code","119df07b":"code","c5c2e8c5":"code","c5608ca6":"code","b84297ae":"code","e3d858a3":"code","57a7f82d":"markdown","8d0e931e":"markdown","089d785e":"markdown","8eacc52d":"markdown","c8d112a1":"markdown","21b88a7e":"markdown","45da4e95":"markdown","bd32bd54":"markdown","23f5df56":"markdown","dd4cca14":"markdown","5a1b8d16":"markdown","2460519f":"markdown","218f2b66":"markdown","37039aa6":"markdown","2ee93b2a":"markdown"},"source":{"81507d3b":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn import preprocessing \n","e749bd97":"file = '..\/input\/mushrooms.csv'\nm_data = pd.read_csv(file)","e04cadf0":"m_data.head()","f01e4a37":"m_data = m_data.apply(preprocessing.LabelEncoder().fit_transform)","6425a31f":"m_data.head()","2bede8ff":"m_data.describe()","de2c7ab2":"df_all_corr = m_data.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_all_corr[df_all_corr['Feature 1'] == 'class']","cb41bffb":"df_all_corr = m_data.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_all_corr[df_all_corr['Feature 1'] == 'gill-size']","a9199970":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n\ny=m_data['class']\nx=m_data.drop(['class'], axis=1)\n\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=11)\nfit = bestfeatures.fit(x,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(x.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","31b87572":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n\netmodel = ExtraTreesClassifier()\netmodel.fit(x,y)\nfeat_importances = pd.Series(etmodel.feature_importances_, index=x.columns).sort_values(kind=\"quicksort\", ascending=False).reset_index()\nprint(feat_importances)","93b0a3b8":"# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model, 10)\nfit = rfe.fit(x, y)\nprint(\"Num Features: {}\".format(fit.n_features_))\nprint(\"Selected Features: {}\".format(fit.support_))\nprint(\"Feature Ranking: {}\".format(fit.ranking_))","68cb7978":"x.columns","4bc8a184":"df_all_corr = m_data.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_all_corr[df_all_corr['Feature 1'] == 'gill-attachment']","1822f2a4":"df_all_corr = m_data.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_all_corr[df_all_corr['Feature 1'] == 'veil-color']","60e1b537":"df_all_corr = m_data.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_all_corr[df_all_corr['Feature 1'] == 'ring-number']","e9745384":"features = ['gill-color', 'gill-attachment', 'ring-type', 'ring-number', 'gill-size', 'bruises', 'stalk-root',\n            'gill-spacing', 'habitat', 'spore-print-color', 'stalk-surface-above-ring', 'class']\n\ndata_prefinal = m_data[features]","86467b9e":"import sklearn as sk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.constraints import maxnorm\n\nx_all = data_prefinal.drop(['class'], axis=1)\ny_all = data_prefinal['class']\n\nx_train, x_val, y_train, y_val = train_test_split(x_all, y_all, test_size=0.33, random_state=23)","94a76d82":"x_all.shape","b836326d":"def print_score(classifier,x_train,y_train,x_val,y_val,train=True):\n    if train == True:\n        print(\"Training results:\\n\")\n        print('Accuracy Score: {0:.4f}\\n'.format(accuracy_score(y_train,classifier.predict(x_train))))\n        print('Classification Report:\\n{}\\n'.format(classification_report(y_train,classifier.predict(x_train))))\n        print('Confusion Matrix:\\n{}\\n'.format(confusion_matrix(y_train,classifier.predict(x_train))))\n        res = cross_val_score(classifier, x_train, y_train, cv=10, n_jobs=-1, scoring='balanced_accuracy')\n        print('Average Accuracy:\\t{0:.4f}\\n'.format(res.mean()))\n        print('Standard Deviation:\\t{0:.4f}'.format(res.std()))\n    elif train == False:\n        print(\"Test results:\\n\")\n        print('Accuracy Score: {0:.4f}\\n'.format(accuracy_score(y_val,classifier.predict(x_val))))\n        print('Classification Report:\\n{}\\n'.format(classification_report(y_val,classifier.predict(x_val))))\n        print('Confusion Matrix:\\n{}\\n'.format(confusion_matrix(y_val,classifier.predict(x_val))))","165c945b":"svcmodel = svm.SVC(kernel='linear', gamma='scale').fit(x_train, y_train)\nsvprediction = svcmodel.predict(x_val)\n\ndef svc_param_selection(X, y, nfolds):\n    Cs = [0.001]\n    gammas = [0.1]\n    param_grid = {'C': Cs, 'gamma' : gammas}\n    grid_search = GridSearchCV(svm.SVC(kernel = 'linear'), param_grid, cv=nfolds)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    print(grid_search.best_params_) \n    return grid_search.best_estimator_\n\nsv_best = svc_param_selection(x_train, y_train, 10)\nsv_prediction = sv_best.predict(x_val)\n","4c89b02e":"print_score(sv_best, x_train, y_train, x_val, y_val, train=True)","83055325":"print_score(sv_best, x_train, y_train, x_val, y_val, train=False)","e5e756b4":"rf = RandomForestClassifier()\nrfmodel = rf.fit(x_train, y_train)\nprediction = rfmodel.predict(x_val)","390139d5":"print_score(rfmodel, x_train, y_train, x_val, y_val, train=True)","81ad0c80":"print_score(rfmodel, x_train, y_train, x_val, y_val, train=False)","2a0a8ae9":"lrmodel = LogisticRegression()\nlrmodel = lrmodel.fit(x_train, y_train)","ea38b03e":"print_score(lrmodel, x_train, y_train, x_val, y_val, train=True)","669dca37":"def knn_param_selection(X, y, nfolds):\n    n_neighbors = [1, 2, 3, 4, 5, 6, 7]\n    weights = ['distance', 'uniform']\n    metric = ['euclidean', 'manhattan']\n    param_grid = {'n_neighbors': n_neighbors, 'weights' : weights, 'metric': metric}\n    grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=nfolds)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    print(grid_search.best_params_) \n    return grid_search.best_estimator_\n\nknn_best = knn_param_selection(x_train, y_train, 10)","7bed5cdf":"print_score(knn_best, x_train, y_train, x_val, y_val, train=True)","93f929f4":"print_score(knn_best, x_train, y_train, x_val, y_val, train=False)","119df07b":"nnmodel = Sequential()\nnnmodel.add(Dense(15, input_dim = 11, activation='relu'))\nnnmodel.add(Dropout(0.2))\nnnmodel.add(Dense(15, activation='relu'))\nnnmodel.add(Dropout(0.2))\nnnmodel.add(Dense(15, activation='sigmoid'))\nnnmodel.add(Dropout(0.2))\nnnmodel.add(Dense(15, activation='relu'))\nnnmodel.add(Dropout(0.2))\nnnmodel.add(Dense(1, activation='sigmoid'))\n\nnnmodel.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['accuracy'])","c5c2e8c5":"nnmodel.fit(x_train, y_train, batch_size=8100, epochs=2000)","c5608ca6":"y_pred=nnmodel.predict(x_val)\ny_pred=(y_pred>0.5)","b84297ae":"print(confusion_matrix(y_val, y_pred))","e3d858a3":"print(classification_report(y_val, y_pred))","57a7f82d":"## Method #3 RFE (Recursive Feature Elimination).\nThis method workd by recursively removing attributes and building a model on those features remaining, we choose that the method show us the 10 more relevant features in this dataset.\nThe methods used for this tasks are:\nsupport_: Shows a boolean list that give what features to choose.\nranking_: Shows a list with a ranking from 1 (important features) to N with N the number of features in the dataset - Number1 features.","8d0e931e":"# Feature selection.\nWe are going to see if the column to predict (class) have a high correlation with some other features, sorting it.","089d785e":"Gill-attachment feature don't have correlation with other feature that has relevation so we can put this as a \nfeature.\nVeil-color is a redundant feature because we have gil-attachment.","8eacc52d":"We see that the global score are high for both scores train and validation.","c8d112a1":"# Machine learning modeling.\nwe are going to compare some models with this data and choose those with good classification_report.","21b88a7e":"We can simply LabelEncode all the features with categorical data.","45da4e95":"We furthermore gridSearch for parameter tunning.\nFor example in the first model Support Vector Machine i choose a invariant linear kernel but with search for Cs and gammas.","bd32bd54":"## Method #2 Model feature_importances.\nWe can simply build a simply model an have a look for feature importances a weighted method for feature selection.","23f5df56":"## Method #1 Feature Selection KBest scores.\nSelectKBest class give us an idea of how important the features are, we give to the class the chi2 score function that stats of non-negative features for classification tasks.","dd4cca14":"The features that i have choose are from all the three method plus previously having removed the features with high correlation transitively.","5a1b8d16":"We are gonna test a Neural Network for classfication purposes, we build our model with an input of 11 (number of features), and hidden layers with 15 and Dropout of 0.2 for generalization purposes, the output layer must be a sigmoid to see the correct values between 0 and 1.\nWe compile with binary_crossentropy for binary classification and optimize with adamax, we can grid search the hidden layer nodes or the optimizer or simply the Dropout parameter, but the classification accuracy are good for this classification problem and doesn't need tunning.","2460519f":"we can see that \"gill-size\" are high correlated with class, we can simply search for features that are high correlated with gill-size, that can give us clues for redundant features that can give error or more training time to our predict models.","218f2b66":"This function try to automate the comparing process between models and different metrics both in trainning and validation data.","37039aa6":"we can observe that, we don't have missing values on the data.","2ee93b2a":"With this model we are reach the perfect classification accuracy and Confusion Matrix and validation scores support our hypothesis."}}