{"cell_type":{"8d6cef44":"code","ba3ab744":"code","36eadb28":"code","c9f272e9":"code","c648d7f0":"code","adc20db4":"code","fb27426f":"code","c60c3007":"code","470b4e62":"code","ce4055d7":"code","a3098b70":"code","695ef82b":"code","b43153bd":"code","c3106d93":"code","1266995e":"code","b5e119f0":"code","ae299dcb":"code","7dffa2a4":"code","4b57c0ec":"code","56d4b54d":"code","93723cdb":"code","71776947":"code","c454a33c":"code","eeee8a02":"code","a3a91e97":"code","fab75ce0":"markdown","9bf9f470":"markdown"},"source":{"8d6cef44":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport gc\nimport sys\nimport datetime\n\nimport pickle\n\nimport subprocess\nimport json\nimport datetime\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, RepeatedStratifiedKFold\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport tensorflow.experimental.numpy as tnp\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nimport tensorflow_addons as tfa\n\nimport seaborn as sns\nfrom scipy.optimize import minimize\n","ba3ab744":"# !pip install vit-keras\n!pip install ..\/input\/vit-keras\/validators-0.18.2-py3-none-any.whl\n!pip install ..\/input\/vit-keras\/vit_keras-0.1.0-py3-none-any.whl","36eadb28":"from vit_keras import vit, utils","c9f272e9":"# Constants\nIMG_SIZE = 384\nCHANNELS = 3\nBATCH_SIZE = 32\nQ = 30\nEPOCHS = 17\nREPETE_NUMBER = 1\nLR = 0.001\nWD = 0.0001\nEARRY_STOP = 5\n\nF_EPOCHS = 4\nF_BATCH_SIZE = 8\nF_LR = 0.000002\nF_WD = 0.0000002\n\n\nDATA_DIR = '..\/input\/petfinder-pawpularity-score'\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE  ","c648d7f0":"# prepare train and test\nTRAIN_FILENAMES = tf.io.gfile.glob(DATA_DIR + '\/train\/*')\nTEST_FILENAMES = tf.io.gfile.glob(DATA_DIR + '\/test\/*')\n\nTRAIN_FILENAMES.sort()\nTEST_FILENAMES.sort()\n\ntrain_df=pd.read_csv(f'{DATA_DIR}\/train.csv', usecols=['Id', 'Pawpularity'])\ntest_df=pd.read_csv(f'{DATA_DIR}\/test.csv', usecols=['Id'])\n\ntrain_df['file_path'] = TRAIN_FILENAMES\ntest_df['file_path'] = TEST_FILENAMES\n\ntrain_df['target_value'] = train_df['Pawpularity']\ntrain_df['stratify_label'] = pd.qcut(train_df['Pawpularity'], q = Q, labels = range(Q))","adc20db4":"confirm_index = []\nwith open('..\/input\/val-index\/val_index.pkl', 'rb') as web:\n    confirm_index = pickle.load(web)\ntrain_confirm = train_df.iloc[confirm_index]\ntrain_df = train_df.drop(confirm_index)","fb27426f":"# https:\/\/www.kaggle.com\/rsmits\/effnet-b2-feature-models-catboost#SET-TPU-\/-GPU\n\ndef build_augmenter(is_labelled):\n    def augment(img):\n        # Only use basic augmentations...too much augmentation hurts performance\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_saturation(img, 0.95, 1.05)\n        img = tf.image.random_brightness(img, 0.05)\n        img = tf.image.random_contrast(img, 0.95, 1.05)\n        img = tf.image.random_hue(img, 0.05)\n        \n        return img\n    \n    def augment_with_labels(img, label):\n        return augment(img), label\n    \n    return augment_with_labels if is_labelled else augment\n\ndef build_decoder(is_labelled):\n    def decode(path):\n        # Read Image\n        file_bytes = tf.io.read_file(path)\n        img = tf.image.decode_jpeg(file_bytes, channels = CHANNELS)\n        \n        # Normalize and Resize\n        img = tf.cast(img, tf.float32) \/ 255.0\n        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n        \n        return img\n    \n    def decode_with_labels(path, label):\n        return decode(path), label\n    \n    return decode_with_labels if is_labelled else decode\n\ndef create_dataset(df, batch_size = BATCH_SIZE, is_labelled = False, augment = False, repeat = False, shuffle = False):\n    decode_fn = build_decoder(is_labelled)\n    augmenter_fn = build_augmenter(is_labelled)\n    \n    # Create Dataset\n    if is_labelled:\n        dataset = tf.data.Dataset.from_tensor_slices((df['file_path'].values, df['target_value'].values))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((df['file_path'].values))\n    dataset = dataset.map(decode_fn, num_parallel_calls = AUTOTUNE)\n    dataset = dataset.map(augmenter_fn, num_parallel_calls = AUTOTUNE) if augment else dataset\n    dataset = dataset.repeat() if repeat else dataset\n    dataset = dataset.shuffle(1024, reshuffle_each_iteration = True) if shuffle else dataset\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    \n    return dataset","c60c3007":"# Set Callbacks\ndef model_checkpoint(kind, fold):\n    return ModelCheckpoint(f'{kind}{fold:03}.h5',\n              verbose = 1, \n              monitor = 'val_root_mean_squared_error', \n              mode = 'min', \n              save_weights_only=False,\n              save_best_only = True)\ndef early_stopping():\n    return EarlyStopping(\n        monitor='val_root_mean_squared_error',\n        min_delta=0.0,\n        patience=EARRY_STOP,\n    )","470b4e62":"# OOF RMSE Placeholder\nall_val_rmse = []\n\n# Stratified Training\n# kfold = StratifiedKFold(n_splits = FEATURE_FOLDS, shuffle = True, random_state = SEED)\nkfold = RepeatedStratifiedKFold(n_splits = 5, n_repeats=REPETE_NUMBER)\n\nfor fold, (train_index, val_index) in enumerate(kfold.split(train_df.index, train_df['stratify_label'])):\n    print(f'\\n===== Fold {fold}\\n')\n\n    # Pre model.fit cleanup\n    tf.keras.backend.clear_session()\n    gc.collect()\n\n    # Create Model\n    base_model = keras.models.load_model('..\/input\/nb013-v005-output\/base_model.h5')\n    base_model.trainable=False\n    \n    x = base_model.output\n\n    prediction = tf.keras.layers.Dense(128, activation=tfa.activations.rrelu)(x)\n    prediction = tf.keras.layers.Dense(1, activation=tfa.activations.rrelu)(prediction)\n    \n    model = keras.Model(inputs = base_model.input, outputs = prediction)\n\n    model.compile(\n        optimizer = tfa.optimizers.AdamW(\n            learning_rate=LR, weight_decay=WD\n        ),\n        loss = tf.keras.losses.MeanSquaredError(),\n        metrics=[tf.keras.metrics.RootMeanSquaredError()],\n    )\n\n    # Create TF Datasets\n    trn = train_df.iloc[train_index]\n    val = train_df.iloc[val_index]\n    \n    # training_dataset = create_dataset(small_trn, batch_size = BATCH_SIZE, is_labelled = True, augment = True, repeat = True, shuffle = True)\n    training_dataset = create_dataset(trn, batch_size = BATCH_SIZE, is_labelled = True, augment = True, repeat = True, shuffle = True)\n    validation_dataset = create_dataset(val, batch_size = BATCH_SIZE, is_labelled = True, augment = False, repeat = True, shuffle = False)\n\n    # Transfer Learning\n    model.fit(training_dataset,\n      epochs = EPOCHS,\n      steps_per_epoch = trn.shape[0] \/\/ BATCH_SIZE,\n      validation_steps = val.shape[0] \/\/ BATCH_SIZE,\n      callbacks = [model_checkpoint(\"t\", fold), early_stopping()],\n      validation_data = validation_dataset,\n      verbose = 1\n    )\n\n    # Fine tuning\n    training_dataset = create_dataset(trn, batch_size = F_BATCH_SIZE, is_labelled = True, augment = True, repeat = True, shuffle = True)\n    model.load_weights(f't{fold:03}.h5')\n    model.trainable = True\n    model.compile(\n        optimizer = tfa.optimizers.AdamW(\n            learning_rate=F_LR, weight_decay=F_WD\n        ),\n        loss = tf.keras.losses.MeanSquaredError(),\n        metrics=[tf.keras.metrics.RootMeanSquaredError()],\n    )\n    \n    history = model.fit(training_dataset,\n      epochs = F_EPOCHS,\n      steps_per_epoch = trn.shape[0] \/\/ F_BATCH_SIZE,\n      validation_steps = val.shape[0] \/\/ F_BATCH_SIZE,\n      callbacks = [model_checkpoint(\"f\", fold), early_stopping()],\n      validation_data = validation_dataset,\n      verbose = 1\n    )\n\n    # Validation Information\n    best_val_rmse = min(history.history['val_root_mean_squared_error'])\n    all_val_rmse.append(best_val_rmse)\n    print(f'\\nValidation RMSE: {best_val_rmse}\\n')\n\n# Summary\nprint(f'Final Mean RMSE for {REPETE_NUMBER*5} Fold CV Training: {np.mean(all_val_rmse)}')","ce4055d7":"for file_name in os.listdir(f'.\/'):\n    if 't' in file_name and '.h5' in file_name:\n        model.load_weights(f'{file_name}')\n        head_input = keras.layers.Input(model.layers[-2].input_shape[1:])\n        head_model = head_input\n\n        for layer in model.layers[-2:]:\n            head_model = layer(head_model)\n\n        head_model = keras.Model(inputs=head_input, outputs=head_model)\n        head_model.compile(\n            optimizer = tfa.optimizers.AdamW(\n                learning_rate=F_LR, weight_decay=F_WD\n            ),\n            loss = tf.keras.losses.MeanSquaredError(),\n            metrics=[tf.keras.metrics.RootMeanSquaredError()],\n        )\n\n        head_model.save(f'h{file_name}')","a3098b70":"confirm_dataset = create_dataset(train_confirm, batch_size = BATCH_SIZE, is_labelled = True, augment = False, repeat = False, shuffle = False)\npredictions = []","695ef82b":"base_model = keras.models.load_model('..\/input\/nb013-v005-output\/base_model.h5')\nbase_output = base_model.predict(confirm_dataset)","b43153bd":"head_models = []\nfor i in range(REPETE_NUMBER*5):\n    head_models.append(keras.models.load_model(f'ht{i:03}.h5'))\nfor head_model in head_models:\n    predictions.append(np.array(head_model.predict(base_output)))","c3106d93":"trained_models = []\nfor i in range(REPETE_NUMBER*5):\n    trained_models.append(keras.models.load_model(f'f{i:03}.h5'))\nfor trained_model in trained_models:\n    predictions.append(np.array(trained_model.predict(confirm_dataset)))","1266995e":"for i, prediction in enumerate(predictions):\n    print(i, prediction.mean(), prediction.std(), np.sqrt(mean_squared_error(train_confirm['Pawpularity'], prediction)))","b5e119f0":"# For measuring similarity\nscore_table = []\nfor prediction1 in predictions:\n    temp_line = []\n    for prediction2 in predictions:\n        temp_line.append(np.sqrt(mean_squared_error(prediction1, prediction2)))\n    score_table.append(temp_line)\nsns.heatmap(score_table)","ae299dcb":"ratio_bet_htf = [0.4, 0.6]\nbest_ratios = []\nfor one_ratio in ratio_bet_htf:\n    for i in range(REPETE_NUMBER*5):\n        best_ratios.append(1.0 * one_ratio \/ (REPETE_NUMBER * 5))","7dffa2a4":"best_ratios","4b57c0ec":"y_pred = np.zeros((train_confirm.shape[0], 1))\nfor i, ratio in enumerate(best_ratios):\n    y_pred += (predictions[i] * ratio)\nprint(y_pred.mean(), y_pred.std(), np.sqrt(mean_squared_error(train_confirm['Pawpularity'], y_pred)))","56d4b54d":"sub_dataset = create_dataset(test_df, batch_size = BATCH_SIZE, is_labelled = False, augment = False, repeat = False, shuffle = False)\npredictions = []","93723cdb":"base_output = base_model.predict(sub_dataset)\nfor head_model in head_models:\n    predictions.append(np.array(head_model.predict(base_output)))","71776947":"for trained_model in trained_models:\n    predictions.append(np.array(trained_model.predict(sub_dataset)))","c454a33c":"y_pred = np.zeros((test_df.shape[0], 1))\n\nfor i, ratio in enumerate(best_ratios):\n    y_pred += (predictions[i] * ratio)\nprint(y_pred.mean(), y_pred.std())","eeee8a02":"test_df['Pawpularity'] = y_pred\ntest_df = test_df[[\"Id\", \"Pawpularity\"]]\ntest_df.to_csv(\"submission.csv\", index=False)","a3a91e97":"test_df","fab75ce0":"I started to learn about machine learning few months ago.\nThus, this notebook may have some wrong points.\nIf you find these or some advices, please comment to tell me; it will help me to learn data science.\n\n---\n\nI used Vision Transformer at keras.\n\nThis is article.\n[An Image is Worth 16x16 Words: Transformes For Image Recognition at Scale. ](https:\/\/arxiv.org\/pdf\/2010.11929.pdf)\n\nI used [vit-keras](https:\/\/github.com\/faustomorales\/vit-keras) library to use that.\n\n<br \/>\n\nFor submit, I can't connect internet, so I save vit_b16 and use from dataset.\n```\nbase_model = vit.vit_b16(\n    image_size=IMG_SIZE,\n    activation='sigmoid',\n    pretrained=True,\n    include_top=False,\n    pretrained_top=False,\n)\nbase_model.save_model('base_model.h5')\n```\n\nThis predictions are made by ensemble of 3 Transfer Learning predictions and 3 Fine tuning predictions.","9bf9f470":"Fine tuning may get more good grade. Thus, I set high ratio of ensemble.\n\nTransfer learning ones (ht) 4 : Fine tuning ones (f) 6"}}