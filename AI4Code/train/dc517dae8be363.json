{"cell_type":{"0c959891":"code","d029f5da":"code","d204de88":"code","056b2100":"code","9124551b":"code","b7e6e9a6":"code","661a0f31":"code","40ee4e52":"code","53dfd8b3":"code","d3d39607":"code","8a68813b":"code","6c25edeb":"code","e9ea26db":"code","ba49cb98":"code","5729068e":"code","eb3151ef":"code","4a879e93":"code","479c8bbc":"code","cc8a0be2":"code","9786087b":"code","f50d6a01":"code","9bd36522":"code","a16b4bf2":"code","f178f0bf":"code","31606bc9":"code","79f6de1b":"code","cba94920":"code","2639ec4d":"code","9576551e":"code","4470604c":"code","c922305b":"code","2d219ea5":"code","69264d2a":"code","05b3349d":"code","f5abcbbe":"code","b3a73975":"code","74da38ec":"code","9eef3cc8":"code","7dd1f030":"code","30f3b940":"markdown","2c06fe03":"markdown","4b9b6d65":"markdown","d3ff4f1b":"markdown","0b603bea":"markdown","6a15d321":"markdown","a5556a26":"markdown","d3eee7e1":"markdown","4eb7307e":"markdown","87c2bbe0":"markdown","cc06b19d":"markdown","48a0df41":"markdown","cc91d62e":"markdown","31d32c3f":"markdown","0e7bb1f5":"markdown","7f94c32b":"markdown","e721613d":"markdown","e35312c8":"markdown","7ad13b52":"markdown","4849c28d":"markdown","490a43e5":"markdown","295b33a7":"markdown","cbf1ffc7":"markdown","75d482a8":"markdown","78be05db":"markdown","a87f80ba":"markdown","d23d54a4":"markdown","4f8268c0":"markdown","e48e37d1":"markdown","6a1e64fc":"markdown"},"source":{"0c959891":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision import datasets, models, transforms\n\nimport PIL\nfrom PIL import Image\n\nimport math\nimport random\nimport seaborn as sn\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom skimage import io\nimport pickle\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\nfrom tqdm import tqdm_notebook\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"PyTorch Version: \", torch.__version__)\nprint(\"Torchvision Version: \", torchvision.__version__)\nprint(\"Pillow Version: \", PIL.PILLOW_VERSION)\n\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","d029f5da":"# input\/... \u043f\u0443\u0442\u044c \u043a \u0434\u0430\u043d\u043d\u044b\u043c \u0442\u0430\u043a\u043e\u0439 \u0436\u0435 \u043a\u0430\u043a \u0432 \u0441\u0441\u044b\u043b\u043a\u0435 \u043e\u043f\u0443\u0431\u043b\u0438\u043a\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 kaggle.com\/alexattia\/the-simpsons-characters-dataset\ntrain_dir = Path('\/kaggle\/input\/the-simpsons-characters-dataset\/simpsons_dataset\/')\ntest_dir = Path('\/kaggle\/input\/the-simpsons-characters-dataset\/kaggle_simpson_testset\/')","d204de88":"class SimpsonTrainValPath():\n\n  def __init__(self, train_dir, test_dir):\n    \n    self.train_dir = train_dir\n    self.test_dir = test_dir\n    self.train_val_files_path = sorted(list(self.train_dir.rglob('*.jpg')))\n    self.test_path = sorted(list(self.test_dir.rglob('*.jpg')))\n    self.train_val_labels = [path.parent.name for path in self.train_val_files_path]\n\n  def get_path(self):\n      \n    train_files_path, val_files_path = train_test_split(self.train_val_files_path, test_size = 0.3, \\\n                                          stratify=self.train_val_labels)\n    \n    files_path = {'train': train_files_path, 'val': val_files_path}\n    \n    return files_path, self.test_path\n  \n  def get_n_classes(self):\n    return len(np.unique(self.train_val_labels))","056b2100":"SimpsonTrainValPath = SimpsonTrainValPath(train_dir, test_dir)\ntrain_path, test_path = SimpsonTrainValPath.get_path()","9124551b":"def train_model(model, dataloaders, criterion, optimizer, save_best_weights_path, save_last_weights_path, best_acc, num_epochs=25, is_inception=False):\n    since = time.time()\n\n    val_acc_history = []\n    val_loss_history = []\n    train_acc_history = []\n    train_loss_history = []\n    lr_find_lr = []\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043c \u0440\u0435\u0436\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u0438\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # \u0420\u0435\u0436\u0438\u043c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438\n            else:\n                model.eval()   # \u0420\u0435\u0436\u0438\u043c \u043e\u0446\u0435\u043d\u043a\u0438\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            for inputs, labels in tqdm_notebook(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # \u043e\u0431\u043d\u043e\u0432\u0438\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430\n                optimizer.zero_grad()\n\n                # forward\n                # \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0438\u0441\u0442\u043e\u0440\u0438\u044e, \u0435\u0441\u043b\u0438 \u0440\u0435\u0436\u0438\u043c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438\n                with torch.set_grad_enabled(phase == 'train'):\n                    # \u0440\u0430\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c loss\n                    # \u0441\u043b\u0443\u0447\u0430\u0439 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 inception\n                    if is_inception and phase == 'train':\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimizer + scheduler \u0435\u0441\u043b\u0438 \u0440\u0435\u0436\u0438\u043c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        scheduler.step()\n                        lr_step = optimizer_ft.state_dict()[\"param_groups\"][0][\"lr\"]\n                        lr_find_lr.append(lr_step)\n\n                # \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            \n            # loss, acc \u044d\u043f\u043e\u0445\u0438\n            epoch_loss = running_loss \/ len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() \/ len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043b\u0443\u0447\u0448\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            # \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c acc \n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n                val_loss_history.append(epoch_loss)\n            else:\n                train_acc_history.append(epoch_acc)\n                train_loss_history.append(epoch_loss)\n        \n        print()\n    # \u0440\u0430\u0441\u043f\u0435\u0447\u0430\u0442\u0430\u0435\u043c \u0432\u0440\u0435\u043c\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u043b\u0443\u0447\u0448\u0438\u043c\u0438 \u0432\u0435\u0441\u0430\u043c\u0438\n    model.load_state_dict(best_model_wts)\n\n    history_val = {'loss': val_loss_history, 'acc': val_acc_history}\n    history_train = {'loss': train_loss_history, 'acc': train_acc_history}\n    \n    return model, history_val, history_train, time_elapsed, lr_find_lr, best_acc","b7e6e9a6":"# feature_extracting = False - \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0443 \u0432\u0441\u0435\u0439 \u0441\u0435\u0442\u0438\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","661a0f31":"def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n    \n    model_ft = None\n    input_size = 0\n       \n    if model_name == \"resnet152\":\n        \"\"\" Resnet152\n        \"\"\"\n        model_ft = models.resnet152(pretrained=use_pretrained)\n        # \u043e\u0442\u043a\u043b\u044e\u0447\u0430\u0435\u0442 \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043e\u0432 \u0443 \u0437\u0430\u043c\u043e\u0440\u043e\u0436\u0435\u043d\u043d\u044b\u0445 \u0441\u043b\u043e\u0435\u0432\n        set_parameter_requires_grad(model_ft, feature_extract)\n        # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043a\u043e\u043b-\u0432\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 \u0432\u0445\u043e\u0434\u044f\u0449\u0438\u0445 \u0432 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0441\u043b\u043e\u0439\n        num_ftrs = model_ft.fc.in_features\n        # \u043d\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0441\u043b\u043e\u0439, \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0439 \u0432\u044b\u0445\u043e\u0434\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n    \n    elif model_name == 'resnext-101-32x8d':\n        \"\"\" ResNeXt-101-32x8d\n        \"\"\"\n        model_ft = models.resnext101_32x8d(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"vgg\":\n        \"\"\" VGG11_bn\n        \"\"\"\n        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"squeezenet\":\n        \"\"\" Squeezenet\n        \"\"\"\n        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n        model_ft.num_classes = num_classes\n        input_size = 224\n\n    elif model_name == \"densenet\":\n        \"\"\" Densenet\n        \"\"\"\n        model_ft = models.densenet161(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n\n    return model_ft, input_size","40ee4e52":"class SimpsonsDataset(Dataset):\n\n    def __init__(self, files_path, data_transforms):\n      self.files_path = files_path\n      self.transform = data_transforms\n      \n      if 'test' not in str(self.files_path[0]):\n        self.labels = [path.parent.name for path in self.files_path]\n        self.label_encoder = LabelEncoder()\n        self.label_encoder.fit(self.labels)\n        \n        with open('label_encoder.pkl', 'wb') as le_dump_file:\n            pickle.dump(self.label_encoder, le_dump_file)\n\n    def __len__(self):\n      return len(self.files_path)\n\n    def __getitem__(self, idx):\n\n      img_path = str(self.files_path[idx]) \n      image = Image.open(img_path)\n      image = self.transform(image)\n      \n      if 'test' in str(self.files_path[0]):\n        return image\n      else: \n        label_str = str(self.files_path[idx].parent.name)\n        label = self.label_encoder.transform([label_str]).item()\n        \n        return image, label","53dfd8b3":"# \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0438\u0437 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u0445 \u0432 initialize_model\nmodel_name = 'resnet152'\n# \u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438\nfc_layer = 'all-st-SGD-m.9-nest-s-cycle-exp-.00001-.05-g.99994-m.8-.9'\n\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435\nnum_classes = SimpsonTrainValPath.get_n_classes()\n\n# \u0420\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430\nbatch_size = 32\n\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445\nnum_epochs = 2\n\n# \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# feature_extract = False - \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0432\u0441\u044e \u043c\u043e\u0434\u0435\u043b\u044c\n# feature_extract = True - \u043e\u0431\u0443\u0447\u0430\u0435\u043c FC\nfeature_extract = False\n\n# \u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0432\u0435\u0441\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438\nsave_last_weights_path = '\/kaggle\/working\/' + model_name + '-' + fc_layer + '_last_weights.pth'\nsave_best_weights_path = '\/kaggle\/working\/' + model_name + '-' + fc_layer + '_best_weights.pth'","d3d39607":"model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n# \u043e\u0442\u043f\u0440\u0430\u0432\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 GPU\nmodel_ft = model_ft.to(device)","8a68813b":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.CenterCrop(input_size),\n        transforms.RandomChoice( [ \n                                  transforms.RandomHorizontalFlip(p=0.5),\n                                  transforms.ColorJitter(contrast=0.9),\n                                  transforms.ColorJitter(brightness=0.1),\n                                  transforms.RandomApply( [ transforms.RandomHorizontalFlip(p=1), transforms.ColorJitter(contrast=0.9) ], p=0.5),\n                                  transforms.RandomApply( [ transforms.RandomHorizontalFlip(p=1), transforms.ColorJitter(brightness=0.1) ], p=0.5),\n                                  ] ),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.CenterCrop(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}","6c25edeb":"image_datasets = {mode: SimpsonsDataset(train_path[mode], data_transforms[mode]) for mode in ['train', 'val']}\nimage_datasets_test = SimpsonsDataset(test_path, data_transforms['val'])","e9ea26db":"dataloaders_dict = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=4),\n                    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True, num_workers=4)}\ndataloader_test = torch.utils.data.DataLoader(image_datasets_test, batch_size=batch_size, shuffle=False, num_workers=4)","ba49cb98":"def imshow(inp, title=None, plt_ax=plt, default=False):\n    \"\"\"Imshow \u0434\u043b\u044f \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt_ax.imshow(inp)\n    if title is not None:\n        plt_ax.set_title(title)\n    plt_ax.grid(False)","5729068e":"fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(8, 8), \\\n                        sharey=True, sharex=True)\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0, 4500))\n    im_val, label = image_datasets['train'][random_characters]\n    # inverse_transform \u044d\u0442\u043e \u043c\u0435\u0442\u043e\u0434 LabelEncoder(), \u043c\u044b \u0437\u0430\u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u0446\u0438\u0444\u0440\u0430\u043c\u0438 \u043a\u043b\u0430\u0441\u0441\u044b, \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e inverse_transform \u0438\u0437 \u0447\u0438\u0441\u0435\u043b \u0432\u0435\u0440\u043d\u0451\u043c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u0430\n    # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0438\u043c\u044f \u043f\u0435\u0440\u0441\u043e\u043d\u0430\u0436\u0430 \u0441 \u0437\u0430\u0433\u043b\u0430\u0432\u043d\u044b\u0445 \u0431\u0443\u043a\u0432\n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                image_datasets['val'].label_encoder.inverse_transform([label])[0].split('_')))\n    imshow(im_val.data.cpu(), \\\n          title=img_label,plt_ax=fig_x)","eb3151ef":"def visualization(train, val, is_loss = True):\n  \n  if is_loss:\n    plt.figure(figsize=(17,10))\n    plt.plot(train, label = 'Training loss')\n    plt.plot(val, label = 'Val loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n  \n  else:\n    plt.figure(figsize=(17,10))\n    plt.plot(train, label = 'Training acc')\n    plt.plot(val, label = 'Val acc')\n    plt.title('Training and validation acc')\n    plt.xlabel('Epochs')\n    plt.ylabel('Acc')\n    plt.legend()\n    plt.show()","4a879e93":"params_to_update = model_ft.parameters()\nprint(\"Params to learn:\")\nif feature_extract:\n    params_to_update = []\n    for name, param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            #print(\"\\t\",name)\nelse:\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            pass\n            #print(\"\\t\",name)","479c8bbc":"base_lr = 0.00001\nmax_lr = 0.05\nlr_find_epochs = 2\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9, nesterov = True)\n\nstep_size = lr_find_epochs * len(dataloaders_dict['train'])\n\nscheduler = optim.lr_scheduler.CyclicLR(optimizer_ft, base_lr = base_lr, max_lr = max_lr, step_size_up=step_size, mode='exp_range', gamma=0.99994, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9, last_epoch=-1)","cc8a0be2":"def search_lr(lr_find_epochs):\n  \n  accs = []\n  lr_find_lr = []\n  acc_sum = 0.0\n\n  for i in range(lr_find_epochs):\n    print(\"epoch {}\".format(i))\n    for inputs, labels in tqdm_notebook(dataloaders_dict['train']):\n      \n      inputs = inputs.to(device)\n      labels = labels.to(device)\n      \n      model_ft.train()\n      optimizer_ft.zero_grad()\n      \n      outputs = model_ft(inputs)\n      loss = criterion(outputs, labels)\n      preds = torch.argmax(outputs, 1)\n      acc_running = torch.sum(preds == labels.data).item()\n      acc_sum += torch.sum(preds == labels.data).item()\n\n      loss.backward()\n      optimizer_ft.step()\n      scheduler.step()\n      \n      lr_step = optimizer_ft.state_dict()[\"param_groups\"][0][\"lr\"]\n      lr_find_lr.append(lr_step)\n      \n      accs.append(acc_running)\n  accs = np.array(accs) \/ acc_sum\n  \n  return lr_find_lr, accs","9786087b":"lr_find_lr, accs = search_lr(lr_find_epochs)","f50d6a01":"plt.figure(figsize=(20,10))\nplt.plot(np.array(lr_find_lr), np.array(accs));","9bd36522":"model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\nmodel_ft = model_ft.to(device)","a16b4bf2":"params_to_update = model_ft.parameters()\nprint(\"Params to learn:\")\nif feature_extract:\n    params_to_update = []\n    for name, param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            #print(\"\\t\",name)\nelse:\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            pass\n            #print(\"\\t\",name)","f178f0bf":"base_lr = 0.0012\nmax_lr = 0.0022\nnum_epoch = 4\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9, nesterov = True)\n# \u0446\u0438\u043a\u043b lr 4 \u044d\u043f\u043e\u0445\u0438\nstep_size = 2 * math.ceil( len(dataloaders_dict['train']) \/ batch_size )\nscheduler = optim.lr_scheduler.CyclicLR(optimizer_ft, base_lr = base_lr, max_lr = max_lr, step_size_up=step_size, mode='exp_range', gamma=0.994, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9, last_epoch=-1)","31606bc9":"val_loss = []\nval_acc = []\ntrain_loss = []\ntrain_acc = []\nlr_cycle = []\nbest_acc = .0","79f6de1b":"for i in range(num_epoch):\n  \n    # \u043a\u0430\u0436\u0434\u044b\u0439 \u0440\u0430\u0437 \u0431\u0443\u0434\u0435\u043c \u043f\u043e\u0434\u0430\u0432\u0430\u0442\u044c \u0441\u0435\u0442\u0438 \u043d\u043e\u0432\u044b\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\n    image_datasets = {mode: SimpsonsDataset(train_path[mode], data_transforms[mode]) for mode in ['train', 'val']}\n\n    dataloaders_dict = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=False, num_workers=4),\n                          'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=False, num_workers=4)}\n\n    model, history_val, history_train, time_elapsed, lr_find_lr, best_acc = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, save_best_weights_path, save_last_weights_path, best_acc = best_acc, num_epochs=1, is_inception=(model_name==\"inception\"))\n\n    val_loss += history_val['loss']\n    val_acc += history_val['acc']\n    train_loss += history_train['loss']\n    train_acc += history_train['acc']\n    lr_cycle += lr_find_lr","cba94920":"plt.figure(figsize=(17,10))\nplt.plot(lr_cycle);","2639ec4d":"visualization(train_acc, val_acc, is_loss = False)","9576551e":"visualization(train_loss, val_loss, is_loss = True)","4470604c":"def predict(model, test_loader):\n    with torch.no_grad():\n        logits = []\n    \n        for inputs in test_loader:\n            inputs = inputs.to(device)\n            model.eval()\n            outputs = model(inputs).cpu()\n            logits.append(outputs)\n            \n    probs = nn.functional.softmax(torch.cat(logits), dim=1).numpy()\n    return probs","c922305b":"def predict_one_sample(model, img_tensor, device=device):\n    with torch.no_grad():\n        # \u043d\u0430\u0434\u043e \u043b\u0438 \u0432 \u043d\u0430\u0448\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0435?\n        img_tensor = img_tensor.to(device)\n        model.eval()\n        y_hat = model(img_tensor).cpu()\n        y_pred = torch.nn.functional.softmax(y_hat, dim=1).numpy()\n    return y_pred","2d219ea5":"def confusion_matrix():\n    # \u0432\u044b\u0442\u0430\u0449\u0438\u043c \u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0438\u0435 \u043b\u0435\u0439\u0431\u043b\u044b!\n    actual = [image_datasets['val'][i][1] for i in range( len(image_datasets['val']) ) ]\n    \n    # \u0432\u044b\u0442\u0430\u0449\u0438\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438, \u0447\u0442\u043e\u0431\u044b \u0438\u0445 \u043f\u043e\u0434\u0430\u0442\u044c \u0432 \u0434\u0430\u0442\u0430\u043b\u043e\u0430\u0434\u0435\u0440, \u0430 \u043f\u043e\u0442\u043e\u043c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0443\n    image = [image_datasets['val'][i][0] for i in range( len(image_datasets['val']) ) ]\n    \n    # \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u043c \u0434\u0430\u0442\u0430\u043b\u043e\u0430\u0434\u0435\u0440\n    img_conf_dataloader = torch.utils.data.DataLoader(image, batch_size=batch_size, shuffle=False, num_workers=4)\n    \n    # \u0432\u044b\u0442\u0430\u0449\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043b\u0435\u0439\u0431\u043b\u044b\n    probs = predict(model_ft, img_conf_dataloader)\n    preds = np.argmax(probs, axis=1)\n    \n    # \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0442\u0430\u0431\u043b\u0438\u0446\u0443, \u0433\u0434\u0435 \u0432 \u043e\u0434\u043d\u043e\u043c \u0441\u0442\u043e\u043b\u0431\u0446\u0435 \u0431\u0443\u0434\u0443\u0442 \u043d\u0441\u0442\u043e\u044f\u0449\u0438\u0435 \u043c\u0435\u0442\u043a\u0438, \u0432\u043e \u0432\u0442\u043e\u0440\u043e\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435\n    df = pd.DataFrame({'actual': actual, 'preds': preds})\n    \n    # margins = False - \u0447\u0442\u043e\u0431\u044b \u043d\u0435 \u0431\u044b\u043b\u043e \u0441\u0442\u0440\u043e\u043a\u0438 all\n    confusion_matrix = pd.crosstab(df['actual'], df['preds'], rownames=['Actual'], colnames=['Predicted'], margins = False)\n    \n    # \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u043d\u0430\u0448 \u0438\u043d\u043a\u043e\u0434\u0435\u0440\n    label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n    \n    # \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u043a\u043b\u0430\u0441\u0441\u043e\u0432\n    # label_encoder.classes_[i] - \u043c\u043e\u0436\u043d\u043e \u0443\u0431\u0435\u0434\u0438\u0442\u044c\u0441\u044f, \u043a\u0430\u043a\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443, \u043a\u0430\u043a\u043e\u0435 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u043a\u0430\u043a\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442\n    yticklabels = label_encoder.classes_\n    \n    plt.subplots(figsize=(20,20))\n\n    sn.heatmap(confusion_matrix, annot=True, fmt=\"d\", linewidths=0.5, cmap=\"YlGnBu\", cbar=False, vmax = 30, yticklabels = yticklabels, xticklabels = yticklabels);","69264d2a":"confusion_matrix()","05b3349d":"import matplotlib.patches as patches\nfrom matplotlib.font_manager import FontProperties\n\nfig, ax = plt.subplots(nrows=3, ncols=3, figsize=(12, 12), \\\n                        sharey=True, sharex=True)\n\nlabel_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0, 1000))\n    im_val, label = image_datasets['val'][random_characters]\n    # inverse_transform \u044d\u0442\u043e \u043c\u0435\u0442\u043e\u0434 LabelEncoder(), \u043c\u044b \u0437\u0430\u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u0446\u0438\u0444\u0440\u0430\u043c\u0438 \u043a\u043b\u0430\u0441\u0441\u044b, \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e inverse_transform \u0438\u0437 \u0447\u0438\u0441\u0435\u043b \u0432\u0435\u0440\u043d\u0451\u043c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u0430\n    # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0438\u043c\u044f \u043f\u0435\u0440\u0441\u043e\u043d\u0430\u0436\u0430 \u0441 \u0437\u0430\u0433\u043b\u0430\u0432\u043d\u044b\u0445 \u0431\u0443\u043a\u0432\n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                image_datasets['val'].label_encoder.inverse_transform([label])[0].split('_')))\n    \n    imshow(im_val.data.cpu(), \\\n          title=img_label, plt_ax=fig_x)\n    \n    actual_text = \"Actual : {}\".format(img_label)\n\n    # \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u043e\u0431\u043b\u0430\u0441\u0442\u044c \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0431\u0443\u0434\u0435\u043c \u0432\u044b\u0432\u043e\u0434\u0438\u0442\u044c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c        \n    fig_x.add_patch(patches.Rectangle((0, 53), 86, 35, color='white'))\n    font0 = FontProperties()\n    font = font0.copy()\n    font.set_family(\"fantasy\")\n    prob_pred = predict_one_sample(model_ft, im_val.unsqueeze(0))\n    # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c\n    predicted_proba = np.max(prob_pred)*100\n    y_pred = np.argmax(prob_pred)\n    \n    predicted_label = label_encoder.classes_[y_pred]\n    predicted_label = predicted_label[:len(predicted_label)\/\/2] + '\\n' + predicted_label[len(predicted_label)\/\/2:]\n    predicted_text = \"{} : {:.0f}%\".format(predicted_label,predicted_proba)\n            \n    fig_x.text(1, 59, predicted_text , horizontalalignment='left', fontproperties=font,\n                    verticalalignment='top',fontsize=8, color='black',fontweight='bold')","f5abcbbe":"probs = predict(model_ft, dataloader_test)","b3a73975":"# \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u043d\u0430\u0448 \u0438\u043d\u043a\u043e\u0434\u0435\u0440\nlabel_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))","74da38ec":"preds = label_encoder.inverse_transform(np.argmax(probs, axis = 1 ))\ntest_filenames = [path.name for path in image_datasets_test.files_path]","9eef3cc8":"my_submit = pd.DataFrame({'Id': test_filenames, 'Expected': preds})\nmy_submit.head()","7dd1f030":"my_submit.to_csv('simspsons.csv', index=False)","30f3b940":"# \u0424\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0443\u0442\u0438 \u043a \u0434\u0430\u043d\u043d\u044b\u043c","2c06fe03":"### \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f","4b9b6d65":"### \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438","d3ff4f1b":"\u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 \u0434\u0432\u0443\u0445 \u044d\u0442\u0430\u043f\u043e\u0432:\n- \u041d\u0430\u0439\u0442\u0438 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d \u0434\u043b\u044f lr, \u0447\u0442\u043e\u0431\u044b \u0438\u0437\u043c\u0435\u043d\u044f\u0442\u044c \u0435\u0433\u043e \u0446\u0438\u043a\u043b\u0438\u0447\u0435\u0441\u043a\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f lr_scheduler.CyclicLR \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 Pytorch\n- \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043d\u0430\u0439\u0434\u0435\u043d\u043d\u044b\u0439 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d","0b603bea":"### \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0433\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f","6a15d321":"# Submit \u043d\u0430 Kaggle","a5556a26":"# \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","d3eee7e1":"#### \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043f\u043e\u0438\u0441\u043a\u0430 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430 lr","4eb7307e":"### \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435","87c2bbe0":"### \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438","cc06b19d":"# \u0414\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043f\u043e\u0434\u0430\u0447\u0438 \u0432 \u0441\u0435\u0442\u044c","48a0df41":"# \u041e\u0442\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430","cc91d62e":"### \u0420\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train, val \u0438 test","31d32c3f":"\u041f\u0440\u043e\u0435\u043a\u0442 \u0432 \u0440\u0430\u043c\u043a\u0430\u0445 \u043a\u0443\u0440\u0441\u0430 DLS \u043e\u0442 \u041c\u0424\u0422\u0418: kaggle.com\/c\/simpsons4","0e7bb1f5":"# \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438","7f94c32b":"\u0412\u043e\u0437\u044c\u043c\u0435\u043c \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d lr, \u0433\u0434\u0435 accuracy \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430\u0435\u0442.","e721613d":"# \u041c\u043e\u0434\u0443\u043b\u044c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438","e35312c8":"#### \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430 Resnet152","7ad13b52":"### \u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445","4849c28d":"### \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438\n\n\u0421\u043e\u0437\u0434\u0430\u0442\u044c \u0432 Google Drive \u043f\u0430\u043f\u043a\u0443 \u0441 **\u0442\u0430\u043a\u0438\u043c \u0436\u0435 (model_name)** \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438, \u0434\u043b\u044f \u0435\u0451 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f.","490a43e5":"# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f","295b33a7":"# \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445","cbf1ffc7":"### \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b","75d482a8":"# \u041c\u043e\u0434\u0435\u043b\u0438","78be05db":"# \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c confusion matrix","a87f80ba":"### Dataloader","d23d54a4":"### Dataset","4f8268c0":"# \u0418\u043c\u043f\u043e\u0440\u0442 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a","e48e37d1":"# \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430","6a1e64fc":"#### \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e learning rate (lr)\n\n\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044e lr \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0446\u0438\u043a\u043b\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0435\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0432 \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u043e\u043c \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0435. \u0412\u043d\u0430\u0447\u0430\u043b\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0435 \u0433\u0440\u0430\u043d\u0438\u0446\u044b \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430. \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043c \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0439 \u0437\u0430\u043f\u0443\u0441\u043a. \u0412\u044b\u0431\u0435\u0440\u0435\u043c step size \u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0447\u0442\u043e\u0431\u044b \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u0437\u0430\u043f\u0443\u0441\u043a\u0430 lr \u043b\u0438\u043d\u0435\u0439\u043d\u043e \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430\u043b. <br>\n\n\u041f\u043e\u0441\u043b\u0435 \u0447\u0435\u0433\u043e \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a accuracy \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e learning rate, \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0438 \u0433\u0440\u0430\u0444\u0438\u043a\u0430 \u0432\u044b\u0431\u0435\u0440\u0435\u043c \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439.\n\n\u041c\u0435\u0442\u043e\u0434: https:\/\/arxiv.org\/pdf\/1506.01186.pdf"}}