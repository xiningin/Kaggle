{"cell_type":{"5fbd166e":"code","1494f1e0":"code","84cbf54a":"code","e865545a":"code","afac8197":"code","057c19bf":"code","5e33dae8":"code","62761797":"code","ae61bcc4":"code","74c45348":"code","83a084cc":"code","a956fbf5":"code","96d89739":"code","ec5a0a0d":"code","505f7864":"code","942bf8e6":"code","d0ed5181":"code","225fc7b4":"code","204677f6":"code","5f0c6a8f":"code","53e4c1d6":"code","89d9c521":"code","2bce38a2":"code","5d405595":"code","ba5655d2":"code","a15a3050":"code","f45a5329":"code","d269fde3":"code","ba99ac65":"code","86d2d26a":"code","09c0cb66":"code","8c6185f2":"code","ce3fb987":"code","cad87626":"code","905901fb":"code","d06c2cf6":"code","c9e48138":"code","8604b694":"code","5db728d0":"code","2e194e13":"code","94d2c0f4":"code","66d9e3c1":"code","21d9dd96":"code","1cb18bb4":"code","d4b59911":"code","fe50aad0":"code","7c007a68":"code","adc98136":"code","df8950df":"code","6647d61a":"code","eed75fd4":"code","c264e029":"code","c4cb30e0":"code","1c81ad96":"markdown","1047a3bc":"markdown","bf8d16cc":"markdown","efc7036f":"markdown","514c89e9":"markdown","82c1d8c8":"markdown","a5810dc5":"markdown","aeec0315":"markdown","35931734":"markdown","6635e0b0":"markdown","e6f711b3":"markdown","88a40f03":"markdown","5914686c":"markdown","32a591ad":"markdown","727b3e01":"markdown","6c4bdf41":"markdown","42fedc05":"markdown","e3bac872":"markdown","a2411358":"markdown","55306645":"markdown","89b9803d":"markdown","50d4c9cf":"markdown","aae2a205":"markdown","bc8e2ef0":"markdown","aa2a5d15":"markdown","216462e5":"markdown","3625b29f":"markdown","b14439fb":"markdown","a6fa0a1d":"markdown","afa50dee":"markdown","7d16589e":"markdown","283d81f7":"markdown","e196d9fb":"markdown","45e33d9d":"markdown","9f006499":"markdown","94fb9576":"markdown","b076f696":"markdown","4407de3d":"markdown","0f95aa52":"markdown","c2b622ad":"markdown","cd7a0505":"markdown","ce683517":"markdown","10bf2936":"markdown","901fd0fe":"markdown","938a128a":"markdown","71b02e46":"markdown","21605259":"markdown","71dfdd66":"markdown","4a5cef5b":"markdown","3574ddf0":"markdown","ffbb9a49":"markdown","ec4a20c9":"markdown","15147f5f":"markdown","5697d20b":"markdown","6ab646a3":"markdown"},"source":{"5fbd166e":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as st\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import PowerTransformer","1494f1e0":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","84cbf54a":"df = pd.read_csv('\/kaggle\/input\/Rice-Gonen andJasmine.csv')\nprint('Number of instances = %d' % (df.shape[0]))\nprint('Number of attributes = %d' % (df.shape[1]))\n\ndf.drop('id',axis = 1,inplace = True)\ndf.sample(5)","e865545a":"df.dtypes","afac8197":"df.describe(include='all')","057c19bf":"print('Number of missing values:')\ndf.isnull().sum()","5e33dae8":"attributes =['Area','MajorAxisLength',\t'MinorAxisLength',\t'Eccentricity',\t'ConvexArea',\t'EquivDiameter',\t'Extent',\t'Perimeter',\t'Roundness',\t'AspectRation']\n\nplt.figure(figsize = (19, 19))\nfor i,j in zip(attributes,range(10)):\n  plt.subplot(6, 3, j+1)\n  sns.boxplot(df[i], palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\nplt.suptitle('Outliers Variable Distribution')\nplt.tight_layout()\nplt.subplots_adjust(top=0.95)\nplt.show()","62761797":"outliers=['MajorAxisLength',\t'Eccentricity',\t'EquivDiameter',\t'Perimeter',\t'Roundness']\nfor i in outliers: \n  Q1 = df[i].quantile(0.25)\n  Q3 = df[i].quantile(0.75)\n  IQR = Q3 - Q1\n  df = df[(df[i] >= Q1 - 1.5*IQR) & (df[i] <= Q3 + 1.5*IQR)]\n  \nattributes = df.columns\nplt.figure(figsize = (19, 19))\nfor i,j in zip(attributes,range(10)):\n  plt.subplot(6, 3, j+1)\n  sns.boxplot(df[i], palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\nplt.suptitle('Outliers Variable Distribution')\nplt.tight_layout()\nplt.subplots_adjust(top=0.95)\nplt.show()","ae61bcc4":"print('Number of duplicate rows: %d' % (df.duplicated().sum()))","74c45348":"print(df.Class.value_counts())\nplt.figure(figsize=(5,5))\nsns.countplot(df['Class'],label=\"Count\")\nplt.show()","83a084cc":"le = LabelEncoder()\nle.fit(df['Class'])\ndf['Class'] = le.transform(df['Class'])\n\nX=df.iloc[:,0:10]\ny=df.iloc[:,10:]","a956fbf5":"for col in X.columns:\n  print(col)\n  ls_score = []\n  std_score = []\n  for depth in [1,2,3,4,5]:\n    tree_model = DecisionTreeClassifier(max_depth=depth)    \n    scores = cross_val_score(tree_model, X[col].to_frame(),y, cv=3, scoring='roc_auc')   \n    ls_score.append(np.mean(scores))\n    std_score.append(np.std(scores))\n  temp = pd.concat([pd.Series([1,2,3,4,5]), pd.Series(ls_score), pd.Series(std_score)], axis=1)\n  temp.columns = ['depth', 'roc_auc_mean', 'roc_auc_std']\n  print(temp)","96d89739":"discrete = X.copy()\ndepth=[4,3,4,2,4,4,4,4,4,2]\nfor col,dep in zip(X.columns,depth):\n  tree = DecisionTreeClassifier(max_depth=dep)\n  tree.fit(X[col].to_frame(), y)\n  discrete[col+'_tree']=tree.predict_proba(X[col].to_frame())[:,1]\ndiscrete.head()","ec5a0a0d":"for col in X.columns:\n  print('FOR '+col+' BINNING IS ')\n  a=pd.concat([discrete.groupby([col+'_tree'])[col].min(),discrete.groupby([col+\"_tree\"])[col].max()], axis=1)\n  print(a)","505f7864":"discrete=discrete.iloc[:,10:]\ndiscrete[\"Class\"]=y\ndiscrete.head()","942bf8e6":"def cramers(data1, data2):\n    confusion_matrix = pd.crosstab(data1,data2)\n    chi2 = st.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\nfor col in discrete.columns:\n  if col!='Class':\n    association=cramers(discrete[col],discrete.Class)\n    if association >= 0.9 or association <= -0.9:\n      print(\"Correlation between Class and \"+col+\" is: \"+str(association))","d0ed5181":"plt.figure(figsize = (12, 12))\nsns.heatmap(X.corr(),annot=True)\nplt.show()","225fc7b4":"columns = np.full((X.corr().shape[0],), False, dtype=bool)\ncolumns[2]=True\ncolumns[3]=True\ncolumns[6]=True\ncolumns[7]=True\nselected_columns = X.columns[columns]\nX = X[selected_columns]\nX.head()","204677f6":"plt.figure(figsize = (19, 19))\nj = 0\nfor i in X.columns:\n  print(\"kurtosis value for \"+i+\" is: %0.2f\"%(st.kurtosis(X[i],fisher=False)))\n  plt.subplot(6, 3, j+1)\n  sns.distplot(X[i],color=\"black\")\n  j += 1\nplt.suptitle('Rice Type Analysis')\nplt.tight_layout()\nplt.subplots_adjust(top=0.95)\nplt.show()","5f0c6a8f":"X[\"Perimeter\"]=X[\"Perimeter\"].transform(func = lambda x : 1\/x)\nprint(\"Kurtosis value for \"+i+\" after transformation is: %0.2f\"%(st.kurtosis(X[\"Perimeter\"],fisher=False)))\nsns.distplot(X[\"Perimeter\"],color=\"green\")\nplt.show()","53e4c1d6":"X[\"MinorAxisLength\"]=X[\"MinorAxisLength\"].transform(func = lambda x : 1\/np.power(x,2.7))\nprint(\"Kurtosis value for \"+i+\" after transformation is: %0.2f\"%(st.kurtosis(X[\"MinorAxisLength\"],fisher=False)))\nsns.distplot(X[\"MinorAxisLength\"],color=\"blue\")\nplt.show()","89d9c521":"X[\"Extent\"]=X[\"Extent\"].transform(func = lambda x : np.power(x,3.95))\nprint(\"Kurtosis value for \"+i+\" after transformation is: %0.2f\"%(st.kurtosis(X[\"Extent\"],fisher=False)))\nsns.distplot(X[\"Extent\"],color=\"red\")\nplt.show()","2bce38a2":"X[\"Eccentricity\"]=X[\"Eccentricity\"].transform(func = lambda x : np.power(np.log(x),2))\nprint(\"Kurtosis value for \"+i+\" after transformation is: %0.2f\"%(st.kurtosis(X[\"Eccentricity\"],fisher=False)))\nsns.distplot(X[\"Eccentricity\"],color=\"yellow\")\nplt.show()","5d405595":"plt.figure(figsize=(5,5))\nsns.heatmap(X.corr(),annot=True)\nplt.show()","ba5655d2":"SS = StandardScaler()\ndata = SS.fit_transform(X)\nX = pd.DataFrame(data)\nX.columns=['MinorAxisLength', 'Eccentricity', 'Extent', 'Perimeter']\nX.describe()","a15a3050":"# Modules to split the data into train and test sets\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\n# Module to calculate the accuracy score on our classifier\nfrom sklearn.metrics import accuracy_score\n\n# Module to find the average of a given list of values\nfrom statistics import mean\n\n# Module to find the square root of a given value\nfrom math import sqrt","f45a5329":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=100)","d269fde3":"def evaluate_train(accuracy, total, parameter, classes=2):\n  train_error=1-accuracy\n  pessi_error=train_error+(parameter\/classes)\/total\n  print('Pessimistic error on train set is: %.3f'% pessi_error)","ba99ac65":"# Model Accuracy, how often is the classifier correct?\ndef evaluate_test(accuracy, length):\n  print(\"Overall Accuracy over Test Data: %.2f\" % (accuracy*100))\n  error=1-accuracy\n  interval = sqrt( (error * (1 - error)) \/ length)\n  print('True Error with 90 percent confidence is: %.3f'% (error*100),'\u00b1 %.3f'% (1.64*interval*100))\n  print('True Error with 80 percent confidence is: %.3f'% (error*100),'\u00b1 %.3f'% (1.28*interval*100))","86d2d26a":"def train_and_evaluate(classifier):\n  train_length=[]\n  test_length=[]\n  test_accuracy = []\n  train_accuracy = []\n\n  # Applyting Stratified 5x5Fold cross validation\n  for train_index, test_index in skf.split(X, y): \n      X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index] \n      y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index] \n      classifier.fit(X_train_fold, y_train_fold.values.ravel())\n      train_length.append(len(train_index))\n      test_length.append(len(test_index))\n      test_accuracy.append(classifier.score(X_test_fold, y_test_fold))\n      train_accuracy.append(classifier.score(X_train_fold, y_train_fold))\n\n  # Plot of Accuracies of various\n  plt.plot(range(0,5),test_accuracy,'bv--')\n  plt.legend(['Test Accuracy'])\n  plt.xlabel('Test Sample')\n  plt.ylabel('Accuracy')\n  plt.show()\n  \n  print('We can see that the test accuracies lie in the range of %.3f' % min(test_accuracy),'to %.3f.'% max(test_accuracy))\n  \n  return (mean(train_accuracy),mean(test_accuracy),mean(train_length),mean(test_length))","09c0cb66":"# Importing the Gaussian Naive Bayes model\nfrom sklearn.tree import DecisionTreeClassifier","8c6185f2":"# Model fitting and evaluation\nmaxdepths = [2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50]\n\n# Training and Test set creation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n\ntrainAcc = []\ntestAcc = []\n\nfor depth in maxdepths:\n    clf = DecisionTreeClassifier(max_depth=depth)\n    clf = clf.fit(X_train, y_train)\n    y_predTrain = clf.predict(X_train)\n    y_predTest = clf.predict(X_test)\n    trainAcc.append(accuracy_score(y_train, y_predTrain))\n    testAcc.append(accuracy_score(y_test, y_predTest))\n\nmax_depth=maxdepths[testAcc.index(max(testAcc))]\n\n# Plot of training and test accuracies    \nplt.plot(maxdepths,trainAcc,'ro-',maxdepths,testAcc,'bv--')\nplt.legend(['Training Accuracy','Test Accuracy'])\nplt.xlabel('Max depth')\nplt.ylabel('Accuracy')\nplt.show()\n\nprint('Max test accuracy value is %.4f'% max(testAcc),'for depth %d.'% max_depth)","ce3fb987":"# Creating the decesion tree classifier object \ndt = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth=max_depth, min_samples_leaf=5)\n\ntrain_accuracy,test_accuracy,train_length,test_length=train_and_evaluate(dt)","cad87626":"!pip install pydotplus","905901fb":"import pydotplus\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz\n\ndot_data = export_graphviz(dt, feature_names=X.columns, class_names=['Jasmine','Gonen'], filled=True, out_file=None) \ngraph = pydotplus.graph_from_dot_data(dot_data)\nImage(graph.create_png())","d06c2cf6":"evaluate_train(train_accuracy,train_length,8)","c9e48138":"evaluate_test(test_accuracy,test_length)","8604b694":"# Importing the Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n# Creating a Gaussian Classifier Object\nnb = GaussianNB()\n\n# Operational Phase\ntrain_accuracy,test_accuracy,train_length,test_length=train_and_evaluate(nb)","5db728d0":"evaluate_train(train_accuracy,train_length,X.shape[1])","2e194e13":"evaluate_test(test_accuracy,test_length)","94d2c0f4":"# Importing a RIPPER Rule Based Classifier Model\nfrom sklearn.neighbors import KNeighborsClassifier","66d9e3c1":"# Model fitting and evaluation\ndistances = [1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50]\n\n# Training and Test set creation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n\ntrainAcc = []\ntestAcc = []\n\nfor distance in distances:\n    knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=distance)\n    knn.fit(X_train, y_train.values.ravel())\n    y_predTrain = knn.predict(X_train)\n    y_predTest = knn.predict(X_test)\n    trainAcc.append(accuracy_score(y_train, y_predTrain))\n    testAcc.append(accuracy_score(y_test, y_predTest))\n\nbest_distance=distances[testAcc.index(max(testAcc))]\n\n# Plot of training and test accuracies    \nplt.plot(distances,trainAcc,'ro-',distances,testAcc,'bv--')\nplt.legend(['Training Accuracy','Test Accuracy'])\nplt.xlabel('Distance')\nplt.ylabel('Accuracy')\nplt.show()\n\nprint('Here, max test accuracy value is',max(testAcc),'for distance',best_distance)","21d9dd96":"# Creating a KNN Classifier Object\nknn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=best_distance)\n\ntrain_accuracy,test_accuracy,train_length,test_length=train_and_evaluate(knn)","1cb18bb4":"evaluate_train(train_accuracy,train_length,knn.n_neighbors)","d4b59911":"evaluate_test(test_accuracy,test_length)","fe50aad0":"!pip install wittgenstein","7c007a68":"# Importing a RIPPER Rule Based Classifier Model\nfrom wittgenstein import RIPPER","adc98136":"# Model fitting and evaluation\nallowances = [1,2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50]\n\n# Training and Test set creation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n\ntrainAcc = []\ntestAcc = []\n\nfor depth in allowances:\n    clf = RIPPER(dl_allowance=depth)\n    clf.fit(X_train, y_train)\n    y_predTrain = clf.predict(X_train)\n    y_predTest = clf.predict(X_test)\n    trainAcc.append(accuracy_score(y_train, y_predTrain))\n    testAcc.append(accuracy_score(y_test, y_predTest))\n\ndl_allowance=allowances[testAcc.index(max(testAcc))]\n\n# Plot of training and test accuracies    \nplt.plot(allowances,trainAcc,'ro-',allowances,testAcc,'bv--')\nplt.legend(['Training Accuracy','Test Accuracy'])\nplt.xlabel('Allowance')\nplt.ylabel('Accuracy')\nplt.show()\n\nprint('Max test accuracy value is %.3f'% max(testAcc),'for dl_allowance %d.'% dl_allowance)","df8950df":"# Creating a RIPPER Classifier Obect\nrb = RIPPER(dl_allowance=dl_allowance)\n\ntrain_accuracy,test_accuracy,train_length,test_length=train_and_evaluate(rb)","6647d61a":"rb.ruleset_.out_pretty()","eed75fd4":"no_rules=len(rb.ruleset_)\nprint('Number of rules generated by the model:',len(rb.ruleset_))","c264e029":"evaluate_train(train_accuracy,train_length,no_rules)","c4cb30e0":"evaluate_test(test_accuracy,test_length)","1c81ad96":"# Classification\n## Setting Up\n### Importing Modules\nWe will first import the require modules for classification and calculation.","1047a3bc":"## Class Imbalance Analysis","bf8d16cc":"## Summary Statistics\nWe will display the summary for all the attributes simultaneously in a table using the **describe()** function. If an attribute is quantitative, it will display its mean, standard deviation and various quantiles (including minimum, median, and maximum) values. If an attribute is qualitative, it will display its number of unique values and the top (most frequent) values.","efc7036f":"Now, we will perform classification by applying various classification algorithms. And then, we'll estimate how accurately our classifier can predict the type of rice by comparing actual test set values and predicted values during the stratified cross validation process.","514c89e9":"### Evaluation on Test Set","82c1d8c8":"## Transforming MinorAxisLength Attribute","a5810dc5":"Since no two attributes are highly correlated, we wont be removing any other feature further.","aeec0315":"# Reading and Understanding Data\n## Importing Libraries","35931734":"From the above result, we select optimal depth for every attributes as follows-\n* Area: 4\n* MajorAxisLength: 3\n* MinorAxisLength: 4\n* Ecentricity: 2\n* ConvexArea: 4\n* EquivDiameter: 4\n* Extent: 4\n* Perimeter: 4\n* Roundness: 4\n* AspectRatio: 2","6635e0b0":"# Dimension Reduction (Again)","e6f711b3":"### Evaluation on Test Set","88a40f03":"## Transforming Extent Attribute","5914686c":"We see that the attributes MinorAxisLength, Ecentricity, Roundness and AspectRation are strongly associated with the class and thus are most interesting.\n","32a591ad":"## Applying 1\/x transformation on Perimeter Attribute\n\n\n\n","727b3e01":"### Operational Phase\nHere, we will train the data on our operational function using the best distance obtained above for our p value and then plot the test accuracies obtained for the 5 rounds of our 5x5 fold cross validation with stratified sampling for our KNN classifier.","6c4bdf41":"# Discretization\nSince all the attributes are continous and our class is binary, we will use **supervised discretization** in continuous attribute to calculate correlation with the class.\n\n## Decision Tree Formation\nFor supervised discretization, we will use target class infomation to form bins with the help of decision trees. But, first, we have to find some optimal parameters, for example, depth of the tree.","42fedc05":"## Data Types","e3bac872":"# Dimension Reduction\nWe will now compute the correlation between pairs of attributes and between attribute and class and plot them in a heat map for visualization which will help in dimension reduction.\n\nsince our data is now in the form of bins so for finding correlation between the class and the attributes we will use Cram\u00e9r's V method which is based on the chi square measure whose value varies from 0 to 1 as weak association and strong association respectively we have printed only those attribute whose association value is greater than 0.9 as strong association","a2411358":"### Identifying Complexity\nWe try to find the complexity stopping threshold value. We will split the data as 75% train and 25% test for this purpose.","55306645":"From the coorelation heatmap, we make the following observations:\n\n* MinorAxisLength is highly correlated to Area (0.95), ConvexArea (0.95), EquivDiameter (0.95), Ecentricity (-0.92), AspectRation (-0.91) and Class (0.977).\n* Ecentricity is highly correlated to MinorAxisLength(-0.92), Roundness(-0.95), AspectRation (0.98) and Class (0.968).\n* Roundness is highly correlated to Ecentricity (-0.95), AspectRation (-0.96) and Class (0.94).\n* AspectRation is highly correlated to MinorAxisLength (-0.91), Ecentricity (0.98), Roundness (-0.96) and Class (0.965).\n\nBased on these findings, we will go with MinorAxisLength, Ecentricity, Extent and Perimeter as our attributes.","89b9803d":"### Displaying the generated rules\nWe can access our trained model using the clf.ruleset_ attribute. A trained ruleset model represents a list of disjunction of conjunctions -- 'V' represents 'or' and '^' represents 'and'. In other words, the model predicts positive class if any of the inner-nested condition-combinations are all true.","50d4c9cf":"# Data Transformation\nIn this section, we will try to make the distribution of continuous numerical data as Normal (Gaussian).\nWe will first plot the data to visualize the type of distribution.\n\nWe will be checking their normality using the **Kurtosis** value and plotting the graph for visualization. Kurtosis can be measures by two types, one is by taking **fisher=true** in which case, normality is when **kurtosis = 0**.  Another type of measuring kurtosis is by taking **fisher=false**, where Pearson definition is used and for normality is achieved for **kurtosis = 3**.","aae2a205":"### Function to Evaluate and Calculate True Error on test set\nAssuming that our sample is drawn independent of the model and each other and since the number of training examples in all the cases will be *n* greater than 30, we can say that with N% probability, the error of the entire distribution (true error) lies in the interval:\n\n![Picture1.jpg](data:image\/jpeg;base64,\/9j\/4AAQSkZJRgABAQEAYABgAAD\/4RqgRXhpZgAATU0AKgAAAAgABgALAAIAAAAmAAAIYgESAAMAAAABAAEAAAExAAIAAAAmAAAIiAEyAAIAAAAUAAAIrodpAAQAAAABAAAIwuocAAcAAAgMAAAAVgAAEUYc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFdpbmRvd3MgUGhvdG8gRWRpdG9yIDEwLjAuMTAwMTEuMTYzODQAV2luZG93cyBQaG90byBFZGl0b3IgMTAuMC4xMDAxMS4xNjM4NAAyMDIwOjEwOjIwIDIyOjE4OjIyAAAGkAMAAgAAABQAABEckAQAAgAAABQAABEwkpEAAgAAAAM3MQAAkpIAAgAAAAM3MQAAoAEAAwAAAAEAAQAA6hwABwAACAwAAAkQAAAAABzqAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMjAyMDoxMDoyMCAyMjoxNDo0NAAyMDIwOjEwOjIwIDIyOjE0OjQ0AAAAAAYBAwADAAAAAQAGAAABGgAFAAAAAQAAEZQBGwAFAAAAAQAAEZwBKAADAAAAAQACAAACAQAEAAAAAQAAEaQCAgAEAAAAAQAACPMAAAAAAAAAYAAAAAEAAABgAAAAAf\/Y\/9sAQwAIBgYHBgUIBwcHCQkICgwUDQwLCwwZEhMPFB0aHx4dGhwcICQuJyAiLCMcHCg3KSwwMTQ0NB8nOT04MjwuMzQy\/9sAQwEJCQkMCwwYDQ0YMiEcITIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIy\/8AAEQgAJAEAAwEhAAIRAQMRAf\/EAB8AAAEFAQEBAQEBAAAAAAAAAAABAgMEBQYHCAkKC\/\/EALUQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29\/j5+v\/EAB8BAAMBAQEBAQEBAQEAAAAAAAABAgMEBQYHCAkKC\/\/EALURAAIBAgQEAwQHBQQEAAECdwABAgMRBAUhMQYSQVEHYXETIjKBCBRCkaGxwQkjM1LwFWJy0QoWJDThJfEXGBkaJicoKSo1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoKDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uLj5OXm5+jp6vLz9PX29\/j5+v\/aAAwDAQACEQMRAD8A9\/ooAKKACigAooAKKACigCpC93OjOHijG9lCtGScBiP7w9KftvP+e0H\/AH5P\/wAVQAbbz\/ntB\/35P\/xVG28\/57Qf9+T\/APFUAG28\/wCe0H\/fk\/8AxVRvJcJNHCbiDzJASqiFug6n73TkfnQAy3uZLrzvIuoH8mQxyAQN8rDqPve4o+1SfZFuRdW5ibG0iFuSTgD73XPFACi5Zrs2gv7M3IXcYQvzgeuN+cciptt5\/wA9oP8Avyf\/AIqgA23n\/PaD\/vyf\/iqNt5\/z2g\/78n\/4qgA23n\/PaD\/vyf8A4qjbef8APaD\/AL8n\/wCKoANt5\/z2g\/78n\/4qkb7UilmuLcKBkkxHj\/x6gCs18yLas93Aoum2w5gb5yQSB970BPNTGS4FwIDcQCQqXA8luQDg\/wAXuPzoAdaTPcL5nnROgJUhYypDA4IOSehq1QAUUAFFABUVxcR20RkkOFyAABkkngAe+aAGfbrY6ibDzP8AShF52zafuZxnPTrT47iOWWWJT+8iIDqeozyD9DQAsk8ULRrJIqGRtiAn7zYJwPfAP5VJQAUUAYmqf2kbayTT22I96RdOFJKxfOeADnlgg47E1C1nqNxqNrF9qvY7U28rSyKdvzEgIBnJzyTzn7oz3oAktrPUG1O\/kkurvyYpE+zIzgLJhcnPH3SWA\/4B9c5hXxC+lq6\/aRdDT2acE4zcsVIVR2C4YZHGCOTQBo30t1otgpinmu5nIeRZJUD7VUBjHuwuc4OD\/ePtUlrK7axHcSK7NJp6suV2k\/NkjHY8rxQBl+HbXVra7smuoZohcQS3V7lsr50r7gmO2wAj8R15pbZHEdrNtYWf9tzSAHpsPmBT9DIQR9RQBdlhlPj23uVgl8hbCSFpRGdu8urAZx6A81tTrdMV+zzQxjv5kRfP5MKAHQLcKG+0SxSHt5cZTH5sa5q2\/t1dFmurw3EmorHLIbaFdoLZJWMEnGMAAEc9yeaAJZ9P1W306xijvb2e6mlgSeTeNsYGDI3ToQrD6sKS7i1e3u7mCFrue1SzzasHG6W4d3yGbjCoNmM44Y9SKAJLC21EXt1NqN7cxwW3lbSGASQhdznGPu5bHb7lVZ7+\/wBR0rXLZ42WWK2crsKsjht20owOeinIbnJHagBdfg1C8u4Lqxgea2sbCW4gRH2+dcMNsYBHou\/\/AL6FXGS5Fz4dikd5LmJnM8jDBZBCysT9XKHFAFvSebrVHUfu2uztPYkIobH4gj8K1KAKOsfb\/wCxb3+y9v2\/yW+z7um\/HHWsi7stV\/s6OK1uL0zyTQI8juF2IGzI+Mnnbn8ccUATXOm3z6tZ28N1erYeXPJPN5\/zbicIg7\/xsfbYKrtb6681zGHmDm9jEEvmAJHbLtJJH8THDjkdT6CgCS0j1C00ue8uriRLx3mW3huZsxrubEQbGewXn\/aNRNc3N9Bpt3PHKkcOpASo4X5QFdMgrwy7ypz\/AIUAV73SdVl1u61KETRSy3ltCjRyY22sXztxnB3sXXnswPat23G7xJfOFOBbQIx7bsyHH5EfmKAM3W7K8ur+yeA6gUhuhK2wx7Quxhlc89SOvvW1AZI7DJFxJIAcCQrvP5cUAZlu13Fp8kIlnt7uVjHbm7cSfNyeOTkdTyfy4qfT9VeXTFvb2OSBmdYpISv+qkzsYZ7jdnnpjmgC6iztCPJlRDvfO9C2fmPuKPLvv+fqD\/vwf\/iqADy77\/n6g\/78H\/4qjy77\/n6g\/wC\/B\/8AiqAEMN6etzbn625\/+KprW127Izz2zMhypNucqcY4+b0oAcYr0jBuYCD\/ANMD\/wDFU02lyYfJMtqYsY2fZjjH03UAP8u+\/wCfqD\/vwf8A4qjy77\/n6g\/78H\/4qgA8u+\/5+oP+\/B\/+Ko8u+\/5+oP8Avwf\/AIqgA8u+\/wCfqD\/vwf8A4qjy77\/n6g\/78H\/4qgA8q+IwbmD\/AL8H\/wCKpBDegYFzbj\/t3P8A8VQAiW93EgSOe2RB0VbcgD\/x6j7Pd+Z5nn2+\/G3d9nOcen3qAJLWCWBdjPD5YGFWOLZj9TVigAooAKKACmsiuhR1DKRggjg0AOpqoqbtqgbjk4HU0AOooAKiuIUubaWCQZSRCjD2IxQBFpqzpp8K3IxMAd\/PfJq1QAUUAFFABRQAVyz2Ori6mmQTYbUUKBphxCqct1wAzDBHoenFAHU0UAFFABRQAUUAFFABRQAUUAFFABRQAUUAFFABRQAUUAFFABRQAUUAFFABRQAUUAFFABRQB\/\/ZAP\/hMehodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvADw\/eHBhY2tldCBiZWdpbj0n77u\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\/eHBhY2tldCBlbmQ9J3cnPz7\/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT\/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT\/wAARCABeAqUDASIAAhEBAxEB\/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL\/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6\/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL\/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6\/9oADAMBAAIRAxEAPwD9U6KKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACivJ\/D9v4j8beLvH6N451vRrbSdbWwtbLT7bTzHHF9htJs5ltZHJLzOclj2xxW3\/wrfxD\/ANFV8Xf+Auj\/APyBQB3tFcF\/wrfxD\/0VXxd\/4C6P\/wDIFH\/Ct\/EP\/RVfF3\/gLo\/\/AMgUCO9orgv+Fb+If+iq+Lv\/AAF0f\/5Ao\/4Vv4h\/6Kr4u\/8AAXR\/\/kCgDvaK4L\/hW\/iH\/oqvi7\/wF0f\/AOQKP+Fb+If+iq+Lv\/AXR\/8A5AoA72iuC\/4Vv4h\/6Kr4u\/8AAXR\/\/kCj\/hW\/iH\/oqvi7\/wABdH\/+QKAO9orgv+Fb+If+iq+Lv\/AXR\/8A5Ao\/4Vv4h\/6Kr4u\/8BdH\/wDkCgDvaK4L\/hW\/iH\/oqvi7\/wABdH\/+QKP+Fb+If+iq+Lv\/AAF0f\/5AoA72iuAf4da\/GrM3xW8WqqjJZrbRwAPX\/jwoT4da\/IqsvxW8WsrDIZbbRyCPX\/jwoA7+iuC\/4Vv4h\/6Kr4u\/8BdH\/wDkCj\/hW\/iH\/oqvi7\/wF0f\/AOQKAO9orgV+HPiBiwHxW8WkqcHFro\/Hf\/nwpf8AhW\/iH\/oqvi7\/AMBdH\/8AkCgDvaK8\/m+Huu28TSy\/FjxZFGoyzvbaOAB6k\/YKf\/wrfxD\/ANFV8Xf+Auj\/APyBQB3tFcF\/wrfxD\/0VXxd\/4C6P\/wDIFH\/Ct\/EP\/RVfF3\/gLo\/\/AMgUAd7RXBf8K38Q\/wDRVfF3\/gLo\/wD8gUf8K38Q\/wDRVfF3\/gLo\/wD8gUAd7RXBf8K38Q\/9FV8Xf+Auj\/8AyBR\/wrfxD\/0VXxd\/4C6P\/wDIFAHe0VwX\/Ct\/EP8A0VXxd\/4C6P8A\/IFH\/Ct\/EP8A0VXxd\/4C6P8A\/IFAHe0VwX\/Ct\/EP\/RVfF3\/gLo\/\/AMgUf8K38Q\/9FV8Xf+Auj\/8AyBQB3tFcF\/wrfxD\/ANFV8Xf+Auj\/APyBR\/wrfxD\/ANFV8Xf+Auj\/APyBQB3tFcF\/wrfxD\/0VXxd\/4C6P\/wDIFH\/Ct\/EP\/RVfF3\/gLo\/\/AMgUAd7RXBf8K38Q\/wDRVfF3\/gLo\/wD8gUf8K38Q\/wDRVfF3\/gLo\/wD8gUAd7RXBf8K38Q\/9FV8Xf+Auj\/8AyBR\/wrfxD\/0VXxd\/4C6P\/wDIFAHe0VwX\/Ct\/EP8A0VXxd\/4C6P8A\/IFH\/Ct\/EP8A0VXxd\/4C6P8A\/IFAHe0VwB+HOvqwU\/FbxaGboPsuj5P\/AJIU7\/hW\/iH\/AKKr4u\/8BdH\/APkCgDvaK4L\/AIVv4h\/6Kr4u\/wDAXR\/\/AJApsnw716Fdz\/FfxYi9NzW2jgc8f8+FAHf0VwX\/AArfxD\/0VXxd\/wCAuj\/\/ACBTf+Fda+GC\/wDC1vFu4gkL9m0fJAxk\/wDHh7j86AO\/orgv+Fb+If8Aoqvi7\/wF0f8A+QK7LSrObT9Pgtri\/uNUmjXDXl0sayyn1YRoiZ\/3VAoGW6KKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA4\/wHo8Ol6546miaRmvtcF1JvIwG+xWseFwOmI1655J5rsK5zwpn+1PFWen9qDH\/AIDQV4d+25+1Bqf7O\/hbw1pfhjT\/ALf428YXradpDSQvNDbFdnmTvGgLSbRIuEA5z7YIB9KUV8uTWcNx4Fl0vS9e+JnjP4hzRD\/iZh9W02H7Uf8AloV\/c2sEIYHCEYwMEPzn0Px5Prmh+B7bUPF2qXVvo2g+H3vdevdKuGtpL27WIBtjQskkYVlZ\/lIVt4GOKAPYKK+I\/wBmfwt8WPjh+xfpmuap491S28dajYXC+H72a\/u4FtojI4gluWik3XT7cHfIDxtyGYFz9Br4Q8WaX4B8CaN4g8UTX8Ol2CjxTrVrPLbXN7JFAPnR0IkUPKpLbSCVJFAHrNFfDv7G5+IX7Qv7OHiPxDeeONWiivrzU4vB7z6hdCayAkkSKS5nSXzbkI2AFdiNq85PzV6zrHxA1f8AYz\/ZHOv\/ABM8RSePfEGg2axy3mHVr66dgsUO8gscuwXzGGSPmbHNAH0TRXgvwZ+Hfifx54PtPF\/xP8Qa\/wD8JHrkIuhoekazdabZaRFIMrbxpbSRl2AIzJIztnoQK4L4AfELxv421H45\/Dmw119Vj8G+JbfR9K1q+lZ76Gymb9+HmYkyywxh9jsMsyjcW60AfW9FfGXwY1bxL8SP2s\/jH4U0\/wAT+IYPhf4WWzsrqzuNUuJriTURkkQ3LyNLDGwDFgjLu2gDAzn2n9nf4W\/EL4a3HjNvHPj6TxlaalqXn6NZsZZP7Nthu+TzZWaRi2VyCSBs4PJwAeyUV5lZ\/F6+8ZeJNc0vwJotl4hg0O5+xajql9qZs7RLkLuaCMpDKzuoK7vlCjcPmJ4rZ+HXxCuvHFxrlteaFNol1o1ytlciSdZo3n27nETqPmQAphiFJ3EFVxyAeZ+KvEX\/AAuz4\/P8MIDL\/wAIp4UtItX8StGwMd9cSsy2li\/H3PkklcZ5KRjGCa96FrHHZ\/ZoR9niWPy0WEBdi4wNuOmB0r5x\/ZNs0uPih+0brbs\/2y68bi0dP4BHDY23lkcA5PmNnPHTHv7T8V\/G0Hw3+GnifxPcyeVFpWnzXW\/GcMqHbx\/vYoA+Xf2Vdc8UeMv2uPjef+E18Q694A8Kvb6Tp1pql80kJunTdcEJheUZcAkcBu\/UfZtfEf7AnwNWf9mlfFXji+vRN4uv7zxNcWtjfT2KRiZsb5GhdWd9sYIydqgjAB3Fu1\/4J8+PPEXj\/wCHfji71LVbrWfDNp4v1Gy8M3eo3DXFydORl8tTK5LyICWCs5LYGM4AoA6T9oTUj8AdcsvjDpkTJpPnw2HjGzgT5LizYhEvWA482HhdxxuRgpbCKK+gIpUnjSSN1kjcBldTkMD0IPcV5V+1hpMOu\/s1\/EixnLrFNolwCYyAwwuRtJB5yBjjrVz9nLUJr79mv4X3sztJcTeEdLmeR3LszGziJJbPJyeueaAOJ0LWo\/2hvjh4ktpA03gb4eXqWCwF\/wB1qOrbFkkeROjpBuVVB\/5aK5r2TxpqXiDSdDe48M6Ha+IdVEiKtjeah9hjKk\/M3m+XJggc428+1eHfsE2kcnwR1LW8ub3XPFev392XIP7z+1LlOuAfuov3uevtX0fQB8q\/s3ftT+N\/i9+0t8Vfhx4n0HQtDtPB0MYjXSp5rmR5TJtYtM4QMMdhEv419VV8Bfsc\/wDKRD9qI4\/ij5\/7bmvv2gAoqnrGqQ6HpN5qNwlxJb2kLzyJZ20lzMyqpJCRRK0kjYHCIpYngAk4rzb\/AIaW8I\/9Aj4gf+G48Q\/\/ACDQB6rRXlX\/AA0t4R\/6BHxA\/wDDceIf\/kGut8D\/ABG0r4hQ3cul2mvWq2rKsg1zw9qGksSwJGwXcERkHByUyBxnGRQB1FFFfEHgz9p8\/tLfFvxkk2r+IvCnwq8I3X2G1Tw7ZX32vXbsbvMeW5giLRRIFP7qNlYllJbHBAPt+ivEfhymuT\/E261Hw9D4mi+H6aTJDLF4qnu\/Nu78PE0LQJeuZo1CGYMxVFYsv3iuR4l8XvEPxA8PftHfAvwdoXi\/ULfxdr1xqV\/4jzdSzWCaeNoRPshkEIZR8qsqghlLcgsCAfbdFeMaL8H\/ABpoP7QFv4mtvHN7L8O49Ee0k8OX17c3ctxfNJk3DGV2RQABjYBjJAAFeFftReK\/iN4L8UfCLStG8R32k\/Efxn4vmghWC+kl06HTYjjymtiwhfMbRuSyFt5YbgBQB9uUV4NrHwO+IKfG7wT4k8P\/ABL1DT\/B2mQyHXdJvrq4u31aZmOR5buYYlI24KKCmDtAya5X4j\/GvxL8Tv2kh8CPAF5N4fi0m1j1Hxd4pgKfabSJ1Dx2tqrqwEjqVJkwdoYYwRmgD6jor5N\/bK1C5\/Zo+CsPj7wl4m8QWmv6PqFpFDHqutXuo22oCSQK8E8M0rI28A\/MApU8qy1J+214y13wL8EfiR4wuNU1bRYLezt7Hwuuh6hLbSfbJVXbcSGGRSSJZNmxsriMHB3GgD6uor5Y1z4NfGT4ifA\/wfd6V8Ubjw38TbqKxutV1jzbhbSJdimSOK0R\/JY8kEsmX6nbnA9++I3xI0b4V+FpNc12WQQCSO3hgtk3zXVxI4SKGJM\/M7uyqBkcnkjrQB1NY\/jLxXp3gTwjrXiTV5hb6VpFlNf3Up\/hiiQu5\/JTXA+K\/ir4w8C+F5\/EGs+BLY2S7USz07W\/PvfMkYJGroYEjA3MAxWV8dg9ec\/8FGNQlh\/Y78YCKR4BfyadYytGwDCKe9gikAJ9UdhzxzQB2X7NWl6p4o0E\/FHxWhPijxVCs8FvKD\/xKdPbDRWcYP3B0Z8ffYAnJArG\/bw8YSfDn9mbxd4rtfEWr+HNR0uD\/QZtHufJkluZSIYkbCksu6QNgY+7nIANe5+HrSLT9B022gXZBDbRxovHChQB0r44\/wCCgwvvih8QPgZ8FNMn+zT+JvEI1W7kkiDxm0s1MkgYZ5\/vBeASoyRQB9F\/s06T4k0X4C+B7Xxhql1rPif+zIpNQvr2czyyyuNxJc8nqB+Fd\/rWi2HiLSbvTNUs4dQ066jMU9rcIHjkQ9VYHqK+QP24RJ8D\/hz4W1fwdr2vWvxF1HxLp+maTdSazdSJcSO53xywGTy2iMYYFfLIHykDIBr7D01rhtPtTd7RdGJTNtGBvwN2B6ZzQB4j8DfFl14O+Jfiv4L61ezX8+h20Os+H766laWa50qZmTy3YqNzwSoVJHGySID7pNdr8avANx4y8KvfaHMdN8ZaMrXmianEgMkcyjJiPHzRygbHQgggg4JVceY\/ElU0f9uX4O3sHy3Gq+GdesbhjgBo45LF1HbJyT3Pbgda+kKAPPfgF8WoPjd8KND8WxwfYru5V4L+xON1reQu0U8RGSRiRGwDztKk4zXoVfMn7DOmxaHY\/GvSrd5XtLP4kal5KysDsWS1s5WUAAYG93PAHXuck\/TdABRRRQAUUUUAFFFFABRWN4v8ZaH4A8PXeu+I9VtdF0i0XfNeXkgRFHYZPUnoAOSeBXIXHx68P2Pgn\/hMr3T9c03wiqCWXV9Q0yS2EMRx+9eCTbOqYOdxjxjmgD0iiuT8afEzRvA63QvBc3dza2MmpT2thD500dqmd0pXPT5SAOrEYUE8Vw+oftb\/AAysvhuPiBHrv2zwT5Ec767CmLZN+MR5cqWmBODCoMitlSoYYoA9kormNS+Imj2C6YkTy6hd6nCbizs7NA8s0YXczjJCgAc5Yj0GTgV5\/pf7Xnwy174eTeONK1ptQ8M20H2i8v1i8qOyAIBSdpSixyZIHlE7znhTQB7PRWD4N8caH4+8G6X4r0K\/W90DUrVb21vSrRq8LDIchwCox6gVy3hn48eHPHVhqeo+ErbVvFWladI0U2o6ZYsbeVl+99nd9oucYPMHmcjHXAoA9HorhNA+OHgrxV4Di8YaNrkOpaHLcCzWSAHzftBkEYgaIgOku9lBRgGGeQKxtD\/aV8E+IvEniLw1Y3N5L4p8P3H2fU9D+zH7Ta\/Lu8xj9wRYI\/e7\/LBIBYEgEA9UorifhN8Y\/Cnxu8O3et+ENSXU7CzvpdOuJFHCXEYUumfutgOvzKSpzwTXbUAFcV48+JSeFNa0Pw\/p9g2teJ9adja6eknlKsCFfOuJHIO2NAw7EkkADkkdrXzN+yHq0vxQ8afGH4m3pV57rxFJ4Z09VUr5NjYKAE5AzmeW4bPX5uvAAAPpDUL6PTLGe7lSaSKFC7LbwvNIQOu1EBZj7KCT2FecfCb9pLwH8bPEXiLQvCl\/qE+r+HjGuqWeoaRd6fJas5IVWW4iQ7vlPGMjv1Fd74m1yDwz4d1TV7p1jt7G2kuZGboFRSx\/lXw9\/wAE8PFsln8J\/GnxCGga14s1zxr4jvvEF1HpUUW6zti+yOFp55Yo3ZRGzeWjFlVx8oyMgH3pXE6D8TIbzx9qfgzV7QaPr9vH9stIzMJI7+zLMEmibA5G0h0IypB6qVZrHwr+Kvhv4zeC7LxT4VvWvdKui6fvYmilikRikkUiMAVdXVlI9RwSMGvE\/wBu2+ufh\/4F8MfFfSlYa14G1qC7DKzDzLKZlju4WxwVdMA5x06igD6aopFYMoIOQeQRUd1dQ2NvJPczR28EalnllYKqgdSSeAKAJayvDvirRvF9nPd6HqtnrFtBcSWks1jOsyRzRttkjYqTh1bgqeQRg1XPjrw1g\/8AFQaUf+32L\/4qvnX9hbxZomnfCrxTFdazp9tJ\/wAJtrzBJrpEODeuQcE9DnPQdfxIB9T0Vn6b4i0rWpHTT9Ts750GWW2uEkIHqQpPqPzrQoAKK4D4ufHLwn8E9OtLnxJdXT3N7J5dppul2ct7e3JAyxjgiVnIABJbGB3NUvAv7R\/w8+JVtpTeHPEdtqV7qMjRR6UuVv4XUEus1u2JIdoBzvAxx6jIB6ZRRRQAUUUUAc74V\/5Cnij\/ALCY7\/8ATtBXiX7UnwY8ReMfid8FviL4a09tbn8CazNPfaTDNHFPcWs6KrNE0jKhZNmdpYbgTjnivbfCrZ1TxSNuMamPx\/0aDn\/PpXRUAcdqfi\/XbiSC20LwpfXEkww99qUkdra2vu4LGVz7IhB6FhXk\/wC254N+Ifjz9mXVfB\/gnTx4g8S619n069aF47VPIdgLiXEknAxk7dxwD3xz9E0UAYPgLwja\/D\/wN4d8MWP\/AB5aLp1vp0HGMpDGsanH0UV59+1tpvjLXP2d\/G+j+ANIk1rxTqlg9ha20U8cLjzRsZw0jKoKqSeT245r1+igDgPgD8NU+DvwX8GeDFA8zRtLgtJmUAB5VQCRuCRy249T1rg\/22vgXrH7QXwG1Lw34fW0n1u3urbUrW0vpWihumglWQwM64K71UqDkAFhkgcj3uigDzDxJ468azeC44vCngW+\/wCEsuofJjj1maCC006YjHmXEgkcuinkiHzS2OOua5z9mX9nGD9mL4a6ra29w3irxlq00mqazqbkQnU70gnAzwi5JUZ6Zya9yooA+Zv2Cfgn4k+Efwr1q\/8AG9gdO8beKtZuNY1SBpEkMe5v3aFlZgcLnv3r6P1PU7XRrCa9vp0trWEbpJZDgKOn86tVz3j7wPp\/xI8J3nh3VpLuLTrxojM1jcNbzEJKkm0SLhlyUAJUhsE4IOCAD5g8Rfs3\/FX4M6h4t8W\/A74padpOhandzeILnwX4m0pJ7CScoC5W63ebErBBnBA7kjt7f+zN8SLv4wfBHwx431LRrfQtW1yBri9tLUER+crtGzAnkg+WCCSTjHJq7qnwdk17TTpWq+N\/E+o6FIPLn0qR7NI7iHGPJkkS2WZkI4P7zLDIYsCc91pel2eiabbafp9rFZWNtGsUNvAgRI0AwFUDgACgD5x\/Z3+1+C\/2mvj\/AOD9QAt49T1Cy8W6XFgASwXEJgldeM5ElqAeoGV9TWh+3h4B8b\/Fb4Bz+C\/A2mNqV1r2qWllqWyaKM29hvLzS\/vGAO0onAOTnjNeg\/E74Y3PiHX9A8Y+G54dP8aaA0iW80xKw3tpLjzrOfaM+WxSNwRyrxIehYN6FavLJawvPEsM7IDJGr7wjY5AbAzg98DNAHknxg8BeJ9P\/Zc13wZ8NwsviW38PrpWlCRkjDlY1j6udoJQN1OMmsb9mT4c638Lfh78PvCFnpUmgeHtA0bZqKXzJ595fyhXk2qhICpIZMsT8xbjIG5veaKAPC\/23PFTeFv2Y\/G4t3P9qapaf2Tp8Sgbprmc7ERcqwzyTyMcdR1r0b4S+DX8A\/CXwb4Tm5fRdDstLfBzkw26RHnJ\/u+tYeu\/DK+8cfFXS9e8QTwnw54cIm0bSYWLebeEc3cxwMMmdiIN2MFs\/NgelUAfNf7E80\/hvTvib8PL5Ps134T8ZamkNu2wH7JdTNe27qBztZLhTznkkdq988Wa5e+HdFkvdP8AD2peKLlWVRpukyWyTuCcFgbmaGPA6nLg+gJ4rivFvwz1G3+J2m\/EHwm9tFq4txp2safcP5MWqWgJKFnCMRLESxQ45ztJAAx6bQB8G\/AHwB8V\/hb+1h8ZfiVq\/wAHPEk+geMip0+Kz1XRHuI9sm796rX6qMj0Y193W8rTW8cjwvbuygmKQqWQ+h2kjP0JFSUUAFFFFABRRRQAV8xfsj\/C3xJ+zJovjrwfrejXOo6dc+I7vW9K1rTdksd5DOqfu2jDb4pEKYO4BTuBBPOPp2igDmvDuqeIta1Jrm+0geH9JSNkS0u5I5ruZyRh2MTskagBvlDOW3gnZtwfBvAPwf8AFurftzeP\/ip4o0c6doenaHbeHvDFwZ45BeRMxlnl2q25NrfLhgM7jivp6igAr5f1D4P+MPGn7e9h4+1nSWg8B+FfDRs9Hu3mhcT3s0haR1UNvXC4U7l5x1xX1BRQAV82eCfhN4g+FP7W\/wAV\/H76M+seGPHVppzw3unMrz2U1vCInimiZ1JVtu5WjRvvYYjFfSdFAHzD8Wvgl4l\/au8feHrTxfpsvhf4TeHLpdQOmT3CNe6\/dDlPMWNmEMCehbexJyFGKi\/bQ+D\/AIw+O3iT4S+FdL0V7zwPH4gj1HxNeieFVjgiwVQo7hmycn5Qf6H6jooAK8k\/aC+EOgftEeGX8EXfiG90DXLKWDWLK\/0mQC5sZopA0UuCCpG5RwcEjoRwa9brz7Uvg7bXXjbUfFun+JfEGha7fpFBLLY3ETxCGMACJYJopIgCRuLbd+ejAcUAeDeE\/Efxw+Efxu8C\/D34oeIvD\/xZ8L+LWuo7XWrfSVsNQtJ4InnDSQofLKYQcgZB5zwc+jftwfDzUfid+yz8QNG0jcdWhsf7Us440LvJNaOtyiKADlmaEKB3JGeM133hz4U6doviqTxPqF\/feJ\/EphNrDqusCAy20BOTDEsMUaIpPXC5Pcmu2oA5H4R+NrH4kfC\/wt4n02bz7HVNOhuYpOOQyDP45zXiHhv4N+K9Z\/by8TfE7xBpclp4V0rw5FpPh6dp4nWaaRgZ3VVYuvygg7lGd3Xjn0\/4V\/Cy8+EOuaxpOjTQyeAL13vrKxkcibS7hmy8MY24aBsll+YFCNoBByPTqAPmL44fCPxP4y\/as+FnjC70i41zwD4Tsb67S108wGT+02UJCrrI6HBUsytnarINxXPPvfgW11230VpvEcsbardTvcPbwyeZFaq2NsKMQMhQBzjqT1HNdFVHXJNRh0m6fSYLe51IIfs8V1KY4i\/bcwBIA68DtQB886k0vjz9vXR4IH82w8CeD7ma6Kj\/AFVxqE8IiXO3qyWkhwWPC8AZJr6OvLyHT7Oe6uHEVvBG0skh6KqjJP5CuH+EPwrj+Gem6rPdXn9reJddvG1HWdVZAhuZyoUBQPuxooCqvsT1Y0fF\/wAFa38SNDj8L2V9FpOhahlNYvVYtcNbgrm3iTGP3oLKzlvlUEAHdkAHk\/7Ben6jP8MfGHi3UJJHXxl4y1TXLTzIxGwttyW0WVAGMra7ugOCM56n6WqloujWXh3R7HStNto7PTrGBLa2t4hhIo0UKqgegAAq7QAUUUUAFFFFABRRRQB8L\/tTaVq\/xe\/b2+DPw2fVLe08O6bo8nitbW7s\/tME93HLKAJY96b1xCmBu45PevpPxR8L77xbo9zb\/ETxlDqHhYJuvdLtLGPT7O5jHJFw7vI5T1AdQR1rZ+I3wZ0D4katoeuXL3ekeKNCd30rxBpTrHeWe8AOo3q8bowHKSI6n0p9n8KYppYpPEXiTXPGPkuJIo9WlgihVh0JhtYoY3\/4GrUAeJf8FE\/FNr8O\/wBmvxpqdjZRz+J\/ElpH4Xt2Xmd0uH2bUGf4d7Ngdz6mvU\/hx8A\/DHh74BeEfhpr+g6br2k6Tptpbz2epWkc8Ms8SqTKyMCpbzAWz68irPxh+AHhj446l4Mu\/Er35HhXVk1izt7ScRxTTLjaswKksnAOAVPvXpVAHgP7ZnijTPhB8BfiB8Qgvk65b+HptGsZ97BVedgsQ25x\/rWQ+vGMik\/ZX+AGieC\/2S\/B\/gDxDoVnqVpcaak2q6bqMCzQyzygSS7kYEH5jn6iu2+O3wD8M\/tEeFbHw74rkvxpVrqEGoGGxmWPz2icOI3JVvkbGDjBwTgg816OiiNVVRhVGBQB8c\/8FGH1GH4Z\/DP4aeHZl0HRfG\/i2w8M3\/2FRCI7J8lo0xjauFGQCMqCvQ19O3l14d+CvwzmuJFt9I8M+G9NLlYkWOOKGJM4VQMZwOAByT71F8UPhP4Z+MHh2PR\/E2n\/AGuGC5jvbS5icx3NlcxsGjnglHzRyKRww7ZByCQeavv2fbDxVBDZ+OPFPiH4haRC6SrpOvNaR2jyJ91pY7W3h87HXbLvXPO3PNAHgv7BekOf2edU8eeObGfS7WbxTqfjG0+2AxBYGiYCYr027C5GQOVB7A1J\/wAE29Al8TeDfiP8VtXhD6l8QvEdxc5dAVeziJSLGeoO989jX1T448C6f488A654Qu5J7DS9W0+XTJX08rHJFFJGYz5ZKlVIU8ZUj2qv8LPhro3we+Hmg+C\/D6zro2i2y2tt9pk8yUqCTlmwMkkk9B1oAu+G\/B+hfD3QZdO8LaBYaNYq0lwun6Xbx20Tytyx2qAoZj1J\/Gvk39n39pb4ffEHUtbtfil4zj0D4nNqdzYS+F9b1OWxjsI0k2xR26EpGWK7W8xcuSxw2AAPpv4r+MdW8E+HbS80bQ9Q1+8n1C3tWg0+2ad4omfMsrKOiqitz6kAAkgHyf8Aag8F+CPjd8NdT0Y+BYfGXivWLRrPSpJtFkE9lMchJZLhkU2oj3F8u6ZAKjJbBAPUvhH4H1PwLpGswaprd5rb32qTX1vJfXb3LwQMqKkQdzkgBM\/VieSSa8e\/4J83Mc3wZ8SxI4aS38ba\/FKvdG+2u20++GU\/jXuPwq8JXfgH4Z+FvDd9e\/2je6TplvZTXWSRK8caqWGecZHGe2K8d+FOiv8AAf8AaC8ceFLn\/R\/C3ju6\/wCEk8PMpPlJebFjvrXGMI+VilAz8wlOMlWoAxP+Ck3jyfwn+zDq2i6ftfWPF93b+HbSIN+8Jnb5mQdThR+o6ZFdH461nSf2OP2M5DHbwwp4Z8Px2UNvFiPz7oxhOMcbmkLN7kmu9+KnwG8N\/GLxH4H1jxDJfmTwjqLanY21rOI4ZpioUecNpLAYyACvfOelaPxk+EHh746eAb\/wh4njnbTLtkkElq4SaGRGDJIhIIDKRkZBHtQB5T+wn8PY\/g3+zp4J8Mancq\/irUrNte1CMj955ly7SktgcbQwTJ6lD6YC\/wDBQm+t7H9j\/wCI32iVIfPsRBGZBwXZwFXocZPc\/mOteyeEfAVt4Wubi\/m1G+17WrmNYZ9W1Mxee8a5KR7Yo440UEk4RFGWJOSSa8Z\/aU0mb43eOfBnwn08NNp0V\/Br\/imWNyq29lC+6KFiOd0zgjaCCANx4IyAfRFqrR2sSvncqAHPJ6fU\/wA6qa\/4f0zxVot7o+tafa6tpV7E0FzZXsKywzRsMMrowIYEdjWhWf4g0O18TaLe6Vem5W0vImhlNndS2swUjB2SxMsiN6MjAjsaAPL\/APhjv4Ff9Ee8D\/8Aggtf\/iK8D\/Yt\/Zm+Eni34Y+JbnXPhl4S1e6t\/GGt2kc17o1vNIkMd46xx7mUkKq4AHAAAwK98\/4ZS8B\/8\/fjj\/w4XiD\/AOTqz9F\/Yy+F3hu1ktdIt\/FmlW0k0ly8Nl4612FGlkbdJIQt6AWZiSW6knJoA7fwD8DPh18K9Qub\/wAG+BvD3ha9uYvImuNH0yG2kkjyG2MyKCVyAceoFdzXDeAfgz4c+GuoXN7os\/iGSe4i8lxrHifU9Uj25B+WO6uJVVsj7ygHGRnBNdzQBxnibXrPT\/EkUGj6Vb6140a2ZI49wi8m3LIWM0wVvLjyFOMEsQNqmq2mLpPhXxXpq6\/qaX\/jbXI5IYHETkCNBveOFRuEMQwMkkbm25JO0DC8dfsveAPiJpmuxavpsx1nV3EkniW3l8vV7ZgQUNvdAb4VXHCLhMFgVIZgX\/A\/9m7wt8CI72fTLzWvEniC+VY7zxJ4pv2v9TuI1OVjMzAbUH91QozgkEigDW+MHhhfGml2WgR6tqejXupPJbJfaPqVxaXNshidmmTypFyVKpgtkAkd8V5J+x38S9W1X4f+Mfh94\/vrq88a\/Dm8fTNVup5pXubq0ZWktbsyfeLOiuAVOcx5Fes+H\/GFv4o+KF9FHY6xbpptl5EM19oF9axStK4aTbPLEsbbfJThSfvfTPj\/AMaPgz4p0z9p7wL8R\/ANokkXiCF\/C3jCNg5jFlzcRXRCkAMpiePcQf8AWqvegD2r4S+DYPCvhtZku9Xu578m4kbVdYu78hSzFAnnyOEAQqMIAOBXcVHBCltDHDGoSONQiqBgAAYArx7wDret+Fvjt4v8HeIdWm1K31i1i8Q6LJMoVQNzQXNvEM8LEEtnIAxmctnLEUAeh+FWB1TxSB1GpjP\/AIDQc\/59Ko+ItF8b3mqSS6H4r0nS9PIG22vdCe6kU45\/eLcxgjPbbx6mr3hXP9qeKfT+0x2\/6doK6KgDzz\/hG\/id\/wBD34e\/8JaX\/wCTqP8AhG\/id\/0Pfh7\/AMJaX\/5Or0OigDzz\/hG\/id\/0Pfh7\/wAJaX\/5Oo\/4Rv4nf9D34e\/8JaX\/AOTq9DooA88\/4Rv4nf8AQ9+Hv\/CWl\/8Ak6j\/AIRv4nf9D34e\/wDCWl\/+Tq9DooA88\/4Rv4nf9D34e\/8ACWl\/+TqP+Eb+J3\/Q9+Hv\/CWl\/wDk6vQ6KAPPP+Eb+J3\/AEPfh7\/wlpf\/AJOpf+Eb+J3\/AEPfh7\/wlpf\/AJOr0KigDz3\/AIRv4nf9D14e\/wDCWl\/+TqT\/AIRv4nf9D34e\/wDCWl\/+Tq9DooA88\/4Rv4nf9D34e\/8ACWl\/+TqP+Eb+J3\/Q9+Hv\/CWl\/wDk6vQ6KAPPP+Eb+J\/\/AEPfh3\/wlpf\/AJOo\/wCEb+J\/\/Q9+Hf8Awlpf\/k6vQ6KBHnv\/AAjfxO\/6Hvw9\/wCEtL\/8nUf8I38Tv+h78Pf+EtL\/APJ1ehUUDPPf+Eb+J3\/Q9+Hv\/CWl\/wDk6j\/hG\/id\/wBD34e\/8JaX\/wCTq9CooA88\/wCEb+J3\/Q9+Hv8Awlpf\/k6j\/hG\/if8A9D34d\/8ACWl\/+Tq9DooEeef8I38T\/wDoe\/Dv\/hLS\/wDydR\/wjfxP\/wCh78O\/+EtL\/wDJ1eh0UAeef8I38T\/+h78O\/wDhLS\/\/ACdR\/wAI38T\/APoe\/Dv\/AIS0v\/ydXodFAHnn\/CN\/E\/8A6Hvw7\/4S0v8A8nUf8I38T\/8Aoe\/Dv\/hLS\/8AydXodFAHnn\/CN\/E\/\/oe\/Dv8A4S0v\/wAnUf8ACN\/E\/wD6Hvw7\/wCEtL\/8nV6HRQB55\/wjfxP\/AOh78O\/+EtL\/APJ1H\/CN\/E\/\/AKHvw7\/4S0v\/AMnV6HRQB55\/wjfxP\/6Hvw7\/AOEtL\/8AJ1H\/AAjfxP8A+h78O\/8AhLS\/\/J1eh0UAeef8I38T\/wDoe\/Dv\/hLS\/wDydR\/wjfxP\/wCh78O\/+EtL\/wDJ1eh0UAeef8I38T\/+h78O\/wDhLS\/\/ACdS\/wDCN\/E7\/oe\/D3\/hLS\/\/ACdXoVFAzz3\/AIRv4nf9D34e\/wDCWl\/+TqT\/AIRv4nf9D34e\/wDCWl\/+Tq9DooA89\/4Rv4nf9D34e\/8ACWl\/+TqT\/hG\/id\/0Pfh7\/wAJaX\/5Or0OigDzz\/hG\/id\/0Pfh7\/wlpf8A5Oo\/4Rv4nf8AQ9+Hv\/CWl\/8Ak6vQ6KAPPf8AhG\/id\/0Pfh7\/AMJaX\/5Oo\/4Rv4nf9D34e\/8ACWl\/+Tq9CooA4C38O\/ElLiJp\/G+gSwKwMkaeGZEZl7gN9tOPrg139FFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABWN4s8H6P440oadrdkt7arNHcIN7RvHIjbkdHQhkYHupBwSOhIrZooAKKKKACsjw\/wCEtI8LNqD6XYx2suoXL3d3MCWknlYklndiWOOignCqAqgKABr0UAFFFFABRRRQAUUUUAFFFFABRRRQAV4n+0szeDx4N+JcKMT4U1MR3xjGWOn3RSKdRjBP7wW7YzjCEkHAx7ZWP4w8NWvjLwrq2hXqK9tqFrJbPuGcblIDD3BwQexANAFTwmwfUvFDLyramCGGMH\/RoK6OvGv2U5Na\/wCFZy2viGKSHV9Pv5NMkExy8kdsqW8Urc9ZI4kfJ5O7mvZaACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArN8SatJoHh7U9ThsbjU5bO2kuFsrRC805VSwjRQCSzYwBjqRWlRQB8jal8a\/id4f\/AGrvA\/wwtNSs9efxBoFxqOuW95bRrBocijKSQGNUd0BxmOR3LA43g\/MNT48\/FTxj8A\/iN8IbKx8WXfi6Xxl4kj0a90HU7SzihS2dW3zwPFDHIjRttxvkcEZBBPNYf7LfhnWPGv7Wfx4+KevaTe6ckV2nhfRxqNq0TPbQHmSPcB8p2r2PXr1qDxdpN7q3\/BQAeKfFtlcJ4b8EeGt3haLyWJ1G+uMLKIBgeYyhmUgZ2nBOAMgA+yKKyfCcmryeG9OfX0gj1loVN0ltny1k7hc9q1qACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooASloooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigD\/9k=)\n\nwhere error<sub>S<\/sub>(h) is the training error achieved with sample S for our model h, Z<sub>N<\/sub>=1.64 for N=90 and Z<sub>N<\/sub>=1.28 for N=80.\n\nSo, we use the average test accuracies and average lengths of the splittted sets to calculate an average of true errors on the test data.","bc8e2ef0":"### Evaluation on Train Set\nIn Naive Bayes classification, we assume the events to be independent and thus we write the probabilities of all the attributtes as a multiplication of them individually. Since we have 4 attributes in our preprocessed dataset, we will use that as our parameter for model complexity while finding the pessimistic error.","aa2a5d15":"## Transforming Eccentricity Attribute","216462e5":"## Binning","3625b29f":"### Evaluation on Test Set","b14439fb":"In the next code block, the list *ls_score* will store the area under the ROC curve while *std_score* will store the std of the area under ROC for different depths of the tree. \n\n> More the area under ROC near to 1, better is the depth.\n\n","a6fa0a1d":"### Evaluation on Test Set","afa50dee":"### Evaluation on Train Set\nSince we are taking k=5 as the number of neighbours to make comparisions for our class label, we will use that as our parameter of model complexity while finding the pessimistic error.","7d16589e":"### Identifying the Max depth required\nWe will split the data as 75% train and 25% test for this purpose.","283d81f7":"## Naive Bayes\nHere, we will train the data and then plot the test accuracies obtained for the 5 rounds of our 5x5 fold cross validation with stratified sampling for our naive bayes classifier.","e196d9fb":"## Duplicate Data\nWe first check for duplicate instances in the dataset.\nThe **duplicated()** function returns a Boolean array that indicates whether each row is a duplicate of a previous row in the table.","45e33d9d":"### Create StratifiedKFold object\nWe will use 5x5 fold validation technique for cross validation with stratified sampling for all our classification models.","9f006499":"We observe that no column contains any null or missing values.","94fb9576":"### Our Operational Function\nHere we will apply our StratifiedKFold object and return an average of training accuracies, test accuracies, and the lengths of the splittted sets.","b076f696":"### Function to find pessimistic error on train set\nWe use the average training accuracies, average lengths of the splittted sets and the parameter to calculate an average of pessimistic error on the training data where we try to give the model a penalty for its complexity. Here, parameter gives the degree of complexity of the model used for finding the pessimistic error. ","4407de3d":"### Evaluation on Train Set\nSince the complexity of our Rule based classifier depends on the number of rules that the model will have to check while finding the class label for the test data, we will use the number of rules generated by our ruleset as our parameter for finding the pessimistic error.","0f95aa52":"### Displaying the generated tree","c2b622ad":"## Desision Tree\nIn this section, we apply ID3- a decision tree classifier to the rice dataset obtained after preprocessing.","cd7a0505":"### Identifying the p value\nWe will split the data as 75% train and 25% test for this purpose.","ce683517":"We can say that data is balance as there is not much difference in the number of instances of both the classes.","10bf2936":"The boxplots suggest that the attributes: MajorAxisLength,\tEccentricity,\tEquivDiameter,\tPerimeter and\tRoundness contain abnormally high values. So, we will be removing (statistical) outliers for these attributes.\n\nAs we know boxplot uses interquartile range to plot outliers and data where IQR is the difference between 1st and 3rd quartile so we use IQR to remove outliers.","901fd0fe":"### Evaluation on Train Set\nWe can see that the generated tree has 8 leaf nodes. Thus, we use the number of leaf nodes as a measure of our model complexity while finding the pessimistic error.","938a128a":"### Operational Phase\nHere, we will train the data on our operational function using the max_depth value obtained above and then plot the test accuracies obtained for the 5 rounds of our 5x5 fold cross validation with stratified sampling for our Decision Tree classifier.","71b02e46":"# Rule Based Classification\nWe will implement this using the RIPPER algorithm\n### Installing Module\nWe will use wittgenstein - a Weka wrapper for Python.","21605259":"## Data Standarziation","71dfdd66":"## Loading Dataset\nWe have used the [Rice Seed Dataset from Kaggle](https:\/\/www.kaggle.com\/seymasa\/rice-dataset-gonenjasmine), which was extracted from two kinds of rice (Gonen, Jasmine).\n\n|                          |        |\n|--------------------------|:------:|\n| DataSet Type  | Continuous |\n| Associated Task  | Classification |\n\n","4a5cef5b":"# Data Quality\n## Missing Values\nWe will count the number of missing values in each column of the data.","3574ddf0":"All the attributes are continuous real valued while the class is binary (categorical).","ffbb9a49":"# Classification with KNN\nWe will take k=5 for this assignment and use Minkowski distance as our metric.","ec4a20c9":"### Operational Phase\nHere, we will train the data on our operational function using the ballowance value obtained above and then plot the test accuracies obtained for the 5 rounds of our 5x5 fold cross validation with stratified sampling for our RIPPER classifier, with the positive class defined as Jasmine.","15147f5f":"The results suggest that there are **no duplicate rows** in the online retail dataset. So, we don't need to remove any row.","5697d20b":"## Comparing Classifiers\nHere is a comparison for how our various models compare, scored by accuracy:\n\n| Model | Accuracy Score |\n|--------------------------|:------:|\n| Rule Based (RIPPER)  | 100.0 |\n| (Gaussian) Naive Bayes  | 98.78 |\n| 5 Nearest Neighbour  | 98.73 |\n| Decision Tree  | 98.61 |\n\nWe can see that besides Ripper, all the other models give almost similar accuracy score. So, no definite comparision can be made about them. Ripper gives an expceptionally high score of 100, which means either it has been able to find a perfect set of rules or some missteps led to this.\n\nWe can still make a relative comparision on these models based on their accuracy score as: **Rule Based (RIPPER) > Naive Bayes > 5 Nearest Neighbour > Decision Tree**. All these accuracy values are acceptable and thus the models perform well.\n\nBased on their true error scores, we can say that besides RIPPER, Naive Bayes has the least true error for both the considered intervals followed by KNN. Decision Tree has a relatively high true error rate. This point to the same ordering of preference as above.","6ab646a3":"## Outliers\nWe will draw a boxplot to identify the columns in the table that contain outliers."}}