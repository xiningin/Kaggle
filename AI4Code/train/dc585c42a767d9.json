{"cell_type":{"9b809b7f":"code","434919ed":"code","dfa9e642":"code","ab2c0b9e":"code","765b7b5c":"code","e39ab01b":"code","67c022e7":"code","87671531":"code","ad147274":"code","1f5b4f46":"code","b83952e1":"code","208fad12":"code","d0564e19":"code","e465e734":"code","d93ca4dc":"code","ffcc94ed":"code","eccd5040":"code","cad7dda3":"code","5a5c0eef":"code","b3b7d140":"code","7577f6b1":"code","7c48e55b":"code","1e73fc0e":"code","3d231e03":"code","215cbd6b":"code","da9a5a22":"code","370e0383":"code","85e893b0":"code","36a0b721":"code","4f0bd452":"code","bfb39618":"code","4a1f7009":"code","a689e848":"code","c0a17ed2":"code","60e606e7":"code","4a66f5c7":"code","c4ad6916":"code","f37499ab":"code","ff94ee05":"code","f500ae21":"code","0a5cafae":"code","0be7c1c6":"code","a24dd1b9":"code","ba524b0b":"code","7c8c7031":"code","b705545e":"markdown","f6b9f5a8":"markdown","95036d23":"markdown","c0fdf33a":"markdown","e3680f91":"markdown","8f925180":"markdown","3c0943f9":"markdown","8bdfb6d3":"markdown","7e490697":"markdown","85ce5340":"markdown","4e5bccbb":"markdown","b2941486":"markdown","06c45b88":"markdown","4f692b61":"markdown","e5921ae4":"markdown","14f8ec88":"markdown","480c1bfa":"markdown","f4c5a96c":"markdown","453430d4":"markdown","4c4c92ba":"markdown","6fe967c8":"markdown","53e499e4":"markdown","8725d687":"markdown","d8c5b140":"markdown","2cbec284":"markdown"},"source":{"9b809b7f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#association rules\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\n\n#clustering\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\n\nfrom IPython.display import display","434919ed":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","dfa9e642":"heroes_info = pd.read_csv('\/kaggle\/input\/superhero-set\/heroes_information.csv')\nsuper_powers = pd.read_csv('\/kaggle\/input\/superhero-set\/super_hero_powers.csv')","ab2c0b9e":"heroes_info.columns\n# heroes_info.head()","765b7b5c":"heroes_info = heroes_info.drop('Unnamed: 0', axis=1)\nheroes_info.head()","e39ab01b":"super_powers.head()","67c022e7":"# Change the column name so that it can be used as id for datasets merge\ncolumn_names = super_powers.columns.values\ncolumn_names[0] = 'name'\nsuper_powers.columns = column_names","87671531":"# Encoding of nulls and surely incorrect data\nheroes_info['Weight'].replace(-99.0, np.nan, inplace=True)\nheroes_info['Height'].replace(-99.0, np.nan, inplace=True)\nheroes_info.replace('-', np.nan,inplace=True)","ad147274":"heroes_info.info()","1f5b4f46":"heroes_info = heroes_info.drop('Skin color', axis=1)","b83952e1":"data = pd.merge(heroes_info, super_powers, on='name')\ndata.info()","208fad12":"print(data.isna().sum()['Weight'])\nprint(data.isna().sum()['Height'])","d0564e19":"data['Height'] = data['Height'].fillna(data.groupby(['Race','Gender'])['Height'].transform('mean'))\ndata['Weight'] = data['Weight'].fillna(data.groupby(['Race','Gender'])['Weight'].transform('mean'))","e465e734":"print(data.isna().sum()['Weight'])\nprint(data.isna().sum()['Height'])","d93ca4dc":"data[data['Height'].isna()].head()\n# data[data['Weight'].isna()]","ffcc94ed":"# data[data.isnull().sum(axis=1) < 3].count()","eccd5040":"data.dropna(inplace=True)","cad7dda3":"data.head()","5a5c0eef":"data.info()","b3b7d140":"data = pd.get_dummies(data, columns=['Gender', 'Eye color', 'Race',\n                                     'Hair color','Publisher','Alignment'],\n                      drop_first=True)\ndata.replace(0, False, inplace=True)\ndata.replace(1, True, inplace=True)","7577f6b1":"data.head()","7c48e55b":"corr_matrix = data.corr().abs()\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\ntri_df = corr_matrix.mask(mask)","1e73fc0e":"corr_matrix","3d231e03":"# corr_matrix = corr_matrix.stack()\npower_cols = data.columns[3:168].tolist()","215cbd6b":"for idx, x in corr_matrix[tri_df > 0.5].stack().sort_values(ascending=False).iteritems():\n    if ((idx[1] in power_cols) & (idx[0] not in power_cols)):\n     print(idx,x)","da9a5a22":"data = pd.read_csv('\/kaggle\/input\/superhero-set\/super_hero_powers.csv')","370e0383":"data.head()","85e893b0":"frequent_itemsets = apriori(data.drop('hero_names',axis=1), min_support=0.05, use_colnames=True)","36a0b721":"rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)","4f0bd452":"# type(rules)\nrules.sort_values(by='confidence', ascending=False).head()","bfb39618":"rules.sort_values(by='lift', ascending=False).head()","4a1f7009":"rules.sort_values(by='leverage', ascending=False).head()","a689e848":"corr = data.corr()\nplt.figure(figsize=(12,8))\nsns.heatmap(corr, cmap='Blues', center=0,linewidths=.1)","c0a17ed2":"data.corr().unstack().sort_values(ascending=False).drop_duplicates().head(20)","60e606e7":"data.corr().unstack().sort_values(ascending=False).drop_duplicates().tail()","4a66f5c7":"squared_dist_sum = []\nk_list = range(1,60)\nfor k in k_list:\n    km_model = KMeans(n_clusters=k)\n    km_model = km_model.fit(data.drop('hero_names', axis=1))\n    squared_dist_sum.append(km_model.inertia_)","c4ad6916":"plt.plot(k_list, squared_dist_sum)\nplt.xlabel('k')\nplt.ylabel('Suma odleg\u0142o\u015bci')\nplt.show()","f37499ab":"km_model = KMeans(16)\nkm_model = km_model.fit(data.drop('hero_names',axis=1))","ff94ee05":"clusters = km_model.predict(data.drop('hero_names',axis=1))\ndata['Cluster'] = clusters\ndata[data['Cluster']==0].head()","f500ae21":"np.random.seed(1)\ntsne_clusters=TSNE(perplexity=27, verbose=1)\ntsne_results=tsne_clusters.fit_transform(data.drop('hero_names', axis=1))\nplt.figure(figsize=(16,16))\nplt.scatter(tsne_results[:,0], tsne_results[:,1], c=km_model.labels_, cmap='plasma')","0a5cafae":"data['Cluster'].value_counts().plot.pie(figsize=(10,10),autopct='%1.f%%')","0be7c1c6":"for i in range(data['Cluster'].nunique()):\n    print(\"Cluster no: {}\".format(i))\n    \n    group_members= data[data['Cluster']==i]\n    group_members_count = data[data['Cluster']==i].count()[0]\n    \n    print(\"Representatives: \", end=' ')\n    \n    reprs_count=5\n    if group_members_count < 5:\n        reprs_count = group_members_count\n        \n    for j in range(reprs_count):\n        print(group_members.iloc[j]['hero_names'], end='; ')\n    print('')\n    \n    super_powers = data[data['Cluster']==i].drop(['hero_names','Cluster'],axis=1).sum().to_dict()\n    super_powers = sorted(super_powers.items(), key=lambda x:x[1], reverse=True)\n    \n    print('Most common skills with percentage of occurance:')\n    for super_power in super_powers[:5]:\n        print(super_power[0] + ' --> ' + format((super_power[1]\/group_members_count), '.0%'))\n    print('##############################################################################')","a24dd1b9":"data['powers_count'] = data.sum(axis=1)","ba524b0b":"data[data['Cluster']==2]['powers_count'].mean()","7c8c7031":"data[data['Cluster']!=2]['powers_count'].mean()","b705545e":"### Pearson Correlation","f6b9f5a8":"### Clustering","95036d23":"#### t-SNE visualisation","c0fdf33a":"As you can see, on average, super heroes from group 2 have less than 5 superpowers, while in the case of the rest the average is over 17 superpowers. This was probably the main reason behind the formation of such a large group of rather not similar superheroes.","e3680f91":"#### Sorting by *lift*","8f925180":"Size of individual groups.","3c0943f9":"#### Sorting by  *confidence*","8bdfb6d3":"The *Elbow method* does not clearly indicate the best number of clusters - you cannot see a clear bend point in the graph, therefore I decided to choose 16 as the number of clusters. Should a problem indicate the need for more homogeneous groups, the number of clusters should be increased.\n","7e490697":"You can see from the above printout that most of the groups have been correctly identified on the basis of the dominant superpowers of their members. Many groups were distinguished very precisely.\n\nGroup 2 definitely stands out from the rest of the group, with 27% of superheroes who could not be classified into the other groups.","85ce5340":"In the above graph, darker areas can be seen in spots, which indicates the correlation of some superpowers, but due to their number, it is unreadable and does not contain all the variables.","4e5bccbb":"# 2. Can we cluster the superheroes based on their superpowers only?","b2941486":"For every cluster:\n5 representatives\n5 most common skills + percentage of occurance","06c45b88":"#### Cluster 2","4f692b61":"#### Converting categorical variables to binary ones","e5921ae4":"Missing data still exists, and as can be seen most of the heros don't have the big part of the information. Having that in mind, I decided to drop those rows and analyse only the heroes with full information.","14f8ec88":"## 1. Checking whether there is any relationship between information about superheroes and superpowers they have","480c1bfa":"#### Fill in the missing data on height and weight - done based on the mean value of race and gender of the specific hero","f4c5a96c":"Variable *Skin color* is deleted, as it contains too many null values.","453430d4":"### Association rules induction","4c4c92ba":"#### Conclusion\n\nThe number of superheroes is relatively small compared to the number of superpowers, making the *support* of some of them, and therefore of the sets containing them, very low. This makes it difficult to draw meaningful conclusions, assuming that the available data would only be a sample, and we would like to generalize them for the entire population. This can be seen, for example, by the values in the *leverage* column, which are very low (around 0.05) and at the same time they have very high *lift* ratio, which promotes strong connections even with little support.\n\nHowever, assuming that the data you have is the entire superhero population, you can draw many conclusions with 100% certainty - for example, many sets of superpowers clearly indicate the simultaneous occurrence of *Super Strength*, and if you sort the rules by the *lift* coefficient, you can see that it has a high value for different sets of superpowers.\n\nTo sum up, association rules confirm the dependencies in the occurrence of individual superpowers, but due to the properties of the data (a large number of parameters, a small number of observations), I believe that they should be viewed with a certain distance.","6fe967c8":"#### Sorting by *leverage*","53e499e4":"### Correlation","8725d687":"The study of the correlation value shows that a large proportion of the superpowers are positively correlated to some extent. In the case of several superpowers visible above, this is a strong correlation, but moderate and low relationships are much more common. Negative correlations are very weak or absent.\n\nI believe that correlation analysis, like the analysis of association rules, confirms the existence of dependencies and provides the premisis that we are able to group superheroes according to their superpowers.","d8c5b140":"### Data preparation","2cbec284":"As you can see above,the correlation between the information about the superhero and their superpower exist - especially it's seeable when it comes to the race of the hero."}}