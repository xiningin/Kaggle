{"cell_type":{"0ff0b9e7":"code","ba7364a8":"code","82e7cc8d":"code","ad56945a":"code","b750bdf6":"code","3c327728":"code","289a4bba":"code","a5573b5b":"code","cd2e5bae":"code","5062dc4d":"code","df1f0614":"code","2d0bcf39":"code","a677030b":"code","93605584":"code","f43232ad":"markdown","bc1d7f02":"markdown","64395e25":"markdown","9bb7f2ca":"markdown","f8c44411":"markdown","ff8c5c08":"markdown","f7382ae5":"markdown","093e80ab":"markdown","c2c89d32":"markdown","cb142423":"markdown","28a1914b":"markdown","4e55ac40":"markdown"},"source":{"0ff0b9e7":"import numpy as np\nimport scipy as sp\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot as plt","ba7364a8":"X, y = datasets.load_iris(return_X_y=True)","82e7cc8d":"print(X[:5, :])","ad56945a":"EX = np.mean(X, axis=0).reshape(1, -1)\nprint(EX)","b750bdf6":"X_zero_mean = X - EX\nprint(X_zero_mean[:5, :])","3c327728":"C = np.dot(X_zero_mean.T, X_zero_mean) \/ (len(X_zero_mean) - 1)\nprint(C)","289a4bba":"C = np.cov(X, rowvar=False)\nprint(C)","a5573b5b":"eig_vals, eig_vecs = np.linalg.eig(C)\nC_prime = np.diag(eig_vals)\nprint(\"C' = \")\nprint(C_prime)","cd2e5bae":"print(\"trace(C) =\", np.trace(C))\nprint(\"trace(C') =\", np.trace(C_prime))","5062dc4d":"two_max_component_idx = np.argsort(eig_vals)[-2:]\ntwo_max_components = eig_vecs[:, list(reversed(two_max_component_idx))]\nprint(\"Transformation matrix =\\n\", two_max_components)","df1f0614":"reduced_X = np.dot(X_zero_mean, two_max_components)","2d0bcf39":"fig, ax = plt.subplots(figsize=(15, 8))\nax.scatter(reduced_X[:, 0], reduced_X[:, 1], c=y, cmap='tab10')\nplt.show()","a677030b":"pca = PCA(n_components=2)\nX_2_comp = pca.fit_transform(X)","93605584":"fig, ax = plt.subplots(figsize=(15, 8))\nax.scatter(X_2_comp[:, 0], X_2_comp[:, 1], c=y, cmap='tab10')\nplt.show()","f43232ad":"Here, each column represents a feature and each row represents an observation. The target is not important for the PCA so we will not print it.\n\nLet's calculate the mean of each column. The result will be a vector of 4 elements since we have 4 features and subtract it from each observation.","bc1d7f02":"# Validation","64395e25":"# Introduction\n\nThis document is derived from the following video: https:\/\/youtu.be\/1cDSlY5Q-Sw\n\n<a href=\"http:\/\/www.youtube.com\/watch?feature=player_embedded&v=1cDSlY5Q-Sw\" target=\"_blank\"><img src=\"http:\/\/i3.ytimg.com\/vi\/1cDSlY5Q-Sw\/maxresdefault.jpg\" \nalt=\"Machine Intelligence - Lecture 3 (PCA, AI and Data)\"\/><\/a>\n\n- **Principal Component Analysis (Main Features Selection)** which components(=features) are important to keep?\n- Significance = variance\n- Intelligence = recognizing the significance\n\nStarting point is a file with a table:\n\n|  #  | $X_1$ | $X_2$ | $X_3$ | $X_4$ | ... | $X_n$ |\n| --- | ----- | ----- | ----- | ----- | --- | ----- |\n|  1  |       |       |       |       |     |       |\n|  2  |       |       |       |       |     |       |\n|  3  |       |       |       |       |     |       |\n| ... |       |       |       |       |     |       |\n|  m  |       |       |       |       |     |       |\n\nYou need data of some sort (e.g. csv, excel, etc.). The columns are called as *features* and rows are called as *observations*. If some of the columns are not changing, why should I use that feature?\n\nHow to calculate variance?\n\n$$\n\\sigma^2 = \\frac{1}{n - 1} X X^T \\\\\nn = \\mid X \\mid\n$$\n\nThis can be computed only if the $X$ has **zero mean**. Thus, you should subtract the mean from $\\boldsymbol{X}$.\n\n$$\nX = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}, \nX^T = \\begin{bmatrix}\n1 & 2 & 3\n\\end{bmatrix} \\\\\n\\sigma^2 = \\frac{1}{3} [1 + 4 + 9]\n$$\n\nLet's first get a simple dataset from scikit-learn to test the idea. First, we should import the libraries that we want to use throughout the notebook.","9bb7f2ca":"If we print the first 5 observations of $X$, you will see the following table.","f8c44411":"Apply the chosen transformation matrix to the data.","ff8c5c08":"Numpy also provides a function to calculate the covariance matrix. We can validate our result with this function.","f7382ae5":"In an orthogonal transformation, the *trace* of a matrix remains the same.\n\n$$\ntrace(C) = trace(C') = \\sum_{i=1}^N \\lambda_i \\\\\n= \\sum_{i=1}^N \\sigma_i^2\n$$","093e80ab":"# Covariance Matrix Calculation\n\nAs you can see, we create a new variable called *X_zero_mean* which is the version of X with the subtracted *Expected Value*. At this point, we can define a covariance matrix $\\Sigma$ which includes all the variances between all possible combinations of features.\n\n$$\n\\Sigma = E[(X - E[X]) \\cdot (X - E[X])^T] \\\\\nC = E[X X^T]\n$$\n\n$$\n\\sigma^2 = var(X) = E[(X - E[X])^2]\n$$\n\nIs the variance and covariance the same? The answer is: Fundametally yes. They are the same things. They try to measure the cange. However, variance is 1-dimensional and covariance is 2-dimensional. In covariance, the question is does the $X_3$ change while $X_2$ is changing? In varaince, the question is does the $X_3$ change?\n\nSuppose that you have only 3 features and you calculate the covariance matrix as follows.\n\n$$\n\\Sigma = X X^T\n$$\n\nThen,\n\n$$\n\\Sigma = \\begin{bmatrix}\nx_1^2 & x_1x_2 & x_1x_3 \\\\\nx_2x_1 & x_2^2 & x_2x_3 \\\\\nx_3x_1 & x_3x_2 & x_3^2\n\\end{bmatrix}\n$$\n\nThe resulting matrix is completely symmetric. The diagonal is the variance and everything else is measuring the covariance.\n\nNow, let's calculate the same operation by hand with the actual data.","c2c89d32":"We will call the mean as *Expected Value* ($E[X]$) from since. Because we may not have the full population data. Thus, calling it as the mean is a bold move. If we increase the number of observations at hand, we expect that the *Expected Value* should approach to the population mean.","cb142423":"As you can see, they are almost same. The difference between them is most certainly negligable. The largest *eigenvalue* is the most important and the smallest one is the least important.\n\n$$\nC'=\\begin{bmatrix}\n\\lambda_1 & 0 & 0 \\\\\n0 & \\lambda_2 & 0 \\\\\n0 & 0 & \\lambda_n \\\\\n\\end{bmatrix}\n$$\n\nHow the diagonal is sorted? Because we just started to calculate from the first components. Naturally, it computed as sorted.\n\n# Principal Components\n\n$\\lambda_1, \\lambda_2, \\lambda_3, ..., \\lambda_{n-2}, \\lambda_{n-1}, \\lambda_n$\n\nImportant <------------> Useless\n\nPick $N' << N$. $N'$ should be much smaller than $N$.\n\nThis process is called *dimensionality reduction*.\n\nPCA is:\n- a linear transformation\n- unsupervised\n- uses statistics and calculus\n- a dimensionality reduction algorithm\n- a visualization algorithm (extremely important)\n- intelligent (because it recognizes significance)","28a1914b":"Now, let's download the dataset and store the features in the $X$ matrix and the target observations in the $y$ vector.","4e55ac40":"As you can see, our result and the Numpy's are completely same. One thing to note is, Numpy assumes that the observations are in the columns and features are in the rows. Because of this, we need to pass the keyword argument *rowvar* as *False*.\n\n> Principal Componenets = Significant things that change.\n\nIf you do not normalize your data by subtracting the *Expected Value*, you cannot trust the first significant component of your data. Because it may shift itself to the average.\n\n$$\nC = E[X X^T]\n$$\n\n> Diagonalizing $C$ using a suitable **orthogonal** transformation matrix $A$ by obtaining $N$ **orthogonal** *special vectors* $u_i$ with *special parameters* $\\lambda_i$.\n\n**Orthogonal** means *perpendicular*. If your matrices are multidimensional and perpendicular to each other, then we call them as orthogonal.\n\n$A^{-1} = A^T$ for orthogonal matrices.\n\nWe want to decrease the redundancy. If two vectors are perpendicular to each they are independant. On the other hand, if the angle between two vectors are less than 90 degrees, then there are some reduntant information between them. Thus, we have to look orthogonal principal components.\n\n$$\nC u_i = \\lambda u_i\n$$\n\nWe can prove that such a vector and a scalar exist.\n\n$$\n\\begin{bmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix} = \n3 \\cdot \\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix} = \n\\begin{bmatrix}\n3 \\\\\n3\n\\end{bmatrix} \\\\\nC \\cdot u_i = \\lambda \\cdot u_i\n$$\n\n$u_i$ is called *eigenvector* and $\\lambda$ is called *eigenvalue*. Here another example:\n\n$$\n\\begin{bmatrix}\n2 & 3 \\\\\n2 & 1\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\n6 \\\\\n4\n\\end{bmatrix} = \n4 \\cdot \\begin{bmatrix}\n6 \\\\\n4\n\\end{bmatrix} = \\begin{bmatrix}\n24 \\\\\n16\n\\end{bmatrix}\n$$\n\nHere, $\\big[\\begin{smallmatrix}\n6\\\\\n4\n\\end{smallmatrix}\\big]$ is the *eigenvector* $u_i$ and $4$ is the *eigenvalue* $\\lambda$.\n\n## Linear Transformation\n\n$$\nu_i = A (X_i - m) \\\\\nX_i = m + A^T u_i\n$$\n\nSince $A^{-1} = A^T$ for orthogonal matrices, so $C' = A C A^T$ such that $C'=\\big[\\begin{smallmatrix}\n\\lambda_1 & 0 & 0 \\\\\n0 & \\lambda_2 & 0 \\\\\n0 & 0 & \\lambda_n \\\\\n\\end{smallmatrix}\\big]$."}}