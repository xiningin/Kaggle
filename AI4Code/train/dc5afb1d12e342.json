{"cell_type":{"6d0c7d95":"code","fc6abbea":"code","4d192246":"code","4a31e7a7":"code","7f0723dd":"code","7c6bb1c6":"code","979df197":"code","35284401":"code","d74e88f8":"code","0073e709":"code","7db6a85e":"code","5ce43980":"code","96aa90c9":"code","bd5509f3":"code","caacd79b":"code","546897fd":"code","ce733203":"code","e09c9f2e":"code","a91ec12e":"code","b5e87af2":"code","4e86b828":"code","8f513a92":"code","156e3fce":"code","f32e0986":"code","3126b3b7":"code","8cb6dde4":"code","de18bfc4":"code","97d0b4c2":"code","99f87f38":"code","724234fb":"code","40c2d315":"code","6074ed07":"code","59ef814d":"code","68d7537c":"markdown"},"source":{"6d0c7d95":"## Importing Libraries\n    \n","fc6abbea":"\nimport pandas as pd\nimport numpy as np","4d192246":"\n\nimport nltk\n#nltk.download('')","4a31e7a7":"from nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import defaultdict\nfrom nltk.corpus import wordnet as wn\n\n","7f0723dd":"\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import model_selection,naive_bayes,svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\n","7c6bb1c6":"corpus = pd.read_csv('..\/input\/sms-spam\/sms_spam.csv')","979df197":"\ncorpus.head()","35284401":"corpus.shape","d74e88f8":"corpus['type'].value_counts()","0073e709":"corpus['text'] = [entry.lower() for entry in corpus['text']]","7db6a85e":"nltk.download('punkt')","5ce43980":"corpus['text'] = [word_tokenize(entry) for entry in corpus['text']]","96aa90c9":"corpus['text'].head()","bd5509f3":"nltk.download('wordnet')","caacd79b":"tag_map = defaultdict(lambda : wn.NOUN)\ntag_map['j'] = wn.ADJ\ntag_map['v'] = wn.VERB\ntag_map['v'] = wn.ADV","546897fd":"nltk.download('stopwords')","ce733203":"stopwords.words('english')","e09c9f2e":"nltk.download('averaged_perceptron_tagger')","a91ec12e":"for index,entry in enumerate(corpus['text']):\n    Final_words = []\n    word_lemmstized = WordNetLemmatizer()\n    for word, tag in pos_tag(entry):\n        if word not in stopwords.words('english') and word.isalpha():\n            word_final = word_lemmstized.lemmatize(word,tag_map[tag[0]])\n            Final_words.append(word_final)\n    corpus.loc[index,'text_final'] = str(Final_words)\n\n","b5e87af2":"corpus.head()","4e86b828":"get_ipython().system('pip install wordcloud')","8f513a92":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt","156e3fce":"comment_words = ' '\nstopwords = set(STOPWORDS)","f32e0986":"for val in corpus.text_final:\n    val = str(val)\n    tokens = val.split()\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n    for words in tokens:\n        comment_words = comment_words + words + ' '\n","3126b3b7":"wordcloud = WordCloud(width = 1000, height =1000, background_color = 'white',\n                      stopwords = stopwords,min_font_size = 10).generate(comment_words)\n\n\nplt.figure(figsize = (4,4), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()\n","8cb6dde4":"Train_X,Test_X,Train_Y,Test_Y = model_selection.train_test_split(corpus['text_final'],corpus['type'],test_size = 0.3)\nencoder = LabelEncoder()\nTrain_Y = encoder.fit_transform(Train_Y)\nTest_Y = encoder.fit_transform(Test_Y)","de18bfc4":"y = Train_Y.tolist()\n#y","97d0b4c2":"Tfidf_vect = TfidfVectorizer(max_features = 5000)\nTfidf_vect.fit(corpus['text_final'])\nTrain_X_Tfidf = Tfidf_vect.transform(Train_X)\nTest_X_Tfidf = Tfidf_vect.transform(Test_X)\n","99f87f38":"data = Train_X_Tfidf.toarray()","724234fb":"data","40c2d315":"naive = naive_bayes.MultinomialNB()\nnaive.fit(Train_X_Tfidf,Train_Y)","6074ed07":"\npredictions_NB = naive.predict(Test_X_Tfidf)\n","59ef814d":"print(\"Naive Bayes Model Accuracy : \",accuracy_score(predictions_NB,Test_Y)*100)\n","68d7537c":"## Accuracy 97%"}}