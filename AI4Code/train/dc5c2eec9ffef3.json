{"cell_type":{"36b8d753":"code","a6503f1c":"code","25bf5997":"code","a68191d8":"code","03cc7e1e":"code","5d9a33d8":"code","5a51dbe7":"code","00afffeb":"code","af1ba8fc":"code","d7643d1d":"code","f8ac5546":"code","b7ebfbf3":"code","d43cbe3c":"code","3e51cfb5":"code","2afb3c49":"code","a4f41016":"code","28865a9d":"code","7de9fb2f":"code","3625cbd9":"code","111001d7":"code","aee2e4dd":"code","86644c50":"code","79406036":"code","25ed25cb":"code","90ab6e04":"code","9df12ff9":"code","c7765876":"code","9f67a555":"code","c1b56376":"code","2bf8f3ad":"code","726f40fe":"code","02465b74":"code","0093b497":"code","d52f5e61":"code","6199f5ea":"code","b71f81f9":"code","2ff23f47":"code","d004ac08":"code","825ed9a5":"code","1bd287c7":"code","d7aec6bf":"code","c31daa7e":"code","c405503c":"code","e0501ce2":"code","8277e163":"code","1f0015e5":"code","c456cc2e":"code","7fdbfd44":"code","3d826036":"code","7391acd8":"code","ebcb1d4a":"code","5626b28a":"code","75052cff":"code","25d7fe6f":"code","3c4bcaec":"code","86fc5999":"code","7b62b07d":"code","df905808":"code","08771e60":"code","d635d198":"code","91d9d2f5":"code","d9bdd429":"code","dac80631":"code","e7591536":"code","0180c051":"code","7342ccc8":"code","1494062f":"code","5fc3806b":"code","e8905278":"code","e6a9041c":"code","06210b4c":"code","07426956":"code","d8f26b56":"code","92f966ca":"code","802909b4":"code","54247d91":"code","b31466b4":"code","b7aa4524":"code","20f386fe":"code","3c90e995":"code","4e8f905b":"code","849ff043":"code","1170cdfa":"code","bcb9cc3f":"code","cdc027d9":"code","ca5dccaf":"code","d42dc66f":"code","92715368":"code","2a2a17d1":"code","71309807":"code","56e6b557":"code","675b7b8e":"code","d3b26be7":"code","94bf944f":"code","def53271":"code","b5012e4d":"code","83b39f9b":"code","eb078baf":"code","5591b0c1":"code","b7799d35":"code","ff875e47":"code","1ed4f2c0":"code","b77d1dc6":"code","7f3580aa":"code","7b77b66b":"code","e0279156":"code","43338279":"code","15bdcd1a":"code","c46b18c7":"code","83db2855":"code","76a7fa7b":"code","a0e68982":"code","b3f057ee":"code","d10eedc4":"code","421a7627":"code","41935881":"code","1435545b":"code","c1b05f12":"code","bbe1c756":"code","fda289d8":"code","300bc8d4":"code","d0e06377":"code","fde2be15":"code","f87c4642":"code","dea00092":"code","52caa75b":"code","1c484d1d":"code","5d5e729d":"code","8e060055":"code","359035d0":"code","3f7f6c73":"code","d2bf08ac":"code","19d79c22":"code","4b1fd1db":"code","de0b4a10":"code","7f0091a2":"code","397b94fa":"code","0fe65908":"code","23915445":"code","882041ef":"code","ec250db1":"code","16912e56":"code","d727257c":"code","4c509f56":"code","2579a41a":"code","da8e49aa":"code","7cd8da1e":"code","b47b72c8":"code","7aa28a39":"code","83630ab6":"code","91303a9a":"code","5b2dccb7":"code","71dbd013":"code","84d33c1b":"code","85942f77":"code","78718752":"code","a085dbb9":"code","4da4cfb3":"code","38b6ee9a":"code","749046e6":"code","fe1b6dca":"code","14057b1b":"code","68cfc62b":"code","92333148":"code","acbae4a4":"code","aa68ab79":"code","b997764c":"code","40f4d025":"code","65f59489":"code","f9569aa6":"code","2a786508":"code","5db70789":"code","9313f9aa":"code","4ae4947f":"code","5a35e95c":"code","2b0f667c":"code","e68b8152":"markdown","0429f520":"markdown","612f01ee":"markdown","920b45c3":"markdown","611bbfda":"markdown","96d91e42":"markdown","75cbe9cd":"markdown","88c02b13":"markdown","ec7f4983":"markdown","db965013":"markdown","d6ce961c":"markdown","37aa3531":"markdown","4a34ceda":"markdown","aa6c2406":"markdown","f024ca6c":"markdown","674bf216":"markdown","5f1e224e":"markdown","6d08c4bc":"markdown","c8a76242":"markdown","3cb8887d":"markdown","3f2a8ffe":"markdown","b282161e":"markdown","f3cc5837":"markdown","980070b3":"markdown","6dc4a633":"markdown","3cd3c153":"markdown","f987fb11":"markdown","aca98b20":"markdown","f0ed7afb":"markdown","4852639a":"markdown","ef83acbd":"markdown","389e82bc":"markdown","4dc7244a":"markdown","c2d18203":"markdown","f9cc1fd1":"markdown","4e84c2af":"markdown","3dc9a43b":"markdown","ed1ca181":"markdown","20499f7d":"markdown","5605f328":"markdown","5ccff6fc":"markdown","2c33de3d":"markdown","7b550637":"markdown","718460bf":"markdown","bd08f874":"markdown","5e318501":"markdown","8be89143":"markdown","7a02acf4":"markdown","75da674f":"markdown","e8616a76":"markdown","fa562bc8":"markdown","f514f006":"markdown","df82c6cd":"markdown"},"source":{"36b8d753":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a6503f1c":"# Standard imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport os","25bf5997":"#reading train data\ntrain=pd.read_csv(\"\/kaggle\/input\/machine-learning-24-hrs-hackathon\/train_SJC.csv\", header=1)\n#reading test data\ntest=pd.read_csv(\"\/kaggle\/input\/machine-learning-24-hrs-hackathon\/Test_SJC.csv\")","a68191d8":"# rename the columns \n\ntrain = train.rename(columns={\"Unnamed: 2\":\"DateReported\", \n                                    \"Unnamed: 7\":\"DependentsOther\", \n                                    \"Unnamed: 11\": \"DaysWorkedPerWeek\"})\ntrain.columns","03cc7e1e":"# Size of the data - rows and columns\ntrain.shape","5d9a33d8":"# Information about the data\ntrain.info()","5a51dbe7":"# First five rows of the data.\ntrain.head()","00afffeb":"train.info()","af1ba8fc":"train['Age'] = pd.to_numeric(train['Age'])\ntrain['DependentChildren'] = pd.to_numeric(train['DependentChildren'])\ntrain['WeeklyWages'] = pd.to_numeric(train['WeeklyWages'])\ntrain['HoursWorkedPerWeek'] = pd.to_numeric(train['HoursWorkedPerWeek'])\ntrain['DaysWorkedPerWeek'] = pd.to_numeric(train['DaysWorkedPerWeek'])\ntrain['InitialIncurredCalimsCost'] = pd.to_numeric(train['InitialIncurredCalimsCost'])\ntrain['UltimateIncurredClaimCost'] = pd.to_numeric(train['UltimateIncurredClaimCost'])","d7643d1d":"train.info()","f8ac5546":"train.shape","b7ebfbf3":"train.drop_duplicates().shape","d43cbe3c":"# While checking for missing values, we find there are missing values\ntrain.isnull().sum()","3e51cfb5":"# Imputaton for numerical values\nfor i in [\"WeeklyWages\"]:\n    train.loc[train.loc[:,i].isnull(),i]=train.loc[:,i].mean()\nfor i in [\"HoursWorkedPerWeek\"]:\n    train.loc[train.loc[:,i].isnull(),i]=train.loc[:,i].mean()","2afb3c49":"# Dropping rows of categorical object\ntrain=train.dropna()","a4f41016":"# Verifying that the records with missing values have been removed.\ntrain.isnull().sum()","28865a9d":"train.shape","7de9fb2f":"train.info()","3625cbd9":"# Copying train dataframe to test separately later\ntrain_orig=train.copy()","111001d7":"# train2 is the second model with all outliers deleted.\ntrain2=train.copy()","aee2e4dd":"train.plot.box(figsize=(15,6))","86644c50":"# Age outlier check\nsns.boxplot(data=train,x='Age')","79406036":"# DendentChildren outlier check\nsns.boxplot(data=train,x='DependentChildren')","25ed25cb":"# DependentsOther outlier check\nsns.boxplot(data=train,x='DependentsOther')","90ab6e04":"# WeeklyWages outlier check\nsns.boxplot(data=train,x='WeeklyWages')","9df12ff9":"# HoursWorkedPerWeek outlier check\nsns.boxplot(data=train,x='HoursWorkedPerWeek')","c7765876":"# DaysWorkedPerWeek outlier check\nsns.boxplot(data=train,x='DaysWorkedPerWeek')","9f67a555":"# InitialIncurredCalimsCost outlier check\nsns.boxplot(data=train,x='InitialIncurredCalimsCost')","c1b56376":"# UltimateIncurredClaimCost outlier check\nsns.boxplot(data=train,x='UltimateIncurredClaimCost')","2bf8f3ad":"# Replacing outliers with nan in HoursWorkedPerWeek\nfor x in ['HoursWorkedPerWeek']:\n    q75,q25 = np.percentile(train.loc[:,x],[75,25])\n    IQR = q75-q25\n \n    max = q75+(1.5*IQR)\n    min = q25-(1.5*IQR)\n \n    train.loc[train[x] < min,x] = np.nan\n    train.loc[train[x] > max,x] = np.nan","726f40fe":"# Replacing outliers with nan in InitialIncurredCalimsCost\nfor x in ['InitialIncurredCalimsCost']:\n    q75,q25 = np.percentile(train.loc[:,x],[75,25])\n    IQR = q75-q25\n \n    max = q75+(1.5*IQR)\n    min = q25-(1.5*IQR)\n \n    train.loc[train[x] < min,x] = np.nan\n    train.loc[train[x] > max,x] = np.nan","02465b74":"# Replacing outliers with nan in UltimateIncurredClaimCost\nfor x in ['UltimateIncurredClaimCost']:\n    q75,q25 = np.percentile(train.loc[:,x],[75,25])\n    IQR = q75-q25\n \n    max = q75+(1.5*IQR)\n    min = q25-(1.5*IQR)\n \n    train.loc[train[x] < min,x] = np.nan\n    train.loc[train[x] > max,x] = np.nan","0093b497":"train.isnull().sum()","d52f5e61":"#Checking the shape of the dataframe before removing outliers\ntrain.shape","6199f5ea":"train = train.dropna(axis = 0)","b71f81f9":"#Checking the shape of the dataframe after removing outliers\ntrain.shape","2ff23f47":"train.describe()","d004ac08":"train2.info()","825ed9a5":"# Replacing outliers with nan in Age\nfor x in ['Age']:\n    q75,q25 = np.percentile(train2.loc[:,x],[75,25])\n    IQR = q75-q25\n \n    max = q75+(1.5*IQR)\n    min = q25-(1.5*IQR)\n \n    train2.loc[train2[x] < min,x] = np.nan\n    train2.loc[train2[x] > max,x] = np.nan","1bd287c7":"# Replacing outliers with nan in WeeklyWages\nfor x in ['WeeklyWages']:\n    q75,q25 = np.percentile(train2.loc[:,x],[75,25])\n    IQR = q75-q25\n \n    max = q75+(1.5*IQR)\n    min = q25-(1.5*IQR)\n \n    train2.loc[train2[x] < min,x] = np.nan\n    train2.loc[train2[x] > max,x] = np.nan","d7aec6bf":"# Replacing outliers with nan in HoursWorkedPerWeek\nfor x in ['HoursWorkedPerWeek']:\n    q75,q25 = np.percentile(train2.loc[:,x],[75,25])\n    IQR = q75-q25\n \n    max = q75+(1.5*IQR)\n    min = q25-(1.5*IQR)\n \n    train2.loc[train2[x] < min,x] = np.nan\n    train2.loc[train2[x] > max,x] = np.nan","c31daa7e":"# Replacing outliers with nan in DaysWorkedPerWeek\nfor x in ['DaysWorkedPerWeek']:\n    q75,q25 = np.percentile(train2.loc[:,x],[75,25])\n    IQR = q75-q25\n \n    max = q75+(1.5*IQR)\n    min = q25-(1.5*IQR)\n \n    train2.loc[train2[x] < min,x] = np.nan\n    train2.loc[train2[x] > max,x] = np.nan","c405503c":"# Replacing outliers with nan in InitialIncurredCalimsCost\nfor x in ['InitialIncurredCalimsCost']:\n    q75,q25 = np.percentile(train2.loc[:,x],[75,25])\n    IQR = q75-q25\n \n    max = q75+(1.5*IQR)\n    min = q25-(1.5*IQR)\n \n    train2.loc[train2[x] < min,x] = np.nan\n    train2.loc[train2[x] > max,x] = np.nan","e0501ce2":"# Replacing outliers with nan in UltimateIncurredClaimCost\nfor x in ['UltimateIncurredClaimCost']:\n    q75,q25 = np.percentile(train2.loc[:,x],[75,25])\n    IQR = q75-q25\n \n    max = q75+(1.5*IQR)\n    min = q25-(1.5*IQR)\n \n    train2.loc[train2[x] < min,x] = np.nan\n    train2.loc[train2[x] > max,x] = np.nan","8277e163":"train2.isnull().sum()","1f0015e5":"train2 = train2.dropna(axis = 0)","c456cc2e":"train2.shape","7fdbfd44":"train.columns","3d826036":"train_cols=['Age', 'DependentChildren', 'DependentsOther', 'WeeklyWages',  'HoursWorkedPerWeek', 'DaysWorkedPerWeek',\n       'ClaimDescription', 'InitialIncurredCalimsCost', 'UltimateIncurredClaimCost']","7391acd8":"sns.pairplot(data=train[train_cols])","ebcb1d4a":"train_corr = train.corr()\nplt.figure(figsize=(15,10))\nsns.heatmap(train_corr, cmap=\"YlGnBu\") # Displaying the Heatmap\nsns.set(font_scale=2,style='white')\n\nplt.title('Heatmap correlation')\nplt.show()","5626b28a":"sns.boxplot(data=train, y='UltimateIncurredClaimCost', x='Age', hue='Gender')","75052cff":"sns.boxplot(data=train, y='UltimateIncurredClaimCost', x='Gender')","25d7fe6f":"sns.boxplot(data=train, y='WeeklyWages', x='Gender')","3c4bcaec":"sns.boxplot(data=train, y='WeeklyWages', x='Age', hue='Gender')","86fc5999":"sns.catplot(data=train, x='Age', col='Gender', kind='box')","7b62b07d":"sns.catplot(data=train, x='Age', col='Gender', kind='violin')","df905808":"train.info()","08771e60":"train.head()","d635d198":"from datetime import datetime","91d9d2f5":"# Converting Dates to datetime data type\ntrain['DateTimeOfAccident']= pd.to_datetime(train['DateTimeOfAccident'])\ntrain['DateReported']= pd.to_datetime(train['DateReported'])","d9bdd429":"# Calculating the difference between the two dates\ntrain['diff_days'] = np.abs(train['DateReported'] - train['DateTimeOfAccident'])\ntrain['diff_days']=train['diff_days']\/np.timedelta64(1,'D')","dac80631":"train","e7591536":"# Dropping the two date features, now that we have the difference between them\ntrain.drop([\"DateTimeOfAccident\", \"DateReported\"],  axis='columns', inplace=True)","0180c051":"# Create a copy of the train processed dataset\ntrain_model1=train.copy()","7342ccc8":"train.describe(include=\"all\")","1494062f":"# Converting Dates to datetime data type\ntrain2['DateTimeOfAccident']= pd.to_datetime(train2['DateTimeOfAccident'])\ntrain2['DateReported']= pd.to_datetime(train2['DateReported'])","5fc3806b":"# Calculating the difference between the two dates\ntrain2['diff_days'] = np.abs(train2['DateReported'] - train2['DateTimeOfAccident'])\ntrain2['diff_days']=train2['diff_days']\/np.timedelta64(1,'D')","e8905278":"# Dropping the two date features, now that we have the difference between them\ntrain2.drop([\"DateTimeOfAccident\", \"DateReported\"],  axis='columns', inplace=True)","e6a9041c":"#copying the train data \ndf_model2=train2.copy()","06210b4c":"# List of columns to drop and Label encode\ndrop_list=['ClaimNumber', 'ClaimDescription']\nle_list=['Gender', 'MaritalStatus', 'PartTimeFullTime']","07426956":"# Dropping features from final model dataframe\ndf_model2.drop(drop_list,  axis='columns', inplace=True)","d8f26b56":"# Label encoding of object features in final model dataframe\nimport sklearn.preprocessing as pre\nle=pre.LabelEncoder()\nfor x in le_list:\n  df_model2[x]=le.fit_transform(df_model2[x])","92f966ca":"# verifying the data types\ndf_model2.info()","802909b4":"# prepare data for final modelling\noutcome2=df_model2['UltimateIncurredClaimCost']\ndf_model2= df_model2.drop('UltimateIncurredClaimCost', axis=1)","54247d91":"outcome2=train2['UltimateIncurredClaimCost']","b31466b4":"# Carrying out minmax scaling and conversion to a dataframe\ndf_model2_scale=pre.minmax_scale(df_model2)\ndf_model2_scale=pd.DataFrame(df_model2_scale, columns=df_model2.columns.tolist())","b7aa4524":"df_model2_scale","20f386fe":"#copying the train data \ndf_model=train.copy()","3c90e995":"# Listing the columns to select the columns to drop and to Label Encode\ndf_model.columns","4e8f905b":"# List of columns to drop and Label encode\ndrop_list=['ClaimNumber', 'ClaimDescription']\nle_list=['Gender', 'MaritalStatus', 'PartTimeFullTime']","849ff043":"# Dropping features from final model dataframe\ndf_model.drop(drop_list,  axis='columns', inplace=True)","1170cdfa":"# Label encoding of object features in final model dataframe\nimport sklearn.preprocessing as pre\nle=pre.LabelEncoder()\nfor x in le_list:\n  df_model[x]=le.fit_transform(df_model[x])","bcb9cc3f":"# verifying the data types\ndf_model.info()","cdc027d9":"outcome=df_model['UltimateIncurredClaimCost']\ndf_model= df_model.drop('UltimateIncurredClaimCost', axis=1)","ca5dccaf":"df_model","d42dc66f":"# Carrying out minmax scaling and conversion to a dataframe\ndf_model_scale=pre.minmax_scale(df_model)\ndf_model_scale=pd.DataFrame(df_model_scale, columns=df_model.columns.tolist())","92715368":"df_model_scale","2a2a17d1":"# defining features and outcome\nfeatures=df_model_scale","71309807":"# Splitting the data\nimport sklearn.model_selection as ms\nx_train, x_test, y_train, y_test=ms.train_test_split(features,outcome,test_size=0.3, random_state=1234561)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","56e6b557":"# Applying KNN Regressor with K=21\nimport sklearn.neighbors as NN\nKNN=NN.KNeighborsRegressor(n_neighbors=21, weights='distance', metric='minkowski')\nKNN.fit(x_train, y_train)","675b7b8e":"pred=KNN.predict(x_test)","d3b26be7":"rss=((y_test-pred)**2).sum()\nmse=np.mean((y_test-pred)**2)\nprint(\"Final rmse value is =\",np.sqrt(np.mean((y_test-pred)**2)))","94bf944f":"# score of test data\nKNN.score(x_test, y_test)","def53271":"# Score of train data\nKNN.score(x_train, y_train)","b5012e4d":"# Applying KNN Regressor with K=190\nKNN=NN.KNeighborsRegressor(n_neighbors=41, weights='distance', metric='minkowski')\nKNN.fit(x_train, y_train)","83b39f9b":"pred=KNN.predict(x_test)","eb078baf":"rss=((y_test-pred)**2).sum()\nmse=np.mean((y_test-pred)**2)\nprint(\"Final rmse value with K=190 is =\",np.sqrt(np.mean((y_test-pred)**2)))","5591b0c1":"KNN.score(x_test, y_test)","b7799d35":"KNN.score(x_train, y_train)","ff875e47":"# defining features and outcome\nfeatures2=df_model2_scale","1ed4f2c0":"# Splitting the data\nimport sklearn.model_selection as ms\nx_train, x_test, y_train, y_test=ms.train_test_split(features2,outcome2,test_size=0.3, random_state=1234561)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","b77d1dc6":"# Applying KNN Regressor with K=21\nimport sklearn.neighbors as NN\nKNN=NN.KNeighborsRegressor(n_neighbors=21, weights='distance', metric='minkowski')\nKNN.fit(x_train, y_train)","7f3580aa":"pred=KNN.predict(x_test)","7b77b66b":"rss=((y_test-pred)**2).sum()\nmse=np.mean((y_test-pred)**2)\nprint(\"Final rmse value is =\",np.sqrt(np.mean((y_test-pred)**2)))","e0279156":"# score of test data\nKNN.score(x_test, y_test)","43338279":"# Score of train data\nKNN.score(x_train, y_train)","15bdcd1a":"# Applying KNN Regressor with K=190\nKNN=NN.KNeighborsRegressor(n_neighbors=190, weights='distance', metric='minkowski')\nKNN.fit(x_train, y_train)","c46b18c7":"pred=KNN.predict(x_test)","83db2855":"rss=((y_test-pred)**2).sum()\nmse=np.mean((y_test-pred)**2)\nprint(\"Final rmse value with K=190 is =\",np.sqrt(np.mean((y_test-pred)**2)))","76a7fa7b":"# score of test data\nKNN.score(x_test, y_test)","a0e68982":"# Score of train data\nKNN.score(x_train, y_train)","b3f057ee":"from numpy import arange\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import RepeatedKFold","d10eedc4":"#define cross-validation method to evaluate model\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)","421a7627":"#define model\nridge_model = RidgeCV(alphas=arange(0, 1, 0.01), cv=cv, scoring='neg_mean_absolute_error')","41935881":"#fit model\nridge_model.fit(x_train, y_train)","1435545b":"ridge_model.score(x_train, y_train)","c1b05f12":"ridge_model.score(x_test, y_test)","bbe1c756":"# lambda that produced the lowest test Score\nprint(ridge_model.alpha_)","fda289d8":"from sklearn.linear_model import Lasso","300bc8d4":"lasso = Lasso()\nlasso.fit(x_train,y_train)","d0e06377":"lasso.score(x_train,y_train)","fde2be15":"lasso.score(x_test,y_test)","f87c4642":"pred_y = lasso.predict(x_test)","dea00092":"rss=((y_test-pred_y)**2).sum()\nmse=np.mean((y_test-pred_y)**2)\nprint(\"Final rmse value with LASSO Regression is =\",np.sqrt(np.mean((y_test-pred_y)**2)))","52caa75b":"test.shape","1c484d1d":"test.head()","5d5e729d":"test.info()","8e060055":"test['Age'] = pd.to_numeric(test['Age'])\ntest['DependentChildren'] = pd.to_numeric(test['DependentChildren'])\ntest['WeeklyWages'] = pd.to_numeric(test['WeeklyWages'])\ntest['HoursWorkedPerWeek'] = pd.to_numeric(test['HoursWorkedPerWeek'])\ntest['DaysWorkedPerWeek'] = pd.to_numeric(test['DaysWorkedPerWeek'])\ntest['InitialIncurredCalimsCost'] = pd.to_numeric(test['InitialIncurredCalimsCost'])","359035d0":"test.isnull().sum()","3f7f6c73":"test['MaritalStatus']=test['MaritalStatus'].fillna(test['MaritalStatus'].mode()[0])","d2bf08ac":"# Converting Dates to datetime data type\ntest['DateTimeOfAccident']= pd.to_datetime(test['DateTimeOfAccident'])\ntest['DateReported']= pd.to_datetime(test['DateReported'])","19d79c22":"# Calculating the difference between the two dates\ntest['diff_days'] = np.abs(test['DateReported'] - test['DateTimeOfAccident'])\ntest['diff_days']=test['diff_days']\/np.timedelta64(1,'D')","4b1fd1db":"# Dropping the two date features, now that we have the difference between them\ntest.drop([\"DateTimeOfAccident\", \"DateReported\"],  axis='columns', inplace=True)","de0b4a10":"test_df=test.copy()","7f0091a2":"# List of columns to drop and Label encode\ndrop_list=['ClaimNumber', 'ClaimDescription']\nle_list=['Gender', 'MaritalStatus', 'PartTimeFullTime']","397b94fa":"# Dropping features from final model dataframe\ntest_df.drop(drop_list,  axis='columns', inplace=True)","0fe65908":"test_df","23915445":"# Label encoding of object features in final model dataframe\nimport sklearn.preprocessing as pre\nle=pre.LabelEncoder()\nfor x in le_list:\n  test_df[x]=le.fit_transform(test_df[x])","882041ef":"test_df.info()","ec250db1":"x_test","16912e56":"# Carrying out minmax scaling and conversion to a dataframe\ntest_df_scale=pre.minmax_scale(test_df)\ntest_df_scale=pd.DataFrame(test_df_scale, columns=test_df.columns.tolist())","d727257c":"test_df_scale","4c509f56":"pred=KNN.predict(test_df_scale)","2579a41a":"pred","da8e49aa":"Sample_Submission = pd.read_csv('\/kaggle\/input\/machine-learning-24-hrs-hackathon\/sample_submission.csv')","7cd8da1e":"Sample_Submission['UltimateIncurredClaimCost'] = pred","b47b72c8":"Sample_Submission","7aa28a39":"cart_df=train_model1.copy()","83630ab6":"cart_df","91303a9a":"cart_df.drop(['ClaimNumber', 'ClaimDescription'],  axis='columns', inplace=True)","5b2dccb7":"#encoding categorical variables\nimport sklearn.preprocessing as pre\nle=pre.LabelEncoder()\ncat_df=cart_df.select_dtypes(exclude=[float, int])\nfor x in cat_df:\n    cart_df[x]=le.fit_transform(cart_df[x])","71dbd013":"cart_df.columns","84d33c1b":"col_ML=['Age', 'Gender', 'MaritalStatus', 'DependentChildren',\n       'DependentsOther', 'WeeklyWages', 'PartTimeFullTime',\n       'HoursWorkedPerWeek', 'DaysWorkedPerWeek', 'InitialIncurredCalimsCost',\n       'UltimateIncurredClaimCost', 'diff_days']","85942f77":"data_ml=cart_df[col_ML]","78718752":"data_ml.dtypes","a085dbb9":"features_c=data_ml.drop('UltimateIncurredClaimCost', axis=1)\noutcome_c=data_ml['UltimateIncurredClaimCost']","4da4cfb3":"# Create training and testing samples\nX_train, X_test, y_train, y_test = ms.train_test_split(features_c, outcome_c, test_size=0.3, random_state=12356)","38b6ee9a":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","749046e6":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeRegressor","fe1b6dca":"dtree = tree.DecisionTreeRegressor(criterion='mse',max_depth=10000, min_samples_split=500, random_state=5)\ndtree.fit(X_train, y_train)","14057b1b":"# use the model to make predictions with the test data\ny_pred = dtree.predict(X_test)","68cfc62b":"y_pred","92333148":"dtree.score(X_train,y_train)","acbae4a4":"dtree.score(X_test,y_test)","aa68ab79":"rss=((y_test-y_pred)**2).sum()\nmse=np.mean((y_test-y_pred)**2)\nprint(\"Final rmse value  is =\",np.sqrt(np.mean((y_test-y_pred)**2)))","b997764c":"# Copy the test data\ncart_test=test.copy()","40f4d025":"cart_test","65f59489":"# List of columns to drop and Label encode\ndrop_list=['ClaimNumber', 'ClaimDescription']\nle_list=['Gender', 'MaritalStatus', 'PartTimeFullTime']","f9569aa6":"# Dropping features from final model dataframe\ncart_test.drop(['ClaimNumber', 'ClaimDescription'],  axis='columns', inplace=True)","2a786508":"#encoding categorical variables\nimport sklearn.preprocessing as pre\nle=pre.LabelEncoder()\ncat_df=cart_test.select_dtypes(exclude=[float, int])\nfor x in cat_df:\n    cart_test[x]=le.fit_transform(cart_test[x])","5db70789":"# Have a final look before processing\ncart_test","9313f9aa":"# Predict the data\ny_pred = dtree.predict(cart_test)","4ae4947f":"# The predicted data\ny_pred","5a35e95c":"# Download and load the predicted data\nSample_Submission = pd.read_csv('\/kaggle\/input\/machine-learning-24-hrs-hackathon\/sample_submission.csv')\nSample_Submission['UltimateIncurredClaimCost'] = y_pred","2b0f667c":"# Upload the final result\nSample_Submission.to_csv('sample_submission_1.csv',index=False)","e68b8152":"From the above outlier detection exercise, the outliers that have been detected are as listed right above. The above outliers will be removed to avoid skewed modelling of the data.","0429f520":"### Checking for outliers and outlier treatment","612f01ee":"From the date fields we can extract the difference in days between the occurrence of the accident and the reporting of the accident ad assess its contribution to the claim amount.","920b45c3":"# Test data final value extraction : For CART","611bbfda":"Comparing the shapes as given above, we can see that there are no duplicate rows.","96d91e42":"## KNN Regressor was used because it is one of the simplest Regression methods having the flexibility to tweak its parameters to obtain the best possible solution. The various values of RMSE and Accuracy scores for different values of K are as tabulated below. This model removes significant outliers capable of affecting the model performance :-\n","75cbe9cd":"# CART","88c02b13":"Here using Ridge regression, the train and test MAE scores are quite close. The lambda value is 0, which means that its accuracy is not better than the KNN models utilised above.","ec7f4983":"#### Applying Ridge Regression.","db965013":"# Feature Engineering","d6ce961c":"## Misc Visualisations","37aa3531":"## Preparing data for modelling","4a34ceda":"# CART was modelled on Trial1 model. It gave better RMSE performance than KNN in specific settings. In particular, it gave the least RMSE with max depth 10000 and min split 500.  Also its train and test accuracies were also closer (within 2-5%) indicating better performance with unmodelled data.\n max_depth     RMSE          min split\n- 20   - 2666.019734982979   min split 2\n- 30   - 2759.5069594688143  min split 2\n- 50   - 2753.8019751392226  min split 2\n- 50   - 2615.84732745761    min split 5\n- 100  - 2445.602112584373   min split 10\n- 1000 - 2445.602112584373   min split 10\n- 1000 - 2151.9974340325257  min split 50\n- 2000 - 2151.9974340325257  min split 50\n- 2000 - 2063.091877553691   min split 100\n- 4000 - 1991.8759474858357  min split 200\n- 10000 - 1972.3416614939124 min split 400\n- 10000 - 1971.2373088988093 min split 500\n- 10000 - 1972.6979534916102 min split 600\n- 10000 - 1978.0536722755796 min split 800\n- 10000 - 1979.5343482734393 min split 1000","aa6c2406":"# train2 code starts","f024ca6c":"# Train2 code. This is a model which removes all outliers. This is to see if there is any performance improvement with this model. In every phase hereforth, there is code for train2. Not to be confused with train1, the model right above this section.","674bf216":"Ridge Regression was applied on train1. Its lambda value was 0 i,e, the performace of KNN algorithms could not be improved more than what was obtained. LASSO Regression was also applied to check its efficacy. Its result was worse off that other KNN  models with a RMSE score of 2375.2045410402397.","5f1e224e":"### Converting object fields which are numerical to numerical datatype.","6d08c4bc":"Using an arbitrary value of K of 21, we are able to achieve a score of 72.1% for test data. We get a RMSE value of 2037.2257.\nTrying to find out the optimum value of K we shall take the square root of n i.e.36123 which is 190.","c8a76242":"### Renaming of column names","3cb8887d":"# Hence, CART has been selected as the algorithm to model the problem.","3f2a8ffe":"### Checking for duplicates and missing values","b282161e":"From the above details and correlating with the downloaded train data opened in MS Excel, it is clear that 12 of the 15 columns have incorrect names because of the way data was saved in the csv file. It is all the more clear from the head of the data given below.","f3cc5837":"The RMSE score of LASSO regression also was worse off than the scores of KNN models.","980070b3":"# Test data final value extraction - For KNN Regression","6dc4a633":"## LASSO Regression","3cd3c153":"# Another model train2 was also modelled whch had all the outliers removed. Its perfocmance was slightly lesser than the above model. Its code can be found in various sections above specifically annotated as \"Trial2 code begins\"\/\"Trial2 code ends\".","f987fb11":"### Using KNN Regressor","aca98b20":"# Machine Learning Model","f0ed7afb":"- k       RMSE           Accuracy\n- k=15 2045.4596666821162 71.84%\n- k=19 2037.7596014381627 72.05%\n- k=21 2037.2257184364971 72.07%\n- k=31 2033.7823125928937 72.17%\n- k=41 2038.9311579335374 72.02%\n- k=51 2048.7288099597713 71.75%\n- k=190 2156.236501704454 68.7%","4852639a":"The spread of values is denser below the value of 200000. ","ef83acbd":"With K as 190, we have an accuracy of 68.7%. It is clearly overfitting. The RMSE value is also slightly larger.\nThe Train accuracy is 99.99%.\n  k      RMSE              Accuracy\n- k=15 2045.4596666821162  71.84%\n- k=19 2037.7596014381627  72.05%\n- k=21 2037.2257184364971  72.07%\n- k=31 2033.7823125928937  72.17%\n- k=41 2038.9311579335374  72.02%\n- k=51 2048.7288099597713  71.75%\n- k=190 2156.236501704454  68.7%","389e82bc":"# Data Loading","4dc7244a":"# From the data and description of the problem, it is clear it is a Regression task.","c2d18203":"### Carrying out Mean imputation for numerical fields and dropping the object data type records with nan values","f9cc1fd1":"Checking each of the numerical features for outliers.","4e84c2af":"## Another model, train2, with all outliers removed was tried out. WIth K=21 it gave RMSE=2040.073402643544. Test score was  0.7122286704037767. The train score was  0.9999785114897611. WIth K=190 the RMSE value was 2169.731088464936. The train score was 0.9999785114897611. The test score was 0.6744874335469009. The values of the first model were slightly better e.g. RMSE with K=21 was 2037 and so on. The values of the first model are available in the comments. The first model, gives slightly better results.\n","3dc9a43b":"## Train the model and make predictions","ed1ca181":"# TRAIN2 CODE ENDS","20499f7d":"## Dropping of unnecessary and processed columns","5605f328":"The data looks more coherent now. Till now out of 36176 records which we had initially, we have dropped 8842 records which is almost 25% of the total data.","5ccff6fc":"From the above heatmap, it is evident that, there is moderate correlation between WeeklyWages and UltimateIncurredClaimsCost. ALso there is strong correlation between InitialIncurredCalimsCost and UltimateIncurredClaimsCost, which is logical. Between the remaining there is nil to almost nil correlation.","2c33de3d":"All the fields visualised above have outliers. In case of Dependent children and Dependent Others, it is possible to have multiple dependents. Hence, these features will be left intact. Age has only 15 outliers. Hence, we won't treat the feature. Also WeeklyWages, HoursWorkedPerWeek and DaysWorkedPerWeek are correlated moderately. Hence, we would remove the outliers from HoursWorkedPerWeek.  The remaining, InitialIncurredCalimsCost and  UltimateIncurredClaimCost will be examined and treated for outliers.","7b550637":" max_depth     RMSE          min split\n- 20   - 2666.019734982979   min split 2\n- 30   - 2759.5069594688143  min split 2\n- 50   - 2753.8019751392226  min split 2\n- 50   - 2615.84732745761    min split 5\n- 100  - 2445.602112584373   min split 10\n- 1000 - 2445.602112584373   min split 10\n- 1000 - 2151.9974340325257  min split 50\n- 2000 - 2151.9974340325257  min split 50\n- 2000 - 2063.091877553691   min split 100\n- 4000 - 1991.8759474858357  min split 200\n- 10000 - 1972.3416614939124 min split 400\n- 10000 - 1971.2373088988093 min split 500\n- 10000 - 1972.6979534916102 min split 600\n- 10000 - 1978.0536722755796 min split 800\n- 10000 - 1979.5343482734393 min split 1000\n","718460bf":"The spread of persons involved in accidents is more uniform for females than it is for males.","bd08f874":"# Train2 code ends","5e318501":"The weekly wages across all ages are 2500 or lower for the majority of the population.","8be89143":"# Reason for selection of the algorithm","7a02acf4":"The best performing model so far is the CART with max depth=10000 and min sample split = 500. Therefore, running test data through the model.","75da674f":"# EDA","e8616a76":"# Data Processing","fa562bc8":"Age, DependentChildren, WeeklyWages, HoursWorkedPerWeek, DaysWorkedPerWeek, InitialIncurredCalimsCost, UltimateIncurredClaimCost are all numerical data type saved as object data type. They need to be converted to numerical fields.","f514f006":"# Train 2 code starts","df82c6cd":"# train2 code ends"}}