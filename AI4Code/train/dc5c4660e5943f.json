{"cell_type":{"a6f3a495":"code","1d2d36f1":"code","a4409d80":"code","0b9a6325":"code","eb5e6191":"code","da432b8f":"code","de3822da":"code","4a5874a6":"code","9efcb84e":"code","7177e4c4":"code","68849531":"code","882d9d54":"code","b2896235":"markdown","deabe0f3":"markdown","2b89204e":"markdown"},"source":{"a6f3a495":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1d2d36f1":"# importing libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","a4409d80":"# Load\nX = pd.read_csv('\/kaggle\/input\/hardwrk\/Linear_X_Train.csv')\ny = pd.read_csv('\/kaggle\/input\/hardwrk\/Linear_Y_Train.csv')\n\n# Convert X,Y to Numpy arrays\nX = X.values\ny = y.values\n\n# Normalisation\nu = X.mean()\nstd = X.std()\nX = (X-u)\/std\n\n\n# Visualise\nplt.style.use('seaborn')\nplt.scatter(X,y,color='blue')\nplt.title(\"Hardwork vs Performance Graph\")\nplt.xlabel(\"Hardwork\")\nplt.ylabel(\"Performance\")\nplt.show()","0b9a6325":"# Algorithm\ndef hypothesis(x,theta):\n    y_ = theta[0] + theta[1]*x\n    return y_\n\ndef gradient(X,Y,theta):\n    m = X.shape[0]\n    grad = np.zeros((2,))\n    for i in range(m):\n        x = X[i]\n        y_ = hypothesis(x,theta)\n        y = Y[i]\n        grad[0] += (y_ - y)\n        grad[1] += (y_ - y)*x\n    return grad\/m\n\ndef error(X,Y,theta):\n    m = X.shape[0]\n    total_error = 0.0\n    for i in range(m):\n        y_ = hypothesis(X[i],theta)\n        total_error += (y_ - Y[i])**2\n        \n    return (total_error\/m)\n    \n\ndef gradientDescent(X,Y,max_steps=100,learning_rate =0.1):\n    \n    theta = np.zeros((2,))\n    error_list = []\n    theta_list = []\n    \n   \n    for i in range(max_steps):\n        \n        # Compute grad\n        grad = gradient(X,Y,theta)\n        e = error(X,Y,theta)[0]\n        \n        \n        #Update theta\n        theta[0] = theta[0] - learning_rate*grad[0]\n        theta[1] = theta[1] - learning_rate*grad[1]\n        # Storing the theta values during updates\n        theta_list.append((theta[0],theta[1]))\n        error_list.append(e)\n        \n    return theta,error_list,theta_list","eb5e6191":"theta,error_list,theta_list = gradientDescent(X,y)","da432b8f":"theta","de3822da":"print(error_list)","4a5874a6":"#theta_list\nplt.plot(error_list)\nplt.title(\"Reduction error over time\")\nplt.show()","9efcb84e":"y_ = hypothesis(X,theta)\nprint(y_)","7177e4c4":"#training + predictions\nplt.scatter(X,y)\nplt.plot(X,y_,color = 'orange',label = \"prediction\")\nplt.legend()\nplt.show()","68849531":"def r2_score(y,y_):\n    num = np.sum((y-y_)**2)\n    denom = np.sum((y- y.mean())**2)\n    score = (1- num\/denom)\n    return score*100","882d9d54":"r2_score(y,y_)","b2896235":"### Section 1\n -->sdf\n -->sd","deabe0f3":"### prediction and best line","2b89204e":"## Linear Regression Algorithm\n---> Hypothesis\n\n--->Gradient\n\n--->Error\n\n--->GradientDecent"}}