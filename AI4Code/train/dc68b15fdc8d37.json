{"cell_type":{"a05a4379":"code","ab40d058":"code","2e378fbf":"code","d9bb1951":"code","18b7bb19":"code","07e37daa":"code","e597a4fb":"code","01a1897b":"code","0700da3d":"code","8acc4dba":"code","7ca9bfb3":"code","38668667":"code","090d4044":"code","98801ae1":"code","41a5b252":"code","5561a77f":"code","48185a96":"code","5c9be8e0":"code","97147043":"code","fc090862":"code","06ab44f8":"code","68347c52":"code","113ce4b2":"code","7ef30a64":"markdown","54024d85":"markdown","8eda704f":"markdown","994eeb96":"markdown","45f10220":"markdown","07f36fc3":"markdown","932f95e4":"markdown","e833f3be":"markdown","cc0bc2ad":"markdown","ca9f346e":"markdown","c8c419a6":"markdown","7193cf2b":"markdown","82479eae":"markdown","7b85a7ce":"markdown"},"source":{"a05a4379":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ab40d058":"data = pd.read_csv('\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv')\nprint(plt.style.available) # look at available plot styles\nplt.style.use('seaborn-dark')","2e378fbf":"# to see features and target variable\n\ndata.head()\n","d9bb1951":"# Well know question is is there any NaN value and length of this data so lets look at info\ndata.info()","18b7bb19":"data.describe()","07e37daa":"color_list = ['blue' if i=='Abnormal' else 'green' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()","e597a4fb":"sns.countplot(x=\"class\", data=data)\ndata.loc[:,'class'].value_counts()","01a1897b":"A = data[data['class'] =='Abnormal']\nN = data[data['class'] == \"Normal\"]","0700da3d":"#scatter plot\nplt.scatter(A.lumbar_lordosis_angle,A.pelvic_radius,color=\"blue\",label=\"abnormal\")\nplt.scatter(N.lumbar_lordosis_angle,N.pelvic_radius,color=\"green\",label=\"normal\")\nplt.xlabel(\"lumbar_lordosis_angle\")\nplt.ylabel(\"pelvic_radius\")\nplt.legend()\nplt.show()","8acc4dba":"data['class'] = [1 if each == 'Abnormal' else 0 for each in data['class']]\ny = data['class'].values\nx_data = data.drop([\"class\"],axis=1)","7ca9bfb3":"data.tail()","38668667":"\n\nx = (x_data- np.min(x_data))\/ (np.max(x_data)- np.min(x_data))","090d4044":"x.head()","98801ae1":"\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3, random_state=1)","41a5b252":"#knn model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)","5561a77f":"prediction","48185a96":"print(\" {} knn score: {}\".format(3,knn.score(x_test,y_test)))","5c9be8e0":"#find k value\nscore_list = []\nfor each in range(1,15):\n    knn2= KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n\nplt.plot(range(1,15),score_list, color='brown')\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","97147043":"# model complexity\nneig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy', color = 'orange')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy', color= 'purple')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","fc090862":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))","06ab44f8":"prediction","68347c52":"print(\" {} knn score: {}\".format(20,knn.score(x_test,y_test)))","113ce4b2":"# model complexity\nneig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy', color = 'orange')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy', color= 'purple')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","7ef30a64":"<a id = '1'><\/a>\n # Load and Check Data","54024d85":"* We can say Abnormal = 1 , Normal =1","8eda704f":"<a id = '2'> <\/a>\n# Exploratary Data Analaysis (EDA)","994eeb96":"# Finding Model Complexity","45f10220":"* Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information.\n* We scale values between 0 and 1.","07f36fc3":"### In this notebook,I use Biomechanical features of orthopedic patients dataset.\n* About in this dataset,\n    * There are 2 task and I used the second task, the categories Disk Hernia and Spondylolisthesis were merged into a single category labelled as 'abnormal'. Thus, the second task consists in classifying patients as belonging to one out of two categories: Normal (100 patients) or Abnormal (210 patients).\n\n    \n<font color = 'red'>    \n   \n# Content:\n    \n1.  [Load and Check Data](#1)\n2. [Exploratory Data Analaysis (EDA)](#2)\n3. [Normalization](#3)\n4. [K-Nearest Neighbors (KNN)](#4)\n  ","932f95e4":"* As you can see our values are created between zeros and ones.","e833f3be":"# Train test split","cc0bc2ad":"<a id = '4'><\/a>\n# K-Nearest Neighbors (KNN)\n\n* KNN: Look at the K closest labeled data points\n* Classification method.\n* First we need to train our data. Train = fit\n* fit(): fits the data, train the data.\n* predict(): predicts the data\n* x: features\n* y: target variables(normal, abnormal)\n* n_neighbors: K. In this example it is 3. it means that Look at the 3 closest labeled data points","ca9f346e":"<a id = '3'><\/a>\n# Normalization","c8c419a6":"# Find k value","7193cf2b":"* As you can see:\n \n * length: 310 (range index)\n * Features are float\n * Target variables are object that is like string","82479eae":"* pd.plotting.scatter_matrix:\n \n * green: normal and blue: abnormal\n * c: color\n * figsize: figure size\n * diagonal: histohram of each features\n * alpha: opacity\n * s: size of marker\n * marker: marker type","7b85a7ce":"* Searborn library has countplot() that counts number of classes\n* Also you can print it with value_counts() method\n"}}