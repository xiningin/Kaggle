{"cell_type":{"2b188ecd":"code","2e2a0d14":"code","6a7fc6e8":"code","b9260a2a":"code","f3014e06":"code","3fce5c04":"code","d279642a":"code","524f977b":"code","f4bcb725":"code","c997aad9":"code","8096cf9f":"code","7a168964":"code","665800ee":"code","2bf6d9bd":"code","f4b1a3ae":"code","fc782710":"code","9938d106":"markdown","48ee1510":"markdown","8b9575d4":"markdown","bbff70af":"markdown","215bf08d":"markdown","4bf74ddf":"markdown","b2e00405":"markdown"},"source":{"2b188ecd":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","2e2a0d14":"#df_train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", nrows=10000)\ndf_train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\ntest_id = df_test.id","6a7fc6e8":"sns.clustermap(df_train.corr())","b9260a2a":"features = list(df_train.columns)\nfeatures.remove('target')","f3014e06":"encoded_df = pd.get_dummies(df_train[features])\nencoded_dft = pd.get_dummies(df_test[features])","3fce5c04":"test_data = encoded_dft","d279642a":"X = encoded_df\nY = df_train['target']","524f977b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.02, random_state = 42)\n","f4bcb725":"n_estimators = [5,20,50,100] # number of trees in the random forest\nmax_features = ['auto', 'sqrt'] # number of features in consideration at every split\nmax_depth = [int(x) for x in np.linspace(10, 120, num = 12)] # maximum number of levels allowed in each decision tree\nmin_samples_split = [2, 6, 10] # minimum sample number to split a node\nmin_samples_leaf = [1, 3, 4] # minimum sample number that can be stored in a leaf node\nbootstrap = [True, False] # method used to sample data points\n\nrandom_grid = {'n_estimators': n_estimators,\n\n'max_features': max_features,\n\n'max_depth': max_depth,\n\n'min_samples_split': min_samples_split,\n\n'min_samples_leaf': min_samples_leaf,\n\n'bootstrap': bootstrap}","c997aad9":"## Importing Random Forest Classifier from the sklearn.ensemble\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()","8096cf9f":"from sklearn.model_selection import RandomizedSearchCV\nrf_random = RandomizedSearchCV(estimator = rf,param_distributions = random_grid,\n               n_iter = 100, cv = 5, verbose=2, random_state=35, n_jobs = -1)","7a168964":"rf_random.fit(X_train, y_train)","665800ee":"print ('Random grid: ', random_grid, '\\n')\n# print the best parameters\nprint ('Best Parameters: ', rf_random.best_params_, ' \\n')","2bf6d9bd":"randmf = RandomForestRegressor(n_estimators = 100, min_samples_split = 6, min_samples_leaf= 4, max_features = 'sqrt', max_depth= 120, bootstrap=False) \nrandmf.fit( X_train, y_train) ","f4b1a3ae":"y_pred_rf1 = pd.DataFrame( { \"actual\": y_test, \n\"predicted_prob\": randmf.predict( \n( X_test ) ) } ) \ny_pred_rf1\n","fc782710":"submission = pd.DataFrame({\"id\": test_id, \"target\":randmf.predict(test_data)})\nsubmission.to_csv('submission.csv', index=False)","9938d106":"### For training it with RandomForest Regressor, I used 10000 rows","48ee1510":"### Feel free to experiment with the parameters yourself ","8b9575d4":"## *Using the best parameters*","bbff70af":"## *Splitting the data into train-test split*","215bf08d":"## *Print the best parameters*","4bf74ddf":"## *Encoding the categorical features*","b2e00405":"## *Hyperparameter tuning using RandomizedSearch CV*"}}