{"cell_type":{"753c906e":"code","9d41c125":"code","11e0b3d6":"code","acf2d2ab":"code","03485401":"code","6f1f5696":"code","e13f04f0":"code","04dbbc27":"code","31cafae7":"code","edb9142f":"code","bc09ae22":"code","ee1b57c8":"code","effe6718":"code","dbbb026f":"code","61c7f906":"code","bbe15750":"code","656c0a54":"code","37550292":"code","df1caa36":"code","5a583597":"code","78f24725":"code","7b54e9c0":"code","ac2d85d7":"code","19897cd6":"code","c66b5a10":"code","b51e163d":"code","81b02d19":"code","42aa6ba0":"code","6ac6bdaf":"code","62cac5af":"code","e584aba6":"code","d1cf6910":"code","43ee5a51":"code","842b6c42":"code","37a4ce36":"code","e1194f6f":"code","8cd1f48e":"code","19487646":"code","49edaf41":"code","2f59bc48":"code","726e1139":"code","c19ee978":"code","1b5d25d5":"code","4b831805":"code","42cfd026":"code","3a073307":"code","9a25fe23":"markdown","d057d09e":"markdown","7620ba92":"markdown","c4946a90":"markdown","835c1626":"markdown","8ba1af9c":"markdown","a94c8530":"markdown","c2fb78a0":"markdown","2b05110d":"markdown","beaca100":"markdown","f1f1a968":"markdown","7c9e25e5":"markdown","23d9c2d3":"markdown","37d43870":"markdown","23e56172":"markdown","d6f58d5f":"markdown","e5fe7fc9":"markdown","a5712a8a":"markdown","27d29370":"markdown","dfa8f63c":"markdown","5ecd3775":"markdown","5da2ce85":"markdown","fb654608":"markdown","c949206a":"markdown","aba8511f":"markdown","b3f96ae0":"markdown","a7e6e004":"markdown","7164bd48":"markdown","f52162ce":"markdown","9f38acef":"markdown","b3fc5f02":"markdown","afd4b73e":"markdown","51c43834":"markdown","78f4ec49":"markdown","16248320":"markdown","cbef5b32":"markdown"},"source":{"753c906e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9d41c125":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%load_ext autoreload\n%autoreload 2","11e0b3d6":"runs = pd.read_csv(\"..\/input\/hkracing\/runs.csv\")\nruns.head()","acf2d2ab":"races = pd.read_csv('..\/input\/hkracing\/races.csv')\nraces.head()","03485401":"runs_data = runs[['race_id', 'won', 'horse_age', 'horse_country', 'horse_type', 'horse_rating',\n       'horse_gear', 'declared_weight', 'actual_weight', 'draw', 'win_odds',\n       'place_odds', 'horse_id']]\nruns_data.head()","6f1f5696":"runs_data.shape","e13f04f0":"races_data = races[['race_id', 'venue', 'config', 'surface', 'distance', 'going', 'race_class', 'date']]\nraces_data.head()","04dbbc27":"races_data.shape","31cafae7":"# merge the two datasets based on race_id column\ndf = pd.merge(runs_data, races_data)\ndf.head()","edb9142f":"df.shape","bc09ae22":"# Display the columns that have the most missing values\ndf.isnull().sum().sort_values(ascending=False)","ee1b57c8":"df = df.dropna()\ndf.shape","effe6718":"df.date = pd.to_datetime(df.date)","dbbb026f":"# Basic information about the dataset\nstart_time = min(df.date).strftime('%d %B %Y')\nend_time = max(df.date).strftime('%d %B %Y')\nno_of_horses = df.horse_id.nunique()\nno_of_races = df.race_id.nunique()\n\nprint(\"\\033[1m\" +f'The dataset was collected from {start_time} to {end_time}, which contains information about {no_of_horses} horses and {no_of_races} races.'+ \"\\033[0m\")","61c7f906":"subset_attributes = ['horse_age','horse_rating', 'declared_weight',\n                     'actual_weight','draw', 'win_odds', 'place_odds',\n                     'surface', 'distance', 'race_class']\n\n\nWinners = round(df[df['won']==1][subset_attributes].describe(),2)\nNot_Winners = round(df[df['won']==0][subset_attributes].describe(),2)\n\npd.concat([Winners, Not_Winners], axis=1, keys=['Winners', 'Not Winners'])","bbe15750":"# Creation of figure with 2 axis\nsns.set(style=\"ticks\")\nsns.set_style(\"darkgrid\")\nfig, ax = plt.subplots(1, 2, figsize=(16, 8))\n\n# Creation of 1st axis\nsns.boxplot(x=\"won\", y=\"win_odds\", data=df,ax=ax[0])\nax[0].legend(loc='upper right')\nax[0].set_title(\"Win odds distribution for non-winners and winners\", fontsize=14)\n\n# Creation of 2nd axis\nsns.boxplot(x=\"won\", y=\"place_odds\", data=df,ax=ax[1])\nax[1].set_title(\"Place odds boxplot for non-winners and winners\", fontsize=14)\nax[1].set(ylim=(0, 40))\n\n# Close the empty Figure 2 created by seaborn.\nplt.close(2)","656c0a54":"# Correlation Matrix Heatmap\nf, ax = plt.subplots(figsize=(15, 10))\ncorr = df.corr()\nhm = sns.heatmap(round(corr,2), annot=True, ax=ax, cmap=\"coolwarm\",fmt='.2f',\n                 linewidths=.05)\nf.subplots_adjust(top=0.93)\nt= f.suptitle('Horses Correlation Heatmap', fontsize=14)","37550292":"features_categorical = ['horse_country', 'horse_type',\n                       'horse_gear','venue', 'config','going','surface','race_class']","df1caa36":"def preprocess_categorical_features(X):   \n    ''' Returns a new DataFrame with dummified columns'''\n    df = X.copy()\n    return pd.get_dummies(df.apply(lambda col: col.astype('category')))","5a583597":"preprocess_categorical_features(df[features_categorical])","78f24725":"df.head()","7b54e9c0":"df.columns","ac2d85d7":"features_numerical = ['horse_age','horse_rating', 'declared_weight',\n                     'actual_weight','draw', 'win_odds', 'place_odds', 'distance']","19897cd6":"from sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\n\ndef preprocess_numerical_features(X):\n    '''\n    Returns a new DataFrame with\n    - Missing values replaced by Column Mean\n    - Features Standard Scaled\n    - Original Features names kept in the DataFrame\n    '''\n    df = X.copy()\n    \n    # Scale feature\n    ct = ColumnTransformer([], remainder=StandardScaler())\n    tmp = ct.fit_transform(df)\n    # keep feature names\n    for (col_index, col_name) in enumerate(list(df.columns)):\n        df[col_name] = tmp[:, col_index]\n    return df","c66b5a10":"preprocess_numerical_features(df[features_numerical])","b51e163d":"data_preprocessed = pd.concat(\n    [\n        df[['won']],\n        preprocess_numerical_features(df[features_numerical]),\n        preprocess_categorical_features(df[features_categorical])\n    ], axis=1)\ndata_preprocessed.shape","81b02d19":"plt.figure(figsize=(6,4))\nsns.countplot(data=data_preprocessed, x='won')\nplt.title('Number of Labels by Class')","42aa6ba0":"# Create a smaller dataset for investigation purpose only\nsample_size = 10000\ntmp = data_preprocessed.sample(sample_size, random_state=414)\nX_small = tmp.drop(columns=['won'])\ny_small = tmp['won']","6ac6bdaf":"# Create X and y\nX = data_preprocessed.drop(columns=['won'])\ny = data_preprocessed['won']\n# Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=414)\nX_train_small, X_test_small, y_train_small, y_test_small = train_test_split(X_small, y_small, random_state=414)","62cac5af":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.calibration import CalibratedClassifierCV\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier","e584aba6":"# DummyClassifier to predict only target 0\ndummy = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\ndummy_pred = dummy.predict(X_test)\n\n# checking unique labels\nprint('Unique predicted labels: ', (np.unique(dummy_pred)))\n\n# checking accuracy\nprint('Test score: ', accuracy_score(y_test, dummy_pred))\n\nprint(classification_report(y_test, dummy_pred))","d1cf6910":"# Modeling the data as is\n# Train model\nlr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n \n# Predict on training set\nlr_pred = lr.predict(X_test)\n\n# Checking accuracy\nprint('Test score: ', accuracy_score(y_test, lr_pred))","43ee5a51":"%%time\n# Simple Random Forest\nforest = RandomForestClassifier(class_weight='balanced')\nforest.fit(X_train, y_train)","842b6c42":"%%time\ny_pred = forest.predict(X_test)","37a4ce36":"unique_elements, counts_elements = np.unique(y_pred, return_counts=True)\nprint(\"Frequency of unique values of the said array:\")\nprint(np.asarray((unique_elements, counts_elements)))","e1194f6f":"print(classification_report(y_test, y_pred))","8cd1f48e":"# First, we start with a wide RandomSearch because we have no idea\n\nmodel = RandomForestClassifier(class_weight='balanced')\n\nsearch_space = {'n_estimators': [int(x) for x in np.linspace(50, 1000, num=20)],\n                'max_depth': [int(x) for x in np.linspace(10, 100, num=10)] + [None],\n                'min_samples_split': [2, 5, 10],\n                'min_samples_leaf': [1, 2, 4],\n                'criterion': ['gini', 'entropy'],\n                'bootstrap': [True, False]\n                }\n\ncv_model = RandomizedSearchCV(model,\n                              scoring='f1_macro',\n                              param_distributions=search_space,\n                              n_jobs=-1,\n                              cv=2,\n                              n_iter=30,\n                              verbose=1)","19487646":"cv_model.fit(X_train, y_train)","49edaf41":"cv_model.best_estimator_","2f59bc48":"y_pred = cv_model.predict(X_test)","726e1139":"unique_elements, counts_elements = np.unique(y_pred, return_counts=True)\nprint(\"Frequency of unique values of the said array:\")\nprint(np.asarray((unique_elements, counts_elements)))","c19ee978":"print(classification_report(y_test, y_pred))","1b5d25d5":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(class_weight='balanced', max_depth=30, \n                                min_samples_split=5, min_samples_leaf=1, n_estimators=800, \n                                criterion='gini', bootstrap=True)\nforest.fit(X_train, y_train)","4b831805":"plt.plot(np.sort(forest.feature_importances_)[::-1])\nplt.xlim(xmax=100, xmin=0)","42cfd026":"# Top 12 features\nfeatures_top_12 = pd.DataFrame(zip(forest.feature_importances_, list(X_train.columns))  \n                              ).sort_values(by=0, ascending=False)[:12]\nfeatures_top_12.reset_index()","3a073307":"features_top_12.reset_index(drop=True).rename(columns={0: 'Feature_importances', 1: \"Top 12 features\"})","9a25fe23":"**Precision** is now higher at 22% but overall we succeed to predict only 21 winners which is low.\n\n**F1_score** improved slightly from our baseline score (48% --> 49%)","d057d09e":"\ud83d\udc47 Let\u2019s do a quick basic descriptive summary statistics on some of the numeric attributes","7620ba92":"# Basic descriptive statistics by winning category","c4946a90":"# Loading Data","835c1626":"First, we will apply some preprocessing methods like standardization or encoding","8ba1af9c":"**Comment** : The win odds and place odds will be for sure the main explicative features to predict the winner","a94c8530":"#### Hyperparameter tuning (on Random Forest Classifier)","c2fb78a0":"![Screenshot%202020-11-08%20at%2021.23.56.png](attachment:Screenshot%202020-11-08%20at%2021.23.56.png)","2b05110d":"Second, we split the data in two : a training set and a testing set.","beaca100":"### Distribution of labels","f1f1a968":"The best estimator is the following:\n`RandomForestClassifier(class_weight='balanced', max_depth=30, min_samples_split=5, min_samples_leaf=1, n_estimators=800, criterion='gini', bootstrap=True)`\n\n\ud83d\udc47 Let's see the performance it gives","7c9e25e5":"# Model","23d9c2d3":"### Random Forest Classifier","37d43870":"## Preprocessing","23e56172":"- **Precision** measures the ability of a model to avoid false positives for a class. This is the number of true positives divided by all positive predictions.\n\n - P\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b = \ud835\udc47\ud835\udc43 \/ \ud835\udc47\ud835\udc43 + \ud835\udc39\ud835\udc43\n \n- **Recall** measures the ability of the model to detect occurences of a class. This is the number of true positives divided by the number of positive values in the test data.\n - R\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59 = \ud835\udc47\ud835\udc43 \/ \ud835\udc47\ud835\udc43 + \ud835\udc39\ud835\udc41","d6f58d5f":"In such an unbalanced problem, accuracy is meaningless: A very dumb model predicting always zeros would have great accuracy, to the detriment of the predictive power of class 1, which has **precision** and **recall** equal to zero!\n","e5fe7fc9":"**Conclusion:** We have run a model that allows us to predict correclty 871 winners.\n\nWe may have better results with other models (SGDClassifier, AdaBoost, KNNClassifier, etc.). \n\nWe may also have better results with resampling techniques: Oversample minority class or Undersample majority class","a5712a8a":"The gradients in the heatmap vary based on the strength of the correlation and you can clearly see it is very easy to spot potential attributes having strong correlations amongst themselves.","27d29370":"## Split Dataset","dfa8f63c":"We will try to train a Random Forest Classifier for our prediction. First, we will use the default hyperparameters","5ecd3775":"Our performance improved a bit. **F1_score** improved slightly from 49% to 61%. \nWe succeed to predict 627 _(=1529*41%)_ winners","5da2ce85":"\ud83d\udca1 **F1_score**, which is the weighted average of precision and recall, would be a good measure for this type of problem.","fb654608":"### categorical features","c949206a":"# Visualisation","aba8511f":"Here we can use the DummyClassifier to always predict \u201cnot win\u201d as a baseline model as we know that there are much more \"not winner\" than \"winner\" in our dataset","b3f96ae0":"### Concatenation of preprocess data","a7e6e004":"Our accuracy score is the same as compared to the dummy classifier above. This tells us that accuracy might not be our best option for measuring performance.","7164bd48":"We can use the Random Forest Classifier in order to find what are the most important features in our prediction","f52162ce":"We got an accuracy score of 92% without training a model. **Accuracy** is the ratio of correct predictions. \n - A\ud835\udc50\ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc66 = (\ud835\udc47\ud835\udc43+\ud835\udc47\ud835\udc41) \/ \ud835\udc5b","9f38acef":"### Baseline and performance metrics","b3fc5f02":"## Forest-based most important features","afd4b73e":"The amount of rows for missing values is relatively small, therefore we decided to drop these rows.","51c43834":"### numerical features","78f4ec49":"## Modeling","16248320":"\ud83d\udc47 Let\u2019s compare this to logistic regression, an actual trained classifier.","cbef5b32":"# Select features and data cleaning"}}