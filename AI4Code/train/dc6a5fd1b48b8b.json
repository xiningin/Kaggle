{"cell_type":{"f72f2651":"code","6c7a6ca0":"code","f33aa470":"code","511fbc24":"code","4e17d0d8":"code","dc6c9d3a":"code","f9aaad0c":"code","f6976d9e":"code","c6fb9d9e":"code","468c9920":"code","a67b4cc7":"code","5e13e45c":"code","1d41843f":"code","1b44e734":"code","2bd7012d":"code","bf9c4098":"code","f72b818d":"code","b8363216":"code","c0850f28":"code","9f30018b":"code","ea23483e":"code","7a2ebe5a":"code","81de3815":"code","52f0d053":"code","5a4b4dcc":"code","d4592575":"code","07c9a5dc":"code","f68a5487":"code","25488d06":"code","992ac4b1":"code","68532b2c":"code","534d242d":"code","5aa670b8":"code","a3de3ee0":"code","7a5b3425":"code","84237bea":"code","64efb009":"code","4e36f34e":"code","ec96e72c":"code","605a3d17":"code","6c279e40":"code","615fdb18":"code","54134e0a":"code","f9391a98":"code","8c4e1cb9":"code","b8ddf6d5":"code","78c0083b":"code","eb186c06":"code","be34d753":"code","129545a1":"code","917fe973":"code","4668b809":"code","9ff74609":"code","de0c521c":"code","8b830304":"code","3ff949dd":"code","d171f76f":"code","fb1dd354":"code","3cc09323":"code","ee4e30aa":"code","8f3820e6":"code","310b45cd":"code","25cfce0d":"code","38c9d82f":"code","6252c2e0":"code","f971d3a0":"code","5d9d1c58":"markdown","0905e121":"markdown","c9e67aff":"markdown","fcf0ced5":"markdown","046d5366":"markdown","80dc337a":"markdown","66463ca1":"markdown","6ee5ce5a":"markdown","42ce3886":"markdown","a47e8f15":"markdown","e026d477":"markdown","a6a56707":"markdown","7f64ae55":"markdown","6fdf3773":"markdown","aa499e68":"markdown","6c3b073c":"markdown","4b3e0b77":"markdown","b9128167":"markdown","11b9f0e4":"markdown","eda1aeb9":"markdown","8ba8f46f":"markdown","763ee365":"markdown","040b4e02":"markdown","dacacd6e":"markdown","eda5bb74":"markdown","178995a9":"markdown","3eb08ba7":"markdown","801cb78d":"markdown"},"source":{"f72f2651":"# making the imports\nimport os\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom functools import reduce\nimport numpy as np\nfrom itertools import permutations, product\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom scipy.sparse import csr_matrix\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nimport math\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import NearestNeighbors\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor","6c7a6ca0":"def beautify_plot(ttl, xl, yl, lg=True):\n    '''\n    Overrides the default title, xlabel, ylabel and legends\n    '''\n    plt.title(ttl)\n    plt.xlabel(xl)\n    plt.ylabel(yl);\n    if lg:\n        plt.legend();\n        \nbase_loc = '\/kaggle\/input\/restaurant-data-with-consumer-ratings\/'","f33aa470":"df_master_train = [pd.read_csv(base_loc+'rating_final.csv')] # read the data\ndf_master_test = [] # prepare the list to reduce the compiled profiling data\nimputation_rules = {} # store rules for imputation\n\ndf_master_train[0].head() # get the head","511fbc24":"df_master_train[0].placeID.nunique(), df_master_train[0].userID.nunique() # get the unique users and restaurants","4e17d0d8":"def get_info(df_temp, df_type):\n    '''\n    Function to get the customized details of a dataframe.\n    '''\n    print(f'The shape is :: {df_temp.shape}\\n\\nThe # duplicates :: {df_temp.duplicated().sum()}')\n    print(f'\\nThe # unique per column ::\\n{df_temp.nunique()}')\n    print(f'\\nThe # NaNs per column ::\\n{df_temp.isna().sum()}')\n    if df_type=='chef':\n        print(f'\\nThe % Primary Key common in ratings and this df :: {pd.merge(df_temp, df_master_train[0]).placeID.nunique()\/130*100:.2f}%')\n    if df_type=='user':\n        print(f'\\nThe % Primary Key common in ratings and this df :: {pd.merge(df_temp, df_master_train[0]).userID.nunique()\/138*100:.2f}%')","dc6c9d3a":"def process_categorical(values):\n    '''\n    Cleans and normalize categorical variable that has text\n    '''\n    return [i.strip().lower().replace(' ','_',).replace('-','_') for i in values]","f9aaad0c":"df1 = pd.read_csv(base_loc+'chefmozaccepts.csv') # read and get info on the data\nget_info(df1, 'chef')","f6976d9e":"# plot the Count of Accepted Payment Types across all Restaurants\ndf1.Rpayment.value_counts().plot(kind='bar')\nbeautify_plot('Count of Accepted Payment Types across all Restaurants\\n', 'Payment Types', 'Count')","c6fb9d9e":"df1['Rpayment'] = process_categorical(df1.Rpayment) # clean the Rpayment column","468c9920":"# plot the Distribution of Restautants accepting unique number of Payemnt Types\ndf1.groupby('placeID').count().sort_values('Rpayment').plot(kind='hist', bins=7);\nbeautify_plot('Distribution of Restautants accepting unique number of Payemnt Types\\n', \n              '# Unique Accepted Payment Types', \n              'Frequency')","a67b4cc7":"# plot the Count of various Payment Types in Restautants which accepts different unique mode(s) of Payment\ndf11 = df1.groupby('placeID').count().reset_index()\ndf11.columns = ['placeID', 'count']\ndf12 = df1.merge(df11).query('count<4').groupby('count').Rpayment.value_counts().unstack()\nfor i in range(0,df12.shape[0]):\n    plt.subplots(1)\n    df12.iloc[i].plot.bar()\n    beautify_plot(f'Count of various Payment Types in Restautants which accepts only {i+1} unique mode(s) of Payment\\n', \n                  'Accepted Payment Types', \n                  'Count')","5e13e45c":"df1_final = pd.get_dummies(df1).groupby('placeID').sum().reset_index() # one hot encode the features\n\n# add the dataframe to the training and testing list\ndf_master_train.append(df1_final)\ndf_master_test.append(df1_final)\n\n# store the imputation rules\nimputation_rules.update({i:{'fillna': 0} for i in df1_final.columns[1:]})","1d41843f":"df2 = pd.read_csv(base_loc+'chefmozcuisine.csv') # read and get info on the data\nget_info(df2, 'chef')","1b44e734":"# plot the Count of Cuisines across all Restaurants\ndf2['Rcuisine'] = process_categorical(df2.Rcuisine)\ncusines = df2.Rcuisine.value_counts()[:10] # get the top 10 cusines\ncusines.plot.barh()\nbeautify_plot('Count of Cuisines across all Restaurants\\n', 'Counts', 'Cuisines');","2bd7012d":"# bin the cuisines which are not in top 10 as others\ndf2['Rcuisine'] = df2.Rcuisine.replace([i for i in df2.Rcuisine.unique() if i not in cusines], 'others')","bf9c4098":"# plot the Counts of various Cusines offered by Restaurants where only single mode of Payment is accepted\ndf1.groupby('placeID').count().query('Rpayment==1').reset_index().merge(df2).Rcuisine.value_counts().plot.bar()\nbeautify_plot('Counts of various Cusines offered by Restaurants where only single mode of Payment is accepted\\n',\n              'Cusines', 'Count');","f72b818d":"# plot the Counts of various Cusines offered by Restaurants where more than one mode of Payment is accepted\ndf1.groupby('placeID').count().query('Rpayment>1').reset_index().merge(df2).Rcuisine.value_counts().plot.bar()\nbeautify_plot('Counts of various Cusines offered by Restaurants where more than one mode of Payment is accepted\\n',\n              'Cusines', 'Count');","b8363216":"df2_final = pd.get_dummies(df2).groupby('placeID').sum().reset_index() # one hot encode the features\n\n# add the dataframe to the training and testing list\ndf_master_train.append(df2_final)\ndf_master_test.append(df2_final)\n\n# store the imputation rules\nimputation_rules.update({i: {'fillna': 1} if i=='Rcuisine_others' else {'fillna':0} for i in df2_final.columns[1:]})","c0850f28":"df3 = pd.read_csv(base_loc+'chefmozhours4.csv') # read and get info on the data\nget_info(df3, 'chef')\n\ndf3.head()","9f30018b":"# plot the Count of days open across all Restaurants\ndf3.groupby('placeID').agg({'days': pd.Series.nunique}).days.value_counts().plot.bar();\nbeautify_plot('Count of days open across all Restaurants\\n', 'Days Open', 'Count');","ea23483e":"df3 = pd.read_csv(base_loc+'chefmozhours4.csv')\n# feature engineer the actual hours\ndf3['hrs'] = df3.hours.apply(lambda x: np.ptp([int(j[:2]) for i in x.split(';') for j in i.split('-') if j]))\ndf3 = df3.replace(0, 24) # replace 0 (came as diff) with 24 for shops which are open round the clock\n\ndf3.drop_duplicates(subset=['placeID', 'days'], inplace=True)\n\n# one hot encode the features\ndf3 = df3.pivot(index='placeID', columns='days', values='hrs')\\\n       .reset_index().rename_axis(None, axis=1)\\\n       .fillna(0).astype(int) # replace 0 for shops which are not open on days (after pivot)\n\n# add the dataframe to the training and testing list\ndf3.columns = ['placeID', 'mon_tue_wed_thu_fri', 'sat', 'sun']\ndf_master_train.append(df3)\ndf_master_test.append(df3)\n\n# store the imputation rules\nimputation_rules.update({i: 'mode' for i in df3.columns[1:]})","7a2ebe5a":"df4 = pd.read_csv(base_loc+'chefmozparking.csv') # read and get info on the data\nget_info(df4, 'chef')","81de3815":"# plot the Counts of Parking Lots across all Restuatrants\ndf4.parking_lot.value_counts().plot.barh();\nbeautify_plot('Counts of Parking Lots across all Restuatrants\\n', 'Parking Lot Types', 'Count');","52f0d053":"df4_final = pd.get_dummies(df4).groupby('placeID').sum().reset_index() # one hot encode the features\n\n# add the dataframe to the training and testing list\ndf_master_train.append(df4_final)\ndf_master_test.append(df4_final)\n\n# store the imputation rules\nimputation_rules.update({i: 'mode' for i in df4_final.columns[1:]})","5a4b4dcc":"df5 = pd.read_csv(base_loc+'usercuisine.csv') # read and get info on the data\nget_info(df5, 'user')","d4592575":"# plot the Counts of various Cusines preferred by all the Customers\ndf5['Ucuisine'] = process_categorical(df5.Rcuisine) # get the top 10 cusines\ndf5.Ucuisine.value_counts()[:10].plot.bar();\nbeautify_plot('Counts of various Cusines preferred by all the Customers\\n', 'Cusines', 'Count');","07c9a5dc":"# bin the cuisines which are not in top 10 as others\ndf5['Ucuisine'] = df5.Ucuisine.replace([i for i in df5.Ucuisine.unique() if i not in cusines], 'others')\n# one hot encode the features\ndf5_final = pd.concat([df5[['userID']], pd.get_dummies(df5.Ucuisine, prefix='Ucuisine')], axis=1)\\\n              .groupby('userID').sum().reset_index()\n\n# add the dataframe to the training and testing list\ndf_master_train.append(df5_final)\ndf_master_test.append(df5_final)\n\n# store the imputation rules\nimputation_rules.update({i: {'fillna': 1} if i=='Ucuisine_others' else {'fillna':0} for i in df5_final.columns[1:]})","f68a5487":"df6 = pd.read_csv(base_loc+'userpayment.csv') # read and get info on the data\ndf6['Upayment'] = process_categorical(df6.Upayment)\nget_info(df6, 'user')","25488d06":"# plot the Counts of Payment Methods preferred by all the Customers\ndf6.Upayment.value_counts().plot(kind='bar');\nbeautify_plot('Counts of Payment Methods preferred by all the Customers\\n', 'Payment Methods', 'Count');","992ac4b1":"# plot the Counts of Payment Methods preferred by Customers who use various mode(s) of Payment(s)\ndf61 = df6.groupby('userID').count().reset_index()\ndf61.columns = ['userID', 'count']\ndf62 = df6.merge(df61).groupby('count').Upayment.value_counts().unstack()\nfor i in range(0,df62.shape[0]):\n    plt.subplots(1)\n    df62.iloc[i].plot.bar()\n    beautify_plot(f'Counts of Payment Methods preferred by Customers who use {i+1} mode(s) of Payment(s)\\n', 'Payment Methods', 'Count');","68532b2c":"df6_final = pd.concat([df6[['userID']], pd.get_dummies(df6.Upayment, prefix='Upayment')], axis=1)\\\n              .groupby('userID').sum().reset_index() # one hot encode the features\n\n# add the dataframe to the training and testing list\ndf_master_train.append(df6_final)\ndf_master_test.append(df6_final)\n\n# store the imputation rules\nimputation_rules.update({i:{'fillna': 1} if i=='Upayment_cash' else {'fillna': 0} for i in df6_final.columns[1:]})","534d242d":"df7 = pd.read_csv(base_loc+'userprofile.csv') # read and get info on the data\nget_info(df7, 'user')","5aa670b8":"df7['age'] = (2020-df7.birth_year) # get the age\n\n# get rid of some of the useless columns\nto_drop = ['latitude', 'longitude', 'height', 'birth_year']\ndf7 = df7.drop(to_drop, axis=1)","a3de3ee0":"# Count of all the categorical variables\ndf7 = df7.replace('?', 'none') # replace the ? with none\nfor i in list(df7.select_dtypes('object').columns)[1:]:\n    df7[i] = process_categorical(df7[i])\n    plt.subplots(1)\n    df7[i].value_counts().plot.bar()\n    beautify_plot(f'Count of {i}(s) among all Users\\n', f'{i}(s)', 'Count');","7a5b3425":"df7['budget'] = df7.budget.replace('none', 'low').replace('high', 'medium') # bin the budget","84237bea":"# plot the Histogram of Weight of all Users\\\ndf7.weight.plot.hist();\nbeautify_plot(f'Histogram of Weight of all Users\\n', 'Frequency', 'Weight');","64efb009":"# plot the Histogram of Age of all Users\\\ndf7.age.plot.hist();\nbeautify_plot(f'Histogram of Age of all Users\\n', 'Frequency', 'Age');","4e36f34e":"df7_final = pd.concat([pd.get_dummies(df7.drop(['userID', 'age', 'weight'], axis=1)), df7[['userID', 'age', 'weight']]], axis=1)\\\n              .groupby('userID').sum().reset_index() # one hot encode the features\n\n# add the dataframe to the training and testing list\ndf_master_train.append(df7_final) \ndf_master_test.append(df7_final)","ec96e72c":"# reduce and create a single dataframe\ndf_train = reduce(lambda x,y: x.merge(y, 'left'), df_master_train)\nassert(df_train.shape[0] == df_master_train[0].shape[0])","605a3d17":"# apply the imputation rules\nprint(f'# NaNs :: {reduce(lambda x,y: x+y, df_train.isna().sum())}')\nfor i in imputation_rules.keys():\n    if type(imputation_rules[i]) == dict: # if it is fillna\n        for j in imputation_rules[i].keys():\n            if j == 'fillna':\n                df_train[i] = df_train[i].fillna(imputation_rules[i][j])\n    else: # if it is mode\n        df_train[i] = df_train[i].fillna(df_train[i].mode()[0])\nprint(f'# NaNs after applying Rules :: {reduce(lambda x,y: x+y, df_train.isna().sum())}')","6c279e40":"# create exhaustive list of places and users which appear in the files\nplaces = set(reduce(lambda x,y: x[['placeID']].append(y[['placeID']]), df_master_test[:4]).placeID)\nusers = set(reduce(lambda x,y: x[['userID']].append(y[['userID']]), df_master_test[4:]).userID)\ndf_test = pd.DataFrame(list(product(places, users)), columns=['placeID', 'userID'])\n\n# take only those which are not rated\ndf_test = df_test.merge(df_train[['placeID', 'userID']], 'outer', indicator=True).query('_merge == \"left_only\"').drop('_merge', axis=1)\nast_shape = df_test.shape\n\n# get the profiles for this master data\ndf_test = [df_test] + df_master_test\ndf_test = reduce(lambda x,y: x.merge(y, 'left'), df_test)\n\nassert(df_test.shape[0] == ast_shape[0])","615fdb18":"# apply the imputation rules\nprint(f'# NaNs :: {reduce(lambda x,y: x+y, df_test.isna().sum())}')\nfor i in imputation_rules:\n    if type(imputation_rules[i]) == dict: # if fillna\n        for j in imputation_rules[i].keys():\n            if j == 'fillna':\n                df_test[i] = df_test[i].fillna(imputation_rules[i][j])\n    else: # if mode\n        df_test[i] = df_test[i].fillna(df_test[i].mode()[0])\nprint(f'# NaNs after applying Rules :: {reduce(lambda x,y: x+y, df_test.isna().sum())}')","54134e0a":"# plot the Overall Rating Counts\ndf_train.rating.value_counts().plot.bar()\nbeautify_plot('Overall Rating Counts\\n', 'Ratings', 'Counts');","f9391a98":"# create the validation training and test data\nto_drop = ['rating', 'service_rating', 'food_rating', 'userID', 'placeID']\nX_train = df_train.drop(to_drop, axis=1).astype('int')\ny_train = df_train.rating\n\nX_vtrain, X_vtest, y_vtrain, y_vtest = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n\ny_vtrain_cont = np.log1p(y_vtrain+np.random.rand())\ny_vtest_cont = np.log1p(y_vtest+np.random.rand())","8c4e1cb9":"def percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred)*0.50))","b8ddf6d5":"res = sm.OLS(y_vtrain_cont, X_vtrain).fit()\nres.summary()","78c0083b":"y_train_pred = list(map(math.floor, np.e**res.predict(X_vtrain)))\nprint('Training Accuracy and Error')\nprint('---------------------------')\nprint(f'Accuracy :: {accuracy_score(y_vtrain, y_train_pred):.2f}\\nError :: {percentage_error(y_vtrain, y_train_pred):.2f}')","eb186c06":"print('Test Accuracy and Error')\nprint('------------------------')\ny_pred = list(map(math.floor, np.e**res.predict(X_vtest)))\nprint(f'Accuracy :: {accuracy_score(y_vtest, y_pred):.2f}\\nError :: {percentage_error(y_vtest, y_pred):.2f}')","be34d753":"dt_res = DecisionTreeRegressor().fit(X_vtrain, y_vtrain_cont)","129545a1":"y_train_pred = list(map(math.floor, np.e**dt_res.predict(X_vtrain)))\nprint('Training Accuracy and Error')\nprint('---------------------------')\nprint(f'Accuracy :: {accuracy_score(y_vtrain, y_train_pred):.2f}\\nError :: {percentage_error(y_vtrain, y_train_pred):.2f}')","917fe973":"print('Test Accuracy and Error')\nprint('------------------------')\ny_pred = list(map(math.floor, np.e**dt_res.predict(X_vtest)))\nprint(f'Accuracy :: {accuracy_score(y_vtest, y_pred):.2f}\\nError :: {percentage_error(y_vtest, y_pred):.2f}')","4668b809":"lr = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=5000)\nlr.fit(X_vtrain, y_vtrain)","9ff74609":"y_train_pred = list(map(math.floor, lr.predict(X_vtrain)))\nprint('Training Accuracy and Error')\nprint('---------------------------')\nprint(f'Accuracy :: {accuracy_score(y_vtrain, y_train_pred):.2f}\\nError :: {percentage_error(y_vtrain, y_train_pred):.2f}')","de0c521c":"print('Test Accuracy and Error')\nprint('------------------------')\ny_pred = list(map(math.floor, lr.predict(X_vtest)))\nprint(f'Accuracy :: {accuracy_score(y_vtest, y_pred):.2f}\\nError :: {percentage_error(y_vtest, y_pred):.2f}')","8b830304":"nb = MultinomialNB()\nnb.fit(X_vtrain, y_vtrain)","3ff949dd":"y_train_pred = list(map(math.floor, nb.predict(X_vtrain)))\nprint('Training Accuracy and Error')\nprint('---------------------------')\nprint(f'Accuracy :: {accuracy_score(y_vtrain, y_train_pred):.2f}\\nError :: {percentage_error(y_vtrain, y_train_pred):.2f}')","d171f76f":"print('Test Accuracy and Error')\nprint('------------------------')\ny_pred = list(map(math.floor, nb.predict(X_vtest)))\nprint(f'Accuracy :: {accuracy_score(y_vtest, y_pred):.2f}\\nError :: {percentage_error(y_vtest, y_pred):.2f}')","fb1dd354":"rf = RandomForestClassifier(n_estimators=50)\nrf.fit(X_vtrain, y_vtrain)","3cc09323":"y_train_pred = list(map(math.floor, rf.predict(X_vtrain)))\nprint('Training Accuracy and Error')\nprint('---------------------------')\nprint(f'Accuracy :: {accuracy_score(y_vtrain, y_train_pred):.2f}\\nError :: {percentage_error(y_vtrain, y_train_pred):.2f}')","ee4e30aa":"print('Test Accuracy and Error')\nprint('------------------------')\ny_pred = list(map(math.floor, rf.predict(X_vtest)))\nprint(f'Accuracy :: {accuracy_score(y_vtest, y_pred):.2f}\\nError :: {percentage_error(y_vtest, y_pred):.2f}')","8f3820e6":"svc = SVC(gamma='auto')\nsvc.fit(X_vtrain, y_vtrain)","310b45cd":"y_train_pred = list(map(math.floor, svc.predict(X_vtrain)))\nprint('Training Accuracy and Error')\nprint('---------------------------')\nprint(f'Accuracy :: {accuracy_score(y_vtrain, y_train_pred):.2f}\\nError :: {percentage_error(y_vtrain, y_train_pred):.2f}')","25cfce0d":"print('Test Accuracy and Error')\nprint('------------------------')\ny_pred = list(map(math.floor, svc.predict(X_vtest)))\nprint(f'Accuracy :: {accuracy_score(y_vtest, y_pred):.2f}\\nError :: {percentage_error(y_vtest, y_pred):.2f}')","38c9d82f":"# store the metrics\nevaluataion = {\n     'Model': {0: 'Linear Regression',\n               1: 'Decision Tree Regressor',\n               2: 'Logistic Regression',\n               3: 'Naive Bayes',\n               4: 'Random Forest',\n               5: 'SVM'},\n     'Testing_Accuracy': {0: 0.21, 1: 0.22, 2: 0.53, 3: 0.52, 4: 0.6, 5: 0.46},\n     'Testing_Error': {0: 0.55, 1: 0.51, 2: 0.29, 3: 0.29, 4: 0.24, 5: 0.32},\n     'Training_Accuracy': {0: 0.17, 1: 0.0, 2: 0.63, 3: 0.48, 4: 0.99, 5: 0.63},\n     'Training_Error': {0: 0.58, 1: 0.51, 2: 0.23, 3: 0.31, 4: 0.01, 5: 0.23},\n     'Type': {0: 'Regression',\n              1: 'Regression',\n              2: 'Classification',\n              3: 'Classification',\n              4: 'Classification',\n              5: 'Classification'}}\nevaluataion = pd.DataFrame(evaluataion)\nevaluataion.groupby('Type').mean().plot.bar();","6252c2e0":"evaluataion.query('Type==\"Classification\"')","f971d3a0":"# make the recommendations\ny_test = pd.DataFrame(nb.predict(df_test.drop(['placeID', 'userID'], axis=1)), columns=['predicted_rating'])\ndf_recommend = pd.concat([df_test[['placeID', 'userID']], y_test], axis=1) # create exhaustive list\n\n# get the top n recommendations and save it in a file\nn = 3\ndf_recommend = df_recommend.sort_values(['userID', 'predicted_rating'], ascending=False).groupby('userID').head(n)\ndf_recommend.to_csv('final_recommendations.csv', index=False)","5d9d1c58":"The above data looks fine and is incorporated. We will create the rule for imputation with mode for any NaNs.","0905e121":"We can see that the Testing Error went down for the classification models on an average by 50% and hence, we would proceed with the classification models.","c9e67aff":"### 5.3 Random Forest","fcf0ced5":"### 1.4 chefmozparking","046d5366":"## 3. Creating Test and Validation Data","80dc337a":"We again need to bin the less popular ones in `others`.\n\nAgain we create the rule of replacing NaNs with others as 1 for User cuisines.","66463ca1":"The data ideally should have `n*3 rows`, i.e the opening and closing hours for each place for Weekdays, Saturdays and Sundays. However we have some as 1 and 2 also. That means, if the shop is not open in either of the weekdays, sat or sun, it does not have a record.\n\n\nWe encounter a number of observations here.\n* Duplicates are present\n* The hours column has 273 unique values\n\nWe propose to do the following for the processing.\n\n* Convert the hours column to number of actual numerical hours. If the hours come as 0, we say the shop is open for 24 hours. This is based on the previous observation of the distribution of days.\n* Remove the duplicates on place and days (cannot have 2 records with different opening and closing timings for the same day)\n* Do a pivot on the days and account the dataset for inclusion in the master file. Do a fillna of `0` for days in which it was not open.\n\nThe rule for imputation when creating the master will be impute by mode.","6ee5ce5a":"We do get quite an interesting pattern here. Previously we saw that **single mode of payments has predominant mode of payment as cash** and again, we see that those restaurants offer mostly **Mexican, American Cuisines and Bars**.\n\nAlso, for two or more mode of payments, there are **cards involved** and as per the distribution of the cuisines, we see that apart from Mexican and American (which can be burger joints), we can see more delicate **European cuisines like International, French, Italian etc**.\n\nOnce we have processed this dataframe, we will account for the NaNs after the join by imputing `1 in others` and `0 in rest of the cuisines`.  ","42ce3886":"* The distribution that we see here is that the Users who like to pay via a single mode, it is always `cash`.\n* For the others, they almost equally like to pay via cash, and cards.\n\nFor the customer who don't have this data, the imputation will happen by adding `1` to cash.","a47e8f15":"## 5. Classification Models\n### 5.1 Logistic regression","e026d477":"We see that we have need to predict the `rating`. Now this is very obvious that all the restaurants will not be there in the ratings dataframe. Again, ideally all the customers should be.\n\nSo, let's first try to see how the restaurant details fit in for the ratings dataset.\n \n`chef*` datasets are for the restaurants. Let's try to see what they have one by one and make them compatible for preparation of the final dataset.\n\n### 1.1 chefmozaccepts","a6a56707":"## 2. Creating Training Data","7f64ae55":"We can see that the linear model didn't have a very good adjusted R2. But it's still acceptable, given the sparse nature.\n\nWhen the accuracy score when the scores were converted back to the classes, the manual % error indicates that in the misclassified ones, it on an average made a mistake of saying +-1 of the actual rating.\n\nOn the test data, the error is 0.55 which means that the model mostly made much mistake to give the ratings ~(+-1 of the actual rating)","6fdf3773":"### 1.7 userprofile","aa499e68":"### 1.3 chefmozhours4","6c3b073c":"A cool observation is that most of the places accepts cash. One more thing to notice is that 'VISA' and 'visa' are different values and it seems that it may be just an error. So, we process the categorical field.","4b3e0b77":"### 4.2 Decision Tree Regression Model","b9128167":"## 7. Predictions and Recommendation","11b9f0e4":"Among the various relations, we can see that the budget after grouping together the `low`, `none` and `high`, `medium` can be a nice way to segment the data for the models and build two different ones.","eda1aeb9":"We see no imbalance in the dataset or presence of outliers in this case. We need to convert the ratings into a continuous value. We will add some noise (0-1 in real line) to the rating and do a log transform.","8ba8f46f":"### 5.2 Naive Bayes","763ee365":"## 6. Comparison of the Models","040b4e02":"We see the observation for those stores which accepts 1,2 and 3 mode of payments.\n* We see that those most of the stores who accepts only 1 mode of payment, accepts cash.\n* We see that those most of the stores who accepts only 2 modes of payment, accepts mostly visa, cash and debit cards.\n* We see that those most of the stores who accepts only 3 mode of payment, accepts mostly visa, mastercards and less cash and lesser debit cards.\n\nThis may reveal a pattern that those stores which accept single mode of payments, does attract people with `low budgets` and those with more attracts people with `high or medium` budget.\n\nWe see that after the merge, there will be some restaurants who will have NaNs for the card types. We can impute them with `0`s. \n\nAlso we see that are around 12 levels. So, we can conclude that we can accept them for the master data after One Hot Encoding.","dacacd6e":"A quick glance reveals that most of the places accepts a single mode of payment.","eda5bb74":"### 1.5 usercuisine","178995a9":"## 4. Regression Models\n### 4.1 Linear Regression Model","3eb08ba7":"### 5.4 SVM","801cb78d":"### 1.6 userpayment"}}