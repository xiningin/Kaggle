{"cell_type":{"d192f729":"code","dfe4cf22":"code","8855a701":"code","e79d4737":"code","28f6cd28":"code","18d08419":"code","29d010a8":"code","f8c7e1b5":"code","d97edba7":"code","fdb0d6e3":"code","b23aaa63":"code","94443254":"code","2ab334dc":"code","14d37919":"code","aafaebb2":"code","865be307":"code","cf99249b":"code","8b2b01f8":"code","fb719297":"code","3e9af632":"code","dbf58c79":"code","775a1349":"code","df5be3d9":"code","c9aa043a":"code","54bec2d6":"code","07ef030f":"code","06ce5d79":"code","568628b9":"code","7cd54162":"code","63788d0f":"code","e3497440":"code","a00e22df":"code","451a8dc5":"code","2dc1b174":"code","b23779fc":"code","0de3472d":"code","7d26a053":"code","824da7fd":"code","44d95baa":"markdown","3d4f768e":"markdown","c2537252":"markdown","bae7a86e":"markdown","9b2e1a80":"markdown","bcba1646":"markdown","abb9b0c9":"markdown","9946d014":"markdown","31e18565":"markdown","fefc3a1c":"markdown","89981150":"markdown","0f620e4b":"markdown","a3c0b3be":"markdown"},"source":{"d192f729":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dfe4cf22":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVC\nfrom scipy.stats import randint\nimport xgboost as xgb\nfrom sklearn import linear_model\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\nimport warnings \nwarnings.filterwarnings('ignore')","8855a701":"train=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.head()","e79d4737":"train.describe()","28f6cd28":"train.info()","18d08419":"sns.set_theme(style='darkgrid')\nsns.scatterplot(x='LotArea',y='SalePrice',data=train)","29d010a8":"sns.scatterplot(x='GrLivArea',y='SalePrice',data=train)","f8c7e1b5":"#Remove Outliers\ntrain[train.LotArea <150000]\ntrain[train.GrLivArea < 4000]","d97edba7":"sns.countplot(x='MSZoning',data=train,palette='viridis')","fdb0d6e3":"sns.boxplot(y='SalePrice',x='SaleCondition',data=train)","b23aaa63":"sns.countplot(x='YrSold',data=train,palette='ocean_r')","94443254":"sns.countplot(x='OverallQual',data=train,palette='inferno_r')","2ab334dc":"fig = sns.barplot(x = 'GarageCars',y = 'SalePrice', data = train)\nfig.set_xticklabels(labels=['No car', '1 car', '2 cars', '3 cars', '4 cars'], rotation=90)\nplt.xlabel(\"Number of cars in Garage\");","14d37919":"sns.kdeplot(x='SalePrice',data=train)","aafaebb2":"sns.kdeplot(x='GrLivArea',data=train)","865be307":"sns.kdeplot(x='LotArea',data=train)","cf99249b":"num_cols=train.select_dtypes(exclude=['object'])\ncat_cols = train.select_dtypes(include=['object'])\nnum_cols_t=test.select_dtypes(exclude=['object'])\ncat_cols_t=test.select_dtypes(include=['object'])\ncat_cols.isnull().sum()","8b2b01f8":"var=num_cols.corr()\n\nplt.figure(figsize=(30,15))\nsns.heatmap(var, center=0, annot=True)\nplt.title(\"Correlation among numerical Attributes\")\nplt.show()","fb719297":"# Dropping features\ndrop=['Id','MSSubClass','OverallCond','BsmtFinSF2','LowQualFinSF','BsmtHalfBath','KitchenAbvGr','EnclosedPorch','3SsnPorch',\n      'PoolArea','MiscVal','MoSold','YrSold','Alley','FireplaceQu','PoolQC','Fence','MiscFeature','Street','Utilities','CentralAir',\n      'Condition2','MSZoning','LandSlope','Heating',\n       'HeatingQC','KitchenQual','GarageType','GarageFinish','SaleType','BsmtExposure','LandSlope']\ntrain.drop(columns=drop,axis=1,inplace=True)\ntest.drop(columns=drop,axis=1,inplace=True)","3e9af632":"train.isnull().sum()","dbf58c79":"train['LotFrontage'].fillna(train['LotFrontage'].mean(),inplace=True)\ntrain['GarageYrBlt'].fillna(train['GarageYrBlt'].median(),inplace=True)\ntrain['MasVnrArea'].fillna(train['MasVnrArea'].median(),inplace=True)\ntrain['BsmtQual'].fillna('TA',inplace=True)\ntrain['BsmtCond'].fillna('NA',inplace=True)\ntrain['BsmtFinType1'].fillna('Unf',inplace=True)\ntrain['BsmtFinType2'].fillna('NA',inplace=True)\ntrain['Electrical'].fillna('SBrkr',inplace=True)\ntrain['GarageQual'].fillna('NA',inplace=True)\ntrain['GarageCond'].fillna('NA',inplace=True)\ntrain['MasVnrType'].fillna('None',inplace=True)\ntrain.isnull().sum() #Great! Now we have zero null values in our training data","775a1349":"# Log tranformation on sale price, gr liv area and lot area\ntrain['LotArea']=np.log(train['LotArea'])\ntrain['SalePrice']=np.log(train['SalePrice'])\ntrain['GrLivArea']=np.log(train['GrLivArea'])\nsns.kdeplot(x='SalePrice',data=train)\nplt.title('After Log Tranformation')","df5be3d9":"x=train.drop(columns='SalePrice',axis=1)\ny=train['SalePrice']","c9aa043a":"from sklearn.preprocessing import LabelEncoder\nx=x.apply(LabelEncoder().fit_transform)","54bec2d6":"x.head()","07ef030f":"# splitting the dataset into train and test data\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state = 42)\nx_train.shape,y_train.shape","06ce5d79":"from sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\n#Gradient Boosting\ngbr = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, max_depth=1, random_state=31).fit(x_train, y_train)\ncv = cross_val_score(gbr, x_train, y_train, cv = 10)\ny_pred = gbr.predict(x_test)\ncv = np.mean(cv)\nr2=r2_score(y_pred,y_test)\nmac=mean_absolute_error(y_pred,y_test)\nprint(\"Cross val score: \" + str(cv))\nprint(\"R^2 score: \" + str(r2))\nprint(\"Mean Absolute Erro: \" + str(mac))","568628b9":"#Random Forest Model\nrf_model=RandomForestRegressor(bootstrap=False, max_depth=500, max_features='auto',\n                       min_samples_leaf=15,criterion='mse', n_jobs=-1, random_state=18).fit(x_train,y_train)\ny_pred=rf_model.predict(x_test)\ncv = cross_val_score(rf_model, x_train, y_train, cv = 10)\ncv=np.mean(cv)\nr2=r2_score(y_pred,y_test)\nme=mean_absolute_error(y_pred,y_test)\ncv=np.mean(cv)\nprint(\"Cross val score: \" + str(cv))\nprint('r2 score :' +str(r2))\nprint('Mean_absolute_error:'+str(me))","7cd54162":"# XGBoost Regressor\nxgb = xgb.XGBRegressor(learning_rate=0.01, n_estimators=1000, objective='reg:squarederror', random_state = 31).fit(x_train, y_train)\ncv = cross_val_score(xgb, x_train, y_train, cv = 10)\ny_pred = xgb.predict(x_test)\ncv = np.mean(cv)\nr2=r2_score(y_pred,y_test)\nmac=mean_absolute_error(y_pred,y_test)\nprint(\"Cross val score: \" + str(cv))\nprint(\"R^2 score: \" + str(r2))\nprint(\"Mean Absolute Error: \" + str(mac))","63788d0f":"#Ada Boost\nada=AdaBoostRegressor(n_estimators=1000,learning_rate=1,random_state=40).fit(x_train,y_train)\ncv=cross_val_score(ada,x_train,y_train,cv=10)\ny_pred=ada.predict(x_test)\ncv=np.mean(cv)\nr2=r2_score(y_pred,y_test)\nmac=mean_absolute_error(y_pred,y_test)\nprint(\"Cross val score: \" + str(cv))\nprint(\"R^2 score: \" + str(r2))\nprint(\"Mean Absolute Error: \" + str(mac))","e3497440":"#Linear Regression\nlr=LinearRegression(fit_intercept=True,n_jobs=-1).fit(x_train,y_train)\ncv=cross_val_score(lr,x_train,y_train,cv=10)\ny_pred=ada.predict(x_test)\ncv=np.mean(cv)\nr2=r2_score(y_pred,y_test)\nmac=mean_absolute_error(y_pred,y_test)\nprint(\"Cross val score: \" + str(cv))\nprint(\"R^2 score: \" + str(r2))\nprint(\"Mean Absolute Error: \" + str(mac))","a00e22df":"test.head()","451a8dc5":"#Let's look for null values in our test dataset\ntest.isnull().sum()","2dc1b174":"test['LotFrontage'].fillna(test['LotFrontage'].mean(),inplace=True)\ntest['GarageYrBlt'].fillna(test['GarageYrBlt'].median(),inplace=True)\ntest['MasVnrArea'].fillna(test['MasVnrArea'].median(),inplace=True)\ntest['Exterior1st'].fillna('VinylSd',inplace=True)\ntest['Exterior2nd'].fillna('VinylSd',inplace=True)\ntest['BsmtQual'].fillna('TA',inplace=True)\ntest['BsmtCond'].fillna('TA',inplace=True)\ntest['BsmtFinType1'].fillna('GLQ',inplace=True)\ntest['BsmtFinType2'].fillna('Unf',inplace=True)\ntest['BsmtFinSF1'].fillna(test['BsmtFinSF1'].median(),inplace=True)\ntest['GarageQual'].fillna('TA',inplace=True)\ntest['GarageCond'].fillna('TA',inplace=True)\ntest['GarageCars'].fillna(test['GarageCars'].median(),inplace=True)\ntest['GarageArea'].fillna(test['GarageArea'].mean(),inplace=True)\ntest['Functional'].fillna('Typ',inplace=True)\ntest['BsmtFullBath'].fillna(test['BsmtFullBath'].median(),inplace=True)\ntest['TotalBsmtSF'].fillna(test['TotalBsmtSF'].mean(),inplace=True)\ntest['BsmtUnfSF'].fillna(test['BsmtUnfSF'].mean(),inplace=True)\ntest['MasVnrType'].fillna('None',inplace=True)\ntest.isnull().sum() # Great! Now we have zero null values in our test dataset","b23779fc":"# Log tranformation on Gr liv area and lot area\ntest['LotArea']=np.log(test['LotArea'])\ntest['GrLivArea']=np.log(test['GrLivArea'])\nsns.kdeplot(x='LotArea',data=test)","0de3472d":"#labelling test data\ntest = test.apply(LabelEncoder().fit_transform)","7d26a053":"#making predictions using gradient boosting regresssor\nprediction=gbr.predict(test)","824da7fd":"sample_submission = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nfinal_data = {'Id': sample_submission.Id, 'SalePrice': prediction}\nfinal_submission = pd.DataFrame(data=final_data)\nfinal_submission.to_csv('submission_file_.csv',index =False)","44d95baa":"Let's try to fix the null values","3d4f768e":"Model Evaluation","c2537252":"The house with garage of 3 cars has the highest sale price.","bae7a86e":"Import necessary libraries","9b2e1a80":"**Feature Engineering**","bcba1646":"**Test Data**","abb9b0c9":"80% of the houses are Residential low density houses and no commercial property is there. A,C and I have zero values in train and test data","9946d014":"We will remove columns containing more than 40% missing values.","31e18565":"Overall Quality has a very high impact on sale price and after looking at the correlation matrix we can drop features with very less correlation to sale price.","fefc3a1c":"We can see outliers in Gr liv area and lot area","89981150":"Sale Price, Gr liv area and lot area are skewed distribution. Hence, we need to scale it for better model accuracy. We will use log transformation.","0f620e4b":"We will use gradient boosting technique as it gives the highest r^2 value","a3c0b3be":"**Exploratory Data Analysis**"}}