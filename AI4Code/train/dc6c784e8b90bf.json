{"cell_type":{"65ed113d":"code","8538f2f8":"code","f942d9bb":"code","f3b3530e":"code","77d57784":"code","d4f2db25":"code","66c2118f":"code","8d82df57":"code","634ac553":"code","e4a74830":"code","66f93d72":"code","c2ff2c90":"code","16252149":"code","dd799802":"code","9c2ed575":"code","9c8829cd":"code","48c8aa9a":"code","5ba5fe12":"code","7c01951f":"code","dcafd1fb":"code","25e6d9a0":"code","08108cfa":"code","a8cb0dee":"code","e376fd34":"code","3d232625":"code","07e52469":"code","24a28205":"code","6dd25e34":"code","ac78c963":"code","c02545d6":"code","38608705":"code","e37f59db":"code","ba4b685d":"code","d5d298c5":"code","beec1efb":"code","5c429beb":"code","3d313493":"code","c694c9a9":"code","893747a4":"code","cddd58c9":"code","6f2849ed":"code","b74b2817":"code","767f4c20":"code","3299edfb":"code","3557142e":"code","32308975":"code","e4e94614":"code","ba45a7dc":"code","d494b2a5":"code","3f0d4352":"code","27d8abaa":"code","254381ae":"code","165a8d71":"code","951e50a6":"code","db17c3ef":"code","271722d2":"code","774c0766":"code","39f7150b":"code","e9a4e13b":"code","34ef761e":"code","895ac041":"code","37c7e64e":"code","ac8f56f5":"code","9395fdf1":"code","fdff082d":"markdown","420e72d5":"markdown","348e2a07":"markdown","ae311de4":"markdown","0fa85e04":"markdown","da784f04":"markdown","6ba11ca4":"markdown","3269d7b8":"markdown","c1b83512":"markdown","507c0b17":"markdown","01058c76":"markdown","e11d986e":"markdown","2eec87ae":"markdown","89a2dc47":"markdown","606777fa":"markdown","8a6dc943":"markdown","d1044d92":"markdown","b60c81fb":"markdown","168ef163":"markdown","7fbf22e0":"markdown","5fae8904":"markdown","2831b45d":"markdown","c43d7d1c":"markdown","96e8403b":"markdown","45f20e7d":"markdown","8f17e031":"markdown","a0324d4b":"markdown","1e291170":"markdown","60e26245":"markdown","b9d3b3a7":"markdown"},"source":{"65ed113d":"import pandas as pd\nimport numpy as np\npd.set_option(\"display.max.columns\",None)\npd.set_option(\"display.max.rows\",None)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\n%matplotlib inline\nfrom sklearn.metrics import mean_absolute_error as MAE\nfrom scipy.stats import skew\nimport fbprophet\nProphet = fbprophet.Prophet\n\nfrom fbprophet import Prophet\nfrom fbprophet.diagnostics import cross_validation\nfrom fbprophet.diagnostics import performance_metrics\nfrom fbprophet.diagnostics import performance_metrics\n\nimport sys\nsys.path.insert(0, '\/kaggle\/input\/walmart-dataset')\nimport utility","8538f2f8":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f942d9bb":"import os\npath = '\/kaggle\/input\/walmart-recruiting-store-sales-forecasting'\nfeatures = pd.read_csv(os.path.join(path, 'features.csv.zip'))\nstores = pd.read_csv(os.path.join(path, 'stores.csv'))\ntest = pd.read_csv(os.path.join(path,'test.csv.zip'))\ntrain = pd.read_csv(os.path.join(path,'train.csv.zip'))\ntrain = train.drop(columns=['IsHoliday'],axis = 1)\nfeatures.head()\n","f3b3530e":"\nstore_features = stores.merge(features,on = 'Store',how = 'inner')","77d57784":"store_data = train.merge(store_features,on = ['Store','Date'],how = 'right')","d4f2db25":"store_data.columns","66c2118f":"dataset = store_data.drop(columns = ['Temperature','Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4',\n       'MarkDown5','Unemployment'],axis = 1)","8d82df57":"dataset.head()","634ac553":"dataset_for_prediction=dataset.dropna()","e4a74830":"dataset_grouped = dataset_for_prediction.groupby(['Date','Store','Dept','CPI'])['Weekly_Sales'].sum().reset_index()","66f93d72":"Store1 = dataset_grouped[dataset_grouped.Store.isin([1])]","c2ff2c90":"Store1.head()","16252149":"steps=0\ndataset_for_prediction1= Store1.copy()\n# dataset_for_prediction1['Actual_sales']=dataset_for_prediction1['Weekly_Sales'].shift(steps)\ndataset_for_prediction1.head(3)","dd799802":"dataset_grouped = dataset_for_prediction1.groupby(['Date','Store','CPI'])['Weekly_Sales'].sum().reset_index()","9c2ed575":"dataset_for_prediction2=dataset_grouped.dropna()\ndataset_for_prediction2['Date'] =pd.to_datetime(dataset_for_prediction2['Date'])\ndataset_for_prediction2.index= dataset_for_prediction2['Date']","9c8829cd":"dataset_for_prediction2.Store.value_counts()\nStore1 = dataset_for_prediction2.copy()","48c8aa9a":"Store1 = Store1.rename(columns = {'Date':'ds','Weekly_Sales':'y'})\ndatetime_series = pd.to_datetime(Store1['ds'])\n\n# create datetime index passing the datetime series\ndatetime_index = pd.DatetimeIndex(datetime_series.values)\n\nStore1_data=Store1.set_index(datetime_index)","5ba5fe12":"Store1_data.head(2)","7c01951f":"SC =  Store1_data[['y']]","dcafd1fb":"SC_with_regressors = utility.add_regressor(SC, Store1_data, varname='Store')\nSC_with_regressors = utility.add_regressor(SC_with_regressors, Store1_data, varname='CPI')\n# SC_with_regressors = utility.add_regressor(SC_with_regressors, Store1_data, varname='Dept')","25e6d9a0":"data_train, data_test = utility.prepare_data(SC_with_regressors, 2012)","08108cfa":"data_test.head()","a8cb0dee":"m = Prophet(changepoint_prior_scale=0.05, interval_width=0.95,growth = 'linear',seasonality_mode = 'multiplicative', \\\n               yearly_seasonality=20, \\\n            weekly_seasonality=True, \\\n#             daily_seasonality=False,\\\n            changepoint_range=0.9)\nm.add_seasonality('weekly', period=7, fourier_order=15)","e376fd34":"m.add_regressor('Store')\n# m.add_regressor('Dept')\nm.add_regressor('CPI')","3d232625":"data_train.head()\ndata_train = data_train.rename(columns = {'index':'ds'})\ndata_test = data_test.rename(columns = {'index':'ds'})\ndata_train.ds = pd.to_datetime(data_train.ds)\ndata_test.ds = pd.to_datetime(data_test.ds)\ndata_test.head()\n","07e52469":"Store = SC_with_regressors[['Store']]\nCPI = SC_with_regressors[['CPI']]\n# Dept = SC_with_regressors[['Dept']]","24a28205":"m.fit(data_train)","6dd25e34":"future = m.make_future_dataframe(periods=len(data_test), freq='7D')\nfutures = utility.add_regressor_to_future(future, [Store,CPI])","ac78c963":"forecast = m.predict(futures)","c02545d6":"pd.plotting.register_matplotlib_converters()\nf = m.plot_components(forecast)","38608705":"plt.figure(figsize=(18, 6))\nm.plot(forecast, xlabel = 'Date', ylabel = 'Weekly-Sales')\nplt.title('Weekly Sales of Store1');","e37f59db":"\nverif = utility.make_verif(forecast, data_train, data_test)","ba4b685d":"pd.plotting.register_matplotlib_converters()\ndef plot_verif1(verif, year=2012):\n    \"\"\"\n    plots the forecasts and observed data, the `year` argument is used to visualise \n    the division between the training and test sets. \n    Parameters\n    ----------\n    verif : pandas.DataFrame\n        The `verif` DataFrame coming from the `make_verif` function in this package\n    year : integer\n        The year used to separate the training and test set. Default 2017\n    Returns\n    -------\n    f : matplotlib Figure object\n    \"\"\"\n    \n    f, ax = plt.subplots(figsize=(15,5))\n    \n    train = verif.loc[:str(year - 1),:]\n    \n    ax.plot(train.index, train.y, 'ko', markersize=3)\n    \n    ax.plot(train.index, train.yhat, color='steelblue', lw=0.5)\n    \n    ax.fill_between(train.index, train.yhat_lower, train.yhat_upper, color='steelblue', alpha=0.3)\n    \n    test = verif.loc[str(year):,:]\n    \n    ax.plot(test.index, test.y, 'ro', markersize=3)\n    \n    ax.plot(test.index, test.yhat, color='coral', lw=0.5)\n    \n    ax.fill_between(test.index, test.yhat_lower, test.yhat_upper, color='coral', alpha=0.3)\n    \n    ax.axvline(str(year), color='0.8', alpha=0.7)\n    \n    ax.grid(ls=':', lw=0.5)\n    \n    return f\n\n# verif.loc[:,'yhat'] = verif.yhat.clip_lower(0)\n# verif.loc[:,'yhat_lower'] = verif.yhat_lower.clip_lower(0)\nf =  plot_verif1(verif,2012)","d5d298c5":"!pip install statsmodels","beec1efb":"from statsmodels.tools.eval_measures import rmse\nimport datetime \nerror=rmse(verif.loc[:'2012','y'].values,verif.loc[:'2012','yhat'].values)\nerror","5c429beb":"# verif['yhat'].plot(legend=True, color='red', figsize=(10,8))\n# dataset_for_prediction1['Weekly_Sales'].plot(legend=True, color='green', figsize=(20,8))\n# dataset_for_prediction1['Weekly_Sales'].plot(legend=True, color='green', figsize=(20,8))\n# verif['y'].plot(legend=True, color='blue', figsize=(10,8))\n\nfig=verif['yhat'].plot(legend=True, color='red', figsize=(10,4))\nfig=verif['y'].plot(legend=True, color='blue')\n# fig.set_figheight(4)\nplt.show()\n","3d313493":"Store1 = dataset_grouped[(dataset_grouped.Store==1)]\nsteps=-1\ndataset_for_prediction1= Store1.copy()\ndataset_for_prediction1['Actual_sales']=dataset_for_prediction1['Weekly_Sales'].shift(steps)\ndataset_for_prediction1=dataset_for_prediction1.dropna()\ndataset_for_prediction1['Date'] =pd.to_datetime(dataset_for_prediction1['Date'])\ndataset_for_prediction1.index= dataset_for_prediction1['Date']\n# dataset_for_prediction1.head()\nX = dataset_for_prediction1[['Store','CPI','Weekly_Sales']]\ny = dataset_for_prediction1[['Actual_sales']]\ny.rename(columns={'Actual_sales':'Next_week_Sale'},inplace = True)\ntrain_size=int(len(Store1) *0.7)\ntest_size = int(len(Store1)) - train_size\ntrain_X, train_y = X[:train_size].dropna(), y[:train_size].dropna()\ntest_X, test_y = X[train_size:].dropna(), y[train_size:].dropna()","c694c9a9":"import statsmodels.api as sm\nseas_d=sm.tsa.seasonal_decompose(X['Weekly_Sales'],model='add',freq=7);\nfig=seas_d.plot()\nfig.set_figheight(4)\nplt.show()","893747a4":"from statsmodels.tsa.stattools import adfuller\ndef test_adf(series, title=''):\n    dfout={}\n    dftest=sm.tsa.adfuller(series.dropna(), autolag='AIC', regression='ct')\n    for key,val in dftest[4].items():\n        dfout[f'critical value ({key})']=val\n    if dftest[1]<=0.05:\n        print(\"Strong evidence against Null Hypothesis\")\n        print(\"Reject Null Hypothesis - Data is Stationary\")\n        print(\"Data is Stationary\", title)\n    else:\n        print(\"Strong evidence for  Null Hypothesis\")\n        print(\"Accept Null Hypothesis - Data is not Stationary\")\n        print(\"Data is NOT Stationary for\", title)","cddd58c9":"y_test=y['Next_week_Sale'][:train_size].dropna()\ntest_adf(y_test, \" Weekly Sales\")","6f2849ed":"!pip install pmdarima","b74b2817":"from pmdarima.arima import auto_arima\nstep_wise=auto_arima(train_y, \n exogenous= train_X,\n start_p=1, start_q=1, \n max_p=7, max_q=7, \n d=1, max_d=7,\n trace=True, \n error_action='ignore', \n suppress_warnings=True, \n stepwise=True)","767f4c20":"step_wise.summary()","3299edfb":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\nmodel= SARIMAX(train_y, seasonal_order=(0,1,0,52),\n exog=train_X,\n order=(0,1,0),\n enforce_invertibility=False, enforce_stationarity=False)\n\nresults= model.fit()\nresults.summary()","3557142e":"%matplotlib inline\n# steps = -1\npredictions= results.predict(start =train_size, end=train_size+test_size+(steps)-1,exog=test_X)\nact= pd.DataFrame(y.iloc[train_size:, :])\npredictions=pd.DataFrame(predictions)\npredictions.reset_index(drop=True, inplace=True)\npredictions.index=test_X.index\npredictions['Actual_sales'] = act['Next_week_Sale']\npredictions.rename(columns={0:'Pred_sales'}, inplace=True)\nFinal_predictions = pd.concat([predictions,act],axis=1)\n# Final_predictions['Pred_sales'].plot(legend=True, color='red', figsize=(20,8))\n# dataset_for_prediction2['Actual_sales'].plot(legend=True, color='blue', figsize=(20,8))\nfig=Final_predictions['Pred_sales'].plot(legend=True, color='red', figsize=(10,4))\nfig=dataset_for_prediction1['Actual_sales'].plot(legend=True, color='blue')\n# fig.set_figheight(4)\nplt.show()\n","32308975":"from statsmodels.tools.eval_measures import rmse\nerror=rmse(predictions['Pred_sales'], predictions['Actual_sales'])\nerror","e4e94614":"## New markdown cell\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor","ba45a7dc":"Store1 = Store1.groupby(['Date','Store','CPI'])['Weekly_Sales'].sum().reset_index()\nStore1.head()","d494b2a5":"Store1.Date = pd.to_datetime(Store1.Date)\nStore1.dtypes","3f0d4352":"# Store1.drop('Date',axis = 1, inplace=True)\nStore1.index = Store1['Date']\nStore1.head()","27d8abaa":"# Store1['sale_year'], Store1['sale_month'], Store1['sale_day'] = Store1.Date.dt.year,Store1.Date.dt.month,Store1.Date.dt.day\nimport re\n#function from fasiai\ndef add_datepart(df, fldnames, drop=True, time=False, errors=\"raise\"):\n    if isinstance(fldnames,str): \n        fldnames = [fldnames]\n    for fldname in fldnames:\n        fld = df[fldname]\n        fld_dtype = fld.dtype\n        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n            fld_dtype = np.datetime64\n\n        if not np.issubdtype(fld_dtype, np.datetime64):\n            df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n        targ_pre = re.sub('[Dd]ate$', '', fldname)\n        attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n        if time: attr = attr + ['Hour', 'Minute', 'Second']\n        for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n        df[targ_pre + 'Elapsed'] = fld.astype(np.int64) \/\/ 10 ** 9\n        if drop: df.drop(fldname, axis=1, inplace=True)\n\nadd_datepart(Store1, 'Date')","254381ae":"Store1.head()","165a8d71":"def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n\nn_trn = int(len(Store1) *0.7)\nx = Store1.drop('Weekly_Sales',axis = 1)\ny = Store1[['Weekly_Sales']]\nX_train, X_test = split_vals(x, n_trn)\ny_train, y_test = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_test.shape","951e50a6":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 20, stop = 500, num = 3)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt',0.5]\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4,3,5]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","db17c3ef":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","271722d2":"rf_random.best_params_","774c0766":"from statsmodels.tools.eval_measures import rmse\n\ndef evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n#     errors = abs(predictions - test_labels)\n    rmse_err = rmse(predictions, test_labels['Weekly_Sales'])\n#     mape = 100 * np.mean(errors \/ test_labels)\n#     accuracy = 100 - mape\n    \n    print('Model Performance')\n    print(f'RMSE: {rmse_err}')\n   \n    return rmse\n\nbest_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_test, y_test)\nprint(random_accuracy)","39f7150b":"import math\nfrom statsmodels.tools.eval_measures import rmse\n\nnp.random.seed(500)\n# m = RandomForestRegressor(n_estimators=39, n_jobs=-1, min_samples_leaf=3,oob_score= True,max_features=0.5) #oob_score=True\nm = RandomForestRegressor(n_estimators=20, n_jobs=-1, min_samples_split=5,min_samples_leaf= 1,max_features=0.5,max_depth=100,bootstrap=False) #oob_score=True\nm.fit(X_train,y_train)\npredictions = m.predict(X_test)\nerror=rmse(predictions, y_test['Weekly_Sales'])\nerror\n# print_score(m)","e9a4e13b":"predictions_df = pd.DataFrame(predictions)\npredictions_df.rename(columns = {0:'Pred'},inplace = True)\npredictions_df.head(10)","34ef761e":"y_test['that'] = list(predictions_df['Pred'])\n","895ac041":"final_rf = pd.concat([y_test,X_test],axis = 1)","37c7e64e":"# final_rf['Weekly_Sales'] = np.exp(final_rf['Weekly_Sales_log'])\nfinal_rf.head()","ac8f56f5":"pd.plotting.register_matplotlib_converters()\nfig=y_train['Weekly_Sales'].plot(legend=True, color='blue', figsize=(15,4))\nfig=final_rf['that'].plot(legend=True, color='red')\nfig=final_rf['Weekly_Sales'].plot(legend=True, color='blue')\n","9395fdf1":"from statsmodels.tools.eval_measures import rmse\nerror=rmse(final_rf['that'], final_rf['Weekly_Sales'])\nerror","fdff082d":"# SARIMAX","420e72d5":"# Implementation of SARIMAX","348e2a07":"**The Purpose of this notebook is to do very basic comparison of all three algorithms SARIMAX, PRophet, Random forest. I have included only few columns for my analysis**","ae311de4":"# Alternate Method - Use the best parameters to implement the RF model","0fa85e04":"# **Checking Stationarity of Data**","da784f04":"**Dividing the data into training and testing**","6ba11ca4":"# Random Forest","3269d7b8":"# RMSE","c1b83512":"RMSE : Prophet : 72916.0944279131\n\nRMSE : SARIMAX : 83318.6341630936\n\nRMSE : Random Forest : 74786.57513396055","507c0b17":"# Creating Prediction dataframe","01058c76":"# RMSE","e11d986e":"# Data Pre-Processing","2eec87ae":"# Best Parameters","89a2dc47":"**Data Pre processing**","606777fa":"**Predictions**","8a6dc943":"**Grouping the data on Weekly Sales for the below mentioned columns**","d1044d92":"**Adding Regressor**","b60c81fb":"# Visualizing results","168ef163":"# RMSE","7fbf22e0":"# Data Preparation for Fbprophet","5fae8904":"# Visualizing Results","2831b45d":"# Importing all the necessary Libraies","c43d7d1c":"# Loading Data","96e8403b":"**Dropping few columns since it was not required for my analysis - I just dropped randomly. It is not the right way to go for it. Since I am just focussing on few columns i.e CPI store and its sales to do the comparison of all three of them, therefore I have dropped all other columns. You can include them while running your analysis**","45f20e7d":"# Model Implementation using best parameters","8f17e031":"**Model Implementation**","a0324d4b":"# Dividing date column into multiple columns such as - year, month, day, week..etc - function taken from fast.ai","1e291170":"# Determining best parameters using RandomizedSearchCV","60e26245":"**Data Preparation**","b9d3b3a7":"# Implementing auto arima to see the step wise summary"}}