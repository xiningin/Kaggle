{"cell_type":{"ac014d32":"code","f8ef2314":"code","5150c682":"code","c63afc9a":"code","ad3a60bd":"code","fb1670b4":"code","da705c34":"code","3c209e79":"code","077aa773":"code","ba029372":"code","d433bbd5":"code","9d951ee9":"code","da030d1e":"code","95c738ab":"code","ec6f8323":"code","e3ea37d1":"code","8f178afc":"code","b54d8cfb":"code","d8aafd82":"code","337972fb":"code","b15486df":"code","8789a1b2":"code","97af1c66":"code","c0fcc0c0":"code","7e80f659":"code","c9539014":"code","b6838339":"code","caa3d934":"code","66cbbb12":"code","cf4842cd":"code","2dbe83b2":"code","e8ad0e2c":"code","d1c12e6c":"code","5cb16581":"code","b285c18e":"code","e8ee4e80":"code","f3a03874":"code","9ebdf748":"code","1fdae158":"code","a6f9a560":"code","90581785":"code","ee41f7e8":"code","13cb2c6e":"code","79ec3ffe":"markdown","38604a30":"markdown","871bdda2":"markdown","348db037":"markdown","4457fca4":"markdown","e05b7f48":"markdown","1c447b5f":"markdown","c35b3077":"markdown","a5a792a6":"markdown","c65bd32c":"markdown","0bf7d712":"markdown","78bd9a54":"markdown","57c69cc5":"markdown","daa21949":"markdown","46935d29":"markdown","e9cdd840":"markdown","edcb3d07":"markdown","4c49c392":"markdown","4da2b159":"markdown","5e1cd348":"markdown","56afa442":"markdown","e98898d0":"markdown","861b694a":"markdown","d21107dd":"markdown","4274ecca":"markdown","8e76525f":"markdown","e48a185b":"markdown","a5e5abac":"markdown","ab61e918":"markdown","09a89f28":"markdown"},"source":{"ac014d32":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","f8ef2314":"train = pd.read_csv('..\/input\/train.csv')\n\ntest = pd.read_csv(\"..\/input\/test.csv\")","5150c682":"train.head()","c63afc9a":"test.head()","ad3a60bd":"train.info()","fb1670b4":"test.info()","da705c34":"def clean_data(data):\n    data['Fare'] = data['Fare'].fillna(data['Fare'].dropna().median())\n    data['Age'] =  data['Age'].fillna(data['Age'].dropna().median())\n    \n    data.loc[data['Sex'] == 'male', 'Sex'] = 0\n    data.loc[data['Sex'] =='female',  'Sex'] = 1\n    \n    data['Embarked'] = data['Embarked'].fillna('S')\n    data.loc[data[\"Embarked\"] == 'S', 'Embarked'] = 0\n    data.loc[data['Embarked'] == 'C', 'Embarked'] = 1\n    data.loc[data['Embarked'] == 'Q', 'Embarked'] =2","3c209e79":"def write_prediction(prediction, name):\n    PassengerId = np.array(test['PassengerId']).astype(int)\n    solution = pd.DataFrame(prediction, PassengerId, columns = ['Survived'])\n    solution.to_csv(name, index_label = ['PassengerId'])","077aa773":"clean_data(train)\nclean_data(test)","ba029372":"train.head()","d433bbd5":"print('check the nan value in train data')\nprint(train.isnull().sum())","9d951ee9":"print('check the nan value in test data')\nprint(test.isnull().sum())","da030d1e":"drop_column = ['Cabin']\ntrain.drop(drop_column, axis=1, inplace = True)\ntest.drop(drop_column,axis=1,inplace=True)","95c738ab":"train.head()","ec6f8323":"test.head()","e3ea37d1":"g = sns.pairplot(data=train, hue='Survived', palette = 'seismic',\n                 size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","8f178afc":"## combine test and train as single to apply some function and applying the feature scaling\nall_data=[train,test]","b54d8cfb":"# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in all_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","d8aafd82":"# Define function to extract titles from passenger names\nimport re\n\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in all_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in all_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', \n                                                 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","337972fb":"## create Range for age features\nfor dataset in all_data:\n    dataset['Age_Range'] = pd.cut(dataset['Age'], bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])","b15486df":"## create RAnge for fare features\nfor dataset in all_data:\n    dataset['Fare_Range'] = pd.cut(dataset['Fare'], bins=[0,7.91,14.45,31,120], labels=['Low_fare','median_fare',\n                                                                                      'Average_fare','high_fare'])","8789a1b2":"#Avoiding dataloss making a copy of both DataSet start working for copy of dataset\ntraindf=train\ntestdf=test","97af1c66":"all_dat=[traindf,testdf]","c0fcc0c0":"for dataset in all_dat:\n    drop_column = ['Age','Fare','Name','Ticket']\n    dataset.drop(drop_column, axis=1, inplace = True)","7e80f659":"#Removing the passenger id from trainning set \ndrop_column = ['PassengerId']\ntraindf.drop(drop_column, axis=1, inplace = True)","c9539014":"all_dat","b6838339":"testdf.head(5)\n","caa3d934":"#Adding the extra feataure in Train data set\ntraindf = pd.get_dummies(traindf, columns = [\"Sex\",\"Title\",\"Age_Range\",\"Embarked\",\"Fare_Range\"],\n                             prefix=[\"Sex\",\"Title\",\"Age_type\",\"Em_type\",\"Fare_type\"])\n","66cbbb12":"traindf.head()","cf4842cd":"#Adding the extra feature in test data set\ntestdf = pd.get_dummies(testdf, columns = [\"Sex\",\"Title\",\"Age_Range\",\"Embarked\",\"Fare_Range\"],\n                             prefix=[\"Sex\",\"Title\",\"Age_type\",\"Em_type\",\"Fare_type\"])","2dbe83b2":"testdf.head()","e8ad0e2c":"#For precaution let final check the training set...\nprint(traindf.isnull().sum())","d1c12e6c":"sns.heatmap(traindf.corr(),annot=True,linewidths=0.2)\nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","5cb16581":"target = traindf['Survived'].values\nfeatures = traindf[['Pclass','SibSp','Parch','FamilySize','Sex_0','Sex_1','Title_Master','Title_Miss','Title_Mr','Title_Mrs','Title_Rare','Age_type_Children','Age_type_Teenage','Age_type_Adult','Age_type_Elder','Em_type_0','Em_type_1','Em_type_2','Fare_type_Low_fare','Fare_type_median_fare','Fare_type_Average_fare','Fare_type_high_fare']].values","b285c18e":"import keras","e8ee4e80":"from keras.models import Sequential\nfrom keras.layers import Dense,Dropout\n\nclassifier = Sequential()\nclassifier.add(Dense(activation=\"relu\", input_dim=22, units=11, kernel_initializer=\"uniform\"))\nclassifier.add(Dense(activation=\"relu\", units=11, kernel_initializer=\"uniform\"))\nclassifier.add(Dropout(0.5))\nclassifier.add(Dense(activation=\"relu\", units=11, kernel_initializer=\"uniform\"))\nclassifier.add(Dropout(0.5))\nclassifier.add(Dense(activation=\"relu\", units=5, kernel_initializer=\"uniform\"))\nclassifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nclassifier.summary()","f3a03874":"history=classifier.fit(features, target, batch_size = 10, nb_epoch = 100,\n    validation_split=0.1,verbose = 1,shuffle=True)","9ebdf748":"drop_column = ['PassengerId']\ntestdf.drop(drop_column, axis=1, inplace = True)\ntestdf.head()","1fdae158":"#predicting the results\nY_pred = classifier.predict(testdf)","a6f9a560":"Y_pred.dtype","90581785":"#Round off the result for submission\nY_pred=Y_pred.round()\nY_pred","ee41f7e8":"#Call above write function to write the out for submission\nwrite_prediction(Y_pred, \"My_output.csv\")","13cb2c6e":"# predictions = classifier.predict(testdf)\n# predictions = pd.DataFrame(predictions, columns=['Survived'])\n# test = pd.read_csv(os.path.join('..\/input', 'test.csv'))\n# predictions = pd.concat((test.iloc[:, 0], predictions), axis = 1)\n# predictions.to_csv('my_output.csv', sep=\",\", index = False)","79ec3ffe":"## Let understand the Problem statment...\n\nIn this challenge, we have to build a predictive model that answers the question: **\u201cwhat sorts of people were more likely to survive?\u201d** using passenger data (ie name, age, gender, socio-economic class, etc).","38604a30":"### As we only cascade the test and train data for applying the feature engeering. We donot mix the train and test data.cascading help in apply feature in single run of code not applying seperately for test and train.And we able to use train and test data sepreatly.","871bdda2":"## End Notes...\n\nThere lot more in this, and I try to give in this as simple as possible.\nI also try to update the kernal, as i get some new things...","348db037":"### Selecting Feature from training set to feed to the neural networks","4457fca4":"### Now clean the data by applying the above clean_data function..","e05b7f48":"Or You can also use the below code for writing the output","1c447b5f":"### <font color='blue'>I hope you find this kernel useful and enjoyable and think that it helped you. PLEASE UPVOTE.<\/font>\nYour comments and feedback are most welcome.","c35b3077":"## little more about the missing Values...\n\nHandling missing values is an essential part of data cleaning and preparation process because almost all data in real life comes with some missing values.\n\nMissing values need to be handled because they reduce the quality for any of our performance metric. It can also lead to wrong prediction or classification and can also cause a high bias for any given model being used.\n\n## WHAT DO WE DO TO MISSING VALUES?\n\nThere are several options for handling missing values each with its own PROS and CONS. However, the choice of what should be done is largely dependent on the nature of our data and the missing values. Below is a summary highlight of several options we have for handling missing values.\n\n1. DROP MISSING VALUES\n2. FILL MISSING VALUES WITH TEST STATISTIC(mean, median, mode).\n3. PREDICT MISSING VALUE WITH A MACHINE LEARNING ALGORITHM(knn).\n\nThere are lot more in this.I recommend to review this article.[click here](https:\/\/towardsdatascience.com\/the-tale-of-missing-values-in-python-c96beb0e8a9d)\n\n**But in our case,  Cabin Featueres has more than 75% of missing data in both Test and train data so we are Drop the Cabin column**","a5a792a6":"## Data Defination...\n\n\n* survival Survival 0 = No, 1 = Yes\n\n* pclass Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n\n* sex Male or Female\n\n* Age Age in years\n\n* sibsp # of siblings \/ spouses aboard the Titanic\n\n* parch # of parents \/ children aboard the Titanic\n\n* ticket Ticket number\n\n* fare Passenger fare\n\n* cabin Cabin number\n\n* embarked Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\n* Variable Notes pclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\n\n* age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n* sibsp: The dataset defines family relations in this way...\n\n* Sibling = brother, sister, stepbrother, stepsister\n\n* Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n* parch: The dataset defines family relations in this way...\n\n* Parent = mother, father\n\n* Child = daughter, son, stepdaughter, stepson\n\n* Some children travelled only with a nanny, therefore parch=0 for them\n","c65bd32c":"### Checking the dataset...\n\nChecking the dataset for datatype, missing value etc...","0bf7d712":"## Now let understand about data...","78bd9a54":"# Little more about the dataset..\nThe data has been split into two groups:\n\ntraining set (train.csv) test set (test.csv) The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.","57c69cc5":"## Start Diving into it...","daa21949":"## Creating the Model...","46935d29":"## preparing the test set for prediction...","e9cdd840":"POSITIVE CORRELATION: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\nNEGATIVE CORRELATION: If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as MultiColinearity as both of them contains almost the same information.\n\nSo do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.","edcb3d07":"## Cleaning Dataset...\n\nACtually dataset contain lot of missing values, unrelated column and also need to applying encoding of text etc.. for specific columns. so, let write some function for that.","4c49c392":"# About this notebook...\n\nThe Objective of this notebook is to give an idea how is the workflow in any predictive modeling problem. How do we check features, how do we add new features and some Machine Learning Concepts. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it.\n\n## <font color='red'>If You Like the notebook and think that it helped you PLEASE UPVOTE.<\/font>","4da2b159":"## Creating new feature \"Title\"...\n\nActually name doesn't contribute much to decide the survival of a person, but title might help to impact the survival like \"miss\" and \"mrs\" are greater chance to survive than \"mr\".","5e1cd348":"## Now again Check for missing values...","56afa442":"## Creating the feature \"Age\"...\n\nIn this we divide the age column into three age group, children, teenage, adult, elder on basis of thier age division.","e98898d0":"# Titanic: Using Deep learning model which achive the accuracy of 80%","861b694a":"# Introduction...\n\n![Titanic-sinking.jpg](attachment:Titanic-sinking.jpg)\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. That's why the name DieTanic. This is a very unforgetable disaster that no one in the world can forget.It took about $7.5 million to build the Titanic and it sunk under the ocean due to collision. \n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle.\n\n\n","d21107dd":"### Creating a features \"Family size\" corresponding to sibsp and Parch...\n\nAs we see above sibsp and Parch are weak feature to decide the survival but with the help of this we can generate the new feature \"Family size\". which can effect the survival more.","4274ecca":"## Creating feature \"Fare\"...\n\nActually fare may or, may not impact directly on survival but, when we divide the fare into fare range like low_fare, median_fare, Averga_fare, and high_fare then we get little bit idea of a people where it present like in first floor, second floor etc.. when incident occur. Beacuse it's position give higher response time to save himself.","8e76525f":"## Applying Feature Engineering...\n\nNow we have clear idea of dependencies of different fetarue on survival of a person. Let's apply Feature Engineering to extract some more features.\n\nFeature engineering is the art of converting raw data into useful features. There are several feature engineering techniques that you can apply to be an artist.","e48a185b":"## Analyze the Correlation between Features...\n\nNow finally merge all the features and understand the correlation between each features....","a5e5abac":"## Reading the dataset","ab61e918":"## Writing the Output in csv file for Submission","09a89f28":"## Writing the Prediction..."}}