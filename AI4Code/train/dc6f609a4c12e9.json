{"cell_type":{"acef6d85":"code","8e278845":"code","8a61c50e":"code","5f166717":"code","aa6c5173":"code","66e83de8":"code","5b242bd1":"code","65eea57c":"code","6378dd05":"code","6c91f872":"code","c48121bf":"code","3f19405c":"code","5cca3266":"code","6e698679":"code","2a7b0629":"code","65d978f0":"markdown","106a67a3":"markdown","c5ea6a22":"markdown","078288a6":"markdown","8efca90f":"markdown","f36909b1":"markdown","64e94cdb":"markdown","0cc7c01c":"markdown","0877e19a":"markdown","4fa10bda":"markdown","62dcc37f":"markdown","70b4abe8":"markdown","f8c5e0f3":"markdown","0ce84c3c":"markdown"},"source":{"acef6d85":"# import packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nprint('Matplotlib: {}'.format(matplotlib.__version__))\n\nimport seaborn as sns\nprint('Seaborn %s' % sns.__version__)\n#-------\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8e278845":"# read input files\ndf_train = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\n\nfeature_cols = [col for col in df_train.columns if col.startswith(\"f\")]","8a61c50e":"df_train.head()","5f166717":"df_test.head()","aa6c5173":"# I use a subset of features here because the full table is quite messy. \n# The purpose is to demonstrate that the value range between the features varies greatly\nfeature_subset=[\"f1\",\"f9\",\"f29\",\"f35\",\"f50\",\"f73\",\"f96\",\"f100\",\"f116\",\"f118\"]\n\ndf_train[feature_subset].describe().T\\\n        .drop(columns=[\"count\", \"25%\", \"50%\", \"75%\"])\\\n        .style.bar(subset=['mean','std'])\\\n        .background_gradient(subset=['max'])","66e83de8":"# now let's look at the same features for the test set\ndf_test[feature_subset].describe().T\\\n        .drop(columns=[\"count\", \"25%\", \"50%\", \"75%\"])\\\n        .style.bar(subset=['mean','std'])\\\n        .background_gradient(subset=['max'])","5b242bd1":"fig, ax = plt.subplots(1, 2, figsize=(16,20), gridspec_kw={'width_ratios': [2, 1]})\ndf_train[feature_cols].isna().sum().plot(kind=\"barh\", ax=ax[0])\ndf_test[feature_cols].isna().sum().plot(kind=\"barh\", color=\"darkgoldenrod\", ax=ax[1])\nax[0].set_title(\"Number of missing values in the feature columns, train set\")\nax[1].set_title(\"Number of missing values in the feature columns, test set\")\n\n# format axes\nfor i in [0,1]:\n    ax[i].spines['bottom'].set_visible(False)\n    ax[i].spines['top'].set_visible(False)\n    ax[i].spines['right'].set_visible(False)\n\nplt.show()\n#df_train[feature_cols].isna().sum().min()\n#df_train[feature_cols].isna().sum().max()","65eea57c":"rows_with_missing_values = df_train[feature_cols].isna().sum(axis=1)\nrows_with_missing_values_test = df_test[feature_cols].isna().sum(axis=1)\n\nfig, ax = plt.subplots(1, 2, figsize=(16,10))\n\nplots = rows_with_missing_values.value_counts().plot(kind=\"bar\",  ax=ax[0])\nplots_t = rows_with_missing_values_test.value_counts().plot(kind=\"bar\",  color=\"darkgoldenrod\", ax=ax[1])\nax[0].set_title(\"Number of rows with missing values, train set\")\nax[1].set_title(\"Number of rows with missing values, test set\")\n\nfor i in [0,1]:\n    ax[i].spines['left'].set_visible(False) #remove the lines around the graph\n    ax[i].spines['top'].set_visible(False)\n    ax[i].spines['right'].set_visible(False)\n    ax[i].get_yaxis().set_ticks([]) # set no ticks\n    ax[i].tick_params(axis='x', rotation=0)\n    ax[i].set_xlabel(\"Number of missing values per row\")\n    #ax[i].set_ylabel(\"Number of rows\")\n\nfor bar in plots.patches:\n    plots.annotate(bar.get_height(),\n                   (bar.get_x() + bar.get_width() \/ 2,\n                    bar.get_height()), ha='center', va='center',\n                   size=10, xytext=(0, 8),\n                   textcoords='offset points')\nfor bar in plots_t.patches:\n    plots_t.annotate(bar.get_height(),\n                   (bar.get_x() + bar.get_width() \/ 2,\n                    bar.get_height()), ha='center', va='center',\n                   size=10, xytext=(0, 8),\n                   textcoords='offset points')\n    \nplt.show()\n# train set: 8 rows have 14 missing values...","6378dd05":"df_train[feature_cols].duplicated().any()","6c91f872":"df_test[feature_cols].duplicated().any()","c48121bf":"cols = df_train[feature_cols].columns.values\nfig, ax = plt.subplots(59, 4, figsize=(16,200))\ncnt = 0\nfor i in cols:\n    p1 = sns.histplot(df_train[i], ax=ax[cnt\/\/2, cnt%2])\n    p1.set(ylabel=None) # no header on y axis\n    p1.set(yticklabels=[]) # no numbers on y axis\n    p1.tick_params(left=False) # remove ticks\n    p1.spines['left'].set_visible(False) #remove the lines around the graph\n    p1.spines['top'].set_visible(False)\n    p1.spines['right'].set_visible(False)\n    p2 = sns.histplot(df_test[i], ax=ax[cnt\/\/2, 2+cnt%2], color='darkgoldenrod')\n    p2.set(ylabel=None)\n    p2.set(yticklabels=[])\n    p2.tick_params(left=False)\n    p2.spines['left'].set_visible(False) \n    p2.spines['top'].set_visible(False)\n    p2.spines['right'].set_visible(False)\n    cnt += 1\n    \n#plt.title(\"Feature Distributions in training set (blue) and test set (golden)\")    \nplt.show()","3f19405c":"# make boxplots of all features, just collapse the cell if your eyes start hurting :)\ncols = df_train[feature_cols].columns.values\nfix, ax =  plt.subplots(len(cols),2, figsize=(18,len(cols)*3.5))\ncnt=0\nfor i,feat in enumerate(cols):\n    p1 = sns.boxplot(data=df_train[feat], orient=\"h\", ax=ax[i,0]).set(xlabel=feat)\n    p2 = sns.boxplot(data=df_test[feat], orient=\"h\", ax=ax[i,1], color=\"darkgoldenrod\").set(xlabel=feat)","5cca3266":"# compare boxplot vs histogram\n# check feature f75 for an example how a boxplot can mislead you\n# check feature f92 for an example where a boxplot reveals additional information\n# check feature 74 and 91, for an interesting comparison\nfeature = \"f40\"\n\nfig, ax = plt.subplots(2, 1, figsize=(15,10))\nfig.suptitle(\"Comparison between boxplot and histogram for the same feature\")\nsns.boxplot(data=df_train[feature], orient=\"h\", ax=ax[0])\nsns.histplot(data=df_train[feature], ax=ax[1])\nplt.show()","6e698679":"# check if there is a correlation between the features -> no\nfig, ax = plt.subplots(1, 2, figsize=(20,8))\n\nsns.heatmap(df_train[feature_cols].corr(), cmap='mako', ax=ax[0])\nsns.heatmap(df_test[feature_cols].corr(), cmap='rocket', ax=ax[1])\nax[0].set_title(\"Correlation between the features, train set\")\nax[1].set_title(\"Correlation between the features, test set\")\nplt.show()","2a7b0629":"plt.figure(figsize=(8,8))\nplots = df_train.claim.value_counts().plot(kind=\"bar\")\nplt.title(\"Values of target variable\")\nplots.spines['left'].set_visible(False) #remove the lines around the graph\nplots.spines['top'].set_visible(False)\nplots.spines['right'].set_visible(False)\nplots.get_yaxis().set_ticks([]) # set no ticks\nplt.xticks(rotation=0)\nfor bar in plots.patches:\n    plots.annotate(bar.get_height(),\n                   (bar.get_x() + bar.get_width() \/ 2,\n                    bar.get_height()), ha='center', va='center',\n                   size=10, xytext=(0, 8),\n                   textcoords='offset points')\nplt.show()","65d978f0":"The distribution are comparable in train and test set. \n\nThe distributions of the features vary greatly. It looks like binning or transforming of some features should be done before modeling.","106a67a3":"There are a comparable number of missing values per row in train and test set. \n\nThe train set has 359464 rows where no values are missing. There are 8 rows where 14 values are missing.\n\n\nThe test set has 185693 rows with no missing values and 1 row with 15 missing values. Remeber the size of the test set is half of the train set.","c5ea6a22":"# Overview","078288a6":"The target variable has a roughly equal amount of 0s and 1s.","8efca90f":"# Feature Analysis, Train & Test sets\n## 1. Missing values","f36909b1":"# Summary\n* There are 118 anonymized features. \n* \"claim\" is the target variable.\n* The training data has 957919 rows. The test data has 493474 rows. That's about half of the training set size.\n* The features have missing values. The amount of missing values is in the same range for all features (train set: from 15168 to 15678 missing values). This is around 1.6% of total values.\n* Roughly 1\/3 of rows have no missing values. The rest has 1-15 missing values.\n* The features have a very different value range. Some features have a really huge range. \n* The features have very different distributions. Some of them invite for binning. \n* There is no correlation between the features.\n\n\nTrain and test set are similar! (As it should be.)","64e94cdb":"# Analysis of target variable","0cc7c01c":"We can see that all features have roughly the same amount of missing values. In the training set the missing values are twice of those in the test set. This is expected, because the size of the training set is twice the size of the test set.","0877e19a":"## 4. Correlation","4fa10bda":"# EDA for Tabular Playground Series September 2021\n\nLooking forward to your feedback. Please upvote if you like it. ","62dcc37f":"Neither train nor test set has duplicated rows.\n\n## 3. Distribution","70b4abe8":"There are nearly too many features for a visual analysis. Yet, there is still something to discover in the boxplots:\n\n* If it wasn't for the boxplot I might have missed the tiny \"hill\" on the right in f74.\n* f26 looks a bit different in train and test. I assume that this might come from random sampling of train and test set.","f8c5e0f3":"## 2. Duplicates","0ce84c3c":"There is no correlation between the features."}}