{"cell_type":{"d71e2813":"code","12ef5103":"code","ce903027":"code","794dea7a":"code","091c3506":"code","192c07d0":"code","cac91005":"code","e16e450d":"code","db280b86":"code","f516c346":"code","b3e09933":"code","5ccd4d35":"code","988efdaf":"code","0e84c2fc":"code","40aceca8":"code","139b425d":"code","062b9a50":"code","c235a9eb":"code","b1f7f73a":"code","9b77ccc1":"code","2ed5e991":"code","645e00f8":"code","981eb956":"code","596771d0":"code","090ee6be":"code","e003fc9b":"code","fc32e898":"code","c12963cf":"code","ce1029dc":"code","e612a756":"code","3fb56f84":"code","7b8330df":"code","fb4cfe3a":"code","4c5d9bd3":"code","ca471a1d":"code","344ff1d7":"code","97ee2154":"code","4bd45e85":"code","767b2f55":"code","7eb72c51":"code","8aa01882":"code","ddfb7f84":"code","a673a4d7":"code","267a841a":"code","51095e4c":"code","e1ff4181":"code","44210396":"code","d0ecc660":"code","afb7804e":"code","b9acf9e5":"markdown","3bf3c984":"markdown","d84f7c4d":"markdown","e3710e01":"markdown","5358b27a":"markdown","6f8438c9":"markdown","2ac1f60f":"markdown","785b370b":"markdown","11f652bf":"markdown","6042e75c":"markdown","1d48e71b":"markdown","fd1d3b54":"markdown","58cba8d7":"markdown","06cb1508":"markdown","f7c3f2bb":"markdown","4d905346":"markdown","e89ec0ff":"markdown","fdd9e84a":"markdown","de9df84e":"markdown","74784aaf":"markdown","0e1b6b0e":"markdown","67ce85df":"markdown","0e053fa1":"markdown","901d4937":"markdown","1ad6955f":"markdown","23d052ea":"markdown","80930ffa":"markdown","3376f24a":"markdown","404110f4":"markdown","c7c9c45e":"markdown","55a322d5":"markdown","ec2c1058":"markdown","c2a57633":"markdown","3119a3f1":"markdown","3f7226cc":"markdown","a318ba12":"markdown","9b4dc531":"markdown","49b1d21c":"markdown","e6800cc8":"markdown","90c8a41c":"markdown","8f594082":"markdown","765b6b5e":"markdown","0d30b382":"markdown","2504d3ad":"markdown","dfeb831e":"markdown","c6c49411":"markdown","94f6b716":"markdown","dd3dff97":"markdown","a083ccd8":"markdown","5cf1fff0":"markdown"},"source":{"d71e2813":"import pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport scipy\nimport matplotlib.pyplot as plt\nimport pylab as pl\nimport sys\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn import tree\nfrom sklearn.naive_bayes import GaussianNB","12ef5103":"student = pd.read_csv('..\/input\/waterpower\/.csv',encoding='cp936')\nstudent.head()","ce903027":"print('Total number of students:',len(student))","794dea7a":"student['\u6210\u7ee9'].describe()","091c3506":"fig = plt.figure(70.70)\nax = fig.add_subplot(1,1,1)\nax.hist(student['\u6210\u7ee9'],bins = 100)\nplt.title('Final Score')\nplt.xlabel('score')\nplt.ylabel('Totle number')\nplt.show()","192c07d0":"most_correlated = student.corr().abs()['\u6210\u7ee9'].sort_values(ascending=False)\nmost_correlated = most_correlated[:12]\nprint(most_correlated)","cac91005":"studentDataset = student[['\u5377\u9762\u6210\u7ee9', '\u4f5c\u4e1a\u6210\u7ee9', '\u4f5c\u4e1a\u63d0\u4ea4\u4e2a\u6570', '\u4f5c\u4e1a\u63d0\u4ea4\u6b21\u6570', '\u6210\u7ee9']]\n\ndataset1 = studentDataset[studentDataset['\u6210\u7ee9'] >= 80]\ndatasetc1 = dataset1.copy()\ndatasetc1['\u6210\u7ee9'] = 1\n\ndataset2 = studentDataset[(studentDataset['\u6210\u7ee9'] < 80)]\ndatasetc2 = dataset2.copy()\ndatasetc2['\u6210\u7ee9'] = 0\n\ndatasetSum = pd.concat([datasetc1, datasetc2])\ndatasetSum.columns =['paperGrades','taskGrades','Number of Job Submissions','Count of Job Submissions','grade']\n\nX = datasetSum.iloc[:, 0:4].values\ny = datasetSum.iloc[:, 4].values","e16e450d":"corr = datasetSum.corr()\nplt.subplots(figsize=(15,10))\nsns.heatmap(corr, annot=True) ","db280b86":"# \u5c06\u6570\u636e\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2,\n                                                    random_state=0)\n\nregr = RandomForestRegressor()\n# regr = RandomForestRegressor(random_state=100,\n#                              bootstrap=True,\n#                              max_depth=2,\n#                              max_features=2,\n#                              min_samples_leaf=3,\n#                              min_samples_split=5,\n#                              n_estimators=3)\npipe = Pipeline([('scaler', StandardScaler()), ('reduce_dim', PCA()),\n                 ('regressor', regr)])\npipe.fit(X_train, y_train)\nypipe = pipe.predict(X_test)\n\nimportances = list(regr.feature_importances_)\n# List of tuples with variable and importance\nprint(importances)\n\n# Saving feature names for later use\ndatasetSum.columns =['paperGrades','taskGrades','Number of Job Submissions','Count of Job Submissions','grade']\nfeature_list = list(datasetSum.columns)[0:4]\n\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n\nplt.rcParams['font.sans-serif']=['SimHei'] #\u7528\u6765\u6b63\u5e38\u663e\u793a\u4e2d\u6587\u6807\u7b7e\nplt.rcParams['axes.unicode_minus'] = False #\u7528\u6765\u6b63\u5e38\u663e\u793a\u8d1f\u53f7\nx_values = list(range(len(importances)))\n# Make a bar chart\nplt.bar(x_values, importances, orientation = 'vertical')\n# Tick labels for x axis\nplt.xticks(x_values, feature_list,rotation=6,)\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');\nplt.show()","f516c346":"# \u9009\u62e97\u79cd\u4e0d\u540c\u7684\u56de\u5f52\u6a21\u578b\u5bf9\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u5bf9\u6bd47\u79cd\u6a21\u578b\u7684MAE\u548cRMSE\n\ndef evaluate(X_train, X_test, y_train, y_test):\n    # Names of models\n    model_name_list = ['Linear Regression', 'ElasticNet Regression',\n                       'Random Forest', 'Extra Trees', 'SVM',\n                       'Gradient Boosted', 'Baseline']\n\n    model1 = LinearRegression()\n    model2 = ElasticNet(alpha=1.0, l1_ratio=0.5)\n    model3 = RandomForestRegressor(n_estimators=100)\n    model4 = ExtraTreesRegressor(n_estimators=100)\n    model5 = SVR(kernel='rbf', degree=3, C=1.0, gamma='auto')\n    model6 = GradientBoostingRegressor(n_estimators=50)\n\n    # Dataframe for results\n    results = pd.DataFrame(columns=['mae', 'rmse'], index=model_name_list)\n\n    # Train and predict with each model\n    for i, model in enumerate([model1, model2, model3, model4, model5, model6]):\n        model.fit(X_train, y_train)\n        predictions = model.predict(X_test)\n\n        # Metrics\n        mae = np.mean(abs(predictions - y_test))\n        rmse = np.sqrt(np.mean((predictions - y_test) ** 2))\n\n        # Insert results into the dataframe\n        model_name = model_name_list[i]\n        results.loc[model_name, :] = [mae, rmse]\n\n    # Median Value Baseline Metrics\n    baseline = np.median(y_train)\n    baseline_mae = np.mean(abs(baseline - y_test))\n    baseline_rmse = np.sqrt(np.mean((baseline - y_test) ** 2))\n\n    results.loc['Baseline', :] = [baseline_mae, baseline_rmse]\n\n    return results\n\nresults = evaluate(X_train, X_test, y_train, y_test)\nprint(results)\n\nplt.figure(figsize=(12, 8))\n\n# Root mean squared error\nax =  plt.subplot(1, 2, 1)\nresults.sort_values('mae', ascending = True).plot.bar(y = 'mae', color = 'b', ax = ax, fontsize=20)\nplt.title('Model Mean Absolute Error', fontsize=20)\nplt.ylabel('MAE', fontsize=20)\n\n# Median absolute percentage error\nax = plt.subplot(1, 2, 2)\nresults.sort_values('rmse', ascending = True).plot.bar(y = 'rmse', color = 'r', ax = ax, fontsize=20)\nplt.title('Model Root Mean Squared Error', fontsize=20)\nplt.ylabel('RMSE',fontsize=20)\nplt.show()","b3e09933":"\nlogreg = LogisticRegression(solver= 'lbfgs' ,max_iter=100)\nlogreg.fit(X, y)\ny_pred = logreg.predict(X)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X, y)))\n\n\nc_m = confusion_matrix(y, y_pred)\nprint(c_m)\n\nsns.heatmap(pd.DataFrame(c_m), annot=True, cmap=\"YlGnBu\", fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\n\nprint(classification_report(y, y_pred))","5ccd4d35":"# creating a model\nmodel = RandomForestClassifier()\n\n# feeding the training data to the model\nmodel.fit(X_train, y_train)\n\n# predicting the x-test results\ny_pred = model.predict(X_test)\n\n# roc_auc_score(y_test, y_pred, multi_class='raise')\n\nprint(\"Training Accuracy :\", model.score(X_train, y_train))\nprint(\"Testing Accuracy :\", model.score(X_test, y_test))","988efdaf":"classifier = RandomForestClassifier(n_estimators = 50, random_state=0)\nclassifier.fit(X_train, y_train)\ny_pred1 = classifier.predict(X_test)\nroc_auc_score(y_test, y_pred1)\ncm = confusion_matrix(y_test, y_pred1)\nplt.figure(figsize=(6,6))\nsns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=0.5, square = True, cmap = 'Pastel1')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nRandmForestAccuracy =roc_auc_score(y_test, y_pred1)\n# RandmForestAccuracy = 'Accuracy Score: {0}'.format(roc_auc_score(y_test, y_pred1))\nprint(RandmForestAccuracy)\nplt.title(all_sample_title, size = 15)","0e84c2fc":"def NBAccuracy(features_train, labels_train, features_test, labels_test):\n    \n    clf = GaussianNB()#\u521b\u5efa\u5206\u7c7b\u5668\n    clf.fit(features_train, labels_train) #\u62df\u5408\u8bad\u7ec3\u5206\u7c7b\u5668\n    pred = clf.predict(features_test) #\u7528\u8bad\u7ec3\u8fc7\u7684\u5206\u7c7b\u5668\u9884\u6d4b\u6d4b\u8bd5\u7279\u5f81\u5bf9\u5e94\u7684\u6807\u7b7e\n    \n    accuracy = clf.score(features_test, labels_test)\n    ###\u5229\u7528\u5206\u7c7b\u5668\u8c03\u7528score\u51fd\u6570\u8ba1\u7b97\u6570\u636e\u7684\u7cbe\u786e\u5ea6\n    return accuracy\n\ndef submitAccuracy():\n    accuracy = NBAccuracy(X_train, y_train,X_test, y_test)\n    return accuracy\nNaiveBayesAccuracy = submitAccuracy()\nprint(NaiveBayesAccuracy)","40aceca8":"clf = SVC(kernel=\"linear\") #\u521b\u5efa\u5206\u7c7b\u5668\nclf.fit(X_train, y_train)#\u62df\u5408\u8bad\u7ec3\u5206\u7c7b\u5668\npred=clf.predict(X_test) #\u9884\u6d4b\u5e76\u5c06\u9884\u6d4b\u7ed3\u679c\u5b58\u653e\u5728\u540d\u4e3apred\u7684\u5217\u8868\u5185\n\nfrom sklearn.metrics import accuracy_score\n###\u501f\u7528sklearn.metric\u6a21\u5757\u4e2d\u7684accuracy_score\u51fd\u6570\u8ba1\u7b97\u7cbe\u786e\u5ea6\nSVMAccuracy = accuracy_score(pred, y_test) #\u8ba1\u7b97\u7cbe\u786e\u5ea6\n\nprint(SVMAccuracy)","139b425d":"clf = tree.DecisionTreeClassifier() #\u521b\u5efa\u4e00\u4e2a\u5206\u7c7b\u5668\n\nclf.fit(X_train,y_train) #\u62df\u5408\u8bad\u7ec3\u5206\u7c7b\u5668\npredictions = clf.predict(X_test)\n\nDTAccuracy=accuracy_score(y_true = y_test, y_pred = predictions)\nprint(DTAccuracy)","062b9a50":"sb=sns.barplot(x=['RandomForest','NaiveBayes','SVM','DecisionTree'], \n                y=[RandmForestAccuracy,NaiveBayesAccuracy,SVMAccuracy,DTAccuracy])\nsb.set_xlabel('Machine Learning Model')\nsb.set_ylabel('Accuracy')\nsb.axes.set_title(\"Different ML Model Accuracy Comparision\")\nplt.show()","c235a9eb":"plt.subplots(figsize=(8,12))\ngrade_counts = student['G3'].value_counts().sort_values().plot.barh(width=.9,color=sns.color_palette('inferno',40))\ngrade_counts.axes.set_title('Number of students who scored a particular grade',fontsize=30)\ngrade_counts.set_xlabel('Number of students', fontsize=30)\ngrade_counts.set_ylabel('Final Grade', fontsize=30)\nplt.show()","b1f7f73a":"b = sns.countplot(student['G3'])\nb.axes.set_title('Distribution of Final grade of students', fontsize = 30)\nb.set_xlabel('Final Grade', fontsize = 20)\nb.set_ylabel('Count', fontsize = 20)\nplt.show()","9b77ccc1":"student.isnull().any()","2ed5e991":"male_studs = len(student[student['sex'] == 'M'])\nfemale_studs = len(student[student['sex'] == 'F'])\nprint('Number of male students:',male_studs)\nprint('Number of female students:',female_studs)","645e00f8":"b = sns.kdeplot(student['age'], shade=True)\nb.axes.set_title('Ages of students', fontsize = 30)\nb.set_xlabel('Age', fontsize = 20)\nb.set_ylabel('Count', fontsize = 20)\nplt.show()","981eb956":"b = sns.countplot('age',hue='sex', data=student)\nb.axes.set_title('Number of students in different age groups',fontsize=30)\nb.set_xlabel(\"Age\",fontsize=30)\nb.set_ylabel(\"Count\",fontsize=20)\nplt.show()","596771d0":"b = sns.boxplot(x='age', y='G3', data=student)\nb.axes.set_title('Age vs Final', fontsize = 30)\nb.set_xlabel('Age', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","090ee6be":"b = sns.swarmplot(x='age', y='G3',hue='sex', data=student)\nb.axes.set_title('Does age affect final grade?', fontsize = 30)\nb.set_xlabel('Age', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","e003fc9b":"b = sns.countplot(student['address'])\nb.axes.set_title('Urban and rural students', fontsize = 30)\nb.set_xlabel('Address', fontsize = 20)\nb.set_ylabel('Count', fontsize = 20)\nplt.show()","fc32e898":"# Grade distribution by address\nsns.kdeplot(student.loc[student['address'] == 'U', 'G3'], label='Urban', shade = True)\nsns.kdeplot(student.loc[student['address'] == 'R', 'G3'], label='Rural', shade = True)\nplt.title('Do urban students score higher than rural students?', fontsize = 20)\nplt.xlabel('Grade', fontsize = 20);\nplt.ylabel('Density', fontsize = 20)\nplt.show()","c12963cf":"b = sns.swarmplot(x='reason', y='G3', data=student)\nb.axes.set_title('Reason vs Final grade', fontsize = 30)\nb.set_xlabel('Reason', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","ce1029dc":"student.corr()['G3'].sort_values()","e612a756":"# Select only categorical variables\ncategory_df = student.select_dtypes(include=['object'])\n\n# One hot encode the variables\ndummy_df = pd.get_dummies(category_df)\n\n# Put the grade back in the dataframe\ndummy_df['G3'] = student['G3']\n\n# Find correlations with grade\ndummy_df.corr()['G3'].sort_values()","3fb56f84":"# selecting the most correlated values and dropping the others\nlabels = student['G3']\n\n# drop the school and grade columns\nstudent = student.drop(['school', 'G1', 'G2'], axis='columns')\n    \n# One-Hot Encoding of Categorical Variables\nstudent = pd.get_dummies(student)","7b8330df":"# Find correlations with the Grade\nmost_correlated = student.corr().abs()['G3'].sort_values(ascending=False)\n\n# Maintain the top 8 most correlation features with Grade\nmost_correlated = most_correlated[:9]\nmost_correlated","fb4cfe3a":"student = student.loc[:, most_correlated.index]\nstudent.head()","4c5d9bd3":"b = sns.swarmplot(x=student['failures'],y=student['G3'])\nb.axes.set_title('Students with less failures score higher', fontsize = 30)\nb.set_xlabel('Number of failures', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","ca471a1d":"family_ed = student['Fedu'] + student['Medu'] \nb = sns.boxplot(x=family_ed,y=student['G3'])\nb.axes.set_title('Educated families result in higher grades', fontsize = 30)\nb.set_xlabel('Family education (Mother + Father)', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","344ff1d7":"b = sns.swarmplot(x=family_ed,y=student['G3'])\nb.axes.set_title('Educated families result in higher grades', fontsize = 30)\nb.set_xlabel('Family education (Mother + Father)', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","97ee2154":"student = student.drop('higher_no', axis='columns')\nstudent.head()","4bd45e85":"b = sns.boxplot(x = student['higher_yes'], y=student['G3'])\nb.axes.set_title('Students who wish to go for higher studies score more', fontsize = 30)\nb.set_xlabel('Higher education (1 = Yes)', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","767b2f55":"b = sns.countplot(student['goout'])\nb.axes.set_title('How often do students go out with friends', fontsize = 30)\nb.set_xlabel('Go out', fontsize = 20)\nb.set_ylabel('Count', fontsize = 20)\nplt.show()","7eb72c51":"b = sns.swarmplot(x=student['goout'],y=student['G3'])\nb.axes.set_title('Students who go out a lot score less', fontsize = 30)\nb.set_xlabel('Going out', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","8aa01882":"b = sns.swarmplot(x=student['romantic_no'],y=student['G3'])\nb.axes.set_title('Students with no romantic relationship score higher', fontsize = 30)\nb.set_xlabel('Romantic relationship (1 = None)', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","ddfb7f84":"# splitting the data into training and testing data (75% and 25%)\n# we mention the random state to achieve the same split everytime we run the code\nX_train, X_test, y_train, y_test = train_test_split(student, labels, test_size = 0.25, random_state=42)","a673a4d7":"X_train.head()","267a841a":"# Calculate mae and rmse\ndef evaluate_predictions(predictions, true):\n    mae = np.mean(abs(predictions - true))\n    rmse = np.sqrt(np.mean((predictions - true) ** 2))\n    \n    return mae, rmse","51095e4c":"# find the median\nmedian_pred = X_train['G3'].median()\n\n# create a list with all values as median\nmedian_preds = [median_pred for _ in range(len(X_test))]\n\n# store the true G3 values for passing into the function\ntrue = X_test['G3']","e1ff4181":"# Display the naive baseline metrics\nmb_mae, mb_rmse = evaluate_predictions(median_preds, true)\nprint('Median Baseline  MAE: {:.4f}'.format(mb_mae))\nprint('Median Baseline RMSE: {:.4f}'.format(mb_rmse))","44210396":"# Evaluate several ml models by training on training set and testing on testing set\ndef evaluate(X_train, X_test, y_train, y_test):\n    # Names of models\n    model_name_list = ['Linear Regression', 'ElasticNet Regression',\n                      'Random Forest', 'Extra Trees', 'SVM',\n                       'Gradient Boosted', 'Baseline']\n    X_train = X_train.drop('G3', axis='columns')\n    X_test = X_test.drop('G3', axis='columns')\n    \n    # Instantiate the models\n    model1 = LinearRegression()\n    model2 = ElasticNet(alpha=1.0, l1_ratio=0.5)\n    model3 = RandomForestRegressor(n_estimators=100)\n    model4 = ExtraTreesRegressor(n_estimators=100)\n    model5 = SVR(kernel='rbf', degree=3, C=1.0, gamma='auto')\n    model6 = GradientBoostingRegressor(n_estimators=50)\n    \n    # Dataframe for results\n    results = pd.DataFrame(columns=['mae', 'rmse'], index = model_name_list)\n    \n    # Train and predict with each model\n    for i, model in enumerate([model1, model2, model3, model4, model5, model6]):\n        model.fit(X_train, y_train)\n        predictions = model.predict(X_test)\n        \n        # Metrics\n        mae = np.mean(abs(predictions - y_test))\n        rmse = np.sqrt(np.mean((predictions - y_test) ** 2))\n        \n        # Insert results into the dataframe\n        model_name = model_name_list[i]\n        results.loc[model_name, :] = [mae, rmse]\n    \n    # Median Value Baseline Metrics\n    baseline = np.median(y_train)\n    baseline_mae = np.mean(abs(baseline - y_test))\n    baseline_rmse = np.sqrt(np.mean((baseline - y_test) ** 2))\n    \n    results.loc['Baseline', :] = [baseline_mae, baseline_rmse]\n    \n    return results","d0ecc660":"results = evaluate(X_train, X_test, y_train, y_test)\nresults","afb7804e":"plt.figure(figsize=(12, 8))\n\n# Root mean squared error\nax =  plt.subplot(1, 2, 1)\nresults.sort_values('mae', ascending = True).plot.bar(y = 'mae', color = 'b', ax = ax, fontsize=20)\nplt.title('Model Mean Absolute Error', fontsize=20) \nplt.ylabel('MAE', fontsize=20)\n\n# Median absolute percentage error\nax = plt.subplot(1, 2, 2)\nresults.sort_values('rmse', ascending = True).plot.bar(y = 'rmse', color = 'r', ax = ax, fontsize=20)\nplt.title('Model Root Mean Squared Error', fontsize=20) \nplt.ylabel('RMSE',fontsize=20)\n\nplt.show()","b9acf9e5":"**7. \u56db\u4e2a\u7279\u5f81\u53d8\u91cf\u76f8\u5173\u6027\u5206\u6790**","3bf3c984":"**10.1 \u968f\u673a\u68ee\u6797\u6a21\u578b\u8ba1\u7b97\u51c6\u786e\u7387**","d84f7c4d":"**5.\u9009\u62e9\u76f8\u5173\u6027\u6700\u9ad8\u7684\u56db\u4e2a\u56e0\u7d20\u8fdb\u884c\u6df1\u5ea6\u7814\u7a76**","e3710e01":"**1.\u5bfc\u5165\u6240\u9700\u51fd\u6570\u5e93**","5358b27a":"**8. \u9009\u62e9\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u505a\u56de\u5f52\u5206\u6790**","6f8438c9":"### Plotting the distribution rather than statistics would help us better understand the data","2ac1f60f":"## Higher education\n\nHigher education was a categorical variable with values yes and no. Since we used one hot encoding it has been converted to 2 variables. So we can safely eliminate one of them (since the values are compliments of each other). We will eliminate higher_no, since higher_yes is more intuitive.","785b370b":"**10.3 SVM\u6a21\u578b\u8ba1\u7b97\u51c6\u786e\u7387**","11f652bf":"## Correlation\n\nNext we find the correlation between various features and the final grade.\n \n### Note: This correlation is only between numeric values","6042e75c":"**3.\u6210\u7ee9\u5206\u5e03\u60c5\u51b5**","1d48e71b":"## Most students are from urban ares, but do urban students perform better than rurual students?","fd1d3b54":"**9. \u4f7f\u7528\u903b\u8f91\u56de\u5f52\u5206\u7c7b\u6a21\u578b\u8ba1\u7b97\u51c6\u786e\u7387**","58cba8d7":"# Predicting the final grade of a student\n\nThe data used is from a Portuguese secondary school. The data includes academic and personal characteristics of the students as well as final grades. The task is to predict the final grade from the student information. (Regression)\n\n### [Link to dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/student+performance)\n\n### Citation:\n\nP. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.\n[Web Link](http:\/\/www3.dsi.uminho.pt\/pcortez\/student.pdf)\n\n### Reference [article](\/home\/dipamvasani7\/Desktop\/Ubuntu\/jupyter_notebooks\/data)","06cb1508":"**11.\u4e0d\u540c\u6a21\u578b\u51c6\u786e\u7387\u5bf9\u6bd4**","f7c3f2bb":"## Does having a romantic relationship affect grade?\n\nAgain because of one hot encoding we have our variable called romantic_no which is slightly less intuitive but I am going to stick with it. Keep in mind that:\n\n- romantic_no = 1 means NO romantic relationship\n- romantic_no = 0 means romantic relationship","4d905346":"## Other features\n\nIt might not be wise to analyse every feature so I will find the features most correlated to the final grade and spend more time on them.","e89ec0ff":"### Histogram might be more useful to compare different ages","fdd9e84a":"# Modeling\n\n### We can create a model in 3 ways\n\n1. Binary classification\n    - G3 > 10: pass\n    - G3 < 10: fail\n2. 5-level classification based on Erasmus grade conversion system\n    - 16-20: very good\n    - 14-15: good\n    - 12-13: satisfactory\n    - 10-11: sufficient\n    -  0-9 : fail\n3. Regression (Predicting G3)\n\n### We will be using the 3rd type","de9df84e":"**6.\u76f8\u5173\u6027\u56e0\u7d20\u70ed\u56fe**","74784aaf":"\n\nThis plot does not tell us much. What we should really plot is the distribution of grade.\n\n","0e1b6b0e":"### Going out with friends","67ce85df":"The ages seem to be ranging from 15 - 19. The students above that age may not necessarily be outliers but students with year drops. Also the gender distribution is pretty even.","0e053fa1":"## Does age have anything to do with the final grade?","901d4937":"### Example of one hot encoding","1ad6955f":"The graph shows a slight downward trend","23d052ea":"### None of the variables has null values so maybe grade 0 does not mean null after all","80930ffa":"## Count of students from urban and rural areas","3376f24a":"**4.\u6210\u7ee9\u76f8\u5173\u6027\u5206\u6790**","404110f4":"**10.\u4f7f\u7528\u591a\u79cd\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u7387\u5206\u6790**","c7c9c45e":"### Grades according to the number of students who scored them","55a322d5":"### MAE - Mean Absolute Error\n### RMSE - Root Mean Square Error","ec2c1058":"### Naive baseline is the median prediction","c2a57633":"## Applying one hot encoding to our data and finding correlation again!\n\n\n### Note: \nAlthough G1 and G2 which are period grades of a student and are highly correlated to the final grade G3, we drop them. It is more difficult to predict G3 without G2 and G1, but such prediction is much more useful because we want to find other factors affect the grade.","3119a3f1":"## Reason to choose this school","3f7226cc":"## Checking the distribution of Age along with gender","a318ba12":"# Encoding categorical variables\n\nA machine learning model cannot deal with categorical variables (except for some models). Therefore we need to find a way to encode them (represent as numbers) before handing them to the model.\n\n## Label encoding\n\nThis method involves assigning one label for each category\n\n| Occupation    | Label         |\n| ------------- |:-------------:|\n| programmer    | 0             |\n| data scientist| 1             |\n| Engineer      | 2             |\n\n\n\nThe problem with label encoding is that the assignment of integers is random and changes every time we run the function. Also the model might give higher priority to larger labels. Label encoding can be used when we have only 2 unique values.\n\n## One hot encoding\n\nThe problem with label encoding is solved by one hot encoding. It creates a new column for each category and uses only binary values. The downside of one hot encoding is that the number of features can explode if the categorical variables have many categories. To deal with this we can perform PCA (or other dimensionality reduction methods) followed by one hot encoding.\n\n| Occupation    | Occupation_prog| Occupation_ds | Occupation_eng |\n| ------------- |:-------------: |:-------------:|:-------------: |\n| programmer    | 1              | 0             | 0              |\n| data scientist| 0              | 1             | 0              |\n| Engineer      | 0              | 0             | 1              |","9b4dc531":"**10.4 \u4e8c\u53c9\u6811\u6a21\u578b\u8ba1\u7b97\u51c6\u786e\u7387**","49b1d21c":"As we can see there are only 2 points in family_ed = 1 hence our conclusion was faulty.","e6800cc8":"There seems to be a slight trend that with the increase in family education the grade moves up (apart from the unusual high value at family_ed = 1 (maybe students whose parents did not get to study have more motivation)\n\n### Note:\n\nI prefer swarm plots over box plots because it is much more useful to see the distribution of data (and also to spot outliers)","90c8a41c":"# Now we will analyse these variables and then train a model","8f594082":"## Hmmmmm!\n\nSomething seems off here. Apart from the high number of students scoring 0, the distribution is normal as expected.\nMaybe the value 0 is used in place of null. Or maybe the students who did not appear for the exam, or were not allowed to sit for the exam due to some reason are marked as 0. We cannot be sure. Let us check the table for null values","765b6b5e":"**10.2 \u6734\u7d20\u8d1d\u53f6\u65af\u6a21\u578b\u8ba1\u7b97\u51c6\u786e\u7387**","0d30b382":"The graph shows that on there is not much difference between the scores based on location.","2504d3ad":"## Next let us take a look at the gender variable","dfeb831e":"**2.\u8bf4\u660e\u8be5\u6570\u636e\u96c6\u57fa\u672c\u60c5\u51b5**","c6c49411":"# Final grade distribution","94f6b716":"### Student with less previous failures usually score higher","dd3dff97":"Most students have an average score when it comes to going out with friends. (normal distribution)","a083ccd8":"We see that age 20 has only 3 data points hence the inconsistency in statistics. Otherwise there seems to be no clear relation of age or gender with final grade","5cf1fff0":"### We see that linear regression is performing the best in both cases"}}