{"cell_type":{"3fa51f59":"code","24065354":"code","5e1b5dee":"code","feebe498":"code","9c436b0e":"code","d06e2fa4":"code","4e6678a5":"code","9ef50e59":"code","6885eb90":"code","6434d87a":"code","5ae77651":"code","39497625":"code","fdab786f":"code","e333f37e":"code","353f5b5c":"code","59d75659":"code","08e7f6ab":"code","1f30f0a5":"code","9a88e50e":"code","362a48d8":"markdown","665e8652":"markdown","11fc842d":"markdown","b1f76f2f":"markdown","00e3f683":"markdown","71f74c46":"markdown"},"source":{"3fa51f59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","24065354":"def load_data():\n    df_train = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv',index_col='ID_code')\n    df_test = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv', index_col='ID_code')\n\n    \n    return df_train,df_test\n","5e1b5dee":"#Label encoding selected categorical columns, while leaving other columns as it is\nfrom sklearn import preprocessing\n\ndef label_encoding(sel_cat,inpX):\n    for col in sel_cat:\n        if col in inpX.columns:\n            le = preprocessing.LabelEncoder()\n            le.fit(list(inpX[col].astype(str).values))\n            inpX[col] = le.transform(list(inpX[col].astype(str).values))\n    return inpX\n","feebe498":"# Returns list of categorical columns, and part of dataset with only categorical columns\ndef categorical_cols(input_df):\n    # Selecting numeric columns in df_train\n    print(input_df.select_dtypes('object').columns)\n    sel_train = input_df.select_dtypes('object').columns.values\n    #print(type(sel_train))\n\n    train = input_df[sel_train]\n    #print(train.describe())\n    return sel_train, train","9c436b0e":"from sklearn.model_selection import train_test_split\n\n#features = sel_features+num_id+sel_cards\n#train = df_train[features]\ndef balanced_sampling(input_df, factor): \n    \n    train = numeric_cols(input_df)\n    y= train['target']\n    # Selecting target 1 and target 0  \n    X_target = train[train.target==1]\n    X_notarget= train[train.target==0]\n    total_target = X_target.shape\n    print(\"Target Size : \",total_target[1],total_target[0])\n    scale_factor = factor\n    X_notarget1=X_notarget.sample(scale_factor*total_target[0])\n    X=pd.concat([X_target,X_notarget1], ignore_index=True)\n    y= X['target']\n    print(X.shape)\n    print(X.sample(10))\n\n    #dropping target column from X\n    X.drop([\"target\"],axis=1,inplace=True)\n    \n    \n    ### Train-test split with Stratification\n    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,  test_size=0.25)\n    return X_train, X_test, y_train, y_test\n","d06e2fa4":"def numeric_cols(input_df):\n    # Selecting numeric columns in df_train\n    print(input_df.select_dtypes('number').columns)\n    sel_train = input_df.select_dtypes('number').columns.values\n    print(type(sel_train))\n\n    train = input_df[sel_train]\n    print(train.describe())\n    return train","4e6678a5":"def preprocess(inp):\n# Filling 0.0 in place of NaN\n    inp.fillna(0.0, inplace=True)\n    inp.sample(10)\n    return inp ","9ef50e59":"df_train,df_test = load_data()\nprint(f'Train dataset has {df_train.shape[0]} rows and {df_train.shape[1]} columns.')\nprint(f'Test dataset has {df_test.shape[0]} rows and {df_test.shape[1]} columns.')","6885eb90":"X_train, X_test, y_train, y_test = balanced_sampling(df_train,3)","6434d87a":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error","5ae77651":"model=xgb.XGBClassifier(tree_method='gpu_hist',random_state=1,learning_rate = 0.01,max_depth = 4, subsample = 0.8, colsample_bytree =  1, gamma = 1)\n#model.fit(X_train, y_train)\n#model.score(X_test,y_test)","39497625":"param_grid = {\n    'n_estimators': [2000,4000]   \n}\n\ngbm = GridSearchCV(model, param_grid, cv=3)\ngbm.fit(X_train, y_train)","fdab786f":"print(\"Best parameters set found on development set:\")\nprint()\nprint(gbm.best_params_)\nprint()\nprint(\"Grid scores on development set:\")\n","e333f37e":"# prediction\ny_pred=gbm.predict(X_test)","353f5b5c":"from sklearn import metrics\ndef eval2(y_test,y_pred):\n    # Model Accuracy, how often is the classifier correct?\n    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n    return 0","59d75659":"eval2(y_test,y_pred)","08e7f6ab":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report \ndef performance_analysis(y_test,y_pred):\n    results = confusion_matrix(y_test, y_pred) \n    print('Confusion Matrix :')\n    print(results) \n    print('Accuracy Score :',accuracy_score(y_test, y_pred))\n    print ('Report : ')\n    print (classification_report(y_test, y_pred))\n    return\n\nperformance_analysis(y_test,y_pred)","1f30f0a5":"def sub3(inpt,clf):\n    # Use df_test with selected columns for final submission\n    y_preds = clf.predict_proba(inpt)[:,1] \n    sample_submission = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/sample_submission.csv', index_col='ID_code')\n    sample_submission['target'] = y_preds\n    sample_submission.to_csv('santander_xgcv_2.csv')\n    return 0\n","9a88e50e":"sub3(df_test,gbm)","362a48d8":"### Data Description\n\nYou are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column.\n\nThe task is to predict the value of target column in the test set.\n\nTraining data contains:\n\n\u2022 ID_code (string);\n\n\u2022 target;\n\n\u2022 200 numerical variables, named from var_0 to var_199;\n\nTest data contains contains:\n\n\u2022 ID_code (string);\n\n\u2022 200 numerical variables, named from var_0 to var_199;\n\nThere are no missing data in train and test datasets. Let's check the numerical values in train and test dataset.\n\n### File descriptions\n\ntrain.csv - the training set.\n\ntest.csv - the test set. \n\nThe test set contains some rows which are not included in scoring.\n\nsample_submission.csv - a sample submission file in the correct format.\n","665e8652":"# Introduction\n\nThis is a simple XGBoost based notebook for predicting customer transactions without using any EDA. It follows a simple pipeline architecture with data loading, balancing imbalanced data, and gridsearch cross validation. You can use similar approach for finding your first quick and dirty but robust solution for similar problems. Solution can then be iteratively refined as you learn more about the problem and data science approaches that can be used for problem solving.","11fc842d":"# Dealing with Imbalanced Sampling","b1f76f2f":"# Load Data","00e3f683":"## Conclusion\n\nWe can see that we get poor recall and f-scores for minority class. This can be addressed by increasing representation of the minority class through SMOTE or some other suitable techniques. \n\n## Note\n\nPlease share, upvote and comment to help me create and share more content for the community.\nThank you all.","71f74c46":"# XGBoost Pipeline with Cross Validation"}}