{"cell_type":{"0312ac9b":"code","64d1e1b3":"code","2a3d652e":"code","1cddacca":"code","b295e9c9":"code","93f2565a":"code","c37ab194":"code","769dd443":"code","4d1b2d27":"code","18acb1d4":"code","764b7137":"code","469ecf7b":"markdown","57dd1c7c":"markdown","3dbdbced":"markdown","86f67f84":"markdown","a56fbe22":"markdown","4000412c":"markdown","ae564fd6":"markdown","1193af98":"markdown"},"source":{"0312ac9b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf # tensorflow\nimport glob \nimport re # regular expression\n# Input data files are available in the \"..\/input\/\" directory.\n\nfrom sklearn.preprocessing import MinMaxScaler","64d1e1b3":"'''\n  This will load the data into memory (without any processing)\n  Each sample will be returned in a dictionary that contains:\n    - the sample_id\n    - the label (if its a train sample)\n    - a pandas dataframe with all the accelerometer\/gyro data for this sample\n    \n  For a list of samples (dictionaries) will be returned seperatelly for train\/test set\n    \n'''\n\n\n\ndef load_data():\n    train_data = [] # we will store the train data here\n    test_data = [] # we will store the test data here\n    sizes = []\n    \n    # read training labels\n    labels = pd.read_csv(\"..\/input\/smartphone_data\/train_labels.csv\")\n    label_dictionary = labels.set_index(\"id\")[\"label\"].to_dict() # get the labels as a dictionary by sample ID\n    \n    # read training data. Iterate files in the train folder\n    for filename in glob.glob(\"..\/input\/smartphone_data\/train\/train_*.csv\"): # iterate through all the files\n        # extract the sample id from the file name\n        m = re.search(r\"\\S+\\_(\\d+).csv\", filename)\n        if m is not None:\n            # if the filename is as expected then store the sample\n            sample_id = int(m.group(1)) # get the group from the filename (regular expression)\n            label = label_dictionary[sample_id] # get the training label from the dictionary created above\n            sample_data = pd.read_csv(filename) # finally read the accelerometer data\n            # store the data in a dictionary for further processing later\n            sample = {\"sample_id\": sample_id, \"label\": label, \"sample_data\" : sample_data}\n            train_data.append(sample)\n      \n    # similarly, load data for test set (notice that no labels exist here)\n    for filename in glob.glob(\"..\/input\/smartphone_data\/test\/test_*.csv\"): # iterate through all the files\n        # extract the sample id from the file name\n        m = re.search(r\"\\S+\\_(\\d+).csv\", filename)\n        if m is not None:\n            # if the filename is as expected then store the sample\n            sample_id = int(m.group(1)) # get the group from the filename (regular expression)\n            label = None\n            sample_data = pd.read_csv(filename) # finally read the accelerometer data\n            # store the data in a dictionary for further processing later\n            sample = {\"sample_id\": sample_id, \"label\": label, \"sample_data\" : sample_data}\n            test_data.append(sample)\n            sizes.append(sample_data.shape[0])\n    return train_data, test_data,sizes\n        \n\n# load data\ntrain_data, test_data, sizes = load_data()","2a3d652e":"pd.Series(sizes).hist(bins=50)","1cddacca":"# lets print an example\nsample = train_data[3]\nprint (sample['label'])\nprint (sample['sample_id'])\ndisplay(sample['sample_data'].head(3))\nsample['sample_data'][['userAcceleration.x','userAcceleration.y','userAcceleration.z']].plot(figsize=(10,5))","b295e9c9":"'''\n  This function will be used to process the data and return your model features\n  Feel free to modify this, but in theory you should return:\n  - For training: \n     - X a n-dimentional numpy array with the training input (or pandas array if only 2 Dimentions are used)\n     - Y (if available) the  labels for these samples. Notice that the order of samples should be the same as train_X. For the test set this will be NaN\n     - original_sample_ids this is to keep track of which sample corresponds to which ID in the original filenames. You will need this to align the results with the submission IDs\n\n  Feature construction:\n      In this example a simple mean and standard deviation is returned for the acceleration values per sample. \n      Therefore, the whole time-series is collapsed into a few numbers. \n      As a results train_X is a 2-D array that fontains one line per sample with 6 fatures (columns)\n  \n  WHAT TO MODIFY:\n      This is one of the functions that you will need to modify. \n      For RNNs\/CNNs you might want to return the whole time-series as numpy arrays. In this case you would need\n      to return 3D numpy arrays in the format (samples, time, features)\n      You can also choose to return a second X-array if you plan to have a multi-input model\n'''\n\nlabels = [\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\n\n\ndef process_sample_data(data, labels):\n    X = []\n    y = []\n    original_sample_ids = []\n    \n    \n    \n    for d in data:\n        original_sample_id = d['sample_id']\n        label     = d['label']\n        sample_data = d['sample_data']\n       \n        \n        #sample_data = pd.DataFrame(scaler.transform(sample_data), columns = d['sample_data'].columns)\n         \n       \n        # Extracting features. In this example we collapse the time series into mean and std for the accelerometer values\n        #features = sample_data[['userAcceleration.x', 'userAcceleration.y', 'userAcceleration.z']].agg(['mean','std']).unstack().tolist()\n        padded = tf.keras.preprocessing.sequence.pad_sequences(sample_data.values.T, 12*1024, dtype=float).T\n        #padded = tf.keras.preprocessing.sequence.pad_sequences([sample_data['userAcceleration.x']], 200, dtype=float).T\n        X.append(padded)\n        y.append(label)\n        original_sample_ids.append(original_sample_id)\n        \n    # convert to X to numpy array (you can also directly store your features in numpy)\n    X = np.array(X)\n    \n    # one-hot encode Y (expected by softmax classification)\n    if labels is None:\n        y = None\n    else:\n        y = pd.get_dummies(y)[labels] # get dummies performs one-hot encode. We also use the \"labels\" list to make sure the order is as expected\n        y = np.array(y)\n    \n    return X, y, original_sample_ids\n        \n  \n\ntrain_X, train_y, train_sample_ids =  process_sample_data(train_data, labels)\ntest_X, _ , test_sample_ids =  process_sample_data(test_data, None)","93f2565a":"print (train_X.shape)  # in this example we have 255 sample to train with 6 features each\nprint (train_y.shape)  # we have 6 possible labels for each sample (\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\")\n\nprint (test_X.shape)   # we have 105 test samples. We don't have the labels for those. our model has to guess them from the input data","c37ab194":"def create_keras_model( num_classes):\n    model = tf.keras.Sequential()\n    # Adds a densely-connected layer with 64 units to the model:\n    \n    model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=5))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation('relu'))\n    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n    model.add(tf.keras.layers.Dropout(0.2))\n    \n    model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=5,))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation('relu'))\n    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n    model.add(tf.keras.layers.Dropout(0.2))\n    \n    model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=5,))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation('relu'))\n    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n    model.add(tf.keras.layers.Dropout(0.1))\n    \n    model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=3,))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation('relu'))\n    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n    model.add(tf.keras.layers.Dropout(0.1))\n    \n    model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=3,))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation('relu'))\n    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n    model.add(tf.keras.layers.Dropout(0.1))\n    \n    \n    model.add(tf.keras.layers.Conv1D(filters=16, kernel_size=3,))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation('relu'))\n    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n    model.add(tf.keras.layers.Dropout(0.1))\n    \n    model.add(tf.keras.layers.Conv1D(filters=16, kernel_size=3,))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation('relu'))\n    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n    model.add(tf.keras.layers.Dropout(0.1))    \n    \n    \n    model.add(tf.keras.layers.CuDNNLSTM(64))\n    model.add(tf.keras.layers.Dropout(0.3))\n    model.add(tf.keras.layers.Dense(16, activation='relu'))\n    # Add a softmax layer with num_classes output units:\n    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n    \n    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n    \n    return model\n","769dd443":"# simple model training. \n# you might want to avoid overfitting by monitoring validation loss and implement early stopping, etc\ndef train_model(model, X, y):\n    model.fit(X, y, epochs=100, batch_size=32, validation_split=0.0)\n\n    \n    \ndef predict(model, X):\n    y_pred = model.predict(X, batch_size=32)\n    return y_pred","4d1b2d27":"# running it\nmodel = create_keras_model(6)\ntrain_model(model, train_X, train_y)","18acb1d4":"model.summary()","764b7137":"y_pred = predict(model, test_X)\n\n# convert predictions to the kaggle format\ny_pred_numerical = np.argmax(y_pred, axis = 1) # one-hot to numerical\ny_pred_cat = [labels[x] for x in y_pred_numerical] # numerical to string label\n\n# generate the table with the correct IDs for kaggle.\n# we get the correct sample ID from the stored array (test_sample_ids)\nsubmission_results = pd.DataFrame({'id':test_sample_ids, 'label':y_pred_cat})\nsubmission_results.to_csv(\"submission.csv\", index=False)\n","469ecf7b":"# Lets train !","57dd1c7c":"# defining the train and predict functions","3dbdbced":"# Here you define your model.\nYou will modify this based on the model you want to try","86f67f84":"# feature extraction\n\nHere you should format the data into a format that is suitable for your model. Feature engineering and pre-processing should happen here","a56fbe22":"\n# Plotting a sample","4000412c":"# Loading data\nFeel free to experiment with your own loader. This will just load all the data into two dictionaries for train and test sets","ae564fd6":"# Checking data format \nIn this example we have a 2D array. One row per sample and 6 features per sample (accel.x_mean, accel.x.std, ...., accel.z_mean, accel.z_std)","1193af98":"# predict with the trained model"}}