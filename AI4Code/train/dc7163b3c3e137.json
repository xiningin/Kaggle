{"cell_type":{"4837cc24":"code","a79dc97a":"code","073d2c06":"code","d8c76134":"code","cb7927ec":"code","1f8c3e45":"code","8ac6cca4":"code","ae418fd5":"code","acb9ec2f":"code","1828adf9":"code","a72c3946":"code","cee520b5":"code","270ddf21":"code","ddf99e2b":"code","cde172d5":"code","3bb175a6":"code","d677e2e8":"code","4466e3f1":"code","d77a2426":"markdown","6186f763":"markdown","75df7160":"markdown","bbdee0c6":"markdown","cd396583":"markdown","f29258a8":"markdown","15a3bc85":"markdown","3a302953":"markdown","07a1db2d":"markdown","b82c32b9":"markdown","79ee6162":"markdown","bfd27ce4":"markdown","334d4768":"markdown","a1dfc5b0":"markdown","5b4c1778":"markdown","d284de53":"markdown","3e1e13f4":"markdown","6a5fe145":"markdown"},"source":{"4837cc24":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder","a79dc97a":"file_path = \"\/kaggle\/input\/salary-predictor\/wage_predict.csv\"\nfd = pd.read_csv(file_path, sep=',')\nfd.info()### get info about the data","073d2c06":"fd.head()","d8c76134":"fd.describe()","cb7927ec":"label_encoder = LabelEncoder()\nfd.iloc[:,0] = label_encoder.fit_transform(fd.iloc[:,0]).astype('int32')","1f8c3e45":"corr = fd.corr()\nsns.heatmap(corr)","8ac6cca4":"columns = np.full((corr.shape[0],), True, dtype=bool) # array of descriptor per column. default descriptor value is True\nfor i in range(corr.shape[0]): # False for each column with high correlation\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.9:\n            if columns[j]:\n                columns[j] = False\nselected_columns = fd.columns[columns]\nfd = fd[selected_columns]","ae418fd5":"cv = KFold(n_splits=10, random_state=1, shuffle=True)\nmin_error = np.Inf\nmodels_params = 0","acb9ec2f":"def seperate_target_from_data(fd):\n    y = fd['monthly_salary'].values  # get pandas values as np\n    fd = fd.drop(columns=['monthly_salary'])\n    X = fd.loc[:, 'occupation':].values\n    return X, y","1828adf9":"def prepocess_dataset(x):\n    # scale while keeping ordinal data\n    min_max_scaler = MinMaxScaler()\n    min_max_scaler.fit(x)\n    x = min_max_scaler.transform(x)\n    return x","a72c3946":"def train_model(x_train, y_train, model):\n    model.fit(x_train, y_train)\n    return model","cee520b5":"def predict_model(x_test, y_test, model):\n    y_predict = model.predict(x_test)\n    error = calc_error(\"GradientBoostingRegressor\", y_test, y_predict)\n    return error","270ddf21":"def calc_error(title, expected, predicted):\n    # calculate errors\n    errors = mean_absolute_error(expected, predicted)\n    return errors","ddf99e2b":"def calc_min_error(error_val, min_error):\n    save_this_value = False\n    if min_error > error_val:\n        min_error = error_val\n        save_this_value = True\n    return min_error, save_this_value","cde172d5":"def evaluate_model(model, kf, learning_rate, depth, fd, min_error):\n    X, y = seperate_target_from_data(fd)\n    local_min = np.inf\n    model_params = None\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        X_train = prepocess_dataset(X_train)\n        model = train_model(X_train, y_train, model)\n        error_val = predict_model(X_train, y_train, model)\n        min_error, save_this_value = calc_min_error(error_val, min_error)\n        if save_this_value:\n            model_params = (learning_rate, depth)\n        local_min , save_this_value = calc_min_error(error_val, local_min)\n        if save_this_value:\n            local_model_params = (learning_rate, depth)    \n    # report error\n    print(type(model).__name__, \" mae =  \", local_min)\n    return min_error, model_params","3bb175a6":"lr = 0\nd= 0\nmodel = LinearRegression()\nmin_error, _ = evaluate_model(model, cv, lr, d, fd, min_error)\nprint(\"best mean_absolute_error (mae) after  LinearRegression = \", min_error)","d677e2e8":"for d in range(3,20):\n    print(\"max_depth= \", d )\n    model = RandomForestRegressor(max_depth=d, random_state=0)\n    min_error, model_params = evaluate_model(model, cv, lr, d, fd, min_error)\n    if model_params != None:\n        best_model_params = model_params\nprint(\"best mean_absolute_error (mae) after RandomForestRegressor = \", min_error, \" with params: \", best_model_params)","4466e3f1":"for lr in (0.1, 0.001, 0.3, 0.2):\n        for d in range(3,9):\n            print(\"learning_rate = \", lr, \"max_depth= \", d )\n            model = GradientBoostingRegressor(learning_rate = lr, max_depth = d)\n            min_error, models_params = evaluate_model(model, cv, lr, d, fd, min_error)\n            if model_params != None:\n                best_model_params = model_params\nprint(\"best GradientBoostingRegressor mean_absolute_error (mae)= \", min_error, \" with params: \", best_model_params)","d77a2426":"### evaluate_model function:\niterates over all fold from the kfoldcross validation function. It preprocess each x_traun dataset, then it train the model with it. After the model is trained, it get predictions from it. The parameters of the model with the minimum error are saved, and we got best model from all the models with different learning-rate and depth. We also get the minimum error each model achieved, for choosing the best model.  ","6186f763":"### calc_error function:\ncalculates the error between predicted results and expected results using mean-square-error","75df7160":"### first try, we use  linear regression model to get good predictions","bbdee0c6":"### prepocess_dataset function:\nnormalize all the features. I choose to normalize the input features with Min-Max scaler. By doing so, all features will be transformed into the range [0,1] meaning that the minimum and maximum value of a feature is going to be 0 and 1, respectively. The main reason of this scaling is to avoid bias. ","cd396583":"### seperate_target_from_data function:\nsplit the dataset into features and target, so we can compare predicted results to the expected. after examining the dataset I realize that industry feature has a negligible influence on the target, thus I didn't include it in the dataset.","f29258a8":"### In the second try we use RandomForest to get better results. we iterate over tree depth to find the best model results ","15a3bc85":"# encode categorical column","3a302953":"# Define global vriables\nThe method of splitting dataset into train and test sets, and then evaluating accuracy by them, is not very reliable. The main reason is the accuracy obtained for one test set can be very different to the accuracy obtained for a different test set. K-fold Cross Validation(CV) provides a solution to this problem by dividing the data into folds and ensuring that each fold is used as a testing set at some point. We will use this method to test the predictor model. ","07a1db2d":"# Conclusion\n### we got the best model that can predict salary according to features. \nfor this dataset it is gradient boosting with learning rate of 0.2 and tree max_depth of 8.","b82c32b9":"# Check corelation to reduce dimensionality\nreduce high correlated columns","79ee6162":"# Salary prediction\n\nIn this notebook, I will try to create salary predictor based on answers given to 2014 Survey of Adult Skills (PIAAC) which was conducted globally..\n\nFirst, we'll import usefull packages.\nThen, we'll load the data, and preprocess it.\nWith the data we'll train two models with different parameters values and chooose the one with the best results.\n\n\n# Import needed packages","bfd27ce4":"# statistical data\nExamine the dataset and check for imbalance","334d4768":"### predict_model function:\ncalculates predicted results. It returns error between predicted results and expected results","a1dfc5b0":"# Load dataset\n\ncheck its description, and examine some samples","5b4c1778":"### calc_min_error function:\nupdates min_error if error_val is smaller. It returns min_error and a flag, save_this_value that signals to save model parameters.  ","d284de53":"### train_model function:\ntrains the model with the current chosen data by kfold cross validation.","3e1e13f4":"### In the third try we examine gradient boosting model in which we iterates over learning rate and max depth of trees. Those values create different models to be examined. ","6a5fe145":"Next, we compare the correlation between features and remove one of two features that have a correlation higher than 0.9"}}