{"cell_type":{"c2a3e2c2":"code","ced73ea3":"code","f1054348":"code","c2421a72":"code","f0baeba0":"code","d30ecde6":"code","2388a704":"code","7377f5c3":"code","41bc1932":"code","bf85a258":"code","f647b1b1":"code","279d646e":"code","dd026a41":"code","cc3d0e35":"code","62027638":"code","bf0ec35c":"code","50f0e27e":"markdown","0f4b9a75":"markdown","17fbf2c1":"markdown","a973a8e4":"markdown"},"source":{"c2a3e2c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        file = (os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ced73ea3":"import tensorflow\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.applications.vgg19 import VGG19\n","f1054348":"image_size = [224, 224]","c2421a72":"vgg = VGG19(input_shape = image_size + [3], weights = 'imagenet', include_top =  False)\n","f0baeba0":"from glob import glob\nfolders = glob('..\/input\/new-plant-diseases-dataset\/New Plant Diseases Dataset(Augmented)\/New Plant Diseases Dataset(Augmented)\/train\/*')\n","d30ecde6":"len(folders)","2388a704":"\nfor layer in (vgg.layers):\n    layer.trainable = False","7377f5c3":"x = Flatten()(vgg.output)","41bc1932":"prediction = Dense(len(folders), activation = 'softmax')(x)","bf85a258":"from keras import Model\n\nmodel = Model(inputs = vgg.input, outputs = prediction)\n","f647b1b1":"model.summary()","279d646e":"model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","dd026a41":"from keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   fill_mode='nearest')\n\nvalid_datagen = ImageDataGenerator(rescale=1.\/255)\n\nbatch_size = 150\nbase_dir = \"..\/input\/new-plant-diseases-dataset\/new plant diseases dataset(augmented)\/New Plant Diseases Dataset(Augmented)\"\n\ntraining_set = train_datagen.flow_from_directory(base_dir+'\/train',\n                                                 target_size=(224, 224),\n                                                 batch_size=batch_size,\n                                                 class_mode='categorical')\n\nvalid_set = valid_datagen.flow_from_directory(base_dir+'\/valid',\n                                            target_size=(224, 224),\n                                            batch_size=batch_size,\n                                            class_mode='categorical')","cc3d0e35":"device_name = tensorflow.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","62027638":"mod = model.fit_generator(\n  training_set,\n  validation_data= valid_set,\n  epochs=10,\n  steps_per_epoch=len(training_set),\n  validation_steps=len(valid_set)\n)","bf0ec35c":"import matplotlib.pyplot as plt\nplt.plot(mod.history['loss'], label='train loss')\nplt.plot(mod.history['val_loss'], label='val loss')\nplt.legend()\nplt.show()\nplt.savefig('LossVal_loss')\n\n# plot the accuracy\nplt.plot(mod.history['accuracy'], label='train acc')\nplt.plot(mod.history['val_accuracy'], label='val acc')\nplt.legend()\nplt.show()\nplt.savefig('AccVal_acc')","50f0e27e":"model=Sequential()\n#1st\nmodel.add(Conv2D(filters = 96, kernel_size = (11,11), strides = (4,4), padding = \"valid\", activation = \"relu\",\n                input_shape = (224,224,3)))\n#Pool\nmodel.add(MaxPooling2D(pool_size = (3,3), strides = (2,2)))\nmodel.add(BatchNormalization())\n\n#2nd\nmodel.add(Conv2D(filters = 256, kernel_size = (5,5), padding = \"same\"))\n#Pool\nmodel.add(MaxPooling2D(pool_size = (3,3), strides = (2,2)))\nmodel.add(BatchNormalization())\n\n#3rd\nmodel.add(Conv2D(filters = 384, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n\n\n#4th\nmodel.add(Conv2D(filters = 384, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n\n#5th\nmodel.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\nmodel.add(MaxPooling2D(pool_size = (3,3), strides = (2,2)))\nmodel.add(BatchNormalization())\n\nmodel.add(Flatten())\n\n#1st Flatten\nmodel.add(Dense(4096, input_shape = (227,227,3), activation = \"relu\"))\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\n\n#2nd Flatten\nmodel.add(Dense(4096, activation = \"relu\"))\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\n\n#3rd Flatten\nmodel.add(Dense(1000, activation = \"relu\"))\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\n\n#output layer\nmodel.add(Dense(38, activation = \"softmax\"))","0f4b9a75":"# Compiling the Model\nfrom keras import optimizers\nmodel.compile(optimizer=optimizers.SGD(lr=0.001, momentum=0.9, decay=0.005),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","17fbf2c1":"from keras import layers\nfor i, layer in enumerate(model.layers):\n   print(i, layer.name)","a973a8e4":"model.summary()"}}