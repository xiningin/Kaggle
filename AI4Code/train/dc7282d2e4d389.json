{"cell_type":{"9012a210":"code","d4bd58de":"code","e31f980d":"code","b457fd98":"code","dde3f6e2":"code","3702ce9d":"code","8ed6a07e":"code","2995fb51":"code","c4665a9b":"code","ed40bef7":"code","07ee3697":"code","b7753489":"code","1df49132":"code","6d109b05":"code","a19c93ea":"code","171ec917":"markdown","ef05b7a5":"markdown","8200cf1c":"markdown","2f2d2d18":"markdown","a25670d8":"markdown","185cf9ea":"markdown"},"source":{"9012a210":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4bd58de":"import torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\n# from torch.utils.data.sampler import SubsetRandomSampler\nfrom PIL import Image\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torch import nn\nfrom matplotlib import pyplot as plt\n\nDATA_ROOT = '..\/input\/buaa-animal-enhanced\/train_stronger'\nTEST_ROOT = '..\/input\/buaa-advance-in-ai-animals\/Animals Dataset\/test'\nSPLIT_RATE = 0.8\nBATCH_SIZE = 100\nLEARNING_RATE = 1e-3\nEPOCHS = 20\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nIMG_MEAN = [0.485, 0.456, 0.406]\nIMG_STD = [0.229, 0.224, 0.225]","e31f980d":"train_transforms = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(IMG_MEAN, IMG_STD)\n])\n\n# val_transforms = transforms.Compose([\n#     transforms.Resize((224,224)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(IMG_MEAN, IMG_STD)\n# ])\n\nfull_dataset = ImageFolder(DATA_ROOT, train_transforms)\nprint(\"train_data\u5927\u5c0f\uff1a\",full_dataset[0][0].size())\n\ntrain_size = int(len(full_dataset)*SPLIT_RATE)\ntrain_dataset ,val_dataset = random_split(full_dataset, [train_size, len(full_dataset)-train_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)","b457fd98":"model_ft = models.densenet121(pretrained=True)\n# \u5c06\u6240\u6709\u7684\u53c2\u6570\u5c42\u8fdb\u884c\u51bb\u7ed3\nfor param in model_ft.parameters():\n    param.requires_grad = False\n    \n# \u6253\u5370\u4e0bFC\u5c42\u7684\u4fe1\u606f\u770b\u770b\u5f53\u524d\u7684\u5206\u7c7b\u60c5\u51b5\nprint(model_ft.classifier)","dde3f6e2":"in_features = model_ft.classifier.in_features\nmodel_ft.classifier = nn.Linear(in_features, 22) # \u4e00\u5c42fc\u5c31\u591f\u4e86\uff0c\u4e0d\u7136\u4f1a\u8fc7\u62df\u5408\nmodel_ft.to(DEVICE)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model_ft.parameters(), lr=LEARNING_RATE)","3702ce9d":"def train(model, device, train_loader, epoch):\n    model.train()\n    train_loss = 0.0\n    for batch_idx, data in enumerate(train_loader):\n        X, y = data\n        X, y = X.to(device), y.to(device) # \u79fb\u5230GPU\u4e0a\n        optimizer.zero_grad()\n        outputs = model(X)\n        loss = criterion(outputs, y)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.data.item()\n        if (batch_idx+1) % 50 == 0:\n            print ('Epoch : %d\/%d, Iter : %d\/%d,  Loss: %.4f'%(epoch+1, EPOCHS, batch_idx+1, len(train_loader), loss.data.item()))\n    \n    # \u4e00\u4e2aepoch\u7ed3\u675f\uff0c\u8f93\u51fa\u5e76\u4fdd\u5b58train_loss\n    train_loss \/= len(train_loader) # \u5bf9\u6bcf\u4e2abatch\u53d6\u5e73\u5747\n    train_loss_history.append(train_loss)\n    print ('Train Epoch: {}\\t Loss: {:.4f}'.format(epoch+1,loss.item()))","8ed6a07e":"def validate(model, device, val_loader):\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    with torch.no_grad():\n        for batch_idx,data in enumerate(val_loader):          \n            X, y = data\n            X, y = X.to(device), y.to(device) # \u79fb\u5230GPU\u4e0a\n            optimizer.zero_grad()\n            outputs = model(X)\n            loss = criterion(outputs, y)\n            \n            val_loss += loss.item() # sum up batch loss\n            preds = torch.argmax(outputs, dim=1)\n            correct += (preds == y).sum()\n            \n    val_loss \/= len(val_loader)\n    val_loss_history.append(val_loss) # \u5bf9\u6bcf\u4e2abatch\u53d6\u5e73\u5747\n    val_acc_history.append(100. * correct \/ len(val_loader.dataset))\n    print('\\nValidation: Average loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n        val_loss, correct, len(val_loader.dataset),\n        100. * correct \/ len(val_loader.dataset)))","2995fb51":"train_loss_history = []\nval_loss_history = []\nval_acc_history = []\nfor epoch in range(EPOCHS):\n    %time train(model=model_ft,device=DEVICE, train_loader=train_loader,epoch=epoch)\n    validate(model=model_ft, device=DEVICE, val_loader=val_loader)","c4665a9b":"from matplotlib import pyplot as plt\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.plot(val_acc_history)\nplt.show();","ed40bef7":"plt.plot(train_loss_history, '-bx')\nplt.plot(val_loss_history, '-rx')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['Train', 'Validation'])","07ee3697":"torch.save(model_ft.state_dict(), \"model_weights.pth\")\nprint(\"Saved PyTorch Model State to model_weights.pth\")","b7753489":"# \u52a0\u8f7d\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u6743\u91cd\nmodel_ft.load_state_dict(torch.load('model_weights.pth'))\n# model_ft.load_state_dict(torch.load('..\/input\/densenet-animal\/model_weights.pth'))","1df49132":"test_set = []\nnames = []\n\nfor filename in sorted(os.listdir(TEST_ROOT), key=lambda t:int(t[:-4])):\n    img_path = os.path.join(TEST_ROOT, filename)\n    img = Image.open(img_path).convert('RGB') # \u6253\u5f00\u56fe\u7247\n    img_tensor = train_transforms(img) # \u9884\u5904\u7406\u56fe\u50cf\n    test_set.append(img_tensor)\n    names.append(filename[:-4]) # \u56fe\u7247\u540d\u79f0\u53bb\u9664\u6269\u5c55\u540d\u540e\u52a0\u5165list\n\nwith torch.no_grad():\n    outputs = model_ft(torch.stack(test_set).to(DEVICE))\n    preds = torch.argmax(outputs, dim=1)\n\nlabels = [full_dataset.classes[idx] for idx in preds] # \u628a\u9884\u6d4b\u51fa\u7684\u4e0b\u6807\u7ed3\u679c\u8f6c\u5316\u4e3a\u7c7b\u522b\u540d\u79f0\u7ed3\u679c","6d109b05":"result = dict(zip(names, labels))\nprint(result)","a19c93ea":"import json\nfilename='test_data.json'\nwith open(filename,'w') as file_obj:\n    json.dump(result,file_obj)","171ec917":"\u5f00\u59cb\u8bad\u7ec3","ef05b7a5":"\u8bbe\u7f6e\u8d85\u53c2\u6570","8200cf1c":"\u8f93\u51fa\u6700\u540e\u8981\u63d0\u4ea4\u7684json\u6587\u4ef6","2f2d2d18":"\u6570\u636e\u52a0\u8f7d\u4e0e\u9884\u5904\u7406","a25670d8":"\u5f00\u59cb\u5bf9\u6d4b\u8bd5\u96c6\u8fdb\u884c\u9884\u6d4b","185cf9ea":"\u5bfc\u5165\u9884\u8bad\u7ec3DenseNet\u6a21\u578b"}}