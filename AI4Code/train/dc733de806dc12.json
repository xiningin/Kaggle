{"cell_type":{"1f8025da":"code","c6715650":"code","bf05e4da":"code","80a859a3":"code","913e6b32":"code","5104c8d4":"code","2efc0988":"code","b931e8a8":"code","e351cc7b":"code","8537531f":"code","3ff86f5e":"code","65d55bb7":"code","4976cc10":"code","bc20e04b":"code","ad25ca1f":"code","07ce3b10":"code","44ab0afb":"code","eb821704":"code","dc0e8a1f":"code","f62c4e15":"code","7bf5f258":"code","23974b2c":"code","09e543c0":"code","28c53ab8":"code","7a92b1af":"code","7da74957":"code","b0565c5d":"code","508c4276":"code","620f12e0":"code","f6c6c603":"code","22f77d2e":"code","9c897e16":"code","fb37dacd":"code","084264ec":"code","d725e272":"code","6dce2c08":"code","6c2bb585":"code","67cd368b":"code","49de007e":"code","2d4d3c87":"code","a2ddf7b1":"code","b5d085a2":"code","f44c867c":"code","e29c25b5":"code","cdb034ac":"code","c52c5d33":"code","94a2f5da":"code","2760779f":"code","b729bae5":"code","9827dd01":"code","ee3d36f5":"code","fdbb1eba":"code","1ebcafcb":"code","266f4e84":"code","30c6769b":"code","d44711d2":"code","9d5d8355":"code","199cd629":"markdown","4bf8303a":"markdown","571eb825":"markdown","35a65c18":"markdown","39da8289":"markdown","56aa60eb":"markdown","5d9482dc":"markdown","b4a1adf4":"markdown","1b83b4ba":"markdown","85cb0f63":"markdown","553ae3ad":"markdown","64bbafe3":"markdown","07a5b344":"markdown","aab0f128":"markdown","3564914d":"markdown","2190386e":"markdown","4547b9a7":"markdown","ac3dd0ad":"markdown","121fb7f2":"markdown","b7d05a4a":"markdown","da58a6ec":"markdown","7630d451":"markdown","691c2c7a":"markdown","eb294568":"markdown","fc1ee805":"markdown","d37ee1fa":"markdown","43046d3c":"markdown","4ef66ef8":"markdown","a430093c":"markdown","80357274":"markdown","d3560659":"markdown","d464ad59":"markdown","e0a9b163":"markdown","b028579f":"markdown","c95954e1":"markdown","9c8921d2":"markdown","3d44f891":"markdown","b09274d7":"markdown","16460c69":"markdown","32e47b6e":"markdown","b08a1244":"markdown","122d1576":"markdown","9818051d":"markdown","9f1074da":"markdown","1b76d7ff":"markdown","5e21c1c6":"markdown","be818e60":"markdown","955a73dd":"markdown","21d06c97":"markdown","e320f0ad":"markdown","50bb5542":"markdown","f9cdad80":"markdown","1bae2a0f":"markdown","5bcff5e0":"markdown","5090d707":"markdown","2e87a000":"markdown","b2070ad4":"markdown","3f595c2a":"markdown","5b495356":"markdown","3fb081e9":"markdown","7a2df3c1":"markdown","168ccae5":"markdown","8349bc03":"markdown","a410769d":"markdown","8b6a3609":"markdown","23562f4e":"markdown","d9610bcc":"markdown","d28e0b4c":"markdown","579d6eac":"markdown","b45ae431":"markdown","fa6a34f9":"markdown","c03f95f6":"markdown","4e5e87cb":"markdown"},"source":{"1f8025da":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab\nimport matplotlib.pyplot as plt\nfrom matplotlib.pylab import rcParams\nimport itertools","c6715650":"# in current versions of ipython, rcParams can be lost between cells if combined with other matplotlib calls\n# See https:\/\/github.com\/ipython\/ipython\/issues\/11098\nrcParams['figure.figsize'] = 17.5, 14","bf05e4da":"df = pd.read_csv(\"..\/input\/MER_T12_06.csv\")\ndf.head()","80a859a3":"df.info()","913e6b32":"dateparse = lambda x: pd.to_datetime(x, format='%Y%m', errors = 'coerce')\ndf = pd.read_csv(\"..\/input\/MER_T12_06.csv\", parse_dates=['YYYYMM'], index_col='YYYYMM', date_parser=dateparse) \ndf.head()","5104c8d4":"df.head(15)","2efc0988":"ts = df[pd.Series(pd.to_datetime(df.index, errors='coerce')).notnull().values]\nts.head(15)","b931e8a8":"ts.dtypes","e351cc7b":"#ss = ts.copy(deep=True)\nts['Value'] = pd.to_numeric(ts['Value'] , errors='coerce')\nts.head()","8537531f":"ts.info()","3ff86f5e":"ts.dropna(inplace = True)","65d55bb7":"Energy_sources = ts.groupby('Description')\nEnergy_sources.head()","4976cc10":"fig, ax = plt.subplots()\nfor desc, group in Energy_sources:\n    group.plot(y='Value', label=desc,ax = ax, title='Carbon Emissions per Energy Source', fontsize = 20)\n    ax.set_xlabel('Time(Monthly)')\n    ax.set_ylabel('Carbon Emissions in MMT')\n    ax.xaxis.label.set_size(20)\n    ax.yaxis.label.set_size(20)\n    ax.legend(fontsize = 16)","bc20e04b":"fig, axes = plt.subplots(3,3, figsize = (30, 20))\nfor (desc, group), ax in zip(Energy_sources, axes.flatten()):\n    group.plot(y='Value',ax = ax, title=desc, fontsize = 18)\n    ax.set_xlabel('Time(Monthly)')\n    ax.set_ylabel('Carbon Emissions in MMT')\n    ax.xaxis.label.set_size(18)\n    ax.yaxis.label.set_size(18)","ad25ca1f":"CO2_per_source = ts.groupby('Description')['Value'].sum().sort_values()","07ce3b10":"# I want to use shorter descriptions for the energy sources\nCO2_per_source.index","44ab0afb":"cols = ['Geothermal Energy', 'Non-Biomass Waste', 'Petroleum Coke','Distillate Fuel ',\n        'Residual Fuel Oil', 'Petroleum', 'Natural Gas', 'Coal', 'Total Emissions']","eb821704":"fig = plt.figure(figsize = (16,9))\nx_label = cols\nx_tick = np.arange(len(cols))\nplt.bar(x_tick, CO2_per_source, align = 'center', alpha = 0.5)\nfig.suptitle(\"CO2 Emissions by Electric Power Sector\", fontsize= 25)\nplt.xticks(x_tick, x_label, rotation = 70, fontsize = 20)\nplt.yticks(fontsize = 20)\nplt.xlabel('Carbon Emissions in MMT', fontsize = 20)\nplt.show()","dc0e8a1f":"Emissions = ts.iloc[:,1:]   # Monthly total emissions (mte)\nEmissions= Emissions.groupby(['Description', pd.Grouper(freq='M')])['Value'].sum().unstack(level = 0)\nmte = Emissions['Natural Gas Electric Power Sector CO2 Emissions'] # monthly total emissions (mte)\nmte.head()","f62c4e15":"mte.tail()","7bf5f258":"import statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import coint, adfuller","23974b2c":"plt.plot(mte)","09e543c0":"def TestStationaryPlot(ts, plot_label = None):\n    rol_mean = ts.rolling(window = 12, center = False).mean()\n    rol_std = ts.rolling(window = 12, center = False).std()\n    \n    plt.plot(ts, color = 'blue',label = 'Original Data')\n    plt.plot(rol_mean, color = 'red', label = 'Rolling Mean')\n    plt.plot(rol_std, color ='black', label = 'Rolling Std')\n    plt.xticks(fontsize = 25)\n    plt.yticks(fontsize = 25)\n    \n    plt.xlabel('Time in Years', fontsize = 25)\n    plt.ylabel('Total Emissions', fontsize = 25)\n    plt.legend(loc='best', fontsize = 25)\n    if plot_label is not None:\n        plt.title('Rolling Mean & Standard Deviation (' + plot_label + ')', fontsize = 25)\n    else:\n        plt.title('Rolling Mean & Standard Deviation', fontsize = 25)\n    plt.show(block= True)","28c53ab8":"def TestStationaryAdfuller(ts, cutoff = 0.01):\n    ts_test = adfuller(ts, autolag = 'AIC')\n    ts_test_output = pd.Series(ts_test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    \n    for key,value in ts_test[4].items():\n        ts_test_output['Critical Value (%s)'%key] = value\n    print(ts_test_output)\n    \n    if ts_test[1] <= cutoff:\n        print(\"Strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary\")\n    else:\n        print(\"Weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n        ","7a92b1af":"TestStationaryPlot(mte, 'unmodified data')","7da74957":"TestStationaryAdfuller(mte)","b0565c5d":"moving_avg = mte.rolling(12).mean()\nplt.plot(mte)\nplt.plot(moving_avg, color='red')\nplt.xticks(fontsize = 25)\nplt.yticks(fontsize = 25)\nplt.xlabel('Time (years)', fontsize = 25)\nplt.ylabel('CO2 Emission (MMT)', fontsize = 25)\nplt.title('CO2 emission from electric power generation', fontsize = 25)\nplt.show()","508c4276":"mte_moving_avg_diff = mte - moving_avg\nmte_moving_avg_diff.head(13)","620f12e0":"mte_moving_avg_diff.dropna(inplace=True)\nTestStationaryPlot(mte_moving_avg_diff, 'moving average')","f6c6c603":"TestStationaryAdfuller(mte_moving_avg_diff)","22f77d2e":"mte_exp_weighted_avg = mte.ewm(halflife=12).mean()\nplt.plot(mte)\nplt.plot(mte_exp_weighted_avg, color='red')\nplt.xticks(fontsize = 25)\nplt.yticks(fontsize = 25)\nplt.xlabel('Time (years)', fontsize = 25)\nplt.ylabel('CO2 Emission (MMT)', fontsize = 25)\nplt.title('CO2 emission from electric power generation', fontsize = 25)\nplt.show()","9c897e16":"mte_ewma_diff = mte - mte_exp_weighted_avg\nTestStationaryPlot(mte_ewma_diff, 'exp weighted moving avg')","fb37dacd":"TestStationaryAdfuller(mte_ewma_diff)","084264ec":"mte_first_difference = mte - mte.shift(1)  \nTestStationaryPlot(mte_first_difference.dropna(inplace=False), 'differencing')","d725e272":"TestStationaryAdfuller(mte_first_difference.dropna(inplace=False))","6dce2c08":"mte_seasonal_difference = mte - mte.shift(12)  \nTestStationaryPlot(mte_seasonal_difference.dropna(inplace=False), 'seasonality difference')\nTestStationaryAdfuller(mte_seasonal_difference.dropna(inplace=False))","6c2bb585":"mte_seasonal_first_difference = mte_first_difference - mte_first_difference.shift(12)  \nTestStationaryPlot(mte_seasonal_first_difference.dropna(inplace=False), 'diff of seasonal diff')","67cd368b":"TestStationaryAdfuller(mte_seasonal_first_difference.dropna(inplace=False))","49de007e":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(mte)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(mte, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()","2d4d3c87":"mte_decompose = residual\nmte_decompose.dropna(inplace=True)\nTestStationaryPlot(mte_decompose, 'decomposing')\nTestStationaryAdfuller(mte_decompose)","a2ddf7b1":"fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(mte_seasonal_first_difference.iloc[13:], lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(mte_seasonal_first_difference.iloc[13:], lags=40, ax=ax2)","b5d085a2":"p = d = q = range(0, 2) # Define the p, d and q parameters to take any value between 0 and 2\npdq = list(itertools.product(p, d, q)) # Generate all different combinations of p, q and q triplets\npdq_x_QDQs = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))] # Generate all different combinations of seasonal p, q and q triplets\nprint('Examples of Seasonal ARIMA parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], pdq_x_QDQs[1]))\nprint('SARIMAX: {} x {}'.format(pdq[2], pdq_x_QDQs[2]))","f44c867c":"aic_results = []\nfor param in pdq:\n    for seasonal_param in pdq_x_QDQs:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(mte,\n                                            order=param,\n                                            seasonal_order=seasonal_param,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n            results = mod.fit()\n            print('ARIMA{}x{} - AIC:{}'.format(param, seasonal_param, results.aic))\n            if results.mle_retvals is not None and results.mle_retvals['converged'] == False:\n                print(results.mle_retvals)\n            aic_results.append(results.aic)\n        except:\n            continue\naic_results.sort()\nprint('Best AIC found: ', aic_results[0])","e29c25b5":"mod = sm.tsa.statespace.SARIMAX(mte, \n                                order=(1,1,1), \n                                seasonal_order=(0,1,1,12),   \n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\nresults = mod.fit()\nprint(results.summary())","cdb034ac":"results.resid.plot()","c52c5d33":"print(results.resid.describe())","94a2f5da":"results.resid.plot(kind='kde')","2760779f":"results.plot_diagnostics(figsize=(15, 12))\nplt.show()","b729bae5":"pred = results.get_prediction(start = 480, end = 522, dynamic=False)\npred_ci = pred.conf_int()\npred_ci.head()","9827dd01":"ax = mte['1973':].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead forecast', alpha=.7)\n\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='r', alpha=.5)\n\nax.set_xlabel('Time (years)')\nax.set_ylabel('NG CO2 Emissions')\nplt.legend()\n\nplt.show()","ee3d36f5":"mte_forecast = pred.predicted_mean\nmte_truth = mte['2013-01-31':]\n\n# Compute the mean square error\nmse = ((mte_forecast - mte_truth) ** 2).mean()\nprint('The Mean Squared Error (MSE) of the forecast is {}'.format(round(mse, 2)))\nprint('The Root Mean Square Error (RMSE) of the forecast: {:.4f}'\n      .format(np.sqrt(sum((mte_forecast-mte_truth)**2)\/len(mte_forecast))))","fdbb1eba":"mte_pred_concat = pd.concat([mte_truth, mte_forecast])","1ebcafcb":"pred_dynamic = results.get_prediction(start=pd.to_datetime('2013-01-31'), dynamic=True, full_results=True)\npred_dynamic_ci = pred_dynamic.conf_int()","266f4e84":"ax = mte['1973':].plot(label='observed', figsize=(20, 15))\npred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n\nax.fill_between(pred_dynamic_ci.index,\n                pred_dynamic_ci.iloc[:, 0],\n                pred_dynamic_ci.iloc[:, 1], \n                color='r', \n                alpha=.3)\n\nax.fill_betweenx(ax.get_ylim(), \n                 pd.to_datetime('2013-01-31'), \n                 mte.index[-1],\n                 alpha=.1, zorder=-1)\n\nax.set_xlabel('Time (years)')\nax.set_ylabel('CO2 Emissions')\n\nplt.legend()\nplt.show()","30c6769b":"# Extract the predicted and true values of our time series\nmte_forecast = pred_dynamic.predicted_mean\nmte_original = mte['2013-01-31':]\n\n# Compute the mean square error\nmse = ((mte_forecast - mte_original) ** 2).mean()\nprint('The Mean Squared Error (MSE) of the forecast is {}'.format(round(mse, 2)))\nprint('The Root Mean Square Error (RMSE) of the forecast: {:.4f}'\n      .format(np.sqrt(sum((mte_forecast-mte_original)**2)\/len(mte_forecast))))","d44711d2":"# Get forecast of 10 years or 120 months steps ahead in future\nforecast = results.get_forecast(steps= 120)\n# Get confidence intervals of forecasts\nforecast_ci = forecast.conf_int()\nforecast_ci.head()","9d5d8355":"ax = mte.plot(label='observed', figsize=(20, 15))\nforecast.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(forecast_ci.index,\n                forecast_ci.iloc[:, 0],\n                forecast_ci.iloc[:, 1], color='g', alpha=.4)\nax.set_xlabel('Time (year)')\nax.set_ylabel('NG CO2 Emission level')\n\nplt.legend()\nplt.show()","199cd629":"We have obtained a model for our time series that can now be used to produce forecasts. We start by comparing predicted values to real values of the time series, which will help us understand the accuracy of our forecast. The get_prediction() and conf_int() attributes allow us to obtain the values and associated confidence intervals for forecasts of the time series.","4bf8303a":"Depending on your platform and statsmodel version, some fit() calls may not converge. \n\nSARIMAX(1, 1, 1)x(0, 1, 1, 12) yields the lowest AIC value of 2003.553. Therefore, we will consider this to be optimal option out of all the parameter combinations. We have identified the set of parameters that produces the best fitting model to our time series data. We can proceed to analyze this particular model in more depth.","571eb825":"The CO2 emission time series dataset is plotted to visualize the dependency of the emission in the power generation with time. ","35a65c18":"# 6) Find optimal parameters and build SARIMA model","39da8289":"**From the bar chart, we can see that the contribution of coal to the total CO2 emission is significant followed by natural gas. **","56aa60eb":"The dynamic=False argument ensures that we produce one-step ahead forecasts, meaning that forecasts at each point are generated using the full history up to that point.\n\nWe can plot the real and forecasted values of the CO2 emission time series to assess how well the model fits.","5d9482dc":"The figure displays the distribution of the residual errors. It shows a little bias in the prediction. Next, we get a density plot of the residual error values, suggesting the errors are Gaussian, but may not be centered on zero.","b4a1adf4":"The first difference improves the stationarity of the series significantly. Let us use also the ***seasonal difference*** to remove the seasonality of the data and see how that impacts stationarity of the data.","1b83b4ba":"## 5.1.3 Transform the dataset to stationary","85cb0f63":"I use a public dataset of monthly carbon dioxide emissions from electricity generation available at the Energy Information Administration and Jason McNeill. The dataset includes CO2 emissions from each energy resource starting January 1973 to July 2016 for reference click [here](https:\/\/www.kaggle.com\/txtrouble\/carbon-emissions\/data). ","553ae3ad":"Now, if we look the Test Statistic and the p-value, taking the seasonal first difference has made our the time series dataset stationary. This differencing procedure could be repeated for the log values, but it didn\u2019t make the dataset any more stationary.","64bbafe3":"### Testing the monthly emissions time series","07a5b344":"# 7) Validating prediction","aab0f128":"# Foreword\n\nThis is a great kernel by which to learn how to analyze seasonality in data. The original version no longer works with current versions of Python 3 libraries required for the analysis (matplotlib, pandas, and perhaps others). I've fixed those problems, a few bugs I found, resolved some spelling errors, and added some features to improve understanding.","3564914d":"The dataset has 8 energy sources of CO2 emission. In the following cell, we will group the CO2 Emission dataset based on the type of energy source.","2190386e":"## A). Moving average","4547b9a7":"The emissions mean and the variation in standard deviation (black line) clearly vary with time. This shows that the series has a trend. So, it is not a stationary. Also, the Test Statistic is greater than the critical values with 90%, 95% and 99% confidence levels. Hence, no evidence to reject the null hypothesis. Therefore, the series is nonstationary. ","ac3dd0ad":"The goal of developing the model is to get a good quality predictive power using dynamic forecast. That is, we use information from the time series up to a certain point, and after that, forecasts are generated using values from previous forecasted time points as follows:","121fb7f2":"# 1) Introduction\nTime series is a collection of data points that are collected at constant time intervals. It is a dynamic or time dependent problem with or without increasing or decreasing trend, seasonality. Time series modeling is a powerful method to describe and extract information from time-based data and help to make informed decisions about future outcomes.\n\nThis notebook explores how to retrieve csv times series dataset, visualizing time series dataset, how to transform dataset into times series, testing if the time series is stationary or not using graphical and Dickey-Fuller test statistic methods, how to transform time series to stationary, how to find optimal parameters to build seasonal Autoregressive Integrated Moving Average (SARIMA) model using grid search method, diagnosing time series prediction, validating the predictive power, forecasting 10 year future CO2 emission from power generation using natural gas.,  \n    \nTo complete this time series analysis, I use the following articles that cover the fundamental concepts about time series modeling:\n\n [Time series forecasting with codes in Python](https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/time-series-forecasting-codes-python\/).\n\n[Statistical forecasting: notes on regression and time series analysis](http:\/\/people.duke.edu\/~rnau\/411home.htm).\n  \n  [Time Series Forecasting: Creating a seasonal ARIMA model with Python ](http:\/\/www.seanabu.com\/2016\/03\/22\/time-series-seasonal-ARIMA-model-in-python\/).\n  \n [Forecast a time series with ARIMA in Python](https:\/\/datascience.ibm.com\/exchange\/public\/entry\/view\/815137c868b916821dec777bdc23013c).\n \n [A Guide to Time Series Forecasting with ARIMA in Python 3](https:\/\/www.digitalocean.com\/community\/tutorials\/a-guide-to-time-series-forecasting-with-arima-in-python-3)\n","b7d05a4a":"The red line shows the rolling mean. Subtract the moving average from the original series. Note that since we are taking average of last 12 values, rolling mean is not defined for first 11 values. ","da58a6ec":"Individually, we can visualize the trend and seasonality effect on CO2 emission from each energy source. For example, the CO2 emission from coal shows a trend of increment from 1973 to 2006 and then declines till 2016. ","7630d451":"# 8) Forecasting","691c2c7a":"Here we can see that the trend, seasonality are separated out from data and we can model the residuals. Lets check stationarity of residuals:","eb294568":"# 6.1 Plot the ACF and PACF charts and find the optimal parameters","fc1ee805":"The first thing we need to do is producing a plot of our time series dataset. From the plot, we will get an idea about the overall trend and seasonality of the series. Then, we will use a statistical method to assess the trend and seasonality of the dataset.  After trend and seasonality are assessed if they are present in the dataset, they will be removed from the series to transform the nonstationary dataset into stationary and the residuals are further analyzed.\n   \nA short summary about stationarity from Wikipedia: A stationary process is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance, if they are present, also do not change over time. \n\nStationarity is an assumption underlying many statistical procedures used in time series analysis, non-stationary data is often transformed to become stationary. The most common cause of violation of stationarity is a trend in the mean, which can be due to either the presence of a unit root or of a deterministic trend. If the nonstationarity is caused by the presence of unit root, stochastic shocks have permanent effects and the process is not mean-reverting. However, if it is caused by a deterministic trend, the process is called a trend stationary process, and stochastic shocks have only transitory effects after which the variable tends toward a deterministically evolving mean.\n    \n A trend stationary process is not strictly stationary, but can easily be transformed into a stationary process by removing the underlying trend, which is solely a function of time. Similarly, processes with one or more unit roots can be made stationary through differencing. An important type of non-stationary process that does not include a trend-like behavior is a cyclostationary process, which is a stochastic process that varies cyclically with time.\n\nNote: Given two jointly distributed random variables X and Y, the conditional probability distribution of Y given X is the probability distribution of Y when X is known to be a particular value.\n","d37ee1fa":"## 4.1\tTime series dataset retrieving","43046d3c":"# 3)\tImport libraries","4ef66ef8":"## C) Eliminating trend and seasonality: Differencing","a430093c":"In this technique, we take average of \u2018k\u2019 consecutive values depending on the frequency of time series (in this case 12 months per year). Here, we will take the average over the past 1 year.","80357274":"## D) Eliminating trend and seasonality: Decomposing","d3560659":"One of the most common method of dealing with both trend and seasonality is differencing. In this technique, we take the difference of the original observation at a particular instant with that at the previous instant. This mostly works well to improve stationarity. First order differencing can be done as follows:","d464ad59":"The coef column shows the weight (i.e. importance) of each feature and how each one impacts the time series. The P>|z| column informs us of the significance of each feature weight. Here, each weight has a p-value close to 0, so it is reasonable to include the features in our model.\n\nWhen fitting seasonal ARIMA models, it is important to run model diagnostics to ensure that none of the assumptions made by the model have been violated. First, we get a line plot of the residual errors, suggesting that there may still be some trend information not captured by the model.","e0a9b163":"\nWhen evaluating and comparing statistical models fitted with different parameters, each can be ranked against one another based on how well it fits the data or its ability to accurately predict future data points. We will use the AIC (Akaike Information Criterion) value, which is conveniently returned with ARIMA models fitted using statsmodels. The AIC measures how well a model fits the data while taking into account the overall complexity of the model. A model that fits the data very well while using lots of features will be assigned a larger AIC score than a model that uses fewer features to achieve the same goodness-of-fit. Therefore, we are interested in finding the model that yields the lowest AIC value.\n\nThe order argument specifies the (p, d, q) parameters, while the seasonal_order argument specifies the (P, D, Q, S) seasonal component of the Seasonal ARIMA model. After fitting each SARIMAX()model, the code prints out its respective AIC score.","b028579f":"Total sum of CO2 emission from each energy group for every year is given as an observation that can be viewed in the NaT row. So, let us first identify and drop the non datetimeindex rows and also use ts to reference the time series dataset instead of the dataframe df. First, let us convert the index to datetime, coerce errors, and filter NaT","c95954e1":"Suggestion, comments and questions are welcome! ","9c8921d2":"* **Autocorrelation Function (ACF)**: It is a measure of the correlation between the time series (ts) with a lagged version of itself. For instance at lag 4, ACF would compare series at time instant \u2018t1\u2019\u2026\u2019t2\u2019 with series at instant \u2018t1-4\u2019\u2026\u2019t2-4\u2019 (t1-4 and t2 being end points of the range).\n* **Partial Autocorrelation Function (PACF)**: This measures the correlation between the ts with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. E.g. at lag 4, it will check the correlation but remove the effects already explained by lags 1 to  3.\n\nTherefore, the next step will be determining the tuning parameters (p and q) of the model by looking at the autocorrelation and partial autocorrelation graphs.  The chart below provides a brief guide on how to read the autocorrelation and partial autocorrelation graphs in order to select the parameters. ","3d44f891":"From, plotting the observed and forecasted values of the time series, we see that the overall forecasts are accurate even when we use the dynamic forecast. All forecasted values (red line) match closely to the original observed (blue line) data, and are well within the confidence intervals of our forecast.","b09274d7":"**Another technique is to take the \u2018weighted moving average\u2019 where more recent values are given a higher weight. The popular method to assign the weights is using the exponential weighted moving average. In this technique, weights are assigned to all previous values with a decay factor.","16460c69":"The most common techniques used to estimate or model trend and then remove it from the time series are \n- Aggregation \u2013 taking average for a time period like monthly\/weekly average\n- Smoothing \u2013 taking rolling averages\n- Polynomial Fitting \u2013 fit a regression model","32e47b6e":"For developing the time series model and make forecasting, I will use the natural gas CO2 emission from the electrical power generation. First, let us slice this data from the ts as follows:  ","b08a1244":"To find the optimal parameters for ARIMA models using the graphical method is not trivial and it is time consuming. We will select the optimal parameter values systematically using the grid search (hyperparameter optimization) method. The grid search iteratively explore different combinations of the parameters. For each combination of parameters, we will fit a new seasonal ARIMA model with the SARIMAX() function from the statsmodels module and assess its overall quality. Once we have explored the entire landscape of parameters, our optimal set of parameters will be the one that yields the best performance for our criteria of interest. \n\n*Note:* larger ranges of p, d, q parameters result in exponentially longer search time. If you try larger than 2, be prepared to wait. I've found better AIC results with larger value range, but end model is essentially the same.\n\nLet's begin by generating the various combination of parameters that we wish to assess:","122d1576":"In this notebook, I have explored how to retrieve CSV dataset, how to transform the dataset into times series, testing if the time series is stationary or not using graphical and Dickey-Fuller test statistic methods, how to transform time series to stationary, how to find optimal parameters to build SARIMA model using grid search method, diagnosing time series prediction, validating the predictive power, forecasting 10 year future CO2 emission from power generation using natural gas.\n\nFuture work: developing a time series model of natural gas forecasting\n","9818051d":"As we can see from the ts data type, the emission value is represented as an object. Let us first convert the emission value into numeric value as follows","9f1074da":"Our primary concern is to ensure that the residuals of our model are uncorrelated and normally distributed with zero-mean. If the seasonal ARIMA model does not satisfy these properties, it is a good indication that it can be further improved.","1b76d7ff":"Both the forecast and associated confidence interval that we have generated can now be used to further explore and understand the time series. The forecast shows that the CO2 emission from natural gas power generation is expected to continue increasing.","5e21c1c6":"The model diagnostic suggests that the model residual is normally distributed based on the following:\n\n- In the top right plot, the red KDE line follows closely with the N(0,1) line. Where, N(0,1) is the standard notation for a normal distribution with mean 0 and standard deviation of 1. This is a good indication that the residuals are normally distributed. The forecast errors deviate somewhat from the straight line, indicating that the normal distribution is not a perfect model for the distribution of forecast errors, but it is not unreasonable.\n- The qq-plot on the bottom left shows that the ordered distribution of residuals (blue dots) follows the linear trend of the samples taken from a standard normal distribution. Again, this is a strong indication that the residuals are normally distributed.\n- The residuals over time (top left plot) don't display any obvious seasonality and appear to be white noise. This is confirmed by the autocorrelation (i.e. correlogram) plot on the bottom right, which shows that the time series residuals have low correlation with lagged versions of itself.\n\nThose observations lead us to conclude that our model produces a satisfactory fit that could help us understand our time series data and forecast future values.","be818e60":"# 2) Time series dataset","955a73dd":"# 5) Natural gas CO2 emission analysis","21d06c97":"The dataset has 6 columns where 2 of them are integer data type and 4 objects and 5096 observations. The above dataset retrieving  method only retrieves the dataset as a dataframe that is not as a time series dataset. To read the dataset as a time series, we have to pass special arguments to the read_csv command as given below.","e320f0ad":"4323 observations have emissions value and therefore, we need to drop the empty rows emissions value. ","50bb5542":"[](http:\/\/)In this technique, start by modeling both trend and seasonality and removing them from the model.","f9cdad80":"When looking to fit time series dataset with seasonal ARIMA model, our first goal is to find the values of SARIMA(p,d,q)(P,D,Q)s that optimize our metric of interest. Before moving directly how to find the optimal values of the parameters let us see the two situations in stationarities: A strictly stationary series with no dependence among the values. This is the easy case wherein we can model the residuals as **white noise.** The second case being a series with significant dependence among values and needs statistical models like ARIMA to forecast future outcomes.\n\n**Auto-Regressive Integrated Moving Average (ARIMA)**: The ARIMA forecasting for a stationary time series is a linear function similar to linear regression. The predictors mainly depend on the parameters (p,d,q) of the ARIMA model:\n\n* Number of **Auto-Regressive (AR) terms (p)**: AR terms are just lags of dependent variable. For instance if p is 4, the predictors for x(t) will depend on x(t-1)\u2026.x(t-4).  This term allows us to incorporate the effect of past values into our model. This would be similar to stating that the weather is likely to be warm tomorrow if it has been warm the past 4 days.\n* Number of **Moving Average(MA) terms (q)**: MA terms are lagged forecast errors in prediction function. This term allows us to set the error of our model as a linear combination of the error values observed at previous time points in the past. For instance if q is 4, the predictors for x(t) will be e(t-1)\u2026.e(t-4) where e(i) is the difference between the moving average at ith instant and actual value.\n* Number of** Differences (d)**: These are the number of nonseasonal differences, i.e.,  if we took the first order difference. So either we can pass the first order difference variable and put d=0 or pass the original observed variable and put d=1. Both will generate same results. This term explains the number of past time points to subtract from the current value. This would be similar to stating that it is likely to be same temperature tomorrow if the difference in temperature in the last three days has been very small.","1bae2a0f":"   ## 4.3\tBar chart of CO2 Emissions per energy source","5bcff5e0":"From the figures, it is evident that there is a trend in the CO2 emission dataset with seasonal variation. So, we can infer that the dataset is not stationary.","5090d707":"# 9) Conclusion","2e87a000":"A formal way of testing stationarity of a dataset is using plotting the moving average or moving variance and see if the series mean and variance varies with time. This approach will be handled by the TestStationaryPlot() method. The second way to test stationarity is to use the statistical test (the Dickey-Fuller Test). The null hypothesis for the test is that the time series is non-stationary. The test results compare a Test Statistic and Critical Values (cutoff value) at different confidence levels. If the \u2018Test Statistic\u2019 is less than the \u2018Critical Value\u2019, we can reject the null hypothesis and say that the series is stationary. This technique will be handled by the TestStationaryAdfuller( ) method given below.\n","b2070ad4":"## Table of Contents\n1.\tIntroduction\n2.\tTime series dataset\n3.\tImport libraries\n4.\tTime series dataset retrieving and visualization\n5.  Natural gas CO2 emission analysis\n    5.1\tTest stationary\n        5.1.1 Graphically test stationary  \n        5.1.2 Test stationarity using Dickey-Fuller test\n        5.1.3 Transform the dataset to stationary\n6.\tFind optimal parameters and build SARIMA model\n7.\tValidating prediction\n8.  Forecasting \n9.\tConclusion","3f595c2a":"In recent years, the natural gas consumption has been increasing. However, the use of coal for power generation has been declining. The plots of CO2 emissions from coal and natural gas show this trend, while declining the CO2 contribution from coal, there is an increment in the contribution of CO2 emission from natural gas.","5b495356":"Overall, our forecasts align with the true values very well, showing an overall similar behavior.\n\nIt is also useful to quantify the accuracy of our forecasts. We will use the MSE (Mean Squared Error), which summarizes the average error of our forecasts. For each predicted value, we compute its distance to the true value and square the result. The results need to be squared so that positive\/negative differences do not cancel each other out.","3fb081e9":"## 5.1.2 Test  stationary using Dickey-Fuller","7a2df3c1":"# 6.2 Grid search ","168ccae5":"First, in the following cells, we will retrieve the monthly CO2 emissions dataset then we will visualize the dataset to decide the type of model we will use to model and analyse our time series (ts).","8349bc03":"## 5.1 Test Stationary","a410769d":"The rolling mean values appear to be varying slightly. The Test Statistic is smaller than the 10% 5%, and 1% of critical values. So, we can say with 99% confidence level that the dataset is a stationary series.","8b6a3609":"The arguments can be explained:\n- parse_dates: This is a key to identify the date time column. Example, the column name is \u2018YYYYMM\u2019.\n- index_col: This is a key that forces pandas to use the date time column as index.\n- date_parser: Converts an input string into datetime variable.","23562f4e":"# 4)\tTime series dataset retrieving and visualization","d9610bcc":"## 4.2\tTime series dataset visualization","d28e0b4c":"The plot_diagnostics object allows us to quickly generate model diagnostics and investigate for any unusual behavior.","579d6eac":"Compared to the original data the seasonal difference also improves the stationarity of the series. The next step is to take the first difference of the seasonal difference.","b45ae431":"We can use the output of this code to plot the time series and forecasts of its future values.","fa6a34f9":"> ## B). Exponential weighted moving average","c03f95f6":"This time series has lesser variations in mean and standard deviation compared to the original dataset. Also, the Test Statistic is smaller than the 5% and 10% critical value, which is better than the original case. There will be no missing values as all values from starting are given weights. So, it will work even with no previous values. In this case, we can say with 95% confidence level the series is a stationary series.","4e5e87cb":"## 5.1.1 Graphically test stationary"}}