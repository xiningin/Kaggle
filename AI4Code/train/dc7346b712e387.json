{"cell_type":{"ea3b6884":"code","b73dcb0c":"code","53ab6073":"code","9b59ead5":"code","0efb1376":"code","64551a24":"code","ad4fd980":"code","54618b2a":"code","070ea272":"code","ee4b5b23":"code","5f1f5fe9":"code","b9385af6":"code","70e8b442":"code","b8221bf0":"code","2513c9d6":"code","a8996988":"code","d6732e03":"code","6f6fd5de":"code","0b616bc8":"code","31f77054":"code","fa68cdbc":"code","264a6581":"code","04ddccdd":"code","d0599dc6":"code","aabd26b6":"code","fff81313":"code","5d2cb930":"code","1264ddb2":"code","784bf83c":"code","9caeb365":"code","3a5deb81":"code","ef55c7ef":"code","d5065a98":"code","01adf89b":"code","b947c3e8":"code","3849f493":"code","3e01f3a8":"code","e11cbd74":"code","6a91d4d6":"code","7101573c":"code","5bbabe40":"code","c827973a":"code","10b8401d":"code","e29fcaf9":"code","b57bf442":"code","03dad48d":"code","7b3f4940":"code","39b551d3":"code","4b3ff966":"code","dc46d219":"code","a720d7b6":"code","20c5eb2a":"code","735356ec":"code","de519f30":"code","8022b50d":"code","44979469":"code","01b03fa1":"code","de052723":"code","b6aa3a72":"code","e49ea673":"code","c751fe80":"code","78929d7e":"code","233aef3a":"code","cbc86eeb":"code","6d28c907":"code","a4e62920":"code","327df480":"code","ba90792f":"code","ddf23214":"code","70a0d155":"code","50efcbb3":"code","d71565a3":"code","c57d1412":"code","531fbc42":"code","c500cfe1":"code","07ac24db":"code","814944ea":"code","f4aa5de4":"code","22c4354a":"code","cd8b08f9":"code","877ce8de":"code","45392997":"code","9f44cc22":"code","2c46e2d5":"code","d4a26848":"code","7647fc8b":"code","439ce726":"code","c82e1e93":"code","73bf32a9":"code","07f9804d":"code","a47ce14f":"code","183f4dd0":"markdown","2ccbcf28":"markdown","9dab146d":"markdown","64be52d7":"markdown","a3836459":"markdown","5c920fcc":"markdown","7e2c87b0":"markdown","229802de":"markdown","0879313c":"markdown"},"source":{"ea3b6884":"ls -la ..\/input","b73dcb0c":"%matplotlib inline\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot","53ab6073":"import os\nimport shutil\nprint(os.listdir(\"..\/input\"))","9b59ead5":"ls -la ..\/input\/keras-pretrained-models","0efb1376":"try:\n    os.makedirs('\/tmp\/.keras\/datasets')\nexcept FileExistsError:\n    pass","64551a24":"try:\n    shutil.copytree(\"..\/input\/keras-pretrained-models\", \"\/tmp\/.keras\/models\")\nexcept FileExistsError:\n    pass","ad4fd980":"import os.path\nimport itertools\nfrom itertools import chain\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.datasets import load_digits\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\n\nimport tensorflow as tf\n\nfrom keras.layers import Input, Embedding, LSTM, GRU, Dense, Dropout, Lambda, \\\n    Conv1D, Conv2D, Conv3D, \\\n    Conv2DTranspose, \\\n    AveragePooling1D, AveragePooling2D, \\\n    MaxPooling1D, MaxPooling2D, MaxPooling3D, \\\n    GlobalAveragePooling1D, \\\n    GlobalMaxPooling1D, GlobalMaxPooling2D, GlobalMaxPooling3D, \\\n    LocallyConnected1D, LocallyConnected2D, \\\n    concatenate, Flatten, Average, Activation, \\\n    RepeatVector, Permute, Reshape, Dot, \\\n    multiply, dot, add, \\\n    PReLU, \\\n    Bidirectional, TimeDistributed, \\\n    SpatialDropout1D, \\\n    BatchNormalization\nfrom keras.models import Model, Sequential\nfrom keras import losses\nfrom keras.callbacks import BaseLogger, ProgbarLogger, Callback, History\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras import regularizers\nfrom keras import initializers\nfrom keras.metrics import categorical_accuracy\nfrom keras.constraints import maxnorm, non_neg\nfrom keras.optimizers import RMSprop\nfrom keras.utils import to_categorical, plot_model\nfrom keras import backend as K","54618b2a":"from PIL import Image\nfrom zipfile import ZipFile\nimport h5py\nimport cv2\nfrom tqdm import tqdm","070ea272":"src_dir = '..\/input\/human-protein-atlas-image-classification'","ee4b5b23":"train_labels = pd.read_csv(os.path.join(src_dir, \"train.csv\"))\nprint(train_labels.shape)\ntrain_labels.head(10)","5f1f5fe9":"test_labels = pd.read_csv(os.path.join(src_dir, \"sample_submission.csv\"))\nprint(test_labels.shape)\ntest_labels.head()","b9385af6":"def show_arr(arr, nrows = 1, ncols = 4, figsize=(15, 5)):\n    fig, subs = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n    for ii in range(ncols):\n        iplt = subs[ii]\n        try:\n            img_array = arr[:,:,ii]\n            if ii == 0:\n                cp = 'Greens'\n            elif ii == 1:\n                cp = 'Blues'\n            elif ii == 2:\n                cp = 'Reds'\n            else:\n                cp = 'Oranges'\n            iplt.imshow(img_array, cmap=cp)\n        except:\n            pass","70e8b442":"def get_arr0(Id, test=False):\n    def fn(Id, color, test=False):\n        if test:\n            tgt = 'test'\n        else:\n            tgt = 'train'\n        with open(os.path.join(src_dir, tgt, Id+'_{}.png'.format(color)), 'rb') as fp:\n            img = Image.open(fp)\n            arr = (np.asarray(img) \/ 255.)\n        return arr\n    res = []\n    for icolor in ['green', 'blue', 'red', 'yellow']:\n        arr0 = fn(Id, icolor, test)\n        res.append(arr0)\n    arr = np.stack(res, axis=-1)\n    return arr","b8221bf0":"arr = get_arr0('00008af0-bad0-11e8-b2b8-ac1f6b6435d0', test=True)\nprint(arr.shape)\nshow_arr(arr)","2513c9d6":"arr = get_arr0('00070df0-bbc3-11e8-b2bc-ac1f6b6435d0')\nprint(arr.shape)\nshow_arr(arr)","a8996988":"SH = (256, 256)\nID_LIST_TRAIN = train_labels.Id.tolist()","d6732e03":"def get_arr(Id, test=False):\n    if test:\n        arr = get_arr0(Id, test=True)\n    else:\n        arr = get_arr0(Id)\n    arr = cv2.resize(arr, SH).astype('float32')\n    return arr","6f6fd5de":"arr = get_arr('00070df0-bbc3-11e8-b2bc-ac1f6b6435d0')\nprint(arr.shape)\nshow_arr(arr)","0b616bc8":"arr = get_arr('00008af0-bad0-11e8-b2b8-ac1f6b6435d0', test=True)\nprint(arr.shape)\nshow_arr(arr)","31f77054":"y_cat_train_dic = {}\nfor icat in range(28):\n    target = str(icat)\n    y_cat_train_5 = np.array([int(target in ee.split()) for ee in train_labels.Target.tolist()])\n    y_cat_train_dic[icat] = y_cat_train_5","fa68cdbc":"up_sample = {}\nfor k in y_cat_train_dic:\n    v = y_cat_train_dic[k].sum()\n    up_sample[k] = np.ceil((train_labels.shape[0]\/28) \/ v)\n\nup_sample","264a6581":"up_sample2 = list(zip(*sorted(list(up_sample.items()), key=lambda x: x[0])))[1]\nup_sample2 = np.array(up_sample2)\nup_sample2","04ddccdd":"import random\n\nclass Seq(object):\n    sections = None\n    index = None\n    \n    def __init__(self, df, extend=False, aug=False, test=False, batch_size=32):\n        self.shaffle = None\n        self.extend = extend\n        self.aug = aug\n        self.test = test\n        self.batch_size = batch_size\n        self.df = df\n        \n        # proccess\n        self.ids = self.df.Id.tolist()\n        self.reversed = sorted(range(SH[0]), reverse=True)\n        \n        # estimate self length\n        self.initialize_it()\n        self.len = 1\n        for _ in self.it:\n            self.len += 1\n        \n        self.initialize_it()\n    \n    def initialize_it(self):\n        if self.shaffle:\n            '''not implemented yet'''\n            raise NotImplementedError\n            #random.seed(self.state)\n            #random.shuffle(self.ids)\n        \n        self.it = iter(range(0, len(self.ids), self.batch_size))\n        self.idx_next = self.it.__next__()\n    \n    def __len__(self):\n        return self.len\n    \n    def __iter__(self):\n        return self\n    \n    def __next__(self):\n        idx = self.idx_next\n        self.ids_part = self.ids[idx:((idx+self.batch_size) if idx+self.batch_size<len(self.ids) else len(self.ids))]\n        res = self.getpart(self.ids_part)\n        try:\n            self.idx_next = self.it.__next__()\n        except StopIteration:\n            self.initialize_it()\n        return res\n    \n    def __getitem__(self, id0):\n        arr, tgts = self.get_data(id0)\n        cat = self.convert_tgts(tgts)\n        return arr, cat\n    \n    k_list = list(range(4))\n    def random_transform(self, arr):\n        k = random.choice(self.k_list)\n        arr0 = np.rot90(arr, k=k)\n        if random.randint(0,1):\n            arr0 = arr0[self.reversed,:,:]\n        if random.randint(0,1):\n            arr0 = arr0[:,self.reversed,:]\n        return arr0\n    \n    def convert_tgts(self, tgts):\n        try:\n            cats = to_categorical(tgts, num_classes=28)\n            cat = cats.sum(axis=0)\n        except TypeError:\n            cat = np.zeros((28,))\n        return cat\n    \n    def get_data(self, id0):\n        arr = get_arr(id0, test=self.test)\n        \n        try:\n            y0 = (self.df.Target[self.df.Id == id0]).tolist()[0]\n            y1 = y0.split()\n            y = [int(ee) for ee in y1]\n        except AttributeError:\n            y = None\n        return arr, y\n    \n    def getpart(self, ids):\n        xs = []\n        ys = []\n        for id0 in ids:\n            self.extend_data(id0, xs, ys)\n        \n        x = np.stack(xs)\n        y = np.stack(ys)\n        x_dummy = np.zeros((len(x), 1))\n        x_ret = {\n            'input': x,\n            'input_cls': y,\n        }\n        y_ret = {\n            'path_fit_cls_img': x_dummy,\n            'path_cls_img_cls': y,\n            'path_fit_imgA': x_dummy,\n            'path_fit_img_cls_img': x_dummy,\n            'path_cls_cls': y,\n            'path_img_cls': y,\n            'path_img_img_cls': y,\n            'path_fit_cls_img_imgE': x_dummy,\n        }\n        return (x_ret, y_ret)\n    \n    def split(self, arr, sections=sections):\n        res0 = np.vsplit(arr, sections)\n        res = [np.hsplit(ee, sections) for ee in res0]\n        res = list(chain.from_iterable(res))\n        return res\n    \n    def extend_data(self, id0, xs, ys):\n        arr0, cat = self[id0]\n        \n        # data augmentation\n        if self.extend:\n            mm = up_sample2[cat==1].max()\n            mm = int(mm)\n            #print(mm)\n            for ii in range(mm):\n                if self.aug:\n                    img = self.random_transform(arr0)\n                else:\n                    img = arr0\n                xs.append(img.flatten())\n                ys.append(cat)\n        else:\n            if self.aug:\n                img = self.random_transform(arr0)\n            else:\n                img = arr0\n            xs.append(img.flatten())\n            ys.append(cat)","d0599dc6":"from keras import applications","aabd26b6":"def make_trainable_false(model_resnet, trainable=False):\n    layers = model_resnet.layers\n    for ilayer in layers:\n        ilayer.trainable = trainable\n    return","fff81313":"img_shape = tuple(list(SH) + [4])\nprint(img_shape)\nimg_dim = np.array(img_shape).prod()\nprint(img_dim)","5d2cb930":"def make_model_cnvt(img_dim, img_shape):\n    '''==============================\n    inputs\n    =============================='''\n    inp = Input(shape=(img_dim,))\n    oup = Reshape(img_shape)(inp)\n    #oup = Conv2D(3, kernel_size=1, strides=1, padding='same')(oup)\n    #oup = Conv2D(3, kernel_size=1, strides=1, padding='same', activation='sigmoid')(oup)\n    oup = Conv2D(3,\n                 kernel_size=1,\n                 strides=1,\n                 padding='same',\n                 activation='tanh',\n                 kernel_regularizer=regularizers.l2(1e-4))(oup)\n    #kernel_regularizer=regularizers.l2(1e-4)\n    model_cnvt = Model(inp, oup, name='model_cnvt')\n    return model_cnvt","1264ddb2":"def make_model_classifier(input_dim=1536):\n    inp_cls = Input((input_dim,))\n    oup_cls = Dense(28)(inp_cls)\n    oup_cls = Activation('sigmoid')(oup_cls)\n    model_classifier = Model(inp_cls, oup_cls, name='classifier')\n    return model_classifier","784bf83c":"def make_model(img_dim, model_cnvt, model_resnet, model_classifier):\n    '''==============================\n    inputs\n    =============================='''\n    inp = Input(shape=(img_dim,), name='input')\n    oup = model_cnvt(inp)\n    oup = model_resnet(oup)\n    oup = model_classifier(oup)\n    oup = Activation('linear', name='path_cls_cls')(oup)\n    \n    model = Model(inp, oup, name='model')\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['categorical_accuracy', 'binary_accuracy'])\n    \n    return {\n        'model_classifier': model_classifier,\n        'model_resnet': model_resnet,\n        'model_cnvt': model_cnvt,\n        'model': model\n    }","9caeb365":"model_cnvt = make_model_cnvt(img_dim, img_shape)\nmodel_cnvt.summary()","3a5deb81":"model_cnvt.layers[2].get_weights()","ef55c7ef":"model_cnvt.load_weights('..\/input\/keras-inceptionresnetv2-resize139x139-005focal\/model_5_cnvt.h5')\nmodel_cnvt.layers[2].get_weights()","d5065a98":"model_resnet = applications.inception_resnet_v2.InceptionResNetV2(\n    include_top=False,\n    weights='imagenet',\n    input_tensor=None,\n    input_shape=list(SH) + [3],\n    pooling='avg',\n    classes=None)","01adf89b":"# model_resnet.summary()","b947c3e8":"model_resnet.layers[1].get_weights()[0][0]","3849f493":"model_resnet.load_weights('..\/input\/keras-inceptionresnetv2-resize139x139-005focal\/model_5_resnet.h5')\nmodel_resnet.layers[1].get_weights()[0][0]","3e01f3a8":"model_classifier = make_model_classifier()\nmodel_classifier.summary()","e11cbd74":"model_classifier.layers[1].get_weights()[0][0]","6a91d4d6":"model_classifier.load_weights('..\/input\/keras-inceptionresnetv2-resize139x139-005focal\/model_5_classifier.h5')\nmodel_classifier.layers[1].get_weights()[0][0]","7101573c":"models = make_model(img_dim, model_cnvt, model_resnet, model_classifier)\nmodels['model'].summary()","5bbabe40":"THRESHOLD = 0.5\n\n# credits: https:\/\/www.kaggle.com\/guglielmocamporese\/macro-f1-score-keras\n\nK_epsilon = K.epsilon()\ndef f1(y_true, y_pred):\n    #y_pred = K.round(y_pred)\n    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K_epsilon)\n    r = tp \/ (tp + fn + K_epsilon)\n\n    f1 = 2*p*r \/ (p+r+K_epsilon)\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\n\ndef f1_loss(y_true, y_pred):\n    \n    #y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K_epsilon)\n    r = tp \/ (tp + fn + K_epsilon)\n\n    f1 = 2*p*r \/ (p+r+K_epsilon)\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1-K.mean(f1)","c827973a":"models['model'].compile(loss=f1_loss,\n                        optimizer='adam',\n                        metrics=['categorical_accuracy', 'binary_accuracy', f1])","10b8401d":"seq = Seq(train_labels, extend=False, aug=True, batch_size=32)\nprint(len(seq))\n\nhst = models['model'].fit_generator(seq, epochs=3,\n                              steps_per_epoch=len(seq),\n                              callbacks=[])","e29fcaf9":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\nax[0].set_title('loss')\nax[0].plot(hst.epoch, hst.history[\"loss\"], label=\"Train loss\")\nax[1].set_title('acc')\nax[1].plot(hst.epoch, hst.history[\"categorical_accuracy\"], label=\"categorical_accuracy\")\nax[1].plot(hst.epoch, hst.history[\"binary_accuracy\"], label=\"binary_accuracy\")\nax[0].legend()\nax[1].legend()","b57bf442":"seq = Seq(train_labels, extend=False, aug=False, batch_size=32)\nprint(len(seq))\nxs, ys = next(seq)\nprint(xs['input'].shape)\ny_pred = models['model_cnvt'].predict(xs['input'])\nprint(y_pred.shape)","03dad48d":"y_pred[0]","7b3f4940":"show_arr(y_pred[0])","39b551d3":"Image.fromarray(np.uint8((y_pred[0]+1)\/2*255))","4b3ff966":"seq_pred = Seq(train_labels, test=False, aug=False, batch_size=128)\nlen(seq_pred)","dc46d219":"pred = models['model'].predict_generator(seq_pred, steps=len(seq_pred), verbose=1)","a720d7b6":"def calc_threshold(pred):\n    ### calc threshold\n    threshold_dic = {}\n    for idx in tqdm(range(28)):\n        m = 0\n        for ii in range(100):\n            threshold0 = ii*0.01\n            f1_val = f1_score(y_cat_train_dic[idx], threshold0<(pred[:,idx]))\n            if m < f1_val:\n                threshold_dic[idx] = threshold0+0.005\n                m = f1_val\n    return threshold_dic","20c5eb2a":"threshold_dic = calc_threshold(pred)\nthreshold_dic","735356ec":"seq_test = Seq(test_labels, test=True, aug=False, batch_size=128)\nseq_test","de519f30":"pred_test = models['model'].predict_generator(seq_test, steps=len(seq_test), verbose=1)","8022b50d":"def make_test(pred):\n    test_labels1 = test_labels.copy()\n    test_labels1['Predicted'] = [str(ee) for ee in np.argmax(pred, axis=1)]\n    print(test_labels1.head())\n    #test_labels1.to_csv(fn0, index=False)\n    \n    test_labels2 = test_labels1.copy()\n    for ii in range(test_labels2.shape[0]):\n        threshold = list(zip(*sorted(list(threshold_dic.items()), key=lambda x:x[0], reverse=False)))[1]\n        idx = threshold < pred[ii,:]\n        tgt = test_labels2['Predicted'][ii]\n        tgt = [tgt] + [str(ee) for ee in np.arange(28)[idx]]\n        tgt = set(tgt)\n        tgt = ' '.join(tgt)\n        test_labels2['Predicted'][ii] = tgt\n    print(test_labels2.head())\n    #test_labels2.to_csv(fn, index=False)\n    return test_labels1, test_labels2","44979469":"test_labels1_1, test_labels1_2 = make_test(pred_test)","01b03fa1":"test_labels1_2.head()","de052723":"test_labels1_2.to_csv('InceptionResNetV1_2.csv', index=False)","b6aa3a72":"'''save weights for later loading'''\nNo = 1\nmodels['model_cnvt'].save_weights('model_{}_cnvt.h5'.format(No))\nmodels['model_resnet'].save_weights('model_{}_resnet.h5'.format(No))\nmodels['model_classifier'].save_weights('model_{}_classifier.h5'.format(No))","e49ea673":"ls -la","c751fe80":"seq = Seq(train_labels, extend=False, aug=True, batch_size=32)\nprint(len(seq))\n\nhst = models['model'].fit_generator(seq, epochs=2,\n                              steps_per_epoch=len(seq),\n                              callbacks=[])","78929d7e":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\nax[0].set_title('loss')\nax[0].plot(hst.epoch, hst.history[\"loss\"], label=\"Train loss\")\nax[1].set_title('acc')\nax[1].plot(hst.epoch, hst.history[\"categorical_accuracy\"], label=\"categorical_accuracy\")\nax[1].plot(hst.epoch, hst.history[\"binary_accuracy\"], label=\"binary_accuracy\")\nax[0].legend()\nax[1].legend()","233aef3a":"seq_pred = Seq(train_labels, test=False, aug=False, batch_size=128)\nlen(seq_pred)\npred = models['model'].predict_generator(seq_pred, steps=len(seq_pred), verbose=1)\nthreshold_dic = calc_threshold(pred)\nthreshold_dic","cbc86eeb":"seq_test = Seq(test_labels, test=True, aug=False, batch_size=128)\nseq_test\npred_test = models['model'].predict_generator(seq_test, steps=len(seq_test), verbose=1)\ntest_labels2_1, test_labels2_2 = make_test(pred_test)\ntest_labels2_2.head()","6d28c907":"test_labels2_2.to_csv('InceptionResNetV2_2.csv', index=False)","a4e62920":"'''save weights for later loading'''\nNo = 2\nmodels['model_cnvt'].save_weights('model_{}_cnvt.h5'.format(No))\nmodels['model_resnet'].save_weights('model_{}_resnet.h5'.format(No))\nmodels['model_classifier'].save_weights('model_{}_classifier.h5'.format(No))","327df480":"seq = Seq(train_labels, aug=True, batch_size=32)\nprint(len(seq))\n\nhst = models['model'].fit_generator(seq, epochs=2,\n                              steps_per_epoch=len(seq),\n                              callbacks=[])","ba90792f":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\nax[0].set_title('loss')\nax[0].plot(hst.epoch, hst.history[\"loss\"], label=\"Train loss\")\nax[1].set_title('acc')\nax[1].plot(hst.epoch, hst.history[\"categorical_accuracy\"], label=\"categorical_accuracy\")\nax[1].plot(hst.epoch, hst.history[\"binary_accuracy\"], label=\"binary_accuracy\")\nax[0].legend()\nax[1].legend()","ddf23214":"seq_pred = Seq(train_labels, test=False, aug=False, batch_size=128)\nlen(seq_pred)\npred = models['model'].predict_generator(seq_pred, steps=len(seq_pred), verbose=1)\nthreshold_dic = calc_threshold(pred)\nthreshold_dic","70a0d155":"seq_test = Seq(test_labels, test=True, aug=False, batch_size=128)\nseq_test\npred_test = models['model'].predict_generator(seq_test, steps=len(seq_test), verbose=1)\ntest_labels3_1, test_labels3_2 = make_test(pred_test)\ntest_labels3_2.head()","50efcbb3":"test_labels3_2.to_csv('InceptionResNetV3_2.csv', index=False)","d71565a3":"'''save weights for later loading'''\nNo = 3\nmodels['model_cnvt'].save_weights('model_{}_cnvt.h5'.format(No))\nmodels['model_resnet'].save_weights('model_{}_resnet.h5'.format(No))\nmodels['model_classifier'].save_weights('model_{}_classifier.h5'.format(No))","c57d1412":"def lr_schedule(epoch):\n    lr = 1e-4\n    print('Learning rate: ', lr)\n    return lr","531fbc42":"lr_scheduler = LearningRateScheduler(lr_schedule)\ncallbacks = [lr_scheduler]\n\nseq = Seq(train_labels, aug=True, batch_size=32)\nprint(len(seq))\n\nhst = models['model'].fit_generator(seq, epochs=2,\n                              steps_per_epoch=len(seq),\n                              callbacks=callbacks)","c500cfe1":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\nax[0].set_title('loss')\nax[0].plot(hst.epoch, hst.history[\"loss\"], label=\"Train loss\")\nax[1].set_title('acc')\nax[1].plot(hst.epoch, hst.history[\"categorical_accuracy\"], label=\"categorical_accuracy\")\nax[1].plot(hst.epoch, hst.history[\"binary_accuracy\"], label=\"binary_accuracy\")\nax[0].legend()\nax[1].legend()","07ac24db":"seq_pred = Seq(train_labels, test=False, aug=False, batch_size=128)\nlen(seq_pred)\npred = models['model'].predict_generator(seq_pred, steps=len(seq_pred), verbose=1)\nthreshold_dic = calc_threshold(pred)\nthreshold_dic","814944ea":"seq_test = Seq(test_labels, test=True, aug=False, batch_size=128)\nseq_test\npred_test = models['model'].predict_generator(seq_test, steps=len(seq_test), verbose=1)\ntest_labels4_1, test_labels4_2 = make_test(pred_test)\ntest_labels4_2.head()","f4aa5de4":"test_labels4_2.to_csv('InceptionResNetV4_2.csv', index=False)","22c4354a":"'''save weights for later loading'''\nNo = 4\nmodels['model_cnvt'].save_weights('model_{}_cnvt.h5'.format(No))\nmodels['model_resnet'].save_weights('model_{}_resnet.h5'.format(No))\nmodels['model_classifier'].save_weights('model_{}_classifier.h5'.format(No))","cd8b08f9":"def lr_schedule(epoch):\n    lr = 1e-4\n    print('Learning rate: ', lr)\n    return lr","877ce8de":"lr_scheduler = LearningRateScheduler(lr_schedule)\ncallbacks = [lr_scheduler]\n\nseq = Seq(train_labels, aug=True, batch_size=32)\nprint(len(seq))\n\nhst = models['model'].fit_generator(seq, epochs=2,\n                              steps_per_epoch=len(seq),\n                              callbacks=callbacks)","45392997":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\nax[0].set_title('loss')\nax[0].plot(hst.epoch, hst.history[\"loss\"], label=\"Train loss\")\nax[1].set_title('acc')\nax[1].plot(hst.epoch, hst.history[\"categorical_accuracy\"], label=\"categorical_accuracy\")\nax[1].plot(hst.epoch, hst.history[\"binary_accuracy\"], label=\"binary_accuracy\")\nax[0].legend()\nax[1].legend()","9f44cc22":"seq_pred = Seq(train_labels, test=False, aug=False, batch_size=128)\nlen(seq_pred)\npred = models['model'].predict_generator(seq_pred, steps=len(seq_pred), verbose=1)\nthreshold_dic = calc_threshold(pred)\nthreshold_dic","2c46e2d5":"seq_test = Seq(test_labels, test=True, aug=False, batch_size=128)\nseq_test\npred_test = models['model'].predict_generator(seq_test, steps=len(seq_test), verbose=1)\ntest_labels5_1, test_labels5_2 = make_test(pred_test)\ntest_labels5_2.head()","d4a26848":"test_labels5_2.to_csv('InceptionResNetV5_2.csv', index=False)","7647fc8b":"'''save weights for later loading'''\nNo = 5\nmodels['model_cnvt'].save_weights('model_{}_cnvt.h5'.format(No))\nmodels['model_resnet'].save_weights('model_{}_resnet.h5'.format(No))\nmodels['model_classifier'].save_weights('model_{}_classifier.h5'.format(No))","439ce726":"ls -la","c82e1e93":"seq = Seq(train_labels, extend=False, aug=False, batch_size=32)\nprint(len(seq))\nxs, ys = next(seq)\nprint(xs['input'].shape)\ny_pred = models['model_cnvt'].predict(xs['input'])\nprint(y_pred.shape)","73bf32a9":"y_pred[0]","07f9804d":"show_arr(y_pred[0])","a47ce14f":"Image.fromarray(np.uint8((y_pred[0]+1)\/2*255))","183f4dd0":"## #5","2ccbcf28":"* no data expand, but augment  \n* try batch_size=128  \n* pre-train\n* tanh\n* no cache","9dab146d":"## save weights for later loading","64be52d7":"### predict and submit","a3836459":"### make model","5c920fcc":"## #4","7e2c87b0":"## #1","229802de":"## #3","0879313c":"## #2"}}