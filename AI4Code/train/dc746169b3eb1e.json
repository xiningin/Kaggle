{"cell_type":{"3bfe7efd":"code","5165c266":"code","e665aebf":"code","1921f43c":"code","dd3c6221":"code","5e526be6":"code","a9daf39d":"code","b2117420":"code","84c43451":"code","4b8fbb54":"code","b98cb652":"code","bdc41d32":"code","c0ab264a":"code","d3ec1c7d":"code","983f720b":"code","53826d56":"code","6d315e4a":"code","e5fc2aea":"code","920e2432":"code","1a59e3f9":"code","3c9887b7":"code","fb034caf":"code","f30d2748":"code","347a32b6":"code","11594e33":"code","54b726b1":"code","6ff7c6e3":"code","e387384e":"code","5791f641":"code","6f18bd30":"code","b3fa5299":"code","6b00b0a2":"code","be2c3070":"code","b1598da8":"markdown","c13be374":"markdown","414fe03a":"markdown","ecae5312":"markdown","25030700":"markdown","4650a7f3":"markdown","bd7c9b92":"markdown","99616b4e":"markdown","9b5d0d1e":"markdown","4a819901":"markdown","73727923":"markdown","6f00c32b":"markdown","28fd77be":"markdown","2ccad222":"markdown","16113504":"markdown","46852642":"markdown","2906b2cd":"markdown","6993dd2c":"markdown","e4da1576":"markdown","4bd9b687":"markdown","a9fad22b":"markdown","c06674f3":"markdown","b92280c8":"markdown","75492f9a":"markdown","b3718078":"markdown","d4a70f02":"markdown","6bcdd1e2":"markdown","a54bf3e6":"markdown","8b7871c8":"markdown","d32e99e7":"markdown","0b96e7b9":"markdown","2f542f4e":"markdown","88c5d17c":"markdown","7d5fb564":"markdown"},"source":{"3bfe7efd":"import numpy as np  \nimport pandas as pd\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle","5165c266":"DATASET_COLUMNS=['target','ids','date','flag','user','text']\nDATASET_ENCODING = \"ISO-8859-1\"\ndataset = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding=DATASET_ENCODING,names=DATASET_COLUMNS, skiprows=795000, nrows = 10000)","e665aebf":"dataset.head()","1921f43c":"dataset.shape","dd3c6221":"dataset.info()","5e526be6":"dataset['target'].value_counts()","a9daf39d":"dataset.isnull().sum()","b2117420":"import seaborn as sns\nsns.countplot(x='target', data=dataset)","84c43451":"data=dataset[['text','target']]","4b8fbb54":"data['target'].unique()","b98cb652":"data['target'] = data['target'].replace(4,1)","bdc41d32":"data['target'].unique()","c0ab264a":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer","d3ec1c7d":"nltk.download('wordnet')","983f720b":"from collections import defaultdict","53826d56":"text = []\nfor i in range(0, int(data.shape[0])):\n    review = re.sub('[^a-zA-Z]', ' ', data['text'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n    lm = nltk.WordNetLemmatizer()\n    review = [lm.lemmatize(word) for word in review]\n    review = ' '.join(review)\n    text.append(review)","6d315e4a":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=5000)","e5fc2aea":"X = cv.fit_transform(text).toarray()","920e2432":"X.shape","1a59e3f9":"Y = data.target","3c9887b7":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.05, random_state = 0)","fb034caf":"from sklearn.model_selection import cross_val_score \nfrom sklearn.metrics import accuracy_score\ndef model_Evaluate(model, x_test,  y_test):\n    y_pred = model.predict(x_test)\n    print(accuracy_score(y_test, y_pred))","f30d2748":"from sklearn.linear_model import LogisticRegression\nclassifier1 = LogisticRegression(random_state = 0)\nclassifier1.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier1, X_test, Y_test)","347a32b6":"from sklearn.neighbors import KNeighborsClassifier\nclassifier2 = KNeighborsClassifier(n_neighbors = 50, metric = \"minkowski\", p=2)\nclassifier2.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier2, X_test, Y_test)","11594e33":"from sklearn.svm import SVC\nclassifier3 = SVC(kernel = \"linear\", random_state = 0)\nclassifier3.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier3, X_test, Y_test)","54b726b1":"from sklearn.svm import SVC\nclassifier4 = SVC(kernel = 'rbf', random_state = 0)\nclassifier4.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier4,X_test, Y_test)","6ff7c6e3":"from sklearn.tree import DecisionTreeClassifier\nclassifier5 = DecisionTreeClassifier(criterion = \"entropy\", random_state = 0)\nclassifier5.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier5, X_test, Y_test)","e387384e":"from sklearn.ensemble import RandomForestClassifier\nclassifier6 = RandomForestClassifier(n_estimators = 200, criterion = \"entropy\", random_state = 0)\nclassifier6.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier6, X_test, Y_test)","5791f641":"from sklearn.naive_bayes import GaussianNB\nclassifier7 = GaussianNB()\nclassifier7.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier7, X_test, Y_test)","6f18bd30":"from xgboost import XGBClassifier\nclassifier8 = XGBClassifier()\nclassifier8.fit(X_train, Y_train)\n\nmodel_Evaluate(classifier8, X_test, Y_test)","b3fa5299":"ann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units=40, activation='relu'))\nann.add(tf.keras.layers.Dense(units=20, activation='relu'))\nann.add(tf.keras.layers.Dense(units=10, activation='relu'))\nann.add(tf.keras.layers.Dense(units=5, activation='relu'))\nann.add(tf.keras.layers.Dense(units=2, activation='relu'))\nann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nann.fit(X_train, Y_train, batch_size = 32, epochs = 100, validation_split=0.2)","6b00b0a2":"y_pred = ann.predict(X_test)\ny_pred = (y_pred > 0.5)","be2c3070":"from sklearn.metrics import accuracy_score\naccuracy_score(Y_test , y_pred)","b1598da8":"**Decision Tree Classifier**","c13be374":"# **Step-8: Model Building**","414fe03a":"# **Step-3: Exploratory Data Analysis**","ecae5312":"# Conclusion","25030700":"*5.2: Print unique values of target*","4650a7f3":"# **Twitter Sentiment** **Analysis** ","bd7c9b92":"*5.6: Cleaning the tweet text*","99616b4e":"**Models used :**\n\n  Logistic Regression\n\n  K-Nearest Neighbors\n\n  Support Vector Machines (S.V.M.)\n\n  Kernel S.V.M.\n\n  Decision Tree Classifier\n\n  Random Forest Classifier\n\n  Naive Bayes\n\n  Artificial Neural Network\n\n  XG-Boost","9b5d0d1e":"*Step 5.5 : Importing necessary Dependencies*","4a819901":"**K-Nearest Neighbors**","73727923":"# **Step-1: Import Necessary Libraries**","6f00c32b":"**Logistic Regression**","28fd77be":"# **Step-2: Read and Load the Dataset**","2ccad222":"**Support Vector Machines (S.V.M.)**","16113504":"# **Step-7: Function For Model Evaluation**","46852642":"*3.4 : Checking for missing values*","2906b2cd":"**XG-Boost** ","6993dd2c":"**Kernel S.V.M.**","e4da1576":"*5.1: Selecting the text and Target column for our further analysis*","4bd9b687":"*K-fold cross Validation*","a9fad22b":"*5.4: Print unique values of target*","c06674f3":"*5.3: Replacing the values to ease understanding. (Assigning 1 to Positive sentiment 4)*","b92280c8":"*3.2: Shape of data*","75492f9a":"**Naive Bayes**","b3718078":"Upon evaluating all the models we can conclude that the **Logistic Regression** is the best model for the above-given dataset.","d4a70f02":"*3.1: Five top records of data*","6bcdd1e2":"**Random Forest Classifier**","a54bf3e6":"# **Step-5: Data Preprocessing**","8b7871c8":"*3.3: Data information*","d32e99e7":"# **Step-6: Splitting our dataset into Train and Test Subset**","0b96e7b9":"# **Step-4: Data Visualization of Target Variables**","2f542f4e":"*5.7: Creating bag of words model \/ Getting tokenization of tweet text*","88c5d17c":"*5.8: Fit the CountVectorizer and transform the data*","7d5fb564":"**Artificial Neural Network**"}}