{"cell_type":{"2fa368fe":"code","4ca15b99":"code","63a806d9":"code","81da5d40":"code","f9e9bca8":"code","e99d6ee0":"code","44bc2b13":"code","e27525dc":"code","9e8d5c33":"code","e053108c":"code","456ae9bf":"code","964956ca":"code","0c412d45":"code","526f7913":"code","13889308":"code","f87e63fb":"code","4fb72d5a":"code","6031c505":"code","fe41eeb5":"code","c64a6c20":"code","657fe1d0":"code","38dd80b9":"code","e720c672":"code","22f3b213":"code","02dc1069":"code","d4a473b8":"code","d0c24724":"code","2d6aeed3":"code","95ba4895":"code","7785b639":"code","c2da6337":"code","d641c5d4":"code","8c496a6a":"code","002fcc08":"code","e374e712":"code","8a0e5b20":"code","03cc7127":"code","52c3b88b":"code","fa1524c8":"code","53ad2fd0":"code","b86d0483":"code","9d3a85fe":"code","7f078648":"code","ff689455":"code","665f1884":"code","80fe6fe8":"code","f9a5b23d":"code","eb9c27d0":"code","c0b450ae":"code","f69d7488":"code","cf47b9f8":"code","3424decf":"code","7612ceb6":"code","e3274ec9":"code","5541ad11":"code","d9455afd":"markdown","ef323ee6":"markdown","8bbe3dd1":"markdown","9ac019cf":"markdown","8667245a":"markdown","0c4888d5":"markdown","c11fba71":"markdown","f40e51d3":"markdown","76440c0e":"markdown","66ca9367":"markdown","841584f4":"markdown","bcb1ea0a":"markdown","1dd3bd3f":"markdown","d221fdc1":"markdown","992c1fe3":"markdown","6063aabe":"markdown","65231997":"markdown","6ceed688":"markdown","80fc3760":"markdown","258a16be":"markdown","46b55228":"markdown","46429f6b":"markdown","601dcd52":"markdown","7a16d5de":"markdown","0b1ebcba":"markdown","ac96caf5":"markdown","6d04f5d8":"markdown","1bedcbd3":"markdown","610b587c":"markdown","ffe8acc7":"markdown","e5ed8d3e":"markdown","bf800aac":"markdown","58e3558c":"markdown","c18c5915":"markdown","40efbd0a":"markdown","e7174e59":"markdown"},"source":{"2fa368fe":"import pickle\nimport re\nimport string\nfrom ast import literal_eval\nfrom collections import Counter\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport spacy\nfrom PIL import Image\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud\n\n%matplotlib inline","4ca15b99":"# load CSV and construct DataFrame\ndf = pd.read_csv('..\/input\/ted-ultimate-dataset\/2020-05-01\/ted_talks_en.csv')\n\nprint(f'Shape: {df.shape}')","63a806d9":"df = df.loc[:, ['talk_id', 'topics', 'transcript']]\ndf.head()","81da5d40":"def find_topic(topic):\n    \"\"\"Returns a list of booleans for talks that contain a topic by index.\n    \n    :param topic: Topics or related topics of a talk\n    \"\"\"\n    has_topic = []\n    for t_list in df['topics']:\n        if topic.lower() in literal_eval(t_list):\n            has_topic.append(1)\n        else:\n            has_topic.append(0)\n    return has_topic","f9e9bca8":"# add columns for selected topics\ndf['is_sex'] = find_topic('sex')\ndf['is_religion'] = find_topic('religion')\ndf['is_politics'] = find_topic('politics')\ndf.head()","e99d6ee0":"# filter DataFrame to only include talks about sex, religion, and politics\ndf = df.loc[(df['is_sex']==1) | (df['is_religion']==1) | \n            (df['is_politics']==1), : ].reset_index(drop=True)\n\n# create new DataFrames for each topic (for later use)\nsex_df = df.loc[(df['is_sex']==1), 'talk_id':'transcript'].reset_index(drop=True)\nreligion_df = df.loc[(df['is_religion']==1), 'talk_id':'transcript'].reset_index(drop=True)\npolitics_df = df.loc[(df['is_politics']==1), 'talk_id':'transcript'].reset_index(drop=True)\n\nprint('Sex', sex_df.shape)\nprint('Religion', religion_df.shape)\nprint('Politics', politics_df.shape)","44bc2b13":"def combine_transcripts(transcript_list):\n    \"\"\"Input a list of transcripts and return them as a corpus.\n    :param list_of_text: Transcript list\"\"\"\n    corpus = ' '.join(transcript_list)\n    return corpus","e27525dc":"def transcripts_to_dict(df, topic_list):\n    \"\"\"Returns a dictionary of transcripts for each topic.\n    \n    :param df: DataFrame\n    :param topic_list: List of topics\n    \"\"\"\n    ted_dict = {}\n\n    for topic in topic_list:\n        # filter DataFrame to specific series and convert it to a list\n        filter_string = 'is_' + str(topic)\n        text_list = df.loc[(df[filter_string]==1), 'transcript'].to_list()\n\n        # call combine_transcripts function to return combined text\n        combined_text = combine_transcripts(text_list)\n\n        # add combined text to dict\n        ted_dict[topic] = combined_text\n    return ted_dict","9e8d5c33":"# create dictionary from the DataFrame\ntranscript_dict = transcripts_to_dict(df, ['sex', 'religion', 'politics'])\n\n# construct DataFrame from dictionary\ndf = pd.DataFrame.from_dict(transcript_dict, orient='index')\ndf.rename({0: 'transcript'}, axis=1, inplace=True)","e053108c":"df.head()","456ae9bf":"def clean_text(text):\n    \"\"\"Returns clean text.\n    Removes:\n        *text in square brackets & parenthesis\n        *punctuation\n        *words containing numbers\n        *double-quotes, dashes\n    \"\"\"\n#     text = text.lower()\n    text = re.sub('[\\[\\(].*?[\\)\\]]', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[\\\u201c\\\u2013]', '', text)\n    return text","964956ca":"# clean text\ndf['transcript'] = pd.DataFrame(df['transcript'].apply(lambda x: clean_text(x)))\nsex_df['transcript'] = pd.DataFrame(sex_df['transcript'].apply(lambda x: clean_text(x)))\nreligion_df['transcript'] = pd.DataFrame(religion_df['transcript'].apply(lambda x: clean_text(x)))\npolitics_df['transcript'] = pd.DataFrame(politics_df['transcript'].apply(lambda x: clean_text(x)))","0c412d45":"# create 'data' directory\n!mkdir data\n\n# pickle DataFrame (checkpoint)\ndf.to_pickle('data\/sex_religion_politics_corpus.pkl')\nsex_df.to_pickle('data\/sex_corpus.pkl')\nreligion_df.to_pickle('data\/religion_corpus.pkl')\npolitics_df.to_pickle('data\/politics_corpus.pkl')","526f7913":"# load SpaCy English model\nnlp = spacy.load('en_core_web_sm')\n\n# transform DataFrames to list of docs (corpus)\nall_srp_texts = df.loc[:, 'transcript'].to_list()\nsex_texts = sex_df.loc[:, 'transcript'].to_list()\nreligion_texts = religion_df.loc[:, 'transcript'].to_list()\npolitics_texts = politics_df.loc[:, 'transcript'].to_list()\n\n# process each corpus\nall_srp_docs = list(nlp.pipe(all_srp_texts))\nsex_docs = list(nlp.pipe(sex_texts))\nreligion_docs = list(nlp.pipe(religion_texts))\npolitics_docs = list(nlp.pipe(politics_texts))","13889308":"# print first 1000 characters\nsex_docs[0].text[:1000]","f87e63fb":"def get_all_text(spaCy_doc, topics):\n    \"\"\"Returns a dictionary of lemmatized text.\n    Keeps alphanumeric characters and non stop words.\n\n    :param spaCy_doc: spaCy Doc object (corpus) and topic list\n    \"\"\"\n    my_dict = {}\n    for ix, doc in enumerate(spaCy_doc):\n        tag = topics[ix]\n        token_list = []\n        for token in doc:\n            if (token.is_alpha==1) & (token.is_stop==0):\n                token_list.append((token.lemma_).lower())\n        my_dict[tag] = ' '.join(token_list)\n    return my_dict","4fb72d5a":"# create dictionary of transcripts with all text\nall_srp_text_dict = get_all_text(all_srp_docs, ['sex', 'religion', 'politics'])","6031c505":"# construct DataFrame\nall_srp_text_df = pd.DataFrame.from_dict(all_srp_text_dict, orient='index')\nall_srp_text_df.rename({0: 'transcript'}, axis=1, inplace=True)\nall_srp_text_df.head()","fe41eeb5":"# pickle DataFrame for later use\nall_srp_text_df.to_pickle('data\/all_srp_text_df.pkl')","c64a6c20":"def get_nouns_adj(spaCy_doc, topic_list):\n    \"\"\"\n    Input a spaCy Doc object (corpus) and topic list.\n    Return a dictionary of lemmatized nouns and adjectives per doc.\n    Keep alphanumeric characters and non stop words.\n    \"\"\"\n    topics = topic_list\n    my_dict = {}\n    for ix, doc in enumerate(spaCy_doc):\n        topic = topics[ix]\n        token_list = []\n        for token in doc:\n            if (token.is_alpha==1) & (token.is_stop==0) & (token.pos_ in ['NOUN', 'ADJ']):\n                token_list.append((token.lemma_).lower())\n        my_dict[topic] = ' '.join(token_list)\n    return my_dict","657fe1d0":"# create dictionary of transcripts with nouns and adjectives\nall_srp_nouns_adj_dict = get_nouns_adj(all_srp_docs, ['sex', 'religion', 'politics'])","38dd80b9":"# construct DataFrame\nall_srp_nouns_adj_df = pd.DataFrame.from_dict(all_srp_nouns_adj_dict, orient='index')\nall_srp_nouns_adj_df.rename({0: 'transcript'}, axis=1, inplace=True)","e720c672":"all_srp_nouns_adj_df","22f3b213":"# pickle DataFrame for later use\nall_srp_nouns_adj_df.to_pickle('data\/all_srp_nouns_adj_df.pkl')","02dc1069":"def get_stop_words(spaCy_doc_obj):\n    \"\"\"Returns a list of stop words from doc object.\n    \n    :param spaCy_doc_obj: spacy.tokens.doc.Doc object\n    \"\"\"\n    stop_words = []\n    for doc in spaCy_doc_obj:\n        for token in doc:\n            if token.is_stop:\n                stop_words.append(token.text.lower())\n    return set(stop_words)","d4a473b8":"# initial stop word list\nstop_words_spacy = list(get_stop_words(all_srp_docs))\n\n# pickle for later use\nwith open('data\/stop_words_spacy.pkl', 'wb') as f:\n    pickle.dump(stop_words_spacy, f)\n    f.close()","d0c24724":"def create_document_term_matrix(df):\n    \"\"\"Input a DataFrame and return a document-term matrix with initial stop words\"\"\"\n    cv = CountVectorizer(stop_words=stop_words_spacy)\n    data_cv = cv.fit_transform(df['transcript'])\n    dtm_df = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n    dtm_df.index = df.index\n    return dtm_df","2d6aeed3":"# create document-term matrices for corpora\nall_srp_all_text_dtm = create_document_term_matrix(all_srp_text_df)\nall_srp_nouns_adj_dtm = create_document_term_matrix(all_srp_nouns_adj_df)","95ba4895":"# pickle document-term matrices (for later use)\nall_srp_all_text_dtm.to_pickle('data\/all_srp_all_text_dtm.pkl')\nall_srp_nouns_adj_dtm.to_pickle('data\/all_srp_nouns_adj_dtm.pkl')","7785b639":"# load pickled document-term matrix\nall_srp_all_text_dtm = pd.read_pickle('data\/all_srp_all_text_dtm.pkl')\n\n# transpose document-term matrix\nall_srp_all_text_dtm_transposed = all_srp_all_text_dtm.transpose()","c2da6337":"# find the top words said by each topic\nn_words = 10\ntop_dict = {}\nfor topic in all_srp_all_text_dtm_transposed.columns:\n    top = all_srp_all_text_dtm_transposed[topic].sort_values(ascending=False).head(n_words)\n    top_dict[topic]= list(zip(top.index, top.values))\n    \n# print the top words said by each topic\nfor topic, top_words in top_dict.items():\n    print(topic)\n    print(', '.join([word for word, count in top_words[0:n_words]]))\n    print('---')","d641c5d4":"# look at the most common top words --> add them to the stop word list\n\n# let's first pull out the top words for each topic\nwords = []\nfor topic in all_srp_all_text_dtm_transposed.columns:\n    top = [word for (word, count) in top_dict[topic]]\n    for t in top:\n        words.append(t)","8c496a6a":"# let's aggregate this list and identify the most common words\nprint(Counter(words).most_common())","002fcc08":"# if all three topics have the top word, exclude it\nadd_stop_words = [word for word, count in Counter(words).most_common() if count >= 3]\nadd_stop_words","e374e712":"# add custom stop words\ncustom_stop_words = [\n#     'sex',\n    'world',\n#     'religion',\n#     'god',\n#     'religious',\n#     'political',\n#     'not',\n    'know',\n    'thing',\n    'know',\n    'think',\n    'come',\n#     'people'\n]","8a0e5b20":"# load initial stop words\nwith open('data\/stop_words_spacy.pkl', 'rb') as f:\n    initial_stop_words = list(pickle.load(f))\n    f.close()","03cc7127":"def update_stop_words(list_to_update, add_stop_words, custom_stop_words):\n    \"\"\"Add custom stop words to stop word list\"\"\"\n    stop_words = list_to_update\n    for word in add_stop_words:\n        stop_words.append(word)\n    for word in custom_stop_words:\n        stop_words.append(word)\n    return stop_words","52c3b88b":"# add new stop words\nstop_words_curated = update_stop_words(initial_stop_words, add_stop_words, custom_stop_words)","fa1524c8":"# recreate document-term matrix\ncv = CountVectorizer(stop_words=stop_words_curated)\n\n# load DataFrame\nall_srp_text_df = pd.read_pickle('data\/all_srp_text_df.pkl')\n\n# count vectorize the DataFrame\ndata_cv = cv.fit_transform(all_srp_text_df['transcript'])\n\n# construct new DataFrame\nall_srp_text_dtm_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\nall_srp_text_dtm_stop.index = all_srp_text_df.index","53ad2fd0":"word_cloud = WordCloud(stopwords=stop_words_curated, background_color='white', colormap='Dark2',\n                       max_font_size=150, random_state=2020, max_words=50)\n\nplt.rcParams['figure.figsize'] = [25, 10]\n\ntopics = ['sex', 'religion', 'politics']\n\n# create subplots for each topic\nfor index, topic in enumerate(all_srp_all_text_dtm_transposed.columns):\n    word_cloud.generate(all_srp_text_df.transcript[topic])\n    \n    plt.subplot(1, 3, index+1)\n    plt.imshow(word_cloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n#     plt.title(topics[index])\n    \nplt.show()","b86d0483":"# load pickled document-term matrix\nall_srp_nouns_adj_dtm = pd.read_pickle('data\/all_srp_nouns_adj_dtm.pkl')\n\n# transpose document-term matrix\nall_srp_nouns_adj_dtm_transposed = all_srp_nouns_adj_dtm.transpose()","9d3a85fe":"# find the top words said by each topic\nn_words = 10\ntop_dict = {}\nfor topic in all_srp_nouns_adj_dtm_transposed.columns:\n    top = all_srp_nouns_adj_dtm_transposed[topic].sort_values(ascending=False).head(n_words)\n    top_dict[topic]= list(zip(top.index, top.values))\n    \n# print the top words said by each topic\nfor topic, top_words in top_dict.items():\n    print(topic)\n    print(', '.join([word for word, count in top_words[0:n_words]]))\n    print('---')","7f078648":"# look at the most common top words --> add them to the stop word list\n\n# let's first pull out the top words for each topic\nwords = []\nfor topic in all_srp_nouns_adj_dtm_transposed.columns:\n    top = [word for (word, count) in top_dict[topic]]\n    for t in top:\n        words.append(t)","ff689455":"# let's aggregate this list and identify the most common words\nprint(Counter(words).most_common())","665f1884":"# if all three topics have the top word, exclude it\nadd_stop_words = [word for word, count in Counter(words).most_common() if count >= 3]\nadd_stop_words","80fe6fe8":"# Add custom stop words\ncustom_stop_words = [\n#     'sex',\n    'world',\n#     'religion',\n#     'god',\n#     'religious',\n#     'political',\n#     'not',\n#     'know',\n#     'thing',\n#     'know',\n#     'think',\n#     'come',\n#     'people'\n]","f9a5b23d":"# load initial stop words\nwith open('data\/stop_words_spacy.pkl', 'rb') as f:\n    initial_stop_words = list(pickle.load(f))\n    f.close()","eb9c27d0":"def update_stop_words(list_to_update, add_stop_words, custom_stop_words):\n    \"\"\"Add custom stop words to stop word list\"\"\"\n    stop_words = list_to_update\n    for word in add_stop_words:\n        stop_words.append(word)\n    for word in custom_stop_words:\n        stop_words.append(word)\n    return stop_words","c0b450ae":"# add new stop words\nstop_words_curated = update_stop_words(initial_stop_words, add_stop_words, custom_stop_words)","f69d7488":"# recreate document-term matrix\ncv = CountVectorizer(stop_words=stop_words_curated)\n\n# load DataFrame\nall_srp_nouns_adj_df = pd.read_pickle('data\/all_srp_nouns_adj_df.pkl')\n\n# count vectorize the DataFrame\ndata_cv = cv.fit_transform(all_srp_nouns_adj_df['transcript'])\n\n# construct new DataFrame\nall_srp_nouns_adj_dtm_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\nall_srp_nouns_adj_dtm_stop.index = all_srp_nouns_adj_df.index","cf47b9f8":"word_cloud = WordCloud(stopwords=stop_words_curated, background_color='white', colormap='Dark2',\n                       max_font_size=150, random_state=2020, max_words=50)\n\nplt.rcParams['figure.figsize'] = [25, 10]\n\ntopics = ['sex', 'religion', 'politics']\n\n# create subplots for each topic\nfor index, topic in enumerate(all_srp_nouns_adj_dtm_transposed.columns):\n    word_cloud.generate(all_srp_nouns_adj_df.transcript[topic])\n    \n    plt.subplot(1, 3, index+1)\n    plt.imshow(word_cloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n#     plt.title(topics[index])\n    \nplt.show()","3424decf":"ted_mask = np.array(Image.open(\"..\/input\/images\/ted_mask.png\"))","7612ceb6":"ted_df = all_srp_nouns_adj_df.copy()","e3274ec9":"all_transcripts = (ted_df.loc['sex', 'transcript']+ ted_df.loc['religion', 'transcript'] \n                   + ted_df.loc['politics', 'transcript'])","5541ad11":"# create a word cloud image\nwc = WordCloud(background_color=\"white\", mask=ted_mask, max_words=500,\n               stopwords=stop_words_curated, contour_width=3, \n               colormap='Dark2', contour_color='white')\n\n# generate a wordcloud\nwc.generate(all_transcripts)\n\n# store to file\nwc.to_file(\"ted_shaped_word_cloud.png\")\n\n# show\nplt.figure(figsize=[20,10])\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","d9455afd":"Our DataFrame `df` now has 3 rows where each value contains all the transcripts combined for the respective topic.","ef323ee6":"## Word Cloud \u2013\u00a0All Text","8bbe3dd1":"### Update stop words","9ac019cf":"### Word clouds","8667245a":"### Update document-term matrix","0c4888d5":"## Introduction\nThis notebook goes through process of creating word clouds.\n\nSpecifically, we'll be walking through:\n\n1. **Cleaning the data** - with popular text pre-processing techniques using spaCy\n2. **Organizing the data** - organize the cleaned data into corpora and a document-term matrix\n3. **Creating word clouds** \u2013 we'll find the most common words from different parts of speech and visualize them\n\nSince using the entire dataset would require high-computation, I'll be reducing the scope to these three topics: `sex`, `religion`, `politics`.","c11fba71":"# TED \u2013 Word Clouds","f40e51d3":"Let's update our document-term matrix with the new list of stop words","76440c0e":"## TED Shape","66ca9367":"I hope you enjoyed going through this notebook! If you have any questions, please feel free to comment.\n\nIf you'd like to learn more about creating word clouds, check out [DataCamp's tutorial](https:\/\/www.datacamp.com\/community\/tutorials\/wordcloud-python).","841584f4":"We'll be using spaCy for natural language processing.  \n\nIf you're interested in other languages, you can checkout the latest available models from [spaCy](https:\/\/spacy.io\/usage\/models).","bcb1ea0a":"Now that we have a corpus, let's lemmatize it and only keep alphanumeric characters without stop words.","1dd3bd3f":"### Stop words","d221fdc1":"Using the new columns we created, we can now filter our DataFrame by these topics and create new DataFrames.","992c1fe3":"### Update stop words","6063aabe":"## Document-Term Matrices","65231997":"We could create word clouds from this point, but we'll be updating the stop words to remove words that add little value including those that are commonly used in all three topics `sex`, `religion`, `politics`.","6ceed688":"Now we'll be doing a similar process as above, but only keeping `nouns & adjectives`.","80fc3760":"We're now ready to do some text pre-processing. This cleaning process can go on forever, but we'll be doing the following:\n\n**First cleaning step:**\n* Remove text in square brackets & parenthesis\n* Remove punctuation\n* Remove numerical values\n* Remove stop words\n* Tokenize text\n\n**Cleaning after tokenization:**\n* Stemming \/ lemmatization\n* Parts of speech tagging\n\nThe result will be two corpora:\n* all text \u2013 contains all parts of speech\n* nouns and adjectives\n\nWe could also include **verbs** or even **named entities**, but I've chosen to show **all text** and **nouns\/adjectives** to keep things simple (and not have you scroll endlessly).","258a16be":"To reduce the scope, let's filter for talks about `sex`, `religion`, `politics` to improve performance. Feel free to do the same for a topic of your choice.","46b55228":"## Data Cleaning","46429f6b":"Let's take a look at the sex corpus (part of it at least).","601dcd52":"# Word Clouds","7a16d5de":"## Clean text","0b1ebcba":"## Word Cloud \u2013\u00a0Nouns and Adjectives","ac96caf5":"### All talks DataFrame\nWith our topic-specific DataFrames created, we can now consruct a `3x1 DataFrame` with all talks for each topic for analyzing most common words.","6d04f5d8":"## Corpora","1bedcbd3":"Let's update our document-term matrix with the new list of stop words","610b587c":"### Corpus: *Nouns & adjectives*","ffe8acc7":"### Word clouds","e5ed8d3e":"### Update document-term matrix","bf800aac":"### Corpus: All text","58e3558c":"![](ted_shaped_word_cloud.png)","c18c5915":"Let's have a little fun and create a shape for our wordcloud.","40efbd0a":"To create our document-term matrices, we'll be using Scikit-Learn's CountVectorizer, where every row will represent a different document and every column will represent a different word.\n\nIn addition, with CountVectorizer, we can remove stop words.","e7174e59":"For this exercise, we'll only need `talk_id`, `transcript`, `topics` columns."}}