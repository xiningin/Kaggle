{"cell_type":{"5955f560":"code","4bb9ffb7":"code","87dd3860":"code","1d530bd6":"code","5589d103":"code","90ec2e45":"code","818ce3fc":"code","1a3e1a1c":"code","42509cbb":"code","757de221":"code","aa52fa77":"code","01756148":"code","df6eb6c5":"markdown"},"source":{"5955f560":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4bb9ffb7":"heart=pd.read_csv(\"..\/input\/heart.csv\")\nheart.head()","87dd3860":"heart.describe()","1d530bd6":"import matplotlib.pyplot as plt \nheart.hist(bins=50,figsize=(20,15))\nplt.show()","5589d103":"#Columns Names\nheart.columns","90ec2e45":"#Data Exploration\nimport seaborn as sns\nheart.target.value_counts()\nsns.countplot(x=\"target\", data=heart, palette=\"bwr\")\nplt.show()","818ce3fc":"df=heart\ncountNoDisease = len(df[df.target == 0])\ncountHaveDisease = len(df[df.target == 1])\nprint(\"Percentage of Patients Haven't Heart Disease: {:.2f}%\".format((countNoDisease \/ (len(df.target))*100)))\nprint(\"Percentage of Patients Have Heart Disease: {:.2f}%\".format((countHaveDisease \/ (len(df.target))*100)))","1a3e1a1c":"df.groupby('target').mean()","42509cbb":"#Create Dummy Variable\n","757de221":"#Logistic Regression\ny = df.target.values\nx_data = df.drop(['target'], axis = 1)\n\n\n","aa52fa77":"#Normalise Data\n\n# Normalize So, the entire range of values of X from min to max are mapped to the range 0 to 1.Min-max normalisation is often known as feature scaling \n# where the values of a numeric range of a feature of data, i.e. a property, are reduced to a scale between 0 and 1. \n# Yi = [Xi - min(X)]\/[max(X) - min(X)]\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data)).values\n\n#Visualisation of Normalized Data\nimport matplotlib.pyplot as plt \nx.hist(bins=50,figsize=(20,15))\nplt.show()","01756148":"#4 different automatic feature selection techniques:\n\n\n# Feature Extraction with RFE\nfrom pandas import read_csv\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n# load data\n\narray = heart.values\nX = array[:,0:13]\nY = array[:,13]\n# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model, 3)\nfit = rfe.fit(X, Y)\nprint(fit.n_features_)\nprint(fit.support_)\nprint(fit.ranking_)\n\n\n#PCA\n# Feature Extraction with PCA\nimport numpy\nfrom pandas import read_csv\nfrom sklearn.decomposition import PCA\narray = heart.values\nX = array[:,0:13]\nY = array[:,13]\n# feature extraction\npca = PCA(n_components=3)\nfit = pca.fit(X)\n# summarize components\nprint (fit.explained_variance_ratio_)\nprint(fit.components_)\n\n#FeatureImportance\nfrom pandas import read_csv\nfrom sklearn.ensemble import ExtraTreesClassifier\narray = heart.values\nX = array[:,0:13]\nY = array[:,13]\n# feature extraction\nmodel = ExtraTreesClassifier()\nmodel.fit(X, Y)\nprint(model.feature_importances_)","df6eb6c5":"**PCA for dimension reduction in order to pass from 14 to 2 or 3 dimension\n**"}}