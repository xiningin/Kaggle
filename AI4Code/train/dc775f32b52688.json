{"cell_type":{"55a76806":"code","c0fc4e14":"code","9d4f438d":"code","2eee6d6f":"code","c10095fb":"code","2ba5eaca":"code","8af00e78":"code","1c3e251b":"code","f2bacf02":"code","33dc02b2":"code","7e868456":"code","7855c17f":"code","ec873c87":"code","2f3e78c6":"code","1b3d2f53":"code","cf58c019":"code","1df16aa7":"code","33676673":"code","9554d17d":"code","00c3cdcb":"code","ace5996d":"code","c417ddda":"code","8af677a9":"code","a9674180":"code","80305b94":"code","dbd68541":"code","edce7526":"code","658bbc9e":"code","ff77cc39":"code","67b026c8":"code","9966adea":"code","9aa234df":"code","66c9eaa9":"code","b7f6533b":"code","a74e4f60":"code","1b8e8d7f":"code","ee45a500":"code","14301de6":"code","fab99081":"code","40375d48":"code","cc098993":"code","e779c42c":"code","1e2d8e57":"code","362caf40":"code","a394e42c":"code","bfa2a891":"code","3e2079be":"code","a4b2350a":"code","92584395":"code","3598a135":"code","57a64c94":"code","8033c171":"code","09672c61":"code","e639a0e0":"code","dd7ddacc":"code","0d654791":"markdown","8f949b5b":"markdown","f4bb88f0":"markdown","b3a16cf6":"markdown","34b2316a":"markdown","b18ca3d8":"markdown","5230d83e":"markdown","18004022":"markdown","518f2aa0":"markdown"},"source":{"55a76806":"data_dir = '..\/input\/optiver-realized-volatility-prediction\/'","c0fc4e14":"import os\nimport glob\nfrom joblib import Parallel, delayed\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport warnings\nfrom sklearn.cluster import KMeans\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)","9d4f438d":"# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x \/ y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))","2eee6d6f":"# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv(data_dir + '\/train.csv')\n    test = pd.read_csv(data_dir + '\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test","c10095fb":"# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    # \u304d\u3063\u3068100\u79d2\u523b\u307f\u306f\u3044\u3044\u7d50\u679c\u3092\u5f97\u308c\u306a\u304b\u3063\u305f\u306e\u3060\u308d\u3046\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n#     df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n#     df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n#     df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n#     df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n#     df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n","2ba5eaca":"# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n#     df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n#     df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n#     df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    #\n    # \u3053\u3053\u304b\u30b9\u30b3\u30a2\u306b\u76f8\u5f53\u5f71\u97ff\u3057\u3066\u308b\u3068\u8a18\u8f09\u3057\u3066\u308b\n    # \u306a\u3093\u3067\u3053\u308c\u304c\u52b9\u3044\u3066\u308b\u304b\u306f\u3044\u307e\u306e\u3068\u3053\u308d\u4e0d\u660e\n    # \u304a\u3063\u3066\u8abf\u3079\u308b\u3053\u3068\u306b\u3059\u308b\u3002\n    #\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        # \u5dee\u5206\u3092\u52d5\u3044\u305f\u5f8c\u306e\u4fa1\u683c\u3067\u5272\u3063\u3066*100\u3057\u3066\u308b\n        # \u5909\u52d5\u984d\u3092\u5909\u52d5\u5f8c\u306e\u4fa1\u683c\u3067\u5272\u308b\u3053\u3068\u3067\u6bd4\u7387\u306b\u3057\u3066\u3044\u308b(\u5c0f\u3055\u3044\u4fa1\u683c\u3060\u3068\u3053\u306e\u5024\u306f\u304a\u304a\u304d\u304f\u306a\u308b)\n        val = (df_diff\/price[1:])*100\n        # \u305d\u308c\u306bvol\u3092\u304b\u3051\u3066\u308b\u306e\u3067\u5909\u52d5\u6bd4\u7387\u306b\u5927\u304d\u3055\u3092\u304b\u3051\u308b\u306e\u3067\u3053\u306e\u5024\u304c\u5927\u304d\u3044\u3068\u5024\u306e\u6bd4\u7387\u304c\u5927\u304d\u304f\u52d5\u3044\u305f\u3053\u3068\u306b\u306a\u308b\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    # time_id\u306b\u5bfe\u5fdc\u3059\u308bdf\u3092\u629c\u304d\u51fa\u3059\u3002\n    # time_id\u306b\u7d71\u8a08\u6307\u6a19\u306a\u306e\u3067\u30ea\u30fc\u30af\u3057\u3066\u3044\u308b\n    \n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]\n        \n        # power\u3063\u3066\u547c\u3070\u308c\u308b\u6307\u6a19\u3092\u5f97\u308b\u3002\n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        \n        # \u5e73\u5747\u4ee5\u4e0a\u306eprice\u306e\u5408\u8a08\u3068\u5e73\u5747\u4ee5\u4e0b\u306eprice\u306e\u5408\u8a08\u5024\n        # \u5168\u304f\u3082\u3063\u3066\u3044\u3089\u306a\u3044\u30c7\u30fc\u30bf\u306b\u3057\u304b\u898b\u3048\u306a\u3044\u3002\n        # \u5916\u308c\u5024\u306e\u5f71\u97ff\u5f15\u304f\u6c17\u304c\u3059\u308b\u3057\n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        \n        #\n        # \u6b63\u306e\u5dee\u5206\u306e\u5408\u8a08\u5024\u3068\u8ca0\u306e\u5dee\u5206\u306e\u5408\u8a08\u5024\n        # \u3044\u308b\u306e\u304b\u3053\u308c\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        \n        \n        # \u504f\u5dee\u306e\u4e2d\u592e\u5024\n        abs_diff = np.median(np.abs(df_id['price'].values - np.mean(df_id['price'].values)))  \n        # \u4fa1\u683c\u306e\u4e8c\u4e57\u306e\u5e73\u5747\u5024\n        energy = np.mean(df_id['price'].values**2)\n        # \u7b2c3-\u7b2c\uff11\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # size\u306b\u5bfe\u3057\u3066\u3082\u3046\u3048\u3068\u540c\u69d8\u306e\u3053\u3068\n        abs_diff_v = np.median(np.abs(df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n        \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n#     df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n#     df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150','time_id'], axis = 1, inplace = True)\n    \n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n","8af00e78":"# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n#     vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility',\n#                 'log_return1_realized_volatility_600', 'log_return2_realized_volatility_600', \n#                 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400',\n# #                 'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', \n#                 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200',\n# #                 'log_return1_realized_volatility_100', 'log_return2_realized_volatility_100', \n#                 'trade_log_return_realized_volatility',\n#                 'trade_log_return_realized_volatility_600', \n#                 'trade_log_return_realized_volatility_400',\n# #                 'trade_log_return_realized_volatility_300',\n# #                 'trade_log_return_realized_volatility_100',\n#                 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","1c3e251b":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","f2bacf02":"# \u91cd\u8981\u5ea6\u89e3\u6790\ndef calc_model_importance(model, feature_names=None, importance_type='gain'):\n    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n                                 index=feature_names,\n                                 columns=['importance']).sort_values('importance')\n    return importance_df","33dc02b2":"def calc_mean_importance(importance_df_list):\n    mean_importance = np.mean(\n        np.array([df['importance'].values for df in importance_df_list]), axis=0)\n    mean_df = importance_df_list[0].copy()\n    mean_df['importance'] = mean_importance\n    \n    return mean_df","7e868456":"# \u753b\u50cf\u4fdd\u5b58\u7528\nimport matplotlib.pyplot as plt\ndef plot_importance(importance_df, title='',\n                    save_filepath=None, figsize=(8, 12)):\n    fig, ax = plt.subplots(figsize=figsize)\n    importance_df.plot.barh(ax=ax)\n    if title:\n        plt.title(title)\n    plt.tight_layout()\n    if save_filepath is None:\n        plt.show()\n    else:\n        plt.savefig(save_filepath)\n    plt.close()","7855c17f":"# Read train and test\ntrain, test = read_train_test()\n\n# Get unique stock ids \ntrain_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntrain_ = preprocessor(train_stock_ids, is_train = True)\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)","ec873c87":"train.shape","2f3e78c6":"pd.to_pickle(train,'train(307)_notau_noKNN.pkl')\npd.to_pickle(test,'test(307)_notau_noKNN.pkl')","1b3d2f53":"train = pd.read_pickle('.\/train(307)_notau_noKNN.pkl')\ntest = pd.read_pickle('test(307)_notau_noKNN.pkl')\ntrain.shape","cf58c019":"test.shape","1df16aa7":"# making agg features\n\n# time_id\u6bce\u306estockid\u6bce\u306etarget\u5909\u6570\u306e\u4e00\u89a7\ntrain_p = pd.read_csv(data_dir + '\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\n# \ncorr = train_p.corr()\n\n# stick_id\u306e\u76f8\u95a2\u4fc2\u6570\u306eindex\u3001\u3064\u307e\u308astockid\nids = corr.index\n\n# 7\u306b\u95a2\u3057\u3066\u306f\u8abf\u3079\u308b(\u30b7\u30eb\u30a8\u30c3\u30c8\u56f3\u3068\u304b\u3067)\n# time_id\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3068\u3057\u305f\u6642\u306estock_id\u76f8\u95a2\u4fc2\u6570\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n# print(kmeans.labels_)\n\n\n# l\u306brange(7)\u3068\u7b49\u3057\u3044\u30af\u30e9\u30b9\u30bf\u3092\u64ae\u3063\u305f\u6642\u306estock_id\u3092\u683c\u7d0d\u3057\u3066\u308b\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    # stock_id\u304c\u6307\u5b9a\u3055\u308c\u3066\u308b\u30af\u30e9\u30b9\u30bf\u3068\u540c\u3058\u3082\u306e(\u5168\u4f53\u30b5\u30f3\u30d7\u30eb)\u3092\u5f15\u3044\u3066\u304f\u308b\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    # time_id\u6bce\u306b\u5e73\u5747\u5024\u3092\u53d6\u308b(\u7570\u306a\u308bstock_id\u3067\u3082\u30af\u30e9\u30b9\u30bf\u304c\u540c\u3058\u306a\u3082\u306e\u540c\u58eb\u306e\u5e73\u5747\u5024\u306b\u306a\u308b)\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    \n    #\n    # stock_id\u305d\u306e\u3082\u306e\u306b\u3044\u307f\u304c\u306a\u304f\u306a\u3063\u305f\u306e\u3067\u6ce8\u610f\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\n    # stock_id\u3092\u30af\u30bf\u30b9\u30bfid\u306b\u5909\u66f4\u3057\u3066\u305d\u306e\u5f8c\u306bc1?\u3063\u3066\u3044\u3046\u306e\u3092\u3064\u3051\u3066\u308b\n    #\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()","33676673":"matTest = []\nmat = []\nkmeans = []","9554d17d":"#mat2 #= mat1.pivot(index='time_id', columns='stock_idmat2\n# \u4f55\u3067\u305d\u3093\u306a\u3053\u3068\u3057\u305f\u3093\u3084\u7b11\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])","00c3cdcb":"# \u3053\u306e\u66f8\u304d\u65b9\u4fbf\u5229\u3001\u899a\u3048\u3066\u304a\u304f\n\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","ace5996d":"# \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u884c\u3063\u305f\u7d50\u679c\u306e\u4f7f\u3046\u7279\u5fb4\u91cf\u3060\u3051\u629c\u304d\u51fa\u3057\u3066\u304d\u3066\u308b\uff0810\/314\uff09\n# 2.5\u306b\u304b\u3093\u3057\u3066\u306f\u30af\u30e9\u30b9\u30bf\u306e\u4e2d\u304c\u3042\u307e\u308a\u306b\u5c0f\u3055\u3044\u306e\u3067\u9664\u304f\n# \u306a\u305c\u3001\u3053\u306e\u7279\u5fb4\u91cf\u306b\u3057\u305f\u306e\u304b\u306f\u4e0d\u660e\u306e\u305f\u3081\u3001\u8ffd\u52a0\u5b9f\u9a13\u304c\u5fc5\u8981\n# log_return2\u3092\u524a\u3063\u305f\u7406\u7531\u306f\u308f\u304b\u3089\u3093\uff08\u304a\u305d\u3089\u304f\u76f8\u95a2\u304c\u4f3c\u3061\u3083\u3046\u304b\u3089\uff09\n\nnnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_2c1',\n     'log_return1_realized_volatility_3c1',     \n     'log_return1_realized_volatility_4c1',\n     'total_volume_mean_0c1',\n     'total_volume_mean_1c1', \n     'total_volume_mean_2c1',\n     'total_volume_mean_3c1', \n     'total_volume_mean_4c1',\n     'trade_size_mean_0c1',\n     'trade_size_mean_1c1', \n     'trade_size_mean_2c1',\n     'trade_size_mean_3c1', \n     'trade_size_mean_4c1',\n     'trade_order_count_mean_0c1',\n     'trade_order_count_mean_1c1',\n     'trade_order_count_mean_2c1',\n     'trade_order_count_mean_3c1',\n     'trade_order_count_mean_4c1',      \n     'price_spread_mean_0c1',\n     'price_spread_mean_1c1',\n     'price_spread_mean_2c1',\n     'price_spread_mean_3c1',\n     'price_spread_mean_4c1',   \n     'bid_spread_mean_0c1',\n     'bid_spread_mean_1c1',\n     'bid_spread_mean_2c1',\n     'bid_spread_mean_3c1',\n     'bid_spread_mean_4c1',       \n     'ask_spread_mean_0c1',\n     'ask_spread_mean_1c1',\n     'ask_spread_mean_2c1',\n     'ask_spread_mean_3c1',\n     'ask_spread_mean_4c1',   \n     'volume_imbalance_mean_0c1',\n     'volume_imbalance_mean_1c1',\n     'volume_imbalance_mean_2c1',\n     'volume_imbalance_mean_3c1',\n     'volume_imbalance_mean_4c1',       \n     'bid_ask_spread_mean_0c1',\n     'bid_ask_spread_mean_1c1',\n     'bid_ask_spread_mean_2c1',\n     'bid_ask_spread_mean_3c1',\n     'bid_ask_spread_mean_4c1',\n] ","c417ddda":"# train\u3068\u304f\u3063\u3064\u3051\u308b\ntrain = pd.merge(train,mat1[nnn],how='left',on='time_id')","8af677a9":"train.shape","a9674180":"train.head()","80305b94":"test.head()","dbd68541":"test = pd.merge(test,mat2[nnn],how='left',on='time_id')","edce7526":"train.shape","658bbc9e":"test.shape","ff77cc39":"test.head()","67b026c8":"# Split features and target\nx = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nx_test = test.drop(['row_id', 'time_id'], axis = 1)\n# Transform stock id to a numeric value\nx['stock_id'] = x['stock_id'].astype(int)\nx_test['stock_id'] = x_test['stock_id'].astype(int)\n\n# Create out of folds array\noof_predictions = np.zeros(x.shape[0])\n# Create test array to store predictions\ntest_predictions = np.zeros(x_test.shape[0])","9966adea":"seed = 29\nparams = {\n    'learning_rate': 0.1,        \n    'lambda_l1': 2,\n    'lambda_l2': 7,\n    'num_leaves': 800,\n    'min_sum_hessian_in_leaf': 20,\n    'feature_fraction': 0.8,\n    'feature_fraction_bynode': 0.8,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 42,\n    'min_data_in_leaf': 700,\n    'max_depth': 4,\n    'seed': seed,\n    'feature_fraction_seed': seed,\n    'bagging_seed': seed,\n    'drop_seed': seed,\n    'data_random_seed': seed,\n    'objective': 'rmse',\n    'boosting': 'gbdt',\n    'verbosity': -1,\n    'n_jobs': -1,\n} ","9aa234df":"from sklearn.model_selection import GroupKFold\ngain_importance_list = []\nsplit_importance_list = []\ngroup = train['time_id']\nkf = GroupKFold(n_splits=5)\n# Iterate through each fold\nfor fold, (trn_ind, val_ind) in enumerate(kf.split(x, groups=group)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n    # Root mean squared percentage error weights\n    train_weights = 1 \/ np.square(y_train)\n    val_weights = 1 \/ np.square(y_val)\n    train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n    val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n    model = lgb.train(params = params, \n                      train_set = train_dataset, \n                      categorical_feature = ['stock_id'],\n                      valid_sets = [train_dataset, val_dataset], \n                      num_boost_round = 5000, \n                      early_stopping_rounds = 30, \n                      verbose_eval = 100,\n                      feval = feval_rmspe)\n\n    # \u3053\u306e\u66f8\u304d\u65b9\u3059\u308b\u3053\u3068\u3067\u3001\u5168\u30c7\u30fc\u30bf\u3092OOf\u306b\u3057\u3066rmspe\u304c\u6c42\u3081\u3089\u308c\u308b\u3001\n    # \u899a\u3048\u3066\u304a\u3044\u305f\u65b9\u304c\u3044\u3044\n    oof_predictions[val_ind] = model.predict(x_val)\n    # Predict the test set\n    test_predictions += model.predict(x_test) \/ 5\n\n    feature_names = x_train.columns.values.tolist()\n    gain_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='gain')\n    gain_importance_list.append(gain_importance_df)\n\n    split_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='split')\n    split_importance_list.append(split_importance_df)\n\nrmspe_score = rmspe(y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","66c9eaa9":"mean_gain_df = calc_mean_importance(gain_importance_list)\nmean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\nmean_gain_df.to_csv('gain_importance_mean groupkfold 352\u3000KNN notau.csv', index=False)","b7f6533b":"test['target'] = test_predictions\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","a74e4f60":"train = pd.read_pickle('.\/train(307)_notau_noKNN.pkl')\ntest = pd.read_pickle('.\/test(307)_notau_noKNN.pkl')","1b8e8d7f":"# \u3053\u3063\u3061\u306f\u4fa1\u683c\u304c\u52d5\u304f\u3069\u3046\u3053\u3046\u3067\u306f\u306a\u304f\u3066\u3001\u53d6\u5f15\u56de\u6570\u306e\u5408\u8a08\u306e\u5024\n# \u5358\u4f4d\u3092\u305d\u308d\u3048\u308b\u306e\u3068\u3001\u50be\u5411\u304c\u306a\u3093\u3068\u306a\u304f\u307f\u3048\u308b\u306e\u304b\uff01\n\ntrain['size_tau2'] = np.sqrt( 1\/ train['trade_order_count_sum'] )\ntest['size_tau2'] = np.sqrt( 1\/ test['trade_order_count_sum'] )\ntrain['size_tau2_450'] = np.sqrt( 0.25\/ train['trade_order_count_sum'] )\ntest['size_tau2_450'] = np.sqrt( 0.25\/ test['trade_order_count_sum'] )\ntrain['size_tau2_300'] = np.sqrt( 0.5\/ train['trade_order_count_sum'] )\ntest['size_tau2_300'] = np.sqrt( 0.5\/ test['trade_order_count_sum'] )\ntrain['size_tau2_150'] = np.sqrt( 0.75\/ train['trade_order_count_sum'] )\ntest['size_tau2_150'] = np.sqrt( 0.75\/ test['trade_order_count_sum'] )\n\n# delta tau\ntrain['size_tau2_600-450'] = train['size_tau2_450'] - train['size_tau2']\ntest['size_tau2_600-450'] = test['size_tau2_450'] - test['size_tau2']\ntrain['size_tau2_600-300'] = train['size_tau2_300'] - train['size_tau2']\ntest['size_tau2_600-300'] = test['size_tau2_300'] - test['size_tau2']\ntrain['size_tau2_600-150'] = train['size_tau2_150'] - train['size_tau2']\ntest['size_tau2_600-150'] = test['size_tau2_150'] - test['size_tau2']\n\ntrain['size_tau2_450-300'] = train['size_tau2_300'] - train['size_tau2_450']\ntest['size_tau2_450-300'] = test['size_tau2_300'] - test['size_tau2_450']\ntrain['size_tau2_450-150'] = train['size_tau2_150'] - train['size_tau2_450']\ntest['size_tau2_450-150'] = test['size_tau2_150'] - test['size_tau2_450']\ntrain['size_tau2_300-150'] = train['size_tau2_150'] - train['size_tau2_300']\ntest['size_tau2_300-150'] = test['size_tau2_150'] - test['size_tau2_300']","ee45a500":"pd.to_pickle(train, 'train(317)_tau_noKNN.pkl')\npd.to_pickle(test, 'test(317)_tau_noKNN.pkl')","14301de6":"train.shape","fab99081":"# Split features and target\nx = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nx_test = test.drop(['row_id', 'time_id'], axis = 1)\n# Transform stock id to a numeric value\nx['stock_id'] = x['stock_id'].astype(int)\nx_test['stock_id'] = x_test['stock_id'].astype(int)\n\n# Create out of folds array\noof_predictions = np.zeros(x.shape[0])\n# Create test array to store predictions\ntest_predictions = np.zeros(x_test.shape[0])","40375d48":"x.shape","cc098993":"seed = 29\nparams = {\n    'learning_rate': 0.1,        \n    'lambda_l1': 2,\n    'lambda_l2': 7,\n    'num_leaves': 800,\n    'min_sum_hessian_in_leaf': 20,\n    'feature_fraction': 0.8,\n    'feature_fraction_bynode': 0.8,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 42,\n    'min_data_in_leaf': 700,\n    'max_depth': 4,\n    'seed': seed,\n    'feature_fraction_seed': seed,\n    'bagging_seed': seed,\n    'drop_seed': seed,\n    'data_random_seed': seed,\n    'objective': 'rmse',\n    'boosting': 'gbdt',\n    'verbosity': -1,\n    'n_jobs': -1,\n} ","e779c42c":"oof = pd.DataFrame()                 # out-of-fold result\nmodels = []                          # models\nscores = 0.0                         # validation score\n\ngain_importance_list = []\nsplit_importance_list = []\n\nfrom sklearn.model_selection import GroupKFold\ngroup = train['time_id']\nkf = GroupKFold(n_splits=5)\n# Iterate through each fold\nfor fold, (trn_ind, val_ind) in enumerate(kf.split(x, groups=group)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n    # Root mean squared percentage error weights\n    train_weights = 1 \/ np.square(y_train)\n    val_weights = 1 \/ np.square(y_val)\n    train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n    val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n    model = lgb.train(params = params, \n                      train_set = train_dataset, \n                      categorical_feature = ['stock_id'],\n                      valid_sets = [train_dataset, val_dataset], \n                      num_boost_round = 5000, \n                      early_stopping_rounds = 30, \n                      verbose_eval = 100,\n                      feval = feval_rmspe)\n\n    # \u3053\u306e\u66f8\u304d\u65b9\u3059\u308b\u3053\u3068\u3067\u3001\u5168\u30c7\u30fc\u30bf\u3092OOf\u306b\u3057\u3066rmspe\u304c\u6c42\u3081\u3089\u308c\u308b\u3001\n    # \u899a\u3048\u3066\u304a\u3044\u305f\u65b9\u304c\u3044\u3044\n    oof_predictions[val_ind] = model.predict(x_val)\n    # Predict the test set\n    test_predictions += model.predict(x_test) \/ 5\n\n    feature_names = x_train.columns.values.tolist()\n    gain_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='gain')\n    gain_importance_list.append(gain_importance_df)\n\n    split_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='split')\n    split_importance_list.append(split_importance_df)\n\nrmspe_score = rmspe(y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","1e2d8e57":"train.shape","362caf40":"mean_gain_df = calc_mean_importance(gain_importance_list)\nmean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\nmean_gain_df.to_csv('gain_importance_mean kfold 317 tau.csv', index=False)","a394e42c":"train.shape","bfa2a891":"# making agg features\n\n# time_id\u6bce\u306estockid\u6bce\u306etarget\u5909\u6570\u306e\u4e00\u89a7\ntrain_p = pd.read_csv(data_dir + '\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\n# \ncorr = train_p.corr()\n\n# stick_id\u306e\u76f8\u95a2\u4fc2\u6570\u306eindex\u3001\u3064\u307e\u308astockid\nids = corr.index\n\n# 7\u306b\u95a2\u3057\u3066\u306f\u8abf\u3079\u308b(\u30b7\u30eb\u30a8\u30c3\u30c8\u56f3\u3068\u304b\u3067)\n# time_id\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3068\u3057\u305f\u6642\u306estock_id\u76f8\u95a2\u4fc2\u6570\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n# print(kmeans.labels_)\n\n\n# l\u306brange(7)\u3068\u7b49\u3057\u3044\u30af\u30e9\u30b9\u30bf\u3092\u64ae\u3063\u305f\u6642\u306estock_id\u3092\u683c\u7d0d\u3057\u3066\u308b\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    # stock_id\u304c\u6307\u5b9a\u3055\u308c\u3066\u308b\u30af\u30e9\u30b9\u30bf\u3068\u540c\u3058\u3082\u306e(\u5168\u4f53\u30b5\u30f3\u30d7\u30eb)\u3092\u5f15\u3044\u3066\u304f\u308b\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    # time_id\u6bce\u306b\u5e73\u5747\u5024\u3092\u53d6\u308b(\u7570\u306a\u308bstock_id\u3067\u3082\u30af\u30e9\u30b9\u30bf\u304c\u540c\u3058\u306a\u3082\u306e\u540c\u58eb\u306e\u5e73\u5747\u5024\u306b\u306a\u308b)\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    \n    #\n    # stock_id\u305d\u306e\u3082\u306e\u306b\u3044\u307f\u304c\u306a\u304f\u306a\u3063\u305f\u306e\u3067\u6ce8\u610f\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\n    # stock_id\u3092\u30af\u30bf\u30b9\u30bfid\u306b\u5909\u66f4\u3057\u3066\u305d\u306e\u5f8c\u306bc1?\u3063\u3066\u3044\u3046\u306e\u3092\u3064\u3051\u3066\u308b\n    #\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()","3e2079be":"matTest = []\nmat = []\nkmeans = []\n#mat2 #= mat1.pivot(index='time_id', columns='stock_idmat2\n# \u4f55\u3067\u305d\u3093\u306a\u3053\u3068\u3057\u305f\u3093\u3084\u7b11\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n\n# \u3053\u306e\u66f8\u304d\u65b9\u4fbf\u5229\u3001\u899a\u3048\u3066\u304a\u304f\n\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","a4b2350a":"# \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u884c\u3063\u305f\u7d50\u679c\u306e\u4f7f\u3046\u7279\u5fb4\u91cf\u3060\u3051\u629c\u304d\u51fa\u3057\u3066\u304d\u3066\u308b\uff0810\/314\uff09\n# 2.5\u306b\u304b\u3093\u3057\u3066\u306f\u30af\u30e9\u30b9\u30bf\u306e\u4e2d\u304c\u3042\u307e\u308a\u306b\u5c0f\u3055\u3044\u306e\u3067\u9664\u304f\n# \u306a\u305c\u3001\u3053\u306e\u7279\u5fb4\u91cf\u306b\u3057\u305f\u306e\u304b\u306f\u4e0d\u660e\u306e\u305f\u3081\u3001\u8ffd\u52a0\u5b9f\u9a13\u304c\u5fc5\u8981\n# log_return2\u3092\u524a\u3063\u305f\u7406\u7531\u306f\u308f\u304b\u3089\u3093\uff08\u304a\u305d\u3089\u304f\u76f8\u95a2\u304c\u4f3c\u3061\u3083\u3046\u304b\u3089\uff09\n\nnnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_2c1',\n     'log_return1_realized_volatility_3c1',     \n     'log_return1_realized_volatility_4c1',\n     'total_volume_mean_0c1',\n     'total_volume_mean_1c1', \n     'total_volume_mean_2c1',\n     'total_volume_mean_3c1', \n     'total_volume_mean_4c1',\n     'trade_size_mean_0c1',\n     'trade_size_mean_1c1', \n     'trade_size_mean_2c1',\n     'trade_size_mean_3c1', \n     'trade_size_mean_4c1',\n     'trade_order_count_mean_0c1',\n     'trade_order_count_mean_1c1',\n     'trade_order_count_mean_2c1',\n     'trade_order_count_mean_3c1',\n     'trade_order_count_mean_4c1',      \n     'price_spread_mean_0c1',\n     'price_spread_mean_1c1',\n     'price_spread_mean_2c1',\n     'price_spread_mean_3c1',\n     'price_spread_mean_4c1',   \n     'bid_spread_mean_0c1',\n     'bid_spread_mean_1c1',\n     'bid_spread_mean_2c1',\n     'bid_spread_mean_3c1',\n     'bid_spread_mean_4c1',       \n     'ask_spread_mean_0c1',\n     'ask_spread_mean_1c1',\n     'ask_spread_mean_2c1',\n     'ask_spread_mean_3c1',\n     'ask_spread_mean_4c1',   \n     'volume_imbalance_mean_0c1',\n     'volume_imbalance_mean_1c1',\n     'volume_imbalance_mean_2c1',\n     'volume_imbalance_mean_3c1',\n     'volume_imbalance_mean_4c1',       \n     'bid_ask_spread_mean_0c1',\n     'bid_ask_spread_mean_1c1',\n     'bid_ask_spread_mean_2c1',\n     'bid_ask_spread_mean_3c1',\n     'bid_ask_spread_mean_4c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_2c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_450_0c1',\n     'size_tau2_450_1c1',\n     'size_tau2_450_2c1',\n     'size_tau2_450_3c1',\n     'size_tau2_450_4c1',\n     'size_tau2_300_0c1',\n     'size_tau2_300_1c1',\n     'size_tau2_300_2c1', \n     'size_tau2_300_3c1', \n     'size_tau2_300_4c1', \n     'size_tau2_150_0c1',\n     'size_tau2_150_1c1',\n     'size_tau2_150_2c1', \n     'size_tau2_150_3c1', \n     'size_tau2_150_4c1',          \n     'size_tau2_600-450_0c1',\n     'size_tau2_600-450_1c1',\n     'size_tau2_600-450_2c1',\n     'size_tau2_600-450_3c1',\n     'size_tau2_600-450_4c1',\n     'size_tau2_600-300_0c1',\n     'size_tau2_600-300_1c1',\n     'size_tau2_600-300_2c1',\n     'size_tau2_600-300_3c1',\n     'size_tau2_600-300_4c1',\n     'size_tau2_600-150_0c1',\n     'size_tau2_600-150_1c1',\n     'size_tau2_600-150_2c1',\n     'size_tau2_600-150_3c1',\n     'size_tau2_600-150_4c1',          \n     'size_tau2_450-300_0c1',\n     'size_tau2_450-300_1c1',\n     'size_tau2_450-300_2c1',\n     'size_tau2_450-300_3c1',\n     'size_tau2_450-300_4c1',\n     'size_tau2_450-150_0c1',\n     'size_tau2_450-150_1c1',\n     'size_tau2_450-150_2c1',\n     'size_tau2_450-150_3c1',\n     'size_tau2_450-150_4c1',            \n     'size_tau2_300-150_0c1',\n     'size_tau2_300-150_1c1',\n     'size_tau2_300-150_2c1',\n     'size_tau2_300-150_3c1',\n     'size_tau2_300-150_4c1',            \n      ] ","92584395":"# train\u3068\u304f\u3063\u3064\u3051\u308b\ntrain = pd.merge(train,mat1[nnn],how='left',on='time_id')\ntest = pd.merge(test,mat2[nnn],how='left',on='time_id')","3598a135":"train.shape","57a64c94":"pd.to_pickle(train, 'train(412)_tau_KNN.pkl')\npd.to_pickle(test, 'test(412)_tau_KNN.pkl')","8033c171":"# Split features and target\nx = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nx_test = test.drop(['row_id', 'time_id'], axis = 1)\n# Transform stock id to a numeric value\nx['stock_id'] = x['stock_id'].astype(int)\nx_test['stock_id'] = x_test['stock_id'].astype(int)\n\n# Create out of folds array\noof_predictions = np.zeros(x.shape[0])\n# Create test array to store predictions\ntest_predictions = np.zeros(x_test.shape[0])","09672c61":"oof = pd.DataFrame()                 # out-of-fold result\nmodels = []                          # models\nscores = 0.0                         # validation score\n\ngain_importance_list = []\nsplit_importance_list = []\n\nfrom sklearn.model_selection import GroupKFold\ngroup = train['time_id']\nkf = GroupKFold(n_splits=5)\n# Iterate through each fold\nfor fold, (trn_ind, val_ind) in enumerate(kf.split(x, groups=group)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n    # Root mean squared percentage error weights\n    train_weights = 1 \/ np.square(y_train)\n    val_weights = 1 \/ np.square(y_val)\n    train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n    val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n    model = lgb.train(params = params, \n                      train_set = train_dataset, \n                      categorical_feature = ['stock_id'],\n                      valid_sets = [train_dataset, val_dataset], \n                      num_boost_round = 5000, \n                      early_stopping_rounds = 30, \n                      verbose_eval = 100,\n                      feval = feval_rmspe)\n\n    # \u3053\u306e\u66f8\u304d\u65b9\u3059\u308b\u3053\u3068\u3067\u3001\u5168\u30c7\u30fc\u30bf\u3092OOf\u306b\u3057\u3066rmspe\u304c\u6c42\u3081\u3089\u308c\u308b\u3001\n    # \u899a\u3048\u3066\u304a\u3044\u305f\u65b9\u304c\u3044\u3044\n    oof_predictions[val_ind] = model.predict(x_val)\n    # Predict the test set\n    test_predictions += model.predict(x_test) \/ 5\n\n    feature_names = x_train.columns.values.tolist()\n    gain_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='gain')\n    gain_importance_list.append(gain_importance_df)\n\n    split_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='split')\n    split_importance_list.append(split_importance_df)\n\nrmspe_score = rmspe(y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","e639a0e0":"mean_gain_df = calc_mean_importance(gain_importance_list)\nmean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\nmean_gain_df.to_csv('gain_importance_mean kfold 412 tau KNN.csv', index=False)","dd7ddacc":"mean_gain_df.set_index('feature_names').filter(like='tau', axis=0).sort_values('importance', ascending=False).head(15)","0d654791":"## Discussion\nCV : 0.22013  \nNo clustering CV : 0.2253  \n\nBy adding the clustering index, the cv was significantly improved.\nHowever, when I looked at the feature importance, it didn't seem to have that much of an effect.  \nI wonder if it just happens to match the results of tuning, or if it goes up as well...\n\n~So, what happens if we cluster by time_id?  \nIf you see a correlation between time_id 1 and 2, and you give a statistical measure, the image is  \nNothing time_id  \nMoving time_id  \nand the actual evaluation will be nice and coherent, right?  \nIt's going to be a lot of code to write, but it's worth it~.   \n\nI can't do clustering because target is a per time_id indicator: ....  \nI don't see any features other than stock_id that would make sense for clustering, so I can\u2019t.","8f949b5b":"# Homebrew Kernel\nThings to check\n- Effects of clustering\uff08stock_id\uff09\n- Effects of tau\n- Effects of KNN + tau\n\nThis kernel is the English translation of this kernel.\nIf you prefer the Japanese version, please see here.  \n[https:\/\/www.kaggle.com\/satoshimts\/tau-vs-no-tau-vs-knn-tau]","f4bb88f0":"# Parameters used\n\nThis is a very detailed part that should be tuned every time if possible, but I'll leave it at that.\n\n\nseed = 29  \nparams = {  \n    'learning_rate': 0.1,          \n    'lambda_l1': 2,  \n    'lambda_l2': 7,  \n    'num_leaves': 800,  \n    'min_sum_hessian_in_leaf': 20,  \n    'feature_fraction': 0.8,  \n    'feature_fraction_bynode': 0.8,  \n    'bagging_fraction': 0.9,  \n    'bagging_freq': 42,  \n    'min_data_in_leaf': 700,  \n    'max_depth': 4,  \n    'seed': seed,  \n    'feature_fraction_seed': seed,  \n    'bagging_seed': seed,  \n    'drop_seed': seed,  \n    'data_random_seed': seed,  \n    'objective': 'rmse',  \n    'boosting': 'gbdt',  \n    'verbosity': -1,  \n    'n_jobs': -1,  \n}   ","b3a16cf6":"## Discussion\n\nCV : 0.21742  \nLB : 0.21906  \nNo clustering tau : CV : 0.22057    \n\nSmall, but effective?  \nLooking at feature importance, size_tau2 might be good?  \n600-450 is also a little higher, so it might be working.  \n\nProbably because the slope of the last 600 seconds is related to the volatility of the next 600 seconds.\n","34b2316a":"## KNN + TAU","b18ca3d8":"# Baseline\n","5230d83e":"# About TAU\n\ntau1 : train['size_tau'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique'] )\nAs for feature_importance, it is a simple statistical measure and has no effect on the LGBM model.  \nThe feature_importance does not work at all.  \n\n\ntau2 : \ntrain['size_tau2'] = np.sqrt( 1\/ train['trade_order_count_sum'] )  \ntrain['size_tau2_150'] = np.sqrt( 0.75\/ train['trade_order_count_sum'] )  \nAs for the \"size_tau2_sum\", I think it will change because it is an indicator per unit of time.  \nThe value of the total number of trades after 150 seconds is converted to 600 seconds, so it should be the estimated total number of trades.  \nIf this is different from the actual total number of trades, it should be an indicator that the volatility has moved too much and is different from the estimate.  \n\nWhy do we need to care about the number of trades...?  \nI think you can just use log_return or something...\n\n## Without clustering indicator feature","18004022":"# About the clustering index\n\nIn this clustering, time_id is used as an instance, and the values of the objective variable for each stock_id are correlated with each stock_id.  \nIn other words, stock_ids with similar correlation values mean that the distributions of the objective variables are similar.  \nThe idea that the distributions are similar means that new features can be obtained by taking the average of the indicators written for each cluster and substituting the indicators of each cluster for each time_id as features.  \n\nNo tau feature in tihs model","518f2aa0":"## Discussion\nCV : 0.22507  \nNo tau : 0.2253  \n\nNo effect ....  \nThere is no effect at all when viewed with feture_importance. "}}