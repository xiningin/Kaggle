{"cell_type":{"d1ef77a2":"code","bb46ce57":"code","3a8180d7":"code","4caeace4":"code","c0ec5dce":"code","ca739839":"code","cfdb9786":"code","83024236":"code","483288c8":"code","4c32717a":"code","9222fcd4":"code","df7da315":"code","54d03018":"code","896e4e2f":"code","1b3009cb":"code","987c46bc":"code","d42c47c5":"code","19fa9851":"code","1b7adb4e":"code","835cf0a4":"code","685f1110":"code","b7c66ae7":"code","b1c73599":"code","c981ade2":"code","a579d514":"code","d07296db":"code","8df3dc89":"code","1bbf6663":"code","56175bd5":"code","d7dead46":"code","3d62f1ac":"code","8a910f4c":"code","6b55df9a":"code","47c1430a":"code","3ddcd6cb":"code","fe3f25fc":"code","535cf316":"code","e59591e4":"code","60bede9f":"code","10e51dce":"code","ba207412":"code","569c830e":"code","8d6e33fe":"code","6a10750c":"code","83ab5484":"code","1ffa3cc8":"code","29aa6d57":"code","cb064210":"code","474274cb":"code","a722ae3a":"code","1f677615":"code","68320824":"code","8a95d15b":"code","a108dc68":"code","e0a6b112":"code","65ce74d4":"code","2d716320":"code","e7978295":"code","6b4af60a":"code","b2392272":"code","19f1d520":"code","23d0de02":"code","e4a922ca":"code","21e9733a":"code","5a9ea942":"code","174fc1cc":"code","9ac777fd":"code","229cbeaf":"code","d8cc3c72":"code","dda636c6":"code","b1998e51":"code","dcceccb7":"code","e57ad4ad":"code","1b2579bb":"code","b4b5bb14":"code","03c345bc":"code","ae9a07cd":"code","2e236e9b":"code","f06c18f5":"code","792a2a61":"code","b41c54ac":"code","b322e8e7":"code","9f74ceab":"code","5a3b6fc3":"code","f37823b1":"code","029dce3e":"code","9c9d93dc":"code","c806fe12":"code","cbf44348":"code","09d1f3ed":"code","4f4a2706":"code","2452146d":"code","02be73ef":"code","827028ac":"code","8f152910":"code","8922b7de":"code","e766d12b":"code","ac2c16c1":"code","e90a5480":"code","67eee980":"code","866afad7":"code","689869f2":"code","5d4d4a5d":"code","2f035d61":"code","02fefd6f":"code","cfe27d88":"code","a1f3b879":"code","66906476":"code","db02f067":"code","6c3c29c1":"code","f9d66773":"code","536ce3cd":"code","55a7fc9e":"code","b8dccbf5":"code","534bf2aa":"code","c022ce41":"code","427b2f56":"code","aaa525d3":"code","d2d28522":"code","24784ae0":"code","d685fd0f":"code","e45e7f09":"code","50a4d37d":"code","9178553a":"code","a80606c4":"code","320cf9ee":"code","c34d2dd6":"code","5ce894bf":"code","3d224461":"code","9b2cf5b0":"code","a361ce07":"code","05be8a84":"code","ce7a0dc8":"code","c95b73c6":"code","a289c507":"code","9ec4f622":"code","3044639e":"code","d80748ad":"code","515ac620":"code","92b15133":"code","6fba51ec":"code","c3894dc7":"code","0384f2a3":"code","4f68e6ba":"code","1cd2a1fb":"code","2799121a":"code","d8e4e0cf":"code","5c7cb5b9":"code","45f0161b":"code","7b410c64":"code","055a91fb":"code","f529548b":"code","9cc00ee6":"code","049fdbe3":"code","9bc06997":"code","f32332e3":"code","6463d51d":"code","1e0097cb":"code","8faf464e":"markdown","c22edd5d":"markdown","335d3921":"markdown","d2453858":"markdown","9a54938b":"markdown","f447b71e":"markdown","5e2a6e8a":"markdown","04c4a219":"markdown","2cc850ef":"markdown","27afcfda":"markdown","b4cca3b8":"markdown","e0f3e047":"markdown","97221e59":"markdown","1f5e0ec5":"markdown","f4bd98f0":"markdown","dca5e39a":"markdown","b1cb4843":"markdown","129e47fe":"markdown","d99ad532":"markdown","d9b099f2":"markdown","9a2ad5ac":"markdown","ba971c2a":"markdown","7017c2c9":"markdown","9708e928":"markdown","81a4f2f5":"markdown","e51a0dd6":"markdown","4b59e41f":"markdown","2528f305":"markdown","8acb9fcd":"markdown","f1fce223":"markdown","bfd3a4c4":"markdown","829197bc":"markdown","75c4c0e8":"markdown","b9fee208":"markdown","bf992bf2":"markdown","95ad50ca":"markdown","efeca9b7":"markdown","4864ebe9":"markdown","fabbcb8c":"markdown","882eb539":"markdown","7f7304fd":"markdown","2e50a296":"markdown","c61a490f":"markdown","2604849c":"markdown","83495994":"markdown","62ec2d36":"markdown","ed625086":"markdown","6162801c":"markdown","0faf8627":"markdown","b1f84212":"markdown","8b009ac1":"markdown","5b834f08":"markdown","285627f1":"markdown","f23c890c":"markdown","fa8ae538":"markdown","6009316e":"markdown","8e8590f7":"markdown","0eb53458":"markdown","18e9135e":"markdown","4ba76855":"markdown","941d2827":"markdown","9a421606":"markdown","95971517":"markdown","165a8aed":"markdown","e4fc9ec8":"markdown","b03be686":"markdown","407bb3e6":"markdown","e8b9ab3c":"markdown","6f210af0":"markdown","12ebf350":"markdown","7c5d3401":"markdown","ff3779a0":"markdown","e7277b7c":"markdown","0d9e66db":"markdown","17ecea9f":"markdown","80aaef2e":"markdown","ae99299d":"markdown","1c4d3a2a":"markdown","bb56b8ea":"markdown","473ba270":"markdown","949effab":"markdown","5a34866c":"markdown","3be6eead":"markdown","3fd001d5":"markdown","4683cd31":"markdown","58057c74":"markdown","1f9a1a0a":"markdown"},"source":{"d1ef77a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bb46ce57":"import pandas as pd\nimport numpy as np\n\n# For Visualization\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","3a8180d7":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ngen_sub = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","4caeace4":"train.head()","c0ec5dce":"# to get the size of datset i.e. rows and columns in training dataset\ntrain.shape","ca739839":"test.head()","cfdb9786":"test.shape","83024236":"gen_sub.head()","483288c8":"gen_sub.shape","4c32717a":"train.info()","9222fcd4":"train.columns","df7da315":"test.info()","54d03018":"test.columns","896e4e2f":"train.describe()","1b3009cb":"# For training Dataset and for test dataset\n\ntrain.columns = [x.lower() for x in train.columns]\ntest.columns = [x.lower() for x in test.columns]","987c46bc":"train.head(2)","d42c47c5":"test.head()","19fa9851":"train.drop(['passengerid','name','ticket'],axis=1,inplace=True)\ntrain.head(2)","1b7adb4e":"train.isnull().sum()","835cf0a4":"train.drop('cabin',axis=1,inplace=True)\ntrain.head()","685f1110":"train.isnull().sum()","b7c66ae7":"# this syntax will compute the mean value, again there are many ways to compute the mean thsi is one of them\nage_mean = train['age'].mean()\n\n\n# now we have to fill that in place of NaN value:\n\ntrain['age'] = train['age'].fillna(age_mean)\n\n# Check for missing value:\ntrain.isnull().sum()\n","b1c73599":"train.isnull().sum()","c981ade2":"train['embarked'].value_counts()","a579d514":"train[train['embarked'].isnull()]","d07296db":"# Embarked column has two missing values - we can either drop the two rows or impute them with the highest occurring value\n# we can dropping them like this \n\n\n#  titanic.dropna(inplace = True)\n\n# But, inted we will fill those with Most Occuring for same sex people","8df3dc89":"sns.countplot(data=train,x='embarked',hue='sex')","1bbf6663":"train['embarked'] = train['embarked'].fillna('S')","56175bd5":"train.isnull().sum()","d7dead46":"train.head(2)","3d62f1ac":"# For The Survived Columns\n\ntrain['survived'].value_counts()","8a910f4c":"sns.countplot(data=train,x='survived')","6b55df9a":"train[['pclass','survived']].groupby(['pclass'],as_index=False).mean().sort_values(by='survived',ascending=False)","47c1430a":"sns.countplot(data=train,x='pclass',hue='survived')","3ddcd6cb":"train[['sex','survived']].groupby(['sex'],as_index=False).mean().sort_values(by='survived',ascending=False)","fe3f25fc":"sns.countplot(data=train,x='sex',hue='survived')","535cf316":"sns.scatterplot(x='pclass', y='fare', data=train, hue=\"survived\")","e59591e4":"plt.figure(figsize = (10,10))\nsns.boxplot(x='pclass', y='fare', data=train, hue=\"survived\")","60bede9f":"plt.figure(figsize = (8,10))\n#continuous variable\nsns.distplot(train['age'])","10e51dce":"train.head(2)","ba207412":"train.sibsp.value_counts()","569c830e":"train[[\"sibsp\", \"survived\"]].groupby(['sibsp'], as_index=False).mean().sort_values(by='survived', ascending=False)","8d6e33fe":"plt.figure(figsize = (8,10))\nsns.countplot(data=train,x='sibsp',hue='survived')","6a10750c":"plt.figure(figsize=(10,10))\nsns.countplot(data=train,x='sibsp',hue='pclass')","83ab5484":"train.parch.value_counts()","1ffa3cc8":"train[[\"parch\", \"survived\"]].groupby(['parch'], as_index=False).mean().sort_values(by='survived', ascending=False)","29aa6d57":"plt.figure(figsize=(20,8))\nsns.countplot(data=train,x='parch',hue='survived')","cb064210":"plt.figure(figsize = (12,8))\nsns.boxplot(x = 'survived',y = 'fare',data = train)","474274cb":"sns.countplot(data=train,x='embarked',hue='survived')","a722ae3a":"plt.figure(figsize = (10,8))\nsns.boxplot(x='embarked', y='fare', data=train, hue=\"survived\")","1f677615":"# Now Pairplot for Getting Overall Picture\n\n# sns.pairplot(train,hue='survived')","68320824":"train.head()","8a95d15b":"sex = pd.get_dummies(train['sex'],drop_first=True)\nsex.head()","a108dc68":"pclass = pd.get_dummies(train['pclass'],drop_first= True)\npclass.head()","e0a6b112":"embarked = pd.get_dummies(train['embarked'],drop_first= True)\nembarked.head()","65ce74d4":"final_train = pd.concat([train,sex,pclass,embarked],axis = 1)\nfinal_train.head()","2d716320":"# dropping columns given that we alreayd have dummy vars for them\nfinal_train = final_train.drop(['sex','embarked','pclass'],axis = 1)","e7978295":"final_train.head()","6b4af60a":"final_train.columns = ['survived','age','sibsp','parch','fare','sex_male','pclass_2','pclass_3','embarked_q','embarked_s']\nfinal_train.head()","b2392272":"plt.figure(figsize=(10,10))\nsns.heatmap(final_train.corr(),annot=True)","19f1d520":"test.head()","23d0de02":"test.shape","e4a922ca":"test.isnull().sum()","21e9733a":"test.drop(['passengerid','name','ticket'],axis=1,inplace=True)\n\ntest.drop('cabin',axis=1,inplace=True)\n\n\n# this syntax will compute the mean value, again there are many ways to compute the mean thsi is one of them\nage_mean = test['age'].mean()\n\n\n# now we have to fill that in place of NaN value:\n\ntest['age'] = test['age'].fillna(age_mean)\n\n# Check for missing value:\ntest.isnull().sum()\n\n","5a9ea942":"test['fare'] = test['fare'].fillna(test['fare'].mean())","174fc1cc":"test.isnull().sum()","9ac777fd":"sex = pd.get_dummies(test['sex'],drop_first=True)\npclass = pd.get_dummies(test['pclass'],drop_first= True)\nembarked = pd.get_dummies(test['embarked'],drop_first= True)","229cbeaf":"final_test = pd.concat([test,sex,pclass,embarked],axis = 1)\nfinal_test.head()","d8cc3c72":"# dropping columns given that we alreayd have dummy vars for them\nfinal_test = final_test.drop(['sex','embarked','pclass'],axis = 1)\nfinal_test.head()","dda636c6":"final_test.columns = ['age','sibsp','parch','fare','sex_male','pclass_2','pclass_3','embarked_q','embarked_s']\nfinal_test.head()","b1998e51":"X_train = final_train.drop('survived',axis=1)\ny_train = final_train['survived']","dcceccb7":"X_test = final_test","e57ad4ad":"X_train.shape,X_test.shape,","1b2579bb":"X_train.columns","b4b5bb14":"X_test.columns","03c345bc":"X_train.head(2)","ae9a07cd":"from sklearn.preprocessing import StandardScaler","2e236e9b":"help(StandardScaler)","f06c18f5":"# making an instance of Standardscaller\n\nscaler = StandardScaler()","792a2a61":"scaled_X_train = scaler.fit_transform(X_train)\nscaled_X_test = scaler.transform(X_test)\n\n","b41c54ac":"y_test = gen_sub['Survived']","b322e8e7":"from sklearn.linear_model import LogisticRegression","9f74ceab":"from sklearn.model_selection import GridSearchCV","5a3b6fc3":"help(LogisticRegression)","f37823b1":"# Depending on warnings you may need to adjust max iterations allowed \n# Or experiment with different solvers\nlog_model = LogisticRegression(solver='saga',multi_class=\"ovr\",max_iter=5000)","029dce3e":"# Penalty Type\npenalty = ['l1', 'l2']\n\n# Use logarithmically spaced C values (recommended in official docs)\nC = np.logspace(0, 4, 10)","9c9d93dc":"grid_model = GridSearchCV(log_model,param_grid={'C':C,'penalty':penalty})","c806fe12":"grid_model.fit(scaled_X_train,y_train)","cbf44348":"grid_model.best_params_","09d1f3ed":"from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,plot_confusion_matrix","4f4a2706":"y_lr_pred = grid_model.predict(scaled_X_test)","2452146d":"accuracy_score(y_test,y_lr_pred)","02be73ef":"# Confusion metrics\n\nconfusion_matrix(y_test,y_lr_pred)","827028ac":"# Plotting Confusion metrics\n\nplot_confusion_matrix(grid_model,scaled_X_test,y_test)","8f152910":"# Printing Classification report for Precision,and recall\n\nprint(classification_report(y_test,y_lr_pred))","8922b7de":"from sklearn.metrics import roc_curve, auc","e766d12b":"from sklearn.metrics import precision_recall_curve,plot_precision_recall_curve,plot_roc_curve","ac2c16c1":"plot_precision_recall_curve(grid_model,scaled_X_test,y_test)","e90a5480":"plot_roc_curve(grid_model,scaled_X_test,y_test)","67eee980":"from sklearn.neighbors import KNeighborsClassifier","866afad7":"# Creating an instance of model\n\n# first we are simply choosing number of neighbors =1\n\n\nknn_model = KNeighborsClassifier(n_neighbors=1)","689869f2":"knn_model.fit(scaled_X_train,y_train)","5d4d4a5d":"y_knn_pred = knn_model.predict(scaled_X_test)","2f035d61":"print(accuracy_score(y_test,y_knn_pred))\nprint('\\n')\nplot_confusion_matrix(knn_model,scaled_X_test,y_test)","02fefd6f":"print(classification_report(y_test,y_knn_pred))","cfe27d88":"help(GridSearchCV)","a1f3b879":"knn = KNeighborsClassifier()","66906476":"# Defining k values in range:\nk_values = list(range(1,20))\n\nparams_grid = {'n_neighbors':k_values}","db02f067":"knn_grid_model = GridSearchCV(knn,param_grid=params_grid,cv=5,scoring='accuracy')","6c3c29c1":"knn_grid_model.fit(scaled_X_train,y_train)","f9d66773":"knn_grid_model.best_params_","536ce3cd":"y_knn_grid_pred = knn_grid_model.predict(scaled_X_test)","55a7fc9e":"print(accuracy_score(y_test,y_knn_grid_pred))\nprint('\\n')\nplot_confusion_matrix(knn_grid_model,scaled_X_test,y_test)","b8dccbf5":"print(classification_report(y_test,y_knn_grid_pred))","534bf2aa":"from sklearn.svm import SVC","c022ce41":"help(SVC)","427b2f56":"# creating an Instance of the Model\n\n# Using default Parameters of model and check the accuracy:\n\nsvc = SVC()\n\n\nsvc.fit(scaled_X_train,y_train)\ny_svc_pred = svc.predict(scaled_X_test)\n\n\n# Model Evaluation\n\nprint(accuracy_score(y_test,y_svc_pred))\nprint('\\n')\nplot_confusion_matrix(svc,scaled_X_test,y_test)","aaa525d3":"# Setting up Hyper-Parameters Grid\n\nparam_grid = {'C':[0.001,0.01,0.1,0.5,1],'gamma':['scale','auto']}\n\n\n# creating model instance\ngrid_svc = GridSearchCV(svc,param_grid)","d2d28522":"grid_svc.fit(scaled_X_train,y_train)","24784ae0":"grid_svc.best_params_","d685fd0f":"y_svc_grid_pred = grid_svc.predict(scaled_X_test)","e45e7f09":"# Model Evaluation\n\nprint(accuracy_score(y_test,y_svc_grid_pred))\nprint('\\n')\nplot_confusion_matrix(grid_svc,scaled_X_test,y_test)","50a4d37d":"print(classification_report(y_test,y_svc_grid_pred))","9178553a":"from sklearn.tree import DecisionTreeClassifier","a80606c4":"# Creating Instance of model\n\ndt_model = DecisionTreeClassifier()","320cf9ee":"dt_model.fit(X_train,y_train)","c34d2dd6":"y_dt_pred = dt_model.predict(X_test)","5ce894bf":"# Model Evaluation\n\nprint(accuracy_score(y_test,y_dt_pred))\nprint('\\n')\nplot_confusion_matrix(dt_model,X_test,y_test)","3d224461":"dt_model.feature_importances_","9b2cf5b0":"# Important Features\n\npd.DataFrame(index=X_train.columns,data=dt_model.feature_importances_,columns=['Feature Importance'])\n","a361ce07":"from sklearn.tree import plot_tree","05be8a84":"plt.figure(figsize=(12,8))\nplot_tree(dt_model);","ce7a0dc8":"plt.figure(figsize=(20,12),dpi=150)\nplot_tree(dt_model,filled=True,feature_names=X_train.columns);","c95b73c6":"help(DecisionTreeClassifier)","a289c507":"help(GridSearchCV)","9ec4f622":"# Defining Paramgrid\n\nmax_depth = list(range(1,30))\n\nparams_grid = {'max_depth':max_depth,'max_leaf_nodes':[2,3,4,5,6,7,8]}\n\ndt_grid_model = GridSearchCV(dt_model,param_grid=params_grid)\n\n","3044639e":"dt_grid_model.fit(X_train,y_train)\n\ny_dt_grid_pred = dt_grid_model.predict(X_test)","d80748ad":"dt_grid_model.best_params_","515ac620":"# Model Evaluation\n\nprint(accuracy_score(y_test,y_dt_grid_pred))\nprint('\\n')\nplot_confusion_matrix(dt_grid_model,X_test,y_test)","92b15133":"from sklearn.ensemble import RandomForestClassifier","6fba51ec":"help(RandomForestClassifier)","c3894dc7":"# Creating an Instance of RF Classifier\n\nrf_model = RandomForestClassifier()","0384f2a3":"rf_model.fit(X_train,y_train)","4f68e6ba":"y_rf_pred = rf_model.predict(X_test)","1cd2a1fb":"# Model Evaluation\n\nprint(accuracy_score(y_test,y_rf_pred))\nprint('\\n')\nplot_confusion_matrix(rf_model,X_test,y_test)","2799121a":"rf_model.feature_importances_","d8e4e0cf":"# Important Features\n\npd.DataFrame(index=X_train.columns,data=rf_model.feature_importances_,columns=['Feature Importance'])\n","5c7cb5b9":"help(RandomForestClassifier)","45f0161b":"# setting up Param_grid\n\nno_of_trees = list(range(10,50,500))\n\nmax_depth = list(range(1,30))\n\n\nparam_grid_rf = {'n_estimators':no_of_trees,'criterion':['gini','entropy'],\n                 'max_depth':max_depth,'max_leaf_nodes':[2,3,4,5,6,7,8]}","7b410c64":"# fitting the model\n\nrf_grid_model = GridSearchCV(rf_model,param_grid=param_grid_rf)","055a91fb":"rf_grid_model.fit(X_train,y_train)\n\ny_rf_grid_pred = rf_grid_model.predict(X_test)","f529548b":"rf_grid_model.best_params_","9cc00ee6":"# Model Evaluation\n\nprint(accuracy_score(y_test,y_rf_grid_pred))\nprint('\\n')\nplot_confusion_matrix(rf_grid_model,X_test,y_test)","049fdbe3":"print('Logistic_Regression_best_accuracy: ',accuracy_score(y_test,y_lr_pred))\nprint('\\n')\nprint('KNN_best_accuracy: ',accuracy_score(y_test,y_knn_grid_pred))\nprint('\\n')\nprint('SVM_best_accuracy: ',accuracy_score(y_test,y_svc_grid_pred))\nprint('\\n')\nprint('Decision_tree_best_accuracy: ',accuracy_score(y_test,y_dt_grid_pred))\nprint('\\n')\nprint('Random_forest_best_accuracy: ',accuracy_score(y_test,y_rf_grid_pred))","9bc06997":"test_pred = dt_grid_model.predict(X_test)","f32332e3":"test2 = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest2.head()","6463d51d":"passengerid = test2['PassengerId']","1e0097cb":"submission = pd.DataFrame({\"PassengerId\": passengerid,\"Survived\": test_pred})\nsubmission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)\nsubmission.to_csv(\"titanic1_submission.csv\", index=False)","8faf464e":"# 1. Logistic Regression:\n","c22edd5d":"It's very much clear form the data that cabin column has most missing value it has 687 missing value from 891 value that is nearly 77% of the data is missing, so this kind of data won't be able to add any good thing in aur analysis so it would be better to drop that kind of column:\n\n**dropping cabin column**","335d3921":"## Model Evaluation","d2453858":"We can Visualize Based on Pclass that-- For every passenger class how many people survived:","9a54938b":"In pclass 3 very less number of passengers Survived","f447b71e":"## Full Cross Validation Grid Search for K Value","5e2a6e8a":"# Ealuating Model performance:-\n\n## Classification Models are evaluated based on these parameters\n\n\n\n - confusion_matrix\n - ROC curve\n - AUC\n - F1 score\n - Precision\n - Recall\n - accuracy_score etc.\n ","04c4a219":"## About data\n\nNow we have a fare idea about the shape and size of the data and we got the complete picture that we have to predict that **weather a passenger survived or not** and the passenger is recognized by the passenger ID.","2cc850ef":"# 5. Random Forest Classifier\n\n\nThis is the Most used Machine Learning Algorithm in any Machine Learning task.\n\nBasically it is based on Ensemble learning -- **Bagging**","27afcfda":"# We will be using cross-validation for better performance and hyper parameters turning GridSearchCV","b4cca3b8":"# Comparing The accuracy of all the Models useds till Now:","e0f3e047":"Tree-Model with hyper-Parameter tuning using Grid-Search","97221e59":"Looks like default parameters were the best parameters for this.","1f5e0ec5":"## Logistic-Regression with Cross-validation and GridSearchCV for hyperparameter tuning:\n\n","f4bd98f0":"# 3. Support Vector Machines\n\n\n## SVM - Classification\n","dca5e39a":"## Evaluating Curves and AUC\n\nSource: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html\n","b1cb4843":"## It's very Important to use - Scaled value in KNN Models","129e47fe":"### Coefficient Interpretation\n\nThings to remember:\n\n* These coeffecients relate to the *odds* and can not be directly interpreted as in linear regression.\n* We trained on a *scaled* version of the data \n* It is much easier to understand and interpret the relationship between the coefficients than it is to interpret the coefficients relationship with the probability of the target\/label class.","d99ad532":"Importing Libraries and importing dataset","d9b099f2":"## Model Evaluation:","9a2ad5ac":"## At first now, we are dealing only with training data:","ba971c2a":"### First of all we will Make all column names lowercase so that we dont have to worry about cases in both train and test dataset","7017c2c9":"## Type of data in dataset:\n\n### 1- Continuous:\n\nAll the columns heaving numerical values are the continuous data columns ex: int64,float64 Dtype columns\n\n### 2- Catagorical:\n\nAll the columns heaving catagories are catagorical columns ex: object Dtype in columns","9708e928":"Now, we can see that we have removed all the Null values form our Dataset.","81a4f2f5":"# EDA on Train and Test data\n\nWe will make changes in test data also where ever needed so that we can stay on same track for both tha data","e51a0dd6":"Information of Training dataset","4b59e41f":"# Model Performance on Classification Tasks","2528f305":"# Tree Based Methods:","8acb9fcd":"Explore the Embarked Column","f1fce223":"Description is shown of all the continous columns and it will be very easy to read if we read it as traspose of it so,\n","bfd3a4c4":"Scalling the data means for any numerical column in our datset making the value between maan 0 and standard daviation 1.","829197bc":"Model Evaluation","75c4c0e8":"## Visualize the Tree\n\nThis function is fairly new, you may want to review the online docs:\n\nOnline Documentation: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.plot_tree.html","b9fee208":"# RF with GridSearchCV for Hyper Parameters Tuning","bf992bf2":"WE will be using 'Pandas' library to explore the data","95ad50ca":"pclass = 1 has the highest fare and survival rates while pclass = 3 is on the other end which possibly indicates that the rooms for this class were somewhere in the middle and not easily accessible.","efeca9b7":"### Model Evaluation","4864ebe9":"## How to choose best value of k:\n\n\nThere are plenty of ways to do that\n- 1. 'Elbow Method' - Run a for loop in range of K values and plot them to choose best optimam value of k Visualy\n- 2. GridSearch to let the Computer find the optimum value of k for us.\n\nHere i am choosing the Second Method:","fabbcb8c":"# 4. Decision-Trees Algorithm","882eb539":"Looks like people travelling alone have lesser chances of survival and passengers of pclass = 3 are travelling alone.","7f7304fd":"### GridSearch for Best Hyper-Parameters\n\nMain parameter choices are regularization penalty choice and regularization C value.","2e50a296":"### Now, plot the correlation of dataset so that we can get that which columns are heighely co-releated","c61a490f":"### Now,check for Null values in the training dataset:\n","2604849c":"## Decision Tree Classifier\n\n## Default Hyperparameters","83495994":"# Preparing Data for Training and Testing","62ec2d36":"so, we are left with 2 null value in embarked column:\nLet's explore Embarked Column","ed625086":"# Description of data","6162801c":"Now, renaming all the columns for our better understanding","0faf8627":"# Importing Dataset","b1f84212":"## Creating final dataframe by concatinating all other columns together to get 1 dataframe","8b009ac1":"Clearly shows that the mean fare for people who survived was higher than the ones who did not survive.","5b834f08":"We can observe the datatype of all the columns present in aur train dataset and if there are any null-values present in the dataset.\n\n","285627f1":"# EDA","f23c890c":"## Now, One by one Value Counts and data Exploration for train data","fa8ae538":"Columns like name,ticket and passengerid are just id-variables and would not add any value to our analysis. Let's drop them at the start.","6009316e":"# RF Classifier\n\n## Default Hyperparameters","8e8590f7":"## Model Evaluation","0eb53458":"# Till Now we have acheived best accuracy through Decision_tree","18e9135e":"Things to remember:\n\n* We trained on a *scaled* version of the data","4ba76855":"## Important: \n \n To get the best result from your logistc regression Algorithm first we have to Scale the data using Standard-Scaller","941d2827":"- what columns we have in our dataset\n\n","9a421606":"Here we can see that the embarked has two missing value ","95971517":"## Model Evaluation","165a8aed":"### What a increase in Accuracy from 76% to 97% ..by just two hyper Parameters turning","e4fc9ec8":"### here within the KNN itself we can observe that only tuning k value for the number of neighbors from 1 to 12 using GridSearch we have significantly increased the accuracy of the model from 74.40% to 87.79 %","b03be686":"## Performing Same EDA on test-data and Making both train and test data symmetrical","407bb3e6":"**Things to Remember\n\n - we are using scaled data for the SVM model","e8b9ab3c":"Here model is not performing well as per our expectation, accuracy is too low","6f210af0":"We can clearly see that for the female 'S' is the most occueing in the dataset and because the missing data columns have both the rows for female sex so i am imputing them with 'S' for now,\n\nAgain there are many more ways to deal with this kind of data one can decide as per the Problem.","12ebf350":"Number of Survival of female is more than Survival of male.","7c5d3401":"We can understand it in the way that if a persone is male it's definded by the value 1 and if has 0 as value then it's female","ff3779a0":"passengers who embarked from port C have paid higher fares and have better survival rates.","e7277b7c":"So, we still have 177 values which are not present in age column which is nearly around 19% missing data so, we have to somehow fill those missing value :\n\nthere are many ways we can fill those missing value like by mean data, modian and mode or some any random value etc.\nBut, i am considering the mean value of the age column to fill thet with:","0d9e66db":"### Data Exploration - EDA","17ecea9f":"Similarly for embarked column:\n","80aaef2e":"let's explore about fare","ae99299d":"## Model Evaluation","1c4d3a2a":"Similary in pclass we have basically 3 classes\n\nclass -1,2 and 3.\n\nso, if class 2 and 3 both are 0 that means that passenger belongs to class 1 similarly for another one too.\n","bb56b8ea":"# Data Prepration","473ba270":"# Using Machine Learning Algorithm- For Classification task\n\nbecause we have to predict that a passenger is Survived or not, so.","949effab":"Using Default parameters of Support Vector classification we are getting an accuracy of 88.75%","5a34866c":"### Using Hyper-Parameters tuning in SCV using Grid-Search CV","3be6eead":"Now, Still we have sex,pclass and embarked etc. as columns which has more than 1 catagory so,","3fd001d5":"Let's more explore about sibling column:\n","4683cd31":"**OR**\n\n\n\nWe can Saparately Put These predicted featires on Decision Tree Classification algo..and predict and can plot our decision tree.","58057c74":"**Things to Remember**\n\n- In tree Based methods we don't need to scale the data it automatically takes care of that, but if we are using sclaed data that won't affect our model performance but,\n\nAs per standard practice I won't be using scaled data, i will be using normal data which is without scaling ","1f9a1a0a":"# 2. KNN -  K Nearest Neighbors - Classification\n"}}