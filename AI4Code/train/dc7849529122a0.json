{"cell_type":{"336e3bd5":"code","7951d7a4":"code","78654b14":"code","09b0b243":"code","fa71739d":"code","83f8f82e":"code","217a948a":"code","0567e9e9":"code","d5fab609":"code","4ea695f9":"code","df52a7c3":"code","69e83f4d":"code","bd7e433d":"code","0bd02b18":"code","6c594488":"code","ded48a09":"code","b05abcd6":"markdown","3ed489f6":"markdown","21b35619":"markdown","e2c95efb":"markdown","1b8be441":"markdown","15386a70":"markdown","8d68a0d8":"markdown","32addc03":"markdown","3bb62f0b":"markdown"},"source":{"336e3bd5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7951d7a4":"# util\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n# deep learning\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\nfrom keras import regularizers\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.callbacks import EarlyStopping\n# sklearn e preprocess\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,  RobustScaler\nfrom sklearn.model_selection import GridSearchCV\n# seed\nfrom numpy.random import seed\n","78654b14":"##\n## Data Load\n##\ntrain_data = pd.read_csv('\/kaggle\/input\/desafio-worcap-2020\/treino.csv', )\ntest_data = pd.read_csv('\/kaggle\/input\/desafio-worcap-2020\/teste.csv')","09b0b243":"train_data.describe()","fa71739d":"##\n## Data Split\n##\ntrain_x = train_data.drop('id', axis=1)\ntrain_x = train_x.loc[:, train_x.columns != 'label'] \ntrain_y = train_data.loc[:,'label'].values \n#\ntest_x = test_data.drop('id', axis=1)\ntest_x = test_x.loc[:, test_x.columns != 'label'] \nprint(train_x.shape, train_y.shape)\nprint(test_x.shape, )\nnum_classes = len(set(train_data['label'].values))\nprint(num_classes,' classes')\nn_features = train_x.shape[1]\nprint(n_features,' features')","83f8f82e":"##\n## Data Categorical-->OneHot Encoder\n##\nencoder = preprocessing.OneHotEncoder(sparse=False)\n\nencoder.fit(train_y.reshape(-1, 1))\ntrain_y_onehot = encoder.transform(train_y.reshape(-1, 1))\nprint(train_y_onehot[:5])\nprint(train_y[:5])\nprint(encoder.inverse_transform(train_y_onehot[:5]))","217a948a":"##\n## Data Scale\n##\ntransformer = RobustScaler([0,1]).fit(train_x)\n#transformer = MinMaxScaler([0,1]).fit(train_x)\n#\ntrain_x_scaled = transformer.transform(train_x)\ntest_x_scaled = transformer.transform(test_x)","0567e9e9":"##\n## Data Reshape to Conv1D Layers \n##\nprint('Original Train Shape:', train_x_scaled.shape)\nprint('Original Test Shape:', test_x_scaled.shape)\n# Shape to Conv1D\ntrain_x_conv1d = train_x_scaled.reshape(train_x_scaled.shape[0],train_x_scaled.shape[1], 1)\ntest_x_conv1d = test_x_scaled.reshape(test_x_scaled.shape[0],test_x_scaled.shape[1], 1)\nprint('#######')\nprint('Train shape to Conv1D', train_x_conv1d.shape)\nprint('Test shape to Conv1D', test_x_conv1d.shape)","d5fab609":"##\n## Best Model (189 epochs)\n## \ntf.random.set_seed(7)\nseed(7)\n#\nconv_model = models.Sequential([\n    layers.Conv1D(32, 5, activation='relu', input_shape=(n_features,1)),\n    layers.MaxPooling1D(pool_size=2, strides=None, padding=\"valid\",),\n    layers.Conv1D(16, 3, activation='relu', input_shape=(n_features,1)),\n    layers.MaxPooling1D(pool_size=2, strides=None, padding=\"valid\",),\n    layers.Flatten(),\n    layers.Dense(16, activation='relu', ),\n    layers.Dense(num_classes, activation='softmax',),\n]) \nlr = 0.001\nconv_model.compile(optimizer=keras.optimizers.Adam(lr),\n              loss=keras.losses.categorical_crossentropy,\n              metrics=['accuracy'])\nconv_model.summary()","4ea695f9":"##\n## Best Model Train \n## \n# Callback \nes = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=100,\n                          restore_best_weights=False)\n# Train with Validation  (20% validation)\nhistory = conv_model.fit(train_x_conv1d, train_y_onehot, epochs=400, verbose=0, \n                         batch_size=len(train_x), callbacks=[es], validation_split=0.2)\n# Results\nacc = '{:.4}'.format(100*history.history['accuracy'][-1])\nval_acc = '{:.4}'.format(100*history.history['val_accuracy'][-1])\nprint('Train Accuracy:', acc)\nprint('Validation Accuracy:', val_acc)\n# Plot Acc Curve\nplt.figure(figsize=(12,4))\nplt.plot(history.history['accuracy'], label='Train Acc '+acc)\nplt.plot(history.history['val_accuracy'], label='Validation Acc '+val_acc)\nplt.legend()\nplt.show()\n# Plot loss\nplt.figure(figsize=(12,4))\nplt.plot(history.history['loss'], 'r', label='Loss')\nplt.legend()\nplt.show()","df52a7c3":"##\n## Best Model \n## \ntf.random.set_seed(7)\nseed(7)\n#\nfinal_conv_model = models.Sequential([\n    layers.Conv1D(32, 5, activation='relu', input_shape=(n_features,1)),\n    layers.MaxPooling1D(pool_size=2, strides=None, padding=\"valid\",),\n    layers.Conv1D(16, 3, activation='relu', input_shape=(n_features,1)),\n    layers.MaxPooling1D(pool_size=2, strides=None, padding=\"valid\",),\n    layers.Flatten(),\n    layers.Dense(16, activation='relu', ),\n    layers.Dense(num_classes, activation='softmax',),\n]) \nlr = 0.001\nfinal_conv_model.compile(optimizer=keras.optimizers.Adam(lr),\n              loss=keras.losses.categorical_crossentropy,\n              metrics=['accuracy'])","69e83f4d":"##\n## Best Model Train \n## \nbest_epoch_num = 256 # may vary\n# Train without Validation and Callbacks\nhistory = final_conv_model.fit(train_x_conv1d, train_y_onehot, epochs=best_epoch_num, \n                               verbose=0, batch_size=len(train_x),)\n# Results\nacc = '{:.4}'.format(100*history.history['accuracy'][-1])\nprint('Train Accuracy:', acc)\n# Plot Acc Curve\nplt.figure(figsize=(12,4))\nplt.plot(history.history['accuracy'], label='Train Acc '+acc)\nplt.legend()\nplt.show()\n# Plot loss\nplt.figure(figsize=(12,4))\nplt.plot(history.history['loss'], 'r', label='Loss')\nplt.legend()\nplt.show()","bd7e433d":"##\n## Best Model Predict \n## \ny_pred = final_conv_model.predict(test_x_conv1d)\ny_pred = encoder.inverse_transform(y_pred).reshape(-1)\n#y_pred = [x.strip(' ') for x in y_pred]\nprint(y_pred[:10])\noutput = pd.DataFrame({'id': test_data.id,'label': y_pred})\noutput.to_csv('result.csv', index=False)\npd.read_csv('result.csv', squeeze=True)","0bd02b18":"##\n## Make Model ( 2 conv1d + 2 maxpooling + 1 dense + output dense layer)\n## \ndef create_model_from_params(params={'conv1_filters':32,\n                                             'conv2_filters':16,\n                                             'conv1_kernel':5,\n                                             'conv2_kernel':3,\n                                             'dense_neur':16,\n                                             'maxp1':2,\n                                             'maxp2':2}):\n    tf.random.set_seed(7)\n    seed(7)\n    # rede conv\n    m = models.Sequential([\n        layers.Conv1D(params['conv1_filters'], params['conv1_kernel'], activation='relu', input_shape=(n_features,1)),\n        layers.MaxPooling1D(pool_size=params['maxp1'], strides=None, padding=\"valid\",),\n        layers.Conv1D(params['conv2_filters'], params['conv2_kernel'], activation='relu', input_shape=(n_features,1)),\n        layers.MaxPooling1D(pool_size=params['maxp2'], strides=None, padding=\"valid\",),\n        layers.Flatten(),\n        layers.Dense(params['dense_neur'], activation='relu', ),\n        layers.Dense(num_classes, activation='softmax',),\n    ]) #\n    lr = 0.001\n    m.compile(optimizer=keras.optimizers.Adam(lr),\n                  loss=keras.losses.categorical_crossentropy,\n                  metrics=['accuracy'])\n    return m","6c594488":"##\n## Grid Search Params\n## \nall_paramns = []\nconv1_filters = reversed([64,32,16,8,4])\nconv2_filters = reversed([64,32,16,8,4])\nconv1_kernel = [5,3]\nconv2_kernel = [5,3]\ndense_neur = [64,32,16,4]\nmaxp1 = [4,2]\nmaxp2 = [4,2]\nfor a in conv1_filters:\n    for b in conv2_filters:\n        for c in conv1_kernel:\n            for d in conv2_kernel:\n                for e in dense_neur:\n                    for f in maxp1:\n                        for g in maxp2:\n                            all_paramns.append({'conv1_filters':a,'conv2_filters':b,'conv1_kernel':c,\n                                                     'conv2_kernel':d,'dense_neur':e,'maxp1':f,'maxp2':g})\n\nprint('Search in', len(all_paramns),' paramns')\nprint(all_paramns[0])","ded48a09":"##\n## Grid Search\n## \nfinal_results = []\nlimit_s = 6\nfor param in all_paramns[:limit_s]:\n    try:\n        model = create_model_from_params(param)\n        print('----------------------------')\n        print(param)\n        es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=100,\n                          restore_best_weights=False)\n        historia = model.fit(train_x_conv1d, train_y_onehot, \n                              epochs=200, verbose=0, batch_size=len(train_x), validation_split=0.2,\n                             callbacks=[es])\n        #\n        \n        acc = '{:.4}'.format(100*historia.history['accuracy'][-1])\n        acc_val = '{:.4}'.format(100*historia.history['val_accuracy'][-1])\n        print('Train acc:',acc)\n        print('Val acc:',acc_val)\n        final_results.append({'paramns':param_atual, 'acc':acc,'acc_val':acc_val})\n        print('----------------------------')\n    except:\n        pass\n        #print('Topology Error')\n    ","b05abcd6":"## Imports ","3ed489f6":"## Final Predictions (to CSV)","21b35619":"### Best Model Topology Experiments (Acc 92%)","e2c95efb":"## Grid Search (Trying Acc >> 92%)","1b8be441":"## Conv 1D Architecture\n> Initially I tried a dense model (only regularized Dense layers) and achieved an **accuracy of 88%**. After revisiting the problem description, I realized that maybe there would be **spatial dependence on the features**, so I tried to use **Convolucinal layers to do feature engineering** and then use a single **dense layer to perform the final classification**","15386a70":"# Important Note\n> During the competition the model with best accuracy was created **without initially defining a random seed**, the **results may therefore vary depending on the seed**, however the topology of maximum accuracy is the one listed below.","8d68a0d8":"## Data Load","32addc03":"## Best Model Topology Final Predictions (Acc 92%)","3bb62f0b":"## Important Improvement!!!\n> Using the **entire training data as batch** (batch_size = len(train_x)) improved the convergence of the model (smoothed the loss curve) and allowed the use of a more robust model **without much overfitting**)"}}