{"cell_type":{"fb7bf456":"code","38d63185":"code","20705306":"code","78f882df":"code","2bfff352":"code","cb82b80f":"code","2c7b4903":"code","9df40c1b":"code","634b783c":"code","9f1e546d":"code","f56bd693":"code","65ee0231":"code","ead052ee":"code","cfa5a342":"code","91e9b0a9":"code","8f5ba4b7":"code","cc6fa81b":"code","9961419d":"code","8517969c":"code","f8cecb10":"code","960ce94b":"code","c7b60fb7":"code","8d039b2a":"code","99e2a539":"code","e5a558d6":"code","39c64385":"code","19a07774":"code","82973efc":"code","48fd33a1":"code","bd3f0467":"code","7c1b8c3d":"code","885af528":"code","64bb659e":"code","2102d652":"code","26250019":"code","8d0e4bf3":"code","927f7fa6":"code","f144e6e3":"code","55c5f81e":"code","30efeac3":"code","94cb5b16":"code","42d1db1d":"code","266ae290":"code","f9257d4e":"code","cf933fca":"code","f0b79920":"code","a0e9341c":"markdown","ebdd0627":"markdown","f9edfa3a":"markdown","8447fc42":"markdown","f42207db":"markdown","679f121f":"markdown","adf1e48a":"markdown","09060b7e":"markdown","0aa6bb9d":"markdown","ac01a921":"markdown","3a80d6a5":"markdown","7980d28f":"markdown","369b683c":"markdown"},"source":{"fb7bf456":"import numpy as np \nimport pandas as pd\n#import matplotlib.pyplot as plt\n#import seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","38d63185":"train=pd.read_csv(\"\/kaggle\/input\/train.csv\")\ntrain.head()","20705306":"#remove outliers\ntrain=train.drop(index=[523,1298],axis=0)","78f882df":"test=pd.read_csv(\"\/kaggle\/input\/test.csv\")","2bfff352":"print('th train data has {} rows and {} features'.format(train.shape[0],train.shape[1]))\nprint('the test data has {} rows and {} features'.format(test.shape[0],test.shape[1]))","cb82b80f":"data=pd.concat([train.iloc[:,:-1],test],axis=0)\nprint('tha data has {} rows and {} features'.format(data.shape[0],data.shape[1]))","2c7b4903":"data.columns","9df40c1b":"data.info()","634b783c":"num_features=data.select_dtypes(include=['int64','float64'])\ncategorical_features=data.select_dtypes(include='object')","9f1e546d":"num_features.describe()","f56bd693":"categorical_features.describe()","65ee0231":"data.isnull().sum().sort_values(ascending=False)[:34]\n#print(categorical_features.isnull().sum().sort_values(ascending=False)[:23])\n#num_features.isnull().sum().sort_values(ascending=False)[:11]","ead052ee":"f = open(\"\/kaggle\/input\/data_description.txt\", \"r\")\n#print(f.read())","cfa5a342":"data = data.drop(columns=['Id','Street','PoolQC','Utilities'],axis=1)","91e9b0a9":"#data['LotFrontage'].fillna(int(data['LotFrontage'].mean()),inplace=True)\ndata['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","8f5ba4b7":"data['LotFrontage'].isnull().sum()","cc6fa81b":"#create a new class 'other'\nfeatures=['Electrical','KitchenQual','SaleType','Exterior2nd','Exterior1st','Alley','Fence', 'MiscFeature','FireplaceQu','GarageCond','GarageQual','GarageFinish','GarageType','BsmtCond','BsmtExposure','BsmtQual','BsmtFinType2','BsmtFinType1','MasVnrType']\nfor name in features:\n    data[name].fillna('Other',inplace=True)","9961419d":"data[features].isnull().sum()","8517969c":"data['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n#data.MSZoning = data.groupby(['MSSubClass'])['MSZoning'].transform(lambda x: x.fillna(x.value_counts()[0]))","f8cecb10":"data['Functional']=data['Functional'].fillna('typ')","960ce94b":"\"\"\"mode=['Electrical','KitchenQual','SaleType','Exterior2nd','Exterior1st']\nfor name in mode:\n    data[name].fillna(data[name].mode()[0],inplace=True)\"\"\"","c7b60fb7":"zero=['GarageArea','GarageYrBlt','MasVnrArea','BsmtHalfBath','BsmtHalfBath','BsmtFullBath','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','GarageCars']\nfor name in zero:\n    data[name].fillna(0,inplace=True)","8d039b2a":"data.isnull().sum().sum()","99e2a539":"data.loc[data['MSSubClass']==60, 'MSSubClass']=0\ndata.loc[(data['MSSubClass']==20)|(data['MSSubClass']==120), 'MSSubClass']=1\ndata.loc[data['MSSubClass']==75, 'MSSubClass']=2\ndata.loc[(data['MSSubClass']==40)|(data['MSSubClass']==70)|(data['MSSubClass']==80), 'MSSubClass']=3\ndata.loc[(data['MSSubClass']==50)|(data['MSSubClass']==85)|(data['MSSubClass']==90)|(data['MSSubClass']==160)|(data['MSSubClass']==190), 'MSSubClass']=4\ndata.loc[(data['MSSubClass']==30)|(data['MSSubClass']==45)|(data['MSSubClass']==180), 'MSSubClass']=5\ndata.loc[(data['MSSubClass']==150), 'MSSubClass']=6","e5a558d6":"object_features = data.select_dtypes(include='object').columns\nobject_features","39c64385":"def dummies(d):\n    dummies_df=pd.DataFrame()\n    object_features = d.select_dtypes(include='object').columns\n    for name in object_features:\n        dummies = pd.get_dummies(d[name], drop_first=False)\n        dummies = dummies.add_prefix(\"{}_\".format(name))\n        dummies_df=pd.concat([dummies_df,dummies],axis=1)\n    return dummies_df","19a07774":"dummies_data=dummies(data)\ndummies_data.shape","82973efc":"data=data.drop(columns=object_features,axis=1)\ndata.columns","48fd33a1":"final_data=pd.concat([data,dummies_data],axis=1)\nfinal_data.shape","bd3f0467":"#Re-spliting the data into train and test datasets\ntrain_data=final_data.iloc[:1458,:]\ntest_data=final_data.iloc[1458:,:]\nprint(train_data.shape)\ntest_data.shape","7c1b8c3d":"# X: independent variables and y: target variable\nX=train_data\ny=train.loc[:,'SalePrice']","885af528":"from sklearn.linear_model import Ridge, RidgeCV, LassoCV, ElasticNet","64bb659e":"model_las_cv = LassoCV(alphas=(0.0001, 0.0005, 0.001, 0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10))\nmodel_las_cv.fit(X,y)\nlas_cv_preds=model_las_cv.predict(test_data)","2102d652":"model_ridge_cv = RidgeCV(alphas=(0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10))\nmodel_ridge_cv.fit(X, y)\nridge_cv_preds=model_ridge_cv.predict(test_data)","26250019":"model_ridge = Ridge(alpha=10, solver='auto')\nmodel_ridge.fit(X, y)\nridge_preds=model_ridge.predict(test_data)","8d0e4bf3":"model_en = ElasticNet(random_state=1, alpha=0.00065, max_iter=3000)\nmodel_en.fit(X, y)\nen_preds=model_en.predict(test_data)","927f7fa6":"import xgboost as xgb","f144e6e3":"model_xgb = xgb.XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\nmodel_xgb.fit(X, y)\nxgb_preds=model_xgb.predict(test_data)","55c5f81e":"from sklearn.ensemble import GradientBoostingRegressor","30efeac3":"model_gbr = GradientBoostingRegressor(n_estimators=3000, \n                                learning_rate=0.05, \n                                max_depth=4, \n                                max_features='sqrt', \n                                min_samples_leaf=15, \n                                min_samples_split=10, \n                                loss='huber', \n                                random_state =42)\nmodel_gbr.fit(X, y)\ngbr_preds=model_gbr.predict(test_data)","94cb5b16":"from lightgbm import LGBMRegressor","42d1db1d":"model_lgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       #min_data_in_leaf=2,\n                                       #min_sum_hessian_in_leaf=11\n                                       )\nmodel_lgbm.fit(X, y)\nlgbm_preds=model_lgbm.predict(test_data)","266ae290":"final_predictions = 0.3 * lgbm_preds + 0.3 * gbr_preds + 0.1 * xgb_preds + 0.3 * ridge_cv_preds","f9257d4e":"#display the first 5 predictions of sale price\nfinal_predictions[:5]","cf933fca":"#make the submission data frame\nsubmission = {\n    'Id': test.Id.values,\n    'SalePrice': final_predictions + 0.007 * final_predictions\n}\nsolution = pd.DataFrame(submission)\nsolution.head()","f0b79920":"#make the submission file\nsolution.to_csv('submission.csv',index=False)","a0e9341c":"display the columns with the number of missing values","ebdd0627":"Let's have fun now with Machine Learning and the Regression algorithms ","f9edfa3a":"It seems we have some features with null values, we'll deal with these values after","8447fc42":"Today, in this notebook, we'll try to solve the problem of house prices, first we'll clean the data, then we'll use Machine Learning regression algorithmes to predict the selling price of a house based on 79 characteristics.","f42207db":"We'll divide the data into numerical and categorical data and verify their descriptive statistics","679f121f":"It seems that we have 34 features with NaN values, before processing these values, it is important to know exactly what each feature means by reading the data description file","adf1e48a":"In our data set we have 80 features, check their names and data types using the dataframe attributes: columns and info","09060b7e":"That's great, our dataset no longer has any missing values. ","0aa6bb9d":"Let's load the required libraries","ac01a921":"We'll combine these two data sets using the concat attribute","3a80d6a5":"Another pre-processing step is to transform categorical features into numerical features","7980d28f":"After reading the description, there is the way we will impute and cleaning the missing values:\n1. impute the mean value to certain numerical features.\n2. create a new class 'other' for certain categorical features.\n3. use the mode value for the other categotical features.\n4. drop the ID and certain unnecessary features","369b683c":"Import train and test data sets, display the first 5 lines of the train data set and explore their shapes"}}