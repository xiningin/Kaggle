{"cell_type":{"c59b757c":"code","0b9ad4da":"code","4aeedd6f":"code","e50109a4":"code","f04a0760":"code","55a84ccf":"code","73a87c43":"code","38cbf1e8":"code","47b66b9f":"code","6985c3de":"code","41812928":"code","42f47067":"code","be3023d9":"code","2bde4933":"markdown","9744e314":"markdown","0dfaa93f":"markdown","8e58e483":"markdown","e4e8e984":"markdown","0d3c8a0a":"markdown","b8673003":"markdown","f4885164":"markdown","6350c873":"markdown","34d8bd69":"markdown","aaa71df9":"markdown","ab8977fc":"markdown","3b189646":"markdown","cfe5fa93":"markdown","8e467a24":"markdown","c6de0564":"markdown","07cd024c":"markdown","c4f165b0":"markdown","b0f1711f":"markdown","3fc21faf":"markdown","7c12955f":"markdown","4dddeab5":"markdown","6af94006":"markdown"},"source":{"c59b757c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set()\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom sklearn.preprocessing import StandardScaler\n\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\ncufflinks.go_offline(connected=True)\ninit_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef draw_missing_data_table(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data","0b9ad4da":"# Path of datasets\ntitanic_df = pd.read_csv('..\/input\/train.csv')\ntitanic_df.head()","4aeedd6f":"missing_values = draw_missing_data_table(titanic_df)\ndisplay(missing_values)\nmissing_values[['Percent']].iplot(kind='bar', xTitle='Features', yTitle='Percent of missing values', title='Percent missing data by feature')","e50109a4":"figure, axes = plt.subplots(1,1,figsize=(20, 8))\nplot = sns.catplot(x=\"Embarked\", y=\"Fare\", hue=\"Sex\", data=titanic_df, palette=('nipy_spectral'), kind=\"bar\", ax=axes)\nplt.close(plot.fig)\nplt.show()\ndisplay(titanic_df[titanic_df['Embarked'].isnull()])","f04a0760":"titanic_df['Embarked'].fillna('C', inplace=True)","55a84ccf":"titanic_df['Age'].fillna(titanic_df['Age'].median(), inplace=True)","73a87c43":"titanic_df['Cabin'].fillna('U', inplace=True)","38cbf1e8":"draw_missing_data_table(titanic_df[['Cabin', 'Age', 'Embarked']])","47b66b9f":"# Deck column from letter contained in cabin\ntitanic_df['Deck'] = titanic_df['Cabin'].str[:1]\ntitanic_df['Deck'] = titanic_df['Cabin'].map({cabin: p for p, cabin in enumerate(set(cab for cab in titanic_df['Cabin']))})\n\n# Title column from title contained in name\ntitanic_df['Title'] = pd.Series((name.split('.')[0].split(',')[1].strip() for name in titanic_df['Name']), index=titanic_df.index)\ntitanic_df['Title'] = titanic_df['Title'].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntitanic_df['Title'] = titanic_df['Title'].replace(['Mlle', 'Ms'], 'Miss')\ntitanic_df['Title'] = titanic_df['Title'].replace('Mme', 'Mrs')\n\n# Famillysize columns obtained by adding number of sibling and parch\ntitanic_df['FamillySize'] = titanic_df['SibSp'] + titanic_df['Parch'] + 1\ntitanic_df['FamillySize'][titanic_df['FamillySize'].between(1, 5, inclusive=False)] = 2\ntitanic_df['FamillySize'][titanic_df['FamillySize']>5] = 3\ntitanic_df['FamillySize'] = titanic_df['FamillySize'].map({1: 'Alone', 2: 'Medium', 3: 'Large'})\n\n# IsAlone and IsChild column, quite explicit\ntitanic_df['IsAlone'] = np.where(titanic_df['FamillySize']!=1, 0, 1)\ntitanic_df['IsChild'] = titanic_df['Age'] < 18\ntitanic_df['IsChild'] = titanic_df['IsChild'].astype(int)    ","6985c3de":"titanic_df = titanic_df.drop(['Name', 'Ticket', 'PassengerId', 'Cabin'], 1)    \ntitanic_df.head()","41812928":"titanic_df = pd.get_dummies(data=titanic_df, drop_first=True)\ntitanic_df.head()","42f47067":"ranges = titanic_df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Deck', 'IsChild']].max().to_frame().T\nranges.iplot(kind='bar', xTitle='Features', yTitle='Range', title='Range of feature before scaling')","be3023d9":"X = titanic_df.drop(['Survived'], 1)\ny = titanic_df['Survived']\n\n# Feature scaling of our data\nsc = StandardScaler()\nX = pd.DataFrame(sc.fit_transform(X.values), index=X.index, columns=X.columns)\nX.head()","2bde4933":"### Both passengers are female who paid 80 dollars as fare for their tickets. Moreover, they have the same ticket and cabin, so they probably had to board at the same place! According to the distribution above, the more probable embarked value for them is Cherbourg (C).. Let's replace these missing values:","9744e314":"### For the age, we have 177 missing values, it's way too much to look a them case by case. We'll replace it by the median, altrough it may exist better solution which take into account other columns. If you find a solution for replacing missing age values which improve a lot the accuracy of your model, please share it in comments !","0dfaa93f":"### And ... we are done ! Our dataset is finally ready to go into a machine learning algorithm ! Don't forget to check my two others kernel for this dataset:\n\n- [**Complete Titanic tutorial with ML, NN & Ensembling**](https:\/\/www.kaggle.com\/nhlr21\/complete-titanic-tutorial-with-ml-nn-ensembling)\n- [**Titanic colorful EDA**](https:\/\/www.kaggle.com\/nhlr21\/titanic-colorful-eda)","8e58e483":"# **General data preparation tutorial**\n","e4e8e984":"1. [**Dealing with missing values**](#missing)\n2. [**Features engineering**](#fe)\n3. [**Handling categorical features**](#catvar)\n4. [**Feature scaling**](#scaler)","0d3c8a0a":"### Finally, cabin column is useful for finding the deck in which the passenger cabin is located, so we'll keep it. Let's replace missing values by 'U', meaning 'Unkown':","b8673003":"## **2. Features engineering** <a id=\"fe\"><\/a>","f4885164":"### If a feature (a column of our dataset) do not have too much of it values missing, we can try to fill in these missing values. There are a lot of methods for filling in these values :\n\n- If there are too much missing data (>60%), you can drop the column :\n\n      titanic_df.drop('Cabin', axis=1, inplace=True)\n      \n- If there are only few missing data (1-2%) you can drop the rows which contain NAN :\n\n      titanic_df['Age'].dropna(inplace=True)\n\n### An better solution for a small amount of missing data is to study each obsevation case by case and replace missing values by looking at other features for this observation and try to find pattern between them to find out what can be the missing value.\n\n- In general, we don't want to loose data. A solution is to replace missing values by the mean or the median of the column. You should prefer median for columns which contains outliers that can skew the mean. \n\n      titanic_df['Age'].fillna(titanic_df['Age'].mean(), 1, inplace=True)\n      titanic_df['Age'].fillna(titanic_df['Age'].median(), 1, inplace=True)\n\n### The strategy of filling missing values depends a lot of the dataset and of your imagination ! So try to be creative, ask why these data are missing and how can I intelligently replace it ! Don't forget to try different replacing methods and to measure how the methods affect the performance of your model. Let's proceed with our titanic dataset :","6350c873":"## **3. Handling categorical features** <a id=\"catvar\"><\/a>","34d8bd69":"## **4. Feature scaling** <a id=\"scaler\"><\/a>","aaa71df9":"### The first problem we encounter when preparing the data to pass it to a ML algorithm is the missing data. Indeed, most of dataset, especially dataset made of real world data, have missing values. For example, we can see here that our titanic dataset have some missing values :","ab8977fc":"[*Deal with missing values*](https:\/\/towardsdatascience.com\/handling-missing-values-in-machine-learning-part-1-dda69d4f88ca)  \n[*Features engineering*](https:\/\/machinelearningmastery.com\/metrics-evaluate-machine-learning-algorithms-python\/)  \n[*Why hot-one Encode in machine learning ?*](https:\/\/machinelearningmastery.com\/why-one-hot-encode-data-in-machine-learning\/)  \n[*Dummy variable trap*](https:\/\/www.algosome.com\/articles\/dummy-variable-trap-regression.html)  \n[*Hot-one Encoding*](https:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding)  \n[*Scaling & Normalization*](https:\/\/medium.com\/greyatom\/why-how-and-when-to-scale-your-features-4b30ab09db5e)  ","3b189646":"### Welcome on this data preparation tutorial. This notebook is aimed for beginners who wants to learn how to correctly prepare a dataset in order to pass it to a machine learning algorithm. I encourage you to fork this notebook, try the code and improve it !\u00b6","cfe5fa93":"### Feature engineering is the art of creating new features from already existing features or from knowledge about data. For example, with a small search on internet, we can identify that the first letter in the values of the cabin column corresponds to the deck of the boat in which the cabin is located. Thus, we can create a 'Deck' feature from the cabin feature. We can also create a Title column which corresponds to the title contained in the name of each passenger. Your imagination is the only limit to creation of features ! But keep in mind that you don't want to create features just to create features, the goal is to improve the accuracy of the model ! Here is few examples of features creation for the titanic dataset:","8e467a24":"### Here is some additional resources you can consult to go further in understanding the different techniques that we are going to see in this notebook :","c6de0564":"## **Table of contents**","07cd024c":"### There is only two missing values for the embarked column, let's try to replace it. Below is the distribution of Embarked according to Fare and sex, and the two observations with missing \"Embarked\" value. Let's look at there two observations and choose the best matching embarked value according to their fare value and sex:","c4f165b0":"## **1. Dealing with missing values & outliers** <a id=\"missing\"><\/a>","b0f1711f":"### As you can notice when looking at the dataset above, we've some categorical features in our data. Categorical features are features which values are non-numeric. Here, we have 4 of them : Sex, Embarked, Title, FamillySize. We need to transform it into numeric features in order to pass it to a machine learning algorithm.\n### A solution is to transform these features into numeric features can be to map the string values with numeric values. This solution is called label encoding. It can be done easily in Python using LabelEncoder class from scikit learn or map method of a pandas dataframe. For example, to label encode the Embarked column of the titanic dataset we have to transform letters corresponding to the embarked location into a number :\n\n - Embarked Cherbourg corresponds to 1\n - Embarked Southampton corresponds to 2\n - Embarked Queenstown corresponds to 3\n\n### The problem of doing this is that the algorithm may see this as a ranking between the three values. A better solution is to use hot-one encoding. Hot-one encoding means create one column per value of the source column (it is called a dummy variable), which take only binary values. For example, the embacked column dummy encoded gives us three columns : Embarked_C, Embarked_S, Embarked_Q. A passenger who embarked at southampton will for example have his Embarked_S column set to 1, whereas the two other embarked columns will be set to 0.\n\n![](https:\/\/www.renom.jp\/notebooks\/tutorial\/preprocessing\/category_encoding\/renom_cat_onehot.png)\n\n### However, by doing this, we create a redundant column : with two of the three Embarked colomns, we can guess easily the value of the third column. For example, a passenger with Embarked_C and Embarked_S set to 0 will have necessarily his Embarked_Q column set to 1. In order to avoid that redundance, called the dummy variable trap, we must drop one of the column made when creating a dummy variable. \n\n*Note: One-hot encoding usually helps, but it can varies on a case-by-case basis. Do not hesitate to test the effect of HO encoding on your model to see if you need it.*\n\n### There is a verry simple way to do hot-one encoding in python, the pandas get_dummies function creates hot-one encoding for all categorical features of a dataset. By adding the argument drop_first=True, we drop one column for each dummy variable to dummy encode: ","3fc21faf":"### Finally, we need to perform normalization on the data. Normalizing the data is necessary because feeding a machine learning model with large or heterogeneous values can trigger large gradient updates that will prevent the gradient descent algorithm from converging. Let's look at the ranges of values for our dataframe:","7c12955f":"### Once we've finished to create our new features, we can delete all useless remaining columns and print the first rows of our dataset:","4dddeab5":"### Ranges are very heterogeneous. One way to change this is by using features scaling. Features scaling will set each column mean to 0 and each column variance to 1. In python, the StandarScaler class of the scikit-learn module allows us to do it vey easily:","6af94006":"![](https:\/\/2s7gjr373w3x22jf92z99mgm5w-wpengine.netdna-ssl.com\/wp-content\/uploads\/2016\/07\/shutterstock_data_prep_-faithie.jpg)"}}