{"cell_type":{"15838591":"code","eee1b375":"code","fd490c71":"code","f2300a0b":"code","f8b5a9ac":"code","5023a778":"code","8d5cebf3":"code","d4b0af45":"code","fbd61969":"code","bb27b347":"code","b06e2394":"code","823d65ec":"code","b4695d55":"code","7ff24a77":"code","45e35849":"code","5a7fe491":"code","5af010d9":"code","ec95a44b":"code","49084cdf":"code","e1bce8bd":"code","344b5010":"code","d9d78d53":"code","c06a25f4":"code","b049fcb2":"code","804f6498":"code","de463cb7":"code","f76e9519":"code","fce8eda8":"code","09d6c312":"code","b47a5a64":"code","c96d5974":"code","0ade983c":"code","a85f7736":"code","ef1cb5c4":"code","631d7fd3":"code","c59cb920":"code","18e87a68":"code","bd13f9ec":"code","b60040c0":"code","e574d9f6":"code","3e329b58":"code","b00c141b":"code","1a4f194d":"markdown","352ed663":"markdown","02ae9292":"markdown","ba552f94":"markdown","dd9ed3a1":"markdown","0bd8e9e4":"markdown","41caa829":"markdown","68452187":"markdown","886ceda6":"markdown","1f1262f6":"markdown","25550c1a":"markdown","7117dbcc":"markdown","63860183":"markdown","c324a2c3":"markdown","a012ff21":"markdown","dacc90ce":"markdown","548d3497":"markdown","2443f326":"markdown","6fbee0ad":"markdown","3372f953":"markdown","fd0cf540":"markdown","6a2beb6e":"markdown","4ea0a6cd":"markdown","b5366bc9":"markdown"},"source":{"15838591":"import numpy as np\nimport pandas as pd\nimport os, sys, math\nimport tensorflow as tf\nfrom pathlib import Path\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# AUTO will be used in tf.data.Dataset API\nAUTO = tf.data.experimental.AUTOTUNE \n\nprint(\"Tensorflow version \" + tf.__version__)","eee1b375":"show_files=0\n\n# if you want to see the full content of the\n# 'kaggle\/input'directory set show_files=1\n\nif show_files:\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))","fd490c71":"SHARDS = 20\nTARGET_SIZE = [512, 512]\nCLASSES = [b'benign', b'malignant']\n\nPATH_DATA=Path('\/kaggle\/input\/siim-isic-melanoma-classification\/')\nPATH_FOLDS=Path('\/kaggle\/input\/siim-stratified-groupkfold-5-folds\/')","f2300a0b":"train=pd.read_csv(PATH_DATA\/'train.csv')\nprint(f\"The shape of the `train` is {train.shape}.\\n\")\nprint(f\"The columns present in `train` are {train.columns.values}.\")","f8b5a9ac":"test=pd.read_csv(PATH_DATA\/'test.csv')\nprint(f\"The shape of the `test` is {test.shape}.\\n\")\nprint(f\"The columns present in `test` are {test.columns.values}.\")","5023a778":"train.isna().sum()","8d5cebf3":"test.isna().sum()","d4b0af45":"median_age=train['age_approx'].median()\nprint(f\"The median age of the patients in the training set is {median_age} years.\")","fbd61969":"train['age_approx'].fillna(median_age, inplace=True)\ntrain.fillna('unknown', inplace=True)\ntest.fillna('unknown', inplace=True)","bb27b347":"print(f\"The total number of NA's after imputation in `train` is {train.isna().sum().sum()}.\")\nprint(f\"The total number of NA's after imputation in `test` is {test.isna().sum().sum()}.\")","b06e2394":"print(\"The unique values of 'age_approx':\")\nprint(np.unique(train['age_approx'].values))\nprint(\"\\nThe unique values of 'sex':\")\nprint(np.unique(train['sex'].values))\nprint(\"\\nThe unique values of 'anatom_site_general_challenge':\")\nprint(np.unique(train['anatom_site_general_challenge'].values))\nprint(\"\\nThe unique values of 'diagnosis':\")\nprint(np.unique(train['diagnosis'].values))","823d65ec":"print(\"The unique values of 'age_approx':\")\nprint(np.unique(test['age_approx'].values))\nprint(\"\\nThe unique values of 'sex':\")\nprint(np.unique(test['sex'].values))\nprint(\"\\nThe unique values of 'anatom_site_general_challenge':\")\nprint(np.unique(test['anatom_site_general_challenge'].values))","b4695d55":"train['age_approx']=train['age_approx'].astype(np.uint8)\ntest['age_approx']=test['age_approx'].astype(np.uint8)","7ff24a77":"np.equal(np.unique(test['anatom_site_general_challenge'].values),\n         np.unique(train['anatom_site_general_challenge'].values)\n        ).all()","45e35849":"train = pd.concat([train, pd.get_dummies(train['sex'], prefix='sex')], axis=1)\ntrain = pd.concat([train, pd.get_dummies(train['anatom_site_general_challenge'], \n                                         prefix='site')], axis=1)\n# train = pd.concat([train, pd.get_dummies(train['diagnosis'], prefix='diagn')], axis=1)\n\ntrain.shape","5a7fe491":"test = pd.concat([test, pd.get_dummies(test['sex'], prefix='sex')], axis=1)\ntest = pd.concat([test, pd.get_dummies(test['anatom_site_general_challenge'],\n                                       prefix='site')], axis=1)\n# the following columns is added for consistency with `train`\ntest['sex_unknown']=np.zeros(len(test))\ntest['sex_unknown']=test['sex_unknown'].astype(np.uint8)\n\ntest.shape","5af010d9":"pd.set_option('display.max_columns', None)\ntest.head()","ec95a44b":"%%time\n\nscaler=StandardScaler()\n\ntrain['age_scaled']=scaler.fit_transform(train['age_approx'].values.reshape(-1, 1))\ntest['age_scaled']=scaler.transform(test['age_approx'].values.reshape(-1, 1))","49084cdf":"excluded_cols=['sex', 'anatom_site_general_challenge', 'diagnosis', ]\n\ncols=[c for c in test.columns if c not in excluded_cols]\n\nprint(cols)\nprint(f\"\\nThe total number of features is {len(cols)}.\")","e1bce8bd":"dataset0 = tf.data.Dataset.from_tensor_slices(dict(test[cols]))","344b5010":"def show_instance(item, special):\n    for k, v in item.items():\n        if k not in special:\n            print(k, v.numpy())\n        else:\n            print(\"Image shape\", v.numpy().shape)","d9d78d53":"def show_ds(ds, n=1, special=['image']):\n    for item in ds.take(n):\n        show_instance(item, special)","c06a25f4":"show_ds(dataset0)","b049fcb2":"def decode_jpeg(data_dict): \n    fname=\"\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/test\/\" \\\n          +data_dict['image_name']+\".jpg\"\n    bits = tf.io.read_file(fname)\n    data_dict['image'] = tf.image.decode_jpeg(bits)  \n    return data_dict","804f6498":"dataset1 = dataset0.map(decode_jpeg, num_parallel_calls=AUTO)","de463cb7":"show_ds(dataset1)","f76e9519":"def show_9(dataset):\n    plt.figure(figsize=(13,13))\n    subplot=331\n    i=0\n    for data in dataset:  \n        i+=1\n        plt.subplot(subplot)\n        plt.axis('off')\n        plt.imshow(data['image'].numpy().astype(np.uint8))\n        subplot += 1\n        if i==9:\n            break\n    plt.tight_layout()\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n    plt.show()","fce8eda8":"show_9(dataset1)","09d6c312":"def resize_and_crop_image(data):\n    # Resize and crop using \"fill\" algorithm:\n    # always make sure the resulting image\n    # is cut out from the source image so that\n    # it fills the TARGET_SIZE entirely with no\n    # black bars and a preserved aspect ratio.\n    w = tf.shape(data['image'])[0]\n    h = tf.shape(data['image'])[1]\n    tw = TARGET_SIZE[1]\n    th = TARGET_SIZE[0]\n    resize_crit = (w * th) \/ (h * tw)\n    data['image'] = tf.cond(resize_crit < 1,\n                            # if true\n                            lambda: tf.image.resize(data['image'], [w*tw\/w, h*tw\/w],\n                                                    method='lanczos3',\n                                                    antialias=True\n                                                   ),\n                            # if false\n                            lambda: tf.image.resize(data['image'], [w*th\/h, h*th\/h],\n                                                    method='lanczos3',\n                                                    antialias=True\n                                                   )\n                           )\n    nw = tf.shape(data['image'])[0]\n    nh = tf.shape(data['image'])[1]\n    data['image'] = tf.image.crop_to_bounding_box(data['image'], \n                                                  (nw - tw) \/\/ 2, \n                                                  (nh - th) \/\/ 2, \n                                                  tw, th\n                                                 )\n    return data, h, w","b47a5a64":"dataset2 = dataset1.map(resize_and_crop_image, num_parallel_calls=AUTO)","c96d5974":"def show_9(dataset):\n    plt.figure(figsize=(13,13))\n    subplot=331\n    i=0\n    for data, h, w in dataset:  \n        i+=1\n        plt.subplot(subplot)\n        plt.axis('off')\n        plt.imshow(data['image'].numpy().astype(np.uint8))\n        subplot += 1\n        if i==9:\n            break\n    plt.tight_layout()\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n    plt.show()","0ade983c":"show_9(dataset2)","a85f7736":"%%time\n\ndisplay_dataset = dataset2.batch(10)\nfor item, h, w in display_dataset.take(10):\n    print(f\"Image batch shape {item['image'].numpy().shape}\")","ef1cb5c4":"def recompress_image(data, h, w):\n\n    data['image'] = tf.cast(data['image'], tf.uint8)\n    data['image'] = tf.image.encode_jpeg(data['image'], \n                                         #quality=100,\n                                         optimize_size=True, \n                                         chroma_downsampling=False)\n    return data, h, w","631d7fd3":"dataset3 = dataset2.map(recompress_image, num_parallel_calls=AUTO)","c59cb920":"nb_images = len(test)\nshard_size = math.ceil(1.0 * nb_images \/ SHARDS)\n\nprint(f\"The total number of images = {nb_images}\")\nprint(f\"The number of  .tfrecord files = {SHARDS}\")\nprint(f\"The number of images in each .tfrecord file = {shard_size}\")","18e87a68":"dataset4 = dataset3.batch(shard_size)","bd13f9ec":"def _bytestring_feature(list_of_bytestrings):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))","b60040c0":"def _int_feature(list_of_ints): # int64\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))","e574d9f6":"def _float_feature(list_of_floats): # float32\n    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))","3e329b58":"def to_tfrecord(tfrec_filewriter, image, image_name, patient_id, \n                age, age_scaled, sex_female, sex_male, sex_unknown, \n                site_head_neck, site_lower_extremity, site_oral_genital, \n                site_palms_soles, site_torso, site_unknown, site_upper_extremity, \n                height, width):\n\n    feature = {\n        # bytestring features\n        \"image\": _bytestring_feature([image]), \n        \"image_name\": _bytestring_feature([image_name]),\n        \"patient_id\": _bytestring_feature([patient_id]), \n        # integer features\n        \"age\": _int_feature([age]),\n        \"sex_female\": _int_feature([sex_female]),        \n        \"sex_male\": _int_feature([sex_male]),\n        \"sex_unknown\": _int_feature([sex_unknown]),\n        \"site_head\/neck\": _int_feature([site_head_neck]),\n        \"site_lower extremity\": _int_feature([site_lower_extremity]),\n        \"site_oral\/genital\": _int_feature([site_oral_genital]),\n        \"site_palms\/soles\": _int_feature([site_palms_soles]), \n        \"site_torso\": _int_feature([site_torso]), \n        \"site_unknown\": _int_feature([site_unknown]), \n        \"site_upper extremity\": _int_feature([site_upper_extremity]),\n        \"height\": _int_feature([height]),\n        \"width\": _int_feature([width]),\n        # float features\n        \"age_scaled\": _float_feature([age_scaled]),\n    }\n    \n    return tf.train.Example(features=tf.train.Features(feature=feature))","b00c141b":"print(\"Writing TFRecords\")\nfor shard, (data, height, width) in enumerate(dataset4):\n    \n#     if shard not in range(SHARDS\/\/2*(N-1), SHARDS\/\/2*N):\n#         continue\n    # batch size used as shard size here\n    shard_size = data['image'].numpy().shape[0]\n    # good practice to have the number of records in the filename\n    filename = \"{:02d}-{}.tfrec\".format(shard, shard_size)\n\n    with tf.io.TFRecordWriter(filename) as out_file:\n        for i in range(shard_size):\n            example = to_tfrecord(out_file,\n                                  # re-compressed image: already a byte string\n                                  data['image'].numpy()[i],\n                                  data['image_name'].numpy()[i],\n                                  data['patient_id'].numpy()[i],\n                                  data['age_approx'].numpy()[i],\n                                  data['age_scaled'].numpy()[i],\n                                  data['sex_female'].numpy()[i],\n                                  data['sex_male'].numpy()[i],\n                                  data['sex_unknown'].numpy()[i],\n                                  data['site_head\/neck'].numpy()[i],\n                                  data['site_lower extremity'].numpy()[i],\n                                  data['site_oral\/genital'].numpy()[i],\n                                  data['site_palms\/soles'].numpy()[i],\n                                  data['site_torso'].numpy()[i],\n                                  data['site_unknown'].numpy()[i],\n                                  data['site_upper extremity'].numpy()[i],\n                                  height.numpy()[i],\n                                  width.numpy()[i]\n                                 )\n\n            out_file.write(example.SerializeToString())\n\n    print(\"Wrote file {} containing {} records\".format(filename, shard_size))","1a4f194d":"### Speed test: too slow\n\nGoogle Cloud Storage is capable of great throughput but has a per-file access penalty. Run the cell below and see that throughput is around 5 images per second (at least this was the speed at the time of writing this notebook).","352ed663":"### Recompress the images\n\nAs we just saw, working with thousands of individual files will be too slow. We have to use the TFRecord format to group files together. To do that, we first need to recompress our images. The bandwidth savings outweight the decoding CPU cost. The bandwidth savings outweight the decoding CPU cost.","02ae9292":"The unique values in `train`:","ba552f94":"We will replace the missing values for `age_approx` with the median age of the patients present in the dataset. As for the other two columns, we will mark the missing values with the word \"unknown\". ","dd9ed3a1":"Observe that the age values are all integer in both `train` and `test`. Let's cast `age_approx` into `np.uint8` format.","0bd8e9e4":"### Acknowledgement\n\nIn this notebook, we follow the approach outlined by Martin G\u00f6rner in [Part 1 of his Keras on TPU series](https:\/\/codelabs.developers.google.com\/codelabs\/keras-flowers-data\/#0).","41caa829":"The unique values in `test`:","68452187":"### Loading training data","886ceda6":"### Setting up basic parameters","1f1262f6":"The `anatom_site_general_challenge` column of the test set contains missing values as well","25550c1a":"### Turning the fold data into a TF dataset","7117dbcc":"### Visualization function","63860183":"### Write dataset to TFRecord files ","c324a2c3":"Note that there is no `diagnosis` column in the test set. ","a012ff21":"### Resizing and cropping","dacc90ce":"Sharding: there will be one \"batch\" of images per file","548d3497":"### Loading libraries","2443f326":"### Imputing missing values\n\nThe `sex`, `age_approx`, and `anatom_site_general_challenge` columns of the training set contain missing values:","6fbee0ad":"### One-hot encoding for categorical variables","3372f953":"Three types of data can be stored in TFRecords: bytestrings, integers and floats. They are always stored as lists, a single data element will be a list of size 1.","fd0cf540":"Yes, it does. Now we will apply one-hot encoding to `sex` and `anatom_site_general_challenge`. We will not be one-hot encoding `diagnosis` since it is present only in the training set. ","6a2beb6e":"### Scaling the age feature","4ea0a6cd":"Checking if `anatom_site_general_challenge` has the same set of values in `train` and `test`:","b5366bc9":"Redefine our plotting finction to account for the new height and width features (alternatively, you can just add these features to the `data` dictionary)."}}