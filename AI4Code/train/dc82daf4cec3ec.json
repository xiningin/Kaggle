{"cell_type":{"4938c211":"code","43ef5269":"code","d84e4a6e":"code","af4dda44":"code","c6bdb09f":"code","ba107d8c":"code","38a18810":"code","bb4f069a":"code","3cf7327a":"code","64e3c5fa":"code","57793ce7":"code","30ada83f":"code","20d35b89":"code","06a05451":"code","ee3479e5":"code","1e7a48ed":"code","9317d8a9":"code","b4bacbda":"code","1f7833ad":"code","cd1353f0":"code","80cf23e7":"code","511fa378":"code","a46038fd":"code","651092ac":"code","5980106e":"code","136243fd":"code","a3b377f2":"code","0b9a11ff":"code","e4055e65":"code","758e82c9":"code","4a078448":"code","eae12359":"code","bff64436":"code","c1cbf701":"code","ed24cf8d":"code","e9e18333":"code","bfe65ffc":"code","069c926b":"code","e9ad63c3":"code","b81c9961":"code","18332280":"code","43c57913":"code","c6990d50":"code","39331fc5":"code","e97afa64":"code","e86afed9":"code","cccf4898":"code","54daa205":"code","dd277911":"code","40c92f58":"code","d6e06527":"code","0cf3419b":"code","518343d7":"code","f06d7c73":"markdown","6ba6ca53":"markdown","3041c892":"markdown","1dc48107":"markdown","311325c5":"markdown","437c4742":"markdown","2fb07f65":"markdown","dfe6d194":"markdown","ce1f5fa9":"markdown","9976b0d6":"markdown","ecd7c465":"markdown","86a87106":"markdown","bf633907":"markdown","25b9e51f":"markdown","38dbf5a1":"markdown","7fa75f84":"markdown","23a615d3":"markdown","0c90c6c3":"markdown","d0b29483":"markdown","0c5d911d":"markdown","5682f2e6":"markdown","db520014":"markdown","290d9e23":"markdown","1c37f34d":"markdown","45552076":"markdown","d7bf53ae":"markdown","e9b6fc68":"markdown","6e53cd69":"markdown","75a70697":"markdown","a71c9afd":"markdown","65e155ef":"markdown","2343d9e0":"markdown","d1b42f39":"markdown","c653f87d":"markdown","99d51e65":"markdown","0cb2241a":"markdown","703da229":"markdown","1c674138":"markdown","8bb7a4d5":"markdown"},"source":{"4938c211":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#from lofo import LOFOImportance # https:\/\/github.com\/aerdem4\/lofo-importance\n%matplotlib inline\nplt.style.use('seaborn-whitegrid')\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom datetime import datetime\nimport math\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom xgboost import XGBRegressor","43ef5269":"# Import data\ndf_f = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip\")\ndf_st = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv\")\ndf_train = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip\")\ndf_test = pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip\")\n\n\n# check the data\ndf_test.head(3)\n#df_test.shape","d84e4a6e":"# Make date column into datetime\ndf_f[\"Date\"] = pd.to_datetime(df_f[\"Date\"])\ndf_train[\"Date\"] = pd.to_datetime(df_train[\"Date\"])\ndf_test[\"Date\"] = pd.to_datetime(df_test[\"Date\"])\n\n# Test if it works\nprint(df_train[0:1].Date, df_train[-1:].Date)\nprint(df_test[0:1].Date, df_test[-1:].Date)\nprint(df_f[0:1].Date, df_f[-1:].Date)","af4dda44":"# Assess the total amount of columns and rows in a more consise way\nprint(df_f.shape)\nprint(df_train.shape)\nprint(df_test.shape)\nprint(df_st.shape)","c6bdb09f":"# Merging training data\ndf_all_train = df_train.merge(df_f, how=\"left\", on=[\"Store\",\"Date\",\"IsHoliday\"])\n#df_all_train.head(3)\n\n# Merging test data\ndf_all_test = df_test.merge(df_f, how=\"left\", on=[\"Store\",\"Date\",\"IsHoliday\"])\ndf_all_test.tail(3)\n","ba107d8c":"# More merging of training data\ndf_all_train = df_all_train.merge(df_st, how=\"left\", on=[\"Store\"])\ndf_all_train.head(3)\n\n# More merging of test data\ndf_all_test = df_all_test.merge(df_st, how=\"left\", on=[\"Store\"])\n#df_all_test.head(3)\n","38a18810":"# Check how our final df_all_train looks\nprint(\"Rows & Columns: \", df_all_train.shape, \"\\nAll columns if the df: \", df_all_train.columns.tolist())\n\n# # Check how our final df_all_test looks\nprint(\"Rows & Columns: \", df_all_test.shape, \"\\nAll columns if the df: \", df_all_test.columns.tolist())\n","bb4f069a":"df_all_train = df_all_train.applymap(lambda x: 1 if x == True else x)\ndf_all_train = df_all_train.applymap(lambda x: 0 if x == False else x)\n\ndf_all_test = df_all_test.applymap(lambda x: 1 if x == True else x)\ndf_all_test = df_all_test.applymap(lambda x: 0 if x == False else x)","3cf7327a":"# Checking the DFs\ndf_all_train.head(3)\n#df_all_test.head(3)","64e3c5fa":"# Check where we find NaN values\n\ntab_info = pd.DataFrame(df_all_train.dtypes).T.rename(index={0:'column Type'}) \ntab_info = tab_info.append(pd.DataFrame(df_all_train.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info = tab_info.append(pd.DataFrame(df_all_train.isnull().sum()\/df_all_train.shape[0]*100).T.\n                                       rename(index={0: 'null values (%)'}))\ntab_info","57793ce7":"#replace Temperature, fule price, CPI and unemployment with averages. \n# This seems more reasonable than replacing them with \"0\"s.                                                \ndf_all_train[[\"Temperature\"]] = df_all_train[[\"Temperature\"]].fillna(df_all_train[[\"Temperature\"]].mean())\ndf_all_train[[\"Fuel_Price\"]] = df_all_train[[\"Fuel_Price\"]].fillna(df_all_train[[\"Fuel_Price\"]].mean())\ndf_all_train[[\"CPI\"]] = df_all_train[[\"CPI\"]].fillna(df_all_train[[\"CPI\"]].mean())\ndf_all_train[[\"Unemployment\"]] = df_all_train[[\"Unemployment\"]].fillna(df_all_train[[\"Unemployment\"]].mean())\n\ndf_all_test[[\"Temperature\"]] = df_all_test[[\"Temperature\"]].fillna(df_all_test[[\"Temperature\"]].mean())\ndf_all_test[[\"Fuel_Price\"]] = df_all_test[[\"Fuel_Price\"]].fillna(df_all_test[[\"Fuel_Price\"]].mean())\ndf_all_test[[\"CPI\"]] = df_all_test[[\"CPI\"]].fillna(df_all_train[[\"CPI\"]].mean())\ndf_all_test[[\"Unemployment\"]] = df_all_test[[\"Unemployment\"]].fillna(df_all_test[[\"Unemployment\"]].mean())","30ada83f":"# Check the results\ndf_all_train.head(3)","20d35b89":"# Replace the NaN values in markdown with 0\ndf_all_train = df_all_train.fillna(0)\ndf_all_test = df_all_test.fillna(0)","06a05451":"# Check the results\ndf_all_train.head(3)","ee3479e5":"# Check NaN values in df_all_train\n\ntab_info = pd.DataFrame(df_all_train.dtypes).T.rename(index={0:'column Type'}) \ntab_info = tab_info.append(pd.DataFrame(df_all_train.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info = tab_info.append(pd.DataFrame(df_all_train.isnull().sum()\/df_all_train.shape[0]*100).T.\n                                       rename(index={0: 'null values (%)'}))\ntab_info","1e7a48ed":"# Check NaN values in df_all_test\n\ntab_info = pd.DataFrame(df_all_test.dtypes).T.rename(index={0:'column Type'}) \ntab_info = tab_info.append(pd.DataFrame(df_all_test.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info = tab_info.append(pd.DataFrame(df_all_test.isnull().sum()\/df_all_test.shape[0]*100).T.\n                                       rename(index={0: 'null values (%)'}))\ntab_info","9317d8a9":"df_all_train = pd.get_dummies(df_all_train, columns=[\"Type\"])\ndf_all_test = pd.get_dummies(df_all_test, columns=[\"Type\"])","b4bacbda":"# Checking the DFs after all of the processing\ndf_all_train.head(3)\n#df_all_test.head(3)","1f7833ad":"# Start with simple .describe() to get the general jist of the dataframe\ndf_all_train.describe()","cd1353f0":"# Plots for the different features in the df: \ndf_all_train[['Date', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']].plot(x='Date', subplots=True, figsize=(20,15))\n\nplt.show()","80cf23e7":"# Average weekly sales for the stores\/departments during a week\ndf_average_sales_weekly = df_all_train.groupby(by=[\"Date\"], as_index = False)[\"Weekly_Sales\"].sum()\n\ndf_average_sales = df_average_sales_weekly.sort_values(\"Weekly_Sales\", ascending=False)\n\n#print(df_average_sales[:10])\n\nplt.figure(figsize=(20,5))\nplt.plot(df_average_sales_weekly.Date, df_average_sales_weekly.Weekly_Sales)\nplt.show","511fa378":"df_all_train[\"month\"] = df_all_train['Date'].dt.month\ndf_all_train = df_all_train.drop([\"Date\"], axis=1)\ndf_all_train = pd.get_dummies(df_all_train, columns=[\"month\"])#, prefix='month')\n#df_all_train = df_all_train.drop('Date', 1)\n\ndf_all_test[\"month\"] = df_all_test['Date'].dt.month\ndf_all_test = df_all_test.drop([\"Date\"], axis=1)\ndf_all_test = pd.get_dummies(df_all_test, columns=[\"month\"])#, prefix='month')\n#df_all_test = df_all_test.drop('Date', 1)","a46038fd":"# Correlation between all features\nsns.heatmap(df_all_train.corr())","651092ac":"# Correlation between Weekly Sales and all other features\ndf_all_train_corr = df_all_train[df_all_train.columns[:]].corr()['Weekly_Sales'][:]\ndf_all_train_corr = df_all_train_corr.drop(\"Weekly_Sales\")\ndf_all_train_corr","5980106e":"# Plot the correlation \ndf_all_train_corr.plot.bar()","136243fd":"# DF x & y\ntrain_all = df_all_train # make it shorter syntax\n\ntrain_X = df_all_train.drop([\"Weekly_Sales\", \"month_8\", \"month_9\", \"month_10\"], axis=1)\ntrain_y = train_all[\"Weekly_Sales\"]\ntest_X = df_all_test # making the syntax shorter --> Sub = submission\n\nX_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.2, shuffle=False, stratify=None)","a3b377f2":"#X_train.shape\n#X_test.shape\n\nX_train.head(3)\n#y_train.head(3)\n\n#X_test.head(3)\n#y_test.head(3)\n\n#test_X.head(3)","0b9a11ff":"reg = LinearRegression()\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)","e4055e65":"# Print out the MAE, MSE & RMSE\nprint(\"MAE: \", metrics.mean_absolute_error(y_test, y_pred)) #MAE\nprint(\"MSE: \", metrics.mean_squared_error(y_test, y_pred)) #MSE\nprint(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_pred))) #RMSE\n\n# rSquared\nscore = r2_score(y_test, y_pred)\nprint(\"R^2:\", score)","758e82c9":"knn = KNeighborsRegressor(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)","4a078448":"from sklearn import metrics\n\n# Print out the MAE, MSE & RMSE\nprint(\"MAE: \", metrics.mean_absolute_error(y_test, y_pred)) #MAE\nprint(\"MSE: \", metrics.mean_squared_error(y_test, y_pred)) #MSE\nprint(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_pred))) #RMSE\n\n# rSquared\nscore = r2_score(y_test, y_pred)\nprint(\"R^2:\", score)","eae12359":"dt = DecisionTreeRegressor()\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)","bff64436":"# Print out the MAE, MSE & RMSE\nprint(\"MAE: \", metrics.mean_absolute_error(y_test, y_pred)) #MAE\nprint(\"MSE: \", metrics.mean_squared_error(y_test, y_pred)) #MSE\nprint(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_pred))) #RMSE\n\n# rSquared\nscore = r2_score(y_test, y_pred)\nprint(\"R^2:\", score)","c1cbf701":"rf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)","ed24cf8d":"# Print out the MAE, MSE & RMSE\nprint(\"MAE: \", metrics.mean_absolute_error(y_test, y_pred)) #MAE\nprint(\"MSE: \", metrics.mean_squared_error(y_test, y_pred)) #MSE\nprint(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_pred))) #RMSE\n\n# rSquared\nscore = r2_score(y_test, y_pred)\nprint(\"R^2:\", score)","e9e18333":"etr_random_best = ExtraTreesRegressor(bootstrap=False, criterion=\"mse\", max_depth=None,\n                                      max_features=\"auto\", max_leaf_nodes=None,\n                                      min_impurity_decrease=0.0, min_impurity_split=None,\n                                      min_samples_leaf=2, min_samples_split=5,\n                                      min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=30,\n                                      oob_score=False, random_state=None, warm_start=False)\netr_random_best.fit(X_train, y_train)","bfe65ffc":"y_pred = etr_random_best.predict(X_test)","069c926b":"# Print out the MAE, MSE & RMSE\nprint(\"MAE: \", metrics.mean_absolute_error(y_test, y_pred)) #MAE\nprint(\"MSE: \", metrics.mean_squared_error(y_test, y_pred)) #MSE\nprint(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_pred))) #RMSE\n\n# rSquared\nscore = r2_score(y_test, y_pred)\nprint(\"R^2:\", score)","e9ad63c3":"xg = XGBRegressor()\nxg.fit(X_train, y_train)\ny_pred = xg.predict(X_test)","b81c9961":"# Print out the MAE, MSE & RMSE\nprint(\"MAE: \", metrics.mean_absolute_error(y_test, y_pred)) #MAE\nprint(\"MSE: \", metrics.mean_squared_error(y_test, y_pred)) #MSE\nprint(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_pred))) #RMSE\n\n# rSquared\nscore = r2_score(y_test, y_pred)\nprint(\"R^2:\", score)","18332280":"rid = Ridge()\nrid.fit(X_train, y_train)\ny_pred = rid.predict(X_test)","43c57913":"# Print out the MAE, MSE & RMSE\nprint(\"MAE: \", metrics.mean_absolute_error(y_test, y_pred)) #MAE\nprint(\"MSE: \", metrics.mean_squared_error(y_test, y_pred)) #MSE\nprint(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_pred))) #RMSE\n\n# rSquared\nscore = r2_score(y_test, y_pred)\nprint(\"R^2:\", score)","c6990d50":"las = Lasso()\nlas.fit(X_train, y_train)\ny_pred = las.predict(X_test)","39331fc5":"# Print out the MAE, MSE & RMSE\nprint(\"MAE: \", metrics.mean_absolute_error(y_test, y_pred)) #MAE\nprint(\"MSE: \", metrics.mean_squared_error(y_test, y_pred)) #MSE\nprint(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_pred))) #RMSE\n\n# rSquared\nscore = r2_score(y_test, y_pred)\nprint(\"R^2:\", score)","e97afa64":"y_pred = etr_random_best.predict(test_X) # Un-test data for the final submission","e86afed9":"#pred = pd.DataFrame(y_pred.astype(str))\npred[\"Weekly_Sales\"] = pd.DataFrame(y_pred.astype(str))","cccf4898":"pred.head(3)","54daa205":"Id = pd.DataFrame(df_test[[\"Store\", \"Dept\",\"Date\"]])\nId.head(3)","dd277911":"Id[\"Id\"] = Id[\"Store\"].astype(str) + \"_\" + Id[\"Dept\"].astype(str) + \"_\" + Id[\"Date\"].astype(str)\nId.head(3)","40c92f58":"Id = pd.DataFrame(Id[\"Id\"])\nId.head(3)","d6e06527":"#submission = pd.DataFrame(Id[\"Id\"]) + pd.DataFrame(pred[\"Weekly_Sales\"])\n#submission = pd.concat([Id[\"Id\"], pred[\"Weekly_Sales\"]])\n#submission pd.DataFrame({'date':df.date,'Anomaly':tmp.Anomaly.combine_first(df.Anomaly)}\n#submission = pd.DataFrame({})\nsubmission = pd.concat([Id,pred[\"Weekly_Sales\"]], axis = 1)","0cf3419b":"#submission.head(3)\nsubmission.shape # Needs to be 115064 predicitons according to the competition","518343d7":"submission.to_csv(\"Weekly_Sales_Prediction.csv\", index=False)","f06d7c73":"_______\n## Decision Tree Regressor","6ba6ca53":"_____\n## Random Forest Regressor","3041c892":"#### Creating dummy variables for each month ","1dc48107":"______\n## XGBoost","311325c5":"_____\n\n## Extra Trees Regressor","437c4742":"_____\n# Results Score from Kaggle\n\n* We get a score of ***3883.47399*** which puts us in a better spot than our benchmark: Hari Khanal\", with the score of ***3985.79966.*** \n\n\n## *WE DID IT!*","2fb07f65":"_______\n# 4. Benchmark\n\n**How will we benchmark our results?**\n\n![ML](https:\/\/www.kaggle.com\/static\/images\/about\/inclass\/howitworks@2x.png)\n\n* The initial benchmarking will be made by looking at three different metrics used for estimating errors, as well as one statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. These are: \n\n**Error Metrics:**\n1. Mean Absolute Error (MAE)\n2. Mean Sqared Error (MSE)\n3. Root Mean Squared Error (RMSE)\n\n**Model Valuation Metric *(robustness of the model):***\n1. R-squared (R^2)\n\n### Kaggle Score Comparison Benchmark\n\nThe final benchmark after selecting our best model will be to use it and compare it to roughly the 50th percentile score, which looks to be around **4000 points**. A competitor close to this that we should beat is \"Hari Khanal\", with the score of **3985.79966**.\n\n* **GOAL:** Get a lower score than Hari. \n* The lower the score, the better the prediction.\n\n\nYou'll find the link to the scoreboard [here](https:\/\/www.kaggle.com\/c\/walmart-recruiting-store-sales-forecasting\/leaderboard). ","dfe6d194":"\n\n_____\n## Linear Regression","ce1f5fa9":"### Done with data cleaning and processing\n\n- We can now proceed to explore the data","9976b0d6":"### Deal with NaN values ","ecd7c465":"### Correlation heat map","86a87106":"________","bf633907":"______\n# 2. Data Exploration\n\n**Ways we will explore the data:**\n- .describe() function - general overview\n- General plots for the different features\n- Get average weekly sales for each store\/department\n- plot of sales \n- correlation heatmap\n\n![Data](https:\/\/codemyviews-blog-post-images.s3.amazonaws.com\/uploads\/machine-learning.png)","25b9e51f":"# 1. Import Libraries & Set up Data\n\n**The first stage.**\n\n![Kaggle](https:\/\/dox4euoyzny9u.cloudfront.net\/images\/blog\/uploads\/dataprocessinggdpr.jpg)\n\n### What will we will do in this section?\n\n* In this section we will import the nessessary libraries and the data from kaggle.com. \n* Link to dataset **[here](https:\/\/www.kaggle.com\/c\/walmart-recruiting-store-sales-forecasting\/overview).**\n\n* We will then proceed and create dataframes (df's) that match each other, so we can try to answer the final question of; ***Predict the department-wide sales for each store***.\n","38dbf5a1":"### What is in our new *df_all* dataframe?\n\n* **What is included?** All columns from the three original files are now in the same dataframe, being; *'Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size'*\n* **Why do it this way?** We got everything in the same df now to proceed to fix NaN values etc to keep everything at the same place. If we need specific columns, we don't need to go through specific files, we have everything at the same place, which makes it easier to keep in check. ","7fa75f84":"### Merge data into 1 DataFrame\n\nWe do this to get some assistance in assessing the features and the time when we proceed with the modeling. ","23a615d3":"#### Check so that the dfs looks alright.","0c90c6c3":"# 6. Summarising Results & Optimal Model\n\n**The #1 model.**\n\n![Model](https:\/\/png.pngtree.com\/template\/20190214\/ourlarge\/pngtree-aquare-letter-d-and-number-one-logo-vector-image_55711.jpg)","d0b29483":"_______\n## KNN Regressor ","0c5d911d":"### *General comments about the graph* \n\n* **Seasonality - *High-Point***: It seems there are some seasonality to our data, where the end of December seems like a popular time to buy our company's specific products. Hy theory is that this has to do with the holidays and people buy gifts for each other, which seems reasonable. \n* **Seasonality - *Low-Point***: Around the end of january it seems to be a pretty steep dip in the different stores sales both during 2011 and 2012. ","5682f2e6":"### Create CSV","db520014":"### Libraries to import","290d9e23":"##### What do we know about our data so far?\n* So we can see we are working with over 421 570 rows of data in the train dataframe, over 5 columns. This looks like it's this large due to the fact of the 45 different stores. \n\n* Moreover, for our test data we have 115 063 rows of data over 5 columns. \n\n\n* Then we have 8190 rows of data over 12 different columns in the Features dataframe. \n\n* Finally we have 45 rows of data over 3 columns in the stores dataframe. \n\n***Next** we will proceed to address the NaN values we could identify in the .head() formula above.*","1c37f34d":"### General comments about the graph\nThe correlation table and graph indicate a few things: \n* **MarkDowns**: Not that significant in terms of weekly sales\n* **Size**: Highest correlation of all, telling us that having a larger store = more sales. Only thing to notice is that this could imply that having larger stores will automatically increase revenues, but it could be linked to the location and being situated where a lot of people are, and therefore it results in higher revenue. In short, we can apply the old saying \"Correlation is not causation\". But it will b\n* **Type**: Type_A seems to have a positive correlation with the \n* **Department**: Positive correlation with sales as well. ","45552076":"### Create Dummy Variables for Type of Store\n\n* We do this in order to analyse the data, as a non-numerical value will not be processed in our models.","d7bf53ae":"# Structure \n\n1. Import libraries and set up Dataframes\n2. Explore the Data\n3. Create Train & Test Data\n4. Set up Benchmark Model Score\n5. Create & Test models \n6. Summary of Results & Pick winning model for submission\n7. Submission Score & Comparison to Benchmark","e9b6fc68":"## Final thoughts\n\n* This project was incredibly fun and spending over 60 hours on it was without a doubt definitely worth it. \n\nCheers. \n\n![celebrate](https:\/\/www.pblworks.org\/sites\/default\/files\/inline-images\/celebration.jpg)","6e53cd69":"### *General comments about the graphs*\n\n* **Discounts**: We have a few spikes, but not too many to make assumptions of the affects of it. It seems like the discounts are a more recent thing appearing in the end of 2011 with the exception of the potential test of 2011-02 where we find a small bump on all discount levels.\n* **Temperature**: it seems like it is decently seasonal, which sounds reasonable when we talk about temperature on a YoY basis. \n* **Fuel Price**: Seems to have a slight positive trend. \n* **Unemployment**: Seems to have a slight negative trend over this timeframe. ","75a70697":"_______\n# 3. Create Train & Test Data.\n\n**Making the data ready to use in our models.**\n\n![data](https:\/\/www.kaggle.com\/static\/images\/host-home\/host-home-research.png)","a71c9afd":"### ACCUARY \n#### Best performer \n\n* Extra Trees Regressor\n    * MAE:  3576.95\n    * MSE:  54517368.39\n    * RMSE:  7383.58\n\n#### Worst performer\n\n* Ridge Regressor\n    * MAE:  11791.42\n    * MSE:  338620263.35\n    * RMSE:  18401.64\n___\n### MODEL ROBUSTNESS \n#### Best performer \n\n* Extra Trees Regressor\n    * R^2 = 0.85\n\n#### Worst performer\n\n* Linear Regression\n    * R^2 = 0.06\n___\n\n## AND THE OVERALL WINNER IS...\n\n![drumroll](data:image\/jpeg;base64,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)\n\n# --> Extra Trees Regressor! <---","65e155ef":"______\n### Final comments about the models\n\n* **Linear models:** We find that the three linear models were not close to assist in the predictions with a R^2 score of less than 0.1.\n\n_______","2343d9e0":"____\n### Weekly sales plot","d1b42f39":"________\n## Lasso","c653f87d":"______\n# 5. Modeling\n\n***Finding the best model.***\n\n![ML](https:\/\/www.kth.se\/polopoly_fs\/1.862339.1554801965!\/image\/startpuff-avdelning_matematisk%20statistik.jpg)\n\n\n#### We will limit this test to 8 different models:\n\n1. Linear Regression (Linear model)\n2. KNeighborsRegressor (KNN) (neighbors model)\n3. Decision Tree Regressor (Tree model)\n4. Random Forest Regressor (Enemble model)\n5. Extra Trees Regressor (Ensemble model)\n6. XGBoost (gradient boosted decision trees)\n7. Ridge (Linear model)\n8. Lasso (Linear model)\n\n\n*We will start by creating train, test and final datasets.*","99d51e65":"# 7. Submission Score & Comparison to Benchmark\n\n***The final stop.***\n\n![put](https:\/\/miro.medium.com\/max\/1200\/1*joDRjoQDx4nJ4R8R5YkfoA.jpeg)","0cb2241a":"________\n## Ridge","703da229":"### Data to import","1c674138":"### Print the predictions for the final submission","8bb7a4d5":"# Demand Forecasting Predictions \n\n***Let's get started.***\n\n![Walmart](https:\/\/nypost.com\/wp-content\/uploads\/sites\/2\/2019\/08\/walmart-tesla.jpg?quality=80&strip=all&w=618&h=410&crop=1)\n\n## Info\n\n* This notebook is for the capstone project for Udacitys Nanodegree - **Machine Learning Engineer**. More information about the project will be provided below. \n\n* To test the prediciton score, we will submit the predictions to the Kaggle competition by Walmart which the data originated from, see link [here](https:\/\/www.kaggle.com\/c\/walmart-recruiting-store-sales-forecasting\/overview). "}}