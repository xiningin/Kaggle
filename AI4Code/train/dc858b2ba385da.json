{"cell_type":{"082a2aab":"code","59dc63a6":"code","1d8a2a27":"code","68543d67":"code","71f2e3f6":"code","3f747ddf":"code","ba0a1620":"code","19931f2e":"code","12382c63":"code","e637762d":"code","c5a21d9e":"code","c05f5839":"code","34d0f50b":"code","9a25c0c7":"code","69b3b390":"code","cbc946a3":"code","7604a267":"code","792e09b3":"code","23ead5c3":"code","a2bc638b":"code","cc30e9d4":"code","ad027e54":"code","c1f64177":"code","91b2e677":"code","5f790764":"code","0db2e5a6":"code","2358f420":"code","b4a32ba2":"code","b093d619":"code","4ff05ada":"code","bad22783":"code","f113e461":"code","9349ee5f":"code","6ae110b9":"code","37ff49b0":"code","16732653":"code","45f92a40":"code","f1d81af6":"code","fc81d3ba":"code","62aa9f30":"code","94aca767":"code","7ba34380":"markdown","71d15939":"markdown","ba46c02a":"markdown","6b49a62b":"markdown","64de3916":"markdown","f0275b81":"markdown","0a20151a":"markdown","0fbdc890":"markdown","3133e347":"markdown","4fd2f238":"markdown","ec9333f9":"markdown","f63b465a":"markdown","5e6c55c1":"markdown","d8f5af0a":"markdown","fda04539":"markdown","b46b63fb":"markdown","77ffb588":"markdown","513ede96":"markdown"},"source":{"082a2aab":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nimport tqdm\nfrom tqdm.auto import tqdm as tqdmp\ntqdmp.pandas()\n\n# NLP\nimport unicodedata, string, re, os\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter, defaultdict\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.dates as mdates\n\n# ignoring warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n# Some settings for visualizations\nsns.set(rc={'axes.facecolor':'black', 'figure.facecolor':'black', \n            'xtick.color': 'white', 'ytick.color': 'white', \n            'grid.color': 'white', 'axes.labelcolor': 'white',\n            'figure.dpi': 150, 'grid.linestyle': ':', 'grid.alpha': .6,\n            'font.family': 'fantasy'})\n\n# RAPIDS\nimport tensorflow as tf\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nprint('RAPIDS',cuml.__version__)","59dc63a6":"all_data = pd.read_csv('..\/input\/reddit-data-science-posts\/reddit_database.csv')\nall_data['created_date'] = all_data['created_date'].astype('datetime64')\nall_data.head()","1d8a2a27":"all_data.info()","68543d67":"all_data.describe().style.background_gradient(cmap = 'inferno')","71f2e3f6":"plt.title('Subreddits (sum of posts)', color = 'white', size = 15)\nsub_redd = all_data.groupby(all_data[\"subreddit\"]).created_date.count().sort_values(ascending = False)\nsub_redd.plot(kind = \"bar\", edgecolor = '#ff4501', color = '#ff4501')\nplt.xlabel('')\nplt.show()","3f747ddf":"plt.title('Posts Distribution', color = 'white', size = 15)\nyear_d = all_data.groupby(all_data[\"created_date\"].dt.year).created_date.count()\nyear_d.plot(kind = \"bar\", edgecolor = '#ff4501', color = '#ff4501')\nplt.xlabel('Posts per year')\nplt.xticks(rotation = 45)\nplt.show()","ba0a1620":"plt.title('Posts Distribution', color = 'white', size = 15)\nmonth_d = all_data.groupby(all_data[\"created_date\"].dt.month).created_date.count()\nmonth_d.plot(kind = \"bar\", edgecolor = '#ff4501', color = '#ff4501')\nplt.xlabel('Year activity (month)')\nplt.xticks(rotation = 'horizontal')\nplt.show()","19931f2e":"plt.title('Posts Distribution', color = 'white', size = 15)\nday_d = all_data.groupby(all_data[\"created_date\"].dt.day).created_date.count()\nday_d.plot(kind = \"bar\", edgecolor = '#ff4501', color = '#ff4501')\nplt.xlabel('Month activity (day)')\nplt.show()","12382c63":"plt.title('Posts Distribution', color = 'white', size = 15)\nhour_d = all_data.groupby(all_data[\"created_date\"].dt.hour).created_date.count()\nhour_d.plot(kind = \"bar\", edgecolor = '#ff4501', color = '#ff4501')\nplt.xlabel('Daily activity (hour)')\nplt.show()","e637762d":"plt.title('Posts score (density)', color = 'white', size = 15)\nsns.kdeplot(all_data['score'], color = '#ff4501', shade = True, \n            alpha = .8, legend = None)\nplt.xlabel('')\nplt.show()","c5a21d9e":"plt.title('Posts number of comments (density)', color = 'white', size = 15)\nsns.kdeplot(all_data['num_comments'], color = '#ff4501', shade = True, \n            alpha = .8, legend = None)\nplt.xlabel('')\nplt.show()","c05f5839":"plt.title('Posts score and comments', color = 'white', size = 15)\nsns.scatterplot(all_data['score'], all_data['num_comments'], \n                color = '#ff4501', alpha = .8, legend = None)\nplt.xlabel('Score')\nplt.ylabel('Number of Comments')\nplt.show()","34d0f50b":"plt.title('Authors were registered (year)', color = 'white', size = 15)\nall_data['author_created_utc'].dropna() \\\n    .apply(lambda x: datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S')) \\\n    .astype('datetime64').dt.year.value_counts(sort = False) \\\n    .plot(kind = \"bar\", edgecolor = '#ff4501', color = '#ff4501')\nplt.xticks(rotation = 45)\nplt.show()","9a25c0c7":"# Necessary functions\ndef text_cleaner(text):\n    \"\"\"\n    Function for clearing text data from unnecessary characters.\n    \"\"\"\n    text = text.lower()\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    text = re.sub('\\[.*?\\]', ' ', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', ' ', text)\n    text = re.sub('\\r', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ndef show_wordcloud(data, stop, mask = None, title = None, color = 'black'):\n    \"\"\"\n    Function for creating wordclouds (with or without mask)\n    \"\"\"\n    from wordcloud import WordCloud, ImageColorGenerator\n    wordcloud = WordCloud(background_color = color,\n                         stopwords = stop,\n                         mask = mask,\n                         max_words = 100,\n                         scale = 3,\n                         width = 4000, \n                         height = 2000,\n                         collocations = False,\n                         random_state = 1)\n    \n    wordcloud = wordcloud.generate(data)\n    \n    plt.figure(1, figsize = (16, 8), dpi = 300)\n    plt.title(title, size = 15)\n    plt.axis('off')\n    if mask is None:\n        plt.imshow(wordcloud, interpolation = \"bilinear\")\n        plt.show()\n    else:\n        image_colors = ImageColorGenerator(mask)\n        plt.imshow(wordcloud.recolor(color_func = image_colors), \n                   interpolation = \"bilinear\")\n        plt.show()\n        \ndef get_top_ngram(corpus, n = 2):\n    \"\"\"\n    Function for creating ngrams of words\n    Input: word corpus and n - number of words in grams\n    \"\"\"\n    vec = CountVectorizer(stop_words = stop, ngram_range = (n, n)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return words_freq","69b3b390":"title_cleaned = all_data['title'].progress_apply(lambda x: text_cleaner(x))\ntitle_cleaned","cbc946a3":"title_length = title_cleaned.str.len()\n\nplt.title('Title length', size = 15, color = 'white')\nsns.distplot(title_length, kde = False, color = '#ff4501', \n             hist_kws = dict(alpha = 1))\nplt.xlabel('Length of title (symbols)')\nplt.show()","7604a267":"title_words = title_cleaned.str.split().map(lambda x: len(x))\n\nplt.title('Title words', size = 15, color = 'white')\nsns.distplot(title_words, kde = False, color = '#ff4501', \n             hist_kws = dict(alpha = 1))\nplt.xlabel('Length of title (words)')\nplt.show()","792e09b3":"title_word_len = title_cleaned.str.split().apply(lambda x: [len(i) for i in x]).map(lambda x: np.mean(x))\n\nplt.title('Title words length', size = 15, color = 'white')\nsns.distplot(title_word_len, kde = False, color = '#ff4501', \n             hist_kws = dict(alpha = 1))\nplt.xlabel('Mean word length in title (symbols)')\nplt.show()","23ead5c3":"words = title_cleaned.str.split().values.tolist()\ntitle_corpus = [word for i in words for word in i]\n\ntitle_counter = Counter(title_corpus)\ntitle_most = title_counter.most_common()\n\nstop = set(stopwords.words('english'))\n\ntitle_top_words, title_top_words_count = [], []\nfor word, count in title_most[:100]:\n    if word not in stop:\n        title_top_words.append(word)\n        title_top_words_count.append(count)","a2bc638b":"plt.title('TOP-10 title words', color = 'white', size = 15)\nsns.barplot(y = title_top_words[:10], x = title_top_words_count[:10], \n            edgecolor = 'black', color = 'red')\nplt.show()","cc30e9d4":"title_word_string = ' '.join(title_corpus)\nshow_wordcloud(title_word_string, stop)","ad027e54":"# Data for bigrams\ntop_title_bigrams = get_top_ngram(title_cleaned, 2)[:10]\ntitle_x, title_y = map(list, zip(*top_title_bigrams))","c1f64177":"plt.title('TOP-10 title bigrams', color = 'white', size = 15)\nsns.barplot(x = title_y, y = title_x, edgecolor = 'black', color = 'red')\nplt.show()","91b2e677":"# Data for bigrams\ntop_title_trigrams = get_top_ngram(title_cleaned, 3)[:10]\ntitle_x, title_y = map(list, zip(*top_title_trigrams))","5f790764":"plt.title('TOP-10 title trigrams', color = 'white', size = 15)\nsns.barplot(x = title_y, y = title_x, edgecolor = 'black', color = 'red')\nplt.show()","0db2e5a6":"post_cleaned = all_data['post'].dropna().progress_apply(lambda x: text_cleaner(x))","2358f420":"post_length = post_cleaned.str.len()\n\nplt.title('Post length', size = 15, color = 'white')\nsns.distplot(post_length, kde = False, color = '#ff4501', \n             hist_kws = dict(alpha = 1))\nplt.xlabel('Length of posts (symbols)')\nplt.show()","b4a32ba2":"post_words = post_cleaned.str.split().map(lambda x: len(x))\n\nplt.title('Post words', size = 15, color = 'white')\nsns.distplot(post_words, kde = False, color = '#ff4501', \n             hist_kws = dict(alpha = 1))\nplt.xlabel('Length of posts (words)')\nplt.show()","b093d619":"post_word_len = post_cleaned.str.split().apply(lambda x: [len(i) for i in x]).map(lambda x: np.mean(x))\n\nplt.title('Posts words length', size = 15, color = 'white')\nsns.distplot(post_word_len, kde = False, color = '#ff4501', \n             hist_kws = dict(alpha = 1))\nplt.xlabel('Mean word length in posts (symbols)')\nplt.show()","4ff05ada":"words = post_cleaned.str.split().values.tolist()\npost_corpus = [word for i in words for word in i]\n\npost_counter = Counter(post_corpus)\npost_most = post_counter.most_common()\n\nstop = set(stopwords.words('english'))\n\npost_top_words, post_top_words_count = [], []\nfor word, count in post_most[:100]:\n    if word not in stop:\n        post_top_words.append(word)\n        post_top_words_count.append(count)","bad22783":"plt.title('TOP-10 post words', color = 'white', size = 15)\nsns.barplot(y = post_top_words[:10], x = post_top_words_count[:10], \n            edgecolor = 'black', color = 'red')\nplt.xticks(rotation = 30)\nplt.show()","f113e461":"post_word_string = ' '.join(post_corpus)\nshow_wordcloud(post_word_string, stop)","9349ee5f":"# Data for bigrams\ntop_post_bigrams = get_top_ngram(post_cleaned, 2)[:10]\ntitle_x, title_y = map(list, zip(*top_post_bigrams))","6ae110b9":"plt.title('TOP-10 post bigrams', color = 'white', size = 15)\nsns.barplot(x = title_y, y = title_x, edgecolor = 'black', color = 'red')\nplt.show()","37ff49b0":"# Data for bigrams\ntop_post_trigrams = get_top_ngram(post_cleaned, 3)[:10]\ntitle_x, title_y = map(list, zip(*top_post_trigrams))","16732653":"plt.title('TOP-10 post trigrams', color = 'white', size = 15)\nsns.barplot(x = title_y, y = title_x, edgecolor = 'black', color = 'red')\nplt.show()","45f92a40":"LIMIT = 1\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    print(e)\nprint('Restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('so RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","f1d81af6":"vectorizer = TfidfVectorizer(stop_words = 'english', binary = True)\ntitle_embeddings = vectorizer.fit_transform(cudf.Series(all_data.loc[:49999, 'title'])).toarray()\nprint('Title embeddings shape:', title_embeddings.shape)","fc81d3ba":"KNN = 10\nmodel = NearestNeighbors(n_neighbors = KNN)\nmodel.fit(title_embeddings)\ndistances, indices = model.kneighbors(title_embeddings)","62aa9f30":"def knn_viz(rows, k_neighbors = 15):\n    for i in rows:\n        plt.figure(figsize = (20, 3))\n        plt.plot(np.arange(k_neighbors), \n                 cupy.asnumpy(distances[i, :k_neighbors]), 'o--', \n                 color = 'red', linewidth = 3, markersize = 12)\n        plt.title('Text Distance From Row %i to Other Rows'%i, \n                  size = 16, color = 'white')\n        plt.ylabel('Distance to Row %i'%i, size = 14)\n        plt.xlabel('Index Sorted by Distance to Row %i'%i, size = 14)\n        plt.show()\n\n        print(all_data.loc[cupy.asnumpy(indices[i, :k_neighbors]), 'title'])","94aca767":"knn_viz(range(5), 5)","7ba34380":"<h1 style='color:white; background:black; border:0'><center>WORK IN PROGRESS...<\/center><\/h1>\n\n# If you are interested, see [this dataset (Reddit Data Science Posts)](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/reddit-data-science-posts).","71d15939":"The r\/MachineLearning subreddit has the largest number of posts. The smallest is r\/kaggle (it is necessary to promote the kaggle community on Reddit!).","ba46c02a":"<h1 style='color:white; background:black; border:0'><center>How is Data Science on Reddit?<\/center><\/h1>\n\n![](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1226967\/2048152\/b7b21a6c2113257ec9ddf7221abfed1a\/dataset-cover.png?t=2021-03-23-12-43-24)\n\n**Data Science Community on Reddit is growing every year. Today, the network is a platform for many professionals and enthusiasts who share valuable materials and experiences. Quite an interesting task is the analysis of posts dedicated to Data Science:**\n- **finding interesting topics**, \n- **studying changes in trends over time**, \n- **predicting the potential popularity of posts on Reddit by its title and text**, etc.\n\n**Over time, I will increase the size of this dataset by adding posts from other subreddits, so that the quality of the analysis will improve.**\n\n<h3 style='color:white; background:black; border:0'><center>This dataset includes over 500,000 posts from 19 Date Science subreddits:<\/center><\/h3>\n\n[r\/analytics](https:\/\/www.reddit.com\/r\/analytics\/), [r\/deeplearning](https:\/\/www.reddit.com\/r\/deeplearning\/), [r\/datascience](https:\/\/www.reddit.com\/r\/datascience\/), [r\/datasets](https:\/\/www.reddit.com\/r\/datasets\/), [r\/kaggle](https:\/\/www.reddit.com\/r\/kaggle\/), [r\/learnmachinelearning](https:\/\/www.reddit.com\/r\/learnmachinelearning\/), [r\/MachineLearning](https:\/\/www.reddit.com\/r\/MachineLearning\/), [r\/statistics](https:\/\/www.reddit.com\/r\/statistics\/), [r\/artificial](https:\/\/www.reddit.com\/r\/artificial\/), [r\/AskStatistics](https:\/\/www.reddit.com\/r\/AskStatistics\/), [r\/computerscience](https:\/\/www.reddit.com\/r\/computerscience\/), [r\/computervision](https:\/\/www.reddit.com\/r\/computervision\/), [r\/dataanalysis](https:\/\/www.reddit.com\/r\/dataanalysis\/), [r\/dataengineering](https:\/\/www.reddit.com\/r\/dataengineering\/), [r\/DataScienceJobs](https:\/\/www.reddit.com\/r\/DataScienceJobs\/), [r\/datascienceproject](https:\/\/www.reddit.com\/r\/datascienceproject\/), [r\/data](https:\/\/www.reddit.com\/r\/data\/), [r\/MLQuestions](https:\/\/www.reddit.com\/r\/MLQuestions\/), [r\/rstats](https:\/\/www.reddit.com\/r\/rstats\/)\n\nData were collected from [pushshift.io API](https:\/\/pushshift.io) (maintained by Jason Baumgartner).\n\n<h3 style='color:white; background:black; border:0'><center>19 datasets (one per one subreddit) include the following data:<\/center><\/h3>\n\n`#` - row index;\n`created_date` - post publication date;\n`created_timestamp` - post publication timestamp;\n`subreddit` - subreddit name;\n`title` - post title;\n`id` - unique operation id;\n`author` - post author;\n`author_created_utc` - author registration date;\n`full_link` - hyperlink to post;\n`score` - ratio of likes and dislikes;\n`num_comments` - the number of comments;\n`num_crossposts` - the number of crossposts; \n`subreddit_subscribers` - the number of subreddit subscribers at the time the post was published;\n`post` - post text.\n\n# If you are interested, see [this dataset (Reddit Data Science Posts)](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/reddit-data-science-posts).\nFeel free to leave your comments on this notebook. I will try to make dataset better and much larger.","6b49a62b":"Then we create a TfidfVectorizer object and transform the first 50 000 titles into a vector representation (no longer allowed by the notebook limitation).","64de3916":"# RAPIDS and post title\n\nNow let's formulate, and immediately solve our first problem. Let's say we want to analyze the titles of all our posts and find similar ones. The task seems to be quite simple: you just need to turn titles into a vector representation and find the distance using KNN (K-Nearest Neighbors). The difficulty arises with the size of the data. Half a million titles (some of which are really big) are a lot for KNN. Therefore, we use RAPIDS - open-source software libraries that give you the freedom to execute end-to-end data science and analytics pipelines entirely on GPUs.\n\nGreat, let's try. First, we will set some parameters for using RAPIDS GPU.","f0275b81":"The ratio between post rating and the number of comments is not so linear. There are posts with a huge number of likes, but almost no comments, and vice versa.","0a20151a":"Now it's time to use the K nearest neighbors method on our embeddings. For each title, we will find 10 nearest neighbors.","0fbdc890":"**The deserved king of all posts is the word \"data\".**","3133e347":"Visual confirmation that the Data Science community on Reddit is growing.","4fd2f238":"Some of the titles are really long! (almost 60 words and 300 characters).","ec9333f9":"### Let's look at the posts data.","f63b465a":"Now let's visualize the nearest neighbors for the first five titles. The graphs show the distance between neighbors (the shorter the distance, the more similar the titles are). Below each graph are the titles themselves.\n\nWe can see that some of the titles are quite unique. And some, such as 0 and 2, have quite similar analogs, or even exact copies.","5e6c55c1":"In total, we have 527 646 posts. Of these, 262 567 have text body. The half is empty because it was either originally without text, or has been removed from the platform.","d8f5af0a":"There are no patterns in activity between months, and days of the month. Activity within a day is logically higher in the evening and at night.","fda04539":"### Load data","b46b63fb":"As expected, the titles are dominated by the words \"data\", \"machine\", \"learning\", \"ai\", \"science\", \"deep\", \"model\" ...\n\nIt is also interesting to look at the bigrams and trigrams of titles.","77ffb588":"# Let's take a look at some interesting data","513ede96":"In the dynamics of authors' registration date, there is a regular increase. True, something has happened in recent years. This is most likely due to the data provided by \"pushshift.io\".\n\n**Let's take a closer look at text data.**"}}