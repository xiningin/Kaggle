{"cell_type":{"fe6e8070":"code","ef33fa96":"code","1c747258":"code","45a72426":"code","7c4c8f16":"code","c435b954":"code","bbfdd7cf":"code","ee1ee2b4":"code","8a0b60d0":"code","b15b96ac":"code","4c0a1b15":"code","967ad93e":"code","a336b9f9":"code","7c371c9b":"code","2ca6b23c":"code","6edf23dd":"code","31698da4":"code","4a253753":"code","06f99dae":"code","4efece9d":"code","b5d4de2f":"code","c58ee922":"code","de752e94":"code","e5844bfb":"code","4281bb4e":"code","5aa83828":"code","e87712e8":"code","ca2e9bb0":"code","bc397d3a":"code","bca0d02d":"code","14d00b6a":"code","7d8c0ec1":"code","21864e9f":"code","eaf56bf1":"code","6955ebf3":"code","e3fab197":"code","a59fa6ef":"code","fbcb2477":"code","7a7d9d92":"code","ea259a79":"code","aeb673dc":"code","d41ca129":"code","dbe218ad":"code","dc31c943":"code","f128f942":"code","15bba83b":"code","03136a6e":"code","a11f8f35":"code","753bc06f":"code","7585add7":"code","b7f73d63":"code","629f05e1":"code","b828fd41":"code","95c265ef":"code","2855c12d":"code","a13fb526":"code","e1539865":"code","76162855":"code","374fe130":"code","99feb507":"code","71284971":"code","07a60457":"code","0ddabd06":"code","790c42f4":"code","5ee06ed2":"code","0c096775":"code","71039ff1":"code","c55df5b8":"code","8db540ab":"code","0d021c24":"code","1da4503e":"code","f0ca2428":"code","b6d77020":"code","622aa161":"code","01f9b83e":"code","c731b273":"code","a71aa04b":"code","4d600ed6":"code","a08a75a1":"code","51215cb2":"code","424ab9c9":"code","ffbb4f0e":"code","73aec278":"code","65d37a91":"markdown","ddaf6852":"markdown","efef8e55":"markdown","cb41d0f5":"markdown","96810a06":"markdown","a8caa995":"markdown","3cebafc5":"markdown","92d36895":"markdown","3ee0fd88":"markdown","94da7a23":"markdown","ea0ceaa6":"markdown","7ebfc819":"markdown","4fcc4c96":"markdown","28ac01c1":"markdown","0802d2ee":"markdown","aab7fbf2":"markdown","d9c3ccb1":"markdown","2b5134e1":"markdown","1b8e3fa2":"markdown","38cc56e4":"markdown","9ce01de4":"markdown","00efac75":"markdown","0a021ac5":"markdown","d4f51d8a":"markdown","bae81e46":"markdown","42f6e9f2":"markdown","b5d9df50":"markdown","b6818b21":"markdown","017fe555":"markdown","fd80ac0f":"markdown","f5efb02c":"markdown","05a372e3":"markdown","117399ca":"markdown","cd714803":"markdown","da0fb2e7":"markdown","851ec86c":"markdown","2cf4bad2":"markdown","a57bce4e":"markdown","0f4641ee":"markdown","2cc33b2f":"markdown","37fcc1cf":"markdown","4b18a92b":"markdown","8cccd514":"markdown","00018a2e":"markdown","985be29f":"markdown","50528cc1":"markdown","8480d6e2":"markdown","008e4ba5":"markdown","f7a86e8a":"markdown","2ab2c39e":"markdown","4602c4d5":"markdown","d9b163fa":"markdown","ac29d716":"markdown","ec35f27d":"markdown","2afc6584":"markdown","7004311d":"markdown","080d6857":"markdown","e8ab766a":"markdown"},"source":{"fe6e8070":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef33fa96":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import KNNImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom datetime import datetime\n\nimport xgboost as xgb\n","1c747258":"train_data = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntest_data = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","45a72426":"train_data.head()","7c4c8f16":"print('Train shape:', train_data.shape)\nprint('Test shape:', test_data.shape)","c435b954":"train_data.info()","bbfdd7cf":"def perc_missing(df):\n    print('***  Count Missing Values ***')\n    print (train_data.isnull().sum().sort_values(ascending=False))\n    print('\\n---------------\\n')\n    print('*** Percentage Missing Values ***')\n    print ((df.isnull().sum() \/ len(df)*100).sort_values(ascending=False))","ee1ee2b4":"perc_missing(train_data)","8a0b60d0":"# columns that has at least one null value\ntrain_data.columns[train_data.isnull().any()].tolist()","b15b96ac":"enrollee_id = test_data['enrollee_id']\ntarget = train_data['target']","4c0a1b15":"train_data.city.value_counts().sort_values()","967ad93e":"train_data.city_development_index.hist()","a336b9f9":"train_data.city_development_index.isnull().sum()","7c371c9b":"train_data.gender.value_counts().sort_values()","2ca6b23c":"plt.figure(figsize=(8,5))\npatches, texts, autotexts = plt.pie(x=train_data.gender.value_counts().tolist(), labels=train_data.gender.value_counts().index, autopct='%1.2f%%')\n\n#make percent texts bigger\nplt.setp(autotexts, fontsize=14)\n\n#make label texts bigger\nplt.setp(texts, fontsize=14)","6edf23dd":"train_data.gender.isnull().sum()","31698da4":"map_gender_lgbm = {'Other': 0, 'Female':1, 'Male':2}\nmap_gender = {'null': 0, 'Other': 1, 'Female':2, 'Male':3}","4a253753":"train_data.relevent_experience.value_counts()","06f99dae":"data = train_data.relevent_experience.value_counts().tolist()\nlabels = train_data.relevent_experience.value_counts().index\n\npatches, texts, autotexts = plt.pie(x=data, labels=labels, autopct='%1.2f%%')\n\n#make percent texts bigger\nplt.setp(autotexts, fontsize=14)\n\n#make label texts bigger\nplt.setp(texts, fontsize=14)","4efece9d":"train_data.relevent_experience.isnull().sum()","b5d4de2f":"map_relevent_experience_lgbm = {'No relevent experience': 0, 'Has relevent experience': 1}\nmap_relevent_experience = {'null':0, 'No relevent experience': 1, 'Has relevent experience': 2}","c58ee922":"train_data.enrolled_university.value_counts()","de752e94":"data = train_data.enrolled_university.value_counts().tolist()\nlabels = train_data.enrolled_university.value_counts().index\n\npatches, texts, autotexts = plt.pie(x=data, labels=labels, autopct='%1.2f%%')\n\n#make percent texts bigger\nplt.setp(autotexts, fontsize=14)\n\n#make label texts bigger\nplt.setp(texts, fontsize=14)","e5844bfb":"train_data.enrolled_university.isnull().sum()","4281bb4e":"map_enrolled_university_lgbm = {'no_enrollment': 0, 'Part time course': 1, 'Full time course' : 2}\nmap_enrolled_university = {'null': 0, 'no_enrollment': 1, 'Part time course': 2, 'Full time course' : 3}","5aa83828":"train_data.education_level.value_counts()","e87712e8":"data = train_data.education_level.value_counts().tolist()\nlabels = train_data.education_level.value_counts().index\n\npatches, texts, autotexts = plt.pie(x=data, labels=labels, autopct='%1.2f%%')\n\n#make percent texts bigger\nplt.setp(autotexts, fontsize=14)\n\n#make label texts bigger\nplt.setp(texts, fontsize=14)","ca2e9bb0":"train_data.education_level.isnull().sum()","bc397d3a":"map_education_level_lgbm = {'Primary School':0, 'High School':1, 'Graduate': 2, 'Masters':3, 'Phd':4}\nmap_education_level = {'null': 0, 'Primary School':1, 'High School':2, 'Graduate': 3, 'Masters':4, 'Phd':5}","bca0d02d":"train_data.major_discipline.value_counts()","14d00b6a":"train_data.major_discipline.isnull().sum()","7d8c0ec1":"map_major_discipline_lgbm = {'No Major':0, 'Arts':1, 'Business Degree':2, 'Other': 3, 'Humanities':4, 'STEM':5}\nmap_major_discipline = {'null': 0, 'No Major':1, 'Arts':2, 'Business Degree':3, 'Other': 4, 'Humanities':5, 'STEM':6}","21864e9f":"train_data.experience.value_counts().sort_index()","eaf56bf1":"plt.figure(figsize=(10,5))\nsns.countplot(train_data.experience, order=['<1','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','>20'])","6955ebf3":"def experience_replace(exp, model):\n    if exp == '>20':\n        return 21\n    elif exp == '<1':\n        return 0\n    elif exp is not np.NaN:\n        return int(exp)\n    elif exp is np.NaN and model != 'LGBM':\n        return 22\n    else:\n        return exp","e3fab197":"train_data.company_size.value_counts().sort_values()","a59fa6ef":"# number of instances based on company_size ordered by company_size\nplt.figure(figsize=(10,5))\nsns.countplot(train_data.company_size, order=['<10','10\/49','50-99','100-500','500-999','1000-4999','5000-9999','10000+'])","fbcb2477":"map_company_size_lgbm = {'<10': 0,'10\/49': 1,'50-99': 2,'100-500': 3,'500-999': 4,'1000-4999':5, '5000-9999':6, '10000+':7}\nmap_company_size = {'null': 0, '<10': 1,'10\/49': 2,'50-99': 3,'100-500': 4,'500-999': 5,'1000-4999':6, '5000-9999':7, '10000+':8}","7a7d9d92":"train_data.company_type.value_counts()","ea259a79":"plt.figure(figsize=(12,8))\nsns.countplot(train_data.company_type)","aeb673dc":"map_company_type_lgbm = {\n    'Pvt Ltd': 5,\n    'Funded Startup':4,\n    'Early Stage Startup':3,\n    'Other':2,\n    'Public Sector':1,\n    'NGO':0\n}\n\nmap_company_type = {\n    'Pvt Ltd': 6,\n    'Funded Startup':5,\n    'Early Stage Startup':4,\n    'Other':3,\n    'Public Sector':2,\n    'NGO':1,\n    'null':0\n}","d41ca129":"train_data.last_new_job.value_counts()","dbe218ad":"# function for replacing the values, and converting them to integer\ndef lastnewjob_replace(lnj, model):\n    if lnj == '>4':\n        return 5\n    elif lnj == 'never':\n        return 0\n    elif lnj is not np.NaN:\n        return int(lnj)\n    elif lnj is np.NaN and model != 'LGBM':\n        return 6\n    else:\n        return lnj","dc31c943":"train_data.training_hours.hist()","f128f942":"train_data.training_hours.isnull().sum()","15bba83b":"train_data.target.hist()","03136a6e":"all_data = pd.concat([train_data.drop(['target','enrollee_id'], axis=1), test_data.drop(['enrollee_id'], axis=1)], axis=0)\nall_data","a11f8f35":"def convert_dataset(df_data, model):\n    \n    # do not change df_data\n    # converting will be done on returned dataset\n    temp_data = df_data.copy()\n    \n    le = LabelEncoder()\n    temp_data.city = le.fit_transform(temp_data.city)\n    temp_data.last_new_job = temp_data.last_new_job.apply(lastnewjob_replace, args=(model,))\n    temp_data.experience = temp_data.experience.apply(experience_replace, args=(model,))\n    \n    # convert categorical values, left missing values as null\n    if model == 'LGBM':\n        temp_data.gender = temp_data.gender.map(map_gender_lgbm)\n        temp_data.relevent_experience = temp_data.relevent_experience.map(map_relevent_experience_lgbm)\n        temp_data.enrolled_university = temp_data.enrolled_university.map(map_enrolled_university_lgbm)\n        temp_data.education_level = temp_data.education_level.map(map_education_level_lgbm)\n        temp_data.major_discipline = temp_data.major_discipline.map(map_major_discipline_lgbm)\n        temp_data.company_size = temp_data.company_size.map(map_company_size_lgbm)\n        temp_data.company_type = temp_data.company_type.map(map_company_type_lgbm)\n        \n        \n    # convert categorical values, encode missing values    \n    else:\n        # first fill NaN values with a string 'null', mapper are handling with 'null' string\n        temp_data.fillna('null', inplace=True)\n        temp_data.gender = temp_data.gender.map(map_gender)\n        temp_data.relevent_experience = temp_data.relevent_experience.map(map_relevent_experience)\n        temp_data.enrolled_university = temp_data.enrolled_university.map(map_enrolled_university)\n        temp_data.education_level = temp_data.education_level.map(map_education_level)\n        temp_data.major_discipline = temp_data.major_discipline.map(map_major_discipline)\n        temp_data.company_size = temp_data.company_size.map(map_company_size)\n        temp_data.company_type = temp_data.company_type.map(map_company_type)\n\n\n        \n    return temp_data","753bc06f":"convert_dataset(all_data, 'LGBM')","7585add7":"all_data = convert_dataset(all_data, 'LGBM')\nlgbm_train_data = all_data.iloc[0:train_data.shape[0], :]\nlgbm_test_data = all_data.iloc[train_data.shape[0]:, :]\n\n# add the id column to test data\nlgbm_test_data.loc[:,'enrollee_id'] = enrollee_id.values\n# add the target column to train data\nlgbm_train_data.loc[:,'target'] = target.values","b7f73d63":"plt.figure(figsize=(12,10))\nsns.heatmap(lgbm_train_data.corr(), annot=True)","629f05e1":"all_data = pd.concat([train_data.drop(['target','enrollee_id'], axis=1), test_data.drop(['enrollee_id'], axis=1)], axis=0)\nall_data = convert_dataset(all_data, 'Others')","b828fd41":"others_train_data = all_data.iloc[0:train_data.shape[0], :]\nothers_test_data = all_data.iloc[train_data.shape[0]:, :]\n\n# add the id column to test data\nothers_test_data.loc[:,'enrollee_id'] = enrollee_id.values\n# add the target column to train data\nothers_train_data.loc[:,'target'] = target.values","95c265ef":"others_train_data","2855c12d":"others_test_data","a13fb526":"# Get the original data, merge it.\nall_data = pd.concat([train_data.drop(['target','enrollee_id'], axis=1), test_data.drop(['enrollee_id'], axis=1)], axis=0)\n# convert it\nall_data = convert_dataset(all_data, 'LGBM')","e1539865":"missing_columns = all_data.columns[all_data.isnull().any()].tolist()\nmissing_columns","76162855":"knn_imputer = KNNImputer(n_neighbors=3)\n\narr = knn_imputer.fit_transform(all_data.loc[:,all_data.columns != 'target'])\nall_data_knn_imputed = pd.DataFrame(arr, columns = all_data.loc[:,all_data.columns != 'target'].columns)","374fe130":"all_data_knn_imputed[missing_columns] = np.round(all_data_knn_imputed[missing_columns])","99feb507":"all_data_knn_imputed","71284971":"train_data_knn_imputed = all_data_knn_imputed.iloc[0:train_data.shape[0], :].copy()\ntest_data_knn_imputed = all_data_knn_imputed.iloc[train_data.shape[0]:, :].copy()\n\n# add the id column to test data\ntest_data_knn_imputed.loc[:,'enrollee_id'] = enrollee_id.values\n# add the target column to train data\ntrain_data_knn_imputed.loc[:,'target'] = target.values","07a60457":"train_data_knn_imputed","0ddabd06":"test_data_knn_imputed","790c42f4":"auc_scores = []\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\nmodel_list = []\nimputer_list = []\ntimestamp = []","5ee06ed2":"def print_report(estimator,X,y, model, imputer):\n    \n    print('\\n\\n','*'*15,'REPORT','*'*15,'\\n')\n    print('Model: ', model)\n    print('Imputer:', imputer)\n    \n    if model == 'LGBM':\n        auc = roc_auc_score(y, estimator.predict(X))\n        y_predict = [1 if x > 0.5 else 0 for x in estimator.predict(X) ]\n        cmatrix = confusion_matrix(y, y_predict)\n    else:\n        auc = roc_auc_score(y, estimator.predict_proba(X)[:,1])\n        y_predict= estimator.predict(X)\n        \n        \n    precision, recall, fscore, support = precision_recall_fscore_support(y, y_predict)\n    accuracy = accuracy_score(y, y_predict)\n    \n    \n    #print\n    print('AUC: ', auc)\n    print('*'*40)\n    print(classification_report(y, y_predict))\n    print('*'*40)\n    \n    if model == 'LGBM':\n        sns.heatmap(cmatrix, annot=True, fmt='d', cmap='Blues')\n    else:\n        plot_confusion_matrix(estimator,X, y, values_format='d')\n    \n    \n    #save\n    auc_scores.append(auc)\n    precision_scores.append(precision[1])\n    f1_scores.append(fscore[1])\n    recall_scores.append(recall[1])\n    accuracy_scores.append(accuracy)\n    model_list.append(model)\n    imputer_list.append(imputer)\n    timestamp.append(datetime.now())","0c096775":"def Logistic_Regression(df, imputer):\n    \n    # one hot encoding for Nominal categorical variables\n    train_data_ohe = pd.get_dummies(df, columns=['gender'], prefix='G', prefix_sep='_')\n    train_data_ohe = pd.get_dummies(df, columns=['enrolled_university'], prefix='EU', prefix_sep='_')\n    train_data_ohe = pd.get_dummies(df, columns=['major_discipline'], prefix='MD', prefix_sep='_')\n    \n    # Grid Search\n    X = train_data_ohe.drop(['target'],axis=1)\n    y = train_data_ohe['target']\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, stratify= y)\n\n    lr = LogisticRegression(max_iter=2000)\n\n    params={'C':np.logspace( -10, 1, 15)}\n\n    gs_lr = GridSearchCV(lr, param_grid = params, scoring=('roc_auc'), cv=5, n_jobs=-1)\n    gs_lr.fit(X_train,y_train)\n    \n    print('Best parameters: ', gs_lr.best_params_)\n    print('Best score: ', gs_lr.best_score_)\n    print_report(gs_lr,X_valid,y_valid, 'LogisticRegression', imputer)","71039ff1":"Logistic_Regression(train_data_knn_imputed, 'KNN')","c55df5b8":"Logistic_Regression(others_train_data, 'Manuel_Mapping')","8db540ab":"def Random_Forest(df, imputer):\n \n    X = df.drop(['target'], axis=1)\n    y = df['target']\n\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, stratify= y)\n\n    rf_clf = RandomForestClassifier(n_estimators=100)\n\n    param_grid = {\n        'max_depth': range(2,20,2),\n        'criterion': ['gini','entropy'],\n        'min_samples_split' : [2,5,10,20,50,100,150]\n    }\n\n    ss = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n    gs_rf = GridSearchCV(rf_clf,param_grid, cv=ss.split(X_train,y_train), scoring='roc_auc', n_jobs=-1)\n    gs_rf.fit(X_train,y_train)\n    \n    \n    print('Best parameters: ', gs_rf.best_params_)\n    print('Best score: ', gs_rf.best_score_)\n    print_report(gs_rf,X_valid,y_valid, 'RandomForest', imputer)","0d021c24":"Random_Forest(train_data_knn_imputed, 'KNN')","1da4503e":"Random_Forest(others_train_data, 'Manuel_Mapping')","f0ca2428":"# gamma means min_split_loss in XGBoost: minimum loss reduction required to make a further partition on a leaf node of the tree.\n# the larger the gamma is, the more conservative the algorithm will be.\n\n# collsample_bytree: is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n\n\ndef XGBoost_(df, imputer):\n\n    X = df.drop(['target'], axis=1)\n    y = df['target']\n\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, stratify= y)\n    \n    xgb_clf = xgb.XGBClassifier(use_label_encoder=False)\n\n\n    parameters = {\n         \"eta\"    : [0.01, 0.05, 0.10] ,\n         \"max_depth\"        : [ 5, 6, 8],\n         \"gamma\"            : [ 0.3, 0.4, 0.5 ],\n         \"colsample_bytree\" : [ 0.4, 0.5 , 0.7 ]\n         }\n\n\n    gs_xgboost = GridSearchCV(xgb_clf, parameters, n_jobs=-1, scoring='roc_auc', cv=3)\n    gs_xgboost.fit(X_train,y_train)\n    \n    print('Best parameters: ', gs_xgboost.best_params_)\n    print('Best score: ', gs_xgboost.best_score_)\n    print_report(gs_xgboost,X_valid,y_valid, 'XGBoost', imputer)\n    \n    return gs_xgboost","b6d77020":"xgboost_knn_model = XGBoost_(train_data_knn_imputed, 'KNN')","622aa161":"xgboost = XGBoost_(others_train_data, 'Manuel_Mapping')","01f9b83e":"def LightGBM_(df, imputer):\n    X = df.drop(['target'], axis=1)\n    y= df['target']\n\n    cat_features = ['city', 'gender', 'enrolled_university', 'education_level', 'major_discipline', 'company_size', 'company_type']\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, shuffle=True, stratify=y, random_state=1301)\n    \n    train_lgbm_dataset_format = lgb.Dataset(X_train, y_train, categorical_feature=cat_features)\n    valid_lgbm_dataset_format = lgb.Dataset(X_valid, y_valid, categorical_feature=cat_features)\n    \n    params = {'objective':'binary',\n          'metric' : 'auc',\n          'boosting_type' : 'gbdt',\n          'colsample_bytree' : 0.93,\n          'num_leaves' : 50,\n          'max_depth' : -1,\n          'n_estimators' : 1000,\n          'min_child_samples': 200, \n          'min_child_weight': 0.08,\n          'reg_alpha': 2,\n          'reg_lambda': 5,\n          'subsample': 0.9,\n          'verbose' : -1,\n          'num_threads' : 4,\n          'learning_rate': 0.015,\n          'random_seed' : 100\n        }\n    \n    lgbm = lgb.train(params,\n                 train_lgbm_dataset_format,\n                 3000,\n                 valid_sets=valid_lgbm_dataset_format,\n                 early_stopping_rounds= 40,\n                 verbose_eval= 10\n                 )\n\n    print_report(lgbm, X_valid, y_valid, model='LGBM', imputer=imputer)\n    \n    return lgbm","c731b273":"lgbm = LightGBM_(lgbm_train_data, 'Manuel_Mapping')","a71aa04b":"results = {'Timestamp': timestamp, 'Model':model_list, 'Imputer': imputer_list, 'AUC':auc_scores, 'Accuracy':accuracy_scores, 'Precision': precision_scores, 'Recall': recall_scores, 'F1_Score': f1_scores}\nresults = pd.DataFrame(results)\nresults","4d600ed6":"feature_importances = pd.concat(\n    [pd.DataFrame(xgboost.best_estimator_.feature_importances_, columns=['Importances']),\n     pd.Series(others_train_data.drop(['target'], axis=1).columns, name='Features')], axis=1).sort_values(by='Importances', ascending=False)","a08a75a1":"plt.figure(figsize=(10,5))\nax = sns.barplot(x='Features', y='Importances', data=feature_importances)\n\nplt.xticks(rotation=30)\nax.set_title('Feature Importances', fontsize='18')","51215cb2":"others_test_data","424ab9c9":"y_pred = xgboost.predict_proba(others_test_data.drop(['enrollee_id'], axis=1))[:,1]","ffbb4f0e":"submission = pd.concat([others_test_data['enrollee_id'], pd.Series(y_pred, name='target')], axis=1)\nsubmission","73aec278":"submission.to_csv('submission.csv',index=False)","65d37a91":"## 2.1 Handle Missing Values <a class=\"anchor\" id=\"chapter2.1\"><\/a>","ddaf6852":"### **--> Company Type**\n- **Nominal Categorical Variable**","efef8e55":"# 4. Model Comparisons <a class=\"anchor\" id=\"chapter4\"><\/a>","cb41d0f5":"- These are ordinal categorical variables which are in string format.\n- They can be converted to numerical variables, by handling >20 and <1 values.\n- If the model is LGBM then missing values left as NaN, otherwise will be encoded.","96810a06":"# \ud83d\udc0d Next DS Job: XGBoost, LightGBM, LogReg, R.Forest","a8caa995":"- Convert >4 and 'never' to numerical values.\n- If model is LGBM, missing values left as NaN, otherwise they are encoded.","3cebafc5":"## 3.4. Model 4: Light GBM <a class=\"anchor\" id=\"chapter3.4\"><\/a>\n\n- For LightGBM, the dataset: manually encoded but missing values left.","92d36895":"- There are missing values.\n- While converting ordinal categorical values to numerical values, the ordering can be considered.","3ee0fd88":"## 3.3 Model 3: XGBoost <a class=\"anchor\" id=\"chapter3.3\"><\/a>","94da7a23":"- Save the columns as series.","ea0ceaa6":"# SUMMARY\n\n1.  This notebook is created to implement different ML models on **HR Analytics: Job Change of Data Scientists** dataset.\n> **(XGBoost, LightGBM, LogisticRegression, RandomForest)**\n1.  The dataset is preprocessed in several ways. \n> **(KNNImputer, LabelEncoding, OneHotEncoding)**\n1.  Finally, the results are compared in order to find the best model and preprocessing combination.\n1.  The best model is used to predict test data.","7ebfc819":"- Categorical values are encoded.\n- Missing values are left as NaN. LightGBM can automatically handle missing values.\n- Check: https:\/\/lightgbm.readthedocs.io\/en\/latest\/Advanced-Topics.html#\n- **convert_dataset(df, 'LGBM')** function will encode the values while considering the order, and leaving missing values as NaN","4fcc4c96":"### **--> Target**\n- Binary classification problem","28ac01c1":"- Call convert_dataset function by giving model name as Others\n- All missing values are encoded","0802d2ee":"- There is no missing value.","aab7fbf2":"- Split the dataset as train and test","d9c3ccb1":"### **--> Enrolled University**\n- **Ordinal Categorical Attribute**","2b5134e1":"- First convert the original dataset without filling missing values.\n- Converting parameter is LGBM, because it will left missing values as NaN\n- Then, missing values will be imputed by using KNN algorithm.","1b8e3fa2":"### **--> Training Hours**","38cc56e4":"- There are a lot of missing values.\n- After conversion missing values will remain as NaN.","9ce01de4":"# Table Of Contents\n\n* [1. EDA & Preparing the Mappings](#chapter1)\n* [2. Data Preparation](#chapter2)\n    * [2.1. Handle Missing Values](#chapter2.1)\n    * [2.2. Dataset for LightGBM](#chapter2.2)\n    * [2.3. Correlation](#chapter2.3)\n    * [2.4. Dataset for LogisticRegression, RandomForest, XGBoost](#chapter2.4)\n        * [2.4.1 Encoding with manual mapping](#chapter2.4.1)\n        * [2.4.2 KNN Imputer](#chapter2.4.1)\n    * [2.3. Dataset Summary](#chapter2.5) \n* [3. Models](#chapter3)\n    * [3.1. Model 1: Logistic Regression](#chapter3.1)\n    * [3.2. Model 2: Random Forest](#chapter3.2)\n    * [3.3. Model 3: XGBoost](#chapter3.3)\n    * [3.4. Model 4: LightGBM](#chapter3.4)\n* [4. Model Comparisons](#chapter4)\n* [5. Feature Importances](#chapter5)\n* [6. Predict aug_test.csv](#chapter6)","00efac75":"### **--> Last New Job**\n- **Numerical Categorical Variable**","0a021ac5":"# 5. Feature Importances <a class=\"anchor\" id=\"chapter5\"><\/a>\n\n- Feature importances based on the **XGBoost** model.","d4f51d8a":"The models will be executed with below dataset configurations.\n\n- LightGBM:\n    - missing values left\n- Logistic Regression, Random Forest, XGBoost (missing values should be handled for these models)\n    - missing values encoded with mapping\n    - missing values imputed with KNN","bae81e46":"### **--> Gender**\n- **Nominal Categorical Attribute**: There is no ordering between gender categories.","42f6e9f2":"### **--> Experience**\n- **Numerical Categorical Variable**","b5d9df50":"- Label Encoder can be used for encoding cities to an integer.","b6818b21":"### **2.4.1. Encoding with manuel mapping**","017fe555":"- Outstanding correlations\n    - Experience vs Relevant Experience: This is self-explanatory relationship.\n    - Experience vs Enrolled University: People with higher experience usually dont enroll university.\n    - Experience vs Last New Job: While experience is increasing, the difference in years between the previus job and current job is increasing\n    - City development index vs Target (Looking for a job change): Negative correlation.","fd80ac0f":"# 2. Data Preparation <a class=\"anchor\" id=\"chapter2\"><\/a>","f5efb02c":"### **2.4.2. KNN Imputer**","05a372e3":"- This is numerical variable that includes non-numeric values like never and >4.\n- These values can be handled with mapping.","117399ca":"# 1. EDA & Preparing the mappings <a class=\"anchor\" id=\"chapter1\"><\/a>","cd714803":"- **others_test_data** is a dataframe that holds preprocessed aug_test.csv records ","da0fb2e7":"### **--> Major Discipline**\n- **Nominal Categorical Attribute**","851ec86c":"## 2.5 Datasets Summary <a class=\"anchor\" id=\"chapter2.5\"><\/a>\n\n- **lgbm_train_data**           : Dataset prepared for the LGBM model. All missing values are left as **NaN**.\n- **others_train_data**         : Dataset prepared for the models except LGBM. All missing values are handled with **manuel mapping**.\n- **train_data_knn_imputed**    : Dataset prepared for the models except LGBM. All missing values are handled with **KNN** imputer.\n\nAnd, corresponding test datasets are prepared.","2cf4bad2":"- There is no missing value for Relevant Experience column.\n- This is ordinal categorical variable, so there is ordering between categories. \n- Having relavant experience > Having no experience.\n- So, while convering them to numerical values, the ordering can be considered.","a57bce4e":"- Missing values can be handled with various ways.\n    - **Deleting** rows or columns that includes at least 1 missing value.\n    - **Encoding** all missing values to same number for a given column.\n    - **Imputation**: Filling the missing values with a relevant value. The relevant value can be\n        - median, mod, mean value of the column.\n        - a value that is found by implementing another machine learning algorithm to predict missing value.\n            - Ex: K-NearestNeigbors algorithm. The algorithm finds a value by evaluating the similar instances","0f4641ee":"## 2.4 Dataset for LogisticRegression, RandomForest, XGBoost <a class=\"anchor\" id=\"chapter2.4\"><\/a>","2cc33b2f":"### **--> Education Level**\n- **Ordinal Categorical Attribute**","37fcc1cf":"### **--> City Development Index**\n- **Continues Numerical Attribute**:","4b18a92b":"- Merge train and test dataset before implementing encoding","8cccd514":"## 3.2. Model 2: Random Forest <a class=\"anchor\" id=\"chapter3.2\"><\/a>","00018a2e":"- There are missing values.\n- After conversion missing values will remain as NaN.","985be29f":"### **--> Relevant Experience**\n- **Ordinal Categorical Attribute**","50528cc1":"- Since the categorical columns are labeled with numerical values, the values found by KNN should be rounded to integer.","8480d6e2":"- There are null values in Gender column. They will be handled later.\n- Categorical values are converted to numerical values by using mapping with dictionary.\n- For LGBM model, missing values left as NaN.","008e4ba5":"## 3.1 Model 1: Logistic Regression <a class=\"anchor\" id=\"chapter3.1\"><\/a>\n\n- For the nominal categorical variables, one hot encoding will be used in order to prevent introducing ordinal relation between values.\n- Tree-based models, such as Decision Trees, Random Forests, and Boosted Trees, typically don't perform well with one-hot encodings with lots of levels.\n- For Logistic regression, one hot encoding will be used for nominal categorical variable, label encoding will be used for ordinal categorical variable.\n- For RandomForest, XGBoost, and LightGBM, LabelEncoding will be used.","f7a86e8a":"- There is no null value in city_development_index column.","2ab2c39e":"## 2.3 Correlation among Columns <a class=\"anchor\" id=\"chapter2.3\"><\/a>","4602c4d5":"- Since the data is skewed, it is better to implement stratified splitting based on target value. ","d9b163fa":"# 6. Predict aug_test.csv <a class=\"anchor\" id=\"chapter6\"><\/a>","ac29d716":"- Based on the AUC score the best model is **XGBoost** which is trained with manually mapped dataset.","ec35f27d":"- This is numerical variable that includes non-numeric values like <1 and >20.\n- These values can be handled with mapping.","2afc6584":"## 2.2 Dataset for LightGBM <a class=\"anchor\" id=\"chapter2.2\"><\/a>","7004311d":"### **--> City**\n- **Nominal Categorical Attribute**: There is no ordering between the cities.","080d6857":"### **--> Company Size**\n> - **Ordinal Categorical Variable**","e8ab766a":"# 3. Models <a class=\"anchor\" id=\"chapter3\"><\/a>\n\n- Function for saving and printing model results for comparison"}}