{"cell_type":{"099dcfbe":"code","1339ba07":"code","e66866a7":"code","0a9a9b0b":"code","da618e07":"code","838cc01d":"code","2d2c16ec":"code","a0f39952":"code","d786b26e":"code","ccf2fd1a":"code","1fb3dc6d":"code","6b1652c6":"code","21fe3821":"code","9407dcf5":"code","ae2b5ed6":"code","e346ae03":"code","4ab3c6ce":"code","db191524":"code","1ca6cfe1":"code","e0459282":"code","62adc241":"code","5ccc457f":"code","4a3e7086":"code","213dc226":"code","fe50a734":"code","74505f56":"code","61737f9a":"code","cc40ce09":"code","a5c4c746":"code","d5647a9b":"code","150fd3ba":"code","61780f29":"code","04168f58":"code","664a78db":"code","d8fb060a":"code","c3f218ad":"code","cde550b1":"code","27d7ddbf":"code","9a3571de":"code","6f6719ca":"code","f988bf39":"code","fee62f2e":"code","a19ce0a1":"code","9577e714":"code","fb000127":"code","91b5ccbb":"code","3dbff5cb":"code","aa6716ff":"code","6a464cc3":"code","4d756ad6":"code","5240288c":"code","3a680d50":"code","daf7c4dd":"code","67a63016":"code","aab2a1fd":"code","484ef9ae":"code","18293926":"code","462c6b88":"code","88fdccb3":"code","66386dc2":"code","5964211a":"code","6fa28700":"code","b484860e":"code","2d0d315a":"code","ce36f010":"code","362de91f":"code","1cb51cec":"code","8f6a5b50":"code","bbd71f15":"code","88f129ae":"code","f658e008":"code","9a8a1215":"code","0d02a01b":"code","61898667":"code","f2eab42c":"code","9c298a14":"code","ebf9687c":"code","25b834eb":"code","a3eb197a":"code","1fb44db9":"code","d5551070":"code","dfabb81e":"code","77c33d91":"code","20913954":"code","5582c3d5":"markdown","74432a49":"markdown","0f631128":"markdown","cf8fb97d":"markdown","0dd6c358":"markdown","40a563b0":"markdown","7b60c376":"markdown","2b357bdf":"markdown","b1b3ed1c":"markdown","b659e342":"markdown","8b6a7100":"markdown","fe6d9064":"markdown","178a5791":"markdown","bf606111":"markdown","a68dee8e":"markdown","8a2178db":"markdown","3940afbd":"markdown","c36e19ea":"markdown","c79b4e50":"markdown","707fb679":"markdown","d552a1d7":"markdown","5801d12d":"markdown","10480692":"markdown","2cbd3e2a":"markdown","b71d2bf8":"markdown","7df8f07a":"markdown","fdae9449":"markdown","700bb250":"markdown","5150e842":"markdown","f9587f63":"markdown"},"source":{"099dcfbe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1339ba07":"#Let's import some of the basic libraries we'll be using.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","e66866a7":"#Let's have a look at our training and test data\n#Our training and test data have the same number of features except the target SalePrice is not present in the test dataset.\npd.pandas.set_option('display.max_columns',None)\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","0a9a9b0b":"df_train.head()","da618e07":"df_test.head()","838cc01d":"df_train.info()\nprint()\ndf_test.info()","2d2c16ec":"df_train.describe()","a0f39952":"df_test.describe()","d786b26e":"print(df_train.shape,df_test.shape)","ccf2fd1a":"#Let's see the number of missing values in every feature for training dataset.\nfeatures = df_train.columns\nfor feature in features:\n    print(feature,':',df_train[feature].isnull().mean()*100,'% missing values')","1fb3dc6d":"#Similarly for the test dataset.\nfeatures = df_test.columns\nfor feature in features:\n    print(feature,':',df_test[feature].isnull().mean()*100,'% missing values')","6b1652c6":"#Now let's see whether the missing data has some impact on the target variable.\n#For this we'll convert all missing values in every feature as 1 and try to see their impact on SalePrice.\nfeatures = df_train.columns\nfor feature in features:\n    data = df_train.copy()\n    data[feature] = np.where(data[feature].isnull(),1,0)\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.title(feature)\n    plt.show()","21fe3821":"numerical_features = [feature for feature in df_train.columns if df_train[feature].dtypes!='O']\nprint('Number of numerical features:',len(numerical_features))\ndf_train[numerical_features].head()","9407dcf5":"#Let's see how these features influence our target variable using a heatmap.\ncorrmatrix = df_train.corr()\ntop_corr_features = corrmatrix.index[abs(corrmatrix[\"SalePrice\"])>=0.5] #We're only selecting highly correlated features.\nplt.figure(figsize=(20,20))\nsns.heatmap(df_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","ae2b5ed6":"#Here we have some DateTime variables such as YearBuilt, YearRemodAdd, GarageYrBlt, YrSold.\n#We can convert these to gain more suitable information such as how old the house by subtracting the YrSold-YearBuilt.\nyear_vars = ['YearBuilt','YearRemodAdd','GarageYrBlt','YrSold']","e346ae03":"#Let's see the impact of these DateTime features with respect to SalePrice\nfor feature in year_vars:\n    if feature!='YrSold':\n        data = df_train.copy()\n        data[feature] = data['YrSold']-data[feature] #We'll calculate the difference between the year features and the year the house was sold for.\n        plt.scatter(data[feature],data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.show()","4ab3c6ce":"#Let's look at our discreet features.\ndiscreet_features=[feature for feature in numerical_features if len(df_train[feature].unique())<25 and feature not in year_vars+['Id']]\nprint('Number of Discreet Features:',len(discreet_features))\ndf_train[discreet_features].head()","db191524":"#Let'see how the discreet features vary with the SalePrice using barplots\nfor feature in discreet_features:\n    data = df_train.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.show()","1ca6cfe1":"#Let'see how the discreet features vary with the SalePrice using scatterplots.\nfor feature in discreet_features:\n    data = df_train.copy()\n    plt.scatter(data[feature],data['SalePrice'])\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.show()","e0459282":"#Let's have a look at our continuous features.\ncontinuous_features = [feature for feature in numerical_features if feature not in discreet_features+year_vars+['Id']]\nprint('Number of Continuous Features:',len(continuous_features))","62adc241":"df_train[continuous_features].head()","5ccc457f":"#Let's see the distrbution of continuous variables using histograms.\nfor feature in continuous_features:\n    data = df_train.copy()\n    data[feature].hist(bins=10)\n    plt.xlabel(feature)\n    plt.ylabel('Frequency')\n    plt.show()","4a3e7086":"#Let'see how the continuous features vary with the SalePrice using scatterplots.\nfor feature in continuous_features:\n    data = df_train.copy()\n    plt.scatter(data[feature],data['SalePrice'])\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.show()","213dc226":"#Finally let's look at our categorical features.\ncategorical_features = [feature for feature in df_train.columns if df_train[feature].dtypes=='O']\nprint('Number of Categorical Features:',len(categorical_features))\ndf_train[categorical_features].head()","fe50a734":"#Let's check how many categories are present in each feature.\nfor feature in categorical_features:\n    print(feature,'has',len(df_train[feature].unique()),'categories')","74505f56":"for feature in categorical_features:\n    print(feature,':',df_train[feature].unique())","61737f9a":"#Let's oberserve the relationship between these categorical features and SalePrice using barplots and boxplots.\nfor feature in categorical_features:\n    data = df_train.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.show()","cc40ce09":"#Let's use boxplots for a better visualization.\nfor feature in categorical_features:\n    sns.boxplot(x=feature,y='SalePrice',data=df_train)\n    plt.xlabel(feature)\n    plt.show()","a5c4c746":"for feature in numerical_features:\n    sns.histplot(df_train[feature],kde=True)\n    plt.show()","d5647a9b":"for feature in year_vars:\n    sns.histplot(df_train[feature],kde=True)\n    plt.show()","150fd3ba":"#Let's see how we can deal with the missing values in categorical_features.\nnan_cat_features = [feature for feature in df_train.columns if df_train[feature].isnull().sum()>1 and df_train[feature].dtypes=='O']\nfor feature in nan_cat_features:\n    print(\"{}: {}% missing values\".format(feature,np.round(df_train[feature].isnull().mean()*100,2)))","61780f29":"#Since features such as 'Alley','PoolQc','Fence','MiscFeature' are having more than 80% of the missing values.We will simply drop them.\ndf_train.drop(['Alley','PoolQC','Fence','MiscFeature'],axis=1,inplace=True)","04168f58":"#Let's create a function which impute missing values in every categorical feature with the most frequent category in that feature.\ndef impute_nan(df,variable):\n    most_frequent_category = df[variable].value_counts().index[0]\n    df[variable].fillna(most_frequent_category,inplace=True)","664a78db":"nan_cat_features_new = ['MasVnrType','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond']","d8fb060a":"for feature in nan_cat_features_new:\n    impute_nan(df_train,feature)","c3f218ad":"df_train[nan_cat_features_new].isnull().sum() # Now there's no missing data in categorical features.","cde550b1":"#Similarly for the test dataset.\nnan_cat_features = [feature for feature in df_test.columns if df_test[feature].isnull().sum()>1 and df_test[feature].dtypes=='O']\nfor feature in nan_cat_features:\n    print('{}: {}% missing values'.format(feature,np.round(df_test[feature].isnull().mean()*100,2)))","27d7ddbf":"df_test.drop(['Alley','PoolQC','Fence','MiscFeature'],axis=1,inplace=True)","9a3571de":"nan_cat_features_new = ['MSZoning','Utilities','MasVnrType','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Functional','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond']","6f6719ca":"def impute_test_nan(df,variable):\n    most_frequent_category = df[variable].value_counts().index[0]\n    df[variable].fillna(most_frequent_category,inplace=True)","f988bf39":"for feature in nan_cat_features_new:\n    impute_test_nan(df_test,feature)","fee62f2e":"df_test[nan_cat_features_new].isnull().sum()","a19ce0a1":"#Let's see how we can deal with missing values in numerical features.\nnan_num_features = [feature for feature in df_train.columns if df_train[feature].isnull().sum()>1 and df_train[feature].dtypes!='O']\nfor feature in nan_num_features:\n    print('{}: {}% missing values'.format(feature,np.round(df_train[feature].isnull().mean()*100,2)))","9577e714":"for feature in nan_num_features:\n    median = df_train[feature].median()\n    df_train[feature].fillna(median,inplace=True)","fb000127":"df_train[nan_num_features].isnull().sum() #Now there's no NaN values.","91b5ccbb":"#Similarly for the test dataset.\nnan_num_features = [feature for feature in df_test.columns if df_test[feature].isnull().sum()>1 and df_test[feature].dtypes!='O']\nfor feature in nan_num_features:\n    print('{}: {}% missing values'.format(feature,np.round(df_test[feature].isnull().mean()*100,2)))","3dbff5cb":"for feature in nan_num_features:\n    median = df_test[feature].median()\n    df_test[feature].fillna(median,inplace=True)\ndf_test[nan_num_features].isnull().sum()","aa6716ff":"#Let's deal with missing values for DateTime variables.\n#Here we will convert the datetime variables into how many years old which will prove to more valuable.\nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n    df_train[feature] = df_train['YrSold']-df_train[feature]","6a464cc3":"df_train[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()","4d756ad6":"#Similarly for test dataset.\nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n    df_test[feature] = df_test['YrSold']-df_test[feature]","5240288c":"df_test[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()","3a680d50":"#Ok so let's use a Q-Q plot to check whether our numerical features are gaussian distributed.\nimport pylab\nimport scipy.stats as stat\n\ndef plot_data(dataset,features):\n    for feature in features:\n        plt.figure(figsize=(10,6))\n        plt.subplot(1,2,1)\n        dataset[feature].hist()\n        plt.subplot(1,2,2)\n        stat.probplot(dataset[feature],dist='norm',plot=pylab)\n        plt.title(feature)\n        plt.show()","daf7c4dd":"#If we have a straight line passing through most points in a Q-Q then the feature is normally distributed.\nnumerical_features = [feature for feature in df_train.columns if df_train[feature].dtypes!='O']\nplot_data(df_train,numerical_features)","67a63016":"#As we have seen in EDA and from the Q-Q plots above that some of the features are skewed. We need to fix this with Logarithmic Transformation.\nskewed_feature = ['LotFrontage','LotArea','GrLivArea']\n\nfor feature in skewed_feature:\n    df_train[feature] = np.log(df_train[feature])","aab2a1fd":"#similarly for the test dataset.\nskewed_feature = ['LotFrontage','LotArea','GrLivArea']\n\nfor feature in skewed_feature:\n    df_test[feature] = np.log(df_test[feature])","484ef9ae":"df_train.drop('Id',axis=1,inplace=True)","18293926":"df_test.drop('Id',axis=1,inplace=True)","462c6b88":"categorical_variables = [feature for feature in df_train.columns if df_train[feature].dtypes=='O']","88fdccb3":"main_df=df_train.copy()","66386dc2":"#Now we'll combine the test data with the training data row-wise to onehotencode the features.\n#The reason we're combining the 2 dataframes is that there are some new categories in the test dataset.\nfinal_df = pd.concat([main_df,df_test],axis=0)","5964211a":"#Let's create a function to encode the categorical features.\ndef onehot_cols(cols): #The function takes a list of categorcial features\/columns.\n    df_final=final_df\n    i=0\n    \n    for fields in cols:\n        print(fields)\n        df1=pd.get_dummies(final_df[fields],drop_first=True)\n        final_df.drop([fields],axis=1,inplace=True)\n        if i==0:\n            df_final=df1.copy()\n        else:\n            df_final=pd.concat([df_final,df1],axis=1)\n        i=i+1\n       \n    df_final=pd.concat([final_df,df_final],axis=1)\n    return df_final","6fa28700":"final_df['SalePrice'] #Since there's no SalePrice in test dataframe that's why we have some NaN values.","b484860e":"final_df = onehot_cols(categorical_variables)","2d0d315a":"final_df.shape","ce36f010":"final_df = final_df.loc[:,~final_df.columns.duplicated()] #Remove duplicated features.","362de91f":"final_df.shape","1cb51cec":"final_df.head()","8f6a5b50":"#Sepearating our final train and test datasets.\ndf_Train = final_df.iloc[:1460,:]\ndf_Test = final_df.iloc[1460:,:]","bbd71f15":"df_Train","88f129ae":"df_Test","f658e008":"df_Test.drop('SalePrice',axis=1,inplace=True)","9a8a1215":"X = df_Train.drop('SalePrice',axis=1)\nY = df_Train['SalePrice']","0d02a01b":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=0)","61898667":"from xgboost import XGBRegressor\nxgb = XGBRegressor()\nxgb.fit(X_train,y_train)\ny_pred = xgb.predict(X_test)\ny_pred","f2eab42c":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nscore = r2_score(y_test,y_pred)\nscore","9c298a14":"print('RMSE:',np.sqrt(mean_squared_error(y_test,y_pred)))","ebf9687c":"n_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nbooster = ['gbtree','gblinear']\nbase_score = [0.25,0.5,0.75,1]\nlearning_rate = [0.05,0.1,0.15,0.20]\nmin_child_weight = [1,2,3,4]\n\n# Define the grid of hyperparameters to search\n\nhyperparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate,\n    'min_child_weight':min_child_weight,\n    'booster':booster,\n    'base_score':base_score\n    }","25b834eb":"#random_cv.fit(X_train,y_train)","a3eb197a":"#random_cv.best_estimator_","1fb44db9":"xgb = XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1)","d5551070":"xgb.fit(X_train,y_train)","dfabb81e":"y_pred = xgb.predict(df_Test)\ny_pred","77c33d91":"test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","20913954":"submission = pd.DataFrame({\"Id\":test[\"Id\"],\"SalePrice\":y_pred})\nsubmission.to_csv('submission.csv',index=False)","5582c3d5":"**Observations: We can see that features with nan values have some relation with the target variable, therefore we'll have to care of them with some imputation\/encoding technique in the Feature Engineering stage.**","74432a49":"Set up the random search with 4-fold cross validation  \nfrom sklearn.model_selection import RandomizedSearchCV  \nrandom_cv = RandomizedSearchCV(estimator=xgb,  \n            param_distributions=hyperparameter_grid,  \n            cv=5,n_iter=50,  \n            scoring='neg_mean_absolute_error',n_jobs = 4,  \n            verbose=5,   \n            return_train_score=True,  \n            random_state=42)","0f631128":"### 4.3 Outliers","cf8fb97d":"### 6.3 Hyperparameter optimization  \nYou can go with GridSearchCV or RandomizedSearchCV. GSCV works better when we less number of parameters to tune, therefore we'll go with RSCV.","0dd6c358":"## 5. Feature Engineering","40a563b0":"**Obersvations: As you can you we have a lot of categorical features present in our dataset therefore we'll try to use some encoding technique in feature technique to convert them to numerical data to feed into our model.** ","7b60c376":"**Observations: We can see from the heatmap and barplots that quite a lot of features such as OverallQual,YearBuilt and others have a high correlation with SalePrice therefore we'll have to take them into consideration during feature selection stage. Some features which are highly correlated with each other will have to be dropped becuase they won't be of any value to the model.**","2b357bdf":"## 7. Conclusion  \nThe model gives pretty decent results using XgBoost with further improvements to be made. I think using feature selection techniques such as correlation,feature_importances,variance threshold and other methods might increase the accuracy of the model, since usually it is the data which goes into the model that really affects the accuracy rather than hyperparamter tuning. LabelEncoding might also give better results since we won't be creating large number of columns. I really enjoyed working on this problem statement, since it was my first time handling a dataset with large number of features. Special thanks to [Krish Naik](https:\/\/www.kaggle.com\/krishnaik06), I've learned a lot from him and this project has given me confidence to be able to work on new projects on my own. I hope you found this notebook helpful.","b1b3ed1c":"### 5.3 Encoding Categorical Features  \nWe can use either OneHotEncoding or LabelEncoding to convert our categorical data to numerical data so that we'll be able to feed it to our model. OneHotEncoding will result in creation of multiple columns which is something you might want to look into. LabelEncoding on the the hand will simply convert the categories into numeric data based on their rank. We'll go with OneHotEncoding since it is a bit simpler to implement.","b659e342":"## 4. Exploratory Data Analysis\nWe'll try to focus on the following:\n\n**1. Missing Data**  \n**2. Numerical\/Categorical Features**  \n**3. Outliers present in the dataset**  \n**4. Factors influencing the SalePrice(target variable)**","8b6a7100":"## 2. Environemnt Setup","fe6d9064":"There are 5 ways in which we can handle missing values in numerical features.\n\n**1. Mean\/Median\/Mode Imputation**  \n**2. Random Sample Imputation**  \n**3. Capturing NaN values with a new feature**  \n**4. End of Distribution Imputation**  \n**5. Arbitrary Value Imputation**  \n\nWe'll be using Median imputation since our dataset contains quite a lot of outliers.","178a5791":"### 4.1 Missing Data","bf606111":"### 5.2 Feature Transformation","a68dee8e":"### 5.1 Handling Missing Values","8a2178db":"### 6.1 Splitting the training data into train and test set for checking accuracy within the training set.","3940afbd":"## 3. Import Basic Libraries","c36e19ea":"**Observations: We can see from the scatter plots that the SalePrice for old houses is low compared to new houses which is not surprising. Therefore these datetieme features will play a valuable role in our model building.**","c79b4e50":"## 1. Overview\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\n**Goal:** Our aim is to predict the house price based on various factors such as number of bedrooms, basements, finishing quality and more.","707fb679":"There are 2 ways by which we can handle missing values in categorical features.\n\n**1. Frequent Catergory Imputation** - In this method we simply replace the NaN values with the most frequent category present in the feature. For example in the feature 'Alley', we can simply replace nan values with 'Paved' since it's the most dominant category present in that feature. This method is easy to implement and is fast however if there are many NaN values,the most frequent labels are going to be represented more often and also lead to some distortion between variables.We will be using this technique since it is much faster.\n\n**2. Adding a feature to catch NaN** - In this method we add new features for those features having NaN values and replace the data with 1 where we encounter missing value and 0 where there's no missing value. Later on we can drop the original features which are having NaN values during feature selection. An alternate technique is to replace all the missing values with the label 'Missing' This will therefore create a new category within the feature.","d552a1d7":"### 4.2 Numerical\/Categorical Data","5801d12d":"As we can see from the boxplots and histograms that we have a lot of outliers. We will be using some transformation techniques for numerical data and for categorical data we can replace them with the mode of that feature.","10480692":"### 6.2 Modelling using XgBoost Regressor","2cbd3e2a":"**Observations: We can see there are lot of missing values in our dataset. Features such as Alley, Fence, MiscFeature, PoolQC have the maximum number of missing values.**","b71d2bf8":"# Table of Contents\n1. **Overview**\n2. **Environment Setup**\n3. **Import Basic Libraries**\n4. **Exploratory Data Analysis**\n5. **Feature Engineering**\n7. **Modelling**\n8. **Conclusion**","7df8f07a":"**Credits:** [Krish Naik](https:\/\/www.kaggle.com\/krishnaik06)","fdae9449":"**Observations: We can see that most of the continuous features are skewed. So we'll have to transform them to gaussian distribution using some transformation technique. We'll be doing this in Feature Engineering so that our Regression models give accurate results.**","700bb250":"**Observations: Most of the features such as LotFrontage,BsmtFinSF1,GrLivArea seem to follow somewhat of a linear relationship with SalePrice having some major outliers.**","5150e842":"**Observations: As you can see that categorical features have importance in SalePrice prediction of a house. Features such as Air Conditioning,Paved\/Unpaved roads have an impact on SalePrice.**","f9587f63":"## 6. Modelling  \nI have tried using Linear Regression, RandomForest Regressor and XgBoost. Linear Regression performed better compared to RandomForest but XgBoost performed the best among the three. You can also try other models."}}