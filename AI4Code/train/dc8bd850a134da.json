{"cell_type":{"120b4d5a":"code","652f0333":"code","66591025":"code","b953d903":"code","f8c881de":"code","56435210":"code","b28e01b8":"code","f351e510":"code","3da338ac":"code","8acb34ff":"code","4f93956e":"code","bbfcfdb1":"code","d590c03b":"code","64305610":"markdown"},"source":{"120b4d5a":"import pandas as pd\nimport numpy as np\nimport json\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.layers import Bidirectional, GRU, LSTM, Layer\nimport keras.backend as K\nimport tensorflow as tf\nimport plotly.express as px\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.cluster import KMeans\nimport os\nimport random\n\nEPOCHS = 60\nBATCH_SIZE = 64\nSEED = 34\nTTA = True\nFOLD_N = 5","652f0333":"def seed_everything(seed = SEED):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\nseed_everything()","66591025":"def pandas_list_to_array(df):    \n    return np.transpose(\n        np.array(df.values.tolist()),\n        (0, 2, 1)\n    )\n\ndef preprocess_inputs(df, token2int, cols=['sequence', 'structure', 'predicted_loop_type']):\n    base_feature = np.transpose(df[cols]\n                                .applymap(lambda seq: [token2int[x] for x in seq])\n                                .values.tolist(), \n                                (0, 2, 1))\n    bpps_sum_feature = np.array(df['bpps_sum'].tolist())[:, :, np.newaxis]\n    bpps_max_feature = np.array(df['bpps_max'].tolist())[:, :, np.newaxis]\n    bpps_nb_feature = np.array(df['bpps_nb'].tolist())[:, :, np.newaxis]\n\n    return np.concatenate([base_feature, bpps_sum_feature, bpps_max_feature, bpps_nb_feature], axis=2)\n\ndef augment_data(augment_df, df):\n    target_df = df.copy()\n    new_df = augment_df[augment_df['id'].isin(target_df['id'])]\n\n    del target_df['structure']\n    del target_df['predicted_loop_type']\n\n    new_df = new_df.merge(target_df, on=['id', 'sequence'], how='left')\n\n    df['cnt'] = df['id'].map(new_df[['id', 'cnt']].set_index('id').to_dict()['cnt'])\n    df['log_gamma'] = 100\n    df['score'] = 1.0\n    df = df.append(new_df[df.columns])\n\n    return df","b953d903":"# Loss function\ndef rmse(y_actual, y_pred):\n    mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n    return tf.keras.backend.sqrt(mse)\n    \ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\ndef mcrmse(y_actual, y_pred, num_scored=len(target_cols)):\n    score = 0\n    for i in range(num_scored):\n        score += rmse(y_actual[:, :, i], y_pred[:, :, i]) \/ num_scored\n    return score","f8c881de":"# Inherit tensorflow.keras.layers.Layer\nclass GRU_Layer(Layer):\n    def __init__(self, hidden_dim, dropout):\n\n        super(GRU_Layer, self).__init__()\n\n        self.custom_layers = Bidirectional(GRU(hidden_dim, \n                                            dropout=dropout, \n                                            return_sequences=True, \n                                            kernel_initializer='orthogonal'))\n    def call(self, inputs):\n        return self.custom_layers(inputs)\n\n\nclass LSTM_Layer(Layer):\n    def __init__(self, hidden_dim, dropout):\n\n        super(LSTM_Layer, self).__init__()\n        self.custom_layers = Bidirectional(LSTM(hidden_dim, \n                                            dropout=dropout, \n                                            return_sequences=True, \n                                            kernel_initializer='orthogonal'))\n\n    def call(self, inputs):\n        return self.custom_layers(inputs)","56435210":"# Based on @tito model \nclass MIX_LSTM_GRU(tf.keras.Model):\n    def __init__(self, seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, \n                hidden_dim=256, model_type=0, token2int=None):\n\n        super(MIX_LSTM_GRU, self).__init__()\n\n        self.token2int = token2int\n        self.embed = L.Embedding(input_dim=len(self.token2int), output_dim=embed_dim)\n        self.dense = L.Dense(5, activation='linear')\n        \n\n        self.pred_len = pred_len\n        self.hidden_dim = hidden_dim\n        self.dropout = dropout\n\n\n        self.type = model_type\n\n    def build(self, inputs):\n\n        self.gru_layers = []\n        self.lstm_layers = []\n        for i in range(2):\n            self.gru_layers.append(GRU_Layer(self.hidden_dim, self.dropout))\n            self.lstm_layers.append(LSTM_Layer(self.hidden_dim, self.dropout))\n\n    def call(self, inputs):\n        categorical_features = inputs[:, :, :3]\n        numerical_features = inputs[:, :, 3:]\n\n        embed = self.embed(categorical_features)\n        reshaped = tf.reshape(embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n        reshaped = tf.keras.layers.concatenate([reshaped, numerical_features], axis=2)\n\n        if self.type == 0:\n            hidden = self.gru_layers[0](reshaped)\n            hidden = self.gru_layers[1](hidden)\n        elif self.type == 1:\n            hidden = self.lstm_layers[0](reshaped)\n            hidden = self.gru_layers[0](hidden)\n        elif self.type == 2:\n            hidden = self.gru_layers[1](reshaped)\n            hidden = self.lstm_layers[1](hidden)\n        elif self.type == 3:\n            hidden = self.lstm_layers[0](reshaped)\n            hidden = self.lstm_layers[1](hidden)        \n\n        truncated = hidden[:, :self.pred_len]\n        out = self.dense(truncated)\n\n        return out","b28e01b8":"def read_bpps_sum(df, root_path):\n    bpps_arr = []\n    for obj_id in df.id.to_list():\n        bppm = np.load(os.path.join(root_path, obj_id + '.npy')).sum(axis=1)\n        bpps_arr.append(bppm)\n    return bpps_arr\n\n\ndef read_bpps_max(df, root_path):\n    bpps_arr = []\n    for obj_id in df.id.to_list():\n        bppm = np.load(os.path.join(root_path, obj_id + '.npy'))\n        bpps_arr.append(np.max(bppm, axis=1))\n    return bpps_arr\n\n\n\ndef read_bpps_nb(df, root_path, bpps_mean, bpps_std):\n    bpps_arr = []\n    for obj_id in df.id.to_list():\n        bpps = np.load(os.path.join(root_path, obj_id + '.npy'))\n        bpps_nb = (bpps > 0).sum(axis=0) \/ bpps.shape[0]\n        bpps_nb = (bpps_nb - bpps_mean) \/ bpps_std\n        bpps_arr.append(bpps_nb)\n    \n    return bpps_arr\n\n\ndef calc_bpps_mean_std(df, root_path):\n    bpps_arr = []\n    for obj_id in df.id.to_list():\n        bpps = np.load(os.path.join(root_path, obj_id + '.npy'))\n        bpps_nb = (bpps > 0).sum(axis=0) \/ bpps.shape[0]\n        bpps_arr.append(bpps_nb)\n    \n\n    return np.mean(bpps_arr), np.std(bpps_arr)\n\ndef add_bpp_feature(train, test, bpps_dir):\n    mean, std = calc_bpps_mean_std(train, bpps_dir)\n    train['bpps_sum'] = read_bpps_sum(train, bpps_dir)\n    test['bpps_sum'] = read_bpps_sum(test, bpps_dir)\n    train['bpps_max'] = read_bpps_max(train, bpps_dir)\n    test['bpps_max'] = read_bpps_max(test, bpps_dir)\n    train['bpps_nb'] = read_bpps_nb(train, bpps_dir, mean, std)\n    test['bpps_nb'] = read_bpps_nb(test, bpps_dir, mean, std)\n\n    return train, test","f351e510":"def group_by_kmean_reactivity(train, token2int):\n    kmeans_model = KMeans(n_clusters=200, random_state=110).fit(preprocess_inputs(train, token2int)[:, :, 0])\n    train['cluster_id'] = kmeans_model.labels_\n    \n    return train","3da338ac":"def train_mix_lstm_gru(train, public_df, private_df, target_cols, token2int, model_type=0, FOLD_N=FOLD_N):\n    Ver='MIX_LSTM_GRU_' + str(model_type) \n    gkf = GroupKFold(n_splits=FOLD_N)\n\n    public_inputs = preprocess_inputs(public_df, token2int)\n    private_inputs = preprocess_inputs(private_df, token2int)\n\n\n    holdouts = []\n    holdout_preds = []\n\n    for cv, (tr_idx, vl_idx) in enumerate(gkf.split(train,  train['reactivity'], train['cluster_id'])):\n        trn = train.iloc[tr_idx]\n        x_trn = preprocess_inputs(trn, token2int)\n        y_trn = np.array(trn[target_cols].values.tolist()).transpose((0, 2, 1))\n        w_trn = np.log(trn.signal_to_noise+1.1)\/2\n\n        val = train.iloc[vl_idx]\n        x_val_all = preprocess_inputs(val, token2int)\n        val = val[val.SN_filter == 1]\n        x_val = preprocess_inputs(val, token2int)\n        y_val = np.array(val[target_cols].values.tolist()).transpose((0, 2, 1))\n\n        model = MIX_LSTM_GRU(model_type=model_type, token2int=token2int)\n        model.compile(tf.keras.optimizers.Adam(), loss=mcrmse)\n\n        model_short = MIX_LSTM_GRU(seq_len=107, pred_len=107, model_type=model_type, token2int=token2int)\n        model_long = MIX_LSTM_GRU(seq_len=130, pred_len=130, model_type=model_type, token2int=token2int)\n\n        history = model.fit(\n            x_trn, y_trn,\n            validation_data = (x_val, y_val),\n            batch_size=BATCH_SIZE,\n            epochs=EPOCHS,\n            sample_weight=w_trn,\n            callbacks=[\n                tf.keras.callbacks.ReduceLROnPlateau(),\n                tf.keras.callbacks.ModelCheckpoint(f'model{Ver}_cv{cv}.h5')\n            ]\n        )\n\n        fig = px.line(\n            history.history, y=['loss', 'val_loss'], \n            labels={'index': 'epoch', 'value': 'Mean Squared Error'}, \n            title='Training History')\n        fig.show()\n\n        model.load_weights(f'model{Ver}_cv{cv}.h5')\n        \n        model_short.compile(optimizer=tf.optimizers.Adam(), loss=mcrmse)\n        model_short.train_on_batch(tf.zeros((1,107,6)), tf.zeros((1,107,5)))\n        model_short.load_weights(f'model{Ver}_cv{cv}.h5')\n\n        model_long.compile(optimizer=tf.optimizers.Adam(), loss=mcrmse)\n        model_long.train_on_batch(tf.zeros((1,130,6)), tf.zeros((1,130,5)))        \n        model_long.load_weights(f'model{Ver}_cv{cv}.h5')\n\n\n        holdouts.append(train.iloc[vl_idx])\n        holdout_preds.append(model.predict(x_val_all))\n        if cv == 0:\n            public_preds = model_short.predict(public_inputs)\/FOLD_N\n            private_preds = model_long.predict(private_inputs)\/FOLD_N\n        else:\n            public_preds += model_short.predict(public_inputs)\/FOLD_N\n            private_preds += model_long.predict(private_inputs)\/FOLD_N\n    return holdouts, holdout_preds, public_df, public_preds, private_df, private_preds","8acb34ff":"data_dir = '\/kaggle\/input\/stanford-covid-vaccine\/'\nbpps_dir = '\/kaggle\/input\/stanford-covid-vaccine\/bpps\/'\ntrain = pd.read_json(data_dir + 'train.json', lines=True)\ntest = pd.read_json(data_dir + 'test.json', lines=True)\nsample_df = pd.read_csv(data_dir + 'sample_submission.csv')\naug_df = pd.read_csv('\/kaggle\/input\/openvaccine-augment\/aug_data.csv')\n\ntoken2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\ntrain, test = add_bpp_feature(train, test, bpps_dir)\ntrain = group_by_kmean_reactivity(train, token2int)\n\nif TTA:\n    train = augment_data(aug_df, train)\n    test = augment_data(aug_df, test)\n\n\ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\ntrain_inputs = preprocess_inputs(train, token2int)\ntrain_labels = pandas_list_to_array(train[target_cols])\npublic_df = test.query(\"seq_length == 107\")\nprivate_df = test.query(\"seq_length == 130\")","4f93956e":"val_df, val_preds, test_df, test_preds = [], [], [], []\ndebug = False\nif debug:\n    nmodel = 1\nelse:\n    nmodel = 4\nfor i in range(nmodel):\n    holdouts, holdout_preds, public_df, public_preds, private_df, private_preds = train_mix_lstm_gru(train, \n                                                                                                     public_df, private_df, \n                                                                                                     target_cols, \n                                                                                                     token2int,model_type=i)\n    val_df += holdouts\n    val_preds += holdout_preds\n    test_df.append(public_df)\n    test_df.append(private_df)\n    test_preds.append(public_preds)\n    test_preds.append(private_preds)","bbfcfdb1":"preds_ls = []\nfor df, preds in zip(test_df, test_preds):\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n        single_df = pd.DataFrame(single_pred, columns=target_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n        preds_ls.append(single_df)\npreds_df = pd.concat(preds_ls).groupby('id_seqpos').mean().reset_index()\n# .mean() is for\n# 1, Predictions from multiple models\n# 2, TTA (augmented test data)\n\npreds_ls = []\nfor df, preds in zip(val_df, val_preds):\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n        single_df = pd.DataFrame(single_pred, columns=target_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n        single_df['SN_filter'] = df[df['id'] == uid].SN_filter.values[0]\n        preds_ls.append(single_df)\nholdouts_df = pd.concat(preds_ls).groupby('id_seqpos').mean().reset_index()","d590c03b":"submission = preds_df[['id_seqpos', 'reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']]\nsubmission.to_csv(f'submission.csv', index=False)\nprint(f'wrote to submission.csv')","64305610":" Many thanks to @tito @xhlulu and @tuckerarrants for their great kernels. This kernels (OOP Refactoring) is based on their works:\n https:\/\/www.kaggle.com\/its7171\/gru-lstm-with-feature-engineering-and-augmentation\n \n https:\/\/www.kaggle.com\/xhlulu\/openvaccine-simple-gru-model\n \n https:\/\/www.kaggle.com\/tuckerarrants\/openvaccine-gru-lstm\n \n I reccommend you reading theirs work for ideas and understanding before look into my kernels. "}}