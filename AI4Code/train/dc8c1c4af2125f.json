{"cell_type":{"aa78bc46":"code","9a214408":"code","ba4c49c0":"code","c05dfd3b":"code","15891c02":"code","27103107":"code","d7b1030e":"code","2b6590e4":"code","7504a1cc":"code","e8948c2d":"code","0a8240f7":"code","d444743c":"code","c3c3b057":"code","1765807d":"code","c77cd99c":"code","e1be5b84":"code","aa06df45":"code","f12006b5":"code","14d17eea":"code","dcda0e0b":"code","02e55b82":"code","c0f8c4ce":"code","09fa9a73":"code","0197a5d8":"code","834991d6":"code","1da493fb":"code","583dd0d0":"code","f115aa40":"code","7f3f82fc":"code","d30930c3":"code","b8805c5e":"code","e3e6f0ba":"code","e00a0851":"code","bf46b7c2":"code","712fe2fc":"code","5c537f9e":"code","8b0e1e4d":"code","c4a794c6":"code","6531e6e9":"code","32f5114f":"code","3a7d8633":"code","dc4eb0e3":"code","20e88df3":"code","684b4e50":"markdown","629a673e":"markdown","050dcd28":"markdown","8224b774":"markdown","5424c5dc":"markdown","270c21e7":"markdown","2fabb972":"markdown","481e134c":"markdown","23097e2e":"markdown","d7211a45":"markdown","bf21a898":"markdown","11852414":"markdown","2a138007":"markdown","e72ec533":"markdown","c39d882d":"markdown","f6448844":"markdown","16f586f5":"markdown","8508ad55":"markdown","9d53d379":"markdown","bd7f6ec1":"markdown","875ec8c5":"markdown","6932508a":"markdown","9cf285f4":"markdown","93c36a0d":"markdown","d35f4a96":"markdown","2f4e18e0":"markdown","90f64dc8":"markdown","333332ce":"markdown","047d972f":"markdown","2cd36078":"markdown","0b2df0f5":"markdown","2235b628":"markdown","6ff70cc9":"markdown","ce5f514e":"markdown","3afb808d":"markdown","af612bcd":"markdown","fdb1e058":"markdown","f4235342":"markdown","0933c5c3":"markdown","156fc3e7":"markdown","5e2f7e52":"markdown","72503e90":"markdown","50dc4b62":"markdown","a790f7be":"markdown","ab840169":"markdown","0423b69f":"markdown","feaabc80":"markdown","32e7f01e":"markdown","f8f0896d":"markdown","214dfe35":"markdown","85b5a3e9":"markdown","ca07afbf":"markdown","46954a49":"markdown","cccc9b76":"markdown","1fdacb94":"markdown","45518b00":"markdown","239aa3d0":"markdown","3e96886e":"markdown"},"source":{"aa78bc46":"# importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(action = 'ignore')","9a214408":"#importing data\ndata = pd.read_csv('..\/input\/banking-churn-prediction\/Banking_churn_prediction.csv')","ba4c49c0":"#first 5 instances using \"head()\" function\ndata.head()","c05dfd3b":"#last 5 instances using \"tail()\" function\ndata.tail()","15891c02":"#finding out the shape of the data using \"shape\" variable: Output (rows, columns)\ndata.shape","27103107":"#Printing all the columns present in data\ndata.columns","d7b1030e":"# A closer look at the data types present in the data\ndata.dtypes","2b6590e4":"# Identifying variables with integer datatype\ndata.dtypes[data.dtypes == 'int64']","7504a1cc":"# converting churn to category\ndata['churn'] = data['churn'].astype('category')\ndata['branch_code'] = data['branch_code'].astype('category')\ndata['customer_nw_category'] = data['customer_nw_category'].astype('category')\ndata.dtypes[data.dtypes == 'int64']","e8948c2d":"# Identifying variables with float datatype\ndata.dtypes[data.dtypes == 'float64']","0a8240f7":"# converting \"dependents\" and \"city\" to their respective types\ndata['dependents'] = data['dependents'].astype('Int64')\ndata['city'] = data['city'].astype('category')\n\n# checking\ndata[['dependents','city']].dtypes","d444743c":"data.dtypes","c3c3b057":"# Manually checking object types\ndata[['gender','occupation','last_transaction']].head(7)","1765807d":"# typecasting \"gender\" and \"occupation\" to category type\ndata['gender'] = data['gender'].astype('category')\ndata['occupation'] = data['occupation'].astype('category')\n\n# checking\ndata[['gender','occupation']].dtypes","c77cd99c":"# creating an instance(date) of DatetimeIndex class using \"last_transaction\"\ndate = pd.DatetimeIndex(data['last_transaction'])","e1be5b84":"# extracting new columns from \"last_transaction\"\n\n# last day of year when transaction was done\ndata['doy_ls_tran'] = date.dayofyear\n\n# week of year when last transaction was done\ndata['woy_ls_tran'] = date.weekofyear\n\n# month of year when last transaction was done\ndata['moy_ls_tran'] = date.month\n\n# day of week when last transaction was done\ndata['dow_ls_tran'] = date.dayofweek","aa06df45":"# checking new extracted columns using datetime\ndata[['last_transaction','doy_ls_tran','woy_ls_tran','moy_ls_tran','dow_ls_tran']].head()","f12006b5":"# Removing the original datetime column\ndata = data.drop(columns = ['last_transaction'])\ndata.dtypes","14d17eea":"# Numerical datatypes\ndata.select_dtypes(include=['int64','float64','Int64']).dtypes","dcda0e0b":"# seggregating variables into groups\ncustomer_details = ['customer_id','age','vintage']\ncurrent_month = ['current_balance','current_month_credit','current_month_debit','current_month_balance']\nprevious_month = ['previous_month_end_balance','previous_month_credit','previous_month_debit','previous_month_balance']\nprevious_quarters = ['average_monthly_balance_prevQ','average_monthly_balance_prevQ2']\ntransaction_date = ['doy_ls_tran','woy_ls_tran','moy_ls_tran','dow_ls_tran']","02e55b82":"# custom function for easy and efficient analysis of numerical univariate\n\ndef UVA_numeric(data, var_group):\n  '''\n  Univariate_Analysis_numeric\n  takes a group of variables (INTEGER and FLOAT) and plot\/print all the descriptives and properties along with KDE.\n\n  Runs a loop: calculate all the descriptives of i(th) variable and plot\/print it\n  '''\n\n  size = len(var_group)\n  plt.figure(figsize = (7*size,3), dpi = 100)\n  \n  #looping for each variable\n  for j,i in enumerate(var_group):\n    \n    # calculating descriptives of variable\n    mini = data[i].min()\n    maxi = data[i].max()\n    ran = data[i].max()-data[i].min()\n    mean = data[i].mean()\n    median = data[i].median()\n    st_dev = data[i].std()\n    skew = data[i].skew()\n    kurt = data[i].kurtosis()\n\n    # calculating points of standard deviation\n    points = mean-st_dev, mean+st_dev\n\n    #Plotting the variable with every information\n    plt.subplot(1,size,j+1)\n    sns.kdeplot(data[i], shade=True)\n    sns.lineplot(points, [0,0], color = 'black', label = \"std_dev\")\n    sns.scatterplot([mini,maxi], [0,0], color = 'orange', label = \"min\/max\")\n    sns.scatterplot([mean], [0], color = 'red', label = \"mean\")\n    sns.scatterplot([median], [0], color = 'blue', label = \"median\")\n    plt.xlabel('{}'.format(i), fontsize = 20)\n    plt.ylabel('density')\n    plt.title('std_dev = {}; kurtosis = {};\\nskew = {}; range = {}\\nmean = {}; median = {}'.format((round(points[0],2),round(points[1],2)),\n                                                                                                   round(kurt,2),\n                                                                                                   round(skew,2),\n                                                                                                   (round(mini,2),round(maxi,2),round(ran,2)),\n                                                                                                   round(mean,2),\n                                                                                                   round(median,2)))","c0f8c4ce":"UVA_numeric(data,customer_details)","09fa9a73":"UVA_numeric(data,current_month)","0197a5d8":"# standard deviation factor\nfactor = 2\n\n# copying current_month\ncm_data = data[current_month]\n\n# filtering using standard deviation (not considering obseravtions > mean + 3* standard deviation)\ncm_data = cm_data[cm_data['current_balance'] < cm_data['current_balance'].mean() + factor*cm_data['current_balance'].std()]\ncm_data = cm_data[cm_data['current_month_credit'] < cm_data['current_month_credit'].mean() + factor*cm_data['current_month_credit'].std()]\ncm_data = cm_data[cm_data['current_month_debit'] < cm_data['current_month_debit'].mean() + factor*cm_data['current_month_debit'].std()]\ncm_data = cm_data[cm_data['current_month_balance'] < cm_data['current_month_balance'].mean() + factor*cm_data['current_month_balance'].std()]\n\n# checking how many points removed\nlen(data), len(cm_data)","834991d6":"UVA_numeric(cm_data,current_month)","1da493fb":"UVA_numeric(data,previous_month)","583dd0d0":"UVA_numeric(data,previous_quarters)","f115aa40":"UVA_numeric(data,transaction_date)","7f3f82fc":"data.select_dtypes(exclude=['int64','float64','Int64']).dtypes","d30930c3":"# Custom function for easy visualisation of Categorical Variables\ndef UVA_category(data, var_group):\n\n  '''\n  Univariate_Analysis_categorical\n  takes a group of variables (category) and plot\/print all the value_counts and barplot.\n  '''\n  # setting figure_size\n  size = len(var_group)\n  plt.figure(figsize = (7*size,5), dpi = 100)\n\n  # for every variable\n  for j,i in enumerate(var_group):\n    norm_count = data[i].value_counts(normalize = True)\n    n_uni = data[i].nunique()\n\n  #Plotting the variable with every information\n    plt.subplot(1,size,j+1)\n    sns.barplot(norm_count, norm_count.index , order = norm_count.index)\n    plt.xlabel('fraction\/percent', fontsize = 20)\n    plt.ylabel('{}'.format(i), fontsize = 20)\n    plt.title('n_uniques = {} \\n value counts \\n {};'.format(n_uni,norm_count))","b8805c5e":"UVA_category(data, ['occupation', 'gender', 'customer_nw_category'])","e3e6f0ba":"UVA_category(data, ['city', 'branch_code'])","e00a0851":"#Plotting \"city\"\nplt.figure(figsize = (5,5), dpi = 120)\ncity_count = data['city'].value_counts(normalize=True)\nsns.barplot(city_count.index, city_count , order = city_count.index)\nplt.xlabel('City')\nplt.ylabel('fraction\/percent')\nplt.ylim(0,0.02)","bf46b7c2":"#Plotting \"branch_code\"\nplt.figure(figsize = (5,5), dpi = 120)\nbranch_count = data['branch_code'].value_counts()\nsns.barplot(branch_count.index, branch_count , order = branch_count.index)\nplt.xlabel('branch_code')\nplt.ylabel('fraction\/percent')\n#plt.ylim(0,0.02)","712fe2fc":"UVA_category(data, ['churn'])","5c537f9e":"# finding number of missing values in every variable\ndata.isnull().sum()","8b0e1e4d":"# custom function for easy outlier analysis\n\ndef UVA_outlier(data, var_group, include_outlier = True):\n  '''\n  Univariate_Analysis_outlier:\n  takes a group of variables (INTEGER and FLOAT) and plot\/print boplot and descriptives\\n\n  Runs a loop: calculate all the descriptives of i(th) variable and plot\/print it \\n\\n\n\n  data : dataframe from which to plot from\\n\n  var_group : {list} type Group of Continuous variables\\n\n  include_outlier : {bool} whether to include outliers or not, default = True\\n\n  '''\n\n  size = len(var_group)\n  plt.figure(figsize = (7*size,4), dpi = 100)\n  \n  #looping for each variable\n  for j,i in enumerate(var_group):\n    \n    # calculating descriptives of variable\n    quant25 = data[i].quantile(0.25)\n    quant75 = data[i].quantile(0.75)\n    IQR = quant75 - quant25\n    med = data[i].median()\n    whis_low = quant25-(1.5*IQR)\n    whis_high = quant75+(1.5*IQR)\n\n    # Calculating Number of Outliers\n    outlier_high = len(data[i][data[i]>whis_high])\n    outlier_low = len(data[i][data[i]<whis_low])\n\n    if include_outlier == True:\n      #Plotting the variable with every information\n      plt.subplot(1,size,j+1)\n      sns.boxplot(data[i], orient=\"v\")\n      plt.ylabel('{}'.format(i))\n      plt.title('With Outliers\\nIQR = {}; Median = {} \\n 2nd,3rd  quartile = {};\\n Outlier (low\/high) = {} \\n'.format(\n                                                                                                   round(IQR,2),\n                                                                                                   round(med,2),\n                                                                                                   (round(quant25,2),round(quant75,2)),\n                                                                                                   (outlier_low,outlier_high)\n                                                                                                   ))\n      \n    else:\n      # replacing outliers with max\/min whisker\n      data2 = data[var_group][:]\n      data2[i][data2[i]>whis_high] = whis_high+1\n      data2[i][data2[i]<whis_low] = whis_low-1\n      \n      # plotting without outliers\n      plt.subplot(1,size,j+1)\n      sns.boxplot(data2[i], orient=\"v\")\n      plt.ylabel('{}'.format(i))\n      plt.title('Without Outliers\\nIQR = {}; Median = {} \\n 2nd,3rd  quartile = {};\\n Outlier (low\/high) = {} \\n'.format(\n                                                                                                   round(IQR,2),\n                                                                                                   round(med,2),\n                                                                                                   (round(quant25,2),round(quant75,2)),\n                                                                                                   (outlier_low,outlier_high)\n                                                                                                   ))","c4a794c6":"UVA_outlier(data, current_month,)","6531e6e9":"UVA_outlier(data, current_month, include_outlier=False)","32f5114f":"UVA_outlier(data, previous_month)","3a7d8633":"UVA_outlier(data, previous_month, include_outlier=False)","dc4eb0e3":"UVA_outlier(data,previous_quarters)","20e88df3":"UVA_outlier(data,previous_quarters, include_outlier = False)","684b4e50":"## 9. Univariate Analysis: Outliers","629a673e":"\nThe succeeding notebook can be found using the following link:\n    <https:\/\/www.kaggle.com\/lonewolf95\/eda-102-bivariate-analysis-hypothesis-testing>","050dcd28":"**Summary of previous_month**\n*    This looks very similar to current_month. Most of the customers perform low amount transactions.","8224b774":"## Overview:\n1. Introduction to problem statement\n2. Hypothesis generation with respect to problem statement\n3. Introduction to dataset\n4. Importing dataset and first impressions\n5. Variable Identification and Typecasting\n6. Univariate Analysis : Numerical Variables\n7. Univariate Analysis : Categorical Variables\n8. Univariate Analysis : Missing Values\n9. Univariate Analysis : Oulier Values\n10. Summary of Univariate Analysis\n\nTopics to be covered in succeeding Notebook\n* Bivariate Analysis\n* Multivariate Analysis\n* Summary of EDA","5424c5dc":"**Things to investigate further down:**\n* Gender: Do the customers with missing gender values have some common behaviour in-\n  * churn: do missing values have any relation with churn?\n\n\n\n* Dependents:\n * Missing values might be similar to zero dependents (people with 0 dependents might have left it blank)\n * churn: do missing values have any relation with churn?\n\n\n\n* Occupation:\n * Do missing values have similar behaviour among themselves(unemployed?) or to any other occupation?\n * do they have some relation with churn?\n\n\n\n* city:\n  * the respective cities can be found using branch_code\n\n\n\n* last_transaction:\n  * checking their previous month and current month and previous_quarter activity might give insight on their last transaction.","270c21e7":"Summary:\n\n*    **Customer id** are a unique number assigned to customers. It is **Okay as Integer**.\n\n*    **branch code** again represents different branches, therefore it should be **convereted to category**.\n\n*    **Age** and **Vintage** are also numbers and hence we are okay with them as integers.\n\n*    **customer_networth_category** is supposed to be an category, **should be converted to category**.\n\n*    **churn** : 1 represents the churn and 0 represents not churn. However, there is no comparison between these two categories. This **needs to be converted to category datatype**.\n","2fabb972":"**Now to visualise the variable groups at once with all the necessary descriptives, let's define a universal\/reusable function to do that.**","481e134c":"**Summary:**\nfor both variable \"city\" and \"branch_code\", there are too many categories. There is clear relation that some branches and cities are more popular with customers and and this trend decreases rapidly.\n\n**Things to investigate further Down**\n* Popular cities and branch code might be able to explain the skewness and outliers of credit\/debit variables.\n* Possibility that cities and branch code with very few accounts may lead to churning.","23097e2e":"### customer_info","d7211a45":"### previous_quarters","bf21a898":"Now considering that we have 18 numerical variables and 5 properties associated with each, Performing univariate analysis can be tiresome. For this reasonn, it is always wise to form a cluster\/group of variables which are similar to each other in nature. The variables can be grouped in many different ways.\n\nIn this EDA I am grouping variable into 5 groups:\n* customer_etails\n* current_month\n* previous_month\n* previous_quarters\n* transaction_date","11852414":"### datetime Data Type","2a138007":"### Float Data Type","e72ec533":"**Summary of current_month**\n*    After Removing extreme\/outliers, plots are still very skewed.\n\n**Things to investigate further down**\n1.    **Is there thete any common trait\/relation between the customers who are performing high transaction credit\/debits?**\n2.    **Customers who are performinng high amount of transactions, are they doinng it every month?**","c39d882d":"**Summary**\n* Occupation\n  * Majority of people are self_employed.\n  * There are extremely few Company Accounts. Might explain Outlier\/Extreme values in credit\/debit.\n\n* Gender:\n  *  Males accounts are 1.5 times more than Female Accounts.\n\n* customer_nw_category:\n  *  Half of all the accounts belong to the 3rd net worth category.\n  *  Less than 15% belong to the highest net worth category.\n\n**Things to investigate further down:**\n* Possibility: Company accounts are the reason behind the outlier transactions.\n* Possibility: customers belonging to the highest net worth category may explain the skewness of the transactions.","f6448844":"### previous quarters","16f586f5":"The first column is the complete date of the last transaction which was done by any given customer.\n\nThe next columns represent the day of year, week of year, month of year, day of week when the last transaction was done.\n\n**Breaking down the date variable** into these granular information will **help us in understand when the last transaction was done from different perspectives**. Now that we have extracted the essentials from the last_transaction variables, we will drop it from the dataset.\n\n","8508ad55":"## Objective: To demonstrate structured format of Performing Exploratory data Analysis.\n","9d53d379":"**Data is fairly small with 28K rows observations, although the number of columns are 21**","bd7f6ec1":"**Grouping Varibales**\n\nSimilar to what we did in the numerical section, we will form group of variables for the categorical variables. Moreover we will define a reusable function to visualise things.\n\n* **customer_info**: gender, occupation, customer_nw_category\n* **account_info**: city, branch_code\n* **churn**","875ec8c5":"**Previous output cell verifies the change in data types of dependents and city variables.**","6932508a":"### account_info","9cf285f4":"Summary:\n\n*    **dependents** is expected to be a whole number. **Should be changed to integer type**\n\n*    **city** variable is also a unique code of a city represented by some interger number. **Should be converted to Category type**\n\n*    Rest of the variables like **credit, balance and debit** are best represented by the float variables.","93c36a0d":"**Summary**\n*    Considering the kurtosis and skewness value  for all 4 of these plots. Outliers\/Extreme values are obvious.","d35f4a96":"**Summary of Customer_Information:**\n\n*    **customer_id**:\n     *    variable is **unique for every customer, Hence uniform** distribution.\n     * This variable **does not contribute any information**\n     * Can be eliminated from data\n\n\n*    **age**:\n    *    Median Age = 46\n    *    **Most customers age between 30 to 66**\n    *    skewness +0.33 : customer age is **negligibly biased towards younger age**\n    *    **kurtosis = -0.17**; very less likely to have extreme\/outlier values.\n\n\n*    **vintage:**\n    *    Most customers joined between 2100 and 2650 days from the day of data extraction.\n    *    **skewness** -1.42 : this is left skewed, **vintage variable is significantly biased towards longer association of customers.**\n    *    **Kurtosis = 2.93**: Extreme values and Outliers are very likely to be present in vintage.\n    *    Most of the customers are old, did rate of new customers decayed over time?\n\n\n\n**Things to Investigate Further down the road:**\n*    The batch of **high number of very Old Age customers** in age variable.\n","2f4e18e0":"**This verifies that the all the variables are present as claimed in the data dictionary**","90f64dc8":"## 7. Univariate Analysis : Categorical Variables","333332ce":"### current_month","047d972f":"**Summary:**\n* If we look at corresponding plots in the outputs above, there seems to be a strong relation between the corresponding plots of previous_month and current_month variables.\n\n* Outliers are significant in number and very similar in number between corresponding plots. Which indicates some inherent undiscovered behviour of Outliers.","2cd36078":"### Integer Data Type","0b2df0f5":"*    **gender** and **occupation** variables **belong to categorical data types**.\n*    **last_transaction** should be a  **datetime variable**.","2235b628":"**Summary**\n* Number of people who churned are 1\/4 times of the people who did not churn in the given data.","6ff70cc9":"### previous_month","ce5f514e":"**Summary**\nThe general trend still follows, it is crutial that we find the out if there is any common trait between the customers doing high high amount of transactions.","3afb808d":"\n**Need to Remove Outliers to visualise these plots**\nAS the bult distribution is not visible, let's trim down the outliers using the rule of standard deviation","af612bcd":"### transaction_date","fdb1e058":"## 4. Reading Files into Python And first Impressions","f4235342":"## 6. Univariate Analysis: Numerical Variables\nWhen dealing with numerical variables, we have to check their properties like:\n* Mean \n* Median \n* Standard Deviation \n* Kurtosis\/skewness\n* distribution\/range","0933c5c3":"**Now that the stage is set, let's perform the univariate analysis and learn what we can.**","156fc3e7":"## 2. Hypothesis Generation for the problem statement:\n**Hypothesis generation is about preparing an exhaustive list of questions or possibilities which directly or indirectly affect the problem statement or the target variablw. It is a very important step as it prevents us from going down for a wild goose chase during EDA. It narrows down the process of performing EDA to the most essential aspects.**\n\n**This step is performed before looking\/gathering dataset**\n\nTo generate hypothesis, we require the following:\n1. Common Sense or Rationality\n2. Domain knowledge if possible\n3. Communication with domain experts\n\nGiven below are the hypothesis we will be working with in this EDA\n\n**On basis of Demographics**\n1. Are females less likely to churn than males?\n2. Are young customers more likely to churn?\n3. Are customers in the lower income bracket more likely to churn?\n4. Are customers with dependent(s) less likely to churn?\n5. Customers with an average family size less than 4 are more likely to churn?\n\n**On the basis of customer behaviour**\n1. Are vintage customers less likely to churn?\n2. Are customers with higher average balance less likely to churn?\n3. Are customers dropping monthly balance highly likely to churn?\n4. Are customers with no transaction is the last 3 months more likely to churn?\n5. Are customers who have large withdrawal amounts in the last month more likely to churn?\n6. Are customers who have large withdrawal amounts in the last quarter more likely to churn?\n7. Customers who have not engaged with the bank in the last quarter are more likely to churn?","5e2f7e52":"**So we can finally see that all the variables are now assigned their respective datatypes.**","72503e90":"**This veriffies that the data was imported successfully (no abnormal absurd entries).**","50dc4b62":"### current_month and previous_month","a790f7be":"**The previous output verifies our conversion**","ab840169":"**Summary**\n*    **Day_of_Year**:\n    *    most of the last transactions were made in the last 60 days from the extraction of data.\n    *    There are transactions which were made also an year ago.\n\n*   **Week_of_year and Month_of_year**: these variable validate the findings from the **day_of_year**.\n*    **Day_of_Week**: Tuesdays are often the favoured day relative to others.\n\n**Things to investigate further Down**\n*    **Customers whose last transaction was 6 months ago, did all of them churn?**","0423b69f":"**We suspected outliers in current_month and previous_month variable groups. We will verify that using box plots**","feaabc80":"## 5. Variable Identification and Typecasting\n\n**This is one of the most important steps, Why?**\n\n**Because pandas is not very good when it comes to recognising the datatype of theimported variables. So in this section, we will be analysing the datatypes of each variables and converting them to respective types.**","32e7f01e":"### customer_information","f8f0896d":"## 1. Introduction to problem statement:\nA Bank wants to take care of customer retention for their product; savings accounts. The bank wants you to identify customers likely to churn balances below the minimum balance. You have the customers information such as age, gender, demographics along with their transactions with the bank. Your task as a data scientist would be to predict the propensity to churn for each customer.","214dfe35":"*    **variables like 'gender', 'occupation' and 'last_transaction' are of type object**. This means that **Pandas was not able to recognise the datatype** of these three variables.","85b5a3e9":"### churn","ca07afbf":"## 10. Summary of Univariate Analysis:\n\n### Investigation directions (for bivariate\/multivariate)\n1. customer_id variable can be dropped.\n2.  Is there there any common trait\/relation between the customers who are performing high transaction credit\/debits?\n   * customer_nw_category might explain that.\n   * Occupation = Company might explain them\n   * popular cities might explain this\n4.  Customers whose last transaction was 6 months ago, did all of them churn? \n5. Possibility that cities and branch code with very few accounts may lead to churning.\n\n\n### Some Insights\n1. Most of the customers lie in the Age between 30-66, but there is also significant customers who are very old(age>85)\n2. Major bulk of the customers opened their account more than 4 years ago! (did customer signups detiorated in recent times?)\n3. Major bulk of customers did their last transaction within last 100 days.\n4. Majority of customers perform small scale transactions. But there are few who perform transactions of huge amounts, consistently.\n","46954a49":"Summary:\n* Outliers in previous two quarters are significantly large but very similar in number.","cccc9b76":"There are a lot of variables visible at one, so let's narrow this down by looking **at one datatype at once**. We will start with **int**\n","1fdacb94":"## 8. Univariate: Missing Values\n**Missing values could be due to several reasons.**\n\nHuman error\n\nPrivacy issues etc","45518b00":"## 3. Introduction to Dataset\n\nThere are multiple variables in the dataset which can be cleanly divided in 3 categories:\n\n### Demographic information about customers\n\n<b>customer_id<\/b> - Customer id\n\n<b>vintage<\/b> - Vintage of the customer with the bank in number of days\n\n<b>age<\/b> - Age of customer\n\n<b>gender<\/b> - Gender of customer\n\n<b>dependents<\/b> - Number of dependents\n\n<b>occupation<\/b> - Occupation of the customer \n\n<b>city<\/b> - City of customer (anonymised)\n\n\n### Bank Related Information for customers\n\n\n<b>customer_nw_category<\/b> - Net worth of customer (3:Low 2:Medium 1:High)\n\n<b>branch_code<\/b> - Branch Code for customer account\n\n<b>days_since_last_transaction<\/b> - No of Days Since Last Credit in Last 1 year\n\n\n### Transactional Information\n\n<b>current_balance<\/b> - Balance as of today\n\n<b>previous_month_end_balance<\/b> - End of Month Balance of previous month\n\n\n<b>average_monthly_balance_prevQ<\/b> - Average monthly balances (AMB) in Previous Quarter\n\n<b>average_monthly_balance_prevQ2<\/b> - Average monthly balances (AMB) in previous to previous quarter\n\n<b>percent_change_credits<\/b> - Percent Change in Credits between last 2 quarters\n\n<b>current_month_credit<\/b> - Total Credit Amount current month\n\n<b>previous_month_credit<\/b> - Total Credit Amount previous month\n\n<b>current_month_debit<\/b> - Total Debit Amount current month\n\n<b>previous_month_debit<\/b> - Total Debit Amount previous month\n\n<b>current_month_balance<\/b> - Average Balance of current month\n\n<b>previous_month_balance<\/b> - Average Balance of previous month\n\n<b>churn<\/b> - Average balance of customer falls below minimum balance in the next quarter (1\/0)","239aa3d0":"**Last cell verifies that gender and occupation variables have been converted successfully**","3e96886e":"### Object Data Type"}}