{"cell_type":{"34c7fa3d":"code","986d5640":"code","819725d1":"code","135b90c1":"code","d5dff91c":"code","9966ff06":"code","ba7fa6b5":"code","3853477f":"code","a7972fb2":"code","486673bf":"code","15478ac4":"code","54be5de4":"code","ae78c107":"code","1ca1ec5f":"code","23a1d492":"code","8096e38b":"code","5d26633e":"code","7d4919b6":"code","23a6592f":"code","617f5ece":"code","40fe2361":"code","ebcd3349":"code","78a97eb8":"code","74b3d417":"code","e4c59d08":"code","5962fbe9":"code","9123d300":"code","a0b68d37":"code","22fa0179":"code","703b460a":"code","b332178b":"code","9a270d8d":"code","d2bc73d5":"code","aa7df38e":"code","fc8de57e":"code","34c03fa6":"code","1cd8ba67":"code","4939adc9":"markdown","787549af":"markdown","0ccdb953":"markdown","b15c3af4":"markdown"},"source":{"34c7fa3d":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","986d5640":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","819725d1":"df.head()\n","135b90c1":"df.isnull().sum()","d5dff91c":"df.shape","9966ff06":"df['keyword'].unique().size","ba7fa6b5":"df['location'].unique().size","3853477f":"data  = df.iloc[:,3:]","a7972fb2":"data.head()","486673bf":"sentences = data['text'].tolist()","15478ac4":"labels = data['target'].tolist()","54be5de4":"import tensorflow as tf\nimport tensorflow.keras as k","ae78c107":"from tensorflow.keras.preprocessing.text import Tokenizer","1ca1ec5f":"from tensorflow.keras.preprocessing.sequence import pad_sequences","23a1d492":"max = 0\nfor i in sentences:\n    if len(i)>max:\n        max = len(i)","8096e38b":"print(max)","5d26633e":"vocab = 80000\noov = '<OOV>'\nembedding = 32\npadding = 'post'\ntruncate = 'post'\nmaxlength = max","7d4919b6":"ratio = 0.8*len(sentences)\nratio = int(ratio)\n# print(ratio)\ntrain = sentences[0:ratio]\ntrain_label = labels[0:ratio]\nval = sentences[ratio:]\nval_labels = labels[ratio:]","23a6592f":"tokenizer =Tokenizer(num_words = vocab, oov_token=oov)\ntokenizer.fit_on_texts(train)\nword_index = tokenizer.word_index\ntraining = tokenizer.texts_to_sequences(train)\ntraining_pad = pad_sequences(training, maxlen=maxlength, padding=padding, truncating=truncate)\n\nvalidation = tokenizer.texts_to_sequences(val)\nvalidation_pad = pad_sequences(validation, maxlen=maxlength, padding=padding, truncating=truncate)","617f5ece":"test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest_sentences = test['text'].tolist()\ntest_sen_token = tokenizer.texts_to_sequences(test_sentences)\ntest_pad = pad_sequences(test_sen_token, maxlen=maxlength, padding=padding, truncating=truncate)\n","40fe2361":"tf.keras.backend.clear_session()\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab, embedding, input_length=maxlength),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(10, activation='relu'),\n#      tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid'),\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","ebcd3349":"val_labels = np.array(val_labels)\ntrain_label = np.array(train_label)","78a97eb8":"model_check = tf.keras.callbacks.ModelCheckpoint('model.h5',save_best_only = True)","74b3d417":"\nnum_epochs = 20\nhistory = model.fit(training_pad, train_label, epochs=num_epochs, validation_data=(validation_pad, val_labels),callbacks = [model_check])","e4c59d08":"# Plotting Learing Curves\nimport matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","5962fbe9":"#After selecting the optimal hyperparameters, now training the model again with the entire train dataset.\n\ntf.keras.backend.clear_session()\nvocab = 120000\noov = '<OOV>'\nembedding = 32\npadding = 'post'\ntruncate = 'post'\nmaxlength = max\n\nmodel_check = tf.keras.callbacks.ModelCheckpoint('model.h5',save_best_only = True)\n\ntokenizer =Tokenizer(num_words = vocab, oov_token=oov)\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\nsentences = tokenizer.texts_to_sequences(sentences)\nsentences_pad = pad_sequences(sentences, maxlen=maxlength, padding=padding, truncating=truncate)\nlabels = np.array(labels)\n\n# validation = tokenizer.texts_to_sequences(val)\n# validation_pad = pad_sequences(validation, maxlen=maxlength, padding=padding, truncating=truncate)\n\nnum_epochs = 10\nhistory = model.fit(sentences_pad,labels, epochs=num_epochs, verbose = 0,callbacks = [model_check])","9123d300":"model =  tf.keras.models.load_model('model.h5')","a0b68d37":"sample = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","22fa0179":"sample.head()","703b460a":"predictions = model.predict(test_pad)","b332178b":"pred = []\nfor i in range(0,len(predictions)):\n    if predictions[i][0]>0.5:\n         pred.append(1)\n    else:\n        pred.append(0)","9a270d8d":"test['target'] = pd.Series(pred)","d2bc73d5":"test.head()","aa7df38e":"submit = pd.DataFrame()\nsubmit['id'] = test['id']","fc8de57e":"submit['target'] = test['target']","34c03fa6":"submit.head()","1cd8ba67":"submit.to_csv('Final_Submit.csv',index = False)","4939adc9":"# Preparing data for model trainning","787549af":"# Loading Data ","0ccdb953":"# Trainning the Model","b15c3af4":"# Generating Predictions"}}