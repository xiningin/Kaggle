{"cell_type":{"553b8390":"code","f3cb3db1":"code","6c7bf2a2":"code","6e4c796a":"code","fa1a64eb":"code","8dd39655":"code","342e3216":"code","e133c6a8":"code","d0b17ed6":"code","5eb86c48":"code","e4914952":"code","dc4d5b7a":"code","62512414":"code","9e92c707":"code","7e4b4b7d":"code","0bdcd2b1":"code","adcac64f":"code","bd34ca8d":"code","225be599":"code","dae0d97a":"code","c29d8cf1":"code","561f056f":"code","14aecc66":"code","69cc4335":"code","c2b7d6d1":"code","49091613":"code","7072dbf2":"code","8d419e62":"code","4afdd74c":"code","016441eb":"code","6fa4de95":"code","a6a7d542":"code","450898e8":"code","ee7a957a":"code","b1519479":"code","414d216c":"code","381b2dd4":"code","5bfe8b28":"code","f4e04b45":"code","668305e8":"code","72edffd5":"code","1b1345dd":"code","d2bdde3d":"code","b7be5aa8":"markdown","1378abd6":"markdown","75a5318e":"markdown","ad61d129":"markdown","40f9993b":"markdown","620bd211":"markdown","a4a6fe15":"markdown","b3fad525":"markdown","d592854a":"markdown","24bd5fd6":"markdown","89b85ce5":"markdown","9379362e":"markdown","b5cabd78":"markdown","0416b7e3":"markdown","7e6f1cdb":"markdown","7137f246":"markdown","45fd0202":"markdown","9a308dd4":"markdown","a0f8b21e":"markdown","8ff8ef47":"markdown","a8e1f289":"markdown","6845ce7b":"markdown","bb1cc246":"markdown"},"source":{"553b8390":"import numpy as np\nimport pandas as pd\nimport os\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import max_error\nfrom sklearn.metrics import mean_squared_log_error","f3cb3db1":"housing = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nhousing.info()","6c7bf2a2":"housing = housing.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'],axis=1)","6e4c796a":"housing.isna().sum()","fa1a64eb":"housing.describe().T","8dd39655":"fig, ax = plt.subplots(figsize=(14,14))\nsns.heatmap(housing.corr(), cmap=\"Blues\");","342e3216":"def select_cols_corr(df_corr, target_col, min_corr, max_corr): \n    #creating df target_corr\n    target_corr = df_corr[target_col].reset_index()\n    return target_corr.loc[(target_corr.iloc[:,1] < max_corr) & (target_corr.iloc[:,1] > min_corr),:]\n\nselect_cols_corr(housing.corr(), 'SalePrice', min_corr=.4, max_corr=.95)","e133c6a8":"num_col = select_cols_corr(housing.corr(), 'SalePrice', min_corr=.4, max_corr=.95).iloc[:,0].tolist()","d0b17ed6":"cat_col = housing.select_dtypes(include=[\"object\"]).columns.to_list()","5eb86c48":"housing.drop(housing.columns.difference(cat_col + num_col + ['SalePrice']), axis=1, inplace=True)","e4914952":"X = housing.drop(columns=['SalePrice'], axis=1)\ny = housing['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.75, random_state=8)\n\nX_train = pd.DataFrame(X_train, columns=X.columns)\nX_test = pd.DataFrame(X_test, columns=X.columns)","dc4d5b7a":"# Categorical pipeline - filling the missing values in categorical columns using SimpleImputer and OneHotEncoder. \ncat_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('one_hot_enc', OneHotEncoder(drop='first'))\n])\n\n# Numerical pipeline - filling the missing values in numerical columns using SimpleImputer and scaling with MinMaxScaler to preserve the shape of the original distribution. \nnum_pipeline = Pipeline(steps=[\n    ('num_impute', SimpleImputer(strategy='mean')),\n    ('scale', MinMaxScaler())\n])\n\nfull_processor = ColumnTransformer(transformers=[\n    ('number', num_pipeline, num_col), \n    ('category', cat_pipeline, cat_col)\n])\n\nlin_model_pipeline = Pipeline(steps=[\n    ('processor', full_processor), \n    ('model', LinearRegression())\n])\n\nlm = lin_model_pipeline.fit(X_train, y_train)","62512414":"lin_model_pipeline.named_steps.model.get_params()","9e92c707":"def plot_predictions(y_true, y_pred):\n    print(\n        f\"\"\"\n        MSE: {mean_squared_error(y_true, y_pred)}\n        RMSE: {mean_squared_error(y_true, y_pred)**0.5}\n        MAE: {mean_absolute_error(y_true, y_pred)}\n        R_SQR: {r2_score(y_true, y_pred)}\n        EXV: {explained_variance_score(y_true, y_pred)}\n        ME: {max_error(y_true, y_pred)}\n        RMSLE: {mean_squared_log_error(y_true, y_pred)**0.5}\n        \"\"\"\n    )\n    max_preds = min([max(y_pred.tolist()), max(y_true.tolist())])\n    min_preds = max([min(y_pred.tolist()), min(y_true.tolist())])\n    print(max_preds, min_preds)\n    # plot\n    plt.figure(figsize=(8,8))\n    sns.scatterplot(x=y_pred, y=y_true)\n    sns.lineplot(x=[min_preds,max_preds], y=[min_preds, max_preds], color='red')\n    plt.ylabel('Reference')\n    plt.xlabel('Predictions')\n    plt.show()\n    \n    errors = y_pred - y_true\n    plt.subplots(figsize=(10, 6))\n    sns.histplot(errors)\n    plt.vlines(x = 0, ymin = 0, ymax = 140, color = 'red')\n    plt.show()\n    \n    p_df = (\n        pd.DataFrame({'y_true':y_true, 'y_pred':y_pred})\n        .assign(error = lambda x: x['y_pred'] - x['y_true'])\n        .sort_values(by = 'y_true')\n        )\n    \n    plt.subplots(figsize = (10, 6))\n    sns.scatterplot(data=p_df, x = 'y_true', y = 'error')\n    plt.hlines(y = 0, xmin = 0, xmax = 700000, color = 'red')\n    plt.show()\n    \nplot_predictions(y_train, lin_model_pipeline.predict(X_train))","7e4b4b7d":"lm_test = lin_model_pipeline.fit(X_test, y_test)\n\nresults = pd.DataFrame({\n    'prediction': lin_model_pipeline.predict(X_test), \n    'true_value': y_test\n})\n\nresults.head()","0bdcd2b1":"plot_predictions(y_test, lin_model_pipeline.predict(X_test))","adcac64f":"coef_lr = pd.DataFrame(lin_model_pipeline['model'].coef_)\ncoef_lr","bd34ca8d":"intercept_lr = lin_model_pipeline['model'].intercept_\nintercept_lr","225be599":"rf_pipeline = Pipeline(steps=[\n    ('processor', full_processor), \n    ('model', RandomForestRegressor())\n])\n\nrf = rf_pipeline.fit(X_train, y_train)","dae0d97a":"plot_predictions(y_train, rf_pipeline.predict(X_train))","c29d8cf1":"rf_test = rf_pipeline.fit(X_test, y_test)\n\nresults = pd.DataFrame({\n    'prediction': rf_pipeline.predict(X_test), \n    'true_value': y_test\n})\n\nresults.head()","561f056f":"plot_predictions(y_test, rf_pipeline.predict(X_test))","14aecc66":"ridge_pipeline = Pipeline(steps=[\n    ('processor', full_processor), \n    ('model', Ridge())\n])\n\nrd = ridge_pipeline.fit(X_train, y_train)","69cc4335":"plot_predictions(y_train, ridge_pipeline.predict(X_train))","c2b7d6d1":"rd_test = ridge_pipeline.fit(X_test, y_test)\n\nresults = pd.DataFrame({\n    'prediction': ridge_pipeline.predict(X_test), \n    'true_value': y_test\n})\n\nresults.head()","49091613":"plot_predictions(y_test, ridge_pipeline.predict(X_test))","7072dbf2":"lasso_pipeline = Pipeline(steps=[\n    ('processor', full_processor), \n    ('model', Ridge())\n])\n\nls = lasso_pipeline.fit(X_train, y_train)","8d419e62":"plot_predictions(y_train, lasso_pipeline.predict(X_train))","4afdd74c":"ls_test = lasso_pipeline.fit(X_test, y_test)\n\nresults = pd.DataFrame({\n    'prediction': lasso_pipeline.predict(X_test), \n    'true_value': y_test\n})\n\nresults.head()","016441eb":"plot_predictions(y_test, lasso_pipeline.predict(X_test))","6fa4de95":"gbr_pipeline = Pipeline(steps=[\n    ('processor', full_processor), \n    ('model', GradientBoostingRegressor())\n])\n\nrd = gbr_pipeline.fit(X_train, y_train)","a6a7d542":"plot_predictions(y_train, gbr_pipeline.predict(X_train))","450898e8":"gbr_test = gbr_pipeline.fit(X_test, y_test)\n\nresults = pd.DataFrame({\n    'prediction': gbr_pipeline.predict(X_test), \n    'true_value': y_test\n})\nresults.head()","ee7a957a":"plot_predictions(y_test, gbr_pipeline.predict(X_test))","b1519479":"rf_pipeline.fit(X_train, np.log(y_train))\nplot_predictions(y_train, np.exp(rf_pipeline.predict(X_train)))","414d216c":"gbr_pipeline.fit(X_train, np.log(y_train))\nplot_predictions(y_train, np.exp(gbr_pipeline.predict(X_train)))","381b2dd4":"house_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nhouse_test.info()","5bfe8b28":"house_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nhouse_test.info()","f4e04b45":"final_submission = house_test.drop(house_test.columns.difference(cat_col + num_col), axis=1)","668305e8":"gbr_pipeline.fit(X, np.log(y))","72edffd5":"SalePricePreds = np.exp(gbr_pipeline.predict(final_submission))","1b1345dd":"submission_file = pd.DataFrame({\n    'Id': house_test['Id'],\n    'SalePrice': SalePricePreds\n}).to_csv('submission.csv', index=None)","d2bdde3d":"house_submission = pd.read_csv(\"submission.csv\")\nhouse_submission","b7be5aa8":"* some columns have missing values, which will be fitted using ```SimpleImputer``` and ```OneHotEncoder``` while creating the pipeline. ","1378abd6":"## Select numerical columns with higher correlation to ```SalePrice```. Will use these numerical columns for further exploration, which provides more precise information about pricing strategy. ","75a5318e":"## Ridge Regression","ad61d129":"I will check different ```regressions``` and ```performance metrics``` to evaluate the best model for house price prediction:\n\nBelow ```performance metrics``` will be computed: \n\n* Mean Absolute Error (MAE): ```absolute``` value of the difference between the ```predicted``` value and the ```true``` value. Tells us how big of an error we can expect from the forecast on average.\n* Mean Squared Error (MSE): average ```squared difference``` between the ```predicted``` values and the ```true``` value.\n* Root Mean Squared Error (RMSE): estimator measuring the ```quality of the fit``` of the model. ```Small``` RMSE means ```predicted``` value to be close to ```true``` values.\n* R\u00b2 score (R_SQR): proportion of the variance for a ```dependent``` variable that's explained by an ```independent``` variable. Range between ```0``` and ```1```.\n* Explained variance score: computes the explained ```variance regression score```. The best possible score is ```1.0```, lower values are worse.\n* Max error: computes the ````maximum residual error````, a metric that captures the worst case error between the ```predicted``` value and the ```true``` value. \n* Root Mean Squared Logarithmic Error (RMSLE): computes a risk metric corresponding to the ```predicted``` value of the squared logarithmic (quadratic) error or loss.","40f9993b":"## LinearRegression","620bd211":"* RGB Regression","a4a6fe15":"# Load final submission file ","b3fad525":"## Logarithmic  transformation","d592854a":"## GradientBoostingRegressor","24bd5fd6":"# Create Pipeline","89b85ce5":"# Split the dataset","9379362e":"* Droping the columns with signifficant amout of null values due to irrelevance. ","b5cabd78":"* Split dataset into train and test datasets. Try the model on train set and implemetn on test set, to check the accuracy. ","0416b7e3":"# Loading the test data","7e6f1cdb":"## RandomForestRegressor","7137f246":"## Fit the model","45fd0202":"## Lasso Regression","9a308dd4":"* Random Forest Regression","a0f8b21e":"### Droping all columns except for numerical, categorical and 'SalePrice'","8ff8ef47":"# Loading the train data and initial exploration","a8e1f289":"## Looking to correlation between the columns to see which columns can cause collinearity or multicollinearity","6845ce7b":"### Create the DF and save for submission fie ","bb1cc246":"# **House Price Prediction**"}}