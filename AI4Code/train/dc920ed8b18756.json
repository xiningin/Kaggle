{"cell_type":{"340f8448":"code","79a2f795":"code","b6af09a6":"code","cd4b0bc8":"code","aea680f9":"code","74646a50":"code","fe70d8af":"code","2441a811":"code","4f10af03":"code","cd5380ff":"code","039c2d8e":"code","d8a48257":"code","053b7b1f":"code","0cfe3a4a":"code","508c3707":"code","ebc1ce2a":"code","bbceead8":"code","62d2b987":"code","152599d5":"code","e6e22bdd":"code","74128f13":"code","c69e86ff":"code","55ea2935":"code","89dec505":"code","afaef5b1":"code","baa63828":"code","72d17649":"code","b08af74d":"code","a6eb8c4d":"code","bfcd37b4":"code","603be66b":"code","870590c2":"code","98aa58f7":"code","b80e1d1c":"code","ee15917f":"code","7dd130a6":"code","99fadd7b":"code","e5171032":"code","df885700":"code","b7159508":"code","02b4725e":"code","ef017c01":"code","e42709d0":"code","caad9a2a":"code","bf0e90b7":"code","3029aec5":"code","fcde44bc":"code","15d73ac4":"code","c6981e18":"code","fcfec3f9":"code","addba57c":"code","5e269d43":"code","728a38be":"code","8ee1287b":"code","5a9f961a":"code","57957c48":"code","e25bfc0e":"code","eef555cc":"code","810e8ffe":"code","7471e563":"code","c09aa138":"code","aa151afe":"code","ebb49fdc":"code","22e21789":"code","c59db3f0":"code","4c081286":"code","842a7765":"code","1dabd6ba":"code","d155f21a":"code","153202a7":"code","e75322ed":"code","12513afb":"code","c0f7512e":"code","ac7cbaf2":"code","c4adda5d":"code","53bbab99":"code","e277dec9":"code","3cb819cf":"code","c6606681":"code","0fe8f690":"code","ec4dae0b":"code","940a6377":"code","275e2597":"code","8dd76559":"code","8c6034e1":"code","bec4d1ee":"code","5310c5e1":"code","1b0995cd":"code","be44d2d7":"code","ce996c51":"code","c4c4e0bb":"code","89443009":"code","b7237deb":"code","745c78b9":"code","62dc30d6":"code","5d69f70e":"code","739e6148":"code","7047ec45":"code","28dcd52b":"code","fef18153":"code","5b6b37fb":"code","6a7269d5":"code","68433149":"code","f2076940":"code","53ffe92f":"code","47c231af":"code","e71afbf1":"code","6c9596b6":"code","47e30c80":"code","8ecae6bb":"code","cdb9ae81":"code","c85e64dc":"code","de7271fc":"code","57f9b4fe":"code","637909aa":"code","0b8f73a1":"code","c556cc0e":"code","95497314":"code","f53fe1af":"code","8d74e160":"code","0feecb13":"code","5a4d465b":"code","eef5feae":"code","01fc05c0":"markdown","9764d653":"markdown","0542df6a":"markdown","f5aa888e":"markdown","fd447cf9":"markdown","cfca2f4b":"markdown","56432fb6":"markdown","ec1bbdc1":"markdown","b814f8ad":"markdown","a1afbfec":"markdown","cec8cec2":"markdown","94b2cdd7":"markdown","52418bd3":"markdown","2b4bd300":"markdown","389cc560":"markdown","13bf4cbe":"markdown","c1ee6b26":"markdown","2f16c38b":"markdown","33fa465d":"markdown","d059e407":"markdown","aaa09b51":"markdown","9a7f9ca8":"markdown"},"source":{"340f8448":"import re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='dark')\nimport plotly.offline as py\nfrom plotly import tools\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,BaggingRegressor, StackingRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nle = LabelEncoder()\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error, r2_score\nimport os","79a2f795":"#read  files \nTrain=pd.read_csv('..\/input\/machinehack-buyers-time-prediction-challenge\/ParticipantData_BTPC\/Train.csv')\nTest=pd.read_csv('..\/input\/machinehack-buyers-time-prediction-challenge\/ParticipantData_BTPC\/Test.csv')\nSample=pd.read_csv('..\/input\/machinehack-buyers-time-prediction-challenge\/ParticipantData_BTPC\/Sample Submission.csv')","b6af09a6":"#pd.set_option('display.max_colwidth', None) #to see full column values #","cd4b0bc8":"pd.concat((Train.nunique(), Test.nunique()), axis = 1)","aea680f9":"Train.head().T","74646a50":"Test.head().T","fe70d8af":"Sample.head().T","2441a811":"Train.info()","4f10af03":"Test.info()","cd5380ff":"print(Train.isnull().any())","039c2d8e":"print(Train.client_agent.isnull().sum())","d8a48257":"print(Test.isnull().any())","053b7b1f":"print(Test.client_agent.isnull().sum())","0cfe3a4a":"#convert time spent to log transform\nTrain['time_spent']= np.log1p(Train['time_spent'])\n","508c3707":"Train['time_spent'].head()","ebc1ce2a":"Train['time_spent'].plot()","bbceead8":"df = pd.concat([Train, Test], axis = 0).reset_index(drop=True)\ndf.shape","62d2b987":"df['purchased']=df['purchased'].astype('bool')\ndf['added_in_cart']=df['added_in_cart'].astype('bool')\ndf['checked_out']=df['checked_out'].astype('bool')\ndf['date']=pd.to_datetime(df['date'])\ndf.head().T","152599d5":"#cleaning the text and extracting product details starting and ending sequence\ndf['client_agent']=df['client_agent'].replace(' ',',',regex=True)\ndf['client_agent']=df['client_agent'].str.replace(r\"\\s*\\([^()]*\\)\",\"\").str.strip()\ndf['client_agent']=df['client_agent'].str.strip('[]')\ndf['client_agent']=df['client_agent'].replace('\/', '',regex=True)\ndf['client_agent']=df['client_agent'].replace(':', '',regex=True)\ndf['client_agent']=df['client_agent'].replace(';',',',regex=True)\ndf['client_agent']=df['client_agent'].replace(',,',',',regex=True)\ndf['client_agent']=df['client_agent'].str.lower()\ndf['client_agent'].fillna('0', inplace=True)\ndf['client_agent'].isnull().any()","e6e22bdd":"df['SW']=df['client_agent'].apply(lambda x : x.split(',')[0])\ndf['product']=df['client_agent'].apply(lambda x : x.split(',')[-1])","74128f13":"df['device']= df['device_details'].apply(lambda x : x.split('-')[0])\ndf['Browser']= df['device_details'].apply(lambda x : x.split('-')[-1])","c69e86ff":"df = df.assign(\n    year        = lambda df: df['date'].dt.year,\n    month       = lambda df: df['date'].dt.month,\n    day         = lambda df: df['date'].dt.day,\n    weekday     = lambda df: df['date'].dt.dayofweek,\n    Weekend_FLG = lambda df: df['weekday'].apply(lambda day: '1' if day in [5,6] else '0'),\n    Quater      = lambda df: df['date'].dt.quarter\n)","55ea2935":"df.isnull().any()","89dec505":"df.head().T","afaef5b1":"df.info()","baa63828":"df.nunique()","72d17649":"df['time_spent'].describe()","b08af74d":"df['min_timespent_per_month']=df.groupby('month')['time_spent'].transform('min')\ndf['max_timespent_per_month']=df.groupby('month')['time_spent'].transform('max')\ndf['mean_timespent_per_month']=df.groupby('month')['time_spent'].transform('mean')\ndf['median_timespent_per_month']=df.groupby('month')['time_spent'].transform('median')","a6eb8c4d":"df['min_timespent_per_Quarter']=df.groupby('Quater')['time_spent'].transform('min')\ndf['max_timespent_per_Quarter']=df.groupby('Quater')['time_spent'].transform('max')\ndf['mean_timespent_per_Quarter']=df.groupby('Quater')['time_spent'].transform('mean')\ndf['median_timespent_per_Quarter']=df.groupby('Quater')['time_spent'].transform('median')\n","bfcd37b4":"df['min_timespent_per_weekday']=df.groupby('weekday')['time_spent'].transform('min')\ndf['max_timespent_per_weekday']=df.groupby('weekday')['time_spent'].transform('max')\ndf['mean_timespent_per_weekday']=df.groupby('weekday')['time_spent'].transform('mean')\ndf['median_timespent_per_weekday']=df.groupby('weekday')['time_spent'].transform('median')","603be66b":"#search unique values in train and test \nTrain['device']= Train['device_details'].apply(lambda x : x.split('-')[0])\nTrain['Browser']= Train['device_details'].apply(lambda x : x.split('-')[-1])\nTest['device']= Test['device_details'].apply(lambda x : x.split('-')[0])\nTest['Browser']= Test['device_details'].apply(lambda x : x.split('-')[-1])","870590c2":"Train['device'].value_counts()","98aa58f7":"Test['device'].value_counts()","b80e1d1c":"Train['Browser'].value_counts()","ee15917f":"Test['Browser'].value_counts()","7dd130a6":"df['min_timespent_per_device']=df.groupby('device')['time_spent'].transform('min')\ndf['max_timespent_per_device']=df.groupby('device')['time_spent'].transform('max')\ndf['mean_timespent_per_device']=df.groupby('device')['time_spent'].transform('mean')\ndf['median_timespent_per_device']=df.groupby('device')['time_spent'].transform('median')","99fadd7b":"df['min_timespent_per_Browser']=df.groupby('Browser')['time_spent'].transform('min')\ndf['max_timespent_per_Browser']=df.groupby('Browser')['time_spent'].transform('max')\ndf['mean_timespent_per_Browser']=df.groupby('Browser')['time_spent'].transform('mean')\ndf['median_timespent_per_Browser']=df.groupby('Browser')['time_spent'].transform('median')","e5171032":"#cleaning the text and extracting product details starting and ending sequence\nTrain['client_agent']=Train['client_agent'].replace(' ',',',regex=True)\nTrain['client_agent']=Train['client_agent'].str.replace(r\"\\s*\\([^()]*\\)\",\"\").str.strip()\nTrain['client_agent']=Train['client_agent'].str.strip('[]')\nTrain['client_agent']=Train['client_agent'].replace('\/', '',regex=True)\nTrain['client_agent']=Train['client_agent'].replace(':', '',regex=True)\nTrain['client_agent']=Train['client_agent'].replace(';',',',regex=True)\nTrain['client_agent']=Train['client_agent'].replace(',,',',',regex=True)\nTrain['client_agent']=Train['client_agent'].str.lower()\nTrain['client_agent'].fillna('0', inplace=True)\nTrain['client_agent'].isnull().any()","df885700":"Train['SW']=Train['client_agent'].apply(lambda x : x.split(',')[0])\nTrain['product']=Train['client_agent'].apply(lambda x : x.split(',')[-1])","b7159508":"#cleaning the text and extracting product details starting and ending sequence\nTest['client_agent']=Test['client_agent'].replace(' ',',',regex=True)\nTest['client_agent']=Test['client_agent'].str.replace(r\"\\s*\\([^()]*\\)\",\"\").str.strip()\nTest['client_agent']=Test['client_agent'].str.strip('[]')\nTest['client_agent']=Test['client_agent'].replace('\/', '',regex=True)\nTest['client_agent']=Test['client_agent'].replace(':', '',regex=True)\nTest['client_agent']=Test['client_agent'].replace(';',',',regex=True)\nTest['client_agent']=Test['client_agent'].replace(',,',',',regex=True)\nTest['client_agent']=Test['client_agent'].str.lower()\nTest['client_agent'].fillna('0', inplace=True)\nTest['client_agent'].isnull().any()","02b4725e":"Test['SW']=Test['client_agent'].apply(lambda x : x.split(',')[0])\nTest['product']=Test['client_agent'].apply(lambda x : x.split(',')[-1])","ef017c01":"a = np.array(Train['SW'].unique())\nb = np.array(Test['SW'].unique())\nSW_replace_list = list(np.setdiff1d(b,a))\nprint(*SW_replace_list)","e42709d0":"c = np.array(Train['product'].unique())\nd = np.array(Test['product'].unique())\nproduct_replace_list = list(np.setdiff1d(d,c))\nprint(*product_replace_list)","caad9a2a":"#Train[Train['SW'].str.contains('product2', regex=False)]\n#Train[Train['SW'].str.contains('product3.', regex=False)]\n#Train[Train['SW'].str.contains('product3.', regex=False)]","bf0e90b7":"#Train[Train['product'].str.contains('ipad7', regex=False)]\n#Train[Train['product'].str.contains('iphone7', regex=False)]\n#Train[Train['product'].str.contains('safari534', regex=False)]\n","3029aec5":"#now replace extra test set values with train set values for prodcut\nreplace_product={'ipad7.1':'ipad7.1.2',\n                'ipad7.1.1':'ipad7.1.2',\n                'iphone7.0':'iphone7.0.4',\n                'safari534.51.22':'safari534.57.2',\n                'safari534.52.7':'safari534.57.2'}\n\n#now replace extra test set values with train set values for SW\nSW_replace={'product2.5.1':'product3.3.1',\n            'product3.2.1':'product3.3.1',\n            'product3.4.0':'product3.3.1' }\n    ","fcde44bc":"median_timespent_per_SW_dict=Train.groupby('SW')['time_spent'].median().to_dict()\nmin_timespent_per_SW_dict=Train.groupby('SW')['time_spent'].min().to_dict()\nmax_timespent_per_SW_dict=Train.groupby('SW')['time_spent'].max().to_dict()","15d73ac4":"\ndf['med_time_per_SW']=df['SW'].apply(lambda x: median_timespent_per_SW_dict[SW_replace[x]] \n                                     if x in SW_replace_list \n                                     else median_timespent_per_SW_dict[x] )\ndf['min_time_per_SW']=df['SW'].apply(lambda x: min_timespent_per_SW_dict[SW_replace[x]] \n                                     if x in SW_replace_list \n                                     else min_timespent_per_SW_dict[x] )\ndf['max_time_per_SW']=df['SW'].apply(lambda x: max_timespent_per_SW_dict[SW_replace[x]] \n                                     if x in SW_replace_list \n                                     else max_timespent_per_SW_dict[x] )","c6981e18":"median_timespent_per_product_dict=Train.groupby('product')['time_spent'].median().to_dict()\nmin_timespent_per_product_dict=Train.groupby('product')['time_spent'].min().to_dict()\nmax_timespent_per_product_dict=Train.groupby('product')['time_spent'].max().to_dict()","fcfec3f9":"\ndf['med_time_per_product']=df['product'].apply(lambda x: \n                                               median_timespent_per_product_dict[replace_product[x]] \n                                               if x in product_replace_list\n                                               else median_timespent_per_product_dict[x] )\ndf['min_time_per_product']=df['product'].apply(lambda x: \n                                               min_timespent_per_product_dict[replace_product[x]] \n                                               if x in product_replace_list \n                                               else min_timespent_per_product_dict[x] )\ndf['max_time_per_product']=df['product'].apply(lambda x: \n                                               max_timespent_per_product_dict[replace_product[x]] \n                                               if x in product_replace_list \n                                               else max_timespent_per_product_dict[x] )","addba57c":"df.info()","5e269d43":"drop_cols=['session_id','client_agent','device_details','date']\ndf.drop(drop_cols, axis = 1, inplace=True)","728a38be":"print(df. columns) ","8ee1287b":"cols_flt=['min_timespent_per_month',\n       'max_timespent_per_month', 'mean_timespent_per_month',\n       'median_timespent_per_month', 'min_timespent_per_Quarter',\n       'max_timespent_per_Quarter', 'mean_timespent_per_Quarter',\n       'median_timespent_per_Quarter', 'min_timespent_per_weekday',\n       'max_timespent_per_weekday', 'mean_timespent_per_weekday',\n       'median_timespent_per_weekday', 'min_timespent_per_device',\n       'max_timespent_per_device', 'mean_timespent_per_device',\n       'median_timespent_per_device', 'min_timespent_per_Browser',\n       'max_timespent_per_Browser', 'mean_timespent_per_Browser',\n       'median_timespent_per_Browser', 'med_time_per_SW', 'min_time_per_SW',\n       'max_time_per_SW', 'med_time_per_product', 'min_time_per_product',\n       'max_time_per_product']\nfor col in cols_flt:\n    df[col] = df[col].astype('float32')","5a9f961a":"df['Weekend_FLG']=df['Weekend_FLG'].astype('bool')","57957c48":"df[['SW','product']] = df[['SW','product']].apply(le.fit_transform)\ndf[['device','Browser']] = df[['device','Browser']].apply(le.fit_transform)","e25bfc0e":"train_proc, test_proc = df[:Train.shape[0]], df[Train.shape[0]:].reset_index(drop = True)","eef555cc":"target = 'time_spent'\nfeatures = [col for col in df.columns if col not in ([target])]","810e8ffe":"trn, val = train_test_split(train_proc, test_size = 0.2, random_state = 1999)\n##### Input for model\nX_trn, X_val = trn[features], val[features]\n##### Target column\ny_trn, y_val = trn[target], val[target]\n##### Features for test data that we will be predicting\nX_test = test_proc[features]","7471e563":"%%time\nlgb = LGBMRegressor(random_state=1999)\n\nlgb.fit(X_trn, y_trn)\n\npreds = lgb.predict(X_val)\npreds = np.abs(preds)\n\nerror = np.sqrt(mean_squared_error(y_val, preds))\nprint(f'mean_squared_log_error is : {error}')","c09aa138":"%%time\nxgb = XGBRegressor()\n\nxgb.fit(X_trn, y_trn)\npreds = xgb.predict(X_val)\npreds = np.abs(preds)\n\nerror = np.sqrt(mean_squared_error(y_val, preds))\n\nprint(f'mean_squared_log_error is : {error}')","aa151afe":"%%time\n\nrf = RandomForestRegressor(random_state = 1999, n_jobs = -1)\n\nrf.fit(X_trn, y_trn)\npreds = rf.predict(X_val)\npreds = np.abs(preds)\n\nerror = np.sqrt(mean_squared_error(y_val, preds))\n\nprint(f'mean_squared_log_error is : {error}')","ebb49fdc":"def cross_val(regressor, train, test, features, name):\n    N_splits = 5\n    \n    oofs = np.zeros(len(train))\n    preds = np.zeros(len(test))\n    \n    target_col = train[target]\n    \n    folds = StratifiedKFold(n_splits = N_splits, shuffle = True,random_state = 1999)\n    stratified_target = pd.qcut( train[target], 10, labels=False, duplicates='drop')\n    for index, (trn_idx, val_idx) in enumerate(folds.split(train, stratified_target)):\n        print(f'\\n==================Fold{index + 1}=============================')\n        \n        #### Train Set\n        X_trn, y_trn = train[features].iloc[trn_idx], train[target].iloc[trn_idx]\n        \n        #### Validation Set\n        X_val, y_val = train[features].iloc[val_idx], train[target].iloc[val_idx]\n        \n        #### Test Set\n        X_test = test[features]\n        \n        if name != 'cat':\n            #### Scaling Data ####\n            scaler = StandardScaler()\n            _ = scaler.fit(X_trn)\n            X_trn = scaler.transform(X_trn)\n            X_val = scaler.transform(X_val)\n            X_test = scaler.transform(X_test)\n        \n        ############ Fitting #############\n        _ = regressor.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], \n                          early_stopping_rounds = 50, verbose = False)\n        \n        ############ Predicting #############\n        val_preds = np.abs(regressor.predict(X_val))\n        test_preds = np.abs(regressor.predict(X_test))\n        error = np.sqrt(mean_squared_error(y_val, val_preds))\n        print(f'\\n Root Log Mean Squared Error for Validation set is : {error}')\n        \n        oofs[val_idx] = val_preds\n        preds += test_preds \/ N_splits\n        \n    total_error = np.sqrt(mean_squared_error(target_col, oofs))\n    print(f'\\n\\Root Log Mean Squared Error for oofs is {total_error}')\n    \n    return oofs, preds\n        ","22e21789":"def normal_cross_val(regressor, train, test, features):\n    N_splits = 5\n    oofs = np.zeros(len(train))\n    preds = np.zeros(len(test))\n    target_col = train[target]\n    folds = StratifiedKFold(n_splits = N_splits, shuffle = True,random_state = 1999)\n    stratified_target = pd.qcut( train[target], 10, labels=False, duplicates='drop')\n    for index, (trn_idx, val_idx) in enumerate(folds.split(train, stratified_target)):\n        print(f'\\n===================Fold{index + 1}=======================')\n        #### Train Set\n        X_trn, y_trn = train[features].iloc[trn_idx], train[target].iloc[trn_idx]\n        #### Validation Set\n        X_val, y_val = train[features].iloc[val_idx], train[target].iloc[val_idx]\n        #### Test Set\n        X_test = test[features]\n        #### Scaling Data ####\n        scaler = StandardScaler()\n        _ = scaler.fit(X_trn)\n        \n        X_trn = scaler.transform(X_trn)\n        X_val = scaler.transform(X_val)\n        X_test = scaler.transform(X_test)\n        ############ Fitting #############\n        _ = regressor.fit(X_trn, y_trn)\n        \n        ############ Predicting #############\n        val_preds = np.abs(regressor.predict(X_val))\n        test_preds = np.abs(regressor.predict(X_test))\n        \n        error = np.sqrt(mean_squared_error(y_val, val_preds))\n        print(f'\\n Root Log Mean Squared Error for Validation set is : {error}')\n        oofs[val_idx] = val_preds\n        preds += test_preds \/ N_splits\n        \n    total_error = np.sqrt(mean_squared_error(target_col, oofs))\n    print(f'\\n\\Root Log Mean Squared Error for oofs is {total_error}')\n    \n    return oofs, preds","c59db3f0":"%%time\nrf_oofs, rf_preds = normal_cross_val(rf, train_proc, test_proc, features)","4c081286":"%%time\nlgb_oofs, lgb_preds = cross_val(lgb, train_proc, test_proc, features, 'lgb')","842a7765":"%%time\nxgb_oofs, xgb_preds = cross_val(xgb, train_proc, test_proc, features, 'xgb')","1dabd6ba":"import optuna\nfrom optuna.samplers import TPESampler","d155f21a":"%%time\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 35)\n    n_estimators = trial.suggest_int(\"n_estimators\", 700, 1500)\n    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 5)\n    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 100,10000)\n    max_features = trial.suggest_uniform('max_features', 0.1, 0.9)\n    model = RandomForestRegressor( \n        max_depth=max_depth,\n        n_estimators = n_estimators,\n        min_samples_split = min_samples_split,\n        max_leaf_nodes = max_leaf_nodes,\n        max_features = max_features,\n        random_state=1999,\n        bootstrap = True,\n        n_jobs = -1\n    )\n    return model\n\nsampler = TPESampler(seed=0)\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X_trn, y_trn)\n    preds = model.predict(X_val)\n    score = np.sqrt(mean_squared_error(y_val,preds))\n    return score\n\nstudy = optuna.create_study(direction=\"minimize\", sampler=sampler)\nstudy.optimize(objective, n_trials=40)\n\nrf_params = study.best_params\nrf_params['random_state'] = 1999\nrf = RandomForestRegressor(**rf_params)\nrf.fit(X_trn, y_trn)\npreds = rf.predict(X_val)\nprint('Optimized RF RMSLE', np.sqrt(mean_squared_error(y_val, preds)))","153202a7":"%%time\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 40)\n    n_estimators = trial.suggest_int(\"n_estimators\", 700, 2000)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.1, 1)\n    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.1, 0.9)\n    num_leaves = trial.suggest_int(\"num_leaves\", 2, 500)\n    #min_child_samples = trial.suggest_int('min_child_samples', 3, 200)\n    reg_alpha = trial.suggest_uniform(\"reg_alpha\", 0.1, 0.9)\n    reg_lambda = trial.suggest_uniform(\"reg_lambda\", 0.1, 0.9)\n    model = LGBMRegressor(\n        max_depth=max_depth,\n        n_estimators = n_estimators,\n        learning_rate=learning_rate, \n        colsample_bytree = colsample_bytree,\n        num_leaves=num_leaves, \n        reg_alpha = reg_alpha,\n        reg_lambda = reg_lambda,\n        #min_child_samples=min_child_samples,\n        random_state=1999,\n        n_jobs = -3\n    )\n    return model\nsampler = TPESampler(seed=0)\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], early_stopping_rounds = 50, verbose = False)\n    preds = model.predict(X_val)\n    score = np.sqrt(mean_squared_error(y_val,preds))\n    return score\n\nstudy = optuna.create_study(direction=\"minimize\", sampler=sampler)\nstudy.optimize(objective, n_trials=60)\n\nlgb_params = study.best_params\nlgb_params['random_state'] = 1999\nlgb = LGBMRegressor(**lgb_params)\nlgb.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], early_stopping_rounds = 50, verbose = False)\npreds = lgb.predict(X_val)\nprint('Optimized LGBM RMSLE', np.sqrt(mean_squared_error(y_val, preds)))","e75322ed":"%%time\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 7, 15)\n    n_estimators = trial.suggest_int(\"n_estimators\", 500, 1500)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.1, 1)\n    subsample = trial.suggest_uniform('subsample', 0.1, 0.99)\n    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.1, 0.9)\n    colsample_bylevel = trial.suggest_uniform('colsample_bylevel', 0.1, 0.9)\n    #num_leaves = trial.suggest_int(\"num_leaves\", 2, 5000)\n    #min_child_samples = trial.suggest_int('min_child_samples', 3, 200)\n    reg_alpha = trial.suggest_int(\"reg_alpha\", 1, 10)\n    reg_lambda = trial.suggest_int(\"reg_lambda\", 1, 10)\n    model = XGBRegressor(\n        max_depth = max_depth,\n        n_estimators = n_estimators,\n        learning_rate=learning_rate, \n        subsample = subsample,\n        colsample_bytree = colsample_bytree,\n        colsample_bylevel = colsample_bylevel,\n        #num_leaves=num_leaves, \n        #min_child_samples=min_child_samples,\n        random_state=0,\n        n_jobs = -3\n    )\n    return model\n\nsampler = TPESampler(seed=0)\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X_trn, y_trn, eval_set = [ (X_val, y_val)], early_stopping_rounds = 50, verbose = False)\n    preds = model.predict(X_val)\n    score = np.sqrt(mean_squared_error(y_val,preds))\n    return score\n\nstudy = optuna.create_study(direction=\"minimize\", sampler=sampler)\nstudy.optimize(objective, n_trials=50)\n\nxgb_params = study.best_params\nxgb_params['random_state'] = 0\nxgb = XGBRegressor(**xgb_params)\nxgb.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], early_stopping_rounds = 50, verbose = False)\npreds = xgb.predict(X_val)\nprint('Optimized XGB RMSLE', np.sqrt(mean_squared_error(y_val, preds)))\n","12513afb":"#%%time  I will check later \n\n#models = {'lgb' : lgb, 'rf' : rf, 'xgb' : xgb}\n\n#for name,model in models.items():\n    #error = predict(model, name)\n    #print(f'Error for {name} is {error}')","c0f7512e":"# 1. Manual Tuned\nlgb_1 = LGBMRegressor(random_state=1999,n_estimators=1000, learning_rate=0.13,num_leaves=70,max_depth=31,\n               reg_lambda=0.3, reg_alpha = 0.7)","ac7cbaf2":"# 2. Optuna Tuned\nparams = {'max_depth': 29, 'n_estimators': 868, 'learning_rate': 0.10130592168165514, 'colsample_bytree': 0.29840872430993026,\n          'num_leaves': 338, 'reg_alpha': 0.7919788672424208, 'reg_lambda': 0.5736739628502263}\n\nlgb_2 = LGBMRegressor(**params)","c4adda5d":"params = {'max_depth': 31, 'n_estimators': 768, 'learning_rate': 0.10395358602462655, 'colsample_bytree': 0.3004582369227073,\n          'num_leaves': 332, 'reg_alpha': 0.7969567856974819, 'reg_lambda': 0.6432689443285323}\n\nlgb_3 = LGBMRegressor(**params)","53bbab99":"# XGBoost\nparams = {'max_depth': 9, 'n_estimators': 500,'learning_rate': 0.1, 'booster' : 'gbtree', 'n_jobs' : -1,\n         'subsample' : 0.9, 'colsample_bytree' : 0.8, 'colsample_bylevel' : 0.6, 'random_state' : 0}\nxgb_1 = XGBRegressor(**params)","e277dec9":"params = {'max_depth': 8, 'n_estimators': 1156, 'learning_rate': 0.1282423144462752,\n          'subsample': 0.8583044649709827,'colsample_bytree': 0.39430648031413884,\n          'colsample_bylevel': 0.439284444843544,'random_state' : 0}\nxgb_2 = XGBRegressor(**params)","3cb819cf":"params = {'max_depth': 9, 'n_estimators': 1329, 'learning_rate': 0.10067225176673156, \n          'subsample': 0.9010792397620144, 'colsample_bytree': 0.4501213056757911, \n          'colsample_bylevel': 0.75993128190449555, 'random_state' : 0}\nxgb_3 = XGBRegressor(**params)","c6606681":"# XGBoost\nparams = {'max_depth': 9, 'n_estimators': 2000,'learning_rate': 0.1, 'booster' : 'gbtree', 'n_jobs' : -1,\n         'subsample' : 0.9, 'colsample_bytree' : 0.8, 'colsample_bylevel' : 0.6, 'random_state' : 0}\nxgb_4 = XGBRegressor(**params)","0fe8f690":"params = {'max_depth': 30, 'n_estimators': 2000, 'min_samples_split' : 2,'max_features' : 'sqrt', 'max_leaf_nodes' : 8000,\n          'bootstrap' : True,'random_state' : 1999, 'n_jobs' : -1}\n\nrf_1 = RandomForestRegressor(**params)","ec4dae0b":"params = {'max_depth': 31, 'n_estimators': 1317, 'min_samples_split': 2, 'max_leaf_nodes': 6653, \n          'max_features': 0.6297197869507615, 'bootstrap' : True,'random_state' : 1999, 'n_jobs' : -1}\n\nrf_2 = RandomForestRegressor(**params)","940a6377":"bag_1 = BaggingRegressor(base_estimator=lgb_1,n_estimators=20,max_samples=0.99,max_features=0.99,\n                       bootstrap=True,n_jobs=-1,random_state=0,verbose=0,)\n","275e2597":"bag_2 = BaggingRegressor(base_estimator=lgb_2,n_estimators=20,max_samples=0.99,max_features=0.99,\n                       bootstrap=True,n_jobs=-1,random_state=0,verbose=0,)","8dd76559":"bag_3 = BaggingRegressor(base_estimator=xgb_1,n_estimators=20,max_samples=0.99,max_features=0.99,\n                       bootstrap=True,n_jobs=-1,random_state=0,verbose=0,)","8c6034e1":"bag_4 = BaggingRegressor(base_estimator=xgb_3,n_estimators=20,max_samples=0.99,max_features=0.99,\n                       bootstrap=True,n_jobs=-1,random_state=0,verbose=0,)","bec4d1ee":"bag_5 = BaggingRegressor(base_estimator=xgb_4,n_estimators=20,max_samples=0.99,max_features=0.99,\n                       bootstrap=True,n_jobs=-1,random_state=0,verbose=0,)","5310c5e1":"train_new = train_proc[[target]].copy()\ntest_new = test_proc[[target]].copy()","1b0995cd":"%%time\nlgb_1_oofs, lgb_1_preds = cross_val(lgb_1, train_proc, test_proc, features, 'lgb')","be44d2d7":"%%time\nlgb_2_oofs, lgb_2_preds = cross_val(lgb_2, train_proc, test_proc, features, 'lgb')","ce996c51":"%%time\nlgb_3_oofs, lgb_3_preds = cross_val(lgb_3, train_proc, test_proc, features, 'lgb')\n","c4c4e0bb":"train_new['lgb_1'] = lgb_1_oofs\ntest_new['lgb_1'] = lgb_1_preds\n\ntrain_new['lgb_2'] = lgb_2_oofs\ntest_new['lgb_2'] = lgb_2_preds\n\ntrain_new['lgb_3'] = lgb_3_oofs\ntest_new['lgb_3'] = lgb_3_preds","89443009":"%%time\nxgb_1_oofs, xgb_1_preds = cross_val(xgb_1, train_proc, test_proc, features, 'xgb')\n","b7237deb":"%%time\nxgb_2_oofs, xgb_2_preds = cross_val(xgb_2, train_proc, test_proc, features, 'xgb')\n","745c78b9":"%%time\nxgb_3_oofs, xgb_3_preds = cross_val(xgb_3, train_proc, test_proc, features, 'xgb')","62dc30d6":"%%time\nxgb_4_oofs, xgb_4_preds = cross_val(xgb_4, train_proc, test_proc, features, 'xgb')\n","5d69f70e":"train_new['xgb_1'] = xgb_1_oofs\ntest_new['xgb_1'] = xgb_1_preds\n\ntrain_new['xgb_2'] = xgb_2_oofs\ntest_new['xgb_2'] = xgb_2_preds\n\ntrain_new['xgb_3'] = xgb_3_oofs\ntest_new['xgb_3'] = xgb_3_preds\n\ntrain_new['xgb_4'] = xgb_4_oofs\ntest_new['xgb_4'] = xgb_4_preds","739e6148":"%%time\nrf_1_oofs, rf_1_preds = normal_cross_val(rf_1, train_proc, test_proc, features)","7047ec45":"%%time\nrf_2_oofs, rf_2_preds = normal_cross_val(rf_2, train_proc, test_proc, features)\n","28dcd52b":"train_new['rf_1'] = rf_1_oofs\ntest_new['rf_1'] = rf_1_preds\n\ntrain_new['rf_2'] = rf_2_oofs\ntest_new['rf_2'] = rf_2_preds","fef18153":"%%time\nbag_1_oofs, bag_1_preds = normal_cross_val(bag_1, train_proc, test_proc, features)\n","5b6b37fb":"%%time\nbag_2_oofs, bag_2_preds = normal_cross_val(bag_2, train_proc, test_proc, features)\n","6a7269d5":"%%time\nbag_3_oofs, bag_3_preds = normal_cross_val(bag_3, train_proc, test_proc, features)","68433149":"%%time\nbag_4_oofs, bag_4_preds = normal_cross_val(bag_4, train_proc, test_proc, features)\n","f2076940":"%%time\nbag_5_oofs, bag_5_preds = normal_cross_val(bag_5, train_proc, test_proc, features)\n","53ffe92f":"train_new['bag_1'] = bag_1_oofs\ntest_new['bag_1'] = bag_1_preds\n\ntrain_new['bag_2'] = bag_2_oofs\ntest_new['bag_2'] = bag_2_preds\n\ntrain_new['bag_3'] = bag_3_oofs\ntest_new['bag_3'] = bag_3_preds\n\ntrain_new['bag_4'] = bag_4_oofs\ntest_new['bag_4'] = bag_4_preds\n\ntrain_new['bag_5'] = bag_5_oofs\ntest_new['bag_5'] = bag_5_preds","47c231af":"ens_features = [c for c in train_new.columns if c not in [target]]","e71afbf1":"%%time\nlevel_1_lgb_oofs, level_1_lgb_preds = cross_val(LGBMRegressor(),\n                                                train_new, test_new, \n                                                ens_features, 'lgb')\n","6c9596b6":"%%time\nlevel_1_xgb_oofs, level_1_xgb_preds = cross_val(xgb_1, train_new, test_new, ens_features, 'xgb')","47e30c80":"%%time\nlevel_1_rf_1_oofs, level_1_rf_1_preds = normal_cross_val(rf_1, train_new, test_new, ens_features)\n","8ecae6bb":"%%time\nlevel_1_lgb_bag_oofs, level_1_lgb_bag_preds =normal_cross_val(BaggingRegressor\n                                                              (base_estimator = LGBMRegressor()),\n                                                              train_new, test_new, ens_features)","cdb9ae81":"%%time\nlevel_1_xgb_bag_oofs, level_1_xgb_bag_preds = normal_cross_val(BaggingRegressor\n                                                               (base_estimator = xgb_1),\n                                                               train_new, test_new,\n                                                               ens_features)","c85e64dc":"%%time\nparams = {'max_depth': 30, 'n_estimators': 1000, 'min_samples_split' : 2,\n          'max_features' : 'sqrt', 'max_leaf_nodes' : 8000,\n          'bootstrap' : True,'random_state' : 1999, 'n_jobs' : -1}\n\nrf = RandomForestRegressor(**params)\n\nlevel_1_rf_bag_oofs, level_1_rf_bag_preds = normal_cross_val(BaggingRegressor\n                                                             (base_estimator = rf),\n                                                             train_new, test_new, \n                                                             ens_features)\n","de7271fc":"ens_train_new = train_proc[[target]].copy()\nens_test_new = test_proc[[target]].copy()\n\nens_train_new['lgb'] = level_1_lgb_oofs\nens_test_new['lgb'] = level_1_lgb_preds\n\nens_train_new['xgb'] = level_1_xgb_oofs\nens_test_new['xgb'] = level_1_xgb_preds\n\nens_train_new['rf'] = level_1_rf_1_oofs\nens_test_new['rf'] = level_1_rf_1_preds\n\nens_train_new['lgb_bag'] = level_1_lgb_bag_oofs\nens_test_new['lgb_bag'] = level_1_lgb_bag_preds\n\nens_train_new['rf1_bag'] = level_1_rf_bag_oofs\nens_test_new['rf1_bag'] = level_1_rf_bag_preds\n\nens_train_new['xgb_bag'] = level_1_xgb_bag_oofs\nens_test_new['xgb_bag'] = level_1_xgb_bag_preds\n","57f9b4fe":"ens_lvl_2_features = [c for c in ens_train_new.columns if c not in [target]]\n","637909aa":"from sklearn.linear_model import LinearRegression,Ridge,ARDRegression,SGDRegressor\nclf = LinearRegression()\n\nens_linear_oofs, ens_linear_preds = normal_cross_val(clf,\n                                                     ens_train_new, ens_test_new, ens_lvl_2_features)\n","0b8f73a1":"clf = Ridge()\n\nens_ridge_oofs, ens_ridge_preds = normal_cross_val(clf,\n                                                   ens_train_new, ens_test_new, ens_lvl_2_features)\n","c556cc0e":"clf = ARDRegression(normalize = True)\n\nens_ARD_oofs, ens_ARD_preds = normal_cross_val(clf,\n                                               ens_train_new, ens_test_new, ens_lvl_2_features)\n","95497314":"preds = ens_linear_preds*0.60 + ens_ridge_preds*0.20 + ens_ARD_preds*0.20\n\n#sample_sub['time_spent']=np.abs((np.exp(preds)-1))\n\n#sample_sub.to_csv('\/kaggle\/working\/Submission.csv', index=False)\n#sample_sub.to_csv(path + '\\\\Stacking.csv', index = False)","f53fe1af":"sample_sub=pd.DataFrame(columns=['time_spent'])","8d74e160":"sample_sub['time_spent']=np.abs((np.exp(preds)-1))\n\nsample_sub.to_csv('\/kaggle\/working\/Submission.csv', index=False)\n","0feecb13":"#sns.heatmap(df.corr(), annot=True, cmap='RdYlGn')","5a4d465b":"#Train=Train.sort_values(by=\"date\")\n#Train.date.head().T","eef5feae":"#time= go.Scatter(x=Train.date,\n                 #y=Train.time_spent.values)\n                \n#layout = go.Layout(title='Time spent on buying', xaxis=dict(title='Date'),\n                   #yaxis=dict(title='(time seconds)'))\n\n#fig = go.Figure(data=[time], layout=layout)\n#py.iplot(fig, filename='h2o-plots')","01fc05c0":"# All Tuned Models\nLGB","9764d653":"1. convert to boolean\n2. convert text to number - device details \n3. like adress and city - find train and test differences \n4. client agent group by time spent for train test \n6. group by device details \n5. gruop by quarter \/ month \/ day of week \n\nneed to check if they need to be done on train and test separate ly \nwhat is my train set \nwhat is my test set \nhow it has been done in address tab \nhow test time spent is filled  \n \n","0542df6a":"**All devices and browser in test are present in train**\ngruping time spent per device and browser ","f5aa888e":"# Predicting With Tuned Models","fd447cf9":"info on time spent median to mean vale \n4.656414\tproduct150264 Android Phone - Android  649\t\n\n7.64116\t4.06065\t6.66716\t- these are time spent values for product150264\nits product is typemobile\nAndroid Phone\tAndroid 4th one \t\n","cfca2f4b":"**unique vales in data frame**","56432fb6":"product2.5.1 product3.2.1 product3.4.0\ntrain these are iphone \n\nproduct3.3.1- 3.4 last one \n","ec1bbdc1":"# Feature Engineering\n\n**1. Extracting Features**","b814f8ad":"only client agent has null value ","a1afbfec":"**now client sw grouping per time spent**\n\nstep one check if any sw is new in test set ie not in train set\nwith what i can replace the new value i need to see","cec8cec2":"ipad7.1 ipad7.1.1 iphone7.0 safari534.51.22 safari534.52.7----  product \nnow device is iphone for first 3 replace thenm with some iphone 7 model present in train \n\nsafari which device model need to see \nsafari537.36 \nsafari537.73.11 - two are in train data \n\n534- is desktop mozilaa5.0 safari browser same desk top safari \n\ntrain data for ipad7 \nproduct4.0.0\tipad7.1.2- train data \nproduct4.2.0\tiphone7.1.1\nproduct4.2.0\tiphone7.0.4\n\nsafari534.57.2 train 4\n\n","94b2cdd7":"**now device grouping per time spent**\n1. step one check if any device is new in test set ie not in train set \n2. with what i can replace the new value i need to see ","52418bd3":"# Preprocessing\nadd both train and test for model creation , create dummy column for time pent in test ","2b4bd300":"# Predicting With All Models","389cc560":"leve 2 stacking ","13bf4cbe":"**which values in test are not in train**","c1ee6b26":"Total 160 values are mising in train datset for client agest feature and same in test it has 59 missing values \n\nThis shows client side software details , so we will fill it as unknown for both in train and test datset later . currently I will do exploratory data analysis on this. ","2f16c38b":"**Dropping features not required any more **","33fa465d":"# Grouping - features - target encoding \ngroup by mothly time spent , quarterly time spent ","d059e407":"# model buidling","aaa09b51":"**Data description of our data set **\n\nsession_id - Unique identifier for every row\n\nsession_number - Session type identifier\n\nclient_agent - Client-side software details\n\ndevice_details -  Client-side device details\n\ndate - Datestamp of the session\n\npurchased - Binary value for any purchase done\n\nadded_in_cart - Binary value for cart activity\n\nchecked_out -  Binary value for checking out successfully\n\ntime_spent - Total time spent in seconds (Target Column)\nRegression Modeling\nAdvance Feature engineering, with Datestamp and Text datatypes\nOptimizing RMSLE score as a metric to generalize well on unseen data","9a7f9ca8":"\n2. train test - need to extract devide details - check any values of test absent from train \n3. then group by men median min max wrt time spent \n4. map those who are new in test from train values \n5. convert these values to lebel transform. \n\n1. train test - need to extract client details - check any values of test absent from train\n3. then group by men median min max wrt time spent \n4. map those who are new in test from train values \n5. convert these values to lebel transform. \n\n10. group by month and quarter \n\n9. change the data types accordingly - groups to float \n6. delete the columns not required "}}