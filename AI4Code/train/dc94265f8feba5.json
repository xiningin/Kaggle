{"cell_type":{"629772b5":"code","89b4e7f9":"code","2b3f0df8":"code","55f744b5":"code","5ee40ebb":"code","e5389eea":"code","7e73364f":"code","6d2ee2c9":"code","dc1f6808":"code","f2d63acf":"code","17c6de99":"code","4d911726":"code","306e60fc":"code","b931aa29":"code","cf79974e":"code","6d4cb6c2":"code","084fcc99":"code","d1ca7720":"code","7418e760":"code","7ef77211":"code","d5d1ccd7":"code","c54f8b19":"code","772d5ea0":"code","3ffacf3c":"code","cf9ca4b0":"code","0258d7e7":"code","9fd4eed7":"code","504b584d":"code","fbdfee7e":"code","504951a3":"code","ffc10faa":"code","ab614cc5":"code","9355237e":"code","60df54fa":"code","6f2e9f81":"code","cfbb2ed2":"code","a0c65f81":"code","d691d796":"code","7d867f22":"code","6cc1154e":"code","e6522226":"code","b48c05f4":"code","8dda32e5":"code","7f82a963":"code","a4602582":"code","16b36d35":"code","8eea9001":"code","6347d3df":"code","13d8299e":"code","61368b88":"code","2b3c5e71":"code","0f58c84e":"code","70d8e280":"code","93e202d7":"code","05dbc8ce":"code","1831fda4":"code","8714c1f3":"code","8a7bbbe5":"code","74f11806":"code","0fb68d4d":"code","cff852cb":"code","ad893181":"code","46b7df53":"code","81bb1ffa":"code","06fa2b39":"code","342bbd4d":"code","c72805ad":"code","9cb1e54b":"code","489f6b0c":"code","ee455d2e":"markdown","0063d216":"markdown","85787665":"markdown","1885a9c4":"markdown","2371e01a":"markdown","b56efd2a":"markdown","9210dbe2":"markdown","0accb06f":"markdown","c89e293a":"markdown","a798f4e4":"markdown","fe5e9385":"markdown","4d3bd822":"markdown","bf5fba25":"markdown","b44ce5d9":"markdown","0d1ff34a":"markdown","888c199d":"markdown","12a8334f":"markdown","ed5432ad":"markdown","8a94e0d7":"markdown","76861302":"markdown","a05bf43f":"markdown","2136d547":"markdown","e544528b":"markdown","b8cc322c":"markdown","595f638d":"markdown","ae5c8d47":"markdown","b04ae9e8":"markdown","25af38f8":"markdown","c6a77950":"markdown","c3794ac9":"markdown","9c58d645":"markdown","d29f4f9b":"markdown","f1e2b68d":"markdown","44b4bb7a":"markdown","4a35a80b":"markdown","401a22b4":"markdown","68d813d9":"markdown","eeeac702":"markdown","c057ac3d":"markdown","05695599":"markdown","d9ba0f69":"markdown","c1276d5b":"markdown"},"source":{"629772b5":"import pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom catboost import Pool, CatBoostClassifier\nfrom scipy.stats import pearsonr, chi2_contingency\nfrom itertools import combinations\nfrom statsmodels.stats.proportion import proportion_confint\n\n%matplotlib inline","89b4e7f9":"data = pd.read_csv('..\/input\/lending-club\/accepted_2007_to_2018q4.csv\/accepted_2007_to_2018Q4.csv',\n    parse_dates=['issue_d'], infer_datetime_format=True)\ndata = data[(data.issue_d >= '2017-01-01 00:00:00') & (data.issue_d < '2019-01-01 00:00:00')]\ndata = data.reset_index(drop=True)\ndata.head()\n#data=data.sample(1000)","2b3f0df8":"data.describe()","55f744b5":"data.columns.values","5ee40ebb":"data.shape","e5389eea":"X = data.copy()\nX.info()","7e73364f":"X.select_dtypes('object').head()","6d2ee2c9":"X['earliest_cr_line'] = pd.to_datetime(X['earliest_cr_line'], infer_datetime_format=True)\nX['sec_app_earliest_cr_line'] = pd.to_datetime(X['sec_app_earliest_cr_line'], infer_datetime_format=True)","dc1f6808":"X['emp_length'] = X['emp_length'].replace({'< 1 year': '0 years', '10+ years': '11 years'})\nX['emp_length'] = X['emp_length'].str.extract('(\\d+)').astype('float')\nX['id'] = X['id'].astype('float')","f2d63acf":"nan_mean = X.isna().mean()\nnan_mean = nan_mean[nan_mean != 0].sort_values()\nnan_mean","17c6de99":"X = X.drop(['desc', 'member_id'], axis=1, errors='ignore')","4d911726":"fill_empty = ['emp_title', 'verification_status_joint']\nfill_max = ['bc_open_to_buy', 'mo_sin_old_il_acct', 'mths_since_last_delinq',\n            'mths_since_last_major_derog', 'mths_since_last_record',\n            'mths_since_rcnt_il', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq',\n            'mths_since_recent_inq', 'mths_since_recent_revol_delinq',\n            'pct_tl_nvr_dlq','sec_app_mths_since_last_major_derog']\nfill_min = np.setdiff1d(X.columns.values, np.append(fill_empty, fill_max))\n\nX[fill_empty] = X[fill_empty].fillna('')\nX[fill_max] = X[fill_max].fillna(X[fill_max].max())\nX[fill_min] = X[fill_min].fillna(X[fill_min].min())","306e60fc":"num_feat = X.select_dtypes('number').columns.values\nX[num_feat].nunique().sort_values()","b931aa29":"X = X.drop(['num_tl_120dpd_2m', 'id'], axis=1, errors='ignore')","cf79974e":"num_feat = X.select_dtypes('number').columns.values\ncomb_num_feat = np.array(list(combinations(num_feat, 2)))\ncorr_num_feat = np.array([])\nfor comb in comb_num_feat:\n    corr = pearsonr(X[comb[0]], X[comb[1]])[0]\n    corr_num_feat = np.append(corr_num_feat, corr)","6d4cb6c2":"high_corr_num = comb_num_feat[np.abs(corr_num_feat) >= 0.9]\nhigh_corr_num","084fcc99":"X = X.drop(np.unique(high_corr_num[:, 1]), axis=1, errors='ignore')","d1ca7720":"cat_feat = X.select_dtypes('object').columns.values\nX[cat_feat].nunique().sort_values()","7418e760":"X = X.drop(['url', 'emp_title'], axis=1, errors='ignore')","7ef77211":"cat_feat = X.select_dtypes('object').columns.values\ncomb_cat_feat = np.array(list(combinations(cat_feat, 2)))\ncorr_cat_feat = np.array([])\nfor comb in comb_cat_feat:\n    table = pd.pivot_table(X, values='loan_amnt', index=comb[0], columns=comb[1], aggfunc='count').fillna(0)\n    corr = np.sqrt(chi2_contingency(table)[0] \/ (table.values.sum() * (np.min(table.shape) - 1) ) )\n    corr_cat_feat = np.append(corr_cat_feat, corr)","d5d1ccd7":"high_corr_cat = comb_cat_feat[corr_cat_feat >= 0.9]\nhigh_corr_cat","c54f8b19":"X = X.drop(np.unique(high_corr_cat[:, 0]), axis=1, errors='ignore')","772d5ea0":"X.shape","3ffacf3c":"keep_list = ['addr_state', 'annual_inc', 'dti', 'earliest_cr_line', 'emp_length', 'fico_range_low', 'home_ownership', 'initial_list_status', 'int_rate', 'loan_amnt', 'loan_status','mths_since_rcnt_il', 'mths_since_recent_bc', 'mths_since_recent_inq', 'mort_acc', 'pub_rec', 'revol_bal', 'revol_util', 'sub_grade', 'term', 'title', 'total_acc', 'verification_status']\nprint(keep_list)\nprint(len(keep_list))","cf9ca4b0":"drop_list = [col for col in X.columns if col not in keep_list]\nprint(drop_list)\nprint(len(drop_list))","0258d7e7":"w = [col for col in keep_list if col not in X.columns]\nprint(w)","9fd4eed7":"X.drop(labels=drop_list, axis=1, inplace=True)","504b584d":"X.columns","fbdfee7e":"X.shape","504951a3":"X.head()","ffc10faa":"X.describe()","ab614cc5":"#Cut down annual income\nX = X[X['annual_inc'] < 350000]\n#Get rid of missing dti i.e. 999\nX = X[X['dti'] < 100]\n#Mortgage accounts <= 10\nX = X[X['mort_acc'] <= 10]\n#Delete few entries with large amount of public deragatory marks\nX = X[X['pub_rec'] <= 6]\n#Trim entries with revolving utilization greater than 100%\nX = X[X['revol_util'] <= 100]\n#Trim entries with revolving balance greater than 200K\nX = X[X['revol_bal'] <= 200000]","9355237e":"X['loan_status'].value_counts()","60df54fa":"X.loc[X['loan_status'] == 'Current', 'loan_status'] = 1\nX.loc[X['loan_status'] == 'Fully Paid', 'loan_status'] = 1\nX.loc[X['loan_status'] == 'In Grace Period', 'loan_status'] = 1\nX.loc[X['loan_status'] == 'Charged Off', 'loan_status'] = 0\nX.loc[X['loan_status'] == 'Late (31-120 days)', 'loan_status'] = 0\nX.loc[X['loan_status'] == 'Late (16-30 days)', 'loan_status'] = 0\nX.loc[X['loan_status'] == 'Default', 'loan_status'] = 0","6f2e9f81":"X['loan_status'] = X['loan_status'].astype(int)","cfbb2ed2":"X['loan_status'].value_counts()","a0c65f81":"XX = X.sample(10000)","d691d796":"g = sns.PairGrid(XX.select_dtypes('number'), hue=\"loan_status\", hue_kws={\"alpha\": [0.01,0.01]}, diag_sharey=False)\ng = g.map_diag(sns.kdeplot, bw=2, shade=True)\ng = g.map_lower(plt.scatter)\ng = g.add_legend()","7d867f22":"XXX = X.copy()\nkjk = XXX.pop('loan_status')\nXXX = XXX.select_dtypes(exclude=['object', 'datetime'])\nXXX.insert(XXX.shape[1], 'loan_status', kjk)","6cc1154e":"#fig, axes = plt.subplots(nrows=14, ncols=14, figsize=(25,25))\n#for i in range(0,14):\n#    for j in range(0,14):\n#        if XXX.dtypes[j] == np.float64:\n#            sns.kdeplot(XXX[XXX['loan_status'] == 1][XXX.columns[i]], XXX[XXX['loan_status'] == 1][XXX.columns[j]], cmap=\"Blues\", shade=True, shade_lowest=False, alpha = .5, ax=axes[i][j])\n#            sns.kdeplot(XXX[XXX['loan_status'] == 0][XXX.columns[i]], XXX[XXX['loan_status'] == 0][XXX.columns[j]], cmap=\"Reds\", shade=True, shade_lowest=False, alpha = .5, ax=axes[i][j])\n\n#fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,10))\n#sns.kdeplot(XXX[XXX['loan_status'] == 'Good']['dti'], XXX[XXX['loan_status'] == 'Good']['fico_range_low'], cmap=\"Blues\", shade=True, shade_lowest=False, alpha = .5, ax=ax)\n#sns.kdeplot(XXX[XXX['loan_status'] == 'Bad']['dti'], XXX[XXX['loan_status'] == 'Bad']['fico_range_low'], cmap=\"Reds\", shade=True, shade_lowest=False, alpha = .5, ax=ax)\n#plt.show()","e6522226":"fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(20,20))\nfor i, ax in enumerate(axes.flatten()):\n    if XXX.dtypes[i] == np.float64:\n        sns.violinplot(x='loan_status', y=XXX.columns[i], data=XXX, ax=ax)","b48c05f4":"XXXX = X.copy()\n\nX_num = XXXX.select_dtypes(exclude=['object', 'datetime'])\nX_cat = XXXX.select_dtypes(exclude=['float64'])\n\nfor i in range(0, X_cat.shape[1]):\n    X_num[X_cat.columns[i]] = X_cat[X_cat.columns[i]].values","8dda32e5":"#fig, axes = plt.subplots(nrows=10, ncols=14, figsize=(30,30))\n#for i in range(14,24):\n#    for j in range(0,14): \n#        sns.violinplot(x=X_num.columns[i], y=X_num.columns[j], data=X_num, ax=axes[i-14][j])","7f82a963":"#fig, axes = plt.subplots(nrows=10, ncols=2, figsize=(15,50))\n#for i in range(14,23):\n#    g = sns.countplot(x=X_num.columns[i], data=X_num[X_num['loan_status'] == 1], ax=axes[i-14][0], alpha = .5)\n#    g = sns.countplot(x=X_num.columns[i], data=X_num[X_num['loan_status'] == 0], ax=axes[i-14][1], alpha = .5)\n#    plt.setp(g.get_xticklabels(), rotation=45)\n    ","a4602582":"X.hist(figsize=(24,24))","16b36d35":"corr = X.corr()\nfig, ax = plt.subplots(figsize=(10,10))\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","8eea9001":"data['loan_status'].value_counts()","6347d3df":"y = data['loan_status'].copy()\ny = y.isin(['Current', 'Fully Paid', 'In Grace Period']).astype('int')\ny.value_counts()","13d8299e":"cm = sns.light_palette(\"green\", as_cmap=True)\npd.crosstab(data['loan_status'], data['grade']).style.background_gradient(cmap = cm)","61368b88":"#X_mod = X[X.grade == 'G'].copy()\nX_mod = X.copy()\nX_mod = X_mod.drop('loan_status', axis=1, errors='ignore')\nX_mod = X_mod.drop(['sub_grade', 'int_rate'], axis=1, errors='ignore')\nX_mod = X_mod.drop(['earliest_cr_line'], axis=1, errors='ignore')\ny_mod = y[X_mod.index]\n\nX_train, X_test, y_train, y_test = train_test_split(X_mod, y_mod, stratify=y_mod, random_state=0)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, random_state=0)","2b3c5e71":"X_train.head(50)","0f58c84e":"X_train.dtypes == 'object'","70d8e280":"(((data.loc[(data['loan_status'] != 'Current') & (data.issue_d >= '2018-01-01 00:00:00'), 'total_pymnt'].sum()\n   -data.loc[(data['loan_status'] != 'Current') & (data.issue_d >= '2018-01-01 00:00:00'), 'funded_amnt'].sum())\n  \/(data.loc[(data['loan_status'] != 'Current') & (data.issue_d >= '2018-01-01 00:00:00'), 'funded_amnt'].sum())))*100","93e202d7":"#cat_feat_ind = (X_train.dtypes == 'object').nonzero()[0]\ncat_feat_ind = [1,3,5,6,7,14]\npool_train = Pool(X_train, y_train, cat_features=cat_feat_ind)\npool_val = Pool(X_val, y_val, cat_features=cat_feat_ind)\npool_test = Pool(X_test, y_test, cat_features=cat_feat_ind)\n\nn = y_train.value_counts()\nmodel = CatBoostClassifier(learning_rate=.5,\n                           iterations=350,\n                           depth=3,\n                           l2_leaf_reg=1,\n                           random_strength=1,\n                           bagging_temperature=1,\n                           #grow_policy='Lossguide',\n                           #min_data_in_leaf=1,\n                           #max_leaves=1,\n                           early_stopping_rounds=50,\n                           class_weights=[1, n[0] \/ n[1]],\n                           verbose=False,\n                           random_state=0)\nmodel.fit(pool_train, eval_set=pool_val, plot=True);","05dbc8ce":"y_pred_test = model.predict(pool_test)\n\nacc_test = accuracy_score(y_test, y_pred_test)\nprec_test = precision_score(y_test, y_pred_test)\nrec_test = recall_score(y_test, y_pred_test)\nprint(f'''Accuracy (test): {acc_test:.3f}\nPrecision (test): {prec_test:.3f}\nRecall (test): {rec_test:.3f}''')\n\ncm = confusion_matrix(y_test, y_pred_test)\nax = sns.heatmap(cm, cmap='viridis_r', annot=True, fmt='d', square=True)\nax.set_xlabel('Predicted')\nax.set_ylabel('True');","1831fda4":"feat = model.feature_names_\nimp = model.feature_importances_\ndf = pd.DataFrame({'Feature': feat, 'Importance': imp})\ndf = df.sort_values('Importance', ascending=False)\nsns.barplot(x='Importance', y='Feature', data=df);","8714c1f3":"corr = X_mod[df['Feature'].values].corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, mask=mask, square=True, cmap='RdBu_r', vmin=-1, vmax=1, annot=True, fmt='.2f');","8a7bbbe5":"#for calculating correct number of bins\nQ1 = X_mod['fico_range_low'].quantile(0.25)\nQ3 = X_mod['fico_range_low'].quantile(0.75)\nIQR = Q3 - Q1\nh=2*IQR*X_mod.shape[0]**(-1\/3)\nbins = print(max(X_mod['fico_range_low'])-min(X_mod['fico_range_low'])\/h)\ngood = X_mod.loc[y_mod == 1, 'fico_range_low']\nbad = X_mod.loc[y_mod == 0, 'fico_range_low']\nsns.distplot(good, bins=bins, label='Good loans', kde=False, norm_hist=True)\nax = sns.distplot(bad, bins=bins, label='Bad loans', kde=False, norm_hist=True)\nax.set_ylabel('Density')\nax.legend();","74f11806":"#for calculating correct number of bins\nQ1 = X_mod['loan_amnt'].quantile(0.25)\nQ3 = X_mod['loan_amnt'].quantile(0.75)\nIQR = Q3 - Q1\nh=2*IQR*X_mod.shape[0]**(-1\/3)\nbins = print(max(X_mod['loan_amnt'])-min(X_mod['loan_amnt'])\/h)\n\ngood = X_mod.loc[y_mod == 1, 'loan_amnt']\nbad = X_mod.loc[y_mod == 0, 'loan_amnt']\nsns.distplot(good, bins=bins, label='Good loans', kde=False, norm_hist=True)\nax = sns.distplot(bad, bins=bins, label='Bad loans', kde=False, norm_hist=True)\nax.set_ylabel('Density')\nax.legend();","0fb68d4d":"#for calculating correct number of bins\nQ1 = X_mod['mths_since_recent_inq'].quantile(0.25)\nQ3 = X_mod['mths_since_recent_inq'].quantile(0.75)\nIQR = Q3 - Q1\nh=2*IQR*X_mod.shape[0]**(-1\/3)\nbins = print(max(X_mod['mths_since_recent_inq'])-min(X_mod['mths_since_recent_inq'])\/h)\n\ngood = X_mod.loc[y_mod == 1, 'mths_since_recent_inq']\nbad = X_mod.loc[y_mod == 0, 'mths_since_recent_inq']\nsns.distplot(good, bins=bins, label='Good loans', kde=False, norm_hist=True)\nax = sns.distplot(bad, bins=bins, label='Bad loans', kde=False, norm_hist=True)\nax.set_ylabel('Density')\nax.legend();","cff852cb":"#for calculating correct number of bins\nQ1 = X_mod['annual_inc'].quantile(0.25)\nQ3 = X_mod['annual_inc'].quantile(0.75)\nIQR = Q3 - Q1\nh=2*IQR*X_mod.shape[0]**(-1\/3)\nbins = print(max(X_mod['annual_inc'])-min(X_mod['annual_inc'])\/h)\n\ngood = X_mod.loc[y_mod == 1, 'annual_inc']\nbad = X_mod.loc[y_mod == 0, 'annual_inc']\nsns.distplot(good, bins=bins, label='Good loans', kde=False, norm_hist=True)\nax = sns.distplot(bad, bins=bins, label='Bad loans', kde=False, norm_hist=True)\nax.set_ylabel('Density')\nax.legend();","ad893181":"y_proba_val = model.predict_proba(pool_val)[:, 1]\np_val, r_val, t_val = precision_recall_curve(y_val, y_proba_val)\nplt.plot(r_val, p_val)\nplt.xlabel('Recall')\nplt.ylabel('Precision');","46b7df53":"y_proba_val = model.predict_proba(pool_val)[:, 1]\nfpr, tpr, thresh = roc_curve(y_val, y_proba_val)\nplt.plot(fpr, tpr)\nplt.xlabel('Recall')\nplt.ylabel('Precision');","81bb1ffa":"p_max = p_val[p_val != 1].max()\nr_max = r_val[r_val != 0].max()\nt_all = np.insert(t_val, 0, 0)\n#t_adj_val = t_all[p_val == p_max]\nt_adj_val = t_all[r_val == r_max]\ny_adj_val = (y_proba_val > t_adj_val).astype(int)\np_adj_val = precision_score(y_val, y_adj_val)\nprint(f'Adjusted precision (validation): {p_adj_val:.3f}')","06fa2b39":"n = y_adj_val.sum()\nci = proportion_confint(p_adj_val * n, n, alpha=0.05, method='wilson')\nprint(f'95% confidence interval for adjusted precision: [{ci[0]:.3f}, {ci[1]:.3f}]')","342bbd4d":"y_proba_test = model.predict_proba(pool_test)[:, 1]\ny_adj_test = (y_proba_test > t_adj_val).astype(int)\np_adj_test = precision_score(y_test, y_adj_test)\nr_adj_test = recall_score(y_test, y_adj_test)\nprint(f'''Adjusted precision (test): {p_adj_test:.3f}\nAdjusted recall (test): {r_adj_test:.3f}''')\n\ncm_test = confusion_matrix(y_test, y_adj_test)\nax = sns.heatmap(cm_test, cmap='viridis_r', annot=True, fmt='d', square=True)\nax.set_xlabel('Predicted')\nax.set_ylabel('True');","c72805ad":"predictions = [round(value) for value in y_adj_test]\npred_df = pd.DataFrame(predictions, columns=['prediction'])\npred_df['actual'] = y_test.reset_index(drop=True)\npred_df.head()","9cb1e54b":"pred_df['actual'].sum()\/pred_df['prediction'].sum()","489f6b0c":"(pred_df['actual'].sum()-pred_df['prediction'].sum())\/pred_df['actual'].sum()*(1\/pred_df.shape[0])*100","ee455d2e":"Fraction of nans for each feature.","0063d216":"Calculation of return on investment for 2018. ROI = ((Total Payments-Total Funded Amounts)\/Total Funded Amounts)*100\nCurrent loans are not counted as they are typically paid off in full.","85787665":"99% precision would be nice, but the recall would be bad. Lets try to increase the recall as much as possible, even if it means losing a few % of precision.","1885a9c4":"There are many features that are very similar to eachother in description. These features are dropped.","2371e01a":"# 1. Introduction","b56efd2a":"Different types of loan status.","9210dbe2":"Convert classification target \"loan status\" to binary (Good = 1, Bad = 0).","0accb06f":"Convert some time features to datetime.","c89e293a":"It is an important problem in lending to predict whether or not a borrower will default on their loan. This accuracy can save lenders a lot of resources. On the other side of the coin, it is important to give each borrower a fighting chance to be checked for many different interacting variables, rather just a few financial indicators.\n\nIn this notebook, I wanted to see how accurate a simple model could be and what features go into these types of decisions, using Lending Club's dataset and CatBoost, a gradient boosting descion tree classifier for data sets with categorical variables. The memory usage is ~14 Gb for the whole csv. Also want to say thank you to Pablo Fesenko for helping me learn some machine learning with his notebook.","a798f4e4":"Lower income seems to be more associated with defaulting.","fe5e9385":"The recall is not very good, this is bad because we do not want false negatives. They are the most expensive losses.\n![1*prjHB1jDuSAlEIUX38D8iw.png](http:\/\/miro.medium.com\/max\/1400\/1*prjHB1jDuSAlEIUX38D8iw.png)","4d3bd822":"ROC curve of model.","bf5fba25":"Now the recall is 1 and the precision is ~93%. Let's see the predctions.","b44ce5d9":"Drop empty features.","0d1ff34a":"# 2. Data Cleaning","888c199d":"Drop highly unique numerical features.","12a8334f":"Low fico scores have a correlation with defaulting.","ed5432ad":"Split train-val-test sets and drop target dependent vaiables. ","8a94e0d7":"Target vaiable y as loan status.","76861302":"Let's set the recall to one and rerun the prediction to see what precision we can get with that.","a05bf43f":"Loans requested closer to recent inquiries seem to be more often defaulted which makes sense.","2136d547":"# 2. Feature Exploration","e544528b":"There are around 38 categorical features.","b8cc322c":"Higher loans seem to be harder to pay off.","595f638d":"Filter out outliers.","ae5c8d47":"Make pearsonr correlation matrix and drop one of each of the highly correlated features.","b04ae9e8":"Drop highly correlated catagorical values.","25af38f8":"All the features in the dataframe.","c6a77950":"The next step is to see how big of an impact that remaining 7% has and calulate the ROI. Maybe there is another way to optimize the precision and recall to optimize profits i.e. maximize interest gained and minimize risk of defaults.","c3794ac9":"The lowest fico range seemes to be the most important feature in this model, along with verification status and months since recent inquiry.","9c58d645":"Empty features are filled with an empty string. Missing features that would regularily be the maximum number such as months since recent inquiry are filled with the max and vice versa with the features that would be the min like employment length.","d29f4f9b":"We are left with these features.","f1e2b68d":"Since we have quite a few categorical features lets try CatBoost, a gradient boosted decision tree that works well with categorical variables. We also need to add weights because there are many more good loans than bad.","44b4bb7a":"Histogram comparing 3 important features with good and bad loans.","4a35a80b":"Percent error","401a22b4":"Correlation matrix of remaining features.","68d813d9":"Drop highly unique categorical features.","eeeac702":"Count of each loan status for each grade given by Lending Club. With grade G having the most frequent charged off loans.","c057ac3d":"Convert employment length range values in string form and id to floats.","05695599":"# 3. Classification Modeling","d9ba0f69":"Bad loan class makes up only about 7% of the total Loan Status","c1276d5b":"Accuracy"}}