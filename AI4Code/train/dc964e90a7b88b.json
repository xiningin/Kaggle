{"cell_type":{"fbdccc85":"code","9feb9ebd":"code","d68bcd38":"code","6d2c7e6a":"code","89e08de4":"code","faff3273":"code","d5b0047a":"code","0ab4437c":"code","83f39268":"code","ba27292d":"code","147181ed":"code","097a9024":"code","fc7d2390":"code","3df1a01a":"code","ad5c3267":"code","f440b57f":"code","32b3f492":"code","334c41d3":"code","3acd48c2":"code","3ffa4651":"code","760c1d3f":"code","b895eebd":"code","243d0574":"code","f3960250":"code","0593b9ec":"code","0a664c7f":"code","4ab0df12":"code","96c2bd5a":"code","f0566a41":"code","d4239448":"code","61c6f32f":"code","25acdf16":"code","f7cfbb82":"code","24c8fc5e":"code","4f787d98":"code","12083107":"code","a193decc":"code","ad32d855":"code","41bbfb29":"code","813f3df6":"code","559d5742":"code","b4427b80":"code","ad34958f":"code","e2182c50":"code","df2f0ef7":"code","91660325":"code","792f49d0":"code","075f0684":"code","be196a0a":"code","2e01b8d0":"code","9915d2c2":"code","a92a59e5":"code","66140526":"code","ae769b22":"code","c864d1f5":"code","272127ee":"code","80fddb48":"code","6f1c7a61":"code","3f8c2212":"code","14c0f538":"code","0cd90526":"code","72fc1783":"code","f93f0461":"code","a4598312":"code","dbe83a1d":"code","fab4460a":"code","954d8596":"code","57ddb923":"code","c361178d":"code","d9722b2e":"code","542d2c62":"code","dee888fc":"code","5c6111fc":"code","3c69c0aa":"code","32a0191c":"code","9fa18ab7":"markdown","ea63f13e":"markdown","ade9cc9f":"markdown","9bc91b5b":"markdown"},"source":{"fbdccc85":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","9feb9ebd":"# here we are gonna read the .csv files avaialable to us \ntrain_csv = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_csv = pd.read_csv(\"..\/input\/titanic\/test.csv\")","d68bcd38":"# printing the first three rows of the train_csv dataset \n# ideally if u dont specify anything it shows 5 rows\ntrain_csv.head(3)","6d2c7e6a":"# checking what all columns in the train_csv dataset contains null values \n# u can use .isnull() also and it will give the same result \ntrain_csv.isna().sum()","89e08de4":"# doing the same for the test_csv dataset \ntest_csv.isna().sum()","faff3273":"# i find the no of rwos and columns using .shape \ntrain_csv.shape","d5b0047a":"\ntotal = train_csv.isnull().sum().sort_values(ascending=False)\npercent_01 = train_csv.isnull().sum()\/train_csv.isnull().count()*100\npercent_02 = (round(percent_01, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_02], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","0ab4437c":"import re\ndeck = {\"A\": \"A\", \"B\": \"B\", \"C\": \"C\", \"D\": \"D\", \"E\": \"E\", \"F\": \"F\", \"G\": \"G\", \"U\": \"U\"}\ndata = [train_csv, test_csv]","83f39268":"# In the prev draft we had deleted Cabin \n# But here we are first gonna extract some valuablinfo from it and then remove it\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(\"U\")\n    #dataset['Deck'] = dataset['Deck'].astype(int)\n# we can now drop the cabin feature   \ntrain_df = train_csv.drop(['Cabin'], axis=1)\ntest_df = test_csv.drop(['Cabin'], axis=1)","ba27292d":"# checking if the csbin volumn is dropped successfully\n# it always a good habit to check if we have managed to successfully fo a certain task by using .head()\ntrain_csv.head(1)","147181ed":"test_csv.head(1)","097a9024":"train_csv[\"Deck\"].value_counts()","fc7d2390":"train_df.groupby('Deck').mean()","3df1a01a":"train_csv.head(1)","ad5c3267":"data = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss') \n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    #dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(\"NA\")\ntrain_csv = train_df.drop(['Name'], axis=1)\ntest_csv = test_df.drop(['Name'], axis=1) ","f440b57f":"# checking the no of different values in the Ticket column and their no of occurances \ntrain_csv[\"Ticket\"].value_counts()","32b3f492":"# checking if there are any repeated rows \n# since same repeated rows do not offer any additional data we must drop then\nd = train_csv.duplicated()\nprint(d.any())","334c41d3":"d = test_csv.duplicated()\nprint(d.any())","3acd48c2":"# later on  we need the passengerIdin our final csv file\nans = test_csv[\"PassengerId\"] ","3ffa4651":"# Also here the  PassengerId is absolutely unique and serve \n# only as an identifier for a person\n# Since they are absolutely unique they do not give us any relevant information\n# and are thus dropped \ntrain_csv.drop(\"PassengerId\",axis = 1,inplace= True)\ntest_csv.drop(\"PassengerId\",axis = 1,inplace= True) ","760c1d3f":"train_csv.head(1)","b895eebd":"# this allows us to check the datatype of each of the columns \ntrain_csv.dtypes","243d0574":"# we are converting the datatype of the Embarked column cuz we cannot apply \n# LabelEncoding to an object dtype column\ntrain_csv[\"Embarked\"] = train_csv[\"Embarked\"].astype(str)\ntest_csv[\"Embarked\"] = test_csv[\"Embarked\"].astype(str)","f3960250":"train_csv.dtypes","0593b9ec":"train_csv[\"Sex\"].value_counts()","0a664c7f":"# checking the no ofmales and females that survived the crash \n# we can notice that there is an uneven balance between the survivors based on their gender\n# a bit later we will observe the difference between survival ability between the men and females \n# alomg with many other factors added in the mix \ntrain_csv[\"Survived\"].value_counts()","4ab0df12":"# seeing the no of classes that were available on the fables titanic \n# and as always there is a starck contrast between the no of people based on Passenger class\ntrain_csv[\"Pclass\"].value_counts()","96c2bd5a":"# here we are gonna use the seaborn lib to display the various graphs that will help us \n# ascertain correctly the relationship between the datasets columns\nimport seaborn as sns\ng = sns.relplot(x=\"Pclass\", y=\"Survived\",hue = \"Sex\",kind=\"line\", data=train_csv)\ng.fig.autofmt_xdate()","f0566a41":"g = sns.relplot(x=\"Age\", y=\"Survived\",hue = \"Sex\",kind=\"line\", data=train_csv)\ng.fig.autofmt_xdate()","d4239448":"g = sns.relplot(x=\"SibSp\", y=\"Survived\",hue = \"Sex\",kind=\"line\", data=train_csv)\ng.fig.autofmt_xdate()","61c6f32f":"import seaborn as sns\nax = sns.countplot(x=\"Embarked\", data=train_csv)","25acdf16":"ax = sns.countplot(x = \"Survived\",hue = \"Embarked\",data = train_csv)","f7cfbb82":"g = sns.relplot(x=\"Sex\", y=\"Survived\", kind=\"line\", data=train_csv)\ng.fig.autofmt_xdate()","24c8fc5e":"g = sns.relplot(x = \"Embarked\", y = \"Survived\",hue = \"Sex\", kind = \"line\",data = train_csv)\ng.fig.autofmt_xdate()","4f787d98":"g = sns.relplot(x = \"Sex\", y = \"Survived\",hue = \"Embarked\", kind = \"line\",data = train_csv)\ng.fig.autofmt_xdate()","12083107":"data = [train_csv, test_csv]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'travelled_alone'] = 'No'\n    dataset.loc[dataset['relatives'] == 0, 'travelled_alone'] = 'Yes'","a193decc":"train_csv.head()","ad32d855":"axes = sns.factorplot('relatives','Survived',data=train_csv, aspect = 2.5, );\n## from the graph below we can see that the people with 1,2, or 3 relatives have a greater chance of survival \n## as compared to those with no or greater than 3 relatives","41bbfb29":"train_csv.head()","813f3df6":"test_csv.head()","559d5742":"# we know that in order to implement  model on the dataset we need to have all the \n# columns in numerical format and so by using LabelEncoder we are gonn convert all the \n# categorical values into numercial values\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ntrain_csv[\"Sex\"] = le.fit_transform(train_csv[\"Sex\"])\ntrain_csv[\"Embarked\"] = le.fit_transform(train_csv[\"Embarked\"])\ntest_csv[\"Sex\"] = le.fit_transform(test_csv[\"Sex\"])\ntest_csv[\"Embarked\"] = le.fit_transform(test_csv[\"Embarked\"])\ntrain_csv[\"travelled_alone\"] = le.fit_transform(train_csv[\"travelled_alone\"])\ntest_csv[\"travelled_alone\"] = le.fit_transform(test_csv[\"travelled_alone\"])\ntrain_csv[\"Deck\"] = le.fit_transform(train_csv[\"Deck\"])\ntest_csv[\"Deck\"] = le.fit_transform(test_csv[\"Deck\"])\ntrain_csv[\"Title\"] = le.fit_transform(train_csv[\"Title\"])\ntest_csv[\"Title\"] = le.fit_transform(test_csv[\"Title\"])","b4427b80":"train_csv[\"Ticket\"] = le.fit_transform(train_csv[\"Ticket\"])\ntest_csv[\"Ticket\"] = le.fit_transform(test_csv[\"Ticket\"])","ad34958f":"train_csv.head(3)","e2182c50":"test_csv.head(3)","df2f0ef7":"train_csv.isna().sum()","91660325":"test_csv.isna().sum()","792f49d0":"# now that we have to replace the NaN values with something\n# we caneither choose to drop the rows altogether if they are sufficently small in nos\n# or we can replace them with some other values like mean, mode, etc.\n#  HERE we are trying to replace the NaN values with their columns mean \n#train_csv[\"Age\"].fillna(train_csv[\"Age\"].mean(),inplace = True)\n#test_csv[\"Age\"].fillna(test_csv[\"Age\"].mean(),inplace = True)\n#test_csv.dropna(inplace = True)","075f0684":"y_train = train_csv.iloc[:,0]\nx_train = train_csv.iloc[:,1:]\nx_test = test_csv.iloc[:,:]","be196a0a":"y_train","2e01b8d0":"# we ccan also use Imputers to predit the NaN values substitute\n# this methodis quite good but requires more processing time and so should be avoided for big datasets\n# here we are using KNNImputer to predict the value of the NaN in the columns \nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer() \nimputer.fit (x_train)\nx_train = imputer.transform(x_train)\n###############\nimputer.fit (x_test)\nx_test = imputer.transform(x_test)","9915d2c2":"train_csv.head(1)","a92a59e5":"array_sum = np.sum(x_train)\narray = np.isnan(array_sum)\narray","66140526":"array_sum1 = np.sum(x_test)\narray = np.isnan(array_sum1)\narray","ae769b22":"# if u notice the values in all the columns then there is quite the difference in their magnitudes\n# this can cause the model to be lean towards one column too much\n# so we use a concept knows as feature scaling in order to reduce the magitudes to between 0 and 1 \n\nfrom sklearn.preprocessing import MinMaxScaler\ns = MinMaxScaler(feature_range=(0, 1))\nx_train = s.fit_transform(x_train)\nx_test = s.fit_transform(x_test)","c864d1f5":"from sklearn.preprocessing import StandardScaler\n# define standard scaler\nscaler = StandardScaler()\n# transform data\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)","272127ee":"x_train.shape","80fddb48":"x_test.shape","6f1c7a61":"from sklearn.feature_selection import VarianceThreshold\ntransform = VarianceThreshold(0)\nx_train = transform.fit_transform(x_train)\nx_test = transform.fit_transform(x_test)\nprint(x_train.shape)","3f8c2212":"# In order to properly evaluate my model i need something on which i can check my accuracy\n# For obvious reasons we cannot use the train set for this\n# thus we need to create a new set known as the validation set. ","14c0f538":"from sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.33, random_state=42)","0cd90526":"# this is the most basic model in classification where we import the model and fit it on the dataset \nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(solver=\"liblinear\")\nmodel.fit(x_train, y_train)","72fc1783":"# now we wanna predict if the people survive or not ","f93f0461":"# here we are gonna use the model and predict the survived of the x_test \ny_pred_logistic = model.predict(x_test)\ny_pred_logistic","a4598312":"# this allows to convert an array into a dataset \nfinal_pred = pd.DataFrame(y_pred_logistic)","dbe83a1d":"final_pred.head(7)","fab4460a":"final_pred.to_csv(\"pred_titanic_github.csv\",index = False) ","954d8596":"from sklearn.metrics import accuracy_score\nmodel.score(x_train, y_train)\nmodel = round(model.score(x_train, y_train) * 100, 2)\nmodel ","57ddb923":"from sklearn.svm import SVC # \"Support vector classifier\"\nmodel1 = SVC(kernel='rbf')\nmodel1.fit(x_train, y_train)","c361178d":"pred_svm =model1.predict(x_test) ","d9722b2e":"model1.score(x_train, y_train)\nmodel1 = round(model1.score(x_train, y_train) * 100, 2)\nmodel1","542d2c62":"from numpy import loadtxt \nfrom xgboost import XGBClassifier","dee888fc":"model2 = XGBClassifier()\neval_set = [(x_train, y_train),(x_valid, y_valid)] \nmodel2.fit(x_train, y_train, early_stopping_rounds=50, eval_metric=[\"logloss\",\"error\"], eval_set=eval_set, verbose=True)  ","5c6111fc":"#make predictions for valid data\npredictions_valid = model2.predict(x_valid)\n# evaluate predictions\naccuracy = accuracy_score(y_valid, predictions_valid)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n","3c69c0aa":"# retrieve performance metrics\nresults = model2.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\nx_axis ","32a0191c":"# plot log loss\nfrom matplotlib import pyplot\nfig, ax = pyplot.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\npyplot.ylabel('Log Loss')\npyplot.title('XGBoost Log Loss')\npyplot.show() ","9fa18ab7":"Logistic Regression","ea63f13e":"Now Here we are going to split the name into titles. This will alos help us acertain whether a person can survive or not based on their name (title).","ade9cc9f":"XG Boost and how to prevent overfitting ","9bc91b5b":"Supoort Vector Machine"}}