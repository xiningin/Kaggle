{"cell_type":{"275edb55":"code","c462f23a":"code","041a2831":"code","03228a02":"code","e96346c8":"code","604a83ac":"code","4d71f7c9":"code","8c91ebaa":"code","abb219a8":"code","c1238422":"code","1a17a13b":"code","9afd646a":"code","f0aa0da4":"code","777bee59":"code","30869d91":"code","1d81fb6d":"code","26914118":"code","35d0f5c2":"code","88d3da82":"code","331ab292":"code","3b4841dd":"code","e0cfec47":"code","c5c0c4ae":"code","21c92523":"code","a06f7afe":"code","90c665ea":"code","046eaf34":"code","ad359c68":"markdown","0d112342":"markdown","22be7ae5":"markdown","25430ea8":"markdown","c316e6dd":"markdown","e9d5eb97":"markdown","54716ff9":"markdown","bec0be4a":"markdown","a55e5104":"markdown","1d62d011":"markdown","87d2ef96":"markdown","e18d3a37":"markdown","cb7b1976":"markdown","79047103":"markdown","bcdbfa45":"markdown","a81adab0":"markdown","b6573b98":"markdown"},"source":{"275edb55":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import CIFAR100\nfrom torchvision.utils import make_grid\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data import random_split, ConcatDataset\nimport torchvision.transforms as tt\n","c462f23a":"stats = ((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025))\ntrain_transform = tt.Compose([\n    tt.RandomHorizontalFlip(),\n    tt.RandomCrop(32, padding=4, padding_mode=\"reflect\"),\n    tt.ToTensor(),\n    tt.Normalize(*stats)\n])\n\ntest_transform = tt.Compose([\n    tt.ToTensor(),\n    tt.Normalize(*stats)\n])","041a2831":"train_data = CIFAR100(download=True, root=\".\/data\", transform=train_transform)\ntest_data = CIFAR100(root=\".\/data\", train=False, transform=test_transform)","03228a02":"for image, label in train_data:\n    print(\"Image shape: \",image.shape)\n    print(\"Image tensor: \", image)\n    print(\"Label: \", label)\n    break","e96346c8":"train_classes_items = dict()\n\nfor train_item in train_data:\n    label = train_data.classes[train_item[1]]\n    if label not in train_classes_items:\n        train_classes_items[label] = 1\n    else:\n        train_classes_items[label] += 1\n\ntrain_classes_items","604a83ac":"test_classes_items = dict()\nfor test_item in test_data:\n    label = test_data.classes[test_item[1]]\n    if label not in test_classes_items:\n        test_classes_items[label] = 1\n    else:\n        test_classes_items[label] += 1\n\ntest_classes_items","4d71f7c9":"BATCH_SIZE = 128\ntrain_dl = DataLoader(train_data, BATCH_SIZE, num_workers=4, pin_memory=True, shuffle=True)\ntest_dl = DataLoader(test_data, BATCH_SIZE, num_workers=4, pin_memory=True)","8c91ebaa":"# for 8 images\ntrain_8_samples = DataLoader(train_data, 8, num_workers=4, pin_memory=True, shuffle=True)\n\ndef imshow(img):\n    img = img \/ 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.figure(figsize = (20,20))\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\ndataiter = iter(train_8_samples)\nimages, labels = dataiter.next()\n\n# print images\nimshow(torchvision.utils.make_grid(images))\nprint(''.join(f'{train_data.classes[labels[j]]:20s}' for j in range(8)))","abb219a8":"def get_device():\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    return torch.device(\"cpu\")\n\ndef to_device(data,device):\n    if isinstance(data,(list,tuple)):\n        return [to_device(x,device) for x in data]\n    return data.to(device,non_blocking=True)\n\n\nclass ToDeviceLoader:\n    def __init__(self,data,device):\n        self.data = data\n        self.device = device\n        \n    def __iter__(self):\n        for batch in self.data:\n            yield to_device(batch,self.device)\n            \n    def __len__(self):\n        return len(self.data)\n","c1238422":"device = get_device()\nprint(device)\n\ntrain_dl = ToDeviceLoader(train_dl, device)\ntest_dl = ToDeviceLoader(test_dl, device)","1a17a13b":"def accuracy(predicted, actual):\n    _, predictions = torch.max(predicted, dim=1)\n    return torch.tensor(torch.sum(predictions==actual).item()\/len(predictions))","9afd646a":"class BaseModel(nn.Module):\n    def training_step(self,batch):\n        images, labels = batch\n        out = self(images)\n        loss = F.cross_entropy(out,labels)\n        return loss\n    \n    def validation_step(self,batch):\n        images, labels = batch\n        out = self(images)\n        loss = F.cross_entropy(out,labels)\n        acc = accuracy(out,labels)\n        return {\"val_loss\":loss.detach(),\"val_acc\":acc}\n    \n    def validation_epoch_end(self,outputs):\n        batch_losses = [loss[\"val_loss\"] for loss in outputs]\n        loss = torch.stack(batch_losses).mean()\n        batch_accuracy = [accuracy[\"val_acc\"] for accuracy in outputs]\n        acc = torch.stack(batch_accuracy).mean()\n        return {\"val_loss\":loss.item(),\"val_acc\":acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))","f0aa0da4":"def conv_shortcut(in_channel, out_channel, stride):\n    layers = [nn.Conv2d(in_channel, out_channel, kernel_size=(1,1), stride=(stride, stride)),\n             nn.BatchNorm2d(out_channel)]\n    return nn.Sequential(*layers)\n\ndef block(in_channel, out_channel, k_size,stride, conv=False):\n    layers = None\n    \n    first_layers = [nn.Conv2d(in_channel,out_channel[0], kernel_size=(1,1),stride=(1,1)),\n                    nn.BatchNorm2d(out_channel[0]),\n                    nn.ReLU(inplace=True)]\n    if conv:\n        first_layers[0].stride=(stride,stride)\n    \n    second_layers = [nn.Conv2d(out_channel[0], out_channel[1], kernel_size=(k_size, k_size), stride=(1,1), padding=1),\n                    nn.BatchNorm2d(out_channel[1])]\n\n    layers = first_layers + second_layers\n    \n    return nn.Sequential(*layers)\n    \n\nclass ResNet(BaseModel):\n    \n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        \n        self.stg1 = nn.Sequential(\n                                   nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=(3),\n                                             stride=(1), padding=1),\n                                   nn.BatchNorm2d(64),\n                                   nn.ReLU(inplace=True),\n                                   nn.MaxPool2d(kernel_size=3, stride=2))\n        \n        ##stage 2\n        self.convShortcut2 = conv_shortcut(64,256,1)\n        \n        self.conv2 = block(64,[64,256],3,1,conv=True)\n        self.ident2 = block(256,[64,256],3,1)\n\n        \n        ##stage 3\n        self.convShortcut3 = conv_shortcut(256,512,2)\n        \n        self.conv3 = block(256,[128,512],3,2,conv=True)\n        self.ident3 = block(512,[128,512],3,2)\n\n        \n        ##stage 4\n        self.convShortcut4 = conv_shortcut(512,1024,2)\n        \n        self.conv4 = block(512,[256,1024],3,2,conv=True)\n        self.ident4 = block(1024,[256,1024],3,2)\n        \n        \n        ##Classify\n        self.classifier = nn.Sequential(\n                                       nn.AvgPool2d(kernel_size=(4)),\n                                       nn.Flatten(),\n                                       nn.Linear(1024, num_classes))\n        \n    def forward(self,inputs):\n        out = self.stg1(inputs)\n        \n        #stage 2\n        out = F.relu(self.conv2(out) + self.convShortcut2(out))\n        out = F.relu(self.ident2(out) + out)\n        out = F.relu(self.ident2(out) + out)\n        out = F.relu(self.ident2(out) + out)\n        \n        #stage3\n        out = F.relu(self.conv3(out) + (self.convShortcut3(out)))\n        out = F.relu(self.ident3(out) + out)\n        out = F.relu(self.ident3(out) + out)\n        out = F.relu(self.ident3(out) + out)\n        out = F.relu(self.ident3(out) + out)\n        \n        #stage4             \n        out = F.relu(self.conv4(out) + (self.convShortcut4(out)))\n        out = F.relu(self.ident4(out) + out)\n        out = F.relu(self.ident4(out) + out)\n        out = F.relu(self.ident4(out) + out)\n        out = F.relu(self.ident4(out) + out)\n        out = F.relu(self.ident4(out) + out)\n        out = F.relu(self.ident4(out) + out)\n        \n        #Classify\n        out = self.classifier(out)#100x1024\n        \n        return out\n        ","777bee59":"model = ResNet(3,100)","30869d91":"model = to_device(model, device)","1d81fb6d":"@torch.no_grad()\ndef evaluate(model,test_dl):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in test_dl]\n    return model.validation_epoch_end(outputs)","26914118":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit (epochs, train_dl, test_dl, model, optimizer, max_lr, weight_decay, scheduler, grad_clip=None):\n    torch.cuda.empty_cache()\n    \n    history = []\n    \n    optimizer = optimizer(model.parameters(), max_lr, weight_decay = weight_decay)\n    \n    scheduler = scheduler(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_dl))\n    \n    for epoch in range(epochs):\n        model.train()\n        \n        train_loss = []\n        \n        lrs = []\n        \n        for batch in train_dl:\n            loss = model.training_step(batch)\n            \n            train_loss.append(loss)\n            \n            loss.backward()\n            \n            if grad_clip:\n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            scheduler.step()\n            lrs.append(get_lr(optimizer))\n        result = evaluate(model, test_dl)\n        result[\"train_loss\"] = torch.stack(train_loss).mean().item()\n        result[\"lrs\"] = lrs\n        \n        model.epoch_end(epoch,result)\n        history.append(result)\n        \n    return history\n            ","35d0f5c2":"epochs = 100\noptimizer = torch.optim.Adam\nmax_lr = 1e-3\ngrad_clip = 0.1\nweight_decay = 1e-5\nscheduler = torch.optim.lr_scheduler.OneCycleLR","88d3da82":"%%time\nhistory = fit(epochs=epochs, train_dl=train_dl, test_dl=test_dl, model=model, \n              optimizer=optimizer, max_lr=max_lr, grad_clip=grad_clip,\n              weight_decay=weight_decay, scheduler=torch.optim.lr_scheduler.OneCycleLR)","331ab292":"def plot_acc(history):\n    plt.plot([x[\"val_acc\"] for x in history],\"-x\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n\ndef plot_loss(history):\n    plt.plot([x.get(\"train_loss\") for x in history], \"-bx\")\n    plt.plot([x[\"val_loss\"] for x in history],\"-rx\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend([\"train loss\",\"val loss\"])\n    \ndef plot_lrs(history):\n    plt.plot(np.concatenate([x.get(\"lrs\",[]) for x in history]))\n    plt.xlabel(\"Batch number\")\n    plt.ylabel(\"Learning rate\")","3b4841dd":"plot_loss(history)","e0cfec47":"plot_acc(history)","c5c0c4ae":"plot_lrs(history)","21c92523":"def predict_image(img, model):\n    xb = to_device(img.unsqueeze(0), device)\n    yb = model(xb)\n    _, preds  = torch.max(yb, dim=1)\n    return test_data.classes[preds[0].item()]","a06f7afe":"img, label = test_data[0]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', test_data.classes[label], ', Predicted:', predict_image(img, model))","90c665ea":"img, label = test_data[1002]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', test_data.classes[label], ', Predicted:', predict_image(img, model))","046eaf34":"torch.save(model.state_dict(), 'cifar100-resnet-project.pth')","ad359c68":"# Evaluation & Fit Function for Training","0d112342":"____\n#### Before we load the data, it is required to first prepare the transformations to be applied. It is an important step to prepare the data for training to avoid overfitting problem.\n____","22be7ae5":"# Necessary Libraries","25430ea8":"___\nRandomHorizontalFlip randomly flips an image with a probability of 50%, and RandomCrop pads an image by 4 pixel on each side then randomly crops 32x32 from the image after padding. We add such transformations to add noise to the data and prevent our model from overfitting. There are also other transformations you can use such as ColorJitter and RandomVerticalFlip etc. but I found these to be sufficient for our purposes. <br \/>\n\nToTensor simply converts the image to a Tensor. Since its a coloured image, it would have 3 channels (R,G,B) so the Tensor would be of size 3x32x32. <br\/>\n\nNormalize takes the mean and standard deviation for each channel of the entire dataset as input. Normalizing scales our data to a similar range of values to make sure that our gradients don\u2019t go out of control.\nNow we just prepare our train and test dataset and then we can explore the data.\n___","c316e6dd":"# BaseModel","e9d5eb97":"# Hyperparameters\n\n- **max_lr**: is the maximum learning rate that we set for learning rate scheduler. For the learning rate scheduler we used OneCycleLR, which sets the learning rate to a low learning rate, gradually increases it to the max learning rate then goes back to a low learning rate. <br\/>\n- **grad_clip**: prevents the gradients to become too large. <br\/>\n- **weight_decay**: essentially tries to make the model simple and helps the model generalise better.\n","54716ff9":"# Plotting","bec0be4a":"# Get CUDA ready","a55e5104":"# CIFAR-100 Image Classification using ResNet (Residual Network) in PyTorch\n\nResNet, short for Residual Network is a specific type of neural network that was introduced in 2015 by Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun in their paper [\u201cDeep Residual Learning for Image Recognition\u201d](https:\/\/arxiv.org\/pdf\/1512.03385.pdf). <br\/>\nResNet was an innovative approach over VGG-like models:\n* Won 1st place in the ILSVRC 2015 classification competition with a top-5 error rate of 3.57% (An ensemble model)\n* Won the 1st place in ILSVRC and COCO 2015 competition in ImageNet Detection, ImageNet localization, Coco detection and Coco segmentation.\n* Replacing VGG-16 layers in Faster R-CNN with ResNet-101. They observed relative improvements of 28%\n* Efficiently trained networks with 100 layers and 1000 layers also.\n\nWe get over 70% accuracy without using any pre-trained model in 100 epochs and it proves the power of ResNet.\n\nIn this notebook, I will be implementing less complex version of ResNets to be able to get results in reasonable amount of time. Even though the implementation is done through CIFAR-100 dataset, it can be used with any image classification dataset. <br\/>Note: Do not forget to change transform values if you use different the dataset. You can find the best parameters from SOTA implementations for well-known datasets.\n\n#### References which I got implementations and explanations.\n- [weiaicunzai\/pytorch-cifar100](https:\/\/github.com\/weiaicunzai\/pytorch-cifar100\/blob\/master\/models\/resnet.py)\n- [papers-with-code](https:\/\/paperswithcode.com\/sota\/image-classification-on-cifar-100)\n- [pytorch-basic-cifar10](https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html)\n- [more-implementation-with-explanations](https:\/\/blog.jovian.ai\/image-classification-of-cifar100-dataset-using-pytorch-8b7145242df1)\n- [more-reading](https:\/\/www.researchgate.net\/publication\/355698607_MEST_Accurate_and_Fast_Memory-Economic_Sparse_Training_Framework_on_the_Edge)\n- [visualization](https:\/\/jovian.ai\/damian-c036\/cifar100-final-project\/v\/5?utm_source=embed)\n\n\nSo we are good to go.\n","1d62d011":"# Loading Data","87d2ef96":"# Save","e18d3a37":"# BATCHSIZE & DataLoader","cb7b1976":"# ResNet Implementation\n\nBaseModel allows us to check and record the results of our model every time we train and pretty much just helps us keep track of our progress. This is what our actual model will inherit. <br \/>\n\nI implemented ResNet with changing a little bit that worked better than the one I got and used as base.<br \/>\n\n#### An example figure of ResNet approach to understand easily what we will see below as code:\n\n![image.png](attachment:5e638e86-7b48-499f-847d-741d69568111.png)\n<br\/><br\/><br\/><br\/>\n____\n\n","79047103":"# Make predictions and see the image with its result\n","bcdbfa45":"# Visualization","a81adab0":"_____","b6573b98":"# Training"}}