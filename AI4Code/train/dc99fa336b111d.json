{"cell_type":{"138c04e1":"code","ac2f458b":"code","1a998af6":"code","26825b45":"code","c2b5b9cb":"code","568a2396":"code","1637317a":"code","82212a81":"code","216bce65":"code","47e31c5b":"code","b26fb7cd":"code","3b1ddf99":"markdown"},"source":{"138c04e1":"from IPython.core.magic import register_cell_magic\nimport os\nfrom pathlib import Path\n\n## define custom magic to save most useful classes and use them in inference notebook \n## instead of copying the code every time you have changes in the classes\n@register_cell_magic\ndef write_and_run(line, cell):\n    argz = line.split()\n    file = argz[-1]\n    mode = 'w'\n    if len(argz) == 2 and argz[0] == '-a':\n        mode = 'a'\n    with open(file, mode) as f:\n        f.write(cell)\n    get_ipython().run_cell(cell)\n    \nPath('\/kaggle\/working\/scripts').mkdir(exist_ok=True)\nmodels_dir = Path('\/kaggle\/working\/models')\nmodels_dir.mkdir(exist_ok=True)","ac2f458b":"\n%%write_and_run scripts\/imports.py\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport torch\nimport numpy as np\nfrom torch import Tensor, nn, tensor\nimport math\nimport pandas as pd\nimport csv\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom pathlib import Path\nfrom fastai.vision.all import *\nfrom fastai.text.all import *\nfrom pathlib import Path\n\n# Make sure to have your glove embeddings stored here\nroot_dir = Path('.')\n\n\n","1a998af6":"%%write_and_run scripts\/model.py\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads, masked):\n        super().__init__()\n        assert d_model % num_heads == 0, \"num_heads must evenly chunk d_model\"\n        self.num_heads = num_heads\n        self.wq = nn.Linear(d_model, d_model, bias=False)  # QQ what if bias=True?\n        self.wk = nn.Linear(d_model, d_model, bias=False)\n        self.wv = nn.Linear(d_model, d_model, bias=False)\n        self.masked = masked\n        self.softmax = nn.Softmax(dim=2)\n\n    def forward(self, q, k, v):\n#         print(q.shape, self.num_heads)\n        qs = self.wq(q).chunk(self.num_heads, dim=2)\n        ks = self.wk(k).chunk(self.num_heads, dim=2)\n        vs = self.wv(v).chunk(self.num_heads, dim=2)\n        outs = []\n        # TODO Use einsum instead of for loop\n        for qi, ki, vi in zip(qs, ks, vs):\n            attns = qi.bmm(ki.transpose(1, 2)) \/ (ki.shape[2] ** 0.5)\n            if self.masked:\n                attns = attns.tril()  # Zero out upper triangle so it can't look ahead\n            attns = self.softmax(attns)\n            outs.append(attns.bmm(vi))\n        return torch.cat(outs, dim=2)\n\n\nclass AddNorm(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.ln = nn.LayerNorm(d_model)\n\n    def forward(self, x1, x2):\n        return self.ln(x1+x2)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.l1 = nn.Linear(d_model, d_model)\n        self.relu = nn.ReLU()\n        self.l2 = nn.Linear(d_model, d_model)\n    def forward(self, x):\n        return self.l2(self.relu(self.l1(x)))\n\n\ndef pos_encode(x):\n    pos, dim = torch.meshgrid(torch.arange(x.shape[1]), torch.arange(x.shape[2]))\n    dim = 2 * (dim \/\/ 2)\n    enc_base = pos\/(10_000**(dim \/ x.shape[2]))\n    addition = torch.zeros_like(x)\n    for d in range(x.shape[2]):\n        enc_func = torch.sin if d % 2 == 0 else torch.cos\n        addition[:,:,d] = enc_func(enc_base[:,d])\n    if x.is_cuda:\n        addition = addition.cuda()\n    return x + addition\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads, masked=False)\n        self.an1 = AddNorm(d_model)\n        self.ff = FeedForward(d_model)\n        self.an2 = AddNorm(d_model)\n\n    def forward(self, x):\n        x = self.an1(x, self.mha(q=x, k=x, v=x))\n        return self.an2(x, self.ff(x))\n\n\nclass AttentionAggregation(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.query = nn.Linear(d_model, 1, bias=False)\n\n    def forward(self, x):  # (b, s, m)\n        attns = self.query(x).softmax(dim=1)  # (b, s, 1)\n        enc = torch.bmm(attns.transpose(1, 2), x)  # (b, 1, m)\n        return enc.squeeze(1)\n\n\nclass LinTanh(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.lin = nn.Linear(d_model, d_model)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        return self.tanh(self.lin(x))\n\n\nclass LinFeatConcat(nn.Module):\n    def __init__(self, d_model, n_feats, n_out):\n        super().__init__()\n        self.lin = nn.Linear(d_model + n_feats, n_out, bias=False)  # TODO what if True?\n\n    def forward(self, x, feats):\n        return self.lin(torch.cat([x, feats], dim=1))\n\n\nclass ReadNetBlock(nn.Module):\n    def __init__(self, d_model, n_heads, n_blocks, n_feats, n_out):\n        super().__init__()\n        self.blocks = nn.Sequential(*[EncoderBlock(d_model=d_model, num_heads=n_heads) for _ in range(n_blocks)])\n        self.lin_tanh = LinTanh(d_model=d_model)\n        self.attn_agg = AttentionAggregation(d_model=d_model)\n        self.lin_feat_concat = LinFeatConcat(d_model=d_model, n_feats=n_feats, n_out=n_out)\n\n    def forward(self, x, feats):  # (b, s, m), (b, f)\n        x = pos_encode(x)\n        x = self.blocks(x)\n        x = self.lin_tanh(x)\n        x = self.attn_agg(x)\n        return self.lin_feat_concat(x, feats)\n\n\nclass GloveEmbedding(nn.Module):\n    def __init__(self, num):\n        super().__init__()\n        # Make embedding\n        self.embed = nn.Embedding(400_000 + 1, num)\n        emb_w = pd.read_csv(\n            root_dir \/ '..\/input\/glove6b200d\/glove.6B.200d.txt', header=None, sep=\" \", quoting=csv.QUOTE_NONE\n        ).values[:, 1:].astype('float64')\n        emb_w = Tensor(emb_w)\n        emb_w = torch.cat([emb_w, torch.zeros(1, num)], dim=0)\n        self.embed.weight = nn.Parameter(emb_w)\n\n    def forward(self, x):\n        return self.embed(x.to(torch.long))\n\n\nclass ReadNet(nn.Module):\n    def __init__(self, embed, d_model, n_heads, n_blocks, n_feats_sent, n_feats_doc):\n        super().__init__()\n        self.embed = embed\n        self.sent_block = ReadNetBlock(\n            d_model=d_model, n_heads=n_heads, n_blocks=n_blocks, n_feats=n_feats_sent, n_out=d_model\n        )\n        self.doc_block = ReadNetBlock(\n            d_model=d_model, n_heads=n_heads, n_blocks=n_blocks, n_feats=n_feats_doc, n_out=d_model + n_feats_doc\n        )\n        self.head = nn.Sequential(\n            nn.Linear(d_model + n_feats_doc, 1),\n        )\n\n    def forward(self, x, feats_sent=None, feats_doc=None):  # (b, d, s) tokens, (b, d, n_f_s), (b, n_f_d)\n        if feats_sent is None: feats_sent = Tensor([])\n        if feats_doc is None: feats_doc = Tensor([])\n        if x.is_cuda:\n            feats_sent = feats_sent.cuda()\n            feats_doc = feats_doc.cuda()\n        x = self.embed(x)\n        b, d, s, m = x.shape\n        x = x.reshape(b * d, s, m)\n        sents_enc = self.sent_block(x, feats_sent.reshape(b * d, -1))  # (b*d, m)\n        docs = sents_enc.reshape(b, d, m)\n        docs_enc = self.doc_block(docs, feats_doc)\n        out = self.head(docs_enc)\n        return out.squeeze(1)\n","26825b45":"%%write_and_run scripts\/dataset.py\n\nclass GloveTokenizer:\n    def __init__(self, num):\n        words = pd.read_csv(\n            root_dir \/ '..\/input\/glove6b200d\/glove.6B.200d.txt', header=None, sep=\" \", quoting=csv.QUOTE_NONE, usecols=[0]\n        ).values\n        words = [word[0] for word in words]\n        self.word2idx = {w: i for i, w in enumerate(words)}\n\n    def __call__(self, sent):\n        toks = [self.word2idx.get(w.lower()) for w in word_tokenize(sent)]\n        return [self.unk_token if t is None else t for t in toks]\n\n    @property\n    def unk_token(self):\n        return 400_000  # We appended this to the end of the embedding to return all zeros\n\n    @property\n    def pad_token(self):\n        return self.unk_token  # Seems that this is the best option for GLOVE\n\n\ndef prepare_txts(txts, tokenizer):\n    # Input: (bs,) str, Output: (bs, max_doc_len, max_sent_len)\n    # We choose to elongate all docs and sentences to the max rather than truncate some of them\n    # TODO: Do this better later:\n    # (1) Truncate smartly (if there is one very long outlier sentence or doc)\n    # (2) Group together docs of similar lengths (in terms of num_sents)\n    docs = [[tokenizer(sent) for sent in sent_tokenize(txt)] for txt in txts]\n    # pkl_save(root_dir\/\"doc_lens\", pd.Series([len(doc) for doc in docs]))\n    max_doc_len = max([len(doc) for doc in docs])\n    docs = [doc + [[]] * (max_doc_len - len(doc)) for doc in docs]\n    # pkl_save(root_dir\/\"sent_lens\", pd.Series([len(sent) for doc in docs for sent in doc]))\n    max_sent_len = max([len(sent) for doc in docs for sent in doc])\n    docs = [[s + [tokenizer.pad_token] * (max_sent_len - len(s)) for s in doc] for doc in docs]\n    return Tensor(docs)\n\n\ndef prepare_txts_cut(txts, tokenizer, max_doc_len=18, max_sent_len=49):\n    docs = [[tokenizer(sent)[:max_sent_len] for sent in sent_tokenize(txt)[:max_doc_len]] for txt in txts]\n    docs = [doc + [[]] * (max_doc_len - len(doc)) for doc in docs]\n    docs = [[s + [tokenizer.pad_token] * (max_sent_len - len(s)) for s in doc] for doc in docs]\n    return Tensor(docs)\n\n\n## TRAIN ## (using fastai)\n\ntokenizer = GloveTokenizer(200)\nembed = GloveEmbedding(200)\n\ndef get_splits(data):\n    num = len(data)\n    idx = list(range(num))\n    random.seed(42)\n    random.shuffle(idx)\n    split = int(num*0.75)\n    return idx[:split], idx[split:]\n\n\ndef get_dls(bs):\n    data = pd.read_csv(root_dir \/ '..\/input\/commonlitreadabilityprize\/train.csv')\n    txts = data.excerpt.tolist()\n    x = prepare_txts_cut(txts, tokenizer)\n    y = data.target.tolist()\n\n    ds = TfmdLists(\n      zip(x, y),\n      tfms=[],\n      splits=get_splits(data),\n    )\n\n    dls = ds.dataloaders(batch_size=bs)\n\n    return dls\n\n\ndef get_model():\n    readnet = ReadNet(\n        embed=embed,\n        d_model=200,\n        n_heads=4,\n        n_blocks=6,\n        n_feats_sent=0,\n        n_feats_doc=0,\n    )\n    readnet = readnet.cuda()\n\n    # Automatically freeze the embedding. We should not be learning this\n    for p in readnet.embed.parameters():\n        p.requires_grad = False\n\n    return readnet\n\n","c2b5b9cb":"\n# learn = Learner(dls=get_dls(16), model=get_model(), loss_func=MSELossFlat())\n# learn.lr_find()\n# learn.fit_one_cycle(30, 3e-5)\n# Learner.export(learn, 'learner')","568a2396":"new_learner = load_learner('learner')","1637317a":"from sklearn.metrics import mean_squared_error \n\ndata = pd.read_csv(root_dir \/ '..\/input\/commonlitreadabilityprize\/train.csv')\ntxts = data.excerpt.tolist()\nx = prepare_txts_cut(txts, tokenizer)\ny = data.target.tolist()\nprint(x.shape, len(y))\n\nds = TfmdLists(\n  zip(x, y),\n  tfms=[],\n  splits=get_splits(data),\n)\n\ndls = ds.dataloaders(batch_size=16)\n\n\n","82212a81":"import tqdm\n\npreds = []\nmodel = new_learner.model.cuda()\nds = torch.utils.data.TensorDataset(x)\ndl = torch.utils.data.DataLoader(ds, shuffle=False, batch_size=16)\nfor batch in tqdm.tqdm(dl):\n    batch = torch.tensor(batch[0]).cuda()\n    pred = model(batch)\n    preds.append(pred.detach())","216bce65":"final_preds = []\nfor b_preds in preds:\n    final_preds.extend(b_preds.tolist())","47e31c5b":"np.sqrt(mean_squared_error(final_preds, y))","b26fb7cd":"torch.save(model, 'readnet.pth')\ntorch.save(tokenizer, 'tokenizer')\n\n","3b1ddf99":"## original author: https:\/\/github.com\/vdefont\/readnet"}}