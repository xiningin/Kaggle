{"cell_type":{"1836f562":"code","1c66da1e":"code","b05af9bf":"code","22881f9c":"code","26fb0199":"code","e8f2f47c":"code","472a65dc":"code","fe8522f3":"code","202c3e22":"code","8aac6513":"code","94d973cc":"code","53d7736b":"code","933257c1":"code","dc9932f5":"code","e0e186e1":"code","d415bab6":"code","a234c008":"code","71d2e2ec":"code","76a407cc":"code","57858948":"code","f38eccbb":"code","45093dd2":"code","afc7289e":"code","f024d3b7":"code","897cfc7f":"code","2cdb3162":"code","bcdd2826":"code","1877250b":"code","f8050eba":"code","4a51f85b":"code","3a42fad6":"code","0bd59d4e":"code","d28626e9":"code","fc19f6ee":"code","1ad914bf":"code","d89d8590":"code","e60a9a52":"code","6434e8b3":"code","f96aa4da":"code","1f19704f":"code","bb8b250d":"code","6431d556":"code","ce9eaa5f":"code","1446828f":"code","e890b7b8":"code","92f39a52":"code","5049f317":"markdown","808c5ca6":"markdown","d619310f":"markdown","bf05fa36":"markdown","ae530376":"markdown","5155dbc8":"markdown","59112b40":"markdown","21c33426":"markdown","115b13e2":"markdown","100bee52":"markdown","fdca9d6c":"markdown","fde85c76":"markdown","ee0eb2f9":"markdown","cae20202":"markdown","33b2d498":"markdown","79ae6354":"markdown"},"source":{"1836f562":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport re\n\n# to avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")\n\nsns.set(style='darkgrid')\n%matplotlib inline","1c66da1e":"data = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\ndf=data.sample(20000).copy()\ndf.head()","b05af9bf":"df.shape","22881f9c":"def null_values(df):\n    \"\"\"a function to show null values with percentage\"\"\"\n    nv=pd.concat([df.isnull().sum(), 100 * df.isnull().sum()\/df.shape[0]],axis=1).rename(columns={0:'Missing_Records', 1:'Percentage (%)'})\n    return nv[nv.Missing_Records>0].sort_values('Missing_Records', ascending=False)\nnull_values(df)","26fb0199":"df.dropna(subset=['RainToday', 'RainTomorrow'], inplace=True)\ndf[['RainToday', 'RainTomorrow']].isnull().sum()","e8f2f47c":"def summary(df, pred=None):\n    obs = df.shape[0]\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Uniques = df.apply(lambda x: x.unique().shape[0])\n    Nulls = df.apply(lambda x: x.isnull().sum())\n    print('Data shape:', df.shape)\n\n    if pred is None:\n        cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n        str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n\n    str.columns = cols\n    print('___________________________\\nData Types:')\n    print(str.Types.value_counts())\n    print('___________________________')\n    display(str.sort_values(by='Nulls', ascending=False))\n\nsummary(df)","472a65dc":"df[['RainToday','RainTomorrow']] = df[['RainToday','RainTomorrow']].replace({'Yes':1, 'No':0})","fe8522f3":"# # Iterative Imputer default=BayesianRidge()\n\n# from sklearn.experimental import enable_iterative_imputer\n# from sklearn.impute import IterativeImputer\n\n# df_multi_imputation = df.copy()\n# idf = df_multi_imputation.drop(['Date','Location'], axis=1)\n# idf = pd.get_dummies(idf, drop_first=True, columns = ['WindDir9am','WindDir3pm','WindGustDir'])\n\n# imp_mean = IterativeImputer(missing_values=np.nan, initial_strategy='mean', random_state=42)\n# df_imputed_bayesian = pd.DataFrame(imp_mean.fit_transform(idf), index=idf.index, columns=idf.columns)\n# null_values(df_imputed_bayesian)","202c3e22":"# summary(df_imputed_bayesian.iloc[:,:50])","8aac6513":"# with decision tree\n# from sklearn.experimental import enable_iterative_imputer\n# from sklearn.impute import IterativeImputer\n# from sklearn.tree import DecisionTreeRegressor\n\n# df_multi_imputation = df.copy()\n# idf = df_multi_imputation.drop(['Date','Location'], axis=1)\n# idf = pd.get_dummies(idf, drop_first=True, columns = ['WindDir9am','WindDir3pm','WindGustDir'])\n\n# imp_mean = IterativeImputer(missing_values=np.nan, initial_strategy='mean', random_state=42, estimator=DecisionTreeRegressor())\n# df_imputed_dt = pd.DataFrame(imp_mean.fit_transform(idf), index=idf.index, columns=idf.columns)\n# null_values(df_imputed_dt)","94d973cc":"# summary(df_imputed_dt.iloc[:,:50])","53d7736b":"# with knn\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.neighbors import KNeighborsRegressor\n\ndf_multi_imputation = df.copy()\nidf = df_multi_imputation.drop(['Date','Location'], axis=1)\nidf = pd.get_dummies(idf, drop_first=True, columns = ['WindDir9am','WindDir3pm','WindGustDir'])\n\nimp_mean = IterativeImputer(missing_values=np.nan, initial_strategy='mean', random_state=42, estimator=KNeighborsRegressor())\ndf_imputed_knn = pd.DataFrame(imp_mean.fit_transform(idf), index=idf.index, columns=idf.columns)\nnull_values(df_imputed_knn)","933257c1":"summary(df_imputed_knn.iloc[:,:60])","dc9932f5":"summary(df_imputed_knn.iloc[:,60:])","e0e186e1":"from numpy import percentile\nfrom scipy.stats import zscore\nfrom scipy import stats\n\ndef outlier_zscore(df, col, min_z=1, max_z = 5, step = 0.1, print_list = False):\n    z_scores = zscore(df[col].dropna())\n    threshold_list = []\n    for threshold in np.arange(min_z, max_z, step):\n        threshold_list.append((threshold, len(np.where(z_scores > threshold)[0])))\n        df_outlier = pd.DataFrame(threshold_list, columns = ['threshold', 'outlier_count'])\n        df_outlier['pct'] = (df_outlier.outlier_count - df_outlier.outlier_count.shift(-1))\/df_outlier.outlier_count*100\n    plt.plot(df_outlier.threshold, df_outlier.outlier_count)\n    best_treshold = round(df_outlier.iloc[df_outlier.pct.argmax(), 0],2)\n    outlier_limit = int(df[col].dropna().mean() + (df[col].dropna().std()) * df_outlier.iloc[df_outlier.pct.argmax(), 0])\n    percentile_threshold = stats.percentileofscore(df[col].dropna(), outlier_limit)\n    plt.vlines(best_treshold, 0, df_outlier.outlier_count.max(), \n               colors=\"r\", ls = \":\"\n              )\n    plt.annotate(\"Zscore : {}\\nValue : {}\\nPercentile : {}\".format(best_treshold, outlier_limit, \n                                                                   (np.round(percentile_threshold, 3), \n                                                                    np.round(100-percentile_threshold, 3))), \n                 (best_treshold, df_outlier.outlier_count.max()\/2))\n    #plt.show()\n    if print_list:\n        print(df_outlier)\n    return (plt, df_outlier, best_treshold, outlier_limit, percentile)","d415bab6":"from scipy.stats import zscore\nfrom scipy import stats\n\ndef outlier_inspect(df, col, min_z=1, max_z = 5, step = 0.5, max_hist = None, bins = 50):\n    fig = plt.figure(figsize=(20, 6))\n    fig.suptitle(col, fontsize=16)\n    plt.subplot(1,3,1)\n    if max_hist == None:\n        sns.distplot(df[col], kde=False, bins = 50)\n    else :\n        sns.distplot(df[df[col]<=max_hist][col], kde=False, bins = 50)\n   \n    plt.subplot(1,3,2)\n    sns.boxplot(df[col])\n    plt.subplot(1,3,3)\n    z_score_inspect = outlier_zscore(df, col, min_z=min_z, max_z = max_z, step = step)\n    \n    plt.subplot(1,3,1)\n    plt.axvline(x=df[col].mean() + z_score_inspect[2]*df[col].std(),color='red',linewidth=1,linestyle =\"--\")\n    plt.axvline(x=df[col].mean() - z_score_inspect[2]*df[col].std(),color='red',linewidth=1,linestyle =\"--\")\n    plt.show()\n    \n    return z_score_inspect","a234c008":"def detect_outliers(df:pd.DataFrame, col_name:str, p=1.5) ->int:\n    ''' \n    this function detects outliers based on k time IQR and\n    returns the number of lower and uper limit and number of outliers respectively\n    '''\n    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n    IQR = third_quartile - first_quartile\n                      \n    upper_limit = third_quartile+(p*IQR)\n    lower_limit = first_quartile-(p*IQR)\n    outlier_count = 0\n                      \n    for value in df[col_name].tolist():\n        if (value < lower_limit) | (value > upper_limit):\n            outlier_count +=1\n    return lower_limit, upper_limit, outlier_count","71d2e2ec":"from scipy import stats\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf_numeric = df_imputed_knn.iloc[:,:16].select_dtypes(include=numerics)\n\nz = np.abs(stats.zscore(df_numeric))\nthreshold = 3\nprint('list of outliers:', '\\n', np.where(z > threshold)[0], '\\n', np.where(z > threshold)[1])\nprint('number of outliers:', np.where(z > threshold)[0].shape)","76a407cc":"# Example: maxTemp\nQ1_maxTemp = df_imputed_knn.MaxTemp.quantile(0.25)\nQ3_maxTemp = df_imputed_knn.MaxTemp.quantile(0.75)\nIQR_maxTemp = Q3_maxTemp - Q1_maxTemp\nupper_maxTemp = Q3_maxTemp + 1.5 * IQR_maxTemp\nlower_maxTemp = Q1_maxTemp - 1.5 * IQR_maxTemp\nprint(f\"\"\"\nQ1_maxTemp = {Q1_maxTemp}\nQ3_maxTemp = {Q3_maxTemp}\nIQR_maxTemp = {IQR_maxTemp}\nupper_maxTemp = {upper_maxTemp}\nlower_maxTemp = {lower_maxTemp}\n\"\"\")","57858948":"# Q1, Q3 and IQR values for each variable\nQ1 = df_numeric.quantile(0.25)\nQ3 = df_numeric.quantile(0.75)\nIQR = Q3 - Q1\npd.concat([Q1, Q3, IQR], axis=1, keys= ['Q1', 'Q3', 'IQR'])","f38eccbb":"features=df_imputed_knn.columns[:16]","45093dd2":"# Number of outliers in each column ignoring zero values by 1.5*IQR\nk=1.5\nprint(f\"Number of Outliers for {k}*IQR\\n\")\n\ntotal=0\nfor col in features:\n    outliers=detect_outliers(df_imputed_knn[df_imputed_knn[col] != 0], col, k)[2]\n    total+=outliers\n    if outliers > 0:\n        print(\"{} outliers in '{}'\".format(outliers,col))\n# print(\"\\n{} OUTLIERS TOTALLY\".format(total))","afc7289e":"def col_plot(df,col_name):\n    plt.figure(figsize=(15,6))\n    \n    plt.subplot(141) # 1 satir x 4 sutun dan olusan ax in 1. sutununda calis\n    plt.hist(df[col_name], bins = 20)\n    f=lambda x:(np.sqrt(x) if x>=0 else -np.sqrt(-x))\n    \n    # \u00fc\u00e7 sigma aralikta(verinin %99.7 sini icine almasi beklenen bolum) iki kirmizi cizgi arasinda\n    plt.axvline(x=df[col_name].mean() + 3*df[col_name].std(),color='red')\n    plt.axvline(x=df[col_name].mean() - 3*df[col_name].std(),color='red')\n    plt.xlabel(col_name)\n    plt.tight_layout\n    plt.xlabel(\"Histogram \u00b13z\")\n    plt.ylabel(col_name)\n\n    plt.subplot(142)\n    plt.boxplot(df[col_name]) # IQR katsayisi, defaultu 1.5\n    plt.xlabel(\"IQR=1.5\")\n\n    plt.subplot(143)\n    plt.boxplot(df[col_name].apply(f), whis = 1.5)\n    plt.xlabel(\"ROOT SQUARE - IQR=1.5\")\n\n    plt.subplot(144)\n    plt.boxplot(np.log(df[col_name]+0.1), whis = 1.5)\n    plt.xlabel(\"LOGARITMIC - IQR=1.5\")\n    plt.show()","f024d3b7":"for i in df_imputed_knn.columns[:16]:\n    col_plot(df_imputed_knn,i)","897cfc7f":"z_scores=[]\nfor i in df_imputed_knn.columns[:16]:\n    z_scores.append(outlier_inspect(df_imputed_knn,i)[2])","2cdb3162":"# verinin 99% confidence interval daki z score lari.\nprint(z_scores)","bcdd2826":"# create columns for 3z scores, new column with z score\ndf_3z=df_imputed_knn.copy()\n\nfor x in df_imputed_knn.columns[:16]:\n    df_3z[x + '_z'] = stats.zscore(df_3z[x])\n\nfor x in df_3z.columns[-len(df_imputed_knn.columns[:16]):]:\n    df_3z = df_3z[(df_3z[x] < 3) & (df_3z[x] > -3)]\n    \n# drop _z columns\ndf_3z = df_3z.drop(columns=df_3z.columns[-len(df_imputed_knn.columns[:16]):])\n\nprint('Number of Outliers:',len(df_imputed_knn)-len(df_3z))","1877250b":"# 3 z aralik disindaki datanin outlier kabul edildiginde olusturulan yeni dataframe\n# boolen degere sahip feature lar degerlendirmeye alinmamistir.\ndf_3z.iloc[:,:16]","f8050eba":"# after dropping outliers\ndf_3z.iloc[:,:16].describe().T.round(2)","4a51f85b":"# before dropping outliers\ndf_imputed_knn.iloc[:,:16].describe().T.round(2)","3a42fad6":"df_imputed_knn.columns","0bd59d4e":"from sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split\n\ndf_isolation_forest = df_imputed_knn.copy()\n\ny_iso = df_isolation_forest['RainTomorrow']\nX_iso = df_isolation_forest.drop(['RainTomorrow'], axis=1)\n\nclf = IsolationForest(n_estimators=100, max_samples='auto', contamination=0.10, random_state=42)\nclf.fit(X_iso)\ny_pred = clf.predict(X_iso)\n\n# the model will predict an inlier with a label of +1 and an outlier with a label of -1\n\noutliers_values = X_iso[clf.predict(X_iso) == -1]\noutliers_values","d28626e9":"f\"{len(outliers_values)} rows are outliers\"","fc19f6ee":"from sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import MinMaxScaler\n\ndf_lof = df_imputed_knn.copy()\n\nscaler = MinMaxScaler()\n\ncolumns = df_lof.columns\n\n#note that we transform the data with MinMaxScaler\ndf_lof_scaled = scaler.fit_transform(df_lof)\ndf_lof_scaled = pd.DataFrame(df_lof, columns=columns)\n\ny = df_lof_scaled['RainTomorrow']\nX = df_lof_scaled.drop(['RainTomorrow'], axis=1)\n\n# fit the model for outlier detection (default)\nclf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n\n# use fit_predict to compute the predicted labels of the training samples\n# (when LOF is used for outlier detection, the estimator has no predict,\n# decision_function and score_samples methods).\n\ny_pred = clf.fit_predict(X)\nX_scores = clf.negative_outlier_factor_\nnp.sort(X_scores)[:100]","1ad914bf":"# threshold = X_scores.mean()-3*X_scores.std()\nthreshold = np.sort(X_scores)[len(X_scores)\/\/10]\nthreshold","d89d8590":"X[(X_scores > threshold)==False].shape","e60a9a52":"plt.figure(figsize=(12,6))\nsns.histplot(x=X_scores)\nplt.xlim((-1.5,-0.9))\nplt.axvline(x=threshold,color='red')\nplt.xlabel(\"Outliers by Threshold in Histogram\")\nplt.show()","6434e8b3":"# Dropping observations with outliers\ndf_numeric_no = df_numeric[~((df_numeric < (Q1 - 1.5 * IQR)) |(df_numeric > (Q3 + 1.5 * IQR))).any(axis=1)]\nprint(df_numeric.shape)\nprint(df_numeric_no.shape)","f96aa4da":"df_numeric_no","1f19704f":"features = df_imputed_knn.columns[:16]\nfeatures","bb8b250d":"# visualize outliers before Replacing outliers of a variable with its median\n\ndf_numeric = df_imputed_knn.iloc[:,:16].select_dtypes(include=numerics)\nplt.figure(figsize=(20,20))\nfor k,v in enumerate(features):\n    plt.subplot(4,4,k+1)\n    sns.boxplot(data=df_numeric, x=v)","6431d556":"for i in features:\n    Q1_feature = df_numeric[i].quantile(0.25)\n    Q3_feature = df_numeric[i].quantile(0.75)\n    IQR_feature = Q3_feature - Q1_feature\n\n    df_numeric[i][((df_numeric[i] < (\n        Q1_feature - 1.5 * IQR_feature)) |(df_numeric[i] > (Q3_feature + 1.5 * IQR_feature)))] = df_numeric[i].median()","ce9eaa5f":"df_numeric","1446828f":"# visualize outliers after Replacing outliers of a variable with its median\nplt.figure(figsize=(20,20))\nfor k,v in enumerate(features):\n    plt.subplot(4,4,k+1)\n    sns.boxplot(data=df_numeric, x=v)","e890b7b8":"# the threshold observation\ndf_imputed_knn[(X_scores == threshold)]","92f39a52":"threshold_row = df_imputed_knn[(X_scores == threshold)]\noutliers = df_imputed_knn[(X_scores < threshold)]\n# We get rid of the indexes of outliers and transformed them into array\noutliers.to_records(index = False)\n# We define a variable for outlier array\noutliers_array = outliers.to_records(index = False)\n\n# We replace all the outliers with the threshold row\noutliers_array[:] = threshold_row.to_records(index = False)\n\ndf_numeric_no3 = df_imputed_knn.copy()\n\n# the outlier observations after replacement by the threshold observation\ndf_numeric_no3[(X_scores < threshold)] = pd.DataFrame(outliers_array, index = df_numeric_no3[(X_scores < threshold)].index)\ndf_numeric_no3[(X_scores < threshold)].head()","5049f317":"### Replacing outliers of a variable with its median","808c5ca6":"### LocalOutlierFactor\nUnsupervised Outlier Detection using Local Outlier Factor (LOF)\n\nThe anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers.","d619310f":"with bayesian and decisian tree models, There are some anomaies in numeric values. Except that temperature features, Any features shouldnt have negative values. So We have to prefer knn for `Multivariate Imputation`","bf05fa36":"## Outliers","ae530376":"while 'RainTomorrow' is the target label, others are the independent features","5155dbc8":"# Missing Value and Outliers Analysis","59112b40":"### Multivariate Imputation ","21c33426":"### Plotting outliers","115b13e2":"### Replacing outliers of a variable according to threshold of LocalOutlierFactor","100bee52":"### Dropping outliers","fdca9d6c":"### Handling Ouliers","fde85c76":"* Her \u00f6rne\u011fin anomali puan\u0131na LocalOutlierFactor denir. Belirli bir \u00f6rne\u011fin yo\u011funlu\u011funun kom\u015fular\u0131na g\u00f6re yerel sapmas\u0131n\u0131 \u00f6l\u00e7er. Anormallik puan\u0131n\u0131n, nesnenin \u00e7evreleyen kom\u015fulu\u011fa g\u00f6re ne kadar izole oldu\u011funa ba\u011fl\u0131 oldu\u011fu i\u00e7in yereldir. \n* Daha kesin olarak, yerellik, uzakl\u0131\u011f\u0131 yerel yo\u011funlu\u011fu tahmin etmek i\u00e7in kullan\u0131lan knn tarafindan verilir. Bir \u00f6rne\u011fin yerel yo\u011funlu\u011funu kom\u015fular\u0131n\u0131n yerel yo\u011funluklar\u0131yla kar\u015f\u0131la\u015ft\u0131rarak, kom\u015fular\u0131ndan \u00f6nemli \u00f6l\u00e7\u00fcde daha d\u00fc\u015f\u00fck yo\u011funlu\u011fa sahip \u00f6rnekler belirlenebilir. Bunlar ayk\u0131r\u0131 de\u011ferler olarak kabul edilir.","ee0eb2f9":"#### Functions","cae20202":"### Isolation Forest\nReturn the anomaly score of each sample using the IsolationForest algorithm\n\nThe IsolationForest \u2018isolates\u2019 observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n\nSince recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n\nThis path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n\nRandom partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.","33b2d498":"### Outliers according to 1.5 * IQR","79ae6354":"### Outliers according to Z-score\n3z >> %99.7 of the data\n![image.png](attachment:image.png)"}}