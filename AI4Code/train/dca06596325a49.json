{"cell_type":{"81df3f83":"code","a2c8d3ec":"code","8d15f6fb":"code","dca522dd":"code","a510b57e":"code","253b3a19":"code","c52f5ee5":"code","c3c7894b":"code","f4cb5ac4":"code","105f0f34":"code","3b9bb30d":"code","27159782":"code","c0639b58":"code","70fadf21":"code","1064d77e":"code","b0c5208e":"code","ba9f7b28":"code","fda9d4d8":"code","9ac17a9c":"code","a17480db":"code","857b42c2":"code","043a690c":"code","2fde134b":"code","c587da53":"code","e018bfdb":"markdown","b24e85b2":"markdown","c16b87f1":"markdown","6c2337eb":"markdown","fb9905c5":"markdown","aa396063":"markdown","6653e0fa":"markdown","92cce047":"markdown","040d9f97":"markdown","ee93fbd5":"markdown","d45836f2":"markdown","1eaa95c8":"markdown"},"source":{"81df3f83":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\ntraining_data = pd.read_csv('..\/input\/train.csv')\ntesting_data = pd.read_csv('..\/input\/test.csv')","a2c8d3ec":"training_data.head()","8d15f6fb":"testing_data.head()","dca522dd":"# Mapping 'Sex' Feature \n\nsex_dict = {'male':0, 'female':1}","a510b57e":"training_data['Sex'] = training_data['Sex'].map(sex_dict) \ntraining_data.head()","253b3a19":"testing_data['Sex'] = testing_data['Sex'].map(sex_dict)\ntesting_data.head()","c52f5ee5":"from sklearn.preprocessing import Imputer\nimp = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\ntraining_data[[\"Age\"]] = imp.fit_transform(training_data[[\"Age\"]]).ravel()\ntesting_data[[\"Age\"]] = imp.fit_transform(testing_data[[\"Age\"]]).ravel()\ntesting_data[[\"Fare\"]] = imp.fit_transform(testing_data[[\"Fare\"]]).ravel()\ntraining_data[[\"Embarked\"]] = training_data[[\"Embarked\"]].fillna(method = 'ffill')\ntesting_data[[\"Embarked\"]] = testing_data[[\"Embarked\"]].fillna(method = 'ffill')","c3c7894b":"training_data.isnull().sum(axis=0)","f4cb5ac4":"testing_data.isnull().sum(axis=0)","105f0f34":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntraining_data[[\"Ticket\"]] = le.fit_transform(training_data[[\"Ticket\"]]).ravel()\ntraining_data[[\"Embarked\"]] = le.fit_transform(training_data[[\"Embarked\"]]).ravel()\ntesting_data[[\"Ticket\"]] = le.fit_transform(testing_data[[\"Ticket\"]])\ntesting_data[[\"Embarked\"]] = le.fit_transform(testing_data[[\"Embarked\"]])","3b9bb30d":"training_data.head()","27159782":"testing_data.head()","c0639b58":"sns.barplot(x=\"Sex\", y=\"Survived\",hue=\"Sex\", data=training_data)\nplt.legend(title='Gender', loc='upper left', labels=['Female', 'Male'])","70fadf21":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=training_data)","1064d77e":"# Where S = 2 , C = 0, Q = 1\nsns.barplot(x=\"Embarked\", y=\"Survived\", data=training_data)","b0c5208e":"sns.barplot(x=\"SibSp\", y=\"Survived\", data=training_data)","ba9f7b28":"sns.barplot(x=\"Parch\", y=\"Survived\", data=training_data)","fda9d4d8":"train_data = training_data.drop(['PassengerId','Name','Cabin','Survived'],axis = 1)\nsurvived = training_data['Survived']\ntest_data = testing_data.drop(['PassengerId','Name','Cabin'], axis = 1)","9ac17a9c":"train_data.head()","a17480db":"test_data.head()","857b42c2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_data, survived, test_size=0.2, random_state=42)","043a690c":"import lightgbm as lgb\nlgbm = lgb.LGBMClassifier(max_depth = 8,            #maximum_depth_of_tree\n                         num_leaves=90,             #no_of_leaves\n                         lambda_l1 = 0.1,           #l1_regularization_value\n                         lambda_l2 = 0.01,          #l2_regularization_value\n                         learning_rate = 0.01,      #learning_rate_for_updating_parameter\n                         max_bin= 350,              #maximum_binning_for_unique_values\n                         n_estimators = 600,        #num_of_trees_to_fit \n                         reg_alpha = 1.6,           #learning_param\n                         colsample_bytree = 0.9,    #randomly_select_feature\n                         subsample = 0.9,           #select_feature_w\/o_randomness\n                         n_jobs = 6)                #no_of_threads_used_for_parallel_processing\nlgbm.fit(X_train,y_train)\npred = lgbm.predict(test_data)","2fde134b":"submission = pd.DataFrame({\"PassengerId\": testing_data[\"PassengerId\"],\"Survived\": pred})\nsubmission.to_csv('submission.csv',index= False)","c587da53":"submission.head()","e018bfdb":"# Modelling","b24e85b2":"Here we are deleteting Survived Column cause it is target value to be predicted \nCabin is dropped due to huge 'NaN' values containing in it \nPassengerID as it is of no use ","c16b87f1":"# Data Visualization","6c2337eb":"# Training","fb9905c5":"Splitting training_data and testing_data","aa396063":"# Prediction","6653e0fa":"# Feature Engineering","92cce047":"\n**Light GBM (Light Gradient Boosting Machine)** is framework of gradient boosting based on decision tree algorithm. It is not similar to XGBoost as LGBM splits tree leaf wise where XGBoost as level wise this results in reduction of loss function. LGBM is new released algorithm available with the key features of faster training speed, compatibility with higher dataset and parallel learning support.\n\n![](https:\/\/miro.medium.com\/max\/792\/1*AZsSoXb8lc5N6mnhqX5JCg.png)\n![](https:\/\/miro.medium.com\/max\/792\/1*whSa8rY4sgFQj1rEcWr8Ag.png)","040d9f97":"In conclusion, I learned that we can train a dataset with any model with just importing \nclass and fitting into training set which hardly need 2-3 lines but choosing right model with the right parameters \nwill give best result as given above . ","ee93fbd5":"Handling missing data with sklearn's Imputer with mean strategy","d45836f2":"Converting categorical data into numerical with LabelEncoder","1eaa95c8":"**Motivate me by upvoting : )**"}}