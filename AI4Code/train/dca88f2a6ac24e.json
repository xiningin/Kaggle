{"cell_type":{"88b3a9a6":"code","1442a45b":"code","24a72c57":"code","3882d124":"code","a677d98f":"code","055c2151":"code","a0aafa82":"code","be847364":"code","79920a9f":"code","40e96945":"code","79cc3441":"code","34f30af1":"code","5124fc64":"code","15a8358b":"code","a960d8d9":"code","d3d99313":"code","b23dfc07":"code","797c3565":"code","12b00dac":"code","e675b61e":"code","14e1dd45":"code","10461da2":"code","f6bc1dad":"code","47a0ba0e":"code","ab4b9a94":"code","15d9e8a8":"code","bb88199c":"code","05a58a21":"markdown","86d5d413":"markdown","af776546":"markdown","5b15d3a7":"markdown","4a1de65a":"markdown","52484f36":"markdown","f868ba2a":"markdown","a14cfe02":"markdown","c10ab6e0":"markdown","3ae60be7":"markdown","512498bf":"markdown","148e0ed7":"markdown","f3e27699":"markdown","73a38027":"markdown","8c3e6e02":"markdown","4e7ae690":"markdown"},"source":{"88b3a9a6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelBinarizer\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom PIL import Image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom kerastuner import RandomSearch\nfrom kerastuner.engine.hyperparameters import HyperParameters\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom keras.callbacks import ReduceLROnPlateau\n","1442a45b":"#reading data\ntrain_data = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv\")\nprint(train_data.shape)\ntrain_data.head()","24a72c57":"test_data = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv\")\nprint(test_data.shape)\ntest_data.head()","3882d124":"y_train = train_data['label']\ny_test = test_data['label']\ndel train_data['label']\ndel test_data['label']","a677d98f":"unique_labels = y_train.unique()\nunique_labels = np.sort(unique_labels)\nunique_labels","055c2151":"# visulaizing the data\nplt.figure(figsize=(15,6))\nsns.set_style(\"darkgrid\");\nsns.countplot(y_train);","a0aafa82":"label_binarizer = LabelBinarizer()\ny_train = label_binarizer.fit_transform(y_train)\ny_test = label_binarizer.fit_transform(y_test)\n\ny_train[:2]","be847364":"def preprocess_image(x):\n    \n    \"\"\"\n    we know that the pixcel values lies between 0-255 but it is obsearved that models performs exceptionally well if we scale pixel values\n    between 0-1\"\"\"\n    x = x\/255\n    x = x.reshape(-1,28,28,1) # convertin it into 28 x 28 gray scaled image\n    \n    return x\n    ","79920a9f":"train_x = preprocess_image(train_data.values)\ntest_x = preprocess_image(test_data.values)","40e96945":"def show_images(images,labels):\n    fig,ax = plt.subplots(2,5)\n    fig.set_size_inches(10, 6)\n    k =0\n    for i in range(2):\n        for j in range(5):\n            ax[i,j].imshow(images[k] , cmap='gray')\n            ax[i,j].set_title(str(unique_labels[np.argmax(y_train[k])]))\n            k = k+1;\n    plt.tight_layout()\n\n    ","79cc3441":"#let's see first 10 images from training set\nshow_images(train_x,y_train)","34f30af1":"#let's visualize test images as well\nshow_images(test_x,y_test)","5124fc64":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(train_x)","15a8358b":"def build_model(hp):  \n  model = keras.Sequential([\n    keras.layers.Conv2D(\n        filters=hp.Int('conv_1_filter', min_value=75, max_value=200, step=25),\n        kernel_size=(3,3),\n        activation='relu',\n        \n        input_shape=(28,28,1)\n    ),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=(2,2),strides=2,padding='same'),\n    keras.layers.Conv2D(\n        filters=hp.Int('conv_2_filter', min_value=50, max_value=125, step=25),\n        kernel_size=(3,3),\n        activation='relu',\n    ),\n    \n    keras.layers.Dropout(\n        rate = hp.Choice('drop_1_rate', values = [0.1,0.5])\n    ),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=(2,2),strides=2,padding='same'),\n    \n    \n    keras.layers.Conv2D(\n        filters=hp.Int('conv_3_filter', min_value=25, max_value=75, step=25),\n        kernel_size=(3,3),\n        activation='relu',\n    ),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=(2,2),strides=2,padding='same'),  \n    keras.layers.Flatten(),\n    keras.layers.Dense(\n        units=hp.Int('dense_1_units', min_value=128, max_value=1024, step=32),\n        activation='relu'\n    ),\n    keras.layers.Dropout(\n        rate = hp.Choice('drop_2_rate', values = [0.1,0.3])\n    ),\n    keras.layers.Dense(24, activation='softmax')\n  ])\n  \n  model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n  \n  return model","a960d8d9":"tuner_search=RandomSearch(build_model,\n                          objective='val_accuracy',\n                          max_trials=5,directory='output',project_name=\"ASLdetection1\")","d3d99313":"tuner_search.search(train_x,y_train,epochs=5,validation_data = (test_x, y_test))","b23dfc07":"model=tuner_search.get_best_models(num_models=1)[0] #this will give us the best tuned model","797c3565":"lr_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 3, verbose=1,factor=0.5, min_lr=0.00001)","12b00dac":"history = model.fit(datagen.flow(train_x,y_train, batch_size = 128) \n                    ,epochs = 20\n                    , validation_data = (test_x, y_test)\n                    , callbacks = [lr_reduction])","e675b61e":"model.summary()","14e1dd45":"epochs = [i for i in range(20)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(16,9)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Validation Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'r-o' , label = 'Testing Loss')\nax[1].set_title('Testing Accuracy & Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","10461da2":"tf.keras.utils.plot_model(\n    model,\n    to_file=\"model.png\",\n    show_shapes=False,\n    show_dtype=False,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)","f6bc1dad":"predictions = model.predict(test_x)","47a0ba0e":"def predictions_to_labels(pred):\n    labels =[]\n    for p in pred:\n        labels.append(unique_labels[np.argmax(p)])\n    return labels","ab4b9a94":"y_pred_labels = predictions_to_labels(predictions)\ny_test_labels = predictions_to_labels(y_test)","15d9e8a8":"accuracy_score(y_test_labels,y_pred_labels)","bb88199c":"cm= confusion_matrix(y_test_labels,y_pred_labels)\nplt.figure(figsize=(20,20))\nsns.heatmap(cm,annot=True,cmap='twilight_shifted')","05a58a21":"let's now plot the model architecture","86d5d413":"Now let's make some predictions on test data and see the confusion metrix","af776546":"well, we got 100% accuracy on our test data .\n# Analyse the trained model","5b15d3a7":"This really nice visual tells us that there are nearly 1k examples for each class output so we can consider this dataset as a balanec dataset because there's no class suffering from very less or too much examples \n\nNow the question arrises that why we didnt check the same thing for test data .... and the ans is we dont really need that <span>&#128516;<\/span> .<br>\nIn our training set we check the balance of the data coz we want to learn our model perfactly for each class we dont want it to be biased for a subset of possible classes, while in test set the whole paper can be from linear algebra , no worries in that case\n","4a1de65a":"now let's train the model ","52484f36":"basically in our dataset we are given lable for the corrosponding gasture and 784 (28 x 28) pixel values represting an image <br>\n* Training data contains 27455 images \n* test data contains 7172 images \n\n# Data preprocessing\n* converting array to images(tensors)\n\n* visulaizing lables and making sure that dataset is balanced\n* performing one hot encoding for lables","f868ba2a":"<p style = \"font-size : 42px; color : #393e46 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #00adb5; border-radius: 5px 5px;\"><strong>Sign Language Identification<\/strong><\/p>\n\n## What are Sign languages ?\n<img style=\"margin: auto; float: center;  border:5px solid #ffb037; width:80%; height : 300px;\"  src=\"https:\/\/image.shutterstock.com\/image-photo\/woman-showing-letters-asl-on-260nw-1305207925.jpg\">\n\n\n<br>\nSign languages are languages that use the visual-manual modality to convey meaning. Sign languages are expressed through manual articulations in combination with non-manual elements. Sign languages are full-fledged natural languages with their own grammar and lexicon .\n\n## Problems Explaination\nWe are given all the different gasture images considered in ASL (Americal Sign Language) and we need to come up with a machine learning model wich can classifiy the images correctly ,or can tell us the class to which the given image belongs.\nBasically we will be building a image classification model . \n<br>\n## Data information\nwe are given 2 comma saperated file(.csv) each on them contains some rows and 785 columns\n* from 2nd columns onwards each column represents the pixel values associated , representing a 28x28 grayscale image\n* first column in each row represnts label with the image\n* There are total 24 lables (in american sign language) A-I,K-Y means A-Z except J and Z\n\nwe will be using cnn to solve this problem .\n\n## What is an convolution neural network\nA Convolutional Neural Network (ConvNet\/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects\/objects in the image and be able to differentiate one from the other.\n<br>\nA Cnn consists of 4 types of nueral layers (collection of 'nodes' operating together at a specific depth within a neural network) .\n* **Convolutional Layer** :  Used to detect features\n* **Non-Linearity Layer** :  Introducing non-linearity to the system\n* **Pooling (Downsampling) Layer** :  Reduces weight count and checks fitness\n* **Flattening Layer** :  Prepares data for Classical Neural Network\n* **Fully-Connected Layer** :  Standard Neural Network used in classification\nI'll not go in depth of each of these layers we will be more focused towards solving this problem,i.e implementation part.","a14cfe02":"# import libraries","c10ab6e0":"<p style = \"font-size : 30px; color : #03506f ; font-family : 'Comic Sans MS'; \"><strong>Conclusion<\/strong><\/p> \n * we took the data from the dataframes and preprocessed the images\n * we performed data augmentation\n * we creadted a cnn model and tuned the hyperparameters with the help of `keras-tuner` library\n * we analysed our model \n<br>\n<p style = \"font-size : 15px; color : #035d6f ; font-family : 'Comic Sans MS'; \">At the end me made prediction of test data and as you can see we got the accuracy of <strong>100%<\/strong><\/p> ","3ae60be7":"so far we have done a lot , we got our images ready right!\n\n## Data Augmentation\nData augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.\n\nThis can expand our dataset artifically and make it robust , that couses variety in data and this can save us from overfitting.","512498bf":"well,well well, what happened in above 3 cells , isn't it looking like a magic, or it's it's going over head, relax i'll try to explaing this\n<br>\nIn the function `build_model` we build a cnn model , we spacified some layers and some of the hyperparameters are tuned using `keras-tuner` , it took each possible set of allowed parameters and trained a model and validated on test data , yep as simple as you read it.\nhave a look at <a href=\"https:\/\/keras-team.github.io\/keras-tuner\/documentation\/hyperparameters\/\">keras-tuner<\/a>\n\n\nok now we have tired different models and it's time to extract the best model from `tuner_search` object ...let's do this ","148e0ed7":"# Now let's create a callback \nA callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training.\n\nwe will reduce learning rate when a metric has stopped improving.\n\nModels often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.\n","f3e27699":"let's write a function which<br>\n* takes images array as input\n* displays 10 images from it(first 10)","73a38027":"## Buidling the CNN model\nwe will be building a CNN model to classify images , as usual we will try a few of conv2d layers along with batchNormalization and regulization(dropout) and then fallten layer then dense layer .\n","8c3e6e02":"<p style = \"font-size : 20px; color : #f55c47 ; font-family : 'Comic Sans MS'; \"><strong>If you like my work, please do Upvote and if you find anything which can be improved please let me know.<\/strong><\/p> ","4e7ae690":"so basically `LabelBinarizer` performed OHE sort of stuff on our training data .<br>\nNow let's write a function wich can take our array of images and can return a well shaped wranggled array ."}}