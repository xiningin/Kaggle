{"cell_type":{"54b74af7":"code","a4772cae":"code","7e2cd949":"code","89d3b84d":"code","0fe7f179":"code","0b304491":"code","83b0bba8":"code","d3246928":"code","12170a4a":"code","0d0ef490":"code","86c55dd4":"code","22b7e2bc":"code","da8888a8":"code","0389172b":"code","6b98db10":"code","22d4b825":"code","6cda92f7":"code","e3456a6a":"code","c9dc3d91":"code","50ac9778":"code","564fdecb":"code","bfdae5ca":"code","b4933594":"code","ed838cd5":"code","3b570b4a":"code","4eb281a4":"code","bc7e2480":"code","c2af95fe":"code","018de1f8":"code","6d8ac89d":"code","df0efe93":"code","e237f789":"code","e2380150":"code","7fab0e5c":"code","ed861f19":"markdown","364538b0":"markdown","34ed0b03":"markdown","d84be231":"markdown","2dcf137d":"markdown","dfb7811b":"markdown","185c74a5":"markdown","21662695":"markdown","3db331d4":"markdown","1c99328c":"markdown","027d1d29":"markdown","fd96ecde":"markdown","6a1e673f":"markdown","44553a49":"markdown","7c93d150":"markdown","99a77054":"markdown","4f85cb07":"markdown","d9a7a5ad":"markdown","588d3a8e":"markdown","b2f8ca53":"markdown","f909278a":"markdown","0dcbda34":"markdown","fcd9c102":"markdown","4c5f7223":"markdown","de79c2e0":"markdown","2789e7a6":"markdown","163e1cd4":"markdown","e3727f69":"markdown","cc3b965f":"markdown","29d9f4b0":"markdown","a57b7ec4":"markdown","175856ba":"markdown","b7805c88":"markdown","c5328c59":"markdown","0a2d74b9":"markdown","9ba9c50b":"markdown"},"source":{"54b74af7":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.impute import SimpleImputer\n\nimport pystan\nimport fbprophet\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense \nfrom keras.metrics import RootMeanSquaredError\nfrom keras.layers import BatchNormalization\nfrom keras.callbacks import EarlyStopping\nfrom keras import backend as K \nfrom keras.layers import LSTM\nfrom keras.layers import GRU","a4772cae":"calendar = pd.read_csv('..\/input\/m5-forecasting-accuracy\/calendar.csv',parse_dates=['date'])\ncalendar.head(3)","7e2cd949":"calendar.info()","89d3b84d":"print('Date Start: ' + str(min(calendar['date'])))\nprint('Date Start: ' + str(max(calendar['date'])))\nprint('\\n')\nprint('# of Unique days')\nprint(calendar['weekday'].value_counts())\nprint('\\n')\nprint('# of Unique year')\nprint(calendar['year'].value_counts())\nprint('\\n')\nprint('# of NaN in event_name_1,event_type_1,event_name_2,event_type_2')\nprint('Total # of missing value in event_name_1: ' + str(len(calendar.loc[calendar['event_name_1'].isna(),'event_name_1'])))\nprint('Total # of missing value in event_type_1: ' + str(len(calendar.loc[calendar['event_type_1'].isna(),'event_type_1'])))\nprint('Total # of missing value in event_name_2: ' + str(len(calendar.loc[calendar['event_name_2'].isna(),'event_name_2'])))\nprint('Total # of missing value in event_type_2: ' + str(len(calendar.loc[calendar['event_type_2'].isna(),'event_type_2'])))\nprint('\\n')\nprint('# of 0&1 for binary variable snap_CA')\nprint(calendar['snap_CA'].value_counts())\nprint('\\n')\nprint('# of 0&1 for binary variable snap_TX')\nprint(calendar['snap_TX'].value_counts())\nprint('\\n')\nprint('# of 0&1 for binary variable snap_WI')\nprint(calendar['snap_WI'].value_counts())","0fe7f179":"sales_train_validation = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nsales_train_validation.head(3)","0b304491":"sales_train_validation.info()","83b0bba8":"print('# of unique ID' + str(len(sales_train_validation['id'].value_counts())))\nprint('\\n')\nprint('# of Unique State')\nprint(sales_train_validation['state_id'].value_counts())\nprint('\\n')\nprint('# of Unique category')\nprint(sales_train_validation['cat_id'].value_counts())\nprint('\\n')\nprint('# of unique item: ' + str(len(sales_train_validation['item_id'].value_counts())))\nprint(sales_train_validation['cat_id'].value_counts())\nprint('\\n')\nprint('# of Unique Store')\nprint(sales_train_validation['store_id'].value_counts())","d3246928":"sell_price = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nsell_price.head(3)","12170a4a":"sell_price.info()","0d0ef490":"print('Statistic for Variable sell_price')\nprint(round(sell_price['sell_price'].describe(),2))\nprint('\\n')\nprint('Sell Price Start at week: ' + str(min(sell_price['wm_yr_wk'])))\nprint('Sell Price End at week: ' + str(max(sell_price['wm_yr_wk'])))\nprint('\\n')\nprint('The Start Week corresponding day: ')\nprint(calendar.loc[calendar['wm_yr_wk'] == 11101,'d'])\nprint('\\n')\nprint('The End Week corresponding day: ')\nprint(calendar.loc[calendar['wm_yr_wk'] == 11621,'d'])","86c55dd4":"### 1: Turn the np.NaN values in event_name_1, event_type_1, event_name_2,event_type_2 to 0. \ncategorical_Imputer = SimpleImputer(missing_values=np.nan,strategy='constant',fill_value=0)\ncalendar.iloc[:,[7,8,9,10]] = categorical_Imputer.fit_transform(calendar.iloc[:,[7,8,9,10]])\n\n### 2: Turn event_name_1, event_type_1, event_name_2,event_type_2 to binary variables\ncalendar['event_name_1'] = calendar['event_name_1'].astype(str)\ncalendar['event_type_1'] = calendar['event_type_1'].astype(str)\ncalendar['event_name_2'] = calendar['event_name_2'].astype(str)\ncalendar['event_type_2'] = calendar['event_type_2'].astype(str)\n# Create Dummy Holder\ncalendar['new_event_name_1'] = 0\ncalendar['new_event_type_1'] = 0\ncalendar['new_event_name_2'] = 0\ncalendar['new_event_type_2'] = 0\n# Create Dummy holder based on original variables\nfor i in range(0,len(calendar['event_name_1'])): \n  if calendar.iloc[i,7] != '0':\n    calendar.iloc[i,14] = 1\n\nfor i in range(0,len(calendar['event_type_1'])): \n  if calendar.iloc[i,8] != '0':\n    calendar.iloc[i,15] = 1\n\nfor i in range(0,len(calendar['event_type_1'])): \n  if calendar.iloc[i,9] != '0':\n    calendar.iloc[i,16] = 1\n\nfor i in range(0,len(calendar['event_type_1'])): \n  if calendar.iloc[i,10] != '0':\n    calendar.iloc[i,17] = 1\n\ncalendar = calendar.drop(labels=['event_name_1','event_type_1','event_name_2','event_type_2'],axis=1)\n\ncalendar = calendar.rename(columns={'new_event_name_1':'event_name_1',\n                                    'new_event_type_1':'event_type_1',\n                                    'new_event_name_2':'event_name_2',\n                                    'new_event_type_2':'event_type_2'})","22b7e2bc":"### 3: Replace weekday to numerical values\ndayOfWeek={'Monday':0, 'Tuesday':1, 'Wednesday':2,'Thursday':3,'Friday':4,'Saturday':5,'Sunday':6}\ncalendar = calendar.replace({'weekday':dayOfWeek})\n\n### 4: Drop variables event_name_1, event_name_2, wday since it is not insightful \ncalendar = calendar.drop(labels=['event_name_1','event_name_2','wday'],axis=1)\n\n### 5: code the year variable to categorical variable to save the RAM size\ncalendar.loc[calendar['year']==2012,'year'] = 5 ### Count in this level: 11159340\ncalendar.loc[calendar['year']==2015,'year'] = 4 ### Count in this level: 11128850\ncalendar.loc[calendar['year']==2014,'year'] = 3 ### Count in this level: 11128850\ncalendar.loc[calendar['year']==2013,'year'] = 2 ### Count in this level: 11128850\ncalendar.loc[calendar['year']==2011,'year'] = 1 ### Count in this level: 10275130\ncalendar.loc[calendar['year']==2016,'year'] = 0 ### Count in this level:  3506350\n\n### 6: Take out the 'd_' in each rows for variable 'd'\ncalendar['d'] = calendar['d'].map(lambda x: x.lstrip('d_'))\ncalendar['d'] = calendar['d'].astype('int64')\n\n### 7: Save the cleaned dataset\ncalendar.to_csv('..\/input\/m5-forecasting-accuracy\/new_calendar.csv',index=False)","da8888a8":"### Calendar Dataset after Data Cleaning\ncalendar.head()","0389172b":"### 1: Melt the data as mentioned before\nsales_train_validation = sales_train_validation.melt(id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],var_name='d',value_name='Unit_Sales')\n\n### 2: Get rid of d_ in d column\nsales_train_validation['d'] = sales_train_validation['d'].map(lambda x: x.lstrip('d_'))\nsales_train_validation['d'] = sales_train_validation['d'].astype('int64')\n\n### 4: Recode cat_id & state_id to numerical expression\nsales_train_validation.loc[sales_train_validation['cat_id']=='HOBBIES','cat_id'] = 0 \nsales_train_validation.loc[sales_train_validation['cat_id']=='HOUSEHOLD','cat_id'] = 1\nsales_train_validation.loc[sales_train_validation['cat_id']=='FOODS','cat_id'] = 2 \n\nsales_train_validation.loc[sales_train_validation['state_id']=='TX','state_id'] = 0\nsales_train_validation.loc[sales_train_validation['state_id']=='WI','state_id'] = 1 \nsales_train_validation.loc[sales_train_validation['state_id']=='CA','state_id'] = 2 ","6b98db10":"### sales_train_validation after data cleaning\nsales_train_validation.head()","22d4b825":"fig,(ax0,ax1) = plt.subplots(nrows=1,ncols=2,figsize=(20,10))\nsns.countplot(x='weekday',data=calendar,hue = 'event_type_1',ax=ax0)\nsns.countplot(x='year',data=calendar,hue = 'event_type_1',ax=ax1)\n\nax0.set_title('Which weekday has most event')\nax1.set_title('Which year has most event')\nplt.show()","6cda92f7":"### KDE Plot\nsns.distplot(sell_price['sell_price'],bins=2,hist=False,rug=True,norm_hist=True,kde_kws={'shade':True})\nplt.title('KDE Plot of Distribution for Sell Price')\nplt.ylabel('Density')\nplt.show()","e3456a6a":"### EDCF Chart: \n# Almost 100% of the sales price are less than $35. There are a couple of outliers larger than 40\nx = np.sort(sell_price['sell_price'])\ny = np.arange(1,len(x)+1) \/ len(x)\n\npercentile = np.array([0,25,50,75,100])\nx_percent = np.percentile(x,q=percentile)\n\nplt.plot(x,y,marker='.',linestyle='none',color='orange')\nplt.plot(x_percent,x_percent\/100,marker='D',linestyle='none',color='green')\nplt.title('EDCF Chart for Sell Price')\nplt.xlabel('Percentage')\nplt.ylabel('EDCF')\nplt.legend(('sell_price','sell_price_percentile'))\nplt.show()","c9dc3d91":"### 1: Prepare the calendar & sales_train_validation datasets\n\n# 1-1: Select `year` start from 2014\ncalendar = calendar[calendar['year'] > 2013]\n# 1-2: Keep the date for FB prophet & d for data merging\ncalendar = calendar.loc[:,['date','d']]\n# 1-3: Keep `id` for indication, `d` for data merging, and `Unit_Sales` for FB prophet prediction\nsales = sales.loc[:,['id','d','Unit_Sales']]\n\n### 2: Merge two datasets\nTime_Series = sales.merge(right=calendar,how='inner',on='d')\n\n### 3: Drop the Unnessasary Columns\nTime_Series.drop(labels='d',axis=1,inplace=True)\n\n### 4: Extract the id info to seperate the whole Time_Sries dataset to aviod long running time\nID = Time_Series['id']\nID.drop_duplicates(inplace=True) # Keep the unique value 30490\n\nTime_Series_one_ID = ID[:7624].values.tolist()\nTime_Series_two_ID = ID[7624:15247].values.tolist()\nTime_Series_three_ID = ID[15247:22870].values.tolist()\nTime_Series_four_ID = ID[22870:].values.tolist()\n\n### 5: Set the id to index, faster filtering time\nTime_Series = Time_Series.set_index('id')\n\n### 6: Extract by ID\nTime_Series_one = Time_Series[Time_Series.index.isin(Time_Series_one_ID)]\nTime_Series_two = Time_Series[Time_Series.index.isin(Time_Series_two_ID)]\nTime_Series_three = Time_Series[Time_Series.index.isin(Time_Series_three_ID)]\nTime_Series_four = Time_Series[Time_Series.index.isin(Time_Series_four_ID)]\n\n### 7: Convert the ID back\nTime_Series_one.reset_index(inplace=True)\nTime_Series_two.reset_index(inplace=True)\nTime_Series_three.reset_index(inplace=True)\nTime_Series_four.reset_index(inplace=True)\n\n### 8: Save the dataset\nTime_Series_one.to_csv('\/content\/drive\/My Drive\/Colab Notebooks\/Data set\/Time Series\/Time_Series_one.csv',index=False)\nTime_Series_two.to_csv('\/content\/drive\/My Drive\/Colab Notebooks\/Data set\/Time Series\/Time_Series_two.csv',index=False)\nTime_Series_three.to_csv('\/content\/drive\/My Drive\/Colab Notebooks\/Data set\/Time Series\/Time_Series_three.csv',index=False)\nTime_Series_four.to_csv('\/content\/drive\/My Drive\/Colab Notebooks\/Data set\/Time Series\/Time_Series_four.csv',index=False)","50ac9778":"Time_Series_one = pd.read_csv('..\/input\/sample-prediction-dataset\/Hao_Time_Series.csv',parse_dates=['date'])","564fdecb":"### 1: Delete the index from previous save \nTime_Series_one.drop(labels='Unnamed: 0',axis=1,inplace=True)\n\n### 2: Set index as id to retrieve specific id related info\nTime_Series_one.set_index('id',inplace=True)\nTime_Series_one.index.name = None\n\n### 3: Get dataset ready for the format required by prophet\nTime_Series_one.rename(columns={'Unit_Sales':'y','date':'ds'},inplace=True)\n\n### 4: Create a list contains each unique ID for prediction use\nTime_id = Time_Series_one.index\nTime_id = Time_id.drop_duplicates()","bfdae5ca":"Time_Series_one.head()","b4933594":"### 5: Create a mini dataset for demonstration\nTime_Series_Mini = Time_Series_one.loc[['HOBBIES_1_001_CA_1_validation','HOBBIES_1_002_CA_1_validation',\n                   'HOBBIES_1_003_CA_1_validation','HOBBIES_1_004_CA_1_validation','HOBBIES_1_005_CA_1_validation'],:]\n\n### 6: Save the Time_Series_Mini_ID for prediction result\nTime_Series_Mini_ID = Time_Series_Mini.index.drop_duplicates().tolist()","ed838cd5":"### 7: Run the prediction by FB Prophet\nholder = []\n\nfor id in Time_Series_Mini_ID: ### Run prediction on each Unique ID by for loop\n  data = Time_Series_Mini.loc[id,:]\n  prophet = fbprophet.Prophet(daily_seasonality=True)\n  prophet = prophet.fit(data)\n# We don't care the past unit sales, but you definitely keep these entries to exam the performance\n  Future_Data = prophet.make_future_dataframe(periods=56,include_history=False) \n  Prediction = prophet.predict(Future_Data)\n  Prediction = Prediction.loc[:,'yhat'] # yhat is the predicted result, so keep only this column\n  holder.append(Prediction)","3b570b4a":"### 8: Turn the result to dataframe and here we have the 56 days unit sales prediction\nResult = pd.DataFrame(holder,index=Time_Series_Mini_ID)","4eb281a4":"### 9: Create an empty holder to get xxx_xx_evaluation ID\nTime_Series_Mini_ID_evaluation = []\n\n# 9-1: Take out 'validation' and add 'evaluation'\nfor iden in Time_Series_Mini_ID:\n    new_id = iden[:-10] + 'evaluation'\n    Time_Series_Mini_ID_evaluation.append(new_id)\n\n# 9-2: Create the full id list\nfor i in Time_Series_Mini_ID_evaluation:\n    Time_Series_Mini_ID.append(i)\n\n### 10: Create the column name list\nResult_Column_List = [\"F\" + str(i) for i in range(1,29)]\n\n### 11: Seperate the Result dataset to required submission format\nResult_validation = Result.iloc[:,0:28]\nResult_evaluation = Result.iloc[:,28:56]\n\n### 12: Change the column name for both dataset\nResult_validation.columns = Result_Column_List\nResult_evaluation.columns = Result_Column_List\n\n### 13: Append two dataset\nResult = pd.concat([Result_validation,Result_evaluation])\n\n### 14: Change the Result dataset's index to required format\nResult.index = Time_Series_Mini_ID\n\n### 15: Reset the index to numerical & rename to `id` \nResult.reset_index(level=0,inplace=True)\nResult.rename(columns = {'index':'id'},inplace=True)","bc7e2480":"### Final Result for Prediction with FB Prophet - Time Series Mini Dataset\nResult","c2af95fe":"### 1: Calendar \n\n# 1-1: Train Calender (d_1 - d_1913)\ncalendar_train = calendar[(calendar['d'] > 0) & (calendar['d'] < 1914)]\ncalendar_train_1 = calendar_train[calendar_train['month']==5]\ncalendar_train_2 = calendar_train[calendar_train['month']==6]\ncalendar_train = pd.concat((calendar_train_1,calendar_train_2))\n# 1-2: Validation Calender (d_1914 - d_1942)\ncalendar_validation = calendar[(calendar['d']>1913) & (calendar['d'] < 1941)]\n# 1-3: Eveluation Calender (d_1942 - d_1969)\ncalendar_eveluation = calendar[calendar['d']>1941]\n\n### 2: Seperate the sell_price for train, validation, and evaluation\n\n### 2-1: Get sell_price for train data \nprice_train_week = calendar_train['wm_yr_wk'].array\nsell_price_train = sell_price[sell_price['wm_yr_wk'].isin(price_train_week)]\n### 2-2: Get sell_price for validation data \nprice_validation_week = calendar_validation['wm_yr_wk'].array\nsell_price_validation = sell_price[sell_price['wm_yr_wk'].isin(price_validation_week)]\n### 2-3: Get sell_price for eveluation data\nprice_eve_week = calendar_eveluation['wm_yr_wk'].array\nsell_price_eveluation = sell_price[sell_price['wm_yr_wk'].isin(price_eve_week)]\n\n### 3: Get the merged Validation & Evaluation dataset\n# 3-1: Validation dataset\nValidation_Data = sell_price_validation.merge(right=calendar_validation,how='left',on='wm_yr_wk',copy=False)\nValidation_Data['id'] = Validation_Data['item_id'] + '_' + Validation_Data['store_id'] + '_validation'  \nValidation_Data.drop(labels=['store_id','item_id'],axis=1,inplace=True)\n# 3-2: Evaluation dataset\nEveluation_Data = sell_price_eveluation.merge(right=calendar_eveluation,how='left',on='wm_yr_wk',copy=False)\nEveluation_Data['id'] = Eveluation_Data['item_id'] + '_' + Eveluation_Data['store_id'] + '_evaluation'  \nEveluation_Data.drop(labels=['store_id','item_id'],axis=1,inplace=True)\n\n### 4: Get the Merged Train Dataset\ntrain_calendar_price = sell_price_train.merge(right=calendar_train,on='wm_yr_wk',how='left',copy=False)\ntrain_dataset = sales_train_validation.merge(right=train_calendar_price,on=['item_id','store_id','d'],how='inner',copy=False)\ntrain_dataset.drop(labels=['item_id','store_id','cat_id','state_id'],axis=1,inplace=True)","018de1f8":"### Until This point the merged and ready-to-use train, validation, and evaluation dataset are ready\n### But to use it in deep learning, we need do some feature engineering\n#__________________________________________________________________________________________________________________________#\n\n### 1: Train Dataset\n\nID = train_dataset['id']\ntrain_dataset.drop(labels=['d','wm_yr_wk','date','event_type_2'],axis=1,inplace=True)\ntrain_dataset.drop(labels=['month','year'],axis=1,inplace=True)\n\n# 1-1: Get New Variable Category based on ID\n\ntrain_dataset['category'] = train_dataset['id'].str[:9]\ntrain_dataset.loc[train_dataset['category']=='HOUSEHOLD','category'] = 0\ntrain_dataset.loc[train_dataset['category']=='HOBBIES_1','category'] = 1\ntrain_dataset.loc[train_dataset['category']=='HOBBIES_2','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_1_0','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_1_1','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_3_4','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_3_2','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_3_3','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_3_7','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_3_1','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_3_6','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_3_0','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_3_5','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_2_3','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_2_1','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_2_0','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_2_2','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_3_8','category'] = 2\ntrain_dataset.loc[train_dataset['category'] == 'FOODS_1_2','category'] = 2\n\n# 1-2: Dummy Encoding weekday & category\n\nweekday = pd.get_dummies(train_dataset['weekday'])\nweekday.columns = ['weekday_0','weekday_1','weekday_2','weekday_3','weekday_4','weekday_5','weekday_6']\ntrain_dataset = pd.concat([train_dataset,weekday],axis=1)\ntrain_dataset.drop(labels='weekday',axis=1,inplace=True)\n\ncategory = pd.get_dummies(train_dataset['category'])\ncategory.columns = ['category_0','category_1','category_2']\ntrain_dataset = pd.concat([train_dataset,category],axis=1)\ntrain_dataset.drop(labels='category',axis=1,inplace=True)\n\n# 1-3: Process the Continouos Variable sell_price and divided by largest value of sell_price\ntrain_dataset['sell_price'] = train_dataset['sell_price'] \/ max(train_dataset['sell_price'])\n\n# 1-4:  Sort the Dataframe to match with the id sequence in Train & make sure the d is ascending\ntrain_ready['d'] = train_dataset.d\nID_v = train_ready['id'] \nsortIndex = dict(zip(ID_v,train_ready.index))\ntrain_ready['SortIndex'] = train_ready['id'].map(sortIndex)\ntrain_ready.sort_values(by=['d','SortIndex'],ascending=[True,True],inplace=True)\ntrain_ready.drop(labels='SortIndex',axis=1,inplace=True)\n\n#__________________________________________________________________________________________________________________________#\n\n### 2: Validation Dataset\n\nID_V = Validation_Data['id']\nValidation_Data.drop(labels=['store_id','item_id','date','event_type_2'],axis=1,inplace=True)\nValidation_Data.drop(labels=['month','year'],axis=1,inplace=True)\n\n# 2-1: Get New Variable Category based on ID\n\nValidation_Data['category'] = Validation_Data['id'].str[:9]\nValidation_Data.loc[Validation_Data['category']=='HOUSEHOLD','category'] = 0\nValidation_Data.loc[Validation_Data['category']=='HOBBIES_1','category'] = 1\nValidation_Data.loc[Validation_Data['category']=='HOBBIES_2','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_1_0','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_1_1','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_3_4','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_3_2','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_3_3','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_3_7','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_3_1','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_3_6','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_3_0','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_3_5','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_2_3','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_2_1','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_2_0','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_2_2','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_3_8','category'] = 2\nValidation_Data.loc[Validation_Data['category'] == 'FOODS_1_2','category'] = 2\n\n# 2-2: Dummy Encoding weekday & category\nweekday = pd.get_dummies(Validation_Data['weekday'])\nweekday.columns = ['weekday_0','weekday_1','weekday_2','weekday_3','weekday_4','weekday_5','weekday_6']\nValidation_Data = pd.concat([Validation_Data,weekday],axis=1)\nValidation_Data.drop(labels='weekday',axis=1,inplace=True)\n\ncategory = pd.get_dummies(Validation_Data['category'])\ncategory.columns = ['category_0','category_1','category_2']\nValidation_Data = pd.concat([Validation_Data,category],axis=1)\nValidation_Data.drop(labels='category',axis=1,inplace=True)\n\n# 2-3: Process the Continouos Variable sell_price and divided by largest value of sell_price\nValidation_Data['sell_price'] = Validation_Data['sell_price'] \/ max(Validation_Data['sell_price'])\n\n# 2-4:  Sort the Dataframe to match with the id sequence in Train & make sure the d is ascending\nvalidation_ready['d'] = Validation_Data.d\nID_v = validation_ready['id'] \nsortIndex = dict(zip(ID_v,validation_ready.index))\nvalidation_ready['SortIndex'] = validation_ready['id'].map(sortIndex)\nvalidation_ready.sort_values(by=['d','SortIndex'],ascending=[True,True],inplace=True)\nvalidation_ready.drop(labels='SortIndex',axis=1,inplace=True)\n\n#__________________________________________________________________________________________________________________________#\n\n### 3: Eveluation Dataset\n\nID_V = Eveluation_Data['id']\nEveluation_Data.drop(labels=['store_id','item_id','date','event_type_2'],axis=1,inplace=True)\nEveluation_Data.drop(labels=['month','year'],axis=1,inplace=True)\n\n# 3-1: Get New Variable Category based on ID\n\nEveluation_Data['category'] = Eveluation_Data['id'].str[:9]\nEveluation_Data.loc[Eveluation_Data['category']=='HOUSEHOLD','category'] = 0\nEveluation_Data.loc[Eveluation_Data['category']=='HOBBIES_1','category'] = 1\nEveluation_Data.loc[Eveluation_Data['category']=='HOBBIES_2','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_1_0','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_1_1','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_3_4','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_3_2','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_3_3','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_3_7','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_3_1','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_3_6','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_3_0','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_3_5','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_2_3','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_2_1','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_2_0','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_2_2','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_3_8','category'] = 2\nEveluation_Data.loc[Eveluation_Data['category'] == 'FOODS_1_2','category'] = 2\n\n# 3-2: Dummy Encoding weekday & category\nweekday = pd.get_dummies(Eveluation_Data['weekday'])\nweekday.columns = ['weekday_0','weekday_1','weekday_2','weekday_3','weekday_4','weekday_5','weekday_6']\nEveluation_Data = pd.concat([Eveluation_Data,weekday],axis=1)\nEveluation_Data.drop(labels='weekday',axis=1,inplace=True)\n\ncategory = pd.get_dummies(Eveluation_Data['category'])\ncategory.columns = ['category_0','category_1','category_2']\nEveluation_Data = pd.concat([Eveluation_Data,category],axis=1)\nEveluation_Data.drop(labels='category',axis=1,inplace=True)\n\n# 3-3: Process the Continouos Variable sell_price and divided by largest value of sell_price\nEveluation_Data['sell_price'] = Eveluation_Data['sell_price'] \/ max(Eveluation_Data['sell_price'])\n\n# 2-4:  Sort the Dataframe to match with the id sequence in Train & make sure the d is ascending\nEvaluation_ready['d'] = Eveluation_Data.d\nID_v = Evaluation_ready['id'] \nsortIndex = dict(zip(ID_v,Evaluation_ready.index))\nEvaluation_ready['SortIndex'] = Evaluation_ready['id'].map(sortIndex)\nEvaluation_ready.sort_values(by=['d','SortIndex'],ascending=[True,True],inplace=True)\nEvaluation_ready.drop(labels='SortIndex',axis=1,inplace=True)","6d8ac89d":"### Take a Look of train dataset. Since the validation & evaluation has same column as validation \n### (Except Unit_Sales is not in both dataset)\ntrain_ready.head()","df0efe93":"### Data preparation for LSTM, turn 2D array to 3D as input\nx_train = train_ready.drop(labels='Unit_Sales',axis=1)\ny_train = train_ready['Unit_Sales']\n\nx_train_float = x_train.to_numpy()\ny_train_float = y_train.to_numpy()\n\nx_train_lstm = x_train_float.reshape((x_train_float.shape[0],1,x_train_float.shape[1])) \ny_train_lstm = y_train_float\n\n\nprint(x_train_float.shape) \nprint(y_train_float.shape)\nprint(x_train_lstm.shape) \nprint(y_train_lstm.shape)","e237f789":"### Use rmse as validation metrics & Early Stopping to evaluate the model performance\nrmse = RootMeanSquaredError()\nmonitor = EarlyStopping(patience=3,monitor='val_root_mean_squared_error') ","e2380150":"### Build First Model as Dense layer - set the baseline\nmodel = Sequential()\nmodel.add(Dense(64,activation='relu',input_shape = (15,)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1))\nmodel.compile(optimizer='Adam',loss='mean_squared_error',metrics=[rmse])\nmodel.fit(x= x_train_float,y= y_train_float,epochs=20,validation_split=0.2,callbacks=[monitor],batch_size=600)\n\ntrain_loss = model.history.history['loss']\ntrain_rmse = model.history.history['root_mean_squared_error']\nvalidation_loss = model.history.history['val_loss']\nvalidation_rmse = model.history.history['val_root_mean_squared_error']\n\n### Plot Loss\nplt.plot(range(1,8),train_loss,'orange',label='train_loss')\nplt.plot(range(1,8),validation_loss,'green',label='validation_loss')\nplt.legend()\nplt.title('Trian & Validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('loss')\nplt.show()\n\n### Plot rmse\nplt.plot(range(1,8),train_rmse,'green',label='Train rmse')\nplt.plot(range(1,8),validation_rmse,'orange',label='Validation rmse')\nplt.legend()\nplt.title('Trian & Validation rmse')\nplt.xlabel('Epoch')\nplt.ylabel('rmse')\nplt.show()","7fab0e5c":"K.clear_session()\n### Build Second Model as LSTM layer\n\nmodel_lstm = Sequential()\nmodel_lstm.add(LSTM(64,input_shape=(x_train_lstm.shape[1],x_train_lstm.shape[2]),return_sequences=True))\nmodel_lstm.add(LSTM(64,return_sequences=True))\nmodel_lstm.add(LSTM(64,return_sequences=True))\nmodel_lstm.add(LSTM(64,return_sequences=True))\nmodel_lstm.add(LSTM(64,return_sequences=True))\nmodel_lstm.add(LSTM(64,return_sequences=True))\nmodel_lstm.add(LSTM(64,return_sequences=True))\nmodel_lstm.add(LSTM(64,return_sequences=True))\nmodel_lstm.add(LSTM(64,return_sequences=True))\nmodel_lstm.add(LSTM(64,return_sequences=True))\nmodel_lstm.add(LSTM(64,return_sequences=True))\nmodel_lstm.add(LSTM(64))\nmodel_lstm.add(Dense(1))\nmodel_lstm.compile(optimizer='rmsprop',loss='mean_squared_error',metrics=[rmse])\nmodel_lstm.fit(x= x_train_lstm,y= y_train_lstm,epochs=20,validation_split=0.2,callbacks=[monitor],batch_size=200)\n\ntrain_loss = model_lstm.history.history['loss']\ntrain_rmse = model_lstm.history.history['root_mean_squared_error']\nvalidation_loss = model_lstm.history.history['val_loss']\nvalidation_rmse = model_lstm.history.history['val_root_mean_squared_error']\n\n### Plot Loss\nplt.plot(range(1,16),train_loss,'orange',label='train_loss')\nplt.plot(range(1,16),validation_loss,'green',label='validation_loss')\nplt.legend()\nplt.title('Trian & Validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('loss')\nplt.show()\n\n### Plot rmse\nplt.plot(range(1,16),train_rmse,'green',label='Train rmse')\nplt.plot(range(1,16),validation_rmse,'orange',label='Validation rmse')\nplt.legend()\nplt.title('Trian & Validation rmse')\nplt.xlabel('Epoch')\nplt.ylabel('rmse')\nplt.show()","ed861f19":"## Submission File Evaluation <a id=\"2.1\"><\/a>\nEach row contains an id that is a concatenation of an `item_id` and a `store_id`, which is either `validation` or `evaluation`. Participants are predicting 28 forecast days `(F1-F28)` of items sold for each row. For the `validation` rows, this corresponds to `d_1914 - d_1941`, and for the `evaluation` rows, this corresponds to `d_1942 - d_1969`.\n\n**Submission File Example**\n\n| ID                            \t| F1 \t| F2 \t| F3 \t| F4 \t| F5 \t| ...... \t| F27 \t| F28 \t|\n|-------------------------------\t|----\t|----\t|----\t|----\t|----\t|--------\t|-----\t|-----\t|\n| HOBBIES_1_001_CA_1_validation \t| 1  \t| 2  \t| 3  \t| 4  \t| 5  \t| ...... \t| 3   \t| 1   \t|\n| HOBBIES_1_002_CA_1_validation \t| 3  \t| 3  \t| 9  \t| 4  \t| 2  \t| ...... \t| 1   \t| 3   \t|\n| ............................. \t| .. \t| .. \t| .. \t| .. \t| .. \t| ...... \t| ..  \t| ..  \t|\n| HOBBIES_1_001_CA_1_evaluation \t| 3  \t| 5  \t| 9  \t| 0  \t| 3  \t| ...... \t| 2   \t| 2   \t|\n| HOBBIES_1_002_CA_1_evaluation \t| 2  \t| 0  \t| 0  \t| 1  \t| 0  \t| ...... \t| 3   \t| 4   \t|","364538b0":"**EDCF Chart for Sell Price**\n> - Almost 100% of the sales price are less than `$35` and there are a couple of outliers larger than `$40`","34ed0b03":"## Variable Dictionary <a id=\"1.2\"><\/a>\n\n[Click to Exam the Datasets](http:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/data)\n\n**File 1: Calendar.csv:**\n\n| Variable                      \t| Explanation                                                                      \t|\n|-------------------------------\t|----------------------------------------------------------------------------------\t|\n| date                          \t| The date in a \u201cy-m-d\u201d format.                                                    \t|\n| wm_yr_wk                      \t| The id of the week the date belongs to.                                          \t|\n| weekday                       \t| The type of the day (Saturday, Sunday, \u2026, Friday).                               \t|\n| wday                          \t| The id of the weekday, starting from Saturday.                                   \t|\n| month                         \t| The month of the date.                                                           \t|\n| year                          \t| The year of the date.                                                            \t|\n| event_name_1                  \t| If the date includes an event, the name of this event.                           \t|\n| event_type_1                  \t| If the date includes an event, the type of this event.                           \t|\n| event_name_2                  \t| If the date includes a second event, the name of this event.                     \t|\n| event_type_2                  \t| If the date includes a second event, the type of this event.                     \t|\n| snap_CA, snap_TX, and snap_WI \t| A binary variable, whether the stores allow SNAP purchases on the examined date. \t","d84be231":"## Data Merging <a id=\"7.1\"><\/a>\n> - Since the evaluation data belongs to May & June, we only merge information related to these month\n","2dcf137d":"## sell_price <a id=\"4.3\"><\/a>\n\n**Takeaways from sales_train_validation data set**\n> - The dataset has `6841121 rows` and `4 columns` \n> - The maximium sell_price is much higher than the mean & median, and it suggests **the data contains outliers**\n\n> - Each row represent each item's sell_price across all store in each week\n> - The data includes sell price from at day 1 to day 1969 (all validation & eveluation data) \n","dfb7811b":"<img src=\"http:\/\/drive.google.com\/uc?export=view&id=16oCFXeI-xxOzKOaqReuzOuPL9uTdGmuK\" width=400px>\n<img src=\"http:\/\/drive.google.com\/uc?export=view&id=1p3dNP9174oYtTJYsaSt4VHlA9t1fJDrr\" width=400px>","185c74a5":"## Data Cleaning <a id=\"5.1\"><\/a>","21662695":"## sales_train_validation <a id=\"4.2\"><\/a>\n\n**Takeaways from sales_train_validation data set**\n> - `30490 rows` represent `30490 unique IDs`\n> - Columns such as `d_1`, `d_2`, `etc` represent the sales for each ID in each day\n> - 5 columns(`item_id`, `dept_id`, `cat_id`, `state_id`, and `store_id`) represent the characteristic of each ID\n\n> - The id is composed by `item_id + store_id + string validation or evaluation`\n> - The dataset only includes `unit sales` from **day 1 to day 1913**\n> - [The data is in wide data format](http:\/\/www.theanalysisfactor.com\/wide-and-long-data\/), **we have to turn it to long data format**\n> - The Final submission file is composed by 30490 for validation and 30490 for evaluation. In total, the **Final Submission should have 60980 rows**","3db331d4":"# Python Package Used <a id=\"3\"><\/a>","1c99328c":"## Calendar <a id=\"4.1\"><\/a>\n\n**Takeaways from Calendar Dataset**\n> - 1969 rows represent `day_1` to `day_1969`\n> - 14 columns represent the characteristic for each days\n> - `Date` from 2011-01-29 to 2016-06-19\n> - The number of unique `weekday` & `wday` are similar\n> - The number of unique `years` are similar except year 2016\n> - Binary variables `snap_CA`, `snap_TX`, and `snap_WI` have roughly 33% `1` and 67% `0`. Not a good variable to include in prediction model. \n> - 4 variables (`event_name_1`,`event_type_1`, `event_name_2`, `event_type_2`) have missing values. And almost all values for these variables are missing","027d1d29":"**Unit Sales Trend by Days & States (Between day 1 to day 1913)** \n> - Seasonality is strong and identical for all three States\n> - Wisconsin and Texas has almost identical sales\n\n<img src=\"http:\/\/drive.google.com\/uc?export=view&id=1HCxCxmy5VVtLir1m9uCsrajHcLgxR4Qw\" width=600px>\n","fd96ecde":"**event_type_1 for each weekday & year**","6a1e673f":"**File 2: Sell_Prices.csv**\n\n| Variable   \t| Explanation                                                                                                    \t|\n|------------\t|----------------------------------------------------------------------------------------------------------------\t|\n| store_id   \t| The id of the store where the product is sold.                                                                 \t|\n| item_id    \t| The id of the product.                                                                                         \t|\n| wm_yr_wk   \t| The id of the week.                                                                                            \t|\n| sell_price \t| The price of the product for the given week\/store. The price is provided per week (average across seven days). \t|","44553a49":"## Project Objective <a id=\"2.2\"><\/a>\nUse the information such as `date`, `sell price`, and `state ID` for all ID between **day 1 to day 1913** to predict the `unit sales` between **day 1914 - day 1941** (Validation Data) and unit sales between **day 1942 - day 1969** (Evaluation Data).\n\n* `Independent Variable`: date, snap_CA, snap_WI, snap_TX, sell_price, etc \n* `Dependent Variable`: Unit Sales ","7c93d150":"**KDE Plot for Sell Price**","99a77054":"## Before You Start Reading\nIt was such a rewardful and exciting journey to participate in the 2020 M5 accuracy competition! Although our team merely ranked at 721st place among 5558 teams, we learned and grew as data analysts. \n\nThis journey allowed us to spend time with new concepts and tools such as time-series data and panel data, LSTM, GRU, Facebook Prophet, and Keras. \n\nWe are not experts in data science, but we would love to be one in the future. If you have any comments or advice, please don't hesitate to let us know in the comment or reach us out on LinkedIn. \n\nThank you for your time, and please enjoy it! \n\n## Team Member:\n1. [Hao Jiang](http:\/\/www.linkedin.com\/in\/umdhaojiang\/)\n2. [Shruthi Suresh](http:\/\/www.linkedin.com\/in\/shruthi-suresh-213157ab\/)\n3. [Alexander Binder](http:\/\/www.linkedin.com\/in\/alexander-jm-binder\/)\n\nSpecial Thanks to Professor [Liye Ma](http:\/\/www.linkedin.com\/in\/liye-ma-7b10492\/) for his guildence and advise along the way","4f85cb07":"# Prediction Methodology 1 - Time Series With Facebook Prophet <a id=\"6\"><\/a>","d9a7a5ad":"## Prediction <a id=\"7.2\"><\/a>\n\n> - For Prediction, I used the LSTM method. However, This is the first time I use deep learning; therefore, the result is horrific\n> - First, I used the dense layer to create a baseline, the RMSE for validation split is approximatly 3.2; Then I used the LSTM layer to create the Final model, but the performance is even worse. \n> - Any suggestion or feedback for this section will be appreciated\n","588d3a8e":"## Prediction <a id=\"6.2\"><\/a>\n> - Because the newly created Time_Series File is a [Panel Data](https:\/\/en.wikipedia.org\/wiki\/Panel_data#:~:text=In%20statistics%20and%20econometrics%2C%20panel,the%20same%20firms%20or%20individuals.); therefore we made the prediction for each unique ID by for loop\n> - The running time for each Time_Series file is roughly 5 hours, so the total running time for four files is close to 20 hours\n> - In here, I will only make prediction for 5 unique ids for the next 56 days (28 for validation & 28 for evaluation) to save time\n\nThe Final Submission score by using FB Prophet is `0.72804`. ","b2f8ca53":"# Prediction Methodology 2 - Time Series With LSTM <a id=\"7\"><\/a>\n","f909278a":"# Evaluation & Project Objective <a id=\"2\"><\/a>","0dcbda34":"**Bar Chart for Unit Sales by Category & States**\n<img src=\"http:\/\/drive.google.com\/uc?export=view&id=18jeZKnaGVJxzQDJbhicA9cWKdfJD9Yy3\" width=600px>","fcd9c102":"**File 3: sales_train_validation**\n\n| Variable                   \t| Explanation                                                  \t|\n|----------------------------\t|--------------------------------------------------------------\t|\n| item_id                    \t| The id of the product.                                       \t|\n| dept_id                    \t| The id of the department the product belongs to.             \t|\n| cat_id                     \t| The id of the category the product belongs to.               \t|\n| store_id                   \t| The id of the store where the product is sold.               \t|\n| state_id                   \t| The State where the store is located.                        \t|\n| d_1, d_2, \u2026, d_i, \u2026 d_1941 \t| The number of units sold at day i, starting from 2011-01-29. \t|","4c5f7223":"### Data Cleaning - Calendar","de79c2e0":"**Pie Chart for Unit Sales by Category - Percentage**\n<img src=\"http:\/\/drive.google.com\/uc?export=view&id=1FK22rK-aoPUFrh0kcSzN4nb_h6_gzt96\" width=600px>","2789e7a6":"### Data Clearning - sales_train_validation","163e1cd4":"**Unit Sales Trend by Days & Category (Between day 1 to day 1913)** \n> - Seasonality is strong and identical for all three categories. \n\n<img src=\"http:\/\/drive.google.com\/uc?export=view&id=1TsllepyJiL-gDQkwT7Y6d5Y8VcG8Yqxs\" width=600px>\n","e3727f69":"## Date Merging for Facebook Prophet <a id=\"6.1\"><\/a>\n> - Facebook Prophet only accept two columns which are the Time Series & the Dependent variable (in this case, Unit Sales) \n> - In this case,The infomation we need can be found at calendar dataset & sales_train_validation dataset and the common variable for both datasets is `d`\n> - For train dataset, we will use infomation from 2014 to aviod long running time","cc3b965f":"## Competition Description <a id=\"1.1\"><\/a>\nIn this competition, participants will use hierarchical sales data from Walmart, the world\u2019s largest company by revenue, to **forecast daily sales for the next 28 days**. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.","29d9f4b0":"# Description <a id=\"1\"><\/a>","a57b7ec4":"# Kaggle M5 Prediction Competition Team Terp Report\n<img src=\"https:\/\/globalbiodefense.com\/wp-content\/uploads\/2014\/06\/umd_robert_h_smith.jpg\" width=\"600px\">\n<img src=\"https:\/\/cdn.mos.cms.futurecdn.net\/nQmYiDoMBoTSewCLEhCouU-1024-80.jpg.webp\" width=\"400px\">","175856ba":"# Data Cleaning & Visualization <a id=\"5\"><\/a>","b7805c88":"## Visualization <a id=\"5.2\"><\/a>","c5328c59":"<img src=\"http:\/\/drive.google.com\/uc?export=view&id=1fV2CwkwKo_iAKMZ6Ezp1vWMLRb-5mGOC\" width=400px>\n<img src=\"http:\/\/drive.google.com\/uc?export=view&id=1hzct4qNzAG6QUWhOKG3WKc5BvPMqlpdz\" width=400px>","0a2d74b9":"# Content of The Report\n\n* [<font size=4>Description<\/font>](#1)\n    * [Competition Description](#1.1)\n    * [Variable Dictionary](#1.2)\n\n* [<font size=4>Evaluation & Project Objective<\/font>](#2)\n    * [Submission File Evaluation](#2.1)\n    * [Project Objective](#2.2)\n    \n* [<font size=4>Python Package Used<\/font>](#3)\n\n* [<font size=4>Initial Data Set Exploration<\/font>](#4)\n    * [Calendar](#4.1)\n    * [Sales_Train_Validation](#4.2)\n    * [Sell_Price](#4.3)\n\n* [<font size=4>Data Cleaning and Visualization<\/font>](#5)\n    * [Data Cleaning](#5.1)\n    * [Visualization](#5.2)\n\n* [<font size=4>Prediction Methodology 1 - Time Series With Facebook Prophet<\/font>](#6)\n    * [Data Merging](#6.1)\n    * [Prediction](#6.2)\n\n* [<font size=4>Prediction Methodology 2 - Time Series With LSTM<\/font>](#7)\n    * [Data Merging](#7.1)\n    * [Prediction](#7.2)","9ba9c50b":"# Initial Data Set Exploration <a id=\"4\"><\/a>"}}