{"cell_type":{"d75bbe5a":"code","6a00aea5":"code","3ae886bd":"code","4b9c06e3":"code","9ad731f2":"code","b5751950":"code","37315a6f":"code","d3757cdb":"code","49904207":"code","10b14295":"code","277e0c18":"code","45a96317":"code","f928a839":"code","f70443ce":"code","9da3876f":"code","536ba409":"code","698d7a03":"code","dd212628":"code","e209fcd3":"code","1c8ba729":"code","ec1e86d8":"code","fcea905e":"code","2cb406e0":"code","3033639c":"code","06d08d13":"markdown","8e3b016c":"markdown","cb4b2b27":"markdown","8bb655bf":"markdown","1865f793":"markdown","59b8462a":"markdown","2486a0bd":"markdown","8ab00725":"markdown","ea3a76f2":"markdown","1be1e35f":"markdown","dee74d69":"markdown","b491f5b0":"markdown"},"source":{"d75bbe5a":"%matplotlib inline","6a00aea5":"import sys\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n\n# Let's print what versions of the libraries we're using\nprint(f\"python\\t\\tv {sys.version.split(' ')[0]}\\n===\")\nfor lib_ in [np, pd, sns, sklearn, ]:\n    sep_ = '\\t' if len(lib_.__name__) > 8 else '\\t\\t'\n    print(f\"{lib_.__name__}{sep_}v {lib_.__version__}\"); del sep_, lib_","3ae886bd":"import os\nos.getcwd()","4b9c06e3":"!ls","9ad731f2":"hitters = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\nhitters = hitters.dropna(inplace=False)\nhitters.head()","b5751950":"X = np.array(hitters.drop([\"Salary\", \"League\", \"Division\", \"NewLeague\"], axis=1))\ny = (hitters[\"Salary\"] >= np.median(hitters[\"Salary\"])).astype(\"int\")","37315a6f":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=10)","d3757cdb":"print(X_train.shape)\nprint(X_test.shape)","49904207":"logit = LogisticRegression(penalty=\"l2\", C=1e5, n_jobs=-1, max_iter=4000)\nlogit.fit(X_train, y_train)\n\ntest_preds = logit.predict_proba(X_test)[:, 1]","10b14295":"fpr, tpr, thresholds = roc_curve(y_test, test_preds)","277e0c18":"# we want to draw the random baseline ROC line too\nfpr_rand = tpr_rand = np.linspace(0, 1, 10)\n\nplt.plot(fpr, tpr)\nplt.plot(fpr_rand, tpr_rand, linestyle='--')\nplt.show()","45a96317":"roc_auc_score(y_test, test_preds)","f928a839":"# create equally space values beteen 10^-10 and 10^10\nc_vals = np.logspace(-10, 10, 20)\n\naucs = []\nfor c_val in c_vals:\n    logit = LogisticRegression(C=c_val)\n    logit.fit(X_train, y_train)\n\n    test_preds = logit.predict_proba(X_test)[:, 1]\n    aucs.append(roc_auc_score(y_test, test_preds))","f70443ce":"aucs","9da3876f":"plt.plot(np.log10(c_vals), aucs)\nplt.xlabel(\"C\")\nplt.ylabel(\"Test AUC\")\nplt.show()","536ba409":"param_grid = {\"C\": np.logspace(2, 8, 50)}","698d7a03":"cv = GridSearchCV(logit, param_grid, cv=10, n_jobs=-1, refit=True, verbose=True)\ncv.fit(X_train, y_train)","dd212628":"cv.best_estimator_","e209fcd3":"cv.best_params_","1c8ba729":"np.log10(1.0\/cv.best_params_['C'])","ec1e86d8":"cv.best_score_","fcea905e":"cv.cv_results_","2cb406e0":"test_preds = cv.best_estimator_.predict_proba(X_test)[:, 1]\ntest_preds","3033639c":"roc_auc_score(y_test, test_preds)","06d08d13":"We can use the test set (which, in this case, should really be called a validation set) to choose the best value of the tuneable parameter `C` of the logisitc regression, which is the inverse of $\\lambda$, the regularization strength.","8e3b016c":"Let's read in the \"Hitters\" dataset from ISLR that has information on baseball players, their stats, and their salaries.  Also, we'll drop any rows with missing values.","cb4b2b27":"Creating a training\/testing split is extremely simple:","8bb655bf":"Let's see what value of $\\lambda$ corresponds to the best C:","1865f793":"We'll get rid of a few categorical columns rather than deal with converting them.  Then we'll create a binary variable for whether a player makes more than the median salary.","59b8462a":"If we re-run the train\/test split, we'll see the variability in this estimate.","2486a0bd":"Next, we'll fit a logistic regression model to the training data and score the test data:","8ab00725":"In this notebook, we'll see how to create train\/test splits in python.  We'll also see the `GridSearchCV` function, which is a very simple way to do cross-validation to select tuneable parameters.","ea3a76f2":"We can see the best values and the grid scores:","1be1e35f":"# Session 02: Train\/Test, Cross-Validation, and Regularization","dee74d69":"Instead of using a train\/test split, scikit-learn has a really nice way to use cross-validation to choose the tuneable parameters of a model.  First, we make a dictionary, where the key is the name of the parameter we want to tune (it has to match the name of the parameter in the model), and the values are the values we want to try:","b491f5b0":"Then, we pass in the model we want to fit and the grid.  The option 'n_jobs' allows us to split the cross-validation over multiple cores of your computer, and `refit` tells it to fit the best performing model on the full dataset once it's done."}}