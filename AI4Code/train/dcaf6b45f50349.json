{"cell_type":{"acab10f2":"code","59932352":"code","cbfd3058":"code","fd624f51":"code","737473b8":"code","3c39f700":"code","2ed23183":"code","33301e8c":"code","93186116":"code","e6e552e7":"code","683ec439":"code","7f98e61b":"code","43fee75e":"markdown","3681088a":"markdown","a1870c4d":"markdown","e55e4af0":"markdown","c40bbc8c":"markdown"},"source":{"acab10f2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"..\/input\"))\n%matplotlib inline","59932352":"df_questions = pd.read_csv('..\/input\/Questions.csv', encoding='iso-8859-1')\ndf_tags = pd.read_csv('..\/input\/Tags.csv', encoding='iso-8859-1')\ndf_questions.head(n=2)","cbfd3058":"grouped_tags = df_tags.groupby(\"Tag\", sort='count').size().reset_index(name='count')\nfig = plt.figure(figsize=(12,10))\ngrouped_tags.plot(figsize=(12,7), title=\"Tag frequency\")","fd624f51":"num_classes = 100\ngrouped_tags = df_tags.groupby(\"Tag\").size().reset_index(name='count')\nmost_common_tags = grouped_tags.nlargest(num_classes, columns=\"count\")\ndf_tags.Tag = df_tags.Tag.apply(lambda tag : tag if tag in most_common_tags.Tag.values else None)\ndf_tags = df_tags.dropna()","737473b8":"counts = df_tags.Tag.value_counts()\nfirstlast = counts[:5].append(counts[-5:])\nfirstlast.reset_index(name=\"count\")","3c39f700":"import re \n\ndef strip_html_tags(body):\n    regex = re.compile('<.*?>')\n    return re.sub(regex, '', body)\n\ndf_questions['Body'] = df_questions['Body'].apply(strip_html_tags)\ndf_questions['Text'] = df_questions['Title'] + ' ' + df_questions['Body']\n\ndef tags_for_question(question_id):\n    return df_tags[df_tags['Id'] == question_id].Tag.values\n\ndef add_tags_column(row):\n    row['Tags'] = tags_for_question(row['Id'])\n    return row\n\ndf_questions = df_questions.apply(add_tags_column, axis=1)\ndf_questions[['Id', 'Text', 'Tags']].head()","2ed23183":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nmultilabel_binarizer = MultiLabelBinarizer()\nmultilabel_binarizer.fit(df_questions.Tags)\nY = multilabel_binarizer.transform(df_questions.Tags)\n\ncount_vect = CountVectorizer()\nX_counts = count_vect.fit_transform(df_questions.Text)\n\ntfidf_transformer = TfidfTransformer()\nX_tfidf = tfidf_transformer.fit_transform(X_counts)","33301e8c":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=9000)\nX_tfidf_resampled, Y_tfidf_resampled = ros.fit_sample(X_tfidf, Y)","93186116":"x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf_resampled, Y_tfidf_resampled, test_size=0.2, random_state=9000)","e6e552e7":"fig = plt.figure(figsize=(20,20))\n(ax_test, ax_train) = fig.subplots(ncols=2, nrows=1)\ng1 = sns.barplot(x=Y.sum(axis=0), y=multilabel_binarizer.classes_, ax=ax_test)\ng2 = sns.barplot(x=y_train_tfidf.sum(axis=0), y=multilabel_binarizer.classes_, ax=ax_train)\ng1.set_title(\"class distribution before resampling\")\ng2.set_title(\"class distribution in training set after resampling\")","683ec439":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import hamming_loss\n\ndef hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n    '''\n    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n    http:\/\/stackoverflow.com\/q\/32239577\/395857\n    '''\n    acc_list = []\n    for i in range(y_true.shape[0]):\n        set_true = set( np.where(y_true[i])[0] )\n        set_pred = set( np.where(y_pred[i])[0] )\n        tmp_a = None\n        if len(set_true) == 0 and len(set_pred) == 0:\n            tmp_a = 1\n        else:\n            tmp_a = len(set_true.intersection(set_pred))\/float(len(set_true.union(set_pred)) )\n        acc_list.append(tmp_a)\n    return np.mean(acc_list)\n\ndef print_score(y_pred, clf):\n    print(\"Clf: \", clf.__class__.__name__)\n    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_test_tfidf)))\n    print(\"Hamming score: {}\".format(hamming_score(y_pred, y_test_tfidf)))\n    print(\"---\")    ","7f98e61b":"nb_clf = MultinomialNB()\nsgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=6, tol=None)\nlr = LogisticRegression()\nmn = MultinomialNB()\n\nfor classifier in [nb_clf, sgd, lr, mn]:\n    clf = OneVsRestClassifier(classifier)\n    clf.fit(x_train_tfidf, y_train_tfidf)\n    y_pred = clf.predict(x_test_tfidf)\n    print_score(y_pred, classifier)","43fee75e":"## Data preparation\n\n*  strip html tags\n* denormalize tables \/ combine data frames with questions and tags into a single data frame","3681088a":"We will only use the top 100 used tags for our classifiers","a1870c4d":"## OneVsRest with different classifiers\nuse OneVsRest strategy to have one classifier for each class\/label","e55e4af0":"# Multi-label text classification with sklearn","c40bbc8c":"## Creating tf-idf feature\n* encode labels\n* compute tf-idf from questions"}}