{"cell_type":{"0a5015de":"code","9226d1d7":"code","5b714621":"code","78547a51":"code","9ed78781":"code","f4e358f7":"code","5477f951":"code","4f9ca0d6":"code","a60ebba0":"code","f0119497":"code","11ceab78":"code","8a0aa1fb":"code","2435b0da":"code","543ea64e":"markdown","8a301fd9":"markdown","82145d1e":"markdown","7f80150a":"markdown","a2235d8e":"markdown","b028ba7b":"markdown","16d4cbed":"markdown","7b664051":"markdown","90fe262f":"markdown","3ab578f0":"markdown","a28315f8":"markdown","82e62d52":"markdown","398d3aa5":"markdown"},"source":{"0a5015de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom IPython.display import Image\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9226d1d7":"data = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndata.head()","5b714621":"data.columns","78547a51":"x_data = data.drop([\"Outcome\"],axis=1)\ny = data.Outcome.values\n\n# we seperate the result( Outcome column ) and other variables from each other.","9ed78781":"x = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))\nx.head()","f4e358f7":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 42)\n\n# we will do matrice product, for matrice product first matrix's column and second matrix's row must be same so we will transpose our train\/test data\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","5477f951":"def fill_weights_and_bias(sizeofcolumn):\n    w = np.full((sizeofcolumn,1),0.01)\n    b = 0.00\n    return w,b\n\n# w = weights, b = bias ","4f9ca0d6":"def sigmoid(z):\n    #sigmoid function returns y_head value\n    y_head = (1 \/ ( 1 + np.exp(-z))) # its formula of sigmoid func.\n    return y_head","a60ebba0":"def forward_backward_propagation(w,b,x_train,y_train):\n    # we must use weights and bias for training model\n    # we must change w and b for appropriate shape to matrice product\n    \n    z = np.dot(w.T,x_train) + b\n    \n    y_head = sigmoid(z)\n    \n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head) #thats formula for our wrong predictions\n    cost = (np.sum(loss))\/x_train.shape[1] # thats average of loss \n    # forward propagation is completed\n    \n    # backward propagation\n    \n    derivative_weight = (np.dot(x_train,((y_head-y_train).T))) \/ x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train) \/ x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    \n    return cost,gradients\n\n    #this func loop 1 times but we want to update our data as we learn new datas.","f0119497":"def update(w,b,x_train,y_train,learning_rate,loopnumber):\n    \n    cost_list = []\n    cost_list2  =[]\n    index = []\n    \n    # updating(learning) parameters is loopnumber times\n    for i in range(1,loopnumber):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)\n        \n        # updating \n        w = w - learning_rate*gradients[\"derivative_weight\"]\n        b = b - learning_rate*gradients[\"derivative_bias\"]\n        \n        # we may want information about progress\n        if( i % 10 == 0):\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after {} times loop: {}\".format(i,cost))\n        \n    # showing progress as visual is important\n    parameters = {\"weights\" : w,\"bias\" : b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Loop\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","11ceab78":"def predict(w,b,x_test):\n    \n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n            \n    # if z is bigger than 0.5, our prediction is sign one (y_head = 1)\n    # if z is smaller than 0.5, our prediction is sign zero ( y_head = 0)\n    \n    for i in range(1,x_test.shape[1]):\n        if (z[0,i] <= 0.5):\n            y_prediction[0,i] == 0\n        else:\n            y_prediction[0,i] == 1\n            \n    return y_prediction","8a0aa1fb":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,loopnumber):\n    # initialize\n    sizeofcolumn = x_train.shape[0]\n    w,b = fill_weights_and_bias(sizeofcolumn)\n    \n    # forward and backward propagation\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,loopnumber)\n    \n    y_prediction_test = predict(parameters[\"weights\"], parameters[\"bias\"], x_test)\n    # y_prediction_test our y values for test data now we will comparise each other\n    \n    #print test erros\n    print(\"Test accuracy is: {}\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100 ))","2435b0da":"logistic_regression(x_train, y_train, x_test, y_test, learning_rate = 1, loopnumber=600)","543ea64e":"# INTRODUCTION\n* In this kernel, we will investigate Indian diabete data and try to apply logistic regression.\n\n<br>Content:\n1. [Import Libraries](#1)\n1. [Reading Data](#2)\n1. [Normalization](#4)\n1. [Train\/Test Split](#5)\n1. [Parameter initialize and sigmoid function](#6)\n1. [Updating Parameters](#7)\n1. [Prediction](#8)\n1. [Logistic Regression](#9)\n1. [Test the Model](#10)","8a301fd9":"<a id=\"10\"><\/a> <br>\n# Testing our Logistic Regression Model","82145d1e":"#### now, features are row and values are column","7f80150a":"<a id=\"5\"><\/a> <br>\n# Train\/Test Split","a2235d8e":"<a id=\"2\"><\/a> <br>\n# Reading Data","b028ba7b":"<a id=\"7\"><\/a> <br>\n# Updating Parameters","16d4cbed":"# =============================================================================\n# Means of Columns\n\n* Pregnanices: Number of times pregnant\n* Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* BloodPressure: Diastolic blood pressure (mm Hg)\n* SkinThickness: Triceps skin fold thickness (mm)\n* Insulin: 2-Hour serum insulin (mu U\/ml)\n* BMI: Body mass index (weight in kg\/(height in m)^2)\n* DiabetesPedigreeFunction: Diabetes pedigree function\n* Age: Age (years)\n* Outcome: Class variable (0 or 1) 268 of 768 are 1, the others are 0\n\n# =============================================================================","7b664051":"<a id=\"6\"><\/a> <br>\n# Parameter Initialize and Sigmoid Function","90fe262f":"<a id=\"1\"><\/a> <br>\n# Import Libraries","3ab578f0":"* Features which have too much numeric value can be dominated less numeric value features\n* so, we normalize our data to predict most truth machine learning model.\n* Normalizing is compress the data values between 0-1 as proportionally.","a28315f8":"<a id=\"9\"><\/a> <br>\n# Logistic Regression","82e62d52":"<a id=\"4\"><\/a> <br>\n# Normalization","398d3aa5":"<a id=\"8\"><\/a> <br>\n# Prediction"}}