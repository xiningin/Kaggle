{"cell_type":{"8f227b0e":"code","ff7decad":"code","e1e1fbb1":"code","feb5b193":"code","497be41d":"code","96cb62a4":"code","e409c727":"code","e2753c7a":"code","e13c68aa":"code","c83b025a":"code","48b25409":"code","23d3fa72":"code","c30ae6a3":"code","8bbc8558":"code","acf2656d":"markdown","2838cd44":"markdown"},"source":{"8f227b0e":"import os\nimport glob\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_addons as tfa\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","ff7decad":"image_size = 256\n\ndef read_image(image_size=image_size):  # run this at the first time to read and save the images and labels as numpy array\n    path = '..\/input\/chest-xray-pneumonia\/chest_xray\/'\n    image_set = []\n    label_set = []\n    for i in ['test','train','val']:\n        for image_loc in os.listdir(path+i+'\/NORMAL'):\n            image = cv2.imread(path+i+'\/NORMAL\/'+image_loc,cv2.IMREAD_GRAYSCALE)\n            image = cv2.resize(image,(image_size,image_size))\n            image = StandardScaler().fit_transform(image)\n            image_set.append(image)\n            label_set.append(0)\n        for image_loc in os.listdir(path+i+'\/PNEUMONIA'):\n            image = cv2.imread(path+i+'\/PNEUMONIA\/'+image_loc,cv2.IMREAD_GRAYSCALE)\n            image = cv2.resize(image,(image_size,image_size))\n            image = StandardScaler().fit_transform(image)\n            image_set.append(image)\n            label_set.append(1)\n    image_set = np.array(image_set)\n    image_set = np.expand_dims(image_set,axis=3)\n    label_set = np.array(label_set)\n    np.save('image_set_%s.npy'%image_size,image_set)\n    np.save('label_set_%s.npy'%image_size,label_set)\n    \n# read_image(image_size)","e1e1fbb1":"path = '..\/input\/building-an-inception-resnetv2-by-yourself\/'\nimage = np.load(glob.glob(path+'image*.npy')[0])\nlabel = np.load(glob.glob(path+'label*.npy')[0])\n# np.save('image_set_%s.npy'%image_size,image)\n# np.save('label_set_%s.npy'%image_size,label)","feb5b193":"def split(image_set, label_set):\n    x_train, x_test, y_train, y_test = train_test_split(image_set, label_set, train_size = 0.8, random_state = np.random)\n    return x_train, x_test, y_train, y_test\n\nx_train, x_test, y_train, y_test = split(image, label)\nprint(x_train.shape)","497be41d":"def datagen():    # for image augmentation but it slows the training process\n    datagen = keras.preprocessing.image.ImageDataGenerator(\n        featurewise_center=False, samplewise_center=False,\n        featurewise_std_normalization=False, samplewise_std_normalization=False,\n        zca_whitening=False, zca_epsilon=1e-06, rotation_range=20, width_shift_range=0.1,\n        height_shift_range=0.1, brightness_range=None, shear_range=0.1, zoom_range=0.1,\n        channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=True,\n        vertical_flip=False, rescale=None, preprocessing_function=None, data_format='channels_last') \n    return datagen\n\ntrain_datagen = datagen()\ntrain_datagen.fit(x_train)","96cb62a4":"###################\nfilters = 32\nkernel_size = (3,3)\nstride = (1,1)\npool_size = (3,3)\n###################","e409c727":"def conv2D(x,filters=filters,kernel=kernel_size,stride=stride,pad='same',activate=True,WN=False):\n    if activate:\n        x = keras.layers.Conv2D(filters=filters,kernel_size=kernel_size,strides=stride,padding=pad,activation='selu')(x)\n    else:\n        x = keras.layers.Conv2D(filters=filters,kernel_size=kernel_size,strides=stride,padding=pad,activation=None)(x)\n    if WN:\n        tfa.addons.WeightNormalization(x)\n    return x\n\ndef selu(x):\n    x = tf.nn.selu(x)\n    return x\n\ndef maxpool2D(x,pool_size=pool_size,stride=stride,pad='same'):\n    x = keras.layers.MaxPool2D(pool_size=pool_size,strides=stride,padding=pad)(x)\n    return x\n\ndef BN(x):\n    x = keras.layers.BatchNormalization()(x)\n    return x\n\ndef concat(x): # input as list\n    x = tf.keras.layers.Concatenate()(x)\n    return x\n\ndef res_add(raw_x,transformed_x,keep_scale):\n    x = tf.keras.layers.Add()([raw_x*keep_scale,transformed_x*(1-keep_scale)])\n    return x\n\ndef stem(x):\n    x = keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(2,2),input_shape=(image_size,image_size,1))(x)\n    x = conv2D(x,64)\n    x_1 = maxpool2D(x,(3,3),(2,2))                 # ---|\n    x_2 = conv2D(x,96,(3,3),(2,2))                 # ---|\n    x = concat([x_1,x_2])    # size\/2, 160 channels\n    x_1 = conv2D(x,64,(1,1))                       # ---|\n    x_1 = conv2D(x_1,96,(3,3))                         #|\n    x_2 = conv2D(x,64,(1,1))                           #|\n    x_2 = conv2D(x_2,64,(1,7))                         #|\n    x_2 = conv2D(x_2,64,(7,1))                         #|\n    x_2 = conv2D(x_2,96,(3,3))                     # ---|\n    x = concat([x_1,x_2])    # size\/2, 192 channels                        \n    x_1 = maxpool2D(x,(3,3),(2,2))                 # ---|\n    x_2 = conv2D(x,192,(3,3),(2,2))                # ---|\n    x = concat([x_1,x_2])    # \n    return x                 # size\/2, 384 channels\n\ndef blockA(x):\n    x_1 = x                  # highway\n    x_2_1 = conv2D(x,32,(1,1))\n    x_2_2 = conv2D(x,32,(1,1))\n    x_2_2 = conv2D(x_2_2,32,(3,3))\n    x_2_3 = conv2D(x,32,(1,1))\n    x_2_3 = conv2D(x_2_3,48,(3,3))\n    x_2_3 = conv2D(x_2_3,64,(3,3))\n    x_2 = concat([x_2_1,x_2_2,x_2_3])\n    x_2 = conv2D(x_2,384,(1,1),activate=False)\n    x = res_add(x_1,x_2,0.2) # x_1 and x_2 must have the same number of channels to add up\n    x = selu(x)\n    return x                 # size fixed, channel fixed\n\ndef reduceA(x):\n    x_1 = maxpool2D(x,(3,3),(2,2))  # size\/2, channel fixed\n    x_2 = conv2D(x,384,(3,3),(2,2))\n    x_3 = conv2D(x,192,(1,1))\n    x_3 = conv2D(x_3,192,(3,3))\n    x_3 = conv2D(x_3,384,(3,3),(2,2))\n    x = concat([x_1,x_2,x_3]) \n    return x                 # size\/2, 1152 channel\n\ndef blockB(x):\n    x_1 = x\n    x_2_1 = conv2D(x,192,(1,1))\n    x_2_2 = conv2D(x,128,(1,1))\n    x_2_2 = conv2D(x_2_2,160,(1,7))\n    x_2_2 = conv2D(x_2_2,192,(7,1))\n    x_2 = concat([x_2_1,x_2_2])\n    x_2 = conv2D(x_2,1152,(1,1),activate=False)\n    x = res_add(x_1,x_2,0.2)\n    x = selu(x)\n    return x                 # size fixed, channel fixed\n\ndef reduceB(x):\n    x_1 = maxpool2D(x,(3,3),(2,2))\n    x_2 = conv2D(x,256,(1,1))\n    x_2 = conv2D(x_2,384,(3,3),(2,2))\n    x_3 = conv2D(x,256,(1,1))\n    x_3 = conv2D(x_3,288,(3,3))\n    x_3 = conv2D(x_3,320,(3,3),(2,2))\n    x = concat([x_1,x_2,x_3])\n    return x                 # size\/2, 1856 channel\n\ndef blockC(x):\n    x_1 = x\n    x_2_1 = conv2D(x,192,(1,1))\n    x_2_2 = conv2D(x,192,(1,1))\n    x_2_2 = conv2D(x_2_2,224,(1,3))\n    x_2_2 = conv2D(x_2_2,256,(3,1))\n    x_2 = concat([x_2_1,x_2_2])\n    x_2 = conv2D(x_2,1856,(1,1),activate=False)\n    x = res_add(x_1,x_2,0.2)\n    x = selu(x)\n    return x                 # size fixed, channel fixed\n    \n\ndef outputs(x):\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(1, activation='sigmoid')(x)      # use sigmoid for binary classification\n    return x\n\ninputs = keras.Input(shape=(image_size,image_size,1))\nx = stem(inputs)\nx = blockA(x)\nx = reduceA(x)\nx = blockB(x)\nx = blockB(x)\nx = reduceB(x)\nx = blockC(x)\noutputs = outputs(x)\n\nmodel = tf.keras.Model(inputs,outputs)\nprint(model.summary())","e2753c7a":"###################\ntotal_epoch = 50\nlr_init = 0.0001\nbatch_size = 8\n###################\n\ndef scheduler(epoch):\n    epoch += 1\n    lr = lr_init\n    threshold = 5\n    depre = tf.math.exp(-0.25 * (epoch - threshold))\n    if epoch <= threshold:\n        return lr_init\n    elif lr > lr_init\/100:\n        lr = lr_init * depre\n        return lr\n    else:\n        return lr\n\nscheduler = keras.callbacks.LearningRateScheduler(scheduler, verbose=1)","e13c68aa":"# adding an earlystop so training process will stop in advance if the metrics (accuracy) doesn't improve for 5 epochs consecutively\nearlystop = keras.callbacks.EarlyStopping(monitor=\"val_acc\",mode=\"max\",verbose=1,patience=5,restore_best_weights=True)","c83b025a":"loss = keras.losses.BinaryCrossentropy()\noptimizer = keras.optimizers.Nadam(learning_rate=lr_init)\nmetrics = [tf.keras.metrics.BinaryAccuracy(name='acc')]\nmodel.compile(loss=loss, optimizer=optimizer, metrics=metrics)","48b25409":"%%time\n\nuse_image_gen=False         # change to True if you want to use image data generator \n\nif use_image_gen:\n    model.fit(train_datagen.flow(x_train, y_train, batch_size=batch_size),\n    steps_per_epoch=(len(x_train)\/batch_size), epochs=total_epoch, callbacks=[callback],\n    validation_data=(x_test, y_test), workers=0, use_multiprocessing=True, shuffle=True)\n    \nelse:\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=total_epoch, callbacks=[scheduler,earlystop], validation_data=(x_test, y_test), shuffle=True)\n\nmodel.save('model.h5')","23d3fa72":"pred = model.predict(x_test)\npred = np.round(pred,0)\nprint('confusion matrix:\\n',metrics.confusion_matrix(y_test,pred))\nprint('precision:\\n',metrics.precision_score(y_test,pred))\nprint('recall:\\n',metrics.recall_score(y_test,pred))\nprint('f1_score:\\n',metrics.f1_score(y_test,pred))","c30ae6a3":"plt.title('model accuracy')\nplt.plot(model.history.history['acc'],label='train accuracy')\nplt.plot(model.history.history['val_acc'],label='test accuracy')\nplt.legend()\nplt.show()","8bbc8558":"plt.title('model loss')\nplt.plot(model.history.history['loss'],label='train loss')\nplt.plot(model.history.history['val_loss'],label='test loss')\nplt.legend()\nplt.show()","acf2656d":"Original research publication can be found here: \n\n[*Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning*](https:\/\/arxiv.org\/abs\/1602.07261)","2838cd44":"#### Features of this model:\n* Bulit based on Inception-Resnetv2 CNN but the size is reduced substantially to save training time\n* Using StandardScaler to scale the image data\n* Using Image Augmentation (turned off for this version)\n* Using NAdam optimization (Adam with Nesterov)\n* Replaced ReLU by SELU for a self-normalizing neural network\n* No Batch Normalization because of using SELU"}}