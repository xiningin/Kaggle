{"cell_type":{"a119f5bc":"code","b2ce45c4":"code","d95fb247":"code","eb04915a":"code","d6e3209b":"code","27867ea6":"code","7b0de331":"code","09449cdd":"code","54decfd0":"code","9953397f":"code","d4b1cf72":"code","07abc671":"code","8a7d7d33":"code","7e242bc2":"code","a39a32f5":"code","84e2088e":"code","fb211b90":"code","502d810d":"code","c80f008c":"code","cdf3945d":"code","5bc3cf5d":"code","f762d8d6":"code","77a6935a":"code","8374f27e":"code","f3b71bf4":"code","d88452a1":"code","fb366528":"code","85f927de":"code","94963ff3":"code","acdc0030":"code","4c35a9b9":"code","2d09cec2":"code","a615f499":"code","b13e8c8c":"code","55e48709":"code","62c04073":"code","1638cdc0":"code","fbaadf7e":"code","b13ec416":"code","accfc6d4":"code","e0e6f19c":"code","39c0edb4":"code","178812ac":"code","dcd967b0":"code","21870f14":"code","2e1b1157":"code","cd599ec8":"code","0b3a8279":"markdown","e680f960":"markdown","7b577a81":"markdown","0b18025a":"markdown","7acbab67":"markdown","e7f71645":"markdown","392d7f3c":"markdown","14ae25e8":"markdown","492c2a0d":"markdown","6ee5110a":"markdown","adc81fe8":"markdown","a113a1a3":"markdown","505d003e":"markdown","d018cbff":"markdown","9b563002":"markdown","fd69dee3":"markdown","a6b6e6e1":"markdown","dd6d645a":"markdown","2c7189a5":"markdown","be66eb43":"markdown","8747ee0c":"markdown","54046a84":"markdown","d0172bb9":"markdown","16fefbb0":"markdown","c1f59d58":"markdown","57aa907b":"markdown","1479c21d":"markdown","33321d23":"markdown","5d896783":"markdown","52b870b5":"markdown","7eddf409":"markdown","f0fe94d1":"markdown","64533303":"markdown","2989cfef":"markdown","eb525b3f":"markdown","453a7b0e":"markdown","397dc71e":"markdown","3a38fe40":"markdown","227d6f3f":"markdown","1bccb8df":"markdown","6e091ecd":"markdown","3004d0b2":"markdown","f812455e":"markdown","b7aedd8a":"markdown","4392d766":"markdown","e6b8b0ef":"markdown","480658ff":"markdown","818770b1":"markdown","ada10b30":"markdown","c2077479":"markdown"},"source":{"a119f5bc":"! pip install tensorflow","b2ce45c4":"import tensorflow as tf\nprint(tf.__version__)","d95fb247":"x = tf.constant([[5,2], [1,3]])\nprint(x)","eb04915a":"x.numpy()","d6e3209b":"print('dtype:', x.dtype)\nprint('shape:', x.shape)","27867ea6":"print(tf.ones(shape=(2,1)))\nprint(tf.zeros(shape=(2,1)))","7b0de331":"tf.random.normal(shape=(2,2), mean=0., stddev=1.)","09449cdd":"tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype='int32')","54decfd0":"initial_val = tf.random.normal(shape=(2,2))\na = tf.Variable(initial_val)\nprint(a)","9953397f":"new_value = tf.random.normal(shape=(2,2))\na.assign(new_value)\nfor i in range(2):\n    for j in range(2):\n        assert a[i, j]== new_value[i,j]","d4b1cf72":"add_value = tf.random.normal(shape=(2,2))\na.assign_add(add_value)\nfor i in range(2):\n    for j in range(2):\n        assert a[i,j] == new_value[i,j] + add_value[i,j]","07abc671":"a = tf.random.normal(shape=(2,2))\nb = tf.random.normal(shape=(2,2))\n\nc = a + b\nd = tf.square(c)\nprint(d)\ne = tf.exp(d)\ne","8a7d7d33":"a = tf.random.normal(shape=(2,2))\nb = tf.random.normal(shape=(2,2))\n\nwith tf.GradientTape() as tape:\n    tape.watch(a)\n    c = tf.sqrt(tf.square(a) + tf.square(b))\n    dc_da = tape.gradient(c, a)\n    print(dc_da)","7e242bc2":"a = tf.Variable(a)\nwith tf.GradientTape() as tape:\n    c = tf.sqrt(tf.square(a) + tf.square(b))\n    dc_da = tape.gradient(c,a)\n    print(dc_da)","a39a32f5":"with tf.GradientTape() as outer_tape:\n  with tf.GradientTape() as tape:\n    c = tf.sqrt(tf.square(a) + tf.square(b))\n    dc_da = tape.gradient(c, a)\n  d2c_da2 = outer_tape.gradient(dc_da, a)\n  print(d2c_da2)","84e2088e":"input_dim = 2\noutput_dim = 1\nlearning_rate = 0.01\n\n# This is our weight matrix\nw = tf.Variable(tf.random.uniform(shape=(input_dim, output_dim)))\n# This is our bias vector\nb = tf.Variable(tf.zeros(shape=(output_dim,)))\n\ndef compute_predictions(features):\n  return tf.matmul(features, w) + b\n\ndef compute_loss(labels, predictions):\n  return tf.reduce_mean(tf.square(labels - predictions))\n\ndef train_on_batch(x, y):\n    with tf.GradientTape() as tape:\n        predictions = compute_predictions(x)\n        loss = compute_loss(y, predictions)\n        \n        dloss_dw, dloss_db = tape.gradient(loss, [w,b])\n    w.assign_sub(learning_rate* dloss_dw)\n    b.assign_sub(learning_rate* dloss_db)\n    return loss","fb211b90":"# Lets generate some artifical data\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Prepare a dataset.\nnum_samples = 10000\nnegative_samples = np.random.multivariate_normal(\n    mean=[0, 3], cov=[[1, 0.5],[0.5, 1]], size=num_samples)\npositive_samples = np.random.multivariate_normal(\n    mean=[3, 0], cov=[[1, 0.5],[0.5, 1]], size=num_samples)\nfeatures = np.vstack((negative_samples, positive_samples)).astype(np.float32)\nlabels = np.vstack((np.zeros((num_samples, 1), dtype='float32'),\n                    np.ones((num_samples, 1), dtype='float32')))\n\nplt.scatter(features[:, 0], features[:, 1], c=labels[:, 0])","502d810d":"# shuffle data\nindices = np.random.permutation(len(features))\nfeatures = features[indices]\nlabels = labels[indices]\n\n# Create a tf.data.Dataset object for easy batched iteration\nds = tf.data.Dataset.from_tensor_slices((features, labels))\nds = ds.shuffle(buffer_size=1024).batch(256)\n\nfor epoch in range(10):\n    for step, (x,y) in enumerate(ds):\n        loss = train_on_batch(x, y)\n    print(f\"Epoch {epoch}: last batch loss= {float(loss)}\")","c80f008c":"predictions = compute_predictions(features)\nplt.scatter(features[:, 0], features[:, 1], c=predictions[:, 0] > 0.5)","cdf3945d":"import time\n\nt0 = time.time()\nfor epoch in range(20):\n  for step, (x, y) in enumerate(ds):\n    loss = train_on_batch(x, y)\nt_end = time.time() - t0\nprint('Time per epoch: %.3f s' % (t_end \/ 20,))","5bc3cf5d":"@tf.function\ndef train_on_batch(x, y):\n    with tf.GradientTape() as tape:\n        predictions = compute_predictions(x)\n        loss = compute_loss(y, predictions)\n        \n        dloss_dw, dloss_db = tape.gradient(loss, [w,b])\n    w.assign_sub(learning_rate* dloss_dw)\n    b.assign_sub(learning_rate* dloss_db)\n    return loss","f762d8d6":"import time\n\nt0 = time.time()\nfor epoch in range(20):\n  for step, (x, y) in enumerate(ds):\n    loss = train_on_batch(x, y)\nt_end = time.time() - t0\nprint('Time per epoch: %.3f s' % (t_end \/ 20,))","77a6935a":"from tensorflow.keras.layers import Layer\n\nclass Linear(Layer):\n    #y= w.x+b\n    def __init__(self, units=32, input_dim=32):\n        super(Linear, self).__init__()\n        w_init = tf.random_normal_initializer()\n        shape=(input_dim, units)\n        self.w = tf.Variable(\n               initial_value=w_init(shape=shape, dtype='float32'),\n              trainable=True)\n        self.w = self.add_weight(shape=shape, initializer='random_normal')\n        b_init = tf.zeros_initializer()\n        self.b = tf.Variable(initial_value=b_init(shape=(units,), dtype='float32'),\n                            trainable=True)\n        \n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n    \n","8374f27e":"# Instantiate our layer.\nlinear_layer = Linear(4, 2)\n\ny = linear_layer(tf.ones((2, 2)))\nassert y.shape==(2,4)","f3b71bf4":"assert linear_layer.weights ==[linear_layer.w, linear_layer.b]\n","d88452a1":"# change this code inside class Linear\nw_init = tf.random_normal_initializer()\nself.w = tf.Variable(initial_value=w_init(shape=shape, dtype='float32'))","fb366528":"# change this code inside class Linear\nself.w = self.add_weight(shape=shape, initializer='random_normal')","85f927de":"class Linear(Layer):\n    # y = w.x + b\n    \n    def __init__(self, units=32):\n        super(Linear, self).__init__()\n        self.units = units\n        \n    def build(self, input_shape):\n        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n                                initializer='random_normal',\n                                trainable=True)\n        self.b = self.add_weight(shape=(self.units, ),\n                                 initializer='random_normal',\n                                 trainable=True)\n        \n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n    \n# Instantiate our lazy layer.\nlinear_layer = Linear(4)\n\n# This will also call `build(input_shape)` and create the weights.\ny = linear_layer(tf.ones((2, 2)))\nassert len(linear_layer.weights) == 2","94963ff3":"from tensorflow.keras.layers import Layer\n\nclass ComputeSum(Layer):\n#Returns the sum of the inputs\n    def __init__(self, input_dim):\n        super(ComputeSum, self).__init__()\n        # Create a non-trainable weight.\n        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),\n                               trainable=False)\n\n    def call(self, inputs):\n        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n        return self.total  \n\nmy_sum = ComputeSum(2)\nx = tf.ones((2, 2))\n\ny = my_sum(x)\nprint(y.numpy())  # [2. 2.]\n\ny = my_sum(x)\nprint(y.numpy())  # [4. 4.]\n\nassert my_sum.weights == [my_sum.total]\nassert my_sum.non_trainable_weights == [my_sum.total]\nassert my_sum.trainable_weights == []","acdc0030":"class MLP(Layer):\n    \"\"\"Simple stack of layers of MLP\"\"\"\n    \n    def __init__(self):\n        super(MLP, self).__init__()\n        self.linear_l = Linear(32)\n        self.linear_2 = Linear(32)\n        self.linear_3 = Linear(10)\n        \n    def call(self, inputs):\n        x = self.linear_l(inputs)\n        x = tf.nn.relu(x)\n        x = self.linear_2(x)\n        x = tf.nn.relu(x)\n        return self.linear_3(x)\n            \n        \nmlp = MLP()\n\n# The first call to the `mlp` object will create the weights.\ny = mlp(tf.ones(shape=(3, 64)))\n\n# Weights are recursively tracked.\nassert len(mlp.weights) == 6","4c35a9b9":"from tensorflow.keras.layers import Layer\n\nclass Dropout(Layer):\n  \n  def __init__(self, rate):\n    super(Dropout, self).__init__()\n    self.rate = rate\n\n  def call(self, inputs, training=None):\n    if training:\n      return tf.nn.dropout(inputs, rate=self.rate)\n    return inputs\n\nclass MLPWithDropout(Layer):\n\n  def __init__(self):\n      super(MLPWithDropout, self).__init__()\n      self.linear_1 = Linear(32)\n      self.dropout = Dropout(0.5)\n      self.linear_3 = Linear(10)\n\n  def call(self, inputs, training=None):\n      x = self.linear_1(inputs)\n      x = tf.nn.relu(x)\n      x = self.dropout(x, training=training)\n      return self.linear_3(x)\n    \nmlp = MLPWithDropout()\ny_train = mlp(tf.ones((2, 2)), training=True)\ny_test = mlp(tf.ones((2, 2)), training=False)","2d09cec2":"# We use an `Input` object to describe the shape and dtype of the inputs.\n# This is the deep learning equivalent of *declaring a type*.\n# The shape argument is per-sample; it does not include the batch size.\n# The functional API focused on defining per-sample transformations.\n# The model we create will automatically batch the per-sample transformations,\n# so that it can be called on batches of data.\ninputs = tf.keras.Input(shape=(16,))\n\n# We call layers on these \"type\" objects\n# and they return updated types (new shapes\/dtypes).\nx = Linear(32)(inputs) # We are reusing the Linear layer we defined earlier.\nx = Dropout(0.5)(x) # We are reusing the Dropout layer we defined earlier.\noutputs = Linear(10)(x)\n\n# A functional `Model` can be defined by specifying inputs and outputs.\n# A model is itself a layer like any other.\nmodel = tf.keras.Model(inputs, outputs)\n\n# A functional model already has weights, before being called on any data.\n# That's because we defined its input shape in advance (in `Input`).\nassert len(model.weights) == 4\n\n# Let's call our model on some data.\ny = model(tf.ones((2, 16)))\nassert y.shape == (2, 10)","a615f499":"from tensorflow.keras import Sequential\n\nmodel = Sequential([Linear(32),\n                    Dropout(0.5),\n                    Linear(10)])\n\ny = model(tf.ones((2, 16)))\nassert y.shape == (2, 10)","b13e8c8c":"bce = tf.keras.losses.BinaryCrossentropy()\n\ny_true = [0., 0., 1., 1.]  # Targets\ny_pred = [1., 1., 1., 0.]  # Predictions\nloss = bce(y_true, y_pred)\nprint(\"Loss\", loss.numpy())","55e48709":"m = tf.keras.metrics.AUC()\nm.update_state([0,1,1,1], [0,1,0,0])\nprint(\"intermediate result:\", m.result().numpy())\n\nm.update_state([1,1,1,1], [0,1,1,0])\nprint(\"final result:\", m.result().numpy())","62c04073":"class BinaryTruePositives(tf.keras.metrics.Metric):\n    def __init__(self, name='binary_true_positives', **kwargs):\n        super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n    \n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.cast(y_true, tf.bool)\n        y_pred = tf.cast(y_pred, tf.bool)\n        \n        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n        values = tf.cast(values, self.dtype)\n        if sample_weight is not None:\n            sample_weight = tf.cast(sample_weight, self.dtype)\n            values = tf.multiply(values, sample_weight)\n        self.true_positives.assign_add(tf.reduce_sum(values))\n        \n    def result(self):\n        return self.true_positives\n    \n    def reset_states(self):\n        self.true_positives.assign(0)\n        \nm = BinaryTruePositives()\nm.update_state([0, 1, 1, 1], [0, 1, 0, 0])\nprint('Intermediate result:', m.result().numpy())\n\nm.update_state([1, 1, 1, 1], [0, 1, 1, 0])\nprint('Final result:', m.result().numpy())\n        ","1638cdc0":"from tensorflow.keras import layers\n\n# Prepare a dataset.\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train = x_train[:].reshape(60000, 784).astype('float32') \/ 255\ndataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndataset = dataset.shuffle(buffer_size=1024).batch(64)\n\n# Instantiate a simple classification model\nmodel = tf.keras.Sequential([\n  layers.Dense(256, activation=tf.nn.relu),\n  layers.Dense(256, activation=tf.nn.relu),\n  layers.Dense(10)\n])\n\n# Instantiate a logistic loss function that expects integer targets.\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# Instantiate an accuracy metric.\naccuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n\n# Instantiate an optimizer.\noptimizer = tf.keras.optimizers.Adam()\n\n# Iterate over the batches of the dataset.\nfor step, (x, y) in enumerate(dataset):\n  # Open a GradientTape.\n  with tf.GradientTape() as tape:\n\n    # Forward pass.\n    logits = model(x)\n\n    # Loss value for this batch.\n    loss_value = loss(y, logits)\n     \n  # Get gradients of loss wrt the weights.\n  gradients = tape.gradient(loss_value, model.trainable_weights)\n  \n  # Update the weights of our linear layer.\n  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n\n  # Update the running accuracy.\n  accuracy.update_state(y, logits)\n  \n  # Logging.\n  if step % 100 == 0:\n    print('Step:', step)\n    print('Loss from last step: %.3f' % loss_value)\n    print('Total running accuracy so far: %.3f' % accuracy.result())","fbaadf7e":"x_test = x_test[:].reshape(10000, 784).astype('float32') \/ 255\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ntest_dataset = test_dataset.batch(128)\n\naccuracy.reset_states()\n\nfor step, (x,y) in enumerate(test_dataset):\n    logits = model(x)\n    accuracy.update_state(y, logits)\n\nprint(\"Final test accuracy: %.3f\" %accuracy.result())","b13ec416":"from tensorflow.keras.layers import Layer\n\nclass ActivityRegularization(Layer):\n  \"\"\"Layer that creates an activity sparsity regularization loss.\"\"\"\n  \n  def __init__(self, rate=1e-2):\n    super(ActivityRegularization, self).__init__()\n    self.rate = rate\n  \n  def call(self, inputs):\n    # We use `add_loss` to create a regularization loss\n    # that depends on the inputs.\n    self.add_loss(self.rate * tf.reduce_sum(tf.square(inputs)))\n    return inputs","accfc6d4":"from tensorflow.keras import layers\n\nclass SparseMLP(Layer):\n  \"\"\"Stack of Linear layers with a sparsity regularization loss.\"\"\"\n\n  def __init__(self, output_dim):\n      super(SparseMLP, self).__init__()\n      self.dense_1 = layers.Dense(32, activation=tf.nn.relu)\n      self.regularization = ActivityRegularization(1e-2)\n      self.dense_2 = layers.Dense(output_dim)\n\n  def call(self, inputs):\n      x = self.dense_1(inputs)\n      x = self.regularization(x)\n      return self.dense_2(x)\n    \n\nmlp = SparseMLP(1)\ny = mlp(tf.ones((10, 10)))\n\nprint(mlp.losses)  # List containing one float32 scalar","e0e6f19c":"# Losses correspond to the *last* forward pass.\nmlp = SparseMLP(1)\nmlp(tf.ones((10, 10)))\nassert len(mlp.losses) == 1\nmlp(tf.ones((10, 10)))\nassert len(mlp.losses) == 1  # No accumulation.\n\n# Let's demonstrate how to use these losses in a training loop.\n\n# Prepare a dataset.\n(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\ndataset = tf.data.Dataset.from_tensor_slices(\n    (x_train.reshape(60000, 784).astype('float32') \/ 255, y_train))\ndataset = dataset.shuffle(buffer_size=1024).batch(64)\n\n# A new MLP.\nmlp = SparseMLP(10)\n\n# Loss and optimizer.\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n\nfor step, (x, y) in enumerate(dataset):\n  with tf.GradientTape() as tape:\n    # Forward pass.\n    logits = mlp(x)\n\n    # External loss value for this batch.\n    loss = loss_fn(y, logits)\n    \n    # Add the losses created during the forward pass.\n    loss += sum(mlp.losses)\n     \n    # Get gradients of loss wrt the weights.\n    gradients = tape.gradient(loss, mlp.trainable_weights)\n  \n  # Update the weights of our linear layer.\n  optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n  \n  # Logging.\n  if step % 100 == 0:\n    print('Loss at step %d: %.3f' % (step, loss))","39c0edb4":"# Prepare a dataset.\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train = x_train.reshape(60000, 784).astype('float32') \/ 255\ndataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndataset = dataset.shuffle(buffer_size=1024).batch(64)\n\n# Instantiate a simple classification model\nmodel = tf.keras.Sequential([\n  layers.Dense(256, activation=tf.nn.relu),\n  layers.Dense(256, activation=tf.nn.relu),\n  layers.Dense(10)\n])\n\n# Instantiate a logistic loss function that expects integer targets.\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# Instantiate an accuracy metric.\naccuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n\n# Instantiate an optimizer.\noptimizer = tf.keras.optimizers.Adam()","178812ac":"model.compile(optimizer=optimizer, loss=loss, metrics=[accuracy])","dcd967b0":"model.fit(dataset, epochs=3)","21870f14":"x_test = x_test[:].reshape(10000, 784).astype('float32') \/ 255\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ntest_dataset = test_dataset.batch(128)\n\nloss, acc = model.evaluate(test_dataset)\nprint('loss: %.3f - acc: %.3f' % (loss, acc))","2e1b1157":"(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train = x_train.reshape(60000, 784).astype('float32') \/ 255\n\nnum_val_samples = 10000\nx_val = x_train[-num_val_samples:]\ny_val = y_train[-num_val_samples:]\nx_train = x_train[:-num_val_samples]\ny_train = y_train[:-num_val_samples]\n\n# Instantiate a simple classification model\nmodel = tf.keras.Sequential([\n  layers.Dense(256, activation=tf.nn.relu),\n  layers.Dense(256, activation=tf.nn.relu),\n  layers.Dense(10)\n])\n\n# Instantiate a logistic loss function that expects integer targets.\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# Instantiate an accuracy metric.\naccuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n\n# Instantiate an optimizer.\noptimizer = tf.keras.optimizers.Adam()\n\nmodel.compile(optimizer=optimizer,\n              loss=loss,\n              metrics=[accuracy])\nmodel.fit(x_train, y_train,\n          validation_data=(x_val, y_val),\n          epochs=3,\n          batch_size=64)","cd599ec8":"# Instantiate a simple classification model\nmodel = tf.keras.Sequential([\n  layers.Dense(256, activation=tf.nn.relu),\n  layers.Dense(256, activation=tf.nn.relu),\n  layers.Dense(10)\n])\n\n# Instantiate a logistic loss function that expects integer targets.\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# Instantiate an accuracy metric.\naccuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n\n# Instantiate an optimizer.\noptimizer = tf.keras.optimizers.Adam()\n\nmodel.compile(optimizer=optimizer,\n              loss=loss,\n              metrics=[accuracy])\n\n# Instantiate some callbacks\ncallbacks = [tf.keras.callbacks.EarlyStopping(),\n             tf.keras.callbacks.ModelCheckpoint(filepath='my_model.keras',\n                                                save_best_only=True)]\n\nmodel.fit(x_train, y_train,\n          validation_data=(x_val, y_val),\n          epochs=30,\n          batch_size=64,\n          callbacks=callbacks)","0b3a8279":"You would typically do:","e680f960":"Note that's also a shortcut method for creating weights: `add_weight`. Instead of doing","7b577a81":"The internal state can be cleared with `metric.reset_states`.\n\nYou can easily roll out your own metrics by subclassing the `Metric` class:\n\n- Create the state variables in `__init__`\n- Update the variables given `y_true` and `y_pred` in `update_state`\n- Return the metric result in `result`\n- Clear the state in `reset_states`\n\nHere's a quick implementation of a `BinaryTruePositives` metric as a demonstration:","0b18025a":"By default, variables are watched automatically, so you don't need to manually `watch` them:","7acbab67":"# Part 2: The Keras API <a id='2'><\/a>\n\nKeras is a Python API for deep learning. It has something for everyone:\n\n- If you're an engineer, Keras provides you with reusable blocks such as layers, metrics, training loops, to support common use cases. It provides a high-level user experience that's accessible and productive.\n\n- If you're a researcher, you may prefer not to use these built-in blocks such as layers and training loops, and instead create your own. Of course, Keras allows you to do this. In this case, Keras provides you with templates for the blocks you write, it provides you with structure, with an API standard for things like Layers and Metrics. This structure makes your code easy to share with others and easy to integrate in production workflows.\n\n- The same is true for library developers: TensorFlow is a large ecosystem. It has many different libraries. In order for different libraries to be able to talk to each other and share components, they need to follow an API standard. That's what Keras provides.\n\nCrucially, Keras brings high-level UX and low-level flexibility together fluently: you no longer have on one hand, a high-level API that's easy to use but inflexible, and on the other hand a low-level API that's flexible but only approachable by experts. Instead, you have a spectrum of workflows, from the  very high-level to the very low-level. Workflows that are all compatible because they're built on top of the same concepts and objects.\n\n![Spectrum of Keras workflows](https:\/\/keras-dev.s3.amazonaws.com\/tutorials-img\/spectrum-of-workflows.png)\n","e7f71645":"## Recursively composing layers <a id='2.3'><\/a>\n\nLayers can be recursively nested to create bigger computation blocks. Each layer will track the weights of its sublayers (both trainable and non-trainable).","392d7f3c":"## Loss Classes <a id='2.7'><\/a>\n\n\nKeras features a wide range of built-in loss classes, like `BinaryCrossentropy`, `CategoricalCrossentropy`, `KLDivergence`, etc. They work like this:","14ae25e8":"## Computing gradients with GradientTape <a id=\"1.5\"><\/a>\n\nOh, and there's another big difference with Numpy: you can automatically retrieve the gradient of any differentiable expression.\n\nJust open a GradientTape, start \"watching\" a tensor via `tape.watch()`, and compose a differentiable expression using this tensor as input:\n","492c2a0d":"Note that you can compute higher-order derivatives by nesting tapes:","6ee5110a":"Note that you can also monitor your loss and metrics on some validation data during `fit`.\n\nAlso, you can call `fit` directly on Numpy arrays, so no need for the dataset conversion:","adc81fe8":"Done!\n\n**Note:** When you use `fit`, by default execution uses static graphs, so you don't need to add any `tf.function` decorators to your model or your layers.\n\nNow let's test it:","a113a1a3":"## An end-to-end example: linear regression <a id=\"1.6\"><\/a>\n\nSo far you've learned that TensorFlow is a Numpy-like library that is GPU or TPU accelerated, with automatic differentiation. Time for an end-to-end example: let's implement a linear regression, the FizzBuzz of Machine Learning.\n\nFor the sake of demonstration, we won't use any of the higher-level Keras components like Layer or MeanSquaredError. Just basic ops.\n","505d003e":"## Random constant tensors <a id=\"1.2\"><\/a>\n\n\nThis is all pretty [normal](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/random\/normal):","d018cbff":"## Callbacks <a id='2.11'><\/a>\n\nOne of the neat features of `fit` (besides built-in support for sample weighting and class weighting) is that you can easily customize what happens during training and evaluation by using [callbacks](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/).\n\nA callback is an object that is called at different points during training (e.g. at the end of every batch or at the end of every epoch) and takes actions, such as saving a model, mutating variables on the model, loading a checkpoint, stopping training, etc.\n\nThere's a bunch of built-in callback available, like `ModelCheckpoint` to save your models after each epoch during training, or `EarlyStopping`, which interrupts training when your validation metrics start stalling.\n\nAnd you can easily [write your own callbacks](https:\/\/www.tensorflow.org\/guide\/keras\/custom_callback). ","9b563002":"And here's an integer tensor with values drawn from a random [uniform](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/random\/uniform) distribution:","fd69dee3":"It\u2019s good practice to create weights in a separate `build` method, called lazily with the shape of the first inputs seen by your layer. Here, this pattern prevents us from having to specify input_dim in the constructor:","a6b6e6e1":"## Acknowledgment\n\nI used this notebook to learn about Tensorflow\/Keras 2.0 and created a copy of colab notebook in Kaggle. The full credit for writing this kernel goes to [fchollet](https:\/\/www.kaggle.com\/fchollet), creator of Keras.","dd6d645a":"## Doing math in TensorFlow <a id=\"1.4\"><\/a>\n\nYou can use TensorFlow exactly like you would use Numpy. The main difference is that your TensorFlow code can run on GPU and TPU.","2c7189a5":"A common way to create constant tensors is via `tf.ones` and `tf.zeros` (just like `np.ones` and `np.zeros`):","be66eb43":"First, call `compile` to configure the optimizer, loss, and metrics to monitor.","8747ee0c":"# Contents\n\n* [<font size=4>Part1. Tensorflow basics<\/font>](#1)\n    * [Tensors](#1.1)\n    * [Random Constant tensors](#1.2)\n    * [Variables](#1.3)\n    * [Doing math in tensorflow](#1.4)\n    * [Computing gradients with gradienttape](#1.5)\n    * [An end-to-end example: linear regression](#1.6)\n    * [Making it fast with tf.function](#1.7)\n\n\n* [<font size=4>Part 2: The Keras API<\/font>](#2)\n    * [The base layer class](#2.1)\n    * [Trainable and Non trainable weights](#2.2)\n    * [Recursively composing layers](#2.3)\n    * [Build in layers](#2.4)\n    * [The training argument in call](#2.5)\n    * [A more Functional way of defining models](#2.6)\n    * [Loss classes](#2.7)\n    * [Metric classes](#2.8)\n    * [Optimizer classes & a quick end-to-end training loop](#2.9)\n    * [A detailed end-to-end example: a Variational AutoEncoder (VAE)](#2.10)\n    * [Callbacks](#2.11)\n\n* [<font size=4>Parting words<\/font>](#3)\n    * [What to do next?](#3.1)","54046a84":"40% reduction, neat. In this case we used a trivially simple model; in general the bigger the model the greater the speedup you can get by leveraging static graphs.\n\nRemember: eager execution is great for debugging and printing results line-by-line, but when it's time to scale, static graphs are a researcher's best friends.\n","d0172bb9":"Then we call `fit` on our model to pass it the data:","16fefbb0":"# Parting words <a id='3.1'><\/a>\n\nI hope this guide has given you a good overview of what's possible with TensorFlow 2.0 and Keras!\n\nRemember that TensorFlow and Keras don't represent a single workflow. It's a spectrum of workflows, each with its own trade-off between usability and flexibility. For instance, you've noticed that it's much easier to use `fit` than to write a custom training loop, but `fit` doesn't give you the same level of granular control for research use cases.\n\nSo use the right tool for the job!\n\nA core principle of Keras is \"progressive disclosure of complexity\": it's easy to get started, and you can gradually dive into workflows where you write more and more logic from scratch, providing you with complete control.\n\nThis applies to both model definition, and model training.\n\n![Model definition: spectrum of workflows](https:\/\/keras-dev.s3.amazonaws.com\/tutorials-img\/model-building-spectrum.png)\n\n![Model training: spectrum of workflows](https:\/\/keras-dev.s3.amazonaws.com\/tutorials-img\/model-training-spectrum.png)\n\n## What to learn next <a id='3.2'><\/a>\n\nNext, there are many more topics you may be interested in:\n\n- [Saving and serialization](https:\/\/www.tensorflow.org\/guide\/keras\/save_and_serialize)\n- [Distributed training on multiple GPUS](https:\/\/www.tensorflow.org\/guide\/distributed_training)\n- [Exporting models to TFLite for deployment on Android or embedded systems](https:\/\/www.tensorflow.org\/lite\/convert\/python_api#converting_a_keras_model_)\n- [Exporting models to TensorFlow.js for deployment in the browser](https:\/\/www.tensorflow.org\/js\/tutorials\/conversion\/import_keras)","c1f59d58":"Let's calculate time again:\n","57aa907b":"The `Layer` class takes care of tracking the weights assigned to it as attributes:","1479c21d":"## Making it fast with tf.function <a id=1.7><\/a>\n\nBut how fast is our current code running?","33321d23":"## A more Functional way of defining models <a id='2.6'><\/a>\n\nTo build deep learning models, you don't have to use object-oriented programming all the time. Layers can also be composed functionally, like this (we call it the \"Functional API\"):","5d896783":"Much like a numpy array, it features the attributes `dtype` and `shape`:","52b870b5":"You update the value of a Variable by using the methods `.assign(value)`, or `.assign_add(increment)` or `.assign_sub(decrement)`:","7eddf409":"A layer instance works like a function. Let's call it on some data:","f0fe94d1":"## Trainable and Non-trainable weights <a id='2.2'><\/a>\n\nWeights created by layers can be either trainable or non-trainable. They're exposed in `trainable_weights` and `non_trainable_weights`. Here's a layer with a non-trainable weight:","64533303":"## Built-in layers <a id='2.4'><\/a>\n\nKeras provides you with a [wide range of built-in layers](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/), so that you don't have to implement your own layers all the time.\n\n- Convolution layers\n- Transposed convolutions\n- Separateable convolutions\n- Average and max pooling\n- Global average and max pooling\n- LSTM, GRU (with built-in cuDNN acceleration)\n- BatchNormalization\n- Dropout\n- Attention\n- ConvLSTM2D\n- etc.\n\nKeras follows the principles of exposing good default configurations, so that layers will work fine out of the box for most use cases if you leave keyword arguments to their default value. For instance, the `LSTM` layer uses an orthogonal recurrent matrix initializer by default, and initializes the forget gate bias to one by default.","2989cfef":"# Part1. Tensorflow basics <a id=\"1\"><\/a>\n\n## Tensors <a id=\"1.1\"><\/a>\nThis is a [constant tensor](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/constant)","eb525b3f":"Now let's train our linear regression by iterating over batch-by-batch over the data and repeatedly calling `train_on_batch`:","453a7b0e":"# TensorFlow 2.0 + Keras Overview for Deep Learning Researchers\n\nWritten by: *@fchollet, October 2019*\n\n---\n\n**This document serves as an introduction, crash course, and quick API reference for TensorFlow 2.0.**\n\n---\n\nTensorFlow and Keras were both released over four years ago (March 2015 for Keras and November 2015 for TensorFlow). That's a long time in deep learning years!\n\nIn the old days, TensorFlow 1.x + Keras had a number of known issues:\n- Using TensorFlow meant manipulating static computation graphs, which would feel awkward and difficult to programmers used to imperative styles of coding.\n- While the TensorFlow API was very powerful and flexible, it lacked polish and was often confusing or difficult to use.\n- While Keras was very productive and easy to use, it would often lack flexibility for research use cases.\n\n---\n### TensorFlow 2.0 is an extensive redesign of TensorFlow and Keras that takes into account over four years of user feedback and technical progress. It fixes the issues above in a big way.\n\n### It's a machine learning platform from the future.\n---\n\nTensorFlow 2.0 is built on the following key ideas:\n\n- Let users run their computation eagerly, like they would in Numpy. This makes TensorFlow 2.0 programming intuitive and Pythonic.\n- Preserve the considerable advantages of compiled graphs (for performance, distribution, and deployment). This makes TensorFlow fast, scalable, and production-ready.\n- Leverage Keras as its high-level deep learning API, making TensorFlow approachable and highly productive.\n- Extend Keras into a spectrum of workflows ranging from the very high-level (easier to use, less flexible) to the very low-level (requires more expertise, but provides great flexibility).\n","397dc71e":"## The `training` argument in `call` <a id='2.5'><\/a>\n\nSome layers, in particular the `BatchNormalization` layer and the `Dropout` layer, have different behaviors during training and inference. For such layers, it is standard practice to expose a `training` (boolean) argument in the `call` method.\n\nBy exposing this argument in `call`, you enable the built-in training and evaluation loops (e.g. `fit`) to correctly use the layer in training and inference.","3a38fe40":"We can reuse our `SparseCategoricalAccuracy` metric instance to implement a testing loop:","227d6f3f":"You can get its value as a Numpy array by calling `.numpy():`","1bccb8df":"## A detailed end-to-end example: a Variational AutoEncoder (VAE) <a id='2.10'><\/a>\n\nIf you want to take a break from the basics and look at a slightly more advanced example, check out this [Variational AutoEncoder](https:\/\/www.tensorflow.org\/guide\/keras\/custom_layers_and_models#putting_it_all_together_an_end-to-end_example) implementation that demonstrates everything you've learned so far:\n\n- Subclassing `Layer`\n- Recursive layer composition\n- Loss classes and metric classes\n- `add_loss`\n- `GradientTape`\n\n## Using build-in loops\n\nIt would be a bit silly if you had to write your own low-level training loops every time for simple use cases. Keras provides you with a built-in training loop on the `Model` class. If you want to use it, either subclass from the `Model` class, or create a `Functional` or `Sequential` model.\n\nTo demonstrate it, let's reuse the MNIST setup from above:","6e091ecd":"Here's how our model performs:","3004d0b2":"The Functional API tends to be more concise than subclassing, and provides a few other advantages (generally the same advantages that functional, typed languages provide over untyped OO development). However, it can only be used to define DAGs of layers -- recursive networks should be defined as `Layer` subclasses instead.\n\nKey differences between models defined via subclassing and Functional models are explained in [this blog post](https:\/\/medium.com\/tensorflow\/what-are-symbolic-and-imperative-apis-in-tensorflow-2-0-dfccecb01021).\n\nLearn more about the Functional API [here](https:\/\/www.tensorflow.org\/alpha\/guide\/keras\/functional).\n\nIn your research workflows, you may often find yourself mix-and-matching OO models and Functional models.\n\nFor models that are simple stacks of layers with a single input and a single output, you can also use the `Sequential` class which turns a list of layers into a `Model`:","f812455e":"## Optimizer classes & a quick end-to-end training loop <a id='2.9'><\/a>\n\nYou don't normally have to define by hand how to update your variables during gradient descent, like we did in our initial linear regression example. You would usually use one of the built-in Keras optimizer, like `SGD`, `RMSprop`, or `Adam`.\n\nHere's a simple MNSIT example that brings together loss classes, metric classes, and optimizers.","b7aedd8a":"## The base Layer class <a id='2.1'><\/a>\n\nThe first class you need to know is [Layer](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Layer). Pretty much everything in Keras derives from it.\n\nA Layer encapsulates a state (weights) and some computation (defined in the call method).","4392d766":"These losses are cleared by the top-level layer at the start of each forward pass -- they don't accumulate. So `layer.losses` always contain only the losses created during the last forward pass. You would typically use these losses by summing them before computing your gradients when writing a training loop.","e6b8b0ef":"## Variables <a id=\"1.3\"><\/a>\n\nVariables are special tensors used to store mutable state (like the weights of a neural network). You create a Variable using some initial value.","480658ff":"Note that loss classes are stateless: the output of `__call__` is only a function of the input.\n\n## Metric classes <a id='2.8'><\/a>\n\nKeras also features a wide range of built-in metric classes, such as `BinaryAccuracy`, `AUC`, `FalsePositives`, etc.\n\nUnlike losses, metrics are stateful. You update their state using the `update_state` method, and you query the scalar metric result using `result`:","818770b1":"Let's compile the training function into a static graph. Literally all we need to do is add the `tf.function` decorator on it:","ada10b30":"## The `add_loss` method \n\nSometimes you need to compute loss values on the fly during a foward pass (especially regularization losses). Keras allows you to compute loss values at any time, and to recursively keep track of them via the `add_loss` method.\n\nHere's an example of a layer that adds a sparsity regularization loss based on","c2077479":"Loss values added via `add_loss` can be retrieved in the `.losses` list property of any `Layer` or `Model`:"}}