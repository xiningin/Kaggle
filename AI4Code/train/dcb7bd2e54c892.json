{"cell_type":{"ff883b9e":"code","a2b10388":"code","8ec47358":"code","093ca2c3":"code","bea84302":"code","cfb252b0":"code","4d07429d":"code","2c571c40":"code","f329db11":"code","0f02f024":"code","3504bca3":"code","06b603c3":"code","d8349bdc":"code","1c7e447a":"code","641ba2db":"code","04c81b6d":"code","3b2fed6a":"code","3d9cc823":"code","331d3c89":"code","0a683fdc":"code","c1d00c93":"code","90425f1e":"code","a8355cfb":"code","98140bc1":"code","7cdf2d3b":"code","62561b87":"code","7bf8c9bc":"code","64599bb2":"markdown","56e4dc27":"markdown","d96ef41f":"markdown","52bf2b63":"markdown","e7816fa0":"markdown","93547d09":"markdown","d7d36daf":"markdown","c5c2c23d":"markdown","b9de4fce":"markdown","70bc6443":"markdown","0e0a2621":"markdown","c45c3e9b":"markdown","7f5958d8":"markdown","ffaaec3a":"markdown","81260600":"markdown","b0a7cbf5":"markdown","924c96ee":"markdown"},"source":{"ff883b9e":"#Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport os\nfrom tqdm import tqdm, tqdm_notebook\nimport random\nimport imageio\nimport shutil\nimport cv2\nfrom PIL import Image\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom numpy.random import seed\nseed(1)\n","a2b10388":"art = pd.read_csv('..\/input\/painter-by-numbers\/imagesinfo.csv')\nart.head()\n","8ec47358":"# Print few random paintings\n\nimages_dir = '..\/input\/painter-by-numbers\/images'\nn = 5\nfig, axes = plt.subplots(1, n, figsize=(20,10))\n\nfor i in range(n):\n    random_art = random.choice(art['filename'].values)\n   \n    random_image_file = '..\/input\/painter-by-numbers\/images\/'+random_art\n    image = plt.imread(random_image_file)\n    axes[i].imshow(image)\n    #axes[i].set_title(\"Art: \" + random_art.replace('_', ' '))\n    axes[i].axis('off')\nprint(random_image_file)\nplt.show()","093ca2c3":"TOP_N = 5\ntop_styles = art['style'].value_counts()[0:TOP_N].reset_index()\ntop_styles.columns = ['style', 'count']\ntop_styles['class_weight'] = top_styles['count'].sum() \/ (top_styles.shape[0] * top_styles['count'])\ntop_styles","bea84302":"# Set class weights - assign higher weights to underrepresented classes\nclass_weights = top_styles['class_weight'].to_dict()\nclass_weights","cfb252b0":"filter_list = top_styles['style']\nfilter_list = filter_list.tolist()\nfilter_list","4d07429d":"fdata = art[art['style'].isin(filter_list)]\nimages = fdata[['filename','style']]\nimages.reset_index(drop= True, inplace=True)\nimages[\"filename\"] = '..\/input\/painter-by-numbers\/images\/'+images[\"filename\"]\n\nartstyles = fdata['style'].unique()\nartstyles\n","2c571c40":"fil_art = pd.read_csv('..\/input\/filtered-painter-by-numbers\/filtered.csv')\nfil_art.head()","f329db11":"fil_art['style'].count()","0f02f024":"filtered_imgdir = \"..\/input\/filtered-painter-by-numbers\/images\"","3504bca3":"# Augment data\nbatch_size = 32\ntrain_input_shape = (224, 224, 3)\nn_classes = top_styles.shape[0]\n\n\ntrain_datagen = ImageDataGenerator(validation_split=0.2,\n                                   rescale=1.\/255.,\n                                   rotation_range=10,\n                                   #width_shift_range=0.5,\n                                   #height_shift_range=0.5,\n                                   shear_range=5,\n                                   #zoom_range=0.4,\n                                   horizontal_flip=True,\n                                   vertical_flip=True\n                                   \n                                  )\n\ntrain_generator = train_datagen.flow_from_directory(directory=filtered_imgdir,\n                                                    class_mode='categorical',\n                                                    target_size=train_input_shape[0:2],\n                                                    batch_size=batch_size,\n                                                    subset=\"training\",\n                                                    shuffle=True,\n                                                    classes=artstyles.tolist()\n                                                   )\n\nvalid_generator = train_datagen.flow_from_directory(directory=filtered_imgdir,\n                                                    class_mode='categorical',\n                                                    target_size=train_input_shape[0:2],\n                                                    batch_size=batch_size,\n                                                    subset=\"validation\",\n                                                    shuffle=True,\n                                                    classes=artstyles.tolist()\n                                                   )\n\nSTEP_SIZE_TRAIN = train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n\/\/valid_generator.batch_size\nprint(\"Total number of batches =\", STEP_SIZE_TRAIN, \"and\", STEP_SIZE_VALID)","06b603c3":"# Print a random paintings and it's random augmented version\nfig, axes = plt.subplots(1, 2, figsize=(20,10))\n\nrandom_art = random.choice(artstyles)\nrandom_image = random.choice(os.listdir(os.path.join(filtered_imgdir, random_art)))\nrandom_image_file = os.path.join(filtered_imgdir, random_art, random_image)\n\n# Original image\nimage = plt.imread(random_image_file)\naxes[0].imshow(image)\naxes[0].set_title(\"An original Image of style: \" + random_art.replace('_', ' '))\naxes[0].axis('off')\n\n# Transformed image\naug_image = train_datagen.random_transform(image)\naxes[1].imshow(aug_image)\naxes[1].set_title(\"A transformed Image of style: \" + random_art.replace('_', ' '))\naxes[1].axis('off')\n\nplt.show()","d8349bdc":"# Load pre-trained model\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=train_input_shape)\n\nfor layer in base_model.layers:\n    layer.trainable = True","1c7e447a":"print(base_model.output)\nprint(Flatten()(base_model.output))","641ba2db":"# Add layers at the end\nX = base_model.output\nX = Flatten()(X)\n\nX = Dense(512, kernel_initializer='he_uniform')(X)\nX = Dropout(0.5)(X)\nX = BatchNormalization()(X)\nX = Activation('relu')(X)\n\nX = Dense(16, kernel_initializer='he_uniform')(X)\nX = Dropout(0.5)(X)\nX = BatchNormalization()(X)\nX = Activation('relu')(X)\n\noutput = Dense(n_classes, activation='softmax')(X)\n\nmodel = Model(inputs=base_model.input, outputs=output)","04c81b6d":"optimizer = Adam(lr=0.0001)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer, \n              metrics=['accuracy'])","3b2fed6a":"n_epoch = 10\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, \n                           mode='auto', restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n                              verbose=1, mode='auto')","3d9cc823":"# Train the model - all layers\nhistory1 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n                              epochs=n_epoch,\n                              shuffle=True,\n                              verbose=1,\n                              callbacks=[reduce_lr],\n                              use_multiprocessing=True,\n                              workers=16,\n                              class_weight=class_weights\n                             )","331d3c89":"# Freeze core ResNet layers and train again \nfor layer in model.layers:\n    layer.trainable = False\n\nfor layer in model.layers[:50]:\n    layer.trainable = True\n\noptimizer = Adam(lr=0.0001)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer, \n              metrics=['accuracy'])\n\nn_epoch = 25\nhistory2 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n                              epochs=n_epoch,\n                              shuffle=True,\n                              verbose=1,\n                              callbacks=[reduce_lr, early_stop],\n                              use_multiprocessing=True,\n                              workers=16,\n                              class_weight=class_weights\n                             )","0a683fdc":"# Merge history1 and history2\nhistory = {}\nhistory['loss'] = history1.history['loss'] + history2.history['loss']\nhistory['accuracy'] = history1.history['accuracy'] + history2.history['accuracy']\nhistory['val_loss'] = history1.history['val_loss'] + history2.history['val_loss']\nhistory['val_accuracy'] = history1.history['val_accuracy'] + history2.history['val_accuracy']\nhistory['lr'] = history1.history['lr'] + history2.history['lr']","c1d00c93":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to output\")","90425f1e":"# Plot the training graph\ndef plot_training(history):\n    acc = history['accuracy']\n    val_acc = history['val_accuracy']\n    loss = history['loss']\n    val_loss = history['val_loss']\n    epochs = range(len(acc))\n\n    fig, axes = plt.subplots(1, 2, figsize=(15,5))\n    \n    axes[0].plot(epochs, acc, 'b-', label='Training Accuracy')\n    axes[0].plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n    axes[0].set_title('Training and Validation Accuracy')\n    axes[0].legend(loc='best')\n    \n    axes[1].plot(epochs, loss, 'b-', label='Training Loss')\n    axes[1].plot(epochs, val_loss, 'r-', label='Validation Loss')\n    axes[1].set_title('Training and Validation Loss')\n    axes[1].legend(loc='best')\n    \n    plt.show()\n    \nplot_training(history)","a8355cfb":"# Prediction accuracy on train data\nscore = model.evaluate_generator(train_generator, verbose=1)\nprint(\"Prediction accuracy on train data =\", score[1])","98140bc1":"# Prediction accuracy on CV data\nscore = model.evaluate_generator(valid_generator, verbose=1)\nprint(\"Prediction accuracy on CV data =\", score[1])","7cdf2d3b":"# Classification report and confusion matrix\nfrom sklearn.metrics import *\nimport seaborn as sns\n\ntick_labels = artstyles.tolist()\n\ndef showClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID):\n    # Loop on each generator batch and predict\n    y_pred, y_true = [], []\n    for i in range(STEP_SIZE_VALID):\n        (X,y) = next(valid_generator)\n        y_pred.append(model.predict(X))\n        y_true.append(y)\n    \n    # Create a flat list for y_true and y_pred\n    y_pred = [subresult for result in y_pred for subresult in result]\n    y_true = [subresult for result in y_true for subresult in result]\n     # Update Truth vector based on argmax\n    y_true = np.argmax(y_true, axis=1)\n    y_true = np.asarray(y_true).ravel()\n    \n    # Update Prediction vector based on argmax\n    y_pred = np.argmax(y_pred, axis=1)\n    y_pred = np.asarray(y_pred).ravel()\n    \n    # Confusion Matrix\n    fig, ax = plt.subplots(figsize=(10,10))\n    conf_matrix = confusion_matrix(y_true, y_pred, labels=np.arange(n_classes))\n    conf_matrix = conf_matrix\/np.sum(conf_matrix, axis=1)\n    sns.heatmap(conf_matrix, annot=True, fmt=\".2f\", square=True, cbar=False, \n                cmap=plt.cm.jet, xticklabels=tick_labels, yticklabels=tick_labels,\n                ax=ax)\n    ax.set_ylabel('Actual')\n    ax.set_xlabel('Predicted')\n    ax.set_title('Confusion Matrix')\n    plt.show()\n    \n    print('Classification Report:')\n    print(classification_report(y_true, y_pred, labels=np.arange(n_classes), target_names=artstyles.tolist()))\n\nshowClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID)","62561b87":"# Prediction\nfrom keras.preprocessing import *\n\nn = 5\nfig, axes = plt.subplots(1, n, figsize=(25,10))\n\nfor i in range(n):\n    random_art = random.choice(artstyles)\n    random_image = random.choice(os.listdir(os.path.join(filtered_imgdir, random_art)))\n    random_image_file = os.path.join(filtered_imgdir, random_art, random_image)\n    \n# Original image\n\n    test_image = image.load_img(random_image_file, target_size=(train_input_shape[0:2]))\n\n    # Predict artist\n    test_image = image.img_to_array(test_image)\n    test_image \/= 255.\n    test_image = np.expand_dims(test_image, axis=0)\n\n    prediction = model.predict(test_image)\n    prediction_probability = np.amax(prediction)\n    prediction_idx = np.argmax(prediction)\n\n    labels = train_generator.class_indices\n    labels = dict((v,k) for k,v in labels.items())\n    \n\n    title = \"Actual style = {}\\nPredicted style = {}\\nPrediction probability = {:.2f} %\" \\\n                .format(random_art.replace('_', ' '), labels[prediction_idx].replace('_', ' '),\n                        prediction_probability*100)\n\n    # Print image\n    axes[i].imshow(plt.imread(random_image_file))\n    axes[i].set_title(title)\n    axes[i].axis('off')\n\nplt.show()","7bf8c9bc":"url = 'https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/0\/03\/Vincent_Willem_van_Gogh_132.jpg\/800px-Vincent_Willem_van_Gogh_132.jpg'\n\nimport imageio\nimport cv2\n\nweb_image = imageio.imread(url)\nweb_image = cv2.resize(web_image, dsize=train_input_shape[0:2], )\nweb_image = image.img_to_array(web_image)\nweb_image \/= 255.\nweb_image = np.expand_dims(web_image, axis=0)\n\n\nprediction = model.predict(web_image)\nprediction_probability = np.amax(prediction)\nprediction_idx = np.argmax(prediction)\n\nprint(\"Predicted style =\", labels[prediction_idx].replace('_', ' '))\nprint(\"Prediction probability =\", prediction_probability*100, \"%\")\n\nplt.imshow(imageio.imread(url))\nplt.axis('off')\nplt.show()","64599bb2":"#  PART I - DATA REFINING - DATASET I \"painter-by-numbers\"\n\n *This part was done on my PC for the sake of efficiency, then the resulting dataset was uploaded and used in PART II*","56e4dc27":"# Getting the top 5 art styles and assigning weights for each of them\n","d96ef41f":"# Evaluate performance","52bf2b63":"# move files to their respective sub folders\nfor i in impressionism:\n        \n    shutil.copy(i, impressionism_folder)\n\n    \nfor i in realism:\n\n    shutil.copy(i , realism_folder)\n   \n    \nfor i in romanticism:\n    \n    shutil.copy(i, romanticism_folder)\n   \n    \nfor i in expressionism:\n\n    shutil.copy(i, expressionism_folder)\n    \n    \nfor i in postimpressionism:\n    \n    shutil.copy(i, postimpressionism_folder)\n  ","e7816fa0":"impressionism = images[images[\"style\"] == \"Impressionism\"]\nimpressionism.reset_index(drop= True, inplace=True)\nimpressionism = impressionism['filename'].values.tolist()\n\n\nrealism = images[images[\"style\"] == \"Realism\"]\nrealism.reset_index(drop= True, inplace=True)\nrealism = realism['filename'].values.tolist()\n\n\nromanticism = images[images[\"style\"] == \"Romanticism\"]\nromanticism.reset_index(drop= True, inplace=True)\nromanticism = romanticism['filename'].values.tolist()\n\n\nexpressionism = images[images[\"style\"] == \"Expressionism\"]\nexpressionism.reset_index(drop= True, inplace=True)\nexpressionism = expressionism['filename'].values.tolist()\n\n\npostimpressionism = images[images[\"style\"] == \"Post-Impressionism\"]\npostimpressionism.reset_index(drop= True, inplace=True)\npostimpressionism= postimpressionism['filename'].values.tolist()\n\n","93547d09":"# Evaluate performance by predicting on random images from dataset","d7d36daf":"# Export filtered data to a new .csv file\n\nfdata.to_csv('filtered.csv')","c5c2c23d":"#  Filtering the data, getting only images of the top 5 art styles:","b9de4fce":"# PART II - BUILDING THE MODEL - DATASET II \"filtered-painter-by-numbers\"","70bc6443":"# Confusion Matrix. Look at the styles which the model thinks are almost similar.","0e0a2621":"# Assigning the top styles to a list","c45c3e9b":"# Training graph","7f5958d8":"# Predicting the style of a random image from the web","ffaaec3a":"mypath = \"..\/input\/painter-by-numbers\/images\"\nonlyfiles = [f for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]\n\n\n# create sub folders in mypath folder\nimpressionism_folder = os.path.join(\".\/\", \"Impressionism\")\nrealism_folder = os.path.join(\".\/\", \"Realism\")\nromanticism_folder = os.path.join(\".\/\", \"Romanticism\")\nexpressionism_folder = os.path.join(\".\/\", \"Expressionism\")\npostimpressionism_folder = os.path.join(\".\/\", \"Post-impressionism\")\n\n# check if they already exits to prevent error\nif not os.path.exists(impressionism_folder):\n    os.makedirs(impressionism_folder)\n\n    \nif not os.path.exists(realism_folder):\n    os.makedirs(realism_folder)\n    \n    \nif not os.path.exists(romanticism_folder):\n    os.makedirs(romanticism_folder)\n    \n    \nif not os.path.exists(expressionism_folder):\n    os.makedirs(expressionism_folder)\n    \n    \nif not os.path.exists(postimpressionism_folder):\n    os.makedirs(postimpressionism_folder)\n","81260600":"**Reading the filtered data from the filtered csv file:**","b0a7cbf5":"# Building the Model","924c96ee":"# Data Augmentation"}}