{"cell_type":{"0ca3e7ba":"code","c098419d":"code","82daeefb":"code","f7e034a9":"code","e54b035f":"code","e2677a76":"code","566b5e4e":"code","2f870ee3":"code","3afff7d5":"code","7acc754a":"code","4095bf27":"code","18b936e2":"code","f2f711d6":"code","340f01fe":"code","a0ea093a":"code","c423cbec":"code","5357ef6c":"code","159872a9":"code","c0c17dee":"code","244ec79c":"markdown","21ce54c8":"markdown","f933a0f6":"markdown","d88c5c3d":"markdown","0303f97f":"markdown","86b7760f":"markdown","70aaff67":"markdown","837be16a":"markdown","8dd171d3":"markdown","4820eb47":"markdown","bcebf46f":"markdown","b856455a":"markdown","4bd0a341":"markdown","e4c38aeb":"markdown","d43a42af":"markdown","fa7c6106":"markdown","a5774b81":"markdown","fc52544e":"markdown","e9715510":"markdown","48a8ff96":"markdown","5cf1deaa":"markdown","beca77bd":"markdown","b659f963":"markdown","31f3b23c":"markdown"},"source":{"0ca3e7ba":"import numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn                 # the torch module to implement the Neural Networks\nimport torch.nn.parallel              # for parallel computations\nimport torch.optim as optim           # for optimizers\nimport torch.utils.data               # tools\nfrom torch.autograd import Variable   # for Stochastic Gradient Descent","c098419d":"movies = pd.read_csv('..\/input\/movie-data\/ml-1m\/movies.dat', sep = '::', \n                    header = None, engine = 'python', encoding = 'latin-1')\nmovies.head()","82daeefb":"users = pd.read_csv('..\/input\/movie-data\/ml-1m\/users.dat', sep = '::', \n                    header = None, engine = 'python', encoding = 'latin-1')\nusers.head()","f7e034a9":"ratings = pd.read_csv('..\/input\/movie-data\/ml-1m\/ratings.dat', sep = '::', \n                    header = None, engine = 'python', encoding = 'latin-1')\nratings.head()","e54b035f":"ratings.shape","e2677a76":"train_set = pd.read_csv('..\/input\/movie-data\/ml=100k\/u1.base', delimiter = '\\t') \ntrain_set.head()","566b5e4e":"train_set = np.array(train_set, dtype = 'int')\ntrain_set.dtype","2f870ee3":"test_set = pd.read_csv('..\/input\/movie-data\/ml=100k\/u1.test', delimiter = '\\t') \ntest_set = np.array(test_set, dtype = 'int')\ntest_set.dtype","3afff7d5":"print('There are', train_set[:,0].max(), 'Users in Training Set')\nprint('There are', train_set[:,1].max(), 'Movies in Training Set')\nprint('There are', test_set[:,0].max(), 'Users in Testing Set')\nprint('There are', test_set[:,1].max(), 'Movies in Testing Set')","7acc754a":"t_users = int(max(max(train_set[:,0]), max(test_set[:,0])))\nt_movies = int(max(max(train_set[:,1]), max(test_set[:,1])))\n\nprint(f'The total number of Users are {t_users} and total number of Movies are {t_movies}')","4095bf27":"def convert(dataset):\n    new_data = []                               \n    for user_id in range(1, (t_users + 1)):       \n        movie_ids = dataset[:,1][dataset[:,0]==user_id]\n        rating_ids = dataset[:,2][dataset[:,0]==user_id]\n        ratings = np.zeros(t_movies)\n        ratings[movie_ids - 1] = rating_ids \n        new_data.append(list(ratings))\n    \n    return new_data","18b936e2":"# applying the function above to training and test set\ntrain_set = convert(train_set)\ntest_set = convert(test_set)","f2f711d6":"train_set[0:1]","340f01fe":"train_set = torch.FloatTensor(train_set)\ntest_set = torch.FloatTensor(test_set)","a0ea093a":"# first we replace all the zeros in train set by -1\n# coz all the zeros are the non-existing ratings for a movie by a user\n# now the new ratings are going to be 0(liked) and 1(disliked), hence the orignal zeros must now have the new value as -1\n# thus, -1 will mean there wasn't a rating for a particular movie by a particular user\n\ntrain_set[train_set == 0] = -1            # movies not rated will be represented by -1\n\n# now we will change orignal ratings of 1 and 2 to 0, i.e. if the movie is rated 1 or 2, means that user disliked the movie\n# also the 'or' logic cannot be used with tensors thus we do the operation for 1 and 2 seperately\ntrain_set[train_set == 1] = 0\ntrain_set[train_set == 2] = 0\n\n# the original ratings greater than 3 will be considered as the user like the movie, hence we represent them by 1\ntrain_set[train_set >= 3] = 1\n\n\n\n# doing the same for test set\ntest_set[test_set == 0] = -1\ntest_set[test_set == 1] = 0\ntest_set[test_set == 2] = 0\ntest_set[test_set >= 3] = 1","c423cbec":"class RBM():\n    def __init__(self, nv, nh):\n        self.W = torch.randn(nh, nv)\n        self.a = torch.randn(1, nh)\n        self.b = torch.randn(1, nv)\n        \n    def sample_h(self, x):\n        wx = torch.mm(x, self.W.t())\n        activation = wx + self.a.expand_as(wx)\n        p_h_given_v = torch.sigmoid(activation)\n        return p_h_given_v, torch.bernoulli(p_h_given_v)\n    \n    def sample_v(self, y):\n        wy = torch.mm(y, self.W)\n        activation = wy + self.b.expand_as(wy)\n        p_v_given_h = torch.sigmoid(activation)\n        return p_v_given_h, torch.bernoulli(p_v_given_h)\n    \n    def train(self, v0, vk, ph0, phk):\n        self.W += (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()\n        self.b += torch.sum((v0-vk), 0)\n        self.a += torch.sum((ph0-phk), 0)","5357ef6c":"nv = len(train_set[0])      # no. of visible nodes\nnh = 100                    # the features to be detected by RBM, hence can define any relevant number\nbatch_size = 100\nrbm = RBM(nv, nh)","159872a9":"nb_epoch = 10        # 10 because as we have a binary outcome and less data, the model will converge quickly\n\n# creating a for loop to iterate through these epochs and in each epoch all observations go in the network \n# and then updating the weights after observations of each batch that passed through the network\n# and then we get our final visible nodes with new ratings for the movies that were not orignally rated\nfor epoch in range(1, nb_epoch+1):\n    train_loss = 0                      \n    s = 0.                          \n    \n    for id_user in range(0, t_users - batch_size, batch_size):\n        vk = train_set[id_user:id_user+batch_size]\n        v0 = train_set[id_user:id_user+batch_size]\n        ph0,_ = rbm.sample_h(v0)\n        \n        for k in range(10):\n            _,hk = rbm.sample_h(vk)\n            _,vk = rbm.sample_v(hk)\n            vk[v0<0] = v0[v0<0] \n    \n        phk,_ = rbm.sample_h(vk)\n        rbm.train(v0, vk, ph0, phk)\n        \n        train_loss += torch.mean(torch.abs(v0[v0 >= 0] - vk[v0 >= 0]))\n        s += 1.\n        \n    print('epoch: ' + str(epoch) + ' loss: ' + str(train_loss\/s) )","c0c17dee":"test_loss = 0                      \ns = 0.                          \n\nfor id_user in range(t_users):           \n    v = train_set[id_user:id_user+1]      \n    vt = test_set[id_user:id_user+1]      \n    \n    if len(vt[vt>=0]) > 0:           \n        _,h = rbm.sample_h(v)\n        _,v = rbm.sample_v(h)\n        \n        test_loss += torch.mean(torch.abs(vt[vt >= 0] - v[vt >= 0]))\n        s += 1.\n\nprint('test_loss: ' + str(test_loss\/s) )","244ec79c":"**Conclusions from test loss of 25%**\n* Here we also predict ratings correctly 3 times out of 4 or in short we get a **75%** accuracy\n\n**Understanding the for loop above:**\n* We don't need a batch size here as its only specific to training \n* `v` is input on which we will make predictions and we use `train_set` coz the input will be used to activate the hidden neurons to get the output `v`\n* `vt` is the target and `test_set` coz we are comparing actual `test_set` results to predicted ones\n* The `if loop` is for getting existing ratings from the target which contains orignal ratings of the test set\n* Updating the `test_loss` and with `vt>0` we are only getting those cells which have an existing rating","21ce54c8":"### Step - 4\n* Importing the Training and Testing set\n* But as a million observations would take a lot of time to process on we load another dataset having same columns but only 100k observations\n* Out of these 100k, we take 80% in the training set\n* A point to note that we already have training and testing set seperately given by the MovieLens Dataset","f933a0f6":"* The 1st column is nothing but the `Movie ID`\n* And this is the info we will use to build a Recommender System rather than on the `Movie Title` which is the 2nd column\n* The 3rd column is the `Genre` of the Movie\n\n**Note -**\n* This Dataset is shown here to give you the idea of the movies\n* We are not going to use this data to make training\/testing set\n\n### Step - 2\n* Importing the Users Data\n* Yet again we will be just seeing the dataset and not work on it","d88c5c3d":"**Conclusions from the above operations -**\n* The 1st 3 elements `0`, `3` and `4` represent that the 1st user **didn't rate the 1st movie** hence rating is 0, the **2nd movie was watched and rated 3** and **3rd movie was watched and rated 4** respectively\n* For any deep learning model, lines are observations going into the network and columns are features that are going to be the input nodes in the network\n* Hence for each user we will have ratings of all movies and 0s included, hence these ratings are going to be the input nodes for this observation going into the network\n* This is how the architecture is supposed to be and that can be built using Pytorch Tensors","0303f97f":"How to interpret these values -","86b7760f":"### Step - 8\n* We are building a recommender system to predict whether a person will like the movie or not\n* Hence, we will be converting our movie ratings ranging from `0 to 5` into a binary variable as `0 (liked)` and `1 (didn't like)`\n* The reason for doing this is because the RBM takes the input vector, and inside this vector it will predict the ratings for the movies which weren't orignally rated by the user\n* The predicted ratings are computed from the existing ratings of the input vector, hence the predicted ratings in the outputs must have same format as the existing ratings in the input","70aaff67":"### Step - 10\n**Creating 1st RBM object:**\n* The weights and biases a and b will be initialized as only the init function will take action\n* init function will take action because it is the default function and as soon as you create an object of a class, the init fucntion is called first and from this object we can call different functions defined in the class\n* And the rest of the functions will be used while training\n","837be16a":"### Step - 11\n* Training the RBM","8dd171d3":"* 1st column is the `User ID`\n* 2nd column is the `Gender`\n* 3rd column is the `Age`\n* 4th column contain's `some codes corresponding to the users job`\n* The last column is the `Zip Code`\n\n### Step - 3\n* Importing the Ratings Data","4820eb47":"* This tells us that the Maximum number of Users is 943 in train set and 462 in test set, which means the last User ID is available in train set which means this number is the total number of users\n* Same goes for the total number of Movies\n* Hence we consider total number of Users as **943** and total number of Movies as **1682**\n* But to make it a global function as the maximum may lie in the test data sometimes and we want our code to automatically select the max of the max of train and test set. Hence we do the following","bcebf46f":"**Understanding the class RBM above -**\n\n**1. __init__ function:**\n* 1st we make the default `init` function each class has and pass a mandatory argument called `self`\n* `self` corresponds to the object that will be created afterwards and we are going to define some variables and it is needed to specify that these variables are the variables of the object that will be created afterwards and not some global variables\n* All the variables that are attached to the object, will be created by putting a `self` before the variable\n* The next parameters in the `init` functions`nv` and `nh` are no. of visible and hidden nodes respectively\n* Now we initialize the parameters of our future objects that will be created from this class, hence this function will initialize all the parameters we will be optimizing while training the RBM\n* These parameters are nothing but weights and biases and are specific to RBM models and we need to specify that these variables are the variables of the object \n* 1st comes the weight hence - `self.W`, the capital W because all the weights will be initialized in the torch tensor\n* These weights are all the parameters of the probabilities of the visible nodes given the hidden nodes\n* And according to theory they are initialized in a matrix of size `nh` and `nv` and this matrix will be a tensor\n* `torch.randn(nh,nv)` will initializes a tensor of size `nh`x`nv` according to Standard Normal Distribution (mean=0 and var=1). Hence we initialized all weights for the probabilities p of the visible node according to the hidden node\n* `self.a = torch.randn(1, nh)` will create a bias of hidden nodes and we add a fake dimension `1` for the batch coz for the fuctions in pytorch cannot expect a single input vector(1d) as an arugument but a 2d tensor where the 1st dimension represents the batch and 2nd one representing the bias\n* Since there is 1 bias for each hidden node and we have `nh` hidden nodes and hence we created the vector of `nh` elements all initialized to some number that follows a Normal Distribution\n* The same is created for the visible nodes using `self.b = torch.randn(1, nv)`\n\n**2. sample_h function:**\n* For every hidden node in the RBM this func. will activate them according to certain probability that we compute in the same function. For each hidden node, this probabilty will be P(h|v) \n* It is this probability that is equal to the Activation function\n* `self` added to access variables in the __init__ function\n* `x` corresponds to visible nodes in the probabilities P(h|v)\n* `wx = torch.mm(x, self.W.t())` - as it is a Sigmoid activation function, we will be multiplying weights `W` with visible nodes and add bias to it and the `.t()` will create a transpose of matrix of weights to make the product consistent\n* Now inside `activation` function, we will be adding `wx` and bias `a` \n* And as every input vector is not treated individually but inside batches, hence we created an extra dimension earlier\n* The bias should be applied to each line of the mini batch, i.e. each line of the dimension created above\n* Hence we use a function `expand_as` which again adds a new dimension for this bias that we are adding and we add a parameter as (wx) to tell as what we want to expand the bias\n* Hence inside the activation we created a linear combination of visible neurons `x` where the co-effecients are weights and to all this we add the bias\n* `p_h_given_v` - Now we compute the activation function that will activate the hidden node and remember that this activation function represents a probability, i.e. a probability that a hidden node will be (1)activated according to the value of visible node\n* Final step is to return the probabilities and sample of all hidden nodes according to probabilities in `p_h_given_v`\n* `torch.bernoulli(p_h_given_v)` will give a vector of 0s(hidden nodes that weren't activated by the sampling) and 1s(hidden nodes that were activated by the sampling)\n\n**3. sample_v function:**\n* Will be the same as above with slight changes\n* `y` corresponds to hidden nodes in the probabilities P(v|h)\n* `wy = torch.mm(y, self.W)` - here we don't take the transpose because previously we were calculating P(h|v) and since W is the matrix of P(v|h), we took the transpose. Hence here we don't need to take the transpose\n* `p_v_given_h` will have probabilities of visible nodes being activated according to the value of hidden nodes\n\n**4. k- step Contrastive Divergence (train function):**\n* Used to approximate the likelihood gradient\n* `self` coz we will update the tensor of weights and bias `a` and `b` that are variables specifically attached to the object\n* `v0` is the input vector of ratings of all movies by 1 user\n* `vk` are the visible nodes obtained after k iterations through the network\n* `ph0` is the vector of probabilities that at the 1st iteration the hidden nodes equal 1 given the values of `v0`\n* `phk` is the vector of probabilities that at the 1st iteration the hidden nodes equal 1 given the values of `vk`\n* Then we update the weights, `bias a` and `bias b` and rest of the code is self explanatory and taken from the formula available in the Research paper An Introduction to Restricted Boltzmann Machines by Asja Fischer and Christian Igel\n![Capture.PNG](attachment:Capture.PNG)\n","b856455a":"This Data has over 1 million observations and 4 columns","4bd0a341":"### Step - 7\n* Converting data into Torch Tensors\n* Pytorch Tensors are nothing but the arrays that contain elements of a single datatype aka multidimensional matrix\n* The only difference is that this is a pytorch array and not a numpy array\n* A network can be built using numpy arrays but doesn't prove to be efficient than tensors","e4c38aeb":"**Step - 4.2**\n* Doing the same for test set","d43a42af":"### Step - 2\nImporting the Movies data\n\n**Arguments used -**\n* Its a `.dat` file which is seperated by `'::'` hence the `sep` is given so\n* Doesn't has a `header` hence set to `None`\n* `engine` = python to make sure dataset gets imported correctly\n* `encoding = latin-1` as some movie titles contain special characters","fa7c6106":"### Step - 5\n* Getting total number of Users and Movies\n\n**Why do this?**\n* Because in the next step we are going to convert the training and test set into a matrix where the lines will be the Users and the columns are going to be Movies and cells are going to be Ratings\n* And in each of the matrices, we will be including all the Users and Movies from the Orignal Dataset\n* If any User didn't give a rating, then a 0 will be added in the cell that corresponds to this user and that movie\n* These matrices are going to have same number of movies and same number of users, will have same number of rows and same number of columns\n* In each of these matrices, each cell of index `Ui` (U is the user and i is the movie) will get the rating for the movie `i` by the user `U`\n* And if this User `U` didn't rate the movie `i`, I will put a 0","a5774b81":"### Step - 6\n* Converting the data into array with `Users` in lines and `Movies` in columns\n* The reason for doing step 5 will make sense here\n* The reason for such restructuring of the data is to get the data into a format of inputs acceptable by the **Restricted Boltzmann Machine (RBM)**\n* RBM is a form of NN where there are some input nodes, features and observations going one by one into the network starting with the input node\n* This is why there is a need to create this array with `observations` in lines and `features` in columns","fc52544e":"**Understanding the function and for loop**\n* First we initialize the list as new_data\n* Then we make a for loop to take all the user ids from the 1 to the max no. of total users\n* `t_users+1` because the range operator ignores the last index and we want all the total users\n* Then we get movie_ids of the all movies for this user, i.e the movies watched by the user and rated as well currently in the loop\n* Now we also want ratings for those movies he watched hence again a condition is applied as shown above\n* As we have obtained the list of movies which the user rated, so the next step would be creating a list of movies which the user didn't rate as well. The elements of this list will correspond to total movies (1682 in this case) and for each movie the user rated we get 1 and 0 if not rated\n* Hence we initialize a list of 1682 zeros and then for that movie that particular user rated, we are going to replace the 0 by the rating actually given by that user by using the movie_ids variable created \n* `movie_ids - 1` is done because the ratings will have the indexing starting from 0 and the movie_ids from 1. Thus to match this difference and avoid it from assigning the 2nd element of movie_ids to the 1st element of ratings, we do a `-1`. We assign this to `rating_ids` as they are real ratings\n* In short we managed to create for each user, the list of all the ratings including the 0s for the movies that weren't rated\n* Now we need to add this list of ratings corresponding to 1 user the `new_data` list that will contain all the different lists for all different users","e9715510":"**Step - 4.1**\n* Converting the dataframe to array\n* Done because Pytorch works best with arrays and not dataframes","48a8ff96":"**Step - 6.1**\n\nWe are going to do this on training as well as test set. Hence it is better to create a function for this\n* Here we don't create a 2D array but a list of lists, i.e one list for each line\/observation or user\n* Since we have 943 users, hence we will have 943 lists and they will be horizontal. The 1st list will correspond to 1st user and so on\n* And in each list we will have the ratings of 1682 movies by the user corresponding to the list\n* Hence rating for each movie by the user will be stored and if the user didn't rate a particular movie. Then we will get a 0 for that cell\n* This explains that the training and test set will have the same size because for noth of them we are considering all the movies and all the users and what we do is put a 0 when a user doesn't rate a movie","5cf1deaa":"# Movie Recommender System using Boltzmann Machines\n\n### Objective\n* The objective here is to create a Recommender System with a binary output YES\/NO to predict whether or not the viewer will like a particular movie\n* This project is sort of a Deep Case Study on RBM which focuses purely upon implementing the theoritical knowledge of a Restricted Boltzmann Machine into Python\n\n### Data Source\n* GroupLens Research has collected and made available rating data sets from the MovieLens web site (http:\/\/movielens.org). The data sets were collected over various periods of time, depending on the size of the set. \n* It has 1 million ratings from 6000 users on 4000 movies. Released in FEB\/2003. \n\n### Why Pytorch?\n* Is has proven more useful and effective than tensorflow when it comes to implementing Boltzmann Machines and Auto Encoders\n* Theano and Tensorflow can be used to implement the same but the framework of Pytorch makes implementation more easier\n* It is more inutuitive, practical and flexible to make any changes in the architecture of your model when it comes to Pytorch\n\n### Goals of this Project :\n* Implementing and knowing the significance of Pytorch and performing Tensor operations\n* To make our own class to implement Restricted Boltzmann Machines\n* Understanding how an RBM works \n* Understanding the techniques of k-step contrastive divergence, updating the weights, probabilities of hidden and visible nodes, etc\n* Calculating the loss function and interpreting the loops and what happens at the backend of an RBM\n\n### Step - 1\nImporting the import Libraries","beca77bd":"**Understanding the for loop above:**\n* `train_loss = 0` the loss variable as every NN has a parameter to minimize the loss function and this variable will increase every time we find errors between predictions and real ratings\n* `s = 0.0` a float counter coz we want to normalize the train loss and to do se we divide this loss by counter, which will increase after every epoch\n* Contrastive Divergence is to be done on all users in batch hence we need another for loop to get batches of users\n* `range(0, t_users - batch_size, 100)` coz we want to go in batches from 0 to 943 in steps of 100. Batch is a hyper-parameter we can tune to get more\/less better performance results on training set\n* `vk` is the input of all observations\/ratings in a batch\n* `train_set[id_user:id_user+batch_size]` coz we want to get the range of every user from that `id_user` upto the `id_user + batch_size`\n* `v0` are movies already rated by the 100 users in this batch\n* `ph0` will be the probabilities that the hidden nodes will be one given the real ratings and we call the `sample_h` function we created above as it returns `p_h_given_v`. `ph0,_` will get us only the first argument in the return statement of that function\n* Another for loop to implement k-step contrastive divergence\n* `_,hk` are the hidden nodes obtained at k'th step of contrastive divergence and we want the 2nd element returned by the `sample_h` function\n* `_,vk` will be sampled visible node after the 1st step of Gibbs samplings and we do that by calling the `sample_v` function on 1st sample of hidden node that is `hk` which was the result of 1st sampling based on 1st visible node\n* Now, when `k` will be equal to 1, we enter into the 2nd step of Gibbs Sampling. Hence `hk` gets updated and since `vk` is the 1st sample of the visible node, the `sample_h(vk)` will return the 2nd sample of hidden node. Then this 2nd sample of hidden node will be the input of `sample_v` function and that will return the new sample of the visible nodes, i.e the 2nd sample of visible node. And again the ratings will be updated. This goes on and on as we get the 3rd sample of hidden node then the 3rd sample of visible node, etc until the end of the loop till we have 10th sample of hidden and visible node each\n* We did this to approximate the Gradient to update the weights and bias. Coz to do that we need the last samples of hidden and visible nodes\n* Now we don't want our model to train on the movies not rated(-1), hence we freeze the visible nodes which have a `-1` rating before training the RBM. In short, it won't be possible to update them using Gibbs sampling\n* `vk[v0 < 0]` to get orignal(v0) ratings `-1` and we tell them to keep the `-1` ratings by `v0[v0<0]`\n* The last essential element before applying the train function is `phk`. Hence we get it first by getting out of the for loop and applying `sample_h` function to last sample of the visible node after the 10 steps i.e. `vk`\n* `train` doesn't return anything, it just updates the weight, hence we don't create a new variable\n* Now training will take place, weights and biases will be updated towards the direction of maximum likelihood. The P(v|h) will be more relevant, and will get the largest weights for probabilities that are more significant. Eventually this will lead to predicted ratings being close to real ratings\n* As we have our predictions now, we need to calculate `train_loss`\n* `[v0 >= 0]` coz we need to consider the ratings that exist\n* Update the counter `s` which normalizes the `train_loss` -> `(train_loss\/s)`\n* `print` function to check how we perform after every epoch and the loss associated with that respective epoch\n\n**Conclusions from training loss obtained - 24.64%**\n* This loss means we will be right 3 out of 4 times while predicting\n* In simple words we got a **76%** accuracy in predicting the ratings of movies by all our users\n\n\n### Step - 12\n* Testing the RBM\n* We will be testing the same on new samples\/observations\n* Let's make predictions on test set without doing any training of course and we will be calcuating `test_loss`\n* This loss would be again nothing but the mean of the absolute distances between the predictions and the actual ratings\n* So our goal is to hope that we get a `test_loss` around `0.2464`","b659f963":"### Step - 9\n* Creating the architecture of Neural Network\n* We have to build a probabilistic graphical model as a Restricted Boltzmann Machine resembles the same\n* To do this, we will be building a class to define how RBM should be build as classes are a practical way to build anything in python\n* We will be passing info like the no. of hidden nodes, weights for probability of visible nodes given the hidden node then the bias for same probability and also the bias for probability of visible node given the hidden node\n* Hence all these parameters are required to initialize the RBM and we will be putting them in a function called `init`\n* To summarize, we will be creating 3 functions\n   * To initialize the RBM object\n   * sample_h to sample the probabilities of hidden nodes given the visible nodes `P(h|v)` \n     * aka Sigmoid activation function\n     * This func. is needed because in training we approximate log likelihood gradient through Gibbs Sampling\n   * sample_v to sample the probabilities of visible nodes given the hidden nodes `P(v|h)`","31f3b23c":"* 1st columns corresponds to `Users` and the `1` that we see here is the `1st User` in this Database\n* 2nd column corresponds to `Movies` and the numbers in it are `Movie IDs`\n* The 3rd column corresponds to the `Ratings` from `1 to 5` \n* 4th column corresponds to the `TimeStamps` when the user rated the movie which has no relevance to our model"}}