{"cell_type":{"877c1378":"code","7695be35":"code","07e09d42":"code","2c84d1ed":"code","8ff1190c":"code","f9571fa5":"code","521aba8d":"code","79ddf086":"markdown","d89803fc":"markdown","b02e763b":"markdown"},"source":{"877c1378":"import os\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport pandas as pd\nfrom tqdm import tqdm\nfrom transformers import (AutoModelForMaskedLM,\n                          AutoTokenizer, LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments)\n\nwarnings.filterwarnings('ignore')\nos.environ[\"WANDB_DISABLED\"] = \"true\"","7695be35":"#model_name = \"roberta-large\"\n#model_name = \"roberta-base\"\nmodel_name = \"allenai\/longformer-base-4096\"\n#model_name = \"allenai\/longformer-large-4096\"\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","07e09d42":"train_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/train'))):\n    train_names.append(f.replace('.txt', ''))\n    with open('..\/input\/feedback-prize-2021\/train\/' + f, 'r', encoding='utf-8') as f:\n        text = ''\n        for line in f.readlines():\n            #text += line.replace('\\n', '').replace('\\xa0', '')\n            text += line.replace('\\n', ' ')\n        train_texts.append(text)","2c84d1ed":"texts = '\\n'.join(train_texts)","8ff1190c":"with open('text.txt', 'w') as f:\n    f.write(texts)","f9571fa5":"tokenizer.save_pretrained(\".\/model_pretrained\") ","521aba8d":"train_dataset = LineByLineTextDataset( \n    tokenizer=tokenizer,\n    file_path=\"text.txt\",  # mention train text file here\n    block_size=1024)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\",  # mention valid text file here\n    block_size=1024)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\ntraining_args = TrainingArguments(\n    output_dir=\".\/model_pretrained_chk\",  # select model path for checkpoint\n    overwrite_output_dir=True,\n    num_train_epochs=4,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=6,\n    #evaluation_strategy='steps',\n    evaluation_strategy='epoch',\n    save_total_limit=1,\n    eval_steps=5000,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end=False,\n    prediction_loss_only=True,\n    learning_rate=5e-5,\n    seed=2021,\n    report_to=\"none\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset)\n\ntrainer.train()\ntrainer.save_model(f'.\/model_pretrained')","79ddf086":"## Further Pretraining","d89803fc":"## Load Data","b02e763b":"## Further Pre-training\n\n### This part is copied from: https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-itpt\n\n### Thanks for his work!\n\nBesides the training data of a target task, we can **further pre-train** a transformer on the data from the same domain.\n\n![image.png](https:\/\/media.springernature.com\/original\/springer-static\/image\/chp%3A10.1007%2F978-3-030-32381-3_16\/MediaObjects\/489562_1_En_16_Fig1_HTML.png)\n\nThe Transformer models are pre-trained on the general domain corpus. For a text classification task \/ regression task in a specific domain, such as Readability Assesment, its data\ndistribution may be different from a transformer trained on a different corpus e.g. RoBERTa trained on BookCorpus, Wiki, CC-News, OpenWebText, Stories. Therefore the idea is, we can further pre-train the transformer with masked language model and next sentence prediction tasks on the domain-specific data. Three further pretraining approaches are performed:\n\n1) `Within-task pre-training (ITPT)`, in which transformer is further pre-trained on the training data of a target task. `This Kernel.`\n\n2) `In-domain pre-training (IDPT)`, in which the pretraining data is obtained from the same domain of a target task. For example, there are several different sentiment classification tasks, which have a similar data distribution. We can further pre-train the transformer on the combined training data from these tasks.\n\n3) `Cross-domain pre-training (CDPT)`, in which the pretraining data is obtained from both the same and other different domains to a target task.\n\n#### Reference1: [How to finetune BERT for Text Classification ?](https:\/\/arxiv.org\/pdf\/1905.05583.pdf)\n#### Reference2: [Don't Stop Pretraining: Adapt Language Models to Domains and Tasks](https:\/\/arxiv.org\/abs\/2004.10964)\n\n> Note: This Kernel implements ITPT i.e. Within-Task Pretraining. First we will pretrain a RoBERTa model and then utilize the same for further finetuing tasks using different strategies."}}