{"cell_type":{"6ca7638c":"code","b2e79896":"code","15c6d085":"code","7195e158":"code","5d39d466":"code","d26138f3":"code","128cfd77":"code","07f64c65":"code","c7738710":"code","4ab03700":"code","2b610380":"code","c47faa8d":"code","5829a892":"code","ca24220c":"code","76373f37":"code","2c536c62":"code","26b96670":"code","47bfd93f":"code","fd06e4e8":"code","d3821d4f":"code","57574d25":"code","1636f4e9":"code","7cec321d":"code","3e5ccfed":"code","25baee8c":"code","cbf8e173":"code","f14a140b":"code","d64a09fa":"code","b49d5c52":"code","d2a8bfae":"code","cac51469":"code","c503caf4":"code","c915500c":"code","4b8ce78e":"code","ec29973d":"code","40738edc":"code","35c6c888":"code","2503f401":"markdown","c6be3bc8":"markdown","71a779c7":"markdown","77e2d7be":"markdown","d09feae1":"markdown","e2b2b7ca":"markdown","6c86f930":"markdown","7710a4d7":"markdown","7e309a60":"markdown","343b1546":"markdown","8ec877eb":"markdown","7191c2c4":"markdown","e9af0d5d":"markdown","15297d24":"markdown","7311f03c":"markdown","a02b93fc":"markdown","8ccec75c":"markdown","315527cf":"markdown","bff9b96a":"markdown","5383f601":"markdown","187a3372":"markdown","a1acec8f":"markdown","57b697dd":"markdown","d58d8f78":"markdown","efea92c3":"markdown","dd229855":"markdown","aa3a77da":"markdown","eb9f603b":"markdown","863b4629":"markdown","9dc320eb":"markdown","ca732dda":"markdown","c6b851c1":"markdown"},"source":{"6ca7638c":"# Data and Visuals\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting visuals\nimport seaborn as sns # plotting visuals\n\n# Infrastructure\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nprint('Library Import Complete')","b2e79896":"#To import the dataset into dataframe\ndf = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\nprint('Import Dataset Complete')","15c6d085":"# To display the first five rows of dataframe\ndf.head()","7195e158":"# To display the number of columns and rows in dataframe\ndf.shape","5d39d466":"# To display dataframe info\ndf.info()","d26138f3":"# To check sum of null values\ndf.isnull().sum()","128cfd77":"# To display basic statistical description of dataframe\ndf.describe()","07f64c65":"# To determine value_counts on dataframe\nclass_names = {0: 'Not Diabetic', 1:'Diabetic'}\nres_var_analysis = df.Outcome.value_counts().rename(index = class_names)  #change df.Outcome to df.[column_name] of outcome variable\n\nprint(res_var_analysis)","c7738710":"# To install pandas-profiling library\n!pip install pandas-profiling --ignore-installed","4ab03700":"# To import ProfileReport from pandas-profiling library\nfrom pandas_profiling import ProfileReport\npp_report = ProfileReport(df)\npp_report.to_file(output_file='pandas_profiling_report.html')","2b610380":"# To install sweetviz library\n!pip install sweetviz --ignore-installed","c47faa8d":"# To import sweetviz\nimport sweetviz as sv\nsw_report = sv.analyze(df)\nsw_report.show_html()","5829a892":"# To import library for train_test_split\nfrom sklearn.model_selection import train_test_split\n\nprint('Import Train_Test_Split Complete')","ca24220c":"# To determine y and X values of train_test_split\ny = df['Outcome']  #dependant variable\nX = df.loc[:, df.columns != 'Outcome'] #independant variables\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.33, random_state=42)\n\nprint('Test_Train Split Complete')","76373f37":"# To import library for accuracy score and linear regression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\nprint('Import Accuracy Score and Logistic Regression Libraries Complete')","2c536c62":"# To initialize logistic regression classifier\nlogreg = LogisticRegression(solver='liblinear') # solver changed to resolve \n\n# To train the model using the training dataset\nlogreg.fit(X_train, y_train)\n\n# To predict the test dataset\ny_predict = logreg.predict(X_test)\n\n# To calculate the model accuracy by comparing y_test and y_predict\nlogreg_accuracy = round(accuracy_score(y_test, y_predict) *100, 3)\n\nprint('Accuracy of Logistic Regression model is: ', logreg_accuracy, '%')","26b96670":"# To import library for linear discriminant analysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nprint('Import Linear Discriminant Analysis Libraries Complete')","47bfd93f":"# To initialize the linear discriminant classifier\nmodel = LinearDiscriminantAnalysis()\n\n# To train the model using the train_test_split dataset\nmodel.fit(X_train, y_train)\n\n# To predict the test dataset\ny_predict = model.predict(X_test)\n\n# To calculate the model accuracy by comparing y_test and y_predict\nlindisc_accuracy = round(accuracy_score(y_test, y_predict) *100, 3)\n\nprint('Accuracy of Logistic Regression model is: ', lindisc_accuracy, '%')","fd06e4e8":"# To import library for Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\nprint('Import Gaussian Naive Bayes Libraries Complete')","d3821d4f":"# To initialize the Gaussian Naive Bayes classifier\nmodel = GaussianNB()\n\n# To train the model using the train_test_split dataset\nmodel.fit(X_train, y_train)\n\n# To predict the test dataset\ny_predict = model.predict(X_test)\n\n# To calculate the model accuracy by comparing y_test and y_predict\ngausnb_accuracy = round(accuracy_score(y_test, y_predict) *100, 3)\n\nprint('Accuracy of Gaussian Naive Bayes model is: ', gausnb_accuracy, '%')","57574d25":"# To import library for decision tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nprint('Import Decision Tree Libraries Complete')","1636f4e9":"# To initialize the decision tree classifier\nmodel = DecisionTreeClassifier()\n\n# To train the model using the train_test_split dataset\nmodel.fit(X_train, y_train)\n\n# To predict the test dataset\ny_predict = model.predict(X_test)\n\n# To calculate the model accuracy by comparing y_test and y_predict\ndtree_accuracy = round(accuracy_score(y_test, y_predict)*100, 3)\n\nprint('Accuracy of Decision Tree model is: ', dtree_accuracy, '%')","7cec321d":"# To import library for random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nprint('Import Random Forest Libraries Complete')","3e5ccfed":"# To initialize the random forest classifier\nmodel = RandomForestClassifier()\n\n# To train the model using the train_test_split dataset\nmodel.fit(X_train, y_train)\n\n# To predict the test dataset\ny_predict = model.predict(X_test)\n\n# To calculate the model accuracy by comparing y_test and y_predict\nrforest_accuracy = round(accuracy_score(y_test, y_predict) *100, 3)\n\nprint('Accuracy of Decision Tree model is: ', rforest_accuracy, '%')","25baee8c":"# To import library for support-vector machine\nfrom sklearn import svm\n\nprint('Import Random Forest Libraries Complete')","cbf8e173":"# To initialize the support-vector machine classifier\nmodel = svm.SVC()\n\n# To train the model using the train_test_split dataset\nmodel.fit(X_train, y_train)\n\n# To predict the test dataset\ny_predict = model.predict(X_test)\n\n# To calculate the model accuracy by comparing y_test and y_predict\nsupvector_accuracy = round(accuracy_score(y_test, y_predict) *100, 3)\n\nprint('Accuracy of Decision Tree model is: ', supvector_accuracy, '%')","f14a140b":"# To import library for k-nearest neighbor\nfrom sklearn.neighbors import KNeighborsClassifier\n\nprint('Import Random Forest Libraries Complete')","d64a09fa":"# To initialize the support-vector machine classifier\nmodel = KNeighborsClassifier()\n\n# To train the model using the train_test_split dataset\nmodel.fit(X_train, y_train)\n\n# To predict the test dataset\ny_predict = model.predict(X_test)\n\n# To calculate the model accuracy by comparing y_test and y_predict\nknn_accuracy = round(accuracy_score(y_test, y_predict) *100, 3)\n\nprint('Accuracy of Decision Tree model is: ', knn_accuracy, '%')","b49d5c52":"# To create dataframe from the models and their accuracy scores\nmodels = pd.DataFrame({\n    'Model':['Logistic Regression','Linear Discriminant Analysis', 'Gaussian Naive Bayes', 'Decision Tree', 'Random Forest', 'Support-Vector Machine', 'K-Nearest Neighbor'],\n    'Score':[logreg_accuracy, lindisc_accuracy, gausnb_accuracy, dtree_accuracy, rforest_accuracy, supvector_accuracy, knn_accuracy]\n})\n\n# To sort the accuracy score values by ascending order\nmodels.sort_values(by='Score', ascending=False)","d2a8bfae":"# To display regression plot\nsns.regplot(x='Glucose', y='Outcome', data=df, logistic=True)","cac51469":"# To import library for confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\nprint('Import Random Forest Libraries Complete')","c503caf4":"# To initialize the confusion matrix using the train_test_split for Y\nconf_matrix = confusion_matrix(y_test, y_predict)\nprint(conf_matrix)","c915500c":"# To plot heatmap of confusion matrix\nsns.heatmap(conf_matrix, annot=True, fmt='f', annot_kws={\"size\": 10})","4b8ce78e":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(model, X_test, y_test)","ec29973d":"# To import library for confusion matrix\nfrom sklearn import metrics","40738edc":"# To display accuracy, precision, and recall of test-train split\nprint('Accuracy:', metrics.accuracy_score(y_test, y_predict))\nprint('Precision:', metrics.precision_score(y_test, y_predict))\nprint('Recall:', metrics.recall_score(y_test, y_predict))","35c6c888":"y_pred_proba = logreg.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\n\nplt.plot(fpr,tpr,label=\"Data 1, AUC=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","2503f401":"## Logistic Regression Plot\n","c6be3bc8":"### Confusion Matrix Findings\n    True positives - 130  |  False positives - 38\n    False negatives - 38  |  True negatives - 48\n\n**Indicates *high* false negative rate**","71a779c7":"### Logistic Regression\n\nhttps:\/\/en.wikipedia.org\/wiki\/Logistic_regression","77e2d7be":"## Confusion Matrix Evaluation Metrics\nhttps:\/\/www.datacamp.com\/community\/tutorials\/understanding-logistic-regression-python\n\nPrecision: Precision is about being precise, i.e., how accurate your model is. In other words, you can say, when a model makes a prediction, how often it is correct. In your prediction case, when your Logistic Regression model predicted patients are going to suffer from diabetes, that patients have 76% of the time.\n\nRecall: If there are patients who have diabetes in the test set and your Logistic Regression model can identify it 58% of the time.","d09feae1":"## Train Test Split  \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html\n\nSplits dataframe into *training* and *testing* data","e2b2b7ca":"## Confusion Matrix\nhttps:\/\/en.wikipedia.org\/wiki\/Confusion_matrix\n\n    TP - True positives (predicted positive are actually positive)\n    FP - False positives (predicted positive are actually negative)\n    TN - True negatives (predicted negative are actually positive)\n    FN - False negatives (predicted negative are actually positive)\n\n<img src= \"https:\/\/i.stack.imgur.com\/D1Lk4.png\" alt =\"Confusion Matrix\">","6c86f930":"### Metadata\n#### Usage Information\n**License:**\nCCO: Public Domain  \n**Visibility:**\nPublic  \n\n#### Maintainers\n**Dataset owner**  \nUCI Machine Learning  \n\n#### Updates  \n**Expected update frequency:**  \nNot specified  \n**Last updated:**  \n2016-10-06  \n**Date created:**  \n2016-10-06  \n**Current version:**  \nVersion 1","7710a4d7":"### Pandas Profiling\nhttps:\/\/github.com\/pandas-profiling\/pandas-profiling  \nGenerates profile reports from a pandas DataFrame. The pandas df.describe() function is great but a little basic for serious exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis.\n\nFor each column the following statistics - if relevant for the column type - are presented in an interactive HTML report:","7e309a60":"## Data Modeling\n1.  Logistic Regression\n2.  Linear Discrimination\n3.  Gaussian Naive Bayes\n4.  Decision Tree\n5.  Random Forest\n6.  Support Vector Machine\n7.  K Nearest Neighbor Model\n\n**Use round() on accuracy_test() to ensure readability and accuracy**","343b1546":"### Response Variable\nhttps:\/\/online.stat.psu.edu\/stat200\/lesson\/1\/1.1\/1.1.2\n\nAlso known as the dependent or outcome variable, its value is predicted or its variation is explained by the explanatory variable; in an experimental study, this is the outcome that is measured following manipulation of the explanatory variable","8ec877eb":"### Sweetviz\nhttps:\/\/pypi.org\/project\/sweetviz\/\n\nSweetviz is an open-source Python library that generates beautiful, high-density visualizations to kickstart EDA (Exploratory Data Analysis) with just two lines of code. Output is a fully self-contained HTML application.\n\nThe system is built around quickly visualizing target values and comparing datasets. Its goal is to help quick analysis of target characteristics, training vs testing data, and other such data characterization tasks.","7191c2c4":"# Pima Indians Diabetes Database\n## UCI Machine Learning\nBen Felder\n*September 2021*","e9af0d5d":"### Gaussian Naive Bayes\nhttps:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier","15297d24":"### Decision Tree\nhttps:\/\/en.wikipedia.org\/wiki\/Decision_tree_model","7311f03c":"### K-Nearest Neighbor\nhttps:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm","a02b93fc":"## Importing Dataset","8ccec75c":"## Importing Libraries","315527cf":"**Classification Rate of 70%**","bff9b96a":"## Visualization Packages","5383f601":"**AUC Score is 0.807, an AUC score of 1 represents *perfect* classifier, and 0.5 represents *worthless* classifier***","187a3372":"**Logistic Regression Model with 76.772% Accuracy is Best Model for Predicting**","a1acec8f":"### Receiver Operating Characteristic Curve\n\nReceiver Operating Characteristic (ROC) curve is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificity.","57b697dd":"### Linear Discrimination Analysis\nhttps:\/\/en.wikipedia.org\/wiki\/Linear_discriminant_analysis","d58d8f78":"### Random Forest\nhttps:\/\/en.wikipedia.org\/wiki\/Random_forest","efea92c3":"## Dataset\n**Dataset Name:** *Pima Indians Diabetes Database*  \n**Dataset Description:** *This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\nThe datasets consists of several medical predictor variables and one target variable, **Outcome**. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.*  \n**Dataset Source:** *Kaggle*  \n**Dataset Link:** https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database  \n***Acknowledgements***: *Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.*","dd229855":"### Observations\n\nAll variables have rows with *zero* data which skews the *min*\n\nPregnancies - average amount is just under 4, maximum of 17  \nGlucose - average amount is 120, maximum of 199 (CDC recommends 80-130mg\/dl pre-meal, <180mg\/dl post-meal)  \nBloodPressure - average amount is 69, maximum of 122 (CDC recommends 120 systolic and 80 diastolic or 120\/80mmHg) \n\n\nOutcome - only contains values between 0 and 1, we will use this as our *Output Variable*  \nOutcome - values of 1 indicates 'Diabetic', otherwise 0","aa3a77da":"### Support-Vector Machine\nhttps:\/\/en.wikipedia.org\/wiki\/Support-vector_machine","eb9f603b":"Confirming there are *0* null values","863b4629":"### Dtale\nhttps:\/\/pypi.org\/project\/dtale\/\n\nD-Tale is the combination of a Flask back-end and a React front-end to bring you an easy way to view & analyze Pandas data structures. It integrates seamlessly with ipython notebooks & python\/ipython terminals. Currently this tool supports such Pandas objects as DataFrame, Series, MultiIndex, DatetimeIndex & RangeIndex.","9dc320eb":"## Model Selection\n### Create Dataframe from Model Accuracy Scores","ca732dda":"### Lux\nhttps:\/\/github.com\/lux-org\/lux\n\nLux is a Python library that facilitate fast and easy data exploration by automating the visualization and data analysis process. By simply printing out a dataframe in a Jupyter notebook, Lux recommends a set of visualizations highlighting interesting trends and patterns in the dataset. Visualizations are displayed via an interactive widget that enables users to quickly browse through large collections of visualizations and make sense of their data.","c6b851c1":"## Data Exploration"}}