{"cell_type":{"4a5420d0":"code","ec15766d":"code","341e6204":"code","1ec18962":"code","f672f76a":"code","5ee5f7d6":"code","60f8f28e":"code","cbf3291f":"code","c6ef696a":"code","39768a98":"code","b1c35252":"markdown","134b10a9":"markdown","dfa454a2":"markdown","f098eea5":"markdown"},"source":{"4a5420d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom mlxtend.frequent_patterns import apriori, association_rules\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec15766d":"groceryData = pd.read_csv('..\/input\/supermarket\/GroceryStoreDataSet.csv',names=['Products'],header=None)","341e6204":"groceryData.head()","1ec18962":"groceryData.info()","f672f76a":"items=[]\nfor i in groceryData.values:\n    items.extend( i[0].split(\",\")  )\nitems=list(set(items))\nitems","5ee5f7d6":"df=pd.DataFrame(data=0,columns=items,index=range(len(groceryData)))\nfor i in df.columns:\n    df[i] = groceryData['Products'].str.contains(i)\ndf","60f8f28e":"df_freq = apriori(df, min_support = 0.1, use_colnames = True)\ndf_freq","cbf3291f":"association_rules(df_freq, metric = \"lift\", min_threshold = 1).sort_values(by=['antecedent support','confidence'],ascending=False).reset_index(drop=True).head(20)","c6ef696a":"df_freq['item_count'] = df_freq['itemsets'].apply(lambda x:len(x))","39768a98":"df_freq[(df_freq['item_count']==2) & (df_freq['support']>0.1)].sort_values(by='support',ascending=False)\n","b1c35252":"Bonus\n\nWe can also count the items and sort them based on the how frequently they have been sold as 2\/3\/4 items","134b10a9":"**Lets extract all unique items**","dfa454a2":"The data frame is ready now. Lets dive into exploring the association between items.\nWe will take 0.1 as min support value. That will eliminate any item has less than 0.1 support value.","f098eea5":"**Result**\n\n* We can see the item which is getting sold the most is in the \"antecedents\" column with its support value in the \"antecedent support\" column. So 1) BREAD 2) COFFEE 3) TEA are getting sold the most.\n* After getting the first item in the \"antecedent\"(with \"antecedent support\" frequency), in the \"consequents\" column we can see the item which are most probably will be sold together(with \"confidence\" value frequency)\n* \"confidence\" tells us how sure we are about selling the 2. item after selling the 1. item(that might(not) be the impact of first item , while \"lift\" tells  how selling the 1. item affect the proabability of selling the 2. item.\n\nSo based on the result some strategical actions can be taken to increase selling items. Also new products can be tried and after analysing the results, the best ones can be replaced with the items which are not getting sold well."}}