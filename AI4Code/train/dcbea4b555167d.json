{"cell_type":{"8becae79":"code","a8810fbd":"code","6f4436ed":"code","5bf6213d":"code","631e9857":"code","132e7902":"code","9fe2f3b7":"code","384329a4":"code","fc70c473":"code","5ba8d631":"markdown","0e282122":"markdown","8fae6b37":"markdown","6b095b50":"markdown","f7fc5296":"markdown","87e0dc51":"markdown","a592c234":"markdown","faa1f913":"markdown"},"source":{"8becae79":"!pip install vit_keras","a8810fbd":"import tensorflow as tf\nfrom tensorflow.keras import backend, optimizers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import GaussianNoise, Dense\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, LearningRateScheduler\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom vit_keras import vit","6f4436ed":"from tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\n\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)\n\ndef allocate_gpu_memory(gpu_number=0):\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n\n    if physical_devices:\n        try:\n            print(\"Found {} GPU(s)\".format(len(physical_devices)))\n            tf.config.set_visible_devices(physical_devices[gpu_number], 'GPU')\n            tf.config.experimental.set_memory_growth(physical_devices[gpu_number], True)\n            print(\"#{} GPU memory is allocated\".format(gpu_number))\n        except RuntimeError as e:\n            print(e)\n    else:\n        print(\"Not enough GPU hardware devices available\")\nallocate_gpu_memory()\nfrom tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","5bf6213d":"train_path = \"\/kaggle\/input\/100-bird-species\/train\"\nvalid_path = \"\/kaggle\/input\/100-bird-species\/valid\"\ntest_path = \"\/kaggle\/input\/100-bird-species\/test\"\n\n# Data augmentation\ntrain_datagen = ImageDataGenerator(\n    rescale=1\/255,\n    horizontal_flip=True,\n    rotation_range=15,\n    zoom_range=0.1,\n)\nvalid_datagen = ImageDataGenerator(rescale=1\/255)\ntest_datagen = ImageDataGenerator(rescale=1\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_path,\n    target_size=(224, 224),\n    batch_size=32,\n    color_mode='rgb',\n    class_mode='sparse',\n    shuffle=True,\n)\n\nvalidation_generator = valid_datagen.flow_from_directory(\n    valid_path,\n    target_size=(224, 224),\n    batch_size=32,\n    color_mode='rgb',\n    class_mode='sparse')\n\ntest_generator = test_datagen.flow_from_directory(\n    test_path,\n    target_size=(224, 224),\n    batch_size=32,\n    color_mode='rgb',\n    class_mode='sparse')\n\n\ndef plotImages(images_arr):\n    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip(images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n    \naugmented_images = [train_generator[0][0][0] for i in range(5)]\nplotImages(augmented_images)","631e9857":"backend.clear_session()\n\nvit_model = vit.vit_l32(\n    image_size=224,\n    pretrained=True,\n    include_top=False,\n    pretrained_top=False\n)\n\nprint(len(vit_model.layers))\nprint(vit_model.layers)","132e7902":"# Decay lr for each 7 epochs\ndef scheduler(epoch: int, lr: float) -> float:\n    if epoch != 0 and epoch % 7 == 0:\n        return lr * 0.1\n    else:\n        return lr\nlr_scheduler_callback = LearningRateScheduler(scheduler)","9fe2f3b7":"finetune_at = 28\n\n# fine-tuning\nfor layer in vit_model.layers[:finetune_at - 1]:\n    layer.trainable = False\n    \nnum_classes = len(validation_generator.class_indices)\n\n# Add GaussianNoise layer for robustness\nnoise = GaussianNoise(0.01, input_shape=(224, 224, 3))\n# Classification head\nhead = Dense(num_classes, activation=\"softmax\")\n\nmodel = Sequential()\nmodel.add(noise)\nmodel.add(vit_model)\nmodel.add(head)\n\nmodel.compile(optimizer=optimizers.Adam(),\n               loss=\"sparse_categorical_crossentropy\",\n               metrics=[\"accuracy\"])\n                      \nhistory = model.fit(\n          train_generator,\n          epochs=100,\n          validation_data=validation_generator,\n          verbose=1, \n          shuffle=True,\n          callbacks=[\n              EarlyStopping(monitor=\"val_accuracy\", patience=10, restore_best_weights=True),\n              lr_scheduler_callback,\n          ])","384329a4":"history_dict = history.history\nloss_values = history_dict[\"loss\"]\nval_loss_values = history_dict[\"val_loss\"]\nacc_values = history_dict[\"accuracy\"]\nval_acc_values = history_dict[\"val_accuracy\"]\nepochs = range(1, len(history_dict[\"accuracy\"]) + 1)\n\nplt.plot(epochs, loss_values, \"bo\", label=\"train\")\nplt.plot(epochs, val_loss_values, \"b\", label=\"valid\")\nplt.title(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\nplt.plot(epochs, acc_values, \"bo\", label=\"train\")\nplt.plot(epochs, val_acc_values, \"b\", label=\"valid\")\nplt.title(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\nimport numpy as np\nprint(\"best val_acc:\", np.max(val_acc_values), \"epoch:\", np.argmax(val_acc_values))\nprint(\"best val_loss:\", np.min(val_loss_values), \"epoch:\", np.argmin(val_loss_values))","fc70c473":"test_loss, test_acc = model.evaluate(test_generator)\nprint(\"Test Accuracy:\", test_acc)","5ba8d631":"# Build model and train model","0e282122":"# Install vit_keras","8fae6b37":"# GPU setting","6b095b50":"# testing","f7fc5296":"# Learning Scheduler","87e0dc51":"# Import modules","a592c234":"# Prepare generators","faa1f913":"# Load pre-trained ViT Model"}}