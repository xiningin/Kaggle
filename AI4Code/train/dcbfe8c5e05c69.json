{"cell_type":{"b96856ec":"code","8112b7cd":"code","daca8d1a":"code","399224e3":"code","11e33a93":"code","805c2a01":"code","4063b349":"code","8d338fd9":"code","aef269e7":"code","71f6766e":"code","166aab06":"code","6ddee451":"code","2a2e1376":"code","e4c61094":"code","ab0112b2":"code","520f4e76":"code","c2287cdf":"code","6ef060ca":"code","8aa44ad9":"code","af27ac44":"code","1102989b":"code","ded7849e":"code","cb4a2148":"code","7b945e74":"code","91646234":"code","0b61f858":"code","48a7b5cb":"code","f6196272":"code","ef25aa98":"code","00830030":"code","e7706869":"code","724fee77":"code","b85e3512":"code","06012f35":"code","265d0b61":"code","0a50d034":"code","b8b4dd29":"code","992501e3":"code","e4fd3015":"code","01ee305d":"markdown","f111bed3":"markdown","eb954078":"markdown","9e06e45f":"markdown","6b363d3e":"markdown","24b1d1c6":"markdown","13ca8fb2":"markdown","4d9f96f1":"markdown","b5f12a68":"markdown","e640dc59":"markdown","496f9bd1":"markdown","86605039":"markdown","e8e677bd":"markdown","8ca0c2e1":"markdown","16dae6db":"markdown","33e43025":"markdown","aceb4ee1":"markdown","2f52c577":"markdown","65a42f5b":"markdown","78db9ed3":"markdown","7044f60c":"markdown"},"source":{"b96856ec":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport datetime as dt\nimport folium\nfrom folium.plugins import HeatMap, HeatMapWithTime\n%matplotlib inline","8112b7cd":"#print(os.listdir('\/kaggle\/input'))\nDATA_FOLDER = \"\/kaggle\/input\/coronavirus-2019ncov\"\n#print(os.listdir(DATA_FOLDER))\n","daca8d1a":"data_df = pd.read_csv(os.path.join(DATA_FOLDER, \"covid-19-all.csv\"))\n#cn_geo_data = os.path.join(GEO_DATA, \"china.json\")\n#wd_geo_data = os.path.join(WD_GEO_DATA, \"world-countries.json\")","399224e3":"#print(f\"Rows: {data_df.shape[0]}, Columns: {data_df.shape[1]}\")\n#data_df.head()\n#data_df.tail()\n#print(f\"Date - unique values: {data_df['Date'].unique()}\")","11e33a93":"data_df['date'] = pd.to_datetime(data_df['Date'])\nimport datetime as dt\ndt_string = dt.datetime.now().strftime(\"%Y-%m-%d\")\nlast_update=data_df['date'].unique().max()\nprint(f\"Kernel last updated on {dt_string} based on data until {last_update}\")","805c2a01":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))\n#missing_data(data_df)","4063b349":"#print(f\"Countries\/Regions:\\n {data_df['Country\/Region'].unique()}\")\n#print(f\"Province\/State:\\n {data_df['Province\/State'].unique()}\")\n#print(f\"Last Update:\\n {data_df['Last Update'].unique()}\")\n#print(f\"Last Update:\\n {data_df['DateTime'].unique()}\")","8d338fd9":"data_cn = data_df.loc[data_df['Country\/Region']==\"Mainland China\"]\ndata_cn = data_cn.sort_values(by = ['Province\/State','date'], ascending=False)\ndata_not_cn=data_df.loc[~(data_df['Country\/Region']==\"Mainland China\")]\ndata_not_cn=data_not_cn.sort_values(by = ['Province\/State','date'], ascending=False)\ndata_countries = data_df.groupby(['Country\/Region','date']).sum().reset_index()\ndata_countries = data_countries.sort_values(by = ['date','Country\/Region'], ascending=True)\n#data_not_cn\nif False:\n    data_cn['Country\/Region']=data_cn['Province\/State']\n    data_countries=pd.concat([data_countries,data_cn])\n    data_countries = data_countries.sort_values(by = ['date','Country\/Region'], ascending=True)\ndata_countries\n","aef269e7":"#filtered_data_last = data_cn.drop_duplicates(subset = ['Province\/State'],keep='first')\n#filtered_data_last = data_not_cn.drop_duplicates(subset = ['Province\/State'],keep='first')","71f6766e":"def plot_count(feature, value, title, df, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    g = sns.barplot(df[feature], df[value],  palette='Set3')\n    g.set_title(\"Number of {}\".format(title))\n    ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n    plt.show()    \n#plot_count('Province\/State', 'Confirmed', 'Confirmed cases (last updated)', filtered_data_last, size=4)","166aab06":"def plot_time_variation(df, y='Confirmed', hue='Province\/State', size=1,title='Title', logscale=False):\n    f, ax = plt.subplots(1,1, figsize=(4*size,3*size))\n    #missing_data(df)\n    #print(y,hue,df)\n    g = sns.lineplot(x=\"date\", y=y, hue=hue, data=df)\n    plt.xticks(list(df['date'].unique()),rotation=90)\n    if logscale:\n        #plt.set_yscale('log')\n        plt.yscale('log')\n    #plt.xticks(rotation=90)\n    #ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n    #print(df[y].max())\n    try:\n        plt.yticks(np.arange(0, df[y].max(), step=2000))\n    except:\n        pass\n    plt.grid()\n    plt.title(f'{title}: {y} cases grouped by {hue}')\n    plt.show()  ","6ddee451":"#data_cn\n#plot_time_variation(data_not_cn, size=4)\nplot_time_variation(data_countries, hue='Country\/Region',size=4, title='Curve base')","2a2e1376":"#plot_time_variation(data_cn.loc[~(data_cn['Province\/State']=='Hubei')], size=4)","e4c61094":"def normalize(a):\n    return a\/a[-1]\n\ndef mse(a,b):\n    return np.sqrt(np.square(a-b).sum())\n\ndef mse_normalized(a,b):\n    return mse(normalize(a),normalize(b))\n\ndef mse_normalized_average(a,b):\n    return mse_normalized(a,b)\/overlap(a,b)\n\ndef mse_average(a,b):\n    return mse(a,b)\/overlap(a,b)\n\ndef mae(a,b):\n    return (np.abs(a-b).sum())\n\ndef mae_normalized(a,b):\n    return mae(normalize(a),normalize(b))\n\ndef hamming(a,b):\n    diff=a-b\n    return np.where(diff==0,0,1).sum()\n\ndef overlap(a,b):\n    diff=a*b\n    return np.where(diff==0,0,1).sum()\n\ndef get_min_dist(a,b,min_overlap=5,dist_func=mse): #e.g. length 10 and 20\n#    min_shift=-len(b)+min_overlap #\n    min_shift=len(a)-len(b)+1 #-9, so that there is at least one future value, which is b[len(a)+min_shift+1]=b[10+9+1]=b[20]\n    max_shift=len(a)-min_overlap #\n    #print(min_shift,max_shift)\n    shifts=list(range(min_shift,max_shift+1,1))\n    dists=np.ones(len(shifts))\n    for i,shift in enumerate(shifts):\n        a_shifted=a\n        b_shifted=b\n        if shift <= 0:\n            a_shifted=np.pad(a, (-shift, 0), 'constant', constant_values=(0))\n        else:\n            b_shifted=np.pad(b, (shift, 0), 'constant', constant_values=(0))\n        #print(a_shifted)\n        #print(b_shifted)\n        #print(b_shifted[0:len(a_shifted)])\n        dist=dist_func(a_shifted,b_shifted[0:len(a_shifted)])\n        #print(dist)\n        dists[i]=dist\n    return np.array([shifts,dists])\n\n#a=np.arange(10)\n#b=np.arange(5)\n#get_min_dist(a,b,min_overlap=3,dist_func=overlap)","ab0112b2":"min_length=10\nmin_overlap=5\n#country=\"Germany\"\ncountry=\"Spain\"\nnormalize_plots=True\nk=5\ndays_to_eval=5\n#temps=[0.0001,0.001,0.005,0.01,0.05,0.1,0.2,0.5,1,2,5,10,20,50,100,500,1000,10000,100000,1000000]\ntemps=[0.0001,0.001,0.01,0.1,1,10,100,1000,10000,100000,1000000]\n#too low values seem to be too extreme, since then the just do max, so that we introduce a min_prob factor, in relation to unit distribution (all equal probability)\nmin_prob_factor=0.1\n#print(\"compute predictions for \"+country)\ndist_function=mse_normalized_average\n#dist_function=mse_normalized\n#dist_function=mse_average\nplot_past_days=14\n","520f4e76":"def filter_regions(data,cat='Confirmed'):\n\n    tss={}\n    dates=data['date'].unique() #should already be sorted\n    regions=data['Country\/Region'].unique()\n    #print(dates)\n\n    for c in regions:\n        ts=data[(data['Country\/Region']==c)]\n        ts=ts[['date',cat]]\n        #filter out the first days where there are 0 cases\n        ts=ts[~(data[cat]==0)]\n        ts=ts[cat].to_numpy()\n        #ts=[data[(data['Country\/Region']==c) & (data['date']==date)]['Confirmed'] for date in dates]\n        #filter \n        if len(ts)>=min_length:\n            tss[c]=ts\n        else:\n            print(f\"remove {c} since only {len(ts)} days\")\n        #print(c,ts)\n    regions=list(tss.keys())\n    return regions, tss, dates\n\ndata=data_countries\nregions, tss, dates=filter_regions(data,cat='Confirmed')","c2287cdf":"# get curve as panda data frame\ndef get_index_df(a,shift,name,weight=1.0, pivot_date=None):\n    if pivot_date is None:\n        df=pd.DataFrame({'date':list(range(shift,shift+len(a))),'Number':a,'name':[name]*len(a),'weight':[weight]*len(a)})\n    else:\n        start_date=pivot_date+pd.Timedelta(days=shift)\n#        df=pd.DataFrame({'date':pd.date_range(start_date, periods=len(a), freq='D'),'Number':a,'name':[name]*len(a),'weight':[weight]*len(a)},dtype=['datetime64',\n#                                                                                                                                                     'float64',\n#                                                                                                                                                     'object',\n#                                                                                                                                                     'float64'])\n        #print(type(a))\n        df=pd.DataFrame({'date':pd.date_range(start_date, periods=len(a), freq='D'),'Number':a,'name':[name]*len(a),'weight':[weight]*len(a)})\n    df[\"Number\"] = pd.to_numeric(df[\"Number\"])\n    #print(missing_data(df))\n    return df\ndef get_first_last_date(country,df,cat='Confirmed'):\n    ts=df[(df['Country\/Region']==country)]\n    ts=ts[['date',cat]]\n    #filter out the first days where there are 0 cases\n    ts=ts[~(data[cat]==0)]\n    return (ts['date'].min(),ts['date'].max())\n    ","6ef060ca":"def get_all_nn(country,data,tss,days_to_cut_off,normalize_plots,dist_func, cat='Confirmed'):\n    do_normalize=normalize_plots\n    pivot_date,last_date=get_first_last_date(country,data,cat=cat)\n    viss=[]\n    dists=[]\n    shifts=[]\n    errors=[]\n    timeseries=[]\n    ts=tss[country]\n    ts=ts[0:len(ts)-days_to_cut_off]\n    for region in regions:\n        if country == region:\n            continue\n        dist=get_min_dist(ts,tss[region],dist_func=dist_func,min_overlap=min_overlap)\n        min_index=np.argmin(dist[1])\n        shift=dist[0,min_index]\n        error=dist[1,min_index]\n        ts_nn=tss[region]\n        days_ahead=len(ts_nn)-len(ts)+int(shift)\n        if do_normalize:\n            last_value_a=ts[-1]\n            #print(int(len(tss[country])-shift))\n            last_value_b=ts_nn[len(ts)-int(shift)-1]\n            ts_nn=ts_nn\/last_value_b*last_value_a\n        vis=get_index_df(ts_nn,int(shift),\"%s ahead=%s dist=%s\"%(region,int(days_ahead),error),pivot_date=pivot_date,weight=error)\n        viss.append(vis)\n        dists.append(dists)\n        shifts.append(shift)\n        errors.append(error)\n        timeseries.append(ts)\n    errors=np.array(errors)\n    return errors,shifts,dists,viss,timeseries\n\n#def get_k_nn(country,data,timeseries,k,normalize_plots,dist_func):\n#    errors,shifts,dists,viss=get_all_nn(country,data,timeseries,normalize_plots,dist_func)\ndef get_k_nn(k,errors,shifts,dists,viss,timeseries):\n    top_ind=errors.argsort()[:k]\n    errors=errors[top_ind]\n    dists=[dists[ind] for ind in top_ind]\n    shifts=[shifts[ind] for ind in top_ind]\n    viss=[viss[ind] for ind in top_ind]\n    timeseries=[timeseries[ind] for ind in top_ind]\n    return errors,shifts,dists,viss,timeseries\n\n#std_start_date=\ndef plot_nn_ts(viss,size=4,future_limit_in_days=5,last_date=None,start_date=pd.Timestamp('2020-03-01'),title='Plot'):\n    #print(viss[0])\n    #print(viss[2])\n    #for col in viss[0].columns:\n    #    print(viss[0][col].dtype)\n    #missing_data(viss[0])\n    #plot_time_variation(viss[0], y='Number',hue='name',size=4,title=title)\n    viss=pd.concat(viss)\n    if future_limit_in_days is not None:\n        viss=viss[(viss['date']<last_date+pd.Timedelta(days=future_limit_in_days))]\n    viss=viss[(viss['date']>=start_date)]\n    #print(viss)\n    plot_time_variation(viss, y='Number',hue='name',size=4,title=title)\n\nimport math\n    \ndef weighted_avg_and_std(values, weights):\n    \"\"\"\n    from https:\/\/stackoverflow.com\/questions\/2413522\/weighted-standard-deviation-in-numpy\n    Return the weighted average and standard deviation.\n\n    values, weights -- Numpy ndarrays with the same shape.\n    \"\"\"\n    average = np.average(values, weights=weights)\n    # Fast and numerically precise:\n    variance = np.average((values-average)**2, weights=weights)\n    return (average, math.sqrt(variance),weights)\n\ndef bounded_softmax(x, temperature):\n    p=softmax(x,temperature)\n    min_prob=min_prob_factor*1.0\/len(x)\n    p=p*(1.0-min_prob_factor)+min_prob\n    #p=p\/sum(p)\n    return p\n\ndef softmax(x,temperature):\n#    return np.exp(x\/temperature)\/sum(np.exp(x\/temperature))\n    x=x\/temperature\n    x=x-max(x)\n    return np.exp(x)\/sum(np.exp(x))\n\n#not used anymore\ndef predict(temperature,ts_length,errors,shifts,timeseries):\n    means=[]\n    stds=[]\n    futureday=0\n    i=0\n    for futureday in range(100):\n        tss=[(errors[i],timeseries[i][int(ts_length-shifts[i]+futureday)]) for i,ts in enumerate(timeseries) if len(timeseries[i])>ts_length-shifts[i]+futureday]\n        if len(tss)>0:\n            e,v=zip(*tss)\n            e=np.array(e)\n            v=np.array(v)\n            weights=softmax(e,temperature)\n            mean,std=weighted_avg_and_std(v,weights)\n            means.append(mean)\n            stds.append(std)\n        else:\n            break\n    return (means,stds,weights)\n\ndef predict2(country,temperature,viss,last_date_given):\n    #print(viss)\n    viss=pd.concat(viss)\n    viss=viss.reset_index(drop=True)\n    viss['date']=pd.to_datetime(viss['date'])\n    #print(viss)\n    max_date=viss['date'].max()\n    first_pred_date=last_date_given+pd.Timedelta(days=1)\n    prediction=[]\n    #print(first_pred_date)\n    #print(max_date)\n    for future_day in pd.date_range(start=first_pred_date, end=max_date, freq='D'):\n        viss_day=viss[(viss['date']==future_day)]\n        values=viss_day['Number'].to_numpy()\n        weights=viss_day['weight'].to_numpy()\n        normalized_weights=bounded_softmax(weights,temperature)\n        #print(future_day,temperature,values,weights,normalized_weights)\n        #print(viss_day)\n        #print(weights)\n        #print(normalized_weights)\n        prediction.append(weighted_avg_and_std(values,normalized_weights))\n        #print(viss_day,values,weights,mean,std)\n    prediction=np.array(prediction).T\n\n    viss=[]\n    viss.append(get_index_df(prediction[0]+prediction[1],0,country+' upper confidence bound',pivot_date=first_pred_date))\n    viss.append(get_index_df(prediction[0],0,country+' pred',pivot_date=first_pred_date))\n    viss.append(get_index_df(prediction[0]-prediction[1],0,country+' lower confidence bound',pivot_date=first_pred_date))\n    return viss,prediction\n\ndef eval_k_on_train(k,temperature,days_to_eval,loss_function,country,data,tss,normalize_plots,dist_function,log=False,cat='Confirmed'):\n    ts=tss[country]\n    pivot_date,last_date=get_first_last_date(country,data,cat=cat)\n    predictions=[]\n    if log:\n        print(\"date real pred  <=\"+country)\n    for days_to_cut_off in range(1,days_to_eval+1,1):\n        before_prediction_date=last_date-pd.Timedelta(days=days_to_cut_off)\n        errors,shifts,dists,viss,timeseries=get_k_nn(k,*get_all_nn(country,data,tss,days_to_cut_off,normalize_plots,dist_function))\n        _,prediction=predict2(country,temperature,viss,before_prediction_date)\n        predictions.append(prediction[0,0])\n        if log:\n            print(\"%s %s %s\"%(before_prediction_date+pd.Timedelta(days=1),ts[-days_to_cut_off],prediction[0,0]))\n        \n    predictions=np.array(predictions)\n    return loss_function(np.array(ts[-days_to_eval:]),predictions)\n","8aa44ad9":"\n\n\n#for temperature in temps:\n#    eval_k_on_train(k,temperature,days_to_eval,mse,country,data,tss,normalize_plots,mse)\nlosses=[eval_k_on_train(k,temperature,days_to_eval,mse,country,data,tss,normalize_plots,dist_function,True) for temperature in temps]\nprint(\"Losses for different temperature (choose min): %s\"%losses)\n#min_index = losses.index(min(losses)) \nmin_index = np.nanargmin(np.array(losses)) \ntemperature=temps[min_index]\nprint(\"chosen temperature %s with loss %s\"%(temperature,losses[min_index]))\n\n","af27ac44":"\nts=tss[country]\npivot_date,last_date=get_first_last_date(country,data,cat='Confirmed')\n#errors,shifts,dists,viss,timeseries=get_all_nn(country,data,tss,normalize_plots,mse)\nerrors,shifts,dists,viss,timeseries=get_k_nn(k,*get_all_nn(country,data,tss,0,normalize_plots,dist_function))\n\n#print(viss)\n\ncountry_vis=get_index_df(ts,0,country,pivot_date=pivot_date)\n\n\npred,pred_series=predict2(country,temperature,viss,last_date)\nprint(\"weights of regions for mixing (weights for linear combination) for first future day (using temperature %s):\\n %s\"%(temperature,pred_series[2,0]))\nplot_nn_ts(viss+[country_vis]+[pred[1]],size=4,last_date=last_date, title=f'Prediction for {country} based on {k} neighbors', start_date=last_date-pd.Timedelta(days=plot_past_days))\n#print(pred_series)\n#viss=pred\n#viss=viss+pred","1102989b":"plot_nn_ts([country_vis]+pred,size=4,last_date=last_date, title=f'Prediction for {country} based on {k} neighbors', start_date=last_date-pd.Timedelta(days=plot_past_days))\n","ded7849e":"table=pred[1]\ntable['lower confidence bound']=pred[2]['Number']\ntable['upper confidence bound']=pred[0]['Number']\ntable.head(10)","cb4a2148":"def predict_and_plot(country,cat='Confirmed',dist_function=mse_normalized_average):\n    #for temperature in temps:\n    #    eval_k_on_train(k,temperature,days_to_eval,mse,country,data,tss,normalize_plots,mse)\n    losses=[eval_k_on_train(k,temperature,days_to_eval,mse,country,data,tss,normalize_plots,dist_function) for temperature in temps]\n    print(losses)\n    #min_index = losses.index(min(losses)) \n    min_index = np.nanargmin(np.array(losses)) \n    temperature=temps[min_index]\n    #print(\"chosen temperature %s with loss %s\"%(temperature,losses[min_index]))\n\n    ts=tss[country]\n    pivot_date,last_date=get_first_last_date(country,data,cat=cat)\n    #print(\"dfadsf\",pivot_date,last_date)\n    #errors,shifts,dists,viss,timeseries=get_all_nn(country,data,tss,normalize_plots,mse)\n    errors,shifts,dists,viss,timeseries=get_k_nn(k,*get_all_nn(country,data,tss,0,normalize_plots,dist_function))\n\n    country_vis=get_index_df(ts,0,country,pivot_date=pivot_date)\n\n\n    pred,pred_series=predict2(country,temperature,viss,last_date)\n    print(\"weights of regions for mixing (weights for linear combination) for first future day (using temperature %s):\\n %s\"%(temperature,pred_series[2,0]))\n    plot_nn_ts(viss+[country_vis]+[pred[1]],size=4,last_date=last_date, title=f'Prediction for {country} based on {k} neighbors', start_date=last_date-pd.Timedelta(days=plot_past_days))\n\n    plot_nn_ts([country_vis]+pred,size=4,last_date=last_date, title=f'Prediction for {country} based on {k} neighbors', start_date=last_date-pd.Timedelta(days=plot_past_days))\n\n    table=pred[1]\n    table['lower confidence bound']=pred[2]['Number']\n    table['upper confidence bound']=pred[0]['Number']\n    return table.head(10)","7b945e74":"predict_and_plot('Germany')","91646234":"predict_and_plot('France')","0b61f858":"predict_and_plot('US')","48a7b5cb":"predict_and_plot('Austria')","f6196272":"try:\n    predict_and_plot('UK')\nexcept:\n    pass\ntry:\n    predict_and_plot('United Kingdom')\nexcept:\n    pass","ef25aa98":"predict_and_plot('Spain',dist_function=mse_normalized_average)\n\n","00830030":"predict_and_plot('Germany',dist_function=mse_average)\n","e7706869":"predict_and_plot('France',dist_function=mse_average)\n","724fee77":"predict_and_plot('US',dist_function=mse_average)","b85e3512":"predict_and_plot('Austria',dist_function=mse_average)","06012f35":"try:\n    predict_and_plot('UK',dist_function=mse_average)\nexcept:\n    pass\ntry:\n    predict_and_plot('United Kingdom',dist_function=mse_average)\nexcept:\n    pass","265d0b61":"plot_time_variation(data_countries, y='Deaths', hue='Country\/Region',size=4, title='Curve base')","0a50d034":"min_length=7\nmin_overlap=3\nregions, tss, dates=filter_regions(data,cat='Deaths')\npredict_and_plot('Spain',cat='Deaths',dist_function=mse_normalized_average)","b8b4dd29":"predict_and_plot('Germany',cat='Deaths',dist_function=mse_normalized_average)","992501e3":"predict_and_plot('France',cat='Deaths',dist_function=mse_normalized_average)","e4fd3015":"predict_and_plot('US',cat='Deaths',dist_function=mse_normalized_average)","01ee305d":"## Filter out curves which are too short","f111bed3":"# Parameters (set them here)\n* min_length: minimum number of days for a curve in the base\n* min_overlap: number of days the curves have to overlap\n* country: country for which to predict\n* normalize_plots: shrink other curves when plotting\n* k: number of neighbors to consider\n* days_to_eval: past days to consider for parameter tuning\n* temps: the temperatures for the softmax function which are tried in the parameter tuning\n* min_prob_factor: the minimum probability for neighbors in relation to their apriori prob\n* dist_function: the distance function to compare the curves\n* plot_past_days: the days to plot into the past\n","eb954078":"# Forecast for France","9e06e45f":"Functions for determining the nearest neighbors, plotting, predicting","6b363d3e":"## Forecast for Spain, showing only Spain\nWith confidence intervals (but not very reliable, since at max based on k samples).","24b1d1c6":"# Forecast for Austria","13ca8fb2":"# Forecast for USA","4d9f96f1":"# Forecast for United Kingdom","b5f12a68":"# Forecast for Germany","e640dc59":"\n## Load packages","496f9bd1":"## Data loading\n* separate data from China and integrate as a single country\n* todo: integrate as separate regions into the base","86605039":"## Functions to compute similarity between curves","e8e677bd":"# Interpretable forecasting the number of confirmed cases of Covid-19 based on count curves of other regions\n\nBased on the curves of confirmed cases of regions all over the world, this analysis tries to predict the development of the counts for a particular region. This is done by comparing the curves to each other and identifying the curves which are most similar to the curve at hand. From a machine learning point of view, this corresponds to the k-nearest neighbors (k-NN) approach (case-based reasoning). The _other_ curves are shifted and scaled to best fit the target curve so that basically only the shape, in this case the growth, is compared.\n## Advantages and Disadvantages\nStatistical models of epidemics are useful since they allow to directly and explicitly model relationships and allow also predictions when there is only scarce data. On the other hand, assumptions may be wrong or simply too simple to model the reality. The adopted approach here in contrast uses real observations to estimate the future. Said differently, the assumptions used by k-NN are real cases.\n\nThe k-NN approach allows to understand the forecast more easily than other approaches. More specifically, the forecast is determined by a linear (weighted) combination of observed curves, so it is easily trackable how the prediction was constructed. Moreover, it is possible to modify the model by for example including or excluding, or changing the weight, of similar regions. This can be done if more information is available, for instance about counter-measures and their time points. On the other hand, this is also a limitation of the approach, since this additional information cannot be integrated currently into the model. \n\nAnother limitation is that no predictions can be effectively be done for some regions. For instance, there is no previous useful observations for mainland China. Or it would be difficult to estimate the future development for Italy since only a few countries are comparably longer in duration and these countries possibly performed different counter-measures and at different times.\n\nThe forecast, especially for time point many days in the future, are often dominated by the curves of China and Italy. Depending on the assumptions about the similarity to those two countries, this prediction can be considered too optimistic or too pessimistic.\n\n## Technical Details\nAveraged and normalized (with respecht to the last observed time point) mean squared error is used to compare the curves. Other measures such as mean absolute error can also be used. \n\nA parameter tuning is done for the weights of the neighbors. The base weights are the distances computed by the errors described above. These are then normalized by a soft-max function with a termperature, which is the only parameter which is tuned. This is tuned on some previous days.\n\n## Dataset and Base\nI use the dataset accessible via Kaggle. As base for the code I used the kernel by TBA.\n\n## Contribute\nCurrently, we only compare the time series based on the counts. In order to model an exponential growth, it could be useful to base the comparisons on the (percentage) growth from one day to the next. My experience with pandas is limited, so that could be a possibility for contributors.\n\nAlso, I was not able to integrate the different regions of China into the base of regions (regions in China can easily be compared to European countries from the size of their populations).\n\nThe plots are quite ugly. So is the code. You are welcome to contribute!\n","8ca0c2e1":"## Some code snippets in case something changes and I need them","16dae6db":"# Predictions with unnormalized mean squared error\nIn comparison to normalized, it considers more the similarity between the absolute counts, i.e., it considers countries with similar populations more.","33e43025":"## Forecasting for Spain in relation to used curves of other countries\nThe ***interpretation*** is the following: based on the most similar _k_ (=5) curves of other countries, the following curve results if we mix up the curves and assume that the curve of Spain will look like the one from country 1 to x1%, country 2 to x2%, etc. The x_i% percentages are given in the following and were computed to best fit in retrospective, i.e., the worked best in order to predict the previous days (based on the days previous to the previous days).","aceb4ee1":"## Do the parameter tuning","2f52c577":"## Forecasting table","65a42f5b":"## Curves which serve as base for the model","78db9ed3":"## Plots of curves which could additionally be used \nBut perhaps overdominating then?","7044f60c":"# Predictions for Deaths (preliminary)\nUsing normalized average MSE. It was also necessary to reduce the overlap"}}