{"cell_type":{"ca48d959":"code","c04e10aa":"code","b0c22888":"code","8c035665":"code","c0b3beca":"code","9826e8ba":"code","539f3642":"code","f5023ca4":"code","ee850e9e":"code","80b7f652":"code","f9eaabcb":"code","b7a24634":"code","a71c419c":"code","ba251ce1":"code","0f21c28f":"code","f209ca45":"code","3883cb01":"code","017a8737":"code","96c41dd9":"code","7a7794fb":"code","38d6bab3":"markdown","785872be":"markdown","423b8cbe":"markdown","c1f52192":"markdown","8a383fc6":"markdown","2478c2c7":"markdown","29e4a620":"markdown","e64cc5a3":"markdown"},"source":{"ca48d959":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","c04e10aa":"df = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/test.csv')\n\ndf.shape, test.shape","b0c22888":"df_all = df.append(test)","8c035665":"df_all.info()","c0b3beca":"df_all.head()","9826e8ba":"mapeamento = {'yes': 1, 'no': 0}\n\ndf_all['edjefa'] = df_all['edjefa'].replace(mapeamento).astype(int)\ndf_all['edjefe'] = df_all['edjefe'].replace(mapeamento).astype(int)\ndf_all['dependency'] = df_all['dependency'].replace(mapeamento).astype(float)\ndf_all['v2a1'].fillna(-1, inplace=True)\ndf_all['v18q1'].fillna(0, inplace=True)\ndf_all['SQBmeaned'].fillna(-1, inplace=True)\ndf_all['meaneduc'].fillna(-1, inplace=True)\ndf_all['rez_esc'].fillna(-1, inplace=True)\n\nfeats = [c for c in df_all.columns if c not in ['Id', 'idhogar', 'Target']]\n\ntrain, test = df_all[~df_all['Target'].isnull()], df_all[df_all['Target'].isnull()]\n","539f3642":"df_all.info()","f5023ca4":"# score de 0.43338\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(max_depth=None, random_state=43, n_jobs=5, n_estimators=707,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2, min_samples_split=5 ,\n                            verbose=0, class_weight='balanced')\n\nforest.fit(train[feats], train['Target'])\n\ntest['Target'] = forest.predict(test[feats]).astype(int)\n\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","ee850e9e":"import matplotlib.pyplot as plt\nfig=plt.figure(figsize=(15, 20))\n\nteste = pd.Series(forest.feature_importances_, index=feats).sort_values()\nteste.head(10).plot.barh()","80b7f652":"from sklearn.ensemble import RandomForestClassifier\n\nfeats2 = [c for c in list(dict(teste.head(10)).keys()) if c not in ['Id', 'idhogar', 'Target']]\nforest2 = RandomForestClassifier(max_depth=None, random_state=43, n_jobs=5, n_estimators=707,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2, min_samples_split=5 ,\n                            verbose=0, class_weight='balanced')","f9eaabcb":"forest.fit(train[feats2], train['Target'])\n\ntest['Target'] = forest.predict(test[feats2]).astype(int)\n\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","b7a24634":"teste = pd.Series(forest.feature_importances_, index=feats).sort_values()","a71c419c":"retirar = list(dict(teste.tail(10)).keys())","ba251ce1":"retirar.append('Id')\nretirar.append('idhogar')\nretirar.append('Target')","0f21c28f":"feats3 = [c for c in df_all.columns if c not in retirar]\ntrain, test = df_all[~df_all['Target'].isnull()], df_all[df_all['Target'].isnull()]","f209ca45":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(max_depth=None, random_state=43, n_jobs=5, n_estimators=707,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2, min_samples_split=5 ,\n                            verbose=0, class_weight='balanced')\n\nforest.fit(train[feats3], train['Target'])\n\ntest['Target'] = forest.predict(test[feats3]).astype(int)\n\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","3883cb01":"teste = pd.Series(forest.feature_importances_, index=feats).sort_values()","017a8737":"import random\n\nretirar = list()\n\ncolunas_escolhidas = list()\nwhile len(retirar) < 10:\n    \n    index_coluna = random.randint(0,140)\n    coluna = teste.index[index_coluna]\n    \n    if coluna not in retirar:\n        retirar.append(coluna)\n    else:\n        continue\n        \nretirar.append('Id')\nretirar.append('idhogar')\nretirar.append('Target')","96c41dd9":"feats4 = [c for c in df_all.columns if c not in retirar]\ntrain, test = df_all[~df_all['Target'].isnull()], df_all[df_all['Target'].isnull()]","7a7794fb":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(max_depth=None, random_state=43, n_jobs=5, n_estimators=707,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2, min_samples_split=5 ,\n                            verbose=0, class_weight='balanced')\n\nforest.fit(train[feats4], train['Target'])\n\ntest['Target'] = forest.predict(test[feats4]).astype(int)\n\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","38d6bab3":"# Teste numero 2","785872be":"agora iremos retirar 3 colunas aleatorias","423b8cbe":"obtive um score de 0.40732, caiu bastante mas nem perto da queda do primeiro teste","c1f52192":"# teste numero 1","8a383fc6":"reiniciando o projeto, agora vou retirar as 10 variaveis que menos teve importancia","2478c2c7":"tive uma queda absurda no score, de 0.43 caiu para 0.08791","29e4a620":"# teste 3","e64cc5a3":"## Verificando as 10 variaveis com mais importancia no primeiro teste"}}