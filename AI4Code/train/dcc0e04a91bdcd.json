{"cell_type":{"7fa2f11b":"code","409171aa":"code","98ecbf4e":"code","b30c503b":"code","6e0f220d":"code","41adace5":"code","5692b238":"code","5a920333":"code","5203e4c2":"code","e26c276f":"code","acd4213e":"code","73554571":"code","4ce9eb3c":"code","a2ae5b86":"code","5e76a9a6":"code","cbe1c04c":"code","c227e987":"code","a574071f":"code","e12dc870":"code","1db13853":"code","b5a8d670":"code","32764b14":"code","1479ecda":"code","12b29f99":"code","64454a6d":"code","1b6c4a72":"code","e2ced4f9":"code","311184ca":"code","06cb24fe":"markdown","59a90536":"markdown","181ea537":"markdown","c0b8ce57":"markdown","3d710db7":"markdown","ab6a06c1":"markdown","d647441c":"markdown","feecf05d":"markdown","bb8f7d63":"markdown","dd152104":"markdown","5a4fb715":"markdown","a4bd0c78":"markdown","8727d026":"markdown","6d8c38aa":"markdown","c24bac8f":"markdown","bf8c2aaf":"markdown","81e32410":"markdown","f60f19dd":"markdown","9b8861bb":"markdown","d27116b3":"markdown","56d560a7":"markdown","05d04cc9":"markdown","b29c13ed":"markdown"},"source":{"7fa2f11b":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torchvision import datasets, transforms, models \nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import random_split\nfrom torch.utils.data.dataloader import DataLoader\nimport matplotlib.pyplot as plt\n%matplotlib inline","409171aa":"data_dir = '..\/input\/flowers-recognition\/flowers'\nName0 = os.listdir(data_dir)\nName=sorted(Name0)\nN=list(range(len(Name)))    \nnormal_mapping=dict(zip(Name,N)) \nreverse_mapping=dict(zip(N,Name)) \nprint(Name)\nprint(len(Name))","98ecbf4e":"dataset=[]\nfor i in tqdm(range(len(Name))):\n    path=os.path.join(data_dir,Name[i])\n    for im in os.listdir(path):          \n        labeli=normal_mapping[Name[i]]\n        img1=cv2.imread(os.path.join(path,im))\n        img2=cv2.resize(img1,dsize=(64,64),interpolation=cv2.INTER_CUBIC)\n        img3=img2.astype(np.float32)\n        image=torch.from_numpy(img3).permute(2,0,1) ###\n        dataset+=[[image,labeli]]","b30c503b":"torch.manual_seed(20)\nrandom.seed(2021)\n\nval_size = len(dataset)\/\/10\ntest_size = len(dataset)\/\/5\ntrain_size = len(dataset) - val_size - test_size\ntrain_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n\nbatch_size = 64\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size, num_workers=2, pin_memory=True)\n\nlen(train_ds), len(val_ds), len(test_ds) ","6e0f220d":"M=list(range(len(dataset)))\nrandom.shuffle(M)\nfig, axs = plt.subplots(4,4,figsize=(12,12))\nfor i in range(16):\n    r=i\/\/4\n    c=i%4\n    img, label = dataset[M[i]]\n    img2=img.permute(1,2,0).numpy().astype(int)\n    ax=axs[r][c].axis(\"off\")\n    ax=axs[r][c].set_title(reverse_mapping[label])\n    ax=axs[r][c].imshow(img2)\nplt.show()","41adace5":"!rm -rf mixers\n!git clone https:\/\/github.com\/d9w\/mixers.git\n!mv mixers\/* .","5692b238":"!pip install einops","5a920333":"from model.mlp_mixer import MLPMixer\nfrom model.convmixer import ConvMixer","5203e4c2":"# Global settings\nhidden = 512\npatch = 8\ndepth = 8\n# MLPMixer settings\ntokens = 256\nchannels = 2048\n# ConvMixer settings\nkernel = 8","e26c276f":"mlp = True\nif mlp:  \n    model = MLPMixer(num_classes=10, image_size=64, patch_size=patch, hidden_dim=hidden, tokens_hidden_dim=tokens, channels_hidden_dim=channels, num_layers=depth)\n    model.load_state_dict(torch.load(\"checkpoints\/mlpmixer.model\"))\nelse:\n    model = ConvMixer(num_classes=10, hidden_dim=hidden, patch_size=patch, kernel_size=kernel, depth=depth)\n    model.load_state_dict(torch.load(\"checkpoints\/convmixer.model\"))","acd4213e":"from model.mlp_mixer import Classifier\n\nflowers_classifier = Classifier(hidden, 5)\nmodel.classifier = flowers_classifier","73554571":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","4ce9eb3c":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","a2ae5b86":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    val_loss = []\n    val_acc = []\n    for batch in val_loader:\n        images, labels = (b.to(device) for b in batch) \n        out = model(images) # Generate predictions\n        val_loss.append(F.cross_entropy(out, labels).detach())  # Calculate loss\n        val_acc.append(accuracy(out, labels))\n    epoch_loss = torch.stack(val_loss).mean()   # Combine losses\n    epoch_acc = torch.stack(val_acc).mean()      # Combine accuracies\n    return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}","5e76a9a6":"evaluate(model, val_loader)","cbe1c04c":"optimizer = torch.optim.AdamW(model.parameters(), 1e-3)","c227e987":"for epoch in range(20):\n    model.train()\n    train_losses = []\n    for batch in tqdm(train_loader):\n        images, labels = (b.to(device) for b in batch)\n        out = model(images)\n        loss = F.cross_entropy(out, labels)\n        train_losses.append(loss)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    result = evaluate(model, val_loader)\n    result['train_loss'] = torch.stack(train_losses).mean().item()\n    print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n        epoch, result['train_loss'], result['val_loss'], result['val_acc']))","a574071f":"import copy\n\ndef total_params(model):\n    return sum((p.numel()-torch.sum(p==0)).cpu().numpy() for p in model.parameters() if p.requires_grad)\n\nfull_model = copy.deepcopy(model)\nall_params = total_params(full_model)\nprint(all_params)","e12dc870":"for i in model.named_parameters():\n    print(i[0], i[1].numel())","1db13853":"layer = model.layers[7].channel_mixing[1].model[0]\nlayer","b5a8d670":"import torch.nn.utils.prune as prune\nprune.random_unstructured(layer, name=\"weight\", amount=0.3)","32764b14":"new_total = total_params(model)\nprint(new_total, all_params, new_total\/all_params)","1479ecda":"prune.remove(layer, \"weight\")","12b29f99":"new_total = total_params(model)\nprint(new_total, all_params, new_total\/all_params)","64454a6d":"parameters_to_prune = ([(model.layers[i].channel_mixing[1].model[0], 'weight') for i in range(8)]+\n                       [(model.layers[i].channel_mixing[1].model[2], 'weight') for i in range(8)])\nprint(parameters_to_prune)","1b6c4a72":"prune.global_unstructured(\n    parameters_to_prune,\n    pruning_method=prune.RandomUnstructured,\n    amount=0.3,\n)\nfor layer, name in parameters_to_prune:\n    prune.remove(layer, name)\n\nnew_total = total_params(model)\nprint(new_total, all_params, new_total\/all_params)","e2ced4f9":"%%time\nevaluate(full_model, val_loader)","311184ca":"%%time\nevaluate(model, val_loader)","06cb24fe":"## Models","59a90536":"We will use medium-sized networks trained on CIFAR10 and fine-tune them for a flower identification task. Pretraining is a common practice with large vision models, as training on a general database can condition the network to better learn a specific task. We will start by loading and visualizing the data.","181ea537":"We can now see a decrease in the total number of weights. It isn't large, but we only pruned a single layer. We can apply this method or use [`global_unstructured`](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.utils.prune.global_unstructured.html#torch.nn.utils.prune.global_unstructured) pruning to remove more weights. For example, we can prune all channel mixing layers like this:","c0b8ce57":"As expected, this model performs close to random ($1\/5$). We'll now train the model to improve its performance. This step is known as \"fine-tuning\" and can be further optimized through the inclusion of dropout, a different learning rate schedule, or other optimizer hyperparameters.","3d710db7":"When we count the number of parameters in the model, we get the same number as before. The weights haven't been removed, they're simply masked. To remove them, do the following:","ab6a06c1":"Your task is to reduce the number of parameters and the computational time of the model while maintaining the accuracy of the fine-tuned model. The following directions may be helpful:\n\n+ pruning in more layers\n+ informed unstructured pruning ([L1Unstructured](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured))\n+ using the ConvMixer model\n+ structured pruning ([LnStructured](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured), [Torch-Pruning](https:\/\/github.com\/VainF\/Torch-Pruning))\n+ further fine-tuning the pruned model\n\nYou will be scored based on the final model size, computational time, and accuracy compared to the fine-tuned model. As this is a multi-objective criteria, there may not be one winning team but multiple. Good luck!","d647441c":"Let's see the accuracy of this model. While it was pretrained, the final layer is newly initialized, so we would expect it to perform no better than random without any training.","feecf05d":"In the MLP-Mixer, we can note that the `channel_mixing` layers have many more parameters than any other layer type. We'll start by pruning one of these layers.","bb8f7d63":"The final layer of the network needs to be replaced as CIFAR10 has 10 classes and this flowers dataset only has 5.","dd152104":"We will be looking at pruning two very recent architectures called \"Mixers\" for pruning. These architectures split an image up into patches and then mix information between features of the same patch and between different patches. \n\n<img src=\"https:\/\/github.com\/isaaccorley\/mlp-mixer-pytorch\/raw\/main\/assets\/architecture.png\" width=\"50%\" height=\"auto\">","5a4fb715":"Trained neural networks have millions or more parameters, many of which are redundant. Removing parameters can speed up a network with little or no cost in performance. This process, known as [pruning](https:\/\/pytorch.org\/tutorials\/intermediate\/pruning_tutorial.html), uses information about the parameters, such as weights which are small, to remove redundant or unnecessary parameters. Pruning can be unstructured, removing individual weights, or structured, removing sections of the network by exploiting its structure, such as in [this library](https:\/\/github.com\/VainF\/Torch-Pruning).\n\n<img src=\"https:\/\/github.com\/VainF\/Torch-Pruning\/raw\/master\/assets\/residual.png\" width=\"50%\" height=\"auto\">","a4bd0c78":"The [MLP-Mixer](https:\/\/github.com\/d9w\/mixers\/blob\/main\/model\/mlp_.py) and [ConvMixer](https:\/\/github.com\/d9w\/mixers\/blob\/main\/model\/convmixer.py) definitions are from [this repository](https:\/\/github.com\/d9w\/mixers\/) which includes models pretrained on CIFAR10. We will clone this repository to use the definitions and pretrained models.","8727d026":"While we did reduce the number of parameters, the evaluation time didn't decrease. Unstructured pruning can make models more compact, but it only increases effiency on hardware specialized for sparse matrices. Structured pruning, however, can decrease the computational time by removing components from the network. The accuracy of our pruned model was also lower than the original, but the pruning we did was random. Using informed pruning can help remove weights without impacting the performance.","6d8c38aa":"Specifically, we'll be looking at [MLP-Mixer](https:\/\/arxiv.org\/pdf\/2105.01601.pdf) and [ConvMixer](https:\/\/openreview.net\/pdf?id=TVHS5Y4dNvM). Note that these architectures are very recent, with ConvMixer still under review, so their performance might not match that of other networks, such as ResNet or InceptionNet. These architectures differ from those and other contemporary architectures because they don't compress information using pooling layers; they are called \"isotropic\" as they maintain an equal size and shape throughout the network. The goal for today is to understand how pruning affects these isotropic architectures.","c24bac8f":"# Finetuning and Pruning Mixer Architectures for Flower Classification","bf8c2aaf":"The following parameters are set for the two architectures and need to stay the same to use the pretrained weights.","81e32410":"Now that we have a trained, fine-tuned model, let's try to slim it down. We'll focus on two metrics: the number of parameters and the time evaluation takes. We'll use the `torch.nn.utils.prune` module ([docs](https:\/\/pytorch.org\/docs\/stable\/nn.html)) which includes structured and unstructured pruning, but you can also use [Torch-Pruning](https:\/\/github.com\/VainF\/Torch-Pruning) for more structured pruning. We'll start with a simple example of unstructured pruning. A more complete tutorial on pruning is [here](https:\/\/pytorch.org\/tutorials\/intermediate\/pruning_tutorial.html).","f60f19dd":"## Dataset - Flower identification","9b8861bb":"The models are defined in `model\/mlp_mixer.py` and `model\/convmixer.py`, which are worth reading.","d27116b3":"We count all parameters which require a gradient and which are non-zero. One way to reduce the load of networks is to make them sparse, ie replace some parameters with 0 values. This can reduce model storage as sparse matrices can be more efficiently stored than dense matrices.\n\nIn order to prune the model, let's observe the size of each layer in the network. Note that this network has 8 blocks but each block contains multiple layers which can be pruned individually.","56d560a7":"This method randomly removes 30% of the weights of this layer by applying a binary mask on top of the layer. Note that we can do better than random weights by using values like the [l1 norm](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.utils.prune.l1_unstructured.html#torch.nn.utils.prune.l1_unstructured).","05d04cc9":"## Pruning Mixers","b29c13ed":"This notebook is a part of the ISAE-Supaero [SDD hackathon](https:\/\/supaerodatascience.github.io\/hackathon.html) of January 2022."}}