{"cell_type":{"c2590303":"code","8db2995b":"code","22e8b6a3":"code","b8ebe0e0":"code","774dacd9":"code","ac84ef7c":"code","c543a933":"code","e53e02de":"code","7cd298d9":"code","12022082":"code","c6c986fb":"markdown","321dc8a4":"markdown","166ad17e":"markdown","89c104a9":"markdown","2980df58":"markdown","64661786":"markdown","0cad5c5f":"markdown","efb8963f":"markdown","e74adf2e":"markdown","6121e302":"markdown","2d8b679b":"markdown"},"source":{"c2590303":"import pandas as pd\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, \\\n    classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\nimport itertools\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.model_selection import cross_val_score\nimport seaborn as sn\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid\nimport time\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom catboost import CatBoostClassifier, Pool\nfrom tqdm import tqdm\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, \\\n    ExtraTreesClassifier, VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.externals import joblib","8db2995b":"data_train = pd.read_csv('..\/input\/defcon-dataset\/train.csv')\ndata_test = pd.read_csv('..\/input\/defcon-dataset\/train.csv')\nprint (data_train.head(5))","22e8b6a3":"print(data_train.DEFCON_Level.value_counts())","b8ebe0e0":"# concat train and test\ndata = pd.concat([data_train, data_test], axis=0)\n# drop 'id' column\ndata = data.drop(columns=['ID'])\n# Retrieve data from the train set\nX = data[:10000].drop(columns=['DEFCON_Level'])\n# Retrieve label from train set\ny = data[:10000].DEFCON_Level\n# Retrieve data from test set\nX_test = data[10000:].drop(columns=['DEFCON_Level'])\n# split train file into train set (0.75) and valid set (0.25). The valid set is used to evaluate the model on the train set\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)","774dacd9":"def feature_importances():\n    for c in data.columns[data.dtypes == 'object']:\n        X_train[c] = X_train[c].factorize()[0]\n    rf = RandomForestClassifier()\n    rf.fit(X_train, y_train)\n    plt.plot(rf.feature_importances_)\n    plt.xticks(np.arange(X_train.shape[1]), X_train.columns.tolist(), rotation=90)\n    plt.show()\n    \nfeature_importances()","ac84ef7c":"corr = X.corr()\ncorr.style.background_gradient(cmap='coolwarm')","c543a933":"def feature_extract():\n    X.Closest_Threat_Distance = X.Closest_Threat_Distance \/ 1.06\n    X.Troops_Mobilized = X.Troops_Mobilized \/ 100\n    values = X.Troops_Mobilized.values.tolist()\n    for idx in range(len(values)):\n        if \".3\" in str(values[idx]):\n            values[idx] = round(values[idx] * 3) \/ 44\n        elif \".6\" in str(values[idx]):\n            values[idx] = round(values[idx] * 3 \/ 2) \/ 44\n    X.Troops_Mobilized = values\n    X.Percent_Of_Forces_Mobilized = X.Percent_Of_Forces_Mobilized \/ 0.01\n    X.Closest_Threat_Distance = X.Closest_Threat_Distance \/ 1.06\nfeature_extract()","e53e02de":"def compare_tree_based_models():\n    random_state = 2\n    classifiers = [SVC(random_state=random_state), DecisionTreeClassifier(random_state=random_state),\n                   AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state), random_state=random_state,\n                                      learning_rate=0.1), RandomForestClassifier(random_state=random_state),\n                   ExtraTreesClassifier(random_state=random_state),\n                   GradientBoostingClassifier(random_state=random_state), MLPClassifier(random_state=random_state),\n                   KNeighborsClassifier(), LogisticRegression(random_state=random_state), LinearDiscriminantAnalysis()]\n\n    cv_results = []\n    for classifier in classifiers:\n        cv_results.append(cross_val_score(classifier, X_train, y=y_train, scoring=\"accuracy\", cv=4, n_jobs=4))\n\n    cv_means = []\n    cv_std = []\n    for cv_result in cv_results:\n        cv_means.append(cv_result.mean())\n        cv_std.append(cv_result.std())\n\n    cv_res = pd.DataFrame(\n        {\"CrossValMeans\": cv_means, \"CrossValerrors\": cv_std, \"Algorithm\": [\"SVC\", \"DecisionTree\", \"AdaBoost\",\n                                                                            \"RandomForest\", \"ExtraTrees\",\n                                                                            \"GradientBoosting\",\n                                                                            \"MultipleLayerPerceptron\", \"KNeighboors\",\n                                                                            \"LogisticRegression\",\n                                                                            \"LinearDiscriminantAnalysis\"]})\n\n    g = sns.barplot(\"CrossValMeans\", \"Algorithm\", data=cv_res, palette=\"Set3\", orient=\"h\", **{'xerr': cv_std})\n    g.set_xlabel(\"Mean Accuracy\")\n    g = g.set_title(\"Cross validation scores\")\n    plt.show()\n\ncompare_tree_based_models()","7cd298d9":"scaler_minmax = MinMaxScaler()\nscaler_standard = StandardScaler()\n# params for decision tree classifier\ndt_param = {\"base_estimator__criterion\": [\"gini\", \"entropy\"],\n            \"base_estimator__max_depth\": list(range(2, 32, 1)),\n            'base_estimator__random_state': [21],\n            'base_estimator__max_features': [10],\n            \"base_estimator__min_samples_leaf\": list(range(5, 20, 5)),\n            'base_estimator__min_samples_split': list(range(5, 20, 5)),\n            'base_estimator__min_impurity_decrease': [0.00005, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01,\n                                                      0.05, 0.1]}\nmax_score = 0\n# use Kfold\nsss = StratifiedKFold(n_splits=4, random_state=None, shuffle=False)\nfor train_index, val_index in sss.split(X, y):\n    start_time = time.time()\n    print(\"Train:\", train_index, \"Val:\", val_index)\n    X_train, X_valid = X.iloc[train_index], X.iloc[val_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[val_index]\n    # standardized data\n    X_train = scaler_standard.fit_transform(X_train)\n    X_valid = scaler_standard.fit_transform(X_valid)\n    # Use BaggingClassifier \n    model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), oob_score=True, random_state=42)\n    # Use GridSearchCV\n    model = GridSearchCV(estimator=model, param_grid=dt_param, cv=4, verbose=3)\n    model.fit(X_train, y_train)\n    print(model.best_score_, model.best_params_)\n    # # tree best estimator\n    model = model.best_estimator_\n    # Save the best params after training\n    joblib.dump(model, 'DecisionTreeClassifier.pkl')\n    # Evaluate the model with a valid set\n    tree_score = cross_val_score(model, X_valid, y_valid, cv=4)\n    print('Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')\n    print(\"time execution :\", time.time() - start_time)\n","12022082":"def write_submission(md):\n    sample_submission = pd.DataFrame()\n    sample_submission['ID'] = data_test.ID\n    sample_submission['DEFCON_Level'] = md.predict(StandardScaler().fit_transform(X_test))\n    sample_submission.to_csv('sample_submission.csv', index=False)\n    print('write sample_submission completed')\n    \n# Load model with best params\nmodel = joblib.load('DecisionTreeClassifier.pkl')\nwrite_submission(model)","c6c986fb":"Visualize data to identify important features","321dc8a4":"After the training, we have found the best params just reload the model and create the sample_submission.csv.","166ad17e":"I will use the decision tree, which model I am most confident in, I will use a combination of Kfold, GridSearchCV and BaggingClassifier to choose the best parameters for this data.","89c104a9":"Visualize correlation matrix, this stage is very important. A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses.","2980df58":"Concat train and test. Drop \"ID\" column and split train data","64661786":"![LeaderBoard](http:\/\/https:\/\/imgur.com\/a\/M9lbxIJ)\nFor now, just using the best Decision and params, I'm standing # 5, you can stuff other algorithms, the contest is still going till March 20.","0cad5c5f":"Compare tree based models to select good model models for this data set.\nThe models I use below are : SVC, DecisionTreeClassifier, AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, MLPClassifier, KNeighborsClassifier, LogisticRegression, LinearDiscriminantAnalysis","efb8963f":"Read data from challenge (train and test dataset)","e74adf2e":"We will preprocess two important fields in the data: Closest_Threat_Distance, Troops_Mobilized","6121e302":"import the necessary libraries","2d8b679b":"Check the number of labels on the train set."}}