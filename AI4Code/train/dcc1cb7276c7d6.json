{"cell_type":{"d7ac7dbd":"code","ef23329c":"code","50160fe9":"code","1db0ac8a":"code","643cc718":"code","50d5bc18":"code","fe281b99":"code","c1574575":"code","2b39e5d9":"code","9e45d6b5":"code","2dd9d196":"code","58b55271":"code","fd8332c5":"code","6999280b":"code","7769cc1d":"code","02ce9979":"code","6db4d7c8":"code","6d57e0e9":"code","fa783da2":"code","2952d34d":"code","30042646":"code","6544f652":"code","35d8ad47":"code","eeee3119":"code","346906f7":"code","92ccb93c":"code","63b1789e":"code","b0683f09":"code","84a3d214":"code","d0e41638":"code","d3468e44":"code","c67156ea":"code","babb28a5":"code","65f2d23d":"code","6054c94e":"code","7e3366f9":"code","b9f49947":"code","73b58a45":"code","8263e8fb":"code","1d4b5bd0":"code","f8c63716":"code","781ffcf8":"code","c281b1a3":"code","65acef8a":"code","d30d1f85":"code","896c809a":"code","96bd76f9":"code","b0b6f0fc":"code","6c7a9b7e":"code","7a0722f5":"code","5886c32d":"code","5af1273d":"markdown","b7c26397":"markdown"},"source":{"d7ac7dbd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef23329c":"import matplotlib.pyplot as plt      # for ploting graphs\nimport seaborn as sns      # To plot heatmaps","50160fe9":"# Reading data from csv file with pandas\ndf = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf.head()","1db0ac8a":"# Cheaking the datatypes and other info\ndf.info()","643cc718":"# Plotting correlation matric on sns heatmap\nsns.heatmap(df.corr() , annot = True)","50d5bc18":"# Fining the survical rate stats\ndf[\"Survived\"].value_counts()","fe281b99":"# Since Name , PassengerId are unique for each data sample , we can drop these cells as they wont contribute for model.\ndf.drop([\"Name\" , \"PassengerId\"] , axis = 1 , inplace = True)","c1574575":"# Grouping and finding the corresponding dependencies\ndef grouping(df):\n    for title in df.columns:\n        res = df[\"Survived\"].groupby(df[title]).mean()\n        print(f\"Comparision of {title} with Survival rate is \")\n        print(res)\n        print(\"\\n\")\n\ngrouping(df)","2b39e5d9":"# Finding missing values\ndf.isna().sum()","9e45d6b5":"df[\"Embarked\"].value_counts()","2dd9d196":"# Since most of the Embarked is \"S\" , we can fill the null value with \"S\"\ndf[\"Embarked\"].fillna(\"S\" , inplace = True)\ndf[\"Embarked\"].value_counts()","58b55271":"df[\"Cabin\"].unique() , df[\"Cabin\"].value_counts()","fd8332c5":"# Since most of the Cabin data is missing we can drop the Cabin column\ndf.drop([\"Cabin\"] , axis = 1 , inplace = True)\ndf.isna().sum()","6999280b":"len(df[\"Ticket\"].unique()) , df[\"Ticket\"].value_counts()","7769cc1d":"# Also since most of the tickets are unique values , we can drop ticket column\ndf.drop([\"Ticket\"] , axis = 1 , inplace = True)\ndf.info()","02ce9979":"# Lets split the training and the test data\nfrom sklearn.model_selection import train_test_split\nX = df.drop([\"Survived\"] , axis = 1)\ny = df[\"Survived\"]\nX_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.2 , random_state = 42)\nlen(X_train) , len(X_test) , len(y_train) , len(y_test)","6db4d7c8":"# Now fill the missing values , we do hear to avoid data leakage.\nmedian = X_train[\"Age\"].median()\nX_train[\"Age\"].fillna(median , inplace = True)\nX_test[\"Age\"].fillna(median , inplace = True)","6d57e0e9":"# Normalizing the numerical data for easy computation\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train[[\"Age\" , \"Fare\"]] = scaler.fit_transform(X_train[[\"Age\" , \"Fare\"]])\nX_test[[\"Age\" , \"Fare\"]] = scaler.transform(X_test[[\"Age\" , \"Fare\"]])","fa783da2":"X_train.head()","2952d34d":"# Here we use onehot encoding...\n# Its always adviced to store previous df when performing these operations\nX_train_temp = X_train.copy()\nX_train_temp = pd.get_dummies(X_train_temp , columns = [\"Sex\" , \"Embarked\"] , prefix = [\"Sex\" , \"Embarked\"])","30042646":"# Performing on the original data set\nX_train = pd.get_dummies(X_train , columns = [\"Sex\" , \"Embarked\"] , prefix = [\"Sex\" , \"Embarked\"])\nX_test = pd.get_dummies(X_test , columns = [\"Sex\" , \"Embarked\"] , prefix = [\"Sex\" , \"Embarked\"])","6544f652":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.metrics import classification_report , f1_score , roc_auc_score","35d8ad47":"random = RandomForestClassifier(random_state = 42)\nlinear = LogisticRegression()\nneighbor = KNeighborsClassifier()\nsvm = svm.SVC()","eeee3119":"random.fit(X_train , y_train)\nlinear.fit(X_train , y_train)\nneighbor.fit(X_train , y_train)\nsvm.fit(X_train , y_train)","346906f7":"print(random.score(X_train , y_train))\nprint(linear.score(X_train , y_train))\nprint(neighbor.score(X_train , y_train))\nprint(svm.score(X_train , y_train))","92ccb93c":"random_preds = random.predict(X_test)\nlinear_preds = linear.predict(X_test)\nneighbor_preds = neighbor.predict(X_test)\nsvm_preds = svm.predict(X_test)","63b1789e":"def metrics(y_test , preds):\n    print(f1_score(y_test , preds))\n    print(roc_auc_score(y_test , preds))\n\nprint(\"Random\")\nmetrics(y_test , random_preds)\nprint(\"\\n\")\n\nprint(\"Linear\")\nmetrics(y_test , linear_preds)\nprint(\"\\n\")\n\nprint(\"Neighbors\")\nmetrics(y_test , neighbor_preds)\nprint(\"\\n\")\n\nprint(\"Svm\")\nmetrics(y_test , svm_preds)\nprint(\"\\n\")","b0683f09":"\n# We can eliminate svm  as it has low score , for grids for grid search cv opeartion.\n\ngrid_random = {\"n_estimators\" : [100 , 120 , 150 , 175 , 200 , 500],\n              \"bootstrap\" : [True , False] , \n              \"max_features\" : ['auto', 'sqrt'] , \n              \"min_samples_leaf\" : [1, 2, 4] , \n              \"class_weight\" : [\"balanced\", \"balanced_subsample\"] , \n              \"random_state\" : [42]}\n\ngrid_linear = {\"max_iter\" : [100 , 150 , 200 , 250 , 500] , \n              \"solver\" : [\"liblinear\"] , \n              \"multi_class\" : [\"ovr\"] , \n              \"random_state\" : [42]}\n\ngrid_neighbor = {\"n_neighbors\" : [3 , 5 , 7 , 10] , \n                \"algorithm\" : [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]}","84a3d214":"# Applying hyperparameters with Crossvalidation and GridsearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nmodel_random = GridSearchCV(RandomForestClassifier() , param_grid = grid_random, cv = 5, verbose=2)\nmodel_linear = GridSearchCV(LogisticRegression() , param_grid = grid_linear, cv = 5, verbose=2)\nmodel_neighbor = GridSearchCV(KNeighborsClassifier() , param_grid = grid_neighbor, cv = 5, verbose=2)","d0e41638":"# Fitting into Grid Secarh Cv\nmodel_linear.fit(X_train , y_train)","d3468e44":"model_random.fit(X_train , y_train)\nmodel_random.best_params_","c67156ea":"print(model_linear.best_params_)\nprint(linear.score(X_test , y_test))\nprint(model_linear.score(X_test , y_test))","babb28a5":"model_neighbor.fit(X_train , y_train)\nmodel_neighbor.best_params_","65f2d23d":"final_random_preds = model_random.predict(X_test)\nfinal_linear_preds = model_linear.predict(X_test)\nfinal_neighbor_preds = model_neighbor.predict(X_test)","6054c94e":"print(\"Random\")\nmetrics(y_test , final_random_preds)\nprint(\"\\n\")\n\nprint(\"Linear\")\nmetrics(y_test , final_linear_preds)\nprint(\"\\n\")\n\nprint(\"Neighbors\")\nmetrics(y_test , final_neighbor_preds)\nprint(\"\\n\")","7e3366f9":"random.score(X_test , y_test) # Before tuning","b9f49947":"model_random.score(X_test , y_test)   #After tuning","73b58a45":"print(classification_report(y_test , random_preds))  # Before tuning","8263e8fb":"print(classification_report(y_test , final_random_preds))  # After tuning","1d4b5bd0":"# Reading test dataset\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest.head()","f8c63716":"# Drop Name , passengerId , Cabin , Ticket\npassengers = test[\"PassengerId\"]  # Storing for final use.\ntest.drop([\"Name\" , \"PassengerId\" , \"Cabin\" , \"Ticket\"] , axis = 1 , inplace = True)","781ffcf8":"# Getting basic info\ntest.info()","c281b1a3":"# Cheaking for missing value\ntest.isna().sum()","65acef8a":"# Filling missing age and fare values with median.\ntest[\"Age\"].fillna(df[\"Age\"].median() , inplace = True)  # To be filled with age of larger data set\ntest[\"Fare\"].fillna(df[\"Fare\"].median() , inplace = True)\n\n# Cheaking for any missing values\ntest.isna().sum()","d30d1f85":"# Normalizing the numerical data for easy computation\ntest[[\"Age\" , \"Fare\"]] = scaler.transform(test[[\"Age\" , \"Fare\"]])","896c809a":"# Creating dummie columns for categorical values\ntest = pd.get_dummies(test , columns = [\"Sex\" , \"Embarked\"] , prefix = [\"Sex\" , \"Embarked\"])","96bd76f9":"# Chaeking for any errors\ntest.head(10)","b0b6f0fc":"test_preds = model_random.predict(test)\ntest_preds","6c7a9b7e":"data = {\"PassengerId\" : passengers , \n       \"Survived\" : test_preds}","7a0722f5":"# Creating DataFrame with Pandas\nfinal = pd.DataFrame(data , columns = [\"PassengerId\" , \"Survived\"])\nfinal.head(10)","5886c32d":"final.to_csv(\"Titanic.csv\" , index = False)","5af1273d":"### Fitting into all models","b7c26397":"### Getting scores"}}