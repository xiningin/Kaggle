{"cell_type":{"31b9873b":"code","b0801c52":"code","e1f9e77d":"code","2c669cd3":"code","5ad437a4":"code","9b5e6404":"code","1e264c50":"code","d91d4e21":"code","1f2f04fc":"code","9b09a5e9":"code","3137bd0a":"code","52c3d3ee":"code","30a91dcb":"code","95a54438":"code","e90fb6db":"code","4495e9a6":"code","8cd3a443":"code","fda445ea":"code","2f13bc4e":"code","52c67b9e":"code","5df0d5b4":"code","e63433d1":"code","0a3a8c76":"code","45f12f31":"code","3e5910ef":"code","fcb64403":"code","1b20418e":"code","28978065":"code","3e596d03":"code","89afce99":"code","9d8bf214":"markdown","5b1ed23b":"markdown","a1f337b6":"markdown","9a156ff3":"markdown"},"source":{"31b9873b":" # The model is for Sentiment classification on IMBD dataset.\n # It is based on neural network - GRU RNN and pre-trained GloVe word embeddings.\n # It uses tensorflow keras APIs.\n    \n # Steps:\n # Step1. load corpus data, pre-process and split into train and test dataset. [pending HTML tags removal]\n # Step2. create a vocabulary index on corpus. tokenize and vectorize corpus data.\n # Step3. load pre-trained GloVe word embeddings.\n # Step4. build and evaluate model on training, validation and test data.","b0801c52":"#import required libraries\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport pathlib\nimport codecs\nimport re\nimport string\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Embedding, GRU, LSTM\nprint(tf.__version__)","e1f9e77d":"#import dataset and analyze dataset\n\ndf = pd.read_csv('..\/input\/movie-review\/labelled_full_dataset.csv')\ndf.isnull().values.any()\ndf.shape","2c669cd3":"df.head()","5ad437a4":"import seaborn as sns\nsns.countplot(x='label', data=df)","9b5e6404":"#view one sample\n\ndf['review'][1]","1e264c50":"# #splitting training data into validation and training\n\n#limiting data to less values to run in less then 5 minutes\nx_train = df.loc[:9999, 'review'].values\ny_train = df.loc[:9999, 'label'].values\nx_test = df.loc[10000:12499, 'review'].values\ny_test = df.loc[10000:12499, 'label'].values\n\n# #total data split\n# x_train = df.loc[:39999, 'review'].values\n# y_train = df.loc[:39999, 'label'].values\n# x_test = df.loc[40000:, 'review'].values\n# y_test = df.loc[40000:, 'label'].values","d91d4e21":"print(len(x_train), len(x_test), len(y_train), len(y_test))","1f2f04fc":"x_train[1]","9b09a5e9":"#create a vocabulary index\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\n#vocabulary = tokenizer.fit_on_texts(df['review'])\nvocabulary = tokenizer.fit_on_texts(x_train) \n#running tokenizer on x_train only for limiting values, else would be run on raw text data\nprint(tokenizer)\nprint(vocabulary)","3137bd0a":"#define vocabulary size\n\nvocabulary_size_max = len(tokenizer.word_index) + 1\nprint(vocabulary_size_max)","52c3d3ee":"#max length for padding\n\n#max_length = max([len(s.split()) for s in df['review']])\nmax_length = max([len(s.split()) for s in x_train]) \n#limiting padding to x_train data else would be run on raw text data\nprint(max_length)","30a91dcb":"#vectorize tokens\n\nx_train_vector = tokenizer.texts_to_sequences(x_train)\nx_test_vector = tokenizer.texts_to_sequences(x_test)\nprint(\"train vector is:\", x_train_vector[1])\nprint(\"test vector is:\", x_test_vector[1])","95a54438":"#pad sequences\n#sequences shorter than the length are padded in the beginning and \n#sequences longer are truncated at the beginning.\n\nx_train_pad = pad_sequences(x_train_vector, maxlen = max_length, padding = 'post')\nx_test_pad = pad_sequences(x_test_vector, maxlen = max_length, padding = 'post')\nprint(\"train padding is:\", x_train_pad[1])\nprint(\"test padding is:\", x_test_pad[1])","e90fb6db":"#tokenizer.word_index.items()","4495e9a6":"#load pre-trained word embedding in a dictionary\n#dictionary with key = word and value = embedding in the file\n\nglove_file = '..\/input\/glove6b50dtxt\/glove.6B.50d.txt'\nembedding_dict = {}\nglove = codecs.open(glove_file, encoding = 'utf8')\nfor line in glove:\n    value = line.split(' ')\n    word = value[0]\n    coef = np.array(value[1:],dtype = 'float32')\n    embedding_dict[word] = coef\nglove.close()","8cd3a443":"print(embedding_dict[\"the\"])","fda445ea":"#embedding matrix with ONLY the words present in the input vocabulary i.e. corpus and\n#their corresponding embedding vector\n#vocab_size = len(token.word_index)+1\n#shape of embedding matrix: vocabulary_size_max, glove_dimension\n\nembedding_matrix = np.zeros((vocabulary_size_max,50))\nfor word,i in tokenizer.word_index.items():\n    embedding_value = embedding_dict.get(word)\n    if embedding_value is not None:\n        embedding_matrix[i] = embedding_value","2f13bc4e":"len(embedding_matrix), embedding_matrix.size","52c67b9e":"print(embedding_matrix[1])","5df0d5b4":"#load the pre-trained word embeddings matrix into an Embedding layer\n# Note that we set trainable=False so as to keep the embeddings fixed\n# (we don't want to update them during training).\n\nembedding_dim = 50 #dimensions of embedding layer\n\nfrom tensorflow.keras.layers import Embedding\n\nembedding_layer = Embedding(\n    input_dim = vocabulary_size_max,\n    output_dim = embedding_dim,\n    input_length = max_length,\n    embeddings_initializer= tf.keras.initializers.Constant(embedding_matrix),\n    trainable = False\n)","e63433d1":"#build model\n#GRU default with tanh activation, recurrent activation default sigmoid\n\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(GRU(units = 16, dropout = 0.2, recurrent_dropout = 0.2, activation = 'tanh'))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.summary()","0a3a8c76":"# # model to learn new word embeddings and NOT use pre-trained\n# #build model\n# #GRU default with tanh activation, recurrent activation default sigmoid\n# model = Sequential()\n# model.add(Embedding(input_dim = vocabulary_size_max, output_dim = embedding_dim, input_length = max_length))\n# model.add(GRU(units = 16, dropout = 0.2, recurrent_dropout = 0.2, activation = 'tanh'))\n# model.add(Dense(1, activation = 'sigmoid'))\n# model.summary()","45f12f31":"#compile model\n\nmodel.compile(optimizer = 'adam', loss = tf.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])","3e5910ef":"#train model\n#training on less epochs to run it in less time. ideal would be to increase epochs.\n\nhistory = model.fit(x = x_train_pad, y = y_train, batch_size = 512, epochs = 5, validation_split = 0.25, verbose = 2)","fcb64403":"# plot loss and accuracy of training and validation\n\nhistory_dict = history.history\nhistory_dict.keys()","1b20418e":"acc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","28978065":"plt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\n\nplt.show()","3e596d03":"#evaluate the model\n\nevaluate = model.evaluate(x = x_test_pad, y = y_test, batch_size = 512, verbose = 1, return_dict = True)","89afce99":"evaluate.keys()\ntest_acc = evaluate['accuracy']\ntest_loss = evaluate['loss']\nprint(\"Test loss: \", test_loss*100, '%')\nprint(\"Test accuracy: \", test_acc*100, '%')","9d8bf214":"# Step1. load corpus data, pre-process and split into train and test dataset.","5b1ed23b":"# Step4. build and evaluate model on training, validation and test data.","a1f337b6":" # Step3. load pre-trained GloVe word embeddings.","9a156ff3":" # Step2. create a vocabulary index on corpus. tokenize and vectorize corpus data."}}