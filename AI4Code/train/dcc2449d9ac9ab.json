{"cell_type":{"e69ffce3":"code","edbc2652":"code","ec12de01":"code","c5e598a1":"code","03484087":"code","e82bd781":"code","2cf22798":"code","220ce302":"code","c5384fb5":"code","8deb5228":"code","96e3273d":"code","7ee3457f":"code","6f062ec3":"code","69e371d1":"code","14dab2b0":"code","99e7ca2e":"code","88e8167f":"code","7557d111":"code","2a2b3393":"code","5cb5f501":"code","028ce59d":"code","b634622a":"code","1b369979":"code","cf1768fe":"code","69c38a2c":"code","b24f073d":"code","9151f76b":"code","82d1dccc":"code","cf1ae6e5":"code","7fe332a8":"code","0614dddc":"code","e9d544ce":"code","bdeae575":"code","8750f29c":"code","7438ab25":"code","118f929d":"code","aad2a90b":"code","16b35656":"code","4d0fd6a5":"code","9537b620":"code","8b310790":"code","837b05d5":"code","48cb0e58":"code","661fed63":"code","3ff345c8":"code","3ab853e4":"code","3e448c81":"code","83f26d5e":"markdown","fd9bcf39":"markdown","cdc97a3b":"markdown","28876f44":"markdown","e981b91f":"markdown","3e461d00":"markdown","ab679c9a":"markdown","baf0d5a5":"markdown","08305818":"markdown","cd3f9f09":"markdown","cc9ad6ac":"markdown","fcd7eef6":"markdown","5afe04ff":"markdown","132093ed":"markdown","4f57e088":"markdown","5e8291f1":"markdown","52c5c395":"markdown","65ffdea1":"markdown","89c542a4":"markdown","29f48ccc":"markdown","0780f79a":"markdown","1d2d51d2":"markdown","61e3e6b2":"markdown","3252c2f4":"markdown","f1649f24":"markdown","2bdef6d5":"markdown","22ca56eb":"markdown","89312e88":"markdown","97fe951b":"markdown","9ed944c5":"markdown","b833c6b6":"markdown","16e6e620":"markdown","d7d365bb":"markdown","a54e616a":"markdown","598b28f5":"markdown","5d6b13ff":"markdown","d1a7b8c9":"markdown","97c31fa6":"markdown","f9f029ed":"markdown","0ab16dd3":"markdown","6168c0aa":"markdown","63049b06":"markdown","3587de72":"markdown","57cd2c78":"markdown","9c8eb806":"markdown","b68275b9":"markdown","28d3cd43":"markdown","031c7acc":"markdown","00f17432":"markdown","edf80de9":"markdown","45f3e3d2":"markdown","d8cf7214":"markdown","29861824":"markdown","05814780":"markdown","926c07cf":"markdown","e0edd2a3":"markdown","3f289fea":"markdown","d22af594":"markdown","0166e965":"markdown","c475308f":"markdown"},"source":{"e69ffce3":"## Database Phase\nimport pandas as pd\nimport numpy as np\n\n# Machine Learning Phase\nimport sklearn \nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\n#Metrics Phase\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\n\n#Visualization Phase\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pylab as pylab\n%matplotlib inline\npd.set_option('display.max_columns', 500)\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","edbc2652":"bank=pd.read_csv(\"..\/input\/bank-marketing-campaigns-dataset\/bank-additional-full.csv\",sep=';')\nbank_copy=bank.copy()\n\n## print shape of dataset with rows and columns and information \nprint (\"The shape of the  data is (row, column):\"+ str(bank_copy.shape))\nprint (bank_copy.info())","ec12de01":"bank_copy.head()","c5e598a1":"bank_copy.dtypes","03484087":"#Checking out the statistical parameters\nbank_copy.describe()","e82bd781":"#Checking out the categories and their respective counts in each feature\nprint(\"Job:\",bank_copy.job.value_counts(),sep = '\\n')\nprint(\"-\"*40)\nprint(\"Marital:\",bank_copy.marital.value_counts(),sep = '\\n')\nprint(\"-\"*40)\nprint(\"Education:\",bank_copy.education.value_counts(),sep = '\\n')\nprint(\"-\"*40)\nprint(\"Default:\",bank_copy.default.value_counts(),sep = '\\n')\nprint(\"-\"*40)\nprint(\"Housing loan:\",bank_copy.housing.value_counts(),sep = '\\n')\nprint(\"-\"*40)\nprint(\"Personal loan:\",bank_copy.loan.value_counts(),sep = '\\n')\nprint(\"-\"*40)\nprint(\"Contact:\",bank_copy.contact.value_counts(),sep = '\\n')\nprint(\"-\"*40)\nprint(\"Month:\",bank_copy.month.value_counts(),sep = '\\n')\nprint(\"-\"*40)\nprint(\"Day:\",bank_copy.day_of_week.value_counts(),sep = '\\n')\nprint(\"-\"*40)\nprint(\"Previous outcome:\",bank_copy.poutcome.value_counts(),sep = '\\n')\nprint(\"-\"*40)\nprint(\"Outcome of this campaign:\",bank_copy.y.value_counts(),sep = '\\n')\nprint(\"-\"*40)","2cf22798":"import missingno as msno \nmsno.matrix(bank_copy)","220ce302":"print('Data columns with null values:',bank_copy.isnull().sum(), sep = '\\n')","c5384fb5":"import plotly.express as px\n\nfig = px.box(bank_copy, x=\"job\", y=\"duration\", color=\"y\")\nfig.update_traces(quartilemethod=\"exclusive\") # or \"inclusive\", or \"linear\" by default\nfig.show()","8deb5228":"fig = px.scatter(bank_copy, x=\"campaign\", y=\"duration\", color=\"y\")\nfig.show()","96e3273d":"plt.bar(bank_copy['month'], bank_copy['campaign'])","7ee3457f":"plt.subplot(231)\nsns.distplot(bank_copy['emp.var.rate'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(232)\nsns.distplot(bank_copy['cons.price.idx'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(233)\nsns.distplot(bank_copy['cons.conf.idx'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(234)\nsns.distplot(bank_copy['euribor3m'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(235)\nsns.distplot(bank_copy['nr.employed'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)","6f062ec3":"sns.violinplot( y=bank_copy[\"marital\"], x=bank_copy[\"cons.price.idx\"] )","69e371d1":"bank_yes = bank_copy[bank_copy['y']=='yes']\n\n\ndf1 = pd.crosstab(index = bank_yes[\"marital\"],columns=\"count\")    \ndf2 = pd.crosstab(index = bank_yes[\"month\"],columns=\"count\")  \ndf3= pd.crosstab(index = bank_yes[\"job\"],columns=\"count\") \ndf4=pd.crosstab(index = bank_yes[\"education\"],columns=\"count\")\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\ndf1.plot.bar(ax=axes[0,0])\ndf2.plot.bar(ax=axes[0,1])\ndf3.plot.bar(ax=axes[1,0])\ndf4.plot.bar(ax=axes[1,1])       ","14dab2b0":"f,ax=plt.subplots(figsize=(10,10))\nsns.heatmap(bank_copy.corr(),annot=True,linewidths=0.5,linecolor=\"black\",fmt=\".1f\",ax=ax)\nplt.show()","99e7ca2e":"plt.figure(figsize = (15, 30))\nplt.style.use('seaborn-white')\nax=plt.subplot(521)\nplt.boxplot(bank_copy['age'])\nax.set_title('age')\nax=plt.subplot(522)\nplt.boxplot(bank_copy['duration'])\nax.set_title('duration')\nax=plt.subplot(523)\nplt.boxplot(bank_copy['campaign'])\nax.set_title('campaign')\nax=plt.subplot(524)\nplt.boxplot(bank_copy['pdays'])\nax.set_title('pdays')\nax=plt.subplot(525)\nplt.boxplot(bank_copy['previous'])\nax.set_title('previous')\nax=plt.subplot(526)\nplt.boxplot(bank_copy['emp.var.rate'])\nax.set_title('Employee variation rate')\nax=plt.subplot(527)\nplt.boxplot(bank_copy['cons.price.idx'])\nax.set_title('Consumer price index')\nax=plt.subplot(528)\nplt.boxplot(bank_copy['cons.conf.idx'])\nax.set_title('Consumer confidence index')\nax=plt.subplot(529)\nplt.boxplot(bank_copy['euribor3m'])\nax.set_title('euribor3m')\nax=plt.subplot(5,2,10)\nplt.boxplot(bank_copy['nr.employed'])\nax.set_title('No of employees')\n","88e8167f":"numerical_features=['age','campaign','duration']\nfor cols in numerical_features:\n    Q1 = bank_copy[cols].quantile(0.25)\n    Q3 = bank_copy[cols].quantile(0.75)\n    IQR = Q3 - Q1     \n\n    filter = (bank_copy[cols] >= Q1 - 1.5 * IQR) & (bank_copy[cols] <= Q3 + 1.5 *IQR)\n    bank_copy=bank_copy.loc[filter]","7557d111":"plt.figure(figsize = (15, 10))\nplt.style.use('seaborn-white')\nax=plt.subplot(221)\nplt.boxplot(bank_copy['age'])\nax.set_title('age')\nax=plt.subplot(222)\nplt.boxplot(bank_copy['duration'])\nax.set_title('duration')\nax=plt.subplot(223)\nplt.boxplot(bank_copy['campaign'])\nax.set_title('campaign')","2a2b3393":"bank_features=bank_copy.copy()\nlst=['basic.9y','basic.6y','basic.4y']\nfor i in lst:\n    bank_features.loc[bank_features['education'] == i, 'education'] = \"middle.school\"\n\nbank_features['education'].value_counts()","5cb5f501":"month_dict={'may':5,'jul':7,'aug':8,'jun':6,'nov':11,'apr':4,'oct':10,'sep':9,'mar':3,'dec':12}\nbank_features['month']= bank_features['month'].map(month_dict) \n\nday_dict={'thu':5,'mon':2,'wed':4,'tue':3,'fri':6}\nbank_features['day_of_week']= bank_features['day_of_week'].map(day_dict) ","028ce59d":"bank_features.loc[:, ['month', 'day_of_week']].head()","b634622a":"bank_features.loc[bank_features['pdays'] == 999, 'pdays'] = 0","1b369979":"bank_features['pdays'].value_counts()","cf1768fe":"dictionary={'yes':1,'no':0,'unknown':-1}\nbank_features['housing']=bank_features['housing'].map(dictionary)\nbank_features['default']=bank_features['default'].map(dictionary)\nbank_features['loan']=bank_features['loan'].map(dictionary)","69c38a2c":"dictionary1={'no':0,'yes':1}\nbank_features['y']=bank_features['y'].map(dictionary1)","b24f073d":"bank_features.loc[:,['housing','default','loan','y']].head()","9151f76b":"dummy_contact=pd.get_dummies(bank_features['contact'], prefix='dummy',drop_first=True)\ndummy_outcome=pd.get_dummies(bank_features['poutcome'], prefix='dummy',drop_first=True)\nbank_features = pd.concat([bank_features,dummy_contact,dummy_outcome],axis=1)\nbank_features.drop(['contact','poutcome'],axis=1, inplace=True)","82d1dccc":"bank_features.loc[:,['dummy_telephone','dummy_nonexistent','dummy_success']].head()","cf1ae6e5":"bank_job=bank_features['job'].value_counts().to_dict()\nbank_ed=bank_features['education'].value_counts().to_dict()","7fe332a8":"bank_features['job']=bank_features['job'].map(bank_job)\nbank_features['education']=bank_features['education'].map(bank_ed)\n","0614dddc":"bank_features.loc[:,['job','education']].head()","e9d544ce":"bank_features.groupby(['marital'])['y'].mean()","bdeae575":"ordinal_labels=bank_features.groupby(['marital'])['y'].mean().sort_values().index\nordinal_labels","8750f29c":"ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\nordinal_labels2","7438ab25":"bank_features['marital_ordinal']=bank_features['marital'].map(ordinal_labels2)\nbank_features.drop(['marital'], axis=1,inplace=True)","118f929d":"bank_features.marital_ordinal.value_counts()","aad2a90b":"bank_scale=bank_features.copy()\nCategorical_variables=['job', 'education', 'default', 'housing', 'loan', 'month',\n       'day_of_week','y', 'dummy_telephone', 'dummy_nonexistent',\n       'dummy_success', 'marital_ordinal']\n\n\nfeature_scale=[feature for feature in bank_scale.columns if feature not in Categorical_variables]\n\n\nscaler=StandardScaler()\nscaler.fit(bank_scale[feature_scale])","16b35656":"scaled_data = pd.concat([bank_scale[['job', 'education', 'default', 'housing', 'loan', 'month',\n       'day_of_week','y', 'dummy_telephone', 'dummy_nonexistent',\n       'dummy_success', 'marital_ordinal']].reset_index(drop=True),\n                    pd.DataFrame(scaler.transform(bank_scale[feature_scale]), columns=feature_scale)],\n                    axis=1)\nscaled_data.head()","4d0fd6a5":"X=scaled_data.drop(['y'],axis=1)\ny=scaled_data.y\n\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)","9537b620":"feat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(17).plot(kind='barh')\nplt.show()","8b310790":"X=scaled_data.drop(['pdays','month','cons.price.idx','loan','housing','emp.var.rate','y'],axis=1)\ny=scaled_data.y\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8,random_state=1)\nprint(\"Input Training:\",X_train.shape)\nprint(\"Input Test:\",X_test.shape)\nprint(\"Output Training:\",y_train.shape)\nprint(\"Output Test:\",y_test.shape)","837b05d5":"#creating the objects\nlogreg_cv = LogisticRegression(random_state=0)\ndt_cv=DecisionTreeClassifier()\nknn_cv=KNeighborsClassifier()\nsvc_cv=SVC()\nnb_cv=BernoulliNB()\ncv_dict = {0: 'Logistic Regression', 1: 'Decision Tree',2:'KNN',3:'SVC',4:'Naive Bayes'}\ncv_models=[logreg_cv,dt_cv,knn_cv,svc_cv,nb_cv]\n\n\nfor i,model in enumerate(cv_models):\n    print(\"{} Test Accuracy: {}\".format(cv_dict[i],cross_val_score(model, X, y, cv=10, scoring ='accuracy').mean()))","48cb0e58":"param_grid = {'C': np.logspace(-4, 4, 50),\n             'penalty':['l1', 'l2']}\nclf = GridSearchCV(LogisticRegression(random_state=0), param_grid,cv=5, verbose=0,n_jobs=-1)\nbest_model = clf.fit(X_train,y_train)\nprint(best_model.best_estimator_)\nprint(\"The mean accuracy of the model is:\",best_model.score(X_test,y_test))","661fed63":"logreg = LogisticRegression(C=0.18420699693267145, random_state=0)\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","3ff345c8":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\",confusion_matrix)\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))","3ab853e4":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([-0.01, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","3e448c81":"svc_classifier = SVC(random_state = 0)\nsvc_classifier.fit(X_train,y_train)\ny_pred=svc_classifier.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\\n\",classification_report(y_test,y_pred))","83f26d5e":"<a id=\"subsection-twenty\"><\/a>\n## Ordinal Encoding ","fd9bcf39":"**Insights:**\n* The leads who have not made a deposit have lesser duration on calls\n* Comparing the average, the blue collar, entrepreneur have high duration in calls and student, retired have less duration in average\n* Large distribution of leads were from self employed clients and management people.","cdc97a3b":"From the bar plot we can see the importances of features based on it's impact towards output. Let's take up the top 15 features","28876f44":"<a id=\"subsection-twentyfour\"><\/a>\n## Feature Selection\nLet's check the feature importances and prune our features to make our model perform well.","e981b91f":"<a id=\"subsection-sixteen\"><\/a>\n## Education- category clubbing\n\nHere we are clubbing category in education such as 'basic.9y','basic.6y','basic.4y' to 'middle school' ","3e461d00":"<a id=\"section-one\"><\/a>\n# Introduction\n\n![12.jpg](attachment:12.jpg)\n<a id=\"subsection-one\"><\/a>\n## Problem Statement\nThere has been a revenue decline for the Portuguese bank and they would like to know what actions to take. After investigation, we found out that the root cause is that their clients are not depositing as frequently as before. Knowing that term deposits allow banks to hold onto a deposit for a specific amount of time, so banks can invest in higher gain financial products to make a profit. In addition, banks also hold better chance to persuade term deposit clients into buying other products such as funds or insurance to further increase their revenues. As a result, the Portuguese bank would like to identify existing clients that have higher chance to subscribe for a term deposit and focus marketing effort on such clients.\n\n## About Dataset \nIt is a dataset that describing Portugal bank marketing campaigns results.Conducted campaigns were based mostly on direct phone calls, offering bank client to place a term deposit. If after all marking afforts client had agreed to place deposit - target variable marked 'yes', otherwise 'no'\n\nSource of the data:\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/bank+marketing\n<a id=\"subsection-two\"><\/a>\n# About Project\nIn this project, I will analyze the Bank lead's dataset and create a classification algorithm with full end feature engineering and EDA\n\n## Project Summary:\nI'm a Data Analyst of XYZ consultancy Ltd. The ABC Portugal Bank approached our service and requested us to create a classfication algorithm to automatically place their prospective leads on having a term deposit in their bank. We will be creating a classification algorithm and also suggest them the insights we derive from this dataset and also help them to narrow down their leads into marketing funnel and in the end make a term deposit.\n<a id=\"subsection-three\"><\/a>\n# Objectives of project:\n\n* Meet and Greet Data\n* Prepare the Data for consumption (Feature Engineering and Selection)\n* Perform Exploratory Analysis (Visualizations)\n* Model the Data using Machine Learning\n* Validate and implement data model\n* Optimize and Strategize\n\n<a id=\"section-two\"><\/a>\n# Prepare Data for Consumption\n<a id=\"subsection-five\"><\/a>\n## Import Libraries\n\nLet's import all necessary libraries for the analysis and along with it let's bring down our dataset","ab679c9a":"<a id=\"subsection-nine\"><\/a>\n## Campaign vs Duration calls","baf0d5a5":"<a id=\"subsection-eighteen\"><\/a>\n## Encoding 999 in pdays as 0\n\nEncoding 999 in pdays feature( i.e clients who haven't been contacted for the previous campaign) into 0","08305818":"<a id=\"subsection-twentyeight\"><\/a>\n## Support vector classifier\nLet's fit our best model from model selection and predict outcome.\n\n**Note: Hyperparameter tuning of SVM took more than hours to run in my device. So I'm bypassing hyperparameter tuning for this section.**","cd3f9f09":"<a id=\"section-five\"><\/a>\n# Modelling our Data\nLet's enter into the crucial phase of building THE machine learning model. Before checking \"what could be the best algorithm for prediction\" we have to decide on the \"why\". It is highly important.\n\n## Why?\nOur main aim is to predict whether there is a deposit made made owing to those values from the features. The output is either going to be 0 or 1. So we can decide that we can use classification models for our problem\n\n## What ?\nTo decide on what can be the best possible classification models let's not waste time running models. Instead we do quality code by creating cross validation and check all the model accuracy at once. After that we will select one model based on it's accuracy.\n<a id=\"subsection-twentysix\"><\/a>\n## Model Selection\nLet's dig onto select the best classifier model ","cc9ad6ac":"<a id=\"subsection-twentyfive\"><\/a>\n## Train and Test Split (80:20)\nLet's drop the required features and split the data into train and test","fcd7eef6":"We have scaled our numerical features as you can see from the head.","5afe04ff":"**Insights:**\n* We can see the campaign were mostly concentrated in the starting of the bank period ( May, June and July)\n* Usually education period starts during that time so there is a possibility that parents make deposits in the name of their children\n* They also have made their campaign in the end of the bank period.","132093ed":"We have sorted the categories based on the mean with respect to our outcome","4f57e088":"We have performed one-hot encoding for the above features and dropped the original features","5e8291f1":"# Table of Contents:\n1. [Introduction](#section-one)\n    - [Problem Statement](#subsection-one)\n    - [About Project](#subsection-two)\n    - [Objectives of project](#subsection-three)\n2. [Prepare Data for Consumption](#section-two)\n    - [Import Libraries](#subsection-five)\n    - [Meet and Greet Data](#subsection-six)\n    - [Data Cleaning](#subsection-seven)\n3. [Data Visualization](#section-three)\n    - [Duration of calls vs Job roles](#subsection-eight)\n    - [Campaign vs Duration calls](#subsection-nine)\n    - [Campaign vs Month](#subsection-ten)\n    - [Distribution of Quarterly Indicators](#subsection-eleven)\n    - [Marital Status vs Price index](#subsection-twelve)\n    - [Positive deposits vs attributes](#subsection-thirteen)\n    - [Correlation plot of attributes](#subsection-fourteen)\n4. [Feature Engineering](#section-four)\n    - [Handling Outliers](#subsection-fifteen)\n    - [Education- category clubbing](#subsection-sixteen)\n    - [Encoding - Month and Day of week](#subsection-seventeen)\n    - [Encoding 999 in pdays as 0](#subsection-eighteen)\n    - [Ordinal Number Encoding](#subsection-nineteen)\n    - [Ordinal Encoding](#subsection-twenty)\n    - [Frequency encoding](#subsection-twentyone)\n    - [Target Guided Ordinal Encoding](#subsection-twentytwo)\n    - [Standardization of numerical variables](#subsection-twentythree)\n    - [Feature Selection](#subsection-twentyfour)\n    - [Train and Test Split (80:20)](#subsection-twentyfive)\n5. [Modelling our Data](#section-five)\n    - [Model Selection](#subsection-twentysix)\n    - [Logistic regression with Hyperparameter tuning](#subsection-twentyseven)\n    - [Support vector classifier](#subsection-twentyeight)\n6. [Conclusion](#section-six)\n    ","52c5c395":"<a id=\"subsection-twelve\"><\/a>\n## Marital Status vs Price index","65ffdea1":"<a id=\"subsection-seventeen\"><\/a>\n## Encoding - Month and Day of week\n\nEncoding the categories in month and day of week to the respective numbers.","89c542a4":"<a id=\"subsection-seven\"><\/a>\n# Data Cleaning\n## Checking for missing values\nFirst lets check it visually","29f48ccc":"**Insights:**\n* The more the duration the calls were, they had higher probability in making a deposit\n* Duration of calls faded as the time period of campaign extended further\n* There were many positive leads in the initial days of campaign ","0780f79a":"**Insights:**\n\n* We got `unknown` category in each feature, we should figure out how to deal with that\n* This campaign only operated during weekdays\n* I can't understand what is `non-existent` category in previous outcome aka `poutcome`, if you have figured out what is it let me know in the comments.","1d2d51d2":"**Insights:**\n\n* The Confusion matrix result is telling us that we have **6399+178** correct predictions and **397+139** incorrect predictions.\n* The Classification report reveals that we have **94%** precision which means the accuracy that the model classifier not to label an instance positive that is actually negative which is important as we shouldn't label a lead as positive in making a term deposit when he\/she isn't interested in making a deposit","61e3e6b2":"**Dataset:** \n\nWe have 4118 instances and 21 features. The information says there are no null values. Fishy right? anyway we will strictly scrutinize each feature and check for suspicious records and manipulate them\n\n**Attributes:**\n**Bank client data:**\n\n1. **Age** : Age of the lead (numeric)\n2. **Job** : type of job (Categorical) \n3. **Marital** : Marital status (Categorical)\n4. **Education** :  Educational Qualification of the lead (Categorical)\n5. **Default:** Does the lead has any default(unpaid)credit (Categorical)\n6. **Housing:** Does the lead has any housing loan? (Categorical) \n7. **loan:** Does the lead has any personal loan? (Categorical)\n\n**Related with the last contact of the current campaign:**\n\n8. **Contact:** Contact communication type (Categorical)\n9. **Month:** last contact month of year (Categorical) \n10. **day_of_week:** last contact day of the week (categorical)\n11. **duration:** last contact duration, in seconds (numeric). \n\n**Important note:** Duration highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n**Other attributes:**\n\n12. **campaign:** number of contacts performed during this campaign and for this client (numeric)\n13. **pdays:** number of days that passed by after the client was last contacted from a previous campaign(numeric; 999 means client was not previously contacted))\n14. **previous:** number of contacts performed before this campaign and for this client (numeric)\n15. **poutcome:** outcome of the previous marketing campaign (categorical)\n\n**Social and economic context attributes**\n\n16. **emp.var.rate:** employment variation rate - quarterly indicator (numeric)\n17. **cons.price.idx:** consumer price index - monthly indicator (numeric)\n18. **cons.conf.idx:** consumer confidence index - monthly indicator (numeric)\n19. **euribor3m:** euribor 3 month rate - daily indicator (numeric)\n20. **nr.employed:** number of employees - quarterly indicator (numeric)\n\n**Output variable (desired target):**\n\n21. **y** - has the client subscribed a term deposit? (binary: 'yes','no')","3252c2f4":"<a id=\"subsection-thirteen\"><\/a>\n## Positive deposits vs attributes","f1649f24":"<a id=\"subsection-twentythree\"><\/a>\n## Standardization of numerical variables","2bdef6d5":"<a id=\"section-three\"><\/a>\n# Data Visualization\nSince we have much numerical data, let's keep our plots much targetted towards our machine learning models. Also let's figure out which feature importances and prune away least important ones\n<a id=\"subsection-eight\"><\/a>\n## Duration of calls vs Job roles","22ca56eb":"<a id=\"subsection-nineteen\"><\/a>\n## Ordinal Number Encoding\nHere we are gonna encode the features which has yes,no and unknown. We'll assign yes:1,no:0 and unknown:-1","89312e88":"**Insights:**\n* We can see there is a high employee variation rate which signifies that they have made the campaign when there were high shifts in job due to conditions of economy\n* The Consumer price index is also good which shows the leads where having good price to pay for goods and services may be that could be the reason to stimulate these leads into making a deposit and plant the idea of savings\n* Consumer confidence index is pretty low as they don't have much confidence on the fluctuating economy\n* The 3 month Euribor interest rate is the interest rate at which a selection of European banks lend one another funds denominated in euros whereby the loans have a maturity of 3 months. In our case the interest rates are high for lending their loans \n* The number of employees were also at peak which can increase their income index that could be the reason the campaign targetted the leads who were employeed to make a deposit\n","97fe951b":"### ROC Curve\nLet's check out the performance of our model through ROC curve","9ed944c5":"We have encoded the yes\/no features with hard encoding ","b833c6b6":"<a id=\"subsection-six\"><\/a>\n## Meet and Greet data\n\nOur first step is to create the get the csv and welcome it. Later we should dissect and perform descriptive analyis. Well that escalated quickly.","16e6e620":"We have encoded the job and education feature based on its frequency ","d7d365bb":"<a id=\"subsection-twentyone\"><\/a>\n## Frequency encoding\nLet's use frequency encoding with job and education features in our dataset","a54e616a":"**Insights:**\n* There are very minute differences among the price index\n* Married leads have considerably have an upper hand as they have index contributing as couple \n","598b28f5":"<a id=\"section-six\"><\/a>\n# Conclusion\n![11.jpg](attachment:11.jpg)\nFrom the EDA and model selection part we can clearly identify duration playing an important attribute in defining the outcome of our dataset. It is absolute that the more the leads are interested in starting a deposit will have higher number of calls and the call duration will be higher than the average. We have also figured out that job and education also acts as a crucial deciding factor and influences the outcome alot.\n\nHere are the few recommendations for the bank than can help improve the deposit rate\n\n* Classify job roles based on corporate tiers and approach all tier 1 employees within few days after the campaign commences\n* Listen to the leads and extract more information to deliver the best deposit plan, which can increase the duration of calls and that can lead to a deposit\n* Approaching the leads during the start of new bank period(May-July) will be a good choice as many have shown positive results from data history\n* Tune the campaign according to the national econometrics, don't chanelize the expenses on campaign when the national economy is performing poor\n","5d6b13ff":"<a id=\"subsection-twentyseven\"><\/a>\n## Logistic regression with Hyperparameter tuning\nLet's fit the model in logistic regression with parameter tuning and figure out the accuracy of our model","d1a7b8c9":"92% accurate. That's really good. Let's check out confusion matrix and see the classification report","97c31fa6":"Great, we have clubbed all the categories in education into one","f9f029ed":"<a id=\"subsection-eleven\"><\/a>\n## Distribution of Quarterly Indicators","0ab16dd3":"We see that many features doesn't have much outliers except for age,duration and campaign. So, let's fix only those features using IQR method.","6168c0aa":"<a id=\"section-four\"><\/a>\n# Feature Engineering\n<a id=\"subsection-fifteen\"><\/a>\n## Handling outliers\nLet's check out our numerical feature outliers through boxplot","63049b06":"From the test results, we can see high accuracy in SVC followed by Logistic regression. Let's fit and predict","3587de72":"We have converted 999 to 0 in pdays","57cd2c78":"**Insights:**\n* Married leads have made high deposits followed by single\n* There were much deposist made during may month as it is the start of bank period\n* Leads who work in administrative position made deposits followed by technicians and blue collar employees\n* Leads who had atleast university degree had made te deposits followed by highschool\n","9c8eb806":"We have hard encoded the month and day of week features","b68275b9":"Now that we have removed outliers, we can proceed for more feature engineering techniques.","28d3cd43":"<a id=\"subsection-ten\"><\/a>\n## Campaign vs Month","031c7acc":"From the ROC curve we can infer that our logistic model has classified the prospective leads who made deposit correctly rather than predicting false positive. The more the ROC curve(red) lies towards the top left side the better our model is. We can choose any value between **0.8 to 0.9** for the threshold value which can reap us true positive results","00f17432":"We have 91% accuracy and Also we have 92% precision","edf80de9":"<a id=\"subsection-fourteen\"><\/a>\n## Correlation plot of attributes","45f3e3d2":"<a id=\"subsection-twentytwo\"><\/a>\n## Target Guided Ordinal Encoding\nLets encode marital feature based on the target 'y' . First let's find the mean of target with respect to  marital feature\n","d8cf7214":"Converted the frequency into key value pairs. Let's map them","29861824":"We have got the best parameters for the model and the mean accuracy is 92.4%","05814780":"We have the records of null values and looks like **we don't have any null values.**","926c07cf":"**Insights:**\n* The indicators have correlation among themselves\n* Number of employees rate is highly correlated with employee variation rate\n* Consumer price index is highly correlated with bank interest rate( higher the price index, higher the interest rate)\n* Employee variation rate also correlates with the bank interest rates\n\n","e0edd2a3":"Looks like we don't have any null values except one. But plots sometimes deceive us, numbers don't. Let's check with the numbers","3f289fea":"## Please leave critical feedback in comment section, also check out my [other notebooks](https:\/\/www.kaggle.com\/notebooks?sortBy=dateRun&group=profile&pageSize=20)","d22af594":"Changed into key:value pairs, let's map them","0166e965":"We have encoded the marital feature","c475308f":"Let's check out the general overview of the dataframe"}}