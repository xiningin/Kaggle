{"cell_type":{"73dabd50":"code","93da3acb":"code","0601cce2":"code","ea7cb0b1":"code","f86b32b2":"code","9a37a457":"code","57052fe2":"code","904ef439":"code","26fdbc20":"code","bda2be7b":"code","adc36cf8":"code","5e47e5b2":"code","67784c48":"markdown","742c3ba2":"markdown","66a34653":"markdown","70c6ec30":"markdown","b3da8d1a":"markdown","b79b9ff1":"markdown","56107f63":"markdown"},"source":{"73dabd50":"# Import the Libraries\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pandas import read_csv\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import confusion_matrix\n\nimport tensorflow as tf                                    # Tensorflow also can perform ANN\nimport keras\nfrom keras.models import Sequential, load_model            # ANN from keras\nfrom keras.layers import Dense, Flatten\n\n%matplotlib inline","93da3acb":"# Function to unflatten the images, in other words, to transform a lot of vectors (LxM*N) into a matrix (LxMxN)\ndef unflatten_images(matriz):\n    L, C = matriz.shape\n    images = np.zeros([L, 28, 28])\n    for n in range(0, L):\n        images[n, :, :] = matriz[n, :].reshape(28, 28)\n    return images","0601cce2":"\"\"\"\nFunction to one_hot_encode the labels which are in a decimal format, in other words, \nto transform a number from (0 to 10) into a binary array (1x10) \n\ne.g.:\n\n5 => [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n\n\"\"\"\ndef one_hot_labels(labels):\n    labels = np.array([list(labels)]).T\n    ohc = OneHotEncoder(categorical_features = [0])\n    labels = ohc.fit_transform(labels).toarray()\n    return labels","ea7cb0b1":"# Reading Train and Test data from csv files \n\ndataTrain = read_csv('..\/input\/Train.csv').values\ndataTest = read_csv('..\/input\/Test.csv').values\n\n\n# Spliting data and classes, and one_hot_encoding the Train and Test labels\n\nimTrain = dataTrain[:, :-1] # Getting the data\ntrain_labels = dataTrain[:, -1] # Getting the classes\nlabelTrain = one_hot_labels(train_labels) # One hot encode to the classes\n\nimTest = dataTest[:, :-1] # Getting the data\ntest_labels = dataTest[:, -1] # Getting the classes\nlabelTest = one_hot_labels(test_labels) # One hot encode to the classes\n\nL, N = imTrain.shape","f86b32b2":"# Build the model\nclassificador = Sequential()\nclassificador.add(Dense(units=100, kernel_initializer='uniform', activation='relu', input_dim=N))\nclassificador.add(Dense(units=10, kernel_initializer='uniform', activation='softmax'))\n\n# Compile and fit the model\nclassificador.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])\nclassificador.fit(imTrain[:60000, :], labelTrain[:60000, :], validation_split=0.1, epochs=50, batch_size=200, verbose=1)\n\n# Evaluate if the model is making good predictions or not\nscores = classificador.evaluate(imTest, labelTest)\nprint('{}: {:.2f}%'.format(classificador.metrics_names[1], scores[1]*100))","9a37a457":"# Function to plot more than one image at a time\ndef plot_images(images):\n    \"Plot a list of MNIST images.\"\n    fig, axes = plt.subplots(nrows=1, ncols=images.shape[0])\n    for j, ax in enumerate(axes):\n        img1 = images[j, :, :]\n        ax.matshow(img1.reshape(28,28), cmap = plt.cm.binary)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()","57052fe2":"# Inspecting if the predictions are good\nindexes = (33, 499, 2902, 7238, 1348, 8999, 9200, 3430, 9501, 6788)\nresultados = classificador.predict(imTest[indexes, :])\nfinal = []\ncorretos = []\nfor cont, n in enumerate(resultados):\n    final.append(np.argmax(n))\n    corretos.append(test_labels[indexes[cont]])\nprint('Predicted Results: {}'.format(final))\nprint('Correct Results:   {}'.format(corretos))\n\ncheck_images = unflatten_images(imTest[indexes, :])\n\nplot_images(check_images)","904ef439":"# Function to plot the confusion matrix\ndef plot_confusion_matrix(Mconf, classes, title=None, normalize=False, cmap=plt.cm.Blues):\n    \"\"\"\n    This function plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    plt.imshow(Mconf, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes, rotation=45)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = Mconf.max() \/ 2.\n    for i in range(Mconf.shape[0]):\n        for j in range(Mconf.shape[1]):\n            plt.text(j, i, format(Mconf[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if Mconf[i, j] > thresh else \"black\")\n    plt.tight_layout()","26fdbc20":"# Plotting the confusion matrix\nresultados = classificador.predict_classes(imTest)\nMconf = confusion_matrix(test_labels, resultados)\nclasses = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\nplot_confusion_matrix(Mconf, classes)","bda2be7b":"# Saving current model\nclassificador.save('mnist.h5')","adc36cf8":"# Loading previous trained model saved\nnew_model = load_model('mnist.h5')","5e47e5b2":"# Summary of the trained model\nnew_model.summary()","67784c48":"Here's the deal, we will build this neural network using the Keras framework:","742c3ba2":"OK, so now we've got the following variables:\n\n- train_labels: Vector (60000x1) with each label image in its indexes. \n- test_labels: Vector (10000x1) with each label image in its indexes. \n- imTrain: Matrix (60000x784) with each train image in its rows.\n- labelTrain: Matrix (60000x10) with each label image in its rows.\n- imTest: Matrix (10000x784) with each test image in its rows.\n- labelTest: Matrix (60000x10) with each test label in its rows.","66a34653":"### We can plot the confusion matrix to see the whole accuracy of our model","70c6ec30":"### Finally, we can save our model to use this configuration later","b3da8d1a":"### Let's begin by building the model\n\nWe define a sequential model for our Multi Layer Perceptron. Then we add a hidden layer with 100 neurons (100 is kind of random) and we specify \"relu\" activation function, following by the output layer with 10 neurons and the \"softmax\" activation function. Both layers have their weigths initialized with random numbers from a uniform distribution.","b79b9ff1":"## Image Recognition using MLP for the MNIST digits dataset\n\n### by Ayrton Casella ","56107f63":"### Now, we're gonna inspect  the results that were given"}}