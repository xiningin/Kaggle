{"cell_type":{"08188218":"code","2d42ce5f":"code","980d1966":"code","43a1f730":"code","47e17248":"code","e0a21af6":"code","66a9e823":"code","7ff045bc":"code","4d8f4706":"code","16b6f6e4":"code","c7de6e30":"code","4bad5bc3":"code","ba5f4173":"code","e9d50197":"code","31cf09e0":"code","7837c39c":"code","4d4908d0":"code","544d5124":"code","87a196ac":"code","eaba864f":"code","b5328da8":"code","18ecdfac":"code","e19da20e":"code","e26be98d":"code","8dc90d2a":"code","a3e5b1d2":"code","a2e2167e":"code","83ee32a7":"code","7a189445":"code","35e01f8d":"code","6f1ad98a":"code","365aa6d3":"code","2401dc98":"code","7ed9192d":"code","019588ae":"code","cefb3ec4":"code","e42d1ed4":"code","d1f39a1f":"code","bfa27ac6":"code","b4543d05":"code","845e9755":"code","62553800":"code","b032098a":"code","aba35e17":"code","2bd9ea9b":"code","e21fed43":"code","17df9411":"code","d0ff1f4a":"code","74d94d47":"code","439719f7":"code","9890c55b":"code","37b2332a":"code","cbe13196":"code","545f2ba7":"code","78cb0c49":"code","b7d1c1af":"code","e18a6784":"markdown","9821026c":"markdown","f1f62600":"markdown","2ad0092e":"markdown","68938a61":"markdown","d0801b0d":"markdown","aa119dce":"markdown","1443e11e":"markdown","b08c012a":"markdown","6259f11d":"markdown","24c66c7a":"markdown","99d3c6c7":"markdown","58de3f09":"markdown","e2987030":"markdown","bb432658":"markdown","4ebff4b8":"markdown","68cacdd4":"markdown","0a9eae55":"markdown","4679de78":"markdown","beacb5de":"markdown","76d32e65":"markdown","f2deb6c9":"markdown","fba0a1d6":"markdown","142ba190":"markdown","e9dda7c7":"markdown","5a5cc2a0":"markdown","3fe09a6f":"markdown","f5da292f":"markdown","ea5f124f":"markdown","e67c4db0":"markdown","7a122ff4":"markdown","a3c657c1":"markdown","f394ac68":"markdown","d942421f":"markdown","c70a9b1f":"markdown","f93f05fb":"markdown","ae6b8566":"markdown","780ad0b8":"markdown","793cf734":"markdown","725a09b2":"markdown","b7020904":"markdown","e2d5c2ab":"markdown","03889d05":"markdown","a2c573f4":"markdown","fb063215":"markdown","9d9de9ce":"markdown","d7b77ba4":"markdown"},"source":{"08188218":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2d42ce5f":"hr = pd.read_csv(\"\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\")","980d1966":"hr.head(5)","43a1f730":"target_count = hr['target'].value_counts()\nsns.barplot(target_count.index , target_count.values)\nplt.ylabel('Number of candidate')\nplt.xlabel('target')","47e17248":"sns.countplot(x=\"gender\", hue=\"target\", data=hr)\nplt.ylabel('Number of candidate')","e0a21af6":"hr.pivot_table(index=['gender'], values='target',aggfunc=['mean', 'count'])","66a9e823":"sns.kdeplot(hr.loc[(hr[\"target\"]==0), \"city_development_index\"], label=\"Non-Job Seeker\")\nsns.kdeplot(hr.loc[(hr[\"target\"]==1), \"city_development_index\"], label=\"Job Seeker\")","7ff045bc":"x=hr['experience'].str.strip('><').fillna(0).astype(int)\ny=hr['training_hours']\nplt.scatter(x, y, marker='o')\nplt.ylabel('Training hours')\nplt.xlabel('Experience (years)')","4d8f4706":"sns.boxenplot(y='training_hours', x='target', data=hr)","16b6f6e4":"import matplotlib.gridspec as gridspec","c7de6e30":"fig = plt.figure(figsize=(15,10))\ngs = fig.add_gridspec(1, 2)\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\n\ntype_major_0 = hr.loc[hr['target']==0].pivot_table(index='company_type', columns='major_discipline', values='target', aggfunc='count')\ntype_major_1 = hr.loc[hr['target']==1].pivot_table(index='company_type', columns='major_discipline', values='target', aggfunc='count')\n\n\nheat_0 = sns.heatmap(ax=ax0, data=type_major_0, cmap=\"OrRd\", cbar=False)\nax0.text(0,-0.5,\"Non-job seeker\",fontsize=20,fontweight='bold')\nheat_1 = sns.heatmap(ax=ax1, data=type_major_1,  yticklabels=False, cmap=\"OrRd\", cbar=False)\nax1.text(1,-0.5,\"Job seeker\",fontsize=20,fontweight='bold')\nax1.set_ylabel('')  \n","4bad5bc3":"total = hr.groupby(['target'])['enrollee_id'].count().reset_index(drop=True)\n\nnot_seek = hr.loc[hr['target']==0].groupby(['target','last_new_job'])['enrollee_id'].count().reset_index()\nseek = hr.loc[hr['target']==1].groupby(['target','last_new_job'])['enrollee_id'].count().reset_index()\n\nnot_seek['percentage'] = not_seek['enrollee_id'].div(total.iloc[0])*100\nseek['percentage'] = seek['enrollee_id'].div(total.iloc[1])*100\n\nax = plt.barh(not_seek.last_new_job, not_seek.percentage, label='Non-job seeker')\nax = plt.barh(seek.last_new_job, seek.percentage, height=0.3, label='Job seeker')\n\nplt.legend()","ba5f4173":"#Finding the ratio of missing values\nhr.isna().sum()\/len(hr)","e9d50197":"hr_train = hr.fillna(0)","31cf09e0":"hr_train['relevent_experience'].unique()","7837c39c":"hr_train['relevent_experience'] = hr_train['relevent_experience'].replace('Has relevent experience',1)\nhr_train['relevent_experience'] = hr_train['relevent_experience'].replace('No relevent experience',0)","4d4908d0":"from sklearn.preprocessing import OrdinalEncoder","544d5124":"hr_train.company_size.unique()","87a196ac":"#The missing values are replaced with 0 abovehead, need to include 0 in the order list though it is numeric\n\nedu_lv = [0,'Primary School','High School','Graduate','Masters','Phd']\nuni = [0,'no_enrollment', 'Part time course', 'Full time course']\ncomp_size = [0,'<10','10\/49','50-99','100-500','500-999','1000-4999','5000-9999','10000+']","eaba864f":"enc = OrdinalEncoder(categories=[uni])","b5328da8":"ordi1 = pd.DataFrame(enc.fit_transform(hr_train[[\"enrolled_university\"]]))","18ecdfac":"#rename column for easy understanding\nordi1 = ordi1.rename(columns={0:\"University\"})","e19da20e":"#Repeat ordinal encoding for education level and company_size\nenc = OrdinalEncoder(categories=[edu_lv])\nordi2 = pd.DataFrame(enc.fit_transform(hr_train[[\"education_level\"]]))","e26be98d":"ordi2 = ordi2.rename(columns={0:\"Education level\"})","8dc90d2a":"enc = OrdinalEncoder(categories=[comp_size])\nordi3 = pd.DataFrame(enc.fit_transform(hr_train[[\"company_size\"]]))\nordi3 = ordi3.rename(columns={0:\"Company size\"})","a3e5b1d2":"hr_train= pd.get_dummies(hr_train, columns=['gender', 'major_discipline', 'company_type'])","a2e2167e":"#check any string in the columns\nfrom pandas.api.types import is_numeric_dtype","83ee32a7":"is_numeric_dtype(hr_train['city_development_index'])","7a189445":"is_numeric_dtype(hr_train['training_hours'])","35e01f8d":"hr_train.experience.unique()","6f1ad98a":"#Simply transform >20 to 21, of course it may be underestimated\n#Simply transform <1 to 0.5 as the mean\nhr_train['experience'] = hr_train['experience'].replace('>20',21)\nhr_train['experience'] = hr_train['experience'].replace('<1',0.5)","365aa6d3":"hr_train.last_new_job.unique()","2401dc98":"hr_train['last_new_job'] = hr_train['last_new_job'].replace('>4',5)\nhr_train['last_new_job'] = hr_train['last_new_job'].replace('never',0)","7ed9192d":"len(hr_train.city.unique())","019588ae":"from sklearn.preprocessing import LabelEncoder","cefb3ec4":"hr_train['city'] = LabelEncoder().fit_transform(hr_train['city'])","e42d1ed4":"hr_train = hr_train.drop(columns=['enrollee_id',\n                       'enrolled_university',\n                       'education_level',\n                       'company_size',\n                      'target'])","d1f39a1f":"hr_train","bfa27ac6":"#concat all features\nX = pd.concat([hr_train, ordi1, ordi2, ordi3], axis=1)","b4543d05":"from sklearn.manifold import TSNE","845e9755":"#Dimension of the embedded space\nX_embedded = TSNE(n_components=2).fit_transform(X)","62553800":"X_embedded.shape","b032098a":"df = pd.DataFrame()\ndf[\"y\"] = hr['target']\ndf[\"dim-1\"] = X_embedded[:,0]\ndf[\"dim-2\"] = X_embedded[:,1]","aba35e17":"sns.scatterplot(x=\"dim-1\", y=\"dim-2\", hue=df.y.tolist(),\n                data=df).set(title=\"T-SNE projection\") ","2bd9ea9b":"from sklearn.svm import SVC","e21fed43":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, hr['target'], test_size = 0.2, random_state=46)","17df9411":"svclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train, y_train)","d0ff1f4a":"y_pred_lin = svclassifier.predict(X_val)","74d94d47":"svclassifier = SVC(kernel='poly', degree=3)\nsvclassifier.fit(X_train, y_train)\ny_pred_poly = svclassifier.predict(X_val)","439719f7":"#Default: rbf\nsvclassifier = SVC()\nsvclassifier.fit(X_train, y_train)\ny_pred_rbf = svclassifier.predict(X_val)","9890c55b":"from sklearn.metrics import classification_report, confusion_matrix","37b2332a":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_val,y_pred_lin))\nprint(classification_report(y_val,y_pred_lin))","cbe13196":"print(confusion_matrix(y_val,y_pred_poly))\nprint(classification_report(y_val,y_pred_poly))","545f2ba7":"print(confusion_matrix(y_val,y_pred_rbf))\nprint(classification_report(y_val,y_pred_rbf))","78cb0c49":"# start with small C\nfor i in range(3,10):\n    svclassifier = SVC(C=i)\n    svclassifier.fit(X_train, y_train)\n    y_pred = svclassifier.predict(X_val)\n    print('RBF with C={}'.format(i))\n    print(classification_report(y_val,y_pred))","b7d1c1af":"#Seem small C doesn't make a big difference, Try bigger C with larger interval to save time\nfor i in range(11,511,100):\n    svclassifier = SVC(C=i)\n    svclassifier.fit(X_train, y_train)\n    y_pred = svclassifier.predict(X_val)\n    print('RBF with C={}'.format(i))\n    print(classification_report(y_val,y_pred))","e18a6784":"Without changing the C parameter, accuracy remains same in different kernels.\nLet's change the C parameter and see if any difference.","9821026c":"It's quite embarrassing for column \"city\" as there is 123 unique cities. Let's simply do label encoding to have only one column","f1f62600":"# TSNE","2ad0092e":"![image.png](attachment:image.png)","68938a61":"I am going to use kdeplot to demostrate the relationship of city development index(CDI) and candidate's tendency of leaving.\n\nThis is introduction from seaborn official website for your reference:\n- A kernel density estimate (KDE) plot is a method for visualizing the distribution of observations in a dataset, analagous to a histogram. ","d0801b0d":"# Employee with more training hours more likely to leave?","aa119dce":"When C=211, the accuracy rise 1%, while when C=311, the accuracy rise to 77%","1443e11e":"# 1. Linear kernel","b08c012a":"Let's get started with ordinal variables: relevent_experience, enrolled_university, education_level, company_size\n\nAs there is no missing value of \"relevent_experience\", and there is just 2 variables: has or hasn't. I just simply transform them into 0,1. ","6259f11d":"# Evaluation","24c66c7a":"# No of changing company before","99d3c6c7":"The popular techniques to transform categorical data to numeric input are one-hot encoding, label encoding and ordinal encoding.\n\nCategorical data can further be divided as nominal and ordinal, usually the former is the majority, such as city, gender, major in this dataset. However, an ordering does exist in some categorical data, relevent experience and education level are good examples.\n\nFor ordinal variables, ordinal encoding is useful for us to tell the machine which variable is better and which should be ranked higher.\n\nAs for nominal variables, ususally one-hot encoding is prefered than label encoding. The difference between one-hot encoding and label encoding is that one-hot encoding will create a binary column for each category in that column, while label encoding encode target  with value between 0 and n-1. Thus, if there are hundreds features in that column, 100 binary columns will be created by one-hot encoding and only one columns will be created in label encoding.\n\nUsing label encoding may mislead the machine that there is an ordering between categories. The application of label encoding is commonly found to encode the labels, as its name stated (Official definition from sklearn: Encode target labels with value between 0 and n_classes-1, used to transform non-numerical labels to numerical labels. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html) \n\nBut if there are 100 categories in one feature, 99 new input vector will then be created which sounds intimidating, but the models can generally handle it in case the instances are far more than the features.\n\nTo conclude, there is no one-size-sit-all or standard encoding method, it just depends on the dataset and the chosen model.","58de3f09":"# 3. RBF kernel","e2987030":"# 3. RBF kernel performance","bb432658":"# Introduction of 3 encoding methods","4ebff4b8":"# Major of job-seeker","68cacdd4":"Two peaks can be observed: more job-seekers at low CDI scores, while less job-seekers at high CDI scores.\n\nTwo possible explanations:\n1. Hypothesis: job-seekers are more ambitious.\n\nUsually, large-scale company or company with very high growth locates in city with high CDI, eg San Jos\u00e9 in Silicon Valley, London in UK. Thus, ppl have less incentives to leave these comapnies, in contrast, ppl in city with low CDI may want to vanish their CVs by seeking a new job\n\n2. Hypothesis: More opportunities in the company located in high CDI scores.\n\nCorportions in high CDI areas should be in larger scale or with well-established date infrastrutture (eg well-built database\/ have data team). Employees can find more opportunities or promotion in planning their career path; However, in low CDI areas, company may be in a smaller scale that may not have a team, or still urdergoing digital transformation that data scientist may also take part in data engeering.\n\nThe actual reason may also be a mix of above explanations.\n\n","0a9eae55":"Job-seeker and non-job seeker literally mix together","4679de78":"Fig 2. dimensionality reduction by tSNE & PCA\n(Source from: https:\/\/towardsdatascience.com\/tsne-degrades-to-pca-d4abf9ef51d3)","beacb5de":"# How many candidate seeking a job change?","76d32e65":"# Modelling","f2deb6c9":"# HR Analytics: Job Change of Data Scientists -- Predict who will leave","fba0a1d6":"# 2. Polynomial kernel performance","142ba190":"![](https:\/\/i2.wp.com\/techvidvan.com\/tutorials\/wp-content\/uploads\/sites\/2\/2020\/05\/Support-Vector-Machines-1.jpg?ssl=1)","e9dda7c7":"# One-hot encoding","5a5cc2a0":"# 1. Linear kernel performance","3fe09a6f":"STEM is the majority, either in total no of employee or job-seeker ratio. This feature doesn't tell a big difference as well.","f5da292f":"It's obvious that employee who haven't changed company or only once have high likelihood to seek a new job currently.","ea5f124f":"# Data massage before modelling","e67c4db0":"# Ordinal encoding","7a122ff4":"# Relationship between experience and training hours","a3c657c1":"- 0 \u2013 Not looking for job change,\n- 1 \u2013 Looking for a job change\n\nNo of not seeking a job change is far more than those seeking a job change, nearly 3 times. ","f394ac68":"Firstly, let's find the ratio of male and female. Most job-seekers appear to be male in terms of absolute no.\n\nHowever, if look at the pivot table, the mean value (ie ratio of job-seeker) of target between male and female is similar. This is because there are far more male data scientist than female and of course more male job seeker will be observed, giving us a illusion that male employee tend to leave.\n","d942421f":"Fig 4. Demonstration of different kernel (Source from: https:\/\/scikit-learn.org\/stable\/modules\/svm.html)","c70a9b1f":"# Male more likely to leave?","f93f05fb":"Previously, I assumeed that ppl with more training hours should be more experienced, ie. positively correlated. However, it seems it doesn't have a relationship.\n\nMaybe data scientists still need to self-learn a lot even if they are less-experienced, or the training hour of employee with more experience doesn't equal to the data science skillset training, but total work experience instead.\n\nAll of above are just guess since the data dictionary of this dataset doesn't clearly stated. ","ae6b8566":"Fig 3. Support vector machine\n(Source from: https:\/\/techvidvan.com\/tutorials\/svm-in-r\/)","780ad0b8":"![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_iris_svc_001.png)","793cf734":"# 2. Polynomial kernel","725a09b2":"Features\n- enrollee_id : Unique ID for enrollee\n- city: City code\n- citydevelopmentindex: Developement index of the city (scaled)\n- gender: Gender of enrolee\n- relevent_experience: Relevent experience of enrolee\n- enrolled_university: Type of University course enrolled if any\n- education_level: Education level of enrolee\n- major_discipline :Education major discipline of enrolee\n- experience: Employee total experience in years\n- company_size: No of employees in current employer's company\n- company_type : Type of current employer\n- lastnewjob: Difference in years between previous job and current job\n- training_hours: training hours completed\n- target: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change","b7020904":"# How the environment shape their tendency?","e2d5c2ab":"Support Vector Machine (SVM)\n- supervised machine learning\n- optimal decision boundary --> maximum margin from the nearest points of all the classes\n- soft margin --> distance between observations and threshold --> allow some misclassifications\n- observations within soft margin --> support vectors\n- find support vector classifiers by kernel functions\n- classify observations in higher dimensions by hyperplane\n- kernal trick --> calculate high-dimensional relationships without actually transforming the data to higher dimension","03889d05":"There can be 2 assumptions beforehand. Employee with more training hours are more skilled, and they are capable of finding better opportunities; Meanwhile, this is also possible that employee with less training hours want to accumulate more experience so they aren't stable once they think they 'learn' enough in that company.\n\nBut from the graph, it seems the two interpretations aren't valid either that there is no significant difference between.","a2c573f4":"# 4. RBF with different values on C parameter","fb063215":"![](https:\/\/docs.microsoft.com\/en-us\/dotnet\/machine-learning\/media\/model-builder-data.png)","9d9de9ce":"TSNE (t-distributed stochastic neighbor embedding)\n- visualizing high-dimensional data\n- nonlinear dimensionality reduction technique\n\nStep:\n- determine the 'similarity' of all points\n- scale the unscaled similarities that they add up to 1\n\nwhy t-distribution?\n- normal distribution alike\n- but isn't as tall in the middle and the tails are taller on the ends","d7b77ba4":"Fig 1. Terms used when describing the dataset, rows are sometimes called as instances.\n(Source from: https:\/\/docs.microsoft.com\/en-us\/dotnet\/machine-learning\/automate-training-with-model-builder)"}}