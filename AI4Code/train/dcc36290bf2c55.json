{"cell_type":{"818cc9a9":"code","42e873cc":"code","932c23ee":"code","33f82c69":"code","c2b76f72":"code","19efe011":"code","e9cc05b6":"code","e47d956c":"code","1a142cee":"code","5ea24faf":"code","d20f3074":"code","d080bc02":"code","1601a395":"code","d2c079b7":"code","8eb2c9c4":"code","712b459d":"code","add8aa2b":"code","ba5cfa46":"code","28300d16":"code","21b007bd":"code","599ac9f8":"code","339e244b":"code","5d367a3a":"code","bcf575a3":"code","63947361":"code","1d43d7d1":"code","6853187b":"code","666f5133":"code","8760756c":"code","85087014":"code","9f9c90cf":"code","f8139348":"code","75e4c180":"code","560c397f":"code","39079245":"code","b3a0669e":"code","846fea99":"code","42eee712":"code","f4af95d6":"code","e3a0f550":"code","126f6bec":"code","f472c5f6":"code","068f2f3c":"code","a51e2ac0":"code","a926f05c":"code","c512b8db":"code","4ab16042":"code","0322bb7b":"code","46e54a80":"code","a7951320":"code","07ed7115":"code","ee2fa140":"code","c0e66f63":"code","a76dfc71":"code","a1bdd55c":"code","1ca4f377":"code","2647b269":"code","c29ee930":"code","ff503e60":"code","2500994c":"code","91672552":"code","efcee162":"markdown","788dcd5c":"markdown","630b5a77":"markdown","dd44e1b6":"markdown","33b70f38":"markdown","387ff2bb":"markdown","eef8dc81":"markdown","7b067ed5":"markdown","a0ce0822":"markdown","091247cb":"markdown","b2dfed76":"markdown","2e8e4f9d":"markdown","b5bec4fc":"markdown","d3e5f72f":"markdown","dbb7a007":"markdown","edfd1736":"markdown","cf11d081":"markdown","9c30c1fa":"markdown","7eec49ee":"markdown"},"source":{"818cc9a9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","42e873cc":"adv_data = pd.read_csv('..\/input\/clicked-on-add\/advertising.csv')","932c23ee":"adv_data.head()","33f82c69":"adv_data.info()","c2b76f72":"adv_data.describe()","19efe011":"plt.style.use('fivethirtyeight')","e9cc05b6":"sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\nadv_data['Age'].hist(bins=30, color = \"red\", edgecolor='black', alpha=0.65, lw=1.5)\nplt.xlabel('Age')","e47d956c":"sns.countplot(data=adv_data, x='Clicked on Ad')","1a142cee":"sns.jointplot(x = 'Age', y ='Area Income', data = adv_data,hue = 'Clicked on Ad',kind = 'kde', color = \"red\", alpha = 0.65)","5ea24faf":"sns.jointplot(x = 'Age', y = 'Daily Time Spent on Site', data = adv_data, kind = 'kde', color = \"red\", alpha = 0.65)","d20f3074":"sns.jointplot(x = 'Daily Time Spent on Site', y = 'Daily Internet Usage', data = adv_data, kind = 'kde', hue = 'Clicked on Ad', alpha = 0.65)","d080bc02":"sns.pairplot(adv_data, hue = 'Clicked on Ad', palette = 'cool_r')","1601a395":"adv_data['Timestamp'] = pd.to_datetime(adv_data['Timestamp'])\n\nadv_data['Hour'] = adv_data['Timestamp'].apply(lambda time: time.hour)\nadv_data['Month'] = adv_data['Timestamp'].apply(lambda time: time.month)\nadv_data['Day of Week'] = adv_data['Timestamp'].apply(lambda time: time.dayofweek)\nadv_data['Year'] = adv_data['Timestamp'].apply(lambda time: time.year)\nadv_data['Minute'] = adv_data['Timestamp'].apply(lambda time: time.minute)\nadv_data['Second'] = adv_data['Timestamp'].apply(lambda time: time.second)","d2c079b7":"adv_data.info()","8eb2c9c4":"adv_data = adv_data.drop(['Ad Topic Line'],axis=1)\nadv_data = adv_data.drop(['Country'],axis=1)\nadv_data = adv_data.drop(['City'],axis=1)\nadv_data = adv_data.drop(['Timestamp'],axis=1)","712b459d":"adv_data.info()","add8aa2b":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing","ba5cfa46":"adv_data.columns","28300d16":"X = adv_data[['Daily Time Spent on Site', 'Age', 'Area Income',\n       'Daily Internet Usage', 'Male', 'Hour', 'Month',\n       'Day of Week', 'Year', 'Minute', 'Second']]\ny = adv_data['Clicked on Ad']","21b007bd":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","599ac9f8":"scaler = preprocessing.StandardScaler().fit(X_train)","339e244b":"scaler","5d367a3a":"from sklearn.datasets import make_classification\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler","bcf575a3":"X, y = make_classification(random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\npipe = make_pipeline(StandardScaler(), LogisticRegression())\npipe.fit(X_train, y_train)  # apply scaling on training data","63947361":"from sklearn.linear_model import LogisticRegression","1d43d7d1":"lr = LogisticRegression()\nlr.fit(X_train,y_train)","6853187b":"predictions1 = lr.predict(X_test)","666f5133":"print(classification_report(y_test,predictions1))\nprint('\\n')\nprint(confusion_matrix(y_test,predictions1))","8760756c":"from sklearn.metrics import accuracy_score\nacclr = accuracy_score(y_test,predictions1)*100\nacclr","85087014":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)","9f9c90cf":"predictions2 = gnb.predict(X_test)","f8139348":"print(classification_report(y_test,predictions2))\nprint('\\n')\nprint(confusion_matrix(y_test,predictions2))","75e4c180":"from sklearn.metrics import accuracy_score\naccgnb = accuracy_score(y_test,predictions2)*100\naccgnb","560c397f":"from sklearn.linear_model import SGDClassifier","39079245":"sgd = SGDClassifier()\nsgd.fit(X_train,y_train)","b3a0669e":"predictions3 = sgd.predict(X_test)","846fea99":"print(classification_report(y_test,predictions3))\nprint('\\n')\nprint(confusion_matrix(y_test,predictions3))","42eee712":"from sklearn.metrics import accuracy_score\naccsgd = accuracy_score(y_test,predictions3)*100\naccsgd","f4af95d6":"from sklearn.neighbors import KNeighborsClassifier","e3a0f550":"knn = KNeighborsClassifier(n_neighbors = 15)\nknn.fit(X_train,y_train)","126f6bec":"predictions4 = knn.predict(X_test)","f472c5f6":"print(classification_report(y_test,predictions4))\nprint('\\n')\nprint(confusion_matrix(y_test,predictions4))","068f2f3c":"from sklearn.metrics import accuracy_score\naccknn = accuracy_score(y_test,predictions4)*100\naccknn","a51e2ac0":"from sklearn.tree import DecisionTreeClassifier","a926f05c":"dtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)","c512b8db":"predictions5 = dtree.predict(X_test)","4ab16042":"print(classification_report(y_test,predictions5))\nprint('\\n')\nprint(confusion_matrix(y_test,predictions5))","0322bb7b":"from sklearn.metrics import accuracy_score\naccdtree = accuracy_score(y_test,predictions5)*100\naccdtree","46e54a80":"from sklearn.ensemble import RandomForestClassifier","a7951320":"rfc = RandomForestClassifier(n_estimators=200)","07ed7115":"rfc.fit(X_train,y_train)","ee2fa140":"predictions6 = rfc.predict(X_test)","c0e66f63":"print(classification_report(y_test,predictions6))\nprint('\\n')\nprint(confusion_matrix(y_test,predictions6))","a76dfc71":"from sklearn.metrics import accuracy_score\naccrfc = accuracy_score(y_test,predictions6)*100\naccrfc","a1bdd55c":"from sklearn.svm import SVC","1ca4f377":"svc_model = SVC()","2647b269":"svc_model.fit(X_train,y_train)","c29ee930":"predictions7 = svc_model.predict(X_test)","ff503e60":"print(classification_report(y_test,predictions7))\nprint('\\n')\nprint(confusion_matrix(y_test,predictions7))","2500994c":"from sklearn.metrics import accuracy_score\naccSVM = accuracy_score(y_test,predictions7)*100\naccSVM","91672552":"conc = [acclr,\n        accgnb,\n        accsgd,\n        accknn,\n        accdtree,\n        accrfc,\n        accSVM]\n\nans = pd.DataFrame(conc,columns=['ACCURACY OF MODELS'],index=['Logistic Regression',\n                                                              'Na\u00efve Bayes',\n                                                              'Stochastic Gradient Descent',\n                                                              'K-Nearest Neighbors',\n                                                              'Decision Tree',\n                                                              'Random Forest',\n                                                              'Support Vector Machine'])\nans","efcee162":"# 5) Descision Tree:","788dcd5c":"# 6) Random Forest:","630b5a77":"## Let's apply our classification models one by one:","dd44e1b6":"### Insight:\n\nSo, we can see that **Logistic Regression** has the best fit then followed by **Stochastic Gradient Descent, Na\u00efve Bayes, Decision Tree, Random Forest, Support Vector Machine, K-Nearest Neighbors**.","33b70f38":"### Let's use FiveThirtyEight again!","387ff2bb":"Let's print the first 5 rows of the dataset:","eef8dc81":"#### Data Cleaning and preprocessing:","7b067ed5":"# 3. Stochastic Gradient Descent:","a0ce0822":"### Let's create a pairplot with the hue defined by the 'Clicked on Ad' column feature:","091247cb":"Getting the info of the dataset, so as to look for the datatype:","b2dfed76":"## So with this short introduciton, let's start with \u270d\ufe0f writing our code.","2e8e4f9d":"# 4. K-Nearest Neighbours:","b5bec4fc":"### Import the Dataset:","d3e5f72f":"# Total Summary:","dbb7a007":"### Import all the required libraries","edfd1736":"# 2. Na\u00efve Bayes:","cf11d081":"# 7) Support Vector Machine:","9c30c1fa":"# 1) Logistic Regression:","7eec49ee":"# Introduction:\n\n### There are 7 Types of Classification Algorithms:\n1. Logistic Regression\n2. Na\u00efve Bayes\n3. Stochastic Gradient Descent\n4. K-Nearest Neighbours\n5. Decision Tree\n6. Random Forest\n7. Support Vector Machine\n\n## Structured Data Classification:\n\n* Classification can be performed on structured or unstructured data.\n* Classification is a technique where we categorize data into a given number of classes.\n* The main goal of a classification problem is to identify the category\/class to which a new data will fall under.\n\n### Few of the terminologies encountered in machine learning \u2013 classification:\n\n#### a. Classifier\nAn algorithm that maps the input data to a specific category.\n\n#### b. Classification model:\nA classification model tries to draw some conclusion from the input values given for training. It will predict the class labels\/categories for the new data.\n\n#### c. Feature:\nA feature is an individual measurable property of a phenomenon being observed.\n\n#### d. Binary Classification:\nClassification task with two possible outcomes. Eg: Gender classification (Male \/ Female).\n\n#### e. Multi-class classification:\nClassification with more than two classes. In multi class classification each sample is assigned to one and only one target label. Eg: An animal can be cat or dog but not both at the same time.\n\n#### f. Multi-label classification:\nClassification task where each sample is mapped to a set of target labels (more than one class). Eg: A news article can be about sports, a person, and location at the same time.\n\n### The following are the steps involved in building a classification model:\n\n1. Initialize the classifier to be used.\n2. Train the classifier:\nAll classifiers in scikit-learn uses a fit(X, y) method to fit the model(training) for the given train data X and train label y.\n3. Predict the target:\nGiven an unlabeled observation X, the predict(X) returns the predicted label y.\n4. Evaluate the classifier model."}}