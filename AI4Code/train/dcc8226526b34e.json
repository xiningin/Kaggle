{"cell_type":{"a22fc6bb":"code","50865257":"code","d26443b3":"code","95af305f":"code","0dc16d74":"code","c21fd749":"code","a261c0b2":"code","fc92e175":"code","42f1cc3c":"code","7c9f1b52":"code","931aea3c":"code","a7136d93":"code","c7ffa6fa":"code","067ec83b":"code","2b16558f":"code","a4970782":"code","0e7f5a47":"code","52b6ef19":"code","c81d1483":"code","cbcbff0f":"code","75387733":"code","11ef8c2a":"code","329bc2bd":"code","f33e2689":"code","91da0636":"code","6cc0729a":"code","4e21585e":"code","8baed89f":"code","37f3dfb4":"code","06ead76c":"code","ac0a01bf":"code","9cc6ee2d":"markdown","2eaed039":"markdown","c386be5f":"markdown","c480f729":"markdown","3ce0eada":"markdown","ab88730a":"markdown","48edee95":"markdown","a4801c1f":"markdown","3fa6b31b":"markdown","7dbeed29":"markdown","366cf3ae":"markdown","340e0d86":"markdown","21344536":"markdown","8a6a30e5":"markdown"},"source":{"a22fc6bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50865257":"df = pd.read_csv('\/kaggle\/input\/amazon-top-50-bestselling-books-2009-2019\/bestsellers with categories.csv')","d26443b3":"df.sort_values(['Year','Name']).head()","95af305f":"df['Name'].value_counts()","0dc16d74":"df[df['Name']=='StrengthsFinder 2.0']","c21fd749":"df.drop_duplicates('Name', keep='last', inplace=True)\ndf.info()","a261c0b2":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf['User Rating'].hist(bins=50)\nplt.show()","fc92e175":"fig, axs = plt.subplots(2, 2, figsize=(16,10))\n\naxs[0,0].hist(df['Reviews'], bins=50)\naxs[0,1].hist(df['Price'], bins=50)\naxs[1,0].hist(df['Year'], bins=50)\naxs[1,1].hist(df['Genre'], bins=50)\n\nplt.show()\n","42f1cc3c":"plt.scatter('Reviews', 'User Rating', data=df, color='r')\nplt.scatter('Price', 'User Rating', data=df, color='b')\nplt.show()","7c9f1b52":"Fiction = df['Genre']=='Fiction'\nNon_Fiction = df['Genre']=='Non Fiction'\nplt.boxplot([df[Fiction]['User Rating'], df[Non_Fiction]['User Rating']])\nplt.show()","931aea3c":"df_dummies=pd.get_dummies(df, drop_first=True, columns=['Year', 'Genre'])\ndf_dummies.head()","a7136d93":"df_dummies.columns","c7ffa6fa":"import seaborn as sns \n\ncolumn_list = ['User Rating', 'Reviews', 'Price', 'Year_2010',\n       'Year_2011', 'Year_2012', 'Year_2013', 'Year_2014', 'Year_2015',\n       'Year_2016', 'Year_2017', 'Year_2018', 'Year_2019',\n       'Genre_Non Fiction']\n\ncorr_matrix = df_dummies[column_list].corr()\nplt.figure(figsize=(16,12))\nsns.heatmap(corr_matrix, annot=True)\nplt.show()","067ec83b":"X0 = df['Reviews'].values.reshape(-1,1)\ny0 = df['User Rating'].values.reshape(-1,1)","2b16558f":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport statsmodels.api as sm\nfrom scipy import stats","a4970782":"def run_lr(X, y):\n    \n    \"\"\"\n    Run linear regression on the data, calculate RMSE, R_squared and plot regression plot on test data\n    \n    Arg: \n    X: Dataframe of independent variables \n    y: Array of predicted variables\n    \n    Returns:\n    Int\n    Figure and ax objects\n    \n    Raise:\n    ValueError: If X is not array or dataframe or y is not array\n    \"\"\"\n    #Train and fit:\n    reg = LinearRegression()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    y_pred_train = reg.predict(X_train)\n    \n    # Get regression score:\n    R_squared = reg.score(X_test, y_test)\n    RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n\n    print(str(RMSE) +','+str(R_squared) + ': RMSE, R_squared')\n    \n    #Plot\n    fig, axs = plt.subplots(1,2, figsize=(16,4))\n    \n    if X.shape[1] == 1: \n         axs[0].scatter(X_test, y_test)\n         axs[0].plot(X_test, y_pred, color='r')\n         axs[0].set(xlabel='X_test', ylabel = 'y_pred', title ='Regression line on test data')\n    \n         axs[1].scatter(X_train, y_train)\n         axs[1].plot(X_train, y_pred_train, color='r')\n         axs[1].set(xlabel='X_train', ylabel = 'y_pred_train', title ='Regression line on train data')\n    \n         plt.show()\n    \n        \n    else:\n        axs[0].scatter(X_test.iloc[:,0], y_test)\n        axs[0].scatter(X_test.iloc[:,0], y_pred, color='r')\n        axs[0].set(xlabel='X_test', ylabel = 'y_pred', title ='Predicted values on test data')\n    \n        axs[1].scatter(X_train.iloc[:,0], y_train)\n        axs[1].scatter(X_train.iloc[:,0], y_pred_train, color='r')\n        axs[1].set(xlabel='X_train', ylabel = 'y_pred_train', title ='Predicted values on train data')\n    \n        plt.show()\n     \n        \n    \n        \n\n    \nrun_lr(X0, y0)","0e7f5a47":"plt.hist(df['Reviews'], bins=100)\noutlier_limit = (df['Reviews'].mean() + 3*df['Reviews'].std())\nplt.axvline(x=outlier_limit, color='r')\nplt.show()","52b6ef19":"df_no_outlier = df_dummies[df_dummies['Reviews'] <= outlier_limit]","c81d1483":"X1 = df_no_outlier['Reviews'].values.reshape(-1,1)\ny1 = df_no_outlier['User Rating'].values.reshape(-1,1)\nprint(len(X1), len(y1))","cbcbff0f":"run_lr(X1, y1)","75387733":"X2 = df_no_outlier['Price'].values.reshape(-1,1)\ny2 = df_no_outlier['User Rating'].values.reshape(-1,1)\nprint(len(X2), len(y2))","11ef8c2a":"run_lr(X2, y2)","329bc2bd":"X3 = df_no_outlier['Genre_Non Fiction'].values.reshape(-1,1)\ny3 = df_no_outlier['User Rating'].values.reshape(-1,1)\nrun_lr(X3, y3)","f33e2689":"y = df_no_outlier['User Rating'].values.reshape(-1,1)\n\nfull_var_list = ['Year_2010',\n       'Year_2011', 'Year_2012', 'Year_2013', 'Year_2014', 'Year_2015',\n       'Year_2016', 'Year_2017', 'Year_2018', 'Year_2019',\n       'Genre_Non Fiction']\nmain_list = ['Reviews', 'Price']\n\nR2 = []\nrMSE = []\n\nfor var in full_var_list:\n    main_list.append(var)\n    X = df_no_outlier[main_list]\n    \n     #Train and fit:\n    reg = LinearRegression()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    y_pred_train = reg.predict(X_train)\n    \n    # Get regression score:\n    R_squared = reg.score(X_test, y_test)\n    RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n    R2.append(R_squared)\n    rMSE.append(RMSE)\n    \n    print('Add ' + var + ' to the model')\n\n    #Plot\n    fig, axs = plt.subplots(1,2, figsize=(16,4))\n    \n    axs[0].scatter(X_test.iloc[:,0], y_test)\n    axs[0].scatter(X_test.iloc[:,0], y_pred, color='r')\n    axs[0].set(xlabel='X_test', ylabel = 'y_pred', title ='Predicted values on test data')\n    \n    axs[1].scatter(X_train.iloc[:,0], y_train)\n    axs[1].scatter(X_train.iloc[:,0], y_pred_train, color='r')\n    axs[1].set(xlabel='X_train', ylabel = 'y_pred_train', title ='Predicted values on train data')\n    \n    plt.show()    ","91da0636":"plt.figure(figsize=(16,8))\nplt.plot(full_var_list, R2, color='r', label = 'R Squared')\nplt.plot(full_var_list, rMSE, color='b', label = 'RMSE')\nplt.legend()\nplt.title('Impact of adding variables to model', fontsize=16)\nplt.show()","6cc0729a":"#Take a look at our data again: \n\nplt.figure(figsize=(16,4))\ndf_no_outlier.boxplot()\nplt.show()","4e21585e":"df_no_outlier.describe()","8baed89f":"y=df_no_outlier['User Rating']\nX=df_no_outlier.iloc[:, 3:]","37f3dfb4":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nsteps=[('scaler', StandardScaler()), ('Ln', LinearRegression())]\n\npipeline = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\nln_scaled = pipeline.fit(X_train, y_train)\n\ny_pred = pipeline.predict(X_test)\n\nR_squared = ln_scaled.score(X_test, y_test)\nRMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint(str(RMSE) +','+str(R_squared) + ': RMSE, R_squared')","06ead76c":"plt.figure(figsize=(16,4))\nplt.scatter(X_test.iloc[:,0], y_pred, color='#ff6037', alpha= 0.8, label='Predicted')\nplt.scatter(X_test.iloc[:,0], y_test, marker='x', label='Actual')\nplt.title('Visual Regression result')\nplt.xlabel('Num of Reviews')\nplt.ylabel('User Rating')\nplt.legend()\nplt.show()","ac0a01bf":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nimport xgboost as xgb\n\na={'xgb_model__n_estimators':[50, 100, 200],'xgb_model__max_depth':[2,5]}\n\nsteps=[('scaler', StandardScaler()), ('xgb_model', xgb.XGBRegressor())]\n\npipeline = Pipeline(steps)\nrandomized_rmse = RandomizedSearchCV(estimator=pipeline, param_distributions=a, n_iter=5, scoring={'MSE':'neg_mean_squared_error', 'R_squared':'r2'}, refit='MSE', cv=10, verbose=1)\n\nrandomized_rmse.fit(X, y)\n\nprint(randomized_rmse.best_estimator_)\nprint(randomized_rmse.best_score_)","9cc6ee2d":"The correlation do not seems to shows much notable relationship. ","2eaed039":"****Generate dummies","c386be5f":"****More EDA","c480f729":"I'm choosing User Rating as the independent variable, eventhough the correlation is alsmot non-existent (-0.056). This is based on business intuition. Looking from the chart a good variables should be Year. But realisticly, do we really believe that there is an intrinsic relationship between user rating and the year the book got published?","3ce0eada":"Now fiction as independent:","ab88730a":"How about Price as independent variable:","48edee95":"# EDA","a4801c1f":"Looking at the chart above, let's just add all variables to this model then","3fa6b31b":"# Check and drop duplicates","7dbeed29":"# Model 1: Simple Linear Regression","366cf3ae":"# Remove outliers","340e0d86":"#  Model 3: All variables and Let's do some scaling","21344536":"# Model 2: Multiple Linear Regression","8a6a30e5":"****Model 4: Tree"}}