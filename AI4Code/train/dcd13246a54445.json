{"cell_type":{"34d74ec9":"code","f05c1f70":"code","0819baec":"code","86ae7338":"code","4b0645e8":"code","fe123b05":"code","efaf9d61":"code","95f4f36e":"code","4c2a2702":"code","80339a3d":"code","758878ff":"code","d53cdda9":"code","1c6f27b3":"code","7acba2ef":"code","503840c2":"code","21ec2ba1":"code","7458ba70":"code","19c37d22":"code","7ac0150b":"code","886ec34d":"code","84a8a5ae":"code","020cfb50":"code","08c72d8c":"code","96e69267":"code","3e5cd656":"code","23878b88":"code","6796c72a":"code","1a08e18c":"code","69824f05":"code","ddabe678":"code","313de8f3":"code","7a3632b8":"code","0451e31b":"code","4403803d":"code","5e242742":"code","0d33a54e":"code","cfd75411":"code","b869291f":"code","21dc0a78":"code","309c2807":"code","e20eeaac":"code","88702281":"code","42fc008b":"code","bb336838":"code","5530b209":"code","cae8ae9e":"code","e916650b":"code","7e0581f7":"markdown","4bbb72ba":"markdown","673d5e47":"markdown","50a0578f":"markdown","9006d5ef":"markdown","e51b1a23":"markdown","9c4a3301":"markdown","c536dc4d":"markdown","3a52e620":"markdown","697c036e":"markdown","7d3de9db":"markdown","9765826c":"markdown","cd1b3e16":"markdown","df9b91af":"markdown","f5599147":"markdown","4e77715d":"markdown","3f629fc7":"markdown","b2c5c1e5":"markdown","b3d8e999":"markdown","a4ce7159":"markdown","544e9415":"markdown","456c28fa":"markdown","ce35ee54":"markdown","4a1ed691":"markdown","82aa6d76":"markdown","5250419d":"markdown","d9285721":"markdown","96807433":"markdown","d547849f":"markdown","fcc343e1":"markdown"},"source":{"34d74ec9":"!nvidia-smi","f05c1f70":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0819baec":"from collections import Counter\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve,auc\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\nfrom xgboost import XGBClassifier\nfrom imblearn.combine import SMOTETomek\nimport optuna\nimport lightgbm as lgbm\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.impute import KNNImputer","86ae7338":"!pip install impyute\nfrom impyute.imputation.cs import mice","4b0645e8":"# !rm -r kuma_utils\n# !git clone https:\/\/github.com\/analokmaus\/kuma_utils.git","fe123b05":"# import sys\n# sys.path.append(\"kuma_utils\/\")\n# from kuma_utils.preprocessing.imputer import LGBMImputer","efaf9d61":"train = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")","95f4f36e":"train.shape, test.shape","4c2a2702":"train.head()","80339a3d":"test.head()","758878ff":"train.info()","d53cdda9":"# number of unique values\nfor col in train.columns:\n    print(f\"{col}: {train[col].nunique()}\")","1c6f27b3":"# numerical features\nnum_cols = ['song_duration_ms', 'acousticness', 'danceability', 'energy',\n            'instrumentalness', 'liveness', 'loudness', 'speechiness', \n            'tempo', 'audio_valence',]","7acba2ef":"def detect_outliers(df, n, features):\n    \"\"\"\n    Takes a dataframe df of features and returns list of indices containing more than n outliers\n    according to Tukey's rule.\n    Parameters:\n        df: Dataframe\n        n: min number of outliers a column should have\n        features: the columns of the dataframe to consider\n        \n    Returns:\n        the row indices containing outliers\n    \"\"\"\n    outlier_indices = []\n    for col in features:\n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col], 75)\n        IQR = Q3-Q1\n        \n        outlier_list = df[(df[col]<(Q1-(1.5*IQR))) | (df[col]>(Q3+(1.5*IQR)))].index\n        outlier_indices.extend(outlier_list)\n        \n    outlier_indices = Counter(outlier_indices)\n    outliers = (k for k, v in outlier_indices.items() if v>n)\n    return outliers","503840c2":"to_drop = detect_outliers(train, 2, num_cols)","21ec2ba1":"len(train.loc[to_drop])","7458ba70":"train.isnull().sum()","19c37d22":"feat = num_cols + ['song_popularity']\ncorr = train[feat].corr()\ncorr.style.background_gradient(cmap='coolwarm')","7ac0150b":"fig, ax = plt.subplots(2, 5, figsize=(16,8))\nsns.kdeplot(train['song_duration_ms'], color='blue',shade=True, ax = ax[0,0])\nax[0,0].set_title(\"Song Duration\")\nax[0,0].grid()\nsns.kdeplot(train['acousticness'], color='orange', shade=True, ax = ax[0,1])\nax[0,1].set_title(\"Acousticness\")\nax[0,1].grid()\nsns.kdeplot(train['danceability'], color='red', shade=True, ax = ax[0,2])\nax[0,2].set_title(\"Danceability\")\nax[0,2].grid()\nsns.kdeplot(train['energy'], color='yellow', shade=True, ax = ax[0,3])\nax[0,3].set_title(\"Energy\")\nax[0,3].grid()\nsns.kdeplot(train['instrumentalness'], color='green', shade=True, ax = ax[0,4])\nax[0,4].set_title(\"Instrumentalness\")\nax[0,4].grid()\nsns.kdeplot(train['liveness'], color='yellow', shade=True, ax = ax[1,0])\nax[1,0].set_title(\"Liveness\")\nax[1,0].grid()\nsns.kdeplot(train['loudness'], color='red', shade=True, ax = ax[1,1])\nax[1,1].set_title(\"Loudness\")\nax[1,1].grid()\nsns.kdeplot(train['speechiness'], color='blue', shade=True, ax = ax[1,2])\nax[1,2].set_title(\"Speechiness\")\nax[1,2].grid()\nsns.kdeplot(train['tempo'], color='orange', shade=True, ax = ax[1,3])\nax[1,3].set_title(\"Tempo\")\nax[1,3].grid()\nsns.kdeplot(train['audio_valence'], color='purple', shade=True, ax = ax[1,4])\nax[1,4].set_title(\"Audio valence\")\nax[1,4].grid()\nfig.tight_layout()\nplt.show()","886ec34d":"sns.barplot(x=train['audio_mode'], y=train['song_popularity'])","84a8a5ae":"sns.barplot(x=train['time_signature'], y=train['song_popularity'])","020cfb50":"sns.barplot(x=train['key'], y=train['song_popularity'])","08c72d8c":"fig, ax = plt.subplots(1, 3, figsize=(16, 6))\nsns.countplot(train['time_signature'], ax = ax[0])\nax[0].set_title('Time signature')\nsns.countplot(train['audio_mode'], ax = ax[1])\nax[1].set_title('Audio mode')\nsns.countplot(train['key'], ax = ax[2])\nax[2].set_title('Key')\nfig.tight_layout()\nplt.show()","96e69267":"sns.kdeplot(train.song_duration_ms[train['song_popularity'] == 0], color=\"red\", shade=True)\nsns.kdeplot(train.song_duration_ms[train['song_popularity'] == 1], color=\"blue\", shade=True)\nplt.grid()\nplt.legend(['Unpopular','Popular'])\nplt.show()","3e5cd656":"sns.kdeplot(train.danceability[train['song_popularity'] == 0], color=\"red\", shade=True)\nsns.kdeplot(train.danceability[train['song_popularity'] == 1], color=\"blue\", shade=True)\nplt.grid()\nplt.legend(['False','True'])\nplt.show()","23878b88":"sns.scatterplot(x='energy', y='loudness', data=train)","6796c72a":"sns.countplot(train['song_popularity'])\nplt.show()","1a08e18c":"len(train[train[\"song_popularity\"]==0])","69824f05":"len(train[train[\"song_popularity\"]==1])","ddabe678":"feat = [col for col in train.columns if col not in (\"id\", \"song_popularity\")]\n# train_data = pd.DataFrame(data_pipeline.fit_transform(train[feat]))","313de8f3":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []","7a3632b8":"X = train[feat]\ny = train['song_popularity']","0451e31b":"# finding the column index of categorical columns\ncat_cols = [\"key\", \"audio_mode\", \"time_signature\"]\n\ncat_indices = []\nfor col in cat_cols:\n    idx = list(X.columns).index(col)\n    cat_indices.append(idx)","4403803d":"cat_indices","5e242742":"# def run(trial, data=X,target=y):\n    \n#     X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=42)\n    \n#     params = {\n#                 'metric': 'auc', \n#                 'random_state': 22,\n#                 'n_estimators': 4000,\n#                 'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"gbdt\"]),\n#                 'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-3, 10.0),\n#                 'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-3, 10.0),\n#                 'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n#                 'bagging_fraction': trial.suggest_categorical('bagging_fraction', [0.6, 0.7, 0.80]),\n#                 'feature_fraction': trial.suggest_categorical('feature_fraction', [0.6, 0.7, 0.80]),\n#                 'learning_rate': trial.suggest_categorical('learning_rate', [0.005, 0.01, 0.02, 0.03, 0.05, 0.1]),\n#                 'max_depth': trial.suggest_int('max_depth', 2, 12, step=1),\n#                 'num_leaves' : trial.suggest_int('num_leaves', 13, 148, step=5),\n#                 'min_child_samples': trial.suggest_int('min_child_samples', 1, 96, step=5),\n#             }\n    \n#     clf = lgbm.LGBMClassifier(**params)  \n#     clf.fit(X_train, y_train,\n#             eval_set=[(X_valid, y_valid), (X_train, y_train)],\n#             categorical_feature=cat_indices,\n#             callbacks=[lgbm.log_evaluation(period=100), \n#                        lgbm.early_stopping(stopping_rounds=100)\n#                       ],\n#            )\n    \n#     y_proba = clf.predict_proba(X_valid)[:, 1]\n#     auc = roc_auc_score(y_valid, y_proba)\n#     return auc","0d33a54e":"# study = optuna.create_study(direction='maximize')\n# study.optimize(run, n_trials=100)","cfd75411":"# study.best_params","b869291f":"# # We get best parameters as:\n# print(\"Best parameters:\")\n# print(\"*\"*50)\n# for param, val in study.best_trial.params.items():\n#     print(f\"{param} :\\t {val}\")\n# print(\"*\"*50)\n# print(f\"Best AUC score: {study.best_value}\")","21dc0a78":"lgbm_params = {\n    'boosting_type': 'gbdt',\n    'lambda_l1': 0.10332366305772285,\n    'lambda_l2': 0.854062211829853,\n    'colsample_bytree': 1.0,\n    'bagging_fraction': 0.7,\n    'feature_fraction': 0.6,\n    'learning_rate': 0.02,\n    'max_depth': 12,\n    'num_leaves': 103,\n    'min_child_samples': 36\n}","309c2807":"X = train[feat]\ny = train['song_popularity']\n\nX_test =test[feat]","e20eeaac":"mice_X = pd.DataFrame(mice(X.values))\nmice_X.columns = X.columns\nmice_X.head()","88702281":"##imputing test data\nmice_test = pd.DataFrame(mice(X_test.values))\nmice_test.columns = X_test.columns","42fc008b":"scaler = StandardScaler()\nmice_scaled_X = pd.DataFrame(scaler.fit_transform(mice_X))\nmice_scaled_test = pd.DataFrame(scaler.transform(mice_test))\n\nmice_scaled_X.columns = mice_X.columns\nmice_scaled_test.columns= mice_test.columns  ","bb336838":"predictions = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(mice_scaled_X, y)):\n    \n    X_train, X_valid = mice_scaled_X.iloc[train_idx], mice_scaled_X.iloc[val_idx]\n    y_train , y_valid = y.iloc[train_idx], y.iloc[val_idx]\n    \n#     iter_imputer = IterativeImputer(max_iter=10)\n#     knn_imputer = KNNImputer(n_neighbors=1)\n#     lgbm_imputer = LGBMImputer(n_iter=100, verbose=True)\n    \n#     X_train_imp = lgbm_imputer.fit_transform(X_train)\n#     X_valid_imp = lgbm_imputer.transform(X_valid)\n    \n    model = lgbm.LGBMClassifier(**lgbm_params)\n    \n    model.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid), (X_train, y_train)],\n            categorical_feature=cat_indices,\n            callbacks=[lgbm.log_evaluation(period=100), \n                       lgbm.early_stopping(stopping_rounds=500)\n                      ],\n           )\n    \n    valid_preds = model.predict_proba(X_valid)[:,1]\n    \n    score = roc_auc_score(y_valid, valid_preds)\n    scores.append(score)\n    print(\"*\"*200)\n    print(f\"Fold: {fold}, AUC: {score}\")\n    print(\"*\"*200)\n    \n#     X_test_imp = lgbm_imputer.transform(X_test)\n    test_pred = model.predict_proba(mice_scaled_test)[:,1]\n    predictions.append(test_pred)","5530b209":"print(f\"Mean AUC score: {np.mean(scores)}\")","cae8ae9e":"preds = np.mean(np.column_stack(predictions), axis=1)\nsub = pd.DataFrame(columns = ['id', 'song_popularity'])\nsub['id'] = test['id']\nsub['song_popularity'] = preds\nsub","e916650b":"sub.to_csv(\"submission.csv\", index=False)","7e0581f7":"## Making predictions using MICE","4bbb72ba":"### Missing values","673d5e47":"#### song_duration_ms","50a0578f":"## Decide the metric","9006d5ef":"### Detecting outliers\n\nWe use Tukey's rule to detect outliers, i.e. values > (1.5 * IQR) from the quartiles are termed as outliers, where IQR is interquartile range\n\nCredits: https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling#2.2-Outlier-detection","e51b1a23":"Release Notes:\n\n* V1: I try out LightGBM and fine tune it using Optuna\n* V2: I deal with missing values using Iterative Imputer\n* V3: Next I try KNN Imputer\n* V4: I try LGBM Imputer\n* V5\/6\/7: I try MICE","9c4a3301":"### Energy vs loudness","c536dc4d":"### Finetuning using Optuna","3a52e620":"### Correlation plot","697c036e":"## Modelling","7d3de9db":"We don't have any rows with more than 2 outliers","9765826c":"1. 'energy' has a high correlation with 'loudness', and is a bit correlated with 'audio_valence' as well.\n2. 'audio_valence' is correlated with 'danceability' along with 'loudness' and 'energy'.\n3.  No feature has a very high correlation with 'song_popularity'. However subpopulations of these features may be correlated to the target, so we need to explore a bit in detail","cd1b3e16":"### Feature Analysis\n\n['id', 'song_duration_ms', 'acousticness', 'danceability', 'energy',\n       'instrumentalness', 'key', 'liveness', 'loudness', 'audio_mode',\n       'speechiness', 'tempo', 'time_signature', 'audio_valence',\n       'song_popularity']","df9b91af":"#### time signature - 4 unique values","f5599147":"1. Distribution of labels in non-uniform, so it is best to use AUC score.\n2. Since it is binary classification, we can use StratifiedKFold","4e77715d":"#### Feature distribution","3f629fc7":"### LightGBM","b2c5c1e5":"### Categorical variables","b3d8e999":"After finetuning LightGBM using Optuna for 100 trials, we get the best parameters as:\n\n{'boosting_type': 'gbdt',\n 'lambda_l1': 0.10332366305772285,\n 'lambda_l2': 0.854062211829853,\n 'colsample_bytree': 1.0,\n 'bagging_fraction': 0.7,\n 'feature_fraction': 0.6,\n 'learning_rate': 0.02,\n 'max_depth': 12,\n 'num_leaves': 103,\n 'min_child_samples': 36}\n \n See version 1 of this notebook to finetune it yourself!","a4ce7159":"\"key\", \"audio_mode\", and \"time_signature\" are probably categorical variables","544e9415":"skewness observed in acousticness, instrumentalness, liveness, loudness, speechiness, we need to apply transformation","456c28fa":"#### audio mode - binary","ce35ee54":"Refer: [this notebook](https:\/\/www.kaggle.com\/nitishkumar3099\/good-score-using-mice-and-lgbm-classifier#2.-Imputation-using-MICE)","4a1ed691":"Refer: [this notebook](https:\/\/www.kaggle.com\/rnepal2\/lightgbm-optuna-song-popularity-prediction\/notebook)","82aa6d76":"## EDA","5250419d":"### Target overlap","d9285721":"## Outliers and missing values","96807433":"## Load data","d547849f":"#### key - 12 unique values","fcc343e1":"#### danceability"}}