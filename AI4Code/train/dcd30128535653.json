{"cell_type":{"651cdd8f":"code","d88b707e":"code","849284c5":"code","283a230f":"code","b6ecded1":"code","fc3bed58":"code","b54dab6d":"code","1d053a9b":"code","c557dda3":"code","cd74f81b":"code","18bbc24a":"code","b8a7b3fe":"code","7f790f38":"code","39bd45dd":"code","2a984b26":"code","79e4ee86":"code","d8ca1219":"code","94f8702e":"code","9d40c37f":"code","b008b320":"code","a4b4e91e":"code","46d3a41c":"code","e82aad53":"code","31335acc":"code","c7da5c0d":"code","8dbfdd71":"code","14d15270":"code","9627fe3f":"code","7b3df384":"code","701dcbce":"code","880909d9":"code","239ed25d":"code","6373a3f9":"code","a8668745":"code","798ae959":"markdown","bc0a9e3b":"markdown","b5dba9e2":"markdown","7354fc5c":"markdown","622a28af":"markdown","394f1163":"markdown","72a3da60":"markdown","77699bfe":"markdown","eedf2e05":"markdown","faca3e85":"markdown","62c49a14":"markdown","abfb956b":"markdown","7bf35d04":"markdown","cd8f25ae":"markdown","59fa4348":"markdown","0b05320b":"markdown","e9f7b4a9":"markdown","5ec68975":"markdown","d2a90ccf":"markdown","84fdbfad":"markdown","70a1832a":"markdown","b41d060e":"markdown","21f43888":"markdown","8143bbf7":"markdown","23574381":"markdown","a056dcd1":"markdown","139de66e":"markdown","e4072e46":"markdown","2d274a70":"markdown"},"source":{"651cdd8f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Data visulisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nfrom sklearn.impute import SimpleImputer\nimport datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.special import boxcox1p\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","d88b707e":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n# Check the shape of the file\nprint('Shape of train dataset is : {}'.format(train.shape))\nprint('Shape of test dataset is : {}\\n'.format(test.shape))\n","849284c5":"### Drop Id from the data\n#-------------------------------------------------------\ny_train = train['SalePrice']\ntrain_ = train.drop('SalePrice', axis=1)\n\ndata = pd.concat([train_, test], axis=0, ignore_index=True)\ndata_Id = data['Id']\ndata = data.drop('Id', axis=1)\ndata.shape","283a230f":"### Segregate numeric and categorical data\n#-------------------------------------------------------\n\n# Select numerical columns \nnum_cols = data.select_dtypes(include=[np.number])\n# Select categorical columns \ncat_cols = data.select_dtypes(include=[object])","b6ecded1":"num_cols.head()","fc3bed58":"# Sort the numeric variables\nnum_vars = ['LotFrontage','LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', \n            'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'YearRemodAdd',\n           'YrSold', 'YearBuilt', 'GarageYrBlt', 'BsmtUnfSF']\n\n# filter numeric data \nnum_data = num_cols[num_vars]\n\n# Filter out non-numeric from the numeric dataset\ncat_vars = list(set(list(num_cols.columns))- set(num_vars))\n\n# Concat non-numeric with categorical data\ncat_data = pd.concat([cat_cols, num_cols[cat_vars]], axis=1)","b54dab6d":"# Map ordinal labels to the data\nordinal_code_mapping = {'Ex': 9, 'Gd': 7, 'TA': 5, 'Fa': 3, 'Po':1,'GLQ':7, 'Rec':5, 'ALQ':5, 'BLQ':4, 'LwQ':2,'Unf':1, 'Gd':7, 'Av':5, 'Mn':3, 'No':1}\ncat_data = cat_data.replace(ordinal_code_mapping)\n\n# Seperate ordinal data from nominal data\ncat_ordinal_data = cat_data.select_dtypes(include=[np.number])\ncat_nominal_data = cat_data.select_dtypes(include=object)","1d053a9b":"#Convert the year columns to datetime format\nnum_data['YearBuilt'] = pd.to_datetime(num_data['YearBuilt'], format='%Y', errors='ignore').dt.year\nnum_data['YearRemodAdd'] = pd.to_datetime(num_data['YearRemodAdd'], format='%Y', errors='ignore').dt.year\nnum_data['YrSold'] = pd.to_datetime(num_data['YrSold'], format='%Y', errors='ignore').dt.year\nnum_data['GarageYrBlt'] = pd.to_datetime(num_data['YrSold'], format='%Y', errors='ignore').dt.year\n\n\n# Construct new variables for years since - Take the difference between the current year and variable year\nnum_data[\"Yrs_Since_YearBuilt\"] = datetime.datetime.now().year - num_data['YearBuilt']  # substract to get the year delta\nnum_data[\"Yrs_Since_YearRemodAdd\"] = datetime.datetime.now().year- num_data['YearRemodAdd']  # substract to get the year delta\nnum_data[\"Yrs_Since_YrSold\"] = datetime.datetime.now().year - num_data['YrSold']  # substract to get the year delta\nnum_data[\"Yrs_Since_GarageYrBlt\"] = datetime.datetime.now().year - num_data['GarageYrBlt']  # substract to get the year delta\n\n# Delete date columns\nnum_data = num_data.drop(['YearBuilt', 'YearRemodAdd', 'YrSold', 'GarageYrBlt'], axis=1)\nnum_data_cols = num_data.columns","c557dda3":"imputer_num = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer_num = imputer_num.fit(num_data)\nnum_data = pd.DataFrame(imputer_num.transform(num_data), columns = num_data_cols)","cd74f81b":"# Check for missing values\nnan_num_cols = num_data.columns[num_data.isnull().any()].tolist()\nprint('Missing Values(%) in numeric variables after imputing NaNs with mean:\\n------------\\n')\nprint(num_data[nan_num_cols].isnull().sum()\/len(num_data)*100)","18bbc24a":"num_data.shape","b8a7b3fe":"cat_ordinal_data = cat_ordinal_data.fillna(0)\n\n# Check for missing values\nnan_cat_ord_cols = cat_ordinal_data.columns[cat_ordinal_data.isnull().any()].tolist()\nprint('Missing Values(%) after replacing NaNs in the ordinal variables with 0 :\\n------------\\n')\nprint(cat_ordinal_data[nan_cat_ord_cols].isnull().sum()\/len(cat_cols)*100)","7f790f38":"cat_ordinal_data = cat_ordinal_data.drop('MSSubClass', axis=1)\ncat_ordinal_data.shape","39bd45dd":"#Add MSSubClass into nominal data\ncat_nominal_data['MSSubClass'] = data['MSSubClass']\ncat_nominal_data_cols = cat_nominal_data.columns\n# lets replace the NaNs in the nominal variables for the NaNs explanation given in the data description\ncat_nominal_data[\"Alley\"].fillna(\"No alley access\", inplace = True) \ncat_nominal_data[\"GarageType\"].fillna(\"No Garage\", inplace = True) \ncat_nominal_data[\"GarageFinish\"].fillna(\"No Garage\", inplace = True) \ncat_nominal_data[\"Fence\"].fillna(\"No Fence\", inplace = True) \ncat_nominal_data[\"MiscFeature\"].fillna(\"No Misc\", inplace = True) ","2a984b26":"# Check for missing values\nnan_cat_nom_cols = cat_nominal_data.columns[cat_nominal_data.isnull().any()].tolist()\nprint('Missing Values(%) after replacing nominal variables with data description:\\n------------\\n')\nprint(cat_nominal_data[nan_cat_nom_cols].isnull().sum()\/len(cat_cols)*100)\n","79e4ee86":"# Missing variables\ncat_nominal_missingvar = list(cat_nominal_data[nan_cat_nom_cols].isnull().sum().index)\n\n# Lets impute Electrical and MasVnrType for mode\nimputer_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputer_cat = imputer_cat.fit(cat_nominal_data[cat_nominal_missingvar])\ncat_nominal_data[cat_nominal_missingvar] = imputer_cat.transform(cat_nominal_data[cat_nominal_missingvar])","d8ca1219":"# Check for missing values\nnan_cat_nom_cols = cat_nominal_data.columns[cat_nominal_data.isnull().any()].tolist()\nprint('Missing Values(%) after replacing nominal variables with mode:\\n------------\\n')\nprint(cat_nominal_data[nan_cat_nom_cols].isnull().sum()\/len(cat_cols)*100)","94f8702e":"#One-Hot encoding\ncat_nominal_encoded = pd.DataFrame()\nfor variables in cat_nominal_data:\n    dummy_var = pd.get_dummies(cat_nominal_data[variables],prefix=variables)\n    cat_nominal_encoded = pd.concat([cat_nominal_encoded,dummy_var], axis=1)\n\ncat_nominal_encoded.head()","9d40c37f":"# Lets combine both numerical and ordinal variables for our numerical analysis\nnum_ord_data = pd.concat([num_data, cat_ordinal_data], axis=1)\nnum_ord_data.shape","b008b320":"# Plot correlation\nnum_corr = num_ord_data.corr()\n#num_corr.style.background_gradient(cmap='coolwarm')","a4b4e91e":"# Compute the correlation and filter for high correlation\ncorr_mat = num_corr.unstack(level=0).sort_values(kind=\"quicksort\")\ncorr_mat_high = corr_mat[(corr_mat>=0.80) & (corr_mat<1)].drop_duplicates()\ncorr_mat_high","46d3a41c":"a = add_constant(num_ord_data)\nvif_mat = pd.Series([variance_inflation_factor(a.values, i) for i in range(a.shape[1])],index=a.columns)\nvif_mat[vif_mat>3]","e82aad53":"# Drop highly correlated numeric variables\n#num_data_ = num_ord_data.drop(['TotalBsmtSF', 'GrLivArea', 'Fireplaces', 'GarageArea', 'PoolArea', 'GarageCond','Yrs_Since_GarageYrBlt'], axis=1)\nnum_data_ = num_ord_data\n\na = add_constant(num_data_)\nvif_mat = pd.Series([variance_inflation_factor(a.values, i) for i in range(a.shape[1])],index=a.columns)\nvif_vars = vif_mat[vif_mat>3]\nvif_vars","31335acc":"num_data_.head()","c7da5c0d":"\n# Compute the skewness of the numeric variables\nskew_mat = num_data_.skew(axis = 0, skipna = True).sort_values(kind=\"quicksort\")\nskewed_vars = skew_mat[(skew_mat>1)]\n\n\n# Box-Cox transformation\nskewed_vars_ = list(skewed_vars.index)\nskew_mat = boxcox1p(num_data_[skewed_vars_],  0.15).skew(axis = 0, skipna = True).sort_values(kind=\"quicksort\")\nskewed_vars_boxcox = skew_mat[(skew_mat>=1.5)]\nskewed_vars_boxcox","8dbfdd71":"#Lets drop PoolArea\n#num_data_ = num_data_.drop('PoolQC', axis=1)","14d15270":"num_data_.head()","9627fe3f":"plt.scatter(train['LotArea'] , train['SalePrice'])\nplt.xlabel('Lot Area')\nplt.ylabel('Sale Price')","7b3df384":"dfs = [data_Id,num_data_,cat_nominal_encoded]\ndata_ = pd.concat(dfs, axis=1, sort=False)","701dcbce":"colnames = list(data_.columns.str.replace(r'[^a-zA-Z\\d]', \"_\"))\ndata_.columns = colnames\ndata_.head()","880909d9":"train_clean = data_.iloc[:1460,:]\ntest_clean = data_.iloc[1460:,:]\n\nprint(train_clean.shape)\nprint(test_clean.shape)","239ed25d":"train_clean_ = train_clean[train_clean['LotArea']<100000]\ntrain_clean_.shape","6373a3f9":"train_clean_['SalePrice'] = y_train\ntrain_clean_.shape","a8668745":"from pandas import DataFrame\ndf_train = DataFrame(train_clean_, columns= train_clean_.columns)\ndf_train.to_csv('train_.csv', index = None, header=True)\n\n\ndf_test = DataFrame(test_clean, columns= test_clean.columns)\ndf_test.to_csv('test_.csv', index = None, header=True)\n","798ae959":"## Merge all the clean numerical and categorical datasets","bc0a9e3b":"# 2. Numerical Data Analysis","b5dba9e2":"### 2.1. Numerical data","7354fc5c":"## Box-Plots to explore outliers","622a28af":"### 2.2 Ordinal data","394f1163":"All these variables are highly correlated, VIF = 1, means no multicolinearity. It should have been until VIF =1.5 or 2, however above 2 is a cause of concern. Lets drop highly correlated variables and then check vif again.","72a3da60":"1stFlrSF      TotalBsmtSF    0.819530\nTotRmsAbvGrd  GrLivArea      0.825489\nFireplaceQu   Fireplaces     0.850144\nGarageCars    GarageArea     0.882475\nPoolQC        PoolArea       0.922200\nGarageQual    GarageCond     0.943719\ndtype: float64","77699bfe":"We can see there are high correlations\n    \nLets run Variance Inflation Factor to check multicollinearting before dropping the variables. However VIF cannot be conducted with NANs. Lets address missing data and then compute VIF.","eedf2e05":"# 2.1 Correlation analysis","faca3e85":"## Clean the column names for space and characters","62c49a14":"# 2.2 Outlier Analysis","abfb956b":"## Box-Cox transform skewed variables","7bf35d04":"From the categorical data Let us seperate ordinal and nominal categorical data. All the below variables belong to Ordinal data:\n\n* array(['No', 'Gd', 'Mn', 'Av', nan], dtype=object)\nBsmtExposure: Refers to walkout or garden level walls\n\n* array(['GLQ', 'ALQ', 'Unf', 'Rec', 'BLQ', nan, 'LwQ'], dtype=object)\nBsmtFinType1 \/ BsmtFinType2\n\n* array(['TA', 'Fa', nan, 'Gd', 'Po', 'Ex']\nBsmtQual \/ BsmtCond \/ GarageQual \/ GarageCond \/ ExterQual \/ ExterCond \/ BsmtQual \/ BsmtCond \/ KitchenQual \/ GarageQual \/ GarageCond \/ FireplaceQu \/ PoolQC\n","cd8f25ae":"## 3.3. Save the clean dataset for modelling","59fa4348":"## 3.2. Remove Outlier","0b05320b":"## 3.1 Split train and test","e9f7b4a9":"# 2. Missing Data","5ec68975":"As per the data description text, NaNs in all the categorical variables mean that variable doesnt exist in the house for example in NaN in basement or fireplace means no basement or no fireplace. For the ordinal variables, lets code NaNs as 0, assuming for example not having a fireplace and poor quality fireplace being the same.","d2a90ccf":"# 3. Reassemble data frames","84fdbfad":"# 6. FOR A COMPREHENSIVE MODELLING TECHNIQUE PLEASE FOLLOW MY OTHER KERNEL\nhttps:\/\/www.kaggle.com\/snehal1409\/comprehensive-regression-techniques\/","70a1832a":"# 5. Characteristics of the clean data\n\n* **Multicolinearity** : Since we are delaing with highly correlated variables, the standard OLS parameter estimates will have large variance. To counter this, you can use regularization - a technique allowing to decrease this variance at the cost of introducing some bias.\n\n\n* ** Highly skewed **\n\n* ** Outliers (high and low)**","b41d060e":"Most of the variables from the above set are categorical and few numeric variables:\n* LotFrontage\/LotArea\/MasVnrArea\/BsmtFinSF1\/BsmtFinSF2\/TotalBsmtSF\/1stFlrSF\/2ndFlrSF\/\n* LowQualFinSF\/GrLivArea\/GarageArea\/WoodDeckSF\/OpenPorchSF\/EnclosedPorch\/3SsnPorch\/ScreenPorch\n* We will retain the year variables in numeric for feature engineering\n","21f43888":"## Feature engineering (Year variable)\nYear column in itself doesnot have any meaning. Lets convert the time since the year variable, for example years since house built year. This will help us analyze the data more, then replace these variables with new variable.","8143bbf7":"Lets remove the four datapoints, for houses more than 100000 Lot Area. The price for these houses are low of the lot area.","23574381":"## 3.3. Append SalePrice","a056dcd1":"### One-Hot Encoding Nominal Variables","139de66e":"### 2.3. Nominal Data","e4072e46":"## Variance Inflation Factor (vif)","2d274a70":"# 1. Data cleaning"}}