{"cell_type":{"0dcb025a":"code","b89bd2e3":"code","cd49e3b6":"code","1bc8630d":"code","bade3814":"code","b8157bb9":"code","9849c9a0":"code","c9c76819":"code","87582603":"code","ef99fdb3":"code","d6ce280c":"markdown"},"source":{"0dcb025a":"import numpy as np\nimport pandas as pd\n\nimport optuna\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\nfrom IPython.display import display\n\nimport pickle\nfrom joblib import dump, load","b89bd2e3":"DEBUG = False\n\ntrain = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\nsubmission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\n\nif DEBUG:\n    train = train[:80*1000]","cd49e3b6":"%%writefile AddFeatures.py\nimport numpy as np\nimport pandas as pd\n\ndef add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    df = df.fillna(0)\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    return df","1bc8630d":"%%time\n\nfrom AddFeatures import add_features\ntrain = add_features(train)\ntest = add_features(test)","bade3814":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id'], axis=1)\n\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)\n\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])\n\ndump(RS, \"RS.gz\")","b8157bb9":"##############3 use 40 length\ntrain = train.reshape(-1, 2, 40, train.shape[-1])\nprint(train.shape)\ngroups = np.zeros(shape=(train.shape[0], 2))\ngroups[:, 0] = np.array(list(range(1, train.shape[0]+1)))\ngroups[:, 1] = np.array(list(range(1, train.shape[0]+1)))\n\ntrain = train.reshape(-1, 40, train.shape[-1])\ngroups = groups.reshape(-1)\nprint(train.shape, groups.shape)\n\ntest = test.reshape(-1, 2, 40, train.shape[-1])\ntest = test.reshape(-1, 40, train.shape[-1])\nprint(test.shape)\n\ntargets = targets.reshape(-1, 2, 40)\ntargets = targets.reshape(-1, 40)\nprint(targets.shape)\n\ndump(train, \"train.gz\")\ndump(targets, \"targets.gz\")\ndump(groups, \"groups.gz\")","9849c9a0":"EPOCH = 360\nBATCH_SIZE = 1024\nNUM_FOLDS = 5 # 20\u7684\u65f6\u5019\u53ef\u4ee5\u52300.6\nnum_select = 5\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nwith tpu_strategy.scope():\n    kf = GroupKFold(n_splits=NUM_FOLDS)\n    test_preds = []\n    folds_dict = {}\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets, groups=groups)):  \n        folds_dict[fold] = (train_idx, val_idx)\n        \n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        model = keras.models.Sequential([\n            keras.layers.Input(shape=train.shape[-2:]),\n            keras.layers.Bidirectional(keras.layers.LSTM(1024, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(512, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True)),\n            keras.layers.Dense(128, activation='selu'),\n            keras.layers.Dense(1),\n        ])\n        model.compile(optimizer=\"adam\", loss=\"mae\")\n\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=10, verbose=1)\n        es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n        checkpoint_filepath = f\"folds{fold}.hdf5\"\n        sv = keras.callbacks.ModelCheckpoint(\n            checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n            save_weights_only=False, mode='auto', save_freq='epoch',\n            options=None\n        )\n\n        model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr, es, sv])\n        test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())","c9c76819":"dump(folds_dict, \"folds_dict.gz\")\n!ls .\/","87582603":"# submission[\"pressure\"] = sum(test_preds)\/NUM_FOLDS\n# ENSEMBLE FOLDS WITH MEDIAN\nsubmission[\"pressure\"] = np.median(np.vstack(test_preds), axis=0)\nsubmission.to_csv(\".\/submission.csv\", index=False)","ef99fdb3":"df_train = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/train.csv\")\ndf_sub = pd.read_csv(\".\/submission.csv\")\n\nunique_pressures = df_train[\"pressure\"].unique()\nsorted_pressures = np.sort(unique_pressures)\n\ntotal_pressures_len = len(sorted_pressures)\n\ndef find_nearest(prediction):\n    insert_idx = np.searchsorted(sorted_pressures, prediction)\n    if prediction >= sorted_pressures[-1] or insert_idx == total_pressures_len:\n        # If the predicted value is bigger than the highest pressure in the train dataset,\n        # return the max value.\n        return sorted_pressures[-1]\n    elif prediction <= sorted_pressures[0] or insert_idx == 0:\n        # Same control but for the lower bound.\n        return sorted_pressures[0]\n    lower_val = sorted_pressures[insert_idx - 1]\n    upper_val = sorted_pressures[insert_idx]\n    return lower_val if abs(lower_val - prediction) < abs(upper_val - prediction) else upper_val\n\ndf_sub[\"pressure\"] = df_sub[\"pressure\"].apply(find_nearest)\ndf_sub.to_csv(\".\/submission.csv\", index=False)","d6ce280c":"In the previous [notebook](https:\/\/www.kaggle.com\/tenffe\/finetune-of-tensorflow-bidirectional-lstm), we use the length of 80 to train the LSTM model. But we know LSTM would not perform well when the length of sequence is too long.\n\nIn this notebook, we use the length of 40 to train the LSTM model. we hope the little change can get a better result."}}