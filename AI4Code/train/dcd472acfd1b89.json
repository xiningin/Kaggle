{"cell_type":{"ede39ab8":"code","cd94dd51":"code","b33abbb8":"code","43c4bf17":"code","df897db9":"code","48609d61":"code","0a3f54ca":"code","8f0e1fa0":"code","875f75ba":"code","f1e537b5":"code","5909cbcc":"code","b2a8d02b":"code","838b08a2":"code","b1f6b3e0":"code","ce49df9d":"code","bb87be9d":"code","c72e6432":"code","7cfffffb":"code","220818ee":"markdown","47234f55":"markdown","b55312f7":"markdown"},"source":{"ede39ab8":"import math\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom keras import layers\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import preprocess_input\n\nimport os\n%matplotlib inline","cd94dd51":"fashion_train_set_original = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')\nfashion_test_set_original = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv')","b33abbb8":"fashion_train_set_original.head()","43c4bf17":"LABELS = {\n    0: 'T-shirt\/top',\n    1: 'Trouser',\n    2: 'Pullover',\n    3: 'Dress',\n    4: 'Coat',\n    5: 'Sandal',\n    6: 'Shirt',\n    7: 'Sneaker',\n    8: 'Bag',\n    9: 'Ankle boot'\n}","df897db9":"def to_28_by_28_gray_img_matrix(imgrow):\n    \"\"\"Transform a row from the dataset into a numpy matrix of shape (28,28) containing the image's pixel values\"\"\"\n    return imgrow[1:].values.reshape((28,28))","48609d61":"fig, axes = plt.subplots(3, 5)\nfig.set_size_inches(10,4)\nfig.tight_layout()\n\nfor i, ax in enumerate(axes.flatten()):\n    imgrow = fashion_train_set_original.iloc[i]\n    ax.imshow(to_28_by_28_gray_img_matrix(imgrow), cmap='gray')\n    ax.axis('off')\n    ax.set_title(LABELS[imgrow['label']])","0a3f54ca":"def reshape_and_normalize_inputs(X):\n    X = X.reshape((X.shape[0], 28, 28, 1))\n    X = X\/255.\n    return X","8f0e1fa0":"X_train, X_dev, y_train, y_dev = train_test_split(\n    fashion_train_set_original.values[:,1:], \n    fashion_train_set_original.values[:,0], \n    test_size=0.20, \n    random_state=2\n)\nX_train = reshape_and_normalize_inputs(X_train)\nX_dev = reshape_and_normalize_inputs(X_dev)","875f75ba":"# transform the label integers into one-hot vectors\nenc = OneHotEncoder()\nenc.fit(y_train.reshape(-1,1))\ny_train_OH = enc.transform(y_train.reshape(-1,1)).toarray()\ny_dev_OH = enc.transform(y_dev.reshape(-1,1)).toarray()","f1e537b5":"def fashion_mnist_model(input_shape):\n    \"\"\"Create a CNN \n    \n    Args:\n        input_shape (tuple of int): the height, width and channels as a tuple\n    \n    Returns:\n        a trainable CNN model\n    \"\"\"\n\n    X_input = Input(input_shape)\n\n    X = Conv2D(20, (3, 3), strides = (1, 1), name = 'conv0')(X_input)\n    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n    X = Activation('relu')(X)\n\n    X = MaxPooling2D((2, 2), name='max_pool_1')(X)\n    \n    X = Conv2D(46, (2, 2), strides = (1, 1), name = 'conv1')(X)\n    X = BatchNormalization(axis = 3, name = 'bn1')(X)\n    X = Activation('relu')(X)\n    \n    X = Flatten()(X)\n    X = Dense(1000, activation='relu', name='fc1')(X)\n    X = Dense(10, activation='softmax', name='fc2')(X)\n\n    model = Model(inputs = X_input, outputs = X, name='fashion_mnist_model')\n\n    return model","5909cbcc":"model = fashion_mnist_model(X_train.shape[1:])\nmodel.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])","b2a8d02b":"model.fit(x = X_train, y = y_train_OH, epochs = 10, batch_size = 64)","838b08a2":"preds = model.evaluate(x = X_dev, y = y_dev_OH)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","b1f6b3e0":"y_dev_predict = np.argmax(model.predict(X_dev), axis=1)","ce49df9d":"print(confusion_matrix(y_dev, y_dev_predict))","bb87be9d":"plt.imshow((X_dev[(y_dev == 6) & (y_dev_predict != 6)]*255.)[6].reshape((28,28)), cmap='gray')","c72e6432":"X_test = fashion_test_set_original.values[:,1:].copy()\ny_test = fashion_test_set_original.values[:,0].copy()\nX_test = reshape_and_normalize_inputs(X_test)\ny_test_OH = enc.transform(y_test.reshape(-1,1))","7cfffffb":"preds = model.evaluate(x = X_test, y = y_test_OH)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","220818ee":"Let's visualise a few of the images:","47234f55":"## Fashion MNIST - Infer the type of a fashion article with Convolutional Neural Network\n\n\nThe details of this dataset can be found [here](https:\/\/www.kaggle.com\/zalando-research\/fashionmnist), but to summarise, it's composed of 28x28 grayscale images of Zalando articles belonging to one of 10 classes (T-shirt, bag, sneaker,..). The goal of this notebook is to build a ConvNet (using Keras) to infer the type of article given only an image. ","b55312f7":"The model often confuses shirts with T-shirts and vice-versa. Indeed, even for humans, some clothes are hard to label, which makes this an acceptable error rate, especially given how simplistic the model is. "}}