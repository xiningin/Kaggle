{"cell_type":{"bce824d2":"code","84e66356":"code","3fa8b4fa":"code","5bf10b64":"code","b59ae978":"code","9be7eac3":"code","ca7dc738":"code","45810557":"code","ae44747e":"code","8aa87e65":"code","8aaea117":"code","9594841a":"code","e5bb6d06":"code","915ade23":"code","a70de3f0":"code","ed77f044":"code","b21b1768":"code","ff0d5b65":"code","724f04b9":"code","ceed997c":"code","4aa2d159":"code","93232b38":"code","b56bfd29":"code","19385213":"code","5448edc7":"code","043b5269":"code","4ac9dc02":"code","7aaea6b6":"code","eb385f3b":"code","c3327a44":"code","a8610c9d":"code","707ac8c3":"code","56bfbad9":"code","6d95d68e":"code","f530e29c":"code","1e207c7e":"code","bac35b31":"code","099da96f":"code","0ab7b79f":"code","1ac6c182":"code","652ac7a6":"code","e2988648":"code","df267abc":"code","306411a7":"code","66f2500b":"code","5ea3eac4":"code","2c4a2617":"code","56d19c43":"code","91d22073":"code","1d7d0a2e":"code","e5dc3e74":"code","87f9aaf4":"code","7d34c7a8":"code","33e6e8a2":"code","1a455c40":"code","7cf4acbb":"code","d4aabc4a":"code","94fcea6b":"code","43af4885":"code","59a820a2":"code","3783dc79":"code","edf53343":"code","e1e891ec":"code","75514123":"code","51dc084b":"markdown","da596e87":"markdown","93801142":"markdown","27dae6cb":"markdown","7be0890c":"markdown"},"source":{"bce824d2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\nimport seaborn as sns\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\nimport missingno\n%matplotlib inline\nimport pandas as pd\ntrain = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn import metrics","84e66356":"# Loading Data from \ntrain = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')","3fa8b4fa":"train.head()\n# key and time signature should be convert into factor as it will be good to convert into that.\n# id , audio_mode, key, time_signature are boolean type of feature, so we cam ingnore those while ploting in KDE or density plot\n# instrumentalness feature should be convert into log as ","5bf10b64":"train.info()","b59ae978":"# check the target variable for class balanced\ntrain.song_popularity.value_counts()\/len(train.song_popularity) *100","9be7eac3":"# train.song_popularity = train.song_popularity.map({True: 1, False: 0})","ca7dc738":"# train.song_popularity.plot(kind = \"bar\")","45810557":"train.song_popularity =train.song_popularity.astype('bool')\nplt.figure(figsize = (12,10))\nax = sns.countplot(data = train , x = 'song_popularity', palette = 'dark',linewidth=5,edgecolor=sns.color_palette(\"dark\", 3))\nplt.title(\"Target Variable: Song Popularity\",weight = 'bold', color = 'black', size = 25)\nplt.ylabel(\" \")\nplt.xlabel(\" \")\nplt.xticks(size = 15, weight ='bold', color = '#742828')\nplt.yticks(size = 15, weight ='bold', color = '#742828')\nplt.show()","ae44747e":"train.isna().sum()","8aa87e65":"# checking how many null value present in each column\nnull_perctage = dict(zip(train.columns.to_list(), [round((i\/len(train))*100,2) for i in train.isna().sum().to_list()]))\nprint(null_perctage)","8aaea117":"plt.figure(figsize=(12,10))\n# plt.bar(null_perctage.keys(), null_perctage.values(), color ='maroon',width = 0.4,)\nplt.bar(null_perctage.keys(), null_perctage.values(), color ='maroon',width = 0.4,)\nplt.xticks(rotation=90)\nplt.xlabel(\"column name\")\nplt.ylabel(\"NUll %\")\nplt.title(\"Missing Value in DataSet\")\nplt.show()","9594841a":"# we can use import missingno package to find the missing number easily\nmissingno.bar(train)","e5bb6d06":"useful_cols = [col for col in train.columns if col not in ['id', 'song_popularity']]\ncols_dist = [col for col in useful_cols if col not in ['key', 'audio_mode', 'time_signature']]\ncolor_ = [ '#9D2417', '#AF41B4', '#003389' ,'#3C5F41',  '#967032', '#2734DE'] \ncmap_ = ['magma', 'copper', 'crest']\n\n\nplt.figure(figsize= (16,18))\nfor i,col in enumerate(train[useful_cols].columns):\n    rand_col = color_[random.sample(range(6), 1)[0]]\n    plt.subplot(5,3, i+1)\n    if col in cols_dist:\n        \n        sns.kdeplot(train[col], color = rand_col, fill = rand_col )\n        plt.title(col,weight = 'bold', color = rand_col)\n        plt.ylabel(\" \")\n        plt.xlabel(\" \")\n        plt.tight_layout()\n    else:\n        sns.countplot(data = train , x = col, palette = cmap_[random.sample(range(3), 1)[0]] )\n        plt.title(col,weight = 'bold', color = 'black')\n        plt.ylabel(\" \")\n        plt.xlabel(\" \")\n        plt.tight_layout()\n        \nplt.subplot(5,3, 14)\nsns.kdeplot(np.log(train['instrumentalness']), color = rand_col, fill = rand_col )\nplt.title('instrumentalness (log transformed)',weight = 'bold', color = rand_col, size = 17)\nplt.ylabel(\" \")\nplt.xlabel(\" \")\nplt.tight_layout()\nplt.show();","915ade23":"# checking the how indiviual column have a effect on Target Variable\nplt.figure(figsize = (20,18))\ncolor_ = [ '#9D2417', '#AF41B4', '#003389' ,'#3C5F41',  '#967032', '#2734DE'] \nfor i in enumerate(train[cols_dist].columns):\n  rand_col = color_[random.sample(range(6), 1)[0]]\n\n  plt.subplot(4,3,i[0]+1)\n  sns.kdeplot(data = train, x = i[1], hue = 'song_popularity', fill = rand_col, color = rand_col )\n  plt.title (i[1], color = 'maroon')\n  plt.xlabel(\" \")\n  plt.ylabel(\" \")\n  plt.xticks(rotation = 45)\n  plt.tight_layout()","a70de3f0":"# lets check if we can remove any unnecessary column using below mentioned method\nimport numpy as np\ndf = train.copy()\ndf.dropna(inplace=True)\ncor  = df.corr()\nnames = df.columns.to_list()\n# plot correlation matrix\n# plt.figure()\nfig = plt.figure(figsize = (20,18))\nax = fig.add_subplot(111)\ncax = ax.matshow(cor, vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,15,1)\nplt.yticks(rotation=30)\nplt.xticks(rotation=40)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names)\nax.set_yticklabels(names)\nplt.show()","ed77f044":"df.plot(kind='density', subplots=True, layout=(5,4), sharex=False,figsize = (18,14))\nplt.show()","b21b1768":"# df.plot(x = \"energy\",y = \"song_popularity\",kind='hist', subplots=True, layout=(5,4), sharex=False,figsize = (18,14))\n# plt.show()","ff0d5b65":"# target impact on columns\ncols_dist = [col for col in useful_cols if col not in ['key', 'audio_mode', 'time_signature']]\nplt.figure(figsize = (20,18))\ncolor_ = [ '#9D2417', '#AF41B4', '#003389' ,'#3C5F41',  '#967032', '#2734DE'] \nfor i in enumerate(train[cols_dist].columns):\n  rand_col = color_[random.sample(range(6), 1)[0]]\n\n  plt.subplot(4,3,i[0]+1)\n  sns.kdeplot(data = train, x = i[1], hue = 'song_popularity', fill = rand_col, color = rand_col )\n  plt.title (i[1], color = 'maroon')\n  plt.xlabel(\" \")\n  plt.ylabel(\" \")\n  plt.xticks(rotation = 45)\n  plt.tight_layout()","724f04b9":"plt.figure(figsize = (12,12))\ncorr_matrix=train.corr()\n \nmatrix = np.tril(corr_matrix) # take lower correlation matrix\n\n# Draw the heatmap with the mask\nsns.heatmap(corr_matrix.T, mask=matrix, square=True, cmap = 'magma')\nplt.xticks(size = 15,color = 'red')\nplt.yticks(size = 15,  color = 'red');","ceed997c":"# Feature-Feature interaction (correlation)\nplt.figure(figsize = (18,10))\n\nplt.subplot(1,2,1)\nsns.scatterplot(data = train, x = 'energy', y = 'acousticness', color = 'black')\nplt.xticks(size = 15,)\nplt.yticks(size = 15,)\n\nplt.subplot(1,2,2)\nsns.scatterplot(data = train, x = 'energy', y = 'acousticness', hue = 'song_popularity')\nplt.xticks(size = 15,)\nplt.yticks(size = 15,);","4aa2d159":"# Feature-target interactions\n# train.song_popularity =train.song_popularity.astype('object')\n# plt.figure(figsize = (20,20))\n# sns.pairplot(data = train, hue = 'song_popularity',vars=cols_dist)","93232b38":"# train.plot(x= \"song_popularity\", kind = \"bar\")","b56bfd29":"# Loading Data from \ntrain = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\npipeline = Pipeline([\n    ('impute', IterativeImputer(max_iter=10,random_state=42,add_indicator=False)),\n    ('transform_', QuantileTransformer())\n])\ny = train.loc[:,['song_popularity']].values\ntrain = pd.DataFrame(pipeline.fit_transform(train.drop(['id','song_popularity'],axis = 1)),columns = train.drop(['id','song_popularity'],axis = 1).columns)\n\nfrom sklearn.preprocessing import StandardScaler\nfeatures = train.columns.to_list()\n#doNotStd =  [\"key\",\"audio_mode\",\"time_signature\"]\n#features = [i for i in features if i not in doNotStd]\nx = train.loc[:, features].values\n# x = train.iloc[:,:-1]\n# y = train.iloc[:,-1]\n# Standardizing the features on Train Dataset\nscaler = StandardScaler()\nxtrain = scaler.fit_transform(x)\n# discreateTrainValue= train.loc[:, doNotStd].values\n# xtrain = np.concatenate((xtrain,discreateTrainValue),axis=1)","19385213":"from sklearn.decomposition import PCA\npca = PCA() # pca = PCA(n_components=15)\npca.fit_transform(xtrain)\npca_variance = pca.explained_variance_\n\nplt.figure(figsize=(8, 6))\nplt.bar(range(len(pca_variance)), pca_variance, alpha=0.5, align='center', label='individual variance')\nplt.legend()\nplt.ylabel('Variance ratio')\nplt.xlabel('Principal components')\nplt.show()\nprint(pd.DataFrame(pca.components_,columns=train.columns))","5448edc7":"# as per the result of PCA \n# we can drop last 3 colums\n# tempo, time_signature,  audio_valence","043b5269":"# lets drop all the NA from the dataaset\n# then convert all the numeric columns to standard sclar\n# use ML algo to get the output \n# as per accuracy score then try other method to remove the imp feature from dataset","4ac9dc02":"# Loading Data from \ntrain = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')","7aaea6b6":"# # remove ID feature from the dataset as ID is continious \n# train.drop(columns = [\"id\"],axis =1, inplace = True)\n# test.drop(columns = [\"id\"],axis =1, inplace = True)\n# # remove all the missing value as missing value are less than 10%\n# #train.fillna(train.mean(), inplace = True)\n# train.dropna(inplace = True)\n# #test.dropna(inplace = True)\n\n# #test.interpolate(method ='linear', limit_direction ='forward',inplace = True)\n# test.fillna(test.mean(), inplace = True)\n# train.reset_index(drop=True, inplace=True)\n# #test.reset_index(drop=True, inplace=True)","eb385f3b":"pipeline = Pipeline([\n    ('impute', IterativeImputer(max_iter=10,random_state=42,add_indicator=False)),\n    ('transform_', QuantileTransformer())\n])\ny = train.loc[:,['song_popularity']].values\ntrain = pd.DataFrame(pipeline.fit_transform(train.drop(['id','song_popularity'],axis = 1)),columns = train.drop(['id','song_popularity'],axis = 1).columns)\n\ntest = pd.DataFrame(pipeline.transform(test.drop('id',axis = 1)),columns = test.drop('id',axis = 1).columns)","c3327a44":"train.head()","a8610c9d":"test.head()","707ac8c3":"# # As per the PCA result remove the feature which has least variance\n# from sklearn.preprocessing import StandardScaler\n# features = train.columns.to_list()\n# doNotStd =  [\"tempo\",\"time_signature\",\"audio_valence\"] # tempo, time_signature,  audio_valence\n# features = [i for i in features if i not in doNotStd]\n# x = train.loc[:, features].values\n# # Standardizing the features on Train Dataset\n# scaler = StandardScaler()\n# xtrain = scaler.fit_transform(x)\n# # discreateTrainValue = train.loc[:, doNotStd].values\n\n# # xtrain = np.concatenate((xtrain,discreateTrainValue),axis=1)\n# # Standardizing the features on Test Dataset\n# test1 = test.loc[:, features].values\n# # discreateTestValue= test.loc[:, doNotStd].values\n# xtest = scaler.transform(test1)\n# # xtest = np.concatenate((xtest,discreateTestValue),axis=1)\n# print(\"xtrain shape {} xtest shape {}\".format(xtrain.shape,xtest.shape))","56bfbad9":"from sklearn.preprocessing import StandardScaler\nfeatures = train.columns.to_list()\ndoNotStd =  [\"key\",\"audio_mode\",\"time_signature\"]\nfeatures = [i for i in features if i not in doNotStd]\nx = train.loc[:, features].values\n# Standardizing the features on Train Dataset\nscaler = StandardScaler()\nxtrain = scaler.fit_transform(x)\ndiscreateTrainValue= train.loc[:, doNotStd].values\nxtrain = np.concatenate((xtrain,discreateTrainValue),axis=1)\n# Standardizing the features on Test Dataset\ntest1 = test.loc[:, features].values\ndiscreateTestValue= test.loc[:, doNotStd].values\nxtest = scaler.transform(test1)\nxtest = np.concatenate((xtest,discreateTestValue),axis=1)\nprint(\"xtrain shape {} xtest shape {}\".format(xtrain.shape,xtest.shape))","6d95d68e":"# from sklearn.preprocessing import StandardScaler\n# features = train.columns.to_list()\n# #\n# # Separating out the features\n# x = train.loc[:, features].values\n\n# # Separating out the target\n# y = train.loc[:,['song_popularity']].values\n\n# # Standardizing the features\n# scaler = StandardScaler()\n# xtrain = scaler.fit_transform(x)\n# test1 = test.loc[:, features].values\n# xtest = scaler.transform(test1)","f530e29c":"# split the data into train and validation set\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(xtrain, y, test_size=.33, random_state=16,stratify =y)# ,stratify =y","1e207c7e":"# preparing the model\nfrom sklearn.linear_model import LogisticRegression\nlrModel = LogisticRegression()","bac35b31":"%%time\nlrModel.fit(x_train, y_train.ravel())","099da96f":"%%time\nlrPredictions = lrModel.predict(x_test)","0ab7b79f":"%%time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics","1ac6c182":"%%time\ncm = metrics.confusion_matrix(y_test, lrPredictions)\nprint(cm)\n# Use score method to get accuracy of model\nscore_train = lrModel.score(x_train, y_train)\nprint(score_train)\nscore_val = lrModel.score(x_test, y_test)\nprint(score_val)\ncr = metrics.classification_report(y_test, lrPredictions)\nprint(cr)","652ac7a6":"plt.figure(figsize=(9,9))\nsns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\nplt.ylabel('Actual label');\nplt.xlabel('Predicted label');\nall_sample_title = 'Accuracy Score: {0}'.format(score_val)\nplt.title(all_sample_title, size = 15);","e2988648":"# check the output on unseen data\n# from sklearn.impute import SimpleImputer\n# import numpy as np\n# sm = SimpleImputer(missing_values=np.nan,strategy='mean')\n# xtest = sm.fit_transform(xtest)\nlrUnseenPredictions = lrModel.predict(xtest)","df267abc":"# # Simple Logistic Regression Model\n# #%%time\n# lrModel = LogisticRegression()\n# lrModel.fit(x_train, y_train)\n# lrPredictions = lrModel.predict(x_test)\n# cm = metrics.confusion_matrix(y_test, lrPredictions)\n# print(cm)\n# cr = metrics.classification_report(y_test, lrPredictions)\n# print(cr)\n# # predict the unseen datasaet\n# Predictions = lrModel.predict(xtest)","306411a7":"# Modify Logistic Regression Model\nfrom sklearn.linear_model import LogisticRegression\nlrModel = LogisticRegression()\nlrModel = LogisticRegression(C= 00.1,class_weight = \"balanced\")\nlrModel.fit(x_train, y_train)\ntrain_score = lrModel.score(x_train, y_train)\nprint(f\"train score {train_score}\")\nlrPredictions = lrModel.predict(x_test)\ncm = metrics.confusion_matrix(y_test, lrPredictions)\nprint(cm)\nval_score = lrModel.score(x_test, y_test)\nprint(f\"validation score {val_score}\")\ncr = metrics.classification_report(y_test, lrPredictions)\nprint(cr)","66f2500b":"%%time\n# Model 3 Rnadom Forest\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\n# rfModel = RandomForestClassifier()\n# # evaluate the model\n# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# n_scores = cross_val_score(rfModel, x_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n# report performance\n# print('Accuracy: %.3f (%.3f)' % (n_scores.mean(), n_scores.std()))\n'''\nAccuracy: 0.624 (0.008)\nCPU times: user 124 ms, sys: 15.2 ms, total: 139 ms\nWall time: 39.9 s\n'''\nrfModel = RandomForestClassifier(n_estimators=44,max_depth=5,class_weight = \"balanced\",max_features= 'auto',random_state=123)\nrfModel.fit(x_train,y_train)\ntrain_score = rfModel.score(x_train,y_train)\nprint(f\"train score {train_score}\")\nrfPredictions = rfModel.predict(x_test)\nval_score = rfModel.score(x_test,y_test)\nprint(f\"validation score {val_score}\")\ncr = metrics.classification_report(y_test, rfPredictions)\nprint(cr)\ncm = metrics.confusion_matrix(y_test, rfPredictions)\nprint(cm)\n","5ea3eac4":"# from sklearn.model_selection import RandomizedSearchCV\n# import numpy as np\n# rfc = RandomForestClassifier()\n# # number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 1, stop = 50, num = 10)]\n# # number of features at every split\n# max_features = ['auto', 'sqrt']\n\n# # max depth\n# max_depth = [int(x) for x in np.linspace(1, 50, num = 11)]\n# max_depth.append(None)\n# # create random grid\n# random_grid = {\n#  'n_estimators': n_estimators,\n#  'max_features': max_features,\n#  'max_depth': max_depth\n#  }\n# # Random search of parameters\n# rfc_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# # Fit the model\n# rfc_random.fit(x_train, y_train)\n# # print results\n# print(rfc_random.best_params_)","2c4a2617":"# rfc = RandomForestClassifier(n_estimators=44, max_features= 'auto', max_depth= 5,class_weight=\"balanced\")\n# rfc.fit(x_train,y_train)\n# rfc_predict = rfc.predict(x_test)\n# rfc_cv_score = cross_val_score(rfc, x_train, y_train, cv=10, scoring='roc_auc')\n# print(\"=== Confusion Matrix ===\")\n# print(metrics.confusion_matrix(y_test, rfc_predict))\n# print('\\n')\n# print(\"=== Classification Report ===\")\n# print(metrics.classification_report(y_test, rfc_predict))\n# print('\\n')\n# print(\"=== All AUC Scores ===\")\n# print(rfc_cv_score)\n# print('\\n')\n# print(\"=== Mean AUC Score ===\")\n# print(\"Mean AUC Score - Random Forest: \", rfc_cv_score.mean())","56d19c43":"# import numpy as np\n# import pandas as pd\n# from datetime import datetime\n# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n# from sklearn.metrics import roc_auc_score\n# from sklearn.model_selection import StratifiedKFold\n# from xgboost import XGBClassifier\n# '''\n# objective='binary:logistic',\n#                           booster='gbtree',\n#                           eval_metric='auc',\n#                           tree_method='hist',\n#                           grow_policy='lossguide'\n# '''\n# # params = {\n# #         'min_child_weight': [1, 5, 10],\n# #         'gamma': [0.5, 1, 1.5, 2, 5],\n# #         'subsample': [0.6, 0.8, 1.0],\n# #         'colsample_bytree': [0.6, 0.8, 1.0],\n# #         'max_depth': [3, 4, 5]\n# #         }\n\n\n# # xgb = XGBClassifier(learning_rate=0.02, n_estimators=50, objective='binary:logistic',\n# #                     silent=True, nthread=1)\n\n# params = {'gamma': [0.1,0.8,1.6],\n#               'learning_rate': [0.01, 0.1, 0.2],\n#               'max_depth': [5,10,15],\n#               'n_estimators': [50,100,300],\n#               'reg_alpha': [0,0.1,0.2]\n#          }\n\n# xgbc = XGBClassifier(objective='binary:logistic',\n#                           booster='gbtree',\n#                           eval_metric='auc',\n#                           tree_method='hist',\n#                           grow_policy='lossguide',n_jobs=8)","91d22073":"# %%time\n# clf = GridSearchCV(estimator=xgbc, param_grid=params, scoring='roc_auc', return_train_score=True, verbose=1, cv=2,n_jobs=8)#cv=3\n# clf.fit(x_train, y_train.ravel())","1d7d0a2e":"# folds = 3\n# param_comb = 5\n\n# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n# random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(xtrain, y), verbose=3, random_state=1001 )\n# random_search.fit(xtrain, y)","e5dc3e74":"# random_search.best_params_","87f9aaf4":"# xgb = XGBClassifier(learning_rate=0.02, n_estimators=50, objective='binary:logistic',\n#                     silent=True, nthread=1,subsample = 1.0,min_child_weight = 5, max_depth = 5,gamma = 5,colsample_bytree = 0.6)\n# xgb.fit(x_train, y_train)","7d34c7a8":"# train_score = xgb.score(x_train,y_train)\n# print(f\"train score {train_score}\")\n# xgbPredictions = xgb.predict(x_test)\n# val_score = xgb.score(x_test,y_test)\n# print(f\"validation score {val_score}\")\n# cr = metrics.classification_report(y_test, xgbPredictions)\n# print(cr)\n# cm = metrics.confusion_matrix(y_test, xgbPredictions)\n# print(cm)","33e6e8a2":"%%time\n# Model 4 XGBOOST\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier()\nxgb_model.fit(x_train, y_train)\nprint(xgb_model)\nxgb_predict = xgb_model.predict(x_test)\nxgb_cv_score = cross_val_score(xgb_model, x_train, y_train, cv=10, scoring='roc_auc')\nprint(\"=== Confusion Matrix ===\")\nprint(metrics.confusion_matrix(y_test, xgb_predict))\nprint('\\n')\nprint(\"=== Classification Report ===\")\nprint(metrics.classification_report(y_test, xgb_predict))\nprint('\\n')\nprint(\"=== All AUC Scores ===\")\nprint(xgb_cv_score)\nprint('\\n')\nprint(\"=== Mean AUC Score ===\")\nprint(\"Mean AUC Score - XG Boost: \", xgb_cv_score.mean())","1a455c40":"# %%time\n# # Model SVC\n# from sklearn.svm import SVC\n# svc = SVC(C=1.0, random_state=1, kernel='poly')\n# # Fit the model\n# svc.fit(x_train,y_train)\n# svc_predict = svc.predict(x_test)\n# scv_cv_score = cross_val_score(svc, x_train, y_train, cv=10, scoring='roc_auc')\n# print(\"=== Confusion Matrix ===\")\n# print(metrics.confusion_matrix(y_test, svc_predict))\n# print('\\n')\n# print(\"=== Classification Report ===\")\n# print(metrics.classification_report(y_test, svc_predict))\n# print('\\n')\n# print(\"=== All AUC Scores ===\")\n# print(scv_cv_score)\n# print('\\n')\n# print(\"=== Mean AUC Score ===\")\n# print(\"Mean AUC Score - Random Forest: \", scv_cv_score.mean())","7cf4acbb":"# !pip install autoxgb --no-deps","d4aabc4a":"# # importing required packages\n# import warnings\n# warnings.filterwarnings('ignore')\n# import pandas as pd\n# import numpy as np\n# from autoxgb import AutoXGB\n# from sklearn.experimental import enable_iterative_imputer\n# from sklearn.impute import IterativeImputer\n# from sklearn.pipeline import Pipeline\n# from sklearn.preprocessing import QuantileTransformer","94fcea6b":"# train = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\n\n# test = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\n\n# # pipeline to replace the null value and to transform the data\n# pipeline = Pipeline([\n#     ('impute', IterativeImputer(max_iter=10,random_state=42,add_indicator=False)),\n#     ('transform_', QuantileTransformer())\n# ])\n\n# train_tr = pd.DataFrame(pipeline.fit_transform(train.drop('id',axis = 1)),columns = train.drop('id',axis = 1).columns)\n# test_tr = pd.DataFrame(pipeline.fit_transform(test.drop('id',axis = 1)),columns = test.drop('id',axis = 1).columns)\n\n# train_tr['id'] = train['id']\n# test_tr['id'] = test['id']\n\n# train = train_tr.copy()\n# test = test_tr.copy()\n\n# train.to_csv('.\/train.csv', index=False)\n# test.to_csv('.\/test.csv', index=False)","43af4885":"# # parameters for AutoXGB\n\n# # path to training data\n# train_filename = '.\/train.csv'\n\n# # path to output folder to store artifacts\n# output =  '.\/output\/'\n\n# # path to test data. if specified, the model will be evaluated on the test data\n# test_filename ='.\/test.csv'\n\n# # task: classification or regression\n# task = \"classification\"\n\n# # an id column\n# idx = 'id'\n\n# # target columns are list of strings\n# targets = ['song_popularity']\n\n# # features columns are list of strings\n# features = ['song_duration_ms', 'acousticness', 'danceability', 'energy',\n#        'instrumentalness', 'key', 'liveness', 'loudness', 'audio_mode',\n#        'speechiness', 'tempo', 'time_signature', 'audio_valence']\n\n# # use_gpu is boolean\n# use_gpu = False\n\n# # number of folds to use for cross-validation\n# num_folds = 10\n\n# # random seed for reproducibility\n# seed = 42\n\n# # number of optuna trials to run\n# num_trials = 20\n\n# # if fast is set to True, the hyperparameter tuning will use only one fold\n# fast = False","59a820a2":"# # Now its time to train the model!\n# axgb = AutoXGB(\n#     train_filename=train_filename,\n#     output=output,\n#     test_filename=test_filename,\n#     task=task,\n#     idx=idx,\n#     targets=targets,\n#     features=features,\n#     use_gpu=use_gpu,\n#     num_folds=num_folds,\n#     seed=seed,\n#     num_trials=num_trials,\n#     fast=fast,\n# )\n# axgb.train()","3783dc79":"# ! pip install --no-deps autoxgb\n# import warnings\n\n# from pathlib import Path\n# import pandas as pd\n# import numpy as np\n\n# import matplotlib.pyplot as plt\n# plt.style.use('ggplot')\n\n# from autoxgb import AutoXGB\n\n# data = Path(\"\/kaggle\/input\/song-popularity-prediction\/\")\n# train_df = pd.read_csv(data\/'train.csv').sample(frac=1)\n# train_df = train_df.astype({'key': 'category', 'audio_mode': 'category'})\n\n# test_df = pd.read_csv(data\/'test.csv')\n# test_df = test_df.astype({'key': 'category', 'audio_mode': 'category'})\n\n\n# from sklearn.experimental import enable_iterative_imputer\n# from sklearn.impute import IterativeImputer\n\n# imputer = IterativeImputer(random_state=0, max_iter=10, initial_strategy='mean')\n\n# train_im = pd.DataFrame(imputer.fit_transform(train_df))\n# test_im = pd.DataFrame(imputer.fit_transform(test_df))\n\n# train_im.columns = train_df.columns\n# test_im.columns = test_df.columns\n\n# train_df = train_im\n# test_df = test_im\n\n# working_dir = Path('\/kaggle\/working')\n\n# train_df.to_csv(working_dir\/'train_impu.csv', index=False)\n# test_df.to_csv(working_dir\/'test_impu.csv', index=False)\n\n\n# ###############################################################################\n# ### required parameters\n# ###############################################################################\n\n# # path to training data\n\n# train_filename = '\/kaggle\/working\/train_impu.csv'\n# # path to output folder to store artifacts\n# output =  '\/kaggle\/working\/auto_xgb'\n\n# ###############################################################################\n# ### optional parameters\n# ###############################################################################\n\n# # path to test data. if specified, the model will be evaluated on the test data\n# # and test_predictions.csv will be saved to the output folder\n# # if not specified, only OOF predictions will be saved\n# # test_filename = \"test.csv\"\n# test_filename ='\/kaggle\/working\/test_impu.csv'\n\n# # task: classification or regression\n# # if not specified, the task will be inferred automatically\n# # task = \"classification\"\n# # task = \"regression\"\n# task = \"classification\"\n\n# # an id column\n# # if not specified, the id column will be generated automatically with the name `id`\n# # idx = \"id\"\n# idx = 'id'\n\n# # target columns are list of strings\n# # if not specified, the target column be assumed to be named `target`\n# # and the problem will be treated as one of: binary classification, multiclass classification,\n# # or single column regression\n# # targets = [\"target\"]\n# # targets = [\"target1\", \"target2\"]\n# targets = ['song_popularity']\n\n# # features columns are list of strings\n# # if not specified, all columns except `id`, `targets` & `kfold` columns will be used\n# # features = [\"col1\", \"col2\"]\n# features = ['song_duration_ms', 'acousticness', 'danceability', 'energy',\n#        'instrumentalness', 'key', 'liveness', 'loudness', 'audio_mode',\n#        'speechiness', 'tempo', 'time_signature', 'audio_valence']\n\n# # categorical_features are list of strings\n# # if not specified, categorical columns will be inferred automatically\n# # categorical_features = [\"col1\", \"col2\"]\n# # categorical_features = ['key','audio_mode']\n\n# # use_gpu is boolean\n# # if not specified, GPU is not used\n# # use_gpu = True\n# # use_gpu = False\n# use_gpu = True\n\n# # number of folds to use for cross-validation\n# # default is 5\n# num_folds = 5\n\n# # random seed for reproducibility\n# # default is 42\n# seed = 42\n\n# # number of optuna trials to run\n# # default is 1000\n# # num_trials = 1000\n# num_trials = 10\n\n# # time_limit for optuna trials in seconds\n# # if not specified, timeout is not set and all trials are run\n# # time_limit = None\n# time_limit = 600\n\n# # if fast is set to True, the hyperparameter tuning will use only one fold\n# # however, the model will be trained on all folds in the end\n# # to generate OOF predictions and test predictions\n# # default is False\n# # fast = False\n# fast = False\n# # Now its time to train the model!\n# axgb = AutoXGB(\n#     train_filename=train_filename,\n#     output=output,\n#     test_filename=test_filename,\n#     task=task,\n#     idx=idx,\n#     targets=targets,\n#     features=features,\n# #     categorical_features=categorical_features,\n#     use_gpu=use_gpu,\n#     num_folds=num_folds,\n#     seed=seed,\n#     num_trials=num_trials,\n#     time_limit=time_limit,\n#     fast=fast,\n# )\n# axgb.train()\n\n\n\n# test = pd.read_csv('\/kaggle\/working\/auto_xgb\/test_predictions.csv')\n# test\n\n\n# test = pd.read_csv('\/kaggle\/working\/auto_xgb\/test_predictions.csv')\n# import numpy as np\n# sample_submission = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\n# sample_submission['song_popularity'] = test['1.0']\n# sample_submission\n# # test['predictions'] = np.where(test['1.0'] >= 0.5 , 1,0)\n# # test.hist()\n\n# sample_submission.to_csv(\"submission.csv\", index=False)","edf53343":"# test = pd.read_csv('.\/output\/test_predictions.csv')\n# test","e1e891ec":"# # final submission\n# test = pd.read_csv('.\/output\/test_predictions.csv')\n# submission = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\n# submission['song_popularity'] = test['1.0']\n# submission.to_csv(\"submission.csv\", index=False)","75514123":"#Predictions = lrModel.predict(xtest)\n#Predictions = rfModel.predict(xtest)\nPredictions =xgb_model.predict(xtest)\nid = [i for i in range(0,len(Predictions))]\n# sample_submission = pd.DataFrame(columns=[\"id\",\"song_popularity\"])\noutput = pd.DataFrame({\"id\":id, \"song_popularity\":Predictions})\noutput.to_csv(\"submission.csv\",index = False)","51dc084b":"# Now lets use PCA to find the important feature from the train data set\n## PCA: principal component analysis","da596e87":"# Let's build the model","93801142":"# Model Performance","27dae6cb":"# Model 1","7be0890c":"# Predict the UNSEEN data"}}