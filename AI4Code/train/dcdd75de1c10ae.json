{"cell_type":{"a1ada237":"code","24259b80":"code","c26337a3":"code","73e3e89a":"code","00dd33a1":"code","5da3ee5d":"code","46c3ff12":"code","17327b43":"code","c3847ab8":"code","00b175c3":"code","83264054":"code","83295bcc":"code","1e824b6c":"code","89789182":"code","49728cb3":"code","e9f5d00f":"code","2157ddc6":"code","f84b001a":"code","bfa4cffd":"code","e4365896":"code","efaa3d41":"code","1e0f257f":"code","df0bf6bb":"code","1bb36185":"code","caa19354":"code","bd14326f":"code","8b92a386":"code","b3b255df":"code","b1c0d78e":"code","2da8d400":"code","5c752810":"code","c7990bd3":"code","a0effa9e":"code","a7c3b58e":"code","f92308e4":"code","923e1cad":"code","2c16d6e0":"code","cb7fbff4":"code","797356b6":"code","21734a79":"code","4b46847f":"code","6809b881":"code","cd0fbc7f":"code","a9725398":"code","8b3be176":"code","8efd9b72":"code","12155503":"code","b329aece":"code","3aa2f05a":"code","e0d9aefe":"code","423ceb44":"code","f24d5c89":"code","fe533e0e":"code","8aea3651":"code","2e90b041":"code","2d5ff315":"code","a7dbfbde":"code","b407c6b2":"code","2e856e8e":"code","16866894":"code","6457000d":"code","fe84094b":"code","4f6e4a55":"code","20c3e10b":"code","bfea4df6":"code","dbdf202b":"code","481c6584":"code","99b3e450":"code","e0e51a8d":"code","b409ce4d":"code","a6616f2b":"code","40c4c2c3":"code","0ef3b861":"code","a3c40920":"code","ff6bbd94":"code","68303886":"code","4bde40b4":"code","9e33b6f1":"code","d07eea1b":"code","72a88003":"code","797aa0c0":"code","3ee83c3f":"code","b70a8a3f":"code","b8917e41":"code","d3b613b2":"code","e9c2b816":"code","120a2ff4":"code","dfdf5f6c":"code","e5ed3163":"code","cec4773c":"code","87517d18":"code","00faf575":"code","327984df":"code","a09054d0":"code","aea57dd5":"code","228d6e6a":"code","5cdb63ba":"code","d4663059":"code","c36d3729":"code","d9f8beae":"code","a2000486":"code","24a6cd4a":"code","a0334c27":"code","267a520c":"code","c55e0862":"code","1c8a85cb":"code","6709b384":"code","7268a16a":"code","48f035c3":"code","813604e8":"code","e30d81a9":"code","9b4326cc":"code","ebbdb050":"code","56ce1c50":"code","1908304e":"code","c0d0010c":"code","99f6075f":"code","9659b2c1":"code","caf1c2a1":"code","060176a6":"code","6ecb7321":"code","a4c6595f":"code","f3e6681c":"code","b2142e90":"code","b384f117":"code","81a5816f":"code","269430bb":"code","ec763c99":"code","6f8ee887":"code","6f214a0e":"markdown","1a73c50f":"markdown","2b4fe3d3":"markdown","25d5d4ee":"markdown","38b0e87e":"markdown","1d96ee34":"markdown","e1a5cbf9":"markdown","168e2c97":"markdown","0b1e703e":"markdown","365a613b":"markdown","6b0926c0":"markdown","80dccb1f":"markdown","79d94eac":"markdown","f98414f4":"markdown","7fa05172":"markdown","18e7efd4":"markdown","d59c5add":"markdown","5d5bff48":"markdown","ee8ee636":"markdown","7d298cc8":"markdown","caa33f63":"markdown","0df838b8":"markdown","022a0ebf":"markdown","62678a83":"markdown","1a5101db":"markdown","a5123d0b":"markdown","78bc1cbd":"markdown","50a1f0e1":"markdown","a016060d":"markdown","c6ddbe47":"markdown","3d7c62e0":"markdown","fd3273dd":"markdown","4676eb82":"markdown","22313f73":"markdown","e5350b63":"markdown","3d961d5b":"markdown"},"source":{"a1ada237":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import xticks\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\nimport tensorflow as tf\nfrom sklearn.metrics import f1_score\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom keras.preprocessing.sequence import pad_sequences\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Flatten,Embedding,Activation,Dropout\nfrom keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D,LSTM\nfrom keras.layers import Bidirectional","24259b80":"# load train and test datasets\ntrain= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","c26337a3":"# check the no. of rows and columns in the dataset\ntrain.shape, test.shape","73e3e89a":"train.head()","00dd33a1":"train.isnull().sum().sort_values(ascending = False)","5da3ee5d":"sns.countplot(x=train.target)","46c3ff12":"#lets save stopwords in a variable\nstop = list(stopwords.words(\"english\"))","17327b43":"# save list of punctuation\/special characters in a variable\npunctuation = list(string.punctuation)","c3847ab8":"# create an object to convert the words to its lemma form\nlemma = WordNetLemmatizer()","00b175c3":"# lets make a combine list of stopwords and punctuations\nsw_pun = stop + punctuation","83264054":"# function to preprocess the messages\ndef preprocess(tweet):\n    tweet = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", tweet) # removing urls \n    tweet = re.sub('[^\\w]',' ',tweet) # remove embedded special characters in words (for example #earthquake)         \n    tweet = re.sub('[\\d]','',tweet) # this will remove numeric characters\n    tweet = tweet.lower()\n    words = tweet.split()  \n    sentence = \"\"\n    for word in words:     \n        if word not in (sw_pun):  # removing stopwords & punctuations                \n            word = lemma.lemmatize(word,pos = 'v')  # converting to lemma    \n            if len(word) > 3: # we will consider words with length  greater than 3 only\n                sentence = sentence + word + ' '             \n    return(sentence)","83295bcc":"# apply preprocessing functions on the train and test datasets\ntrain['text'] = train['text'].apply(lambda s : preprocess(s))\ntest ['text'] = test ['text'].apply(lambda s : preprocess(s))","1e824b6c":"# function to remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","89789182":"# applying the function on the train and the test datasets\ntrain['text'] = train['text'].apply(lambda s : remove_emoji(s))\ntest ['text'] = test ['text'].apply(lambda s : remove_emoji(s))\n","49728cb3":"# function to create vocab\nfrom collections import Counter\ndef create_vocab(df):\n    vocab = Counter()\n    for i in range(df.shape[0]):\n        vocab.update(df.text[i].split())\n    return(vocab)","e9f5d00f":"# concatenate training and testing datasets\nmaster=pd.concat((train,test)).reset_index(drop=True)\n\n# call vocabulary creation function on master dataset\nvocab = create_vocab(master)\n\n# lets check the no. of words in the vocabulary\nlen(vocab)","2157ddc6":"# lets check the most common 50 words in the vocabulary\nvocab.most_common(50)\n","f84b001a":"# create the final vocab by considering words with more than one occurence\nfinal_vocab = []\nmin_occur = 2\nfor k,v in vocab.items():\n    if v >= min_occur:\n        final_vocab.append(k)","bfa4cffd":"# lets check the no. of the words in the final vocabulary\nvocab_size = len(final_vocab)\nvocab_size","e4365896":"# function to filter the dataset, keep only words which are present in the vocab\ndef filter(tweet):\n    sentence = \"\"\n    for word in tweet.split():  \n        if word in final_vocab:\n            sentence = sentence + word + ' '\n    return(sentence)","efaa3d41":"# apply filter function on the train and test datasets\ntrain['text'] = train['text'].apply(lambda s : filter(s))\ntest ['text'] = test ['text'].apply(lambda s : filter(s))","1e0f257f":"# lets take a look at the update training dataset\ntrain.text.head()","df0bf6bb":"# the different units into which you can break down text (words, characters, or n-grams) are called tokens, \n# and breaking text into such tokens is called tokenization, this can be achieved using Tokenizer in Keras\n\nfrom keras.preprocessing.text import Tokenizer\n# fit a tokenizer\ndef create_tokenizer(lines):\n    # num_words = vocab_size will create a tokenizer,configured to only take into account the vocab_size(6025)\n    tokenizer = Tokenizer(num_words=vocab_size)\n    # Build th word index, Turns strings into lists of integer indices\n    tokenizer.fit_on_texts(lines) \n    return tokenizer","1bb36185":"# create and apply tokenizer on the training dataset\ntokenizer = create_tokenizer(train.text)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","caa19354":"# converting texts into vectors\ntrain_text = tokenizer.texts_to_matrix(train.text, mode = 'freq')","bd14326f":"# Test train split \nX_train, X_test, y_train, y_test = train_test_split(train_text, train.target, test_size = 0.2, random_state = 42)","8b92a386":"# function to calculate f1 score for each epoch\nimport keras.backend as K\ndef get_f1(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val\n","b3b255df":"# define the model\ndef define_model(n_words):\n    # define network\n    model = Sequential()\n    model.add(Dense(1024, input_shape=(n_words,), activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(512,activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(256,activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    # compile network\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics = [get_f1])\n    \n    # summarize defined model\n    model.summary()\n    return model","b1c0d78e":"X_train.shape","2da8d400":"callbacks_list = [EarlyStopping(monitor='get_f1',patience=10,),\nModelCheckpoint(filepath='.\/NN.h5',monitor='val_loss',save_best_only=True)\n]","5c752810":"# create the model\nn_words = X_train.shape[1]\nmodel = define_model(n_words)","c7990bd3":"#fit network\nhistory = model.fit(X_train,y_train,epochs=100,verbose=2,callbacks=callbacks_list,validation_split=0.2)","a0effa9e":"acc = history.history['get_f1']\nval_acc = history.history['val_get_f1']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","a7c3b58e":"import keras\n\ndependencies = {\n    'get_f1': get_f1\n}\n\n\n# load the model from disk\nloaded_model_NN = keras.models.load_model('.\/NN.h5',custom_objects=dependencies)","f92308e4":"# prediction on the test dataset\n#X_test_Set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\ny_pred = loaded_model_NN.predict_classes(X_test)","923e1cad":"# important metrices\nprint(classification_report(y_test, y_pred))","2c16d6e0":"test_id = test.id\ntest.drop([\"id\",\"location\",\"keyword\"],1,inplace = True)\n\n# apply tokenizer on the test dataset\ntest_set = tokenizer.texts_to_matrix(test.text, mode = 'freq')","cb7fbff4":"# make predictions on the test dataset\ny_test_pred = loaded_model_NN.predict_classes(test_set)","797356b6":"# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.head()","21734a79":"sub.to_csv('submission_NN.csv',index=False)","4b46847f":"from keras.layers import Embedding\n# The Embedding layer takes at least two arguments: the number of possible tokens (here, 5,000: 1 + maximum word index)\n#and the dimensionality of the embeddings (here, 64).\n#embedding_layer = Embedding(5000, 64)","6809b881":"# Number of words to consider as features\nmax_features = vocab_size\n\n# Cuts off the text after this number of words (among the max_features most common words)\nmaxlen = 100","cd0fbc7f":"# create and apply tokenizer on the training dataset\ntokenizer = create_tokenizer(train.text)","a9725398":"from keras import preprocessing\n# conver text to sequences\nsequences = tokenizer.texts_to_sequences(train.text)\n#print(sequences)","8b3be176":"# Turns the lists of integers into a 2D integer tensor of shape (samples, maxlen), padding shorter sequences with 0s\ntrain_text = preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)","8efd9b72":"# Test train split \nX_train, X_test, y_train, y_test = train_test_split(train_text, train.target, test_size = 0.2, random_state = 42)","12155503":"# build the model\nmodel = Sequential()\n# Specifies the maximum input length to the Embedding layer so you can later flatten the embedded inputs. \n\n# After the Embedding layer, the activations have shape (samples, maxlen, 8)\nmodel.add(Embedding(vocab_size, 8, input_length=maxlen))\n\n# Flattens the 3D tensor of embeddings into a 2D tensor of shape (samples, maxlen * 8)\nmodel.add(Flatten())\n\n# Dense layer for classification\nmodel.add(Dense(1, activation='sigmoid'))\n\n# compile the model\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[get_f1])\nmodel.summary()","b329aece":"callbacks_list = [\nEarlyStopping(\nmonitor='get_f1',\npatience=1,\n),\nModelCheckpoint(filepath='.\/embd.h5',monitor='val_loss',save_best_only=True)\n]","3aa2f05a":"# train the model\nhistory = model.fit(X_train, y_train,\nepochs=100,\nbatch_size=32,\ncallbacks=callbacks_list,\nvalidation_split=0.2)","e0d9aefe":"# check model performance\nacc = history.history['get_f1']\nval_acc = history.history['val_get_f1']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","423ceb44":"dependencies = {\n    'get_f1': get_f1\n}\n\n\n# load the model from disk\nloaded_model_embd = keras.models.load_model('.\/embd.h5',custom_objects=dependencies)","f24d5c89":"# prediction on the test dataset\ny_pred = loaded_model_embd.predict_classes(X_test)\n\n# important metrices\nprint(classification_report(y_test, y_pred))","fe533e0e":"# conver text to sequences\nsequences = tokenizer.texts_to_sequences(test.text)\n# Turns the lists of integers into a 2D integer tensor of shape (samples, maxlen)\ntest_text = preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)","8aea3651":"# make predictions on the test dataset\ny_test_pred = loaded_model_embd.predict_classes(test_text)\n\n# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.head()","2e90b041":"sub.to_csv('submission_embedding.csv',index=False)","2d5ff315":"# Considers only the top 5000 words in the dataset\nmax_words = 5000","a7dbfbde":"import os\nglove_dir = \"..\/input\/glove6b100dtxt\/\"\nembeddings_index = {}\nf = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))","b407c6b2":"embedding_dim = 100\n\nembedding_matrix = np.zeros((max_words, embedding_dim))\n\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector # Words not found in the embedding index will be all zeros.","2e856e8e":"# lets use the same model architecture we used earlier\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","16866894":"model.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = False","6457000d":"# Compile the model\nmodel.compile(optimizer='rmsprop',\nloss='binary_crossentropy',\nmetrics=[get_f1])","fe84094b":"callbacks_list = [\nEarlyStopping(\nmonitor='get_f1',\npatience=1,\n),\nModelCheckpoint(filepath='.\/pre_embd.h5',monitor='val_loss',save_best_only=True)\n]","4f6e4a55":"# train the model\nhistory = model.fit(X_train, y_train,\nepochs=100,\nbatch_size=32,\ncallbacks=callbacks_list,\nvalidation_split=0.2)","20c3e10b":"# check model performance\nacc = history.history['get_f1']\nval_acc = history.history['val_get_f1']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","bfea4df6":"dependencies = {\n    'get_f1': get_f1\n}\n\n\n# load the model from disk\nloaded_model_pre_embd = keras.models.load_model('.\/pre_embd.h5',custom_objects=dependencies)","dbdf202b":"# prediction on the test dataset\n#X_test_Set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\ny_pred = loaded_model_pre_embd.predict_classes(X_test)\n\n# important metrices\nprint(classification_report(y_test, y_pred))","481c6584":"# make predictions on the test dataset\ny_test_pred = loaded_model_pre_embd.predict_classes(test_text)\n\n# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.head()","99b3e450":"sub.to_csv('submission_pre_embedding.csv',index=False)","e0e51a8d":"from keras.layers import Embedding, SimpleRNN\nmodel = Sequential()\nmodel.add(Embedding(max_words, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","b409ce4d":"callbacks_list = [\nEarlyStopping(\nmonitor='get_f1',\npatience=1,\n),\nModelCheckpoint(filepath='.\/SRNN.h5',monitor='val_loss',save_best_only=True)\n]","a6616f2b":"model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[get_f1])\n\nhistory = model.fit(X_train, y_train,\nepochs=100,\nbatch_size=128,\ncallbacks=callbacks_list,\nvalidation_split=0.2)","40c4c2c3":"# check model performance\nacc = history.history['get_f1']\nval_acc = history.history['val_get_f1']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","0ef3b861":"dependencies = {\n    'get_f1': get_f1\n}\n\n\n# load the model from disk\nloaded_model_SRNN = keras.models.load_model('.\/SRNN.h5',custom_objects=dependencies)","a3c40920":"# prediction on the test dataset\n#X_test_Set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\ny_pred = loaded_model_SRNN.predict_classes(X_test)\n\n# important metrices\nprint(classification_report(y_test, y_pred))","ff6bbd94":"# make predictions on the test dataset\ny_test_pred = loaded_model_SRNN.predict_classes(test_text)\n\n# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.head()","68303886":"sub.to_csv('submission_SRNN.csv',index=False)","4bde40b4":"from keras.layers import Embedding, SimpleRNN\nmodel = Sequential()\nmodel.add(Embedding(max_words, 32))\nmodel.add(SimpleRNN(32,return_sequences=True))\nmodel.add(SimpleRNN(32,return_sequences=True))\nmodel.add(SimpleRNN(32,return_sequences=True))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","9e33b6f1":"callbacks_list = [\nEarlyStopping(\nmonitor='get_f1',\npatience=1,\n),\nModelCheckpoint(filepath='.\/STRNN.h5',monitor='val_loss',save_best_only=True)\n]","d07eea1b":"model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[get_f1])\n\nhistory = model.fit(X_train, y_train,\nepochs=100,\nbatch_size=128,\ncallbacks=callbacks_list,\nvalidation_split=0.2)","72a88003":"# check model performance\nacc = history.history['get_f1']\nval_acc = history.history['val_get_f1']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","797aa0c0":"dependencies = {\n    'get_f1': get_f1\n}\n\n\n# load the model from disk\nloaded_model_STRNN = keras.models.load_model('.\/STRNN.h5',custom_objects=dependencies)","3ee83c3f":"# prediction on the test dataset\n#X_test_Set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\ny_pred = loaded_model_STRNN.predict_classes(X_test)\n\n# important metrices\nprint(classification_report(y_test, y_pred))","b70a8a3f":"# make predictions on the test dataset\ny_test_pred = loaded_model_STRNN.predict_classes(test_text)\n\n# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.head()","b8917e41":"sub.to_csv('submission_stackRNN.csv',index=False)","d3b613b2":"from keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(LSTM(32))\nmodel.add(Dense(1, activation='sigmoid'))\n","e9c2b816":"model.compile(optimizer='rmsprop',\nloss='binary_crossentropy',\nmetrics=[get_f1])","120a2ff4":"callbacks_list = [\nEarlyStopping(\nmonitor='get_f1',\npatience=1,\n),\nModelCheckpoint(filepath='.\/LSTM.h5',monitor='val_loss',save_best_only=True)\n]","dfdf5f6c":"history = model.fit(X_train, y_train,\nepochs=100,\nbatch_size=128,\ncallbacks=callbacks_list,\nvalidation_split=0.2)","e5ed3163":"# check model performance\nacc = history.history['get_f1']\nval_acc = history.history['val_get_f1']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","cec4773c":"dependencies = {\n    'get_f1': get_f1\n}\n\n\n# load the model from disk\nloaded_model_LSTM = keras.models.load_model('.\/LSTM.h5',custom_objects=dependencies)","87517d18":"# prediction on the test dataset\n#X_test_Set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\ny_pred = loaded_model_LSTM.predict_classes(X_test)\n\n# important metrices\nprint(classification_report(y_test, y_pred))","00faf575":"# make predictions on the test dataset\ny_test_pred = loaded_model_LSTM.predict_classes(test_text)\n\n# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.head()","327984df":"sub.to_csv('LSTM.csv',index=False)","a09054d0":"model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dense(1, activation='sigmoid'))","aea57dd5":"model.compile(optimizer='rmsprop',\nloss='binary_crossentropy',\nmetrics=[get_f1])","228d6e6a":"callbacks_list = [\nEarlyStopping(\nmonitor='get_f1',\npatience=1,\n),\nModelCheckpoint(filepath='.\/BILSTM.h5',monitor='val_loss',save_best_only=True)\n]","5cdb63ba":"history = model.fit(X_train, y_train,\nepochs=100,\nbatch_size=128,\ncallbacks=callbacks_list,\nvalidation_split=0.2)","d4663059":"# check model performance\nacc = history.history['get_f1']\nval_acc = history.history['val_get_f1']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","c36d3729":"dependencies = {\n    'get_f1': get_f1\n}\n\n\n# load the model from disk\nloaded_model_BILSTM = keras.models.load_model('.\/BILSTM.h5',custom_objects=dependencies)","d9f8beae":"# prediction on the test dataset\n#X_test_Set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\ny_pred = loaded_model_BILSTM.predict_classes(X_test)\n\n# important metrices\nprint(classification_report(y_test, y_pred))","a2000486":"# make predictions on the test dataset\ny_test_pred = loaded_model_BILSTM.predict_classes(test_text)\n\n# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.head()","24a6cd4a":"sub.to_csv('BiLSTM.csv',index=False)","a0334c27":"from keras.layers import GRU\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(GRU(32))\nmodel.add(Dense(1, activation='sigmoid'))","267a520c":"model.compile(optimizer='rmsprop',\nloss='binary_crossentropy',\nmetrics=[get_f1])","c55e0862":"callbacks_list = [\nEarlyStopping(\nmonitor='get_f1',\npatience=1,\n),\nModelCheckpoint(filepath='.\/GRU.h5',monitor='val_loss',save_best_only=True)\n]","1c8a85cb":"history = model.fit(X_train, y_train,\nepochs=100,\nbatch_size=128,\ncallbacks=callbacks_list,\nvalidation_split=0.2)","6709b384":"# check model performance\nacc = history.history['get_f1']\nval_acc = history.history['val_get_f1']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","7268a16a":"dependencies = {\n    'get_f1': get_f1\n}\n\n# load the model from disk\nloaded_model_GRU = keras.models.load_model('.\/GRU.h5',custom_objects=dependencies)","48f035c3":"# prediction on the test dataset\n#X_test_Set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\ny_pred = loaded_model_GRU.predict_classes(X_test)\n\n# important metrices\nprint(classification_report(y_test, y_pred))","813604e8":"# make predictions on the test dataset\ny_test_pred = loaded_model_GRU.predict_classes(test_text)\n\n# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.head()","e30d81a9":"sub.to_csv('GRU.csv',index=False)","9b4326cc":"from keras.layers import GRU\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32))\nmodel.add(Dense(1, activation='sigmoid'))","ebbdb050":"model.compile(optimizer='rmsprop',\nloss='binary_crossentropy',\nmetrics=[get_f1])","56ce1c50":"callbacks_list = [\nEarlyStopping(\nmonitor='get_f1',\npatience=1,\n),\nModelCheckpoint(filepath='.\/SGRU.h5',monitor='val_loss',save_best_only=True)\n]","1908304e":"history = model.fit(X_train, y_train,\nepochs=100,\nbatch_size=128,\ncallbacks=callbacks_list,\nvalidation_split=0.2)","c0d0010c":"# check model performance\nacc = history.history['get_f1']\nval_acc = history.history['val_get_f1']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","99f6075f":"dependencies = {\n    'get_f1': get_f1\n}\n\n\n# load the model from disk\nloaded_model_SGRU = keras.models.load_model('.\/SGRU.h5',custom_objects=dependencies)","9659b2c1":"# prediction on the test dataset\n#X_test_Set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\ny_pred = loaded_model_SGRU.predict_classes(X_test)\n\n# important metrices\nprint(classification_report(y_test, y_pred))","caf1c2a1":"# make predictions on the test dataset\ny_test_pred = loaded_model_SGRU.predict_classes(test_text)\n\n# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.head()","060176a6":"sub.to_csv('SGRU.csv',index=False)","6ecb7321":"from keras.layers import GRU\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(GRU(32,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))\nmodel.add(GRU(32))\nmodel.add(Dense(1, activation='sigmoid'))","a4c6595f":"model.compile(optimizer='rmsprop',\nloss='binary_crossentropy',\nmetrics=[get_f1])","f3e6681c":"callbacks_list = [\nEarlyStopping(\nmonitor='get_f1',\npatience=1,\n),\nModelCheckpoint(filepath='.\/DSGRU.h5',monitor='val_loss',save_best_only=True)\n]","b2142e90":"history = model.fit(X_train, y_train,\nepochs=100,\nbatch_size=128,\ncallbacks=callbacks_list,\nvalidation_split=0.2)","b384f117":"# check model performance\nacc = history.history['get_f1']\nval_acc = history.history['val_get_f1']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","81a5816f":"dependencies = {\n    'get_f1': get_f1\n}\n\n\n# load the model from disk\nloaded_model_DSGRU = keras.models.load_model('.\/DSGRU.h5',custom_objects=dependencies)","269430bb":"# prediction on the test dataset\n#X_test_Set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\ny_pred = loaded_model_DSGRU.predict_classes(X_test)\n\n# important metrices\nprint(classification_report(y_test, y_pred))","ec763c99":"# make predictions on the test dataset\ny_test_pred = loaded_model_DSGRU.predict_classes(test_text)\n\n# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.head()","6f8ee887":"sub.to_csv('GRUDropOut.csv',index=False)","6f214a0e":"### Data Cleaning","1a73c50f":"There is a huge difference between training and validation accuracies and losses","2b4fe3d3":"Model's performance not yet improved, lets stack some layers.","25d5d4ee":"# Model Building & Evaluation","38b0e87e":"### 5. Stack multiple SimpleRNN layers","1d96ee34":"### LOADING THE GLOVE EMBEDDINGS IN THE MODEL\n\nThe Embedding layer has a single weight matrix: a 2D float matrix where each entry i is the word vector meant to be associated with index i. Simple enough. \n\nLoad the GloVe matrix we prepared into the Embedding layer, the first layer in the model","e1a5cbf9":"lets consider only those words which have appeared more than once in the corpus\n","168e2c97":"Lets create our own vocabulary","0b1e703e":"### 7. Bi-Direction LSTM","365a613b":"# Introduction\n\nThis particular challenge is perfect for data scientists looking to get started with Natural Language Processing.\n\nCompetition Description\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster. \n\nTake an example:\nThe author explicitly uses the word \u201cABLAZE\u201d but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it\u2019s less clear to a machine.\n\nIn this competition, you\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. You\u2019ll have access to a dataset of 10,000 tweets that were hand classified.","6b0926c0":"### 3. Neural Network with pre trained Embedding Layer(GLOVE)","80dccb1f":"### 4.SIMPLE RNN","79d94eac":"vocab size reduced drastically from 16k to 6k","f98414f4":"# Stacked GRU with Dropouts","7fa05172":"Now we will apply texts_to_matrix() function to convert text into vectors.\n\nThe texts_to_matrix() function on the Tokenizer can be used to create one vector per document provided per input. The length of the vectors is the total size of the vocabulary, which is 6025 here (we passed 6025 as num_words into tokenizer)\n\nThis function provides a suite of standard bag-of-words model text encoding schemes that can be provided via a mode argument to the function.\n\nThe modes available include:\n\n* \u2018binary\u2018: Whether or not each word is present in the document. This is the default.\n* \u2018count\u2018: The count of each word in the document.\n* \u2018tfidf\u2018: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n* \u2018freq\u2018: The frequency of each word as a ratio of words within each document.","18e7efd4":"We\u2019ll build an embedding matrix that you can load into an Embedding layer. \n\nIt must be a matrix of shape (max_words, embedding_dim), where each entry i contains the embedding_dim-dimensional vector for the word of index i in the reference word index (built during tokenization). \n\nNote that index 0 isn\u2019t supposed to stand for any word or token\u2014it\u2019s a placeholde","d59c5add":"# Vocabulary creation\n","5d5bff48":"Didn't help, lets try LSTM","ee8ee636":"# GRU","7d298cc8":"### 1. Neural Network","caa33f63":"Now lets apply this vocab on our train and test datasets, we will keep only those words in training and testing datasets which appear in the vocabulary","0df838b8":"Embedding layer that learnt embeddings with the model training proved to be better than pre trained embedding.","022a0ebf":"We will create an Artificial Neural Network, this competition is evaluated on f1 scores,which is not shown by default after every epoch, so lets create a function to  achieve the same.","62678a83":"In order to get accurate results from the predictive model, we need to remove these stop words & punctuations.\n\nApart from removing these stopwords & puncuations, we would also convert all the messages in lowercase so that words like \"Go\" & \"go\" can be treated as same word and not different words.\n\nWe will also convert the words to its lemma form (for example, lemma of word \"running\" would be run), converting words to their lemmas would also help improving the predictive power of our model.\n\nWe would also remove embedded special characters from the tweets, for example, #earthquake should be replaced by earthquake\n\nWe also need to remove the \"URLs\" from the tweets\n\nAnd then finally we remove the digits from the tweets\n\nLets write a small function \"preprocess\" to achive all these tasks.","1a5101db":"We got to a f1 score of 81%, which is pretty good considering that we\u2019re only looking at the first 20 words in every review. But note that merely flattening the embedded sequences and training a single Dense layer on top leads to a model that treats each word in the input sequence separately, without considering inter-word relationships and sentence structure (for example, this model would likely treat both \u201cthis movie is a bomb\u201d and \u201cthis movie is the bomb\u201d as being negative reviews). \n\nIt\u2019s much better to add recurrent layers or 1D convolutional layers on top of the embedded sequences to learn features that take into account each sequence as a whole. We will do this later.","a5123d0b":"LSTM doing a decent job here, lets try Bi directional LSTM","78bc1cbd":"We can see a lots of null values for \"keyword\" and \"location\" columns","50a1f0e1":"**We have a balanced dataset, which is good**","a016060d":"Neural Network with Embedding layer seems to the best model for this classification task.\n\n# Please upvote if you like this kernel.","c6ddbe47":"# Predictions on the test dataset","3d7c62e0":"# Data Preprocessing","fd3273dd":"# Stacked GRU","4676eb82":"# Welcome to my Kernel ! ","22313f73":"### 6. LSTM","e5350b63":"### 2. Neural Network with Embedding Layer","3d961d5b":"# Model using Word Embeddings\n\nAnother popular and powerful way to associate a vector with a word is the use of dense word vectors, also called `word embeddings`. \n\nThe Embedding layer is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. It takes integers as input, it looks up these integers in an internal dictionary, and it returns the associated vectors. It\u2019s effectively a dictionary lookup.\n\nWhereas the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros), and very high-dimensional (same dimensionality as the number of words in the vocabulary), word embeddings are low dimensional floating-point vectors (that is, dense vectors, as opposed to sparse vectors); \n\nUnlike the word vectors obtained via one-hot encoding, word embeddings are learned from data. It\u2019s common to see word embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large vocabularies. \n\nOn the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or greater (capturing a vocabulary of 6,025 tokens, above). So, word embeddings pack more information into far fewer dimensions.\n\n### There are two ways to obtain word embeddings:\n\n* Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors in the same way you learn the\nweights of a neural network.\n\n* Load into your model word embeddings that were precomputed using a different machine-learning task than the one you\u2019re trying to solve. These are called\npretrained word embeddings."}}