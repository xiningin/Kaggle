{"cell_type":{"1fa03ad7":"code","9f3bfd3e":"code","20434342":"code","fd3b59b5":"code","ab9a2f33":"code","59813da0":"code","c6eb7cd2":"code","618c259f":"code","93e054ee":"code","58a19784":"code","e6a6f385":"code","75e6fb49":"code","587bb2d8":"code","b732c5ae":"code","72ac7727":"code","17e18874":"code","8c64dad9":"code","a15bd54e":"code","a7a9372f":"code","3f1cc14e":"code","c4bffe0f":"code","9635c22b":"code","48523c22":"code","9c913890":"code","746d3a6e":"code","d4d79189":"code","1ef83149":"code","4da0f06b":"code","6dd2b5b4":"code","f98e4d37":"code","a7630c89":"code","a88eddf9":"code","0d7f6c29":"code","b55d1739":"code","662a2b0c":"code","bbd3a8d0":"code","089a3fdd":"code","cdc55850":"code","e2e2fa38":"code","1041b5ae":"code","2ac309dd":"code","4818b679":"code","1e65905e":"code","988dfd11":"code","5486f1fb":"code","e48d7a0e":"code","c0210a33":"code","b2175392":"code","9af79a8f":"code","2acb893a":"code","d1a77a8a":"code","585ada62":"code","090afe59":"code","431701f6":"code","a58e52e9":"code","2e2df16b":"code","feea6612":"code","4ec2a91a":"code","d6799449":"code","5acc8878":"code","abf52d92":"code","01bededa":"code","e10cc08b":"code","cd424c87":"markdown","f09ee4c0":"markdown","6a2548d6":"markdown"},"source":{"1fa03ad7":"import os\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport librosa \nimport librosa.display\n\nfrom IPython.display import Audio\nplt.style.use('seaborn-white')","9f3bfd3e":"TESS = \"..\/input\/toronto-emotional-speech-set-tess\/tess toronto emotional speech set data\/TESS Toronto emotional speech set data\/\"\nRAV = \"..\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\"\nSAVEE = \"..\/input\/surrey-audiovisual-expressed-emotion-savee\/ALL\/\"","20434342":"RAVS = \"..\/input\/ravdess-emotional-song-audio\/audio_song_actors_01-24\"","fd3b59b5":"dir_list = os.listdir(SAVEE)\n\nemotion=[]\npath = []\nfor i in dir_list:\n    if i[-8:-6]=='_a':\n        emotion.append('angry')\n    elif i[-8:-6]=='_d':\n        emotion.append('disgust')\n    elif i[-8:-6]=='_f':\n        emotion.append('fear')\n    elif i[-8:-6]=='_h':\n        emotion.append('happy')\n    elif i[-8:-6]=='_n':\n        emotion.append('neutral')\n    elif i[-8:-6]=='sa':\n        emotion.append('sad')\n    elif i[-8:-6]=='su':\n        emotion.append('surprise')\n    else:\n        emotion.append('unknown') \n    path.append(SAVEE + i)\n\nSAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\nSAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\nprint('SAVEE dataset')\nSAVEE_df.head()","ab9a2f33":"path = []\nemotion = []\ndir_list = os.listdir(TESS)\n\nfor i in dir_list:\n    fname = os.listdir(TESS + i)   \n    for f in fname:\n        if i == 'OAF_angry' or i == 'YAF_angry':\n            emotion.append('angry')\n        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n            emotion.append('disgust')\n        elif i == 'OAF_Fear' or i == 'YAF_fear':\n            emotion.append('fear')\n        elif i == 'OAF_happy' or i == 'YAF_happy':\n            emotion.append('happy')\n        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n            emotion.append('neutral')                                \n        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n            emotion.append('surprise')               \n        elif i == 'OAF_Sad' or i == 'YAF_sad':\n            emotion.append('sad')\n        else:\n            emotion.append('Unknown')\n        path.append(TESS + i + \"\/\" + f)\n\nTESS_df = pd.DataFrame(emotion, columns = ['labels'])\nTESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nprint('TESS dataset')\nTESS_df.head()","59813da0":"dir = os.listdir(RAV)\nfor a in dir:\n    files = os.listdir(RAV + '\/' + a)\n        \n    for file in files: \n        part = file.split('.')[0]\n        part = part.split(\"-\")           \n            \n#         temp = int(part[4])\n        print(part[2])","c6eb7cd2":"dir = os.listdir(RAV)\n\nmales = []\nfemales = [] \n        \nfor actor in dir:\n       \n    files = os.listdir(RAV + '\/' + actor)\n        \n    for file in files: \n        part = file.split('.')[0]\n        part = part.split(\"-\")           \n            \n        temp = int(part[6])        \n                \n        if part[2] == '01':\n            emotion = 'neutral'\n        elif part[2] == '02':\n            emotion = 'calm'\n        elif part[2] == '03':\n            emotion = 'happy'\n        elif part[2] == '04':\n            emotion = 'sad'\n        elif part[2] == '05':\n            emotion = 'angry'\n        elif part[2] == '06':\n            emotion = 'fear'\n        elif part[2] == '07':\n            emotion = 'disgust'\n        elif part[2] == '08':\n            emotion = 'surprise'\n        else:\n            emotion = 'unknown'\n            \n        if temp%2 == 0:\n            path = (RAV + '\/' + actor + '\/' + file)\n            females.append([emotion, path]) \n        else:\n            path = (RAV + '\/' + actor + '\/' + file)\n            males.append([emotion, path])   \n    \n   \nRavFemales_df = pd.DataFrame(females)\nRavFemales_df.columns = ['labels', 'path']\n\nRavMales_df = pd.DataFrame(males)\nRavMales_df.columns = ['labels', 'path']\n\nprint('RAVDESS datasets')\nRavFemales_df.head()\nRavMales_df.head()","618c259f":"dir = os.listdir(RAVS)\n\nmales = []\nfemales = [] \n        \nfor actor in dir:\n       \n    files = os.listdir(RAVS + '\/' + actor)\n        \n    for file in files: \n        part = file.split('.')[0]\n        part = part.split(\"-\")           \n            \n        temp = int(part[6])        \n                \n        if part[2] == '01':\n            emotion = 'neutral'\n        elif part[2] == '02':\n            emotion = 'calm'\n        elif part[2] == '03':\n            emotion = 'happy'\n        elif part[2] == '04':\n            emotion = 'sad'\n        elif part[2] == '05':\n            emotion = 'angry'\n        elif part[2] == '06':\n            emotion = 'fear'\n        elif part[2] == '07':\n            emotion = 'disgust'\n        elif part[2] == '08':\n            emotion = 'surprise'\n        else:\n            emotion = 'unknown'\n            \n        if temp%2 == 0:\n            path = (RAVS + '\/' + actor + '\/' + file)\n            females.append([emotion, path]) \n        else:\n            path = (RAVS + '\/' + actor + '\/' + file)\n            males.append([emotion, path])   \n    \n   \nRavSFemales_df = pd.DataFrame(females)\nRavSFemales_df.columns = ['labels', 'path']\n\nRavSMales_df = pd.DataFrame(males)\nRavSMales_df.columns = ['labels', 'path']\n\nprint('RAVSDESS datasets')\nRavSFemales_df.head()\nRavSMales_df.head()","93e054ee":"dfs = pd.concat([RavSMales_df, RavSFemales_df], axis=0)","58a19784":"test = dfs[:50]","e6a6f385":"test.shape","75e6fb49":"RavMales_df.path[2]","587bb2d8":"Males = pd.concat([SAVEE_df, RavMales_df], axis = 0)\nMales.to_csv(\"males_emotions_df.csv\", index = False)\n\nFemales = pd.concat([TESS_df, RavFemales_df], axis = 0)\nFemales.to_csv(\"females_emotions_df.csv\", index = False)","b732c5ae":"Females.head()","72ac7727":"Males.head()","17e18874":"df = pd.concat([Females, Males])\ndf.head()","8c64dad9":"df.shape","a15bd54e":"df.labels","a7a9372f":"def noise(data):\n    noise_amp = 0.04*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.70):\n    return librosa.effects.time_stretch(data, rate)\n\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.8):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n\ndef higher_speed(data, speed_factor = 1.25):\n    return librosa.effects.time_stretch(data, speed_factor)\n\ndef lower_speed(data, speed_factor = 0.75):\n    return librosa.effects.time_stretch(data, speed_factor)\n","3f1cc14e":"path = path = '..\/input\/ravdess-emotional-song-audio\/Actor_01\/03-02-01-01-01-01-01.wav'\ndata, sample_rate = librosa.load(path)","c4bffe0f":"plt.figure(figsize=(10,3))\nx = noise(data)\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","9635c22b":"plt.figure(figsize=(10,3))\nx = higher_speed(data)\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","48523c22":"sample_rate = 22050\n\ndef extract_features(data):\n    \n    result = np.array([])\n    \n    spectrogram = librosa.feature.melspectrogram(data,sr=sample_rate,n_mels=128,fmax=8000)\n    db_spec = librosa.power_to_db(spectrogram)\n    log_spectrogram = np.mean(db_spec.T, axis = 0)\n    result = np.array(log_spectrogram)\n    \n    return result\n\ndef get_features(path):\n    data, sample_rate = librosa.load(path, duration=3, offset=0.5, res_type='kaiser_fast') \n    \n    res1 = extract_features(data)\n    result = np.array(res1)\n    \n    #noised\n    noise_data = noise(data)\n    res2 = extract_features(noise_data)\n    result = np.vstack((result, res2)) \n    \n    #stretched\n    stretch_data = stretch(data)\n    res3 = extract_features(stretch_data)\n    result = np.vstack((result, res3))\n    \n    #shifted\n    shift_data = shift(data)\n    res4 = extract_features(shift_data)\n    result = np.vstack((result, res4))\n    \n    #pitched\n    pitch_data = pitch(data, sample_rate)\n    res5 = extract_features(pitch_data)\n    result = np.vstack((result, res5)) \n    \n    #speed up\n    higher_speed_data = higher_speed(data)\n    res6 = extract_features(higher_speed_data)\n    result = np.vstack((result, res6))\n    \n    #speed down\n    lower_speed_data = higher_speed(data)\n    res7 = extract_features(lower_speed_data)\n    result = np.vstack((result, res7))\n\n#     result = res1 + res2 + res3 + res4 + res5 + res6 + res7\n    \n    return result","9c913890":"for path, emotion in zip(df.path, df.labels):\n  features = get_features(path)\n  print(features.shape)","746d3a6e":"X, y = [], []\nfor path, emotion in zip(df.path, df.labels):\n  features = get_features(path)\n  for ele in features:\n    X.append(ele)\n    y.append(emotion)\n\nprint(f'Check Shapes: \\n Features: {len(X)}, labels: {len(y)}')","d4d79189":"def setup_dataframe(features, labels):\n    df = pd.DataFrame(features)\n    df['labels'] = labels\n    df.to_csv(f'df.csv', index=False)\n    \n    print(f'dataframe')\n    df.sample(frac=1).head()\n    \n    return df","1ef83149":"Features = setup_dataframe(X, y)","4da0f06b":"df.head()","6dd2b5b4":"import os\nos.chdir(r'..\/working')\nfrom IPython.display import FileLink\nFileLink(r'df.csv')","f98e4d37":"Features = pd.read_csv('.\/df.csv')","a7630c89":"X","a88eddf9":"Features.head()","0d7f6c29":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split","b55d1739":"X = Features.iloc[: ,:-1].values\ny = Features['labels'].values","662a2b0c":"encoder = OneHotEncoder()\n\ny = encoder.fit_transform(np.array(y).reshape(-1,1)).toarray()","bbd3a8d0":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.20, shuffle=True)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","089a3fdd":"X_train","cdc55850":"scaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","e2e2fa38":"X_train = np.expand_dims(X_train, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape, y_train.shape , X_test.shape , y_test.shape","1041b5ae":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint","2ac309dd":"def build_model(in_shape):\n\n    model=Sequential()\n    model.add(Conv1D(256, kernel_size=6, strides=1, padding='same', activation='relu', input_shape=(in_shape, 1)))\n    model.add(AveragePooling1D(pool_size=4, strides = 2, padding = 'same'))\n\n    model.add(Conv1D(128, kernel_size=6, strides=1, padding='same', activation='relu'))\n    model.add(AveragePooling1D(pool_size=4, strides = 2, padding = 'same'))\n\n    model.add(Conv1D(128, kernel_size=6, strides=1, padding='same', activation='relu'))\n    model.add(AveragePooling1D(pool_size=4, strides = 2, padding = 'same'))\n    model.add(Dropout(0.2))\n\n    model.add(Conv1D(64, kernel_size=6, strides=1, padding='same', activation='relu'))\n    model.add(MaxPooling1D(pool_size=4, strides = 2, padding = 'same'))\n        \n    model.add(Flatten())\n    model.add(Dense(units=32, activation='relu'))\n    model.add(Dropout(0.3))\n\n    model.add(Dense(units=8, activation='softmax'))\n    model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n          \n        \n    return model","4818b679":"def model_build_summary(mod_dim, tr_features, val_features, val_labels):\n    model = build_model(mod_dim)\n    model.summary()\n    \n    score = model.evaluate(val_features, val_labels, verbose = 1)\n    accuracy = 100*score[1]\n    \n    return model","1e65905e":"def show_graphs(history):\n    epochs = [i for i in range(n_epochs)]\n    fig , ax = plt.subplots(1,2)\n    train_acc = history.history['accuracy']\n    train_loss = history.history['loss']\n    test_acc = history.history['val_accuracy']\n    test_loss = history.history['val_loss']\n\n    fig.set_size_inches(30,12)\n    ax[0].plot(epochs , train_loss , label = 'Training Loss')\n    ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n    ax[0].set_title('Training & Testing Loss')\n    ax[0].legend()\n    ax[0].set_xlabel(\"Epochs\")\n\n    ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n    ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n    ax[1].set_title('Training & Testing Accuracy')\n    ax[1].legend()\n    ax[1].set_xlabel(\"Epochs\")\n    plt.show()","988dfd11":"model = model_build_summary(X_train.shape[1], X_train, X_test, y_test)","5486f1fb":"batch_size = 32\nn_epochs = 50","e48d7a0e":"history = model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, validation_data=(X_test, y_test))","c0210a33":"score = model.evaluate(X_train,y_train, verbose = 0)\nprint(\"training Accuracy: {0:.2%}\".format(score[1]))\n\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint(\"testing Accuracy: {0:.2%}\".format(score[1]))","b2175392":"show_graphs(history)","9af79a8f":"model.save('model1.h5')","2acb893a":"os.chdir(r'..\/working')\nFileLink(r'model1.h5')","d1a77a8a":"test_path = '..\/input\/ravdess-emotional-song-audio\/Actor_03\/03-02-01-01-01-01-03.wav'\n\n","585ada62":"X_1, y_1 = [], []\nfeatures = get_features(test_path)\nfor ele in features:\n    X_1.append(ele)\n    y_1.append(emotion)\n\nprint(f'Check Shapes: \\n Features: {len(X_1)}, labels: {len(y_1)}')","090afe59":"X_, y_ = [], []\nfor path, emotion in zip(test.path, test.labels):\n  features = get_features(path)\n  for ele in features:\n    X_.append(ele)\n    y_.append(emotion)\n\nprint(f'Check Shapes: \\n Features: {len(X_)}, labels: {len(y_)}')","431701f6":"Features1 = setup_dataframe(X_1, y_1)","a58e52e9":"Features1.shape","2e2df16b":"X_1 = Features1.iloc[: ,:-1].values\ny_1 = Features1['labels'].values\n\ny_1 = encoder.fit_transform(np.array(y_1).reshape(-1,1)).toarray()\nX_1 = scaler.transform(X_1)\nX_1 = np.expand_dims(X_1, axis=2)","feea6612":"pred_test = model.predict(X_1)\ny_pred = encoder.inverse_transform(pred_test)\ny_test_ = encoder.inverse_transform(y_1)","4ec2a91a":"print(X_1.shape)\nprint(y_1.shape)","d6799449":"preds = model.predict(X_1)\npreds.shape","5acc8878":"np.argmax(preds)","abf52d92":"preds","01bededa":"score = model.evaluate(X_1,y_1, verbose = 0)\nscore","e10cc08b":"y_preds = model.predict(X_test)","cd424c87":"## Modelling","f09ee4c0":"## Data Prep","6a2548d6":"## Data Augmentation"}}