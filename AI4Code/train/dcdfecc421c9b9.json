{"cell_type":{"1020a94e":"code","a6612062":"code","adaa1a26":"code","500e5756":"code","908a7a69":"code","59ac475b":"code","48aaae6b":"code","0e2a0fff":"code","217c4d5c":"code","e0ac405d":"code","24c5775a":"code","58de0da8":"code","0a1e8289":"code","1cde43b8":"code","e34afe45":"code","aafeb6a5":"code","649d37ef":"code","9569f17a":"code","63dd847e":"code","b4cd36fc":"code","6b54e128":"code","ef23b485":"code","fc5ba616":"code","584f54e0":"code","27812e33":"code","8652a139":"code","cc5ea276":"code","19cdcc18":"code","7e026e9f":"code","6d8a6986":"code","e461290c":"code","d42b09e7":"code","27812b79":"code","aed3cb8e":"code","cb6dc2a9":"code","546084f4":"code","d58ee50e":"code","b3b4bb7a":"code","cebc24a7":"code","25b0b1d2":"code","12cbd068":"code","9e80945c":"code","4433b066":"code","8da05f91":"code","b566c7bc":"code","f95f715a":"code","a56e798b":"code","fe318128":"code","825c9602":"code","ffc38aac":"code","c5cd0efb":"code","b8720cb8":"code","a90a6b66":"code","de927ca6":"code","22d24474":"code","0b0d60e2":"markdown","4a59729b":"markdown","337383fa":"markdown","6ac621e4":"markdown","ce3fee20":"markdown","8d0b9207":"markdown","ae06195e":"markdown","b168dea7":"markdown","3b4cae06":"markdown","a9497b59":"markdown","7b79b432":"markdown","f841d96a":"markdown","f27d55bd":"markdown","2887fda7":"markdown","927ccd45":"markdown","9941ac7a":"markdown","d65e0ea2":"markdown","5e0c8686":"markdown","b93e9c67":"markdown","f9298594":"markdown","fb58ed1f":"markdown","39b1ad56":"markdown","b1525987":"markdown","74dd6bad":"markdown","3dc014fd":"markdown","0e77714d":"markdown","199760ae":"markdown","21a513ee":"markdown","25c4c1c2":"markdown"},"source":{"1020a94e":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport gc\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set(font_scale=1.2)\n\nimport warnings\n\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 100)\n# dir(pd.options.display)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\n\nplt.style.use('ggplot')\n\nimport missingno as msno\nimport datetime as dt\nimport IPython.display as ipd\nimport librosa\n\nfrom random import sample\nfrom collections import OrderedDict\nfrom datetime import timedelta\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nnp.random.seed(2019)","a6612062":"SAMPLE_RATE = 44100","adaa1a26":"train = pd.read_csv(\"..\/input\/train_curated.csv\")\ntrain['is_curated'] = True\ntrain_noisy = pd.read_csv('..\/input\/train_noisy.csv')\ntrain_noisy['is_curated'] = False\ntrain = pd.concat([train, train_noisy], axis=0)\ndel train_noisy","500e5756":"train.sample(5)","908a7a69":"train['n_label'] = train.labels.str.split(',').apply(lambda x: len(x))","59ac475b":"train.query('is_curated == True').n_label.value_counts()","48aaae6b":"train.query('is_curated == False').n_label.value_counts()","0e2a0fff":"cat_gp = train[train.n_label == 1].groupby(\n    ['labels', 'is_curated']).agg({\n    'fname':'count'\n}).reset_index()\ncat_gpp = cat_gp.pivot(index='labels', columns='is_curated', values='fname').reset_index().set_index('labels')\n\nplot = cat_gpp.plot(\n    kind='barh',\n    title=\"Number of samples per category\",\n    stacked=True,\n    color=['deeppink', 'darkslateblue'],\n    figsize=(15,20))\nplot.set_xlabel(\"Number of Samples\", fontsize=20)\nplot.set_ylabel(\"Label\", fontsize=20);","217c4d5c":"# sampling an audio in train_curated\nsamp = train[(train.n_label == 1) & (train.is_curated == True)].sample(1)\nprint(samp.labels.values[0])\nipd.Audio('..\/input\/train_curated\/{}'.format(samp.fname.values[0]))","e0ac405d":"# sampling an audio in train_noisy\nsamp_n = train[(train.n_label == 1) & \n               (train.is_curated == False) & \n               (train.labels == samp.labels.values[0])].sample(1)\nprint(samp_n.labels.values[0])\nipd.Audio('..\/input\/train_noisy\/{}'.format(samp_n.fname.values[0]))","24c5775a":"# trim silent part\nwav, _ = librosa.core.load(\n    '..\/input\/train_curated\/{}'.format(samp.fname.values[0]),\n    sr=SAMPLE_RATE)\nwav_tr = librosa.effects.trim(wav)[0]\nwav_n, _ = librosa.core.load(\n    '..\/input\/train_noisy\/{}'.format(samp_n.fname.values[0]),\n    sr=SAMPLE_RATE)\nwav_tr_n = librosa.effects.trim(wav_n)[0]\nprint('After trimmed curated wav: {:,}\/{:,}'.format(len(wav_tr), len(wav)))\nprint('After trimmed noisy wav: {:,}\/{:,}'.format(len(wav_tr_n), len(wav_n)))","58de0da8":"melspec = librosa.feature.melspectrogram(\n    librosa.resample(wav_tr, SAMPLE_RATE, SAMPLE_RATE\/2),\n    sr=SAMPLE_RATE\/2,\n    n_fft=1764,\n    hop_length=220,\n    n_mels=64\n)\nlogmel = librosa.core.power_to_db(melspec)\n\nmelspec_n = librosa.feature.melspectrogram(\n    librosa.resample(wav_tr_n, SAMPLE_RATE, SAMPLE_RATE\/2),\n    sr=SAMPLE_RATE\/2,\n    n_fft=1764,\n    hop_length=220,\n    n_mels=64\n)\nlogmel_n = librosa.core.power_to_db(melspec_n)","0a1e8289":"fig, ax = plt.subplots(2, 1, figsize=(15, 10))\nfor i, l in enumerate([logmel, logmel_n]):\n    if i==0: \n        ax[i].set_title('curated {}'.format(samp.labels.values[0]))\n    else:\n        ax[i].set_title('noisy {}'.format(samp_n.labels.values[0]))\n    im = ax[i].imshow(l, cmap='Spectral', interpolation='nearest',\n                      aspect=l.shape[1]\/l.shape[0]\/5)","1cde43b8":"mfcc = librosa.feature.mfcc(wav_tr, \n                            sr=SAMPLE_RATE, \n                            n_fft=1764,\n                            hop_length=220,\n                            n_mfcc=64)\nmfcc_n = librosa.feature.mfcc(wav_tr_n, \n                              sr=SAMPLE_RATE, \n                              n_fft=1764,\n                              hop_length=220,\n                              n_mfcc=64)\n\nfig, ax = plt.subplots(2, 1, figsize=(15, 10))\nfor i, m in enumerate([mfcc, mfcc_n]):\n    if i==0: \n        ax[i].set_title('curated {}'.format(samp.labels.values[0]))\n    else:\n        ax[i].set_title('noisy {}'.format(samp_n.labels.values[0]))\n    im = ax[i].imshow(m, cmap='Spectral', interpolation='nearest',\n                      aspect=m.shape[1]\/m.shape[0]\/5)","e34afe45":"print('Unique number of multi label : {}'.format(train.loc[train.n_label > 1, 'labels'].nunique()))\nprint('Unique number of multi label in curated data : {}'.format(\n    train.loc[(train.n_label > 1) & (train.is_curated == True), 'labels'].nunique()))\nprint('Unique number of multi label in noisy data : {}'.format(\n    train.loc[(train.n_label > 1) & (train.is_curated == False), 'labels'].nunique()))    ","aafeb6a5":"cat_gp = train[(train.n_label > 1) & (train.is_curated == True)].groupby(\n    'labels').agg({'fname':'count'})\ncat_gp.columns = ['counts']\n\nplot = cat_gp.sort_values(ascending=True, by='counts').plot(\n    kind='barh',\n    title=\"Number of Audio Samples per Category\",\n    color='deeppink',\n    figsize=(15,30))\nplot.set_xlabel(\"Number of Samples\", fontsize=20)\nplot.set_ylabel(\"Label\", fontsize=20);","649d37ef":"label_set = set(train.loc[(train.n_label == 2) & (train.is_curated == True), 'labels']) & set(\n    train.loc[(train.n_label == 2) & (train.is_curated == False), 'labels'])\n\nlabel_samp = np.random.choice(list(label_set), 1)[0]\nsamp = train[(train.labels == label_samp) & (train.is_curated == True)].sample(1)\nprint(label_samp)\nipd.Audio('..\/input\/train_curated\/{}'.format(samp.fname.values[0]))","9569f17a":"# sampling an audio in train_noisy\nsamp_n = train[(train.labels == label_samp) & (train.is_curated == False)].sample(1)\nprint(samp_n.labels.values[0])\nipd.Audio('..\/input\/train_noisy\/{}'.format(samp_n.fname.values[0]))","63dd847e":"# trim silent part\nwav, _ = librosa.core.load(\n    '..\/input\/train_curated\/{}'.format(samp.fname.values[0]),\n    sr=SAMPLE_RATE)\nwav_tr = librosa.effects.trim(wav)[0]\nprint('After trimmed curated wav: {:,}\/{:,}'.format(len(wav_tr), len(wav)))\n\nwav_n, _ = librosa.core.load(\n    '..\/input\/train_noisy\/{}'.format(samp_n.fname.values[0]),\n    sr=SAMPLE_RATE)\nwav_tr_n = librosa.effects.trim(wav_n)[0]\nprint('After trimmed noisy wav: {:,}\/{:,}'.format(len(wav_tr_n), len(wav_n)))","b4cd36fc":"melspec = librosa.feature.melspectrogram(\n    librosa.resample(wav_tr, SAMPLE_RATE, SAMPLE_RATE\/2),\n    sr=SAMPLE_RATE\/2,\n    n_fft=1764,\n    hop_length=220,\n    n_mels=64\n)\nlogmel = librosa.core.power_to_db(melspec)\n\nmelspec_n = librosa.feature.melspectrogram(\n    librosa.resample(wav_tr_n, SAMPLE_RATE, SAMPLE_RATE\/2),\n    sr=SAMPLE_RATE\/2,\n    n_fft=1764,\n    hop_length=220,\n    n_mels=64\n)\nlogmel_n = librosa.core.power_to_db(melspec_n)","6b54e128":"fig, ax = plt.subplots(2, 1, figsize=(15, 10))\nif samp_n.labels.values[0] == samp.labels.values[0]:\n    for i, l in enumerate([logmel, logmel_n]):\n        if i==0: \n            ax[i].set_title('curated {}'.format(samp.labels.values[0]))\n        else:\n            ax[i].set_title('noisy {}'.format(samp_n.labels.values[0]))\n        im = ax[i].imshow(l, cmap='Spectral', interpolation='nearest',\n                          aspect=l.shape[1]\/l.shape[0]\/5)\n","ef23b485":"mfcc = librosa.feature.mfcc(wav_tr, \n                            sr=SAMPLE_RATE, \n                            n_fft=1764,\n                            hop_length=220,\n                            n_mfcc=64)\nmfcc_n = librosa.feature.mfcc(wav_tr_n, \n                              sr=SAMPLE_RATE, \n                              n_fft=1764,\n                              hop_length=220,\n                              n_mfcc=64)\n\nfig, ax = plt.subplots(2, 1, figsize=(15, 10))\nfor i, m in enumerate([mfcc, mfcc_n]):\n    if i==0: \n        ax[i].set_title('curated {}'.format(samp.labels.values[0]))\n    else:\n        ax[i].set_title('noisy {}'.format(samp_n.labels.values[0]))\n    im = ax[i].imshow(m, cmap='Spectral', interpolation='nearest',\n                      aspect=m.shape[1]\/m.shape[0]\/5)","fc5ba616":"label_set = set(train.loc[(train.n_label == 3) & (train.is_curated == True), 'labels']) & set(\n    train.loc[(train.n_label == 3) & (train.is_curated == False), 'labels'])\n\nlabel_samp = np.random.choice(list(label_set), 1)[0]\nsamp = train[(train.labels == label_samp) & (train.is_curated == True)].sample(1)\nprint(label_samp)\nipd.Audio('..\/input\/train_curated\/{}'.format(samp.fname.values[0]))","584f54e0":"# sampling an audio in train_noisy\nsamp_n = train[(train.labels == label_samp) & (train.is_curated == False)].sample(1)\nprint(samp_n.labels.values[0])\nipd.Audio('..\/input\/train_noisy\/{}'.format(samp_n.fname.values[0]))","27812e33":"# trim silent part\nwav, _ = librosa.core.load(\n    '..\/input\/train_curated\/{}'.format(samp.fname.values[0]),\n    sr=SAMPLE_RATE)\nwav_tr = librosa.effects.trim(wav)[0]\nprint('After trimmed curated wav: {:,}\/{:,}'.format(len(wav_tr), len(wav)))\n\nwav_n, _ = librosa.core.load(\n    '..\/input\/train_noisy\/{}'.format(samp_n.fname.values[0]),\n    sr=SAMPLE_RATE)\nwav_tr_n = librosa.effects.trim(wav_n)[0]\nprint('After trimmed noisy wav: {:,}\/{:,}'.format(len(wav_tr_n), len(wav_n)))","8652a139":"melspec = librosa.feature.melspectrogram(\n    librosa.resample(wav_tr, SAMPLE_RATE, SAMPLE_RATE\/2),\n    sr=SAMPLE_RATE\/2,\n    n_fft=1764,\n    hop_length=220,\n    n_mels=64\n)\nlogmel = librosa.core.power_to_db(melspec)\n\nmelspec_n = librosa.feature.melspectrogram(\n    librosa.resample(wav_tr_n, SAMPLE_RATE, SAMPLE_RATE\/2),\n    sr=SAMPLE_RATE\/2,\n    n_fft=1764,\n    hop_length=220,\n    n_mels=64\n)\nlogmel_n = librosa.core.power_to_db(melspec_n)","cc5ea276":"fig, ax = plt.subplots(2, 1, figsize=(15, 10))\nif samp_n.labels.values[0] == samp.labels.values[0]:\n    for i, l in enumerate([logmel, logmel_n]):\n        if i==0: \n            ax[i].set_title('curated {}'.format(samp.labels.values[0]))\n        else:\n            ax[i].set_title('noisy {}'.format(samp_n.labels.values[0]))\n        im = ax[i].imshow(l, cmap='Spectral', interpolation='nearest',\n                          aspect=l.shape[1]\/l.shape[0]\/5)\n","19cdcc18":"mfcc = librosa.feature.mfcc(wav_tr, \n                            sr=SAMPLE_RATE, \n                            n_fft=1764,\n                            hop_length=220,\n                            n_mfcc=64)\nmfcc_n = librosa.feature.mfcc(wav_tr_n, \n                              sr=SAMPLE_RATE, \n                              n_fft=1764,\n                              hop_length=220,\n                              n_mfcc=64)\n\nfig, ax = plt.subplots(2, 1, figsize=(15, 10))\nfor i, m in enumerate([mfcc, mfcc_n]):\n    if i==0: \n        ax[i].set_title('curated {}'.format(samp.labels.values[0]))\n    else:\n        ax[i].set_title('noisy {}'.format(samp_n.labels.values[0]))\n    im = ax[i].imshow(m, cmap='Spectral', interpolation='nearest',\n                      aspect=m.shape[1]\/m.shape[0]\/5)","7e026e9f":"samp = train[(train.n_label == 4) & (train.is_curated == True)].sample(1)\nprint(samp.labels.values[0])\nipd.Audio('..\/input\/train_curated\/{}'.format(samp.fname.values[0]))","6d8a6986":"wav, _ = librosa.core.load(\n    '..\/input\/train_curated\/{}'.format(samp.fname.values[0]),\n    sr=SAMPLE_RATE)\nwav_tr = librosa.effects.trim(wav)[0]\nprint('After trimmed: {:,}\/{:,}'.format(len(wav_tr), len(wav)))","e461290c":"wav_tr = librosa.resample(wav_tr, SAMPLE_RATE, SAMPLE_RATE\/2)\nmelspec = librosa.feature.melspectrogram(wav_tr,\n                                         sr=SAMPLE_RATE\/2,\n                                         n_fft=1764,\n                                         hop_length=220,\n                                         n_mels=64)\nlogmel = librosa.core.power_to_db(melspec)","d42b09e7":"fig, ax = plt.subplots(figsize=(15, 5))\nim = ax.imshow(logmel, cmap='Spectral', interpolation='nearest',\n          aspect=logmel.shape[1]\/logmel.shape[0]\/5)\nax.set_title('log-mel');","27812b79":"mfcc = librosa.feature.mfcc(wav_tr, sr=SAMPLE_RATE, \n                            n_fft=1764,\n                            hop_length=220,\n                            n_mfcc=64)\nfig, ax = plt.subplots(figsize=(15, 5))\nim = ax.imshow(mfcc, cmap='Spectral', interpolation='nearest',\n          aspect=mfcc.shape[1]\/mfcc.shape[0]\/5)\nax.set_title('MFCC');","aed3cb8e":"samp = train[train.n_label == 5].sample(1)\nprint(samp.labels.values[0])\nipd.Audio('..\/input\/train_noisy\/{}'.format(samp.fname.values[0]))","cb6dc2a9":"wav, _ = librosa.core.load(\n    '..\/input\/train_noisy\/{}'.format(samp.fname.values[0]),\n    sr=SAMPLE_RATE)\nwav_tr = librosa.effects.trim(wav)[0]\nprint('After trimmed: {:,}\/{:,}'.format(len(wav_tr), len(wav)))","546084f4":"wav_tr = librosa.resample(wav_tr, SAMPLE_RATE, SAMPLE_RATE\/2)\nmelspec = librosa.feature.melspectrogram(wav_tr,\n                                         sr=SAMPLE_RATE\/2,\n                                         n_fft=1764,\n                                         hop_length=220,\n                                         n_mels=64)\nlogmel = librosa.core.power_to_db(melspec)","d58ee50e":"fig, ax = plt.subplots(figsize=(15, 5))\nim = ax.imshow(logmel, cmap='Spectral', interpolation='nearest',\n          aspect=logmel.shape[1]\/logmel.shape[0]\/5)\nax.set_title('log-mel');","b3b4bb7a":"mfcc = librosa.feature.mfcc(wav_tr, sr=SAMPLE_RATE, \n                            n_fft=1764,\n                            hop_length=220,\n                            n_mfcc=64)\nfig, ax = plt.subplots(figsize=(15, 5))\nim = ax.imshow(mfcc, cmap='Spectral', interpolation='nearest',\n          aspect=mfcc.shape[1]\/mfcc.shape[0]\/5)\nax.set_title('MFCC');","cebc24a7":"samp = train[(train.n_label == 6) & (train.is_curated == True)].sample(1)\nprint(samp.labels.values[0])\nipd.Audio('..\/input\/train_curated\/{}'.format(samp.fname.values[0]))","25b0b1d2":"wav, _ = librosa.core.load(\n    '..\/input\/train_curated\/{}'.format(samp.fname.values[0]),\n    sr=SAMPLE_RATE)\nwav_tr = librosa.effects.trim(wav)[0]\nprint('After trimmed: {:,}\/{:,}'.format(len(wav_tr), len(wav)))","12cbd068":"wav_tr = librosa.resample(wav_tr, SAMPLE_RATE, SAMPLE_RATE\/2)\nmelspec = librosa.feature.melspectrogram(wav_tr,\n                                         sr=SAMPLE_RATE\/2,\n                                         n_fft=1764,\n                                         hop_length=220,\n                                         n_mels=64)\nlogmel = librosa.core.power_to_db(melspec)","9e80945c":"fig, ax = plt.subplots(figsize=(15, 5))\nim = ax.imshow(logmel, cmap='Spectral', interpolation='nearest',\n          aspect=logmel.shape[1]\/logmel.shape[0]\/5)\nax.set_title('log-mel');","4433b066":"mfcc = librosa.feature.mfcc(wav_tr, sr=SAMPLE_RATE, \n                            n_fft=1764,\n                            hop_length=220,\n                            n_mfcc=64)\nfig, ax = plt.subplots(figsize=(15, 5))\nim = ax.imshow(mfcc, cmap='Spectral', interpolation='nearest',\n          aspect=mfcc.shape[1]\/mfcc.shape[0]\/5)\nax.set_title('MFCC');","8da05f91":"samp = train[train.n_label == 7].sample(1)\nprint(samp.labels.values[0])\nipd.Audio('..\/input\/train_noisy\/{}'.format(samp.fname.values[0]))","b566c7bc":"wav, _ = librosa.core.load(\n    '..\/input\/train_noisy\/{}'.format(samp.fname.values[0]),\n    sr=SAMPLE_RATE)\nwav_tr = librosa.effects.trim(wav)[0]\nprint('After trimmed: {:,}\/{:,}'.format(len(wav_tr), len(wav)))","f95f715a":"wav_tr = librosa.resample(wav_tr, SAMPLE_RATE, SAMPLE_RATE\/2)\nmelspec = librosa.feature.melspectrogram(wav_tr,\n                                         sr=SAMPLE_RATE\/2,\n                                         n_fft=1764,\n                                         hop_length=220,\n                                         n_mels=64)\nlogmel = librosa.core.power_to_db(melspec)","a56e798b":"fig, ax = plt.subplots(figsize=(15, 5))\nim = ax.imshow(logmel, cmap='Spectral', interpolation='nearest',\n          aspect=logmel.shape[1]\/logmel.shape[0]\/5)\nax.set_title('log-mel');","fe318128":"mfcc = librosa.feature.mfcc(wav_tr, sr=SAMPLE_RATE, \n                            n_fft=1764,\n                            hop_length=220,\n                            n_mfcc=64)\nfig, ax = plt.subplots(figsize=(15, 5))\nim = ax.imshow(mfcc, cmap='Spectral', interpolation='nearest',\n          aspect=mfcc.shape[1]\/mfcc.shape[0]\/5)\nax.set_title('MFCC');","825c9602":"test = pd.read_csv('..\/input\/sample_submission.csv')\nfor c in test.columns[1:]:\n    cc = c.replace('(', '\\(').replace(')', '\\)')\n    train.loc[:, c] = train['labels'].str.contains(cc)\n    if (train.loc[:, c] > 1).sum():\n        raise Exception('label key \"{}\" are duplicated !'.format(c))","ffc38aac":"train.head()","c5cd0efb":"tmp = train.loc[train.is_curated == True, \n                'Accelerating_and_revving_and_vroom':].sum(axis=0).to_frame().T\ntmp = pd.concat(\n    [tmp, train.loc[train.is_curated == False, \n                    'Accelerating_and_revving_and_vroom':].sum(axis=0).to_frame().T]\n)\ntmp['total_label'] = tmp.loc[:, 'Accelerating_and_revving_and_vroom':].sum(axis=1)\ntmp.index = ['curated', 'noisy']\ntmp","b8720cb8":"fig, ax = plt.subplots(figsize=(10, 20))\ntmp.iloc[:, :-1].T.plot.barh(color=['deeppink', 'darkslateblue'], ax=ax);","a90a6b66":"sns.set(style=\"white\")\nsns.set(font_scale=1)\ntrain_cor = train.loc[train.is_curated == True, test.columns[1:]].corr()\nmask = np.zeros_like(train_cor, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nfig, ax = plt.subplots(figsize=(25, 25))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(train_cor, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .1});","de927ca6":"multi_label = test.columns[1:][train.loc[\n    (train.n_label > 1) & (train.is_curated == True), test.columns[1:]].sum() > 0]","22d24474":"sns.set(font_scale=1.7)\ntrain_cor = train.loc[\n    (train.n_label > 1) & (train.is_curated == True), multi_label].corr()\nmask = np.zeros_like(train_cor, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.clustermap(train_cor, metric='correlation',\n               cmap=cmap, center=0, linewidths=.5, figsize=(30, 30));","0b0d60e2":"* Some records in noisy train data have 2, 3, 4, 5, 6, 7 labels.\n* 24000 labels","4a59729b":"Before computing co-occurrence, let's check the number of each label in curated and noisy data.","337383fa":"<a id=\"ld\"><\/a>\n# <center>2. LOAD DATA<\/center>","6ac621e4":"<a id=\"3l\"><\/a>\n## 4.2 With 3 labels","ce3fee20":"<a id=\"multil\"><\/a>\n# <center> 4. Multi Labeled <\/center>","8d0b9207":"There are a lot of kinds of muti labels.  ","ae06195e":"<a id=\"4l\"><\/a>\n## 4.3 With 4 labels\n<b>\n    There are no common record with same multi labels in curated and noisy.  \n    So, here we will check only curated audios.\n<\/b>","b168dea7":"For easy-to-see visuzalization,  \nwe select only multi-labeled records and use seaborns cluastermap.  ","3b4cae06":"<font color=crimson size=4><b> More to Go. Stay tuned. <\/b><\/font>\n<font color=crimson size=2><b> Apr. 15. 2019 updated <\/b><\/font>\n---\nIn previous kernel I have posted, I did not consider multi labels for simplicity.  \n[Beginner's Guide to Auidio Data 2](https:\/\/www.kaggle.com\/maxwell110\/beginner-s-guide-to-audio-data-2)  \nHere I will do simple explorations,  \n\n- wav sounds\n- spectrogram\n- co-occurence  \n  \nin multi labeled records.\n\n# Contents\n1. [LOAD PACKAGES](#lp)\n2. [LOAD DATA](#ld)\n3. [Single Labeled](#singlel)  \n4. [Multi Labeled](#multil)  \n    4.1 [With 2 labels](#2l)  \n    4.2 [With 3 labels](#3l)  \n    4.3 [With 4 labels](#4l)  \n    4.4 [With 5 labels](#5l)  \n    4.5 [With 6 labels](#6l)  \n    4.6 [With 7 labels](#7l)  \n    4.7 [Co-Occurence](#coo)  \n***\n","a9497b59":"Could you confirm the same audio in both curated and noisy ? If so, maybe you are lucky :)","7b79b432":"<img src=\"https:\/\/annotator.freesound.org\/static\/img\/freesound_logo_color.png\" alt=\"Drawing\" style=\"width: 500px;\"\/>","f841d96a":"<a id=\"singlel\"><\/a>\n# <center> 3. Single Labeled <\/center>","f27d55bd":"Let's check curated and noisy data with same label.","2887fda7":"<a id=\"6l\"><\/a>\n## 4.5 With 6 labels\n<b>\n    There are no common record with same multi labels in curated and noisy.  \n    So, here we will check only curated audios.\n<\/b>","927ccd45":"1.   Half of multi-labeled categories have only `1` record in curated train data.  \n2.   Similar audios tend to appear at once (e.g. Acoustic guitar, Strum).  \n  \nLet's listen multi-labeled audios and check log-mel and MFCC.  ","9941ac7a":"# <center> EOF <\/center>","d65e0ea2":"<a id=\"5l\"><\/a>\n## 4.4 With 5 labels\n<b>\n    There are no record with 5 labels in curated data.\n<\/b>","5e0c8686":"***\n<br>\n<font size=7 color=darkgray><b>\nFreesound Audio Tagging 2019\n<\/b><\/font>\n<br>","b93e9c67":"<a id=\"2l\"><\/a>\n## 4.1 With 2 labels","f9298594":"* Some records in curated train data have 2, 3, 4, 6 labels.  \n* 5752 labels in curated train data","fb58ed1f":"<b><font color=gray size=6> Findings <\/font><\/b>\n  \n1. Multi-labeled spectrograms look to have some patterns at the same timestamp or in different time periods.  \n2. Signals of spectrograms are mixiture of multiple sounds. I think this is the intrinsic difference from image data.  Band pass filters may help to separate multiple sounds and we can use those separated spectrogram data as an input to CNN models at once.\n3. A lot of noisy data seem to have wrong labels. In the previous competition, using the robust loss function to suppress the effect of mislabeled data was one of the important points to get high score. We must care of that again.","39b1ad56":"<center><b><font size=4 color=deepskyblue>\n    Thanks for reading.  \n    <br>\n    Happy Kaggling!\n    <\/font><\/b><\/center>\n<br>","b1525987":"After considering multi label, we can see all kinds of labels in curated and noisy data.","74dd6bad":"<a id=\"coo\"><\/a>\n## 4.7 Co-Occurrence in curated train data\n\nWe confirmed that noisy train data seems to have a lot of mislabeled records.  \nThen we will use only curated train data to compute co-occurrence.","3dc014fd":"I think the audios with multi label are more noisy (mean wrong labeled).","0e77714d":"The label seems valid ?","199760ae":"<a id=\"7l\"><\/a>\n## 4.6 With 7 labels\n<b>\n    There are no record with 7 labels in curated.  \n<\/b>","21a513ee":"<a id=\"lp\"><\/a>\n# <center>1. LOAD PACKAGES<\/center>","25c4c1c2":"Let's visualize only multi labeled data in curated train."}}