{"cell_type":{"cb00d277":"code","733c46b5":"code","25de19ec":"code","e36c7962":"code","57cbd26e":"code","c964f84e":"code","110f94c2":"code","2aa74a6d":"code","699c9386":"code","c72068f3":"code","7e5af448":"code","fef7ab49":"code","bc8b35aa":"code","78dbb586":"code","0566829c":"code","7b57c6ce":"code","ff032883":"code","a7ea1f91":"code","4df43aa7":"code","e62c6d98":"code","a0a1b5f0":"code","04ac1af2":"code","0f7a5b1f":"code","7677e0f6":"code","da5ffa78":"code","bdb28e4c":"code","43fb213c":"code","33101b97":"code","86ef218a":"code","1e74d311":"code","edf750b2":"code","05750696":"code","f12076ae":"code","16e89df9":"markdown"},"source":{"cb00d277":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","733c46b5":"train_csv_file=pd.read_csv('..\/input\/train.csv')\ntrain_csv_file","25de19ec":"print(os.listdir(\"..\/input\/audio_train\/audio_train\"))","e36c7962":"filename_label={\"..\/input\/audio_train\/audio_train\/\"+k:v for k,v in zip(train_csv_file.fname.values, train_csv_file.label.values)}","57cbd26e":"filename_label['..\/input\/audio_train\/audio_train\/fff81f55.wav']","c964f84e":"import librosa","110f94c2":"\ndef audio_norm(data):\n\n    max_data = np.max(data)\n    min_data = np.min(data)\n    data = (data-min_data)\/(max_data-min_data)\n    return data\n\n","2aa74a6d":"input_length = 16000*2\ndef load_audio_file(file_path, input_length=input_length):\n    data = librosa.core.load(file_path, sr=16000)[0] #, sr=16000\n    if len(data)>input_length:\n        \n        \n        max_offset = len(data)-input_length\n        \n        offset = np.random.randint(max_offset)\n        \n        data = data[offset:(input_length+offset)]\n        \n        \n    else:\n        \n        if input_length > len(data):\n            max_offset = input_length - len(data)\n\n            offset = np.random.randint(max_offset)\n        else:\n            offset = 0\n        \n        \n        data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n        \n        \n    data = audio_norm(data)\n    return data","699c9386":"import glob\ntrain_files = glob.glob(\"..\/input\/audio_train\/audio_train\/*.wav\")","c72068f3":"train_files","7e5af448":"import matplotlib.pyplot as plt","fef7ab49":"data_base = load_audio_file(train_files[9])\nfig = plt.figure(figsize=(14, 8))\nplt.title('Raw wave : %s ' % (filename_label[train_files[0]]))\nplt.ylabel('Amplitude')\nplt.plot(np.linspace(0, 1, input_length), data_base)\nplt.show()","bc8b35aa":"list_labels = sorted(list(set(train_csv_file.label.values)))","78dbb586":"list_labels","0566829c":"label_to_int = {k:v for v,k in enumerate(list_labels)}","7b57c6ce":"label_to_int","ff032883":"int_to_label = {v:k for k,v in label_to_int.items()}","a7ea1f91":"int_to_label","4df43aa7":"file_to_int = {k:label_to_int[v] for k,v in filename_label.items()}","e62c6d98":"file_to_int","a0a1b5f0":"n_class=41","04ac1af2":"from keras.models import Sequential\nfrom keras.layers import Conv1D,MaxPool1D,Dropout,GlobalMaxPool1D,Dense","0f7a5b1f":"model=Sequential()\nmodel.add(Conv1D(16,kernel_size=9,activation='relu',input_shape=(input_length,1)))\nmodel.add(Conv1D(16,kernel_size=9,activation='relu'))\nmodel.add(MaxPool1D(pool_size=16))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Conv1D(32,kernel_size=3,activation='relu'))\nmodel.add(Conv1D(32,kernel_size=3,activation='relu'))\nmodel.add(MaxPool1D(pool_size=4))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Conv1D(32,kernel_size=3,activation='relu'))\nmodel.add(Conv1D(32,kernel_size=3,activation='relu'))\nmodel.add(MaxPool1D(pool_size=4))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Conv1D(64,kernel_size=3,activation='relu'))\nmodel.add(Conv1D(64,kernel_size=3,activation='relu'))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dropout(rate=0.2))\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dense(n_class,activation='softmax'))\n          \n","7677e0f6":"model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['acc'])","da5ffa78":"model.summary()","bdb28e4c":"batch_size=32","43fb213c":"from random import shuffle\nfrom sklearn.model_selection import train_test_split","33101b97":"def chunker(seq, size):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))","86ef218a":"def train_generator(list_files, batch_size=batch_size):\n    while True:\n        shuffle(list_files)\n        for batch_files in chunker(list_files, size=batch_size):\n            batch_data = [load_audio_file(fpath) for fpath in batch_files]\n            batch_data = np.array(batch_data)[:,:,np.newaxis]\n            batch_labels = [file_to_int[fpath] for fpath in batch_files]\n            batch_labels = np.array(batch_labels)\n            \n            yield batch_data, batch_labels","1e74d311":"tr_files, val_files = train_test_split(train_files, test_size=0.1)","edf750b2":"len(tr_files)","05750696":"model.fit_generator(train_generator(tr_files), steps_per_epoch=len(tr_files)\/\/batch_size, epochs=2,\n                    validation_data=train_generator(val_files), validation_steps=len(val_files)\/\/batch_size,use_multiprocessing=True, workers=8, max_queue_size=20)","f12076ae":"model.save_weights(\"audio_tagging.h5\")","16e89df9":"filename_label will store the file names as the keys and  the lable as the vlaue"}}