{"cell_type":{"afdc104c":"code","3549092e":"code","63a01442":"code","9b894921":"code","ab287e6f":"code","902160d5":"code","ac6abb5a":"code","de72b8be":"code","2d644d70":"code","f2cf1fcf":"code","a8ea1858":"code","75108448":"code","e03e7c2f":"code","c554806e":"code","b15e314b":"code","733f5314":"code","f473669c":"code","980c53f7":"code","9ae0b155":"code","e1f42a7a":"code","767d2933":"code","acafc8cf":"code","a01c3b81":"code","684c3629":"markdown","42f8aa99":"markdown","e257644d":"markdown","cfc057a4":"markdown","f8dba9ea":"markdown","f464ef38":"markdown","e05bad91":"markdown","328def5d":"markdown","f9280202":"markdown","5834e08d":"markdown","629b036b":"markdown","882b7da1":"markdown","58e35a50":"markdown","251a2cc7":"markdown","75b2986c":"markdown","0c6a22da":"markdown","9faac932":"markdown","c1f62680":"markdown","0656a7d9":"markdown","1dc6abde":"markdown","91d31689":"markdown","9c5a64d0":"markdown"},"source":{"afdc104c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n\ndt=pd.read_csv(\"..\/input\/Reviews.csv\")\n''' We only used the csv file to read and understand the daata '''\nprint(dt.head(2))\n","3549092e":"print(\"The shape of the data :\",dt.shape)\nprint(\"The column names are :\",dt.columns.values)","63a01442":"# data cleaning\nimport sqlite3 \ncon = sqlite3.connect('..\/input\/database.sqlite') ","9b894921":"user_list=pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE  Score != 3 LIMIT 5000\"\"\", con)\n# we are using sql as it will be easy to limit the 5000 users using sql query\nuser_list.shape","ab287e6f":"# i checked for entire review data in the begining i got dense error while applying toDense function to the vectorized data \n#  so i am limiting the review data to 5K which is working fine\n\n# we can determine the review is positive or not if score is 3\n# print(user_list.columns.values)\n# print(set(user_list))\n\n\nsort_data=user_list.sort_values('ProductId', axis=0,kind='mergesort', inplace=False, ascending=True)\n# The use of sort_values is mentioned here https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.sort_values.html\n\n\"\"\"\nI have observed that when i took the whole reviews\n    3287690 no of rows have came when subset is {'UserId', 'ProfileName', 'Time'}\n    3632240 no of rows have came when subset is {'UserId', 'ProfileName', 'Time', 'Summary'}\n    so there may be scenario in which 2 comments getting update by same user at same time so taking 4 attributes will make it unique\n    2 comments by same user can get updated at same time may be due to multiple devices or network issue\n\"\"\"\n# case 1 which i tried earlier and observed the above issue\n# Eliminating the duplicate data points based on: 'UserId', 'ProfileName', 'Time'\n# sort_data1=user_list.drop_duplicates(subset={'UserId', 'ProfileName', 'Time'}, keep='first', inplace=False)\n# data1 = sort_data1[sort_data1['HelpfulnessDenominator'] >= sort_data1['HelpfulnessNumerator']]\n\n# case 2 which we are using now\n# Eliminating the duplicate data points based on: 'UserId', 'ProfileName', 'Time', 'Summary'\nsort_data.drop_duplicates(subset=['UserId', 'ProfileName', 'Time', 'Summary'], keep='first', inplace=True)\n## There are some users 'SELECT * FROM Reviews WHERE ProductId=7310172001   UserId=AJD41FBJD9010 Time=1233360000'\n##  which has same summary so taking summary also unique\n\n\n# kepping inplace=True as it will save memory instead of holding duplicate values seperately in other variable\n\ndata = sort_data[sort_data['HelpfulnessDenominator'] >= sort_data['HelpfulnessNumerator']]\n# as HelpfulnessDenominator should cannot be less than HelpfulnessNumerator\n\nprint(\"The size which remained after deduplication is : \")\n\nprint(data.shape)\n#print(data1.size)  here data1 is used to understand the data when we used subset parameter as 'UserId', 'ProfileName', 'Time'\n","902160d5":"# sort_data.merge(sort_data1,indicator = True, how='left').loc[lambda x : x['_merge']!='both'] \nprint(data[:5])\n","ac6abb5a":"#just checking the random text reviews to understand the data format\nar=[2500,300,2342,0,1000]\nprint(\"Checking the random texts to understand and applying the above mentioned cleaning techniques\")\nfor i in ar:\n    print(i)\n    print(data[\"Text\"].values[i])\n    print(\"=\"*50)\n    ","de72b8be":"import re\n\ndef removeHtml(text):\n    cleanTxt=re.sub(re.compile('<.*>'),' ',text)\n    return cleanTxt\n\n# contractions words are taken from https:\/\/stackoverflow.com\/a\/47091490\/4084039\ncontractions = {\"ain't\": \"am not \/ are not \/ is not \/ has not \/ have not\",\"aren't\": \"are not \/ am not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he had \/ he would\",\"he'd've\": \"he would have\",\"he'll\": \"he shall \/ he will\",\"he'll've\": \"he shall have \/ he will have\",\"he's\": \"he has \/ he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how has \/ how is \/ how does\",\"I'd\": \"I had \/ I would\",\"I'd've\": \"I would have\",\"I'll\": \"I shall \/ I will\",\"I'll've\": \"I shall have \/ I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\"it'd\": \"it had \/ it would\",\"it'd've\": \"it would have\",\"it'll\": \"it shall \/ it will\",\"it'll've\": \"it shall have \/ it will have\",\"it's\": \"it has \/ it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\"she'd\": \"she had \/ she would\",\"she'd've\": \"she would have\",\"she'll\": \"she shall \/ she will\",\"she'll've\": \"she shall have \/ she will have\",\"she's\": \"she has \/ she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so as \/ so is\",\"that'd\": \"that would \/ that had\",\"that'd've\": \"that would have\",\"that's\": \"that has \/ that is\",\"there'd\": \"there had \/ there would\",\"there'd've\": \"there would have\",\"there's\": \"there has \/ there is\",\"they'd\": \"they had \/ they would\",\"they'd've\": \"they would have\",\"they'll\": \"they shall \/ they will\",\"they'll've\": \"they shall have \/ they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had \/ we would\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what shall \/ what will\",\"what'll've\": \"what shall have \/ what will have\",\"what're\": \"what are\",\"what's\": \"what has \/ what is\",\"what've\": \"what have\",\"when's\": \"when has \/ when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\"where's\": \"where has \/ where is\",\"where've\": \"where have\",\"who'll\": \"who shall \/ who will\",\"who'll've\": \"who shall have \/ who will have\",\"who's\": \"who has \/ who is\",\"who've\": \"who have\",\"why's\": \"why has \/ why is\",\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had \/ you would\",\"you'd've\": \"you would have\",\"you'll\": \"you shall \/ you will\",\"you'll've\": \"you shall have \/ you will have\",\"you're\": \"you are\",\"you've\": \"you have\"}\n\ndef decontracted(text):\n    temp_txt=\"\"\n    for ele in text.split(\" \"):\n        if ele in contractions:\n            temp_txt = temp_txt+ \" \"+ contractions[ele].split(\"\/\")[0] # we are taking the only first value before the \/ so to avoid duplicate words\n        else:\n            temp_txt = temp_txt+ \" \" +ele\n    return  temp_txt\n\nstopwords=[\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"]\n# removed the words \"no\",\"nor\",\"not\",\n# the words like \"don't\",\"aren't\" are there in the list as the text is decontracted before\n\nfrom tqdm import tqdm\ncleaned_reviews = []\n# tqdm is for printing the status bar\ni=0\nfor txt in tqdm(data['Text'].values):\n    txt = removeHtml( re.sub(r\"http\\S+\", \" \", txt)) # remove all the <html > tags and http links (step 1)\n    \n    txt = re.sub(r'[?|.|!|*|@|#|\"|,|)|(|\\|\/]', r'', txt) # removing punctuations (step 2)\n    txt = re.sub('[^A-Za-z]+', ' ', txt)  # checking the alphanumeric characters (step 3)\n    txt = re.sub(\"\\S*\\d\\S*\", \" \", txt).strip() # removing numeric characters \n    txt = decontracted(txt)  # to remove the contacted words (step 6)\n   \n    # https:\/\/gist.github.com\/sebleier\/554280\n    txt = ' '.join(e.lower() for e in txt.split() if e.lower() and len(e)>2 not in stopwords) \n    txt = ' '.join(e for e in txt.split() if e!=(len(e) *e[0])  not in stopwords) \n    \n    # to check characters like 'a' 'aaaa' 'bbbbb' 'hhhhhhhhhh' 'mmmmmmm' which doesn't make sense\n    \n    # (step 4) and  (step 6) checking if length is less than 2 and converting to lower case\n    \n    cleaned_reviews.append(txt.strip())\n    \nprint(data['Text'].values[:2])    \nprint(\"#\"*50+\"to compare the changes\\n\\n\")\nprint(cleaned_reviews[:2])    \n  ","2d644d70":"# Making the seperation of positive reviews in seperate columns\ndef sep(score):\n    if score<3:\n        return 'Positive'\n    else :\n        return 'Negative'\nType_review = data['Score']\nType_review = Type_review.map(sep)\ndata.loc[:,'Type'] = Type_review\nprint(data[\"Type\"][:6])","f2cf1fcf":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer  = CountVectorizer() #in scikit-learn\n'''\nvectorizer.fit(cleaned_reviews)\n# fit - Learn a vocabulary dictionary of all tokens in the raw documents.\nBag_of_words = vectorizer.transform(cleaned_reviews) \n# Here cleaned_reviews is our data corpus\n'''\n # this line replaces the above 2 lines \nBag_of_words = vectorizer.fit_transform(cleaned_reviews)\n\nprint(\"Shape of the Bag of words formed :\",Bag_of_words.get_shape())\nprint(vectorizer.get_feature_names()[:10])\n","a8ea1858":"# Bag of words are formed in the  above code\n# as we see the first 10 attributes some doesn't make sense But there occurence is very low and are negligible\nprint(\"Bag of words data type :\",type(Bag_of_words))\nprint(\"Cleaned reviews data type :\",type(cleaned_reviews))\nBag_of_words_dense = Bag_of_words.todense()\n# converting sparse matrix to dense as we need dense matrix for standardising\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\n\nBag_of_words_Standardised= StandardScaler().fit_transform(Bag_of_words_dense)\n","75108448":"'''\n perplexity 2 ,iterations 250  ---  the iteration is very close\n perplexity 50 ,iterations 250\n perplexity 50 ,iterations 5000\n perplexity 150 ,iterations 5000\n perplexity 400 ,iterations 5000\n\n'''\nar=[(2,250),(50,250),(50,5000),(150,5000),(400,5000)]\ni1=1\n\nfor i in tqdm(ar):\n    i1=i1+1\n    plt.subplot(330+i1)\n    Bag_of_words_model = TSNE(n_components=2,perplexity=i[0],random_state = 0,n_iter=i[1])\n    # n_iter value should be atleast 250\n    Bag_of_words_data = Bag_of_words_model.fit_transform(Bag_of_words_Standardised)\n    \n    Bag_of_words_vstackdata = np.vstack((Bag_of_words_data.T,data['Type'])).T\n    \n    Bag_of_words_plotData  = pd.DataFrame(Bag_of_words_vstackdata,columns=('1st Higher Dimension','2nd Higher Dimension','Category'))\n    sns.FacetGrid(Bag_of_words_plotData,hue='Category',size=8).map(plt.scatter,'1st Higher Dimension','2nd Higher Dimension').add_legend()\n    \n    plt.title('Bag of words n_iter='+str(i[1])+' Perplexity = '+str(i[0]))\n    plt.show()\n","e03e7c2f":"\n'''\nThis block contains generation of Bi-grams and N-grams\n'''\n\nvectorizer  = CountVectorizer(ngram_range=(1,2),max_features=5000, min_df=10) #in scikit-learn\n\n# ngram_range --- 2 parameters min and maximum\n\n# min_df ---When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature.\n\n# build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n\nBi_grams = vectorizer.fit_transform(cleaned_reviews)\n\nprint(\"Shape of the Bag of words formed :\",Bi_grams.get_shape())\nprint(vectorizer.get_feature_names()[:10])\n","c554806e":"'''\n perplexity 50 ,iterations 5000\n perplexity 150 ,iterations 5000\n perplexity 400 ,iterations 5000\n\n'''\nar=[(50,5000),(150,5000),(400,5000)]\ni1=1\n\nfor i in tqdm(ar):\n    i1=i1+1\n    plt.subplot(330+i1)\n    Bi_gram_model = TSNE(n_components=2,perplexity=i[0],random_state = 0,n_iter=i[1])\n    # n_iter value should be atleast 250\n    Bi_gram_data = Bi_gram_model.fit_transform(Bag_of_words_Standardised)\n    \n    Bi_gram_vstackdata = np.vstack((Bi_gram_data.T,data['Type'])).T\n    \n    Bi_gram_plotData  = pd.DataFrame(Bi_gram_vstackdata,columns=('1st Higher Dimension','2nd Higher Dimension','Category'))\n    sns.FacetGrid(Bi_gram_plotData,hue='Category',size=8).map(plt.scatter,'1st Higher Dimension','2nd Higher Dimension').add_legend()\n    \n    plt.title('Bi grams n_iter='+str(i[1])+' Perplexity = '+str(i[0]))\n    plt.show()\n","b15e314b":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n'''\nThis block contains generation of Tf-Idf data\n'''\ntf_idf_vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n# # ngram_range --- 2 parameters min and maximum\ntf_idf_data = tf_idf_vectorizer.fit_transform(cleaned_reviews)\n\nprint(tf_idf_vectorizer.get_feature_names()[:10])\nprint('='*50)\nprint(\"data type of  data : \",type(tf_idf_data))\nprint('='*50)\nprint(\"size of data :\",tf_idf_data.get_shape())\n\n","733f5314":"'''\n perplexity 50 ,iterations 5000\n perplexity 150 ,iterations 5000\n perplexity 400 ,iterations 5000\n\n'''\nfrom sklearn.preprocessing import StandardScaler\n\n\ni1=1\ntf_idf_densedata =tf_idf_data.todense()\nTfIdf_standardized_data = StandardScaler().fit_transform(tf_idf_densedata)\n#print(TfIdf_standardized_data[:10])\nar=[(30,5000),(50,5000),(150,5000),(700,5000)]\n#ar=[]\nfor i in tqdm(ar):\n    i1=i1+1\n    plt.subplot(330+i1)\n    tf_idf_model = TSNE(n_components=2,perplexity=i[0],random_state = 0,n_iter=i[1])\n    # n_iter value should be atleast 250\n    \n    tf_idf_fit_data = tf_idf_model.fit_transform(TfIdf_standardized_data)\n    \n    tf_idf_vstackdata = np.vstack((tf_idf_fit_data.T,data['Type'])).T\n    \n    tf_idf_plotData  = pd.DataFrame(tf_idf_vstackdata,columns=('1st Higher Dimension','2nd Higher Dimension','Category'))\n    sns.FacetGrid(tf_idf_plotData,hue='Category',size=6).map(plt.scatter,'1st Higher Dimension','2nd Higher Dimension').add_legend()\n    \n    plt.title('tf-idf n_iter='+str(i[1])+' Perplexity = '+str(i[0]))\n    plt.grid()\n    plt.show()\nprint(TfIdf_standardized_data.shape)","f473669c":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\n\ni=0\nlist_of_words=[]\nfor sentance in cleaned_reviews:\n    list_of_words.append(sentance.split())\n\nw2v_model=Word2Vec(list_of_words,min_count=5,size=50, workers=4)\n# training the w2v model\nprint(\"Data type of word 2 vec model : \",type(w2v_model))","980c53f7":"\n''' here w2v_model.wv.vocab contains all the vocabulary words in the reviews '''\nprint(\"number of words in vocabulary :\",len(w2v_model.wv.vocab),\"\\n\")\nprint(w2v_model.wv.most_similar('taste'),\"\\n\")\nprint('='*50,\"\\n\")\nprint(w2v_model.wv.most_similar('yummy'),\"\\n\")\n\nw2v_words = list(w2v_model.wv.vocab)","9ae0b155":"''' Construction of Avg Word 2 vec from built word 2 vec '''\n\nsentence_vectors=[]\nfor sent in list_of_words:\n    sent_vec_var = np.zeros(50)\n    count_word =0\n    for word in sent :\n        if word in w2v_words:\n            vector = w2v_model.wv[word]\n            sent_vec_var += vector\n            count_word += 1\n    if count_word !=0:\n        sent_vec_var \/= count_word\n    sentence_vectors.append(sent_vec_var)    \nprint(\"The size of sentence : \",len(sentence_vectors))        \n\n","e1f42a7a":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\n\navgw2v_standardized = StandardScaler().fit_transform(sentence_vectors)\n\ni1=1\n\nar=[(30,5000),(50,5000),(700,5000)]\n#ar=[]\nfor i in tqdm(ar):\n    i1=i1+1\n    plt.subplot(330+i1)\n    avgw2v_model = TSNE(n_components=2,perplexity=i[0],n_iter=i[1])\n    \n    avgw2v_fit_data = avgw2v_model.fit_transform(avgw2v_standardized)\n    \n    avgw2v_vstackdata = np.vstack((avgw2v_fit_data.T,data['Type'])).T\n    \n    avgw2v_plotData  = pd.DataFrame(avgw2v_vstackdata,columns=('1st Higher Dimension','2nd Higher Dimension','Category'))\n    sns.FacetGrid(avgw2v_plotData,hue='Category',size=8).map(plt.scatter,'1st Higher Dimension','2nd Higher Dimension').add_legend()\n    \n    plt.title('tf-idf n_iter='+str(i[1])+' Perplexity = '+str(i[0]))\n    plt.grid()\n    plt.show()\n","767d2933":"\ntf_idf_vectorizer_model = TfidfVectorizer()\ntf_idf_vectorizer_model.fit(cleaned_reviews)\n\n\ndictionary = dict(zip(tf_idf_vectorizer_model.get_feature_names(), list(tf_idf_vectorizer_model.idf_)))\n# we are converting a dictionary with word as a key, and the idf as a value\n","acafc8cf":"tfidf_features = tf_idf_vectorizer_model.get_feature_names()\n# tfidf words\/col-names\ntfidf_sentence_vectors = [];\n\n# the tfidf-w2v for each sentence\/review is stored in this list\nrow=0;\n# tf_idf_data is constructed already in TFIDF\nfor sent in list_of_words:\n    sent_vec = np.zeros(50)\n    weight_sum =0\n    for word in sent:\n        if word in tfidf_features and word in w2v_words:\n            vector = w2v_model.wv[word]\n            # to reduce the computation we are \n            # dictionary[word] = idf value of word in whole courpus\n            # sent.count(word) = tf valeus of word in this review\n            tf_idf = dictionary[word]*(sent.count(word)\/len(sent))\n            sent_vec += (vector * tf_idf)\n            weight_sum += tf_idf\n    if weight_sum != 0:\n        sent_vec \/= weight_sum\n    tfidf_sentence_vectors.append(sent_vec)\n    row += 1    \nprint(len(tfidf_sentence_vectors))            \nprint(tfidf_sentence_vectors[0])            ","a01c3b81":"\nTfidf_w2v_standardized = StandardScaler().fit_transform(tfidf_sentence_vectors)\n\ni1=1\n\nar=[(30,5000),(50,5000),(700,5000)]\n#ar=[]\nfor i in tqdm(ar):\n    i1=i1+1\n    plt.subplot(330+i1)\n    Tfidf_w2v_model = TSNE(n_components=2,perplexity=i[0],n_iter=i[1])\n    \n    Tfidf_w2v_fit_data = Tfidf_w2v_model.fit_transform(avgw2v_standardized)\n    \n    Tfidf_w2v_vstackdata = np.vstack((Tfidf_w2v_fit_data.T,data['Type'])).T\n    \n    Tfidf_w2v_plotData  = pd.DataFrame(Tfidf_w2v_vstackdata,columns=('1st Higher Dimension','2nd Higher Dimension','Category'))\n    sns.FacetGrid(Tfidf_w2v_plotData,hue='Category',size=8).map(plt.scatter,'1st Higher Dimension','2nd Higher Dimension').add_legend()\n    \n    plt.title('tf-idf n_iter='+str(i[1])+' Perplexity = '+str(i[0]))\n    plt.grid()\n    plt.show()\n    ","684c3629":"### 4) Featurization\n\nThe below are the ways we are applying t-sne\n\na. Bag of Words\n","42f8aa99":"#### Construction of avg W2V","e257644d":"#### Plotting of TFIDF weighted W2V vectors","cfc057a4":"### 1) Data Reading","f8dba9ea":"for  construction of TFIDF weighted W2V vectors and avg w2v i have refered this site :\nhttps:\/\/www.kaggle.com\/prasoon05\/t-sne-amazon-fine-food-reviews\n","f464ef38":"#### 4.b T-sne for Bi-Grams and n-Grams.","e05bad91":"#### 4.c T-sne for TF-IDF","328def5d":"#### 4.a  Bag of Words","f9280202":"#### T-Sne on Bag of words","5834e08d":" Objective :\n \n1) To understand and Perform T-Sne for Amazon Food reviews By using these four techniques :\n                (i) BoW                 (ii) TF-IDF                (iii) Word2Vec                (iv) TFIDF-W2V\n                \n2) To Understand the behaviour of T-Sne with different perplexity and iterations with different techniques\n\n3) We are plotting to seperate the review between positive and negative\n\nHere i am assuming the review below 3 as negative and above 3 as positive and 3 to be neutral and ignoring the neutral review\n    ","629b036b":"\nData includes:\n- Reviews from Oct 1999 - Oct 2012\n- 568,454 reviews\n- 256,059 users\n- 74,258 products\n- 260 users with > 50 reviews","882b7da1":"### 3) Text Preprocessing\n\nHence in the Preprocessing phase we do the following in the order below:-\n\n1. By removing the html tags\n2. Remove any punctuations or limited set of special characters like , or . or # etc.\n3. Check if the word is made up of english letters and is not alpha-numeric\n4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n5. Convert the word to lowercase\n6. to Remove the English contractions \n7. Remove Stopwords\n8. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)\n\n","58e35a50":"### 2) Data Cleaning","251a2cc7":"#### Construction of TFIDF weighted W2V vectors","75b2986c":"\n#### 4.d   TNSE for Text Avg W2V vectors\n","0c6a22da":"Attribute Information\n\n1)Id - Row Id\n\n2)ProductId - Unique identifier for the product\n\n3)UserId - Unqiue identifier for the user\n\n4)ProfileName - Profile name of the user\n\n5)HelpfulnessNumerator - Number of users who found the review helpful\n\n6)HelpfulnessDenominator - Number of users who indicated whether they found the review helpful or not\n\n7)Score - Rating between 1 and 5\n\n8)Time - Timestamp for the review\n\n9)Summary - Brief summary of the review\n\n10)Text - Text of the review\n\n11) Adding a Attribute Type to determine if the comment is positive or negative based on the review\n","9faac932":"Context:\n\nThis dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.","c1f62680":"#### 4.e TNSE on Text TFIDF weighted W2V vectors","0656a7d9":"####  Construction of W2V","1dc6abde":"Conclusion :\n\n1) Computationally it takes more time to compute for large data\n\n2) The data is getting overlapped in most of the cases.\n\n3) By Applying various vector representaion of words techniques and different perplexity and iterations we are unable to seperate the data points\n","91d31689":"# Amazon Fine Food Reviews Analysis using TSNE","9c5a64d0":"#### Plotting T-Sne for avg W2V"}}