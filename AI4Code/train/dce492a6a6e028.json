{"cell_type":{"229d005f":"code","f1924333":"code","69c83495":"code","7d859d95":"code","6b2837d3":"code","fa658787":"code","2ddcbc2e":"code","3d2858e9":"code","f2ee5d64":"code","88fabc5f":"code","7e259e57":"code","2999a44a":"code","09a6611d":"code","878f3464":"code","73cea454":"code","c8833ab7":"code","7db0a6fc":"code","25d9a5b3":"code","c1e3d3d8":"code","d347314a":"code","652d1081":"code","e9057553":"code","2890d60e":"code","7a7a3adf":"code","1115ddd9":"code","b04b52e1":"code","b0c7a8c8":"code","69e53227":"code","1572aca9":"code","a47d5a1b":"code","05a4bf04":"code","a3fbb372":"code","7903a23a":"code","c8509fcb":"markdown","4d603631":"markdown","81d23858":"markdown","7670da5e":"markdown","d2266b1d":"markdown","72bae817":"markdown","8dd6d6db":"markdown"},"source":{"229d005f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f1924333":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.svm import SVR\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor","69c83495":"data = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/bmw.csv')\ndata.head()","7d859d95":"data.isnull().sum()","6b2837d3":"data.describe()","fa658787":"(data['model'].value_counts()\/len(data))*100","2ddcbc2e":"plt.figure(figsize=(24,5))\nplt.xticks(rotation = 20)\nsns.barplot(x = data['model'], y = data['price'], data = data, hue = data['transmission'])","3d2858e9":"transmission_counts = dict(data['transmission'].value_counts())\nplt.title('Transmission Distribution', size = 20)\nplt.pie(transmission_counts.values(), labels=transmission_counts.keys(), textprops={'size' : 14}, autopct='%1.2f%%')\nplt.show()","f2ee5d64":"plt.figure(figsize = (15,5))\nsns.barplot(x = data['year'], y = data['price'])","88fabc5f":"plt.figure(figsize=(15,5))\nsns.scatterplot(x = data['mileage'], y = data['price'], hue = data['year'])","7e259e57":"sns.countplot(x = data['fuelType'])","2999a44a":"plt.hist(data['engineSize'], bins=5, color='brown')\nplt.show()","09a6611d":"sns.pairplot(data = data)","878f3464":"corr = data.corr()\ncorr_dataFrame = corr['price'].sort_values(ascending=False).to_frame()\ns = corr_dataFrame.style.background_gradient(cmap = 'coolwarm')\ns","73cea454":"data['car_age'] = 2021 - data['year']\ndata = data.drop(columns = ['year'])\n\nnumerical_variables = [var for var in data.columns if data[var].dtype != 'O']\nprint('There are {} numerical variables'.format(len(numerical_variables)))\nprint('The numerical variables are: ', numerical_variables)\n\nplt.figure(figsize=(12,8))\nplt.title('Numerical Variables in BMW Dataset')\ndata[numerical_variables].boxplot(color = 'brown')\nplt.show()","c8833ab7":"data[data['price'] >= 90000]","7db0a6fc":"data[data['mileage'] >= 200000]","25d9a5b3":"i1 = data[data.mileage >= 200000].index\ni2 = data[data.price >= 90000].index\ndata = data.drop(i1)\ndata = data.drop(i2)","c1e3d3d8":"data_expanded = pd.get_dummies(data)\ndata_expanded.head()","d347314a":"std = StandardScaler()\ndata_expanded_std = std.fit_transform(data_expanded)\ndata_expanded_std = pd.DataFrame(data_expanded_std, columns = data_expanded.columns)\nprint(data_expanded.shape)","652d1081":"data_expanded_std.head()","e9057553":"x_train, x_test, y_train, y_test = train_test_split(data_expanded_std.drop(columns = ['price']), data_expanded_std[['price']])\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","2890d60e":"def test_models(models, x_train, x_test, y_train, y_test):\n\n    np.random.seed(42)\n\n    model_mse = {}\n    model_mape = {}    \n    model_r2 = {}\n\n    for name, model in models.items():\n        model.fit(x_train, y_train.values.ravel())\n        y_preds = model.predict(x_test)\n        model_mse[name] = mean_squared_error(y_test, y_preds)\n        model_mape[name] = np.mean(np.abs((np.array(y_test)-np.array(y_preds))\/np.array(y_test)))*100\n        model_r2[name] = r2_score(y_test, y_preds)\n\n    model_mse = pd.DataFrame(model_mse, index = ['MSE']).transpose()\n    model_mse = model_mse.sort_values('MSE', ascending=False)\n\n    model_mape = pd.DataFrame(model_mape, index = ['MAPE']).transpose()\n    model_mape = model_mape.sort_values('MAPE', ascending=False)\n\n    model_r2= pd.DataFrame(model_r2, index = ['R2']).transpose()\n    model_r2 = model_r2.sort_values('R2')\n\n    return model_mse, model_mape, model_r2","7a7a3adf":"models = {'LinearRegression' : LinearRegression(),\n          'KNeighborsRegressor': KNeighborsRegressor(),\n          'DecisionTreeRegressor': DecisionTreeRegressor(),\n          'RandomForestRegressor':RandomForestRegressor(),\n          'GradientBoostingRegressor': GradientBoostingRegressor(),\n          'SVM': SVR()\n        }","1115ddd9":"model_mse,model_mape,model_r2 = test_models(models, x_train, x_test, y_train, y_test)","b04b52e1":"model_mse","b0c7a8c8":"model_mape","69e53227":"model_r2","1572aca9":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout","a47d5a1b":"model = Sequential()\n\nmodel.add(Dense(37, activation = 'relu'))\nmodel.add(Dense(24, activation = 'relu'))\nmodel.add(Dense(8, activation = 'relu'))\nmodel.add(Dense(1))\n\nmodel.compile(\n    optimizer = 'adam',\n    loss = tf.keras.losses.MSE\n)","05a4bf04":"history = model.fit(x = x_train, y =  y_train, epochs = 200)","a3fbb372":"y_preds = model.predict(x_test)\ny_preds = pd.DataFrame(y_preds)\nr2_nn_result = r2_score(y_test, y_preds)","7903a23a":"r2_nn_result","c8509fcb":"# Conclusion\nRandomForestRegressor and Neural Nwtwork gave the best results on the Dataset with an R2 score of nearly 95%.","4d603631":"We'll use the BMW dataset.","81d23858":"# Handling Outliers","7670da5e":"# Using Neural Network","d2266b1d":"We don't have any null values which is a good sign.","72bae817":"Let's check the percentage of different car models in the dataset.","8dd6d6db":"Now using One Hot Encoding on categorical data.\nI am using pd.get_dummies for the same."}}