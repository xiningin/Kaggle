{"cell_type":{"dbf5a813":"code","2bf5de5c":"code","6f0681e6":"code","ae7e5c15":"code","c9557a87":"code","82f69a87":"code","ea7658f1":"code","7c14ec82":"code","6805521c":"code","ca2e0060":"code","bf5184ac":"code","f2d740f3":"code","0519f9f0":"code","4b6d3703":"code","b6058354":"code","a5d0bc15":"code","25124f1a":"code","af2f5e76":"code","ab8535d7":"code","5f4ae2e1":"code","482f7ea9":"code","7a1e8496":"code","2dd66a0c":"code","726ca460":"code","f0d7fe1b":"code","1fc9a370":"code","ffe8d3be":"code","26e8ca65":"code","8c968a3d":"code","6fbbae5a":"code","6012888e":"code","1a36b38d":"code","3680ec6f":"code","601313b2":"code","7262f5b8":"code","6589c4c1":"code","0e985b48":"code","62cc32f7":"code","6e59e09d":"code","c32f41dd":"code","0dc22fba":"code","8ef986c2":"code","3d1fc6db":"code","36d78ac5":"code","3f5f06fb":"code","896de1ec":"code","328ff96a":"code","cf1679cf":"code","aba50155":"code","4ac97167":"markdown","277132fd":"markdown","5339a712":"markdown","e0df3be7":"markdown","75097800":"markdown","44a76412":"markdown"},"source":{"dbf5a813":"from collections import Counter\nimport operator\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nimport re\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom tensorflow.keras.preprocessing.text import one_hot, Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, TimeDistributed, RepeatVector, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\nfrom tensorflow.keras.models import Model\n# setting the style of the notebook to be monokai theme  \n# this line of code is important to ensure that we are able to see the x and y axes clearly\n# If you don't run this code line, you will notice that the xlabel and ylabel on any plot is black on black and it will be hard to see them. \n","2bf5de5c":"# load the data\ndf_english = pd.read_csv(\"..\/input\/french-to-eng\/small_vocab_en\", sep = '\/t', names = ['english'])\ndf_french = pd.read_csv(\"..\/input\/french-to-eng\/small_vocab_fr\", sep = '\/t', names = ['french'])","6f0681e6":"df_english","ae7e5c15":"df_french","c9557a87":"df = pd.concat([df_english, df_french], axis = 1)","82f69a87":"df","ea7658f1":"# download nltk packages\nnltk.download('punkt')\n\n# download stopwords\nnltk.download(\"stopwords\")","7c14ec82":"# function to remove punctuations\ndef remove_punc(x):\n    return re.sub('[!#?,.:\";]', '', x)","6805521c":"df['french'] = df['french'].apply(remove_punc)\ndf['english'] = df['english'].apply(remove_punc)","ca2e0060":"english_words = []\nfrench_words  = []","bf5184ac":"def get_unique_words(x, word_list):\n    for word in x.split():\n        if word not in word_list:\n            word_list.append(word)\n\ndf[\"english\"].apply(lambda x: get_unique_words(x, english_words));\ndf[\"french\"].apply(lambda x: get_unique_words(x, french_words));","f2d740f3":"# number of unique words in french\ntotal_english_words = len(english_words)\ntotal_french_words = len(french_words)\nprint(total_english_words)\nprint(total_french_words)","0519f9f0":"# Obtain list of all words in the dataset\nwords = []\nfor i in df['english']:\n    for word in i.split():\n        words.append(word)\n    \nwords","4b6d3703":"# Obtain the total count of words\nenglish_words_counts = Counter(words)\nenglish_words_counts","b6058354":"# sort the dictionary by values\nenglish_words_counts = sorted(english_words_counts.items(), key = operator.itemgetter(1), reverse = True)","a5d0bc15":"english_words_counts","25124f1a":"english_words, english_counts = zip(*english_words_counts)\nenglish_words = list(english_words)\nenglish_counts = list(english_counts)","af2f5e76":"english_words","ab8535d7":"english_counts","5f4ae2e1":"# Plot barplot using plotly \nfig = px.bar(x = english_words, y = english_counts)\nfig.show()","482f7ea9":"# plot the word cloud for text that is Real\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000, width = 1600, height = 800 ).generate(\" \".join(df.english))\nplt.imshow(wc, interpolation = 'bilinear')","7a1e8496":"df.english[0]\nnltk.word_tokenize(df.english[0])","2dd66a0c":"# Maximum length (number of words) per document. We will need it later for embeddings\nmaxlen_english = -1\nfor doc in df.english:\n    tokens = nltk.word_tokenize(doc)\n    if(maxlen_english < len(tokens)):\n        maxlen_english = len(tokens)\nprint(\"The maximum number of words in any document = \", maxlen_english)","726ca460":"# Obtain list of all words in the dataset\nwords = []\nfor i in df['french']:\n    for word in i.split():\n        words.append(word)\n    \nwords","f0d7fe1b":"# Obtain the total count of words\nfrench_words_counts = Counter(words)\nfrench_words_counts","1fc9a370":"# sort the dictionary by values\nfrench_words_counts = sorted(french_words_counts.items(), key = operator.itemgetter(1), reverse = True)","ffe8d3be":"french_words_counts","26e8ca65":"french_words, french_counts = zip(*french_words_counts)\nfrench_words = list(french_words)\nfrench_counts = list(french_counts)","8c968a3d":"french_words","6fbbae5a":"french_counts","6012888e":"# Plot barplot using plotly \nfig = px.bar(x = french_words, y = french_counts)\nfig.show()","1a36b38d":"# plot the word cloud for text that is Real\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000, width = 1600, height = 800 ).generate(\" \".join(df.french))\nplt.imshow(wc, interpolation = 'bilinear')","3680ec6f":"df.french[0]\nnltk.word_tokenize(df.french[0])","601313b2":"# Maximum length (number of words) per document. We will need it later for embeddings\nmaxlen_french = -1\nfor doc in df.french:\n    tokens = nltk.word_tokenize(doc)\n    if(maxlen_french < len(tokens)):\n        maxlen_french = len(tokens)\nprint(\"The maximum number of words in any document = \", maxlen_french)","7262f5b8":"def tokenize_and_pad(x, maxlen):\n    #  a tokenier to tokenize the words and create sequences of tokenized words\n    tokenizer = Tokenizer(char_level = False)\n    tokenizer.fit_on_texts(x)\n    sequences = tokenizer.texts_to_sequences(x)\n    padded = pad_sequences(sequences, maxlen = maxlen, padding = 'post')\n    \n    return tokenizer, sequences, padded","6589c4c1":"# tokenize and padding to the data \nmaxlen = max(maxlen_english, maxlen_french)\nx_tokenizer, x_sequences, x_padded = tokenize_and_pad(df.english, maxlen)\ny_tokenizer, y_sequences, y_padded = tokenize_and_pad(df.french,  maxlen)","0e985b48":"# Total vocab size, since we added padding we add 1 to the total word count\nenglish_vocab_size = total_english_words + 1\nprint(\"Complete English Vocab Size:\", english_vocab_size)","62cc32f7":"# Total vocab size, since we added padding we add 1 to the total word count\nfrench_vocab_size = total_french_words + 1\nprint(\"Complete French Vocab Size:\", french_vocab_size)","6e59e09d":"print(\"The tokenized version for document\\n\", df.english[-1:].item(),\"\\n is : \", x_padded[-1:])","c32f41dd":"print(\"The tokenized version for document\\n\", df.french[-1:].item(),\"\\n is : \", y_padded[-1:])","0dc22fba":"# function to obtain the text from padded variables\ndef pad_to_text(padded, tokenizer):\n\n    id_to_word = {id: word for word, id in tokenizer.word_index.items()}\n    id_to_word[0] = ''\n\n    return ' '.join([id_to_word[j] for j in padded])","8ef986c2":"pad_to_text(y_padded[0], y_tokenizer)","3d1fc6db":"# Train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x_padded, y_padded, test_size = 0.1)","36d78ac5":"# Sequential Model\nmodel = Sequential()\n# embedding layer\nmodel.add(Embedding(english_vocab_size, 256, input_length = maxlen, mask_zero = True))\n# encoder\nmodel.add(LSTM(256))\n# decoder\n# repeatvector repeats the input for the desired number of times to change\n# 2D-array to 3D array. For example: (1,256) to (1,23,256)\nmodel.add(RepeatVector(maxlen))\nmodel.add(LSTM(256, return_sequences= True ))\nmodel.add(TimeDistributed(Dense(french_vocab_size, activation ='softmax')))\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","3f5f06fb":"# change the shape of target from 2D to 3D\ny_train = np.expand_dims(y_train, axis = 2)\ny_train.shape","896de1ec":"# train the model\nmodel.fit(x_train, y_train, batch_size=1024, validation_split= 0.1, epochs=10)","328ff96a":"# save the model\nmodel.save(\"weights.h5\")","cf1679cf":"# function to make prediction\ndef prediction(x, x_tokenizer = x_tokenizer, y_tokenizer = y_tokenizer):\n    predictions = model.predict(x)[0]\n    id_to_word = {id: word for word, id in y_tokenizer.word_index.items()}\n    id_to_word[0] = ''\n    return ' '.join([id_to_word[j] for j in np.argmax(predictions,1)])","aba50155":"for i in range(5):\n    print('Original English word - {}\\n'.format(pad_to_text(x_test[i], x_tokenizer)))\n    print('Original French word - {}\\n'.format(pad_to_text(y_test[i], y_tokenizer)))\n    print('Predicted French word - {}\\n\\n\\n\\n'.format(prediction(x_test[i:i+1])))","4ac97167":"# PERFORM DATA CLEANING","277132fd":"# PREPARE THE DATA BY PERFORMING TOKENIZATION AND PADDING","5339a712":"# ASSESS TRAINED MODEL PERFORMANCE\n","e0df3be7":"# BUILD AND TRAIN THE MODEL ","75097800":"# VISUALIZE CLEANED UP DATASET","44a76412":"# IMPORT LIBRARIES AND DATASETS"}}