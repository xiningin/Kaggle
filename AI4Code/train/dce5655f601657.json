{"cell_type":{"4ac2a363":"code","2210eac1":"code","d0ab68c2":"code","62d1cca4":"code","ce9de8f6":"code","ab48d48d":"code","77de07a2":"code","65872515":"code","b3b5fb00":"code","39386a9b":"code","84e98956":"code","35eb3d56":"code","1fd1e0f4":"code","8e9c606f":"code","f82844d9":"code","1a8d8c5c":"code","7402a228":"code","b632537c":"code","791db0b1":"code","287d65c1":"code","889df200":"code","e20ee22b":"code","3711ce27":"code","55e21248":"code","3c246b92":"code","140ee90b":"code","fe39cb65":"code","7d7aaac5":"code","14ec08ec":"code","f31efab6":"code","63f5b7de":"code","4acb2d44":"code","607b6a03":"code","400b88df":"code","b1f34cc4":"code","0d74e725":"code","c17bf542":"code","d5e73328":"code","d39975bb":"code","33c13784":"code","cbbdedc8":"code","72655558":"code","4c1392f3":"code","7262cd98":"markdown","f990db03":"markdown","8ad19461":"markdown","318087b5":"markdown","6a966aae":"markdown","13553f71":"markdown"},"source":{"4ac2a363":"!unzip --qq ..\/input\/dogs-vs-cats\/train.zip -d .\n!unzip -qq ..\/input\/dogs-vs-cats\/test1.zip -d .","2210eac1":"import torchvision\nimport torch.nn as nn\nimport torchvision.transforms as v_transforms\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport pandas as pd\nimport glob\nimport torch\nimport os\nimport matplotlib.pyplot as plt","d0ab68c2":"all_training_files = glob.glob(\".\/train\/*.jpg\")[:1000]","62d1cca4":"os_training_files = os.listdir(\".\/train\")[:1000]","ce9de8f6":"os_training_files[0]","ab48d48d":"len(all_training_files)","77de07a2":"data_df = pd.DataFrame()\ndata_df['path'] = all_training_files","65872515":"data_df.sample(10)","b3b5fb00":"# manipulate str, path","39386a9b":"os.path.basename(all_training_files[1]).split('.')[0]","84e98956":"data_df['data_class'] = data_df['path'].apply(lambda x: os.path.basename(x).split('.')[0])","35eb3d56":"data_df['encoded_class'] = data_df['data_class'].map({\"dog\": 0, \"cat\": 1})","1fd1e0f4":"data_df","8e9c606f":"import numpy as np","f82844d9":"image = Image.open(np.random.choice(all_training_files))\nprint(image.size)\nimage","1a8d8c5c":"transform = v_transforms.Compose(\n    [v_transforms.Resize((256, 256)),\n     v_transforms.ToTensor(),\n#      v_transforms.Normalize((0.5), (0.5))\n    ])\n\nbatch_size = 8","7402a228":"ind = 1","b632537c":"print(data_df.iloc[ind]['encoded_class']), \nImage.open(data_df.iloc[ind]['path'])","791db0b1":"class CatDogDataset(object):\n    def __init__(self, data_df, transform=None):\n        self.data_df = data_df\n        self.transform = transform\n        \n        \n    def __len__(self):\n        return data_df.shape[0]\n    \n    def __getitem__(self, idx):\n        image_path = data_df.iloc[idx]['path']\n        label = data_df.iloc[idx]['encoded_class']\n        image = Image.open(image_path)\n        if transform is not None:\n            image = transform(image)\n            \n        return image, label\n            \n        ","287d65c1":"cat_dog_dataset = CatDogDataset(data_df=data_df, transform=transform)","889df200":"len(cat_dog_dataset), data_df.shape[0]","e20ee22b":"image, label = cat_dog_dataset[0]\n\nplt.imshow(image.numpy().transpose(1, 2, 0))\nplt.axis(\"off\")\nplt.title(f\"Class: {label}\")\nplt.show()","3711ce27":"image.shape, label","55e21248":"train_data_loader = DataLoader(cat_dog_dataset, batch_size=16)","3c246b92":"help(DataLoader)","140ee90b":"for batch in train_data_loader:\n    break","fe39cb65":"images, labels = batch","7d7aaac5":"def display_images(images, labels): \n\n    f, ax = plt.subplots(4,4, figsize=(10,15))\n\n    for i, image_id in enumerate(images[:16]):\n        image = images[i]\n        label = labels[i]\n        ax[i\/\/4, i%4].imshow(image.numpy().transpose(1, 2, 0)) \n        ax[i\/\/4, i%4].axis('off')\n\n        ax[i\/\/4, i%4].set_title(f\"Class: {label}\")\n    plt.tight_layout()\n    plt.show() ","14ec08ec":"display_images(images, labels)","f31efab6":"!pip install --quiet torchsummary","63f5b7de":"import torchvision\nimport torchsummary","4acb2d44":"alexnet_model = torchvision.models.alexnet(pretrained=False)","607b6a03":"torchsummary.summary(alexnet_model, input_size=(3, 224, 224), device=\"cpu\")","400b88df":"alexnet_model","b1f34cc4":"alexnet_model.features","0d74e725":"class SamplePretrainedModel(nn.Module):\n    def __init__(self, num_classes):\n        super(SamplePretrainedModel, self).__init__()\n        self.num_classes = num_classes\n        self.base_model = torchvision.models.alexnet(pretrained=True).features\n        self.fc = nn.Linear(12544, num_classes)\n\n    def forward(self, x):\n        x = self.base_model(x)\n\n        x = torch.flatten(x, start_dim=1)\n        x = self.fc(x)\n        return x","c17bf542":"sample_pretrained_model = SamplePretrainedModel(num_classes=2)","d5e73328":"sample_pretrained_model(torch.rand(1, 3, 256, 256)).shape","d39975bb":"sample_pretrained_model","33c13784":"!pip install timm","cbbdedc8":"import timm","72655558":"timm_model = timm.create_model('resnet18', num_classes=2)","4c1392f3":"torchsummary.summary(timm_model, input_size=(3, 224, 224), device=\"cpu\")","7262cd98":"## [timm](https:\/\/fastai.github.io\/timmdocs\/) library","f990db03":"# Data","8ad19461":"## torchvision","318087b5":"# Pretrained model","6a966aae":"# Dataset","13553f71":"# Library "}}