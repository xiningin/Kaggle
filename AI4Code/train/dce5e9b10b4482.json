{"cell_type":{"04d18160":"code","5b4a3a83":"code","ddfae354":"code","1a2004a7":"code","f36d6656":"code","0526e575":"code","8b6a0b21":"code","90b473fb":"code","ab285b19":"code","eb35dafb":"code","f849772d":"code","5d2fa87d":"code","aa1f24d1":"code","7ae8d66e":"code","2b537fbf":"code","c08a371e":"code","1cb52eb4":"code","d46de175":"code","0baaae39":"code","919c1f37":"code","e03eaa23":"code","6b817c46":"code","dd814042":"code","050e2525":"code","0dc9338c":"code","117b3d40":"code","0778f760":"code","849e29fd":"code","1168366e":"code","909a00a4":"code","ebb6dffc":"code","0c0a5097":"code","e86482de":"code","31ffb4c8":"code","23917069":"code","66056f5b":"code","440afbab":"code","81fdcc7f":"code","cc8fbf89":"code","26cd372a":"code","b99e21f3":"code","8b13f155":"code","be79157a":"code","01e640c3":"code","cd945914":"code","1bf48bb2":"code","57ca59a0":"code","8975ae66":"code","b4c08801":"code","a84dbdab":"code","dcdc0139":"code","bed9a7a7":"code","3a81f24c":"code","e44bcfb4":"code","a9c8641e":"code","3e4cb7e9":"code","b53f2448":"code","73dc69e0":"code","a1883906":"code","7c372251":"code","e23716e6":"code","b4d6e321":"code","0915f100":"code","987a727f":"code","c78d32e5":"code","53beea3f":"code","42b97903":"code","b8bec834":"code","21c5b0e5":"code","2ebbb1af":"code","d6935a70":"code","a1b926fd":"code","d4fba131":"code","eec6ce65":"code","a9717b03":"code","8e8bb4d6":"code","3ec77795":"code","f2f69612":"code","739969c6":"code","c196b0c9":"code","57495124":"code","421efba5":"code","0f9156d7":"code","1e8807e3":"code","a51e4bc7":"code","9cd637e6":"code","a2d065fb":"code","2a2413b2":"code","635033ad":"code","b671c636":"code","989b7372":"code","36b90406":"code","68531a2e":"code","7de36fd6":"code","b2f037bc":"code","d833f1bb":"code","9369329b":"code","4a59d668":"code","0dda3c3d":"code","d691ea26":"code","b9d08e77":"code","0c3184f7":"code","4deb64ca":"code","b6a80d5f":"code","c0906172":"markdown","d610e1cd":"markdown","e07cbda0":"markdown","7f90f48c":"markdown","4d2acaa6":"markdown","3b816c8f":"markdown","ebb2eed2":"markdown","163e1c45":"markdown","ca88c541":"markdown","dd000c1d":"markdown","eaeecae6":"markdown","00d4e1bd":"markdown","a546aeb9":"markdown","0cbc68f8":"markdown","beecb5f1":"markdown","209d5bd4":"markdown","599bf3b5":"markdown","ed2d3c32":"markdown","b2645fb4":"markdown","7c7f4c16":"markdown","4344b767":"markdown","9629ed76":"markdown","9920386f":"markdown","f9750a0e":"markdown","8696ebde":"markdown","c900e100":"markdown","ce06bb06":"markdown","d7ea5399":"markdown","27ca32a8":"markdown","94a7eb4d":"markdown","23e1211a":"markdown","ea44397f":"markdown","f036a380":"markdown","71981b1c":"markdown","06d1932f":"markdown","62f62996":"markdown","ed2f016e":"markdown","9c89c8d0":"markdown","2b7de44e":"markdown","88133c5b":"markdown","e8980e2f":"markdown","0ca2e2e1":"markdown","8c76a886":"markdown","6ebbdf5e":"markdown","eebd15e4":"markdown","278eaf0b":"markdown","2e5c91e7":"markdown","0a1e1941":"markdown","f15e752c":"markdown"},"source":{"04d18160":"import numpy as np                # useful for mathematical numeric operations\nimport pandas as pd               # Useful for data structuring\/Framing operations\nimport matplotlib.pyplot as plt   # Useful for data visualization\n%matplotlib inline\nimport seaborn as sns             # Useful for data visualization\nimport warnings\nwarnings.filterwarnings('ignore')","5b4a3a83":"Vehicle_df = pd.read_csv('..\/input\/vehicle-unsupervised-learning-project\/vehicle_Unsupervised Learning_project.csv')","ddfae354":"# Check the head of Data frame\n\nVehicle_df.head()","1a2004a7":"# Check the tail of this Data frame\n\nVehicle_df.tail()","f36d6656":"# Let us check the shape of the Data frame\n\nprint(f'Shape of the Dataframe is:- {Vehicle_df.shape}')\n\nTotal_rows = Vehicle_df.shape[0]\nTotal_columns = Vehicle_df.shape[1]\nprint(\"\")\nprint(f'Total Number of rows in Data set are = {Total_rows}')\nprint(\"\")\nprint(f'Total Number of Columns in Data set are = {Total_columns}')","0526e575":"# Now check the type of attributes we have in data set\n\nVehicle_df.info()\nVehicle_df.dtypes","8b6a0b21":"# Check the presence of any missing values in this data set\n\nVehicle_df.isna().sum()","90b473fb":"Vehicle_df.isnull().sum()","ab285b19":"# Let us check the distribution of data using 5 point summary\n\nVehicle_df.describe().round(2).T","eb35dafb":"# Check the Value counts of Categorical attribut \"Class\" in this data set\n\nprint(f'''=========================================\n**Summary of Class Attribute**\n=========================================\n{Vehicle_df['class'].value_counts()}''')","f849772d":"nullValues = pd.DataFrame(Vehicle_df['circularity'].isnull())\n\nVehicle_df[nullValues['circularity']==True] # Displays the missing value in one of the attribute","5d2fa87d":"# Instead of dropping the missing value rows from data set we will replace these null values with median nummber of each attribute.\n\nVehicle_df.median()","aa1f24d1":"# replace the missing values with median value.\n# Note, we do not need to specify the column names below\n# every column's missing value is replaced with that column's median respectively  (axis =0 means columnwise)\n\nmedianFiller = lambda x: x.fillna(x.median())\n\nVehicle_df_new = Vehicle_df.drop(['class'], axis=1) # We have to drop the categorical column from data frame to use median value in other columns","7ae8d66e":"Vehicle_df_new =Vehicle_df_new.apply(medianFiller, axis=1) # Missing values are treated with median number now","2b537fbf":"class_index = Vehicle_df['class'] # Let us again add class attribute to data frame by reindexing it.\n\nVehicle_df_new['class'] =class_index\n\nVehicle_df_new.head()","c08a371e":"# Recheck if missing values are treated and not present in new Data frame.\n\nVehicle_df_new.isnull().sum()","1cb52eb4":"# Check in 5 point summary distribution as well\n\nVehicle_df_new.describe().round().T","d46de175":"# We will check co-relation among all attributes to understand the relationship they have.\n\ncorelation = plt.cm.viridis   # Color range used in heat map\nplt.figure(figsize=(30,15))\nplt.title('Corelation Between Features', y=1.02, size=20);\nsns.heatmap(data=Vehicle_df_new.corr().round(2), linewidths=0.2, vmax=1, square=True, annot=True, cmap=corelation, linecolor='white');\n","0baaae39":"sns.pairplot(Vehicle_df_new, diag_kind='kde')","919c1f37":"# Explore the data distribution for \"Class\" column against other independent attributes using box plot\n\nplt.figure(figsize=(20,6))\nplt.subplot(1,3,1)\nplt.title('Compactness Vs Class')\nsns.boxplot(Vehicle_df_new['compactness'], Vehicle_df_new['class'], palette='Greens')\n\n# Subplot 2\nplt.subplot(1,3,2)\nplt.title('Circularity Vs Class')\nsns.boxplot(Vehicle_df_new['circularity'], Vehicle_df_new['class'], palette='Reds')\n\n# Subplot 3\nplt.subplot(1,3,3)\nplt.title('Dist Circularity Vs Class')\nsns.boxplot(Vehicle_df_new['distance_circularity'], Vehicle_df_new['class'], palette='Greys')\n\n# Subplot 4\nplt.figure(figsize=(23,6))\nplt.subplot(1,4,1)\nplt.title('Radious Ratio Vs Class')\nsns.boxplot(Vehicle_df_new['radius_ratio'], Vehicle_df_new['class'], palette='Greens')\n\n# Subplot 5\nplt.subplot(1,4,2)\nplt.title('pr.axis_aspect_ratio Vs Class')\nsns.boxplot(Vehicle_df_new['pr.axis_aspect_ratio'], Vehicle_df_new['class'], palette='Reds')\n\n# Subplot 6\nplt.subplot(1,4,3)\nplt.title('max.length_aspect_ratio Vs Class')\nsns.boxplot(Vehicle_df_new['max.length_aspect_ratio'], Vehicle_df_new['class'], palette='Greys')","e03eaa23":"# Explore the data distribution for \"Class\" column against other independent attributes using box plot\n\nplt.figure(figsize=(20,6))\nplt.subplot(1,3,1)\nplt.title('Scatter Ratio Vs Class')\nsns.boxplot(Vehicle_df_new['scatter_ratio'], Vehicle_df_new['class'], palette='Greens')\n\n# Subplot 2\nplt.subplot(1,3,2)\nplt.title('Elongatedness Vs Class')\nsns.boxplot(Vehicle_df_new['elongatedness'], Vehicle_df_new['class'], palette='Reds')\n\n# Subplot 3\nplt.subplot(1,3,3)\nplt.title('pr.axis_rectangularity Vs Class')\nsns.boxplot(Vehicle_df_new['pr.axis_rectangularity'], Vehicle_df_new['class'], palette='Greys')\n\n# Subplot 4\nplt.figure(figsize=(23,6))\nplt.subplot(1,4,1)\nplt.title('Max length rectangularity Vs Class')\nsns.boxplot(Vehicle_df_new['max.length_rectangularity'], Vehicle_df_new['class'], palette='Greens')\n\n# Subplot 5\nplt.subplot(1,4,2)\nplt.title('Scaled Variance Vs Class')\nsns.boxplot(Vehicle_df_new['scaled_variance'], Vehicle_df_new['class'], palette='Reds')\n\n# Subplot 6\nplt.subplot(1,4,3)\nplt.title('Scaled Variance 1 Vs Class')\nsns.boxplot(Vehicle_df_new['scaled_variance.1'], Vehicle_df_new['class'], palette='Greys')","6b817c46":"# Explore the data distribution for \"Class\" column against other independent attributes using box plot\n\nplt.figure(figsize=(20,6))\nplt.subplot(1,3,1)\nplt.title('Scaled Radius of Gyration Vs Class')\nsns.boxplot(Vehicle_df_new['scaled_radius_of_gyration'], Vehicle_df_new['class'], palette='Greens')\n\n# Subplot 2\nplt.subplot(1,3,2)\nplt.title('Scaled Radius of Gyration 1 Vs Class')\nsns.boxplot(Vehicle_df_new['scaled_radius_of_gyration.1'], Vehicle_df_new['class'], palette='Reds')\n\n# Subplot 3\nplt.subplot(1,3,3)\nplt.title('Skewness About Vs Class')\nsns.boxplot(Vehicle_df_new['skewness_about'], Vehicle_df_new['class'], palette='Greys')\n\n# Subplot 4\nplt.figure(figsize=(23,6))\nplt.subplot(1,4,1)\nplt.title('Skewness About 1 Vs Class')\nsns.boxplot(Vehicle_df_new['skewness_about.1'], Vehicle_df_new['class'], palette='Greens')\n\n# Subplot 5\nplt.subplot(1,4,2)\nplt.title('Skewness About 2 Vs Class')\nsns.boxplot(Vehicle_df_new['skewness_about.2'], Vehicle_df_new['class'], palette='Reds')\n\n# Subplot 6\nplt.subplot(1,4,3)\nplt.title('Hollows Ratio 1 Vs Class')\nsns.boxplot(Vehicle_df_new['hollows_ratio'], Vehicle_df_new['class'], palette='Greys')","dd814042":"# Check the data distribution of each column using histogram\nVehicle_df_new.hist(figsize=(20,20));","050e2525":"Vehicle_df_new.corr().round(2).T","0dc9338c":"Vehicle_df_new.head()","117b3d40":"# There are all numeric attributes excluding \"class\"\n\nX = Vehicle_df_new.drop(['class'], axis=1) # Independent attribute selection from Data frame\n\ny = Vehicle_df_new['class']  # Dependent variable from Data frame, though there is no such statement but we are considering it\n\ny_scaled = pd.get_dummies(y, drop_first=True)  # As this attribute is categorical in nature hence conversion in to numeric form is required","0778f760":"# Let us now standardised the unit length across attributes & hence we will calculate zscore for all numeric observations.\n\nfrom scipy.stats import zscore\nX_scaled = X.apply(zscore)","849e29fd":"X_scaled.head().round(2) # Top 5 reading of scaled data frame","1168366e":"# Import train-test model for spliting data into 70:30 ratio\n\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.30, random_state=101)","909a00a4":"print(\"{0:0.2f}% Data in training Set\".format(len(X_train)\/len(X.index)*100))\nprint(\"\")\nprint(\"{0:0.2f}% Data in testing Set\".format(len(X_test)\/len(X.index)*100))","ebb6dffc":"# Distribution of Class attributes\n\nprint(\"Distribution of Cars in Original Data :{0}({1:0.2f}%)\".format(len(Vehicle_df_new.loc[Vehicle_df_new['class']=='car']), (len(Vehicle_df_new.loc[Vehicle_df_new['class']=='car'])\/len(Vehicle_df_new.index))*100))\nprint(\"Distribution of Buses in Original Data   :{0}({1:0.2f}%)\".format(len(Vehicle_df_new.loc[Vehicle_df_new['class']=='bus']), (len(Vehicle_df_new.loc[Vehicle_df_new['class']=='bus'])\/len(Vehicle_df_new.index))*100))\nprint(\"Distribution of Vans in Original Data   :{0}({1:0.2f}%)\".format(len(Vehicle_df_new.loc[Vehicle_df_new['class']=='van']), (len(Vehicle_df_new.loc[Vehicle_df_new['class']=='van'])\/len(Vehicle_df_new.index))*100))","0c0a5097":"# Import SVM library for Model building\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nfrom sklearn.metrics import roc_auc_score, roc_curve, recall_score, precision_score\n\nsvc =SVC(random_state=101)\nsvc.fit(X_train, y_train) # SVM Model Training\n\nprint(\"Accuracy on training set: {:.4f}\".format(svc.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.4f}\".format(svc.score(X_test, y_test)))","e86482de":"y_pred_svm = svc.predict(X_test) # Support vector machine model is ready for Predictions \n\nprint (f'Accuracy of SVM Model is ={round(accuracy_score(y_test, y_pred_svm),4)*100}%')","31ffb4c8":"print(classification_report(y_test, y_pred_svm)) # Classification Report of SVM model.","23917069":"y_train_pred = svc.predict(X_train) # Prediction of Training Data","66056f5b":"cm_svm = plt.cm.Reds_r # Color Scheme for confusion metrics\ncm1 = confusion_matrix(y_train,y_train_pred, labels=['car','bus','van']) # Confusion metrix of SVM Model on Trainig Data\nplt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nplt.title(\"CM of Training Data\")\ncm1_df = pd.DataFrame(cm1, columns=[i for i in [\"Actual Car\", \"Actual Bus\", \"Actual Van\"]], index=[i for i in [\"Predict Car\",\"Predict Bus\", \"Predict Van\"]])\nsns.heatmap(data=cm1_df, annot=True, fmt='.5g', cmap=cm_svm)\n\ncm_svm1 = plt.cm.Blues_r # Color Scheme for confusion metrics\ncm2 = confusion_matrix(y_test,y_pred_svm, labels=['car','bus','van']) # Confusion metrix of SVM Model on Testing Data\nplt.subplot(1,3,2)\nplt.title(\"CM of Test Data\")\ncm2_df = pd.DataFrame(cm2, columns=[i for i in [\"Actual Car\", \"Actual Bus\", \"Actual Van\"]], index=[i for i in [\"Predict Car\",\"Predict Bus\", \"Predict Van\"]])\nsns.heatmap(data=cm2_df, annot=True, fmt='.5g', cmap=cm_svm1)","440afbab":"# Let us first check the performance of few models using Train Test Split (70:30 Ratio)\n\nfrom sklearn.linear_model import LogisticRegression  # Import logistics model from sklearn lib\nfrom sklearn.ensemble import RandomForestClassifier  # import Random forest classifier from sklearn lib","81fdcc7f":"# Logistic Regreesion Model\n\nlr =LogisticRegression()\nlr.fit(X_train,y_train)\nprint(\"Accuracy of Logistic Model is {0:0.2f}%\".format (round(lr.score(X_test,y_test)*100,3)))","cc8fbf89":"# Support Vector Machine Model\n\nsvc =SVC(random_state=101)\nsvc.fit(X_train,y_train)\nprint(\"Accuracy of SVM Model is {0:0.2f}%\".format (round(svc.score(X_test,y_test)*100,3)))","26cd372a":"# Random Forest Model\n\nrf =RandomForestClassifier(n_estimators=40, random_state=101)\nrf.fit(X_train,y_train)\nprint(\"Accuracy of Randome Forest Model is {0:0.2f}%\".format (round(rf.score(X_test,y_test)*100,3)))","b99e21f3":"# Import neccessary Package for K-fold Cross validation\n# Cross validation technique uses different set of data in each fold & thus gives holistic representation of model performance.  \n\nfrom sklearn.model_selection import cross_val_score","8b13f155":"K_Fold_lr = cross_val_score(LogisticRegression(), X,y).round(3) # K-fold cross validation using Logistics Regression\n\nK_Fold_lr","be79157a":"# SVM with default C & gamma values\n\nK_Fold_SVM = cross_val_score(SVC(random_state=101), X,y).round(3) # K-fold cross validation using Support Vector Machine\n\nK_Fold_SVM","01e640c3":"# SVM with optimized Gamma=0.0001 & C=100 Values\n\nK_Fold_SVM1 = cross_val_score(SVC(gamma=0.0001, C=100, random_state=101), X ,y).round(3) # K-fold cross validation using Support Vector Machine\n\nK_Fold_SVM1","cd945914":"# Random Forest Model with default parameter Values\n\nK_Fold_rf = cross_val_score(RandomForestClassifier(), X,y).round(3)\n\nK_Fold_rf","1bf48bb2":"# Random Forest Model with optimized parameter Values\n\nK_Fold_rf1 = cross_val_score(RandomForestClassifier(n_estimators=26, random_state=101), X,y).round(3)\n\nK_Fold_rf1","57ca59a0":"print('Avg. K-Fold Score of Logistic Regression {0:0.2f}%'.format(K_Fold_lr.mean()*100))\nprint(\"\")\nprint('Avg. K-Fold Score of SVM {0:0.2f}%'.format(K_Fold_SVM1.mean()*100))\nprint(\"\")\nprint('Min K-Fold Score of SVM {0:0.2f}%'.format(K_Fold_SVM1.min()*100))\nprint(\"\")\nprint('Max K-Fold Score of SVM {0:0.2f}%'.format(K_Fold_SVM1.max()*100))\nprint(\"\")\nprint('Avg. K-Fold Score of Random Forest {0:0.2f}%'.format(K_Fold_rf.mean()*100))","8975ae66":"# Import neccessary libraries to use PCA\n\nfrom sklearn.decomposition import PCA","b4c08801":"# Create Covariance matrix of Scaled Data now\n\ncovMatrix = np.cov(X_scaled, rowvar=False).round(2)\n\nprint(f'''Covariance Matrix of Scaled Data is Below\n\n{covMatrix}''')","a84dbdab":"#generating the eigen values and the eigen vectors\n\ne_value, e_vectors = np.linalg.eig(covMatrix)\n\nprint(f'''Eigen Values of given matrix are\n==========================================\n{e_value.round(2)}''')\n\nprint(\"==========================================\")\n\nprint(f'''Eigen Vectors of given matrix are\n==========================================\n{e_vectors.round(2)}''')","dcdc0139":"# Build PCA with default variables here 18\npca = PCA(n_components=18, random_state=101)\npca.fit(X_scaled)","bed9a7a7":"# Eigen Values using PCA method\n\nprint(pca.explained_variance_)","3a81f24c":"# Eigen Vectors using PCA method\n\nprint(pca.components_)","e44bcfb4":"# Let us check the percentage of variance explained by each eigen value\n\nprint(pca.explained_variance_ratio_)","a9c8641e":"# Let us check the cumulative explained variance of the Data\n\nprint(f'''Cumulative Explained Variance :\n==================================================================\n{np.cumsum(pca.explained_variance_ratio_)*100}''')","3e4cb7e9":"plt.figure(figsize=(20,10))\nplt.bar(list(range(1,19)), pca.explained_variance_ratio_ , alpha=0.8, align='center', color='Green', label = 'Individual explained variance')\nplt.step(list(range(1,19)), np.cumsum(pca.explained_variance_ratio_), where='mid', color='Red', label = 'Cumulative explained variance')\nplt.ylabel('Variation Explained', y=0.5, size=15)\nplt.xlabel('Eigen Value', size=15)\nplt.title('PCA plot of Variance & Eigen Values',y=1.02, size=20 )\nplt.legend(loc='best')\nplt.show()","b53f2448":"# let us reduce the dimesions to 9\n\npca_9 = PCA(n_components=9, random_state=101)\npca_9.fit(X_scaled)","73dc69e0":"# Eigen Values after dimension reduction\n\nprint(pca_9.explained_variance_)","a1883906":"# Eigen Vector after dimension reduction\nprint(pca_9.components_)","7c372251":"# Let us create the new training Data after dimensionality reduction\n\nX_pca = pca_9.transform(X_scaled) \n\nX_pca = pd.DataFrame(X_pca)","e23716e6":"# Check the distribution of new data after dimension reduction\n\nsns.pairplot(X_pca, diag_kind='kde')","b4d6e321":"# Let us split the PCA data in to training & testing in 70:30 Ratio\n\nX_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.3, random_state=101) ","0915f100":"X_train_pca.head() # Showing head of the new data frame having 9 variables after dimension reduction","987a727f":"y_train_pca.head()","c78d32e5":"print(X_train_pca.shape) # Data after dimensionality reduction\nprint(X_train.shape)     # Original Data","53beea3f":"# Let us now check the performance of support vector machine after PCA\n\nsvc_pca = SVC(gamma=0.01, C=100,random_state=101)\nsvc_pca.fit(X_train_pca, y_train) # SVM model is trained using training Data","42b97903":"y_pred_svm_pca = svc_pca.predict(X_test_pca) # SVM model is ready for predictions.","b8bec834":"print(\"SVM_PCA Accuracy on training set: {:.4f}\".format(svc_pca.score(X_train_pca, y_train_pca)))\nprint(\"SVM_PCA Accuracy on test set: {:.4f}\".format(svc_pca.score(X_test_pca, y_test_pca)))","21c5b0e5":"print (f'Accuracy of SVM Model is ={round(accuracy_score(y_test_pca, y_pred_svm_pca),4)*100}%')","2ebbb1af":"print(classification_report(y_test_pca,y_pred_svm_pca))","d6935a70":"y_train_pred_pca = svc_pca.predict(X_train_pca) # Prediction of training Data by SVM model","a1b926fd":"cm_svm_pca = plt.cm.Greys_r # Color Scheme for confusion metrics\ncm3 = confusion_matrix(y_train_pca,y_train_pred_pca, labels=['car','bus','van']) # Confusion metrix of SVM Model on Trainig Data\nplt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nplt.title(\"CM of Training Data\")\ncm3_df = pd.DataFrame(cm3, columns=[i for i in [\"Actual Car\", \"Actual Bus\", \"Actual Van\"]], index=[i for i in [\"Predict Car\",\"Predict Bus\", \"Predict Van\"]])\nsns.heatmap(data=cm3_df, annot=True, fmt='.5g', cmap=cm_svm_pca)\n\ncm_svm1_pca = plt.cm.Greens_r # Color Scheme for confusion metrics\ncm4 = confusion_matrix(y_test_pca,y_pred_svm_pca, labels=['car','bus','van']) # Confusion metrix of SVM Model on Testing Data\nplt.subplot(1,3,2)\nplt.title(\"CM of Test Data\")\ncm4_df = pd.DataFrame(cm4, columns=[i for i in [\"Actual Car\", \"Actual Bus\", \"Actual Van\"]], index=[i for i in [\"Predict Car\",\"Predict Bus\", \"Predict Van\"]])\nsns.heatmap(data=cm4_df, annot=True, fmt='.5g', cmap=cm_svm1_pca)","d4fba131":"# SVM with default C & gamma values using K-fold cross validation method\n\nK_Fold_SVM_pca = cross_val_score(SVC(random_state=101), X_pca,y).round(3) # K-fold cross validation using Support Vector Machine\n\nK_Fold_SVM_pca","eec6ce65":"# SVM with optimized C & gamma values using K-fold cross validation method\n\nK_Fold_SVM_pca1 = cross_val_score(SVC(gamma=0.01, C=100,random_state=101), X_pca,y).round(3) # K-fold cross validation using Support Vector Machine\n\nK_Fold_SVM_pca1","a9717b03":"print(f'''Avg. K-Fold Cross Validation Score of SVM Model with Optimized parameter Values on PCA Data\n\n{round(K_Fold_SVM_pca1.mean(),4)*100}%\n==========================================================================================\nMin Cross Validation Score of SVm Model\n{round(K_Fold_SVM_pca1.min(),4)*100}%\n==========================================================================================\n{round(K_Fold_SVM_pca1.max(),4)*100}%''')","8e8bb4d6":"# Let us check the distribution of Data in Independent variables using box plot\n\nX.boxplot(figsize=(20,10))","3ec77795":"data = X  # Where X is Dataframe of independent attributes (Target column 'class' in dropped from this)\n       \ndef replace(group):\n    median, std = group.median(), group.std()  #Get the median and the standard deviation of every group \n    outliers = (group - median).abs() > 2*std # Subtract median from every member of each group. Take absolute values > 2std\n    group[outliers] = group.median()       \n    return group\n\nX_Outliers = (data.transform(replace)) # Independent variables after outlier replaced by median values.","f2f69612":"# Let us recheck the outliers presence in independent Parameters\n\nX_Outliers.boxplot(figsize=(20,10))","739969c6":"# Let us now standardised the unit length across attributes & hence we will calculate zscore for all numeric observations.\n\nfrom scipy.stats import zscore\n\nX_scaled_Outliers = X_Outliers.apply(zscore)\n\nX_scaled_Outliers.head()  # Scaled Dataframe after treating Outliers","c196b0c9":"# Let us split the new data in 70:30 ratio using train-test split\n\nX_Out_train, X_Out_test, y_Out_train, y_Out_test = train_test_split(X_scaled_Outliers, y, test_size=0.30, random_state=101)","57495124":"# Let us check SVM model performance on Data where outliers are treated by median value (X_scaled_outlier Data Frame)\n\nsvc_outlier =SVC(random_state=101)\nsvc_outlier.fit(X_Out_train, y_Out_train) # SVM Model Training on Outlier treated Data\n\nprint(\"Accuracy on training set: {:.4f}\".format(svc_outlier.score(X_Out_train, y_Out_train)*100))\nprint(\"Accuracy on test set: {:.4f}\".format(svc_outlier.score(X_Out_test, y_Out_test)*100))","421efba5":"y_pred_svm_Out = svc_outlier.predict(X_Out_test)\n\nprint(\"Accuracy of SVM on test Data is {:.3f}%\".format(round(accuracy_score(y_Out_test, y_pred_svm_Out),3)*100))","0f9156d7":"# Confusion Matrix\n\nprint(confusion_matrix(y_Out_test, y_pred_svm_Out))","1e8807e3":"# SVM Performance using K-Fold Cross validation Method on Outlier treated Data\n\nK_Fold_SVM_Out = cross_val_score(SVC(gamma=0.1, C=10, random_state=101), X_scaled_Outliers ,y).round(3) # K-fold cross validation using Support Vector Machine\n\nK_Fold_SVM_Out","a51e4bc7":"print('Avg. K-Fold Score of SVM after removing Outliers {0:0.2f}%'.format(K_Fold_SVM_Out.mean()*100))\nprint(\"\")\nprint('Max K-Fold Score of SVM  after removing Outliers {0:0.2f}%'.format(K_Fold_SVM_Out.max()*100))","9cd637e6":"# Build PCA with default variables here 18\n\npca_Out = PCA(n_components=18, random_state=101)\npca_Out.fit(X_scaled_Outliers)","a2d065fb":"# Eigen Values using PCA method\n\nprint(pca_Out.explained_variance_)","2a2413b2":"# Eigen Vectors using PCA method\n\nprint(pca_Out.components_)","635033ad":"# Let us check the cumulative explained variance of the Data\n\nprint(f'''Cumulative Explained Variance :\n==================================================================\n{np.cumsum(pca_Out.explained_variance_ratio_)*100}''')","b671c636":"# let us reduce the dimesions to 9 (on Outliers removed Data)\n\npca_Out_9 = PCA(n_components=9, random_state=101)\npca_Out_9.fit(X_scaled_Outliers)","989b7372":"# Let us create the new training Data after dimensionality reduction\n\nX_pca_Out = pca_Out_9.transform(X_scaled_Outliers) \n\nX_pca_Out = pd.DataFrame(X_pca_Out)","36b90406":"# Let us split the Outlier removed PCA data in to training & testing in 70:30 Ratio\n\nX_train_pca_Out, X_test_pca_Out, y_train_pca_Out, y_test_pca_Out = train_test_split(X_pca_Out, y, test_size=0.3, random_state=101)","68531a2e":"# Let us now check the performance of support vector machine after PCA (dimensionality Reduction) using Train-Test Split\n\nsvc_outlier_pca = SVC(gamma=0.2, C=10,random_state=101)\nsvc_outlier_pca.fit(X_train_pca_Out, y_train_pca_Out) # SVM model is trained using training Data","7de36fd6":"print(\"SVM_PCA Accuracy on training set after Dimension & Outlier Removal: {:.4f}\".format(svc_outlier_pca.score(X_train_pca_Out, y_train_pca_Out)*100))\nprint(\"SVM_PCA Accuracy on test set after Dimension & Outlier Removal: {:.4f}\".format(svc_outlier_pca.score(X_test_pca_Out, y_test_pca_Out)*100))","b2f037bc":"y_pred_svm_pca_Out = svc_outlier_pca.predict(X_test_pca_Out) # Model is ready to predict the outcome now","d833f1bb":"# Now lets check the performance of SVM using K-fold cross validation method on outlier removed Data\n\nK_Fold_SVM_pca_Out = cross_val_score(SVC(gamma=0.2, C=10,random_state=101), X_pca_Out,y).round(3) # K-fold cross validation using Support Vector Machine\n\nK_Fold_SVM_pca_Out","9369329b":"# Import neccessary Libraries to compare the performance of the models\n\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score","4a59d668":"# We will create pandas data frame for each model for performance evaluation.\n\nPerformance_df1 = pd.DataFrame({'Method':['Train-Test Split'],'Model Name':['SVM on Raw Data'], 'Testing Data Accuracy':[accuracy_score(y_test,y_pred_svm)],\n                              'Recall (Min)':[recall_score(y_test,y_pred_svm, average=None).mean()], 'Precision (Max)':[precision_score(y_test,y_pred_svm, average=None).mean()],\n                              'F1-Score':[f1_score(y_test,y_pred_svm, average=None).mean()]})\n\nPerformance_df2 = pd.DataFrame({'Method':['Train-Test Split'],'Model Name':['SVM after PCA'], 'Testing Data Accuracy':[accuracy_score(y_test_pca,y_pred_svm_pca)],\n                              'Recall (Min)':[recall_score(y_test_pca,y_pred_svm_pca, average=None).mean()], 'Precision (Max)':[precision_score(y_test_pca,y_pred_svm_pca, average=None).mean()],\n                              'F1-Score':[f1_score(y_test_pca,y_pred_svm_pca, average=None).mean()]})\n\nPerformance_df3 = pd.DataFrame({'Method':['Train-Test Split'],'Model Name':['SVM After Outlier Removal'], 'Testing Data Accuracy':[accuracy_score(y_Out_test,y_pred_svm_Out)],\n                              'Recall (Min)':[recall_score(y_Out_test,y_pred_svm_Out, average=None).mean()], 'Precision (Max)':[precision_score(y_Out_test,y_pred_svm_Out, average=None).mean()],\n                              'F1-Score':[f1_score(y_Out_test,y_pred_svm_Out, average=None).mean()]})\n\n\nPerformance_df4 = pd.DataFrame({'Method':['Train-Test Split'],'Model Name':['SVM after PCA & Outlier Removal'], 'Testing Data Accuracy':[accuracy_score(y_test_pca_Out,y_pred_svm_pca_Out)],\n                              'Recall (Min)':[recall_score(y_test_pca_Out,y_pred_svm_pca_Out, average=None).mean()], 'Precision (Max)':[precision_score(y_test_pca_Out,y_pred_svm_pca_Out, average=None).mean()],\n                              'F1-Score':[f1_score(y_test_pca_Out,y_pred_svm_pca_Out, average=None).mean()]})\n\n                                \nPerformance_df5 = pd.DataFrame({'Method':['K-Fold Cross Validation'],'Model Name':['SVM on Raw Data'], 'Testing Data Accuracy':[K_Fold_SVM1.mean()],\n                              'Recall (Min)':[K_Fold_SVM1.min()], 'Precision (Max)':[K_Fold_SVM1.max()],'F1-Score':['-']})                                \n                                                 \n    \nPerformance_df6 = pd.DataFrame({'Method':['K-Fold Cross Validation'],'Model Name':['SVM after PCA'], 'Testing Data Accuracy':[K_Fold_SVM_pca1.mean()],\n                              'Recall (Min)':[K_Fold_SVM_pca1.min()], 'Precision (Max)':[K_Fold_SVM_pca1.max()],'F1-Score':['-']})\n\nPerformance_df7 = pd.DataFrame({'Method':['K-Fold Cross Validation'],'Model Name':['SVM After Outlier Removal'], 'Testing Data Accuracy':[K_Fold_SVM_Out.mean()],\n                              'Recall (Min)':[K_Fold_SVM_Out.min()], 'Precision (Max)':[K_Fold_SVM_Out.max()],'F1-Score':['-']})  \n\nPerformance_df8 = pd.DataFrame({'Method':['K-Fold Cross Validation'],'Model Name':['SVM after PCA & Outlier Removal'], 'Testing Data Accuracy':[K_Fold_SVM_pca_Out.mean()],\n                              'Recall (Min)':[K_Fold_SVM_pca_Out.min()], 'Precision (Max)':[K_Fold_SVM_pca_Out.max()],'F1-Score':['-']})","0dda3c3d":"# Let us concat the above data frames for final comparision of Model performance\n\nConcat_df = pd.concat([Performance_df1,Performance_df2, Performance_df3, Performance_df4, \n                       Performance_df5, Performance_df6, Performance_df7, Performance_df8])\n\nConcat_df.set_index(keys=[['1', '2', '3', '4', '5', '6', '7', '8']]).round(3)","d691ea26":"# Import neccessary package to perform the operation\nfrom sklearn.model_selection import GridSearchCV\n\ngs = GridSearchCV(svc_outlier_pca, cv=10, param_grid={'kernel': ('linear','poly','rbf','sigmoid'),\n             'gamma': ('auto', 'scale'), 'C':[1,10,20,30,40,50,60,70,80,90,100]})","b9d08e77":"gs.fit(X_train_pca_Out,y_train_pca_Out) # Griedsearch is trained using training Data to identify the best set of Parameters for SVM Model","0c3184f7":"gs.best_params_  # This command will give us best set of parameters to optimize the model performance","4deb64ca":"gs.cv_results_['params'] # Will print list of all parameters in the model","b6a80d5f":"gs.cv_results_['mean_test_score'] # This operation will give the result of SVM Model for each set of parameters","c0906172":"* From the above corelation matrix it is visible that many attributes are strongly corelated with each other. \n\n\n* Positive corelation >90% is observed between\n\n    1. distance_circularity & scatter_ratio\n    2. scatter_ratio & scaled_variance\n    3. scatter_ratio & scaled_variance.1\n    4. scaled_variance & scaled_variance.1\n    \n\n* Negative corelation >90% is observed between\n\n    1. elongatedness & scatter_ratio\n    2. elongatedness & scaled_variance\n    3. elongatedness & scaled_variance.1\n  \n  \n* All other attributes are having corelation in range of 40% to 80%.\n\n\n* This strong corelation among features will lead to multicolinearity and may affect the performance of Model.","d610e1cd":"* Data is having relationship among attributes, data distribution is showing more than two peaks for many features indicating towards more than 2 clusters of similar type can be formed.","e07cbda0":"<font color = 'blue'>**Performance of SVM model is improved by good extent after handling outliers from the Original Data, We will now finally compare the performance of various models to conclude the project** ","7f90f48c":"# <font color='blue'>5. Split & Build Model using Final Scaled Data (After Dimension Reduction)","4d2acaa6":"<font color = 'blue'>**From the above principal component analysis it is very clear that 9 attributes out of 18 are carrying 95.15% information, and rest 9 feature have only 4.95% information which is actually a noise. Hence based on these statistics we will now implement dimensionality reduction using PCA on given Data set.**","3b816c8f":"* <font color ='green'> **SVM model is performing better after optimizing the parameter values (gamma & C) with minimum score of 93.5% & average score of 95.4%. Random state is kept common (101) in order have similar data while checking performance of the model.**","ebb2eed2":"# GriedSearch Technique to find best Parameters of the Model (SVM)","163e1c45":"* Above pair plots are much cleaned now in terms of corelation between attributes, all nine features are independent of each other and carrying 95.15% of information from original data, that means 4.95% data in 9 other variables was noise and impacting performance of the model.","ca88c541":"* <font color='green'>**When we remove outliers and replace them with median or mean, the distribution shape changes, the standard deviation becomes tighter & thus creates new outliers. But The new outliers would be much closer to the centre than original outliers so we accept them without modifying them.**","dd000c1d":"* Avg. compactness, distance circularity & radious ratio of car is higher than bus & van\n\n* Buses are having high outliers as compared to car & van","eaeecae6":"<font color='blue'>**Support Vector Machine model is performing better with accuracy of 93.70% than other models using train-test split method, however train-test split is not always a best way for model performance due to fixed training set & testing set, hence let us try much better technique of K-fold cross validation.**","00d4e1bd":"**<font color = 'red'> Outliers are clearly visible in many independent variables (indicated by the black circles) & Spread of data on each dimension (indicated by the whiskers is long ... due to the outliers)If the outliers are addressed, the data will be much cleaned. Let us handle these outliers with medina values instead of Mean as some of them of skewed in nature and mean might not be the good fit due presence of some extreme values in data set for each indivisual attribute.**","a546aeb9":"* Above five attributes are strongly co-related with each other & may mislead the mode performance. We have to handle it throguh either column elimination or feature extraction techniques.\n\n\n* These attributes can be combined as one & rest other will be used as it is as of now.","0cbc68f8":"## <font color ='red'>3.1 Bivariate Analysis\n\n* To plot multiple pairwise bivariate distribution in dataset, we will use pair plot function from seaborn library, this function create a matrix of axes and shows relationship for each pair of column in Dataframe, it also draws the unvariate distribution of each attribute in on the digonal axis.\n\n\n* This distribution is valid for numeric attributes only","beecb5f1":"<font color='red'> **Inference from Support Vector Machine Model**  <font color='black'>\n\n1. Model is giving very good accuracy of 93.7% for the testing Data, however accuracy of training data is little higher & indicating towards overfitting problem.\n\n\n2. Good Recall score of 90% (bus), 94% (Car) & 98% (van) for all class.\n\n\n3. Good precision score of 98% (bus), 97% (Car) & 84% (van) for all class.\n\n\n4. Cars are having balanced recall & precision scores as compared to buses & vans because it is a majority class (50% observation) in given data set.\n\n\n5. F1-Scores are also good & noticable.","209d5bd4":"## <font color = 'red'> 4.4 Dimensionality Reduction with PCA\n    \n    \n   Now 9 dimensions seems very reasonable, with 9 principal components (features) we can explain over 95% of the variation in the original data!","599bf3b5":"## <font color ='red'>4.2 K-Fold Cross Validation","ed2d3c32":"# PCA on Outlier removed Data","b2645fb4":"<font color='green'>**It is clear from above two operations that missing values are fixed now in given data set by replacing median values, data is cleaned now and ready for further analysis**","7c7f4c16":"<font color='red'>**So we have total 429 Cars, 218 Buses & 199 Vans in the data set.**","4344b767":"**From the above hyper parameter study using Griedsearch method for SVM model, we came to know that the optimized set of parameters for the model will be as below**\n\n1. kernel = rbf\n\n2. gamma = scaled\n\n3. C = 10","9629ed76":"<font color='red'> **Inference from Support Vector Machine Model after PCA**  <font color='black'>\n\n1. Model is giving very good accuracy of 93.7% for the testing Data, however accuracy of training data (97.64%) is on higher side, parameter tweaking may help to balance the performance . \n\n\n2. Good Recall score of 93% (bus), 94% (Car) & 95% (van) for all class.\n\n\n3. Good precision score of 98% (bus), 96% (Car) & 85% (van) for all class.\n\n\n4. Cars are having balanced recall & precision scores as compared to buses & vans because it is a majority class (50% observation) in given data set.\n\n\n5. F1-Scores are 95%, 95% & 89% for bus, car & van respectively.","9920386f":"### <font color = 'red'>Note:- Gridsearch to find appropriate hyperparametes was not covered under unsupervised learning Module but scoring of the project has 3 numbers for this step. it will be covered under 'Featurization Model Selection & Tunning' Module. However based on some research & self exercises i have tried to use it on SVM model. Please consider accordingly.","f9750a0e":"* <font color='red'>Above data shows that 95% of the information is carried by 9 attributes out of 18 attributes, let us build PCA using these 9 attributes & ignore the other variables.","8696ebde":"## <font color='red'>5.3 Compare the SVM model performance with & without PCA","c900e100":"* <font color ='blue'> So here we have missing values in many columns, which we have to handle via mean or median methods to clean the data before model building.","ce06bb06":" **Insights from 5 point summary**\n \n   1. Attributes are having very close values of mean & median and thus looks like almost gaussian distribution, however it is either right or left skewed in reality.\n   \n     \n   2. Value count is not equal for all attributes, indicating towards missing values in some features.\n   \n     \n   3. Attributes like \"**radius_ratio\", \"scatter_ratio\", \"scaled_variance.1**\" etc are having outliers in data set.\n   \n     \n   4. Attributes having outliers have high **standard variation** (e.g scaled_variance.1, scatter_ratio).","d7ea5399":"# <font color = 'indigo'> Outliers Handling & Model performance on that Data","27ca32a8":"## <font color ='red'> 4.1 Support Vector Machine (SVM)","94a7eb4d":"### Confusion Matrix (after PCA)","23e1211a":"## <font color='red'> 5.1 SVM Performance after PCA","ea44397f":"* Hollow ratio of bus is lower than car & van\n\n* Avg. scaled radius of gyration1 of Bus is higher than van & car\n\n* outliers are present in many attributes\n\n\n<font color ='red'> Though the detailed description of attributes are not available for detailed inference, we tried to find out some information based on above box plots.","f036a380":"### Inference: - \n\n1. K-fold cross validation technique has splited the given data in to five folds. This method gives the range of performance for model (e.g. min value, max value etc)\n\n\n2. These 5-folds have unique training & testing data sets to validate the performance of the Model.\n\n\n3. Avg. K-fold cross validation score of Support vector machine is better than other Models.","71981b1c":"**We have 50.71% cars, 25.77% Buses & 23.52% Vans in given Data under class attribute. Data is somewhat imbalanced and model could have tendency of biasness towards mojority class Car.**","06d1932f":"* This data frame is having 19 features (attributes) & 846 rows (Data Object) in it.","62f62996":"## <font color ='red'>5.2 K-Fold Cross Validation after PCA","ed2f016e":"* Confusion metrix is showing overall good prediction by SVM model, however few misclassifications are observed for all categories. Let us try impact on performance using K-fold cross validation method.","9c89c8d0":"# <font color = 'blue'>3. Exploratory Data Analysis (EDA) & Data Cleaning","2b7de44e":"* <font color = 'red'> eigen vectors helps us in identifying the important directions of given data, and dimension which doesn't have much magnitude could be ignored as these attributes\/dimensions are not carrying much information. ","88133c5b":"**Confusion metrix is showing overall good prediction by SVM model, however few misclassifications are observed for all categories. Let us try hands using K-fold cross validation method.**","e8980e2f":"## <font color ='red'>4.3 Principal Component Analysis (PCA)\n    \n\n    \nPCA is a method of reducing the dimensionality of data without loosing any important information. When we have lot of attributes with colinearity exist among them in data set, feature extraction\/dimension reduction techniques helps us in mitigating such scenarios without loosing important information from the data & PCA is one such method.","0ca2e2e1":"### Confusion Matrix (SVM)","8c76a886":"# <font color ='blue'>1. Import All Basic Libraries","6ebbdf5e":"# <font color='blue'>4. Model Building & Slicing (70:30 Ratio)","eebd15e4":"* Avg. scaled variance & scater ratio of car is higher than bus & van\n\n* Buses are having high outliers as compared to car & van\n\n* Avg. Elongatedness of van is higher than car & bus.","278eaf0b":"* <font color='red'> All the attributes are numeric float(14) & int(4) in nature as per problem statement excluding Class column which is categorical.","2e5c91e7":"## <font color='red'> Final Inference\n   \n   \n   \n   \n   1. From the above comparision of model performances it is clear that SVM is giving good results.\n   \n   \n   2. Testing Data aacuracy for Raw data & PCA are same at 93.7%, however precision & F1-Score are slightly better in case of PCA as compared to raw data.\n   \n   \n   3. But in case of K-fold cross validation method raw data is giving better validation score of 97.4% as compared to 95.4% score after PCA technique.\n  \n   \n   4. After removing outliers from the original data by replacing them \"Median\" values performance of SVM model is further improved to 94.5% in train-test split method with very good recall (94%), precision (94.5%) & F1-Score of 94.26%\n   \n   \n   5. Accuracy & other performance parameters further improved after PCA implementation on data where outliers were removed to 94.9% with better recall (94%), precision (95.4%) & F1-Score of 94.68%\n\n\n   6. But in case of K-fold cross validation method raw data where outliers were removed is giving better validation score of avg 98.1% as compared to 95.4% score after PCA technique.\n   \n   \n   7. One thing is very clear that even after reducing dimensionality from 18 to 9 attribute using PCA, model performance didn't degraded, which means PCA did a fairly good job in eliminating noise (colinearity) from orignal data.\n  \n     \n   8. Principal component analysis is convinient way for reducing unwanted data from original data in order to ensure better & optimized outcome. \n   \n   \n   9. Random State (101) is kept common throught the program to kept the same set of test data.\n   \n   \n   10. We know that there is little class imbalance in in target column (cars are having 50% share as compared to Van & Bus) but we didn't performed upsampling method due to good results in all classes & not much biasness observed towards \"Car\".\n   ","0a1e1941":"# <font color = 'blue'>2. Load the Source Data file\n    \n  * <font color ='black'>We will use pandas package to load the source CSV file here for further analysis.","f15e752c":"# Data Description:\n\nThe data contains features extracted from the silhouette of vehicles in different angles. Four \"Corgie\" model vehicles were used for the experiment: a double decker bus, Cheverolet van, Saab 9000 and an Opel Manta 400 cars. This particular combination of vehicles was chosen with the expectation that the bus, van and either one of the cars would be readily distinguishable, but it would be more difficult to distinguish between the cars.\n\n\n# Domain:\n\nObject recognition\n\n# Context:\n\nThe purpose is to classify a given silhouette as one of three types of vehicle, using a set of features extracted from the silhouette. The vehicle may be viewed from one of many different angles.\n\n# Attribute Information:\n\n   * All the features are geometric features extracted from the silhouette.\n   * All are numeric in nature.\n\n# Learning Outcomes:\n\n   * Exploratory Data Analysis\n   * Reduce number dimensions in the dataset with minimal information loss\n   * Train a model using Principle Components\n\n# Objective:\n\nApply dimensionality reduction technique \u2013 PCA and train a model using principle components instead of training the model using just the raw data."}}