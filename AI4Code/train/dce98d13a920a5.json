{"cell_type":{"c9c1425a":"code","9deefbc1":"code","2c02890e":"code","812b2acd":"code","cf68d930":"code","fbfad8f8":"code","ef480269":"code","ee2354b7":"code","3e30390d":"code","debf446b":"code","2d21c617":"code","a1b56e74":"code","67ac081d":"code","40e5a64a":"code","a7927b2d":"code","9bee9146":"code","93cabb6a":"code","af3c5fbb":"code","eabc940f":"code","6d849c53":"code","35a82219":"code","5dfb83fb":"code","1b48ac81":"code","f2398a15":"code","601694f7":"code","4c02f2f1":"code","66616d4b":"code","effb2af4":"code","675fec77":"markdown","1e306dda":"markdown","95b19f36":"markdown","fc77a44a":"markdown","a4ecaa46":"markdown","5ad2eba3":"markdown","3405ae8f":"markdown","757891a2":"markdown","c5e15a2c":"markdown","7f4a61cb":"markdown","f15f1a3a":"markdown","0b5e848d":"markdown","d7abf7ec":"markdown","f7abbc80":"markdown","891e5c11":"markdown","56c7b98c":"markdown","6b8b72a4":"markdown"},"source":{"c9c1425a":"import pandas as pd\nimport numpy as np\n#import codecs\nimport matplotlib.pyplot as plt","9deefbc1":"# Import Data\n\ninput_file = open(\"..\/input\/nlp-starter-test\/socialmedia_relevant_cols.csv\", \"r\",encoding='utf-8', errors='replace')\n\n# read_csv will turn CSV files into dataframes\nquestions = pd.read_csv(input_file)","2c02890e":"questions.head()","812b2acd":"questions.choose_one.value_counts()","cf68d930":"# to clean data\ndef normalise_text (text):\n    text = text.str.lower()\n    text = text.str.replace(r\"\\#\",\"\")\n    text = text.str.replace(r\"http\\S+\",\"URL\")\n    text = text.str.replace(r\"@\",\" \")\n    text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \")\n    text = text.str.replace(\"\\s{2,}\", \" \")\n    return text","fbfad8f8":"questions[\"text\"]=normalise_text(questions[\"text\"])\n#could save to another file","ef480269":"questions.head()","ee2354b7":"import spacy\nnlp = spacy.load(\"en_core_web_lg\")","3e30390d":"#apply the spaCy nlp pipeline\ndoc = questions[\"text\"].apply(nlp) \n\n#you could extract a lot more information once you pass the data through the nlp pipeline, such as POS tagging, recognising important entities, etc. ","debf446b":"max_sent_len=max(len(doc[i]) for i in range(0,len(doc)))\nprint(\"length of longest sentence: \", max_sent_len)\n#point to be noted this is the number of tokens in the sentence, NOT words\n\n\nvector_len=len(doc[0][0].vector)\nprint(\"length of each word vector: \", vector_len)","2d21c617":"#creating the 3D array\ntweet_matrix=np.zeros((len(doc),max_sent_len,vector_len))\nprint(tweet_matrix[0:2,0:3,0:4]) #test print","a1b56e74":"for i in range(0,len(doc)):\n    for j in range(0,len(doc[i])):\n        tweet_matrix[i][j]=doc[i][j].vector","67ac081d":"list_labels = np.array(questions[\"class_label\"])\nprint(list_labels.shape[0])\nprint(tweet_matrix.shape[0])","40e5a64a":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data import random_split","a7927b2d":"#if you need to convert numpy ndarray to tensor explicitely\n#tweet_matrix = torch.from_numpy(tweet_matrix)","9bee9146":"#for GPU - CUDA\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Assuming that we are on a CUDA machine, this should print a CUDA device:\n\nprint(device)","93cabb6a":"len_for_split=[int(tweet_matrix.shape[0]\/4),int(tweet_matrix.shape[0]*(3\/4))]\nprint(len_for_split)","af3c5fbb":"test, train=random_split(tweet_matrix,len_for_split)","eabc940f":"test.dataset.shape","6d849c53":"# Hyperparameters\nnum_epochs = 25\nnum_classes = 3\nlearning_rate = 0.001\nbatch_size=100","35a82219":"# to transform the data and labels\nclass MyDataset(Dataset):\n    def __init__(self, data, target, transform=None):\n        self.data = torch.from_numpy(data).float()\n        self.target = torch.from_numpy(target).long()\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        x = self.data[index]\n        y = self.target[index]\n        \n        if self.transform:\n            x = self.transform(x)\n        \n        return x, y\n    \n    def __len__(self):\n        return len(self.data)","5dfb83fb":"#load labels #truncating total data to keep batch size 100\nlabels_train=list_labels[train.indices[0:8100]]\nlabels_test=list_labels[test.indices[0:2700]]\n\n#load train data\ntraining_data=train.dataset[train.indices[0:8100]].astype(float)\n#training_data=training_data.unsqueeze(1)\n\n#load test data\ntest_data=test.dataset[test.indices[0:2700]].astype(float)\n#test_data=test_data.unsqueeze(1)\n\ndataset_train = MyDataset(training_data, labels_train)\ndataset_test = MyDataset(test_data, labels_test)\n\n\n#loading data batchwise\ntrain_loader = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n","1b48ac81":"## setting up the CNN network\n\n#arguments(input channel, output channel, kernel size, strides, padding)\n            \n            #layer 1x : \n            # height_out=(h_in-F_h)\/S+1=(72-x)\/1+1=73-x\n            # width_out=(w_in-F_w)\/S+1=(384-384)\/1+1=1\n            # no padding given\n            # height_out=(70-x)\/(70-x)=1 \n            # width_out=1\/1=1\n            \n\n\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.layer13 = nn.Sequential(\n            nn.Conv2d(1, 100, kernel_size=(3,vector_len), stride=1,padding=0), \n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(70,1), stride=1))\n        self.layer14 = nn.Sequential(\n            nn.Conv2d(1, 100, kernel_size=(4,vector_len), stride=1,padding=0), \n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(69,1), stride=1))\n        self.layer15 = nn.Sequential(\n            nn.Conv2d(1, 100, kernel_size=(5,vector_len), stride=1,padding=0), \n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(68,1), stride=1))\n        #self.layer2 = nn.Sequential(\n            #nn.Conv2d(15, 30, kernel_size=5, stride=1, padding=0),\n            #nn.ReLU(),\n            #nn.MaxPool2d(kernel_size=2, stride=2))\n        self.drop_out = nn.Dropout()\n        #concat operation\n        self.fc1 = nn.Linear(1 * 1 * 100 * 3, 30)\n        self.fc2 = nn.Linear(30, 3)\n        #self.fc3 = nn.Linear(100,3)\n        \n    def forward(self, x):\n        x3 = self.layer13(x)\n        x4 = self.layer14(x)\n        x5 = self.layer15(x)\n        x3 = x3.reshape(x3.size(0), -1)\n        x4 = x4.reshape(x4.size(0), -1)\n        x5 = x5.reshape(x5.size(0), -1)\n        x3 = self.drop_out(x3)\n        x4 = self.drop_out(x4)\n        x5 = self.drop_out(x5)\n        out = torch.cat((x3,x4,x5),1)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        return(out)","f2398a15":"#creating instance of our ConvNet class\nmodel = ConvNet()\nmodel.to(device) #CNN to GPU\n\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\n#CrossEntropyLoss function combines both a SoftMax activation and a cross entropy loss function in the same function\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","601694f7":"# Train the model\ntotal_step = 8100\/batch_size\n\nloss_list = []\nacc_list = []\nval_acc_list = []\n\nfor epoch in range(num_epochs):\n    loss_list_element = 0\n    acc_list_element = 0\n    for i, (data_t, labels) in enumerate(train_loader):\n        data_t=data_t.unsqueeze(1)\n        data_t, labels = data_t.to(device), labels.to(device)\n        \n        # Run the forward pass\n        outputs = model(data_t)\n        loss = criterion(outputs, labels)\n        loss_list_element += loss.item()\n        #print(\"==========forward pass finished==========\")\n            \n        # Backprop and perform Adam optimisation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        #print(\"==========backward pass finished==========\")\n        \n        # Track the accuracy\n        total = labels.size(0)\n        _, predicted = torch.max(outputs.data, 1)\n        correct = (predicted == labels).sum().item()\n        acc_list_element += correct\n        \n        \n    loss_list_element = loss_list_element\/np.shape(labels_train)[0]\n    acc_list_element = acc_list_element\/np.shape(labels_train)[0]\n    print('Epoch [{}\/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n          .format(epoch + 1, num_epochs, loss_list_element,acc_list_element * 100))  \n    loss_list.append(loss_list_element)\n    acc_list.append(acc_list_element)\n    \n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for data_t, labels in test_loader:\n            data_t=data_t.unsqueeze(1)\n            data_t, labels = data_t.to(device), labels.to(device)\n\n            outputs = model(data_t)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        val_acc_list.append(correct \/ total)\n\n    print('Test Accuracy of the model: {} %'.format((correct \/ total)*100))\n    print()\n        \n        ","4c02f2f1":"## evaluating model\n\n# Test the model\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for data_t, labels in test_loader:\n        data_t=data_t.unsqueeze(1)\n        data_t, labels = data_t.to(device), labels.to(device)\n        \n        outputs = model(data_t)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Test Accuracy of the model: {} %'.format((correct \/ total) * 100))\n\n","66616d4b":"import matplotlib.pyplot as plt","effb2af4":"plt.xlabel(\"runs\")\nplt.ylabel(\"normalised measure of loss\/accuracy\")\nx_len=list(range(len(acc_list)))\nplt.axis([0, max(x_len), 0, 1])\nplt.title('result of convNet')\nloss=np.asarray(loss_list)\/max(loss_list)\nplt.plot(x_len, loss, 'r.',label=\"loss\")\nplt.plot(x_len, acc_list, 'b.', label=\"accuracy\")\nplt.plot(x_len, val_acc_list, 'g.', label=\"val_accuracy\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show\n","675fec77":"### Training the model","1e306dda":"### Declare Hyperparameters","95b19f36":"### create labels","fc77a44a":"### Embedding using spaCy ","a4ecaa46":"### Train test split","5ad2eba3":"# Sentence Classification using CNN\n\n## The word vectors have been created using spaCy (english language model)\nThe sentence matrices are padded \n\n## The CNN used is quite simple \nIt is based on the [work](https:\/\/arxiv.org\/abs\/1408.5882) of Yoon Kim published in 2014\n*  Each of the word vectors are of k dimension\n*  the sentence length is n, (n=length of longest sentence)\n*  n x k matrix represents a sentence\n\n*  Filters of size 3xk, 4xk and 5xk are used along with maxpooling to extract features from the sentences. \n\n![](https:\/\/i.imgur.com\/oEMRsZY.png)\n* A fully connected dropout layer with softmax is used \n\n\n\n\n\n\n","3405ae8f":" ### Form 3d numpy array for storing word vectors, to be used for classifying\n \n * Each sentence is a matrix and these would be used as Tensors by the CNN\n \n * The dimensions of each matrix would be: **length of longest sentence** x **length of vector**\n \n * The dimension of the tensor :\n     * **number of tweets** x **length of longest sentence** x **length of vector**","757891a2":"Let us look at the cleaned text once","c5e15a2c":"### Next we set about normalising the tweets, so that the classification algorithm does not get confused with irrelevant information. ","7f4a61cb":"### Setting up the CNN model","f15f1a3a":"## **Data Pre Processing**\n*  Cleaning the data\n*  Building word vectors","0b5e848d":"## **CNN in Pytorch**","d7abf7ec":"#### The tweets classification are as follows:\n* **Not Relevant**(label==0) If the Tweet is not about Disasters.\n* **Relevant**(label==1) If the Tweet is about Disasters.\n* **Not Relevant**(label==2) If the Tweet is ambiguous. ","f7abbc80":"#### Let us get a glimpse at the data and check it out","891e5c11":"### Load Data\n\nThe dataset is loaded in batches with the Dataset class and Dataloader Module from torch.utils.data","56c7b98c":"### Plot a graph to trace model performance","6b8b72a4":"### Evaluating the model"}}