{"cell_type":{"4cc3bd53":"code","2c892b4a":"code","a393df89":"code","9da933d5":"code","98e4a18b":"code","70e50ce3":"code","dbabe128":"code","77e939b2":"code","368326b2":"code","f93426cf":"code","980c2117":"code","fb2132e4":"code","d9815b68":"code","fc6e1f6c":"code","d2620780":"code","cc671289":"code","0e80d6b1":"code","df355a49":"code","bd88c86d":"code","d10b70f5":"code","000da418":"code","81f8e296":"code","6a970b9c":"code","c17ac9a4":"code","82718fef":"code","bf54da43":"code","5950c8d6":"code","79a13d59":"code","57490ec6":"code","fbd5d1ae":"code","6930c3f6":"code","1af7de06":"code","96306bb4":"code","7aba856b":"code","edbcdc5c":"code","8eed497e":"markdown","4cbafbeb":"markdown","1e124c52":"markdown","9867bced":"markdown","cb6a6483":"markdown","178ea0dc":"markdown","101b3b58":"markdown","178133ee":"markdown"},"source":{"4cc3bd53":"#Import Dependencies\n\nimport numpy as np\nfrom numpy import nan\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense","2c892b4a":"data = pd.read_csv('..\/input\/power-consumption-of-house\/power_consumption_of_house.txt', sep = ';', parse_dates = True, low_memory = False)\n\ndata['date_time'] = data['Date'].str.cat(data['Time'], sep= ' ')\ndata.drop(['Date', 'Time'], inplace= True, axis = 1)\n\ndata.set_index(['date_time'], inplace=True)\ndata.replace('?', nan, inplace=True)\ndata = data.astype('float')\ndata.head()","a393df89":"#First check how many values are null\nnp.isnan(data).sum()\n\n#fill the null value\n\ndef fill_missing(data):\n    one_day = 24*60\n    for row in range(data.shape[0]):\n        for col in range(data.shape[1]):\n            if np.isnan(data[row, col]):\n                data[row, col] = data[row-one_day, col]\n\nfill_missing(data.values)\n\n#Again check the data after filling the value\nnp.isnan(data).sum()","9da933d5":"data.describe()\ndata.shape","98e4a18b":"data.head()","70e50ce3":"# Converting the index as date\ndata.index = pd.to_datetime(data.index)","dbabe128":"data = data.resample('D').sum()","77e939b2":"data.head()","368326b2":"\nfig, ax = plt.subplots(figsize=(18,18))\n\nfor i in range(len(data.columns)):\n    plt.subplot(len(data.columns), 1, i+1)\n    name = data.columns[i]\n    plt.plot(data[name])\n    plt.title(name, y=0, loc = 'right')\n    plt.yticks([])\nplt.show()\nfig.tight_layout()","f93426cf":"years = ['2007', '2008', '2009', '2010']\n\nfig, ax = plt.subplots(figsize=(18,18))\nfor i in range(len(years)):\n    plt.subplot(len(years), 1, i+1)\n    year = years[i]\n    active_power_data = data[str(year)]\n    plt.plot(active_power_data['Global_active_power'])\n    plt.title(str(year), y = 0, loc = 'left')\nplt.show()\nfig.tight_layout()","980c2117":"fig, ax = plt.subplots(figsize=(18,18))\n\nfor i in range(len(years)):\n    plt.subplot(len(years), 1, i+1)\n    year = years[i]\n    active_power_data = data[str(year)]\n    active_power_data['Global_active_power'].hist(bins = 200)\n    plt.title(str(year), y = 0, loc = 'left')\nplt.show()\nfig.tight_layout()","fb2132e4":"# for full data\n\nfig, ax = plt.subplots(figsize=(18,18))\n\nfor i in range(len(data.columns)):\n    plt.subplot(len(data.columns), 1, i+1)\n    name = data.columns[i]\n    data[name].hist(bins=200)\n    plt.title(name, y=0, loc = 'right')\n    plt.yticks([])\nplt.show()\nfig.tight_layout()","d9815b68":"data_train = data.loc[:'2009-12-31', :]['Global_active_power']\ndata_train.head()","fc6e1f6c":"data_test = data['2010']['Global_active_power']\ndata_test.head()","d2620780":"data_train.shape","cc671289":"data_test.shape","0e80d6b1":"data_train = np.array(data_train)\nprint(data_train)\n\nX_train, y_train = [], []\nfor i in range(7, len(data_train)-7):\n    X_train.append(data_train[i-7:i])\n    y_train.append(data_train[i:i+7])\n    \nX_train, y_train = np.array(X_train), np.array(y_train)\nX_train.shape, y_train.shape","df355a49":"pd.DataFrame(X_train).head()","bd88c86d":"x_scaler = MinMaxScaler()\nX_train = x_scaler.fit_transform(X_train)\n\ny_scaler = MinMaxScaler()\ny_train = y_scaler.fit_transform(y_train)","d10b70f5":"X_train = X_train.reshape(1098, 7, 1)","000da418":"X_train.shape","81f8e296":"model = Sequential()\nmodel.add(LSTM(units = 200, activation = 'relu', input_shape=(7,1)))\nmodel.add(Dense(7))\n\nmodel.compile(loss='mse', optimizer='adam')","6a970b9c":"model.summary()","c17ac9a4":"model.fit(X_train, y_train, epochs = 100)","82718fef":"data_test = np.array(data_test)","bf54da43":"X_test, y_test = [], []\n\nfor i in range(7, len(data_test)-7):\n    X_test.append(data_test[i-7:i])\n    y_test.append(data_test[i:i+7])","5950c8d6":"X_test, y_test = np.array(X_test), np.array(y_test)","79a13d59":"\nX_test = x_scaler.transform(X_test)\ny_test = y_scaler.transform(y_test)","57490ec6":"X_test = X_test.reshape(331,7,1)","fbd5d1ae":"X_test.shape","6930c3f6":"y_pred = model.predict(X_test)","1af7de06":"y_pred = y_scaler.inverse_transform(y_pred)\ny_pred","96306bb4":"y_true = y_scaler.inverse_transform(y_test)\ny_true","7aba856b":"\ndef evaluate_model(y_true, y_predicted):\n    scores = []\n    \n    #calculate scores for each day\n    for i in range(y_true.shape[1]):\n        mse = mean_squared_error(y_true[:, i], y_predicted[:, i])\n        rmse = np.sqrt(mse)\n        scores.append(rmse)\n    \n    #calculate score for whole prediction\n    total_score = 0\n    for row in range(y_true.shape[0]):\n        for col in range(y_predicted.shape[1]):\n            total_score = total_score + (y_true[row, col] - y_predicted[row, col])**2\n    total_score = np.sqrt(total_score\/(y_true.shape[0]*y_predicted.shape[1]))\n    \n    return total_score, scores","edbcdc5c":"evaluate_model(y_true, y_pred)","8eed497e":"# Prepare test dataset and test LSTM model","4cbafbeb":"# Evaluate the Model","1e124c52":"# Build LSTM Network","9867bced":"## Modeling Methods  \nThere are many modeling methods and few of those are as follows\n\nNaive Methods -> Naive methods would include methods that make very simple, but often very effective assumptions.  \nClassical Linear Methods -> Classical linear methods include techniques are very effective for univariate time series forecasting  \nMachine Learning Methods -> Machine learning methods require that the problem be framed as a supervised learning problem.  \nk-nearest neighbors.  \nSVM  \nDecision trees  \nRandom forest  \nGradient boosting machines  \nDeep Learning Methods -> combinations of CNN LSTM and ConvLSTM, have proven effective on time series classification tasks  \nCNN  \nLSTM  \nCNN - LSTM  ","cb6a6483":"# Exploring Active power consumption for each year","178ea0dc":"## What can we predict\n\nForecast hourly consumption for the next day.  \nForecast daily consumption for the next week.  \nForecast daily consumption for the next month.    \nForecast monthly consumption for the next year.  ","101b3b58":"# Power consumption distribution with histogram","178133ee":"# Prepare Training data"}}