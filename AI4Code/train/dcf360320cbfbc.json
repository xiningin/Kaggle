{"cell_type":{"cfaa7ad6":"code","da4e2a41":"code","786446c1":"code","1f7a5ee5":"code","2f13d617":"code","ba595fba":"code","62e0bd6e":"code","53762a37":"code","84450cb4":"code","10bcc305":"code","6befba1f":"code","f190f91b":"code","c6185aea":"code","fdf6e6df":"code","0e56d68f":"code","39c9e458":"code","cb1886b1":"code","dcb38b38":"code","56b9efd7":"code","194555a1":"code","467deabe":"code","695cc1c9":"code","101e7049":"code","eeb5b67f":"code","e7235b66":"code","11b71038":"code","ac79f5ab":"code","8b395d0c":"code","635abbe2":"markdown","fde254b8":"markdown","063d83b5":"markdown","71b41104":"markdown","170e9e1e":"markdown","70f75269":"markdown","fa753a6c":"markdown","e563ebce":"markdown","5c3653ba":"markdown","5c4f6b14":"markdown","f404b668":"markdown","1f0c20ff":"markdown","04ce2e4d":"markdown","28b5debb":"markdown"},"source":{"cfaa7ad6":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nprint(\"TensorFlow version:\", tf.__version__)","da4e2a41":"# set pandas\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 10)\npd.set_option('display.width', 500)","786446c1":"# When set to true, the network will use two months of data for validation.\n# Set to false to train on the full training set!\nvalidate_while_training = False","1f7a5ee5":"number_of_items = 30490\ntest_days = 28\nn_days_train = int(2*365) # Memory issues when training on more data\n\nif validate_while_training:\n    validation_days = test_days * 2  # (2 months for validation)\nelse:\n    validation_days = 0\n    \nlast_train_day = 1913 - validation_days # Last day used for training\nlast_val_day = last_train_day + validation_days # Last day used for validation\ndays_train_ini = last_train_day - n_days_train # First day used for training\n\ndays_back = 14 # Number of days to pass to the CNN (history)\ndays_predict = 1 # The network predicts one day at the time","2f13d617":"# Read the sales data\nsales_train_validation = pd.read_csv(\n    '\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv'\n)","ba595fba":"# Replace the strings of the categorical variables by an integer\ncat_col = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\nfor col in cat_col:\n    sales_train_validation[col] = sales_train_validation[col].astype('category')\n    sales_train_validation[col] = sales_train_validation[col].cat.codes","62e0bd6e":"# Transpose sales_train_validation\nsales_train_validation = sales_train_validation.T","53762a37":"# Extract categorical data from the sales dataset\nitems = sales_train_validation.iloc[1:6, :]","84450cb4":"# Split the evaluation set on training, validation and test (test is here the kaggle validation set)\nsales_val = sales_train_validation.iloc[6 + last_train_day:6 + last_val_day, :]\nsales_test = sales_train_validation.iloc[6 + last_val_day:, :]\nsales_train = sales_train_validation.iloc[6 + days_train_ini:6 + last_train_day, :]\ndel sales_train_validation # save memory\nprint(sales_train.shape)\nprint(sales_val.shape)\nprint(sales_test.shape)\nprint(items.shape)","10bcc305":"# Normalize the sales using MinMax\nscaler = MinMaxScaler(feature_range= (0,1))\nsales_train = scaler.fit_transform(sales_train.values)\nif validate_while_training:\n    sales_val = scaler.transform(sales_val.values)","6befba1f":"# Get the input for the first prediction of the model (i.e. day 1) \ninput_sales= sales_train[-days_back:, :]","f190f91b":"calendar = pd.read_csv(\n    '\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv',\n)\n\n# drop the columns that are not used\ndrop_cols_cal = ['wm_yr_wk', 'date', 'weekday', 'month', 'year', 'wday']\ncalendar.drop(drop_cols_cal, axis='columns', inplace=True)","c6185aea":"# Make event_name_1 a categorical variable\ncalendar.loc[:, 'event_name_1'] = calendar.event_name_1.astype('category')","fdf6e6df":"# Add boolean for event 1\ncalendar['isevent1'] = 0\ncalendar.loc[~calendar.event_name_1.isna(), 'isevent1'] = 1\ncalendar.loc[calendar.event_name_1.isna(), 'isevent1'] = 0","0e56d68f":"# Separate training and test calendars\ncalendar_train = calendar.iloc[days_train_ini:last_train_day, :]\ncalendar_val = calendar.iloc[last_train_day:last_val_day, :]\ncalendar_test = calendar.iloc[last_val_day:, :]\ndel calendar\nprint(calendar_train.shape)\nprint(calendar_val.shape)\nprint(calendar_test.shape)","39c9e458":"# Training\nX_train = []\nisevent1_train = []\nitems_train = []\ny_train = []\n\nfor i in range(days_back, last_train_day - days_train_ini - days_back):\n    X_train.append(sales_train[i-days_back:i])\n    isevent1_train.append(calendar_train.iloc[i:i+days_predict, -1].values)\n    items_train.append(items.values)\n    y_train.append(sales_train[i:i+days_predict])","cb1886b1":"# Validation\nX_val = []\nisevent1_val = []\nitems_val = []\ny_val = []\n\nfor i in range(days_back, len(sales_val)):\n    X_val.append(sales_val[i-days_back:i])\n    isevent1_val.append(calendar_val.iloc[i:i+days_predict, -1].values)\n    items_val.append(items.values)\n    y_val.append(sales_val[i:i+days_predict])","dcb38b38":"#Convert to np array\nX_train = np.array(X_train)\nisevent1_train = np.array(isevent1_train)\ny_train = np.array(y_train)\nX_val = np.array(X_val)\nisevent1_val = np.array(isevent1_val)\ny_val = np.array(y_val)\n\nitems_train = np.array(items_train)\nitems_val = np.array(items_val)\n\nprint('Shape of X_train: ', X_train.shape)\nprint('Shape of isevent1_train: ', isevent1_train.shape)\nprint('Shape of y_train: ', y_train.shape)\nprint('Shape of X_val: ', X_val.shape)\nprint('Shape of isevent1_val: ', isevent1_val.shape)\nprint('Shape of y_val: ', y_val.shape)\n\nprint('Shape of items_train: ', items_train.shape)\nprint('Shape of items_val: ', items_val.shape)","56b9efd7":"# Reshape the arrays (this could be done with more compact code, but it was quickly\n# adapted from the CNN for all items)\nX_train = X_train.reshape((X_train.shape[0]*number_of_items, days_back, 1))\nX_val = X_val.reshape((X_val.shape[0]*number_of_items, days_back, 1))\ny_train = y_train.reshape((y_train.shape[0]*number_of_items, days_predict, 1))\ny_val = y_val.reshape((y_val.shape[0]*number_of_items, days_predict, 1))\n\nisevent1_train = np.tile(isevent1_train, (number_of_items, 1))\nisevent1_val = np.tile(isevent1_val, (number_of_items, 1))\n\nitems_train = items_train.reshape((items_train.shape[0]*number_of_items, items.shape[0], 1))\nitems_train = np.moveaxis(items_train, 1, -1)\nitems_val = items_val.reshape((items_val.shape[0]*number_of_items, items.shape[0], 1))\nitems_val = np.moveaxis(items_val, 1, -1)\n\nprint('Shape of X_train: ', X_train.shape)\nprint('Shape of isevent1_train: ', isevent1_train.shape)\nprint('Shape of y_train: ', y_train.shape)\nprint('Shape of X_val: ', X_val.shape)\nprint('Shape of isevent1_val: ', isevent1_val.shape)\nprint('Shape of y_val: ', y_val.shape)\nprint('Shape of items_train: ', items_train.shape)\nprint('Shape of items_val: ', items_val.shape)","194555a1":"del calendar_train, sales_train","467deabe":"def get_sequential_CNN():\n    # Categorical input\n    cat_input = tf.keras.Input(shape=(days_predict, items.shape[0]))\n    isevent1_input = tf.keras.Input(shape=(days_predict, 1))\n\n    # CNN\n    inputs_sales = tf.keras.Input(shape=(days_back, 1))\n    sales = tf.keras.layers.Conv1D(32, 7, strides=1, activation='relu')(inputs_sales)\n    sales = tf.keras.layers.BatchNormalization()(sales)\n    sales = tf.keras.layers.Dropout(0.2)(sales)\n    sales = tf.keras.layers.Conv1D(64, 5, strides=1, activation='relu')(sales)\n    sales = tf.keras.layers.BatchNormalization()(sales)\n    sales = tf.keras.layers.Dropout(0.2)(sales)\n    sales = tf.keras.layers.AveragePooling1D(4)(sales)\n\n    # Concatenate CNN and categorical data\n    concat = tf.keras.layers.Concatenate()([sales, cat_input, isevent1_input])\n    concat = tf.keras.layers.BatchNormalization()(concat)\n\n    # Dense\n    dense = tf.keras.layers.Dense(100, activation='relu')(concat)\n    dense = tf.keras.layers.BatchNormalization()(dense)\n    dense = tf.keras.layers.Dense(100, activation='relu')(dense)\n    dense = tf.keras.layers.BatchNormalization()(dense)\n\n    # Output layer\n    out = tf.keras.layers.Dense(1)(dense)\n\n    model = tf.keras.Model(inputs=[inputs_sales, cat_input, isevent1_input], outputs=out)\n\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    model.compile(loss=\"mean_squared_error\", optimizer=opt)\n    \n    return model\n","695cc1c9":"model = get_sequential_CNN()\nprint(model.summary())\ntf.keras.utils.plot_model(model, show_shapes=False)","101e7049":"# Fitting the network to the Training set\nepoch_no=5\nbatch_size=30490\n\nif validate_while_training:\n    model.fit([X_train, items_train, isevent1_train], \n              y_train, \n              epochs = epoch_no, \n              batch_size = batch_size,\n              validation_data=([X_val, items_val, isevent1_val], y_val))\nelse:\n    model.fit([X_train, items_train, isevent1_train], \n              y_train, \n              epochs = epoch_no, \n              batch_size = batch_size)\n    \n#model.save('CNN_per_item_avg_3ep_2048batch.h5')","eeb5b67f":"# Initialize X_test\nX_test = []\npredictions = []\n\nX_test.append(input_sales[0:days_back, :])\nX_test = np.array(X_test)\nX_test = X_test.reshape((X_test.shape[0]*number_of_items, days_back, 1))","e7235b66":"# Loop for each of the 28 test days\n# Note that negative sales are set to 0 during each iteration\nitems_test = items.T.values[:, np.newaxis, :]\nfor j in range(days_back,days_back + test_days):\n    isevent1_test = np.tile(calendar_test.iloc[j-days_back, -1], (number_of_items,1))\n    test_input = [X_test[:,j - days_back:j, :].reshape(number_of_items, days_back, 1),\n                  items_test,\n                  isevent1_test]\n    predicted_sales = model.predict(test_input)\n    testInput = np.array(predicted_sales)\n    testInput[testInput < 0] = 0\n    X_test = np.append(X_test, testInput).reshape(number_of_items, j + 1, 1)\n    predicted_sales = testInput\n    predictions.append(predicted_sales)\n\npredictions = scaler.inverse_transform(np.array(predictions).reshape(28,30490))\npredictions = pd.DataFrame(data=predictions)\nprint(predictions.shape)","11b71038":"print(predictions.head(10))\nprint(predictions.shape)\nprint(sales_test.head(10))","ac79f5ab":"# Simple mse\nerror = mean_squared_error(sales_test.values, predictions.values, squared=True)\nprint(error)","8b395d0c":"submission = pd.DataFrame(data=np.array(predictions).reshape(28,30490))\n\nsubmission[submission < 0] = 0\n\nsubmission = submission.T\n    \nsubmission = pd.concat((submission, submission), ignore_index=True)\n\nsample_submission = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv\")\n    \nidColumn = sample_submission[[\"id\"]]\n    \nsubmission[[\"id\"]] = idColumn  \n\ncols = list(submission.columns)\ncols = cols[-1:] + cols[:-1]\nsubmission = submission[cols]\n\ncolsdeneme = [\"id\"] + [f\"F{i}\" for i in range (1,29)]\n\nsubmission.columns = colsdeneme\n\nsubmission.to_csv(\"\/kaggle\/working\/submission_cnn_per_item.csv\", index=False)","635abbe2":"# Train the model","fde254b8":"# Calendar data","063d83b5":"The model was constructed to resemble the structure of the CNN for all items. That is, 2 convolutional layers with average pooling to result in a single output per neuron.\nThe output from the convolutional part is concatenated with the categorical information (item-specific) and the isevent1 boolean.\nTwo fully connected layers were added, since the network performed really bad with a single one. \n\nThis model has not been designed to achieve the lowest possible score, and it has not been fine tuned. The purpose of the model is to investigate whether CNNs perform better when using correlations between the sales of different items or when processing a single item at the time.","71b41104":"# Construct the training, validation and test data arrays","170e9e1e":"The code to test the model was taken and adapted from the public notebook https:\/\/www.kaggle.com\/bountyhunters\/baseline-lstm-with-keras-0-7 ","70f75269":"# Set some constants","fa753a6c":"# Test the model","e563ebce":"This is only relevant for the model testing (i.e. when we want to predict the sales in the kaggle validation period). We extract the last 14 days from the training data, since that is the input neede to predict the day 1 of the kaggle validation set.","5c3653ba":"# Construct the model","5c4f6b14":"# Sales data","f404b668":"In the following, a boolean is added to indicate whether a day is a special event as defined by event_name_1. This was inspired by the public notebook https:\/\/www.kaggle.com\/bountyhunters\/baseline-lstm-with-keras-0-7.","1f0c20ff":"# Generate the submission file\n\nThis piece of code was taken from the public notebook https:\/\/www.kaggle.com\/bountyhunters\/baseline-lstm-with-keras-0-7","04ce2e4d":"The way to arrange the data is inspired by the notebook: https:\/\/www.kaggle.com\/bountyhunters\/baseline-lstm-with-keras-0-7. The data pipeline has been here arranged in a similar fashion and parts of the code have been taken and adapted from the public notebook","28b5debb":"## Visual comparison of the predicted sales with the true sales"}}