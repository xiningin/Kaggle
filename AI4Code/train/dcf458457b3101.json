{"cell_type":{"95533aa6":"code","3e635375":"code","3f4037db":"code","373576df":"code","1bd317ef":"code","740425a3":"code","7d4d8fdb":"code","e3c6b4db":"code","2f3ac1c2":"code","9478fcbe":"code","64a7e397":"code","08b5c76f":"code","95c8e75d":"code","f61d81a8":"code","7dfb45ee":"code","02fcaace":"markdown","60201015":"markdown","e4346f44":"markdown","99e29a4a":"markdown"},"source":{"95533aa6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_probability as tfp\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport gresearch_crypto\nimport gc\n\n\npd.set_option('display.max_columns', None)\n\nDEBUG = False","3e635375":"train = pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv').set_index(\"timestamp\")\nassets = pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv')\nassets_names = dict(zip(assets.Asset_ID, assets.Asset_Name))\n#for assets sorting \nassets_order = pd.read_csv('..\/input\/g-research-crypto-forecasting\/supplemental_train.csv').Asset_ID[:14]\nassets_order = dict((t,i) for i,t in enumerate(assets_order))\n\nif DEBUG:\n    train = train[1000000:12000000]","3f4037db":"def add_features(df):\n    df['Upper_Shadow'] = df['High'] - np.maximum(df['Close'], df['Open'])\n    df['Lower_Shadow'] = np.minimum(df['Close'], df['Open']) - df['Low']\n    return df","373576df":"# data periods where all assets presented\ntrain['assets']=1\ntrain['assets']=train.groupby(by = train.index)['assets'].sum()\ntrain['asset_name'] = train.Asset_ID.map(assets_names)\ntrain['asset_name'].value_counts()\n\nall_same_time = train[train['assets']==14][['Asset_ID', 'Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'asset_name','VWAP','Target']]\nall_same_time.shape","1bd317ef":"corr_target = all_same_time.reset_index().pivot(index='asset_name', columns='timestamp')['Target'].transpose().corr()\ncorr_matrix = all_same_time.reset_index().drop(['Target', 'Asset_ID'], axis=1).pivot(index='asset_name', columns='timestamp').transpose().corr()\n\nfig, ax = plt.subplots(1,2,figsize=(20,8))\nsns.heatmap(np.round(corr_target, 2), annot=True, ax=ax[0], square=True)\nsns.heatmap(np.round(corr_matrix, 2), annot=True, ax=ax[1], square=True)\nax[0].title.set_text('Asset Targets correlation')\nax[1].title.set_text('Asset Features correlation')\n","740425a3":"fig = make_subplots(rows=7, cols=2,shared_xaxes=True, vertical_spacing=0.03, subplot_titles=tuple([assets_names[i] for i in range(14)]))\n\ndata = all_same_time[1000:2400]\ndata['time'] = [pd.to_datetime(x, unit='s') for x in data.index]\nfor i in range(14):\n    \n    coin = data[data.Asset_ID == i]\n    name = assets_names[i]\n\n    fig.add_trace(go.Scatter(x=coin['time'], y=coin['VWAP'], name = name + ', VWAP'),row=i\/\/2+1, col= i%2 +1)\n\nfig.update_layout(height=1000, title_text=' Weighted average prices')\nfig.show()","7d4d8fdb":"train=train.dropna()\ntrain","e3c6b4db":"VWAP_max = np.max(train[np.isfinite(train.VWAP)].VWAP)\nVWAP_min = np.min(train[np.isfinite(train.VWAP)].VWAP)\nprint(VWAP_max, \"\\n\", VWAP_min)","2f3ac1c2":"assets=train.Asset_ID.to_numpy()\ntargets = train['Target'].to_numpy() \n\nfeatures = ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Upper_Shadow','Upper_Shadow']\ntrain = add_features(train)[features]","9478fcbe":"#scaler = QuantileTransformer(n_quantiles=10000, output_distribution='normal', random_state=0)\nscaler = RobustScaler()\n\ntrain.VWAP = np.nan_to_num(train.VWAP, posinf=VWAP_max, neginf=VWAP_min)\ntrain = scaler.fit_transform(train)\ntrain.shape","64a7e397":"##https:\/\/github.com\/tensorflow\/tensorflow\/issues\/37495\ndef MaxCorrelation(y_true,y_pred):\n    \"\"\"\n    Goal is to maximize correlation between y_pred, y_true. Same as minimizing the negative.\n    \"\"\"\n    return -tf.math.abs(tfp.stats.correlation(y_pred,y_true, sample_axis=None, event_axis=None))\n\ndef Correlation(y_true,y_pred):\n\n    return tf.math.abs(tfp.stats.correlation(y_pred,y_true, sample_axis=None, event_axis=None))\n\ndef get_model():  \n    asset_input = keras.Input(shape=(1,))\n    feat_input = keras.Input(shape=(train.shape[-1:]))\n    \n    x = layers.Embedding(15, 16, input_length=1)(asset_input)\n    \n    x = keras.layers.Flatten()(x)\n    combined = keras.layers.Concatenate()([x, feat_input])\n     \n    x = layers.Dense(units=512)(combined)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(units=192)(x)\n    #x = layers.Dense(units=128)(x)\n    x = layers.Dense(units=96)(x)\n    #out = layers.Dense(units=1, activation='tanh')(x)\n    out = layers.Dense(units=1)(x)\n    \n    model = keras.Model(inputs=[asset_input, feat_input], outputs=out)\n    \n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5), \n                  #loss = 'mse', \n                loss = 'cosine_similarity',\n                  #loss = MaxCorrelation,\n                  metrics=[Correlation]\n                    )\n    \n    return model  \n\nmodel=get_model()\nmodel.summary()","08b5c76f":"size = list(range(len(train)))\n\ntrain_ind,test_ind = train_test_split(np.array(size), shuffle=True,random_state=42, test_size=0.15)","95c8e75d":"X_train, y_train, assets_train = train[train_ind], targets[train_ind], assets[train_ind]\nX_valid, y_valid, assets_valid = train[test_ind], targets[test_ind], assets[test_ind]","f61d81a8":"tf.random.set_seed(0)\nBATCH_SIZE=2**15\n\n#estop = keras.callbacks.EarlyStopping(monitor='val_Correlation', patience=7, verbose=0, mode='max',restore_best_weights=True)\nestop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min',restore_best_weights=True)\nscheduler = keras.optimizers.schedules.ExponentialDecay(1e-4, (5e-2*(len(X_train))\/BATCH_SIZE), 1e-3)\n#scheduler = keras.optimizers.schedules.ExponentialDecay(1e-3, (1e-4*(len(X_train))\/BATCH_SIZE), 1e-3)\nlr = keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)\n        \nmodel.fit([assets_train, X_train], y_train, \n          validation_data = ([assets_valid, X_valid], y_valid), \n          epochs = 20, batch_size = BATCH_SIZE, \n          shuffle=True, callbacks = [lr, estop])\n","7dfb45ee":"env = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    asset = test_df.Asset_ID.to_numpy()\n    test = add_features(test_df)[features]\n    test.VWAP = np.nan_to_num(test.VWAP, posinf=VWAP_max, neginf=VWAP_min)\n    test = scaler.transform(test)\n    y_pred = model.predict([asset, test]).squeeze()\n    \n    sample_prediction_df['Target'] = y_pred\n    #display(test_df)\n    #display(sample_prediction_df)    \n    env.predict(sample_prediction_df)","02fcaace":"### Data preparation","60201015":"### Submission","e4346f44":"### Model","99e29a4a":"### Assets correlation"}}