{"cell_type":{"f74684be":"code","ca968c44":"code","0b28e153":"code","db4b6b99":"code","ac526997":"code","17fa140e":"code","6ac918f7":"code","d4c0540f":"code","037414d5":"code","afc1a2e6":"code","f29ccf1d":"code","150e4f46":"markdown","2477b3b9":"markdown","7c61386f":"markdown","7d860416":"markdown","a08d3592":"markdown","ea1a97b5":"markdown","da4b21b7":"markdown"},"source":{"f74684be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ca968c44":"import pandas as pd\nimport numpy as np\nimport os\nimport pandas_profiling\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom matplotlib.pyplot import figure","0b28e153":"train = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-1\/train.csv\")  #(source: kaggle forecasting challenge..)\ntrain[\"Date\"] = pd.to_datetime(train[\"Date\"])\n\n\n## recovered cases: \nrecover = pd.read_csv(\"\/kaggle\/input\/novel-corona-virus-2019-dataset\/time_series_covid_19_recovered.csv\")  ## source: https:\/\/www.kaggle.com\/sudalairajkumar\/novel-corona-virus-2019-dataset\nrecover = pd.melt(recover, id_vars=['Province\/State', 'Country\/Region', 'Lat', 'Long'], var_name='Date')\nrecover[\"Date\"] = pd.to_datetime(recover[\"Date\"], format= \"%m\/%d\/%y\")\nrecover = recover.rename(columns= {\"value\": \"recovered\"})\nrecover = recover[[\"Lat\", \"Long\", \"Country\/Region\", \"Date\", \"recovered\"]].drop_duplicates()\n\ntrain = pd.merge(train, recover, on= [\"Lat\", \"Long\", \"Country\/Region\", \"Date\"], how=\"left\")","db4b6b99":"## data at country level\nfinal = train.groupby([\"Country\/Region\", \"Date\"])[\"ConfirmedCases\",\"Fatalities\", \"recovered\"].sum().reset_index()\n\n## consider only when case>0\nfinal = final[final[\"ConfirmedCases\"]>0]\n\n## since recovered dataset is having data till 2020-03-23\nfinal = final[final[\"Date\"]<= \"2020-03-23\"]\n\nprint(final.Date.min())\nprint(final.Date.max())","ac526997":"## check if there is any bug while merging  dataset coming from 2 different source:\nfinal[final[\"ConfirmedCases\"]< final[\"recovered\"]]","17fa140e":"## adding active cases\nfinal[\"active_cases\"] = final[\"ConfirmedCases\"] - final[\"recovered\"]","6ac918f7":"# finction to get confirmed cases, active cases, velocity and acceleration of a given country: \n\n'''\ncountry : name of the country. \ndt: day span you want to take aggregation on. let's say if select 7 days: we will calculate velocity, acceleration etc wrt 7th day lag value\ncutoff: cases cutoff below which you want to ignore your analysis. because graph is very noisy\/unstable for very low cases making difficult for any interpretation. \n\nreturn: dataframe with all required metrics like cases (confirmed\/active), velocity and acceleration..\n'''\n\ndef gen_growth (country, dt=1, cutoff = 5): \n    temp = final[(final[\"Country\/Region\"] == country)& (final[\"ConfirmedCases\"]>= cutoff)].groupby(\"Date\")[\"ConfirmedCases\", \"active_cases\"].sum().reset_index()\n    \n    # cases velocity\n    train_lag =temp.shift(periods=dt)\n    temp['lag_confirmedcases']=train_lag['ConfirmedCases']\n    temp[\"velocity\"] = temp['ConfirmedCases'] - temp['lag_confirmedcases']\n\n    # Acceleration\n    train_lag =temp.shift(periods=dt)\n    temp['lag_velocity']=train_lag['velocity']\n    temp[\"acceleration\"] = temp['velocity'] - temp['lag_velocity']\n    \n    temp[\"confirm_scale\"] = temp[\"ConfirmedCases\"]*10\/temp[\"ConfirmedCases\"].max()\n    temp[\"active_scale\"] = temp[\"active_cases\"]*10\/temp[\"active_cases\"].max()\n    temp[\"velocity_scale\"] = temp[\"velocity\"]*10\/temp[\"velocity\"].max()\n    temp[\"acc_scale\"] = temp[\"acceleration\"]*10\/temp[\"acceleration\"].max()\n    \n    return temp[[\"Date\",\"ConfirmedCases\",  \"confirm_scale\", \"active_scale\", \"velocity_scale\", \"acc_scale\"]]","d4c0540f":"\ncutoff = 10\ndt = 7  ## graphical representation will be smoothned for higher dt \nnum = 30  ## get graphs of top 20 countries with higher corona cases.. \ntemp_countries = final.groupby(['Country\/Region']).ConfirmedCases.max().reset_index().sort_values(\"ConfirmedCases\", ascending=False).head(num)\ncountries = temp_countries['Country\/Region'].unique()\n\nfor cont in countries:\n    figure(num=None, figsize=(12, 4), dpi=80, facecolor='w', edgecolor='k')\n    temp = gen_growth(cont, dt=dt, cutoff = cutoff)\n    max_case = temp.ConfirmedCases.max()\n    \n    plt.plot(list(temp.confirm_scale))\n    plt.plot(list(temp.active_scale))\n    plt.plot(list(temp.velocity_scale))\n    plt.plot(list(temp.acc_scale))\n    plt.legend(['confirmed cases', \"active cases\", \"velocity\", \"acceleration\"], loc='upper left')\n    plt.title(cont+\"_\"+str(max_case))\n\n    plt.show()   ","037414d5":"temp_countries = final.groupby(['Country\/Region']).ConfirmedCases.max().reset_index().sort_values(\"ConfirmedCases\", ascending=False).head(30)\ncountries = temp_countries['Country\/Region'].unique()\n\ntemp_final = pd.DataFrame()\n\nfor cont in countries:\n    temp = gen_growth(cont, dt=7, cutoff = 10)\n    temp = temp[temp[\"Date\"]== temp.Date.max()]\n    temp[\"country\"] = cont\n    temp_final = temp_final.append(temp)\n    \nbins = [-10, 0, 4,  8, 10]  \nlabels = [-1, 0, 1, 2]\ntemp_final['velocity_binned'] = pd.cut(temp_final['velocity_scale'],  bins=bins, labels=labels)\n\nbins = [-10, 0, 3,  8, 10]  \nlabels = [-1, 0, 1, 2]\ntemp_final['acc_binned'] = pd.cut(temp_final['acc_scale'],  bins=bins, labels=labels)\n\ntemp_final = temp_final[~(temp_final[\"acc_scale\"].isna())].reset_index()\ntemp_final","afc1a2e6":"### Creating heat map:\n\ndf1 = pd.DataFrame(list(range(-1,3)))\ndf2 = pd.DataFrame(list(range(0,3)))\n\ndf1['key'] = 0\ndf2['key'] = 0\n\nmatrix = pd.merge(df1, df2, on=\"key\")\nmatrix = matrix[[\"0_x\", \"0_y\"]].rename(columns= {\"0_x\": \"acceleration\", \"0_y\": \"velocity\"})\nmatrix[\"risk\"] = [0, 1, 10,        ## High value indicating high risk and lower values as lower risk .. \n                  1, 10, 20, \n                  10, 20, 30,\n                  20, 30, 40]\n\nmatrix = matrix.pivot(\"acceleration\", \"velocity\", \"risk\")\nheat_map = sns.heatmap(matrix)","f29ccf1d":"## countries according to risk:\nlowest = temp_final[(temp_final[\"velocity_binned\"] == 0)& (temp_final[\"acc_binned\"] <= 0) ].country.unique()\nlower = temp_final[(temp_final[\"velocity_binned\"] <= 1)& (temp_final[\"acc_binned\"] <= 0) & (~(temp_final[\"country\"].isin(lowest)))].country.unique()\nmoderate = temp_final[(temp_final[\"velocity_binned\"] <= 1)& (temp_final[\"acc_binned\"] <= 1) & (~(temp_final[\"country\"].isin(lowest)))& (~(temp_final[\"country\"].isin(lower)))].country.unique()\n\n\nprint(\"lowest risk:\", lowest)\nprint(\"lower risk:\", lower)\nprint(\"moderate risk:\", moderate)\nprint(\"high risk: almost rest of the countries.. \")","150e4f46":"### Data Prep","2477b3b9":"#### Let's plot countries on matrix of Velocity and acceleration: ","7c61386f":"## analysis on confirmed cases:\n\n##### 1. we want to get plot of (for each country):\n    a) confirmed cases on scale of 0-10 (10 being the max confirmed cases for that specific country; 0 being the lowest)\n    b) active cases on scale of 0-10 (10 being the max active cases for that specific country; 0 being the lowest)\n    c) velocity on scale of 0-10 (velocity is # new cases coming out that day)\n    d) acceleration scaled (acceleartion is growth rate of new cases coming out; negative acceleration means that new cases coming out at a given day is decreasing wrt previous days)\n    \n##### 2. We should consider aggregation over 5-7 days (as mentioned in \"gen_growth\" function definition by using parameter \"dt\"). \n    We are assuming that, sometime country might report fewer cases for a day (may be due to different time zones etc..) and report higher cases (some cases might occured previous day). So, we are sort off avergaing out the metrices bases on provided parameter \"dt\"","7d860416":"### Heatmap plot:\n1. Darker portion of the map indicating \"Lower risk\" and Lighter portion indicating \"High risk\" zone.. \n2. Countries like \"China\" having negative acceleration (-1 in heatmap above) and very low velocity (0 in heatmap) are mostly in safe zone (low risk zone where situation is under control). Please refer binned column to refer high or low velocity\/acceleration..\n3. While, country like \"Italy\" has both velocity and acceleration very high. Hence being presented under very high risk zone.","a08d3592":"## Some inferences and validation:\n\n1. Denmark was first country in Europe to declare lockdown. We can see that acceleration went to negative number (almost equal to one was the highest in past). Hence, active cases is getting little stagnated. Demark has announced lockdown till 13th April. We started getting positive impact of this.(reference: https:\/\/www.thelocal.dk\/20200320\/why-is-denmarks-lockdown-so-much-more-severe-than-swedens)\n2. Japan case study to tackle corona virus case. We can easily interpret how it went down in between and then went up for some time and further going down using case study mentioned. Initially one state of Japan took action (complete lockdown) but other states were lenient. But they took lesson from that state and implemented complete lockdown..  \n(reference: https:\/\/www.washingtonpost.com\/world\/asia_pacific\/japan-coronavirus-wakayama\/2020\/03\/22\/02da83bc-65f5-11ea-8a8e-5c5336b32760_story.html)  \n3. Behrain also declared lockdown. Positive impact of this one.. Reference: (https:\/\/www.al-monitor.com\/pulse\/originals\/2020\/03\/bahrain-pardon-prisoners-coronavirus-formula-one.html )\n\n\n### here, our thought process is just to check any particular country doing well to handle COVID-2019. And check out what went well in the favour of those countries and then, try to apply similar things in other countries. \n##### we can take a good example of Denmark and Sweden: Both countries are very similar in terms of demogrpahics etc. But Denmark did pretty well in managing this pandemic. But Sweden was very lenient in initial phases of COVID-2019. Although, Growth rate of new cases is decreasing in sweden but still positive (means: growth of new cases is not exponential). But, on other side, growth rate in case of denmark is negative. (source: https:\/\/www.thelocal.dk\/20200320\/why-is-denmarks-lockdown-so-much-more-severe-than-swedens)\n    ","ea1a97b5":"#### Please suggest any edits if you have any to make this better.. ","da4b21b7":"### Outcome expected from this notebook: \n\nHi All,\n\nThis notebook is just to give an estimation on \"Where different countries stand in terms of Risk due to COVID-2019\". Also, trying to find out Countries doing well in handling this. Using this, we can take lessons from those countries. \n\nWe can keep updating the different dataset (source mentioned) used in this notebook to get updated \"Risk level\" for each country. We understand that, we do not have domain knowldege; hence providing flexibility to update any parameters and find the results which can be used anywhere to handle this pandemic. \n\nHope everything will be get normal soon!!"}}