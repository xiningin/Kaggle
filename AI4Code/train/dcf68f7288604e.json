{"cell_type":{"80d4425a":"code","983b8009":"code","32645e0a":"code","8b773359":"code","71060346":"code","5d846a93":"code","41fb5dc6":"code","7bdc29ec":"code","794352d0":"code","6fd8de65":"code","5684d09f":"code","9f7df3ac":"code","83bc4b32":"code","f306f13a":"code","20a5c5a4":"code","f897e610":"code","58e051b9":"code","7abb8e9e":"code","8383077a":"code","6ca2f526":"code","1a30ba8a":"code","f1422189":"code","8648e04d":"code","329d5ba3":"code","67fb281e":"code","17cf1aa4":"code","8b811ae4":"code","f915549d":"code","e864daf4":"code","bfb1b3e5":"code","e167b276":"code","be604bc9":"code","84ef4d94":"code","8b97e211":"code","9af2a092":"code","fd94885e":"code","b9c605e5":"code","5a21eb9f":"code","4245833b":"code","1e7ee919":"code","0b394d28":"code","c9d18332":"code","85281e6c":"code","fada65e8":"code","48d7c815":"code","b7a02c90":"code","84a3fe19":"markdown","867badb1":"markdown","bd4ce453":"markdown","93ad20e4":"markdown","7d11a4eb":"markdown","e0d5f01f":"markdown","b2c3aa25":"markdown","29c3d449":"markdown","12768d12":"markdown","f34b02f4":"markdown","a94055f2":"markdown","5c1d5b84":"markdown","ebf6334c":"markdown","3b75c95c":"markdown","1fce0456":"markdown","65ca2ea2":"markdown","5c26feb1":"markdown","3f17f0e7":"markdown","81b65d04":"markdown","11ef53f2":"markdown","2f7ed555":"markdown","06d6eef8":"markdown","749fe893":"markdown","b16f3cf2":"markdown","5d7848d0":"markdown","92b6446f":"markdown","4555fd65":"markdown","3ada7668":"markdown","8010cec5":"markdown","3bc98465":"markdown","0e79bb45":"markdown","2dd2c767":"markdown","5de9dfdf":"markdown","1fd9f28e":"markdown","21212036":"markdown","43f7e7a6":"markdown","1bc6a9db":"markdown","91116440":"markdown","749ff2b8":"markdown","f0d0bcb4":"markdown","b60668bc":"markdown","5ec4c1aa":"markdown","0625e5cc":"markdown","4c52b118":"markdown","65f39df1":"markdown","b82fc1e4":"markdown","edf923a9":"markdown","3c1a8bd8":"markdown","6612a7b9":"markdown","c237bf3f":"markdown","dea56a81":"markdown","30401bc7":"markdown","a9132bf3":"markdown","cfc10533":"markdown","d1235be5":"markdown","d5bb4a75":"markdown","3963d393":"markdown","5fdd7c3f":"markdown","258c2e10":"markdown","6bc95577":"markdown","229be7af":"markdown","64229394":"markdown","f9c1b7c8":"markdown","028457aa":"markdown","c5e46d78":"markdown","d9bad749":"markdown","fc7cc004":"markdown","3f4e5112":"markdown","37ac6dcf":"markdown","1f426085":"markdown"},"source":{"80d4425a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","983b8009":"####IMPORT LIBARAIES FOR DATA PROCESSING & ANALYSIS####\n\nimport pandas as pd \nimport numpy as np \n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","32645e0a":"#Read in the csv with one header row (header = 0) and convert empty string, space, question mark, or non-breaking space to NaN\ndf_churn = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv', header = 0,  sep = \",\",na_values =  [\"\",\" \",\"?\"])","8b773359":"#Check the shape of the dataset\ndf_churn.shape\nprint('The shape of the dataset is ' + str(df_churn.shape))","71060346":"#Check the data type for each column and number of records\n\n#we can notice TotalCharges column has null values as it only has 7032 records compared to others with 7043 records\ndf_churn.info()","5d846a93":"#check the first 5 rows of the dataset \ndf_churn.head()","41fb5dc6":"#check the last 5 rows of the dataset \ndf_churn.tail()","7bdc29ec":"# explore the numeric attributes by showing descriptive statistics \ndf_churn.describe(include = [np.number])","794352d0":"# explore the categorical attributes by showing descriptive statistics \ndf_churn.describe(include = [np.object])","6fd8de65":"print(df_churn.isnull().any())#check which columns have missing values \n\nprint(\"\\n\")\n\nprint(\"There are\",df_churn.isnull().any().sum(),\" columns contain missing values\") #determine how many columns have missing values\n\nprint(\"\\n\")\n\nprint(\"There are\",df_churn.isnull().any(axis = 1).sum(),\"rows contain missing values\") #determine how many rows have missing values\n\nprint(\"\\n\")\n\n#checking percentage of missing values for each column \nfor i in (df_churn.columns):\n    # count number of rows with missing values\n    n_miss = df_churn[i].isnull().sum()\n    perc = n_miss \/ df_churn.shape[0] * 100\n    print('> {}, Missing: {}  total {} %' .format (i, n_miss, perc))\n    \nprint(\"\\n\")\n\nprint(\"Following are the rows with missing values\")\n\ndf_churn[df_churn.isnull().any(axis = 1)== True]  #display rows with missing values","5684d09f":"df_churn.fillna({\"TotalCharges\": df_churn[\"MonthlyCharges\"].median()},inplace = True)\n\nprint(\"There are\",df_churn.isnull().any().sum(),\"column(s) has (have) missing values\")","9f7df3ac":"# double check if the missing values for 0 tenure have been replaced by mean value of monthly charges\ndf_churn.loc[df_churn['tenure'] == 0]","83bc4b32":"# Checking the format consistency in categorical attributes\n\ncolumn_cata = ['gender','Partner','PhoneService','MultipleLines','InternetService',\n               'OnlineSecurity','OnlineBackup', 'DeviceProtection',\n               'TechSupport','StreamingTV','StreamingMovies', 'Contract', 'PaperlessBilling', \n               'PaymentMethod', 'Churn']\n\nfor c in df_churn[column_cata]:\n    print(df_churn[c].value_counts())\n    print(\"*\"*50)","f306f13a":"####### checking dupliated row, and no duplicated rows have been found #######\nprint('There (is) are '+ str(df_churn.duplicated().sum()) + ' duplicated row(s)')\nduplicated_rows = df_churn[df_churn.duplicated()]\nduplicated_rows","20a5c5a4":"#Plot boxplot for 'tenure'\nimport plotly.express as px\nfig = px.box(df_churn['tenure'])\nfig.update_layout(height=400, width=500,\n                  title_text=\"tenure\")\nfig.show()","f897e610":"#Plot boxplot for 'MonthlyCharges' \n\nimport plotly.express as px\nfig = px.box(df_churn['MonthlyCharges'])\nfig.update_layout(height=400, width=500,\n                  title_text=\"MonthlyCharges\")\nfig.show()","58e051b9":"#Plot boxplot for 'TotalCharges' \n\nimport plotly.express as px\nfig = px.box(df_churn['TotalCharges'])\nfig.update_layout(height=400, width=500,\n                  title_text=\"TotalCharges\")\nfig.show()","7abb8e9e":"## PLOT PIE CHART TO INVESTIGATE THE TARGET PROPORTION\nfig = px.pie(df_churn['Churn'].value_counts().reset_index().rename(columns={'index':'Type'}), \n             values='Churn', names='Type', title='Churn (Target) Distribution',\n             color_discrete_sequence=['rgba(255, 140, 184, 0.5)','rgba(93, 164, 214, 0.5)'])\n\nfig.update_traces(textposition = 'inside', \n                  textinfo = 'percent+label',\n                  labels=['No Churn','Churn'],\n                  marker=dict(line=dict(color='#000000', width=2)))\n\nfig.update_layout({\"title\": {\"text\": \"Target (Churn) Distribution\",\n                             \"font\": {\"size\": 20}}})\nfig.update_layout( autosize = False, width = 400, height = 400)\nfig.show()","8383077a":"import seaborn as sns\n## PLOT PAIR PLOT TO INVESTIFATE THE CORRELATIONS BETWEEN NUMERIC ATTRIBUTES USING PAIRPLOT\ncols = [\"TotalCharges\", \"MonthlyCharges\", \"tenure\"] \npairplot_feature = df_churn[cols]\nsns.pairplot(pairplot_feature)","6ca2f526":"## USING HEATMAP TO VISUALISE THE CORRELATION VALUES\ndf_heatmap = df_churn.drop(columns = ['Churn','SeniorCitizen'])\ncorr = df_heatmap.corr().round(3)\nplt.figure(figsize = (10,6))\nsns.heatmap(corr, annot = True, cmap = 'RdPu');\nplt.show()","1a30ba8a":"######## DROPPING IRRELEVANT COLUMNS #######\ndf_churn.drop(columns=['customerID'],axis = 1,inplace = True)\nprint(\"Checking if the columns has been dropped\")\ndf_churn.head()","f1422189":"#######ENCODING INPUT ATTRIBUTES & THE TARGET CLASS ATTRIBUTE#######\ndf_churn['Churn'].replace(to_replace='No',  value=0, inplace=True)\ndf_churn['Churn'].replace(to_replace='Yes', value=1, inplace=True)\n\ndf_dummies = pd.get_dummies(df_churn)\ndf_dummies.head()","8648e04d":"#######VISUALISE THE CORRELATION#####\nplt.figure(figsize=(20,13))\ndf_dummies.corr()['Churn'].sort_values(ascending = False).plot(kind='barh')","329d5ba3":"#CALCULATE THE CORRELATION IN ASCENDING ORDER\nprint (\"Correlation between the target class attribute to other input attributes\")\nprint (\"*\" * 100)\ndf_dummies.corr()['Churn'].sort_values(ascending = False).round(2)","67fb281e":"## PREPARING FOR THE BARPLOT FOR DISCOVERING THE RELATIONSHIPS\nimport plotly.graph_objs as go\nimport plotly.offline as py\n\ndef barplot(var_select, x_no_numeric) :\n    churn_bar = df_churn[(df_churn['Churn'] != 0)]\n    no_churn_bar = df_churn[(df_churn['Churn'] == 0)]\n    churn_rate = pd.DataFrame(pd.crosstab(df_churn[var_select],df_churn['Churn']), )\n    churn_rate['Attr%'] = churn_rate[1] \/ (churn_rate[1] + churn_rate[0]) * 100\n    if x_no_numeric == True  : \n        churn_rate = churn_rate.sort_values(1, ascending = False)\n\n    trace1 = go.Bar(\n        x = churn_bar[var_select].value_counts().keys().tolist(),\n        y = churn_bar[var_select].value_counts().values.tolist(),\n        text = churn_bar[var_select].value_counts().values.tolist(),\n        textposition = 'auto',\n        name ='Churn : yes',opacity = 0.8, marker=dict(\n        color = 'rgba(255, 140, 184, 0.5)',\n        line = dict(color='#000000',width=1)))\n\n    trace2 = go.Bar(\n        x = no_churn_bar[var_select].value_counts().keys().tolist(),\n        y = no_churn_bar[var_select].value_counts().values.tolist(),\n        text = no_churn_bar[var_select].value_counts().values.tolist(),\n        textposition = 'auto',\n        name = 'Churn : no', opacity = 0.8, marker=dict(\n        color = 'rgba(93, 164, 214, 0.5)',\n        line = dict(color='#000000',width=1)))\n    \n    trace3 =  go.Scatter(   \n        x = churn_rate.index,\n        y = churn_rate['Attr%'],\n        yaxis = 'y2',\n        name = '% Churn', opacity = 0.6, marker=dict(\n        color = 'black',\n        line = dict(color='#000000',width=0.5\n        )))\n\n    layout = dict(title =  str(var_select),  autosize = False,\n                        height  = 500,\n                        width   = 800,\n                        xaxis = dict(), \n                        yaxis = dict(title= 'Count'), \n                        yaxis2 = dict(range= [-0, 75], \n                        overlaying = 'y', \n                        anchor = 'x', \n                        side = 'right',\n                        zeroline = False,\n                        showgrid = False, \n                        title = '% Churn'\n                         ))\n\n    fig = go.Figure(data=[trace1, trace2, trace3], layout = layout)\n    fig.update_layout(legend=dict(\n    y = 0.99,\n    xanchor = \"right\",\n    x = 1.3\n))\n    py.iplot(fig)","17cf1aa4":"## PLOT THE DEMOGRAPHIC-RELATED ATTRIBUTES AGAINST THE TARGET CLASS ATTRIBUTE\nbarplot('gender', True)\nbarplot('SeniorCitizen', True)\nbarplot('Dependents', True)\nbarplot('Partner', True)","8b811ae4":"## PLOT THE SERVICE-RELATED ATTRIBUTES AGAINST THE TARGET CLASS ATTRIBUTE\nbarplot('PhoneService', True)\nbarplot('MultipleLines', True)\nbarplot('InternetService', True)\nbarplot('OnlineSecurity', True)\nbarplot('OnlineBackup', True)\nbarplot('DeviceProtection', True)\nbarplot('TechSupport', True)\nbarplot('StreamingTV', True)\nbarplot('StreamingMovies', True)","f915549d":"# PLOT BILLING-RELATED ATTRIBUTES AGAINST THE TARGET CLASS ATTRIBUTE\nbarplot('PaperlessBilling', True)\nbarplot('PaymentMethod', True)","e864daf4":"# PLOT THE CONTRACT AGAINST THE TARGET CLASS ATTRIBUTE\nbarplot('Contract', True)","bfb1b3e5":"#PLOT BOXPLOT TO EXPLORE CONTRACT, MONTHLY CHARGES & THE TARGET ATTRIBTE\n\nfig = px.box(df_churn, x='Contract', y='MonthlyCharges', color='Churn',\n         color_discrete_sequence=['cornflowerblue','plum'])\n\nfig.update_layout(width=800,\n                  height=400,                  \n                  title=dict(text= 'Contract & Monthly Charges VS the Target Attribute',x=0.5))\n\nfig","e167b276":"# PLOT KERNEL DENSITY ESTIMATE TO EXPLORE THE DISTRIBUTION\nacc_num_features = ['MonthlyCharges','TotalCharges','tenure']\n\nplt.figure(figsize=(10,12))\n\nfor i,col in enumerate(acc_num_features):    \n    plt.subplot(3,1,i + 1)  \n    ax = sns.kdeplot(df_churn.loc[(df_churn['Churn'] == 1), col], label = 'Churn',color = 'plum',shade = True)\n    ax = sns.kdeplot(df_churn.loc[(df_churn['Churn'] == 0), col], label = 'No Churn',color='cornflowerblue',\n                     shade = True)\n    ax.legend([\"Churn\",\"Not Churn\"]);\n    \nplt.show()","be604bc9":"## USE ORDINALECODER TO ENCODE ALL CATEGORICAL ATTRIBUTES. \nfrom sklearn.preprocessing import OrdinalEncoder\noe = OrdinalEncoder()\n\ndf_churn[['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', \n         'InternetService','OnlineSecurity','OnlineBackup','DeviceProtection',\n         'TechSupport','StreamingTV','StreamingMovies','Contract','PaperlessBilling','PaymentMethod']]= oe.fit_transform(df_churn[['gender', \n                                                                                                                                   'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','Contract','PaperlessBilling','PaymentMethod']])\ndf_churn[['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','Contract','PaperlessBilling','PaymentMethod']]= df_churn[['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','Contract','PaperlessBilling','PaymentMethod']].astype(int)\n\nprint('Features after encoding')\ndf_churn.head()","84ef4d94":"## PLOT HEATMAP FOR ALL FEATURES\nplt.figure(figsize=(25, 10))\n\ncorr = df_churn.apply(lambda x: pd.factorize(x)[0]).corr()\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nax = sns.heatmap(corr, mask = mask, xticklabels = corr.columns, yticklabels = corr.columns, cmap = 'RdPu',\n                 annot = True, linewidths = .2)","8b97e211":"## FIND THE FEATURE IMPORTANCE \nfrom sklearn.ensemble import RandomForestClassifier\nimport plotly.graph_objects as go\n\nX = df_churn.drop(columns=['Churn'])\ny = df_churn['Churn']\nclf = RandomForestClassifier(n_estimators = 100,random_state = 0).fit(X,y)\n\nimpFeature = pd.Series(data = clf.feature_importances_, index = X.columns).sort_values(ascending = False)\n\nfig = go.Figure(go.Bar(\n            x = impFeature.values,\n            y = impFeature.index,\n            orientation = 'h'))\n\nfig.update_layout({\"title\": {\"text\": \"Feature Importance using Random Forest Classifier\",\n                             \"font\": {\"size\": 20}}})\nfig.show()","9af2a092":"### PRINT OUT THE IMPORTANCE FIGURES\nimpFeature","fd94885e":"## USE STANDARD SCALER IN SKLEARN LIBRARY TO STANDARDISE FEATURES BEFORE MODELLING\nfrom sklearn.preprocessing import StandardScaler\nsScaler = StandardScaler()\n\nstd_cols = ['gender','SeniorCitizen','Partner', 'Dependents','tenure', 'MultipleLines', \n            'InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport',\n            'StreamingTV','StreamingMovies','Contract','PaperlessBilling','PaymentMethod','MonthlyCharges',\n            'TotalCharges']\nstd_X = sScaler.fit_transform(df_churn[std_cols])\n\nstd_X = pd.DataFrame(std_X, columns = std_cols)\n\nstd_X.mean(axis=0)\n\nstd_X.std\nnp.mean(std_X, axis = 0)\nnp.var(std_X, axis = 0)\n\n\nstd_X.head()# PRINT OUT FIRST 5 ROWS AFTER STANDARDISATION","b9c605e5":"from sklearn.model_selection import train_test_split\n\nX = std_X\ny = df_churn['Churn']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 1,stratify = y)\n\nprint ('X_train:') \n\nprint (X_train.head())\nprint(\"*\"*100)\nprint ('y_train:')\n\nprint (y_train.head())\nprint(\"*\"*100)\nprint ('X_test:')\n\nprint (X_test.head())\n\nprint ('y_test:')\nprint(\"*\"*100)\nprint (y_test.head())","5a21eb9f":"# IMPORT CLASSIFERS\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# IMPORT VALIDATION & GRID SEARCH LIBRARIES\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\n\n# INITIALISE FOLD \nkfold = StratifiedKFold(n_splits = 10, random_state = 128, shuffle = True)","4245833b":"print(\"This might take a while, please wait patiently...\")\n\n#TEST THE PERFORMANCE BEFORE CROSS VALIDATION & HYPER-PARAMETER TUNING \ndt_model = DecisionTreeClassifier(random_state = 11)\n\ndt_train = dt_model.fit(X_train,y_train)\n\ny_dtpredict = dt_train.predict(X_test)\n\nacc_dt_original = accuracy_score(y_test,y_dtpredict)\n\nprint(\"Model accuracy before cross validation & hyper-parameter tuning\",acc_dt_original.round(5))\n\n\n#IMPLEMENT GRID SEARCH TO FIND THE BEST HYPER-PARAMETERS\n\ngrid_dt = {'max_depth':range(1,15,2),'min_samples_split':range(10,70,5)}\n\nbest_dt_model = GridSearchCV(estimator = dt_model,cv = kfold, param_grid = grid_dt, n_jobs = 10,\n                             return_train_score = True, scoring = 'accuracy')\n\nbest_dt_model.fit(X_train, y_train)\n\nprint('Best parameters for Decision Tree:', best_dt_model.best_params_)\n\nprint('The Accuracy of Decision Tree Model after hyper-parameters tuning and cross validation: ',\n      (best_dt_model.best_score_).round(5))\n\n#TEST THE MODEL PERFORMANCE ON TEST SET\n\ndt_y_predict = best_dt_model.predict(X_test)\n\nacc_dt_test = accuracy_score(y_test,dt_y_predict)\nprint(\"Decision Tree accuracy score on test data after parameter tuning and 10-Fold cross validation: \", \n      acc_dt_test.round(5))\n\n#GET THE BEST PARAMETERS\nbest_max_depth_dt = best_dt_model.best_params_.get('max_depth')\nbest_min_samples_spilt_dt = best_dt_model.best_params_.get('min_samples_split')","1e7ee919":"print(\"This might take a while, please wait patiently...\")\n\n#TEST THE PERFORMANCE BEFORE CROSS VALIDATION & HYPER-PARAMETER TUNING \nrf_model = RandomForestClassifier(random_state = 12)\n\nrf_train = rf_model.fit(X_train,y_train)\n\ny_rfpredict = rf_train.predict(X_test)\n\nacc_rf_original = accuracy_score(y_test,y_rfpredict)\n\nprint(\"Model accuracy before cross validation & hyper-parameter tuning\",acc_rf_original.round(5))\n\n\n#IMPLEMENT GRID SEARCH TO FIND THE BEST HYPER-PARAMETERS\n\ngrid_rf = {'max_depth':range(1,15,2),'n_estimators':range(100,600,100)}\n\nbest_rf_model = GridSearchCV(estimator = rf_model, cv = kfold, param_grid = grid_rf, n_jobs = 10, return_train_score = True, scoring = 'accuracy')\n\nbest_rf_model.fit(X_train, y_train)\n\nprint('Best parameters for Random Forest:', best_rf_model.best_params_)\n\nprint('The Accuracy of Random Forest Model after hyper-parameters tuning and cross validation: ',\n      (best_rf_model.best_score_).round(5))\n\n#TEST THE MODEL PERFORMANCE ON TEST SET\n\nrf_y_predict = best_rf_model.predict(X_test)\n\nacc_rf_test = accuracy_score(y_test,rf_y_predict)\nprint(\"Random Forest accuracy score on test data after parameter tuning and training with 10-Fold cross validation: \", acc_rf_test.round(5))\n\n#GET THE BEST PARAMETERS\nbest_max_depth_rf = best_rf_model.best_params_.get('max_depth')\nbest_n_estimators_rf = best_rf_model.best_params_.get('n_estimators')","0b394d28":"print(\"This might take a while, please wait patiently...\")\n\n#TEST THE PERFORMANCE BEFORE CROSS VALIDATION & HYPER-PARAMETER TUNING \nada_model = AdaBoostClassifier(random_state = 13)\n\nada_train = ada_model.fit(X_train,y_train)\n\ny_adapredict = ada_train.predict(X_test)\n\nacc_ada_original = accuracy_score(y_test,y_adapredict)\n\nprint(\"Model accuracy before cross validation & hyper-parameter tuning\",acc_ada_original.round(5))\n\n#IMPLEMENT GRID SEARCH TO FIND THE BEST HYPER-PARAMETERS\n\ngrid_ada = {'n_estimators':range(100,600,100)}\n\nbest_ada_model = GridSearchCV(estimator = ada_model, cv = kfold, param_grid = grid_ada, n_jobs = 10, return_train_score = True, scoring = 'accuracy')\n\nbest_ada_model.fit(X_train, y_train)\n\nprint('Best parameters for Adaboost :', best_ada_model.best_params_)\n\nprint('The Accuracy of Adaboost Model after hyper-parameters tuning and cross validation: ',\n      (best_ada_model.best_score_).round(5))\n\n#TEST THE MODEL PERFORMANCE ON TEST SET\n\nada_y_predict = best_ada_model.predict(X_test)\n\nacc_ada_test = accuracy_score(y_test,ada_y_predict)\n\nprint(\"Adaboost accuracy score on test data after parameter tuning and training with 10-Fold cross validation: \", acc_ada_test.round(5))\n\n#GET THE BEST PARAMETERS\nbest_n_estimators_ada = best_ada_model.best_params_.get('n_estimators')","c9d18332":"# IMPORT LIBRARIES\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score \nfrom sklearn.metrics import f1_score \nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn import metrics","85281e6c":"#CONFUSION MATRIX FOR DECISION TREE\ncm_dt = confusion_matrix(y_test,dt_y_predict)\n\n#PLOT THE MATRIX\n\nimport plotly.figure_factory as ff\nx = ['No Churn', 'Churn']\ny = ['No Churn','Churn']\nfig = ff.create_annotated_heatmap(cm_dt, x=x,y=y)\nfig.update_layout(height=400, width=500,title_text=\"Decision Tree Confusion Matrix\")\nfig.show()\n\n\n#CONFUSION MATRIX FOR RANDOM FOREST \ncm_rf = confusion_matrix(y_test,rf_y_predict)\n\n#PLOT THE MATRIX\n\nimport plotly.figure_factory as ff\nx = ['No Churn', 'Churn']\ny = ['No Churn','Churn']\n\nfig = ff.create_annotated_heatmap(cm_rf, x = x,y = y)\n\nfig.update_layout(height = 400, width = 500,title_text = \"Random Forest Confusion Matrix\")\n\nfig.show()\n\n\n#CONFUSION MATRIX FOR ADABOOST \ncm_ada = confusion_matrix(y_test,ada_y_predict)\n\n#PLOT THE MATRIX\n\nimport plotly.figure_factory as ff\nx = ['No Churn', 'Churn']\ny = ['No Churn','Churn']\n\nfig = ff.create_annotated_heatmap(cm_ada, x = x,y = y)\n\nfig.update_layout(height = 400, width = 500,title_text = \"Adaboost Confusion Matrix\")\n\nfig.show()","fada65e8":"evaluation_summary= []\ncols = ['Accuracy', 'Precision', 'F1', 'Recall']\nclassifiers_names = ['Decision Tree','Random Forest','Adaboost']\nmodels = [DecisionTreeClassifier(max_depth = best_max_depth_dt, min_samples_split = best_min_samples_spilt_dt),\n          RandomForestClassifier(max_depth = best_max_depth_rf, n_estimators = best_n_estimators_rf),\n          AdaBoostClassifier(n_estimators = best_n_estimators_ada)]\n\nfor i in models:\n    each_summary = []\n    model = i\n    model.fit(X_train,y_train)\n    prediction = model.predict(X_test)\n    each_summary.append(metrics.accuracy_score(y_test,prediction).round(5))\n    each_summary.append(metrics.precision_score(y_test,prediction).round(5))\n    each_summary.append(metrics.f1_score(y_test,prediction).round(5))\n    each_summary.append(metrics.recall_score(y_test,prediction).round(5))\n    \n    evaluation_summary.append(each_summary)\n    \nfinal_summary = pd.DataFrame(evaluation_summary,index = classifiers_names, columns = cols)\n\nfinal_summary","48d7c815":"# PLOT ROC CURVE\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import plot_roc_curve\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nmodels = [DecisionTreeClassifier(max_depth = best_max_depth_dt, min_samples_split = best_min_samples_spilt_dt),\n          RandomForestClassifier(max_depth = best_max_depth_rf, n_estimators = best_n_estimators_rf),\n          AdaBoostClassifier(n_estimators = best_n_estimators_ada)]\n\nfor model in models:\n    model.fit(X_train,y_train)\n    \nfor i in models:\n    plot_roc_curve(i, X_test, y_test, ax = ax)\n    plt.legend(fontsize = 12)\n    plt.title('ROC CURVE',fontsize = 18)\n    \nplt.show()","b7a02c90":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nax = plt.figure(figsize = (10, 10))\nax = plt.gca()\n\nfor model in models:\n    model.fit(X_train,y_train)\n\nfor i in models:\n    plot_precision_recall_curve(i, X_test, y_test,ax = ax)\n    plt.legend(fontsize = 12)\n    plt.title('PRECISION_RECALL CURVE',fontsize = 18)\n\nplt.show()\n","84a3fe19":"**Findings**\n\n1) Customers with paperless billing are twice as likely to churn as those without.\n\n2) Interestingly, the churn rate is high for customers who pay with electronic check. It is about 2.4 times higher than mailed checks, 2.6 times higher than bank transfers, and 3 times higher than credit cards.","867badb1":"<a id = \"4\" ><\/a>\n### <b>2.2 Loading Data<\/b>","bd4ce453":"<a id = \"24\" ><\/a>\n# <span style=\"font-family:arial; font-size:28px;\"> 5. DATA PARTITIONING<\/span>","93ad20e4":"<a id = \"2\" ><\/a>\n# <span style=\"font-family:arial; font-size:28px;\"> 2. DATA PREPARATION<\/span>\n\n<a id = \"3\" ><\/a>\n### <b>2.1 Importing libraries for data preparation<\/b>","7d11a4eb":"<a id = \"32\" ><\/a>\n### <b>7.3 ROC CURVE<\/b> ","e0d5f01f":"Bar & line charts, boxplots, and kernel density estimate charts were created for data analysis using tools from the plotly and seaborn library. \n\nThe visualisations will show:\n\n* The number for churning customers and staying customers as well as churn rate for the contract attribute \n* The boxplot for contract & monthly charges against the churn\n* The distribution of \u201ctenure\u201d, \u201cMonthlyCharges\u201d and \u201cTotalCharges\u201d by considering the churn","b2c3aa25":"<a id = \"33\" ><\/a>\n### <b>7.4 PRECISON-RECALL CURVE<\/b> ","29c3d449":"<a id = \"18\" ><\/a>\n### <b>3.7 Exploring the relationship between the billing-related attributes and the target class attribute  <\/b>\n* PaperlessBilling \n* PaymentMethod ","12768d12":"**Findings**\n\nAccording to the previous EDA, the heatmap and the feature importance chart, it was decided to exclude the phone service attribute as it had little relationship to the target attribute.","f34b02f4":"<a id=\"toc\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n\n* [Data Description](#1)\n\n\n* [Data Preparation](#2)\n    * [Importing Python libraries for data preparation](#3)\n    * [Loading Data](#4)\n    * [Data Checking against the raw data](#5)\n    * [Statistics Exploration](#34)\n    * [Checking Missing values](#6)\n    * [Imputing Missing values](#7)\n    * [Checking format consistency for categorical attributes](#8)\n    * [Checking Duplicated Rows](#9)\n    * [Checking Outliers for numeric attributes](#10)\n    \n    \n* [Data Exploration](#11)\n\n    * [Exploring the Target Attribute](#12)\n    * [Exploring Correlations between numeric attributes](#13)\n    * [Feature Encoding for Exploration Purpose](#14)\n    * [Exploring correlations between all input attributes and the target attribute](#15)\n    * [Exploring the relationship between demographic-related attributes and the target class attribute](#16)\n    * [Exploring the relationship between the service-related attributes and the target class attribute](#17)\n    * [Exploring the relationship between the billing-related attributes and the target class attribute](#18)\n    * [Exploring the relationship between the account-related attributes and the target class attribute](#19)\n\n\n* [Feature Engineering](#20)\n\n    * [Feature Encoding Before Modelling](#21)\n    * [Feature Selection](#22)\n    * [Feature Standardisation](#23)\n\n\n* [Data Partitioning](#24)\n\n    \n* [Modelling](#25)\n    * [Decision Tree](#26)\n    * [Random Forest](#27)\n    * [Adaboost](#28)\n     \n    \n* [Model Evaluation](#29)\n    * [Confusion Matrix](#30)\n    * [Performance Summary ](#31)\n    * [ROC CURVE](#32)\n    * [PRECISION-RECALL CURVE](#33)\n    \n\n* [Conclusion](#35)","a94055f2":"<a id = \"8\" ><\/a>\n### <b>2.7 Checking format consistency for categorical attributes<\/b>","5c1d5b84":"Three tree-based models were selected. Cross-validation - StratifiedKfold and hyper-parameter tuning were used in the modelling process to avoid overfitting. ","ebf6334c":"**Findings:**\n\n1) Only a small percentage of customers do not have phone service.\n\n2) The churn rate between customers with and without phone service is quite similar. Therefore, it is considered to drop the phone service attribute for modelling.\n\n3) The churn rate between customers with multiple lines and customers without multiple lines is quite similar. Therefore, it is considered to drop the multiple lines attribute for modelling.\n\n4) Compared to DSL internet service, Fiber optic has a churn rate of 42% which is around 2.2 times higher. Therefore, the company needs to keep an eye on this internet service to find out if customers left for service quality or other reasons.\n\n5) Customers with DSL internet service has a low churn rate.\n\n6) Customers with tech support have a low churn rate.\n\n7) Customers with online security have a low churn rate.\n\n8) The churn rate of customers who use services, such as tech support, streaming TV, streaming movies, online backup, device protection, and online services, is lower than the churn rate of customers without these services.","3b75c95c":"Reasons for using decision tree:\n\n* Straightforward\n* Easy to understand, interpret \n* Requrie less computational resources \n* Not affected by correlation between input variables \n\nDownside: \n* Greedy as they tend to stop until all leaves are pure\n* Overfitting \n\nThus, hyper-parameter tuning is a must step. sklearn.model_selection.GridSearchCV is a package from scikit-learn library that was used to identify the best combination of the selected hyperparameters for the models.\n\nHyperparmeters : max_depth and min_samples_split are tuned to avoid the tree going to deep that leads to overfitting.","1fce0456":"<a id = \"31\" ><\/a>\n### <b>7.2 Performance Summary Table <\/b> ","65ca2ea2":"Another common matrix for evaluating the quality of classifier output is the precision-recall metric. It is especially useful when classes are imbalanced. The precision-recall curve illustrates the trade-off precision and recall depending on various threshold. An ideal model is to have a higher area under the curve, which implies high recall and high precision; in another words, low false positive and low false negative rates.","5c26feb1":"**Findings**\n\nBased on the graph, the random forest model and the AdaBoost model tend to have the same performance. However, since our dataset is imbalanced, only using ROC curve may not be sufficient. Thus, it was decided to use another curve \u2013 precision-recall curve to further evaluate the performance.","3f17f0e7":"An ROC curve plots the true positive rate versus the false positive rate for various outcomes and is widely used to test a binary classifier\u2019s diagnostic ability when the discrimination threshold is changed. AUC (area under the ROC curve) is a measure to evaluate the ability of a model to avoid false classification. Therefore, the higher the AUC the better the model performs. ","81b65d04":"<a id = \"26\" ><\/a>\n### <b>6.1 Decision Tree<\/b>","11ef53f2":"Based on the outcomes, \u201cTotalCharges\u201d contains 11 rows of missing values, which is around 0.156% of the total rows. Besides, by observing these 11 rows all have tenure values equal to 0 and with No churn Label. This may imply that these customers may have been with the company less than a month, so no charges have yet been occurred. Therefore, it is decided to impute these missing values using median of the monthly charges, which is more suitable than using mean values of the total charges.","2f7ed555":"Several techniques and methods have been applied across the entire supervised machine learning process and used machine learning models to predict an unbalanced binary classification problem. \n\nData exploration was performed across 20 features and the target variable (churn).  A feature engineering process was applied including feature encoding, feature selection, and feature standardization.\n\nAs the dataset is unbalanced, stratified sampling was also used when splitting the data set into training and testing data, in order to ensure the minority target class was appropriately represented in the training and testing data sets. \n\nThree tree-based machine learning classification models were developed, trained using 10-fold stratified cross validation to help minimise the chance of over or underfitting, and underwent hyperparameter tuning, in order to try and find an optimal model which had the highest predictive power with respect to predicting customer churn.  \n\nStratified 10-fold cross validation and hyperparameter tuning were used to optimise the models. To better understand the performance of the models, confusion matrices, classification accuracy, precision, recall, F1-score, ROC curve and Precision-Recall curve were applied. \n\nIt is believed that overall, the random forest model performs the best. However, the final decision still needs to be made from the business point of view as there may be some limitations of this model, for instance, longer training time, hard to interpret, requires high computational power, etc.","06d6eef8":"<a id = \"5\" ><\/a>\n### <b>2.3 Data Checking against the raw data<\/b>","749fe893":"<a id = \"35\" ><\/a>\n# <span style=\"font-family:arial; font-size:28px;\"> 8. CONCLUSION <\/span>","b16f3cf2":"**Findings**\n\n1) Month-to-month contracts are the most dominant contract for the company.\n\n2) Customers in month-to-month contracts have a high churn rate. Almost four times higher than one-year contract and fourteen times higher than two-year contract.\n\n3) Customers who sign longer contracts are less likely to leave the company.\n\n4) Customers with higher monthly charges are more likely to churn than those with lower charges in all contract types\n\n5) Customers are less likely to churn if they have been with the company for a long period of time.\n\n6) It is interesting to note that the probability of churning decreases after reaching a certain level of total charges.","5d7848d0":"Reasons for using random forest \n* Is an ensemble method that builds unique and uncorrelated trees to overcome the potential of overfitting. \n* Creates unrelated and unique trees using the technique of bagging or bootstrap aggregations\n* Aims to overcome the potential bias and overfitting issues of a single decision tree\n* Each individual tree will be trained parallelly using different training data and features and the final result for a classification problem is based on major vote. \n* It tends to be more reliable\n\nDownsides: \n* Longer training time \n* Difficult to interpret \n* Requires more computational power\n\n\nHyperparmeters : n_estimator, and max_depth are selected to tune to optimise the final performance.\n\nn_estimator: control the number of trees \n\nmax_depth: control the depth of the trees","92b6446f":"<a id = \"21\" ><\/a>\n### <b>4.1 Feature Encoding Before Modelling<\/b>","4555fd65":"Since all categorical attributes have been converted into numeric values, the relationship between the input attributes and the target attribute can be examined. ","3ada7668":"![](http:\/\/www.voxco.com\/wp-content\/uploads\/2021\/09\/Everything-you-need-to-know-about-Customer-Churn1.jpg.webp)","8010cec5":"<a id = \"9\" ><\/a>\n### <b>2.8 Checking Duplicated Rows<\/b>","3bc98465":"The dataset was obtained from Kaggle, which contains around 7000 rows and 21 columns with 17 categorical columns, and 3 numerical columns, and 1 label column. \n1. **`customerID`**: A unique ID that identifies each customer.\n\n2. **`gender`**: The customer\u2019s gender: Female,Male\n\n3. **`SeniorCitizen`**: Indicates if the customer is 65 or older: Yes, No\n\n4. **`Partner`**: Indicates if the customer has partner: Yes, No\n\n5. **`Dependents`**: Indicates if the customer lives with any dependents: Yes, No. Dependents could be children, parents, grandparents, etc.\n\n6. **`tenure`**: Indicates the total amount of months that the customer has been with the company.\n\n7. **`PhoneService`**: Indicates if the customer subscribes to home phone service with the company: Yes, No\n\n8. **`Multiple Lines`**: Indicates if the customer subscribes to multiple telephone lines with the company: Yes, No, No phone service\n\n10. **`Internet Service`**: Indicates if the customer subscribes to Internet service with the company: No, DSL, Fiber optic, No\n\n11. **`Online Security`**: Indicates if the customer subscribes to an additional online security service provided by the company: Yes, No, No internet service\n\n12. **`OnlineBackup`**: Indicates if the customer subscribes to an additional online backup service provided by the company: Yes, No, No internet service\n\n13. **`DeviceProtection`**: Indicates if the customer subscribes to an additional device protection plan for their Internet equipment provided by the company: Yes, No, No internet service\n\n14. **`TechSupport`**: Indicates if the customer subscribes to an additional technical support plan from the company with reduced wait times: Yes, No, No internet service\n\n15. **`StreamingTV`**: Indicates if the customer uses their Internet service to stream television programing from a third party provider: Yes, No, No internet service\n\n16. **`Streaming Movies`**: Indicates if the customer uses their Internet service to stream movies from a third party provider: Yes, No, No internet service\n\n17. **`Contract`**: Indicates the customer\u2019s current contract type: Month-to-Month, One Year, Two Year.\n\n18. **`Paperless Billing`**: Indicates if the customer has chosen paperless billing: Yes, No\n\n19. **`Payment Method`**: Indicates how the customer pays their bill: Bank transfer, Credit card, Electronic check, Mailed Check\n\n20. **`Monthly Charge`**: Indicates the customer\u2019s current total monthly charge for all their services from the company.\n\n21. **`Total Charges`**: Indicates the customer\u2019s total charges, calculated to the end of the quarter specified above.\n\n22. **`Churn`**: Yes = the customer left the company this quarter. No = the customer remained with the company. Directly related to Churn Value.\n\nReference:\nhttps:\/\/www.kaggle.com\/blastchar\/telco-customer-churn","0e79bb45":"<a id = \"27\" ><\/a>\n### <b>6.2 Random Forest<\/b>","2dd2c767":"**Findings**\n* No format inconsistencies have been detected.","5de9dfdf":"<a id = \"1\" ><\/a>\n# <span style=\"font-family:arial; font-size:28px;\"> 1. DATA DESCRIPTION <\/span>","1fd9f28e":"**Findings**\n* No outliers have been found","21212036":"Bar & line charts were created for data analysis using tools from the plotly library. The combination graphs will show the number for churning customers and staying customers as well as the churn rate for the demographic-related attributes","43f7e7a6":"<a id = \"34\" ><\/a>\n### <b>2.4 Statistics Exploration<\/b>","1bc6a9db":"**Findings**\n\nOn the basis of the summary table, it is evident that the random forest model has the highest accuracy score. In light of the imbalanced nature of the data, focusing only on accuracy score is insufficient. As a result, other matrices such as precision, f1 and recall were also considered. It is found that the random forest model has the highest precision score while the Adaboost model has the highest recall and F1 score. Therefore, it is decided to use several curves to make further evaluation.","91116440":"<a id = \"23\" ><\/a>\n### <b>4.3 Feature Standardisation<\/b>","749ff2b8":"* In order to better explore the categorical attributes, they were converted into dummy variables. This would provide a more comprehensive perspective on the attributes.","f0d0bcb4":"**Findings:**\n\n1) Churn rates between male and female customers are similar. Therefore, it is considered to drop the gender attribute for modelling.\n\n2) The churn rate for senior citizens is fairly high (42%) which needs to be given attention.\n\n3) The likelihood of churn for customers with dependents is lower than for customers without dependents.\n\n4) Consumers with partners are less likely to churn than consumers who are single.","b60668bc":"<a id = \"29\" ><\/a>\n# <span style=\"font-family:arial; font-size:28px;\"> 7.MODEL EVALUATION<\/span>","5ec4c1aa":"<a id = \"20\" ><\/a>\n# <span style=\"font-family:arial; font-size:28px;\"> 4. FEATURE ENGINEERING<\/span>","0625e5cc":"Bar & line charts were created for data analysis using tools from the plotly library. The combination graphs will show the number for churning customers and staying customers as well as churn rate for the billing-related attributes","4c52b118":"The main objective for the classification problem is to correctly identify customers who are likely to churn as churning customers so that the company can retain those customers before they leave. Different matrices and curves will be used for evaluation, which will be explained in the following subsections. ","65f39df1":"**Findings:**\n* 73.5% of the data in the dataset is recorded as not churning customers and only 26.5% of the records are churning customers. Therefore, it was concluded that the dataset is moderately imbalanced. Therefore, it was decided to use stratification for the data splitting before modelling so that the proportions for each class are preserved. This can avoid the situation of overfitting.","b82fc1e4":"<a id = \"14\" ><\/a>\n### <b>3.3 Feature Encoding for Exploration purpose<\/b>","edf923a9":"**Findings:**\n\n1) Customers with month-to-month contracts have a relatively high likelihood of churning.\n\n2) Customers without online security or tech support are more likely to churn.\n\n3) Customers with Fiber optic internet service are more likely to churn, which might indicate they may not be satisfied with this service. Reasons could be price, speed, reliability of the service.\n\n4) Customers using electronic checks are more likely to churn than those using other forms of payment.\n\n5) Customers without device protection or online backup are more likely to churn.\n\n6) Long-term customers are more loyal and less likely to churn.\n\n7) Customers without the Internet service or have certain services like streaming movies, device protection, tech support, online backup, and streaming TV but do not have the Internet services are less likely to churn.\n","3c1a8bd8":"An in-depth data analysis was further conducted by dividing the attributes into 4 categories: 1) demographic-related attributes, 2) service-related attributes, 3) billing-related attributes, 4) account-related attributes, and exploring each category against the target attributes. The details are included in the following four subsections.","6612a7b9":"<a id = \"16\" ><\/a>\n### <b>3.5 Exploring the relationship between demographic-related attributes and the target class attribute <\/b>\n* gender\n* SeniorCitizen\n* Partner\n* Dependents ","c237bf3f":"**Findings:**\n\n1) \u201cTotalCharges\u201d and \u201cMonthlyCharges\u201d are moderately correlated with a correlation equals to 0.65.\n\n2) \u201cTotalCharges\u201d and \u201ctenure\u201d are highly correlated with a correlation equals to 0.83.\n\n3) \u201cTotalCharges\u201d is positively skewed\n\n4) Customers who have been with the company below 4 months or above 70 months have the highest frequency.","dea56a81":"A performance summary table, including accuracy, precision, F1 and recall scores was also created for comparison. The accuracy score of a model refers to its ability to correctly identify every class while precision score measures the percentage of correct positive predictions that are correct. Recall is a metric used to determine the proportion of actual positives that were identified correctly, thus indicating missed positive predictions. The F1 score is a weighted average of recall and precision. Therefore, both false negatives and false positives are considered, and it is useful for evaluate uneven distributed target class.","30401bc7":"<a id = \"7\" ><\/a>\n### <b>2.6 Imputing Missing values<\/b>","a9132bf3":"<a id = \"6\" ><\/a>\n### <b>2.5 Checking Missing values<\/b>","cfc10533":"<a id = \"28\" ><\/a>\n### <b>6.3 Adaboost<\/b>","d1235be5":"<a id = \"15\" ><\/a>\n### <b>3.4 Exploring correlations between all input attributes and the target attribute<\/b>","d5bb4a75":"### Project Objective \n\nWith a subscription-based business model, customer retention is one of the key performance indicators for a telecommunication company. Competition is tough in the industry as customers can choose from a variety of providers. Hence, one unhappy experience may give rise to a customer moving to the competitor, resulting in churn. Due to the telco market becoming increasingly saturated, there should be a greater focus on preventing customer churn, as the cost of attracting new subscribers is substantially higher than retaining existing customers. There are two approaches to manage customer churn which consist of reactive and proactive. Therefore, instead of reacting after being abandoned, the objective of the company is to detect in advance, those customers with a considerable risk of churning. This will enable the company to enact appropriate and effective marketing strategies to proactively engage with these customers, noting that different customers may behave differently and have different preferences. By doing this, it is more likely to maximise the likelihood of retaining those customers. As an example, a company could provide some targeted incentives such as exclusive promotional pricing or gift vouchers to those high-risk customers, in order to keep them for another one or two years and extend their lifetime value to the company. This could lead to significant savings for the company and may contribute to the company\u2019s potential growth.\n\n**Business problem**:\n\nTo meet the objective above, it is important to understand and more importantly, be able to model, the factors that are going to have an impact on the churn? For instance:\n\n* How do different contract types impact the churn?  \n\n* Does the gender of the customer affect the churn rate?  \n\n* Do senior citizens have more or less likelihood of churn?  \n\n* Do customers with dependents (family) have more or less likelihood to churn?  \n\n* Are there any service offerings which the company needs to pay more attention to?\n\n* Is there any relationship between churn and monthly charges?  \n\n* Is a longer term customer more loyal, or more likely to churn than a new customer?\n\n* Is there any relationship between method of payment and churn? \n\nThe **scope** of this project is to find the relationships between the available input attributes and whether the customer churned, and select the most important attributes that can be utilised by supervised machine learning models and find the best model to predict if a customer will **STAY** or **LEAVE** the company, so the company can take corrective action.","3963d393":"**Findings**\n\nBased on the graph, the random forest model tends to have a better performance. ","5fdd7c3f":"### END OF THE NOTEBOOK :D","258c2e10":"Confusion matrix is commonly used to summarise the prediction results on a classification problem. It identifies the number of true positive, true negative, false positive, and false negative. For the case of this project, the best model should be the one with high true positive rates and low false negative rate.","6bc95577":"<a id = \"17\" ><\/a>\n### <b>3.6 Exploring the relationship between the service-related attributes and the target class attribute  <\/b>\n* PhoneService\n* MultipleLines\n* InternetService\n* OnlineSecurity\n* OnlineBackup\n* DeviceProtection\n* TechSupport \n* StreamingTV\n* StreamingMovies","229be7af":"Reasons for using Adaboost:\n* Ensemble learning algorithm \n* Unlike random forest it works sequentially rather than in a parallel manner. \n* It aims to convert the weak learners to strong leaners by learning from the mistakes from previous models\n* The results of the prediction are analysed for each model, and any data points that were classified incorrectly are given higher weight. \n* A weighting is also assigned to the tree based on its error rate, and the lower the error rate, the higher the voting weight.\n* More reliable and less likely to overfit \n\nDownsides:\n* Requires more processing time \n* Requires more computational power\n\nn_estimator is the most important hyperparameter for AdaBoost in order to control the number of models to train iteratively, so it was tuned to optimise the model performance.","64229394":"**Findings**\n\nBased on the confusion matrix, the AdaBoost model performs the best in terms of false negative rate while the random forest model performs the best in terms of true positive rate.","f9c1b7c8":"<a id = \"11\" ><\/a>\n# <span style=\"font-family:arial; font-size:28px;\"> 3. DATA EXPLORATION & VISUALISATION<\/span>\n\n<a id = \"12\" ><\/a>\n### <b>3.1 Exploring the Target Attribute<\/b>","028457aa":"<a id = \"19\" ><\/a>\n### <b>3.8 Exploring the relationship between the account-related attributes and the target class attribute  <\/b>\n* contract\n* monthly charges\n* total charges\n* tenure","c5e46d78":"<a id = \"25\" ><\/a>\n# <span style=\"font-family:arial; font-size:28px;\"> 6. MODELLING<\/span>","d9bad749":"<a id = \"10\" ><\/a>\n### <b>2.9 Checking Outliers for numeric attributes<\/b>","fc7cc004":"<a id = \"30\" ><\/a>\n### <b>7.1 Confusion Matrix<\/b>","3f4e5112":"<a id = \"13\" ><\/a>\n### <b>3.2 Exploring Correlations between numeric attributes<\/b>","37ac6dcf":"<a id = \"22\" ><\/a>\n### <b>4.2 Feature Selection<\/b>","1f426085":"Bar & line charts were created for data analysis using tools from the plotly library. The combination graphs will show the number for churning customers and staying customers as well as churn rate for the service-related attributes"}}