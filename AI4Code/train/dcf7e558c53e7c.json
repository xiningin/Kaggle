{"cell_type":{"5da2243d":"code","8b4af9b3":"code","9e635d14":"code","5adfa8d1":"code","a921361d":"code","8fb9ba24":"code","e1ed9bfc":"code","ff3fc4e1":"code","f39b31ab":"code","0bb1c4a2":"code","8bdf0b80":"code","018a92cb":"code","d9ab7d5b":"code","d661d640":"code","6ce25c1a":"code","ef151713":"code","70247eb4":"code","9e83a998":"code","9ce3ebaa":"code","7841988d":"code","2b18f9b5":"code","c38a7663":"code","bce92b2a":"code","73d0aee3":"code","9f6f7732":"code","9a0ad5e4":"code","e070f6a9":"code","c3f87ecd":"code","d1d2d7a5":"code","57bb5115":"code","b2290671":"code","efb00c65":"code","091663b8":"code","59aa8618":"code","9c515fde":"code","8c6df123":"code","a393c3f8":"code","d8861d3f":"markdown","f1d56412":"markdown","47b23d95":"markdown","05d637bd":"markdown","df50cec5":"markdown","0130b539":"markdown","7f2d8027":"markdown","bd9a542a":"markdown","c7e81a65":"markdown","7b3d0bf9":"markdown","20c84ce4":"markdown","a9980f37":"markdown","4da6d374":"markdown"},"source":{"5da2243d":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error,classification_report, confusion_matrix\nimport keras \nfrom keras.layers import Dense\nfrom keras.models import Sequential\n\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))","8b4af9b3":"train = pd.read_csv(\"..\/input\/train.csv\")\ntrain.tail()","9e635d14":"test = pd.read_csv(\"..\/input\/test.csv\")\ntest.tail()","5adfa8d1":"sns.heatmap(train.isnull(),yticklabels=False,cmap='viridis')","a921361d":"# We look at the number of missing elements in descending order\ntrain.isnull().sum().sort_values(ascending=False)","8fb9ba24":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=train,palette='rainbow')","e1ed9bfc":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=train,palette='RdBu_r')","ff3fc4e1":"plt.figure(figsize=(12,7))\ntrain['Age'].hist(bins=40,alpha=0.9,color='green')","f39b31ab":"#As we have the problem with the age we must find a way to answer to this situation\nplt.figure(figsize=(12,7))\nsns.boxplot(x='Pclass',y='Age',data=train,palette='seismic')","0bb1c4a2":"def About_age(cols):\n    Age=cols[0]\n    Pclass=cols[1]\n    if pd.isnull(Age):\n        if Pclass==1:\n            return 37\n        elif Pclass==2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age","8bdf0b80":"train['Age']=train[['Age','Pclass']].apply(About_age, axis=1)\ntrain['Embarked'].fillna('S')","018a92cb":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","d9ab7d5b":"#We drop the cabin class\ntrain.drop(\"Cabin\",axis=1,inplace=True)","d661d640":"#Let us show the dataframe \ntrain.head()","6ce25c1a":"#Le us check informations of our data\n\ntrain.info()","ef151713":"# Coiverting categoritical feature \n\nsex= pd.get_dummies(train['Sex'],drop_first=True)\nembark=pd.get_dummies(train['Embarked'], drop_first=True)\ntrain.drop(['Sex','Embarked','Name','Ticket'], axis=1,inplace=True)","70247eb4":"train.head()","9e83a998":"train=pd.concat([train,sex,embark], axis=1)\ntrain.head()","9ce3ebaa":"train.describe()","7841988d":"#create class\nclass Titanic_model:\n    \n    def __init__(self, data=None, cols=None, name='Survived'):\n        \n        self.name = name # target\n        self.data = data # feature\n        self.cols = cols # feature columns name\n        self.listof_model = {'LinearRegression': LinearRegression(), \n                'KNeighborsRegression':KNeighborsRegressor(),\n                'RandomForestRegression': RandomForestRegressor(),\n               'GradientBoostingRegression': GradientBoostingRegressor(),\n                'XGBoostRegression': XGBRegressor(),\n                'adaboost':AdaBoostRegressor()} # list of different learner\n    \n    #Read csv file\n    def read(self, file):\n        return pd.read_csv(file)\n    \n    def multi_categorical_plot(self, data):\n    \n        \"\"\" plot a categorical feature\n        \n            data: float64 array  n_observationxn_feature\n        \n        \"\"\"\n        # Find a feature that type is object\n        string = []\n        for i in data.columns:\n            if data[i].dtypes == \"object\":\n                string.append(i)\n    \n        fig = plt.figure(figsize=(20,5))\n        fig.subplots_adjust(wspace=0.2, hspace = 0.3)\n        for i in range(1,len(string)+1):\n            ax = fig.add_subplot(2,3,i)\n            sns.countplot(x=string[i-1], data=data, ax=ax)\n            ax.set_title(f\" {string[i-1]} countplot\")\n            \n    def distplot_multi(self, data):\n        \"\"\" plot multi distplot\"\"\"\n    \n        \n        from scipy.stats import norm\n        cols = []\n        \n        #Feature that is int64 or float64 type \n        for i in data.columns:\n            if data[i].dtypes == \"float64\" or data[i].dtypes == 'int64':\n                cols.append(i)\n        \n        gp = plt.figure(figsize=(15,10))\n        gp.subplots_adjust(wspace=0.4, hspace=0.4)\n        for i in range(1, len(cols)+1):\n            ax = gp.add_subplot(2,3,i)\n            sns.distplot(data[cols[i-1]], fit=norm, kde=False)\n            ax.set_title('{} max. likelihood gaussian'.format(cols[i-1]))\n            \n    def boxplot_multi(self, data):\n        \n        \"\"\" plot multi box plot\n            hue for plotting categorical data\n        \"\"\"\n    \n        cols = []\n        for i in data.columns:\n            if data[i].dtypes == \"float64\" or data[i].dtypes == 'int64':\n                cols.append(i)\n    \n        gp = plt.figure(figsize=(15,10))\n        gp.subplots_adjust(wspace=0.4, hspace=0.4)\n        for i in range(1, len(cols)+1):\n            ax = gp.add_subplot(2,3,i)\n            sns.boxplot(x = cols[i-1], data=data)\n            ax.set_title('Boxplot for {}'.format(cols[i-1]))\n            \n    def correlation_plot(self, data, vrs= 'Survived'):\n    \n        \"\"\"\n        This function plot only a variable that are correlated with a target  \n        \n            data: array m_observation x n_feature\n            vrs:  target feature (n_observation, )\n            cols: interested features\n        \"\"\"\n        \n        cols = []\n        for i in data.columns:\n            if data[i].dtypes == \"float64\" or data[i].dtypes == 'int64':\n                cols.append(i)\n                \n        feat = list(set(cols) - set([vrs]))\n    \n        fig = plt.figure(figsize=(15,10))\n        fig.subplots_adjust(wspace = 0.3, hspace = 0.25)\n        for i in range(1,len(feat)+1):\n        \n            gp = data.groupby(feat[i-1]).agg('mean').reset_index()\n        \n            if len(feat) < 3:\n                ax = fig.add_subplot(1,3,i)\n            else:\n                n = len(feat)\/\/2 + 1\n                ax = fig.add_subplot(2,n,i)\n            \n            ax.scatter(data[feat[i-1]], data[vrs], alpha=.25)\n            ax.plot(gp[feat[i-1]], gp[vrs], 'r-', label='mean',  linewidth=1.5)\n            ax.set_xlabel(feat[i-1])\n            ax.set_ylabel(vrs)\n            ax.set_title('Plotting data {0} vs {1}'.format(vrs, feat[i-1]))\n            ax.legend(loc='best')\n            \n    # Standardize data\n    def standardize(self, data):\n        data = (data - data.mean())\/data.std()\n        return data\n            \n            \n    def VIF(self, data):\n        \"\"\" \n        This function compute variance inflation factor for data that all feature are multicolinear\n        \n        if the outcome is 1, it is okay\n        if it is between 1 and 5, it shows low to average colinearity, and above 5 generally means highly \n        redundant and variable should be dropped\n        \"\"\" \n        # Apply the standardize method to each feature and save it to a new data\n        std_data = data.apply(self.standardize, axis=0)\n    \n        from statsmodels.stats.outliers_influence import variance_inflation_factor\n    \n        vif = pd.DataFrame()\n        vif['VIF_FACTOR'] = [variance_inflation_factor(std_data.values, i) for i in range(std_data.shape[1])]\n    \n        vif['feature'] = std_data.columns\n    \n        return vif\n    \n    \n    def split_data(self):\n        \"\"\"\n        This function splits data to train set and target set\n        \n        data: matrix feature n_observation x n_feature dimension\n        name: target  (n_observation, )\n        cols: interested feature\n        \n        return xtrain, xtest, ytrain, ytest\n        \"\"\"\n    \n        train = self.data[self.cols]\n        target = self.data[self.name]\n    \n        return train_test_split(train, target, random_state=42, test_size=0.2, shuffle=True)\n    \n    def spearman_pearson_correlation(self, data):\n        \n        \n        gp = plt.figure(figsize=(15,5))\n        cols = ['pearson', 'spearman']\n        gp.subplots_adjust(wspace=0.4, hspace=0.4)\n        for i in range(1, len(cols)+1):\n            ax = gp.add_subplot(1,2,i)\n            sns.heatmap(data.corr(method=cols[i-1]), annot=True)\n            ax.set_title('{} correlation'.format(cols[i-1]))\n        \n        \n        plt.show()\n    \n    \n    def learner_selection(self):\n\n        \"\"\"\n            This function compute differents score measure like cross validation,\n            r2, root mean squared error and mean absolute error.\n            listof_model: dictionary type containing different model algorithm.     \n        \"\"\" \n    \n        result = {}\n        \n        x, _, y, _ = self.split_data() # take only xtrain and ytrain\n    \n        for cm in list(self.listof_model.items()):\n        \n            name = cm[0]\n            model = cm[1]\n        \n            cvs = cross_val_score(model, x, y, cv=10).mean()\n            ypred = cross_val_predict(model, x, y, cv=10)\n            r2 = r2_score(y, ypred)\n            mse = mean_squared_error(y, ypred)\n            mae = mean_absolute_error(y, ypred)\n            rmse = np.sqrt(mse)\n        \n            result[name] = {'cross_val_score': cvs, 'rmse': rmse, 'mae': mae, 'r2': r2}\n        \n            print('{} model done !!!'.format(name))\n        \n        \n        return pd.DataFrame(result)\n    \n    \n    def training_evaluate(self, algorithm):\n        \n        \"\"\"This function train and evaluate our model to find r2, rmse and mae\"\"\"\n        \n        result = {}\n        xtrain, xtest, ytrain, ytest = self.split_data()\n        \n        learner = self.listof_model[algorithm] # learner selected in model_selection function\n        \n        model = learner.fit(xtrain, ytrain)\n        ypred = model.predict(xtest)\n        \n        r2 = learner.score(xtest, ytest)\n        rmse =  np.sqrt(mean_squared_error(ytest, ypred))\n        mae = mean_absolute_error(ytest, ypred)\n        \n        result['car price measure'] = {'r2':round(r2, 3),  'rmse':round(rmse, 3), 'mae':round(mae, 3)}\n        \n        return  pd.DataFrame(result)\n","2b18f9b5":"model=Titanic_model()","c38a7663":"model.spearman_pearson_correlation(train)","bce92b2a":"model.correlation_plot(train)","73d0aee3":"train_cols = ['Parch', 'Pclass', 'SibSp','Fare','Age','PassengerId'] #take columns","9f6f7732":"model.VIF(train[train_cols])","9a0ad5e4":"train_model = Titanic_model(data=train, cols=train_cols) #select best algorithm","e070f6a9":"train_model.learner_selection()","c3f87ecd":"X_train, X_test, y_train, y_test=train_test_split(train.drop(['Survived'], axis=1), train['Survived'], \n                                                  test_size=0.10,random_state=101)\n","d1d2d7a5":"seq=Sequential()","57bb5115":"seq.add(Dense(units=32,init='uniform',activation='relu',input_dim=9))","b2290671":"seq.add(Dense(units=1,init='uniform',activation='sigmoid',input_dim=9))","efb00c65":"seq.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])","091663b8":"seq.fit(X_train,y_train, batch_size=32,nb_epoch=300,verbose=0)","59aa8618":"seq_pred=seq.predict(X_test)","9c515fde":"seq_pred=[1 if y>0.5 else 0 for y in seq_pred]\nseq_pred","8c6df123":"print(confusion_matrix(y_test, seq_pred))","a393c3f8":"print(classification_report(y_test, seq_pred))","d8861d3f":"We must convert categorical feature to dimmy variables in terms that our machine learning algorithme be able to directely take inn those featire as input. Note that in statistic and econometrics, particulrity in *regression analysis*, a dummy variable is one that takes only the value $0$ or $1$ to indicate the abscence or presence pf some categorical effect that may be expected to shift outcome.  ","f1d56412":"# 2. Import data","47b23d95":"We note that there were few deaths in class 1 and therefore more survivors in this class who are mostly women. There were therefore more deaths in class 3 and fewer survivors in this class compared to class one.","05d637bd":"# 1. Import libraries ","df50cec5":"We find that in the titanic there are more people between 20 and 40 years old.","0130b539":"# Summary\n\nThe objective here is to compare different machine learning algorithms in order to solve a problem. The idea of this work is purely educational, it aims to show the limits of statistical algorithms to solve a problem and especially to differentiate between a classification problem and a regression problem.","7f2d8027":"We are almost able to clean our data now looking at the figure we have to remove the items from the cabin that have been causing us problems so far but that we will use the method drop()","bd9a542a":"# 3. Exploratory data\n\nTo see where we are missing data, we use the seaborn delivery to create a simple heat map. Let's start by understanding the missing data.","c7e81a65":"# Train and test split \n","7b3d0bf9":"It is clear from this graph that many men died in the sinking of the Titanic.","20c84ce4":"That is great our data is ready for our model","a9980f37":"# Artificial Neural Network (ANN)\n\nSo as different model dont gives us the goog occuracy it's times to apply another algorithme here the Artiffiicial Network","4da6d374":"Great so we can guess the average age of the passengers in each class of the Titanic. In class 1, the passenger is on average 37 years old, in class 2, passengers are on average 29 years old and in class 3, the passenger is on average 24 years old, as shown in the graph above."}}