{"cell_type":{"aeeefadc":"code","d984d40c":"code","6079f0c2":"code","cf18d227":"code","caf0bf10":"code","8f6b1344":"code","b1fef0d0":"code","5a700972":"code","e8593eb3":"code","ae50e165":"code","474934cc":"code","0f8e626e":"code","2362c909":"code","4679b8ff":"code","19b3467f":"code","f3d8ae7a":"code","c44afb3b":"code","c50166ce":"code","a1bcf8d1":"code","d1a291c7":"code","c172473c":"code","771d5dd2":"code","7b019ec8":"code","57115617":"code","0ec41816":"code","93b55055":"code","84267549":"code","34427f84":"code","13d7d423":"code","e120401e":"code","a481f04f":"code","753ae717":"code","68eed1d4":"code","cb3abde7":"code","08ea1a75":"code","b61f49c5":"code","25ee98ae":"code","8636046f":"code","f305c0d9":"code","d4dccb4b":"code","a0ca9e2e":"code","167a0e23":"code","fac9ae85":"code","534efb3a":"code","0a906e55":"code","29d7bd47":"code","10286e97":"code","dbf76d50":"code","1cd5ad33":"code","49ce15f0":"code","48f43b5f":"code","9198de00":"code","3da70eb3":"code","bb8b9ef2":"code","7ca13b5a":"code","8ece3807":"code","2be68228":"code","17ee23bf":"code","4d5e9b9b":"code","81cd416f":"code","a3f2b7db":"code","65385e8c":"code","3cc67a72":"code","0fe8ddf2":"code","b269fb79":"code","459ea623":"code","015d2d03":"code","804429ab":"code","97fd7cab":"code","a06bfdf9":"code","31e8d33f":"code","a79d3931":"code","44c8cc93":"code","d29ce630":"code","66b4defc":"code","b6f56db9":"code","7c6c9dab":"code","95b41e1a":"code","e2575d73":"code","94b95bba":"code","4cb81af6":"code","3768c6b0":"code","1b63614e":"code","9c4e0145":"code","dc9320b9":"code","03e7b55a":"code","b6e41ef9":"markdown","700e0249":"markdown","d0e99022":"markdown","6272e358":"markdown","f179b021":"markdown","14cd0d13":"markdown","b042eea3":"markdown","15f583cd":"markdown","ce38ac8f":"markdown","e82f29b2":"markdown","d9b45b6b":"markdown","5c3a1d5a":"markdown","3c128ad4":"markdown","84d6337e":"markdown","8bf16019":"markdown","70276718":"markdown","cecd2397":"markdown","2fc20045":"markdown","ae5856b5":"markdown","a5bfcecd":"markdown","4cf11062":"markdown","9ed80652":"markdown"},"source":{"aeeefadc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n#import required packages\n#basics\nimport pandas as pd \nimport numpy as np\n\n#misc\nimport gc\nimport time\nimport warnings\n\n#stats\nfrom scipy.misc import imread\nfrom scipy import sparse\nimport scipy.stats as ss\n\n#viz\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\nimport matplotlib_venn as venn\n\n#nlp\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \n\n\n#FeatureEngineering\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf \n\n\n#settings\nstart_time=time.time()\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\neng_stopwords = set(stopwords.words(\"english\"))\nwarnings.filterwarnings(\"ignore\")\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, Masking\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\nimport scipy as sp\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping, ReduceLROnPlateau\nfrom sklearn.preprocessing import OneHotEncoder\n","d984d40c":"## Common Variables for Notebook \nROOT = '\/kaggle\/input\/google-quest-challenge\/'","6079f0c2":"## load the data \ntrain = pd.read_csv(ROOT+'train.csv')\ntest = pd.read_csv(ROOT+'test.csv')\nsub = pd.read_csv(ROOT+'sample_submission.csv')","cf18d227":"## Quick look at the train\ntrain.head()","caf0bf10":"#Quick look at the test \ntest.head()","8f6b1344":"## Quick look at the sample data \nsub.head()\n","b1fef0d0":"## Get the shape of the data\ntrain_len, test_len ,sub_len = len(train.index), len(test.index),len(sub.index)\nprint(f'train size: {train_len}, test size: {test_len} , sample size: {sub_len}')","5a700972":"## Count the missing values \nmiss_val_train = train.isnull().sum(axis=0) \/ train_len\nmiss_val_train = miss_val_train[miss_val_train> 0] * 100\nmiss_val_train","e8593eb3":"## Number of train columns\nlen(list(train.columns))","ae50e165":"## Check the scoring for questions\nall_train_columns = list(train.columns)\nquestion_answer_cols = all_train_columns[:11]\nquestion_target_cols = all_train_columns[11:32]\nanswer_target_cols  = all_train_columns[32:41]","474934cc":"## Check one question and answer  \nquestiont = train[\"question_title\"][0]\nquestionb = train[\"question_body\"][0]\nanswer1 = train[\"answer\"][0]\n\nprint(f\"The First Question Topic  is : {questiont}\\n\\n \")\nprint(f\"The First Question Details are :  \\n\\n {questionb}\\n\\n \")\nprint(f\"The First answer is :\\n\\n {answer1}\\n\\n \")","0f8e626e":"## Check target scoring for question\n\ntrain[question_target_cols].loc[0]","2362c909":"## Check target scoring for answer\ntrain[answer_target_cols].loc[0]","4679b8ff":"## How many distinct users asked more than 10 questions  ?\nuser_q_grp = train.question_user_name.value_counts()\nuser_q_grp.loc[user_q_grp>10].plot(kind='bar', figsize=(30,10), fontsize=10).legend(prop={'size': 20})","19b3467f":"## How many distinct users have answered more than 10 questions?\nuser_a_grp = train.question_user_name.value_counts()\nuser_a_grp.loc[user_a_grp>10].plot(kind='bar', figsize=(30,10), fontsize=10).legend(prop={'size': 20})","f3d8ae7a":"##Lets see what kind of quesitons Mike asked\nprint( f'First Question Asked by Mike : \\n\\n {train.loc[train.question_user_name ==\"Mike\"][\"question_body\"].values[1]}')","c44afb3b":"## Another question asked by Mike \nprint( f'Second Question Asked by Mike : \\n\\n {train.loc[train.question_user_name ==\"Mike\"][\"question_body\"].values[2]}')","c50166ce":"## How Many Mike ?\n\ntrain.loc[train.question_user_name ==\"Mike\"][\"question_user_page\"].values","a1bcf8d1":"## What is the distribution of all question ranking columns \n\ntrain[question_target_cols]","d1a291c7":"## lets see some distributions of questions targets\nplt.figure(figsize=(20, 5))\n\nsns.distplot(train[question_target_cols[0]], hist= False , rug= False ,kde=True, label =question_target_cols[0],axlabel =False )\nsns.distplot(train[question_target_cols[1]], hist= False , rug= False,label =question_target_cols[1],axlabel =False)\nsns.distplot(train[question_target_cols[2]], hist= False , rug= False,label =question_target_cols[2],axlabel =False)\nsns.distplot(train[question_target_cols[3]], hist= False , rug= False,label =question_target_cols[3],axlabel =False)\nsns.distplot(train[question_target_cols[4]], hist= False , rug= False,label =question_target_cols[4],axlabel =False)\nplt.show()","c172473c":"## lets see some distributions of answer targets\nplt.figure(figsize=(20, 5))\n\nsns.distplot(train[answer_target_cols[0]], hist= False , rug= False ,kde=True, label =answer_target_cols[0],axlabel =False )\nsns.distplot(train[answer_target_cols[1]], hist= False , rug= False,label =answer_target_cols[1],axlabel =False)\n#sns.distplot(train[answer_target_cols[2]], hist= False , rug= False,label =answer_target_cols[2],axlabel =False)\n#sns.distplot(train[answer_target_cols[3]], hist= False , rug= False,label =answer_target_cols[3],axlabel =False)\nsns.distplot(train[answer_target_cols[4]], hist= False , rug= False,label =answer_target_cols[4],axlabel =False)\nplt.show()\n## Removed two columns as value was quite high and other graphs were not visible .","771d5dd2":"# Lets see how the mean value of one target feature for questions changes based on category\nfor idx in range(20):\n    df = train.groupby('category')[question_target_cols[idx]].mean()\n        \n    fig, axes = plt.subplots(1, 1, figsize=(10,10))\n    axes.set_title(question_target_cols[idx])\n    df.plot(label=question_target_cols[idx])\n    plt.show()\n\n","7b019ec8":"## Lets see the words of first question\n\nplt.figure(figsize=(20, 5))\n\ntext = train.question_body[0]\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","57115617":"### Lets see the words of first answer\nplt.figure(figsize=(20, 5))\n\ntext = train.answer[0]\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","0ec41816":"##How many words are there in all questions ? \n\ntext = \" \".join(question_body for question_body in train.question_body)\nprint (\"There are {} words in the combination of all questions.\".format(len(text)))","93b55055":"## Load all questions in word cloud \n\nstopwords = set(STOPWORDS)\nstopwords.update([\"gt\", \"lt\", \"one\", \"use\", \"will\",\"using\"]) ## I found this list by first time running this cell without stopwords\n\n# Generate a word cloud image\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n# Display the generated image:\n# the matplotlib way:\nplt.figure(figsize=(20, 10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","84267549":"##Lets do some unnecessary but nice masking\nimport cv2\nim = cv2.imread(\"..\/input\/worldcloud2\/question_mark_col.png\")\nimg = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n# Generate a word cloud image\nwc = WordCloud(background_color=\"black\", max_words=1000, mask=img,\n               stopwords=stopwords,max_font_size=90, random_state=42)\nwc.generate(text)\nimage_colors = ImageColorGenerator(img)\n# Display the generated image:\n# the matplotlib way:\nplt.figure(figsize=[12,12])\nplt.imshow(wc.recolor(color_func=image_colors), interpolation=\"bilinear\")\nplt.axis(\"off\")\n_=plt.show()","34427f84":"#clean comments\nclean_mask=np.array(Image.open(\"..\/input\/wordcloud3\/Answer.jpg\"))\nclean_mask=clean_mask[:,:,1]\n#wordcloud for clean comments\nans = \" \".join(answer for answer in train.answer)\nwc= WordCloud(background_color=\"black\",max_words=2000,mask=clean_mask,stopwords=stopwords)\nwc.generate(ans)\nplt.figure(figsize=(20,10))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Answers\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\nplt.show()\n","13d7d423":"import cv2\nim = cv2.imread(\"..\/input\/wordcloud3\/Answer.jpg\")\nimg = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\nans = \" \".join(answer for answer in train.answer)\n# Generate a word cloud image\nwc = WordCloud(background_color=\"white\",max_words=1000, mask=img,\n               stopwords=stopwords,max_font_size=90, random_state=42)\nwc.generate(ans)\nimage_colors = ImageColorGenerator(img)\n# Display the generated image:\n# the matplotlib way:\nplt.figure(figsize=[12,12])\nplt.imshow(wc.recolor(color_func=image_colors), interpolation=\"bilinear\")\nplt.axis(\"off\")\n_=plt.show()","e120401e":"# https:\/\/www.kaggle.com\/jagangupta\/stop-the-s-toxic-comments-eda\/data as reference ","a481f04f":"plt.figure(figsize=(20,20))\nsns.heatmap(train[question_target_cols].corr(),vmin=-1,cmap='coolwarm')","753ae717":"## add category as label\ntrain['cat_label'] = train['category'].rank(method='dense', ascending=False).astype(int)","68eed1d4":"## lets see if its correlated to any category \nquestion_target_cols.append('cat_label')\n\nplt.figure(figsize=(20,20))\nsns.heatmap(train[question_target_cols].corr(),vmin=-1,cmap='coolwarm')","cb3abde7":"##Lets do the same for answers \nplt.figure(figsize=(20,20))\nsns.heatmap(train[answer_target_cols].corr(),vmin=-1,cmap='coolwarm')","08ea1a75":"##Baseline from https:\/\/www.kaggle.com\/ryches\/mean-of-categories-benchmark]\ntarget_cols =question_target_cols+answer_target_cols\ntrain[\"cat_host\"]= train[\"category\"]+train[\"host\"]\ncategory_means_map = train.groupby([\"cat_host\"])[target_cols].mean().T.to_dict()\npreds = train[\"cat_host\"].map(category_means_map).apply(pd.Series)","b61f49c5":"category_means_map.keys()","25ee98ae":"from scipy.stats import spearmanr\noverall_score = 0\nfor col in target_cols:\n    overall_score += spearmanr(preds[col], train[col]).correlation\/len(target_cols)\n    print(col, spearmanr(preds[col], train[col]).correlation)","8636046f":"overall_score","f305c0d9":"##Baseline from https:\/\/www.kaggle.com\/ryches\/mean-of-categories-benchmark\ntarget_cols =question_target_cols+answer_target_cols\ntest[\"cat_host\"]= test[\"category\"]+test[\"host\"]\n#category_means_map = train.groupby([\"cat_host\"])[target_cols].mean().T.to_dict()\n#preds = train[\"cat_host\"].map(category_means_map).apply(pd.Series)","d4dccb4b":"test_preds = test[\"cat_host\"].map(category_means_map).apply(pd.Series)","a0ca9e2e":"test_preds","167a0e23":"sub = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")\nfor col in target_cols:\n    sub[col] = test_preds[col]","fac9ae85":"sub.fillna(value=0.000000,inplace=True)","534efb3a":"sub.to_csv(\"submission.csv\", index = False)","0a906e55":"sub.describe()","29d7bd47":"sub.loc[sub['question_asker_intent_understanding'].isna() == True]","10286e97":"train.iloc[:,1:3]","dbf76d50":"#train_test_merge - analyse questions\nmerge=pd.concat([train.iloc[:,0:3],test.iloc[:,0:3]])\ndf_q=merge.reset_index(drop=True)","1cd5ad33":"## Indirect features\n\n#Sentense count in each comment:\n    #  '\\n' can be used to count the number of sentences in each comment\ndf_q['count_sent']=df_q[\"question_body\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n#Word count in each comment:\ndf_q['count_word']=df_q[\"question_body\"].apply(lambda x: len(str(x).split()))\n#Unique word count\ndf_q['count_unique_word']=df_q[\"question_body\"].apply(lambda x: len(set(str(x).split())))\n#Letter count\ndf_q['count_letters']=df_q[\"question_body\"].apply(lambda x: len(str(x)))\n#punctuation count\ndf_q[\"count_punctuations\"] =df_q[\"question_body\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n#upper case words count\ndf_q[\"count_words_upper\"] = df_q[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n#title case words count\ndf_q[\"count_words_title\"] = df_q[\"question_body\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n#Number of stopwords\ndf_q[\"count_stopwords\"] = df_q[\"question_body\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n#Average length of the words\ndf_q[\"mean_word_len\"] = df_q[\"question_body\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","49ce15f0":"#derived features\n#Word count percent in each comment:\ndf_q['word_unique_percent']=df_q['count_unique_word']*100\/df_q['count_word']\n#derived features\n#Punct percent in each comment:\ndf_q['punct_percent']=df_q['count_punctuations']*100\/df_q['count_word']","48f43b5f":"train.iloc[:,2:]","9198de00":"#serperate train and test features\ntrain_feats=df_q.iloc[0:len(train),]\ntest_feats=df_q.iloc[len(train):,]\nlen(train)","3da70eb3":"train[question_target_cols]","bb8b9ef2":"question_target_cols.append('qa_id')","7ca13b5a":"train_q = pd.merge(train_feats,train[question_target_cols], on='qa_id',how='left')\ntrain_q","8ece3807":"from matplotlib.ticker import StrMethodFormatter\n\n\ntrain_q['count_sent'].loc[train_q['count_sent']>10] = 10 \nplt.figure(figsize=(12,6))\n## sentenses\nplt.subplot(121)\nplt.suptitle(\"Are longer questions more clear?\",fontsize=20)\nsns.violinplot(y='count_sent',x='question_well_written', data=train_q,split=True)\nplt.xlabel('Clear?', fontsize=12)\nplt.ylabel('# of sentences', fontsize=12)\nplt.title(\"Number of sentences in each question\", fontsize=15)\nplt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.2f}'))\n# words\ntrain_q['count_word'].loc[train_q['count_word']>1000] = 1000\nplt.subplot(122)\nsns.violinplot(y='count_word',x='question_well_written', data=train_q,split=True,inner=\"quart\")\nplt.xlabel('Clear?', fontsize=12)\nplt.ylabel('# of words', fontsize=12)\nplt.title(\"Number of words in each question\", fontsize=15)\nplt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.2f}')) # 2 decimal places\n#plt.show()","2be68228":"train_q['count_word']","17ee23bf":"#https:\/\/github.com\/ahmedbesbes\/Quora-Insincere-Questions-Classification\n# import keras tokenizing utilities \nfrom keras.preprocessing import text, sequence\n\n# import tensorboardX in case we want to log metrics to tensorboard (requires tensorflow installed - optional)\n# import spacy for tokenization\nimport spacy\nfrom tqdm import tqdm_notebook\ntqdm_notebook().pandas()\n# fastText is a library for efficient learning of word representations and sentence classification\n# https:\/\/github.com\/facebookresearch\/fastText\/tree\/master\/python\n# I use it with a pre-trained english embedding that you can fetch from the official website\n#import fastText\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\n","4d5e9b9b":"nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'tagger'])","81cd416f":"train = pd.read_csv(ROOT+'train.csv')\ntest = pd.read_csv(ROOT+'test.csv')\nsub = pd.read_csv(ROOT+'sample_submission.csv')","a3f2b7db":"def decontract(text):\n    text = re.sub(r\"(W|w)on(\\'|\\\u2019)t \", \"will not \", text)\n    text = re.sub(r\"(C|c)an(\\'|\\\u2019)t \", \"can not \", text)\n    text = re.sub(r\"(Y|y)(\\'|\\\u2019)all \", \"you all \", text)\n    text = re.sub(r\"(Y|y)a(\\'|\\\u2019)ll \", \"you all \", text)\n    text = re.sub(r\"(I|i)(\\'|\\\u2019)m \", \"i am \", text)\n    text = re.sub(r\"(A|a)isn(\\'|\\\u2019)t \", \"is not \", text)\n    text = re.sub(r\"n(\\'|\\\u2019)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\\u2019)re \", \" are \", text)\n    text = re.sub(r\"(\\'|\\\u2019)d \", \" would \", text)\n    text = re.sub(r\"(\\'|\\\u2019)ll \", \" will \", text)\n    text = re.sub(r\"(\\'|\\\u2019)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\\u2019)ve \", \" have \", text)\n    return text\n\ndef clean_text(x):\n\n    x = str(x)\n    for punct in \"\/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019':\n        x = x.replace(punct, '')\n    return x\n\ndef clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '12345', x)\n    x = re.sub('[0-9]{4}', '1234', x)\n    x = re.sub('[0-9]{3}', '123', x)\n    x = re.sub('[0-9]{2}', '12', x)\n    return x\n\ndef preprocess(x):\n    x= decontract(x)\n    x=clean_text(x)\n    x=clean_numbers(x)\n    return x\n\ntrain['question_body'] = train['question_body'].progress_map(lambda q: preprocess(q))\ntrain['answer'] = train['answer'].progress_map(lambda q: preprocess(q))\ntrain['question_title'] = train['question_title'].progress_map(lambda q: preprocess(q))\n\n\ntest['question_body'] = test['question_body'].progress_map(lambda q: preprocess(q))\ntest['answer'] = test['answer'].progress_map(lambda q: preprocess(q))\ntest['question_title'] = test['question_title'].progress_map(lambda q: preprocess(q))","65385e8c":"# define tokenization parameters \n\nMAX_WORDS = 40000\nMAX_LEN = 500","3cc67a72":"all_questions = (train['question_body']+'' +train['answer']).tolist() + (test['question_body']+''+test['answer']).tolist()\nlen(all_questions)","0fe8ddf2":"from keras.preprocessing.text import Tokenizer\n\ntrain['text'] = train['question_body'] + ' ' + train['answer'] +' '+ train['question_title']\ntest['text'] = test['question_body'] + ' ' + test['answer'] +' '+ test['question_title']\n\n\nfull_text = list(train['text'].values) + list(test['text'].values)\n\ntk = Tokenizer(lower = False, filters='')\ntk.fit_on_texts(full_text)\nprint(f'Number of words in the dictionary: {len(tk.word_index)}')\n\ntrain_df, valid_df, y_train, y_valid = train_test_split(train, train[sub.columns[1:]], test_size=0.1)\ntrain_tokenized = tk.texts_to_sequences(train_df['question_body'] + ' ' + train_df['answer']+ ' '+ + train_df['question_title'])\nvalid_tokenized = tk.texts_to_sequences(valid_df['question_body'] + ' ' + valid_df['answer']+ ' '+ valid_df['question_title'])\ntest_tokenized = tk.texts_to_sequences(test['text'])\n\nmax_len = 500\nX_train = pad_sequences(train_tokenized, maxlen = max_len)\nX_valid = pad_sequences(valid_tokenized, maxlen = max_len)\nX_test = pad_sequences(test_tokenized, maxlen = max_len)\n","b269fb79":"!ls '..\/input\/wikinews300d1mvec\/wiki-news-300d-1M.vec'","459ea623":"def get_embedding_path(embedding):\n    embedding_zoo = {\"crawl\": \"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\",\"crawl_sub\":\"..\/input\/fasttext-crawl-300d-2m-with-subword\/crawl-300d-2m-subword\/crawl-300d-2M-subword.vec\",\n                \"glove\": \"..\/input\/glove840b\/glove.840B.300d.txt\",\"paragram\":\"..\/input\/paragram-300-sl999\/paragram_300_sl999.txt\", \"wikinews\":\"..\/input\/wikinews300d1mvec\/wiki-news-300d-1M.vec\" }\n    return embedding_zoo.get(embedding)\n\nembed_size = 300\nmax_features = 100000\n","015d2d03":"def get_coefs(word,*arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef build_matrix(embedding, tokenizer):\n    embedding_path= get_embedding_path(embedding)\n    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding='utf-8'))\n\n    word_index = tk.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features:\n            continue\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n\n\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","804429ab":"\"\"\"%%time\nembedding_matrix_1 = build_matrix(\"crawl_sub\", tk)\n#embedding_matrix_2 = build_matrix(\"paragram\", tk)\nembedding_matrix_3 = build_matrix(\"glove\", tk)\n#embedding_matrix_4 = build_matrix(\"wikinews\", tk)\n\n#embedding_matrix = np.mean((1.28*embedding_matrix_1, 0.72*embedding_matrix_2,2*embedding_matrix_3), axis=0)\nembedding_matrix =np.mean((embedding_matrix_1, embedding_matrix_3), axis=0)\n#del embedding_matrix_2,\ndel embedding_matrix_1,embedding_matrix_3\ngc.collect()\nnp.shape(embedding_matrix)\"\"\"","97fd7cab":"%%time\nembedding_matrix_crawl = build_matrix(\"crawl\", tk)","a06bfdf9":"\"\"\"import pickle\nGLOVE_EMBEDDING_PATH = '\/kaggle\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl' \ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i >= max_features:\n            continue\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\"\"\"","31e8d33f":"#tic = time.time()\n#embedding_matrix_glove,_ = build_matrix(tk.word_index,GLOVE_EMBEDDING_PATH)","a79d3931":"#embedding_matrix =np.mean((embedding_matrix_crawl, embedding_matrix_glove), axis=0)\nembedding_matrix = embedding_matrix_crawl\ndel embedding_matrix_crawl#,embedding_matrix_glove\ngc.collect()","44c8cc93":"\"\"\"From built-in optimizer classes.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport copy\nfrom six.moves import zip\n\nfrom keras import backend as K\nfrom keras.utils.generic_utils import serialize_keras_object\nfrom keras.utils.generic_utils import deserialize_keras_object\nfrom keras.legacy import interfaces\n\nfrom keras.optimizers import Optimizer\n\nclass AdamW(Optimizer):\n    \"\"\"AdamW optimizer.\n    Default parameters follow those provided in the original paper.\n    # Arguments\n        lr: float >= 0. Learning rate.\n        beta_1: float, 0 < beta < 1. Generally close to 1.\n        beta_2: float, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n        decay: float >= 0. Learning rate decay over each update.\n        weight_decay: float >= 0. Weight decay (L2 penalty) (default: 0.025).\n        batch_size: integer >= 1. Batch size used during training.\n        samples_per_epoch: integer >= 1. Number of samples (training points) per epoch.\n        epochs: integer >= 1. Total number of epochs for training. \n    # References\n        - [Adam - A Method for Stochastic Optimization](http:\/\/arxiv.org\/abs\/1412.6980v8)\n        - [Fixing Weight Decay Regularization in Adam](https:\/\/arxiv.org\/abs\/1711.05101)\n    \"\"\"\n\n    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., weight_decay=0.025, \n                 batch_size=1, samples_per_epoch=1, \n                 epochs=1, **kwargs):\n        super(AdamW, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.lr = K.variable(lr, name='lr')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n            self.batch_size = K.variable(batch_size, name='batch_size')\n            self.samples_per_epoch = K.variable(samples_per_epoch, name='samples_per_epoch')\n            self.epochs = K.variable(epochs, name='epochs')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n\n    @interfaces.legacy_get_updates_support\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.initial_decay > 0:\n            lr = lr * (1. \/ (1. + self.decay * K.cast(self.iterations,\n                                                      K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n        '''Bias corrections according to the Adam paper\n        '''\n        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) \/\n                     (1. - K.pow(self.beta_1, t)))\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        self.weights = [self.iterations] + ms + vs\n\n        for p, g, m, v in zip(params, grads, ms, vs):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n            \n            '''Schedule multiplier eta_t = 1 for simple AdamW\n            According to the AdamW paper, eta_t can be fixed, decay, or \n            also be used for warm restarts (AdamWR to come). \n            '''\n            eta_t = 1.\n            p_t = p - eta_t*(lr_t * m_t \/ (K.sqrt(v_t) + self.epsilon))\n            if self.weight_decay != 0:\n                '''Normalized weight decay according to the AdamW paper\n                '''\n                w_d = self.weight_decay*K.sqrt(self.batch_size\/(self.samples_per_epoch*self.epochs))\n                p_t = p_t - eta_t*(w_d*p) \n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'decay': float(K.get_value(self.decay)),\n                  'weight_decay': float(K.get_value(self.weight_decay)),\n                  'batch_size': int(K.get_value(self.batch_size)),\n                  'samples_per_epoch': int(K.get_value(self.samples_per_epoch)),\n                  'epochs': int(K.get_value(self.epochs)),\n                  'epsilon': self.epsilon}\n        base_config = super(AdamW, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","d29ce630":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        \"\"\"\n        Keras Layer that implements an Attention mechanism for temporal data.\n        Supports Masking.\n        Follows the work of Raffel et al. [https:\/\/arxiv.org\/abs\/1512.08756]\n        # Input shape\n            3D tensor with shape: `(samples, steps, features)`.\n        # Output shape\n            2D tensor with shape: `(samples, features)`.\n        :param kwargs:\n        Just put it on top of an RNN Layer (GRU\/LSTM\/SimpleRNN) with return_sequences=True.\n        The dimensions are inferred based on the output shape of the RNN.\n        Example:\n            model.add(LSTM(64, return_sequences=True))\n            model.add(Attention())\n        \"\"\"\n        self.supports_masking = True\n        #self.init = initializations.get('glorot_uniform')\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight(shape=(input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight(shape=(input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        # eij = K.dot(x, self.W) TF backend doesn't support it\n\n        # features_dim = self.W.shape[0]\n        # step_dim = x._keras_shape[1]\n\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n    #print weigthted_input.shape\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        #return input_shape[0], input_shape[-1]\n        return input_shape[0],  self.features_dim\n","66b4defc":"from keras.layers import *\nfrom keras.models import *\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.initializers import *\nfrom keras.optimizers import *\nimport keras.backend as K\nfrom keras.callbacks import *","b6f56db9":"class AttentionWeightedAverage(Layer):\n    \"\"\"\n    Computes a weighted average of the different channels across timesteps.\n    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n    \"\"\"\n\n    def __init__(self, return_attention=False, **kwargs):\n        self.init = initializers.RandomUniform(seed=10000)\n        self.supports_masking = True\n        self.return_attention = return_attention\n        super(AttentionWeightedAverage, self).__init__(** kwargs)\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(ndim=3)]\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight(shape=(input_shape[2], 1),\n                                 name='{}_W'.format(self.name),\n                                 initializer=self.init)\n        self.trainable_weights = [self.W]\n        super(AttentionWeightedAverage, self).build(input_shape)\n\n    def call(self, x, mask=None):\n        # computes a probability distribution over the timesteps\n        # uses 'max trick' for numerical stability\n        # reshape is done to avoid issue with Tensorflow\n        # and 1-dimensional weights\n        logits = K.dot(x, self.W)\n        x_shape = K.shape(x)\n        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n\n        # masked timesteps have zero weight\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            ai = ai * mask\n        att_weights = ai \/ (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n        weighted_input = x * K.expand_dims(att_weights)\n        result = K.sum(weighted_input, axis=1)\n        if self.return_attention:\n            return [result, att_weights]\n        return result\n\n    def get_output_shape_for(self, input_shape):\n        return self.compute_output_shape(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        output_len = input_shape[2]\n        if self.return_attention:\n            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n        return (input_shape[0], output_len)\n\n    def compute_mask(self, input, input_mask=None):\n        if isinstance(input_mask, list):\n            return [None] * len(input_mask)\n        else:\n            return None\nclass AdamW(Optimizer):\n    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1\/4)\n                 epsilon=1e-8, decay=0., **kwargs):\n        super(AdamW, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n            self.wd = K.variable(weight_decay, name='weight_decay') # decoupled weight decay (2\/4)\n        self.epsilon = epsilon\n        self.initial_decay = decay\n\n    @interfaces.legacy_get_updates_support\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        wd = self.wd # decoupled weight decay (3\/4)\n\n        learning_rate = self.learning_rate\n        if self.initial_decay > 0:\n            learning_rate *= (1. \/ (1. + self.decay * K.cast(self.iterations,\n                                                  K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n        lr_t = learning_rate * (K.sqrt(1. - K.pow(self.beta_2, t)) \/\n                     (1. - K.pow(self.beta_1, t)))\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        self.weights = [self.iterations] + ms + vs\n\n        for p, g, m, v in zip(params, grads, ms, vs):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n            p_t = p - lr_t * m_t \/ (K.sqrt(v_t) + self.epsilon) - learning_rate * wd * p # decoupled weight decay (4\/4)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'learning_rate': float(K.get_value(self.learning_rate)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'decay': float(K.get_value(self.decay)),\n                  'weight_decay': float(K.get_value(self.wd)),\n                  'epsilon': self.epsilon}\n        base_config = super(AdamW, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","7c6c9dab":"max_len = 500\nmax_features =embedding_matrix.shape[0]\nembedding_matrix.shape","95b41e1a":"# Compatible with tensorflow backend\nclass SpearmanRhoCallback(Callback):\n    def __init__(self, training_data, validation_data, patience, model_name):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n        \n        self.patience = patience\n        self.value = -1\n        self.bad_epochs = 0\n        self.model_name = model_name\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n        if rho_val >= self.value:\n            self.value = rho_val\n        else:\n            self.bad_epochs += 1\n        if self.bad_epochs >= self.patience:\n            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n            self.model.stop_training = True\n            #self.model.save_weights(self.model_name)\n        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n        return rho_val\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","e2575d73":"def build_model_zoo(X_train, y_train, X_valid, y_valid, embedding_matrix, lr=0.0,lr_d=0.0, units=0, spatial_dr=0.0, dense_units=128,\n                dr=0.1, epochs=5, use_attention=True,model_type = 'bigrucnn',batch_size=256):\n\n    spatialdropout=0.20\n    rnn_units=64\n    weight_decay=0.07\n    filters=[100, 80, 30, 12]\n    file_path = f'{MODEL_TYPE}_best_model.hdf5'\n    \n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    #early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 30)\n    #spearman = \n    scheduler = ReduceLROnPlateau(patience=3)\n    if model_type == 'bigruatt':\n        inp = Input(shape = (max_len,))\n        x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n        x1 = SpatialDropout1D(spatial_dr)(x)\n\n        x_gru = Bidirectional(GRU(units * 2, return_sequences = True))(x1)\n        if use_attention:\n            x_att = Attention(max_len)(x_gru)\n            x = Dropout(dr)(Dense(dense_units, activation='relu') (x_att))\n        else:\n            x_att = Flatten() (x_gru)\n            x = Dropout(dr)(Dense(dense_units, activation='relu') (x_att))\n\n        x = BatchNormalization()(x)\n    #x = Dropout(dr)(Dense(int(dense_units \/ 2), activation='relu') (x))\n        x = Dense(30, activation = \"sigmoid\")(x)\n        model = Model(inputs = inp, outputs = x)\n    elif model_type == 'bigrucnn':\n        inp = Input(shape=(max_len,))\n        x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n        x = SpatialDropout1D(spatialdropout)(x)\n        x = Bidirectional(GRU(rnn_units, return_sequences=True))(x)\n\n        x1 = Conv1D(filters=filters[0], activation='relu', kernel_size=1, \n                    padding='same', kernel_initializer=glorot_uniform(seed=110000))(x)\n        x2 = Conv1D(filters=filters[1], activation='relu', kernel_size=2, \n                    padding='same', kernel_initializer=glorot_uniform(seed=120000))(x)\n        x3 = Conv1D(filters=filters[2], activation='relu', kernel_size=3, \n                    padding='same', kernel_initializer=glorot_uniform(seed=130000))(x)\n        x4 = Conv1D(filters=filters[3], activation='relu', kernel_size=5, \n                    padding='same', kernel_initializer=glorot_uniform(seed=140000))(x)\n\n    \n        x1 = GlobalMaxPool1D()(x1)\n        x2 = GlobalMaxPool1D()(x2)\n        x3 = GlobalMaxPool1D()(x3)\n        x4 = GlobalMaxPool1D()(x4)\n\n        c = concatenate([x1, x2, x3, x4])\n        x = Dense(200, activation='relu', kernel_initializer=glorot_uniform(seed=111000))(c)\n        x = Dropout(0.2, seed=10000)(x)\n        x = BatchNormalization()(x)\n        x = Dense(30, activation=\"sigmoid\", kernel_initializer=glorot_uniform(seed=110000))(x)\n        model = Model(inputs=inp, outputs=x)\n    elif model_type == 'poolrnn':\n        inp = Input(shape=(max_len,))\n        embedding_layer = Embedding(max_features,\n                               embed_size,\n                                weights=[embedding_matrix],\n                                input_length=max_len,\n                                trainable=False)(inp)\n        embedding_layer = SpatialDropout1D(spatialdropout, seed=1024)(embedding_layer)\n\n        rnn_1 = Bidirectional(GRU(rnn_units, return_sequences=True, \n                                   kernel_initializer=glorot_uniform(seed=10000), \n                                   recurrent_initializer=Orthogonal(gain=1.0, seed=123000)))(embedding_layer)      \n\n        last = Lambda(lambda t: t[:, -1], name='last')(rnn_1)\n        maxpool = GlobalMaxPooling1D()(rnn_1)\n        attn = AttentionWeightedAverage()(rnn_1)\n        average = GlobalAveragePooling1D()(rnn_1)\n\n        c = concatenate([last, maxpool, attn], axis=1)\n        c = Reshape((3, -1))(c)\n        c = Lambda(lambda x:K.sum(x, axis=1))(c)\n        x = BatchNormalization()(c)\n        x = Dense(200, activation='relu', kernel_initializer=glorot_uniform(seed=111000))(x)\n        x = Dropout(0.2, seed=1024)(x)\n        x = BatchNormalization()(x)\n        output_layer = Dense(30, activation=\"sigmoid\", kernel_initializer=glorot_uniform(seed=111000))(x)\n        model = Model(inputs=inp, outputs=output_layer)\n        \n        \n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, validation_data=(X_valid, y_valid), \n                        verbose = 0, callbacks = [check_point,SpearmanRhoCallback(training_data=(X_train, y_train), validation_data=(X_valid, y_valid),\n                                       patience=5, model_name=f'best_model_batch.h5')])\n    #model = load_model(file_path)\n    return model\n","94b95bba":"kfold = KFold(n_splits=3, random_state=42, shuffle=True)\nbestscore = []\nbestloss = []\ny_test = np.zeros((X_test.shape[0], ))\noof = np.zeros((X_valid.shape[0], ))\npredict_list = []\nepochs = [15, 20, 17, 18]\nval_list = []\nval_pred_list = []\nfor i, (train_index, valid_index) in enumerate(kfold.split(X_train,y_train.values)):\n    print(len(train_index))\n    print(len(valid_index))\n    val_list += list(valid_index)\n    print('FOLD%s'%(i+1))\n    if i ==0 :\n        MODEL_TYPE='bigruatt'\n        BATCH_SIZE=64\n    elif i ==1 :\n        MODEL_TYPE='bigrucnn'\n        BATCH_SIZE = 64\n    else:\n        MODEL_TYPE = 'poolrnn'\n        BATCH_SIZE = 64\n        \n    X_tr, X_val, Y_tr, Y_val = X_train[train_index], X_train[valid_index], y_train.values[train_index], y_train.values[valid_index]\n    model = build_model_zoo(X_tr, Y_tr, X_val, Y_val, embedding_matrix, lr = 1e-2, units = 64,\n                    spatial_dr = 0.1, dense_units=128, dr=0.1, epochs=30,model_type=MODEL_TYPE,batch_size=BATCH_SIZE) ##There was a bug in earlier Kernel\n    \n    valid_pred = model.predict(X_valid, batch_size = 256, verbose = 1)\n    score = 0\n    for j, col in enumerate(train[sub.columns[1:]]):\n        score += np.nan_to_num(spearmanr(y_valid[col], valid_pred[:, j]).correlation)\n    print(score \/ (j + 1))    \n    prediction = np.nan_to_num(model.predict(X_test, batch_size = 256, verbose = 1))\n    predict_list.append(prediction)\n    val_pred_list.append(valid_pred)","4cb81af6":"valid_pred = model.predict(X_valid, batch_size = 1024, verbose = 1)\nscore = 0\nfor j, col in enumerate(train[sub.columns[1:]]):\n        score += np.nan_to_num(spearmanr(y_valid[col], valid_pred[:, j]).correlation)\nprint(score \/ (j + 1))    ","3768c6b0":"val_pred_mean = 0.4*val_pred_list[0]+0.3*val_pred_list[1]+0.3*val_pred_list[2]\nscore = 0\nfor i, col in enumerate(train[sub.columns[1:]]):\n    score += np.nan_to_num(spearmanr(y_valid[col], val_pred_mean[:, i]).correlation)\n    print(score)","1b63614e":"val_pred_mean","9c4e0145":"print(score \/ (i + 1))","dc9320b9":"#prediction = np.nan_to_num(model.predict(X_test, batch_size = 1024, verbose = 1))\nprediction_mean = 0.33*predict_list[0]+0.33*predict_list[1]+0.34*predict_list[2]","03e7b55a":"sub[sub.columns[1:]] = sigmoid(prediction_mean)\nsub.to_csv('submission.csv', index=False)","b6e41ef9":"## Lets list down the inferences that I have drawn from here\n1. Question Interestingness self and others are very correlated \n2. Its is also correlated that short answer seeking questions are commonly accepted \n3. Question is well written and It is interesting to self are correlated \n4. Ofcourse the option seeking questions are more conversational \n5. It is surprising to see that fact seeking and opinion seeking questions are strongly inversely correlated . Because I thought Fact+Bias = Opinion","700e0249":" # More EDA","d0e99022":"# Data Cleaning and Preprocessing Utilities","6272e358":"## TO BE CONTINUED...","f179b021":"unlike questions , here we can see strong correlation between attributes . e.g.  Answer to be helpful , it has to be plausible and satisfactory . ","14cd0d13":"Some helper code and optimizer","b042eea3":"## FOLD Implementation . Different Fold different Model ","15f583cd":"We can see that there is some relevant information about lens and tube","ce38ac8f":"#### BASELINE TAKEN FROM RYCHES KERNEL ADDED SOME TWIST ","e82f29b2":"# Data Understanding","d9b45b6b":"## Competition Details\nComputers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.\n\nHumans are better at addressing subjective questions that require a deeper, multidimensional understanding of context - something computers aren't trained to do well\u2026yet.. Questions can take many forms - some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong.\n\n\n\nUnfortunately, it\u2019s hard to build better subjective question-answering algorithms because of a lack of data and predictive models. That\u2019s why the CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects.\n\nIn this competition, you\u2019re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get!\n\nDemonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like.\n","5c3a1d5a":"We can understand that the person asking question is actually seeking an option so value is 1.000, question is clear and well written , hence 1.0000.\n\nNot sure about question body critical and why it is 0.33333","3c128ad4":"Answer is indeed well written and it is helpful too . It is also relevant to the question therefore the value is 1.000000","84d6337e":"Both seem to be very different kind of questions . Lets verify how many Mike's we have ?","8bf16019":"We need to consider the question_user_page and answer_user_page to identify potentially unique users . ","70276718":"## Embedding Concatenation ? Lets try ","cecd2397":"## Model Definition","2fc20045":"### AdamW optimizer ","ae5856b5":"### Models taken from @artgor @canming\n### Callback taken from @abazdyrev's amazing Kernel ","a5bfcecd":"As expected Science Questions are majorly of type Fact Seeking, it does not expect short answer.\nLife Arts have very less commonly accepted answers . The questions are mostly option seeking in nature.","4cf11062":"#### Taken from https:\/\/www.kaggle.com\/artgor\/eda-and-baseline-in-keras\/data","9ed80652":"looks like question is interesting on not it is defined by type . "}}