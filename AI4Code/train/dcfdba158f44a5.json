{"cell_type":{"8cfd507a":"code","da930a25":"code","20cf240b":"code","88c732f1":"code","6738f744":"code","efc1a60c":"code","0b5122f5":"code","8e28be6f":"code","61bf836f":"code","edd5082d":"code","8d05abdb":"code","200ceb7a":"code","f719af8a":"code","dda822c5":"code","84fbec6d":"code","8370dbe3":"code","4a6e85d4":"code","9c7c75bc":"code","97060077":"code","f08bc6c7":"code","1ad6931e":"code","9bbfbb98":"code","3291660f":"code","13d04ee4":"code","19865639":"code","96433cd2":"code","e3e3f8a2":"code","7de4d4e2":"code","d9427a52":"code","7d56b27f":"code","dfd35c2f":"code","78e9ff7e":"code","2e036a86":"code","f1847f46":"code","c4cdb977":"code","5dc2200b":"code","08974649":"code","6da32c62":"code","c42985b7":"code","076d3eb1":"code","f44520dc":"code","4f8255ac":"code","c3c53b93":"code","9ac5de03":"code","c1339e28":"code","fb8e0f2f":"code","6d90fd3c":"code","ce63beca":"code","bcc5c3af":"code","30cd02dd":"code","c50e07a8":"code","6ab0e415":"code","1051f3df":"code","9bdfc354":"code","6f3964c6":"code","def2ab57":"code","ae8a3c1f":"code","55dbb5d4":"code","00189a75":"code","1947947d":"code","a8d93d33":"code","8e49f237":"code","54b206a3":"markdown","480bf22a":"markdown"},"source":{"8cfd507a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","da930a25":"#Importing the basic libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.offline as py\nfrom plotly import tools\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')","20cf240b":"#Reading the dataset \ncars = pd.read_csv('\/kaggle\/input\/car-evaluation-data-set\/car_evaluation.csv')\ncars.shape\n","88c732f1":"#Since our dataset doesn't contain the name of columns, the column names were assigned \ncars.columns = ['Buying', 'Maint', 'Doors','Persons','LugBoot','Safety','Evaluation']","6738f744":"#Taking an overview of data\ncars.sample(10)","efc1a60c":"a_df=[]\nfor i in cars.values:\n    if i[6] == 'acc':\n        a_df.append(i)","0b5122f5":"df=pd.DataFrame(a_df)","8e28be6f":"df.sample(10)","61bf836f":"#Let's check if there are any missing values in our dataset \ncars.isnull().sum()","edd5082d":"#We see that there are no missing values in our dataset \n#Let's take a more analytical look at our dataset \ncars.describe()","8d05abdb":"#We realize that our data has categorical values \ncars.columns","200ceb7a":"#Lets find out the number of cars in each evaluation category\ncars['Evaluation'].value_counts().sort_index()\n","f719af8a":"fig = {\n  \"data\": [\n    {\n      \"values\": [1210,384,69,65],\n      \"labels\": [\n        \"Unacceptable\",\n        \"Acceptable\",\n        \"Good\",\n        \"Very Good\"\n      ],\n      \"domain\": {\"column\": 0},\n      \"name\": \"Car Evaluation\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .6,\n      \"type\": \"pie\"\n    }],\n  \"layout\": {\n        \"title\":\"Distribution of Evaluated Cars\",\n        \"grid\": {\"rows\": 1, \"columns\": 1},\n        \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 36\n                },\n                \"showarrow\": False,\n                \"text\": \"\",\n                \"x\": 0.5,\n                \"y\": 0.5\n            }\n        ]\n    }\n}\npy.iplot(fig, filename='cars_donut')","dda822c5":"#cars.Evaluation.replace(('unacc', 'acc', 'good', 'vgood'), (0, 1, 2, 3), inplace = True)\n#cars.Buying.replace(('vhigh', 'high', 'med', 'low'), (3, 2, 1, 0), inplace = True)\n#cars.Maint.replace(('vhigh', 'high', 'med', 'low'), (3, 2, 1, 0), inplace = True)\n#cars.Doors.replace(('5more'),(5),inplace=True)\n#cars.Persons.replace(('more'),(5),inplace=True)\n#cars.LugBoot.replace(('small','med','big'),(0,1,2),inplace=True)\n#cars.Safety.replace(('low','med','high'),(0,1,2),inplace=True)","84fbec6d":"cars.Doors.replace(('5more'),('5'),inplace=True)\ncars.Persons.replace(('more'),('5'),inplace=True)\n","8370dbe3":"features = cars.iloc[:,:-1]\nfeatures[:5]\na=[]\nfor i in features:\n    a.append(features[i].value_counts())","4a6e85d4":"buy = pd.crosstab(cars['Buying'], cars['Evaluation'])\nmc = pd.crosstab(cars['Maint'], cars['Evaluation'])\ndrs = pd.crosstab(cars['Doors'], cars['Evaluation'])\nprsn = pd.crosstab(cars['Persons'], cars['Evaluation'])\nlb = pd.crosstab(cars['LugBoot'], cars['Evaluation'])\nsfty = pd.crosstab(cars['Safety'], cars['Evaluation'])\n","9c7c75bc":"buy","97060077":"data = [\n    go.Bar(\n        x=a[0].index, # assign x as the dataframe column 'x'\n        y=buy['unacc'],\n        name='Unacceptable'\n    ),\n    go.Bar(\n        x=a[0].index,\n        y=buy['acc'],\n        name='Acceptable'\n    ),\n    go.Bar(\n        x=a[0].index,\n        y=buy['good'],\n        name='Good'\n    ),\n    go.Bar(\n        x=a[0].index,\n        y=buy['vgood'],\n        name='Very Good'\n    )\n\n]\n\nlayout = go.Layout(\n    barmode='stack',\n    title='Selling Price vs Evaluation'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='distri')","f08bc6c7":"data = [\n    go.Bar(\n        x=a[0].index, # assign x as the dataframe column 'x'\n        y=mc['unacc'],\n        name='Unacceptable'\n    ),\n    go.Bar(\n        x=a[0].index,\n        y=mc['acc'],\n        name='Acceptable'\n    ),\n    go.Bar(\n        x=a[0].index,\n        y=mc['good'],\n        name='Good'\n    ),\n    go.Bar(\n        x=a[0].index,\n        y=mc['vgood'],\n        name='Very Good'\n    )\n\n]\n\nlayout = go.Layout(\n    barmode='stack',\n    title='Maintainance cost vs Evaluation'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='cars_donut')","1ad6931e":"data = [\n    go.Bar(\n        x=a[2].index, # assign x as the dataframe column 'x'\n        y=drs['unacc'],\n        name='Unacceptable'\n    ),\n    go.Bar(\n        x=a[2].index,\n        y=drs['acc'],\n        name='Acceptable'\n    ),\n    go.Bar(\n        x=a[2].index,\n        y=drs['good'],\n        name='Good'\n    ),\n    go.Bar(\n        x=a[2].index,\n        y=drs['vgood'],\n        name='Very Good'\n    )\n\n]\n\nlayout = go.Layout(\n    barmode='stack',\n    title='Doors vs Evaluation'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='cars_donut')","9bbfbb98":"data = [\n    go.Bar(\n        x=a[3].index, # assign x as the dataframe column 'x'\n        y=prsn['unacc'],\n        name='Unacceptable'\n    ),\n    go.Bar(\n        x=a[3].index,\n        y=prsn['acc'],\n        name='Acceptable'\n    ),\n    go.Bar(\n        x=a[3].index,\n        y=prsn['good'],\n        name='Good'\n    ),\n    go.Bar(\n        x=a[3].index,\n        y=prsn['vgood'],\n        name='Very Good'\n    )\n\n]\n\nlayout = go.Layout(\n    barmode='stack',\n    title='Number of Passengers vs Evaluation'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='cars_donut')","3291660f":"data = [\n    go.Bar(\n        x=a[4].index, # assign x as the dataframe column 'x'\n        y=lb['unacc'],\n        name='Unacceptable'\n    ),\n    go.Bar(\n        x=a[4].index,\n        y=lb['acc'],\n        name='Acceptable'\n    ),\n    go.Bar(\n        x=a[4].index,\n        y=lb['good'],\n        name='Good'\n    ),\n    go.Bar(\n        x=a[4].index,\n        y=lb['vgood'],\n        name='Very Good'\n    )\n\n]\n\nlayout = go.Layout(\n    barmode='stack',\n    title='Luggage Boot vs Evaluation'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='cars_donut')","13d04ee4":"data = [\n    go.Bar(\n        x=a[5].index, # assign x as the dataframe column 'x'\n        y=sfty['unacc'],\n        name='Unacceptable'\n    ),\n    go.Bar(\n        x=a[5].index,\n        y=sfty['acc'],\n        name='Acceptable'\n    ),\n    go.Bar(\n        x=a[5].index,\n        y=sfty['good'],\n        name='Good'\n    ),\n    go.Bar(\n        x=a[5].index,\n        y=sfty['vgood'],\n        name='Very Good'\n    )\n\n]\n\nlayout = go.Layout(\n    barmode='stack',\n    title='Safety vs Evaluation'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='cars_donut')","19865639":"#We need to encode the categorical data \n#We have two options, either we use label encoder or one hot encoder \n#We use label encoder when our target variable changes with increase or decrease in that feature variable \n#We use One hot encoder when a target variable depends upon the feature variable ","96433cd2":"#Dividing the dataframe into x features and y target variable\nx = cars.iloc[:, :-1]\ny = cars.iloc[:, 6]","e3e3f8a2":"x.columns = ['Buying', 'Maint', 'Doors','Persons','LugBoot','Safety']\ny.columns=['Evaluation']","7de4d4e2":"x.head()","d9427a52":"#Using pandas dummies function to encode the data into categorical data\nx = pd.get_dummies(x, prefix_sep='_', drop_first=True)","7d56b27f":"x.sample(5)","dfd35c2f":"y.describe()","78e9ff7e":"x=x.values\ny=y.values","2e036a86":"#And the rest of them to be categorically encoded: ['Buying', 'Maint', 'Doors', 'Persons','Safety','Evaluation']","f1847f46":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)\n","c4cdb977":"\"\"\"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\"\"\"","5dc2200b":"x_train[:5]","08974649":"y_train[:5]","6da32c62":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n","c42985b7":"#Using ogistic regression\nclf = LogisticRegression(random_state = 0)\nclf.fit(x_train, y_train)","076d3eb1":"y_pred = clf.predict(x_test)\nf1_LR=f1_score(y_test,y_pred, average='macro')\nprint(\"Training Accuracy: \",clf.score(x_train, y_train))\nprint(\"Testing Accuracy: \", clf.score(x_test, y_test))\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(classification_report(y_test,y_pred)) ","f44520dc":"#Using KNN classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclf.fit(x_train, y_train)","4f8255ac":"y_pred = clf.predict(x_test)\nf1_KNN=f1_score(y_test,y_pred, average='macro')\nprint(\"Training Accuracy: \",clf.score(x_train, y_train))\nprint(\"Testing Accuracy: \", clf.score(x_test, y_test))\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(classification_report(y_test,y_pred))","c3c53b93":"#Using Linear SVC\nfrom sklearn.svm import SVC\nclf = SVC(kernel = 'linear', random_state = 0)\nclf.fit(x_train, y_train)","9ac5de03":"y_pred = clf.predict(x_test)\nf1_SVC_Linear=f1_score(y_test,y_pred, average='macro')\nprint(\"Training Accuracy: \",clf.score(x_train, y_train))\nprint(\"Testing Accuracy: \", clf.score(x_test, y_test))\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(classification_report(y_test,y_pred))","c1339e28":"#Using rbf SVC\nfrom sklearn.svm import SVC\nclf = SVC(kernel = 'rbf', random_state = 0)\nclf.fit(x_train, y_train)","fb8e0f2f":"y_pred = clf.predict(x_test)\nf1_SVC_rbf=f1_score(y_test,y_pred, average='macro')\nprint(\"Training Accuracy: \",clf.score(x_train, y_train))\nprint(\"Testing Accuracy: \", clf.score(x_test, y_test))\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(classification_report(y_test,y_pred))","6d90fd3c":"#Using NB classifier\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(x_train, y_train)","ce63beca":"#GaussianNB?","bcc5c3af":"y_pred = clf.predict(x_test)\nprint(\"Training Accuracy: \",clf.score(x_train, y_train))\nprint(\"Testing Accuracy: \", clf.score(x_test, y_test))\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(classification_report(y_test,y_pred))","30cd02dd":"#Trying decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclf.fit(x_train, y_train)","c50e07a8":"y_pred = clf.predict(x_test)\nf1_DT=f1_score(y_test,y_pred, average='macro')\nprint(\"Training Accuracy: \",clf.score(x_train, y_train))\nprint(\"Testing Accuracy: \", clf.score(x_test, y_test))\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(classification_report(y_test,y_pred))","6ab0e415":"#Trying Random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators = 25, criterion = 'entropy', random_state = 0)\nclf.fit(x_train, y_train)","1051f3df":"y_pred = clf.predict(x_test)\nf1_RF=f1_score(y_test,y_pred, average='macro')\nprint(\"Training Accuracy: \",clf.score(x_train, y_train))\nprint(\"Testing Accuracy: \", clf.score(x_test, y_test))\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(classification_report(y_test,y_pred))","9bdfc354":"#Now trying the NB classifier again, this time without dummy variables \nx_new = cars.iloc[:,:-1]","6f3964c6":"from sklearn.preprocessing import LabelEncoder","def2ab57":"lae = LabelEncoder()\nx_new=x_new.apply(lambda col: lae.fit_transform(col))\nx_new.head()","ae8a3c1f":"x_new=x_new.values","55dbb5d4":"x_train_new, x_test_new= train_test_split(x_new, test_size = 0.25, random_state = 0)\n","00189a75":"clf_new = GaussianNB(priors=None)\nclf_new.fit(x_train_new, y_train)","1947947d":"y_train[:10]","a8d93d33":"y_pred = clf_new.predict(x_test_new)\nf1_NB=f1_score(y_test,y_pred, average='macro')\nprint(\"Training Accuracy: \",clf_new.score(x_train_new, y_train))\nprint(\"Testing Accuracy: \", clf_new.score(x_test_new, y_test))\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(classification_report(y_test,y_pred))","8e49f237":"models=['Linear SVC', 'Kernel SVC','Logistic Regression','Decision Tree Classifier','Random Forest Classifier','Naive Bayes Classifier' ]\nfig = go.Figure(data=[\n    go.Bar(name='f1_score', x=models, y=[f1_SVC_Linear,f1_SVC_rbf,f1_LR,f1_DT,f1_RF,f1_NB])])\nfig.show()","54b206a3":"The in depth analysis and dscription for the project can be found in the blogs :\n\n[The Classifier Part 1](http:\/\/medium.com\/@harjotspahwa\/the-classifier-part-1-2-18f3c70d01fe)\n\n[The Classifier Part 2](http:\/\/medium.com\/@harjotspahwa\/the-classifier-part-2-2-38df3de1f9b3)","480bf22a":"Note that this is WRONG implementation of Naive Bayes classifier. Since the Independence assumption of NB classifier states that the features shoud not be correlated to each other; so when creating the dummy variables, we make family of dependent features and hence we get such a terrible accuracy. So after trying out a couple more algorithms, I've done this one properly :)"}}