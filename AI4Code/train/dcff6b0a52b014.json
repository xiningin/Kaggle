{"cell_type":{"e2a0346f":"code","1026edbb":"code","df9be98d":"code","19f90efe":"code","27ebd3a3":"code","c9643156":"code","8dc8428b":"code","e69b1aa6":"code","0751f38c":"code","c6205836":"markdown","fb452a1a":"markdown","cda85e82":"markdown","7f6ebb00":"markdown","492b1ba5":"markdown"},"source":{"e2a0346f":"import hashlib\nimport os\nimport pathlib\nimport platform\nimport sys\nimport warnings\n\nimport pandas as pd\n\nfrom tqdm import tqdm","1026edbb":"# Filter warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')","df9be98d":"# Get working directory\ntry:\n    path_working_dir = os.path.abspath(os.path.dirname(os.path.abspath(__file__)))\nexcept:\n    path_working_dir = os.path.abspath(str(pathlib.Path().resolve()))","19f90efe":"# Set input directory\npath_input  = '..\/input\/rsna-intracranial-hemorrhage-detection'\npath_output = '.\/'\nassert os.path.exists(path_input) == True","27ebd3a3":"# Calc hashsum of files with selected file extension\nlist_output = []\nlist_ext    = ['.csv', '.dcm']\nfor (dirpath, dirnames, filenames) in os.walk(path_input):\n    print(dirpath.replace(path_input, '.'), file=sys.stderr)\n    for filename in tqdm(filenames):\n        if max([filename.find(ext) for ext in list_ext]) > -1:\n            with open(os.path.join(dirpath, filename), 'rb') as fp:\n                fp_read = fp.read()\n                list_output.append([os.path.join(dirpath.replace(path_input, '.'), filename), hashlib.md5(fp_read).hexdigest(), hashlib.sha1(fp_read).hexdigest()])","c9643156":"# Output result\ndf_output = pd.DataFrame(list_output, columns=['filename', 'md5', 'sha1'])\ndf_output = df_output.sort_values(by='filename')\ndf_output.to_csv(os.path.join(path_output, 'df_output.csv'), index=False)","8dc8428b":"df_output.head()","e69b1aa6":"!md5sum df_output.csv","0751f38c":"!sha1sum df_output.csv","c6205836":"### Change the next cell in your local-machine","fb452a1a":"## Let's enjoy this competition! \ud83c\udf89","cda85e82":"# Let's verify the downloaded files not broken\n\n* I'll use MD5 and SHA1 hasusum to verify downloaded files and Kaggle Notebook's files are same.\n* To use, you run this notebook in your local-machine, and check df_output.csv's hashsume are same in this notebook.\n* Check your dataset's filepath in your local-machine. **Please check the 4th cell.**","7f6ebb00":"## Calculate hashsum","492b1ba5":"## Check df_output.csv's hashsum\n* `md5sum` and `sha1sum` commands can use in Ubuntu\n* `md5` and `openssl sha1` commands can use in OS X\n* `certutil -hashfile df_output.csv MD5` and `certutil -hashfile df_output.csv SHA1` commands can use in Windows 7-10"}}