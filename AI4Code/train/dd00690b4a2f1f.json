{"cell_type":{"f6fdd94a":"code","84535c62":"code","031d1de6":"code","44164eb9":"code","6704e355":"code","a2a93095":"code","6ab8de8f":"code","17c27d71":"code","664eef01":"code","8d6245bb":"code","708a09f3":"code","52f14420":"markdown","26b3cd3d":"markdown"},"source":{"f6fdd94a":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations,callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import initializers\nimport tqdm as tqdm\nfrom keras.models import Model","84535c62":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')","031d1de6":"target = pd.get_dummies(train['target'])","44164eb9":"class Decision_Tree(keras.Model):\n    def __init__(self, depth, num_features, used_features_rate, num_classes):\n        super(Decision_Tree, self).__init__()\n        self.depth = depth\n        self.num_leaves = 2 ** depth\n        self.num_classes = num_classes\n        num_used_features = int(num_features * used_features_rate)    \n        one_hot = np.eye(num_features)                                \n        sampled_feature_indicies = np.random.choice(\n            np.arange(num_features), num_used_features, replace=False\n        )                                                            \n        self.used_features_mask = one_hot[sampled_feature_indicies]   \n        self.pi = tf.Variable(\n            initial_value = tf.random_normal_initializer()(\n            shape = [self.num_leaves, self.num_classes]\n            ),\n            dtype=\"float32\",\n            trainable=True,\n        )\n        \n        self.decision_fn = layers.Dense(\n            units=self.num_leaves, \n            activation=\"sigmoid\", \n            name=\"decision\"\n            )\n\n    def call(self, features):\n        batch_size = tf.shape(features)[0]\n        features = tf.matmul(\n            features, \n            self.used_features_mask, \n            transpose_b=True\n            )  \n        decisions = tf.expand_dims(\n            self.decision_fn(features),\n            axis=2\n            )  \n        decisions = layers.concatenate(\n            [decisions, 1 - decisions],\n            axis=2\n            ) \n\n        mu = tf.ones([batch_size, 1, 1]) \n        begin_idx = 1\n        end_idx = 2\n\n        for level in range(self.depth):\n            mu = tf.reshape(mu, [batch_size, -1, 1])  \n            mu = tf.tile(mu, (1, 1, 2))  \n            level_decisions = decisions[:, begin_idx:end_idx, :]  \n            mu = mu * level_decisions  \n            begin_idx = end_idx\n            end_idx = begin_idx + 2 ** (level + 1)\n\n        mu = tf.reshape(mu, [batch_size, self.num_leaves])  \n        probabilities = keras.activations.softmax(self.pi)  \n        outputs = tf.matmul(mu, probabilities) \n        \n        return outputs","6704e355":"class Decision_Forest(keras.Model):\n    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n        super(Decision_Forest, self).__init__()\n        self.ensemble = []\n        self.num_classes = num_classes\n        \n        for _ in range(num_trees):\n            self.ensemble.append(\n                Decision_Tree(depth, \n                              num_features,\n                              used_features_rate,\n                              self.num_classes)\n                                )\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        outputs = tf.zeros([batch_size, \n                            num_classes])\n        \n        for tree in self.ensemble:\n            outputs += tree(inputs)\n            \n        outputs \/= len(self.ensemble)\n        \n        return outputs\n\nnum_trees = 30\n#depth = 5\ndepth = 6\n\nused_features_rate = 0.5\nnum_classes = 9\nnum_features = 30\nforest_model = Decision_Forest(\n                        num_trees,\n                        depth, \n                        num_features,\n                        used_features_rate,\n                        num_classes\n                        )","a2a93095":"metrics = [tf.keras.metrics.CategoricalCrossentropy()]\nloss = tf.keras.losses.CategoricalCrossentropy()\n\nes = tf.keras.callbacks.EarlyStopping(\n    #monitor='val_loss', min_delta=0.0000001, patience=2, verbose=0,\n    monitor='val_loss', min_delta=0.0000001, patience=10, verbose=0,\n\n    mode='min', baseline=None, restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n   # monitor='val_loss', factor=0.5, patience=2, verbose=0,\n    monitor='val_loss', factor=0.5, patience=10, verbose=0,\n\n    #mode='min', min_delta=0.0000001, cooldown=0, min_lr=10e-7)\n    mode='min', min_delta=0.0000001, cooldown=0, min_lr=10e-7)","6ab8de8f":"N_FOLDS = 10\nSEED = 2021\noof_embedding = np.zeros((train.shape[0],9))\npred_embedding = np.zeros((test.shape[0],9))\noof_forest = np.zeros((train.shape[0],9))\npred_forest = np.zeros((test.shape[0],9))\n\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (tr_idx, ts_idx) in enumerate(skf.split(train,train.iloc[:,-1])):\n    print(f\"\\n===== TRAINING FOLD {fold} =====\\n\")\n       \n    X_train = train.iloc[:,1:-1].iloc[tr_idx]\n    y_train = target.iloc[tr_idx]\n    X_test = train.iloc[:,1:-1].iloc[ts_idx]\n    y_test = target.iloc[ts_idx]\n    \n\n    #----------NN Model definition ----------\n    \n    inp = layers.Input(shape = (75,))\n    x = layers.Embedding(400, 8, input_length = 256)(inp)\n    x = layers.Flatten()(x)\n    # API is the future imput layer for decision forest :\n    API = layers.Dense(30, \n                       activation='relu',\n                       kernel_initializer='random_uniform',\n                       bias_initializer=initializers.Constant(0.1))(x)\n    x = layers.Dropout(0.3)(API)\n    x = layers.Dense(50, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(30, activation = 'relu')(x)\n    output = layers.Dense(9, activation = 'softmax')(x)\n\n    #----------Models instantiation ---------\n\n    model_embedding = Model(inp,output)\n    model_embedding_without_head = tf.keras.models.Model(inputs=model_embedding.inputs,outputs=API)\n    model_forest = Model(inp,forest_model(API))\n\n    #----------NN Model training ------------\n\n    model_embedding.compile(tf.keras.optimizers.Adam(learning_rate=0.0001),\n            loss = loss ,\n            metrics = metrics)\n\n    model_embedding.fit(X_train,y_train,\n            validation_data=(X_test,y_test),\n            epochs=150,\n            verbose=0,\n            batch_size = 256,\n            callbacks=[es,plateau])\n     \n    #----------NN Model prediction------------\n    \n    oof_embedding[ts_idx] = model_embedding.predict(X_test)\n    score_embedding = log_loss(y_test, oof_embedding[ts_idx])\n    print(f\"\\nFOLD {fold} Score for NN model {score_embedding}\\n\")\n    pred_embedding += model_embedding.predict(test.iloc[:,1:]) \/ N_FOLDS\n    \n    \n    #----------Model forest training -----------\n    \n    model_forest.compile(tf.keras.optimizers.Adam(learning_rate=0.001),\n            loss = loss,\n            metrics = metrics)\n    \n    model_forest.fit(X_train,y_train,\n                    validation_data = (X_test,y_test),\n                    batch_size = 256,\n                    epochs = 50,\n                    verbose = 0,\n                    callbacks = [es,plateau])\n    \n    #----------Model forest prediction------------ \n        \n    oof_forest[ts_idx] = model_forest.predict(X_test)\n    score_forest = log_loss(y_test, oof_forest[ts_idx])\n    print(f\"\\nFOLD {fold} Score for decision forest : {score_forest}\\n\")\n    \n    pred_forest += model_forest.predict(test.iloc[:,1:]) \/ N_FOLDS\n    \nscore_embedding = log_loss(target, oof_embedding)\nprint(f\"\\n=== FINAL SCORE FOR NN MODEL : {score_embedding}===\\n\")   \n\nscore_forest = log_loss(target, oof_forest)\nprint(f\"\\n=== FINAL SCORE FOR DECISION FOREST : {score_forest}===\\n\")  ","17c27d71":"submission1 = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\nsubmission1['Class_1']=pred_forest[:,0]\nsubmission1['Class_2']=pred_forest[:,1]\nsubmission1['Class_3']=pred_forest[:,2]\nsubmission1['Class_4']=pred_forest[:,3]\nsubmission1['Class_5']=pred_forest[:,4]\nsubmission1['Class_6']=pred_forest[:,5]\nsubmission1['Class_7']=pred_forest[:,6]\nsubmission1['Class_8']=pred_forest[:,7]\nsubmission1['Class_9']=pred_forest[:,8]","664eef01":"submission1.to_csv(\"Keras_forest.csv\", index=False)","8d6245bb":"submission2 = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\nsubmission2['Class_1']=pred_embedding[:,0]\nsubmission2['Class_2']=pred_embedding[:,1]\nsubmission2['Class_3']=pred_embedding[:,2]\nsubmission2['Class_4']=pred_embedding[:,3]\nsubmission2['Class_5']=pred_embedding[:,4]\nsubmission2['Class_6']=pred_embedding[:,5]\nsubmission2['Class_7']=pred_embedding[:,6]\nsubmission2['Class_8']=pred_embedding[:,7]\nsubmission2['Class_9']=pred_embedding[:,8]","708a09f3":"submission2.to_csv(\"Keras_embedding.csv\", index=False)","52f14420":"<h3> Decision Forest building (\"concatenation\" of decision trees)\n\n> Indented block","26b3cd3d":"**<h3> Definition model and training + prediction**"}}