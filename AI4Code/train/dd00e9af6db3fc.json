{"cell_type":{"dbb3dcb6":"code","58fe9809":"code","6e73edec":"code","515ac637":"code","9fd84ea1":"code","fbe8762f":"code","c04da52b":"code","b76b5c0d":"code","40b3151d":"code","2a3cab5b":"code","0d15ddc0":"code","415da481":"code","9cb70d86":"code","4c23c49c":"code","a27a5714":"code","377292b3":"code","1e54d34a":"code","64ce0db6":"code","d9364c5b":"code","19b70dc9":"code","2c821275":"code","ec1e1c7c":"code","f03ab35d":"code","54ee53aa":"code","89f49e00":"code","4b9aca77":"code","1b657166":"code","38a56108":"code","079b1b3b":"code","7719ec10":"code","d54818cf":"code","95b2b1ea":"code","957ac414":"code","c37df7d4":"code","11e2aabf":"code","35c08546":"code","0d12bc89":"code","f28deea2":"code","89e0dfbb":"code","73c40a3f":"code","21b87a6c":"code","b11f030f":"code","66c22c86":"code","d15952d3":"code","d0208d76":"code","450324b4":"code","9c9b32fd":"code","9e2923ff":"code","9d488d78":"code","167a9db3":"code","365338a7":"code","3b1079e8":"code","14bc5ad3":"code","3927a31c":"code","c8cc2d53":"code","39f937bf":"code","942c8e09":"code","8ee36b6f":"code","311774de":"code","7ba51dd7":"code","e0cf176c":"code","6fc1c9aa":"markdown","2ea65591":"markdown","519fb965":"markdown","a3de9a48":"markdown","e318979c":"markdown","2dc86f25":"markdown","ed9ba7d3":"markdown","dc89afb8":"markdown","91863ff0":"markdown"},"source":{"dbb3dcb6":"#import all we needed module\nimport numpy as np\nimport pandas as pd \nimport sqlite3\nimport matplotlib.pyplot as plt\nfrom  sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\n#from sklearn import cross_validation******** this is not working\nfrom collections import Counter\nfrom sklearn.naive_bayes import MultinomialNB","58fe9809":"import os\nprint(os.listdir(\"..\/input\/\")) ","6e73edec":"con = sqlite3.connect('..\/input\/amazon-fine-food-reviews\/database.sqlite')\n\n# we neglect the review having a score = 3\n\nfiltered_data = pd.read_sql_query('''select *from reviews where Score !=3''',con)\n\ndef partition(x):\n    if x<3 :\n        return 'negative'\n    return 'positive'\nactualScore = filtered_data['Score']\npositiveNegative = actualScore.map(partition)\nfiltered_data['Score'] = positiveNegative","515ac637":"display = pd.read_sql_query('''select * from reviews where Score != 3 and userId = \"AR5J8UI46CURR\" order by ProductID''',con)\ndisplay.head()","9fd84ea1":"final = sorted_data.drop_duplicates(subset={'UserId','ProfileName','Time','Text'})\nfinal.shape","fbe8762f":"sorted_data = filtered_data.sort_values('ProductId',axis = 0,ascending=True,inplace = False,kind='quicksort',na_position='last')","c04da52b":"#we remove duplication using HelpfulnessDenominator and HelpfulnessNumerator.\n\nfinal = final[final.HelpfulnessNumerator<= final.HelpfulnessDenominator]","b76b5c0d":"import re \n\ni = 0\nfor sent in final['Text'].values:\n    if(len(re.findall('<.>*?',sent))):\n        print(i)\n        print(sent)\n        break;\n    i+=1","40b3151d":"import string\nimport re\nimport nltk\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nstop = set(stopwords.words('english')) #set of stopwords\nsno = nltk.stem.SnowballStemmer('english') # initialising snowball stemmer\n\ndef cleanhtml(sentence): #function to clean word of any html tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr , ' ',sentence)\n    return cleantext\ndef cleanpunc(sentence): #function to clean word of any punctuation or special character\n    cleaned = re.sub(r'[?|!|\\,|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r'',cleaned)\n    return cleaned\nprint(stop)\nprint('***********************************************')\nprint(sno.stem('tasty'))","2a3cab5b":"i = 0\nstrl = ' '\nfinal_string  = []\nall_positive_words=[] # store words from +ve reviews here\nall_negattive_words=[]#store words from -ve reviews here\ns= ''\n\nfor sent in final['Text'].values:\n    filtered_sentence = []\n    sent = cleanhtml(sent) #remove html tag\n    \n    for w in sent.split():\n        for cleaned_word in cleanpunc(w).split():\n            if((cleaned_word.isalpha()) & (len(cleaned_word)>2)):\n                if(cleaned_word.lower() not in stop):\n                    s = (sno.stem(cleaned_word.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if(final['Score'].values)[i] == 'positive':\n                        all_positive_words.append(s) #list of all words use to store +ve list \n                    if (final['Score'].values)[i] == 'negative':\n                        all_negattive_words.append(s) #list of all words use to store -Ve list\n                else:\n                    continue\n            else:\n                continue\n                \n    #print filtered sentens \n    strl = b\" \".join(filtered_sentence) #final string of cleaned words\n    final_string.append(strl)\n    i+=1","0d15ddc0":"final['CleanedText']=final_string","415da481":"final.head(3) #below the processed review can be seen in the CleanedText Column \n\n\n# store final table into an SQlLite table for future.\nconn = sqlite3.connect('finalassignment.sqlite')\nc=conn.cursor()\nconn.text_factory = str\nfinal.to_sql('Reviews', conn, schema=None, if_exists='replace', index=True, index_label=None, chunksize=None, dtype=None)","9cb70d86":"import sqlite3\ncon = sqlite3.connect('finalassignment.sqlite')","4c23c49c":"cleaned_data = pd.read_sql_query('select * from Reviews', con)","a27a5714":"cleaned_data.shape","377292b3":"cleaned_data['Score'].value_counts()","1e54d34a":"# To randomly sample 5k points from both class\n\ndata_p = cleaned_data[cleaned_data['Score'] == 'positive'].sample(n = 5000)\ndata_n = cleaned_data[cleaned_data['Score'] == 'negative'].sample(n = 5000)\nfinal_10k = pd.concat([data_p, data_n])\nfinal_10k.shape","64ce0db6":"# Sorting data based on time\nfinal_10k['Time'] = pd.to_datetime(final_10k['Time'], unit = 's')\nfinal_10k = final_10k.sort_values(by = 'Time')","d9364c5b":"# function compute the alpha value \n\ndef naive_bayes(X_train , y_train):\n    \n    alpha_value = np.arange(1,500,0.5)\n    \n    # empty list that will hold cv value\n    cv_scores = []\n    \n    #perform 10-fold cross validation\n    for alpha in alpha_value:\n        mnb = MultinomialNB(alpha = alpha)\n        scores = cross_val_score(mnb , X_train , y_train , cv = 10 , scoring = 'accuracy')\n        cv_scores.append(scores.mean())\n        \n    # changing misclassification error\n    MSE = [1 - x for x in cv_scores]\n    \n    #determining best alpha\n    optimal_alpha = alpha_value[MSE.index(min(MSE))]\n    print('\\nThe optimal number of alpha is %d :'% optimal_alpha)\n    \n    #plot misclassification error vs alpha\n    plt.plot(alpha_value ,MSE , marker = '*')\n    \n   \n    #for xy in zip(alpha_values, np.round(MSE,3)):\n        #plt.annotate('(%s, %s)' % xy, xy=xy, textcoords='data')\n    plt.title(\"Misclassification Error vs alpha\")\n    plt.xlabel('value of alpha')\n    plt.ylabel('Misclassification Error')\n    plt.show()\n\n    #print(\"the misclassification error for each value of alpha is : \", np.round(MSE,3))\n    return optimal_alpha","19b70dc9":"# 10k data which will use to train model after vectorization\nX = final_10k[\"CleanedText\"]\nprint(\"shape of X:\", X.shape)","2c821275":"# class label\ny = final_10k[\"Score\"]\nprint(\"shape of y:\", y.shape)","ec1e1c7c":"# split data into train and test where 70% data used to train model and 30% for test\n\nfrom sklearn.model_selection import train_test_split\nX_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\nprint(X_train.shape, y_train.shape, x_test.shape , y_test.shape)","f03ab35d":"# Train Vectorizor\nfrom sklearn.feature_extraction.text import CountVectorizer \n\nbow = CountVectorizer()\nX_train = bow.fit_transform(X_train)\nX_train","54ee53aa":"# convert test text data to its vectorizor\nx_test = bow.transform(x_test)","89f49e00":"x_test.shape","4b9aca77":"# To choose optimal_alpha using cross validation\n\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import model_selection\n\n\noptimal_alpha_bow = naive_bayes(X_train, y_train)\noptimal_alpha_bow","1b657166":"# instantiate learning model alpha = optimal_alpha\nnb_optimal =  MultinomialNB(alpha = optimal_alpha_bow)\n\n# fitting the model\nnb_optimal.fit(X_train, y_train)\n#knn_optimal.fit(bow_data, y_train)\n\n# predict the response\npred = nb_optimal.predict(x_test)","38a56108":"# to get all feature name\n\nbow_features = bow.get_feature_names()","079b1b3b":"# To count feature for each class while fitting the model\n# Number of samples encountered for each (class, feature) during fitting\n\nfeat_count = nb_optimal.feature_count_\nfeat_count.shape","7719ec10":"# Number of samples encountered for each class during fitting\n\nnb_optimal.class_count_","d54818cf":"# Empirical log probability  of feature given a class \n\nlog_prob = nb_optimal.feature_log_prob_\nlog_prob","95b2b1ea":"feature_prob = pd.DataFrame(log_prob , columns = bow_features)\nfeature_prob_tr = feature_prob.T\nfeature_prob_tr.shape","957ac414":"# to show top 10 feature from both class\n#feature Impportance\n\nprint('Top 10 Negative Feature :', feature_prob_tr[0].sort_values(ascending = False)[0:10])\nprint('------------------------------------------------------------------------------------')\nprint('Top 10 Postive Feature : ', feature_prob_tr[1].sort_values(ascending = False)[0:10])","c37df7d4":"# Accuracy on train data\ntrain_acc_bow = nb_optimal.score(X_train, y_train)\nprint(\"Train accuracy : %f%%\" % (train_acc_bow))","11e2aabf":"# Error on train data\ntrain_err_bow = 1-train_acc_bow \nprint(\"Train Error %f%%\" % (train_err_bow))","35c08546":"# evaluate accuracy on test data\nacc_bow = accuracy_score(y_test, pred) * 100\nprint('\\nThe accuracy of the naive bayes classifier for alpha = %d is %f%%' % (optimal_alpha_bow, acc_bow))","0d12bc89":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, pred)\ncm","f28deea2":"# plot confusion matrix to describe the performance of classifier.\nimport seaborn as sns\nclass_label = [\"negative\", \"positive\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\")\nplt.title(\"Confusiion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","89e0dfbb":"# To show main classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","73c40a3f":"# model for knn with bag of word\nmodels = pd.DataFrame({'Model': ['Naive Bayes with Bow'], 'Hyper Parameter(K)': [optimal_alpha_bow], 'Train Error': [train_err_bow], 'Test Error': [100-acc_bow], 'Accuracy': [acc_bow ], 'Train Accuracy': [train_acc_bow ]}, columns = [\"Model\", \"Hyper Parameter(K)\", \"Train Error\", \"Test Error\", \"Accuracy\" , \"Train Accuracy\"])\nmodels.sort_values(by='Accuracy', ascending=False)","21b87a6c":"# # 10k data which will use to train model after vectorization\n\nX = final_10k['CleanedText']\nX.shape","b11f030f":"# target \/ class label\n\ny = final_10k['Score']\ny.shape","66c22c86":"#split data\n\nX_train , x_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state = 42)\nprint(X_train.shape , x_test.shape , y_train.shape , y_test.shape)","d15952d3":"# Train Vectorizor\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\nX_train = tf_idf_vect.fit_transform(X_train)\nX_train","d0208d76":"# convert test data to its vectorizor\n\nx_test = tf_idf_vect.transform(x_test)\nx_test.shape","450324b4":"# to chossing optimal alpha using cv\n\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import model_selection\n\n\noptimal_alpha_tfidf = naive_bayes(X_train, y_train)\noptimal_alpha_tfidf","9c9b32fd":"# instantiate learning model alpha = optimal_alpha\nnb_optimal = MultinomialNB(alpha = optimal_alpha_tfidf)\n\n# fitting the model\nnb_optimal.fit(X_train, y_train)\n#knn_optimal.fit(bow_data, y_train)\n    \n# predict the response\npred = nb_optimal.predict(x_test)","9e2923ff":"# To get all the features name \n\ntfidf_features = tf_idf_vect.get_feature_names()","9d488d78":"# To count feature for each class while fitting the model\n# Number of samples encountered for each (class, feature) during fitting\n\nfeat_count = nb_optimal.feature_count_\nfeat_count.shape","167a9db3":"# Number of samples encountered for each class during fitting\n\nnb_optimal.class_count_","365338a7":"# Empirical log probability of features given a class(i.e. P(x_i|y))\n\nlog_prob = nb_optimal.feature_log_prob_\nlog_prob","3b1079e8":"feature_prob = pd.DataFrame(log_prob, columns = tfidf_features)\nfeature_prob_tr = feature_prob.T\nfeature_prob_tr.shape","14bc5ad3":"# to show top 10 feature from both class\n#feature Impportance\n\nprint('Top 10 Negative Feature :', feature_prob_tr[0].sort_values(ascending = False)[0:10])\nprint('------------------------------------------------------------------------------------')\nprint('Top 10 Postive Feature : ', feature_prob_tr[1].sort_values(ascending = False)[0:10])","3927a31c":"# Accuracy on train data\ntrain_acc_tfidf = nb_optimal.score(X_train, y_train)\nprint(\"Train accuracy : %f%%\" % (train_acc_tfidf))","c8cc2d53":"# Error on train data\ntrain_err_tfidf = 1-train_acc_tfidf\nprint(\"Train Error %f%%\" % (train_err_tfidf))","39f937bf":"# evaluate accuracy\nacc_tfidf = accuracy_score(y_test, pred) * 100\nprint('\\nThe accuracy of the naive bayes classifier for alpha = %d is %f%%' % (optimal_alpha_tfidf, acc_tfidf))","942c8e09":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, pred)\ncm","8ee36b6f":"import seaborn as sns\nclass_label = [\"negative\", \"positive\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","311774de":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","7ba51dd7":"# model for knn with bag of word\nmodels = pd.DataFrame({'Model': ['Naive Bayes with Tf-Idf'], 'Hyper Parameter(K)': [optimal_alpha_tfidf], 'Train Error': [train_err_tfidf], 'Test Error': [100-acc_tfidf], 'Accuracy': [acc_tfidf ], 'Train Accuracy': [train_acc_tfidf ]}, columns = [\"Model\", \"Hyper Parameter(K)\", \"Train Error\", \"Test Error\", \"Accuracy\" , \"Train Accuracy\"])\nmodels.sort_values(by='Accuracy', ascending=False)","e0cf176c":"# model performence table\nmodels = pd.DataFrame({'Model': ['Naive Bayes with Bow', \"Naive Bayes with TFIDF\"], 'Hyper Parameter(alpha)': [optimal_alpha_bow, optimal_alpha_tfidf], 'Train Error': [train_err_bow, train_err_tfidf], 'Test Error': [100-acc_bow, 100-acc_tfidf], 'Accuracy': [acc_bow, acc_tfidf]}, columns = [\"Model\", \"Hyper Parameter(alpha)\", \"Train Error\", \"Test Error\", \"Accuracy\"])\nmodels.sort_values(by='Accuracy', ascending=False)","6fc1c9aa":"Now above six step is find probabilit for any word so this not impect in train and test acc...\nthis for only understanding purpose......................","2ea65591":"**Exploratory Data Analysis(EDA)**","519fb965":"now this six stape not need find for find train and test accuracy but only find to log proobability and our understading.\n","a3de9a48":"**Conclusions:-**\n1. Naive bayes are good at text classification task like spam filtering, sentimental analysis, RS etc. \n2. As we know when a model performs good on training data but poor performence on unseen data(test data)i.e. its dependent on training data only, tends to overfits and when a model perform poor performence on training data and good performence on test data i.e. it fails to learn relationship in training data tends to underfit. We need to balance between both i.e. reduce training error and balance error between both training and testing which is balanced in this case.\n3. Another concept bias vs variance is also related with underfitting and overfitting. when a model has high bias and low variance tend to underfitting and its reverse- high variance and low bias called overfitting and we balanced using cross-validataion. As it is shown in below table where both models have low trainig error and test error.\n4. overall, both of the models are performing well on unseen data.\n5. As we are not applying naive bayes on word2vec representation because it sometimes gives -ve value(i.e. if two word have 0 cosine similarity the word is completly orthogonal i.e. they are not related with each other. and 1 represents perfect relationship between word vector. whereas -ve similarity means they are perfect opposite relationship between word) and we know naive bayes assume that presence of a particular feature in a class is unrelated to presence of any other feature, which is most unlikely in real word. Although, it works well.\n6. And from point # 5, features are dependent or there are relationship between features. So applying naive bayes on dependent feature does not make any sense.","e318979c":"# Amazon Fine Food Review- Using Naive Bayes\n","2dc86f25":"#  Bag Of Word (BOW)","ed9ba7d3":"Now above 6 line is only for log probanility....\nwithout use this we find our accuracy and error also....\nthis is for only understanding purpose...........\nnow we write a accuracy code...","dc89afb8":"# TF - IDF","91863ff0":"we find to log probability so its not effect on accuracy data........"}}