{"cell_type":{"b645443c":"code","2329dec7":"code","48e79eaa":"code","2e0d091d":"code","0595404a":"code","e0692220":"code","e0645d0d":"code","bd59f1fc":"code","caba646b":"code","78d0684e":"code","c141cf5a":"code","257a0dc3":"code","d38de121":"code","72feb55b":"code","41eff995":"code","74ed3da4":"code","35253309":"code","205de456":"code","42729846":"code","85e4b849":"code","fce06b04":"code","23633fe4":"code","773c8001":"code","7ec24885":"code","05a59da4":"code","38367694":"code","a57d2403":"code","609117fd":"code","9c9ebcbd":"code","db1fb736":"code","82e80649":"code","cc72688a":"code","b8583deb":"code","c049770a":"code","3e1fea09":"code","957eab58":"code","3af891b3":"code","32f26aef":"code","2626cd0d":"code","35471ad8":"code","152c6af6":"code","d9d7d095":"code","eb856a69":"code","e5ef452c":"code","9e905286":"code","ea89e75a":"code","4292ba97":"code","674a9792":"code","22b61ea2":"code","d273948b":"code","26f3774c":"code","4176ce0c":"code","cd9484b2":"code","41a0177a":"code","10ab9ac4":"code","6eaff2e4":"code","9c784ec0":"code","87da0919":"code","b7068b4d":"code","5f4dc8e7":"code","cdaec720":"code","ef9faefb":"markdown","1195d4d3":"markdown","de6baf17":"markdown","6177a320":"markdown","0d6a3bec":"markdown","e550cf5a":"markdown","23bf37af":"markdown","9c93605e":"markdown","6a0c6bd2":"markdown","b275f683":"markdown","24d53884":"markdown","04e377f5":"markdown","9fdb288d":"markdown","0362181e":"markdown","11ac64c7":"markdown","c03d7503":"markdown","0c9fe58b":"markdown","a307ff36":"markdown","1cdfdf99":"markdown","239ef967":"markdown","5d4df05d":"markdown","eda07746":"markdown","518b65ad":"markdown","34330ef6":"markdown","8739973a":"markdown","3fc5cabf":"markdown","6dae7315":"markdown","5ac81cd5":"markdown","cc6c1f05":"markdown","f1688656":"markdown","29544b7d":"markdown","a6522eca":"markdown","8d96b1a3":"markdown","b00b93a6":"markdown","b28847e1":"markdown","7f1362f4":"markdown","18a1a06e":"markdown","14145019":"markdown","d0a47b9b":"markdown"},"source":{"b645443c":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')","2329dec7":"Data_train = pd.read_csv(r'..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nData_test = pd.read_csv(r'..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_id = Data_train['Id']\ntest_id = Data_test['Id']","48e79eaa":"Data_all = pd.concat([Data_train, Data_test])\nprint(Data_all.shape)\nData_all = Data_all.drop(labels='Id', axis=1)\nData_all.head(10)","2e0d091d":"var = 'GrLivArea'\ndata = pd.concat([Data_all['SalePrice'], Data_all[var]], axis=1) # axis=0, the columns are connected along\ndata.plot.scatter(x=var, y='SalePrice')","0595404a":"print(Data_all.shape)","e0692220":"sns.distplot(Data_train['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(Data_train['SalePrice'], plot=plt)","e0645d0d":"Data_train['SalePrice'] = np.log1p(Data_train['SalePrice'])\n\nsns.distplot(Data_train['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(Data_train['SalePrice'], plot=plt)","bd59f1fc":"y_train = Data_train.SalePrice.values","caba646b":"Data_all.drop(['SalePrice'], axis=1, inplace=True)\nprint(Data_all.shape)\nData_all.head()","78d0684e":"Total_na = Data_all.isnull().sum().sort_values(ascending=False)\nPercent_na = (Data_all.isnull().sum()\/Data_all.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([Total_na, Percent_na], axis=1, keys=['Total','Percent'])\nmissing_data.head(35)","c141cf5a":"corrmat = Data_train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9)","257a0dc3":"Data_all['PoolQC'] = Data_all['PoolQC'].fillna('None')\nData_all['MiscFeature'] = Data_all['MiscFeature'].fillna('None')\nData_all['Alley'] = Data_all['Alley'].fillna('None')\nData_all['Fence'] = Data_all['Fence'].fillna('None')\nData_all['FireplaceQu'] = Data_all['FireplaceQu'].fillna('None')","d38de121":"Data_all['LotFrontage'] = Data_all.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","72feb55b":"for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    Data_all[col] = Data_all[col].fillna('None')","41eff995":"for col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n    Data_all[col] = Data_all[col].fillna(0)","74ed3da4":"for col in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n    Data_all[col] = Data_all[col].fillna(0)","35253309":"for col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n    Data_all[col] = Data_all[col].fillna('None')","205de456":"Data_all['MasVnrType'] = Data_all['MasVnrType'].fillna('None')\nData_all['MasVnrArea'] = Data_all['MasVnrArea'].fillna(0)","42729846":"Data_all['MSZoning'] = Data_all['MSZoning'].fillna(Data_all['MSZoning'].mode()[0])","85e4b849":"Data_all = Data_all.drop(['Utilities'], axis=1)","fce06b04":"Data_all['Functional'] = Data_all['Functional'].fillna('Typ')","23633fe4":"Data_all['Electrical'] = Data_all['Electrical'].fillna(Data_all['Electrical'].mode()[0])","773c8001":"Data_all['KitchenQual'] = Data_all['KitchenQual'].fillna(Data_all['KitchenQual'].mode()[0])","7ec24885":"Data_all['Exterior1st'] = Data_all['Exterior1st'].fillna(Data_all['Exterior1st'].mode()[0])\nData_all['Exterior2nd'] = Data_all['Exterior2nd'].fillna(Data_all['Exterior2nd'].mode()[0])","05a59da4":"Data_all['SaleType'] = Data_all['SaleType'].fillna(Data_all['SaleType'].mode()[0])","38367694":"Data_all.isnull().sum().sum()","a57d2403":"Data_all.head(10)","609117fd":"Data_all['MSSubClass'] = Data_all['MSSubClass'].apply(str)","9c9ebcbd":"Data_all['OverallCond'] = Data_all['OverallCond'].astype(str)\nData_all['YrSold'] = Data_all['YrSold'].astype(str)\nData_all['MoSold'] = Data_all['MoSold'].astype(str)","db1fb736":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(Data_all[c].values)) \n    Data_all[c] = lbl.transform(list(Data_all[c].values))","82e80649":"Data_all.head()","cc72688a":"Data_all['TotalSF'] = Data_all['TotalBsmtSF'] + Data_all['1stFlrSF'] + Data_all['2ndFlrSF']","b8583deb":"Data_all = pd.get_dummies(Data_all)\nprint(Data_all.shape)\nData_all.head(10)","c049770a":"Data_train = Data_all[:1460]\nData_test = Data_all[1460:]","3e1fea09":"print(Data_train.shape)\nprint(Data_test.shape)","957eab58":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(Data_train.values)\n    rmse= np.sqrt(-cross_val_score(model, Data_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","3af891b3":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split","32f26aef":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","2626cd0d":"score_lasso = rmsle_cv(lasso).mean()\nprint(\"rmse_cv=\", score_lasso)","35471ad8":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","152c6af6":"score_ENet = rmsle_cv(ENet).mean()\nprint(\"rmse_cv=\", score_ENet)","d9d7d095":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","eb856a69":"score_KRR = rmsle_cv(KRR).mean()\nprint(\"rmse_cv=\", score_KRR)","e5ef452c":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","9e905286":"score_GBoost = rmsle_cv(GBoost).mean()\nprint(\"rmse_cv=\", score_GBoost)","ea89e75a":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","4292ba97":"score_model_xgb = rmsle_cv(model_xgb).mean()\nprint(\"rmse_cv=\", score_model_xgb)","674a9792":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","22b61ea2":"score_model_lgb = rmsle_cv(model_lgb).mean()\nprint(\"rmse_cv=\", score_model_lgb)","d273948b":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.metrics import mean_squared_error","26f3774c":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","4176ce0c":"averaged_models = AveragingModels(models = (ENet, GBoost, lasso, model_lgb))\nscore = rmsle_cv(averaged_models)","cd9484b2":"print(\"rmse_cv=\", score.mean())","41a0177a":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","10ab9ac4":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, model_lgb),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)","6eaff2e4":"print(\"rmse_cv=\", score.mean())","9c784ec0":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","87da0919":"stacked_averaged_models.fit(Data_train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(Data_train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(Data_test.values))\nprint(rmsle(y_train, stacked_train_pred))","b7068b4d":"model_lgb.fit(Data_train, y_train)\nlgb_train_pred = model_lgb.predict(Data_train)\nlgb_pred = np.expm1(model_lgb.predict(Data_test.values))\nprint(rmsle(y_train, lgb_train_pred))","5f4dc8e7":"print(rmsle(y_train,stacked_train_pred*0.70 + lgb_train_pred*0.30 ))","cdaec720":"sub = pd.DataFrame()\nensemble = stacked_pred*0.70 + lgb_pred*0.30\nsub['SalePrice'] = ensemble\nsub.to_csv('result.csv')","ef9faefb":"using the cross_val_score to do cross validation and calculate the RMSE","1195d4d3":"Show the outliers. There are lots of outliers in different features, here \"SalePrice\" is as an example. \nAcctually, I didn't delete any outliers in this kernal. I believe the prediction accuracy can be imporeved by dropping some outliers, it's deserve for a try.\n\nin the below figure, obviously, there are two outliers in the bottom right corner, you can drop it using the following code.\nData_all = Data_all.drop(Data_all[(Data_all['GrLivArea']>4000) & (Data_all['SalePrice']<200000)].index)","de6baf17":"the NA of 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond' can be also filled with None","6177a320":"dummy all the features","0d6a3bec":"Ensemble stacking models and lightgbm","e550cf5a":"drop SalePrice in the all data, so we can do features engineering for train and test both.","23bf37af":"define the evaluation metrics RMSE","9c93605e":"KernelRidge uses L2 regularization","6a0c6bd2":"save SalePrice for actual value.","b275f683":"check if there is any NA in the data set","24d53884":"We can see that SalePrice is right skewed, usually, we use Log-type-transformation dealing with it. We use log1p here, and the results show that the data is normal distribution now.","04e377f5":"Then, we calculate the ensemble model-stacking. ENet, GBoost, model_lgb are as the base-models, and lasso is as the meta-model. You may try different base-models and meta-model.","9fdb288d":"The following features contain several NA so that we replaced it with majority.","0362181e":"import the required modules","11ac64c7":"LotFrontage represents linear feet of street connected to property.\nWe can use the Neighboor category and fill with the mean of same Neighboor category","c03d7503":"like lasso model, ElasticNet model uses L1 and L2 regularization simultaneously.","0c9fe58b":"First, we calculte the average results of the selected models, while the KRR is not good as others so we ingore it.","a307ff36":"I loaded the training and test data from the CSV file, you can choose the routes of yours","1cdfdf99":"Extrem gradient boosting algorithm","239ef967":"\n<font face=\"\u9ed1\u9ad4\" size=6> Ensemble-stacking method to predict the house prices\n\n<font color=#0099ff size=4> Yongbao Chen <br> September 2021\n\nThis is an updated version by using ensemble-stacking method to predict the house price! Comparing with [the single prediction method by using lightgbm algorithm](https:\/\/www.kaggle.com\/yongbaochen\/lightgbm-model-to-predict-the-house-prices).\n\nThe stacking method were using Enet, GBoost and lightgbm as base models, while using lasso as the meta model. \n\nI got the final score of 0.11887 (Top 5% of the leaderboard), which is an great approvement compared with single lightgbm algorithm (0.17141, only top 78%).\n\nI've refered to [Stacked Regressions to predict House Prices](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) by Serigne, [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) by Pedro Marcelino, I really appreciate that.","5d4df05d":"We calculated the mean score of these five-fold results","eda07746":"LightGMB score, it was defined previously.","518b65ad":"Gradient boosting is an ensemble learning method(boosting), differ from stacking, stacking","34330ef6":"Finally, we reach to the end and save our prediction results.","8739973a":"stacking models score","3fc5cabf":"the NA of 'GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath' can be also filled with 0","6dae7315":"Transform some numerical features to categorical features, cause they are not really representing value.","5ac81cd5":"The attribute value of Utilities are all 'AllPub', except one 'NoSeWa' and 2 NA. This feature is not useful for our prediction, so can delete it directely.","cc6c1f05":"add an additional feature TotalSF","f1688656":"import the required modules for prediction model development","29544b7d":"Data transformation from nor-normalization(or skewed) to normal distribution, cause lots of models presume that the data is normal distrution.","a6522eca":"Althoght some features are not important according to the coorelation heatmap, we used almost all the features in our developed models. In our first version of kernel, we used around 10 features only to develop the model.","8d96b1a3":"show the missing data and missing percentage of each features","b00b93a6":"LabelEncoder other categorical features","b28847e1":"concatenate the training and test data, so that, I can dealing with the input features together, such as features engineering, missing data, outliers and so on.","7f1362f4":"cause there are some outliers we didn't remove, we use Robustscaler for developing a Robuster model.\nusing make_pipeline can be an efficient way to do this.\nLasso is a linear model, it uses L1 regularization to lower overfitting.","18a1a06e":"a variant of Gradient boosting regression","14145019":"Dealing with the missing data.\nPoolQC represents Pool quality, NA means No pool, so it can be replaced by None.\nAlso, MiscFeature represents miscellaneous feature not covered in other categories, Alley, Fence, and FireplaceQu, the NA can be replaced by None","d0a47b9b":"rebuild the training and test data set.\nAttention, if you drop some samples, the shape will be different."}}