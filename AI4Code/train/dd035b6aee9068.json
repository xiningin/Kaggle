{"cell_type":{"3c6bcf13":"code","aed863e9":"code","de24e15d":"code","82cea653":"code","525c1578":"code","021c1a2e":"code","37618e91":"code","287562f8":"code","3d3b6532":"code","3130fb41":"code","114b076b":"code","ac78a91d":"code","158cd8f4":"code","6dc6f9fe":"markdown","131409e7":"markdown","07426a2e":"markdown","a9f5d913":"markdown","cf02176b":"markdown","ef1cbcbd":"markdown","39dbf4ab":"markdown","3b9534ab":"markdown","51d620c1":"markdown"},"source":{"3c6bcf13":"#import packages I'll need\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import mean_squared_error\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","aed863e9":"#read database\ndf = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')\ndf.head()","de24e15d":"#standardize the data, even if for Random Forests is not necessary I'll be using Linear Regression later\n\nscaler = preprocessing.StandardScaler()\n\n#train\/test split\ntrain_db, test_db  = train_test_split(df, test_size = 0.2, random_state = 10)\n\n#isolate dv\ntrain_label = train_db['Chance of Admit '].copy()\ntest_label = test_db['Chance of Admit '].copy()\n\ntrain_db = train_db.drop(['Chance of Admit '], axis = 1)\ntest_db = test_db.drop(['Chance of Admit '], axis = 1)\n\n#standardization\ntrain_db_scaled = scaler.fit_transform(train_db)\ntest_db_scaled = scaler.transform(test_db)","82cea653":"#test Random Forests without any tuning\n\nforest_reg = RandomForestRegressor()\n\nforest_reg.fit(train_db_scaled, train_label)\npredictions_forest = forest_reg.predict(test_db_scaled)\nmse_forest = mean_squared_error(predictions_forest, test_label)\nrmse_forest = np.sqrt(mse_forest)\nrmse_forest","525c1578":"# random forests with hyperparameters\n\n# let's see what hyperparameters we have now\nfrom pprint import pprint\npprint(forest_reg.get_params())","021c1a2e":"#  Random Hyperparameter Grid Search\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 5, stop = 800, num = 30)]\n\nmax_features = ['auto', 'sqrt']\n\nmax_depth = [int(x) for x in np.linspace(5, 2000, num = 30)]\nmax_depth.append(None)\n\nmin_samples_split = [2, 4, 6, 10]\n\nmin_samples_leaf = [1,2, 4, 6, 10]\n\nbootstrap = [True, False]\n\n# Create the random grid I'll use at the next step\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","37618e91":"#test 100 random parameter combinations, with 3 fold cross validation\n\nforest_reg2 = RandomForestRegressor()\nrf_random = RandomizedSearchCV(estimator = forest_reg2, param_distributions = random_grid, n_iter = 100, cv = 3, \n                               verbose=2, random_state=10, n_jobs = -1)\n\nrf_random.fit(train_db_scaled, train_label)","287562f8":"#check best parameters\n\nrf_random.best_params_","3d3b6532":"#define a parameter grid close to what seemed to be the best parameters from\n#the random search and test each one with Grid Search\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [500,800,1000,1200, 1400, 1500],\n    'max_features': ['sqrt'],\n    'min_samples_leaf': [1,2,3,4],\n    'min_samples_split': [3,4,5,6],\n    'n_estimators': [600,700,750,800]\n}\n\n\nrf = RandomForestRegressor()\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n\ngrid_search.fit(train_db_scaled, train_label)\ngrid_search.best_params_","3130fb41":"#let's see the best choice\nbest_grid = grid_search.best_estimator_\nbest_grid","114b076b":"#now let's check the new rmse\npredictions_grid = best_grid.predict(test_db_scaled)\nmse_forest_grid = mean_squared_error(predictions_grid, test_label)\nrmse_forest_grid = np.sqrt(mse_forest_grid)\nrmse_forest_grid","ac78a91d":"#test regression\n\nlin_reg = LinearRegression()\n\nlin_reg.fit(train_db_scaled, train_label)\npredictions_lin = lin_reg.predict(test_db_scaled)\nmse_lin = mean_squared_error(predictions_lin, test_label)\nrmse_lin = np.sqrt(mse_lin)\nrmse_lin","158cd8f4":"# test Voting Regressor\n\nfrom sklearn.ensemble import VotingRegressor\n\nvoting_reg = VotingRegressor(estimators=[('lr', lin_reg), ('fr', forest_reg), ('bg', best_grid)])\nvoting_reg.fit(train_db_scaled, train_label)\n\npredictions_voting_reg = voting_reg.predict(test_db_scaled)\nmse_v = mean_squared_error(predictions_voting_reg, test_label)\nrmse_v = np.sqrt(mse_v)\nrmse_v","6dc6f9fe":"**This notebook focuses mostly on Random Forests, then compares the results with a Linear Regression and in the end uses a Voting Regressor to try and get the lowest mean squared error. The steps are as follows:**\n* testing a simple, non-tuned Random Forest (I)\n* testing a tuned Random Forest with the help of Grid Search Cross Validation (II), after a Random Hyperparameter Search (III)\n* test a simple Linear Regression (IV)\n* test all three of them in a Voting Regressor (V) ","131409e7":"# II","07426a2e":"# I","a9f5d913":"A result similar to a non-tuned Random Forest.","cf02176b":"And indeed the Voting Regressor got the best result of all, a rmse of 0.053 ( for the first run, in the second run rmse_v got to 0.054 so it doesn't go so far from the Random Forest) .","ef1cbcbd":"# IV","39dbf4ab":"# V","3b9534ab":"# III ","51d620c1":"So there is an improvement from 0.057 to 0.054 in the rmse value, not incredible but still, it's an improvement."}}