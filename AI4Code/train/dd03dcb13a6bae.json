{"cell_type":{"eb3ffeb7":"code","116b804d":"code","5a2fa3d6":"code","c64941c8":"code","458f06f9":"code","162400cd":"code","0c0dbd35":"code","d8b28170":"code","171b58a6":"code","476c2628":"code","1926727d":"code","cedeb29b":"code","18508ba4":"code","a6d46e93":"code","8ea8f204":"code","f63ef07b":"code","4c1d2918":"code","d058a270":"code","7700740f":"code","f3666e10":"code","5e9a38de":"code","881cac01":"code","e23818d6":"code","76991f50":"code","5a2d260b":"code","ae12a5a7":"code","2c8aac7e":"code","b8dfdf20":"code","f05af36c":"code","89549e4f":"code","a06b2836":"code","ba5194b6":"code","1c3f6745":"code","23a00adf":"markdown","884c32a0":"markdown","bf34bb54":"markdown","7bac6f5a":"markdown","adca305a":"markdown","e0e4a8d9":"markdown","d5551124":"markdown","34eb714a":"markdown"},"source":{"eb3ffeb7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","116b804d":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pylab as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport seaborn as sns\nfrom scipy.stats import mstats\nfrom tqdm import tqdm\n\nfrom sklearn import metrics, model_selection, feature_selection, ensemble, gaussian_process, linear_model, naive_bayes, neighbors, svm, tree, discriminant_analysis, model_selection\nfrom xgboost import XGBClassifier\nfrom imblearn import under_sampling, over_sampling\n\n\nfrom IPython.display import Image\nfrom io import StringIO\n\nimport warnings\nwarnings.filterwarnings('ignore')","5a2fa3d6":"%matplotlib inline\nimport numpy as np\nimport pandas as pd","c64941c8":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ngender_submission=pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ndata = pd.concat([train, test], sort=True)\ndata","458f06f9":"data.isnull().sum()","162400cd":"#\u6b20\u640d\u5024\u51e6\u7406\ndata['Sex'].replace(['male','female'],[0, 1], inplace=True)\n\ndata['Embarked'].fillna(('S'), inplace=True)\ndata['Embarked'] = data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ndata['Fare'].fillna(np.mean(data['Fare']), inplace=True)\n# data['Mr']=data.Name.apply(lambda x:1 if 'Mr' in x else 0)\n# data['Mrs']=data.Name.apply(lambda x:1 if 'Mrs' in x else 0)\n# data['Miss']=data.Name.apply(lambda x:1 if 'Miss' in x else 0)\n# age_avg = data['Age'].mean()\n\ndata['Age'].fillna(int(age_avg), inplace=True)\n\ndelete_columns = [ 'PassengerId', ]\ndata.drop(delete_columns, axis = 1, inplace = True)\n","0c0dbd35":"data['Salutation'] = data.Name.str.extract(' ([A-Za-z]+).', expand=False)\ndata['Salutation'] = data['Salutation'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndata['Salutation'] = data['Salutation'].replace('Mlle', 'Miss')\ndata['Salutation'] = data['Salutation'].replace('Ms', 'Miss')\ndata['Salutation'] = data['Salutation'].replace('Mme', 'Mrs')\ndel data['Name']\nSalutation_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\ndata[\"FamilySize\"] = data[\"SibSp\"] + data[\"Parch\"] + 1\n\ndata['Salutation'] = data['Salutation'].map(Salutation_mapping)\ndata['Salutation'] = data['Salutation'].fillna(0)\n\n\ndata['Ticket_Lett'] = data['Ticket'].apply(lambda x: str(x)[0])\ndata['Ticket_Lett'] = data['Ticket_Lett'].apply(lambda x: str(x))\ndata['Ticket_Lett'] = np.where((data['Ticket_Lett']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']), data['Ticket_Lett'], np.where((data['Ticket_Lett']).isin(['W', '4', '7', '6', 'L', '5', '8']), '0','0'))\ndata['Ticket_Len'] = data['Ticket'].apply(lambda x: len(x))\ndel data['Ticket']\ndata['Ticket_Lett']=data['Ticket_Lett'].replace(\"1\",1).replace(\"2\",2).replace(\"3\",3).replace(\"0\",0).replace(\"S\",3).replace(\"P\",0).replace(\"C\",3).replace(\"A\",3)\n\n\ndata['Cabin_Lett'] = data['Cabin'].apply(lambda x: str(x)[0]) \ndata['Cabin_Lett'] = data['Cabin_Lett'].apply(lambda x: str(x)) \ndata['Cabin_Lett'] = np.where((data['Cabin_Lett']).isin([ 'F', 'E', 'D', 'C', 'B', 'A']),data['Cabin_Lett'], np.where((data['Cabin_Lett']).isin(['W', '4', '7', '6', 'L', '5', '8']), '0','0'))\ndel data['Cabin'] \ndata['Cabin_Lett']=data['Cabin_Lett'].replace(\"A\",1).replace(\"B\",2).replace(\"C\",1).replace(\"0\",0).replace(\"D\",2).replace(\"E\",2).replace(\"F\",1) \n\n# data['IsAlone'] = 0\n# data.loc[data['FamilySize'] == 1, 'IsAlone'] = 1","d8b28170":"data","171b58a6":"train = data[:len(train)]\ntest = data[len(train):]","476c2628":"y_train = train['Survived']\nX_train = train.drop('Survived', axis = 1)\nX_test = test.drop('Survived', axis = 1)","1926727d":"#\u751f\u5b58\u8005\u306e\u5272\u5408\ntrain.Survived.sum()\/train.Survived.count()\n#\u307e\u3041\u5747\u8861\u3068\u3044\u3046\u3053\u3068\u3067\u3044\u3044\u304b\u3082\u3001\u4e00\u5fdc\u3000\u30c7\u30fc\u30bf\u306e\u6574\u5f62\u3092\u884c\u3046\n","cedeb29b":"from imblearn import under_sampling, over_sampling\ncols = train.columns.tolist()\ncols.remove('Survived')\n\npositive_cnt = int(train['Survived'].sum())\nrus = under_sampling.RandomUnderSampler(sampling_strategy={0:positive_cnt, 1:positive_cnt}, random_state=0)\ndata_x_sample, data_y_sample  = rus.fit_sample(train[cols], train[['Survived']])\n","18508ba4":"data_x_sample","a6d46e93":"#recv\u3092\u7528\u3044\u3066\u7279\u5fb4\u91cf\u9078\u629e\u3092\u884c\u3046\nfeature_importance_models = [\n    ensemble.AdaBoostClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    tree.DecisionTreeClassifier(),\n    XGBClassifier()\n]\n \nscoring = ['accuracy']\ndf_rfe_cols_cnt = pd.DataFrame(columns=['cnt'], index=cols)\ndf_rfe_cols_cnt['cnt'] = 0\n \nfor i, model in tqdm(enumerate(feature_importance_models), total=len(feature_importance_models)):\n    \n    rfe = feature_selection.RFECV(model, step=3)\n    rfe.fit(data_x_sample, data_y_sample)\n    rfe_cols = train[cols].columns.values[rfe.get_support()]\n    df_rfe_cols_cnt.loc[rfe_cols, 'cnt'] += 1\n    \ndf_rfe_cols_cnt.plot(kind='bar', figsize=(15, 5))\nplt.show()\n#\u307e\u3041\u307e\u3041\u3069\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u3082\u52b9\u3044\u3066\u3044\u308b\u3002\u5bb6\u65cf\u7dcf\u6570\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\u3057\u3066\u307f\u308b","8ea8f204":"data[\"familiynumber\"]=data.Parch+data.SibSp+1","f63ef07b":"data","4c1d2918":"#Familiynumber\u3092\u8a2d\u3051\u3066recv\u3092\u884c\u3046\n\ntrain = data[:len(train)]\ntest = data[len(train):]\ny_train = train['Survived']\nX_train = train.drop('Survived', axis = 1)\nX_test = test.drop('Survived', axis = 1)\n\ncols = train.columns.tolist()\ncols.remove('Survived')\n\npositive_cnt = int(train['Survived'].sum())\nrus = under_sampling.RandomUnderSampler(sampling_strategy={0:positive_cnt, 1:positive_cnt}, random_state=0)\ndata_x_sample, data_y_sample  = rus.fit_sample(train[cols], train[['Survived']])\n#recv\u3092\u7528\u3044\u3066\u7279\u5fb4\u91cf\u9078\u629e\u3092\u884c\u3046\nfeature_importance_models = [\n    ensemble.AdaBoostClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    tree.DecisionTreeClassifier(),\n    XGBClassifier()\n]\n \nscoring = ['accuracy']\ndf_rfe_cols_cnt = pd.DataFrame(columns=['cnt'], index=cols)\ndf_rfe_cols_cnt['cnt'] = 0\n \nfor i, model in tqdm(enumerate(feature_importance_models), total=len(feature_importance_models)):\n    \n    rfe = feature_selection.RFECV(model, step=3)\n    rfe.fit(data_x_sample, data_y_sample)\n    rfe_cols = train[cols].columns.values[rfe.get_support()]\n    df_rfe_cols_cnt.loc[rfe_cols, 'cnt'] += 1\n    \ndf_rfe_cols_cnt.plot(kind='bar', figsize=(15, 5))\nplt.show()\n#\u5bb6\u65cf\u306e\u6570\u3092\u8ffd\u52a0\u3057\u305f\u3068\u3053\u308d\u3001Parch,SibSp\u306e\u91cd\u8981\u5ea6\u304c\u6e1b\u5c11\u3057\u305f\u3002","d058a270":"# #Embarked,Parch,SibSp\u3092\u9664\u3044\u3066\u30e2\u30c7\u30eb\u9078\u629e\u3092\u884c\u3046\u5834\u5408\u306f\u3053\u3061\u3089\u306e\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\u3092\u5916\u3059\u3002\u4eca\u56de\u306f\u591a\u3044\u7279\u5fb4\u91cf\u3067\u691c\u8a0e\n# x_cols = df_rfe_cols_cnt[df_rfe_cols_cnt['cnt'] >= 4].index\n# x_cols","7700740f":"#Embarked,Parch,SibSp\u3092\u9664\u304b\u305a\u30e2\u30c7\u30eb\u9078\u629e\u3092\u884c\u3046\nx_cols = df_rfe_cols_cnt.index\nx_cols","f3666e10":"positive_cnt = int(train.Survived.sum())\nrus = under_sampling.RandomUnderSampler(sampling_strategy={0:positive_cnt, 1:positive_cnt}, random_state=0)\ndata_x_sample, data_y_sample = rus.fit_sample(train[x_cols], train[['Survived']])\n\nlen(data_x_sample), len(data_y_sample), data.Survived.sum()","5e9a38de":"# \u7279\u5fb4\u91cf\u3092\u9078\u629e\u3057\u3066\u3001\u8907\u6570\u306e\u30e2\u30c7\u30eb\u3067\u7cbe\u5ea6\u3092\u8abf\u67fb\u3059\u308b\nimport lightgbm as lgb\n \nmodels = [\n \n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n \n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.RidgeClassifierCV(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n \n    #xgboost\n    XGBClassifier(),\n    lgb.LGBMClassifier()    \n]\n \ndf_compare = pd.DataFrame(columns=['name', 'train_accuracy', 'valid_accuracy', 'time'])\nscoring = ['accuracy']\n \nfor model in tqdm(models):\n    \n    name = model.__class__.__name__\n    \n    cv_rlts = model_selection.cross_validate(model, data_x_sample, data_y_sample, scoring=scoring, cv=10, return_train_score=True)\n \n    for i in range(10):\n        s = pd.Series([name, cv_rlts['train_accuracy'][i], cv_rlts['test_accuracy'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name=name+str(i))\n        df_compare = df_compare.append(s)\n        \nplt.figure(figsize=(12,8))\nsns.boxplot(data=df_compare, y='name', x='valid_accuracy', orient='h', linewidth=0.5, width=0.5)\nplt.grid()\nplt.show()\n","881cac01":"# \u7cbe\u5ea6\u306e\u826f\u3044\u30e2\u30c7\u30eb\u3092\u9078\u3093\u3067\u3001\u6295\u7968\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\n \nvote_models = [\n \n    #Ensemble Methods\n    ('abc', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etsc', ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n \n    #Gaussian Processes\n    #('gpc', gaussian_process.GaussianProcessClassifier()),\n    \n    #GLM\n    ('lrcv', linear_model.LogisticRegressionCV()),\n    #('rccv', linear_model.RidgeClassifierCV()), # unable soft voting\n    \n    #Navies Bayes\n    #('bnb', naive_bayes.BernoulliNB()),\n    #('gnb', naive_bayes.GaussianNB()),\n    \n    #Nearest Neighbor\n    #('knc', neighbors.KNeighborsClassifier()),\n    \n    #Trees    \n    #('dtc', tree.DecisionTreeClassifier()),\n    #('etc', tree.ExtraTreeClassifier()),\n    \n    #Discriminant Analysis\n    #('lda', discriminant_analysis.LinearDiscriminantAnalysis()),\n    #('qda', discriminant_analysis.QuadraticDiscriminantAnalysis()),\n \n    #xgboost\n    ('xgbc', XGBClassifier()),\n    \n    #lightgbm\n    ('lgbm',lgb.LGBMClassifier())\n    \n]\n \ndf_compare = pd.DataFrame(columns=['name', 'valid_accuracy', 'time'])\nscoring = ['accuracy']\n \nvote_hard_model = ensemble.VotingClassifier(estimators=vote_models, voting='hard')\ncv_rlts = model_selection.cross_validate(vote_hard_model, data_x_sample, data_y_sample, cv=10, scoring=scoring)\nfor i in range(10):\n    s = pd.Series(['hard', cv_rlts['test_accuracy'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name='hard'+str(i))\n    df_compare = df_compare.append(s)\n    \nvote_soft_model = ensemble.VotingClassifier(estimators=vote_models , voting='soft')\ncv_rlts = model_selection.cross_validate(vote_soft_model, data_x_sample, data_y_sample, cv=10, scoring=scoring)\nfor i in range(10):\n    s = pd.Series(['soft', cv_rlts['test_accuracy'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name='soft'+str(i))\n    df_compare = df_compare.append(s)\n    \nplt.figure(figsize=(12,3))\nsns.boxplot(data=df_compare, y='name', x='valid_accuracy', orient='h', linewidth=0.5, width=0.5)\nplt.grid()\nplt.show()","e23818d6":"\n# \u5404\u30e2\u30c7\u30eb\u306e\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30b0\u30ea\u30c3\u30c9\u30b5\u30fc\u30c1\n\ngrid_n_estimator = [10, 50, 100, 300]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\ngrid_param = [\n    \n    #AdaBoostClassifier\n    [{ \n        'n_estimators': grid_n_estimator, #default=50\n        'learning_rate': grid_learn, #default=1\n        #'algorithm': ['SAMME', 'SAMME.R'], #default=\u2019SAMME.R\n        'random_state': grid_seed\n    }],\n    \n    #BaggingClassifier\n    [{\n        'n_estimators': grid_n_estimator, #default=10\n        'max_samples': grid_ratio, #default=1.0\n        'random_state': grid_seed\n     }],\n\n    #ExtraTreesClassifier\n    [{\n        'n_estimators': grid_n_estimator, #default=10\n        'criterion': grid_criterion, #default=\u201dgini\u201d\n        'max_depth': grid_max_depth, #default=None\n        'random_state': grid_seed\n     }],\n\n    #GradientBoostingClassifier\n    [{\n        #'loss': ['deviance', 'exponential'], #default=\u2019deviance\u2019\n        'learning_rate': [.05], #default=0.1\n        'n_estimators': [300], #default=100\n        #'criterion': ['friedman_mse', 'mse', 'mae'], #default=\u201dfriedman_mse\u201d\n        'max_depth': grid_max_depth, #default=3   \n        'random_state': grid_seed\n     }],\n\n    #RandomForestClassifier\n    [{\n        'n_estimators': grid_n_estimator, #default=10\n        'criterion': grid_criterion, #default=\u201dgini\u201d\n        'max_depth': grid_max_depth, #default=None\n        'oob_score': [True], #default=False\n        'random_state': grid_seed\n     }],\n    \n    #LogisticRegressionCV\n    [{\n        'fit_intercept': grid_bool, #default: True\n        #'penalty': ['l1','l2'],\n        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n        'random_state': grid_seed\n     }],\n    \n    # ExtraTreeClassifier\n    [{}],\n    \n    # LinearDiscriminantAnalysis\n    [{}],\n    \n    #XGBClassifier\n    [{\n        'learning_rate': grid_learn, #default: .3\n        'max_depth': [1,2,4,6,8,10], #default 2\n        'n_estimators': grid_n_estimator, \n        'seed': grid_seed  \n     }],\n    #LGBMClassifier\n    [{\n        'learning_rate':grid_learn,\n        'n_estimators':grid_n_estimator,\n        'max_depth':[1,2,4,6,8,10],\n        'min_child_weight':[0.5,1,2],\n        'min_child_samples':[5,10,20],\n        'subsample':[0.8],\n        'colsample_bytree':[0.8],\n        'verbose':[-1],\n        'num_leaves':[80]}]\n    \n]\n\nfor model, param in tqdm(zip(vote_models, grid_param), total=len(vote_models)):\n    \n    best_search = model_selection.GridSearchCV(estimator=model[1], param_grid=param, scoring='roc_auc')\n    best_search.fit(data_x_sample, data_y_sample)\n\n    best_param = best_search.best_params_\n    model[1].set_params(**best_param)","76991f50":"#\u3000\u6295\u7968\u30e2\u30c7\u30eb\u306e\u4f5c\u6210\u3001soft\u30e2\u30c7\u30eb\u3068hard\u30e2\u30c7\u30eb\u306e\u4f5c\u6210\u3068\u691c\u8a3c\ndf_compare = pd.DataFrame(columns=['name', 'valid_accuracy', 'time'])\nscoring = ['accuracy']\n \nvote_hard_model = ensemble.VotingClassifier(estimators=vote_models, voting='hard')\ncv_rlts = model_selection.cross_validate(vote_hard_model, data_x_sample, data_y_sample, cv=10, scoring=scoring)\nfor i in range(10):\n    s = pd.Series(['hard',  cv_rlts['test_accuracy'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name='hard'+str(i))\n    df_compare = df_compare.append(s)\n    \nvote_soft_model= ensemble.VotingClassifier(estimators=vote_models , voting='soft')\ncv_rlts = model_selection.cross_validate(vote_soft_model, data_x_sample, data_y_sample, cv=10, scoring=scoring)\nfor i in range(10):\n    s = pd.Series(['soft',  cv_rlts['test_accuracy'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name='soft'+str(i))\n    df_compare = df_compare.append(s)\n    \nplt.figure(figsize=(12,3))\nsns.boxplot(data=df_compare, y='name', x='valid_accuracy', orient='h',  linewidth=0.5, width=0.5)\nplt.grid()\nplt.show()","5a2d260b":"df_compare.groupby('name').mean().sort_values(by='valid_accuracy', ascending=False)\n","ae12a5a7":"from sklearn.model_selection import KFold, train_test_split\ntrain_x, valid_x, train_y, valid_y = train_test_split(data_x_sample, data_y_sample, test_size=0.3, random_state=0)\n\nvote_soft_model.fit(train_x, train_y)\n\npred = vote_soft_model.predict(valid_x)\n\nfig, axs = plt.subplots(ncols=2,figsize=(15,5))\n\nsns.heatmap(metrics.confusion_matrix(valid_y, pred), vmin=0, annot=True, fmt='d', ax=axs[0])\naxs[0].set_xlabel('Predict')\naxs[0].set_ylabel('Ground Truth')\naxs[0].set_title('Accuracy: {}'.format(metrics.accuracy_score(valid_y, pred)))\nfpr, tpr, thresholds = metrics.roc_curve(valid_y, pred)\naxs[1].plot(fpr, tpr)\naxs[1].set_title('ROC curve')\naxs[1].set_xlabel('False Positive Rate')\naxs[1].set_ylabel('True Positive Rate')\naxs[1].grid(True)\nplt.show()","2c8aac7e":"vote_hard_model.fit(train_x, train_y)\n\npred = vote_hard_model.predict(valid_x)\n\nfig, axs = plt.subplots(ncols=2,figsize=(15,5))\n\nsns.heatmap(metrics.confusion_matrix(valid_y, pred), vmin=0, annot=True, fmt='d', ax=axs[0])\naxs[0].set_xlabel('Predict')\naxs[0].set_ylabel('Ground Truth')\naxs[0].set_title('Accuracy: {}'.format(metrics.accuracy_score(valid_y, pred)))\nfpr, tpr, thresholds = metrics.roc_curve(valid_y, pred)\naxs[1].plot(fpr, tpr)\naxs[1].set_title('ROC curve')\naxs[1].set_xlabel('False Positive Rate')\naxs[1].set_ylabel('True Positive Rate')\naxs[1].grid(True)\nplt.show()","b8dfdf20":"\n# \u52d5\u304b\u3059\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u660e\u793a\u7684\u306b\u8868\u793a\nparams = {\"learning_rate\":[0.1,0.3,0.5],\n        \"max_depth\": [2,3,5,10],\n         \"subsample\":[0.5,0.8,0.9,1],\n         \"colsample_bytree\": [0.5,1.0],\n         }\n# \u30e2\u30c7\u30eb\u306b\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u751f\u6210\nmod = XGBClassifier()\n# \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u63a2\u7d22\ngv =  model_selection.GridSearchCV(mod, params, cv = 10, scoring= 'roc_auc', n_jobs =-1)\n\n#\u3000train\u30c7\u30fc\u30bf\u3068test\u30c7\u30fc\u30bf\u306b\u5206\u5272\ntrain_x, valid_x, train_y, valid_y = train_test_split(data_x_sample, data_y_sample, test_size=0.3, random_state=0)\n\n\n# \u4e88\u6e2c\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\ngv.fit(train_x,train_y)\n\n","f05af36c":"gv =  model_selection.GridSearchCV(mod, params, cv = 10, scoring= 'roc_auc', n_jobs =-1)\ncv_rlts = model_selection.cross_validate(gv, data_x_sample, data_y_sample, cv=10, scoring=scoring)\nfor i in range(10):\n    s = pd.Series(['xgboost',  cv_rlts['test_accuracy'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name='xgb'+str(i))\n    df_compare = df_compare.append(s)\n    \nplt.figure(figsize=(12,3))\nsns.boxplot(data=df_compare, y='name', x='valid_accuracy', orient='h',  linewidth=0.5, width=0.5)\nplt.grid()\nplt.show()\n","89549e4f":"train_x, valid_x, train_y, valid_y = train_test_split(data_x_sample, data_y_sample, test_size=0.3, random_state=0)\n\ngv.fit(train_x, train_y)\n\npred = gv.predict(valid_x)\n\nfig, axs = plt.subplots(ncols=2,figsize=(15,5))\n\nsns.heatmap(metrics.confusion_matrix(valid_y, pred), vmin=0, annot=True, fmt='d', ax=axs[0])\naxs[0].set_xlabel('Predict')\naxs[0].set_ylabel('Ground Truth')\naxs[0].set_title('Accuracy: {}'.format(metrics.accuracy_score(valid_y, pred)))\nfpr, tpr, thresholds = metrics.roc_curve(valid_y, pred)\naxs[1].plot(fpr, tpr)\naxs[1].set_title('ROC curve')\naxs[1].set_xlabel('False Positive Rate')\naxs[1].set_ylabel('True Positive Rate')\naxs[1].grid(True)\nplt.show()","a06b2836":"X_test.isnull().sum()","ba5194b6":"#\u6700\u521d\u306f\u30e2\u30c7\u30eb\u8a55\u4fa1\u306e\u305f\u3081\u306b\u5747\u8861\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\u3092\u884c\u3044\u3001\u305d\u306e\u30c7\u30fc\u30bf\u3092\u5206\u5272\u3057\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\u3092\u884c\u306a\u3063\u305f\n#\u63d0\u51fa\u3059\u308b\u30e2\u30c7\u30eb\u306f\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u5168\u3066\u4f7f\u3044\u5b66\u7fd2\u3059\u308b\u3002\u30c7\u30fc\u30bf\u6570\u591a\u3044\u65b9\u304c\u7cbe\u5ea6\u826f\u304f\u306a\u308a\u305d\u3046\u3060\u3057\u3002\nfrom sklearn.model_selection import KFold, train_test_split\n# train_x, valid_x, train_y, valid_y = train_test_split(data_x_sample, data_y_sample, test_size=0.3, random_state=0)\n\nvote_soft_model.fit(X_train, y_train)\n\npred = vote_soft_model.predict(X_test)\n\n","1c3f6745":"sub = pd.DataFrame(pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")['PassengerId'])\nsub['Survived'] = list(map(int, pred))\nsub.to_csv(\"submission.csv\", index = False)","23a00adf":"\u3042\u3093\u307e\u308asoft\u3082hard\u3082\u5909\u308f\u3089\u306a\u3044\u3002\u4ee5\u4e0b\u306f\u5747\u8861\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u6df7\u5408\u884c\u5217\u306e\u4f5c\u6210\n","884c32a0":"# 2.\u7279\u5fb4\u91cf\u8a2d\u8a08","bf34bb54":"# 0.\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8","7bac6f5a":"# 3.\u30e2\u30c7\u30eb\u8a2d\u8a08,\u8a55\u4fa1","adca305a":"\u3071\u3063\u3068\u898bABC,BC,ETSC,GBC,RFC,XGB,LRBC,LGBM\u3042\u305f\u308a\u304c\u826f\u3055\u305d\u3046","e0e4a8d9":"# 4.\u30e2\u30c7\u30eb\u63d0\u51fa","d5551124":"# 1.\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u51fa\u3057\n","34eb714a":"soft\u306e\u65b9\u304c\u3084\u3084\u3044\u3044\u611f\u3058\u304c\u3059\u308b\u306e\u3067\u3001soft\u306e\u6295\u7968\u30e2\u30c7\u30eb\u3067\u63d0\u51fa\n"}}