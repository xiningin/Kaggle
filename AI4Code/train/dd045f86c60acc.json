{"cell_type":{"aab9e24c":"code","9a922ce7":"code","bae93c67":"code","cad91b4e":"code","25c080b3":"code","2eed771f":"code","c8c0314a":"code","cb4b0a26":"code","68455890":"code","9076d527":"code","6ce6ab84":"code","e619e677":"code","fb4f1ed1":"code","7749de99":"code","c4464f0c":"code","e2921196":"code","eed655fc":"code","85da2102":"code","78a2f151":"code","676c7318":"code","e809898e":"markdown","b3076a63":"markdown","e5e4278f":"markdown","3e413ab8":"markdown","f40d18ef":"markdown","6de3935b":"markdown"},"source":{"aab9e24c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a922ce7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns', 100)\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn import preprocessing\nfrom sklearn import feature_selection\nimport warnings\nwarnings.filterwarnings('ignore')\nSEED = 42","bae93c67":"test_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","cad91b4e":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","25c080b3":"train_data.head()","2eed771f":"test_data.head()","c8c0314a":"train_data.shape","cb4b0a26":"test_data.shape","68455890":"train_data.info()","9076d527":"dfs = [train_data, test_data]\nfor df in dfs:\n    temp = df.isnull().sum()\n    print(temp.loc[temp!=0], '\\n')","6ce6ab84":"test_data.info()","e619e677":"train_data['LT_Salesprice']=np.log1p(train_data['SalePrice'])\nplt.hist(train_data['LT_Salesprice'],color = 'black')\nplt.show()\ntrain_data['LT_Salesprice'].skew()","fb4f1ed1":"# Lets explore the correlations in our data set \nplt.figure(figsize=(20,20))\nsns.heatmap(train_data.corr())\nplt.show()","7749de99":"def data_cleaning(df):\n    \n\n\n    #Handling Null Values\n    df['MSZoning'].fillna(value = df['MSZoning'].mode()[0],inplace=True)\n    df.drop(['Alley','FireplaceQu','PoolQC','MiscFeature','Fence'], axis = 'columns',inplace = True)\n    df['LotFrontage'].fillna(df['LotFrontage'].dropna().mean(),inplace = True)\n    \n    for Bsmt in ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n                 'BsmtFinType2','BsmtFinSF1','BsmtUnfSF','BsmtFinSF2','BsmtFullBath','BsmtHalfBath','TotalBsmtSF'] :\n        df[Bsmt].fillna((df[Bsmt].mode()[0]),inplace=True)\n       \n    for garage in ['GarageType','GarageYrBlt','GarageFinish','GarageQual','GarageCond','GarageCars','GarageArea']:\n        df[garage].fillna((df[garage].mode()[0]),inplace=True)   \n    \n    for other in ['SaleType','Functional','KitchenQual',\n                  'Electrical','MasVnrType','Exterior1st','Exterior2nd','Utilities','MasVnrArea']:\n        df[other].fillna((df[other].mode()[0]),inplace=True)  \n    \n    # print(df.isnull().sum())\n    \n    \n    # List of numerical variables\n    numerical_features = [feature for feature in df.columns if df[feature].dtypes != 'O']\n    print('Number of numerical variables: ', len(numerical_features))\n    \n    # Visualise the numerical variables\n    df[numerical_features].head()\n    \n    #Some Features aren't numerical as well as categorical.So we need to make few changes in it.\n    year_feature = [feature for feature in numerical_features if 'Yr' in feature or 'Year' in feature]\n    year_feature\n    df.groupby('YrSold')['SalePrice'].median().plot()\n    plt.show()\n    \n    #Numerical features are of two types - Discrete & Continuos\n    discrete_feature = [feature for feature in numerical_features if len(df[feature].unique())<25 and feature not in year_feature+['id']]\n      \n    continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature+year_feature+['Id']]\n    \n    for feature in continuous_feature :\n        data = df.copy()\n        if 0 in data[feature].unique() :\n            pass\n        else:\n            data[feature] = np.log(data[feature])\n            data['SalePrice'] = np.log(data['SalePrice'])\n            plt.scatter(data[feature],data['SalePrice'])\n            plt.xlabel(feature)\n            plt.ylabel('Salesprice')\n            plt.show()\n            \n    #Outliers\n    #If u have lots of outliers replace nan with mode or median\n    for feature in continuous_feature :\n        data = df.copy()\n        if 0 in data[feature].unique() :\n            pass\n        else:\n            data[feature] = np.log(data[feature])\n            data.boxplot(column=feature)\n            plt.ylabel(feature)\n            plt.title(feature)\n            plt.show()\n            \n    #Changing the years column to numerical data        \n    for feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n       \n        df[feature]=df['YrSold']-df[feature]\n        \n    #Categorical Features\n    categorical_features=[feature for feature in df.columns if df[feature].dtypes=='O']\n    len(categorical_features)  \n    for feature in categorical_features:\n        temp=df.groupby(feature)['SalePrice'].count()\/len(df)\n        temp_df=temp[temp>0.01].index\n        df[feature]=np.where(df[feature].isin(temp_df),df[feature],'Rare_var')\n    df.shape    \n    \n    for features in categorical_features:\n        dummies = pd.get_dummies(df[features])\n        merged = pd.concat([df,dummies],axis = 'columns')\n        df = merged.copy()\n    \n    for feature in categorical_features:\n        df.drop(feature, axis = 'columns',inplace = True)\n        \n    df.drop('LT_Salesprice', axis = 'columns',inplace = True) \n    return df","c4464f0c":"Dataset = pd.concat([train_data,test_data])\nclean_data = data_cleaning(Dataset)","e2921196":"clean_test = clean_data.iloc[1460:,:]\nclean_test.to_csv('CleanTestData.csv',index = False)","eed655fc":"clean_train = clean_data.iloc[:1460,:]\nclean_train.to_csv('CleanTrainData.csv',index = False)","85da2102":"X_train = clean_train.drop('SalePrice',axis = 'columns')\ny_train = clean_train.SalePrice\nX_test = clean_test.drop('SalePrice',axis = 'columns')","78a2f151":"from sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators = 25)\n# scaler.inverse_transform(X_test)\nmodel.fit(X_train,y_train)\ny_test = model.predict(X_test)\ny_test","676c7318":"submission = pd.DataFrame(columns=['Id', 'SalePrice'])\nsubmission['Id'] = X_test['Id']\nsubmission['SalePrice'] = y_test\n\nsubmission.to_csv('MySubmission.csv', index=False)\nprint(\"submission succesfull\")","e809898e":"**Splitting the Merged data into train and test data as before**","b3076a63":"**Data Visualization**","e5e4278f":"**RandomForestRegressor**","3e413ab8":"**Exporting Predicted Values**","f40d18ef":"**Merging Train & Test Data**","6de3935b":"**Data Cleaning**"}}