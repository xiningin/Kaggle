{"cell_type":{"a14a8bee":"code","b1a1fea1":"code","9581e895":"code","26f89564":"code","daf505c1":"code","292659b8":"code","1d4867ea":"code","7bc96864":"code","6b10516f":"code","495cf171":"code","ff5c6d4f":"code","42d7217e":"code","8fd34015":"code","9d043cac":"code","76e8114b":"code","93950684":"code","f82a466c":"code","ab49e0a1":"code","19ebca80":"code","a6f32b77":"code","2c17ff84":"code","29044ada":"code","3285b7f4":"code","0fdb7ba4":"markdown","da2622af":"markdown","db6d7436":"markdown","0677bf86":"markdown","0d5a280f":"markdown","7687c7a6":"markdown","6957fae6":"markdown","f61f7bdd":"markdown","51ecd3d0":"markdown","07a06881":"markdown","196f54f3":"markdown","3c372c52":"markdown","e4cacf08":"markdown","73a1410e":"markdown","79b47ebb":"markdown"},"source":{"a14a8bee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b1a1fea1":"import numpy as np\nimport random\nimport pprint\nimport pandas as pd\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.python.framework import ops\nfrom sklearn.model_selection import train_test_split\ntf.disable_eager_execution()","9581e895":"data = pd.read_csv(r\"\/kaggle\/input\/bpi-challenge-2017-dataset\/BPI Challenge 2017.csv\", engine=\"c\")","26f89564":"data.keys()","daf505c1":"data=data[data['EventOrigin']!='Workflow']","292659b8":"data=data.drop(['Action','org:resource','EventOrigin','EventID','lifecycle:transition','case:LoanGoal','case:ApplicationType','case:concept:name','OfferID'],axis=1)","1d4867ea":"useful=['A_Cancelled','A_Pending','O_Create Offer','A_Denied']\ndata=data[data['concept:name'].isin(useful)].reset_index()\ndata","7bc96864":"statlist=[]\nfor i in range(len(data)):\n    if data['concept:name'][i]=='O_Create Offer':\n        stats=[]\n        stats.append(data['case:RequestedAmount'][i])\n        stats.append(data['FirstWithdrawalAmount'][i])\n        stats.append(data['NumberOfTerms'][i])\n        stats.append(data['Accepted'][i])\n        stats.append(data['MonthlyCost'][i])\n        stats.append(data['Selected'][i])\n        stats.append(data['CreditScore'][i])\n        stats.append(data['OfferedAmount'][i])\n    elif data['concept:name'][i]=='A_Pending':\n        stats.append(1)\n        statlist.append(stats)\n    elif data['concept:name'][i]=='A_Denied':\n        stats.append(-1)\n        statlist.append(stats)\n    else:\n        stats.append(0)\n        statlist.append(stats)","6b10516f":"statlist=pd.DataFrame(statlist,columns=['RequestedAmount','FirstWithdrawalAmount','NumberOfTerms','Accepted','MonthlyCost','Selected','CreditScore','OfferedAmount','Labels','NULL'])\nstatlist=statlist.drop(['NULL'],axis=1)","495cf171":"statlist","ff5c6d4f":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabel = LabelEncoder()\nstatlist['Accepted']=label.fit_transform(statlist['Accepted'])\nstatlist['Selected']=label.fit_transform(statlist['Selected'])","42d7217e":"dataX = (statlist[['RequestedAmount','FirstWithdrawalAmount','NumberOfTerms','Accepted','MonthlyCost','Selected','CreditScore','OfferedAmount']])\ndataY = statlist[['Labels']]\ndataX = np.asarray(dataX)\ndataY = np.asarray(dataY)","8fd34015":"\nX_train, X_test, Y_train, Y_test = train_test_split(dataX, dataY, test_size=0.33, random_state=42)\nX_train=X_train.T\nX_test=X_test.T\nY_train=Y_train.T\nY_test=Y_test.T","9d043cac":"X_train.shape","76e8114b":"Y_train.shape","93950684":"def create_placeholders(n_x, n_y):\n    \"\"\"\n   \n    \n    Arguments:\n    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n    \n    Returns:\n    X -- placeholder for the data input, of shape [n_x, None] and dtype \"tf.float32\"\n    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"tf.float32\"\n    \n    \n    \"\"\"\n\n    \n    X = tf.placeholder(tf.float32,[n_x,None],name='X')\n    Y = tf.placeholder(tf.float32,[n_y,None],name='Y')\n    \n    \n    return X, Y","f82a466c":"def initialize_parameters():\n    \n    \n    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n        \n    \n    W1 = tf.get_variable('W1',[25,8],initializer=tf.glorot_uniform_initializer)\n    b1 = tf.get_variable('b1',[25,1],initializer=tf.zeros_initializer())\n    W2 = tf.get_variable('W2',[12,25],initializer=tf.glorot_uniform_initializer)\n    b2 = tf.get_variable('b2',[12,1],initializer=tf.zeros_initializer())\n    W3 = tf.get_variable('W3',[6,12],initializer=tf.glorot_uniform_initializer)\n    b3 = tf.get_variable('b3',[6,1],initializer=tf.zeros_initializer())\n   \n\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2,\n                  \"W3\": W3,\n                  \"b3\": b3}\n    \n    return parameters","ab49e0a1":"def forward_propagation(X, parameters):\n    \"\"\"\n    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n                  the shapes are given in initialize_parameters\n\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    \"\"\"\n    \n    # Retrieve the parameters from the dictionary \"parameters\" \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n    \n               # Numpy Equivalents:\n    Z1 = tf.add(tf.matmul(W1,X),b1)                                              # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)\n    Z2 = tf.add(tf.matmul(W2,A1),b2)                                              # Z2 = np.dot(W2, A1) + b2\n    A2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n    Z3 = tf.add(tf.matmul(W3,A2),b3)                                              # Z3 = np.dot(W3, A2) + b3\n    \n    \n    return Z3","19ebca80":"def compute_cost(Z3, Y):\n    \"\"\"\n    Computes the cost\n    \n    Arguments:\n    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n    Y -- \"true\" labels vector placeholder, same shape as Z3\n    \n    Returns:\n    cost - Tensor of the cost function\n    \"\"\"\n    \n    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n    logits = tf.transpose(Z3)\n    labels = tf.transpose(Y)\n    \n    \n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels))\n   \n    return cost","a6f32b77":"import math\nimport matplotlib.pyplot as plt","2c17ff84":"def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    mini_batch_size - size of the mini-batches, integer\n    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n    np.random.seed(seed)\n    \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m\/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","29044ada":"def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0000001,\n          num_epochs = 1000, minibatch_size = 32, print_cost = True ):\n    \"\"\"\n    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n    \n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    seed = 3                                          # to keep consistent results\n    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n    n_y = Y_train.shape[0]                            # n_y : output size\n    costs = []                                        # To keep track of the cost\n    \n    # Create Placeholders of shape (n_x, n_y)\n    \n    X, Y = create_placeholders(n_x, n_y)\n    \n\n    # Initialize parameters\n    \n    parameters = initialize_parameters()\n    \n    \n    # Forward propagation: Build the forward propagation in the tensorflow graph\n   \n    Z3 = forward_propagation(X, parameters)\n   \n    \n    # Cost function: Add cost function to tensorflow graph\n    \n    cost = compute_cost(Z3, Y)\n   \n    \n    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n    \n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n    \n    \n    # Initialize all the variables\n    init = tf.global_variables_initializer()\n\n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n        \n        # Run the initialization\n        sess.run(init)\n        \n        # Do the training loop\n        for epoch in range(num_epochs):\n\n            epoch_cost = 0.                       # Defines a cost related to an epoch\n            num_minibatches = int(m \/ minibatch_size) # number of minibatches of size minibatch_size in the train set\n            seed = seed + 1\n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n\n            for minibatch in minibatches:\n\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                \n                # IMPORTANT: The line that runs the graph on a minibatch.\n                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n                ### START CODE HERE ### (1 line)\n                _ , minibatch_cost = sess.run([optimizer, cost],{X: minibatch_X, Y: minibatch_Y})\n                ### END CODE HERE ###\n                \n                epoch_cost += minibatch_cost \/ minibatch_size\n\n            # Print the cost every epoch\n            if print_cost == True and epoch % 100 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n            if print_cost == True and epoch % 5 == 0:\n                costs.append(epoch_cost)\n                \n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per fives)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # lets save the parameters in a variable\n        parameters = sess.run(parameters)\n        print (\"Parameters have been trained!\")\n\n        # Calculate the correct predictions\n        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n\n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n       \n        return parameters","3285b7f4":"parameters = model(X_train, Y_train, X_test, Y_test)","0fdb7ba4":"**Initialising parametres**","da2622af":"**<h2>Explaining the dataset<\/h2>**","db6d7436":"**Printing the keys in dataset**","0677bf86":"<h4>removing the rows which contains \"Workflow\" in EventOrigin column. <\/h4>","0d5a280f":"# Reading dataset","7687c7a6":"# Importing Libraries","6957fae6":"<h2> Creating own dataframe <\/h2>","f61f7bdd":"<h1> Model creation begins <\/h1>","51ecd3d0":"<h4>Dropping the useless data<\/h4>","07a06881":"<h2> Label encoding <\/h2>","196f54f3":"<h1> Training and testing split<\/h1>","3c372c52":"<h3> And our data part is:-<\/h3>","e4cacf08":"<h3> And resulting dataframe is:- <\/h3>","73a1410e":"<h2>In BPI 2017 dataset, we got to know that a process always starts from an activity labelled as \"A_Create Application\" and ends at activities labelled as \"A_Pending\u201d, \"A_Denied\u201d or \"A_Cancelled\".<\/h2><br>\n<h3>i)\"A_Pending\" - All documents have been received and the assessment is positive.<br>\nThe loan is paid to the customer.<br> \nii)\"A_Denied\" - The application doesn't satisfy the acceptance criteria. Hence, it is cancelled by the bank.<br>\niii)\"A_Cancelled\" - The application is cancelled if applicant does not get back to the bank after an offer was sent out.<br>\nIn addition to the above mention activities there is one more activity known as \u201cO_Create Offer\u201d which contains the data required for prediction of the outcome of the process. <br><\/h3>","79b47ebb":"<h1>Cleansing the dataset<\/h1>"}}