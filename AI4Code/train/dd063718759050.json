{"cell_type":{"f6db012f":"code","74507f33":"code","b56b98de":"code","f0d9faaa":"code","84eb0791":"code","6b9fa231":"code","0450dd83":"code","21b78de2":"code","657a0e26":"code","1138e8de":"code","d099b841":"code","7f44f4f3":"code","2a137ad0":"code","2133ac73":"code","865d29d0":"code","c8fc2dd4":"markdown","d40e8630":"markdown","04f1605f":"markdown","0ce67350":"markdown","b436795b":"markdown","5543a4f0":"markdown","3e98c891":"markdown","9300e214":"markdown","8da7e946":"markdown","76c5061f":"markdown","9002814d":"markdown","fab30f64":"markdown","4c45ad06":"markdown","a6d1e4bc":"markdown","964ff9cd":"markdown","a7339020":"markdown","dcc7c9e9":"markdown","39128336":"markdown","7da58262":"markdown","1ce20f4e":"markdown","23874536":"markdown","f259c6be":"markdown","4d2a3e90":"markdown","26baccdb":"markdown","50cdd211":"markdown","790f7dd6":"markdown"},"source":{"f6db012f":"import pandas as pd  \nimport numpy as np\nimport pandas_profiling\nimport seaborn as sns\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","74507f33":"#df = pd.read_csv(\"C:\/Users\/Marie-JeanneVieille\/Documents\/Happiness_2017.csv\")\ndf = pd.read_csv(\"..\/input\/world-happiness\/2017.csv\")\nprint(\"Le fichier a \" + str(df.shape[0]) + \" lignes et \" + str(df.shape[1]) + \" colonnes\")","b56b98de":"#Liste des colonnes et leur type \ndf.dtypes","f0d9faaa":"# 5 premi\u00e8res lignes du dataset\ndf.head(5)","84eb0791":"pandas_profiling.ProfileReport(df)","6b9fa231":"# Matrice des corr\u00e9lations : \ncor = df.corr() \nsns.heatmap(cor, square = True, cmap=\"coolwarm\",linewidths=.5,annot=True )\n#Pour choisr la couleur du heatmap : https:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html","0450dd83":"#Chargement du fonds de carte \n# Dispo ici https:\/\/tapiquen-sig.jimdofree.com\/english-version\/free-downloads\/world\/\nmap_df = gpd.read_file('..\/input\/world-country\/World_Countries.shp')\n\n#Jointure avec nos donn\u00e9es (on ne conserve que Country et Happiness.Rank)\nmap_df = map_df.set_index('COUNTRY').join(df[['Country','Happiness.Score']].set_index('Country'))\nmap_df.dropna(inplace=True)\nmap_df.reset_index(inplace=True)\n\n#Pr\u00e9paration de la carte\n# on fixe les seuils pour la couleur\nvmin, vmax = 0, 8\n# cr\u00e9ation de la figure et des axes\nfig, ax = plt.subplots(1, figsize=(18, 5))\n\n# Cr\u00e9ation de la carte\nmap_df.plot(column='Happiness.Score', cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8')\n# On supprime l'axe des abscisses\nax.axis('off')\n\n# On ajoute un titre\nax.set_title('Happiness.Score par pays', fontdict={'fontsize': '16', 'fontweight' : '2'})\n\n# On cr\u00e9\u00e9 la l\u00e9gende\nsm = plt.cm.ScalarMappable(cmap='Blues', norm=plt.Normalize(vmin=vmin, vmax=vmax))\nsm._A = []\n\n# On ajoute la l\u00e9gende\ncbar = fig.colorbar(sm)","21b78de2":"# On transforme Country en index\npd.DataFrame.set_index(df, 'Country',inplace=True)\n\n# On supprime 3 colonnes\ndf.drop(columns =['Happiness.Rank','Whisker.high', 'Whisker.low' ], inplace=True)\n","657a0e26":"#On stocke Happiness.Score (la variable \u00e0 pr\u00e9dire) dans cible\ncible = np.array(df['Happiness.Score'])\n\n#On supprime Happiness.Score du dataset\ndf= df.drop('Happiness.Score', axis = 1)\n\n#On conserve les noms de variable \u00e0 part\nliste_variables = list(df.columns)\n\n#On convertit le dataset en array\ndf = np.array(df)","1138e8de":"#On cr\u00e9\u00e9 4 dataset : \n#   - x_train contient 75% de x  \n#   - y_train contient le appiness.Score associ\u00e9 \u00e0 x_train\n# => x_train et y_train permettront d'entra\u00eener l'algorithme\n#\n#   - x_test contient 25% de x  \n#   - y_test contient le appiness.Score associ\u00e9 \u00e0 x_test\n# => x_test et y_test permettront d'\u00e9valuer la performance de l'algorithme une fois entrain\u00e9 sur le train\n\nx_train,x_test,y_train,y_test=train_test_split(df,cible,test_size=0.25, random_state=2020)","d099b841":"#On importe l'algorithme \u00e0 partir de sklearn\nfrom sklearn.ensemble import RandomForestRegressor\n\n#On cr\u00e9\u00e9 un Random Forest de 100 arbres \nrf = RandomForestRegressor(n_estimators = 100, random_state = 2020)\n\n#Et on lance le training sur notre dataset de train\nrf.fit(x_train, y_train)","7f44f4f3":"#On applique le mod\u00e8le que l'on vient d'entra\u00eener sur l'\u00e9chantillon de test\npredictions = rf.predict(x_test)","2a137ad0":"#On va calculer plusieurs erreurs entre la valeur pr\u00e9dite et le score de bonheur r\u00e9el (que nous avions stock\u00e9 dans y_test)\n#     - MAE : Mean Asolute Error\n#     - MAPE : Mean Absolute Percentage Error \n\n# MAE \nerreurs = abs(predictions - y_test)\nprint('Mean Absolute Error:', round(np.mean(erreurs), 2))","2133ac73":"# MAPE\nmape = 100 * (erreurs \/ y_test)\nprint('Mean Absolute Percentage Error :', round(np.mean(mape), 2), '%.')","865d29d0":"importances = rf.feature_importances_\nindices = np.argsort(importances)\n\n# style du graphique \nplt.style.use('fivethirtyeight')\n%matplotlib inline\n\nplt.figure(1)\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [liste_variables[i] for i in indices])\nplt.xlabel('Relative Importance')","c8fc2dd4":"## Set up","d40e8630":"Le Dataset est de bonne qualit\u00e9, il y a peu de travail \u00e0 faire ici :\n    - Transformer country en index puisqu'il s'agit de l'identifiant unique \n    - Supprimer Happiness.Rank, Whisker.high et Whisker.low","04f1605f":"La moyenne des erreurs est de 0,32 donc en moyenne on arrive \u00e0 pr\u00e9dire le score de bonheur \u00e0 0.32 pr\u00e8s","0ce67350":"On fait un \u00e9tat des lieux de la qualit\u00e9 des donn\u00e9es : \n    - Donn\u00e9es manquantes\n    - Donn\u00e9es corr\u00e9l\u00e9es\n    - Valeurs extr\u00eames\n    - Statistiques descriptives (moyenne, \u00e9cart-type, ...)\n    - Distribution des variables\n    \nRien de tel que la librairie pandas_profiling pour calculer tout cela automatiquement","b436795b":"#### Analyse de la qualit\u00e9 des donn\u00e9es : pandas_profiling","5543a4f0":"On transforme les donn\u00e9es en Numpy arrays pour pouvoir les utiliser dans le mod\u00e8le","3e98c891":"## Exploration","9300e214":"#### Liste et type de donn\u00e9es","8da7e946":"### Data Prep","76c5061f":"<!-- wp:paragraph -->\n<p>Nous allons cr\u00e9er un mod\u00e8le de pr\u00e9diction avec un Random Forest en passant par l'ensemble de ces \u00e9tapes : <\/p>\n<!-- \/wp:paragraph -->\n\n<!-- wp:list -->\n<ul><li>Chargement des donn\u00e9es<\/li><li>Exploration et visualisation des donn\u00e9es<\/li><li>Cr\u00e9ation d'un \u00e9chantillon d'apprentissage et de test<\/li><li>Phase d'apprentissage avec un algorithme Random Forest<\/li><li>\u00c9valuation de la performance sur l'\u00e9chantillon de test<\/li><li>Interpr\u00e9tation des r\u00e9sultats <\/li><\/ul>\n<!-- \/wp:list -->\n\nVous pouvez retrouver l'ensemble de l'article sur [lovelyanalytics.com](http:\/\/lovelyanalytics.com)","9002814d":"Avant de coder l'algorithme de pr\u00e9diction du score de bonheur nous allons faire un peu d'exploration du jeu de donn\u00e9es. L'id\u00e9e est de mieux comprendre les liens entre les diff\u00e9rentes variables et leur lien avec la variable \u00e0 pr\u00e9dire Happiness.Score. Cette premi\u00e8re \u00e9tape descriptive est importante, elle vous permettra de mieux comprendre les r\u00e9sultats de votre algorithme et vous pourrez vous assurer que tout est coh\u00e9rent. ","fab30f64":"Bilan pandas_profiling : \n    - pandas_profiling a \u00e9limin\u00e9 2 variables : Whisker.high et Whisker.low qui sont fortement corr\u00e9l\u00e9es avec Happiness.Score. Effectivement, quand on regarde la d\u00e9finition de ces 2 variables on voit que ces 2 variables correspondent \u00e0 l'intervalle de confiance de Happiness.Score. On peut donc les \u00e9carter pour notre analyse. On peut \u00e9galement supprimer Happiness.Rank\n    - Aucune valeur manquante\n    - 9 variables num\u00e9riques et 1 variable textuelle (on avait d\u00e9j\u00e0 calcul\u00e9 cette info un peu plus haut) \n    \n=> Globalement ce dataset est propre.\n\nOn regarde ensuite dans le d\u00e9tail chaque variable","4c45ad06":"## Chargement des donn\u00e9es","a6d1e4bc":"#### Analyse des corr\u00e9lations ","964ff9cd":"### Exploration & Visualisation","a7339020":"### Apprentissage","dcc7c9e9":"### Mod\u00e9lisation","39128336":"On analyse le contenu du fichier, le nom des colonnes et leur type, on explore les premi\u00e8res lignes du fichier","7da58262":"On choisit de faire l'apprentissage sur un \u00e9chantillon d'apprentissage de 75% des donn\u00e9es et de faire le test sur 25% des donn\u00e9es. On va \u00e9galement s\u00e9parer la variable \u00e0 pr\u00e9dire Happiness.Score des variables de pr\u00e9diction","1ce20f4e":"Le heatmap permet de repr\u00e9senter visuellement les corr\u00e9lations entre les variables. \nPlus la valeur est proche de 1 (couleur rouge fonc\u00e9e) plus la corr\u00e9lation est positive et forte. Au contraire plus la corr\u00e9lation est proche de 0 (bleu fonc\u00e9e) plus la corr\u00e9lation est n\u00e9gative et forte. \n\nConstats : \n\nCorr\u00e9lations avec la variable cible Happiness.score : \n     - Happiness.score est correl\u00e9 positivement avec Economy.GDP.per.Capita., Family et Health..Life.Expectancy. (donc globalement quand l'indicateur family augmente, Happiness.score augmente aussi) \n     - Happiness.score est correl\u00e9 n\u00e9gativement avec Generosity\n     - Pour les autres variables la corr\u00e9lation est plus faible \n     \nCorr\u00e9lations entre les autres variables : \n     - 2 variables semblent assez correl\u00e9es positivement : Health..Life.Expectancy. et Economy..GDP.per.Capita.\n     - G\u00e9n\u00e9rosite et Dystopia.Residual sont correl\u00e9es n\u00e9gativement avec la plupart des variables ","23874536":"On calcule les variables d'importance du mod\u00e8le, c'est \u00e0 dire celles qui contribuent le plus ","f259c6be":"#### G\u00e9olocalisation","4d2a3e90":"### Interpr\u00e9tation des r\u00e9sultats","26baccdb":"### Test","50cdd211":"##### Random Forest","790f7dd6":"#### Split du dataset en train et test"}}