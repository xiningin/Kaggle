{"cell_type":{"d6e48dfc":"code","d71fb884":"code","be3d5d8d":"code","adf09013":"code","e8c6bc64":"code","1a06ef67":"code","a5416313":"code","f9dd1f25":"code","25eedcaf":"code","61113fa4":"code","6e07080e":"code","0e0a5f27":"code","93beec57":"code","ec65e3bb":"code","fe53c754":"code","13fc721c":"code","8d710736":"code","b7a14e9b":"code","0fa9d399":"code","368fdd41":"code","8cfaf964":"code","314245c8":"code","0bc7eee2":"code","80353a1a":"markdown","46adce92":"markdown","b6674cec":"markdown","96713af8":"markdown","670f2eca":"markdown","f05af373":"markdown","9c37f034":"markdown"},"source":{"d6e48dfc":"# Reference - https:\/\/www.kaggle.com\/docs\/tpu\nfrom kaggle_datasets import KaggleDatasets\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()","d71fb884":"import tensorflow as tf\nprint(tf.__version__)","be3d5d8d":"# Configure TPU\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nBATCH_SIZE = 32 * tpu_strategy.num_replicas_in_sync","adf09013":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n# fix random seed for reproducibility\nseed = 42\nnp.random.seed(seed)\ntf.random.set_seed(seed)","e8c6bc64":"data_dir = tf.keras.utils.get_file(\n      fname='SST-2.zip',\n      origin='https:\/\/firebasestorage.googleapis.com\/v0\/b\/mtl-sentence-representations.appspot.com\/o\/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8',\n      extract=True)\ndata_dir = os.path.join(os.path.dirname(data_dir), 'SST-2')","1a06ef67":"train = os.path.join(data_dir, \"train.tsv\")\nvalid = os.path.join(data_dir, \"dev.tsv\")\ntest = os.path.join(data_dir, \"test.tsv\")","a5416313":"train_dataset = pd.read_csv(train, sep='\\t')\nvalid_dataset = pd.read_csv(valid, sep='\\t')\ntest_dataset = pd.read_csv(test, sep='\\t')","f9dd1f25":"train_dataset.info()","25eedcaf":"valid_dataset.info()","61113fa4":"train_dataset.head()","6e07080e":"train_reviews = train_dataset['sentence'].values\ntrain_sentiments = train_dataset['label'].values\n\nvalid_reviews = valid_dataset['sentence'].values\nvalid_sentiments = valid_dataset['label'].values\n\ntest_reviews = test_dataset['sentence'].values\n\ntrain_reviews.shape, valid_reviews.shape, test_reviews.shape","0e0a5f27":"tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')","93beec57":"import tqdm\n\ndef create_bert_input_features(tokenizer, docs, max_seq_length):\n    \n    all_ids, all_masks = [], []\n    for doc in tqdm.tqdm(docs, desc=\"Converting docs to features\"):\n        tokens = tokenizer.tokenize(doc)\n        if len(tokens) > max_seq_length-2:\n            tokens = tokens[0 : (max_seq_length-2)]\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n        ids = tokenizer.convert_tokens_to_ids(tokens)\n        masks = [1] * len(ids)\n        # Zero-pad up to the sequence length.\n        while len(ids) < max_seq_length:\n            ids.append(0)\n            masks.append(0)\n        all_ids.append(ids)\n        all_masks.append(masks)\n    encoded = np.array([all_ids, all_masks])\n    return encoded","ec65e3bb":"MAX_SEQ_LENGTH = 500\n\ntrain_features_ids, train_features_masks = create_bert_input_features(tokenizer, train_reviews, \n                                                                      max_seq_length=MAX_SEQ_LENGTH)\nval_features_ids, val_features_masks = create_bert_input_features(tokenizer, valid_reviews, \n                                                                  max_seq_length=MAX_SEQ_LENGTH)\n#test_features = create_bert_input_features(tokenizer, test_reviews, max_seq_length=MAX_SEQ_LENGTH)\nprint('Train Features:', train_features_ids.shape, train_features_masks.shape)\nprint('Val Features:', val_features_ids.shape, val_features_masks.shape)","fe53c754":"# Create TensorFlow datasets for better performance\ntrain_ds = (\n    tf.data.Dataset\n    .from_tensor_slices(((train_features_ids, train_features_masks), train_sentiments))\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n    \nvalid_ds = (\n    tf.data.Dataset\n    .from_tensor_slices(((val_features_ids, val_features_masks), valid_sentiments))\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)","13fc721c":"def get_training_model():\n    inp_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_ids\")\n    inp_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_masks\")\n    inputs = [inp_id, inp_mask]\n\n    hidden_state = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')(inputs)[0]\n    pooled_output = hidden_state[:, 0]    \n    dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled_output)\n    drop1 = tf.keras.layers.Dropout(0.25)(dense1)\n    dense2 = tf.keras.layers.Dense(256, activation='relu')(drop1)\n    drop2 = tf.keras.layers.Dropout(0.25)(dense2)\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(drop2)\n\n\n    model = tf.keras.Model(inputs=inputs, outputs=output)\n    model.compile(optimizer=tf.optimizers.Adam(learning_rate=2e-5, \n                                               epsilon=1e-08), \n                  loss='binary_crossentropy', metrics=['accuracy'])\n    return model","8d710736":"# Compile the model with TPU Strategy\nwith tpu_strategy.scope():\n    model = get_training_model()\n    \n# Train the model\nstart = time.time()\nmodel.fit(train_ds, \n    validation_data=valid_ds,\n    epochs=3)\nend = time.time()\nprint(f\"Training takes {end-start} seconds.\")","b7a14e9b":"model.save_weights('distillbert_ft_wts.h5')","0fa9d399":"converter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \n                                       tf.lite.OpsSet.SELECT_TF_OPS]\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\nopen(\"distilbert_sst_tflite.tflite\", \"wb\").write(tflite_model)","368fdd41":"!ls -lh distilbert_sst_tflite.tflite","8cfaf964":"converter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \n                                       tf.lite.OpsSet.SELECT_TF_OPS]\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.float16]\ntflite_model = converter.convert()\nopen(\"distilbert_sst_tflite_fp16.tflite\", \"wb\").write(tflite_model)","314245c8":"!ls -lh distilbert_sst_tflite_fp16.tflite","0bc7eee2":"# # ==============Representative dataset====================\n# train_features_ids = train_features_ids.astype(np.int32)\n# train_features_masks = train_features_masks.astype(np.int32)\n# train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features_ids, \n#     train_features_masks))\n\n# def representative_dataset_gen():\n#     for feature_id, feature_mask in train_tf_dataset.take(10):\n#         yield [feature_id, feature_mask]\n\n# # ==============Conversion====================\n# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \n#                                        tf.lite.OpsSet.SELECT_TF_OPS]\n# converter.representative_dataset = representative_dataset_gen\n# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n# tflite_model = converter.convert()\n# open(\"distilbert_sst_tflite_int.tflite\", \"wb\").write(tflite_model)","80353a1a":"### Dynamic-range","46adce92":"Reference - https:\/\/github.com\/huggingface\/tflite-android-transformers\/blob\/master\/models_generation\/distilbert.py","b6674cec":"### Float16\n","96713af8":"Slightly overfits. With more careful hyperparameter tuning this may be prevented. ","670f2eca":"***Integer quantization isn't supported currently for this model.*** ","f05af373":"### Integer","9c37f034":"*This notebook is adapted from [here](https:\/\/github.com\/dipanjanS\/deep_transfer_learning_nlp_dhs2019\/blob\/master\/notebooks\/6%20-%20Transformers%20-%20DistilBERT.ipynb). The purpose of this notebook is to show how to convert a custom DistilBERT-based model to TensorFlow Lite.*"}}