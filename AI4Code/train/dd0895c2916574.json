{"cell_type":{"4c53285c":"code","5aae1f8b":"code","82af42b7":"code","65901c47":"code","ed212fd7":"code","7d8b0ae6":"code","b6078937":"code","0989014d":"code","460cb5fb":"code","bf5897b1":"code","7c8dae95":"code","961b4f2d":"code","a596bbab":"code","bd69fe09":"code","55f4d145":"code","87c4b4eb":"code","ef72deac":"code","8d1920ad":"code","913e2c23":"code","7ee70500":"code","57ca7559":"code","3e36d1a6":"code","87f82d26":"code","c497abec":"code","c840191a":"code","52cc50cd":"code","1c76531b":"code","d9950dd1":"code","78d19373":"code","fca7ff43":"code","cf7b4128":"code","4df1e2a7":"code","6004dc7f":"code","786e3274":"code","474cb262":"code","91140668":"code","ef119f14":"code","43e5e069":"code","a1f35d21":"code","7aa19234":"code","db168d0c":"code","2201e02c":"code","88411453":"code","586d7c57":"code","c951a408":"code","cf37382b":"code","e1af25db":"code","bb1bcf51":"code","c88b264c":"code","443c0fab":"code","27d89baf":"code","a299508e":"code","6375e1d1":"code","916c5916":"code","febcceda":"code","f5ac23a4":"code","b7f7dc99":"code","14a96bb1":"code","55301a61":"code","b5733e6b":"code","f974d5d9":"code","ff0d5ad9":"code","f3a7309e":"code","cec2b962":"code","2f0e96ad":"code","5fd7d14f":"code","dfe9453a":"code","25ba1eaa":"code","644537d5":"code","b89b9bf2":"markdown","60d5739b":"markdown","a2d1c50f":"markdown","84289a82":"markdown","5ad15b62":"markdown","b0aea14a":"markdown","ea9fc086":"markdown","28cf985c":"markdown","6f18434a":"markdown","9658f66d":"markdown","dec9abdd":"markdown","cf6f2885":"markdown","1187cc76":"markdown","3dc92a14":"markdown","c2f3c755":"markdown","72f2cd12":"markdown","54a23574":"markdown","1cf52d44":"markdown","da0a7208":"markdown","4beaa98d":"markdown","4a7cd154":"markdown","5b9157d3":"markdown","fb30b95e":"markdown","b8efc1fb":"markdown","84afd2ad":"markdown","99667e9e":"markdown","0ea2ead6":"markdown","db8f3106":"markdown","4c0cef73":"markdown","59203be9":"markdown","8a2290b0":"markdown","55ef4d7c":"markdown","25e5ccd9":"markdown","4991ebf6":"markdown","fe082443":"markdown","e071f5af":"markdown","0af68aaa":"markdown","4fb819af":"markdown","1c85d5b3":"markdown","7a5701ec":"markdown","93aaf257":"markdown","f1b6ef7c":"markdown","93ab92ef":"markdown","b67c74da":"markdown","8779e855":"markdown","5c9ba63e":"markdown","2871e024":"markdown","7509b8d6":"markdown","8c1228d2":"markdown"},"source":{"4c53285c":"#import libraries\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,PolynomialFeatures,RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet\nfrom sklearn.linear_model import LassoCV,RidgeCV,ElasticNetCV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import GridSearchCV,KFold,RandomizedSearchCV,StratifiedKFold,cross_val_score\nfrom sklearn.metrics import r2_score\nsns.set_context(\"paper\", font_scale = 1, rc={\"grid.linewidth\": 3})\npd.set_option('display.max_rows', 100, 'display.max_columns', 400)\nfrom scipy.stats import skew,boxcox_normmax\nfrom scipy.special import boxcox1p\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRFRegressor,XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor","5aae1f8b":"#loading data\ntrain= pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.head()","82af42b7":"test.head()","65901c47":"# Let's look at the number of rows and columns in the dataset\nprint(train.shape)\nprint(test.shape)","ed212fd7":"# Getting insights of the features and outliers\ntrain.describe([0.25,0.50,0.75,0.99])","7d8b0ae6":"# Summary of the training dataset\nprint(train.info())","b6078937":"#summary of testing dataset\nprint(test.info())","0989014d":"#Checking percentage of null values present in training dataset \nmissing_num= train[train.columns].isna().sum().sort_values(ascending=False)\nmissing_perc= (train[train.columns].isna().sum()\/len(train)*100).sort_values(ascending=False)\nmissing= pd.concat([missing_num,missing_perc],keys=['Total','Percentage'],axis=1)\nmissing_train= missing[missing['Percentage']>0]\nmissing_train","460cb5fb":"#Checking percentage of null values present in testing dataset \nmissing_num= test[test.columns].isna().sum().sort_values(ascending=False)\nmissing_perc= (test[test.columns].isna().sum()\/len(test)*100).sort_values(ascending=False)\nmissing= pd.concat([missing_num,missing_perc],keys=['Total','Percentage'],axis=1)\nmissing_test= missing[missing['Percentage']>0]\nmissing_test","bf5897b1":"numerical = train.select_dtypes(include=['int64','float64']).drop(['SalePrice','Id'],axis=1)\nnumerical.head()","7c8dae95":"categorical = train.select_dtypes(exclude=['int64','float64'])\ncategorical.head()","961b4f2d":"def showvalues(ax,m=None):\n    for p in ax.patches:\n        ax.annotate(\"%.1f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\\\n                    ha='center', va='center', fontsize=14, color='k', rotation=0, xytext=(0, 7),\\\n                    textcoords='offset points',fontweight='light',alpha=0.9) ","a596bbab":"plt.figure(figsize=(20,20))\nplt.subplot(2,1,1)\nax1=sns.barplot(x=missing_train.index,y='Percentage',data=missing_train)\nshowvalues(ax1)\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.subplot(2,1,2)\nax2=sns.barplot(x=missing_test.index,y='Percentage',data=missing_test)\nshowvalues(ax2)\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.show()","bd69fe09":"# Dropping Id column from train and test test\ntrain.drop('Id',axis=1,inplace=True)\ntest.drop('Id',axis=1,inplace=True)\nprint(train.shape)\nprint(test.shape)","55f4d145":"len(train.select_dtypes(include=['int64','float64']).columns)","87c4b4eb":"#Visualising numerical predictor variables with Target Variables\ntrain_num = train.select_dtypes(include=['int64','float64'])\nfig,axs= plt.subplots(12,3,figsize=(20,80))\n#adjust horizontal space between plots \nfig.subplots_adjust(hspace=0.6)\nfor i,ax in zip(train_num.columns,axs.flatten()):\n    sns.scatterplot(x=i, y='SalePrice', hue='SalePrice',data=train_num,ax=ax,palette='viridis_r')\n    plt.xlabel(i,fontsize=12)\n    plt.ylabel('SalePrice',fontsize=12)\n    #ax.set_yticks(np.arange(0,900001,100000))\n    ax.set_title('SalePrice'+' - '+str(i),fontweight='bold',size=20)","ef72deac":"##Visualising Categorical predictor variables with Target Variables\ndef facetgrid_boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n    \n\nf = pd.melt(train, id_vars=['SalePrice'], value_vars=sorted(train[categorical.columns]))\ng = sns.FacetGrid(f, col=\"variable\", col_wrap=3, sharex=False, sharey=False, size=5)\ng = g.map(facetgrid_boxplot, \"value\", \"SalePrice\")","8d1920ad":"# Distribution of Target variable (SalePrice)\nplt.figure(figsize=(8,6))\nsns.distplot(train['SalePrice'],hist_kws={\"edgecolor\": (1,0,0,1)})","913e2c23":"# Skew and kurtosis for SalePrice \nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","7ee70500":"#Applying log transformation to remove skewness and make target variable normally distributed\ntrain['SalePrice'] = np.log1p(train['SalePrice'])","57ca7559":"#Plotting graph again to see if its normally distributed or not and see outliers\n# Distribution of Target variable (SalePrice)\nplt.figure(figsize=(8,6))\nsns.distplot(train['SalePrice'],hist_kws={\"edgecolor\": (1,0,0,1)})","3e36d1a6":"#Correlation between variables to check multicollinearity \n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nplt.subplots(figsize = (30,20))\nmask = np.zeros_like(train_num.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n#Plotting heatmap\nsns.heatmap(train_num.corr(), cmap=sns.diverging_palette(20, 220, n=200), mask = mask, annot=True, center = 0)","87f82d26":"## Deleting those two values with outliers. \ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop = True, inplace = True)\n","c497abec":"y=train['SalePrice']\ntrain_df=train.drop('SalePrice',axis=1)\ntest_df = test\ndf_all= pd.concat([train_df,test_df]).reset_index(drop=True)","c840191a":"df_all['age']=df_all['YrSold']-df_all['YearBuilt']\n# Some of the non-numeric predictors are stored as numbers; convert them into strings \n#will convert those columns into dummy variables later.\ndf_all[['MSSubClass']] = df_all[['MSSubClass']].astype(str) \ndf_all['YrSold'] = df_all['YrSold'].astype(str) #year\ndf_all['MoSold'] = df_all['MoSold'].astype(str) #month\n","52cc50cd":"#Functional: Home functionality (Assume typical unless deductions are warranted)\ndf_all['Functional'] = df_all['Functional'].fillna('Typ')\ndf_all['Electrical'] = df_all['Electrical'].fillna('SBrkr') #Filling with modef\n# data description states that NA refers to \"No Pool\"\ndf_all[\"PoolQC\"] = df_all[\"PoolQC\"].fillna(\"None\")\n# Replacing the missing values with 0, since no garage = no cars in garage inferred from data dictionary\ndf_all['GarageYrBlt'] = df_all['GarageYrBlt'].fillna(0)\ndf_all['KitchenQual'] = df_all['KitchenQual'].fillna(\"TA\")\ndf_all['Exterior1st'] = df_all['Exterior1st'].fillna(df_all['Exterior1st'].mode()[0])\ndf_all['Exterior2nd'] = df_all['Exterior2nd'].fillna(df_all['Exterior2nd'].mode()[0])\ndf_all['SaleType'] = df_all['SaleType'].fillna(df_all['SaleType'].mode()[0])\n# Replacing the missing values with None inferred from data dictionary \nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    df_all[col] = df_all[col].fillna('None')\n# Replacing the missing values with None \n# NaN values for these categorical basement df_all, means there's no basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    df_all[col] = df_all[col].fillna('None')\n#Replacing missing value it with median beacuse of outliers\ndf_all['LotFrontage'] = df_all.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n# Replacing the missing values with None \n# We have no particular intuition around how to fill in the rest of the categorical df_all\n# So we replace their missing values with None\nobjects = []\nfor i in df_all.columns:\n    if df_all[i].dtype == object:\n        objects.append(i)\ndf_all.update(df_all[objects].fillna('None'))\n\nnumeric_dtypes = [ 'int64','float64']\nnumerics = []\nfor i in df_all.columns:\n    if df_all[i].dtype in numeric_dtypes:\n        numerics.append(i)\ndf_all.update(df_all[numerics].fillna(0))\n\ndf_all['MSZoning'] = df_all.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","1c76531b":"#Checking percentage of null values present in dataset \nmissing_perc= (df_all[df_all.columns].isna().sum()\/len(df_all)*100).sort_values(ascending=False)\nprint(missing_perc[missing_perc>0].sum()) #No missing values\n","d9950dd1":"# We have already removed skewness from target variable (SalePrice) and made it normally distributed.\n# Lets find out if numerical predictor variables are largely skewed or not\ndf_all_num = df_all.select_dtypes(include=['int64','float64'])\nskew_features = df_all_num.apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features","78d19373":"f, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=df_all_num , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","fca7ff43":"# Normalize skewed features using a Box-Cox power transformation, we can use other techniques but am using boxpox\n# as it works very well on this dataset\nfor i in skew_index:\n    df_all[i] = boxcox1p(df_all[i], boxcox_normmax(df_all[i] + 1.002))","cf7b4128":"# Let's make sure we handled all the skewed values\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=df_all[skew_index] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","4df1e2a7":"#NOt useful columns in our predictions, more than 99% rows have same value.\nprint(df_all['Utilities'].value_counts())\n#NOt useful columns in our predictions, more than 99% rows have same value.\nprint(df_all['Street'].value_counts())\n#NOt useful columns in our predictions, more than 99% rows have same value.\nprint(df_all['PoolQC'].value_counts())","6004dc7f":"df_all=df_all.drop(['Utilities', 'Street', 'PoolQC'], axis=1) # not useful df_all, evident from above\n# vintage house with remodified version of it plays a important role in prediction(i.e. high price )\ndf_all['YrBltAndRemod']=df_all['YearBuilt']+df_all['YearRemodAdd']\n#Overall area for all floors and basement plays an important role, hence creating total area in square foot column\ndf_all['Total_sqr_footage'] = (df_all['BsmtFinSF1'] + df_all['BsmtFinSF2'] +\n                                 df_all['1stFlrSF'] + df_all['2ndFlrSF'])\n# Creating derived column for total number of bathrooms column\ndf_all['Total_Bathrooms'] = (df_all['FullBath'] + (0.5 * df_all['HalfBath']) +\n                               df_all['BsmtFullBath'] + (0.5 * df_all['BsmtHalfBath']))\n#Creating derived column for total porch area \ndf_all['Total_porch_sf'] = (df_all['OpenPorchSF'] + df_all['3SsnPorch'] + df_all['EnclosedPorch'] + \\\n                              df_all['ScreenPorch'] + df_all['WoodDeckSF'])\n","786e3274":"df_all['has_pool'] = df_all['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['has_2ndfloor'] = df_all['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['has_garage'] = df_all['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['has_bsmt'] = df_all['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['has_fireplace'] = df_all['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['has_openporch'] =df_all['OpenPorchSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['has_wooddeck'] =df_all['WoodDeckSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['has_enclosedporch'] = df_all['EnclosedPorch'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['has_3ssnporch']=df_all['3SsnPorch'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['has_openporch'] = df_all['OpenPorchSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['has_screenporch'] = df_all['ScreenPorch'].apply(lambda x: 1 if x > 0 else 0)\n\n","474cb262":"#<-------------------------- Check Again ----------------------->\ndf_all['TotalBsmtSF'] = df_all['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\ndf_all['2ndFlrSF'] = df_all['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\ndf_all['LotFrontage'] = df_all['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\ndf_all['MasVnrArea'] = df_all['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\ndf_all['BsmtFinSF1'] = df_all['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n\n","91140668":"def log_transform(result, features):\n    m = result.shape[1]\n    for feature in features:\n        result = result.assign(newcol=pd.Series(np.log(1.01+result[feature])).values)   \n        result.columns.values[m] = feature + '_log'\n        m += 1\n    return result\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd']\n\ndf_all = log_transform(df_all, log_features)","ef119f14":"df_all_num= df_all.select_dtypes(include=['float64','int64']).columns  # Numerical columns\ndf_all_temp = df_all.select_dtypes(exclude=['float64','int64']) # selecting object and categorical features only\ndf_all_dummy= pd.get_dummies(df_all_temp)\ndf_all=pd.concat([df_all,df_all_dummy],axis=1) # joining converted dummy feature and original df_all dataset\ndf_all= df_all.drop(df_all_temp.columns,axis=1) #removing original categorical columns\ndf_all.shape","43e5e069":"X= df_all[:len(train)] #converted into train data\nZ_test= df_all[len(train):] #test data\nprint('Train Data Shape:',X.shape) #train set shape\nprint('Test Data Shape:',Z_test.shape)  #test set shape","a1f35d21":"#based on describe method and scatter plot, removing outliers\noutl_col = ['GrLivArea','GarageArea','TotalBsmtSF','LotArea']\n\ndef drop_outliers(x):\n    list = []\n    for col in outl_col:\n        Q1 = x[col].quantile(.25)\n        Q3 = x[col].quantile(.99)\n        IQR = Q3-Q1\n        x =  x[(x[col] >= (Q1-(1.5*IQR))) & (x[col] <= (Q3+(1.5*IQR)))] \n    return x   \nX = drop_outliers(X)\noutliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])\nprint(X.shape)","7aa19234":"def redundant_feature(df):\n    redundant = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        count_max = counts.iloc[0]\n        if count_max \/ len(df) * 100 > 99.94:\n            redundant.append(i)\n    redundant = list(redundant)\n    return redundant\n\n\nredundant_features = redundant_feature(X)\n\nX = X.drop(redundant_features, axis=1)\nZ_test = Z_test.drop(redundant_features, axis=1)","db168d0c":"print('Train Data Shape:',X.shape) #train set shape\nprint('Test Data Shape:',Z_test.shape)  #test set shape","2201e02c":"kfold= KFold(n_splits=11,random_state=42,shuffle=True) #kfold cross validation","88411453":"# Error function to compute error\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n#Assigning scoring paramter to 'neg_mean_squared_error' beacause 'mean_squared_error' is not \n# available inside cross_val_score method\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfold))\n    return (rmse)","586d7c57":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","c951a408":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)","cf37382b":"ridge=Ridge()\nparams= {'alpha': [5,8,10,10.1,10.2,10.3,10.35,10.36,11,12,15]}\nscaler=RobustScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\ngrid_ridge=GridSearchCV(ridge, param_grid=params,cv=kfold,scoring='neg_mean_squared_error')\ngrid_ridge.fit(X_train,y_train)\nalpha = grid_ridge.best_params_\nridge_score = grid_ridge.best_score_\nprint(\"The best alpha value found is:\",alpha['alpha'],'with score:',ridge_score)\n\nridge_alpha=Ridge(alpha=alpha['alpha'])\nridge_alpha.fit(X_train,y_train)\ny_pred_train=ridge_alpha.predict(X_train)\ny_pred_test=ridge_alpha.predict(X_test)\n\nprint('RMSE train = ',rmsle(y_train,y_pred_train))\nprint('RMSE test = ',rmsle(y_test,y_pred_test))","e1af25db":"scores={}\nalphas_ridge = [15, 15.1, 15.2, 15.3, 15.4, 15.5] #Best value of alpha parmaters for Ridge regression\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_ridge, cv=kfold))\nscore = cv_rmse(ridge)\nprint(score)\nprint(\"ridge RMSE: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['ridge'] = (score.mean(), score.std()) #Printing standard deviation to check deviation of scores\n\n","bb1bcf51":"alphas_lasso = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008] #Best value of alpha parmaters for lasso\nlasso = make_pipeline(RobustScaler(), LassoCV(alphas=alphas_lasso, cv=kfold))\nscore = cv_rmse(lasso)\nprint(score)\nprint(\"lasso RMSE: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lasso'] = (score.mean(), score.std()) #Printing standard deviation to check deviation of scores\n","c88b264c":"alpha_elnet= [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\nl1ratio_elnet = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1000000, alphas=alpha_elnet, \\\n                                                        cv=kfold, l1_ratio=l1ratio_elnet))\nscore=cv_rmse(elasticnet)\nprint(score)\nprint(\"Elasticnet RMSE: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['elasticnet'] = (score.mean(), score.std()) #Printing standard deviation to check deviation of scores","443c0fab":"svr = make_pipeline(RobustScaler(), SVR(C= 19, epsilon= 0.008, gamma=0.00015))\nscore=cv_rmse(svr)\nprint(score)\nprint(\"SVR RMSE: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['svr'] = (score.mean(), score.std()) #Printing standard deviation to check deviation of scores","27d89baf":"gbr= GradientBoostingRegressor(n_estimators=6000,learning_rate=0.01,max_depth=3,\\\n                              min_samples_leaf=15,max_features='sqrt',min_samples_split=10,loss='huber',\\\n                              random_state=42)\nscore=cv_rmse(gbr)\nprint(score)\nprint(\"GBR RMSE: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gbr'] = (score.mean(), score.std()) #Printing standard deviation to check deviation of scores","a299508e":"lgbm =  LGBMRegressor(objective='regression', num_leaves=4,learning_rate=0.01, n_estimators=6000,\n                                       max_bin=200, bagging_fraction=0.75,bagging_freq=5, bagging_seed=7,\n                                       feature_fraction=0.2,feature_fraction_seed=7,verbose=-1)\nscore=cv_rmse(lgbm)\nprint(score)\nprint(\"LGBM RMSE: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgbm'] = (score.mean(), score.std()) #Printing standard deviation to check deviation of scores","6375e1d1":"xgb = XGBRegressor(learning_rate=0.01,n_estimators=3460, max_depth=3, min_child_weight=0, gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7, objective='reg:squarederror', nthread=-1,\n                                     scale_pos_weight=1, seed=27, reg_alpha=0.00006)\nscore=cv_rmse(xgb)\nprint(score)\nprint(\"XGBOOST RMSE: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgb'] = (score.mean(), score.std()) #Printing standard deviation to check deviation of xgb","916c5916":"#bc = BaggingRegressor(n_estimators=3000,max_features=280)\n#score=cv_rmse(bc)\n#print(score)\n#print(\"XGBOOST RMSE: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n#scores['bc'] = (score.mean(), score.std()) #Printing standard deviation to check deviation of xgb","febcceda":"etr = ExtraTreesRegressor(n_estimators=60,random_state=42)\nscore=cv_rmse(etr)\nprint(score)\nprint(\"XGBOOST RMSE: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['etr'] = (score.mean(), score.std()) #Printing standard deviation to check deviation of xgb","f5ac23a4":"stack_reg = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgb, lgbm),\n                                meta_regressor=xgb,\n                                use_features_in_secondary=True)","b7f7dc99":"lasso_final= lasso.fit(X,y)","14a96bb1":"ridge_final=ridge.fit(X,y)","55301a61":"elasticnet_final=elasticnet.fit(X,y)","b5733e6b":"svr_final=svr.fit(X,y)","f974d5d9":"gbr_final=gbr.fit(X,y)","ff0d5ad9":"lgbm_final=lgbm.fit(X,y)","f3a7309e":"xgb_final=xgb.fit(X,y)","cec2b962":"stack_reg_final=stack_reg.fit(X,y)","2f0e96ad":"def blend_models_predict(X):\n    return ((0.025* elasticnet_final.predict(X)) + \\\n            (0.025 * lasso_final.predict(X)) + \\\n            (0.025 * ridge_final.predict(X)) + \\\n            (0.025* svr_final.predict(X)) + \\\n            (0.62 * gbr_final.predict(X)) + \\\n            (0.03 * xgb_final.predict(X)) + \\\n            (0.03 * lgbm_final.predict(X)) + \\\n            (0.22 * stack_reg_final.predict(np.array(X))))  ##best best best best *5","5fd7d14f":"print('RMSLE score on train data:')\nblended_score=rmsle(y, blend_models_predict(X))\nprint(blended_score)\nscores['blended'] = (blended_score, 0)","dfe9453a":"# Plot the predictions for each model\n#sns.set_style(\"white\")\nfig = plt.figure(figsize=(20, 8))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","25ba1eaa":"print('Prediction_Submission')\nsubmission = submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(Z_test)))","644537d5":"q1 = submission['SalePrice'].quantile(0.005)\nq2 = submission['SalePrice'].quantile(0.995)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\".\/submission_prediction.csv\", index=False)","b89b9bf2":"#### Outlier Treatment","60d5739b":"<b>`Merging train and test data to do handle missing values in both train and test data, handling datatype issues, skewness and transformation.`","a2d1c50f":"## <font color=darkred>`If you found this notebook helpful , some upvotes would be very much appreciated - That will keep me motivated :)`\n","84289a82":"# XGBOOST Regressor","5ad15b62":"## 3. Data Preparation ","b0aea14a":"<b>Features are looking normally distributed now, not much skewness in the distribution of predictor variables.","ea9fc086":"# <font color=darkgreen>Machine Learning models implented in this kernel version:\n- Cross Validation: Using 11-fold cross-validation \n- GridSearchCV for tuning hyperparamters and doing cv\n- Models:\n    - Ridge\n    - Lasso\n    - Elastic Net\n    - XGBoost\n    - Extra tree Regressor\n    - Bagging Regressor\n    - Support vector Regressor(SVR)\n    - Gradient Boositng\n    - Light GBM\n    - StackingCV Regressor\n    - To make final predictions, I blended above models predictions together to get more robust predictions.Blending model using best models.\n","28cf985c":"### Lightgbm","6f18434a":"# Support Vector Regressor(SVR)","9658f66d":"### XGBoost","dec9abdd":"### Creating more derived feature by doing log transormation","cf6f2885":"#### I have done detailed exploratory data analysis of House Prices dataset long with different machine learning models and finding the best suited ML model. \n- Our goal is to predict the accrate SalePrice with given features.\n- Our models are evaluated on the Root-Mean-Squared-Error (RMSE).","1187cc76":"# Elastic Net Regression ","3dc92a14":"## <font color=darkred>`If you found this notebook helpful , some upvotes would be very much appreciated - That will keep me motivated :)`\n","c2f3c755":"# Gradient Boosting Regressor","72f2cd12":"# STACKING REGRESSOR","54a23574":"####  _Robust Scaler: Used when have outliers and dont wnat them to have much influence. Robust scaler standardizes a feature by removing the median and dividing each feature by Interquartile Range. Outliers have less influence than Minmaxscaler. range is larger than Minmax scaler and Standard Scaler_","1cf52d44":"# <font color=darkgred>Problem Statement\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each house.","da0a7208":"# RIDGE (L2 regularization)","4beaa98d":"#### Creating dummy variables for categorical and object variables","4a7cd154":"_<b>few observation from boxplots,As we can see,SalePrice for fullbath=3 is higher than 0,1, or 2. SalePrice for OverallQal=10 which is very excellent is higher than others._","5b9157d3":"![house](https:\/\/housepriceprediction.com\/wp-content\/uploads\/2018\/07\/11679.4c96032809c10d54e3e216015aecf32a_XL.jpg)","fb30b95e":"#### Handling Skewness in predictor variables\n\nWhy we are removing Skewness from our model and predictor variables?\n- For coefficients to be interpretable, linear regression assumes a bunch of things. \n    - `Homoscedasticity`( i.e. The errors your model commits should have the same variance) and error terms should be `normally distributed`.\n    - Following the linear regression assumptions is important if you want to either `interpret the coefficients` and can be used in business goals.\n    - When the dependent variable is as skewed as our data is, the residuals usually will be too. Hence, we are `handling skewness` in our data.\n    - <b>This model will then be used to understand how exactly the prices vary with the variables`.","b8efc1fb":"### Elastic net","84afd2ad":"#### _19 attributes have missing values and 5 features( PoolQC,MiscFeature,Alley,Fence,FireplaceQu) have missing percentage greater than 45%_","99667e9e":"## THANK YOU :)","0ea2ead6":"### 2. Data Visualisation\n\nLet's now spend some time doing what is arguably the most important step - **understanding the data**.\n- Understanding the distribution of various numeric variables \n- If there is some obvious multicollinearity going on, this is the first place to catch it\n- Here's where you'll also identify if some predictors directly have a strong association with the outcome variable\n\nThere are 1460 instances of training data and 1459 of test data. Total number of attributes equals 81 in train data and 80 in `<b>test data excluding SalePrice<\/b>`, of which 36 is quantitative, 43 categorical + Id and SalePrice.\n\n<b>`Numerical features`<\/b>: `1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold`\n\n<b>`Categorical features`<\/b>: `Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities,`","db8f3106":"## <font color=darkgreen>Approach:\n- <i><b>Data cleaning and statistical analysis.\n- Exploratory Data Analysis and visualisations.\n- Machine learning modelling and Prediction(Lasso, Ridge, Boosting Algorithms, Blend Models\n- Finding the best Machine learning model based on various score","4c0cef73":"#### Handling Missing Values for 19 features which have missing values mentioned above","59203be9":"#### _<font color=darkred>Inference: SalePrice is not normally distributed, it is positively or right skewed_","8a2290b0":"### Cross Validation","55ef4d7c":"<b>`Now SalePrice is normally distributed`","25e5ccd9":"# BAGGING Regressor","4991ebf6":"# <font color='darkbrown'><---Advanced Regression (Predicting Housing Price)--->\n","fe082443":"### Splitting data into train and test set","e071f5af":"## Fitting Models on train set","0af68aaa":"### Creating Derived features from raw attributes","4fb819af":"### Lasso","1c85d5b3":"### Gradient Boositng","7a5701ec":"### Stacking Regressor","93aaf257":"### 1. Data Understanding and Exploration","f1b6ef7c":"### Ridge","93ab92ef":"### SVR","b67c74da":"## Function to show values on bar plot","8779e855":"<b>`For each feature, it counts the values of that feature. If the most recurrent value of the feature is repeated almost in all the instances (*zeros \/ len(X) * 100 > 99.94*). Then it drops these features because their values are almost the same for all instances and will not help in learning process and those features are not useful in our prediction.`","5c9ba63e":"### Plotting Missing value in train and test data","2871e024":"# LASSO (L1 regularization)","7509b8d6":"_<b>As we can see, the multicollinearity still exists in various features. However, we will keep them for now and let the models(e.g. Regularization models such as Lasso, Ridge) do the clean up later on. Let's go through some of the correlations that still exists._\n\n* There is 0.83 or 83% correlation between **GarageYrBlt** and **YearBuilt**. \n* 83% correlation between **TotRmsAbvGrd** and **GrLivArea**. \n* 89% correlation between **GarageCars** and **GarageArea**. \n* Similarly many other features such as**BsmtUnfSF**, **FullBath** have good correlation with other independent feature.","8c1228d2":"# LGBM Regressor"}}