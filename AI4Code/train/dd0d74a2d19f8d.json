{"cell_type":{"3038ee4a":"code","eb3ca0f6":"code","6e83468b":"code","6d494e8a":"code","07f45327":"code","ab1ccb05":"code","54f6b397":"code","8f4ed0ab":"code","28909d53":"code","444536ea":"code","2d776cd9":"code","e712a961":"code","b0090c41":"code","733e68f5":"code","6e53de80":"code","6aa8128d":"markdown","75fb76ca":"markdown","f291fe14":"markdown"},"source":{"3038ee4a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb3ca0f6":"x_temp = np.load(\"\/kaggle\/input\/sign-language-digits-dataset\/X.npy\")\ny_temp = np.load(\"\/kaggle\/input\/sign-language-digits-dataset\/Y.npy\")\n\nX = np.concatenate((x_temp[204:409],x_temp[822:1027]), axis = 0)\n\nresulSetTrue = np.ones(205)\nresultSetFalse = np.zeros(205)\n\nY = np.concatenate((resulSetTrue,resultSetFalse), axis = 0) \n\nprint(\"X shape : \",X.shape)\nprint(\"Y shape : \",Y.shape)","6e83468b":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X,Y,test_size = 0.15)\n\n#Now we have three dimensional array,but we need 2 dimensional array\nx_train_reduced = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2])\nx_test_reduced = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2])\n\nprint(\"After reducing...\")\nprint(\"X Train shape : \",x_train_reduced.shape)\nprint(\"Y Train shape : \",x_test_reduced.shape)\n","6d494e8a":"#Now Transpose\nx_train = x_train_reduced.T\nx_test = x_test_reduced.T\ny_train = y_train.T\ny_test = y_test.T","07f45327":"def initialize_parametres(x_train, y_train):\n    parametres = {\n        \"weight1\" : np.random.randn(3,x_train.shape[0]) * 0.1,\n        \"bias1\" : np.random.randn(3,1),\n        \"weight2\" : np.random.randn(y_train.shape[0],3) * 0.1,\n        \"bias2\" : np.random.randn(y_train.shape[0],1)\n    }\n    \n    return parametres","ab1ccb05":"def sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","54f6b397":"def compute_cost(A2, y):\n    logProbs = np.multiply(np.log(A2), y)\n    cost = -np.sum(logProbs)\/y.shape[0]\n    \n    return cost","8f4ed0ab":"def forward_propagation(x_train, parametres):\n    Z1 = np.dot(parametres[\"weight1\"], x_train) + parametres[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parametres[\"weight2\"], A1) + parametres[\"bias2\"]\n    A2 = sigmoid(Z2)\n    \n    cache = {\n        \"Z1\" : Z1,\n        \"A1\" : A1,\n        \"Z2\" : Z2,\n        \"A2\" : A2\n    }\n    \n    return cache","28909d53":"def backward_propagation(parametres, cache, X ,Y):\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parametres[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads\n","444536ea":"def update_parameters(parameters, grads, learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters","2d776cd9":"def predict(parameters,x_test):\n    cache = forward_propagation(x_test,parameters)\n    A2 = cache[\"A2\"]\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    for i in range(A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","e712a961":"def two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n    cost_list = []\n    index_list = []\n\n    parameters = initialize_parametres(x_train, y_train)\n\n    for i in range(0, num_iterations):\n\n        cache = forward_propagation(x_train, parameters)\n        \n        A2 = cache[\"A2\"]\n\n        cost = compute_cost(A2, y_train)\n\n        grads = backward_propagation(parameters, cache, x_train, y_train)\n\n        parameters = update_parameters(parameters, grads)\n        \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict(parameters,x_test)\n    y_prediction_train = predict(parameters,x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters","b0090c41":"parameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=1000)","733e68f5":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.models import load_model\n\nx_train = x_train_reduced\nx_test = x_test_reduced\ny_train = y_train\ny_test = y_test\n","6e53de80":"def createNeuralNetworks():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\n\nclassifier = KerasClassifier(build_fn = createNeuralNetworks, epochs = 100) #iteration = epochs\naccuracies = cross_val_score(estimator=classifier, X=x_train, y=y_train, cv=3)\nmean = accuracies.mean()\nvariance = accuracies.std()\n\nprint(\"Accuracy Mean = \",str(mean))\nprint(\"Accuracy Variance = \",str(variance))","6aa8128d":"# With Keras","75fb76ca":"# WITHOUT KERAS","f291fe14":"* **ANN**"}}