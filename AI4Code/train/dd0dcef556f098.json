{"cell_type":{"872d78d0":"code","fca24949":"code","08497059":"code","5ae07b89":"code","f18a19f2":"code","25fe537e":"code","810be85d":"code","5fd28132":"code","d41375f6":"code","804bb942":"code","100f0636":"code","3921ed1b":"code","9e5433e2":"code","0f9da331":"code","fbba930e":"markdown","d6d176de":"markdown","c8e86175":"markdown","a0e50194":"markdown","950c5886":"markdown","e6df8d92":"markdown","608b6119":"markdown","ce7b1a28":"markdown","5c078a4e":"markdown","bf091bed":"markdown","4aac382f":"markdown","4869c473":"markdown","ba45f519":"markdown"},"source":{"872d78d0":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom itertools import count\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense,Embedding,Bidirectional,Dropout,SpatialDropout1D,GlobalMaxPool1D,LSTM,BatchNormalization,Conv1D,MaxPool1D\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras import regularizers","fca24949":"train = pd.read_csv(\"..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv.zip\",sep=\"\\t\")\ntest = pd.read_csv(\"..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv.zip\",sep=\"\\t\")","08497059":"train.head()","5ae07b89":"sns.countplot(train['Sentiment'])\nplt.title(\"No of Tweet Sentiments\")","f18a19f2":"def create_vocabulary(df):\n    counter = count(2)  # index 0 reserved for padding, index 1 for UNK token\n    vocabulary = dict()\n    lemmatizer = WordNetLemmatizer()\n    for k in df['Phrase']:\n        tokens = k.lower().split(\" \")\n        for token in tokens:\n            lemmatoken = lemmatizer.lemmatize(token)\n            if lemmatoken in vocabulary:\n                continue\n            vocabulary[lemmatoken] = next(counter)\n    print(\"Vocabulary length: {}\".format(max(vocabulary.values())))  \n    return vocabulary","25fe537e":"def preprocess_df(df, vocabulary, max_sentence_length):\n    vocabulary_length = max(vocabulary.values())\n    X = []\n    # Use the same function for test sets.\n    Y = label_binarize(df.Sentiment.to_xarray(), classes=[0, 1, 2, 3, 4]) if 'Sentiment' in df else None\n    lemmatizer = WordNetLemmatizer()\n    for sample in df.iterrows():\n        tokens = sample[1]['Phrase'].lower().split(\" \")\n        vocab_tokens = []\n        for i in range(max_sentence_length):\n            try:\n                vocab_tokens.append(vocabulary.get(lemmatizer.lemmatize(tokens[i]), 1))  # 1 : UNK token\n            except IndexError:\n                vocab_tokens.append(0)  # 0 : padding token\n        X.append(vocab_tokens)\n    return np.asarray(X), Y","810be85d":"vocabulary = create_vocabulary(train)\nX, Y = preprocess_df(train, vocabulary, 52)","5fd28132":"train_X,x_valid,train_Y,y_valid = train_test_split(X,Y,test_size=0.2,random_state=42)","d41375f6":"model = keras.models.Sequential()\nmodel.add(keras.layers.Embedding(input_dim=15189, output_dim=10, mask_zero=True))\n\nmodel.add(Conv1D(128,3,activation='relu',padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv1D(128,3,activation='relu',padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv1D(128,3,activation='relu',padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.15))\n\nmodel.add(Conv1D(64,3,activation='relu',padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.15))\n\nmodel.add(Conv1D(64,3,activation='relu',padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.15))\n\nmodel.add(Bidirectional(LSTM(1280,recurrent_dropout=0.5,dropout=0.2,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(640,recurrent_dropout=0.5,dropout=0.2,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(640,recurrent_dropout=0.5,dropout=0.2,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(320,recurrent_dropout=0.5,dropout=0.2,return_sequences=True)))\n\nmodel.add(GlobalMaxPool1D())\n\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.15))\n\nmodel.add(Dense(32,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.15))\n\nmodel.add(Dense(32,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.15))\n\nmodel.add(Dense(5,activation='softmax'))\n\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\nmodel.summary()","804bb942":"history=model.fit(x=train_X, y=train_Y, batch_size=256, epochs=15,validation_data=(x_valid,y_valid))","100f0636":"def plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history[\"val_\"+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string,\"val_\"+string])\n    plt.show()\nplot_graphs(history,'acc')\nplot_graphs(history,'loss')","3921ed1b":"test_X,test_Y = preprocess_df(test, vocabulary, 52)","9e5433e2":"predictions = model.predict(x=np.asarray(test_X))\n\nprediction_results = pd.concat([test,\n                                pd.DataFrame([np.argmax(k) for k in predictions], columns=['Sentiment'])],\n                               axis=1)","0f9da331":"submission = prediction_results[['PhraseId', 'Sentiment']]\nsubmission.to_csv('submission.csv', index=False)","fbba930e":"Predicting the model on the test dataset and converting results into the desired format","d6d176de":"Calling the functions defined above","c8e86175":"Preprocessing the test data","a0e50194":"Importing the dataset. The dataset can be downloaded at https:\/\/www.kaggle.com\/c\/movie-review-sentiment-analysis-kernels-only\/data","950c5886":"Submitting results","e6df8d92":"The **function preprocess_df(df, vocabulary, max_sentence_length)** defined below is used to pre process the dataframe before sending into the model to be defined later on. The function converts the sentiment labels into inteher values. Also it converts the phrases into lemmatized tokens of their words represented by their respective indices in the vocabulary created earlier.","608b6119":"# **SENTIMENT CLASSIFIER**\n\nSentiment Classification is a very important part of Natural Language Processing. This notebook uses a CNN + Bi-LSTM model on a dataset with phrases extracted from the rotten tomatoes dataset and classifies them into 5 different sentiments.","ce7b1a28":"Fitting the model","5c078a4e":"Train-Test Split","bf091bed":"Defining the **model**. It uses 1D CNN layers followed by few bi-LSTM layers and ending with dense layers.","4aac382f":"Plotting the loss and accuracy with respect to epochs","4869c473":"The **create_vocabulary(df)** function defined below creates an indexed vocabulary from the lemmatized tokens of words present in the dataframe passed to it.","ba45f519":"Importing the essential libraries"}}