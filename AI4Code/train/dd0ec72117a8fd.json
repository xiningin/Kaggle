{"cell_type":{"a1e449a1":"code","ba9c06d9":"code","cdf7f6e6":"code","e9587c6c":"code","531b5e72":"code","f5d3e456":"code","63809371":"code","9099ea02":"code","8c044a35":"code","a6fc0113":"code","727a1939":"code","6f2ea2e1":"code","255255a4":"code","33a5be43":"code","0ef1ba1f":"code","55faa87a":"code","0a2420ae":"code","17066f6e":"code","5a280be9":"code","e13c82c6":"code","b9c1479b":"code","fd7897da":"code","e1e5c847":"code","b037471b":"code","5a699e72":"code","365e5487":"code","7394a128":"code","895d53a4":"markdown","2b423390":"markdown","6209132f":"markdown","1d6cc4a7":"markdown","ae4b252c":"markdown","20d58647":"markdown","48a7e40d":"markdown","6d9df5a5":"markdown","c2a9a035":"markdown"},"source":{"a1e449a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ba9c06d9":"# basic modules\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport cv2\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nimport imgaug\nimport random\nfrom imgaug import augmenters as iaa\nfrom imgaug import parameters as iap\nfrom PIL import Image\n\n#keras modules\nfrom keras.preprocessing.image import img_to_array, load_img\n\n#Pytorch modules\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torchvision.utils import save_image\nfrom torch.nn.utils import spectral_norm\n\n#Crop images using bounding box\nimport xml.etree.ElementTree as ET\nimport glob","cdf7f6e6":"img_list =os.listdir(\"..\/input\/all-dogs\/all-dogs\/\")","e9587c6c":"len(img_list)","531b5e72":"root_images=\"..\/input\/all-dogs\/all-dogs\/\"\nroot_annots=\"..\/input\/annotation\/Annotation\/\"\ncroped_images=\"..\/croped_images\/\"","f5d3e456":"all_images=os.listdir(\"..\/input\/all-dogs\/all-dogs\/\")\nprint(f\"Total images : {len(all_images)}\")\n\nbreeds = glob.glob('..\/input\/annotation\/Annotation\/*')\nannotation=[]\nfor b in breeds:\n    annotation+=glob.glob(b+\"\/*\")\nprint(f\"Total annotation : {len(annotation)}\")\n\nbreed_map={}\nfor annot in annotation:\n    breed=annot.split(\"\/\")[-2]\n    index=breed.split(\"-\")[0]\n    breed_map.setdefault(index,breed)\n    \nprint(f\"Total Breeds : {len(breed_map)}\")","63809371":"def bounding_box(image):\n    bpath=root_annots+str(breed_map[image.split(\"_\")[0]])+\"\/\"+str(image.split(\".\")[0])\n    tree = ET.parse(bpath)\n    root = tree.getroot()\n    objects = root.findall('object')\n    for o in objects:\n        bndbox = o.find('bndbox') # reading bound box\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n        \n    return (xmin,ymin,xmax,ymax)","9099ea02":"if not os.path.exists(croped_images):\n    os.mkdir(croped_images)","8c044a35":"for i,image in enumerate(all_images):\n    #print(image)\n    bbox=bounding_box(image)\n    im=Image.open(os.path.join(root_images,image))\n    im=im.crop(bbox)\n    im.save(croped_images + image, quality=95)","a6fc0113":"len(os.listdir(\"..\/croped_images\/\"))","727a1939":"temp_img = load_img('..\/croped_images\/n02085620_3423.jpg')\ntemp_img_array  = img_to_array(temp_img)","6f2ea2e1":"temp_img","255255a4":"temp_img_array.shape","33a5be43":"sns.set_style(\"white\")\ncount = 1\nplt.figure(figsize=[20, 20])\nfor img_name in img_list[:15]:\n    #print(\"..\/input\/all-dogs\/all-dogs\/%s.jpg\" % img_name)\n    img = cv2.imread(\"..\/croped_images\/%s\" % img_name)[...,[2, 1, 0]]\n    plt.subplot(5, 5, count)\n    plt.imshow(img)\n    count += 1\n    \nplt.show()","0ef1ba1f":"manualSeed = random.randint(1000, 10000)\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)","55faa87a":"if not os.path.exists('..\/result_images\/'):\n    os.mkdir('..\/result_images\/')","0a2420ae":"# Root directory for dataset\ndataroot = \"..\/\"\nimage_size = 64\nnc = 3\nnz = 128\nngf = 64\nndf = 64\nnum_epochs = 200\nlr = 0.0001\nbeta1 = 0.5\n# Number of GPUs available. Use 0 for CPU mode.\nngpu = 1\n\n# Initial_setting\nworkers = 2\nbatch_size=64  \nnz = 100\nnch_g = 64\nnch_d = 64\nn_epoch = 200\nlr = 0.0002\nbeta1 = 0.5\noutf = '..\/result_images'\ndisplay_interval = 100\nsave_fake_image_interval = 1500\nplt.rcParams['figure.figsize'] = 10, 6","17066f6e":"# Dataset Creator\nrand_aff = random.uniform(3.0, 15.0)\nrand_flip = random.uniform(0.3, 1.0)\nrand_trans = random.uniform(0.3, 0.7)\nrand_contr = random.uniform(0.2, 0.9)\nrandom_transforms = [transforms.ColorJitter(contrast=rand_contr), transforms.RandomAffine(degrees=rand_aff)]\n\ndataset = dset.ImageFolder(root=dataroot,\n                          transform=transforms.Compose([\n                          transforms.RandomResizedCrop(64, scale=(0.9, 1.0), ratio=(1., 1.)),\n                          transforms.RandomHorizontalFlip(),\n                          transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),\n                          transforms.ToTensor(),\n                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                      ]))  \n\n# Create the dataloader\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers)\n\n# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")","5a280be9":"real_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))","e13c82c6":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:            \n        m.weight.data.normal_(0.0, 0.02)\n        m.bias.data.fill_(0)\n    elif classname.find('Linear') != -1:        \n        m.weight.data.normal_(0.0, 0.02)\n        m.bias.data.fill_(0)\n    elif classname.find('BatchNorm') != -1:    \n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)","b9c1479b":"class Generator(nn.Module):\n    def __init__(self, nz=100, nch_g=64, nch=3):\n        super(Generator, self).__init__()\n        self.layers = nn.ModuleDict({\n            'layer0': nn.Sequential(\n                spectral_norm(nn.ConvTranspose2d(nz, nch_g * 8, 4, 1, 0)),     \n                nn.BatchNorm2d(nch_g * 8),                      \n                nn.ReLU()                                       \n            ),  # (100, 1, 1) -> (512, 4, 4)\n            'layer1': nn.Sequential(\n                spectral_norm(nn.ConvTranspose2d(nch_g * 8, nch_g * 4, 4, 2, 1)),\n                nn.BatchNorm2d(nch_g * 4),\n                nn.ReLU()\n            ),  # (512, 4, 4) -> (256, 8, 8)\n            'layer2': nn.Sequential(\n                spectral_norm(nn.ConvTranspose2d(nch_g * 4, nch_g * 2, 4, 2, 1)),\n                nn.BatchNorm2d(nch_g * 2),\n                nn.ReLU()\n            ),  # (256, 8, 8) -> (128, 16, 16)\n\n            'layer3': nn.Sequential(\n                spectral_norm(nn.ConvTranspose2d(nch_g * 2, nch_g, 4, 2, 1)),\n                nn.BatchNorm2d(nch_g),\n                nn.ReLU()\n            ),  # (128, 16, 16) -> (64, 32, 32)\n            'layer4': nn.Sequential(\n                spectral_norm(nn.ConvTranspose2d(nch_g, nch, 4, 2, 1)),\n                nn.Tanh()\n            )   # (64, 32, 32) -> (3, 64, 64)\n        })\n\n    def forward(self, z):\n        for layer in self.layers.values():  \n            z = layer(z)\n        return z","fd7897da":"class Discriminator(nn.Module):\n    def __init__(self, nch=3, nch_d=64):\n        super(Discriminator, self).__init__()\n        self.layers = nn.ModuleDict({\n            'layer0': nn.Sequential(\n                spectral_norm(nn.Conv2d(nch, nch_d, 4, 2, 1)),     \n                nn.LeakyReLU(negative_slope=0.2)    \n            ),  # (3, 64, 64) -> (64, 32, 32)\n            'layer1': nn.Sequential(\n                spectral_norm(nn.Conv2d(nch_d, nch_d * 2, 4, 2, 1)),\n                nn.BatchNorm2d(nch_d * 2),\n                nn.LeakyReLU(negative_slope=0.2)\n            ),  # (64, 32, 32) -> (128, 16, 16)\n            'layer2': nn.Sequential(\n                spectral_norm(nn.Conv2d(nch_d * 2, nch_d * 4, 4, 2, 1)),\n                nn.BatchNorm2d(nch_d * 4),\n                nn.LeakyReLU(negative_slope=0.2)\n            ),  # (128, 16, 16) -> (256, 8, 8)\n            'layer3': nn.Sequential(\n                spectral_norm(nn.Conv2d(nch_d * 4, nch_d * 8, 4, 2, 1)),\n                nn.BatchNorm2d(nch_d * 8),\n                nn.LeakyReLU(negative_slope=0.2)\n            ),  # (256, 8, 8) -> (512, 4, 4)\n            'layer4': spectral_norm(nn.Conv2d(nch_d * 8, 1, 4, 1, 0))\n            # (512, 4, 4) -> (1, 1, 1)\n        })\n\n    def forward(self, x):\n        for layer in self.layers.values():  \n            x = layer(x)\n        return x.squeeze()     \n","e1e5c847":"print('device:', device)\n\nnetG = Generator(nz=nz, nch_g=nch_g).to(device)\nnetG.apply(weights_init)    \nprint(netG)\n\nnetD = Discriminator(nch_d=nch_d).to(device)\nnetD.apply(weights_init)\nprint(netD)","b037471b":"criterion = nn.MSELoss()\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5)  \noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5)  \n\nfixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)  # save_fake_image\u7528\u30ce\u30a4\u30ba\uff08\u56fa\u5b9a\uff09\nLoss_D_list, Loss_G_list = [], []\n\nsave_fake_image_count = 1","5a699e72":"#####    trainig_loop\nfor epoch in tqdm(range(n_epoch)):\n    for itr, data in enumerate(dataloader):\n        real_image = data[0].to(device)   # \u672c\u7269\u753b\u50cf\n        sample_size = real_image.size(0)  # \u753b\u50cf\u679a\u6570\n        noise = torch.randn(sample_size, nz, 1, 1, device=device)   # \u5165\u529b\u30d9\u30af\u30c8\u30eb\u751f\u6210\uff08\u6b63\u898f\u5206\u5e03\u30ce\u30a4\u30ba\uff09       \n        real_target = torch.full((sample_size,), 1., device=device)   # \u76ee\u6a19\u5024\uff08\u672c\u7269\uff09\n        fake_target = torch.full((sample_size,), 0., device=device)   # \u76ee\u6a19\u5024\uff08\u507d\u7269\uff09\n\n        #--------  Update Discriminator  ---------\n        netD.zero_grad()    # \u52fe\u914d\u306e\u521d\u671f\u5316\n\n        output = netD(real_image)   # Discriminator\u304c\u884c\u3063\u305f\u3001\u672c\u7269\u753b\u50cf\u306e\u5224\u5b9a\u7d50\u679c\n        errD_real = criterion(output, real_target)  # \u672c\u7269\u753b\u50cf\u306e\u5224\u5b9a\u7d50\u679c\u3068\u76ee\u6a19\u5024\uff08\u672c\u7269\uff09\u306e\u4e8c\u4e57\u8aa4\u5dee\n        D_x = output.mean().item()  # output\u306e\u5e73\u5747 D_x \u3092\u8a08\u7b97\uff08\u5f8c\u3067\u30ed\u30b0\u51fa\u529b\u306b\u4f7f\u7528\uff09\n\n        fake_image = netG(noise)    # Generator\u304c\u751f\u6210\u3057\u305f\u507d\u7269\u753b\u50cf\n\n        output = netD(fake_image.detach())  # Discriminator\u304c\u884c\u3063\u305f\u3001\u507d\u7269\u753b\u50cf\u306e\u5224\u5b9a\u7d50\u679c\n        errD_fake = criterion(output, fake_target)  # \u507d\u7269\u753b\u50cf\u306e\u5224\u5b9a\u7d50\u679c\u3068\u76ee\u6a19\u5024\uff08\u507d\u7269\uff09\u306e\u4e8c\u4e57\u8aa4\u5dee\n        D_G_z1 = output.mean().item()  # output\u306e\u5e73\u5747 D_G_z1 \u3092\u8a08\u7b97\uff08\u5f8c\u3067\u30ed\u30b0\u51fa\u529b\u306b\u4f7f\u7528\uff09\n\n        errD = errD_real + errD_fake    # Discriminator \u5168\u4f53\u306e\u640d\u5931\n        errD.backward()    # \u8aa4\u5dee\u9006\u4f1d\u64ad\n        optimizerD.step()   # Discriminatoe\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u66f4\u65b0\n\n        #---------  Update Generator   ----------\n        netG.zero_grad()    # \u52fe\u914d\u306e\u521d\u671f\u5316        \n        output = netD(fake_image)   # \u66f4\u65b0\u3057\u305f Discriminator\u3067\u3001\u507d\u7269\u753b\u50cf\u3092\u5224\u5b9a\n        errG = criterion(output, real_target)   # \u507d\u7269\u753b\u50cf\u306e\u5224\u5b9a\u7d50\u679c\u3068\u76ee\u6a19\u5024\uff08\u672c\u7269\uff09\u306e\u4e8c\u4e57\u8aa4\u5dee\n        errG.backward()     # \u8aa4\u5dee\u9006\u4f1d\u64ad\n        D_G_z2 = output.mean().item()  # output\u306e\u5e73\u5747 D_G_z2 \u3092\u8a08\u7b97\uff08\u5f8c\u3067\u30ed\u30b0\u51fa\u529b\u306b\u4f7f\u7528\uff09\n\n        optimizerG.step()   # Generator\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\n\n        if itr % display_interval == 0:\n            print('[{}\/{}][{}\/{}] Loss_D: {:.3f} Loss_G: {:.3f} D(x): {:.3f} D(G(z)): {:.3f}\/{:.3f}'\n                  .format(epoch + 1, n_epoch,itr + 1, len(dataloader),errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n            Loss_D_list.append(errD.item())\n            Loss_G_list.append(errG.item())\n\n        if epoch == 0 and itr == 0:     \n            vutils.save_image(real_image, '{}\/real_samples.png'.format(outf),normalize=True, nrow=8)\n\n        if itr % save_fake_image_interval == 0 and itr > 0:\n            fake_image = netG(fixed_noise)\n            vutils.save_image(fake_image.detach(), '{}\/fake_samples_{:03d}.png'.format(outf, save_fake_image_count),normalize=True, nrow=8)\n            save_fake_image_count +=1\n\n    # ---------  save fake image  ----------\n    fake_image = netG(fixed_noise)  \n    vutils.save_image(fake_image.detach(), '{}\/fake_samples_epoch_{:03d}.png'.format(outf, epoch + 1),\n                      normalize=True, nrow=8)\n\n    # ---------  save model  -----------\n    if (epoch + 1) % 10 == 0:   # 10\u30a8\u30dd\u30c3\u30af\u3054\u3068\u306b\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\u3059\u308b\n        torch.save(netG.state_dict(), '{}\/netG_epoch_{}.pth'.format(outf, epoch + 1))\n        torch.save(netD.state_dict(), '{}\/netD_epoch_{}.pth'.format(outf, epoch + 1))\n\n# plot graph\nplt.figure()    \nplt.plot(range(len(Loss_D_list)), Loss_D_list, color='blue', linestyle='-', label='Loss_D')\nplt.plot(range(len(Loss_G_list)), Loss_G_list, color='red', linestyle='-', label='Loss_G')\nplt.legend()\nplt.xlabel('iter (*100)')\nplt.ylabel('loss')\nplt.title('Loss_D and Loss_G')\nplt.grid()\nplt.savefig('Loss_graph.png') ","365e5487":"if not os.path.exists('..\/output_images'):\n    os.mkdir('..\/output_images')\n    \nim_batch_size = 50\nn_images=10000\n\nfor i_batch in tqdm(range(0, n_images, im_batch_size)):\n    gen_z = torch.randn(im_batch_size, nz, 1, 1, device=device)\n    gen_images = netG(gen_z)\n    images = gen_images.to(\"cpu\").clone().detach()\n    images = images.numpy().transpose(0, 2, 3, 1)\n    for i_image in range(gen_images.size(0)):\n        save_image(gen_images[i_image, :, :, :], os.path.join('..\/output_images', f'image_{i_batch+i_image:05d}.png'))","7394a128":"import shutil\nshutil.make_archive('images', 'zip', '..\/output_images')","895d53a4":"## show sample images","2b423390":"## import module","6209132f":"## make Generator","1d6cc4a7":"## loading datasets","ae4b252c":"## Beginer LSGAN LESSON using Pytorch","20d58647":"## save images","48a7e40d":"## sample 15 train images","6d9df5a5":"## make Discriminator","c2a9a035":"## check train images using torch utils"}}