{"cell_type":{"d66ec1bd":"code","2f8444c3":"code","dfab32e5":"code","ee709dd2":"code","1c504956":"code","c98120de":"code","aabd8fdc":"code","217c1c52":"code","522ae4b5":"code","158a310a":"code","704df023":"code","606883b7":"code","7a56ce21":"code","9b6f6139":"code","f498cf56":"code","106f7258":"code","61e0a8ff":"code","e7f5ddd2":"code","2ca25030":"code","5048d624":"code","469855f2":"code","525abe17":"markdown","225b957d":"markdown","b256fcae":"markdown","6877397d":"markdown","535d1b63":"markdown","b6ba5f54":"markdown","a454636c":"markdown","881727dc":"markdown","9f7ece47":"markdown","4a848ba7":"markdown","be17882b":"markdown","87215fb9":"markdown","395ce9d9":"markdown","15981a91":"markdown","1eacc4b5":"markdown","cf067b26":"markdown","cb1ef5fd":"markdown","336e1ce2":"markdown","b8f0f632":"markdown","3ecf3395":"markdown","773860ba":"markdown","a9533d4f":"markdown","2083aaff":"markdown","79d3071a":"markdown","deef9340":"markdown","9684c015":"markdown","9b4a25d5":"markdown","8b730295":"markdown"},"source":{"d66ec1bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \n%matplotlib inline\nfrom scipy.stats import zscore\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f8444c3":"df = pd.read_csv('..\/input\/customer-spend-dataset\/Cust_Spend_Data(1).csv')","dfab32e5":"df.info()","ee709dd2":"df.head()","1c504956":"df_Attr=df.iloc[:,2:]\ndf_Attr.head()","c98120de":"df_Attr_Scaled=df_Attr.apply(zscore)\ndf_Attr_Scaled.head(10)","aabd8fdc":"#importing seaborn for statistical plots\nsns.pairplot(df_Attr_Scaled, height=2,aspect=2 , diag_kind='kde')","217c1c52":"from sklearn.cluster import AgglomerativeClustering \n\nmodel = AgglomerativeClustering(n_clusters=3, affinity='euclidean',  linkage='average')\nmodel.fit(df_Attr_Scaled)","522ae4b5":"df_Attr['labels'] = model.labels_\ndf_Attr.head()","158a310a":"df_Attr[df_Attr['labels'] == 0]","704df023":"df_Attr[df_Attr['labels'] == 1]","606883b7":"df_Attr[df_Attr['labels'] == 2]","7a56ce21":"df_Clust = df_Attr.groupby(['labels'])\ndf_Clust.mean()","9b6f6139":"from scipy.cluster.hierarchy import cophenet, dendrogram, linkage\nfrom scipy.spatial.distance import pdist  #Pairwise distribution between data points","f498cf56":"# cophenet index is a measure of the correlation between the distance of points in feature space and distance on dendrogram\n# closer it is to 1, the better is the clustering\n\nZ = linkage(df_Attr_Scaled, metric='euclidean', method='average')","106f7258":"c, coph_dists = cophenet(Z , pdist(df_Attr_Scaled))\nc ","61e0a8ff":"plt.figure(figsize=(10, 5))\nplt.title('Agglomerative Hierarchical Clustering Dendogram')\nplt.xlabel('sample index')\nplt.ylabel('Distance')\ndendrogram(Z, leaf_rotation=30,color_threshold = 40, leaf_font_size= 10)\nplt.tight_layout()","e7f5ddd2":"# cophenet index is a measure of the correlation between the distance of points in feature space and distance on dendrogram\n# closer it is to 1, the better is the clustering\n\nZ = linkage(df_Attr_Scaled, metric='euclidean', method='complete')\nc, coph_dists = cophenet(Z , pdist(df_Attr_Scaled))\n\nc","2ca25030":"plt.figure(figsize=(10, 5))\nplt.title('Agglomerative Hierarchical Clustering Dendogram')\nplt.xlabel('sample index')\nplt.ylabel('Distance')\ndendrogram(Z, leaf_rotation=90.,color_threshold=90,  leaf_font_size=10. )\nplt.tight_layout()","5048d624":"# cophenet index is a measure of the correlation between the distance of points in feature space and distance on dendrogram\n# closer it is to 1, the better is the clustering\n\nZ = linkage(df_Attr_Scaled, metric='euclidean', method='ward')\nc, coph_dists = cophenet(Z , pdist(df_Attr_Scaled))\n\nc","469855f2":"plt.figure(figsize=(10, 5))\nplt.title('Agglomerative Hierarchical Clustering Dendogram')\nplt.xlabel('sample index')\nplt.ylabel('Distance')\ndendrogram(Z, leaf_rotation=90.,color_threshold=600,  leaf_font_size=10. )\nplt.tight_layout()","525abe17":"* The [cophenetic correlation](https:\/\/en.wikipedia.org\/wiki\/Cophenetic_correlation#:~:text=In%20statistics%2C%20and%20especially%20in,the%20original%20unmodeled%20data%20points.) coefficient is a measure of how faithfully a dendrogram preserves the pairwise distances between the original unmodeled data points.","225b957d":"Now we are importing [Aggolomerative Clustering](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.AgglomerativeClustering.html) algorithm based on Hierarchical Clustering to perform the job.We can use divisive clustering also to perform heirarchical clustering.It is avaliable from the sklearn.cluster library of python. \n\n* Now creating a model of Aggolomerative Clustering and using it.\n* We are using **3 clusters**. We can determine the value by using any of the method like Elbow, Silhouette\n* affinity metric used to compute the linkage. Here we use **Euclidean distance** measure.\n* Additionally we are keeping the linkage among the cluster at average. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. **\u2018average\u2019** uses the **average of the distances** of each observation of the two sets. ","b256fcae":"Let's have more analysis of the Clustering by inspecting the mean of each group for all the attributes of customer spend to idetify the generalized values of each category of customers","6877397d":"Let's check the data distribution of the different columns with KDE using pairplot","535d1b63":"# **2. EDA and Data Preprocessing**","b6ba5f54":"**Distance Measure method : 'average'**","a454636c":"Getting information of the dataset like columns and their datatypes, record count etc.","881727dc":"Let's look at the data in the dataset.","9f7ece47":"* As we can see, there isn't any imporvement in the cophenetic correlation coefficient after changing the **pairwise distance** measurement method from **'complete'** to **'ward'**. ","4a848ba7":"**Distance Measure method : 'ward'**","be17882b":"Here we'll load the dataset in the pandas dataframe using the *read_csv()* function","87215fb9":"We are appling the scaling to the provided data to increase the homoginity while applying the clustering algorithm. We are using [Z-score](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.zscore.html) to perform normalization.","395ce9d9":"As we can see that some of the attribures are not useful in the analysis, so we are removing them from the selection and creating a new dataFrame of attributes containing useful attributes.","15981a91":"# **1. Loading the Dataset**","1eacc4b5":"# **Conclusion**","cf067b26":"* Here cophenetic correlation index is **0.868** which is nearer to 1. This is the indication that the dendrogram will be representing the more accurately presearved pairwise distance data of original data points.","cb1ef5fd":"# Hierarchical Clustering : Insight (Dataset: Customer Spend)","336e1ce2":"* Now let's change the type of linkage between the sets of points in the clusters. Previously we took **'average'**, now we try with **'complete'** and plot the dendrogram again. Other options for** linkage parameter method** are [*'single','complete','weighted','centroid','median' and 'ward'*](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/reference\/generated\/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage). We'll see some of them here. \n* Even we can tryout other **linkage parameter distance metric** like [*\u2018braycurtis\u2019, \u2018canberra\u2019, \u2018chebyshev\u2019, \u2018cityblock\u2019, \u2018correlation\u2019, \u2018cosine\u2019, \u2018dice\u2019, \u2018euclidean\u2019, \u2018hamming\u2019, \u2018jaccard\u2019, \u2018jensenshannon\u2019, \u2018kulsinski\u2019, \u2018mahalanobis\u2019, \u2018matching\u2019, \u2018minkowski\u2019, \u2018rogerstanimoto\u2019, \u2018russellrao\u2019, \u2018seuclidean\u2019, \u2018sokalmichener\u2019, \u2018sokalsneath\u2019, \u2018sqeuclidean\u2019, \u2018wminkowski\u2019, \u2018yule\u2019*](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.spatial.distance.pdist.html?highlight=pdist#scipy.spatial.distance.pdist).","b8f0f632":"**Distance Measure method : 'complete'**","3ecf3395":"# **3. Apply Clustering**","773860ba":"* As we can see, there isn't much change in the cophenetic correlation coefficient after changing the **pairwise distance** measurement method from **'average'** to **'complete'**. ","a9533d4f":"* Now to finalize the work of Hiearchical Clustering we carried out, we will show the results via best way of representation for the Clustering. We are going to plot the [dendrogram](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/reference\/generated\/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram) for the knowledge representation.","2083aaff":"Now let's check the labels of each category and the records falling in it. by checking labels one by one.","79d3071a":"So here we have generated the clusters from the provided data using Agglomerative clustering method. This is an example of Hierarchical clustering. You can explore other method of clustering also to perform the task. Keep posted here with your notebooks.\n\nYour comments are appreciated for other contributions..! Thanks","deef9340":"From the plot above we can conclude that,\n> **there are no correlation between given attributes, so no Multi-colinearity is present in data**\n\nSo we can proceed further to apply clustering algorithm","9684c015":"This synthesized dataset contains customer's spending data. To analyze the customer's spending we are going to use the Hierarchical Clustering algorithm. Given dataset contains following columns:\n\n> * Cust_ID : Shows the customerID assigned by the system\n> * Name : Name of the customer\n> * Avg_Mthly_Spend : Customer's Average monthly spending\n> * No_Of_Visits : Count of the customer's visit to store\n> * Apparel_Items : Count of Apparel item\n> * FnV_Items : Count of Food and Vegetables items\n> * Staples_Items : Count of Staples items ","9b4a25d5":"Now appending the identified category of each record to the original data by introducing a new column called **'label'**","8b730295":"# **4. Building the Dendrogram**"}}