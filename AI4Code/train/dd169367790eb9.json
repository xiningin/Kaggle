{"cell_type":{"e79ee8e6":"code","7d1a1b12":"code","9c92e60f":"code","8051de87":"code","9631074d":"code","389873e3":"code","78bcf862":"code","cc8b441f":"code","7e9fa3bd":"code","497a2309":"code","654ece2e":"code","bda59bcf":"code","393896c4":"code","9e6499ed":"code","2fe61f30":"code","ba1c03ed":"code","894781ea":"code","35087c59":"code","f7aebae6":"code","775a987b":"code","4fbeca8a":"code","b6cb9f87":"code","58411fc3":"code","4cee46a5":"code","e8250d5a":"code","9526b267":"code","f0151e92":"code","43ab740c":"code","bff85e59":"code","4803f821":"code","4c2b051a":"code","ae53ef53":"code","dcbdad7c":"code","80ac106c":"code","721ef9af":"code","431fda7e":"code","2570db88":"code","234d45f9":"code","f11ef985":"code","2bb01d2d":"code","b10a1931":"code","22a9aa7a":"code","204c9fb0":"code","92427822":"code","05ae1729":"code","37acd0c0":"code","22d58a7d":"code","5954d7be":"code","a8a82783":"markdown","6da86390":"markdown","29eee604":"markdown","28b49e36":"markdown","c01e938d":"markdown","5fcc181a":"markdown","f0c07634":"markdown","cae06c43":"markdown","48266b02":"markdown","b123748a":"markdown","863f31d4":"markdown"},"source":{"e79ee8e6":"import pandas as pd # \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.datasets as sk\nimport plotly.express as px\nimport plotly.subplots as pxs\nimport skimage.io as ski\nimport plotly.graph_objects as pxg\nimport librosa as lr\nimport librosa.display as lrd\n\nplt.rcParams['figure.figsize'] = [9, 9] # \u0434\u0435\u043b\u0430\u0435\u043c \u0432\u0441\u0435 \u0443\u0447\u0430\u0441\u0442\u043a\u0438 \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0431\u043e\u043b\u044c\u0448\u0435","7d1a1b12":"#iris data set\niris_data = pd.DataFrame(data=sk.load_iris().data, columns=sk.load_iris().feature_names)","9c92e60f":"# iris dataset shape\niris_data.shape","8051de87":"# \u0440\u0430\u0441\u043f\u0435\u0447\u0430\u0442\u0430\u0442\u044c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 iris dataset\niris_data.head(9)","9631074d":"# iris \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u044f\niris_data.corr().style.background_gradient(cmap=\"coolwarm\")","389873e3":"# \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u0430\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043e iris\niris_data.describe()","78bcf862":"# plotting iris\npx.bar(iris_data)","cc8b441f":"# fashion mnist\nfashion_mnist_data = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv')","7e9fa3bd":"# fashion mnist dataset shape\nfashion_mnist_data.shape","497a2309":"# \u043f\u0435\u0440\u0432\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 - \u044d\u0442\u0438\u043a\u0435\u0442\u043a\u0430 \u043e\u0434\u0435\u0436\u0434\u044b\nclothing_labels = {}\nfor el in set(fashion_mnist_data.label):\n    clothing_labels[el] = list(fashion_mnist_data.label).count(el)\nset(clothing_labels), clothing_labels","654ece2e":"# \u043d\u0430\u043d\u043e\u0441\u0438\u043c \u043d\u0430 \u043a\u0430\u0440\u0442\u0443 \u0432\u0441\u0435 \u044d\u0442\u0438\u043a\u0435\u0442\u043a\u0438 \u043d\u0430 \u043e\u0434\u0435\u0436\u0434\u0435\nfig = px.pie(values=clothing_labels.values(), names=clothing_labels.keys())\nfig.show()","bda59bcf":"# plot some clothes\npx.imshow(fashion_mnist_data.iloc[:,1:].iloc[0].values.reshape(28,28), color_continuous_scale='gray')","393896c4":"# plot some clothes\npx.imshow(fashion_mnist_data.iloc[:,1:].iloc[3].values.reshape(28,28), color_continuous_scale='gray')","9e6499ed":"# plot some clothes\npx.imshow(fashion_mnist_data.iloc[:,1:].iloc[6].values.reshape(28,28), color_continuous_scale='gray')","2fe61f30":"# plot some clothes\npx.imshow(fashion_mnist_data.iloc[:,1:].iloc[9].values.reshape(28,28), color_continuous_scale='gray')","ba1c03ed":"# \u0447\u0438\u0442\u0430\u0435\u043c chinese mnist dataset\nchinese_mnist_data = pd.read_csv(\"\/kaggle\/input\/chinese-mnist\/chinese_mnist.csv\")","894781ea":"# shape of the dataset\nchinese_mnist_data.shape","35087c59":"# \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u0438\u0442\u0430\u0439\u0441\u043a\u0438\u0435 \u0438\u0435\u0440\u043e\u0433\u043b\u0438\u0444\u044b\nlen(chinese_mnist_data.code.unique())","f7aebae6":"# \u0441\u0442\u043e\u043b\u0431\u0446\u044b \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\nchinese_mnist_data.keys()","775a987b":"# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0438\ndef return_image(el):\n    return f\"input_{el[0]}_{el[1]}_{el[2]}.jpg\"","4fbeca8a":"# \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \"image\", \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0439 \u0432\u0441\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\nchinese_mnist_data['image'] = chinese_mnist_data.apply(return_image, axis=1)","b6cb9f87":"# plot some chinese \npx.imshow(ski.imread(f\"\/kaggle\/input\/chinese-mnist\/data\/data\/{chinese_mnist_data.image[0]}\"), color_continuous_scale='gray')","58411fc3":"# plot some chinese \npx.imshow(ski.imread(f\"\/kaggle\/input\/chinese-mnist\/data\/data\/{chinese_mnist_data.image[3333]}\"), color_continuous_scale='gray')","4cee46a5":"# plot some chinese \npx.imshow(ski.imread(f\"\/kaggle\/input\/chinese-mnist\/data\/data\/{chinese_mnist_data.image[6666]}\"), color_continuous_scale='gray')","e8250d5a":"# \u0447\u0438\u0442\u0430\u0435\u043c \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 mnist\nsign_mnist = pd.read_csv(\"\/kaggle\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv\")","9526b267":"# sign mnist shape\nsign_mnist.shape","f0151e92":"# \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0438\u043c\u0432\u043e\u043b\u044b \u043d\u0430 \u044f\u0437\u044b\u043a\u0435 \u0436\u0435\u0441\u0442\u043e\u0432\nsign_mnist.label.unique()","43ab740c":"# \u0431\u0440\u0438\u0442\u0430\u043d\u0441\u043a\u0438\u0435 \u0431\u0443\u043a\u0432\u044b \u0438 \u0438\u0445 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\nsign_lab_unique = {}\nfor el in set(sign_mnist.label):\n    sign_lab_unique[el] = list(sign_mnist.label).count(el)\nsign_lab_unique","bff85e59":"# \u0431\u0440\u0438\u0442\u0430\u043d\u0441\u043a\u0438\u0435 \u0431\u0443\u043a\u0432\u044b \u0438 \u0438\u0445 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\npx.pie(values=sign_lab_unique.values(), names=sign_lab_unique.keys())","4803f821":"# plot some sign language\npx.imshow(sign_mnist.iloc[:,1:].iloc[0].values.reshape(28,28), color_continuous_scale='gray')","4c2b051a":"# plot some sign language\npx.imshow(sign_mnist.iloc[:,1:].iloc[3].values.reshape(28,28), color_continuous_scale='gray')","ae53ef53":"# plot some sign language\npx.imshow(sign_mnist.iloc[:,1:].iloc[9].values.reshape(28,28), color_continuous_scale='gray')","dcbdad7c":"# \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u0442\u044c \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \"sound\"\nsound_set = pd.read_csv(\"\/kaggle\/input\/urbansound8k\/UrbanSound8K.csv\")","80ac106c":"# some sound data\nsound_set.head(9)","721ef9af":"# \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0441\u044b \u0432 \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\nsound_unique = {}\nfor el in set(sound_set.classID):\n    sound_unique[el] = list(sound_set.classID).count(el)\nsound_unique","431fda7e":"# \u043f\u0435\u0440\u0435\u0434\u043d\u0438\u0439 \u043f\u043b\u0430\u043d \/ \u0444\u043e\u043d\u043e\u0432\u044b\u0435 \u0437\u0432\u0443\u043a\u0438 (2 - \u0437\u0430\u0434\u043d\u0438\u0439, 1 - \u043f\u0435\u0440\u0435\u0434\u043d\u0438\u0439)\nsound_for_back = {}\nfor el in set(sound_set.salience):\n    sound_for_back[el] = list(sound_set.salience).count(el)\nsound_for_back","2570db88":"# plot unique classes\npx.pie(values=sound_unique.values(), names=sound_unique.keys())","234d45f9":"# \u043f\u0435\u0440\u0435\u0434\u043d\u0438\u0439 \u043f\u043b\u0430\u043d \/ \u0444\u043e\u043d\u043e\u0432\u044b\u0435 \u0437\u0432\u0443\u043a\u0438 (2 - \u0437\u0430\u0434\u043d\u0438\u0439, 1 - \u043f\u0435\u0440\u0435\u0434\u043d\u0438\u0439)\npx.pie(values=sound_for_back.values(), names=sound_for_back.keys())","f11ef985":"# \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043f\u0443\u0442\u0438 \u043a\u043e \u0432\u0441\u0435\u043c \u0444\u0430\u0439\u043b\u0430\u043c\nsound_set['path'] = sound_set.apply(lambda el: f\"\/kaggle\/input\/urbansound8k\/fold{el.fold}\/{el.slice_file_name}\", axis=1)","2bb01d2d":"# \u0437\u0432\u0443\u043a \u0441\u043e\u0431\u0430\u043a\u0438\ndog_bark, dog_bark_freq = lr.load(f\"{sound_set.iat[0,8]}\")","b10a1931":"# \u0437\u0432\u0443\u043a \u0440\u0435\u0431\u0435\u043d\u043a\u0430\nchld_pl, chld_pl_freq = lr.load(f\"{sound_set.iat[1,8]}\")","22a9aa7a":"# \u043a\u043e\u043d\u0434\u0438\u0446\u0438\u043e\u043d\u0435\u0440\nair_cond, air_cond_freq = lr.load(f\"{sound_set.iat[4102,8]}\")","204c9fb0":"# plot \u043b\u0430\u0439 \u0441\u043e\u0431\u0430\u043a\u0438\nlrd.waveplot(dog_bark, sr=dog_bark_freq)","92427822":"# plot \u0437\u0432\u0443\u043a \u0440\u0435\u0431\u0435\u043d\u043a\u0430\nlrd.waveplot(chld_pl, sr=chld_pl_freq)","05ae1729":"# plot \u043a\u043e\u043d\u0434\u0438\u0446\u0438\u043e\u043d\u0435\u0440\nlrd.waveplot(air_cond, sr=air_cond_freq)","37acd0c0":"# \u0441\u043f\u0435\u043a\u0442\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u043b\u0430\u044f \u0441\u043e\u0431\u0430\u043a\u0438\nlrd.specshow(lr.amplitude_to_db(abs(lr.stft(dog_bark))), sr=dog_bark_freq, x_axis=\"time\", y_axis=\"hz\")\nplt.title(\"dog bark spect\")\nplt.colorbar()","22d58a7d":"# \u0441\u043f\u0435\u043a\u0442\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0434\u0435\u0442\u0441\u043a\u043e\u0439 \u0438\u0433\u0440\u044b\nlrd.specshow(lr.amplitude_to_db(abs(lr.stft(chld_pl))), sr=chld_pl_freq, x_axis=\"time\", y_axis=\"hz\")\nplt.title(\"child play spect\")\nplt.colorbar()","5954d7be":"# \u0441\u043f\u0435\u043a\u0442\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u043a\u043e\u043d\u0434\u0438\u0446\u0438\u043e\u043d\u0435\u0440\u0430\nlrd.specshow(lr.amplitude_to_db(abs(lr.stft(air_cond))), sr=air_cond_freq, x_axis=\"time\", y_axis=\"hz\")\nplt.title(\"air cond spect\")\nplt.colorbar()","a8a82783":"========================================<font size='4'>urban sound<\/font>======================================","6da86390":"<font size='3'>conclusion (mnist fashion) mnist fashion dataset contains 10000 photos of different types of clothes. there's 10 types of clothes (T-shirt\/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot). this dataset can be used for machine learning for clothes classification<\/font>","29eee604":"========================================<font size='4'>sign language mnist<\/font>======================================","28b49e36":"========================================<font size='4'>chinese mnist<\/font>======================================","c01e938d":"<font size='3'>conclusion (iris): iris dataset is a dataset about Iris plant. This dataset contains 4 columns (outer leaf height, outer leaf width, inner leaf height, inner leaf width), and 150 rows (150 plants). This dataset can be used to generate more information about Iris plants<\/font>","5fcc181a":"=======================================<font size='4'>iris<\/font>=======================================","f0c07634":"<font size='3'>conclusion (sign mnist) sign mnist dataset can be used for machine learning algorithm for understanding sign language. this dataset contains 25 letters in 28x28 format.<\/font>","cae06c43":"<font size='3'>conclusion (chinese mnist) chinese mnist dataset contains images with chinese characters. this dataset can be used to train a learning algorithm to classify chinese letters and translate chinese text (use in translator)<\/font>","48266b02":"========================================<font size='4'>fashion mnist<\/font>======================================","b123748a":"========================================<font size='4'>urban sound 8k spectogram<\/font>======================================","863f31d4":"<font size='3'>conclusion (urban sound) this dataset contains .wav files of different sounds. you can represent sound files as array of numbers. also this dataset contains foreground and background sounds as well. dataset has 9 different types of audio fragments (child play, dog bark, air cond, etc)<\/font>"}}