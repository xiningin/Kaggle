{"cell_type":{"f0886645":"code","315314c6":"code","7b077d44":"code","a6e42e07":"code","ce5e31cc":"code","6bd99055":"code","eca5a017":"code","33ff2b8f":"code","da306adf":"code","1f65419a":"code","bd24b231":"code","9c821020":"code","7031f1c7":"code","d8d3a20e":"code","992fd701":"code","2991d408":"code","bacc4c1d":"code","36105870":"code","d366738f":"code","992d7da7":"code","4478757a":"code","488037be":"code","d2d8e894":"code","1f3c5c3c":"code","e301b0bb":"code","8ec4b23b":"code","57abcdcf":"code","caa8e9f2":"code","8c616fde":"code","967b65b2":"code","e3894cdd":"code","c08de33d":"code","e364b9d7":"code","cd88bb1e":"code","297f6b3a":"code","aae4f9d6":"code","66643468":"code","c40cc15c":"code","03550872":"code","51c5b51a":"code","de16d94b":"code","ec1b8b68":"code","8a0560c8":"code","1fe08cf9":"code","d8bec759":"code","e7d8cf46":"code","14dda5c0":"code","e63503db":"markdown","d184301e":"markdown","2d0d0e51":"markdown","d9d6b289":"markdown","4f7c8338":"markdown","0b7069c6":"markdown","80163c9b":"markdown"},"source":{"f0886645":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","315314c6":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy .stats import norm \n\ndf = pd.read_csv('..\/input\/preprocess-choc\/dfn.csv')\ndf","7b077d44":"%matplotlib inline\nplt.hist(df.rating,bins=20,rwidth=0.8)\nplt.xlabel('rating')\nplt.ylabel('count')\nplt.show()","a6e42e07":"from scipy.stats import norm\nimport numpy as np\nfrom matplotlib import pyplot as plt\nplt.hist(df.rating,bins=20,rwidth=0.8)\nplt.xlabel('rating')\nplt.ylabel('count')\nrng=np.arange(df.rating.min(),df.rating.max(),0.1)\nplt.plot(rng,norm.pdf(rng,df.rating.mean(),df.rating.std()))\nplt.show()\n#matplotlib. rcParams['figure.figsize']=(10,6)\n\n#i dont know why the bell curve isnt plotting in Kaggle(was plotting in JN),Trouble shoot and let me know","ce5e31cc":"#max rating df.rating.max()\n\n#mean rating df.rating.mean()\n\n#std. deviation of rating df.rating.std()\n","6bd99055":"#so my upper limit will be my mean value plus 3 sigma\nupper_limit=df.rating.mean()+3*df.rating.std()\nupper_limit","eca5a017":"#my lowar limit will be my mean - 3 sigma\nlowar_limit=df.rating.mean()-3*df.rating.std()\nlowar_limit","33ff2b8f":"#now that my outliers are defined, i want to see what are my outliers\ndf[(df.rating>upper_limit)|(df.rating<lowar_limit)]","da306adf":"#now we will visualise the good data\nnew_data=df[(df.rating<upper_limit)& (df.rating>lowar_limit)]\nnew_data","1f65419a":"#shape of our new data\nnew_data.shape","bd24b231":"#shape of our outliers\ndf.shape[0]-new_data.shape[0]","9c821020":"#now we will calculate the z score of all our datapoints and display in a dataframe\ndf['zscore']=(df.rating-df.rating.mean())\/df.rating.std()\ndf","7031f1c7":"![z.png]","d8d3a20e":"#figuring out all the datapoints more than 3\ndf[df['zscore']>3]","992fd701":"#figuring out all the datapoints less than 3\ndf[df['zscore']<-3]","2991d408":"#displaying the outliers with respect to the zscores\ndf[(df.zscore<-3)|(df.zscore>3)]","bacc4c1d":"new_data_1=df[(df.zscore>-3)& (df.zscore<3)]\nnew_data_1","36105870":"figure=df.boxplot(column=\"rating\", figsize=(20,20))","d366738f":"figure=new_data_1.boxplot(column=\"rating\", figsize=(20,20))","992d7da7":"from scipy.stats import norm\nimport numpy as np\nfrom matplotlib import pyplot as plt\nplt.hist(df.rating,bins=20,rwidth=0.8)\nplt.xlabel('rating')\nplt.ylabel('count')\nrng=np.arange(df.rating.min(),df.rating.max(),0.1)\nplt.plot(rng,norm.pdf(rng,df.rating.mean(),df.rating.std()))\nplt.show()\n#matplotlib. rcParams['figure.figsize']=(10,6)\n\n#i dont know why the bell curve isnt plotting in Kaggle(was plotting in JN),Trouble shoot and let me know","4478757a":"from scipy.stats import norm\nimport numpy as np\nfrom matplotlib import pyplot as plt\nplt.hist(new_data_1.rating,bins=20,rwidth=0.8)\nplt.xlabel('rating')\nplt.ylabel('count')\nrng=np.arange(new_data_1.rating.min(),new_data_1.rating.max(),0.1)\nplt.plot(rng,norm.pdf(rng,new_data_1.rating.mean(),new_data_1.rating.std()))\nplt.show()\n#matplotlib. rcParams['figure.figsize']=(10,6)\n\n#i dont know why the bell curve isnt plotting in Kaggle(was plotting in JN),Trouble shoot and let me know","488037be":"df=df.drop(['zscore'],axis=1)","d2d8e894":"df.describe()","1f3c5c3c":"Q1=df.rating.quantile(0.25)\nQ3=df.rating.quantile(0.75)\nQ1,Q3\n#WHICH MEANS THAT Q1 CORRESPONDS TO 25% OF ALL THE HEIGHT DISTRIBUTION IS BELOW 3.0\n#Q3 CORRESPONDS TO 75% OF ALL THE HEIGHT DISTRIBUTION IS BELOW 3.5","e301b0bb":"#NOW WE WILL CALCULATE THE IQR\nIQR=Q3-Q1\nIQR","8ec4b23b":"#NOW WE WILL DEFINE THE UPPER LIMITS AND LOWAR LIMITS\nLOWAR_LIMIT=Q1-1.5*IQR\nUPPER_LIMIT=Q3+1.5*IQR\nLOWAR_LIMIT,UPPER_LIMIT","57abcdcf":"#NOW WE SHALL DISPLY THE OUTLIERS rating\ndf[(df.rating<LOWAR_LIMIT)|(df.rating>UPPER_LIMIT)]","caa8e9f2":"#NOW WE WILL DISPLAY THE REMAINING SAMPLES ARE WITHIN THE RANGE\nWithout_outliers_data = df[(df.rating>LOWAR_LIMIT)&(df.rating<UPPER_LIMIT)]","8c616fde":"figure=df.boxplot(column=\"rating\", figsize=(20,20))\n","967b65b2":"figure=Without_outliers_data.boxplot(column=\"rating\", figsize=(20,20))","e3894cdd":"from scipy.stats import norm\nimport numpy as np\nfrom matplotlib import pyplot as plt\nplt.hist(df.rating,bins=20,rwidth=0.8)\nplt.xlabel('rating')\nplt.ylabel('count')\nrng=np.arange(df.rating.min(),df.rating.max(),0.1)\nplt.plot(rng,norm.pdf(rng,df.rating.mean(),df.rating.std()))\nplt.show()\n#matplotlib. rcParams['figure.figsize']=(10,6)\n\n#i dont know why the bell curve isnt plotting in Kaggle(was plotting in JN),Trouble shoot and let me know","c08de33d":"from scipy.stats import norm\nimport numpy as np\nfrom matplotlib import pyplot as plt\nplt.hist(Without_outliers_data.rating,bins=20,rwidth=0.8)\nplt.xlabel('rating')\nplt.ylabel('count')\nrng=np.arange(Without_outliers_data.rating.min(),df.rating.max(),0.1)\nplt.plot(rng,norm.pdf(rng,Without_outliers_data.rating.mean(),Without_outliers_data.rating.std()))\nplt.show()\n#matplotlib. rcParams['figure.figsize']=(10,6)\n\n#i dont know why the bell curve isnt plotting in Kaggle(was plotting in JN),Trouble shoot and let me know","e364b9d7":"df_enc = pd.read_csv('..\/input\/preprocess-choc\/10 best RD_Feature')\ndf_enc","cd88bb1e":"a = df_enc.loc[:,~df_enc.columns.duplicated()]\na","297f6b3a":"\nb = a.drop('rating', axis = 1)","aae4f9d6":"X = b.iloc[:,0:11]  \ny = a.iloc[:,2]    \n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train,y_train, X_test,y_test = train_test_split(X, y, test_size=0.3)","66643468":"y","c40cc15c":"\nfrom scipy import stats\n\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\nimport matplotlib.dates as md\nfrom scipy.stats import norm\n%matplotlib inline \nimport seaborn as sns \nsns.set_style(\"whitegrid\") #possible choices: white, dark, whitegrid, darkgrid, ticks\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\npd.set_option('float_format', '{:f}'.format)\npd.set_option('max_columns',250)\npd.set_option('max_rows',150)","03550872":"clf = IsolationForest(max_samples='auto', random_state = 1, contamination= 0.02)\npreds = clf.fit_predict(X)\ndf['isoletionForest_outliers'] = preds\ndf['isoletionForest_outliers'] = df['isoletionForest_outliers'].astype(str)\ndf['isoletionForest_scores'] = clf.decision_function(X)\nprint(df['isoletionForest_outliers'].value_counts())\n","51c5b51a":"df[152:156]","de16d94b":"!pip install eif","ec1b8b68":"import eif as iso","8a0560c8":"fig, ax = plt.subplots(figsize=(20, 7))\nax.set_title('Distribution of Extended Isolation Scores', fontsize = 15, loc='center')\nsns.distplot(df['isoletionForest_scores'],color='red',label='if',hist_kws = {\"alpha\": 0.5});","1fe08cf9":"\n\nfig, ax = plt.subplots(figsize=(30, 7))\nax.set_title('Extended Outlier Factor Scores Outlier Detection', fontsize = 15, loc='center')\n\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], color='g', s=3., label='Data points')\nradius = (df['isoletionForest_scores'].max() - df['isoletionForest_scores']) \/ (df['isoletionForest_scores'].max() - df['isoletionForest_scores'].min())\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], s=2000 * radius, edgecolors='r', facecolors='none', label='Outlier scores')\nplt.axis('tight')\nlegend = plt.legend(loc='upper left')\nlegend.legendHandles[0]._sizes = [10]\nlegend.legendHandles[1]._sizes = [20]\nplt.show();\n\n","d8bec759":"\n\nclf = LocalOutlierFactor(n_neighbors=11)\ny_pred = clf.fit_predict(X)\n\ndf['localOutlierFactor_outliers'] = y_pred.astype(str)\nprint(df['localOutlierFactor_outliers'].value_counts())\ndf['localOutlierFactor_scores'] = clf.negative_outlier_factor_\n\n","e7d8cf46":"fig, ax = plt.subplots(figsize=(20, 7))\nax.set_title('Distribution of Local Outlier Factor Scores', fontsize = 15, loc='center')\nsns.distplot(df['localOutlierFactor_scores'],color='red',label='eif',hist_kws = {\"alpha\": 0.5});\n","14dda5c0":"\n\nfig, ax = plt.subplots(figsize=(30, 7))\nax.set_title('Local Outlier Factor Scores Outlier Detection', fontsize = 15, loc='center')\n\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], color='g', s=3., label='Data points')\nradius = (df['localOutlierFactor_scores'].max() - df['localOutlierFactor_scores']) \/ (df['localOutlierFactor_scores'].max() - df['localOutlierFactor_scores'].min())\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], s=2000 * radius, edgecolors='r', facecolors='none', label='Outlier scores')\nplt.axis('tight')\nlegend = plt.legend(loc='upper left')\nlegend.legendHandles[0]._sizes = [10]\nlegend.legendHandles[1]._sizes = [20]\nplt.show();\n\n","e63503db":"# **3.Inter Quartile Range**","d184301e":"\n\n#max rating\ndf.rating.max()\n#mean rating\ndf.rating.mean()\n#std. deviation of rating\ndf.rating.std()","2d0d0e51":"# **2. Zscore**","d9d6b289":"# 5. Local outliers method","4f7c8338":"# 1.MinMax method","0b7069c6":"# 4. Isolation forest method","80163c9b":"# Now we will try to remove the outliers by z scores\n# z score tells how many standard deviations  away a data point is\n# in our case mean is 3.198 and std deviation is 0.434\n# so our Z SCORE for datapoint 4.5 is 4.5-3.198(mean)\/0.434(std)= 1.847"}}