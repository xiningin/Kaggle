{"cell_type":{"741ca849":"code","7e37212e":"code","a39ea12d":"code","7601ce4f":"code","00555002":"code","4709b167":"code","73fc734b":"code","86b10d19":"code","dfdad411":"code","5a0b0abf":"code","d3c45641":"markdown","8ce08c62":"markdown","16828460":"markdown","15744be5":"markdown","f00ef75c":"markdown","6a243116":"markdown","93b4ac11":"markdown","a82f0ce6":"markdown","1c1cfdbd":"markdown"},"source":{"741ca849":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import TensorBoard\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import accuracy_score\nfrom keras.utils import np_utils\nimport itertools","7e37212e":"#load dataset\ndata = np.load('..\/input\/orl-faces\/ORL_faces.npz') \n\n# load the \"Train Images\"\nx_train = data['trainX']\n#normalize every image\nx_train = np.array(x_train,dtype='float32')\/255\n\nx_test = data['testX']\nx_test = np.array(x_test,dtype='float32')\/255\n\n# load the Label of Images\ny_train= data['trainY']\ny_test= data['testY']\n\n# show the train and test Data format\nprint('x_train : {}'.format(x_train[:]))\nprint('Y-train shape: {}'.format(y_train))\nprint('x_test shape: {}'.format(x_test.shape))","a39ea12d":"x_train, x_valid, y_train, y_valid= train_test_split(\n    x_train, y_train, test_size=.05, random_state=1234,)","7601ce4f":"im_rows=112\nim_cols=92\nbatch_size=512\nim_shape=(im_rows, im_cols, 1)\n\n#change the size of images\nx_train = x_train.reshape(x_train.shape[0], *im_shape)\nx_test = x_test.reshape(x_test.shape[0], *im_shape)\nx_valid = x_valid.reshape(x_valid.shape[0], *im_shape)\n\nprint('x_train shape: {}'.format(y_train.shape[0]))\nprint('x_test shape: {}'.format(y_test.shape))","00555002":"\n#filters= the depth of output image or kernels\n\ncnn_model= Sequential([\n    Conv2D(filters=36, kernel_size=7, activation='relu', input_shape= im_shape),\n    MaxPooling2D(pool_size=2),\n    Conv2D(filters=54, kernel_size=5, activation='relu', input_shape= im_shape),\n    MaxPooling2D(pool_size=2),\n    Flatten(),\n    Dense(2024, activation='relu'),\n     Dropout(0.5),\n    Dense(1024, activation='relu'),\n    Dropout(0.5),\n    Dense(512, activation='relu'),\n    Dropout(0.5),\n    #20 is the number of outputs\n    Dense(20, activation='softmax')  \n])\n\ncnn_model.compile(\n    loss='sparse_categorical_crossentropy',#'categorical_crossentropy',\n    optimizer=Adam(lr=0.0001),\n    metrics=['accuracy']\n)","4709b167":"\ncnn_model.summary()","73fc734b":"history=cnn_model.fit(\n    np.array(x_train), np.array(y_train), batch_size=512,\n    epochs=250, verbose=2,\n    validation_data=(np.array(x_valid),np.array(y_valid)),\n)","86b10d19":"scor = cnn_model.evaluate( np.array(x_test),  np.array(y_test), verbose=0)\n\nprint('test los {:.4f}'.format(scor[0]))\nprint('test acc {:.4f}'.format(scor[1]))","dfdad411":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","5a0b0abf":"predicted =np.argmax(cnn_model.predict(x_test), axis=-1)\n\n#print(predicted)\n#print(y_test)\nynew = cnn_model.predict_classes(x_test)\n\n\nAcc=accuracy_score(y_test, ynew)\nprint(\"accuracy : \")\nprint(Acc)\n#\/tn, fp, fn, tp = confusion_matrix(np.array(y_test), ynew).ravel()\ncnf_matrix=confusion_matrix(np.array(y_test), ynew)\n\ny_test1 = np_utils.to_categorical(y_test, 20)\n\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    #print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\n\nprint('Confusion matrix, without normalization')\nprint(cnf_matrix)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix[1:10,1:10], classes=[0,1,2,3,4,5,6,7,8,9],\n                      title='Confusion matrix, without normalization')\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix[11:20,11:20], classes=[10,11,12,13,14,15,16,17,18,19],\n                      title='Confusion matrix, without normalization')\n\nprint(\"Confusion matrix:\\n%s\" % confusion_matrix(np.array(y_test), ynew))\nprint(classification_report(np.array(y_test), ynew))","d3c45641":"Evaluate the test data","8ce08c62":"\nShow the model's parameters.","16828460":"# **Step 4**\n\nfor using the CNN, we need to change The size of images ( The size of images must be the same)","15744be5":"# Step 3\nSplit DataSet : Validation data and Train\n\nValidation DataSet: this data set is used to minimize overfitting.If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you're overfitting your neural network and you should stop training.\n\n**Note:** we usually use 30 percent of every dataset as the validation data but Here we only used 5 percent because the number of images in this dataset is very low.","f00ef75c":"# Step 2\nLoad Dataset :\nAfter loading the Dataset you have to normalize every image.\n\n**Note:** an image is a Uint8 matrix of pixels and for calculation, you need to convert the format of the image to float or double","6a243116":"# step 8\nPlot Confusion Matrix","93b4ac11":"Step 6\nTrain the Model\n\nNote: You can change the number of epochs","a82f0ce6":"# **Step 5**\n\nBuild CNN model: CNN have 3 main layer:\n\n1-Convolotional layer\n2- pooling layer\n3- fully connected layer\n\nwe could build a new architecture of CNN by changing the number and position of layers.","1c1cfdbd":"# Step 7\nplot the result"}}