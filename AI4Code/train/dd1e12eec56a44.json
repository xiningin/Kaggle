{"cell_type":{"71208794":"code","27dbba4c":"code","460c0843":"code","9e1af57f":"code","744ff841":"code","a0802434":"code","8501e80c":"code","d2a6e673":"code","68dee34f":"code","2a5c6cb8":"code","c1e7513b":"code","609f26ee":"code","af6b6f05":"markdown","e02ca4c2":"markdown","1c79eacf":"markdown","40aad42b":"markdown","cde40ddb":"markdown","8e697912":"markdown","f2fec5bc":"markdown","8f29d335":"markdown","c58c072b":"markdown","0da851bf":"markdown","521ac661":"markdown","3ef454e0":"markdown","94751ae6":"markdown","a5a53981":"markdown","cbd8d59c":"markdown","af89e9df":"markdown"},"source":{"71208794":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom keras.layers import Dense, Flatten, BatchNormalization, Dropout\nfrom keras.applications import InceptionV3\nfrom glob import glob","27dbba4c":"train_path = \"..\/input\/100-bird-species\/285 birds\/train\"\ntest_path = \"..\/input\/100-bird-species\/285 birds\/test\"\nvalidation_path = \"..\/input\/100-bird-species\/285 birds\/valid\"","460c0843":"img = load_img(train_path + \"\/ANHINGA\/001.jpg\")\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"Sample Anhinga Image\")\nplt.show()\n\nplt.figure()\n\nimg = load_img(train_path + \"\/BALD EAGLE\/018.jpg\")\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"Sample Bald Eagle Image\")\nplt.show()","9e1af57f":"fig, axs = plt.subplots(2, 2, figsize=(8, 8))\naxs[0,0].imshow(load_img(train_path + \"\/BARN OWL\/018.jpg\"))\naxs[0,0].axis(\"off\")\n\naxs[0,1].imshow(load_img(train_path + \"\/ALBATROSS\/001.jpg\"))\naxs[0,1].axis(\"off\")\n\naxs[1,0].imshow(load_img(train_path + \"\/CANARY\/107.jpg\"))                        \naxs[1,0].axis(\"off\")\n\naxs[1,1].imshow(load_img(train_path + \"\/CROW\/100.jpg\"))\naxs[1,1].axis(\"off\")\n\nplt.show()","744ff841":"className = glob(train_path + \"\/*\")\nNumberofClass = len(className)\nprint(\"NumberofClass:\", NumberofClass)","a0802434":"train_datagen = ImageDataGenerator(rescale=1\/255)\nvalidation_datagen=ImageDataGenerator(rescale=1\/255)\ntest_datagen=ImageDataGenerator(rescale=1\/255)","8501e80c":"batch_size = 256\ntrain_datagen = ImageDataGenerator(rescale=1\/255,\n                  shear_range=0.3,\n                  horizontal_flip=True,\n                  zoom_range=0.3\n                  )\nval_datagen = ImageDataGenerator(rescale=1\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n                train_path,\n                target_size=(224,224),\n                batch_size=batch_size,\n                color_mode=\"rgb\",\n                class_mode=\"categorical\"\n                )\n\nval_generator = val_datagen.flow_from_directory(\n                validation_path,\n                target_size=(224,224),\n                batch_size=batch_size,\n                color_mode=\"rgb\",\n                class_mode=\"categorical\"\n                )","d2a6e673":"IncV3 = InceptionV3(include_top = False, weights = \"imagenet\",input_shape = (224,224,3))","68dee34f":"model = Sequential()\n\nmodel.add(IncV3)\n    \nfor layer in model.layers:\n    layer.trainable = False\n\n\nmodel.add(Flatten())\n\nmodel.add(Dense(units = 2048, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units = NumberofClass, activation = \"softmax\"))\n","2a5c6cb8":"model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])","c1e7513b":"history = model.fit(train_generator,\n                    validation_data = val_generator,\n                    epochs = 5,\n                    batch_size = batch_size)\n                    ","609f26ee":"plt.figure(figsize=(8,8))\nplt.plot(history.history['loss'], color='b', label=\"Training loss\")\nplt.plot(history.history['val_loss'], color='r', label=\"Validation loss\")\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.figure(figsize=(8,8))\nplt.plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nplt.plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nplt.legend()\nplt.show()","af6b6f05":"## Import Libraries","e02ca4c2":"## V\u0131sualization","1c79eacf":"## Compiling the Model","40aad42b":"## InceptionV3 Model","cde40ddb":"This notebook consists of the following topics:\n* About InceptionV3\n* About Dataset\n* Import Libraries\n* Loading Data\n* Visualization \n* Data Processing\n* Data Augmentation\n* InceptionV3 Model\n* Compiling the Model\n* Fitting\n* Evaluating the Model","8e697912":"InceptionV3 is a convolutional neural network for assisting in image analysis and object detection, and got its start as a module for Googlenet. It is the third edition of Google's Inception Convolutional Neural Network, originally introduced during the ImageNet Recognition Challenge. Just as ImageNet can be thought of as a database of classified visual objects, Inception helps classification of objects in the world of computer vision.","f2fec5bc":"## Data Augmentation","8f29d335":"## About Dataset","c58c072b":"## Fitting","0da851bf":"# Birds Image Classification with InceptionV3","521ac661":"## Evaluating the Model","3ef454e0":"## About InceptionV3","94751ae6":"The data set consists of 40930 training images, 1425 test images, 1425 validation images and 125 different classes. Images are in 224x224x3 format.","a5a53981":"## Data Processing","cbd8d59c":"![](http:\/\/paperswithcode.com\/media\/methods\/inceptionv3onc--oview_vjAbOfw.png![image.png](attachment:b909ba6d-0ceb-4c56-8b7d-43ccbecbce19.png)![image.png](attachment:7ef18d29-4a33-4384-8649-21bbfb15176c.png)![image.png](attachment:23823979-57e0-4392-943d-929cffd13758.png))","af89e9df":"## Loading Data"}}