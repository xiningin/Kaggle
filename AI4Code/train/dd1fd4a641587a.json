{"cell_type":{"4e6bd74e":"code","f4de3784":"code","46f4c1f5":"code","8715b8c2":"code","aee4527f":"code","8ff8e348":"code","ea4c3539":"code","38d2b1f9":"code","9401a6db":"code","3d7edbc1":"code","5d445aca":"code","8b695daf":"code","2e4fe49d":"code","cf2f64ed":"code","38454cb0":"code","77d41ef2":"code","1fe0f7f7":"code","cec541e6":"code","8efd5f80":"code","be4bc3bb":"code","12b3a21a":"code","9985c4c3":"code","b5c22fbd":"code","8fa21457":"code","f08f1a9b":"code","625c97d5":"code","59e2bae6":"code","2921fa88":"code","8592c5c2":"code","d48a1d7c":"code","5505a610":"code","02fefa94":"code","f1695032":"code","21f8ea2d":"code","c9189f79":"markdown","16c9f7bb":"markdown","7caad5d8":"markdown","f0969cd5":"markdown","67733b7c":"markdown","0722a351":"markdown","8afa4a2c":"markdown","be473dcf":"markdown","0ca1a74c":"markdown","2818b3f2":"markdown","c0f72c00":"markdown","186c43aa":"markdown","bd85ee8b":"markdown","30df3dae":"markdown","0666713c":"markdown","c0b4b302":"markdown","b2f5c1ad":"markdown","c35418e8":"markdown","38c52417":"markdown","1207f6d0":"markdown"},"source":{"4e6bd74e":"!pip install texthero","f4de3784":"!pip install -U spacy","46f4c1f5":"!pip install lime","8715b8c2":"import numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport texthero as hero  # text processing\nimport seaborn as sns # plotting\nfrom tqdm.auto import tqdm # progress bars\nfrom lime.lime_text import LimeTextExplainer # Model Explanation\nfrom sklearn import metrics # from here and below used for machine learning\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import MaxAbsScaler, FunctionTransformer\nfrom sklearn.model_selection import cross_validate, train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer","aee4527f":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport sys\nimport os\n\nfor dirname, _, filenames in os.walk(\"\/kaggle\/input\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8ff8e348":"# load data\ndf = pd.read_csv(\n    \"\/kaggle\/input\/kosovo-news-articles-dataset\/Kosovo-News-Articles.csv\",\n    dtype={\"content\": str, \"title\": str, \"category\": str, \"author\": str, \"source\": str},\n    parse_dates=[\"date\"],\n)","ea4c3539":"def preprocess_text(s):\n    \"\"\"A text processing pipeline for cleaning up text using the hero package.\"\"\"\n    s = hero.fillna(s)\n    s = hero.lowercase(s)\n    s = hero.remove_digits(s)\n    s = hero.remove_punctuation(s)\n    s = hero.remove_diacritics(s)\n    s = hero.remove_whitespace(s)\n    return s\n\n# A list of stopwords taken from https:\/\/github.com\/arditdine\/albanian-nlp\nSTOPWORDS = [\"e\", \"te\", \"i\", \"me\", \"qe\", \"ne\", \"nje\", \"a\", \"per\", \"sh\", \"nga\", \"ka\", \"u\", \"eshte\", \"dhe\", \"shih\", \"nuk\",\n             \"m\", \"dicka\", \"ose\", \"si\", \"shume\", \"etj\", \"se\", \"pa\", \"sipas\", \"s\", \"t\", \"dikujt\", \"dike\", \"mire\", \"vet\",\n             \"bej\", \"ai\", \"vend\", \"prej\", \"ja\", \"duke\", \"tjeter\", \"kur\", \"ia\", \"ku\", \"ta\", \"keq\", \"dy\", \"ben\", \"bere\",\n             \"behet\", \"dickaje\", \"edhe\", \"madhe\", \"la\", \"sa\", \"gjate\", \"zakonisht\", \"pas\", \"veta\", \"mbi\", \"disa\", \"iu\",\n             \"mos\", \"c\", \"para\", \"dikush\", \"gje\", \"be\", \"pak\", \"tek\", \"fare\", \"beri\", \"po\", \"bie\", \"k\", \"do\", \"gjithe\",\n             \"vete\", \"mund\", \"kam\", \"le\", \"jo\", \"beje\", \"tij\", \"kane\", \"ishte\", \"jane\", \"vjen\", \"ate\", \"kete\", \"neper\",\n             \"cdo\", \"na\", \"marre\", \"merr\", \"mori\", \"rri\", \"deri\", \"b\", \"kishte\", \"mban\", \"perpara\", \"tyre\", \"marr\",\n             \"gjitha\", \"as\", \"vetem\", \"nen\", \"here\", \"tjera\", \"tjeret\", \"drejt\", \"qenet\", \"ndonje\", \"nese\", \"jap\",\n             \"merret\", \"rreth\", \"lloj\", \"dot\", \"saj\", \"nder\", \"ndersa\", \"cila\", \"veten\", \"ma\", \"ndaj\", \"mes\", \"ajo\",\n             \"cilen\", \"por\", \"ndermjet\", \"prapa\", \"mi\", \"tere\", \"jam\", \"ashtu\", \"kesaj\", \"tille\", \"behem\", \"cilat\",\n             \"kjo\", \"menjehere\", \"ca\", \"je\", \"aq\", \"aty\", \"prane\", \"ato\", \"pasur\", \"qene\", \"cilin\", \"teper\", \"njera\",\n             \"tej\", \"krejt\", \"kush\", \"bejne\", \"ti\", \"bene\", \"midis\", \"cili\", \"ende\", \"keto\", \"kemi\", \"sic\", \"kryer\",\n             \"cilit\", \"atij\", \"gjithnje\", \"andej\", \"siper\", \"sikur\", \"ketej\", \"ciles\", \"ky\", \"papritur\", \"ua\",\n             \"kryesisht\", \"gjithcka\", \"pasi\", \"kryhet\", \"mjaft\", \"ketij\", \"perbashket\", \"ata\", \"atje\", \"vazhdimisht\",\n             \"kurre\", \"tone\", \"keshtu\", \"une\", \"sapo\", \"rralle\", \"vetes\", \"ishin\", \"afert\", \"tjetren\", \"ketu\", \"cfare\",\n             \"to\", \"anes\", \"jemi\", \"asaj\", \"secila\", \"kundrejt\", \"ketyre\", \"pse\", \"tilla\", \"mua\", \"nepermjet\", \"cilet\",\n             \"ndryshe\", \"kishin\", \"ju\", \"tani\", \"atyre\", \"dic\", \"yne\", \"kudo\", \"sone\", \"sepse\", \"cilave\", \"kem\", \"ty\",\n             \"t'i\", \"nbsp\", \"tha\", \"re\", \"the\"]\n\n\ndef make_clf(classifier, scaler=None, feature_extractor=None, use_dense=False):\n    \"\"\"Function for generating pipelines for our experiment.\"\"\"\n    steps = []\n\n    if feature_extractor is not None:\n        steps.append([\"feature_extractor\", feature_extractor])\n    if use_dense:\n        steps.append(\n            [\"to_dense\", FunctionTransformer(lambda x: x.todense(), accept_sparse=True)]\n        )\n    if scaler is not None:\n        steps.append([\"scaler\", scaler])\n    steps.append([\"classifier\", classifier])\n\n    return Pipeline(steps)","38d2b1f9":"data = pd.concat(\n    [\n        df.loc[(~df[\"content\"].isna()) & (df[\"source\"] == \"Kungulli\")],\n        df.loc[(~df[\"content\"].isna()) & (df[\"source\"] == \"Kallxo.com\")].sample(\n            n=699, random_state=42\n        ),\n    ]\n)","9401a6db":"data[\"satire_news\"] = np.where(data[\"source\"] == \"Kungulli\", 1, 0)\ndata[\"satire_news\"] = data[\"satire_news\"].astype(bool)","3d7edbc1":"data[\"content\"] = data[\"content\"].str.replace(\"Kungulli.com\", \"\")\ndata[\"content\"] = data[\"content\"].str.replace(\"Kungulli\", \"\")\ndata[\"content\"] = data[\"content\"].str.replace(\"kungulli\", \"\")\n\ndata[\"content\"] = data[\"content\"].str.replace(\"KALLXO.com\", \"\")\ndata[\"content\"] = data[\"content\"].str.replace(\"Kallxo.com\", \"\")\ndata[\"content\"] = data[\"content\"].str.replace(\"Kallxo\", \"\")\ndata[\"content\"] = data[\"content\"].str.replace(\"KALLXO\", \"\")\ndata[\"content\"] = data[\"content\"].str.replace(\"kallxo\", \"\")","5d445aca":"data[\"preprocessed_content\"] = preprocess_text(data[\"content\"])","8b695daf":"# Remove stopwords\ndata[\"preprocessed_content_without_stopwords\"] = data[\"preprocessed_content\"].apply(\n    lambda x: \" \".join([word for word in x.split() if word not in (STOPWORDS)])\n)","2e4fe49d":"print(data.loc[data[\"source\"]==\"Kungulli\"][\"content\"].values[0])","cf2f64ed":"print(data.loc[data[\"source\"]==\"Kallxo.com\"][\"content\"].values[0])","38454cb0":"# Defining colors for both sources to be used in our plots\npalette = [\"#ffa600\", \"#003f5c\"]","77d41ef2":"# Kungulli wordcloud\nhero.wordcloud(\n    data.loc[data[\"source\"] == \"Kungulli\"][\"preprocessed_content_without_stopwords\"],\n    max_words=100,\n    width=1900,\n    height=400,\n    background_color=palette[0],\n)","1fe0f7f7":"# Kallxo.com wordcloud\nhero.wordcloud(\n    data.loc[data[\"source\"] == \"Kallxo.com\"][\"preprocessed_content_without_stopwords\"],\n    max_words=100,\n    width=1900,\n    height=400,\n    background_color=palette[1],\n)","cec541e6":"sns.set(rc={'figure.figsize':(20,10)})\nsns.set_theme(style=\"whitegrid\")","8efd5f80":"# texthero pipeline and plot\ndata[\"pca\"] = (\n    data[\"preprocessed_content_without_stopwords\"].pipe(hero.tfidf).pipe(hero.pca)\n)\n\nplot_values = np.stack(data[\"pca\"], axis=1)\nsns.scatterplot(\n    data=data,\n    x=plot_values[0],\n    y=plot_values[1],\n    hue=\"satire_news\",\n    style=\"satire_news\",\n    markers={True: \"X\", False: \"s\"},\n    alpha=0.5,\n    palette=palette.reverse(),\n)","be4bc3bb":"data[\"article_word_count\"] = data[\"content\"].apply(lambda x: len(x.split()))\n\nax = sns.histplot(data=data, x=\"article_word_count\", hue=\"satire_news\", palette=palette)\nax.set(xlabel='Article Word Count', ylabel='Frequency')","12b3a21a":"classifiers = [\n    GaussianNB(),\n    LinearSVC(random_state=42),\n    KNeighborsClassifier(),\n    RandomForestClassifier(random_state=42),\n    LogisticRegression(solver=\"liblinear\", random_state=42),\n    DecisionTreeClassifier(random_state=42),\n    MultinomialNB(),\n]\n\nscalers = [MaxAbsScaler(), None]\nprediction_columns = [\"content\", \"preprocessed_content\", \"preprocessed_content_without_stopwords\"]","9985c4c3":"results = []\n\nfor prediction_column in tqdm(prediction_columns):\n    X = data[prediction_column].to_numpy()\n    y = data[\"satire_news\"].astype(int).values\n    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n    \n    for classifier in tqdm(classifiers):\n        for scaler in scalers:\n            clf = make_clf(\n                feature_extractor=TfidfVectorizer(),\n                use_dense=True,\n                scaler=scaler,\n                classifier=classifier,\n            )\n            train_scores = cross_validate(\n                clf,\n                X_train,\n                y_train,\n                cv=5,\n                scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"],\n            )\n            clf.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n            results.append(\n                {\n                    \"prediction_column\": prediction_column,\n                    \"classifier\": type(classifier).__name__,\n                    \"scaler\": type(scaler).__name__,\n                    \"fit_time\": train_scores[\"fit_time\"].mean(),\n                    \"score_time\": train_scores[\"score_time\"].mean(),\n                    \"train_accuracy\": train_scores[\"test_accuracy\"].mean(),\n                    \"train_precision\": train_scores[\"test_precision_macro\"].mean(),\n                    \"train_recall\": train_scores[\"test_recall_macro\"].mean(),\n                    \"train_f1\": train_scores[\"test_f1_macro\"].mean(),\n                    \"test_accuracy\": metrics.accuracy_score(y_test, y_pred),\n                    \"test_precision\": metrics.precision_score(\n                        y_test, y_pred, average=\"macro\"\n                    ),\n                    \"test_recall\": metrics.recall_score(\n                        y_test, y_pred, average=\"macro\"\n                    ),\n                    \"test_f1\": metrics.f1_score(y_test, y_pred, average=\"macro\"),\n                }\n            )","b5c22fbd":"results_df = pd.DataFrame(results)","8fa21457":"ax = sns.barplot(x=\"test_accuracy\", y=\"prediction_column\", data=results_df)\nax.set(xlabel='Average Test Accuracy', ylabel='Article Content Preprocessing Approach')","f08f1a9b":"ax = sns.barplot(x=\"test_accuracy\", y=\"scaler\", data=results_df)\nax.set(xlabel='Average Test Accuracy', ylabel='Scaling Method')","625c97d5":"ax = sns.barplot(x=\"test_accuracy\", y=\"classifier\", data=results_df)\nax.set(xlabel='Average Test Accuracy', ylabel='Classifier')","59e2bae6":"ax = sns.barplot(x=\"fit_time\", y=\"classifier\", data=results_df)\nax.set(xlabel='Average Fitting Time (s)', ylabel='Classifier')","2921fa88":"best_results_df = results_df[results_df.groupby(['prediction_column'])['test_accuracy'].transform(max) == results_df['test_accuracy']]","8592c5c2":"best_results_df[\"pipeline\"] = best_results_df[\"prediction_column\"] + \"-\" + best_results_df[\"scaler\"] + \"-\" + best_results_df[\"classifier\"]","d48a1d7c":"ax = sns.barplot(x=\"test_accuracy\", y=\"pipeline\", data=best_results_df)\nax.set(xlabel='Test Accuracy', ylabel='Pipeline')","5505a610":"X = data[\"preprocessed_content_without_stopwords\"].to_numpy()\ny = data[\"satire_news\"].astype(int).values\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\nclf = make_clf(\n    feature_extractor=TfidfVectorizer(),\n    use_dense=True,\n    scaler=MaxAbsScaler(),\n    classifier=LogisticRegression(solver=\"liblinear\"),\n)\n\nclf.fit(X_train, y_train)","02fefa94":"explainer = LimeTextExplainer(class_names=[\"Not Satirical\", \"Satirical\"])","f1695032":"exp = explainer.explain_instance(X_test[7], clf.predict_proba)\nexp.show_in_notebook(text=True)","21f8ea2d":"exp = explainer.explain_instance(X_test[5], clf.predict_proba)\nexp.show_in_notebook(text=True)","c9189f79":"# Satirical News Detection and Analysis in Albanian\n\nIn this notebook, we will attempt Satirical News Detection and Analysis in Albanian utilizing news articles obtained from the Albanian News Article Dataset from two separate sources, Kungulli.com and Kallxo.com, and try to predict the kind of article using supervised machine learning.\n\n## Experiment Setup\n\nThe experiment will be carried out utilizing a Cross-validation (5-fold) experiment setup for internal validation, referred as the training score, and an external testing set for external validation, referred as the testing score, as shown in the Figure below:\n\n![Experiment Setup](https:\/\/i.imgur.com\/E0z2vGN.png)\n\nFor our article contents, we will evaluate a set of classifiers, scaling methods, and three distinct types of preprocessing approaches.\n\nClassifiers used:\n- GaussianNB\n- LinearSVC\n- KNeighborsClassifier\n- RandomForestClassifier\n- LogisticRegression\n- DecisionTreeClassifier\n- MultinomialNB\n\nScaling methods used:\n- Nonetype, alias no scaling at all used\n- MaxAbsScaler\n\nArticle content preprocessing approach:\n- content - Using the article's raw content as found in the original dataset.\n- preprocessed_content - The `preprocess_text` function is used to preprocess the content of the article.\n- preprocessed_content_without_stopwords - Stopwords are also deleted once the content has been preprocessed.\n\n## Getting started\n\nTo begin, we must install the necessary packages.","16c9f7bb":"## Explorative Data Analysis\n\nLet's start with an example article from each source:","7caad5d8":"To be unable to distinguish between articles just based on the name of the news source present in the content of the articles, as appears to be the situation with Kungulli.com articles. Kungulli will be eliminated from the content as a word. To be fair, same will be done for Kallxo.com.","f0969cd5":"## Imports","67733b7c":"Furthermore, when we look at the scaling methods used, we can see that scaling the data has no effect on improving the classifier's performance.","0722a351":"## Utility code","8afa4a2c":"## Results\n\nLets evaluate the results from the experiment.","be473dcf":"After applying PCA on the `preprocessed_content_without_stopwords` column and visualizing the features with a scatter plot, the outputs are shown below. It is noticeable that the feature structures of the two kinds of articles differ significantly.","0ca1a74c":"## Preprocessing\n\nTo proceed, we only want to consider satirical news articles collected from the satire website Kungulli.com, and to have some genuine news data, the same number of news articles were taken from Kallxo.com as a credible source of news present in the dataset.","2818b3f2":"Furthermore, satirical articles on Kungulli.com tend to be lengthier in terms of word count when compared to non-satirical articles.","c0f72c00":"Another intriguing discovery is that the top three performing algorithms have the lowest training time, resulting in the most time and performance effective possibilities for solutions.","186c43aa":"Finally, the three best performing pipelines, as well as their respective testing accuracy, are represented below:","bd85ee8b":"According to the explanations, terms like `partine`, `kryeministri`, and `albin` had a large influence on satirical prediction, which matches with the Kungulli.com wordcloud from our EDA section, where they were the most common words in those sort of articles. On the other hand, it's worth noting that terms like `thuhet` and `jepet` have a strong Not Satirical impact, which makes sense logically because they're words you use to refer to another source or someone else.","30df3dae":"We will provide a LIME explanation for both a non-satirical and satirical piece below:","0666713c":"## Supervised Machine Learning Prediction","c0b4b302":"When we look at classifiers individually, we can quickly identify the top performing algorithms who are most suited for this type of task. More precisely:\n\n1. LinearSVC\n2. LogisiticRegression\n3. MultinomialNB\n4. RandomForestClassifier","b2f5c1ad":"## Conclusion\n\nWe were able to assess several classifiers, scaling methods, and text preprocessing approaches in this notebook to discover the best performing pipeline for tackling satirical news detection tasks in Albanian, which, to the best of my knowledge, has never been done previously. This study sets the stage for future research in this area, as well as potential extensions to automated fake news detection in Albanian, if the correct data is found.","c35418e8":"### Explaining our predictions\n\nHigh test set accuracy is nice and all, but how does our model categorize the articles it sees? We may use the LIME package for this, which is a project that explains what machine learning classifiers (or models) do. We can assess the relevance of different words for our model's prediction using LIME. This helps us to obtain a better understanding of the categorization process and determine which terms have the greatest influence on our model.\n\nBelow we've trained a `preprocessed_content_without_stopwords-NoneType-LogisiticRegression` pipeline and used it with LIME to explain testing news articles to us.","38c52417":"Let us begin by comparing our article content preprocessing approaches in the Figure below, where we can see the average test set accuracies for each. According to the figure, there appears to be a relationship between the quantity of preprocessing and the performance of the classifiers, with the largest amount of preprocessing resulting in the highest average performance.","1207f6d0":"## Load data"}}