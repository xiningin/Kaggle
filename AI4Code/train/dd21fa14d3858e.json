{"cell_type":{"172076a9":"code","67d5d57d":"code","118555f1":"code","7a45c446":"code","ff163591":"code","086a539d":"code","3e90cb44":"code","4d630e30":"code","a4cae97d":"code","e1152da1":"markdown","a2f233f7":"markdown","5346471c":"markdown","48d2bdbf":"markdown","3315d26a":"markdown","a2030af6":"markdown","a9b96753":"markdown"},"source":{"172076a9":"import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization\nfrom keras.optimizers import Adam","67d5d57d":"# Load csv file into numpy arrays\ndata = np.genfromtxt('..\/input\/winequalityred\/winequality-red.csv', delimiter=',', unpack=True, skip_header=1)\n\n# Slice input and output (assume quality >= 7 is good and labelled as 1, otherwise 0)\nX = data[:-1]\ny = np.array([data[-1] >= 7]).astype(int).reshape(len(data[-1]), 1)\n\n# Normalize the input\nX = StandardScaler().fit_transform(X)\n\n# Split the train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X.T, y, test_size=0.2, random_state=1)","118555f1":"# Construct neutral network\nmodel = Sequential([\n    Dense(16, input_dim=11, activation='relu', kernel_initializer='he_uniform'),\n    Dense(32, activation='relu'),\n    Dense(32, activation='relu'),\n    BatchNormalization(),\n    Dense(1, activation='sigmoid'),\n])","7a45c446":"# Set Adam optimizer parameters\nopt_adam = Adam(learning_rate=0.012, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0)\n\n# Compile the model\nmodel.compile(optimizer=opt_adam, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Run the model\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500,\n                    verbose=0,\n                    batch_size=32)","ff163591":"# Evaluate prediction accuracy\n_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n_, test_acc = model.evaluate(X_test, y_test, verbose=0)\nprint('Train accuracy: %.3f, Test accuracy: %.3f' % (train_acc, test_acc))","086a539d":"# Plot loss during training\nplt.subplot(2, 1, 1)\nplt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n\n# Plot accuracy during training\nplt.subplot(2, 1, 2)\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()\n","3e90cb44":"from sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier","4d630e30":"for lr in np.arange(0, 1, 0.1):\n    model = XGBClassifier(learning_rate=lr, n_estimators=100)\n\n    eval_set = [(X_train, y_train), (X_test, y_test)]\n    eval_metric = [\"auc\", \"error\"]\n    result = model.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set, verbose=False)\n\n    # print(result.evals_result()['validation_1']['error'])\n\n    # make predictions for test data\n    y_pred = model.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n\n    # evaluate predictions\n    accuracy = accuracy_score(predictions, y_test)\n    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0), 'learning rate: %.4f' % lr)","a4cae97d":"plt.plot(result.evals_result()['validation_1']['error'])\nplt.show()","e1152da1":"Clearly learning rate of 0.2 can achieve the highest accuracy, at about 93%. ","a2f233f7":"I have tuned the learning rate to 0.12 after several hours' testing on my local computer, using random search. \n\nTip: use np.random.uniform(0.1, 0.15, 50) in a for loop to find an optimal learning rate. ","5346471c":"This is a simple and short dataset with about 1,600 entries. Let's see how Neural Network works in the first place. ","48d2bdbf":"In this notebook, I will only set a train set and a test set, with 80\/20 split. \nI also assume that red wine is deemed as \"good quality\" if its quality score is at least 7.0","3315d26a":"Still not perfect, but good enough. I will end my model selection here. \n\nThere are many other kinds of models worth trying, such as random forest, lightGBM, etc, and I will visit them in future learning. ","a2030af6":"Although the test accuracy exceeds 90%, the graph does not look quite good. It shows unpleasant oscillation during training, especially at the start of running. \n\nI have tried to explore the actual cause of this fluctation, and unfortunately a lower learning rate does not seem to help :(. I appreciate if I can get some tips at the comments below!\n\nPerhaps a simple NN is not the best solution to fit this data set. Now I will try XGBoost.","a9b96753":"After reading data into numpy arrays, let's build a two-hidden-layer simple NN.\nI put a batch normalization before the output layer to avoid weight matrix explosion. "}}