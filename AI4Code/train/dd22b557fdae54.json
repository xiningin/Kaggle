{"cell_type":{"6a498a36":"code","24a4c288":"code","6482621d":"code","b4ef126b":"code","12b98f31":"code","65ff6d4c":"code","fb99ffbe":"code","436d15e6":"code","4633209e":"markdown","63bbc101":"markdown","64c0a2ed":"markdown","9abde691":"markdown","47ceec6f":"markdown","15b5cad2":"markdown","b0c80e25":"markdown","41880565":"markdown"},"source":{"6a498a36":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\n\n%matplotlib inline","24a4c288":"# Returns sigmoid of a numpy array\ndef sigmoid(x):\n    return 1\/ (1 + np.exp(-x))\n\n# Returns the derivative of sigmoid of the array wrt an array\ndef d_sigmoid(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\n# Returns tanh of a numpy array\ndef tanh(x):\n    return (np.exp(x) - np.exp(-x))\/(np.exp(x) + np.exp(-x))\n\n# Returns the derivative of tanh of the array wrt an array\ndef d_tanh(x):\n    return 1 - np.power(tanh(x), 2)\n\n# Cost function J:\ndef J(A, Y):\n#     print((Y @ np.log(A).T)[0][0], (np.log(1 - A).T)[0][0] , A.shape[1])\n    return -1 * ((Y @ np.log(A).T)[0][0] + ((1 - Y) @ np.log(1 - A).T)[0][0] ) \/ A.shape[1]\n\n# Initialise the parameters and variables of the neural network:\ndef init_layers(neural_network, X):\n    np.random.seed(2) \n    A = {0: X}\n    Z = {}\n    m = X.shape[1]\n    \n    if neural_network['W'] == None:\n        W = {}\n    else:\n        W = neural_network['W']\n    \n    if neural_network['B'] == None:\n        B = {}\n    else:\n        B = neural_network['B']\n    \n    for curr in range(1, L + 1):\n        if 'W' not in neural_network or neural_network['W'] == None:\n            W[curr] = np.random.randn(n[curr], n[curr - 1]) * 0.1\n        if 'B' not in neural_network or neural_network['B'] == None:\n            B[curr] = np.random.randn(n[curr], 1) * 0.1\n        Z[curr] = np.zeros((n[curr], m))\n        A[curr] = np.zeros((n[curr], m))\n    neural_network['W'] = W\n    neural_network['B'] = B\n    return A, Z\n \n# Forward Propagation through all the layers of the neural network:\ndef forward_propagation(neural_network, A, Z):\n    L = neural_network['L']\n    activation_fn = neural_network['activation_fn']\n    W = neural_network['W']\n    B = neural_network['B']\n\n    for curr in range(1, L + 1):\n        Z[curr] = W[curr] @ A[curr - 1] + B[curr]\n        A[curr] = activation_fn[curr](Z[curr])\n\n        \n# Backward Propagation through all the layers of the neural network:\ndef backward_propagation(neural_network, X, Y, A, Z):\n    L = neural_network['L']\n    activation_fn = neural_network['activation_fn']\n    d_activation_fn = neural_network['d_activation_fn']\n    W = neural_network['W']\n    B = neural_network['B']\n    n = neural_network['n']\n    m = X.shape[1]\n    \n    for curr in range(L, 0, -1):\n        if curr == L:\n            dZ = ((A[L] - Y)\/(A[L]*(1-A[L]))) * d_activation_fn[curr](Z[curr])\n        else:\n            dZ = (W[curr + 1].T @ dZ) * d_activation_fn[curr](Z[curr])\n        \n        dB = np.sum(dZ, axis = 1, keepdims = True)\/X.shape[1]\n        dW = dZ @ A[curr - 1].T \/ m\n        B[curr] -= alpha * dB\n        W[curr] -= alpha * dW\n\n    neural_network['W'] = W\n    neural_network['B'] = B        \n    \n\n# Assigns the class of the prediction from its sigma value:\ndef predict_class(y):\n    if y < 0.5:\n        return 0\n    else:\n        return 1\n# Vectorize the function so that it works on the whole numpy array\npredict_class = np.vectorize(predict_class)\n\n# Predict the output given the test values:\ndef predict(neural_network, X):\n    X = X.T\n    L = neural_network['L']\n    W = neural_network['W']\n    B = neural_network['B']\n    activation_fn = neural_network['activation_fn']\n\n    for curr in range(1, L + 1):\n        Z = W[curr] @ X + B[curr]\n        X = activation_fn[curr](Z)\n    \n    return predict_class(X).T\n\n# Compare predictions to actual and find accuracy score:\ndef accuracy_score(A, Y):\n    A = predict_class(A)\n    return (A.T @ Y + (1 - A).T @ (1 - Y))[0][0]\/Y.shape[0]\n\n# Trains the network on the training set \ndef fit(neural_network, X, Y, iterations = 1000, alpha = 0.01, Verbose = False):\n    X = X.T\n    Y = Y.T\n    \n    assert len(n) == L + 1\n    assert len(activation_fn) == L\n    assert len(d_activation_fn) == L\n    assert n[0] == X.shape[0]\n    assert n[L] == Y.shape[0]\n    \n    A, Z = init_layers(neural_network, X)\n\n    cost = []\n    accuracy = []\n    for i in range(iterations):\n        forward_propagation(neural_network, A, Z)\n        backward_propagation(neural_network, X, Y, A, Z)\n        cost.append(J(A[L], Y))\n        accuracy.append(accuracy_score(A[L].T, Y.T))\n\n    if Verbose:\n        plt.title('Cost Function')\n        plt.plot(cost)\n        plt.show()\n\n        plt.title('Training Accuracy')\n        plt.plot(accuracy)\n        plt.show()\n\n    return accuracy[-1]\n\n","6482621d":"# Copied from Coursera Deep Learning assignment utils\ndef plot_decision_boundary(model, X, y, title):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    Z = model(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.title(title)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.scatter(X[:, 0], X[:, 1], c=y.reshape((y.shape[0],)).tolist(), cmap=plt.cm.Spectral)\n    plt.show()\n\n","b4ef126b":"# Create DataSet; Copied from Coursera Deep Learning assignment utils\ndef load_planar_dataset():\n    np.random.seed(2)\n    m = 500 # number of examples\n    N = int(m\/2) # number of points per class\n    D = 2 # dimensionality\n    X = np.zeros((m,D)) # data matrix where each row is a single example\n    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n    a = 4 # maximum ray of the flower\n\n    for j in range(2):\n        ix = range(N*j,N*(j+1))\n        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n        Y[ix] = j\n    X, X_test, Y, Y_test = train_test_split(X, Y, test_size = 0.2,random_state = 2)\n\n    return X, X_test, Y, Y_test\n                # OR\n# Load existing DataSet\ndef get_data(train_file, test_file = None):\n    if test_file == None:\n        frame = pd.read_csv(train_file)\n        data = frame.values\n        np.random.shuffle(data)\n        return data\n    else:\n        train_frame = pd.read_csv(train_file)\n        test_frame = pd.read_csv(test_file, error_bad_lines=False, quoting = 2)\n\n        train_data = train_frame.values\n        test_data = test_frame.values\n        np.random.shuffle(train_data)\n        np.random.shuffle(test_data)\n\n        return train_data, test_data\n\ndef get_training_testing_sets(train_file, test_file = None):\n    if test_file == None:\n        data = get_data(train_file)\n        train_data, test_data = train_test_split(data)\n    else:\n\n        train_data, test_data = get_data(train_file, test_file)\n\n    X_train = train_data[:, 1:]\n    Y_train = train_data[:, :1]\n    X_test = test_data[:, 1:]\n    Y_test = test_data[:, :1]\n    Y_train = np.where(Y_train == -1, 0, Y_train)\n    Y_test = np.where(Y_test == -1, 0, Y_test)\n    print(X_train.shape, X_test.shape)\n    \n    return X_train, Y_train, X_test, Y_test\n","12b98f31":"# Creating or Loading the Data:\nX, Y, X_test, Y_test =  get_training_testing_sets('..\/input\/data1.csv')\n\n# Viewing the data\nplt.scatter(X[:, 0], X[:, 1], c = Y.reshape(Y.shape[0],).tolist(), cmap=plt.cm.Spectral);\nplt.show()\nprint(X.shape)\nprint(Y.shape)\n\n# Sanity Check:\nprint('training sample:', X[0], Y[0])\nprint('testing sample: ',X_test[0], Y_test[0])","65ff6d4c":"# Fitting Logistic Regression over the data to see the decision boundary (Just for observation):\nclf = LogisticRegression()\nclf.fit(X, Y)\n\n\nplot_decision_boundary(lambda x: clf.predict(x), X, Y, 'Decision Boundary Using Logistic Regression')\n\n","fb99ffbe":"    \n# Defining the dimensions of the neural network and constants of gradient descent\nL = 2\nn = {0: 2, 1: 4, 2: 1}\nactivation_fn = {1: tanh, 2: sigmoid}\nd_activation_fn = {1: d_tanh, 2: d_sigmoid }\nalpha = 0.02\niterations = 10000\n\n# Creating the neural network\nneural_network = {'L': L, 'n': n, 'W': None, 'B': None, 'activation_fn': activation_fn, 'd_activation_fn': d_activation_fn}\n\n# Training the network:\ntraining_accuracy = fit(neural_network, X, Y, iterations, alpha, Verbose = True)\nplot_decision_boundary(lambda x: predict(neural_network, x), X, Y, 'Decision Boundary using Neural Networks - DataSet 2)')\n\nprint('training_accuracy:', training_accuracy)\n\n# Testing the network on Testing Data\nY_predicted = predict(neural_network, X_test)\ntesting_accuracy = accuracy_score(Y_predicted, Y_test)\nprint('testing_accuracy: ',testing_accuracy)","436d15e6":"# Loading the Data:\n\n# Linearly seperable dataset:\n# X, Y, X_test, Y_test =  get_training_testing_sets('..\/input\/data0.csv')\n# Non Linear data sets:\nX, X_test, Y, Y_test = load_planar_dataset()\n# X, Y, X_test, Y_test =  get_training_testing_sets('..\/input\/data1.csv')\n\n# Viewing the data\nplt.scatter(X[:, 0], X[:, 1], c = Y.reshape(Y.shape[0],).tolist(), cmap=plt.cm.Spectral);\nplt.show()\nprint(X.shape)\nprint(Y.shape)\n\n# Sanity Check:\nprint('training sample:', X[0], Y[0])\nprint('testing sample: ',X_test[0], Y_test[0])\n\n# Fitting Logistic Regression over the data to see the decision boundary (Just for observation):\n\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(X, Y)\n\n\nplot_decision_boundary(lambda x: clf.predict(x), X, Y, 'Decision Boundary Using Logistic Regression')\n\n\n\n\n# Defining the dimensions of the neural network and constants of gradient descent\nL = 2\nn = {0: 2, 1: 4, 2: 1}\nactivation_fn = {1: tanh, 2: sigmoid}\nd_activation_fn = {1: d_tanh, 2: d_sigmoid }\nalpha = 0.02\niterations = 10000\n\n# Creating the neural network\nneural_network = {'L': L, 'n': n, 'W': None, 'B': None, 'activation_fn': activation_fn, 'd_activation_fn': d_activation_fn}\n\n# Training the network:\ntraining_accuracy = fit(neural_network, X, Y, iterations, alpha, Verbose = True)\nplot_decision_boundary(lambda x: predict(neural_network, x), X, Y, 'Decision Boundary using Neural Netowrks - DataSet 2')\n\nprint('training_accuracy:', training_accuracy)\n\n# Testing the network on Testing Data\nY_predicted = predict(neural_network, X_test)\ntesting_accuracy = accuracy_score(Y_predicted, Y_test)\nprint('testing_accuracy: ',testing_accuracy)","4633209e":"**Helper function to plot the decision boundary:**","63bbc101":"**Fitting Logistic Regression over the data to see the decision boundary**","64c0a2ed":"**Creating the neural network and fitting it**","9abde691":"**Trying it out on another data set:**","47ceec6f":"**Coding out the neural network and associated functions:**","15b5cad2":"**Notation:**\n\n$A[0] = X.T$      \n\n$A[i]$ is the output of layer $i - 1$ and input to layer $i$ with dimension $(n[i], m)$ where $n[i]$ is the number of nodes or features in layer $i$ and $m$ is the number of training examples.\n\n$W[i]$ and $B[i]$ map $A[i - 1]$ to $Z[i]$. $W[i]$ and $B[i]$ have dimensions $(n[i], n[i - 1])$ and $(n[i], 1)$ respectively. $Z[i]$ has dimensions of $A[i]$ \n\n$A[i] = activation\\_function(Z[i])$\n\n$J$ denotes the cost function.\n\n$$J = \\frac{-1}{m} (Y * logA[L] + (1 - Y) * log(1 - A[L]))$$\n\n$*$ denotes Hadamard Product and $@$ denotes dot product\n\n$$Z[curr] = W[curr] @ A[curr - 1] + B[curr]$$\n\n$$A[curr] = activation\\_fn[curr](Z[curr])$$\n\n**Derivative of Cost function wrt Z:**\n\n$$\\frac {dJ}{dZ[curr]}$$\n\n$$    = \\frac {dJ}{dZ[curr + 1]} * \\frac {dZ[curr + 1]}{dZ[curr]}$$\n\n$$    = \\frac {dJ}{dZ[curr + 1]} * \\frac {dZ[curr + 1]}{dA[curr]} * \\frac {dA[curr]}{dZ[curr]}$$\n\n$$    = \\frac {dJ}{dZ[curr + 1]} * W[curr + 1].T * \\frac{d(activation\\_fn(Z))}{dZ}$$\n\n$$    = W[curr + 1].T * \\frac {dJ}{dZ[curr + 1]} * \\frac{d(activation\\_fn(Z))}{dZ} $$\n\nindex of last layer = number of layers = $L$\n$$\\frac {dJ}{dZ[L]}$$ \n\n$$    = \\frac {dJ}{dA[L]} * \\frac {dA[L]}{dZ[L]}$$\n\n$$    = \\frac {d( - (Y * logA[L] + (1 - Y) * log(1 - A[L])))}{dA[L]} * \\frac{d activation_fn[curr](Z)}{dZ} $$\n\n$$    = (- \\frac{Y}{A} + \\frac{(1 - Y)}{(1 - A)}) * \\frac{d(activation\\_fn(Z))}{dZ}$$\n\n\n$$    = \\frac{A - Y}{A * (1 - A)} * \\frac{d(activation\\_fn(Z))}{dZ}$$\n\n**Derivative of the cost function wrt the parameter W:**\n\n$$\\frac {dJ}{dW}$$ \n\n$$    = \\frac {dJ}{dZ} @ \\frac {dZ}{dW}$$\n\n$$    = \\frac {dJ}{dZ} @ A[curr - 1].T$$\n\n**Derivative of the cost function wrt the parameter B:**\n\n$$\\frac {dJ}{dB}$$\n\n$$    = \\frac {dJ}{dZ} * \\frac {dZ}{dB} $$\n\n$$    = \\frac {dJ}{dZ} * 1$$\n\n$$    = \\frac {dJ}{dZ} $$\n\n\n**Gradient Descent on W and B follow the formula**\n\n$$\\theta = \\theta - \\alpha * \\frac{dJ}{d\\theta}$$","b0c80e25":"**Layout of the Notebook:**      \nA brief about the notation used and a small deerivation of the basic formulae followed by the code:\n1. Importing numpy, matplotlib.pyplot pandas, sklearn, etc.    \n1. Coded out the neural network in the first block (Start reading from fit() for better understanding of how this code works)       \n1. Written the helper function for plotting decision boundary of the model\n1. Written the helper functions to create or load the data (One dataset has been loade while the other has been created)\n1. Loading the data\n1. Fitting logistic regression over the data to observe linear decision boundary\n1. Fitting the neural network, testing the accuracy on testing data, plotting the decision boundary\n1. Loading another set of data and doing step 5 and 6 on it","41880565":"**Helper functions to load the datasets:**"}}