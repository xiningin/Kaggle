{"cell_type":{"3d652fbb":"code","20e0b08f":"code","0bfc7768":"code","febb499d":"code","a94f5153":"code","f952f8f6":"code","043d75b9":"code","243103f9":"code","f4091ac0":"code","7c5a367c":"code","a477e7e9":"code","5dc8c1d4":"code","06a6dcc5":"code","0af15bbb":"code","a354216c":"code","7652a57f":"code","01d90570":"code","b5521db0":"code","38bff95d":"code","38954c31":"code","251b3536":"code","d2226b13":"code","708e15a1":"code","22242c53":"code","a198111a":"code","cea79615":"code","9f4a4eae":"code","f645e077":"code","5d6c44d8":"code","e0e77ca0":"code","bd9339d0":"code","9f82f2bd":"code","0573ca6a":"code","90d6614e":"code","bb4ac0c0":"code","119b5a90":"code","5d79b7f9":"code","c91d2c28":"code","203edce4":"code","0ae7c84e":"code","160fa91b":"code","ae2a7f25":"code","5a79d8e3":"code","88dc4f5f":"code","d16a8dac":"code","ceab9db1":"code","e69e305c":"markdown","9359f077":"markdown","e767c5f8":"markdown","40cfec0c":"markdown","d6d15b1e":"markdown","19673a0a":"markdown","abb40909":"markdown","26fcc5ab":"markdown","6ec1aa47":"markdown","1960eff2":"markdown"},"source":{"3d652fbb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import models, layers\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow.keras import callbacks","20e0b08f":"print(\"Is GPU available? \",tf.test.is_gpu_available)\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","0bfc7768":"df = pd.read_csv(\"..\/input\/stock-time-series-20050101-to-20171231\/AAPL_2006-01-01_to_2018-01-01.csv\" , infer_datetime_format = True)","febb499d":"np.round(df.describe(),2)","a94f5153":"df.head()","f952f8f6":"df.info()","043d75b9":"df['Date']= pd.to_datetime(df['Date'])","243103f9":"df.Date.value_counts()","f4091ac0":"df[df.duplicated(['Date'], keep=False)]","7c5a367c":"df.isna().sum()","a477e7e9":"df.Open.plot()","5dc8c1d4":"scaler = MinMaxScaler()","06a6dcc5":"def return_train_val_test(dataset, col_name, test_days, time_steps): \n    '''\n    returns X_train, y_train, X_val, y_val, X_test, y_test\n    \n    *********\n    arguments\n    *********\n    test_days (int) : this will determine the amount of days reserved for the val test\n    with the remainder of it going to training set.\n    \n    time_steps (int) : the amount of time interval in each batch of the mini time-series \n    \n    '''\n    train_set = dataset[col_name][:-test_days].values.reshape(-1,1)\n    val_set = dataset[col_name][len(train_set):len(train_set)+int(test_days\/2)].values.reshape(-1,1)\n    test_set = dataset[col_name][-int(test_days\/2):].values.reshape(-1,1)\n    \n    #scales data\n    train_set = scaler.fit_transform(train_set)\n    val_set = scaler.transform(val_set)\n    test_set = scaler.transform(test_set)\n    \n    X_train = []\n    X_test = []\n    X_val = []\n    y_val = []\n    y_train = []\n    y_test = []\n    \n    for i in range(len(train_set)-time_steps):\n        sample_X_train = train_set[i:i+time_steps]\n        sample_y_train = train_set[i+time_steps]\n        \n        X_train.append(sample_X_train)\n        y_train.append(sample_y_train)\n    \n    for i in range(len(val_set) - time_steps):\n        sample_X_val = val_set[i:i+time_steps]\n        sample_y_val = val_set[i+time_steps]\n        \n        X_val.append(sample_X_val)\n        y_val.append(sample_y_val)\n    \n    X_test = test_set[:time_steps]\n    y_test = test_set[time_steps:]\n    \n    X_train = np.array(X_train)\n    X_test = np.array(X_test)\n    X_val = np.array(X_val)\n    y_val = np.array(y_val)\n    y_train = np.array(y_train)\n    y_test = np.array(y_test)\n    \n    X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n    X_val = np.reshape(X_val,(X_val.shape[0],X_val.shape[1],1))\n    \n    return X_train, y_train, X_val, y_val, X_test, y_test","0af15bbb":"def return_rnn_model(X_train, y_train, X_val, y_val, cells, epochs):\n    \n    '''\n    returns a rnn model with history\n    '''\n    \n    model = models.Sequential()\n    model.add(layers.SimpleRNN(cells, input_shape=(X_train.shape[1],1)))\n    model.add(layers.Dense(1))\n    \n    model.compile(optimizer = 'rmsprop', loss = 'mean_squared_error')\n    \n    early_callback = callbacks.EarlyStopping(patience = 50)\n    \n    history = model.fit(X_train, y_train, epochs = epochs, validation_data=(X_val, y_val), callbacks = [early_callback])\n        \n    return model , history","a354216c":"def predict_timeseries(X_test, model, predict_days):\n    \n    '''\n    returns the prediction\n    '''\n    \n    predictions = []\n    \n    X_test_copy = np.reshape(X_test.copy(),(1,-1,1))\n    \n    for _ in range(predict_days):\n        pred = model.predict(X_test_copy)\n        predictions.append(pred)\n        \n        X_test_copy[:,:-1,:] = X_test_copy[:,1:,:]\n        X_test_copy[:,-1,:] = pred\n    \n    return predictions","7652a57f":"def plot_train_val(history):\n    '''\n    plots the train validation \n    '''\n    train_loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    plt.plot(range(len(train_loss)), train_loss, 'y', label = 'Train Loss')\n    plt.plot(range(len(train_loss)), val_loss, 'b', label = 'Validation Loss')\n    plt.title(\"Training vs Validation Loss\")\n    plt.legend()\n    plt.show","01d90570":"def plot_predictions(X_test, y_test, model):\n    \n    '''\n    plots predictions\n    '''\n    \n    y_pred = predict_timeseries(X_test, model, len(y_test))\n\n    plt.plot(range(len(X_test)), X_test, label = 'training values')\n    plt.plot(range(len(X_test), len(X_test)+len(y_test)), y_test, 'b--', label = 'actual values')\n    plt.plot(range(len(X_test), len(X_test)+len(y_test)), np.array(y_pred).reshape(-1),'r+', label = 'predicted values')\n    plt.legend()\n    plt.grid(True)\n    plt.show()","b5521db0":"len(df)","38bff95d":"test_days = int(len(df)*(1\/10))\ntime_steps = int(test_days * (3\/10))\n\nprint('test_days is \\t{}'.format(test_days))\nprint('time_steps is \\t{}'.format(time_steps))\n\nX_train, y_train, X_val, y_val, X_test, y_test = return_train_val_test(dataset=df, \n                                                                       col_name='Open',\n                                                                       test_days = test_days, \n                                                                       time_steps = time_steps)","38954c31":"print('X_train shape is {}'.format(X_train.shape))\nprint('X_test shape is {}'.format(X_test.shape))\nprint('X_val shape is {}'.format(X_val.shape))\nprint('y_val shape is {}'.format(y_val.shape))\nprint('y_train shape is {}'.format(y_train.shape))\nprint('y_test shape is {}'.format(y_test.shape))","251b3536":"simple_rnn , simple_rnn_history = return_rnn_model(X_train, \n                                                   y_train,\n                                                   X_val,\n                                                   y_val,\n                                                   cells= 1,\n                                                   epochs=10)","d2226b13":"plot_predictions(X_test,\n                 y_test,\n                 model=simple_rnn)","708e15a1":"plot_train_val(simple_rnn_history)","22242c53":"more_complex_rnn_A ,more_complex_rnn_A_history = return_rnn_model(X_train,y_train, X_val, y_val,50,100)","a198111a":"plot_predictions(X_test,\n                 y_test, \n                 model = more_complex_rnn_A)","cea79615":"plot_train_val(more_complex_rnn_A_history)","9f4a4eae":"#Test days are reduced and time steps are much smaller since it is proportionate to test days\n\ntest_days = int(len(df)*(1\/20))\ntime_steps = int(test_days * (1\/10))\n\nprint('test_days is \\t{}'.format(test_days))\nprint('time_steps is \\t{}'.format(time_steps))\n\n\nX_train, y_train,X_val, y_val,X_test, y_test = return_train_val_test(dataset=df, \n                                                                    col_name='Open',\n                                                                    test_days = test_days, \n                                                                    time_steps = time_steps)\n\nmore_complex_rnn_B , more_complex_rnn_B_history = return_rnn_model(X_train,y_train,X_val, y_val,50,100)","f645e077":"plot_predictions(X_test,\n                 y_test, \n                 model = more_complex_rnn_B)","5d6c44d8":"plot_train_val(more_complex_rnn_B_history)","e0e77ca0":"#Test days and time steps are further reduced\n\ntest_days = int(len(df)*(1\/35))\ntime_steps = int(test_days * (3\/10))\n\nprint('test_days is \\t{}'.format(test_days))\nprint('time_steps is \\t{}'.format(time_steps))\n\n\nX_train, y_train,X_val, y_val,X_test, y_test = return_train_val_test(dataset=df, \n                                                                    col_name='Open',\n                                                                    test_days = test_days, \n                                                                    time_steps = time_steps)\n\nmore_complex_rnn_C, more_complex_rnn_C_history = return_rnn_model(X_train,y_train,X_val, y_val,50,100)","bd9339d0":"plot_predictions(X_test,\n                 y_test, \n                 model = more_complex_rnn_C)","9f82f2bd":"plot_train_val(more_complex_rnn_C_history)","0573ca6a":"def return_lstm_model(X_train, y_train ,X_val, y_val,cells,epochs):\n    model = models.Sequential()\n    model.add(layers.LSTM(cells, input_shape=(X_train.shape[1],1)))\n    model.add(layers.Dense(1))\n    \n    model.compile(optimizer = 'rmsprop', loss = 'mean_squared_error')\n    \n    early_callback = callbacks.EarlyStopping(patience = 50)\n    \n    history = model.fit(X_train, y_train, epochs = epochs, validation_data=(X_val, y_val), callbacks = [early_callback])\n        \n    return model , history   ","90d6614e":"print('test_days is \\t{}'.format(test_days))\nprint('time_steps is \\t{}'.format(time_steps))\n\nlstm_model ,lstm_model_history = return_lstm_model(X_train,y_train,X_val, y_val,1,10)","bb4ac0c0":"plot_predictions(X_test, y_test, lstm_model)","119b5a90":"plot_train_val(lstm_model_history)","5d79b7f9":"print('test_days is \\t{}'.format(test_days))\nprint('time_steps is \\t{}'.format(time_steps))\n\nlstm_model, lstm_model_history = return_lstm_model(X_train,y_train,X_val, y_val,50,500)","c91d2c28":"plot_predictions(X_test, y_test, lstm_model)","203edce4":"plot_train_val(lstm_model_history)","0ae7c84e":"def return_gru_model(X_train, y_train,X_val, y_val,cells,epochs):\n    model = models.Sequential()\n    model.add(layers.LSTM(cells, input_shape=(X_train.shape[1],1)))\n    model.add(layers.Dense(1))\n    \n    model.compile(optimizer = 'rmsprop', loss = 'mean_squared_error')\n    \n    early_callback = callbacks.EarlyStopping(patience = 50)\n    \n    history = model.fit(X_train, y_train, epochs = epochs, validation_data=(X_val, y_val), callbacks = [early_callback])\n        \n    return model , history","160fa91b":"print('test_days is \\t{}'.format(test_days))\nprint('time_steps is \\t{}'.format(time_steps))\n\ngru_model , gru_model_history = return_gru_model(X_train,y_train,X_val, y_val,1,10)","ae2a7f25":"plot_predictions(X_test, y_test, gru_model)","5a79d8e3":"plot_train_val(gru_model_history)","88dc4f5f":"print('test_days is \\t{}'.format(test_days))\nprint('time_steps is \\t{}'.format(time_steps))\n\ngru_model , gru_model_history = return_gru_model(X_train,y_train,X_val, y_val,50,500)","d16a8dac":"plot_predictions(X_test, y_test, gru_model)","ceab9db1":"plot_train_val(gru_model_history)","e69e305c":"Next we define a bunch of helper functions in order to make my life easier.\n\nFirst we will need to have the function to provide the train test validation split. This will be less straight forward as we are doing time series and as there is no available tool we can use in sklearn, we will need to conjure up a function on our own\n\nSecondly we will need to have a function to allow us to predict. Once again since this is time series and the conventional way of doing prediction is not applicable here and we will be customising our own.\n\nWe would also need a function to create a model for us instead of re-creating again and again. This allows us to customise aspects of the model.\n\nLastly our prediction can be plotted out so that we are able to visualise the prediction against the original values.","9359f077":"# Modelling Time","e767c5f8":"## LSTM model","40cfec0c":"We will now attempt to construct another model using GRU. Hopefully this will provide us better results.","d6d15b1e":"## GRU model","19673a0a":"We will now attempt to construct another model using LSTM. Hopefully this will provide us better results.","abb40909":"# Introduction\n\nIn this notebook, I try my hand at time series.\n\nI will only use one stock, APPLE using only one variable to predict stock prices.","26fcc5ab":"There are no missing or duplicated values hence there is no need for imputing or cleaning","6ec1aa47":"Lets use the open figure as the target variable and ignore the rest for the sake of simplicity","1960eff2":"## RNN Model"}}