{"cell_type":{"438b68d2":"code","3c88fe54":"code","cf24b9c3":"code","d2d3f63c":"code","8192a414":"code","c4ada5be":"code","4461849b":"code","dfe0d1a0":"code","23085c9a":"code","ac931f73":"code","a7f0f959":"code","90930452":"code","536a3265":"code","ec5f7961":"code","a19f0f97":"code","46e7e708":"code","d86a0ef2":"code","89d25e05":"code","700c33e1":"code","cb5a1a94":"code","741796dc":"code","3d57762b":"code","97663dda":"code","cb019f36":"code","8491423a":"code","2a091f92":"code","7e3d08b8":"markdown","e1eec3c6":"markdown","53d8d169":"markdown","8a68e613":"markdown","1409164a":"markdown","bffa7768":"markdown","18f8ef7e":"markdown","7e129c2a":"markdown","09eddabb":"markdown","7cd99efb":"markdown","fc03d7bc":"markdown","8804e740":"markdown"},"source":{"438b68d2":"import numpy as np                               # linear algebra\nimport pandas as pd                              # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport re                                        # to handle regular expressions\nfrom string import punctuation                   # to extract the puntuation symbols\n\nfrom nltk.tokenize import word_tokenize          # to divide strings into tokens\nfrom nltk.stem import WordNetLemmatizer          # to lemmatize the tokens\nfrom nltk.corpus import stopwords                # to remove the stopwords \n\nimport random                                    # for generating (pseudo-)random numbers\nimport matplotlib.pyplot as plt                  # to plot some visualizations\n\nimport tensorflow as tf            \nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.callbacks import EarlyStopping","3c88fe54":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nelse:\n    print('Found GPU at: {}'.format(device_name))","cf24b9c3":"train_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_data  = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","d2d3f63c":"train_data.info()","8192a414":"train_data.sample(3)","c4ada5be":"pd.options.display.max_colwidth = 300   # to show the whole tweets\n\nprint(\"Example of a real tweet: \\n\" +train_data[train_data['target']==1].sample()['text'].values[0])\nprint(\"*\"*100)\nprint(\"Example of a fake tweet: \\n\" +train_data[train_data['target']==0].sample()['text'].values[0])","4461849b":"# cleaning the text\n\ndef clean_text(text):\n    '''Make text lowercase, remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('[%s]' % re.escape(punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to both test and train datasets\ntrain_data['text'] = train_data['text'].apply(lambda x: clean_text(x))\ntest_data['text'] = test_data['text'].apply(lambda x: clean_text(x))","dfe0d1a0":"# tokenizing the text\n\ntrain_data['text'] = train_data['text'].apply(lambda x:word_tokenize(x))\ntest_data['text'] = test_data['text'].apply(lambda x:word_tokenize(x))","23085c9a":"# removing stopwords (defined in nltk.corpus.stopwords)\n\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words \n\ntrain_data['text'] = train_data['text'].apply(lambda x : remove_stopwords(x))\ntest_data['text'] = test_data['text'].apply(lambda x : remove_stopwords(x))","ac931f73":"# lemmatizing the text entries\n\ndef lemmatize_text(text):\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(w) for w in text]  ##Notice the use of text.\n\ntrain_data['text'] = train_data['text'].apply(lambda x : lemmatize_text(x))\ntest_data['text'] = test_data['text'].apply(lambda x : lemmatize_text(x))","a7f0f959":"# converting list of strings into joint strings\n\ndef concatenate_text(text):\n    return ' '.join(text)\n\ntrain_data['text'] = train_data['text'].apply(lambda x : concatenate_text(x))\ntest_data['text'] = test_data['text'].apply(lambda x : concatenate_text(x))","90930452":"train_data.sample(5)","536a3265":"path_to_glove_file = '\/kaggle\/input\/glove42b300dtxt\/glove.42B.300d.txt'\n\nembeddings_index = {}\n\nf = open(path_to_glove_file, 'r', encoding='utf8')\nfor line in f:\n    splitLine = line.split(' ')\n    word = splitLine[0]                                  # the first entry is the word\n    coefs = np.asarray(splitLine[1:], dtype='float32')   # these are the vectors representing word embeddings\n    embeddings_index[word] = coefs\nprint(\"Glove data loaded! In total:\",len(embeddings_index),\" words.\")","ec5f7961":"def train_val_split(df, validation_split):\n    \"\"\"\n    This function generates the training and validation splits from an input dataframe\n    \n    Parameters:\n        dataframe: pandas dataframe with columns \"text\" and \"target\" (binary)\n        validation_split: should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the validation split\n    \n    Returns:\n        train_samples: list of strings in the training dataset\n        val_samples: list of strings in the validation dataset\n        train_labels: list of labels (0 or 1) in the training dataset\n        val_labels: list of labels (0 or 1) in the validation dataset      \n    \"\"\"\n       \n    text = df['text'].values.tolist()                         # input text as list\n    targets = df['target'].values.tolist()                    # targets\n    \n#   Preparing the training\/validation datasets\n    \n    seed = random.randint(1,50)   # random integer in a range (1, 50)\n    rng = np.random.RandomState(seed)\n    rng.shuffle(text)\n    rng = np.random.RandomState(seed)\n    rng.shuffle(targets)\n\n    num_validation_samples = int(validation_split * len(text))\n\n    train_samples = text[:-num_validation_samples]\n    val_samples = text[-num_validation_samples:]\n    train_labels = targets[:-num_validation_samples]\n    val_labels = targets[-num_validation_samples:]\n    \n    print(f\"Total size of the dataset: {df.shape[0]}.\")\n    print(f\"Training dataset: {len(train_samples)}.\")\n    print(f\"Validation dataset: {len(val_samples)}.\")\n    \n    return train_samples, val_samples, train_labels, val_labels","a19f0f97":"train_samples, val_samples, train_labels, val_labels = train_val_split(train_data, 0.1)","46e7e708":"def make_embedding_matrix(train_samples, val_samples, embeddings_index):\n    \"\"\"\n    This function computes the embedding matrix that will be used in the embedding layer\n    \n    Parameters:\n        train_samples: list of strings in the training dataset\n        val_samples: list of strings in the validation dataset\n        embeddings_index: Python dictionary with word embeddings\n    \n    Returns:\n        embedding_matrix: embedding matrix with the dimensions (num_tokens, embedding_dim), where num_tokens is the vocabulary of the input data, and emdebbing_dim is the number of components in the GloVe vectors (can be 50,100,200,300)\n        vectorizer: TextVectorization layer      \n    \"\"\"\n    \n    vectorizer = TextVectorization(max_tokens=30000, output_sequence_length=50)\n    text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n    vectorizer.adapt(text_ds)\n    \n    voc = vectorizer.get_vocabulary()\n    word_index = dict(zip(voc, range(len(voc))))\n      \n    num_tokens = len(voc)\n    \n    hits = 0\n    misses = 0\n\n#   creating an embedding matrix\n    embedding_dim = len(embeddings_index['the'])\n    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # Words not found in embedding index will be all-zeros\n            embedding_matrix[i] = embedding_vector\n            hits += 1\n        else:\n            misses += 1\n#     print(\"Converted %d words (%d misses)\" % (hits, misses))\n    print(f\"Converted {hits} words ({misses} misses).\")\n\n    return embedding_matrix, vectorizer","d86a0ef2":"embedding_matrix, vectorizer = make_embedding_matrix(train_samples, val_samples, embeddings_index)","89d25e05":"def initialize_nn(embedding_matrix):\n    \"\"\"\n    This function initializes Keras model for binary text classification\n    \n    Parameters:\n        embedding matrix with the dimensions (num_tokens, embedding_dim), where num_tokens is the vocabulary size of the input data, and emdebbing_dim is the number of components in the GloVe vectors\n    \n    Returns:\n        model: Keras model    \n    \"\"\"\n    \n    num_tokens = embedding_matrix.shape[0]\n    embedding_dim = embedding_matrix.shape[1]\n    \n    embedding_layer = Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n    trainable=False,                # we are not going to train the embedding vectors\n    )\n    \n#   Here we define the architecture of the Keras model. \n    int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n    x = embedding_layer(int_sequences_input) \n    x = layers.Dropout(0.5)(x)\n    x = layers.LSTM(128,return_sequences=True)(x)\n    x = layers.Conv1D(128, 3, activation='relu')(x)\n    x = layers.GlobalMaxPooling1D()(x)\n    x = layers.Dense(64, activation=\"relu\")(x)\n    preds = layers.Dense(1, activation=\"sigmoid\")(x)\n    model = keras.Model(int_sequences_input, preds)\n    \n    return model","700c33e1":"initial_model = initialize_nn(embedding_matrix)\ninitial_model.summary()","cb5a1a94":"def train_nn(model, train_samples, val_samples, train_labels, val_labels, vectorizer, stop = False):\n    \"\"\"\n    This function fits the training data using validation data to calculate metrics.\n    \n    Parameters:\n        model: preinitialized Keras model\n        train_samples: list of strings in the training dataset\n        val_samples: list of strings in the validation dataset\n        train_labels: list of labels (0 or 1) in the training dataset\n        val_labels: list of labels (0 or 1) in the validation dataset\n        vectorizer: TextVectorization layer\n        stop (Boolean): flag for Early Stopping (aborting training when a monitored metric has stopped improving)\n    \n    Returns:\n        model: trained Keras model\n        history: callback that can be used to track the learning process\n    \"\"\"\n    \n    print('')\n    print(\"Training the model...\")\n    \n    model.compile(loss=\"binary_crossentropy\", \n              optimizer=\"adam\", \n              metrics=[\"acc\"])\n    \n    x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n    x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n    \n    y_train = np.asarray(train_labels).astype('float32').reshape((-1,1))\n    y_val = np.asarray(val_labels).astype('float32').reshape((-1,1))\n    \n    if stop:\n        early_stopping = EarlyStopping(monitor='val_loss', patience=1)\n        history = model.fit(x_train, y_train, batch_size=32, epochs=20, validation_data=(x_val, y_val), callbacks=[early_stopping], verbose=1)\n    else:\n        history = model.fit(x_train, y_train, batch_size=32, epochs=20, validation_data=(x_val, y_val), verbose=1)\n        \n    return model, history","741796dc":"model, history = train_nn(initial_model, train_samples, val_samples, train_labels, val_labels, vectorizer, stop=False)","3d57762b":"# here we define a function to plot the history of Keras model training\n\nplt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    fig = plt.figure(figsize=(12,5))\n    ax1 = fig.add_subplot(121)    \n    ax1.plot(x, acc, 'b', label='Training acc')\n    ax1.plot(x, val_acc, 'r', label='Validation acc')\n    ax1.set_title('Training and validation accuracy')\n    ax1.set_ylim(0,1)\n    ax1.legend()\n    \n    ax2 = fig.add_subplot(122)\n    ax2.plot(x, loss, 'b', label='Training loss')\n    ax2.plot(x, val_loss, 'r', label='Validation loss')\n    ax2.set_title('Training and validation loss')\n    ax2.legend()","97663dda":"plot_history(history)","cb019f36":"train_samples, val_samples, train_labels, val_labels = train_val_split(train_data, validation_split = 0.1)\n\nembedding_matrix, vectorizer = make_embedding_matrix(train_samples, val_samples, embeddings_index)\n\ninitial_model = initialize_nn(embedding_matrix)\n\nmodel, history = train_nn(initial_model, train_samples, val_samples, train_labels, val_labels, vectorizer, stop=True)","8491423a":"def suggest_nn(df, model):\n    \"\"\"\n    This function generates (binary) targets from a dataframe with column \"text\" using trained Keras model\n    \n    Parameters:\n        df: pandas dataframe with column \"text\"\n        model: Keras model (trained)\n    \n    Output:\n        predictions: list of suggested targets corresponding to string entries from the column \"text\"\n    \"\"\"\n    \n    string_input = keras.Input(shape=(1,), dtype=\"string\")\n    x = vectorizer(string_input)\n    preds = model(x)\n    end_to_end_model = keras.Model(string_input, preds)\n\n    probabilities = end_to_end_model.predict(df[\"text\"])\n    \n    predictions = [1 if i > 0.5 else 0 for i in probabilities]\n    \n    return predictions","2a091f92":"predictions = suggest_nn(test_data, model)\n\nsubmission_data = {\"ID\": test_data.id.tolist(), \"target\": predictions}\n\nsubmission_df = pd.DataFrame(submission_data)\n\nsubmission_df.to_csv('submission.csv', index=False)","7e3d08b8":"# Preprocessing input data\n### We are going to apply the following steps to both train and test data: text cleaning, tokenization, removing common words, lemmatization.","e1eec3c6":"# Importing required libraries","53d8d169":"### As one can see, the model's training accuracy approaches unity as epoch number increaces, while the validation accuracy after just a few epochs reaches some optimal values and then fluctuates at around 0.78. So, we observe overfitting and can apply \"early stopping\" to abort training when validation loss hits the minimum. This is done in the following cell, where all helper functions are brought together.","8a68e613":"### The following piece of code creates an embedding matrix (for more information, refer to https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html). In this case, the embedding matrix is N-by-300 matrix, where N is the total count of distinct words in the input dataframe. If the word is not found in the embeddings_index, then the corresponding column is all-zeroes.","1409164a":"# Loading the data","bffa7768":"### Finally, the last helper function is making predictions on the previously unsees dataset.","18f8ef7e":"# Making helper functions\n### Next, we are going to build some helper functions. The first one randomly divides the dataframe into two subsamples, for training and validation, respectively.","7e129c2a":"# Intro\n### This notebook describes the end-to-end solution of a binary text classification problem using the GloVe word embeddings and Keras library. The training dataset consists of disaster tweets labeled either 1 (Real) or 0 (Fake). The objective is to build a machine learning algorithm to predict whether a new tweet is about a real disaster or not.    \n![Presentation1.png](attachment:Presentation1.png)","09eddabb":"# Importing the GloVe embeddings\n\n### GloVe (stands for Global Vectors) is a word embedding technique that uses a word-word co-occurence statistics from a very large corpus. Individual components of the word vectors are not necessarily interpretable, however they can super efficient at getting the semantics of sentences.\n\n### You can download the GloVe embeddings from the official webpage (https:\/\/nlp.stanford.edu\/projects\/glove\/), or import them to your kernel by pressing \"Add data\" in the sidebar and searching for the required file. In this notebook, we are using Common Crawl (42B tokens, 1.9M vocab, 300 d vectors) pretrained vectors, but feel free to check other versions.","7cd99efb":"### The following function initializes the Keras model ready for training. The architecture of this neural network is the following: Input layer -> Embedding layer -> DropOut -> LSTM -> Conv1D -> GlobalMaxPooling1D -> Dense -> Output.","fc03d7bc":"### So, the training dataset contains 7613 rows. We are going to take into account only the column \"text\" each value of which has the corresponding label in the column \"target\". \n\n### Here are examples of tweets from two categories:","8804e740":"### We have loaded 1917494 words and their 300-dimensional representations. The dictionary embeddings_index contains all these relations. You can check yourself the loaded vector for any given word by calling embeddings_index[word]."}}