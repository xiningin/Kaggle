{"cell_type":{"e91b502b":"code","a65d33d7":"code","fb7d9889":"code","a9ef9591":"code","da99ee2b":"code","8c3abde1":"code","cfb44408":"code","30b646cd":"code","a3ddf57f":"code","3c1024c0":"code","0b6d3fcd":"code","50a98124":"code","ce7d56bd":"code","e97a553f":"code","a2617367":"code","f8607b82":"code","286d9f6d":"code","9bed75ac":"code","e6e032bb":"code","b03bcb48":"code","d69e0ecc":"code","950d61a4":"code","2dbf62c9":"code","238d4b0a":"code","79079cd6":"code","7372f953":"code","14a18f23":"code","2cce8fc2":"code","e5b5c110":"code","e273071a":"code","facfb396":"code","79d7db66":"code","c4a688ed":"code","9f98e045":"code","f22a7ec7":"code","6c516319":"code","7e21f76f":"code","4e0f5ce7":"code","4da8eca3":"code","1fc2dbb8":"code","9b5e518c":"code","10018dda":"markdown","07060717":"markdown","3b44bbfd":"markdown","2804895c":"markdown","118ef74a":"markdown","daa41693":"markdown","7d3c3843":"markdown","9372d72f":"markdown","bb001712":"markdown","f31c9b46":"markdown","35e48d63":"markdown","a8fe6b6e":"markdown","b328ee30":"markdown","a34a653f":"markdown","452445b2":"markdown","a00fadaa":"markdown","23cabde2":"markdown","a444128d":"markdown","2df68030":"markdown","af9a777c":"markdown","e5937cc4":"markdown","13d80352":"markdown","a16c115a":"markdown","260a263c":"markdown","909240b3":"markdown","5dd252da":"markdown","317a479a":"markdown","4de6a9a5":"markdown","0da55e63":"markdown","f55a3958":"markdown","f217dff4":"markdown","37d8b836":"markdown","cd77d2d2":"markdown","9988d507":"markdown","164e26fd":"markdown","33ad17ab":"markdown","f57f4bb7":"markdown","3cbb10a2":"markdown","78d35b34":"markdown"},"source":{"e91b502b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport gc\n#text preprocess\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nimport re\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.manifold import TSNE\nfrom scipy import sparse\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, log_loss, f1_score","a65d33d7":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain.head()","fb7d9889":"print(\"Number of data points in train data\", train.shape)\nprint('-'*50)\nprint(\"The attributes of data :\", train.columns.values)","a9ef9591":"y_value_counts = train['target'].value_counts()\nprint(\"Number of tweets that are real disaster \", y_value_counts[1], \", (\", (y_value_counts[1]\/(y_value_counts[1]+y_value_counts[0]))*100,\"%)\")\nprint(\"Number of tweets that are not disaster \", y_value_counts[0], \", (\", (y_value_counts[0]\/(y_value_counts[1]+y_value_counts[0]))*100,\"%)\")\n\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(aspect=\"equal\"))\nrecipe = [\"Disaster\", \"Not disaster\"]\n\ndata = [y_value_counts[1], y_value_counts[0]]\n\nwedges, texts = ax.pie(data, wedgeprops=dict(width=0.5), startangle=-40)\n\nbbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\nkw = dict(xycoords='data', textcoords='data', arrowprops=dict(arrowstyle=\"-\"),\n          bbox=bbox_props, zorder=0, va=\"center\")\n\nfor i, p in enumerate(wedges):\n    ang = (p.theta2 - p.theta1)\/2. + p.theta1\n    y = np.sin(np.deg2rad(ang))\n    x = np.cos(np.deg2rad(ang))\n    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n    ax.annotate(recipe[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n                 horizontalalignment=horizontalalignment, **kw)\n\nax.set_title(\"Number of tweets that are disaster and not disaster\")\n\nplt.show()","da99ee2b":"train.info()","8c3abde1":"plt.figure(figsize=(15,6))\nsns.set_style('darkgrid')\nplt.subplot(1, 2, 1)\nplt.title('train keyword column') \nx=train['keyword'].isnull().value_counts()\nsns.barplot(['non null','null'],x)\n\nsns.set_style('darkgrid')\nplt.subplot(1, 2, 2)\nplt.title('train location column') \nx=train['location'].isnull().value_counts()\nsns.barplot(['non null','null'],x)\nplt.show()\n\nprint(\"train column Keyword percentage of null value is %.2f\" %(train.keyword.isnull().sum()\/train.keyword.notnull().sum()*100))\nprint(\"train column location percentage of null value is %.2f\" %(train.location.isnull().sum()\/train.location.notnull().sum()*100))","cfb44408":"train['keyword'].fillna(\"No keyword\",inplace=True)\ntest['keyword'].fillna(\"No keyword\",inplace=True)","30b646cd":"train.drop(['location'],axis=1,inplace=True)\ntest.drop(['location'],axis=1,inplace=True)","a3ddf57f":"train.shape","3c1024c0":"word_count = train.text.str.split().apply(len).value_counts()\nprint('total number of words present in each text feature')\nprint(word_count)","0b6d3fcd":"word_dict = dict(word_count)\nword_dict = dict(sorted(word_dict.items(), key=lambda kv: kv[1]))\n\n\nind = np.arange(len(word_dict))\nplt.figure(figsize=(20,5))\np1 = plt.bar(ind, list(word_dict.values()))\n\nplt.ylabel('Number of tweets')\nplt.xlabel('Number of words in each text')\nplt.title('Words for each text of the tweets')\nplt.xticks(ind, list(word_dict.keys()))\nplt.show()","50a98124":"disaster_word_count = train[train['target']==1]['text'].str.split().apply(len)\ndisaster_word_count = disaster_word_count.values\n\nnot_disater_word_count = train[train['target']==0]['text'].str.split().apply(len)\nnot_disater_word_count = not_disater_word_count.values","ce7d56bd":"# https:\/\/glowingpython.blogspot.com\/2012\/09\/boxplot-with-matplotlib.html\nplt.boxplot([disaster_word_count, not_disater_word_count])\nplt.title('Words for each text of the tweets')\nplt.xticks([1,2],('Disaster','Not a Disaster'))\nplt.ylabel('Words in tweets')\nplt.grid()\nplt.show()","e97a553f":"plt.figure(figsize=(10,3))\nsns.distplot(disaster_word_count, hist=False, label=\"Disaster\")\nsns.distplot(not_disater_word_count, hist=False, label=\"Not a Disaster\")\nplt.title('Words for each text of the tweets')\nplt.xlabel('Number of words in each text')\nplt.legend()\nplt.show()","a2617367":"word_count_k = train.keyword.value_counts()\nprint('total number of words present in each keyword')\nprint(word_count_k)","f8607b82":"sns.distplot(word_count_k.values)\nplt.title('distribution of unique words in keyword')\nplt.xlabel('Number of unique words in keyword feature')\nplt.show()","286d9f6d":"train.keyword = train.keyword.str.replace('%20', ' ', regex=True)\ntest.keyword = test.keyword.str.replace('%20', ' ', regex=True)","9bed75ac":"keyword_count = train.keyword.str.split().apply(len).value_counts()\nprint('total number of words present in each text feature')\nprint(keyword_count)\nkeyword_dict = dict(keyword_count)\nkeyword_dict = dict(sorted(keyword_dict.items(), key=lambda kv: kv[1]))\n\n\nind_key = np.arange(len(keyword_dict))\nplt.figure(figsize=(20,5))\np1 = plt.bar(ind_key, list(keyword_dict.values()))\n\nplt.ylabel('Number of tweets')\nplt.xlabel('Number of words in each text')\nplt.title('Words for each text of the tweets')\nplt.xticks(ind_key, list(keyword_dict.keys()))\nplt.show()","e6e032bb":"train.keyword[101:120]","b03bcb48":"train.text[101:120]","d69e0ecc":"train['n_special_word'] = train['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#') or x.startswith('@') or x.isdigit()]))\ntest['n_special_word'] = test['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#') or x.startswith('@') or x.isdigit()]))\n\ntrain['freq_keyword'] = train.groupby('keyword')['keyword'].transform('count')\ntest['freq_keyword'] = test.groupby('keyword')['keyword'].transform('count')\n\ntrain['textlen'] = train['text'].str.len() \ntrain['keywordlen'] = train['keyword'].str.len()\ntrain['text_n_words'] = train['text'].apply(lambda row: len(row.split(\" \")))\ntrain['keyword_n_words'] = train['keyword'].apply(lambda row: len(row.split(\" \")))\n\ntest['textlen'] = test['text'].str.len() \ntest['keywordlen'] = test['keyword'].str.len()\ntest['text_n_words'] = test['text'].apply(lambda row: len(row.split(\" \")))\ntest['keyword_n_words'] = test['keyword'].apply(lambda row: len(row.split(\" \")))\n\ndef normalized_word_Common(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['keyword'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['text'].split(\" \")))    \n    return 1.0 * len(w1 & w2)\ntrain['word_Common'] = train.apply(normalized_word_Common, axis=1)\ntest['word_Common'] = test.apply(normalized_word_Common, axis=1)\n\ndef normalized_word_Total(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['keyword'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['text'].split(\" \")))    \n    return 1.0 * (len(w1) + len(w2))\ntrain['word_Total'] = train.apply(normalized_word_Total, axis=1)\ntest['word_Total'] = test.apply(normalized_word_Total, axis=1)\n\ndef normalized_word_share(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['keyword'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['text'].split(\" \")))    \n    return 1.0 * len(w1 & w2)\/(len(w1) + len(w2))\ntrain['word_share'] = train.apply(normalized_word_share, axis=1)\ntest['word_share'] = test.apply(normalized_word_share, axis=1)\n\ntrain.head(5)","950d61a4":"# To get the results in 4 decemal points\nSAFE_DIV = 0.0001 \n\nSTOP_WORDS = stopwords.words(\"english\")\n\ndef preprocess(x):\n    x = str(x).lower()\n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"\u2032\", \"'\").replace(\"\u2019\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n                           .replace(\"%\", \" percent \").replace(\"\u20b9\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"\u20ac\", \" euro \").replace(\"'ll\", \" will\")\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    x = re.sub(r\"\\w+:\\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\\/[^\\s\\\/]*))*\", \"\", x)\n    x = x.replace(\"  \", \" \")\n    x = x.replace(\"_\", \" \")\n    \n    \n    porter = PorterStemmer()\n    pattern = re.compile('\\W')\n    \n    if type(x) == type(''):\n        x = re.sub(pattern, ' ', x)\n    \n    \n    if type(x) == type(''):\n        x = porter.stem(x)\n        example1 = BeautifulSoup(x, \"lxml\")\n        x = example1.get_text()\n               \n    \n    return x","2dbf62c9":"train[\"text\"] = train[\"text\"].apply(preprocess)\ntest[\"text\"] = test[\"text\"].apply(preprocess)\ntrain[\"keyword\"] = train[\"keyword\"].apply(preprocess)\ntest[\"keyword\"] = test[\"keyword\"].apply(preprocess)","238d4b0a":"train.info()","79079cd6":"train_disaster = train[train['target'] == 1]\ntrain_not_disaster = train[train['target'] == 0]\ndis = ' '.join(train_disaster.text)\nnot_dis = ' '.join(train_not_disaster.text)\nstopwordswc = set(STOPWORDS)","7372f953":"\nwc1 = WordCloud(background_color=\"white\", max_words=len(dis), stopwords=stopwordswc)\nwc1.generate(dis)\nwc2 = WordCloud(background_color=\"white\", max_words=len(not_dis), stopwords=stopwordswc)\nwc2.generate(not_dis)\n\nfig = plt.figure(figsize=(20,15))\n\nax = fig.add_subplot(1,2,1)\nplt.title('Word Cloud for Disaster Text')\nax.imshow(wc1, interpolation='bilinear')\nax.axis('off')\n\nax = fig.add_subplot(1,2,2)\nplt.title(\"Word Cloud for not Disaster Text\")\nax.imshow(wc2, interpolation='bilinear')\nax.axis('off')","14a18f23":"# Using TSNE for Dimentionality reduction for 15 Features(Generated after cleaning the data) to 3 dimention\ndfp_subsampled = train[0:5000]\nX = MinMaxScaler().fit_transform(dfp_subsampled[['freq_keyword', 'textlen', 'keywordlen', 'text_n_words', 'keyword_n_words', 'word_Common', 'word_Total', 'word_share', 'n_special_word']])\ny = dfp_subsampled['target'].values","2cce8fc2":"tsne2d = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","e5b5c110":"df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n\n# draw the plot in appropriate place in the grid\nsns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, height=8,palette=\"Set1\",markers=['s','o'])\nplt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\nplt.show()","e273071a":"y_train = train[\"target\"].values\ntrain = train.drop(['target'], axis=1)","facfb396":"import pickle\nfrom tqdm import tqdm","79d7db66":"with open('..\/input\/glove-vectors\/glove_vectors', 'rb') as f:\n    model = pickle.load(f)\n    glove_words =  set(model.keys())","c4a688ed":"# TFIDF Word2Vec\n# compute TFIDF word2vec for each review.\ndef train_tfidfw2v(x):\n    tfidf_model = TfidfVectorizer()\n    tfidf_model.fit(x)\n    # we are converting a dictionary with word as a key, and the idf as a value\n    dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n    tfidf_words = set(tfidf_model.get_feature_names())\n    tfidf_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\n    for sentence in tqdm(x): # for each review\/sentence\n        vector = np.zeros(300) # as word vectors are of zero length\n        tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n        for word in sentence.split(): # for each word in a review\/sentence\n            if (word in glove_words) and (word in tfidf_words):\n                vec = model[word] # getting the vector for each word\n                # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n                tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n                vector += (vec * tf_idf) # calculating tfidf weighted w2v\n                tf_idf_weight += tf_idf\n        if tf_idf_weight != 0:\n            vector \/= tf_idf_weight\n        tfidf_w2v_vectors.append(vector)\n    return tfidf_w2v_vectors","9f98e045":"tfidf_w2v_vectors_keyword = train_tfidfw2v(train['keyword'])\ntfidf_w2v_vectors_text = train_tfidfw2v(train['text'])\n\ntfidf_w2v_vectors_keyword_test = train_tfidfw2v(test['keyword'])\ntfidf_w2v_vectors_text_test = train_tfidfw2v(test['text'])","f22a7ec7":"x_train_fre_len_n_common_total_share_special = StandardScaler().fit_transform(train[['freq_keyword', 'textlen', 'keywordlen', 'text_n_words', 'keyword_n_words', 'word_Common', 'word_Total', 'word_share', 'n_special_word']])\nx_test_fre_len_n_common_total_share_special = StandardScaler().fit_transform(test[['freq_keyword', 'textlen', 'keywordlen', 'text_n_words', 'keyword_n_words', 'word_Common', 'word_Total', 'word_share', 'n_special_word']])\nprint(\"after Standardizing numerical features\")\nprint(x_train_fre_len_n_common_total_share_special.shape, y_train.shape)\nprint(x_test_fre_len_n_common_total_share_special.shape)","6c516319":"X_train_tfidf_w2v = sparse.csr_matrix(np.hstack((tfidf_w2v_vectors_keyword, \n                                                   tfidf_w2v_vectors_text,\n                                                   x_train_fre_len_n_common_total_share_special)))\n\nX_test_tfidf_w2v = sparse.csr_matrix(np.hstack((tfidf_w2v_vectors_keyword_test, \n                                                  tfidf_w2v_vectors_text_test,\n                                                  x_test_fre_len_n_common_total_share_special)))\n\n\nprint(\"Final Data matrix for tfidf set 2\")\nprint(X_train_tfidf_w2v.shape, y_train.shape)\nprint(X_test_tfidf_w2v.shape)","7e21f76f":"def plot_roc_curve(fpr_tr, tpr_tr):\n    '''\n    plot the ROC curve for the FPR and TPR value\n    '''\n    plt.plot(fpr_tr, tpr_tr, 'k.-', color='green', label='ROC_train AUC = {:0.2f} '.format(auc(fpr_tr, tpr_tr)))\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","4e0f5ce7":"def find_best_threshold(threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    return t\n\ndef predict_with_best_t(proba, threshould):\n    predictions = []\n    for i in proba:\n        if i>=threshould:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","4da8eca3":"import xgboost as xgb\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(X_train_tfidf_w2v, label=y_train)\n\nwatchlist = [(d_train, 'train')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n\npredict_y = bst.predict(d_train)\nprint(\"The test log loss is:\",log_loss(y_train, predict_y, eps=1e-15))\n\nfpr_tfidf, tpr_tfidf, t_tfidf = roc_curve(y_train, predict_y)\nprint('F1 score',f1_score(y_train,predict_with_best_t(predict_y, find_best_threshold(t_tfidf,fpr_tfidf,tpr_tfidf))))\nplot_roc_curve(fpr_tfidf,tpr_tfidf)","1fc2dbb8":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nd_test = xgb.DMatrix(X_test_tfidf_w2v)\ny_ = bst.predict(d_test)\ny_pred = predict_with_best_t(y_, find_best_threshold(t_tfidf,fpr_tfidf,tpr_tfidf))","9b5e518c":"submission['target'] = y_pred\nsubmission.to_csv('submission.csv', index=False)","10018dda":"<h2> 7.1 XGB with hyperparameter tuning <\/h2>","07060717":"**Note:**\n- maximum word count is 3. \n- there are 6387 single word, 1132 two words and 33 three words in keyword feature.","3b44bbfd":"### Text feature","2804895c":"**Note:**\n- from above chart there are almost 500 tweets have words count of 11 to 20\n- Very few tweets have words >30","118ef74a":"<h3> 5.1 Plotting Word clouds <\/h3>","daa41693":"**Note:**\n- From above information we found that there are null values in keywords and location","7d3c3843":"# 6. Make Data Model Ready:","9372d72f":"#### Drop location feature","bb001712":"<p>twitter is a place where users post and interact with messages known as \"tweets\". tweets are limited to 280 characters. this tweets could be image\/words.\n<\/p>","f31c9b46":"# This is my first prediction Competition @ Kaggle :) Hope it is helpful. Please upvote if you like this kernel.","35e48d63":"### 2.2 Univariate Analysis:","a8fe6b6e":"# 2 Exploratory Data Analysis\n<h3>  2.1 Distribution of data points among output classes<\/h3>","b328ee30":"Keyword replace NaN with string","a34a653f":"**Note:**\n- Keyword feature is nothing but the keyword used in the text feature. so ther is correlation between this two features. \n- based on this we will be extracting couple of features which will be used in our model.","452445b2":"**Note:**\n- we clearly identified that number words frequency in tweets says its is disaster or not are almost equal. \n- word counts are equally balanced in both class we can find it from box plot and distribution plot.","a00fadaa":"<h2> Type of Machine Leaning Problem <\/h2>","23cabde2":"Files\n\n    train.csv - the training set\n    test.csv - the test set\n    sample_submission.csv - a sample submission file\n\nColumns\n\n    id - a unique identifier for each tweet\n    text - the text of the tweet\n    location - the location the tweet was sent from (may be blank)\n    keyword - a particular keyword from the tweet (may be blank)\n    target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","a444128d":"# 3 Basic Feature Extraction (before cleaning)","2df68030":"<h1> Description <\/h1>","af9a777c":"__ Problem Statement __\n- Twitter has become an important communication channel in times of emergency.it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster.\n- Now we are tasked with predict whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.","e5937cc4":"<h1> Data Overview <\/h1>\n<p> \n- Data will be in a file Train.csv & test .csv <br>\n- Train.csv contains 5 columns : id, text, location, keyword, target <br>\n- Number of rows in Train.csv = 7613\n<p>","13d80352":"### keyword feature","a16c115a":"#### Checking for null or empty values again","260a263c":"### 6.1 Encoding text feature ","909240b3":"### 5.2 2D Visualization using TSNE","5dd252da":"# 7. Machine Learning Model","317a479a":"<p> It is a binary classification problem, for a given datapoints or tweets we need to predict if the tweet is about real disaster or not. <\/p>","4de6a9a5":"**Note:**\n- for location column we are having almost 50% of null values and there are 3341 unique words. \n- its better to discard location feature than to impute it. As imputing almost 40% of your data would be introducing significant amount of error in it.\n- we will be keeping the keyword column because it will not have any impact over data imbalance. since the percentage of null value in keyword column is very low 0.8% which is 61 out of 7613 data points.","0da55e63":"### Checking for null values","f55a3958":"<h1 style=\"text-align:center;font-size:30px;\" > Disaster Tweets <\/h1>","f217dff4":"Let us now construct a few features like:\n - ____n_special_word____ = number of special words count starts with @, # and digits from text feature\n - ____freq_keyword____ = Frequency of keyword \n - ____textlen____ = Length of text\n - ____keywordlen____ = Length of keyword\n - ____text_n_words____ = Number of words in text\n - ____keyword_n_words____ = Number of words in keyword\n - ____word_Common____ = (Number of common unique words in keyword and text)\n - ____word_Total____ =(Total num of words in text + Total num of words in keyword)\n - ____word_share____ = (word_common)\/(word_Total)\n","37d8b836":"**Note:**\n* total of 221 unique words present in the keyword feature.\n* there are few words which occurs more often between 30 to 40 times\n* all the words present in each keyword feature have only single word with %20 which is space.\n* \u201c%20,\u201d it represents a space in an encoded URL\n* we will be replacing the %20 with space in all keyword feature and do the data analysis.","cd77d2d2":"# 1. Reading Data","9988d507":"#### Concatinating all the features: (standardscalar + tfidfW2v)","164e26fd":"# 5. Visualization","33ad17ab":"# 4 Preprocessing of Text","f57f4bb7":"#### TFIDF-W2V","3cbb10a2":"- Preprocessing:\n    - Removing html tags \n    - Removing Punctuations\n    - Performing stemming\n    - Removing Stopwords\n    - Expanding contractions etc.","78d35b34":"### 6.2 encoding numerical features:"}}