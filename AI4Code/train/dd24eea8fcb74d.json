{"cell_type":{"96d8f6d2":"code","a92445f6":"code","b8185f71":"code","12fc0fac":"code","89ace52d":"code","2741e46a":"code","38fc87ff":"code","e50279d3":"code","656941b9":"code","dc9b676f":"code","a06d55ec":"code","53af3c11":"code","c60c704c":"code","5e5bd35f":"code","da8fafcb":"code","830c3970":"code","1fdc24aa":"code","023f46b4":"code","696763fc":"code","ee2ec764":"code","b5dc4dc5":"code","00337857":"code","035296b0":"code","71079214":"code","9dae7631":"code","c5519899":"code","e2632b2f":"code","3eb1c45b":"code","46718032":"code","f77a1113":"code","b1f52514":"code","4d256b07":"code","a164be18":"code","7d4876ba":"code","f65a78ea":"code","fce9aa8e":"code","0c6e9a61":"code","71df4eab":"markdown","54416cb4":"markdown"},"source":{"96d8f6d2":"import os, gc\nfrom fastai.text import *\nfrom tqdm import tqdm_notebook as tqdm\nprint(os.listdir(\"..\/input\"))","a92445f6":"# make training deterministic\/reproducible\ndef seed_everything(seed=42):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()\n\ndef f1_score(y_pred, targets):\n    epsilon = 1e-07\n\n    y_pred = y_pred.argmax(dim=1)\n#     targets = targets.argmax(dim=1)\n\n    tp = (y_pred*targets).float().sum(dim=0)\n    tn = ((1-targets)*(1-y_pred)).float().sum(dim=0)\n    fp = ((1-targets)*y_pred).float().sum(dim=0)\n    fn = (targets*(1-y_pred)).sum(dim=0)\n\n    p = tp \/ (tp + fp + epsilon)\n    r = tp \/ (tp + fn + epsilon)\n\n    f1 = 2*p*r \/ (p+r+epsilon)\n    f1 = torch.where(f1!=f1, torch.zeros_like(f1), f1)\n    return f1.mean()","b8185f71":"EMBED_SIZE = 100\nMAX_FEATURES = 150000\nMAX_LENGTH = 100\nEMBEDDING_FILE = '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'","12fc0fac":"# train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n# train_df.head()\ntrain_df = pd.read_csv('..\/input\/train.csv')\n\ninsincere_df = train_df[train_df.target==1]\nsincere_df = train_df[train_df.target==0]\n\nsincere_df = sincere_df.iloc[np.random.permutation(len(sincere_df))]\nsincere_df = sincere_df[:int(len(insincere_df)*2)]\n\ndel train_df\n\ntrain_df = pd.concat([insincere_df, sincere_df])\ntrain_df = train_df.iloc[np.random.permutation(len(train_df))]\n\ndel insincere_df\ndel sincere_df\ngc.collect()","89ace52d":"%%time\nmispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n                \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \n                \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n                \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n                \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n                \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n                \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n                \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n                \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n                \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n                \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n                \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n                \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n                \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n                \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n                \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n                \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n                \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n                \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \n                \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \n                \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \n                \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n                \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n                \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n                \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n                \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \n                \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n                \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \n                \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n                \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n                \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n                \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n                \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n                \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n                \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n                \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', \n                'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', \n                'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', \n                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', \n                'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', \n                'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', \n                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', \n                'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', \n                'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', \n                'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', \n                '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', \n                'demonitisation': 'demonetization', 'demonitization': 'demonetization', \n                'demonetisation': 'demonetization', \"n\u2019t\": \"not\", \"n't\": \"not\", \"\u2019ve\": \"have\",\n                \"\u2019re\": \"are\", \"\u2019ll\": \"will\", \"howmuch\": \"how much\", \"i`m\": \"I am\", \"can`t\": \"can not\",\n                \"dosen't\": \"does not\", \"what's\u200b\": \"what is\", \"did't\": \"did not\", \"doesn`t\": \"dose not\",\n                \"ya'll\": \"you alll\", \"it`s\": \"it is \", \"does'nt\": \"does not\", \"what`s\": \"what is\",\n                \"dosn't\": \"does not\", \"is'nt\": \"is not\", \"don'y\": \"do not you\", \"wan't\": \"will not\",\n                \"that`s\": \"that is\", \"didn`t\": \"dod not\", \"hold'em\": \"holdaem\", \"din't\": \"did not\",\n                \"isn't\": \"is not\"}\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\nrx_xxxheight = re.compile('\\d+[\\'\"]\\d+(.\\d+)?')\nrx_xxxtime_or_score = re.compile('\\d+:\\d+')\n\ndef replace(match):\n        return mispellings[match.group(0)]\n\ndef replace_typical_misspell(text):\n    text = rx_xxxheight.sub('xxxheight', text)\n    text = rx_xxxtime_or_score.sub('xxxtime_or_score', text)\n    return mispellings_re.sub(replace, text)\n\n# Clean speelings\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))","2741e46a":"%%time\n# Load weights\ndef get_coefs(w, *x):\n    return w,np.asarray(x, dtype='float32')\n\n\nkeywords = ['xxunk','xxpad','xxbos','xxfld','xxmaj','xxup','xxrep','xxwrep','xxxheight', 'xxxtime_or_score']\n\nemb_mean = -0.0033469964\nemb_std = 0.109855406\nnb_r_words = len(keywords)\n\nwith open(EMBEDDING_FILE) as f:\n    total, size = f.readline().split(' ')\n    total = int(total)\n    \n    embeddings = torch.zeros((total+nb_r_words, EMBED_SIZE), dtype=torch.float32)\n    embeddings.normal_(emb_mean, emb_std)\n#     embeddings = np.random.normal(emb_mean, emb_std, (total+nb_r_words, EMBED_SIZE)).astype(np.float32)\n    \n    for i, line in enumerate(tqdm(f, total=total)):\n        word, weight = get_coefs(*line.split(' '))\n        embeddings[i+nb_r_words] = torch.from_numpy(weight[:EMBED_SIZE])\n        keywords.append(word)","38fc87ff":"# emb_mean,emb_std = embeddings.mean(), embeddings.std()\n# emb_mean,emb_std # (-0.00334699644103647, 0.10985540269880754)","e50279d3":"vocab = Vocab(itos=keywords)","656941b9":"train_df = train_df.iloc[np.random.permutation(len(train_df))]\ncut = int(0.1 * len(train_df)) + 1\ntrain_df, valid_df = train_df[cut:], train_df[:cut]","dc9b676f":"%%time\ndata = TextDataBunch.from_df(path='.',\n                             train_df=train_df, \n                             valid_df=valid_df,\n                             test_df=test_df,\n                             text_cols='question_text', \n                             label_cols='target',\n                             max_vocab=MAX_FEATURES,\n                            vocab=vocab)\nprint(len(data.vocab.itos))\ndata.save()\ndel train_df\ndel valid_df \ndel test_df \ndel data\ngc.collect()","a06d55ec":"%%time\ndata = TextClasDataBunch.load(path='.', bs=64)\ndata.show_batch()","53af3c11":"# # Load weights\n# def get_coefs(w, *x):\n#     return w,np.asarray(x, dtype='float16')\n\n# embeddings_index = {}\n# with open(EMBEDDING_FILE) as f:\n#     total, size = f.readline().split(' ')\n#     total = int(total)\n    \n#     for line in tqdm(f, total=total):\n#         word, weight = get_coefs(*line.split(' '))\n#         embeddings_index[word] = weight[:EMBED_SIZE]","c60c704c":"# %%time\n# # mean, std\n# all_embs = np.stack(list(embeddings_index.values()))\n# emb_mean,emb_std = all_embs.mean(), all_embs.std(dtype=np.float32)\n# del all_embs\n# gc.collect()","5e5bd35f":"# # random weights\n# vocab_size = len(data.vocab.itos)\n# embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, EMBED_SIZE)).astype(np.float32)\n# embedding_matrix.shape","da8fafcb":"# words_without_vec = 0\n# # map pre-trained weights with our data\n# for i, word in enumerate(tqdm(data.vocab.itos)):\n#     if i >= vocab_size: continue\n#     embedding_vector = embeddings_index.get(word)\n#     if embedding_vector is not None: \n#         embedding_matrix[i] = embedding_vector\n#     else:\n#         words_without_vec = words_without_vec+1\n#         print(i, word)\n\n# embedding_matrix = torch.from_numpy(embedding_matrix) \n# del embeddings_index\n# gc.collect()","830c3970":"# class QuoraInsincere(nn.Module):\n#     def __init__(self, embedding_wights):\n#         super(QuoraInsincere, self).__init__()\n#         self.embeddings = nn.Embedding.from_pretrained(embedding_wights, freeze=False)\n#         self.linear1 = nn.Linear(EMBED_SIZE*MAX_LENGTH, 1)\n        \n#     def forward(self, inputs):\n#         x = self.embeddings(inputs)\n#         x = x.view(x.size(0), -1)\n#         x = self.linear1(x)\n#         x = torch.sigmoid(x)        \n#         return x\n# model = QuoraInsincere(embeddings)","1fdc24aa":"learner = text_classifier_learner(data, drop_mult=0.5, emb_sz=EMBED_SIZE, nl=1, nh=10, max_len=MAX_LENGTH)","023f46b4":"learner.model","696763fc":"# load our new weights to extsing model\nencoderModel =  next(learner.model.children())\n# encoder = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\nencoderModel.encoder.load_state_dict({'weight': embeddings})","ee2ec764":"learner.unfreeze()","b5dc4dc5":"learner.lr_find()","00337857":"learner.recorder.plot(skip_end=5)","035296b0":"learner.metrics.append(f1_score)","71079214":"learner.fit_one_cycle(1, 7e-2, moms=(0.8,0.7))","9dae7631":"learner.save('first')","c5519899":"learner.load('first');","e2632b2f":"learner.freeze_to(-2)\nlearner.fit_one_cycle(2, slice(1e-3,7e-2), moms=(0.8,0.7))","3eb1c45b":"learner.save('second')","46718032":"learner.load('second');","f77a1113":"learner.unfreeze()\nlearner.fit_one_cycle(4, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7))","b1f52514":"# learner.fit_one_cycle(3, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7))","4d256b07":"%time learner.predict(\"How much does a tutor earn in Bangalore?\")","a164be18":"preds = learner.get_preds(ds_type=DatasetType.Test)","7d4876ba":"preds = preds[0].argmax(dim=1)\npreds.sum()","f65a78ea":"test_df = pd.read_csv('..\/input\/test.csv')","fce9aa8e":"test_df.drop(['question_text'], axis=1, inplace=True)\ntest_df['prediction'] = preds.numpy()","0c6e9a61":"test_df.to_csv(\"submission.csv\", index=False)","71df4eab":"### Classifier","54416cb4":"### Test set"}}