{"cell_type":{"27da9541":"code","c0b8c7f7":"code","d030e5ef":"code","c25d4cb1":"code","d3b6c82b":"code","cb019e9f":"code","bcdb327b":"code","ec9d22e3":"code","ca524c89":"code","f02d3ad3":"code","1d822cb0":"code","dcb51711":"code","2759eecf":"code","1630d9cb":"code","0280a414":"code","7589f580":"code","03546678":"code","7986a148":"code","6d4eba20":"code","1e7fefd0":"code","b656e4ba":"code","5698a52a":"code","c2361f74":"code","850c83d5":"code","c52908c5":"markdown","6cda0865":"markdown","9a02579e":"markdown","23e706e3":"markdown","31843156":"markdown","b4007429":"markdown","63bc6680":"markdown","7d86bf7c":"markdown","282da625":"markdown","002636d3":"markdown","8b459a1b":"markdown","17024218":"markdown","88986392":"markdown"},"source":{"27da9541":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n\nfrom sklearn.utils import shuffle\n\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import LancasterStemmer\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\n\nimport xgboost as xgb\n\nimport keras \nfrom keras.models import Sequential, Model \nfrom keras import layers\nfrom keras.layers import Dense, Dropout, Input, Embedding\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c0b8c7f7":"col_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ndf = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', header=None, encoding='ISO-8859-1', names=col_names)\ndf.head()","d030e5ef":"# No. of observations in each classes\n\nfig = plt.figure(figsize=(8,8))\ntargets = df.groupby('target').size()\nlabels = [r'Positive', r'Negative']\ntargets.plot(kind='pie', subplots=True, figsize=(10, 8), autopct = \"%.2f%%\", colors=['yellow','red'], labels=labels)\nplt.title(\"No. of observations in each class\",fontsize=16)\nplt.ylabel(\"\")\nplt.legend()\nplt.show()","c25d4cb1":"# Word cloud for Positive\n\nplt.figure(figsize=(14,7))\nword_cloud = WordCloud(stopwords = STOPWORDS, max_words = 200, width=1366, height=768, background_color=\"white\").generate(\" \".join(df[df.target==4].text))\nplt.imshow(word_cloud,interpolation='bilinear')\nplt.axis('off')\nplt.title('Most common words in positive sentiment tweets.',fontsize=20)\nplt.show()","d3b6c82b":"# Word cloud for Negative\n\nplt.figure(figsize=(14,7))\nword_cloud = WordCloud(stopwords = STOPWORDS, max_words = 200, width=1366, height=768, background_color=\"white\").generate(\" \".join(df[df.target==0].text))\nplt.imshow(word_cloud,interpolation='bilinear')\nplt.axis('off')\nplt.title('Most common words in negative sentiment tweets.',fontsize=20)\nplt.show()","cb019e9f":"english_stopwords = stopwords.words('english')\nstemmer = SnowballStemmer('english')\nregex = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"","bcdb327b":"word_bank = []\n\ndef preprocessing(text):\n    review = re.sub('[^a-zA-Z]',' ',text) \n    review = review.lower()\n    review = review.split()\n    ps = LancasterStemmer()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n    return ' '.join(review)","ec9d22e3":"df_new = shuffle(df, random_state=45)\ntemp = df_new[1:700000]\ntemp['target'].value_counts()","ca524c89":"temp['text'] = temp['text'].apply(lambda x: preprocessing(x))","f02d3ad3":"y = temp['target']\nle = LabelEncoder()\ny = le.fit_transform(y)","1d822cb0":"X_train_1, X_test_1, y_train, y_test = train_test_split(temp['text'], y, test_size = 0.2, random_state = 45)","dcb51711":"tfidf = TfidfVectorizer(max_features = 550)\nX_train = tfidf.fit_transform(X_train_1).toarray() \nX_test = tfidf.transform(X_test_1).toarray()","2759eecf":"X_train.shape, X_test.shape","1630d9cb":"classifier = LogisticRegression()\nclassifier.fit(X_train, y_train) \n\ny_pred = classifier.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","0280a414":"nb = GaussianNB()\nnb.fit(X_train, y_train)\n\ny_pred_nb = nb.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_nb)) \ncm = confusion_matrix(y_test, y_pred_nb)\nprint(cm)\nprint(classification_report(y_test, y_pred_nb)) ","7589f580":"clf = xgb.XGBClassifier()\nclf.fit(X_train, y_train)  \ny_pred_xgb = clf.predict(X_test)\ncm = confusion_matrix(y_test, y_pred_xgb)\nprint(cm)\nprint(classification_report(y_test, y_pred_xgb))\nprint(accuracy_score(y_test, y_pred_xgb)) ","03546678":"model = Sequential()\n\nmodel.add(Dense(256, activation='relu', input_dim=550))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adadelta',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","7986a148":"history = model.fit(X_train, y_train, epochs=20, batch_size=50,\n                   validation_data=(X_test,y_test))\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","6d4eba20":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","1e7fefd0":"y_pred_new = model.predict(X_test)\ny_pred = np.where(y_pred_new>0.5,1,0)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(classification_report(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred)) ","b656e4ba":"model = Sequential()\n\nmodel.add(Dense(256, activation='relu', input_dim=550))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","5698a52a":"history1 = model.fit(X_train, y_train, epochs=50, batch_size=64,\n                   validation_data=(X_test,y_test))\n\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","c2361f74":"# summarize history for accuracy\nplt.plot(history1.history['accuracy'])\nplt.plot(history1.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history1.history['loss'])\nplt.plot(history1.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","850c83d5":"y_pred_new = model.predict(X_test)\ny_pred = np.where(y_pred_new>0.5,1,0)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(classification_report(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred)) ","c52908c5":"## EDA","6cda0865":"**The Accuracy scores by various classifiers are:**<br>\nLogistic Regression: 73.414% <br>\nNaive Bayes Classifier: 70.166% <br>\nXG Boost Classifier: 73.256% <br>\nNeural Network 1: 68.827% <br>\nNeural Network 2: 73.802% <br>","9a02579e":"# Conclusion","23e706e3":"## Importing Libraries and Dataset","31843156":"### Neural Network - 1","b4007429":"### Logistic Regression","63bc6680":"## Neural Network 2","7d86bf7c":"# Sentiment Classification","282da625":"## TF-IDF Transformation","002636d3":"### Naive Bayes Classifier","8b459a1b":"## Splitting the data","17024218":"## Preprocessing","88986392":"### XGBoost"}}