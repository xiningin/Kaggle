{"cell_type":{"6e269f39":"code","ba658348":"code","4a8c7085":"code","fd39c737":"code","b3b2e1f3":"code","449079ae":"code","0154b283":"code","5af82dbd":"code","533bf8cd":"code","a66e8389":"code","c46d12a3":"code","84eb2056":"code","46d7722e":"code","85006e0e":"code","f31edb71":"code","19d3fff5":"code","a269ca5b":"code","fe85a45f":"code","c83c8f98":"code","60289574":"code","fe07cd05":"code","85b5830a":"code","a3de3d25":"code","06a75656":"code","8dbb2c52":"code","10ef947f":"code","a67ad224":"code","3caf4b1a":"code","f64c3051":"code","dae07417":"code","aae905f4":"code","e11be187":"code","cdd13c8b":"code","10b0af0e":"code","b55386fb":"code","6ab535c7":"code","15402b56":"markdown","b928b490":"markdown","69f02778":"markdown","3b4b7c90":"markdown","e36ffb8a":"markdown","2cc87771":"markdown","99fc7125":"markdown","d7e74ab3":"markdown","a04c919b":"markdown","fb64e792":"markdown","dbdd7d40":"markdown","5a3a01a9":"markdown","a89dfa56":"markdown","42baf7ad":"markdown","10241e2a":"markdown","08412b99":"markdown","20ccb5a6":"markdown","a0001785":"markdown","7cdb5c5c":"markdown","372f9085":"markdown","f6dbb967":"markdown","06624ce2":"markdown","2e20da2c":"markdown","6cb140fd":"markdown","175e81c4":"markdown","e06db23c":"markdown","efb8a15b":"markdown","dce164e2":"markdown","5868eb43":"markdown","75c94368":"markdown"},"source":{"6e269f39":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_validate, StratifiedKFold\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, Lasso, ElasticNet, Ridge\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.svm import SVR\nfrom sklearn import metrics\n\n\n%matplotlib inline","ba658348":"red = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","4a8c7085":"red.shape","fd39c737":"red.head()","b3b2e1f3":"red.info()","449079ae":"pd.DataFrame({\"Type\": red.dtypes,\n              \"Unique\": red.nunique(),\n              \"Null\": red.isnull().sum(),\n              \"Null percent\": red.isnull().sum() \/ len(red),\n              \"Mean\": red.mean(),\n              \"Std\": red.std()})","0154b283":"red.describe().T","5af82dbd":"red.hist(bins=50, figsize=(15,12));","533bf8cd":"print(f\"Percentage of quality scores\")\nred[\"quality\"].value_counts(normalize=True) * 100","a66e8389":"corr_matrix = red.corr()\ncorr_matrix","c46d12a3":"plt.figure(figsize=(15,10))\nsns.heatmap(red.corr(), annot=True, cmap='coolwarm')\nplt.show()","84eb2056":"corr_matrix[\"quality\"].drop(\"quality\").sort_values(ascending=False)","46d7722e":"plt.figure(figsize=(8,5))\ncorr_matrix[\"quality\"].drop(\"quality\").sort_values(ascending=False).plot(kind='bar')\nplt.title(\"Attribute correlations with quality\")\nplt.show()","85006e0e":"predict_columns = red.columns[:-1]\npredict_columns","f31edb71":"X = red[predict_columns]\ny = red[\"quality\"]","19d3fff5":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)","a269ca5b":"X_train.shape, y_train.shape","fe85a45f":"X_test.shape, y_test.shape","c83c8f98":"def evaluate_model(estimator, X_train, y_train, cv=10, verbose=True):\n    \"\"\"Print and return cross validation of model\n    \"\"\"\n    scoring = [\"neg_mean_absolute_error\", \"neg_mean_squared_error\", \"r2\"]\n    scores = cross_validate(estimator, X_train, y_train, return_train_score=True, cv=cv, scoring=scoring)\n    \n    val_mae_mean, val_mae_std = -scores['test_neg_mean_absolute_error'].mean(), \\\n                                -scores['test_neg_mean_absolute_error'].std()\n    \n    train_mae_mean, train_mae_std = -scores['train_neg_mean_absolute_error'].mean(), \\\n                                    -scores['train_neg_mean_absolute_error'].std()\n    \n    val_mse_mean, val_mse_std = -scores['test_neg_mean_squared_error'].mean(), \\\n                                -scores['test_neg_mean_squared_error'].std()\n    \n    train_mse_mean, train_mse_std = -scores['train_neg_mean_squared_error'].mean(), \\\n                                    -scores['train_neg_mean_squared_error'].std()\n    \n    val_rmse_mean, val_rmse_std = np.sqrt(-scores['test_neg_mean_squared_error']).mean(), \\\n                                  np.sqrt(-scores['test_neg_mean_squared_error']).std()\n    \n    train_rmse_mean, train_rmse_std = np.sqrt(-scores['train_neg_mean_squared_error']).mean(), \\\n                                      np.sqrt(-scores['train_neg_mean_squared_error']).std()\n    \n    val_r2_mean, val_r2_std = scores['test_r2'].mean(), scores['test_r2'].std()\n    \n    train_r2_mean, train_r2_std = scores['train_r2'].mean(), scores['train_r2'].std()\n\n    \n    result = {\n        \"Val MAE\": val_mae_mean,\n        \"Val MAE std\": val_mae_std,\n        \"Train MAE\": train_mae_mean,\n        \"Train MAE std\": train_mae_std,\n        \"Val MSE\": val_mse_mean,\n        \"Val MSE std\": val_mse_std,\n        \"Train MSE\": train_mse_mean,\n        \"Train MSE std\": train_mse_std,\n        \"Val RMSE\": val_rmse_mean,\n        \"Val RMSE std\": val_rmse_std,\n        \"Train RMSE\": train_rmse_mean,\n        \"Train RMSE std\": train_rmse_std,\n        \"Val R2\": val_r2_mean,\n        \"Val R2 std\": val_r2_std,\n        \"Train R2\": train_rmse_mean,\n        \"Train R2 std\": train_r2_std,\n    }\n    \n    if verbose:\n        print(f\"val_MAE_mean: {val_mae_mean} - (std: {val_mae_std})\")\n        print(f\"train_MAE_mean: {train_mae_mean} - (std: {train_mae_std})\")\n        print(f\"val_MSE_mean: {val_mse_mean} - (std: {val_mse_std})\")\n        print(f\"train_MSE_mean: {train_mse_mean} - (std: {train_mse_std})\")\n        print(f\"val_RMSE_mean: {val_rmse_mean} - (std: {val_rmse_std})\")\n        print(f\"train_RMSE_mean: {train_rmse_mean} - (std: {train_rmse_std})\")\n        print(f\"val_R2_mean: {val_r2_mean} - (std: {val_r2_std})\")\n        print(f\"train_R2_mean: {train_r2_mean} - (std: {train_r2_std})\")\n\n    return result","60289574":"models = [LinearRegression(), Lasso(alpha=0.1), ElasticNet(),\n          Ridge(), ExtraTreesRegressor(), RandomForestRegressor()]\n\nmodel_names = [\"Lineal Regression\", \"Lasso\", \"ElasticNet\",\n               \"Ridge\", \"Extra Tree\", \"Random Forest\"]","fe07cd05":"mae = []\nmse = []\nrmse = []\nr2 = []\n\nfor model in range(len(models)):\n    print(f\"Paso {model+1} de {len(models)}\")\n    print(f\"...running {model_names[model]}\")\n    \n    rg_scores = evaluate_model(models[model], X_train, y_train)\n    \n    mae.append(rg_scores[\"Val MAE\"])\n    mse.append(rg_scores[\"Val MSE\"])\n    rmse.append(rg_scores[\"Val RMSE\"])\n    r2.append(rg_scores[\"Val R2\"])","85b5830a":"df_result = pd.DataFrame({\"Model\": model_names,\n                          \"MAE\": mae,\n                          \"MSE\": mse,\n                          \"RMSE\": rmse,\n                          \"R2\": r2})\ndf_result","a3de3d25":"df_result.sort_values(by=\"RMSE\", ascending=False).plot.barh(\"Model\", \"RMSE\");","06a75656":"df_result.sort_values(by=\"R2\").plot.barh(\"Model\", \"R2\");","8dbb2c52":"param_grid = [\n    {'n_estimators': range(10, 300, 10), 'max_features': [2, 3, 4, 5, 8, \"auto\"], 'bootstrap': [True, False]}\n]\n\n\nxtree_reg = ExtraTreesRegressor(random_state=42, n_jobs=-1)\n\ngrid_search = GridSearchCV(xtree_reg, param_grid, cv=5, \n                           scoring='neg_mean_squared_error', \n                           return_train_score=True)\n\ngrid_search.fit(X_train, y_train)","10ef947f":"grid_search.best_params_","a67ad224":"final_model = grid_search.best_estimator_\ny_pred = final_model.predict(X_test)","3caf4b1a":"print(f\"MAE: {metrics.mean_absolute_error(y_test, y_pred)}\")\nprint(f\"MSE: {metrics.mean_squared_error(y_test, y_pred)}\")\nprint(f\"RMSE: {np.sqrt(metrics.mean_squared_error(y_test, y_pred))}\")\nprint(f\"R2: {final_model.score(X_test, y_test)}\")","f64c3051":"plt.figure(figsize=(10,8))\nplt.scatter(y_test, y_pred, alpha=0.1)\nplt.xlabel(\"Real\")\nplt.ylabel(\"Predicted\")\nplt.show()","dae07417":"feature_importances = final_model.feature_importances_\nfeature_importances","aae905f4":"sorted(zip(feature_importances, X_test.columns), reverse=True)","e11be187":"feature_imp = pd.Series(feature_importances, index=X_train.columns).sort_values(ascending=False)\nfeature_imp.plot(kind='bar')\nplt.title('Feature Importances')","cdd13c8b":"df_resul = pd.DataFrame({\"Pred\": y_pred,\n              \"Real\": y_test,\n              \"error\": y_pred - y_test,\n              \"error_abs\": abs(y_pred - y_test)})","10b0af0e":"df_resul[\"error\"].plot.hist(bins=40, density=True)\nplt.title(\"Error distribution\")\nplt.xlabel(\"Error\");","b55386fb":"df_resul.groupby(\"Real\")[\"error_abs\"].mean()","6ab535c7":"df_resul.groupby(\"Real\")[\"error_abs\"].mean().plot.bar()\nplt.title(\"MAE distribution\")\nplt.ylabel(\"MAE\")\nplt.xlabel(\"Quality\");","15402b56":"The model that gives the best results is **extra trees**. RMSE = 0.577591 and R2 = 0.477845. Let's fine tune it.","b928b490":"How are the features distributed?","69f02778":"More generally, What's the MAE that occurs in each quality score?","3b4b7c90":"## Introduction","e36ffb8a":"## Get the Data","2cc87771":"OK, we're going train several quick-and-dirty models from different categories using standard parameters. We selected some of the regression models: Linear Regression, Lasso, ElasticNet, Ridge, Extre Trees, and RandomForest.","99fc7125":"Create the training and test datasets:","d7e74ab3":"We are going to check the correlations between the attributes of the dataset:","a04c919b":"## Frame the problem","fb64e792":"## Conclusions\n\nAfter testing various models, the one that provided the best results is ExtraTrees. After fine tuning it, we get a significant improvement.\n\nThe basic line regression model offers an R2: 0.323021 and RMSE: 0.657899. The Extra Tree model offers an R2: 0.529512 and RMSE: 0.570954. However, the R2 score is still very low. According to the value obtained from R2, our model can barely explain 52% of the variance. That is, the percentage of relationship between the variables that can be explained by our model is 52.95%.\n\nAccording to the MAE distribution graph, we can see that our model is not good for extreme scores. In fact, it is not capable of predicting any score of 3 or 8. As we saw in the distribution of the target variable, it is very unbalanced, there are hardly any observations for the extreme values, so the model does not have enough data training for all quality scores.\n\nAs a final consideration, we should try to approach modeling as a classification problem, to evaluate if it offers better results than a regression problem. We will see it in **[part 2](https:\/\/www.kaggle.com\/sgtsteiner\/red-wine-quality-multiclass-classification)** and **[part 3](https:\/\/www.kaggle.com\/sgtsteiner\/red-wine-quality-binary-classification)** of this analysis.","dbdd7d40":"Let's check how our target variable, the quality score, is distributed:","5a3a01a9":"## Fine-Tune","a89dfa56":"## Shortlist Promising Models","42baf7ad":"![](https:\/\/cdn.pixabay.com\/photo\/2016\/03\/09\/11\/53\/wine-glasses-1246240_1280.jpg)","10241e2a":"### Check the size and type of data","08412b99":"### Imports","20ccb5a6":"Let's see how the errors are distributed:","a0001785":"Well, a little better!","7cdb5c5c":"This notebook is part of a trilogy in which I will approach the wine quality dataset from several different approaches:\n\n+ Part 1: Supervised Learning - Regression\n+ [Part 2: Supervised Learning - Multiclass Classification](https:\/\/www.kaggle.com\/sgtsteiner\/red-wine-quality-multiclass-classification)\n+ [Part 3: Supervised Learning - Binary Classification](https:\/\/www.kaggle.com\/sgtsteiner\/red-wine-quality-binary-classification)","372f9085":"# Wine Quality Prediction - Part 1 - Regression","f6dbb967":"Let's see which features are most relevant:","06624ce2":"Create the predictor set and the set with the target variable:","2e20da2c":"We show only the correlations of the target variable with the rest of the attributes:","6cb140fd":"Let's see the performance of each of them:","175e81c4":"It is significantly unbalanced. Most instances (82%) have scores of 6 or 5.","e06db23c":"We have a dataset that contains various characteristics of red and white variants of the Portuguese \"Vinho Verde\" wine. We have chemical variables, such as the amount of alcohol, citric acid, acidity, density, pH, etc; as well as a sensorial and subjective variable such as the score with which a group of experts rated the quality of the wine: between 0 (very bad) and 10 (very excellent).\n\nThey ask us to build a model that can predict the quality score given these biochemical indicators.\n\nFor this first part of the study, we are going to consider that it is a **regression problem**.","efb8a15b":"It's the moment of truth! Let's see the performance on the test set:","dce164e2":"## Explore the Data","5868eb43":"## Prepare the Data","75c94368":"Mmmmm, there are no nulls, what a data set!"}}