{"cell_type":{"7c03c405":"code","d5e07525":"code","9f639703":"code","55894ea9":"code","67295e39":"code","f47500a7":"code","dc3b0be4":"code","cbcda786":"code","d7024333":"code","9035166b":"code","06baafe6":"code","6bdb8031":"code","9543ae65":"code","d314f3dd":"code","5abaded7":"code","ce558e1a":"code","306b4d8f":"code","541bd31e":"code","8e7889eb":"code","d379dd41":"code","148c3f46":"code","d4a0b884":"code","b262e8b5":"code","72ad6a7f":"code","92c94a95":"code","f8543428":"code","d2b6a675":"code","22035222":"code","fe8b1257":"code","611700b3":"code","27d19c48":"code","6b60849a":"markdown","896a1b2a":"markdown","ac07c315":"markdown","3f0b32ce":"markdown","fd8842cb":"markdown","3931a807":"markdown","a8d84bfa":"markdown","2293f20e":"markdown","e0311d0c":"markdown","e5e3ecf8":"markdown","f460361a":"markdown","49166d2d":"markdown"},"source":{"7c03c405":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d5e07525":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","9f639703":"train.head()","55894ea9":"train.shape","67295e39":"from sklearn.metrics import roc_auc_score, classification_report\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.decomposition import PCA","f47500a7":"pca = PCA(0.9)","dc3b0be4":"X, y = train.drop('label', axis=1), train['label']","cbcda786":"X_pca = pca.fit_transform(X)","d7024333":"X_pca.shape","9035166b":"# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=14, stratify=y)\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=14, stratify=y)","06baafe6":"from sklearn.linear_model import LogisticRegression\nlogit = LogisticRegression(random_state=14, solver='liblinear')","6bdb8031":"%%time\nlogit.fit(X_train_pca, y_train)","9543ae65":"print(classification_report(y_test, logit.predict(X_test_pca)))","d314f3dd":"from sklearn import svm","5abaded7":"clf = svm.SVC(probability=True)","ce558e1a":"%%time\nclf.fit(X_train_pca, y_train)","306b4d8f":"print(classification_report(y_test, clf.predict(X_test_pca)))","541bd31e":"lin_clf = svm.LinearSVC()","8e7889eb":"%%time\nlin_clf.fit(X_train_pca, y_train)","d379dd41":"print(classification_report(y_test, lin_clf.predict(X_test_pca)))","148c3f46":"from sklearn.neighbors import KNeighborsClassifier","d4a0b884":"knn = KNeighborsClassifier()","b262e8b5":"knn.fit(X_train_pca, y_train)","72ad6a7f":"print(classification_report(y_test, knn.predict(X_test_pca)))","92c94a95":"test_pca = pca.transform(test)","f8543428":"test_pca.shape","d2b6a675":"test['ImageId']  = test.index + 1","22035222":"%%time\nclf.fit(X_pca, y)","fe8b1257":"output = pd.DataFrame()\noutput['ImageId'] = test['ImageId']\noutput['Label'] = clf.predict(test_pca)\noutput.to_csv('submission.csv',index=False)","611700b3":"# knn.fit(X_pca, y)","27d19c48":"# output = pd.DataFrame()\n# output['ImageId'] = test['ImageId']\n# output['Label'] = knn.predict(test_pca)\n# output.to_csv('output_knn.csv',index=False)","6b60849a":"## PCA","896a1b2a":"## 3 LinearSVC","ac07c315":"784 features.\nWith so many features, even linear models can take a very long time to learn. Let us reduce the dimensionality of the feature space using the principal component method, preserving 90% of the variance of the data.","3f0b32ce":"The models with the best accuracy are SVC and kNN.\nMake a final prediction using these methods.","fd8842cb":"## 2 SVC","3931a807":"## 1 LogisticRegression","a8d84bfa":"## 4 KNeighborsClassifier","2293f20e":"## Let's look at the training data and the size of the dataset","e0311d0c":"## Read the data","e5e3ecf8":"## train_test_split\nlet's divide the training data to check the quality of different models. Try models of logistic regression, method of reference vectors and nearest neighbors.","f460361a":"## Final Prediction \n","49166d2d":"There are 87 attributes, with that many models will work pretty quickly."}}