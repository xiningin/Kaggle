{"cell_type":{"a2da66c3":"code","a763a53d":"code","2b91c404":"code","35665264":"code","1bcace11":"code","4e65e725":"markdown","fcb56e07":"markdown","e3dc2175":"markdown","ad82618b":"markdown","91931306":"markdown"},"source":{"a2da66c3":"import pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import rankdata","a763a53d":"jc_train_df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\nprint(f\"jc_train_df:{jc_train_df.shape}\")\njc_test_df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv\")\n\ntemp_df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv\")\njc_test_df = jc_test_df.merge ( temp_df, on =\"id\")\nprint(f\"jc_test_df:{jc_test_df.shape}\")\njc_test_df = jc_test_df.query (\"toxic != -1\")\nprint(f\"jc_test_df:{jc_test_df.shape}\")\ndf = jc_train_df.append(jc_test_df)\n\n\ndf[\"toxic_flag\"] = df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].sum(axis=1)\ndf = df.rename(columns={'comment_text': 'text'})\n\n\n\n#undersample non toxic comments  on Toxic Comment Classification Challenge\nmin_len = (df['toxic_flag'] >= 1).sum() \ndf_y0_undersample = df[df['toxic_flag'] == 0].sample(n=int(min_len*2.5),random_state=201)\ndf = pd.concat([df[df['toxic_flag'] >= 1], df_y0_undersample])\n\ntoxic = 0.71\nsevere_toxic = 0.75\nobscene = 1.47\nthreat = 0.0\ninsult = 0.66\nidentity_hate = 1.36 \n\n\ndf['y'] = df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].max(axis=1)\ndf['y'] = df[\"y\"]+df['toxic']*toxic\ndf['y'] = df[\"y\"]+df['severe_toxic']*severe_toxic\ndf['y'] = df[\"y\"]+df['obscene']*obscene\ndf['y'] = df[\"y\"]+df['threat']*threat\ndf['y'] = df[\"y\"]+df['insult']*insult\ndf['y'] = df[\"y\"]+df['identity_hate']*identity_hate\ny = df['y'].values","2b91c404":"vec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(3, 5) )\n\nX = vec.fit_transform(df['text'])\n\nX.shape","35665264":"model = Ridge(alpha = 1.0)\nmodel.fit(X, df['y'])\n\n\n### validate\ndf_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\n\nX_less_toxic = vec.transform(df_val['less_toxic'])\nX_more_toxic = vec.transform(df_val['more_toxic'])\n\np1 = model.predict(X_less_toxic)\np2 = model.predict(X_more_toxic)\n\n# Validation Accuracy\n(p1< p2).mean()","1bcace11":"df_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nX_test = vec.transform(df_sub['text'])\nscore = model.predict(X_test)\n\n\n## to enforce unique values on score\ndf_sub['score'] = rankdata(score, method='ordinal')\n\ndf_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\n\ndf_sub.head()","4e65e725":"# Submission","fcb56e07":"# Model","e3dc2175":"# TF-IDF","ad82618b":"A variation of https:\/\/www.kaggle.com\/julian3833\/jigsaw-incredibly-simple-naive-bayes-0-768 ","91931306":"# Create train data\n\nUsing data from [Toxic Comment Classification Challenge](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge)\n\nThe target was multioutput, we turn it into linear,  using weighted toxic behaviors\n\nThe types of toxicity are: 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'"}}