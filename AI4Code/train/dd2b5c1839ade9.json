{"cell_type":{"c0bd2a16":"code","15bceadf":"code","8d328752":"code","6aaff3fc":"code","4cfbabb0":"code","95eb8afd":"code","65a5984d":"code","15fc402b":"code","eb3af554":"code","99322d86":"code","130f03b3":"code","448f2fdf":"code","b0cdfb13":"code","29ed5940":"code","1fcdf1ff":"code","636e4c98":"code","634036c0":"code","e0f32941":"code","7b932e74":"code","7a7ff9a3":"code","9c0056c1":"code","8ba11c6c":"code","59a5da5f":"code","fac65501":"code","8eae8763":"code","321d5af4":"code","570fd67d":"code","4b134b05":"code","a823a9ad":"code","625afedc":"code","5bba8b9c":"code","71781851":"code","fbaa80d5":"code","969770ef":"markdown","134b7418":"markdown","1742102a":"markdown","ccb58b15":"markdown","5113942b":"markdown","0674c1b9":"markdown","1bdd338c":"markdown","4fd73fc4":"markdown","327eec5a":"markdown","66c3a99f":"markdown","5abfe977":"markdown","b2b301b1":"markdown","f16d62c4":"markdown","72b5eaad":"markdown"},"source":{"c0bd2a16":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import PorterStemmer\nword_stemmer = PorterStemmer() \nlemmatizer = WordNetLemmatizer()\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","15bceadf":"import tensorflow as tf\nimport time","8d328752":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\npath = \"..\/input\/songgeneration\/dataset\/lyrics-ImagineDragons(2).txt\"                  \nreader = open(path,\"r\").read()\nsong_names=[]\nnum_songs = 0\nlines_in_song= []\nnum_words_in_line=[]\nnum_words_in_song = []\nwords_in_song=[]\nnum_lines=0\ntotal_words = []","6aaff3fc":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\nfor line in reader.split(\"\\n\"):\n  if line.startswith(\"-\"):\n    song_names.append(line.split(\"-\")[1])\n    num_words_in_line.append([])\n    words_in_song.append([])\n    num_songs+=1\n    lines_in_song.append(num_lines-2)\n    num_lines=0\n    continue\n  num_lines+=1\n  for word in line.split(\" \"):\n    if word.isdigit():\n      continue\n    word = word.lower()\n    word = word_stemmer.stem(word)\n    words_in_song[num_songs-1].append(word)\n  if line.split(\" \") != \"\":\n    num_words_in_line[num_songs-1].append(len(line.split(\" \")))\n\nlines_in_song.append(num_lines)\nlines_in_song.remove(lines_in_song[0])\nnum_words_in_line = [i[0:len(i)-2] for i in num_words_in_line]\nwords_in_song = [i[0:len(i)-2] for i in words_in_song]\nnum_words_in_song = [len(x) for x in words_in_song]\n\n\nlines_count = np.mean(lines_in_song).round().astype(\"int32\")\nwords_count= np.mean(num_words_in_song).round().astype(\"int32\")","4cfbabb0":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\nno_punct = \"\"\npunctuations = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\nfor i,song in enumerate(words_in_song):\n  for j,word in enumerate(song):\n    for ch in word:\n      if ch not in punctuations:\n        no_punct = no_punct + ch\n    words_in_song[i][j] = no_punct\n    total_words.append(no_punct)\n    no_punct=\"\"\n\ndict_words = {}\nfor word in total_words:\n  if dict_words.get(word,0) ==0:\n    dict_words[word] =1\n  else:\n    dict_words[word] +=1\n\ndict_words_order = dict( sorted(dict_words.items(),\n                           key=lambda x: x[1]))\nvocab = set(total_words)\nvocab_len = len(vocab)\ntext = \"\"\nfor word in total_words:\n  text = text+ \" \"+word","95eb8afd":"##Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\nprint(\"Number of songs:\",num_songs)\nprint(\"Song names:\",song_names)\nprint(\"Lines in each song:\",lines_in_song)\n#print(\"Words in each line:\",num_words_in_line)\n#print(\"Words in each song:\",words_in_song)\n#print(\"Total words:\",total_words)\nprint(\"Total number of words:\",len(total_words))\n#print(\"Vocabulary:\",vocab)\nprint(\"vocabulary size:\",vocab_len)\nprint(\"Total number of lines:\",np.sum(lines_in_song))\nprint(\"Average number of lines in song:\",lines_count)\nprint(\"Average number of words in song:\",words_count)","65a5984d":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\nchar2idx = {u:i for i,u in enumerate(vocab)}\nidx2char = np.array(list(vocab))\ntext_as_int = np.array([char2idx[c] for c in text.split(\" \")[1:]])\nprint(idx2char)","15fc402b":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\nseq_len = words_count\nexamples_per_epoch = len(total_words)\/\/(seq_len+1)\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\nsequences = char_dataset.batch(seq_len+1,drop_remainder =True)\n'''for item in sequences.take(5):\n  print(repr(' '.join(idx2char[item.numpy()])))'''\ndef split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","eb3af554":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\n#print a data sample\n'''for input_example, target_example in  dataset.take(1):\n    print('Input data: ', repr(' '.join(idx2char[input_example.numpy()])))\n    print('Target data:', repr(' '.join(idx2char[target_example.numpy()])))\n    for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))'''","99322d86":"BATCH_SIZE=8\nBUFFER_SIZE=500\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)\ndataset","130f03b3":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\nembedding_dim = 256\nrnn_units = 1024\ndef create_model(vocab_size,embedding_dim,rnn_units,batch_size):\n  model = tf.keras.Sequential([\n          tf.keras.layers.Embedding(vocab_size,embedding_dim,\n                                     batch_input_shape = [batch_size,None]),\n          tf.keras.layers.LSTM(rnn_units,return_sequences= True,stateful =True,recurrent_initializer = \"glorot_uniform\"),\n          tf.keras.layers.Dense(vocab_size)\n  ])\n  return model\nprint(vocab_len)","448f2fdf":"model = create_model(\n    vocab_size=vocab_len,\n    embedding_dim=embedding_dim,\n    rnn_units=rnn_units,\n    batch_size=BATCH_SIZE)","b0cdfb13":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\nfor input, target in dataset.take(1):\n  example = model(input)\n  sample = tf.random.categorical(example[0], num_samples=1)\n  sample = tf.squeeze(sample,axis=-1).numpy()","29ed5940":"# summary of the model layers and weights\nmodel.summary()","1fcdf1ff":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\n#loss function\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_loss = loss(target, example)\nprint(\"Prediction shape: \", example.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"scalar_loss:      \", example_loss.numpy().mean())","636e4c98":"model.compile(optimizer='adam', loss=loss)\ncheckpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)\nEPOCHS = 25\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","634036c0":"tf.train.latest_checkpoint(checkpoint_dir)","e0f32941":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\nmodel = create_model(vocab_len, embedding_dim, rnn_units, batch_size=1)\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel.build(tf.TensorShape([1, None]))\nmodel.save(\".\/generate_text.h5\")","7b932e74":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\ndef generate_text(model, start_string,words_count):\n    num_generate = words_count\n    input_eval = char2idx[start_string.lower()]\n    input_eval = tf.expand_dims(input_eval, 0)\n    text_generated = []\n    temperature = 1.0\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model.predict(input_eval)\n        predictions = tf.squeeze(predictions, 0)\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        input_eval = tf.expand_dims([predicted_id], 0)\n        text_generated.append(idx2char[predicted_id])\n    return (start_string +\" \"+ ' '.join(text_generated))","7a7ff9a3":"song_raw=generate_text(model,\"mathematic\",words_count) #Choose one word from the text. I chose Mathematic.\nprint(song_raw)","9c0056c1":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\nwords_per_line =[]\nfor i in num_words_in_line:\n  for j in i:\n    words_per_line.append(j)\nseq_len = lines_count\nexamples_per_epoch = len(words_per_line)\/\/(seq_len+1)\n\nprint(examples_per_epoch)\nprint(seq_len)\n\nwords_per_line_str = [str(i) for i in words_per_line]\nfor i in range(max(words_per_line)):\n  if i+1 not in words_per_line:\n    words_per_line_str.append(str(i+1))\nvocab = sorted(set(words_per_line_str))\n\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\ntext_as_int = np.array([char2idx[c] for c in words_per_line_str])","8ba11c6c":"char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\nsequences = char_dataset.batch(seq_len+1, drop_remainder=True)","59a5da5f":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\ndef split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\ndataset = sequences.map(split_input_target)\nBATCH_SIZE = 2\nBUFFER_SIZE = 50\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\nvocab_size = max(words_per_line)","fac65501":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\nembedding_dim = 64\nrnn_units = 512\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                                  batch_input_shape=[batch_size, None]),\n        tf.keras.layers.LSTM(rnn_units,\n                            return_sequences=True,\n                            stateful=True,\n                            recurrent_initializer='glorot_uniform'),\n        tf.keras.layers.Dense(vocab_size)\n    ])\n    return model\nmodel_lines = build_model(\n    vocab_size=len(vocab),\n    embedding_dim=embedding_dim,\n    rnn_units=rnn_units,\n    batch_size=BATCH_SIZE)\n\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","8eae8763":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\nmodel_lines.compile(optimizer='adam', loss=loss)\n\ncheckpoint_dir = '.\/training_checkpoints_lines'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)\nEPOCHS = 15\nhistory = model_lines.fit(dataset, epochs=EPOCHS,steps_per_epoch=1, callbacks=[checkpoint_callback])","321d5af4":"print(vocab)","570fd67d":"model_lines.summary()","4b134b05":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\ntf.train.latest_checkpoint(checkpoint_dir)\nmodel_lines = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\nmodel_lines.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel_lines.build(tf.TensorShape([1, None]))\nmodel_lines.save(\".\/generate_lines.h5\")","a823a9ad":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\ndef generate_text(model, start_string,lines_count):\n    num_generate = lines_count\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n    lines_generated = []\n    lines_generated.append(start_string)\n    temperature = 1.0\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model.predict(input_eval)\n        predictions = tf.squeeze(predictions, 0)\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        input_eval = tf.expand_dims([predicted_id], 0)\n        lines_generated.append(idx2char[np.array(input_eval)[0][0]])\n    return lines_generated","625afedc":"words_per_line=generate_text(model_lines,\"5\",lines_count)\nprint(words_per_line)","5bba8b9c":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data\n\nlines = [int(i) for i in words_per_line]\nsong_words = song_raw.split(\" \")\ncount=0\nfinal_song = \"\"\nfor i in lines:\n  for j in range(i):\n    count+=1\n    if count> len(song_words):\n      break\n    final_song = final_song+ \" \"\n    final_song= final_song+ song_words[count-1]\n  final_song+=\"\\n\"","71781851":"print(final_song)","fbaa80d5":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thank you Shanmukh @shanmukh05 for the script.' )","969770ef":"#Shanmukh's script is so perfect. I've only changed ayy to Mathematic from the lyrics.  ","134b7418":"#Loading latest checkpoint we created","1742102a":"#Generating Text","ccb58b15":"#Remove Punctuations.","5113942b":"#FINAL SONG (use the above two models to get the final song).","0674c1b9":"#Model with Glorot uniform initializer, also called Xavier uniform initializer.\n\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/v2.4.1\/tensorflow\/python\/keras\/initializers\/initializers_v2.py#L509-L546\n\nhttps:\/\/keras.io\/api\/layers\/initializers\/","1bdd338c":"#Code by Shanmukh https:\/\/www.kaggle.com\/shanmukh05\/song-generation-with-rnn\/data","4fd73fc4":"#Dataset Preparation","327eec5a":"Above model just creates words in song, but we need it line wise, below model will fulfill tis need.\n\n#GENERATE NUM WORDS PER LINE","66c3a99f":"#Preprocessing of lyrics file","5abfe977":"#RNN and LSTM\n\n\"Simple multi-layered neural networks are classifiers which when given a certain input, tag the input as belonging to one of the many classes.\"\n\n\"RNNs are very apt for sequence classification problems and the reason they\u2019re so good at this is that they\u2019re able to retain important data from the previous inputs and use that information to modify the current output.\"\n\n\"Long Short Term Memory is a RNN architecture which addresses the problem of training over long sequences and retaining memory. LSTMs solve the gradient problem by introducing a few more gates that control access to the cell state.\"\n\nhttps:\/\/becominghuman.ai\/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow-1907a5bbb1fa","b2b301b1":"#Be prepared to wait for the snippet below.","f16d62c4":"#Character to ID conversion and vice versa.","72b5eaad":"![](https:\/\/miro.medium.com\/max\/2560\/1*bf5ibhobq2UZiy11UXjZeg.png)becominghuman.ai"}}