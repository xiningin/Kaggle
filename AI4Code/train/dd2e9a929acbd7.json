{"cell_type":{"7a95b520":"code","4cbe785f":"code","afd656f9":"code","762fda17":"code","ae79073d":"code","56486044":"code","a9c8bd9d":"code","a695f502":"code","c84259df":"code","46b93f26":"code","23686ba5":"code","2f046af1":"code","8bf7db1c":"code","5d4ff4a9":"code","8ef9866b":"code","5fc7436b":"code","9a71f928":"code","5e9bb840":"code","f29ea56e":"code","5e30aec7":"code","b3e53059":"code","fd678d81":"code","be34fb14":"code","73a316a5":"code","c1ff63a0":"code","1cb70774":"code","69f4fdd3":"code","ee9e6772":"code","077ef504":"code","5b5c8276":"code","b4e431fb":"code","b1f628a6":"code","07a3cab3":"code","38641816":"code","17d29fe8":"markdown","68fd3a2b":"markdown","af2ddf35":"markdown","afb40d72":"markdown","62a43efb":"markdown","cf1a835a":"markdown","1f355e3d":"markdown","fc83564b":"markdown","f8aa9610":"markdown","3b16073f":"markdown","8271a55d":"markdown"},"source":{"7a95b520":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport pprint\nimport joblib\nfrom functools import partial\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classifiers\nimport lightgbm as lgb\n\n# Model selection\nfrom sklearn.model_selection import KFold\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, DeltaYStopper\nfrom skopt.space import Real, Categorical, Integer","4cbe785f":"# Loading data \nX = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")","afd656f9":"# Preparing data as a tabular matrix\ny = X.song_popularity\nX = X.set_index('id').drop('song_popularity', axis='columns')\nX_test = X_test.set_index('id')","762fda17":"# Dealing with categorical data\ncategoricals = [item for item in X.columns if 'cat' in item]\ncat_values = np.unique(X[categoricals].values)\ncat_dict = dict(zip(cat_values, range(len(cat_values))))\n\nX[categoricals] = X[categoricals].replace(cat_dict).astype('category')\nX_test[categoricals] = X_test[categoricals].replace(cat_dict).astype('category')","ae79073d":"# Reporting util for different optimizers\ndef report_perf(optimizer, X, y, title=\"model\", callbacks=None):\n    \"\"\"\n    A wrapper for measuring time and performances of different optmizers\n    \n    optimizer = a sklearn or a skopt optimizer\n    X = the training set \n    y = our target\n    title = a string label for the experiment\n    \"\"\"\n    start = time()\n    \n    if callbacks is not None:\n        optimizer.fit(X, y, callback=callbacks)\n    else:\n        optimizer.fit(X, y)\n        \n    d=pd.DataFrame(optimizer.cv_results_)\n    best_score = optimizer.best_score_\n    best_score_std = d.iloc[optimizer.best_index_].std_test_score\n    best_params = optimizer.best_params_\n    \n    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n           + u\"\\u00B1\"+\" %.3f\") % (time() - start, \n                                   len(optimizer.cv_results_['params']),\n                                   best_score,\n                                   best_score_std))    \n    print('Best parameters:')\n    pprint.pprint(best_params)\n    print()\n    return best_params","56486044":"from sklearn.metrics import roc_auc_score\n","a9c8bd9d":"# Setting the scoring function\nscoring = make_scorer(partial(roc_auc_score), \n                      greater_is_better=True)","a695f502":"from sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=7,\n                      shuffle=True, \n                      random_state=0)\n\ncv_strategy = list(skf.split(X, y))\n\n","c84259df":"# Setting the basic regressor\nreg = lgb.LGBMRegressor(boosting_type='gbdt',\n                        metric='rmse',\n                        objective='regression',\n                        n_jobs=1, \n                        verbose=-1,\n                        random_state=0)","46b93f26":"# Setting the search space\nsearch_spaces = {\n    'learning_rate': Real(0.01, 1.0, 'log-uniform'),     # Boosting learning rate\n    'n_estimators': Integer(30, 5000),                   # Number of boosted trees to fit\n    'num_leaves': Integer(2, 512),                       # Maximum tree leaves for base learners\n    'max_depth': Integer(-1, 256),                       # Maximum tree depth for base learners, <=0 means no limit\n    'min_child_samples': Integer(1, 256),                # Minimal number of data in one leaf\n    'max_bin': Integer(100, 1000),                       # Max number of bins that feature values will be bucketed\n    'subsample': Real(0.01, 1.0, 'uniform'),             # Subsample ratio of the training instance\n    'subsample_freq': Integer(0, 10),                    # Frequency of subsample, <=0 means no enable\n    'colsample_bytree': Real(0.01, 1.0, 'uniform'),      # Subsample ratio of columns when constructing each tree\n    'min_child_weight': Real(0.01, 10.0, 'uniform'),     # Minimum sum of instance weight (hessian) needed in a child (leaf)\n    'reg_lambda': Real(1e-9, 100.0, 'log-uniform'),      # L2 regularization\n    'reg_alpha': Real(1e-9, 100.0, 'log-uniform'),       # L1 regularization\n   }","23686ba5":"# Wrapping everything up into the Bayesian optimizer\nopt = BayesSearchCV(estimator=reg,                                    \n                    search_spaces=search_spaces,                      \n                    scoring=scoring,                                  \n                    cv=cv_strategy,                                           \n                    n_iter=60,                                        # max number of trials\n                    n_points=3,                                       # number of hyperparameter sets evaluated at the same time\n                    n_jobs=-1,                                        # number of jobs\n                    iid=False,                                        # if not iid it optimizes on the cv score\n                    return_train_score=False,                         \n                    refit=False,                                      \n                    optimizer_kwargs={'base_estimator': 'GP'},        # optmizer parameters: we use Gaussian Process (GP)\n                    random_state=0)                                   # random state for replicability","2f046af1":"# Running the optimizer\noverdone_control = DeltaYStopper(delta=0.0001)               # We stop if the gain of the optimization becomes too small\ntime_limit_control = DeadlineStopper(total_time=60 * 60*1) # We impose a time limit (1 hours)\n\nbest_params = report_perf(opt, X, y,'LightGBM_regression', \n                          callbacks=[overdone_control, time_limit_control])","8bf7db1c":"# Transferring the best parameters to our basic regressor  \nreg = lgb.LGBMRegressor(boosting_type='gbdt',\n                        metric='rmse',   \n                        objective='binary',\n                        n_jobs=1, \n                        verbose=-1,\n                        random_state=0,\n                         **best_params)","5d4ff4a9":"# Fitting the regressor on all the data\nreg.fit(X, y) ## binary","8ef9866b":"# Preparing the submission\nsubmission = pd.DataFrame({'id':X_test.index, \n                           'song_popularity': reg.predict(X_test).ravel()})\n\nsubmission.to_csv(\"submission_nk_stratify.csv\", index = False)","5fc7436b":"submission_nk = pd.read_csv(\"submission_nk_stratify.csv\")","9a71f928":"submission_nk.head()","5e9bb840":"# Transferring the best parameters to our basic regressor\nreg2 = lgb.LGBMRegressor(boosting_type='gbdt',\n                        metric='rmse',   \n                        objective='regression',\n                        n_jobs=1, \n                        verbose=-1,\n                        random_state=0,\n                         **best_params)","f29ea56e":"reg2.fit(X, y)  ##regression","5e30aec7":"# Preparing the submission\nsubmission = pd.DataFrame({'id':X_test.index, \n                           'song_popularity': reg2.predict(X_test).ravel()})\n\nsubmission.to_csv(\"submission_nk2_startify.csv\", index = False)","b3e53059":"submission_nk_2 = pd.read_csv(\"submission_nk2_stratify.csv\")","fd678d81":"submission_nk_2.head()\n","be34fb14":"#sub = pd.DataFrame({'id':X_test.index,\n#   'song_popularity': (submission_nk_2['song_popularity'].values *0.5 + submission_nk['song_popularity'].values *0.5)})\n#sub.to_csv(\"meansub.csv\", index = False)","73a316a5":"#sub","c1ff63a0":"# Setting the validation strategy\nkf = KFold(n_splits=5, shuffle=True, random_state=0)","1cb70774":"# Wrapping everything up into the Bayesian optimizer\nopt2 = BayesSearchCV(estimator=reg,                                    \n                    search_spaces=search_spaces,                      \n                    scoring=scoring,                                  \n                    cv=kf,                                           \n                    n_iter=60,                                        # max number of trials\n                    n_points=3,                                       # number of hyperparameter sets evaluated at the same time\n                    n_jobs=-1,                                        # number of jobs\n                    iid=False,                                        # if not iid it optimizes on the cv score\n                    return_train_score=False,                         \n                    refit=False,                                      \n                    optimizer_kwargs={'base_estimator': 'GP'},        # optmizer parameters: we use Gaussian Process (GP)\n                    random_state=0)                                   # random state for replicability","69f4fdd3":"# Running the optimizer\noverdone_control = DeltaYStopper(delta=0.0001)               # We stop if the gain of the optimization becomes too small\ntime_limit_control = DeadlineStopper(total_time=60 * 60*1) # We impose a time limit (1 hours)\n\nbest_params = report_perf(opt2, X, y,'LightGBM_regression', \n                          callbacks=[overdone_control, time_limit_control])","ee9e6772":"# Transferring the best parameters to our basic regressor  \nreg = lgb.LGBMRegressor(boosting_type='gbdt',\n                        metric='rmse',   \n                        objective='binary',\n                        n_jobs=1, \n                        verbose=-1,\n                        random_state=0,\n                         **best_params)","077ef504":"reg.fit(X, y)  ##regression","5b5c8276":"submission = pd.DataFrame({'id':X_test.index, \n                           'song_popularity': reg2.predict(X_test).ravel()})\n\nsubmission.to_csv(\"submission_nk_kf.csv\", index = False)","b4e431fb":"submission","b1f628a6":"# Transferring the best parameters to our basic regressor  \nreg2 = lgb.LGBMRegressor(boosting_type='gbdt',\n                        metric='rmse',   \n                        objective='regression',\n                        n_jobs=1, \n                        verbose=-1,\n                        random_state=0,\n                         **best_params)","07a3cab3":"reg2.fit(X, y)  ##regression","38641816":"submission = pd.DataFrame({'id':X_test.index, \n                           'song_popularity': reg2.predict(X_test).ravel()})\n\nsubmission.to_csv(\"submission_nk2_kf.csv\", index = False)","17d29fe8":"First, we create a wrapper function to deal with running the optimizer and reporting back its best results.","68fd3a2b":"https:\/\/www.kaggle.com\/robikscube\/don-t-overfit-ii-first-look-and-eda","af2ddf35":"Finally we runt the optimizer and wait for the results. We have set some limits to its operations: we required it to stop if it cannot get consistent improvements from the search (DeltaYStopper) and time dealine set in seconds (we decided for 6 hours).","afb40d72":"We set up a generic LightGBM regressor.","62a43efb":"# 1. Data preparation","cf1a835a":"# Prediction on test data","1f355e3d":"We set up a  5-fold cross validation","fc83564b":"# Setting up optimization","f8aa9610":"We define a search space, expliciting the key hyper-parameters to optimize and the range where to look for the best values.\n","3b16073f":"We then define the Bayesian optimization engine, providing to it our LightGBM, the search spaces, the evaluation metric, the cross-validation. We set a large number of possible experiments and some parallelism in the search operations.","8271a55d":"Having got the best hyperparameters for the data at hand, we instantiate a lightGBM using such values and train our model on all the available examples.\n\nAfter having trained the model, we predict on the test set and we save the results on a csv file."}}