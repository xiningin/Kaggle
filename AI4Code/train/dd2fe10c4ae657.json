{"cell_type":{"8e04ae13":"code","5c84ea14":"code","116aa1d5":"code","184ae75e":"code","853f7722":"code","ca29a4b5":"code","b114d151":"code","7c7a55a5":"code","43503eaf":"code","2f0cdaa7":"code","454d7876":"code","3b5dc136":"code","5a1c2b16":"code","c9510d23":"code","48c060a8":"code","2ffc25d4":"code","7d774a54":"code","8cf68c77":"code","57b2b9d2":"code","6454806e":"code","aa825143":"code","87279446":"code","f548ba12":"markdown","2b1265e2":"markdown","9b115c9e":"markdown","585579f0":"markdown","5910d358":"markdown","aa995fca":"markdown","418dc030":"markdown","3912f92b":"markdown","7cd9425a":"markdown","e04a2d45":"markdown","5f2280fc":"markdown","e1e7dada":"markdown","7a141199":"markdown","acac7edb":"markdown","20ff8782":"markdown","5913bdf7":"markdown"},"source":{"8e04ae13":"import os\nimport torch\nfrom torchtext import data","5c84ea14":"SEED = 1234\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","116aa1d5":"os.listdir(\"..\/input\/imbd-sentiment-classification-dataset\/data\")","184ae75e":"TEXT = data.Field()\nLABEL = data.LabelField(dtype=torch.float)\n\nfields = {'text': ('text', TEXT), 'label': ('label', LABEL)}\n\ntrain_data, test_data = data.TabularDataset.splits(\n    path = '..\/input\/imbd-sentiment-classification-dataset\/data',\n    train = 'train.json',\n    test = 'test.json',\n    format = 'json',\n    fields = fields\n)","853f7722":"import random\ntrain_data, valid_data = train_data.split(random_state = random.seed(SEED))","ca29a4b5":"idx = 4\nprint(\"text :\",\" \".join(train_data[idx].__dict__['text']) )\nprint(\"\\nlabel :\",train_data[idx].__dict__['label'] )","b114d151":"MAX_VOCAB_SIZE = 25000\nTEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\nLABEL.build_vocab(train_data)","7c7a55a5":"print(len(TEXT.vocab))\nprint(len(LABEL.vocab))","43503eaf":"TEXT.vocab.stoi","2f0cdaa7":"LABEL.vocab.stoi","454d7876":"BATCH_SIZE = 64\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"device : {device}\")\ntrain_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n    (train_data,valid_data,test_data),\n    batch_size = BATCH_SIZE,\n    sort = False,\n    device = device, # last comma is optional\n)","3b5dc136":"import torch.nn as nn\n\nclass RNN(nn.Module):\n\n    def __init__(self,input_dim, embedding_dim, hidden_dim, output_dim):\n        super().__init__()\n\n        self.embedding = nn.Embedding(input_dim, embedding_dim)\n        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n        self.fc = nn.Linear(hidden_dim,output_dim)\n\n    def forward(self, text):\n        embedded = self.embedding(text)\n        output, hidden = self.rnn(embedded)\n\n        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n\n        return self.fc(hidden.squeeze(0))\n","5a1c2b16":"INPUT_DIM = len(TEXT.vocab)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\n\nmodel = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)","c9510d23":"def count_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"No. of trainable parameters : {count_params(model):,}\")","48c060a8":"import torch.optim as optim\noptimizer = optim.SGD(model.parameters(), lr=1e-3)","2ffc25d4":"criterion = nn.BCEWithLogitsLoss()","7d774a54":"model = model.to(device)\ncriterion = criterion.to(device)","8cf68c77":"def bin_acc(preds, y):\n    rounded = torch.round(torch.sigmoid(preds))\n    correct = (rounded==y).float()\n    return sum(correct)\/len(correct)","57b2b9d2":"def train(model, iterator, optimizer, criterion):\n\n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()    \n\n    for batch in iterator:\n        optimizer.zero_grad()\n\n        predictions = model(batch.text).squeeze(1)\n        loss = criterion(predictions, batch.label)\n        acc = bin_acc(predictions, batch.label)\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss\n        epoch_acc += acc\n\n    return epoch_loss\/len(iterator), epoch_acc\/len(iterator)","6454806e":"def evaluate(model, iterator, criterion):\n\n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()    \n    with torch.no_grad():\n        for batch in iterator:\n\n            predictions = model(batch.text).squeeze(1)\n            loss = criterion(predictions, batch.label)\n            acc = bin_acc(predictions, batch.label)\n\n            epoch_loss += loss\n            epoch_acc += acc\n\n    return epoch_loss\/len(iterator), epoch_acc\/len(iterator)","aa825143":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","87279446":"N_EPOCHS = 5\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n    \n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'simple_rnn.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')","f548ba12":"Creating an instance of the model class with specific hyper-parameters","2b1265e2":"splitting the train data into train and validation sets","9b115c9e":"Setting the loss function","585579f0":"Simple RNN model in Pytorch","5910d358":"an example of train data","aa995fca":"Loading loss function and model to the device","418dc030":"Setting up the optimizer","3912f92b":"Here I would show you just the setup. Therefore it would perform very badly.\nYou need to change this notebook (optimizer, model, etc.) to build a good classifier.","7cd9425a":"loading the dataset","e04a2d45":"function to calculate accuracy metrics","5f2280fc":"The training Loop","e1e7dada":"setting the seed","7a141199":"some imports","acac7edb":"Here I would show you\n* How to load this dataset using torchtext\n* how to build vocabulary using torchtext which are super handy\n* how to build a simple rnn based model using pytorch\n* how to train it for sentiment classification task","20ff8782":"building vocabulary out of train data only\n* it automatically creates mapping from words to numericals and vice cersa","5913bdf7":"Creating Iterators to load batches of data during training"}}