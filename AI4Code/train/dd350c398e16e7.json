{"cell_type":{"aaec06ce":"code","fa4c234a":"code","2309ea60":"code","88bfaab6":"code","5aa15178":"code","a6dd8714":"code","1be819f6":"code","5bf583d3":"code","6eba21a7":"code","632e3a15":"code","1ae22b8f":"code","b15729b5":"code","15d15b3f":"code","d9f5b7a6":"code","39ef3dee":"code","837b4437":"code","14bfdc97":"code","505b64c5":"markdown","f877165d":"markdown","40187d85":"markdown","80636286":"markdown","815a21ef":"markdown","859cff1f":"markdown","7dcffc2e":"markdown","fb383a61":"markdown","b4276b2d":"markdown","62980648":"markdown","78613c0c":"markdown","c409d07b":"markdown","b5c1e172":"markdown"},"source":{"aaec06ce":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom sklearn.model_selection import StratifiedKFold\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os","fa4c234a":"path = \"..\/input\/contradictory-my-dear-watson\"\nos.listdir(path)","2309ea60":"df_train = pd.read_csv(os.path.join(path,\"train.csv\"))\ndf_test = pd.read_csv(os.path.join(path,\"test.csv\"))\ndf_train['origin'] = 'ori'\ndf_test['origin'] = 'ori'\ntrain_trans = pd.read_csv('..\/input\/contradictorydatasettranslate\/train_translate_all.csv')\ntrain_trans['origin'] = 'trans' \ndf_train = pd.concat([df_train, train_trans], axis=0).reset_index()","88bfaab6":"df_train.head()","5aa15178":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","a6dd8714":"MIXED_PRECISION = False\nXLA_ACCELERATE = False\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","1be819f6":"MODEL = 'jplu\/tf-xlm-roberta-base'\nEPOCHS = 10\nMAX_LEN = 96\n\n# Our batch size will depend on number of replic\nBATCH_SIZE= 16 * strategy.num_replicas_in_sync\nAUTO = tf.data.experimental.AUTOTUNE\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","5bf583d3":"def lang_embding(lang, trans):\n    langc = ['English', 'French', 'Thai', 'Turkish', 'Urdu', 'Russian',\n           'Bulgarian', 'German', 'Arabic', 'Chinese', 'Hindi', 'Swahili',\n           'Vietnamese', 'Spanish', 'Greek']\n    lang_code = ['0000', '0001', '0010', '0011', '0100', '0101', '0110', '0111',\n                '1000', '1001', '1010', '1011', '1100', '1101', '1110', '1111']\n    lang_code = dict(zip(langc, lang_code))\n    trans_code = {'ori':'0', 'trans':'1'}\n\n    enc = lang_code[lang] + trans_code[trans]\n\n    vec = [int(i) for i in enc]\n    return vec","6eba21a7":"lang_embding('English', 'ori')","632e3a15":"def quick_encode(df,maxlen=100):\n    \n    values = df[['premise','hypothesis']].values.tolist()\n    lang_emb = [lang_embding(row['language'], row['origin']) for index,row in df.iterrows()]\n    tokens=tokenizer.batch_encode_plus(values,max_length=maxlen,pad_to_max_length=True)\n    return np.array(tokens['input_ids']), np.array(lang_emb)\n\nx_train, x_lang = quick_encode(df_train)\nx_test, x_test_lang = quick_encode(df_test)\ny_train = df_train.label.values","1ae22b8f":"def create_dataset(X,y ,val=False, batch_size= BATCH_SIZE):\n    dataset = tf.data.Dataset.from_tensor_slices((X,y)).shuffle(len(X))\n    if not val:\n        dataset = dataset.repeat().batch(batch_size).prefetch(AUTO)\n    else:\n        dataset = dataset.batch(batch_size).prefetch(AUTO)\n    return dataset\ntest_dataset = (tf.data.Dataset.from_tensor_slices(((x_test, x_test_lang)))).batch(BATCH_SIZE)\n        ","b15729b5":"from tensorflow.keras.layers import Concatenate\n\ndef build_model(transformer, max_len):\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n    input_lang = Input(shape=(5,), dtype=tf.float32, name='language_tag')\n    sequence_output = transformer(input_ids)[0]\n    cls_token = sequence_output[:,0,:]\n    cls_token = Concatenate()([cls_token, input_lang])\n    cls_token = Dense(32, activation='relu')(cls_token)\n    out = Dense(3, activation='softmax')(cls_token)\n    \n    model = Model(inputs = [input_ids,input_lang], outputs = out)\n    model.compile(Adam(lr=1e-5),\n                  loss = 'sparse_categorical_crossentropy', \n                  metrics=['accuracy'])\n    return model","15d15b3f":"pred_test = np.zeros((df_test.shape[0],3))\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nval_score = []\nhistory = []\nfor fold, (train_ind, valid_ind) in enumerate(skf.split(x_train, y_train)):\n        print(\"fold\",fold+1)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        train_data = create_dataset((x_train[train_ind], x_lang[train_ind]), y_train[train_ind],val=False)\n        valid_data = create_dataset((x_train[valid_ind], x_lang[valid_ind]), y_train[valid_ind],val=True)\n    \n        Checkpoint=tf.keras.callbacks.ModelCheckpoint(f\"roberta_base.h5\", monitor='val_loss', verbose=0, save_best_only=True,\n        save_weights_only=True, mode='min')\n        \n        with strategy.scope():\n            transformer_layer = TFAutoModel.from_pretrained(MODEL)\n            model = build_model(transformer_layer, max_len=MAX_LEN)\n        n_steps = len(train_ind)\/\/BATCH_SIZE\n        print(\"training model {} \".format(fold+1))\n\n        train_history = model.fit(\n        train_data,\n        steps_per_epoch=n_steps,\n        validation_data=valid_data,\n        epochs=EPOCHS,callbacks=[Checkpoint],verbose=1)\n        \n        print(\"Loading model...\")\n        model.load_weights(f\"roberta_base.h5\")\n        print(\"fold {} validation accuracy {}\".format(fold+1,np.mean(train_history.history['val_accuracy'])))\n        print(\"fold {} validation loss {}\".format(fold+1,np.mean(train_history.history['val_loss'])))\n        \n        history.append(train_history)\n\n        val_score.append(np.mean(train_history.history['val_accuracy']))\n        \n        print('predict on test....')\n        preds=model.predict((x_test, x_test_lang),verbose=1)\n        pred_test+=preds\/5","d9f5b7a6":"plt.figure(figsize=(15,10))\n\nfor i,hist in enumerate(history):\n\n    plt.subplot(3,2,i+1)\n    plt.plot(np.arange(EPOCHS),hist.history['accuracy'],label='train accu')\n    plt.plot(np.arange(EPOCHS),hist.history['val_accuracy'],label='validation acc')\n    plt.gca().title.set_text(f'Fold {i+1} accuracy curve')\n    plt.legend()","39ef3dee":"submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\nsubmission.prediction = np.argmax(pred_test, axis=1)\nsubmission.head()","837b4437":"submission.to_csv('submission.csv', index=False)","14bfdc97":"submission['prob0'] = pred_test[:,0]\nsubmission['prob1'] = pred_test[:,1]\nsubmission['prob2'] = pred_test[:,2]\nsubmission.to_csv('submission_prob.csv', index=False)","505b64c5":"## Model","f877165d":"## Install package","40187d85":"## EDA NOTEBOOK: https:\/\/www.kaggle.com\/doanquanvietnamca\/contradictory-my-dear-watson-eda","80636286":"## Multilingualism \nis an interesting issue today. After jsaw, kaggle organized this contest to develop NLP community in multilingual processing. I try to build the simplest model that can handle this task quickly and without consuming too much computational resources. The use of the single language paradigm was not focused on me. So I combine information from language and words to make this model. Initial results are very good. \n> Upvote me if you like it!","815a21ef":"## BERT MODEL: https:\/\/www.kaggle.com\/doanquanvietnamca\/trainning-mbert-tpu-pytorch","859cff1f":"## TPU CONFIG","7dcffc2e":"## Encode text","fb383a61":"## Plot val\/loss ","b4276b2d":"## ENSEMBLE: https:\/\/www.kaggle.com\/doanquanvietnamca\/ensemble-submission-csv ","62980648":"## Use translated Tag TO HAVE MORE INFORMATION","78613c0c":"## Mixed Precision and\/or XLA\nThe following booleans can enable mixed precision and\/or XLA on GPU\/TPU. By default TPU already uses some mixed precision but we can add more. These allow the GPU\/TPU memory to handle larger batch sizes and can speed up the training process. The Nvidia V100 GPU has special Tensor Cores which get utilized when mixed precision is enabled. Unfortunately Kaggle's Nvidia P100 GPU does not have Tensor Cores to receive speed up.","c409d07b":"![graph](https:\/\/github.com\/manhlab\/datascience-kaggle-experence\/blob\/master\/data\/graph..png?raw=true)","b5c1e172":"## Define model"}}