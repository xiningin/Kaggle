{"cell_type":{"d836cd80":"code","a0623287":"code","33842260":"code","5662f202":"code","4bbe2845":"code","0951d7ef":"code","5edc3a4c":"code","4a0fa74f":"code","6baa2888":"code","1132db67":"code","3ac4bc34":"code","d0d05e10":"code","b458743e":"code","09e0c4fb":"code","76a5fe05":"code","2fa9136b":"code","623111f5":"code","cd585e5d":"code","ba6f0d82":"code","e7852755":"code","138b37f4":"markdown","b3c0e308":"markdown","b6c02179":"markdown","9cea4dae":"markdown","acad69fc":"markdown","22c80c08":"markdown","f39ce2e4":"markdown","ef805744":"markdown","15d9023a":"markdown","ce1cb0d7":"markdown","3e3ade08":"markdown","d6c69d66":"markdown","3dba6fbf":"markdown","d4989bcc":"markdown","1b461f69":"markdown"},"source":{"d836cd80":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf = data.copy()\n\ndisplay(df.head())\ndf.info()","a0623287":"sns.countplot(df['DEATH_EVENT'], palette=['blue', 'red'])\nplt.title('Target Feature Counts', fontsize=20);","33842260":"plt.pie(df['sex'].value_counts().values, \n        labels=['Men', 'Women'], \n        colors=['cyan', 'pink'], \n        autopct='%1.f%%', \n        shadow=True, \n        startangle=45, \n        textprops={'fontsize':25});","5662f202":"sns.countplot(df['smoking'],\n              palette=['orange', 'brown'])\nplt.title('Smokers and Non-smokers Counts', fontsize=20);","4bbe2845":"print('Age Statistics of the Patients' + '\\n\\n' + str(df.age.describe()))","0951d7ef":"sns.boxplot(df.age)\nplt.title('Age Statistics of the Patients', fontsize=20);","5edc3a4c":"plt.figure(figsize=(15,6))\nsns.countplot(df['age'], hue=df['smoking'], palette=['blue', 'red'], alpha=0.7)\nplt.title(\"Age and Smoking\", fontsize=20)\nplt.xticks(rotation=90)\nplt.yticks(list(range(0,27,3)))\nplt.grid();","4a0fa74f":"plt.figure(figsize=(11,11))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation of the Features', fontsize=20);","6baa2888":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ndf_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\ndf_scaled.head()","1132db67":"X = df_scaled.drop('DEATH_EVENT', axis=1).values\ny = df_scaled['DEATH_EVENT'].values","3ac4bc34":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n\nprint('X_train : ', X_train.shape)\nprint('y_train : ', y_train.shape)\nprint('X_test  : ', X_test.shape)\nprint('y_test  : ', y_test.shape)","d0d05e10":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n# Defining the model creation and evaluation function, so we don't have to write it again and again.\n\ndef model_and_eval(max_features, n_estimators, random_state):\n    rf = RandomForestClassifier(max_features=max_features, random_state=random_state)\n    rf.fit(X_train, y_train)\n    y_pred = rf.predict(X_test)\n    \n    # Printing the model scores:\n    print('Mean accuracy  : %.2f' % accuracy_score(y_test, y_pred))\n    print('Mean precision : %.2f' % precision_score(y_test, y_pred))\n    print('Mean recall    : %.2f' % recall_score(y_test, y_pred))\n    print('Mean f1 score  : %.2f' % f1_score(y_test, y_pred))\n    \n    # Creating the confusion matrix:\n    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, annot_kws={\"fontsize\":20}, fmt='d', cbar=False, cmap='PuBu')\n    plt.title('Confusion Matrix of the Model', color='navy', fontsize=15)\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Actual Values');","b458743e":"model_and_eval(max_features='auto', n_estimators=100, random_state=10)","09e0c4fb":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'n_estimators' : [10, 15, 20, 30, 50, 100, 150], # Number of decision trees\n              'max_features' : [0.5, 2, 5, 10, 12]}            # Number of features to consider at each split\n\nrf = RandomForestClassifier(random_state=10)\n\ngs = GridSearchCV(rf, param_grid, cv=10, n_jobs=-1)\n\ngs.fit(X_train, y_train)\n\nprint('Best Parameter ', gs.best_params_)","76a5fe05":"model_and_eval(max_features=gs.best_params_['max_features'], n_estimators=gs.best_params_['n_estimators'], random_state=10)","2fa9136b":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\n\nft_imp = pd.Series(rf.feature_importances_, index=df.iloc[:,:12].columns).sort_values()\n\nft_imp.plot(kind='barh')\nplt.title('Feature Importance', fontsize=20);","623111f5":"X_new = df_scaled[['time', 'serum_creatinine', 'ejection_fraction', 'age', 'creatinine_phosphokinase', 'platelets', 'serum_sodium', 'smoking']]\n\nX_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=10)","cd585e5d":"from sklearn.model_selection import GridSearchCV\n\nn_estimators = list(range(1, 101))\n\nparam_grid = {'n_estimators' : n_estimators,\n              'max_features' : [2, 5, 10, 12]}\n\nrf = RandomForestClassifier(random_state=42)\n\ngs = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1)\n\ngs.fit(X_train, y_train)\n\nscores = gs.cv_results_['mean_test_score']\n\nprint('Best Parameter ', gs.best_params_)","ba6f0d82":"best_x = gs.best_params_['n_estimators']\nbest_y = gs.cv_results_['mean_test_score'][gs.best_params_['n_estimators']-1]\n\nplt.figure(figsize=(15,5))\nsns.lineplot(n_estimators, scores[:100], color='navy')\nplt.plot(best_x, best_y, marker='o', markersize=8, color=\"red\", label='best_param')\nplt.xlabel('n_estimators')\nplt.ylabel('Accuracy')\nplt.title('Random Forest n_estimators and Accuracy Plot', fontsize=20)\nplt.xticks(np.arange(0, 100, 5), rotation=45)\nplt.grid();","e7852755":"model_and_eval(max_features=2 , n_estimators=25 , random_state=10)","138b37f4":"Split our dataset into train and test samples.","b3c0e308":"# EDA:\n\nFirst of all I want to see the target feature's counts.","b6c02179":"Grid search cross validation gave us '44' as the best n_estimator. But we can see from the graph above that it's unnecessary to build our random forest model with 44 decision tress. It is wiser to choose a lower n_estimator with a good mean accuracy, so we don't have to use up that much computation power for our model. I am going to choose '25' as my optimum n_estimator.","9cea4dae":"## Feature Importance and Increasing the Performance\n\nI want to see the important features for the random forest model.","acad69fc":"Let's see the tuned model with the best parameters:","22c80c08":"What is the age distribution of the patients?","f39ce2e4":"## Default Model","ef805744":"**As you can see from above, we could obtain similar accuracy, precision, recall and f1 score with only 8 features and 25 tress. That means, with less computation power, we can still obtain the similar results.**","15d9023a":"Let's see our default model. I used default max_features and n_estimators values, and used random_state to have the same results each run.","ce1cb0d7":"## Model Tuning","3e3ade08":"# HEART FAILURE CLASSIFICATION\n\nWith this study, I am going to use Random Forest Classifier model to predict mortality by heart failure.\n\n* First of all I will use data visualization technics to understand our dataset.  \n\n* Then I will use the random forest classifier with its' default parameters and make some predictions. We will see the accuracy, precision, recall, f1 scores for our default model. And I will demonstrate the results with a confusion matrix.\n\n* Later I will tune the default model with the n_estimators (number of trees) and max_features (number of features for the best split) parameters by using gridsearch cross validation technic.\n\n* We will again evaluate the tuned model and see for any improvement.\n\n* It is not always good idea to use the best parameters. The best parameters could improve our accuracy (or precision, recall etc.), but at the same time we should consider about the computation power and the model performance. For some application it could be wiser to choose some other values for parameters in spite of the decrease in the model scores.\n\n* For the optimum model, I will investigate the important features. Then I will use only those features for the new model. \n\n* Also I will plot the n_estimator parameters and the model scores to find an optimum n_estimator value similar to the best parameter value. \n\n* By using the less features and a smaller n_estimator (number of trees), our random forest model will perform faster but still with good model scores.","d6c69d66":"It looks like the dataset is unbalanced.\n\nWhat are the percentages of the male and the female patients?","3dba6fbf":"From the heatmap above, we can see the strongly correlated features with the 'DEATH_EVENT'. Those features are **'age', 'ejection_fraction', 'serum_creatinine', 'serum_sodium', 'time'**. Later in this study I will investigate the important features from the random forest model. And we will see the similarity of the features.\n\nFor the ease of calculations I want to scale the dataset.","d4989bcc":"Let's use only the important features to decrease our model's calculations and increase our model's performance.","1b461f69":"Now it's time to seperate the features and the target variable."}}