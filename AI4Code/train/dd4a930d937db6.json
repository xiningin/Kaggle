{"cell_type":{"68985922":"code","8cf61efc":"code","c473d05d":"code","1869686a":"code","652d037f":"code","295c6329":"code","0df42b07":"code","091532a7":"code","99e5bcba":"code","fdb95405":"code","ee9618cb":"code","944dfcef":"code","950122fe":"code","59b01980":"code","5086308a":"code","8cdff1ae":"code","8c3d31d8":"code","79dde69d":"code","5b1d5478":"code","43470868":"code","2b4b3065":"code","a154f4e0":"code","2ed97cda":"code","9070bc77":"code","7b44a8a0":"code","b82449e1":"code","7bb5435e":"code","16c4357c":"code","fa2ea051":"code","f039207d":"code","8bdae36c":"code","7d82442f":"code","5c09753b":"code","79ad1232":"code","a0789c99":"code","87815029":"code","1d4ea306":"code","f87d273c":"code","17a7f99a":"code","e4f96549":"code","e8e3d808":"code","d6c6d802":"code","6e5e478a":"code","b2cde948":"markdown","06bdb585":"markdown","8f08ed07":"markdown","0af57300":"markdown","af4b1ed7":"markdown","ef8a1ffc":"markdown","65cb8543":"markdown","56f8fe0e":"markdown","436980d2":"markdown","abf31176":"markdown","f3017c5f":"markdown","5291d29d":"markdown","81a83e38":"markdown","881273a7":"markdown","5a1d1145":"markdown","f0a48ea7":"markdown","61df04b3":"markdown","aacc0ac3":"markdown","41d60f9d":"markdown","2253db82":"markdown"},"source":{"68985922":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndf = pd.read_csv('..\/input\/creditcard.csv')\ndf.head()","8cf61efc":"df.describe()","c473d05d":"# Good No Null Values!\ndf.isnull().sum().max()","1869686a":"df.columns","652d037f":"# The classes are heavily skewed\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')\n\nprint(df['Class'].value_counts())","295c6329":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('Class', data=df, palette=colors)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)","0df42b07":"from sklearn.preprocessing import StandardScaler, RobustScaler\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\ndf.drop(['Time','Amount'], axis=1, inplace=True)\nscaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\ndf.head()","091532a7":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"train len=\", len(train_index), \"Test:\", test_index, \"test len=\", len(test_index))\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\nprint(\"##len x train\" , original_Xtrain.shape)\nprint(\"##len x test\" , original_Xtest.shape)\n\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint(train_counts_label)\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","99e5bcba":"df = df.sample(frac=1) # shuffled randomly before creating the subsamples\n\n# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","fdb95405":"print('Distribution of the Classes in the subsample dataset')\nprint(new_df['Class'].value_counts()\/len(new_df))\n\nsns.countplot('Class', data=new_df, palette=colors)\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()\n\nprint(new_df.shape)","ee9618cb":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire DataFrame\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\nax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\nplt.show()","944dfcef":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V17\", data=new_df, palette=colors, ax=axes[0])\naxes[0].set_title('V17 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=colors, ax=axes[1])\naxes[1].set_title('V14 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=colors, ax=axes[2])\naxes[2].set_title('V12 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, palette=colors, ax=axes[3])\naxes[3].set_title('V10 vs Class Negative Correlation')\n\nplt.show()","950122fe":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V11\", data=new_df, palette=colors, ax=axes[0])\naxes[0].set_title('V11 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=new_df, palette=colors, ax=axes[1])\naxes[1].set_title('V4 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=new_df, palette=colors, ax=axes[2])\naxes[2].set_title('V2 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V19\", data=new_df, palette=colors, ax=axes[3])\naxes[3].set_title('V19 vs Class Positive Correlation')\n\nplt.show()","59b01980":"# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\nv14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\nv14_cut_off = v14_iqr * 1.5\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V10 outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\nprint('----' * 44)\n\n# -----> V12 removing outliers from fraud transactions\nv12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 Lower: {}'.format(v12_lower))\nprint('V12 Upper: {}'.format(v12_upper))\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\nprint('V12 outliers: {}'.format(outliers))\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))\nprint('----' * 44)\n\n# Removing outliers V10 Feature\nv10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\nv10_iqr = q75 - q25\n\nv10_cut_off = v10_iqr * 1.5\nv10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\nprint('V10 Lower: {}'.format(v10_lower))\nprint('V10 Upper: {}'.format(v10_upper))\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\nprint('V10 outliers: {}'.format(outliers))\nprint('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))","5086308a":"f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))\n\ncolors = ['#B3F9C5', '#f9c5b3']\n# Boxplots with outliers removed\n# Feature V14\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df, ax=ax1, palette=colors)\nax1.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\nax1.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature 12\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=ax2, palette=colors)\nax2.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\nax2.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.3), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature V10\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=ax3, palette=colors)\nax3.set_title(\"V10 Feature \\n Reduction of outliers\", fontsize=14)\nax3.annotate('Fewer extreme \\n outliers', xy=(0.95, -16.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\nplt.show()","8cdff1ae":"len(new_df)","8c3d31d8":"X = new_df.drop('Class', axis=1)\ny = new_df['Class']","79dde69d":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","5b1d5478":"classifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}","43470868":"from sklearn.model_selection import cross_val_score\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", key, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","2b4b3065":"from sklearn.model_selection import GridSearchCV\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n# logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_ \nprint(log_reg)\n\n# KNears best estimator\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\nknears_neighbors = grid_knears.best_estimator_ \nprint(knears_neighbors)\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\nsvc = grid_svc.best_estimator_\nprint(svc)\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\ntree_clf = grid_tree.best_estimator_\nprint(tree_clf)","a154f4e0":"log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')","2ed97cda":"undersample_X = df.drop('Class', axis=1)\nundersample_y = df['Class']\n\nfor train_index, test_index in sss.split(undersample_X, undersample_y):\n    print(\"Train samples:\", len(train_index), \"Test samples:\", (test_index))\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    break\n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values\n\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\nfor train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n    print(len(train), len(test))\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) # SMOTE happens during Cross Validation not before..\n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n\n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(undersample_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(undersample_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(undersample_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(undersample_ytrain[test], undersample_prediction))\n    \nprint('=' * 100)\nprint(\"Accuracy\", np.mean(undersample_accuracy))\nprint(\"Precision\", np.mean(undersample_precision))\nprint(\"Recall\", np.mean(undersample_recall))\nprint(\"F1\", np.mean(undersample_f1))\nprint(\"AUC\", np.mean(undersample_auc))","9070bc77":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator, X, y, ylim, cv, n_jobs, plt, ax, title, train_sizes=np.linspace(.1, 1.0, 5)):\n    if ylim is not None:\n        plt.ylim(*ylim)\n\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1,color=\"#ff9124\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",label=\"Cross-validation score\")\n    ax.set_title(title, fontsize=14)\n    ax.set_xlabel('Training size (m)')\n    ax.set_ylabel('Score')\n    ax.grid(True)\n    ax.legend(loc=\"best\")\n    \ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\nplot_learning_curve(log_reg, X_train, y_train, (0.87, 1.01), cv, 4, plt, ax1, \"Logistic Regression Learning Curve\")\nplot_learning_curve(knears_neighbors, X_train, y_train, (0.87, 1.01), cv, 4, plt, ax2, \"Knears Neighbors Learning Curve\")\nplot_learning_curve(svc, X_train, y_train, (0.87, 1.01), cv, 4, plt, ax3, \"Support Vector Classifier \\n Learning Curve\")\nplot_learning_curve(tree_clf, X_train, y_train, (0.87, 1.01), cv, 4, plt, ax4, \"Decision Tree Classifier \\n Learning Curve\")","7b44a8a0":"print(len(X_train), len(y_train))","b82449e1":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5, method=\"decision_function\")\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5, method=\"decision_function\")\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)","7bb5435e":"from sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))","16c4357c":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\nplt.show()","fa2ea051":"from sklearn.metrics import precision_recall_curve\n\nprecision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)\nprint(precision.mean(), recall.mean(), threshold.mean() )","f039207d":"from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred = log_reg.predict(X_train)\n\n# Overfitting Case (this is subsampled data set vaules)\nprint('---' * 45)\nprint('Overfitting: \\n')\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\nprint('---' * 45)\n\n# How it should look like, (this is full data set values)\nprint('---' * 45)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\nprint('---' * 45)","8bdae36c":"undersample_y_score = log_reg.decision_function(original_Xtest)\nprint(undersample_y_score)","7d82442f":"from sklearn.metrics import average_precision_score\nundersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\nprint('Average precision-recall score: {0:0.2f}'.format(undersample_average_precision))","5c09753b":"from sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(12,6))\n\nprecision, recall, thresholds_ = precision_recall_curve(original_ytest, undersample_y_score)\n\nplt.step(recall, precision, color='#004a93', alpha=0.2, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2, color='#48a6ff')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n    undersample_average_precision), fontsize=16)","79ad1232":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\nprint('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n\n# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# Classifier with optimal parameters\nlog_reg_sm = LogisticRegression()\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n# Implementing SMOTE Technique \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('---' * 45)","a0789c99":"labels = ['No Fraud', 'Fraud']\nsmote_prediction = best_est.predict(original_Xtest)\nprint(classification_report(original_ytest, smote_prediction, target_names=labels))","87815029":"y_score = best_est.decision_function(original_Xtest)\n\naverage_precision = average_precision_score(original_ytest, y_score)\nprint('Average precision-recall score: {0:0.2f}'.format(average_precision))","1d4ea306":"fig = plt.figure(figsize=(12,6))\n\nprecision, recall, _ = precision_recall_curve(original_ytest, y_score)\n\nplt.step(recall, precision, color='r', alpha=0.2, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2, color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision), fontsize=16)","f87d273c":"# SMOTE Technique (OverSampling) After splitting and Cross Validating\nsm = SMOTE(ratio='minority', random_state=42)\n\n#Synthetic data created by SMOTE\nXsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)","17a7f99a":"# Logistic Regression\nt0 = time.time()\nlog_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm.fit(Xsm_train, ysm_train)\nt1 = time.time()\nprint(\"Fitting oversample data took :{} sec\".format(t1 - t0))","e4f96549":"from sklearn.metrics import confusion_matrix\n\n# Logistic Regression fitted using SMOTE technique\ny_pred_log_reg = log_reg_sm.predict(X_test)\n\n# Other models fitted with UnderSampling\ny_pred_knear = knears_neighbors.predict(X_test)\ny_pred_svc = svc.predict(X_test)\ny_pred_tree = tree_clf.predict(X_test)\n\nlog_reg_cf = confusion_matrix(y_test, y_pred_log_reg)\nkneighbors_cf = confusion_matrix(y_test, y_pred_knear)\nsvc_cf = confusion_matrix(y_test, y_pred_svc)\ntree_cf = confusion_matrix(y_test, y_pred_tree)\n\nfig, ax = plt.subplots(2, 2,figsize=(22,12))\n\nsns.heatmap(log_reg_cf, ax=ax[0][0], annot=True, cmap=plt.cm.copper)\nax[0, 0].set_title(\"Logistic Regression \\n Confusion Matrix\", fontsize=14)\nax[0, 0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0, 0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(kneighbors_cf, ax=ax[0][1], annot=True, cmap=plt.cm.copper)\nax[0][1].set_title(\"KNearsNeighbors \\n Confusion Matrix\", fontsize=14)\nax[0][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(svc_cf, ax=ax[1][0], annot=True, cmap=plt.cm.copper)\nax[1][0].set_title(\"Suppor Vector Classifier \\n Confusion Matrix\", fontsize=14)\nax[1][0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[1][0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(tree_cf, ax=ax[1][1], annot=True, cmap=plt.cm.copper)\nax[1][1].set_title(\"DecisionTree Classifier \\n Confusion Matrix\", fontsize=14)\nax[1][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[1][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nplt.show()\n\n#By definition a confusion matrix C is such that Cij is equal to the number of observations known to be in group  but predicted to be in group .\n#Thus in binary classification, the count of true negatives is C00, false negatives is C10, true positives is C11 and false positives is C01.","e8e3d808":"print(log_reg_cf)","d6c6d802":"from sklearn.metrics import classification_report\n\nprint('Logistic Regression:')\nprint(classification_report(y_test, y_pred_log_reg))\n\nprint('KNears Neighbors:')\nprint(classification_report(y_test, y_pred_knear))\n\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, y_pred_svc))\n\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, y_pred_tree))","6e5e478a":"# Final Score in the test set of logistic regression\nfrom sklearn.metrics import accuracy_score\n\n# Logistic Regression with Under-Sampling\ny_pred = log_reg.predict(X_test)\nundersample_score = accuracy_score(y_test, y_pred)\n\n# Logistic Regression with SMOTE Technique (Better accuracy with SMOTE)\ny_pred_sm = best_est.predict(original_Xtest)\noversample_score = accuracy_score(original_ytest, y_pred_sm)\n\nd = {'Technique': ['Random UnderSampling', 'Oversampling (SMOTE)'], 'Score': [undersample_score, oversample_score]}\nfinal_df = pd.DataFrame(data=d)\n\n# Move column\nscore = final_df['Score']\nfinal_df.drop('Score', axis=1, inplace=True)\nfinal_df.insert(1, 'Score', score)\n\nfinal_df","b2cde948":"### Boxplot After removing outliers","06bdb585":"## Scalling Time and Amount features","8f08ed07":"### Undersampling with NearMiss","0af57300":"### Measuresing accuracy of each model with its unseen test data samples","af4b1ed7":"## Random Under-Sampling:\n\n\nIn this phase of the project we will implement *\"Random Under Sampling\"* which basically consists of removing data in order to have a more <b> balanced dataset <\/b> and thus avoiding our models to overfitting. contaning 50% fraud and 50% non fraud","ef8a1ffc":"### UnderSampling\n\n\n## Learning Curves:\n<ul>\n<li>The <b>wider the  gap<\/b>  between the training score and the cross validation score, the more likely your model is <b>overfitting (high variance)<\/b>.<\/li>\n<li> If the score is low in both training and cross-validation sets<\/b> this is an indication that our model is <b>underfitting (high bias)<\/b><\/li>\n<li><b> Logistic Regression Classifier<\/b>  shows the best score in both training and cross-validating sets.<\/li>\n<\/ul>","65cb8543":"### imbalanace of data is vishualized","56f8fe0e":"### SMOTE Technique (Over-Sampling):\n\n<b>SMOTE<\/b> stands for Synthetic Minority Over-sampling Technique.  Unlike Random UnderSampling, SMOTE creates new synthetic points in order to have an equal balance of the classes. This is another alternative for solving the \"class imbalance problems\". <br><br>","436980d2":"### Removing outliers","abf31176":"### Analyzing outliers","f3017c5f":"# Test Data with Logistic Regression:","5291d29d":"Before it was like \nClassifiers:  LogisiticRegression Has a training score of 94.0 % accuracy score|\nClassifiers:  KNearest Has a training score of 93.0 % accuracy score|\nClassifiers:  Support Vector Classifier Has a training score of 93.0 % accuracy score|\nClassifiers:  DecisionTreeClassifier Has a training score of 89.0 % accuracy score|","81a83e38":"### using grid search to find best parameters on above split train set of the 984 subsample","881273a7":"### Learning cureves of each classifiers with subsampled data","5a1d1145":"##  Equally Distributing and Correlating: \n","f0a48ea7":"### cross validation score of best estimators:  data = train split of sub sample","61df04b3":"### cross validation score of each classifier with subsampled data","aacc0ac3":"### Splitting the Data (Original DataFrame)\n  ","41d60f9d":"Length of X (train): 227846 | Length of y (train): 227846\nLength of X (test): 56961 | Length of y (test): 56961\n---------------------------------------------------------------------------------------------------------------------------------------*\n\naccuracy: 0.9417200820315653\nprecision: 0.05876332710591692\nrecall: 0.9111002921129504\nf1: 0.10871673527050744","2253db82":"<h3> Correlation Matrices <\/h3>"}}