{"cell_type":{"50272e7d":"code","151edc44":"code","d02cbc83":"code","7b00c579":"code","7e5a30b7":"code","7fa91046":"code","d57ff461":"code","bd5ae99c":"code","65a7d4ce":"code","96e843ca":"code","95cca82e":"code","14e1d914":"code","b827ccdd":"code","8230adb3":"code","c4be4fd2":"code","fd23e2f3":"code","1d44f8ee":"code","6cc030c4":"code","ef41b18a":"code","11de6db2":"code","1cc037a3":"code","291859fd":"code","9187b782":"code","e7ee1633":"code","c630f3c1":"code","deed0588":"code","f1560def":"code","31217825":"code","7cfc266a":"code","118fb655":"code","05b97ea5":"code","3bfd80c9":"code","51bce4cc":"code","b958b837":"code","db2f0cd6":"code","3022d23d":"code","a164d763":"code","caf92e6d":"code","4334fce0":"code","e6e84310":"code","ff91fc3a":"code","8bb07982":"code","0e3b8655":"code","675c868b":"code","5cff363b":"code","537f45e2":"code","c0f7daf3":"code","d38ec533":"code","601da3dd":"code","69dbbc6b":"code","7122e6ab":"code","30a49c50":"code","f8cae9b0":"code","79b92823":"code","d6dd07ea":"code","64c9b8cd":"code","b63be874":"code","a59474f8":"code","858261d7":"code","9b56a0c7":"code","9890bd34":"code","4cb57fcc":"code","7a5e9753":"markdown","cb129368":"markdown","ce855093":"markdown","93acb662":"markdown","b9f6ca28":"markdown","2bad102c":"markdown","78144e49":"markdown","e3cb753a":"markdown","eef8aaec":"markdown","53af855f":"markdown","ec64cc2c":"markdown","40ab0c36":"markdown","1d0a0a0f":"markdown","d7f0a66e":"markdown","e4c68147":"markdown","22af1d55":"markdown","9a9a79cd":"markdown","1905a0eb":"markdown","af7f06f0":"markdown","19e4c711":"markdown","90a7d8ce":"markdown","d0d78ea9":"markdown","7bcdda86":"markdown","d0e75323":"markdown","e3c58ea2":"markdown","270cd7ff":"markdown","11d950ea":"markdown","50755c66":"markdown","30a04c2a":"markdown","1adb4c70":"markdown","fdcb9b97":"markdown","88938d78":"markdown","2910529f":"markdown","29dda1b0":"markdown","54db2af7":"markdown","fa01b382":"markdown","f4ea329c":"markdown","a8d7a4da":"markdown","2bb486bd":"markdown","8ac8d27a":"markdown","a6da1956":"markdown","54930c62":"markdown","18faf390":"markdown","e6f16934":"markdown","af987db8":"markdown","f35d6189":"markdown","edc95773":"markdown","26f333b6":"markdown","c4510234":"markdown","3767011d":"markdown","a2f7f4bc":"markdown","1966cd8d":"markdown","449ebc48":"markdown","2de5211e":"markdown","4ac08075":"markdown","73969d22":"markdown","1ca78f47":"markdown","b3431dc2":"markdown","b9b1c1b4":"markdown","051ce7fa":"markdown","e9fd6426":"markdown","acb1ab71":"markdown","6b9f7949":"markdown","9d8b4400":"markdown","9db64e80":"markdown","d3b11032":"markdown","e79c64fc":"markdown","3e5899e8":"markdown","37ca1d6e":"markdown","1e416a67":"markdown","6a53dade":"markdown","f6dc6450":"markdown","341724f5":"markdown"},"source":{"50272e7d":"# imports\nimport os, sys\n\n# third party imports\nimport numpy as np\nimport keras.layers","151edc44":"# local imports.\nsys.path.append('\/kaggle\/input\/voxelmorph\/voxelmorph\/ext\/pynd-lib\/')\nsys.path.append('\/kaggle\/input\/voxelmorph\/voxelmorph\/ext\/pytools-lib\/')\nsys.path.append('\/kaggle\/input\/voxelmorph\/voxelmorph\/ext\/neuron\/')\nsys.path.append('\/kaggle\/input\/voxelmorph\/voxelmorph\/')\nimport voxelmorph as vxm\nimport neuron","d02cbc83":"# import\n# You should most often have this import together with all other imports at the top, \n# but we include here here explicitly to show where data comes from\nfrom keras.datasets import mnist","7b00c579":"# load the data. \n# `mnist.load_data()` already splits our data into train and test.  \n# (x_train_load, y_train_load), (x_test_load, y_test_load) = mnist.load_data()\n\n# unfortunately the above seems to fail on the keras kernel\n# so we will load it from a pre-downloaded mnist numpy file\nmnist_file = '\/kaggle\/input\/learn2reg-mnist\/mnist.npz'\nx_train_load = np.load(mnist_file)['x_train']\ny_train_load = np.load(mnist_file)['y_train']\nx_test_load = np.load(mnist_file)['x_test']\ny_test_load = np.load(mnist_file)['y_test']\n\n# extract only instances of the digit 5\nx_train = x_train_load[y_train_load==5, ...]\ny_train = y_train_load[y_train_load==5]\nx_test = x_test_load[y_test_load==5, ...]\ny_test = y_test_load[y_test_load==5]\n\n# let's get some shapes to understand what we loaded.\nprint('shape of x_train: ', x_train.shape)\nprint('shape of y_train: ', y_train.shape)","7e5a30b7":"nb_val = 1000 # keep 10,000 subjects for validation\nx_val = x_train[-nb_val:, ...]  # this indexing means \"the last nb_val entries\" of the zeroth axis\ny_val = y_train[-nb_val:]\nx_train = x_train[:-nb_val, ...]\ny_train = y_train[:-nb_val]","7fa91046":"nb_vis = 5\n\n# choose nb_vis sample indexes\nidx = np.random.choice(x_train.shape[0], nb_vis, replace=False)\nexample_digits = [f for f in x_train[idx, ...]]\n\n# plot\nneuron.plot.slices(example_digits, cmaps=['gray'], do_colorbars=True);","d57ff461":"# fix data\nx_train = x_train.astype('float')\/255\nx_val = x_val.astype('float')\/255\nx_test = x_test.astype('float')\/255\n\n# verify\nprint('training maximum value', x_train.max())","bd5ae99c":"# re-visualize\nexample_digits = [f for f in x_train[idx, ...]]\nneuron.plot.slices(example_digits, cmaps=['gray'], do_colorbars=True);","65a7d4ce":"pad_amount = ((0, 0), (2,2), (2,2))\n\n# fix data\nx_train = np.pad(x_train, pad_amount, 'constant')\nx_val = np.pad(x_val, pad_amount, 'constant')\nx_test = np.pad(x_test, pad_amount, 'constant')\n\n# verify\nprint('shape of training data', x_train.shape)","96e843ca":"ndims = 2\nvol_shape = x_train.shape[1:]\nnb_enc_features = [32, 32, 32, 32]\nnb_dec_features = [32, 32, 32, 32, 32, 16]","95cca82e":"# first, let's get a unet (before the final layer)\nunet = vxm.networks.unet_core(vol_shape, nb_enc_features, nb_dec_features);","14e1d914":"# inputs\nprint('numer of inputs', len(unet.inputs))\nmoving_input_tensor = unet.inputs[0]\nfixed_input_tensor = unet.inputs[1]\n    \n# output\nprint('output:', unet.output)","b827ccdd":"# transform the results into a flow field.\ndisp_tensor = keras.layers.Conv2D(ndims, kernel_size=3, padding='same', name='disp')(unet.output)\n\n# check\nprint('displacement tensor:', disp_tensor)","8230adb3":"# a cool aspect of keras is that we can easily form new models via tensor pointers:\ndef_model = keras.models.Model(unet.inputs, disp_tensor)\n# def_model will now *share layers* with the UNet -- if we change layer weights \n# in the UNet, they change in the def_model ","c4be4fd2":"spatial_transformer = neuron.layers.SpatialTransformer(name='spatial_transformer')\n\n# warp the image\nmoved_image_tensor = spatial_transformer([moving_input_tensor, disp_tensor])","fd23e2f3":"inputs = [moving_input_tensor, fixed_input_tensor]\noutputs = [moved_image_tensor, disp_tensor]\nvxm_model = keras.models.Model(inputs, outputs)","1d44f8ee":"# losses. Keras recognizes the string 'mse' as mean squared error, so we don't have to code it\nlosses = ['mse', vxm.losses.Grad('l2').loss]\n\n# usually, we have to balance the two losses by a hyper-parameter.\nlambda_param = 0.05\nloss_weights = [1, lambda_param]","6cc030c4":"vxm_model.compile(optimizer='Adam', loss=losses, loss_weights=loss_weights)","ef41b18a":"def vxm_data_generator(x_data, batch_size=32):\n    \"\"\"\n    generator that takes in data of size [N, H, W], and yields data for our vxm model\n    \n    Note that we need to provide numpy data for each input, and each output\n    \n    inputs:  moving_image [bs, H, W, 1], fixed_image [bs, H, W, 1]\n    outputs: moved_image  [bs, H, W, 1], zeros [bs, H, W, 2]\n    \"\"\"\n    # preliminary sizing\n    vol_shape = x_data.shape[1:] # extract data shape\n    ndims = len(vol_shape)\n    \n    # prepare a zero array the size of the deformation. We'll explain this below.\n    zero_phi = np.zeros([batch_size, *vol_shape, ndims])\n    \n    while True:\n        # prepare inputs\n        # inputs need to be of the size [batch_size, H, W, number_features]\n        #   number_features at input is 1 for us\n        idx1 = np.random.randint(0, x_data.shape[0], size=batch_size)\n        moving_images = x_data[idx1, ..., np.newaxis]\n        idx2 = np.random.randint(0, x_data.shape[0], size=batch_size)\n        fixed_images = x_data[idx2, ..., np.newaxis]\n        inputs = [moving_images, fixed_images]\n        \n        # outputs\n        # we need to prepare the \"true\" moved image.  \n        # Of course, we don't have this, but we know we want to compare \n        # the resulting moved image with the fixed image. \n        # we also wish to penalize the deformation field. \n        outputs = [fixed_images, zero_phi]\n        \n        yield inputs, outputs        ","11de6db2":"# let's test it\ntrain_generator = vxm_data_generator(x_train)\ninput_sample, output_sample = next(train_generator)\n\n# visualize\nslices_2d = [f[0,...,0] for f in input_sample + output_sample]\ntitles = ['input_moving', 'input_fixed', 'output_moved_ground_truth', 'zero']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);","1cc037a3":"nb_epochs = 10\nsteps_per_epoch = 100\nhist = vxm_model.fit_generator(train_generator, epochs=nb_epochs, steps_per_epoch=steps_per_epoch, verbose=2);","291859fd":"# as with other imports, this import should be at the top, or use notebook matplotlib magic\n# we keep it here to be explicit why we need it\nimport matplotlib.pyplot as plt\n\ndef plot_history(hist, loss_name='loss'):\n    \"\"\"\n    Quick function to plot the history \n    \"\"\"\n    plt.figure()\n    plt.plot(hist.epoch, hist.history[loss_name], '.-')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.show()\n\nplot_history(hist)","9187b782":"# let's get some data\nval_generator = vxm_data_generator(x_val, batch_size = 1)\nval_input, _ = next(val_generator)","e7ee1633":"val_pred = vxm_model.predict(val_input)","c630f3c1":"# %timeit is a 'jupyter magic' that times the given line over several runs\n%timeit vxm_model.predict(val_input)","deed0588":"# visualize\nslices_2d = [f[0,...,0] for f in val_input + val_pred]\ntitles = ['input_moving', 'input_fixed', 'predicted_moved', 'deformation_x']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);","f1560def":"neuron.plot.flow([val_pred[1].squeeze()], width=5);","31217825":"# extract only instances of the digit 5\nx_sevens = x_train_load[y_train_load==7, ...].astype('float')\/255\nx_sevens = np.pad(x_sevens, pad_amount, 'constant')\n\nseven_generator = vxm_data_generator(x_sevens, batch_size=1)\nseven_sample, _ = next(seven_generator)\nseven_pred = vxm_model.predict(seven_sample)","7cfc266a":"# visualize\nslices_2d = [f[0,...,0] for f in seven_sample + seven_pred]\ntitles = ['input_moving', 'input_fixed', 'predicted_moved', 'deformation_x']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);","118fb655":"factor = 5\nval_pred = vxm_model.predict([f*factor for f in val_input])\n\n# visualizeb\nslices_2d = [f[0,...,0] for f in val_input + val_pred]\ntitles = ['input_moving', 'input_fixed', 'predicted_moved', 'deformation_x']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);","05b97ea5":"# we've prepared the data in the following files\n# prepared as N x H x W\ncore_path = '\/kaggle\/input\/mri-2d\/'\nx_train = np.load(os.path.join(core_path, 'train_vols.npy'))\nx_val = np.load(os.path.join(core_path, 'validate_vols.npy'))\n# x_test = np.load(os.path.join(core_path, 'test_vols.npy'))\n\nvol_shape = x_train.shape[1:]\nprint('train shape:', x_train.shape)","3bfd80c9":"# extract some brains\nnb_vis = 5\nidx = np.random.randint(0, x_train.shape[0], [5,])\nexample_digits = [f for f in x_train[idx, ...]]\n\n# visualize\nneuron.plot.slices(example_digits, cmaps=['gray'], do_colorbars=True);","51bce4cc":"# unet\nunet = vxm.networks.unet_core(vol_shape, nb_enc_features, nb_dec_features);\ndisp_tensor = keras.layers.Conv2D(ndims, kernel_size=3, padding='same', name='disp')(unet.output)\n\n# spatial transfomer\nspatial_transformer = neuron.layers.SpatialTransformer(name='image_warping')\nmoved_image_tensor = spatial_transformer([unet.inputs[0], disp_tensor])\n\n# final model\nvxm_model = keras.models.Model(unet.inputs, [moved_image_tensor, disp_tensor])","b958b837":"# losses. Keras recognizes the string 'mse' as mean squared error, so we don't have to code it\nlosses = ['mse', vxm.losses.Grad('l2').loss]\n\n# usually, we have to balance the two losses by a hyper-parameter.\nlambda_param = 0.01\nloss_weights = [1, lambda_param]","db2f0cd6":"vxm_model.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss=losses, loss_weights=loss_weights)","3022d23d":"# let's test it\ntrain_generator = vxm_data_generator(x_train, batch_size=8)\ninput_sample, output_sample = next(train_generator)\n\n# visualize\nslices_2d = [f[0,...,0] for f in input_sample + output_sample]\ntitles = ['input_moving', 'input_fixed', 'output_sample_true', 'zero']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);","a164d763":"nb_epochs = 10\nsteps_per_epoch = 10","caf92e6d":"hist = vxm_model.fit_generator(train_generator, epochs=nb_epochs, steps_per_epoch=steps_per_epoch, verbose=2);","4334fce0":"# for the purpose of the tutorial we ran very few epochs.  \n# Here we load a model that was run for 10 epochs and 100 steps per epochs\nvxm_model.load_weights('\/kaggle\/input\/learn2reg-unsupervised-models\/brain_2d_shortrun.h5')","e6e84310":"# as before, let's visualize what happened\nplot_history(hist)","ff91fc3a":"val_generator = vxm_data_generator(x_val, batch_size = 1)","8bb07982":"val_input, _ = next(val_generator)","0e3b8655":"val_pred = vxm_model.predict(val_input)","675c868b":"# visualize\nslices_2d = [f[0,...,0] for f in val_input + val_pred]\ntitles = ['input_moving', 'input_fixed', 'predicted_moved', 'deformation_x']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);","5cff363b":"flow = val_pred[1].squeeze()[::3,::3]\nneuron.plot.flow([flow], width=5);","537f45e2":"vxm_model.save_weights('brain_2d_shortrun.h5')","c0f7daf3":"vxm_model.load_weights('\/kaggle\/input\/learn2reg-unsupervised-models\/brain_2d_shortrun.h5')\nour_val_pred = vxm_model.predict(val_input)\n\nvxm_model.load_weights('\/kaggle\/input\/learn2reg-unsupervised-models\/brain_2d_mseonly.h5')\nmse_val_pred = vxm_model.predict(val_input)","d38ec533":"# visualize both models\nslices_2d = [f[0,...,0] for f in [val_input[1]] + our_val_pred ]\ntitles = ['input_fixed', 'our_pred_moved', 'our_disp_x']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);\n\n# visualize both models\nslices_2d = [f[0,...,0] for f in [val_input[1]] + mse_val_pred]\ntitles = ['input_fixed', 'mse_pred_moved', 'mse_pred_x']\nneuron.plot.slices(slices_2d, titles=titles, cmaps=['gray'], do_colorbars=True);","601da3dd":"neuron.plot.flow([f[1].squeeze()[::3,::3] for f in [our_val_pred, mse_val_pred]], width=10);","69dbbc6b":"# our data will be of shape 160 x 192 x 224\nvol_shape = [160, 192, 224]\nndims = 3","7122e6ab":"nb_enc_features = [16, 32, 32, 32]\nnb_dec_features = [32, 32, 32, 32, 32, 16, 16]","30a49c50":"# unet\nunet = vxm.networks.unet_core(vol_shape, nb_enc_features, nb_dec_features);\ndisp_tensor = keras.layers.Conv3D(ndims, kernel_size=3, padding='same', name='disp')(unet.output)\n\n# spatial transfomer\nspatial_transformer = neuron.layers.SpatialTransformer(name='image_warping')\nmoved_image_tensor = spatial_transformer([unet.inputs[0], disp_tensor])\n\n# final model\nvxm_model = keras.models.Model(unet.inputs, [moved_image_tensor, disp_tensor])","f8cae9b0":"val_volume_1 = np.load('\/kaggle\/input\/learn2reg-mri-3d\/subject_1_vol.npz')['vol_data']\nseg_volume_1 = np.load('\/kaggle\/input\/learn2reg-mri-3d\/subject_1_seg.npz')['vol_data']\nval_volume_2 = np.load('\/kaggle\/input\/learn2reg-mri-3d\/atlas_norm_3d.npz')['vol']\nseg_volume_2 = np.load('\/kaggle\/input\/learn2reg-mri-3d\/atlas_norm_3d.npz')['seg']\n\n\nval_input = [val_volume_1[np.newaxis, ..., np.newaxis], val_volume_2[np.newaxis, ..., np.newaxis]]","79b92823":"vxm_model.load_weights('\/kaggle\/input\/learn2reg-unsupervised-models\/\/cvpr2018_vm2_cc.h5')","d6dd07ea":"val_pred = vxm_model.predict(val_input);","64c9b8cd":"moved_pred = val_pred[0].squeeze()\npred_warp = val_pred[1]","b63be874":"mid_slices_fixed = [np.take(val_volume_2, vol_shape[d]\/\/2, axis=d) for d in range(ndims)]\nmid_slices_fixed[1] = np.rot90(mid_slices_fixed[1], 1)\nmid_slices_fixed[2] = np.rot90(mid_slices_fixed[2], -1)\n\nmid_slices_pred = [np.take(moved_pred, vol_shape[d]\/\/2, axis=d) for d in range(ndims)]\nmid_slices_pred[1] = np.rot90(mid_slices_pred[1], 1)\nmid_slices_pred[2] = np.rot90(mid_slices_pred[2], -1)\nneuron.plot.slices(mid_slices_fixed + mid_slices_pred, cmaps=['gray'], do_colorbars=True, grid=[2,3]);","a59474f8":"warp_model = vxm.networks.nn_trf(vol_shape)","858261d7":"warped_seg = warp_model.predict([seg_volume_1[np.newaxis,...,np.newaxis], pred_warp])","9b56a0c7":"from pytools import plotting as pytools_plot\nimport matplotlib\n\n[ccmap, scrambled_cmap] = pytools_plot.jitter(255, nargout=2)\nscrambled_cmap[0, :] = np.array([0, 0, 0, 1])\nccmap = matplotlib.colors.ListedColormap(scrambled_cmap)","9890bd34":"mid_slices_fixed = [np.take(seg_volume_1, vol_shape[d]\/\/1.8, axis=d) for d in range(ndims)]\nmid_slices_fixed[1] = np.rot90(mid_slices_fixed[1], 1)\nmid_slices_fixed[2] = np.rot90(mid_slices_fixed[2], -1)\n\nmid_slices_pred = [np.take(warped_seg.squeeze(), vol_shape[d]\/\/1.8, axis=d) for d in range(ndims)]\nmid_slices_pred[1] = np.rot90(mid_slices_pred[1], 1)\nmid_slices_pred[2] = np.rot90(mid_slices_pred[2], -1)\n\nslices = mid_slices_fixed + mid_slices_pred\nfor si, slc  in enumerate(slices):\n    slices[si][0] = 255\nneuron.plot.slices(slices, cmaps = [ccmap], grid=[2,3]);","4cb57fcc":"%timeit vxm_model.predict(val_input)","7a5e9753":"# Preamble\n## Setup of environment","cb129368":"Note that separating your data in *only* train\/test **often leads to problems**   \nYou wouldn't want to iteratively (A) build a model, (B) train on training data, and (C) test on test data  \nDoing so will **overfit to you test set** (because you will have adapted your algorithm to your test data  \n\nWe will split the 'training' into 'train\/validate' data, and keep the test set for later  \nAnd will only look at the test data at the very end (once we're ready to submit the paper!)  ","ce855093":"# CNN Model","93acb662":"### Visualize Data","b9f6ca28":"### Model","2bad102c":"let's explore the model bit","78144e49":"~3ms per registration is quite fast, even for MNIST.  \nLet's visualize the results","e3cb753a":"---","eef8aaec":"Clearly, this is not converged, and you should run it to convergence.  \nFor the purposes of this tutorial, we'll move on.","53af855f":"One last change. Later on, we'll see that some of the most popular models  \nlike to have inputs that are sized as multiples of 2^N for N being the number of layers  \nHere, we force our images to be size 32 (2x 2^4)","ec64cc2c":"In a **supervised setting** we would have ground truth deformations $\\phi_{gt}$,  \nand we could use a supervised loss like MSE $= \\| \\phi - \\phi_{gt} \\|$","40ab0c36":"We will now register slightly more realistic data: MRIs of the brain.  \nTo be able to train and easily register during this tutorial,  \nwe will first extract the middle slice of brain scans. \n\nNothat because this task does not capture deformations in the third dimensions,  \ncertain  correspondances are not exactly possible.  Nonetheless, this exercise  \nwill illustrate registration with more realistic complex images.   \n\nThe brains have been intensity-normalized affinely aligned, and skull stripped  \nwith FreeSurfer, to enable focusing on deformable registration  ","1d0a0a0f":"As before, we create a model based on a unet, plus a spatial transformer","d7f0a66e":"Let's visualize the flow a bit better","e4c68147":"### Model","22af1d55":"# Generalization \nHow do learning-based methods generalize beyond training distribution ?","9a9a79cd":"To achieve (1), we need to *warp* input image $m$.  \nTo do this, we use a spatial transformation network layer, which essentially does linear interpolation  \nThis layer is defined in [neuron](https:\/\/github.com\/adalca\/neuron)","1905a0eb":"We're going to abstract the UNet for this tutorial and use a function from [voxelmorph](http:\/\/voxelmorph.mit.edu)  \n`vxm.networks.unet_core()` enables this","af7f06f0":"okay, we need to make sure the final output has 2 features,  \nrepresenting the deformation at each voxel","19e4c711":"# 3D MRI brain scan registration","90a7d8ce":"We'll start with some common imports  ","d0d78ea9":"Let's visualize the segmentations, and essentially make sure they are not crazy","7bcdda86":"Next, we import two packages that will help us   \n- [neuron](https:\/\/github.com\/adalca\/neuron) is a library for medical image analysis with tensorflow  \n- [voxelmorph](http:\/\/voxelmorph.mit.edu) is a deep-learning based registration library  ","d0e75323":"---","e3c58ea2":"interesting, it still works! So it **generalized beyond what we expected**. Why?  \nLocally, parts of the 7s look similar to the 5s, so the  registration algorithm still tries to match local neighborhoods.\n\nLet's try a different variation.  \nWhat if we just modify the (original) set, but multiplied the intensities by a factor?","270cd7ff":"# Registration","11d950ea":"Believeing we are done loading, it's always great to visualize the data  \nHere, we use some tools from a package called `neuron`, which uses matplotlib  \nYou could use matplotlib as well directly, but it would just be a bit messier  \nand here we want to illustrate the main concepts.  ","50755c66":"Let's register","30a04c2a":"To make sure the moved_image is close to the fixed image, and  \nto achieve smoothness loss of $\\phi$ in (2), we will want these two as outputs from the full model ","1adb4c70":"Given that the displacement $\\phi$ is output from the network,  \nwe need to figure out a loss to tell if it makes sense","fdcb9b97":"Looks good!  \n\nHowever, luckily we included a **colorbar**, which shows us that the data is in [0, 255].  \nIn neural networks it's often great to work in the ranges of [0, 1] or [-1, 1] or around there.  \nLet's fix this. \n\nIn general, you should always plot your data with colorbars, which helps you catch issues before training  ","88938d78":"Looks good, time to **train the model**","2910529f":"# Tutorial Moved\nThe tutorial has been moved, with a focus on **voxelmorph**, to http:\/\/tutorial.voxelmorph.net\/.   \nThis kaggle tutorial will still work, but we recommend you use the new tutorial.\n\n\n\n# Introduction to Unsupervised Deep Learning Registration\n[Adrian Dalca](http:\/\/adalca.mit.edu) for the [learn2reg 2019 tutorial](http:\/\/learn2reg.github.io). Introductory slides for this tutorial are [here](https:\/\/github.com\/learn2reg\/tutorials2019\/blob\/master\/slides\/Learn2reg_tutorial_unsupervided_AdrianDalca.pdf). \n\nUnsupervised is also often referred to end-to-end\/self-supervised\n\nPlease first run this jupyter notebook *within kaggle*, since all the paths and data have been set up.  \n\n### Outline\n- **Core concepts with MNIST**   \nWe will first learn to deal with data, building a model, training, registration and generalization\n- **More realistic complexity: Brain MRI (2D slices)**  \nWe will then show how these models work for 2d slices of brain scans, presenting a more complex scenario    \n- **Realistic 3D Brain MRI**  \nWe will illustrate full 3D registration\n- **Advances topics**  \nFinally, we close with more advanced topics, including diffeomorphisms and fine-tuning deformations","29dda1b0":"---","54db2af7":"# Data","fa01b382":"Evaluating registration results is tricky.  \nThe first tendancy is to look at the images (as above),  \nand conclude that if they match, The registration has succeeded. \n\nHowever, this can be achieved by an optimization that only penalizes the image matching term  \nFor example, in the next cell we compare our model with one that wastrained on maximizing MSE only (no smoothness loss)","f4ea329c":"let's see some results","a8d7a4da":"To train, we need to make sure the data is in the right format and fed to the model the way we want it  \nkeras models can be trained with `model.fit`, which requires all the data to be in a big array, or `model.fit_generator`, which requires a python generator that gives you batches of data\n\nLet's code a simple data generator based on the MNIST data","2bb486bd":"let's save out model weights, we'll use them in a bit","8ac8d27a":"# Train Model","a6da1956":"---","54930c62":"What we often do isntead of use **external anotations** for evaluation  \nOne way is using anatomical segmentations.  \n\nIn the next section, we demonstrate the use of a 3D model, and show how to evaluate it with segmentations ","18faf390":"### Loss","e6f16934":"The main idea in **unsupervised registration** is to use loss inspired by classical registration  \n\nWithout supervision, how do we know this deformation is good?  \n(1) make sure that $m \\circ \\phi$ ($m$ warped by $\\phi$) is close to $f$  \n(2) regularize $\\phi$ (often meaning make sure it's smooth)  ","af987db8":"---","f35d6189":"# Registration of Brain MRI","edc95773":"As with MNIST, we'll start with losses of 'mse' and spatial smoothing","26f333b6":"Throughout this tutorial we assume that the images have been rigidly aligned in a (roughly) similar space.  \nRigid alignment is also possible with unsupervised learning-based registration, but it's not our focus here.  ","c4510234":"In our tests, a run is 10s, for an entire 3D volume.  \nClassically, this would take hours.","3767011d":"This broke down! Why? In this case, the network has never seen even parts of this image.  ","a2f7f4bc":"We're going to start by registering 2D MNIST digits, and then move on to medical data later.  \nIf the data is small (like 2D MNIST), you can often load it in memory, which enables for faster training and testing.  \nIf the data is large (large 3D scans), we will need to load the scans on demand. More on this later...\n\nFirst, we're going to **load the data**  \nLuckily, MNIST comes with the keras framework, so we can just load it here as follows\n\n","1966cd8d":"## runtime","449ebc48":"Understanding when the network generalizes and when it does not is very important, and still a part of active research  ","2de5211e":"*Registration*: `predict()` essentially executes the network given an input.","4ac08075":"from experimentation, we have found the Adam optimizer learning rate of `1e-4`  \nperforms better than `1e-3` for this problem. ","73969d22":"It's always a good idea to visualize the loss, not just read off the numbers  \nThis will give us a better idea of whether it's converged, etc.  \nTensorflow offers a powerful interactive system for visualizing called tensorboard  \nFor this short tutorial, we will simply plot the loss","1ca78f47":"# Evaluation","b3431dc2":"Finally, we can compile the model. \nThis sets up the model for training, by associating the model with a loss and an optimizer","b9b1c1b4":"Let's look at the segmentations! To do this, we'll need to warp segmentations. ","051ce7fa":"Finally, let's define the actual loss. The way keras works, we need to define a loss for each output.    \nThe first loss is easy, it's simply MSE between the warped image $m \\circ \\phi$.\nFor the second, we will use a spatial gradient of the displacement.  \nWe won't code this from scratch here, but we'll use the `voxelmorph` implementation.","e9fd6426":"The 208 volumes are of size `160x192`.  \nLet's look at some","acb1ab71":"Given two images, out goal is to find the deformation between them  \n\nIn learning-based methods, we use a network that takes in two images $m$ and $f$ (e.g. MNIST digits of size 32x32)    \nand outputs a dense deformation $\\phi$ (e.g. size 32x32x2, because at each pixel we want a vector telling us where to go)  \n\n**Note**: registration also includes (or refers to) affine transforms, but we ignore that here  ","6b9f7949":"Finally, we get to 3D models, which are of particular interest in medical image analysis. \n\nHowever, due to the size of the models and data, we won't be able to train a model  \nwithin a short tutorial time. Instead, here we assume one has been trained, and demonstrate its use.\nYou can train one very similar to how you trained the 2D models above.","9d8b4400":"Load a trained 3D model","9db64e80":"### Validation data","d3b11032":"Luckily, we can use the same generator as before, since we're using the same format.  \nLet's test it first","e79c64fc":"With pair-wise optimization methods (like most classical methods),  \nto register a new pair you would need to optimize a deformation field.  \n\nWith learning based registration, we simply evaluate the network for a new input pair  ","3e5899e8":"and that's it! \n\nEven though this is on MNIST only, let's see how long this takes","37ca1d6e":"An important advantage of learning-based methods is the dramatically lowered runtime","1e416a67":"At first look, the mse-only model matches the fixed image better  \nBut we can see that it obtains a deformation field that is very noisy, unlikely to be anatomically meaningful  ","6a53dade":"We're first going to prepare a colormap","f6dc6450":"An important caveat to learning-based registration is that  \nThey will, in general, only register samples fromt he distribution they've been trained from  \n\nSo, what happens if we register two 7s?","341724f5":"### References\nA good chunk of this tutorial build on work from [voxelmorph](https:\/\/github.com\/voxelmorph\/voxelmorph) ([TMI](https:\/\/arxiv.org\/abs\/1809.05231) and [MedIA](https:\/\/arxiv.org\/abs\/1903.03545)) and code from [neuron](https:\/\/github.com\/adalca\/neuron) ([CVPR](http:\/\/arxiv.org\/abs\/1903.03148))"}}