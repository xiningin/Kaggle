{"cell_type":{"366e2961":"code","50200f7b":"code","d42cd470":"code","2b8f7f36":"code","0f52b5ba":"code","f57468a0":"code","5258057f":"code","d5626e7c":"code","c902b50c":"code","d74a12ee":"code","16f237bb":"code","7d041368":"code","5f704334":"code","454885df":"code","df2a8599":"code","b91c2445":"code","3d50cbb9":"code","585ba6b6":"code","d6b5fd47":"code","0d0446a2":"code","62a7dccb":"code","0db68c8a":"code","ff61d77a":"code","359f44eb":"code","0cd986d0":"code","d7e25683":"code","b599d061":"code","ca52e5e2":"code","975f2124":"code","cbd29a23":"code","0e17c0be":"code","49a38eb5":"code","09f48755":"code","48663abe":"code","01d87a1d":"code","15e73056":"code","2109d27d":"code","12b38f1e":"code","10a0e99f":"code","dda2df56":"code","acdf5067":"code","d8db3eb8":"code","9620c5f8":"code","c47f5ee7":"markdown","ab49c03c":"markdown","c6837d91":"markdown","f8908bcf":"markdown","2665a8c6":"markdown","5f256186":"markdown","8ffcf794":"markdown","957e4f5d":"markdown","b7cfc65c":"markdown","46e12038":"markdown","9c74f02e":"markdown","593e951b":"markdown","c0904118":"markdown","d316892e":"markdown","f57b597c":"markdown","c17fec54":"markdown","83f64522":"markdown","4fd8d58e":"markdown","fc0bb1f1":"markdown","8a10c40d":"markdown","51a21d1c":"markdown","699d5f35":"markdown","db17a460":"markdown","9dc94a22":"markdown","9829a462":"markdown","ca06eb9b":"markdown","7d944f77":"markdown","aca0fc48":"markdown","61f7ee4b":"markdown","77a75641":"markdown","982793c3":"markdown","67b58ad0":"markdown","69c5686f":"markdown","74c34463":"markdown","45026bf6":"markdown","7ad1c029":"markdown","d3cdad65":"markdown","ccec6c26":"markdown","600f06a7":"markdown","5864d471":"markdown","013f8b40":"markdown"},"source":{"366e2961":"\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning)\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.compose import make_column_transformer\nfrom imblearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, PolynomialFeatures\nfrom sklearn.metrics import classification_report, confusion_matrix,roc_auc_score, roc_curve,precision_recall_curve\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold,GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import f1_score, precision_score,recall_score,accuracy_score, plot_roc_curve, cohen_kappa_score\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.cluster import KMeans\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport matplotlib.pyplot as plt\nimport eli5\npd.options.mode.chained_assignment = None\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","50200f7b":"# Import dataset \n\ndf = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","d42cd470":"df.head()","2b8f7f36":"print(f'Dataset contains {df.shape[0]} rows, and {df.shape[1]} columns.')","0f52b5ba":"# Inspect all the features data types and missing values\npd.concat([df.dtypes,df.isna().sum(),df.nunique()],axis=1).rename(columns={0: \"Types\", 1: \"Nulls\", 2:'Nunique'})","f57468a0":"df.drop('customerID', axis=1, inplace=True)\n# Replace spaces with null values in column of total charges\ndf['TotalCharges'] = df[\"TotalCharges\"].replace(\" \",np.nan).astype(float)\nprint(f'TotalCharges number of Nan values: {df[\"TotalCharges\"].isna().sum()}')\nprint(f'TotalCharges percentage of Nan values: {np.round(df[\"TotalCharges\"].isna().sum()\/len(df)*100,3)}%')\n","5258057f":"# Fill misiing values with midian\ndf['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].median())\n\n# Describe statistis for numerical features\ndf.describe().T.round(2)","d5626e7c":"val = df[\"Churn\"].value_counts().values.tolist()\nlabels = 'Not Churned', 'Churned'\nfig1, ax1 = plt.subplots(figsize=(8,8))\nax1.pie(val, explode=(0,0.1), labels=labels, autopct='%1.1f%%',colors = [ \"#56738f\" ,'#e74c3c'],\n        shadow=True, startangle=90,  textprops={'fontsize': 20, 'weight':'bold'})\nax1.axis('equal') \nplt.show()","c902b50c":"# visualizing the Probability Density of each continuous variable, and churn rate by deciles. \ndf_num = df.copy()\ndf_num['Churn'] = df_num['Churn'].map({'No':0, 'Yes':1})\n\nfor i in ('tenure','MonthlyCharges', 'TotalCharges'):\n    \n    plt.figure(figsize=(14,5))\n    sns.set(font_scale = 1.1)\n    sns.kdeplot(df_num[df_num['Churn']== 1][str(i)], color=\"#e74c3c\", shade=True, label = 'Churn', alpha=0.7)\n    sns.kdeplot(df_num[df_num['Churn']== 0][str(i)], color=\"#34495e\", shade=True, label = 'Not Churn', alpha=0.7)\n    plt.title(f'Customer Churned - {i}', size=15,fontweight='bold')\n    plt.show();\n    \n\n    \n    plt.figure(figsize=(14,5))\n    sns.set(font_scale = 1.1)\n    churn_mc = df_num.groupby(pd.qcut(df_num[i],10,precision=0))[['Churn']].mean().round(2)\n    sns.set(font_scale = 1.1)\n    ax = sns.barplot(data=churn_mc, x=churn_mc.index,y=churn_mc.Churn,hue=churn_mc['Churn'],palette=(\"OrRd\"),dodge=False);\n    ax.set_yticklabels(['{:,.0%}'.format(x) for x in ax.get_yticks()]); plt.title(f'By {i} deciles');plt.legend(\"\")\n    plt.show()\n    \n    ","d74a12ee":"# countplot for each category + churn perecetage \nplt.style.use('seaborn-dark')\nfig, ax = plt.subplots(figsize = (10, 100))\n\nfor n, i in enumerate(df.columns[df.nunique() <5].drop('Churn')):\n    \n    ax = plt.subplot(20, 1, n+1)\n    no = df[i].value_counts().plot(kind='bar', ax=ax,width=.5,  color=\"#34495e\", alpha=0.8)\n    plt.title(f'Customer Churned by {i}', fontweight='bold',size=20)\n    churn = df[df.Churn=='Yes'][i].value_counts().plot(kind='bar',ax=ax, width=.4, color=\"#e74c3c\", alpha=0.75)\n    plt.xticks(rotation=0,fontsize=17);\n    plt.legend(['Total', 'Churn rate'],fontsize= 15)\n    fig.subplots_adjust(hspace=0.3)\n    bars = ax.patches\n    half = int(len(bars)\/2)\n    left_bars = bars[:half]\n    right_bars = bars[half:]\n    \n    for left, right in zip(left_bars, right_bars):\n        height_l = left.get_height()\n        height_r = right.get_height()\n        total = height_l\n        ax.text(right.get_x() + right.get_width()\/2, 70,\n                '{0:.0%}'.format(height_r\/(total)), ha=\"center\", fontsize=25, color='w')\n    \n    plt.xlabel(\"\"); \n    plt.tight_layout();\n","16f237bb":"# In order to get all the correlation between the features, I first factorized the features of the object type,\n# and then I joined the numerical data types.\ncorr = (df.select_dtypes(include='object').apply(lambda x: pd.factorize(x)[0])\\\n       .join(df.select_dtypes(include=['float','int64']))).corr().round(2)\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(14, 10))\n    ax = sns.heatmap(corr,mask=mask, cmap='Blues',annot=True,annot_kws={\"size\":8},linewidths=0.5);\n    sns.set(font_scale=1.5);\n    plt.title('Cooralation Matrix', size=20);","7d041368":"# Correlation of Churn with other variables\ncorr['Churn'].drop('Churn').sort_values().plot(kind='bar', figsize=(14,6));","5f704334":"binary_categories=[]\nfor col in df.columns:\n    if df[col].isin(['Yes']).any():\n        binary_categories.append(col)\n        print(col)","454885df":"binary_categories_dic = {'Yes': 1,\n                         'No': 0,\n                         'No internet service': 0,\n                         'No phone service': 0}\n\nfor col in binary_categories:\n    df[col] = df[col].map(binary_categories_dic)\n    \n# ordering columns by dtypes\ndf = df[df.dtypes.sort_values().index.tolist()]\n\n# grouping features by type (numerical, categorical)\nnum_features = ['tenure','MonthlyCharges', 'TotalCharges']\ncat_features = df.select_dtypes(include='object').columns.tolist()\nname_num_features = df.drop(['Churn'], axis=1).select_dtypes(include=['int64','float']).columns.tolist()\ndf.info()","df2a8599":"# stepes for pipeline transformation\ncolumn_trans = make_column_transformer(\n                (StandardScaler(), num_features),\n                (OneHotEncoder(), cat_features),\n                 remainder='passthrough')","b91c2445":"# Splitting the data-set into independent (Churn) and dependent features\n\nX = df.drop('Churn', axis=1)\ny = df.Churn.values\n\n# Split the dataset into train (60%), validate(20%), test (20%)\n# We'll use stratify parameter to ensure the proportion of the class labels in each subset is the same.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=12)\nX_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=12)\nprint(f'training set: {X_train.shape[0]}')\nprint(f'validation set: {X_val.shape[0]}')\nprint(f'testing set: {X_test.shape[0]}')","3d50cbb9":"res_features = num_features\npass_features = [e for e in name_num_features if e not in num_features]\npipe = Pipeline([('ct', column_trans), ('classifier', LogisticRegression())]).fit(X_test, y_test)\nexp_features = res_features+list(pipe.named_steps['ct'].named_transformers_['onehotencoder']\\\n                                 .get_feature_names(cat_features)) + pass_features\n\n# Creating a parallel dateset for later exploration and feature engineering & selection \nsm = SMOTE(random_state=4)\nX_train_res = pd.DataFrame(column_trans.fit_transform(X_train), columns = exp_features)\nX_test_res  = pd.DataFrame(column_trans.transform(X_test), columns = exp_features)\nX_train_smote, y_train_smote = sm.fit_sample(X_train_res, y_train)\nX_train_smote = pd.DataFrame(X_train_smote, columns = exp_features)","585ba6b6":"# Creating ML-models dictionary (with default setting)\n\nmodels={\n    'LogReg'            : LogisticRegression(max_iter=2000),\n    'DecisionTree'      : DecisionTreeClassifier(),\n    'RandomForest'      : RandomForestClassifier(),\n    'SVM'               : SVC(probability=True),\n    'KNN'               : KNeighborsClassifier(),\n    'AdaBoost'          : AdaBoostClassifier(),\n    'XGBoost'           : XGBClassifier(),\n    'LightGBM'          : LGBMClassifier(),\n    \n}\n\n\n# Cross validation on train data\n\ndf_metrics=pd.DataFrame([])\npredicts={}\n\nfor model_name in models.keys():\n    \n    metrics={}\n    pipe = Pipeline([('ct', column_trans), ('sm', SMOTE(random_state=12)), ('classifier', models[model_name])])\n    pred = pipe.fit(X_train, y_train)\n    predicts[model_name] = pred.predict(X_val)\n    metrics['accuracy']= cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean()\n    metrics['roc_auc'] = cross_val_score(pipe, X_train, y_train, cv=5, scoring='roc_auc').mean()\n    metrics['precision'] = cross_val_score(pipe, X_train, y_train, cv=5, scoring='precision').mean()\n    metrics['recall'] = cross_val_score(pipe, X_train, y_train, cv=5, scoring='recall').mean()\n    metrics['f1'] = cross_val_score(pipe, X_train, y_train, cv=5, scoring='f1').mean()\n    df_metrics=pd.concat([df_metrics,pd.DataFrame(metrics,index=[model_name]).T],axis=1)\n    \n\n\ndf_metrics.T.style.highlight_max(color='lightgreen').set_precision(3)","d6b5fd47":"df_metrics.T.plot(kind='bar', figsize=(14,7));\nplt.legend(loc=(1.04,0)); plt.xticks(rotation=45);","0d0446a2":"# Compare how well each algorithm used to identify true positive (sensitivity) vs. false positive (specificity).\n\nmodel_test = {}\nfig, ax = plt.subplots(figsize=(12,8))\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--');\nplt.title('ROC Curves'); ax.margins(0,0)\n\nfor n, model_name in enumerate(models.keys()):\n  \n    pipe = Pipeline([('ct', column_trans), ('sm', SMOTE(random_state=12)), ('classifier', models[model_name])])\n    model_test[model_name] = pipe.fit(X_train, y_train)\n    if n == 0:\n        disp = plot_roc_curve(model_test[model_name], X_val,  y_val, name=model_name, ax=ax)\n    else:\n        plot_roc_curve(model_test[model_name], X_val, y_val, name=model_name, ax=disp.ax_, lw=2) ","62a7dccb":"plt.subplots(figsize = (15, 40))\n\nfor n, model in enumerate(models.keys()):\n    \n    plt.subplot(8, 2, n+1)\n    cm = confusion_matrix(y_val, predicts[model])\n    sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True,\n            cmap = 'Blues_r', cbar=False, annot_kws={\"fontsize\":20});\n    plt.title(model)\n    plt.ylabel('Actual Churn');\n    plt.xlabel('Predicted Churn');\n    plt.tight_layout()   ","0db68c8a":"def cv_new_features(new_X_train_features):\n    \n    ''' \n    cross validation without and with the new features.\n     '''\n    \n    # cross-validation without new features\n    pipe = Pipeline([('sm', SMOTE(random_state=12)), ('classifier', LogisticRegression(max_iter=2000))])\n    sf_auc = cross_val_score(pipe, X_train_res, y_train, cv=5, scoring='roc_auc').mean()\n    sf_f1  = cross_val_score(pipe, X_train_res, y_train, cv=5, scoring='f1').mean()\n    # cross-validation with new features\n    pipe = Pipeline([('sm', SMOTE(random_state=12)), ('classifier', LogisticRegression(max_iter=2000))])\n    sf_auc2 = cross_val_score(pipe, new_X_train_features, y_train, cv=5, scoring='roc_auc').mean()\n    sf_f2  = cross_val_score(pipe, new_X_train_features, y_train, cv=5, scoring='f1').mean()\n    print(f'Results before adding new features, Auc: {round(sf_auc,3)}  f1:{round(sf_f1,3)}')\n    print(f'Results after adding new features, Auc: {round(sf_auc2,3)}  f1:{round(sf_f2,3)}')","ff61d77a":"# Add Kmeans clustering. We will create another column (km) which represents the cluster to which the sample belongs,\n# using first the Kmeans algorithm which basically refers to the collection of data points that are aggregated together\n# due to certain similarities. W'll yuse defualt setting of 8 clusters.\n\nX_train_km, X_test_km =X_train_res, X_test_res\nkm = KMeans()\nkm.fit(X_train_km)\nX_train_km['km'] = km.labels_\nX_test_km['km'] = km.predict(X_test_km)\nohe = OneHotEncoder() # one hot encoding 8 clusters.\nX_train_km = X_train_km.join(pd.DataFrame(ohe.fit_transform(X_train_km['km'].values.reshape(-1, 1))\\\n                                          .toarray())).drop('km', axis=1)\nX_test_km = X_test_km.join(pd.DataFrame(ohe.transform(X_test_km['km'].values.reshape(-1, 1))\\\n                                          .toarray())).drop('km', axis=1)\n\ncv_new_features(X_train_km)","359f44eb":"# add polynomial features (created by raising existing features to an exponent) to the numerical features(num_features).\n# \nX_train_pf, X_test_pf = X_train_res, X_test_res\ndata = X_train_pf[num_features]\ntrans = PolynomialFeatures(degree=2, interaction_only=False)\ndata = trans.fit_transform(data)\npf = pd.DataFrame(data, columns=trans.get_feature_names())\nprint(pf.head()) # the numerical features ('tenure',MonthlyCharges',TotalCharges' as x0,x1,x2) and their polynomials generated.\nX_train_pf = pd.concat([X_train_res, pf.drop(['1','x0', 'x1', 'x2'], axis=1)], axis=1)","0cd986d0":"cv_new_features(X_train_pf)","d7e25683":"# Build a dictionary of hyperparameters for evrey ML algorithm\nsearch_parms = dict()\n\nsearch_parms['LogReg'] =            {'classifier__C': [0.1, 0.5, 0.75, 1, 10],\n                                    'classifier__solver':['lbfgs','saga']}\n\n\nsearch_parms['DecisionTree'] =     {\n                                    'classifier__max_depth':[3,6,10,15,25,30,None],\n                                    'classifier__min_samples_leaf':[1,2,5,10,15,30],\n                                    'classifier__max_leaf_nodes': [2, 5,10]}\n\nsearch_parms['RandomForest'] =     {\n                                    'classifier__n_estimators': [10, 100, 1000],\n                                    'classifier__max_depth':[5,8,15,25,30,None],\n                                    'classifier__min_samples_leaf':[1,2,5,10,15,30],\n                                    'classifier__max_leaf_nodes': [2, 5,10]}\n\nsearch_parms['SVM'] =              {\n                                   'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n                                   'classifier__gamma': [1,0.1,0.01,0.001],\n                                   'classifier__kernel': ['rbf', 'poly', 'sigmoid']}\n\nsearch_parms['KNN'] =              {\n                                    'classifier__n_neighbors':[4,5,6,7,8,9,10],\n                                    'classifier__leaf_size':[1,2,3,5],\n                                    'classifier__weights':['uniform', 'distance'],\n                                    'classifier__algorithm':['auto', 'ball_tree','kd_tree','brute']}\n\n\nsearch_parms['AdaBoost'] =         {\n                                    'classifier__learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3, 0.5],\n                                    'classifier__n_estimators': [50, 75, 100, 200, 300, 500]}\n\n\nsearch_parms['XGBoost'] =          {\n                                   'classifier__min_child_weight': [1, 5, 10],\n                                   'classifier__gamma': [0.5, 1, 1.5, 2, 5],\n                                   'classifier__subsample': [0.6, 0.8, 1.0],\n                                   'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n                                   'classifier__max_depth': [3, 4, 5]}\n\nsearch_parms['LightGBM'] =         {\n                                   'classifier__max_depth': [25,50, 75],\n                                   'classifier__learning_rate' : [0.01,0.05,0.1],\n                                   'classifier__num_leaves': [300,900,1200],\n                                   'classifier__n_estimators': [200]}\n","b599d061":"# Creating scores metrics dictionary\n\nscorers = {\n            'accuracy': 'accuracy',\n            'AUC':      'roc_auc',\n            'Recall':   'recall',\n            'precision':'precision',\n            'f1':        'f1'}\n\n\ndf_metrics_gs=pd.DataFrame([])\npredicts={}\nb_parms={}\n\nfor model_name in models.keys():\n\n    metrics={}\n    pipe =                Pipeline([('ct', column_trans), ('sm', SMOTE(random_state=12)), ('classifier', models[model_name])])\n    predicts[model_name] = RandomizedSearchCV(pipe, search_parms[model_name],cv=5,scoring=scorers,\n                                              refit='AUC', random_state=42).fit(X_train, y_train)\n    metrics['accuracy']  = predicts[model_name].cv_results_['mean_test_accuracy'].mean()\n    metrics['precision'] = predicts[model_name].cv_results_['mean_test_precision'].mean()\n    metrics['recall']    = predicts[model_name].cv_results_['mean_test_Recall'].mean()\n    metrics['roc_auc']   = predicts[model_name].cv_results_['mean_test_AUC'].mean() \n    metrics['f1']        = predicts[model_name].cv_results_['mean_test_f1'].mean()\n    b_parms[model_name]  = predicts[model_name].best_params_ \n    df_metrics_gs =        pd.concat([df_metrics_gs,pd.DataFrame(metrics,index=[model_name]).T],axis=1)\n\n\n\ndf_metrics_gs.T.style.highlight_max(color='lightgreen').set_precision(3)","ca52e5e2":"# Ranked Models after RandomsearchCV based on roc_auc and f1 score\ndf_metrics_gs.T[['roc_auc', 'f1']].round(3).rank(pct=True).sum(axis=1).sort_values(ascending=False)","975f2124":"# Results of the models chosen from the cross validation process.\n\ntop_models = df_metrics_gs.T.iloc[[0,5]].round(3)\ntop_models","cbd29a23":"# Let's sort all the features by correlation to the target\ncorr_features= abs(X_train_smote.corrwith(pd.DataFrame(y_train_smote)[0])).sort_values(ascending=False)\ncorr_features_to_test_var = corr_features.index.tolist()\ncorr_features.head()","0e17c0be":"# loop from 3 to length of features list and add another feature each time\n# cross-validate and store in a DataFrame\n\nf1_results = pd.DataFrame(columns=top_models.index, index=np.arange(3,(len(corr_features))), dtype='int')\n\nfor n in range(3,len(corr_features_to_test_var)+1):\n    X_train_f = pd.DataFrame(X_train_res, columns = exp_features)[corr_features_to_test_var[0:n]]\n    \n    for model in top_models.index:\n        pipe = Pipeline([('sm', SMOTE(random_state=12)), ('classifier', models[model])])\n        f1_results.loc[n,str(model)] = cross_val_score(pipe, X_train_f, y_train, cv=5, scoring='f1').mean()\n        \nprint('Model name - best F1 score, Number of features')\nprint([(x, round(f1_results[x].max(),3), f1_results[x].idxmax()) for x in f1_results.columns])\nf1_results.plot();\nplt.title('F1 Score');plt.xlabel('Number of features');\nplt.plot(f1_results.iloc[:,0].idxmax(), f1_results.iloc[:,0].max(), 'o', markersize=10,c=\"b\", mew=4,)\nplt.plot(f1_results.iloc[:,1].idxmax(), f1_results.iloc[:,1].max(), 'o', markersize=10, c=\"orange\", mew=4);","49a38eb5":"def report(y_true, y_pred):\n    print(\"Accuracy = \" , accuracy_score(y_true, y_pred).round(3))\n    print(\"Precision = \" ,precision_score(y_true, y_pred).round(3))\n    print(\"Recall = \" ,recall_score(y_true, y_pred).round(3))\n    print(\"F1 Score = \" ,f1_score(y_true, y_pred).round(3))\n    print(\"Cohen kappa = \" ,cohen_kappa_score(y_true, y_pred).round(3))","09f48755":"# best hyperparameters after RandomizedSearchCV\nprint(b_parms['AdaBoost'])","48663abe":"# changing hyperparamters model, and test on validation and test datasets\n\npipe_ada = Pipeline([('ct', column_trans), ('sm', SMOTE(random_state=12)),\n                     ('classifier', AdaBoostClassifier(n_estimators = 500, learning_rate = 0.1))])\nada_clf = pipe_ada.fit(X_train, y_train)\nclf_pred_val = ada_clf.predict(X_val)\nclf_pred_test = ada_clf.predict(X_test)\nprint('Test report on Validation set:')\nreport(y_val, clf_pred_val)\nprint('\\n')\nprint('\\n\\nTest report on Test set:')\nreport(y_test, clf_pred_test)\ncm = confusion_matrix(y_val, clf_pred_val)\nsns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True,\n        cmap = 'Blues_r', cbar=False, annot_kws={\"fontsize\":20});\nplt.title('AdaBoost - test set')\nplt.ylabel('Actual Churn');\nplt.xlabel('Predicted Churn');\nplt.tight_layout()   ","01d87a1d":"eli5.explain_weights(pipe_ada.named_steps['classifier'], top=50, feature_names =exp_features)","15e73056":"# best hyperparameters after RandomizedSearchCV\nb_parms['LogReg']","2109d27d":"# Validation set \npipe_lr =  Pipeline([('ct', column_trans), ('sm', SMOTE(random_state=12)),\n                     ('classifier', LogisticRegression(solver = 'saga', C = 0.5, max_iter=2000))])\nlr_clf = pipe_lr.fit(X_train, y_train)\nclf_pred_val = lr_clf.predict(X_val)\nclf_pred_test = lr_clf.predict(X_test)\nprint('Test report on Validation set:')\nreport(y_val, clf_pred_val)\nprint('\\n')\n# print('\\n\\nTest report on Test set:')\n# report(y_test, clf_pred_test)\ncm = confusion_matrix(y_val, clf_pred_val)\nsns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True,\n        cmap = 'Blues_r', cbar=False, annot_kws={\"fontsize\":20});\nplt.title('Logistic Regression - validation set')\nplt.ylabel('Actual Churn');\nplt.xlabel('Predicted Churn');\nplt.tight_layout()   ","12b38f1e":"eli5.explain_weights(pipe_lr.named_steps['classifier'], top=50, feature_names = exp_features)","10a0e99f":"clf_pred_proba = pipe_lr.predict_proba(X_val)[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_val, clf_pred_proba)\n\nmark = np.argwhere(thresholds == min(thresholds, key=lambda x:abs(x-0.5)))\nplt.figure(figsize=(10,7.5))\nplt.step(recall, precision, color='b', alpha=0.2, where='post')\nplt.plot(recall[mark], precision[mark], 'o', markersize=10,label=\"0.5 threshold\", fillstyle=\"none\", c=\"k\", mew=4)\nplt.fill_between(recall, precision, alpha=0.2, color='b')\nplt.title('Precision-Recall curve. Logistic Regression (val set)')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.0])\nplt.xlim([0.0, 1.0]); plt.legend(loc=1);","dda2df56":"# Logistic Regression Precision, recall and F1 for different threshold values.\nprecision, recall, thresholds = precision_recall_curve(y_val, clf_pred_proba)\nplt.figure(figsize=(11,7.5))\nplt.plot(thresholds, recall[1:], label=\"Recall\",linewidth=3)\nplt.plot(thresholds, precision[1:], label=\"Precision\",linewidth=3);\nplt.plot(thresholds, (2 * (precision[1:] * recall[1:]) \/ (precision[1:] + recall[1:])), label=\"f1\",linewidth=3);\nplt.title('Precision, recall and F1 for different threshold values (val set)');\nplt.xlabel('Threshold');plt.ylabel('Proportion')\nplt.axvline(x=0.5,color='r', lw=2, linestyle='--', alpha=0.7, label='Classifier threshold 0.5');\nplt.legend();","acdf5067":"f1 = (2 * (precision[1:] * recall[1:]) \/ (precision[1:] + recall[1:]))\nadjusted_threshold = thresholds[np.argmax(f1)].round(3)\nprint(f'Currently F1 score: {f1_score(y_val, clf_pred_val).round(3)}')\nprint(f'Max F1 score: {np.max(f1).round(3)}')\nprint(f'New adjusted classifier threshold: {adjusted_threshold}')","d8db3eb8":"lr_clf_decision_thr = (pipe_lr.predict_proba(X_test)[:,1] > adjusted_threshold)\ncm = confusion_matrix(y_test, lr_clf_decision_thr)\nplt.subplots(figsize = (8,6))\nsns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True,cmap = 'Blues_r', cbar=False, annot_kws={\"fontsize\":25});\nplt.title(f'Logistic Regression. threshold -> {adjusted_threshold}\\n Test Data Results')\nplt.ylabel('Actual Churn');plt.xlabel('Predicted Churn');plt.tight_layout();\nreport(y_test, lr_clf_decision_thr)","9620c5f8":"# divides a data series into 10 parts, and group based on the predicted churn probability (value between 0.0 and 1.0).\n# in ascending order, that first decile contain highest probability score, and then calculate the true churn rate per group.\n\nlift = pd.DataFrame({'Churn':y_test,\n                     'Pred':pipe_lr.predict_proba(X_test)[:,1].round(2)})\ngrouped = lift.groupby(pd.qcut(lift['Pred'],10,labels=False)+1)\nlift_df = pd.DataFrame()\nlift_df['min_prob'] = grouped.min()['Pred']\nlift_df['max_prob'] = grouped.max()['Pred']\nlift_df['#customers'] = grouped.size()\nlift_df['churn'] = grouped.sum()['Churn']\nlift_df['%d_churn_rate'] = round(grouped.sum()['Churn'] \/ grouped.size(),2)\nlift_df['%g_churn_rate'] = round(lift_df['churn']\/y_test.sum(),2)#.apply('{:.0%}'.format)\nlift_df['%base_rate'] = round(y_test.mean(),3)\nlift_df['lift'] = round(lift_df['%d_churn_rate'] \/ lift_df['%base_rate'],2)\nlift_df = lift_df.sort_values(by=\"min_prob\", ascending=False)\nlift_df['d_churn_rate'] = lift_df['%d_churn_rate']#.apply('{:.0%}'.format)\nlift_df.index = range(1,11)\nlift_df.index.rename('Decile', inplace=True)\nfig, ax = plt.subplots(figsize=(11,8))\nplt.bar(lift_df.index,lift_df['%d_churn_rate']);\nplt.xlabel('Customers'); plt.ylabel('Churn Rate');plt.xticks(lift_df.index)\nax.set_ylabel('Churn Rate', fontsize=16,size=20, rotation=360, labelpad=35);\nax.yaxis.set_label_coords(-0.05,1.04)\nax.grid(False)\nplt.axhline(lift_df['%base_rate'].mean(), color='r', linestyle='--', lw=2.5);\nplt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()]);\nax2 = ax.twinx()\nplt.plot(lift_df.index,lift_df['lift'], lw=3.5, color='navy', marker='.', markersize=20)\nplt.margins(0,0);ax2.grid(False); plt.ylim(0,3);plt.ylabel('Lift')\nplt.gca().set_xticklabels(['{:.0f}%'.format(x*10) for x in plt.gca().get_xticks()]);\nplt.yticks(color='navy')\nax2.set_ylabel('Lift', fontsize=16, color='navy', size=20, rotation=360, labelpad=35);\nax2.yaxis.set_label_coords(+1.05,1.1)\nlift_df","c47f5ee7":"Logitic Regresion model, Confusion matrix results for validation data:\n* 79% of churned customers predicted correctly (recall).\n* 53 of the churn predictions are correct (precision).","ab49c03c":"<b>a.<\/b> Clustring","c6837d91":"Having regard to the results of the auc score as well as the comparison of other metrics,<br>\nwe opt to proceed with <b><u>AdaBoost<\/u><\/b> and <b><u>Logistic Regression<\/u><\/b> models.\n\nTo maximize the performance of the models for our goal, let's first <b>explore<\/b> them, <b>tune<\/b> some hyperparameters,<br>\n    and then <b>test<\/b> our model on the test data (X_test).","f8908bcf":"Now, we are going to make a comparison between various methods of machine learning.<br>\nFirst, we'll create a dictionary of models with default setting, and one for training and predicting on val set<br>\nNext, loop over each model while:<br>\n* Making a pipeline for preprocessing the training data each time with SMOTE.<br>\n* Evaluation by cross-validation (5 folds) on various performance metrics.<br>\n* building a new dataframe for the results.\n* Compare the results and visualize the ROC curves and the Confusion matrices for each of them.\n* Randomized search on hyper parameters for evrey model with refit using the best f1 score found\n* Choose 2 of our best models based on the relevant business matrices.\n* Tuning threshold for better results.\n* Test the models with unseen data (testing set).\n\n<u>Note<\/u>: We cross validate only on the training subsets (X_train_smote, y_train_smote),<br>\nso essentially we take each time a proportion of the training data to validate our model,<br>\nusing the pipeline also protects us from data leakage.\n\n","2665a8c6":"<b>Feature selection<\/b>","5f256186":"## Data preprocessing","8ffcf794":"###  Let's look at Correlation between the features","957e4f5d":"<b>Comparing ROC Curves derived From our models on validation sets<\/b>","b7cfc65c":"Our main objective is to predict whether a customer is going to churn,<br>\nand the recall score is one of the moset relevent for that purpose.<br>\n0.749 recall score, for AdaBoost model, means that for every 100 costumers that churn we're predicting 75 of them.<br>\nNow, there's a trade-off between Recall (tp\/(tp+fn) and Precision (tp\/(tp+fp), means increasing one parameter leads to decreasing of the other.<br>\nIn our case (AdaBoost) high recall comes with low-precision, which means that for every 100 costumers that we predict as possibly churn,<br> only 54 of them will eventually be churned.<br>\n","46e12038":"AdaBoost model, Confusion matrix results for test data:\n* 74% of churned customers predicted correctly (recall).\n* 57% of the churn predictions are correct (precision).","9c74f02e":"We see that there is a high correlation between the additional services,<br>that is, if someone is already adding one service then the likelihood of adding more services is high.","593e951b":"### Introduction:<br>\nCustomer retention is one of the key aspects of <b>C<\/b>ustomer <b>R<\/b>elationship <b>M<\/b>anagement. \n\n\nCustomer churn (aka, customer attrition, customer turnover, or customer defection) is the loss of clients or customers,<br>\nand, customer churn rate, is the percentage of customers who have stopped using your company's product or service during a certain period of time.\n\nManaging client churn is a major challenge facing companies, particularly those offering subscription-based services.\n\nThe goal is to predict customer churn using Machine Learning model. <br>Such models will create a list of customers that are most susceptible to churn, so that they can be given priority to a targeted customer retention plan.\n\nThis case study modeling based on telco-customer-churn dataset from Kaggle, with over 7000 customers (rows), and each column contains customer\u2019s attributes (features).","c0904118":"26.5% churn rate \n\nWe'll start with the <b>numerical features<\/b>, and look at the distribution for those who've been churned, and for those who haven't.\n","d316892e":" We can already see from those plots that:\n* Recent customers will be more likely to churn.\n* Customers bringing higher revenue (monthly charges) often have a greater risk of churning.","f57b597c":"### AdaBoost","c17fec54":"<b>Tuning our model (using our RandomizedSearchCV results dictionary), validation, testing results (unseen data - X_test) and,<br> feature Importance<\/b>","83f64522":"<b>b.<\/b> Polynomial features","4fd8d58e":"## Customer Churn Prediction \n","fc0bb1f1":"<b>Our model has not been enhanced by clustering and polynomial feature, so they are not applied to our data set.<\/b>","8a10c40d":"## Models","51a21d1c":"One other way to visualize the data is to use the x-axis as the threshold, and plot the precision and recall:","699d5f35":"<b>Comparing confusion matrices (validation set) derived from our models<\/b>","db17a460":"<b>From the chart above and as far as business objectives are concerned, <br>we can adjust our model classification threshold (decision threshold) to maximize the F1-Score (weighted average of Precision and Recall) in order to achieve higher recall rate, with the cost of higher false positive.\nThe main assumptions here are that acquiring a new customer is between five and 25 times more expensive than retaining an existing one,<br>\nand increasing customer retention rates, even with a small portion, can significantly increase company profits<\/b>","9dc94a22":"### Decile analysis & Lift","9829a462":"### Let's explore our Logistic Regression model with the precision-recall curve that shows the tradeoff between precision and recall for different thresholds.","ca06eb9b":"Let's look at the first 5 rows","7d944f77":"<b>The exploratory data analysis provided us with business understanding what features are related to customer's churn.<br>This can provide actionable insights that can help to make product, pricing and marketing decisions more effective.<br>\nThe company for example can re-price or stop providing fiber optics internet service, as well as lower monthly charges for new customers.<br>\nAs, for the models, the company can implement one or multiple churn prediction models (Ensemble) ,<br>and then combine them to increase customer retention for more profitability results.<\/b>","aca0fc48":"## Let's explore our dataset","61f7ee4b":"Showing Logistic Regression model weights for each feature, showing how influential it might have been to contribute to the final decision:","77a75641":"According to the results of Cross Validation, it can be seen that LogReg and AdaBoost had the best relevant scores (f1, roc_auc).<br>\nIt is important to note that the results above are with the default parameters of each and every model and in a short time we will check the models again, after adjusting the hyperparameters with a RandomizedSearchCV.","982793c3":"<b>Now we will adjust the classifier to the New adjusted classifier threshold, and finaly test on our the test-set(unseen data).<\/b>","67b58ad0":"<b>Feature engineering -<\/b><br>\n","69c5686f":"<b>Randomized search on hyper parameters for evrey model with refit using the best RocAuc score found.<\/b> ","74c34463":"If we only target 10% of the customers in the first group (that has the higher probability of 0.77\tto 0.90), we can expect to catch nearly three times more churning customers than we would by targeting the same number of people randomly. The churn customers in the first group account for 26% of all churn customers.","45026bf6":"## Content\n#### Each row represents a customer, each column contains customer\u2019s attributes.\n\nThe data set includes information about:\n\n* Customers who left within the last month \u2013 the column is called <b>Churn<\/b>.<br>\n* Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies.<br>\n* Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges.<br>\n* Demographic info about customers \u2013 gender, age range, and if they have partners and dependents.","7ad1c029":"### Logistic Regression","d3cdad65":"roc_auc (the area under the ROC curve) evaluates the ability of the method to classify correctly,\nand the classifier with the highest AUC score will be considered better.","ccec6c26":"<u>Hyperparameters<\/u>","600f06a7":"### Now, let 's explore the categorical features and their relationship to our variable target.","5864d471":"### Summary","013f8b40":"<b><u>To summarize the categorical features, we can see that<\/u>:<\/b>\n\n\n* There is no impact on Churn by gender.\n* SeniorCitizens have higher churn rate.\n* Customers without partner have higher churn rate as well costumers without dependents.\n* Customers with Fiber optic internet service have higher churn rate than others.\n* Costumers without online security, back-up, device protection and technical support have higher churn rate.\n* Short-term contracts are subject to higher churn rates.\n* Customers with paperless billing have higher churn rate.\n* As far as payment methods, the electronic check has a very high churn rate.\n\n"}}