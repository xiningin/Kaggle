{"cell_type":{"c9963278":"code","b7a7b3dc":"code","8cca8bed":"code","a493854a":"code","7f0da8f4":"code","ecdbe62a":"code","6d725024":"code","9e35b001":"code","159d6566":"code","c6dc8220":"code","aa0d3b27":"markdown"},"source":{"c9963278":"!export NLTK_DATA=..\/input\/infersent\/punkt\/punkt\n!ls \/kaggle\/input\/infersentrepo\/repository\/facebookresearch-InferSent-940c003\n!cd \/kaggle\/working\nimport sys\nsys.path.append('\/kaggle\/input\/infersentrepo\/repository\/facebookresearch-InferSent-940c003')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"\/kaggle\/input\/infersentrepo\/repository\/facebookresearch-InferSent-940c003\"))\nimport nltk\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nfrom random import randint\nimport numpy as np\nimport torch\nfrom models import InferSent\n!ls \/kaggle\/input","b7a7b3dc":"model_version = 1\nMODEL_PATH = \"\/kaggle\/input\/infersent\/infersent%s.pkl\" % model_version\nparams_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\nmodel = InferSent(params_model)\nmodel.load_state_dict(torch.load(MODEL_PATH))","8cca8bed":"use_cuda = False\nmodel = model.cuda() if use_cuda else model\n# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\nW2V_PATH = '\/kaggle\/input\/quora-insincere-questions-classification\/embeddings\/glove.840B.300d\/glove.840B.300d.txt' if model_version == 1 else ''\nmodel.set_w2v_path(W2V_PATH)","a493854a":"# Load embeddings of K most frequent words\nmodel.build_vocab_k_words(K=100000)","7f0da8f4":"a=pd.read_csv('\/kaggle\/input\/petfinder-adoption-prediction\/train\/train.csv')\nb=a['Description'].values\nf = open(\"\/kaggle\/working\/pet.txt\", \"w\")\nf = open(\"\/kaggle\/working\/pet.txt\", \"a+\")","ecdbe62a":"for i in range(10000):\n    f.write(str(b[i])+'\\n')","6d725024":"# Load some sentences\nsentences = []\nwith open('\/kaggle\/working\/pet.txt') as f:\n    for line in f:\n        sentences.append(line.strip())\nprint(len(sentences))","9e35b001":"sentences[:5]","159d6566":"embeddings = model.encode(sentences, bsize=128, tokenize=False, verbose=True)\nprint('nb sentences encoded : {0}'.format(len(embeddings)))","c6dc8220":"len(model.encode(['the cat eats.'])[0])","aa0d3b27":"# Some Remarks:\n1. The encoder only works for English sentences, so you'll need an alternative for other languages in the text\n2. This is merely a work in progress, but this is something to refer to when you're trying to install an external package without using the internet\n3. I will probably do some text cleaning in further versions"}}