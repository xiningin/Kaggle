{"cell_type":{"f5b6d574":"code","87fb4435":"code","ad927218":"code","ec749119":"code","0328c267":"code","32540694":"code","fd7d7642":"code","004940af":"code","0176d6ed":"code","804340b0":"code","c8e88b54":"code","22f433ff":"code","88dfa695":"code","6fdfd7ec":"code","e96f9f7e":"code","4cf5992c":"code","522f56f5":"code","8d50ec53":"code","4b387526":"code","0654a7a3":"code","46352c4c":"code","6252c113":"code","d9d56560":"code","022b925e":"code","a0db2cb3":"code","09051fe0":"code","daafe16d":"code","5d817026":"code","c110b206":"code","e7c90e09":"code","d8cad67f":"code","d277df1c":"code","aae310d2":"code","1fe4cedd":"code","b5384236":"code","1caa5c5e":"code","8801aa87":"code","009420c2":"code","2c83d7a1":"code","5caef45f":"code","f7fb677b":"code","ad76550f":"code","9b60a91b":"code","719cf97b":"code","cf4c626a":"code","3f4eb690":"code","77ab6515":"code","561990ad":"code","099d68d2":"code","351d32f1":"code","2dd9f0f6":"code","5ba8bd1c":"code","16af856b":"code","69aa277a":"code","c73d604a":"code","87f313e2":"code","de6b2b7d":"code","2a31f07a":"code","91dc9082":"code","113476a4":"code","a0067fc3":"markdown","3de6249d":"markdown","e562aa24":"markdown","4ea0f7c3":"markdown","8152c860":"markdown","a4e2fda3":"markdown","756a4f3c":"markdown","0bf46e3e":"markdown","c6b3865a":"markdown","0e3921d7":"markdown","18c47a8a":"markdown","0e6e821a":"markdown","6e19675a":"markdown","79d4e2a9":"markdown","c6b0ac47":"markdown","a393d9d6":"markdown","08eef307":"markdown","2895ec0d":"markdown","e02c5d67":"markdown","cb101b3a":"markdown","ae430563":"markdown","97c883f7":"markdown","c1e5a720":"markdown","f3200ea2":"markdown","224e552c":"markdown","48c9f75d":"markdown","0c217d5a":"markdown","1b1ed3b2":"markdown","9119f63b":"markdown","16d5c30e":"markdown","14b212b3":"markdown","30a1df5f":"markdown","d096372e":"markdown","99685ee3":"markdown","ce730366":"markdown","2f44d272":"markdown","d60b0be1":"markdown","a2f8b2c0":"markdown","ea57214a":"markdown","5bd50978":"markdown","eff5685d":"markdown","0dca6d41":"markdown"},"source":{"f5b6d574":"import time\nimport numpy as np # linear algebra\nnp.set_printoptions(suppress=True)\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport plotly.graph_objects as go\nimport plotly.io as pio\nimport plotly.express as px\npio.templates.default = \"plotly_white\"\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder","87fb4435":"train_data=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\npd.set_option('display.max_columns', 500) # make all of the columns display\nprint(train_data.shape)\ntrain_data.describe()","ad927218":"print(train_data.columns)","ec749119":"X_train = train_data.loc[:,'MSSubClass':'SaleCondition']\ny_train = train_data.SalePrice\nX_test = test_data.loc[:,'MSSubClass':'SaleCondition']","0328c267":"def missing_ratio(all_data):\n    all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\n    all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n    missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n    return missing_data","32540694":"train_na=missing_ratio(X_train)\ntest_na=missing_ratio(X_test)\ncompare_na = pd.concat([train_na, test_na], axis=1)\ncompare_na.columns=['Train Missing Ratio','Test Missing Ratio']\ncompare_na.fillna(0)","fd7d7642":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x=compare_na.index,\n    y=compare_na[\"Train Missing Ratio\"],\n    name='Train Data'\n))\nfig.add_trace(go.Bar(\n    x=compare_na.index,\n    y=compare_na[\"Test Missing Ratio\"],\n    name='Test Data'\n))\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=99, x1=32, y1=99,\n    line=dict(\n        width=4,\n        dash=\"dashdot\",\n        color=\"#FFA15A\"   \n    ))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(barmode='group', xaxis_tickangle=-45,title=\"Missing Ratio Of Features In Train And Test set\",)\nfig.show()","004940af":"drop_col=list(compare_na[(compare_na['Train Missing Ratio'] > 99) | (compare_na['Test Missing Ratio'] >99)].index)\ndrop_col","0176d6ed":"# drop columns with high missing ratio (>99%)\nX_train= X_train.drop(drop_col, axis=1)\nX_test= X_test.drop(drop_col, axis=1)\nprint(X_train.shape ,X_test.shape)","804340b0":"def manu_miss(all_data):\n    #transform NA according tofeature meaning\n    for NA_to_0 in ('MasVnrArea' ,'GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n        all_data[NA_to_0] = all_data[NA_to_0].fillna(0)\n    \n    #transform NA to None, according to feature meaning\n    for NA_to_None in ('MiscFeature','Alley','Fence','FireplaceQu','GarageType', 'GarageFinish','GarageQual', 'GarageCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','MasVnrType'):\n        all_data[NA_to_None] = all_data[NA_to_None].fillna('None')\n    \n    #transform NA to specific value, according to definition\n    all_data['Functional'] = all_data['Functional'].fillna('Typ')\n    \n    all_data['LotFrontage']=all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x:x.fillna(x.median()))\n    all_data['MSZoning']=all_data.groupby('Neighborhood')['MSZoning'].transform(lambda x:x.fillna(x.mode()[0]))\n    all_data['Electrical']=all_data.groupby('YearRemodAdd')['Electrical'].transform(lambda x:x.fillna(x.mode()[0]))\n    all_data['KitchenQual']=all_data.groupby('ExterQual')['KitchenQual'].transform(lambda x:x.fillna(x.mode()[0]))\n    all_data['SaleType']=all_data.groupby('SaleCondition')['SaleType'].transform(lambda x:x.fillna(x.mode()[0]))\n    \n    return all_data","c8e88b54":"X_train=manu_miss(X_train)\nX_test=manu_miss(X_test)","22f433ff":"sim_impute_num= SimpleImputer(strategy=\"median\")\nimputed_X_train_num =pd.DataFrame( sim_impute_num.fit_transform(X_train._get_numeric_data()))\nimputed_X_test_num = pd.DataFrame( sim_impute_num.transform(X_test._get_numeric_data()))\nimputed_X_train_num.columns=X_train._get_numeric_data().columns\nimputed_X_test_num.columns=X_test._get_numeric_data().columns\nprint(\"missing ratio in x train numeric columns:\",missing_ratio(imputed_X_train_num),\"\\nmissing ratio in x test numeric columns:\",missing_ratio(imputed_X_test_num))","88dfa695":"sim_impute_cat= SimpleImputer(strategy=\"most_frequent\")\nimputed_X_train_cat =pd.DataFrame( sim_impute_cat.fit_transform(X_train.select_dtypes(exclude=[np.number])))\nimputed_X_test_cat = pd.DataFrame( sim_impute_cat.transform(X_test.select_dtypes(exclude=[np.number])))\nimputed_X_train_cat.columns=X_train.select_dtypes(exclude=[np.number]).columns\nimputed_X_test_cat.columns=X_test.select_dtypes(exclude=[np.number]).columns\nprint(\"missing ratio in x train numeric columns:\",missing_ratio(imputed_X_train_cat),\"\\nmissing ratio in x test numeric columns:\",missing_ratio(imputed_X_test_cat))\n","6fdfd7ec":"X_train = pd.concat([imputed_X_train_num, imputed_X_train_cat], axis=1)\nX_test = pd.concat([imputed_X_test_num, imputed_X_test_cat], axis=1)\nX_train.shape","e96f9f7e":"def ordered_feature_transform(all_data):\n    QualMap={'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0}\n    Qualmap_features=('ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual','FireplaceQu','GarageQual','GarageCond')\n    for map_feature in Qualmap_features:\n        all_data[map_feature]=all_data[map_feature].map(QualMap)\n\n    BsmtExposureMap={'Gd':4,'Av':3,'Mn':2,'No':1,'None':0}\n    all_data['BsmtExposure']=all_data['BsmtExposure'].map(BsmtExposureMap)\n\n    BsmtFinTypeMap={'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'None':0}\n    BsmtFinTypemap_features=('BsmtFinType1','BsmtFinType2')\n    for map_feature in BsmtFinTypemap_features:\n        all_data[map_feature]=all_data[map_feature].map(BsmtFinTypeMap)\n\n    UtilitiesMap={'AllPub':4,'NoSewr':3,'NoSeWa':2,'ELO':1,'None':0}\n    all_data['Utilities']=all_data['Utilities'].map(UtilitiesMap)\n\n    GarageFinishMap={'Fin':3,'RFn':2,'Unf':1,'None':0}\n    all_data['GarageFinish']=all_data['GarageFinish'].map(GarageFinishMap)\n\n    CentralAirMap={'Y':1,'N':0}\n    all_data['CentralAir']=all_data['CentralAir'].map(CentralAirMap)\n    \n    MSSubClassMap={'180':1, '30':2, '45':2,'190':3, '50':3, '90':3,'85':4, '40':4, '160':4,'70':5, '20':5, '75':5, '80':5, '150':5,'120': 6, '60':6}\n    all_data['MSSubClass'] = all_data['MSSubClass'].astype(int).apply(str)\n    all_data['MSSubClass']=all_data['MSSubClass'].map(MSSubClassMap)\n    \n    MSZoningMap={'C (all)':1, 'RH':2, 'RM':2, 'RL':3, 'FV':4}\n    all_data['MSZoning']=all_data['MSZoning'].map(MSZoningMap)\n    \n    NeighborhoodMap={'MeadowV':1,'IDOTRR':2, 'BrDale':2,'OldTown':3, 'Edwards':3, 'BrkSide':3,'Sawyer':4, 'Blueste':4, 'SWISU':4, 'NAmes':4,\n                     'NPkVill':5, 'Mitchel':5,'SawyerW':6, 'Gilbert':6, 'NWAmes':6,'Blmngtn':7, 'CollgCr':7, 'ClearCr':7, 'Crawfor':7,\n                      'Veenker':8, 'Somerst':8, 'Timber':8,'StoneBr':9,'NoRidge':10, 'NridgHt':10}\n    all_data['Neighborhood']=all_data['Neighborhood'].map(NeighborhoodMap)\n    \n    HouseStyleMap={'1.5Unf':1,'1.5Fin':2, '2.5Unf':2, 'SFoyer':2, '1Story':3, 'SLvl':3,'2Story':4, '2.5Fin':4}\n    all_data['HouseStyle']=all_data['HouseStyle'].map(HouseStyleMap)\n    \n    MasVnrTypeMap={'BrkCmn':1, 'None':1, 'BrkFace':2, 'Stone':3}\n    all_data['MasVnrType']=all_data['MasVnrType'].map(MasVnrTypeMap)\n    \n    FoundationMap={'Slab':1, 'BrkTil':2, 'CBlock':2, 'Stone':2,'Wood':3, 'PConc':4}\n    all_data['Foundation']=all_data['Foundation'].map(FoundationMap)\n    \n    GarageTypeMap={'CarPort':1, 'None':1,'Detchd':2,'2Types':3, 'Basment':3,'Attchd':4, 'BuiltIn':5}\n    all_data['GarageType']=all_data['GarageType'].map(GarageTypeMap)\n    \n    SaleConditionMap={'AdjLand':1, 'Abnorml':2, 'Alloca':2, 'Family':2, 'Normal':3, 'Partial':4}\n    all_data['SaleCondition']=all_data['SaleCondition'].map(SaleConditionMap)\n\n\n    return all_data","4cf5992c":"X_train=ordered_feature_transform(X_train)\nX_test=ordered_feature_transform(X_test)","522f56f5":"def add_new_feature(all_data):\n    all_data['BsmtFin'] =all_data['BsmtFinType1']*all_data['BsmtFinSF1']+all_data['BsmtFinType2']*all_data['BsmtFinSF2']\n    all_data['haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['LargeHouseLevel'] = all_data['GrLivArea'].apply(lambda x: 1 if x > 2750 else 0)\n    all_data['OldHouse'] = all_data['YearBuilt'].apply(lambda x: 1 if x < 1910 else  0 )\n    all_data['BsmtUnfSF_cat'] = all_data['BsmtUnfSF'].apply(lambda x: 1 if x <=500 else  2 if x <= 1000 else 3 if x <= 1500 else 4 if x <= 2000 else 5)\n    all_data['Qual']=all_data['BsmtQual']+all_data['HeatingQC']+all_data['KitchenQual']+all_data['FireplaceQu']+all_data['GarageQual']\n    all_data[\"AllSF\"] = all_data[\"GrLivArea\"] + all_data[\"TotalBsmtSF\"]\n    all_data['Total_porch'] = all_data['OpenPorchSF']+ all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF']\n    all_data['YrBltAndRemod'] = all_data['YearBuilt'] + all_data['YearRemodAdd']\n    all_data['Total_Bathrooms'] = all_data['FullBath'] + (0.5 * all_data['HalfBath']) + all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath'])\n    all_data['Total_sqr_footage'] = all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n    return all_data","8d50ec53":"X_train=add_new_feature(X_train)\nX_test=add_new_feature(X_test)","4b387526":"def drop_rest(all_data):\n    all_data=all_data.drop(['BsmtFinType1','BsmtFinSF1','BsmtFinType2','BsmtFinSF2','MiscFeature','1stFlrSF','2ndFlrSF','LowQualFinSF'], axis=1)\n    return all_data","0654a7a3":"X_train=drop_rest(X_train)\nX_test=drop_rest(X_test)","46352c4c":"def manu_dum(all_data):\n    for new_col in ('Artery','Feedr','Norm','RRNn','RRAn','PosN','PosA','RRNe','RRAe'):\n        all_data[new_col]=0.0\n        for col in ('Condition1','Condition2'):\n            all_data.loc[all_data[col] == new_col, new_col] = 1.0\n    all_data=all_data.drop(['Condition1','Condition2'], axis=1)\n    \n    for new_col in ('AsbShng','AsphShn','BrkComm','BrkFace','CBlock','CemntBd','HdBoard','ImStucc','MetalSd','Other','Plywood','Stone','Stucco','VinylSd','Wd Shng','WdShing'):\n        all_data[new_col]=0.0\n        for col in ('Exterior1st','Exterior2nd'):\n            all_data.loc[all_data[col] == new_col, new_col] = 1.0\n    all_data=all_data.drop(['Exterior1st','Exterior2nd'], axis=1)\n\n    return all_data","6252c113":"X_train=manu_dum(X_train)\nX_test=manu_dum(X_test)","d9d56560":"ObjectData=X_train.dtypes[X_train.dtypes == \"object\"].index\nprint(X_train[ObjectData].columns)\ncat_features=list(X_train[ObjectData].columns)","022b925e":"cat_features=['Street', 'Alley', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope',\n        'RoofStyle', 'RoofMatl', 'Heating', 'Electrical',\n       'Functional', 'PavedDrive', 'Fence', 'SaleType']","a0db2cb3":"for cat_feature in cat_features:\n    # Apply the label encoder to each column in train set and test set\n    encoder = LabelEncoder()\n    encoder.fit(list(X_train[cat_feature].values))\n    X_train[cat_feature] = encoder.transform(list(X_train[cat_feature].values))\n    X_test[cat_feature] = encoder.transform(list(X_test[cat_feature].values))","09051fe0":"X_train.select_dtypes(exclude=[np.number]).columns","daafe16d":"X_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)\nX_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\nprint(X_train.shape,X_test.shape)","5d817026":"from scipy.stats import norm, skew #for some statistics\n\nnumeric_feats = X_train.dtypes[X_train.dtypes != \"object\"].index\n# Check the skew of all numerical features\nskewed_feats = X_train[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nprint(skewness.head(30))\n#skewness = skewness[abs(skewness) > 0.75]\n\nX_train[skewness.index] = np.log1p(X_train[skewness.index])\nX_test[skewness.index] = np.log1p(X_test[skewness.index])\ny_train= np.log1p(y_train) # saleprice","c110b206":"from sklearn.ensemble import IsolationForest\n# identify outliers in the training dataset\niso = IsolationForest(max_samples=50, random_state=52,contamination=0.005).fit(X_train)","e7c90e09":"X_train[\"Outlier\"]=iso.predict(X_train)\nX_test[\"Outlier\"]=iso.predict(X_test)","d8cad67f":"print(X_train[\"Outlier\"].value_counts())\nprint(X_test[\"Outlier\"].value_counts())","d277df1c":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import Lasso\nmodel_select = SelectFromModel(Lasso(alpha=0.0005,tol=0.001))\nms = model_select.fit(X_train,y_train)\n\nselected_feat = X_train.columns[(ms.get_support())]\nprint('total features: {}'.format((X_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint('features with coefficients shrank to zero: {}'.format(\n      np.sum(ms.estimator_.coef_ == 0)))\n\n#Number of features which coefficient was shrank to zero\nshrink_zero_num=np.sum(ms.estimator_.coef_ == 0)\nprint(\"Number of features which coefficient was shrank to zero: \",shrink_zero_num)\n\n#Identifying the removed features\nremoved_feats = X_train.columns[(ms.estimator_.coef_ == 0).ravel().tolist()]\nprint(\"The removed features: \",removed_feats)","aae310d2":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=200)\nrf = rf.fit(X_train,y_train)\n\n# show the importance of each feature\nrf_importance=pd.DataFrame({'Features':np.array(X_train.columns),'Importance':rf.feature_importances_*100 }).sort_values(by='Importance', ascending=False)\nrf_importance","1fe4cedd":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x=rf_importance.Features,\n    y=rf_importance.Importance\n))\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=0.05, x1=84, y1=0.05,\n    line=dict(\n        width=4,\n        dash=\"dashdot\",\n        color=\"#FFA15A\"   \n    ))\n\nfig.update_layout(barmode='group', xaxis_tickangle=-45,title=\"Tree-based Feature Importance\",xaxis_title=\"Features\",\n    yaxis_title=\"Importance(%)\",)\nfig.show()\n\nprint(\"There are {} features' importance > 0.05%\".format(len(rf_importance[rf_importance['Importance']>0.05])))","b5384236":"# use SelectFromModel from SKLearn\nmodel = SelectFromModel(rf, prefit=True,threshold=0.0005)\n#X_new = model.transform(X_train)\ncolumns_select_rf=X_train.columns[model.get_support()]\nprint(\"kept {} features after tree-based feature selection\".format(len(columns_select_rf)),\"\\n\\nThe features are : \",columns_select_rf)","1caa5c5e":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n\ndef select_features(X_train, y_train, X_test):\n    # configure to select all features\n    fs = SelectKBest(score_func=f_regression, k='all')\n    # learn relationship from training data\n    fs.fit(X_train, y_train)\n    # transform train input data\n    X_train_fs = fs.transform(X_train)\n    # transform test input data\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n \nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\nclf_importance=pd.DataFrame({'Features':np.array(X_train.columns),'Importance':fs.scores_ }).sort_values(by='Importance', ascending=False)\nclf_importance","8801aa87":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x=clf_importance.Features,\n    y=clf_importance.Importance\n))\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=1, x1=74, y1=1,\n    line=dict(\n        width=4,\n        dash=\"dashdot\",\n        color=\"#FFA15A\"   \n    ))\n\nfig.update_layout(barmode='group', xaxis_tickangle=-45,title=\"Feature Importance Based-on Correlation\",xaxis_title=\"Features\",\n    yaxis_title=\"Importance\",)\nfig.show()\n\ncolumns_select_cor=np.array(clf_importance[clf_importance['Importance']>1].Features)\nprint(\"kept {} features which importance >1 based-on correlation feature selection\".format(len(columns_select_cor)),\"\\n\\nThe features are : \",columns_select_cor)","009420c2":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_regression\n\ndef select_features(X_train, y_train, X_test):\n    # configure to select all features\n    fs = SelectKBest(score_func=mutual_info_regression, k='all')\n    # learn relationship from training data\n    fs.fit(X_train, y_train)\n    # transform train input data\n    X_train_fs = fs.transform(X_train)\n    # transform test input data\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\nclf_importance=pd.DataFrame({'Features':np.array(X_train.columns),'Importance':fs.scores_ }).sort_values(by='Importance', ascending=False)\nclf_importance","2c83d7a1":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x=clf_importance.Features,\n    y=clf_importance.Importance\n))\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=0.005, x1=74, y1=0.005,\n    line=dict(\n        width=4,\n        dash=\"dashdot\",\n        color=\"#FFA15A\"   \n    ))\n\nfig.update_layout(barmode='group', xaxis_tickangle=-45,title=\"Feature Importance Based-on Mutual Information Feature Selection\",xaxis_title=\"Features\",\n    yaxis_title=\"Importance\",)\nfig.show()\n\ncolumns_select_mul=np.array(clf_importance[clf_importance['Importance']>0.005].Features)\nprint(\"kept {} features which importance > 0.005 based-on mutual information feature selection\".format(len(columns_select_mul)),\"\\n\\nThe features are : \",columns_select_mul)","5caef45f":"X_train=X_train[columns_select_mul]\nX_test=X_test[columns_select_mul]","f7fb677b":"from sklearn.manifold import TSNE\n\nstart_time = time.time()\ntsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300,random_state=1)\ntsne_results = tsne.fit_transform(X_train)\n\ndf_subset=pd.DataFrame(tsne_results)\ndf_subset.columns=['tsne-2d-one','tsne-2d-two']\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nplt.figure(figsize=(5,4))\nsns.scatterplot(\n    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n    \n    palette=sns.color_palette(\"hls\", 10),\n    data=df_subset,\n    legend=\"full\",\n    alpha=0.3\n)","ad76550f":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3, random_state=25).fit(X_train)\nX_train['cluster']=kmeans.labels_\nX_test['cluster']=kmeans.predict(X_test)","9b60a91b":"pd.DataFrame(kmeans.labels_).value_counts()","719cf97b":"X_train['cluster']=X_train['cluster'].astype(str) \nX_test['cluster']=X_test['cluster'].astype(str) \nX_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)\nX_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)","cf4c626a":"# Create correlation matrix\ncorr_matrix = X_train.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\nprint(\"These features will be drop: \",to_drop)","3f4eb690":"X_train=X_train.drop(X_train[to_drop], axis=1)\nX_test=X_test.drop(X_test[to_drop], axis=1)","77ab6515":"from sklearn.model_selection import train_test_split\nX_tra,X_val,y_tra,y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 25)\nprint(X_tra.shape)\nprint(X_val.shape)\nprint(y_tra.shape)\nprint(y_val.shape)","561990ad":"from sklearn import linear_model\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn import metrics\nstart_time = time.time()\n\nregr = make_pipeline(RobustScaler(),linear_model.LinearRegression())\nregr.fit(X_tra,y_tra)\n\ny_tra_regr_pred=regr.predict(X_tra)\nprint(\"The MSE on train set:\",np.sqrt(metrics.mean_squared_error(y_tra, y_tra_regr_pred)))\n\ny_val_regr_pred = regr.predict(X_val)\nprint(\"The MSE on validation set:\",np.sqrt(metrics.mean_squared_error(y_val, y_val_regr_pred)))\n\n#Cross-validation\nfrom sklearn.model_selection import cross_val_score\nregr_mse = cross_val_score(regr , X_train, y_train, scoring='neg_mean_squared_error',cv=5)\nprint(\"MSE on Cross-validation:\\n\",np.sqrt(-regr_mse))\nprint(\"The mean MSE on Cross-validation is {:.3f}\".format(np.mean(np.sqrt(-regr_mse))))\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nplt.figure('regr')\nplt.plot([np.expm1(y_val.min()),np.expm1(y_val.max())],[np.expm1(y_val.min()),np.expm1(y_val.max())],'k--',lw=2)\nplt.scatter(np.expm1(y_val),np.expm1(y_val_regr_pred))\nplt.xlabel('Fact Price')\nplt.ylabel('Predict Price')\nplt.title('Predict Effect Using Linear Regression')\nplt.show()","099d68d2":"from sklearn import linear_model\nfrom sklearn.linear_model import RidgeCV\n\nrcv = RidgeCV(alphas=np.arange(3.0,6.0,0.1))\nrcv.fit(X_tra,y_tra)\n\nprint(rcv.alpha_)\n\ny_tra_rcv_pred=rcv.predict(X_tra)\nprint(\"\u4f7f\u7528Ridge Regression cv\u6a21\u578b\u7684\u5747\u65b9\u6839\u8bef\u5dee\u4e3a(train):\",np.sqrt(metrics.mean_squared_error(y_tra, y_tra_rcv_pred)))\ny_rcv_pred = rcv.predict(X_val)\nprint(\"\u4f7f\u7528Ridge Regression cv\u6a21\u578b\u7684\u5747\u65b9\u6839\u8bef\u5dee\u4e3a(Validation):\",np.sqrt(metrics.mean_squared_error(y_val, y_rcv_pred)))","351d32f1":"start_time = time.time()\n\nregr_ridge= make_pipeline(RobustScaler(), linear_model.Ridge(alpha = 4.7))\nregr_ridge.fit(X_tra,y_tra)\n\ny_tra_ridge_pred=regr_ridge.predict(X_tra)\nprint(\"The MSE on train set:\",np.sqrt(metrics.mean_squared_error(y_tra,y_tra_ridge_pred)))\n\ny_val_ridge_pred = regr_ridge.predict(X_val)\nprint(\"The MSE on validation set:\",np.sqrt(metrics.mean_squared_error(y_val, y_val_ridge_pred)))\n\n#Cross-validation\nfrom sklearn.model_selection import cross_val_score\nregr_mse = cross_val_score(regr_ridge, X_train, y_train, scoring='neg_mean_squared_error',cv=5)\nprint(\"MSE on Cross-validation:\\n\",np.sqrt(-regr_mse))\nprint(\"The mean MSE on Cross-validation is {:.3f}\".format(np.mean(np.sqrt(-regr_mse))))\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nplt.figure('ridge_regr')\nplt.plot([np.expm1(y_val.min()),np.expm1(y_val.max())],[np.expm1(y_val.min()),np.expm1(y_val.max())],'k--',lw=2)\nplt.scatter(np.expm1(y_val),np.expm1(y_val_ridge_pred))\nplt.xlabel('Fact Price')\nplt.ylabel('Predict Price')\nplt.title('Predict Effect Using Ridge Regression')\nplt.show()","2dd9f0f6":"from sklearn.linear_model import LassoCV\nlscv = LassoCV(alphas=np.arange(0.0001,0.0005,0.00001))\nlscv.fit(X_tra,y_tra.values.ravel())\nprint('Best parameters found by randomised search:', lscv.alpha_, '\\n')","5ba8bd1c":"start_time = time.time()\n\nlasso= make_pipeline(RobustScaler(),linear_model.Lasso(alpha=0.00043999999999999996 ))\nlasso.fit(X_tra,y_tra)\n\ny_tra_lasso_pred=lasso.predict(X_tra)\nprint(\"The MSE on train set:\",np.sqrt(metrics.mean_squared_error(y_tra,y_tra_lasso_pred)))\n\ny_val_lasso_pred = lasso.predict(X_val)\nprint(\"The MSE on validation set:\",np.sqrt(metrics.mean_squared_error(y_val, y_val_lasso_pred)))\n\n#Cross-validation\nfrom sklearn.model_selection import cross_val_score\nregr_mse = cross_val_score(lasso, X_train, y_train, scoring='neg_mean_squared_error',cv=5)\nprint(\"MSE on Cross-validation:\\n\",np.sqrt(-regr_mse))\nprint(\"The mean MSE on Cross-validation is {:.3f}\".format(np.mean(np.sqrt(-regr_mse))))\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nplt.figure('ridge_regr')\nplt.plot([np.expm1(y_val.min()),np.expm1(y_val.max())],[np.expm1(y_val.min()),np.expm1(y_val.max())],'k--',lw=2)\nplt.scatter(np.expm1(y_val),np.expm1(y_val_lasso_pred))\nplt.xlabel('Fact Price')\nplt.ylabel('Predict Price')\nplt.title('Predict Effect Using Lasso Regression')\nplt.show()","16af856b":"from sklearn.ensemble import RandomForestRegressor\nfrom scipy.stats import randint as sp_randint\nstart_time = time.time()\n\nforest = make_pipeline(RobustScaler(), RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n                      max_features=0.95, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=3, min_samples_split=5,\n                      min_weight_fraction_leaf=0.0, n_estimators=100,\n                      n_jobs=None, oob_score=True, random_state=23,\n                      verbose=0, warm_start=False) )\n\nforest.fit(X_tra,y_tra.values.ravel())\n\ny_tra_forest_pred=forest.predict(X_tra)\nprint(\"The MSE on train set:\",np.sqrt(metrics.mean_squared_error(y_tra, y_tra_forest_pred)))\n\ny_val_forest_pred = forest.predict(X_val)\nprint(\"The MSE on validation set:\",np.sqrt(metrics.mean_squared_error(y_val, y_val_forest_pred)))\n\n\n#Cross-validation\nregr_mse = cross_val_score(forest, X_train, y_train, scoring='neg_mean_squared_error',cv=5)\nprint(\"MSE on Cross-validation:\\n\",np.sqrt(-regr_mse))\nprint(\"The mean MSE on Cross-validation is {:.3f}\".format(np.mean(np.sqrt(-regr_mse))))\n\n#caculate running time\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nplt.figure('forest')\nplt.plot([np.expm1(y_val.min()),np.expm1(y_val.max())],[np.expm1(y_val.min()),np.expm1(y_val.max())],'k--',lw=2)\nplt.scatter(np.expm1(y_val),np.expm1(y_val_forest_pred))\nplt.xlabel('Fact Price')\nplt.ylabel('Predict Price')\nplt.title('Predict Effect Using RandomForest')\nplt.show()\n","69aa277a":"from xgboost import XGBRegressor\nstart_time = time.time()\n\nxgb =  make_pipeline(RobustScaler(),XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.82, gamma=0,\n             importance_type='gain', learning_rate=0.2,\n             max_delta_step=0, max_depth=5, min_child_weight=7, missing=None,\n             n_estimators=200, n_jobs=-1, nthread=None, objective='reg:squarederror',\n             random_state=3, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n             seed=None, silent=None, subsample=0.7526315789473684, verbosity=1)) \n\nxgb.fit(X_tra,y_tra.values.ravel())\nprint (\"Validation:\", xgb.score(X_val,y_val))\n\ny_tra_xgb_pred=xgb.predict(X_tra)\nprint(\"The MSE on train set:\",np.sqrt(metrics.mean_squared_error(y_tra, y_tra_xgb_pred)))\n\ny_val_xgb_pred = xgb.predict(X_val)\nprint(\"The MSE on validation set:\",np.sqrt(metrics.mean_squared_error(y_val, y_val_xgb_pred)))\n\n\n#Cross-validation\nregr_mse = cross_val_score(xgb, X_train, y_train, scoring='neg_mean_squared_error',cv=5)\nprint(\"MSE on Cross-validation:\\n\",np.sqrt(-regr_mse))\nprint(\"The mean MSE on Cross-validation is {:.3f}\".format(np.mean(np.sqrt(-regr_mse))))\n\n#caculate running time\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nplt.figure('XGBoost')\nplt.plot([np.expm1(y_val.min()),np.expm1(y_val.max())],[np.expm1(y_val.min()),np.expm1(y_val.max())],'k--',lw=2)\nplt.scatter(np.expm1(y_val),np.expm1(y_val_xgb_pred))\nplt.xlabel('Fact Price')\nplt.ylabel('Predict Price')\nplt.title('Predict Effect Using XGBoost')\nplt.show()","c73d604a":"from  lightgbm  import LGBMRegressor\nstart_time = time.time()\n\nlgb = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nlgb.fit(X_tra,y_tra.values.ravel())\nprint (\"Validation:\", lgb.score(X_val,y_val))\n\ny_tra_lgb_pred=lgb.predict(X_tra)\nprint(\"The MSE on train set:\",np.sqrt(metrics.mean_squared_error(y_tra, y_tra_lgb_pred)))\n\ny_val_lgb_pred = lgb.predict(X_val)\nprint(\"The MSE on validation set:\",np.sqrt(metrics.mean_squared_error(y_val, y_val_lgb_pred)))\n\n\n#Cross-validation\nregr_mse = cross_val_score(lgb, X_train, y_train, scoring='neg_mean_squared_error',cv=5)\nprint(\"MSE on Cross-validation:\\n\",np.sqrt(-regr_mse))\nprint(\"The mean MSE on Cross-validation is {:.3f}\".format(np.mean(np.sqrt(-regr_mse))))\n\n#caculate running time\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nplt.figure('lgb')\nplt.plot([np.expm1(y_val.min()),np.expm1(y_val.max())],[np.expm1(y_val.min()),np.expm1(y_val.max())],'k--',lw=2)\nplt.scatter(np.expm1(y_val),np.expm1(y_val_lgb_pred))\nplt.xlabel('Fact Price')\nplt.ylabel('Predict Price')\nplt.title('Predict Effect Using LightGBM')\nplt.show()","87f313e2":"from sklearn.svm import SVR\nstart_time = time.time()\n\nsvr = SVR(kernel = 'poly')\nsvr.fit(X_tra,y_tra.values.ravel())\nprint (\"Validation:\", svr.score(X_val,y_val))\n\n\ny_tra_svr_pred=svr.predict(X_tra)\nprint(\"The MSE on train set:\",np.sqrt(metrics.mean_squared_error(y_tra, y_tra_svr_pred)))\n\ny_val_svr_pred = svr.predict(X_val)\nprint(\"The MSE on validation set:\",np.sqrt(metrics.mean_squared_error(y_val, y_val_svr_pred)))\n\n\n#Cross-validation\nregr_mse = cross_val_score(svr, X_train, y_train, scoring='neg_mean_squared_error',cv=5)\nprint(\"MSE on Cross-validation:\\n\",np.sqrt(-regr_mse))\nprint(\"The mean MSE on Cross-validation is {:.3f}\".format(np.mean(np.sqrt(-regr_mse))))\n\n#caculate running time\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nplt.figure('svr')\nplt.plot([np.expm1(y_val.min()),np.expm1(y_val.max())],[np.expm1(y_val.min()),np.expm1(y_val.max())],'k--',lw=2)\nplt.scatter(np.expm1(y_val),np.expm1(y_val_svr_pred))\nplt.xlabel('Fact Price')\nplt.ylabel('Predict Price')\nplt.title('Predict Effect Using SVR')\nplt.show()","de6b2b7d":"def stacked(all_data):\n    stacked_pred=0.1*regr.predict(all_data)+0.1*regr_ridge.predict(all_data)+\\\n            0.1*lasso.predict(all_data)+0.1*forest.predict(all_data)+0.2*xgb.predict(all_data)+\\\n            0.2*lgb.predict(all_data)+0.2*svr.predict(all_data)\n    return stacked_pred","2a31f07a":"start_time = time.time()\n\nstacked_train_pred=stacked(X_tra)\nprint(\"The MSE on train set:\",np.sqrt(metrics.mean_squared_error(y_tra, stacked_train_pred)))\n\nstacked_val_pred = stacked(X_val)\nprint(\"The MSE on validation set:\",np.sqrt(metrics.mean_squared_error(y_val, stacked_val_pred)))\n\n#caculate running time\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nplt.figure('stack')\nplt.plot([np.expm1(y_val.min()),np.expm1(y_val.max())],[np.expm1(y_val.min()),np.expm1(y_val.max())],'k--',lw=2)\nplt.scatter(np.expm1(y_val),np.expm1(stacked_val_pred))\nplt.xlabel('Fact Price')\nplt.ylabel('Predict Price')\nplt.title('Predict Effect Using Stacked Model')\nplt.show()","91dc9082":"Prediction_final=np.expm1(stacked(X_test))","113476a4":"sub = pd.DataFrame()\nsub['Id'] = test_data['Id']\nsub['SalePrice'] = Prediction_final\nprint(sub.head())\nsub.to_csv('submission.csv',index=False)","a0067fc3":"<a id='Drop-Highly-Correlated-Features'><\/a>\n## Drop Highly Correlated Features","3de6249d":"<a id='Skewed-Data-Process'><\/a>\n## Skewed Data Process","e562aa24":"Add new features:","4ea0f7c3":"<a id='Lasso-Regression'><\/a>\n## Lasso Regression","8152c860":"<a id='XGBoost'><\/a>\n## XGBoost","a4e2fda3":"<a id='set-up'><\/a>\n## Set up environment and import data","756a4f3c":"<a id='Submission'><\/a>\n# Submission","0bf46e3e":"Drop no need columns:","c6b3865a":"### Customize","0e3921d7":"Manualy Select","18c47a8a":"<a id='Create-New-Feature-Through-Kmeans'><\/a>\n## Create New Feature Through Kmeans","0e6e821a":"<a id='linear-regression'><\/a>\n## linear regression","6e19675a":"use strategy='median' to impute other numeric columns","79d4e2a9":"<a id='Category-Feature-Transformation'><\/a>\n## Category Feature Transformation","c6b0ac47":"<a id='Missing-Value'><\/a>\n## Missing Value","a393d9d6":"<a id='Ramdom-Forest'><\/a>\n## Ramdom Forest","08eef307":"Apply feature selected","2895ec0d":"<a id='Stacked-Model'><\/a>\n## Stacked Model","e02c5d67":"View the missing ratio in train and test dataset respectively.","cb101b3a":"### Label Encoder","ae430563":"<a id='LightGBM'><\/a>\n## LightGBM","97c883f7":"<a id='Feature-Selection'><\/a>\n## Feature Selection","c1e5a720":"<a id='Outlier-Detection'><\/a>\n## Outlier Detection","f3200ea2":"<a id='Dataset-Split'><\/a>\n# Dataset Split","224e552c":"Lasso-based feature selection","48c9f75d":"Correlation Feature Selection","0c217d5a":"<a id='Ridge-Regression'><\/a>\n## Ridge Regression","1b1ed3b2":"Transform ordered features:","9119f63b":"<a id='Modeling'><\/a>\n# Modeling","16d5c30e":"<a id='Initialization'><\/a>\n# Initialization","14b212b3":"tsne visualization","30a1df5f":"mark outlier","d096372e":"Manualy Process Missing Values","99685ee3":"use strategy='most_frequent' to impute other non-numeric columns","ce730366":"### Dummy","2f44d272":"Mutual Information Feature Selection","d60b0be1":"Manually dummy","a2f8b2c0":"<a id='Feature-Engineer'><\/a>\n# Feature Engineer","ea57214a":"* [Initialization](#Initialization)\n    - [Set up environment and import data](#set-up)\n* [Feature Engineer](#Feature-Engineer)\n    - [Missing Value](#Missing-Value)\n    - [Category Feature Transformation](#Category-Feature-Transformation)\n    - [Skewed Data Process](#Skewed-Data-Process)  \n    - [Outlier Detection](#Outlier-Detection)\n    - [Feature Selection](#Feature-Selection)\n    - [Create New Feature Through Kmeans](#Create-New-Feature-Through-Kmeans)  \n    - [Drop Highly Correlated Features](#Drop-Highly-Correlated-Features) \n* [Dataset Split](#Dataset-Split)\n* [Model](#Model)\n    - [linear regression](#linear-regression)\n    - [Ridge Regression](#Ridge-Regression)\n    - [Lasso Regression](#Lasso-Regression)\n    - [Ramdom Forest](#Ramdom-Forest)\n    - [XGBoost](#XGBoost)\n    - [LightGBM](#LightGBM) \n    - [SVR](#SVR)\n    - [Stacked Model](#Stacked-Model) \n* [Submission](#Submission)","5bd50978":"<a id='SVR'><\/a>\n## SVR","eff5685d":"check if still exist non numeric features","0dca6d41":"Tree-based feature selection"}}