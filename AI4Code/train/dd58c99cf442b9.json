{"cell_type":{"b95358a6":"code","a4e69ea8":"code","d7b6cd3c":"code","d0dad664":"code","ac6791c1":"code","a5276ef7":"code","9248ed18":"code","3c0fd27b":"code","d43503a3":"code","e355a914":"code","371a1c2d":"code","b953687a":"code","072fff08":"code","66213690":"code","279de119":"code","a1fda199":"code","42982e63":"code","f1bed479":"code","66120574":"code","3c92efe5":"code","08f0017c":"code","ba44cea6":"markdown"},"source":{"b95358a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n\n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4e69ea8":"! unzip ..\/input\/platesv2\/plates.zip","d7b6cd3c":"! ls","d0dad664":"! ls plates\/train\/\n","ac6791c1":"! pip install tf-nightly ","a5276ef7":"import tensorflow as tf\ntf.__version__","9248ed18":"image_size = (200,200)\nbatch_size = 10\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    \"plates\/train\",\n    validation_split=0.3,\n    subset=\"training\",\n    seed=1337,\n    image_size=image_size,\n    batch_size=batch_size\n)\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    \"plates\/train\",\n    validation_split=0.3,\n    subset=\"training\",\n    seed=1337,\n    image_size=image_size,\n    batch_size=batch_size)","3c0fd27b":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(int(labels[i]))\n        plt.axis(\"off\")","d43503a3":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\ndata_augmentation = keras.Sequential(\n    [\n        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n        layers.experimental.preprocessing.RandomRotation(0.45),\n        layers.experimental.preprocessing.RandomContrast(0.4),\n        layers.experimental.preprocessing.RandomZoom(0.1)\n    ]\n)","e355a914":"plt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        augmented_images = data_augmentation(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")\n","371a1c2d":"augmented_train_ds = train_ds.map(\n  lambda x, y: (data_augmentation(x, training=True), y))","b953687a":"augmented_train_ds","072fff08":"def make_model(input_shape, num_classes):\n    inputs = keras.Input(shape=input_shape)\n    # Image augmentation block\n    x = data_augmentation(inputs)\n\n    # Entry block\n    x = layers.experimental.preprocessing.Rescaling(1.0 \/ 255)(x)\n    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    previous_block_activation = x  # Set aside residual\n\n    for size in [128, 256, 512, 728]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n        # Project residual\n        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n            previous_block_activation\n        )\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    if num_classes == 2:\n        activation = \"sigmoid\"\n        units = 1\n    else:\n        activation = \"softmax\"\n        units = num_classes\n\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(units, activation=activation)(x)\n    return keras.Model(inputs, outputs)\n\n\nmodel = make_model(input_shape=image_size + (3,), num_classes=2)\nkeras.utils.plot_model(model, show_shapes=True)","66213690":"epochs = 50\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.h5\"),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n]\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-3),\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.fit(\n    train_ds, epochs=epochs, callbacks=callbacks, validation_data=val_ds,\n)","279de119":"img = keras.preprocessing.image.load_img(\n    \".\/plates\/train\/dirty\/0006.jpg\", target_size=image_size\n)\nimg_array = keras.preprocessing.image.img_to_array(img)\nimg_array = tf.expand_dims(img_array, 0)  # Create batch axis\n\npredictions = model.predict(img_array)\nscore = predictions[0]\nprint(\n    \"This image is %.2f percent dirty and %.2f percent clean.\"\n    % (100 * (1 - score), 100 * score)\n)","a1fda199":"from keras.preprocessing.image import ImageDataGenerator\ntest_datagen = ImageDataGenerator()\ntest_generator = test_datagen.flow_from_directory(  \n        'plates',\n        classes=['test'],\n        target_size = (200, 200),\n        batch_size = 1,\n        shuffle = False,        \n        class_mode = None)  ","42982e63":"test_generator.reset()\npredict = model.predict_generator(test_generator, steps = len(test_generator.filenames))\nlen(predict)","f1bed479":"import pandas as pd\nsub_df = pd.read_csv('..\/input\/platesv2\/sample_submission.csv', index_col='id')\nsub_df","66120574":"sub_df.label.value_counts()","3c92efe5":"sub_df['label'] = predict\nsub_df['label'] = sub_df['label'].apply(lambda x: 'dirty' if x > 0.5 else 'cleaned')\nsub_df.head()","08f0017c":"sub_df.to_csv('sub.csv', index=False)","ba44cea6":"# install TF nightly"}}