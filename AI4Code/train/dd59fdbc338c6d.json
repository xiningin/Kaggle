{"cell_type":{"5b0073c2":"code","54f9c667":"code","943376b3":"code","2692ee69":"code","11ba70ff":"code","911f48c5":"code","41f32e1a":"code","2d1b8470":"code","0562b1ee":"code","16f9d029":"code","82620c96":"code","e52e1ba8":"code","7fe60529":"code","782ef14c":"code","90f97db9":"code","2fbb31ed":"code","9c92c421":"code","f20d61ec":"code","ffdd646e":"code","1e4a6e00":"code","7afe9b4e":"code","5efc61e3":"code","46fd600f":"code","ece7ab3d":"code","8636bf41":"code","9440d11d":"code","99decca1":"code","a1b26c70":"code","6255cebd":"code","16acffd4":"code","8c283ae2":"code","ffe8d166":"code","72b2a060":"code","1d5848ba":"code","33c14188":"code","aa27ed1d":"code","56d24fd4":"code","1749acd5":"code","2cd88932":"code","7232d4b6":"code","e77d6cfe":"code","6374aefe":"code","ff888670":"code","077db06b":"code","c204b9b8":"code","38e000c8":"code","3f054556":"code","73e55e9a":"code","8fa13346":"code","a7a29288":"markdown","db5e12b1":"markdown","32477686":"markdown","d78d024b":"markdown","df9e5dac":"markdown","58959b38":"markdown","280e6ba9":"markdown","89244897":"markdown","1e80ab99":"markdown","8b79c1d7":"markdown","387fd43a":"markdown","611f8629":"markdown","e51e9c80":"markdown","20c62e37":"markdown","103c4a9b":"markdown"},"source":{"5b0073c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","54f9c667":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","943376b3":"df = pd.read_csv(\"..\/input\/insurance\/insurance.csv\")","2692ee69":"df.head()","11ba70ff":"df.info()","911f48c5":"df.describe().T","41f32e1a":"plt.figure(figsize=(10,8))\nsns.heatmap(df.corr())\nplt.show()","2d1b8470":"df.hist(figsize=(15,10))\nplt.show()","0562b1ee":"plt.figure(figsize=(9,6))\nsns.kdeplot(data = df , x=\"bmi\", hue=\"sex\", shade=True, palette=\"mako\")\nplt.show()","16f9d029":"plt.style.use(\"fivethirtyeight\")\n\nfig=plt.figure(figsize=(15,7))\nax=fig.add_subplot(121)\nax=sns.boxplot(df.sex, df.charges, palette=\"Pastel1\")\n\nax=fig.add_subplot(122)\nax=sns.boxplot(df.smoker,df.charges, palette=\"Set2\")\nax.set_ylabel(\"\")\nplt.show()","82620c96":"plt.figure(figsize=(15,7))\nsns.boxplot(data=df, x=\"children\",y=\"charges\", hue=\"sex\", palette=\"Pastel1\")\nplt.show()","e52e1ba8":"plt.figure(figsize=(15,7))\nsns.violinplot(data=df, x=\"region\", y=\"charges\", hue=\"smoker\", palette=\"crest\")\nplt.show()","7fe60529":"plt.figure(figsize=(12,6))\nsns.scatterplot(data=df, x=\"age\", y=\"charges\", hue=\"smoker\", palette=\"inferno_r\")\nplt.show()","782ef14c":"plt.figure(figsize=(12,6))\nsns.scatterplot(data=df, x=\"bmi\", y=\"charges\", hue=\"smoker\", palette=\"mako_r\")\nplt.show()","90f97db9":"df.isnull().sum()","2fbb31ed":"df.duplicated().sum()","9c92c421":"df.info()","f20d61ec":"# Children column int64 but is not true , this column should be object or categorical\ndf.children.unique()","ffdd646e":"df.children = df.children.astype(\"object\")","1e4a6e00":"cat_cols=df.select_dtypes(include=\"object\").columns\nnum_cols=df.select_dtypes(exclude=\"object\").columns","7afe9b4e":"cat_cols","5efc61e3":"num_cols","46fd600f":"from sklearn.preprocessing import OneHotEncoder\n\nohe= OneHotEncoder()\nohe_data = ohe.fit_transform(df[cat_cols]).toarray()\nohe_cols = ohe.get_feature_names(cat_cols)\n\nohe_df = pd.DataFrame(data=ohe_data, columns=ohe_cols)\n\ndf = df.join(ohe_df)","ece7ab3d":"df.drop(cat_cols, axis=1, inplace=True)\ndf.head()","8636bf41":"num_cols ","9440d11d":"num_cols = num_cols[1:]","99decca1":"# Outliers - zscore\n\nbefore = df.shape[0]\n\nfor col in num_cols :\n    \n    mean = df[col].mean()\n    std = df[col].std()\n    \n    max_val = mean + 3*std\n    min_val = mean - 3*std\n    \n    outliers = df[ (df[col]>max_val) | (df[col]<min_val)].index\n    \n    df.drop(outliers, axis=0, inplace=True)\n    \n\nafter = df.shape[0]\n\nprint(\"Total Number of Outleirs :\",(before-after))","a1b26c70":"# Skewness \n# if skewness > 0.5 ,  this is a high skewness\n\nfrom scipy.stats import skew\n\nskew_cols = df[num_cols].apply(lambda x : skew(x)).sort_values(ascending=False)\n\nskew_cols = skew_cols[skew_cols>0.5].index\n\ndf[skew_cols] = np.log1p(df[skew_cols])","6255cebd":"df[num_cols].skew()","16acffd4":"# Normalization\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\ncols = df.columns\n\ndata = scaler.fit_transform(df)\n\ndf = pd.DataFrame(data=data, columns=cols)\n\ndf.head()","8c283ae2":"X = df.drop(\"charges\", axis=1)\ny= df[\"charges\"]","ffe8d166":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=9) ","72b2a060":"from sklearn.metrics import mean_squared_error, r2_score","1d5848ba":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\n\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\n\nlr_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nlr_acc = round(r2_score(y_test,y_pred),2)*100\nprint(\"RMSE of Linear Regression:\",lr_rmse)\nprint(f\"Accuracy of Linear Regression {lr_acc} %\")","33c14188":"plt.figure(figsize=(8,6))\nsns.regplot(x=y_test, y=y_pred, \n            scatter_kws=dict(color=\"#4e75b5\"),\n            line_kws=dict(color=\"#b05862\", linewidth=3))\nplt.xlabel(\"True\")\nplt.ylabel(\"Pred\")\nplt.title(\"Linear Regression\")\nplt.show()","aa27ed1d":"from sklearn.tree import DecisionTreeRegressor\ndt_reg = DecisionTreeRegressor(max_depth=4, random_state=9)\ndt_reg.fit(X_train, y_train)\n\ny_pred = dt_reg.predict(X_test)\n\n\ndt_reg_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndt_reg_acc = round(r2_score(y_test, y_pred),2)*100\nprint(\"RMSE of DT Regressor:\",dt_reg_rmse)\nprint(f\"Accuracy of DT Regressor {dt_reg_acc} %\")","56d24fd4":"plt.figure(figsize=(8,6))\nsns.regplot(x=y_test, y=y_pred, \n            scatter_kws=dict(color=\"#4e75b5\"),\n            line_kws=dict(color=\"#b05862\", linewidth=3))\nplt.xlabel(\"True\")\nplt.ylabel(\"Pred\")\nplt.title(\"DT Regressor\")\nplt.show()","1749acd5":"from sklearn.ensemble import RandomForestRegressor\n\nrf_reg = RandomForestRegressor(max_depth=4 , random_state=9)\n\nrf_reg.fit(X_train, y_train)\n\ny_pred = rf_reg.predict(X_test)\n\nrf_reg_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nrf_reg_acc = round(r2_score(y_test, y_pred),2)*100\n\nprint(\"RMSE of RF Regressor:\",rf_reg_rmse)\nprint(f\"Accuracy of RF Regressor {rf_reg_acc} %\")","2cd88932":"plt.figure(figsize=(8,6))\nsns.regplot(x=y_test, y=y_pred, \n            scatter_kws=dict(color=\"#4e75b5\"),\n            line_kws=dict(color=\"#b05862\", linewidth=3))\nplt.xlabel(\"True\")\nplt.ylabel(\"Pred\")\nplt.title(\"RF Regressor\")\nplt.show()","7232d4b6":"from sklearn.ensemble import GradientBoostingRegressor\n\ngb_reg = GradientBoostingRegressor(random_state=9)\n\ngb_reg.fit(X_train, y_train)\n\ny_pred = gb_reg.predict(X_test)\n\ngb_reg_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ngb_reg_acc = round(r2_score(y_test, y_pred),1)*100\n\nprint(\"RMSE of GB Regressor:\",gb_reg_rmse)\nprint(f\"Accuracy of GB Regressor {gb_reg_acc} %\")","e77d6cfe":"plt.figure(figsize=(8,6))\nsns.regplot(x=y_test, y=y_pred, \n            scatter_kws=dict(color=\"#4e75b5\"),\n            line_kws=dict(color=\"#b05862\", linewidth=3))\nplt.xlabel(\"True\")\nplt.ylabel(\"Pred\")\nplt.title(\"GB Regressor\")\nplt.show()","6374aefe":"from sklearn.ensemble import AdaBoostRegressor\n\nada_reg = AdaBoostRegressor(random_state=9)\nada_reg.fit(X_train, y_train)\ny_pred = ada_reg.predict(X_test)\n\nada_reg_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nada_reg_acc = round(r2_score(y_test, y_pred),2)*100\n\nprint(\"RMSE of GB Regressor:\",ada_reg_rmse)\nprint(f\"Accuracy of GB Regressor {ada_reg_acc} %\")","ff888670":"plt.figure(figsize=(8,6))\nsns.regplot(x=y_test, y=y_pred, \n            scatter_kws=dict(color=\"#4e75b5\"),\n            line_kws=dict(color=\"#b05862\", linewidth=3))\nplt.xlabel(\"True\")\nplt.ylabel(\"Pred\")\nplt.title(\"AdaBoost Regressor\")\nplt.show()","077db06b":"from xgboost import XGBRegressor\n\nxgb_reg = XGBRegressor(random_state=9)\nxgb_reg.fit(X_train, y_train)\n\ny_pred = xgb_reg.predict(X_test)\n\nxgb_reg_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nxgb_reg_acc = round(r2_score(y_test, y_pred),2)*100\n\nprint(\"RMSE of GB Regressor:\",xgb_reg_rmse)\nprint(f\"Accuracy of GB Regressor {xgb_reg_acc} %\")","c204b9b8":"plt.figure(figsize=(8,6))\nsns.regplot(x=y_test, y=y_pred, \n            scatter_kws=dict(color=\"#4e75b5\"),\n            line_kws=dict(color=\"#b05862\", linewidth=3))\nplt.xlabel(\"True\")\nplt.ylabel(\"Pred\")\nplt.title(\"XGBoost Regressor\")\nplt.show()","38e000c8":"rmse_scores = {\"Linear Regression\": [lr_rmse],\n                  \"Decision Tree Regressor\": [dt_reg_rmse],\n                  \"Random Forest\": [rf_reg_rmse],\n                  \"Gradient Boosting Regressor\":[gb_reg_rmse],\n                  \"Ada Boost Regressor\" : [ada_reg_rmse],\n                  \"XGBRegressor\":[xgb_reg_rmse]\n              }\nrmse_scores = pd.DataFrame(rmse_scores)","3f054556":"plt.figure(figsize=(20,8))\nsns.barplot(rmse_scores.columns, rmse_scores.iloc[0], palette=\"Set2\")\nplt.title(\"RMSE of All Models\")\nplt.show()","73e55e9a":"r2_acc = {\"Linear Regression\": [lr_acc],\n                  \"Decision Tree Regressor\": [dt_reg_acc],\n                  \"Random Forest\": [rf_reg_acc],\n                  \"Gradient Boosting Regressor\":[gb_reg_acc],\n                  \"Ada Boost Regressor\" : [ada_reg_acc],\n                  \"XGBRegressor\":[xgb_reg_acc]\n              }\nr2_acc= pd.DataFrame(r2_acc)","8fa13346":"plt.figure(figsize=(20,8))\nsns.barplot(r2_acc.columns, r2_acc.iloc[0], palette=\"Set2\")\nplt.title(\"R2 Accuracy Scores %\")\nplt.show()","a7a29288":"## Null-Duplicated Values","db5e12b1":"## Gradient Boosting Regressor","32477686":"# Evaluating All Models","d78d024b":"## Decision Tree Regressor","df9e5dac":"# EDA","58959b38":"## Linear Regression","280e6ba9":"### Columns\n\n- **age:** age of primary beneficiary\n- **sex:** insurance contractor gender, female, male\n- **bmi:** Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,\nobjective index of body weight (kg \/ m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n- **children:** Number of children covered by health insurance \/ Number of dependents\n- **smoker:** Smoking\n- **region:** the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n- **charges:** Individual medical costs billed by health insurance","89244897":"## Random Forest","1e80ab99":"## Adaboost Regressor","8b79c1d7":"# Models","387fd43a":"# Data Preprocessing","611f8629":"### Numerical Columns \n    Outliers - Skewness - Normalization\n\n    - i don't necessary any process on Age column so i will skip this","e51e9c80":"## Encoding","20c62e37":"### Categorical Columns Encoding","103c4a9b":"## XGBoost Regressor"}}