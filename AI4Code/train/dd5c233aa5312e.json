{"cell_type":{"9c77b4fc":"code","a4025691":"code","f2a4798b":"code","dbb0a9fe":"code","373f402f":"code","499891e6":"code","cc236422":"code","1194344a":"code","88512f58":"code","7b48918a":"code","5a857c74":"code","09d41598":"code","91ba9220":"code","dfb467b4":"code","28ab7bea":"code","e836e852":"code","ef591afb":"code","8429e76a":"code","13ba0588":"code","d8d4e10a":"code","30e5e1f4":"code","9c9d0e52":"code","1429aff9":"code","33b732e9":"code","56224cd5":"code","90d28379":"code","ed9479ee":"code","08556d53":"code","6972ed93":"code","a8f5ffed":"code","eded121c":"code","0b463723":"code","2c2033db":"code","e7796336":"code","4ad98cc6":"code","cdf8e263":"code","c75cb37f":"code","38c0b57f":"code","7f5ce159":"code","9f587282":"code","2aee3f3c":"code","0c7a27bf":"code","4417040d":"code","0ad9916b":"code","fac79d45":"code","742405d2":"code","4cc266d8":"code","6d1217cc":"code","ee9df876":"code","cf30774d":"code","ee357366":"code","87baf9d0":"code","fac3d50c":"code","645f447f":"code","128ebc01":"code","bf6949e5":"code","540a9ff4":"code","ce2ce833":"code","2212cf19":"code","bb355348":"code","159d6418":"code","4aabff7c":"code","cf892c2e":"code","6f5a03f6":"code","423ae7c7":"code","caa3a521":"code","9e506512":"code","3a9946c8":"code","f8653add":"code","b90ef2d8":"code","7055eb74":"code","8a550b33":"code","a9d3d446":"code","b94703ce":"code","cb35e3cb":"code","ebe21cf0":"code","5d1a7cef":"code","a6e82ca6":"code","da70e449":"code","e37d7631":"code","3f84738d":"code","5e64b8e8":"code","786a3cd5":"code","b0a8b0ee":"code","cdf2f928":"code","b5d28070":"code","0993ea0a":"code","a6ef6824":"code","5b0c19f0":"code","ed42df2b":"code","3a467976":"code","08959230":"code","0257818d":"code","0122e864":"code","3990b7dc":"code","2a9e50fa":"code","881bc333":"code","858ed11b":"code","a3cfe9eb":"code","b2d5918f":"code","a0278c6b":"code","0da29cce":"code","7cbdf865":"code","c1e16c78":"code","e31945e3":"code","70855dba":"code","130c8726":"code","2f4b97a7":"code","560b7079":"code","78d334c2":"code","26c54a5f":"code","4288fe0a":"code","364fd667":"markdown","bad71c20":"markdown","741be847":"markdown","ed45e900":"markdown","16be4ba4":"markdown","0602cc0f":"markdown","5a775a52":"markdown","84e1278b":"markdown","63445421":"markdown","70485d87":"markdown","4256d8dc":"markdown","44b35d1a":"markdown","883d7504":"markdown","bc1aa1ad":"markdown","e6d651b7":"markdown","a6d11d33":"markdown","f53cd6e7":"markdown","24745a19":"markdown"},"source":{"9c77b4fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# My imports\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a4025691":"data_root = \"..\/input\"\noutput_dir = '.\/'","f2a4798b":"test = pd.read_csv(os.path.join(data_root, \"test.csv\"))\ntest.head()","dbb0a9fe":"train = pd.read_csv(os.path.join(data_root, \"train.csv\"))\ntrain.head()","373f402f":"merchants = pd.read_csv(os.path.join(data_root, \"merchants.csv\"))\nmerchants.head()","499891e6":"historical_transactions = pd.read_csv(os.path.join(data_root, \"historical_transactions.csv\"))\nhistorical_transactions.head()","cc236422":"new_merchant_transactions = pd.read_csv(os.path.join(data_root, \"new_merchant_transactions.csv\"))\nnew_merchant_transactions.head()","1194344a":"engineered_test = pd.DataFrame()","88512f58":"engineered_test = test","7b48918a":"engineered_train = pd.DataFrame()\nengineered_train_outlier = pd.DataFrame()","5a857c74":"plt.figure()\ntrain['target'].hist()\nplt.show()","09d41598":"train[train['target'] < -20]['target'].value_counts()","91ba9220":"engineered_train = train[train['target'] > -20]\nengineered_train.head()","dfb467b4":"engineered_train_outlier = train[train['target'] < -20]\nengineered_train_outlier.head()","28ab7bea":"fig, ax = plt.subplots(1, 6, figsize=(24,4))\nengineered_train['feature_1'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal', title='train feature_1')\nengineered_train['feature_2'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown', title='train feature_2')\nengineered_train['feature_3'].value_counts().sort_index().plot(kind='bar', ax=ax[2], color='gold', title='train feature_3')\ntest['feature_1'].value_counts().sort_index().plot(kind='bar', ax=ax[3], color='teal', title='test feature_1')\ntest['feature_2'].value_counts().sort_index().plot(kind='bar', ax=ax[4], color='brown', title='test feature_2')\ntest['feature_3'].value_counts().sort_index().plot(kind='bar', ax=ax[5], color='gold', title='test feature_3')\nplt.show()","e836e852":"fig, ax = plt.subplots(1, 6, figsize=(24,4))\nfor i in range(5):\n    title = \"feature_1=\" + str(i+1)\n    engineered_train[engineered_train['feature_1'] == i+1]['target'].hist(ax=ax[i], bins=50)\n    ax[i].set_title(title)\nengineered_train['target'].hist(ax=ax[-1], bins=50)\nax[-1].set_title('overall train target')\nfig.suptitle('Target distribution with all feature_1 values')\nplt.show()","ef591afb":"fig, ax = plt.subplots(1, 4, figsize=(20,4))\nfor i in range(3):\n    title = \"feature_2=\" + str(i+1)\n    engineered_train[engineered_train['feature_2'] == i+1]['target'].hist(ax=ax[i], bins=50)\n    ax[i].set_title(title)\nengineered_train['target'].hist(ax=ax[-1], bins=50)\nax[-1].set_title('overall train target')\nfig.suptitle('Target distribution with all feature_2 values')\nplt.show()","8429e76a":"fig, ax = plt.subplots(1, 3, figsize=(12,4))\nfor i in range(2):\n    title = \"feature_3=\" + str(i)\n    engineered_train[engineered_train['feature_3'] == i]['target'].hist(ax=ax[i], bins=50)\n    ax[i].set_title(title)\nengineered_train['target'].hist(ax=ax[-1], bins=50)\nax[-1].set_title('overall train target')\nfig.suptitle('Target distribution with all feature_3 values')\nplt.show()","13ba0588":"def month_to_timestamp(month):\n    time_format = '%Y-%m'\n    if type(month) is str:\n        timestamp = datetime.strptime(month,time_format).timestamp()\n        return timestamp\n    else:\n        return 1.5 * 1000000000","d8d4e10a":"engineered_train['first_active_month_timestamp'] = train[train['target'] > -20]['first_active_month'].apply(month_to_timestamp)","30e5e1f4":"engineered_train_outlier['first_active_month_timestamp'] = train[train['target'] < -20]['first_active_month'].apply(month_to_timestamp)","9c9d0e52":"engineered_test['first_active_month_timestamp'] = test['first_active_month'].apply(month_to_timestamp)","1429aff9":"engineered_train.head()","33b732e9":"engineered_train_outlier.head()","56224cd5":"engineered_test.head()","90d28379":"plt.figure()\nengineered_train['first_active_month_timestamp'].value_counts().sort_index().plot(label='train')\nengineered_test['first_active_month_timestamp'].value_counts().sort_index().plot(label='test')\nplt.legend()\nplt.show()","ed9479ee":"plt.figure()\nengineered_train.groupby('first_active_month_timestamp')['target'].mean().plot()\nplt.title('mean target given timestamp')\nplt.ylabel('mean target')\nplt.show()","08556d53":"plt.figure()\nengineered_train.groupby('first_active_month_timestamp')['target'].var().plot()\nplt.title('var target given timestamp')\nplt.ylabel('var target')\nplt.show()","6972ed93":"new_merchant_transactions.head()","a8f5ffed":"city_counts = new_merchant_transactions['city_id'].value_counts()\nunique_city = len(city_counts)\nmax_city = city_counts.idxmax()\nmax_count = city_counts[max_city]\nmax_percentage = max_count \/ len(new_merchant_transactions)\nprint('unique city: ', unique_city)\nprint('majority city: ', max_city)\nprint('count: ', max_count)\nprint('percentage: ', max_percentage)","eded121c":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\ntrain.merge(new_merchant_transactions[new_merchant_transactions['city_id'] == 69].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=40, ax=ax[0])\ntrain.merge(new_merchant_transactions[new_merchant_transactions['city_id'] != 69].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=40, ax=ax[1])\nax[0].set_title('with city_id as 69')\nax[1].set_title('with city_id as not 69')\nplt.show()","0b463723":"plt.figure(figsize=(20,4))\nnew_merchant_transactions['subsector_id'].value_counts().sort_index().plot(kind='bar')\nplt.title('Distribution of subsector_id in new_merchant_transactions')\nplt.xlabel('subsector_id')\nplt.ylabel('count')\nplt.show()","2c2033db":"fig, ax = plt.subplots(1, 2, figsize=(24,8))\ntrain.merge(new_merchant_transactions.groupby('card_id')['subsector_id'].nunique().to_frame(), how='inner', on='card_id').plot.scatter(y='target', x='subsector_id', ax=ax[0])\nengineered_train.merge(new_merchant_transactions.groupby('card_id')['subsector_id'].nunique().to_frame(), how='inner', on='card_id').plot.scatter(y='target', x='subsector_id', ax=ax[1])\nax[0].set_title('for all the training data')\nax[1].set_title('for training data without outliers')\nfig.suptitle('target and subsector_id coverage')\nplt.show()","e7796336":"fig, ax = plt.subplots(1, 2, figsize=(24,8))\ntrain.merge(new_merchant_transactions.groupby('card_id')['purchase_amount'].mean().to_frame(), how='inner', on='card_id').plot.scatter(y='target', x='purchase_amount', ax=ax[0])\nengineered_train.merge(new_merchant_transactions.groupby('card_id')['purchase_amount'].mean().to_frame(), how='inner', on='card_id').plot.scatter(y='target', x='purchase_amount', ax=ax[1])\nax[0].set_title('for all the training data')\nax[1].set_title('for training data without outliers')\nfig.suptitle('target and mean purchase amount')\nplt.show()","4ad98cc6":"engineered_train['new_purchase_mean'] = engineered_train.merge(new_merchant_transactions.groupby('card_id')['purchase_amount'].mean().to_frame(), how='left', on='card_id')['purchase_amount']","cdf8e263":"engineered_test['new_purchase_mean'] = engineered_test.merge(new_merchant_transactions.groupby('card_id')['purchase_amount'].mean().to_frame(), how='left', on='card_id')['purchase_amount']","c75cb37f":"fig, ax = plt.subplots(1, 2, figsize=(24,8))\ntrain.merge(new_merchant_transactions.groupby('card_id')['purchase_amount'].sum().to_frame(), how='inner', on='card_id').plot.scatter(y='target', x='purchase_amount', ax=ax[0])\nengineered_train.merge(new_merchant_transactions.groupby('card_id')['purchase_amount'].sum().to_frame(), how='inner', on='card_id').plot.scatter(y='target', x='purchase_amount', ax=ax[1])\nax[0].set_title('for all the training data')\nax[1].set_title('for training data without outliers')\nfig.suptitle('target and total purchase amount')\nplt.show()","38c0b57f":"engineered_train['new_purchase_sum'] = engineered_train.merge(new_merchant_transactions.groupby('card_id')['purchase_amount'].sum().to_frame(), how='left', on='card_id')['purchase_amount'].fillna(0)","7f5ce159":"engineered_test['new_purchase_sum'] = engineered_test.merge(new_merchant_transactions.groupby('card_id')['purchase_amount'].sum().to_frame(), how='left', on='card_id')['purchase_amount'].fillna(0)","9f587282":"plt.figure()\nnew_merchant_transactions['category_3'].value_counts().sort_index().plot(kind='bar')\nplt.title('Distribution of category_3 in new_merchant_transactions')\nplt.xlabel('category_3')\nplt.ylabel('count')\nplt.show()","2aee3f3c":"fig, ax = plt.subplots(1, 3, figsize=(15,5))\ntrain.merge(new_merchant_transactions[new_merchant_transactions['category_3'] == 'A'].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[0])\ntrain.merge(new_merchant_transactions[new_merchant_transactions['category_3'] == 'B'].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[1])\ntrain.merge(new_merchant_transactions[new_merchant_transactions['category_3'] == 'C'].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[2])\nax[0].set_title('with category_2 as A')\nax[1].set_title('with category_2 as B')\nax[2].set_title('with category_2 as C')\nplt.show()","0c7a27bf":"plt.figure()\nnew_merchant_transactions['category_2'].value_counts().sort_index().plot(kind='bar')\nplt.title('Distribution of category_2 in new_merchant_transactions')\nplt.xlabel('category_2')\nplt.ylabel('count')\nplt.show()","4417040d":"fig, ax = plt.subplots(1, 5, figsize=(15,3))\ntrain.merge(new_merchant_transactions[new_merchant_transactions['category_2'] == 1].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[0])\ntrain.merge(new_merchant_transactions[new_merchant_transactions['category_2'] == 2].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[1])\ntrain.merge(new_merchant_transactions[new_merchant_transactions['category_2'] == 3].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[2])\ntrain.merge(new_merchant_transactions[new_merchant_transactions['category_2'] == 4].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[3])\ntrain.merge(new_merchant_transactions[new_merchant_transactions['category_2'] == 5].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[4])\nax[0].set_title('with category_2 as 1')\nax[1].set_title('with category_2 as 2')\nax[2].set_title('with category_2 as 3')\nax[3].set_title('with category_2 as 4')\nax[4].set_title('with category_2 as 5')\nplt.show()","0ad9916b":"plt.figure()\nnew_merchant_transactions['category_1'].value_counts().sort_index().plot(kind='bar')\nplt.title('Distribution of category_1 in new_merchant_transactions')\nplt.xlabel('category_1')\nplt.ylabel('count')\nplt.show()","fac79d45":"fig, ax = plt.subplots(1, 2, figsize=(10,4))\ntrain.merge(new_merchant_transactions[new_merchant_transactions['category_1'] == 'N'].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[0])\ntrain.merge(new_merchant_transactions[new_merchant_transactions['category_1'] == 'Y'].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[1])\nax[0].set_title('with category_1 as Y')\nax[1].set_title('with category_1 as N')\nplt.show()","742405d2":"plt.figure()\nnew_merchant_transactions['installments'].value_counts().sort_index().plot(kind='bar')\nplt.title('Distribution of installments in new_merchant_transactions')\nplt.xlabel('installments')\nplt.ylabel('count')\nplt.show()","4cc266d8":"new_merchant_transactions[(new_merchant_transactions['installments'] == -1) | (new_merchant_transactions['installments'] == 999)]['installments'].value_counts()","6d1217cc":"fig, ax = plt.subplots(1, 2, figsize=(10,4))\ntrain.merge(new_merchant_transactions[new_merchant_transactions['installments'] == -1].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[0])\ntrain.merge(new_merchant_transactions[new_merchant_transactions['installments'] != -1].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[1])\nax[0].set_title('with installments as -1')\nax[1].set_title('with installments as not -1')\nplt.show()","ee9df876":"fig, ax = plt.subplots(1, 2, figsize=(10,4))\ntrain.merge(new_merchant_transactions[(new_merchant_transactions['installments'] == 0) | (new_merchant_transactions['installments'] == 1)].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[0])\ntrain.merge(new_merchant_transactions[(new_merchant_transactions['installments'] != 0) & (new_merchant_transactions['installments'] != 1)].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[1])\nax[0].set_title('with installments as 0 or 1')\nax[1].set_title('with installments as otherwise')\nplt.show()","cf30774d":"new_merchant_transactions['authorized_flag'].value_counts()","ee357366":"plt.figure()\nnew_merchant_transactions['state_id'].value_counts().sort_index().plot(kind='bar')\nplt.title('Distribution of state_id in new_merchant_transactions')\nplt.xlabel('state_id')\nplt.ylabel('count')\nplt.show()","87baf9d0":"fig, ax = plt.subplots(1, 2, figsize=(10,4))\ntrain.merge(new_merchant_transactions[new_merchant_transactions['state_id'] == 9].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[0])\ntrain.merge(new_merchant_transactions[new_merchant_transactions['state_id'] != 9].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=20, ax=ax[1])\nax[0].set_title('with state_id as 9')\nax[1].set_title('with state_id as not 9')\nplt.show()","fac3d50c":"fig, ax = plt.subplots(1, 2, figsize=(10,4))\nengineered_train.merge(new_merchant_transactions[new_merchant_transactions['state_id'] == 9].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=40, ax=ax[0])\nengineered_train.merge(new_merchant_transactions[new_merchant_transactions['state_id'] != 9].groupby('card_id')['installments'].mean().to_frame(), how='inner', on='card_id')['target'].hist(bins=40, ax=ax[1])\nax[0].set_title('with state_id as 9')\nax[1].set_title('with state_id as not 9')\nplt.show()","645f447f":"unique_in_new_merchant_transactions = len(new_merchant_transactions['card_id'].value_counts())\nunique_in_train = len(engineered_train['card_id'].value_counts())\ncommon_in_two = len(pd.merge(engineered_train, new_merchant_transactions.groupby('card_id')['month_lag'].mean().to_frame(), on='card_id', how='inner'))\nprint('unique card ids in new_merchant_transactions: ', unique_in_new_merchant_transactions)\nprint('unique card ids in train: ', unique_in_train)\nprint('common values: ', common_in_two)\nprint('missing values: ', unique_in_train - common_in_two)","128ebc01":"mean_month_lag = new_merchant_transactions['month_lag'].mean()\nprint('mean month_lag: ', mean_month_lag)\nengineered_train['month_lag'] = train[train['target'] > -20].merge(new_merchant_transactions.groupby('card_id')['month_lag'].mean().to_frame(), how='left', on='card_id')['month_lag'].fillna(mean_month_lag)\nengineered_train.head()","bf6949e5":"engineered_train_outlier['month_lag'] = train[train['target'] < -20].merge(new_merchant_transactions.groupby('card_id')['month_lag'].mean().to_frame(), how='left', on='card_id')['month_lag'].fillna(mean_month_lag)\nengineered_train_outlier.head()","540a9ff4":"plt.figure()\nengineered_train['month_lag'].hist(bins=40)\nplt.show()","ce2ce833":"fig, ax = plt.subplots(1, 2, figsize=(10,4))\nengineered_train[engineered_train['month_lag'] == 1.0]['target'].hist(bins=20, ax=ax[0])\nengineered_train[engineered_train['month_lag'] == 2.0]['target'].hist(bins=20, ax=ax[1])\nplt.show()","2212cf19":"fig, ax = plt.subplots(1, 10, figsize=(30,3))\nfor i in range(10):\n    title = str(1.0+0.1*i) + '~' + str(1.0+0.1*(i+1))\n    engineered_train[(engineered_train['month_lag'] > 1.0+0.1*i) & (engineered_train['month_lag'] < 1.0+0.1*(i+1))]['target'].hist(bins=20, ax=ax[i])\n    ax[i].set_title(title)\nplt.show()","bb355348":"historical_transactions.head()","159d6418":"installments_999 = historical_transactions[historical_transactions['installments'] == 999]","4aabff7c":"plt.figure()\ninstallments_999['authorized_flag'].value_counts().sort_index().plot(kind='bar')\nplt.show()","cf892c2e":"count_999 = len(historical_transactions[historical_transactions['installments'] == 999])\nunauthorized_count = len(historical_transactions[historical_transactions['authorized_flag'] == 'N'])\nprint('count_999: ', count_999)\nprint('unauthorized_count: ', unauthorized_count)","6f5a03f6":"installments_999_unique_cards = installments_999.groupby('card_id')['installments'].mean().to_frame()","423ae7c7":"print('total unique 999 cards: ', len(installments_999_unique_cards))\ninstallments_999_unique_cards.head()","caa3a521":"plt.figure()\ntrain.merge(installments_999_unique_cards, how='inner', on='card_id')['target'].hist(bins=40)\nplt.show()","9e506512":"fig, ax = plt.subplots(1, 2, figsize=(24,8))\ntrain.merge(historical_transactions.groupby('card_id')['purchase_amount'].mean().to_frame(), how='inner', on='card_id').plot.scatter(y='target', x='purchase_amount', ax=ax[0])\nengineered_train.merge(historical_transactions.groupby('card_id')['purchase_amount'].mean().to_frame(), how='inner', on='card_id').plot.scatter(y='target', x='purchase_amount', ax=ax[1])\nax[0].set_title('for all the training data')\nax[0].set_xlim([-10, 90])\nax[1].set_title('for training data without outliers')\nax[1].set_xlim([-10, 90])\nfig.suptitle('target and mean purchase amount')\nplt.show()","3a9946c8":"engineered_train['history_purchase_mean'] = engineered_train.merge(historical_transactions.groupby('card_id')['purchase_amount'].mean().to_frame(), how='left', on='card_id')['purchase_amount']","f8653add":"engineered_test['history_purchase_mean'] = engineered_test.merge(historical_transactions.groupby('card_id')['purchase_amount'].mean().to_frame(), how='left', on='card_id')['purchase_amount']","b90ef2d8":"fig, ax = plt.subplots(1, 2, figsize=(24,8))\ntrain.merge(historical_transactions.groupby('card_id')['purchase_amount'].sum().to_frame(), how='inner', on='card_id').plot.scatter(y='target', x='purchase_amount', ax=ax[0])\nengineered_train.merge(historical_transactions.groupby('card_id')['purchase_amount'].sum().to_frame(), how='inner', on='card_id').plot.scatter(y='target', x='purchase_amount', ax=ax[1])\nax[0].set_title('for all the training data')\nax[0].set_xlim([-800, 500])\nax[1].set_title('for training data without outliers')\nax[1].set_xlim([-800, 500])\nfig.suptitle('target and mean purchase amount')\nplt.show()","7055eb74":"engineered_train['history_purchase_sum'] = engineered_train.merge(historical_transactions.groupby('card_id')['purchase_amount'].sum().to_frame(), how='left', on='card_id')['purchase_amount'].fillna(0)","8a550b33":"engineered_test['history_purchase_sum'] = engineered_test.merge(historical_transactions.groupby('card_id')['purchase_amount'].sum().to_frame(), how='left', on='card_id')['purchase_amount'].fillna(0)","a9d3d446":"min_first_active_month = min(engineered_train['first_active_month_timestamp'].min(), engineered_test['first_active_month_timestamp'].min())\nmax_first_active_month = max(engineered_train['first_active_month_timestamp'].max(), engineered_test['first_active_month_timestamp'].max())\nprint('first_active_month range: ', min_first_active_month, ' ~ ', max_first_active_month)\nfirst_active_month_range = max_first_active_month - min_first_active_month\nprint('range: ', first_active_month_range)\nplt.figure()\nengineered_test['first_active_month_timestamp'].hist()\nplt.show()","b94703ce":"def normalize_timestamp(timestamp):\n    return (timestamp - min_first_active_month) \/ first_active_month_range","cb35e3cb":"engineered_train['normalized_timestamp'] = engineered_train['first_active_month_timestamp'].apply(normalize_timestamp)","ebe21cf0":"engineered_test['normalized_timestamp'] = engineered_test['first_active_month_timestamp'].apply(normalize_timestamp)","5d1a7cef":"def normalize_target(target):\n    return (target + 20) \/ 40","a6e82ca6":"engineered_train['normalized_target'] = engineered_train['target'].fillna(0).apply(normalize_target)","da70e449":"plt.figure()\nengineered_train['normalized_target'].hist(bins=40)\nplt.show()","e37d7631":"min_history_purchase_sum = min(engineered_train['history_purchase_sum'].min(), engineered_test['history_purchase_sum'].min())\nmax_history_purchase_sum = max(engineered_train['history_purchase_sum'].max(), engineered_test['history_purchase_sum'].max())\nhistory_purchase_sum_na = engineered_train['history_purchase_sum'].isna().count()\nhistory_purchase_sum_not_na = len(engineered_train) - history_purchase_sum_na\nprint('history_purchase_sum_na: ', history_purchase_sum_na)\nprint('history_purchase_sum_not_na: ', history_purchase_sum_not_na)\nprint('history_purchase_sum range: ', min_history_purchase_sum, ' ~ ', max_history_purchase_sum)\nhistory_purchase_sum_range = max_history_purchase_sum - min_history_purchase_sum\nprint('range: ', history_purchase_sum_range)\nplt.figure()\nengineered_train['history_purchase_sum'].hist()\nplt.show()\nplt.figure()\nengineered_train[(engineered_train['history_purchase_sum'].fillna(0) < 200) & (engineered_train['history_purchase_sum'].fillna(0) > -500)]['history_purchase_sum'].hist(bins=40)\nplt.show()","3f84738d":"def normalize_history_purchase_sum(history_purchase_sum):\n    if history_purchase_sum < -500.0:\n        return 0\n    elif history_purchase_sum > 200.0:\n        return 1\n    else:\n        return (history_purchase_sum + 500.0) \/ 700.0","5e64b8e8":"engineered_train['normalized_history_purchase_sum'] = engineered_train['history_purchase_sum'].fillna(0).apply(normalize_history_purchase_sum)","786a3cd5":"engineered_test['normalized_history_purchase_sum'] = engineered_test['history_purchase_sum'].fillna(0).apply(normalize_history_purchase_sum)","b0a8b0ee":"fig, ax = plt.subplots(1, 2, figsize=(16,5))\nengineered_train['normalized_history_purchase_sum'].hist(ax=ax[0], bins=40)\nengineered_test['normalized_history_purchase_sum'].hist(ax=ax[1], bins=40)\nplt.show()","cdf2f928":"min_history_purchase_mean = min(engineered_train['history_purchase_mean'].min(), engineered_test['history_purchase_mean'].min())\nmax_history_purchase_mean = max(engineered_train['history_purchase_mean'].max(), engineered_test['history_purchase_mean'].max())\nprint('max_history_purchase_mean range: ', min_history_purchase_mean, ' ~ ', max_history_purchase_mean)\nhistory_purchase_mean_range = max_history_purchase_mean - min_history_purchase_mean\nprint('range: ', history_purchase_mean_range)\nplt.figure()\nengineered_train[(engineered_train['history_purchase_mean'].fillna(0) < 2) & (engineered_train['history_purchase_mean'].fillna(0) > -1)]['history_purchase_mean'].hist(bins=40)\nplt.show()","b5d28070":"def normalize_history_purchase_mean(history_purchase_mean):\n    if history_purchase_mean < -1:\n        return 0\n    elif history_purchase_mean > 2:\n        return 1\n    return (history_purchase_mean + 1) \/ 3","0993ea0a":"engineered_train['normalized_history_purchase_mean'] = engineered_train['history_purchase_mean'].fillna(0).apply(normalize_history_purchase_mean)","a6ef6824":"engineered_test['normalized_history_purchase_mean'] = engineered_test['history_purchase_mean'].fillna(0).apply(normalize_history_purchase_mean)","5b0c19f0":"fig, ax = plt.subplots(1, 2, figsize=(16,5))\nengineered_train['normalized_history_purchase_mean'].hist(ax=ax[0], bins=40)\nengineered_test['normalized_history_purchase_mean'].hist(ax=ax[1], bins=40)\nplt.show()","ed42df2b":"historical_transactions[:100].to_csv('light_historical_transactions.csv')","3a467976":"engineered_train.head()","08959230":"engineered_test.head()","0257818d":"engineered_train[:100].to_csv('light_engineered_train.csv')","0122e864":"engineered_test[:100].to_csv('light_engineered_test.csv')","3990b7dc":"engineered_train.to_csv('engineered_train.csv')","2a9e50fa":"engineered_test.to_csv('engineered_test.csv')","881bc333":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers.normalization import BatchNormalization","858ed11b":"model = Sequential()\nmodel.add(Dense(64, activation='relu', input_dim=3))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='sgd', loss='mean_squared_error')","a3cfe9eb":"tags = ['normalized_timestamp', 'normalized_history_purchase_sum', 'normalized_history_purchase_mean']","b2d5918f":"training_set = engineered_train[tags].values\ntraining_set","a0278c6b":"training_target = engineered_train['target'].values\ntraining_target","0da29cce":"testing_set = engineered_test[tags].values\ntesting_set","7cbdf865":"history = model.fit(x=training_set, y=training_target, epochs=50)","c1e16c78":"pred = model.predict(x=testing_set, verbose=1)","e31945e3":"pred.flatten()","70855dba":"print('prediction length: ', len(pred.flatten().tolist()))\nprint('engineered test length: ', len(engineered_test['card_id']))","130c8726":"output_result = pd.DataFrame({'card_id': engineered_test['card_id'].values.tolist(), 'target': pred.flatten().tolist()})","2f4b97a7":"output_result.head()","560b7079":"def denormalize_target(target):\n    return target * 40 - 20","78d334c2":"output_result['target'] = engineered_train['target'].fillna(0.5).apply(normalize_target)","26c54a5f":"output_result.fillna(0.0)","4288fe0a":"output_result.fillna(0.0).to_csv('output_result.csv', index=False)","364fd667":"Huh? Is this some kind of place holder for spam cards??\nMaybe a binary classifier should be used to determine this group","bad71c20":"Wat? really? Maybe only historial data make some differentiation in this field?","741be847":"# Do some preprocessing to make life easier","ed45e900":"It seems that time indicates both the mean and variance of the target value","16be4ba4":"So -1 might just mean it is missing. It doesn't indicate anything. For 999, there are too few in new_merchant_transactions. Will have to find out in historical one","0602cc0f":"# Take a look at the so called training features","5a775a52":"Well... very likely it is another useless feature","84e1278b":"Hmmmmm, but is this just because it is a indication of more purchase?","63445421":"hmmmmm what can I say ...","70485d87":"# This section outputs some files that can be used instead of the original files","4256d8dc":"# Now historical data. Please give me something useful :(","44b35d1a":"# Do some sanity check to see if it make sense at all","883d7504":"This tells that although it is mostly normal distribution, but the two extremes encodes some information","bc1aa1ad":"1. Huh? 999 and -1? What is that exactly?","e6d651b7":"# Work in progress","a6d11d33":"It seems that feature 1, 2, and 3 are pretty much useless\n\nThen how about active month?","f53cd6e7":"Hmmm mystery solved. it seems that 999 is kind of spam that cannot get authorized, but is that useful?","24745a19":"# Now what does new_merchant_transactions give us?"}}