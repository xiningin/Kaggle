{"cell_type":{"6dd36c3d":"code","c40ac3cd":"code","5cccbeb7":"code","e0cdb87b":"code","55205b63":"code","070349df":"code","89968fc9":"code","74b413e5":"code","bedd5907":"code","49dde205":"markdown"},"source":{"6dd36c3d":"import os,re,zipfile\nimport pandas as pd\nimport numpy as np\nfrom types import SimpleNamespace\nfrom matplotlib import pyplot as plt\nimport itertools\nplt.style.use('dark_background')\nplt.style.use('seaborn')\n# Torch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ntorch.backends.cudnn.deterministic = True  \n\n# metrics\nfrom sklearn import metrics\n\n# Data processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.notebook import tqdm","c40ac3cd":"dev = torch.device('cuda')\ndev","5cccbeb7":"df = pd.read_csv('\/kaggle\/input\/quora-question-pairs\/train.csv.zip')[['question1','question2','is_duplicate']].dropna()\ndups = df[df.is_duplicate == 1].copy()\nno_dups = df[df.is_duplicate == 0].copy()\nsplit_fact = 2\ndf = pd.concat([dups[:int(len(dups) \/ split_fact)], no_dups[:int(len(no_dups) \/ split_fact)]],ignore_index=True)\ndf","e0cdb87b":"import tensorflow as tf\nimport tensorflow_hub as hub\nclass UniversalSentenceEncoder:\n\n    def __init__(self, encoder='universal-sentence-encoder', version='4'):\n        self.version = version\n        self.encoder = encoder\n        self.embd = hub.load(f\"https:\/\/tfhub.dev\/google\/{encoder}\/{version}\",)\n\n    def embed(self, sentences):\n        return self.embd(sentences)\n\n    def squized(self, sentences):\n        return np.array(self.embd(tf.squeeze(tf.cast(sentences, tf.string))))","55205b63":"ue = UniversalSentenceEncoder()","070349df":"train,test = train_test_split(df,test_size=0.33,random_state=42,stratify=df['is_duplicate'])","89968fc9":"%%time\nx_q1_train = torch.from_numpy(ue.squized(train['question1'].values)).type(torch.FloatTensor).to(dev)\nx_q2_train = torch.from_numpy(ue.squized(train['question2'].values)).type(torch.FloatTensor).to(dev)\ny_train = torch.from_numpy(train['is_duplicate'].values).type(torch.LongTensor).to(dev)\n\nx_q1_test = torch.from_numpy(ue.squized(test['question1'].values)).type(torch.FloatTensor).to(dev)\nx_q2_test = torch.from_numpy(ue.squized(test['question2'].values)).type(torch.FloatTensor).to(dev)\ny_test = torch.from_numpy(test['is_duplicate'].values).type(torch.LongTensor).to(dev)","74b413e5":"b_size = 256\ntrain_dl = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_q1_train,x_q2_train,y_train), batch_size=b_size, shuffle=True)\ntest_dl = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_q1_test,x_q2_test,y_test), batch_size=b_size, shuffle=True)","bedd5907":"class Net(nn.Module):\n\n    def __init__(self):\n        super(Net,self).__init__()\n\n        self.q1_lin = nn.Linear(in_features=512,out_features=1024)\n        self.q2_lin = nn.Linear(in_features=512,out_features=1024)\n        self.lin1 = nn.Linear(in_features=2048,out_features=1024)\n        self.lin2 = nn.Linear(in_features=1024,out_features=512)\n        self.lin3 = nn.Linear(in_features=512,out_features=256)\n        self.lin4 = nn.Linear(in_features=256,out_features=128)\n        self.lin5 = nn.Linear(in_features=128,out_features=2)\n\n    # here we take the input data and pass it through the chain of layers\n    def forward(self,q1,q2):\n        q1 = self.q1_lin(q1)\n        q2 = self.q2_lin(q2)\n        x = torch.cat((q1,q2),dim=1)\n        # print(x.size())\n        x = self.lin1(x)\n        x = self.lin2(x)\n        x = self.lin3(x)\n        x = self.lin4(x)\n        x = self.lin5(x)\n        return x\n\n# instance our model\nmodel = Net().to(dev)\n# set the number of epochs\nepochs = 100\n# criterion aka loss function -> find more on pytorch doc\ncriterion = nn.CrossEntropyLoss()\n\n# optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n\n# create 3 lists to store the losses and accuracy at each epoch\ntrain_losses, test_losses, accuracy = [0]*epochs, [0]*epochs,[0]*epochs\n\n\n# in this current case we don't use batches for training and we pass the whole data at each epoch\nfor e in tqdm(range(epochs)):\n\n    for q1,q2,label in train_dl:\n\n        optimizer.zero_grad()\n        q1 = q1.float()\n        q2 = q2.float()\n        # Comput train loss\n        y_pred = model(q1,q2)\n        loss = criterion(y_pred, label)\n        \n        loss.backward()\n\n        optimizer.step()\n\n        # store train loss\n        train_losses[e] = loss.item()\n    \n    for q1,q2,label in test_dl:    \n        # Compute the test stats\n        with torch.no_grad():\n            # Turn on all the nodes\n            model.eval()\n            q1 = q1.float()\n            q2 = q2.float()\n            # Comput test loss\n            ps = model(q1,q2)\n            loss = criterion(ps, label)\n\n            # store test loss\n            test_losses[e] = loss.item()\n            \n            # # Compute accuracy\n            top_p, top_class = ps.topk(1, dim=1)\n        \n            equals = (top_class == label.view(*top_class.shape))\n            \n            # # store accuracy\n            accuracy[e] = torch.mean(equals.type(torch.FloatTensor))\n\n# Print the final information\nprint(f'Accuracy  : {100*accuracy[-1].item():0.2f}%')\nprint(f'Train loss: {train_losses[-1]}')\nprint(f'Test loss : {test_losses[-1]}')\n    \n# Plot the results\nfig,ax = plt.subplots(1,2,figsize=(20,5))\nax[0].set_ylabel('Accuracy')\nax[0].set_xlabel('Epochs')\nax[0].set_title('Model Accuracy')\nax[0].plot(accuracy)\n\nax[1].set_ylabel('Loss')\nax[1].set_xlabel('Epochs')\nax[1].set_title('Train\/Test Losses')\nax[1].plot(train_losses, label='train')\nax[1].plot(test_losses, label='test')\nax[1].legend()   \n\nplt.tight_layout()","49dde205":"# Looking for my twin question using \ud83d\udd25"}}