{"cell_type":{"5b2c0745":"code","14b28597":"code","15e9d206":"code","0f5a22a5":"code","5d36bbc4":"code","b355a7e7":"code","8f1b6600":"code","c95303e4":"code","7db57d4c":"code","7c4ba1ae":"code","289b6f18":"code","fa4dacb1":"code","6d712693":"code","e2625c47":"code","fd7850ea":"code","59730183":"code","630a1584":"code","e2fe2b0a":"markdown","d036c52a":"markdown","8f81d5d8":"markdown","51029e64":"markdown","512dfa23":"markdown","9d9a9d8d":"markdown","45169272":"markdown","00339526":"markdown","e95c5958":"markdown"},"source":{"5b2c0745":"# Code derived from *Deep Learning with Python*\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n%matplotlib inline","14b28597":"from keras.datasets import imdb\n\nword_index = imdb.get_word_index()\ninv_word_index = dict([\n    (v, k) for (k, v) in word_index.items()\n])\nword_index = None\ndef decode_review(review):\n    # First three indices of the word index are padding, start, & unknown\n    return ' '.join([inv_word_index.get(i-3, '?') for i in review])\n    \n#decode_review(train_data[0])","15e9d206":"# Get data from IMDB\n(train_data, train_label), (test_data, test_label) = imdb.load_data(num_words=10000)\n\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1\n    return results\n\n# Vectorize training data\nX_tr = vectorize_sequences(train_data)\nX_test = vectorize_sequences(test_data)\n\n# Convert knowns to floating point scores, which allows us to\n# round to the the nearest score on output\ny_tr = train_label.astype('float32')\ny_test = test_label.astype('float32')\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_tr, y_tr,\n                                            test_size=0.25,\n                                            random_state=0x1337)\n\n# cleanup\ntrain_data, train_label, test_data, test_label = None, None, None, None","0f5a22a5":"from keras.models import Sequential\nfrom keras.layers import Dense\n\nfrom keras import optimizers, losses, metrics\n\ninput_shape = X_train.shape[1:]\n\norig_model = Sequential([\n    Dense(16, activation='relu', input_shape=input_shape),\n    Dense(16, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\n# model.compile(\n#     optimizer='rmsprop',\n#     loss='binary_crossentropy',\n#     metrics='accuracy',\n# )\norig_model.compile(\n    optimizer=optimizers.RMSprop(lr=0.001),\n    loss=losses.binary_crossentropy,\n    metrics=('accuracy',),\n)","5d36bbc4":"history = orig_model.fit(X_train, y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(X_valid, y_valid))","b355a7e7":"history_data = pd.DataFrame(history.history)\nhistory_data[['loss', 'val_loss']].plot(\n    xlabel='Epoch',\n    ylabel='Loss',\n    #xticks=list(range(0, 24, 3)),\n    title='Training data for IMDB Reviews'\n)","8f1b6600":"model = Sequential([\n    Dense(16, activation='relu', input_shape=input_shape),\n    Dense(16, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.fit(X_tr, y_tr, epochs=4, batch_size=512)","c95303e4":"test_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f'Test loss: {test_loss:.4f}; Accuracy: {test_accuracy*100:.2f}%')","7db57d4c":"model_small = Sequential([\n    Dense(4, activation='relu', input_shape=input_shape),\n    Dense(4, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nmodel_small.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory_small = model_small.fit(X_train, y_train,\n                                epochs=20,\n                                batch_size=512,\n                                validation_data=(X_valid, y_valid))\nsmall_his = pd.DataFrame(history_small.history)\nmodel_small = None","7c4ba1ae":"small_his[['loss', 'val_loss']].plot()\nsmall_his = None","289b6f18":"model_small2 = Sequential([\n    Dense(4, activation='relu', input_shape=input_shape),\n    Dense(4, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nmodel_small2.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel_small2.fit(X_tr, y_tr,\n                 epochs=7,\n                 batch_size=512)\nresults_small = model_small2.evaluate(X_test, y_test)\nprint('Loss: {:.4f}; Accuracy: {:.2f}%'.format(\n    results_small[0], results_small[1]*100))\nmodel_small2 = None","fa4dacb1":"model_big = Sequential([\n    Dense(512, activation='relu', input_shape=input_shape),\n    Dense(512, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\nmodel_big.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhist_big = model_big.fit(X_train, y_train,\n                         epochs=20,\n                         batch_size=512,\n                         validation_data=(X_valid, y_valid))\npd.DataFrame(hist_big.history)[['loss', 'val_loss']].plot()\n\n#res_big = model_big.evaluate(X_test, y_test)\n#print('Loss: {:.4f}; Accuracy: {:.2f}%'.format(res_big[0], res_big[1]*100))\n\nmodel_big, hist_big = None, None","6d712693":"model_big2 = Sequential([\n    Dense(512, activation='relu', input_shape=input_shape),\n    Dense(512, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\nmodel_big2.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel_big2.fit(X_tr, y_tr,\n              epochs=2,\n              batch_size=512)\nbig_loss, big_acc = model_big2.evaluate(X_test, y_test)\nmodel_big2 = None","e2625c47":"print('Accuracy by layer node count\\n')\nprint('Small   (4): {:.2f}%\\nNormal (16): {:.2f}%\\nBig   (512): {:.2f}%'.format(\n    results_small[1]*100,\n    test_accuracy*100,\n    big_acc*100\n))\nsmall_hist, results_small, hist_big, big_history = None, None, None, None","fd7850ea":"from keras.layers import Dropout\nfrom keras.regularizers import l1, l2\n\n_d = 0.50\nmodel = Sequential([\n    # Use L2 regularization\n    #Dense(16, activation='relu', input_shape=input_shape,\n    #      kernel_regularizer=l2(0.001)),\n    #Dense(16, activation='relu', kernel_regularizer=l2(0.001)),\n    \n    # Use dropout regularization\n    Dense(16, activation='relu', input_shape=input_shape),\n    Dropout(_d),\n    Dense(16, activation='relu'),\n    Dropout(_d),\n    Dense(1, activation='sigmoid')\n])\nmodel.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(X_train, y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(X_valid, y_valid))","59730183":"pd.DataFrame(history.history)[['loss', 'val_loss']].plot()","630a1584":"_d = 0.50\nmodel = Sequential([\n    # Use L2 regularization\n    #Dense(16, activation='relu', input_shape=input_shape,\n    #      kernel_regularizer=l2(0.001)),\n    #Dense(16, activation='relu', kernel_regularizer=l2(0.001)),\n    \n    # Use dropout regularization\n    Dense(16, activation='relu', input_shape=input_shape),\n    Dropout(_d),\n    Dense(16, activation='relu'),\n    Dropout(_d),\n    Dense(1, activation='sigmoid')\n])\nmodel.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.fit(X_tr, y_tr, batch_size=512, epochs=5, verbose=False)\nprint('on a normal model with 16 nodes, a dropout of {:.2f}% yields {:.2f}%'.format(\n    _d*100,\n    model.evaluate(X_test, y_test, verbose=False)[1]*100\n))","e2fe2b0a":"# A Basic Model\nNow let's define our model using Tensorflow\/Keras:","d036c52a":"Our accuracy seemed to peak around epoch #4. So let's retain the model using only\n4 epochs:","8f81d5d8":"And now we train our model:","51029e64":"## Bigger\nLet's try making the layers bigger with 512 nodes. How does that affect performance?\n\nPrediction: Performance will *slightly* improve but not much (if even significant).","512dfa23":"Ok, we can decode an array, but how can we do ML with this?\nLet's turn the reviews into a one-hot array of whether a word is contained within\nthe review's text.","9d9a9d8d":"The smaller networks performed just as well as the largest network. Clearly size isn't the issue here.\n\n# Regularization\nNow let's try regularizing the weights in our network. This helps avoid overfitting","45169272":"## Size Comparison\nSo let's see how the layer node count affected accuracy:","00339526":"Now let's train the model on the entire training set:","e95c5958":"## Smaller\nLet's try making a smaller model with only 4 nodes per layer. How will this affect the performance of the network?\n\nPrediction: It will negatively affect the network performance"}}