{"cell_type":{"a7fdac81":"code","cab3f5e4":"code","47700ff1":"code","8f20b39e":"code","fec5a482":"code","19dbbd02":"code","518f4bee":"code","24afeb31":"code","870c5ce9":"code","7fb3554e":"code","547d679c":"code","79a0799b":"code","9e68ae05":"code","a6a80364":"code","82c65741":"code","0cf0b555":"code","890b4cb6":"code","3050f11c":"code","acdaf5a4":"code","12ce1cb4":"code","4b1b816e":"code","c888f11c":"code","2532692e":"code","c0ffa6b7":"code","ac2d2e26":"code","51faa057":"code","4d3e8098":"code","e986ab2c":"markdown","81f9c71a":"markdown","f46f487f":"markdown","65f1b88d":"markdown","695a64a8":"markdown","125735dd":"markdown","1a955b48":"markdown","953b7ece":"markdown","06472168":"markdown","31bdbffa":"markdown","488ba505":"markdown","27ceabb4":"markdown","8fe57fa6":"markdown","0747cd7d":"markdown","74651620":"markdown","7aa63cbe":"markdown","8e53fe06":"markdown","8df4c18e":"markdown","14e273c2":"markdown","54dc8a4d":"markdown","4882d510":"markdown","f4b3d10c":"markdown","a06d0b38":"markdown","9835f89f":"markdown","0bc45b45":"markdown","9515876d":"markdown","6a0ca576":"markdown","fae96ff1":"markdown","fbca7dc2":"markdown","ad8afe0a":"markdown"},"source":{"a7fdac81":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","cab3f5e4":"credit_data = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\",parse_dates=True)","47700ff1":"credit_data.head()","8f20b39e":"import matplotlib.pyplot as plt\nimport seaborn as sb","fec5a482":"class_count=pd.value_counts(credit_data['Class'],sort=True).sort_index()\nprint(class_count)\nplt.xlabel('class label')\nplt.ylabel('frequency')\nclass_count.plot(kind='bar')\nplt.legend()\n","19dbbd02":"class_count.plot(kind='pie')","518f4bee":"from sklearn.preprocessing import scale\nprint(credit_data['Amount'])\ncredit_data['norm_Amount']= scale(credit_data['Amount'])\nprint(credit_data['norm_Amount'])\n","24afeb31":"\ncredit_data.drop(['Amount','Time'],axis=1)","870c5ce9":"y= credit_data['Class']\nx=credit_data.loc[:,credit_data.columns != 'Class']\n#print(x)","7fb3554e":"from sklearn.model_selection import train_test_split\n\ntrain_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.3,random_state=0)","547d679c":"X= pd.concat([train_x,train_y],axis=1)\n","79a0799b":"fraud_trans = X.loc[X['Class']==1,:]\nprint(len(fraud_trans))\ntrue_trans = X.loc[X['Class']!=1,:]\nprint(len(true_trans))","9e68ae05":"from sklearn.utils import resample\n#perform downsampling\ndownsample=resample(true_trans,replace=False,n_samples=len(fraud_trans),random_state=0)\n#concat downsampled majority class and minority class\nX_downsample = pd.concat([downsample,fraud_trans])\n#print(X_downsample)\nprint(pd.value_counts(X_downsample['Class']==1))","a6a80364":"X_train_downsample,X_valid_downsample,y_train_downsample,y_valid_downsample = train_test_split(X_downsample.loc[:,X_downsample.columns != 'Class'] ,X_downsample['Class'],test_size=0.3,random_state=1)\n","82c65741":"print(X_downsample.loc[:,X_downsample.columns != 'Class'].head())\nprint(X_downsample['Class'].head())","0cf0b555":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix","890b4cb6":"def error_metric(my_pred,model,y_valid):\n    recall=recall_score(y_valid,my_pred)\n    f1=f1_score(y_valid,my_pred)\n    print(\"recall value on validation dataset\",recall)\n    print(\"f1 value on validation dataset\",f1)\n    \n    #check the error metrics on our test data\n    \n    pred= model.predict(test_x)\n    recall=recall_score(test_y,pred)\n    f1=f1_score(test_y,pred)\n    print(\"recall value on test dataset\",recall)\n    print(\"f1 value on test dataset\",f1)\n    \n    print(\"confusion matrix for our test dataset\"+\"\\n\", confusion_matrix(test_y,pred))","3050f11c":"def rfc(X_train,X_valid,y_train,y_valid):\n    model = RandomForestClassifier(n_estimators=10,criterion=\"entropy\",random_state=1)\n    model.fit(X_train,y_train)\n    my_pred=model.predict(X_valid)\n    error_metric(my_pred,model,y_valid)","acdaf5a4":"def xgb(X_train,X_valid,y_train,y_valid):\n    model = XGBClassifier(n_estimator=500)\n    model.fit(X_train,y_train)\n    my_pred=model.predict(X_valid)\n    error_metric(my_pred,model,y_valid)","12ce1cb4":"rfc(X_train_downsample,X_valid_downsample,y_train_downsample,y_valid_downsample)","4b1b816e":"xgb(X_train_downsample,X_valid_downsample,y_train_downsample,y_valid_downsample)","c888f11c":"X_train_upsample,X_valid_upsample,y_train_upsample,y_valid_upsample = train_test_split(train_x,train_y,test_size=0.3,random_state=1)","2532692e":"X= pd.concat([X_train_upsample,y_train_upsample],axis=1)","c0ffa6b7":"fraud_trans = X.loc[X['Class']==1,:]\nprint(len(fraud_trans))\ntrue_trans = X.loc[X['Class']!=1,:]\nprint(len(true_trans))","ac2d2e26":"from sklearn.utils import resample\n#perform downsampling\nupsample=resample(fraud_trans,replace=True,n_samples=len(true_trans),random_state=0)\n#concat downsampled majority class and minority class\nX_upsample = pd.concat([upsample,true_trans])\n#print(X_downsample)\nprint(pd.value_counts(X_upsample['Class']==1))\n\n\ny_train_upsample= X_upsample['Class']\nX_train_upsample = X_upsample.loc[:,X_upsample.columns != 'Class']","51faa057":"rfc(X_train_upsample,X_valid_upsample,y_train_upsample,y_valid_upsample)","4d3e8098":"xgb(X_train_upsample,X_valid_upsample,y_train_upsample,y_valid_upsample)","e986ab2c":"Divide the dataset into training and test in a ratio of 70:30","81f9c71a":"Divide the undersampled dataset into training and validation test to tune our parameters according to the output on validation data.","f46f487f":"If you liked my work, do upvote. Thanks","65f1b88d":"Lets view the first 5 rows of our dataset","695a64a8":"Here we will perform under sampling on our majority class. This means that we will randomly choose n samples from our true transactions such that those n samples are equal or similar to the number of transactions in our minority class (fraudulent transactions). This is done to remove skewness from our dataset.","125735dd":"As we can see above, the class label 1 is not visible properly. Let do a pie chart analysis to see if that make a difference ","1a955b48":"Import all the library necessary for model training","953b7ece":"As we can see, the fraud incidents in credit card transaction are very less as compared to the true transactions.\n\nLets scale the 'Amount' variable to have zero mean and unit variance. This will make learning by our ML model better and faster ","06472168":"Merge both of them into a single dataset","31bdbffa":"As we can see in the above output, fraud and true transaction are equal and data in minority class has been increased to match majority class.","488ba505":"Lets use XGBClassifier","27ceabb4":"We will read the credit card using the pandas read_csv command","8fe57fa6":"This is my first Kernal. Do tell me if there are any mistakes or area for improvement.\n\nIn this notebook, we will be dealing with the problem of detecting credit card fraudulence. The problem is kind of different from some comman classification problems in sense that the output class is highly skewed. \n\nThis is problematic situation if we apply machine learning on the raw dataset without handling the skewness in the dataset as most of ML algorithm are designed to work on dataset with almost similar ration of class labels.\n\nAlso most ML alorithm uses accuracy metric as a go to metric for most regression and classification problems. In this case, accuracy metric can be highly misleading as it isnt the correct representation of the answer we are looking for\n\nThis problem can be dealt in the following manners-\n\n1) preprocessing the dataset before training it in ML model\n\n2) choosing an appropriate ML model for the task.\n\n3) Choosing an approriate error metric ","0747cd7d":"# **Undersampling Majority Class**","74651620":"Oversampling\/upsamplig is adding more copy of the minority class to make the minority and majority ratio similar. It is a good choice when we dont have a lot of data.\n\n**Note** Always split your data into test and train sets before trying oversampling techniques. Oversampling before splitting the data can allow the exact same data points to be present in both the test and train datasets. Hence causing data leakage. \n","7aa63cbe":"Divide the training dataset into 2 parts. One part containg the fraudulent transactions which are our minority class. Other part contain the true transactions which are our majority class.","8e53fe06":"We wont be needing 'Amount' and 'Time' variable in our analysis hence we will drop them from our dataset","8df4c18e":"Call the functions in which we defined our ML models to check the accuracy on the test set","14e273c2":"Let's view our data","54dc8a4d":"The datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. \n\nFeatures V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. \n\nThe feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","4882d510":"Split the data into features(x) and class variable(y)","f4b3d10c":"Perform Upsampling here","a06d0b38":"Seperate the fraud and true transactions","9835f89f":"As we can see here, XGB classifier performs better than our random forest classifier.","0bc45b45":"# Upsampling Minority Class","9515876d":"Join the training data so that we can perform sampling on our training dataset","6a0ca576":"As we can see from above, all the features except Time and Amount as scaled and it requires us to scale Amount feature also.\nBut before moving on to scaling Amount feature,lets see how are the labels (Class) of our target variable are distributed.","fae96ff1":"Lets use Random Forest Classifier","fbca7dc2":"Hence we will perform train - validation split before upsampling our data","ad8afe0a":"Before learning about the metrics, lets clear ourselves with some basic terminology\ntrue positives (TP): These are cases in which we predicted yes (the transaction was fraud), and they it was actually fraud\ntrue negatives (TN): We predicted no, and the transaction was not a fraud\nfalse positives (FP): We predicted yes, but it wasnt a fraud transaction. (Also known as a \"Type I error.\")\nfalse negatives (FN): We predicted no, but it was a fraud transaction. (Also known as a \"Type II error.\")\n\nThe metrics which can give us better insights are-\n\n1) Confusion Matrix - Matrix which divide the output prediction into the above 4 category\n\n2) precision- When it predicts yes, how often is it correct?\n\n3) Recall - When it's actually yes, how often does it predict yes?\n\n4) F1 score- This is a weighted average of the true positive rate (recall) and precision.\n"}}