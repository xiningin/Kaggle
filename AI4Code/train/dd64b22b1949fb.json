{"cell_type":{"ab8ef0a9":"code","25cc6bb5":"code","ad8e3b20":"code","a3545296":"code","8b3ba12a":"code","362a5e89":"code","ce141d02":"code","f3f5aec3":"code","31f4e4df":"code","29e65771":"code","6d8a42f6":"code","71e78f10":"code","db48f914":"code","cc2f3835":"code","93e6a20b":"code","f88bc755":"code","25d4e2f1":"code","cb284fa6":"code","232b939b":"code","d6ff4379":"code","b4382470":"code","a3b7cef2":"code","7ec9a930":"code","f9e7f09e":"code","7de3d4bc":"code","4a8ea34e":"code","d02f258a":"code","5b84c3fe":"code","2d5b2761":"code","2bb16287":"code","cf9c6b41":"code","3814eccc":"code","fbd18d39":"code","ffef9a76":"code","83be98d3":"code","bf2df9cd":"code","57f89c20":"code","371867d5":"code","4aa78858":"code","13181b87":"code","dc846798":"code","85c08a11":"code","129684f8":"code","8fdaf533":"code","55641306":"code","d726d110":"code","314fbc3e":"code","1b61b2fa":"code","eb7988b1":"code","caa96316":"markdown","2a659cd6":"markdown","70a4825d":"markdown","bce79670":"markdown","0943f813":"markdown","3f6a2db0":"markdown","50c4fc88":"markdown","c97d6e94":"markdown"},"source":{"ab8ef0a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","25cc6bb5":"df = pd.read_csv('\/kaggle\/input\/openintro-possum\/possum.csv')","ad8e3b20":"import seaborn as sns\nimport matplotlib.pyplot as plt","a3545296":"df.info()","8b3ba12a":"df['Pop'].value_counts()","362a5e89":"df['sex'].value_counts()","ce141d02":"#converting object data types into integters\ndf['Pop']=list(map(int,df['Pop']=='other'))\ndf['sex'] = list(map(int, df['sex']=='f'))","f3f5aec3":"df.head()","31f4e4df":"# dropping case column as it is a redundant index\ndf.drop('case', axis = 1, inplace = True)","29e65771":"df.isnull().sum()","6d8a42f6":"df.columns","71e78f10":"# we'll impute the three rows with missing data based on the mean in these rows\nfrom sklearn.impute import SimpleImputer\nimp_mean = SimpleImputer(missing_values = np.nan, strategy='mean')\ndf = imp_mean.fit_transform(df)","db48f914":"# reconverting to pandas dataframe and checking our work.\ndf = pd.DataFrame(df, columns = ['site', 'Pop', 'sex', 'age', 'hdlngth', 'skullw', 'totlngth',\n       'taill', 'footlgth', 'earconch', 'eye', 'chest', 'belly'])\ndf.isnull().sum()","cc2f3835":"# All looks good!\ndf.head()","93e6a20b":"# what do we have here? \n\n# some interesting pocket correlations: \n\n# foot length positively correlated with ear conch size\n# hind length, total length and skull width positvely correlated.\nplt.figure(figsize = (12,8))\nsns.heatmap(df.corr(), annot = True, cmap = 'Spectral')","f88bc755":"#earconch vs. footlength by sex. Orange is female\nsns.scatterplot(x = 'footlgth', y = 'earconch', data =df, hue = 'sex')","25d4e2f1":"#skull width vs. hind length by sex. Orange is female.\nsns.scatterplot(x = 'skullw', y = 'hdlngth', data =df, hue = 'sex')","cb284fa6":"#skull width vs. total length by sex. Orange is female.\nsns.scatterplot(x = 'skullw', y = 'totlngth', data =df, hue = 'sex')","232b939b":"#hind length vs. total length by sex. Orange is female.\nsns.scatterplot(x = 'hdlngth', y = 'totlngth', data =df, hue = 'sex')","d6ff4379":"# distribution of age in dataset. More males present, but similar distribution of both sexes.\nsns.histplot(x = 'age', data =df, hue = 'sex')","b4382470":"# distribution of hind length in dataset. Evenly distibuted between males and females.\nsns.histplot(x = 'hdlngth', data =df, hue = 'sex')","a3b7cef2":"# distribution of skull width in dataset. Evenly distibuted between males and females.\nsns.histplot(x = 'skullw', data =df, hue = 'sex')","7ec9a930":"# distribution of total length in dataset. Female population appears to skew toward longer size.\nsns.histplot(x = 'totlngth', data =df, hue = 'sex')","f9e7f09e":"# distribution of tail length in dataset. Intesting bimodel distribution here. \nsns.histplot(x = 'taill', data =df, hue = 'sex')","7de3d4bc":"# distribution of foot length in dataset. Sampled males skew towards smaller foot. Sampled females skew \n#towards slightly larger foot.\nsns.histplot(x = 'footlgth', data =df, hue = 'sex')","4a8ea34e":"# distribution of ear conch in dataset. Another bimodel distribution in both sexes.\nsns.histplot(x = 'earconch', data =df, hue = 'sex')","d02f258a":"# distribution of eye in dataset. Normal distirbution.\nsns.histplot(x = 'eye', data =df, hue = 'sex')","5b84c3fe":"# distribution of chest in dataset. Somewhat normal distirbution.\nsns.histplot(x = 'chest', data =df, hue = 'sex')","2d5b2761":"# distribution of belly in dataset. Somewhat normal distirbution.\nsns.histplot(x = 'belly', data =df, hue = 'sex')","2bb16287":"#Given how similar the sexes are in all meaasured features, can we accurately classify male and female possums? \n\n# This will be our first classification task with this dataset","cf9c6b41":"X = df.drop('sex', axis = 1)\ny = df['sex']","3814eccc":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)","fbd18d39":"# We'll train several classifiers: Logistic regression, Decision Tree Classifier, Random Forest Classifer and \n# XGBoost Classifer\n\n#Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver = 'newton-cg')\nlr.fit(X_train,y_train)\nlr_pred = lr.predict(X_test)","ffef9a76":"# Logistic regression struggles here\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nprint(classification_report(y_test,lr_pred))\nprint('\\n')\nprint(confusion_matrix(y_test,lr_pred))\n\nlr_acc = accuracy_score(y_test,lr_pred)","83be98d3":"#Decision Tree Classifer\n\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\ndtc_pred = dtc.predict(X_test)","bf2df9cd":"# Decision tree classification struggles even more.\n\nprint(classification_report(y_test,dtc_pred))\nprint('\\n')\nprint(confusion_matrix(y_test,dtc_pred))\n\ndtc_acc = accuracy_score(y_test,dtc_pred)","57f89c20":"#Random Forest Classifer\n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(X_train,y_train)\nrfc_pred = rfc.predict(X_test)","371867d5":"# Random forest roughly matches logistic regression in performance\n\nprint(classification_report(y_test,rfc_pred))\nprint('\\n')\nprint(confusion_matrix(y_test,rfc_pred))\n\nrfc_acc = accuracy_score(y_test,rfc_pred)","4aa78858":"from xgboost import XGBClassifier\nxgc = XGBClassifier(use_label_encoder = False)\nxgc.fit(X_train,y_train)\nxgc_pred = xgc.predict(X_test)","13181b87":"# Random forest roughly matches logistic regression in performance\n\nprint(classification_report(y_test,xgc_pred))\nprint('\\n')\nprint(confusion_matrix(y_test,xgc_pred))\n\nxgc_acc = accuracy_score(y_test,xgc_pred)","dc846798":"from sklearn.neighbors import KNeighborsClassifier\nknc = KNeighborsClassifier(2)\nknc.fit(X_train,y_train)\nknc_pred = knc.predict(X_test)","85c08a11":"print(classification_report(y_test,knc_pred))\nprint('\\n')\nprint(confusion_matrix(y_test,knc_pred))\n\nknc_acc = accuracy_score(y_test,knc_pred)","129684f8":"from sklearn.svm import SVC\nsvc = SVC(kernel='linear')\nsvc.fit(X_train,y_train)\nsvc_pred = svc.predict(X_test)","8fdaf533":"print(classification_report(y_test,svc_pred))\nprint('\\n')\nprint(confusion_matrix(y_test,svc_pred))\n\nsvc_acc = accuracy_score(y_test,svc_pred)","55641306":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train,y_train)\ngnb_pred = gnb.predict(X_test)","d726d110":"print(classification_report(y_test,gnb_pred))\nprint('\\n')\nprint(confusion_matrix(y_test,gnb_pred))\n\ngnb_acc = accuracy_score(y_test,gnb_pred)","314fbc3e":"# The poor success here is due to the similarity between male and female possums.\nresults = pd.DataFrame({'Model': ['Logistic Regression', 'Decision Tree Classifier',\\\n                                  'Random Forest Classifier', 'XGBoost Classifier',\\\n                                  'KNeighbors Classifier', 'SVC', 'GaussianNB'], 'Accuracy':\\\n                       [lr_acc, dtc_acc, rfc_acc,xgc_acc, knc_acc, svc_acc, gnb_acc]})\nresults","1b61b2fa":"knc_i_accs = []\n\nfor i in range(1,41):\n    knc_i = KNeighborsClassifier(i)\n    knc_i.fit(X_train,y_train)\n    knc_i_pred = knc_i.predict(X_test)\n    knc_i_acc = accuracy_score(y_test, knc_i_pred)\n    knc_i_accs = np.append(knc_i_accs, knc_i_acc)\n","eb7988b1":"# k = 2 yields the highest accuracy. such a low k is suspect and not likely to tranfser well to other\n# data points\n\nplt.plot(np.arange(1,41), knc_i_accs, marker = 'o', linestyle = '--')\nplt.xlabel('k')\nplt.ylabel('accuracy')","caa96316":"# Optimizing KNeighbors","2a659cd6":"# Data and target feature","70a4825d":"# Sex classification results","bce79670":"# train test split","0943f813":"# Data Cleaning","3f6a2db0":"# KNeighbors wins!","50c4fc88":"# Classification of Sex","c97d6e94":"# Exploratory Data Analysis"}}