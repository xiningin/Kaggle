{"cell_type":{"947071e9":"code","1ba42ce8":"code","1cedaf5e":"code","a8b30fc3":"code","0fb3cffe":"code","3373716d":"code","e605d47e":"code","df363ba6":"code","f5bba62d":"code","3f95a9c3":"code","38db5672":"code","6ff8b8fe":"code","9eb39db6":"code","22272648":"code","5008ab5e":"code","9b658be0":"code","c90a6082":"code","816e73f2":"code","ebd5d48a":"code","80b2699b":"code","55db301a":"code","ee8df4c5":"code","04cc8c5d":"markdown","44b20c24":"markdown","8ad0c297":"markdown","a75021d2":"markdown","a792b281":"markdown","a7f7e2fc":"markdown","38b1d2a0":"markdown","ecbf4fde":"markdown"},"source":{"947071e9":"# Import the necessary packages\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sklearn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\nfrom sklearn.metrics import r2_score, roc_auc_score, roc_curve, classification_report\nfrom lightgbm.sklearn import LGBMClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","1ba42ce8":"# Import and read dataset\ninput_ = \"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\"\ndf = pd.read_csv(input_)\n\ndf.head(10)","1cedaf5e":"plt.figure(figsize=(15,15))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","a8b30fc3":"df.describe()","0fb3cffe":"x = df.drop(columns='DEATH_EVENT')\ny = df['DEATH_EVENT']\n\nmodel = LGBMClassifier()\nmodel.fit(x,y)\nprint(model.feature_importances_)\nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","3373716d":"for i in range(0,len(df.columns)):\n    print(\"{} = {}\".format(i,df.columns[i]))","e605d47e":"# Delete outlier\ndf = df[df['ejection_fraction']<70]","df363ba6":"inp_data = df.drop(df[['DEATH_EVENT']], axis=1)\n#inp_data = df.iloc[:,[11,6,2,4,7,0,8]]\nout_data = df[['DEATH_EVENT']]\n\nX_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2, random_state=0, shuffle=True)\n\n## Applying Transformer\n#sc= StandardScaler()\n#sc = MinMaxScaler()\n#X_train = sc.fit_transform(X_train)\n#X_test = sc.fit_transform(X_test)","f5bba62d":"## X_train, X_test, y_train, y_test Shape\n\nprint(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","3f95a9c3":"# I coded this method for convenience and to avoid writing the same code over and over again\n\ndef result(clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    \n    print('Accuracy Score    : {:.4f}'.format(accuracy_score(y_test, y_pred)))\n    print('LightGBM f1-score      : {:.4f}'.format(f1_score( y_test , y_pred)))\n    print('LightGBM precision     : {:.4f}'.format(precision_score(y_test, y_pred)))\n    print('LightGBM recall        : {:.4f}'.format(recall_score(y_test, y_pred)))\n    print(\"LightGBM roc auc score : {:.4f}\".format(roc_auc_score(y_test,y_pred)))\n    print(\"\\n\",classification_report(y_pred, y_test))\n    \n    plt.figure(figsize=(6,6))\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    sns.heatmap((cf_matrix \/ np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")\n    plt.title(\"LightGBM Confusion Matrix (Rate)\")\n    plt.show()\n    \n    cm = confusion_matrix(y_test,y_pred)\n    plt.figure(figsize=(6,6))\n    sns.heatmap(cm, annot=True, cmap=\"Blues\",\n                xticklabels=[\"FALSE\",\"TRUE\"],\n                yticklabels=[\"FALSE\",\"TRUE\"],\n                cbar=False)\n    plt.title(\"LightGBM Confusion Matrix (Number)\")\n    plt.show()\n    \n    \ndef report(**params):\n    scores = [] \n    for i in range(0,250): # 250 samples\n        X_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2, shuffle=True)\n        sc = StandardScaler()\n        clf = LGBMClassifier(**params)\n        X_train = sc.fit_transform(X_train)\n        X_test = sc.fit_transform(X_test)\n        clf.fit(X_train, y_train)\n        scores.append(accuracy_score(clf.predict(X_test), y_test)) \n        \n    Importance = pd.DataFrame({'Importance':clf.feature_importances_*100},index=df.columns[:12])\n    Importance.sort_values(by='Importance',axis=0,ascending=True).plot(kind='barh',color='lightblue')\n    plt.xlabel('Importance for variable');\n    plt.hist(scores)\n    plt.show()\n    print(\"Best Score: {}\\nMean Score: {}\".format(np.max(scores), np.mean(scores)))","38db5672":"clf = LGBMClassifier()\nresult(clf)\nreport()","6ff8b8fe":"param_grid = {\n    'min_child_weight': np.arange(1,20,1),\n    'colsample_bytree': np.linspace(0.5,2,11)\n}\n\nclf = LGBMClassifier()\ngrid = GridSearchCV(clf, param_grid, n_jobs=-1, verbose=2, cv=10)\ngrid.fit(X_train, y_train)\ngrid.best_params_","9eb39db6":"clf = LGBMClassifier(\n    min_child_weight= 0.6,\n    colsample_bytree= 0.65,\n    n_jobs=-1\n)\n\nresult(clf)","22272648":"report(\n    max_depth= 1,\n    min_child_weight= 1,\n)","5008ab5e":"param_grid = {\n    \"n_estimators\": [10,100,1000,10000 ]\n}\n\nclf = LGBMClassifier()\ngrid = GridSearchCV(clf, param_grid, n_jobs=-1, verbose=2, cv=10)\ngrid.fit(X_train, y_train)\ngrid.best_params_","9b658be0":"clf = LGBMClassifier(\n    max_depth= 1,\n    min_child_weight= 1,\n    gamma = 0.0,\n    colsample_bytree= 0.5,\n    n_estimators=10\n)\n\nresult(clf)","c90a6082":"report(\n    max_depth= 1,\n    min_child_weight= 1,\n    gamma = 0.0,\n    colsample_bytree= 0.5,\n    n_estimators=10\n)","816e73f2":"param_grid = {\n 'reg_alpha': [0.001, 0.005, 0.01, 0.05]\n}\n\nclf = LGBMClassifier()\ngrid = GridSearchCV(clf, param_grid, n_jobs=-1, verbose=2, cv=10)\ngrid.fit(X_train, y_train)\ngrid.best_params_","ebd5d48a":"clf = LGBMClassifier(\n    max_depth= 1,\n    min_child_weight= 1,\n    gamma = 0.0,\n    colsample_bytree= 0.5,\n    n_estimators=10,\n    reg_alpha=0.001\n)\n\nresult(clf)","80b2699b":"report(\n    max_depth= 1,\n    min_child_weight= 1,\n    colsample_bytree= 0.5,\n    n_estimators=10,\n    reg_alpha=0.001\n)","55db301a":"clf = LGBMClassifier(\n    max_depth= 1,\n    n_estimators=100,\n    colsample_bytree=0.9,\n    gamma=0.5,\n    learning_rate=0.01,\n    \n)\n\nresult(clf)","ee8df4c5":"report(\n    max_depth= 1,\n    n_estimators=100,\n    colsample_bytree=0.9,\n    gamma=0.5,\n    learning_rate=0.01,\n)","04cc8c5d":"---","44b20c24":"---","8ad0c297":"---","a75021d2":"## What is LightGBM?\nWith the rapid increase in data size and variety in recent years, the importance given to algorithm optimizations is increasing. For this reason, as an alternative to the Gradient Boosting algorithm, algorithms that can accept versions of Gradient Boosting such as XGBoost, LightGBM, Catboost have been developed. It is aimed to achieve faster training and higher accuracy with these algorithms.\n\nLightGBM is a boosting algorithm developed in 2017 as part of the Microsoft DMTK (Distributed Machine Learning Toolkit) project. Compared to other boosting algorithms, it has advantages such as high processing speed, ability to process large data, less resource (RAM) usage, high prediction rate, parallel learning and support for GPU learning. According to the article \"LightGBM: A Highly Ef \ufb01 cient Gradient Boosting Decision Tree\", in which the model is introduced, it has been concluded that LightGBM is 20 times faster than other models.\n\n\n\n## HOW DOES IT WORK?\nLightGBM is a histogram-based algorithm. It reduces calculation cost by making variables with continuous value discrete. The training time of decision trees is directly proportional to the calculation and therefore the number of divisions. Thanks to this method, both training time is shortened and resource use is reduced.\n\nTwo strategies, level-wise or depth-wise, or leaf-wise, can be used in learning in decision trees. With the level-oriented strategy, the balance of the tree is maintained while the tree grows. In the leaf-oriented strategy, the division process from the leaves, which reduces the loss, continues. LightGBM differs from other boosting algorithms thanks to this feature. The model has less error rate and learns faster with leaf-oriented strategy. However, the leaf-focused growth strategy causes the model to be prone to over-learning in cases where the number of data is low. Therefore, the algorithm is more suitable for use in big data. In addition, parameters such as tree depth and the number of leaves can be optimized to prevent excessive learning.\n\n![](https:\/\/www.veribilimiokulu.com\/wp-content\/uploads\/2020\/04\/Capture1.png)\n\nLightGBM also uses two techniques different from other algorithms. These are Gradient Based One Way Sampling and Special Variable Package that deals with the number of data samples and variables.\n\nGradient-based One-Side Sampling (GOSS): GOSS aims to reduce the number of data while maintaining the accuracy of decision trees. Traditional Gradient Boosting scans through all data samples to calculate the information gain for each variable, but GOSS only uses the key data. Thus, the number of data is reduced without much affecting the distribution of the data.\n\nExclusive Feature Bundling (EFB): EFB aims to reduce the number of variables without damaging the accuracy rate and increase the efficiency of model training accordingly. EFB has two process steps. These are creating packages and combining variables in the same package. With EFB, sparse features are combined to create more intense features. Accordingly, it leads to a decrease in complexity and faster training with lower memory consumption.\n\nIn summary, EFB combines variables to reduce dimensionality, while GOSS reduces data size to compute knowledge acquisition by neglecting less important data. With these two functions, LightGBM increases the efficiency of the training process.\n\n\n\n## PARAMETER OPTIMIZATION\n\n![](https:\/\/www.veribilimiokulu.com\/wp-content\/uploads\/2020\/04\/walking-on-magical-tree-59-1366x768-1.jpg)\n\nLearning_rate, max_dept, num_leaves, min_data_in_leaf parameters can be optimized to prevent excessive learning in LightGBM, feature_fraction, bagging_fraction and num_iteration parameters can be optimized to speed up learning time.\n\n**Num_leaves** is the number of leaves to be found in the tree. It is the most important parameter used in controlling the complexity of the tree. It must be less than 2 ^ (max_dept) to avoid over-learning. For example, when max_depth = 7, setting the num_leaves value to 127 can cause over learning. Setting it to 70 or 80 may achieve better accuracy.\n\n**Max_dept** is used to limit the depth of the tree to be built. It should be optimized to avoid over-learning. Too much branching will lead to excessive learning, less branching will lead to incomplete learning.\n\n**Min_data_in_leaf** is one of the important parameters to be used to prevent over learning. Its optimum value depends on the data size and num_leaves. Setting it to a large value can inhibit the growth of the tree and cause incomplete learning.\n\n**Learning_rate** is a value between 0-1 to scale installed trees. Smallness of this value will help better prediction power. However, it will increase the duration of education and increase the possibility of over learning.\n\n**Feature_fraction,** the variable to be used in each iteration; bagging_fraction are parameters by which the number of data to be used in each iteration can be set. Num_iteration is the number of iterations to be made in the learning process. The feature_fraction, bagging_fraction and num_iteration numbers are directly related to the learning period. The less these numbers are, the less the education period will be. However, it is very important to pay attention to incomplete learning. The optimum number can be found by making many trials.","a792b281":"![LightGBM](https:\/\/i.ibb.co\/PFRbFKw\/LightGBM.png)\n\n- **ML Part 1** - Logistic Regression\n- **ML Part 2** - K-Nearest Neighbors (KNN)\n- **ML Part 3** - Support Vector Machine (SVM)\n- **ML Part 4** - Artificial Neural Network (NN)\n- **ML Part 5** - Classification and Regression Tree (CART)\n- **ML Part 6** - Random Forests\n- **ML Part 7** - Gradient Boosting Machines (GBM)\n- **ML Part 8** - XGBoost\n- **ML Part 9 - LightGBM**\n- **ML Part 10** - CatBoost","a7f7e2fc":"---\n","38b1d2a0":"## Advanced Metod","ecbf4fde":"---\n\n## Simple Metod\nI applied XGBoost directly without changing anything and the result is as follows:"}}