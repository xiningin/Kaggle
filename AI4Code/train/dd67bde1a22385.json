{"cell_type":{"004eaee0":"code","6bfad5af":"code","8477a636":"code","b29876df":"code","a22e08d3":"code","c66e1759":"code","76f68d5c":"code","309fcfef":"code","bebf113b":"code","e22b383b":"code","ac9ab95d":"code","0fdb89b0":"code","e4e3ea1a":"code","13f907d8":"code","4799f2c1":"code","5f8d4003":"code","213814ad":"markdown","7cb1c91e":"markdown","c3dd2ac4":"markdown","1ff0fd82":"markdown","b7d30b01":"markdown","8422f2ec":"markdown"},"source":{"004eaee0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6bfad5af":"# Import necessary libraries\nimport sklearn\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport hypertools as hyp\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.layers import Dense, BatchNormalization, LSTM\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras import callbacks\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport matplotlib.pyplot as plt","8477a636":"# Load and take a peek at dataset\ndata = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\ndata.head()","b29876df":"# Check for null values\ndata.isnull().sum()","a22e08d3":"# Generate descriptive statistics\ndata.describe()","c66e1759":"# Check degree of class imbalance \ndata.groupby('DEATH_EVENT').size()","76f68d5c":"# 3D scatter for PCA using Hypertools\nlabels = data['DEATH_EVENT']\nhyp.plot(data, '.', hue=labels, reduce='PCA',legend=labels.unique().tolist())","309fcfef":"# Normalize based on (https:\/\/sebastianraschka.com\/Articles\/2015_pca_in_3_steps.html#standardizing)\nhyp.plot(data, '.', hue=labels, reduce='PCA',normalize='within', legend=labels.unique().tolist())","bebf113b":"# Correlation matrix\ncorr=data.corr()\nsns.heatmap(corr)","e22b383b":"# Seperate data into features, target\ny = (data[\"DEATH_EVENT\"]).to_numpy()\nX_raw= (data.drop([\"DEATH_EVENT\"],axis=1))\n# Getting column names here to be used later\ncol_names=X_raw.columns.tolist()\nX_raw = X_raw.to_numpy()","ac9ab95d":"# Train, Test Split. Then normalize features (note - we fit the transform on training data, then apply to train and test data in order to avoid data leakage)\nX_raw_train, X_raw_test, y_train, y_test = train_test_split(X_raw, y, test_size=0.2, random_state=1) \n# Fit transform on training data\nscaler=StandardScaler()\nscaler.fit(X_raw_train)\n# Normalize train and test data by applying fitted transform\nX_train = scaler.transform(X_raw_train)\nX_test = scaler.transform(X_raw_test)","0fdb89b0":"# Test decision tree on test data for baseline, training on rest of the data (don't need validation split because not tuning hyperparameters)\n# Train model\nDT_model = DecisionTreeClassifier(random_state=0)\nDT_model.fit(X_train, y_train)\n# Test model measuring accuracy and MCC, display results\nDT_MCC = matthews_corrcoef(y_test, DT_model.predict(X_test))\nDT_ACC = accuracy_score(y_test, DT_model.predict(X_test))\nprint(DT_MCC)\nprint(DT_ACC)\n\n\n# See whether normalizing our data had an impact (by training and testing on data that wasn't normalized)\nDT_model_raw = DecisionTreeClassifier(random_state=0)\nDT_model_raw.fit(X_raw_train, y_train)\nprint(matthews_corrcoef(y_test, DT_model_raw.predict(X_raw_test)))\nprint(accuracy_score(y_test, DT_model_raw.predict(X_raw_test)))\n","e4e3ea1a":"# Train ANN\/NN (neural network). Use early stopping to mitigate overfitting. Use sklearn wrapper to be able to more easily investigate important features. \n# Note that sometimes training will get stuck at validation accuracy of around 0.7083. If this happens, re-run this cell. \n# Define early stopping behaviour\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, \n    patience=20, \n    restore_best_weights=True)\n\n# Function to create model \ndef create_model():\n    model = Sequential()\n    \n    model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\n    model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\n    model.add(Dense(units = 7, kernel_initializer = 'uniform', activation = 'relu'))\n    model.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n    model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return model\n\n# Create model using Keras wrapper for sklearn\nmodel = KerasClassifier(build_fn=create_model)\n# Train model, display results of training\nhistory = model.fit(X_train, y_train, batch_size = 32, epochs = 500, validation_split=0.2, callbacks=[early_stopping])\nval_accuracy = np.mean(history.history['val_accuracy'])\nprint(\"\\n%s: %.2f%%\" % ('val_accuracy', val_accuracy*100))","13f907d8":"# Test NN on test data\nNN_MCC = (matthews_corrcoef(y_test, model.predict(X_test)))\nNN_ACC = (accuracy_score(y_test, model.predict(X_test)))\n\nprint(NN_MCC)\nprint(NN_ACC)\n","4799f2c1":"# A quick visualization comparing model performance. Save to PDF. \ndict_to_plot = {\"DT_MCC\": DT_MCC, \"DT_ACC\": DT_ACC, \"NN_MCC\": NN_MCC, \"NN_ACC\": NN_ACC}\ncolor_to_plot = ['black', 'blue']\nf=plt.figure()\nplt.bar(dict_to_plot.keys(), dict_to_plot.values(), color=color_to_plot)\nplt.title(\"Performance of DT (Decision Tree) and NN (Neural Network)\")\nf.savefig(\"Results.pdf\", bbox_inches='tight')","5f8d4003":"# Having seen that performance is relatively good, and competitive with decision tree model, use neural network model to investigate potentially important features through finding permutation importances\nperm = PermutationImportance(model, random_state=1).fit(X_test,y_test)\neli5.show_weights(perm, feature_names = col_names)","213814ad":"<h2> Thank you for your time! Hope you learned something :) <\/h2>","7cb1c91e":"**<h1><center>Heart Failure Prediction<\/center><\/h1>**<h1>\n<center><img src=\"https:\/\/afmc.org\/wp-content\/uploads\/2017\/02\/heartfailure.jpg\" width=\"600\"><\/center>","c3dd2ac4":"<h2>Part 1: EDA (Data Quality Check, Exploration, Visualization)<\/h2>","1ff0fd82":"<h2> Part 2: Data Preparation, Model Training and Assessment <\/h2> ","b7d30b01":"<h2> Part 3: Potentially Important Features <\/h2>","8422f2ec":"The dataset studied here has 12 features which can be used to predict mortality by heart failure (which is our label). More info about this dataset can be found here: https:\/\/www.kaggle.com\/andrewmvd\/heart-failure-clinical-data\n\nOther notebooks that helped me prepare this one and which I would like to give credit to:\n\nhttps:\/\/www.kaggle.com\/shivarajmishra\/ann-vs-rest-endgame-for-heart-failure-prediction\n\nhttps:\/\/www.kaggle.com\/karnikakapoor\/heart-failure-prediction-ann\n"}}