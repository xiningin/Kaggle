{"cell_type":{"b8eb9d53":"code","da60b1ec":"code","95cf0582":"code","82d57960":"code","5fd0ae49":"code","72eeb45b":"code","49db44a8":"code","0f251b12":"code","56058db0":"code","bd772243":"code","395b3194":"markdown","8824850e":"markdown","82303ee1":"markdown","c0ceb6e5":"markdown","32920c3e":"markdown"},"source":{"b8eb9d53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da60b1ec":"\n\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics, preprocessing\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import utils\n\nimport os, gc, sys, random\nimport joblib\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n    \n\n\ndef set_seed(seed=4242):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed()","95cf0582":"def auc(y_true, y_pred):\n    def fallback_auc(y_true, y_pred):\n        try:\n            return metrics.roc_auc_score(y_true, y_pred)\n        except:\n            return 0.5\n    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)","82d57960":"def create_model(data, catcols):    \n    inputs = []\n    outputs = []\n    for c in catcols:\n        num_unique_values = int(data[c].nunique())\n        embed_dim = int(min(np.ceil((num_unique_values)\/2), 50))\n        inp = layers.Input(shape=(1,))\n        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n        out = layers.SpatialDropout1D(0.3)(out)\n        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n        inputs.append(inp)\n        outputs.append(out)\n    \n    x = layers.Concatenate()(outputs)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.Dense(400, activation=\"relu\")(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.Dense(400, activation=\"relu\")(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.BatchNormalization()(x)\n    \n    y = layers.Dense(2, activation=\"softmax\")(x)\n\n    model = Model(inputs=inputs, outputs=y)\n    return model","5fd0ae49":"train = pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","72eeb45b":"train.head()","49db44a8":"test[\"target\"] = -1\ndata = pd.concat([train, test]).reset_index(drop=True)\n\nfeatures = [x for x in train.columns if x not in [\"enrollee_id\", \"target\"]]\n\nfor feat in features:\n    lbl_enc = preprocessing.LabelEncoder()\n    data[feat] = lbl_enc.fit_transform(data[feat].fillna(\"-1\").astype(str).values)\n\ntrain = data[data.target != -1].reset_index(drop=True)\ntest = data[data.target == -1].reset_index(drop=True)\ntest_data = [test.loc[:, features].values[:, k] for k in range(test.loc[:, features].values.shape[1])]","0f251b12":"oof_preds = np.zeros((len(train)))\ntest_preds = np.zeros((len(test)))\n\nskf = StratifiedKFold(n_splits=20)\nfor train_index, test_index in skf.split(train, train.target.values):\n    X_train, X_test = train.iloc[train_index, :], train.iloc[test_index, :]\n    X_train = X_train.reset_index(drop=True)\n    X_test = X_test.reset_index(drop=True)\n    y_train, y_test = X_train.target.values, X_test.target.values\n    model = create_model(data, features)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n    X_train = [X_train.loc[:, features].values[:, k] for k in range(X_train.loc[:, features].values.shape[1])]\n    X_test = [X_test.loc[:, features].values[:, k] for k in range(X_test.loc[:, features].values.shape[1])]\n    \n    es = callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=5,\n                                 verbose=1, mode='max', baseline=None, restore_best_weights=True)\n\n    rlr = callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5,\n                                      patience=3, min_lr=1e-6, mode='max', verbose=1)\n    \n    model.fit(X_train,\n              utils.to_categorical(y_train),\n              validation_data=(X_test, utils.to_categorical(y_test)),\n              verbose=1,\n              batch_size=512,\n              callbacks=[es, rlr],\n              epochs=100\n             )\n    valid_fold_preds = model.predict(X_test)[:, 1]\n    test_fold_preds = model.predict(test_data)[:, 1]\n    oof_preds[test_index] = valid_fold_preds.ravel()\n    test_preds += test_fold_preds.ravel()\n    print(metrics.roc_auc_score(y_test, valid_fold_preds))\n    K.clear_session()","56058db0":"metrics.roc_auc_score(train.target.values, oof_preds)","bd772243":"answer = np.load('..\/input\/job-change-dataset-answer\/jobchange_test_target_values.npy')\ntest_preds\/=10\n\nmetrics.roc_auc_score(answer, test_preds)","395b3194":"more readings:\n- https:\/\/arxiv.org\/abs\/1604.06737\n- https:\/\/towardsdatascience.com\/enhancing-categorical-features-with-entity-embeddings-e6850a5e34ff\n- https:\/\/medium.com\/@davidheffernan_99410\/an-introduction-to-using-categorical-embeddings-ee686ed7e7f9\n- https:\/\/keras.io\/api\/layers\/core_layers\/embedding\/\n\n    ","8824850e":"### Entity Embeddings of Categorical Variables in Neural Networks","82303ee1":"The advantage of doing this compared to the traditional approach of creating dummy variables (i.e. doing one hot encodings), is that each day can be represented by four numbers instead of one, hence we gain higher dimensionality and much richer relationships.\n\nIn other words, entity embedding automatically learn the representation of categorical features in multi-dimensional spaces which puts values with similar effect to the target output value close to each other helping neural networks to solve the problem. You may think of if we were embedding states in a country for a sales problem, similar states in terms of sales would be closer to each in this projected space.","c0ceb6e5":"Tune keras model can boost performance ...","32920c3e":"By entity embeding We map categorical variables in a function approximation problem into Euclidean spaces, which are the entity embeddings of the categorical variables. The mapping is learned by a neural network during the standard supervised training process. Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables.\n\nIt can rapidly generate great results on structured data without having to resort to feature engineering or apply domain specific knowledge. The technique is relatively straight forward, and simply involves turning the categorical variables into numbers and then assigning each value an embedding vector:\n\n![ee](https:\/\/cdn-images-1.medium.com\/max\/800\/0*-YAcJFgksIKMvCnH)"}}