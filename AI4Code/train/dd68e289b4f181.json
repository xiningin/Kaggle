{"cell_type":{"c4860ec4":"code","7201991c":"code","039806be":"code","aad23734":"code","ed88aad0":"code","8c90060e":"code","33026730":"code","1864c4ef":"code","642813f2":"code","06714cc3":"code","491639c9":"code","21c82d01":"markdown","bf97b0d6":"markdown","fb172d0e":"markdown","b9bbe038":"markdown","aa784a52":"markdown","c8d05a13":"markdown","beeea145":"markdown","6440059f":"markdown","ce399e8d":"markdown","c7fb5d26":"markdown","45b553f4":"markdown","93bc2260":"markdown","81cabc4a":"markdown","54339a5b":"markdown"},"source":{"c4860ec4":"# Import the needed packages\nimport numpy as np  # numerical processing\nimport pandas as pd # dataframes\nimport random       # random number generation   \n\nfrom sklearn.feature_extraction import text \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer #Added this for calculating word importance\nimport spacy #Package for Tokenization, Lemmatization (normalizing word representation)","7201991c":"data = [('I am very happy', 'pos'),\n        ('I am satisfied with this.', 'pos'),\n        ('This is the best', 'pos'),\n        ('This is extremely Good!', 'pos'),\n        ('This is not bad', 'pos'),    #Problem 4: Added negation\n        ('I am not that sad', 'pos'),  #Problem 4: Added negation\n        ('I am quite positive about it', 'pos'), #Problem 1: Added 'positive' keyword\n        (\"I am really satisfied\", 'pos'),\n\n        ('I am extremely sad', 'neg'),\n        ('This is very bad', 'neg'),\n        ('It is BAD!', 'neg'),\n        ('This is extremely bad', 'neg'),\n        ('This is not good', 'neg'),  #Problem 4: Added negation\n        ('I am not satisfied with this', 'neg'), #Problem 4: Added negation\n        ('I am really very negative about this', 'neg') #Problem 1: Added 'negative' keyword \n       ]\n\n#Make it into a DataFrame for easier processing later on\ndf = pd.DataFrame(data, columns=['text','label']) #make dataframe from dictionary\ndisplay(df.head(5)) #display first 10 to see it everything is ok\ndf.to_csv(\"train_data.csv\", index=False) #save to csv","039806be":"import nltk\nfrom nltk.stem.porter import *\nfrom nltk import word_tokenize\n\n#Create own tokenization (method of splitting text into individual word tokens)\n#Problem 3 - Fix typos by replacing words, e.g represent 'the best' and 'good' the same way\nclass LemmaTokenizer:\n    def __init__(self):\n        self.sp = spacy.load('en_core_web_sm') #load english language data\n    def __call__(self, doc):\n        replacements = {'goood': 'good'} #Problem 3: replace specific tokens, e.g., common typos\n        tokens = self.sp(doc) #tokenize the document (split into words) - doc is one sentence\n        return [replacements.get(t.lemma_,t.lemma_) for t in tokens] #replace some tokens\n    \n#More on Porter tokenization: https:\/\/stackabuse.com\/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library\/ \nclass PorterTokenizer: \n    def __init__(self):\n        self.stemmer = PorterStemmer() #Create porter tokenizer\n    def __call__(self, doc):\n        replacements = {'goood': 'good'} #Problem 3: replace specific tokens, e.g., common typos\n        return [replacements.get(self.stemmer.stem(t), self.stemmer.stem(t)) for t in word_tokenize(doc)]  \n    \n#Sentences\nsentences = [\"These are the best\", \"This is good\", \"I'm good\", \"I am best\"] #some example test sentences\n\npTok = PorterTokenizer() # Create an object of Porter Tokenizer\nlTok = LemmaTokenizer() # Create an object of Lemma Tokenaize\n\n#loop through the sentences\nfor sent in sentences:\n    print(\"--Sentence: \\\"\"+sent+\"\\\"--\")\n    pTokens = pTok.__call__(sent) #extract tokens from sentence using PorterTokenizer (essentially split into a list of words)\n    lTokens = lTok.__call__(sent)\n    print(\"\\tPorter:\", end=' ')\n    for token in pTokens:\n        print(token, end=' | ') #print the unchanged word -> lemmatized version\n    print(\"\\n\\tLemma:\", end=' ')\n    for token in lTokens:\n        print(token, end=' | ') #print the unchanged word -> lemmatized version\n    print(\"\\n\")\n    ","aad23734":"#Converting words to number using Scikit-learn CountVectorizer\n\n#print(text.ENGLISH_STOP_WORDS)\n     \n#Problem 3: tokenizer=PorterTokenizer(), LemmaTokenizer() - own tokenization that can fix typos and normalize words (lemmatization)\n#Problem 2: stop_words=text.ENGLISH_STOP_WORDS.difference({'not'}) - removing stop words, such as 'is', 'this', 'that', 'it'\n#          Porter specific - stop_words=['!','.','am','about','veri','is', 'i','it','quit','that','the','with','thi','extrem']\n#          Lemma specific - stop_words=['this','that','the','i','.','!','with','-PRON-','about', 'be', 'very', 'quite', 'extremely', 'really']\n#Problem 4: ngram_range=(1,2) - we will not rely on just inidividual words, but on tuples as well\n\ncount_vect = CountVectorizer(\n    tokenizer = LemmaTokenizer(),\n    stop_words = ['about', 'this', 'quite', 'that', 'the', '!', '.', 'i', '-PRON-', 'with'],\n    ngram_range=(1,2)\n    \n) #create the CountVectorizer object for spliting sentence into words and replaceing them with numbers\n                            \n#Learn the vocabulary and transform our dataset from words to ids\nword_counts = count_vect.fit_transform(df['text'].values) #Transform the text into numbers (bag of words features)\n\n#get our vocabulary, index represents id\nvocabulary = count_vect.get_feature_names()\n#Loop through the vocabulary and print word id and words:\nprint(\"Vocabulary:\", len(vocabulary), \"phrases\")\nfor word_id, word in enumerate(vocabulary):\n    print(str(word_id)+\" -> \"+str(vocabulary[word_id])) #Show ID and word\n    \nprint(\"\\nSentences:\", df['text'].values) #just show the list of our sentences for reference\n    \nprint(\"\\nSize (sentenes x words):\", word_counts.shape) #Display the size of our array (list of lists)\nprint(\"Representation of our sentences as an array of word counts:\") \n    \n#Check how text sentences from our data were replaced by numbers\nprint(word_counts.toarray()) #represent the ","ed88aad0":"#Transform our dataset to prioritize unique words\ntfidf_transformer = TfidfTransformer(use_idf=True)\nword_importance = tfidf_transformer.fit_transform(word_counts) #calculates word importance based on data\n\n#Print a list of our sentences\nprint(\"Sentences:\", df['text'].values) #just show the list of our sentences for reference\n\n#Display the size of our array (list of lists)\nprint(\"\\nSize (sentenes x words):\", word_importance.shape) \nprint(\"Representation of our sentences as an array of importance scores:\") \n#Display an array (list of lists) of tf-idf scored phrases\nprint(np.round(word_importance.toarray(),2))\n\n#This looks closer at one sentence from our dataset\ndoc = 13 #Which document from our dataframe to show\nfeature_index = word_importance[doc,:].nonzero()[1] #get only non-zero (present) phrases in that document\ntfidf_scores = zip(feature_index, [word_importance[doc, x] for x in feature_index]) #get the tf-idf score for each phrase\nfor w, s in [(vocabulary[i], s) for (i, s) in tfidf_scores]:\n    print(w, s)","8c90060e":"#Import different classifiers from Scikit-learn: \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB \nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\n\n#Create the machine learning classifier object\nclf = LinearSVC() \n\nX = word_importance #features of sentences (word importance numbers)\ny = df['label'].values #correct labels for each document\n#print(\"X:\", X.toarray())\n#print(\"y:\", y)\n\n#Train the classifer on our data\n#First argument is an array (list of lists) representing words present in each sentence\n#Second argument are the names of our classes ('pos' and 'neg' in this case)\nclf.fit(X, y) #Change: word_counts changed to word_importance","33026730":"#Import accuracy_score function that calculates classification accuracy\n#check: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html\nfrom sklearn.metrics import accuracy_score\n\n#predict labels for all the sentences\npred_label = clf.predict(X) #Change: word_counts change to word_importance\n#Calculate accuracy on our training data, parameters: correct labels, predicted labels\nprint(\"Mean accuracy on training data:\", accuracy_score(df['label'].values, pred_label))\n\n#Let's put true classes and predictions next to one another\ndisplay(pd.DataFrame({\"sentence\":df['text'].values, \n                      \"true label\":df['label'].values, \n                      \"predicted\": pred_label}))","1864c4ef":"#Let's transform the sentence into numbers\nsentences = ['I feeling not bad', \"It is extremely bad\", \"This is not that good at all\", \"I am really satisfied\"]\nfor s in sentences:\n    print(\"'%s': %s\" % (s, clf.predict( tfidf_transformer.transform( count_vect.transform([s])))) )\n\n#Let's loop through individual extracted words and their importance\nwc = count_vect.transform([\"I am really very satisfied\"]) #get phrase count\nwi = tfidf_transformer.transform(wc) #get phrase importance weight\nfor word_id, importance in enumerate(wi.toarray()[0]):\n    print(\"[\"+str(word_id)+\"]\"+str(vocabulary[word_id])+\" -> \"+str(importance))","642813f2":"# Problem 1: Previously unseen words\nsentences = [\"This is exceptionally bad!\", \"This is exceptionally good!\", \n             \"This is very positive!\", \"This is very negative!\"]\nprint(\"--- Problem 1: Previously unseen words ---\")\nfor s in sentences:\n    wc = count_vect.transform([s]) #get phrase count\n    wi = tfidf_transformer.transform(wc) #get phrase importance weight\n    print(\"'%s': %s\" % (s, clf.predict(wi)) )\n\n#display(df)\n    \n# Problem 2: Unimportant words influencing the prediction, e.g;\"it\", \"is\", \"extremely\"\nprint(\"\\n--- Problem 2: Unimportant words influencing the prediction ---\")\nsentences = ['This is extremely good', 'It is extremely good']\nfor s in sentences:\n    wc = count_vect.transform([s]) #get phrase count\n    wi = tfidf_transformer.transform(wc) #get phrase importance weight\n    print(\"'%s': %s\" % (s, clf.predict( wi)) )\n\n# Problem 3: Typos, different pronounciation\nprint(\"\\n--- Problem 3: Typos, different pronounciation ---\")\nsentences = ['This is extremely good', 'This is extremely goood']\nfor s in sentences:\n    wc = count_vect.transform([s]) #get phrase count\n    wi = tfidf_transformer.transform(wc) #get phrase importance weight\n    print(\"'%s': %s\" % (s, clf.predict(wi)) )\n    #bow = count_vect.transform([s])\n    #print(bow.toarray())\n\n# Problem 4: Negations (importance of word ordering)\nsentences = [\"This is good!\", \"This is not good!\", \"This is bad!\", \"This is not that bad!\"]\nprint(\"\\n--- Problem 4: Word sequences (negations) --- \")\nfor s in sentences:\n    wc = count_vect.transform([s]) #get phrase count\n    wi = tfidf_transformer.transform(wc) #get phrase importance weight\n    print(\"'%s': %s\" % (s, clf.predict(wi)) ) ","06714cc3":"from sklearn.model_selection import train_test_split #import a function that does splitting of the data\nfrom sklearn import metrics # Import a number of metrics from scikit-learn\n\n#This code splits our dataset into training subset (X_train, y_train) and testing subset (X_test, y_test)\nX_train, X_test, y_train, y_test = train_test_split(df['text'].values, # The features of our text documents\n                                                    y,                 # The correct labels for examples\n                                                    test_size=0.20,    # How much of the data do we put aside for testing, 0.2 -> 20%\n                                                    random_state=0)    #  A random 'seed' number used to impact the randomization (same 'seed' will give the same split)\n\nprint(\"--Train data---\")\nprint(X_train)\nprint(y_train)\n\nprint(\"\\n---Test data---\")\nprint(X_test)\nprint(y_test)\n\n#Let's train our classifer only on the training subset of our data\nwc = count_vect.transform(X_train) #get phrase count\nwi = tfidf_transformer.transform(wc) #get phrase importance weight\n#Train the classifier on the test subset of the data\nclf.fit(wi, y_train)\n\n#Let's evaluate its performance only on unseen data (testing subset)\nwc = count_vect.transform(X_test) #get phrase count\nwi = tfidf_transformer.transform(wc) #get phrase importance weight\n#Predict text labels for the test subset of the data\npred_label = clf.predict(wi)\nprint(\"Predictions:\", pred_label)\n\nprint(\"\\n---Accuracy on test data---\")\nprint(\"Mean accuracy on test data:\", accuracy_score(y_test, pred_label))","491639c9":"#import packages needed for cross-validation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\n# Transform text data into phrase counts (wc) and then phrase importance scores (wi)\nwc = count_vect.transform(df['text'].values) #get phrase count\nwi = tfidf_transformer.transform(wc) #get phrase importance weight\n\n# Assigning featues to X and correct labels to y\nX = wi #input, features of our sentences\ny = df['label'].values #output, correct labels\n\n#Random split of the data int k subsets\ncv = StratifiedKFold(3,              # The number of splits (subsets we divide our data into)           \n                     shuffle=True,   # Should it be random?\n                     random_state=0) # Randomization 'seed', if the same number is used the random split will be the same\n\n#Perform the cross validation, train on n-1 splits and test on the remaining n-th split\nscores = cross_val_score(clf, X, y, cv=cv, scoring='accuracy')\n\nprint(\"Scores for each fold:\", scores) # A list of individual scores for each split\nprint(\"Mean accuracy score: %0.2f\" % np.mean(scores)) #Average score","21c82d01":"# Step 1: Prepare some simple data","bf97b0d6":"## Cross-Validation\nSplitting our whole dataset into k subsets (e.g., equally into 3 subsets) and then using k-1 (e.g., 2 out of 3) to train our classifier and the remaining one subset to test how well it performed\n\n![](https:\/\/qph.fs.quoracdn.net\/main-qimg-29c6f21ce298acfa228f37448f844ab8)\n* StratifiedKFold: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html\n* cross_val_score: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html\n* Cross-validation: https:\/\/machinelearningmastery.com\/k-fold-cross-validation\/","fb172d0e":"## Breaking the ML\n#### Problems from part 1\n* 1. Previously unseen words\n> \"This is exceptionally bad!\" -> 'neg' <br \/>\n> \"This is exceptionally good!\" -> 'pos' <br \/>\n> \"This is very positive!\" -> 'pos' <br \/>\n> \"This is very negative!\" -> 'pos'\n* 2. Unimportant words influencing the prediction\n> \"This is extremely good\" -> 'pos' <br \/>\n> \"It is extremely good\" -> 'neg'\n* 3. Typos, word variations\n> \"This is extremely good\" -> 'pos' <br \/>\n> \"This is extremely goood\" -> 'neg\n* 4. Negations (importance of word sequences)\n> \"This is good!\" -> 'pos' <br \/> \n> \"This is not good!\" -> 'pos' <br \/> \n> \"This is bad!\" -> 'neg' <br \/> \n> \"This is not that bad!\" -> 'neg'","b9bbe038":"Further reading: https:\/\/towardsdatascience.com\/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f","aa784a52":"## a. Normalizing word representation (tokenization, lemmatization)","c8d05a13":"# Step 3: Train a text classifier\nWe can use the numberic representation of the sentences to try to learn a machine learning model to differentiate between different types of documents. This is called classification, we want to assign each document into a class. In our case we have two classes: 'pos', 'neg' <br \/>\n#### Resources:\n* https:\/\/stackabuse.com\/overview-of-classification-methods-in-python-with-scikit-learn\/\n* https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html\n* https:\/\/scikit-learn.org\/stable\/supervised_learning.html","beeea145":"## b. Counting word occurences (Bag-of-Words with CountVectorizer)\nMachine learning algorithms are good with numbers; we have to extract or convert the text data into numbers without losing much of the information. One way to do such transformation is Bag-Of-Words (BOW) which gives a number to each word but that is very inefficient. So, a way to do it is by CountVectorizer: it counts the number of words in the document i.e it converts a collection of text documents to a matrix of the counts of occurences of each word in the document.","6440059f":"## Train-test split\nWe want to get a sense of how well will our text classifier perform on a new, unseen data. Train-test split try to simulate this by splitting our whole data into <i>training subset<\/i> and <i>testing subset<\/i>. We can then train our classifer only on the examples from the training subset and evaluate how well it performas on the unseen examples from the testing subset. This will give us an estimate of how well it can perform with unseen data. Splits can be 80% train, 20% test or 70% train, 30% test.\n\n* train_test_split: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html\n","ce399e8d":"## Classifying new text","c7fb5d26":"# Step 2: Turn words into numbers\nMachine Learning algorithms can not directly process the text documents in their original form. They expect numbers rather than the raw text. We need to convert text to numbers in some way. One simple way to do it is to assign a number to each word and reuse this number if the word is seen again. In this case we will be able to see if some words are mor common in some document types than others.\n* Count Vectorizer documentation: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html\n* Bag of Words text representation: https:\/\/machinelearningmastery.com\/gentle-introduction-bag-words-model\/\n* Text preprocessing: https:\/\/blog.cambridgespark.com\/tutorial-preprocessing-text-data-a8969189b779\n* Porter Tokenizer, Lemma Tokenizer: https:\/\/stackabuse.com\/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library\/ | https:\/\/www.datacamp.com\/community\/tutorials\/stemming-lemmatization-python\n* Spacy package for text processing: https:\/\/spacy.io\/usage\/linguistic-features","45b553f4":"# Text Classification - part 2\nScikit-learn: https:\/\/scikit-learn.org\/stable\/\n\n<ul>\n<li>Step 1: Preparing some simple data\n<li><div style=\"padding: 10px 0\">Step 2: Turning words into numbers (suitable for classification)<\/div>\n    <ul>\n    <li>a. Normalizing word representation (tokenization, lemmatization)\n    <li>b. Counting word occurences (Bag-of-Words with CountVectorizer)\n    <li>c. Prioritizing unique words (TF\/IDF score)\n    <\/ul>\n<li> Step 3: Training a text classifier\n<li> Step 4: Evaluating the classifier\n<li> Step 5: Better evaluation approaches\n<\/ul>\n\n<b> Problems from part 1 <\/b>\n* 1. Previously unseen words\n> \"This is exceptionally bad!\" -> 'neg' <br \/>\n> \"This is exceptionally good!\" -> 'pos' <br \/>\n> \"This is very positive!\" -> 'pos' <br \/>\n> \"This is very negative!\" -> 'pos'\n* 2. Unimportant words influencing the prediction\n> \"This is extremely good\" -> 'pos' <br \/>\n> \"It is extremely good\" -> 'neg'\n* 3. Typos, word variations\n> \"This is extremely good\" -> 'pos' <br \/>\n> \"This is extremely goood\" -> 'neg\n* 4. Negations (importance of word sequences)\n> \"This is good!\" -> 'pos' <br \/> \n> \"This is not good!\" -> 'pos' <br \/> \n> \"This is bad!\" -> 'neg' <br \/> \n> \"This is not that bad!\" -> 'neg'","93bc2260":"# Step 4: Evaluating the classifier","81cabc4a":"## c. Prioritize unique words (TF\/IDF score)\nIn case of CountVectorizer (above), we are just counting the number of words in the document and many times it happens that some words like \"are\",\"you\",\"hi\",etc are very large in numbers and that would dominate our results in machine learning algorithm.\n* **How is TF-IDF different from CountVectorizer (Bag-of-Words above)?** <br \/>\nSo, TF-IDF (stands for Term-Frequency-Inverse-Document Frequency) weights down the common words occuring in almost all the documents and give more importance to the words that appear in a subset of documents. TF-IDF works by penalizing these common words by assigning them lower weights while giving importance to some rare words in a particular document.\n* **How exactly does TF-IDF work?** <br \/>\nConsider the below sample table which gives the count of terms(tokens\/words) in two documents.\n![](https:\/\/i.imgur.com\/iVOI1TQ.png)\nNow, let us define a few terms related to TF-IDF.\n\n* **TF (Term Frequency):** Denotes the contribution of the word to the document i.e. words relevant to the document should be frequent.\n<p><i><div style=\"background-color: #eeeeee; padding:10px 0px\">TF = (Number of times term t appears in a document)\/(Number of terms in the document)<\/div><\/i><\/p>\n<p>\n    <ul>\n<li>TF(This, Document1) = 1\/8\n<li>TF(This, Document2) = 1\/5\n    <\/ul>\n<\/p>\n\n* **IDF (Inverse Document Frequency):** If a word has appeared in all the document, then probably that word is not relevant to a particular document. But, if it has appeared in a subset of documents then probably the word is of some relevance to the documents it is present in.\n<p><i><div style=\"background-color: #eeeeee; padding:10px 0px\">IDF = log(N\/n), where, N is the number of documents and n is the number of documents a term t has appeared in.<\/div><\/i><\/p>\n<p>\n    <ul>\n<li>IDF(This) = log(2\/2) = 0. \n<li>IDF(Messi) = log(2\/1) = 0.301.\n        <\/ul>\n<\/p>\n\n<p>Now, let us compare the **TF-IDF** for a common word \u2018This\u2019 and a word \u2018Messi\u2019 which seems to be of relevance to Document 1.<\/p>\n<p>\n    <ul>\n<li>TF-IDF(This,Document1) = (1\/8) * (0) = 0\n<li>TF-IDF(This, Document2) = (1\/5) * (0) = 0\n<li>TF-IDF(Messi, Document1) = (4\/8) * 0.301 = 0.15\n<\/ul>\n<\/p>\n\n<p>So, for Document1 , TF-IDF method heavily penalises the word \u2018This\u2019 but assigns greater weight to \u2018Messi\u2019. So, this may be understood as \u2018Messi\u2019 is an important word for Document1 from the context of the entire corpus.<\/p>\n\n* Credit for explanation: https:\/\/www.kaggle.com\/divsinha\/sentiment-analysis-countvectorizer-tf-idf\n* More on TF\/IDF: https:\/\/www.youtube.com\/watch?v=ouEVPRMHR1U","54339a5b":"# Step 5: Better evaluation approaches"}}