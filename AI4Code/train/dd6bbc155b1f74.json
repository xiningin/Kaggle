{"cell_type":{"1be99e1d":"code","c932f2b6":"code","2cb56b39":"code","21d24618":"code","e7751029":"code","19aeb515":"code","988d405a":"code","7ff0b6a1":"code","a205cb55":"code","cc2fe4c4":"code","520cbee6":"code","bc50680c":"code","796e7437":"code","53a2e5b4":"code","12eb099a":"code","a6bb9213":"code","53af0178":"code","2aba837a":"code","15f4001b":"code","aa385abf":"code","aa8f0f37":"code","3e8e3116":"code","33669349":"code","1c654600":"code","7983cabd":"code","54c25e99":"code","9956a809":"code","17721a67":"code","5add97bc":"code","a731e06b":"code","162e367e":"code","f501a6e8":"markdown","92b28df0":"markdown","3c5adf44":"markdown"},"source":{"1be99e1d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c932f2b6":"import matplotlib.pyplot as plt","2cb56b39":"df = pd.read_csv(\"\/kaggle\/input\/openintro-possum\/possum.csv\")\ndf.head()","21d24618":"df.info()","e7751029":"from sklearn.preprocessing import minmax_scale\nfrom sklearn.model_selection import train_test_split","19aeb515":"def prepro(df):\n    \n    df = df.copy()\n    \n    for col in df.columns:\n        if df[col].dtype != \"object\":\n            df[col].fillna(df[col].mean(),inplace=True)\n            \n    df.drop(columns=[\"case\"])\n    \n    df[\"Pop\"] = df[\"Pop\"].replace({\"Vic\":0,\"other\":1})\n    \n    X = df.drop(\"sex\",axis=1)\n    y = df[\"sex\"]\n\n    X = pd.DataFrame(minmax_scale(X),index=X.index,columns=X.columns)\n    \n    return train_test_split(X, y, test_size=0.3)\n    ","988d405a":"X_train,X_test,y_train,y_test = prepro(df)","7ff0b6a1":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC","a205cb55":"knn = KNeighborsClassifier()\nlogmod = LogisticRegression()\nsvc = SVC()\n\nknn.fit(X_train,y_train)\nlogmod.fit(X_train,y_train)\nsvc.fit(X_train,y_train)\n\nprint(\"K Nearest Neighbours Model Score: {}\".format(knn.score(X_test,y_test)))\nprint(\"\")\nprint(\"Logistic Regression Model Score: {}\".format(logmod.score(X_test,y_test)))\nprint(\"\")\nprint(\"Support Vector Model Score: {}\".format(svc.score(X_test,y_test)))","cc2fe4c4":"errors = []\nfor i in range(1,30):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    \n    p = knn.predict(X_test)\n    \n    errors.append(np.mean(p != y_test))\n\nplt.figure(figsize=(12,6))\nplt.plot(range(1,30),errors,ls=\"--\",marker=\"X\",\n        markerfacecolor=\"red\")","520cbee6":"knn = KNeighborsClassifier(n_neighbors=11)\nknn.fit(X_train,y_train)\nprint(\"KNN Score: \"+str(knn.score(X_test,y_test)))","bc50680c":"predicted = knn.predict(X_test)","796e7437":"from sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(y_test,predicted))\nprint(classification_report(y_test,predicted))","53a2e5b4":"df.head()","12eb099a":"from sklearn.decomposition import PCA","a6bb9213":"def prepro(df):\n    \n    df = df.copy()\n    \n    for col in df.columns:\n        if df[col].dtype != \"object\":\n            df[col].fillna(df[col].mean(),inplace=True)\n            \n    \n    df[\"Pop\"] = df[\"Pop\"].replace({\"Vic\":0,\"other\":1})\n    \n    X = df.drop(columns=[\"sex\",\"case\"],axis=1)\n    y = df[\"sex\"].replace({\"m\":1,\"f\":0})\n    \n    pca = PCA(n_components=2)\n    X = pca.fit_transform(X)\n    \n    return X,y","53af0178":"X,y = prepro(df)","2aba837a":"plt.figure(figsize=(8,6))\nplt.scatter(X[:,0],X[:,1],c=y)","15f4001b":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)","aa385abf":"knn = KNeighborsClassifier()\nlogmod = LogisticRegression()\nsvc = SVC()\n\nknn.fit(X_train,y_train)\nlogmod.fit(X_train,y_train)\nsvc.fit(X_train,y_train)\n\nprint(\"K Nearest Neighbours Model Score: {}\".format(knn.score(X_test,y_test)))\nprint(\"\")\nprint(\"Logistic Regression Model Score: {}\".format(logmod.score(X_test,y_test)))\nprint(\"\")\nprint(\"Support Vector Model Score: {}\".format(svc.score(X_test,y_test)))","aa8f0f37":"preds = logmod.predict(X_test)\n\nprint(classification_report(y_test,preds))","3e8e3116":"def prepro(df):\n    \n    df = df.copy()\n    \n    for col in df.columns:\n        if df[col].dtype != \"object\":\n            df[col].fillna(df[col].mean(),inplace=True)\n            \n    \n    df[\"Pop\"] = df[\"Pop\"].replace({\"Vic\":0,\"other\":1})\n    \n    X = df.drop(columns=[\"sex\",\"case\"],axis=1)\n    y = df[\"sex\"].replace({\"m\":1,\"f\":0})\n    \n    pca = PCA(n_components=3)\n    X = pca.fit_transform(X)\n    \n    return X,y","33669349":"X,y = prepro(df)","1c654600":"import cufflinks as cf\ncf.go_offline()","7983cabd":"pcdf = pd.DataFrame(X,columns=[\"PC1\",\"PC2\",\"PC3\"])","54c25e99":"pcdf = pd.concat([pcdf,pd.DataFrame(y)],axis=1)\npcdf.head()","9956a809":"pcdf[\"sex_as_str\"] = pcdf[\"sex\"].replace({1:\"m\",0:\"f\"})","17721a67":"pcdf.iplot(kind=\"scatter3d\",\n          x=\"PC1\",y=\"PC2\",z=\"PC3\",\n          categories=\"sex_as_str\",\n          size=4)","5add97bc":"X = pcdf.drop([\"sex_as_str\",\"sex\"],axis=1)\n\ny = pcdf[\"sex\"]\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n\nlogmod = LogisticRegression()\n\nlogmod.fit(X_train,y_train)\n\nlogmod.score(X_test,y_test)","a731e06b":"p = logmod.predict(X_test)","162e367e":"print(classification_report(y_test,p))","f501a6e8":"unfortunately there is no clear definition between our two classes even when reduced to two principal components","92b28df0":"Still, with a 3d plot of three principal components, it is difficult to see a delineation between classes.\nNevertheless, I will run the logistic model on the three principal components","3c5adf44":"Can principal component analysis help imporve accuracy?"}}