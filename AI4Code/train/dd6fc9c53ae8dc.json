{"cell_type":{"805f6ec1":"code","9eed35fa":"code","3af2969e":"code","c231d59a":"code","0d588340":"code","d9a48264":"code","e993b7ac":"code","47f2b12f":"code","1b38e86f":"code","6b424891":"code","952ed470":"code","5e4d0723":"code","a2afa742":"code","382102fb":"code","643e64bc":"code","5f06bf46":"code","e4ee2b7c":"code","8b98de7a":"code","94d2eb1e":"code","b98ff43c":"code","2ad7061d":"code","24bc3074":"code","da258acb":"code","fa757223":"code","057e983c":"code","4824e76b":"code","43ddbfe5":"code","9997e841":"code","388c0f1c":"code","10364274":"code","07523eb6":"code","779d31c2":"code","817626bb":"code","e41be322":"code","2b56eeea":"markdown","74c57f39":"markdown","074941ca":"markdown","2ab4c5b2":"markdown","b9038fb6":"markdown","f9ce1845":"markdown","53827d3c":"markdown","45a6bb91":"markdown","38becb59":"markdown","7a7fcdc8":"markdown","260df1d4":"markdown","7726babc":"markdown","76679545":"markdown","9d2e8ac5":"markdown","971e9c50":"markdown","b38dca23":"markdown","329396fe":"markdown","09ca8651":"markdown","01889d7a":"markdown","846f9036":"markdown","47e994fb":"markdown","99defd92":"markdown"},"source":{"805f6ec1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9eed35fa":"ride_sharing=pd.read_csv(\"..\/input\/datacleansing\/ride_sharing_fixingDuplicates.csv\")\n\n# Print the information of ride_sharing\nprint(ride_sharing.info())\n\n# Print summary statistics of user_type column\nprint(ride_sharing['user_type'].describe())","3af2969e":"# Convert user_type from integer to category\nride_sharing['user_type_cat'] = ride_sharing['user_type'].astype(\"category\")\n\n# Write an assert statement confirming the change\nassert ride_sharing['user_type_cat'].dtype == 'category'\n\n# Print new summary statistics \nprint(ride_sharing['user_type_cat'].describe())","c231d59a":"# Strip duration of minutes\nride_sharing['duration_trim'] = ride_sharing['duration'].str.strip(\"minutes\")\n\n# Convert duration to integer\nride_sharing['duration_time'] = ride_sharing['duration_trim'].astype(\"int\")\n\n# Write an assert statement making sure of conversion\nassert ride_sharing['duration_time'].dtype == 'int'\n\n# Print formed columns and calculate average ride duration \nprint(ride_sharing[['duration','duration_trim','duration_time']])\nprint(ride_sharing['duration_time'].mean())","0d588340":"ride_sharing.info()\n# Convert tire_sizes to integer\n\nride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n\n# Set all values above 27 to 27\nride_sharing.loc[ride_sharing['tire_sizes'] > 27, \"tire_sizes\"] = 27\n\n# Reconvert tire_sizes back to categorical\nride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n\n# Print tire size description\nprint(ride_sharing['tire_sizes'].describe())","d9a48264":"import datetime as dt\n# Convert ride_date to datetime\nride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date'])\n\n# Save today's date\ntoday = dt.date.today()\n\n# Set all in the future to today's date\nride_sharing.loc[ride_sharing['ride_dt'].dt.date > today, 'ride_dt'] = today\n\n# Print maximum of ride_dt column\nprint(ride_sharing['ride_dt'].max())","e993b7ac":"ride_sharing_dupl=pd.read_csv(\"..\/input\/datacleansing\/ride_sharing_findingDuplicates.csv\")\nprint(ride_sharing_dupl.head())\n# Find duplicates\nduplicates = ride_sharing_dupl.duplicated(subset='ride_id', keep=False)\n\n# Sort your duplicated rides\nduplicated_rides = ride_sharing_dupl[duplicates].sort_values('ride_id')\n\n# Print relevant columns of duplicated_rides\nprint(duplicated_rides[['ride_id','duration','user_birth_year']])","47f2b12f":"# Drop complete duplicates from ride_sharing\nride_dup = ride_sharing_dupl.drop_duplicates()\n#print(ride_dup)\n\n# Create statistics dictionary for aggregation function\nstatistics = {'user_birth_year': 'min', 'duration': 'mean'}\n\n# Group by ride_id and compute new statistics\nride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n\n# Find duplicated values again\nduplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\nduplicated_rides = ride_unique[duplicates == True]\n#print(duplicated_rides)\n\n# Assert duplicates are processed\nassert duplicated_rides.shape[0] == 0","1b38e86f":"airlines=pd.read_csv(\"..\/input\/datacleansing\/airlines_final.csv\")\n# Print categories DataFrame\n#print(categories)\n\n# Print unique values of survey columns in airlines\nprint('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\nprint('Safety: ', airlines['safety'].unique(), \"\\n\")\nprint('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")","6b424891":"categories=pd.read_csv(\"..\/input\/datacleansing\/categories.csv\")\n# Find the cleanliness category in airlines not in categories\ncat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n\n# Find rows with that category\ncat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n\n# Print rows with inconsistent category\nprint(airlines[cat_clean_rows])\n\n# Print rows with consistent categories only\nprint(airlines[~cat_clean_rows])","952ed470":"# Print unique values of both columns\nprint(airlines['dest_region'].unique())\nprint(airlines['dest_size'].unique())","5e4d0723":"# Lower dest_region column and then replace \"eur\" with \"europe\"\nairlines['dest_region'] = airlines['dest_region'].str.lower()\nairlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})","a2afa742":"# Remove white spaces from `dest_size`\nairlines['dest_size'] = airlines['dest_size'].str.strip()\n\n# Verify changes have been effected\nprint(airlines['dest_region'].unique())\nprint(airlines['dest_size'].unique())","382102fb":"# Create ranges for categories\nlabel_ranges = [0, 60, 180, np.inf]\nlabel_names = ['short', 'medium', 'long']\n\n# Create wait_type column\nairlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n                                labels = label_names)\n\n# Create mappings and replace\nmappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n            'Thursday': 'weekday', 'Friday': 'weekday', \n            'Saturday': 'weekend', 'Sunday': 'weekend'}\n\nairlines['day_week'] = airlines['day'].replace(mappings)\nprint(airlines['day_week'].unique())","643e64bc":"arilinesWithNames=pd.read_csv(\"..\/input\/datacleansing\/airlines_WithNames.csv\")\nprint(arilinesWithNames.head())\nprint(arilinesWithNames['full_name'])\n# Replace \"Dr.\" with empty string \"\"\narilinesWithNames['full_name'] = arilinesWithNames['full_name'].str.replace(\"Dr.\",\"\")\n\n# Replace \"Mr.\" with empty string \"\"\narilinesWithNames['full_name'] = arilinesWithNames['full_name'].str.replace(\"Mr.\",\"\")\n\n# Replace \"Miss\" with empty string \"\"\narilinesWithNames['full_name'] = arilinesWithNames['full_name'].str.replace(\"Miss\",\"\")\n# Replace \"Ms.\" with empty string \"\"\narilinesWithNames['full_name'] = arilinesWithNames['full_name'].str.replace(\"Ms.\",\"\")\n\n# Assert that full_name has no honorifics\nassert arilinesWithNames['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False","5f06bf46":"airlinesWithSurvey=pd.read_csv(\"..\/input\/datacleansing\/airlines_WithSurvey.csv\")\n# Store length of each row in survey_response column\nresp_length = airlinesWithSurvey['survey_response'].str.len()\n\n# Find rows in airlines where resp_length > 40\nairlines_survey = airlinesWithSurvey[resp_length > 40]\n\n# Assert minimum survey_response length is > 40\nassert airlines_survey['survey_response'].str.len().min() > 40\n\n# Print new survey_response column\nprint(airlines_survey['survey_response'])","e4ee2b7c":"bankingWithCurr=pd.read_csv(\"..\/input\/datacleansing\/banking_WithCurrency.csv\")\nprint(bankingWithCurr.head())\n# Find values of acct_cur that are equal to 'euro'\nacct_eu = bankingWithCurr['acct_cur'] == 'euro'\n\n# Convert acct_amount where it is in euro to dollars\nbankingWithCurr.loc[acct_eu, 'acct_amount'] = bankingWithCurr.loc[acct_eu, 'acct_amount'] * 1.1\n\n# Unify acct_cur column by changing 'euro' values to 'dollar'\nbankingWithCurr.loc[acct_eu, 'acct_cur'] = 'dollar'\n\n# Assert that only dollar currency remains\nassert bankingWithCurr['acct_cur'].unique() == 'dollar'\nprint(bankingWithCurr.head())","8b98de7a":"# Print the header of account_opend\nprint(bankingWithCurr['account_opened'].head())\n\n# Convert account_opened to datetime\nbankingWithCurr['account_opened'] = pd.to_datetime(bankingWithCurr['account_opened'],\n                                           # Infer datetime format\n                                           infer_datetime_format = True,\n                                           # Return missing value for error\n                                           errors = 'coerce') \n\n# Get year of account opened\nbankingWithCurr['acct_year'] = bankingWithCurr['account_opened'].dt.strftime('%Y')\n\n# Print acct_year\nprint(bankingWithCurr['acct_year'])","94d2eb1e":"banking=pd.read_csv(\"..\/input\/datacleansing\/banking_dirty.csv\")\nprint(banking.head())\n# Store fund columns to sum against\nfund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n\n# Find rows where fund_columns row sum == inv_amount\ninv_equ = banking[fund_columns].sum(axis=1) == banking[\"inv_amount\"]\n\n# Store consistent and inconsistent data\nconsistent_inv = banking[inv_equ]\ninconsistent_inv = banking[~inv_equ]\n\n# Store consistent and inconsistent data\nprint(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])","b98ff43c":"print(banking.info())\nbanking[\"birth_date\"]=pd.to_datetime(banking[\"birth_date\"])\n# Store today's date and find ages\ntoday = dt.date.today()\nages_manual = today.year - banking[\"birth_date\"].dt.year\n\n# Find rows where age column == ages_manual\nage_equ = ages_manual== banking[\"Age\"]\n\n# Store consistent and inconsistent data\nconsistent_ages = banking[age_equ]\ninconsistent_ages = banking[~age_equ]\n\n# Store consistent and inconsistent data\nprint(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])","2ad7061d":"import matplotlib.pyplot as plt\nimport missingno as msno\nbankingMissing=pd.read_csv(\"..\/input\/datacleansing\/banking_Missing.csv\")\nprint(bankingMissing.head())\nprint(bankingMissing.info())\n# Print number of missing values in banking\nprint(bankingMissing.isna().sum())\n\n# Print number of missing values in banking\nprint(bankingMissing.isna().sum())\n\n# Visualize missingness matrix\nmsno.matrix(bankingMissing)\nplt.show()","24bc3074":"# Isolate missing and non missing values of inv_amount\nmissing_investors = bankingMissing[bankingMissing[\"inv_amount\"].isna()]\ninvestors = bankingMissing[~bankingMissing[\"inv_amount\"].isna()]","da258acb":"print(investors.describe())\nprint(missing_investors.describe())","fa757223":"# Sort banking by age and visualize\nbanking_sorted = bankingMissing.sort_values(by=\"age\")\nmsno.matrix(banking_sorted)\nplt.show()","057e983c":"bankingMissing=pd.read_csv(\"..\/input\/datacleansing\/banking_MissingCustIDAndAccntAmnt.csv\")\n# Drop missing values of cust_id\nbanking_fullid = bankingMissing.dropna(subset = ['cust_id'])\n\n# Compute estimated acct_amount\nacct_imp = banking_fullid[\"inv_amount\"] * 5\n\n# Impute missing acct_amount with corresponding acct_imp\nbanking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n\n# Print number of missing values\nprint(banking_imputed.isna().sum())","4824e76b":"restaurants=pd.read_csv(\"..\/input\/datacleansing\/restaurants_L2_dirty.csv\")\nprint(restaurants.head())\nprint(restaurants.info())\n# Import process from fuzzywuzzy\nfrom fuzzywuzzy import process \n\n# Store the unique values of cuisine_type in unique_types\nunique_types = restaurants[\"type\"].unique()\n\n# Calculate similarity of 'asian' to all values of unique_types\nprint(process.extract('asian', unique_types, limit = len(unique_types)))\n\n# Calculate similarity of 'american' to all values of unique_types\nprint(process.extract('american', unique_types, limit=len(unique_types)))\n\n# Calculate similarity of 'italian' to all values of unique_types\nprint(process.extract('italian',unique_types,limit=len(unique_types)))","43ddbfe5":"categories=['italian', 'asian', 'american']\n# Inspect the unique values of the cuisine_type column\nprint(restaurants['type'].unique())","9997e841":"# Create a list of matches, comparing 'italian' with the cuisine_type column\nmatches = process.extract('italian',restaurants['type'],limit=len(restaurants))\n\n# Inspect the first 5 matches\nprint(matches[0:5])","388c0f1c":"\n# Iterate through the list of matches to italian\nfor match in matches:\n  # Check whether the similarity score is greater than or equal to 80\n  if match[1] >=80:\n    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n    restaurants.loc[restaurants[\"type\"]==match[0],'type']='italian'","10364274":"# Iterate through categories\nfor cuisine in categories:  \n  # Create a list of matches, comparing cuisine with the cuisine_type column\n  matches = process.extract(cuisine, restaurants['type'], limit=len(restaurants.type))\n\n  # Iterate through the list of matches\n  for match in matches:\n     # Check whether the similarity score is greater than or equal to 80\n    if match[1] >= 80:\n      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n      restaurants.loc[restaurants['type'] == match[0]] = cuisine\n      \n# Inspect the final result\nprint(restaurants['type'].unique())","07523eb6":"pip install recordlinkage","779d31c2":"import recordlinkage\nrestaurants=pd.read_csv(\"..\/input\/datacleansing\/restaurantsNew.csv\")\nrestaurants_new=pd.read_csv(\"..\/input\/datacleansing\/resturantsNewPairing.csv\")\n\n# Create an indexer and object and find possible pairs\nindexer = recordlinkage.Index()\n\n# Block pairing on cuisine_type\nindexer.block(\"cuisine_type\")\n\n# Generate pairs\npairs = indexer.index(restaurants, restaurants_new)","817626bb":"# Create a comparison object\ncomp_cl = recordlinkage.Compare()\n\n# Find exact matches on city, cuisine_types - \ncomp_cl.exact('city', 'city', label='city')\ncomp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\n\n# Find similar matches of rest_name\ncomp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) \n\n# Get potential matches and print\npotential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\nprint(potential_matches)","e41be322":"# Isolate potential matches with row sum >=3\nmatches = potential_matches[potential_matches.sum(axis=1) >= 3]\n\n# Get values of second column index of matches\nmatching_indices = matches.index.get_level_values(1)\n\n# Subset restaurants_new based on non-duplicate values\nnon_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n\n# Append non_dup to restaurants\nfull_restaurants = restaurants.append(non_dup)\nprint(full_restaurants)","2b56eeea":"### Uniform currencies","74c57f39":"Hence above shows missing data is only for youn customers","074941ca":"### Treating duplicates","2ab4c5b2":"### Uniform dates","b9038fb6":"### keeping it descriptive","f9ce1845":"## Advanced data problems","53827d3c":"We have determined that the distance cutoff point for remapping typos of 'american', 'asian', and 'italian' cuisine types stored in the cuisine_type column should be 80.","45a6bb91":"### Removing titles and taking names","38becb59":"### Inconsistent categories","7a7fcdc8":"### Remapping categories","260df1d4":"### Remapping categories","7726babc":"## Common data problems\n### Data information and its structure","76679545":"### Summing strings and concatenating numbers","9d2e8ac5":"## Record linkage","971e9c50":"### Missing investors","b38dca23":"### Missing pattern","329396fe":"### Tire size constraint","09ca8651":"### Verify data integrity","01889d7a":"### Similarity using fuzzywuzzy","846f9036":"### Identify pairing in record linkage","47e994fb":"## Text and categorical data problems\n### finding consistency","99defd92":"### Finding duplicates"}}