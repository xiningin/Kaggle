{"cell_type":{"785c72c3":"code","bba609cb":"code","a84a9858":"code","c3d81b59":"code","e227c5bf":"code","a720398d":"code","a135a9a4":"code","82581e2b":"code","251bb987":"code","773492f1":"code","7e12f821":"code","16f7c269":"code","31fbf57d":"code","3d7486f1":"code","5659eaef":"code","c67ce7cf":"code","d166a090":"code","75016e7a":"code","21eb91ef":"code","3e86ccd6":"code","61f14a16":"code","40d087b0":"code","a23e01d7":"code","52fa3ee9":"code","d6cc0c50":"code","20022b4a":"code","7cee7d25":"code","31289e5f":"code","6518190a":"code","ce4850c3":"code","50b8e627":"code","fa66e6e9":"code","fa1da980":"code","15690e73":"code","ecff6b16":"code","a0cee6ec":"code","9277955a":"code","78ba1597":"code","15910477":"markdown","7346624e":"markdown","3f9c81a5":"markdown","60d54878":"markdown","34cbdeba":"markdown","57afffd8":"markdown","48cf44ab":"markdown","c3230199":"markdown","a7150583":"markdown","c6b03951":"markdown","5ec47717":"markdown","7f5ab3c1":"markdown","c7edf3a9":"markdown","7b880610":"markdown","3afb161a":"markdown","d77178e7":"markdown","2322f55e":"markdown","de12819e":"markdown","9a02488f":"markdown","03661d23":"markdown","933e12b8":"markdown","cc4ce2d5":"markdown","a623432a":"markdown","19415845":"markdown","c29126b5":"markdown","a03fea9e":"markdown","83c4bc3b":"markdown","54d5c11f":"markdown","800cd8ce":"markdown","a7fe266f":"markdown","2cb3d061":"markdown","09608948":"markdown","04c65dcc":"markdown","71d4e0ee":"markdown","9e792ac5":"markdown","a6134703":"markdown","e950e067":"markdown","075a9b03":"markdown","6b54f318":"markdown","68bb491a":"markdown"},"source":{"785c72c3":"import os\nimport json\nimport collections\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt","bba609cb":"PATH_BASE = \"..\/input\/herbarium-2021-fgvc8\/\"\nPATH_TRAIN = os.path.join(PATH_BASE, \"train\/\")\nPATH_TRAIN_META = os.path.join(PATH_TRAIN, \"metadata.json\")\n\n\nwith open(PATH_TRAIN_META) as json_file:\n    metadata = json.load(json_file)","a84a9858":"metadata.keys()","c3d81b59":"len(metadata[\"annotations\"]), len(metadata[\"images\"])","e227c5bf":"print(metadata[\"annotations\"][0])\nprint(metadata[\"images\"][0])\nprint(metadata[\"categories\"][0])\nprint(metadata[\"licenses\"][0])\nprint(metadata[\"institutions\"][0])","a720398d":"len(set([annotation[\"category_id\"] for annotation in metadata[\"annotations\"]]))","a135a9a4":"ids = []\ncategories = []\npaths = []\n\nfor annotation, image in zip(metadata[\"annotations\"], metadata[\"images\"]):\n    assert annotation[\"image_id\"] == image[\"id\"]\n    ids.append(image[\"id\"])\n    categories.append(annotation[\"category_id\"])\n    paths.append(image[\"file_name\"])\n        \ndf_meta = pd.DataFrame({\"id\": ids, \"category\": categories, \"path\": paths})","82581e2b":"df_meta","251bb987":"df_meta[\"category\"].value_counts()","773492f1":"d_categories = {category[\"id\"]: category[\"name\"] for category in metadata[\"categories\"]}\nd_families = {category[\"id\"]: category[\"family\"] for category in metadata[\"categories\"]}\nd_orders = {category[\"id\"]: category[\"order\"] for category in metadata[\"categories\"]}\n\ndf_meta[\"category_name\"] = df_meta[\"category\"].map(d_categories)\ndf_meta[\"family_name\"] = df_meta[\"category\"].map(d_families)\ndf_meta[\"order_name\"] = df_meta[\"category\"].map(d_orders)\ndf_meta","7e12f821":"def visualize_train_batch(paths, categories, families, orders):\n    plt.figure(figsize=(16, 16))\n    \n    for ind, info in enumerate(zip(paths, categories, families, orders)):\n        path, category, family, order = info\n        \n        plt.subplot(2, 3, ind + 1)\n        \n        image = cv2.imread(os.path.join(PATH_TRAIN, path))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        plt.imshow(image)\n        \n        plt.title(\n            f\"FAMILY: {family} ORDER: {order}\\n{category}\", \n            fontsize=10,\n        )\n        plt.axis(\"off\")\n    \n    plt.show()","16f7c269":"def visualize_by_id(df, _id=None):\n    tmp = df.sample(6)\n    if _id is not None:\n        tmp = df[df[\"category\"] == _id].sample(6)\n\n    visualize_train_batch(\n        tmp[\"path\"].tolist(), \n        tmp[\"category_name\"].tolist(),\n        tmp[\"family_name\"].tolist(),\n        tmp[\"order_name\"].tolist(),\n    )","31fbf57d":"visualize_by_id(df_meta, 22344)","3d7486f1":"visualize_by_id(df_meta, 42811)","5659eaef":"visualize_by_id(df_meta, 1719)","c67ce7cf":"visualize_by_id(df_meta, 1)","d166a090":"visualize_by_id(df_meta)","75016e7a":"df_submission = pd.read_csv(\n    \"..\/input\/herbarium-2021-fgvc8\/sample_submission.csv\",\n    index_col=0,\n)","21eb91ef":"df_submission[\"Predicted\"] = 25229","3e86ccd6":"df_submission.to_csv(\"submission.csv\")","61f14a16":"pd.read_csv(\"submission.csv\", index_col=0)","40d087b0":"FULL_PIPELINE = False","a23e01d7":"import os\nimport random\n\nimport numpy as np\nfrom numpy import save, load\nimport pandas as pd\nimport cv2\nimport albumentations as A\nfrom albumentations import pytorch as ATorch\nimport torch\nfrom torch.utils import data as torch_data\nfrom torch import nn as torch_nn\nfrom torch.nn import functional as torch_functional\nimport torchvision\nfrom tqdm import tqdm\nfrom sklearn.metrics.pairwise import euclidean_distances","52fa3ee9":"class MobileNetV2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        tmp_net = torch.hub.load(\n            \"pytorch\/vision:v0.6.0\", \"mobilenet_v2\", pretrained=True\n        )\n        self.net = torch_nn.Sequential(*(list(tmp_net.children())[:-1]))\n\n    def forward(self, x):\n        return self.net(x)","d6cc0c50":"class DataRetriever(torch_data.Dataset):\n    def __init__(\n        self, \n        paths, \n        categories=None,\n        transforms=None,\n        base_path=PATH_TRAIN\n    ):\n        self.paths = paths\n        self.categories = categories\n        self.transforms = transforms\n        self.base_path = base_path\n          \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, index):\n        img = cv2.imread(os.path.join(self.base_path, self.paths[index]))\n        img = cv2.resize(img, (224, 224))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n        \n        if self.categories is None:\n            return img\n        \n        y = self.categories[index] \n        return img, y\n    \n    \ndef get_transforms():\n    return A.Compose(\n        [\n            A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                p=1.0\n            ),\n            ATorch.transforms.ToTensorV2(p=1.0),\n        ], \n        p=1.0\n    )","20022b4a":"df_train = df_meta[[\"category\", \"path\"]].sort_values(by=\"category\")\n\ndf_train","7cee7d25":"tmp_path = df_train[\"path\"].tolist()\ntmp_category = df_train[\"category\"].tolist()\n# If FULL_PIPELINE is False we use small subset of data\nif not FULL_PIPELINE:\n    tmp_path = tmp_path[:256 * 8]\n    tmp_category = tmp_category[:256 * 8]\n\ntrain_data_retriever = DataRetriever(\n    tmp_path,\n    tmp_category,\n    transforms=get_transforms(),\n)\n\ntrain_loader = torch_data.DataLoader(\n    train_data_retriever,\n    batch_size=256,\n    shuffle=False,\n    num_workers=8,\n)","31289e5f":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = MobileNetV2()\nmodel.to(device)\nmodel.eval();","6518190a":"category_counts = collections.Counter(df_train[\"category\"].tolist())","ce4850c3":"final_vectors = np.zeros((len(category_counts), 1280))\n\nwith torch.no_grad():\n    for batch in tqdm(train_loader):\n        X, y = batch\n        vectors = model(X.to(device)).mean(axis=(2, 3))\n        \n        _y = y.numpy().tolist()\n        for ind in range(len(_y)):\n            final_vectors[_y[ind]] += vectors[ind].cpu().numpy().copy() \/ category_counts[_y[ind]]","50b8e627":"save(\"average_vectors.npy\", final_vectors)","fa66e6e9":"final_vectors = load(\"average_vectors.npy\")","fa1da980":"PATH_TEST = os.path.join(PATH_BASE, \"test\/\")\nPATH_TEST_META = os.path.join(PATH_TEST, \"metadata.json\")\n\n\nwith open(PATH_TEST_META) as json_file:\n    metadata = json.load(json_file)\n\n    \nid2path = {\n    img[\"id\"]: img[\"file_name\"] for img in metadata[\"images\"]\n}","15690e73":"df_submission = pd.read_csv(\n    \"..\/input\/herbarium-2021-fgvc8\/sample_submission.csv\",\n    index_col=0,\n)\n\ndf_submission[\"Id\"] = df_submission.index\ndf_submission[\"Path\"] = df_submission[\"Id\"].map(lambda x: id2path[x])","ecff6b16":"tmp_path = df_submission[\"Path\"].tolist()\n# If FULL_PIPELINE is False we use small subset of data\nif not FULL_PIPELINE:\n    tmp_path = tmp_path[:256 * 2]\n\ntest_data_retriever = DataRetriever(\n    tmp_path,\n    transforms=get_transforms(),\n    base_path=PATH_TEST,\n)\n\ntest_loader = torch_data.DataLoader(\n    test_data_retriever,\n    batch_size=256,\n    shuffle=False,\n    num_workers=8,\n)","a0cee6ec":"res = []\n\nwith torch.no_grad():\n    for ind, X in enumerate(tqdm(test_loader)):\n        vectors = model(X.to(device)).mean(axis=(2, 3))\n        tmp = euclidean_distances(vectors.cpu().numpy(), final_vectors)\n        res.extend(list(tmp.argmin(axis=1)))","9277955a":"df_submission.iloc[:len(res), 0] = res\n\ndf_submission[[\"Predicted\"]].to_csv(\"submission.csv\")\n\npd.read_csv(\"submission.csv\", index_col=0)","78ba1597":"PATH_PREPARED_SUBMISSION = \"..\/input\/herbarium-2021-submissions\/submission-mobilenetv2-mean.csv\"\n\nprepared_subnission = pd.read_csv(PATH_PREPARED_SUBMISSION, index_col=0)\nprepared_subnission.to_csv(\"prepared_submission.csv\")\n\npd.read_csv(\"prepared_submission.csv\", index_col=0)","15910477":"$$F_1 = 2\\frac{precision \\cdot recall}{precision+recall}$$","7346624e":"Submissions are evaluated using the [macro F1 score](#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html).","3f9c81a5":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:brown; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation<\/center><\/h3>\n\n* [Overview](#1)\n* [Data Visualization](#2)\n    \n    \n* [Competition Metric](#100)\n* [Sample Submission](#101)\n    \n    \n* [Modeling](#200)","60d54878":"The data has been approximately split 80%\/20% for training\/test. Each category has at least 1 instance in both the training and test datasets. Note that the test set distribution is slightly different from the training set distribution. The training set contains species with hundreds of examples, but the test set has the number of examples per species capped at a maximum of 10.","34cbdeba":"### Define your dataset class for getting image samples","57afffd8":"<a id=\"1\"><\/a>\n<h2 style='background:brown; border:0; color:white'><center>Overview<center><h2>","48cf44ab":"## Work In Progress...","c3230199":"$$recall = \\frac{TP}{TP+FN}$$","a7150583":"where:","c6b03951":"<a id=\"101\"><\/a>\n<h2 style='background:brown; border:0; color:white'><center>Sample Submission<center><h2>","5ec47717":"### The idea: Create for each category abstract vector from some model (MobileNetV2) and find nearest vector for each train sample","7f5ab3c1":"The training and test set contain images of herbarium specimens from nearly 65,000 species of vascular plants. Each image contains exactly one specimen. The text labels on the specimen images have been blurred to remove category information in the image.","c7edf3a9":"<a id=\"100\"><\/a>\n<h2 style='background:brown; border:0; color:white'><center>Competition Metric<center><h2>","7b880610":"<a id=\"200\"><\/a>\n<h2 style='background:brown; border:0; color:white'><center>Modeling<center><h2>","3afb161a":"In \"macro\" F1 a separate F1 score is calculated for each species value and then averaged.","d77178e7":"### Initialize the model","2322f55e":"### Save and load category vectors (you can pretrain them)","de12819e":"# Herbarium 2021: Half-Earth Challenge - FGVC8 - Exploratory Data Analysis\n\nQuick Exploratory Data Analysis for [Herbarium 2021: Half-Earth Challenge - FGVC8](https:\/\/www.kaggle.com\/c\/herbarium-2021-fgvc8) challenge    \n\nThe Herbarium 2021: Half-Earth Challenge is to identify vascular plant specimens provided by the [New York Botanical Garden (NY)](https:\/\/www.nybg.org\/), [Bishop Museum (BPBM)](https:\/\/www.bishopmuseum.org\/), [Naturalis Biodiversity Center (NL)](https:\/\/www.naturalis.nl\/en), [Queensland Herbarium (BRI)](https:\/\/www.qld.gov.au\/environment\/plants-animals\/plants\/herbarium), and [Auckland War Memorial Museum (AK)](https:\/\/www.aucklandmuseum.com\/).","9a02488f":"### Save results to submission file","03661d23":"$$precision = \\frac{TP}{TP+FP}$$","933e12b8":"### I prepared processed by the algorithm described above submission file for all data, you can use it for fast submission","cc4ce2d5":"### Get test paths","a623432a":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25558\/logos\/header.png)","19415845":"### Let's take for each category (target) all images from the train set and after processing average their vectors\n","c29126b5":"### Save output vectors from the model and average by category","a03fea9e":"### Random samples","83c4bc3b":"### First level elements","54d5c11f":"### Create DataFrame with main information","800cd8ce":"### Read the metadata file","a7fe266f":"### Find name and family of the classes by their ids","2cb3d061":"### Define the model\n\nYou can use any of the pretrained models, for example:\n- [PYTORCH HUB FOR RESEARCHERS](https:\/\/pytorch.org\/hub\/research-models)\n- [TORCHVISION.MODELS](https:\/\/pytorch.org\/vision\/stable\/models.html)","09608948":"<a id=\"2\"><\/a>\n<h2 style='background:brown; border:0; color:white'><center>Data Visualization<center><h2>","04c65dcc":"### Check the number of images and their annotations","71d4e0ee":"### One of the most frequently class from train data","9e792ac5":"### Import libraries","a6134703":"### Classes distribution","e950e067":"### Get test output vectors and find the nearest train vector (by euclidean distance) and take its category","075a9b03":"### Calculate the total number of classes","6b54f318":"### Check first samples from each key","68bb491a":"### Create test data loader"}}