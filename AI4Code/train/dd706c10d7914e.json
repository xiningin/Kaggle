{"cell_type":{"41b2aba3":"code","1daec845":"code","557f4279":"code","8da5448c":"code","40981fdf":"code","f1b2a525":"code","78717a35":"code","383043d3":"code","d5c5abc6":"code","c1f4325f":"code","d268739e":"code","e3ee0a8f":"code","9da2f5d5":"code","27aa33ae":"code","8aa4e1ac":"code","93ae4d87":"code","306d8357":"markdown","38524f03":"markdown","525dd972":"markdown","05f112dc":"markdown","cd1ebd8f":"markdown","a184cd21":"markdown","db44c38c":"markdown","c921d61a":"markdown","d3fcd225":"markdown","cc6f2f0f":"markdown","9f732aa6":"markdown","9e01a9a2":"markdown","a7fad6f6":"markdown","69397c14":"markdown","5653bf34":"markdown","9f08fd9f":"markdown","af452b18":"markdown","285fc8d0":"markdown","14ef1689":"markdown","59f50d02":"markdown","be2c0de1":"markdown"},"source":{"41b2aba3":"import nltk\nfrom nltk.corpus import movie_reviews\n# movie_reviews.readme()","1daec845":"raw = movie_reviews.raw()\nprint(raw[:3000])","557f4279":"print(raw[0])","8da5448c":"corpus = movie_reviews.words()\nprint(corpus)","40981fdf":"print(corpus[0])","f1b2a525":"freq_dist = nltk.FreqDist(corpus)\nprint(freq_dist)\nprint(freq_dist.most_common(50))\nfreq_dist.plot(50)","78717a35":"reviews = []\nfor i in range (0,len(movie_reviews.fileids())):\n    reviews.append(movie_reviews.raw(movie_reviews.fileids()[i]))","383043d3":"print(reviews[0])","d5c5abc6":"from nltk.tokenize import word_tokenize, sent_tokenize\nsentences = nltk.sent_tokenize(reviews[0])\nwords = nltk.word_tokenize(reviews[0])\nprint(sentences[0])\nprint(words[0])","c1f4325f":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\ntokens = tokenizer.tokenize(reviews[0].lower())\nprint(tokens)","d268739e":"from nltk.corpus import stopwords\ntokens = [token for token in tokens if token not in stopwords.words('english')]\nprint(tokens)","e3ee0a8f":"from nltk.stem import PorterStemmer, WordNetLemmatizer\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ntest_word = \"worrying\"\nword_stem = stemmer.stem(test_word)\nword_lemmatise = lemmatizer.lemmatize(test_word)\nword_lemmatise_verb = lemmatizer.lemmatize(test_word, pos=\"v\")\nword_lemmatise_adj = lemmatizer.lemmatize(test_word, pos=\"a\")\nprint(word_stem, word_lemmatise, word_lemmatise_verb, word_lemmatise_adj)","9da2f5d5":"# nltk.help.upenn_tagset()","27aa33ae":"from nltk import pos_tag\npos_tokens = nltk.pos_tag(tokens) \nprint(pos_tokens)","8aa4e1ac":"corpus_tokens = tokenizer.tokenize(raw.lower())\nvocab = sorted(set(corpus_tokens))","93ae4d87":"print(\"Tokens:\", len(corpus_tokens))\nprint(\"Vocabulary:\", len(vocab))","306d8357":"### Dataset\n\nTo perform natural language processing, we need some data containing natural language to work with. Often you can collect your own data for projects by scraping the web or downloading existing files in txt, csv, or json format. The NLTK library helpfully comes with a few large datasets built in and these are easy to import directly. These include classic novels, film scripts, tweets, speeches, and even real-life conversations overheard in New York. We will be using a set of movie reviews for our analysis. \n\nAfter importing a selected dataset, you can call a \u2018readme\u2019 function to learn more about the structure and purpose of the collected data. \n","38524f03":"We can calculate the frequency distribution (counting the number of times each word is used in the corpus), and then plot or print out the top k words. This should show us which words are used most often. Let\u2019s take a look at the top 50. ","525dd972":"As you will see, this outputs each of the reviews with no formatting at all. If we print just the first element, the output is a single character.","05f112dc":"### Part-of-Speech Tagging\n\nAs briefly mentioned, part-of-speech tagging refers to tagging a word with a grammatical category. To do this requires context on the sentence in which the word appears - for example, which words it is adjacent to, and how its definition could change depending on this. The following function can be called to view a list of all possible part-of-speech tags.","cd1ebd8f":"### Conclusion\n\nWe have been able to transform movie reviews from long raw text documents into something that a computer can begin to understand. These structured forms can be used for data analysis or as input into machine learning algorithms to determine topics discussed, analyse sentiment expressed, or infer meaning. \n\nYou can now go ahead and use some of the NLP concepts discussed to start working with text-based data in your own projects. The NLTK movie reviews data set is already categorised into positive and negative reviews, so see if you can use the tools from this tutorial to aid you in creating a machine learning classifier to predict the label of a given review. ","a184cd21":"### Introduction\n\nIn the broad field of artificial intelligence, the ability to parse and understand natural language is an important goal with many types of application. Online retailers and service providers may wish to analyse the feedback of their reviews; governments may need to understand the content of large-scale surveys and responses; or researchers may attempt to determine the sentiments expressed towards certain topics or people on social media. There are many areas where the processing of natural language is required, and this tutorial will step through some of the key elements involved. \n\nThe difficulty of understanding natural language is tied to the fact that text data is unstructured. This means it doesn\u2019t come in an easy format to analyse and interpret. When performing data analysis, we want to be able to evaluate the information quantitatively, but text is inherently qualitative. Natural language processing is the process of transforming a piece of text into a structured format that a computer can process and begin to understand.","db44c38c":"The extensive list includes PoS tags such as VB (verb in base form), VBD (verb in past tense), VBG (verb as present participle) and so on.\n\nTo generate the tags for our tokens, we simply import the library and call the pos_tag function.","c921d61a":"This has reduced the number of tokens in the first review from 726 to 343 - so we can see that nearly half the words in this instance were essentially redundant. ","d3fcd225":"### Vocabulary\n\nSo far we\u2019ve mostly been testing these NLP tools on the first individual review in our corpus, but in a real task we would be using all 2000 reviews. If this were the case, our corpus would generate a total of 1,336,782 word tokens after the tokenization step. But the resulting list is a collection of each use of a word, even if the same word is repeated multiple times. For instance the sentence \u201cHe walked and walked\u201d generates the tokens [\u2018he\u2019, \u2018walked\u2019, \u2018and\u2019, \u2018walked\u2019]. This is useful for counting frequencies and other such analysis, but we may want a data structure containing every unique word used in the corpus, for example [\u2018he\u2019, \u2018walked\u2019, \u2018and\u2019]. For this, we can use the Python set function. ","cc6f2f0f":"As you can see, the stemmer in this case has outputted the word \u2018worri\u2019 which is not a real word, but a stemmed version of \u2018worry\u2019. For the lemmatizer to correctly lemmatize the word \u2018worrying\u2019 it needs to know whether this word has been used as a verb or adjective. The process for assigning these contextual tags is called part-of-speech tagging and is explained in the following section. Although stemming is a less thorough approach compared to lemmatization, in practise it oftens perform equally or only negibly worse. You can test other words such as \u2018walking\u2019 and see that in this case the stemmer and lemmatizer would give the same result.  ","9f732aa6":"# An Overview of Natural Language Processing with Python NLTK","9e01a9a2":"We can start trying to understand the data by simply printing words and frequencies to the console, to see what we are dealing with. To get the entire collection of movie reviews as one chunk of data, we use the raw text function (though we will limit what we print to the terminal to the first 3000 characters for readability).","a7fad6f6":"The first line of our output is telling us how many total words are in our corpus (1,583,820 outcomes), and how many unique words this contains (39,768 samples). The next line then begins the list of the top 50 most frequently appearing words, and we can save our plotted graph as a .png file.\n\nWe can start to see that attempting analysis on the text in its natural format is not yielding useful results - the inclusion of punctuation and common words such as \u2018the\u2019 does not help us understand the content or meaning of the text. The next few sections will explain some of the most common and useful steps involved in NLP, which begin to transform the given text documents into something that can be analysed and utilised more effectively.","69397c14":"We can use the \u2018words\u2019 function to split our movie reviews into individual words, and store them all in one corpus. Now we can start to analyse things like individual words and their frequencies within the corpus.","5653bf34":"### Stop Words\n\nWe\u2019re starting to head towards a more concise representation of the reviews, which only holds important and useful parts of the text. One further key step in NLP is the removal of stop words, for example \u2018the\u2019, \u2018and\u2019, \u2018to\u2019, which add no value in terms of content or meaning and are used very frequently in almost all forms of text. To do this we can run our document against a predefined list of stop words and remove matching instances.","9f08fd9f":"The movie reviews are stored according to a file ID such as 'neg\/cv000_29416.txt', so this code loops through each of the file IDs, and assigning the raw text associated with that ID to an empty \u2018reviews\u2019 list. We will now be able to call, for example, review[0] to look at the first review by itself.","af452b18":"### Tokenization\n\nWhen we previously split our raw text document into individual words, this was a process very similar to the concept of tokenisation. Tokenisation aims to take a document and break it down into individual \u2018tokens\u2019 (often words), and store these in a new data structure. Other forms of minor formatting can be applied here too. For example, all punctuation could be removed.\n\nTo test this out on an individual review, first we will need to split our raw text document up, this time by review instead of by word.\n","285fc8d0":"There are lots of options for tokenizing in NLTK which you can read about in the API documentation [here](http:\/\/www.nltk.org\/api\/nltk.tokenize.html). We are going to combine a regular expression tokenizer to remove punctuation, along with the Python function for transforming strings to lowercase, to build our final tokenizer.","14ef1689":"### Lemmatization and Stemming\n\nOften different inflections of a word have the same general meaning (at least in terms of data analysis), and it may be useful to group them together as one. For example, instead of handling the words \u2018walk\u2019, \u2018walks\u2019, \u2018walked\u2019, \u2018walking\u2019 individually, we may want to treat them all as the same word. There are two common approaches to this - lemmatization and stemming.\n\nLemmatization takes any inflected form of a word and returns its base form - the lemma. To achieve this, we need some context to the word use, such as whether it is a noun or adjective. Stemming is a somewhat cruder attempt at generating a root form, often returning a word which is simply the first few characters that are consistent in any form of the word (but not always a real word itself). To understand this better, let's test an example.\n\nFirst we\u2019ll import and define a stemmer and lemmatizer - there are different versions available but these are two of the most popularly used. Next we\u2019ll test the resulting word generated by each approach.","59f50d02":"This gives us the vocabulary of a corpus, and we can use it to compare sizes of vocabulary in different texts, or percentages of the vocabulary which refer to a certain topic or are a certain PoS type. For example, whilst the total size of the corpus of reviews contains 1,336,782 words (after tokenization), the size of the vocabulary is 39,696. ","be2c0de1":"Now taking the first review in its natural form, we can break it down even further using tokenizers. The sent_tokenize function will split the review into tokens of sentences, and word_tokenize will split the review into tokens of words. "}}