{"cell_type":{"8ece6473":"code","feed7031":"code","2c76aa28":"code","8b9b0468":"code","1fd2c5a9":"code","7ab55ceb":"code","4af869f0":"code","0150b774":"code","462ec358":"code","d16314af":"code","ce785081":"code","21972e71":"code","815db7db":"code","f6586f02":"markdown","064f351f":"markdown","bffd9287":"markdown"},"source":{"8ece6473":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","feed7031":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir('..\/input'))","2c76aa28":"# Import Libraries\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset","8b9b0468":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset","1fd2c5a9":"train = pd.read_csv(r\"..\/input\/train.csv\",dtype = np.float32)\ntrain.head()","7ab55ceb":"# Prepare Dataset\n# load data\ntrain = pd.read_csv(r\"..\/input\/train.csv\",dtype = np.float32)\n\n# split data into features(pixels) and labels(numbers from 0 to 9)\ntargets_numpy = train.label.values   # values \ub97c \ud638\ucd9c\ud574\uc57c ndarray\ub97c \ubc18\ud658\ud568. \uc544\ub2c8\uba74 \uadf8\ub0e5 series \ub098 dataframe \ubc18\ud658. \nfeatures_numpy = train.loc[:,train.columns != \"label\"].values\/255 # normalization\n\n# train test split. Size of train data is 80% and size of test data is 20%. \nfeatures_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n                                                                             targets_numpy,\n                                                                             test_size = 0.2,\n                                                                             random_state = 42) \n\n# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n# \uc77c\ub2e8 ndarray\ub97c \ud150\uc11c\ub85c \ub9cc\ub4e4\uc790(torch.from_numpy) \n# feature\ub791 target\ubaa8\ub450 float\uc774 dtype\uc778\ub370, targets\ub294 \ubaa8\ub450 \uc815\uc218\uc774\ubbc0\ub85c type(torch.LongTensor)\ub97c \ud1b5\ud574 \uc815\uc218\ud0c0\uc785\uc73c\ub85c \ubc14\uafb8\uc5b4\uc900\ub2e4\nfeaturesTrain = torch.from_numpy(features_train)\ntargetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n\n# create feature and targets tensor for test set.\nfeaturesTest = torch.from_numpy(features_test)\ntargetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n\n# batch_size, epoch and iteration\n# \uc0ac\uc774\uc988 100\uc9dc\ub9ac \ubbf8\ub2c8\ubc30\uce58\ub97c 10000\ub9cc\ud07c \ub3cc\ub9ac\ub824\uba74 epochs\ub294 \ubc11\uc5d0 \ucc98\ub7fc \uacc4\uc0b0\ub418\uc5b4\uc57c \uaca0\uc9c0?\n# \uc989, \uc77c\ub2e8 n_iter\ubd80\ud130 \uc815\ud574\ub193\uc740 \uac83. \nbatch_size = 100\nn_iters = 10000\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\n# https:\/\/wikidocs.net\/55580 \ucc38\uc870\n# Dataset\uc73c\ub85c dataset\uc744 \ub9cc\ub4e4\uace0, dataloader \ub85c mini batch\ub85c \ub098\ub204\uc5b4\uc900\ub2e4. dataloader\uc758 \uad6c\uc131\uc694\uc18c \uac01\uac01\uc740 \ubbf8\ub2c8\ubc30\uce58(x, y) \uc774\ub2e4. \ntrain = TensorDataset(featuresTrain,targetsTrain)\ntest = TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n\n# visualize one of the images in data set\nplt.imshow(features_numpy[10].reshape(28,28)) # \ub2e4\uc2dc 255 \uc548\uacf1\ud574\uc900\ub2e4. \nplt.axis(\"off\")\nplt.title(str(targets_numpy[10]))\nplt.savefig('graph.png')\nplt.show()","4af869f0":"num_epochs = n_iters \/ (len(features_train) \/ batch_size)\nprint(n_iters, len(features_train), batch_size)","0150b774":"# Create RNN Model\n# RNN model \uc744 \ub9cc\ub4e4\uac74\ub370, \ud074\ub798\uc2a4\ud615\uc2dd\uc73c\ub85c \ub9cc\ub4e4\uac70\ub2e4. \n# \uc6b0\uc120 nn.Module\uc744 \uc0c1\uc18d\ubc1b\uc744\uac70\uace0, self.rnn \uc5d0\uc11c nn.RNN\uc744 \uc774\uc6a9\ud574 \ubaa8\ub378\uc744 \uad6c\uccb4\uc801\uc73c\ub85c \ub9cc\ub4e0\ub2e4.\n# \uc774\uac78 \uc65c \ud558\ub0d0\uba74, RNN\uc740 \uc544\uc8fc \ub2e4\uc591\ud55c \ubaa8\uc591\uc73c\ub85c \ud65c\uc6a9\uc774 \uac00\ub2a5\ud558\uae30 \ub54c\ubb38\uc5d0 \uc785\ub9db\uc5d0 \ub9de\uac8c \ub9cc\ub4e4\uae30 \uc704\ud574\uc11c\ub294 class\ub97c \ub530\ub85c \ub9cc\ub4dc\ub294\uac8c \uc720\uc6a9\ud558\ub2e4. \uc2dc\ubc8c\n\nclass RNNModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n# super \uc5d0 \ubc11\uacfc \uac19\uc774 \uc778\uc9c0\uac00 \ubd99\uc73c\uba74 RNNModel \ud074\ub798\uc2a4\uc758 \ubd80\ubaa8(nn.Module)\ub85c\ubd80\ud130 __init__\uc744 \uc0c1\uc18d\ubc1b\ub294\ub2e4. \n        super(RNNModel, self).__init__()\n        \n        # Number of hidden dimensions, hidden unit\uc758 \uc218 \n        self.hidden_dim = hidden_dim\n        \n        # Number of hidden layers, \ub2e8\uce35\uc778\uac00 \ud639\uc740 \uc5ec\ub7ec\uac1c\ub85c \uc313\uc740 RNN\uc778\uac00.. default \ub294 1.\n        self.layer_dim = layer_dim\n        \n        # RNN\n        # \uc778\ud48b\uc758 \ucc28\uc6d0\uacfc \uc544\uc6c3\ud48b\uc758 \ucc28\uc6d0(hidden_dim), layer\uc758 \uc218\ub97c \uc54c\ub824\uc8fc\uba74 \ubaa8\ub378 \ub9cc\ub4e4\uc5b4\uc900\ub2e4. \n        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n        \n        # Readout layer\n        # \ub9c8\uc9c0\ub9c9\uc5d0\ub294 timestep \ubcc4\ub85c \ub098\uc628 \ubca1\ud130\ub4e4(hidden_dim)\uc744 \uc120\ud615\uacb0\ud569\ud574\uc11c \ucd5c\uc885\uc801\uc778 output_dim \uc744 \uc0b0\ucd9c\ud55c\ub2e4. \uc989, \ub2e8\uce35\uc2e0\uacbd\ub9dd\uc744 \ud558\ub098 \ub9cc\ub4e0\ub2e4. \n        self.fc = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        \n        # Initialize hidden state with zeros\n        # x.size(0) \uc774 \ubc30\uce58 \uc0ac\uc774\uc988(batch_first = True \uc774\ubbc0\ub85c \uc774 \ubaa8\ub378\uc5d0\uc11c \uc778\ud48b\uacfc \uc544\uc6c3\ud48b\uc740 \uc544\ub798\uc640 \uac19\uc740 \ud615\ud0dc\ub85c \uc81c\uacf5.\n        # x.shape = (batch, seq_len, input_size)\n        # h0 \uc740 \ub9e8\ucc98\uc74c hidden state \ub85c\uc11c \ub2e8\uce35\uc774\ub77c\uba74 (1, batch_size, hidden unit \uc218) \uc758 \ud615\ud0dc\uc774\ub2e4. \n        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n            \n        # One time step\n        out, hn = self.rnn(x, h0)\n        # -1\uc778 \uc774\uc720\ub294 \ub9c8\uc9c0\ub9c9 timestep\uc5d0\uc11c\uc758 activation(hidden unit) \ub9cc \uc0ac\uc6a9\ud558\ub294 \uac83.\n        out = self.fc(out[:, -1, :]) \n        return out\n\n# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 8000\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\ntrain = TensorDataset(featuresTrain,targetsTrain)\ntest = TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n    \n# Create RNN\ninput_dim = 28    # input dimension\nhidden_dim = 100  # hidden layer dimension\nlayer_dim = 1     # number of hidden layers\noutput_dim = 10   # output dimension\n\nmodel = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n# Cross Entropy Loss \n# CrossEntropyLoss \ub294 \uadf8 \uc790\uccb4\ub85c softmax \uae30\ub2a5\uc774 \ud3ec\ud568\ub428.\nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.05\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","462ec358":"# torch.max(data, num)\ub97c \ud558\uba74 data \uc5d0\uc11c axis =num \ubc29\ud5a5\uc73c\ub85c \uac00\uc7a5 \ud070 \uac12\uc744 \ub9ac\ud134\ud558\uace0, \uadf8 \uac12\uc758 \ud589\uc778\ub371\uc2a4\ub97c \ub450\ubc88\uc9f8\ub85c \ubc18\ud658\ud55c\ub2e4. \na=  torch.randn(12,3)\nprint(a)\ntorch.max(a, 1)\n","d16314af":"#input_dim \uc740 input size\uc600\uace0, seq_dim \uc740 \uc2dc\ud000\uc2a4\uae38\uc774\nseq_dim = 28  \nloss_list = []\niteration_list = []\naccuracy_list = []\ncount = 0\n# train_loader \ub294 \ud558\ub098\uc758 \ubc30\uce58\ub97c \ubc18\ud658\ud558\uba70 \uadf8 \ubc30\uce58\ub294 x(image)\uc640 y(image)\ub97c \ud3ec\ud568\ud55c\ub2e4. \nfor epoch in range(num_epochs):\n    # \uc5ec\uae30\uc11c images\uc640 labels\ub294 '\ud558\ub098'\uc758 \ubc30\uce58\uc758 \ud53c\uccd0\uac12\uacfc \ub808\uc774\ube14\uac12\uc744 \uc758\ubbf8\ud568. \uc989, 100\uac1c \uc9dc\ub9ac \n    for i, (images, labels) in enumerate(train_loader):\n        # \uc624\uc789 \uc2e0\uae30\ud558\ub124. 28x28 \uc9dc\ub9ac \uc774\ubbf8\uc9c0\ub97c seq_dim = 28, input_dim = 28 \uc9dc\ub9ac RNN\uc5d0 \ub123\uc5b4\ubfcc\ub124\n        train  = Variable(images.view(-1, seq_dim, input_dim))\n        #print(train.shape) => (100,28,28) \uc774\ub2e4. \n        labels = Variable(labels)\n            \n        # Clear gradients\n        # \uc774\ubbf8 \ubaa8\ub378\uc774\ub791 \uc635\ud2f0\ub9c8\uc774\uc800\ub294 \uc815\uc758\ud574\ub1a8\uace0, \ub79c\ub364\uc73c\ub85c \ucd08\uae30\ud654\ub418\uc5c8\ub358 \uac00\uc911\uce58\uac12\ub4e4\uc744 0\uc73c\ub85c \ucd08\uae30\ud654\ud558\uace0 \uc2dc\uc791\ud55c\ub2e4. \n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        #print(outputs.shape) => (100,10) = (batch_size, output_dim)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        # iteration\uc774 250\uc758 \ubc30\uc218\uac00 \ub420 \ub54c\ub9c8\ub2e4 test set\uc5d0\uc11c\uc758 accuracy(\ubc30\uce58 \ub204\uc801)\uc744 \uad6c\ud560 \uac83.\n        # \uadf8\ub9ac\uace0 500\uc774 \ub418\uba74 print\n        if count % 250 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # RNN\uc744 \ud1b5\ud55c \uc774\ubbf8\uc9c0 \ubd84\uc11d\uc774 \ub0af\uc124\ub2e4. \n                # Variable \uc744 \ud1b5\ud574 \ubcc0\uc218\uc120\uc5b8\uc744 \ud588\ub294\ub370, \uc774\uc81c\ub294 tensor \ub85c \ubc14\ub85c \ubcc0\uc218\uc120\uc5b8 \ud558\ub294 \ub4ef. \n                images = Variable(images.view(-1, seq_dim, input_dim))\n                \n                # Forward propagation\n                outputs = model(images)\n                #print(outputs.shape)\n                \n                # Get predictions from the maximum value\n                # batch_size\uac00 100\uc774\ubbc0\ub85c (100,10) \uc774 \ubc18\ud658\ub418\ub294\ub370, \n                # \uac01 \ud589(axis=1)\uc5d0\uc11c \uac00\uc7a5 \ud070 \uac12\uc758 \uc778\ub371\uc2a4(=\uc608\uc0c1 \ubc88\ud638)\ub97c [1] \ud1b5\ud574 \ubc18\ud658\ud55c\ub2e4. \uc6d0\ub798\ub294 \uac12\uc790\uccb4\ub97c \ud3ec\ud568\ud574 2\uac1c\ub97c \ubc18\ud658. \n                predicted = torch.max(outputs.data, 1)[1]\n                #print(output.data.shape)\n                \n                # Total number of labels\n                # \uadf8\ub0e5 torch.size() \ub97c \ud558\uba74 torch.Size\uac00 \ubc18\ud658\ub418\ubbc0\ub85c 0\uc744 \ub123\uc5b4\uc8fc\uc5b4 \uc2e4\uc81c \uac12\uc744 \ubc18\ud658\ud55c\ub2e4. \n                total += labels.size(0)\n                # \uc774\uac74 test set \uc5d0\uc11c 1 iteration\uc5d0\uc11c\uc758 \ub204\uc801 accuracy \ub97c \uad6c\ud558\uae30 \uc704\ud574 \uc774\ub807\uac8c \ud568. \n                correct += (predicted == labels).sum()\n            # \uc774\uac74 \uacb0\uad6d \ubc30\uce58\ub97c 250 \ub2e8\uc704\ub85c \ud6c8\ub828\ud588\uc744 \ub54c, \uadf8 \uc2dc\uc810\uc5d0\uc11c test set\uc5d0\uc11c \ubc30\uce58\ub85c \ubaa8\ub378\uc744 \ub3cc\ub838\uc744 \ub54c, \ub204\uc801 accuracy\n            # correct \ub294 float \uc774\ubbc0\ub85c int type \uc778 total \ub3c4 float\uc73c\ub85c \ubc14\uafb8\uc5b4\uc8fc\uc5b4\uc57c \uc5f0\uc0b0\uc774 \uac00\ub2a5. \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            # \uadf8\ub0e5 loss\uac00 \uc544\ub2c8\ub77c .data \ub97c \ud574\uc11c \ub9ac\uc2a4\ud2b8\ub85c \ub118\uae30\ub124\n            # \ubcc0\uc218\ub294 \uc774\ub7f0\uc2dd\uc73c\ub85c \ub370\uc774\ud130 \ub118\uae40.\n            loss_list.append(loss.data)  \n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n            if count % 500 == 0:\n                # Print Loss\n                print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data[0], accuracy))","ce785081":"seq_dim = 28\nloss_list= []\niteration_list = []\naccuracy_list = []\n\ncount = 0\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        train = Variable(images.view(-1, seq_dim, input_dim))\n        labels = Variable(labels)\n        \n        optimizer.zero_grad()\n        outputs = model(train)\n        \n        loss = error(outputs, labels)\n        loss.backward()\n        \n        optimizer.step()\n        \n        count += 1\n        \n        if count % 250 == 0 :\n            correct = 0\n            total = 0\n            for images, labels in test_loader:\n                images = Variable(images.view(-1, seq_dim, input_dim))\n                outputs = model(images)\n                \n                predicted = torch.max(outputs.data, 1)[1]\n                total += labels.size(0)\n                correct += (predicted == labels).sum()\n            accuracy = 100 * correct \/ float(total)\n            \n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n            \n            if count % 500 == 0:\n                print('iterations: {}, loss = {}, accuracy = {} %'.format(count, loss, accuracy))\n                \n                \n                \n                                ","21972e71":"# \uacb0\uacfc\uac00 \uc774\ub530\uad6c\ucc98\ub7fc \ubcf4\uc774\ub294\uac74 \uc0ac\uc2e4 \ub0b4\uac00 \uc704\uc5d0 \uc140\uc744 \uc5ec\ub7ec\ubc88 \ub3cc\ub824\uc11c \uadf8\ub7f0\uac70\n# \uc654\ub2e4\uac14\ub2e4\ud558\ub294 \uac83\ucc98\ub7fc \ubcf4\uc774\uc9c0\ub9cc loss\ub294 \ub0ae\uace0, acc \ub294 \ub192\ub2e4. \n# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"RNN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"RNN: Accuracy vs Number of iteration\")\nplt.savefig('graph.png')\nplt.show()","815db7db":"plt.plot(iteration_list, loss_list)\nplt.xlabel('num of iteration')\nplt.ylabel('Loss')\nplt.title('RNN: Loss for iteration')\nplt.show()\n\nplt.plot(iteration_list, accuracy_list, color = 'red')\nplt.xlabel('num of iteration')\nplt.ylabel('accuracy')\nplt.title(\"RNN: accuracy for iteration\")\nplt.savefig('graph.png')\nplt.show()","f6586f02":"<a id=\"1\"><\/a> <br>\n### Recurrent Neural Network (RNN)\n- RNN is essentially repeating ANN but information get pass through from previous non-linear activation function output.\n- **Steps of RNN:**\n    1. Import Libraries\n    1. Prepare Dataset\n    1. Create RNN Model\n        - hidden layer dimension is 100\n        - number of hidden layer is 1 \n    1. Instantiate Model\n    1. Instantiate Loss\n        - Cross entropy loss\n        - It also has softmax(logistic function) in it.\n    1. Instantiate Optimizer\n        - SGD Optimizer\n    1. Traning the Model\n    1. Prediction","064f351f":"### Conclusion\nIn this tutorial, we learn: \n1. Basics of pytorch\n1. Linear regression with pytorch\n1. Logistic regression with pytorch\n1. Artificial neural network with with pytorch\n1. Convolutional neural network with pytorch\n    - https:\/\/www.kaggle.com\/kanncaa1\/pytorch-tutorial-for-deep-learning-lovers\/code\n1. Recurrent neural network with pytorch\n1. Long-Short Term Memory (LSTM)\n    - https:\/\/www.kaggle.com\/kanncaa1\/long-short-term-memory-with-pytorch\n\n<br> If you have any question or suggest, I will be happy to hear it ","bffd9287":"## INTRODUCTION\n- It\u2019s a Python based scientific computing package targeted at two sets of audiences:\n    - A replacement for NumPy to use the power of GPUs\n    - Deep learning research platform that provides maximum flexibility and speed\n- pros: \n    - Iinteractively debugging PyTorch. Many users who have used both frameworks would argue that makes pytorch significantly easier to debug and visualize.\n    - Clean support for dynamic graphs\n    - Organizational backing from Facebook\n    - Blend of high level and low level APIs\n- cons:\n    - Much less mature than alternatives\n    - Limited references \/ resources outside of the official documentation\n- I accept you know neural network basics. If you do not know check my tutorial. Because I will not explain neural network concepts detailed, I only explain how to use pytorch for neural network\n- Neural Network tutorial: https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners \n- The most important parts of this tutorial from matrices to ANN. If you learn these parts very well, implementing remaining parts like CNN or RNN will be very easy. \n<br>\n<br>**Content:**\n1. Basics of Pytorch, Linear Regression, Logistic Regression, Artificial Neural Network (ANN), Concolutional Neural Network (CNN)\n    - https:\/\/www.kaggle.com\/kanncaa1\/pytorch-tutorial-for-deep-learning-lovers\/code\n1. [Recurrent Neural Network (RNN)](#1)\n1. Long-Short Term Memory (LSTM)\n    - https:\/\/www.kaggle.com\/kanncaa1\/long-short-term-memory-with-pytorch"}}