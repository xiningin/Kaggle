{"cell_type":{"b0443c9b":"code","48e7109e":"code","4d05cc25":"code","e95dc7b9":"code","4103581a":"code","88e704c6":"code","168737f0":"code","47dc34f1":"markdown","d5eb8460":"markdown","22d19f2f":"markdown","41cb8eba":"markdown","1aaa04d9":"markdown","1715a430":"markdown","6cae6a84":"markdown"},"source":{"b0443c9b":"import numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport os\nimport pickle\n\nfor i in os.listdir(\"..\/input\"):\n    try:\n        data = pd.read_pickle('..\/input\/'+i)\n    except Exception as e:\n        print(i, \"Error loading file\", repr(e))\n\nx_train = data[\"x_train\"]\ny_train = data[\"y_train\"]\nx_test = data[\"x_test\"]\ny_test = data[\"y_test\"]","48e7109e":"from tensorflow import keras\nprint(keras.__version__)\nfrom keras_preprocessing.sequence import pad_sequences\n# Needed to partiotion the data because of time out.\n#partition = round(len(x_train)\/2)\npartition = 300000\npre_processed_train_x = pad_sequences(x_train[:partition], maxlen=1000)\npre_processed_test_x = pad_sequences(x_test[:partition], maxlen=1000)\n","4d05cc25":"input_length = pre_processed_train_x.shape[1]\n\n\n'''\nCheck shape dimensions\naval=-1\nfor train_array in pre_processed_train_x:\n    for contender in train_array:\n        if contender > aval:\n            aval=contender\nprint(aval)\n'''\nprint(\"max:\", 1000, \"min:\", min(pre_processed_train_x[2]))\nprint(\"input_length:\", input_length)\ninput_dim = 1000 - min(pre_processed_train_x[2])\nprint(\"input_dim:\", input_dim)","e95dc7b9":"from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n\n#### OVERRIDE ###\nbatch_size = 128\noutput_dim = 256\nmax_features = data[\"vocab_size\"]\n\nmodel = keras.Sequential()\nmodel.add(Embedding(input_dim=max_features, output_dim=output_dim, input_length=input_length))\nmodel.add(LSTM(batch_size))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(optimizer = 'rmsprop',\n             loss = 'binary_crossentropy',\n             metrics=['accuracy'])\n\nprint(model.summary())\n","4103581a":"# NB small batch sizes take a long time; batch_size = 512\nwith tf.device('\/GPU:0'):\n    history = model.fit(pre_processed_train_x, y_train[:partition], epochs=10, batch_size=256)\n    \n# save the model\nmodel.save(\"lstm_model.h5\")\n\n","88e704c6":"val_loss, val_acc = model.evaluate(pre_processed_test_x, y_test[:partition])\n\n","168737f0":"import matplotlib.pyplot as plt\n# Plot training & validation accuracy values\nplt.plot(model.history.history['acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.show()\n\nplt.plot(model.history.history['loss'], 'orange')\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(model.history.history['acc'])\nplt.plot(model.history.history['loss'], 'orange')\n\nplt.title('Loss & Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Accuracy', 'Loss'], loc='upper left')\nplt.show()\nprint(\"Loss: \", val_loss, \"Accuracy:\", val_acc)","47dc34f1":"### Importing the Data","d5eb8460":"### Pre Processing the Data","22d19f2f":"### Defining the Neural Net","41cb8eba":"### Learning the classifier","1aaa04d9":"### Train the model","1715a430":"## Implementation Part 2 - Using _keras_","6cae6a84":"### Evaluation"}}