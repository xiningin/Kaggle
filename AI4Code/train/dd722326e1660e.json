{"cell_type":{"1b11b906":"code","da6f84e4":"code","8f8ea665":"code","c6ad96fd":"code","b45b8021":"code","99b746dc":"code","781f7c59":"code","05bbdbd6":"code","6f327964":"code","d0918601":"markdown","9fa858d5":"markdown","fdb9f932":"markdown","04f431f7":"markdown","ff808ef9":"markdown"},"source":{"1b11b906":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt","da6f84e4":"from keras.datasets import boston_housing\n\n(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()","8f8ea665":"# take a look at the data\n\nprint(f'Training data : {train_data.shape}')\nprint(f'Test data : {test_data.shape}')\nprint(f'Training sample : {train_data[0]}')\nprint(f'Training target sample : {train_targets[0]}')","c6ad96fd":"mean = train_data.mean(axis=0)\ntrain_data -= mean\nstd = train_data.std(axis=0)\ntrain_data \/= std\n\ntest_data -= mean\ntest_data \/= std\n\n# Note that the quantities used for normalizing the test data are computed using the\n# training data. You should never use in your workflow any quantity computed on the\n# test data, even for something as simple as data normalization.","b45b8021":"from keras import models\nfrom keras import layers\n\ndef build_model():\n    model = models.Sequential()\n    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(1))\n\n    model.compile(optimizer='rmsprop',\n              loss='mse',\n              metrics=['mae'])\n    return model","99b746dc":"k = 4\nnum_val_samples = len(train_data) \/\/ k\nnum_epochs = 100\nall_scores = []\n\nfor i in range(k):\n    print(f'Processing fold # {i}')\n    val_data = train_data[i * num_val_samples: (i+1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i+1) * num_val_samples]\n    \n    partial_train_data = np.concatenate(\n                            [train_data[:i * num_val_samples],\n                            train_data[(i+1) * num_val_samples:]],\n                            axis=0)\n    partial_train_targets = np.concatenate(\n                            [train_targets[:i * num_val_samples],\n                            train_targets[(i+1)*num_val_samples:]],\n                            axis=0)\n    model = build_model()\n    model.fit(partial_train_data,\n              partial_train_targets,\n              epochs=num_epochs,\n              batch_size=1,\n              verbose=0)\n    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n    all_scores.append(val_mae)","781f7c59":"print(f'all_scores : {all_scores}')\nprint(f'mean all scores : {np.mean(all_scores)}')","05bbdbd6":"model = build_model()\nmodel.fit(train_data, train_targets, epochs=80, batch_size=16, verbose=0)\ntest_mse_score, test_mae_score = model.evaluate(test_data, test_targets)","6f327964":"test_mae_score","d0918601":"## K-fold validation","9fa858d5":"## Preparing the data\n\nWe are going to do a feature normalization . Feature normalizaion is when you subtract the mean of the feature from each feature and divide each result by the standard deviation.\n","fdb9f932":"**This is an example from Deep Learning With Python book.**","04f431f7":"## Loading the dataset","ff808ef9":"## Building the network"}}