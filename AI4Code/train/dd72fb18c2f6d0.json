{"cell_type":{"988b3abc":"code","41d09ec1":"code","9f777a51":"code","5ae591cc":"code","3d4e243d":"code","5e4d5ea0":"code","9a55146c":"code","1c95285f":"code","4347cc0f":"code","67dac78b":"code","a19af42d":"code","1d235d49":"code","7a1364c6":"code","6f123e11":"code","07a51f26":"code","9b34efe0":"code","aae8efb6":"code","ea7b8329":"code","228a1db1":"code","b8a79032":"markdown","52ae909b":"markdown","0055a42b":"markdown","ba022b7a":"markdown","3acf1a9f":"markdown","ba43468f":"markdown","ec1ee000":"markdown","3bd2de5b":"markdown","cd8f2548":"markdown","1820059f":"markdown","013f30ba":"markdown","6d68b11d":"markdown","2f1ba949":"markdown","76c3af5c":"markdown","be4fd3d9":"markdown","a32fdab4":"markdown"},"source":{"988b3abc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings\nwarnings.filterwarnings('ignore') \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","41d09ec1":"data = pd.read_csv('\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv')\ndata3 = pd.read_csv('\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_3C_weka.csv')","9f777a51":"data.head()","5ae591cc":"data.describe()","3d4e243d":"type(data)","5e4d5ea0":"data.isnull().sum()","9a55146c":"def discrete_univariate(dataset, discrete_feature):\n    fig, axarr=plt.subplots(nrows=1,ncols=2, figsize=(8,5))\n      \n    dataset[discrete_feature].value_counts().plot(kind=\"bar\",ax=axarr[0])\n    dataset[discrete_feature].value_counts().plot.pie(autopct=\"%1.1f%%\",ax=axarr[1])\n        \n    plt.tight_layout()\n    plt.show()","1c95285f":"discrete_univariate(dataset=data , discrete_feature=\"class\")","4347cc0f":"discrete_univariate(dataset=data3, discrete_feature=\"class\")","67dac78b":"sns.pairplot(data ,hue =\"class\",palette=\"husl\")\nplt.show()","a19af42d":"#%%  Normal =1  Abnormal =0\ndata['class'] = [1 if each == \"Normal\" else 0 for each in data['class']]\n","1d235d49":"\ny = data.loc[:,'class']\n\nx1 = data.loc[:,data.columns != 'class']","7a1364c6":"x = (x1 - np.min(x1))\/(np.max(x1)-np.min(x1))","6f123e11":"\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2 ,random_state=1)\n","07a51f26":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x_train)\n\nX_train = scaler.transform(x_train)\nX_test = scaler.transform(x_test)","9b34efe0":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 23) \nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(23,knn.score(x_test,y_test)))\n","aae8efb6":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","ea7b8329":"error=[]\nfor i in range(1,40):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    pred_i = knn.predict(x_test)\n    error.append(np.mean(pred_i != y_test)) \n    \n    \n    ","228a1db1":"plt.figure(figsize=(20, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')\nplt.show()","b8a79032":"## Training and Predictions","52ae909b":"## Importing the Dataset","0055a42b":"## Plotting Data\n","ba022b7a":"## Evaluating the Algorithm\nFor evaluating an algorithm, confusion matrix, precision, recall and f1 score are the most commonly used metrics. The confusion_matrix and classification_report methods of the sklearn.metrics can be used to calculate these metrics.","3acf1a9f":"* There is no missing value ,so we dont need to perform on it .","ba43468f":"## Train Test Split","ec1ee000":"The above script executes a loop from 1 to 40. In each iteration the mean error for predicted values of test set is calculated and the result is appended to the error list.\n\nThe next step is to plot the error values against K values. Execute the following script to create the plot:","3bd2de5b":"From the output we can see that the mean error is closest to zero when the value of the K is 20 ,21 .","cd8f2548":"## Preprocessing","1820059f":"The output graph looks like this:","013f30ba":"## Feature Scaling\nBefore making any actual predictions, it is always a good practice to scale the features so that all of them can be uniformly evaluated","6d68b11d":"The results show that our KNN algorithm was able to classify all the 62 records in the test set with 81% accuracy, which is well enough. ","2f1ba949":"# KNN Algorithms Tutorial\n1. Importing Libraries\n1. Importing the Dataset\n1. Plotting Data\n1. Preprocessing\n1. Train Test Split\n1. Feature Scaling\n1. Training and Predictions\n1. Evaluating the Algorithm\n1. Comparing Error Rate with the K Value","76c3af5c":"## Importing Libraries\n","be4fd3d9":"Normalization for better understand ","a32fdab4":"## Comparing Error Rate with the K Value\n* One way to help you find the best value of K is to plot the graph of K value and the corresponding error rate for the dataset.\n* In this section, we will plot the mean error for the predicted values of test set for all the K values between 1 and 40."}}