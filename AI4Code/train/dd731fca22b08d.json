{"cell_type":{"0edfc460":"code","5a967107":"code","56bd63bc":"code","de204ac4":"code","1298ab1e":"code","96097aaf":"code","70490081":"code","cd89c783":"code","586f1b02":"code","6de0462e":"code","cf1c1941":"code","a5c237ba":"code","ea84792f":"code","9c179df9":"code","b9408aba":"code","5918e3a4":"code","fc96ad2f":"code","413dbcb0":"code","8c743098":"code","83cb0fb3":"code","10a79fd3":"markdown","54f2affc":"markdown","6fd038ff":"markdown","8e86795f":"markdown","043f59de":"markdown","10b590ba":"markdown","6bf79aa3":"markdown","1996731a":"markdown"},"source":{"0edfc460":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5a967107":"!pip install transformers\n!pip install --upgrade transformers\n!pip install simpletransformers","56bd63bc":"#A package for easing return of multiple values\n!ln -sf \/opt\/bin\/nvidia-smi \/usr\/bin\/nvidia-smi","de204ac4":"#GPUtil is a Python module for getting the GPU status from NVIDA GPUs using nvidia-smi.\n!pip install gputil\n\n#Cross-platform lib for process and system monitoring in Python.\n!pip install psutil","1298ab1e":"#importing other necessary packages and ClassificationModel for bert\nfrom tqdm import tqdm\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom simpletransformers.classification import ClassificationModel\nfrom sklearn.preprocessing import LabelEncoder\n\nimport torch\nfrom scipy.special import softmax","96097aaf":"train = pd.read_csv('..\/input\/train-full\/train.csv')\ntest = pd.read_csv('..\/input\/independence-data-av\/test.csv')\nsample_sub = pd.read_csv('..\/input\/independence-data-av\/sample_submission.csv')","70490081":"train_copy = pd.read_csv('..\/input\/train-full\/train.csv')\ntrain_copy.head()","cd89c783":"train[\"text\"] = train[\"TITLE\"] + train[\"ABSTRACT\"]\ntest[\"text\"] = test[\"TITLE\"] + test[\"ABSTRACT\"]","586f1b02":"!pip install clean-text[gpl]\nfrom cleantext import clean\ndef text_cleaning(text):\n    text=clean(text,\n    fix_unicode=True,               # fix various unicode errors\n    to_ascii=True,                  # transliterate to closest ASCII representation\n    lower=True,                     # lowercase text\n    no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n    no_urls=True,                  # replace all URLs with a special token\n    no_emails=True,                # replace all email addresses with a special token\n    no_phone_numbers=True,         # replace all phone numbers with a special token\n    no_numbers=True,               # replace all numbers with a special token\n    no_digits=True,                # replace all digits with a special token\n    no_currency_symbols=True,      # replace all currency symbols with a special token\n    no_punct=True,                 # fully remove punctuation\n    replace_with_url=\"<URL>\",\n    replace_with_email=\"<EMAIL>\",\n    replace_with_phone_number=\"<PHONE>\",\n    replace_with_number=\"<NUMBER>\",\n    replace_with_digit=\"0\",\n    replace_with_currency_symbol=\"<CUR>\",\n    lang=\"en\"                       # set to 'de' for German special handling\n    )\n    return text","6de0462e":"for i in range(len(train)):\n    train['text'].iloc[i]=text_cleaning(train['text'].iloc[i])\n    \nfor i in range(len(test)):\n    test['text'].iloc[i]=text_cleaning(test['text'].iloc[i])    ","cf1c1941":"train['text'].iloc[0]","a5c237ba":"test['text'].iloc[0]","ea84792f":"target_classes = [\"Computer Science\" ,\"Physics\" , \"Mathematics\", \"Statistics\" , \"Quantitative Biology\" , \"Quantitative Finance\"]\ntrain['label'] = train[target_classes].values.tolist()\n\nle = LabelEncoder()\ntrain['label'] = le.fit_transform(train['label'].astype(str))\ntrain = train[[\"text\",\"label\"]]\n\ntest = test[[\"text\"]]\n#initialising test labels\ntest[\"label\"] = 1","9c179df9":"model = ClassificationModel('bert', 'bert-base-uncased', use_cuda=True,num_labels=24, args={'train_batch_size':32,\n                                                                                             'reprocess_input_data': True,\n                                                                                             'overwrite_output_dir': True,\n                                                                                             'fp16': False,\n                                                                                             'do_lower_case': False,\n                                                                                             'num_train_epochs': 2,\n                                                                                             'max_seq_length': 256,\n                                                                                             'regression': False,\n                                                                                             'manual_seed': 2,\n                                                                                             \"learning_rate\":4e-5,\n                                                                                             'weight_decay':0.0,\n                                                                                             \"save_eval_checkpoints\": False,\n                                                                                             \"save_model_every_epoch\": False,\n                                                                                             \"silent\": False})\n\nmodel.train_model(train)","b9408aba":"test_result, test_model_outputs, test_wrong_predictions = model.eval_model(test)","5918e3a4":"predictions = softmax(test_model_outputs,axis=1)\nfinal_pred = [np.argmax(x) for x in predictions]","fc96ad2f":"final_pred","413dbcb0":"sub1=sample_sub.copy()\nsub1['target'] = le.inverse_transform(final_pred)\nfrom ast import literal_eval\nsub1.loc[:,'target'] = sub1.loc[:,'target'].apply(lambda x: literal_eval(x))\nsub1[target_classes] = pd.DataFrame(sub1.target.tolist(), index= sub1.index)\nsub1.drop(\"target\",axis=1,inplace = True)","8c743098":"sub1","83cb0fb3":"sub1.to_csv('sub_new1.csv',index = False)","10a79fd3":"### Cleaning text using clean-text","54f2affc":"### Get the evaluations from training bert","6fd038ff":"## Using bert for the task","8e86795f":"### Processing and converting integer classes back to string classes","043f59de":"### Using LabelEncoder to convert string classes into integers","10b590ba":"## Problem - Multi-class classification\n\n### We have title and abstract of various projects, classified into\n\n1)Computer Science\t\n2)Physics\t\n3)Mathematics\t\n4)Statistics\t\n5)Quantitative Biology\t\n6)Quantitative Finance","6bf79aa3":"### Installing transformers and upgrading it so that it is compatible with simpletransformers.\n### Then, installing simpletransformers.","1996731a":"### Running the ClassificationModel and training"}}