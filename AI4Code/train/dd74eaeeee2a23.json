{"cell_type":{"21246c84":"code","08a570d6":"code","b42bdb6b":"code","be83e8bb":"code","c718c246":"code","0d5d71fd":"code","5ee686d1":"code","28c40d3b":"code","aa8d62c6":"code","e655737a":"code","97f4b910":"code","1db558e7":"code","02f84739":"code","48bcb835":"markdown","e6a6a838":"markdown","88852756":"markdown","4acbda0e":"markdown","f7cd9a0a":"markdown","42ba58c4":"markdown","d985d8ab":"markdown","088c5013":"markdown","514c28a0":"markdown","fc309a4d":"markdown","944b8c56":"markdown","835f06a9":"markdown","0bfb9a23":"markdown"},"source":{"21246c84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings \nwarnings.filterwarnings('ignore')\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","08a570d6":"x_1 = np.load('..\/input\/X.npy')\ny_1 = np.load('..\/input\/Y.npy')\nimg_size = 64 # our pixel size.\n\nplt.subplot(1,2,1)   # subplot : sign zero\nplt.imshow(x_1[205]) # show image at specified index number\nplt.axis('off')      # don't show the axis of sign zero\nplt.subplot(1,2,2)   # subplot : sign one\nplt.imshow(x_1[822]) # I didn't remove the axis since we can see sizes of our image.","b42bdb6b":"x = np.concatenate((x_1[204:409], x_1[822:1027]), axis=0) # concatenate two array along axis 0 (horizontally)\n# Now we create an appropriate array for outputs consisting zeros & ones.\nz = np.zeros(205)\no = np.ones(205)\ny = np.concatenate((z,o), axis=0).reshape(x.shape[0], 1) # concatenate 'z' & 'o' and make it a 2D array\n                                                         # (first it was a 1D array with shape (410, ) )\n# let's see the shapes of input and output\nprint('x shape: {}'.format(x.shape))\nprint('y shape: {}'.format(y.shape))","be83e8bb":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 42)\nprint(x_train.shape)","c718c246":"x_train_new = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\nx_test_new = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\nprint('x_train new : {}\\nx_test_new : {}'.format(x_train_new.shape, x_test_new.shape))","0d5d71fd":"x_train = x_train_new.T\nx_test = x_test_new.T\ny_train = y_train.T\ny_test = y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","5ee686d1":"def initialize_params(dimension):\n    w = np.full((dimension, 1), 0.01) # first value of weights are 0.01\n    b = 0                             # first value of bias is zero\n    return w, b                       # return the necessary values that'll be used later in order to finish functions everytime\n\ndef sigmoid(z):\n    y_head = 1\/ (1+ np.exp(-z))\n    return y_head","28c40d3b":"# Forward & Backward Propagation\ndef forward_backward_propagation(w, b, x_train, y_train):\n    # Forward Propagation\n    z = np.dot(w.T, x_train) + b                                     # z function consisting parameters w, b \n    y_head = sigmoid(z)                                              # get the probability through sigmoid fucntion\n    loss = -(1 - y_train)*np.log(1 - y_head) - y_train*np.log(y_head)# the formula of loss function\n    cost = (np.sum(loss)) \/ (x_train.shape[1])                       # cost function : sum of the loss function of every image\n    \n    # Backward Propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # derivative of cost function with respect to 'w'\n    derivative_bias = np.sum(y_head-y_train) \/ x_train.shape[1]                 # derivative of cost function with respect to 'b'\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias} # creating a dictionary to store w & b values\n    \n    return cost,gradients   # return cost and gradients which will be used later\n\n","aa8d62c6":"# Updating Parameters\ndef update(w, b, x_train, y_train, learning_rate, number_of_iterarion):\n\n    costs = [] # that'll be used for visualizate cost with respect to iteration count\n    index = [] # same here as one row above\n\n    for i in range(number_of_iterarion):    # do it as iteration count\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)  # return updated cost and gradients every iteration by calling function.\n     \n        w = w - learning_rate * gradients[\"derivative_weight\"]              # update weights\n        b = b - learning_rate * gradients[\"derivative_bias\"]                # update bias\n        if i%10 == 0:                                            # in every ten iterations\n            costs.append(cost)                                   # store cost in list(costs)\n            index.append(i)                                      # store iteration number in the list : index\n            print (\"cost at iteration {} : {}\".format(i, cost))\n    \n    parameters = {\"w\": w,\"b\": b}  # most optimized weights & bias is stored in 'parameters' dictionary\n    plt.plot(index,costs)         # plotting index vs costs\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters # return parameters which will be used later\n","e655737a":"# Predicting with Test Data\ndef predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T, x_test) + b)   # new \"y_head\" function full of probabilistic values of x_test data\n    y_pred = np.zeros((1,x_test.shape[1])) # form a matrix full of zeros so as to change later\n    for i in range(x_test.shape[1]):       # for every values of \"y_head\"\n        if z[0,i] <= 0.5:                  # if y_head <= 0.5 which is our threshold,\n            y_pred[0,i] = 0                # then predict it as sign zero.\n        else:\n            y_pred[0,i] = 1                # else if it's greater than 0.5, then let it be sign one.\n\n    return y_pred                          # return the matrix of predicted values y_pred","97f4b910":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, iterations):\n    \n    dimension =  x_train.shape[0]      # set the dimension on the same value as images\n    w,b = initialize_params(dimension) # return w & b\n\n    parameters = update(w, b, x_train, y_train, learning_rate, iterations)  # forward & backward propagation : return brand new updated params\n    \n    y_pred_test  = predict(parameters[\"w\"],parameters[\"b\"], x_test)  # overfit check if it occured an overfitting situation, then the accuracy of-\n    y_pred_train = predict(parameters[\"w\"],parameters[\"b\"], x_train) # test values would be significantly low as opposed to accuracy of train values.      \n\n    print(\"train accuracy: {}\".format(100 - np.mean(np.abs(y_pred_train - y_train)) * 100)) # train accuracy\n    print(\"test accuracy:  {}\".format(100 - np.mean(np.abs(y_pred_test - y_test)) * 100))   # test acuracy - no overfitting or underfitting\n    \nlogistic_regression(x_train, y_train, x_test, y_test, learning_rate = 0.02, iterations = 200) # perform logistic regression classification\n","1db558e7":"# Grid Search\n\nfrom sklearn.model_selection import GridSearchCV    # import Grid Search Cross Validation to find optimujm parameters\nfrom sklearn.linear_model import LogisticRegression # import Logistic Regression\nlr = LogisticRegression()  \n\nparameters = {'C':np.logspace(-5,5,11), 'penalty':['l1', 'l2']} # create a dictionary  within Logistic Regression parameters inside\nlr_cv = GridSearchCV(lr, parameters, cv = 10)                   # method, parameters of that method, count of Cross Validation.\nlr_cv.fit(x_train.T, y_train.T)                                 # fit the model for our values\n\nprint('tuned hyperparameters : {}'.format(lr_cv.best_params_))  # Now we'll se best parameters among the \"parameters\"\nprint('best score: {}'.format(lr_cv.best_score_))               # Best score of the Logistic Regression with best parameters","02f84739":"# Logistic Regression\n\nlr2 = LogisticRegression(C=1.0, penalty = 'l2') # Use the parameters within Logistic Regression\nlr2.fit(x_train.T, y_train.T)                   # fit the model for our train values\n\nprint('score for test values: {}'.format(lr2.score(x_test.T, y_test.T))) # test score","48bcb835":"* Our main purpose in logistic regression to set the parameters w and b to reduce the **cost function**.\n* Cost function is the sum of losses for every image.\n* So to reduce the losses, so cost, we need to update our parameters by using **Gradient Descent Method**. \n* For every values of parameters w and b, there is a corresponding cost values on the curve.\n* After forward propagation which is until we lost cost function as seen in the code below, we take gradients of weights and bias, and update them with the formulas;\n> \nLearning rate is an hyperparameters tuned by us, different values must be tried to find the minimum parameters, so it's always good to start with a small learning rate. But if it's too small, it'll affect the speed of a model; vice versa the minimum can be missed and never be reached again.\n       b = b - learning_rate*(gradient of b)\n       w = w - learning_rate*(gradient of w)\nBy the way these formulas are coming from the **derivative of cost function with respect to 'w' & 'b'**. You can check it if you want to confirm.\n","e6a6a838":"* Our parameters are \"w\" and \"b\" standing for \"Weights\" and \"Bias\"\n* Our main function at first step is : **z = w.T * x + b**\n* T stands for transpose of matrix 'w' which is in shape (4096, 1)  here. Which means there are weights corresponding to every pixel or sample of an image\/class.\n* Then, we need to put our function \"z\" into the sigmoid fucntion to calculate its probability of result being 0 or 1 with respect to the threshold we'll determine. \n>     y_head = sigmoid(z) = 1 \/ ( 1 + e^(-z) )\n \n","88852756":"* Our data can be called as \"numbers in sign language\" having images of numbers in sign language from 0 to 9.\n* Since we'll do a \"logistic regression classification\", we'll use only 2 outputs which are sign 0 and sign 1.\n* Our sign images do have a size of 64x64 (64x64 pixels)\n* Sign 0 is between indices 204 & 408 having totally 205 samples. Sign 1 is between indices 822 & 1027 having 206 samples. But we'll use 205 each of them.\n* Know that, 205 samples are really insufficient for a deep learning data, but that's kinda tutorial for me, for any of you reading this kernel.\n* Let's get it started by importing and showing our samples below!","4acbda0e":"* We have 3D arrays of x_train & x_test. So in order to be able to use it in our model, we need to turn them into 2D arrays.","f7cd9a0a":"**Initializing Parameters \"w\" & \"b\"**\n* Let the first values of parameters be 0.01 and 0 respectively. And the dimensions of parameters have to be qualified for dot product with matrix \"x\". As it's been told before, count of weights have to be the same as pixels (4096). \n","42ba58c4":"* **Deep learning** : A machine learning technique learning features directly from data.\n* This means we don't need any feature extraction as opposed to machine learning.\n* Deep learning has been started using, since the machine learning algorithms couldn't handle big amounts of data like having millions of samples in terms of performance.\n\n* **Usage of fileds** : Speech recognition, image classification, natural language processing, recommendation systems etc.\n\n* **Differences between machine learning & deep learning** \n* In machine learning, we have to give the features of an output manually. As regard to deep learning, it learns and classify the features itself from data when we get data into a neural network.\nConsider some images of dogs. In machine learning we've got to give feature samples manually and imply feature extraction. Then we imply a proper classification technique to the data for its telling us if it's a dog or not.\nBut in deep learning, we just give a data full of dog images to our model and this model makes above machine learning requirements in itself.","d985d8ab":"* Let's split our x & y for training and testing. I choose %90 for training data and rest for testing data.\n* So our **test_size = 0.9**\n* **random_state** can be anything but the **'None'**. Each number that random_state equals determines a specific 'seed' in data, so everytime we run the same train & test data reserved by user. ","088c5013":"* Now it's time to implement slicing to 'y_1' to seperate sign zero & sign one. ","514c28a0":"* It's a binary classification algorithm in which there are just 2 different outputs we can label them as 0 and 1. That's why in this kernel we'll only use 2 different signs that are sign zero and sign one.\n* Logistic regression can be considered as the simplest neural network.\n","fc309a4d":"**OVERVIEW THE DATA SET**","944b8c56":"**INTRODUCTION**","835f06a9":"**LOGISTIC REGRESSION WITH SCIKIT-LEARN**\n* Altough the logistic regression has the similar mentality with deep learning, it's a machine learning classification algorithm. So there is a library we can import and tune our parameters for best accuracy after **Grid Search**.","0bfb9a23":"**LOGISTIC REGRESSION**"}}