{"cell_type":{"7b853e9a":"code","6b6d5d8f":"code","cc824eae":"code","9a2ee5f3":"code","4081f7d9":"code","ba66933b":"code","af99fd6a":"code","1c365e7d":"code","c84d3fc5":"code","7077c2ce":"code","6e59bd97":"markdown","df32e946":"markdown","31847a01":"markdown","101c7f7e":"markdown","5c363ec9":"markdown","9905cc3d":"markdown","3eeb3906":"markdown","6b7b163d":"markdown","384d4c4e":"markdown"},"source":{"7b853e9a":"pip install google   # This Library used to auto search contents from google search on a topic and its not pre-installed at kaggle.","6b6d5d8f":"# Let's import all necessary libraries.\nimport nltk\nimport requests\nimport bs4\nimport wordcloud\nimport googlesearch\nimport PIL\nimport numpy as np\nimport pandas as pd","cc824eae":"# Here this webscrap function returns the content of any website and showscarp function print this content in a proper UI. \n\ndef webscrap(keyword,url_index=1):\n    url_range = url_index\n    # Search the keyword and take \"url_index\" number site.\n    url = [i for i,j in zip(googlesearch.search(keyword),range(url_range))][url_index-1] \n    # Scraping the whole content of that site.\n    content = bs4.BeautifulSoup(requests.get(url).content).text    \n    return content\n\ndef showscrap(keyword,url_index=1):\n    print(webscrap(keyword,url_index))\n\n    \n# Lets scrap an article from https:\/\/towardsdatascience.com\/\nshowscrap('Data Science and Machine learning as a career towardsdatascience') ","9a2ee5f3":"# Scrapping Contents from towardsdatascience\ncontents = webscrap('Data Science and Machine learning as a career towardsdatascience')  \n# mask for circular pattern\nmask = np.array(PIL.Image.open('..\/input\/wcmimg1123\/phploeBuh.png')) \n# Initialize the WordCloud by that mask with background of white.\nwc = wordcloud.WordCloud(background_color='white',mask=mask)   \n# Lets visualize the WordCloud on the contents scrapped from towardsdatascience article from web.\nwc.generate(contents).to_image()        ","4081f7d9":"# Sentence Tokenization\n# This technique split the text on \".\"\n# Scrapping Contents from towardsdatascience\ncontents = webscrap('Data Science and Machine learning as a career towardsdatascience')  \n# Returns a list of all sentences contains in the contents\nsent_tokens = nltk.sent_tokenize(contents)      \nprint('Number of sentences :',len(sent_tokens))\nsent_tokens","ba66933b":"# Word Tokenization\n# This technique split the text on \" \"\n# Lets take the first sentance from the contents in towardsdatascience article\n# This will returns a list of words in the sentance.\nword_tokens = nltk.word_tokenize(sent_tokens[0]) \nprint('Number of Words :',len(word_tokens))\nword_tokens","af99fd6a":"# Stemming translates the words into its small versions but this may not meaningful.\nstemmed_words = []\n# Take the word list for the example that tokenized just before a cell \nfor word in word_tokens:   \n    # remove all the english stopwords from the list. Stopwords means all the unnecessary words which is very commmon like \"I\", \"we\", \"is\", \"the\", etc.\n    if word not in nltk.corpus.stopwords.words() :    \n        # Lets stemming the words and store tor a new list\n        stemmed_words.append(nltk.stem.PorterStemmer().stem(word))\n        \n# Join all the words in the form of sentance.\n' '.join(stemmed_words)                ","1c365e7d":"# Lemmatization also translates the words into its small versions but this is meaningful.\nlemmatized_words = []\n# Take the same source for example\nfor word in word_tokens: \n    if word not in nltk.corpus.stopwords.words() :\n        # Lets lemmetize the words and store tor a new list\n        lemmatized_words.append(nltk.stem.WordNetLemmatizer().lemmatize(word))  \n' '.join(lemmatized_words)","c84d3fc5":"# Preprocessing for Bag of Words\nimport re # This re library used remove noises from the text or corpus\nlemmet = nltk.stem.WordNetLemmatizer()\ntext = webscrap('Data Science and Machine learning as a career towardsdatascience')\nsentances = text.split('.')\nnew_sentences = []\nfor sentence in sentances:\n    # Lets remove the noises from the sentances\n    new_sent = re.sub('[^a-z]',' ',sentence.lower()) \n    # lemmetization of words \n    new_sentences.append(' '.join([lemmet.lemmatize(word) for word in new_sent.split() if not word in nltk.corpus.stopwords.words()]))  \n\n# Bag of Words\n# Bag of words counts the frequency of the words appeared in sentences and creates the features on it.\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Initialize bag of words by max features of 200 means it creates features top 200 words on appearence. \nbow = CountVectorizer(max_features=200)     \n# returns a sparse matrix as output after calculations\noutput = bow.fit_transform(new_sentences)    \nprint('BOW output Shape :',output.shape)\n# To display all the columns in the data frame\npd.set_option('display.max_columns',None) \n# get_feature_names() returns the words respect to the features\noutput_df = pd.DataFrame(output.toarray(),columns=bow.get_feature_names())  \noutput_df.head()","7077c2ce":"# TF-IDF Stands for Term Frequency an Inverse Document Frequency.\n\n# Preprocessing for TF-IDF\nimport re                        # This re library used remove noises from the text or corpus\nlemmet = nltk.stem.WordNetLemmatizer()\ntext = webscrap('Data Science and Machine learning as a career towardsdatascience')\nsentances = text.split('.')\nnew_sentences = []\nfor sentence in sentances:\n    # Lets remove the noises from the sentances\n    new_sent = re.sub('[^a-z]',' ',sentence.lower())       \n    # lemmetization of words \n    new_sentences.append(' '.join([lemmet.lemmatize(word) for word in new_sent.split() if not word in nltk.corpus.stopwords.words()]))    \n\n\n# TF-IDF\n# TF-IDF calculates the importance of words and create features on this.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Initialize TF-IDF by max features of 200 means it creates features top 200 words on importances. \ntfidf = TfidfVectorizer(max_features=200)         \n# returns a sparse matrix as output after calculations\noutput = tfidf.fit_transform(new_sentences)     \nprint('TF-IDF output Shape :',output.shape)\noutput_df = pd.DataFrame(output.toarray(),columns=tfidf.get_feature_names())\noutput_df.head()","6e59bd97":"## **Word Cloud**","df32e946":"# <center> **NLTK, Web Scraping, WordCloud, an Word Vectorization : Get started to NLP**","31847a01":"# <center> **Thank You**","101c7f7e":"## **NLTK - Tokenization**","5c363ec9":"## **Word Vectorization Technique 1 : Bag of Words or Count Vectorization**","9905cc3d":"## **Stemming**","3eeb3906":"## **Word Vectorization Technique 2 : TF-IDF Vectorization**","6b7b163d":"## **Lemmatization**","384d4c4e":"## **Web Scraping**"}}