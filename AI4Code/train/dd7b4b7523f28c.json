{"cell_type":{"164cefc3":"code","2162f4ba":"code","4916a24e":"code","291dfc50":"code","264f79aa":"code","f94c6722":"code","85911649":"code","b37360d1":"code","4547a4d3":"code","743303c4":"code","e7ebc46d":"code","025fdb14":"markdown","9125653a":"markdown"},"source":{"164cefc3":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport time\nimport pandas as pd","2162f4ba":"# Defining paths to the weights and configuration file with model of Neural Network\nweights_path = '..\/input\/yolo-coco-data\/yolov3.weights'\nconfiguration_path = '..\/input\/yolo-coco-data\/yolov3.cfg'\n#\u95be\u5024\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u8a2d\u5b9a\uff08\u306a\u306b\u304c\u9055\u3046\u304b\u308f\u304b\u3063\u3066\u306a\u3044\u3067\u3059\uff0e\uff09\nprobability_minimum = 0.5\nthreshold = 0.3\nlabels = open('..\/input\/yolo-coco-data\/coco.names').read().strip().split('\\n')  # \u691c\u51fa\u3067\u304d\u308b\u30e9\u30d9\u30eb\nnetwork = cv2.dnn.readNetFromDarknet(configuration_path, weights_path)#\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8aad\u307f\u8fbc\u307f\nlayers_names_all = network.getLayerNames()  # list of layers' names\nlayers_names_output = [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]  # list of layers' names","4916a24e":"def yolo_detection(path):\n    #class\u3067\u66f8\u3044\u305f\u307b\u3046\u304c\u3044\u3044\u304b\u3082\u3067\u3059\u304c\u4e00\u65e6\u3053\u308c\u3067\u3044\u304d\u307e\u3059\uff0e\n    image_input = cv2.imread(path)# Image\u304b\u3089blob\u306b\u5909\u63db\u3059\u308b\n    image_input_shape = image_input.shape# Getting image shape\n    blob = cv2.dnn.blobFromImage(image_input, 1 \/ 255.0, (416, 416), swapRB=True, crop=False)#blob\u3068\u3044\u3046\u578b\u306b\u5909\u63db\u3059\u308b\u307f\u305f\u3044\n    # Slicing blob and transposing to make channels come at the end\n    blob_to_show = blob[0, :, :, :].transpose(1, 2, 0)\n    # Calculating at the same time, needed time for forward pass\n    network.setInput(blob)  # setting blob as input to the network\n    output_from_network = network.forward(layers_names_output)\n\n    np.random.seed(42)\n    colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n    bounding_boxes = []\n    confidences = []\n    class_numbers = []\n    h, w = image_input_shape[:2]  # Slicing from tuple only first two elements\n\n\n    for result in output_from_network:\n        # Going through all detections from current output layer\n        for detection in result:\n            # Getting class for current object\n            scores = detection[5:]\n            class_current = np.argmax(scores)\n\n            # Getting confidence (probability) for current object\n            confidence_current = scores[class_current]\n\n            # Eliminating weak predictions by minimum probability\n            if confidence_current > probability_minimum:\n                # Scaling bounding box coordinates to the initial image size\n                # YOLO data format keeps center of detected box and its width and height\n                # That is why we can just elementwise multiply them to the width and height of the image\n                box_current = detection[0:4] * np.array([w, h, w, h])\n\n                # From current box with YOLO format getting top left corner coordinates\n                # that are x_min and y_min\n                x_center, y_center, box_width, box_height = box_current.astype('int')\n                x_min = int(x_center - (box_width \/ 2))\n                y_min = int(y_center - (box_height \/ 2))\n\n                # Adding results into prepared lists\n                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n                confidences.append(float(confidence_current))\n                class_numbers.append(class_current)\n\n    # It is needed to make sure the data type of the boxes is 'int'\n    # and the type of the confidences is 'float'\n    # https:\/\/github.com\/opencv\/opencv\/issues\/12789\n    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n\n    # Check point\n    # Showing labels of the detected objects\n    obj_name = []\n    for i in range(len(class_numbers)):\n        obj_name.append(labels[int(class_numbers[i])])\n    return obj_name,bounding_boxes,confidences,class_numbers,results,colours,image_input","291dfc50":"from pathlib import Path\nfrom tqdm import tqdm\np = Path(\"..\/input\/data-science-autumn-2021\/train\/train\")\npic_names = []\nobj_names = []\nc = 0\nfor path in tqdm(p.glob(\"*\")):\n    pic_names.append(path.stem)\n    obj_name,bounding_boxes,confidences,class_numbers,results,colours,image_input = yolo_detection(path.as_posix())\n    obj_names.append(obj_name)\n    c += 1\n#     if c > 3:\n#         break","264f79aa":"import pandas as pd\ndf = pd.DataFrame(obj_names)\ndf[\"pic_name\"] = pic_names\ndf[\"pic_name\"] = df[\"pic_name\"].astype('str').str.zfill(4)\ndf = df.sort_values(by = \"pic_name\")","f94c6722":"df.to_csv(\"train_yolo.csv\")","85911649":"p = Path(\"..\/input\/data-science-autumn-2021\/test\/test\")\npic_names = []\nobj_names = []\nc = 0\nfor path in tqdm(p.glob(\"*\")):\n    pic_names.append(path.stem)\n    obj_name,bounding_boxes,confidences,class_numbers,results,colours,image_input = yolo_detection(path.as_posix())\n    obj_names.append(obj_name)\n    c += 1\n#     if c > 4:\n#         break","b37360d1":"df = pd.DataFrame(obj_names)\ndf[\"pic_name\"] = pic_names\ndf[\"pic_name\"] = df[\"pic_name\"].astype('str').str.zfill(4)\ndf = df.sort_values(by = \"pic_name\")\ndf.to_csv(\"test_yolo.csv\")","4547a4d3":"\npic_names = []\nobj_names = []\npath = \"..\/input\/data-science-autumn-2021\/test\/test\/1001.jpg\"\nobj_name,bounding_boxes,confidences,class_numbers,results,colours,image_input = yolo_detection(path)\nobj_names.append(obj_name)\n","743303c4":"# Checking if there is at least one detected object\nif len(results) > 0:\n    # Going through indexes of results\n    for i in results.flatten():\n        # Getting current bounding box coordinates\n        x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n        box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n\n        # Preparing colour for current bounding box\n        colour_box_current = [int(j) for j in colours[class_numbers[i]]]\n\n        # Drawing bounding box on the original image\n        cv2.rectangle(image_input, (x_min, y_min), (x_min + box_width, y_min + box_height),\n                      colour_box_current, 2)\n\n        # Preparing text with label and confidence for current bounding box\n        text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])], confidences[i])\n\n        # Putting text with label and confidence on the original image\n        cv2.putText(image_input, text_box_current, (x_min, y_min - 7), cv2.FONT_HERSHEY_SIMPLEX,\n                    0.5, colour_box_current, 2)","e7ebc46d":"#\u7d50\u679c\u3000\u3042\u3093\u307e\u308a\u5bb6\u3068\u95a2\u4fc2\u306a\u3044\u304b\u3082\u3067\u3059\u304c\u4e00\u5fdc\u753b\u50cf\u304b\u3089\u306a\u3093\u3089\u304b\u306e\u60c5\u5831\u304c\u306c\u304d\u3068\u308c\u307e\u3057\u305f\u3002\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.imshow(cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB))\nplt.show()","025fdb14":"https:\/\/www.kaggle.com\/valentynsichkar\/yolo-v3-with-opencv","9125653a":" end-to-end\u306e\u4e88\u6e2c(\u753b\u50cf\u304b\u3089\u76f4\u63a5\u4fa1\u683c\u4e88\u6e2c\u306e\u4e88\u6e2c) \u306f\u5c11\u3057\u30cf\u30fc\u30c9\u30eb\u9ad8\u3044\u306e\u3067\uff0cyolo\u3067\u691c\u51fa\u3067\u304d\u308b\u3082\u306e\u3092\u3068\u308a\u3042\u3048\u305a\u307f\u3066\u307f\u307e\u3057\u3087\u3046\uff0e\uff08\u753b\u50cf\u7cfb\u521d\u5fc3\u8005\u306a\u306e\u3067\u304a\u8a31\u3057\u304f\u3060\u3055\u3044\uff0e\uff09\n \n \u6700\u5f8c\u306b\u53ef\u8996\u5316\u3057\u3066\u3044\u307e\u3059\u3002\u30b3\u30fc\u30c9\u8a73\u3057\u3044\u5185\u5bb9\u306f\u79c1\u3082\u308f\u304b\u3063\u3066\u3044\u306a\u3044\u90e8\u5206\u304c\u3042\u308b\u306e\u3067\n \u6700\u4f4e\u9650\u52d5\u3051\u3070\u3044\u3044\u304b\u306a\u3068\u3044\u3046\u611f\u3058\u3067\u3059\u3002\n \u7d50\u679c\u306fcsv\u30d5\u30a1\u30a4\u30eb\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u6d3b\u7528\u4e0b\u3055\u3044\uff0e"}}