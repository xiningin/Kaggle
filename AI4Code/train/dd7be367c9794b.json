{"cell_type":{"c41d30f7":"code","c85d28dc":"code","dc44fd10":"code","c14f92c3":"code","d8025c6d":"code","f3fed9f6":"code","58206ede":"code","7ab5aed5":"code","bcf94d4c":"code","aba111f7":"code","31acad86":"code","a2e444cf":"code","c85ae240":"code","ee78c9e9":"code","f9d829a7":"code","406f85b7":"code","4cdf5cb7":"code","3e1edf51":"code","c4d2a665":"code","97b895fe":"code","0633d84a":"code","4334d852":"code","a9185075":"code","c9beba29":"code","d16e650a":"code","bf078703":"code","d8284cd6":"code","ce364b64":"code","77648996":"code","f202f201":"code","b65a5ac2":"code","eb33f952":"code","2c0b6e71":"code","4549e6a2":"markdown","56861b43":"markdown","48665c82":"markdown","fdb0ad43":"markdown","66b9ef2e":"markdown","b6436222":"markdown","91d58b77":"markdown","c414696a":"markdown","cf780235":"markdown","b67bc913":"markdown","2a749274":"markdown","536a4a37":"markdown","0860e16a":"markdown","35098bff":"markdown","589a58f4":"markdown","ee629a31":"markdown","2b5cd4a9":"markdown"},"source":{"c41d30f7":"!pip install pyspark","c85d28dc":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType\nfrom pyspark.sql.functions import split, count, when, isnan, col, regexp_replace\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","dc44fd10":"spark = SparkSession.builder.appName('First Session').getOrCreate()\n\nprint('Spark Version: {}'.format(spark.version))","c14f92c3":"#Defining a Schema\nschema = StructType([StructField('mpg', FloatType(), nullable = True),\n                     StructField('cylinders', IntegerType(), nullable = True),\n                     StructField('displacement', FloatType(), nullable = True),\n                     StructField('horsepower', StringType(), nullable = True),\n                     StructField('weight', IntegerType(), nullable = True),\n                     StructField('acceleration', FloatType(), nullable = True),\n                     StructField('model year', IntegerType(), nullable = True),\n                     StructField('origin', IntegerType(), nullable = True),\n                     StructField('car name', StringType(), nullable = True)])\n\nfile_path = '\/kaggle\/input\/autompg-dataset\/auto-mpg.csv'\n\ndf = spark.read.csv(file_path,\n                    header = True,\n                    inferSchema = True,\n                    nanValue = '?')\n\ndf.show(5)","d8025c6d":"#Check Missing Values\ndef check_missing(dataframe):\n    \n    return dataframe.select([count(when(isnan(c) | col(c).isNull(), c)). \\\n                             alias(c) for c in dataframe.columns]).show()\n\ncheck_missing(df)","f3fed9f6":"#Handling Missing Values\ndf = df.na.drop()\n\ndf = df.withColumn(\"horsepower\", df[\"horsepower\"].cast(IntegerType())) #convert horsepower from string to int\n\ndf.show(5)","58206ede":"#Check column names\ndf.columns","7ab5aed5":"#Display data with pandas format\ndf.toPandas().head()","bcf94d4c":"#Check the schema\ndf.printSchema()","aba111f7":"#Renaming Columns\ndf = df.withColumnRenamed('model year', 'model_year')\n\ndf = df.withColumnRenamed('car name', 'car_name')\n\ndf.show(3)","31acad86":"#Get infos from first 4 rows\nfor car in df.head(4):\n    print(car, '\\n')","a2e444cf":"#statistical summary of dataframe\ndf.describe().show()","c85ae240":"#describe with specific variables\ndf.describe(['mpg', 'horsepower']).show()","ee78c9e9":"#describe with numerical columns\ndef get_num_cols(dataframe):\n    \n    num_cols = [col for col in dataframe.columns if dataframe.select(col). \\\n                dtypes[0][1] in ['double', 'int']]\n    \n    return num_cols\n\nnum_cols = get_num_cols(df)\n    \ndf.describe(num_cols).show()","f9d829a7":"#Lets get the cars with mpg more than 23\ndf.filter(df['mpg'] > 23).show(5)","406f85b7":"#Multiple Conditions\ndf.filter((df['horsepower'] > 80) & \n          (df['weight'] > 2000)).select('car_name').show(5)","4cdf5cb7":"#Sorting\ndf.filter((df['mpg'] > 25) & (df['origin'] == 2)). \\\norderBy('mpg', ascending = False).show(5)","3e1edf51":"#Get the cars with 'volkswagen' in their names, and sort them by model year and horsepower\ndf.filter(df['car_name'].contains('volkswagen')). \\\norderBy(['model_year', 'horsepower'], ascending=[False, False]).show(5)","c4d2a665":"df.filter(df['car_name'].like('%volkswagen%')).show(3)","97b895fe":"#Get the cars with 'toyota' in their names\ndf.filter(\"car_name like '%toyota%'\").show(5)","0633d84a":"df.filter('mpg > 22').show(5)","4334d852":"#Multiple Conditions\ndf.filter('mpg > 22 and acceleration < 15').show(5)","a9185075":"df.filter('horsepower == 88 and weight between 2600 and 3000') \\\n.select(['horsepower', 'weight', 'car_name']).show()","c9beba29":"#Brands\ndf.createOrReplaceTempView('auto_mpg')\n\ndf = df.withColumn('brand', split(df['car_name'], ' ').getItem(0)).drop('car_name')\n\n#Replacing Misspelled Brands\nauto_misspelled = {'chevroelt': 'chevrolet',\n                   'chevy': 'chevrolet',\n                   'vokswagen': 'volkswagen',\n                   'vw': 'volkswagen',\n                   'hi': 'harvester',\n                   'maxda': 'mazda',\n                   'toyouta': 'toyota',\n                   'mercedes-benz': 'mercedes'}\n\nfor key in auto_misspelled.keys():\n    \n    df = df.withColumn('brand', regexp_replace('brand', key, auto_misspelled[key]))\n\ndf.show(5)","d16e650a":"#Avg Acceleration by car brands\ndf.groupBy('brand').agg({'acceleration': 'mean'}).show(5)","bf078703":"#Max MPG by car brands\ndf.groupBy('brand').agg({'mpg': 'max'}).show(5)","d8284cd6":"#Check brand frequences first\ndf.groupby('brand').count().orderBy('count', ascending = False).show(5)","ce364b64":"def one_hot_encoder(dataframe, col):\n    \n    indexed = StringIndexer().setInputCol(col).setOutputCol(col + '_cat'). \\\n    fit(dataframe).transform(dataframe) #converting categorical values into category indices\n    \n    ohe = OneHotEncoder().setInputCol(col + '_cat').setOutputCol(col + '_OneHotEncoded'). \\\n    fit(indexed).transform(indexed)\n    \n    ohe = ohe.drop(*[col, col + '_cat'])\n    \n    return ohe\n\ndf = one_hot_encoder(df, col = 'brand')\ndf.show(5)","77648996":"#Vector Assembler\ndef vector_assembler(dataframe, indep_cols):\n    \n    assembler = VectorAssembler(inputCols = indep_cols,\n                                outputCol = 'features')\n\n    output = assembler.transform(dataframe).drop(*indep_cols)\n    \n    return output\n\ndf = vector_assembler(df, indep_cols = df.drop('mpg').columns)\ndf.show(5)","f202f201":"train_data, test_data = df.randomSplit([0.8, 0.2])\n\nprint('Train Shape: ({}, {})'.format(train_data.count(), len(train_data.columns)))\nprint('Test Shape: ({}, {})'.format(test_data.count(), len(test_data.columns)))","b65a5ac2":"lr = LinearRegression(labelCol = 'mpg',\n                      featuresCol = 'features',\n                      regParam = 0.3) #avoid overfitting\n\nlr = lr.fit(train_data)","eb33f952":"def evaluate_reg_model(model, test_data):\n    \n    print(model.__class__.__name__.center(70, '-'))\n    model_results = model.evaluate(test_data)\n    print('R2: {}'.format(model_results.r2))\n    print('MSE: {}'.format(model_results.meanSquaredError))\n    print('RMSE: {}'.format(model_results.rootMeanSquaredError))\n    print('MAE: {}'.format(model_results.meanAbsoluteError))\n    print(70*'-')\n\nevaluate_reg_model(lr, test_data)","2c0b6e71":"#End Session\nspark.stop()","4549e6a2":"## Train-Test Split","56861b43":"# Preprocessing\n\n## Encoding Brands","48665c82":"- <code>describe()<\/code> represents the statiscal summary of dataframe but it also uses the string variables","fdb0ad43":"# What is Spark?\n- Spark is one of the latest technologies being used to quickly and easily handle Big Data\n- It is an open source project on **Apache**\n- It was first released in February 2013 and has exploded in popularity due to it\u2019s ease of use and speed\n- It was created at the AMPLab at UC Berkeley\n- Spark is 100 times faster than **Hadoop MapReduce**\n- Spark does not store anything unless any action is applied on the data\n\n![image.png](attachment:679c1237-df7b-44cd-8a55-32ba9cbd0e3e.png)","66b9ef2e":"# Libraries and Utilities\n\nWe need to install pyspark first","b6436222":"# PySpark DataFrame Basics\n\n- Spark DataFrames hold data in a column and row format.\n- Each column represents some feature or variable.\n- Each row represents an individual data point.\n- They are able to input and output data from a wide variety of sources.\n- We can then use these DataFrames to apply various transformations on the data.\n- At the end of the transformation calls, we can either show or collect the results to display or for some final processing.","91d58b77":"<p style=\"padding: 10px;\n          color:black;\n          text-align: center;\n          font-size:250%;\">\nPySpark Tutorial\n<\/p>","c414696a":"# GroupBy and Aggregate Operations","cf780235":"## Filtering with SQL","b67bc913":"# Spark DataFrame Basic Operations\n\n## Filtering & Sorting","2a749274":"# Multiple Linear Regression with PySpark\n\n## Fit the Model","536a4a37":"**If you liked this notebook, please upvote** \ud83d\ude0a\n\n**If you have any suggestions or questions, feel free to comment!**\n\n**Best Wishes!**","0860e16a":"# Machine Learning\n\n- Machine learning is a method of data analysis that automates analytical model building. \n- Using algorithms that iteratively learn from data, machine learning allows computers to find hidden insights without being explicitly programmed where to look.\n\n## Supervised Learning\n\n- Spark\u2019s MLlib is mainly designed for **Supervised** and **Unsupervised Learning** tasks, with most of its algorithms falling under those two categories.\n- Supervised learning algorithms are trained using labeled examples, such as an input where the desired output is known. \n- For example, a piece of equipment could have data points labeled either \u201cF\u201d (failed) or \u201cR\u201d (runs). \n- The learning algorithm receives a set of inputs along with the corresponding correct outputs, and the algorithm learns by comparing its actual output with correct outputs to find errors. \n- It then modifies the model accordingly.\n- Through methods like classification, regression, prediction and gradient boosting, supervised learning uses patterns to predict the values of the label on additional unlabeled data. \n- Supervised learning is commonly used in applications where historical data predicts likely future events.\n- For example, it can anticipate when credit card transactions are likely to be fraudulent or which insurance customer is likely to file a claim.\n- Or it can attempt to predict the price of a house based on different features for houses for which we have historical price data.\n\n## Unsupervised Learning\n\n- Unsupervised learning is used against data that has no historical labels. \n- The system is not told the \"right answer.\" The algorithm must figure out what is being shown. \n- The goal is to explore the data and find some structure within.\n- For example, it can find the main attributes that separate customer segments from each other. \n- Popular techniques include self-organizing maps, nearest-neighbor mapping, k-means clustering and singular value decomposition.\n- One issue is that it can be difficult to evaluate results of an unsupervised model!","35098bff":"# Loading Data","589a58f4":"# Machine Learning with PySpark\n\n- Spark has its own MLlib for Machine Learning.\n- The future of MLlib utilizes the Spark 2.0 DataFrame syntax.\n- One of the main \u201cquirks\u201d of using MLlib is that you need to format your data so that eventually  it just has one or two columns:\n    - Features, Labels (Supervised)\n    - Features (Unsupervised)\n- This requires a little more data processing work than some other machine learning libraries, but the big upside is that this exact same syntax works with distributed data, which is no small feat for what is going on \u201cunder the hood\u201d!\n- When working with Python and Spark with MLlib, the documentation examples are always with nicely formatted data.\n- A huge part of learning MLlib is getting comfortable with the documentation!\n- Being able to master the skill of finding information (not memorization) is the key to becoming a great Spark and Python developer!\n- Let\u2019s jump to it now!\n\nLearn more in here: https:\/\/spark.apache.org\/mllib\/\n\n![image.png](attachment:9f379daa-23dc-4f1d-9886-47f6a23764a7.png)","ee629a31":"## Model Evaluation","2b5cd4a9":"# Creating a SparkSession"}}