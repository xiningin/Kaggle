{"cell_type":{"62dcd746":"code","1541ca84":"code","89977d00":"code","065cf11b":"code","22b5d26e":"code","323419c0":"code","8110accf":"code","ed3fb703":"code","7348e6ea":"code","ce738ca7":"code","54afb2fc":"markdown","8438312f":"markdown","dcec2793":"markdown","034f6a9f":"markdown","1fccb246":"markdown","545256ab":"markdown","cd08ced3":"markdown","a6dac75e":"markdown","49276104":"markdown"},"source":{"62dcd746":"import pandas as pd\nimport numpy as np\nfrom colorama import Fore as f\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\nimport re\nimport gc\nfrom warnings import filterwarnings\nimport tensorflow as tf\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom IPython.display import clear_output\nimport plotly.express as px\nfrom nltk.corpus import stopwords as st\nimport nltk\nfrom nltk.corpus import webtext\nfrom nltk.probability import FreqDist\nfrom collections import deque\nimport plotly\nplotly.offline.init_notebook_mode (connected = True)\n\n\nfilterwarnings(\"ignore\")","1541ca84":"data=pd.read_csv('..\/input\/wikibooks-dataset\/english-wikibooks\/en-books-dataset.csv')","89977d00":"data.head()","065cf11b":"title=data['title'][200]\nlink=data['url'][200]\nabstract=data['abstract'][200]\nbody=data['body_text'][200]\nprint(f.YELLOW+\"Title : \",f.CYAN,title)\nprint(f.YELLOW+\"Url : \",f.CYAN,link)\nprint(f.YELLOW+\"Abstract : \",f.CYAN,abstract)\nprint(f.YELLOW+\"Body : \",f.CYAN,' '.join(body.split('.')[:2]))","22b5d26e":"stopwords=list(STOPWORDS)\n# Generate a word cloud image\ndata['body_text']=data['body_text'].apply(lambda x : str(x))\ntext=' '.join(data['body_text'].loc[:1000])\nmask = np.array(Image.open(\"..\/input\/wiki-logo\/Wikipedia-logo_%28inverse%29.png\"))\narrow=np.array(Image.open(\"..\/input\/arrowww\/8iAj9Xbia.png\"))\narrow=cv2.resize(arrow,(300,300))\nanother_image=np.array(Image.open('..\/input\/multicoloredflower\/Multi coloured flower.jpg'))\nmask=cv2.resize(mask,(600,600))\nanother_image=cv2.resize(another_image,(600,600))\nwordcloud_fra = WordCloud(stopwords=stopwords, background_color=\"white\", mode=\"RGBA\", max_words=1000, mask=mask).generate(text)\nfig, ax = plt.subplots(1,3,figsize=(18,8),gridspec_kw={'width_ratios': [3, 1,3]})\nax[0].imshow(mask)\nax[0].set_axis_off()\n\nax[1].imshow(arrow)\nax[1].set_axis_off()\n# create coloring from image\nimage_colors = ImageColorGenerator(another_image)\nax[2].imshow(wordcloud_fra.recolor(color_func=image_colors), interpolation=\"bilinear\")\nax[2].set_axis_off()\n# store to file\nplt.savefig(\"wiki.png\", format=\"png\")\ndel text\ngc.collect()","323419c0":"tex=' '.join(data['title']).lower()\nstop_words = set(st.words('english')) \ntex=re.sub(r'(?is)[^a-zA-Z0-9 ]','',tex)\ntex=re.sub(r'^[^ ]*', '', tex)\nwith open(\"\/usr\/share\/nltk_data\/corpora\/webtext\/Output.txt\", \"w\") as text_file:\n    text_file.write(tex)\nnltk.download('webtext')\nwt_words = webtext.words('.\/Output.txt')\ndata_analysis = nltk.FreqDist(wt_words)\n# Let's take the specific words only if their frequency is greater than 3.\nfor i in stop_words :\n    del data_analysis[i]\ndata_analysis = {k: v for k, v in sorted(data_analysis.items(), key=lambda item: item[1],reverse=True)}\nkey=list(data_analysis.keys())[:100]\nitem=list(data_analysis.values())[:100]","8110accf":"df=pd.DataFrame(columns=['Words','Count','x','y'])\nradius=12\nto_append=[key[0],item[0]\/40,0,0]\na_series = pd.Series(to_append, index = df.columns)\ndf = df.append(a_series, ignore_index=True)\ncount=1\nfor j in range(4) :\n    x=radius*j\/4\n    y=radius**2-x**2\n    to_append = [key[count],item[count],x,y]\n    a_series = pd.Series(to_append, index = df.columns)\n    df = df.append(a_series, ignore_index=True)\n    count+=1\nfor j in range(4):\n    x=radius-radius*j\/4\n    y=-1*(radius**2-x**2)\n    to_append = [key[count],item[count],x,y]\n    a_series = pd.Series(to_append, index = df.columns)\n    df = df.append(a_series, ignore_index=True)\n    count+=1\nfor j in range(4):\n    x=-1*(radius*j\/4)\n    y=-1*(radius**2-x**2)\n    to_append = [key[count],item[count],x,y]\n    a_series = pd.Series(to_append, index = df.columns)\n    df = df.append(a_series, ignore_index=True)\n    count+=1\nfor j in range(4):\n    x=-radius+(radius*j\/4)\n    y=radius**2-x**2\n    to_append = [key[count],item[count],x,y]\n    a_series = pd.Series(to_append, index = df.columns)\n    df = df.append(a_series, ignore_index=True)\n    count+=1\n    \ndef rotate_n(df,number):\n    x=[]\n    p=df.iloc[1:].values\n    for i in range(len(p)):\n        x.append([p[i,2],p[i,3]])\n    t=x.copy()\n    time=1\n    ho=len(x)\n    times=[time]*ho\n    for i in range(number-1):\n        time+=1\n        t = deque(t)\n        t.rotate(-1)\n        t = list(t)\n        x.extend(t)\n        times.extend([time]*ho)\n    x=np.array(x)\n    dn=pd.DataFrame()\n    words=[]\n    count=[]\n    for i in range(len(p)):\n        words.append(p[i,0])\n        count.append(p[i,1])\n    words=words*number\n    count=count*number\n    dn['Words']=words\n    dn['Count']=count\n    dn['x']=x[:,0]\n    dn['y']=x[:,1]\n    dn['Time']=times\n    for i in range(1,len(set(times))+1):\n        to_append = [df['Words'][0],df['Count'][0],df['x'][0],df['y'][0],i]\n        a_series = pd.Series(to_append, index = dn.columns)\n        dn = dn.append(a_series, ignore_index=True)\n        \n    return dn\ndf=rotate_n(df,20)\n\n        \n# for i in range(0,len(key)-3,3):\n#     for j in range(k):\n#         x=radius*j\/k\n#         y=radius**2-x**2\n#         to_append = [key[i],item[i],x,y,time]\n#         a_series = pd.Series(to_append, index = df.columns)\n#         df = df.append(a_series, ignore_index=True)\n#         y2=0\n#         to_append = [key[i+1],item[i+1],x,y2,time]\n#         a_series = pd.Series(to_append, index = df.columns)\n#         df = df.append(a_series, ignore_index=True)\n#         y3=-1*(radius**2-x**2)\n#         to_append = [key[i+1],item[i+1],x,y3,time]\n#         a_series = pd.Series(to_append, index = df.columns)\n#         df = df.append(a_series, ignore_index=True)\n#     time+=1\n        \n# df['Count']=df['Count'].apply(lambda x : int(x))\nfig=px.scatter(df,x='x',y='y',text='Words',size='Count',size_max=40,color='Count',\n               color_continuous_scale='solar',labels={'x':'','y':''},template='plotly_dark',animation_frame='Time')\nfig.add_layout_image(\n        dict(\n            source=\"https:\/\/upload.wikimedia.org\/wikipedia\/en\/thumb\/8\/80\/Wikipedia-logo-v2.svg\/1024px-Wikipedia-logo-v2.svg.png\",\n            xref=\"x\",\n            yref=\"y\",\n            x=-8,\n            y=40,\n            sizex=30,\n            sizey=105,\n            opacity=0.5,\n            layer=\"below\")\n)\nfig.add_layout_image(\n        dict(\n            source=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/d\/de\/Wikipedia-logo_%28inverse%29.png\/986px-Wikipedia-logo_%28inverse%29.png\",\n            xref=\"x\",\n            yref=\"y\",\n            x=2,\n            y=40,\n            sizex=30,\n            sizey=105,\n            opacity=0.5,\n            layer=\"below\")\n)\nfig.update_xaxes(range=[-radius-6,radius+6],showgrid=False)\nfig.update_yaxes(range=[-radius**2-16,radius**2+16],showgrid=False)\nfig.show()","ed3fb703":"# Let's Make a search engine using TFIDF Vectorizer :)\nmodel = TfidfVectorizer(stop_words='english', binary=True, max_features=100_000)\ntext_embeddings = model.fit_transform(data.title).toarray()","7348e6ea":"def search_it(search,print_n_top=10,return_them=False):\n    n=print_n_top\n    search_embedding=model.transform([search]).toarray()\n    areas=np.matmul(text_embeddings,search_embedding.T).T\n    k=areas[0]\n    k=sorted(k,reverse=True)\n    k=k[:n]\n    k=set(k)\n    idx=[]\n    for j in k :\n        idx.extend(np.where(areas[0,]==j)[0])\n    o=data.iloc[idx].title.values\n    l=[]\n    count=1\n    if len(o)>print_n_top:\n        o=o[:print_n_top]\n    for i in o :\n        print(f.YELLOW,str(count),')',end=' ')\n        print(f.CYAN,i)\n        l.append(i)\n        count+=1\n    if return_them :\n        return l\n         ","ce738ca7":"ss=input(f.YELLOW+'Search : ')\nl=search_it(ss,return_them=True)\ns=input(f.YELLOW+\"Choose Your option\")\ns=l[int(s)-1]\nclear_output(wait=True)\ntitle=data[data['title']==s]['title'].values[0]\nlink=data[data['title']==s]['url'].values[0]\nabstract=data[data['title']==s]['abstract'].values[0]\nbody=data[data['title']==s]['body_text'].values[0]\nprint(f.YELLOW+\"Title : \",f.CYAN,title)\nprint(f.YELLOW+\"Url : \",f.CYAN,link)\nprint(f.YELLOW+\"Abstract : \",f.CYAN,abstract)\nprint(f.YELLOW+\"Body : \",f.CYAN,' '.join(body.split('.')[:2]))","54afb2fc":"<p style=\"font-family:courier;font-size:300%;color:#184d47;background-color:#e4d3cf;\">Importing Packages<\/p>","8438312f":"<p style=\"font-family:courier;font-size:300%;background-color:#e4d3cf;\">Wordcloud For The Text Body<\/p>","dcec2793":"<p style=\"font-family:courier;font-size:300%;background-color:#e4d3cf;\">Top N Words Of The Data<\/p>","034f6a9f":"<p style=\"font-family:courier;font-size:300%;color:#184d47;background-color:#e4d3cf;\">Importing Data<\/p>","1fccb246":"<p style=\"font-family:courier;font-size:300%;background-color:#e4d3cf;\">What is this data all about ??<\/p>\n<ul style=\"font-family:courier;font-size:200%;\"><li>Title : Title of the wikibook <\/li>\n    <li>Url : Link to the wiki book <\/li>\n    <li>Abstract : A Summary of the wiki book <\/li>\n    <li>Body text : Content of the wiki book <\/li>\n    <li> Body html : Html code of the wiki book <\/li>","545256ab":"<p style=\"font-family:courier;font-size:150%;background-color:#e4d3cf;\">Would personally recommend running the animation ( Idea taken from solar system) <\/p>","cd08ced3":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/6\/6a\/2011Sendai-NOAA-Energylhvpd9-05.jpg\/1280px-2011Sendai-NOAA-Energylhvpd9-05.jpg)","a6dac75e":"<p style=\"font-family:courier;font-size:300%;background-color:#e4d3cf;\">Search Engine<\/p>","49276104":"<p style=\"font-size:80%\">wikipedia logo from https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/d\/de\/Wikipedia-logo_%28inverse%29.png\/986px-Wikipedia-logo_%28inverse%29.png<\/p>"}}