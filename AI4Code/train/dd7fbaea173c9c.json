{"cell_type":{"ff19d7d1":"code","aa0c6cce":"code","6f6237a5":"code","dcfcb01f":"code","4d3018d1":"code","1a7779f0":"code","c2decca4":"code","4a93dd09":"code","3fa10b17":"code","3479b14f":"code","5f20dda5":"code","d7862673":"code","c5f3fdb9":"code","0b183f42":"code","65182095":"code","b69bb648":"code","1726b3a5":"code","47472d8b":"code","a19d3f2b":"code","a15f1a7b":"code","df036c62":"code","40409c95":"code","8875bd83":"code","eb3d6509":"code","8d246554":"code","9673304a":"code","4cc065a5":"code","e476a293":"code","3b2be55b":"code","f418233b":"code","1884ddb2":"code","a7dc7d64":"code","8c05d73f":"code","4944a76b":"code","ed55cd07":"code","df94f039":"code","86eeb5fa":"code","a259c40d":"code","823f8c73":"markdown","b5b1f54f":"markdown","753ece42":"markdown","81e8eadb":"markdown","52866ec8":"markdown","af543f59":"markdown","d6424c54":"markdown","987f1264":"markdown","39429eaa":"markdown","f87603fc":"markdown"},"source":{"ff19d7d1":"# Importing necessary models \n\nimport pandas as pd\nimport matplotlib as mp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.metrics import r2_score\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","aa0c6cce":"def fish_data_import():\n    \"\"\"\n    Function useful for importing a file and converting it to a dataframe\n    \"\"\"\n    datafile = pd.read_csv('\/kaggle\/input\/fish-market\/Fish.csv')\n    return datafile","6f6237a5":"# Importing the dataset file\nfish_df = fish_data_import()","dcfcb01f":"fish_df.info()","4d3018d1":"fish_df","1a7779f0":"fish_df.describe()","c2decca4":"# Defining a function for Horizonal bar plot \n\ndef plot_counts_bar(data,column,fig_size=(16,9),col='blue',col_annot='grey',water_m=False,water_text='KedNat'):\n    \"\"\"\n    Function plot_counts_bar plots a horizontal bar graph for Value counts for a given Dataframe Attribute.\n    This is much useful in analysis phase in Datascience Projects where data counts for a particular attributes needs to be visualized.\n    Mandatory inputs to this function. \n        1. 'data' where dataframe is given as input \n        2. 'column' where column name is given as input for which we need the value counts.\n    Optional inputs to this function:\n        1. 'fig_size' which represent the figure size for this plot. Default input is (16,9)\n        2. 'col' which represents the color of the bar plot. Default input is 'blue'\n        3. 'col_annot' which represents the color of annotations. Default input is 'grey'\n        4. 'water_m' which represents if we need a watermark text. Default input is boolean as False\n        5. 'water_text' which inputs a string variable used for watermark. Default is KedNat\n    \"\"\"\n    \n    # Figure Size \n    fig, ax = plt.subplots(figsize =fig_size) \n\n    # Defining the dataframe for value counts\n    df = data[column].value_counts().to_frame()\n    df.reset_index(inplace=True)\n    df.set_axis([column ,'Counts'], axis=1, inplace=True)\n    X_data = df[column]\n    y_data = df['Counts']\n\n    # Horizontal Bar Plot \n    ax.barh(X_data, y_data , color=col) \n\n    # Remove axes splines \n    for s in ['top', 'bottom', 'left', 'right']: \n        ax.spines[s].set_visible(False)\n\n    # Remove x, y Ticks \n    ax.xaxis.set_ticks_position('none') \n    ax.yaxis.set_ticks_position('none') \n\n    # Add padding between axes and labels \n    ax.xaxis.set_tick_params(pad = 5) \n    ax.yaxis.set_tick_params(pad = 10) \n\n    # Show top values \n    ax.invert_yaxis()\n    \n    # Add annotation to bars \n    for i in ax.patches: \n        plt.text(i.get_width()+0.2, i.get_y()+0.5,str(round((i.get_width()), 2)),fontsize = 10, fontweight ='bold',color =col_annot) \n\n    # Add Plot Title \n    title = 'Counts of each '+column\n    ax.set_title(title, loc ='left', fontweight=\"bold\" , fontsize=16) \n    \n    # Add Text watermark \n    if water_m == True:\n        fig.text(0.9, 0.15, water_text, fontsize = 12, color ='grey', ha ='right', va ='bottom', alpha = 0.7) \n\n    ax.get_xaxis().set_visible(False)\n\n    # Show Plot \n    plt.show() ","4a93dd09":"# Plotting the species counts on entire dataset\nplot_counts_bar(fish_df,'Species',(10,6),col='green',col_annot='blue')","3fa10b17":"# Defining a function for Stratified split on a given column\n\ndef stratified_split(data,column,testsize=0.2):\n    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n    for train_index, test_index in split.split(data, data[column]):    \n        strat_train_set = fish_df.loc[train_index]    \n        strat_test_set = fish_df.loc[test_index]\n        return(strat_train_set,strat_test_set)","3479b14f":"# Splitting into train and test dataset on basis of Stratified split column of Species\ntrain_set,test_set = stratified_split(fish_df,'Species')","5f20dda5":"# Counts of species on test set\nplot_counts_bar(test_set,'Species',(10,6),col='green',col_annot='blue')","d7862673":"# fish_df will now be Training set\nfish_df = train_set.copy()","c5f3fdb9":"fish_df.info()","0b183f42":"# Defining a function for Heatmap on a given data\ndef heat_map(data,fig_size=(8,8)):\n\n    fig, ax = plt.subplots(figsize=fig_size)\n    heatmap = sns.heatmap(data,\n                          square = True,\n                          linewidths = .2,\n                          cmap = 'YlGnBu',\n                          cbar_kws = {'shrink': 0.8,'ticks' : [-1, -.5, 0, 0.5, 1]},\n                          vmin = -1,\n                          vmax = 1,\n                          annot = True,\n                          annot_kws = {'size': 12})\n\n    #add the column names as labels\n    ax.set_yticklabels(data.columns, rotation = 0)\n    ax.set_xticklabels(data.columns)\n\n    sns.set_style({'xtick.bottom': True}, {'ytick.left': True})","65182095":"# Heatmap for Training set\nheat_map(fish_df.corr())","b69bb648":"heat_map(fish_df[['Length1','Length2','Length3']].corr())","1726b3a5":"# Defining a function to calculate Pearson-correlation and p-values w.r.t given label columns\ndef peason_test(data,label):\n    \"\"\"\n    This function gives the resultant Person correlation and p-value for given set of numeric attrivutes w.r.t labelled column.\n    Inputs asre as follows:\n    \"data\" : dataframe of numeric columns . eg : df[['A','B',C]]\n    \"label\": Name of labelled column which is present in dataframe in data input. eg : 'label'\n    \"\"\"\n    \n    print('Pearson Correlation and p-values continous values\\n')\n    for i in list(data.columns):\n        p_cor, p_val = pearsonr(data[label],data[i])\n        print('For '+i+' :')\n        print('    Pearson correlation :'+str(round(p_cor,5)))\n        print('    p-value             :'+str(p_val))","47472d8b":"# Defining column lists which will be useful further\nnum_cols =   ['Weight','Length1','Length2','Length3','Height','Width']\ncat_cols =   ['Species']\nlabel_cols = ['Weight']\nlabel = 'Weight'","a19d3f2b":"# Calculating Pearsons correlation tests \npeason_test(fish_df[num_cols],'Weight')","a15f1a7b":"# transform data can be used to transform a given set of dataframe using data cleansing.\n# Onehot Encoded values for Species are also created\n\ndef transform_data(data):\n    \"\"\"\n    Function used to transform a given set of dataframe. Eg Train or Test dataframe\n    This is used to \n    --> fix the data with Null values\n    --> remove the unnecessary columns.\n    --> create Labelencoded or OneHotEncoded attributes\n    \"\"\"\n    data.drop(['Length1','Length2'],axis=1,inplace=True)\n    result_df = pd.get_dummies(data,columns=['Species'],prefix=['Species'])\n    return result_df","df036c62":"# Transforming the Training set\nfish_train = transform_data(fish_df)","40409c95":"fish_train","8875bd83":"# Redefining the lists of attributes \nnum_cols = ['Weight','Length3','Height','Width']\nohe_cols = ['Species_Bream','Species_Parkki','Species_Perch','Species_Pike','Species_Roach','Species_Smelt','Species_Whitefish']\nindependent_features = ['Length3', 'Height', 'Width', 'Species_Bream','Species_Parkki', 'Species_Perch', 'Species_Pike', 'Species_Roach','Species_Smelt', 'Species_Whitefish']","eb3d6509":"# Defining a simple function for Scatter plot useful for analysis\ndef scatter_plot(data,hue=None,kind='scatter'):\n    \"\"\"\n    Scatter plot function defined for understanding the data relations between numeric attributes in dataframe.\n    'data' is used to input the dataframe with the set of numeric attributes. Eg : df[['A','B','C']]\n    \"\"\"\n    sns.pairplot(data,hue=hue,kind=kind,corner=False)","8d246554":"# Scatter plot for Numeric data\nscatter_plot(fish_train[num_cols])","9673304a":"# Creating poynomial_bestfit which outputs the best polynomial for for a given data with label\n\ndef Poynomial_bestfit(data,X_col,y_col,degrees=list(range(1,5)),cv=5):\n    \"\"\"\n    Poynomial_bestfit find the best of degree for a given dataset.\n    Inputs are as follows:\n    'data' : dataframe as an input which has dependent and independent attributes. eg : df\n    'X_col': independent variables as a list. eg : ['A','B','C']\n    'y_col': dependent variables as a string input. eg : 'label'\n    \"\"\"\n    plt.rcParams[\"figure.figsize\"] = [9,4]    \n    degrees = degrees \n    best_score = 0\n    best_degree = 0\n    score_l = []\n    for degree in degrees:\n            X = np.asanyarray(data[X_col])\n            y = data[[y_col]]\n            poly_features = PolynomialFeatures(degree = degree)\n            X_poly = poly_features.fit_transform(X)\n            polynomial_regressor = LinearRegression(normalize=False)\n            polynomial_regressor.fit(X_poly, y)\n            scores = cross_val_score(polynomial_regressor, X_poly, y, cv=cv)\n            scores_mean = round(scores.mean(),2)\n            score_l.append(scores_mean)\n            if scores_mean > best_score :\n                best_score = scores_mean\n                best_degree = degree\n    plot_df = pd.DataFrame({'Degrees': degrees,'Scores': score_l})\n    plot_df.plot(kind='line',x='Degrees',y='Scores')\n    plt.title('Scores for Polynomial fit for attributes ')\n    print('Best Score  is :'+str(best_score))\n    print('Best Degree is :'+str(best_degree))\n    plt.show()","4cc065a5":"Poynomial_bestfit(fish_train,independent_features,'Weight')","e476a293":"# rmse_check function is useful in giving the RMSE for different Regression models\n\ndef rmse_check(X,y,degree=1,cv=5):\n    \"\"\"\n    Function to be used to check the RMSE for different Regression techniques to as to compare errors on different models.\n    The Models include Polynomial regression for a degree as degree.\n    Decision Tree regressor model and Random forest regressor.\n    Inputs include dataframe of Independent variables as X and Dependent variables as y for a given no of splits as cv.\n    Outputs as RMSE errors for a given model.\n    This helps us to decide on which model is best fit for a given training data.\n    \"\"\"\n\n    X = np.asanyarray(X)\n\n    poly_features = PolynomialFeatures(degree = degree)\n    X_poly = poly_features.fit_transform(X)\n    poly_reg = LinearRegression()\n    scores = cross_val_score(poly_reg,X_poly,y,scoring=\"neg_mean_squared_error\", cv=cv)\n    poly_rmse_scores = np.sqrt(-scores)\n    print('RMSE for Polynomial regression of degree '+str(degree)+' is :'+str(poly_rmse_scores.mean()))\n    \n    tree_reg = DecisionTreeRegressor()\n    scores = cross_val_score(tree_reg, X,y,scoring=\"neg_mean_squared_error\", cv=cv)\n    tree_rmse_scores = np.sqrt(-scores)\n    print('RMSE for Decision Tree regressor is :'+str(tree_rmse_scores.mean()))\n    \n    forest_reg = RandomForestRegressor()\n    scores = cross_val_score(forest_reg, X,y,scoring=\"neg_mean_squared_error\", cv=cv)\n    forest_rmse_scores = np.sqrt(-scores)\n    print('RMSE for Random Forest regressor is :'+str(forest_rmse_scores.mean()))","3b2be55b":"# Calculating RMSE for Training data. Data is split into 10 folds and means of scores is calculated\nrmse_check(fish_train[independent_features], fish_train[label],2,10)","f418233b":"# poly_model_trains the model for entire training data and returns the predicted labels along with model\ndef poly_model_train(X,y,degree,train_set=False):\n    \"\"\"\n    Used to train the Polynomial model for a given degree. \n    Inputs are Independent attributes as X and Dependent variables as y for a given degree as degree\n    Returns the trained Polynomial model as poly_reg and Predicted label for training set as yhat_train.\n    \"\"\"\n    poly_features = PolynomialFeatures(degree = degree)\n    X = np.asanyarray(X)\n    X_poly = poly_features.fit_transform(X)\n    poly_reg = LinearRegression()\n    poly_reg.fit(X_poly, y)\n    yhat_train = poly_reg.predict(X_poly)\n    return (poly_reg,yhat_train)","1884ddb2":"# Training the test data for polynomial regression of degree 2\npoly_reg_model , yhat_train = poly_model_train(fish_train[independent_features],fish_train[label],2)","a7dc7d64":"# Defining a distribution plot function to plot Actual vs Predicted labels\ndef distributionPlot(RedFunction, BlueFunction, RedName, BlueName,x_label,y_label,title):\n    \"\"\"\n    This is used to create a distribution plots with Actual label vs Predicted label.\n    Useful for understanding where is the gap in prediction for different values of y\n    \"\"\"\n    width = 8\n    height = 6\n    plt.figure(figsize=(width, height))\n\n    ax1 = sns.distplot(RedFunction, hist=False, color=\"r\", label=RedName)\n    ax2 = sns.distplot(BlueFunction, hist=False, color=\"b\", label=BlueName, ax=ax1)\n\n    plt.title(title)\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n\n    plt.show()\n    plt.close()","8c05d73f":"# Distribition Plot for Training data \ndistributionPlot(fish_train[label], yhat_train, \"Actual Values (Train Polynomial reg)\", \"Predicted Values (Train Polynomial reg)\", \n                 'Weight of Fish','Propotion of Fish',\n                 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution')","4944a76b":"# Creating a Test dataset\nfish_test = test_set.copy()","ed55cd07":"# Transforming the Test dataset using transform_data function\nfish_test = transform_data(fish_test)","df94f039":"# Creating function poly_model_test for Predicting the test dataset using model created using Training data\ndef poly_model_test(X,y,poly_model,degree):\n    \"\"\"\n    This is used for predicting the Test dataset scores of polynomial regression for a given model input as poly_reg_model.\n    This function prints the R2 value for the give test data and returns predicted label as  yhat_test\n    \"\"\"\n    poly_features = PolynomialFeatures(degree = degree)\n    X = np.asanyarray(X)\n    X_poly_test = poly_features.fit_transform(X)\n    yhat_test = poly_model.predict(X_poly_test)\n    print('R2 for test data '+str(r2_score(yhat_test ,y)))\n    return(yhat_test)","86eeb5fa":"# Checking the R2 score for Test data using function poly_model_test\nyhat_test = poly_model_test(fish_test[independent_features],fish_test[label],poly_reg_model,2)","a259c40d":"# Distribution Plot on Test dataset\n\ndistributionPlot(fish_test[label], yhat_test, \"Actual Values (Test)\", \"Predicted Values (Test)\", \n                 'Weight of Fish','Propotion of Fish',\n                 'Distribution  Plot of  Predicted Value Using Testing Data vs Testing Data Distribution')","823f8c73":"All Attributes have high correlations and good p-values.\n\nBut out of Lenghts Lenght3 is higly correlated with Weight than Length1 and Lenght2. \nHence we can drop Length1 and Lenght2","b5b1f54f":"As seen above results shows that degree 2 is best fit in training data for Polynomial regression. The R2 significantly drops later. \n\nNow Lets test the RMSE for different models which include Polynomial with degree2 , DecisionTreeRegressor and RandomForestRegressor","753ece42":"### Training dataset for Analysis and Model Building","81e8eadb":"As seen above we can see a high correlation between Length1 and Length2 of 1.\n\nLength1 and Length3 are also highly correlated with 0.99\n\nAdding all three can create Multicollinerity. Hence we need to pick the best one based on pearson value and p-value test w.r.t. Weight","52866ec8":"### Testing data ","af543f59":"### Splitting the data into Train and Test","d6424c54":"# Fish market Dataset from Kaggle\n\n### Kaggle link : https:\/\/www.kaggle.com\/aungpyaeap\/fish-market\n\n### Content:\nThis dataset is a record of 7 common different fish species in fish market sales. \nWith this dataset, a predictive model can be performed using machine friendly data and estimate the weight of fish can be predicted.\n\n### Task :\nBuild a machine learning model so as to predict the weight of the fish given its parameters of Lengths , Height , Widths and Fish type","987f1264":"<b> R2 score is 98.4% which is good on Test dataset. \nHence we can say that Polynomial regression with degree2 fits the data well and can be used for prediction of weight of the fish for a given Length3 , Height , Width and Species <\/b>","39429eaa":"Its clear that Polynomial regression of degree 2 has better RMSE scores than others.\n\nHence the same is used for creating model and tesing","f87603fc":"Distribution plot looks so clean with almost neglible predictive errors for all weights. But remember the same dataset is used here to train the data and hence model already knows the data used for prediction. There could be also overfitting here which can only be evident after checking the testing set results and its distribution plots "}}