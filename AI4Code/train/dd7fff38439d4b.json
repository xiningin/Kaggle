{"cell_type":{"813245f8":"code","7adf627f":"code","a282d11c":"code","10ef9dd2":"code","b408778d":"code","a12018b7":"code","65a7e562":"code","ab6902a9":"code","b7dc3603":"code","4156bc7d":"code","5b9ca941":"code","3d086533":"code","cd16235b":"code","33e8b584":"code","774144ee":"code","64fcdd67":"code","501b125d":"code","3659bbc9":"code","06adceb4":"code","7025b692":"code","966045f7":"code","68cc911b":"code","92d43622":"code","e8f6cf4b":"code","4fba6a1d":"code","f7d90ea9":"code","d8fff207":"code","0d6762fd":"code","3486ddde":"code","9d10290e":"code","63b2e3b7":"code","5e5a317f":"code","3d4c5d7b":"code","61c2180f":"markdown","66a99f0f":"markdown","d75b2a7b":"markdown","cdb401ca":"markdown","9fbd2795":"markdown","b76f1e07":"markdown","43ddfc82":"markdown","d3bb4b15":"markdown","9396b10c":"markdown","9fe8549b":"markdown","11ed515c":"markdown","7beea91a":"markdown","b09bd83d":"markdown","9fd4c17b":"markdown","9eb7a666":"markdown","4a18bc9d":"markdown","d9c40e81":"markdown","3275a54b":"markdown","d8376ef1":"markdown","996e5fe7":"markdown"},"source":{"813245f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# import plotting libraries\nimport matplotlib.pyplot as plt\n#import sns for better plots, it is handy to manage subplots\nimport seaborn as sns","7adf627f":"data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndata.head(5)","a282d11c":"data.describe()","10ef9dd2":"# plotting only one variable\nsns.catplot(x=\"Sex\", y=\"Survived\", kind=\"bar\",data=data)","b408778d":"# similarly one can plot any other categorical & Ordinal variable\nsns.catplot(x=\"Pclass\", y=\"Survived\", kind=\"bar\",data=data)","a12018b7":"# plot multiple variables together\nsns.catplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", kind=\"bar\", data=data)","65a7e562":"#An alternative style for visualizing the same information is offered by the pointplot() function. \n#This function also encodes the value of the estimate with height on the other axis, \n#but rather than showing a full bar, it plots the point estimate and confidence interval. \n#Additionally, pointplot() connects points from the same hue category. \n#This makes it easy to see how the main relationship is changing as a function of the hue semantic, \n#because your eyes are quite good at picking up on differences of slopes:\nsns.catplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", kind=\"point\", data=data)\n# this confirms that there is an interaction effect between sex and pclass","ab6902a9":"data.groupby('Sex')['Survived'].mean()","b7dc3603":"# similarly one can calculate the survival rate for Pclass or any other categorical & ordinal variable\n# or, calculate the survival rate in combination of a few variables\ndata.groupby(['Pclass', 'Sex'])['Survived'].mean()","4156bc7d":"# use other features to interpolate the missing age\n# this part is taking from another notebook\n# https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\ndata['Initial']=0\nfor i in data:\n    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\npd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap='summer_r') #Checking the Initials with the Sex","5b9ca941":"data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\ndata.groupby('Initial')['Age'].mean() #lets check the average age by Initials","3d086533":"## Assigning the NaN Values with the Ceil values of the mean ages\ndata.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\ndata.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\ndata.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\ndata.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\ndata.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46\ndata.Age.isnull().any() #So no null values left finally ","cd16235b":"df =data[data.Sex == 'male']\nsns.distplot(df['Age'],  kde=False,label='Male')\n\ndf =data[data.Sex == 'female']\nsns.distplot(df['Age'],  kde=False,label='female')\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('Survival count')\nplt.xlabel('Age')\nplt.ylabel('Count')","33e8b584":"grid = sns.FacetGrid(data, col='Survived',height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","774144ee":"grid = sns.FacetGrid(data, col='Survived',row='Sex', height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","64fcdd67":"# a mix of categorical and continuous varibales makes it difficult to choose proper models\n# also the adjacent ages don't make a big difference (being 20 or 21 doesn't change much) but more the big changes in age (20 vs 80)\n# so lets categorize the age variable into 5 levels\ndata['Age_band']=0\ndata.loc[data['Age']<=16,'Age_band']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32),'Age_band']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48),'Age_band']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64),'Age_band']=3\ndata.loc[data['Age']>64,'Age_band']=4","501b125d":"# sns.catplot(x=\"Age_band\", y=\"Survived\", row=\"Pclass\", hue='Sex', kind=\"box\", orient=\"h\", data=data)\nsns.catplot(x=\"Age_band\", y=\"Survived\", col=\"Pclass\", hue=\"Sex\", kind=\"point\", data=data)","3659bbc9":"data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\ndata['IsAlone'] = 1 #initialize to yes\/1 is alone\ndata['IsAlone'].loc[data['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1","06adceb4":"# turn sex into integers instead of string\ndata['Sex'] = data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)","7025b692":"data.columns","966045f7":"# drop the rest columns\ndroplist = ['PassengerId', 'Name', 'Age', 'SibSp','Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Initial', 'FamilySize']\ndata.drop(droplist,axis=1,inplace=True)\nsns.heatmap(data.corr(),annot=True,cmap='RdYlGn')\nfig=plt.gcf()\nplt.xticks()\nplt.yticks()\nplt.show()","68cc911b":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import GridSearchCV","92d43622":"data.head(5)","e8f6cf4b":"train,val=train_test_split(data,test_size=0.3,random_state=42,stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\nval_X=val[val.columns[1:]]\nval_Y=val[val.columns[:1]]","4fba6a1d":"# first fit a logistic regression\nmodel = LogisticRegression()\nmodel.fit(train_X,train_Y)\nprediction=model.predict(val_X)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction,val_Y))","f7d90ea9":"# since the dataset is small, having a fixed portion of data as validation data is quite expensive\n# we use k-fold cross validation to alternate the train and validation set\n# split the data into 10 equal parts\nX=data[data.columns[1:]]\nY=data['Survived']\nkfold = KFold(n_splits=10, random_state=22) ","d8fff207":"logistic_cv_result = cross_val_score(LogisticRegression(),X,Y, cv = kfold,scoring = \"accuracy\")\nprint('The mean accuracy of the Logistic Regression under 10-fold validation is: ', np.mean(logistic_cv_result), \n      'std is: ', np.std(logistic_cv_result))","0d6762fd":"# similarly we do the same to decision tree\ntree_cv_result = cross_val_score(DecisionTreeClassifier(),X,Y, cv = kfold,scoring = \"accuracy\")\nprint('The mean accuracy of the decision tree under 10-fold validation is: ', np.mean(tree_cv_result), \n      'std is: ', np.std(tree_cv_result))","3486ddde":"# random forest\nforest_cv_result = cross_val_score(RandomForestClassifier(n_estimators=100),X,Y, cv = kfold,scoring = \"accuracy\")\nprint('The mean accuracy of the random forest under 10-fold validation is: ', np.mean(forest_cv_result), \n      'std is: ', np.std(forest_cv_result))","9d10290e":"# random forest has a required hyper-pqrqmers: number of trees\n# we can use cross validatio and grid search to find a good number of it\nn_estimators=range(100,1000,100)\nhyper={'n_estimators':n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=132),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)\nprint(gd.best_params_)","63b2e3b7":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","5e5a317f":"# formulate test data in the same way\ntest_data['Sex'] = test_data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\ntest_data['Age_band']=0\ntest_data.loc[test_data['Age']<=16,'Age_band']=0\ntest_data.loc[(test_data['Age']>16)&(test_data['Age']<=32),'Age_band']=1\ntest_data.loc[(test_data['Age']>32)&(test_data['Age']<=48),'Age_band']=2\ntest_data.loc[(test_data['Age']>48)&(test_data['Age']<=64),'Age_band']=3\ntest_data.loc[test_data['Age']>64,'Age_band']=4\n\ntest_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\ntest_data['IsAlone'] = 1 #initialize to yes\/1 is alone\ntest_data['IsAlone'].loc[test_data['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1","3d4c5d7b":"# take features\nfeatures = [\"Pclass\", \"Sex\", \"Age_band\", \"IsAlone\"]\nX_test = pd.get_dummies(test_data[features])\n\n# first re-fit a logistic regression on all the training data\n# model = LogisticRegression()\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(X,Y)\n# make prediction\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","61c2180f":"### **visualize continuous variables**","66a99f0f":"## apply model on test data and submit","d75b2a7b":"![](https:\/\/i.imgur.com\/AC9Bq63.png)","cdb401ca":"## Modeling","9fbd2795":"The variables are in different types: categorical (sex, embarked, sibsp, parch, embarked), Ordinal (pclass), and continuous (age). And we visualize them in different ways","b76f1e07":"### **visualize catgorical & Ordinal variables**","43ddfc82":"we have seen previously that having siblings, parents, or children also seem to make a difference in the survival rate, \ncan we make that a feature?","d3bb4b15":"### Logistic regression","9396b10c":"An overview of the dataset by calling describe().\n \nWhat information can be extracted?\nhow many survived?\nIs there any missing values?","9fe8549b":"The logistic regression function can be written as:\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/4a5e86f014eb1f0744e280eb0d68485cb8c0a6c3)\nThis way we can keep one side as the conventional linear regression. The left side of the equation is call log-odds, or probit. \nHow do we link this to the binary dependent variable (survival)? We can first reformulate the function a litte bit. By simple algebraic manipulation, the probability that Y=1 is:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/11e03264d56bc270428fae2334fbf3ef11b003c2)\n\nIt means with a logistic function, we can transform the linear regression output to a probability that is bounded between 0 and 1:\n![](https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--yt2nSddw--\/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000\/https:\/\/dev-to-uploads.s3.amazonaws.com\/i\/l7xj9gkzufp00gt2txzu.png)\nWe take 0.5 as cutoff value to turn the probility into categorical values.\n\nThere are many different ways to understand logistic regression, for example, using latent variable:\n","11ed515c":"also we should use cross_validation to make sure the fitted model performance is generalisable to test data","7beea91a":"there are many different models available, here we explore 3 different models:\n* logistic regression (regression type)\n* decision tree (tree type)\n* random forest (bagging + randomize features) \\\n\\\nother models that are also applicable:\n* SVM\n* KNN\/other clustering models\n* Peceptron\n* ...","b09bd83d":"### random forest","9fd4c17b":"## feature engineering","9eb7a666":"## Exploratory Data Analysis","4a18bc9d":"> ![](https:\/\/annalyzin.files.wordpress.com\/2016\/07\/decision-trees-titanic-tutorial.png)","d9c40e81":"* survivial rate is low (< 40%)\n* age has missing values, and it spreads out widely\n* it is hard to tell much about the categorical\/ordinal variables by this way\n* Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\n* Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\n* PassengerId may be dropped from training dataset as it does not contribute to survival.\n* Name feature is relatively non-standard, may not contribute directly to survival.\n* ....\nwe look for a better way to explore the relationships --> plots","3275a54b":"Age has missing value.                         \nHow to interpolate the missing ages? By mean? By mode?","d8376ef1":"lets look at continuous variable (age) now.\nBut remember there was one problem? And what kind of plots do you think is good for visualizing continuous values?","996e5fe7":"### decision tree"}}