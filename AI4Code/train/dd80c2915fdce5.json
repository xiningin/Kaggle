{"cell_type":{"101a149d":"code","7251c5d5":"code","adf5bcba":"code","83e6eeec":"code","5b2a0da7":"code","1f02842d":"code","5b4d5aa6":"code","de04277a":"code","15c6a882":"code","40977f96":"code","887446ab":"code","85c0818b":"code","bce10aba":"code","4482657d":"code","a2cc4125":"code","d9c8d88d":"code","740685bc":"code","a3086133":"code","b43f8e0c":"code","25d7b551":"code","87627109":"code","681b760c":"code","f930de5f":"code","3d25ec9b":"code","4340aa95":"code","62abcbae":"code","b1cbe66b":"code","eb4916de":"code","46601a7f":"code","d4efd466":"code","42a5ced2":"code","6dedbdaa":"code","d593070b":"code","a5295a77":"code","0dea0693":"code","93751e35":"markdown","75d4cd06":"markdown","46336d49":"markdown","e7146f0e":"markdown","3b0c8fa3":"markdown","d1ef0445":"markdown","907bd704":"markdown","87a6adc3":"markdown","9af11650":"markdown","c28917cf":"markdown","bc4f8511":"markdown","d2d66789":"markdown","1c6a08e8":"markdown","720b0b43":"markdown","c085f77c":"markdown","63ecb7f6":"markdown","bcf39bd9":"markdown","bbee4140":"markdown","8f1bdf9a":"markdown","911ad0c8":"markdown"},"source":{"101a149d":"!pip install pysolr\n!pip install transformers\n!pip install pandas","7251c5d5":"from pysolr import Solr\nimport json\nimport os\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, BertForQuestionAnswering\nimport shutil\nimport torch\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom nltk.corpus import stopwords\nimport requests\nfrom bs4 import BeautifulSoup","adf5bcba":"stop_words = set(stopwords.words('english'))\ntitle_dict = {}","83e6eeec":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"solr_server\")\nsolr = Solr(secret_value_0)","5b2a0da7":"def preprocess(outer_folder, folder_name):\n    fname = \"..\/input\/CORD-19-research-challenge\/\"+outer_folder+folder_name + \"\/\"\n    contents = os.listdir(fname)\n    address = \"processed\/\" + folder_name + \"\/\"\n    new_files = []\n    result = solr.search(\"path:\"+ folder_name + \" path: \" + folder_name + \"\/\", rows=40000, **{\"fl\":\"id\"})\n    dct = set()\n    for i in result:\n        dct.add(address + i[\"id\"])\n    new_docs = set()\n    for i in contents:\n        d = json.load(open(fname+i))\n        id = d['paper_id']\n        path = folder_name\n        title=d['metadata']['title']\n        abstract = \"\"\n        if 'abstract' in d:\n            abst = d['abstract']\n            for j in abst:\n                abstract = abstract + j[\"text\"] + \"\\n\"\n        body = \"\"\n        for j in d['body_text']:\n            body = body + j[\"text\"] + \"\\n\"\n        data = {}\n        title_dict[id]=title\n        data['title']=title\n        data['abstract']=abstract\n        data['id']=id\n        data['body']=body\n        data['path']=path\n        with open(address+i,'w') as out:\n            json.dump(data,out)\n        new_docs.add(address + id)\n        #new_files.append(address+i)\n    to_add = new_docs.difference(dct)\n    to_remove = dct.difference(new_docs)\n    return [*to_add,], [*to_remove,]","1f02842d":"# Create directory structure for processed files\ndef modify_directory_structure():\n    if not os.path.exists(\"processed\"):\n        os.mkdir(\"processed\")\n\n    if not os.path.exists(\"processed\/biorxiv_medrxiv\"):\n        os.mkdir(\"processed\/biorxiv_medrxiv\")\n    if not os.path.exists(\"processed\/biorxiv_medrxiv\/pdf_json\"):\n        os.mkdir(\"processed\/biorxiv_medrxiv\/pdf_json\")\n\n    if not os.path.exists(\"processed\/comm_use_subset\"):\n        os.mkdir(\"processed\/comm_use_subset\")\n    if not os.path.exists(\"processed\/comm_use_subset\/pdf_json\"):\n        os.mkdir(\"processed\/comm_use_subset\/pdf_json\")\n    if not os.path.exists(\"processed\/comm_use_subset\/pmc_json\"):\n        os.mkdir(\"processed\/comm_use_subset\/pmc_json\")\n\n    if not os.path.exists(\"processed\/noncomm_use_subset\"):\n        os.mkdir(\"processed\/noncomm_use_subset\")\n    if not os.path.exists(\"processed\/noncomm_use_subset\/pdf_json\"):\n        os.mkdir(\"processed\/noncomm_use_subset\/pdf_json\")\n    if not os.path.exists(\"processed\/noncomm_use_subset\/pmc_json\"):\n        os.mkdir(\"processed\/noncomm_use_subset\/pmc_json\")\n\n    if not os.path.exists(\"processed\/custom_license\"):\n        os.mkdir(\"processed\/custom_license\")\n    if not os.path.exists(\"processed\/custom_license\/pdf_json\"):\n        os.mkdir(\"processed\/custom_license\/pdf_json\")\n    if not os.path.exists(\"processed\/custom_license\/pmc_json\"):\n        os.mkdir(\"processed\/custom_license\/pmc_json\")\n","5b4d5aa6":"#Updating index in batches of 500 to prevent http request timeout\ndef update_index(new_files):\n    j = 0\n    while j < len(new_files): \n        x = []\n        for i in new_files[j:j+500]:\n            if 'PMC' in i:\n                r = json.load(open(i+'.xml.json','r'))\n            else:\n                r = json.load(open(i+'.json','r'))\n            x.append(r)\n        j = j + 500\n        solr.add(x)\n        \n# Remove those documents from the index which have been removed from dataset\ndef clean_index(removable_files):\n    j = 0\n    while j < len(removable_files): \n        x = \"\"\n        for i in removable_files[j:j+10]:\n            x = x + \"id: \" + i.split('\/')[-1] + \" \"\n        j = j + 10 #Update is done in batches of size 10 due to URL size restriction\n        solr.delete(q=x)\n           \ndef handle_changes():\n    modify_directory_structure()\n    new_files = []\n    removable_files = []\n    n1, r1 = preprocess('biorxiv_medrxiv\/','biorxiv_medrxiv\/pdf_json')\n    new_files = new_files + n1\n    removable_files = removable_files + r1\n    print(len(new_files))\n    print(new_files[:10])\n    \n    n1, r1 = preprocess('comm_use_subset\/','comm_use_subset\/pdf_json')\n    new_files = new_files + n1\n    removable_files = removable_files + r1\n    \n    n1, r1 = preprocess('comm_use_subset\/','comm_use_subset\/pmc_json')\n    new_files = new_files + n1\n    removable_files = removable_files + r1\n    print(len(new_files))\n    \n    \n    n1, r1 = preprocess('noncomm_use_subset\/','noncomm_use_subset\/pdf_json')\n    new_files = new_files + n1\n    removable_files = removable_files + r1\n    \n    n1, r1 = preprocess('noncomm_use_subset\/','noncomm_use_subset\/pmc_json')\n    new_files = new_files + n1\n    removable_files = removable_files + r1\n    print(len(new_files))\n    \n    \n    n1, r1 = preprocess('custom_license\/','custom_license\/pdf_json')\n    new_files = new_files + n1\n    removable_files = removable_files + r1\n    \n    n1, r1 = preprocess('custom_license\/','custom_license\/pmc_json')\n    new_files = new_files + n1\n    removable_files = removable_files + r1\n    print(len(new_files))\n    \n    print(str(len(new_files)) + \" new files were found\")\n    print(\"Modifying search index... This might take some time\")\n    print(new_files[:10])\n    update_index(new_files)\n    clean_index(removable_files)\n    print(\"done updating\")","de04277a":"handle_changes()","15c6a882":"def scraper():\n    terms = []\n    for i in range(26):\n        html = \"https:\/\/www.medicinenet.com\/medications\/alpha_\" + chr(97+i) + '.htm'\n        r = requests.get(html)\n        soup = BeautifulSoup(r.content, 'lxml')\n        c = soup.find('div', attrs = {'id':'AZ_container'})\n        z = c.findAll('li')\n        z = [i.text for i in z]\n        terms = terms + z\n    return terms","40977f96":"!git clone https:\/\/github.com\/glutanimate\/wordlist-medicalterms-en\nwordnet_lemmatizer = WordNetLemmatizer()\n\nwords = open('wordlist-medicalterms-en\/wordlist.txt').readlines()\nwords = [wordnet_lemmatizer.lemmatize(i.strip()) for i in words]\nwords = words + scraper() + ['COVID-19']","887446ab":"def query_maker(query_nlp, medical_words):\n    \"\"\"\n        Formulates the query to send to Solr server for retrieving relevant documents.\n    \"\"\"\n    \n    query_words = query_nlp.strip().split()\n    query = \"(body:\"\n    essentials = []\n    \n    for i in query_words:\n        if i in stop_words:\n            continue\n        if i[0]=='+':\n            essentials.append(i)\n        elif wordnet_lemmatizer.lemmatize(i) in medical_words or i in medical_words or i.lower() in medical_words:\n            essentials.append(i)\n        else:\n            query = query + \" \" + i\n    query = query + \")\"\n    if query==\"(body:)\":\n        query=\"\"\n    for i in essentials:\n        if i[0]=='+':\n            query = query + \" body:\" + i[1:] + \"^4\"\n        else:\n            query = query + \" \" + \"+body: \" + i\n    print(query)\n    essesntials = [i for i in essentials if i[0]!='+']\n    return query, essentials","85c0818b":"def get_relevant_docs(query, max_docs=10, show_hits=False):\n    \"\"\"\n        Return contents of the relevant documents and score corresponding to a query\n    \"\"\"\n    result = solr.search(query, rows=max_docs, **{\"fl\":\"*,score\"}) #rows is length of result returned by server\n    doc_names = []\n    for i in result.docs:\n        if i[\"path\"][-1]=='\/':\n            doc_names.append((str(i[\"path\"])+str(i[\"id\"]),i[\"score\"]))\n        else:\n            doc_names.append((str(i[\"path\"])+\"\/\"+str(i[\"id\"]),i[\"score\"]))\n    docs = []\n    if show_hits:\n        print(result.hits)\n    for i in doc_names:\n        if \"pmc\" in i[0]:\n            dname = i[0] + \".xml.json\"\n        else:\n            dname = i[0] + \".json\"\n        docs.append((json.load(open('processed\/'+dname)), i[1]))\n    return docs","bce10aba":"def get_docs(query):\n    q, keywords = query_maker(query, words)\n    kw = [[i]+get_synonyms(i) for i in keywords]\n    result = get_relevant_docs(q,100,True)\n    p_ids = [i[0][\"id\"] for i in result]\n    titles = [i[0][\"title\"] for i in result]\n    abstracts = [i[0][\"abstract\"] for i in result]\n    body_lens = [len(i[0][\"body\"].split()) for i in result]\n    scores = [i[1] for i in result]\n    data = {\"id\":p_ids, \"score\":scores,\"title\":titles,\"length of body\":body_lens, \"abstract\":abstracts}\n    df = pd.DataFrame (data)\n    return result, kw, df","4482657d":"# tokenizer = AutoTokenizer.from_pretrained(\"ktrapeznikov\/biobert_v1.1_pubmed_squad_v2\")\n\n# model = AutoModelForQuestionAnswering.from_pretrained(\"ktrapeznikov\/biobert_v1.1_pubmed_squad_v2\")\n\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"gdario\/biobert_bioasq\")\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"gdario\/biobert_bioasq\")\n\n# from transformers import AutoTokenizer, AutoModelForQuestionAnswering, BertForQuestionAnswering, BertTokenizer\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n\ntokenizer.add_tokens(['corona'])\ntokenizer.encode('assay')\nmodel.resize_token_embeddings(len(tokenizer))\n\n\n\nmodel = model.cuda()","a2cc4125":"import nltk \nfrom nltk.corpus import wordnet \n\n\ndef get_synonyms(word):\n    synonyms = [] \n    for syn in wordnet.synsets(word): \n        for l in syn.lemmas(): \n            synonyms.append(l.name())\n    return synonyms","d9c8d88d":"def has_keywords(context, keywords):\n    c2 = context.lower()\n    k = 0\n    for i in keywords:\n        #print(i)\n        b = False\n        for j in i:\n            if j.lower() in c2:\n                b = True\n                #k = k + 1\n            else:\n                continue\n        if b:\n            k = k + 1\n    return k > (2*len(keywords))\/3","740685bc":"from spacy.lang.en import English # updated\nfrom nltk.corpus import wordnet\nimport re\n\nnlp = English()\nnlp.max_length = 5101418\nnlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\npattern = re.compile(\"^\\[[0-9]*\\]$\")\n\ndef answer(context, question):\n    #context = context.lower()\n    context = context.replace('COVID-19', 'coronavirus disease\/COVID-19')\n    context = context.replace('MERS-CoV', 'MERS-coronavirus')\n    context = context.replace('SARS-CoV-2', 'coronavirus')\n    y = tokenizer.encode(question, context, max_length=256)\n    sep_index = y.index(tokenizer.sep_token_id)\n    num_seg_a = sep_index + 1\n    #print(num_seg_a)\n    num_seg_b = len(y) - num_seg_a\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n    #y = y.cuda()\n    with torch.no_grad():\n        start_scores, end_scores = model(torch.tensor([y]).cuda(), token_type_ids=torch.tensor([segment_ids]).cuda())\n    #start_scores, end_scores = model(torch.tensor([y]), token_type_ids=torch.tensor([segment_ids]))\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n    if answer_start<num_seg_a:\n        return ()\n    else:\n        \n        fstops = [ i for i in range(len(y)) if y[i] == 119]\n        fstops = [num_seg_a] + fstops + [len(y)-1]\n        start = 0\n        for i in fstops:\n            if i + 1 <= answer_start:\n                start = i + 1\n        for i in fstops:\n            if i >= answer_end:\n                end = i\n                break\n        start = max(start, num_seg_a)\n        #print(\"Here\")\n        return [start_scores[0][answer_start] + end_scores[0][answer_end],tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(y[answer_start:answer_end+1])).strip().replace('corona virus', 'coronavirus'), \n                tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(y[start:end+1])).strip().replace('corona virus', 'coronavirus')]           \n    \ndef non_redundant(out, answers):\n    for i in answers:\n        if out[1]==i[1] or out[2]==i[2]:\n            return False\n    return True\n    \ndef get_answer(doc, question, keywords):\n    docx = nlp(doc[\"body\"].replace('\u25a0', ' '))\n    sentences = [sent.string.strip() for sent in docx.sents]\n    i = 0\n    context = \"\"\n    answers = []\n    prev_i = 0\n    while i < len(sentences):  #Use sliding window to search for answers in the document\n        if len(context.split())>=100:\n            if has_keywords(context, keywords): #Search for answers only if the context has all the identified keywords\n                out = answer(context, question)\n                if len(out) > 0 and non_redundant(out, answers) and out[1] not in question:\n                    if not pattern.match(out[1]):\n                        answers.append(out)\n                        prev_i += 2 #Slide the window further if an answer is found\n                        out = []\n            context = \"\"\n            i = prev_i + 1\n            prev_i = i\n        else:\n            context = context + \" \" + sentences[i]\n            i = i + 1\n            if i==len(sentences) and has_keywords(context, keywords):\n                out = answer(context, question)\n                if len(out) > 0 and non_redundant(out, answers) and out[1] not in question:\n                    \n                    if not pattern.match(out[1]):\n                        answers.append(out)\n                        out = []\n    answers = answers[1:]\n    if len(answers) > 0:\n        answers.sort()\n        answers.reverse()\n        answers = answers[:2]\n        answers = [[doc[\"id\"], i] for i in answers]\n        return answers\n    else:\n        return None","a3086133":"retrieval_queries = [ ['Clinical trials to investigate effect viral inhibitors like naproxen on coronavirus patients', \n                       'Clinical trials to investigate effect viral inhibitors like clarithromycin on coronavirus patients', \n                       'Clinical trials to investigate effect viral inhibitors like minocyclineth on coronavirus patients'],\n                     ['evaluate complication of antibody dependent enhancement in vaccine recipients'],\n                     ['animal models for vaccine evaluation and efficacy in trials and prediction for human vaccine'],\n                     ['capabilities to discover a therapeutic for COVID-19',\n                      'clinical effectiveness studies to discover therapeutics for COVID-19',\n                      'clinical effectiveness studies to include antiviral agents for COVID-19'],\n                     ['accelerate production of therapeutics',\n                      'timely and equitable distribution of therapeutics to people'],\n                     ['efforts targeted at universal coronavirus vaccine'],\n                     ['animal models for vaccine evaluation and efficacy in trials and prediction for human vaccine and stadardize challenge studies'],\n                     ['develop prophylaxis for COVID-19',\n                      'clinical studies and prioritize in healthcare workers for COVID-19'],\n                     ['evaluate or assess risk for enhanced disease after vaccination'],\n                     ['assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models with therapeutics']\n                    ]\n\nquestions = [ ['What are the clinical trials to investigate effect of naproxen on coronavirus patients?',\n               'What are the clinical trials to investigate effect of clarithromycin on coronavirus patients?',\n               'What are the clinical trials to investigate effect of monocyclineth on coronavirus patients?'\n              ],\n             ['how to evaluate complications of antibody dependent enhancement in vaccine recipients?'],\n             ['What are the best animal models and their predictive value for human vaccine for coronavirus?'],\n             ['What are the capabilities to discover a therapeutic for COVID-19?',\n             'Which are the clinical effectiveness studies to discover therapeutics for COVID-19?',\n             'Which are the clinical effectiveness studies to include antiviral agents for COVID-19?'],\n             ['How to increase production capacity of therapeutics?',\n             'How were therapeutics distributed to the population'],\n             ['What are the efforts targeted at universal coronavirus vaccine?'],\n             ['What are the efforts to develop animal models and standardize challenge studies?'],\n             ['What are the efforts to develop prophylaxis for COVID-19?',\n             'What are the efforts to develop clinical studies and prioritize in healthcare workers for COVID-19?'],\n             ['What are the approaches to evaluate risk for enhanced disease after vaccination?'],\n             ['What are the assays procedures to evaluate vaccine immune response and process development for vaccines with suitable animal models?']            \n            ]","b43f8e0c":"answer_set = []\nfor i in zip(retrieval_queries, questions):\n    answers = []\n    docs = []\n    for j in zip(i[0],i[1]):\n        result, keywords, df = get_docs(j[0])\n        incl = 0\n        for k in range(min(100, len(result))):\n            x = get_answer(result[k][0], j[1], keywords)\n            torch.cuda.empty_cache()\n            if x is not None:\n                toadd = []\n                for ax in x:\n                    adding = True\n                    for ans in answers:     \n                        if ans[1][1] == ax[1][1] or ans[1][2] == ax[1][2]:\n                            adding=False\n                    if adding:\n                        toadd.append(ax)\n                    \n                if len(toadd) > 0:\n                    incl = incl + 1\n                    answers = answers + toadd\n                if incl == 7:\n                    break\n    answer_set.append(answers)","25d7b551":"len(os.listdir('processed\/custom_license\/pdf_json'))","87627109":"pd.set_option('display.max_colwidth', -1)\ndef display_answer(result):\n    p_ids = [i[0] for i in result]\n    titles = [title_dict[i[0]] for i in result]\n    answers = [i[1][2] for i in result]\n    data = {\"id\":p_ids,\"title\":titles, \"Answer\":answers}\n    df = pd.DataFrame (data)\n    return df","681b760c":"display_answer(answer_set[0])","f930de5f":"display_answer(answer_set[1])","3d25ec9b":"display_answer(answer_set[2])","4340aa95":"display_answer(answer_set[3])","62abcbae":"display_answer(answer_set[4])","b1cbe66b":"display_answer(answer_set[5])","eb4916de":"display_answer(answer_set[6])","46601a7f":"display_answer(answer_set[7])","d4efd466":"display_answer(answer_set[8])","42a5ced2":"display_answer(answer_set[9])","6dedbdaa":"shutil.rmtree('processed')","d593070b":"task_questions = [\n    'Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.',\n    'Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.',\n    'Exploration of use of best animal models and their predictive value for a human vaccine.',\n    'Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.',\n    'Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.',\n    'Efforts targeted at a universal coronavirus vaccine.',\n    'Efforts to develop animal models and standardize challenge studies',\n    'Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers',\n    'Approaches to evaluate risk for enhanced disease after vaccination',\n    'Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]'\n]","a5295a77":"def prepare_submission(answer_set):\n    spids = []\n    stitles = []\n    sanswers = []\n    squestions = []\n    qnum = 0\n    for result in answer_set:\n        p_ids = [i[0] for i in result]\n        titles = [title_dict[i[0]] for i in result]\n        answers = [i[1][2] for i in result]\n        questions = [task_questions[qnum] for i in result]\n        spids = spids + p_ids\n        stitles = stitles + titles\n        sanswers = sanswers + answers\n        squestions = squestions + questions\n        qnum = qnum + 1\n    data = {\"question\":squestions,\"id\":spids,\"title\":stitles, \"Answer\":sanswers}\n    df = pd.DataFrame (data)\n    df.to_csv('submission.csv',index=False)","0dea0693":"prepare_submission(answer_set)","93751e35":"7. Efforts to develop animal models and standardize challenge studies\n","75d4cd06":"# Question Answering\nWe use pretrained BioBert model finetuned for question answering on BioASQ dataset. We use HuggingFace Transformer library which provides an easy to use implementation of a large number of language models. The model has been built using the vocabulary of BertBase thus doesn't have a large number of biological\/medical terms in its vocabulary.","46336d49":"# COVID-19 Kaggle Challenge\n![image.png](attachment:image.png)\n\n**Task: What do we know about vaccines and therapeutics?** \n\nThe goal of this task is to use the given medical data set and find answers to questions related to the ongoing COVID-19 pandemic.\n","e7146f0e":"3. Exploration of use of best animal models and their predictive value for a human vaccine.","3b0c8fa3":"Questions and Queries for the task","d1ef0445":"# Discussion\n\n**Advantage:**\n* Method is scalable and the search index can be built incrementally. The method deals with natural language queries(some terms may require boosting to get better search results) so doesn't require expertise to use \n\n**Drawback:** \n* Our approach uses pretrained model(BioBert with Base vocab). Training the model using domain specific vocabulary or using pretrained model with domain specific vocabulary can improve question answering.\n\n\n**Future directions:**\n* Using language model with vocabulary of medical terms.\n* Indexing sentence vectors(using USE) and searching for relevant text using approximate nearest neighbour method and then applying question answering on relevant text.","907bd704":"# Overview of approach\n\n\n![Untitled%20Diagram.png](attachment:Untitled%20Diagram.png)\n\nThe approach can be divided into two main steps:\n1. Document Retrieval\n2. Question Answering","87a6adc3":"8. Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\n","9af11650":"4. Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\n","c28917cf":"5. Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.","bc4f8511":"9. Approaches to evaluate risk for enhanced disease after vaccination","d2d66789":"10. Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]\n","1c6a08e8":"2. Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\n","720b0b43":"# Findings","c085f77c":"# **Document Search\/Retrieval System**\n\nWe use an [Apache Solr](https:\/\/lucene.apache.org\/solr\/) server hosted on an AWS EC2 instance to retrieve list of relevant documents using the pysolr library.\n\nThe server returns the folder and file name for the relevant documents. The index doesn't store a copy of the text in the document. \n\nIndex was uses synonym check, Stop word removal and Stemming.\n\nDocuments are ranked using BM25\n","63ecb7f6":"1. Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication","bcf39bd9":"6. Efforts targeted at a universal coronavirus vaccine.","bbee4140":"Solr Index can be built incrementally so the index can be modified to include new documents and delete the removed documents. ","8f1bdf9a":"**Handling BERT's input size limitation**\n\n\nBERT has a limit of 512 tokens only while the average length of the paragraphs in the dataset is much higher. To handle this limitation we use a sliding window approach which takes small window and keeps sliding it over the text(sentences) from the retrieved paper searching for answer.\n\nAs the process of searching for answer is time consuming, we have set a criteria for a piece of text to be considered for being searched for answer. The text window has to contain at least 2\/3rd of the keywords(or their synonyms) present in the query. \n\nEven though Bert can take a maximum of 512 tokens at a time, we are not using it to its full capacity, the reason for this is that the accuracy of Bert deteriorates for longer context. ","911ad0c8":"# Document Retrieval query formulation\nTo retrieve data we modify natural language queries based on the occurence of medical terms. To identify the medical terms we compare the terms in query with a list of medical terms and set the terms' occurence mandatory in a document to be retrieved. \n\nWe use two sources for making up the list of medical terms:\n\n1. https:\/\/github.com\/glutanimate\/wordlist-medicalterms-en \n2. https:\/\/www.medicinenet.com\/medications\/alpha_[a-z]\n\nThe first source is available as text file which can be downloaded directly while the second source is scraped to provide list of chemicals and salts used in medicine. We provide code for scraping the data."}}