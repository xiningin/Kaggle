{"cell_type":{"32eb09f6":"code","88140503":"code","cbb580a5":"code","4acdac93":"code","b7247f03":"code","7a4e7acd":"code","6099b0ca":"code","9c8b8f7c":"code","e2e6e1e4":"code","c43d5e9e":"code","b6a48eac":"code","dd604a73":"code","eea2fdd6":"code","84bee12c":"code","9ba706b9":"code","74904994":"markdown","72ac47d3":"markdown","dfae41e4":"markdown"},"source":{"32eb09f6":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport re \nimport scipy\nfrom scipy import sparse\nfrom IPython.display import display\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt \nimport time\nimport scipy.optimize as optimize\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.linear_model import Ridge\nimport zipfile\nimport string\nimport nltk\nimport string\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words(\"english\")) \nlemmatizer = WordNetLemmatizer() ","88140503":"train_csv_zip_path = '..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip'\nwith zipfile.ZipFile(train_csv_zip_path) as zf:\n    zf.extractall('.\/')\n","cbb580a5":"train_csv_path = '.\/train.csv'\nsample_sub_path = '..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv'\ncomments_to_score_path = '..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv'\nval_path='..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv'","4acdac93":"df_train = pd.read_csv(\".\/train.csv\")\ndf_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")","b7247f03":"df_train.head()","7a4e7acd":"def clean_text(text):\n#replace the html characters with \" \"\n    text=re.sub('<.*?>', ' ', text)  \n#remove the punctuations\n    text = text.translate(str.maketrans(' ',' ',string.punctuation))\n#consider only alphabets and numerics\n    text = re.sub('[^a-zA-Z]',' ',text)  \n#replace newline with space\n    text = re.sub(\"\\n\",\" \",text)\n#convert to lower case\n    text = text.lower()\n#split and join the words\n    text=' '.join(text.split())\n    return text\n\ndef stopwords(input_text, stop_words):\n    word_tokens = word_tokenize(input_text) \n    output_text = [w for w in word_tokens if not w in stop_words]\n    output = [] \n    for w in word_tokens: \n        if w not in stop_words:\n            output.append(w)\n            \n    text = ' '.join(output)\n    return text\n\n","6099b0ca":"unrelevant_words = ['wiki','wikipedia','page']\n#Clean step 1, 2 and 3\ndf_train['comment_text'] = df_train['comment_text'].apply(lambda x: ''.join([w for w in clean_text(x) if w not in unrelevant_words]))\n\n#Clean Step 4\ndf_train['comment_text'] = df_train['comment_text'].apply(lambda x: ''.join([w for w in stopwords(x,stop_words)]))\n\n#Clean Step 5\ndf_train['comment_text'] = df_train['comment_text'].apply(lambda x: ''.join([w for w in lemmatizer.lemmatize(x)]))\n\ndf_train.head()","9c8b8f7c":"# Create a score that messure how much toxic is a comment\nrandom_score = {'obscene': 0.20, 'toxic': 0.40, 'threat': 0.6, \n            'insult': 0.65, 'severe_toxic': 0.9, 'identity_hate': 0.9}\n\nfor category in random_score:\n    df_train[category] = df_train[category] * random_score[category]\n\ndf_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].mean(axis=1)\ndf_train['y'] = df_train['score']\n\nmin_len = (df_train['y'] > 0).sum()  # len of toxic comments\ndf_non_tox = df_train[df_train['y'] == 0].sample(n=min_len, random_state=201)  # take non toxic comments\ndf_train_new = pd.concat([df_train[df_train['y'] > 0], df_non_tox])  # make new df\ndf_train_new.head()\n","e2e6e1e4":"n_samples_toxic = len(df_train[df_train['score'] != 0])\nn_samples_normal = len(df_train) - n_samples_toxic\n\nidx_to_drop = df_train[df_train['score'] == 0].index[n_samples_toxic\/\/5:]\ndf_train = df_train.drop(idx_to_drop)\n\nprint(f'Reduced number of neutral text samples from {n_samples_normal} to {n_samples_toxic\/\/5}.')\nprint(f'Total number of training samples: {len(df_train)}')","c43d5e9e":"df_tragets = pd.DataFrame(pd.unique(df_train['score'].values), columns=['target_value']).sort_values(by='target_value', ascending = True).reset_index(drop=True)\nTHRESHOLD = df_tragets['target_value'].quantile(q=0.2)\ndf_train['sentiment'] = df_train['score'].map(lambda x: 1 if x < THRESHOLD else 2 if x < THRESHOLD*2 else 3 if x < THRESHOLD*3 else 4 if x < THRESHOLD*4 else 5)\n\ndf_train = df_train[['comment_text','sentiment']].reset_index(drop=True)\ndf_train","b6a48eac":"tf_idf_vect = TfidfVectorizer(analyzer='word',stop_words= 'english')\nX = tf_idf_vect.fit_transform(df_train['comment_text']).toarray()\nX","dd604a73":"df_test = pd.read_csv(comments_to_score_path)\n\n#Clean step 1, 2 and 3\ndf_test['text'] = df_test['text'].apply(lambda x: ''.join([w for w in clean_text(x) if w not in unrelevant_words]))\n\n#Clean Step 4\ndf_test['text'] = df_test['text'].apply(lambda x: ''.join([w for w in stopwords(x,stop_words)]))\n\n#Clean Step 5\ndf_test['text'] = df_test['text'].apply(lambda x: ''.join([w for w in lemmatizer.lemmatize(x)]))\n\ndf_test.head(3)","eea2fdd6":"tf_idf_vect = TfidfVectorizer(analyzer='word',stop_words= 'english')\nY = tf_idf_vect.fit_transform(df_test['text']).toarray()\nY","84bee12c":"score=[]\nfor i in range(len(df_train['sentiment'])): \n    score.append(df_train['sentiment'][i])","9ba706b9":"# Define initial best params and MAE\\\nfrom numpy import arange\n\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    # Other parameters\n    #'objective':'reg:linear',\n}\ngridsearch_params = [\n    (max_depth, min_child_weight, eta)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n    for eta in arange(0.1,1,0.1)\n]\nfrom sklearn.metrics import mean_absolute_error\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\ndtrain = xgb.DMatrix(X,score)\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight, eta in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}, eta={}\".format(\n                             max_depth,\n                             min_child_weight,\n                             eta ))    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    params['eta'] = eta\n # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        #num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=5\n    )    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight, eta)\n        print(\"Best params: {}, {}, {}, MAE: {}\".format(best_params[0], best_params[1],best_params[2], min_mae))","74904994":"data collecting","72ac47d3":"data preprocessing\n\nfollow the instructions: https:\/\/medium.com\/analytics-vidhya\/text-cleaning-in-natural-language-processing-nlp-bea2c27035a6\n\nData Preprocessing must include the follows:\n\nRemoving HTML characters,ASCII\n\nConvert Text to Lowercase\n\nRemove Punctuation's\n\nRemove Stop words\n\nTokenization\n\nStemming vs Lemmatization\n","dfae41e4":"reference:\n\nhttps:\/\/medium.com\/analytics-vidhya\/text-cleaning-in-natural-language-processing-nlp-bea2c27035a6"}}