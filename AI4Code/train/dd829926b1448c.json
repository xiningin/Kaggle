{"cell_type":{"db9251c4":"code","31f68a70":"code","9451165c":"code","f2ff78e8":"code","3a75ace6":"code","e957eb4f":"code","b0dafda6":"code","c1eda60a":"code","d37da719":"code","fcbbf7bc":"code","e82dc4bc":"code","bc712768":"code","2b1433c3":"code","00652cd1":"code","d99960e0":"code","331d30be":"code","9a60f464":"code","91d55185":"code","d4c43a11":"code","133368aa":"code","35356d22":"markdown","3017f42b":"markdown","cbb02364":"markdown","79eeeff7":"markdown","96b2e77d":"markdown","a3098aaa":"markdown","2063dd55":"markdown","6251589d":"markdown","4d327e5f":"markdown","9275b60d":"markdown","a0ce0f01":"markdown","5462f8ce":"markdown","4bf20a48":"markdown","5d2f51fc":"markdown","526eba77":"markdown"},"source":{"db9251c4":"import os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.utils import plot_model\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","31f68a70":"def predict_n_validate_individual_samples(index):\n    plt.figure()\n    plt.imshow(test_x[index].reshape(28, 28))\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.show()\n    print(\"Actual Value: \", class_names[test_y[index]])\n    print(\"Model Prediction: \", class_names[np.argmax(model.predict(test_x[index].reshape(1, 28, 28, 1)))])\n    \n# code to visualize individual sample\ndef plot_sample(df, index):\n    plt.figure()\n    plt.imshow(df[index].reshape(28, 28))\n    plt.colorbar()\n    plt.grid(False)\n    plt.show()","9451165c":"train_images = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')\ntest_images = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv')\n\nprint(train_images.shape)\nprint(test_images.shape)","f2ff78e8":"train_images.label.value_counts().plot(kind='bar')\nplt.show()","3a75ace6":"# Separate out the label column from the training features\n# This step makes it easier to handle the data for training \n\n# code below using python slicing\n# [:, 1:] - means - all rows, all columns except first column\n# .values converts the pandas dataframe into a numpy array\n\ntrain_x = train_images.iloc[:, 1:].values\ntrain_y = train_images.label.values\n\ntest_x = test_images.iloc[:, 1:].values\ntest_y = test_images.label.values","e957eb4f":"# Assign the class names to a list\n# This helps better understand the class of predicted as well as the\n# displayed image\n\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","b0dafda6":"# Code for visualizing 25 images in the dataset\n# It is always important to frequently visualize and validate the data before training the model\n\nplt.figure(figsize=(10,10)) # create an empty figure\nfor i in range(25): # loop through first 25 rows\n    plt.subplot(5,5,i+1) # create a 5x5 subplot and iterate through each\n    plt.xticks([]) # removing the ticks on x-axis\n    plt.yticks([]) # removing the ticks on y-axis\n    plt.grid(False) # disabling the grid\n    \n    # before plotting the image\n    # the image is reshape into 28x28 shape\n    # this is because the original data is a single 1d array with 784 columns \n    # each indicating a pixel value\n    \n    plt.imshow(train_x[i].reshape(28, 28)) \n    plt.xlabel(class_names[train_y[i]]) # assigning the labels\nplt.show() ","c1eda60a":"# Using the helper function defined earlier\nplot_sample(train_x, 0)","d37da719":"train_x = train_x \/ 255.0\ntest_x = test_x \/ 255.0\n\nprint(\"Sample image after Scaling\")\nplot_sample(train_x, 0)","fcbbf7bc":"# using the train_test_split function from scikit-learn\n# specifying a test size of 20%\ntrain_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2)","e82dc4bc":"# keras Sequential allows us to stack different layers of \n# the model architecture\nmodel = Sequential([\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(10, activation='softmax') # 10 units as we have 10 output classes, softmax activation as we need the probability output\n])","bc712768":"model.compile(optimizer='adam',\n              loss=SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","2b1433c3":"# the model metrics and training performance is captured \n# in variable model_history\nmodel1_history = model.fit(train_x.reshape(train_x.shape[0], 28, 28, 1), \n                    train_y, \n                    epochs=10,\n                    validation_data=(val_x.reshape(val_x.shape[0], 28, 28, 1), val_y),\n                   )","00652cd1":"# Plotting the model architecture\nplot_model(model)","d99960e0":"acc = model1_history.history['accuracy']\nval_acc = model1_history.history['val_accuracy']\nloss = model1_history.history['loss']\nval_loss = model1_history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\n\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","331d30be":"# Evalution on the test set\ntest_loss, test_acc = model.evaluate(test_x.reshape(test_x.shape[0], 28, 28, 1), test_y)\nprint('\\nTest accuracy:', test_acc)","9a60f464":"predict_n_validate_individual_samples(100)","91d55185":"model2 = Sequential([\n    Conv2D(filters=32, kernel_size=3),\n    MaxPooling2D((2, 2)),\n    Dropout(0.2),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(10, activation='softmax') \n])\n\nmodel2.compile(optimizer='adam',\n              loss=SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nmodel2_history = model2.fit(train_x.reshape(train_x.shape[0], 28, 28, 1), \n                    train_y, \n                    epochs=10,\n                    validation_data=(val_x.reshape(val_x.shape[0], 28, 28, 1), val_y),\n                   )","d4c43a11":"acc = model2_history.history['accuracy']\nval_acc = model2_history.history['val_accuracy']\nloss = model2_history.history['loss']\nval_loss = model2_history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\n\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","133368aa":"# Evalution on the test set\ntest_loss, test_acc = model2.evaluate(test_x.reshape(test_x.shape[0], 28, 28, 1), test_y)\nprint('\\nTest accuracy:', test_acc)","35356d22":"## Let's add a Convolution Layer, Pooling Layer and Dropout \ud83e\udde0\n\n*To improve the model we can add a Convolution layer and Pooling Layer* \n \n**For the baseline model\n - ```Convolution layer``` - *Applies mathematical convolution over image and thus helps the neural network focus on specific features in the images.*\n - ```Pooling Layer``` - *Pooling helps reduce the number of trainable parameters in the image, without compromising the information contained in each pixel. <br>There are different approaches to Pooling, we will use MaxPooling where each output will be MAX of a particular region.*\n - ```Dropout``` - *Dropout eliminates random nodes and acts as a regularizer. Adding a dropout layer can help reduce overfitting.*","3017f42b":"## Fashion MNIST - A beginner's guide.. \ud83d\udcdd\n\n*This notebook covers basic steps, a beginner should take while approaching an image classification problem.<br>\nWe begin by building a baseline Neural Network model, then progressively adapts techniques to improve the model evaluation metric.*\n\n**While approaching any machine learning task, there are 3 key steps to be defined**:\n\n1. ```Understanding the problem\/data```\n2. ```Defining the model```\n3. ```Choosing the evaluation metric```\n\n*Building a baseline model gives a solid working framework for the solution. It then becomes much easier to adapt other techniques and iterate throught the framework.*\n\n## Dataset \ud83d\udcbd\n\n*Fashion-MNIST is a dataset of Zalando's article images\u2014consisting of a training set of 60,000 examples and a test set of 10,000 examples. <br>Each example is a 28x28 grayscale image, associated with a label from 10 classes.*\n\n\ud83d\udcd6 **Data description**\n - ```Each row is a separate image```\n - ```Column 1 is the class label.```\n - ```Remaining columns are pixel numbers (784 total).```\n - ```Each value is the darkness of the pixel (1 to 255)```\n \n\ud83c\udff7\ufe0f **Labels**\n - ```0``` T-shirt\/top\n - ```1``` Trouser\n - ```2``` Pullover\n - ```3``` Dress\n - ```4``` Coat\n - ```5``` Sandal\n - ```6``` Shirt\n - ```7``` Sneaker\n - ```8``` Bag\n - ```9``` Ankle boot\n \n\ud83c\udfaf **Goal:** To build an algorithm to accurately predict the clothing category from images. \n\n**How to read this notebook?** \nI have added a brief explanation of the data transformation and modelling techniques used in a **Why**\u2753 section before each code block. <br>This will help understand the need for such a transformation. \nIn addition, I have tried to explain each line of code with comments next to the syntax.","cbb02364":"- *After defining the model we have to compile the model.*\n- *Optimizer decides the technique to be used for adjusting the weights and bias of the learning algorithm.*\n- *Here we use an ```ADAM(Adaptive Moment Estimation)``` optimizer.*\n- *loss = ```SparseCategoricalCrossentropy``` - calculates cross entropy loss between predicted and actual values*","79eeeff7":"<img src=\"https:\/\/i.imgur.com\/BzoPAKJ.png\" width=\"100%\" height=\"30%\">","96b2e77d":"## Conclusion \ud83c\udfcc\ufe0f\u200d\n\n- *The main idea here is to understand the framework for image handling and classification modelling using keras*\n- *We started with a simple neural network and achieved a reasonable accuracy of 88%.*\n- *Later by adding a Convolution, Pooling and Dropout layer the test set accuracy slightly improved to 92%* \ud83d\ude0a\n- *This can be further improved by applying data augmentation techniques and more layers.*\n","a3098aaa":"- *The baseline model has 90% training set accuracy and 88% validation set accuracy*\n- *This accuracy for a simple network with just 10 epochs of training is considerably good. However, let's add more layers and try improving the model*","2063dd55":"**Reshape the data before fitting the model**\n\n**Why**\u2753\n\n- *The model expects a ```4D Tensor``` as input*\n- *Hence we have to reshape the training data.*\n- *At present the training data has 784 columns each containing the pixel values*\n- *By reshaping we convert it to a format = (#of samples, img_rows, img_cols, #of channels)*\n- *i.e. (60000, 28, 28, 1)*","6251589d":"**Rescale\/Normalize the images**\n\n**Why**\u2753\n\n- *The above figure shows a colorbar going from 0 to 255. The highest value is 255. It's always a best approach to rescale\/normalize the data before feeding it to neural networks.<br> Normalizing the data, speeds up learning and leads to faster convergence of the learning algorithm.*","4d327e5f":"\u2714\ufe0fThe above plot shows that the data is well sampled. There is equal distribution of each class.\nThus we can choose ```Accuracy``` as the model evaluation metric.\n\n**Evaluation Metric:** Accuracy\n $$Accuracy = \\frac{1}{n_{samples}}\\Sigma_{i=0}^{n_{samples}-1}1{\\Big({y_i - \\hat{y_i}}\\Big)}$$\n where \n> * $y_i$ : original value\n> * $\\hat{y_i}$ : predicted value\n> * $n$ : number of rows in the test data","9275b60d":"**Creating a validation set**\n\n**Why**\u2753\n\n- *While training a machine learning model, it is a good approach to evaluate the model side-by-side against a set of unseen data.<br> This set used for validation is called a validation set. Here we consider 20% of the training data as the validation set. <br>There are techniques like K-Fold, Stratified K-fold.etc which can be used to create a batch of training and validation samples*","a0ce0f01":"\u2714\ufe0f The scaled image, has a mean close to 0.","5462f8ce":"## Baseline Model - Neural Network (2 layers) \ud83e\udde0\n\n*It's always good to start with a simple base model. This ensures that we have a working framework which can later be *modified to improve the evaluation metric.* \n*At later stages:*\n - *Changes to the data include implementing data augmentation techniques*\n - *Changes to the model include trying out different model architectures by changing the layers*\n \n**For the baseline model** *we will be using a Flatten layer and two Dense layers. With a softmax activation at the final layer as we need the output probability of each class.*\n - ```Flatten layer``` - *transforms the image to a full connected 1-d array of 28x28 = 784 pixels*\n - ```Dense Layer``` - *Fully connected layer*","4bf20a48":"**Check the distribution of target column in the dataset**\n\n**Why**\u2753\n\n- *Checking the distribution of the target column helps us better understand the data. In real world, datasets are often skewed, that is, <br>there will be an ```unequal distribution of samples``` which adversely affects the learning and model performance. In such cases, <br>the model will tend to overfit to a particular set of samples. Techniques like undersampling, oversampling, smote-analysis, etc. <br>help handle such scenarios, however we will not discuss it in detail.*\n- *It also helps define ```the evaluation metric``` to be considered while analyzing the performance.*","5d2f51fc":"#### Helper Functions","526eba77":"### Model Inferencing \ud83c\udfaf\n\n- *It is important to validate the predictions of the model*\n- *This acts as a feedback loop, in tuning the model.*\n\nChoose random samples and compare the predicted value with the ground truth."}}