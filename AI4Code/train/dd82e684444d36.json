{"cell_type":{"12d2f1bb":"code","d4ca7a2d":"code","b1d50f33":"code","c22db486":"code","fd108ccb":"code","5e661ebc":"code","a98c85a4":"code","e1c6ef8e":"code","efaa547c":"code","5d614009":"code","d1bd067c":"code","239684fb":"code","b4669456":"code","18744307":"code","f8ee5dfc":"code","b0ad2139":"code","af89282b":"code","986222a7":"code","d04555e1":"markdown","d1dbd63e":"markdown","3a5cc7a5":"markdown","745f8c09":"markdown","67d2102d":"markdown","c925a4c0":"markdown"},"source":{"12d2f1bb":"import re\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport tensorflow as tf\nimport logging\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras import backend as K\nimport transformers\nfrom transformers import RobertaTokenizer, TFRobertaModel\nfrom kaggle_datasets import KaggleDatasets\ntf.get_logger().setLevel(logging.ERROR)\nfrom kaggle_datasets import KaggleDatasets\nfrom cloud_tpu_client import Client\nfrom sklearn import datasets\nfrom sklearn import model_selection\n\nfrom numba import cuda\n\n\n\nfrom IPython.display import SVG\n\n\nfrom tensorflow.keras.callbacks import Callback\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n\n\nimport tensorflow_addons as tfa\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n\nimport matplotlib.pyplot as plt\n\nfrom transformers import AutoTokenizer,AutoModelForSeq2SeqLM,AutoModel","d4ca7a2d":"#Client().configure_tpu_version(tf.__version__, restart_type='ifNeeded')","b1d50f33":"tf.__version__","c22db486":"transformers.__version__","fd108ccb":"# Configurations\nEPOCHS = 100\n\ntpu_on = 0\ndebug = 1\n# Seed\nSEED = 12345\n\n# Learning rate\nLR = 1e-4\nLR1 = 1e-3\n\n# Verbosity\nVERBOSE = 2\n\n\n# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n","5e661ebc":"if(not debug):\n    print(\"saving tokenizer,model for offline inference...\\n\")\n    save_path = '\/kaggle\/working\/'\n    tokenizer.save_pretrained(save_path)\n    def build_roberta_base_model():\n        transformer = TFRobertaModel.from_pretrained(MODEL)\n\n        return transformer\n\n\n    modelweight = build_roberta_base_model()\n    modelweight.save_weights('tf_model.h5')\n\n    configuration = modelweight.config\n\n    configuration.save_pretrained(save_path)\n\n    del modelweight\n    if(tpu_on):\n        print(\"\\ntpu on, clearing memory for next fold........\\n\\n\\n\")\n        tf.tpu.experimental.initialize_tpu_system(tpu) \n    gc.collect()","a98c85a4":"tr = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntr.excerpt[118]","e1c6ef8e":"max = 0\nfor i in range(len(tr)):\n    if(len(tr.excerpt[i]) > max):\n        max = len(tr.excerpt[i])\n    \nmax","efaa547c":"# https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds\/notebook\n\ndef create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","5d614009":"#LR = 0.000040\ndef build_lrfn(lr_start=0.000410, lr_max=0.00001, \n               lr_min=0.00001, lr_rampup_epochs=7, \n               lr_sustain_epochs=0, lr_exp_decay=.87):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn\n\nplt.figure(figsize=(10, 7))\n\nlrfn = build_lrfn()\nplt.plot([i for i in range(35)], [lrfn(i) for i in range(35)]);","d1bd067c":"from tensorflow.keras.callbacks import *\nfrom tensorflow.keras import backend as K\nimport numpy as np\n\nclass CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https:\/\/arxiv.org\/abs\/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w\/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi\/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1\/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary\/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations\/(2*self.step_size))\n        x = np.abs(self.clr_iterations\/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())","239684fb":"from transformers import TFRobertaForSequenceClassification\n# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\n# This function tokenize the text according to a transformers model tokenizer\ndef regular_encode(texts, tokenizer = None, maxlen = 512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts,\n        padding = 'max_length',\n        truncation = True,\n        max_length = maxlen,\n    )\n    \n    return np.array(enc_di['input_ids'])\n\n# This function encode our training sentences\ndef encode_texts(x_train,tokenizer, x_val, MAX_LEN):\n    x_train = regular_encode(x_train.tolist(), tokenizer = tokenizer, maxlen = MAX_LEN)\n    x_val = regular_encode(x_val.tolist(), tokenizer = tokenizer, maxlen = MAX_LEN)\n    return x_train, x_val\n\n# Function to transform arrays to tensors\ndef transform_to_tensors(x_train, x_val, y_train, y_val,BATCH_SIZE):\n    \n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((x_train, y_train))\n        .repeat()\n        .shuffle(2048)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n    \n    valid_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((x_val, y_val))\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n    \n    return train_dataset, valid_dataset\n\n# Function to build our model\ndef build_roberta_model(max_len = 512,MODEL = None):\n    #transformer = TFRobertaModel.from_pretrained('..\/input\/training-tf-roberta-large-on-mlm\/mlm_tf-roberta-large\/')\n    transformer = TFRobertaModel.from_pretrained(MODEL)\n    #transformer = AutoModel.from_pretrained(MODEL)\n\n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    sequence_output = transformer(input_word_ids)[0]\n  \n    # We only need the cls_token, resulting in a 2d array\n    cls_token = sequence_output[:, 0, :]\n    #cls_token = tf.keras.layers.Dropout(0.3)(cls_token)\n\n    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = [output])\n    ''' \n    cycle = tfa.optimizers.CyclicalLearningRate(\n            initial_learning_rate = LR,\n            maximal_learning_rate =  1e-3,\n            step_size =  200,\n            scale_fn = lambda x: 1.,\n            scale_mode = 'cycle',\n            name = 'CyclicalLearningRate'\n    )\n    '''\n    optimizer = tf.keras.optimizers.Adam(lr = LR)\n    opt = tfa.optimizers.SWA(optimizer, start_averaging=8, average_period=3)\n    model.compile(optimizer = opt ,  #tfa.optimizers.RectifiedAdam(lr = LR)\n                  loss = [tf.keras.losses.MeanSquaredError()],\n                  metrics = [tf.keras.metrics.RootMeanSquaredError()])\n    return model\n","b4669456":"os.listdir('..\/')","18744307":"# Function to build our model\nfrom transformers import RobertaConfig\ndef build_model(max_len = 512,MODEL = None):\n    print(\"extract embedding... \")\n    ''' \n    comment\n    '''\n    config = RobertaConfig(dropout=0.4, attention_dropout=0.4)\n    config.output_hidden_states = False\n    #transformer_model = TFRobertaModel.from_pretrained(MODEL, config = config)\n    \n    transformer_model = TFRobertaModel.from_pretrained('..\/input\/distilroberta-base-pretrained-on-commonlit\/mlm_tfdistilroberta-base\/', config = config)\n\n    #transformer_model = TFRobertaModel.from_pretrained(MODEL)\n\n    #input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n    #input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n    #embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n\n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    embedding_layer = transformer_model(input_word_ids)[0]\n    \n    \n    cls_token = embedding_layer[:,0,:]\n    X =  tfa.layers.GroupNormalization()(cls_token)\n    \n \n    X = tf.keras.layers.Dense(4096, activation='relu')(X)\n   \n   \n    X = tf.keras.layers.Dense(1, activation='linear')(X)\n  \n\n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = [X])\n    ''' \n    for layer in model.layers[:2]:\n        layer.trainable = False\n    '''\n    optimizer = tf.keras.optimizers.Adam(lr = LR)\n    optimizer = tfa.optimizers.SWA(optimizer, start_averaging=30, average_period=3)\n    model.compile(optimizer = optimizer ,  #tfa.optimizers.RectifiedAdam(lr = LR)\n                  loss = [tf.keras.losses.MeanSquaredError()],\n                  metrics = [tf.keras.metrics.RootMeanSquaredError()])\n    return model\n","f8ee5dfc":"\n# Function to train and evaluate our model\ndef train_and_evaluate(trainStrategy = \"stratifiedkfold\",modelType = \"xlm\"):\n    \n    # Read our training data\n    #df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\n    \n    # read training data\n    df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\n    # Create out of folds array to store predictions\n    oof_predictions = np.zeros(len(df))\n    \n    print(trainStrategy,modelType)\n    \n    if(modelType == \"distilroberta\"):\n        print(\"distilroberta base model training........\")\n        # Number of folds for training\n        FOLDS = 5\n        # Max length\n        MAX_LEN = 300\n        # Get the trained model we want to use\n        MODEL = 'distilroberta-base' #jplu\/tf-xlm-roberta-base\n\n        # Let's load our model tokenizer\n        tokenizer = RobertaTokenizer.from_pretrained('..\/input\/distilroberta-base-pretrained-on-commonlit\/mlm_tfdistilroberta-base\/')\n        #tokenizer = AutoTokenizer.from_pretrained(MODEL)\n      \n        BATCH_SIZE = 32 * strategy.num_replicas_in_sync \n        \n        # Learning rate\n        LR = 1e-4\n        LR1 = 1e-3\n\n\n    else:\n        print(\"xlmr base model training........\")\n        # Number of folds for training\n        FOLDS = 5\n        # Max length\n        MAX_LEN = 192\n        # Get the trained model we want to use\n        MODEL = 'jplu\/tf-xlm-roberta-base' #jplu\/tf-xlm-roberta-large google\/mt5-large\n        # Let's load our model tokenizer\n        #tokenizer = RobertaTokenizer.from_pretrained(MODEL)\n        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n        BATCH_SIZE = 16 * strategy.num_replicas_in_sync \n        \n       \n\n    print(MAX_LEN)\n        \n\n    #df = clean_data(df, input_columns ) \n    if (trainStrategy == \"stratifiedkfold\"):\n        \n        # create folds\n        df = create_folds(df, num_splits=FOLDS)\n\n\n        # Seed everything\n        seed_everything(SEED)\n\n        # Initiate kfold object with shuffle and a specific seed\n        #kfold = KFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n        \n        for fold in range(FOLDS):\n         \n            trn_ind = np.where((df['kfold'] != fold))[0]\n            val_ind = np.where((df['kfold'] == fold))[0]\n\n            print(len(trn_ind),'=total training indexes \\n \\n',len(val_ind),'=total validation indexes \\n')\n            print(\"........................\\n\")\n            #print(val_ind)\n            print(f'Training fold {fold + 1}')\n            K.clear_session()\n            # Get text features and target\n            x_train, x_val = df['excerpt'].iloc[trn_ind], df['excerpt'].iloc[val_ind]\n            y_train, y_val = df['target'].iloc[trn_ind].values, df['target'].iloc[val_ind].values\n            # Encode our text with Roberta tokenizer\n            x_train, x_val = encode_texts(x_train,tokenizer, x_val, MAX_LEN)\n            # Function to transform our numpy array to a tf Dataset\n            train_dataset, valid_dataset = transform_to_tensors(x_train, x_val, y_train, y_val,BATCH_SIZE)\n            # Build model\n            with strategy.scope():\n                #model = build_roberta_model(max_len = MAX_LEN,MODEL = MODEL)\n                model =  build_model(max_len = MAX_LEN,MODEL = MODEL)\n            # Model checkpoint\n            ''' \n            count = 0\n            for layer in model.layers:\n                count +=1\n                if count < 5: #freezing first 4 layers\n                    print(\"freezing layers...\")\n                    layer.trainable = False\n            \n            for w in model.get_layer(MODEL).weights:\n                print(\"freezing .......\")\n                w._trainable = False\n            '''\n            \n            '''\n            for layer in model.layer[:2]:\n                layer.trainable = False\n            '''\n            checkpoint = tf.keras.callbacks.ModelCheckpoint(f'Roberta_{modelType}_{trainStrategy}_{SEED}_{fold + 1}.h5', \n                                                            monitor = 'val_root_mean_squared_error', \n                                                            verbose = VERBOSE, \n                                                            save_best_only = True,\n                                                            save_weights_only = True, \n                                                            mode = 'min')\n      \n            es = EarlyStopping(monitor='val_root_mean_squared_error', mode='min', patience=20, \n                       restore_best_weights=True, verbose=1)\n            lr_callback = LearningRateScheduler(lrfn, verbose=1)\n\n            #LR = 0.000040\n\n            reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss',  \n                                        factor=0.15, patience=15, \n                                        verbose=1, mode='min', \n                                        epsilon=0.0001, cooldown=1, min_lr=0.00001)\n\n\n            steps = x_train.shape[0] \/\/ BATCH_SIZE \n            print(\"total steps\",steps)\n            # Set CLR options\n            clr_step_size = steps\n            base_lr = 1e-4\n            max_lr = 1e-3\n            mode='triangular'\n            clr = CyclicLR(base_lr=base_lr, max_lr=max_lr, step_size=clr_step_size, mode=mode)\n\n\n            # Training phase\n            callback_list = [checkpoint,clr] #lr_callback reduceLROnPlat es\n            #model.load_weights(f'Roberta_{modelType}_{trainStrategy}_{SEED}_{fold + 1}.h5')\n            history = model.fit(train_dataset,\n                                batch_size = BATCH_SIZE,\n                                epochs = EPOCHS,\n                                verbose = VERBOSE,\n                                callbacks = [callback_list],\n                                validation_data = valid_dataset,\n                                steps_per_epoch = steps)\n            \n            # Load best epoch weights\n            model.load_weights(f'Roberta_{modelType}_{trainStrategy}_{SEED}_{fold + 1}.h5')\n            # Predict validation set to save them in the out of folds array\n            val_pred = model.predict(valid_dataset)\n            oof_predictions[val_ind] = val_pred.reshape(-1)\n            print(\"\\n\\n\\n\\n\")\n            print('-'*50)\n            del train_dataset, valid_dataset\n            del model\n            if(tpu_on):\n                print(\"\\ntpu on, clearing memory for next fold........\\n\\n\\n\")\n                tf.tpu.experimental.initialize_tpu_system(tpu) \n            gc.collect()\n         \n\n    else:\n        print(trainStrategy,'\\n')\n        # Initiate kfold object with shuffle and a specific seed\n        kfold = KFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n\n\n        for fold, (trn_ind, val_ind) in enumerate(kfold.split(df)):\n\n            print('-'*50)\n            print(f'Training fold {fold + 1}')\n            K.clear_session()\n            # Get text features and target\n            x_train, x_val = df['excerpt'].iloc[trn_ind], df['excerpt'].iloc[val_ind]\n            y_train, y_val = df['target'].iloc[trn_ind].values, df['target'].iloc[val_ind].values\n            # Encode our text with Roberta tokenizer\n            x_train, x_val = encode_texts(x_train,tokenizer, x_val, MAX_LEN)\n            # Function to transform our numpy array to a tf Dataset\n            train_dataset, valid_dataset = transform_to_tensors(x_train, x_val, y_train, y_val,BATCH_SIZE)\n            # Build model\n            \n            with strategy.scope():\n                model = build_roberta_model(max_len = MAX_LEN,MODEL = MODEL)\n            # Model checkpoint\n            checkpoint = tf.keras.callbacks.ModelCheckpoint(f'Roberta_{modelType}_{trainStrategy}_{SEED}_{fold + 1}.h5', \n                                                            monitor = 'val_root_mean_squared_error', \n                                                            verbose = VERBOSE, \n                                                            save_best_only = True,\n                                                            save_weights_only = True, \n                                                            mode = 'min')\n            steps = x_train.shape[0] \/\/ BATCH_SIZE \n            # Set CLR options\n            clr_step_size = steps\n            base_lr = LR\n            max_lr = LR1\n            mode='triangular'\n            clr = CyclicLR(base_lr=base_lr, max_lr=max_lr, step_size=clr_step_size, mode=mode)\n            \n            es = EarlyStopping(monitor='val_root_mean_squared_error', mode='min', patience=20, \n                       restore_best_weights=True, verbose=1)\n            \n            reduceLROnPlat = ReduceLROnPlateau(monitor='val_root_mean_squared_error',  \n                                        factor=0.07, patience=10, \n                                        verbose=1, mode='min', \n                                        epsilon=0.0001, cooldown=1, min_lr=0.00001)\n\n\n            # Training phase\n            callback_list = [checkpoint,reduceLROnPlat] #lr_callback reduceLROnPlat\n            #model.load_weights('..\/input\/commonlit-readability-roberta-tf\/Roberta_Base_123_1.h5')\n         \n            # Training phase\n            history = model.fit(train_dataset,\n                                batch_size = BATCH_SIZE,\n                                epochs = EPOCHS,\n                                verbose = VERBOSE,\n                                callbacks = [callback_list],\n                                validation_data = valid_dataset,\n                                steps_per_epoch = steps)\n\n\n            # Load best epoch weights\n            model.load_weights(f'Roberta_{modelType}_{trainStrategy}_{SEED}_{fold + 1}.h5')\n            # Predict validation set to save them in the out of folds array\n            val_pred = model.predict(valid_dataset)\n            oof_predictions[val_ind] = val_pred.reshape(-1)\n            print(\"\\n\\n\\n\\n\")\n            print('-'*50)\n            del train_dataset, valid_dataset\n            del model\n            if(tpu_on):\n                print(\"\\ntpu on, clearing memory for next fold........\\n\\n\\n\")\n                tf.tpu.experimental.initialize_tpu_system(tpu) \n            gc.collect()\n         \n\n\n    \n\n    print('\\n')\n    print('-'*50)\n    # Calculate out of folds root mean squared error\n    oof_rmse = np.sqrt(mean_squared_error(df['target'], oof_predictions))\n    print(f'Our out of folds RMSE is {oof_rmse}')\n    ","b0ad2139":"%%time\n\ntrain_and_evaluate(trainStrategy=\"kfold\",modelType = \"distilroberta\")#stratifiedkfold\n","af89282b":"%%time\n\n#train_and_evaluate(trainStrategy=\"stratifiedkfold\")\n","986222a7":"\n%%time\n\n\n#train_and_evaluate(trainStrategy=\"stratifiedkfold\")\n","d04555e1":"**Experiment - 3**\n\ntrain xlm roberta base 8 stratified kfolds  ","d1dbd63e":"**Experiment - 1**\n\ntrain distilroberta base 5 stratifiedkfold ","3a5cc7a5":"for training on tpu, use tpu_on = 1. i tried hard on tpu but couldn't get better result after tpu training,compared to gpu training","745f8c09":"**Experiment - 2**\n\ntrain xlmr base 5 kfolds\n\n","67d2102d":"# References : \n1. https:\/\/www.kaggle.com\/ragnar123\/commonlit-readability-roberta-tf\n2. https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds\n3. https:\/\/github.com\/mhmoodlan\/cyclic-learning-rate\/blob\/master\/clr.py\n4. https:\/\/www.kaggle.com\/ajax0564\/training-tf-roberta-on-mlm","c925a4c0":"![talkIsCheap](https:\/\/quotefancy.com\/media\/wallpaper\/3840x2160\/1700728-Linus-Torvalds-Quote-Talk-is-cheap-Show-me-the-code.jpg)"}}