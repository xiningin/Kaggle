{"cell_type":{"f4509373":"code","80d16c4b":"code","d4d208b6":"code","699b363f":"code","0080f866":"code","2b786637":"code","efb6ed55":"code","3ef1ed9e":"code","d93728b9":"code","6377e5ca":"code","4e1cde92":"code","6cfb9549":"code","750d2945":"code","a9a042fe":"code","f3a98fd4":"code","6d5b339c":"code","b7bf42cd":"code","1830f875":"code","013b57ae":"code","7c307899":"code","6b3a9a98":"code","25875c74":"code","fa079572":"code","4ac5d3a0":"code","b458e0ca":"code","8a9e029b":"code","fa36d087":"code","10d22262":"code","98c1eba1":"code","2b02dcbc":"code","12be7061":"code","355bc185":"code","c070c0e3":"markdown","b8ca9032":"markdown","66adc3b8":"markdown","ff0a179d":"markdown","c1535285":"markdown","72f873aa":"markdown","18fddb00":"markdown","e460588c":"markdown","3f037d9d":"markdown","ca2438dd":"markdown","cb5147ff":"markdown","61748696":"markdown","246823a2":"markdown","0db237a1":"markdown","7c908468":"markdown","6cde3d9e":"markdown","6f0265dd":"markdown","7357097c":"markdown","d1a64edf":"markdown","11319d1e":"markdown","9a2c2a25":"markdown","c7965fd1":"markdown","9d4277b9":"markdown","81b36cf9":"markdown","c05878be":"markdown"},"source":{"f4509373":"from IPython.display import Image\nImage(\"..\/input\/Dance_Robots_Comic.jpg\")","80d16c4b":"PART_1_SKIP = True\nPART_2_SKIP = True\nPART_3_SKIP = True","d4d208b6":"import numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport skimage\nfrom PIL import Image\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.util import crop, pad\nfrom skimage.morphology import label\nfrom skimage.color import rgb2gray, gray2rgb\n\nimport os\n\nimport zipfile\nz = zipfile.ZipFile(\"Dancer_Images.zip\", \"w\")","699b363f":"cap = cv2.VideoCapture('..\/input\/Shadow Dancers 1 Hour.mp4')\nprint(cap.get(cv2.CAP_PROP_FPS))","0080f866":"%%time\n\nif PART_1_SKIP == False:\n    try:\n        if not os.path.exists('data'):\n            os.makedirs('data')\n    except OSError:\n        print ('Error: Creating directory of data')\n\n    currentFrame = 0\n    count = 0\n    TRAIN_SIZE = 27000\n    FRAME_SKIP = 2\n    IMG_WIDTH = 96\n    IMG_HEIGHT = 64\n    IMG_CHANNELS = 1\n    X_train = np.zeros((TRAIN_SIZE, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype='float32')\n    \n    video = cv2.VideoWriter('Simple_Shadow_Dancer_Video.avi',cv2.VideoWriter_fourcc(*\"MJPG\"), 30, (IMG_WIDTH, IMG_HEIGHT), False)\n\n    while(count < TRAIN_SIZE):\n        try:\n            ret, frame = cap.read()\n\n            if currentFrame % FRAME_SKIP == 0:\n                count += 1\n                if count % int(TRAIN_SIZE\/10) == 0:\n                    print(str((count\/TRAIN_SIZE)*100)+\"% done\")\n                # preprocess frames\n                img = frame\n                img = rgb2gray(img)\n                img = resize(img, (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), mode='constant', preserve_range=True)\n                img[img > 0.2] = 255\n                img[img <= 0.2] = 0\n                # save frame to zip and new video sample\n                name = '.\/data\/frame' + str(count) + '.jpg'\n                cv2.imwrite(name, img)\n                video.write(gray2rgb(img.astype('uint8')))\n                z.write(name)\n                os.remove(name)\n                # save image to training set if training directly to part 2\n                img = img.astype('float32') \/ 255.\n                X_train[count] = img\n        except:\n            print('Frame error')\n            break\n        currentFrame += 1\n\n    print(str(count)+\" Frames collected\")\n    cap.release()\n    z.close()\n    video.release()","2b786637":"import os\nimport sys\nimport random\nimport warnings\nfrom pylab import imshow, show, get_cmap\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nimport skimage\nfrom PIL import Image\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.util import crop, pad\nfrom skimage.morphology import label\n\nfrom keras.models import Model, load_model, Sequential\nfrom keras.layers import Input, Dense, UpSampling2D, Flatten, Reshape\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\nfrom keras import backend as K\nimport tensorflow as tf\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed","efb6ed55":"%%time\nif PART_1_SKIP:\n    IMG_WIDTH = 96\n    IMG_HEIGHT = 64\n    IMG_CHANNELS = 1\n    INPUT_SHAPE=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n    TRAIN_PATH = '..\/input\/dancer_images\/data\/'\n    train_ids = next(os.walk(TRAIN_PATH))[2]\n    X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype='float32')\n    missing_count = 0\n    print('Getting training images ... ')\n#     sys.stdout.flush()\n    for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n        path = TRAIN_PATH +'frame'+ str(n+1) + '.jpg'\n        try:\n            img = imread(path)\n            img = img.astype('float32') \/ 255.\n            img = resize(img, (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), mode='constant', preserve_range=True)\n            X_train[n-missing_count] = img\n        except:\n            print(\" Problem with: \"+path)\n            missing_count += 1\n\n    print(\"Done! total missing: \"+ str(missing_count))\nelse:\n    INPUT_SHAPE=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)","3ef1ed9e":"for n in range(10,15):\n    imshow(X_train[n].reshape(IMG_HEIGHT,IMG_WIDTH))\n    plt.show()","d93728b9":"def Encoder():\n    inp = Input(shape=INPUT_SHAPE)\n    x = Conv2D(128, (4, 4), activation='elu', padding='same',name='encode1')(inp)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode2')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode3')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode4')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode5')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode6')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode7')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode8')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same',name='encode9')(x)\n    x = Flatten()(x)\n    x = Dense(256, activation='elu',name='encode10')(x)\n    encoded = Dense(128, activation='sigmoid',name='encode11')(x)\n    return Model(inp, encoded)\n\nencoder = Encoder()\nencoder.summary()","6377e5ca":"D_INPUT_SHAPE=[128]\ndef Decoder():\n    inp = Input(shape=D_INPUT_SHAPE, name='decoder')\n    x = Dense(256, activation='elu', name='decode1')(inp)\n    x = Dense(768, activation='elu', name='decode2')(x)\n    x = Reshape((4, 6, 32))(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode3')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode4')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode5')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode6')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (2, 2), activation='elu', padding='same', name='decode7')(x)\n    x = Conv2D(128, (3, 3), activation='elu', padding='same', name='decode8')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (2, 2), activation='elu', padding='same', name='decode9')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode10')(x)\n    x = Conv2D(128, (3, 3), activation='elu', padding='same', name='decode11')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode12')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same', name='decode13')(x)\n    x = Conv2D(16, (2, 2), activation='elu', padding='same', name='decode14')(x)\n    decoded = Conv2D(1, (2, 2), activation='sigmoid', padding='same', name='decode15')(x)\n    return Model(inp, decoded)\n\ndecoder = Decoder()\ndecoder.summary()","4e1cde92":"def Autoencoder():\n    inp = Input(shape=INPUT_SHAPE)\n    x = Conv2D(128, (4, 4), activation='elu', padding='same',name='encode1')(inp)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode2')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode3')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode4')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode5')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode6')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode7')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode8')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same',name='encode9')(x)\n    x = Flatten()(x)\n    x = Dense(256, activation='elu',name='encode10')(x)\n    encoded = Dense(128, activation='sigmoid',name='encode11')(x)\n    x = Dense(256, activation='elu', name='decode1')(encoded)\n    x = Dense(768, activation='elu', name='decode2')(x)\n    x = Reshape((4, 6, 32))(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode3')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode4')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode5')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode6')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (2, 2), activation='elu', padding='same', name='decode7')(x)\n    x = Conv2D(128, (3, 3), activation='elu', padding='same', name='decode8')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (2, 2), activation='elu', padding='same', name='decode9')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode10')(x)\n    x = Conv2D(128, (3, 3), activation='elu', padding='same', name='decode11')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode12')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same', name='decode13')(x)\n    x = Conv2D(16, (2, 2), activation='elu', padding='same', name='decode14')(x)\n    decoded = Conv2D(1, (2, 2), activation='sigmoid', padding='same', name='decode15')(x)\n    return Model(inp, decoded)\n\nmodel = Autoencoder()\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()","6cfb9549":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=4, \n                                            verbose=1, \n                                            factor=0.5,\n                                            min_lr=0.00001)\n\ncheckpoint = ModelCheckpoint(\"Dancer_Auto_Model.hdf5\",\n                             save_best_only=True,\n                             monitor='val_loss',\n                             mode='min')\n\nearly_stopping = EarlyStopping(monitor='val_loss',\n                              patience=8,\n                              verbose=1,\n                              mode='min',\n                              restore_best_weights=True)","750d2945":"class ImgSample(Callback):\n\n    def __init__(self):\n       super(Callback, self).__init__() \n\n    def on_epoch_end(self, epoch, logs={}):\n        sample_img = X_train[50]\n        sample_img = sample_img.reshape(1, IMG_HEIGHT, IMG_WIDTH, 1)\n        sample_img = self.model.predict(sample_img)[0]\n        imshow(sample_img.reshape(IMG_HEIGHT,IMG_WIDTH))\n        plt.show()\n\n\nimgsample = ImgSample()\nmodel_callbacks = [learning_rate_reduction, checkpoint, early_stopping, imgsample]\nimshow(X_train[50].reshape(IMG_HEIGHT,IMG_WIDTH))","a9a042fe":"%%time\nif PART_2_SKIP == False:\n    model.fit(X_train, X_train,\n              epochs=30, \n              batch_size=32,\n              verbose=2,\n              validation_split=0.05,\n            callbacks=model_callbacks)\nelse:\n    model = load_model('..\/input\/Dancer_Auto_Model.hdf5')\n    model.load_weights(\"..\/input\/Dancer_Auto_Weights.hdf5\")","f3a98fd4":"decoded_imgs = model.predict(X_train)","6d5b339c":"plt.figure(figsize=(20, 4))\nfor i in range(5,10):\n    # original\n    plt.subplot(2, 10, i + 1)\n    plt.imshow(X_train[i].reshape(IMG_HEIGHT, IMG_WIDTH))\n    plt.axis('off')\n \n    # reconstruction\n    plt.subplot(2, 10, i + 1 + 10)\n    plt.imshow(decoded_imgs[i].reshape(IMG_HEIGHT, IMG_WIDTH))\n    plt.axis('off')\n \nplt.tight_layout()\nplt.show()","b7bf42cd":"model.save('Dancer_Auto_Model.hdf5')\nmodel.save_weights(\"Dancer_Auto_Weights.hdf5\")","1830f875":"encoder = Encoder()\ndecoder = Decoder()\n\nencoder.load_weights(\"Dancer_Auto_Weights.hdf5\", by_name=True)\ndecoder.load_weights(\"Dancer_Auto_Weights.hdf5\", by_name=True)\n\n\ndecoder.save('Dancer_Decoder_Model.hdf5') \nencoder.save('Dancer_Encoder_Model.hdf5')\n\ndecoder.save_weights(\"Dancer_Decoder_Weights.hdf5\")\nencoder.save_weights(\"Dancer_Encoder_Weights.hdf5\")","013b57ae":"encoder_imgs = encoder.predict(X_train)\nprint(encoder_imgs.shape)\nnp.save('Encoded_Dancer.npy',encoder_imgs)","7c307899":"decoded_imgs = decoder.predict(encoder_imgs[0:11])\n\nplt.figure(figsize=(20, 4))\nfor i in range(5,10):\n    # reconstruction\n    plt.subplot(1, 10, i + 1)\n    plt.imshow(decoded_imgs[i].reshape(IMG_HEIGHT, IMG_WIDTH))\n    plt.axis('off')\n \nplt.tight_layout()\nplt.show()","6b3a9a98":"import numpy as np\nimport pandas as pd\nimport keras as K\nimport random\nimport sqlite3\nimport cv2\nimport os\n\nfrom skimage.color import rgb2gray, gray2rgb\nfrom skimage.transform import resize\nfrom skimage.io import imread, imshow\nimport matplotlib.pyplot as plt\n\nfrom keras.layers import Input, Dropout, Dense, concatenate, Embedding\nfrom keras.layers import Flatten, Activation\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.utils import np_utils\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.layers import LSTM, CuDNNGRU, CuDNNLSTM\nfrom keras.layers import MaxPooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n\nimport warnings\nwarnings.filterwarnings('ignore')","25875c74":"if PART_2_SKIP:\n    Dance_Data = np.load('..\/input\/Encoded_Dancer.npy')\nelse:\n    Dance_Data = encoder_imgs\n\nDance_Data.shape","fa079572":"TRAIN_SIZE = Dance_Data.shape[0]\nINPUT_SIZE = Dance_Data.shape[1]\nSEQUENCE_LENGTH = 70\n\nX_train = np.zeros((TRAIN_SIZE-SEQUENCE_LENGTH, SEQUENCE_LENGTH, INPUT_SIZE), dtype='float32')\nY_train = np.zeros((TRAIN_SIZE-SEQUENCE_LENGTH, INPUT_SIZE), dtype='float32')\nfor i in range(0, TRAIN_SIZE-SEQUENCE_LENGTH, 1 ): \n    X_train[i] = Dance_Data[i:i + SEQUENCE_LENGTH]\n    Y_train[i] = Dance_Data[i + SEQUENCE_LENGTH]\n\nprint(X_train.shape)\nprint(Y_train.shape)","4ac5d3a0":"def get_model():\n    inp = Input(shape=(SEQUENCE_LENGTH, INPUT_SIZE))\n    x = CuDNNLSTM(512, return_sequences=True,)(inp)\n    x = CuDNNLSTM(256, return_sequences=True,)(x)\n    x = CuDNNLSTM(512, return_sequences=True,)(x)\n    x = CuDNNLSTM(256, return_sequences=True,)(x)\n    x = CuDNNLSTM(512, return_sequences=True,)(x)\n    x = CuDNNLSTM(1024,)(x)\n    x = Dense(512, activation=\"elu\")(x)\n    x = Dense(256, activation=\"elu\")(x)\n    outp = Dense(INPUT_SIZE, activation='sigmoid')(x)\n    \n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='mse',\n                  optimizer=Adam(lr=0.0002),\n                  metrics=['accuracy'],\n                 )\n\n    return model\n\nmodel = get_model()\n\nmodel.summary()","b458e0ca":"checkpoint = ModelCheckpoint(\"Ai_Dance_RNN_Model.hdf5\",\n                             monitor='loss',\n                             verbose=1,\n                             save_best_only=True,\n                             mode='min')\n\nearly = EarlyStopping(monitor=\"loss\",\n                      mode=\"min\",\n                      patience=3,\n                     restore_best_weights=True)\n\nmodel_callbacks = [checkpoint, early]","8a9e029b":"%%time\nif PART_3_SKIP == False:\n    model.fit(X_train, Y_train,\n              batch_size=64,\n              epochs=60,\n              verbose=2,\n              callbacks = model_callbacks)\nelse:\n    model = load_model('..\/input\/Ai_Dance_RNN_Model.hdf5')\n    model.load_weights('..\/input\/Ai_Dance_RNN_Weights.hdf5')","fa36d087":"model.save(\"Ai_Dance_RNN_Model.hdf5\")\nmodel.save_weights('Ai_Dance_RNN_Weights.hdf5')","10d22262":"%%time\nDANCE_LENGTH  = 6000\nLOOPBREAKER = 10\n\nx = np.random.randint(0, X_train.shape[0]-1)\npattern = X_train[x]\noutp = np.zeros((DANCE_LENGTH, INPUT_SIZE), dtype='float32')\nfor t in range(DANCE_LENGTH):\n    x = np.reshape(pattern, (1, pattern.shape[0], pattern.shape[1]))\n    pred = model.predict(x)\n    result = pred[0]\n    outp[t] = result\n    new_pattern = np.zeros((SEQUENCE_LENGTH, INPUT_SIZE), dtype='float32') \n    new_pattern[0:SEQUENCE_LENGTH-1] = pattern[1:SEQUENCE_LENGTH]\n    new_pattern[-1] = result\n    pattern = np.copy(new_pattern)\n    ####loopbreaker####\n    if t % LOOPBREAKER == 0:\n        pattern[np.random.randint(0, SEQUENCE_LENGTH-10)] = Y_train[np.random.randint(0, Y_train.shape[0]-1)]","98c1eba1":"if PART_2_SKIP:\n    Decoder = load_model('..\/input\/Dancer_Decoder_Model.hdf5')\n    Decoder.load_weights('..\/input\/Dancer_Decoder_Weights.hdf5')\nelse:\n    Decoder = load_model('Dancer_Decoder_Model.hdf5')\n    Decoder.load_weights('Dancer_Decoder_Weights.hdf5')\n\nDance_Output = Decoder.predict(outp)\nDance_Output.shape","2b02dcbc":"IMG_HEIGHT = Dance_Output[0].shape[0]\nIMG_WIDTH = Dance_Output[0].shape[1]\n\nfor row in Dance_Output[0:10]:\n    imshow(row.reshape(IMG_HEIGHT,IMG_WIDTH))\n    plt.show()","12be7061":"video = cv2.VideoWriter('AI_Dance_Video.avi', cv2.VideoWriter_fourcc(*\"XVID\"), 20.0, (IMG_WIDTH, IMG_HEIGHT),False)\n\nfor img in Dance_Output:\n    img = resize(img, (IMG_HEIGHT,IMG_WIDTH), mode='constant', preserve_range=True)\n    img = img * 255\n    img = img.astype('uint8')\n    video.write(img)\n    cv2.waitKey(50)\n    \nvideo.release()","355bc185":"from IPython.display import Image\nImage(\"..\/input\/Dance_Robots_Comic2.png\")","c070c0e3":"## Save Models and Create Encoded Dataset","b8ca9032":"## Create Compressed Dance Sequences\n\nOur model will look at the last 70 frames and attemp to predict the 71st. As such, sur X variable will be an array of 70 (compressed) frames in sequence and our Y variable will be the 71st frame. This block chops our Dance_Data into such sequences of frames.","66adc3b8":"## Output the Dance\n\nBefore we can save the video, we need to decode the frames back into images using the decoder we made in part 2.","ff0a179d":"# Conclusion\n\nAll and all, I am very pleased with the results. There is plenty of room to grow so I am going continue to improve and use this notebook for building more interesting dance and video bots. I consider this the first of a series of related coding projects.\n\nIf you enjoyed and learned something from this notebook, please like, comment, and check out some of my other coding projects on Kaggle and Github.","c1535285":"## Part 2 Results\n\nThe results are really good, there is only maybe a touch of blurriness around the hands after decoding. The image actually looks better than the binary image we started with. We can confidently proceed knowing that we can encode and decode the images without issue. I am quite happy with these results\n\n### Possible Improvements\n\n- The autoencoder works so well that we could do more without much issue, either compress further or use more detailed images.\n\n- The Autoencoder could be used to make a much much larger training set. Even if the uncompressed images get to big for the memory limit, it is possible to just train the autoencoder on a subset of the images then compress the whole set after. A 128 array is not that big, I don't foresee resource exhaustion errors being an major issue, even for much larger datasets.\n","72f873aa":"## Save Video","18fddb00":"# AI Dance Part 2: Autoencoder Compression\n\nNow that we have the preprocessed frames from the shadow dancer video, we will still need to compress them much further to fit them into our RNN model. Among the many uses of autoencoders is making specialized compression models. In this section, we will train an autoencoder on our dance images and use it to compress the images into a much smaller numpy array, saving the model so that we can decode the images later.","e460588c":"# Part 3 Results\n\nThe results of the video are surprisingly crisp. Even small things like the swish of the skirt or swoop of the hair are caught in the video. Like in the youtube video, these results are pretty overfit and the computer is mostly duplicating the dances. However, there are some interesting variations and deformations in the video. The dancer will sometimes shrink and expand its arms or compress into a blob and reform. Playing with the model or the loopbreaker can lead to some interesting results.\n\n### Possible Improvements\n\n- The RNN model could use more and varied dances to train on. A cheap way to do this is just take more frames from the video in part 1. There is also a 5 hour version of these dancing silhouettes. (However, only 3 hours are usable)\n\n- I don't think that the model needs any more layers, it is large enough as is, but readjusting the shape might make it more efficient. On text data RNNs, I tried using 1D convolution layers with mixed results. (It speeds up the training time a lot but the model is more prone to looping) Might work here though.","3f037d9d":"## Sample the Autoencoder Results\n\nThe reconstructions look pretty close to the originals, then the autoencoder works.","ca2438dd":"## Train RNN Model","cb5147ff":"# AI Dance Part 3: Train AI w\/ RNNs\n\nIf you have read any of my text generating notebooks or know text generating AIs this next part will be familiar with you. If not, here is one of my related notebooks: \n\nThe Pythonic Python Script for Making Monty Python Scripts: https:\/\/www.kaggle.com\/valkling\/pythonicpythonscript4makingmontypythonscripts\n\nFor the dancing AI, the technique is pretty much the same. We will use our compressed pictures to make n length sequences as input that the model will use to predict the n+1 frame in the sequence. The differences are:\n\n- The input\/outputs will not be in one-hot encoding but rather an array of floats between 0 and 1\n\n- We will need a larger brain for our model to make it work.\n\n- We will need to decode the results after to turn them into a usable video.\n","61748696":"# Teaching an AI to Dance\n\nWatch a sample output from this notebook here: https:\/\/youtu.be\/1IvLdoXSoaU\n\nNLP and image CNNs are all the rage right now, here we combine techniques from both to have a computer learn to make it's own dance videos. This notebook is a consolidation of 3 smaller notebooks I made for this project: \n\nPart 1-Video Preprocessing: We will take the frames from a dance video of silhouettes, preprocess them to smaller and simpler, and add them to a zip file in sequence.\n\nPart 2-Autoencoder Compression: To save even more memory for our model, we will compress these frames with an Autoencoder into a much smaller numpy array.\n\nPart 3-Train AI w\/ RNNs: We will put these compressed frames into sequences and train a model to create more.\n\n\nI based this kernel off the project in this youtube video: https:\/\/www.youtube.com\/watch?v=Sc7RiNgHHaE While he does not share his code, the steps expressed in the video were clear enough to piece together this project. Thanks to Kaggle's kernals GPU and some alterations, we can achieve even better results in less time than what is shown in the video. While still pretty computationally expensive for modern computing power, using these techniques for a dancing AI opens up the groundwork for AI to predict on and create all types of different videos.\n\n\n### Skip Training\n\nThe results of the 3 parts are already recorded in the dataset and each part can work independently from each other by loading the pretrained data. Setting the following variables to *True* will skip the training for that part and just use the pretrained data instead. *False* will train through that step.\n\nTop to bottom, this whole notebook takes around ~4 hours to train with all 3 skips set to *False*. \n","246823a2":"## Part 1 Results\n\nThe dancer comes out clearly but a bit blocky. There is a bit of dirt in the frames but not too much and less than in the youtube video. The arms occasionally clip during quick motions, but that happens a lot in the original video as well just from the normal green screen clipping.\n\n### Possible Improvements\n\n- The frames of black and dirt as the video transitions to new dancers could be controlled and cleaned for\n\n- Changing the binary threshold from 0.2. This can be a tradeoff between getting more of the dancer's pixels and picking up more dirt from the background.\n\n- To that effect, careful use of Image thresholding packages might work to cut out the dancer from the background too (but also might not work due to the constantly changing background in the video)\n\n- It is always an option to take larger and\/or more frames of dancing for training, as long as we still got memory for it.\n","0db237a1":"## Preprocess the Video\n\nIn this step we will take each frame in the video and add them to a zip file in sequence. We will also preprocess the frames in the following way to save space and make it easier for are models to process them later:\n\n-Only take every other frame: We don't need every frame and will mean that we won't need to use as many frames to look further back in time during with the RNN model later.\n\n-Turn the image to grayscale: Don't need color channels at all to capture a shadow.\n\n-Resize the image to 64 by 96 pixels: Much smaller file size and 64 by 96 is easily divided which makes it easier to structure the autoencoder without data loss later\n\n-Make it binary: turn the lighter pixels white and the darker pixels black. This simplifies the input to the most important pixels. (The blockiness gets evened out as we go through the next sections.)\n","7c908468":"## Callbacks","6cde3d9e":"## Decode a Sample to Double Check Results\n\nIf the encoder and decoder models are working correctly, the dancer should appear like in the reconstruction of the autoencoder above.","6f0265dd":"## Create the Models\n\nIn addition to the Autoencoder model, we will also prepare an encoder and decoder for later. It is important to give the layers the same unique names and shapes in all 3 as we will be using the keras load_weights by_name option to copy our trained Autoencoder weights to each respective layer later.","7357097c":"## Callbacks","d1a64edf":"### Custom Image Sample Callback\n\nHere is a custom callback I made named ImgSample. It tests the result of the autoencoder after every epoch by desplaying an sample image. The goal is to have the dancer come into focus as clearly as possible.","11319d1e":"## Generate New Computer Generated Dances\n\nThis block generates new dance sequences in the style of the video of DANCE_LENGTH size in frames. It takes a random seed pattern from the training set, predicts the next frame, adds it to the end of the pattern and drops the first frame of the pattern and predicts on the new pattern and so forth. The default DANCE_LENGTH of 6000 frames is 5 minutes of video at 20 FPS.\n\nPretty much the AI will try to accurately duplicate the Dance video but inevitably makes errors, and those errors compound, but is still trained well enough that it ends up making similar, but not quite the same, dances.\n\nThe LOOPBREAKER is used to add noise to the prediction pattern, replacing a random frame in the pattern with a random frame in the Dance_Data after every LOOPBREAKER frames. This noise can be used to force the AI to change up what it is doing. This can stop undertrained models from looping or overtrained models from duplication the training data too closely. Setting it too low, on the other hand, can cause the results to distort more. It is worth playing around with this setting and is a quick and dirty way to adjust the dance output post training.","9a2c2a25":"# AI Dance Part 1: Video Preprocessing\n\nWe will be using the same video of training as in the youtube video. It is a >1 hour of go-go dancing female silhouettes. This is ideal as most other green screen dancing videos are too short, loops, and\/or messy for easy preprocessing. While not greatly varied, this dancing is not looped and we do not have to cobble together multiple dance videos (which might also require different preprocessing steps for each).\n\nHere is the original video: https:\/\/www.youtube.com\/watch?v=NdSqAAT28v0","c7965fd1":"## Train the Autoencoder","9d4277b9":"## Read in Images","81b36cf9":"## Read in Data\n\nWhen processing this type of model on text data, each character is expressed in one hot arrays between ~50-100, (depending on the unique characters in the text to consider). Our data is in 128 numpy arrays, so it is not that much more load on our model to consider our compressed images over single characters of a text document.","c05878be":"## Create the RNN Model\n\nThe model is simply 6 LSTM layers stacked on top of each other. While text data only needs around 2-4 LSTM layers to work, the dance data benifits from a few more as the result is not categorical this time and a large brain allows for more \"creativity\"(variation) on the AIs part. (Note: CuDNNLSTM layers are just LSTM layers that automatically optimize for the GPU. They run a lot faster than standard LSTM layers at the cost of customization options)"}}