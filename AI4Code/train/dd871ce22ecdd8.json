{"cell_type":{"3c68586f":"code","5f0bab68":"code","6b7ed2b3":"code","23580e53":"code","0bf5085d":"code","cdceb553":"code","5f6a550a":"code","0fd917c3":"code","199987e1":"code","83c092be":"code","687a5634":"code","ab0929a4":"code","d444b6bc":"code","f43a5c72":"code","aa248441":"markdown","ea43b3eb":"markdown","c181a1e1":"markdown","063aba93":"markdown","8d6f7199":"markdown","744f2da6":"markdown","7fb1a185":"markdown","78204baa":"markdown","17d9a45a":"markdown","2eec82d6":"markdown","422ab99d":"markdown","6789648b":"markdown","db8dedf6":"markdown"},"source":{"3c68586f":"import numpy as np\nimport pandas as pd\nimport os\nimport tokenizers\nimport string\nimport torch\nimport re\nimport transformers\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\nfrom IPython.core.display import display, HTML\n","5f0bab68":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').head(100).fillna('')\ntrain.head()","6b7ed2b3":"import pandas as pd\nfrom sklearn import model_selection\n\n\ntrain = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/train.csv\")\ntrain = train.dropna().reset_index(drop=True)\ntrain[\"kfold\"] = -1\n\n\nkf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (trn_, val_) in enumerate(kf.split(X=train, y=train.sentiment.values)):\n    print(len(trn_), len(val_))\n    train.loc[val_, 'kfold'] = fold\n\ntrain.to_csv(\"train_folds.csv\", index=False)","23580e53":"MAX_LEN = 96\nROBERTA_PATH = '..\/input\/roberta-base\/'\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=ROBERTA_PATH+'vocab.json', \n    merges_file=ROBERTA_PATH+'merges.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\n\n\nEPOCHS = 2\nTRAIN_BATCH_SIZE = 64\nVALID_BATCH_SIZE = 64\nTRAINING = False\n\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\nsentiment_tar = {'positive': 1, 'negative': 2, 'neutral': 0}\ndevice = torch.device('cuda')\nDO_QUES_ANS = False","0bf5085d":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len, do_ques_ans=False):\n    \n    # FIND TEXT \/ SELECTED_TEXT OVERLAP\n    tweet = \" \" + \" \".join(str(tweet).split())\n    selected_text =  \" \".join(str(selected_text).split())\n    \n    \n    start_idx = tweet.find(selected_text)\n    end_idx = start_idx + len(selected_text)\n    \n    char_targets = [0] * len(tweet)\n    if start_idx != None and end_idx != None:\n        for ct in range(start_idx, end_idx):\n            char_targets[ct] = 1  \n            \n    tok = tokenizer.encode(tweet)\n    tweet_ids = tok.ids\n    \n    # OFFSETS, CHAR CENTERS\n    tweet_offsets = tok.offsets\n    char_centers = [(offset[0] + offset[1]) \/ 2 for offset in tweet_offsets]\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n            \n#     print(target_idx)\n    \n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    stok = [sentiment_id[sentiment]]\n    \n    \n    input_ids = [0] + tweet_ids + [2]\n    token_type_ids = [0] + [0] * (len(tweet_ids) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] + tweet_offsets + [(0, 0)]\n    targets_start += 1\n    targets_end += 1\n        \n    if do_ques_ans:\n        input_ids = [0] + stok + [2] + [2] + tweet_ids + [2]\n        token_type_ids = [0, 0, 0, 0] + [0] * (len(tweet_ids) + 1)\n        mask = [1] * len(token_type_ids)\n        tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n        targets_start += 4\n        targets_end += 4\n\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n#         print(char_centers)\n        char_centers = char_centers + ([0] * padding_length)\n    \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets,\n        'char_cent': char_centers,\n        'sentiment_tar': sentiment_tar[sentiment]\n    }\n\n\nclass TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item], \n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n\n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_tweet': data[\"orig_tweet\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long),\n            'char_cent': torch.tensor(data['char_cent'], dtype=torch.long),\n            'sentiment_tar': data['sentiment_tar']\n        }","cdceb553":"class TweetModel(transformers.BertPreTrainedModel):\n    \"\"\"BERT model for QA and classification tasks.\n    \n    Parameters\n    ----------\n    config : transformers.BertConfig. Configuration class for BERT.\n    Returns\n    -------\n    classifier_logits : torch.Tensor with shape (batch_size, num_classes).\n        Classification scores of each labels.\n    \"\"\"\n\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        self.roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.classifier = nn.Linear(768, 3)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        hidden_states, pooled_output, _ = self.roberta(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids)\n        \n\n        # classification\n        pooled_output = self.drop_out(pooled_output)\n        classifier_logits = self.classifier(pooled_output)\n\n        return hidden_states, classifier_logits","5f6a550a":"import torch.nn as nn\nimport torch\nimport numpy as np\n\ndef loss_fn(output, target):\n    loss_func = nn.CrossEntropyLoss()\n    loss = loss_func(output, target)\n    return loss\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels, device):\n    pred_flat = np.argmax(preds, axis=1).flatten() \n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)\n\n\nclass AverageMeter():\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n        \n        \nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.0003):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score","0fd917c3":"def train_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n    losses = AverageMeter()\n    accuracy_score = AverageMeter()\n\n    tk0 = tqdm(data_loader, total=len(data_loader))\n\n    accr = 0\n    \n    for bi, d in enumerate(tk0):\n        ids = d['ids']\n        token_type_ids = d['token_type_ids']\n        mask = d['mask']\n        targets_start = d['targets_start']\n        targets_end = d['targets_end']\n        sentiment = d['sentiment']\n        orig_selected = d['orig_selected']\n        orig_tweet = d['orig_tweet']\n        targets_start = d['targets_start']\n        targets_end = d['targets_end']\n        offsets = d['offsets']\n        targets = d['sentiment_tar']\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.long)\n\n        model.zero_grad()\n        _, outputs = model(\n            input_ids=ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n            \n        outputs = torch.sigmoid(outputs).cpu().detach().numpy().tolist()\n        outputs = np.argmax(outputs, axis=1)\n        targets = targets.cpu().detach().numpy()\n\n        accuracy = metrics.accuracy_score(targets, outputs)\n        accuracy_score.update(accuracy.item(), ids.size(0))\n        losses.update(loss.item(), ids.size(0))\n        tk0.set_postfix(loss=losses.avg, accuracy = accuracy_score.avg)\n    return losses.avg, accuracy_score.avg\n        \n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    losses = AverageMeter()\n    accuracy_score = AverageMeter()\n\n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d['ids']\n            token_type_ids = d['token_type_ids']\n            mask = d['mask']\n            targets_start = d['targets_start']\n            targets_end = d['targets_end']\n            sentiment = d['sentiment']\n            orig_selected = d['orig_selected']\n            orig_tweet = d['orig_tweet']\n            targets_start = d['targets_start']\n            targets_end = d['targets_end']\n            offsets = d['offsets'].numpy()\n            targets = d['sentiment_tar']\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.long)\n            targets_end = targets_end.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.long)\n\n            _, outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n                token_type_ids=token_type_ids\n            )\n\n            loss = loss_fn(outputs, targets)\n            outputs = torch.sigmoid(outputs).cpu().detach().numpy().tolist()\n            outputs = np.argmax(outputs, axis=1)\n            targets = targets.cpu().detach().numpy().tolist()\n\n            accuracy = metrics.accuracy_score(targets, outputs)\n            accuracy_score.update(accuracy.item(), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n            tk0.set_postfix(loss=losses.avg, accuracy = accuracy_score.avg)\n\n\n    return losses.avg, accuracy_score.avg","199987e1":"def run(fold):\n    dfx = pd.read_csv('train_folds.csv')\n\n    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n\n    train_dataset = TweetDataset(\n        tweet=df_train.text.values,\n        sentiment=df_train.sentiment.values,\n        selected_text=df_train.selected_text.values\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        num_workers=0\n    )\n\n    valid_dataset = TweetDataset(\n        tweet=df_valid.text.values,\n        sentiment=df_valid.sentiment.values,\n        selected_text=df_valid.selected_text.values\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=0\n    )\n    \n    \n    device = torch.device('cuda')\n    model_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\n    model_config.output_hidden_states = True\n    model = TweetModel(conf=model_config)\n    model.to(device)\n\n    num_train_steps = int(len(df_train) \/ TRAIN_BATCH_SIZE * EPOCHS)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0, \n        num_training_steps=num_train_steps\n    )\n    \n    es = EarlyStopping(patience=2, mode='max')\n    print(f\"Training is Starting for fold={fold}\")\n\n    for epoch in range(EPOCHS):\n        train_loss, train_acc = train_fn(train_data_loader, model, optimizer, device, scheduler)\n#         print(f'Training loss is {train_loss}, Training Accuracy is {train_acc}')\n        val_loss, val_acc = eval_fn(valid_data_loader, model, device)\n        print(f'Validation loss is {val_loss}, Validation Accuracy is {val_acc}')\n        es(val_acc, model, model_path=f'model_{fold}.bin')\n        if es.early_stop:\n            print('Early Stopping')\n            break","83c092be":"# training only for fold = 0\n\nif TRAINING:\n    run(0)\n","687a5634":"model_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\nmodel_config.output_hidden_states = True\nmodel = TweetModel(conf=model_config)\n\nif TRAINING:\n    model.load_state_dict(torch.load('model_0.bin'))\nelse:\n    model.load_state_dict(torch.load('..\/input\/roberta-base-cam\/model_0.bin'))\n    \nmodel.to(device)","ab0929a4":"pr = {0:'NEUTRAL',1:'POSITIVE',2:'NEGATIVE'}","d444b6bc":"import os\nimport argparse\n\nimport cv2\nimport torch\nimport numpy as np\nfrom torch.nn import functional as F\nimport torchvision.transforms as transforms\n\n\n\ndef create_cam(model, device):\n    dfx = pd.read_csv('train_folds.csv')\n    df_train = dfx[dfx.kfold != fold].reset_index(drop=True).tail(90)\n\n    train_dataset = TweetDataset(\n        tweet=df_train.text.values,\n        sentiment=df_train.sentiment.values,\n        selected_text=df_train.selected_text.values\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=0\n    )\n    \n    \n    finalconv_name = 'classifier'\n    # hook\n    feature_blobs = []\n    def hook_feature(module, input, output):\n        feature_blobs.append(output.cpu().data.numpy())\n\n    model._modules.get(finalconv_name).register_forward_hook(hook_feature)\n    params = list(model.parameters())\n    # get weight only from the last layer(linear)\n    weight_softmax = np.squeeze(params[-2].cpu().data.numpy())\n    \n\n    \n    for bi, d in enumerate(train_data_loader):\n        \n        ids = d['ids']\n        token_type_ids = d['token_type_ids']\n        mask = d['mask']\n        sentiment = d['sentiment']\n        orig_tweet = d['orig_tweet']\n        targets = d['sentiment_tar']\n        orig_selected = d['orig_selected']\n        char_cent = d['char_cent']\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.long)\n        char_cent = char_cent.to(device, dtype=torch.long)\n\n        hidden_states, outputs = model(\n            input_ids=ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n    \n        mask = mask.cpu().detach().numpy().tolist()  \n        char_cent = char_cent.cpu().detach().numpy().tolist()  \n    \n        for index in range(0, ids.shape[0]):\n\n            last_conv_output = hidden_states[index]\n            pred_vec = outputs[index]\n            last_conv_output = np.squeeze(last_conv_output)\n            last_conv_output = torch.sigmoid(last_conv_output).cpu().detach().numpy().tolist() \n\n            pred_vec = torch.sigmoid(pred_vec).cpu().detach().numpy().tolist()  \n            pred = np.argmax(pred_vec)\n            layer_weights = weight_softmax[pred, :]\n            final_output = np.dot(last_conv_output, layer_weights)\n            \n            \n            if len(orig_tweet[index])>95: continue #too wide for display\n            if pr[pred]!=sentiment[index].upper(): continue #skip misclassified\n\n            # PLOT INFLUENCE VALUE\n            print()\n            plt.figure(figsize=(20,3))\n            \n            idx = np.sum(mask[index])\n            v = np.argsort(final_output[:idx-1])\n\n            mx = final_output[v[-1]]; x = max(-10,-len(v))\n            mn = final_output[v[x]]\n            \n            plt.plot(char_cent[index][:idx-2],final_output[1:idx-1],'o-')\n            plt.plot([1,95],[mn,mn],':')\n            plt.xlim((0,95))\n            plt.yticks([]); plt.xticks([])\n            plt.title(f'Predict label is {pr[pred]} True label is {sentiment[index]}',size=16)\n            plt.show()\n\n            # DISPLAY ACTIVATION TEXT\n            html = ''\n            for j in range(1,idx):\n                x = (final_output[j]-mn)\/(mx-mn)\n                html += \"<span style='background:{};font-family:monospace'>\".format('rgba(255,255,0,%f)'%x)\n                tokenizer = TOKENIZER\n                html += tokenizer.decode( [ids[index][j]] )\n                html += \"<\/span>\"\n            html += \" (predict)\"\n            display(HTML(html))\n\n\n            # DISPLAY TRUE SELECTED TEXT\n            tweet = \" \".join(orig_tweet[index].lower().split()) \n            selected_text = \" \".join(orig_selected[index].lower().split())\n            sp = tweet.split(selected_text)\n            html = \"<span style='font-family:monospace'>\"+sp[0]+\"<\/span>\"\n            for j in range(1,len(sp)):\n                html += \"<span style='background:yellow;font-family:monospace'>\"+selected_text+'<\/span>'\n                html += \"<span style='font-family:monospace'>\"+sp[j]+\"<\/span>\"\n            html += \" (true)\"\n            display(HTML(html))\n            print()\n","f43a5c72":"create_cam(model, device)","aa248441":"## Loading Saved Model","ea43b3eb":"## Utils","c181a1e1":"# Load Data","063aba93":"## Train and Evaluation Functions","8d6f7199":"## Preparing DataLoaders","744f2da6":"# Display Influential Subtext\nThe code to highlight the text with different colors is from notebook [here][1]. The plot above each sentence indicates the strength of influence of the words below. The x axis is the sentence and the y axis is the strenth of influence in determining the sentiment prediction.\n\n[1]: https:\/\/www.kaggle.com\/jeinsong\/html-text-segment-visualization","7fb1a185":"# Train Model\n\nWe have divided the train data into 5 folds, and used fold 0 for training. Our model achieves 80% validation accuracy predicting `sentiment` from Tweet `text`.","78204baa":"# roBERTa Tokenization\n\nBelow tokenization code logic is inspired by Abhishek's PyTorch notebook [here](https:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds)\n","17d9a45a":"## Create Train and Validation Folds","2eec82d6":"# roBERTa Model\nWe use pretrained roBERTa base model and add a classification head.","422ab99d":"# CAM Extraction Model\nAn example of CAM extraction for image recognition in pytorch is [here][1]. The basic idea is as follows. When our model classifies a text, it will activate one of three classification outputs (pos, neg, neu). We trace that output backwards to see which words contributed the most to the decision.\n\n[1]: https:\/\/github.com\/jacobgil\/pytorch-grad-cam","6789648b":"# Pytorch roBERTa - Unsupervised Text Selection\nWow, this notebook does not use `selected_text`. This notebook only uses the columns `text` and `sentiment`. We train a roBERTa model to predict sentiment (pos, neg, neu) from Tweet `text` achieving 80% accuracy. We then display what part of the text was influencial in deciding whether the text was postive, negative, or neutral. This is **unsupervised** learning because we are learning the `selected_text` without using the `selected_text`.\n\nThis is the PyTorch version of this great notebook by Chris Deotte [here](https:\/\/www.kaggle.com\/cdeotte\/unsupervised-text-selection)\n\nIf you like this work, please upvote the below kernels, as it's there idea.\n\n* https:\/\/www.kaggle.com\/nkoprowicz\/a-simple-solution-using-only-word-counts\n* https:\/\/www.kaggle.com\/cdeotte\/unsupervised-masks-cv-0-60\n* https:\/\/www.kaggle.com\/cdeotte\/unsupervised-text-selection\n* https:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds","db8dedf6":"## Load Data"}}