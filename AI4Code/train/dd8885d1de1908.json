{"cell_type":{"ed813646":"code","ca4f57ff":"code","cee6e3d1":"code","6f35b688":"code","df475fb3":"code","ec50f6c1":"code","e4bfe16e":"code","18e748a9":"code","0c77f0af":"code","efd26d1c":"code","e43a36f9":"code","c07ff199":"code","830d2dfe":"code","c403b39e":"code","9b1980ac":"code","2e76feed":"code","9f8edc90":"code","31a9711d":"code","84a33a8d":"code","3fd1b5b6":"code","b1c2da4d":"code","a9c8accd":"code","f5f291ea":"code","dc7f9eee":"code","daaa6fa2":"code","ad61a084":"code","0bd4074e":"code","f931e2b0":"code","faeff95e":"code","dfd70a25":"code","57100d84":"markdown","e31defca":"markdown","14d9837e":"markdown","4e4ae6a5":"markdown","6125b4b1":"markdown","25b125ff":"markdown","226aabc6":"markdown"},"source":{"ed813646":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","ca4f57ff":"# to visualise al the columns in the dataframe\npd.pandas.set_option('display.max_columns', None)\ndataset=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/housetrain.csv\")\n","cee6e3d1":"dataset.head()","6f35b688":"## Always remember there way always be a chance of data leakage so we need to split the data first and then apply feature\n## Engineering\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(dataset,dataset['SalePrice'],test_size=0.1,random_state=0)","df475fb3":"X_train.shape, X_test.shape","ec50f6c1":"## Let us capture all the nan values\n## First lets handle Categorical features which are missing\nfeatures_nan=[feature for feature in dataset.columns if dataset[feature].isnull().sum()>1 and dataset[feature].dtypes=='O']\n\nfor feature in features_nan:\n    print(\"{}: {}% missing values\".format(feature,np.round(dataset[feature].isnull().mean(),4)))","e4bfe16e":"\n## Replace missing value with a new label\ndef replace_cat_feature(dataset,features_nan):\n    data=dataset.copy()\n    data[features_nan]=data[features_nan].fillna('Missing')\n    return data\n\ndataset=replace_cat_feature(dataset,features_nan)\n\ndataset[features_nan].isnull().sum()","18e748a9":"dataset.head()","0c77f0af":"\n## Now lets check for numerical variables the contains missing values\nnumerical_with_nan=[feature for feature in dataset.columns if dataset[feature].isnull().sum()>1 and dataset[feature].dtypes!='O']\n\n## We will print the numerical nan variables and percentage of missing values\n\nfor feature in numerical_with_nan:\n    print(\"{}: {}% missing value\".format(feature,np.around(dataset[feature].isnull().mean(),4)))","efd26d1c":"\n## Replacing the numerical Missing Values\n\nfor feature in numerical_with_nan:\n    ## We will replace by using median since there are outliers\n    median_value=dataset[feature].median()\n    \n    ## create a new feature to capture nan values\n    dataset[feature+'nan']=np.where(dataset[feature].isnull(),1,0)\n    dataset[feature].fillna(median_value,inplace=True)\n    \ndataset[numerical_with_nan].isnull().sum()","e43a36f9":"\ndataset.head(10)","c07ff199":"\n## Temporal Variables (Date Time Variables)\n\nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n       \n    dataset[feature]=dataset['YrSold']-dataset[feature]","830d2dfe":"\ndataset.head()","c403b39e":"dataset[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()","9b1980ac":"dataset.head()","2e76feed":"import numpy as np\nnum_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\n\nfor feature in num_features:\n    dataset[feature]=np.log(dataset[feature])","9f8edc90":"dataset.head()","31a9711d":"categorical_features=[feature for feature in dataset.columns if dataset[feature].dtype=='O']","84a33a8d":"categorical_features","3fd1b5b6":"for feature in categorical_features:\n    temp=dataset.groupby(feature)['SalePrice'].count()\/len(dataset)\n    temp_df=temp[temp>0.01].index\n    dataset[feature]=np.where(dataset[feature].isin(temp_df),dataset[feature],'Rare_var')","b1c2da4d":"dataset.head(10)","a9c8accd":"for feature in categorical_features:\n    labels_ordered=dataset.groupby([feature])['SalePrice'].mean().sort_values().index\n    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}\n    dataset[feature]=dataset[feature].map(labels_ordered)","f5f291ea":"dataset.head(10)","dc7f9eee":"\nscaling_feature=[feature for feature in dataset.columns if feature not in ['Id','SalePerice'] ]\nlen(scaling_feature)","daaa6fa2":"scaling_feature","ad61a084":"dataset.head()","0bd4074e":"feature_scale=[feature for feature in dataset.columns if feature not in ['Id','SalePrice']]\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nscaler.fit(dataset[feature_scale])","f931e2b0":"scaler.transform(dataset[feature_scale])","faeff95e":"# transform the train and test set, and add on the Id and SalePrice variables\ndata = pd.concat([dataset[['Id', 'SalePrice']].reset_index(drop=True),\n                    pd.DataFrame(scaler.transform(dataset[feature_scale]), columns=feature_scale)],\n                    axis=1)","dfd70a25":"data.head()","57100d84":"## Please upvote","e31defca":"# Feature Scaling","14d9837e":"##  Missing value","4e4ae6a5":"## What is feature engineering\n + Feature engineering is the process of using domain knowledge to extract\n   features from raw data via data mining techniques. \n   \n + These features can be used to improve the performance of machine learning algorithms.","6125b4b1":"## Numerical Variables\n\nSince the numerical variables are skewed we will perform log normal distribution","25b125ff":"# We will be performing all the below steps in Feature Engineering\n\n+ Missing values\n\n+ Temporal variables\n\n+ Categorical variables: remove rare labels\n\n+ Standarise the values of the variables to the same range","226aabc6":"# Handling Rare Categorical Feature\n\nWe will remove categorical variables that are present less than 1% of the observations"}}