{"cell_type":{"897e4437":"code","a616ba0d":"code","ec5c2023":"code","819d8137":"code","ed2f4a05":"code","25967c07":"code","433458f3":"code","7f35b222":"code","973939d0":"code","beabdca3":"code","c6c6be87":"code","a1a02155":"code","9b428950":"code","dd1143b6":"code","5b3e84f0":"markdown","56fe01a4":"markdown","90394fe0":"markdown","cecfb5c4":"markdown","9120226a":"markdown","b8e2e890":"markdown","1451fb0a":"markdown","405dc505":"markdown","a68cd1a2":"markdown"},"source":{"897e4437":"import os\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nimport itertools\nfrom sklearn.metrics import *","a616ba0d":"X_train = pd.read_csv(\"..\/input\/X_train.csv\")\nX_test = pd.read_csv(\"..\/input\/X_test.csv\")\ny_train = pd.read_csv(\"..\/input\/y_train.csv\")\nsub = pd.read_csv(\"..\/input\/sample_submission.csv\")","ec5c2023":"X_train.head()","819d8137":"plt.figure(figsize=(15, 5))\nsns.countplot(y_train['surface'])\nplt.title('Target distribution', size=15)\nplt.show()","ed2f4a05":"# https:\/\/www.kaggle.com\/jesucristo\/1-smart-robots-complete-notebook-0-73\/notebook\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(X_train.iloc[:,3:].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","25967c07":"#https:\/\/www.kaggle.com\/prashantkikani\/help-humanity-by-helping-robots\n\ndef fe(data):\n    \n    df = pd.DataFrame()\n    data['totl_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 +\n                             data['angular_velocity_Z']**2)** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 +\n                             data['linear_acceleration_Z'])**0.5\n    data['totl_xyz'] = (data['orientation_X']**2 + data['orientation_Y']**2 +\n                             data['orientation_Z'])**0.5\n   \n    data['acc_vs_vel'] = data['totl_linr_acc'] \/ data['totl_anglr_vel']\n    \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        df[col + '_maxtoMin'] = df[col + '_max'] \/ df[col + '_min']\n        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])\/2\n    return df\n","433458f3":"%%time\nX_train = fe(X_train)\nX_test = fe(X_test)\nprint(X_train.shape)","7f35b222":"X_train.head()","973939d0":"le = LabelEncoder()\ny_train['surface'] = le.fit_transform(y_train['surface'])","beabdca3":"X_train.fillna(0, inplace = True)\nX_test.fillna(0, inplace = True)\nX_train.replace(-np.inf, 0, inplace = True)\nX_train.replace(np.inf, 0, inplace = True)\nX_test.replace(-np.inf, 0, inplace = True)\nX_test.replace(np.inf, 0, inplace = True)","c6c6be87":"def k_folds(X, y, X_test, k):\n    folds = StratifiedKFold(n_splits = k, shuffle=True, random_state=2019)\n    y_test = np.zeros((X_test.shape[0], 9))\n    y_oof = np.zeros((X.shape[0]))\n    score = 0\n    for i, (train_idx, val_idx) in  enumerate(folds.split(X, y)):\n        clf =  RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n        clf.fit(X_train.iloc[train_idx], y[train_idx])\n        y_oof[val_idx] = clf.predict(X.iloc[val_idx])\n        y_test += clf.predict_proba(X_test) \/ folds.n_splits\n        score += clf.score(X.iloc[val_idx], y[val_idx])\n        print('Fold: {} score: {}'.format(i,clf.score(X.iloc[val_idx], y[val_idx])))\n    print('Avg Accuracy', score \/ folds.n_splits) \n        \n    return y_oof, y_test ","a1a02155":"y_oof, y_test = k_folds(X_train, y_train['surface'], X_test, k= 50)","9b428950":"from sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_oof,y_train['surface'])","dd1143b6":"y_test = np.argmax(y_test, axis=1)\nsubmission = pd.read_csv(os.path.join(\"..\/input\/\",'sample_submission.csv'))\nsubmission['surface'] = le.inverse_transform(y_test)\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(10)","5b3e84f0":"Here we can discover strong correlation between angular_velocity_Z and angular_velocity_Y.\n","56fe01a4":"### Load Data","90394fe0":"### Submission","cecfb5c4":"### Ouput\n\nWe encode our targets","9120226a":"## Modeling","b8e2e890":"This kernel is based on [Theo Viel](https:\/\/www.kaggle.com\/theoviel\/deep-learning-starter) , [Gabriel Preda](https:\/\/www.kaggle.com\/gpreda\/robots-need-help), [Nanashi](https:\/\/www.kaggle.com\/jesucristo\/1-smart-robots-complete-notebook-0-73\/notebook) and [Vansh Jatana](https:\/\/www.kaggle.com\/vanshjatana\/help-humanity-by-helping-robots-4e306b\/notebook). Here I applied RandomForestClassifier,  set random_state higher and kfold to 50.","1451fb0a":"### Feature Engineer","405dc505":"## Reference\n[Gabriel Preda](https:\/\/www.kaggle.com\/gpreda\/robots-need-help)  \n[Theo Viel](https:\/\/www.kaggle.com\/theoviel\/deep-learning-starter)  \n[Nanashi](https:\/\/www.kaggle.com\/jesucristo\/1-smart-robots-complete-notebook-0-73)\n##### Please leave an upvote, it is always appreciated!","a68cd1a2":"### $k$-Folds"}}