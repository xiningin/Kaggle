{"cell_type":{"26f8bec4":"code","60085bbf":"code","bea61f17":"code","8e878c45":"code","d997bea9":"code","4c1600d6":"code","7785fc73":"code","f46771a3":"code","7e4f5774":"code","758c000b":"code","3f3e94e0":"code","250ba108":"code","10dde84c":"code","a62a9499":"code","d9f29f45":"code","a7a15c88":"markdown","36425ede":"markdown","269643e0":"markdown","447468f2":"markdown","a8e9b64a":"markdown","0227a24e":"markdown","a3d6deb2":"markdown"},"source":{"26f8bec4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")","60085bbf":"df = pd.read_csv(\"..\/input\/bigmart\/final_bigmart_imputted.csv\")\ndf.head()","bea61f17":"df.drop(df.columns[[4, 5, 6, 7, 10, 11, 12, 13, 14]], axis = 1, inplace = True)","8e878c45":"df","d997bea9":"sns.countplot(x='Outlet_Type', data=df)","4c1600d6":"categorical_col = []\nfor column in df.columns:\n    if df[column].dtype == object and len(df[column].unique()) <= 50:\n        categorical_col.append(column)\n        \ndf['Outlet_Type'] = df.Outlet_Type.astype(\"category\").cat.codes","7785fc73":"categorical_col.remove('Outlet_Type')","f46771a3":"# Transform categorical data into dummies\n# categorical_col.remove(\"Attrition\")\n# data = pd.get_dummies(df, columns=categorical_col)\n# data.info()\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel = LabelEncoder()\nfor column in categorical_col:\n    df[column] = label.fit_transform(df[column])","7e4f5774":"from sklearn.model_selection import train_test_split\n\nX = df.drop('Outlet_Type', axis=1)\ny = df.Outlet_Type\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","758c000b":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","3f3e94e0":"from sklearn.tree import DecisionTreeClassifier\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(X_train, y_train)\n\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=False)","250ba108":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n    \"criterion\":(\"gini\", \"entropy\"), \n    \"splitter\":(\"best\", \"random\"), \n    \"max_depth\":(list(range(1, 20))), \n    \"min_samples_split\":[2, 3, 4], \n    \"min_samples_leaf\":list(range(1, 20)), \n}\n\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_cv = GridSearchCV(tree_clf, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=3)\ntree_cv.fit(X_train, y_train)\nbest_params = tree_cv.best_params_\nprint(f\"Best paramters: {best_params})\")\n\ntree_clf = DecisionTreeClassifier(**best_params)\ntree_clf.fit(X_train, y_train)\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=False)","10dde84c":"from IPython.display import Image\nfrom six import StringIO\nfrom sklearn.tree import export_graphviz\nimport pydot\n\nfeatures = list(df.columns)\nfeatures.remove(\"Outlet_Type\")","a62a9499":"dot_data = StringIO()\nexport_graphviz(tree_clf, out_file=dot_data, feature_names=features, filled=True)\ngraph = pydot.graph_from_dot_data(dot_data.getvalue())\nImage(graph[0].create_png())","d9f29f45":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(n_estimators=100)\nrf_clf.fit(X_train, y_train)\n\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=False)","a7a15c88":"## 2. Decision Tree Classifier Hyperparameter tuning","36425ede":"### Visualization of a tree","269643e0":"#Decision Tree & Random Forest Implementation in python for BigMart\n\nWe will use Decision Tree & Random Forest in Predicting the outlet type for Bigmart dataset","447468f2":"# Applying Tree & Random Forest algorithms","a8e9b64a":"# Exploratory Data Analysis","0227a24e":"# Data Processing","a3d6deb2":"## 1. Decision Tree Classifier\n\n**Decision Tree parameters:**\n- `criterion`: The function to measure the quality of a split. Supported criteria are \"`gini`\" for the Gini impurity and \"`entropy`\" for the information gain.\n***\n- `splitter`: The strategy used to choose the split at each node. Supported strategies are \"`best`\" to choose the best split and \"`random`\" to choose the best random split.\n***\n- `max_depth`: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than `min_samples_split` samples.\n***\n- `min_samples_split`: The minimum number of samples required to split an internal node.\n***\n- `min_samples_leaf`: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression.\n***\n- `min_weight_fraction_leaf`: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n***\n- `max_features`: The number of features to consider when looking for the best split.\n***\n- `max_leaf_nodes`: Grow a tree with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n***\n- `min_impurity_decrease`: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n***\n- `min_impurity_split`: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf."}}