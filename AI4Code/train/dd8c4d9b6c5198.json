{"cell_type":{"d5c4620f":"code","20b95017":"code","76fd7ab8":"code","d59a3fc6":"code","1156acd4":"code","bca8f8e8":"code","1755eb9d":"code","9a64fc69":"code","9ab5688e":"code","56941077":"code","0a36265e":"code","4479187b":"code","3075f1a8":"code","241ff49c":"markdown","6ec83aaf":"markdown","8f624bc3":"markdown","95bcdac2":"markdown","36c2bb48":"markdown","499a022d":"markdown","af6b7e4b":"markdown","acd0e3ef":"markdown","ab4f3ab1":"markdown","e9ba5b04":"markdown","665a634a":"markdown","28a65814":"markdown","7959d1bd":"markdown","ff906a02":"markdown","2cc7c7b3":"markdown","bbb99e29":"markdown","03e90ccd":"markdown","4497e978":"markdown","03412a7d":"markdown"},"source":{"d5c4620f":"import gym # openAi gym\nfrom gym import envs\nimport numpy as np \nimport datetime\nimport keras \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom time import sleep\n\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\nfrom rl.memory import SequentialMemory\nfrom rl.core import Processor\nfrom rl.callbacks import FileLogger, ModelIntervalCheckpoint\nprint(\"OK\")","20b95017":"print(envs.registry.all())\n","76fd7ab8":"env = gym.make('Taxi-v2')\nenv.reset()\nenv.render()","d59a3fc6":"# Let's first do some random steps in the game so you see how the game looks like\n\nrew_tot=0\nobs= env.reset()\nenv.render()\nfor _ in range(6):\n    action = env.action_space.sample() #take step using random action from possible actions (actio_space)\n    obs, rew, done, info = env.step(action) \n    rew_tot = rew_tot + rew\n    env.render()\n#Print the reward of these random action\nprint(\"Reward: %r\" % rew_tot)    \n    ","1156acd4":"# action space has 6 possible actions, the meaning of the actions is nice to know for us humans but the neural network will figure it out\nprint(env.action_space)\nNUM_ACTIONS = env.action_space.n\nprint(\"Possible actions: [0..%a]\" % (NUM_ACTIONS-1))\n\n","bca8f8e8":"print(env.observation_space)\nprint()\nenv.env.s=42 # some random number, you might recognize it\nenv.render()\nenv.env.s = 222 # and some other\nenv.render()","1755eb9d":"# Value iteration algorithem\nNUM_ACTIONS = env.action_space.n\nNUM_STATES = env.observation_space.n\nV = np.zeros([NUM_STATES]) # The Value for each state\nPi = np.zeros([NUM_STATES], dtype=int)  # Our policy with we keep updating to get the optimal policy\ngamma = 0.9 # discount factor\nsignificant_improvement = 0.01\n\ndef best_action_value(s):\n    # finds the highest value action (max_a) in state s\n    best_a = None\n    best_value = float('-inf')\n\n    # loop through all possible actions to find the best current action\n    for a in range (0, NUM_ACTIONS):\n        env.env.s = s\n        s_new, rew, done, info = env.step(a) #take the action\n        v = rew + gamma * V[s_new]\n        if v > best_value:\n            best_value = v\n            best_a = a\n    return best_a\n\niteration = 0\nwhile True:\n    # biggest_change is referred to by the mathematical symbol delta in equations\n    biggest_change = 0\n    for s in range (0, NUM_STATES):\n        old_v = V[s]\n        action = best_action_value(s) #choosing an action with the highest future reward\n        env.env.s = s # goto the state\n        s_new, rew, done, info = env.step(action) #take the action\n        V[s] = rew + gamma * V[s_new] #Update Value for the state using Bellman equation\n        Pi[s] = action\n        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n    iteration += 1\n    if biggest_change < significant_improvement:\n        print (iteration,' iterations done')\n        break","9a64fc69":"# Let's see how the algorithm solves the taxi game\nrew_tot=0\nobs= env.reset()\nenv.render()\ndone=False\nwhile done != True: \n    action = Pi[obs]\n    obs, rew, done, info = env.step(action) #take step using selected action\n    rew_tot = rew_tot + rew\n    env.render()\n#Print the reward of these actions\nprint(\"Reward: %r\" % rew_tot)  ","9ab5688e":"NUM_ACTIONS = env.action_space.n\nNUM_STATES = env.observation_space.n\nQ = np.zeros([NUM_STATES, NUM_ACTIONS]) #You could also make this dynamic if you don't know all games states upfront\ngamma = 0.9 # discount factor\nalpha = 0.9 # learning rate\nfor episode in range(1,1001):\n    done = False\n    rew_tot = 0\n    obs = env.reset()\n    while done != True:\n            action = np.argmax(Q[obs]) #choosing the action with the highest Q value \n            obs2, rew, done, info = env.step(action) #take the action\n            Q[obs,action] += alpha * (rew + gamma * np.max(Q[obs2]) - Q[obs,action]) #Update Q-marix using Bellman equation\n            #Q[obs,action] = rew + gamma * np.max(Q[obs2]) # same equation but with learning rate = 1 returns the basic Bellman equation\n            rew_tot = rew_tot + rew\n            obs = obs2   \n    if episode % 50 == 0:\n        print('Episode {} Total Reward: {}'.format(episode,rew_tot))","56941077":"# Let's see how the algorithm solves the taxi game by following the policy to take actions delivering max value\n\nrew_tot=0\nobs= env.reset()\nenv.render()\ndone=False\nwhile done != True: \n    action = np.argmax(Q[obs])\n    obs, rew, done, info = env.step(action) #take step using selected action\n    rew_tot = rew_tot + rew\n    env.render()\n#Print the reward of these actions\nprint(\"Reward: %r\" % rew_tot)  ","0a36265e":"# Let's show the game layout\n\nenv = gym.make('FrozenLake-v0')\nrew_tot=0\nobs= env.reset()\nenv.render()\n","4479187b":"env = gym.make('FrozenLake-v0')\nenv.reset()\nNUM_ACTIONS = env.action_space.n\nNUM_STATES = env.observation_space.n\nQ = np.zeros([NUM_STATES, NUM_ACTIONS]) #You could also make this dynamic if you don't know all games states upfront\ngamma = 0.95 # discount factor\nalpha = 0.01 # learning rate\nepsilon = 0.1 #\nfor episode in range(1,500001):\n    done = False\n    obs = env.reset()\n    while done != True:\n        if np.random.rand(1) < epsilon:\n            # exploration with a new option with probability epsilon, the epsilon greedy approach\n            action = env.action_space.sample()\n        else:\n            # exploitation\n            action = np.argmax(Q[obs])\n        obs2, rew, done, info = env.step(action) #take the action\n        Q[obs,action] += alpha * (rew + gamma * np.max(Q[obs2]) - Q[obs,action]) #Update Q-marix using Bellman equation\n        obs = obs2   \n        \n    if episode % 5000 == 0:\n        #report every 5000 steps, test 100 games to get avarage point score for statistics and verify if it is solved\n        rew_average = 0.\n        for i in range(100):\n            obs= env.reset()\n            done=False\n            while done != True: \n                action = np.argmax(Q[obs])\n                obs, rew, done, info = env.step(action) #take step using selected action\n                rew_average += rew\n        rew_average=rew_average\/100\n        print('Episode {} avarage reward: {}'.format(episode,rew_average))\n        \n        if rew_average > 0.8:\n            # FrozenLake-v0 defines \"solving\" as getting average reward of 0.78 over 100 consecutive trials.\n            # Test it on 0.8 so it is not a one-off lucky shot solving it\n            print(\"Frozen lake solved\")\n            break\n ","3075f1a8":"# Let's see how the algorithm solves the frozen-lakes game\n\nrew_tot=0.\nobs= env.reset()\ndone=False\nwhile done != True: \n    action = np.argmax(Q[obs])\n    obs, rew, done, info = env.step(action) #take step using selected action\n    rew_tot += rew\n    env.render()\n\nprint(\"Reward:\", rew_tot)  ","241ff49c":"# Reinforcement learning developments\nAfter the first publication of DQN many deeplearning Reinforcement Learning algorithms have been invented\/tried, Some main ones in chronological order:  DQN, Double DQN, Duelling DQN, Deep Deterministic Policy Gradient, Continuous DQN (CDQN or NAF) ,  A2C\/A3C,  Proximal Policy Optimization Algorithms, etc, etc. Let's see if we can also try them out on Kaggle and have some fun.  \nIn my next notebook you will find the instructions to install gym[atari] so you are able to unleash the power op A.I. on these iconic Atari games like breakout, pacman, spaceinvaders, etc.\n\nhttps:\/\/www.kaggle.com\/charel\/learn-by-example-reinforcement-learning-gym-setup  \n\n\nHoped you liked my notebook (upvote top right), my way to conribute back to this fantastic Kaggle platform and community.\n","6ec83aaf":"A nice place to test this is in the game [Frozen lakes](https:\/\/gym.openai.com\/envs\/FrozenLake-v0\/) of OpenAI\/Gym.  \n\n Short description: \"Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\"  \n\nNotice that the game is not deterministic anymore: \"won't always move in the direction you intend\". Note it is really slippery, the chance you move into the direction you want is actually not that big\n\nS- Start  \nG - Goal  \nF- Frozen (safe)  \nH- Hole (dead)  ","8f624bc3":"# Markov decision process(MDP)\nThe Taxi game is an example of an [Markov decision process ](https:\/\/en.wikipedia.org\/wiki\/Markov_decision_process). The game can be described in states, possible actions in a state (leading to a next state with a certain probability) with a reward.\n\nThe word Markov refers to [Markovian property](https:\/\/en.wikipedia.org\/wiki\/Markov_property) which means that the state is independent of any previous states history, not on the sequence of events that preceded it. The current state encapsulates all that is needed to decide the future actions, no memory needed.\n\n[small video to explain Markov model](https:\/\/www.youtube.com\/watch?v=EqUfuT3CC8s)  \n\nIn terms of Reinforcement Learning the Environment is modelled as a markov model and the agent needs to take actions in this environment to maximize the amount of reward. Since the agent sees only the outside of the environment (the effects of it actions) it is often referred to as the hidden markov model which needs to be learned.","95bcdac2":"# Policy\nPolicy (\u03c0): The strategy that the agent employs to determine next action 'a'  in state 's'. Note that it does not state if it is a good or bad policy, it is a policy.   The policy is normally noted with the greek letter \u03c0.\nOptimal policy (\u03c0*), policy which maximizes the expected reward. Among all the policies taken, the optimal policy is the one that optimizes to maximize the amount of reward received or expected to receive over a lifetime.  \n\nSo how do we find the optimal policy (\u03c0*) that is maximize our reward (and win the game) given the Taxi environment with the Markov model .  \n","36c2bb48":"# Basic Q-learning algorithm  \nThe[ Q-learning](https:\/\/en.wikipedia.org\/wiki\/Q-learning) algorithm is centred around the actor (in this case the Taxi) and starts exploring based on trial-and-error to update its knowledge about the model and hence path to the best reward.\nThe core of the idea is the Q-matrix Q(s, a). It contains the maximum discounted future reward when we perform action a in state s. Or in other words Q(s, a) gives estimates the best course of action a in state s. Q-learning learns by trail and error and updates its policy (Q-matrix) based on reward.  to state it simple: the best it can do given a state it is in.  \nAfter every step we update Q(s,a) using the reward, and the max Q value for new state resulting from the action. This update is done using the action value formula (based upon the Bellman equation) and allows state-action pairs to be updated in a recursive fashion (based on future values).   \nThe Bellman equation is extended with a learning rate (if you put learning rate = 1 it comes back to the basic Bellman equation) :\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/f0\/Q-l%C3%A6ring_formel_1.png\/800px-Q-l%C3%A6ring_formel_1.png\">\n\nSource: [Wikimedia Commons](https:\/\/commons.wikimedia.org\/wiki\/File:Q-l%C3%A6ring_formel_1.png)\n\nNote: [Temporal difference learning ](https:\/\/en.wikipedia.org\/wiki\/Temporal_difference_learning) and [Sarsa](https:\/\/en.wikipedia.org\/wiki\/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action) algorithems explored simular value expressions. Q-learning was the basis for Deep Q-learning (Deep referring to Neural Network technology). so, let's see how the Q-learning algorithm works.\n","499a022d":"# The Taxi game\nThere are four designated locations in the grid world indicated by R(ed), B(lue), G(reen), and Y(ellow). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drive to the passenger's location, pick up the passenger, drive to the passenger's destination (another one of the four specified locations), and then drop off the passenger. Once the passenger is dropped off, the episode ends. The taxi cannot pass thru a wall.\n\nActions: \nThere are 6 discrete deterministic actions:\n    - 0: move south\n    - 1: move north\n    - 2: move east \n    - 3: move west \n    - 4: pickup passenger\n    - 5: dropoff passenger\n    \nRewards: \nThere is a reward of -1 for each action and an additional reward of +20 for delievering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n    \nRendering:\n    - blue: passenger\n    - magenta: destination\n    - yellow: empty taxi\n    - green: full taxi\n    - other letters: locations\n\n\nhttps:\/\/gym.openai.com\/envs\/Taxi-v2\/\n\n\n","af6b7e4b":"# Model vs Model-free based methods\nIt nicely solves it. Still it feels a bit like cheating in terms of reinforcement learning. We have to know all environment states\/transitions upfront so the algorithm works. In RL literature they refer to it as model based methods.\nWhat if not all states are known upfront and we need to find out while we are learning. Hence we enter the realm of model-free based methods.\n\n ","acd0e3ef":"# Value iteration algorithm\n So' let's start first in \"memory lane\", the early days of reinforcement learning. Value iteration is the \"hello world\" of reinforcement learning methods to find an optimal policy to maximize reward for e.g. a Markov decision process problem.   \n\nThe value iteration is centred around the game states. The core of the idea is to calculate the value (expected long-term maximum result) of each state. The algorithm loops over all states (s) and possible actions (a) to explore rewards of a given action and calculates the maximum possible action\/reward and stores it in V[s]. The algorithm iterates\/repeats until V[s] is not (significantly) improving anymore. The Optimal policy (\u03c0*) is then to take every time the action to go state with the highest value. This value iteration algorithm is an example of what is referred to as [dynamic programming](https:\/\/en.wikipedia.org\/wiki\/Dynamic_programming) (DP) in literature. There are other DP techniques to solve this like policy iteration, etc but it can also be solved by a recursive program (a function that calls itself, look at the Bellman equation, it is a recursive definition).  \nAnyhow, lets see how this value iteration works.\n\n","ab4f3ab1":"# Interacting with the Gym environment  \nSource: [OpenAI](https:\/\/openai.com\/)  \nOPenAI\/gym makes it relative straightforward to interact with the game.  \n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*7Ae4mf9gVvpuMgenwtf8wA.png\">\n\nEach timestep, the agent chooses an action, and the environment returns an observation and a reward.  \n\n*observation, reward, done, info = env.step(action) *  \n* observation (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game line Taxi.\n* reward (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n* done (boolean): whether it\u2019s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n* info (dict): ignore, diagnostic information useful for debugging. Official evaluations of your agent are not allowed to use this for learning.  \n\nLet's first do some random steps in the game so you see how the game looks like\n","e9ba5b04":"Let's start learning using some build-in games in Gym that do not require additonal installs. Let's start with a very basic game called Taxi.\n","665a634a":"# DQN\nNice, isn't it. In case of 500 observation states and 6 actions (Taxi game) or 16 observations and 4 moves (Frozen-lake game) the Q-matrix is a manageable matrix. Imagine you got a full Atari game screen of pixels as an observation and it becomes quickly visible the Q-matrix solution will not cope. Also the Q-learning agent does not have the ability to estimate value for unseen states, it has no clue which action to take and goes back to random action as best.  \n\nTo deal with these problems, Deep Q-Network (DQN) removes the two-dimensional Q-matrix by introducing a Neural Network. So it leverages a Neural Network to estimate the Q-value function. The input for the network is the current game state, while the output is the corresponding Q-value for each of the actions.  \n![alt text](https:\/\/cdn-images-1.medium.com\/max\/800\/1*3ZgGbUpEyAZb9POWijRq4Q.png)\n\nIn 2014 Google DeepMind published a [paper](https:\/\/arxiv.org\/pdf\/1312.5602.pdf) titled \"Playing Atari with Deep Reinforcement Learning\" that can play Atari 2600 games at expert human levels. This was the first breakthruogh in applying deep neural networks for reinforcement learning.\n\n![alt text](https:\/\/cdn-images-1.medium.com\/max\/800\/1*vUMIoHkl-PuIjbTqbtn8dA.png)\n\n","28a65814":"# State  \nState (s): This represents the board state of the game and in gym returned it is returned as observation. State: a numeric representation of what the agent is observing at a particular moment of time in the environment.  \nIn case of Taxi the observation is an integer, 500 different states are possible that translate to a nice graphic visual format with the render function. Note that this is specific for the Taxi game, in case of e.g. an Atari game the observation is the game screen with many coloured pixels.\n","7959d1bd":"Welcome to my third notebook on Kaggle. I did record my notes so it might help others in their journey to understand Neural Networks by examples (in this case Reinforcement Learning with Gym from OpenAI).  \nReinforcement learning is the process of learning by interacting with an environment. Reinforcement learning is also blessed with a lot of history and hence terminology, that does not always make it easier. So I have tried to put it into context so I can understand and hopefully others as well. It does not intend to give a complete history lesson but the key concepts are important to understand because they are often referred to in modern neural network reinforcement learning. Have fun!  \n\nThis notebook is my way to contribute back to the Kaggle platform and A.I. community.\n\nI would like to acknowledge this tutorial for providing ideas and code, learning by example:  \nhttps:\/\/www.oreilly.com\/learning\/introduction-to-reinforcement-learning-and-openai-gym  \nAlso special thanks to Move 37 course to enrich my understanding\/knowledge:  \nhttps:\/\/www.theschool.ai\/courses\/move-37-course\/  \nIf you are new to Neural Networks you might want to have a look at my other notebooks:  \nhttps:\/\/www.kaggle.com\/charel\/learn-neural-networks-by-example-mnist-digits  \nhttps:\/\/www.kaggle.com\/charel\/learn-by-example-rnn-lstm-gru-time-series  \n\n","ff906a02":"So, what is the magic, how does it solve it? \n\nThe Q-matrix is initialized with zero's. So initially it starts moving randomly until it hits a state\/action with rewards or state\/actions with a penalty. For understanding, let's simplify the problem that it needs to go to a certain drop-off position to get a reward. So random moves get no rewards but by luck (brute force enough tries) the state\/action is found where a reward is given. So next game the immediate actions preceding this state\/action will direct toward it by use of the Q-Matrix. The next iteration the actions before that, etc, etc. In other words, it solves \"the puzzle\" backwards from end-result (drop-off passenger) towards steps to be taken to get there in a iterative fashion.  \n\nNote that in case of the Taxi game there is a reward of -1 for each action. So if in a state the algorithm explored eg south which let to no value the Q-matrix is updated to -1 so next iteration (because values were initialized on 0) it will try an action that is not yet tried and still on 0. So also by design it encourages systematic exploration of states and actions \n\nIf you put the learning rate on 1 the game also is solved. Reason is that there is only one reward (dropoff passenger), so the algorithm will find it whatever learning rate. In case a game has more reward places the learning rate determines if it should prioritize longer term or shorter term rewards\n","2cc7c7b3":"# Gym  \nGym is released by Open AI in 2016 (http:\/\/gym.openai.com\/docs\/). It is a toolkit for developing and comparing reinforcement learning algorithms. OpenAI\u2019s mission is to ensure that artificial general intelligence benefits all of humanity.\n<img src=\"https:\/\/i.imgur.com\/ria9HOm.jpg width=300\" width=\"800px\">\n\nSource: [OpenAI](https:\/\/openai.com\/)\n\nIn 2018 Gym-retro was released as its successor: https:\/\/blog.openai.com\/gym-retro\/\n\n\nThere are many many games made available.  Let's review all the possible game environments in gym, see list below.  \n\n  \n","bbb99e29":"# exploration vs. exploitation\n\nThe taxi game has one place with the reward: dropoff passenger +20. What if it also had a reward: stop at coffee shop +2 points. If the trial-and-error found that value first and in subsequent iterations started to optimize the route to it, how do we know it would also find the bigger reward of dropoff passenger?  \nWhat if the the Taxi game actions are not deterministic (eg due to slippery roads I want to go north there is an 80% chance to go north and 10% chance to go west and 10% chance to go east), how would we ensure we still find the optimal policy?\n\nOur algorithm to exploit action = np.argmax(Q[obs]) time and time again will not cope with these more complex environments. In Reinforcement literature this is called the crucial tradeoff between \"exploitation\" and \"exploration\".\n* Exploitation: Make the best decision given current information (Go to the restaurent you know you like)\n* Exploration: Gather more information (go to a new restaurent to find out if you like it)\n\nSome approaches:  \n\n* Epsilon Greedy  \nWe exploit the current situation with probability 1\u200a\u2014\u200aepsilon and explore a new option with probability epsilon,  the rates of exploration and exploitation are fixed\n* Epsilon-Decreasing  \nWe exploit the current situation with probability 1\u200a\u2014\u200aepsilon and explore a new option with probability epsilon, with epsilon decreasing over time. \n* Thompson sampling: the rates of exploration and exploitation are dynamically updated with respect to the entire probability distribution of each arm\n* Epsilon-Decreasing with Softmax\nWe exploit the current situation with probability 1\u200a\u2014\u200aepsilon and explore a new option with probability epsilon, with epsilon decreasing over time.  In the case of exploring a new option, we don\u2019t just pick an option at random, but instead we estimate the outcome of each option, and then pick based on that (this is the softmax part).  \n* Etc  \n\nThe code below I tried with epsilon-greedy","03e90ccd":"# Action \nAction (a): the input the agent provides to the environment. So what are the action commands the agents can give to the enironment? The env.action_space will tell you\n\nWhat is the meaning of the actions? For the deep learning algorithm it should not matter, it should sort it out independent of the meaning of the action. But for humans it is handy to have the description, so we can understand the actions.   \n\nIn case of the Taxi game [0..5]:  \n* 0: move south\n* 1: move north\n* 2: move east \n* 3: move west \n* 4: pickup passenger\n* 5: dropoff passenger\n  \n","4497e978":"Analyzing the moves. It looks like that if you want to move rigth there is a significant chance you move up or down, simular if you want to move up there is a significant chance you move left or right, etc. So the algorithm learned that if you are on the frozen tile left column second row and you want to move down it is risky to give the down command because you could move to the right into the hole. So it gives the left command because if will keep you on the tile or move you up or down, but not to thr right.  \nOr in other words, the algorithm learned to take that actions with the least risk to (accidently slip) drown into a hole. Also interesting to se it learned as first move to go left, this to avoid you move right which is the more dangerous road.  \n\nNote: there is no 100% score possible. By consitently moving away from a hole you can safely traverse all fields except 1 (second row, thrid column) on which you could glide into due to slippery ice.  \n\nAlso good to notice the algorithm uses tenthousands of iterations to find the optimal policy, while this is a 4 by 4 playing field...","03412a7d":"# Bellman equation\nWe will make use of the basic Bellman equation for deterministic environments to solve a problem that is described as a Markov model, see figure below:\n\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/06039a80d0b3ef5b1eee6f05b9ae77867cda3026\">\n\nSource: [Wikimedia Commons](https:\/\/commons.wikimedia.org\/wiki\/File:Q-l%C3%A6ring_formel_1.png)\n\nwhere\n* R(s,a) = Reward of action a in state s\n* P(s'|s,a)= Probability of going to state s' given action a in state s. The Taxi game actions are deterministic (no such a thing as if I want to go north there is an 80% chance to go north and 10% chance to go west and 10% chance to go east). so the probability that selected action will lead to expected state is 100%. So ignore it for this game, it is always 1.\n* \u03b3 = Discount factor gamma, how much discount is applicable for the future rewards. It must be between 0 and 1. The higher gamma the higher the focus on long term rewards\n\nThe value iteration algorithm makes use of the equation in the form of:  \n* Value V(s): The expected long-term return with discount, as opposed to the short-term reward R. V\u03c0(s) is defined as the expected long-term return of the current state sunder policy \u03c0.  \n\nThe Q learning algorithm makes use of the equation in the form of:   \n* Q-matrix or action-value Q(s,a): Q-matrix is similar to Value, except that it takes an extra parameter, the action a. Q\u03c0(s, a) refers to the long-term return of the current state s, taking action a under policy \u03c0."}}