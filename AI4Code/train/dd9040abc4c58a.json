{"cell_type":{"1689c338":"code","3994d8ca":"code","c8d27ac4":"code","62666d83":"code","80703181":"code","c689da18":"code","4537606a":"code","f8aff053":"code","694c6c4f":"code","f82ad689":"code","9bd8bca6":"markdown","03dc198f":"markdown","b15c10b4":"markdown","d94b5206":"markdown"},"source":{"1689c338":"%%time\nimport warnings, sys\nwarnings.filterwarnings(\"ignore\")\n\n# Thanks to Chris's RAPIDS dataset, it only takes around 1 min to install offline\n!cp ..\/input\/rapids\/rapids.0.15.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","3994d8ca":"import sys\nsys.path.append(\"..\/input\/moa-scripts\")\nfrom moa import load_datasets, preprocess, split, submit, submit_preds\nfrom metrics import logloss\nfrom oof import OOFTrainer, update_dict\nfrom multilabel import MultiLabel\n\nimport pandas as pd \nimport numpy as np \nimport joblib\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# from sklearn.linear_model import LogisticRegression\nfrom cuml import LogisticRegression","c8d27ac4":"X, y, y_nonscored, test, submission = load_datasets(\"..\/input\/lish-moa\")\nX, y, test, test_control = preprocess(X, y, test, standard=False, onehot=True)\nX, y, X_holdout, y_holdout, split_index, index, classnames, features = split(X, y, n_folds=5, holdout=False)\njoblib.dump((index, split_index, features), \"index.pkl\") # for ensemble sanity check","62666d83":"np.random.seed(42)","80703181":"# def crossval(X, y, hparams, split_index, loss_function):\n#     hp = hparams.copy()\n#     history = []\n#     for i in np.unique(split_index):\n#         it, iv = split_index!=i, split_index==i\n#         xt, yt, xv, yv = X[it], y[it], X[iv], y[iv]\n#         model = LogisticRegression(**hparams)\n#         model.fit(xt, yt)\n#         p = model.predict_proba(xv)[:, 1]\n#         history.append(loss_function(p, yv))\n#     return np.mean(history)\n\n# multilabel_lr = MultiLabel(LogisticRegression, {}, 1)\n\n# # find best `C` by grid search individually for each label\n# grid = [\n#     update_dict(lr_hparams,  {'C':1e-4})\n#     ,update_dict(lr_hparams, {'C':1e-3})\n#     ,update_dict(lr_hparams, {'C':2.5e-3})\n#     ,update_dict(lr_hparams, {'C':5e-3})\n#     ,update_dict(lr_hparams, {'C':7.5e-3})\n#     ,update_dict(lr_hparams, {'C':1e-2})\n#     ,update_dict(lr_hparams, {'C':2.5e-2})\n#     ,update_dict(lr_hparams, {'C':5e-2})\n#     ,update_dict(lr_hparams, {'C':7.5e-3})\n#     ,update_dict(lr_hparams, {'C':1e-2})\n#     ,update_dict(lr_hparams, {'C':5e-2})\n#     ,update_dict(lr_hparams, {'C':1.0})\n#     ,update_dict(lr_hparams, {'C':5.0})\n# ]\n# multilabel_lr.grid_search(X, y, grid, split_index, logloss)\n# hparams = multilabel_lr.hparams_","c689da18":"hparams = \\\n[{'C': 0.05}, {'C': 0.0025}, {'C': 0.0001}, {'C': 0.0025}, {'C': 0.001}, {'C': 0.001}, {'C': 0.001}, {'C': 0.0025}, {'C': 0.05}, {'C': 0.005}, {'C': 0.001}, {'C': 0.01}, {'C': 0.05}, {'C': 0.01}, {'C': 0.001}, {'C': 0.01}, {'C': 0.001}, {'C': 0.001}, {'C': 0.001}, {'C': 0.001}, {'C': 0.0025}, {'C': 0.0025}, {'C': 0.0025}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.01}, {'C': 0.01}, {'C': 0.01}, {'C': 0.005}, {'C': 0.0025}, {'C': 0.0025}, {'C': 0.005}, {'C': 0.005}, {'C': 0.025}, {'C': 0.05}, {'C': 0.005}, {'C': 0.05}, {'C': 0.025}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.01}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.0025}, {'C': 0.0075}, {'C': 0.05}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.005}, {'C': 0.001}, {'C': 0.0075}, {'C': 0.01}, {'C': 0.01}, {'C': 0.001}, {'C': 0.0025}, {'C': 0.005}, {'C': 0.0025}, {'C': 0.005}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.001}, {'C': 0.01}, {'C': 0.0075}, {'C': 0.005}, {'C': 0.025}, {'C': 0.005}, {'C': 0.005}, {'C': 0.0025}, {'C': 0.0025}, {'C': 0.005}, {'C': 0.0025}, {'C': 0.0025}, {'C': 0.0075}, {'C': 0.01}, {'C': 0.01}, {'C': 0.005}, {'C': 0.0025}, {'C': 0.0025}, {'C': 0.0025}, {'C': 0.025}, {'C': 0.005}, {'C': 0.0025}, {'C': 0.005}, {'C': 0.0075}, {'C': 0.025}, {'C': 0.0025}, {'C': 0.0075}, {'C': 0.01}, {'C': 0.025}, {'C': 0.001}, {'C': 0.0075}, {'C': 0.0025}, {'C': 0.001}, {'C': 0.01}, {'C': 0.01}, {'C': 0.01}, {'C': 0.001}, {'C': 0.001}, {'C': 0.0025}, {'C': 0.0075}, {'C': 0.0025}, {'C': 0.01}, {'C': 0.001}, {'C': 0.001}, {'C': 0.05}, {'C': 0.01}, {'C': 0.005}, {'C': 0.025}, {'C': 0.01}, {'C': 0.01}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.005}, {'C': 0.0025}, {'C': 0.01}, {'C': 0.005}, {'C': 0.025}, {'C': 0.025}, {'C': 0.01}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.001}, {'C': 0.005}, {'C': 0.025}, {'C': 0.05}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.0025}, {'C': 0.001}, {'C': 0.001}, {'C': 0.025}, {'C': 0.0075}, {'C': 0.0025}, {'C': 0.005}, {'C': 0.005}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.005}, {'C': 0.001}, {'C': 0.01}, {'C': 0.01}, {'C': 0.001}, {'C': 0.0025}, {'C': 0.005}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.01}, {'C': 0.01}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.005}, {'C': 0.0075}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.001}, {'C': 0.01}, {'C': 0.001}, {'C': 0.0075}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.001}, {'C': 0.05}, {'C': 0.001}, {'C': 0.01}, {'C': 0.005}, {'C': 0.0001}, {'C': 0.005}, {'C': 0.05}, {'C': 0.01}, {'C': 0.025}, {'C': 0.01}, {'C': 0.01}, {'C': 0.005}, {'C': 0.0025}, {'C': 0.001}, {'C': 0.0025}, {'C': 0.005}, {'C': 0.0025}, {'C': 0.0025}, {'C': 0.01}, {'C': 0.001}, {'C': 0.005}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.025}, {'C': 0.0075}, {'C': 0.0075}, {'C': 0.001}, {'C': 0.005}, {'C': 0.0075}, {'C': 0.01}, {'C': 0.001}, {'C': 0.0075}, {'C': 0.01}, {'C': 0.005}, {'C': 0.01}, {'C': 0.0025}, {'C': 0.01}, {'C': 0.01}, {'C': 0.001}, {'C': 0.05}, {'C': 0.005}]\njoblib.dump(hparams, 'hparams.pkl');","4537606a":"%%time\nestimator_params = \\\ndict(\n    estimator=LogisticRegression,\n    hparams=hparams,\n    verbose=0\n)\noof_model = OOFTrainer(MultiLabel, estimator_params, logloss, save_models=False)\noof_model.fit(X, y, test, split_index, verbose=False)\nprint(f'logloss={logloss(y, oof_model.train_prediction_):.6f}')\njoblib.dump(oof_model, \"oof.pkl\");","f8aff053":"# # hack logloss 1: filter\n# n = y.shape[1]\n# t_lower = [0] * n\n\n# for i in tqdm(range(n)):\n#     baseline = logloss(y[:, i], oof_model.train_prediction_[:, i])\n#     best_loss = baseline\n#     pred = oof_model.train_prediction_[:, i].copy()\n#     for t in [1e-7, 1e-6, 1e-5] + list(np.arange(1e-4, 1e-2, 1e-4)):\n#         pred = np.where(pred <= t, 0, pred)\n#         loss = logloss(y[:, i], pred)\n#         if loss < best_loss:\n#             t_lower[i] = t\n#             best_loss = loss\n            \n# filt_pred = oof_model.train_prediction_.copy()\n# test_pred = oof_model.test_prediction_.copy()\n\n# for i,tl in zip(range(n), t_lower):\n#     pred = filt_pred[:, i].copy()\n#     filt_pred[:, i] = np.where(pred <= tl, 0, pred)\n#     pred = test_pred[:, i].copy()\n#     test_pred[:, i] = np.where(pred <= tl, 0, pred)\n\n# print(f'post processing: logloss={logloss(y, filt_pred):.6f}')\n# joblib.dump(t_lower, 't_lower.pkl');\n# submit_preds(test_pred, submission, test_control, classnames)","694c6c4f":"# submit\nsubmit_preds(oof_model.test_prediction_, submission, test_control, classnames)","f82ad689":"pd.read_csv('submission.csv').head().iloc[:, :8]","9bd8bca6":"## Post-processing","03dc198f":"## Finetune params for every class","b15c10b4":"## Preprocessing","d94b5206":"## OOF predict"}}