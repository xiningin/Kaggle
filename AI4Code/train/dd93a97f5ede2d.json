{"cell_type":{"7343f170":"code","6b8c6a37":"code","61c5efd9":"code","56a9a91d":"code","3c9ce05f":"code","41cae14c":"code","19cee02a":"code","74b4ba58":"code","57b69f80":"code","4dec5577":"code","c8f9b599":"code","91ae8b97":"code","0b3848e8":"code","9b8fa2d2":"code","4f44a3ad":"code","c4f859be":"code","9593fe6d":"code","69a4a2f4":"code","e37ce75e":"code","d1bc1029":"code","8ec8754c":"code","efeedb39":"code","6ab04c64":"code","08839bb0":"code","b132d145":"code","3931dc1a":"code","aac6335b":"code","0fb3ba44":"code","9c5fc8b8":"code","6cfae85e":"code","8f82cbc4":"code","2cc3f9f5":"code","7ff36d01":"code","329162b1":"code","30a66957":"code","832ac7d4":"code","b6c27b30":"code","428d0973":"code","56a16afb":"code","fbc41c94":"code","f7122a94":"code","c0d17aae":"code","1bfe0a14":"markdown","b2e681b9":"markdown","107dc049":"markdown","1c9ce6bc":"markdown","6f0a21d8":"markdown","43294d1b":"markdown"},"source":{"7343f170":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6b8c6a37":"train = pd.read_csv('..\/input\/data-clean\/clean_train.csv')\ntest = pd.read_csv('..\/input\/data-clean\/clean_test.csv')","61c5efd9":"print(list(train.columns))","56a9a91d":"tid=test['TransactionID']","3c9ce05f":"# Create correlation matrix\ncorr_matrix = train.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.70)]","41cae14c":"len(to_drop)","19cee02a":"droplist=['Date','_Hours','_Days','_Hours','id_23','id_27','id_33','id_30','id_34','id_16','id_31','id_28','id_29','id_15','id_35','id_36','id_37','id_38']\ntrain=train.drop(columns=droplist,axis=1)\ntrain=train.drop(columns=to_drop,axis=1)\ntest=test.drop(columns=to_drop,axis=1)","74b4ba58":"train.info()","57b69f80":"train=train.drop('Unnamed: 0',axis=1)\ntest=test.drop('Unnamed: 0',axis=1)","4dec5577":"# list(train.select_dtypes(include=['float64']).columns)","c8f9b599":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical = [col for col in list(train.select_dtypes(include=numerics)) if col in train.columns]\ncategorical = [col for col in list(train.select_dtypes(include='object')) if col in train.columns]","91ae8b97":"categorical","0b3848e8":"train_copy = train\nnumerical.remove('isFraud')","9b8fa2d2":"category_counts = {}\nfrom sklearn import preprocessing\nfor f in train.columns:\n    if  train[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        category_counts[f] = len(list(lbl.classes_)) + 1\n#         test[f] = lbl.transform(list(test[f].values))  \ntrain= train.reset_index()\n# test = test.reset_index()","4f44a3ad":"for f in test.columns:\n    if  test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(test[f].values))\n        test[f] = lbl.transform(list(test[f].values))  \n#         test = test.reset_index()","c4f859be":"train=train.drop('index',axis=1)","9593fe6d":"# train_ = train[\"isFraud\"]\n# train = train.drop([\"isFraud\"],axis=1)","69a4a2f4":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\n\nfor column in numerical:\n    scaler = StandardScaler()\n    if train[column].max() > 100 and train[column].min() >= 0:\n        train[column] = np.log1p(train[column])\n        test[column] = np.log1p(test[column])\n    scaler.fit(np.concatenate([train[column].values.reshape(-1,1), test[column].values.reshape(-1,1)]))\n    train[column] = scaler.transform(train[column].values.reshape(-1,1))\n    test[column] = scaler.transform(test[column].values.reshape(-1,1))","e37ce75e":"target = 'isFraud'","d1bc1029":"def get_input_features(df):\n    X = {'numerical':np.array(df[numerical])}\n    for cat in categorical:\n        X[cat] = np.array(df[cat])\n    return X","8ec8754c":"from keras.layers import Concatenate, Input, Dense, Embedding, Flatten, Dropout, BatchNormalization, SpatialDropout1D\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import Model\nfrom keras.optimizers import  Adam\nimport keras.backend as k\nfrom keras.optimizers import SGD\nimport graphviz","efeedb39":"category_counts","6ab04c64":"def make_model():\n    k.clear_session()\n\n    categorical_inputs = []\n    for cat in categorical:\n        categorical_inputs.append(Input(shape=[1], name=cat))\n\n    categorical_embeddings = []\n    for i, cat in enumerate(categorical):\n        categorical_embeddings.append(\n            Embedding(category_counts[cat], int(np.log1p(category_counts[cat]) + 1), name = cat + \"_embed\")(categorical_inputs[i]))\n    \n    categorical_logits = Concatenate(name = \"categorical_conc\")([Flatten()(SpatialDropout1D(.1)(cat_emb)) for cat_emb in categorical_embeddings])\n#     categorical_logits = Dropout(.5)(categorical_logits)\n\n    numerical_inputs = Input(shape=[train[numerical].shape[1]], name = 'numerical')\n    numerical_logits = Dropout(.1)(numerical_inputs)\n  \n    x = Concatenate()([\n        categorical_logits, \n        numerical_logits,\n    ])\n#     x = categorical_logits\n    x = BatchNormalization()(x)\n    x = Dense(128, activation = 'relu')(x)\n    x = Dropout(.2)(x)\n    x=  BatchNormalization()(x)\n    x = Dense(96, activation = 'relu')(x)\n#     x = Dropout(.4)(x)\n    x=  BatchNormalization()(x)\n    x = Dense(96, activation = 'relu')(x)\n#     x = Dropout(.4)(x)\n    x=  BatchNormalization()(x)\n    x = Dense(96, activation = 'relu')(x)\n    x = Dropout(.2)(x)\n    x=  BatchNormalization()(x)\n    x = Dense(128, activation = 'relu')(x)\n    x = Dropout(.2)(x)\n    out = Dense(1, activation = 'sigmoid')(x)\n    \n\n    model = Model(inputs=categorical_inputs + [numerical_inputs],outputs=out)\n    loss = \"binary_crossentropy\"\n    model.compile(optimizer=SGD(lr = 0.03), loss = loss)\n    return model","08839bb0":"from sklearn.model_selection import train_test_split, StratifiedKFold,KFold\nX_train, X_val = train_test_split(train, test_size=0.2, random_state=42)","b132d145":"# train = get_input_features(train)\nX_train = get_input_features(X_train)\nX_valid = get_input_features(X_val)\nX_test = get_input_features(test)","3931dc1a":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold,KFold\nimport keras\ny_train, y_valid = train_test_split(train_copy[target], test_size=0.2, random_state=42)\nrlr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=1, mode='auto', verbose = 1)\nkeras.callbacks.EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=2,\n                              verbose=0, mode='auto')","aac6335b":"model = make_model()\nbest_score = 0\npatience = 0\nfor i in range(200):\n    if patience < 4:\n        hist = model.fit(X_train, y_train, validation_data = (X_valid,y_valid), batch_size = 512, epochs = 1, verbose = 1)\n        valid_preds = model.predict(X_valid, batch_size = 512, verbose = True)\n        score = roc_auc_score(y_valid, valid_preds)\n        print(score)\n        if score > best_score:\n            model.save_weights(\"model.h5\")\n            best_score = score\n            patience = 0\n        else:\n            patience += 1\n            pass","0fb3ba44":"print(best_score)","9c5fc8b8":"# from keras.layers.normalization import BatchNormalization\n# from keras.callbacks import LearningRateScheduler\n# # Creating the model\n# model = Sequential()\n\n# # Inputing the first layer with input dimensions\n# model.add(Dense(100, \n#                 activation='relu',  \n#                 input_dim=111,\n#                 kernel_initializer='uniform'))\n# #The argument being passed to each Dense layer (18) is the number of hidden units of the layer. \n# # A hidden unit is a dimension in the representation space of the layer.\n\n# #Stacks of Dense layers with relu activations can solve a wide range of problems\n# #(including sentiment classification), and you\u2019ll likely use them frequently.\n\n# # Adding an Dropout layer to previne from overfitting\n# model.add(Dropout(0.50))\n# model.add(BatchNormalization())\n# #adding second hidden layer \n# model.add(Dense(128,\n#                 kernel_initializer='uniform',\n#                 activation='relu'))\n\n# # Adding another Dropout layer\n# model.add(Dropout(0.50))\n# model.add(BatchNormalization())\n# model.add(Dense(128,\n#                 kernel_initializer='uniform',\n#                 activation='relu'))\n\n# # model.add(Dense(256,\n# #                 kernel_initializer='uniform',\n# #                 activation='relu'))\n# # model.add(BatchNormalization())\n# model.add(Dropout(0.50))\n# model.add(BatchNormalization())\n# # model.add(Dense(128,\n# #                 kernel_initializer='uniform',\n# #                 activation='relu'))\n# # model.add(BatchNormalization())\n# # Adding another Dropout layer\n# # model.add(Dropout(0.50))\n\n# # adding the output layer that is binary [0,1]\n# model.add(Dense(1,\n#                 kernel_initializer='uniform',\n#                 activation='sigmoid'))\n# #With such a scalar sigmoid output on a binary classification problem, the loss\n# #function you should use is binary_crossentropy\n# annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)\n# #Visualizing the model\n# model.summary()","6cfae85e":"# #Creating an Stochastic Gradient Descent\n# sgd = SGD(lr = 0.02, momentum = 0.9)\n\n# # Compiling our model\n# model.compile(optimizer = 'Adam',\n#                    loss = 'binary_crossentropy', \n#                    metrics = ['accuracy'])\n# #optimizers list\n# #optimizers['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n\n# # Fitting the ANN to the Training set\n# # model.fit(X_train, y_train, \n# #                batch_size = 128, \n# #                epochs = 30, verbose=2)\n# model.fit(X_train, y_train, nb_epoch=15, batch_size=64, validation_split=0.2, verbose = 2)","8f82cbc4":"y_pred = model.predict(X_test)","2cc3f9f5":"z = np.hstack(y_pred)\nprint(len(z))","7ff36d01":"sub = pd.DataFrame()\nsub['TransactionID'] = tid\nsub['isFraud'] = z","329162b1":"# from sklearn.model_selection import train_test_split\n# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)","30a66957":"# from keras.layers.normalization import BatchNormalization\n# from scipy import optimize\n\n# model = Sequential()\n\n# model.add(Dense(100,input_dim=141,kernel_initializer='uniform',\n#                 activation='relu'))\n\n# model.add(Dropout(0.40))\n\n# model.add(BatchNormalization())\n\n# # model.add(Activation('relu'))\n\n# model.add(Dense(100),activation='relu')\n\n# model.add(Dropout(0.40))\n\n# model.add(BatchNormalization())\n\n# # model.add(Activation('relu'))\n\n# model.add(Dense(1,kernel_initializer='uniform',activation='sigmoid'))\n\n# model.compile(optimizer=Adam(lr=0.02), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\n# annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)","832ac7d4":"# from sklearn.metrics import roc_auc_score\n# from keras.callbacks import Callback\n\n# class printAUC(Callback()):\n    \n#     def __init__(self, X_train, y_train):\n\n#         super(printAUC, self).__init__()\n\n#         self.bestAUC = 0\n\n#         self.X_train = X_train[0]\n\n#         self.y_train = y_train[0]\n\n\n#     def on_epoch_end(self, epoch, logs={}):\n\n#         pred = self.model.predict(np.array(self.X_train))\n\n#         auc = roc_auc_score(self.y_train, pred)\n\n#         print(\"Train AUC: \" + str(auc))\n\n#         pred = self.model.predict(self.validation_data[0])\n\n#         auc = roc_auc_score(self.validation_data[1], pred)\n\n#         print (\"Validation AUC: \" + str(auc))\n#         if (self.bestAUC < auc) : \n\n#             self.bestAUC = auc\n\n#             self.model.save(\"bestNet.h5\", overwrite=True)\n\n#         return","b6c27b30":"# model.fit(X_train,y_train, batchsize=32, epochs = 30, callbacks=[annealer, printAUC(X_train, y_train)], validationdata = (X_val,Y_val), verbose=2)","428d0973":"# scores = model.evaluate(X_train, y_train, batch_size=30)\n# print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","56a16afb":"sub[['TransactionID','isFraud']].to_csv('submission.csv',index=False)","fbc41c94":"# # Fit the model\n# history = model.fit(X_train, y_train, validation_split=0.20, \n#                     epochs=18, batch_size=10, verbose=0)\n\n# # list all data in history\n# print(history.history.keys())","f7122a94":"# # summarizing historical accuracy\n# plt.plot(history.history['acc'])\n# plt.plot(history.history['val_acc'])\n# plt.title('Model Accuracy')\n# plt.ylabel('Accuracy')\n# plt.xlabel('Epoch')\n# plt.legend(['train', 'test'], loc='upper left')\n# plt.show()","c0d17aae":"# # summarize history for loss\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('model loss')\n# plt.ylabel('loss')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'test'], loc='upper left')\n# plt.show()","1bfe0a14":"### Below implementation is from [Mobius](http:\/\/https:\/\/www.kaggle.com\/arashnic)","b2e681b9":"### Removing high correl features","107dc049":"### Some great kernels to learn from-","1c9ce6bc":" ### Model Implementation\n * Different combination of layers can be created to boost scores","6f0a21d8":"* https:\/\/www.kaggle.com\/mirichoi0218\/ann-making-model-for-binary-classification\n* https:\/\/www.kaggle.com\/parthsuresh\/binary-classifier-using-keras-97-98-accuracy\n* https:\/\/www.kaggle.com\/kabure\/titanic-eda-keras-nn-pipelines\n* https:\/\/www.kaggle.com\/karthik7395\/binary-classification-using-neural-networks\/data\n* https:\/\/www.kaggle.com\/harnalashok\/deep-learning-for-binary-classification\n* https:\/\/www.kaggle.com\/deepthiappam\/keras-binary-classification-neural-networks\n* https:\/\/www.kaggle.com\/c\/avito-demand-prediction\/discussion\/59917\n* http:\/\/blog.kaggle.com\/2018\/01\/18\/an-intuitive-introduction-to-generative-adversarial-networks\/\n* https:\/\/www.kaggle.com\/devm2024\/keras-model-for-beginners-0-210-on-lb-eda-r-d\n* http:\/\/blog.kaggle.com\/2017\/06\/15\/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova\/","43294d1b":"### Label Encoding"}}