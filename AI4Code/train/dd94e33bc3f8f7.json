{"cell_type":{"370f5f0b":"code","bf289f36":"code","aa9cc0f4":"code","593ea6ae":"code","dacb0f16":"code","e96f82ca":"code","164838f1":"code","829511ac":"code","15e54d5a":"code","92882caa":"code","bddc6b90":"code","02a5c2ec":"code","b49b9f9d":"code","2f577c89":"code","275a7bdd":"code","48ddb504":"code","ccf51425":"code","1b64f40c":"code","85c90357":"code","34a4db95":"code","d4ac11e4":"code","4af5a82e":"code","ba2f1b21":"code","d24b4c21":"code","e6b5e368":"code","ac5acb7f":"code","23dcd4aa":"code","4b4f282b":"code","f7b2eeae":"markdown","5c910e7c":"markdown","6458fe86":"markdown","b87d9dbc":"markdown","74057b02":"markdown","c9f1812a":"markdown","58bd1fd1":"markdown","e293677f":"markdown","715c3ec2":"markdown","c46292df":"markdown","035f377f":"markdown","af479586":"markdown","ed271145":"markdown","90c3d46e":"markdown","eb070de1":"markdown","4b613689":"markdown","7d032c3a":"markdown","47522d69":"markdown","72f76788":"markdown"},"source":{"370f5f0b":"import numpy as np \nimport pandas as pd \nimport sentencepiece as spm\nimport re, json, gc, pickle, shutil, sys\nimport random, os, nltk, multiprocessing\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom scipy.stats import spearmanr\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nfrom gensim.summarization.textcleaner import get_sentences\n\nfrom tqdm import tqdm_notebook as tqdm\nimport seaborn as sns\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\ngc.enable()\n\nseed = 3\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\nROOT  = '\/kaggle\/input\/google-quest-challenge\/' \ndata  = pd.read_csv(ROOT+'train.csv')\ntest  = pd.read_csv(ROOT+'test.csv')\nsub   = pd.read_csv(ROOT+'sample_submission.csv')\n\ndata.head(3)","bf289f36":"text_col        = ['question_title',\n                   'question_body',\n                   'answer']\n\nnumeric_col     = []\n\ncategoricals    = ['category',\n                   'host']\n\nlabel_col       = ['question_asker_intent_understanding',\n                   'question_body_critical',\n                   'question_conversational',\n                   'question_expect_short_answer',\n                   'question_fact_seeking',\n                   'question_has_commonly_accepted_answer',\n                   'question_interestingness_others',\n                   'question_interestingness_self',\n                   'question_multi_intent',\n                   'question_not_really_a_question',\n                   'question_opinion_seeking',\n                   'question_type_choice',\n                   'question_type_compare',\n                   'question_type_consequence',\n                   'question_type_definition',\n                   'question_type_entity',\n                   'question_type_instructions',\n                   'question_type_procedure',\n                   'question_type_reason_explanation',\n                   'question_type_spelling',\n                   'question_well_written',\n                   'answer_helpful',\n                   'answer_level_of_information',\n                   'answer_plausible',\n                   'answer_relevance',\n                   'answer_satisfaction',\n                   'answer_type_instructions',\n                   'answer_type_procedure',\n                   'answer_type_reason_explanation',\n                   'answer_well_written']","aa9cc0f4":"def simple_tokenizer(Text, ReturnList=True, with_punc=False, lower=True):           \n    Text = re.sub(r'\\s\\d{2}:\\d{2}',' <hour> ', Text)\n    Text = re.sub(r'\\s\\d{1,2}[.\/]\\d{1,2}[.\/](\\d{4}|\\d{2})',' <date> ', Text)    \n    #Text = re.sub('[0-9,-]{5,}', '', Text)    \n    #Text = re.sub('[0-9]{1,}'  , '', Text)\n    \n    if with_punc:\n        Text = re.sub(r'\\n+\\s*',' \\n ', Text)\n        Text = re.sub(r'\\t+',' \\t ', Text)\n        Text = re.sub(r'([,]+\\s*)+',' , ', Text)\n        Text = re.findall(r\"[\\w'.]+[\\\"'.][\\w']+|[\\w']+|[.,!?:;$]\", Text)    \n    else:\n        Text = re.findall(r\"[\\w'.]+[\\\"'.][\\w']+|[\\w']+\", Text) \n    \n    if lower:\n         Text = [txt.lower() for txt in Text]\n    \n    if ReturnList:\n        return Text\n    return ' '.join(Text)","593ea6ae":"data['is_same_user'] = data.apply(lambda row: 1 if row['question_user_name']==row['answer_user_name'] else 0, axis=1)\ntest['is_same_user'] = test.apply(lambda row: 1 if row['question_user_name']==row['answer_user_name'] else 0, axis=1)\nnumeric_col.append('is_same_user')\n\ndef feature_extractor(data):\n    meta_features = []\n    for col in tqdm(text_col): \n\n        pct_Question_Marks       = []\n        pct_Exclamation_marks    = [] \n        pct_newlines             = []\n        pct_repeated_chars       = []\n        total_len                = []\n        number_of_words_list     = []\n        number_of_sentences_list = []\n\n        for text in data[col].values:\n\n            nuber_of_Question_Mark     = text.count('?')\n            number_of_Exclamation_mark = text.count('!')\n            number_of_words            = len(simple_tokenizer(text))\n            number_of_sentences        = len(list(get_sentences(text)))\n\n            lenn = len(text)\n\n            pct_Question_Mark    = nuber_of_Question_Mark    \/number_of_words if number_of_words>0 else 0\n            pct_Exclamation_mark = number_of_Exclamation_mark\/number_of_words if number_of_words>0 else 0\n\n            prev_char = 'random init char'\n            counter = 0\n            repeated_counts = []\n            for current_char in text:\n                if current_char==prev_char:\n                    counter+=1\n                else:\n                    counter = 0\n                repeated_counts.append(counter)\n                prev_char = current_char\n\n            pct_repeated_char = sum(repeated_counts)\/number_of_words if number_of_words>0 else 0\n\n            pct_Question_Marks.append(pct_Question_Mark)\n            pct_Exclamation_marks.append(pct_Exclamation_mark)\n            pct_repeated_chars.append(pct_repeated_char)\n            total_len.append(lenn)   \n            number_of_words_list.append(number_of_words)     \n            number_of_sentences_list.append(number_of_sentences) \n\n        data[col+'_Question_Marks']      = pct_Question_Marks\n        data[col+'_Exclamation_marks']   = pct_Exclamation_marks\n        data[col+'_repeated_chars']      = pct_repeated_chars\n        data[col+'_number_of_words']     = number_of_words_list\n        data[col+'_number_of_sentences'] = number_of_sentences_list\n    \n        meta_features += [col+'_Question_Marks', \n                          col+'_Exclamation_marks', \n                          col+'_repeated_chars', \n                          col+'_number_of_words',\n                          col+'_number_of_sentences']\n\n    return data, meta_features\n\n\ndata, meta_features = feature_extractor(data)\ntest, meta_features = feature_extractor(test)\n\nnumeric_col += meta_features\ndata.head(3)","dacb0f16":"scalers = {}\n\nfor col in numeric_col:\n    Scaler    = MinMaxScaler()\n    data[col] = Scaler.fit_transform(data[col].values.reshape(-1, 1)) \n    assert Scaler.n_samples_seen_ == len(data[col])\n    test[col] = Scaler.transform(test[col].values.reshape(-1, 1))\n    scalers[col] = Scaler","e96f82ca":"# corr_matrix = data[meta_features+label_col].corr().abs()\n# corr        = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)).stack().sort_values(ascending=False))\n# corr.head(30)","164838f1":"data = data.astype({cat:'category' for cat in categoricals})\n\ndef Get_Cat2IDs(data, categoricals_cols, IX_start):\n    encode = {}\n    for col in categoricals_cols:\n        encode[col] = dict(enumerate(data[col].cat.categories, start=IX_start)) \n\n    Cat2Int = {}\n    for cat in categoricals_cols:\n        ValueKey = {}\n        for key, value in encode[cat].items():\n            ValueKey[value] = key\n        Cat2Int[cat] = ValueKey\n    return Cat2Int\n    \nCat2Int = Get_Cat2IDs(data = data, \n                      categoricals_cols = categoricals, \n                      IX_start = 1) #index 0 will be for OOV\ncategorical_sizes = [data[c].nunique() + 1 for c in categoricals]\n\n\ncat_oov_value = 0\nfor col in categoricals:\n    data[col] = data[col].map(Cat2Int[col])\n\nfor col in categoricals:\n    test[col] = test[col].map(Cat2Int[col]).fillna(cat_oov_value)\n    \ndel Cat2Int\ngc.collect()","829511ac":"all_sentences = [] \n\nfor col in text_col:\n    for text in list(data[col].values)+list(test[col].values):\n        sentences = get_sentences(text)\n        for sentence in sentences:\n            all_sentences.append(sentence)\n            \nwith open('one_file_train_SentencePice.txt','a', encoding=\"utf8\") as wf:\n    for sentence in all_sentences:\n        wf.write(sentence + '\\n')","15e54d5a":"spm.SentencePieceTrainer.Train('--input=one_file_train_SentencePice.txt --model_prefix=sp_model --vocab_size=2000 --pad_id=3')","92882caa":"sp = spm.SentencePieceProcessor()\nsp.Load('sp_model.model')","bddc6b90":"for n in range(10):\n    print(sp.SampleEncodeAsPieces('New York', -1, 0.1))","02a5c2ec":"def get_token2ID():\n    with open('sp_model.vocab', 'r', encoding=\"utf8\") as f:\n        lines = f.readlines()\n    tokens      = list(enumerate([line.split(\"\\t\")[0] for line in lines]))\n    token_to_id = dict([(v,k) for (k,v) in tokens])\n    return token_to_id\n\ntoken_to_id = get_token2ID()","b49b9f9d":"sss = ShuffleSplit(n_splits=1, test_size=0.15, random_state=3)\ntrain_IX, test_IX = iter(next(sss.split(data[text_col+numeric_col+categoricals], data[label_col])))   \n\ntrain = data.loc[train_IX]\nval   = data.loc[test_IX]\n\ntrain.reset_index(drop=True, inplace=True)\nval.  reset_index(drop=True, inplace=True)","2f577c89":"def create_path(folder):\n    if not os.path.isdir(folder):\n        os.makedirs(folder)\n        os.makedirs(f'{folder}\/categorical')\n        os.makedirs(f'{folder}\/numerical')\n        os.makedirs(f'{folder}\/question_title')\n        os.makedirs(f'{folder}\/question_body')\n        os.makedirs(f'{folder}\/answer')\n        os.makedirs(f'{folder}\/label')\n    else:\n        shutil.rmtree(folder)\n        os.makedirs(folder)\n        os.makedirs(f'{folder}\/categorical')\n        os.makedirs(f'{folder}\/numerical')\n        os.makedirs(f'{folder}\/question_title')\n        os.makedirs(f'{folder}\/question_body')\n        os.makedirs(f'{folder}\/answer')\n        os.makedirs(f'{folder}\/label')\n            \ncreate_path(folder='data')","275a7bdd":"def save_data(data, folder, part):\n    \n    for IX, row in tqdm(data.iterrows(), total=len(data)):\n\n        #question_body\n        sentences = list(get_sentences(row['question_body']))\n        sentences = json.dumps(sentences)\n        with open(f'{folder}\/question_body\/' +str(row['qa_id'])+'.json', 'w') as f:\n            json.dump(sentences, f)\n\n        #answer \n        sentences = list(get_sentences(row['answer']))\n        sentences = json.dumps(sentences)\n        with open(f'{folder}\/answer\/'        +str(row['qa_id'])+'.json', 'w') as f:\n            json.dump(sentences, f)    \n\n        #question_title \n        sentences = [row['question_title']]\n        sentences = json.dumps(sentences)\n        with open(f'{folder}\/question_title\/'+str(row['qa_id'])+'.json', 'w') as f:\n            json.dump(sentences, f)    \n\n        #numerical features:\n        numericals = torch.tensor(row[numeric_col])\n        torch.save(numericals, \n                   f'{folder}\/numerical\/'    +str(row['qa_id'])+'.pt')\n\n        #categorical features:\n        categorical = torch.tensor(row[categoricals])\n        torch.save(categorical, \n                   f'{folder}\/categorical\/'  +str(row['qa_id'])+'.pt')\n        \n        #labels:\n        if part!='test':\n            labels = torch.tensor(row[label_col])\n            torch.save(labels, \n                       f'{folder}\/label\/'     +str(row['qa_id'])+'.pt')","48ddb504":"save_data(train,'data','train')\nsave_data(val,  'data','val')\nsave_data(test, 'data','test')","ccf51425":"[n for n in data.qa_id.values if n in test.qa_id.values]","1b64f40c":"def to_sequence(text):\n    seq = []\n    for token in text:\n        if token in token_to_id:\n            id = token_to_id[token]\n        else:\n            id = token_to_id['<unk>']\n        seq.append(id)\n    return np.array(seq, dtype='int16')","85c90357":"class QA_Dataset(Dataset):\n    def __init__(self, \n                 data, \n                 cat_path        ='\/categorical\/', \n                 num_path        ='\/numerical\/', \n                 Q_title_path    ='\/question_title\/',\n                 Q_body_path     ='\/question_body\/',\n                 answer_path     ='\/answer\/',\n                 label_path      = None, \n                 folder          ='data',\n                 ID_Column       ='qa_id',\n                 padding_val     = token_to_id['<pad>'],\n                 max_n_words     = 100,\n                 max_n_sentances = 100,\n                 augmentation    = True):\n        \n        self.data            = data        \n        self.cat_path        = cat_path\n        self.num_path        = num_path\n        self.Q_title_path    = Q_title_path\n        self.Q_body_path     = Q_body_path\n        self.answer_path     = answer_path\n        self.label_path      = label_path   \n        self.folder          = folder   \n        self.ID_Column       = ID_Column\n        self.padding_val     = padding_val\n        self.max_n_words     = max_n_words\n        self.max_n_sentances = max_n_sentances\n        self.augmentation    = augmentation\n        self.len             = self.data.shape[0]\n        \n    def __getitem__(self, IXs):\n\n        ID = self.data.loc[IXs, self.ID_Column]\n\n        ### categorical ###\n        categorical = torch.load(self.folder+self.cat_path+str(ID)+'.pt')\n        \n        ### numerical ###\n        numerical   = torch.load(self.folder+self.num_path+str(ID)+'.pt')\n        \n        \n        ### question title ###\n        Q_title_IDs = []\n        with open(self.folder+self.Q_title_path+str(ID)+'.json') as file:\n            Q_title_sentance = json.loads( json.load(file) )\n        subwords     = (sp.SampleEncodeAsPieces(Q_title_sentance[0], -1, 0.1) \n                        if self.augmentation \n                        else sp.EncodeAsPieces(Q_title_sentance[0]))\n        subwords_IDs = to_sequence(subwords)\n        Q_title_IDs  = torch.tensor(subwords_IDs) \n        \n        \n        ### question body: ###\n        Q_body_IDs   = []  \n        n_words_QBody = []\n        with open(self.folder+self.Q_body_path+str(ID)+'.json') as file:\n            Q_body_sentances = json.loads( json.load(file) )\n        for sentance in Q_body_sentances:\n            subwords = (sp.SampleEncodeAsPieces(sentance, -1, 0.1) \n                        if self.augmentation \n                        else sp.EncodeAsPieces(sentance))\n            if len(subwords) > self.max_n_words:\n                subwords = subwords[:self.max_n_words]\n            subwords_IDs = to_sequence(subwords)\n            Q_body_IDs.append(torch.tensor(subwords_IDs))\n        if len(Q_body_IDs)==0:\n            Q_body_IDs = [torch.tensor([self.padding_val], dtype=torch.int16)]\n        elif len(Q_body_IDs) > self.max_n_sentances:\n            Q_body_IDs = Q_body_IDs[:self.max_n_sentances]\n        Q_body_IDs = pad_sequence(Q_body_IDs,  \n                                  batch_first=True, \n                                  padding_value=self.padding_val)\n        \n        \n        ### Answer ###\n        answer_IDs = []\n        with open(self.folder+self.answer_path+str(ID)+'.json') as file:\n            answer_sentances = json.loads( json.load(file) )\n        for sentance in answer_sentances:\n            subwords     = (sp.SampleEncodeAsPieces(sentance, -1, 0.1) \n                            if self.augmentation \n                            else sp.EncodeAsPieces(sentance))\n            if len(subwords) > self.max_n_words:\n                subwords = subwords[:self.max_n_words]            \n            subwords_IDs = to_sequence(subwords)\n            answer_IDs.append(torch.tensor(subwords_IDs))\n        if len(answer_IDs)==0:\n            answer_IDs = [torch.tensor([self.padding_val], dtype=torch.int16)]\n        elif len(answer_IDs) > self.max_n_sentances:\n            answer_IDs = answer_IDs[:self.max_n_sentances]            \n        answer_IDs     = pad_sequence(answer_IDs,  \n                                      batch_first=True, \n                                      padding_value=self.padding_val)\n        \n        \n        ### labels ###\n        if self.label_path is not None:\n            labels  = torch.load(self.folder+self.label_path+str(ID)+'.pt')\n            \n            \n            return [categorical, numerical, Q_title_IDs,Q_body_IDs, answer_IDs, labels]\n        return     [categorical, numerical, Q_title_IDs,Q_body_IDs, answer_IDs]\n    \n    def __len__(self):\n        return self.len","34a4db95":"class MyCollator(object):\n    def __init__(self, \n                 padding_val = token_to_id['<pad>'], \n                 is_test     = False):\n        \n        self.padding_val = padding_val\n        self.is_test     = is_test\n    \n    def __call__(self, batch):\n        \n        cat            =     [item[0]          for item in batch]\n        num            =     [item[1]          for item in batch]\n        Q_title_IDs    =     [item[2]          for item in batch]\n        Q_body_IDs     =     [item[3]          for item in batch]\n        Q_body_MaxSent = max([item[3].shape[0] for item in batch])\n        Q_body_MaxWord = max([item[3].shape[1] for item in batch])\n        answer_IDs     =     [item[4]          for item in batch]\n        answer_MaxSent = max([item[4].shape[0] for item in batch])\n        answer_MaxWord = max([item[4].shape[1] for item in batch])\n        \n        if not self.is_test:\n            labels     =     [item[5]          for item in batch]\n        \n        \n        Q_title_IDs    = pad_sequence(Q_title_IDs, \n                                      batch_first=True, \n                                      padding_value=self.padding_val)\n        \n        BATCH_SIZE     = len(Q_body_IDs)\n        pad_Q_body_IDs = Q_body_IDs[0].new_full(size = (BATCH_SIZE, \n                                                        Q_body_MaxSent, \n                                                        Q_body_MaxWord), \n                                                fill_value = self.padding_val)\n        \n        for IX, QB_IDs in enumerate(Q_body_IDs): \n            pad_Q_body_IDs[IX, :QB_IDs.shape[0], :QB_IDs.shape[1]] = QB_IDs\n            \n            \n        pad_answer_IDs = answer_IDs[0].new_full(size = (BATCH_SIZE, \n                                                        answer_MaxSent, \n                                                        answer_MaxWord), \n                                                fill_value = self.padding_val)    \n        for IX, A_ID in enumerate(answer_IDs): \n            pad_answer_IDs[IX, :A_ID.shape[0], :A_ID.shape[1]] = A_ID\n            \n            \n        if not self.is_test:\n            return [torch.stack(cat),        \n                    torch.stack(num),        \n                    Q_title_IDs.type(torch.long),\n                    pad_Q_body_IDs.type(torch.long), \n                    pad_answer_IDs.type(torch.long), \n                    torch.stack(labels)]\n        \n        return [torch.stack(cat),        \n                torch.stack(num),        \n                Q_title_IDs.type(torch.long),\n                pad_Q_body_IDs.type(torch.long), \n                pad_answer_IDs.type(torch.long)]","d4ac11e4":"class QA_Net(nn.Module):\n    def __init__(self,\n                 EMBEDDING_DIM,\n                 HIDDEN_SIZE):\n        super(QA_Net, self).__init__()\n        \n        self.word_LSTM     = nn.LSTM(input_size   = EMBEDDING_DIM, \n                                    hidden_size  = HIDDEN_SIZE, \n                                    batch_first  = True, \n                                    bidirectional= True)\n        self.word_proj    = nn.Linear(in_features=2*HIDDEN_SIZE, \n                                      out_features= HIDDEN_SIZE)\n        \n        word_atten_weight = torch.randn((HIDDEN_SIZE,1))\n        nn.init.xavier_uniform_(word_atten_weight)     \n        self.word_context = nn.Parameter(word_atten_weight)\n        \n        self.softmax      = nn.Softmax(dim=1)\n        \n        self.sent_LSTM    = nn.LSTM(input_size   =2*HIDDEN_SIZE, \n                                    hidden_size  =HIDDEN_SIZE, \n                                    batch_first  =True, \n                                    bidirectional=True)\n        \n        self.sent_proj    = nn.Linear(in_features  = 2*HIDDEN_SIZE, \n                                      out_features = HIDDEN_SIZE)\n\n        sent_atten_weight = torch.randn((HIDDEN_SIZE,1))\n        nn.init.xavier_uniform_(sent_atten_weight)             \n        self.sent_context = nn.Parameter(sent_atten_weight)\n        \n    def forward(self, embeddings, batch_size):\n\n        result,_ = self.word_LSTM(embeddings)\n        \n        u_it     = torch.tanh(self.word_proj(result))\n        \n        w_scores = self.softmax(u_it.matmul(self.word_context)) \n        result   = result.mul(w_scores)\n        result   = torch.sum(result, dim=1)\n        \n        #from token level to sentance level\n        result   = result.view(batch_size,\n                               int(result.shape[0]\/batch_size), \n                               result.shape[-1])\n        \n        result,_ = self.sent_LSTM(result)\n        \n        u_i      = torch.tanh(self.sent_proj(result))\n        s_scores = self.softmax(u_i.matmul(self.sent_context))\n        result   = result.mul(s_scores)\n        result   = torch.sum(result, dim=1)\n        \n        return result        ","4af5a82e":"class QTitle_Net(nn.Module):\n    def __init__(self,\n                 EMBEDDING_DIM,\n                 HIDDEN_SIZE):\n        super(QTitle_Net, self).__init__()\n        \n        self.lstm            =  nn.LSTM(input_size    = EMBEDDING_DIM, \n                                        hidden_size   = HIDDEN_SIZE,\n                                        batch_first   = True,\n                                        bidirectional = True,\n                                        num_layers    = 1) \n        \n        self.word_proj    = nn.Linear(in_features =2*HIDDEN_SIZE, \n                                      out_features=HIDDEN_SIZE)\n        \n        word_atten_weight = torch.randn((HIDDEN_SIZE,1))\n        nn.init.xavier_uniform_(word_atten_weight)                     \n        self.word_context = nn.Parameter(word_atten_weight)\n        \n        self.softmax      = nn.Softmax(dim=1)        \n        \n    def forward(self, embeddings): \n        \n        result,_ = self.lstm(embeddings)\n        \n        u_it     = torch.tanh(self.word_proj(result))\n        w_scores = self.softmax(u_it.matmul(self.word_context)) \n        result   = result.mul(w_scores)\n        result   = torch.sum(result, dim=1)\n        \n        return result","ba2f1b21":"class Main_Model(nn.Module):\n    def __init__(self,\n                 categorical_sizes = categorical_sizes,\n                 EMBEDDING_DIM     = 100,\n                 HIDDEN_SIZE       = 100,\n                 len_numerical     = len(numeric_col),\n                 out_size          = len(label_col)):\n        super(Main_Model, self).__init__()\n\n        embedding_dims      = [(c, min(50, (c+1)\/\/2)) for c in categorical_sizes]        \n        self.cat_embeddings = nn.ModuleList([nn.Embedding(x, y) \n                                             for x, y in embedding_dims])        \n        total_embs_size     = sum([y for x, y in embedding_dims])        \n        total_nums_and_embs = total_embs_size + len_numerical      \n\n        self.text_embedding = nn.Embedding(len(token_to_id), EMBEDDING_DIM)\n\n        self.QTitle_net     = QTitle_Net(EMBEDDING_DIM, HIDDEN_SIZE)\n        self.QBody_Net      = QA_Net(EMBEDDING_DIM, HIDDEN_SIZE)\n        self.answer_Net     = QA_Net(EMBEDDING_DIM, HIDDEN_SIZE)\n\n        total_size          = 2*HIDDEN_SIZE+2*HIDDEN_SIZE+2*HIDDEN_SIZE+total_nums_and_embs\n\n        self.fc1            = nn.Linear(total_size, 300)\n        self.dropout1       = nn.Dropout(0.3)\n        self.fc2            = nn.Linear(300, 128)\n        self.dropout2       = nn.Dropout(0.3)\n        self.fc3            = nn.Linear(128, 64)\n        self.dropout3       = nn.Dropout(0.3)\n        self.out            = nn.Linear(64, out_size)\n\n    def forward(self, categorical, numerical, QTitle_IDs, QBody_IDs, answer_IDs):\n        \n        batch_size =  categorical.shape[0]\n\n        cat_embs   = [emb_layer(categorical[:, i]) \n                      for i, emb_layer in enumerate(self.cat_embeddings)]        \n        cat_embs   = torch.cat(cat_embs, 1)        \n\n        QTitle_emb = self.text_embedding(QTitle_IDs)\n        QTitle_out = self.QTitle_net(QTitle_emb)\n        \n        QBody_IDs  = QBody_IDs.view(QBody_IDs.shape[0] * QBody_IDs.shape[1], \n                                    QBody_IDs.shape[-1])\n        QBody_emb  = self.text_embedding(QBody_IDs)\n        QBody_out  = self.QBody_Net(QBody_emb,\n                                    batch_size)    \n        \n        answer_IDs = answer_IDs.view(answer_IDs.shape[0]*answer_IDs.shape[1], \n                                     answer_IDs.shape[-1])\n        answer_emb = self.text_embedding(answer_IDs)\n        answer_out = self.answer_Net(answer_emb,\n                                     batch_size)\n        \n        embeddings = torch.cat([cat_embs, \n                                numerical, \n                                QTitle_out, \n                                QBody_out, \n                                answer_out], 1) \n        \n        result     = F.relu(self.fc1(embeddings))\n        result     = self.dropout1(result)\n        \n        result     = F.relu(self.fc2(result))\n        result     = self.dropout2(result)\n        \n        result     = F.relu(self.fc3(result))\n        result     = self.dropout3(result)\n        \n        result     = self.out(result)\n        \n        return result","d24b4c21":"def sigmoid(x):                                        \n    return 1 \/ (1 + np.exp(-x))\n\ndef mean_spearman_correlation_score(y_true, y_pred):\n    return np.mean([spearmanr(y_pred[:, idx], y_true[:, idx]).correlation \n                    for idx in range(len(label_col))])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","e6b5e368":"epochs           = 100\nBATCH_SIZE       = 64\nmax_patience     = 10\nlabel_path       = '\/label\/'\nloader_n_workers = 0 if device.type=='cpu' else multiprocessing.cpu_count()\n\nTrainSet = QA_Dataset(data       = train,\n                      label_path = label_path)\n\nValSet   = QA_Dataset(data       = val,\n                      label_path = label_path)\n\ntrain_loader = DataLoader(TrainSet, \n                          batch_size = BATCH_SIZE, \n                          shuffle    = True,\n                          collate_fn = MyCollator(),\n                          num_workers= loader_n_workers,\n                          pin_memory = True)\n\nval_loader   = DataLoader(ValSet, \n                          batch_size = BATCH_SIZE, \n                          shuffle    = False,\n                          collate_fn = MyCollator(),\n                          num_workers= loader_n_workers,\n                          pin_memory = True)\n\nmodel = Main_Model()\nmodel.to(device)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nscheduler = ReduceLROnPlateau(optimizer, \n                              mode='max', \n                              factor=0.01, \n                              patience=3, \n                              verbose=True)\n\nBest_val_loss = 0\nbest_counter = 1\nfor epochs in tqdm(range(epochs)):\n    model.train()\n    \n    total_loss, counter = 0, 0\n    train_preds, train_trues = [], []\n    for cat, num, Q_title_IDs, Q_body_IDs, answer_IDs, labels in tqdm(train_loader, \n                                                                      leave=False):\n\n        predictions = model.forward(cat.to(device, non_blocking=True), \n                                    num.to(device, non_blocking=True), \n                                    Q_title_IDs.to(device,non_blocking=True),\n                                    Q_body_IDs.to(device, non_blocking=True), \n                                    answer_IDs.to(device, non_blocking=True))\n        \n        optimizer.zero_grad()\n        loss = criterion(predictions, labels.to(device, non_blocking=True))\n        total_loss+=loss.item()\n        counter+=1\n        loss.backward()\n        optimizer.step()\n        \n        train_preds.append(predictions.cpu().detach().numpy())\n        train_trues.append(labels.cpu().detach().numpy())\n        \n        print(f'train - batch loss: {loss} avg BCE loss: {total_loss\/counter} ' ,end='\\r')\n        torch.cuda.empty_cache()    \n        \n    train_preds = np.concatenate(train_preds)\n    train_trues = np.concatenate(train_trues)\n    train_loss  = mean_spearman_correlation_score(train_trues, \n                                                  sigmoid(train_preds))\n    \n    print('epoch number: {}'.format(epochs+1))\n    print(f'training   MSCS loss: {train_loss}')\n       \n        \n    y_preds = []\n    y_trues = []\n    model.eval()\n    with torch.no_grad():\n        for cat, num, Q_title_IDs, Q_body_IDs, answer_IDs, labels in val_loader:\n\n            predictions = model.forward(cat.to(device, non_blocking=True), \n                                        num.to(device, non_blocking=True), \n                                        Q_title_IDs.to(device, non_blocking=True), \n                                        Q_body_IDs.to(device, non_blocking=True), \n                                        answer_IDs.to(device, non_blocking=True))\n\n            y_preds.append(predictions.cpu().detach().numpy())\n            y_trues.append(labels.cpu().detach().numpy())\n                \n        y_preds = np.concatenate(y_preds)\n        y_trues = np.concatenate(y_trues)\n\n        val_loss = mean_spearman_correlation_score(y_trues, \n                                               sigmoid(y_preds))\n       \n        print('Validation MSCS loss:', val_loss)\n\n        is_best = val_loss > Best_val_loss\n        Best_val_loss = max(val_loss, Best_val_loss)\n        best_counter+=1\n        \n        if is_best:\n            best_counter = 1\n            print('Best validation score so far!')\n            torch.save(model.state_dict(), 'Best_Model.pt')\n        if best_counter>=max_patience:\n            print('last epochs is:',epochs+1)\n            break\n            \n    scheduler.step(val_loss)\n    ","ac5acb7f":"n_augmentation= 50\n\ntestSet       = QA_Dataset(data = test)\n\ntest_loader   = DataLoader(testSet, \n                           batch_size = 1, \n                           shuffle    = False,\n                           collate_fn = MyCollator(is_test = True ),\n                           num_workers=0)\nmodel = Main_Model()\nmodel.load_state_dict(torch.load('Best_Model.pt'))\nmodel.to(device)\nmodel.eval()\n\nsum_pred = np.zeros((len(test),len(label_col)))\nwith torch.no_grad():\n    for i in tqdm(range(n_augmentation)):\n        y_preds = []\n        for cat, num, Q_title_IDs,Q_body_IDs, answer_IDs in test_loader:\n\n            predictions = model.forward(cat.type(torch.long).to(device), \n                                        num.to(device), \n                                        Q_title_IDs.to(device),\n                                        Q_body_IDs.to(device), \n                                        answer_IDs.to(device))\n\n            y_preds.append(predictions.cpu().detach().numpy())\n            torch.cuda.empty_cache()\n\n        y_preds = sigmoid(np.concatenate(y_preds))\n\n        sum_pred += y_preds\n        \nmean_pred = sum_pred \/ n_augmentation","23dcd4aa":"mean_pred.shape","4b4f282b":"submission = pd.DataFrame(mean_pred, columns=label_col)\nsubmission['qa_id'] = test['qa_id'].values\nsubmission = submission[['qa_id']+label_col]\nsubmission.to_csv('submission.csv', index=False)\nshutil.rmtree('data')\nsubmission.head(3)","f7b2eeae":"**Test Time Augmentation (TTA)**\n\nAnother uniqueness I didn't see in other NLP based kernels is that at inference time, I'm going to get prediction multiple times for each test sample (each time SentencePiece will give a different subwords) and eventually take all the prediction's average.","5c910e7c":"I'll make a dictionary that maps each token to its ID number:","6458fe86":"**Training SentencePiece model:**","b87d9dbc":"Scaling features with MinMaxScaler:","74057b02":"**Loading the trained model:**","c9f1812a":"Check if there is any qa_id that is also in the training set and test set:","58bd1fd1":"Train val split:","e293677f":"In order to produce batchs in Hirarchical attention network, I need to pad the data in sentence level and word level.\nSo in each batch I'm going to check the maximum number of words in each sentence and the maximum number of sentences in any question\/answer and pad as the maximum.","715c3ec2":"Question body and Answer text input:","c46292df":"Convert each category name to ID number:","035f377f":"**The Model:**\n\nIn this kernel I'm going to build a model that will get 5 inputs:\n1. **Question title text**\n    * This input will enter into standard LSTM layer, followed by Attention.\n2. **Question body text**\n    * Because this input can be long, I will split it into sentences and then using LSTM layer, followed by attention layer, I will get a vector representation for every sentence. that representation will enter into LSTM layer, followed by attention layer to get a document vector representation. This architecture called \"*Hierarchical Attention Network*\": \n      https:\/\/www.cs.cmu.edu\/~.\/hovy\/papers\/16HLT-hierarchical-attention-networks.pdf\n3. **Answer text**\n    * Same as section 2.\n4. **Categorical features**\n    * Each of the categories will be represented by a vector as in this paper:\n>  *\"Entity Embeddings of Categorical Variables\"*\n> https:\/\/arxiv.org\/abs\/1604.06737\n5. **Numerical features**\n    * Simply connected to all other vectors in the lower layers.\n\n","af479586":"**Testing:**","ed271145":"**Training the model:**","90c3d46e":"**Meta features**\n\nI saw that there are many examples where the user who asked the question is the same user who wrote the answer. So I'll add a binary feature for that.\n\nI will also add numerical features that describe the number of sentences, the number of words, the number of exclamation marks, and the number of question marks divided by the number of words in the title of the question, the question body, and in the answer.","eb070de1":"**Preparing the data for training SentencePiece:**\n\nTo train the SentencePiece we need to prepare one text file that contains all the text we want to train on when the sentences are separated with a new line.","4b613689":"**Text augmentation example**\n\nYou can see that 'New York' is segmented differently on each SampleEncode call:","7d032c3a":"Question title input:","47522d69":"**Google QUEST Q&A Labeling Competition**\n\nIn this notebook I'm going to deal with Goole CrowdSource Q&A Labeling data. \nThe data for this competition includes questions and answers from various StackExchange properties. the task is to predict target values of 30 labels for each question-answer pair.\n\n\n\n","72f76788":"**SentencePiece**\n\nThe uniqueness of this kernel is that I am going to use SentencePiece which in each epoch will take a different subwords. this should lead to good regularization.\nThe idea is taken from this paper:\n> *Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates*\n> https:\/\/arxiv.org\/abs\/1804.10959\n"}}