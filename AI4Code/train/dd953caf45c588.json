{"cell_type":{"e2ba982a":"code","57df0fd1":"code","1fdcebef":"code","a72f61bd":"code","e14bfa68":"code","52c79fd5":"code","e8917abf":"code","fdfccd5a":"code","8d6dd090":"code","c6956a1d":"code","6959fb0a":"code","ba95bc37":"code","cd20a135":"code","cf006f3c":"code","4fdf7df9":"code","07c699cd":"code","eca827ba":"code","220468ed":"code","8b250d8c":"code","c76cf53e":"code","5a215f17":"code","a39d68a8":"code","e0391295":"code","aed2a9f2":"code","43fe2ad1":"code","cddfef5b":"code","68acc492":"code","6df3d99e":"code","590b8788":"code","f5559c3c":"code","4b8d05d2":"code","5bd36a42":"code","317f8e61":"code","a5aa5a34":"code","ec90bd4f":"code","8785dc97":"code","57976ea2":"code","c3a5aa35":"code","fc229fc9":"code","f7498f85":"code","3d895290":"code","01f92ec0":"code","23b5230b":"code","c7ec3ff7":"code","7a21e54d":"code","4516a659":"code","fa3bce8b":"code","6b293846":"code","44685de3":"code","1b15297b":"code","91fbb6bf":"code","a4179969":"code","b0ccd565":"code","0cb09d29":"code","a2f313fa":"code","92e87b93":"code","2029f8f7":"code","b2c626a0":"code","b5fec8e4":"code","213dd74d":"code","77702dc4":"code","91a867f7":"code","e5d69a42":"code","91bde59f":"code","d680843d":"code","868cc587":"code","b0139537":"code","a978ecb0":"code","d5c885b9":"code","0dff2430":"code","c48046e5":"code","b23f5eb2":"code","b46d3ab9":"code","d9932d55":"code","17bb556c":"code","4ccfbde3":"code","ac4f23ae":"code","98c50be5":"code","8694619f":"code","b79ad6f0":"code","a196e620":"code","c1ef5747":"code","5576f74c":"code","02d55d5e":"markdown","bfd2dac6":"markdown","9a9b7e03":"markdown","130b7866":"markdown","6c2e5d39":"markdown","104e4cbd":"markdown","c42c1f0a":"markdown","33fdf15c":"markdown","70cb8603":"markdown","3742b6c5":"markdown","81b06b40":"markdown","e581bce2":"markdown","07d625b4":"markdown","0a3103c9":"markdown","4157fb80":"markdown","d89a12da":"markdown"},"source":{"e2ba982a":"!pip install ..\/input\/sacremoses\/sacremoses-master\/\n!pip install ..\/input\/transformers\/transformers-master\/","57df0fd1":"import pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nDATA_DIR = '..\/input\/google-quest-challenge'","1fdcebef":"!ls ..\/input","a72f61bd":"os.listdir(\"..\/input\/roberta-transformers-pytorch\/roberta-base\")","e14bfa68":"os.listdir(\"..\/input\/qaxlnetbasecasedaugdiffswaanswer\")","52c79fd5":"os.listdir(\"..\/input\/qaxlnetbasecasedaugdiffswaquestion\")","e8917abf":"os.listdir(\"..\/input\/qabertbaseuncasedaugdiffswaanswer\")","fdfccd5a":"os.listdir(\"..\/input\/qabertbaseuncasedaugdiffswaquestion\")","8d6dd090":"os.listdir(\"..\/input\/qabertbasecasedaugdiffswaanswer\")","c6956a1d":"os.listdir(\"..\/input\/qabertbasecasedaugdiffswaquestion\")","6959fb0a":"os.listdir(\"..\/input\/qabertbasecasedaugdiffv2swa\")","ba95bc37":"os.listdir(\"..\/input\/qabertuncasedaugdiffv2swa\")","cd20a135":"os.listdir(\"..\/input\/qaxlnetbasecasedaugdiff\")","cf006f3c":"os.listdir(\"..\/input\/qarobertabasecasedaugdiffswaquestion\")","4fdf7df9":"sub = pd.read_csv(f'{DATA_DIR}\/sample_submission.csv')\nsub.head()","07c699cd":"TARGET_COLUMNS = sub.columns.values[1:].tolist()\nTARGET_COLUMNS","eca827ba":"train = pd.read_csv(f'{DATA_DIR}\/train.csv')\ntrain.head()","220468ed":"test = pd.read_csv(f'{DATA_DIR}\/test.csv')\ntest.head()","8b250d8c":"import torch\n#import torch.utils.data as data\nfrom torchvision import datasets, models, transforms\nfrom transformers import *\nfrom sklearn.utils import shuffle\nimport random\nfrom math import floor, ceil\nfrom sklearn.model_selection import GroupKFold\n\nMAX_LEN = 512\n#MAX_Q_LEN = 250\n#MAX_A_LEN = 259\nSEP_TOKEN_ID = 102\n\nclass QuestDataset(torch.utils.data.Dataset):\n    def __init__(self, df, model_type=\"bert-base-cased\", max_len=512, content=\"Question_Answer\", train_mode=True, labeled=True):\n        self.df = df\n        self.train_mode = train_mode\n        self.labeled = labeled\n        self.max_len = max_len\n        self.content = content\n        bert_tokenizer_path = '..\/input\/pretrained-bert-models-for-pytorch\/' + model_type + '-vocab.txt'\n        xlnet_tokenizer_path = '..\/input\/xlnet-pretrained-models-pytorch\/' + model_type + '-spiece.model'\n        roberta_tokenizer_path = '..\/input\/roberta-transformers-pytorch\/roberta-base\/vocab.json'\n        roberta_tokenizer_merges_file = '..\/input\/roberta-transformers-pytorch\/roberta-base\/merges.txt'\n        if model_type == \"bert-base-uncased\":\n            self.tokenizer = BertTokenizer.from_pretrained(bert_tokenizer_path)\n        elif model_type == \"bert-base-cased\":\n            self.tokenizer = BertTokenizer.from_pretrained(bert_tokenizer_path)\n        elif model_type == \"xlnet-base-cased\":\n            self.tokenizer = XLNetTokenizer.from_pretrained(xlnet_tokenizer_path)\n        elif model_type == \"roberta-base\":\n            self.tokenizer = RobertaTokenizer(vocab_file=roberta_tokenizer_path, merges_file=roberta_tokenizer_merges_file)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        token_ids, seg_ids = self.get_token_ids(row)\n        if self.labeled:\n            labels = self.get_label(row)\n            return token_ids, seg_ids, labels\n        else:\n            return token_ids, seg_ids\n\n    def __len__(self):\n        return len(self.df)\n\n    def select_tokens(self, tokens, max_num):\n        if len(tokens) <= max_num:\n            return tokens\n        if self.train_mode:\n            num_remove = len(tokens) - max_num\n            remove_start = random.randint(0, len(tokens)-num_remove-1)\n            return tokens[:remove_start] + tokens[remove_start + num_remove:]\n        else:\n            return tokens[:max_num\/\/2] + tokens[-(max_num - max_num\/\/2):]\n        \n    def trim_input_single_content(self, title, content, max_sequence_length=512, \n                t_max_len=30, c_max_len=512-30-4, num_token=3):\n\n        t = self.tokenizer.tokenize(title)\n        c = self.tokenizer.tokenize(content)\n\n        t_len = len(t)\n        c_len = len(c)\n\n        if (t_len+c_len+num_token) > max_sequence_length:\n\n            if t_max_len > t_len:\n                t_new_len = t_len\n                c_max_len = c_max_len + floor((t_max_len - t_len)\/2)\n            else:\n                t_new_len = t_max_len\n\n            if c_max_len > c_len:\n                c_new_len = c_len \n            else:\n                c_new_len = c_max_len\n\n\n            if t_new_len+c_new_len+num_token > max_sequence_length:\n                raise ValueError(\"New sequence length should be less or equal than %d, but is %d\" \n                                 % (max_sequence_length, (t_new_len+c_new_len+num_token)))\n            \n            # truncate\n            if len(t) - t_new_len > 0:\n                t = t[:t_new_len\/\/4] + t[len(t)-t_new_len+t_new_len\/\/4:]\n            else:\n                t = t[:t_new_len]\n\n            if len(c) - c_new_len > 0:\n                c = c[:c_new_len\/\/4] + c[len(c)-c_new_len+c_new_len\/\/4:]\n            else:\n                c = c[:c_new_len]\n\n        # some bad cases\n        if (len(t) + len(c) + num_token > max_sequence_length):\n            more_token = len(t) + len(c) + num_token - max_sequence_length\n            c = c[:(len(c)-more_token)]\n        \n        return t, c\n            \n    def trim_input(self, title, question, answer, max_sequence_length=MAX_LEN, \n                t_max_len=30, q_max_len=239, a_max_len=239, num_token=4):\n\n        t = self.tokenizer.tokenize(title)\n        q = self.tokenizer.tokenize(question)\n        a = self.tokenizer.tokenize(answer)\n\n        t_len = len(t)\n        q_len = len(q)\n        a_len = len(a)\n\n        if (t_len+q_len+a_len+num_token) > max_sequence_length:\n\n            if t_max_len > t_len:\n                t_new_len = t_len\n                a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n                q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n            else:\n                t_new_len = t_max_len\n\n            if a_max_len > a_len:\n                a_new_len = a_len \n                q_new_len = q_max_len + (a_max_len - a_len)\n            elif q_max_len > q_len:\n                a_new_len = a_max_len + (q_max_len - q_len)\n                q_new_len = q_len\n            else:\n                a_new_len = a_max_len\n                q_new_len = q_max_len\n\n\n            if t_new_len+a_new_len+q_new_len+num_token > max_sequence_length:\n                raise ValueError(\"New sequence length should be %d, but is %d\" \n                                 % (max_sequence_length, (t_new_len+a_new_len+q_new_len+num_token)))\n\n            \n            # truncate\n            if len(t) - t_new_len > 0:\n                t = t[:t_new_len\/\/4] + t[len(t)-t_new_len+t_new_len\/\/4:]\n            else:\n                t = t[:t_new_len]\n\n            if len(q) - q_new_len > 0:\n                q = q[:q_new_len\/\/4] + q[len(q)-q_new_len+q_new_len\/\/4:]\n            else:\n                q = q[:q_new_len]\n\n            if len(a) - a_new_len > 0:\n                a = a[:a_new_len\/\/4] + a[len(a)-a_new_len+a_new_len\/\/4:]\n            else:\n                a = a[:a_new_len]\n\n        return t, q, a\n        \n    def get_token_ids(self, row):\n        \n        num_token = 4\n        \n        if self.content == \"Question\":\n            num_token -= 1\n        elif self.content == \"Answer\":\n            num_token -= 1\n        \n        if self.content == \"Question_Answer\":   \n            t_max_len=30\n            q_max_len=int((self.max_len-t_max_len-num_token)\/2)\n            a_max_len=(self.max_len-t_max_len - num_token - int((self.max_len-t_max_len-num_token)\/2))\n        elif self.content == \"Question\":\n            t_max_len=30\n            q_max_len=self.max_len-t_max_len-num_token\n            a_max_len=0\n        elif self.content == \"Answer\":\n            t_max_len=30\n            q_max_len=0\n            a_max_len=self.max_len-t_max_len-num_token  \n        else:\n            raise NotImplementedError\n        \n        if self.content == \"Question_Answer\":\n            t_tokens, q_tokens, a_tokens = self.trim_input(row.question_title, row.question_body, row.answer, max_sequence_length=self.max_len, \\\n                t_max_len=t_max_len, q_max_len=q_max_len, a_max_len=a_max_len, num_token=num_token)\n        elif self.content == \"Question\":\n            t_tokens, c_tokens = self.trim_input_single_content(row.question_title, row.question_body, max_sequence_length=self.max_len, \\\n                t_max_len=t_max_len, c_max_len=q_max_len, num_token=num_token)\n        elif self.content == \"Answer\":\n            t_tokens, c_tokens = self.trim_input_single_content(row.question_title, row.answer, max_sequence_length=self.max_len, \\\n                t_max_len=t_max_len, c_max_len=a_max_len, num_token=num_token)\n        else:\n            raise NotImplementedError\n\n        if self.content == \"Question_Answer\":\n            tokens = ['[CLS]'] + t_tokens + ['[SEP]'] + q_tokens + ['[SEP]'] + a_tokens + ['[SEP]']\n        elif ((self.content == \"Question\") or (self.content == \"Answer\")):\n            tokens = ['[CLS]'] + t_tokens + ['[SEP]'] + c_tokens + ['[SEP]']\n        else:\n            raise NotImplementedError\n                \n        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        if len(token_ids) < self.max_len:\n            token_ids += [0] * (self.max_len - len(token_ids))\n        ids = torch.tensor(token_ids)\n        seg_ids = self.get_seg_ids(ids)\n        \n        return ids, seg_ids\n    \n    def get_seg_ids(self, ids):\n        seg_ids = torch.zeros_like(ids)\n        seg_idx = 0\n        first_sep = True\n        for i, e in enumerate(ids):\n            seg_ids[i] = seg_idx\n            if e == self.tokenizer.sep_token_id:\n                if first_sep:\n                    first_sep = False\n                else:\n                    seg_idx = 1\n        pad_idx = torch.nonzero(ids == 0)\n        seg_ids[pad_idx] = 0\n\n        return seg_ids\n\n    def get_label(self, row):\n        #print(row[TARGET_COLUMNS].values)\n        return torch.tensor(row[TARGET_COLUMNS].values.astype(np.float32))\n\n    def collate_fn(self, batch):\n        token_ids = torch.stack([x[0] for x in batch])\n        seg_ids = torch.stack([x[1] for x in batch])\n    \n        if self.labeled:\n            labels = torch.stack([x[2] for x in batch])\n            return token_ids, seg_ids, labels\n        else:\n            return token_ids, seg_ids\n\ndef get_test_loader(model_type=\"bert-base-cased\", max_len=512, content=\"Question_Answer\", batch_size=4):\n    df = pd.read_csv(f'{DATA_DIR}\/test.csv')\n    ds_test = QuestDataset(df, model_type, max_len=max_len, content=content, train_mode=False, labeled=False)\n    loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=ds_test.collate_fn, drop_last=False)\n    loader.num = len(df)\n    \n    return loader, ds_test.tokenizer\n        \ndef get_train_val_loaders(model_type=\"bert-base-cased\", max_len=512, content=\"Question_Answer\", batch_size=4, val_batch_size=4, ifold=0):\n    df = pd.read_csv(f'{DATA_DIR}\/train.csv')\n    df = shuffle(df, random_state=42)\n    #split_index = int(len(df) * (1-val_percent))\n    gkf = GroupKFold(n_splits=5).split(X=df.question_body, groups=df.question_body)\n    for fold, (train_idx, valid_idx) in enumerate(gkf):\n        if fold == ifold:\n            df_train = df.iloc[train_idx]\n            df_val = df.iloc[valid_idx]\n            break\n\n    #print(df_val.head())\n    #df_train = df[:split_index]\n    #df_val = df[split_index:]\n\n    print(df_train.shape)\n    print(df_val.shape)\n\n    ds_train = QuestDataset(df_train, model_type, max_len=max_len, content=content)\n    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=ds_train.collate_fn, drop_last=True)\n    train_loader.num = len(df_train)\n\n    ds_val = QuestDataset(df_val, model_type, max_len=max_len, content=content, train_mode=False)\n    val_loader = torch.utils.data.DataLoader(ds_val, batch_size=val_batch_size, shuffle=False, num_workers=2, collate_fn=ds_val.collate_fn, drop_last=False)\n    val_loader.num = len(df_val)\n    val_loader.df = df_val\n\n    return train_loader, val_loader, ds_train.tokenizer\n\ndef test_train_loader():\n    loader, _, _ = get_train_val_loaders(\"xlnet-base-cased\", 512, \"Question\", 4, 4, 1)\n    for ids, seg_ids, labels in loader:\n        print(ids)\n        print(seg_ids.numpy())\n        print(labels)\n        break\ndef test_test_loader():\n    loader, _ = get_test_loader(\"roberta-base\", 512, \"Question\", 4)\n    for ids, seg_ids in loader:\n        print(ids)\n        print(seg_ids)\n        break","c76cf53e":"test_test_loader()","5a215f17":"test_train_loader()","a39d68a8":"from transformers import *\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass QuestModel(nn.Module):\n    def __init__(self, model_type=\"xlnet-base-cased\", tokenizer=None, n_classes=30, hidden_layers=[-1, -3, -5, -7, -9]):\n        super(QuestModel, self).__init__()\n        self.model_name = 'QuestModel'\n        self.model_type = model_type\n        self.hidden_layers = hidden_layers\n        if model_type == \"bert-base-uncased\":\n            bert_model_config = '..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/bert_config.json'\n            bert_config = BertConfig.from_json_file(bert_model_config)\n            bert_config.output_hidden_states = True\n            # haven't update question_answer model\n            if n_classes == 30:\n                bert_config.hidden_dropout_prob = 0.1\n            else:\n                bert_config.hidden_dropout_prob = 0\n            model_path = os.path.join('..\/input\/pretrained-bert-models-for-pytorch\/' + model_type)\n            self.bert_model = BertModel.from_pretrained(model_path, config=bert_config)   \n        elif model_type == \"bert-base-cased\":\n            bert_model_config = '..\/input\/pretrained-bert-models-for-pytorch\/bert-base-cased\/bert_config.json'\n            bert_config = BertConfig.from_json_file(bert_model_config)\n            bert_config.output_hidden_states = True\n            model_path = os.path.join('..\/input\/pretrained-bert-models-for-pytorch\/' + model_type)\n            self.bert_model = BertModel.from_pretrained(model_path, config=bert_config)   \n        elif model_type == \"xlnet-base-cased\":\n            xlnet_model_config = '..\/input\/xlnet-pretrained-models-pytorch\/xlnet-base-cased-config.json'\n            xlnet_config = XLNetConfig.from_json_file(xlnet_model_config)\n            xlnet_config.output_hidden_states = True\n            xlnet_config.hidden_dropout_prob = 0\n            model_path = os.path.join('..\/input\/xlnet-pretrained-models-pytorch\/' + model_type + '-pytorch_model.bin')\n            self.xlnet_model = XLNetModel.from_pretrained(model_path, config=xlnet_config)   \n        elif model_type == \"xlnet-large-cased\":\n            xlnet_model_config = '..\/input\/xlnet-pretrained-models-pytorch\/xlnet-large-cased-config.json'\n            xlnet_config = XLNetConfig.from_json_file(xlnet_model_config)\n            xlnet_config.output_hidden_states = True\n            xlnet_config.hidden_dropout_prob = 0\n            model_path = os.path.join('..\/input\/xlnet-pretrained-models-pytorch\/' + model_type + '-pytorch_model.bin')\n            self.xlnet_model = XLNetModel.from_pretrained(model_path, config=xlnet_config)  \n        elif model_type == \"roberta-base\":\n            roberta_model_config = '..\/input\/roberta-transformers-pytorch\/roberta-base\/config.json'\n            roberta_config = RobertaConfig.from_json_file(roberta_model_config)\n            roberta_config.output_hidden_states = True\n            roberta_config.hidden_dropout_prob = 0\n            model_path = os.path.join('..\/input\/roberta-transformers-pytorch\/roberta-base\/pytorch_model.bin')\n            self.roberta_model = RobertaModel.from_pretrained(model_path, config=roberta_config)  \n            self.roberta_model.resize_token_embeddings(len(tokenizer)) \n        \n        if model_type == \"bert-base-uncased\":\n            self.hidden_size = 768\n        elif model_type == \"bert-large-uncased\":\n            self.hidden_size = 1024\n        elif model_type == \"bert-base-cased\":\n            self.hidden_size = 768\n        elif model_type == \"xlnet-base-cased\":\n            self.hidden_size = 768\n        elif model_type == \"xlnet-large-cased\":\n            self.hidden_size = 1024\n        elif model_type == \"roberta-base\":\n            self.hidden_size = 768\n        else:\n            raise NotImplementedError\n            \n        self.fc_1 = nn.Linear(self.hidden_size * len(hidden_layers), self.hidden_size)\n        self.fc = nn.Linear(self.hidden_size, n_classes)\n            \n        self.selu = nn.SELU()\n        self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n        self.dropouts = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n\n    def forward(self, ids, seg_ids):\n        attention_mask = (ids > 0)\n        \n        if ((self.model_type == \"bert-base-uncased\") \\\n            or (self.model_type == \"bert-base-cased\") \\\n            or (self.model_type == \"bert-large-uncased\") \\\n            or (self.model_type == \"bert-large-cased\")):\n        \n            outputs = self.bert_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n            hidden_states = outputs[2]\n            \n            # pooled_out = outputs[1] #  N * 768\n        \n            # sequence_out = torch.unsqueeze(outputs[0][:, 0], dim=-1) # N * 512 * 768 * 1, hidden_states[-1]\n            # fuse_hidden = sequence_out\n            \n            # 13 (embedding + 12 transformers) for base\n            # 26 (embedding + 25 transformers) for large\n            \n            # concat hidden\n            for i in range(len(self.hidden_layers)):\n                if i == 0:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = torch.mean(hidden_states[hidden_layer], dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    fuse_hidden = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                else:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = torch.mean(hidden_states[hidden_layer], dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    h = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                    fuse_hidden = torch.cat([fuse_hidden, h], dim=-1)\n                    \n            fuse_hidden = fuse_hidden.reshape(fuse_hidden.shape[0], -1)\n            h = self.relu(self.fc_1(fuse_hidden))\n        \n        elif ((self.model_type == \"xlnet-base-cased\") \\\n            or (self.model_type == \"xlnet-large-cased\")):\n\n            attention_mask = attention_mask.float()\n            outputs = self.xlnet_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n            hidden_states = outputs[1]\n            \n            # last_hidden_out = outputs[0]\n            # mem = outputs[1], when config.mem_len > 0\n            \n            # concat hidden, summary_type=\"first\", first_dropout = 0\n            for i in range(len(self.hidden_layers)):\n                if i == 0:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    fuse_hidden = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                else:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    h = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                    fuse_hidden = torch.cat([fuse_hidden, h], dim=-1)\n        \n            fuse_hidden = fuse_hidden.reshape(fuse_hidden.shape[0], -1)\n            h = self.relu(self.fc_1(fuse_hidden))\n        elif (self.model_type == \"roberta-base\"):\n\n            attention_mask = attention_mask.float()\n            outputs = self.roberta_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n            # outputs = self.roberta_model(input_ids=ids, attention_mask=attention_mask)\n            hidden_states = outputs[2]\n            \n            for i in range(len(self.hidden_layers)):\n                if i == 0:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    fuse_hidden = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                else:\n                    hidden_layer = self.hidden_layers[i]\n                    # hidden_state = hidden_states[hidden_layer].mean(dim=1)\n                    hidden_state = hidden_states[hidden_layer][:, 0]\n                    h = torch.unsqueeze(hidden_state, dim=-1) # N * 768 * 1\n                    fuse_hidden = torch.cat([fuse_hidden, h], dim=-1)\n        \n            fuse_hidden = fuse_hidden.reshape(fuse_hidden.shape[0], -1)\n            h = self.relu(self.fc_1(fuse_hidden))\n            \n            \n            \n        for j, dropout in enumerate(self.dropouts):\n            \n            if j == 0:\n                logit = self.fc(dropout(h))\n            else:\n                logit += self.fc(dropout(h))\n                \n        return logit \/ len(self.dropouts)\n    \ndef test_model(model_type=\"bert-base-cased\", hidden_layers=[-1, -3, -5, -7, -9]):\n    x = torch.tensor([[1,2,3,4,5, 0, 0], [1,2,3,4,5, 0, 0]])\n    seg_ids = torch.tensor([[0,0,0,0,0, 0, 0], [0,0,0,0,0, 0, 0]])\n    model = QuestModel(model_type=model_type, hidden_layers=hidden_layers)\n\n    y = model(x, seg_ids)\n    print(y)","e0391295":"test_model(model_type=\"bert-base-cased\", hidden_layers=[-3, -4, -5, -6, -7])","aed2a9f2":"def create_bert_base_uncased_models():\n    models = []\n    for i in range(10):\n        model = QuestModel(model_type=\"bert-base-uncased\", hidden_layers=[-1, -3, -5, -7, -9])\n        model.load_state_dict(torch.load(f'..\/input\/qabertuncasedaugdiffv2swa\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_cased_models():\n    models = []\n    for i in range(10):\n        model = QuestModel(model_type=\"bert-base-cased\", hidden_layers=[-1, -3, -5, -7, -9])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbasecasedaugdiffv2swa\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_xlnet_base_cased_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"xlnet-base-cased\", hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qaxlnetbasecasedaugquestionanswerswa\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_xlnet_base_cased_question_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"xlnet-base-cased\", n_classes=21, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qaxlnetbasecasedaugdiffswaquestion\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_xlnet_base_cased_answer_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"xlnet-base-cased\", n_classes=9, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qaxlnetbasecasedaugdiffswaanswer\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\n\ndef create_bert_base_uncased_question_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-uncased\", n_classes=21, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbaseuncasedaugdiffswaquestion\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_uncased_answer_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-uncased\", n_classes=9, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbaseuncasedaugdiffswaanswer\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_cased_question_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-cased\", n_classes=21, hidden_layers=[-2, -4, -6, -8, -10])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbasecasedaugdiffswaquestion\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_bert_base_cased_answer_models():\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"bert-base-cased\", n_classes=9, hidden_layers=[-2, -4, -6, -8, -10])\n        model.load_state_dict(torch.load(f'..\/input\/qabertbasecasedaugdiffswaanswer\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_roberta_base_models(tokenizer):\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"roberta-base\", tokenizer=tokenizer, n_classes=30, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qarobertabaseaugdiffswa\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_roberta_base_question_models(tokenizer):\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"roberta-base\", tokenizer=tokenizer, n_classes=21, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qarobertabasecasedaugdiffswaquestion\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models\n\ndef create_roberta_base_answer_models(tokenizer):\n    models = []\n    for i in range(5):\n        model = QuestModel(model_type=\"roberta-base\", tokenizer=tokenizer, n_classes=9, hidden_layers=[-3, -4, -5, -6, -7])\n        model.load_state_dict(torch.load(f'..\/input\/qarobertabasecasedaugdiffswaanswer\/fold_{i}_checkpoint_swa.pth'))\n        model.eval()\n        models.append(model)\n    return models","43fe2ad1":"from tqdm import tqdm\nimport torch\ndef predict(models, test_loader):\n    all_scores = []\n    with torch.no_grad():\n        for ids, seg_ids in tqdm(test_loader, total=test_loader.num \/\/ test_loader.batch_size):\n            ids, seg_ids = ids.cuda(), seg_ids.cuda()\n            scores = []\n            for model in models:\n                model = model.cuda()\n                outputs = torch.sigmoid(model(ids, seg_ids)).cpu()\n                scores.append(outputs)\n            all_scores.append(torch.mean(torch.stack(scores), 0))\n\n    all_scores = torch.cat(all_scores, 0).numpy()\n    \n    return all_scores","cddfef5b":"test_loader, tokenizer = get_test_loader(model_type=\"roberta-base\", content=\"Question_Answer\", batch_size=32)","68acc492":"roberta_base_models = create_roberta_base_models(tokenizer)\nroberta_base_preds = predict(roberta_base_models, test_loader)","6df3d99e":"del roberta_base_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","590b8788":"test_loader, _ = get_test_loader(model_type=\"xlnet-base-cased\", batch_size=32)","f5559c3c":"xlnet_base_cased_models = create_xlnet_base_cased_models()\nxlnet_base_cased_preds = predict(xlnet_base_cased_models, test_loader)","4b8d05d2":"del xlnet_base_cased_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","5bd36a42":"test_loader, _ = get_test_loader(model_type=\"xlnet-base-cased\", content=\"Question\", batch_size=32)","317f8e61":"xlnet_base_cased_question_models = create_xlnet_base_cased_question_models()\nxlnet_base_cased_question_preds = predict(xlnet_base_cased_question_models, test_loader)","a5aa5a34":"del xlnet_base_cased_question_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","ec90bd4f":"test_loader, _ = get_test_loader(model_type=\"xlnet-base-cased\", content=\"Answer\", batch_size=32)","8785dc97":"xlnet_base_cased_answer_models = create_xlnet_base_cased_answer_models()\nxlnet_base_cased_answer_preds = predict(xlnet_base_cased_answer_models, test_loader)","57976ea2":"del xlnet_base_cased_answer_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","c3a5aa35":"xlnet_base_cased_question_answer_preds = np.concatenate([xlnet_base_cased_question_preds, xlnet_base_cased_answer_preds], axis=1)","fc229fc9":"test_loader, tokenizer = get_test_loader(model_type=\"roberta-base\", content=\"Question\", batch_size=32)","f7498f85":"roberta_base_question_models = create_roberta_base_question_models(tokenizer)\nroberta_base_question_preds = predict(roberta_base_question_models, test_loader)","3d895290":"del roberta_base_question_models, test_loader, tokenizer\ntorch.cuda.empty_cache()\ngc.collect()","01f92ec0":"test_loader, tokenizer = get_test_loader(model_type=\"roberta-base\", content=\"Answer\", batch_size=32)","23b5230b":"roberta_base_answer_models = create_roberta_base_answer_models(tokenizer)\nroberta_base_answer_preds = predict(roberta_base_answer_models, test_loader)","c7ec3ff7":"del roberta_base_answer_models, test_loader, tokenizer\ntorch.cuda.empty_cache()\ngc.collect()","7a21e54d":"roberta_base_question_answer_preds = np.concatenate([roberta_base_question_preds, roberta_base_answer_preds], axis=1)","4516a659":"test_loader, _ = get_test_loader(model_type=\"bert-base-cased\", content=\"Question\", batch_size=32)","fa3bce8b":"bert_base_cased_question_models = create_bert_base_cased_question_models()\nbert_base_cased_question_preds = predict(bert_base_cased_question_models, test_loader)","6b293846":"del bert_base_cased_question_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","44685de3":"test_loader, _ = get_test_loader(model_type=\"bert-base-cased\", content=\"Answer\", batch_size=32)","1b15297b":"bert_base_cased_answer_models = create_bert_base_cased_answer_models()\nbert_base_cased_answer_preds = predict(bert_base_cased_answer_models, test_loader)","91fbb6bf":"del bert_base_cased_answer_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","a4179969":"bert_base_cased_question_answer_preds = np.concatenate([bert_base_cased_question_preds, bert_base_cased_answer_preds], axis=1)","b0ccd565":"test_loader, _ = get_test_loader(model_type=\"bert-base-uncased\", content=\"Question\", batch_size=32)","0cb09d29":"bert_base_uncased_question_models = create_bert_base_uncased_question_models()\nbert_base_uncased_question_preds = predict(bert_base_uncased_question_models, test_loader)","a2f313fa":"del bert_base_uncased_question_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","92e87b93":"test_loader, _ = get_test_loader(model_type=\"bert-base-uncased\", content=\"Answer\", batch_size=32)","2029f8f7":"bert_base_uncased_answer_models = create_bert_base_uncased_answer_models()\nbert_base_uncased_answer_preds = predict(bert_base_uncased_answer_models, test_loader)","b2c626a0":"del bert_base_uncased_answer_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","b5fec8e4":"bert_base_uncased_question_answer_preds = np.concatenate([bert_base_uncased_question_preds, bert_base_uncased_answer_preds], axis=1)","213dd74d":"test_loader, _ = get_test_loader(model_type=\"bert-base-cased\", batch_size=32)","77702dc4":"bert_base_cased_models = create_bert_base_cased_models()\nbert_base_cased_preds = predict(bert_base_cased_models, test_loader)","91a867f7":"del bert_base_cased_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","e5d69a42":"test_loader, _ = get_test_loader(model_type=\"bert-base-uncased\", batch_size=32)","91bde59f":"bert_base_uncased_models = create_bert_base_uncased_models()\nbert_base_uncased_preds = predict(bert_base_uncased_models, test_loader)","d680843d":"del bert_base_uncased_models, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","868cc587":"# preds = bert_base_uncased_question_answer_preds\npreds = ((bert_base_uncased_preds + bert_base_uncased_question_answer_preds)\/2.0 \\\n         + (xlnet_base_cased_preds + xlnet_base_cased_question_answer_preds)\/2.0 \\\n         + (bert_base_cased_preds + bert_base_cased_question_answer_preds)\/2.0 \\\n         + (roberta_base_preds + roberta_base_question_answer_preds)\/2.0)\/4.0\n# preds = bert_base_uncased_preds\n# preds = roberta_base_question_answer_preds","b0139537":"sub[TARGET_COLUMNS] = bert_base_uncased_question_answer_preds\nsub.to_csv('submission_bert_base_uncased.csv', index=False)\nsub[TARGET_COLUMNS] =roberta_base_preds\nsub.to_csv('submission_roberta_base.csv', index=False)\n# sub[TARGET_COLUMNS] = xlnet_base_cased_preds\n# sub.to_csv('submission_xlnet_base_cased.csv', index=False)","a978ecb0":"sub[TARGET_COLUMNS] = preds","d5c885b9":"sub.head()","0dff2430":"test = pd.read_csv(f'{DATA_DIR}\/test.csv')","c48046e5":"test = test.set_index('qa_id').join(sub.set_index('qa_id'))","b23f5eb2":"test.head()","b46d3ab9":"from sklearn.preprocessing import MinMaxScaler\n    \ndef postprocessing(oof_df):\n   \n    scaler = MinMaxScaler()\n    \n    # type 1 column [0, 0.333333, 0.5, 0.666667, 1]\n    # type 2 column [0, 0.333333, 0.666667]\n    # type 3 column [0.333333, 0.444444, 0.5, 0.555556, 0.666667, 0.777778, 0.8333333, 0.888889, 1]\n    # type 4 column [0.200000, 0.266667, 0.300000, 0.333333, 0.400000, \\\n    # 0.466667, 0.5, 0.533333, 0.600000, 0.666667, 0.700000, \\\n    # 0.733333, 0.800000, 0.866667, 0.900000, 0.933333, 1]\n    \n    # comment some columns based on oof result\n    \n    ################################################# handle type 1 columns\n    type_one_column_list = [\n       'question_conversational', \\\n       'question_has_commonly_accepted_answer', \\\n       'question_not_really_a_question', \\\n       'question_type_choice', \\\n       'question_type_compare', \\\n       'question_type_consequence', \\\n       'question_type_definition', \\\n       'question_type_entity', \\\n       'question_type_instructions', \n    ]\n    \n    oof_df[type_one_column_list] = scaler.fit_transform(oof_df[type_one_column_list])\n    \n    tmp = oof_df.copy(deep=True)\n    \n    for column in type_one_column_list:\n        \n        oof_df.loc[tmp[column] <= 0.16667, column] = 0\n        oof_df.loc[(tmp[column] > 0.16667) & (tmp[column] <= 0.41667), column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.41667) & (tmp[column] <= 0.58333), column] = 0.500000\n        oof_df.loc[(tmp[column] > 0.58333) & (tmp[column] <= 0.73333), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.73333), column] = 1\n    \n    \n    \n    ################################################# handle type 2 columns      \n#     type_two_column_list = [\n#         'question_type_spelling'\n#     ]\n    \n#     for column in type_two_column_list:\n#         if sum(tmp[column] > 0.15)>0:\n#             oof_df.loc[tmp[column] <= 0.15, column] = 0\n#             oof_df.loc[(tmp[column] > 0.15) & (tmp[column] <= 0.45), column] = 0.333333\n#             oof_df.loc[(tmp[column] > 0.45), column] = 0.666667\n#         else:\n#             t1 = max(int(len(tmp[column])*0.0013),2)\n#             t2 = max(int(len(tmp[column])*0.0008),1)\n#             thred1 = sorted(list(tmp[column]))[-t1]\n#             thred2 = sorted(list(tmp[column]))[-t2]\n#             oof_df.loc[tmp[column] <= thred1, column] = 0\n#             oof_df.loc[(tmp[column] > thred1) & (tmp[column] <= thred2), column] = 0.333333\n#             oof_df.loc[(tmp[column] > thred2), column] = 0.666667\n    \n    \n    \n    ################################################# handle type 3 columns      \n    type_three_column_list = [\n       'question_interestingness_self', \n    ]\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    oof_df[type_three_column_list] = scaler.fit_transform(oof_df[type_three_column_list])\n    tmp[type_three_column_list] = scaler.fit_transform(tmp[type_three_column_list])\n    \n    for column in type_three_column_list:\n        oof_df.loc[tmp[column] <= 0.385, column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.385) & (tmp[column] <= 0.47), column] = 0.444444\n        oof_df.loc[(tmp[column] > 0.47) & (tmp[column] <= 0.525), column] = 0.5\n        oof_df.loc[(tmp[column] > 0.525) & (tmp[column] <= 0.605), column] = 0.555556\n        oof_df.loc[(tmp[column] > 0.605) & (tmp[column] <= 0.715), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.715) & (tmp[column] <= 0.8), column] = 0.833333\n        oof_df.loc[(tmp[column] > 0.8) & (tmp[column] <= 0.94), column] = 0.888889\n        oof_df.loc[(tmp[column] > 0.94), column] = 1\n        \n        \n        \n    ################################################# handle type 4 columns      \n    type_four_column_list = [\n        'answer_satisfaction'\n    ]\n    scaler = MinMaxScaler(feature_range=(0.2, 1))\n    oof_df[type_four_column_list] = scaler.fit_transform(oof_df[type_four_column_list])\n    tmp[type_four_column_list] = scaler.fit_transform(tmp[type_four_column_list])\n    \n    for column in type_four_column_list:\n        \n        oof_df.loc[tmp[column] <= 0.233, column] = 0.200000\n        oof_df.loc[(tmp[column] > 0.233) & (tmp[column] <= 0.283), column] = 0.266667\n        oof_df.loc[(tmp[column] > 0.283) & (tmp[column] <= 0.315), column] = 0.300000\n        oof_df.loc[(tmp[column] > 0.315) & (tmp[column] <= 0.365), column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.365) & (tmp[column] <= 0.433), column] = 0.400000\n        oof_df.loc[(tmp[column] > 0.433) & (tmp[column] <= 0.483), column] = 0.466667\n        oof_df.loc[(tmp[column] > 0.483) & (tmp[column] <= 0.517), column] = 0.500000\n        oof_df.loc[(tmp[column] > 0.517) & (tmp[column] <= 0.567), column] = 0.533333\n        oof_df.loc[(tmp[column] > 0.567) & (tmp[column] <= 0.633), column] = 0.600000\n        oof_df.loc[(tmp[column] > 0.633) & (tmp[column] <= 0.683), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.683) & (tmp[column] <= 0.715), column] = 0.700000\n        oof_df.loc[(tmp[column] > 0.715) & (tmp[column] <= 0.767), column] = 0.733333\n        oof_df.loc[(tmp[column] > 0.767) & (tmp[column] <= 0.833), column] = 0.800000\n        oof_df.loc[(tmp[column] > 0.883) & (tmp[column] <= 0.915), column] = 0.900000\n        oof_df.loc[(tmp[column] > 0.915) & (tmp[column] <= 0.967), column] = 0.933333\n        oof_df.loc[(tmp[column] > 0.967), column] = 1\n    \n    \n    ################################################# round to i \/ 90 (i from 0 to 90)\n    oof_values = oof_df[TARGET_COLUMNS].values\n    DEGREE = len(oof_df)\/\/45*9\n#     if degree:\n#         DEGREE = degree\n#     DEGREE = 90\n    oof_values = np.around(oof_values * DEGREE) \/ DEGREE  ### 90 To be changed\n    oof_df[TARGET_COLUMNS] = oof_values\n    \n    return oof_df","d9932d55":"test = postprocessing(test)","17bb556c":"for column in TARGET_COLUMNS:\n    print(test[column].value_counts())","4ccfbde3":"sub = test[TARGET_COLUMNS].reset_index()","ac4f23ae":"sub[ sub[TARGET_COLUMNS] > 1.0] = 1.0","98c50be5":"sub.head()","8694619f":"test = pd.read_csv(f'{DATA_DIR}\/test.csv')","b79ad6f0":"n=test['url'].apply(lambda x:(('ell.stackexchange.com' in x) or ('english.stackexchange.com' in x))).tolist()\nspelling=[]\nfor x in n:\n    if x:\n        spelling.append(0.5)\n    else:\n        spelling.append(0.)","a196e620":"sub['question_type_spelling'] = spelling","c1ef5747":"sub['question_type_spelling'].value_counts()","5576f74c":"sub.to_csv('submission.csv', index=False)","02d55d5e":"## predict with bert-base-uncased","bfd2dac6":"### Required Imports\n\nI've added imports that will be used in training too","9a9b7e03":"## predict with bert-base-cased","130b7866":"### bert-base (uncased-v2, cased-v2) swa + xlnet (5 folds, seed 2333) + bert-base-uncased (question + answer) swa + bert-base-cased (question + answer) swa + xlnet (question + answer) swa + roberta (question + answer) + postprocessing (seefun's version)","6c2e5d39":"## predict with xlnet-base-cased question and answer","104e4cbd":"## predict with bert-base-uncased question and answer","c42c1f0a":"### Generate Submission","33fdf15c":"## predict with bert-base-cased question and answer","70cb8603":"**Pytorch BERT baseline**","3742b6c5":"# Assign postprocessed result","81b06b40":"## predict with roberta-base question and answer","e581bce2":"## predict with xlnet-base-cased","07d625b4":"### Define dataset","0a3103c9":"# Postprocessing","4157fb80":"## Build Model","d89a12da":"## predict with roberta-base"}}