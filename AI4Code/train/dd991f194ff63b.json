{"cell_type":{"98357ba7":"code","db8c1066":"code","66c66eca":"code","37d59070":"code","ff226187":"code","6b74e644":"code","6b8acfcc":"code","92533966":"code","f7d045d0":"code","81725557":"code","ad09f9ed":"code","8dbe5661":"code","f192a39b":"code","fb3a5e90":"code","e2467d62":"code","f3f3425b":"code","b6e05da9":"code","40c6f082":"code","5db47ead":"code","68cedea3":"code","33bda7f4":"code","1e86f16c":"code","6710bf15":"code","06e51b92":"code","1e718617":"code","8002b2f1":"code","2d23c3c9":"code","39f380f5":"code","b254b7b2":"code","1adee54e":"code","b4096e08":"code","b5ba8482":"code","1d7a81c0":"code","c452ac90":"code","9b3bb238":"code","e76736fd":"markdown","34bf8fda":"markdown","05f3456d":"markdown","2e7b6255":"markdown","7c255d87":"markdown","98d14404":"markdown","6247a025":"markdown","8fe01c7f":"markdown","545e8ccc":"markdown","d8f60192":"markdown","33ccfd51":"markdown","a9fa4233":"markdown","a8f724b3":"markdown","dba879e6":"markdown","3c6577a8":"markdown","61b03162":"markdown","4cd85f26":"markdown","d1453894":"markdown","1d3faa31":"markdown","c2a08a62":"markdown","d57836be":"markdown","2ba90ba2":"markdown","581735a0":"markdown","47d15832":"markdown","3b7ee0c3":"markdown","025aa71b":"markdown","09888b26":"markdown","2afb90ee":"markdown","f9a35694":"markdown","78d57330":"markdown","599c1748":"markdown","f3c8ee9f":"markdown","66cb595c":"markdown","43d5db19":"markdown"},"source":{"98357ba7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport string, re\nfrom bs4 import BeautifulSoup\nfrom wordcloud import WordCloud\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom keras.preprocessing import text, sequence\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf","db8c1066":"# Set the filepaths for each dataset\nfake_filepath = '..\/input\/fake-and-real-news-dataset\/Fake.csv'\ntrue_filepath = '..\/input\/fake-and-real-news-dataset\/True.csv'\n\n# Read the csv files from the filepaths\nfake_data = pd.read_csv(fake_filepath)\ntrue_data = pd.read_csv(true_filepath)","66c66eca":"true_data.head()","37d59070":"fake_data.head()","ff226187":"# Add a 'fake' column to each dataframe\ntrue_data['fake'] = 0\nfake_data['fake'] = 1\n\n# concatenate the two together\ndata = pd.concat([true_data, fake_data], ignore_index=True)\n\ndata.info()","6b74e644":"plt.figure(figsize=(12, 6))\n\nsns.countplot(x='subject', hue='fake', data=data)","6b8acfcc":"del data['subject']\n\ndata.head()","92533966":"del data['date']\n\ndata.head()","f7d045d0":"data['text'] = data['title'] + \" \" + data['text']\n\ndel data['title']\n\ndata.head()","81725557":"stop = set(stopwords.words('english'))\n# add punctuation to the list of stopwords\npunctuation = list(string.punctuation)\nstop.update(punctuation)\n# these were common words at the start of almost every real article, so I chose to remove them so the model is not biased\nstop.update(['washington', 'reuters', '(reuters)'])","ad09f9ed":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    return text\n\n#Apply function on review column\ndata['text']= data['text'].apply(denoise_text)\n\ndata.head()","8dbe5661":"# Text that is Fake\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop).generate(\" \".join(data[data.fake == 1].text))\nplt.imshow(wc , interpolation = 'bilinear')","f192a39b":"# Text that is not Fake\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop).generate(\" \".join(data[data.fake == 0].text))\nplt.imshow(wc , interpolation = 'bilinear')","fb3a5e90":"# set random seed for reproducibility\nnp.random.seed(1)\n\ndef train_dev_test_split(df, target_col, proportions):\n    \"\"\"\n    Helper function to randomly split a binary classification dataset into training, dev and test sets, \n    with proportional amounts of positive and negative examples\n    \"\"\"\n    assert(sum(proportions) == 1.0)\n    train_size, dev_size, test_size = proportions\n    \n    # separate the lists of indices for each value of the target variable\n    false_index = df[df[target_col] == 1].index\n    true_index = df[df[target_col] == 0].index\n    \n    # randomly choose train_size% of the positive and negative examples for the training set\n    false_train_indices = np.random.choice(false_index, int(train_size * len(false_index)), replace=False)\n    true_train_indices = np.random.choice(true_index, int(train_size * len(true_index)), replace=False)\n    train_indices = list(false_train_indices) + list(true_train_indices)\n    train = df.iloc[train_indices]\n    \n    # select the remaining data to split between dev and test sets\n    rem_df = df.iloc[list(set(df.index)-set(train_indices))]\n    rem_false_index = rem_df[rem_df[target_col] == 1].index\n    rem_true_index = rem_df[rem_df[target_col] == 0].index\n    false_dev_indices = np.random.choice(rem_false_index, int((dev_size \/ (dev_size + test_size)) * len(rem_false_index)), replace=False)\n    true_dev_indices = np.random.choice(rem_true_index, int((dev_size \/ (dev_size + test_size)) * len(rem_true_index)), replace=False)\n    dev_indices = list(false_dev_indices) + list(true_dev_indices)\n    dev = df.iloc[dev_indices]\n\n    # finally create the test set\n    test_indices = list(set(df.index) - set(train_indices + dev_indices))\n    test = df.iloc[test_indices]\n    return train, dev, test\n\ntrain, dev, test = train_dev_test_split(data, 'fake', (0.6, 0.2, 0.2))\nX_train, X_dev, X_test = train.text, dev.text, test.text\ny_train, y_dev, y_test = train.fake, dev.fake, test.fake","e2467d62":"sns.countplot(x='fake', data=data)","f3f3425b":"sns.countplot(x='fake', data=train)","b6e05da9":"sns.countplot(x='fake', data=dev)","40c6f082":"sns.countplot(x='fake', data=test)","5db47ead":"# Set max words and max length hyperparameters\nmax_features = 10000\nmax_len = 300","68cedea3":"# Fit the tokenizer on the training data\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)","33bda7f4":"# Tokenize and pad each set of texts\ntokenized_train = tokenizer.texts_to_sequences(X_train)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=max_len)\n\ntokenized_dev = tokenizer.texts_to_sequences(X_dev)\nX_dev = sequence.pad_sequences(tokenized_dev, maxlen=max_len)\n\ntokenized_test = tokenizer.texts_to_sequences(X_test)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=max_len)","1e86f16c":"EMBEDDING_FILE = '..\/input\/glove-twitter\/glove.twitter.27B.100d.txt'","6710bf15":"# Create a dictionary of words and their feature vectors from the embedding file\ndef get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","06e51b92":"all_embs = np.stack(list(embeddings_index.values()))\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\n\n# Find dims of embedding matrix\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n\n# Randomly initialize the embedding matrix\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n\n# Add each vector to the embedding matrix, corresponding to each token that we set earlier\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","1e718617":"batch_size = 256\nepochs = 10\nembed_size = 100","8002b2f1":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)","2d23c3c9":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=max_len, trainable=False))\n#LSTM \nmodel.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\nmodel.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\nmodel.add(Dense(units = 32 , activation = 'relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])","39f380f5":"model.summary()","b254b7b2":"history = model.fit(X_train, y_train, batch_size = batch_size , validation_data = (X_dev,y_dev) , epochs = epochs , callbacks = [learning_rate_reduction])","1adee54e":"# Save the model for future use\nmodel.save(\"model\")","b4096e08":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(X_train,y_train)[1]*100)\nprint(\"Accuracy of the model on Dev Data is - \" , model.evaluate(X_dev,y_dev)[1]*100)\nprint(\"Accuracy of the model on Test Data is - \" , model.evaluate(X_test,y_test)[1]*100)","b5ba8482":"epochs = [i for i in range(10)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Dev Accuracy')\nax[0].set_title('Training & Dev Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Dev Loss')\nax[1].set_title('Training & Dev Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","1d7a81c0":"pred = model.predict_classes(X_test)\n\nprint(classification_report(y_test, pred, target_names = ['Fake','Not Fake']))","c452ac90":"confusion_matrix(y_test,pred)","9b3bb238":"real_title = \"Portland protests: Trump threatens to send officers to more US cities\"\nreal_text = \"President Donald Trump has threatened to send more federal law enforcement officers to major US cities to control ongoing protests.\\\nMr Trump on Monday criticised a number of cities run by 'liberal Democrats', including Chicago and New York, saying their leaders were afraid to act.\\\nHe said officers sent to Oregon had done a 'fantastic job' restoring order amid days of protests in Portland.\\\nDemocrats accuse Mr Trump of trying to rally his Conservative base.\\\nPresident Trump, a Republican, has been trailing in opinion polls behind his Democratic rival, Joe Biden, ahead of November's election.\\\nLast month, Mr Trump declared himself the 'president of law and order' in the wake of widespread protests over the death in police custody of African-American man George Floyd.\\\nSpeaking at the White House on Monday, Mr Trump reiterated his call for law and order.\\\n'We're sending law enforcement,'' he told reporters. 'We can't let this happen to the cities.'\\\nHe specifically named New York City, Chicago, Philadelphia, Detroit, Baltimore and Oakland in discussing problems with violence.\"\n\nfake_title = \"FBI Agent, Who Exposed Hillary Clinton Cover-up, Found Dead\"\nfake_text = \"An FBI Special Agent, who was anticipated to expose the extent of Clinton and Obama malpractice and corruption in the \u201cOperation Fast and Furious\u201d \\\ncover-up before a US Federal Grand Jury, has been found dead at his home. The FBI official\u2019s wife was also found dead at the scene with the couple both being murdered \\\nusing the 52-year-old agent\u2019s own gun.Special Agent David Raynor was \u201cstabbed multiple times\u201d and \u201cshot twice with his own weapon,\u201d according to local media reports. \\\nRaynor\u2019s tragic death comes just one day before he was due to testify before a US Federal Grand Jury. \\\nHe was widely expected to testify that Hillary Clinton acted illegally to protect Obama administration crimes while covering up the Fast and Furious scandal. \\\nRaynor\u2019s wife, Donna Fisher, was also found dead at the scene. An autopsy will be completed to determine the exact cause of death, according to police. According to the \\\nBaltimore Sun:Authorities, who are offering a $215,000 reward for tips in Suiter\u2019s killing, have struggled to understand what happened. \\\nThe detective was shot with his own gun, which was found at the scene. Two other shots were fired from the gun, and Davis said there were signs of a brief \\\nstruggle.Special Agent Raynor\u2019s suspicious death is the latest in a sequence of disturbing deaths in Baltimore connected to the Clinton\/Obama cover-up of \\\nOperation Fast and Furious.When President Trump took power, the US Justice Department opened another investigation into Operation Fast and Furious as it pertained \\\nto the Baltimore Police Department and impaneled a US Federal Grand Jury. One of the main witnesses was Detective Sean Suiter, an 18-year veteran of the FBI.However, \\\nDetective Suiter was gunned down in November, in eerily similar circumstances to Special Agent Raynor, also one day before he could testify. \\\nSpecial Agent Raynor was leading US Deputy Attorney General Rod Rosenstein\u2019s and FBI Director Christopher Wray\u2019s investigation into the murder of Detective Sean Suiter. \\\nRaynor believed Suiter was silenced before he could testify that the Obama administration was criminally complicit in allowing guns to flow into the hands of criminals on the \\\nMexican border. \\\nThese guns were involved in the murder of a US Federal Officer, among others, and is seen by investigators as the \u201cAchilles heel of the Obama regime.\u201dThe murder of Border \\\nPatrol Agent Brian Terry is one of but a very few Obama administration crimes that have no statute of limitations as it involved the killing of a US Federal Officer. \\\nLeaked Wikileaks emails also prove Hillary Clinton was fully knowledgeable about the crime\u2014thus making her liable to criminal charges. \\\nLast\u2019s week\u2019s bombshell Inspector General\u2019s reports have exposed yet more Hillary Clinton and Obama Administration crimes.The report, that was released last Thursday, \\\nrevealed that the FBI had discovered evidence that Hillary Clinton and the Clinton Foundation had committed \u201csexual crimes against children.\u201d The report also shows that Obama \\\nlied to cover-up parts of these investigations that exposed child trafficking.However, the IG report proves that the evidence of these crimes has been covered-up and \\\nswept under the carpet by those acting at the highest levels.\"\n\n# Denoise and tokenise the title and article contents and add them to an array\ntext = [denoise_text(real_title + \" \" + real_text), denoise_text(fake_title + \" \" + fake_text)]\n\ntokenized_new = tokenizer.texts_to_sequences(text)\nX_new = sequence.pad_sequences(tokenized_new, maxlen=max_len)\n\n# Make predictions on the new examples\npredictions = model.predict_classes(X_new)\n\nmapping = {0: 'Not fake',\n           1: 'Fake'}\n\npredictions = np.vectorize(mapping.get)(predictions)\n\nprint(\"BBC News article is predicted to be: {}\".format(predictions[0, 0]))\nprint(\"Fake news article is predicted to be: {}\".format(predictions[1, 0]))","e76736fd":"While the first few rows of the false stories look like this:","34bf8fda":"The aim of this notebook is to provide an example of how to train an NLP model to detect fake news. Fake news is becoming a bigger and bigger issue in society, with disinformation becoming more widely shared on social media. So, a model to detect and filter out such stories would help to regain a lot of trust in the news we consume. The dataset used is approx. 20,000 examples of both real and fake news stories respectively, consisting of a title, text, subject and date. This notebook will preprocess the text, use GloVe word embeddings to map each word to a numeric vector, then pass the text through an LSTM network. The network will then classify each example as real or fake.","05f3456d":"We can see that the loss essentially decreases on each epoch, which could suggest further training may slightly improve accuracy even further! Let's now examine the classification report and confusion matrix of the model.","2e7b6255":"Our confusion matrix shows no particular bias to false negatives or false positives, and in general performs very well! Now, let's try out our model on some articles taken from the web. The first is a BBC news article about the Portland protests (not fake), while the second is a well-known fake news article about Hillary Clinton. We apply the same preprocessing steps as before to the text, then feed each one into the algorithm.","7c255d87":"The model is designed as follows: we first have the embedding layer, which takes each tokenized word to its unique feature vector. Then each sequence of feature vectors is passed through two LSTM units, of sizes 128 and 64 respectively, followed by a 32 unit fully connected layer, and a logistic unit with sigmoid activation function. ","98d14404":"# Text Cleaning","6247a025":"We now tokenise (assign it to a unique number) each word so that it can be passed into the embedding matrix later. We set a maximum number of words to be considered (10000). We also pad each sentence with zeros so that it is exactly 300 words in length.","8fe01c7f":"Unfortunately, the truth value of a story can be essentially predicted by the subject title, so we must remove the subject column to avoid overfitting to this particular dataset.","545e8ccc":"Now, let's split the data into training, development and testing sets. It is important that there is roughly an equal distribution of fake and true stories in each set, and the split will be 60-20-20.","d8f60192":"So now the training set, dev set and test set are stored in train, dev and test respectively. As we can see from the below plots, they each have an equal proportion of real and fake stories.","33ccfd51":"Let's create one big dataframe containing both real and fake stories, with a column named 'fake' that labels each type respectively.","a9fa4233":"Now we sit back and relax while backpropagation works its magic!","a8f724b3":"In order to pass text into a neural network model, we must first perform a number of preprocessing steps. First, we remove stopwords from the text. Stopwords are essentially words such as 'a', 'the' and 'and', which occur often in written text but essentially add no substance. We use an imported set of stopwords from the stopwords library.","dba879e6":"# Designing and Training the Model","3c6577a8":"The easiest column to explore initially is the subject column. Let's first plot the distribution of the different subject titles, and see how this relates to the truth value of the story.","61b03162":"Let's now visualise how each set of articles looks with a wordcloud.","4cd85f26":"# Imports","d1453894":"Now all the text has been padded and tokenised, we are ready to create the model. The first step in the model is to use the GloVe embedding to assign each word to a feature vector which has some meaning in the context of the model, instead of an arbitrary token. We load the GloVe embeddings from an external source.","1d3faa31":"Let's also remove the date column.","c2a08a62":"As a further regularisation technique, we add in learning rate reduction.","d57836be":"First, let's load the datasets.","2ba90ba2":"There is no obvious pattern in the word clouds, but we do see a slight bias towards 'Hillary Clinton' and 'Barack Obama' in the fake articles.","581735a0":"Now the embedding matrix is created, we need to set some other hyperparameters, before finally creating the network architecture. We intend to use batch gradient descent, so we need to set a batch size and a number of epochs. These are parameters that can be tuned!","47d15832":"# Model Performance","3b7ee0c3":"We now remove stopwords, and strip any unnecessary punctuation.","025aa71b":"Unsurprisingly, the model gets it spot on! A really satisfactory result on a really fun dataset to work with.","09888b26":"# News: Fake vs Real","2afb90ee":"The first few rows of the true stories are as follows:","f9a35694":"# **I hope you enjoyed reading this and found it informative. If you have any questions let me know in the comments!**","78d57330":"We see that the model performs remarkably well on each of our three datasets! Incredibly, it achieves over 99.5% accuracy on each of the three sets. Let's now see how the cost and the accuracy evolved over training.","599c1748":"# **Loading Datasets**","f3c8ee9f":"It would be useful to have the title and text of the article concatenated into one 'text' column, for easier modelling.","66cb595c":"Our first step is simply to check the accuracy of the model on each of the sets. ","43d5db19":"# Initial Data Exploration and Data Cleaning"}}