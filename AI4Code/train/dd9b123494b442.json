{"cell_type":{"a01fb2e6":"code","0e26d0e2":"code","49892bc6":"code","6f5f8fbd":"code","2088ed0c":"code","2044b02b":"code","79a63a5e":"code","8c5f3b38":"code","ec11c02d":"code","c106c90a":"code","fdf05aca":"code","f00342f8":"code","53fcd310":"code","e488fd75":"code","04927cda":"code","a0e12584":"code","c161a428":"code","080f0c14":"code","68fe2fd4":"code","14e2fdf3":"code","7ee60e30":"code","5fb5f628":"code","437a2a1b":"code","932f6baf":"code","ae23bb7b":"code","36eca79b":"code","9f5e3293":"markdown","7e495662":"markdown","5634eddb":"markdown","e24b98a1":"markdown","4d9eba9c":"markdown","e0f2711a":"markdown","9636550a":"markdown","8e7a7129":"markdown","6565b37a":"markdown","5d9b9b74":"markdown","ffabbd55":"markdown","11ddda57":"markdown","99cb8e4a":"markdown","3aea0d34":"markdown","61662206":"markdown","e531d4b5":"markdown","7d0db243":"markdown","73b4ff26":"markdown","0a144ea8":"markdown","a6652485":"markdown","d5e11f33":"markdown","457fe045":"markdown","53dc9d8c":"markdown","a79b5fe4":"markdown","8acd5594":"markdown","1a88915a":"markdown","4b47447c":"markdown","adaca1b8":"markdown","946e666e":"markdown","2c28a584":"markdown","b2643153":"markdown","03dfbbc8":"markdown","5f6d329c":"markdown","c4911d43":"markdown"},"source":{"a01fb2e6":"# Import pandas\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n#import matplotlib as plt\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import stats as ss\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV","0e26d0e2":"cc_apps = pd.read_csv(\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/credit-screening\/crx.data\",header=None,delimiter = ',')\n# Inspect data\ncc_apps.columns= ['Male','Age','Debt',\t'Married',\t\n                  'BankCustomer',\t'EducationLevel',\t'Ethnicity',\t'YearsEmployed',\t'PriorDefault',\t'Employed',\n                  'CreditScore',\t'DriversLicense',\t'Citizen',\t'ZipCode',\t'Income',\t'Approved']\n\nCC_data2 = cc_apps\ncc_apps","49892bc6":"# Print Descriptive statistics\ncc_apps.describe()","6f5f8fbd":"# Print DataFrame information \ncc_apps.tail(17)","2088ed0c":"# Replace the '?'s with Na\ncc_apps=cc_apps.replace('?',np.NaN)\n# Inspect the missing values again\ncc_apps.tail(17)","2044b02b":"# Impute the missing values with mean imputation\ncc_apps.fillna(cc_apps.mean(), inplace=True)\n\n# Count the number of NaNs in the dataset to verify \nprint(cc_apps.isnull().values.sum())","79a63a5e":"# Iterate over each column of cc_apps\nfor col in cc_apps.columns:\n    # Check if the column is of object type\n    if cc_apps[col].dtypes == 'object': \n        cc_apps[col] = cc_apps[col].fillna(cc_apps[col].mode().iloc[0])\n\n# Count the number of NaNs in the dataset and print the counts to verify.\nprint(cc_apps.isnull().values.sum())","8c5f3b38":"#verify null value\ncc_apps.tail(17)","ec11c02d":"def plotDistPlot(col):\n    sns.distplot(col)\n    plt.show()\n    \nplotDistPlot(cc_apps['Age'])\nplotDistPlot(cc_apps['Debt'])\nplotDistPlot(cc_apps['YearsEmployed'])\nplotDistPlot(cc_apps['CreditScore'])\nplotDistPlot(cc_apps['Income'])","c106c90a":"#correlation matrix\ncorrmat = cc_apps.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","fdf05aca":"sns.set()\ncols = ['Age', 'Income', 'CreditScore', 'Debt', 'YearsEmployed','Approved']\nsns.pairplot(cc_apps[cols],hue='Approved', size = 2.5)\nplt.show();","f00342f8":"def corr_plot(x, y, data, xlab, ylab, hue='Approved'):\n    ax = sns.relplot(x=x, y=y, hue=hue, data=data);\n    ax.set(xlabel=xlab, ylabel=ylab)\n    for t, l in zip(ax._legend.texts, ['Accepted', 'Declined']): \n        t.set_text(l)\n    ax.add_legend(loc='center right', bbox_to_anchor=(1.25, 0.5), ncol=1)\n\n    return ax\n","53fcd310":" ax = corr_plot('CreditScore', 'Debt', data = cc_apps, xlab = 'Credit Score', ylab = 'Debt') ","e488fd75":"import matplotlib as plt\nax = corr_plot('CreditScore', 'Income', data = cc_apps, xlab = 'Credit Score', ylab = 'Income')\nplt.pyplot.xlim(-1,25)\nplt.pyplot.ylim(-1,20000)\nplt.pyplot.show()","04927cda":"cc_apps['Approved'].value_counts().sort_index()","a0e12584":"import matplotlib.pyplot as plt\nlabels = ['Approved', 'Declined']\ncolors = ['pink', 'lightblue']\nsize = [307, 383]\nexplode = [0.05, 0.02]\n\nplt.rcParams['figure.figsize'] = (8, 8)\nplt.pie(size, labels = labels, colors = colors, explode = explode, shadow = True, autopct = \"%.1f%%\")\nplt.title('Credit Card Approval', fontsize = 20)\nplt.axis('off')\nplt.legend()\nplt.show()","c161a428":"# Import LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\n# Instantiate LabelEncoder\nle=LabelEncoder()\n\n# Iterate over all the values of each column and extract their dtypes\nfor col in cc_apps.columns:\n    # Compare if the dtype is object\n    if cc_apps[col].dtypes=='object':\n    # Use LabelEncoder to do the numeric transformation\n        cc_apps[col]=le.fit_transform(cc_apps[col])\ncc_apps.info()","080f0c14":"# Import train_test_split\nfrom sklearn.model_selection import train_test_split\n# Drop the features 11 and 13 and convert the DataFrame to a NumPy array\ncc_apps = cc_apps.drop(['DriversLicense', 'ZipCode'], axis=1)\ncc_apps = cc_apps.values\n\n# Segregate features and labels into separate variables\nX,y = cc_apps[:,0:13] , cc_apps[:,13]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                y,\n                                test_size=0.20,\n                                random_state=42)\n","68fe2fd4":"# Import MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\n# Instantiate MinMaxScaler and use it to rescale X_train and X_test\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.fit_transform(X_test)","14e2fdf3":"# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Instantiate a LogisticRegression classifier with default parameter values\nlogreg = LogisticRegression()\n# Fit logreg to the train set\nlogreg.fit(rescaledX_train,y_train)","7ee60e30":"# Use logreg to predict instances from the test set and store it\ny_pred = logreg.predict(rescaledX_test)\n\n# Get the accuracy score of logreg model and print it\nprint(\"Accuracy of logistic regression classifier: \", logreg.score(rescaledX_test,y_test))","5fb5f628":"# Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n# Define the grid of values for tol and max_iter\ntol = [0.01,0.001,0.0001]\nmax_iter = [100,150,200]\n# Create a dictionary where tol and max_iter are keys and the lists of their values are corresponding values\nparam_grid = dict(tol=tol, max_iter=max_iter)","437a2a1b":"# Instantiate GridSearchCV with the required parameters\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\n# Use scaler to rescale X and assign it to rescaledX\nrescaledX = scaler.fit_transform(X)\n\n# Fit data to grid_model\ngrid_model_result = grid_model.fit(rescaledX, y)\n\n# Summarize results\nbest_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\n\n# Creating a dictionary to save the best results\nbest_models = {'Logistic': best_score}\n\n\nprint(f'Best Score: {best_score}, using {best_params}')","932f6baf":"from sklearn.ensemble import RandomForestClassifier \nrf = RandomForestClassifier(n_estimators=500)\nrf.fit(rescaledX_train, y_train)\ny_pred = rf.predict(rescaledX_test)\nprint(\"Random Forest classifier has accuracy of: \", rf.score(rescaledX_test, y_test)) ","ae23bb7b":"importances = rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()","36eca79b":"# copy of CC_data is in CC_data2\nCC_data2 = CC_data2.drop(['Approved'], axis=1)\n\nfeatures = CC_data2.columns\nimportances = rf.feature_importances_\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","9f5e3293":"\n**Is there any relation between debt level and credit score?**","7e495662":"##Introduction\n**Commercial** banks receive\u00a0a lot\u00a0of  applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual\u2019s credit report. \n\n**The decision** of approving a credit card is majorly\ndependent on the personal and financial background of\u00a0 the\u00a0applicant. Factors like, age, gender, income, employment status, credit history and other attributes all carry weight in the approval decision\n\n\n","5634eddb":"## 6. Preprocessing the data (part i)\n<p>The missing values are now successfully handled.<\/p>\n<p>There is still some minor but essential data preprocessing needed before we proceed towards building our machine learning model. We are going to divide these remaining preprocessing steps into three main tasks:<\/p>\n<ol>\n<li>Convert the non-numeric data into numeric.<\/li>\n<li>Split the data into train and test sets. <\/li>\n<li>Scale the feature values to a uniform range.<\/li>\n<\/ol>\n<p>First, we will be converting all the non-numeric values into numeric ones. We do this because not only it results in a faster computation but also many machine learning models (and especially the ones developed using scikit-learn) require the data to be in a strictly numeric format.  ","e24b98a1":"This graph indicates the feature importance for the given data set in making the decision of credit card request should get approved or not. Prior default is coming as one of the most important features, followed by credit score, years of employment, debt, income, and age. Bottom 4 are not at all playing any significant role.","4d9eba9c":"## Splitting the dataset into train and test sets\n<p>We have successfully converted all the non-numeric values to numeric ones.<\/p>\n<p>Now, we will split our data into train set and test set to prepare our data for two different phases of machine learning modeling: training and testing. Ideally, no information from the test data should be used to scale the training data or should be used to direct the training process of a machine learning model. Hence, we first split the data and then apply the scaling.<\/p>\n<p>Also, features like <code>DriversLicense<\/code> and <code>ZipCode<\/code> are not as important as the other features in the dataset for predicting credit card approvals. We should drop them to design our machine learning model with the best set of features. In Data Science literature, this is often referred to as <em>feature selection<\/em>. <\/p>","e0f2711a":"##Problem Statement\nBanking industries received so many applications for credit card request. Going through each request manually can be very time consuming, also prone to human errors.\n\n\nA credit card application might be denied \n\n\n\n*   A Mistake on the application \n*   A error on your credit report \n*   Insufficient income\n*   Shaky credit history \n*   Too many credit application at on time\n\n\n\n\n\n\n","9636550a":"## Table of contents\n\n1. [Introduction](#Introduction)\n \n2. [Required libraries](#Required-libraries)\n\n4. [Problem Statement](#Required-libraries)\n\n5. [Ojbectives](#Ojbectives)\n\n6. [Checking the data](#Checking-the-data)\n\n7. [Handling the missing values (part i)](#Handling-the-missing-values-(part-i))\n\n8. [Handling the missing values (part ii)](#Handling-the-missing-values-(part-i))\n\n9. [Exploratory Data Analysis(EDA)](#Exploratory-Data-Analysis(EDA))\n\n10. [Splitting the dataset into train and test sets](#Splitting-the-dataset)\n\n11. [Preprocessing the data](#Preprocessing-the-data)\n\n12. [Data Modeling and Evaluation](#Data-Modeling-and-Evaluation)\n\n    - [Logistic Regession](#Logistic-Regession)\n\n    - [Random Forest](#Forest](#Parameter-tuning))\n\n\n12. [Conclusions](#Conclusions)\n\n\n13. [Acknowledgements](#Acknowledgements)","8e7a7129":"###Grid searching and making the model perform better","6565b37a":"**Plotting the Correlation Matrix**","5d9b9b74":"##Ojbectives\nThis task can be automated with the power of machine learning\nand pretty much every commercial bank does so nowadays. \nIn this project, you will build an automatic credit card approval \npredictor using machine learning techniques\n\nBy analysing the data, we will build a predictor \nmodel by using some well-known pre-processing methods\nsuch as \n\n\n*   imputing missing values\n*   label encoding\n*   scaling the columns values\n*   modeling and evaduation\n\n\n\n","ffabbd55":"These initial plots showed that all variables have distributions that are skewed to the right, indicating that the data is not well-distributed about the mean. In order to reduce the skew,log transformations were applied and then plotted again.\n\n","11ddda57":"We can discern that there is no clear correlation between the credit score of an individual and the debt associated with them. Though it seems like individual with lower credit scores had their applications declined, still the applications were approved for people with higher debt and higher credit score was approved. Hence, the business must set some factor of debt to credit score for making sure that in case of major national or global financial adversity or the adversity faced by individual's own financial decisions, they are still able to maintain payments of their bill to considerable extent.\n\n","99cb8e4a":"##Data Modeling and Evaluation\n### Logistic Regression","3aea0d34":"**Load dataset**","61662206":"Random Forest Classifier, we can find the most important features","e531d4b5":"##8. Exploratory Data Analysis(EDA)","7d0db243":"## Acknowledgements\n\nMany thanks to [Ashish Tripathi](https:\/\/medium.com\/@ashish.tripathi1207\/credit-card-approval-prediction-model-in-python-c0e07677058e\/) that provide  articles of credit card approval in medium and many articles from kaggles:\n\n*   https:\/\/www.kaggle.com\/noureddineizmar\/predicting-credit-card-approvals,\n\n*   https:\/\/www.kaggle.com\/mmmarchetti\/tutorial-predicting-credit-card-aprovals\n*  https:\/\/www.kaggle.com\/rikdifos\/credit-card-approval-prediction-using-ml\n\n\n\n","73b4ff26":"### Finding the best performing model","0a144ea8":"##Handling the missing values (part i)\n<p>We've uncovered some issues that will affect the performance of our machine learning model(s) if they go unchanged:<\/p>\n<ul>\n<li>Our dataset contains both numeric and non-numeric data (specifically data that are of <code>float64<\/code>, <code>int64<\/code> and <code>object<\/code> types). Specifically, the features 2, 7, 10 and 14 contain numeric values (of types float64, float64, int64 and int64 respectively) and all the other features contain non-numeric values.<\/li>\n<li>The dataset also contains values from several ranges. Some features have a value range of 0 - 28, some have a range of 2 - 67, and some have a range of 1017 - 100000. Apart from these, we can get useful statistical information (like <code>mean<\/code>, <code>max<\/code>, and <code>min<\/code>) about the features that have numerical values. <\/li>\n<li>Finally, the dataset has missing values, which we'll take care of in this task. The missing values in the dataset are labeled with '?', which can be seen in the last cell's output.<\/li>\n<\/ul>\n<p>Now, let's temporarily replace these missing value question marks with NaN.<\/p>","a6652485":"On looking at the target field, it seems a fair balance data. This means we will have enough observation to train our model for both the classes in order to predict correctly. I indicated approval and 0 indicated rejections.\n","d5e11f33":"To start with, the distribution of 5 continuous variables Age, Debt, Credit Score, Income and Years employed was observed to get a sense of the nature of the dataset.\n\n","457fe045":"Its confirmed ! The individuals having higher Income and higher credit score get their applications approved. Whereas, the ones who do not have good Income and good credit score lie on the bottom left corner of our chart in orange which means that they are the ones who were declined.\n\nThe other insight that is easily discernible from above is the people good credit history and lower income do get their applications approved. Hence, this bolsters our assumption or generic fact in market that the people with good credit history get credit cards easily. Thus, credit score is dominating variable it seems in determing the dependent variable.\n\n","53dc9d8c":"##Required libraries\nThe primary libraries that we'll be using are:\n\n* **NumPy**: Provides a fast numerical array structure and helper functions.\n* **pandas**: Provides a DataFrame structure to store data in memory and work with it easily and efficiently.\n* **scikit-learn**: The essential Machine Learning package in Python.\n* **matplotlib**: Basic plotting library in Python; most other Python plotting libraries are built on top of it.\n* **Seaborn**: Advanced statistical plotting library. ","a79b5fe4":"##Checking the data","8acd5594":"Credit Card Applications Approval\n\n**Subject**: Artificial intelligence (AI)\n\n**Lecturer**:  Mr.  Rina  Buoy\n\n**Team Members**:\n\n*   Ry Run\n*   Sek  Samnang\n* Sopha  Rothmony\n* Khy  Vichka","1a88915a":"##Handling the missing values (part iii)\n<p>We have successfully taken care of the missing values present in the numeric columns. There are still some missing values to be imputed for columns 0, 1, 3, 4, 5, 6 and 13. All of these columns contain non-numeric data and this why the mean imputation strategy would not work here. This needs a different treatment. <\/p>\n<p>We are going to impute these missing values with the most frequent values as present in the respective columns.","4b47447c":"##Conclusions\nFrom this initial analysis, we are able to conclude  that the most significant factors in determining the outcome of a credit application are Employment, Income, Credit Score and Prior Default.\n\nBased on these insights, we can work on building some predictive models. They can be used by analysts in financial sector and be incorporated to automate the credit approval process. These results can also serve as a source of information for the consumers.\n\nModern credit analyses employ many additional variables like the criminal records of applicants, their health information, net balance between monthly income and expenses. A dataset with these variables could be acquired. It\u2019s also possible to add complementary variables to the dataset. This will make the credit simulations more  , similar to what is done by the banks before a credit is approved\n\nFurthermore, On  applying  the  **Random  Forest  Model**,  we  have  achieved  the  accuracy  of  **86%**  which  is  significantly  highest\n","adaca1b8":"## 8. Preprocessing the data (part ii)\n<p>The data is now split into two separate sets - train and test sets respectively. We are only left with one final preprocessing step of scaling before we can fit a machine learning model to the data. <\/p>\n<p>Now, let's try to understand what these scaled values mean in the real world. Let's use <code>CreditScore<\/code> as an example. The credit score of a person is their creditworthiness based on their credit history. The higher this number, the more financially trustworthy a person is considered to be. So, a <code>CreditScore<\/code> of 1 is the highest since we're rescaling all the values to the range of 0-1.<\/p>","946e666e":"**Import Libraries**","2c28a584":"The correlation matrix graphically gives us an idea of how features correlate with each other and can help us predict what are the features that are most relevant for the prediction.\n\n","b2643153":"**Is there a correlation between the Income earned by an individual and the credit score they posses?**","03dfbbc8":"##Handling the missing values (part ii)\n<p>We replaced all the question marks with NaNs. This is going to help us in the next missing value treatment that we are going to perform.<\/p>\n<p>An important question that gets raised here is <em>why are we giving so much importance to missing values<\/em>? Can't they be just ignored? Ignoring missing values can affect the performance of a machine learning model heavily. While ignoring the missing values our machine learning model may miss out on information about the dataset that may be useful for its training. Then, there are many models which cannot handle missing values implicitly such as LDA. <\/p>\n<p>So, to avoid this problem, we are going to impute the missing values with a strategy called mean imputation.<\/p>","5f6d329c":"## Random Forest Classifier","c4911d43":"**Percentage of Credit Cards Approval**"}}