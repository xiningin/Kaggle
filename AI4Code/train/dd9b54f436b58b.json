{"cell_type":{"c3f6bb39":"code","342c51f9":"code","fe90d01f":"code","fd7da173":"code","8eef0c83":"code","373aad4e":"code","8ed21275":"code","d203feac":"code","76030833":"code","3c1ebd8b":"code","ac9846db":"code","aef8d09c":"code","634e35c0":"code","31c01538":"code","f67b2d95":"code","e7d11563":"code","3aa3aea5":"code","ac544cb6":"code","35983377":"code","88279212":"markdown","36901b89":"markdown","19faf4b2":"markdown","e6887f45":"markdown","6fb09803":"markdown","2df92f69":"markdown","ff59be05":"markdown","c8a18a24":"markdown","621e0b13":"markdown","b0054d06":"markdown"},"source":{"c3f6bb39":"import gc\nimport os\nimport warnings\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm_notebook\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, GlobalAveragePooling2D\nfrom keras import layers\nfrom keras.optimizers import SGD, RMSprop\n\n\nimport os\nprint(os.listdir(\"..\/input\"))","342c51f9":"#efficientnet download\n!pip install -U efficientnet==0.0.4\nfrom efficientnet import EfficientNetB3","fe90d01f":"#crop data directory\nDATA_PATH = '..\/input\/car-crop'\nos.listdir(DATA_PATH)","fd7da173":"#original data directory\nDATA_PATH2 = '..\/input\/2019-3rd-ml-month-with-kakr'\nos.listdir(DATA_PATH2)","8eef0c83":"#semi_data directory\nDATA_PATH3 = '..\/input\/semi-detaset'\nos.listdir(DATA_PATH3)","373aad4e":"#crop merge directory\nDATA_PATH4 = '..\/input\/car-crop2'\nos.listdir(DATA_PATH4)","8ed21275":"# \uc774\ubbf8\uc9c0 \ud3f4\ub354 \uacbd\ub85c\nTRAIN_IMG_PATH = os.path.join(DATA_PATH, 'train')\nTEST_IMG_PATH = os.path.join(DATA_PATH, 'test')\n\n# CSV \ud30c\uc77c \uacbd\ub85c\ndf_train = pd.read_csv(os.path.join(DATA_PATH2, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(DATA_PATH2, 'test.csv'))\ndf_class = pd.read_csv(os.path.join(DATA_PATH2, 'class.csv'))\n\n# \ubc84\uc804 1\uc758 submission load\ndf_semi = pd.read_csv(os.path.join(DATA_PATH3, 'Pseudo Labelsing.csv'))\n\n#\ubc84\uc804 1\uc5d0\uc11c test\ub97c tset\ub85c \uc800\uc7a5\ud558\uc5ec \ubcc0\uacbd\ud574\uc90c \nname = list(map(lambda x:  x.replace(\"tset\", \"test\"),df_semi['img_file']))\ndf_semi['img_file']=name\ndf_semi['img_file'] = df_semi['img_file']+'.jpg'\ndf_semi.head(5)","d203feac":"df_train[\"class\"] = df_train[\"class\"].astype('str')\ndf_semi[\"class\"] = df_semi[\"class\"].astype('str')\ndf_train = df_train[['img_file', 'class']]\ndf_test = df_test[['img_file']]\n\n# train\uacfc semi \ub370\uc774\ud130 \ubcd1\ud569\ndf_train2 = pd.concat([df_train, df_semi],axis=0)\n\n\nits = np.arange(df_train2.shape[0])\ntrain_idx, val_idx = train_test_split(its, train_size = 0.8, random_state=42)\n\nX_train = df_train2.iloc[train_idx, :]\nX_val = df_train2.iloc[val_idx, :]\n\nprint(X_train.shape)\nprint(X_val.shape)\nprint(df_test.shape)\ndf_train2.head(5)","76030833":"#ref: https:\/\/github.com\/yu4u\/cutout-random-erasing\/blob\/master\/cifar10_resnet.py\ndef get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1\/0.3, v_l=0, v_h=255, pixel_level=False):\n    def eraser(input_img):\n        img_h, img_w, img_c = input_img.shape\n        p_1 = np.random.rand()\n\n        if p_1 > p:\n            return input_img\n\n        while True:\n            s = np.random.uniform(s_l, s_h) * img_h * img_w\n            r = np.random.uniform(r_1, r_2)\n            w = int(np.sqrt(s \/ r))\n            h = int(np.sqrt(s * r))\n            left = np.random.randint(0, img_w)\n            top = np.random.randint(0, img_h)\n\n            if left + w <= img_w and top + h <= img_h:\n                break\n\n        if pixel_level:\n            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n        else:\n            c = np.random.uniform(v_l, v_h)\n\n        input_img[top:top + h, left:left + w, :] = c\n\n        return input_img\n\n    return eraser","3c1ebd8b":"import keras.backend as K\nfrom keras.legacy import interfaces\nfrom keras.optimizers import Optimizer\n\n\nclass AdamAccumulate(Optimizer):\n\n    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., amsgrad=False, accum_iters=1, **kwargs):\n        if accum_iters < 1:\n            raise ValueError('accum_iters must be >= 1')\n        super(AdamAccumulate, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.lr = K.variable(lr, name='lr')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n        self.amsgrad = amsgrad\n        self.accum_iters = K.variable(accum_iters, K.dtype(self.iterations))\n        self.accum_iters_float = K.cast(self.accum_iters, K.floatx())\n\n    @interfaces.legacy_get_updates_support\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n\n        completed_updates = K.cast(K.tf.floordiv(self.iterations, self.accum_iters), K.floatx())\n\n        if self.initial_decay > 0:\n            lr = lr * (1. \/ (1. + self.decay * completed_updates))\n\n        t = completed_updates + 1\n\n        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) \/ (1. - K.pow(self.beta_1, t)))\n\n        # self.iterations incremented after processing a batch\n        # batch:              1 2 3 4 5 6 7 8 9\n        # self.iterations:    0 1 2 3 4 5 6 7 8\n        # update_switch = 1:        x       x    (if accum_iters=4)  \n        update_switch = K.equal((self.iterations + 1) % self.accum_iters, 0)\n        update_switch = K.cast(update_switch, K.floatx())\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        gs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        else:\n            vhats = [K.zeros(1) for _ in params]\n\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        for p, g, m, v, vhat, tg in zip(params, grads, ms, vs, vhats, gs):\n\n            sum_grad = tg + g\n            avg_grad = sum_grad \/ self.accum_iters_float\n\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * avg_grad\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(avg_grad)\n\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                p_t = p - lr_t * m_t \/ (K.sqrt(vhat_t) + self.epsilon)\n                self.updates.append(K.update(vhat, (1 - update_switch) * vhat + update_switch * vhat_t))\n            else:\n                p_t = p - lr_t * m_t \/ (K.sqrt(v_t) + self.epsilon)\n\n            self.updates.append(K.update(m, (1 - update_switch) * m + update_switch * m_t))\n            self.updates.append(K.update(v, (1 - update_switch) * v + update_switch * v_t))\n            self.updates.append(K.update(tg, (1 - update_switch) * sum_grad))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, (1 - update_switch) * p + update_switch * new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon,\n                  'amsgrad': self.amsgrad}\n        base_config = super(AdamAccumulate, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","ac9846db":"# Parameter\nimg_size = (300, 300)\nimage_size = 300\nnb_train_samples = len(X_train)\nnb_validation_samples = len(X_val)\nnb_test_samples = len(df_test)\nepochs = 30\nbatch_size = 32\n\n# Define Generator config\ntrain_datagen =ImageDataGenerator(\n    rescale=1.\/255,\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=False,\n    fill_mode='nearest',\n    preprocessing_function = get_random_eraser(v_l=0, v_h=1),\n    )\n\nval_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)","aef8d09c":"#generator\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=X_train, \n    directory='..\/input\/car-crop2\/train2_crop',\n    x_col = 'img_file',\n    y_col = 'class',\n    target_size = img_size,\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=batch_size,\n    seed=42\n)\n\nvalidation_generator = val_datagen.flow_from_dataframe(\n    dataframe=X_val, \n    directory='..\/input\/car-crop2\/train2_crop',\n    x_col = 'img_file',\n    y_col = 'class',\n    target_size = img_size,\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=batch_size\n)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=df_test,\n    directory='..\/input\/car-crop\/test_crop',\n    x_col='img_file',\n    y_col=None,\n    target_size= img_size,\n    color_mode='rgb',\n    class_mode=None,\n    batch_size=batch_size,\n    shuffle=False\n)","634e35c0":"#model\nopt = AdamAccumulate(lr=0.001, decay=1e-5, accum_iters=5)\nEfficientNet_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n\n\nmodel = Sequential()\nmodel.add(EfficientNet_model)\nmodel.add(layers.GlobalAveragePooling2D())\nmodel.add(layers.Dense(2048, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(196, activation='softmax'))\nmodel.summary()\n\n#compile\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])","31c01538":"def get_steps(num_samples, batch_size):\n    if (num_samples % batch_size) > 0 :\n        return (num_samples \/\/ batch_size) + 1\n    else :\n        return num_samples \/\/ batch_size","f67b2d95":"%%time\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\n#model path\nMODEL_SAVE_FOLDER_PATH = '.\/model\/'\nif not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n    os.mkdir(MODEL_SAVE_FOLDER_PATH)\n\nmodel_path = MODEL_SAVE_FOLDER_PATH + '{epoch:02d}-{val_loss:.4f}.hdf5'\n\npatient = 3\ncallbacks_list = [\n     EarlyStopping(\n        # \ubaa8\ub378\uc758 \uac80\uc99d \uc815\ud655\ub3c4 \ubaa8\ub2c8\ud130\ub9c1\n        monitor='val_loss',\n        # patient(\uc815\uc218)\ubcf4\ub2e4 \uc815\ud655\ub3c4\uac00 \ud5a5\uc0c1\ub418\uc9c0 \uc54a\uc73c\uba74 \ud6c8\ub828 \uc885\ub8cc\n        patience=patient, \n        # \uac80\uc99d\uc5d0 \ub300\ud574 \ud310\ub2e8\ud558\uae30 \uc704\ud55c \uae30\uc900, val_loss\uacbd\uc6b0 \uac10\uc18c\ub418\ub294 \uac83\uc774\ubbc0\ub85c min\n        mode='min', \n        #\uc5bc\ub9c8\ub098 \uc790\uc138\ud558\uac8c \uc815\ubcf4\ub97c \ub098\ud0c0\ub0bc\uac83\uc778\uac00.\n        verbose=1\n                          \n    ),\n    ReduceLROnPlateau(\n        monitor = 'val_loss', \n        #\ucf5c\ubc31 \ud638\ucd9c\uc2dc \ud559\uc2b5\ub960(lr)\uc744 \uc808\ubc18\uc73c\ub85c \uc904\uc784\n        factor = 0.5, \n        #\uc704\uc640 \ub3d9\uc77c\n        patience = patient \/ 2, \n        #\ucd5c\uc18c\ud559\uc2b5\ub960\n        min_lr=0.00001,\n        verbose=1,\n        mode='min'\n    ),\n    ModelCheckpoint(\n        filepath=model_path,\n        monitor ='val_loss',\n        # val_loss\uac00 \uc88b\uc9c0 \uc54a\uc73c\uba74 \ubaa8\ub378\ud30c\uc77c\uc744 \ub36e\uc5b4\uc4f0\uc9c0 \uc54a\ub294\ub2e4\n        save_best_only = True,\n        verbose=1,\n        mode='min') ]\n\n    \n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = get_steps(nb_train_samples, batch_size),\n    epochs=epochs,\n    validation_data = validation_generator,\n    validation_steps = get_steps(nb_validation_samples, batch_size),\n    callbacks = callbacks_list\n)\ngc.collect()","e7d11563":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, label='Training acc')\nplt.plot(epochs, val_acc, label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.ylim(0.9,1)\nplt.show()","3aa3aea5":"plt.plot(epochs, loss, label='Training loss')\nplt.plot(epochs, val_loss, label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.ylim(0,0.5)\nplt.show()","ac544cb6":"%%time\ntest_generator.reset()\nprediction = model.predict_generator(\n    generator = test_generator,\n    steps = get_steps(nb_test_samples, batch_size),\n    verbose=1\n)","35983377":"submission = pd.read_csv(os.path.join(DATA_PATH2, 'sample_submission.csv'))\npredicted_class_indices=np.argmax(prediction, axis=1)\n\n# Generator class dictionary mapping\nlabels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\n\nsubmission[\"class\"] = predictions\nsubmission.to_csv(\"submission_all.csv\", index=False)\nsubmission.head()","88279212":"# Package","36901b89":"### \ubc30\uacbd\n- \uc774\ubc88 \ub300\ud68c\uc758 class\ub294 196\uac1c\ub85c \ub9e4\uc6b0 \ub9ce\uc2b5\ub2c8\ub2e4. \n- \ud6c8\ub828\ub370\uc774\ud130\uc14b\uc744 class\ub85c \ub098\ub204\uac8c \ub418\uba74 \ub370\uc774\ud130\uac00 \ub9e4\uc6b0 \uc801\uace0 \uadf8\ub7ec\ubbc0\ub85c \ub354\uc6b1 \ub354 \ub9ce\uc740 \ub370\uc774\ud130\uac00 \uc788\uc73c\uba74 \uc798 \ud559\uc2b5\uc2dc\ud0ac \uc218 \uc788\uc9c0\uc54a\uc744\uae4c? \uc0dd\uac01\ud588\uc2b5\ub2c8\ub2e4.\n\n### Semi-supervised Learning\n- \uadf8\ub798\uc11c Semi-supervised Learning \uae30\ubc95 \uc911 Pseudo Labelling\uc744 \uc0ac\uc6a9\ud558\ub824\uace0 \ud569\ub2c8\ub2e4.\n- Pseudo Labelling\uc758 \uc808\ucc28\ub294  1) \ud6c8\ub828\ub370\uc774\ud130\ub97c \ud1b5\ud55c \ubaa8\ub378\uc0dd\uc131 2) \ud14c\uc2a4\ud2b8\ub370\uc774\ud130 \uc608\uce21 3) \ud6c8\ub828\ub370\uc774\ud130\uc5d0 \ud655\uc2e4\ud558\uac8c \uc608\uce21\ub41c \ud14c\uc2a4\ud2b8\ub370\uc774\ud130 \ucd94\uac00 4) \uacb0\ud569\ub370\uc774\ud130\ub97c \ud1b5\ud55c \ubaa8\ub378\uc0dd\uc131 5) \ud14c\uc2a4\ud2b8\ub370\uc774\ud130 \uc608\uce21\uc785\ub2c8\ub2e4.\n\n### reference\n- [version2](https:\/\/www.kaggle.com\/kimtaegwan\/keras-semi-supervised-learning?scriptVersionId=18397555)[Public 0.930] \uc744 \ud1b5\ud558\uc5ec test\uc14b\uc758 label\uc744 0.99\uc774\uc0c1\uc73c\ub85c \uc608\uce21\ud55c testset\ub9cc \uac00\uc838\uc654\uc2b5\ub2c8\ub2e4.\n\n baseline \uc7a1\uc744 \ub54c \ub3c4\uc6c0\ub418\ub294 \ucee4\ub110\uc785\ub2c8\ub2e4. \ucc38\uace0\ud558\uc2dc\uace0 \ub9ce\uc774 upvote \ub20c\ub7ec\uc8fc\uc138\uc694~\n- [3rd ML Month Car Model Classification Baseline](https:\/\/www.kaggle.com\/fulrose\/3rd-ml-month-car-model-classification-baseline)\n- [3rd ML Month Car Model Classification (Xception)](https:\/\/www.kaggle.com\/janged\/3rd-ml-month-car-model-classification-xception)\n- [3rd ML Month Keras EfficientNet](https:\/\/www.kaggle.com\/easter3163\/3rd-ml-month-keras-efficientnet)\n- [Cutout Augmentation (on Keras EfficientNet)](https:\/\/www.kaggle.com\/seriousran\/cutout-augmentation-on-keras-efficientnet)","19faf4b2":"# Parameter","e6887f45":"# Predict","6fb09803":"# Model","2df92f69":"# acc \/ loss Plot","ff59be05":"# <center>3rd ML Month - Keras Semi-supervised Learning <\/center>","c8a18a24":"# train\/test data Split","621e0b13":"# File Directory Setting","b0054d06":"# Submission"}}