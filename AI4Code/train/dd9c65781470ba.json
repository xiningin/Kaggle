{"cell_type":{"ab154fcc":"code","eaa994d0":"code","f0247b65":"code","f28e695a":"code","50f7c41a":"code","359182f7":"code","ce3bf619":"code","af2a22fc":"code","41fe25af":"code","f16a2961":"code","ee5f1aa5":"code","efe32ace":"code","49fd3a39":"code","7f2d8587":"code","cd4d8eb3":"code","bb4064cf":"code","3bcb88ab":"code","ffd509c7":"code","347fb504":"code","e46e090c":"code","94ab4d8a":"code","736037af":"code","16a27fcc":"code","9a4ea1d0":"code","3cb7dfbe":"code","7902f5f2":"markdown","c6e82a9c":"markdown","b44c711c":"markdown","1eb6872c":"markdown","59d19214":"markdown","0a7cb014":"markdown","7f2720be":"markdown","83b6ea22":"markdown","53e8bee2":"markdown","60b43994":"markdown","b3b4f8d4":"markdown","2074bcca":"markdown","c17dd527":"markdown","ad3262d7":"markdown","8e3bdc3b":"markdown","77686713":"markdown","57be55a6":"markdown","3c26d8c3":"markdown","aecdc9d0":"markdown","7b34f462":"markdown","1dd79d5c":"markdown","1fbd81a1":"markdown","a69f9aa1":"markdown","2c05cc3f":"markdown","7a85eeb7":"markdown","ffa20609":"markdown","9c1cf6e6":"markdown","2d55c0e7":"markdown","5a05363a":"markdown","ccb2d41d":"markdown","7c5d57cc":"markdown","03b594b7":"markdown","533581b5":"markdown"},"source":{"ab154fcc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%pylab inline\nfrom pandas import Series, DataFrame \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","eaa994d0":"missing_values = ['?','--','-','??','.','unknown']\nauto=pd.read_csv('\/kaggle\/input\/Automobile_data.csv',na_values=missing_values)\nauto.head(100)","f0247b65":"auto=auto[['make','fuel-type','aspiration','num-of-doors','drive-wheels',\n       'engine-location','engine-type','num-of-cylinders','fuel-system',\n       'symboling','wheel-base','length','width','height',\n       'curb-weight','engine-size','bore','stroke','compression-ratio',\n       'horsepower','peak-rpm','city-mpg','highway-mpg','price']]\nauto.dtypes","f28e695a":"auto.isnull().sum()","50f7c41a":"auto.dropna(inplace=True)","359182f7":"auto.loc[(auto['make'] == 'vw'),'make']='volkswagen'\nauto.loc[(auto['make'] == 'vokswagen'),'make']='volkswagen'\nauto.loc[(auto['make'] == 'maxda'),'make']='mazda'\nauto.loc[(auto['make'] == 'toyouta'),'make']='toyota'\nauto.loc[(auto['make'] == 'porcshce'),'make']='porsche'","ce3bf619":"plt.figure(figsize=(8,6))\nauto['make'].value_counts().plot(kind='barh')\nplt.xlabel('Number of Models')\nplt.ylabel('Company')\nplt.title(\"Company model frequency diagram\")","af2a22fc":"auto['symboling'].value_counts().plot(kind='barh')\nplt.ylabel('Risk rating')\nplt.xlabel('Number of vehicles')\nplt.title(\"Insurance risk ratings of vehicles\")","41fe25af":"plt.figure(figsize=(10,6))\nauto.groupby(['make','symboling'])['make'].count().nlargest(10).plot(kind='barh')\nplt.xlabel('Number of Vehicles')\nplt.title('Top 10 Company - Risk Rating vehicles frequency')","f16a2961":"auto.groupby(['symboling'])['price'].mean().plot(kind='barh')\nplt.xlabel('Average Price')\nplt.title('Average Pricing in various Risk Rating')","ee5f1aa5":"auto['num-of-doors'].value_counts().plot(kind='barh')\nplt.ylabel('Door Count')\nplt.xlabel('Number of vehicles')\nplt.title(\"Door count frequency of vehicles\")","efe32ace":"plt.subplot(2, 1, 1)\nauto['drive-wheels'].value_counts().plot(kind='barh')\nplt.ylabel('Wheel Driven Type')\nplt.xlabel('Number of vehicles')\nplt.title(\"Wheel Driven Type frequency of vehicles\")\n\nplt.subplot(2, 1, 2)\nauto.groupby(['drive-wheels'])['price'].mean().plot(kind='barh')\nplt.xlabel('Average Price')\nplt.title('Average Pricing in various Driving wheel types')\n\nplt.tight_layout()","49fd3a39":"plt.figure(figsize=(12,12))\nsns.heatmap(auto.corr(),annot=True)","7f2d8587":"sns.lmplot('price','engine-size', auto);","cd4d8eb3":"auto.info()","bb4064cf":"auto.head()","3bcb88ab":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ny=auto.iloc[:,-1].values\ni_value=[]\nr2_value=[]\nfor i in range(9,23,1):\n    X=auto.iloc[:,i:i+1].values  \n    \n    # Breaking X and y in Training and Test Set\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=0)\n    \n    # Training and Testing the model\n    regressor = LinearRegression()\n    regressor.fit(X_train,y_train)\n    y_pred = regressor.predict(X_test)\n    r2_value.append(r2_score(y_test,y_pred).round(4))\n    i_value.append(i)\n    \n# Plotting the r2 Score with different features\nplt.plot(i_value,r2_value,marker='o',mfc='red',mec='red',color='blue')\nplt.xlabel('Feature')\nplt.ylabel('R2_Score')\nplt.title('Feature VS R2_Score')\nplt.show()","ffd509c7":"# Importing required liberaries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Retriving useful columns in X and y\nX=auto.iloc[:,15:16].values\ny=auto.iloc[:,-1].values\n\n# Breaking X and y in Training and Test Set\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=0)\n\n# Training the model with training set\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)\n\n# Predicting the results of test set \ny_pred = regressor.predict(X_test)\n\n# Checking the R2 Score\nprint(r2_score(y_test,y_pred).round(4))","347fb504":"X=auto.iloc[:,0:-1].values\ny=auto.iloc[:,-1].values\n\nprint('Number of features before encoding = ',shape(X)[1])\n\n# Encoding the Categorical Columns using One Hot Encoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct=ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0,1,2,3,4,5,6,7,8,9])],remainder='passthrough')\nX=ct.fit_transform(X)\n\nprint('Number of features after encoding = ',shape(X)[1])","e46e090c":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nrfe = RFE(estimator = LinearRegression())  \nrfe.fit(X,y)\nX = rfe.transform(X)\nnp.shape(X)","94ab4d8a":"from sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ni_value=[]\nr2_value=[]\nfor i in range(2,np.size(X,1)+1):\n    \n    # Breaking X and y in Training and Test Set\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n    \n    # Applying PCA\n    pca = PCA(n_components = i)\n    X_train = pca.fit_transform(X_train)\n    X_test = pca.transform(X_test)\n    \n    # Training And Testing the model\n    regressor = LinearRegression()\n    regressor.fit(X_train,y_train)\n    y_pred = regressor.predict(X_test)\n    r2_value.append(r2_score(y_pred,y_test).round(4))\n    i_value.append(i)\n    \n# Plotting the r2 Score with different number of components for PCA\nplt.plot(i_value,r2_value,marker='o',mfc='red',mec='red',color='blue')\nplt.xlabel('Number of Components')\nplt.ylabel('R2_Score')\nplt.title('No_of_Components VS R2_Score')\nplt.grid(b=None)\nplt.show()","736037af":"# Breaking X and y in Training and Test Set\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n\n# Applying PCA with best n_components value\nn=r2_value.index(max(r2_value))+2\npca = PCA(n_components = n)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\nprint(n)","16a27fcc":"# Training the model\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)\n\n# Predicting the Price\ny_train_pred = regressor.predict(X_train)\ny_pred = regressor.predict(X_test)","9a4ea1d0":"from sklearn.metrics import r2_score\nprint('R2 Score of Train Set = ',r2_score(y_train,y_train_pred).round(4))\nprint('R2 Score of Test Set  = ',r2_score(y_test,y_pred).round(4))","3cb7dfbe":"import sklearn.metrics as metrics\ndef regression_results(y_true, y_pred):\n\n    # Regression metrics\n    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n    mse=metrics.mean_squared_error(y_true, y_pred) \n    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n    r2=metrics.r2_score(y_true, y_pred)\n    print('explained_variance     : ', round(explained_variance,4))    \n    print('mean_squared_log_error : ', round(mean_squared_log_error,4))\n    print('r2   : ', round(r2,4))\n    print('MAE  : ', round(mean_absolute_error,4))\n    print('MSE  : ', round(mse,4))\n    print('RMSE : ', round(np.sqrt(mse),4))\n    \nregression_results(y_test,y_pred)","7902f5f2":"### Checking Null Values","c6e82a9c":"## Importing Liberaries","b44c711c":"Here, The dimentionality reduction technique PCA(Principal Component Analysis) is used. PCA can be thought of as a projection method where data with m-columns (features) is projected into a subspace with m or fewer columns, whilst retaining the essence of the original data.\n\nFrom the graph, we are getting saturated R2 score after 24 components. ","1eb6872c":"### R2 Score","59d19214":"The graph shows that the averaging pricing of the vehicles with rating '1' or '2' are cheapest while with rating '-1' and '-2' are costlier while vehicles with rating '3' also falls in same category.  ","0a7cb014":"### Arranging Columns","7f2720be":"Retriving the company of the vehicles from the make column. Using the 'make' column as given, will not be much useful as all 205 rows have unique values. While initial name defines the company like audi, alfa-romero etc.","83b6ea22":"From this Heatmap, its visible that 'Price' column is having highest correlation with the columns 'enginesize' with 0.87 and 'curbweight' with 0.84.","53e8bee2":"# Simple Linear Regression","60b43994":"Here +3 indicates that the auto is risky while -3 shows that it is probably pretty safe. There are very few cars which are having -2 and -1 as safety standards. While most of the cars are with 0 and 1 safety rating. ","b3b4f8d4":"Here,Recurssive Feature Elimination(RFE) is used eleminate those features which are not having significant effect in prediction and 66 features are now reduced to 34 features.\n\nIn RFE first, estimator is trained on initial set of features and importance of features is obtained by 'feature_importances_' attribute. Then, least important features are pruned from current set of features.\n\nThat procedure is recursively repeated on the pruned set until the desired number of features to select is reached.","2074bcca":"### Training and Testing the model","c17dd527":"The graph shows the rating preferences of different vechiles in the market by the company. Such as most of the cars with '0' rating comes from 'Peugeot' and 'Toyota'. The Top 10 rating count of each company also shows that the companies prefer to launch vechiles with rating '1' to '-1'. ","ad3262d7":"# Multi Linear Regression","8e3bdc3b":"### Performing EDA","77686713":"### Reading Dataset","57be55a6":"The One Hot Encoding and take either Dataframe or Array as input but return only array as output, due to that the increamented columns have to be named saperately.","3c26d8c3":"### Implementation of Multi Linear Regression (Applied RFE and PCA)","aecdc9d0":"The top 3 vehicle companies are Toyota folloed by Nissan and Mazda. Then comes Mitsubishi, Honda and Volkswagen.","7b34f462":"### Implementation of Simple Linear Regression for Best feature","1dd79d5c":"### Dimentionality Reduction","1fbd81a1":"### Spliting the dataframe ininto Independent(X) and Dependent(y) variables and Encoding them","a69f9aa1":"### Best Features Selection","2c05cc3f":"## Pick the best variable for making a simple linear regression model","7a85eeb7":"After encoding, there is an increment of 46 features which is because all the categrical features are encoded and a total of 46 categorical values are present in the dataset and each categorical value is now having a seperate column with values [0,1].","ffa20609":"# Conclusion","9c1cf6e6":"By Perforing the EDA on the dataset, come to know that the 'Engine size' and 'Curb Weight' is positively correalted with price while city-mpg is negatively correlated with price and  increase in horsepower reduces the mileage and is negetively coorelated.\n\nThe Simple Linear Regression is implemented using 'Engine Size' as independent feature as Engine Size is the highest coorelated feature with 'Price' with coorelation 0.0.89 and getting R2 Score as 0.80\n\nThe R2 Score for Multi Linear Regression is '0.9396'. The Multi Linear Regression, the categorical features are first encoded followed by Feature selection Approch which is RFE(Recurssive feature Elemination). RFE is used to get important feaures as after encoding feature count was 69 which then reduced to 34.\n        \n34 are still a large amount of features for the model, So, Dimentionality Reduction Techinique PCA(Principal Component Analysis) is used in iterative manner and results are plotted, the number of components are selected by analysing the graph, which is 28 followed by training the model on these componens.\n\nAt last, conclusion is by using the Multi Linear Regression, there is a huge increment in the R2 score in training 0.9396 and testing 0.947. ","2d55c0e7":"Fixing the company names as some of the names are bit disturbed, as 'volkswagen' is entered as 'vw', 'toyota' as 'toyouta' etc. Without fixing then will result in recieving more columns while doing encoding as the model will treat it as a different company.  ","5a05363a":"Even after knowing that the 'enginesize' column is having highest correlation with 'price', will still check every numeric column ( just for demostration only)","ccb2d41d":"As we can see, the columns 15th and 16th are giving the highest R2 score, where 15th column is 'curbweight' and 16th is 'enginesize'.\n\nSo, will take 'enginesize' column for Simple Linear Regression as independent feature(x).","7c5d57cc":"The fist graph shos that companies prefer to build front wheel driven (fwd) vehicles more and are cheaper. While very few vechiles have four wheel driven (4wd) system and are bit costlier than 'fwd' vehicles.","03b594b7":"From simple linear regression, the R2 score is 0.8028 which is highest with respect to other features. ","533581b5":"### Some other metrics"}}