{"cell_type":{"266aaaa8":"code","6cc3c76d":"code","1cdebb5a":"code","4d6ddfa6":"code","79fc0814":"code","affa506b":"code","639a029b":"code","5d44ef10":"code","3589d3ec":"code","52aa03b6":"code","85ff68ba":"code","0c53381c":"code","8d4df52d":"code","3d03b76a":"code","2620cdfd":"code","b5449287":"code","a386d914":"code","9e8ea2a5":"code","329ee971":"code","9a987d19":"code","f3550e03":"code","ed16a4e9":"code","6fe426dd":"code","5ee80907":"code","69b27a43":"markdown","74f363f0":"markdown","eaa20717":"markdown","5f1276bb":"markdown","1867de61":"markdown","9d10e932":"markdown","5b26c9f8":"markdown","4ffdd8e4":"markdown","fae220cd":"markdown","d246c6df":"markdown","5e9e8350":"markdown","13571c3f":"markdown"},"source":{"266aaaa8":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nimport gresearch_crypto\nimport datetime\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_absolute_error as MSE\nfrom tqdm.notebook import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nTRAIN_CSV = '\/kaggle\/input\/g-research-crypto-forecasting\/train.csv'\nASSET_DETAILS_CSV = '\/kaggle\/input\/g-research-crypto-forecasting\/asset_details.csv'\n\nSEED = 2021\n\nREMOVE_LB_TEST_OVERLAPPING_DATA = True","6cc3c76d":"'''\n!cp ..\/input\/talibinstall\/ta-lib-0.4.0-src.tar.gzh  .\/ta-lib-0.4.0-src.tar.gz\n!tar -xzvf ta-lib-0.4.0-src.tar.gz > null\n!cd ta-lib && .\/configure --prefix=\/usr > null && make  > null && make install > null\n!cp ..\/input\/talibinstall\/TA-Lib-0.4.21.tar.gzh TA-Lib-0.4.21.tar.gz\n!pip install TA-Lib-0.4.21.tar.gz > null\n!pip install ..\/input\/talibinstall\/numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl >null\nimport talib as ta\n'''","1cdebb5a":"def fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nfix_all_seeds(SEED)","4d6ddfa6":"df_train = pd.read_csv(TRAIN_CSV)\ndf_train = df_train.dropna(subset=['Target']).reset_index(drop=True)","79fc0814":"df_test = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_test.csv')\ndf_test.head()","affa506b":"df_asset_details = pd.read_csv(ASSET_DETAILS_CSV).sort_values(\"Asset_ID\")\ndf_asset_details","639a029b":"\nl =len(df_train)\nl = l\/\/3\ndf_train = df_train[l:].reset_index(drop=True)\ndf_train\n\n","5d44ef10":"# Remove the future\nif REMOVE_LB_TEST_OVERLAPPING_DATA:\n    df_train['datetime'] = pd.to_datetime(df_train['timestamp'], unit='s')\n    df_valid = df_train[(df_train['datetime'] > '2021-06-13 00:00:00')].reset_index(drop=True)\n    df_train = df_train[(df_train['datetime'] < '2021-06-13 00:00:00')].reset_index(drop=True)\n    df_train  = df_train.drop(['datetime'],axis=1)\n    df_valid  = df_valid.drop(['datetime'],axis=1)\nelse:\n    df_train['datetime'] = pd.to_datetime(df_train['timestamp'], unit='s')\n    df_valid = df_train[(df_train['datetime'] > '2021-06-13 00:00:00')].reset_index(drop=True)\n    df_train  = df_train.drop(['datetime'],axis=1)\n    df_valid  = df_valid.drop(['datetime'],axis=1)\n","3589d3ec":"df_valid = df_valid.dropna(subset=['Target']).reset_index(drop=True)\ndf_valid","52aa03b6":"#%%capture\n#'''\ndf_train = df_train.fillna(-1)\ndf_train.VWAP = df_train.VWAP.replace(np.inf,0)\ndf_train.VWAP = df_train.VWAP.replace(-np.inf,0)\n\ndf_valid = df_valid.fillna(-1)\ndf_valid.VWAP = df_valid.VWAP.replace(np.inf,0)\ndf_valid.VWAP = df_valid.VWAP.replace(-np.inf,0)\n\ndfs = df_train.iloc[:, 2:-1].apply(lambda x: (x-x.mean())\/x.std(), axis=0)\ndf_train.iloc[:, 2:-1] = dfs \n\ndfs = df_valid.iloc[:, 2:-1].apply(lambda x: (x-x.mean())\/x.std(), axis=0)\ndf_valid.iloc[:, 2:-1] = dfs \n#'''","85ff68ba":"#asset_weight_dict =df_asset_details[['Asset_ID','Weight']].to_dict()\n#df_train['Weight'] = df_train['Asset_ID'].map(asset_weight_dict['Weight'])\/10 \u610f\u5473\u304c\u306a\u3044\n#df_valid['Weight'] = df_valid['Asset_ID'].map(asset_weight_dict['Weight'])\/10\n#df_train","0c53381c":"df_valid","8d4df52d":"train = pd.DataFrame(index=(df_train['timestamp'].unique()))\ntrain['timestamp'] = df_train['timestamp'].unique()\nfor i in range(len((df_train.Asset_ID.unique()))):\n    data = df_train[df_train.Asset_ID == i].copy()\n    for x in data.columns:\n        if x !='timestamp':\n            if x == 'Asset_ID':\n                continue \n            else:\n                \n                    train[f\"{x}_{i}\"]  = data[f\"{x}\"] \n        else:\n            data.set_index(\"timestamp\",inplace=True)  ","3d03b76a":"del df_train\ntrain","2620cdfd":"\nvalid = pd.DataFrame(index=(df_valid['timestamp'].unique()))\nvalid['timestamp'] = df_valid['timestamp'].unique()\nfor i in range(len((df_valid.Asset_ID.unique()))):\n    data = df_valid[df_valid.Asset_ID == i].copy()\n    for x in data.columns:\n        if x !='timestamp':\n            if x == 'Asset_ID':\n                continue #train[f\"{x}_{i}\"]  = data[f\"{x}\"].astype(int)\n            else:\n               \n                valid[f\"{x}_{i}\"]  = data[f\"{x}\"] \n        else:\n            data.set_index(\"timestamp\",inplace=True)  \n            \ndel df_valid","b5449287":"valid","a386d914":"#train = train.sample(n=len(train), random_state=0)\nvalid = valid.sample(n=len(valid), random_state=0)","9e8ea2a5":"test = pd.DataFrame(index=(df_test['timestamp'].unique()))\nfor i in range(14):\n    data = df_test[df_test.Asset_ID == i].copy()\n    for x in data.columns:\n        if x !='timestamp':\n            if x == 'Asset_ID':\n                continue #test[f\"{x}_{i}\"]  = data[f\"{x}\"].astype(int)\n            else:\n                if x!='group_num' and x!='row_id':\n                    test[f\"{x}_{i}\"]  = data[f\"{x}\"] \n        else:\n            data.set_index(\"timestamp\",inplace=True)  ","329ee971":"test","9a987d19":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\n#!pip install optuna\nimport optuna \nimport optuna.integration.lightgbm as lgbo","f3550e03":"def hlco_ratio(df): \n    return (df['High'] - df['Low'])\/(df['Close']-df['Open'])\ndef upper_shadow(df):\n    return\ndef lower_shadow(df):\n    return","ed16a4e9":"def get_features(df):\n    \n    times = pd.to_datetime(df.index,unit=\"s\",infer_datetime_format=True)\n    df[\"hour\"] = times.hour  \n    df[\"dayofweek\"] = times.dayofweek \n    df[\"day\"] = times.day \n    for i in range(14):\n        df[f'Upper_Shadow_{i}'] =  df[f'High_{i}'] - np.maximum(df[f'Close_{i}'], df[f'Open_{i}'])\n        df[f'hlco_ratio_{i}'] =  (df[f'High_{i}'] - df[f'Low_{i}'])\/(df[f'Close_{i}']-df[f'Open_{i}'])\n        df[f'Lower_Shadow_{i}'] =  np.minimum(df[f'Close_{i}'], df[f'Open_{i}']) - df[f'Low_{i}']\n        df[f'diff_upper{i}'] =df[f'Upper_Shadow_{i}'].diff().fillna(0)\n        df[f'diff_ratio_{i}'] =df[f'hlco_ratio_{i}'].diff().fillna(0)\n        df[f'diff_Shadow_{i}'] = df[f'Lower_Shadow_{i}'].diff().fillna(0)\n        \n        df[f'open2close_{i}'] = df[f'Close_{i}'] \/ df[f'Open_{i}']\n        df[f'high2low_{i}'] = df[f'High_{i}'] \/ df[f'Low_{i}']\n        \n        #mean_price = df[[f'Open_{i}', f'High_{i}', f'Low_{i}', f'Close_{i}']].mean(axis=1)\n        #median_price = df[[f'Open_{i}', f'High_{i}', f'Low_{i}', f'Close_{i}']].median(axis=1)\n        #df[f'high2mean_{i}'] = df[f'High_{i}'] \/ mean_price\n        #df[f'low2mean_{i}'] = df[f'Low_{i}'] \/ mean_price\n        #df[f'high2median_{i}'] = df[f'High_{i}'] \/ median_price\n        #df[f'low2median_{i}'] = df[f'Low_{i}'] \/ median_price\n        #df[f'volume2count_{i}'] = df[f'Volume_{i}'] \/ (df[f'Count_{i}'] + 1)\n    \n        ##df[f\"high_div_low_{i}\"] = df[f\"High_{i}\"] \/ df[f\"Low_{i}\"]\n        #df[f\"open_sub_close_{i}\"] = df[f\"Open_{i}\"] - df[f\"Close_{i}\"]\n    return df ","6fe426dd":"    models={}\n    train = get_features(train)\n    valid = get_features(valid)\n    #train = df_proc.fillna(-1)\n    #train = train.drop(['Asset_ID','Target'],axis=1)\n    for i in range(14): \n        X = train.copy() #[train.Asset_ID==i].reset_index(drop=True)\n        \n        X = X.dropna(subset=[f\"Target_{i}\"])\n        y = X[f\"Target_{i}\"]\n        #y= np.log1p(y)\n        \n        for x in range(14): \n            X = X.drop([f'Target_{x}'],axis=1)\n            \n        print(f'Asset_ID = {i} Training',datetime.datetime.now().strftime('%Y\/%m\/%d %H:%M:%S'))\n        #X = X.fillna(-1)\n        #X = scaler.fit_transform(X)\n        #model = LGBMRegressor()\n        model = LGBMRegressor(n_estimators=1500,num_leaves=700,learning_rate=0.1,silent=True)\n        model.fit(X, y)     #@\n        models[i] = model\n        del X\n        del y\n        \n        X = valid.copy() #[train.Asset_ID==i].reset_index(drop=True)\n        X = X.dropna(subset=[f\"Target_{i}\"])\n        y = X[f\"Target_{i}\"]\n        for x in range(14): \n            X = X.drop([f'Target_{x}'],axis=1)\n        #X = X.fillna(-1)        \n        #X = scaler.fit_transform(X)\n        x_pred = model.predict(X)\n        #print(x_pred,y.values)\n        print('Test score for LR baseline: ', f\"{np.corrcoef(x_pred, y.values)[0,1]:.5f}\")\n        del X\n        del y\n        del x_pred","5ee80907":"import pickle\nwith open('models','wb') as web:\n    pickle.dump(models,web)","69b27a43":"February 1, 2022 - Final submission deadline.","74f363f0":"### \u30c7\u30fc\u30bf\u3092\u76f4\u8fd1\u306e\u3082\u306e\u3060\u3051\u306b\u3059\u308b\n\n### Keep data only recent","eaa20717":"# Train ","5f1276bb":"### \u5168Coin\u306b\u4f55\u3089\u304b\u306e\u95a2\u4fc2\u304c\u3042\u308b\u3068\u524d\u63d0\u3092\u304a\u3044\u305fNotebook\u3002\n\n### Notebook that assumes that all Coins have something to do with it.","1867de61":"## Keep only values _before_ the LB test set","9d10e932":"![image.png](attachment:08b18d88-eef7-43a5-8734-ab2b3cdb4202.png)","5b26c9f8":"# Data Merge","4ffdd8e4":"\ud83d\ude3a\ud83d\ude05\u3299\ud83d\udd30\ud83d\uddd1\u2b1b\ud83d\udfe5\ud83d\udfe8\ud83d\udfe9","fae220cd":"## `\u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u975e\u9023\u7d9a\u5316\n\n## `Discontinuity of training data","d246c6df":"# \u884c\u5217\u306e\u6a19\u6e96\u5316\n\n# Matrix standardization","5e9e8350":"# Data Load","13571c3f":"# Function"}}