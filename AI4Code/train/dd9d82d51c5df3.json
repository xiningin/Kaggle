{"cell_type":{"87ea0be2":"code","7a4087e4":"code","c1b9ca2e":"code","b45ff906":"code","ebc15400":"code","aebe1533":"code","d5b92594":"code","7fb3a1d4":"code","adc7e354":"code","4aabd9ee":"code","382e3902":"code","66b66c35":"code","16202472":"code","23db9005":"code","a470ba8f":"code","f6c083c3":"code","27d5fe51":"code","a3f3ee81":"code","6e4616b0":"code","3dfb6fd3":"code","677adf2d":"markdown","850a2c30":"markdown","854bbac3":"markdown"},"source":{"87ea0be2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport stat\n\ndata = pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\ndata.isnull().sum()\ndata.info()\ndata.describe()\n\ndata=pd.DataFrame(data)\ndata.shape\ndupli_rows=data[data.duplicated()]\n\ndata=data.drop_duplicates()\ndata.shape\n\nmask = np.triu(np.ones_like(data.corr()))\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(data.corr(method=\"spearman\"),annot=True,linewidth=.5,cmap=\"YlOrRd\",vmin=-1,vmax=1,mask=mask,ax=ax)\nplt.show()\n\n\nfor col in data.columns:\n     sns.boxplot(x=data[col])\n     plt.show()\n\n\nfrom scipy import stats\nz = np.abs(stats.zscore(data))\ndata=data[(z<3).all(axis=1)]\ndata.shape\n\nX=data.iloc[:, :-1].values\ny=data.iloc[:, -1].values","7a4087e4":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.25,random_state=0)","c1b9ca2e":"##logistic regression classifciation\nfrom sklearn.linear_model import LogisticRegression\nclassifier=LogisticRegression(random_state=0)\nclassifier.fit(X_train,y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nlogreg0=accuracy_score(y_test, y_pred)\nlogreg0","b45ff906":"##naivebayes\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier_nb=GaussianNB()\nclassifier_nb.fit(X_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier_nb.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nnb0=accuracy_score(y_test, y_pred)\nnb0","ebc15400":"#svm\n\nfrom sklearn.svm import SVC\nclassifier_svm=SVC(kernel=\"linear\",random_state=0)\nclassifier_svm.fit(X_train,y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier_svm.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nsuppvm0=accuracy_score(y_test, y_pred)\nsuppvm0","aebe1533":"#kernel svm\n\nfrom sklearn.svm import SVC\nclassifier_ksvm=SVC(kernel=\"rbf\",random_state=0)\nclassifier_ksvm.fit(X_train,y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier_ksvm.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nker_svm0=accuracy_score(y_test, y_pred)\nker_svm0","d5b92594":"from sklearn.tree import DecisionTreeClassifier\nclassifier_dt=DecisionTreeClassifier(criterion=\"entropy\",random_state=0)\nclassifier_dt.fit(X_train,y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier_dt.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\ndt0=accuracy_score(y_test, y_pred)\ndt0","7fb3a1d4":"##random forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier_rf.fit(X_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier_rf.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nrf0=accuracy_score(y_test, y_pred)\nrf0","adc7e354":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nknn0=accuracy_score(y_test, y_pred)\nknn0","4aabd9ee":"data2 = pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\ndupli_rows=data2[data2.duplicated()]\ndata2=data2.drop_duplicates()\nz = np.abs(stats.zscore(data2))\ndata3 = data2[(z<3).all(axis=1)]\ndata3.shape","382e3902":"data3=data3.drop([\"fbs\",\"trtbps\",\"chol\",\"restecg\"],axis=1)\nX=data3.iloc[:, :-1].values\ny=data3.iloc[:, -1].values\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.25,random_state=0)","66b66c35":"##logistic regression classifciation\nfrom sklearn.linear_model import LogisticRegression\nclassifier=LogisticRegression(random_state=0)\nclassifier.fit(X_train,y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nlogreg=accuracy_score(y_test, y_pred)\nlogreg","16202472":"##naivebayes\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier_nb=GaussianNB()\nclassifier_nb.fit(X_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier_nb.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nnb=accuracy_score(y_test, y_pred)\nnb","23db9005":"#svm\n\nfrom sklearn.svm import SVC\nclassifier_svm=SVC(kernel=\"linear\",random_state=0)\nclassifier_svm.fit(X_train,y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier_svm.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nsuppvm=accuracy_score(y_test, y_pred)\nsuppvm","a470ba8f":"#kernel svm\n\nfrom sklearn.svm import SVC\nclassifier_ksvm=SVC(kernel=\"rbf\",random_state=0)\nclassifier_ksvm.fit(X_train,y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier_ksvm.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nker_svm=accuracy_score(y_test, y_pred)\nker_svm","f6c083c3":"#knn\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nknn=accuracy_score(y_test, y_pred)\nknn","27d5fe51":"##decision tree\n\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier_dt=DecisionTreeClassifier(criterion=\"entropy\",random_state=0)\nclassifier_dt.fit(X_train,y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier_dt.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\ndt=accuracy_score(y_test, y_pred)\ndt","a3f3ee81":"##random forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier_rf.fit(X_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier_rf.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nrf=accuracy_score(y_test, y_pred)\nrf","6e4616b0":"print(\"Accuracies after dropping columns are: \\n\",\"Logistic Regression Classification:\",logreg*100,\"\\n\",\"Naive-Bayes:\",nb*100,\"\\n\",\"SVM:\",suppvm*100,\"\\n\",\"Kernel SVM:\",ker_svm*100,\"\\n\",\"KNN:\",knn*100,\"\\n\",\"Decision Tree:\",dt*100,\"\\n\",\"Random Forest:\",rf*100,\"\\n\")","3dfb6fd3":"print(\"Accuracies before dropping columns are: \\n\",\"Logistic Regression Classification:\",logreg0*100,\"\\n\",\"Naive-Bayes:\",nb0*100,\"\\n\",\"SVM:\",suppvm0*100,\"\\n\",\"Kernel SVM:\",ker_svm0*100,\"\\n\",\"KNN:\",knn0*100,\"\\n\",\"Decision Tree:\",dt0*100,\"\\n\",\"Random Forest:\",rf0*100,\"\\n\")\n","677adf2d":"After loading the datset, I tackle the pre processing part by removing the duplicates first and checking the shape of the data. \nBy plotting boxplots, I can clearly see many outliers in many of the columns. I use the z score method to remove outliers. Anything whose value is above 3 is dropped and the rows come down to 287.","850a2c30":"After obtaining all the accuracies, we notice that columns fbs, trtbps,chol and restecg have very low correlation to the output.\nWe check the new accuarcies after dropping these columns to see if there is any improvement.","854bbac3":"After comapring both accuracies, we can see that kernel svm, KNN and decision tree classification models are not the best for this problem statement.\nBefore dropping the columns-\nwe see that Naive BAyes seems like a good fit with just a margin above Logistic classification. \n\nHowever, after dropping the columns,we can see that logistic regressions stands it ground, naive bayes performace slighlty decreases while random forest slightly improves its performance.\nWe can perhaps conclude that Logistic classification is the best out of all due to its consistent accuracy, followed by random forest."}}