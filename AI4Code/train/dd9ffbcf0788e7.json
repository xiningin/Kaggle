{"cell_type":{"8868b86b":"code","e5b6d624":"code","80879160":"code","c430f5f7":"code","9af7cdf8":"code","7bc8096a":"code","e87b9c45":"code","36b7c27b":"markdown","ed0e12c9":"markdown","f405826b":"markdown","e04308ba":"markdown","da97f864":"markdown"},"source":{"8868b86b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import AdaBoostRegressor as Ada\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.neural_network import MLPRegressor as MP\n","e5b6d624":"train = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\n","80879160":"train.head()\nprint(\"Train length \",len(train))\ntest.head()\nprint(\"Test length \", len(test))\ntarget = train[\"target\"]\n#Create submission file\nsub = pd.DataFrame(test[\"id\"])","c430f5f7":"A = Ada(random_state = 42)\n","9af7cdf8":"train['exerpt_len'] = train.excerpt.str.len()\n\ntest['exerpt_len'] = test.excerpt.str.len()\n","7bc8096a":"C=CV(strip_accents='unicode')\n\ntrain_dict = C.fit_transform(train.excerpt.append(test.excerpt))\nnames = C.get_feature_names()\n#Drop out all the possible conflicting column headings....\nnew_test = test.drop([\"url_legal\",\"license\",\"id\",\"excerpt\"],axis=1)\nnew_train = train.drop([\"url_legal\",\"license\",\"id\",\"excerpt\",\"target\",\"standard_error\"],axis=1)\n#...join together\ntrain_dict=pd.DataFrame(train_dict.toarray(),columns=names)\ntest_dict=pd.DataFrame(C.transform(test.excerpt).toarray(),columns=names)\n\nname_len = pd.DataFrame(names)\nname_len[\"len\"] = name_len[0].str.len()\nndw=[]\nawl=[]\ngreater_than_6 = []\nmost =[]\n\n#Looking at word complexity etc...\nfor i in range(len(train)): #Subtly different length....\n    check = train_dict.iloc[i]      #Get the line\n    num_dist_word = check[check>0]  #Anything with a count >0\n    ndw.append(len(num_dist_word))  #How long is that list\n    len_ndw = num_dist_word.index.str.len()   #How long is each word in that list\n    awl.append(sum(len_ndw) \/ len(len_ndw)) \n    junk = len_ndw>6    #test to see how many words are >6 ??\n    greater_than_6.append(len(junk.nonzero()[0]))#Count TRUE\n    stuff = len_ndw.value_counts()#Find the most popular word length\n    most.append(stuff.index[0])\ntrain['num_dist_word'] = ndw\ntrain['avg_word_len'] = awl\ntrain['g_t_6'] = greater_than_6\ntrain['most_pop_len'] = most\n#repetition...could have made a func???\nndw=[]\nawl=[]\ngreater_than_6 = []\nmost =[]\n\nfor i in range(len(test_dict)):\n    check = test_dict.iloc[i]\n    num_dist_word = check[check>0]\n    ndw.append(len(num_dist_word))\n    len_ndw = num_dist_word.index.str.len()\n    awl.append(sum(len_ndw) \/ len(len_ndw))\n    junk = len_ndw>6\n    greater_than_6.append(len(junk.nonzero()[0]))\n    stuff = len_ndw.value_counts()\n    most.append(stuff.index[0])\ntest['num_dist_word'] = ndw\ntest['avg_word_len'] = awl\ntest['g_t_6'] = greater_than_6\ntest['most_pop_len'] = most\n#new_test = new_test.join(test_dict)\n#new_train = new_train.join(train_dict)","e87b9c45":"#Drop out all the possible conflicting column headings....\nnew_test = test.drop([\"url_legal\",\"license\",\"id\",\"excerpt\"],axis=1)\nnew_train = train.drop([\"url_legal\",\"license\",\"id\",\"excerpt\",\"standard_error\"],axis=1)\n\n#Treat target separate so we can convert back easier??\ntarget = new_train.pop('target')\n#Normalisation\n# copy the data\nnew_train_scaled = new_train.copy()\n  \n# apply normalization techniques\nfor column in new_train_scaled.columns:\n    new_train_scaled[column] = (new_train_scaled[column] - new_train_scaled[column].min()) \/ (new_train_scaled[column].max() - new_train_scaled[column].min())    \n\n# copy the data\nnew_test_scaled = new_test.copy()\n  \n# apply normalization\nfor column in new_test_scaled.columns:\n    #Use the original new_train values\n    new_test_scaled[column] = (new_test_scaled[column] - new_train[column].min()) \/ (new_train[column].max() - new_train[column].min())    \n\nt_min = target.min()\nt_max = target.max()\n\ntarget_scaled = (target - t_min) \/ (t_max - t_min)    \n\n\nA = Ada(random_state = 42)\n\nA.fit(new_train,target_scaled)\n\n\"\"\"\n#Create Keras model\nlen_in = new_train_scaled.shape[1]\nlen_in_2 = (len_in*2) *2\n\nmodel = Sequential()\nmodel.add(layers.Dense(len_in_2,activation=\"relu\",input_dim=len_in))\nmodel.add(layers.Dense(len_in_2,activation=\"relu\"))\n#model.add(layers.SimpleRNN(len_in_2))\n#model.add(layers.Dense(len_in_2,activation=\"relu\"))\nmodel.add(layers.Dense(1,activation=\"sigmoid\"))\n\nmodel.compile(optimizer='Adam',loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\nmodel.fit(new_train_scaled,target_scaled,batch_size=1, epochs=10, shuffle=False)\n\"\"\"\nmodel = MP(activation='tanh',random_state = 42, hidden_layer_sizes=(100,100,25),verbose=1,solver='adam',)\nmodel.fit(new_train_scaled, target_scaled)\n\nA_pred = A.predict(new_test_scaled)\n\nNN_pred = model.predict(new_test_scaled)\n#Stack them??\nNN_pred = (A_pred + NN_pred)\/2\n\nNN_pred_rescale = (NN_pred * (t_max - t_min)) + t_min\n#**Need to rescale output to remove normalisation\n\n#sub[\"target\"] = A.predict(new_test)\nsub[\"target\"] = NN_pred_rescale\n\n\nsub.to_csv(\"submission.csv\", index=False)","36b7c27b":"Load in the train and test data....","ed0e12c9":"Brief EDA...we know that some is potentially irrelevant ie: copyright and error, as they're not included in the testing\/scoring sets.","f405826b":"Train and test....","e04308ba":"Setup a simple Ada Boost training","da97f864":"Simple Feature Engineer"}}