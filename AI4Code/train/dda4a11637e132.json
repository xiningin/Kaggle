{"cell_type":{"66558512":"code","603d97dd":"code","8d49fb7b":"code","c164e04a":"code","203aa305":"code","00f6b654":"code","26be9430":"code","165fa17c":"code","f018680a":"code","ed7234a3":"code","eec1caab":"code","5428b451":"code","eaf8e6c3":"code","2b15f99e":"code","e24c8d60":"code","dbf5da31":"code","73655b93":"code","13d152b2":"code","a9bf8e32":"code","d80ac058":"code","011e5e7b":"code","98423e66":"code","997b9501":"code","c4d0e91a":"code","4f54e587":"code","5bd31930":"code","0b29e4a3":"code","83692c92":"code","c0ccd7c4":"code","83bd543a":"code","e1263439":"code","fb021ef8":"code","135e9287":"code","60016b81":"code","db194276":"code","4ee4a630":"code","3e500c31":"code","734bd8b5":"markdown","874527b0":"markdown","7753702b":"markdown","b1a55712":"markdown","1acf90c9":"markdown","927f4ea7":"markdown","f27fa1c7":"markdown","10d9d8d8":"markdown","f8a2ff71":"markdown","a7dc8b04":"markdown","c8ecac44":"markdown","6890acd9":"markdown","5f1ba9ec":"markdown"},"source":{"66558512":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","603d97dd":"from sklearn.datasets import load_boston\ndf = load_boston()\nX = pd.DataFrame(df.data,columns=df.feature_names)","8d49fb7b":"X","c164e04a":"y = df.target","203aa305":"for i in X.columns:\n    sns.scatterplot(i,y,data = X)\n    plt.show()","00f6b654":"plt.figure(figsize = (10,7))\nsns.heatmap(X.corr(),annot = True)","26be9430":"import statsmodels.api as sm\nXc = sm.add_constant(X)\nlinreg = sm.OLS(y,Xc).fit()\nlinreg.summary()","165fa17c":"Xc.drop(['INDUS','AGE'],axis = 1,inplace = True)","f018680a":"linreg = sm.OLS(y,Xc).fit()\nlinreg.summary()","ed7234a3":"linreg.rsquared","eec1caab":"np.sqrt(linreg.mse_resid)","5428b451":"X = Xc.drop('const',axis = 1)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X,y)","eaf8e6c3":"model.score(X,y)","2b15f99e":"y = pd.DataFrame(y,columns=['medv'])","e24c8d60":"from sklearn.model_selection import KFold\nfrom sklearn import metrics\nkf = KFold(n_splits=5,shuffle=True,random_state=0)\nfor model,name in zip([model],['Linear_Regression']):\n    rmse = []\n    for train_idx,test_idx in kf.split(X,y):\n        X_train,X_test = X.iloc[train_idx,:],X.iloc[test_idx,:]\n        y_train,y_test = y.iloc[train_idx,:],y.iloc[test_idx,:]\n        model.fit(X_train,y_train)\n        y_pred = model.predict(X_test)\n        mse = metrics.mean_squared_error(y_test,y_pred)\n        print(np.sqrt(mse))\n        rmse.append(np.sqrt(mse))\n    print('RMSE scores : %0.03f (+\/- %0.05f) [%s]'%(np.mean(rmse), np.var(rmse,ddof = 1), name))","dbf5da31":"from sklearn.linear_model import Ridge,Lasso,ElasticNet\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","73655b93":"m1 = LinearRegression()\nm2 = Ridge(alpha=0.5,normalize=True) # Scaling is mandatory for all distance based calculations\nm3 = Lasso(alpha=0.1,normalize=True)\nm4 = ElasticNet(alpha=0.01,l1_ratio=0.92,normalize=True)","13d152b2":"# from sklearn.model_selection import GridSearchCV\n# params = { 'alpha' : np.arange(0.01,1,0.01) }\n# #           ,'l1_ratio' : np.arange(0.1,1,0.01)}\n# gscv = GridSearchCV(m3,params,cv = 5,scoring = 'neg_mean_squared_error')\n# gscv.fit(X,y)\n# gscv.best_params_","a9bf8e32":"model = m1.fit(X_train,y_train)\nsns.barplot(x = X.columns,y = sorted(model.coef_[0]))\nplt.title('LR coefficients')","d80ac058":"model = m2.fit(X_train,y_train)\nsns.barplot(x = X.columns,y = sorted(model.coef_[0]))\nplt.title('Ridge coefficients')","011e5e7b":"model = m3.fit(X_train,y_train)\nsns.barplot(x = X.columns,y = sorted(model.coef_))\nplt.title('LASSO coefficients')","98423e66":"model = m4.fit(X_train,y_train)\nsns.barplot(x = X.columns,y = sorted(model.coef_))\nplt.title('ElasticNet coefficients')","997b9501":"from sklearn.model_selection import KFold\nfrom sklearn import metrics\nkf = KFold(n_splits=5,shuffle=True,random_state=0)\nfor model,name in zip([m1,m2,m3,m4],['Linear_Regression','Ridge','LASSO','ElasticNet']):\n    rmse = []\n    for train_idx,test_idx in kf.split(X,y):\n        X_train,X_test = X.iloc[train_idx,:],X.iloc[test_idx,:]\n        y_train,y_test = y.iloc[train_idx,:],y.iloc[test_idx,:]\n        model.fit(X_train,y_train)\n        y_pred = model.predict(X_test)\n        mse = metrics.mean_squared_error(y_test,y_pred)\n        rmse.append(np.sqrt(mse))\n    print('RMSE scores : %0.03f (+\/- %0.05f) [%s]'%(np.mean(rmse), np.var(rmse,ddof = 1), name))\n    print()","c4d0e91a":"print('Bias error increased after Lasso : ',(5.875-4.829)\/5.875 * 100,\"%\")","4f54e587":"print('Variance error decreased after Lasso : ',(0.53924 - 0.41470)\/0.53924 * 100,\"%\")","5bd31930":"mpg_df = pd.read_csv('..\/input\/auto-mpg-pratik.csv')","0b29e4a3":"mpg_df.head()","83692c92":"sns.pairplot(mpg_df)","c0ccd7c4":"X_update = mpg_df.drop(['mpg', 'cylinders', 'displacement', 'horsepower',\n       'acceleration', 'car name'],axis = 1)\ny = mpg_df['mpg']","83bd543a":"from sklearn.preprocessing import PolynomialFeatures\nqr = PolynomialFeatures(degree=2)\nx_qr = qr.fit_transform(X_update[['weight']])\nx_qr = x_qr[:,2:]\nx_qr_df = pd.DataFrame(x_qr,columns=['weight_square'])","e1263439":"df_final = pd.concat([X_update,x_qr_df,y],axis = 1)\ndf_final","fb021ef8":"import statsmodels.api as sm\nX = df_final.drop('mpg',axis = 1)\ny = df_final['mpg']\nXc = sm.add_constant(X)\nlr = sm.OLS(y,Xc).fit()\nlr.summary()","135e9287":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nm2 = Ridge(alpha=0.06,normalize=True) # Scaling is mandatory for all distance based calculations\nm3 = Lasso(alpha=0.37,normalize=True)\nm4 = ElasticNet(alpha=0.01,l1_ratio=0.1,normalize=True)","60016b81":"y = pd.DataFrame(y,columns=['mpg'])","db194276":"from sklearn.model_selection import KFold\nfrom sklearn import metrics\nkf = KFold(n_splits=5,shuffle=True,random_state=0)\nfor model,name in zip([model,m2,m3,m4],['Quadratic_Regression','Ridge','Lasso','ElasticNet']):\n    rmse = []\n    for train_idx,test_idx in kf.split(X,y):\n        X_train,X_test = X.iloc[train_idx,:],X.iloc[test_idx,:]\n        y_train,y_test = y.iloc[train_idx,:],y.iloc[test_idx,:]\n        model.fit(X_train,y_train)\n        y_pred = model.predict(X_test)\n        mse = metrics.mean_squared_error(y_test,y_pred)\n        rmse.append(np.sqrt(mse))\n    print('RMSE scores : %0.03f (+\/- %0.05f) [%s]'%(np.mean(rmse), np.std(rmse,ddof = 1), name))\n    print()","4ee4a630":"print('Bias error increased after Ridge : ',(3.409-3.021)\/3.409 * 100,\"%\")","3e500c31":"print('Variance error decreased after Ridge : ',(0.36898 - 0.31105)\/0.36898 * 100,\"%\")","734bd8b5":"# ElasticNet Regression\nCost function Lasso = min( MSE + a * { sum_i=1_to_p(abs(beta_i))} + b * {sum_i=1_to_p(beta_i ** 2)} )\n\nl1_ratio = a\/(a+b)\n\np : Number of features\n\nIf, l1_ratio = 1 ->Lasso\n\nIf, l1_ratio = 0 ->Ridge","874527b0":"So consider after regularization,\n\nBias error : 4.958\n\nVariance error : 0.4236","7753702b":"So here we need to sacrifice just 2.6% of bias error to get a benifit of 21.44% variance error. So we would choose the 2nd model to be better than the 1st one.","b1a55712":"Bias error : 4.829\n\nVariance error = 0.53924","1acf90c9":"# Boston Dataset","927f4ea7":"***Please UpVote if you like the work!!!***","f27fa1c7":"1. Percentage of bias error the 1st model is better than the 2nd model : ((4.958 - 4.829)\/4.958)*100 = 2.60%\n2. Percentage of variance error the 2nd model is better than the 1st model : ((0.53924 - 0.4236)\/0.53924)*100 = 21.44%","10d9d8d8":"# Lasso Regression (L1)\nCost function Lasso = min( MSE + alpha * { sum_i=1_to_p(abs(beta_i)) } )\n\np : Number of features \n\nalpha : range(0-1) - typically we play between 0 to 0.5\n\nExample : \n\nBefore Lasso regularization :\n\ny = 0.836 * x1 + 0.438 * x2 - 0.386 * x3 + 0.158 * x4\n\nAfter Lasso regularization :\n\ny = 0.81 * x1 + 0.399 * x2 - 0.170 * x3 + 0 * x4\n\nIf we overdo lasso, it will underfit as most of the features will be eliminated. So the value of alpha should not be high.","f8a2ff71":"# Polynomial Linear Regression","a7dc8b04":"***Please UpVote if you like the work!!!***","c8ecac44":"##### Objective of REGULARIZATION : To avoid the model to overfit, i.e, to control\/reduce the variance error at the cost of bias error. So we need to decide whether the bias error sacrifice is worth or not.","6890acd9":"##### Giving a range estimate rather than giving a point estimate is always a more believable strategy. This can be achieved by using k-fold cross validation.","5f1ba9ec":"# Ridge Regression (L2)\nCost function Ridge = min( MSE + alpha * {sum_i=1_to_p(beta_i ** 2)} )\n\np : Number of features \n\nalpha : range(0-1) - typically we play between 0 to 0.5\n\nRidge will add higher penalty to those features which contribute less to predict the dependent variable. It will scale down the coefficients of those weaker features which is creating a larger residues (features with low coefficients) comparable to those features creating a lower residues (features with high coefficients). Those features which are highly correlated to dependent variables will create less residues, whereas those features which are weakly correlated to dependent variables will create higher residues. So the Ridge will scale down the magnitude in such a way that highly correlated features, it will scale down less and the least correlated features it will scale down more.\n\nExample : \n\nBefore Ridge regularization :\n\ny = 0.836 * x1 + 0.438 * x2 - 0.386 * x3 + 0.158 * x4\n\nAfter Ridge regularization :\n\ny = 0.81 * x1 + 0.399 * x2 - 0.170 * x3 + 0.023 * x4"}}