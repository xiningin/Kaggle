{"cell_type":{"88eb12af":"code","c9167c37":"code","f1930721":"code","b142c981":"code","b8f491c6":"code","7cdde47c":"code","4cdce0fc":"code","3082d05f":"code","ec2585ed":"code","baf3593f":"code","27c78a32":"code","7d7d936e":"code","1ff59565":"code","e9a9fc14":"code","4001cd94":"code","a78b3cd9":"code","9841a075":"code","7bab5dec":"code","7450c5fd":"code","1be727dc":"code","25b74c54":"code","c1413717":"markdown","e80259e7":"markdown","81bd3e4a":"markdown","c9a1f771":"markdown","e0d67cd8":"markdown","09637242":"markdown","bc31bf1d":"markdown","3581f99a":"markdown","fe6f7436":"markdown","653cbc7e":"markdown","be419c01":"markdown","203a2c1c":"markdown","455ff05d":"markdown"},"source":{"88eb12af":"# --- PACKAGE IMPORT\nimport os                                      \nimport re\nimport sys\nimport math\nimport codecs\nimport itertools\n\nimport numpy as np                             # numpy      - working with numbers & math things\nimport pandas as pd                            # pandas     - excel but way cooler\nimport seaborn as sns\n\nfrom tqdm import tqdm                          # tqdm       - for fancy progress bars\nfrom tabulate import tabulate                  # tabulate   - for fancy printing dataframes\nimport matplotlib.pyplot as plt                # matplotlib - for fancy vis\nfrom IPython.core.display import display, HTML # Ipython    - for fancy notebook sugar\n\n# --- CONFIGURATION\n\n# BASE PATH WHEN USING A KAGGLE KERNEL\nkaggle_slug ='..\/input\/kali-2020-usrsharewordlists-directory\/KALI2020_usr_share_wordlists'\n\n# BASE PATH WHEN USING KALI 2020\nkali_path = '\/usr\/share\/wordlists\/'\n\nbase = kaggle_slug + kali_path # CHANGE THIS IF RUNNING LOCALLY\n\n# FOR MAKING DEBUGGING FASTER\ninclude_rockyou = True\nrockyou_100000s_of_rows = 100\n\n# CASCADING STYLES\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\")","c9167c37":"def count_raw_lines(fname):\n    # tries to get an idea of how many lines are each file.\n    try:\n        i = 0\n        with open(fname) as f:\n            for i, l in enumerate(f):\n                pass\n        return i + 1\n    except:\n        try:\n            if str(sys.exc_info()[0]) == \"<class 'UnicodeDecodeError'>\":\n                i = 0\n                with codecs.open(fname, encoding='latin-1') as f:\n                    for i, l in enumerate(f):\n                        pass\n                return i + 1\n            else:\n                exit(0)\n        except:\n            print(sys.exc_info()[0])","f1930721":"def grab_valid_lines(fname):\n    # \n    valid_lines = []\n    try:\n        i = 0\n        with open(fname) as f:\n            for i, l in enumerate(f):\n                if l[0] is not '#': # TODO: check for multiline comments\n                    valid_lines.append(l.replace('\\n', ''))\n                pass\n    except:\n        try:\n            if str(sys.exc_info()[0]) == \"<class 'UnicodeDecodeError'>\":\n                i = 0\n                with codecs.open(fname, encoding='latin-1') as f:\n                    for i, l in enumerate(f):\n                        if l[0] is not '#': # TODO: check for multiline comments\n                            valid_lines.append(l.replace('\\n', ''))\n                        pass\n            else:\n                exit(0)\n        except:\n            print(sys.exc_info()[0])\n    try:\n        valid_lines.remove('')\n    except:\n        pass\n    return valid_lines","b142c981":"keeb_layout = [\n    { 'row': 0, 'lo' : '`1234567890-= ',  'up' : '~!@#$%^&*()_+ ' },   # ** NOTES ** \n    { 'row': 1, 'lo' : ' qwertyuiop[]\\\\', 'up' : ' QWERTYUIOP{}|' },   # Escaped \\ and '\n    { 'row': 2, 'lo' : ' asdfghjkl;\\' ',  'up' : ' ASDFGHJKL:\" '  },   # Using a ' ' for padding where the tab, caps, shift, delete & return keys are.\n    { 'row': 3, 'lo' : ' zxcvbnm,.\/ ',    'up' : ' ZXCVBNM<>? '   }    # Typically the left hand handles all 6-keys from the left, the right handles the rest.\n]\n\n# TODO: find a way to visualize keyboard keypress sequences with centroids of keys and all that, rather than this ugly matrix-type method","b8f491c6":"def sequence_analysis(string):\n    # Password Keystroke Sequence Analysis \n    \n    seq = [ ]\n    shift_presses = 0\n    #     shift_key = { 'char': 'shift', 'shift' : True, 'pos' : '', 'hand': 'either', 'row' : 3, 'finger': 4 }\n    \n    for i in range(0, len(string)):\n        char = string[i]\n#         print(\" *** --> \" + char + \" <-- ***\")\n        payload = {\n            'char'  : char,  # The character\n            'shift' : False, # True\/False is SHIFT is being held?\n            'pos'   : 0,     # How many keys from the left is the key?\n            'hand'  : 'R',   # Which hand is doing the typing? \n            'row'   : 0,     # Which row of the qwerty keyboard?\n            'finger': 0,     # Which of the 8 fingers is responsible for that key?\n            'bear'  : 0      # Bearing of latest action in Radians.\n            # TODO: Add 4 Finger Usage (no spacebar, no thumbs, no?)\n        }\n        for r in range(0, len(keeb_layout)):\n            row = keeb_layout[r]\n            if char in row['lo']:\n                payload['up'] = False\n                payload['shift'] = False\n                payload['pos'] = row['lo'].find(char)\n                payload['row'] = r\n                if payload['pos'] <= 5: payload['hand'] = 'L'\n#                 print(\"lowercase row_\" + str(r) + \" pos_\" + str(payload['pos']) + \" \" + payload['hand'])\n            if char in row['up']:\n                payload['up'] = True\n                payload['pos'] = row['up'].find(char)\n                payload['row'] = r\n                if payload['pos'] <= 5: payload['hand'] = 'L'\n                if len(seq) == 0:\n                    payload['shift'] = True\n                    shift_presses += 1\n                else:\n                    if seq[len(seq)-1]['shift'] == True:\n                        payload['shift'] = True\n                    else:\n                        shift_presses += 1\n                        payload['shift'] = True\n#                 print(\"uppercase row_\", str(r) + \" pos_\" + str(payload['pos'])  + \" \" + payload['hand'])\n\n        seq.append(payload)\n        \n    step_count = len(seq) + shift_presses\n\n    total_manhattan_distance = 0  # TODO: Change these back to lists\n    total_xy_distance = 0         # TODO: Change these back to lists\n    total_hand_switches = 0   \n    current_hand = ''\n    bears = [ ]\n    \n    for b in range(0, len(seq)):\n        \n        step = seq[b]\n        x_1 = step['pos']\n        y_1 = step['row']\n        \n        if b == 0:\n            current_hand = step['hand']\n        if b > 0:\n            x_2 = seq[b-1]['pos']\n            y_2 = seq[b-1]['row']\n\n            lat1 = math.radians(x_1)\n            lat2 = math.radians(x_2)\n            diffLong = math.radians(y_2 - y_1)\n            x = math.sin(diffLong) * math.cos(lat2)\n            y = math.cos(lat1) * math.sin(lat2) - (math.sin(lat1) * math.cos(lat2) * math.cos(diffLong))\n            bears.append(round(math.atan2(x, y)))\n            \n            total_manhattan_distance += (abs( x_2 - x_1 ) + abs( y_2 - y_1 ))\n            total_xy_distance += math.sqrt((x_2 - x_1)**2 + (y_2 - y_1)**2)  \n            \n            if step['hand'] != current_hand:\n                current_hand = step['hand']\n                total_hand_switches += 1\n    \n    try: \n        your_average_bear = round(sum(bears) \/ len(bears) + 1, 2)\n    except:\n        your_average_bear = 0\n                \n    return {\n        'switch' : total_hand_switches,         # of times the next key uses your other hand.\n        'crow'   : round(total_xy_distance, 2), # Distance Travelled, as the crow flies.\n        'man'    : total_manhattan_distance,    # The Manhattan distance.\n        'steps'  : step_count,                  # of steps in the sequence, including pressing shift.\n        'bear'   : your_average_bear            # Average Compas Bearing In Radians Boo Boo.\n    }\n\n# print(sequence_analysis('Spring2017'))","7cdde47c":"def further_analysis(string):\n#     print(string)\n    \n    return {\n        'dated'    : False,  # Is there a date?\n        'repeats'  : False,  # Are there repeating characters?\n        'eng_word' : False   # Is there an English word?   \n    }\n# print(further_analysis('Spring2017'))","4cdce0fc":"df_meta = pd.read_csv('..\/input\/kali-2020-usrsharewordlists-directory\/wordlists_metadata.csv')\n\n# ********** METADATA NOTES *************\n# ========== 'format' COLUMN ============\n# * XXX means PASSWORD\n# * YYY means USERNAME\n# * EXT means EXTENSION\n# * SUB means SUBDOMAIN\n# * URL means URL\n# * PSN means POISONOUS\n# * DIR means DIRECTORY\n# * PATH means PATH\n# * NAME means NAMES\n# * WORD means WORD\n# * FILE means FILENAME\n# * OTHER means TBD\/WIP\n\n# ========== 'order' COLUMN ============\n# * SENS means ORDERED BY SENSITIVITY\n# * FREQ means ORDERED BY FREQUENCY\n# * ALPHA means ORDERED APHABETICALLY\n\ndisplay(HTML('<center>' + tabulate(df_meta.head(5), headers='keys', tablefmt='html') + '<\/center>'))","3082d05f":"data = []\ncols = [ \n    'file_name', \n    'path_rel', \n    'path_abs',\n    'path_kali',\n    'file_size', \n    'raw_line_count', \n    'extension', \n    'description',\n    'separator',\n    'list_type',\n    'list_order',\n    'row_format'\n]\n\nexcluded_files = [ 'rockyou.txt' ]\nif include_rockyou: excluded_files = [ ]\n\nfor dirname, _, filenames in os.walk(base):\n    filenames = [f for f in filenames]\n    for filename in filenames:\n#         print(\"=== Processing: \" + filename)\n\n        if filename not in excluded_files:\n        \n            ### FILE ENUMERATION \n            path_abs = (dirname + '\/' + filename).replace(\"\/\/\",\"\/\")\n            path_kali = (dirname + '\/' + filename).replace(kaggle_slug,\"\").replace(\"\/\/\",\"\/\")\n            file_size = os.path.getsize(path_abs)\n            extension = filename.split('.')[len(filename.split('.')) - 1]\n            raw_line_count = count_raw_lines(path_abs)\n            kali_abs_path = path_abs.replace(kaggle_slug, '')\n\n            ### PULL METADATA\n            meta_row = df_meta[df_meta['kali_abs_path'] == kali_abs_path ].squeeze()\n\n            ### APPEND SMALL LIST TO BIG LIST\n            data.append([\n                filename,\n                dirname.replace(base,'\/'),\n                path_abs,\n                path_kali,\n                file_size,\n                raw_line_count,\n                extension,\n                meta_row['desc'],       # description\n                meta_row['separator'],  # separator\n                meta_row['type'],       # list_type\n                meta_row['order'],      # list_order\n                meta_row['format']      # row_format\n            ])\n\ndf_wordlists = pd.DataFrame(data, columns = cols)\n\nprint(\"=-   Total Files In \/wordlists\/:  \" + str(len(df_wordlists) - 1))","ec2585ed":"display(HTML(tabulate(df_wordlists.head(5), headers='keys', tablefmt='html')))","baf3593f":"df_wordlists['raw_line_count'] = pd.to_numeric(df_wordlists['raw_line_count'].fillna(0), downcast='integer')\n\nfirst_filter = cols  # TMP, no column filtering yet\n\ndf_wordlists_filtered = df_wordlists.filter(items=first_filter)\n\nprint(\"=-   Total Files In \/wordlists\/:  \" + str(len(df_wordlists_filtered)) + \"   (from reading all files in the \/wordlists\/ directory)\")\n\nduplicate_filenames = df_wordlists['file_name'].value_counts().loc[ lambda x : x > 1 ].index.tolist()\n\nduplicate_file_indices = []\n\nfor duplicate_filename in duplicate_filenames:\n    df_duplicates = df_wordlists_filtered[ df_wordlists_filtered['file_name'] == duplicate_filename ]\n    unique_filesizes = df_duplicates.drop_duplicates('file_size', keep='first')\n    if len(unique_filesizes) == 1:\n        duplicate_file_indices.append(unique_filesizes.last_valid_index())\n\nprint(\"=-   Duplicate Files Found:       \" + str(len(duplicate_file_indices)) + \"    (same filename & number of bytes, different directory)\")\n   \ndf_deduped = df_wordlists_filtered.drop(df_wordlists_filtered.index[duplicate_file_indices])\n\nprint(\"=-   Unique Files In \/wordlists\/: \" + str(len(df_deduped)))\n\ndf_clean = df_deduped\n\ndisplay(HTML(tabulate(df_clean.head(5), headers='keys', tablefmt='html')))","27c78a32":"# ISOLATE ALL FILES CONTAINING PASSWORDS\npass_file_types = [ \n    'pass_common', \n#     'pass_default', \n    'cred_common'\n#     'cred_default' \n]\n\ndf_pass_files = df_clean[df_clean['list_type'].isin(pass_file_types)]\n\nprint(\"=-  # of Files Containing Passwords:    \" + str(len(df_pass_files)))\n\n# FILTER OUT FORMATS I DON'T WANT YET\ndf_pass_files_formatted = df_pass_files[df_pass_files['row_format'].isin([ 'XXX', 'YYY XXX' ])]\n\n# FILTER OUT FORMATS I DON'T WANT YET\ndf_pass_files_sorted = df_pass_files_formatted[df_pass_files_formatted['list_order'].isin([ 'freq' ])]\n\nprint(\"=-  # of Pass Files Sorted by Freq:     \" + str(len(df_pass_files_sorted)))\n\ndisplay(HTML(tabulate(df_pass_files_sorted.head(5), headers='keys', tablefmt='html')))","7d7d936e":"# EXTRACT UNIQUE LIST OF ABSOLUTE PATHS\npass_file_list = df_pass_files_sorted['path_abs'].unique()\n\n# BEGIN THE LIST OF BAD PASSWORDS WITH THE WORST PASSWORD IN THE GAME\nraw_passlist = [ { 'p': 'password', 'i' : 1 } ]\n#            'p' is for password, 'i' is for rank\n\n# LOOP THROUGH EACH FILE AND EXTRACT EACH PASSWORD\nfor path in pass_file_list:\n    file = path.split(\"\/\")[len(path.split(\"\/\"))-1]\n#     print(\"===- PROCESSING: \" + file)\n    \n    file_passlist = []\n    lines = grab_valid_lines(path)\n    row_format = str(df_pass_files.iloc[list(pass_file_list).index(path)]['row_format']).replace(\"\\n\", \"\")\n    list_order = str(df_pass_files.iloc[list(pass_file_list).index(path)]['list_order']).replace(\"\\n\", \"\")\n    target = 'XXX'\n\n    for i in range(0, len(lines)):\n\n        line = lines[i]\n        payload = { 'p': 'password', 'i' : 0 }\n        if row_format == 'YYY XXX':\n            try:\n                payload['pass'] = line.split(\" \")[1]   # try-catch here because 'no password' is an option sometimes\n            except:\n                pass\n        if row_format == 'XXX': \n            payload['p'] = line\n        \n        if i + 1 < rockyou_100000s_of_rows * 100000:\n            payload['i'] = i + 1\n\n            file_passlist.append(payload)\n\n    raw_passlist.extend(file_passlist)\n\nprint(\"=-  # of Raw Passwords Extracted:       \" + f\"{len(raw_passlist):,}\" )\n\nseen_passes = set()\ndeduped_passlist = []\nfor obj in raw_passlist:\n    if obj['p'] not in seen_passes:\n        deduped_passlist.append(obj)\n        seen_passes.add(obj['p'])\n\nprint(\"=-  # of Duplicate Passwords Removed:   -\" + f\"{len(raw_passlist) - len(deduped_passlist):,}\" )\n\nprint(\"=-  # of Unique Passwords Extracted:    \" + f\"{len(deduped_passlist):,}\" )","1ff59565":"#### Look at me.\n\ndel seen_passes\ndel raw_passlist\ndel file_passlist\ndel duplicate_filenames\ndel duplicate_file_indices\n\ndel df_deduped\ndel df_wordlists\ndel df_duplicates\ndel df_pass_files\ndel df_pass_files_sorted\ndel df_wordlists_filtered\ndel df_pass_files_formatted\n\nanalysis_passlist = deduped_passlist\ndel deduped_passlist\n\n#### I am the garbage collector now.","e9a9fc14":"engineered_passlist = [  ]\n\nfor i in tqdm(range(0, len(analysis_passlist))):\n    pw_obj = analysis_passlist[i]\n    pw = pw_obj['p']\n\n    # BASIC STRING ANALYSIS\n    pw_char_count = len(pw)\n    pw_num_count = len(re.sub(\"[^0-9]\", \"\", pw))\n    pw_cap_count = len(re.sub(\"[^A-Z]\", \"\", pw))\n    pw_low_count = len(re.sub(\"[^a-z]\", \"\", pw))\n    pw_sym_count = pw_char_count - ( pw_num_count + pw_cap_count + pw_low_count )\n    \n    # KEYSTROKE SEQUENCE ANALYSIS\n    key_seq_ana = sequence_analysis(pw)\n    \n    # RANKNING (pertinant to frequency-ordered password lists, which save time)\n#     pw_rank = pw_obj['i']\n\n    # JOIN THE OTHERS\n    engineered_passlist.append({\n        'pw'         : pw,\n        'char_cnt'   : pw_char_count,\n        'num_cnt'    : pw_num_count,\n        'cap_cnt'    : pw_cap_count,\n        'low_cnt'    : pw_low_count,\n        'sym_cnt'    : pw_sym_count,\n        'switches'   : key_seq_ana['switch'],\n        'xy_dist'    : key_seq_ana['crow'],\n        'man_dist'   : key_seq_ana['man'],\n        'steps'      : key_seq_ana['steps'],\n        'bear'       : key_seq_ana['bear'],\n        'rank'       : pw_obj['i']\n    })\n    \ndel analysis_passlist # 'You have run out of memory' \/\/ Is this big data?","4001cd94":"df_passwords = pd.DataFrame(engineered_passlist)\n\ndel engineered_passlist # The ol' switcheroo","a78b3cd9":"df_passwords = df_passwords[df_passwords.index < int( len(df_passwords) \/ 2 )]  \n\nprint(\"=- Working with: \" + str(len(df_passwords)) + \" Unique Passwords\")\n\n# display(HTML(tabulate(df_passwords.sort_values(by=['rank'],ascending=False).head(5), headers='keys', tablefmt='html')))\ndisplay(HTML('<center>' + tabulate(df_passwords.head(5), headers='keys', tablefmt='html')+'<center>'))","9841a075":"plt.figure(figsize=(16,13))\n\nplt.rcParams.update({ \"figure.facecolor\": (0, 0, 0, 0), \"axes.facecolor\": (0, 0, 0, 0) })\n\nsns.heatmap(\n    df_passwords.corr(), \n    annot = True, \n    center = 0,  \n    cmap = \"Spectral\", \n    mask = np.triu(np.ones_like(df_passwords.corr(), dtype=bool))\n).set_title('A corrplot using the features engineered above.')\n\nplt.show()","7bab5dec":"for variable in df_passwords.columns:\n    \n    if variable not in [ 'pw', 'rank' ] :\n        \n        plt.figure(figsize=(16,4))\n        \n        plt.hist(\n            df_passwords[variable], \n            color = 'blue', \n            edgecolor = 'black', \n            bins = 500\n        )\n\n        sns.distplot(\n            df_passwords[variable], \n            hist=True, \n            kde=False, \n            bins=69, \n            color = 'blue',\n            hist_kws={'edgecolor':'black'}\n        )\n\n        plt.title('Frequency Inspection For: ' + variable)\n        plt.ylabel('Frequency')\n        plt.xlabel(variable)\n\n        plt.show()","7450c5fd":"# for index, row in df_meta.iterrows():\n#     display(HTML('<div style=\"border:2px solid black;height:275px;border-radius:4px;padding:6px;margin:5px;\">' +\n#                       '<h1>' + row['file_name'] + '<\/h1>' +\n#                       '<p>TYPE: ' + row['type'] + '<\/p>' +\n#                       '<a href=\"' + row['file_name'] + '\">INFO<\/a>' + \n#                       '<p>SOURCE: ' + row['src'] + '<\/p>' + \n#                       '<p>' + row['desc'] + '<\/p>' + \n#                  '<\/div>'\n#                 ))","1be727dc":"# this is where I'll take all the stuff and put it into organized buckets.\n\n# * Maybe something like this? : \n# ```\n# \/ettu\/cred\/                   # CREDENTIAL COMBOS (USER&nbsp;PASS\\n)\n#         \/default\/ \n#             all.txt \n#             \/oracle\/\n#             \/apache\/\n#             \/google\/\n#             ...\n# \/ettu\/pass\/                   # PASSWORDS (pass\\n)\n#         \/admin\/ --> sml.txt, med.txt, lrg.txt\n#         \/users\/ --> sml.txt, med.txt, lrg.txt\n# \/ettu\/dir\/  # DIRECTORY NAMES\n#         \/www\/\n#         \/cms\/\n# \/ettu\/psn\/  # POISON (injection\/attack strings)\n#         \/inj\/\n#             \/sql\/\n#             \/web\/\n# ```\n\n# * Output an intuitive file-structure for a reorganized \/wordlists\/ directory for the next time I do the OSCP\n# * Maybe saved to \/opt\/ or \/etc\/ or just \/tmp\/...?\n# * Add all new TLDs to a list\n# * Add other alphabets","25b74c54":"# \u00af\\_(\u30c4)_\/\u00af","c1413717":"### KEYBOARD DISTANCE FEATURE\n\nSo yesterday I wrote this","e80259e7":"# [3] Filesystem Data Rectangularization ","81bd3e4a":"# [5] Frequency-Ranked Password Preprocessing","c9a1f771":"# Kali 2020 \/Wordlist\/ Cleanup Project\n\n** I GAVE THIS A GOOD 3-4 DAYS, GIVING IT A BREAK FOR `<UNDEFINED AMOUNT OF TIME>` **\n\nI just took the [PWK](https:\/\/www.offensive-security.com\/pwk-oscp\/), got 45 pts on the OSCP, all in the first 3hrs. I found that the code I had written myself to solve a problem like the BoF machine was the most reliable way to avoid wasting time, and _tempus_ definitely _fugit-ed_ for a good 6hrs fumbling with inefficient bruteforce. I think this notebook is me trying harder.\n\nI sort of see an opportunity to update the way the wordlists in Kali 2020 are organized and maintained. So I'm going to give it a shot in this notebook and maybe later turn it into a quicker script on [Github](https:\/\/github.com\/u\/EricCasey) any user could run to keep everything organized and updated.\n\n## Purpose\n\n_vis-\u00e0-vis_ ANALYSIS : Do Feature Engineering & Exploratory Data Analysis.  \n_vis-\u00e0-vis_ UPDATES  : A script that recompiles and updates the default wordlists in Kali Linux 2020.  \n_vis-\u00e0-vis_ OUTPUT   : To output a folder full of wordlists with a filestructure suitable for heuristics.  \n\n## Findings So Far...\n* There are 15 duplicate files. (same name & size, different directories)\n* There are 3 different files named 'common.txt' and they're all different.\n* \/usr\/share\/wordlists\/metasploit\/vnc_passwords.txt is one line; just 'password' _gee thanks_\n* There are a couple dialects of Spanish files but no French, Italian, what about all the other languages?\n* test_ext.txt is just all permutations of 3-letter TLDs, most wouldn't even be valid right?\n\n## Future Goals\n* _vis-a-vis_ ANALYSIS : EDA on Frequency-Ranked Password Lists\n* _vis-a-vis_ ANALYSIS : Investigate rockyou overlap\n* _vis-a-vis_ ANALYSIS : Time-series (based on leak_year)\n* _vis-a-vis_ ANALYSIS : Can I merge this with wikipedia.\n* _vis-a-vis_ UPDATES  : Generate alternate-language password lists\n* _vis-a-vis_ FUN      : Interactive haveibeenpwned-style block\n\n## Table of Contents\n* [0] Imports & Config  \n* [1] Feature Extraction Functions  \n    * Filesystem Reading  \n    * Sequence Analysis  \n    * NLP Analysis  \n* [2] The Metadata  \n* [3] Filesystem Data Rectangularization  \n* [4] Filesystem Analysis & Preprocessing  \n* [5] Frequency-Ranked Password Preprocessing  \n* [6] Frequency-Ranked Password Analysis  \n* [7] Metadata Review \/ Reference  \n* [8] Cleanup & Output  ","e0d67cd8":"# [1] FEATURE EXTRACTION FUNCTIONS","09637242":"## Filesystem Re-Org & Output","bc31bf1d":"<center><img src=\"https:\/\/i.imgur.com\/yLmJ6iq.png\"\/><\/center>","3581f99a":"# [6] Frequency-Ranked Password Analysis","fe6f7436":"# [2] The Metadata","653cbc7e":"# [7] APPENDICES\n\n## Metadata Reference","be419c01":"# [0] IMPORTS & CONFIG","203a2c1c":"# [4] Filesystem Analysis & Preprocessing","455ff05d":"What Did I Learn\n\n* "}}