{"cell_type":{"8f073588":"code","86dc502a":"code","b4b9ecf7":"code","62ebcfa1":"code","92c16014":"code","785acb1d":"code","739ebe8e":"code","9a9fdea2":"code","0d1eae58":"code","cb4a9cd9":"code","9312e890":"code","e463dae3":"code","cfac04f1":"code","7ebba146":"code","d0d7ab12":"code","63d8b788":"code","e1890ea7":"code","a9a55783":"code","07948e6b":"code","27575fd8":"markdown","7d539830":"markdown","396c0457":"markdown","5c3636fd":"markdown","20973f06":"markdown"},"source":{"8f073588":"# 1) Using RandomForest model as introduced by the Tute\n# 2) Estimated the missing Age and hence implemented them for tranning\n# 3) Some interesting inference:\n#    Sex=female->survived-Up\/Higher Pclass ->survived-Up\n#    Rev are all male, with 0 survival rate (they serve the Lord), although only 7 Rev on the ship.\n#    Male Dr. can have higher survival rate (they are welcomed by lifeboat\/they know to to survice) \n#    Master are all male and they have higher survival rate than usual male\n#    Married woman (Mrs) has survival rate 10% higher than sigle lady (Miss)","86dc502a":"#P1 Database liabaries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n#P2 Machine learning liabaries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold,cross_val_score,cross_val_predict,GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n#P3 Load the Data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","b4b9ecf7":"# 2.1 Select the label and features in train_data for further analysis\ncol_take=['Survived','Pclass', 'Sex', 'Age', 'SibSp', 'Parch','Fare','Embarked']\ndfC=train_data[col_take].copy() # check point ->dfC to be cleansed\n# 2.2 Turn the Sex into binary value\ndfC.Sex=train_data['Sex'].map({'male':0,'female':1})\n# 2.3 Calculate the average age in different sub-groups (Sex and Pclass)\nage_median=train_data.groupby(['Sex','Pclass']).Age.median()\ndisplay(age_median)","62ebcfa1":"#2.4 A func to estimate the missing value by averaging the not null value in sub_group\ndef group_fill(rec,median):\n    sex=rec['Sex']\n    Pclass=rec['Pclass']\n    ind=tuple((sex,Pclass))\n    est=median[ind]\n    return est\n#2.5 Estimate the missing age and fill the Null age\nageEst=train_data[train_data['Age'].isnull()].apply(lambda x: group_fill(x,age_median),axis=1)\ndfC.loc[:,'Age'].fillna(value=ageEst,inplace=True)","92c16014":"#3.5 Extract title of passenger name\ntitle=train_data['Name'].apply(lambda x:x.split(',')[1].split('.')[0]).copy()\ntitle=title.str.strip()\ndfC['Title']=title","785acb1d":"#2.6 Repeat the same process to the test data\n#1 select the features\ncol_take_T=['Pclass', 'Sex', 'Age', 'SibSp', 'Parch','Fare','Embarked'] \ndfC_T=test_data[col_take_T]\n#2 turn Sex into binary\ndfC_T.Sex=test_data['Sex'].map({'male':0,'female':1})\n#3 Averaging age in each subgroup\nage_medianT=test_data.groupby(['Sex','Pclass']).Age.median()\n#4 Apply the 'short func' to fill the missing age\nageEst_test=test_data[test_data['Age'].isnull()].apply(lambda x: group_fill(x,age_medianT),axis=1)\ndfC_T.loc[:,'Age'].fillna(value=ageEst_test,inplace=True)\n#5 Extract the title from passenger name\ntitle_T=test_data['Name'].apply(lambda x:x.split(',')[1].split('.')[0]).copy()\ntitle_T=title_T.str.strip()\ndfC_T['Title']=title_T","739ebe8e":"dfC_T.Title[0]","9a9fdea2":"#2.7 Special case found-> one of the fare is missing\n# dfC_T.isna().any().sum()\n# Estimate by median fare in each sub-group( Male's is cheaper, lower class is cheaper)\nfare_medianT=test_data.groupby(['Sex','Pclass']).Fare.median()\nest_Fare=test_data[test_data['Fare'].isnull()].apply(lambda x: group_fill(x,fare_medianT),axis=1)\ndfC_T.loc[:,'Fare'].fillna(value=est_Fare,inplace=True)\ndisplay(dfC_T)","0d1eae58":"#3.1 Check the correlation\n# 1. The age is not correlated to survival rate\n# 2. The Pclass and Sex are inportant factor, so is the fare (but the fare is also correlated to Class)\n# 3. SibSp and Parch not very correlated to survival (may need some feature engineering?)\n# 4. Par and SibSp are highly correlated to each otder\ndata_exp=dfC.corr()\nf=plt.figure(figsize=(10,5))\nax=f.add_subplot(111)\nsns.heatmap(data_exp,ax=ax,vmin=-1,vmax=1,annot=True,annot_kws={\"size\": 20},cmap='RdBu')\nax.set_xticklabels(ax.get_xticklabels(), rotation = 0, fontsize = 16)\nax.set_yticklabels(ax.get_yticklabels(), rotation = 0, fontsize = 16)","cb4a9cd9":"## 3.2 Use 4 type of people instead of Sex, normal Mr, Master, Mrs, Miss, \n# display(dfC.Title.unique())\ntest=dfC.groupby(['Sex','Title']).count()\nl1=[]\nl2=[]\nfor i in test.index:\n    if i[0] == 0:\n        l1.append(i[1])\n    else:\n        l2.append(i[1])\nl1.remove('Master')\nl2.remove('Mrs')","9312e890":"title_dict={'Mr':l1,'Master':'Master','Miss':l2,'Mrs':'Mrs'}\ntitle_dict","e463dae3":"dfC.Title.map(title_dict)","cfac04f1":"dfC.groupby(['Sex','Title'])[['Survived']].mean()","7ebba146":"title_dict","d0d7ab12":"# dfC.groupby(['Title'])[['Survived']].mean() \n# title_surv=dfC[dfC['Survived']==1].groupby(['Title'])['Survived'].count()\n# title_pass=dfC[dfC['Survived']==0].groupby(['Title'])['Survived'].count()\n# f2=plt.figure(figsize=(10,5))\n# ax2=f2.add_subplot(111)\n# ax2.bar(title_surv.index,title_surv)\n# ax2.bar(title_pass.index,title_pass)","63d8b788":"#3.1 Call the solvers\nstd=StandardScaler()\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n#3.2 Standarise the data\ncol_for_std=['Pclass','Age','SibSp','Parch','Fare']\nstd.fit(dfC[col_for_std])\ndfC_std=dfC.copy()\ndfC_std[col_for_std]=std.transform(dfC[col_for_std])\nY=dfC_std['Survived']\n# Feature selection\nX=dfC_std.iloc[:,1:] #-> can skip the Fare\nfs=['Pclass','Sex','Fare','Age']\nX=dfC_std.loc[:,fs]\nX","e1890ea7":"#3.3 Sovle\nmodel.fit(X,Y)\nmodel.score(X,Y)","a9a55783":"#3.4 Validate the test_data\ndfC_T_std=dfC_T.copy()\n# Transform the validating data to the same scale as that in train_data\ndfC_T_std[col_for_std]=std.transform(dfC_T_std[col_for_std])\n# Feature selection\n# X_T=dfC_T_std.iloc[:,0:]#-> can skip the Fare\nX_T=dfC_T_std.loc[:,fs]\nX_T\n# 3.5 Predict the labels of test_data\nY_pred=model.predict(X_T)\n# y_out=pd.DataFrame(index=test_data.PassengerId,columns=['Survived'],data=Y_pred)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': Y_pred})\noutput","07948e6b":"#3.5  Output the data\noutput.to_csv('my_submission.csv', index=False)","27575fd8":"### 3.Applying machine learning algarithms to predict the survivals in test dataset","7d539830":"> ## 2.Cleanse the data\nSome of the columns(features) may not be useful, but they serve as a good practice.\nTicket and Carbin are ignored (due to missing\/incomplete values)","396c0457":"# Project_T1: Survival Prediction on Titanic  Passengers","5c3636fd":"## 3. Feature Selection","20973f06":"> ## 1. Load the libaries and data "}}