{"cell_type":{"ddfd9a21":"code","0eaeadbb":"code","285339b0":"code","cc5bb995":"code","eead74fc":"code","3feeb9f4":"code","8792b323":"code","47f39f7e":"code","96e4b2a2":"code","8a72d06c":"code","1e57b8ec":"code","066d31ef":"code","8569095c":"code","4191993d":"code","026b88ba":"code","483f4d50":"code","408bf15c":"code","fee8e961":"code","ac97d770":"code","fd8748f7":"markdown","cafdff98":"markdown","4812c6e7":"markdown","9659d095":"markdown","89036dc3":"markdown","bfbd656d":"markdown","7ade5eab":"markdown","0bbfc1c6":"markdown","4a8b7ed3":"markdown","87b16d02":"markdown"},"source":{"ddfd9a21":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","0eaeadbb":"df_train = pd.read_csv(\"\/kaggle\/input\/petfinder-pawpularity-score\/train.csv\",)\ndf_test = pd.read_csv(\"\/kaggle\/input\/petfinder-pawpularity-score\/test.csv\")","285339b0":"df_train.isna().sum()","cc5bb995":"df_train.describe()","eead74fc":"plt.figure(figsize = (10,5))\nsns.set_palette(\"pastel\")\nsns.histplot(data = df_train, x = \"Pawpularity\", kde = True)\nplt.title('Distribution of Pawpularity Scores', fontsize = 20)\nplt.axvline(df_train[\"Pawpularity\"].mean(), c = \"green\", ls = \"--\", label = \"Mean Pawpularity\")\nplt.axvline(df_train[\"Pawpularity\"].median(), c = \"black\", ls = \"--\", label = \"Median Pawpularity\")\nplt.legend()\nplt.show()","3feeb9f4":"plt.figure(figsize = (15, 10))\nsns.heatmap(df_train.corr(), annot = True)","8792b323":"print(df_train.corr(\"spearman\")[\"Pawpularity\"].sort_values(ascending = False))\nprint(df_train.corr(\"kendall\")[\"Pawpularity\"].sort_values(ascending = False))","47f39f7e":"# Preparing our training set\nX = df_train.copy()\ny = X.pop(\"Pawpularity\")\n\nX.drop(\"Id\", axis = 1, inplace = True)","96e4b2a2":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\nmodel = RandomForestRegressor()\nscore = cross_val_score(\n    model, X, y, scoring = \"neg_root_mean_squared_error\"\n)\n\nbaseline_score = - score.mean()\nbaseline_score","8a72d06c":"# First submission with baseline score, just to see the performance on test set\nX_test = df_test.drop(\"Id\", axis = 1)\n\nmodel_1 = RandomForestRegressor()\nmodel_1.fit(X, y)\n\ny_pred = model_1.predict(X_test).round(2)\n\ndic = {\"Id\": df_test[\"Id\"], \"Pawpularity\": y_pred}\ndf_submit = pd.DataFrame(data = dic)\ndf_submit.to_csv(\"submission.csv\", index = False)","1e57b8ec":"def score_dataset(X, y, model):\n    model.fit(X, y)\n    score = cross_val_score(\n        model, X, y, scoring = \"neg_root_mean_squared_error\"\n    )\n\n    score = - score.mean()\n    return score","066d31ef":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y):\n    X = X.copy()\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features = discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name = \"MI Scores\", index = X.columns)\n    mi_scores = mi_scores.sort_values(ascending = False)\n    return mi_scores\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending = True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","8569095c":"mi_scores = make_mi_scores(X, y)\nmi_scores","4191993d":"def drop_uninformative(df, mi_scores):\n    return df.loc[:, mi_scores > 0.0]\n\nX = drop_uninformative(X, mi_scores)\n\nrmse = score_dataset(X, y, RandomForestRegressor(n_estimators = 500))\nrmse","026b88ba":"# Second submission with RF regressor, use just mutual info\nX_test = df_test.drop(\"Id\", axis = 1)\nX_test = drop_uninformative(X_test, mi_scores)\n\nmodel_2 = RandomForestRegressor(n_estimators = 500)\nmodel_2.fit(X, y)\n\ny_pred = model_2.predict(X_test).round(2)\n\ndic = {\"Id\": df_test[\"Id\"], \"Pawpularity\": y_pred}\ndf_submit = pd.DataFrame(data = dic)\ndf_submit.to_csv(\"submission.csv\", index = False)","483f4d50":"from sklearn.ensemble import GradientBoostingRegressor\n\nmodel = GradientBoostingRegressor()\nscore_dataset(X, y, model)","408bf15c":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor()\nscore_dataset(X, y, model)","fee8e961":"# score_dataset(X, y, XGBRegressor(n_estimators = 5000, learning_rate = 0.01, n_jobs = 4))\n# 20.60","ac97d770":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nmodel = XGBRegressor(n_estimators = 1000, learning_rate = 0.5, n_jobs = 4)\nmodel.fit(X_train, y_train, early_stopping_rounds = 5, eval_set = [(X_valid, y_valid)], verbose = False)\npredict = model.predict(X_valid)\n\nfrom sklearn.metrics import mean_squared_error\n\nmean_squared_error(predict, y_valid, squared = False)","fd8748f7":"# Feature Engineering","cafdff98":"## Cluster\n","4812c6e7":"Seens like random forest regressor do quite a good job.","9659d095":"## Baseline score","89036dc3":"Positive correlations:\n- Eyes with Face (0.58): If you see decently clear face, you may see the eyes as well.\n- Occlusion with Human (0.63): Probably the part of humam body is the undesired object blocking part of the pet.\n\nNegative correlations:\n- Blur with Eyes (-0.51): When Blur is true the Eyes is automatically set to zero as stated in the competition's data descriptiion.\n- Group with Near (-0.32): If we have more than one pet in the photo, you need to zoom out to take a picture.","bfbd656d":"## PCA","7ade5eab":"## Library","0bbfc1c6":"We don't have nulls","4a8b7ed3":"## Load Data","87b16d02":"## Mutual Information"}}