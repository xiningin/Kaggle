{"cell_type":{"bea8d537":"code","5721850a":"code","242bcabd":"code","3113dca0":"code","3f32e2d7":"code","1f5aba55":"code","2e43df50":"code","686ae00a":"code","047e61ca":"code","0c97dc82":"code","ae7a756d":"code","5c1aef9b":"code","f25e95ec":"code","0aca7366":"code","78c40302":"code","73b5cd27":"code","a2aee6e6":"code","47cc87c0":"code","58debcb9":"code","7d6db20d":"code","38cc396d":"code","83ce2d15":"code","d3970c35":"code","b31b8464":"code","34ccf080":"code","7dd705ab":"code","801c35f7":"code","07c2bd2b":"code","9b6a6052":"code","5e35e5d2":"code","e5d51fce":"code","5cf9dcb8":"code","01223239":"code","e664fd12":"code","d4821d9f":"markdown"},"source":{"bea8d537":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nfrom scipy.interpolate import interp1d\nimport gc\nimport soundfile as sf\n# Librosa Libraries\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import roc_auc_score, label_ranking_average_precision_score\nfrom sklearn.model_selection import GroupKFold\nimport warnings\nwarnings.filterwarnings('ignore')","5721850a":"import sys\nsys.path.append('..\/input\/iterative\/iterative_stratification-0.1.6-py3-none-any.whl')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","242bcabd":"trainfiles = glob.glob( '..\/input\/rfcx-species-audio-detection\/train\/*.flac' )\ntestfiles = glob.glob( '..\/input\/rfcx-species-audio-detection\/test\/*.flac' )\nlen(trainfiles), len(testfiles), trainfiles[0]","3113dca0":"traint = pd.read_csv( '..\/input\/rfcx-species-audio-detection\/train_tp.csv' )\ntraint['t_dif'] = traint['t_max'] - traint['t_min']\ntraint['f_dif'] = traint['f_max'] - traint['f_min']\n\ntrainf = pd.read_csv( '..\/input\/rfcx-species-audio-detection\/train_fp.csv' )\ntrainf['t_dif'] = trainf['t_max'] - trainf['t_min']\ntrainf['f_dif'] = trainf['f_max'] - trainf['f_min']\n\ntraint.shape, trainf.shape","3f32e2d7":"traint.head()","1f5aba55":"trainf.head()","2e43df50":"trainf.t_min.hist(bins=100)\ntrainf.t_max.hist(bins=100, alpha=0.5)","686ae00a":"trainf.f_min.hist(bins=20)\ntrainf.f_max.hist(bins=20, alpha=0.5)","047e61ca":"trainf.t_dif.hist(bins=100)\n","0c97dc82":"trainf.f_dif.unique()","ae7a756d":"trainf.nunique()","5c1aef9b":"traint.nunique()","f25e95ec":"data, samplerate = sf.read(trainfiles[0]) \nprint( data.shape, samplerate )\nlibrosa.display.waveplot(y = data, sr = samplerate, color = \"#B14D\")","0aca7366":"traint.describe()","78c40302":"trainf.describe()","73b5cd27":"\"\"\"\nTRAIN = []\nTARGET = []\nfor i in tqdm(range(traint.shape[0])):\n\n    fn = traint.recording_id.values[i]\n    tmin = traint.t_min.values[i]\n    tmax = traint.t_max.values[i]\n    fmin = traint.f_min.values[i]\n    fmax = traint.f_max.values[i]\n    #print(tmin,tmax, fmin,fmax )\n\n    data, samplerate = sf.read( '..\/input\/rfcx-species-audio-detection\/train\/'+fn+'.flac')\n    #print( data.shape, samplerate )\n    var_time = np.arange(0,data.shape[0]) \/ samplerate\n\n    data = data[ np.where( (var_time>=tmin)&(var_time<=tmax) )[0] ]\n\n    varfft = np.abs( np.fft.fft(data)[:(len(data)\/\/2)] )\n    x = np.linspace(0, len(varfft), num=len(varfft), endpoint=True)\n    f1 = interp1d(x, varfft, kind='cubic')\n    x = np.linspace(0, len(varfft), num=1000, endpoint=True)\n    varfft = f1(x)\n    \n    TRAIN.append( varfft )\n    TARGET.append( traint.species_id.values[i] )\n    \nFT = np.stack(TRAIN)\nTARGET = np.array(TARGET)\nFT.shape, len(TARGET)\n\"\"\"\ndata = np.load('..\/input\/training-testing-data\/FT.npz')\nFT = data['a']\ndata = np.load('..\/input\/training-testing-data\/TARGET.npz')\nTARGET = data['a']","a2aee6e6":"np.unique(TARGET, return_counts=True)","47cc87c0":"\"\"\"\nfrom joblib import Parallel, delayed\n\ndef extract_features( fn ):\n    data, samplerate = sf.read( '..\/input\/rfcx-species-audio-detection\/train\/'+fn+'.flac')\n\n    varfft = np.abs( np.fft.fft(data)[:(len(data)\/\/2)] )\n    x = np.linspace(0, len(varfft), num=len(varfft), endpoint=True)\n    f1 = interp1d(x, varfft, kind='cubic')\n    x = np.linspace(0, len(varfft), num=1000, endpoint=True)\n    varfft = f1(x)\n    \n    return varfft\n    \nFP = Parallel(n_jobs=4)(delayed(extract_features)(fn) for i in tqdm(trainf.recording_id.values))\nFP = np.stack(FP)\ngc.collect()\nFP.shape\n\"\"\"\ndata = np.load('..\/input\/training-testing-data\/FP.npz')\nFP = data['a']","58debcb9":"\"\"\"\ndef extract_features( fn ):\n    data, samplerate = sf.read(fn)\n\n    varfft = np.abs( np.fft.fft(data)[:(len(data)\/\/2)] )\n    x = np.linspace(0, len(varfft), num=len(varfft), endpoint=True)\n    f1 = interp1d(x, varfft, kind='cubic')\n    x = np.linspace(0, len(varfft), num=1000, endpoint=True)\n    varfft = f1(x)\n    \n    return varfft\n    \nTEST = Parallel(n_jobs=4)(delayed(extract_features)(fn) for fn in tqdm(testfiles))\nTEST = np.stack(TEST)\ngc.collect()\nTEST.shape\n\n\"\"\"\ndata = np.load('..\/input\/training-testing-data\/TEST.npz')\nTEST = data['a']","7d6db20d":"TRAIN = np.vstack( (FT, FP) )\nTRAIN.shape","38cc396d":"tt = traint[['recording_id','species_id']].copy()\ntf = trainf[['recording_id','species_id']].copy()\n\ntf['species_id'] = -1\n\nTRAIN_TAB = pd.concat( (tt, tf) )\n\nfor i in range(24):\n    TRAIN_TAB['s'+str(i)] = 0\n    TRAIN_TAB.loc[TRAIN_TAB.species_id==i,'s'+str(i)] = 1\n\nTRAIN_TAB.shape","83ce2d15":"TRAIN_TAB.head()","d3970c35":"from sklearn.preprocessing import StandardScaler\n\nstd = StandardScaler()\nstd.fit( np.vstack((TRAIN,TEST)) )\n\nTRAIN = std.transform(TRAIN)\nTEST  = std.transform(TEST)\ngc.collect()","b31b8464":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras import layers,regularizers,Sequential,backend,callbacks,optimizers,metrics,losses","34ccf080":"def create_model(num_columns,target_columns):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(num_columns),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5), \n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(500, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),        \n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(target_columns, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n                  loss=losses.BinaryCrossentropy(label_smoothing=0.000001),metrics=['AUC']\n                  )\n    return model","7dd705ab":"tar_col = []\nfor tgt in range(24):\n    tar_col.append('s'+str(tgt))","801c35f7":"TRAIN_TARGETS = TRAIN_TAB[tar_col].copy()","07c2bd2b":"TRAIN = np.array(TRAIN,dtype='float64')\nTRAIN_TARGETS = TRAIN_TARGETS[tar_col].values\nTRAIN_TARGETS = np.array(TRAIN_TARGETS,dtype='float64')","9b6a6052":"TRAIN.argmax()","5e35e5d2":"sub = pd.DataFrame({'recording_id': [f.split('\/')[-1].split('.')[0] for f in testfiles] })\nytrain = np.zeros((TRAIN.shape[0],len(tar_col)))\nytest = np.zeros((TEST.shape[0],len(tar_col)))\nN_SPLITS = 5 \nSEED = 2\ngroups = TRAIN_TAB['recording_id'].values\nfor tar in range(24):\n    print(f'---------------------TARGET{tar}------------------------')     \n    for n, (tr, te) in enumerate(GroupKFold(N_SPLITS).split(TRAIN, TRAIN_TAB[tar_col[tar]],groups)):\n                model = create_model(TRAIN.shape[1],1)\n                print(f'FOLD-{n}')\n                checkpoint_path_model = f'-model-repeat:{0}_Fold:{n}.hdf5'\n                reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=0, epsilon=1e-4, mode='min')\n                cb_checkpt = ModelCheckpoint(checkpoint_path_model, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                     save_weights_only = True, mode = 'min')\n           \n                model.fit(TRAIN[tr,:],\n                      TRAIN_TARGETS[tr,tar],\n                      validation_data=(TRAIN[te,:], TRAIN_TARGETS[te,tar]),\n                      epochs= 30 , batch_size=128,\n                      callbacks=[reduce_lr_loss, cb_checkpt], verbose=0,\n                      )     \n                os.remove(checkpoint_path_model)\n                ytrain[te,tar] += model.predict(TRAIN[te,:])[:,0]\n                ytest[:,tar] +=  model.predict(TEST)[:,0]\/(N_SPLITS)\n            \n                FOLD_SCORE = roc_auc_score(TRAIN_TARGETS[te,tar],ytrain[te,tar])\n                print(f'FOLD-{n}-AUC score = {FOLD_SCORE}')","e5d51fce":"AUC_scores = []\nfor n  in range(len(tar_col)): \n    AUC_scores.append(roc_auc_score(TRAIN_TARGETS[:,n], ytrain[:,n]))\n    print( f'Target{n} AUC', roc_auc_score(TRAIN_TARGETS[:,n], ytrain[:,n]) )\nprint(f'mean of AUC scores = {np.mean(AUC_scores)}') \nprint(f'label_ranking_average_precision_score = {label_ranking_average_precision_score(TRAIN_TARGETS,ytrain)}')","5cf9dcb8":"TRAIN_TAB[tar_col] =  ytrain\nsub[tar_col] = ytest","01223239":"sub.head()","e664fd12":"sub.to_csv('submission.csv', index=False)","d4821d9f":"# Min and Max frequencies are: 93 and 10687"}}