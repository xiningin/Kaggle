{"cell_type":{"957fc445":"code","6aadcd41":"code","ff52a7f3":"code","16aa99dd":"code","a2fa7749":"code","c2344c17":"code","ccb0d3e1":"code","6310bd8e":"code","3f5e26d1":"code","325170b1":"code","919c67cd":"code","40b86392":"code","402b84ac":"code","3fa39f61":"code","5849a419":"code","0be24520":"code","c9f47f2d":"code","f32fa90f":"code","ce0c84b4":"code","52bd4867":"code","5ef04355":"code","220dbeaf":"code","6c9fe523":"code","2256da91":"code","03711f2e":"code","40436bb1":"code","245b1ddd":"code","8e25696f":"code","2bdc798f":"code","a6847ad1":"code","88b56cfe":"code","3d350dd9":"code","9b60cfce":"code","9d1d013f":"code","1eabef30":"code","1804e765":"code","af1e095e":"code","b6fe31bb":"code","a09ddc67":"code","bbe0ae16":"code","fb304a92":"code","74d9ba9b":"code","42de69ef":"code","4e38302e":"code","3fa3e662":"code","e27d55a9":"code","d7ea297a":"code","4ea9fd33":"code","d62a12b0":"code","bbc7a77f":"code","2fa64814":"code","9a7594b7":"code","f8d4c1d3":"code","36fe2155":"code","beaef935":"code","060797cf":"code","007b06c9":"code","021efbb8":"code","6e74e6c6":"code","c9e0c62b":"code","164f3aa4":"code","797c86d3":"code","e547955b":"code","45076d67":"code","54988390":"code","5cad7b22":"code","340d096e":"code","22c61ead":"code","27fb8466":"code","e456cdb9":"code","71072ef3":"code","c5107f5f":"code","1f3e8078":"code","33734c90":"code","947204f6":"code","e3643ea8":"code","60ccc966":"code","9499ce90":"code","09cf80df":"code","caffc55a":"code","418da398":"code","80c54290":"code","915265e9":"code","d01322ca":"code","8d54fa48":"code","27fd1047":"code","efd3c7aa":"code","15b47210":"code","9f6ff5ed":"code","fa4a5c1a":"code","2750728e":"code","170cc4ca":"code","aff27363":"code","1952141a":"code","56885003":"code","ecae5e98":"code","d31d3cee":"code","3605ec21":"code","1e51c4e7":"code","e441a371":"code","15689a41":"code","186b8445":"code","0dcc9d20":"code","7870e220":"code","0de4bb58":"code","8413b363":"code","b5cd694a":"code","4b9932d1":"code","73186a3f":"code","6e0973eb":"code","730c3e1d":"code","5d320ef8":"code","1a05bb8a":"code","dc044ac8":"code","b7ff4829":"code","9e24164f":"code","a9458c37":"code","b857d2e6":"code","94fc67ba":"code","5b9e343a":"code","bd6385ca":"code","3e414a07":"code","14c1250c":"code","e68880a7":"code","5db9ddf2":"code","0a0abdbe":"code","5c2d54a7":"code","a59b8580":"markdown","6c58b5cf":"markdown","c298ef3b":"markdown","d6705f3b":"markdown","ee0d1bc9":"markdown","95ceab16":"markdown","9fb57627":"markdown","0479a6aa":"markdown","db56ce91":"markdown","a2e4ad6d":"markdown","d06eae07":"markdown","31d764b9":"markdown","42887bda":"markdown","752e0ec6":"markdown","91d6b32d":"markdown","1f0599a8":"markdown","ad8d72ad":"markdown","8a117301":"markdown","ec8c2683":"markdown","c381ffbd":"markdown","cf7a8e45":"markdown","e847ccec":"markdown","c5c348b9":"markdown","49d304c2":"markdown","ec752813":"markdown","7bba575a":"markdown","1496f837":"markdown","1d54124f":"markdown","9fa6fc2a":"markdown","dbefbf75":"markdown","9363e02f":"markdown","d6fafdc3":"markdown","320c0db3":"markdown","edc5bc04":"markdown","96648e5c":"markdown","0158db7d":"markdown","ee4bc645":"markdown","96fe76b5":"markdown","68144750":"markdown","1e510b45":"markdown","0feb1ac6":"markdown","8c6cf9be":"markdown","fc05ff77":"markdown","a5fe9b0e":"markdown","8c7c5a7d":"markdown","8887e473":"markdown","4d82c39c":"markdown","565af373":"markdown","179f8ae9":"markdown","43d576eb":"markdown","fecd8f1b":"markdown","455cc0bb":"markdown","e2c398f2":"markdown","da531bd4":"markdown","97a422d8":"markdown","83f71795":"markdown","ededf930":"markdown","676dcd55":"markdown","9e1c7163":"markdown","5b47daab":"markdown","9ed7ea8e":"markdown","f31e03bb":"markdown","3ab502a5":"markdown","ae2fd45f":"markdown","3296064a":"markdown","e636c813":"markdown","dd787c58":"markdown","6966dc72":"markdown","63ae99df":"markdown","60f57f59":"markdown","fe73a9d9":"markdown","9a6d7661":"markdown","c4a26662":"markdown","45b9027b":"markdown","f9dd3317":"markdown","93d35188":"markdown","9178c81b":"markdown","48d6e339":"markdown","6063458c":"markdown"},"source":{"957fc445":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.feature_extraction import FeatureHasher\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","6aadcd41":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\n\ntrain_id = df_train['Id']\ntest_id = df_test['Id']\n\ndf_train.drop(columns=['Id'], inplace=True)\ndf_test.drop(columns=['Id'], inplace=True)\n\nntrain = df_train.shape[0]\nntest = df_test.shape[0]\n\nall_data = pd.concat((df_train, df_test)).reset_index(drop=True)","ff52a7f3":"print(df_test.shape, df_train.shape, all_data.shape)\nprint(df_test.shape[0] + df_train.shape[0])","16aa99dd":"# see the decoration\ndf_train.columns","a2fa7749":"# do this in order to view all the columns, otherwise pandas just shows a summary\npd.set_option('display.max_columns', None) \n\n# Lets see what the data looks like\ndf_train.head(5)","c2344c17":"df_train['SalePrice'].describe()","ccb0d3e1":"# Separate out all the numeric features in the data\nnum_columns = df_train._get_numeric_data().columns\nprint(num_columns)\nprint(len(num_columns))\n\n# Separate out all the non-numeric features in the data\ncateg_columns = pd.Index(list(set(df_train.columns) - set(num_columns)))\nprint(categ_columns)\nprint(len(categ_columns))","6310bd8e":"# moving features from num_columns to categ_columns\ncateg_columns = categ_columns.append(pd.Index(['MSSubClass', 'OverallQual', 'OverallCond']))\nnum_columns = num_columns.drop(['MSSubClass', 'OverallQual', 'OverallCond'])","3f5e26d1":"print(len(num_columns), len(categ_columns))","325170b1":"df_train[num_columns].head(5)","919c67cd":"def barplot_with_anotate(feature_list, y_values):\n    x_pos = np.arange(len(feature_list))\n\n    plt.bar(x_pos, y_values);\n    plt.xticks(x_pos, feature_list, rotation=270);\n    for i in range(len(feature_list)):\n        plt.text(x=x_pos[i]-0.3, y=y_values[i]+1.0, s=y_values[i])","40b86392":"feature_lengths_sorted = sorted([len(df_train[feature].unique()) for feature in num_columns])\nbarplot_with_anotate(num_columns, feature_lengths_sorted)\nplt.rcParams[\"figure.figsize\"] = [20, 12]","402b84ac":"num_discrete_columns = []\n\nfor feature in num_columns:\n    feature_len = len(df_train[feature].unique())\n    if feature_len < 30:\n        num_discrete_columns.append(feature)\nnum_discrete_columns = pd.Index(num_discrete_columns)\nnum_cont_columns = pd.Index(list(set(num_columns) - set(num_discrete_columns)))","3fa39f61":"# print the details of those discrete valued features\nfor feature in num_discrete_columns:\n    feature_len = len(df_train[feature].unique())\n    print(feature, feature_len)\n    print(df_train[feature].unique())","5849a419":"# The features LowQualFinSF, 3SsnPorch, PoolArea, MiscVal \n# belong to the list of continous features as they take on values from a continous distribution.\nnum_cont_columns = num_cont_columns.append(pd.Index(['LowQualFinSF', '3SsnPorch', 'PoolArea', 'MiscVal']))\nnum_discrete_columns = num_discrete_columns.drop(['LowQualFinSF', '3SsnPorch', 'PoolArea', 'MiscVal'])","0be24520":"# print the details of the continuous valued features\nfor feature in sorted(num_cont_columns, key=lambda feature: len(df_train[feature].unique()), reverse=True):\n    feature_len = len(df_train[feature].unique())\n    print(feature, feature_len)","c9f47f2d":"# The 3 year related features - YearBuilt, GarageYrBlt and YearRemodAdd belong to the list of discrete features.\nnum_discrete_columns = num_discrete_columns.append(pd.Index(['YearBuilt', 'GarageYrBlt', 'YearRemodAdd']))\nnum_cont_columns = num_cont_columns.drop(['YearBuilt', 'GarageYrBlt', 'YearRemodAdd'])","f32fa90f":"df_train[categ_columns].head(5)","ce0c84b4":"# lets look at what the categorical features represent\nfor feature in sorted(categ_columns, key=lambda feature: len(df_train[feature].unique()), reverse=True):\n    feature_len = len(df_train[feature].unique())\n    print(feature, feature_len)\n    print(df_train[feature].unique())","52bd4867":"# remove duplicate columns\nnum_cont_columns = num_cont_columns.drop_duplicates()\nnum_discrete_columns = num_discrete_columns.drop_duplicates()\ncateg_columns = categ_columns.drop_duplicates()","5ef04355":"print(\"Categorical columns: \", len(categ_columns))\nprint(\"Continuous-valued numeric columns: \", len(num_cont_columns))\nprint(\"Discrete-valued numeric columns: \", len(num_discrete_columns))\nprint(\"-\"*10)\nprint(\"Total columns: \", df_train.shape[1])","220dbeaf":"df_train.drop(df_train[df_train[\"GrLivArea\"] > 4000].index, inplace=True)\ndf_train.shape\n\nntrain = df_train.shape[0]\nntest = df_test.shape[0]\n\nall_data = pd.concat((df_train, df_test)).reset_index(drop=True)","6c9fe523":"df_train[num_cont_columns].describe()","2256da91":"def missing_features(data, column_set):\n    incomplete_features = {feature: data.shape[0]-sum(data[feature].value_counts())\n                                   for feature in column_set\n                                   if not sum(data[feature].value_counts()) == data.shape[0]}\n    incomplete_features_sorted = sorted(incomplete_features, key=lambda feature: incomplete_features[feature], reverse=True)\n    incompleteness = [round((incomplete_features[feature]\/data.shape[0])*100, 2) for feature in incomplete_features_sorted]\n    barplot_with_anotate(incomplete_features_sorted, incompleteness)\n    plt.ylabel(\"Percentage (%) of values that are missing\")\n    plt.rcParams[\"figure.figsize\"] = [16, 8]\n    \n    for feature, percentage in zip(incomplete_features_sorted, incompleteness):\n        print(feature, incomplete_features[feature], \"(\", percentage, \")\")","03711f2e":"missing_features(all_data, num_cont_columns)","40436bb1":"all_data.loc[:, 'LotFrontage'].fillna(all_data['LotFrontage'].mean(), inplace=True)\n\nfor feature in ['GarageArea', 'MasVnrArea', 'TotalBsmtSF', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF']:\n    all_data.loc[:, feature].fillna(0, inplace=True)","245b1ddd":"# first we view a scatter plot of each feature vs. SalePrice\n\nnum_cont_columns_list = list(num_cont_columns)\n\n# show max of 6 features in each row\nmax_in_row = 6\n\nprint(len(num_cont_columns_list))\nfor i in range(0, len(num_cont_columns_list), max_in_row):\n    sns.pairplot(df_train, x_vars=num_cont_columns_list[i:i+max_in_row], y_vars=['SalePrice'])","8e25696f":"# then we see the correlations\nprint('Correlation of each feature with SalePrice:')\ncorr = df_train[num_cont_columns].corr()\ncorr_sorted = corr.sort_values([\"SalePrice\"], ascending = False)\nprint(corr_sorted['SalePrice'])\n\nsns.set(font_scale=1.10)\nplt.figure(figsize=(8, 8))\n\nsns.heatmap(corr)","2bdc798f":"all_data['TotPorchSF'] = all_data['OpenPorchSF'] + all_data['ScreenPorch'] + \\\n                         all_data['3SsnPorch'] + all_data['EnclosedPorch']\nnum_cont_columns = num_cont_columns.append(pd.Index(['TotPorchSF']))","a6847ad1":"(df_train['PoolArea'] == 0).value_counts()","88b56cfe":"all_data.drop(columns=['PoolArea'], inplace=True)\nnum_cont_columns = num_cont_columns.drop(['PoolArea'])","3d350dd9":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\nnum_cont_columns = num_cont_columns.append(pd.Index(['TotalSF']))","9b60cfce":"all_data.drop(columns=['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'], inplace=True)\nnum_cont_columns = num_cont_columns.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'])","9d1d013f":"df_train = all_data[:ntrain][:]\nsns.distplot(df_train['SalePrice'])","1eabef30":"SalePriceLog = np.log1p(df_train['SalePrice'])\nsns.distplot(SalePriceLog)","1804e765":"df_train[num_discrete_columns].describe()","af1e095e":"missing_features(all_data, num_discrete_columns)","b6fe31bb":"all_data.loc[:, 'GarageYrBlt'].fillna(all_data['YearBuilt'], inplace=True)\nall_data.loc[:, 'BsmtFullBath'].fillna(0, inplace=True)\nall_data.loc[:, 'BsmtHalfBath'].fillna(0, inplace=True)\nall_data.loc[:, 'GarageCars'].fillna(0, inplace=True)","a09ddc67":"# print the details of those discrete valued features\nfor feature in num_discrete_columns:\n    feature_len = len(df_train[feature].unique())\n    print(feature, feature_len)\n    print(df_train[feature].unique())","bbe0ae16":"print('Total: ', len(num_discrete_columns))\ncorr = df_train[list(num_discrete_columns) + ['SalePrice']].corr()\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\nprint(corr['SalePrice'])","fb304a92":"all_data.drop(columns=['GarageArea'], inplace=True)\nnum_cont_columns = num_cont_columns.drop(['GarageArea'])","74d9ba9b":"df_train[['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']].describe()","42de69ef":"# Since the min of all 3 is 1872, we could replace their values with something like `year % 1872`\n\nall_data['YearBuilt'] = all_data['YearBuilt'] % 1872\nall_data['YearRemodAdd'] = all_data['YearRemodAdd'] % 1872\nall_data['GarageYrBlt'] = all_data['GarageYrBlt'] % 1872","4e38302e":"all_data['Bath'] = all_data['FullBath'] + 0.5*all_data['HalfBath']\nall_data.drop(columns=['FullBath', 'HalfBath'], inplace=True)\nnum_discrete_columns = num_discrete_columns.drop(['FullBath', 'HalfBath'])","3fa3e662":"all_data['BsmtBath'] = all_data['BsmtFullBath'] + 0.5*all_data['BsmtHalfBath']\nall_data.drop(columns=['BsmtFullBath', 'BsmtHalfBath'], inplace=True)\nnum_discrete_columns = num_discrete_columns.drop(['BsmtFullBath', 'BsmtHalfBath'])","e27d55a9":"missing_features(all_data, categ_columns)","d7ea297a":"for feature in ['PoolQC', 'MiscFeature', 'Alley', 'Fence']:\n    print(df_train[feature].value_counts())","4ea9fd33":"# drop PoolQC\nall_data.drop(columns=['PoolQC'], inplace=True)\ncateg_columns = categ_columns.drop(['PoolQC'])\n\n# filling NA\nall_data.fillna(value= {'MiscFeature': 'NA',\n                        'Fence': 'NA',\n                        'Alley': 'NA'}, inplace=True)\n\n# Shed\nall_data['Shed'] = all_data['MiscFeature'] == 'Shed'\nall_data.drop(columns=['MiscFeature'], inplace=True)\ncateg_columns = categ_columns.drop(['MiscFeature'])\ncateg_columns = categ_columns.append(pd.Index(['Shed']))","d62a12b0":"filling_dict = {'FireplaceQu': 'NA',\n                'GarageFinish': 'NA',\n                'GarageQual': 'NA',\n                'GarageType': 'NA',\n                'GarageCond': 'NA',\n                'BsmtExposure': 'NA',\n                'BsmtFinType2': 'NA',\n                'BsmtFinType1': 'NA',\n                'BsmtCond': 'NA',\n                'BsmtQual': 'NA',\n                'MasVnrType': 'None',\n                'Exterior1st': 'Other',\n                'Exterior2nd': 'Other',\n                'SaleType': 'Oth'}\n\nfor feature in ['Electrical', 'MSZoning', 'Functional', 'Utilities', 'KitchenQual']:\n    filling_dict[feature] = all_data[feature].mode().item()","bbc7a77f":"# Now, handle the rest of the incomplete features\nall_data.fillna(value=filling_dict, inplace=True)","2fa64814":"# Neighborhood is an ordinal feature but we can't decide on the order responsibly unless we know the Ames city\n# Hence we drop it\nall_data.drop(columns=['Neighborhood'], inplace=True)\ncateg_columns = categ_columns.drop(['Neighborhood'])","9a7594b7":"# GarageQual and GarageCond measure the same thing\nall_data.drop(columns=['GarageQual'], inplace=True)\ncateg_columns = categ_columns.drop(['GarageQual'])","f8d4c1d3":"# OverallQual and OverallCond seem to measure the same thing\n# yet they have a very different correlation coefficient with SalePrice\ndf_train[['SalePrice', 'OverallQual', 'OverallCond']].corr()","36fe2155":"# drop OverallCond\nall_data.drop(columns=['OverallCond'], inplace=True)\ncateg_columns = categ_columns.drop(['OverallCond'])","beaef935":"# new feature extraction\nall_data[\"NewerDwelling\"] = all_data[\"MSSubClass\"].isin([20, 60, 120, 160])","060797cf":"# NewerDwelling, BldgType and HouseStyle together capture everything that MSSubClass can tell\n# Hence we drop MSSubClass\nall_data.drop(columns=['MSSubClass'], inplace=True)\ncateg_columns = categ_columns.drop(['MSSubClass'])","007b06c9":"# simplity MSZoning as Residential or Non-residential\nall_data['Residential'] = all_data['MSZoning'].isin(['RH', 'RL', 'RP', 'RM'])\n\n# and drop MSZoning\nall_data.drop(columns=['MSZoning'], inplace=True)\ncateg_columns = categ_columns.drop(['MSZoning'])","021efbb8":"# simplify Exterior1st and Exterior2nd \nprint(df_train['Exterior1st'].value_counts())","6e74e6c6":"# We keep only the top 4 and move the rest to `Other`\nall_data['Exterior1st'][-all_data['Exterior1st'].isin(all_data['Exterior1st'].value_counts().index[0:4])] \\\n    = 'Other'\nall_data['Exterior2nd'][-all_data['Exterior2nd'].isin(all_data['Exterior1st'].value_counts().index[0:4])] \\\n    = 'Other'","c9e0c62b":"# simplify SaleType\nall_data['SaleType'].value_counts()","164f3aa4":"# keep only the top 3 and move rest to `Oth`\nall_data['SaleType'][-all_data['SaleType'].isin(all_data['SaleType'].value_counts().index[0:3])] = 'Oth'","797c86d3":"# simplify Functional\nall_data['Functional'][all_data['Functional'].isin(['Min1', 'Min2'])] = 'Min'\nall_data['Functional'][all_data['Functional'].isin(['Maj1', 'Maj2'])] = 'Maj'","e547955b":"# GarageType can either be Attchd or Detchd\n# new feature - GarageDetchd\nall_data['GarageDetchd'] = all_data['GarageType'].replace({'BuiltIn': 'Attchd', \n    'Basment': 'Attchd', 'CarPort': 'Attchd', '2Types': 'Detchd'})\n\n# drop GarageType\nall_data.drop(columns=['GarageType'], inplace=True)\ncateg_columns = categ_columns.drop(['GarageType'])","45076d67":"# Step 1: Convert ordinal features to numbers\nreplace_dict =   {'LotShape': {'IR3': 0, 'IR2': 1, 'IR1': 2, 'Reg': 3},\n                  'Utilities': {'ELO': 0, 'NoSeWa': 1, 'NoSewr': 2, 'AllPub': 3},\n                  'LandSlope': {'Sev': 0, 'Mod': 1, 'Gtl': 2},\n                  'ExterQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n                  'ExterCond': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n                  'BsmtQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n                  'BsmtCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n                  'BsmtExposure': {'NA': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n                  'BsmtFinType1': {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n                  'BsmtFinType2': {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n                  'HeatingQC': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n                  'Electrical': {'Mix': 0, 'FuseP': 1, 'FuseF': 2, 'FuseA': 3, 'SBrkr': 4},\n                  'KitchenQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4},\n                  'Functional': {'Sal': 0, 'Sev': 1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2': 5, 'Min1': 6,\n                                 'Typ': 7},\n                  'FireplaceQu': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n                  'GarageFinish': {'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},\n                  'GarageCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n                  'PavedDrive': {'N': 0, 'P': 1, 'Y': 2},\n                  'Fence': {'NA': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4},\n                  'Condition1': {'Artery': 0, 'Feedr': 0, 'RRAn': 1, 'RRAe': 1, 'RRNn': 2, 'RRNe': 2, \n                                 'Norm': 5, 'PosA': 10, 'PosN': 11},\n                  'Condition2': {'Artery': 0, 'Feedr': 0, 'RRAn': 1, 'RRAe': 1, 'RRNn': 2, 'RRNe': 2, \n                                 'Norm': 5, 'PosA': 10, 'PosN': 11}}\nall_data = all_data.replace(replace_dict)\n\n# all the other features of categ_columns are nominal\nnominal_columns = categ_columns.drop(list(replace_dict.keys()))\nnominal_columns = nominal_columns.drop(['OverallQual'])\nordinal_columns = pd.Series(list(replace_dict.keys()).append('OverallQual'))","54988390":"# Step 2: One-hot encoding for nominal features\nall_data = pd.get_dummies(all_data)","5cad7b22":"all_data.shape","340d096e":"# BsmtFinType1 * BsmtFinSF1\nall_data['BsmtFin1_Type*SF'] = all_data['BsmtFinType1'] * all_data['BsmtFinSF1']\n\n# BsmtFinType2 * BsmtFinSF2\nall_data['BsmtFin2_Type*SF'] = all_data['BsmtFinType2'] * all_data['BsmtFinSF2']\n\n# ExterQual * ExterCond\nall_data['Exter_Qual*Cond'] = all_data['ExterQual'] * all_data['ExterCond']\n\n# KitchenQual * no. of kitchens\nall_data['Kitchen_no*Qual'] = all_data['KitchenAbvGr'] * all_data['KitchenQual']\n\n# Condition1 * Condition2\nall_data['Condition1*Contition2'] = all_data['Condition1'] * all_data['Condition2']\n\n# OverallQual * TotalSF\nall_data['OverallQual*TotalSF'] = all_data['OverallQual'] * all_data['TotalSF']\n\n# BsmtQual * BsmtCond * BsmtExposure\nall_data['BsmtQual*Cond*Expo'] = all_data['BsmtQual'] * all_data['BsmtCond'] * all_data['BsmtExposure']","22c61ead":"# get a list of the top 10 most correlated features\ncorr = all_data[all_data.columns].corr()\ncorr_sorted = corr.sort_values([\"SalePrice\"], ascending = False)\ntop10_features = list(corr_sorted['SalePrice'][1:11].keys())","27fb8466":"# generate polynomial features and add to existing DataFrame\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\npoly_features.fit_transform(all_data[top10_features])\npoly_features.get_feature_names(top10_features)\n\npoly_df = pd.DataFrame(poly_features.transform(all_data[top10_features]), \n                                     columns=poly_features.get_feature_names(top10_features))\n\nall_data = pd.concat([all_data, poly_df], axis=1)\n\nprint(\"Final shape of data: \", all_data.shape)","e456cdb9":"poly_df.head(5)","71072ef3":"# then we see the correlations\ncorr = all_data[all_data.columns].corr()\ncorr_sorted = corr.sort_values([\"SalePrice\"], ascending = False)\n\n#corr_sorted['SalePrice'][0:50]\nbest_corr = corr_sorted[corr_sorted['SalePrice']>0.1]['SalePrice']\nworst_corr = corr_sorted[corr_sorted['SalePrice']<-0.1]['SalePrice']\nselected_corr = pd.concat([best_corr, worst_corr])\n\nprint(len(list(selected_corr)))\nprint(len(list(best_corr)), len(list(worst_corr)))","c5107f5f":"black_list = [elem for elem in list(all_data.columns) if elem not in list(selected_corr.keys())]\nall_data.drop(columns=black_list, inplace=True)\n\nall_data.shape","1f3e8078":"attrs = corr.drop('SalePrice').drop('SalePrice', axis=1)\n\nthreshold = 0.8\nimportant_corrs = (attrs[abs(attrs) > threshold][attrs != 1.0]) \\\n    .unstack().dropna().to_dict()\n    \nprint(\"There are\", len(important_corrs), \"pairs of non-target features with considerable correlations amongst them\")","33734c90":"all_data = all_data.loc[:,~all_data.columns.duplicated()]","947204f6":"nvalidate = int(0.2 * ntrain)\nntrain = int(ntrain - nvalidate)\n\ndf_train = all_data[:ntrain][:]\ndf_validate = all_data[ntrain:ntrain+nvalidate][:]\ndf_test = all_data[ntrain+nvalidate:][:]\ndf_test = df_test.drop('SalePrice', axis=1)","e3643ea8":"print(df_train.shape)\nprint(df_validate.shape)\nprint(df_test.shape)\nprint(ntrain, nvalidate, ntest)","60ccc966":"X_train= df_train.drop('SalePrice', axis= 1)\nY_train= df_train['SalePrice']\nY_train_log = SalePriceLog[:ntrain]\n\nX_validate = df_validate.drop('SalePrice', axis=1)\nY_validate = df_validate['SalePrice']\nY_validate_log = SalePriceLog[ntrain:ntrain+nvalidate]\n\nX_test= df_test","9499ce90":"# make sure that there are no NaNs\nmissing_features(X_train, X_train.columns)\nmissing_features(X_test, X_test.columns)\nmissing_features(X_validate, X_validate.columns)","09cf80df":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error","caffc55a":"def rmse_score(Y_true, Y_pred):\n    return round(np.sqrt(mean_squared_error(np.log(Y_true), np.log(Y_pred))), 5)","418da398":"linreg = LinearRegression(normalize=True)\nlinreg.fit(X_train, Y_train)\n\nY_train_pred = linreg.predict(X_train)\nY_train_pred[Y_train_pred < 1] = 1\nY_validate_pred = linreg.predict(X_validate)\nY_validate_pred[Y_validate_pred<1] = 1\n\nacc_lin_train = rmse_score(Y_train.values, Y_train_pred)\nacc_lin_validate = rmse_score(Y_validate.values, Y_validate_pred)\n\nprint(\"RMSE on train: \", acc_lin_train)\nprint(\"RMSE on validation: \", acc_lin_validate)","80c54290":"linreg_on_log = LinearRegression(normalize=True)\nlinreg_on_log.fit(X_train, Y_train_log)\n\nY_train_pred = np.exp(linreg_on_log.predict(X_train))\nY_train_pred[Y_train_pred < 0] = 0\nY_validate_pred = np.exp(linreg_on_log.predict(X_validate))\nY_validate_pred[Y_validate_pred<0] = 0\n\nacc_lin_train = rmse_score(Y_train.values, Y_train_pred)\nacc_lin_validate = rmse_score(Y_validate.values, Y_validate_pred)\n\nprint(\"RMSE on train: \", acc_lin_train)\nprint(\"RMSE on validation: \", acc_lin_validate)","915265e9":"lasso = Lasso()\nlasso.fit(X_train, Y_train)\n\nacc_lasso_train = rmse_score(Y_train.values, lasso.predict(X_train))\nacc_lasso_validate = rmse_score(Y_validate.values, lasso.predict(X_validate))\n\nprint(\"RMSE on train: \", acc_lasso_train)\nprint(\"RMSE on validation: \", acc_lasso_validate)","d01322ca":"lasso_on_log = Lasso()\nlasso_on_log.fit(X_train, Y_train_log)\n\nacc_lasso_train = rmse_score(Y_train.values, np.exp(lasso_on_log.predict(X_train)))\nacc_lasso_validate = rmse_score(Y_validate.values, np.exp(lasso_on_log.predict(X_validate)))\n\nprint(\"RMSE on train: \", acc_lasso_train)\nprint(\"RMSE on validation: \", acc_lasso_validate)","8d54fa48":"ridge = Ridge()\nridge.fit(X_train, Y_train)\n\nacc_ridge_train = rmse_score(Y_train.values, ridge.predict(X_train))\nacc_ridge_validate = rmse_score(Y_validate.values, ridge.predict(X_validate))\n\nprint(\"RMSE on train: \", acc_ridge_train)\nprint(\"RMSE on validation: \", acc_ridge_validate)","27fd1047":"ridge_on_log = Ridge()\nridge_on_log.fit(X_train, Y_train_log)\n\nacc_ridge_train = rmse_score(Y_train.values, np.exp(ridge_on_log.predict(X_train)))\nacc_ridge_validate = rmse_score(Y_validate.values, np.exp(ridge_on_log.predict(X_validate)))\n\nprint(\"RMSE on train: \", acc_ridge_train)\nprint(\"RMSE on validation: \", acc_ridge_validate)","efd3c7aa":"elastic_net = ElasticNet()\nelastic_net.fit(X_train, Y_train)\n\nacc_en_train = rmse_score(Y_train.values, elastic_net.predict(X_train))\nacc_en_validate = rmse_score(Y_validate.values, elastic_net.predict(X_validate))\n\nprint(\"RMSE on train: \", acc_en_train)\nprint(\"RMSE on validation: \", acc_en_validate)","15b47210":"elastic_net_on_log = ElasticNet()\nelastic_net_on_log.fit(X_train, Y_train_log)\n\nacc_en_train = rmse_score(Y_train.values, np.exp(elastic_net_on_log.predict(X_train)))\nacc_en_validate = rmse_score(Y_validate.values, np.exp(elastic_net_on_log.predict(X_validate)))\n\nprint(\"RMSE on train: \", acc_en_train)\nprint(\"RMSE on validation: \", acc_en_validate)","9f6ff5ed":"decision_tree = DecisionTreeRegressor()\ndecision_tree.fit(X_train, Y_train)\n\nacc_decision_tree_train = rmse_score(Y_train.values, decision_tree.predict(X_train))\nacc_decision_tree_validate = rmse_score(Y_validate.values, decision_tree.predict(X_validate))\n\nprint(\"RMSE on train: \", acc_decision_tree_train)\nprint(\"RMSE on validation: \", acc_decision_tree_validate)","fa4a5c1a":"decision_tree_on_log = DecisionTreeRegressor()\ndecision_tree_on_log.fit(X_train, Y_train_log)\n\nacc_decision_tree_train = rmse_score(Y_train.values, np.exp(decision_tree_on_log.predict(X_train)))\nacc_decision_tree_validate = rmse_score(Y_validate.values, np.exp(decision_tree_on_log.predict(X_validate)))\n\nprint(\"RMSE on train: \", acc_decision_tree_train)\nprint(\"RMSE on validation: \", acc_decision_tree_validate)","2750728e":"random_forest = RandomForestRegressor(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nacc_random_forest_train = rmse_score(Y_train.values, random_forest.predict(X_train))\nacc_random_forest_validate = rmse_score(Y_validate.values, random_forest.predict(X_validate))\n\nprint(\"RMSE on train: \", acc_random_forest_train)\nprint(\"RMSE on validation: \", acc_random_forest_validate)","170cc4ca":"random_forest_on_log = RandomForestRegressor(n_estimators=100)\nrandom_forest_on_log.fit(X_train, Y_train_log)\n\nacc_random_forest_train = rmse_score(Y_train.values, np.exp(random_forest_on_log.predict(X_train)))\nacc_random_forest_validate = rmse_score(Y_validate.values, np.exp(random_forest_on_log.predict(X_validate)))\n\nprint(\"RMSE on train: \", acc_random_forest_train)\nprint(\"RMSE on validation: \", acc_random_forest_validate)","aff27363":"xgboost = XGBRegressor()\nxgboost.fit(X_train, Y_train)\n\nacc_xgboost_train = rmse_score(Y_train.values, xgboost.predict(X_train))\nacc_xgboost_validate = rmse_score(Y_validate.values, xgboost.predict(X_validate))\n\nprint(\"RMSE on train: \", acc_xgboost_train)\nprint(\"RMSE on validation: \", acc_xgboost_validate)","1952141a":"xgboost_on_log = XGBRegressor()\nxgboost_on_log.fit(X_train, Y_train_log)\n\nacc_xgboost_train = rmse_score(Y_train.values, np.exp(xgboost_on_log.predict(X_train)))\nacc_xgboost_validate = rmse_score(Y_validate.values, np.exp(xgboost_on_log.predict(X_validate)))\n\nprint(\"RMSE on train: \", acc_xgboost_train)\nprint(\"RMSE on validation: \", acc_xgboost_validate)","56885003":"models = pd.DataFrame({\n    'Model': ['Linear Regression',\n              'Lasso Regression',\n              'Ridge Regression',\n              'ElasticNet Regression',\n              'Random Forest', \n              'Decision Tree',\n              'XGBoost'],\n    'Train Score': [acc_lin_train,\n                    acc_lasso_train,\n                    acc_ridge_train,\n                    acc_en_train,\n                    acc_random_forest_train, \n                    acc_decision_tree_train,\n                    acc_xgboost_train],\n    'Cross-Validation Score': [acc_lin_validate,\n                    acc_lasso_validate,\n                    acc_ridge_validate,\n                    acc_en_validate,\n                    acc_random_forest_validate, \n                    acc_decision_tree_validate,\n                    acc_xgboost_validate]})\nmodels.sort_values(by='Cross-Validation Score', ascending=True)","ecae5e98":"import operator\n\ndef find_best_alpha(alphas, model):\n    rmse = [rmse_score(Y_validate.values, np.exp(model(alpha=alpha).fit(X_train, Y_train_log).predict(X_validate))) \n            for alpha in alphas]\n    cv_ridge = pd.Series(rmse, index = alphas)\n    cv_ridge.plot(title = \"On Validation set\")\n    plt.xlabel(\"alpha\")\n    plt.ylabel(\"rmse\")\n\n    min_index, min_value = min(enumerate(rmse), key=operator.itemgetter(1))\n\n    print(\"Minimum RMSE =\", min_value, \"found at alpha =\", alphas[min_index])\n\n    return (alphas[min_index], min_value)","d31d3cee":"(ridge_alpha, _) = find_best_alpha(alphas=[0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300],\n                                   model=Ridge)","3605ec21":"(elasticNet_alpha, _) = find_best_alpha(alphas=[0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10, 30, 100],\n                                        model=ElasticNet)","1e51c4e7":"(lasso_alpha, _) = find_best_alpha(alphas=[0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10, 30, 100, 300, 1000],\n                                   model=Lasso)","e441a371":"from sklearn.model_selection import RandomizedSearchCV","15689a41":"ridge_random_grid = {'alpha': [int(x) for x in np.linspace(start=0.5*ridge_alpha, stop=1.5*ridge_alpha, num=10)],\n                     'fit_intercept': [True],\n                     'normalize': [True, False],\n                     'copy_X': [True],\n                     'max_iter': [10, 30, 100, 300, 1000, 3000, 10000],\n                     'tol': [0.0001],\n                     'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n                     'random_state': [42]}\nridge = Ridge()\nridge_random = RandomizedSearchCV(estimator = ridge, \n                                  param_distributions = ridge_random_grid, \n                                  n_iter = 100, \n                                  verbose=1, \n                                  random_state=42, \n                                  n_jobs = -1)\n\nridge_random.fit(X_train, Y_train_log)","186b8445":"print(rmse_score(Y_validate.values, ridge_random.best_estimator_.predict(X_validate)))","0dcc9d20":"ridge_random.best_params_","7870e220":"elasticNet_random_grid = {'alpha': [int(x) for x in np.linspace(start=0.5*elasticNet_alpha, \n                                                                stop=1.5*elasticNet_alpha, \n                                                                num=10)],\n                          'l1_ratio': [0.01, 0.03, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n                          'fit_intercept': [True],\n                          'normalize': [True, False],\n                          'precompute': [True, False],\n                          'copy_X': [True],\n                          'max_iter': [10, 30, 100, 300, 1000, 3000, 10000],\n                          'tol': [0.0001],\n                          'warm_start': [True, False],\n                          'positive': [True, False],\n                          'selection': ['cyclic', 'random'],\n                          'random_state': [42]}\n\nelastic_net = ElasticNet()\nelastic_net_random = RandomizedSearchCV(estimator = elastic_net, \n                                  param_distributions = elasticNet_random_grid, \n                                  n_iter = 100, \n                                  verbose=1, \n                                  random_state=42, \n                                  n_jobs = -1)\n\nelastic_net_random.fit(X_train, Y_train_log)","0de4bb58":"print(rmse_score(Y_validate.values, elastic_net_random.best_estimator_.predict(X_validate)))","8413b363":"elastic_net_random.best_params_","b5cd694a":"lasso_random_grid = {'alpha': [int(x) for x in np.linspace(start=0.5*lasso_alpha, \n                                                                stop=1.5*lasso_alpha, \n                                                                num=10)],\n                     'fit_intercept': [True],\n                     'normalize': [True, False],\n                     'precompute': [True, False],\n                     'copy_X': [True],\n                     'max_iter': [10, 30, 100, 300, 1000, 3000, 10000],\n                     'tol': [0.0001],\n                     'warm_start': [True, False],\n                     'positive': [True, False],\n                     'selection': ['cyclic', 'random'],\n                     'random_state': [42]}\n\nlasso = Lasso()\nlasso_random = RandomizedSearchCV(estimator = lasso, \n                                  param_distributions = lasso_random_grid, \n                                  n_iter = 100, \n                                  verbose=1, \n                                  random_state=42, \n                                  n_jobs = -1)\n\nlasso_random.fit(X_train, Y_train_log)","4b9932d1":"print(rmse_score(Y_validate.values, lasso_random.best_estimator_.predict(X_validate)))","73186a3f":"lasso_random.best_params_","6e0973eb":"feature_importances = pd.DataFrame({\n    'Features': X_train.columns,\n    'Importances': random_forest.feature_importances_\n})\nfeature_importances = feature_importances.sort_values(by='Importances', ascending=False)","730c3e1d":"best = {}\n\nfor feature_count in range(10, 210, 10):\n    best[feature_count] = {'feature_list': list(feature_importances['Features'][:feature_count])}","5d320ef8":"for feature_set_size, features in best.items():\n    train_accuracy_sum = 0\n    validation_accuracy_sum = 0\n    \n    nattempts = 3\n    for attempt in range(nattempts):\n        \n        X_train_best = X_train[features['feature_list']]\n        X_validate_best = X_validate[features['feature_list']]\n        X_test_best = X_test[features['feature_list']]\n\n        random_forest = RandomForestRegressor(n_estimators=100)\n        random_forest.fit(X_train_best, Y_train)\n\n        acc_random_forest_train = rmse_score(Y_train.values, random_forest.predict(X_train_best))\n        acc_random_forest_validate = rmse_score(Y_validate.values, random_forest.predict(X_validate_best))\n        \n        train_accuracy_sum += acc_random_forest_train\n        validation_accuracy_sum += acc_random_forest_validate\n\n    features['train_accuracy_avg'] = train_accuracy_sum\/nattempts\n    features['validation_accuracy_avg'] = validation_accuracy_sum\/nattempts\n    \n    print(\"No. of features:\", feature_set_size)\n    print(\"Train accuracy: \", features['train_accuracy_avg'])\n    print(\"Cross-Validation accuracy: \", features['validation_accuracy_avg'])","1a05bb8a":"feature_sizes_sorted = sorted(best.keys(), \n                              key=lambda feature_count: best[feature_count]['validation_accuracy_avg'])\naccuracies = [best[nfeatures]['validation_accuracy_avg'] for nfeatures in feature_sizes_sorted]\n\nfor nfeatures, accuracies in zip(feature_sizes_sorted, accuracies):\n    print(nfeatures, \": \", round(accuracies, 5))","dc044ac8":"Y_validate_lasso_in = (ridge_random.best_estimator_.predict(X_validate) +\n                         elastic_net_random.best_estimator_.predict(X_validate) +\n                         lasso_random.best_estimator_.predict(X_validate) +\n                         xgboost.predict(X_validate)) \/ 4","b7ff4829":"Y_validate_lasso_out = (ridge_random.best_estimator_.predict(X_validate) +\n                         elastic_net_random.best_estimator_.predict(X_validate) +\n                         #lasso_random.best_estimator_.predict(X_validate) +\n                         xgboost.predict(X_validate)) \/ 3","9e24164f":"rmse_score(Y_validate.values, Y_validate_lasso_in)","a9458c37":"rmse_score(Y_validate.values, Y_validate_lasso_out)","b857d2e6":"final_train = all_data[:ntrain+nvalidate][:]\nfinal_test = all_data[ntrain+nvalidate:][:]\n\nfinal_train_X = final_train.drop('SalePrice', axis= 1)\nfinal_train_Y = final_train['SalePrice']\nfinal_train_Y_log = SalePriceLog\nfinal_test_X = final_test.drop('SalePrice', axis=1)","94fc67ba":"# make sure there are no missing features\nmissing_features(final_train_X, final_train_X.columns)","5b9e343a":"ridge_random_grid = {'alpha': [int(x) for x in np.linspace(start=0.5*ridge_alpha, stop=1.5*ridge_alpha, num=10)],\n                     'fit_intercept': [True],\n                     'normalize': [True, False],\n                     'copy_X': [True],\n                     'max_iter': [10, 30, 100, 300, 1000, 3000, 10000],\n                     'tol': [0.0001],\n                     'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n                     'random_state': [42]}\nridge = Ridge()\nridge_random = RandomizedSearchCV(estimator = ridge, \n                                  param_distributions = ridge_random_grid, \n                                  n_iter = 100, \n                                  verbose=1, \n                                  random_state=42, \n                                  n_jobs = -1)\n\nridge_random.fit(final_train_X, final_train_Y_log)","bd6385ca":"elasticNet_random_grid = {'alpha': [int(x) for x in np.linspace(start=0.5*elasticNet_alpha, \n                                                                stop=1.5*elasticNet_alpha, \n                                                                num=10)],\n                          'l1_ratio': [0.01, 0.03, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n                          'fit_intercept': [True],\n                          'normalize': [True, False],\n                          'precompute': [True, False],\n                          'copy_X': [True],\n                          'max_iter': [10, 30, 100, 300, 1000, 3000, 10000],\n                          'tol': [0.0001],\n                          'warm_start': [True, False],\n                          'positive': [True, False],\n                          'selection': ['cyclic', 'random'],\n                          'random_state': [42]}\n\nelastic_net = ElasticNet()\nelastic_net_random = RandomizedSearchCV(estimator = elastic_net, \n                                  param_distributions = elasticNet_random_grid, \n                                  n_iter = 100, \n                                  verbose=1, \n                                  random_state=42, \n                                  n_jobs = -1)\n\nelastic_net_random.fit(final_train_X, final_train_Y_log)","3e414a07":"xgboost = XGBRegressor()\nxgboost.fit(final_train_X, final_train_Y_log)","14c1250c":"rmse_score(final_train_Y.values, ridge_random.best_estimator_.predict(final_train_X))","e68880a7":"rmse_score(final_train_Y.values, elastic_net_random.best_estimator_.predict(final_train_X))","5db9ddf2":"rmse_score(final_train_Y.values, xgboost.predict(final_train_X))","0a0abdbe":"final_Y_pred = np.exp((ridge_random.best_estimator_.predict(final_test_X) +\n                   elastic_net_random.best_estimator_.predict(final_test_X) +\n                   xgboost.predict(final_test_X)) \/ 3)","5c2d54a7":"submission = pd.DataFrame({'Id': test_id,\n                           'SalePrice': final_Y_pred})\nsubmission.to_csv('submission.csv', index=False)","a59b8580":"Lets look at the number of unique values that each numeric feature takes","6c58b5cf":"This is what the new features look like:","c298ef3b":"Upon eyballing the graphs and looking at the correlation table, it seems that LowQualFinSF, 3SsnPorch, ScreenPorch, PoolArea, MiscVal, EnclosedPorch, BsmtFinSF2, LotFrontage, 2ndFlrSF by themselves don't have much correlation with the SalePrice.","d6705f3b":"I believe that the model will perform much better if I reduce the multicollinearity.\n\nUsing PCA could do the job but it would also create uncomprehensible features. I need to use some better alternative. Please comment below if you have one in mind..","ee0d1bc9":"## 1.4. Summary:","95ceab16":"### ElasticNet Regression","9fb57627":"### Adding polynomials of the top 10 most correlated features","0479a6aa":"__`GarageArea`__:\n\nNotice that GarageCars in this discrete variables list and GarageArea in continuous variables list, both convey the garage capacity. Since GarageCars has a higher correlation, we **drop GarageArea.**","db56ce91":"__`TotalBsmtSF + 1stFlrSF + 2ndFlrSF`__\n\nDefine a new feature - `TotalSF`","a2e4ad6d":"#### `MSSubClass`","d06eae07":"__`BsmtBath`__ = __`BsmtFullBath`__ + 0.5*__`BsmtHalfBath`__","31d764b9":"*Lasso Regression* -","42887bda":"## 1.2. Numerical features - *up close*","752e0ec6":"#### `Exterior1st` and `Exterior2nd`","91d6b32d":"### Handling multicollinearity","1f0599a8":"## Improving the models","ad8d72ad":"### Linear Regression","8a117301":"-------","ec8c2683":"**Divide the train data into `df_train` and `df_validate`.**","c381ffbd":"### Feature engineering","cf7a8e45":"Now train the final model on the entire training set.","e847ccec":" The features are more than 50% empty - `PoolQC`, `MiscFeature`, `Alley`, `Fence`. Let's see their value counts.\n","c5c348b9":"### Random Forest","49d304c2":"First, I will manually find the best `alpha` for the regularised regression models because that is the most important hyperparameter. \n\nThen, I will use `sklearn`'s `RandomizedSearchCV` to search for the best values of the remaining hyperparameters. `RandomizedSearchCV` chooses a possibly best combination of hyperparameters among a grid of hyperparameter ranges. It randomly samples combinations from the grid and returns the one that gave the best accuracy. ","ec752813":"**Log-transform of `SalePrice`**","7bba575a":"#### Transforming categorical features\n* Ordered numbering - for ordinal features\n* Some other numbering format - for nominal features\n\nThis way we help to prevent in embedding any false assumptions into our data because\n* With ordered features there is a sense of structure and order in the values that those features take (for eg. - Excellent > Good > Poor)\n* With nominal data if we use some order in the numbering, we embed false structure to our data ","1496f837":"## 2.3. Categorical features:\n### Preprocessing\n#### Filling out the missing values -","1d54124f":"*Ridge regression* -","9fa6fc2a":"# 1. Data exploration:","dbefbf75":"---------------","9363e02f":"## 1.1. Separate the numerical and categorical features","d6fafdc3":"__`Bath`__ = __`FullBath`__ + 0.5*__`HalfBath`__","320c0db3":"We notice that MSSubClass, OverallQual and OverallCond take numeric values but are actually categorical in nature.","edc5bc04":"### Creating newer features by combining existing ones","96648e5c":"## 1.3. Categorical features - *up close*:","0158db7d":"### XGBoost","ee4bc645":"And drop `TotalBsmtSF`, `1stFlrSF` and `2ndFlrSF`","96fe76b5":"### 1.2.1. Separating out the continuous and discrete features:","68144750":"#### `Functional`","1e510b45":"#### `GarageType`","0feb1ac6":"__`OpenPorchSF`__ + __`EnclosedPorchSF`__ + __`3SsnPorch`__ + __`ScreenPorch`__\n\nThere are 4 \"porch-area\" related features - OpenPorchSF, EnclosedPorchSF, 3SsnPorch, ScreenPorch.   \nOf these EnclosedPorch and 3SsnPorch have a small negative regression value while OpenPorchSF and ScreenPorch have small positive values.\n\nLets **combine them all and form a new feature**","8c6cf9be":"### Ridge Regression","fc05ff77":"[The detailed analysis](https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf) provided by the dataset author has helped in creating this kernel.  \nAlso [here's a link](https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock\/DataDocumentation.txt) to the detailed description of the dataset.\n\nThese kernels were of great help to me:\n\n* https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n* https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard \n* https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models\n* https:\/\/www.kaggle.com\/humananalog\/xgboost-lasso\n* https:\/\/www.kaggle.com\/poonaml\/house-prices-data-exploration-and-visualisation\n\nI am new to data science. It would be really awesome if you could tell me the mistakes and how I could improve the kernel in the comments below.","a5fe9b0e":"* `GarageYrBlt`: We fill the missing values with the corresponding `YearBuilt` values (assume that the garage was built along with the house)\n* `BsmtFullBath`: 0 (NaN probably means not present)\n* `BsmtHalfBath`: 0 (NaN probably means not present)\n* `GarageCars`: 0 (NaN probably means not present)","8c7c5a7d":"### Filling in missing values:","8887e473":"## 2.2. Discrete valued numeric features:","4d82c39c":"# 2. Preprocessing:\nWe handle the separate kinds of features in this order:  \n* First, the continuous valued numeric features\n* Second, discrete valued numeric features\n* Third, categorical features, i.e, both nominal and ordinal features together\n\nBut first, the author of the dataset recommends that we drop the data points with GrLivArea > 4000 as these are outliers. ","565af373":"### Fill in the missing values:","179f8ae9":"#### Finding the best `alpha` -","43d576eb":"For the final model, I am going to use those models that gave me an `RMS error < 0.13`i.e, __Ridge Regression__, __ElasticNet Regression__, __Lasso regression__ and the __XGBoost__ models.\n\nI have found, through trial and error, that stacking **_all those models except the Lasso model_**, gives the best results.","fecd8f1b":"## 2.4. Final feature engineering","455cc0bb":"Here's the approach that I take in this kernel:\n\n**1. Data Exploration -**\n    Here, I separate out the features as belonging to one of the 3 classes -\n    \n    * `num_cont_columns` for numerical continuous features\n    * `num_discrete_columns` for numerical discrete features\n    * `categ_columns` for categorical features\n       \n   I do this because there are different steps of preprocessing required for each of those features. By separating them, I am able to create a clean analysis of the data.\n\n**2. Preprocessing -**\n    As I said, I divide the preprocessing stage in 3 steps where I analyse-\n    \n     * Continuous valued numeric features\n     * Discrete valued numeric features\n     * And finally, the Categorical features\n        \n   In each of those steps, I do the missing values imputation and feature engineering as required.\n\n**3. Modelling -**\n\n   I will train the following regression models -\n    * Simple regression\n    * Ridge regression\n    * ElasticNet regression\n    * Lasso regression\n    \n   And the following ensemble models - \n    * Random Forest\n    * XGBoost\n    \n    Finally, I took the top 3 best performing models and stacked them up for the final submission model.","e2c398f2":"#### `SaleType`","da531bd4":"#### More preprocessing -","97a422d8":"__`PoolArea`__\n\nPoolArea looks like it is mostly zero.\n\nLets have a look..","83f71795":"#### Dropping features","ededf930":"**Separate the true `SalePrice` column from the rest of the data**","676dcd55":"## Final submission model","9e1c7163":"### Lasso Regression","5b47daab":"#### We change the date related features\n\nIt would be helpful if we could map the features that represent some date (like say, *1888*) to some small number (like say, *88*). \nThere are 3 year related features - YearBuilt, YearRemodAdd, GarageYrBlt.\n","9ed7ea8e":"Points to note:\n* We should drop `PoolQC` as it is mostly empty and because we have already dropped `PoolArea`\n* The [feature description file](https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock\/DataDocumentation.txt) says that `MiscFeature`, `Alley` and `Fence` have a value `NA` to represent the absence of those features. Yet, no datapoint has that value. This probably means that during the collecting of data, the person left this column blank instead of filling `NA`. So, we should take the empty values to mean `NA`\n* `MiscFeature` is mostly just used to say the presence of Shed. So, we could replace it with a feature - `Shed`","f31e03bb":"No columns are so empty that we need to drop them (I choose a 50% mark). Here's how we fill missing values:\n\n* LotFrontage - *mean of the column*\n* MasVnrArea, GarageArea, TotalBsmtSF, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF - \n    *0 (NA probably means that the features is absent)*","3ab502a5":"The final predicted values - ","ae2fd45f":"*ElasticNet Regression* -","3296064a":"### Hyperparameter tuning:","e636c813":"It is highly skewed: the ratio of non-zero to zeros is less than 1\/200. We're better off **dropping PoolArea**","dd787c58":"#### `MSZoning`","6966dc72":"*Ridge Regression* -","63ae99df":"#### Improving `Random Forest` -\n\nThe `Random Forest` model achieves the best accuracy (of __86.24%__ when I ran it locally) on the cross-validation set. \n\n**Note: _The huge difference between the `Cross-Validation Score`s of `Random Forest` and `XGBoost` as compared to`Decision_Tree` and the fact that the `Decision Tree` model reached ~100% accuracy on Training data, highlights that it is easy to overfit a `Decision Tree`. The former 2 being an ensemble of `Decision Tree`s, are inherently good at avoiding overfitting._**\n\nNow, let's try to tweak the model to further improve it.\n\n**Reducing the number of features:**\n\nA general rule of thumb in Machine Learning is that the more features you have, the greater are the chances of your model overfitting the data. Extra features can decrease performance because they may \u201cconfuse\u201d the model by giving it irrelevant data that prevents it from learning the actual relationships.\n\nScikit-learn's `RandomForestRegressor` has a useful attribute - `feature_importances_` - which gives each feature a score based on how important it is in the prediction process.\n\nWe use it to extract the `n`-best features upon which we train our final model.\n\nWe find the value of `n` by iteratively training the model on number of features in the multiple of 10s. Then, we take at the value which returned the best score on the validation set.","60f57f59":"### Decision Tree","fe73a9d9":"There are a total of 80 unique features in the dataset.\n\nIn the detailed description that the dataset, the author divided the features into 4 types:\n\n* Numerical:\n   3. Discrete (14)\n   4. Continuous (20)\n* Categorical:\n   1. Nominal (23)\n   2. Ordinal (23)\n\nTotal no. of Categorical features = 23+23 = 46   \nTotal no. of Numerical features = 14 + 20 = 34","9a6d7661":"*Lasso Regression* -","c4a26662":"# 3. Modelling","45b9027b":"## 2.1. Continuous valued numeric features:","f9dd3317":"#### Using `RandomizedSearchCV`","93d35188":"Okay, the best validation set RMSE of `0.13736`. This is almost the same as the accuracy I got using just the default model.\n\nWell, it was a promising avenue to look into. So, its okay that it did not improve the results.","9178c81b":"### Dropping uncorrelated features\n\nWe drop the features which have a `|correlation score| < 0.1`","48d6e339":"After eyeballing the previous output, we can observe that all the features that take on less than 30 unique values throughout the dataset are discrete. Let's separate them out.","6063458c":"*ElasticNet regression* -"}}