{"cell_type":{"f71e0bad":"code","195f84c0":"code","abc5e957":"code","e163494d":"code","c0f7589f":"code","976683fb":"code","44f44a73":"code","24230500":"code","6af00714":"code","cd64d480":"code","63f619d5":"code","2fc87f8f":"code","90aeee4a":"code","b0f20a69":"code","ab2a9f95":"code","ed1a23a3":"code","95d1c7bc":"code","8dee0c85":"code","4e4be3f1":"code","60a16a53":"code","954a8b38":"code","3e58adef":"code","f0555e7d":"code","998c9a8a":"code","a41a0135":"code","32508bbd":"code","abc7d5b4":"code","bfea6110":"code","a42ba2f8":"markdown","8fdfdc6b":"markdown","630702c2":"markdown","59b76da5":"markdown","e4753ec9":"markdown","cbf55da0":"markdown","88d6da5b":"markdown","9942a018":"markdown","165e473d":"markdown","d71936b5":"markdown","ddeb1aa9":"markdown","d623db49":"markdown","bd03e8aa":"markdown","e64217db":"markdown","24914cba":"markdown","69fe4440":"markdown","8e660a3d":"markdown","a3cb2685":"markdown","eb2b4b9f":"markdown","6d83e6a8":"markdown","024465db":"markdown","034642b9":"markdown","1fdeab1b":"markdown","fcf93afa":"markdown"},"source":{"f71e0bad":"import pandas as pd\n\ndata = pd.read_csv('..\/input\/257k-gaiadr2-sources-with-photometry.csv', dtype={'source_id': str})","195f84c0":"len(data)","abc5e957":"should_remove_set = set(pd.read_csv('..\/input\/257k-gaiadr2-should-remove.csv', dtype={'source_id': str})['source_id'])","e163494d":"data = data[~data['source_id'].isin(should_remove_set)]\ndata.reset_index(inplace=True, drop=True)","c0f7589f":"len(data)","976683fb":"assert len(data) == len(set(data['source_id']))","44f44a73":"import inspect\n\npd_concat_argspec = inspect.getfullargspec(pd.concat)\npd_concat_has_sort = 'sort' in pd_concat_argspec.args\n\ndef pd_concat(frames):\n    # Due to Pandas versioning issue\n    new_frame = pd.concat(frames, sort=False) if pd_concat_has_sort else pd.concat(frames)\n    new_frame.reset_index(inplace=True, drop=True)\n    return new_frame","24230500":"import types\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler \n\nnp.random.seed(201808011)\n\ndef model_results(data_frame, label_extractor, var_extractor, trainer_factory, id_column='source_id', n_splits=2, n_runs=3, scale=False, max_n_training=None):\n    '''\n    Returns a frame with source_id, response and residual columns, with the same ordering as data_frame.\n    '''\n    sum_series = pd.Series([0] * len(data_frame))\n    for r in range(n_runs):\n        shuffled_frame = data_frame.sample(frac=1)\n        shuffled_frame.reset_index(inplace=True, drop=True)\n        response_frame = pd.DataFrame(columns=[id_column, 'response'])\n        kf = KFold(n_splits=n_splits)\n        for train_idx, test_idx in kf.split(shuffled_frame):\n            train_frame = shuffled_frame.iloc[train_idx]\n            if max_n_training is not None:\n                train_frame = train_frame.sample(max_n_training)\n            test_frame = shuffled_frame.iloc[test_idx]\n            train_labels = label_extractor(train_frame) if isinstance(label_extractor, types.FunctionType) else train_frame[label_extractor]\n            train_vars = var_extractor(train_frame)\n            test_vars = var_extractor(test_frame)\n            if scale:\n                scaler = StandardScaler()  \n                scaler.fit(train_vars)\n                train_vars = scaler.transform(train_vars)  \n                test_vars = scaler.transform(test_vars) \n            trainer = trainer_factory()\n            fold_model = trainer.fit(train_vars, train_labels)\n            test_responses = fold_model.predict(test_vars)\n            test_id = test_frame[id_column]\n            assert len(test_id) == len(test_responses)\n            fold_frame = pd.DataFrame({id_column: test_id, 'response': test_responses})\n            response_frame = pd_concat([response_frame, fold_frame])\n        response_frame.sort_values(id_column, inplace=True)\n        response_frame.reset_index(inplace=True, drop=True)\n        assert len(response_frame) == len(data_frame), 'len(response_frame)=%d' % len(response_frame)\n        sum_series += response_frame['response']\n    cv_response = sum_series \/ n_runs\n    assert len(cv_response) == len(data_frame)\n    sorted_result = pd.DataFrame({\n        id_column: np.sort(data_frame[id_column].values), \n        'response': cv_response})\n    data_frame_partial = pd.DataFrame({id_column: data_frame[id_column]})\n    merged_frame = pd.merge(data_frame_partial, sorted_result, how='inner', on=id_column, sort=False)\n    data_frame_labels = label_extractor(data_frame) if isinstance(label_extractor, types.FunctionType) else data_frame[label_extractor]\n    merged_frame['residual'] = data_frame_labels - merged_frame['response']\n    assert len(merged_frame) == len(data_frame)\n    return merged_frame","6af00714":"import math\nimport scipy.stats as stats\n\ndef print_evaluation(data_frame, label_column, response_frame):\n    _response = response_frame['response']\n    _label = label_column(data_frame) if isinstance(label_column, types.FunctionType) else data_frame[label_column]\n    _error = _label - _response\n    assert sum(response_frame['residual'] == _error) == len(data_frame)\n    _rmse = math.sqrt(np.sum(_error ** 2) \/ len(data_frame))\n    _correl = stats.pearsonr(_response, _label)[0]\n    print('RMSE: %.4f | Correlation: %.4f' % (_rmse, _correl,), flush=True)","cd64d480":"TEST_TARGET_COLUMN = 'gsc23_b_mag'\nMAX_N_TRAINING = 50000","63f619d5":"def get_gaia_vars(data_frame):\n    var_list = [data_frame['phot_g_mean_mag'], data_frame['phot_bp_mean_mag'], data_frame['phot_rp_mean_mag']]\n    return np.transpose(var_list)","2fc87f8f":"def get_position_vars(data_frame):\n    ra_rad = np.deg2rad(data_frame['ra'])\n    dec_rad = np.deg2rad(data_frame['dec'])    \n    ra_sin = np.sin(ra_rad)\n    ra_cos = np.cos(ra_rad)\n    dec_sin = np.sin(dec_rad)\n    dec_cos = np.cos(dec_rad)\n    feature_list = [ra_sin, ra_cos, dec_sin, dec_cos]    \n    return np.transpose(feature_list)        ","90aeee4a":"from sklearn import linear_model\n\ndef get_linear_trainer():\n    return linear_model.LinearRegression()","b0f20a69":"from sklearn.neural_network import MLPRegressor\n\ndef get_mag_res_trainer():\n    return MLPRegressor(hidden_layer_sizes=(80), max_iter=500, alpha=0.02, random_state=np.random.randint(1, 10000))","ab2a9f95":"def model_position_bias(data_frame, target_column, var_extractor, verbose=False):\n    # Model target magnitude as a function of Gaia magnitudes\n    m_results = model_results(data, target_column, var_extractor, get_linear_trainer)\n    if verbose:\n        print_evaluation(data_frame, target_column, m_results)\n    m_frame = pd.DataFrame({\n        'source_id': m_results['source_id'],\n        'residual': m_results['residual'],\n        'ra': data_frame['ra'],\n        'dec': data_frame['dec']\n    })\n    # Model m_frame['residual'] as a function of position variables.\n    m_res_results = model_results(m_frame, 'residual', get_position_vars, \n                                  get_mag_res_trainer, n_runs=4, scale=True, max_n_training=MAX_N_TRAINING)       \n    if verbose:\n        print_evaluation(m_frame, 'residual', m_res_results)\n    return m_res_results","ed1a23a3":"test_result = model_position_bias(data, TEST_TARGET_COLUMN, get_gaia_vars, verbose=True)","95d1c7bc":"sample_indexes = np.random.choice(len(data), 10000, replace=False)\nassert len(sample_indexes) == len(set(sample_indexes))","8dee0c85":"data_sample = data.iloc[sample_indexes]","4e4be3f1":"test_result_sample = test_result.iloc[sample_indexes]","60a16a53":"import matplotlib.pyplot as plt","954a8b38":"%%html\n<!-- Allow bigger output cells -->\n<style>\n.output_wrapper, .output {\n    height:auto !important;\n    max-height: 1500px;\n}\n<\/style>","3e58adef":"import matplotlib.cm as cm\nfrom matplotlib.colors import LinearSegmentedColormap\n\nplt.rcParams['figure.figsize'] = (14, 7)\nplt.scatter(data_sample['ra'], data_sample['dec'], s=2, c=test_result_sample['response'].values, cmap=cm.coolwarm_r)\nplt.title('Positional bias of %s relative to Gaia DR2' % TEST_TARGET_COLUMN)\nplt.xlabel('Right Ascension (degrees)')\nplt.ylabel('Declination (degrees)')\nplt.colorbar()\nplt.show()","f0555e7d":"CORRECTION_PREFIX = 'adj_'","998c9a8a":"def correct_mag_column(data_frame, column_name, verbose=False):\n    column_result = model_position_bias(data_frame, column_name, get_gaia_vars, verbose=verbose)\n    corrected_mags = data_frame[column_name] - column_result['response']\n    if verbose:\n        correl = stats.pearsonr(data_frame[column_name], corrected_mags)[0]\n        print('Replacing column %s | Correlation: %.4f' % (column_name, correl,))    \n    data_frame[CORRECTION_PREFIX + column_name] = corrected_mags","a41a0135":"mag_columns = ['allwise_w1', 'allwise_w2', 'allwise_w3', 'allwise_w4', \n        'gsc23_v_mag', 'gsc23_b_mag',\n        'ppmxl_b1mag', 'ppmxl_b2mag', 'ppmxl_r1mag', 'ppmxl_imag', \n        'tmass_j_m', 'tmass_h_m', 'tmass_ks_m', \n        'tycho2_bt_mag', 'tycho2_vt_mag']","32508bbd":"for mag_column in mag_columns:\n    correct_mag_column(data, mag_column, verbose=False)\nprint('Done.')","abc7d5b4":"_ = model_position_bias(data, CORRECTION_PREFIX + TEST_TARGET_COLUMN, get_gaia_vars, verbose=True)","bfea6110":"data.to_csv('254k-gaiadr2-sources-adj-mags.csv')","a42ba2f8":"## Testing position bias modeling\nThere are 15 non-Gaia magnitude columns we will be correcting. To illustrate how it will work, we'll pick one of them and do a run-through. Change the TEST_TARGET_COLUMN variable below to test the process with a different column.","8fdfdc6b":"## Visualization of bias\nCross-database error is considerably larger than the biases we'd like to model, so the bias is not easy to visualize directly. Looking at outliers is one way to notice the problem. It's easiest to visualize the bias by looking at residual model responses. They can be thought of as regional residual averages.\n\nWe'll sample 10 thousand stars and put them in scatter charts. Color (red to blue) will be used to depict the residual value of each star.","630702c2":"Let's define a function that executes the model pipeline. The function will return a frame containing the cross-validated *response* of the residual model. That *response* series is what we will use to correct magnitudes.","59b76da5":"Once we have trained the linear model, we will also want to model its residuals. The presumption is that residuals should not depend on position. If they do, a correction is warranted.\n\nThe position variables of the residual model will be sine and cosine of Right Ascension and Declination. Why not use Right Ascension and Declination directly? The variable space is not actually shaped like a rectangle. It's essentially the surface of a sphere, so we use 4 variables instead. It's theoretically more sound, and it does work better.","e4753ec9":"Finally, we'll execute a routine that will call the correction function for each column. This can take several minutes to run.","cbf55da0":"## Correction routine\nThe following function takes a magnitude column and creates a new column with 'adj_' as prefix. The new column will contain the original magnitude values minus the response of the position-based residual model.","88d6da5b":"The following function can be used to train regression models by averaging out multiple runs of k-fold cross-validation. ","9942a018":"## Introduction\nDatabases of stellar magnitude that use different photometric filters are useful in determining the spectral characteristics of stars. If different databases have position-based biases relative to one another, then stellar characteristics \u2013 as well as error \u2013 will appear to depend spuriously on a star's position in the sky.\n\nWe'll use machine learning to explore these magnitude biases that depend on position and see if we can correct them so they are in line with Gaia DR2.","165e473d":"## Sanity check\nWhat if we apply the model pipeline to one of the adjusted columns?","d71936b5":"The following function is a trainer factory that creates linear regressors.","ddeb1aa9":"Each target magnitude column will be modeled as a linear function of the 3 Gaia magnitude columns returned by the following function.","d623db49":"## Modeling helper functions\nThis is just boilerplate:","bd03e8aa":"The evaluation results above correspond to the linear model, and the position-based residual model, in that order. \n\nIf we subtract the *response* column of *test_result* from the target magnitude column, its position bias would be substantially corrected.","e64217db":"The dataset contains some duplicates and apparent Gaia systematics which we will remove, based on results from [a prior kernel](https:\/\/www.kaggle.com\/solorzano\/removal-of-gaia-dr2-stars-with-apparent-systematic).","24914cba":"We'll also define a function we can use to evaluate the results produced by the *model_results* function.","69fe4440":"## Data\nWe will use a [dataset](https:\/\/www.kaggle.com\/solorzano\/257k-gaia-dr2-stars) containing 257K Gaia DR2 stars, with cross-identified photometry from GSC 2.3, PPMXL, 2MASS, Tycho2 and AllWISE. ","8e660a3d":"## Acknowledgments\n\nThis work has made use of data from the European Space Agency (ESA) mission Gaia (https:\/\/www.cosmos.esa.int\/gaia), processed by the Gaia Data Processing and Analysis Consortium (DPAC, https:\/\/www.cosmos.esa.int\/web\/gaia\/dpac\/consortium). Funding for the DPAC has been provided by national institutions, in particular the institutions participating in the Gaia Multilateral Agreement.","a3cb2685":"Note also that Gaia DR2 seems to have its own positional biases, which have not been corrected here.","eb2b4b9f":"## Overview\nDatabases of stellar magnitude have a position-dependent bias relative to Gaia DR2. We first train linear models of Gaia magnitudes vs. others, and then we model the cross-validated residuals of the linear models as a function of position, using a Neural Network.  Magnitudes from AllWISE, GSC 2.3, PPMXL, 2MASS and Tycho2 are corrected accordingly. Results are made available in the output tab.","6d83e6a8":"These are all the magnitude columns we will be correcting:","024465db":"## Output\nThe entire dataset will be written to the output tab. It contains the new adjusted magnitude columns, but we're keeping the original ones too. Keep in mind that the new columns add some noise and could even carry some information loss.","034642b9":"For magnitude residuals, here we have a factory of Neural Network regressors:","1fdeab1b":"Let's try out the function with our test magnitude column. ","fcf93afa":"What the second *correlation* value tells us is that only a small amount of positional bias remains. Trying to further correct for it could add noise unnecessarily."}}