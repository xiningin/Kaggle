{"cell_type":{"4afc867d":"code","571cb9d7":"code","fdad499c":"code","3d991cec":"code","faa8c234":"code","bd532093":"code","74f26588":"code","cb619074":"code","bd8d402c":"code","1d82387a":"code","01776c69":"code","c9713e97":"code","5ec28087":"code","a3ed4d92":"code","674064ce":"code","6a02d330":"code","c3c4ec1d":"code","a705c6df":"code","165d0857":"code","3c93f658":"code","cdc81d9b":"code","48bf0ede":"code","b401e94e":"code","dc73e119":"code","8452074d":"code","3755ea5f":"code","69801a03":"code","7c79f521":"code","d64cd764":"code","8d27a9e2":"code","414dd906":"code","afc563b1":"code","67002730":"code","342e1f85":"code","62352788":"code","019f88bb":"code","f5f561f6":"code","4044bdd2":"code","1b7dffe1":"code","e069c8c7":"code","d9ebfd21":"code","205c500d":"code","1a5c66ef":"code","63d25351":"code","e08ed32b":"code","9e45635b":"code","dddedd27":"code","3c229196":"code","266015e0":"code","eef3f740":"code","63c67a9f":"markdown","137f1845":"markdown","8d3680ee":"markdown","0fe680df":"markdown","ad406083":"markdown","db21844a":"markdown","3e826c15":"markdown","31b89528":"markdown","4b383eae":"markdown","91e272e3":"markdown","24d40876":"markdown","1b949e02":"markdown","5f3cdc6e":"markdown","bc1b59f6":"markdown","060c0bfd":"markdown","2c56648f":"markdown","19664852":"markdown"},"source":{"4afc867d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \n%matplotlib inline\nwarnings.filterwarnings('ignore')","571cb9d7":"df=pd.read_csv('..\/input\/wine-quality\/winequalityN.csv')","fdad499c":"df.head()","3d991cec":"df.describe()","faa8c234":"df.dtypes","bd532093":"df.isnull().sum()","74f26588":"\nfor cols,value in df.items():\n if cols!='type':\n  df[cols]=df[cols].fillna(df[cols].mean())\n","cb619074":"df.isnull().sum()","bd8d402c":" df.groupby('type')[['quality']].mean()","1d82387a":" df.groupby('alcohol')[['quality']].mean()","01776c69":"alco=pd.cut(df['alcohol'],[8,10,12,14])\ndf.pivot_table('quality',['type',alco])","c9713e97":"fig, ax = plt.subplots(ncols=6, nrows=2, figsize=(20,10))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df.items():\n    if col != 'type':\n        sns.boxplot(y=col, data=df, ax=ax[index])\n        index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)","5ec28087":"fig, ax = plt.subplots(ncols=6, nrows=2, figsize=(20,10))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df.items():\n    if col != 'type':\n        sns.distplot(value, ax=ax[index])\n        index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)","a3ed4d92":"from scipy import stats\ndf['free sulfur dioxide']=stats.boxcox(df['free sulfur dioxide'])[0]\n","674064ce":"df['density']=stats.boxcox(df['density'])[0]","6a02d330":"sns.distplot(df['free sulfur dioxide'])","c3c4ec1d":"sns.distplot(df['density'])","a705c6df":"sns.countplot(df['quality'])","165d0857":"corr=df.corr()\nplt.figure(figsize=(20,15))\nsns.heatmap(corr, annot=True, cmap='coolwarm')","3c93f658":"x=df.drop(columns=['type','quality'])","cdc81d9b":"x.shape\nx.dtypes","48bf0ede":"y=df['quality']\ny1=df['quality']","b401e94e":"y.value_counts()","dc73e119":"y.dtypes","8452074d":"from imblearn.over_sampling import SMOTE\noversample = SMOTE(k_neighbors=4)\nx, y = oversample.fit_resample(x, y)","3755ea5f":"y.value_counts()","69801a03":"print(x.shape)\nprint(y.shape)","7c79f521":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x, y, test_size=0.2, random_state=0)","d64cd764":"##SCALING THE DATA SET\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nx_train =sc.fit_transform(x_train)\nx_test=sc.fit_transform(x_test)\n","8d27a9e2":"def models(x_train, y_train,x_test,y_test):\n\n\n  ## KNN\n\n  from sklearn.neighbors import KNeighborsClassifier\n  knn= KNeighborsClassifier(n_neighbors=5, metric='minkowski',p=2)\n  knn.fit(x_train, y_train)\n\n  ##SVC(LINEAR)\n\n  from sklearn.svm import SVC\n  svc_lin=SVC(kernel='linear',random_state=0)\n  svc_lin.fit(x_train,y_train)\n\n  ##svc(rbf kernel)\n\n  from sklearn.svm import SVC\n  svc_rbf=SVC(kernel='rbf',random_state=0)\n  svc_rbf.fit(x_train,y_train)\n\n  ##gaussianNB\n\n  from sklearn.naive_bayes import GaussianNB\n  gb=GaussianNB()\n  gb.fit(x_train,y_train)\n\n  ##DECSION TREE\n\n  from sklearn.tree import DecisionTreeClassifier\n  tree=DecisionTreeClassifier(criterion= 'entropy' , random_state=0)\n  tree.fit(x_train,y_train)\n\n  ##Randomforestclassifier\n\n  from sklearn.ensemble import RandomForestClassifier\n  forest=RandomForestClassifier(n_estimators=10, criterion = 'entropy', random_state=0)\n  forest.fit(x_train,y_train)\n    \n  ##EXTRA TREE CLASSIFIER\n\n  from sklearn.ensemble import ExtraTreesClassifier\n  extra = ExtraTreesClassifier()\n  extra.fit(x_train,y_train)\n\n  print('[0]KNN  ACCURACY: ',knn.score(x_test,y_test))\n  print('[1]SVC(LINEAR) ACCURACY: ',svc_lin.score(x_test,y_test))\n  print('[2] SVC_RBF ACCURACY: ',svc_rbf.score(x_test,y_test))\n  print('[3] GAUSSIAN NB ACCURACY: ',gb.score(x_test,y_test))\n  print('[4]DECCISION TREE ACCURACY: ',tree.score(x_test,y_test))\n  print('[5]RANDOM FOREST ACCURACY: ',forest.score(x_test,y_test))\n  print('[6] EXTRA TREE CLASSIFIER:',extra.score(x_test,y_test))\n  return knn,svc_lin,svc_rbf,gb,tree,forest,extra\n","414dd906":"model=models(x_train,y_train,x_test,y_test)","afc563b1":"df=df.drop(['type'],axis=1)","67002730":"df.columns","342e1f85":"forest=model[5]\nimportances=pd.DataFrame({'feature':df.iloc[:,0:11].columns, 'importance' : np.round(forest.feature_importances_,3)} )\nimportances=importances.sort_values('importance',ascending=False).set_index('feature')\nimportances","62352788":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model[5], random_state=1).fit(x_test, y_test)\neli5.show_weights(perm, feature_names = x.columns.tolist())","019f88bb":"x1=x_train\nx1=df.drop(columns=['quality','residual sugar'])\nx1.shape","f5f561f6":"y1=df['quality']","4044bdd2":"y1.value_counts()","1b7dffe1":"from imblearn.over_sampling import SMOTE\noversample = SMOTE(k_neighbors=4)\nx1, y1 = oversample.fit_resample(x1, y1)","e069c8c7":"y1.value_counts()","d9ebfd21":"x_train1,x_test1,y_train1,y_test1=train_test_split(x1, y1, test_size=0.2, random_state=0)","205c500d":"def models(x_train, y_train,x_test,y_test):\n\n\n  ## KNN\n\n  from sklearn.neighbors import KNeighborsClassifier\n  knn= KNeighborsClassifier(n_neighbors=5, metric='minkowski',p=2)\n  knn.fit(x_train, y_train)\n\n  ##SVC(LINEAR)\n\n  from sklearn.svm import SVC\n  svc_lin=SVC(kernel='linear',random_state=0)\n  svc_lin.fit(x_train,y_train)\n\n  ##svc(rbf kernel)\n\n  from sklearn.svm import SVC\n  svc_rbf=SVC(kernel='rbf',random_state=0)\n  svc_rbf.fit(x_train,y_train)\n\n  ##gaussianNB\n\n  from sklearn.naive_bayes import GaussianNB\n  gb=GaussianNB()\n  gb.fit(x_train,y_train)\n\n  ##DECSION TREE\n\n  from sklearn.tree import DecisionTreeClassifier\n  tree=DecisionTreeClassifier(criterion= 'entropy' , random_state=0)\n  tree.fit(x_train,y_train)\n\n  ##Randomforestclassifier\n\n  from sklearn.ensemble import RandomForestClassifier\n  forest=RandomForestClassifier(n_estimators=10, criterion = 'entropy', random_state=0)\n  forest.fit(x_train,y_train)\n    \n  ##EXTRA TREE CLASSIFIER\n\n  from sklearn.ensemble import ExtraTreesClassifier\n  extra = ExtraTreesClassifier()\n  extra.fit(x_train,y_train)\n\n  print('[0]KNN  ACCURACY: ',knn.score(x_test,y_test))\n  print('[1]SVC(LINEAR) ACCURACY: ',svc_lin.score(x_test,y_test))\n  print('[2] SVC_RBF ACCURACY: ',svc_rbf.score(x_test,y_test))\n  print('[3] GAUSSIAN NB ACCURACY: ',gb.score(x_test,y_test))\n  print('[4]DECCISION TREE ACCURACY: ',tree.score(x_test,y_test))\n  print('[5]RANDOM FOREST ACCURACY: ',forest.score(x_test,y_test))\n  print('[6] EXTRA TREE CLASSIFIER:',extra.score(x_test,y_test))\n  return knn,svc_lin,svc_rbf,gb,tree,forest,extra\n","1a5c66ef":"model=models(x_train,y_train,x_test,y_test)","63d25351":"from sklearn.model_selection import GridSearchCV,cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier","e08ed32b":"forest_eval = cross_val_score(estimator = model[5], X = x_train, y = y_train, cv = 5)\nforest_eval","9e45635b":"tree_eval = cross_val_score(estimator = model[6], X = x_train, y = y_train, cv = 4)\ntree_eval.mean()","dddedd27":"n_estimators = [int(x) for x in np.linspace(start = 10, stop = 150, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [2,300]\n# Minimum number of samples required to split a node\nmin_samples_split = [2,5]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 3]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]","3c229196":"param_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n","266015e0":"rf_Model = ExtraTreesClassifier()\nrf_Grid = GridSearchCV(estimator = rf_Model, param_grid = param_grid, cv = 4, verbose=2, n_jobs = 4)\nrf_Grid.fit(x_train, y_train)\nrf_Grid.best_params_\n","eef3f740":"rf_Grid.best_score_","63c67a9f":"# **HANDLING MISSING VALUES**","137f1845":"Only free sulfhur and total sulfur are little bit co-related","8d3680ee":"# **CO0RELATION MATRIX** ","0fe680df":"# **FEATURE  IMPORTANCE**","ad406083":"# **ACCURACY USING CROSS VALIDATION**","db21844a":"Free sulphur and density are not that normally distributed.So,trnasforming into normal distribution using boxcox","3e826c15":"**as we can see the permutation and feature importance of sulphates and resuidual suagr are relatively low in comparison with others so we can remove it to increase the accuracy of the model**","31b89528":"# **PERMUTATION IMPORTANCE**","4b383eae":"ACCURACY INCREASED FROM 88.01 TO 88.16","91e272e3":"# **IMBALANCE DATASET**","24d40876":"# **HYPERPARAMETER TUNING**","1b949e02":"# **EDA**","5f3cdc6e":"Over sampling to handle imbalance data.","bc1b59f6":"# ANY SUGGESTIONS WILL BE APPRICIATED.PLEASE DO UPVOTE IF U LIKED IT.","060c0bfd":"# **MODEL TRAINING**","2c56648f":"Oversampling to handle imbalance dataset","19664852":"# CROSS CHECKING BY TRAINING MODELS"}}