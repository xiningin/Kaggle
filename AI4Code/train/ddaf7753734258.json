{"cell_type":{"7f4a95f3":"code","e0165983":"code","56c8a1f7":"code","96679624":"code","60b05932":"code","5874a4b5":"code","7da40013":"code","324ea33a":"code","316621cd":"code","759d4b69":"code","08e41adc":"code","433afc98":"code","9a6b1e34":"code","4245f8a9":"code","0f4e772a":"code","20774183":"code","aca13c84":"code","1b344796":"code","b58780a4":"code","622d7551":"code","79817809":"code","d8839987":"code","5b2e9abf":"code","d6b53d61":"code","08b7cc73":"code","16d47171":"markdown","15b4ec78":"markdown","0cc4ee31":"markdown","20554458":"markdown","8aa05aa3":"markdown","af079dec":"markdown","f4e352c0":"markdown","9d476b1c":"markdown","2debe651":"markdown","1427db9b":"markdown","b11b34d8":"markdown","174aeb18":"markdown","add78b96":"markdown","7a5edff3":"markdown","7e9f01e7":"markdown","db73cbb0":"markdown","1ceffc8f":"markdown","e89f3b46":"markdown","e08e15a5":"markdown"},"source":{"7f4a95f3":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport random\nfrom numpy.random.mtrand import RandomState\n\nseed = 12\nrandom.seed(seed)\nrng = RandomState(seed)\nnp.random.seed(seed)","e0165983":"# The reward function depends on the likelihood of clicking\n\ndef reward(arm_prob, step = 0):\n    '''\n    Reward function. \n    '''\n\n    return rng.choice([1, 0], p=[arm_prob, 1.0 - arm_prob])","56c8a1f7":"# Total ads\ntotal_arms  = 50\n\n# probs of ads\narms        = [random.betavariate(1.4, 5.4) for i in range(total_arms)]\n\n# Rouns test\nrounds      = 2000","96679624":"fig = px.histogram(x=arms, labels={'x': 'Prob(click)', 'count': 'Anuncios'}, histnorm='probability')\nfig.update_layout(template=\"plotly_white\")\nfig.show()","60b05932":"data = []\nfor i, a in enumerate(arms):\n    i_rounds = np.array([i+1 for i in range(rounds)])\n    values   = np.random.binomial(1, arms[i], rounds) #+ decay\n    values   = np.cumsum(values)\/i_rounds# if cum else row.values\n    \n    data.append(go.Scatter(name=\"Arm \"+str(i), x=[i for i in range(rounds)], y=values))\n\nfig = go.Figure(data=data)\nfig.update_layout(template=\"plotly_white\", \n                  xaxis_title_text='Time Step', \n                  yaxis_title_text=\"Cummulative Mean clicke\",\n                  title=\"Cumulative mean click received over time\")\nfig","5874a4b5":"def build_plot_rewards(policy = \"\", rewards = [], cum=False):\n    x = [i+1 for i in range(len(rewards))]\n    y = np.cumsum(rewards) if cum else np.cumsum(rewards)\/x\n    return go.Scatter(name=policy, x=x, y=y)\n\n\ndef plot_cum_mean_reward(experiments, rounds, arms, cum=False):\n\n    data = []\n    for name, obj in experiments.items():\n        _arms, rewards = run(rounds, arms, reward, obj)\n        data.append(build_plot_rewards(name, rewards, cum=cum))\n\n    fig = go.Figure(data=data)\n    fig.update_layout(template=\"plotly_white\", \n                      xaxis_title_text='Time Step', \n                      yaxis_title_text=\"Cummulative Mean Reward\",\n                      title=\"Cumulative mean reward received over time\")\n    return fig   \n\ndef plot_exploration_arm(rounds, arms, arms_rewards):\n    count_per_arms = {}\n    for a in range(len(arms)):\n        count_per_arms[a] = np.zeros(rounds)\n\n    for r in range(rounds):\n        count_per_arms[arms_rewards[r]][r] = 1\n\n    fig = go.Figure()\n    x   = (np.array(range(rounds)) + 1)\n\n    for arm, values in count_per_arms.items():    \n        fig.add_trace(go.Scatter(\n            name=\"Arm \"+str(arm),\n            x=x, y=np.cumsum(values),\n            hoverinfo='x+y',\n            mode='lines',\n            line=dict(width=0.5),\n            stackgroup='one',\n            groupnorm='percent' # define stack group\n        ))\n\n    fig.update_layout(template=\"plotly_white\", \n                  xaxis_title_text='Time Step', \n                  yaxis_title_text=\"Cummulative Exploration Arm\",\n                  title=\"Cumulative Exploration Arms over time\",\n                  yaxis_range=(0, 100))\n\n    return fig              ","7da40013":"class Bandit(object):\n    # Base Badit Class\n    #\n    def __init__(self, total_arms, seed=42):\n        self._total_arms   = total_arms\n        self._rng          = RandomState(seed)\n        self._arms_rewards =  {}\n        \n    def act(self):\n        pass\n    \n    def update(self, arm, reward):\n        if arm in self._arms_rewards:\n            self._arms_rewards[arm].append(reward)\n        else:\n            self._arms_rewards[arm] = [reward]\n    \n    def reduction_rewards(self, func= np.mean):\n        return np.array([func(self._arms_rewards[i]) for i in range(self._total_arms)])","324ea33a":"def run(iteraction, arms, func_reward, policy, verbose=False):\n    \n    arms_rewards  = {}\n    rewards       = []\n    arms_selected = []\n    \n    # init rewards\n    for arm, prob in enumerate(arms):\n        policy.update(arm, 0)\n\n    # run env\n    for step in range(iteraction):\n        arm    = policy.act()\n        \n        reward = func_reward(arms[arm], step=step)\n        \n        policy.update(arm, reward)\n        \n        arms_selected.append(arm)\n        rewards.append(reward)\n        \n        if verbose:\n            print(\"{}: arm {} with reward {}\".format(step, arm, reward))\n    \n    if verbose:\n        print(\"Reward Cum: {}\".format(np.sum(rewards)))\n    \n    return arms_selected, rewards\n\n","316621cd":"class RandomPolicy(Bandit):\n    # Random Select ARM\n    #\n    def __init__(self, total_arms, seed=42):\n        super().__init__(total_arms, seed=42)\n\n    def act(self):\n        return self._rng.choice(list(self._arms_rewards.keys()))\n    \narms_selected, rewards = run(10, arms, reward, RandomPolicy(total_arms), verbose=True)    ","759d4b69":"experiments = {\"Random\": RandomPolicy(total_arms)}\n\nplot_cum_mean_reward(experiments,rounds, arms)","08e41adc":"arms_rewards, random_rewards = run(iteraction = rounds, \n                                   arms= arms, \n                                   func_reward = reward, \n                                   policy = RandomPolicy(total_arms))\nplot_exploration_arm(rounds, arms, arms_rewards)","433afc98":"class EpsilonGreedy(Bandit):\n    def __init__(self, total_arms, epsilon=0.1, seed = 42):\n        super().__init__(total_arms, seed=seed)\n        self._total_arms = total_arms\n        self._epsilon    = epsilon\n\n    def act(self):\n        '''\n        Choice an arm\n        '''\n\n        if self._rng.choice([True, False], p=[self._epsilon, 1.0 - self._epsilon]):\n            action = self._rng.randint(0, self._total_arms)\n        else:\n            action = np.argmax(self.reduction_rewards())\n\n        return action","9a6b1e34":"# Simulate\n\nexperiments = {\n    \"Random\": RandomPolicy(total_arms),\n    \"Epsilon e=0.0\": EpsilonGreedy(total_arms, epsilon=0.0),\n    \"Epsilon e=0.1\": EpsilonGreedy(total_arms, epsilon=0.1),\n    \"Epsilon e=0.2\": EpsilonGreedy(total_arms, epsilon=0.2),\n    \"Epsilon e=0.5\": EpsilonGreedy(total_arms, epsilon=0.5),\n}\n \n\nplot_cum_mean_reward(experiments, rounds, arms)","4245f8a9":"arms_rewards, random_rewards = run(iteraction = rounds, \n                                   arms= arms, \n                                   func_reward = reward, \n                                   policy = EpsilonGreedy(total_arms, epsilon=0.1))\nplot_exploration_arm(rounds, arms, arms_rewards)","0f4e772a":"from scipy.special import softmax, expit","20774183":"class SoftmaxExplorer(Bandit):\n    def __init__(self, total_arms, logit_multiplier = 1.0, seed = 42):\n        super().__init__(total_arms, seed=seed)\n        self._logit_multiplier = logit_multiplier\n\n    def act(self):\n        reward_mean  = self.reduction_rewards(np.mean)\n        \n        reward_logit = expit(reward_mean)\n\n        arms_probs   = softmax(self._logit_multiplier * reward_logit)\n\n        action       = self._rng.choice(list(range(self._total_arms)), p = arms_probs)\n        \n        return action  ","aca13c84":"# Simulate\nexperiments = {\n    \"Random\": RandomPolicy(total_arms),\n    \"Softmax Exploration x1\": SoftmaxExplorer(total_arms, logit_multiplier=1),\n    \"Softmax Exploration x10\": SoftmaxExplorer(total_arms, logit_multiplier=10),\n    \"Softmax Exploration x100\": SoftmaxExplorer(total_arms, logit_multiplier=100),\n    \"Softmax Exploration x500\": SoftmaxExplorer(total_arms, logit_multiplier=500),\n}\n\nplot_cum_mean_reward(experiments, rounds, arms)","1b344796":"arms_rewards, random_rewards = run(iteraction = rounds, \n                                   arms= arms, \n                                   func_reward = reward, \n                                   policy = SoftmaxExplorer(total_arms, logit_multiplier=100))\nplot_exploration_arm(rounds, arms, arms_rewards)","b58780a4":"class UCB(Bandit):\n    def __init__(self, total_arms, c = 2, seed = 42):\n        super().__init__(total_arms, seed=seed)\n        self._c            = c\n        self._times        = 1\n        self._action_times = np.zeros(total_arms)\n    \n\n    def act(self):\n        reward_mean      = self.reduction_rewards()\n\n        confidence_bound = reward_mean + \\\n                            self._c * np.sqrt(\\\n                                  np.log(self._times) \/ (self._action_times + 0.1))  # c=2\n        \n        action       = np.argmax(confidence_bound)\n\n        self._times += 1\n        self._action_times[action] += 1\n        \n        return action","622d7551":"\n# Simulate\nexperiments = {\n    \"Random\": RandomPolicy(total_arms),\n    \"UCB c=0.01\": UCB(total_arms, c=0.01),\n    \"UCB c=0.5\": UCB(total_arms, c=0.5),\n    \"UCB c=1\": UCB(total_arms, c=1),\n    \"UCB c=2\": UCB(total_arms, c=2),\n}\n\nplot_cum_mean_reward(experiments, rounds, arms)","79817809":"arms_rewards, random_rewards = run(iteraction = rounds, \n                                   arms= arms, \n                                   func_reward = reward, \n                                   policy = UCB(total_arms, c=0.5))\nplot_exploration_arm(rounds, arms, arms_rewards)","d8839987":"class ThompsonSampling(Bandit):\n    def __init__(self, total_arms, seed = 42):\n        super().__init__(total_arms, seed=seed)        \n        self._alpha        = np.ones(total_arms)\n        self._beta         = np.ones(total_arms)\n    \n    def act(self):\n        reward_prior = [random.betavariate(self._alpha[i], self._beta[i]) \n                            for i in range(self._total_arms)]\n        \n        return np.argmax(reward_prior)\n    \n    def update(self, arm, reward):\n        super().update(arm, reward)        \n        \n        self._alpha[arm] += reward\n        self._beta[arm]  += 1 - reward","5b2e9abf":"# Simulate\nexperiments = {\n    \"Random\": RandomPolicy(total_arms),\n    \"Tompson Sampling\": ThompsonSampling(total_arms),\n}\n\nplot_cum_mean_reward(experiments, rounds, arms)","d6b53d61":"arms_rewards, random_rewards = run(iteraction = rounds, \n                                   arms= arms, \n                                   func_reward = reward, \n                                   policy = ThompsonSampling(total_arms))\nplot_exploration_arm(rounds, arms, arms_rewards)","08b7cc73":"# Simulate\nexperiments = {\n    \"UCB c=0.5\":    UCB(total_arms, c=0.5),\n    \"Tompson Sampling\": ThompsonSampling(total_arms),\n    \"Softmax Exploration x100\": SoftmaxExplorer(total_arms, logit_multiplier=100),\n    \"e-greedy e=0.1\": EpsilonGreedy(total_arms, epsilon=0.1),\n    \"Random\": RandomPolicy(total_arms),\n}\n\nplot_cum_mean_reward(experiments, rounds, arms)","16d47171":"Of the methods tested, the best for the 2000 steps was UCB. Tompson Sampling's method was still adjusting and it would probably look good too. All methods were above the random in all steps. It was possible to observe that depending on the parameter of the exploration method, the results vary widely.\n\nBy converting the click probabilities for each method, it is possible to arrive at an estimated gain value when using the method in question. \n\n```$(model, step) = [Prob(click|model)*valClick + (1-Prob(Click|model))*ValNoClick]*step```","15b4ec78":"The best arm is the \"Arm 32\". It has the highest click probability of all, which is 68%.  The MaB algorithms have to find it.","0cc4ee31":"The exploitation of each ad is carried out in the same way, regardless of the reward given","20554458":"Using a random algorithm the average reward is ~0.19. That is to say that for each iteration it would have a probability of ~0.19 for the ad to be clicked. ","8aa05aa3":"## Epsilon Greedy","af079dec":"**Thus, the UCB collects R\\\\$ 0.38 per step. Over the 2000 steps the total amount collected is R\\\\$ 760, if we used Random it would be R\\\\$ 40,00**","f4e352c0":"## Problem Definition\n\nWe are a digital ad-sales company and our main goal is to make a profit from selling the ads. We must choose the best ad, among several in our database to be displayed. Each click we earn R\\\\$ 1.00 and for each display ads we spend a total of R\\\\$ 0.20.  Therefore, we must create an algorithm that maximizes the company's financial return.\n\n* In this case, the available actions are clicked or no click\n* The reward will be boolean 1 if click or 0 not.","9d476b1c":"## Compare Results","2debe651":"### References\n\n* https:\/\/github.com\/marlesson\/MaB-Experiments\n* https:\/\/www.manifold.ai\/exploration-vs-exploitation-in-reinforcement-learning\n* https:\/\/towardsdatascience.com\/reinforcement-learning-multi-arm-bandit-implementation-5399ef67b24b\n* https:\/\/lilianweng.github.io\/lil-log\/2018\/01\/23\/the-multi-armed-bandit-problem-and-its-solutions.html\n* https:\/\/towardsdatascience.com\/solving-multiarmed-bandits-a-comparison-of-epsilon-greedy-and-thompson-sampling-d97167ca9a50","1427db9b":"The click probability distribution of all ads","b11b34d8":"## UCB - Upper Confidence Bound","174aeb18":"## Random","add78b96":"## Softmax Exploration","7a5edff3":"## Thompson Sampling","7e9f01e7":"We will create 100 ads with different probabilities of being clicked. The MaB algorithms must choose what should be presented without knowing this probability distribution","db73cbb0":"Model                | Prob(click)   | Click \\\\$ | NoClick \\\\$ | Total \\\\$\n-------------------- | ------------- | --------|-----------|--------\nUCB                  | 0.58          | 0,464   |  -0,084   |  0,38  \nTompson Sampling     | 0.57          | 0,456   |  -0,086   |  0,37 \nSoftmax Exploration  | 0.54          | 0,432   |   -0,092  |  0,34 \ne-Greedy             | 0.50          | 0,4     |   -0,1    |  0,3 \nRandom               | 0.22          | 0,176   |   -0,156  |  0,02 ","1ceffc8f":"Show cumulative mean reward received over time","e89f3b46":"#  What is Multi-armed Bandits?\n\n![](https:\/\/miro.medium.com\/max\/3000\/1*vaG1QOAdAgVsRWfFYjug_Q.png)\n\nIn probability theory, the multi-armed bandit problem (sometimes called the K-[1] or N-armed bandit problem[2]) is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice.[3][4] \n\nThis is a classic reinforcement learning problem that exemplifies the exploration-exploitation tradeoff dilemma. The name comes from imagining a gambler at a row of slot machines (sometimes known as \"one-armed bandits\"), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine.[5] \n\n![](http:\/\/)The multi-armed bandit problem also falls into the broad category of stochastic scheduling. - https:\/\/en.wikipedia.org\/wiki\/Multi-armed_bandit\n\n* Problem Definition\n* Methods\n * e-greedy\n * Softmax Exploration\n * UCB\n * Thompson Sampling\n* Results","e08e15a5":"Simulation of ground-truth probabilities rewards of each arm"}}