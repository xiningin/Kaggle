{"cell_type":{"5bed7c24":"code","ee276d40":"code","2f3df680":"code","7122f343":"code","34e5df8a":"code","d38432bc":"code","f43837f0":"code","bc741923":"code","3510c74b":"code","ca030890":"code","90f528b4":"code","c4c88b40":"code","b8d310e7":"code","f48f80f4":"code","e4b17b16":"code","ce1fc396":"code","e37fa082":"code","71a917cb":"code","d0cd4ad8":"code","88951b3d":"code","c2661894":"code","b3c1126e":"code","375d4cf1":"code","f00b9be9":"code","282a4f74":"code","23d8b4d5":"code","5e71f5a0":"code","add4dc97":"code","de293d5a":"code","b107b114":"code","3a47c09b":"code","f71a0e72":"code","0672b09e":"code","d6438606":"code","8346997d":"code","c32dadb1":"code","a64ca70e":"markdown","215e8899":"markdown","cc790efd":"markdown","ef19b432":"markdown"},"source":{"5bed7c24":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))","ee276d40":"train = pd.read_excel('..\/input\/Train_Data.xlsx')","2f3df680":"import fastai\nfrom fastai.text import *\nfrom fastai.callbacks import *","7122f343":"# check the contents of the dat set\ntrain.head()","34e5df8a":"train['question_text'][2]","d38432bc":"train['question_text'][21]","f43837f0":"from sklearn.model_selection import train_test_split\ntrain, val = train_test_split(train)","bc741923":"# Language model data bunch\ndata_lm = TextLMDataBunch.from_df('.', train,val,text_cols='question_text',label_cols='target')","3510c74b":"#save the preprocessed data\ndata_lm.save()","ca030890":"# Classifier model data\ndata_clas  = TextClasDataBunch.from_df('.', train_df=train,text_cols='question_text',label_cols='target',valid_df=val,vocab=data_lm.train_ds.vocab)","90f528b4":"data_clas.save()","c4c88b40":"data_clas.show_batch()","b8d310e7":"data_clas.vocab.itos[:10]","f48f80f4":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3,pretrained=True)\n","e4b17b16":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","ce1fc396":"learn.fit_one_cycle(2, 5.75E-02,callbacks=[SaveModelCallback(learn, name=\"best_lm\")], moms=(0.8,0.7))","e37fa082":"learn.save('fit_head')","71a917cb":"learn.unfreeze()","d0cd4ad8":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","88951b3d":"learn.fit_one_cycle(3,3.98E-04,callbacks=[SaveModelCallback(learn, name=\"best_lm\")], moms=(0.8,0.7))","c2661894":"learn.load('best_lm')","b3c1126e":"learn.save_encoder('AIBoot_enc')","375d4cf1":"learn1 = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.3)","f00b9be9":"learn1.load_encoder('AIBoot_enc')","282a4f74":"learn1.lr_find()","23d8b4d5":"learn1.recorder.plot(suggestion=True)","5e71f5a0":"best_clf_lr = learn1.recorder.min_grad_lr\nbest_clf_lr","add4dc97":"learn1.fit_one_cycle(1, best_clf_lr)","de293d5a":"learn1.freeze_to(-2)","b107b114":"learn1.fit_one_cycle(1, best_clf_lr)","3a47c09b":"learn1.unfreeze()","f71a0e72":"learn1.lr_find()\nlearn1.recorder.plot(suggestion=True)","0672b09e":"learn1.fit_one_cycle(3, 2e-3)","d6438606":"learn1.show_results()","8346997d":"learn.predict('Is it just me or have you ever been')","c32dadb1":"'Is it just me or have you ever been in this phase wherein you became ignorant to the people you once loved, completely disregarding their feelings\/lives so you get to have something go your way and feel temporarily at ease. How did things change?'","a64ca70e":"Here we mostly follow the training scheme described by Jeremy Howard in fast.ai,\ntaking a pretrained language model, fine-tuning it with unlabeled data, then fine-tuning classification head for our particular task.\n\nhttp:\/\/nlp.fast.ai\/classification\/2018\/05\/15\/introducting-ulmfit.html\n\nhttps:\/\/docs.fast.ai\/text.html#Quick-Start:-Training-an-IMDb-sentiment-model-with-ULMFiT","215e8899":"Quickly check the content of some of the Questions ","cc790efd":"Language Model Prediction","ef19b432":"https:\/\/docs.fast.ai\/text.transform.html#BaseTokenizer\n\n**The rules are all listed below, here is the meaning of the special tokens:**\n\nUNK (xxunk) is for an unknown word (one that isn't present in the current vocabulary)\n\nPAD (xxpad) is the token used for padding, if we need to regroup several texts of different lengths in a batch\n\nBOS (xxbos) represents the beginning of a text in your dataset\n\nFLD (xxfld) is used if you set mark_fields=True in your TokenizeProcessor to separate the different fields of texts \n(if your texts are loaded from several columns in a dataframe)\n\nTK_MAJ (xxmaj) is used to indicate the next word begins with a capital in the original text\n\nTK_UP (xxup) is used to indicate the next word is written in all caps in the original text\n\nTK_REP (xxrep) is used to indicate the next character is repeated n times in the original text (usage xxrep n {char})\n\nTK_WREP(xxwrep) is used to indicate the next word is repeated n times in the original text (usage xxwrep n {word})\n"}}