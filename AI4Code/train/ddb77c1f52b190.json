{"cell_type":{"33c99087":"code","ac1a6304":"code","9d629740":"code","e540b4fe":"code","06d50142":"code","142f6bd8":"code","7908afac":"code","d6fc967c":"code","05d66960":"code","5722b06a":"code","5dbdab80":"code","60a5e66c":"code","03b78daa":"code","ad44d65c":"code","02aab158":"code","48577008":"code","c9717a33":"code","e576a125":"code","2d40d3cc":"code","75cf68dd":"code","b505f0d8":"code","be91ad7d":"code","1fd5742f":"code","1523d0f7":"code","1b31e863":"code","31c01658":"code","504f7de8":"code","1b048a77":"code","e4035938":"code","9e18bc6b":"code","8d44a007":"markdown","c0ac540d":"markdown","8a884000":"markdown","8148368c":"markdown","ee654b8b":"markdown","b7135d4c":"markdown","f4d8147a":"markdown","0535b041":"markdown","49dd4310":"markdown","280aa5f3":"markdown","db462fbf":"markdown","35d74eaf":"markdown","c9a99f88":"markdown","0243731b":"markdown","338a7c77":"markdown","2752c11a":"markdown","3a7f37b6":"markdown","d2b98504":"markdown","09918813":"markdown","97e645f2":"markdown","bd806241":"markdown","6ee63899":"markdown","7d7b484d":"markdown","68fdb65c":"markdown","d9d518d3":"markdown","18d1ad59":"markdown","c8c4099c":"markdown","a38400b4":"markdown","50fb48f1":"markdown","df597e8f":"markdown","59e91d6f":"markdown","b8d3dae3":"markdown","4168b021":"markdown","45ad8180":"markdown","9f12a03a":"markdown","b52b438e":"markdown","fb8edb7d":"markdown"},"source":{"33c99087":"from featurewiz_py import featurewiz","ac1a6304":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nrandom_state = 123","9d629740":"df = pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\", index_col=\"customerID\")\nprint(df.shape)\ndf.head()","e540b4fe":"df[\"TotalCharges\"] = df[\"TotalCharges\"].apply(pd.to_numeric, errors='coerce')\ndf[\"SeniorCitizen\"] = df[\"SeniorCitizen\"].apply(lambda x: \"Yes\" if x == 0 else \"No\")","06d50142":"help(featurewiz)","142f6bd8":"target = 'Churn'","7908afac":"feats = featurewiz(df,target)","d6fc967c":"print(len(feats))\nfeats","05d66960":"df = df.dropna()","5722b06a":"df.isnull().sum()","5dbdab80":"X = df.drop(columns=[\"Churn\"])\ny = df[\"Churn\"]","60a5e66c":"scaler = StandardScaler()\nX[X.select_dtypes(\"number\").columns] = scaler.fit_transform(X.select_dtypes(\"number\"))","03b78daa":"ordEnc = OrdinalEncoder(dtype=np.int)\nX[X.select_dtypes(\"object\").columns] = ordEnc.fit_transform(X.select_dtypes(\"object\"))","ad44d65c":"labEnc = LabelEncoder()\ny = labEnc.fit_transform(y)","02aab158":"estimator = LogisticRegression(random_state=random_state)\nrfecv = RFECV(estimator=estimator, cv=StratifiedKFold(10, random_state=random_state, shuffle=True), scoring=\"accuracy\")\nrfecv.fit(X, y)","48577008":"plt.figure(figsize=(8, 6))\nplt.plot(range(1, len(rfecv.grid_scores_)+1), rfecv.grid_scores_)\nplt.grid()\nplt.xticks(range(1, X.shape[1]+1))\nplt.xlabel(\"Number of Selected Features\")\nplt.ylabel(\"CV Score\")\nplt.title(\"Recursive Feature Elimination (RFE)\")\nplt.show()\n\nprint(\"The optimal number of features: {}\".format(rfecv.n_features_))","c9717a33":"rfe_feats = X.columns[rfecv.support_]\nprint(len(rfe_feats))\nrfe_feats","e576a125":"X_rfe = X[feats]\nX_rfe.head(1)","2d40d3cc":"print(\"\\\"X\\\" dimension: {}\".format(X.shape))\nprint(\"\\\"X\\\" column list:\", X.columns.tolist())\nprint(\"\\\"X_rfe\\\" dimension: {}\".format(X_rfe.shape))\nprint(\"\\\"X_rfe\\\" column list:\", X_rfe.columns.tolist())","75cf68dd":"X_train, X_test, X_rfe_train, X_rfe_test, y_train, y_test = train_test_split(X, X_rfe, y, \n                                                                             train_size=0.8, \n                                                                             stratify=y,\n                                                                             random_state=random_state)\nprint(\"Train size: {}\".format(len(y_train)))\nprint(\"Test size: {}\".format(len(y_test)))","b505f0d8":"clf_keys = [\"Logistic Regression\", \"Support Vector Machine\", \"Naive Bayes\", \"k-Nearest Neighbors\",\n            \"Stochastic Gradient Descent\", \"Decision Tree\", \"AdaBoost\", \"Multi-layer Perceptron\"]\nclf_values = [LogisticRegression(random_state=random_state), SVC(kernel=\"linear\", random_state=random_state),\n              GaussianNB(), KNeighborsClassifier(), SGDClassifier(random_state=random_state),\n              DecisionTreeClassifier(random_state=random_state), AdaBoostClassifier(random_state=random_state), \n              MLPClassifier(random_state=random_state, max_iter=1000)]\nclf_rfe_keys = [\"Logistic Regression\", \"Support Vector Machine\", \"Naive Bayes\", \"k-Nearest Neighbors\",\n                \"Stochastic Gradient Descent\", \"Decision Tree\", \"AdaBoost\", \"Multi-layer Perceptron\"]\nclf_rfe_values = [LogisticRegression(random_state=random_state), SVC(kernel=\"linear\",random_state=random_state),\n                  GaussianNB(), KNeighborsClassifier(), SGDClassifier(random_state=random_state),\n                  DecisionTreeClassifier(random_state=random_state), AdaBoostClassifier(random_state=random_state), \n                  MLPClassifier(random_state=random_state, max_iter=1000)]\nclfs = dict(zip(clf_keys, clf_values))\nclfs_rfe = dict(zip(clf_rfe_keys, clf_rfe_values))\n\n# Original dataset\nprint(\"Model training using original data: started!\")\nfor clf_name, clf in clfs.items():\n    clf.fit(X_train, y_train)\n    clfs[clf_name] = clf\n    print(clf_name, \"training: done!\")\nprint(\"Model training using original data: done!\\n\")\n\n# Feature-selected dataset\nprint(\"Model training using feature-selected data: started!\")\nfor clf_rfe_name, clf_rfe in clfs_rfe.items():\n    clf_rfe.fit(X_rfe_train, y_train)\n    clfs_rfe[clf_rfe_name] = clf_rfe\n    print(clf_rfe_name, \"training: done!\")\nprint(\"Model training using feature-selected data: done!\")","be91ad7d":"# Original dataset\nacc = []\nfor clf_name, clf in clfs.items():\n    y_pred = clf.predict(X_test)\n    acc.append(accuracy_score(y_test, y_pred))\n\n# Feature selected dataset\nacc_rfe = []\nfor clf_rfe_name, clf_rfe in clfs_rfe.items():\n    y_rfe_pred = clf_rfe.predict(X_rfe_test)\n    acc_rfe.append(accuracy_score(y_test, y_rfe_pred))\n    \nacc_all = pd.DataFrame({\"Original dataset\": acc, \"Feature-selected dataset\": acc_rfe},\n                       index=clf_keys)\nacc_all","1fd5742f":"print(\"Accuracy\\n\" + acc_all.mean().to_string())\n\nax = acc_all.plot.bar(figsize=(10, 8))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height().round(3)), (p.get_x()*0.985, p.get_height()*1.002))\nplt.ylim((0.7, 0.82))\nplt.xticks(rotation=90)\nplt.title(\"All Classifier Accuracies\")\nplt.grid()\nplt.show()","1523d0f7":"scoring = [\"accuracy\", \"roc_auc\"]\n\nscores = []\n# Original dataset\nprint(\"Cross-validation on original data: started!\")\nfor clf_name, clf in clfs.items():\n    score = pd.DataFrame(cross_validate(clf, X, y, cv=StratifiedKFold(10, random_state=random_state, shuffle=True), scoring=scoring)).mean()\n    scores.append(score)\n    print(clf_name, \"cross-validation: done!\")\ncv_scores = pd.concat(scores, axis=1).rename(columns=dict(zip(range(len(clf_keys)), clf_keys)))\nprint(\"Cross-validation on original data: done!\\n\")\n\nscores = []\n# Feature-selected dataset\nprint(\"Cross-validation on feature-selected data: started!\")\nfor clf_name, clf in clfs_rfe.items():\n    score = pd.DataFrame(cross_validate(clf, X_rfe, y, cv=StratifiedKFold(10, random_state=random_state, shuffle=True), scoring=scoring)).mean()\n    scores.append(score)\n    print(clf_name, \"cross-validation: done!\")\ncv_scores_rfe = pd.concat(scores, axis=1).rename(columns=dict(zip(range(len(clf_keys)), clf_keys)))\nprint(\"Cross-validation on feature-selected data: done!\")","1b31e863":"# Accuracy\ncv_acc_all = pd.concat([cv_scores.loc[\"test_accuracy\"].rename(\"Original data\"), cv_scores_rfe.loc[\"test_accuracy\"].rename(\"Feature-selected data\")], \n                       axis=1)\n\nprint(\"Cross-validation accuracy\\n\" + cv_acc_all.mean().to_string())\nax = cv_acc_all.plot.bar(figsize=(10, 8))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height().round(3)), (p.get_x()*0.985, p.get_height()*1.003))\nplt.xticks(rotation=90)\nplt.ylim((0.7, 0.82))\nplt.title(\"Cross-validation Accuracy\")\nplt.grid()\nplt.legend()\nplt.show()","31c01658":"# ROC AUC\ncv_roc_auc_all = pd.concat([cv_scores.loc[\"test_roc_auc\"].rename(\"Original data\"), cv_scores_rfe.loc[\"test_roc_auc\"].rename(\"Feature-selected data\")], \n                           axis=1)\n\nprint(\"Cross-validation ROC AUC score\\n\" + cv_roc_auc_all.mean().to_string())\nax = cv_roc_auc_all.plot.bar(figsize=(10, 8))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height().round(3)), (p.get_x()*0.985, p.get_height()*1.003))\nplt.xticks(rotation=90)\nplt.ylim((0.63, 0.88))\nplt.title(\"Cross-validation ROC AUC Score\")\nplt.grid()\nplt.legend()\nplt.show()","504f7de8":"# Fit time\ncv_fit_time_all = pd.concat([cv_scores.loc[\"fit_time\"].rename(\"Original data\"), cv_scores_rfe.loc[\"fit_time\"].rename(\"Feature-selected data\")], \n                           axis=1)\n\nprint(\"Cross-validation fit time\\n\" + cv_fit_time_all.mean().to_string())\nax = cv_fit_time_all.plot.bar(figsize=(10, 8))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height().round(3)), (p.get_x()*0.985, p.get_height()*1.003))\nplt.xticks(rotation=90)\nplt.yscale(\"log\")\nplt.title(\"Cross-validation Fit Time\")\nplt.grid()\nplt.legend()\nplt.show()","1b048a77":"importance = abs(clfs[\"Logistic Regression\"].coef_[0])\nplt.barh(X.columns.values[importance.argsort()], importance[importance.argsort()])\nplt.title(\"Logistic Regression - Feature Importance (Original Data)\")\nplt.grid()\nplt.show()\n\nimportance_rfe = abs(clfs_rfe[\"Logistic Regression\"].coef_[0])\nplt.barh(X_rfe.columns.values[importance_rfe.argsort()], importance_rfe[importance_rfe.argsort()])\nplt.title(\"Logistic Regression - Feature Importance (Feature-selected Data)\")\nplt.grid()\nplt.show()","e4035938":"importance = clfs[\"AdaBoost\"].feature_importances_\nplt.barh(X.columns.values[importance.argsort()], importance[importance.argsort()])\nplt.title(\"AdaBoost - Feature Importance (Original Data)\")\nplt.grid()\nplt.show()\n\nimportance_rfe = clfs_rfe[\"AdaBoost\"].feature_importances_\nplt.barh(X_rfe.columns.values[importance_rfe.argsort()], importance_rfe[importance_rfe.argsort()])\nplt.title(\"AdaBoost - Feature Importance (Feature-selected Data)\")\nplt.grid()\nplt.show()","9e18bc6b":"importance = abs(clfs[\"Support Vector Machine\"].coef_[0])\nplt.barh(X.columns.values[importance.argsort()], importance[importance.argsort()])\nplt.title(\"Support Vectore Machine - Feature Importance (Original Data)\")\nplt.grid()\nplt.show()\n\nimportance_rfe = abs(clfs_rfe[\"Support Vector Machine\"].coef_[0])\nplt.barh(X_rfe.columns.values[importance_rfe.argsort()], importance_rfe[importance_rfe.argsort()])\nplt.title(\"Support Vectore Machine - Feature Importance (Feature-selected Data)\")\nplt.grid()\nplt.show()","8d44a007":"The top 5 important features of both AdaBoost models are slightly different. AdaBoost classifier that was trained on original data includes \"PaymentMethod\" on the fifth rank of its feature importance, while this feature is not selected during the RFE step. The rest of these important features are similar in both models.","c0ac540d":"From these three models, \"tenure\", \"MonthlyCharges\", and \"TotalCharges\" are always appeared on the top 5 important features of each model.","8a884000":"To validate the accuracy result and evaluate the performance of these two models furthermore, do k-fold cross-validation with $k = 10$ on the whole dataset.\nMetrics to validate are: accuracy, and ROC AUC score.","8148368c":"Check the accuracy of these two models, for now.","ee654b8b":"Let's check the feature importance of AdaBoost classifier for comparison.","b7135d4c":"<a id='Model-Evaluation'><\/a>\n# D. Model Evaluation","f4d8147a":"Encode each categorical feature by using ordinal encoder.","0535b041":"# **We are going to use a new Kaggle Utility Script called Featurewiz-Py to perform Recursive Feature Elimination (RFE) to Predict Customer Churn** \nLet us see how simple and fast this new library from Python is to reduce features!\n\nSteps:\n1.  Add \"Utility Script\" from File Menu on top\n2.  Look for \"featurewiz\" from the list of scripts available from the pop-up screen\n3. Select it and add it to your notebook\n4. You can then import that library using the command:\nfrom featurewiz_py import featurewiz","49dd4310":"Make a bar plot of all accuracy results to visualize them.","280aa5f3":"Do feature selection by using recursive feature elimination (RFE). Use Logistic Regression classifier as the estimator, and set the fold (k) for cross-validation to 10.","db462fbf":"Also, don't forget to encode the label.","35d74eaf":"<a id='Feature-Engineering-and-Selection'><\/a>\n# B. Feature Engineering and Selection","c9a99f88":"Make a line plot of number of selected features against cross-validation score. Then, print the optimal number of features.","0243731b":"From the result above, the mean accuracy of feature-selected data is slightly higher (0.3% higher) than the mean accuracy of the original data. The model that has the best accuracy is Support Vector Machine trained on feature-selected data with 79.6% accuracy. Multi-layer Perceptron accuracy improved by 2.3% with training on feature-selected data. But, there are some classifiers (Naive Bayes, k-Nearest Neighbors, Stochastic Gradient Descent, and AdaBoost) that don't get the advantage from training on feature-selected data.\n\nTo ensure this result, evaluate the model by using cross-validation.","338a7c77":"# This notebook is derived from the excellent notebook on RFE by Gabriel Daely. Many Thanks!\nhttps:\/\/www.kaggle.com\/gabrieldaely\/recursive-feature-elimination-rfe-implementation","2752c11a":"Notice that it took just 10 seconds to run on this data and it detected GPU's automatically in this Kaggle machine and using GPU's, it performed its feature selection to speed it up! Let us see what features it selected...","3a7f37b6":"<a id='Feature-Importance'><\/a>\n# E. Feature Importance","d2b98504":"Recursive feature elimination (RFE) is very useful to select only necessary features, save the training time, and still get similar accuracy, or even higher than the original data. RFE is popular because it is easy to configure and use and because it is effective at selecting those features (columns) in a training dataset that are more or most relevant in predicting the target variable. The feature importance of feature-selected data is also still preserved and is quite the same with original data based on the observation above.","09918813":"Use StandarScaler to standardize all numerical features, so their mean and standard deviation are zero and one, respectively.","97e645f2":"Let's try these following classifiers to make the machine learning model, and compare their performance for the original and feature-selected dataset.\n* Logistic Regression\n* Support Vector Machine (linear kernel)\n* Naive Bayes\n* k-Nearest Neighbors\n* Stochastic Gradient Descent\n* Decision Tree\n* AdaBoost\n* Multi-layer Perceptron","bd806241":"From the accuracy result, the mean accuracy of feature-selected data is 0.75% higher than the mean accuracy of the original data. The best accuracy here is Logistic Regression model trained on feature-selected data with 80.4% accuracy. Multi-layer Perceptron accuracy got the highest improvement by 2.8% with training on feature-selected data. Both SVM and AdaBoost accuracies of feature-selected data are slightly lower (only 0.1% lower) than the accuracies of original data. Remember, feature-selected data only has **9 features** while original data has 19 features.\n\nThe models that have the best ROC AUC score are Logistic Regression and AdaBoost with an ROC AUC score of 0.844. The ROC AUC result is not much different from the accuracy result. But there are some classifiers (Logistic Regression, Naive Bayes, and AdaBoost) that have slightly lower ROC AUC score of feature-selected data than the ROC AUC score of original data.\n\nAll models that were trained on feature-selected data have faster fit time than the one that was trained on original data. It is obviously because the number of features trained on those models.","6ee63899":"Separate features and label column into two variables, X and y.","7d7b484d":"From the steps above, the data is reduced to only 9 features from 19 features in the original data.\n\nNow, let's compare their performance on various machine learning models.","68fdb65c":"Find the feature importance of the predictive model that has been made. In this case, use Logistic Regression because it has the highest accuracy among all models.","d9d518d3":"# Now let's import and run Featurewiz to select features","18d1ad59":"Also find the feature importance of Support Vector Machine.","c8c4099c":"Compare the dimension of DataFrame \"X\" and \"X_rfe\".","a38400b4":"Top 5 important features of both Logistic Regression models are the same (\"tenure\", \"PhoneService\", \"Contract\", \"TotalCharges\", and \"MonthlyCharges\"). The rest of these important features are quite the same in both models.","50fb48f1":"Let's visualize cross-validation accuracy, ROC AUC score, and fit time results.","df597e8f":"Split the feature-selected DataFrame into train and test set. Also, do the same thing on the original DataFrame.","59e91d6f":"Do some changes on \"SeniorCitizen\" and \"TotalCharges\" data type to make them appropriate.","b8d3dae3":"<a id='Summary'><\/a>\n# F. Summary","4168b021":"<a id='Build-Some-ML-Models'><\/a>\n# C. Build Some ML Models","45ad8180":"Make a new DataFrame called \"X_rfe\" that contains selected features.","9f12a03a":"## Contents\n1. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n2. [Feature Engineering and Selection](#Feature-Engineering-and-Selection)\n3. [Build Some ML Models](#Build-Some-ML-Models)\n4. [Model Evaluation](#Model-Evaluation)\n5. [Feature Importance](#Feature-Importance)\n6. [Summary](#Summary)","b52b438e":"Import the dataset.","fb8edb7d":"The top 5 important features of both Support Vector Machine models are kind of different. SVM classifier that was trained on original data placed \"tenure\" on the fifth rank, while the other model placed \"tenure\" on the first rank."}}