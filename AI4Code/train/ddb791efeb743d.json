{"cell_type":{"cefe8e65":"code","b29e0ba3":"code","87fc8db2":"code","d3c16d59":"code","7e5d1e65":"code","8408a6e1":"code","7c643990":"code","7b43e20e":"code","8987e819":"code","265d4c32":"code","5b542fa8":"code","922c7d8f":"code","cfbd5451":"code","53552a2f":"code","58ab0b29":"code","af621a87":"code","1282be35":"code","8fb12c83":"code","4cfe7bba":"code","4e50969c":"code","7b286450":"code","5dd71284":"code","4167d463":"code","f046fd9c":"code","d284ac22":"code","8a650972":"code","bc57b863":"code","a34a2a82":"code","b64b4e98":"code","e9980a28":"code","40680136":"code","45f586fd":"code","36d20557":"code","4d038f69":"code","4e6daad0":"code","39b0ab08":"code","ad573d88":"code","3274e93a":"code","0c5e79a9":"code","ad5e5744":"code","8a37dee4":"code","4fc3b47d":"code","ff59ce26":"code","32c09172":"code","df1472f3":"code","8eb6770d":"code","2bc68e04":"code","9d10daac":"code","9beed430":"code","76bac9bd":"code","3ebaac70":"code","7d9421e0":"code","eaef3ea7":"markdown","50bb41ca":"markdown","7686aa27":"markdown","ac10b1ca":"markdown","fdb5013d":"markdown","5efcdd35":"markdown","db2cfc43":"markdown"},"source":{"cefe8e65":"import sys\n!curl -s https:\/\/course.fast.ai\/setup\/colab | bash\n!git clone https:\/\/github.com\/yabhi0807\/libml1.git \/kaggle\/tmp\/fastai # This is my repo with all the fastai(updated) libraries \nsys.path.append('\/kaggle\/tmp\/fastai')\n!mkdir \/kaggle\/tmp\/data\/\n!ln -s \/kaggle\/tmp\/* \/kaggle\/working\/","b29e0ba3":"!mkdir \/kaggle\/tmp\/data\/arxiv\/\n!cp -r ..\/input\/arxiv-dataset\/* \/kaggle\/tmp\/data\/arxiv\/\n!ls \/kaggle\/tmp\/data\/arxiv\/","87fc8db2":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai.model import fit\nfrom fastai.dataset import *\n\nimport torchtext\nfrom torchtext import vocab, data\nfrom torchtext.datasets import language_modeling\n\nfrom fastai.rnn_reg import *\nfrom fastai.rnn_train import *\nfrom fastai.nlp import *\nfrom fastai.lm_rnn import *\n\nimport dill as pickle\nimport random","d3c16d59":"bs,bptt = 64,70","7e5d1e65":"!pip install feedparser","8408a6e1":"import os, requests, time\n# feedparser isn't a fastai dependency so you may need to install it.\nimport feedparser\nimport pandas as pd\n\n\nclass GetArXiv(object):\n    def __init__(self, pickle_path, categories=list()):\n        \"\"\"\n        :param pickle_path (str): path to pickle data file to save\/load\n        :param pickle_name (str): file name to save pickle to path\n        :param categories (list): arXiv categories to query\n        \"\"\"\n        if os.path.isdir(pickle_path):\n            pickle_path = f\"{pickle_path}{'' if pickle_path[-1] == '\/' else '\/'}all_arxiv.pkl\"\n        if len(categories) < 1:\n            categories = ['cs*', 'cond-mat.dis-nn', 'q-bio.NC', 'stat.CO', 'stat.ML']\n        # categories += ['cs.CV', 'cs.AI', 'cs.LG', 'cs.CL']\n\n        self.categories = categories\n        self.pickle_path = pickle_path\n        self.base_url = 'http:\/\/export.arxiv.org\/api\/query'\n\n    @staticmethod\n    def build_qs(categories):\n        \"\"\"Build query string from categories\"\"\"\n        return '+OR+'.join(['cat:'+c for c in categories])\n\n    @staticmethod\n    def get_entry_dict(entry):\n        \"\"\"Return a dictionary with the items we want from a feedparser entry\"\"\"\n        try:\n            return dict(title=entry['title'], authors=[a['name'] for a in entry['authors']],\n                        published=pd.Timestamp(entry['published']), summary=entry['summary'],\n                        link=entry['link'], category=entry['category'])\n        except KeyError:\n            print('Missing keys in row: {}'.format(entry))\n            return None\n\n    @staticmethod\n    def strip_version(link):\n        \"\"\"Strip version number from arXiv paper link\"\"\"\n        return link[:-2]\n\n    def fetch_updated_data(self, max_retry=5, pg_offset=0, pg_size=1000, wait_time=15):\n        \"\"\"\n        Get new papers from arXiv server\n        :param max_retry: max number of time to retry request\n        :param pg_offset: number of pages to offset\n        :param pg_size: num abstracts to fetch per request\n        :param wait_time: num seconds to wait between requests\n        \"\"\"\n        i, retry = pg_offset, 0\n        df = pd.DataFrame()\n        past_links = []\n        if os.path.isfile(self.pickle_path):\n            df = pd.read_pickle(self.pickle_path)\n            df.reset_index()\n        if len(df) > 0: past_links = df.link.apply(self.strip_version)\n\n        while True:\n            params = dict(search_query=self.build_qs(self.categories),\n                          sortBy='submittedDate', start=pg_size*i, max_results=pg_size)\n            response = requests.get(self.base_url, params='&'.join([f'{k}={v}' for k, v in params.items()]))\n            entries = feedparser.parse(response.text).entries\n            if len(entries) < 1:\n                if retry < max_retry:\n                    retry += 1\n                    time.sleep(wait_time)\n                    continue\n                break\n\n            results_df = pd.DataFrame([self.get_entry_dict(e) for e in entries])\n            max_date = results_df.published.max().date()\n            new_links = ~results_df.link.apply(self.strip_version).isin(past_links)\n            print(f'{i}. Fetched {len(results_df)} abstracts published {max_date} and earlier')\n            if not new_links.any():\n                break\n\n            df = pd.concat((df, results_df.loc[new_links]), ignore_index=True)\n            i += 1\n            retry = 0\n            time.sleep(wait_time)\n\n        print(f'Downloaded {len(df)-len(past_links)} new abstracts')\n        df.sort_values('published', ascending=False).groupby('link').first().reset_index()\n        df.to_pickle(self.pickle_path)\n        return df\n\n    @classmethod\n    def load(cls, pickle_path):\n        \"\"\"Load data from pickle and remove duplicates\"\"\"\n        return pd.read_pickle(cls(pickle_path).pickle_path)\n\n    @classmethod\n    def update(cls, pickle_path, categories=list(), **kwargs):\n        \"\"\"\n        Update arXiv data pickle with the latest abstracts\n        \"\"\"\n        cls(pickle_path, categories).fetch_updated_data(**kwargs)\n        return True","7c643990":"PATH='data\/arxiv\/'\n\nALL_ARXIV = f'{PATH}all_arxiv.pkl'\n\n# all_arxiv.pkl: if arxiv hasn't been downloaded yet, it'll take some time to get it - go get some coffee\nif not os.path.exists(ALL_ARXIV): GetArXiv.update(ALL_ARXIV)\n\n# arxiv.csv: see dl1\/nlp-arxiv.ipynb to get this one\ndf_mb = pd.read_csv(f'{PATH}arxiv.csv')\ndf_all = pd.read_pickle(ALL_ARXIV)","7b43e20e":"def get_txt(df):\n    return '<CAT> ' + df.category.str.replace(r'[\\.\\-]','') + ' <SUMM> ' + df.summary + ' <TITLE> ' + df.title\ndf_mb['txt'] = get_txt(df_mb)\ndf_all['txt'] = get_txt(df_all)\nn=len(df_all); n","8987e819":"os.makedirs(f'{PATH}trn\/yes', exist_ok=True)\nos.makedirs(f'{PATH}val\/yes', exist_ok=True)\nos.makedirs(f'{PATH}trn\/no', exist_ok=True)\nos.makedirs(f'{PATH}val\/no', exist_ok=True)\nos.makedirs(f'{PATH}all\/trn', exist_ok=True)\nos.makedirs(f'{PATH}all\/val', exist_ok=True)\nos.makedirs(f'{PATH}models', exist_ok=True)","265d4c32":"for (i,(_,r)) in enumerate(df_all.iterrows()):\n    dset = 'trn' if random.random()>0.1 else 'val'\n    open(f'{PATH}all\/{dset}\/{i}.txt', 'w').write(r['txt'])","5b542fa8":"for (i,(_,r)) in enumerate(df_mb.iterrows()):\n    lbl = 'yes' if r.tweeted else 'no'\n    dset = 'trn' if random.random()>0.1 else 'val'\n    open(f'{PATH}{dset}\/{lbl}\/{i}.txt', 'w').write(r['txt'])","922c7d8f":"!python -m spacy download en_core_web_md","cfbd5451":"!python -m spacy link en_core_web_md en_core_web_md","53552a2f":"from spacy.symbols import ORTH\n\n# install the 'en' model if the next line of code fails by running:\n#python -m spacy download en              # default English model (~50MB)\n#python -m spacy download en_core_web_md  # larger English model (~1GB)\n# my_tok = en_core_web_md.load()\nmy_tok = spacy.load('en_core_web_md')\n# or\n# import en_core_web_md\n# my_tok = en_core_web_md.load()\n\nmy_tok.tokenizer.add_special_case('<SUMM>', [{ORTH: '<SUMM>'}])\nmy_tok.tokenizer.add_special_case('<CAT>', [{ORTH: '<CAT>'}])\nmy_tok.tokenizer.add_special_case('<TITLE>', [{ORTH: '<TITLE>'}])\nmy_tok.tokenizer.add_special_case('<BR \/>', [{ORTH: '<BR \/>'}])\nmy_tok.tokenizer.add_special_case('<BR>', [{ORTH: '<BR>'}])\n\ndef my_spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(x)]","58ab0b29":"TEXT = data.Field(lower=True, tokenize=my_spacy_tok)\nFILES = dict(train='trn', validation='val', test='val')\nmd = LanguageModelData.from_text_files(f'{PATH}all\/', TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)\npickle.dump(TEXT, open(f'{PATH}models\/TEXT.pkl','wb'))","af621a87":"len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)","1282be35":"TEXT.vocab.itos[:12]","8fb12c83":"' '.join(md.trn_ds[0].text[:150])","4cfe7bba":"em_sz = 200\nnh = 500\nnl = 3\nopt_fn = partial(optim.Adam, betas=(0.7, 0.99))","4e50969c":"learner = md.get_model(opt_fn, em_sz, nh, nl,\n    dropout=0.05, dropouth=0.1, dropouti=0.05, dropoute=0.02, wdrop=0.2)\n# dropout=0.4, dropouth=0.3, dropouti=0.65, dropoute=0.1, wdrop=0.5\n#                dropouti=0.05, dropout=0.05, wdrop=0.1, dropoute=0.02, dropouth=0.05)\nlearner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\nlearner.clip=0.3","7b286450":"learner.fit(3e-3, 1, wds=1e-6)","5dd71284":"learner.fit(3e-3, 3, wds=1e-6, cycle_len=1, cycle_mult=2)","4167d463":"learner.save_encoder('adam2_enc')","f046fd9c":"learner.fit(3e-3, 10, wds=1e-6, cycle_len=5, cycle_save_name='adam3_10')","d284ac22":"learner.save_encoder('adam3_10_enc')","8a650972":"learner.fit(3e-3, 8, wds=1e-6, cycle_len=10, cycle_save_name='adam3_5')","bc57b863":"learner.fit(3e-3, 1, wds=1e-6, cycle_len=20, cycle_save_name='adam3_20')","a34a2a82":"learner.save_encoder('adam3_20_enc')","b64b4e98":"learner.save('adam3_20')","e9980a28":"def proc_str(s): return TEXT.preprocess(TEXT.tokenize(s))\ndef num_str(s): return TEXT.numericalize([proc_str(s)])","40680136":"m=learner.model","45f586fd":"s=\"\"\"<CAT> cscv <SUMM> algorithms that\"\"\"","36d20557":"def sample_model(m, s, l=50):\n    t = num_str(s)\n    m[0].bs=1\n    m.eval()\n    m.reset()\n    res,*_ = m(t)\n    print('...', end='')\n\n    for i in range(l):\n        n=res[-1].topk(2)[1]\n        n = n[1] if n.data[0]==0 else n[0]\n        word = TEXT.vocab.itos[n.data[0]]\n        print(word, end=' ')\n        if word=='<eos>': break\n        res,*_ = m(n[0].unsqueeze(0))\n\n    m[0].bs=bs","4d038f69":"sample_model(m,\"<CAT> csni <SUMM> algorithms that\")","4e6daad0":"sample_model(m,\"<CAT> cscv <SUMM> algorithms that\")","39b0ab08":"sample_model(m,\"<CAT> cscv <SUMM> algorithms. <TITLE> on \")","ad573d88":"sample_model(m,\"<CAT> csni <SUMM> algorithms. <TITLE> on \")","3274e93a":"sample_model(m,\"<CAT> cscv <SUMM> algorithms. <TITLE> towards \")","0c5e79a9":"sample_model(m,\"<CAT> csni <SUMM> algorithms. <TITLE> towards \")","ad5e5744":"TEXT = pickle.load(open(f'{PATH}models\/TEXT.pkl','rb'))","8a37dee4":"class ArxivDataset(torchtext.data.Dataset):\n    def __init__(self, path, text_field, label_field, **kwargs):\n        fields = [('text', text_field), ('label', label_field)]\n        examples = []\n        for label in ['yes', 'no']:\n            fnames = glob(os.path.join(path, label, '*.txt'));\n            assert fnames, f\"can't find 'yes.txt' or 'no.txt' under {path}\/{label}\"\n            for fname in fnames:\n                with open(fname, 'r') as f: text = f.readline()\n                examples.append(data.Example.fromlist([text, label], fields))\n        super().__init__(examples, fields, **kwargs)\n\n    @staticmethod\n    def sort_key(ex): return len(ex.text)\n    \n    @classmethod\n    def splits(cls, text_field, label_field, root='.data',\n               train='train', test='test', **kwargs):\n        return super().splits(\n            root, text_field=text_field, label_field=label_field,\n            train=train, validation=None, test=test, **kwargs)","4fc3b47d":"ARX_LABEL = data.Field(sequential=False)\nsplits = ArxivDataset.splits(TEXT, ARX_LABEL, PATH, train='trn', test='val')","ff59ce26":"md2 = TextData.from_splits(PATH, splits, bs)","32c09172":"#            dropout=0.3, dropouti=0.4, wdrop=0.3, dropoute=0.05, dropouth=0.2)","df1472f3":"from sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\ndef prec_at_6(preds,targs):\n    precision, recall, _ = precision_recall_curve(targs==2, preds[:,2])\n    print(recall[precision>=0.6][0])\n    return recall[precision>=0.6][0]","8eb6770d":"# dropout=0.4, dropouth=0.3, dropouti=0.65, dropoute=0.1, wdrop=0.5\nm3 = md2.get_model(opt_fn, 1500, bptt, emb_sz=em_sz, n_hid=nh, n_layers=nl, \n           dropout=0.1, dropouti=0.65, wdrop=0.5, dropoute=0.1, dropouth=0.3)\nm3.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\nm3.clip=25.","2bc68e04":"# this notebook has a mess of some things going under 'all\/' others not, so a little hack here\n!ln -sf ..\/all\/models\/adam3_20_enc.h5 {PATH}models\/adam3_20_enc.h5\nm3.load_encoder(f'adam3_20_enc')\nlrs=np.array([1e-4,1e-3,1e-3,1e-2,3e-2])","9d10daac":"m3.freeze_to(-1)\nm3.fit(lrs\/2, 1, metrics=[accuracy])\nm3.unfreeze()\nm3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)","9beed430":"m3.fit(lrs, 2, metrics=[accuracy], cycle_len=4, cycle_save_name='imdb2')","76bac9bd":"prec_at_6(*m3.predict_with_targs())","3ebaac70":"m3.fit(lrs, 4, metrics=[accuracy], cycle_len=2, cycle_save_name='imdb2')","7d9421e0":"prec_at_6(*m3.predict_with_targs())","eaef3ea7":"After the downloading it either import it directly or create a symbolic link","50bb41ca":"### Data","7686aa27":"### Test","ac10b1ca":"## Language modeling","fdb5013d":"### Train","5efcdd35":"**Interrupted Kenrel bec this will take a lot of time**","db2cfc43":"### Sentiment"}}