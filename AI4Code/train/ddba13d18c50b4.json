{"cell_type":{"79a3b6df":"code","9bacb15d":"code","2ce448fa":"code","624f3769":"code","1c240d0c":"code","dbdc4a4b":"code","c42989d5":"code","bcb729d3":"code","3e034566":"code","7bf29cb0":"code","04571968":"code","36122442":"code","7b70bdd2":"code","d265d7f1":"code","3a7348f6":"code","54b5127b":"code","0e71fc2c":"code","79d17b45":"code","cb5cca82":"code","3e0270a9":"code","c0a9247d":"code","7488b443":"code","a42dacde":"code","2e9bcb4b":"code","d9a1e55b":"code","c9b7a7c4":"code","7487cdfd":"code","76d19474":"code","74ded1dd":"code","08fbed34":"code","24e73767":"code","4aacdf03":"code","304a30b3":"code","b5e88924":"code","bc0e9093":"code","07bd36e5":"code","948ad720":"code","6419ab9e":"code","62b1d9a3":"code","ff49f0f9":"code","e912715c":"code","dbf6c6ab":"code","ffa9d921":"code","19009561":"code","bf97da17":"code","d1da9076":"code","bd017dc0":"code","ac6cef05":"code","fa7b306b":"markdown","79864da2":"markdown","629e941b":"markdown","8c9704d2":"markdown","17fbb359":"markdown","a9dbd338":"markdown","29b06d59":"markdown","33799e52":"markdown","46352a90":"markdown","b68d7f56":"markdown","f75a9f12":"markdown","c9b14421":"markdown","25ff59a1":"markdown","d3330814":"markdown","eaab07e0":"markdown","d81996b6":"markdown","53baaa0f":"markdown","d0f585be":"markdown","321c0a56":"markdown","fbf26e15":"markdown","8436556d":"markdown","714ccd19":"markdown","e967f98e":"markdown","7a1884f6":"markdown"},"source":{"79a3b6df":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport os\nimport json\nimport re\nimport nltk\nimport zipfile\n\nfrom datetime import datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer","9bacb15d":"for t in ['train','test']:\n    with zipfile.ZipFile(\"..\/input\/whats-cooking\/{}.json.zip\".format(t),\"r\") as z:\n        z.extractall(\".\")\n    \nwith open('.\/train.json') as data_file:    \n    data = json.load(data_file)\n    \nwith open('.\/test.json') as test_file:\n    test = json.load(test_file)","2ce448fa":"df = pd.DataFrame(data)\ntest_df = pd.DataFrame(test)\n\ntest_ids = test_df['id']\n\ndf.head()","624f3769":"(df.isnull().sum() \/ len(df))*100 # No null values in train","1c240d0c":"(test_df.isnull().sum() \/ len(test_df))*100 # No null values in test","dbdc4a4b":"fig, ax = plt.subplots(figsize=(10,10))\nper_vals = round(df[\"cuisine\"].value_counts(normalize=True)*100, 2)\nfor i, v in enumerate(per_vals):\n    ax.text(v + 3, i + .25, str(v)+\"%\", color='blue', fontweight='bold')\ndf[\"cuisine\"].value_counts().plot.barh(ax = ax)\nplt.show()","c42989d5":"fig, ax = plt.subplots(figsize=(22,7))\nextensive_ing_list = []\nfor x in df['ingredients']:\n    for y in x:\n        extensive_ing_list.append(y)\n        \nextensive_ing_list = pd.Series(extensive_ing_list)\nextensive_ing_list.value_counts().sort_values(ascending=False).head(30).plot.bar(ax = ax)","bcb729d3":"cuisine = df[\"cuisine\"].unique()\n\nall_cus = dict()\nfor cs in cuisine:\n    i = []\n    for ing_list in df[df['cuisine']==cs]['ingredients']:\n        for ing in ing_list:\n            i.append(ing)\n    all_cus[cs] = i\n\nall_cus.keys()","3e034566":"for key in all_cus.keys():\n    fig, ax = plt.subplots(figsize=(25,2))\n    pd.Series(all_cus[key]).value_counts().head(25).plot.bar(ax=ax, title=key)\n    plt.show()","7bf29cb0":"# for key in all_cus.keys():\n#     fig, ax = plt.subplots(figsize=(25,2))\n#     pd.Series(all_cus[key]).value_counts().tail(25).plot.bar(ax=ax, title=key)\n#     plt.show()","04571968":"def preprocess_df(df):\n    \n    def process_string(x):\n        x = [\" \".join([WordNetLemmatizer().lemmatize(q) for q in p.split()]) for p in x] #Lemmatization\n        x = list(map(lambda x: re.sub(r'\\(.*oz.\\)|crushed|crumbles|ground|minced|powder|chopped|sliced','', x), x))\n        x = list(map(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x), x))   # To remove everything except a-z and A-Z\n        x = \" \".join(x)                                 # To make list element a string element \n        x = x.lower()\n        return x\n    \n    df = df.drop('id',axis=1)\n    df['ingredients'] = df['ingredients'].apply(process_string)\n    \n    return df","36122442":"def get_cuisine_cumulated_ingredients(df):\n    cuisine_df = pd.DataFrame(columns=['ingredients'])\n\n    for cus in cuisine:\n        st = \"\"\n        for x in df[df.cuisine == cus]['ingredients']:\n            st += x\n            st += \" \"\n        cuisine_df.loc[cus,'ingredients'] = st\n\n    cuisine_df = cuisine_df.reset_index()\n    cuisine_df = cuisine_df.rename(columns ={'index':'cuisine'})\n    return cuisine_df","7b70bdd2":"df = preprocess_df(df)\ntest_df = preprocess_df(test_df)\n\ncuisine_df = get_cuisine_cumulated_ingredients(df)","d265d7f1":"df.head()","3a7348f6":"train = df['ingredients']\ntarget = df['cuisine']\ntest = test_df['ingredients']","54b5127b":"def count_vectorizer(train, test=None):\n    cv = CountVectorizer()\n    train = cv.fit_transform(train)\n    if test is not None:\n        test = cv.transform(test)\n        return train, test, cv\n    else:\n        return train, cv","0e71fc2c":"# train_cv , test_cv, cv = count_vectorizer(train,test)\n# cuisine_data_cv, cuisine_cv = count_vectorizer(cuisine_df['ingredients'])","79d17b45":"def tfidf_vectorizer(train, test=None):\n    tfidf = TfidfVectorizer(stop_words='english',\n                             ngram_range = ( 1 , 1 ),analyzer=\"word\", \n                             max_df = .57 , binary=False , token_pattern=r'\\w+' , sublinear_tf=False)\n    train = tfidf.fit_transform(train)\n    if test is not None:\n        test = tfidf.transform(test)\n        return train, test, tfidf\n    else:\n        return train, tfidf","cb5cca82":"train_tfidf, test_tfidf, tfidf = tfidf_vectorizer(train,test)\ncuisine_data_tfidf, cuisine_tfidf = tfidf_vectorizer(cuisine_df['ingredients'])","3e0270a9":"from sklearn.cluster import KMeans\nfrom sklearn.decomposition import KernelPCA,PCA,TruncatedSVD\n\ndef get_kmeans_wcss(data, n_limit=15):\n    wcss = [] #Within cluster sum of squares (WCSS)\n    for i in range(1,n_limit):\n        km = KMeans(init='k-means++', n_clusters=i, n_init=10)\n        km.fit(data)\n        wcss.append(km.inertia_)\n    plt.title(\"Elbow Method\")\n    plt.plot(range(1, n_limit), wcss)\n    plt.xlabel(\"Number of clusters\")\n    plt.ylabel(\"WCSS\")\n    return wcss\n    \n    \ndef kmeans(data, n):\n    km = KMeans(init='k-means++', n_clusters=n, n_init=10)\n    km = km.fit(data)\n    return km.predict(data), km \n\n\ndef get_PCA(data, n_components=2):\n    pca = PCA(n_components = n_components)\n    reduced_data = pca.fit_transform(data)\n    explained_variance = pca.explained_variance_ratio_\n    print(explained_variance)\n    return reduced_data, pca, explained_variance\n\ndef get_kernel_PCA(data, n_components=2, kernel='rbf'):\n    kpca = KernelPCA(n_components = 2, kernel = kernel)\n    reduced_data = kpca.fit_transform(data)\n    explained_variance = kpca.explained_variance_ratio_\n    print(explained_variance)\n    return reduced_data, kpca, explained_variance\n\ndef get_TSVD(data, n_components=2, n_ittr=5, algorithm='randomized'):\n    tsvd = TruncatedSVD(n_components=n_components, n_iter=n_ittr, algorithm=algorithm)\n    reduced_data = tsvd.fit_transform(data)\n    explained_variance = tsvd.explained_variance_ratio_\n    print(explained_variance)\n    return reduced_data, tsvd, explained_variance\n\n\n\ndef create_pca_graph(cluster_pca, red_pca, n_clus):\n\n    c_mask = []\n    c_x = []\n    c_y = []\n    \n    for i in range(0,n_clus):\n        c_mask.append([x for x in cluster_pca==i])\n    \n    for i in range(0,n_clus):\n        c_x.append([a[0] for a, b in zip(red_pca, c_mask[i]) if b])\n        c_y.append([a[1] for a, b in zip(red_pca, c_mask[i]) if b])\n\n    colours = ['red','blue','green','orange','purple','cyan','black','magenta']\n    \n    for i in range(0,n_clus):\n        plt.scatter(c_x[i], c_y[i], s=30, c=colours[i], label='Cluster {}'.format(i))\n        \n        \n#     for i in range(0,20):\n#         label = label_list[i]\n#         plt.annotate(label, (c_x[i],c_y[i]), textcoords=\"offset points\", xytext=(0,10), # distance from text to points (x,y)\n#                      ha='center') # horizontal alignment can be left, right or center\n        \n     \n    plt.title(\"Clusters of PCA\")\n    plt.xlabel(\"PCA 1\")\n    plt.ylabel(\"PCA 2\")\n    plt.legend()\n    plt.show()","c0a9247d":"# red_tsvd, tsvd, var_tsvd = get_TSVD(train_cv,2)  #Used because train_cv is a sparse matrix. PCA won't work\n# red_pca, pca, var_pca = get_PCA((train_cv).toarray(),2)\n# red_pca, pca, var_pca = get_PCA((train_tfidf).toarray(),2)\n# red_tsvd, tsvd, var_tsvd = get_TSVD(train_tfidf,2)  #Used because train_tfidf is a sparse matrix. PCA won't work\n# red_kpca, kpca, var_kpca = get_kernel_PCA(train_cv,2)  #Uses excessive RAM","7488b443":"red_cuisine_pca, cus_pca, var_cus_pca = get_PCA((cuisine_data_tfidf).toarray(),2)","a42dacde":"%%time\nwcss_pca = get_kmeans_wcss(red_cuisine_pca,20)","2e9bcb4b":"cluster_cus_pca, km_cus_pca = kmeans(red_cuisine_pca,3)\ncluster_cus_pca","d9a1e55b":"create_pca_graph(cluster_cus_pca, red_cuisine_pca, 3)","c9b7a7c4":"# cuisine_df[cluster_cus_pca==0]['cuisine']\n# cuisine_df[cluster_cus_pca==1]['cuisine']\n# cuisine_df[cluster_cus_pca==2]['cuisine']","7487cdfd":"%%time\nwcss = get_kmeans_wcss(train_tfidf,30)","76d19474":"cluster, km = kmeans(train_tfidf,19) # train_cv or train_tfidf\ncluster_test = km.predict(test_tfidf)\ncluster","74ded1dd":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(cluster.reshape(-1, 1))\ncluster_encoded = enc.transform(cluster.reshape(-1, 1)).toarray()","08fbed34":"cluster_test_encoded = enc.transform(cluster_test.reshape(-1, 1)).toarray()","24e73767":"train_tfidf_nonsparse = np.append((train_tfidf).toarray(), cluster_encoded, axis=1)","4aacdf03":"test_tfidf_nonsparse = np.append((test_tfidf).toarray(), cluster_test_encoded, axis=1)","304a30b3":"print(\"TRAINING DATASET: Added cluster of shape {} to train_cv of shape {} as a column\".format(cluster_encoded.shape, train_tfidf.shape))\nprint(\"TESTING DATASET: Added cluster of shape {} to test_cv of shape {} as a column\".format(cluster_test_encoded.shape, test_tfidf.shape))","b5e88924":"from scipy import sparse\n\n# train = sparse.csr_matrix(train_tfidf_nonsparse)\n# test = sparse.csr_matrix(test_tfidf_nonsparse)\n\ntrain = train_tfidf # USE THIS FOR BEST RESULTS (0.8106)\ntest = test_tfidf # USE THIS FOR BEST RESULTS (0.8106)","bc0e9093":"from sklearn.svm import LinearSVC, SVC\nfrom sklearn.metrics import f1_score\n\nparam_grid = {'C': [0.001, 0.1, 1, 10, 50, 100, 500, 1000, 5000],  \n              'penalty': ['l1','l2'],\n             'loss': ['hinge','squared hinge']} \n\ngrid = GridSearchCV(LinearSVC(), param_grid, refit = True, verbose = 3, n_jobs=-1, scoring='f1_micro')","07bd36e5":"%%time\ngrid.fit(train, target) ","948ad720":"grid.best_params_","6419ab9e":"grid.best_score_","62b1d9a3":"from sklearn.metrics import f1_score\nfrom sklearn.svm import LinearSVC, SVC\n\ndef evalfn(C, gamma):\n    s = SVC(C=float(C), gamma=float(gamma), kernel='rbf', class_weight='balanced')\n    f = cross_val_score(s, train, target, cv=5, scoring='f1_micro')\n    return f.max()","ff49f0f9":"from bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import cross_val_score\nnew_opt = BayesianOptimization(evalfn, {'C': (0.1, 1000),  \n              'gamma': (0.0001, 1)  })","e912715c":"############################################\n### OPTIMIZED PARAMETERS ARE SHOWN BELOW ###\n##  HYPER PARAMETER OPT IS TIME CONSUMING ##\n############################################\n\n# %%time\n# new_opt.maximize(n_iter=15, init_points=3)   ","dbf6c6ab":"# new_opt.max","ffa9d921":"# OPTIMIZED PARAMETERS\n# {'target': 0.7945391461758937,\n#  'params': {'C': 604.5300203551828, 'gamma': 0.9656489284085462}}\n\n# With cluster(n=19) as a parameter:\n# {'target': 0.7940917661847894,\n#  'params': {'C': 509.674609734803, 'gamma': 0.724238238886398}}\n\nC = 604.5300203551828\ngamma = 0.9656489284085462\n\nclf = SVC(C=float(C), gamma=float(gamma), kernel='rbf')","19009561":"%%time\nclf.fit(train, target)","bf97da17":"import pickle\nfrom datetime import datetime\n\nnow = datetime.now()\nprint(\"MODEL SAVED AT {}\".format(now))\nmodel_name = \"SVC-whats-cooking-trial-final2-{}.pickle.dat\".format(now)\npickle.dump(clf, open(model_name, \"wb\"))","d1da9076":"# clf = pickle.load(open(\"SVC-whats-cooking-trial-final2-{}.pickle.dat\", \"rb\"))","bd017dc0":"y_pred = clf.predict(test)","ac6cef05":"my_submission = pd.DataFrame({'id':test_ids})\nmy_submission['cuisine'] = y_pred\nnow = datetime.now()\nmy_submission.to_csv('submission_{}.csv'.format(now), index=False)\nprint('Saved file to disk as submission_{}.csv.'.format(now))","fa7b306b":"#### 25 LEAST USED INGREDIENTS- CUISINE WISE","79864da2":"#### 25 MOST USED INGREDIENTS- CUISINE WISE","629e941b":"# EDA","8c9704d2":"There are 20 different types of cuisine to classify. It gives an intuition that certain groups of cuisine may have much more similarity than others. We can try to find such groups as well","17fbb359":"## TFiDF Vectorizer","a9dbd338":"### Adding cluster as a feature\n","29b06d59":"In order to visualize clusters, let us reduce the data using PCA","33799e52":"### Presenting a solution to get into top 7% of leaderboard using Support Vector Classifier with an accuracy score of 0.81063","46352a90":"### NOTE: Don't add cluster for best results. (Skip this section. Move to Model Development)","b68d7f56":"### String Preprocess","f75a9f12":"**CLUSTER 1: <br>**\n> GREEK<br>\n> SPANISH<br>\n> ITALIAN<br>\n> FRENCH<br>\n> MOROCCAN<br>\n> RUSSIAN<br>\n\n<br><br>\n**CLUSTER 2: <br>**\n> FILIPINO<br>\n> CHINESE<br>\n> THAI<br>\n> VIETNAMESE<br>\n> KOREAN<br>\n\n<br><br>\n**CLUSTER 3: <br>**\n> SOUTHERN US<br>\n> INDIAN<br>\n> JAMAICAN<br>\n> MEXICAN<br>\n> BRITISH<br>\n> CAJUN CREOLE<br>\n> BRAZILIAN<br>\n> JAPANESE<br>\n> IRISH<br>","c9b14421":"WCSS shows number of clusters = 19 can be an apt choice (elbow point)","25ff59a1":"We can notice there are 3 clusters of cuisines","d3330814":"## Submission","eaab07e0":"WCSS for reduced cuisine dataset shows that number of clusters = 3 should be an apt choice (elbow point)","d81996b6":"### Creating ingredients per cuisine","53baaa0f":"## Linear SVC","d0f585be":"<center><img src=\"https:\/\/media3.s-nbcnews.com\/j\/newscms\/2019_41\/3044956\/191009-cooking-vegetables-al-1422_ae181a762406ae9dce02dd0d5453d1ba.nbcnews-fp-1200-630.jpg\" alt=\"Cooking Image from Google\"><\/center>","321c0a56":"## Count Vectorizer","fbf26e15":"## Creating Actual Clusters ","8436556d":"# Preprocessing","714ccd19":"# Model Development","e967f98e":"# FERTIG","7a1884f6":"## Cluster as a parameter"}}