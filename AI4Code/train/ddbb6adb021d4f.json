{"cell_type":{"f6e7e41e":"code","946a24d3":"code","d995675e":"code","435e454d":"code","cc8e0056":"code","183ff651":"code","ec2e0a44":"code","d4318b94":"code","438abe9b":"code","9ac0d60c":"code","5c3631cb":"code","754e5255":"code","b52b040a":"code","07fb6dd4":"code","be2e2ae3":"markdown","e4fc9c75":"markdown","6698c220":"markdown","83ec862d":"markdown"},"source":{"f6e7e41e":"import numpy as np\nimport pandas as pd\nimport os","946a24d3":"df = pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')\ndf.head()","d995675e":"df.shape","435e454d":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","cc8e0056":"le = LabelEncoder()","183ff651":"ds = df.apply(func=le.fit_transform) # this will be applied to each column : meaning of apply\nds.head()","ec2e0a44":"data = ds.values # becomes a list\ndata","d4318b94":"X = data[:, 1:] # all colsexcept 1st\ny = data[:, 0] # all rows, just 0 col\nX.shape, y.shape","438abe9b":"class CustomNB:\n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n        \n    # label = which class you want this prob for\n    def prior_prob(self, label): \n        total = self.y_train.shape[0]\n        class_examples = np.sum(self.y_train == label)\n        return class_examples \/ float(total)  # python 2\n    \n    # P(Xi=red|y=label) - ith feature (feature col = i) for a single example\n    def conditional_prob(self, feature_col, feature_val, label):\n        # out of all the examples, what mushrooms have feature as feature_val in the feature_col that belongs to that class label\n        X_filtered = self.X_train[self.y_train==label] # all the examples in class label\n        numerator = np.sum(X_filtered[:, feature_col] == feature_val)\n        denominator = len(X_filtered)\n        return numerator \/ denominator\n    \n    # we are going to do this for all the 22 features that we have for each example\n    def predict_point(self, X_test):\n        # X_test is a single example with n features\n        classes = np.unique(self.y_train) # By default from 0\n        n_features = self.X_train.shape[1]\n        post_pro = []\n        # post prob for each class\n        for label in classes:\n            # post_prob = prior * likelihood\n            likehood = 1.0\n            for feature in range(n_features):\n                cond = self.conditional_prob(feature, X_test[feature], label)\n                likehood *= cond\n            prior = self.prior_prob(label)\n            post = prior * likehood\n            post_pro.append(post)\n        \n        # ans = max value from all labels\n        return np.argmax(post_pro) # return the index of the largest value in array\n    \n    def predict(self, X_test):\n        result = []\n        for point in X_test:\n            result.append(self.predict_point(point))\n        return np.array(result)\n    \n    def score(self, X_test, y_test):\n        return (self.predict(X_test) == y_test).mean()","9ac0d60c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","5c3631cb":"model = CustomNB()\nmodel.fit(X_train, y_train)","754e5255":"model.predict(X_test[:10])","b52b040a":"y_test[:10]","07fb6dd4":"ans = model.score(X_test, y_test)\nprint('Accuracy of the custom Naive Bayes model is :',ans * 100)","be2e2ae3":"# Custom Naive Bayes","e4fc9c75":"### Splitting Data","6698c220":"# Accuracy","83ec862d":"# Preprocessing - convert to numeric data"}}