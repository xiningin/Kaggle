{"cell_type":{"2c4ce71d":"code","5461b381":"code","a6108a89":"code","46b96321":"code","1de9db1a":"code","2e98155a":"code","cc11c83f":"code","6b1a0195":"code","124274fd":"code","751a5cc7":"code","de761128":"code","6ec58472":"code","b7f28041":"code","54f66855":"code","21be3a83":"code","83fe938b":"code","8bb2bd50":"code","6e27178e":"code","c7c57c1d":"code","8c447eb4":"code","da4c837c":"code","a16542b2":"code","3dfad3fa":"code","efb0343b":"code","e06a37f4":"code","75bf2167":"code","f4abfb1b":"code","74cce6d1":"code","8ca4cac1":"code","502dc583":"code","8f7cf372":"code","ecb666be":"code","f9778891":"code","6e2be639":"code","95a0c53a":"code","5ad77b62":"code","e52ad923":"code","48ae2ac6":"code","8778d5df":"code","20cc2d2b":"code","20822198":"code","b0edc7de":"code","93ce55a5":"code","0591d59b":"code","2b258eb2":"code","ac630a7a":"code","992244a6":"code","32ae7a88":"code","51bfa024":"code","387c36f8":"code","14af6ee5":"code","9a071bb1":"code","ea53860d":"code","b5e5b91b":"code","41d6cc63":"code","51f03363":"code","e74c1473":"code","b62428fd":"code","7e2785eb":"code","964f4f07":"code","b8cbc71b":"code","294aacc9":"code","126d6307":"code","58d1cb9b":"code","d4557e87":"code","7f34c4bf":"code","fd6096c4":"code","6eaf823e":"code","cb4eb0f7":"code","baf772b4":"code","b0ed7445":"code","f3df6274":"code","db0580ec":"code","62a0eb1b":"code","294c3c01":"code","6f7d3239":"code","a2cf65a7":"code","f10c5dc6":"code","42eea1e7":"code","25cb37a9":"code","1fa812fc":"code","a6252ffe":"code","f9a51ce3":"code","87263e35":"code","13b462cd":"code","d340897f":"code","344fd9c9":"code","e82ad28f":"code","af9ce8ec":"code","ffa1308d":"code","bcadf7f3":"code","a0466b9c":"code","29844bcb":"code","3c4b4520":"code","127bc562":"code","5427ace6":"code","c9696658":"code","cbe6787a":"code","c4e9b36f":"code","879a19d0":"code","68bf7d31":"code","77309aa4":"code","f6b3ec75":"code","91ddc345":"code","db0d7fec":"markdown","5c412580":"markdown","f03cbc01":"markdown","f4827f45":"markdown","073c172a":"markdown","5a4533a5":"markdown","e90669b6":"markdown","999ef522":"markdown","4fbf27cb":"markdown","1e603227":"markdown","e1537ba4":"markdown","495ef40c":"markdown","905f4e9a":"markdown","6b033b18":"markdown","e1c84078":"markdown","9fefcafb":"markdown","40c7c7d3":"markdown","e2c85540":"markdown","4ac4da9d":"markdown","1a37cbcf":"markdown","9facbe14":"markdown","95269418":"markdown","d3a1af53":"markdown","bdaf43b5":"markdown","5992de29":"markdown","dcf351e7":"markdown","1f33ebf6":"markdown","68cb4de4":"markdown","8446996c":"markdown","f8d68b1b":"markdown","7e80a6a6":"markdown","834b2486":"markdown","9df40fe0":"markdown","61efe1a2":"markdown","38aba86f":"markdown","828fc8ac":"markdown","7fb2312d":"markdown","06563a2a":"markdown","03d18e18":"markdown","d76905f5":"markdown","07d97417":"markdown","332865e4":"markdown","3f02f4b9":"markdown","6c7c6c27":"markdown","8ed7e97d":"markdown","c8f0577b":"markdown","1b360832":"markdown","5389282e":"markdown","413f4dc1":"markdown","ba099e83":"markdown","78c96b03":"markdown","b86de81c":"markdown","6af98a9f":"markdown","5ee92823":"markdown","d1c65c2c":"markdown","843678d9":"markdown","369c5e53":"markdown","065b4646":"markdown","151bd445":"markdown","fb9880b3":"markdown","b30be3ce":"markdown","dfe9cf60":"markdown","4b2d5c37":"markdown","ba828de1":"markdown","a0f34f8d":"markdown","52772a45":"markdown","a0b88e63":"markdown","25dfeab4":"markdown","602ef330":"markdown","c2727741":"markdown","ca56a295":"markdown","78633e37":"markdown","7d4002f4":"markdown","c02dd49e":"markdown","81cb78aa":"markdown","4c64181a":"markdown","e5cfaa06":"markdown","0f2b1ce9":"markdown","88ffadf0":"markdown","cbb3b45f":"markdown","874118c4":"markdown","61cd5400":"markdown","dce042eb":"markdown","53d813a8":"markdown","3a0228c3":"markdown","0e02f2c3":"markdown","4e57146f":"markdown","63835b17":"markdown","4823a5e1":"markdown","b178463e":"markdown","07f8bd9e":"markdown","5e9485da":"markdown","0b1c442f":"markdown","112b5475":"markdown"},"source":{"2c4ce71d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport gc   # To clean up dataframes\nimport warnings   # Hate these\nwarnings.filterwarnings('ignore')","5461b381":"alcdata = pd.read_csv(\"..\/input\/iiitb-ai511ml2020-assignment-1\/Assignment\/alcoholism\/student-mat.csv\")\nfifadata = pd.read_csv(\"..\/input\/iiitb-ai511ml2020-assignment-1\/Assignment\/fifa18\/data.csv\")","a6108a89":"# Never hurts to look at all the data first\n\nprint(alcdata.info())\nalcdata.sample(10)   # Yes I am not using df.head(), so as to look at random data","46b96321":"# I like green and yellow :-) ... sorry\ncorrelations = alcdata.corr().round(2)\nplt.figure(figsize=(12., 12.))\nsns.heatmap(correlations, cmap='viridis', vmin=-1, annot=correlations)\nplt.title(\"Correlation matrix of all the non-categorical data\")\nplt.show()","1de9db1a":"print(f\"Mean of G1: {alcdata.G1.mean(axis=0)}\")\nprint(f\"Mean of G2: {alcdata.G2.mean(axis=0)}\")\nprint(f\"Mean of G3: {alcdata.G3.mean(axis=0)}\")","2e98155a":"# Get the mean for every student\nalcdata['overall_grade'] = alcdata[['G1', 'G2', 'G3']].mean(axis=1)\n\n# Remove the individual grades, in another dataframe, thus preserving the original data\nalcdata_modified = alcdata.drop(columns=['G1', 'G2', 'G3'])\n\n# Go ahead and plot the correlations now\ncorrelations = alcdata_modified.corr().round(2)\nplt.figure(figsize=(14., 12.))\nsns.heatmap(correlations, cmap='viridis', vmin=-1, annot=correlations)\nplt.title(\"Correlation matrix of all the non-categorical data\")\nplt.show()","cc11c83f":"# First find out all the categorical columns and then find the unique values\n\n# I needed a loop for this, but it is only for selecting a columns, so no trouble with performance!\nfor col in alcdata.select_dtypes('object').columns:\n    print(alcdata[col].value_counts(), end='\\n\\n')","6b1a0195":"# Ok so let me make a list of most of the binary columns. These can be label encoded as: yes -> 1; no -> 0\nbinary_encoded = ['romantic', 'internet', 'higher', 'nursery', 'activities', 'paid', 'famsup', 'schoolsup']\n\n# Now a list of columns, which needs to be one hot encoded. This is remove any sort of ordering\ncategorical = ['school', 'sex', 'address', 'famsize', 'Mjob', 'Fjob', 'reason', 'guardian', 'Pstatus']\n\n# Some of the columns were already Label Encoded, I need to convert them to categorical data\npre_label_encoded = ['Medu', 'Fedu', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health']","124274fd":"for col in pre_label_encoded:\n    alcdata_modified[col] = alcdata_modified[col].astype('category')\n    \n    if col in alcdata.columns:\n        alcdata[col] = alcdata[col].astype('category')","751a5cc7":"sns.boxplot(data=alcdata, x='reason', y='overall_grade')\nplt.show()","de761128":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\nsns.boxplot(data=alcdata, x='Walc', y='overall_grade', ax=ax1)\nsns.boxplot(data=alcdata, x='Dalc', y='overall_grade', palette='viridis', ax=ax2)","6ec58472":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\nsns.boxplot(data=alcdata, x='Medu', y='overall_grade', ax=ax1)\nsns.boxplot(data=alcdata, x='Fedu', y='overall_grade', palette='viridis', ax=ax2)","b7f28041":"# Let me get the correlations for the encoded famrel and Pstatus\n\nfor i in range(1, 6):\n    print(f\"Correlation of (famrel == {i}) with overall grades: {ordered[np.where(ordered[:, 1] == 'famrel_'+str(i))[0][0], 0]}\")\n\nprint()\nfor i in ['T', 'A']:\n    print(f\"Correlation of (Pstatus == {i}) with overall grades: {ordered[np.where(ordered[:, 1] == 'Pstatus_'+i)[0][0], 0]}\")","54f66855":"plt.figure(figsize=(14,7))\nsns.boxplot(data=alcdata, x='famrel', y='overall_grade')\nplt.show()","21be3a83":"plt.figure(figsize=(14,7))\nsns.boxplot(data=alcdata, x='Pstatus', y='overall_grade')\nplt.xticks(range(2), labels=['Living apart', 'Living together'])\nplt.show()","83fe938b":"# First get all the column names\n\nalcdata.drop(columns=['G1', 'G2', 'G3'], inplace=True)\nalcdata.columns.values","8bb2bd50":"plt.hist(alcdata.overall_grade, histtype='step')\nplt.title('Distribution of grades')\nplt.show()","6e27178e":"for feature in alcdata.select_dtypes('int').columns.values:\n    plt.figure()\n    sns.countplot(alcdata[feature])\n    plt.title(feature.capitalize())\n    plt.show()","c7c57c1d":"print(fifadata.info())\npd.set_option('max_columns', None)\nfifadata.sample(5)","8c447eb4":"# Removing some irrelavant columns, along with the positions of the players\n# I don't really know what they mean\ndrop_cols = ['Unnamed: 0', 'ID', 'Photo', 'Flag', 'Club Logo',\n             'LS', 'ST', 'RS', 'LW', 'LF', 'CF', 'RF', 'RW', 'LAM',\n           'CAM', 'RAM', 'LM', 'LCM', 'CM', 'RCM', 'RM', 'LWB', 'LDM', 'CDM',\n           'RDM', 'RWB', 'LB', 'LCB', 'CB', 'RCB', 'RB', 'Real Face', 'Jersey Number']\n\nfifadata.drop(columns=drop_cols, inplace=True)","da4c837c":"price_cols = ['Value', 'Wage', 'Release Clause']\nfifadata[price_cols].fillna({'Release Clause': '$0K', 'Wage': '$0K', 'Value': '$0K'}, inplace=True)\n\n# Convert currency into floats, with proper scaling for number of zeros\ndef price_process(x):\n    if type(x) == str:\n        x = x.strip()\n        if x.endswith('K'):\n            return x[1:-1]\n        elif x.endswith('M'):\n            return str(float(x[1:-1]) * 1000)\n        else:\n            return x[1: ]\n    elif type(x) == float:\n        return x\n    else:\n        return 0\n\n# Convert weight from objects to floats\ndef weight_process(x):\n    if type(x) == str:\n        x = x.strip()\n        if x.endswith('lbs'):\n            return x.replace('lbs', '')\n    elif type(x) == float:\n        return x\n    else:\n        return 0\n\n# Convert height from feets + inches to inches only \ndef height_process(x):\n    if type(x) == str:\n        x = x.strip()\n        if \"'\" in x:\n            temp = [int(t) for t in x.split(\"'\")]\n            return temp[0]*12+temp[1]\n    elif type(x) == float:\n        return x\n    else:\n        return 0\n    \n# Get year from date string\ndef date_process(x):\n    if type(x) == str:\n        x = x.strip()\n        if \",\" in x:\n            return x.split(',')[-1].strip()\n        elif x.startswith('2'):\n            return x\n        else:\n            return \"0\"\n    else:\n        return \"0\"\n    \nfor col in price_cols:\n    fifadata[col] = fifadata[col].apply(price_process)\n    fifadata[col] = fifadata[col].astype('float')\n    \nfifadata.Weight = fifadata.Weight.apply(weight_process).astype('float')\nfifadata.Height = fifadata.Height.apply(height_process).astype('float')\nfifadata.Joined = fifadata.Joined.apply(date_process).astype('int')\nfifadata['Contract Valid Until'] = fifadata['Contract Valid Until'].apply(date_process).astype('int')\n\nfifadata['Contract Valid Until'].fillna(fifadata['Contract Valid Until'].mean(), inplace=True)\nfifadata['Joined'].fillna(fifadata.Joined.mean(), inplace=True)\nfifadata['Loaned From'].fillna('None', inplace=True)","a16542b2":"pd.set_option('max_columns', None)\nfifadata.sample(5)","3dfad3fa":"temp = fifadata[['Club', 'Overall', 'Potential', 'Value', 'International Reputation', 'Work Rate', 'Wage']]\ntemp['Contract_Duration'] = fifadata.loc[:, 'Contract Valid Until'] - fifadata.loc[:, 'Joined']\n\ntemp['Work Rate'] = temp['Work Rate'].astype('category')\ntemp.dropna(subset=['Work Rate'], inplace=True)\ntemp['Work Rate'] = temp['Work Rate'].cat.codes","efb0343b":"temp['n_players'] = 1\ntemp = temp.groupby('Club').agg({\n    'Overall': 'mean',\n    'Potential': 'mean',\n    'Value': 'mean',\n    'International Reputation': 'mean',\n    'Work Rate': 'mean',\n    'Wage': 'mean',\n    'Contract_Duration': 'mean',\n    'n_players': 'sum'\n})\n\ntemp['Wage_per_player'] = temp.Wage \/ temp.n_players\ntemp.drop(columns=['Wage', 'n_players'], inplace=True)\n\ntemp.sample(5)","e06a37f4":"temp.sort_values(by=['Overall', 'Potential', 'Value', 'Wage_per_player', 'International Reputation', 'Contract_Duration'],\n                ascending=[False, False, False, True, False, False]).head(10)","75bf2167":"plt.figure(figsize=(10,8))\nsns.lineplot(data=fifadata, y='Potential', x='Age', legend='full')\nplt.title(\"Relationship between a player's potential and age\")\nplt.show()","f4abfb1b":"plt.figure(figsize=(10,8))\nsns.lineplot(data=fifadata, y='Value', x='Age', legend='full')\nplt.title(\"Relationship between a player's value and age\")\nplt.ylabel(\"Value in million euros\")\nplt.show()","74cce6d1":"kwargs = {'alpha': 0.8, 'color': 'k', 'linestyle':'--'}\n\nplt.figure(figsize=(16,8))\nsns.lineplot(data=fifadata, y='SprintSpeed', x='Age')\nsns.lineplot(data=fifadata, y='Stamina', x='Age')\nplt.title(\"Estimation of a player's pace and age\")\nplt.ylabel(\"Amplitude\")\nplt.legend(['Sprint Speed', 'Stamina'])\nplt.xticks(range(14, 46, 1))\nplt.axvline(26, **kwargs)\nplt.axvline(27, **kwargs)\nplt.show()","8ca4cac1":"cols = ['Age', 'Overall', 'Value', 'Wage', 'Special',\n       'International Reputation', 'Weak Foot', 'Skill Moves',\n       'Crossing', 'Finishing', 'HeadingAccuracy',\n       'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy',\n       'LongPassing', 'BallControl', 'Acceleration', 'SprintSpeed',\n       'Agility', 'Reactions', 'Balance', 'ShotPower', 'Jumping',\n       'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions',\n       'Positioning', 'Vision', 'Penalties', 'Composure', 'Marking',\n       'StandingTackle', 'Potential']","502dc583":"from sklearn.preprocessing import MinMaxScaler\n\nfifadata[cols] = MinMaxScaler().fit_transform(fifadata[cols])","8f7cf372":"# I could make out better from the weird color scheme\ncorrelations = fifadata[cols].corr().round(3)\nplt.figure(figsize=(24., 12.))\nsns.heatmap(correlations, cmap='hot', vmin=-1, annot=correlations)\nplt.title(\"Correlation matrix of all possible relevant features\")\nplt.show()","ecb666be":"plt.figure(figsize=(16,8))\nsns.lineplot(data=fifadata, y='Overall', x='Potential')\nsns.lineplot(data=fifadata, y='Reactions', x='Potential')\nsns.lineplot(data=fifadata, y='Composure', x='Potential')\nsns.lineplot(data=fifadata, y='ShortPassing', x='Potential')\nsns.lineplot(data=fifadata, y='Special', x='Potential')\nplt.title(\"Estimation of what affects a player's potential\")\nplt.ylabel(\"Amplitude\")\nplt.legend(['Overall', 'Reactions', 'Composure', 'ShortPassing', 'Special'])\nplt.show()","f9778891":"plt.figure(figsize=(16,8))\nsns.lineplot(data=fifadata, y='International Reputation', x='Wage')\nsns.lineplot(data=fifadata, y='Reactions', x='Wage')\nsns.lineplot(data=fifadata, y='Overall', x='Wage')\nsns.lineplot(data=fifadata, y='Potential', x='Wage')\nsns.lineplot(data=fifadata, y='Composure', x='Wage')\nplt.title(\"Estimation of what affects a player's wage\")\nplt.ylabel(\"Amplitude\")\nplt.legend(['International Reputation', 'Reactions', 'Overall', 'Potential', 'Composure'])\nplt.show()","6e2be639":"temp = fifadata[['Club', 'Age']].groupby('Club').agg([('Age_Min', 'min'), \n                                                     ('Age_Mean', 'mean'),\n                                                     ('Age_Max', 'max')])\ntemp.columns = temp.columns.droplevel(0)\ntemp = temp.reset_index().sort_values(by='Age_Mean')\n\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(16, 10))\nsns.countplot(data=temp, x='Age_Min', ax=ax1)\nax1.set_title('Minimum age of players across clubs')\n\nsns.countplot(data=temp, x='Age_Max', ax=ax2)\nax2.set_title('Maximum age of players across clubs')\n\nsns.boxplot(data=temp, x='Age_Mean', ax=ax3)\nax3.set_title('Average age of players across clubs')\n\nplt.tight_layout()\nplt.show()","95a0c53a":"temp = fifadata[['Club', 'Age']]\ntemp['Ave_Age'] = 0\n\ndef filtfunc(x):\n    club_sel = fifadata[fifadata['Club'] == x][['Age']]\n    age_sel = club_sel[club_sel.Age == club_sel.Age.min()]\n    return age_sel.count()\n\ntemp = temp.groupby('Club').agg({'Age': [('Min_Age', 'min')], 'Ave_Age': 'mean'})\ntemp.columns = temp.columns.droplevel()\ntemp = temp.reset_index()\ntemp['n_Min_Age'] = temp['Club'].apply(lambda x: filtfunc(x))\n\ntemp.reset_index().sort_values(by=['Min_Age', 'n_Min_Age'], ascending=[True, False])","5ad77b62":"temp.reset_index().sort_values(by=['n_Min_Age', 'Min_Age'], ascending=[False, True])","e52ad923":"accidata1 = pd.read_csv(\"..\/input\/iiitb-ai511ml2020-assignment-1\/Assignment\/accidents\/accidents_2005_to_2007.csv\")\naccidata2 = pd.read_csv(\"..\/input\/iiitb-ai511ml2020-assignment-1\/Assignment\/accidents\/accidents_2009_to_2011.csv\")\naccidata3 = pd.read_csv(\"..\/input\/iiitb-ai511ml2020-assignment-1\/Assignment\/accidents\/accidents_2012_to_2014.csv\")","48ae2ac6":"#enter code\/answer in this cell. You can add more code\/markdown cells below for your answer. \naccidata = pd.concat([accidata1, accidata2, accidata3])\naccidata.shape, accidata1.shape, accidata2.shape, accidata3.shape","8778d5df":"del accidata1, accidata2, accidata3\ngc.collect()\n\n# The output column is categorical, not an integer. So typecast.\naccidata[['Accident_Severity']] = accidata[['Accident_Severity']].astype('category')\n\n# Paranthesis aren irritating, remove them!\n# Also shorten the extremely elaborate column names\naccidata = accidata.rename({'Location_Easting_OSGR': 'Easting_OSGR',\n                            'Location_Northing_OSGR': 'Northing_OSGR',\n                            'Local_Authority_(District)': 'Local_Authority_District',\n                           'Local_Authority_(Highway)': 'Local_Authority_Highway',\n                           'Did_Police_Officer_Attend_Scene_of_Accident': 'Officer_Attend',\n                           'Pedestrian_Crossing-Human_Control': 'PC_Human_Control',\n                           'Pedestrian_Crossing-Physical_Facilities': 'PC_Physical_Facilities',\n                           'Special_Conditions_at_Site': 'Special_Conditions'}, axis=1)\naccidata.info()","20cc2d2b":"accidata.Junction_Detail.unique()","20822198":"accidata.drop('Junction_Detail', axis=1, inplace=True)","b0edc7de":"accidata.sample(5)","93ce55a5":"temp = accidata[['Day_of_Week', 'Number_of_Casualties']].groupby('Day_of_Week').sum().reset_index()\ntemp = temp.sort_values('Number_of_Casualties', ascending=False)\ntemp","0591d59b":"assert temp.Number_of_Casualties.sum() == accidata.Number_of_Casualties.sum()","2b258eb2":"accidata[['Day_of_Week', 'Speed_limit']].groupby('Day_of_Week').agg([np.max, np.min]).reset_index()","ac630a7a":"# First of all these are categorical values, so let me find out all the unique values\nprint(accidata.Light_Conditions.value_counts(), end=\"\\n\\n\")\nprint(accidata.Weather_Conditions.value_counts(), end=\"\\n\\n\")\n\nprint(\"Count of accidents, grouped by severity level\")\nprint(accidata.Accident_Severity.value_counts())","992244a6":"temp = accidata[['Light_Conditions', 'Accident_Severity']].dropna()\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('Light_Conditions').agg(np.count_nonzero).reset_index()\ntemp","32ae7a88":"cat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\ntemp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp.Light_Conditions, 'o', markersize=12)\nplt.grid()\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"Light Conditions\")\nplt.show()\n\ntemp","51bfa024":"temp = accidata[['Weather_Conditions', 'Accident_Severity']].dropna()\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('Weather_Conditions').agg(np.count_nonzero).reset_index()\ntemp","387c36f8":"cat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\ntemp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp.Weather_Conditions, 'o', markersize=12)\nplt.grid()\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"Weather Conditions\")\nplt.show()\n\ntemp","14af6ee5":"# Take a look at the data\nprint(accidata.columns)\n\n# These columns didn't show up, so manually selecting them\naccidata[['Time',\n       'Local_Authority_District', 'Local_Authority_Highway', '1st_Road_Class',\n       '1st_Road_Number', 'Road_Type', 'Speed_limit',\n       'Junction_Control', '2nd_Road_Class', '2nd_Road_Number',\n       'PC_Human_Control']].sample(10)","9a071bb1":"accidata.Accident_Severity.value_counts()","ea53860d":"latlong_OSGR = accidata[['Easting_OSGR', 'Northing_OSGR', 'Latitude', 'Longitude']].corr()\nsns.heatmap(latlong_OSGR, annot=latlong_OSGR,\n            vmin=-1, linewidths=0.8)","b5e5b91b":"plt.figure(figsize=(20, 8))\nsns.countplot(data=accidata, x='Police_Force', hue=\"Accident_Severity\", palette='viridis')\nplt.show()","41d6cc63":"# Proving that there are just too many dates to be useful\n# Would be useful if I was doing a time-series analysis\n\naccidata.Date.unique().shape, accidata.Time.unique().shape","51f03363":"accidata['Week_Of_Year'] = accidata[['Date']].apply(lambda x: pd.to_datetime(x, format=\"%d\/%m\/%Y\"))\naccidata['Week_Of_Year'] = accidata['Week_Of_Year'].apply(lambda x: x.weekofyear)\n\naccidata['Quadrant'] = pd.to_datetime(accidata['Time'], format=\"%H:%M\").dt.hour\naccidata['Quadrant'] = pd.cut(accidata['Quadrant'], bins=[-1., 5., 11., 17., 23.], labels=['q1', 'q2', 'q3', 'q4'])\naccidata['Quadrant'] = accidata['Quadrant'].astype('category')\n\naccidata[['Date', 'Week_Of_Year', 'Year', 'Time', 'Quadrant']].sample(5)","e74c1473":"temp = accidata[['Day_of_Week', 'Accident_Severity']].dropna()\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('Day_of_Week').agg(np.count_nonzero).reset_index()\ncat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\ntemp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp.Day_of_Week, 'o', markersize=12)\nplt.grid()\nplt.title('Severity of accidents grouped by road class')\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"Day of week\")\nplt.show()\n\ntemp","b62428fd":"accidata.Local_Authority_District.unique().shape, accidata.Local_Authority_Highway.unique().shape","7e2785eb":"accidata['1st_Road_Class'].unique(), accidata['2nd_Road_Class'].unique()","964f4f07":"accidata['1st_Road_Number'].unique().shape, accidata['2nd_Road_Number'].unique().shape","b8cbc71b":"temp = accidata[['1st_Road_Class', 'Accident_Severity']].dropna()\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('1st_Road_Class').agg(np.count_nonzero).reset_index()\ncat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\ntemp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp['1st_Road_Class'], 'o', markersize=12)\nplt.grid()\nplt.title('Severity of accidents grouped by road class')\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"1st_Road_Class\")\nplt.show()\n\ntemp","294aacc9":"temp = accidata[['2nd_Road_Class', 'Accident_Severity']].dropna()\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('2nd_Road_Class').agg(np.count_nonzero).reset_index()\ncat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\ntemp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp['2nd_Road_Class'], 'o', markersize=12)\nplt.grid()\nplt.title('Severity of accidents grouped by road class')\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"2nd_Road_Class\")\nplt.show()\n\ntemp","126d6307":"accidata.Road_Type.unique(), accidata.Road_Surface_Conditions.unique()","58d1cb9b":"temp = accidata[['Road_Type', 'Accident_Severity']].dropna()\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('Road_Type').agg(np.count_nonzero).reset_index()\ncat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\ntemp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp.Road_Type, 'o', markersize=12)\nplt.grid()\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"Road type\")\nplt.show()\n\ntemp","d4557e87":"temp = accidata[['Road_Surface_Conditions', 'Accident_Severity']].dropna()\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('Road_Surface_Conditions').agg(np.count_nonzero).reset_index()\ncat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\ntemp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp.Road_Surface_Conditions, 'o', markersize=12)\nplt.grid()\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"Road Surface Conditions\")\nplt.show()\n\ntemp","7f34c4bf":"accidata.Junction_Control.unique()","fd6096c4":"temp = accidata[['Junction_Control', 'Accident_Severity']].dropna()\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('Junction_Control').agg(np.count_nonzero).reset_index()\ncat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\ntemp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp.Junction_Control, 'o', markersize=12)\nplt.grid()\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"Junction control method\")\nplt.show()\n\ntemp","6eaf823e":"temp = accidata[['Speed_limit', 'Accident_Severity']].dropna()\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('Speed_limit').agg(np.count_nonzero).reset_index()\n\ntemp","cb4eb0f7":"cat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\ntemp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp.Speed_limit, 'o', markersize=12)\nplt.grid()\n\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"Speed limit\")\nplt.show()\n\ntemp","baf772b4":"print(accidata.PC_Physical_Facilities.value_counts(), end='\\n\\n')\nprint(accidata.PC_Human_Control.value_counts())","b0ed7445":"temp = accidata[['PC_Physical_Facilities', 'Accident_Severity']].dropna()\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('PC_Physical_Facilities').agg(np.count_nonzero).reset_index()\ntemp","f3df6274":"cat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\ntemp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp.PC_Physical_Facilities, 'o', markersize=12)\nplt.grid()\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"Pedestrian Crossing Physical Facilities\")\nplt.show()","db0580ec":"temp = accidata[['PC_Human_Control', 'Accident_Severity']].dropna()\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('PC_Human_Control').agg(np.count_nonzero).reset_index()\ntemp","62a0eb1b":"cat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\ntemp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp.PC_Human_Control, 'o', markersize=12)\nplt.grid()\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"Pedestrian Crossing Physical Facilities\")\nplt.show()","294c3c01":"accidata.Carriageway_Hazards.value_counts()","6f7d3239":"temp = accidata[['Carriageway_Hazards', 'Accident_Severity']].dropna()\ntemp = temp[temp.Carriageway_Hazards != 'None']\n\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('Carriageway_Hazards').agg(np.count_nonzero).reset_index()\n\ntemp","a2cf65a7":"cat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\ntemp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp.Carriageway_Hazards, 'o', markersize=12)\nplt.grid()\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"Carriageway related hazards\")\nplt.show()","f10c5dc6":"accidata.Special_Conditions.value_counts()","42eea1e7":"temp = accidata[['Special_Conditions', 'Accident_Severity']].dropna()\ntemp = temp[temp.Special_Conditions != 'None']\n\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('Special_Conditions').agg(np.count_nonzero).reset_index()\ncat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\ntemp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp.Special_Conditions, 'o', markersize=12)\nplt.grid()\nplt.axvline(x=0.01)\nplt.axvline(x=0.13)\nplt.axvline(x=0.87)\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"Existance of any special conditions at site\")\nplt.show()\n\ntemp","25cb37a9":"accidata.Urban_or_Rural_Area.value_counts()","1fa812fc":"temp = accidata[['Urban_or_Rural_Area', 'Accident_Severity']].dropna()\n\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('Urban_or_Rural_Area').agg(np.count_nonzero).reset_index()\ncat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\ntemp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp.Urban_or_Rural_Area, 'o', markersize=12)\nplt.grid()\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"Urban_or_Rural_Area\")\nplt.show()\n\ntemp","a6252ffe":"print(accidata.Officer_Attend.value_counts())\nprint(accidata.LSOA_of_Accident_Location.unique().shape)","f9a51ce3":"temp = accidata[['Officer_Attend', 'Accident_Severity']].dropna()\n\ntemp = pd.get_dummies(data=temp, columns=['Accident_Severity'])\ntemp = temp.groupby('Officer_Attend').agg(np.count_nonzero).reset_index()\ncat_cols = ['Accident_Severity_'+str(i) for i in range(1, 4)]\n#temp[cat_cols] = temp[cat_cols].div(temp[cat_cols].sum(axis=1), axis=0)\n\nplt.figure(figsize=(10, 8))\nplt.plot(temp[cat_cols], temp.Officer_Attend, 'o', markersize=12)\nplt.grid()\nplt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\"])\nplt.xlabel(\"Probabilty of accident\")\nplt.ylabel(\"Officer_Attend\")\nplt.show()\n\ntemp","87263e35":"from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split","13b462cd":"categorical_cols = ['Day_of_Week', 'Year', '2nd_Road_Class', 'Road_Type',\n                    'PC_Human_Control', 'PC_Physical_Facilities', 'Light_Conditions',\n                    'Road_Surface_Conditions', 'Officer_Attend', 'Quadrant',\n                   'Junction_Control', 'Weather_Conditions', 'Urban_or_Rural_Area']\n\ndrop_cols = ['Easting_OSGR', 'Northing_OSGR', 'LSOA_of_Accident_Location', '1st_Road_Number',\n            '2nd_Road_Number', 'Local_Authority_District', 'Local_Authority_Highway', 'Date', 'Time',\n            'Special_Conditions', 'Carriageway_Hazards', 'Accident_Index', '1st_Road_Class']\n\nnormalize_cols = ['Police_Force', 'Number_of_Vehicles', 'Number_of_Casualties']\nscale_cols = ['Longitude', 'Latitude', 'Speed_limit']\n\noutput = \"Accident_Severity\"\n\naccidata[normalize_cols] = accidata[normalize_cols].astype('float')\naccidata[scale_cols] = accidata[scale_cols].astype('float')\naccidata[output] = accidata[output].astype('category')\naccidata[categorical_cols] = accidata[categorical_cols].astype('category')","d340897f":"accidata[normalize_cols] = StandardScaler().fit_transform(accidata[normalize_cols])\naccidata[scale_cols] = MinMaxScaler().fit_transform(accidata[scale_cols])","344fd9c9":"accidata.drop(columns=drop_cols, inplace=True)\naccidata.drop_duplicates(inplace=True, ignore_index=True)","e82ad28f":"types = pd.api.types.CategoricalDtype(categories=\n                                      ['Missing', 'Automatic traffic signal', 'Giveway or uncontrolled', 'Stop Sign', 'Authorised person'])\naccidata.Junction_Control = accidata.Junction_Control.astype(types)\naccidata.Junction_Control.fillna('Missing', inplace=True)\n\naccidata.dropna(subset=['PC_Human_Control', 'PC_Physical_Facilities', 'Road_Surface_Conditions',\n                        'Officer_Attend', 'Weather_Conditions', 'Quadrant', 'Latitude', 'Longitude'],\n                how='any', inplace=True)","af9ce8ec":"for feature in categorical_cols:\n    print(f\"{feature}: {accidata[feature].unique()}\", end='\\n\\n')","ffa1308d":"accidata['Road_Type'].cat.rename_categories({\n    'Single carriageway': 'Single_Cway',\n    'Dual carriageway': 'Dual_Cway',\n    'One way street': 'One_Way',\n    'Slip road': 'Slip_Road'\n}, inplace=True)\n\naccidata['PC_Human_Control'].cat.rename_categories({\n    'None within 50 metres': 'None_LTE_50',\n    'Control by other authorised person': 'Auth_Person',\n    'Control by school crossing patrol': 'School_Person', \n}, inplace=True)\n\naccidata['PC_Physical_Facilities'].cat.rename_categories({\n    'No physical crossing within 50 meters': 'None_LTE_50',\n    'Pedestrian phase at traffic signal junction': 'Pedes_TSJnc',\n    'non-junction pedestrian crossing': 'Pedes_NJnc',\n    'Zebra crossing': 'Zebra',\n    'Central refuge': 'Central_Refuge',\n    'Footbridge or subway': 'Bridge_Subway'\n}, inplace=True)\n\naccidata['Light_Conditions'].cat.rename_categories({\n    'Daylight: Street light present': 'Daylight',\n    'Darkness: Street lights present and lit': 'Dark_Light_Lit',\n    'Darkeness: No street lighting': 'Dark_No_Light',\n    'Darkness: Street lighting unknown': 'Dark_Light_NA',\n    'Darkness: Street lights present but unlit': 'Dark_Light_Unlit'\n}, inplace=True)\n\naccidata['Road_Surface_Conditions'].cat.rename_categories({\n    'Flood (Over 3cm of water)': 'Flood'\n}, inplace=True)\n\naccidata['Junction_Control'].cat.rename_categories({\n    'Giveway or uncontrolled': 'Giveway',\n    'Automatic traffic signal': 'Automatic',\n    'Stop Sign': 'Stop',\n    'Authorised person': 'Auth_Person'\n}, inplace=True)\n\naccidata['Weather_Conditions'].cat.rename_categories({\n    'Fine without high winds': 'Fine_NHW',\n    'Fine with high winds': 'Fine_HW',\n    'Raining without high winds': 'Rain_NHW',\n    'Raining with high winds': 'Rain_HW',\n    'Snowing without high winds': 'Snow_NHW',\n    'Snowing with high winds': 'Snow_HW',\n    'Fog or mist': 'Fog_Mist'\n}, inplace=True)","bcadf7f3":"def getCompleteDataframe(data):\n    y = data[output]\n    encoder = OneHotEncoder()\n    temp = data[categorical_cols]\n\n    temp = pd.DataFrame(encoder.fit_transform(temp).toarray())\n    temp.columns = encoder.get_feature_names(categorical_cols)\n\n    data.drop(columns=categorical_cols + [output], inplace=True)\n    data = pd.concat([data.reset_index(), temp], axis=1)\n    \n    return data, y","a0466b9c":"X, y = getCompleteDataframe(accidata)\nX = X.drop(columns=['index']).reset_index()","29844bcb":"X.isna().sum().sort_values(ascending=False)","3c4b4520":"X.columns","127bc562":"# Very few rows belong to this category\nX.drop(['Urban_or_Rural_Area_3'], axis=1, inplace=True)","5427ace6":"train_X, test_X, train_Y, test_Y = train_test_split(X, y, \n                                                    test_size=0.1, random_state=43, shuffle=True)\n\ntrain_X.shape, test_X.shape, train_Y.shape, test_Y.shape","c9696658":"from sklearn.linear_model import LogisticRegressionCV","cbe6787a":"model = LogisticRegressionCV(cv=5, n_jobs=-1, verbose=1)\nmodel.fit(train_X, train_Y)","c4e9b36f":"model.score(test_X, test_Y)","879a19d0":"from sklearn.metrics import plot_confusion_matrix","68bf7d31":"plot_confusion_matrix(model, test_X, test_Y)\nplt.show()","77309aa4":"train_Y.value_counts(), train_Y.shape","f6b3ec75":"balanced_model = LogisticRegressionCV(cv=5, class_weight=\"balanced\",\n                                      n_jobs=-1, multi_class='multinomial')\n\nbalanced_model.fit(train_X, train_Y)","91ddc345":"print(f\"Accuracy score: {balanced_model.score(test_X, test_Y)}\")\n\nplot_confusion_matrix(balanced_model, test_X, test_Y)\nplt.show()","db0d7fec":"`Junction_Detail` has *0* non-null entries? Let me look at the data and drop the column if needed","5c412580":"\n### 5. What is the age distribution in different clubs? Which club has most players young?","f03cbc01":"Let me take a look at the relationship between OSGR and the Latitude-Longitude. Expect them to be very closely related.","f4827f45":"Wow, there is a 100% correlation between Longitude and the Easting; and Latitude and the Northing. This is to be expected, as they pretty much tell the same thing(one is on normal maps, the other on ordinal maps). I will be dropping the Ordinal map coordinates, because latitude and longitude are easier to work with.\n\nLet me go to the next column, the *Police_Force*.","073c172a":"# Part - 3\n## UK Road Accidents Data\n\n\nThe UK government amassed traffic data from 2000 and 2016, recording over 1.6 million accidents in the process and making this one of the most comprehensive traffic data sets out there. It's a huge picture of a country undergoing change.","5a4533a5":"Just a final check whether there is any more missing data in the dataset","e90669b6":"One final check of all the categories in the categorical features. Need to remove all the NaN values from the categorical data. One of the columns: `Junction_Control` contains too many missing items, so I will make a new category for all the missing data, instead of dropping the rows.\n\nBut first, let me remove all the irrelevant features","999ef522":"Wow, parents who have no education seem to have children who score the best! If score was a metric in the real world, this would be really great. Apart from this, students with more educated parents seem to score better on average(and in general too). A few outliers are present too, maybe students who have no real motivation to study.(no judgements)","4fbf27cb":"(Missing data alert!)","1e603227":"I value performance over money, so I will be sorting in that order","e1537ba4":"Well encoding everything is all fine and nice. But let me check for over-correlated columns and stuff. \n\nFirst I will drop the columns I have mentioned somewhere above.\n\nAfter that I will take a look at the VIF score","495ef40c":"### 3. On each day of the week, what is the maximum and minimum speed limit on the roads the accidents happened?","905f4e9a":"### 5. To predict the severity of the accidents which columns do you think are unnecessary and should be dropped before implementing a regression model. Support your statement using relevant plots and hypotheses derived from them.","6b033b18":"So the highest three features seem to be ```Overall```, ```Reactions``` and ```Composure```. None of the negative correlations seem to be strong enough to inversely affect the potential of a player. \n\nLet me get individual plots with each to confirm the top 3 features.","e1c84078":"First get all the column names.","9fefcafb":"Yep. Bye bye LSOA, I already got the latitude and longitude.","40c7c7d3":"Uh numbers instead of proper names? I don't know how to interpret that, so let me look at the plot and see if that makes any sense.","e2c85540":"One correlation matrix to answer them all.\n\nFrom the heatmap above, ```International Reputation```, ```Overall``` and ```Reactions```  seem to have a very high correlation with the ```Wage``` of a player. Why don't I consider ```Value```? Because a high wage pretty much leads to high value. Those are not traits of the footballer itself.\n\nLet me verify it once.","4ac4da9d":"The *Accident_Index* plays no role in the severity of the accident. Its a unique identifier that I would use for evaluation submissions maybe. But not needed right now.","1a37cbcf":"There is an observeable trend here. Uncontrolled\/Stop Signs are not really effective and lead to more severe accidents. What I will do with this column is one-hot encode it and then drop the column indicating the missing values. That would be preferabale to dropping all the rows with missing data\n\nA seemingly important one now: `Speed_limit`. This one is a bit tricky, because the speed limit is likely to vary depending on the time of day. This information is partly captured by my `Quadrant` column. But the number of bins(quadrants here) *might* affect the outcome. ","9facbe14":"Huh????? I'm beat. No idea why balancing the class weights would just shift over the predicted labels... Makes no sense to me.\n\nI give up due to lack of time. But this needs addressing.","95269418":"Let me scale all the data into proper ranges, as per what I have summarized in the huuuge table","d3a1af53":"Now start working on the actually finding what decides \"economical\". The features in the dataset relates to the performance of individual players individually. \n\nTo me economical means how much performance the club is getting out of the players, for every Euro that they are paying the players. For this I need metrics to estimate the value of each of the players. Another thing to see is the number of players in the club and how much they are paid on average. \n\nSo I'll look at the following parameters:\n1. ```Overall``` and ```Potential```: averaged over all the players in the club\n2. ```Value``` vs ```Wage```: again, averaged over all the players. Apart from this, the ```Release Clause``` is important too. I expect strong correlation with the value of the player.\n3. Number of players in the club.\n4. Average of the```International Reputation``` of each player in the club.\n5. ```Contract Duration```(```= Contract Valid Until - Joined```) matters too. If a player is good, then the club might want too retain them for longer. This feature needs to be created.\n5. I feel that the ```Work Rate``` of the player matters too. So I will probably label encode it and then take the mean.","bdaf43b5":"# Instructions\n1. We will be conducting the entire assignment through this notebook. You will be entering your code in the cells provided, and any explanation and details asked in markdown cells. \n2. You are free to add more code and markdown cells for describing your answer, but make sure they are below the question asked and not somewhere else. \n3. The notebook needs to be submitted on LMS. You can find the submission link [here](https:\/\/lms.iiitb.ac.in\/moodle\/mod\/assign\/view.php?id=13932). \n4. The deadline for submission is **11th October, 2020 11:59PM**.","5992de29":"So with greater stamina, the player is likely to be able to run more and longer. The peak speed is sort of an indication of how fast the player moves. These are incomplete without knowing how long a player moves, but they show **peak pace between 26-27 years of age.**","dcf351e7":"Ok, so there is skew in a lot of the features. Most of the categorical data also has quite a bit of class imbalance which is clearly revealed over here. Now since nothing can be done about the categorical skews, I am going to let them be. Let me list the imbalanced features for the truly integer fields:\n\n1. Age: This feature is really skewed in the sense that fewer older students are present. No skew removing technique can fix this really, but we can apply log here, but the left 'tail' of the distribution will never be from a Gaussian distribution.\n2. Travel time: A good majority of the students spend less time travelling. Nothing can be done to remove the skew and I think that is fine. The distribution looks like a decaying exponential here.(rather than a Gaussian)\n3. Study time: This one shows *some* resemblance with a Gaussian distribution. Since there are only 4 distinct values along the x axis, we can't really center the distribution. It will be fine I guess.\n4. Failures: Again, more of a decaying exponential than a Gaussian distribution. I **wouldn't** do anything about this one, because the orginal distribution tells us something: fewer number of students are likely to fail more number of times. I feel that this is something important and must be preserved.\n5. Absences: This one leaves a lot to be desired. It looks like one side of a Gaussian distribution. Will it work? I don't know\n\nHonestly, I don't really get why skew removal works. For MSE loss, the error terms must be following a Gaussian distribution for it to work. But skew removal will most certainly lead to loss of information about the original distribution of the data. Which is partly why I can't answer this question properly.","1f33ebf6":"Ok, so the three grades are strongly correlated(as is to be expected). Also, each of their correlations with all the other data seems to be exactly the same.\n\nSo let me try to combine them by taking their mean, which is like saying that the students got the same marks in the three subjects","68cb4de4":"### 6. Implement a basic Logistic Regression Model using scikit learn with cross validation = 5, where you predict the severity of the accident (Accident_Severity). Note that here your goal is not to tune appropriate hyperparameters, but to figure out what features will be best to use.","8446996c":"This column *is* useless. Drop it!","f8d68b1b":"### If you are still alive after going through this much nonsense, congrats\nYou really are awesome(no I am not trying to bribe you into giving me extra marks. In fact cut marks for making this so long) Its just irritating to do this much EDA(even though what I have done is *extremely* simplistic).\n\n**TLDR(of the last 40 cells or so)**: Chuck columns with too many categories, Simplify the date\/time columns. Use most of the rest, which affect a majority of the entries.","7e80a6a6":"I'm sorry for the following graph, but I can't make out the difference between the continuous colors of ```heatmap```. So I annotated with the values","834b2486":"Same story as `Carriageway_Hazards`. Will try with and without dropping it and check performance. I don't want to have a very sparse training data matrix. So I will check the VIF score later too.\n\nFeature `Urban_or_Rural_Area`. This one can be very important(logically). Let me verify.","9df40fe0":"# Part - 2\n## FIFA 2019  Data\n","61efe1a2":"So new young players aren't worth that much (budding talent). As they gain experience and become older, their value starts peaking. As they become older, their value starts decreasing as their potential decreases.\n\n--------------------\n\nTheir no direct columns indicating the pace of the player(how much they move around during a game). So I will use ```SprintSpeed``` and ```Stamina``` as a proxy to figure this out. I am getting away with not scaling the data, because these features are measured on the same scale initially. I do scale the data later on.","38aba86f":"Addition before submission: I could have used much better plots for the EDA, but I am completely worn down. Found this really nice way to [plot](https:\/\/matplotlib.org\/3.3.2\/gallery\/lines_bars_and_markers\/horizontal_barchart_distribution.html#sphx-glr-gallery-lines-bars-and-markers-horizontal-barchart-distribution-py), but a bit too late.","828fc8ac":"### Important point\n\nI think **Accident_Severity** type 3 is the *least* severe accident. I am drawing this conclusion from the fact that most of the accidents are of type 3, So the order is:\n- Accident_Severity == 1  are the **most** severe accidents\n- Accident_Severity == 2  are **moderately** severe accidents\n- Accident_Severity == 3  are the **least** severe accidents","7fb2312d":"Great! Now I have the the Week_Of_Year and Quadrant (of day) columns sorted out. The quadrant column will need categorical encoding later on.\n\nThe day of week seems to affect the severity of accidents. More, moderately severe accidents on weekends by the looks of it.\n\nOnwards to `Local_Authority_District` and `Local_Authority_Highway`. These happen to be categorical data. They are sort of area codes for the regions in which the cases were recorded. For the final prediction I **am** going to drop this column. It might be helpful to know where ab accident happened, which might tell something about the severity of the accident. But the number of unique values is just too much to encode.","06563a2a":"### 4. Which features directly contribute to the wages of the players?","03d18e18":"Ok, so ```Reactions```, ```Composure``` and ```ShortPassing``` are the better choice. Correlation doesn't work that well huh. Some features which have low correlation value, might affect the Potential more than my selected features do. Meh, I'm too tired to care.","d76905f5":"Accuracy is fine, but I want to see the confusion matrix for this, because this is a classification task","07d97417":"# Part - 1\n## Alcohol Consumption Data\nThe following data was obtained in a survey of students' math course in secondary school. It contains a lot of interesting social, gender and study information about students. \n","332865e4":"That is ... inconclusive. Towards the lower side of the x axis, the data makes sense. Lesser the number of police, the more the accidents. Towards the higher end, where there are more police, the number of accidents seems to spike up suddenly. There might be other factors at play, which I am not able to interpret.\n\n-------------------------\n\nNow about the date, I could keep it as it is, but the granularity of the data is just too much. So what I am going to do is convert it to the **week of year** and the **year**(which already exists). The same applies to the *Time* column too. So what I am going to do with the time is bin them into 6 hour bins.\n\nI don't think using even Week of Year as an integer column makes any sense at all. It is more of a categorical column, but I can't encode it at the risk of blowing up the total number of columns. So...I'll just let it be an integer(**but this is wrong**)","3f02f4b9":"Before I start any analysis, need to reduce the number of columns and process some of the features","6c7c6c27":"Moving on to the road class and road numbers","8ed7e97d":"This was as to be expected. A player's potential becomes lesser as (s)he becomes older. And then there are the superhuman outlier players, who show extreme talent near the age of 45. (As coaches?)","c8f0577b":"### Note from student\n--------------------------\n\n0. I used a private notebook on Kaggle for this. So the folder structure for the dataset follows the same pattern\n1. This notebook takes <10 minutes to run from start to end.\n2. It takes upto **10GB** of RAM to run(most of this is due to the last 10 cells or so, while training the Logistic regression model.\n3. I have tried to keep some of the comments light, so as to not bore you out totally with 300+ cells\n4. I worked on this over several days in different states of mind. So my grammer, plot colors and analysis shows a little fluctuation over cells.\n5. I wrote these just before submission. I'm sorry for the extreme length of the notebook and the relatively poor graphs.","1b360832":"Ok, so intuitively, better lighting conditions are definitely going to help *decrease* the number of accidents.\nAlso, the presence of rain and snow is more likely to increase the number of accidents.\n\nOne thing to note is that the raw numbers don't make a lot of sense by themselves. There are more number of total accidents reported during daytime. But that doesn't mean that these accidents are more severe. It makes better sense to look at the percentage of the severities of the accidents, over each light\/weather condition.\n\nLet me plot the results to see how it goes","5389282e":"Just something to note: LogisticRegression(CV) adds L2 normalization on default. This is not really what I want here, but ok.","413f4dc1":"Before I do that, let me check the mean grade in each subject, just to verify that students don't perform exceptionally poorly\/well in any one of the subjects","ba099e83":"Ok, we sort of see a trend here. Fewer students drink more and those who drink more are more likely to score lesser. However, a few among those who drink the most seem to perform better than people who drink less. So the booze does help somehow \ud83d\ude02(no I don't drink, thank you very much).","78c96b03":"While I am at it, let me look at the `Day_of_Week` column too","b86de81c":"### Encoding and dropping first\n----------------------------------\n\nSummary of what I am going to do with each feature\n\n|  Column     |   Conclusion   |\n|-----------------|---------------------------|\n|Accident_Index           |  separate(remove for training) |\n|Easting_OSGR             |  drop in favour of longitude  |\n|Northing_OSGR            |  drop in favour of latitude  |\n|LSOA_of_Accident_Location|  drop in favour of latitude-longitude  |\n|1st_Road_Number          |  drop due to too many categories  |\n|2nd_Road_Number          |  drop due to too many categories  |\n|Local_Authority_District |  drop due to too many categories  |  \n]Local_Authority_Highway  |  drop due to too many categories  |\n|Date                     |  *refactor into week of year* |\n|Time                     |  *refactor into quadrants*  |\n|Longitude                |  retain, min-max scaling? |\n|Latitude                 |  retain, min-max scaling? |\n|Police_Force             |  retain, normalize |\n|Number_of_Vehicles       |  retain, normalize |   \n|Number_of_Casualties     |  retain, normalize |\n|Speed_limit              |  retain, min-max scaling?  |   \n|Day_of_Week              |  retain, experiment with categorical encode |\n|Year                     |  retain, experiment with categorical encode |\n|1st_Road_Class           |  retain, categorical encode  |\n|2nd_Road_Class           |  retain, categorical encode  |\n|Road_Type                |  retain, categorical encode  |\n|PC_Human_Control         |  retain, categorical encode  |\n|PC_Physical_Facilities   |  retain, categorical encode  |\n|Light_Conditions         |  retain, categorical encode  |\n|Road_Surface_Conditions  |  retain, categorical encode  |\n|Officer_Attend           |  retain, categorical encode  |\n|Junction_Control         |  retain, categorical encode, drop None category|\n|Weather_Conditions       |  retain, categorical encode, drop specific category |\n|Special_Conditions       |  experiment with dropping as it affects <2% of dataset |\n|Carriageway_Hazards      |  experiment with dropping as it affects <2% of dataset |\n|Urban_or_Rural_Area      |  experiment with OHE  |\n|Accident_Severity        |  **to be predicted**  |","6af98a9f":"That's a green flag to go ahead with taking the mean of the scores!","5ee92823":"And that's a tragedy. Typical when there is a massive class imbalance. How do I know this? I checked it faaar above somewhere. Let me check it again, then train a new model, this time with adjusted class weights","d1c65c2c":"Yeah... horrible. Makes all the preprocessing useless. So let me set the ```class_weight``` to ```balanced```, which should take care of this.\n\nSide note: I tried a lot of different solvers and other normalizations and stuff. Didn't work.","843678d9":"### 2. What is the relationship between age and individual potential of the player? How does age influence the players' value? At what age does the player exhibit peak pace ?","369c5e53":"### 2. If there is a need for encoding some of the features,  how would you go  about it? \nWould you consider combining certain encodings together ?\n","065b4646":"### 2. What are the number of casualties in each day of the week? Sort them in descending order. ","151bd445":"Ok, so the `x_Road_Class` are categorical data. I will be taking a look at their relationship with the `Accident_Severity` in just a minute. `x_Road_Number` must be dropped however. The number of unique values is just too much to use effectively.","fb9880b3":"Time to group by clubs","b30be3ce":"### 1. Try to visualize correlations between various features and grades and see which features have a significant impact on grades. \nTry to engineer the three grade parameters (G1, G2 and G3) as one feature for such comparisons.\n\n","dfe9cf60":"Taking some of these to be binary encoded(label encoded) is a little bit risky. But I will try and make an educated guess. As it is, we already have a lot of columns, categorical encoding will only make it worse. \n\n\\[**Updated edit**\\]: after the question was changed, I removed my encodings and am going to look at a few scatter plots or something now.\n\nI am choosing the following features for analysis: ```Walc\/Dalc```, ```reason```, ```Medu\/Fedu``` features's effect on ```overall_grade```.","4b2d5c37":"### 3. What skill sets are helpful in deciding a player's potential? How do the traits contribute to the players' potential? ","ba828de1":"Ok, so I am going to do this for the dataset before one hot encoding(```alcdata``` DataFrame).","a0f34f8d":"Ok, there are no noticeable changes in the correlation plot, which is only to be expected, unless students performed much better in one single subject. This was verified by the averages that I calculated earlier.\n\n### Analysis time\n---------------------------\nThese are my *initial* analysis only. They are likely to change later on(and they do). **Please see the next part for the continued answer for this part**. This is only because the question was changed after I did all and I didn't have the heart to remove all this.\n\n#### Positive correlations\n1. The *Medu* and *Fedu* columns seem to show the highest (positive)correlation with the *overall_grade*. So the educational qualifications of parents\/gaurdians seems to matter here. The mother's education has a higher effect on the student's grade. Probably because she can help the student study better. Having educated parents might mean that they can help at home. Thi is evident from the significant **negative** correlation between *Medu\/Fedu* and *failures*.\n\n2. Interestingly, how long a student studies seems to matter less than the parents education. Anyways, studying longer does seem to help quite a bit.\n\n3. Family relations is poorly (positively) correlated with the grade of the student. Will explore this later on.\n\n4. Suprisingly, the amount of free time a student has seems to barely affect their performance. This *might* be indicative of that the fact they aren't really stressed out about anything (unlike us all). \n\n*(See cells below for further analysis on points 3 and 4)*\n\n\n#### Negative correlations\n1. The number of past *failures* seems to have a slighly negative correlation with the *overall_grade*. Again, to be expected, because the more number of times the student tried, the better they got at it. The same is applicable with the age of the student. The more times they failed, the older they got and hence got better over time.\n\n2. We also see a some negative correlation between the *traveltime* and the *overall_grade*. Again, expected. The lesser time you spend travelling, the more time you have for doing other things.\n\n3. People who spend more time going out with friends(*goout*), have a lower score. Expected, as they are loosing out on time.\n\n4. The weekday and weekend alcohol consumption seems to be weakly (negatively)correlated with the overall grade. The same applies to health. So alcohol consumption does **not** seem to have a very significant impact. One interesting thing to observe is the correlation between overall alcohol consumption and going out with friends. So friends seem to have a bad effect on each other. ","52772a45":"### 4. What is the importance of Light and Weather conditions in predicting accident severity? What does your intuition say and what does the data portray?","a0b88e63":"Again, this is counter-intuitive and not very conclusive. A smaller fraction of the most severe accidents seem to happen in snowy\/rainy conditions. The same is applicable to the moderately severe accidents too: relatively higher percentage of the accidents occur in *normal* conditions. I guess the numbers tell that an overwhelming majority of the accidents happened in normal conditions. One thing that I can do to capture the effect of weather conditions is to one hot encode the feature and then *drop the column corresponding to \"Fine without high winds\".*\n\nThis behaviour could be because fewer people drive in rainy\/snowy conditions. We don't know, but going ahead dropping both of these columns might be the better thing to do.","25dfeab4":"#### Improved observations\nSo the above analysis is not complete, by any means. I have missed out on the importance of all the categorical data. Further, some of the columns are actually categorical, but they were *label encoded* in the dataset. While that sort of works for the analysis, it is not acceptable for training the model. I suspect that this label encoding in the dataset is leading to incorrect correlations for some of the columns, like *famrel* and *freetime*.\n\nThe same is very likely for some of the negative correlations like the effect of alcohol consumption on the overall grades. My initial analysis seems to contradict common sense.\n\n---------------------------\nAnd yes, I will be using different types of encodings for each one of these, depending on the format of the data.","602ef330":"This looks like a very useful column! As it should, intuitvely. Higher the speed limit, higher the chance of a more severe accident.\n\nUp next, `PC_Physical_Facilities` and `PC_Human_Control`. I will also be taking a look at the raw numbers here, apart from the one plot that I have been using everywhere.","c2727741":"Let me look at the correlations before starting any sort of engineering.\nThis will give me a fair idea of what to expect after I combine the 3 grades.","ca56a295":"First off, I need to scale the features to the same range","78633e37":"Ok for this I am going to choose the three most important features which can be used to rate a player's performance. Then compare those 3 with the age of the players.\n\nFor choosing the features, I will use correlation, because it is numeric data. I have selectively removed a features directly, because I did this several times and ended up with a massive plot, which is just irritating. Removed quite a few features, which are strongly correlated with each other, without affecting the player's potential significantly.\n\nBefore doing these comparisons, I will scale all the data to the same \\[0, 1\\] range.","7d4002f4":"### 1. Which clubs are the most economical? How did you decide that?","c02dd49e":"For these two features, the raw numbers really tell the tale!. Having someone regulate the flow of traffic(`PC_Human_Control`) seems to help a lot.\n\nThe same is applicable fot `PC_Physical_Facilities` too. Having no corssing within 50m seems to increase the number of accidents a lot. People try to cross the roads where they are not supposed to.(they should learn from Indians). But anyways, this isn't a good thing and the numbers further prove that.\n\nOnto `Carriageway_Hazards` now. I expect this feature to not affect a majority of the entries. But lets see","81cb78aa":"Amazing! We see some good correlation here. I will be using these columns after categorical encoding.( I have no idea why `2nd_Road_Class` == 0 didn't show up in the plot. However it is present in the dataframe, that's all matters).\n\nHowever, having two road classes is sort of redundant and doesn't add a lot of value. So I will eventually drop ```1st_Road_Class```.\n\nWhile I am on Roads, let me look at `Road_Type` and `Road_Surface_Conditions`.","4c64181a":"First off, let me check how many accidents were of each of the different severity levels.(I figured this out after training the model.) For further analysis, scroll right to the end of the notebook.","e5cfaa06":"I printed the dataframe, because after divison by the mean, there are no type 1\/2 accidents when speed limit is 15 units. This felt weird, but it is true and its not wrong data. I mean its hard to cause an accident at such slow speeds right?","0f2b1ce9":"\n### 4. Figure out which features in the data are skewed, and propose a way to remove skew from all such columns. ","88ffadf0":"Ok, the youngest players are only 16 years old(yeah I wasted my life :-( ).\n\nThere are too many unique clubs to plot individual boxplots here. The boxplot for the mean age of the teams is interesting. There seem to be a few outlier teams with old players. Similarly there is one team with an average age of ~20.2 years only!\n\nNow for the club with the most number of young players. So here, the question was not clear to me, so I did two things: one is clubs with the most number of players aged 16 years. Another with the max number of players with the minimum age *within* the team. \n\nThe second one makes less sense, but meh, its easy to do, so why not.","cbb3b45f":"Aha!! ```International Reputation``` has a weird effect on a player's wages, the correlation values have misled me earlier. Based on these plots, my final answer would be, in order of importance: ```Composure```, ```Reactions``` and ```Potential``` . Why didn't I choose ```Overall```? Because it is highly correlated with the **potential of a player**.  ","874118c4":"The ```reason``` column tells us why a student choose the school they are in. Even in such a small dataset, there is a wide variety of grades for each of the categories. The interquartile range between 25% to 75% quite wide too. This makes this column a rather poor choice for deciding how a student will score.","61cd5400":"### 1. The very first step should be to merge all the 3 subsets of the data.","dce042eb":"Nonsense! We can't make a proper logical conclusion from this. Looks like calculating the correlation between encoded categorical data is not the right way to analyze the data.\n\n\nP.S.: this analysis was done long before I learnt that correlation doesn't work. Learnt it the hard way here.\n\nSo let me look at another way to find the relationship","53d813a8":"\n### 3. Try to find out how family relation(famrel) and parents cohabitation(Pstatus) affect grades of students. \n","3a0228c3":"This feature *also* makes perfect sense. But the problem is that the toal number of unique columns is large by now already. This particular feature affect a small minority(<2%) of the entire dataset. So I will most likely be dropping this column, just to decrease the overall dimensionality.\n\nLet me look at the `Special_Conditions_at_Site` column now.","0e02f2c3":"Again, we see some reasonable looking plots coming out of these columns. The severity of accidents in slip roads and roundabouts is more. Also, higher chance of more severe accidents in wet roads. So I am going to keep these columns and encode them later on.\n\nOne interesting thing to observe is that snowy weather conditions and snow filled roads don't seem to lead to more severe accidents. So my analysis was wrong. The `Weather_Condition` plot *does* make sense. So I will be keeping it.\n\nUp next, `Junction_Control`. I already know that there is missing data in this column. Let me figure out how to interpret this column after some analysis. (I am tired already:-( )","4e57146f":"Ok bad news.... looks like a *poorer* ```famrel```(family relationship) is leading to higher average ```overall_grade```. Similarly, Living apart seems to marginally improve average grades. Both of these are counter-intuitive. Or, I am interpretting the boxplots in a completely wrong way.","63835b17":"Some of the categories are extremely long, I am going to shorten them manually.\nThis next cell is just housekeeping and a pain. Skip ahead!","4823a5e1":"**No the graph is meaningless!!**..at a first look atleast. An officer would visit *after* the accident. Makes sense that a police officer would attend to more severe accidents.\n\nWhat we really need to do is a temporal analysis. Were there accidents *after* officer(s) visited a site is what really counts. I will be keeping this column as it is.(Don't know how to do a time-based analysis, feedback will help greatly)","b178463e":"# Data import\nThe data required for this assignment can be downloaded from the following [link](https:\/\/www.kaggle.com\/dataset\/e7cff1a2c6e29e18684fe6b077d3e4c42f9a7ae6199e01463378c60fe4b4c0cc), it's hosted on kaggle. Do check directory paths on your local system.  ","07f8bd9e":"**Juventus** it is according to the number! Honestly.... I don't watch football matches, but I guess Real Madrid and Barcelona are worth more. Juventus ain't bad either.","5e9485da":"Looking at the percentage of accidents in each lighting condition: we see that a slightly greater percentage of the more severe accidents occur in the darkness. Not very conclusive really. I took percentages here to normalize out the effect of the number of instances recorded. The raw numbers might not make sense, due to a variety of factors. But they do tell us that a majority of the accidents in sunny conditions. I will use this feature as is, after OHE.\n\nLet me repeat the same analysis for *Weather_Conditions*","0b1c442f":"First look at the data","112b5475":"I don't really know how to interpret this because the categories are not named. The overall counts are too close to disinguish either. I guess category 3 is some remote place or something, because there are *no* highly severe accidents at all.\n\nLast two fetures: `Officer_Attend` and `LSOA_of_Accident_Location` I think LSOA is something like a postal code in the UK. Drop it most likely, because there will be too many unique values."}}