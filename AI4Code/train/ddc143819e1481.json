{"cell_type":{"f5318f16":"code","0cad73a0":"code","de840e1a":"code","46b43ffa":"code","b15b181c":"code","0966469f":"code","d9a39213":"code","fef71072":"code","3a795be9":"code","da363ffa":"code","b57e335a":"code","3dbf5e86":"code","2cb9a5f2":"code","1ee81cac":"code","60db0389":"code","25d56a28":"code","598c3766":"code","b51ed860":"code","8cf3aac8":"code","763c38fc":"code","8b18f735":"code","08300d1e":"code","bec9d22c":"markdown","33e0ebf6":"markdown","37101621":"markdown","0fedd4d8":"markdown","1e197252":"markdown","cff5c4ef":"markdown","81efdd2e":"markdown","a6db17e0":"markdown"},"source":{"f5318f16":"import os\n\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\n\nimport spacy\nimport nltk\nimport numpy as np\nimport ast\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nimport wordcloud\nfrom colorama import Fore, Back, Style\n\nimport wandb\n\nfrom IPython.display import IFrame\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","0cad73a0":"train_df = pd.read_csv(\"..\/input\/feedback-prize-2021\/train.csv\")\nnum_rows = len(train_df)\nTRAIN_PATH = \"..\/input\/feedback-prize-2021\/train\"\n\ntrain_files = os.listdir(TRAIN_PATH)\ntrain_ids = [i.split(\".\")[0] for i in train_files]\ntrain_paths = [os.path.join(TRAIN_PATH,f) for f in train_files]\n\nclasses = [\"Claim\", \"Evidence\", \"Position\", \"Concluding Statement\", \"Lead\", \"Counterclaim\", \"Rebuttal\", \"None\"]\ncrma = {\n    classes[0] : Fore.BLUE,\n    classes[1] : Fore.RED,\n    classes[2] : Fore.MAGENTA,\n    classes[3] : Fore.YELLOW,\n    classes[4] : Fore.GREEN,\n    classes[5] : Fore.WHITE,\n    classes[6] : Fore.CYAN,\n    classes[7] : Fore.LIGHTBLUE_EX\n}\nsr_crma = Style.RESET_ALL\n\ntrain_df.head(2)","de840e1a":"def read_txt(filepath):\n    with open(filepath, \"r\") as f:\n        text = f.read()\n    return text\n\ndef get_row(text, df, pos, count):\n    row_curr, row_next = df.iloc[pos], df.iloc[pos+1]\n    txt_id, discourse_id = row_curr.id, row_curr.discourse_id\n    \n    if pos == 0 and row_curr.discourse_start!=0:\n        start, end = 0, int(row_curr.discourse_start)\n        start_word, end_word = 0, int(row_curr.predictionstring.split(\" \")[-1])\n    elif pos == len(df)-1 and row_curr.discourse_end!=len(text):\n        start, end = int(row_curr.discourse_end), len(text)\n        start_word, end_word = int(row_curr.predictionstring.split(\" \")[-1])+1, len(text.split(\" \"))-1\n    else:\n        start, end = int(row_curr.discourse_end), int(row_next.discourse_start)\n        start_word = int(row_curr.predictionstring.split(\" \")[-1])+1\n        end_word = int(row_next.predictionstring.split(\" \")[0])\n    \n    pred_str = [str(start_word+i) for i in range(end_word-start_word)] \n    pred_str = \" \".join(pred_str)\n    discourse_text = text[start:end]\n    return [\n            txt_id,discourse_id,\n            start, end,\n            discourse_text,\n            \"None\", \"None \" + str(count),\n            pred_str\n           ]","46b43ffa":"##-----------------------------------------------\n#Getting list of text that is missing in csv file\n##-----------------------------------------------\nnone_ls = []\nfor txt_id in tqdm(train_ids):\n    df = train_df[train_df[\"id\"] == txt_id]\n    text = read_txt(os.path.join(TRAIN_PATH,txt_id+\".txt\"))\n    count = 0\n    \n    for i in range(len(df)-1):\n        row_i0, row_i1 = df.iloc[i], df.iloc[i+1]\n        if row_i1.discourse_start > row_i0.discourse_end+1:\n            none_ls.append(get_row(text, df, i, count))\n            count += 1\n            \n        if row_i1.discourse_end < len(text) and i == len(df)-1:\n            start, end = int(row_i1.predictionstring.split(\" \")[-1])+1, len(text.split(\" \"))\n            pred_str = [str(start+i) for i in range(end-start)] \n            pred_str = \" \".join(pred_str)\n            none_ls.append([\n                row_i1.id, row_i1.discourse_id,\n                row_i1.discourse_end, len(text),\n                text[int(row_i1.discourse_end):len(text)],\n                \"None\", \"None \"+str(count),\n                pred_str\n            ])","b15b181c":"##---------------------------------------\n#Adding text with None class to DataFrame\n##---------------------------------------\nnone_df = pd.DataFrame(none_ls,columns=list(train_df.columns))\nnone_df.head()\n\ndf = pd.concat([train_df,none_df])\ndf = df.sort_values(by=[\"id\",\"discourse_start\"],ascending=[True,True]).reset_index(drop=True)\ndf.tail(3)","0966469f":"try:\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('''If you want to use your W&B account, Follow these steps :\n            -> go to Add-ons {Below name of notebook} -> Secrets -> Add a new Secret\n            -> Label = wandb_api\n            -> Value = W&B access token from https:\/\/wandb.ai\/authorize \n         ''')","d9a39213":"WANDB_CONFIG = {\n   \"project_name\" : \"Feedback Prize - Evaluating Student Writing\",\n   \"group_name\" : \"PyTorch Training\",\n   \"job_type_data\" : \"Data Visualization\",\n   \"anonymity\" : \"must\",\n    \"artifact\" : \"training_data\"\n}\n\nrun = wandb.init(\n    project = WANDB_CONFIG[\"project_name\"],\n    group = WANDB_CONFIG[\"group_name\"],\n    job_type = WANDB_CONFIG[\"job_type_data\"],\n    anonymous= WANDB_CONFIG[\"anonymity\"]\n)","fef71072":"wb_table = wandb.Table(columns = [\n    'id', 'discourse_id', 'discourse_start', 'discourse_end',\n    'discourse_text', 'discourse_type', 'discourse_type_num', 'predictionstring'\n])\n\nfor i in range(len(df)):\n    row = df.loc[i]\n    wb_table.add_data(\n        row[\"id\"],\n        row[\"discourse_id\"],\n        row[\"discourse_start\"],\n        row[\"discourse_end\"],\n        row[\"discourse_text\"],\n        row[\"discourse_type\"],\n        row[\"discourse_type_num\"],\n        row[\"predictionstring\"]\n    )\n    \nwandb.log({'Data Visualization': wb_table})\nrun.finish()","3a795be9":"frame = IFrame(run.url, width=1280, height=720)\nframe","da363ffa":"train_report = ProfileReport(df,title=\"Metadata of Training images\")\ntrain_report.to_file(\".\/train_metadata.html\")\ntrain_report","b57e335a":"##------------------------------------\n#Print a discourse coloured by classes\n##------------------------------------\nfor cls in classes:\n    print(f\"{crma[cls]} {cls} {sr_crma} \", end=\" \")\n\nprint(\"\\n\\n\")\ntmp_df = df[df[\"id\"] == train_ids[7]]\nfor i in range(len(tmp_df)):\n    row = tmp_df.iloc[i]\n    print(f\"{crma[row.discourse_type]}{row.discourse_text}{sr_crma}\", end=\" \")","3dbf5e86":"##----------------------\n#Distribution of classes\n##----------------------\n\nclasses_count = [list(df[\"discourse_type\"]).count(i) for i in classes]\n\nfig = px.bar(x = classes, y = classes_count, color = classes,color_continuous_scale=\"Emrld\") \nfig.update_xaxes(title=\"Classes\")\nfig.update_yaxes(title = \"Count\")\nfig.update_layout(showlegend = True,\n    title = {\n        'text': 'Discourse Class Distribution ',\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()","2cb9a5f2":"##-----------------------\n#Length of Text and Words\n##-----------------------\ntext_length, word_length = {}, {}\n\nfor i,path in tqdm(enumerate(train_paths)):\n    txt = read_txt(path)\n    text_length[train_ids[i]] = len(txt)\n    word_length[train_ids[i]] = len(txt.split(\" \"))\n    \ndf[\"text_length\"] = df[\"id\"].map(lambda x : text_length[x])\ndf[\"word_length\"] = df[\"id\"].map(lambda x : word_length[x])","1ee81cac":"fig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(\n    go.Histogram(x=list(text_length.values()), name=\"Length of text\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(word_length.values()), name=\"Number of words\"),\n    row=1, col=2\n)\n\nfig.update_layout(height=500, width=900, title_text=\"Histogram of length of text and number of words\")\nfig.show()","60db0389":"##-------------------------------------------------\n#Distribution of each class in different discourses\n##-------------------------------------------------\ncount_dict = {\n    classes[0] : [],\n    classes[1] : [],\n    classes[2] : [],\n    classes[3] : [],\n    classes[4] : [],\n    classes[5] : [],\n    classes[6] : [],\n    classes[7] : []\n}\n\nfor txt_id in tqdm(train_ids):\n    tmp = df[df[\"id\"] == txt_id]\n    for c in classes:\n        count_dict[c].append(len(tmp[tmp[\"discourse_type\"] == c]))","25d56a28":"fig = make_subplots(rows=4, cols=2, start_cell=\"bottom-left\")\nr = [1,1,2,2,3,3,4,4]\nc = [1,2,1,2,1,2,1,2]\n\nfor i,cls in enumerate(classes):\n    fig.add_trace(go.Histogram(x=count_dict[cls], name=cls),row=r[i], col=c[i])\n\nfig.update_layout(height=800, width=900, title_text=\"Distribution of each class in different discourses\")\nfig.show()","598c3766":"##--------------------------------\n#Information of each class in text\n##--------------------------------\nstart, end, length, words = {}, {}, {}, {}\n\nfor cls in classes:\n    tmp = df[df[\"discourse_type\"] == cls]\n    \n    tmp[\"count\"] = tmp[\"predictionstring\"].map(lambda x : len(x.split(\" \")))\n    start[cls] = np.array(tmp[\"discourse_start\"])\/np.array(tmp[\"text_length\"])\n    end[cls] = np.array(tmp[\"discourse_end\"])\/np.array(tmp[\"text_length\"])\n    length[cls] = np.array(tmp[\"discourse_end\"] - tmp[\"discourse_start\"] + 1)\/np.array(tmp[\"text_length\"])\n    words[cls] = np.array(tmp[\"count\"])\/np.array(tmp[\"text_length\"])","b51ed860":"##---------------------------\n#Start position of each Class\n##---------------------------\n\nfig = make_subplots(rows=4, cols=2)\n\nfor i,cls in enumerate(classes):\n    fig.add_trace(go.Box(x=start[cls], name=cls),row=r[i], col=c[i])\n    \nfig.update_layout(height=800, width=900, title_text=\"Start Position of text for each class [Normalized]\")\nfig.show()","8cf3aac8":"##-------------------------\n#End position of each Class\n##-------------------------\n\nfig = make_subplots(rows=4, cols=2)\n\nfor i,cls in enumerate(classes):\n    fig.add_trace(go.Box(x=end[cls], name=cls),row=r[i], col=c[i])\n    \nfig.update_layout(height=800, width=900, title_text=\"End Position of text for each class [Normalized]\")\nfig.show()","763c38fc":"##----------------------------\n#Length of text per each Class\n##----------------------------\n\nfig = make_subplots(rows=4, cols=2)\n\nfor i,cls in enumerate(classes):\n    fig.add_trace(go.Box(x=length[cls], name=cls),row=r[i], col=c[i])\n    \nfig.update_layout(height=800, width=900, title_text=\"Length of text for each class [Normalized]\")\nfig.show()","8b18f735":"##-------------------------------------\n#Number of words in text per each Class\n##-------------------------------------\n\nfig = make_subplots(rows=4, cols=2)\n\nfor i,cls in enumerate(classes):\n    fig.add_trace(go.Box(x=words[cls], name=cls),row=r[i], col=c[i])\n    \nfig.update_layout(height=800, width=900, title_text=\"Number of words text for each class [Normalized]\")\nfig.show()","08300d1e":"wcloud = wordcloud.WordCloud(stopwords=wordcloud.STOPWORDS, max_font_size=80, max_words=6000,\n                      width = 600, height = 400,\n                      background_color='black').generate(' '.join(txt for txt in df[\"discourse_text\"]))\n\nfig, ax = plt.subplots(figsize=(14,10))\nax.imshow(wcloud, interpolation='bilinear')\nax.set_axis_off()\nplt.imshow(wcloud)","bec9d22c":"<h1 style = \"font-family: 'Lucida Console', 'Courier New', monospace; background: rgb(44,169,201);\nbackground: linear-gradient(180deg, rgba(44,169,201,1) 0%, rgba(1,94,125,1) 100%); border-radius: 20px; font-size:30px; text-align:center; \"> Data Preprocessing \u2692\ufe0f\ud83d\udd2c<\/h1>\n\n<h2 style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px; font-size:30px; text-align:center; ; color:#FF69B4\">Necessary Functions<\/h2>","33e0ebf6":"<h1 style = \"font-family: 'Lucida Console', 'Courier New', monospace; background: rgb(44,169,201);\nbackground: linear-gradient(180deg, rgba(44,169,201,1) 0%, rgba(1,94,125,1) 100%); border-radius: 20px; font-size:30px; text-align:center; \">Initialize Constants \ud83d\udd30<\/h1>","37101621":"<center style = \"font-family: 'Lucida Console', 'Courier New', monospace;\">\n    <img src = \"https:\/\/takelessons.com\/blog\/wp-content\/uploads\/2014\/05\/Essay.jpg\" width=400 height = 200>\n    <h1 style = \"background: rgb(44,169,201);\nbackground: linear-gradient(180deg, rgba(44,169,201,1) 0%, rgba(1,94,125,1) 100%);border-radius: 20px; font-size:30px\">Feedback Prize - Evaluating Student Writing \ud83e\uddd1\u200d\u270f\ufe0f\ud83e\uddfe<\/h1>\n<\/center>\n\n<div style = \"background: rgb(224,224,224);border-radius: 42px;\">\n    <h1 style = \"font-family: Consolas; text-align:center; color:#FF69B4\">Introduction<\/h1>\n    <h2 style = \"font-family: Consolas; text-align:center\">Why this Competition \u2753<\/h2>\n    <p style = \"font-family : Lucida Sans Typewriter\">\n     One way to help students improve their writing is via automated feedback tools, which evaluate student writing and provide personalized feedback. There are currently numerous automated writing feedback tools, but they all have limitations. Many often fail to identify writing structures, such as thesis statements and support for claims, in essays or do not do so thoroughly. \n    <\/p>\n    <h2 style = \"font-family: Consolas; text-align:center\">Goal of Competition \ud83e\udd45<\/h2>\n    <p style = \"font-family : Lucida Sans Typewriter\">\n    Our goal is to identify elements in student writing. More specifically, you will automatically segment texts and classify argumentative and rhetorical elements in essays written by 6th-12th grade students. You'll have access to the largest dataset of student writing ever released in order to test your skills in natural language processing, a fast-growing area of data science.\n    <\/p>\n<\/div>\n\n<h2 style = \"font-family: Consolas\">More Details<\/h2>\n<p style = \"font-family : Lucida Sans Typewriter\">Check <a href = \"https:\/\/www.kaggle.com\/c\/feedback-prize-2021\">competition page<\/a> for details<\/p>\n<h2 style = \"font-family : Comic Sans MS\">Let's dive in \u2b07\ufe0f<\/h2>\n\n<center><img src = \"https:\/\/img.shields.io\/badge\/Upvote-If%20you%20found%20this%20notebook%20useful-blue\" width=400 height = 400><\/center>","0fedd4d8":"<div style = \"font-family : Lucida Sans Typewriter;background: rgb(224,224,224);border-radius: 25px;\">\n    <h2 style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px; font-size:30px; text-align:center; ; color:#FF69B4\">Pandas Profiling<\/h2>\n    <center><img src = \"https:\/\/pandas-profiling.github.io\/pandas-profiling\/docs\/assets\/logo_header.png\" width=200 height = 200><\/center>\n    <p style = \"font-family : Lucida Sans Typewriter\">\n    Pandas profiling is an open source Python module with which we can quickly do an exploratory data analysis with just a few lines of code. Besides, if this is not enough to convince us to use this tool, it also generates interactive reports in web format that can be presented to any person, even if they don\u2019t know programming.\n    <\/p>  \n    <a href = \"https:\/\/pandas-profiling.github.io\/pandas-profiling\/\">Go to offocial website for documentation<\/a>\n<\/div>","1e197252":"<h1 style = \"font-family: 'Lucida Console', 'Courier New', monospace; background: rgb(44,169,201);\nbackground: linear-gradient(180deg, rgba(44,169,201,1) 0%, rgba(1,94,125,1) 100%); border-radius: 20px; font-size:30px; text-align:center; \">Import Libraries \ud83d\udcda<\/h1>","cff5c4ef":"<h2 style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px; font-size:30px; text-align:center; ; color:#FF69B4\">Word Cloud<\/h2>","81efdd2e":"<div style = \"font-family : Lucida Sans Typewriter;background: rgb(224,224,224);border-radius: 25px;\">\n    <h2 style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px; font-size:30px; text-align:center; ; color:#FF69B4\">Weights and Biases<\/h2>\n    <center><img src = \"https:\/\/i.imgur.com\/KISYcqD.png\" width=200 height = 200><\/center>\n    <a href = \"https:\/\/wandb.ai\/anony-moose-172647\/Feedback%20Prize%20-%20Evaluating%20Student%20Writing?workspace=user-shanmukh\"; style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px; text-align:center; font-size:25px\">Checkout Dashboard created for this notebook<\/a>\n    <p style = \"font-family : Lucida Sans Typewriter\">\n    Weights and Biases is a set of Machine Learning tools used for experiment tracking, dataset versioning, and collaborating on ML projects. Weights and Biases is useful in many applications such as\n    <\/p>  \n    <ul>\n          <li>Experiment Tracking<\/li>\n          <li>Hyperparameter Tuning<\/li>\n          <li>Data Visualization<\/li>\n          <li>Data and model Versioning<\/li>\n          <li>Collaborative Reports<\/li>\n    <\/ul>\n    <a href = \"https:\/\/wandb.ai\/site\">Go to offocial website for more tutorials and Documentation<\/a>\n<\/div>","a6db17e0":"<br>\n<h3 style = \"font-family: Consolas; text-align:center; color:#FF0000\">If you come this far, you could've got some insights from this notebook. An upvote would be very helpful :). Kindly comment if there are any doubts or mistakes<\/h3>\n\n<center><img src = \"https:\/\/img.shields.io\/badge\/Completed-The%20End-brightgreen\" width=300 height = 300><\/center>"}}