{"cell_type":{"ce67c9af":"code","80546f3f":"code","6f20d447":"code","1850e7ac":"code","cc6f7832":"code","0260b208":"code","7395e793":"code","5669aa94":"code","18d24391":"code","99953892":"code","31a707a4":"code","e92da1e6":"code","9636a71a":"code","23a35167":"code","01f81629":"code","d9ec2eca":"code","56964e52":"code","82a559b7":"code","a02333c9":"code","7ef3c18c":"code","41decec9":"code","731cba00":"code","ad1b07ed":"code","df558da9":"code","a1f737db":"code","8adea2db":"code","c4bbf056":"markdown","c86752a1":"markdown","3ef54f9e":"markdown","039cb35c":"markdown","6c9e3249":"markdown","de834300":"markdown","968dd5ca":"markdown","63790c17":"markdown"},"source":{"ce67c9af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80546f3f":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf","6f20d447":"df.columns","1850e7ac":"Data_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nData_test","cc6f7832":"cols_with_missing = [[col,df[col].unique(),df[col].isna().sum()] for col in df.columns\n                     if df[col].isnull().any()]\ncols_with_missing","0260b208":"drop_col = ['Alley','FireplaceQu','PoolQC','Fence','MiscFeature']","7395e793":"new_df = df.drop(drop_col, axis=1)\nnew_df","5669aa94":"imputed_col = ['LotFrontage','MasVnrArea','GarageYrBlt']","18d24391":"impu_df = df[imputed_col]\nimpu_df","99953892":"from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nimputed_df = pd.DataFrame(my_imputer.fit_transform(impu_df))\n# Imputation removed column names; put them back\nimputed_df.columns = impu_df.columns","31a707a4":"imputed_df","e92da1e6":"new_df.update(imputed_df)","9636a71a":"new_df[imputed_col].isnull().sum()","23a35167":"new_df.isnull().sum()","01f81629":"# Get list of categorical variables\ns = (new_df.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","d9ec2eca":"from sklearn.preprocessing import LabelEncoder\ncols = object_cols\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(new_df[c].values)) \n    new_df[c] = lbl.transform(list(new_df[c].values))","56964e52":"y = new_df['SalePrice']\nX = new_df.drop(columns=['SalePrice'])\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2)","82a559b7":"para = list(range(100, 1001, 100))\nprint(para)","a02333c9":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nresults = {}\nfor n in para:\n    print('para=', n)\n    model = RandomForestRegressor(n_estimators=n, random_state=1)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    mae = mean_absolute_error(y_true=y_valid, y_pred=preds)\n    print (mae)\n    results[n] = mae\n    print('--------------------------')","7ef3c18c":"import matplotlib.pylab as plt\n# sorted by key, return a list of tuples\nlists = sorted(results.items()) \np, a = zip(*lists) # unpack a list of pairs into two tuples\nplt.plot(p, a)\nplt.show()","41decec9":"best_para = min(results, key=results.get)\nprint('best para', best_para)\nprint('value', results[best_para])","731cba00":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nresults_XG = {}\nfor n in para:\n    print('para=', n)\n    model_XG = XGBRegressor(n_estimators=n,learning_rate=0.05, random_state=1)\n    model_XG.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False)\n    preds = model_XG.predict(X_valid)\n    mae = mean_absolute_error(y_true=y_valid, y_pred=preds)\n    print (mae)\n    results_XG[n] = mae\n    print('--------------------------')","ad1b07ed":"best_para_XG = min(results_XG, key=results_XG.get)\nprint('best para', best_para_XG)\nprint('value', results_XG[best_para_XG])","df558da9":"import matplotlib.pylab as plt\n# sorted by key, return a list of tuples\nlists = sorted(results_XG.items()) \np, a = zip(*lists) # unpack a list of pairs into two tuples\nplt.plot(p, a)\nplt.show()","a1f737db":"\nfinal_model = XGBRegressor(n_estimators=200,learning_rate=0.05, random_state=1)\nmodel_XG.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False)\nfinal_preds = model_XG.predict(X_valid)\nmae = mean_absolute_error(y_true=y_valid, y_pred=final_preds)\nprint (mae)\nprint('--------------------------')","8adea2db":"# Save test predictions to file\noutput = pd.DataFrame({'Id': Data_test.index,\n                       'SalePrice': final_preds})\noutput.to_csv('submission.csv', index=False)","c4bbf056":"# Final Model","c86752a1":"# Missing Values","3ef54f9e":"Missing data (Imputation)","039cb35c":"# XGBoost","6c9e3249":"# Prepare X, y and Split data","de834300":"# Categorical Variables\nLabel Encoding","968dd5ca":"Missing data (Drop)","63790c17":"# Model"}}