{"cell_type":{"97d51fd2":"code","b52f866b":"code","c9543f33":"code","4d50c1e8":"code","edab756a":"code","ca1ac6ef":"code","1900ba3a":"code","507f4b44":"code","54d40806":"code","2bb6d9a2":"code","76797f13":"code","42c76c0c":"code","5b6dd837":"code","df409bc2":"code","fb259a34":"code","e5ab5d70":"code","6d7b18b5":"code","6dbea9f8":"code","eb379834":"code","13f2fffb":"code","c7a6ce90":"code","11c7391a":"code","1538fee1":"code","239619b8":"code","52b60e2a":"code","1e512a19":"code","629805c5":"code","e2e1002e":"code","e247dbc4":"markdown","c915cfdc":"markdown","d469c4c3":"markdown","54f16f18":"markdown","90c52645":"markdown","2a2e5d7e":"markdown","466845ec":"markdown","aa217834":"markdown","af072c1e":"markdown","f8b9d99f":"markdown","713287c0":"markdown","d7f070ac":"markdown","c9b456bb":"markdown","02ac54ec":"markdown","6143c056":"markdown","55e8911d":"markdown","fcdb592b":"markdown","9131f341":"markdown","c0357aa1":"markdown","d06502e8":"markdown","808eb75a":"markdown","de6e1158":"markdown","90811815":"markdown","d294b870":"markdown","09a10ec5":"markdown","5b1ca998":"markdown"},"source":{"97d51fd2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer #use TFIDF transformer to change text vector created by count vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import svm,naive_bayes\nfrom sklearn.svm import SVC #Support Vector Machine\nfrom sklearn.metrics import accuracy_score\n\nimport tensorflow as tf\n# import tensorflow_datasets as tfds\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense,Embedding,Flatten\n\n#!pip install install ITU-Turkish-NLP-Pipeline-Caller\n#import pipeline_caller\n#caller = pipeline_caller.PipelineCaller()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b52f866b":"hb = pd.read_csv(\"..\/input\/hepsiburada\/hepsiburada.csv\")","c9543f33":"hb.shape","4d50c1e8":"hb.head(5)","edab756a":"hb['Rating'].value_counts()","ca1ac6ef":"hb['Review'].dropna(inplace=True)","1900ba3a":"hb['Review'] = [entry.lower() for entry in hb['Review']]","507f4b44":"rating = hb[\"Rating\"].values.tolist()\nreview = hb[\"Review\"].values.tolist()\n\nreview_train, review_test, rating_train, rating_test = train_test_split(review,rating,test_size=0.3)\n\nprint(len(review_train))\nprint(len(review_test))","54d40806":"tokenizer = Tokenizer(num_words = 10000)","2bb6d9a2":"review_tokens = tokenizer.fit_on_texts(review)\nreview_train_tokens = tokenizer.fit_on_texts(review_train)\nreview_test_tokens = tokenizer.fit_on_texts(review_test)","76797f13":"word_index = tokenizer.word_index\nword_index","42c76c0c":"review_seq = tokenizer.texts_to_sequences(review)\nreview_train_seq = tokenizer.texts_to_sequences(review_train)\nreview_test_seq = tokenizer.texts_to_sequences(review_test)","5b6dd837":"print(review[18])\nprint(review_seq[18])","df409bc2":"word_count = tokenizer.word_counts\nword_count = pd.DataFrame.from_dict(word_count,orient='index') # Turning the list into a dataframe\nword_count.columns = ['freq']\nword_count.sort_values(by=['freq'],ascending = False)","fb259a34":"tokens_count = [len(tokens) for tokens in review_seq]","e5ab5d70":"tokens_count = np.array(tokens_count)","6d7b18b5":"max_token = np.max(tokens_count)\nmax_index = np.argmax(tokens_count)\nprint(\"token count of review that has maxium number of token: \",max_token)\nprint(\"\\nindex of review that has maximum number of token: \",max_index)\nprint(\"\\n\",review[max_index])","6dbea9f8":"review_train_pad = pad_sequences(review_train_seq, maxlen=max_token)\nreview_test_pad = pad_sequences(review_test_seq, maxlen=max_token)\nreview_pad = pad_sequences(review_seq, maxlen = max_token)\n\nprint(review_train_pad.shape)","eb379834":"print(np.array(review_train_seq[18]))\nprint(review_train_pad[18])","13f2fffb":"count_vect = CountVectorizer(max_features = 1000)\nreview_train_vect = count_vect.fit_transform(review_train)\nreview_test_vect = count_vect.fit_transform(review_test)","c7a6ce90":"tfidf_vect = TfidfVectorizer(max_features = 1000)\ntfidf_vect.fit(review_train)","11c7391a":"tfidf_review_train = tfidf_vect.transform(review_train)\ntfidf_review_test = tfidf_vect.transform(review_test)\nprint(tfidf_review_train)","1538fee1":"SVM = svm.SVC(C =1.0, kernel ='linear', degree =3, gamma ='auto')\nSVM.fit(tfidf_review_train,review_train)\n#predictions_SVM = SVM.predict(tfidf_review_test)\n#print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)","239619b8":"nb = naive_bayes.MultinomialNB()\nnb.fit(tfidf_review_train,review_train)\npredictions_NB = nb.predict(tfidf_review_test)\nprint(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)","52b60e2a":"model = Sequential([\n    Embedding(\n      input_dim = num_words,\n      output_dim = 50, \n      input_length = max_token),\n    Dense(1, activation=\"sigmoid\")])","1e512a19":"model.compile(\n    loss=\"binary_crossentropy\",\n    optimizer='adam',\n    metrics=[\"accuracy\"])","629805c5":"model.summary()","e2e1002e":"model.fit(review_train_pad, np.array(rating_train), epochs=10, batch_size=256)","e247dbc4":"Padding all reviews to equalize all the reviews token number","c915cfdc":"As we see all the reviews have 295 tokens, maximum length of all reviews","d469c4c3":"We have 243,497 reviews labeled as 0 or 1 which indicates negative and positive respectively\n","54f16f18":"#### Importing neccessary packages","90c52645":"Transforming each text in texts to a sequence of integers","2a2e5d7e":"Getting the count of tokens of each review","466845ec":"Applying tokenization","aa217834":"Printing a review itself and with sequence integers based on word frequency of the reviews","af072c1e":"## Modeling","f8b9d99f":"Lowering all the text to avoid the duplications in tokenization. Because 'guzel' and 'Guzel' counting as two tokens but it should be one","713287c0":"## Preparation","d7f070ac":"Listing most used 1000 words with their counts","c9b456bb":"#### Reading dataset and creating dataframe hb","02ac54ec":"### Neural Network","6143c056":"Turning the list into a numpy array in order to analyze easily","55e8911d":"## Preproccessing","fcdb592b":"As we can see in the words and indices list, the word \"\u00fcr\u00fcn\" has index 4 while \"s\u00fcper\" has 73 and so on","9131f341":"229,821 of the reviews labeled as positive and 13,676 of them as negative","c0357aa1":"### Support Vector Machine","d06502e8":"We will use the 1000 most frequently used words","808eb75a":"Seperating dataset into rating and review data frames and splitting into training and test sets with 30%","de6e1158":"### Naive Bayes","90811815":"Getting the number of tokens and printing the review with maximum number of tokens","d294b870":"Checking blanks rows in data and dropping if there is any","09a10ec5":"Creating the vocabulary index based on the word frequency","5b1ca998":"Listing most used 1000 words with their indices"}}