{"cell_type":{"2a4f9663":"code","c2c72c68":"code","76dc7832":"code","9de6c0a5":"code","761a7d5d":"code","e2171f50":"code","197c5493":"code","cd9d8621":"code","6c7dfde1":"code","d669138c":"code","8f27e62f":"code","b229a445":"code","8d597905":"code","6fde861d":"code","a90de251":"code","51225aef":"code","9c6cdfeb":"code","27df7ad8":"code","55624d83":"code","07ec76de":"code","cebed454":"code","53294325":"code","3e1d7575":"code","6ca0d38b":"code","967b3a2d":"code","8451e69f":"code","f90c0ccd":"code","62875c00":"code","415ea001":"code","072c2b57":"code","b921de0a":"code","256a9260":"code","8c4c7cf9":"code","e9720ee0":"code","487f840f":"code","a1c58652":"code","5148a732":"code","ab5e7509":"code","bbe8c46f":"markdown","83d60892":"markdown","7231507f":"markdown","cc5048e8":"markdown","880657a4":"markdown","68a69834":"markdown","d65b38f8":"markdown","d1dc9891":"markdown","8e0c008a":"markdown","0387fb8a":"markdown","cee92428":"markdown","e531c61b":"markdown","9ee0e919":"markdown","816eb5a1":"markdown","f9c98e30":"markdown","edc92ba1":"markdown","9a761fef":"markdown","fa50e616":"markdown","744e1179":"markdown","5fd46f35":"markdown","f69ffbea":"markdown","0d780ecf":"markdown","27dfef1f":"markdown","aa0e941f":"markdown","c207c0e1":"markdown","bddbe594":"markdown","9aeb7747":"markdown","6bbaa599":"markdown","a6beecde":"markdown","17edb509":"markdown","0a7a6c85":"markdown","4856486d":"markdown","007a3d68":"markdown","3fe9d969":"markdown","708f7b8c":"markdown","816fdcd4":"markdown","77329b74":"markdown","b9d9cb9b":"markdown","cd6bc00c":"markdown","57ce4f49":"markdown","b5553703":"markdown","17b8ad0e":"markdown","ba89c244":"markdown","412a3d81":"markdown","2d94173e":"markdown","92f1f127":"markdown","238d1d7d":"markdown","d09d0436":"markdown","2a2b4070":"markdown","5ea27fdc":"markdown","e3e6d8b5":"markdown","08b4f019":"markdown","8d1664d2":"markdown","14ddb3db":"markdown","80198f12":"markdown"},"source":{"2a4f9663":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#To scale the data using z-score \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n#Algorithms to use\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Metrics to evaluate the model\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c2c72c68":"data = pd.read_csv(\"..\/input\/loan-predication\/train_u6lujuX_CVtuZ9i (1).csv\")\ndata.head()","76dc7832":"data.info()","9de6c0a5":"data.Loan_ID.nunique()","761a7d5d":"data.drop(columns=['Loan_ID'], inplace = True)","e2171f50":"#Creating a list of numerical columns\nnum_cols = num_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']\n\n#printing summary statistics\n\ndata[num_cols].describe()","197c5493":"#Converting the scale of loan term from months to years\ndata['Loan_Amount_Term']=data['Loan_Amount_Term']\/12","cd9d8621":"#Adding the applicant and co-applicant income to get the total income per application\ndata['total_income']=data['ApplicantIncome'] + data['CoapplicantIncome']","6c7dfde1":"#Dropping the columns as we created a new column which captures the same information\ndata.drop(columns=['ApplicantIncome', 'CoapplicantIncome'], inplace=True)","d669138c":"for col in ['LoanAmount', 'total_income']:\n    print(col)\n    print('Skew :',round(data[col].skew(),2))\n    plt.figure(figsize=(15,4))\n    plt.subplot(1,2,1)\n    data[col].hist(bins=10, grid=False)\n    plt.ylabel('count')\n    plt.subplot(1,2,2)\n    sns.boxplot(x=data[col])\n    plt.show()","8f27e62f":"#Creating list of categorical columns\ncat_col= ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History','Property_Area', 'Loan_Status']\n\nfor col in cat_col:\n    print(data[col].value_counts(normalize=True)) #Write your code here\n    print('*'*40)                #Print the * 40 times to separate different variables","b229a445":"#Imputing missing values with mode for the categorical variables \nfor col in ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History']:\n    data[col].fillna(value=data[col].mode()[0], inplace=True)","8d597905":"#Replacing 0's with null values in loan amount \ndata.LoanAmount.replace(0, np.nan, inplace=True)\n\n#Imputing null values in loan amount with the median\ndata.LoanAmount.fillna(value=data.LoanAmount.median(), inplace=True)","6fde861d":"data.isnull().sum()","a90de251":"data.replace({\"Loan_Status\":{'N':0,'Y':1}},inplace=True) # label encoding","51225aef":"plt.figure(figsize=(10,6))\nsns.regplot(x='LoanAmount', y='total_income', data=data)\nplt.show()","9c6cdfeb":"sns.boxplot(x=data.Loan_Status, y=data.total_income)","27df7ad8":"sns.countplot(x=data.Credit_History, hue=data.Loan_Status)","55624d83":"sns.countplot(x=data.Education, hue=data.Loan_Status)","07ec76de":"sns.countplot(x=data.Property_Area, hue=data.Loan_Status)","cebed454":"#Separating target variable and other variables\n\nX= data.drop(columns = ['Loan_Status'])\nY= data.Loan_Status","53294325":"#Creating dummy variables \n#drop_first=True is used to avoid redundant variables\nX = pd.get_dummies(X, drop_first=True)","3e1d7575":"#Splitting the data into train and test sets\nX_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.30,random_state=1)","6ca0d38b":"#function to print classification report and get confusion matrix in a proper format\n\ndef metrics_score(actual, predicted):\n    print(classification_report(actual, predicted))\n    cm = confusion_matrix(actual, predicted)\n    plt.figure(figsize=(8,5))\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels=['Not Eligible', 'Eligible'], yticklabels=['Not Eligible', 'Eligible'])\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    plt.show()","967b3a2d":"#define logistic regression model \nlog_reg= LogisticRegression(random_state = 1)\n\n#fit the model\nlog_reg.fit(X_train,y_train)","8451e69f":"pd.Series(log_reg.coef_[0], index=X_train.columns).sort_values(ascending=False)","f90c0ccd":"odds = np.exp(log_reg.coef_[0]) #finding the odds\n\n#adding the odds to a dataframe and sorting the values\npd.DataFrame(odds, X_train.columns, columns=['odds']).sort_values(by='odds', ascending=False) ","62875c00":"#predict on the training data \ny_pred_train = log_reg.predict(X_train) #Write your code here\n\n# Checking performance on the training data\nmetrics_score(y_train, y_pred_train)","415ea001":"y_scores=log_reg.predict_proba(X_train) #predict_proba gives the probability of each observation belonging to each class\n\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores[:,1])\n\n#Plotting values of precisions, recalls, and thresholds\nplt.figure(figsize=(10,7))\nplt.plot(thresholds, precisions[:-1], 'b--', label='precision')\nplt.plot(thresholds, recalls[:-1], 'g--', label = 'recall')\nplt.xlabel('Threshold')\nplt.legend(loc='upper left')\nplt.ylim([0,1])\nplt.show()","072c2b57":"#calculating the exact threshold where precision and recall are equal.\nfor i in np.arange(len(thresholds)):\n    if precisions[i]==recalls[i]:\n        print(thresholds[i])","b921de0a":"optimal_threshold1 = 0.75\nmetrics_score(y_train, y_scores[:,1]>optimal_threshold1)","256a9260":"#Checking performance on the testing data\ny_pred_test = log_reg.predict_proba(X_test)\nmetrics_score(y_test, y_pred_test[:,1]>optimal_threshold1)","8c4c7cf9":"knn = KNeighborsClassifier()\nss = StandardScaler()\n\n# We select the best value of k for which the error rate is the least in the validation data\n# Let us loop over a few values of the k to determine the best k\n\ntrain_error = []\ntest_error = []\nknn_many_split = {}\n\nerror_df_knn = pd.DataFrame()\nfeatures = X.columns\n\nfor k in range(1,21):\n    train_error = []\n    test_error = []\n    lista = []\n    knn = KNeighborsClassifier(n_neighbors=k)\n    for i in range(30):\n        x_train_new, x_val, y_train_new, y_val = train_test_split(X_train, y_train, test_size = 0.20)\n    \n        #let us scale the data\n        x_train_new = ss.fit_transform(x_train_new)  #fit_transform the training data\n        x_val = ss.transform(x_val) #transform the validation set\n        #Fitting knn on training data\n        knn.fit(x_train_new, y_train_new)\n        #Calculating error on training and validation data\n        train_error.append(1 - knn.score(x_train_new, y_train_new)) \n        test_error.append(1 - knn.score(x_val, y_val))\n    lista.append(sum(train_error)\/len(train_error))\n    lista.append(sum(test_error)\/len(test_error))\n    knn_many_split[k] = lista\n\nknn_many_split","e9720ee0":"# Extracting the train and the test error for each k in a list for easy plotting\n\nkltest = []\nvltest = []\nfor k, v in knn_many_split.items():\n    kltest.append(k)\n    vltest.append(knn_many_split[k][1])\n\nkltrain = []\nvltrain = []\n\nfor k, v in knn_many_split.items():\n    kltrain.append(k)\n    vltrain.append(knn_many_split[k][0])\n\n# Plotting k vs error \nplt.figure(figsize=(10,6))\nplt.plot(kltest,vltest, label = 'test' )\nplt.plot(kltrain,vltrain, label = 'train')\nplt.legend()\nplt.show()","487f840f":"# scaling the data\nscaler=StandardScaler()\nX_train_scaled=pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)  #fit_transform the training data\nX_test_scaled=pd.DataFrame(scaler.transform(X_test), columns=X_test.columns) #transform the testing data","a1c58652":"#Fitting the model on the scaled data\nknn = KNeighborsClassifier(n_neighbors = 7, weights='distance')\nknn.fit(X_train_scaled, y_train)","5148a732":"#predicting on train data\ny_pred_train_knn = knn.predict(X_train_scaled)# write your code here\n\n#checking performance of the model\nmetrics_score(y_train, y_pred_train_knn)","ab5e7509":"#predict on test data\ny_pred_test_knn = knn.predict(X_test_scaled)# Write your code here\n\n#checking performance of the model\nmetrics_score(y_test, y_pred_test_knn)","bbe8c46f":"**Reading confusion matrix (clockwise):**\n\n- **True Positive**: Predicting the customer is not eligible and the customer is actually not eligible\n- **False Negative**: Predicting the customer is eligible but the customer is actually not eligible\n- **True Negative**: Predicting the customer is eligible and the customer is actually eligible\n- **False Positive**: Predicting the customer is not eligible but the customer is actually eligible","83d60892":"**Observations:**\n- The plot shows that the loan amount is positively correlated with total income. This implies that the loan amount for higher-income applicants is progressively higher.\n- There are some outliers visible, showing applicants with low income having been given loans of a higher amount.","7231507f":"**We are now done with data preprocessing**","cc5048e8":"**Let's check the coefficient of each dependent variable in the data**","880657a4":"**Observations:**\n- We can see that the train error and the test error are more or less similar for K more than or equal to 7.\n- This implies that we would get generalized results if we choose K=7.\n- So, let's scale the full data and fit the model on the entire training set. ","68a69834":"- Models cannot take non-numeric inputs. So, we will first create dummy variables for all the categorical variables.\n- We will then split the data into train and test sets.","d65b38f8":"**Summary Statistics for numerical columns**","d1dc9891":"**Observations:**\n\n* The odds of an applicant's loan application being approved is around 20 times higher for a person who meets the loan approval requirements compared to someone who doesn't\n\n* The odds of an applicant's loan application being approved is over double if he is married compared to someone who is not. \n\n","8e0c008a":"Let's check how credit history is related to loan status","0387fb8a":"## Bivariate Analysis","cee92428":"**Observation:**\n\n* We see a weak performance on the scaled testing data, as the recall score is only 0.31 for the 0. There were 42 people that our Knn model predicted to be eligible who are actually not eligible, which is concerning given that we want to minimize this value as much as possible. ","e531c61b":"**Observations:**\n\n- There are 614 observations and 13 columns in the data.\n- ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, Credit_History, and Loan_Status are numeric data types. The rest of the variables are of the object data type.\n- There are several columns with less than 614 non-null entries i.e. these columns have missing values.\n- Loan_ID column is an identifier. Let's check if each entry of the column is unique.","9ee0e919":"## Feature Engineering","816eb5a1":"**Observations:**\n\n* The mean applicant income is around $5403.46\n\n* The minimum value for applicant income is $150 which is very small and could be an outlier since it is very far from both the mean and the median. \n\n* There are a good number of co applicants with no income, as can be observed by thet fact that the 25% quantile of coapplicants have an income of 0. ","f9c98e30":"**Before training the model, let's choose the appropriate model evaluation criterion as per the problem on hand.**\n\n### Model evaluation criterion:\n\n#### Since this is a binary classification problem, the model can make wrong predictions in one of two ways:\n1. Predicting a customer is not eligible for a loan when the customer actually is eligible - Loss of opportunity\n2. Predicting a customer is eligible for a loan when the customer is actually not - Financial loss\n\n#### Which case is more important? \n* Predicting that the customer is eligible when he is not, because the person might default or not be able to return the loan, which would result in financial loss to the company.\n\n#### How to reduce this loss i.e need to reduce False Negatives?\n* In classification, the class of interest is considered the positive class. Here, the class of interest is 0 i.e. identifying non-eligible customers. So, the company wants to minimize the number of false negatives, in other words **Recall** is the evaluation criterion to be maximized.","edc92ba1":"**Now, let's check the performance of the model on the training and testing data**","9a761fef":"**So let's compare the performance of the model on the training and testing sets after changing the threshold**","fa50e616":"Also, let's create a function to calculate and print the classification report and confusion matrix so that we don't have to rewrite the same code repeatedly for each model.","744e1179":"Let's check the relationship between education and loan status","5fd46f35":"## Imputing missing values","f69ffbea":"- KNN is a distance based algorithm and all distance based algorithms are affected by the scale of the data.\n- We will scale the attributes (dataframe X defined above) before building the KNN model.\n\n**But before actually building the model, we need to identify the value of K to be used in KNN. We will perform the following steps for the same.**\n\n- For every value of K (from 1 to 20), we split the train set into new train and validation set (30 times)\n- We scale the training data and validation data\n- We take the average of the error on these train and the validation sets for each K\n- We plot the average train vs the validation set error for all Ks \n- Finally, we choose a suitable K from the plot where the two errors are comparable","0d780ecf":"**Finally, we need to encode our loan status column to make it eligible for modeling**","27dfef1f":"**Observations:**\n- We can see that graduate customers are more likely to get loans.\n","aa0e941f":"### Checking percentage of each category for categorical variables","c207c0e1":"## Context: \n---\n\nCredit risk is the risk of loss resulting from the failure by a borrower to repay the principal and interest owed to the lender. The lender uses the interest payments from the loan to compensate for the risk of potential losses. When the borrower defaults on his\/her obligations, it causes an interruption in the cash flow of the lender.\n\nIn the banking sector, this is an important factor to be considered before approving the loan of an applicant in order to cushion the lender from loss of cash flow and reduce the severity of losses. \n\n## Data Dictionary:\n--------------------------\n\nThe data contains the following attributes:\n\n* **Loan_ID**: Unique Loan ID\n* **Gender**: Gender of the applicant - Male\/Female\n* **Married**: Whether the applicant is married or not (Yes\/No)\n* **Dependents**: Number of dependents of the applicant\n* **Education**: Applicant's education (Graduate\/Not Graduate)\n* **Self_Employed**: Whether the applicant is self-employed (Yes\/No)\n* **ApplicantIncome**: The income of the applicant (\\$)\n* **CoapplicantIncome**: The co-applicant's income in case of a joint loan and 0 otherwise (\\$)\n* **LoanAmount**: Loan amount (dollars in thousands) \n* **Loan_Amount_Term**: Term of loan in months\n* **Credit_History**: Whether the applicant's credit history meets required guidelines\n* **Property_Area**: The area the property pertaining to the loan belongs to - Urban\/Semi-Urban\/Rural\n* **Loan_Status**: Loan approved (1 - Yes, 0 - No)","bddbe594":"**Checking the distribution and outliers for each column in the data**","9aeb7747":"**Now, let's check the performance of the model on the training set**","6bbaa599":"Let's check the relationship of loan amount with the total income","a6beecde":"**Observations:**\n\n- The plot shows that more loans are approved for properties in semi-urban areas. \n- This could be due to several reasons. The bank might be charging higher interest rates for semi-urban areas or the current customer base of the company from semi-urban areas might actually be more eligible for home loans based on loan applicant features. We cannot be certain as we don't have the data to support this claim.\n\n**Now that we have processed and explored our data, let's prepare it for modeling.**","17edb509":"**Precision-Recall curve for Logistic Regression**","0a7a6c85":"**Observations:**\n\n* The vast majority of applicants are male (81%). \n\n* 65% of applicants are married, and 57% of them have no dependents\n\n* 78% of the applicants are graduates\n\n* 85% of the applicants are self employed\n\n* 84% of the applicants have credit histories that meet the required guidelines\n\n* The property area among the applicants is roughly evenly split across semiurban, urban, and rural, with semi-urban having a slightly higher portion (37.9%)","4856486d":"**Checking for null values**","007a3d68":"**Dropping Loan_ID column**","3fe9d969":"## Conclusion","708f7b8c":"## Importing necessary libraries and overview of the dataset","816fdcd4":"**Observations:**\n- Except for some outliers, there is no major difference between the income of those customers who are eligible versus those who are not. \n- This implies that income alone might not be a good indicator of a customer's loan eligibility.","77329b74":"**Observations:**\n\n* We can see that even though the precision dropped for class 0, the recall score is much higher after using the optimal threshold, and now our model is a lot more appropriate after improvement. \n\n* Similar performance on both the training data and the test data","b9d9cb9b":"Let's check if property area is related with loan status","cd6bc00c":"**Observations:**\n\n* The performance on the training set is very strong, as it predicts perfectly who is eligible and who isn't. The precission, recall, and and accuracy are all optimal at 100%. ","57ce4f49":"## Preparing data for modeling","b5553703":"**Observations:**\n- We can see that all the entries of this column are unique. Hence, this column would not add any value to our analysis. \n- Let's drop this column.","17b8ad0e":"## Exploratory Data Analysis and Data Preprocessing","ba89c244":"**Observations**\n\n- The plot shows that credit history is an important factor while checking loan eligibility. There are very few customers whose loan was approved even when their credit history did not meet required guidelines.","412a3d81":"- The threshold of 0.72 would give a balanced precision and recall.\n- We can choose the threshold to be a bit higher to make the recall higher but then the precision would drop.\n- **This decision depends on the stakeholders and other business driven factors.** For example, if the company can bear the cost of some false positives and be more conservative while approving loans then we can choose for the recall to be higher.\n- Let's choose the recall to be 0.75 for the current scenario, since we have mentioned that the company wants to optimize recall.\n- **Note:** We also don't want the precision to be so poor that it outweighs the benefits of a higher recall. ","2d94173e":"* Using multiple models, EDA, and visualization, we were able to identify the key factors that are involved with whether or not a loan application will be accepted, with the biggest one being credit history. \n\n* Our Logistic regression model gave thet highest recall score which is the measure that we wanted to maximize, since we don't want to grant loans to applicants who are unable to repay them, since it would hurt the company.","92f1f127":"## Logistic Regression","238d1d7d":"**Observations:**\n\n- Both the variables are highly skewed to the right and have many outliers which can be expected as the data contains different types of areas - Rural, Urban & Semi-Urban. \n- We can observe from the histogram that majority of values for total income are less than 10,000K dollars.\n\nNow, let's check the percentage of observations in each category for all the categorical variables.","d09d0436":"## Building Classification Models","2a2b4070":"**Let's now find the odds calculated from the logistic regression model coefficients**","5ea27fdc":"**Observations:**\n\n* We see around 82% accuracy on our training set.\n\n* The recall score is only 44% for class 0 which is low, considering we want to get a strong recall value for our specific problem. Thus, this model will not perform well for us. ","e3e6d8b5":"---\n# Classification - Loan Eligibility Prediction\n---","08b4f019":"**Loading Data**","8d1664d2":"### K - Nearest Neighbors (KNN)","14ddb3db":"Let's check the relatioship of total income with loan status","80198f12":"**Observations:**\n- We can see that precision and recall are balanced for threshold of about 0.7.\n- Let's try to calculate the exact threshold where precision and recall are equal."}}