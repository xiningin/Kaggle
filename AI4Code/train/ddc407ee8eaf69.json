{"cell_type":{"a9370cdc":"code","d26d3706":"code","63b46ca2":"code","70fab9b4":"code","7f08d325":"code","a950b8a5":"code","c29e4c73":"code","79ada4f2":"code","54591ca9":"code","55aedd4b":"code","3fbcaf15":"code","30a8cd9e":"code","c47c67f0":"code","21937875":"code","dc837246":"code","93f0c1de":"code","e5ec877e":"code","e283e024":"code","f3b2db3b":"code","22d59e8b":"code","4135dfb5":"code","8fd08f3c":"code","f5fa79fe":"code","34c37362":"code","b737a5e6":"code","3ff2271f":"code","eff8f611":"code","b581e56b":"code","224b878d":"markdown","61a32ff0":"markdown","26fe95ff":"markdown","d7d3b9a6":"markdown","7d48ec78":"markdown","093864f8":"markdown","fe4f576e":"markdown","0bb261a1":"markdown","dca77020":"markdown","d3f5dd75":"markdown","bbb49765":"markdown","c29be5cf":"markdown","95b353e9":"markdown","e7827fdb":"markdown","cef1dc62":"markdown","c22087b3":"markdown","e2a8c40a":"markdown","d0476625":"markdown","b4454adc":"markdown","df71a1fc":"markdown","ae0af79d":"markdown","38b4a421":"markdown","3cd7262a":"markdown","1a1aaed5":"markdown","90acc1a4":"markdown","646d75c4":"markdown","cada4b47":"markdown","ee6c97b0":"markdown","7d76f153":"markdown","f3f2d271":"markdown","4e860bd4":"markdown"},"source":{"a9370cdc":"# import packages\n# matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve\nfrom sklearn.metrics import recall_score, classification_report, auc, roc_curve\nfrom sklearn.metrics import precision_recall_fscore_support, f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom pylab import rcParams\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers\n\n#set random seed and percentage of test data\nRANDOM_SEED = 314 #used to help randomly select the data points\nTEST_PCT = 0.2 # 20% of the data\n\n#set up graphic style in this case I am using the color scheme from xkcd.com\nrcParams['figure.figsize'] = 14, 8.7 # Golden Mean\nLABELS = [\"Normal\",\"Fraud\"]","d26d3706":"col_list = [\"cerulean\",\"scarlet\"]# https:\/\/xkcd.com\/color\/rgb\/\nsns.set(style='white', font_scale=1.75)\nsns.set_palette(sns.xkcd_palette(col_list))","63b46ca2":"df = pd.read_csv(\"..\/input\/creditcard.csv\")","70fab9b4":"df.head(5)","7f08d325":"df.shape","a950b8a5":"# No of null values in dataset\ndf.isnull().values.sum()","c29e4c73":"pd.value_counts(df['Class'])","79ada4f2":"count_classes = pd.value_counts(df['Class'], sort = True)\ncount_classes.plot(kind = 'bar', rot=0, figsize=(14, 8))\nplt.xticks(range(2), LABELS)\nplt.title(\"Frequency by observation number\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Number of Observations\");","54591ca9":"normal_df = df[df['Class']==0]\nfraud_df = df[df['Class']==1]","55aedd4b":"normal_df.Amount.describe()","3fbcaf15":"fraud_df.Amount.describe()","30a8cd9e":"#plot of high value transactions\nbins = np.linspace(200, 2500, 100)\nplt.figure(figsize=(14, 8))\nplt.hist(normal_df.Amount, bins, alpha=1, density=True, label='Normal')\nplt.hist(fraud_df.Amount, bins, alpha=0.6, density=True, label='Fraud')\nplt.legend(loc='upper right')\nplt.title(\"Amount by percentage of transactions (transactions \\$200+)\")\nplt.xlabel(\"Transaction amount (USD)\")\nplt.ylabel(\"Percentage of transactions (%)\");\nplt.show()","c47c67f0":"bins = np.linspace(0, 48, 48) #48 hours\nplt.figure(figsize=(14, 8))\nplt.hist((normal_df.Time\/(60*60)), bins, alpha=1, density=True, label='Normal')\nplt.hist((fraud_df.Time\/(60*60)), bins, alpha=0.6, density=True, label='Fraud')\nplt.legend(loc='upper right')\nplt.title(\"Percentage of transactions by hour\")\nplt.xlabel(\"Transaction time as measured from first transaction in the dataset (hours)\")\nplt.ylabel(\"Percentage of transactions (%)\");\n# plt.hist((df.Time\/(60*60)),bins)\nplt.show()","21937875":"plt.figure(figsize=(14, 8))\nplt.scatter((normal_df.Time\/(60*60)), normal_df.Amount, alpha=0.6, label='Normal')\nplt.scatter((fraud_df.Time\/(60*60)), fraud_df.Amount, alpha=0.9, label='Fraud')\nplt.title(\"Amount of transaction by hour\")\nplt.xlabel(\"Transaction time as measured from first transaction in the dataset (hours)\")\nplt.ylabel('Amount (USD)')\nplt.legend(loc='upper right')\nplt.show()","dc837246":"df_norm = df.copy()\n# Reshape your data either using array.reshape(-1, 1) if your data has a single feature\n# or array.reshape(1, -1) if it contains a single sample.\ndf_norm['Time'] = StandardScaler().fit_transform(df_norm['Time'].values.reshape(-1, 1))\ndf_norm['Amount'] = StandardScaler().fit_transform(df_norm['Amount'].values.reshape(-1, 1))","93f0c1de":"train_x, test_x = train_test_split(df_norm, test_size=TEST_PCT, random_state=RANDOM_SEED)\ntrain_x = train_x[train_x.Class == 0]       # where normal transactions\ntrain_x = train_x.drop(['Class'], axis=1)   # drop the class column\n\n\ntest_y = test_x['Class']                    # save the class column for the test set\ntest_x = test_x.drop(['Class'], axis=1)     # drop the class column\n\ntrain_x = train_x.values                    # transform to ndarray\ntest_x = test_x.values                      # transform to ndarray","e5ec877e":"nb_epoch = 50\nbatch_size = 128\ninput_dim = train_x.shape[1] #num of columns, 30\nencoding_dim = 18\nhidden_dim1 = 10 #int(encoding_dim \/ 2) #i.e. 7\nhidden_dim2 = 6\nlearning_rate = 1e-7","e283e024":"# This returns a tensor\ninput_layer = Input(shape=(input_dim, ))\n\n# a layer instance is callable on a tensor, and returns a tensor\n# Dense implements the operation: output = activation(dot(input, kernel) + bias), where\n# activation is the element-wise activation function passed as the activation argument, \n# kernel is a weights matrix created by the layer, and \n# bias is a bias vector created by the layer (only applicable if use_bias is True).\n\n# activity_regularizer: Regularizer function applied to the output of the layer\nencoder = Dense(encoding_dim, activation=\"tanh\", \n                activity_regularizer=regularizers.l1(learning_rate))(input_layer)\nencoder = Dense(hidden_dim1, activation=\"elu\")(encoder)\nencoder = Dense(hidden_dim2, activation=\"tanh\")(encoder)\ndecoder = Dense(hidden_dim2, activation='elu')(encoder)\ndecoder = Dense(hidden_dim1, activation='tanh')(decoder)\ndecoder = Dense(input_dim, activation='elu')(decoder)\n\n# This creates a model that includes\n# the Input layer and four Dense layers\nautoencoder = Model(inputs=input_layer, outputs=decoder)","f3b2db3b":"# Configure the learning process, by compiling the model\nautoencoder.compile(optimizer='adam',\n                    metrics=['accuracy'],\n                    loss='mean_squared_error')\n\n# Saving the model\ncp = ModelCheckpoint(filepath=\"autoencoder_fraud.h5\",\n                     save_best_only=True,\n                     verbose=0)\n\n# TensorBoard basic visualizations.\n# This callback writes a log for TensorBoard, \n# which allows you to visualize dynamic graphs of your training and test metrics\ntb = TensorBoard(log_dir='.\/logs',\n                 histogram_freq=0,\n                 write_graph=True,\n                 write_images=True)\n\n# Starts training\n# autoencoder: same training(x) and target data(y)\n# validation_data: tuple (x_val, y_val) on which \n# to evaluate the loss and any model metrics at the end of each epoch.\n\n# History.history attribute is a record of training loss values \n# and metrics values at successive epochs.\nhistory = autoencoder.fit(x=train_x, y=train_x,\n                          epochs=nb_epoch,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          validation_data=(test_x, test_x),\n                          verbose=1,\n                          callbacks=[cp, tb]).history","22d59e8b":"# Visualizing metrics on local system\n# !tensorboard --logdir='logs'","4135dfb5":"autoencoder = load_model('autoencoder_fraud.h5')","8fd08f3c":"plt.figure(figsize=(14, 8))\nplt.plot(history['loss'], linewidth=2, label='Train')\nplt.plot(history['val_loss'], linewidth=2, label='Test')\nplt.legend(loc='upper right')\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\n#plt.ylim(ymin=0.70,ymax=1)\nplt.show()","f5fa79fe":"test_x_predictions = autoencoder.predict(test_x)\nmse = np.mean(np.power(test_x - test_x_predictions, 2), axis=1)\nerror_df = pd.DataFrame({'Reconstruction_error': mse,\n                        'True_class': test_y})\nerror_df.describe()","34c37362":"false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df.True_class,\n                                                      error_df.Reconstruction_error)\nroc_auc = auc(false_pos_rate, true_pos_rate,)\n\nplt.figure(figsize=(14, 8))\nplt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)\nplt.plot([0,1],[0,1], linewidth=5)\n\nplt.xlim([-0.01, 1])\nplt.ylim([0, 1.01])\nplt.legend(loc='lower right')\nplt.title('Receiver operating characteristic curve (ROC)')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","b737a5e6":"# calculates precision\/recall using reconstruction error as the decision function\n# returns: \n# precision_rt: Precision values such that element i is the precision of predictions with \n#               score >= thresholds[i] and the last element is 1.\n# recall_rt: Decreasing recall values such that element i is the recall of predictions with \n#           score >= thresholds[i] and the last element is 0.\n# threshold_rt = Increasing thresholds on the decision function used to compute\n#                precision and recall.\nprecision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class,\n                                                               error_df.Reconstruction_error)\n\npr_auc = auc(recall_rt, precision_rt,)\n\nplt.figure(figsize=(14, 8))\nplt.plot(recall_rt, precision_rt, linewidth=5, label='AUC = %0.3f'% pr_auc)\nplt.legend(loc='upper right')\nplt.title('Recall vs Precision')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.show()","3ff2271f":"plt.figure(figsize=(14, 8))\nplt.plot(threshold_rt, precision_rt[1:], label=\"Precision\",linewidth=5)\nplt.plot(threshold_rt, recall_rt[1:], label=\"Recall\",linewidth=5)\nplt.title('Precision and recall for different threshold values')\nplt.xlabel('Threshold')\nplt.ylabel('Precision\/Recall')\nplt.legend()\nplt.show()","eff8f611":"threshold_fixed = 4\ngroups = error_df.groupby('True_class')\nfig, ax = plt.subplots(figsize=(14, 8))\n\nfor name, group in groups:\n    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',\n            label= \"Fraud\" if name == 1 else \"Normal\")\nax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1],\n          colors=\"r\", zorder=100, label='Threshold')\nax.legend()\nplt.title(\"Reconstruction error for different classes\")\nplt.ylabel(\"Reconstruction error\")\nplt.xlabel(\"Data point index\")\nplt.show();","b581e56b":"# As with autoencoders, it is the assumption that fraud or anomalies will suffer \n# from a detectably high reconstruction error, predicting class as 1 (Fraud)\n# if reconstruction error is greater than threshold\npred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]\nconf_matrix = confusion_matrix(error_df.True_class, pred_y)\n\nplt.figure(figsize=(12, 12))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","224b878d":"## Model Loss\nThe loss of our current model seems to be converging and so more training epochs are not likely going to help. Let's explore this visually to confirm.","61a32ff0":"## Balance of Data Visualization\nLet's get a visual confirmation of the unbalanced data in this fraud dataset.","26fe95ff":"Since the fraud cases are relatively few in number compared to bin size, we see the data looks predictably more variable. In the long tail, especially, we are likely observing only a single fraud transaction. It would be hard to differentiate fraud from normal transactions by transaction amount alone.","d7d3b9a6":"## Autoencoder Layer Structure and Parameters\nAutoencoder has symmetric encoding and decoding layers that are \"dense\".\nWe are reducing the input into some form of simplified encoding and then expanding it again. The input and output dimension is the feature space (e.g. 30 columns), so the encoding layer should be smaller by an amount that expect to represent some feature. In this case, I am encoding 30 columns into 14 dimensions so I am expecting high-level features to be represented by roughly two columns (30\/14 = 2.1). Of those high-level features, I am expecting them to map to roughly seven hidden\/latent features in the data.\n\nAdditionally, the epochs, batch size, learning rate, learning policy, and activation functions were all set to values empirically good values.","7d48ec78":"# Model Setup: Basic Autoencoder\nNow that more simplistic methods are not proving that useful, we are justified in exploring our autoencoder to see if it does a little better.","093864f8":"## ROC Curve Check\nReceiver operating characteristic curves are an expected output of most binary classifiers. Since we have an imbalanced data set they are somewhat less useful. Why? Because you can generate a pretty good-looking curve by just simply guessing everything is the normal case because there are so proportionally few cases of fraud. ","fe4f576e":"# Autoencoders and Why You Should Use Them\nAutoencoders are a type of neural network that takes an input (e.g. image, dataset), boils that input down to core features, and reverses the process to recreate the input. Although it may sound pointless to feed in input just to get the same thing out, it is in fact very useful for a number of applications. The key here is that the autoencoder boils down (encodes) the input into some key features that it determines in an unsupervised manner. Hence the name \"autoencoder\" \u2014 it automatically encodes the input.\n\nIt is the assumption in using autoencoders that fraud or anomalies will suffer from a detectably high reconstruction error. ","0bb261a1":"# Exploratory Data Analysis","dca77020":"Source: [datascience.com](http:\/\/www.datascience.com\/blog\/fraud-detection-with-tensorflow?utm_medium=email&_hsenc=p2ANqtz-9IZ7jpDKVn9-SJB_tVIKpBXp3QVcAC6cZivlugt6ptH8oHWCgodII9b0b_uC-Wyz3oVoXiPn4eBfDfEXqVPbPhHw1SaA&_hsmi=65196762&utm_content=65196762&utm_source=hs_email&hsCtaTracking=ed11a1c4-fe1e-4029-9f9c-fe0f8f753f9c%7C1d08bcf9-3f15-4273-a51c-a8369b589fe4)","d3f5dd75":"Hour \"zero\" corresponds to the hour the first transaction happened and not necessarily 12-1am. Given the heavy decrease in normal transactions from hours 1 to 8 and again roughly at hours 24 to 32, I am assuming those time correspond to nighttime for this dataset. If this is true, fraud tends to occur at higher rates during the night. Statistical tests could be used to give evidence for this fact, but are not in the scope of this article. Again, however, the potential time offset between normal and fraud transactions is not enough to make a simple, precise classifier.","bbb49765":"## Reconstruction Error Check\nAutoencoders are trained to reduce reconstruction error which we show below:","c29be5cf":"# Model Evaluation","95b353e9":"In anomaly detection datasets it is common to have the areas of interest \"washed out\" by abundant data. In this dataset,  a lot of low-value transactions that will be generally uninteresting (buying cups of coffee, lunches, etc). This abundant data is likely to wash out the rest of the data, so looking at transactions which are $200+","e7827fdb":"Precision and recall are the eternal tradeoff in data science, so at some point you have to draw an arbitrary line, or a threshold. Where this line will be drawn is essentially a business decision. In this case, you are trading off the cost between missing a fraudulent transaction and the cost of falsely flagging the transaction as a fraudulent even when it is not.","cef1dc62":"## Model Training and Logging\nBelow is where we set up the actual run including checkpoints and the tensorboard.","c22087b3":"# Import and Check Data\nThe data contains 284,807 European credit card transactions that occurred over two days with 492 fraudulent transactions. Everything except the time and amount has been reduced by a Principle Component Analysis (PCA) for privacy concerns.","e2a8c40a":"## Normalize and Scale Data","d0476625":"# Creating The Model","b4454adc":"Although the mean is a little higher in the fraud transactions, it is certainly within a standard deviation and so is unlikely to be easy to discriminate in a highly precise manner between the classes with pure statistical methods.","df71a1fc":"Both time and amount have very different magnitudes, which will likely result in the large magnitude value \"washing out\" the small magnitude value. It is therefore common to scale the data to similar magnitudes. As most of the data (other than 'time' and 'amount') result from the product of a PCA analysis. The PCA done on the dataset transformed it into standard-normal form. I will do the same to the 'time' and 'amount' columns.","ae0af79d":"## Visual Exploration of the Transaction Amount Data","38b4a421":"## Summary Statistics of the Transaction Amount Data\nWe will cut up the dataset into two data frames, one for normal transactions and the other for fraud.","3cd7262a":"## Confusion Matrix\nFinally, we take a look at a traditional confusion matrix for the 20% of the data we randomly held back in the testing set. Here I really take a look at the ratio of detected fraud cases to false positives. A 1:10 ratio is a fairly standard benchmark if there are no business rules or cost tradeoffs that dominate that decision.","1a1aaed5":"Indeed the data seems to be cleaned and loaded as we expect. Now we want to check if we have the expected number of normal and fraudulent rows of data. We will simply pull the \"Class\" column and count the number of normal (0) and fraud (1) rows.","90acc1a4":"## Recall vs. Precision Thresholding\nNow let's look at recall vs. precision to see the trade-off between the two.\n","646d75c4":"\nThe counts are as expected (284,315 normal transactions and 492 fraud transactions). As is typical in fraud and anomaly detection in general, this is a very unbalanced dataset. ","cada4b47":"Again, this is not enough to make a good classifier. For example, it would be hard to draw a line that cleanly separates fraud and normal transactions.","ee6c97b0":"## Reconstruction Error vs Threshold Check","7d76f153":"## Dividing Training and Test Set\nNow we split the data into training and testing sets according to the percentage and with a random seed we wrote at the beginning of the code.","f3f2d271":"## Visual Exploration of the Data by Hour","4e860bd4":"## Visual Exploration of Transaction Amount vs. Hour"}}