{"cell_type":{"4c8e15a9":"code","0e40d853":"code","b04d29e3":"code","780ee92a":"code","ee1540d7":"code","a26e7331":"code","91ddc3c4":"code","512fcae1":"code","4bf6afdf":"code","32182840":"code","7f3236f7":"code","67d9bfbb":"code","1a2e28b0":"code","4fea568c":"code","694b8733":"code","f5d31362":"code","f0f50cd5":"code","c2c6dcee":"code","31552739":"code","e890208b":"code","fc61f917":"code","406531c1":"code","e0f90fd9":"code","b5185b0d":"code","258c8efd":"code","552febe9":"code","380cec4d":"code","8b57445a":"code","7a45a8a5":"code","5085b37b":"code","50f867f4":"code","17c097c6":"code","81e17951":"code","acbfc2c2":"code","c2346417":"code","cf11d262":"code","d6a237c6":"code","1532b812":"markdown","029c777d":"markdown","cb055f9b":"markdown","faca8528":"markdown","7e44b01e":"markdown","f71d864e":"markdown","9e16713e":"markdown","96d7d223":"markdown","082a38f0":"markdown","2035d7c3":"markdown","0649c91b":"markdown","0cc407ff":"markdown","366afe67":"markdown","0409f97e":"markdown","3cc6ac8c":"markdown","e92d4d01":"markdown","0392a4e5":"markdown","441201ab":"markdown","e4da5861":"markdown","f8886fe7":"markdown","903d8778":"markdown","47a3d9ae":"markdown","2efcee1c":"markdown","1bb3683c":"markdown","9dc0fc18":"markdown"},"source":{"4c8e15a9":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","0e40d853":"train = pd.read_csv('..\/input\/bank-marketing-campaign\/bank-full - Copy.csv', sep=';')\ntest = pd.read_csv('..\/input\/bank-marketing-campaign\/bank.csv', sep=';')","b04d29e3":"display(train.head())\ndisplay(test.head())","780ee92a":"train.info(), test.info()","ee1540d7":"obj_columns = []\nint_columns = []\nfor i, x in enumerate(train.dtypes.tolist()):\n    if x == 'object':\n        obj_columns.append(train.columns[i])\n    elif x == 'int64':\n        int_columns.append(train.columns[i])","a26e7331":"for x in obj_columns:\n    print(f'Number of Unique Values in {x} column: ', train[x].nunique())","91ddc3c4":"def graph(name, u):\n    train[name].value_counts().plot(kind=\"bar\",ax=u, color=colors)\n    \n    plt.setp(u.get_xticklabels(), rotation=0)\n    u.set_title(name, fontsize=11, fontdict={\"fontweight\": \"bold\"})\n    \n    for p in u.patches:\n        text = str(int(p.get_height()))\n        u.annotate(text, (p.get_x()+p.get_width()\/2, p.get_height()+100),\n                   ha=\"center\", va='center', fontsize=8, fontweight=\"bold\")\n\n###############################################################################\n# EXPLORATORY DATA ANALYSIS\n\nfig2, ax2 = plt.subplots(4,2, figsize=(11, 10), gridspec_kw={\"wspace\" : 0.4, \"hspace\" : 0.3, \"top\": 0.95})\n\ncolors=[\"#ff0000\",\"#ff8000\",\"#ffff00\",\"#80ff00\",\"#00ff00\", \"#00ff80\", \"#00ffff\", \"#0080ff\", \"#0000ff\", \"#8000ff\", \"#ff00ff\", \"#ff0080\"]\n\ngraph(\"loan\",ax2[0,0])\ngraph(\"marital\",ax2[0,1])\ngraph(\"education\",ax2[1,0])\ngraph(\"default\",ax2[1,1])\ngraph(\"contact\",ax2[2,0])\ngraph(\"poutcome\",ax2[2,1])\ngraph(\"month\",ax2[3,0])\ngraph(\"housing\",ax2[3,1])\nplt.rcParams['axes.axisbelow'] = True","512fcae1":"jobs = train['job'].unique().tolist()","4bf6afdf":"labels1 = []\nfor x in jobs:\n    labels1.append(x + '\\n' + str(round(len(train[(train['job'] == x) & (train['y'] == 'yes')]) \/ len(train[train['job'] == x]) * 100, 2)) + '%')","32182840":"plt.figure(figsize=(10,5))\nsns.countplot(x = train['job'], hue=train['y'])\nlocs, label = plt.xticks()\nnew_xticks = labels1\n_ = plt.xticks(locs, new_xticks, rotation=45) \n_ = plt.xlabel('Job (Subscription Rate)')","7f3236f7":"c_train = train.copy()","67d9bfbb":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom numpy import asarray\nfrom sklearn.preprocessing import StandardScaler","1a2e28b0":"o_encoder = OrdinalEncoder()\nb_encoder = preprocessing.LabelBinarizer()\nl_encoder = preprocessing.LabelEncoder()","4fea568c":"X = c_train.drop(['y'], axis=1)\ny = c_train['y']","694b8733":"oh_list = ['job', 'marital', 'contact', 'poutcome']","f5d31362":"for a in oh_list:\n    if a == 'contact':\n        d = pd.get_dummies(X[a], prefix='ct')\n        X = pd.concat([X,d], axis=1)\n        del X[a]\n    elif a == 'poutcome':\n        d = pd.get_dummies(X[a], prefix='pc')\n        X = pd.concat([X,d], axis=1)\n        del X[a]\n    else:\n        d = pd.get_dummies(X[a], drop_first=True)\n        X = pd.concat([X,d], axis=1)\n        del X[a]\nX.head()","f0f50cd5":"a = o_encoder.fit_transform(asarray(X['education']).reshape(-1,1))\na = a.reshape(45211,)\nX['education'] = a\nX.head()","c2c6dcee":"b_list = ['loan', 'housing', 'default']","31552739":"for l in b_list:\n    a = b_encoder.fit_transform(X[l])\n    X[l] = a\n\nX.head()","e890208b":"a = l_encoder.fit_transform(y)\ny = a\ny","fc61f917":"import math\nX['month'] = X['month'].map({'jan':1,\n               'feb':2,\n               'mar':3,\n               'apr':4,\n               'may':5,\n               'jun':6,\n               'jul':7,\n               'aug':8,\n               'sep':9,\n               'oct':10,\n               'nov':11,\n               'dec':12})\nX['month'] = X['month'].apply(lambda x: math.sin(2*math.pi*x\/12))","406531c1":"scale_col = ['age', 'balance', 'duration', 'day']","e0f90fd9":"for l in scale_col:\n    sc = StandardScaler()\n    scaled_X = sc.fit_transform(asarray(X[l]).reshape(-1,1))\n    X[l] = scaled_X.reshape(45211,)\nX.head()","b5185b0d":"values = model1.coef_[0]\nnames = X_train.columns\n\nimportance = pd.DataFrame({\"value\": values, \"name\": names}).sort_values(\"value\")\nimportance = importance.set_index(\"name\")\n\n# TOP20 FACTORS\ntop20 = pd.concat([importance[\"value\"].head(10),importance[\"value\"].tail(10)])\n\nfig, ax = plt.subplots(figsize=(12,5), gridspec_kw={\"top\": 0.90, \"bottom\":0.05, \"left\":0.2})\n\ntop20.plot.barh(ax=ax)\n\nplt.rcParams['axes.axisbelow'] = True\nplt.ylabel(\"variable name\")\nplt.grid(True)\nplt.title(\"Classification - TOP20 features (importance)\")","258c8efd":"plt.figure(figsize = (10, 6))\ncolors={'no':'red', 'yes':'green'}\nplt.scatter(train['balance'], train['duration'], c=train['y'].map(colors))\nplt.show()","552febe9":"from sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score\nfrom sklearn import metrics","380cec4d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)","8b57445a":"model1 = LogisticRegression(max_iter=1000)\nmodel1.fit(X_train, y_train)\npred1 = model1.predict(X_test)\nacc1 = accuracy_score(pred1, y_test)\nprint(classification_report(pred1, y_test))\nprint(acc1)","7a45a8a5":"model2 = GaussianNB()\nmodel2.fit(X_train, y_train)\npred2 = model2.predict(X_test)\nacc2 = accuracy_score(pred2, y_test)\nprint(classification_report(pred2, y_test))\nprint(acc2)","5085b37b":"model3 = KNeighborsClassifier()\nmodel3.fit(X_train, y_train)\npred3 = model3.predict(X_test)\nacc3 = accuracy_score(pred3, y_test)\nprint(classification_report(pred3, y_test))\nprint(acc3)","50f867f4":"model4= DecisionTreeClassifier(max_depth=10, min_samples_leaf=15)\nmodel4.fit(X_train, y_train)\npred4 = model4.predict(X_test)\nacc4 = accuracy_score(pred4, y_test)\nprint(classification_report(pred4, y_test))\nprint(acc4)","17c097c6":"model5 = RandomForestClassifier()\nmodel5.fit(X_train, y_train)\npred5 = model5.predict(X_test)\nacc5 = accuracy_score(pred5, y_test)\nprint(classification_report(pred5, y_test))\nprint(acc5)","81e17951":"model6 = SVC()\nmodel6.fit(X_train, y_train)\npred6 = model6.predict(X_test)\nacc6 = accuracy_score(pred6, y_test)\nprint(classification_report(pred6, y_test))\nprint(acc6)","acbfc2c2":"len(X_test.columns), len(set(X_test.columns))","c2346417":"model7 = XGBClassifier()\nmodel7.fit(X_train, y_train)\npred7 = model7.predict(X_test)\nacc7 = accuracy_score(pred7, y_test)\nprint(classification_report(pred7, y_test))\nprint(acc7)","cf11d262":"acc_table = pd.DataFrame({'Model': ['Logistic Regression',\n                                   'Naive Bayes',\n                                   'KNN',\n                                   'Decision Tree',\n                                   'Random Forest Tree',\n                                   'SVC',\n                                   'XGB'],\n                         'Accuracy Score': [acc1,\n                                           acc2,\n                                           acc3,\n                                           acc4,\n                                           acc5,\n                                           acc6,\n                                           acc7]})\nacc_table = acc_table.sort_values(by='Accuracy Score', ascending=False)\nacc_table.style.background_gradient(cmap='Blues')","d6a237c6":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\ny_scores = model7.predict_proba(X_test)\nfpr, tpr, threshold = roc_curve(y_test, y_scores[:, 1])\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve of kNN')\nplt.show()","1532b812":"This graphs illustrate how data is distributed for each feature. \n\nAs you can see on the graphs above, most of the features have outstanding counts for one specific value compared to other values. Only housing feature has evenly distributed data. I thought for default feature, if one value has over 95% of data, is it okay if I remove the feature since it cannot be a feature that affects the output.","029c777d":"**poutcome success, duration have the most positive effect on the prediction and poutcome unknown and contact unknown have the most negative effect on the prediction**","cb055f9b":"# **ROC Curve of the Best Model (XGBoost Classifier)**","faca8528":"***Dividing Columns into Numeric Columns and Object Columns***","7e44b01e":"# **Model Comparison Table**","f71d864e":"# **Data Exploration**","9e16713e":"**Encoding Plan** <br>\n* **One Hot Encoding** : 'job, marital, contact, poutcome' <br>\n* **Binary Encoding** : 'loan, housing, default' <br>\n* **Label Encoding**: 'y' <br>\n* **Ordinal Encoding**: 'education' <br>\n* **Sin\/Cosine Encoding**: 'month' <br>","96d7d223":"> Ordinal Encoding","082a38f0":"> Referred to @datark1's notebook on Mushroom Classification. 'Mushrooms - EDA, logistic regression, features'","2035d7c3":">  ","0649c91b":"> Referred to @datark1's notebook on Mushroom Classification. 'Mushrooms - EDA, logistic regression, features'","0cc407ff":"> **Job Distribution**","366afe67":"> Label Encoding","0409f97e":"**I thought the value of the 'month' column should not be treated as numbers (December is not 12 times of January). Since December and January are apart from only a month as much as January and February are, I have decided to convert this integer value with a sin() function so that my logic above makes sense.**","3cc6ac8c":"# **Feature Importance**","e92d4d01":"# **Data Visualization**","0392a4e5":"> No null Values detected","441201ab":"> Make a copy of a data frame","e4da5861":"> Sin\/Cosine Encoding (Month)","f8886fe7":"Check the data distribution with two continuous features which are duration and balance.","903d8778":"# **Data Preprocessing**","47a3d9ae":"# **Machine Learning**","2efcee1c":"**Job Feature Visualization**","1bb3683c":"> Binary Encoding","9dc0fc18":"> One Hot Encoding"}}