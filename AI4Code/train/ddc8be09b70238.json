{"cell_type":{"36259205":"code","bff10411":"code","a4eccf1e":"code","776a8010":"code","537aef7e":"code","c0570778":"code","63390190":"code","e5d74c8f":"code","3a13e843":"code","44dda6fc":"code","00e40515":"code","25ce3aea":"code","0e8945af":"code","76d63925":"code","efe46235":"code","92ce2c6b":"code","3b0cb185":"code","21e0cc97":"code","37164110":"code","d8609d6a":"markdown","efbd158a":"markdown","f8012b62":"markdown","e23ce3dd":"markdown","f3252e92":"markdown","a9a47062":"markdown"},"source":{"36259205":"# Import all necessary librarys \n\nimport pandas as pd \nimport numpy as np \n\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns \n\nfrom sklearn.model_selection import train_test_split \n\nfrom sklearn.linear_model import Lasso, LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import StandardScaler \n","bff10411":"# In here we use a large dataset with 133 features and 114321 records\ndata = pd.read_csv('\/kaggle\/input\/paribas_train.csv')\ndata.shape","a4eccf1e":"data.head()","776a8010":"# Lets take down only numerical variable \n# After removing all categorical features total numerical features is 114\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_val = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_val]\ndata.shape","537aef7e":"# Now seperate Train and Test sets \n\nX_train, X_test, y_train, y_test = train_test_split(\n                                data.drop(labels=['target', 'ID'], axis=1),\n                                data['target'],\n                                test_size=.3, random_state=0)\nX_train.shape, X_test.shape","c0570778":"# Create linear model \nscaller = StandardScaler()\nscaller.fit(X_train.fillna(0))","63390190":"# select lasso l1 properly \n\nsel = SelectFromModel(LogisticRegression(C=1, penalty='l1', solver='liblinear'))\nsel.fit(scaller.transform(X_train.fillna(0)), y_train)","e5d74c8f":"data.head()","3a13e843":"# Lets take down only numerical variable \n# After removing all categorical features total numerical features is 114\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_val = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_val]\ndata.shape","44dda6fc":"# Now seperate Train and Test sets \n\nX_train, X_test, y_train, y_test = train_test_split(\n                                data.drop(labels=['target', 'ID'], axis=1),\n                                data['target'],\n                                test_size=.3, random_state=0)\nX_train.shape, X_test.shape","00e40515":"# select lasso l1 properly \n\nsel = SelectFromModel(LogisticRegression(C=1, penalty='l1', solver='liblinear'))\nsel.fit(scaller.transform(X_train.fillna(0)), y_train)","25ce3aea":"# Okay lets visualize those features what we keep \nsel.get_support()","0e8945af":"# Now, make a list with selected featires \n\nselected_features = X_train.columns[(sel.get_support())]\n\nprint('Total Features    = ', format(X_train.shape[1]))\nprint('Selected Features = ', format(len(selected_features)))\n\ncoeffieient = format(np.sum(sel.estimator_.coef_ == 0))\nprint('Features with coefficients shrank to zero =',coeffieient)\n","76d63925":"# Now find the thoese features (22) which we are going to remove\n\nremoved_features = X_train.columns[(sel.estimator_.coef_ == 0).ravel().tolist()]\nremoved_features","efe46235":"# Now we have to remove the features from training and testing set \n# After removing 22 features we have 90 features remain both train and test set\n\nX_train_selected = sel.transform(X_train.fillna(0))\nX_test_selected = sel.transform(X_test.fillna(0))\n  \nX_train_selected.shape, X_test_selected.shape","92ce2c6b":"# Now seperate Train and Test sets \n\nX_train, X_test, y_train, y_test = train_test_split(\n                                data.drop(labels=['target', 'ID'], axis=1),\n                                data['target'],\n                                test_size=.3, random_state=0)\nX_train.shape, X_test.shape","3b0cb185":"# Create linear model \nscaler = StandardScaler()\nscaler.fit(X_train.fillna(0))\n\nridge = SelectFromModel(LogisticRegression(C=1, penalty='l2'))\nridge.fit(scaler.transform(X_train.fillna(0)), y_train)","21e0cc97":"# Now, make a list with selected featires \n\nselected_features = X_train.columns[(ridge.get_support())]\n\nprint('Total Features    = ', format(X_train.shape[1]))\nprint('Selected Features = ', format(len(selected_features)))\n\ncoeffieient = format(np.sum(ridge.estimator_.coef_ == 0))\nprint('Features with coefficients shrank to zero =',coeffieient)\n","37164110":"selected_features","d8609d6a":"# It is a good practice to flow\n1. **Ridge** = Large Features\n2. **lasso** = Small Features","efbd158a":"To solve this (Under-fitting, Over-Fitting) problems we could use This (Ridge and Lasso) techniques for features engineering.","f8012b62":"### In the machine learning operations most often our model get **under-fitting** and **over-fitting**. But why it happends. \n\n# **Under-Fitting**:\nBasically, when we have few features on a dataset and the score is poor for both traning and testing set then the problem is under-fitting. \n\n# **Over-fitting**:\nAnd on the other hand, when we have large number of features on the dataset and the score is poor for both traning and testing set then the problem is over-fitting. \n\nOkay, In simple word, Ridge and Lasso are the same and simple technique to reduce the model complexity and prevent model from overfitting which may result from simple linear regression. ","e23ce3dd":"# Data Preprocessing","f3252e92":"# Regularisation: Generally it has 3 types \n1. L1 regularisation ( Lasso)\n2. L2 regularisation (Ridge)\n3. L1\/L2 regularisation (Elasticc net)","a9a47062":"## L2 regularisation dose not shrink coefficient to zero "}}