{"cell_type":{"2cdcbfdf":"code","133794ff":"code","a7659ffb":"code","5026ae14":"code","c6a7282e":"code","092a2b31":"code","e16842ee":"code","1a160f1b":"code","51909506":"code","443f0b29":"code","14313aae":"code","c99088e4":"code","a189f941":"code","3c3ba7c3":"code","62ab1fdb":"code","77375209":"code","1b5f5309":"code","e93cc40a":"code","f4c3707b":"code","d5ec3c79":"code","b3d5b1c8":"code","70574b6b":"code","56686df2":"code","e054a805":"code","5dd3a320":"code","94c0d16b":"code","29687d4f":"code","c43fe4ca":"code","53422d7d":"code","5a120285":"code","2d179b3d":"code","cd822e48":"code","74e46217":"code","f89b4fff":"code","e206424a":"code","1121bf0d":"code","5bf5dace":"code","991d1e4d":"code","19eba4c2":"code","569eb077":"code","3991e912":"code","7c3a6b27":"code","dd004271":"code","471b44b5":"code","8930e4c2":"code","4a64fa23":"code","fa16c192":"code","4bdc6453":"code","020044ea":"code","0c7b22db":"code","712a6edc":"code","e83b396e":"code","fdee3a2d":"code","d51052f6":"code","e4f7582b":"code","b0aa5b3c":"code","9c86e29f":"code","9ad9e9fd":"code","ca080414":"code","ca4f301e":"code","22e82b0a":"code","87c6c754":"code","996a0983":"markdown","ba12794a":"markdown","e2a96c0c":"markdown","e9027c51":"markdown","b77b5255":"markdown","a8ace879":"markdown","2e042997":"markdown","2b472a0c":"markdown","46bbc0e3":"markdown","0d8d0162":"markdown","e8cdd983":"markdown","c705e100":"markdown","0c379289":"markdown","0e035d3d":"markdown","9e030d4a":"markdown","05e07b35":"markdown","2e45661b":"markdown","fbceecad":"markdown","ccfa1f2f":"markdown","56841894":"markdown","74b99b20":"markdown"},"source":{"2cdcbfdf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","133794ff":"# Lets set display options\npd.options.display.max_columns = 200\npd.options.display.max_rows = 200\n\n# Lets read training dataset from kaggle\nprices_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\nprices_df","a7659ffb":"# Lets import required sklearn models\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Identify input and target columns\ninput_cols, target_col = prices_df.columns[1:-1], prices_df.columns[-1]\ninputs_df, targets = prices_df[input_cols].copy(), prices_df[target_col].copy()\n\n# Identify numeric and categorical columns\nnumeric_cols = prices_df[input_cols].select_dtypes(include=np.number).columns.tolist()\ncategorical_cols = prices_df[input_cols].fillna('').select_dtypes(include='object').columns.tolist()\n\n# Impute and scale numeric columns\nimputer = SimpleImputer().fit(inputs_df[numeric_cols])\ninputs_df[numeric_cols] = imputer.transform(inputs_df[numeric_cols])\nscaler = MinMaxScaler().fit(inputs_df[numeric_cols])\ninputs_df[numeric_cols] = scaler.transform(inputs_df[numeric_cols])\n\n# One-hot encode categorical columns\nencoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(inputs_df[categorical_cols].fillna(''))\nencoded_cols = list(encoder.get_feature_names(categorical_cols))\ninputs_df[encoded_cols] = encoder.transform(inputs_df[categorical_cols].fillna(''))\n\n# Create training and validation sets\ntrain_inputs, val_inputs, train_targets, val_targets = train_test_split(\n    inputs_df[numeric_cols + encoded_cols], targets, test_size=0.25, random_state=42)","5026ae14":"from sklearn.tree import DecisionTreeRegressor","c6a7282e":"# Create the model\ndtr = DecisionTreeRegressor(random_state=42)","092a2b31":"# Fit the model to the training data\ndtr.fit(train_inputs, train_targets)","e16842ee":"from sklearn.metrics import mean_squared_error","1a160f1b":"tree_train_preds = dtr.predict(train_inputs)","51909506":"tree_train_rmse = np.sqrt(mean_squared_error(train_targets, tree_train_preds, squared=False))","443f0b29":"tree_val_preds = dtr.predict(val_inputs)","14313aae":"tree_val_rmse = np.sqrt(mean_squared_error(val_targets, tree_val_preds, squared=False)) ","c99088e4":"print('Train RMSE: {}, Validation RMSE: {}'.format(tree_train_rmse, tree_val_rmse))","a189f941":"import matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree, export_text\nimport seaborn as sns\nsns.set_style('darkgrid')\n%matplotlib inline","3c3ba7c3":"plt.figure(figsize=(30,15))\n\n# Visualize the tree graphically using plot_tree\nplot_tree(dtr, filled=True)\nplt.show()","62ab1fdb":"# Visualize the tree textually using export_text\ntree_text = export_text(dtr)","77375209":"# Display the first few lines\nprint(tree_text[:2000])","1b5f5309":"# Check feature importance\ntree_importances = []\nfor importance, name in sorted(zip(dtr.feature_importances_, train_inputs.columns),reverse=True)[:5]:\n  tree_importances.append([name,importance])","e93cc40a":"tree_importance_df = pd.DataFrame(tree_importances)\ntree_importance_df.columns = ['feature', 'importance']","f4c3707b":"tree_importance_df","d5ec3c79":"plt.title('Decision Tree Feature Importance')\nsns.barplot(data=tree_importance_df.head(10), x='importance', y='feature');","b3d5b1c8":"from sklearn.ensemble import RandomForestRegressor","70574b6b":"# Create the model\nrf1 = RandomForestRegressor(max_depth=2, random_state=0)","56686df2":"# Fit the model\nrf1.fit(train_inputs, train_targets)","e054a805":"rf1_train_preds = rf1.predict(train_inputs)","5dd3a320":"rf1_train_rmse = np.sqrt(mean_squared_error(train_targets, rf1_train_preds, squared=False))","94c0d16b":"rf1_val_preds = rf1.predict(val_inputs)","29687d4f":"rf1_val_rmse = np.sqrt(mean_squared_error(val_targets, rf1_val_preds, squared=False))","c43fe4ca":"print('Train RMSE: {}, Validation RMSE: {}'.format(rf1_train_rmse, rf1_val_rmse))","53422d7d":"def test_params(**params):\n    model = RandomForestRegressor(random_state=42, n_jobs=-1, **params).fit(train_inputs, train_targets)\n    train_rmse = mean_squared_error(model.predict(train_inputs), train_targets, squared=False)\n    val_rmse = mean_squared_error(model.predict(val_inputs), val_targets, squared=False)\n    return train_rmse, val_rmse","5a120285":"test_params(n_estimators=20, max_depth=20)","2d179b3d":"test_params(n_estimators=50, max_depth=10, min_samples_leaf=4, max_features=0.4)","cd822e48":"def test_param_and_plot(param_name, param_values):\n    train_errors, val_errors = [], [] \n    for value in param_values:\n        params = {param_name: value}\n        train_rmse, val_rmse = test_params(**params)\n        train_errors.append(train_rmse)\n        val_errors.append(val_rmse)\n    plt.figure(figsize=(10,6))\n    plt.title('Overfitting curve: ' + param_name)\n    plt.plot(param_values, train_errors, 'b-o')\n    plt.plot(param_values, val_errors, 'r-o')\n    plt.xlabel(param_name)\n    plt.ylabel('RMSE')\n    plt.legend(['Training', 'Validation'])","74e46217":"test_param_and_plot('max_depth', [5, 10, 15, 20, 25, 30, 35])","f89b4fff":"from sklearn.ensemble import RandomForestRegressor","e206424a":"# Create the model with custom hyperparameters\nrf2 = RandomForestRegressor(max_depth=20, random_state=42, n_jobs=-1)","1121bf0d":"# Train the model\nrf2.fit(train_inputs, train_targets)","5bf5dace":"rf2_train_preds = rf2.predict(train_inputs)","991d1e4d":"rf2_train_rmse = np.sqrt(mean_squared_error(train_targets, rf2_train_preds, squared=False))","19eba4c2":"rf2_val_preds = rf2.predict(val_inputs)","569eb077":"rf2_val_rmse = np.sqrt(mean_squared_error(val_targets, rf2_val_preds, squared=False))","3991e912":"print('Train RMSE: {}, Validation RMSE: {}'.format(rf2_train_rmse, rf2_val_rmse))","7c3a6b27":"rf2_importance_df = pd.DataFrame({\n    'feature': train_inputs.columns,\n    'importance': rf2.feature_importances_\n}).sort_values('importance', ascending=False)","dd004271":"rf2_importance_df","471b44b5":"sns.barplot(data=rf2_importance_df, x='importance', y='feature')\nplt.show()","8930e4c2":"test_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","4a64fa23":"test_df","fa16c192":"test_df[numeric_cols] = imputer.transform(test_df[numeric_cols])\ntest_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\ntest_df[encoded_cols] = encoder.transform(test_df[categorical_cols].fillna(''))","4bdc6453":"test_inputs = test_df[numeric_cols + encoded_cols]","020044ea":"test_preds = rf2.predict(test_inputs)","0c7b22db":"test_preds","712a6edc":"submission_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","e83b396e":"submission_df","fdee3a2d":"submission_df['SalePrice'] = test_preds","d51052f6":"submission_df.to_csv('submission.csv', index=False)","e4f7582b":"from IPython.display import FileLink\nFileLink('submission.csv')","b0aa5b3c":"def predict_input(model, single_input):\n    input_df = pd.DataFrame([single_input])\n    input_df[numeric_cols] = imputer.transform(input_df[numeric_cols])\n    input_df[numeric_cols] = scaler.transform(input_df[numeric_cols])\n    input_df[encoded_cols] = encoder.transform(input_df[categorical_cols].fillna('').values)\n    return model.predict(input_df[numeric_cols + encoded_cols])[0]","9c86e29f":"sample_input = { 'MSSubClass': 20, 'MSZoning': 'RL', 'LotFrontage': 77.0, 'LotArea': 9320, 'Street': 'Pave', 'Alley': None, 'LotShape': 'IR1', 'LandContour': 'Lvl', 'Utilities': 'AllPub', 'LotConfig': 'Inside', 'LandSlope': 'Gtl', 'Neighborhood': 'NAmes', 'Condition1': 'Norm', 'Condition2': 'Norm', 'BldgType': '1Fam', 'HouseStyle': '1Story', 'OverallQual': 4, 'OverallCond': 5, 'YearBuilt': 1959, 'YearRemodAdd': 1959, 'RoofStyle': 'Gable', 'RoofMatl': 'CompShg', 'Exterior1st': 'Plywood', 'Exterior2nd': 'Plywood', 'MasVnrType': 'None','MasVnrArea': 0.0,'ExterQual': 'TA','ExterCond': 'TA', 'Foundation': 'CBlock','BsmtQual': 'TA','BsmtCond': 'TA','BsmtExposure': 'No','BsmtFinType1': 'ALQ', 'BsmtFinSF1': 569,'BsmtFinType2': 'Unf','BsmtFinSF2': 0,'BsmtUnfSF': 381, 'TotalBsmtSF': 950,'Heating': 'GasA','HeatingQC': 'Fa','CentralAir': 'Y','Electrical': 'SBrkr', '1stFlrSF': 1225, '2ndFlrSF': 0, 'LowQualFinSF': 0, 'GrLivArea': 1225, 'BsmtFullBath': 1, 'BsmtHalfBath': 0, 'FullBath': 1, 'HalfBath': 1, 'BedroomAbvGr': 3, 'KitchenAbvGr': 1,'KitchenQual': 'TA','TotRmsAbvGrd': 6,'Functional': 'Typ', 'Fireplaces': 0,'FireplaceQu': np.nan,'GarageType': np.nan,'GarageYrBlt': np.nan,'GarageFinish': np.nan,'GarageCars': 0, 'GarageArea': 0,'GarageQual': np.nan,'GarageCond': np.nan,'PavedDrive': 'Y', 'WoodDeckSF': 352, 'OpenPorchSF': 0, 'EnclosedPorch': 0,'3SsnPorch': 0, 'ScreenPorch': 0, 'PoolArea': 0, 'PoolQC': np.nan, 'Fence': np.nan, 'MiscFeature': 'Shed', 'MiscVal': 400, 'MoSold': 1, 'YrSold': 2010, 'SaleType': 'WD', 'SaleCondition': 'Normal'}","9ad9e9fd":"predicted_price = predict_input(rf2, sample_input)","ca080414":"print('The predicted sale price of the house is ${}'.format(predicted_price))","ca4f301e":"import joblib","22e82b0a":"house_prices_rf = {\n    'model': rf2,\n    'imputer': imputer,\n    'scaler': scaler,\n    'encoder': encoder,\n    'input_cols': input_cols,\n    'target_col': target_col,\n    'numeric_cols': numeric_cols,\n    'categorical_cols': categorical_cols,\n    'encoded_cols': encoded_cols\n}","87c6c754":"joblib.dump(house_prices_rf, 'house_prices_rf.joblib')","996a0983":"## Decision Tree","ba12794a":"### Saving the Model","e2a96c0c":"Lets visualize the decision tree (graphically and textually) and display feature importances as a graph. Limit the maximum depth of graphical visualization to 3 levels.","e9027c51":" Let's make predictions using the random forest regressor.","b77b5255":"Let's also view and plot the feature importances.","a8ace879":"## Hyperparameter Tuning\n\nLet us now tune the hyperparameters of our model.","2e042997":"We can now make predictions using our final model.","2b472a0c":"Let's make predictions and evaluate final model. ","46bbc0e3":"Let's save it as a CSV file and download it.","0d8d0162":"From the above graph, it appears that the best value for `max_depth` is around 20, beyond which the model starts to overfit.","e8cdd983":"First, we need to reapply all the preprocessing steps.","c705e100":"Let's replace the values of the `SalePrice` column with our predictions.","0c379289":"Let's generate predictions on the training and validation sets using the trained decision tree, and compute the RMSE loss.","0e035d3d":"Let's also define a helper function to test and plot different values of a single parameter.","9e030d4a":"### Making Predictions on Single Inputs","05e07b35":"Let's train a random forest regressor using the training set.","2e45661b":"Let's train a decision tree regressor using the training set.","fbceecad":"## Making Predictions on the Test Set\n\nLet's make predictions on the test set provided with the data.","ccfa1f2f":"Let's define a helper function `test_params` which can test the given value of one or more hyperparameters.","56841894":"Let's train a random forest regressor model with your best hyperparameters to minimize the validation loss.","74b99b20":"## Random Forest Regressor"}}