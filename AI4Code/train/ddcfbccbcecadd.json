{"cell_type":{"b50f0be9":"code","d84ccb2e":"code","37f2722e":"code","86bf05aa":"code","f852f649":"code","04fe021a":"code","cf4f6b7e":"code","8c2d9748":"code","bd351c5f":"code","7621e908":"code","f819da5c":"code","e58f250f":"code","ceb87b4f":"code","34c06717":"code","79c9f3bb":"code","9f5c23bd":"code","a09a20a7":"code","29b56c90":"code","65982191":"code","ce832dd3":"code","31ea43a8":"code","9ad9643d":"code","d46af374":"code","1c9e8ccf":"code","9a307f17":"code","8b070eeb":"code","450181b5":"code","f7404f42":"code","d9737850":"code","97d8e3e8":"code","1e6e388b":"code","326cd9f7":"code","66d67059":"code","ca7e83de":"code","e9e92f7c":"code","8e67c76c":"code","f51c4d0b":"code","8cda2f4a":"markdown","1a70b2bf":"markdown","f49238f1":"markdown","4df1bbcd":"markdown","91627885":"markdown","b4e8e752":"markdown","2e260383":"markdown","5ed497c5":"markdown","b5311d5f":"markdown","3aea44f9":"markdown","8f613750":"markdown","f8209382":"markdown","2326b2a0":"markdown","5e81a8d4":"markdown","fd79bb4b":"markdown","620232d4":"markdown","4b5c2f3a":"markdown","41580093":"markdown","aaa33cf4":"markdown","a70e8cac":"markdown","e8907902":"markdown","f0fd9ef4":"markdown","39f423d2":"markdown","c15d1ee6":"markdown","31851f67":"markdown","27b2f3f3":"markdown","3d1ac934":"markdown","301d5a6a":"markdown","9725d23c":"markdown","e46f7d33":"markdown","bf9b2b2b":"markdown","120107d7":"markdown","2e655d2a":"markdown","6953287e":"markdown","daf02006":"markdown","d09b8797":"markdown","621e3711":"markdown","35b26c98":"markdown","e3bb073e":"markdown"},"source":{"b50f0be9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n# Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.tree import export_graphviz\n\n# Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8\n\nimport math\nimport copy\n\nfrom collections import defaultdict\n\nimport warnings\nwarnings.filterwarnings('ignore')","d84ccb2e":"dfTrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndfTest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndfTestIndex = dfTest[\"PassengerId\"]\ndatas = [dfTrain, dfTest]","37f2722e":"for df in datas:\n    df.info()","86bf05aa":"dict_titles = {'Mr.': 1,\n               'Mrs.': 2, \n               'Miss.': 3, \n               'Ms.': 4, \n               'Rev.': 5, \n               'Dr.': 6, \n               'Master.': 7, \n               'Don.': 8,\n               'Major.': 9,\n               'Mme.': 10,\n               'Mlle.': 11,\n               'Col.': 12,\n               'Capt.': 13, \n               'Jonkheer.': 14, \n               'Countess.': 15, \n               'Sir.': 16, \n               'Lady.': 17,\n               'Dona.': 18 # just one person in Test data\n              }\n\nfor df in datas:\n    for i in range(len(df)):\n        for title in dict_titles:\n            if title in df.loc[i,'Name']:\n                df.loc[i,'TitleIndex']=int(dict_titles[title])\n    df[\"TitleIndex\"]=df[\"TitleIndex\"].astype(int)\n\nprint(dfTrain[\"TitleIndex\"].value_counts())\nprint(dfTest[\"TitleIndex\"].value_counts())\n","f852f649":"print(dfTrain.Embarked.value_counts())\nprint(\"Total: {}\".format(dfTrain.Embarked.value_counts().sum()))\nprint(\"Number of NaN values: {}\".format(len(dfTrain[dfTrain.Embarked.isna()]['PassengerId'])))\nfor df in datas:\n    df[\"Embarked\"].fillna(\"Q\", inplace=True)\n    if set(df.Embarked.unique())=={\"S\",\"C\",\"Q\"}: # check if already this replacement was done\n        df[\"Embarked\"] = df[\"Embarked\"].map({\"S\": 2,\"C\": 1,\"Q\": 0}).astype(int)\nprint(\"After transformation\")\nfor df in datas:\n    print(df.Embarked.value_counts())","04fe021a":"for df in datas:\n    df[\"FamilySize\"] = df[\"SibSp\"]+df[\"Parch\"]+1\n    df.drop([\"SibSp\",\"Parch\"],axis=1, inplace=True)\n\n    df[\"Age\"].fillna(df[\"Age\"].dropna().median(),inplace=True)\n    df['AgeRange'] = pd.cut(df['Age'], [0,5,18,50,100], labels=[1,2,3,4])\n    df.drop([\"Age\"],axis=1, inplace=True)\n    df[\"AgeRange\"]=df[\"AgeRange\"].astype(int)    ","cf4f6b7e":"for df in datas:\n    df[\"Sex\"] = df[\"Sex\"].map({\"female\":1,\"male\":0}).astype(int)","8c2d9748":"for df in datas:\n    df.drop([\"PassengerId\",\"Name\",\"Ticket\",\"Fare\",\"Cabin\"],axis=1,inplace=True)","bd351c5f":"for df in datas:\n    print(df.info())","7621e908":"#for vColVal in set([i for i in dfTrain.columns])-set(['Survived']):\n#    dfTrain[vColVal] = (dfTrain[vColVal] - dfTrain[vColVal].mean()) \/ (dfTrain[vColVal].std())    \n#print(dfTrain.mean(),dfTrain.var())\n\n#for vColVal in set([i for i in dfTest.columns])-set(['Survived']):\n#    dfTest[vColVal] = (dfTest[vColVal] - dfTest[vColVal].mean()) \/ (dfTest[vColVal].std())\n#print(dfTest.mean(),dfTest.var())","f819da5c":"CLF = [\n    # navies_bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    # linear_model\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n\n    # neighbors (K Nearest Neighbors)\n    neighbors.KNeighborsClassifier(),\n\n    # tree (Decision Tree)\n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),    \n    \n    # ensemble\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    # gaussian_processes\n    gaussian_process.GaussianProcessClassifier(),\n        \n    # svm (Support Vector Machine)\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    # discriminant_analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n    \n    # xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]","e58f250f":"cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) \n\nCLF_cols = ['Name', 'Parameters','Train Accuracy mean()', 'Test Accuracy mean()', 'Test Accuracy in 3sigma' ,'Time']\nCLF_compare = pd.DataFrame(columns = CLF_cols)\n\nlabels = dfTrain['Survived']\ndf = pd.DataFrame()\ndf = dfTrain.drop('Survived',axis=1)\n\nrow_index = 0\nfor alg in CLF:\n    CLF_compare.loc[row_index, 'Name'] = alg.__class__.__name__\n    CLF_compare.loc[row_index, 'Parameters'] = str(alg.get_params())\n    \n    cv_results = model_selection.cross_validate(alg, df, labels, cv=cv_split, return_train_score=True)\n\n    CLF_compare.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n    CLF_compare.loc[row_index, 'Train Accuracy mean()'] = cv_results['train_score'].mean()\n    CLF_compare.loc[row_index, 'Test Accuracy mean()'] = cv_results['test_score'].mean()   \n    CLF_compare.loc[row_index, 'Test Accuracy in 3sigma'] = cv_results['test_score'].std()*3\n\n    row_index+=1\n    \nCLF_compare.sort_values(by=['Test Accuracy mean()'], ascending=False, inplace=True)\nCLF_compare","ceb87b4f":"if False: # if you want to continue investigation - set this parameter to False\n    data_train = dfTrain\n    labels_train = data_train[\"Survived\"]\n    data_train = data_train.drop(\"Survived\",axis=1)    \n\n    xgb_clf = XGBClassifier()          \n    xgb_clf = xgb_clf.fit(data_train, labels_train)    \n    cv_results = model_selection.cross_validate(xgb_clf, data_train, labels_train, cv=5, return_train_score=True)\n    for p in ['fit_time', 'train_score', 'test_score']:\n        print(\"{:<15}:{:>15.3f}\".format(p,cv_results[p].mean()))    \n    \n    best_res = xgb_clf.predict(dfTest)   \n    dfFinal=pd.DataFrame({\n        \"PassengerId\":dfTestIndex,\n        \"Survived\": best_res })\n    dfFinal.to_csv(\"submission.csv\",index=False)","34c06717":"data_train = dfTrain\nlabels_train = data_train[\"Survived\"]\ndata_train = data_train.drop(\"Survived\",axis=1)\n\nxgb_clf = XGBClassifier()\nxgb_clf.fit(data_train, labels_train)\ncv_results = model_selection.cross_validate(xgb_clf, data_train, labels_train, cv=5, return_train_score=True)\nprint(\"Before tuning\", xgb_clf.score(data_train, labels_train))\nfor p in ['fit_time', 'train_score', 'test_score']:\n    print(\"{:<15}:{:>15.3f}\".format(p, cv_results[p].mean()))       \n    \n# tune model\n# parameters description can be found at http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\ngrid_n_estimator = [10, 50, 100, 300]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_seed = [0]\nparams = {\n            'learning_rate': grid_learn, #default: .3\n            'max_depth': [1,2,4,6,8,10], #default 2\n            'n_estimators': grid_n_estimator, \n            'seed': grid_seed  \n}\n\n# choose best model parameters with GridSearchCV:\n# cv: cross-validation ration\n# n_jobs: how many parallel tasks will be started (-1 means \"all possible\")\n# verbose: verbosity of logging\nxgb_grid = model_selection.GridSearchCV(estimator=xgb_clf, param_grid=params, scoring='roc_auc', cv=10, n_jobs=-1, verbose=False)\nxgb_grid.fit(data_train, labels_train)\ncv_results = model_selection.cross_validate(xgb_grid, data_train, labels_train, cv=5, return_train_score=True)\n\nprint(\"After tuning\", xgb_grid.score(data_train, labels_train))\nfor p in ['fit_time', 'train_score', 'test_score']:\n    print(\"{:<15}:{:>15.3f}\".format(p, cv_results[p].mean()))    ","79c9f3bb":"print('Raw AUC score:', xgb_grid.best_score_)\nfor param_name in sorted(xgb_grid.best_params_.keys()):\n    print(\"%s: %r\" % (param_name, xgb_grid.best_params_[param_name]))\nxgb_grid.best_estimator_.learning_rate    ","9f5c23bd":"tuned_xgb_clf = XGBClassifier(learning_rate=xgb_grid.best_estimator_.learning_rate, \n                              max_depth=xgb_grid.best_estimator_.max_depth,\n                              n_estimators=xgb_grid.best_estimator_.n_estimators,\n                              seed=xgb_grid.best_estimator_.seed)\ntuned_xgb_clf.fit(data_train, labels_train)\ncv_results = model_selection.cross_validate(tuned_xgb_clf, data_train, labels_train, cv=5, return_train_score=True)\nprint(\"After tuning\", tuned_xgb_clf.score(data_train, labels_train))\nfor p in ['fit_time', 'train_score', 'test_score']:\n    print(\"{:<15}:{:>15.3f}\".format(p, cv_results[p].mean()))       \n","a09a20a7":"if False:\n    best_res = xgb_grid.predict(dfTest)\n    \n    dfFinal=pd.DataFrame({\n        \"PassengerId\": dfTestIndex,\n        \"Survived\": best_res })\n    dfFinal.to_csv(\"submission.csv\", index=False)","29b56c90":"data = dfTrain\nlabels = data[\"Survived\"]\ndata = data.drop(\"Survived\",axis=1)\n\nlogreg = linear_model.LogisticRegression()\nlogreg.fit(data,labels)\ncv_results = model_selection.cross_validate(logreg, data, labels, cv=5, return_train_score=True)\nprint(\"Before tuning\", logreg.score(data, labels))\nfor p in ['fit_time', 'train_score', 'test_score']:\n    print(\"{:<15}:{:>15.3f}\".format(p, cv_results[p].mean()))  \n\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression    \npenalty = ['l1', 'l2'] # penalization norm NB only 'saga' supports 'elasticnet', penalty 'none' is not supported by 'liblinear'\n#dual = [True, False] # Dual or primal formulation\ntol = [0.01, 0.001] # tolerance for stopping criteria\nC = np.logspace(0,4,10) # Inverse of regularization strength; must be a positive float.\nmax_iter = np.arange(10,210,10) # default=100 Maximum number of iterations taken for the solvers to converge.\n#fit_intercept = [True, False]\n#solver = ['liblinear', 'sag', 'saga'] # default=\u2019lbfgs\u2019, NB 'newton-cg', 'lbfgs'only for l2 \n\nparameters_dict = dict(penalty=penalty, tol=tol, C=C, max_iter=max_iter)\n\nxgb_grid = model_selection.GridSearchCV(estimator=logreg, param_grid=parameters_dict, cv=5, verbose=1)\nmodel = xgb_grid.fit(data, labels)\n\ncv_results = model_selection.cross_validate(model, data, labels, cv=5, return_train_score=True)\nprint(\"After tuning\", model.score(data, labels))\nfor p in ['fit_time', 'train_score', 'test_score']:\n    print(\"{:<15}:{:>15.3f}\".format(p, cv_results[p].mean()))  \n\n#logreg_rep = metrics.classification_report(y_test, logreg_res)\n#print (logreg_rep)\n","65982191":"df = pd.DataFrame(logreg.get_params(), index=['Before tune'])\ndf.loc['After tune'] = model.best_estimator_.get_params()\ndf","ce832dd3":"res1 = model.predict(dfTest)\nres2 = logreg.predict(dfTest)\n\nfor i in range(len(res1)):\n    if res1[i]==res2[i]:\n        print(\"!\", end=\"\")\n    else:\n        print(\"0\", end=\"\")","31ea43a8":"data = dfTrain\nlabels = data[\"Survived\"]\ndata = data.drop(\"Survived\",axis=1)\n\n#X_train, X_test, y_train, y_test = model_selection.train_test_split(data.values, labels, test_size=0.3, random_state=1)\n#knn = neighbors.KNeighborsClassifier(n_neighbors=4)\n#knn.fit(X_train, y_train)  \n#knn_res = knn.predict(X_test)\n\nparam_grid = {'n_neighbors': np.arange(1,8), 'p': [1,2]}\ncv = 3\nestimator_kNN = neighbors.KNeighborsClassifier()\noptimazer_kNN = model_selection.GridSearchCV(estimator_kNN, param_grid, cv = cv)\noptimazer_kNN.fit(data, labels)\nprint(optimazer_kNN.best_score_)\nprint(optimazer_kNN.best_params_)\n\n\n\n#print (\"knn.score={}\".format(knn.score(X_test, y_test)))  \n#knn_rep = metrics.classification_report(y_test, knn_res)\n#print (knn_rep)","9ad9643d":"data = dfTrain\nlabels = data[\"Survived\"]\ndata = data.drop(\"Survived\",axis=1)\n\nparam_grid = {'n_estimators': np.arange(20,101,10), 'min_samples_split': np.arange(4,11, 1)}\ncv = 3\nestimator_rf = ensemble.RandomForestClassifier()\noptimazer_rf = model_selection.GridSearchCV(estimator_rf, param_grid, cv=cv)\noptimazer_rf.fit(data, labels)\nprint(optimazer_rf.best_score_)\nprint(optimazer_rf.best_params_)\n","d46af374":"data_train = dfTrain\nlabels_train = data_train[\"Survived\"]\ndata_train = data_train.drop(\"Survived\",axis=1)\n\ndata_test = dfTest\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(data_train.values, labels_train, test_size=0.3, random_state=17)\n\ntree_clf = tree.DecisionTreeClassifier()\ntree_clf.fit(X_train, y_train)\ncv_results = model_selection.cross_validate(tree_clf, data_train.values, labels_train, cv  = 5)\n\n#feature selection\ntree_clf_rfe = feature_selection.RFECV(tree_clf, step = 1, scoring = 'accuracy', cv = 5)\ntree_clf_rfe.fit(X_train, y_train)\nX_rfe = data_train.columns.values[tree_clf_rfe.get_support()]\ncv_results = model_selection.cross_validate(tree_clf_rfe, data_train[X_rfe], labels_train, cv  = 5)\n\n#tune model\nparam_grid = {\n              'criterion': ['gini', 'entropy'],\n              'max_depth': range(1,11),\n              'min_samples_split': [2,5,10,.03,.05],\n              'min_samples_leaf': [1,5,10,.03,.05],\n              'max_features': range(1,6),\n              'random_state': [0]\n             }\n#choose best model with grid_search:\ntree_clf = model_selection.GridSearchCV(tree_clf, param_grid=param_grid, scoring = 'roc_auc', cv=5, n_jobs=-1, verbose=False)\ntree_clf.fit(X_train, y_train)\ncv_results = model_selection.cross_validate(tree_clf, data_train[X_rfe], labels_train, cv  = 5)\n\ntree_rep = cv_results","1c9e8ccf":"print('Raw AUC score:', tree_clf.best_score_)\nfor param_name in sorted(tree_clf.best_params_.keys()):\n    print(\"%s: %r\" % (param_name, tree_clf.best_params_[param_name]))","9a307f17":"if False:\n    best_res = tree_clf.predict(dfTest)\n    \n    dfFinal=pd.DataFrame({\n        \"PassengerId\": dfTestIndex,\n        \"Survived\": best_res })\n    dfFinal.to_csv(\"submission.csv\", index=False)","8b070eeb":"data = dfTrain \nlabels = data[\"Survived\"]\ndata = data.drop(\"Survived\",axis=1)\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(data.values, labels, test_size=0.3, random_state=1)\n#gbc = XGBClassifier()\n#gbc.fit(X_train, y_train)\n#gbc_res=gbc.predict(X_test)\n\ngbc = ensemble.GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\ngbc_res = gbc.predict(X_test)\nprint(\"Before tuning, gbc.score={}\".format(gbc.score(X_test, y_test)))\ngbc_rep = metrics.classification_report(y_test, gbc_res)\nprint (gbc_rep)\n\n#tune hyper-parameters: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\ngrid_n_estimator = [10, 50, 100, 300]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\nparam_grid = {'learning_rate': [.05], #default=0.1\n            'n_estimators': grid_n_estimator, #default=100\n            'criterion': ['friedman_mse', 'mse', 'mae'], #default=\u201dfriedman_mse\u201d\n            'max_depth': grid_max_depth, #default=3   \n            'random_state': grid_seed\n             }\n     \ntune_model = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = 3)\ntune_model.fit(X_train, y_train)\ngbc_res=tune_model.predict(X_test)\nprint(\"After tuning, gbc.score={}\".format(tune_model.score(X_test, y_test)))\ngbc_rep = metrics.classification_report(y_test, gbc_res)\nprint (gbc_rep)","450181b5":"print('Raw AUC score:', tune_model.best_score_)\nfor param_name in sorted(tune_model.best_params_.keys()):\n    print(\"%s: %r\" % (param_name, tune_model.best_params_[param_name]))","f7404f42":"if True:\n    best_res = tune_model.predict(dfTest)\n    \n    dfFinal=pd.DataFrame({\n        \"PassengerId\": dfTestIndex,\n        \"Survived\": best_res })\n    dfFinal.to_csv(\"submission.csv\", index=False)","d9737850":"data = dfTrain \nlabels = data[\"Survived\"]\ndata = data.drop(\"Survived\", axis=1)\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(data.values, labels, test_size=0.3, random_state=1)\n\nsvc_clf = svm.SVC()\nsvc_clf.fit(X_train, y_train)\nsvc_res = svc_clf.predict(X_test)\n\nprint(\"Before tuning, svc_clf.score={}\".format(svc_clf.score(X_test, y_test)))\nsvc_rep = metrics.classification_report(y_test, svc_res)\nprint (svc_rep)\n\nparam_grid = {\n            'C': [1,2,3,4,5], #default=1.0\n            'gamma': [.1, .25, .5, .75, 1.0], #default: auto\n            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n            'probability': [True],\n            'random_state': [0]\n}\n     \ntune_model = model_selection.GridSearchCV(svm.SVC(), param_grid=param_grid, scoring = 'roc_auc', cv = 3)\ntune_model.fit(df, labels)\nsvc_res = tune_model.predict(X_test)\nprint(\"After tuning, svc.score={}\".format(tune_model.score(data.values, labels)))\nsvc_rep = metrics.classification_report(y_test, svc_res)\nprint (svc_rep)    \n","97d8e3e8":"print('Raw AUC score:', tune_model.best_score_)\nfor param_name in sorted(tune_model.best_params_.keys()):\n    print(\"%s: %r\" % (param_name, tune_model.best_params_[param_name]))","1e6e388b":"print(\"Logistic regression results:\")\nprint(logreg_rep)\nprint(\"K nearest neighbours results:\")\nprint(knn_rep)\nprint(\"Decision tree results:\")\nprint(tree_rep)\nprint(\"Gradient boosting results:\")\nprint(gbc_rep)\nprint(\"SVC:\")\nprint(svc_rep)","326cd9f7":"print(X_rfe)","66d67059":"data_for_predictions = dfTest","ca7e83de":"data_train = dfTrain\nlabels_train = data_train[\"Survived\"]\ndata_train = data_train.drop(\"Survived\",axis=1)\ntuned_xgb_clf.fit(data_train, labels_train)","e9e92f7c":"best_res = tuned_xgb_clf.predict(data_for_predictions)","8e67c76c":"dfFinal=pd.DataFrame({\n    \"PassengerId\":dfTestIndex,\n    \"Survived\": best_res })","f51c4d0b":"dfFinal.to_csv(\"submission.csv\",index=False)","8cda2f4a":"All data transformation below (unnecessary features drop, new features create, data type change, etc) are based on exploratory data analyze (EDA) made in notebook https:\/\/www.kaggle.com\/yurychernyshov\/titanic-task-exploratory-analyze.","1a70b2bf":"<a href='#toc'>Back to TOC<\/a>","f49238f1":"I collect below all models which I am familiar with and try to compare them.","4df1bbcd":"<a href='#toc'>Back to TOC<\/a>","91627885":"# Experiments with different models","b4e8e752":"<a href='#toc'>Back to TOC<\/a>","2e260383":"## \"Name\" feature: extract title","5ed497c5":"## \"Embarked\" feature: change embarkation port letter (\"S\", \"C\" or \"Q\") to digit.","b5311d5f":"# Random Forest <a name='random_forest'>","3aea44f9":"# Compare results. Conclusions. <a name='conclusions'>","8f613750":"## Drop unnecessary features \"PassengerId\", \"Name\", \"Ticket\", \"Fare\", \"Cabin\"","f8209382":"# Data preprocessing <a name='data_preprocessing'>","2326b2a0":"# Logistic Regression <a name='logistic_regression'>","5e81a8d4":"## XGBClassifier <a name='xgbclf'>","fd79bb4b":"# Import data <a name='import_data'>","620232d4":"# SVC <a name='svc'>","4b5c2f3a":"<a href='#toc'>Back to TOC<\/a>","41580093":"# K Neighbors Classifier <a name='knn'>","aaa33cf4":"<a href='#toc'>Back to TOC<\/a>","a70e8cac":"# Write results to submission.csv <a name='record'>","e8907902":"Now we see that train_score and test_score are better than before tuning.","f0fd9ef4":"<a href='#toc'>Back to TOC<\/a>","39f423d2":"<a href='#toc'>Back to TOC<\/a>","c15d1ee6":"You can try to use the winner immediately - XGBClassifier() with default settings (as is). Baseline is 0.77990.","31851f67":"## \"SibSp\", \"Parch\", \"Age\" features: create new features \"FamilySize\", \"AgeRange\"","27b2f3f3":"## Normalization is not needed to all models (e.g. for DecisionTree), it can be done in proper part of notebook below (if needed)","3d1ac934":"# Compare different models <a name='compare_models'>","301d5a6a":"<a href='#toc'>Back to TOC<\/a>","9725d23c":"# Decision Tree <a name='tree'>","e46f7d33":"<a href='#toc'>Back to TOC<\/a>","bf9b2b2b":"## \"Sex\" feature: change words \"male\" and \"female\" to digits","120107d7":"# Table of contents <a name='toc'>\n* [Import data.](#import_data)\n* [Usefull functions definition.](#functions)\n* [Data preprocessing.](#data_preprocessing)\n* [Compare different models.](#compare_models)\n* [XGBClassifier.](#xgbclf)\n* [Logistic Regression.](#logistic_regression)\n* [KNN.](#knn)\n* [Random Forest.](#random_forest)\n* [Decision Tree.](#tree)\n* [Gradient Boosting.](#gbc)\n* [SVC.](#svc)\n* [Compare results. Conclusions.](#conclusions)\n* [Write results to submission.csv.](#record)\n   \n    ","2e655d2a":"# Gradient Boosting <a name='gbc'>","6953287e":"# Useful functions definitions <a name='functions'>","daf02006":"<a href='#toc'>Back to TOC<\/a>","d09b8797":"<a href='#toc'>Back to TOC<\/a>","621e3711":"<a href='#toc'>Back to TOC<\/a>","35b26c98":"<a href='#toc'>Back to TOC<\/a>","e3bb073e":"We already use information from \"Name\" (in \"TitleIndex\"). I plan to analyze \"Ticket\", \"Fare\", \"Cabin\" more precisely in future (it seems now that there are no usefull information in those features)."}}