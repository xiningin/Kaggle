{"cell_type":{"3494e7d6":"code","20f42181":"code","4c15e506":"code","f932a7a1":"code","b4f6c900":"code","b7e3a01d":"code","81f1c2c0":"code","e9f36e5a":"code","7a31081d":"code","0e0d7f39":"code","a3e58d50":"code","402c6730":"code","cae4c52d":"code","051cdb8f":"code","06d08fa0":"code","e69db78a":"code","49797f8a":"code","c5214485":"code","373b103e":"code","a056638c":"code","19e771c5":"code","4d4355e0":"code","9306b795":"code","f70116ff":"code","219880e3":"code","4eaee93b":"code","e8f52a16":"code","2942196b":"code","a022de79":"markdown","09416cc6":"markdown","51edced4":"markdown","217e3aa4":"markdown","9a5c28ef":"markdown","fde907ef":"markdown","0744f5ab":"markdown","92a83411":"markdown","fe5487eb":"markdown","e2764e89":"markdown"},"source":{"3494e7d6":"import pandas as pd\n\n# load the first dataset\nnews_dataset = pd.read_csv(\"..\/input\/fake-news-dataset\/train.csv\")","20f42181":"news_dataset.info()","4c15e506":"news_dataset.head()","f932a7a1":"news_dataset['class'].value_counts()","b4f6c900":"news_dataset[news_dataset['class'] == 'February 5, 2017']","b7e3a01d":"news_dataset['Unnamed: 6'].value_counts()","81f1c2c0":"import numpy as np\n\n# shifting the column values in the respective places\nnews_dataset.iloc[504, 2] = news_dataset.iloc[504, 3]\nnews_dataset.iloc[504, 3] = news_dataset.iloc[504, 4]\nnews_dataset.iloc[504, 4] = news_dataset.iloc[504, 5]\nnews_dataset.iloc[504, 5] = news_dataset.iloc[504, 6]\nnews_dataset.iloc[504, 6] = np.nan","e9f36e5a":"news_dataset.iloc[504]","7a31081d":"news_dataset.drop(columns=['index', 'Unnamed: 6'], inplace=True)","0e0d7f39":"news_dataset.info()","a3e58d50":"news_dataset.to_csv('news_dataset_1.csv', index=False)","402c6730":"# sanity check\nnews_dataset = pd.read_csv('news_dataset_1.csv')\nnews_dataset.info()","cae4c52d":"news_dataset.head()","051cdb8f":"for col in news_dataset.columns:\n    print(news_dataset.iloc[0][col] + \"\\n\")","06d08fa0":"news_dataset['subject'].value_counts()","e69db78a":"news_dataset['class'].value_counts() \/ news_dataset.shape[0]","49797f8a":"from nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r\"\\w+\")\n\ndef tokenize_text(x):\n    \"\"\"\n    x: a Pandas Series\n    \n    returns a pandas series of token (str) lists\n    \"\"\"\n    \n    return x.apply(tokenizer.tokenize)\n\ndef standardize_text(x):\n    \"\"\"\n    x: a Pandas Series\n    \"\"\"\n    \n    x = x.str.replace(r\"http\\S+\", \"\")\n    x = x.str.replace(r\"http\", \"\")\n    x = x.str.replace(r\"@\\S+\", \"\")\n    x = x.str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n\\ ]\", \"\")\n    x = x.str.replace(r\"@\", \"at\")\n    x = x.str.lower()\n    return x\n\nenglish_stopwords = stopwords.words('english')\n\ndef remove_stopwords(token_list):\n    token_list = [tok for tok in token_list if tok not in english_stopwords]\n    return token_list\n\ndef remove_stopwords_from_series(x):\n    \"\"\"\n    x: a Pandas Series of token lists\n    \"\"\"\n    \n    x = x.apply(remove_stopwords)\n    return x\n\ndef standard_tokens_from_text(x):\n    \"\"\"\n    x: a Pandas Series of strings\n    \"\"\"\n    \n    x = standardize_text(x)\n    x = tokenize_text(x)\n    x = remove_stopwords_from_series(x)\n    return x","c5214485":"news_dataset['title_tokens'] = standard_tokens_from_text(news_dataset['title'])\nnews_dataset['text_tokens'] = standard_tokens_from_text(news_dataset['text'])","373b103e":"news_dataset['title_tokens']","a056638c":"news_dataset.info()","19e771c5":"# separate the dataset into fake and real\nfake_news = news_dataset[news_dataset['class'] == 'Fake']\nreal_news = news_dataset[news_dataset['class'] == 'Real']","4d4355e0":"news_dataset[(news_dataset['title'].apply(lambda x: len(x)<=10))]","9306b795":"from collections import Counter\nfrom nltk.util import ngrams\n\ndef count_ngrams(token_list, counter, n):\n    if len(token_list) >= n:\n        counter.update(ngrams(token_list, n))\n\n# count anagrams in the titles of fake news\nfake_title_unigram_counts = Counter()\nfake_news['title_tokens'].apply(lambda x: count_ngrams(x, fake_title_unigram_counts, 1));\n\n# count anagrams in the titles of real news\nreal_title_unigram_counts = Counter()\nreal_news['title_tokens'].apply(lambda x: count_ngrams(x, real_title_unigram_counts, 1));","f70116ff":"# count bigrams in the titles of fake news\nfake_title_bigram_counts = Counter()\nfake_news['title_tokens'].apply(lambda x: count_ngrams(x, fake_title_bigram_counts, 2));\n\n# count bigrams in the titles of real news\nreal_title_bigram_counts = Counter()\nreal_news['title_tokens'].apply(lambda x: count_ngrams(x, real_title_bigram_counts, 2));","219880e3":"# count trigrams in the titles of fake news\nfake_title_trigram_counts = Counter()\nfake_news['title_tokens'].apply(lambda x: count_ngrams(x, fake_title_trigram_counts, 3));\n\n# count trigrams in the titles of real news\nreal_title_trigram_counts = Counter()\nreal_news['title_tokens'].apply(lambda x: count_ngrams(x, real_title_trigram_counts, 3));","4eaee93b":"import nltk\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')\nplt.title('Top 20 Unigrams in Fake News')\nnltk.FreqDist(fake_title_unigram_counts).plot(20, cumulative=False, color = 'r');\n\nplt.title('Top 20 Unigrams in Real News')\nnltk.FreqDist(real_title_unigram_counts).plot(20, cumulative=False, color = 'b');","e8f52a16":"plt.title('Top 20 Bigrams in Fake News')\nnltk.FreqDist(fake_title_bigram_counts).plot(20, cumulative=False, color = 'r');\n\nplt.title('Top 20 Bigrams in Real News')\nnltk.FreqDist(real_title_bigram_counts).plot(20, cumulative=False, color = 'b');","2942196b":"plt.title('Top 20 Trigrams in Fake News')\nnltk.FreqDist(fake_title_trigram_counts).plot(20, cumulative=False, color = 'r');\n\nplt.title('Top 20 Trigrams in Real News')\nnltk.FreqDist(real_title_trigram_counts).plot(20, cumulative=False, color = 'b');","a022de79":"The record seems to have been shifted to the right due to the id value being repeated at the beginning.","09416cc6":"There seems to be a single wrong value in the class column","51edced4":"# Exploring the Fake News Data","217e3aa4":"Find the most common unigrams and bigrams in fake and real news.","9a5c28ef":"## Exploring the features","fde907ef":"## Visualizing the data","0744f5ab":"Standardize the data by removing punctuation, links, mentions and stopwords","92a83411":"Notes about the data:\n\n* text: this dataset seems to have the apostrophe (single quote character) removed. But this is okay since all punctuation will probably be removed during data preparation. Text seems to have links and mentions (e.g. @RogerJStoneJr) that are probably useless.\n* subject: I don't think I will use this column as a feature since I want the model to detect fakeness purely based on the title and text. Also most of the categories can be considered \"politics\" so I don't think it's going to be very useful.\n* date: I won't use the date as a feature.\n* class: This is the target the model should try to predict. It is a string that can be either 'Fake' or 'Real'. Needs to be converted to binary (1 or 0). The ratio of fake to real articles in the dataset is about 52% to 48%.\n\nThe dataset has no missing (nan) values.","fe5487eb":"## Loading the data","e2764e89":"Saving the fixed dataset."}}