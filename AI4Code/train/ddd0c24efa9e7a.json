{"cell_type":{"26caf54b":"code","05c7a9a2":"code","4bc06593":"code","f087941a":"code","ffcc00a0":"code","76b30126":"code","b0dad8a3":"code","c8c743f6":"code","8f4859fd":"code","6b011943":"code","451fd17c":"code","f3b14a74":"code","61ba901f":"markdown","15796069":"markdown","5b34b3bc":"markdown","9145e0ab":"markdown","fb845c8e":"markdown","f5e1d665":"markdown"},"source":{"26caf54b":"import warnings\nwarnings.filterwarnings('ignore')\nimport string\nimport re\nfrom unicodedata import normalize\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential,load_model\nfrom keras.layers import LSTM,Dense,Embedding,RepeatVector,TimeDistributed\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.translate.bleu_score import corpus_bleu\nimport pandas as pd\nfrom string import punctuation\nimport matplotlib.pyplot as plt\nfrom IPython.display import Markdown, display\ndef printmd(string):\n    # Print with Markdowns    \n    display(Markdown(string))","05c7a9a2":"# How many sentences will be used\n# Limit the sentences to 10.000 on Kaggle to avoid exceding the\n# available RAM space\n# Build a generator to avoid this issue\ntotal_sentences = 10000\n\n# Load the dataset\ndataset = pd.read_csv(\"..\/input\/englishportuguese-translation\/por.txt\", \n                      sep='\\t', \n                      nrows = total_sentences, \n                      names=[\"EN\",\"PT\", \"Attribution\"], \n                      header=None)\n\n# What proportion of the sentences will be used for the test set\ntest_proportion = 0.1\ntrain_test_threshold = int( (1-test_proportion) * total_sentences)\n\nprintmd(f'## {total_sentences} \"parallel sentences\" will be loaded (original sentence + its translation)')\nprintmd(f'## {train_test_threshold} \"parallel sentences\" will be used to train the model')\nprintmd(f'## {total_sentences-train_test_threshold} \"parallel sentences\" will be used to test the model')","4bc06593":"# Shuffle the dataset\ndataset = dataset.sample(frac=1, random_state=0)\ndataset.iloc[1000:1010]","f087941a":"def clean(string):\n    # Clean the string\n    string = string.replace(\"\\u202f\",\" \") # Replace no-break space with space\n    string = string.lower()\n    \n    # Delete the punctuation and the numbers\n    for p in punctuation + \"\u00ab\u00bb\" + \"0123456789\":\n        string = string.replace(p,\" \")\n        \n    string = re.sub('\\s+',' ', string)\n    string = string.strip()\n           \n    return string\n\ndataset.drop(labels = \"Attribution\", axis = 1,inplace = True)\n\n# Clean the sentences\ndataset[\"EN\"] = dataset[\"EN\"].apply(lambda x: clean(x))\ndataset[\"PT\"] = dataset[\"PT\"].apply(lambda x: clean(x))\n\n# Select one part of the dataset\ndataset = dataset.values\ndataset = dataset[:total_sentences]\n\n# split into train\/test\ntrain, test = dataset[:train_test_threshold], dataset[train_test_threshold:]\n\n# Define the name of the source and of the target\n# This will be used in the outputs of this notebook\nsource_str, target_str = \"Portuguese\", \"English\"\n\n# The index in the numpy array of the source and of the target\nidx_src, idx_tar = 1, 0","ffcc00a0":"# Display the result after cleaning\npd.DataFrame(dataset[1000:1010])","76b30126":"def create_tokenizer(lines):\n    # fit a tokenizer\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n \ndef max_len(lines):\n    # max sentence length\n    return max(len(line.split()) for line in lines)\n\ndef encode_sequences(tokenizer, length, lines):\n    # encode and pad sequences\n    X = tokenizer.texts_to_sequences(lines) # integer encode sequences\n    X = pad_sequences(X, maxlen=length, padding='post') # pad sequences with 0 values\n    return X\n \ndef encode_output(sequences, vocab_size):\n    # one hot encode target sequence\n    ylist = list()\n    for sequence in sequences:\n        encoded = to_categorical(sequence, num_classes=vocab_size)\n        ylist.append(encoded)\n    y = np.array(ylist)\n    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n    return y\n \n# Prepare target tokenizer\ntar_tokenizer = create_tokenizer(dataset[:, idx_tar])\ntar_vocab_size = len(tar_tokenizer.word_index) + 1\ntar_length = max_len(dataset[:, idx_tar])\nprintmd(f'\\nTarget ({target_str}) Vocabulary Size: {tar_vocab_size}')\nprintmd(f'Target ({target_str}) Max Length: {tar_length}')\n\n# Prepare source tokenizer\nsrc_tokenizer = create_tokenizer(dataset[:, idx_src])\nsrc_vocab_size = len(src_tokenizer.word_index) + 1\nsrc_length = max_len(dataset[:, idx_src])\nprintmd(f'\\nSource ({source_str}) Vocabulary Size: {src_vocab_size}')\nprintmd(f'Source ({source_str}) Max Length: {src_length}\\n')\n \n# Prepare training data\ntrainX = encode_sequences(src_tokenizer, src_length, train[:, idx_src])\ntrainY = encode_sequences(tar_tokenizer, tar_length, train[:, idx_tar])\ntrainY = encode_output(trainY, tar_vocab_size)\n\n# Prepare test data\ntestX = encode_sequences(src_tokenizer, src_length, test[:, idx_src])\ntestY = encode_sequences(tar_tokenizer, tar_length, test[:, idx_tar])\ntestY = encode_output(testY, tar_vocab_size)","b0dad8a3":"def create_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n    # Create the model\n    model = Sequential()\n    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n    model.add(LSTM(n_units))\n    model.add(RepeatVector(tar_timesteps))\n    model.add(LSTM(n_units, return_sequences=True))\n    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n    return model\n \n# Create model\nmodel = create_model(src_vocab_size, tar_vocab_size, src_length, tar_length, 256)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\n\nhistory = model.fit(trainX, \n          trainY, \n          epochs=200, \n          batch_size=64, \n          validation_split=0.1, \n          verbose=1,\n          callbacks=[\n                        EarlyStopping(\n                        monitor='val_loss',\n                        patience=2,\n                        restore_best_weights=True\n                    )\n            ])","c8c743f6":"pd.DataFrame(history.history).plot()\nplt.title(\"Loss\")\nplt.show()","8f4859fd":"def word_for_id(integer, tokenizer):\n    # map an integer to a word\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n \ndef predict_seq(model, tokenizer, source):\n    # generate target from a source sequence\n    prediction = model.predict(source, verbose=0)[0]\n    integers = [np.argmax(vector) for vector in prediction]\n    target = list()\n    for i in integers:\n        word = word_for_id(i, tokenizer)\n        if word is None:\n            break\n        target.append(word)\n    return ' '.join(target)\n\ndef compare_prediction(model, tokenizer, sources, raw_dataset, limit=20):\n    # evaluate a model\n    actual, predicted = [], []\n    src = f'{source_str.upper()} (SOURCE)'\n    tgt = f'{target_str.upper()} (TARGET)'\n    pred = f'AUTOMATIC TRANSLATION IN {target_str.upper()}'\n    print(f'{src:30} {tgt:25} {pred}\\n')\n    \n    for i, source in enumerate(sources): # translate encoded source text\n        source = source.reshape((1, source.shape[0]))\n        translation = predict_seq(model, tar_tokenizer, source)\n        raw_target, raw_src = raw_dataset[i]\n        print(f'{raw_src:30} {raw_target:25} {translation}')\n        if i >= limit: # Display some of the result\n            break\n \n# test on some training sequences\nprint('### Result on the Training Set ###')\ncompare_prediction(model, tar_tokenizer, trainX, train)\n\n# test on some test sequences\nprint('\\n\\n### Result on the Test Set ###')\ncompare_prediction(model, tar_tokenizer, testX, test)","6b011943":"# It takes long to compute the BLEU Score\n\ndef bleu_score(model, tokenizer, sources, raw_dataset):\n    # Get the bleu score of a model\n    actual, predicted = [], []\n    for i, source in enumerate(sources):\n        # translate encoded source text\n        source = source.reshape((1, source.shape[0]))\n        translation = predict_seq(model, tar_tokenizer, source)\n        raw_target, raw_src = raw_dataset[i]\n        actual.append([raw_target.split()])\n        predicted.append(translation.split())\n        \n    bleu_dic = {}\n    bleu_dic['1-grams'] = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n    bleu_dic['1-2-grams'] = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n    bleu_dic['1-3-grams'] = corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))\n    bleu_dic['1-4-grams'] = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n    \n    return bleu_dic\n\n# Compute the BLEU Score\nbleu_train = bleu_score(model, tar_tokenizer, trainX, train)\nbleu_test = bleu_score(model, tar_tokenizer, testX, test)","451fd17c":"plt.bar(x = bleu_train.keys(), height = bleu_train.values())\nplt.title(\"BLEU Score with the training set\")\nplt.ylim((0,1))\nplt.show()","f3b14a74":"plt.bar(x = bleu_test.keys(), height = bleu_test.values())\nplt.title(\"BLEU Score with the test set\")\nplt.ylim((0,1))\nplt.show()","61ba901f":"# Train a Machine Translation model PT->EN with BLEU Score\n### *Use a list a translated sentences in French and English to train the model*\n### \\#Keras \\#Deep Learning \\#NLTK \\#BLEU Score\n\n*CC-BY 2.0 (France) Attribution: tatoeba.org*","15796069":"# 2. Create and train the model<a class=\"anchor\" id=\"2\"><\/a>","5b34b3bc":"# 3. Result on the test set<a class=\"anchor\" id=\"3\"><\/a>","9145e0ab":"## 1. Data Preprocessing<a class=\"anchor\" id=\"1\"><\/a><a class=\"anchor\" id=\"1\"><\/a>","fb845c8e":"# 4. Prediction evaluation with BLEU <a class=\"anchor\" id=\"4\"><\/a>\n\nBLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine's output and that of a human: \"the closer a machine translation is to a professional human translation, the better it is\" \u2013 this is the central idea behind BLEU. BLEU was one of the first metrics to claim a high correlation with human judgements of quality, and remains one of the most popular automated and inexpensive metrics.\n\nScores are calculated for individual translated segments\u2014generally sentences\u2014by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. Intelligibility or grammatical correctness are not taken into account.\n\nBLEU's output is always a number between 0 and 1. This value indicates how similar the candidate text is to the reference texts, with values closer to 1 representing more similar texts. Few human translations will attain a score of 1, since this would indicate that the candidate is identical to one of the reference translations. For this reason, it is not necessary to attain a score of 1. Because there are more opportunities to match, adding additional reference translations will increase the BLEU score.\n\n\n## Algorithm\n\nBLEU uses a modified form of precision to compare a candidate translation against multiple reference translations. The metric modifies simple precision since machine translation systems have been known to generate more words than are in a reference text. This is illustrated in the following example from Papineni et al. (2002)\n\n![example bleu](https:\/\/i.imgur.com\/mrdpwun.png)\n\nOf the seven words in the candidate translation, all of them appear in the reference translations. Thus the candidate text is given a unigram precision of P, where *m* is number of words from the candidate that are found in the reference, and *wt* is the total number of words in the candidate. This is a perfect score, despite the fact that the candidate translation above retains little of the content of either of the references.\n\nThe modification that BLEU makes is fairly straightforward. For each word in the candidate translation, the algorithm takes its maximum total count, *m_max*, in any of the reference translations. In the example above, the word \"the\" appears twice in reference 1, and once in reference 2. Thus *m_max* = 2.\n\nFor the candidate translation, the count *mw* of each word is clipped to a maximum of *m_max* for that word. In this case, \"the\" has *m_w* = 7 and *m_max*=2, thus *m_w* is clipped to 2. These clipped counts *m_w* are then summed over all distinct words in the candidate. This sum is then divided by the total number of unigrams in the candidate translation. In the above example, the modified unigram precision score would be: P = 1 \/ 7\n\nIn practice, however, using individual words as the unit of comparison is not optimal. Instead, BLEU computes the same modified precision metric using n-grams. The length which has the \"highest correlation with monolingual human judgements\" was found to be four. The unigram scores are found to account for the adequacy of the translation, how much information is retained. The longer n-gram scores account for the fluency of the translation, or to what extent it reads like \"good English\". ([source](https:\/\/en.wikipedia.org\/wiki\/BLEU))\n\n![bleu score 2](https:\/\/i.imgur.com\/tNYaD64.png)","f5e1d665":"![USA Brazil](https:\/\/i.imgur.com\/tPnRdbi.png)\n\n# Table of contents\n\n[<h3>1. Data Preprocessing<\/h3>](#1)\n\n[<h3>2. Create and train the model<\/h3>](#2)\n\n[<h3>3. Result on the test set<\/h3>](#3)\n\n[<h3>4. Prediction evaluation with BLEU<\/h3>](#4)\n"}}