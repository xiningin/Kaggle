{"cell_type":{"3f160190":"code","5f7a72b6":"code","04b7295d":"code","43d8f12f":"code","00fed155":"code","890695ec":"code","336d719c":"code","563bda65":"code","b109c2f1":"code","cb03f9d6":"code","e9c230dd":"code","a41dd335":"code","b656e76c":"code","4025d351":"code","4df16e64":"code","c18b6a4b":"code","1d4a22f1":"code","6817b0dc":"code","fe0f8ef7":"code","de7275a7":"code","8acf9ed9":"code","c1a92512":"code","7f81bdf0":"code","a0a62d14":"code","38f978e2":"code","702b4419":"code","4e06a680":"code","707445b2":"code","16b2fa03":"code","e82f0218":"code","154c9e1c":"markdown","2387b61c":"markdown","d6c364d7":"markdown","387fe24b":"markdown","cac52664":"markdown","5e26f3e9":"markdown","fae1cd5d":"markdown","1d3d7683":"markdown","497d8e01":"markdown","c644af35":"markdown","2b60ff28":"markdown","e39e7b32":"markdown","8ca0b0ec":"markdown","40e2ded5":"markdown","e359033f":"markdown","dedb2395":"markdown","2a10d70b":"markdown","fa904b7e":"markdown","a19322ab":"markdown","dd10ca69":"markdown","28836b23":"markdown","de50b92b":"markdown","ba5da5fa":"markdown","27b0c0bd":"markdown","8a03207d":"markdown","8d524bec":"markdown","0dbcd9e5":"markdown","bf84cf00":"markdown","ede290b0":"markdown","f5847d10":"markdown"},"source":{"3f160190":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.decomposition import PCA","5f7a72b6":"!pip install git+https:\/\/github.com\/DrGFreeman\/rps-cv.git","04b7295d":"!ln -s ..\/input\/rps-cv-images .\/img","43d8f12f":"from rpscv import imgproc","00fed155":"X, y = imgproc.generateGrayFeatures()","890695ec":"X.shape, y.shape","336d719c":"img = X[100].reshape((200, 300))\nplt.imshow(img, cmap='gray')\nplt.xticks([])\nplt.yticks([])\nplt.show()","563bda65":"np.unique(y, return_counts=True)","b109c2f1":"for i in np.unique(y):\n    print(i, imgproc.utils.gestureTxt[i])","cb03f9d6":"pca = PCA(n_components=40)\npca.fit(X)","e9c230dd":"plt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.scatter(range(pca.n_components_), pca.explained_variance_ratio_)\nplt.xlabel('Principal component')\nplt.ylabel('Explained variance ratio')\nplt.title('Explained variance ratio by principal component')\nplt.grid()\nplt.subplot(1, 2, 2)\nplt.plot(pca.explained_variance_ratio_.cumsum())\nplt.xlabel('Principal component')\nplt.ylabel('Explained variance ratio')\nplt.title('Cummulative explained variance ratio')\nplt.grid()\nplt.tight_layout()\nplt.show()","a41dd335":"pca.components_.shape","b656e76c":"plt.imshow(pca.components_[0].reshape((200, 300)), cmap='gray')\nplt.xticks([])\nplt.yticks([])\nplt.show()","4025d351":"pca.mean_.shape","4df16e64":"plt.imshow(pca.mean_.reshape((200, 300)), cmap='gray')\nplt.xticks([])\nplt.yticks([])\nplt.show()","c18b6a4b":"pc_imgs = pca.components_.reshape((len(pca.components_), 200, 300))\n\nnb_col = 4\nnb_row = pc_imgs.shape[0] \/\/ nb_col\nplt.figure(figsize=(4 * nb_col, 3.2 * nb_row))\nfor i in range(nb_col * nb_row):\n    plt.subplot(nb_row, nb_col, i+1)\n    plt.imshow(pc_imgs[i], cmap='gray')\n    plt.title(\"Principal component {:d}\\nExpl. var. ratio {:.4f}\".format(i, pca.explained_variance_ratio_[i]))\n    plt.xticks(())\n    plt.yticks(())\nplt.show()","1d4a22f1":"X_pca = pca.transform(X)\n\nX_pca.shape","6817b0dc":"X_pca[123]","fe0f8ef7":"img_from_pcs = np.dot(X_pca[123], pca.components_) + pca.mean_\n\nimg_from_pcs.shape","de7275a7":"plt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.imshow(X[123].reshape((200, 300)), cmap='gray')\nplt.title('Original image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1, 2, 2)\nplt.imshow(img_from_pcs.reshape((200, 300)), cmap='gray')\nplt.title('Image from principal components')\nplt.xticks([])\nplt.yticks([])\nplt.tight_layout()\nplt.show()","8acf9ed9":"X_from_pcs = pca.inverse_transform(X_pca)\n\nX_from_pcs.shape","c1a92512":"rock = X[100]\npaper = X[800]\nscissors = X[1500]","7f81bdf0":"def show_img_pcs(img):\n    plt.figure(figsize=(16, 4))\n    \n    # Display original image\n    plt.subplot(1, 3, 1)\n    plt.imshow(img.reshape(200, 300), cmap='gray');\n    plt.title('Original image')\n    plt.xticks(())\n    plt.yticks(())\n    \n    #Display principal components magnitude\n    plt.subplot(1, 3, 2)\n    img_pc = pca.transform([img])\n    plt.bar(range(1, img_pc.shape[1] + 1), img_pc[0,:])\n    plt.title('Image principal components magnitude')\n    plt.xlabel('Principal component')\n    plt.ylabel('Magnitude')\n    \n    # Display reconstituted image\n    plt.subplot(1, 3, 3)\n    plt.imshow(pca.inverse_transform(img_pc).reshape(200, 300), cmap=plt.cm.gray)\n    plt.title('Image reconstituted from principal components')\n    plt.xticks(())\n    plt.yticks(())\n    \n    plt.tight_layout()\n    plt.show()","a0a62d14":"show_img_pcs(rock)","38f978e2":"show_img_pcs(paper)","702b4419":"show_img_pcs(scissors)","4e06a680":"def progressive_plot(index):\n    img = X[index]\n    img_pc = X_pca[index]\n    plt.figure(figsize=(16, 4 * (pca.n_components_ + 1)))\n    for i in range(-1, pca.n_components_):\n        # Display the bargraph of image pc features\n        plt.subplot(pca.n_components_ + 1, 3, 3 * i + 4)\n        if i == -1:\n            plt.imshow(img.reshape((200, 300)), cmap='gray')\n            plt.title('Original image')\n            plt.xticks([])\n            plt.yticks([])\n        else:\n            bars = plt.bar(range(pca.n_components_), img_pc, color='lightgray')\n            for j in range(i):\n                bars[j].set_color('#6495ED')\n            bars[i].set_color('r')\n            plt.title('Image principal components magnitude')\n            plt.xlabel('Principal component')\n            plt.ylabel('Magnitude')\n        # Display the left image (principal component vector being added)\n        plt.subplot(pca.n_components_ + 1, 3, 3 * i + 5)\n        if i == -1:\n            plt.imshow(pca.mean_.reshape((200, 300)), cmap='gray')\n            plt.title('Mean')\n            plt.xticks([])\n            plt.yticks([])\n        else:\n            plt.imshow((img_pc[i] * pca.components_[i]).reshape((200, 300)), cmap='gray')\n            plt.title('Principal component vector {} * {:.3g}'.format(i, img_pc[i]))\n            plt.xticks([])\n            plt.yticks([])\n        # Display the right image (progressively reconstituted image)\n        plt.subplot(pca.n_components_ + 1, 3, 3 * i + 6)\n        if i == -1:\n            plt.imshow(pca.mean_.reshape((200, 300)), cmap='gray')\n            plt.title('Mean')\n            plt.xticks([])\n            plt.yticks([])\n        else:\n            plt.imshow((np.dot(img_pc[:i+1], pca.components_[:i+1]) + pca.mean_).reshape((200, 300)), cmap='gray')\n            plt.title('Principal components 0-{} + mean'.format(i))\n            plt.xticks([])\n            plt.yticks([])\n    plt.tight_layout()\n    plt.show()","707445b2":"progressive_plot(800)","16b2fa03":"progressive_plot(100)","e82f0218":"progressive_plot(1500)","154c9e1c":"The `inverse_transform` method of the PCA class allows to perform this transformation from the principal component space to the original image feature space on the entire array of transformed images with ease.","2387b61c":"Lets now plot all the 40 principal component vectors as images. We include the explained variance ratio of each principal component in the title as reference.","d6c364d7":"Create a symbolic link *img* in the current directory pointing to the dataset of images. The *rpscv* module requires the images to be available at this location.","387fe24b":"Along with the principal component vectors, the PCA transformer also computes the mean of all the input images. This mean is stored as the `mean_` attribute of the PCA object.","cac52664":"As expected, the mean has the same shape as the input features vector and the principal components vectors. We can therefore visualize the mean after reshaping it to the dimensions of the original images.","5e26f3e9":"# Setup\nImport the required libraries.","fae1cd5d":"# Introduction\nThis kernel presents a visualisation of the Principal Component Analysis (PCA) method used as dimensionality reduction in my **Rock-Paper-Scissors game using computer vision and machine learning on the Raspberry Pi** project (ref. [https:\/\/github.com\/DrGFreeman\/rps-cv](https:\/\/github.com\/DrGFreeman\/rps-cv)).\n\n![RPS gui animated gif](https:\/\/github.com\/DrGFreeman\/rps-cv\/raw\/master\/img\/doc\/rps.gif)\n\nThe objective is to develop an intuition of the meaning of PCA applied to images rather than to present a mathematically rigourous definition of PCA. For the latter, refer to [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis) which does a much better job at this than I could ever do.\n\n## Dataset\nThe dataset consits of the unprocessed images captured using the **Rock-Paper-Scissors** game and is available on Kaggle at [https:\/\/www.kaggle.com\/drgfreeman\/rockpaperscissors](https:\/\/www.kaggle.com\/drgfreeman\/rockpaperscissors).","1d3d7683":"We now apply the `progressive_plot` function to an image of the *rock* gesture. Notice how some of the principal component vectors are used to \"hide\" the extended fingers. This is opposite to the *paper* gesture image where the same principal components were used to \"add\" the extended fingers to the mean image.","497d8e01":"The `y` array stores the lables corresponding to each image encoded as integers.","c644af35":"Our transformed features array `X_pca` now represent each image by a vector of dimension 40. We have therefore reduced our feature space from 60,000 dimensions to 40 dimensions, a reduction factor of 1500X !\n\nWe can pick any image from the transformed feature array and look at its content.","2b60ff28":"There are 2188 images in the dataset. The `X` array stores the images, each represented as a vector of 60,000 pixel intensity values. These correspond to the 200 x 300 pixels of the original images in a flattened form.\n\nWe can pick any image from the `X` array, reshape it to its original dimensions and display it using the pyplot `imshow` function:","e39e7b32":"We apply the `progressive_plot` function to an image of the *paper* gesture.","8ca0b0ec":"Lets now pick three images, one image of each class, and display the original image, the transformed feature vector and the image reconstituted from the principal component vectors.","40e2ded5":"Let's check the shapes of the features `X` and labels `y` arrays.","e359033f":"The plots of the principal components vectors show that the first principal components encode relatively simple shapes. As the principal component number increases, the shapes represented by each principal component becomes more and more complex.\n\n## Representation of images by their principal components\n\nWhen using PCA as a means of dimensionality reduction, we want to encode each image in a vector of lower dimension than the original. This vector corresponds to factors by which we can multiply each of the principal component vectors to represent the original image. Let's go through this step by step.\n\nFirst, we use the PCA object `transform` method to transform our original image feature vectors into their principal component representation.","dedb2395":"We define a function to apply the same plot setup to the three images.","2a10d70b":"The text labels corresponding to the integer labels are stored in the `improc.utils.gesturesTxt` dictionary.","fa904b7e":"As we specified, the principal components array contains 40 components, each represented by a vector of dimension 60,000.\n\n**The principal component vectors always have the same shape as the input features vector**.\n\nThis means that we can visualize the principal components the same way as we visualize the input images.\nLet's look at the first principal component vector reshaped to dimensions 200 x 300.","a19322ab":"We reshape this image vector to 200 x 300 pixels and display it alongside the original image.","dd10ca69":"And plot the three images.","28836b23":"FInally, we apply the `progressive_plot` function to an image of the *scissors* gesture. In this case different principal component vectors are selectively used to define the two fingers extended in a \"V\" shape.","de50b92b":"We create a function to plot show the contribution of each of the principal component vectors in the progressive reconstitution of an image based on its features in the principal component space. The function takes as argument the index of the image to be plotted.","ba5da5fa":"We can use the 40 coefficients from this vector to multiply the 40 principal component vectors. This corresponds to the dot product of the transformed feature vector by the array of principal components. We can then add the PCA mean vector and obtain an image vector reconstructed from the principal components.","27b0c0bd":"# Summary\nWe have seen how Principal Component Analysis extracts a set of orthogonal principal component vectors from the array of input image vectors. These principal components vectors have the same shape as the original images and can therefore be visualized in the same way as the input images. The first principal component vectors represent relatively simple patterns and explain most of the variance in the dataset whereas the higher principal component vectors represent pattens of increasing complexity and a smaller fraction of the total explained variance in the dataset.\n\nWe have seen that the high dimensional feature vectors of the input images can be transformed into smaller vectors of length equal to the number of principal components. These vectors represent how much of each principal component vector is present in the image. These vectors of reduced dimension can be used as feature vectors representing the original images in a supervised or unsupervised machine learning model.\n\nFinally, we have seen how we can use the mean of all images along with a linear superposition of the principal component vectors to reconstitute approximations of the original images. To do so, the coefficients from the reduced dimension image feature vectors are used to multiply each principal component vector, the sum of which is added to the mean. Performing this linear superposition progressively allowed us to develop an intuition of the meaning of the coefficients of the transformed image feature vectors.","8a03207d":"Clone and install the *rpscv* package from the **Rock-Paper-Scissors** game project repository on GitHub ([DrGFreeman\/rps-cv](https:\/\/github.com\/DrGFreeman\/rps-cv) ). This package contains the *imgproc* module with functions for pre-processing of the images.","8d524bec":"# Dataset exploration\nWe use the *generateGrayFeatures* function of the *imgproc* module to load the image data `X` and labels `y`. This functions pre-processes the images to remove the green background and returns an array of grayscale images along with a vector of image labels.","0dbcd9e5":"The principal components are orthogonal vectors, in the original feature space, where the first component represents the axis of maximum variance in the data, the second components represents the second axis of maximum variance, and so on, in order of decreasing value of explained variance.\n\nThe explained variance ratio (*explained_variance* \/ *total_variance*) of each component can be accessed from the `explained_variance_ratio_` attribute of the PCA object.\n\nLet's plot the explained variance ratio of each principal component.","bf84cf00":"# Principal Component Analysis\nTo perform Principal Component Analysis, we use the `PCA` transformer from the Scikit-Learn `decomposition` module. For this demonstration, we choose to extract the 40 first principal components. When using PCA as a dimensionality reduction step in a machine learning pipeline, the number of principal components to extract is a parameter that can be tuned along with other model hyperparameters.\n\nWith the PCA transformer defined, we fit it to the array of image feature vectors `X`.","ede290b0":"Import the *imgproc* module of the *rpscv* package","f5847d10":"We can see that the first 40 principal components explain more than 80% of the total variance and the first 5 components explain more than 50% of the total variance.  As the number of components increases, the variance explained by each new component reduces.\n\n## Principal component vectors\n\nThe calculated principal components are available as the `components_` attribute of the fitted PCA object.\n\nLet's look at the shape of the principal components array."}}