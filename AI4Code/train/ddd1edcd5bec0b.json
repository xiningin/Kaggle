{"cell_type":{"cd0ca2df":"code","2c150fcd":"code","6d64c74a":"code","00c9888b":"code","5e09e07e":"code","89bbb867":"code","76e91f65":"code","501ccd39":"code","0068d2a9":"code","eb730cdf":"code","2e119792":"code","1964101c":"code","7769395c":"code","387155c2":"code","55348231":"code","5c27fe88":"code","65f57470":"code","e9364d80":"code","af06d5a6":"code","1a12a313":"code","ad6f50f4":"code","f0edd78b":"code","84a19b5f":"code","bbde3404":"code","2388b082":"code","b3a38a50":"code","a922ecbf":"code","52b37ede":"code","e116bb35":"code","25eeee93":"code","c7b25135":"code","597cedd2":"code","c53c9122":"code","7c2a8b34":"code","3540f9be":"code","a9bf8d07":"code","cc3668f3":"code","b9472147":"code","b9b57814":"code","447d9d2b":"code","902d4898":"code","e5de4dd0":"code","60b7a864":"code","b3738e71":"code","70f7fc74":"code","6a7f13cc":"code","a8c5dbf1":"code","8256b16c":"code","34c58e54":"code","f3394aa5":"code","61f13dc4":"code","a777ac69":"code","74784c12":"code","960a86f3":"code","75d10993":"code","cbab0b0f":"code","0e6ca9f8":"code","456e88e3":"code","47d28718":"code","5c30ebe9":"code","3066b37b":"code","9bbc2b1d":"code","74bf6bcd":"code","7b151997":"code","65f54333":"code","4bf5dd3d":"code","2fc951ff":"code","a21242d6":"code","583e240a":"code","ae47e446":"code","75680d8f":"code","96695f72":"code","1f898f81":"code","39a8b516":"code","ebfa67ac":"code","ed0c36fd":"code","27e5caad":"code","047c52c8":"markdown","b399c464":"markdown","f76e7687":"markdown","e3538dc6":"markdown","00abd2c7":"markdown","31429249":"markdown","16b5cecc":"markdown","c2adf05b":"markdown","3fc60ddc":"markdown","699452d3":"markdown","adf7d120":"markdown","93ff1b76":"markdown","7f2a46c7":"markdown","7b06cad0":"markdown","c88a1490":"markdown","485d9b3f":"markdown","ee531730":"markdown","12bdc483":"markdown","b90771cb":"markdown","37f56a4d":"markdown"},"source":{"cd0ca2df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2c150fcd":"# install packages to use state of art pretrained Sentiment model\n!pip  install vaderSentiment\n!pip install textblob","6d64c74a":"import pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec \nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\nimport matplotlib_venn as venn\nfrom gensim.summarization.summarizer import summarize\nimport datetime \nimport statistics as stat\nimport vaderSentiment\n","00c9888b":"sample_submission = pd.read_csv(\"..\/input\/hacky-zs\/dataset\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/hacky-zs\/dataset\/test_file.csv\")\ntrain = pd.read_csv(\"..\/input\/hacky-zs\/dataset\/train_file.csv\")","5e09e07e":"train.shape","89bbb867":"train.info()","76e91f65":"train.head(10)","501ccd39":"df= train\nprint(\"source_len\",len(df['Source'].unique()))\nprint(\"source_varients\",df['Source'].unique())\nprint(\"topic_varients\",df['Topic'].unique())","0068d2a9":"def topic_obama(data):\n    if data == 'obama':\n        return 1\n    else:\n        return 0\n    \ndef topic_economy(data):\n    if data == 'economy':\n        return 1\n    else:\n        return 0\ndef topic_microsoft(data):\n    if data == 'microsoft':\n        return 1\n    else:\n        return 0\ndef topic_palestine(data):\n    if data == 'palestine':\n        return 1\n    else:\n        return 0\n\ndf['topic_obama'] = df['Topic'].apply(topic_obama)\ndf['topic_economy'] = df['Topic'].apply(topic_economy)\ndf['topic_microsoft'] = df['Topic'].apply(topic_microsoft)\ndf['topic_palestine'] = df['Topic'].apply(topic_palestine)","eb730cdf":"dt = test\n#test dataset\ndef topic_obama(data):\n    if data == 'obama':\n        return 1\n    else:\n        return 0\n    \ndef topic_economy(data):\n    if data == 'economy':\n        return 1\n    else:\n        return 0\ndef topic_microsoft(data):\n    if data == 'microsoft':\n        return 1\n    else:\n        return 0\ndef topic_palestine(data):\n    if data == 'palestine':\n        return 1\n    else:\n        return 0\n\ndt['topic_obama'] = dt['Topic'].apply(topic_obama)\ndt['topic_economy'] = dt['Topic'].apply(topic_economy)\ndt['topic_microsoft'] = dt['Topic'].apply(topic_microsoft)\ndt['topic_palestine'] = dt['Topic'].apply(topic_palestine)","2e119792":"df","1964101c":"#check for rows which are white spaces\nprint(df[df['Source']==' '])\n\n#converts all empty values into Nan\ndf","7769395c":"#count no of nan\ndf.isnull().sum()","387155c2":"df = df.dropna()\ndf.shape","55348231":"source = df['Source'].unique()\nfor i in  source:\n    print(i )","5c27fe88":"#no of unique sources\nprint('No of Unique sources=',len(df['Source'].unique()))\n\n#no of unique sources which are mentioned only once \nval_count = df['Source'].value_counts()\ncount = 0  \nfor i in val_count: \n    if i ==1:\n        count += 1\nprint(\"No of sources which aren't repeated=\",count)","65f57470":"df['Month']=pd.DatetimeIndex(df['PublishDate']).month\ndf['Year']= pd.DatetimeIndex(df['PublishDate']).year\ndf['Day'] =pd.to_datetime(df['PublishDate']).dt.weekday_name\ndf['Time'] =pd.to_datetime(df['PublishDate']).dt.time\ndf['Date_day'] = pd.to_datetime(df['PublishDate']).dt.day\n# dropping the publish date column after extracting the data out\ndf.drop(columns=['PublishDate'])","e9364d80":"dt['Month']=pd.DatetimeIndex(dt['PublishDate']).month\ndt['Year']= pd.DatetimeIndex(dt['PublishDate']).year\ndt['Day'] =pd.to_datetime(dt['PublishDate']).dt.weekday_name\ndt['Time'] =pd.to_datetime(dt['PublishDate']).dt.time\ndt['Date_day'] = pd.to_datetime(dt['PublishDate']).dt.day\n# dropping the publish date column after extracting the data out\ndt.drop(columns=['PublishDate'])","af06d5a6":"#frequency of news articles for each day  \ndf['Day'].value_counts()","1a12a313":"#frequency of news articles for each month \ndf['Month'].value_counts()","ad6f50f4":"#frequency of news articles for each Year \ndf['Year'].value_counts()","f0edd78b":"#frequency of news articles for each time interval\nprint(df['Time'].value_counts())\n#no of unique sources which are mentioned only once \nval_count = df['Time'].value_counts()\ncount = 0  \nfor i in val_count: \n    if i ==1:\n        count += 1\nprint(count) \n\n# as the number of time interval with one data point is more we will keep those values","84a19b5f":"df['Date_day'].value_counts()","bbde3404":"year_list = (2002,2008,2012)\nmonth_list = [9,8,7,6,4]\ndf.drop(df[(df['Year']== 2002) |\n                                   (df['Year']== 2008)|\n                                   (df['Year']== 2012) |\n                                   (df['Month']== 4)|\n                                   (df['Month']== 6)|\n                                   (df['Month']== 7)|\n                                   (df['Month']== 8)|\n                                   (df['Month']== 9)\n                                  ].index, inplace=True)\n","2388b082":"#data frame shape after removal of all the unimportant rows \ndf.shape","b3a38a50":"dt.shape","a922ecbf":"#gather sentiment data for each day \nday = df['Day'].unique()\nsentiment_title=[]\nsentiment_headline= []\nfor i in day:\n    df1=df[df['Day'].str.contains(i)]\n    sentiment_title.append(df1['SentimentTitle'].mean())\n    sentiment_headline.append(df1['SentimentHeadline'].mean())\n    \n%matplotlib inline\n\npos = list(range(len(sentiment_title)))\nwidth = 0.25\n\nfig, ax = plt.subplots(figsize=(10,5))\n    \nplt.bar(pos, sentiment_title, width, alpha=0.5, color='#EE3224')\nplt.bar([p+width for p in pos], sentiment_headline, width, alpha=0.5, color='#F78F1E')\n    \nax.set_ylabel('sentiment_score')\nax.set_title('Sentiment vs Day')\nax.set_xticks([p +  width for p in pos])\nax.set_xticklabels(day)\nplt.xlim(min(pos)-width, max(pos)+width*4)\nplt.ylim([min(sentiment_title+sentiment_headline)+0.001, max(sentiment_title+ sentiment_headline)+0.001])\nplt.legend(['Sentiment_Title', 'Sentiment_Headline'], loc='upper left')\nplt.grid()\nplt.show()","52b37ede":"#gather sentiment data for each day \nday = df['Date_day'].unique()\nsentiment_title=[]\nsentiment_headline= []\nfor i in day:\n    df1=df[df['Date_day']==i]\n    sentiment_title.append(df1['SentimentTitle'].mean())\n    sentiment_headline.append(df1['SentimentHeadline'].mean())\n    \n%matplotlib inline\n\npos = list(range(len(sentiment_title)))\nwidth = 0.25\n\nfig, ax = plt.subplots(figsize=(10,5))\n    \nplt.bar(pos, sentiment_title, width, alpha=0.5, color='#EE3224')\nplt.bar([p+width for p in pos], sentiment_headline, width, alpha=0.5, color='#F78F1E')\n    \nax.set_ylabel('sentiment_score')\nax.set_title('Sentiment vs Date_Day')\nax.set_xticks([p +  width for p in pos])\nax.set_xticklabels(day)\nplt.xlim(min(pos)-width, max(pos)+width*4)\nplt.ylim([min(sentiment_title+sentiment_headline)+0.001, max(sentiment_title+ sentiment_headline)+0.001])\nplt.legend(['Sentiment_Title', 'Sentiment_Headline'], loc='upper left')\nplt.grid()\nplt.show()","e116bb35":"#gather sentiment data for each month\nmonth = df['Month'].unique()\nsentiment_title=[]\nsentiment_headline= []\nfor i in month:\n    df1=df[df['Month'] == i]\n    sentiment_title.append(df1['SentimentTitle'].mean())\n    sentiment_headline.append(df1['SentimentHeadline'].mean())\n    \n%matplotlib inline\n\npos = list(range(len(sentiment_title)))\nwidth = 0.25\n\nfig, ax = plt.subplots(figsize=(10,5))\n    \nplt.bar(pos, sentiment_title, width, alpha=0.5, color='#EE3224')\nplt.bar([p + width for p in pos], sentiment_headline, width, alpha=0.5, color='#F78F1E')\n    \nax.set_ylabel('sentiment_score')\nax.set_title('Sentiment vs Month')\nax.set_xticks([p + width for p in pos])\nax.set_xticklabels(day)\nplt.xlim(min(pos)-width, max(pos)+width*4)\nplt.ylim([min(sentiment_title+sentiment_headline)+0.001, max(sentiment_title+ sentiment_headline)+0.001])\nplt.legend(['Sentiment_Title', 'Sentiment_Headline'], loc='upper left')\nplt.grid()\nplt.show()","25eeee93":"#gather sentiment data for respective year \nmonth = df['Year'].unique()\nsentiment_title=[]\nsentiment_headline= []\nfor i in month:\n    df1=df[df['Year'] == i]\n    sentiment_title.append(df1['SentimentTitle'].mean())\n    sentiment_headline.append(df1['SentimentHeadline'].mean())\n    \n%matplotlib inline\n\npos = list(range(len(sentiment_title)))\nwidth = 0.25\n\nfig, ax = plt.subplots(figsize=(10,5))\n    \nplt.bar(pos, sentiment_title, width, alpha=0.5, color='#EE3224')\nplt.bar([p + width for p in pos], sentiment_headline, width, alpha=0.5, color='#F78F1E')\n    \nax.set_ylabel('sentiment_score')\nax.set_title('Sentiment vs Year')\nax.set_xticks([p + width for p in pos])\nax.set_xticklabels(day)\nplt.xlim(min(pos)-width, max(pos)+width*4)\nplt.ylim([min(sentiment_title+sentiment_headline)+0.001, max(sentiment_title+ sentiment_headline)+0.001])\nplt.legend(['Sentiment_Title', 'Sentiment_Headline'], loc='upper left')\nplt.grid()\nplt.show()","c7b25135":"#analysing the correlation between SentimentTitle and SentimentHeadline\nmatplotlib.style.use('ggplot')\n\nplt.scatter(df['SentimentTitle'], df['SentimentHeadline'])\nplt.show()","597cedd2":"# Analysing the correlation between given variables\n#pd.plotting.scatter_matrix(df, figsize=(15, 20))\n#plt.show()","c53c9122":"#Analysing the sentiment of title and headline\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyzer = SentimentIntensityAnalyzer()\ndef sentiment_analyse(text):\n    return analyzer.polarity_scores(text)['compound']\n\ndf['TitlePolarity']= df['Title'].apply(sentiment_analyse)\ndf['HeadlinePolarity']=df['Headline'].apply(sentiment_analyse)\ndt['TitlePolarity']= dt['Title'].apply(sentiment_analyse)\ndt['HeadlinePolarity']=dt['Headline'].apply(sentiment_analyse)","7c2a8b34":"from textblob import TextBlob\ndef sentiment_analyse_tb(text):\n    testimonial = TextBlob(text)\n    return testimonial.sentiment.polarity\n\ndf['TitlePolarity_tb']= df['Title'].apply(sentiment_analyse_tb)\ndf['HeadlinePolarity_tb']=df['Headline'].apply(sentiment_analyse_tb)\ndt['TitlePolarity_tb']= dt['Title'].apply(sentiment_analyse_tb)\ndt['HeadlinePolarity_tb']=dt['Headline'].apply(sentiment_analyse_tb)","3540f9be":"#comparing between two methods textblob and vader\nplt.scatter(df['TitlePolarity'], df['TitlePolarity_tb'])\nplt.show()","a9bf8d07":"plt.scatter(df['HeadlinePolarity'], df['HeadlinePolarity_tb'])\nplt.show()","cc3668f3":"plt.scatter(df['TitlePolarity'], df['SentimentTitle'])\nplt.show()","b9472147":"plt.scatter(df['HeadlinePolarity'], df['SentimentHeadline'])\nplt.show()","b9b57814":"plt.scatter(df['TitlePolarity_tb'], df['SentimentTitle'])\nplt.show()","447d9d2b":"plt.scatter(df['HeadlinePolarity_tb'], df['SentimentHeadline'])\nplt.show()","902d4898":"plt.scatter(((df['TitlePolarity_tb']+df['TitlePolarity'])\/2), df['SentimentTitle'])\nplt.show()","e5de4dd0":"plt.scatter(((df['HeadlinePolarity_tb']+df['HeadlinePolarity'])\/2), df['SentimentHeadline'])\nplt.show()","60b7a864":"#df1['SentimentTitle'] = ((dt['TitlePolarity']+dt['TitlePolarity_tb'])\/2)\n#df1['SentimentHeadline'] = ((df1['HeadlinePolarity']+df1['HeadlinePolarity_tb'])\/2)\n#df1=df1[['IDLink','SentimentTitle','SentimentHeadline']]\n#df1.to_csv(\"submission1.csv\",index=False)","b3738e71":"#train\ndf['Hours']= df['Time'].apply(lambda x: x.hour)\ndf['Minutes']= df['Time'].apply(lambda x: x.minute)\n\n#test\ndt['Hours']= dt['Time'].apply(lambda x: x.hour)\ndt['Minutes']= dt['Time'].apply(lambda x: x.minute)","70f7fc74":"onehotvar = pd.get_dummies(df['Day'])\ndf= pd.concat([df,onehotvar],axis =1)\n\n#test data set \nonehotvar = pd.get_dummies(dt['Day'])\ndt= pd.concat([dt,onehotvar],axis =1)","6a7f13cc":"df.columns","a8c5dbf1":"df_title = df[['TitlePolarity','TitlePolarity_tb','topic_obama', 'topic_economy', 'topic_microsoft',\n       'topic_palestine','Year','Month', 'Date_day','Friday', 'Monday', 'Saturday','Sunday', 'Thursday',\n        'Tuesday', 'Wednesday', 'Hours', 'Minutes','Facebook', 'GooglePlus', 'LinkedIn', 'SentimentTitle',\n       ]]\ndt_title = dt[['TitlePolarity','TitlePolarity_tb','topic_obama', 'topic_economy', 'topic_microsoft',\n       'topic_palestine','Year','Month', 'Date_day','Friday', 'Monday', 'Saturday',\n       'Sunday', 'Thursday', 'Tuesday', 'Wednesday', 'Hours', 'Minutes','Facebook', 'GooglePlus', 'LinkedIn',\n       ]]\n\ndf_headline = df[['HeadlinePolarity','HeadlinePolarity_tb','topic_obama', 'topic_economy', 'topic_microsoft',\n       'topic_palestine','Year','Month', 'Date_day', 'Friday', 'Monday', 'Saturday',\n       'Sunday', 'Thursday', 'Tuesday', 'Wednesday', 'Hours', 'Minutes','Facebook', 'GooglePlus', 'LinkedIn',\n       'SentimentHeadline']]\ndt_headline = dt[['HeadlinePolarity','HeadlinePolarity_tb','topic_obama', 'topic_economy', 'topic_microsoft',\n       'topic_palestine','Year','Month', 'Date_day', 'Friday', 'Monday', 'Saturday',\n       'Sunday', 'Thursday', 'Tuesday', 'Wednesday', 'Hours', 'Minutes','Facebook', 'GooglePlus', 'LinkedIn'\n                 ]]","8256b16c":"df_title['titlepolarity'] = (df['TitlePolarity']+df['TitlePolarity_tb'])\/2\ndt_title['titlepolarity'] = (dt['TitlePolarity']+dt['TitlePolarity_tb'])\/2\n\ndf_headline['headlinepolarity'] = (df['HeadlinePolarity']+df['HeadlinePolarity_tb'])\/2\ndt_headline['headlinepolarity'] = (dt['HeadlinePolarity']+dt['HeadlinePolarity_tb'])\/2","34c58e54":"df_title =df_title.drop(['TitlePolarity', 'TitlePolarity_tb'], axis=1)\ndt_title =dt_title.drop(['TitlePolarity', 'TitlePolarity_tb'], axis=1)\n\ndf_headline =df_headline.drop(['HeadlinePolarity','HeadlinePolarity_tb'], axis=1)\ndt_headline =dt_headline.drop(['HeadlinePolarity','HeadlinePolarity_tb'], axis=1)","f3394aa5":"df_title= pd.concat([df_title['titlepolarity'],df_title.iloc[:,0:20]],axis=1)\ndf_headline= pd.concat([df_headline['headlinepolarity'],df_headline.iloc[:,0:20]],axis=1)\n\n#test\ndt_title= pd.concat([dt_title['titlepolarity'],dt_title.iloc[:,0:19]],axis=1)\ndt_headline= pd.concat([dt_headline['headlinepolarity'],dt_headline.iloc[:,0:19]],axis=1)","61f13dc4":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n# for title \nX_train, X_test, y_train, y_test = train_test_split(df_title.iloc[:,0:20], df_title.iloc[:,20], test_size=0.33, random_state=42)\n\n# for Headline\nXh_train, Xh_test, yh_train, yh_test = train_test_split(df_headline.iloc[:,0:20], df_headline.iloc[:,20], test_size=0.33, random_state=42)","a777ac69":"print(X_train.shape,X_test.shape)\nprint(Xh_train.shape,Xh_test.shape)","74784c12":"# for title\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 100, random_state = 42,max_depth = 15,verbose =1)\nrf.fit(X_train, y_train)\n","960a86f3":"\n#for headline\nrfh = RandomForestRegressor(n_estimators = 100, random_state = 42, max_depth = 15,verbose =1)\nrfh.fit(Xh_train, yh_train)","75d10993":"#prediction\npredictions = rf.predict(X_test)\npredictions_h = rfh.predict(Xh_test)","cbab0b0f":"from sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_test, predictions)","0e6ca9f8":"from sklearn.metrics import mean_absolute_error\nmean_absolute_error(yh_test, predictions_h)","456e88e3":"# pred test\ndt['SentimentTitle'] = rf.predict(dt_title)\ndt['SentimentHeadline'] = rfh.predict(dt_headline)\n\n#submission file \nsub = dt[['IDLink','SentimentTitle','SentimentHeadline']]\nsub.to_csv(\"submission3.csv\",index = False)","47d28718":"feature_list = list(X_train.columns)\n# Get numerical feature importances \nimportances = list(rf.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","5c30ebe9":"# Import matplotlib for plotting and use magic command for Jupyter Notebooks\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Set the style\nplt.style.use('fivethirtyeight')\n# list of x locations for plotting\nx_values = list(range(len(importances)))\n# Make a bar chart\nplt.bar(x_values, importances, orientation = 'vertical')\n# Tick labels for x axis\nplt.xticks(x_values, feature_list, rotation='vertical')\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');","3066b37b":"rf_most_important = RandomForestRegressor(n_estimators= 1000, random_state=42, max_depth=15,n_jobs=-1,verbose=1)\n# Extract the two most important features\n\ntrain_important = X_train.drop(['topic_microsoft','topic_palestine','Thursday','Sunday','Saturday','Monday','Friday','Year'],axis =1)\ntest_important = X_test.drop(['topic_microsoft','topic_palestine','Thursday','Sunday','Saturday','Monday','Friday','Year'],axis=1)\n# Train the random forest\nrf_most_important.fit(train_important, y_train)\n# Make predictions and determine the error\npredictions = rf_most_important.predict(test_important)","9bbc2b1d":"#error\nmean_absolute_error(y_test, predictions)","74bf6bcd":"# Import tools needed for visualization\nfrom sklearn.tree import export_graphviz\nimport pydot\n# Pull out one tree from the forest\ntree = rf.estimators_[5]\n# Import tools needed for visualization\nfrom sklearn.tree import export_graphviz\nimport pydot\n# Pull out one tree from the forest\ntree = rf.estimators_[5]\nprint(\"debug1\")\n# Export the image to a dot file\nexport_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n# Use dot file to create a graph\nprint(\"debug2\")\n(graph, ) = pydot.graph_from_dot_file('tree.dot')\nprint(\"debug3\")\n# Write graph to a png file\ngraph.write_png('tree.png')","7b151997":"\nfeature_list = list(Xh_train.columns)\n# Get numerical feature importances \nimportances = list(rfh.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","65f54333":"# Import matplotlib for plotting and use magic command for Jupyter Notebooks\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Set the style\nplt.style.use('fivethirtyeight')\n# list of x locations for plotting\nx_values = list(range(len(importances)))\n# Make a bar chart\nplt.bar(x_values, importances, orientation = 'vertical')\n# Tick labels for x axis\nplt.xticks(x_values, feature_list, rotation='vertical')\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');","4bf5dd3d":"rfh_most_important = RandomForestRegressor(n_estimators= 100, random_state=42,max_depth=15,n_jobs=-1,verbose=1)\n# Extract the two most important features\n\ntrain_important = Xh_train.drop(['topic_obama','topic_economy','topic_microsoft','topic_palestine','Tuesday','Thursday','Sunday','Saturday','Monday','Friday','Year'],axis =1)\ntest_important = Xh_test.drop(['topic_obama','topic_economy','topic_microsoft','topic_palestine','Tuesday','Thursday','Sunday','Saturday','Monday','Friday','Year'],axis=1)\n# Train the random forest\nrfh_most_important.fit(train_important, yh_train)\n# Make predictions and determine the error\npredictions_h = rfh_most_important.predict(test_important)\n#Error\nmean_absolute_error(yh_test, predictions_h)","2fc951ff":"dt_title = dt_title.drop(['topic_microsoft','topic_palestine','Thursday','Sunday','Saturday','Monday','Friday','Year'],axis =1)\n\ndt_headline = dt_headline.drop(['topic_obama','topic_economy','topic_microsoft','topic_palestine','Tuesday','Thursday','Sunday','Saturday','Monday','Friday','Year'],axis =1)\n","a21242d6":"dt['SentimentTitle'] = rf_most_important.predict(dt_title)\ndt['SentimentHeadline'] = rfh_most_important.predict(dt_headline)","583e240a":"sub = dt[['IDLink','SentimentTitle','SentimentHeadline']]\nsub.to_csv(\"submission4_rf_optimised.csv\",index = False)","ae47e446":"est_range = [100,200,500,1000,1500]\nfrom sklearn.model_selection import validation_curve\ntrain_scoreNum, test_scoreNum = validation_curve(\n                                RandomForestRegressor(),\n                                X = X_train, y = y_train, \n                                param_name = 'n_estimators',\n                                param_range = est_range,\n                                scoring=\"neg_mean_absolute_error\", cv = 3)","75680d8f":"import pandas as pd\nnf = pd.read_csv(\"..\/input\/hacky-zs1\/News_Final.csv\")","96695f72":"nf = nf.iloc[55946:,:]","1f898f81":"nf = nf[nf['Headline'].notna()]","39a8b516":"nf = nf.iloc[:,6:8]","ebfa67ac":"#nf.reset_index(inplace = True) \nnf= nf.iloc[:,1:3]","ed0c36fd":"df= pd.concat([test['IDLink'],nf],axis =1)","27e5caad":"df.to_csv(\"submission1.csv\",index= False)","047c52c8":"### Validation Curves","b399c464":"## Creating two datasets for Title and Headline","f76e7687":"## Getting Important Features","e3538dc6":"Getting the day, month and year seperately to analyze the data based on them  ","00abd2c7":"Import Packages","31429249":" ## Hyperparam Tuning ","16b5cecc":"From the above code snippet we can see that the average sentiment on any given day turns out to be slightly negative","c2adf05b":"### For HeadlineSentiment","3fc60ddc":"We will remove data from the month and year where we have very few data points say the months and year where number of news articles are single digit ","699452d3":"#### For submission1 (Only Vader and TextBlob)","adf7d120":"### Applying Pretrained Models ","93ff1b76":"#### Visualizing the correlations","7f2a46c7":"# Approach 1(Random Forest)","7b06cad0":"create two variables hours and minutes to represent the hours and miutes time ","c88a1490":"### Visualise Single Tree from Random Forest ","485d9b3f":"### The above graph suggest that there is no correlation between the two variables SentimentTitle and SentimentHeadline","ee531730":"It can be observed that the news was mostly collected from january, February, March, October,November, December Months","12bdc483":"### For Title Sentiment","b90771cb":"# Hack","37f56a4d":"## Visualizations "}}