{"cell_type":{"ed856d24":"code","51657c86":"code","90033730":"code","6904a16c":"code","c56d9dfe":"code","1075825c":"code","f148008c":"code","ed22f999":"code","8f09c550":"code","0c5a35d2":"code","3a3f5435":"code","afd81862":"code","ea2b374c":"code","52137812":"code","221eb40b":"code","b5b47445":"code","d3f53f1c":"code","99e324a3":"code","fb28d465":"code","0a93f266":"code","278e23a4":"code","2df982c5":"code","14fecf17":"code","e628e599":"code","a3f6ff2a":"code","d3d3f5b7":"code","601b063c":"code","63f97655":"code","d1655b11":"code","6420e3aa":"code","56ddc999":"code","723ad842":"code","d73c2cec":"code","4a6d4b63":"code","a7fff691":"code","cd968542":"code","2e581159":"code","135ca97d":"code","f15bfa7f":"code","8bc2d048":"code","5794b783":"code","a587724d":"code","ab67b192":"code","38817700":"markdown","0a563604":"markdown","3dd71080":"markdown","42dafb84":"markdown","dd9412e3":"markdown","d8d00b58":"markdown","8ce37629":"markdown","9890c12b":"markdown","f76908c1":"markdown","907c7696":"markdown","b2aec400":"markdown","217646a7":"markdown","5a4f2eda":"markdown","88e222c5":"markdown","06be62ec":"markdown","0d5b686b":"markdown","22bd5133":"markdown","6e84604b":"markdown","534a8524":"markdown","32b663d2":"markdown","07e1886c":"markdown","b56cf5b9":"markdown","e25619c9":"markdown","fcbf6d73":"markdown","b86b4917":"markdown","0410fead":"markdown"},"source":{"ed856d24":"# Import necesaary packages\nimport numpy as np \nimport pandas as pd \nimport seaborn as sb\nimport matplotlib.pyplot as plt\n% matplotlib inline\nfrom sklearn.ensemble import RandomForestClassifier\n# suppress warnings from final output\nimport warnings\nwarnings.simplefilter(\"ignore\")\n# List files provided\nimport os\nprint(os.listdir(\"..\/input\"))\nimport xgboost\nfrom sklearn.metrics import explained_variance_score\nfrom xgboost import XGBRegressor","51657c86":"# Load datasets\ntrain=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')\ntrain.head()","90033730":"# To operate on both dataframes simulatenously\ncombine=[train,test]","6904a16c":"# Check if any data is duplicated\nsum(train['Id'].duplicated()),sum(test['Id'].duplicated())","c56d9dfe":"# Drop Id column from train dataset\ntrain.drop(columns=['Id'],inplace=True)","1075825c":"miss_cols=['MasVnrType','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n           'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond']\nzero_list=['LotFrontage','MasVnrArea','GarageArea','GarageCars','BsmtFinSF1','BsmtFinSF2',\n           'BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']\ndrop_cols=['PoolQC','Fence','MiscFeature','GarageYrBlt','Alley']\nfor df in combine:\n    # Fill missing values in cetgorical variables with None\n    for col in miss_cols:\n        df[col]=df[col].fillna('None')\n    # Fill missing values in numerical variables with 0\n    for col in zero_list:\n        df[col]=df[col].fillna(0)\n    # Drop columns with large number of missing values\n    df.drop(columns=drop_cols,inplace=True)\n","f148008c":"# Fill missing value in Electrical in train dataset\nindex=train[train['Electrical'].isnull()].index\ntrain.loc[index,'Electrical']=train['Electrical'].mode()[0]","ed22f999":"# Fill missing values in categorical variables in test dataset\nmode_list=['Utilities','Exterior1st','Exterior2nd','KitchenQual','Functional','SaleType','MSZoning']\nfor col in mode_list:\n    mode=test[col].mode()\n    test[col]=test[col].fillna(mode[0])","8f09c550":"# Check if the above operations worked correctly\ntrain.isnull().sum().max(),test.isnull().sum().max()","0c5a35d2":"# Plot the correlation matrix\nimp_list=['SalePrice','OverallQual', 'YearBuilt', 'TotalBsmtSF', 'GrLivArea' ,'GarageArea',\n         'GarageCars','LotArea','PoolArea']\ncorrmat = train[imp_list].corr()\nf, ax = plt.subplots(figsize=(10, 9))\nsb.heatmap(corrmat, vmax=.8, square=True,cbar=True, annot=True, fmt='.2f', annot_kws={'size': 10});","3a3f5435":"# Set the base color as blue\nbase_color=sb.color_palette()[0]","afd81862":"# Univariate plot SalePrice\nsb.distplot(train['SalePrice']);","ea2b374c":"# Univariate plot of SalePrice using log transform\nbins=10**np.arange(0,np.log(train['SalePrice'].max())+0.05,0.05)\nplt.hist(data=train,x='SalePrice',bins=bins);\nplt.xscale('log');\nplt.xlim(10000,1000000);\nxticks=[10000,30000,100000,300000,1000000]\nplt.xticks(xticks,xticks);\nplt.xlabel('Sale Price in $');\nplt.title('Distribution of Sale Price');","52137812":"# Univariate plot of Overall Quality\nsb.countplot(data=train,x='OverallQual',color=base_color);\nplt.title('Distribution of Overall-Quality');","221eb40b":"# Bivariate plot of SalePrice and OverallQual\nsb.boxplot(data=train,x='OverallQual',y='SalePrice',color=base_color);\nplt.title('Sale Price vs. Overall Quality');","b5b47445":"# Bivariate plot SalePrice and YearBuilt\nplt.figure(figsize=(18,6))\nsb.barplot(data=train,x='YearBuilt',y='SalePrice',color=base_color);\nplt.xticks(rotation=90);\nplt.title('SalevPrice vs. Year Built');","d3f53f1c":"# Bivariate plot of Sale Price and Basement Area \nsb.regplot(data=train,x='TotalBsmtSF',y='SalePrice',scatter_kws={'alpha':1\/5});","99e324a3":"# Removing outliers from Basement Area\nindex=train[train['TotalBsmtSF']>4000].index\ntrain.drop(index,inplace=True)","fb28d465":"# Bivariate plot of Sale Price and Basement Area \nplt.figure(figsize=(10,5))\nbins_x = np.arange(0, train['TotalBsmtSF'].max()+300, 300)\nbins_y = np.arange(0, train['SalePrice'].max()+50000, 50000)\nh2d=plt.hist2d(data=train,x='TotalBsmtSF',y='SalePrice',cmin=0.5,cmap='viridis_r',bins=[bins_x,bins_y]);\nplt.colorbar();\nplt.xlabel('Basement Area in Square Feet');\nplt.ylabel('Sale Price in $');\nplt.title('Sale Price vs. Basement Area ');\ncounts = h2d[0]\n# loop through the cell counts and add text annotations for each\nfor i in range(counts.shape[0]):\n    for j in range(counts.shape[1]):\n        c = counts[i,j]\n        if c >= 100: # increase visibility on darkest cells\n            plt.text(bins_x[i]+150, bins_y[j]+25000, int(c),\n                     ha = 'center', va = 'center', color = 'white')\n        elif c > 0:\n            plt.text(bins_x[i]+150, bins_y[j]+25000, int(c),\n                     ha = 'center', va = 'center', color = 'black')","0a93f266":"# Bivariate plot of Sale Price and Above Ground Area\nsb.regplot(data=train,x='GrLivArea',y='SalePrice',scatter_kws={'alpha':1\/5});","278e23a4":"# Remove outliers from Above Ground Area\nindex=train[train['GrLivArea']>4000].index\ntrain.drop(index,inplace=True)","2df982c5":"# Bivariate plot of Sale Price and Above Ground Area\nplt.figure(figsize=(10,5))\nbins_x = np.arange(0, train['GrLivArea'].max()+300, 300)\nbins_y = np.arange(0, train['SalePrice'].max()+50000, 50000)\nh2d=plt.hist2d(data=train,x='GrLivArea',y='SalePrice',cmin=0.5,cmap='viridis_r',bins=[bins_x,bins_y]);\nplt.colorbar();\nplt.xlabel('Above Ground Area in Square Feet');\nplt.ylabel('Sale Price in $');\nplt.title('Sale Price vs. Above Ground Area ');\ncounts = h2d[0]\n# loop through the cell counts and add text annotations for each\nfor i in range(counts.shape[0]):\n    for j in range(counts.shape[1]):\n        c = counts[i,j]\n        if c >= 100: # increase visibility on darkest cells\n            plt.text(bins_x[i]+150, bins_y[j]+25000, int(c),\n                     ha = 'center', va = 'center', color = 'white')\n        elif c > 0:\n            plt.text(bins_x[i]+150, bins_y[j]+25000, int(c),\n                     ha = 'center', va = 'center', color = 'black')","14fecf17":"sb.regplot(data=train,x='GarageArea',y='SalePrice',scatter_kws={'alpha':1\/5});","e628e599":"# Bivariate plot of Sale Price and Garage Area\nplt.figure(figsize=(10,5))\nbins_x = np.arange(0, train['GarageArea'].max()+100, 100)\nbins_y = np.arange(0, train['SalePrice'].max()+50000, 50000)\nh2d=plt.hist2d(data=train,x='GarageArea',y='SalePrice',cmin=0.5,cmap='viridis_r',bins=[bins_x,bins_y]);\nplt.colorbar();\nplt.xlabel('Garage Area in Square Feet');\nplt.ylabel('Sale Price in $');\nplt.title('Sale Price vs. Garage Area ');\ncounts = h2d[0]\n# loop through the cell counts and add text annotations for each\nfor i in range(counts.shape[0]):\n    for j in range(counts.shape[1]):\n        c = counts[i,j]\n        if c >= 100: # increase visibility on darkest cells\n            plt.text(bins_x[i]+50, bins_y[j]+25000, int(c),\n                     ha = 'center', va = 'center', color = 'white')\n        elif c > 0:\n            plt.text(bins_x[i]+50, bins_y[j]+25000, int(c),\n                     ha = 'center', va = 'center', color = 'black')","a3f6ff2a":"# Bivariate plot between all the areas\nplt.figure(figsize=(16,10))\nplt.subplot(3,3,1)\nsb.regplot(data=train,x='TotalBsmtSF',y='GrLivArea',x_jitter=0.3,scatter_kws={'alpha':1\/5});\nplt.subplot(3,3,2)\nsb.regplot(data=train,x='TotalBsmtSF',y='GarageArea',x_jitter=0.3,scatter_kws={'alpha':1\/5});\nplt.subplot(3,3,3)\nsb.regplot(data=train,x='GrLivArea',y='GarageArea',x_jitter=0.3,scatter_kws={'alpha':1\/5});","d3d3f5b7":"# Bivariate plot of Quality vs. Area\nplt.figure(figsize=(10,7))\nplt.subplot(3,1,1)\nsb.boxplot(data=train,y='TotalBsmtSF',x='OverallQual',color=base_color);\nplt.subplot(3,1,2)\nsb.boxplot(data=train,y='GarageArea',x='OverallQual',color=base_color);\nplt.subplot(3,1,3)\nsb.boxplot(data=train,y='GrLivArea',x='OverallQual',color=base_color);","601b063c":"# Multivariate plot of SalePrice vs. YearBuilt and OverallQual\nplt.figure(figsize=(18,7))\nsb.pointplot(data=train,x='YearBuilt',y='SalePrice',hue='OverallQual',palette='viridis_r',linestyles=\"\");\nplt.xticks(rotation=90);\nplt.title('Sale Price vs. Year Built and Overall Quality');\n","63f97655":"# Multivariate plot of SalePrice Vs. Quality and GarageQual, BsmtQual\nplt.figure(figsize=(14,7))\nplt.subplot(2,1,1)\nplt.title('Sale Price Vs. Quality and Garage Quality, Basement Quality');\nsb.pointplot(data=train,hue='OverallQual',y='SalePrice',x='GarageQual',palette='viridis_r',linestyles=\"\",\n            order=['None','Po','Fa','TA','Gd','Ex']);\nplt.legend(loc=1);\nplt.subplot(2,1,2)\nsb.pointplot(data=train,hue='OverallQual',y='SalePrice',x='BsmtQual',palette='viridis_r',linestyles=\"\",\n            order=['None','Po','Fa','TA','Gd','Ex']);\nplt.legend(loc=1);\n","d1655b11":"def mean_poly(x, y, bins = 10, **kwargs):\n    # set bin edges if none or int specified\n    if type(bins) == int:\n        bins = np.linspace(x.min(), x.max(), bins+1)\n    bin_centers = (bin_edges[1:] + bin_edges[:-1]) \/ 2\n\n    # compute counts\n    data_bins = pd.cut(x, bins, right = False,\n                       include_lowest = True)\n    means = y.groupby(data_bins).mean()\n\n    # create plot\n    plt.errorbar(x = bin_centers, y = means, **kwargs)","6420e3aa":"# Lineplot of SalePrice and TotalBsmtSF using OverallQual as hue\nbin_edges = np.arange(0, train['TotalBsmtSF'].max()+250, 250)\ng = sb.FacetGrid(data = train, hue = 'OverallQual',size=7,palette='viridis_r');\ng.map(mean_poly, \"TotalBsmtSF\", \"SalePrice\", bins = bin_edges);\ng.set_ylabels('mean(SalePrice)');\ng.add_legend();\nplt.title('SalePrice vs. Basement Area by Overall Quality');","56ddc999":"# Lineplot of SalePrice and GrLivArea using OverallQual as hue\nbin_edges = np.arange(0, train['GrLivArea'].max()+250, 250)\ng = sb.FacetGrid(data = train, hue = 'OverallQual',size=7,palette='viridis_r');\ng.map(mean_poly, \"GrLivArea\", \"SalePrice\", bins = bin_edges);\ng.set_ylabels('mean(SalePrice)');\ng.add_legend();\nplt.title('SalePrice vs. Above Ground Area by Overall Quality');","723ad842":"# Lineplot of SalePrice and GarageArea using OverallQual as hue\nbin_edges = np.arange(0, train['GarageArea'].max()+150, 150)\ng = sb.FacetGrid(data = train, hue = 'OverallQual',size=7,palette='viridis_r');\ng.map(mean_poly, \"GarageArea\", \"SalePrice\", bins = bin_edges);\ng.set_ylabels('mean(SalePrice)');\ng.add_legend();\nplt.title('SalePrice vs. Garage Area by Overall Quality');","d73c2cec":"# Drop object features\n'''\nfor df in combine:\n    df.drop(columns=['MSZoning','SaleCondition','SaleType','PavedDrive','GarageCond','GarageQual','GarageFinish',\n                    'GarageType','FireplaceQu','Functional','KitchenQual','Heating','HeatingQC','CentralAir',\n                     'Electrical','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure',\n                     'BsmtFinType1','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Street',\n                     'LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1',\n                     'Condition2','BldgType','HouseStyle','BsmtFinType2' ],inplace=True)\n'''","4a6d4b63":"# Merge the two datasets\nntrain = train.shape[0]\nntest = test.shape[0]\nall_data = pd.concat((train, test))","a7fff691":"# Get dummy variables\nall_data=pd.get_dummies(all_data)","cd968542":"# Seperate the combined dataset into test and train data\ntest=all_data[all_data['SalePrice'].isnull()]\ntrain=all_data[all_data['Id'].isnull()]","2e581159":"# Check if the new and old sizes are equal\nassert train.shape[0]==ntrain\nassert test.shape[0]==ntest","135ca97d":"# Drop extra columns\ntest.drop(columns='SalePrice',inplace=True)\ntrain.drop(columns='Id',inplace=True)\ntest['Id']=test['Id'].astype(int)","f15bfa7f":"X_train=train.drop(columns='SalePrice')\nY_train=train['SalePrice']\nX_test=test.drop(columns='Id')","8bc2d048":"'''\n# Apply Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=1000)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest\n'''","5794b783":"# Apply XGBRegressor\nxgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n                           colsample_bytree=1, max_depth=7)\nxgb.fit(X_train,Y_train)\nY_pred = xgb.predict(X_test)","a587724d":"final_df = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": Y_pred\n    })\n","ab67b192":"# Save the dataframe to a csv file\nfinal_df.to_csv('submission.csv',index=False)","38817700":">**Observation**\nThere is a positive correlation between SalePrice and BasementArea. Most of the houses have a basement between 500-1500 square feet.","0a563604":">**Observation**\nSalePrice and GarageArea have a positive correlation. Bulk of the data lies between 200-600 square feet.\n\n","3dd71080":">**Observation**\nIt is clear from the above plot that quality of houses have increased over the years and hance the increase in prices of the house. The sudden price increase that we saw in the plot of SalePrice vs. YearBuilt is because the quality of those houses is the highest.","42dafb84":"### Multivariate Analysis","dd9412e3":"### Bivariate Analysis","d8d00b58":">**Observation**\nIt is very clear from the above line plots that as the area(above ground, basement and garage) and quality  of the house increases the price of the house also increases. All the three variables have a positive correlation.","8ce37629":"## House Prices: Advanced Regression Techniques\n**Problem statement**\n\nThis is the Kaggle competition [House Prices: Advanced Regression Techniques](http:\/\/https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview).\n\n**Extract**\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\nThe aim of this project is to predict house sale prices using machine learning.\n","9890c12b":">**Observation** Majority of the houses in the dataset have average quality rating with minimum at the extreme ends.","f76908c1":">**Observation**\nIt is obvious from the above plots that as area increases the quality of the house also increases.","907c7696":">**Observation** Features that have strong and positive correlation with SalePrice are Overall Ouality, Year Built, Basement Area, Above groud area and Garage Area.\nSalePrice has a strong positive correlation with these features meaning that if any of the features increase in value then SalePrice will also increase.","b2aec400":">**Observation**\nThe points after 1200 in Garage Area may look like outliers but the reason for low price can be on of the other factors. Lets leave it as it is.","217646a7":"## Data Wrangling\nData wrangling is the step where we deal with missing or incorrect data. \nLets analyze our dataset and find areas we need to work on.\nIn train and test dataset I found the following problem areas:\n1. Missing data in categoraical variables - Variables like MasVnrType,BsmtQual etc have NaN if that particular feature is not present in that house. We can fill these missing values with None.\n2. Missing data in numerical variables - The Basement and Garage variables have missing values. Upon furthur analysis I found that the missing values correspond to the house that do not have a basement or a garage. We can fill NaN values with 0 for these variables.\n3. In train dataset 'Electrical' column has one value missing- We can fill the missing value with the most common value of this column.\n4. In test dataset the 'Utilities','Exterior1st','Exterior2nd','KitchenQual','Functional','SaleType','MSZoning' columns have one or two values missing. Instead of dropping these variables we can fill the missing values with the most common occurance.\n6. Few columns like 'PoolQC','Fence','MiscFeature','GarageYrBlt','Alley' have a large number of missing values. We can drop these columns.","5a4f2eda":">**Observation**\nHigh quality in garage and basement quality leads to high overall quality and thus high sale price.","88e222c5":">**Observation**\nThere is an oultlier present in the BasementArea feature. Lets remove it and look at the plot again.","06be62ec":"> Our main focus to find features that influence price of a house.\nLets plot the correlation matrix to see which features have a strong correlation with SalePrice.","0d5b686b":"## Model and Predict\nPreviuosly I had used Random Forest for my predcition, while it is a good algorithm the XGBRegressor gives better results.\nI have not removed the code for Random Forest.","22bd5133":"### Univariate Analysis","6e84604b":">**Observation**\nFrom the above plot of SalePrice vs. OverallQual it is clear that the Quality has a strong positive correlation with Sale Price. As quality increases the price of the house also increases. ","534a8524":">**Observation** The distribution of SalePrice is unimodal in nature with a peak at 1500000 dollars.  ","32b663d2":">**Observation**\nSalePrice and AboveGroundArea have a positive correlation. Bulk of the data lies between 1000-2000 square feet.","07e1886c":"## Visualisations","b56cf5b9":">**Observation**\nLets remove the outliers and plot the graph again.","e25619c9":">**Observation**\nWe can see a steady increase in the SalePrice as the value of YearBuilt increases. There are sudden peaks in between. This can be due to inflation or some other reason. We can get a clear picture when we plot it with other variables. The large error bars indicate that there are less number of sale listed for that year.","fcbf6d73":"If my approach was helpful to you please upvote this notebook. I will apply new algorithms as and when I learn them. :)","b86b4917":"The plot of SalePrice is skewed in nature. Lets apply log transform to a normal distribution curve.","0410fead":">**Observation**\n![](http:\/\/)All three have a positive correlation with each other."}}