{"cell_type":{"aa78960f":"code","ad589bbb":"code","a115c234":"code","df6e6756":"code","a683a92e":"code","bb1f1d1a":"code","fec254db":"code","eb043983":"markdown","66a7f38f":"markdown","364340bf":"markdown","aecfb945":"markdown","c08e867a":"markdown","3660301a":"markdown","1746a538":"markdown","28eb5749":"markdown","2cc88435":"markdown","757d1d26":"markdown","75f3a38f":"markdown"},"source":{"aa78960f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad589bbb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n \n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\nimport mne\nfrom tqdm import tqdm\nimport os\nimport gc\n\n# clustering algorithms.\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import AffinityPropagation\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import SpectralClustering\n\n# import statements\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nimport matplotlib.pyplot as plt","a115c234":"X,Y=make_blobs(n_samples=500, n_features=2,centers=4,random_state=42)","df6e6756":"# create scatter plot\nplt.scatter(X[:,0],X[:,1],c=Y, cmap='rainbow')","a683a92e":"kmeans_obj=KMeans(n_clusters=3).fit(X)","bb1f1d1a":"# Attributes.\nprint(\"(Number of clusters x Number of features): \", kmeans_obj.cluster_centers_.shape)\nprint('\\n')\nprint(\" Index of centroid of each cluster: \", kmeans_obj.labels_)  # cluster indexing. starts from 0.","fec254db":"plt.scatter(X[:,0],X[:,1],c=kmeans_obj.labels_,cmap='rainbow')","eb043983":"#### Hi! \ud83d\udd90\n\n#### Thanks for reaching here ;). Writing a kernel after a long time. \n\n#### In this kernel, we will learn about the clustering thorugh an applied example. I learnt a lot about clustering through my last internship and would like to share some of that. This is just Version 1 of the kernel. We will gradually add more material to this kernel. Make sure you drop a \ud83d\udc4d to stay updated.\n\n#### Do drop a \ud83d\udc4d if you like the work !!!\n\n#### Let's dive right in \ud83d\udc47","66a7f38f":"#### Notice that since we forced it to extract 3 clusters it has merged the two clusters in blue \ud83d\ude2c . \n\n#### Change n_clusters=4 to see the effect. Do comment your observations \ud83e\udd1e","364340bf":"#### Breaking it down \u270d:- \n\n#### 1. Using the KMeans algorithm. We have given *n_clusters=3* i.e. we have initialized 3 centroids. In simple terms, we want our algorithm to detect three clusters in the supplied data.\n\n#### 2. Using the *fit()* method we feed the data to the alorithm and want it to detect those 3 clusters.","aecfb945":"## Making random data to begin with ...","c08e867a":"#### 1. The K-Means Algorithm\n\n","3660301a":"#### Breaking it down \u270d:- \n\n#### 1. We have created 500 samples or instances. \n\n#### 2. There are 2 features say, feature_1 and feature_2. This would be more clear as we move onto the scatter plot. \n\n#### 3. Now there are 4 centers (see center parameter). This actually means that the data has 4 clsuters. So, a clustering algorithm whem applied to detect clusters should ideally detect 4 clusters.","1746a538":"#### Stay tuned for more !!! \ud83d\ude0e","28eb5749":"## Importing the modules","2cc88435":"## Beginning to cluster....","757d1d26":"#### A scatter plot\n\nHere, I am plotting a basic scatter plot. To get a more intercative plot try usin [plotly](https:\/\/plotly.com\/).\n","75f3a38f":"#### Breaking it down \u270d:- \n\n#### 1. Look at the first attribute. We get as many clusters as *'n_clusters'*. Now if u see \ud83e\udd13, each centroid is represeted by the number of features that each sample has. Now remember that we have chosen 2 features when we were creating this sample data \ud83d\ude09.\n\n#### 2. Next we have list of the numbers. Now it basically shows the index of the cluster to which each observation is assigned. \n\n#### So, if we have 3 clusters then 0 means first clsuter, 1 means second clsuter and so on.. Note that indexin starts at zero \ud83d\ude43. "}}