{"cell_type":{"109e627c":"code","8528a7e8":"code","f08e6517":"code","83defb31":"code","89fd0a70":"code","a73dc637":"code","cb7ae4a2":"code","21884300":"code","abebfa13":"code","a3a38a85":"code","3d5a9afb":"code","b4854b54":"code","005203ad":"code","6b7a7c30":"markdown","99a162f9":"markdown","b5be07a8":"markdown","0ade003d":"markdown"},"source":{"109e627c":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\n\nfrom keras.layers import * \nfrom keras.models import Model, Sequential, load_model\nfrom keras import backend as K \nfrom keras import optimizers \nfrom keras.callbacks import * \nfrom keras.backend import clear_session\n\nprint(os.listdir(\"..\/input\"))","8528a7e8":"df_train = pd.read_csv(\"..\/input\/X_train.csv\")\ndf_test = pd.read_csv(\"..\/input\/X_test.csv\")\ndf_y = pd.read_csv(\"..\/input\/y_train.csv\")","f08e6517":"from sklearn.preprocessing import LabelEncoder\n\n# encode class values as integers so they work as targets for the prediction algorithm\nencoder = LabelEncoder()\ny = encoder.fit_transform(df_y[\"surface\"])\ny_count = len(list(encoder.classes_))","83defb31":"label_mapping = {i: l for i, l in enumerate(encoder.classes_)}","89fd0a70":"print(\"Data Frame Shape (Train then Test, then correct training labels)\")\nprint(df_train.shape, df_test.shape, y.shape)\nprint(\"Numpy Array Shape (Train then Test then correct training labels)\")\n\n# --- Convert Training, Testing, and Labels into Numpy arrays\n\nnum_train = int(df_train.shape[0] \/ 128)\n\n# Use this for 1D Convolutions\n# X_train = np.reshape(np.array(df_train), (num_train,128,13))\n# remove potential leakage info \n# X_train = X_train[:,:,3:14]\n\n# Use this for 2D Convolutions\nX_train = np.reshape(np.array(df_train), (num_train,128,13,1))\n# remove potential leakage info \nX_train = X_train[:,:,3:14,:]\n\nnum_test = int(df_test.shape[0] \/ 128)\n\n# Use this for 1D Convolutions\n# X_test = np.reshape(np.array(df_test), (num_test,128,13))\n# remove potential leakage info \n# X_test = X_test[:,:,3:14]\n\n# Use this for 2D Convolutions\nX_test = np.reshape(np.array(df_test), (num_test,128,13,1))\n# remove potential leakage info \nX_test = X_test[:,:,3:14,:]\n\ny_array = np.array(y)\n# use one hot encoding\ny_one_hot = np.zeros((y_array.shape[0],y_count))\ny_one_hot[np.arange(y_array.shape[0]),y_array] = 1\nprint(X_train.shape, X_test.shape, y_one_hot.shape)\nnum_features = X_train.shape[2]","a73dc637":"df_train.describe()","cb7ae4a2":"# Sanity Check of conversion from data frame to numpy\n# average values in TRAINING array conversion below - should be equal to the individual data frame feature \"Mean\" values above\n\nfor i in range (num_features) :\n    print(X_train[:,:,i].mean())","21884300":"def make_model() :\n    \n    scale = 15\n    # scale = 100\n\n    # use a simple sequential convolutional neural network model\n    model2 = Sequential()\n    \n    # Start with a droput to slow learning and avoid local minima which in turn will prevent overfitting on the training set \n    model2.add(Dropout(0.1, input_shape=(128,10,1)))\n    \n    # The first colvolutional layer looks for basic patterns (such as slope) within a sensor's time sequenced data\n    # but will also look across all of the sensors for patterns where the may correlate\n    # Assume there are three basic kinds of slope (+, 0, and -) and there are 2 groups of 3 sensors, and 1 group of 3 sensors, \n    # so (3^3) * (3^3) * (3^4) = 3^10 = 59 k combinations of slopes across the sensors\n    # We can use this as a rough measure of how big to dimension our first layer - let's assume that only \"scale\" out of 1000 of these are \"important\"\n    model2.add(Conv2D(int(60 * scale), (3,1), strides = 1, activation='relu'))\n    \n    # The following convolutional layer(s) are really 1D, and look for larger and larger time patterns (second order, scale increase, etc.)\n    # The use of the strides > 1 in the convolution layer risks the possibility of phase dependence \n    # This could conceivably cause the same time data shifted by one or two samples to appear different, so instead we use pooling strides\n    # These strides can be reduced at the cost of network size, speed, and possible overfitting \n    # If it is found to be needed, we can compensate for overfitting with larger dropout, or more dropouts between layers, or larger \"batch_size\"\n    model2.add(Dropout(0.1))\n    model2.add(MaxPooling2D(pool_size = (2,1), strides = (2,1)))\n    model2.add(Conv2D(int(10 * scale), (3,1), strides = 1, activation='relu'))\n    model2.add(Dropout(0.1))\n    model2.add(MaxPooling2D(pool_size = (2,1), strides = (2,1)))\n    model2.add(Conv2D(int(5 * scale), (3,1), strides = 1, activation='relu'))\n    model2.add(Dropout(0.1))\n    model2.add(MaxPooling2D(pool_size = (2,1), strides = (2,1)))\n    model2.add(Conv2D(int(5 * scale), (3,1), strides = 1, activation='relu'))\n    model2.add(Dropout(0.1))\n    model2.add(MaxPooling2D(pool_size = (2,1), strides = (2,1)))\n    model2.add(Conv2D(int(2 * scale), (3,1), strides = 1, activation='relu'))\n    model2.add(Dropout(0.1))\n    model2.add(Conv2D(int(2 * scale), (3,1), strides = 1, activation='relu'))\n    \n    # After these convolutional layers, we are covering a large chunk of the entire sensor data set of 128 samples\n    # and are ready to classify these \"wavelet-like\" learned patterns using perceptron-style neural netowrks (\"Dense\")\n    model2.add(Flatten())\n    \n    # Include a couple of dense layers in case the classes are not linearly seperable by this point\n    model2.add(Dropout(0.1))\n    model2.add(Dense(int(2 * scale), activation='relu'))\n    model2.add(Dropout(0.1))\n    model2.add(Dense(int(1 * scale), activation='relu'))\n    \n    # Finally, mirror the \"one-hot\" classification scheme with a softmax output layer\n    model2.add(Dropout(0.1))\n    model2.add(Dense(y_count, activation = 'softmax'))\n    \n    # Binary Cross Entropy (or log loss) is used as the error function\n    model2.compile(loss='categorical_crossentropy', optimizer='adam')\n    return model2","abebfa13":"# we will be slowing down the learning using \"Dropouts\" (see above) so the patience needed to exit local minima can be large\npatience = 15\n# Probably will never reach this many epochs, but want to use a number larger than what we expect\nepochs = 300\n# Divide the data into 15 different versions of training\/validation\nn_fold = 15\n# Using KFold instead of StratifiedKFold becuase there is a low degree of confidence that the test classification distributions \n# or more importantly, the real world classification probabilities, are equal to those found in the training set\nfolds = KFold(n_splits=n_fold, shuffle=False, random_state=1234)\n\nsam = X_train.shape[0]\ncol = X_train.shape[1]\n\nsam_test = X_test.shape[0]\ncol_test = X_test.shape[1]\n\nprediction = np.zeros((sam_test, y_count))\nprediction_train = np.zeros((sam, y_count))\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X_train,y_one_hot)):\n    print('Fold', fold_n)\n    X_train2, X_valid2 = X_train[train_index], X_train[valid_index]\n    y_train2, y_valid2 = y_one_hot[train_index], y_one_hot[valid_index]\n    K.clear_session()\n    model = make_model()\n    print(model.summary())\n    checkpointer = ModelCheckpoint('Net1', verbose=1, save_best_only=True)\n    earlystopper = EarlyStopping(patience = patience, verbose=0) \n    results = model.fit(X_train2, y_train2, epochs = epochs, batch_size = 32,\n                    callbacks=[earlystopper, checkpointer], validation_data=[X_valid2, y_valid2])\n    model = load_model('Net1')\n    # For each fold, we will accummulate our opinion of the final classification\n    prediction_train  += model.predict(X_train)\/n_fold\n    # Note, in a real world deployment, we may have \"n_fold\" neural networks, each rendering their opinion - but here\n    # to save memory and disk space, we dispose of each NN when the classification is done\n    prediction += model.predict(X_test)\/n_fold \n    print()","a3a38a85":"pred_y = prediction_train.argmax(axis = 1)\nnum_correct = 0\nfor i in range(y_array.shape[0]) :\n    if pred_y[i] == y_array[i] :\n        num_correct += 1\n        \nprint(\"Score on Training Data =\", num_correct \/ y_array.shape[0])","3d5a9afb":"prediction","b4854b54":"prediction.shape","005203ad":"ss = pd.read_csv('..\/input\/sample_submission.csv')\n# ss['surface'] = encoder.inverse_transform(prediction.astype(int))\nss['surface'] = encoder.inverse_transform(prediction.argmax(axis = 1))\nss.to_csv('rf.csv', index=False)\nss.head(10)","6b7a7c30":"# Kernel for CareerCon 2019 \n(see contest for details https:\/\/www.kaggle.com\/c\/career-con-2019)\n## Paul A. Nussbaum\n### Demonstrates Signal Analysis and Pattern Recognition for Machine Learning\n#### Executive Summary\nInertial measurement sensors on a moving robot record signals representing different accelerations as they vary over time. This algorithm seeks to observe these signals of bouncing and bumping, and from that determine which category of floor type the robot is rolling over. To accomplish this, the original signals are analyzed and presented to a machine learning algorithm which seeks to recognize the different patterns.\n#### Details\n* There are eight floor type classifications, numbered in the training data 0 through 7 in the following order: 'carpet', 'concrete', 'fine_concrete', 'hard_tiles', 'hard_tiles_large_space',  'soft_pvc', 'soft_tiles', 'tiled', and 'wood'\n\n#### Revision History\n* v03 - Current revision. Added one-hot encoding, and turned on GPU. Tuned 2D layers a bit.\n* v02 - Converted to Keras CNN using Numpy arrays.\n* v01 - original fork (see top of page)\n","99a162f9":"## Load the Data","b5be07a8":"Forked from (https:\/\/www.kaggle.com\/donkeys\/my-little-eda-with-random-forest\/log) My Little EDA with Random Forest; Courtesy of \"averagemn\"","0ade003d":"# Run CNN classifier:"}}