{"cell_type":{"13a781da":"code","7fca5c8e":"code","644fb1fe":"code","c6fcfe47":"code","425b0ca8":"code","7f6ac6a4":"code","f1536975":"code","35e0caa1":"code","68fe2422":"code","a33cee1e":"code","c7021e21":"code","2cfd8902":"code","e0e6881c":"code","36068e61":"code","d435dd26":"code","289ca52b":"code","a18320d9":"code","40b96dcc":"code","4940e8df":"code","eb917795":"code","6a27317f":"code","8ee678ec":"code","d9ef3746":"code","aba01922":"code","f95eee14":"code","b775ec93":"code","4d9ea7ce":"code","e8a0e9ad":"code","491201d8":"code","83b8a753":"code","cbc18edb":"code","c2ce298a":"code","45b9e20e":"code","d6cae42a":"code","3456efe5":"code","fe027ac4":"code","fee7d558":"code","bcf41069":"code","f3754e41":"code","133ccc59":"code","0dd4843f":"code","727959f6":"code","fc49c4e4":"code","2a1be0a4":"code","437789b8":"code","83c346d4":"code","edec2760":"code","f7a2e179":"code","bb25ce5f":"code","0b595b43":"code","3cd59254":"code","0ce0fc19":"code","2362e916":"code","38136889":"code","11c36c9f":"code","a02b0d2d":"code","f2e9bbab":"code","aa1e32e8":"code","ef3f93db":"code","95bf0867":"code","1f10ecbd":"code","cf40a6ed":"code","07faa534":"code","715f7908":"code","02f2e813":"code","cf704af4":"code","516374db":"code","e2d01b9e":"code","36b22dc2":"code","693c1282":"code","21a18e4f":"code","29f5f430":"code","d649bcb3":"code","05671ae1":"code","164fce7b":"code","e1ab204d":"code","75707035":"code","f38f1bf4":"code","f948685b":"code","60d0e797":"code","6564c338":"code","03735de3":"code","8328c4de":"code","0f9720cd":"code","8e9c5f29":"code","2b482a8c":"code","c3a376d1":"code","9eab6cb2":"markdown","81e081c7":"markdown","0664917f":"markdown","8126ee27":"markdown","41ce72b8":"markdown","776ff650":"markdown","485df830":"markdown","d3365486":"markdown","b75d8392":"markdown","ebe9d749":"markdown","02d3dfa5":"markdown","013b4a59":"markdown","79a42a83":"markdown","6bac8e3c":"markdown","48589890":"markdown","bf4d1c99":"markdown","c4d38b62":"markdown","0d8a59ce":"markdown","39f6df52":"markdown","ae28dc7e":"markdown","dbf9f78c":"markdown","055ae823":"markdown","6f0c36d1":"markdown","24ddd552":"markdown","8f58300c":"markdown","35ec40b9":"markdown","102cc6c4":"markdown","5cc868be":"markdown","ea5a7e84":"markdown","46c7dcb9":"markdown","6157a67c":"markdown","e67ff7c5":"markdown","ae1075a0":"markdown","a66c1cad":"markdown","e6398515":"markdown","fe256473":"markdown","de250fe6":"markdown","5c2ace53":"markdown","beccfddc":"markdown","9b4fbd27":"markdown","4ece53a4":"markdown","11b122a0":"markdown","c194c5af":"markdown","af446e29":"markdown","0f62390f":"markdown","51048a6e":"markdown","3d29bb2f":"markdown","b1e23ce5":"markdown","a70f45f4":"markdown","0b016fe0":"markdown","458604e4":"markdown","7325a843":"markdown","38a406df":"markdown","5f68102b":"markdown","5e8cdd21":"markdown","84463f32":"markdown","fafa8d9b":"markdown","1e965605":"markdown","f9eba853":"markdown","933bc07c":"markdown","efcf268d":"markdown","24d41e34":"markdown","a9d7916c":"markdown","657e7428":"markdown","f4e27fc4":"markdown","ee7d4917":"markdown","022ffe50":"markdown","dde07a87":"markdown","c986511d":"markdown","adbbf526":"markdown","650d1bd0":"markdown","c3ffdf08":"markdown","5c0fbbc0":"markdown","f87f6c0c":"markdown","1a2cf105":"markdown","7273b10e":"markdown","d29b6a6b":"markdown","24c80d41":"markdown","b994edb1":"markdown","1f6c2fd0":"markdown","b85a2b84":"markdown","83f3741a":"markdown","0267afaa":"markdown","6e3681ee":"markdown","0461d4a5":"markdown","9f74537e":"markdown","857e1085":"markdown","645c928b":"markdown","0d2e460e":"markdown","8ebc6707":"markdown","3bd8e8c0":"markdown","c4bb8f99":"markdown","3429dcb4":"markdown","b267fc71":"markdown","3081887d":"markdown","1ff06e7d":"markdown"},"source":{"13a781da":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import RANSACRegressor\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport warnings\nwarnings.filterwarnings('ignore')","7fca5c8e":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","644fb1fe":"train.head()","c6fcfe47":"train.info()","425b0ca8":"def missing_props(df):\n    missing_values = []\n    for i in df.columns:\n        missing_values.append(round(df[i].isnull().sum() \/ len(df), 3))\n    missing_props = pd.DataFrame(list(zip(df.columns, missing_values)), columns = [\"Var\", \"Prop_Missing\"]).sort_values(by = \"Prop_Missing\", ascending = False)\n    \n    return missing_props[missing_props[\"Prop_Missing\"] != 0]","7f6ac6a4":"table1 = missing_props(train)","f1536975":"table1","35e0caa1":"train = train.drop(columns = [\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\"])","68fe2422":"#Lot Frontage is the amount of street connected to the property (ft).  NA values represent \n#houses with no connected street, probably in rural areas.\n\ntrain[\"LotFrontage\"] = train[\"LotFrontage\"].fillna(0)","a33cee1e":"#All garage variables have the same amount of missing values, illustrating that these values \n#are within the same rows.  These houses have no garage.\n\ntrain[\"GarageYrBlt\"] = train[\"GarageYrBlt\"].fillna(\"No Garage\")\ntrain[\"GarageCond\"] = train[\"GarageCond\"].fillna(\"No Garage\")\ntrain[\"GarageType\"] = train[\"GarageType\"].fillna(\"No Garage\")\ntrain[\"GarageFinish\"] = train[\"GarageFinish\"].fillna(\"No Garage\")\ntrain[\"GarageQual\"] = train[\"GarageQual\"].fillna(\"No Garage\")","c7021e21":"#All basement variables have the same amount of missing values, illustrating that these values \n#are within the same rows.  These houses have no basement.\n\ntrain[\"BsmtFinType1\"] = train[\"BsmtFinType1\"].fillna(\"No Basement\")\ntrain[\"BsmtFinType2\"] = train[\"BsmtFinType2\"].fillna(\"No Basement\")\ntrain[\"BsmtExposure\"] = train[\"BsmtExposure\"].fillna(\"No Basement\")\ntrain[\"BsmtQual\"] = train[\"BsmtQual\"].fillna(\"No Basement\")\ntrain[\"BsmtCond\"] = train[\"BsmtCond\"].fillna(\"No Basement\")","2cfd8902":"#These missing values represent houses with no masonry vaneers.\n\ntrain[\"MasVnrArea\"] = train[\"MasVnrArea\"].fillna(0)\ntrain[\"MasVnrType\"] = train[\"MasVnrType\"].fillna(\"None\")","e0e6881c":"#Lastly, these missing values have no known electrical system.\n\ntrain[\"Electrical\"] = train[\"Electrical\"].fillna(\"None\")","36068e61":"train.head()","d435dd26":"train[\"TotalInsideArea\"] = train[\"TotalBsmtSF\"] + train[\"GrLivArea\"] + train[\"GarageArea\"]","289ca52b":"train[\"TotalOutsideArea\"] = train[\"WoodDeckSF\"] + train[\"OpenPorchSF\"] + train[\"EnclosedPorch\"] + train[\"3SsnPorch\"] + train[\"ScreenPorch\"] + train[\"PoolArea\"]","a18320d9":"train[\"Pool\"] = train[\"PoolArea\"].apply(lambda x: \"Yes\" if x > 0 else \"No\")","40b96dcc":"train = train.drop(columns = [\"TotalBsmtSF\", \"GrLivArea\", \"GarageArea\", \"WoodDeckSF\", \"OpenPorchSF\",\n                              \"EnclosedPorch\", \"PoolArea\", \"3SsnPorch\", \"ScreenPorch\", \"1stFlrSF\", \"2ndFlrSF\",\n                             \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"MasVnrArea\"])","4940e8df":"import statsmodels.api as sm","eb917795":"def model_summary(df):\n    \n    x = df.drop(columns = [\"SalePrice\"]).select_dtypes(exclude=['object'])\n    y = df[\"SalePrice\"].values\n    \n    model_original = sm.OLS(y, x).fit()\n    return model_original.summary()","6a27317f":"model_summary(train)","8ee678ec":"corr_matrix = train.corr()\npd.options.display.float_format = \"{:,.2f}\".format\nplt.figure(figsize = (12,10))\nsns.heatmap(corr_matrix)\nplt.title(\"Correlation Matrix of All Continuous Variables (Target = SalePrice)\")\nplt.show()","d9ef3746":"var_corrs = pd.DataFrame((corr_matrix[\"SalePrice\"] ** 2).sort_values(ascending = False))\nvar_corrs","aba01922":"def corr_matrix_filter(corr_matrix, r2): # <- We can experiment with different r^2 thresholds when we tune our model\n    \n    corr_matrix2 = corr_matrix[corr_matrix[\"SalePrice\"] ** 2 >= r2]\n    \n    low_corrs = list(corr_matrix[corr_matrix[\"SalePrice\"] ** 2 < r2].index)\n    \n    if r2 != 0:\n        corr_matrix2 = corr_matrix2.drop(columns = low_corrs)\n\n    return corr_matrix2","f95eee14":"corr_matrix2 = corr_matrix_filter(corr_matrix, 0.1) #only includes variables with r^2 >= 0.1\npd.options.display.float_format = \"{:,.2f}\".format\nplt.figure(figsize = (12,10))\nsns.heatmap(corr_matrix2, annot = True)\nplt.title(\"Filtered Correlation Matrix of Continuous Variables (SalePrice R^2 > 0.1)\")\nplt.savefig(\"corrmap1.png\")\nplt.show()","b775ec93":"pearson_selection_cont = list(corr_matrix2.index)\npearson_selection_cont","4d9ea7ce":"train_cont = train[pearson_selection_cont]\nmodel_summary(train_cont)","e8a0e9ad":"cat_vars = train.select_dtypes(include = \"object\")","491201d8":"cat_vars_encode = pd.get_dummies(cat_vars)","83b8a753":"cat_vars_encode[\"SalePrice\"] = train[\"SalePrice\"]","cbc18edb":"cat_vars_encode.head()","c2ce298a":"corr_matrix_cat = cat_vars_encode.corr()","45b9e20e":"var_corrs_cat = pd.DataFrame((corr_matrix_cat[\"SalePrice\"] ** 2).sort_values(ascending = False))\nvar_corrs_cat.head(22)","d6cae42a":"corr_matrix_cat2 = corr_matrix_filter(corr_matrix_cat, 0.1)","3456efe5":"plt.figure(figsize = (12,10))\nsns.heatmap(corr_matrix_cat2)\nplt.title(\"Filtered Correlation Matrix of Categorical Variables (SalePrice R^2 > 0.1)\")\nplt.savefig(\"corrmap2.png\")\nplt.show()","fe027ac4":"relevant_cat_vars = list(corr_matrix_cat2.index)","fee7d558":"remove_underscores = []\nfor i in relevant_cat_vars:\n    remove_underscores.append(i.split(\"_\", 1)[0]) #Splits the variables from their distinct values\npearson_selection_cat = list(set(remove_underscores)) #Removes repeats to get list of relevant variables.","bcf41069":"pearson_selection_cat","f3754e41":"pearson_selection_cat.remove('SalePrice') #The target variable, sale price, is already included in continous list","133ccc59":"train_cat = train[pearson_selection_cat]","0dd4843f":"train2 = pd.concat([train_cont, train_cat], axis = 1)","727959f6":"train2.head()","fc49c4e4":"list(train2.columns)","2a1be0a4":"def graph_cont(df, var):\n    plt.hist(df[var])\n    plt.xlabel(f\"{var}\")\n    plt.ylabel(\"Count\")\n    plt.title(f\"Distribution of {var}\")\n    plt.show()","437789b8":"[graph_cont(train_cont, i) for i in train_cont.columns]","83c346d4":"def graph_disc(df, var):\n    sns.countplot(df[var])\n    plt.xlabel(f\"{var}\")\n    plt.ylabel(\"Count\")\n    plt.title(f\"Distribution of {var}\")\n    plt.xticks(rotation = 90)\n    plt.show()","edec2760":"[graph_disc(train_cat, i) for i in train_cat.columns]","f7a2e179":"def scatter_plot(df, var):\n    plt.scatter(df[var], df[\"SalePrice\"])\n    plt.xlabel(f\"{var}\")\n    plt.ylabel(\"SalePrice\")\n    plt.title(f\"{var} vs. SalePrice\")\n    plt.xticks(rotation = 90)\n    plt.show()","bb25ce5f":"[scatter_plot(train_cont, i) for i in train_cont.columns]","0b595b43":"def box_plot(train2, df, var):\n    df[\"SalePrice\"] = train2[\"SalePrice\"]\n    sns.boxplot(df[var], df[\"SalePrice\"])\n    plt.xlabel(f\"{var}\")\n    plt.ylabel(\"SalePrice\")\n    plt.title(f\"{var} vs. SalePrice\")\n    plt.xticks(rotation = 90)\n    plt.show()","3cd59254":"[box_plot(train2, train_cat, i) for i in train_cat.columns if i != \"SalePrice\"]","0ce0fc19":"for i in list(train2.select_dtypes(exclude = \"object\").columns):\n    sm.qqplot(train2[i])\n    plt.title(f\"QQ Plot for {i}\")\n    plt.show()","2362e916":"train2[\"SalePrice\"] = np.log1p(train2[\"SalePrice\"])","38136889":"graph_cont(train2, \"SalePrice\")","11c36c9f":"train2[\"TotalInsideArea\"] = train_cont[\"TotalInsideArea\"]\ntrain2[\"TotalInsideArea\"] = np.log1p(train2[\"TotalInsideArea\"])","a02b0d2d":"graph_cont(train2, \"TotalInsideArea\")","f2e9bbab":"scatter_plot(train2, \"TotalInsideArea\")","aa1e32e8":"train2 = train2[(train2[\"SalePrice\"] > 11) & (train2[\"TotalInsideArea\"] < 9)]","ef3f93db":"scatter_plot(train2, \"TotalInsideArea\")","95bf0867":"train2[\"TotalOutsideArea\"] = np.log1p(train2[\"TotalOutsideArea\"])","1f10ecbd":"#train2 = train2[train2[\"TotalOutsideArea\"] != 0]","cf40a6ed":"len(list(train[train[\"TotalOutsideArea\"] == 0]))","07faa534":"graph_cont(train2, \"TotalOutsideArea\")","715f7908":"scatter_plot(train2, \"TotalOutsideArea\")","02f2e813":"train_set = pd.get_dummies(train2)","cf704af4":"x_vals = train_set.drop(columns = [\"SalePrice\"])\ny_val = train_set[\"SalePrice\"].values.reshape(-1,1)","516374db":"import math\n\ndef RMSLE(predict, target):\n    \n    total = 0 \n    \n    for k in range(len(predict)):\n        \n        LPred= np.log1p(predict[k]+1)\n        LTarg = np.log1p(target[k] + 1)\n        \n        if not (math.isnan(LPred)) and  not (math.isnan(LTarg)): \n            \n            total = total + ((LPred-LTarg) **2)\n        \n    total = total \/ len(predict)  \n    \n    return np.sqrt(total)","e2d01b9e":"def create_model(x_vals, y_val, model_type, t):\n    \n    x_train, x_test, y_train, y_test = train_test_split(x_vals, y_val, test_size = 0.2) #splitting into train and test\n    \n    model = model_type\n    model.fit(x_train, y_train) #fitting the model\n    \n    y_train_pred = np.expm1(model.predict(x_train)) #predicting and converting back from log(SalePrice)\n    y_test_pred = np.expm1(model.predict(x_test))\n    y_train = np.expm1(y_train)\n    y_test = np.expm1(y_test)\n    \n    if t == \"test\":\n        return RMSLE(y_test_pred, y_test) #evaluating\n    elif t == \"train\":\n        return RMSLE(y_train_pred, y_train)","36b22dc2":"print(\"Average Train Accuracy:\", round(np.mean([create_model(x_vals, y_val, LinearRegression(), \"train\") for i in range(100)]), 4))\nprint(\"Average Test Accuracy:\", round(np.mean([create_model(x_vals, y_val, LinearRegression(), \"test\") for i in range(100)]), 4))","693c1282":"print(\"Average Train Accuracy:\", round(np.mean([create_model(x_vals, y_val, Ridge(alpha = 0.01), \"train\") for i in range(100)]), 4))\nprint(\"Average Test Accuracy:\", round(np.mean([create_model(x_vals, y_val, Ridge(alpha = 0.01), \"test\") for i in range(100)]), 4))","21a18e4f":"print(\"Average Train Accuracy:\", round(np.mean([create_model(x_vals, y_val, Lasso(alpha = 0.01), \"train\") for i in range(100)]), 4))\nprint(\"Average Test Accuracy:\", round(np.mean([create_model(x_vals, y_val, Lasso(alpha = 0.01), \"test\") for i in range(100)]), 4))","29f5f430":"print(\"Average Train Accuracy:\", round(np.mean([create_model(x_vals, y_val, ElasticNet(alpha = 0.01), \"train\") for i in range(100)]), 4))\nprint(\"Average Test Accuracy:\", round(np.mean([create_model(x_vals, y_val, ElasticNet(alpha = 0.01), \"test\") for i in range(100)]), 4))","d649bcb3":"print(\"Average Train Accuracy:\", round(np.mean([create_model(x_vals, y_val, RANSACRegressor(), \"train\") for i in range(100)]), 4))\nprint(\"Average Test Accuracy:\", round(np.mean([create_model(x_vals, y_val, RANSACRegressor(), \"test\") for i in range(100)]), 4))","05671ae1":"train_sizes, train_scores, validation_scores = learning_curve(\nestimator = Ridge(),\nX = x_vals,\ny = y_val, train_sizes = [1,10,50,100,300,600,900], cv = 5,\nscoring = 'neg_mean_squared_error')","164fce7b":"train_scores_mean = -train_scores.mean(axis = 1)\nvalidation_scores_mean = -validation_scores.mean(axis = 1)","e1ab204d":"plt.plot(train_sizes, train_scores_mean, label = 'Training error')\nplt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\nplt.ylabel('MSE')\nplt.xlabel('Training set size')\nplt.title('Learning curves for Linear regression model')\nplt.legend()\nplt.show()","75707035":"#cleaning and feature engineering\n\ntest[\"TotalBsmtSF\"] = test[\"TotalBsmtSF\"].fillna(0)\ntest[\"GrLivArea\"] = test[\"GrLivArea\"].fillna(0)\ntest[\"GarageArea\"] = test[\"GarageArea\"].fillna(0)\ntest[\"GarageCars\"] = test[\"GarageCars\"].fillna(0)\ntest[\"GarageYrBlt\"] = test[\"GarageYrBlt\"].fillna(\"No Garage\")\ntest[\"GarageCond\"] = test[\"GarageCond\"].fillna(\"No Garage\")\ntest[\"GarageType\"] = test[\"GarageType\"].fillna(\"No Garage\")\ntest[\"GarageFinish\"] = test[\"GarageFinish\"].fillna(\"No Garage\")\ntest[\"GarageQual\"] = test[\"GarageQual\"].fillna(\"No Garage\")\ntest[\"BsmtFinType1\"] = test[\"BsmtFinType1\"].fillna(\"No Basement\")\ntest[\"BsmtFinType2\"] = test[\"BsmtFinType2\"].fillna(\"No Basement\")\ntest[\"BsmtExposure\"] = train[\"BsmtExposure\"].fillna(\"No Basement\")\ntest[\"BsmtQual\"] = test[\"BsmtQual\"].fillna(\"No Basement\")\ntest[\"BsmtCond\"] = test[\"BsmtCond\"].fillna(\"No Basement\")\ntest[\"MasVnrArea\"] = test[\"MasVnrArea\"].fillna(0)\ntest[\"MasVnrType\"] = test[\"MasVnrType\"].fillna(\"None\")\ntest[\"TotalInsideArea\"] = test[\"TotalBsmtSF\"] + test[\"GrLivArea\"] + test[\"GarageArea\"]\ntest[\"TotalOutsideArea\"] = test[\"WoodDeckSF\"] + test[\"OpenPorchSF\"] + test[\"EnclosedPorch\"] + test[\"3SsnPorch\"] + test[\"ScreenPorch\"] + test[\"PoolArea\"]\ntrain2_drop = train2.drop(columns = [\"SalePrice\"])","f38f1bf4":"test.info()","f948685b":"#creating test and train\n\nx_train = train_set.drop(columns = [\"SalePrice\"])\ny_train = train_set[\"SalePrice\"].values\nx_test = test[train2_drop.columns] #accessing only the features I selected earlier in the notebook\nx_test = pd.get_dummies(x_test)\n\n#preprocessing\n\nx_test[\"TotalInsideArea\"] = np.log1p(x_test[\"TotalInsideArea\"])\nx_train = x_train[x_train[\"TotalInsideArea\"] < 9]\nx_test[\"TotalOutsideArea\"] = np.log1p(x_test[\"TotalOutsideArea\"])\nx_train = x_train[x_train[\"TotalOutsideArea\"] != 0]\n\n#fitting the Ridge model (most accurate from earlier)\n    \nmodel = Ridge()\nmodel.fit(x_train, y_train) #fitting the model\n    \ny_train_pred = np.expm1(model.predict(x_train)) #predicting and converting back from log(SalePrice)\ny_test_pred = np.expm1(model.predict(x_test))","60d0e797":"x_train.info()","6564c338":"y_train_pred","03735de3":"y_test_pred","8328c4de":"preds = y_test_pred","0f9720cd":"ids = np.array(test[\"Id\"])","8e9c5f29":"submissions = pd.DataFrame({\"Id\":ids, \"SalePrice\":preds})","2b482a8c":"submissions.head()","c3a376d1":"submissions.to_csv(\"submission2.csv\", index = False)","9eab6cb2":"First, scatterplots with continuous variables","81e081c7":"Again, some outliers exist but we have achieved linearity and normality after taking away the values of 0.","0664917f":"### Testing Normality With Histograms","8126ee27":"One example of an insight is that the \"NridgHt\" and \"StoneBr\" neighborhoods appear to be the most expensive.","41ce72b8":"## QQ-Plots","776ff650":"The remaining variables with missing columns will be filled depending on their context.","485df830":"Because the sales price distribution is only skewed by some extremely high values, we will scale it logarithmically.","d3365486":"This is the notebook for my Applied Regression project.  All code for my models and visualizations that back up my report can be found here.\n\nThis is also my first machine learning notebook on Kaggle.  If you have any feedback, feel free to like and comment as I have much to learn! :)","b75d8392":"# 3. Feature Engineering","ebe9d749":"# Regression Project Notebook","02d3dfa5":"This section will involve creating new helpful variables from the ones we already have.","013b4a59":"Now, the data.","79a42a83":"We only removed 64 observations by removing the 0's.","6bac8e3c":"Our quality variables seem to the strongest correlation with sale price.  After testing multiple thresholds, 0.1 seemed to work best for the categorical variables, as well.","48589890":"### SalePrice (Target)","bf4d1c99":"All of these variables (the text before the \"_\") will be used in our model.  ","c4d38b62":"Unlike TotalInsideArea, this engineered variable follows more of a log-normal distribution.  We can logarithmically transform but many of the values will stick to 0 since some houses have no outside area.  These will be removed.","0d8a59ce":"## Preparing Model","39f6df52":"An example of an insight is that most houses in this dataset are in the \"NAmes\" neighborhood (probably Northern Ames).  ","ae28dc7e":"Lastly, we will use use a learning curve to test the validity of our model.","dbf9f78c":"I will be using numpy and pandas to clean my data, feature engineer, and perform any other operations on my dataframes.  I will be using matplotlib and seaborn for visualizations and Scikit-Learn for modeling and preprocessing.","055ae823":"This relationship is now nearly perfectly linear, but we can see that a few outliers are skewing the data.  ","6f0c36d1":"### TotalInsideArea","24ddd552":"In addition to the continuous features that were selected in the previous section, we must find out which categorical features best predict the sale price.  First, these variables will be hot one encoded.","8f58300c":"Note that sales price is the target variable.  Most of these distributions will have to be normalized when we begin scaling.","35ec40b9":"Correlation heatmaps are an excellent way to visualize the relationships between all of the continuous variables in a dataset and decide on what's relevant based on what R^2 thresholds give us the best models.  The colorscale represents R values.","102cc6c4":"## 9. Conclusion \/ Create Kaggle Submissions","5cc868be":"Now, the same correlation matrix process as before with our dataframe of binary encoded values.","ea5a7e84":"All missing values are now filled and the data is clean.","46c7dcb9":"### Basic Linear Regression","6157a67c":"# 4. Feature Selection","e67ff7c5":"# 7. Modeling","ae1075a0":"# 6. Scaling and Normalizing ","a66c1cad":"# 2. Cleaning","e6398515":"## Bivariate Analysis","fe256473":"# 8. Evaluating the Model","de250fe6":"Interestingly, The R^2 value went down though the F-statistic more than doubled.","5c2ace53":"## Normalizing and Removing Outliers","beccfddc":"This is the type of variable that would serve best as a boolean because houses with any pool at all might be much different from houses without a pool regardless of pool size, which is already now taken into account in total outside area.","9b4fbd27":"The two curves very closely converage, so the balance between over and underfitting is quite good.","4ece53a4":"### LASSO","11b122a0":"### Pearson's Correlation","c194c5af":"### RANSAC","af446e29":"### Dropping subsets of engineered variables","0f62390f":"## Univariate Analysis","51048a6e":"Ridge appears to be the most accurate model on average.  LASSO is the worst.","3d29bb2f":"Now, we will scale the necessary variables using different methods depending on the nature of the distribution.  I will also be removing clear outliers manually in this section but will use test a robust regressor later on to remove them by means of an algorithm.","b1e23ce5":"### Boxplots","a70f45f4":"Now, let's visualize how sales price varies among all categorical variables.","0b016fe0":"### Pool?","458604e4":"Note that categorical features will be encoded when we begin the modeling process.  But this dataframe is ready for EDA.","7325a843":"### Ridge Regression","38a406df":"Finally, we can train and test our model with various linear regression techniques  Again, we can go back and play around with R^2 thresholds on continuous and categorial variables, as well as different methods of scaling, to see which features create the best models.\n\nThe models will be evaluated by RMSLE (Room Mean Squared Logarithmic Error).","5f68102b":"# 5. Exploratory Data Analysis","5e8cdd21":"### Total Outside Area of the House (Sq Ft.)","84463f32":"This table represents R^2 values instead of R values unlike the heatmap which is why no negative numbers are present.  The inside square footage variable correlates with sales price the strongest.\n\nNow, a closer look at the most correlated variables.","fafa8d9b":"After testing many r^2 values by running the entirety of this completed notebook, the threshold resulting in the best model (best balance between overfitting and underfitting) is 0.1.  These are the continous features we will use in our model (including target variable, sale price):  ","1e965605":"All positive relationships that appear exponential.  These should become linear relationships after scaling.\n\nThere also exist some outliers we can remove.","f9eba853":"Now, we will visualize how sales price varies among all of our variables.","933bc07c":"The insignificant variables with many missing values will be removed:","efcf268d":"This value represents total area of the house, not including outside area.","24d41e34":"We can also scale TotalInsideArea logarithimically as it's mostly normal with the exception of a few very large outliers.","a9d7916c":"Before modeling, all of our variables should be scaled down to unit variance.","657e7428":"As we have an enormous amount of features, it would be useful to pick ones we use in our model before we perform any data analysis or preprocessing.\n\nNote: Usually, I would perform this step later in the process.  However, due to the large number of features that might lower the quality and speed of EDA\/heatmaps, I will select features now.","f4e27fc4":"### Pearson's Correlation","ee7d4917":"### Total Inside Area of the House (Sq Ft.)","022ffe50":"Now, a dataframe of all variables with out target strength of relationship with sales price. ","dde07a87":"## Selecting Categorical Features","c986511d":"The sole purpose of this section is to create a file that can be submitted to the Kaggle leaderboard.  I must apply most the same steps from the rest of this notebook to the testing set from Kaggle that will be used for the leaderboard.","adbbf526":"This algorithm punishes outliers based on the RANSAC algorithm.","650d1bd0":"Univariate analysis illustrates basic distributions of the variables of interest.  We will scale these distributions to be normal.","c3ffdf08":"### Elastic Net","5c0fbbc0":"The scatterplot is now without outliers.","f87f6c0c":"This section will involve dealing with the missing values and removing any unnecessary variables.   ","1a2cf105":"OverallQual, FullBath, TotRmsAbvGrd, and GarageCars are the only distributions that are approximately normal.  Note that a couple are numerical discrete values.","7273b10e":"This value represents the total area of the property on the outside of the house, including any porches, pool, and deck. ","d29b6a6b":"## Selecting Continuous Features","24c80d41":"The last step before creating our model is to normalize all of our data and remove any outliers.  In the EDA section, we already made some histograms and scatterplots to test normality and linearity, though we will now use more advanced techiniques.","b994edb1":"# 1. Importing Libraries and Data","1f6c2fd0":"Now, here are the categorical variables that will be used:","b85a2b84":"We can see the F-statistic is very high, as is adjusted R^2.  We will attempt to get these measures higher after selecting features.","83f3741a":"If we include any of these variables in our model in addition to the engineered variables, we could risk overfitting to to multicollinearity.","0267afaa":"## Putting it Together","6e3681ee":"Our final dataset ready to model:","0461d4a5":"### Bar Plots","9f74537e":"Before performing feature selection methods, let's take a look at our model summary with all the continuous variables.","857e1085":"The process of EDA will allow us to derive insights from our selected variables and help us see how we will scale the features and target.","645c928b":"Finally, we run a summary of the new model to see what has changed.","0d2e460e":"### Testing Linearity With Scatterplots","8ebc6707":"First, some basic histograms and bar plots.","3bd8e8c0":"The specifics of these modeling techniques will be elaborated on in the presentation and paper.","c4bb8f99":"We will be working with 80 features including house square footage, year built, quality rating, etc.  The target variable we are attempting to predict with a high level of accuracy is SalePrice.  There are 1460 total houses in the data, but we can see there exist missing values in some of the columns.","3429dcb4":"Now, bar graphs of categorical variables.","b267fc71":"### TotalOutsideArea","3081887d":"Finally, we fit the model, make predictions, and evaluate performance.","1ff06e7d":"First, histograms of continuous variables."}}