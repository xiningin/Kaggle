{"cell_type":{"89702362":"code","928996cb":"code","dc713219":"code","bfd65cbf":"code","02311dbf":"code","4bced5ce":"code","5d3bbfd2":"code","c23c4832":"code","3497a38b":"code","9310c3ae":"code","0d5ef1a1":"code","036bb664":"code","08141e2f":"code","f5c8b800":"code","75bdd202":"code","438b6569":"code","a98b5b8d":"code","0662f0cf":"code","47a89d65":"code","99a087ed":"code","39f3d0b1":"code","013aae0d":"code","950fe26d":"code","3ab02618":"code","18a3f6ae":"code","2490e809":"code","47e2bcec":"code","5070a5b2":"code","34190719":"code","940a2f6e":"code","30341323":"code","3da0193a":"code","b66b41de":"code","6de111a1":"code","07a5748c":"code","f865ae29":"code","20d9d829":"code","c231ae96":"code","01aeec9b":"code","505a9c85":"code","910a4134":"code","d261afaf":"code","78217c84":"code","f3bfdd52":"code","5aa4d547":"code","497c6c24":"code","064bf2f0":"code","0e504c4f":"code","a07e579b":"code","14432890":"code","1c424083":"code","831ef046":"code","a7d94f46":"code","83067808":"code","5c41eb8c":"code","7d80751f":"code","7a63bea2":"code","1d4b9a53":"code","00c76a3a":"code","9893df72":"code","a157727f":"code","a846754e":"code","7f386910":"code","bad1e527":"code","e8a3bafd":"code","9fd7817a":"code","d140c7f8":"code","4953d70b":"code","00781f7f":"code","435131a8":"code","cbf7df8e":"code","21da6639":"code","6603cbb9":"code","61a20b03":"code","8e0661f3":"code","3662f04a":"code","e10cbb4f":"code","d37e1f59":"code","3a859455":"code","3e9b2d11":"code","8237378f":"code","12883c65":"code","37aeb19e":"code","5b55e451":"code","e29e413f":"code","ea90fb7b":"code","10150b92":"code","e5bf1cd2":"code","d52d3bb5":"code","71ec550f":"code","4e1cf1f5":"code","5f267c4c":"code","0d255812":"code","f72dc212":"code","e3cf3499":"code","e7f9cbb6":"code","99c32c33":"code","3299f96f":"code","b39f59c4":"markdown","9681394d":"markdown","471d6106":"markdown","bd74fd38":"markdown","65821764":"markdown","3840a34b":"markdown","d0fa606d":"markdown","627ac235":"markdown","9da35289":"markdown","149914a8":"markdown","ae4d59b3":"markdown","b3d71e25":"markdown","cf141636":"markdown","fa5c1d81":"markdown","7a81c4e3":"markdown","e2e1e892":"markdown","e7f97618":"markdown","461b394d":"markdown","e462bec6":"markdown","bb12f40e":"markdown","c6f6bbae":"markdown","35493f7b":"markdown","12cd46ba":"markdown","f2681675":"markdown","f7a7ac8a":"markdown","eb87df95":"markdown","7f05edd2":"markdown","a52f3719":"markdown","ce9fd92c":"markdown","e3c2826f":"markdown","30367123":"markdown","da13f957":"markdown","eedcc5ec":"markdown","a573dc91":"markdown","62aaf856":"markdown","dae8d1d8":"markdown","e15160f4":"markdown","b01531ec":"markdown","e7dd3dcf":"markdown","17f00b77":"markdown","d724a335":"markdown","ce53f89a":"markdown","93d4695f":"markdown"},"source":{"89702362":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","928996cb":"train_dataset = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_dataset = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\ndf_train = train_dataset.copy()\ndf_train_baseline = train_dataset.copy()\n\ndf_test = test_dataset.copy()\ndf_test_baseline = test_dataset.copy()\n\nsample_submission.head()","dc713219":"df_train.head()","bfd65cbf":"df_train.info()","02311dbf":"plt.figure(figsize=(17,8))\nsns.stripplot(x='Neighborhood',y='SalePrice',data=df_train)\nplt.xticks(rotation=90)\nplt.show()","4bced5ce":"plt.figure(figsize=(17,8))\nsns.scatterplot(x='YearBuilt',y='SalePrice',data=df_train)\nsns.scatterplot(x='YearRemodAdd', y='SalePrice', data=df_train, alpha=0.7)\nplt.xticks(rotation=90)\nplt.legend(['Built','Remodeled'])\nplt.show()","5d3bbfd2":"plt.figure(figsize=(10,8))\nsns.stripplot(x='YrSold',y='SalePrice',data=df_train)\nplt.show()","c23c4832":"plt.figure(figsize=(10,10))\nsns.scatterplot(x='LotArea',y='SalePrice',data=df_train)\nplt.show()","3497a38b":"plt.hist(df_train['LotArea'],bins=10)","9310c3ae":"features = ['LotArea','BsmtFinSF1','BsmtFinSF2','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','GarageArea']\n\nfig, ax = plt.subplots(2,4, figsize=(19,10))\n\nk=0\nfor i in range(2):\n    for j in range(4):\n        sns.boxplot(x=features[k],data=df_train, ax=ax[i,j])\n        k+=1\nplt.show()","0d5ef1a1":"df_train_baseline = df_train_baseline.dropna(axis=1)\n\nbsl_cols = list(df_train_baseline.columns)\nbsl_cols.remove('SalePrice')\n\ndf_test_baseline = df_test_baseline[bsl_cols]\ndf_test_baseline = df_test_baseline.dropna(axis=0)\n\ndf_test_baseline.info()","036bb664":"import matplotlib.pyplot as plt\n\ndef print_missing_data(df, plot=False):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\n    print(missing_data.head(30))\n    if plot:\n        missing_data[missing_data['Total']>0].plot(kind='bar', y='Total')\n        plt.show()\n        \nprint_missing_data(train_dataset, plot=True)","08141e2f":"df_test_baseline.info()","f5c8b800":"all_bsmt_cols =['BsmtCond','BsmtQual','BsmtExposure','BsmtFinType1','BsmtFinSF1','BsmtFinType2',\n                'BsmtFinSF2','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']\n\ntrain_dataset.loc[train_dataset['BsmtCond'].isnull(), all_bsmt_cols]","75bdd202":"df_train.loc[(df_train['BsmtFinType2'].isnull())&(df_train['BsmtCond'].notnull()), all_bsmt_cols]","438b6569":"pd.crosstab(index=df_train['BsmtFinType2'],columns=df_train['BsmtFinType1'])","a98b5b8d":"df_bsmt_exp = df_train.loc[(df_train['SalePrice']>150000)&(df_train['TotalBsmtSF']>2000),all_bsmt_cols]\n\nCrosstabResult = pd.crosstab(index=df_bsmt_exp['BsmtFinType2'],columns=df_bsmt_exp['BsmtFinType1'])\nCrosstabResult","0662f0cf":"df_train.loc[(df_train['BsmtExposure'].isnull())&(df_train['BsmtCond'].notnull()), all_bsmt_cols]","47a89d65":"pd.crosstab(index=df_train['BsmtFinType1'],columns=df_train['BsmtExposure'])","99a087ed":"df_train.at[332,'BsmtFinType2'] = 'Unf'\ndf_train.at[948,'BsmtExposure'] = 'No'\n\nbasement_cat_cols = ['BsmtCond','BsmtQual','BsmtExposure','BsmtFinType1','BsmtFinType2']\n\nfor feature in basement_cat_cols:\n    df_train[feature] = df_train[feature].fillna('None')","39f3d0b1":"garage_cols = ['GarageType', 'GarageFinish','GarageQual','GarageCond','GarageArea']\n\ndf_train.loc[(df_train['GarageType'].isnull())&(df_train['GarageArea']!=0), garage_cols]","013aae0d":"for feature in garage_cols:\n    df_train[feature] = df_train[feature].fillna('None')\n    \ndf_train['GarageYrBlt'] = df_train['GarageYrBlt'].fillna(0)","950fe26d":"print_missing_data(df_train, plot=True)","3ab02618":"veneer = ['MasVnrType', 'MasVnrArea']\n\ntrain_dataset.loc[train_dataset['MasVnrType'].isnull(), veneer]","18a3f6ae":"df_train['MasVnrType'] = df_train['MasVnrType'].fillna('None')\ndf_train['MasVnrArea'] = df_train['MasVnrArea'].fillna(0)","2490e809":"df_train.loc[(df_train['PoolQC'].isnull())&(df_train['PoolArea']!=0),['PoolQC','PoolArea']]","47e2bcec":"df_train['PoolQC'] = df_train['PoolQC'].fillna('None')","5070a5b2":"misc = train_dataset.loc[train_dataset['MiscFeature'].notnull(),['MiscFeature', 'SalePrice']]\nnon_misc = train_dataset.loc[train_dataset['MiscFeature'].isnull(),['MiscFeature', 'SalePrice']]\n\nprint(non_misc['SalePrice'].mean())\nprint(misc['SalePrice'].mean())","34190719":"df_train['MiscFeature'] = df_train['MiscFeature'].fillna('None')","940a2f6e":"electric_mode = df_train['Electrical'].mode()[0]\ndf_train['Electrical'] = df_train['Electrical'].fillna(electric_mode)","30341323":"df_train['Alley'] = df_train['Alley'].fillna('None')\ndf_train['Fence'] = df_train['Fence'].fillna('None')\ndf_train['FireplaceQu'] = df_train['FireplaceQu'].fillna('None')","3da0193a":"fig, ax = plt.subplots(1,2, figsize=(13,5))\ndf_train['LotFrontage'].hist(ax=ax[0])\ndf_train.boxplot('LotFrontage',ax=ax[1])\nplt.show()","b66b41de":"lot_front_med = df_train.LotFrontage.median()\ndf_train['LotFrontage'] = df_train['LotFrontage'].fillna(lot_front_med)","6de111a1":"print_missing_data(df_train)","07a5748c":"def label_counter(train, test):\n    categorical_cols = train.select_dtypes(include=['object']).columns\n    train_cat = train[categorical_cols]\n    test_cat = test[categorical_cols]\n    test = {feature:(train_cat[feature].nunique(), test_cat[feature].nunique()) for feature in categorical_cols}\n    return test\n\nlabel_counter(df_train, df_test)","f865ae29":"print_missing_data(df_test, plot=True)","20d9d829":"features = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu']\n\nfor feat in features:\n    df_test[feat] = df_test[feat].fillna('None')","c231ae96":"garage_cols = ['GarageType', 'GarageFinish','GarageQual','GarageCond','GarageArea','GarageYrBlt','GarageCars']\n\ndf_test.loc[df_test['GarageCars'].isnull(), garage_cols]","01aeec9b":"garage_finish_mode = df_train.loc[df_train['GarageType']=='Detchd', 'GarageFinish'].mode()[0]\ngarage_qual_mode = df_train.loc[df_train['GarageType']=='Detchd', 'GarageQual'].mode()[0]\ngarage_cond_mode = df_train.loc[df_train['GarageType']=='Detchd', 'GarageCond'].mode()[0]\ngarage_area_med = df_train.loc[df_train['GarageType']=='Detchd', 'GarageArea'].median()\ngarage_year_mean = int(df_train.loc[df_train['GarageType']=='Detchd', 'GarageYrBlt'].mean())\ngarage_cars_mode = df_train.loc[df_train['GarageType']=='Detchd', 'GarageCars'].mode()[0]\n\n# Values to impute for Detached Garage Types\nprint('Mode of Garage Finish:')\nprint(garage_finish_mode,'\\n')\n\nprint('Mode of Garage Quality:')\nprint(garage_qual_mode, '\\n')\n\nprint('Mode of Garage Finish:')\nprint(garage_cond_mode,'\\n')\n\nprint('Mode of Garage Finish:')\nprint(garage_area_med,'\\n')\n\nprint('Mean of Garage Year Built:')\nprint(garage_year_mean,'\\n')\n\nprint('Garage Cars Mode:')\nprint(garage_cars_mode)","505a9c85":"df_test.at[666,'GarageFinish'] = garage_finish_mode\ndf_test.at[666, 'GarageQual'] = garage_qual_mode\ndf_test.at[666, 'GarageCond'] = garage_cond_mode\ndf_test.at[666, 'GarageYrBlt'] = garage_year_mean\n\ndf_test.at[1116,'GarageFinish'] = garage_finish_mode\ndf_test.at[1116, 'GarageQual'] = garage_qual_mode\ndf_test.at[1116, 'GarageCond'] = garage_cond_mode\ndf_test.at[1116, 'GarageArea'] = garage_area_med\ndf_test.at[1116, 'GarageYrBlt'] = garage_year_mean\ndf_test.at[1116, 'GarageCars'] = garage_cars_mode","910a4134":"garage_cols.remove('GarageYrBlt')\n\nfor feature in garage_cols:\n    df_test[feature] = df_test[feature].fillna('None')\n\ndf_test['GarageYrBlt'] = df_test['GarageYrBlt'].fillna(0)","d261afaf":"all_bsmt_cols = ['BsmtCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n                'BsmtFinSF1','BsmtFinSF2','TotalBsmtSF','BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF']\n\nbsmt_cat_cols = ['BsmtCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\nbsmt_num_cols = ['BsmtFinSF1','BsmtFinSF2','TotalBsmtSF','BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF']\n\ndf_test.loc[(df_test['BsmtCond'].isnull())&(df_test['BsmtFinType1'].notnull()), all_bsmt_cols]","78217c84":"gdqual_bsmt_mode = df_train.loc[df_train['BsmtQual']=='Gd','BsmtCond'].mode()[0]\ntaqual_bsmt_mode = df_train.loc[df_train['BsmtQual']=='TA','BsmtCond'].mode()[0]\n\ndf_test.at[580, 'BsmtCond'] = gdqual_bsmt_mode\ndf_test.at[725, 'BsmtCond'] = taqual_bsmt_mode\ndf_test.at[1064, 'BsmtCond'] = taqual_bsmt_mode","f3bfdd52":"df_test.loc[(df_test['BsmtExposure'].isnull())&(df_test['BsmtFinType1'].notnull()), all_bsmt_cols]","5aa4d547":"bsmt_qual_mode_exp = df_train.loc[df_train['BsmtQual']=='Gd','BsmtExposure'].mode()[0]\n\ndf_test.at[27,'BsmtExposure'] = bsmt_qual_mode_exp\ndf_test.at[888,'BsmtExposure'] = bsmt_qual_mode_exp","497c6c24":"df_test.loc[(df_test['BsmtQual'].isnull())&(df_test['BsmtFinType1'].notnull()), all_bsmt_cols]","064bf2f0":"bsmt_fa_cond_mode_qual = df_train.loc[df_train['BsmtCond']=='Fa','BsmtQual'].mode()[0]\nbsmt_TA_cond_mode_qual = df_train.loc[df_train['BsmtCond']=='TA','BsmtQual'].mode()[0]\n\ndf_test.at[757,'BsmtQual'] = bsmt_fa_cond_mode_qual\ndf_test.at[758,'BsmtQual'] = bsmt_TA_cond_mode_qual","0e504c4f":"df_test.loc[df_test['BsmtFullBath'].isnull(), all_bsmt_cols]","a07e579b":"for feature in bsmt_cat_cols:\n    df_test[feature] = df_test[feature].fillna('None')\n    \nfor feature in bsmt_num_cols:\n    df_test[feature] = df_test[feature].fillna(0)","14432890":"print_missing_data(df_test)","1c424083":"df_test['LotFrontage'] = df_test['LotFrontage'].fillna(lot_front_med)","831ef046":"df_test.loc[df_test['MasVnrType'].isnull(),veneer]","a7d94f46":"veneer_mode = df_train.loc[(df_train['MasVnrArea']>150)&(df_train['MasVnrArea']<250),'MasVnrType'].mode()[0]\n\ndf_test.at[1150,'MasVnrType'] = veneer_mode\ndf_test['MasVnrType'] = df_test['MasVnrType'].fillna('None')\ndf_test['MasVnrArea'] = df_test['MasVnrArea'].fillna(0)","83067808":"df_test.loc[df_test['MSZoning'].isnull(),:]","5c41eb8c":"pd.crosstab(index=df_train['MSZoning'], columns=df_train['MSSubClass'])","7d80751f":"df_test['MSZoning'] = df_test['MSZoning'].fillna('RL')","7a63bea2":"df_test.loc[df_test['Utilities'].isnull(),:]","1d4b9a53":"df_test['Utilities'] = df_test['Utilities'].fillna('AllPub')","00c76a3a":"functional_mode = df_train['Functional'].mode()[0]\ndf_test.loc[df_test['Functional'].isnull(),:]","9893df72":"df_test['Functional'] = df_test['Functional'].fillna(functional_mode)","a157727f":"exterior_cols = ['ExterQual','ExterCond','Exterior1st','Exterior2nd','OverallQual', 'RoofStyle']\n\ndf_test.loc[df_test['Exterior1st'].isnull(),exterior_cols]","a846754e":"df_train.loc[(df_train['ExterQual']=='TA')&(df_train['RoofStyle']=='Flat'),exterior_cols]","7f386910":"exterior1st_mode = df_train.loc[(df_train['ExterQual']=='TA')&(df_train['RoofStyle']=='Flat'), 'Exterior1st'].mode()[0]\nexterior2nd_mode = df_train.loc[(df_train['ExterQual']=='TA')&(df_train['RoofStyle']=='Flat'), 'Exterior2nd'].mode()[0]","bad1e527":"df_test['Exterior1st'] = df_test['Exterior1st'].fillna(exterior1st_mode)\ndf_test['Exterior2nd'] = df_test['Exterior2nd'].fillna(exterior2nd_mode)","e8a3bafd":"pd.crosstab(index=df_train['RoofStyle'], columns=df_train['OverallQual'])","9fd7817a":"kitchen = ['KitchenAbvGr', 'KitchenQual','OverallQual']\ndf_test.loc[df_test['KitchenQual'].isnull(), kitchen]","d140c7f8":"pd.crosstab(index=df_test['OverallQual'], columns=df_test['KitchenQual'])","4953d70b":"df_test.at[95,'KitchenQual'] = 'TA'\ndf_test['KitchenQual'] = df_test['KitchenQual'].fillna('TA')","00781f7f":"df_test.loc[df_test['SaleType'].isnull(),:]","435131a8":"pd.crosstab(index=df_train['SaleType'],columns=df_train['SaleCondition'])","cbf7df8e":"sale_type_mode = df_test['SaleType'].mode()[0]\ndf_test['SaleType'].fillna(sale_type_mode, inplace=True)","21da6639":"print_missing_data(df_test)","6603cbb9":"cat_cols = list(df_train.select_dtypes(include='object').columns)\n\nfor col in cat_cols:\n    df_train[col].value_counts().sort_values().plot(kind = 'barh')\n    plt.title(col)\n    plt.xlabel('Number of Occurences')\n    plt.show()","61a20b03":"label_counter(df_train, df_test)","8e0661f3":"import seaborn as sns\n\nhousing_tr = df_train.drop(['Id','SalePrice'],axis=1).copy()\nhousing_ts = df_test.drop(['Id'],axis=1).copy()\n\ncor = housing_tr.corr()","3662f04a":"plt.figure(figsize=(18,15))\nsns.heatmap(cor, cmap=plt.cm.CMRmap_r)\nplt.show()","e10cbb4f":"def correlation(dataset, threshold):\n    col_corr = set()\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i,j]) > threshold:\n                colname = corr_matrix.columns[i]\n                col_corr.add(colname)\n    return col_corr","d37e1f59":"drop_features = correlation(housing_tr, 0.7)\nhousing_tr.drop(drop_features, axis=1, inplace=True)\nhousing_ts.drop(drop_features, axis=1, inplace=True)","3a859455":"housing_tr.info()","3e9b2d11":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression","8237378f":"cat_cols_train = df_train.select_dtypes(['object']).columns\n\ncat_data = df_train.loc[:,cat_cols_train].values\nprice = df_train.iloc[:,-1].values\n\ncat_data = cat_data.astype(str)\n\noe = OrdinalEncoder()\noe.fit(cat_data)\ncat_data = oe.transform(cat_data)","12883c65":"fs = SelectKBest(score_func=mutual_info_regression, k='all')\nfs.fit(cat_data,price)\n\nscores = fs.scores_\n\nfeaturesMI = pd.Series(scores, index=cat_cols_train).sort_values(ascending=False)\nselected_cols = featuresMI[featuresMI<0.1].index","37aeb19e":"featuresMI[selected_cols].plot.bar()\nplt.show()","5b55e451":"housing_tr.drop(selected_cols, axis=1, inplace=True)\nhousing_ts.drop(selected_cols, axis=1, inplace=True)","e29e413f":"housing_tr.info()","ea90fb7b":"num_cols_train = housing_tr.select_dtypes(['int64','float64']).columns\n\nnum_data = housing_tr.loc[:,num_cols_train].values\nprice = df_train.iloc[:,-1].values\n\nfs = SelectKBest(score_func=mutual_info_regression, k='all')\nfs.fit(num_data,price)\nnum_score = fs.scores_\n\nnum_featuresMI = pd.Series(num_score, index=num_cols_train)\nselected_numcols = num_featuresMI[num_featuresMI<0.1].index\n\nselected_numcols","10150b92":"housing_tr.drop(selected_numcols, axis=1, inplace=True)\nhousing_ts.drop(selected_numcols, axis=1, inplace=True)","e5bf1cd2":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split","d52d3bb5":"df_train_baseline","71ec550f":"train_bsl = df_train_baseline.drop(['Id','SalePrice'],axis=1)\ntest_bsl = df_train_baseline.drop(['Id'],axis=1)\n\nbsl_cat = train_bsl.select_dtypes(include = ['object']).columns\nbsl_num = train_bsl.select_dtypes(exclude = ['object']).columns\n\nbsl_train_num = train_bsl[bsl_num]\nbsl_train_cat = train_bsl[bsl_cat]\nbsl_train_cat['train'] = 1\n\nbsl_test_num = test_bsl[bsl_num]\nbsl_test_cat = test_bsl[bsl_cat]\nbsl_test_cat['train'] = 0\n\nbsl_combined_cat = pd.concat([bsl_train_cat, bsl_test_cat])\nbsl_combined_cat_encode = pd.get_dummies(bsl_combined_cat)\n\nbsl_train_cat_enc = bsl_combined_cat_encode.loc[bsl_combined_cat_encode['train']==1,:]\nbsl_test_cat_enc = bsl_combined_cat_encode.loc[bsl_combined_cat_encode['train']==0,:]","4e1cf1f5":"bsl_train_enc = pd.concat([bsl_train_num, bsl_train_cat_enc], axis=1)\nbsl_test_enc = pd.concat([bsl_test_num, bsl_test_cat_enc], axis=1)\n\nX = bsl_train_enc\nX_test = bsl_test_enc\ny = df_train_baseline['SalePrice']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","5f267c4c":"# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42, verbose=1)\n\n# Train the model on training data\nrf.fit(X_train, y_train)\nrf.score(X_val, y_val)","0d255812":"categorical_features = housing_tr.select_dtypes(include = [\"object\"]).columns\nnumerical_features = housing_tr.select_dtypes(exclude = [\"object\"]).columns\n\ntrain_num = housing_tr[numerical_features]\ntrain_cat = housing_tr[categorical_features]\ntrain_cat['train'] = 1\n\ntest_num = housing_ts[numerical_features]\ntest_cat = housing_ts[categorical_features]\ntest_cat['train'] = 0\n\ncombined_cat = pd.concat([train_cat,test_cat])\ncombined_cat_encode = pd.get_dummies(combined_cat)\n\ntrain_cat_enc = combined_cat_encode.loc[combined_cat_encode['train']==1,:]\ntest_cat_enc = combined_cat_encode.loc[combined_cat_encode['train']==0, :]\n\ntrain_cat_enc = train_cat_enc.drop(['train'], axis=1)\ntest_cat_enc = test_cat_enc.drop(['train'], axis=1)\n\ntest_cat_enc","f72dc212":"df_train_enc = pd.concat([train_num, train_cat_enc], axis=1)\ndf_test_enc = pd.concat([test_num, test_cat_enc], axis=1)\n\nX = df_train_enc\nX_test = df_test_enc\ny = train_dataset['SalePrice']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","e3cf3499":"# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42, verbose=1)\n\n# Train the model on training data\nrf.fit(X_train, y_train)\nrf.score(X_val, y_val)\n\ny_pred = rf.predict(X_test)","e7f9cbb6":"sample_submission","99c32c33":"submission = pd.DataFrame()\nsubmission['Id'] = test_dataset['Id']\nsubmission['SalePrice'] = y_pred\nsubmission.to_csv(\"submission.csv\", index=False)","3299f96f":"submission.head()","b39f59c4":"**Garage Features**\n\nAs we did with the Basement affiliated features, we will inspect to see if null values are truly missing values assigned by error or if they correspond to properties without a Garage. Since there number of missing values is identical across all columns, it is safe to assume that all missing values were included on purpose. We validate this by assessing whether or not the GarageArea is consistently 0 since we know that GarageArea does not contain any missing values. We do this by adding the second condition into the the third line of code. The results back our claim that Garage NaN values correspond to properties wihout garages rather than errors in reporting.","9681394d":"**Electrical**\n\nThere is only one missing value in the electricity column. We are not given the option of None based on the list of possible values given in the text document. Thus we will use the mode of electricity values of all samples in the training dataset","471d6106":"***How does sales prices fluctuate between neighborhoods?***\n\nFrom the stripplot below we see that neighborhood has a distinct relationship with the price of the a particular property. For most neighborhoods, house prices generally fluctuate between $100,000 to $300,000. However, we observe that neighborhoods such as Northridge, Northridge Heights, and Stone Brook have a greater concentration of more expensive properties while neighborhoods such as Brookside, Old Town, and Sawyer have properties concentrated at lower values","bd74fd38":"Based on the results of our crosstabs, we have found values that would allow us to impute values to the missing data based on properties that are similar in nature in terms of basement specifications. We will use the pandas \"at\" function to impute the two samples of interest and then fill the remaining missing values with \"None\" based on the assumptions made earlier.","65821764":"# Advanced Regression Techniques: Ames Housing Dataset","3840a34b":"**MS Zoning**","d0fa606d":"After completing the final feature, we will double check to see that all of our transformations were performed as desired:","627ac235":"**Lot Frontage**\n\nBefore we decide to impute a numerical column, it is important to refer to the distribution of values for that feature. We use a histogram and boxplot of the data to visualize the distribution and presence of outliers in the data. Since the data is skewed and contains a high number of outliers we will impute the values using the median rather than the mean","9da35289":"**Are older houses sold for less?**\n\nFrom the scatter plot below, we see that there is a nonlinear relationship betwween the year a house was built and the price it was sold at. This is a relationship we expect to see within the dataset. Notice how the relationship between the year a house was remodeled and the sale price is nearly identical to the that of the year it was built. This is important because we may look to drop one of these features as we do not want the features we select to be redundant. ","149914a8":"Next, we turn our attention to the extra missing value in the BsmtExposure column. Similarly, we utilize crosstabs to gain an idea of the distribution of properties in the area and the values they have for our features of interest. Based on other basement features of the sample of interest, we can see that most properties in the area with unfinished basements do not have exposure as specified by the BsmtExposure feature","ae4d59b3":"## Baseline Model\n\nFor our baseline model we will drop columns with missing values in the training dataset. Along with the corresponding columns we will drop the remaining rows with missing values in the test dataset. While this outlook on data cleaning may be quick and painless and thus useful when one wants to explore various ML algorithms, I believe that this taking the time to process the data is crucial and thus must be handled with care. Complete and comprehensive datasets are hard to come by and it is important that we exhaust all the data we are given when building models!","b3d71e25":"Attributes that are highly correlated:\n* 1st Floor Space (1stFlrSF) & Total Basement Square Feet\n* Total Rooms Above Ground (TotRmsAbvGd) & GrLivArea\n* Garage Area & GarageCars","cf141636":"**Basement Features**\n\nThere are a total of 10 features that describe the basement quality, condition, and features in the dataset. Out of the 10 features, 5 of them are missing roughly 3% of their values. From the plot above, we notice that there two features have 38 missing values while the others only have 37. Therefore we can assume there could be an error in data entry present within these features.\n\nWe begin by looking at the Basement Condition and Basement Quality features which are described by the following set of labels:\n* **Ex**: Excellent\n* **Gd**: Good\n* **Ta**: Typical\n* **Fa**: Fair\n* **Po**: Poor\n* **NA**: No Basement\n\nBoth of these features have 37 missing values, which we can hypothesize to be properties without basements. By displaying all of the samples containing missing values for these columns, we notice that there is a uniformity in missing data for all 37 values. This validates our assumption that these samples are not corrupted by error.\n\nTransformation: Convert null values for BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2 to 'None' rather than NaN","fa5c1d81":"**Veneer Types**\n\nWe turn our attention to the veneer associated features. Before we drop these values, let's make sure that the missing values for veneer related columns are consistent with each other and not scattered across the dataset. We see that they are all contained within the same set of samples and thus most likely missing because the house did not have a veneer. ","7a81c4e3":"### Training Dataset\nThe function *print_missing_data* will allow us to display and visualize the percentage of missing data within the given dataset. This function will make it easier to validate our transformations and keep track of our progress throughout the data cleaning process.\n\nFor the training set we notice that there are 19 features with missing values, with varying percentages of missing data. I am quite skeptical that such a great percentage of data was excluded by error. Let us dive into cleaning and see if we can exhaust as much data as possible for this model!","e2e1e892":"**Functional**","e7f97618":"**Remaining Categorical Features**\n\nThe remaining categorical features all have None listed as possible values. Given that these are all features that are not expected to be included in your average property, we will impute \"None\" to fill in the remaining missing values","461b394d":"\n## Discrepancy Detection & Transformations\n\nIn this section, we will investigate the dataset for discrepancies such as missing values, outliers\/noise, and other inconsistencies. First, we will start with the training dataset by searching for missing values. For this competition, we will use the text file to provide us with additional context on how to handle missing values for particular features. It is important for the user to determine whether or not a missing value has occurred due to error in the data entry process or if the values actually correspond to true data. We must not drop sampler nor features for the sake of convenience without any evidence to back our assumptions. \n\n***Methods to consider for missing value imputation***\n* Ignore the sample\/row\n* Fill in the missing value manually\n* Use a global constant to fill in the missing value\n* Use a measure of central tendency for the feature to fill in the missing value (mean\/median\/mode)\n* Use central tendency for samples containing similar properties\n* Use the most probable value to fill in the missing value (Regression\/Inference Based Methods)","e462bec6":"**Categorical Features**\n\nFirst we use an ordinal encoder to encode the labels. We then fit and transform the features using the SelectKBest model, which automates the MI calculations, and print out the scores. We chose to filter out the features with a Mutual Information score less than 0.1. This corresponds to roguhly 30 features. ","bb12f40e":"# Exploratory Data Analaysis\n\nLet's take a look at some key relationships between features of interest and the sale price.  ","c6f6bbae":"**Lot Frontage**","35493f7b":"## Feature Selection\n\nBefore we begin to think about implementing our model, we must select the most useful features for predicting Sale Prices. We use two methods to determine which features to keep and which ones to drop; **Correlation**, and **Mutual Information**\n\n","12cd46ba":"**SaleCondition**","f2681675":"***Does the year a house is sold play a role in the price it was sold at?***\n\nThe plot below visualizes house prices based on the year that they were sold. To my surprise, I would have expected a drop off in price value as this data aligns with the housing crisis that hit the nation in the late 2000's. Nevertheless, the data shows that house prices seem evenly distributed throughout the 5 years of data that were collected","f7a7ac8a":"In this notebook we'll be diving into the Ames Housing Dataset. The Ames Housing dataset is the alternative to the widely used Boston Housing dataset and I'm excited to get my hands dirty with the data.\n\nFor this project, I'll be focusing on sklearn's RandomForest Regressor to predict sales prices. Random Forest algorithms are widely believed to be an \"off the shelf\" ensemble method robust against outliers. I will use this notebook to compile a baseline model with minimal data cleaning methods and no feature selection strategies and measure its performance against model with cleaned data using various methods as well as widely used feature selection techniques. \n\nWhile I have no expectations for the outcome of this experiment, I am intrigued to see how useful data processing methods can be for tree based methods. It will be interesting to see how I can improve upon the results in comparison to the baseline model and how it will force me to explore the various proccessing and feature engineering methods widely documented in literature. With that being said, let's get started!","eb87df95":"This leaves us with two suspects, which we identify to be the 332nd and 948th samples in the training dataset. The first sample mentioned is missing the BsmntFinType2 feature. \n\nBsmntFinType2 Labels: \n* **GLQ** Good Living Quarters\n* **ALQ** Average Living Quarters\n* **BLQ** Below Average Living Quarters\n* **Rec** Average Rec Room\n* **LVQ** Low Quality\n* **Unf** Unfinished\n* **NA** No Basement\n\nWe can use the remaining features to impute a label for this feature based on properties with similar specs (i.e. Overall Size, Type 1 Finish Quality, SalePrice, etc.)\n\nThe second sample mention is missing the Basement Exposure feature which consists of the set of possible labels.\n\nBsmtExposure Labels:\n* **Gd** Good Expsure\n* **Av** Average Expsure\n* **Mn** Minimum Expsure\n* **No** No Expsure\n* **NA** No Basement","7f05edd2":"**Utilities**","a52f3719":"# Conclusion\n\nBased on the accuracy score, our baseline model performed just as well on the validation set as our model with extensive preprocessing. Even though the data processing aspect of this notebook was quite time consuming, I believe that there are many improvements that can be made to achieve a higher accuracy score. Here are some takeaways that I have from embarking on this problem:\n\n* Random Forests are truly robust and enhanced data preprocessing and feature engineering techniques are necessary to improve performance\n* While the results may not be conclusive, I still hold the belief that it is necessary to take the time to carefully clean data and withold from dropping data for the sake of convenience\n* I would like to test this experiment on a larger dataset with more samples and features to see if results still hold\n* Many of the features that were cleaned ended up being dropped during feature selection. Perhaps a more comprehensive EDA would allow me to foresee the likelihood of particular features being dropped\n\nSome improvements to consider in the future:\n* Perfrom search methods for hyperparameter tuning (GridSearch, RandomSearch, Bayesian Optimization)\n* Test different encoding methods\n* Resample data so that labels for categorical features have higher representation in the modelling stage\n* Compare results for RF with that of other models (Ridge & Lasso Regression, Neural Networks, etc.)\n* Investigate model performance for different thresholds in feature selection stage\n\nIf you've made it to this point, thanks for walking through this trial with me! Any and all feedback is welcome so please feel free to leave a comment or suggestion. While I didn't get the results I initially expected, I was pleasantly surprised with the outcome and am motivated to explore deeper into this topic. ","ce9fd92c":"# Data Cleaning\n\n","e3c2826f":"**Pool Features**\n\nThe pool feature has the most amount of missing values out of all of the features in the dataset. In the text file we see that NA is an option for properties that do not have a pool which can be read as a missing value by pandas. We will validate that this is the case. While this feature will most likely be dropped in our feature selection phase, well refrain from dropping it until advised to do so later","30367123":"**Numeric Features**\n\nWe will deploy the same methodology for the numeric data, except this time we have no need to encode the values.","da13f957":"**Exterior**","eedcc5ec":"We can use the crosstab function from the pandas library to understand how properties are distributed based on BsmtFinType1 and BsmtFinType2. First we will perform a crosstab on the entire training set, and then narrow our search for properties with similar sale price and basement size to see if our results are in agreement. The tables generate lead us to believe that the the basement type 2 feature is most likely Unf (unfinished)","a573dc91":"### Mutual Information\n\nThe essence of Mutual Information is summarized concisely by Wikipedia:\n> In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables. More specifically, it quantifies the \"amount of information\" (in units such as shannons (bits), nats or hartleys) obtained about one random variable by observing the other random variable.\n\nMutual information is a valuable tool in measuring how effective a particular feature is as an indicator for the target variable. In other words, it measure how much information we gain about the target variable, knowing a given variable. MI has a scaling system such that the minimum possible score is 0 and with no absolute maximum, however increase in values are on a logarithmic scale. To calcualte the MI scores we will separate the varaibles into categorical and numeric features and act on them separately. The formula for Mutual Information","62aaf856":"# Label Encoding and Model Development","dae8d1d8":"**Loading the datasets**\n\nWe will load the training and test datasets and copy them to create our main and baselline sets. We will leave the original datasets untouched for reference later on.","e15160f4":"### Test Datset\n\nWe will take a similar approach to the training dataset, being mindful not to impute values that can potentially bias our data. First we define a function that will allow us to see the unique labels used in both the training and test dataset. We may want to avoid features that have an unequal amount of labels in both sets. In real life scenarios, we may not have the luxury of performing this operation but it will help us against overfitting on the training set. \n\nThe initial results from our label counter function, we see that for many columns, some feature labels are not present in both sets. This will be an issue when encoding labels later on. \n\nWe will refrain from dropping any samples and will take similar approach as we did with the training set. It is important to note that all imputed values will be done so from the training set as we must assume that we have no prior knowledge of the test set when we evaluate our model. ","b01531ec":"**Basement Features**","e7dd3dcf":"**Veneer Types**","17f00b77":"**Kitchen Quality**","d724a335":"**Garage Features**\n\n","ce53f89a":"**MiscFeature**\n\nMiscellaneous features such as tennis court (as listed in the text document) may be a positive indicator for price prediction. While there may not be enough instances in our dataset to convince us to keep it, we will impute the missing values with \"None\" and most likely drop it in the feature selection stage","93d4695f":"### Correlation\n\nWe measure correlation between two variables through Pearson's correlation coefficient. It is defined by the covariance of the two selected features divided by the product of their standard deviations. For a sample, we substitute the estimates for covariance and standard deviations as represented by the formula below\n\n$$r_{xy} =  \\frac{\\Sigma^{n}_{i=1}\\left(x_i-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{ \\sqrt{\\Sigma^{n}_{i=1}\\left(x_{i}-\\bar{x}\\right)^2} \\sqrt{\\Sigma^{n}_{i=1}\\left(y_{i}-\\bar{y}\\right)^2}}$$\n\nIn the equation x and y represent two columns. We will use pandas corr() function to generate a matrix of correlation calculations within the dataset and plot the values through a heatmap. We will then filter out and drop the columns with a pearson correlation coefficient greater than 0.7 to reduce the redundancy of features in our dataset."}}