{"cell_type":{"7b5ced99":"code","b73bcd1f":"code","c379cd12":"code","2c890308":"code","7ff5a2f5":"code","c4033321":"code","d1b51011":"code","f40d981e":"code","83813198":"code","7e00ddd0":"code","3ca64281":"code","2c51676e":"code","3b3b98af":"code","b46b7da2":"code","63e9015a":"code","4f3ac24e":"code","71fe18e0":"code","b930c5a6":"code","503e31d5":"code","7c928ef7":"code","cf98538a":"code","e4b75396":"code","a248a906":"code","b520f3c7":"code","288e3e70":"code","bea00079":"code","913efa12":"code","0770092b":"code","2415dd2c":"code","25d9d120":"code","2b611754":"code","a1752da6":"code","c84319b6":"code","1e15481e":"code","d0c420a1":"code","1630fb84":"code","5763d8a7":"markdown","6d10e109":"markdown","1ed2754d":"markdown","19100577":"markdown","1025ef1f":"markdown","ac2a3df5":"markdown","b3e0d33f":"markdown","4fe43edb":"markdown","72f7a5f1":"markdown","1d5de9e9":"markdown","ee081972":"markdown","fef3c93d":"markdown","5733fcfb":"markdown","4543e6a9":"markdown","7314ac53":"markdown","2612790f":"markdown","ddce245c":"markdown","e60a41f4":"markdown","c8685893":"markdown","4c3a129d":"markdown","62e24b46":"markdown","f9ef5b60":"markdown","a7a10c2b":"markdown","f4d1dc7f":"markdown","038ac829":"markdown","565ca405":"markdown","e6fc8593":"markdown","3ab2323f":"markdown","415f9769":"markdown","28aa1b53":"markdown","45fc8578":"markdown","04227402":"markdown","f98662a1":"markdown","c2e94709":"markdown","684eb33b":"markdown","a54f05d6":"markdown","998a11eb":"markdown","3de684db":"markdown","93c8679b":"markdown","1605eb72":"markdown","fb72e90b":"markdown","5c8c515e":"markdown","8d644ee3":"markdown","93fa977b":"markdown","ffd98f96":"markdown","daa17fe3":"markdown"},"source":{"7b5ced99":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly as pl\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b73bcd1f":"data = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","c379cd12":"data.head()","2c890308":"data.columns","7ff5a2f5":"data.corr()","c4033321":"f,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()","d1b51011":"data.corr()[\"target\"].sort_values()","f40d981e":"data.info()","83813198":"data.isnull().sum()","7e00ddd0":"data.describe()","3ca64281":"pd.crosstab(data.age,data.target)","2c51676e":"crosstabAge = pd.crosstab(data.age,data.target)\ncrosstabAge.plot(kind=\"bar\",figsize=(20,8), color =\"cmyk\", alpha = 0.8) #alpha is opacity\nplt.title('Heart Disease Frequency Given Ages')\nplt.xlabel('Ages')\nplt.ylabel('Frequency')\nplt.legend([\"Have not Disease\", \"Have Disease\"])\nplt.show()","3b3b98af":"data[\"target\"].value_counts()","b46b7da2":"sns.countplot(data[\"target\"], palette=\"Set2\")\nplt.xlabel(' 0 = Not Have Disease,  1 = Have Disease')","63e9015a":"crosstabSex = pd.crosstab(data[\"sex\"], data[\"target\"])\ncrosstabSex","4f3ac24e":"crosstabSex.plot(kind=\"bar\", figsize=(15,6), color=\"cmyk\")\nplt.title(\"Heart Disease Frequency Given Sex\")\nplt.xticks(rotation=0)\nplt.xlabel(\"0 = Female , 1 = Male\")\nplt.ylabel(\"Frequency\")\nplt.legend([\"Have not Disease\", \"Have Disease\"])","71fe18e0":"crosstabFbs = pd.crosstab(data[\"fbs\"], data[\"target\"])\ncrosstabFbs","b930c5a6":"crosstabFbs.plot(kind=\"bar\", figsize=(10, 8), color=\"cmyk\")\nplt.xticks(rotation=0)\nplt.xlabel(\"Fasting Blood Sugar < 120 : 0 | Fasting Blood Sugar > 120 : 1\")\nplt.ylabel(\"Frequency\")\nplt.legend([\"Have not Disease\", \"Have Disease\"])","503e31d5":"data.fbs.value_counts()","7c928ef7":"crosstabExang = pd.crosstab(data[\"exang\"], data[\"target\"])\ncrosstabExang","cf98538a":"crosstabExang.plot(kind=\"bar\", figsize=(10, 8), color=\"cmyk\")\nplt.xticks(rotation=0)\nplt.xlabel(\"Exercise Induced Angina 0 : No | 1 : Yes\")\nplt.ylabel(\"Frequency\")\nplt.legend([\"Have not Disease\", \"Have Disease\"])","e4b75396":"y = data.target.values #values convert values onto numpy array\nx_data = data.drop([\"target\"], axis=1) #except for target the other columns is our x data","a248a906":"print(y)","b520f3c7":"x = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data)).values","288e3e70":"x","bea00079":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)","913efa12":"#transpose matrices\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T","0770092b":"def initialize_weights_and_bias(dimension):\n    weight = np.full((dimension, 1), 0.01)\n    bias = 0.0\n    return weight,bias","2415dd2c":"np.full((10, 1), 0.01)","25d9d120":"def sigmoid(z):\n    y_head = 1\/(1 + np.exp(-z))\n    return y_head","2b611754":"def forward_backward_propagtion(weight, bias, x_train, y_train):\n    # forward propagation\n    z = np.dot(weight.T,x_train) + bias # z = b + w1.x1 + w2.x2 + .... + w302.x302\n    y_head = sigmoid(z)\n    loss = -(y_train*np.log(y_head) + (1-y_train)*np.log(1-y_head))\n    cost = np.sum(loss) \/ x_train.shape[1] #x_train.shape[1] for normalization\n    \n    # backward propagation\n    derivative_weight = np.dot(x_train,((y_head-y_train).T))\/x_train.shape[1] #simple derivative\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"Derivative Weight\" : derivative_weight, \"Derivative Bias\" : derivative_bias} #for storage\n     \n    return cost,gradients\n","a1752da6":"def update(weight, bias, x_train, y_train, learning_rate, iteration) :\n    cost_list = []\n    index = []\n    \n    # updating(learning) parameters in number_of_iterarion times\n    for i in range(iteration):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagtion(weight,bias,x_train,y_train)\n        # update\n        weight = weight - learning_rate * gradients[\"Derivative Weight\"]\n        bias = bias - learning_rate * gradients[\"Derivative Bias\"]\n        \n        cost_list.append(cost)\n        index.append(i)\n        print (\"Cost after iteration %i: %f\" %(i, cost))\n        \n    parameters = {\"weight\": weight,\"bias\": bias}\n    \n    print(\"iteration:\",iteration)\n    print(\"cost:\",cost)\n\n    plt.plot(index,cost_list)\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n\n    return parameters, gradients","c84319b6":"def predict(weight, bias, x_test):\n    # x_test is an input for forward propagation\n    z = np.dot(weight.T,x_test) + bias\n    y_head = sigmoid(z)\n\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0)\n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n    return y_prediction","1e15481e":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, iteration):\n    # initialize\n    dimension = x_train.shape[0]\n    weight,bias = initialize_weights_and_bias(dimension)\n     \n    parameters, gradients = update(weight, bias,x_train, y_train, learning_rate, iteration)\n\n    y_prediction = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n    print(\"Accuracy of Model : {}%\".format((100 - np.mean(np.abs(y_prediction - y_test))*100)))","d0c420a1":"logistic_regression(x_train,y_train,x_test,y_test,2,200)","1630fb84":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n\nlr.fit(x_train.T, y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T))) #","5763d8a7":"![sigmoid.png](attachment:f8a01b1c-2b33-494c-b5d8-ed02046e014d.png)","6d10e109":"## Read Data","1ed2754d":"#### Basically in neural networks (logistic regression is fundamental form of neural networks), you forward propagate to get the output and compare it with the real value to get the error. Now, to minimize the error, you propagate backwards by finding the derivative of error with respect to each weight and then subtracting this value from the weight value. The purpose of this calculations is optimize the algorithm. This operation can be done with gradient descent method.","19100577":"#### Lets define x and y values","1025ef1f":"#### As you see the the information we wanted, they are grouped","ac2a3df5":"#### For whom asking what is np.full : ","b3e0d33f":"#### But what about the numbers of the target ?","4fe43edb":"#### The first thing I wondered is dependecy between illness and age:","72f7a5f1":"### X_normalized = (x - x minimum)\/(x maximum - x minimum)","1d5de9e9":"#### First of all, we need to make normalization on data because in this data there are some values like 140, 250 (trestbs, chol) and there are binary values like 0 and 1. This may cause overtower between datas on features. To prevent this, we are doing normalization.","ee081972":"#### In this tutorial, first we do exploratory data analysis then we generate our logistic regression model by hand then tried it by sklearn","fef3c93d":"#### Lets continue to code :","5733fcfb":"#### According to table, men have more tendecy to have heart diseases.","4543e6a9":"# Heart Disease with Logistic Regression","7314ac53":"### Cordivacular diseases, in colloquial speech heart diseases can be big trouble for human kind. In this kernel we are going to try to make analysis about this illness by investigating different parameters.","2612790f":"#### This is the data types that what we will work on. 12 integers and 1 float and all of them is filled with non-null. We can double check like that also :","ddce245c":"![grad2.png](attachment:47016ac7-1ee3-4dda-bdad-a7e5b90dc3d9.png)","e60a41f4":"#### According to table, 41, 51, 52, 54 are the ages that heart diseases mostly seen.","c8685893":"#### Now its time to getting more familiar with the data :","4c3a129d":"#### This is loss function. Sumation for each value is cost function. If you predict 1 and the result is 1 you will get 0 loss otherwise big amount of lost.","62e24b46":"#### What about the difference between man and woman ?","f9ef5b60":"#### In this tutorial, I am going to write logistic regression code by myself basicaly. After that with sklearn.","a7a10c2b":"## Sklearn","f4d1dc7f":"## Visualization","038ac829":"### So, How Its Work ?","565ca405":"#### Another interesting result. Exercise induced angina is the problem that chest pain, relaxation and pressure hiss caused by ischemia or corner spasm in the heart muscle of the day. ","e6fc8593":"## Forward-Backward Propagation and Gradient Descent","3ab2323f":"#### This result actually suprised me because i expected more datas on fasting blood sugar > 1 but there are not much.","415f9769":"## Conclusion","28aa1b53":"### Sigmoid Function","45fc8578":"#### Initially we need to view a pandas function which is crosstab. Crosstab computes a simple cross tabulation of two (or more) factors. By default computes a frequency table of the factors unless an array of values and an aggregation function are passed. For example: \n","04227402":"#### Alright its time to split to train and test","f98662a1":"![heart.jpg](attachment:f8b66335-d09d-4f53-a455-28c08c263d5c.jpg)","c2e94709":"![Inkedweights and bias_LI.jpg](attachment:318d4528-bf75-490e-8596-b03256ed4367.jpg)","684eb33b":"#### Well, what are those things ?\n* Inputs (x1, x2, x3 ... xn) are our specific values on each column\n* Weights are coefficent \n* Bias is interception\n* Activation functions are mathematical functions like unit step function, sigmoid etc. We will use sigmoid\n* Basicaly its working based on very familiar expression y = wx + b\n* In this scenario , z = b + w1.x1 + w2.x2 + .... + w302.x302\n* y_pred is sigmoid(z) (will work on sigmoid a little later)","a54f05d6":"## First of all I want to have a look at the features meanings :\n####  * age       --> age\n#### * sex       --> gender in binary (1:male, 0:female)\n#### * cp        --> chest pain type\n#### * trestbps  --> resting blood pressure (in mm Hg on admission to the hospital)\n#### * chol      --> serum cholestoral in mg\/dl\n#### * fbs       --> (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n#### * restecg   --> resting electrocardiographic results\n#### * thalach   --> maximum heart rate achieved\n#### * exang     --> exercise induced angina (1 = yes; 0 = no)\n#### * oldpeak   --> ST depression induced by exercise relative to rest\n#### * slope     --> the slope of the peak exercise ST segment\n#### * ca        --> number of major vessels (0-3) colored by flourosopy\n#### * thal      --> 3 = normal; 6 = fixed defect; 7 = reversable defect\n#### * target    --> have disease or not (1=yes, 0=no)","998a11eb":"### Loss = (Y)(-log(Y_pred)) + (1-Y)(-log(1-Y_pred))","3de684db":"#### So whats next ? fbs : Fasting Blood Sugar","93c8679b":"## Logistic Regression","1605eb72":"![gradient descent.png](attachment:2578a93a-6185-4e3f-a155-996fa0dd2951.png)","fb72e90b":"#### Sigmoid function returns probabilistic values. For example at the point 2, nearly %75 probability this value classified as 1","5c8c515e":"#### By this method, you can have a look at what you want to dig in. In this senerio target must be our target :","8d644ee3":"### Initializing Weights and Bias","93fa977b":"#### The values must be defined. In initialize_weights_and_bias function I defined the initial values. This values are not very important because after forward and bacward propagations values will be updated.","ffd98f96":"#### This is corrolation table. By the helping of this table we can see that if features have corrolation or not (if values is close to 1 there is positive corrolation, if near to -1 negative corrolation, and if it near to 0 there are no corrolation). As you see there are not much corrolated features in our data. The map you will see in below is corrolation map:","daa17fe3":"#### This is updating equations. By taking derivative of cost function according to weight and bias. Then multiply it with \u03b1 learning rate. Step by step model will be updated by this method. Learning rate can be say like learning speed. In other words, Incremental step is stepping by the helping of learning rate."}}