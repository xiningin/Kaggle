{"cell_type":{"8fc8245a":"code","7145fbe3":"code","3a547359":"code","e60b7c67":"code","b036022a":"code","e4ef64cd":"code","70884872":"code","6b594c7f":"code","af1aee90":"code","c297ce5e":"code","09dab754":"code","8c361203":"code","27d79036":"code","d047c6c8":"code","a1c9c73e":"code","99e77476":"code","40de9fc0":"code","65b75d0a":"code","06c40932":"code","65a01c61":"code","585a2d00":"code","cdef907c":"code","1ab3e920":"code","c813f031":"code","b4576104":"code","7ca1c9a5":"code","04bed0ee":"code","fbff0f00":"code","8b77e5a6":"code","771911e7":"code","7b67e6aa":"code","a9bec62e":"code","50d89e7a":"code","ca701b4a":"code","1acb9089":"code","5046dced":"code","b9141872":"code","f3e43593":"code","df7ed02a":"code","c56b6d5c":"code","75fea0ec":"code","db22a22c":"code","248af452":"code","17ba6ebb":"code","0aec0216":"code","2f665355":"code","ae73a564":"code","99ed1c44":"code","6205320b":"code","885db0ff":"code","d45d70df":"code","cf877c02":"code","72aa0709":"code","0abfc469":"code","652909b9":"code","de314681":"code","0e177d77":"code","fa18bd9a":"markdown","4fe63a9c":"markdown","25efa418":"markdown","465f984f":"markdown","726ce897":"markdown","95c1a5ad":"markdown","e22ca3f9":"markdown","a8608e0e":"markdown","7592a88f":"markdown","910c7f5c":"markdown","a3bc01df":"markdown","90836fdb":"markdown","5fa687b8":"markdown","cdaee5b7":"markdown","ef0f3335":"markdown","839e36cb":"markdown","163e4c12":"markdown","b1027b12":"markdown","43bdd564":"markdown","88b21f37":"markdown","eb3074ee":"markdown","98bde915":"markdown","debabada":"markdown"},"source":{"8fc8245a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nplt.rc(\"font\", size=14)\nimport seaborn as sns\nsns.set(style=\"white\")   #white background style for seaborn plots\nsns.set(style=\"whitegrid\", color_codes=True)\nimport warnings\nwarnings.simplefilter(action='ignore')","7145fbe3":"# let's import the data\ndata = pd.read_csv('\/kaggle\/input\/insurance-claims\/insurance_claims.csv')\n\n# Spliting the dataframe in 9:1 for train and validation dataset. The validation portion will\n# be used for model validation\ntrain_df = data.sample(frac = .95, random_state=42)\ntest_df  = data.drop(train_df.index)\n\nprint('The number of samples into the train data is {}.'.format(train_df.shape[0]))\nprint('The number of samples into the test data is {}.'.format(test_df.shape[0]))","3a547359":"train_df.head()","e60b7c67":"## Observation : There are few columns which contains \"?\" as value, we need to replace it with null value for data inputation\ntrain_df = train_df.replace('?',np.NaN)","b036022a":"train_df.isnull().sum()","e4ef64cd":"# Delete _c39 columns. seems like it is junk\ntrain_df.drop(['_c39'], axis = 1,inplace=True)\ntest_df.drop(['_c39'], axis = 1,inplace=True)","70884872":"## print the missing value columns\ntrain_df[['collision_type','property_damage','police_report_available']].head()  ","6b594c7f":"## Fill collision_type with maximum repeatable value ( mode value)\nprint(train_df['collision_type'].mode())\ntrain_df['collision_type'].fillna((train_df['collision_type'].mode()[0]), inplace=True)\n\n## FIll property_damage with 'NO' as missing information implies that no property has been damaged.\ntrain_df['property_damage'].fillna('NO', inplace=True)\n\n## Fill police_report_available with NO value and missing information indicates that police has not been informed\ntrain_df['police_report_available'].fillna('NO', inplace=True)\n\n## print the missing value columns\ntrain_df[['collision_type','property_damage','police_report_available']].head()  \n","af1aee90":"## Verify if the dataframe have no more missing values\ntrain_df.isnull().sum()","c297ce5e":"print(train_df['fraud_reported'].value_counts())\nsns.countplot(x='fraud_reported', data=train_df, palette='Set2',hue = 'fraud_reported')\nplt.show()","09dab754":"# percentage of non fraud claim in train dataset\nprint('percentage of non fraud claim in train dataset is %.2f%%' %(((train_df['fraud_reported'] == 'N').sum()\/train_df.shape[0])*100))\n# percentage of fraud claim reported in train dataset\nprint('percentage of non fraud claim in train dataset is %.2f%%' %(((train_df['fraud_reported'] == 'Y').sum()\/train_df.shape[0])*100))\n","8c361203":"# let's describe the data\n# It will demonstrate the count, mean, std dev, min, max, etc values for the Numerical features present in the data.\ntrain_df.describe()","27d79036":"#check the correlation\ntrain_df.corr()","d047c6c8":"# plotting correlation heatmap\nf, ax = plt.subplots(figsize=(10, 5))\ndataplot=sns.heatmap(train_df.corr(),cmap=\"YlGnBu\", annot=False)","a1c9c73e":"# lets check the covriance\ntrain_df.cov()","99e77476":"f, ax = plt.subplots(figsize=(10, 5))\nplot = sns.countplot(x='insured_hobbies', data=train_df, palette='Set2', hue='fraud_reported')\nplot.set_xticklabels(plot.get_xticklabels(), rotation=75)     # for readibility\nplt.show()","40de9fc0":"f, ax = plt.subplots(figsize=(10, 5))\nplot = sns.countplot(x='insured_sex', data=train_df, palette='Set2', hue='fraud_reported')\nplot.set_xticklabels(plot.get_xticklabels(), rotation=0)     # for readibility\nplt.show()","65b75d0a":"f, ax = plt.subplots(figsize=(10, 5))\nplot = sns.countplot(x='insured_education_level', data=train_df, palette='Set2',hue='fraud_reported')\nplot.set_xticklabels(plot.get_xticklabels(), rotation=0)     # for readibility\nplt.show()","06c40932":"f, ax = plt.subplots(figsize=(10, 5))\nplot = sns.countplot(x='insured_occupation', data=train_df, palette='Set2',hue='fraud_reported')\nplot.set_xticklabels(plot.get_xticklabels(), rotation=75)     # for readibility\nplt.show()","65a01c61":"#Damage Analysis\nprint(train_df['incident_severity'].value_counts())","585a2d00":"import matplotlib.pyplot as plt\n  \nlabels = 'Minor Damage', 'Total Loss', 'Major Damage', 'Trivial Damage'\nsizes = [312, 258, 248, 82]\nexplode = (0, 0, 0, 0)\n  \nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode = explode,\n        labels = labels, autopct ='% 1.1f %%',\n        shadow = True, startangle = 90)\nax1.axis('equal')\n  \nax1.set_title('Damage Analysis')\nplt.show()","cdef907c":"#### Step 4.1 Drop the dates columns\ntrain_df.drop(['policy_bind_date','incident_date'],axis=1, inplace=True)","1ab3e920":"#### Step 4.2 Encode the fraud reported to numberical value\ntrain_df['fraud_reported'] = train_df['fraud_reported'].replace(('Y','N'),(1,0))","c813f031":"### Step 4.3 Correlation between policy_state and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['policy_state','fraud_reported']].groupby(['policy_state'],as_index=False).mean().sort_values(by = 'policy_state', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for policy_state\ntrain_df['policy_state'] = train_df['policy_state'].replace(a_dict)\n","b4576104":"### Step 4.4 Correlation between policy_state and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['policy_csl','fraud_reported']].groupby(['policy_csl'],as_index=False).mean().sort_values(by = 'policy_csl', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for policy_csl\ntrain_df['policy_csl'] = train_df['policy_csl'].replace(a_dict)","7ca1c9a5":"### Step 4.5 Correlation between policy_state and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['insured_sex','fraud_reported']].groupby(['insured_sex'],as_index=False).mean().sort_values(by = 'insured_sex', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for insured_sex\ntrain_df['insured_sex'] = train_df['insured_sex'].replace(a_dict)","04bed0ee":"### Step 4.6 Correlation between insured_education_level and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['insured_education_level','fraud_reported']].groupby(['insured_education_level'],as_index=False).mean().sort_values(by = 'insured_education_level', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for insured_education_level\ntrain_df['insured_education_level'] = train_df['insured_education_level'].replace(a_dict)","fbff0f00":"### Step 4.7 Correlation between insured_education_level and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['insured_occupation','fraud_reported']].groupby(['insured_occupation'],as_index=False).mean().sort_values(by = 'insured_occupation', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for insured_occupation\ntrain_df['insured_occupation'] = train_df['insured_occupation'].replace(a_dict)","8b77e5a6":"### Step 4.8 Correlation between insured_hobbies and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['insured_hobbies','fraud_reported']].groupby(['insured_hobbies'],as_index=False).mean().sort_values(by = 'insured_hobbies', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for insured_hobbies\ntrain_df['insured_hobbies'] = train_df['insured_hobbies'].replace(a_dict)","771911e7":"### Step 4.9 Correlation between insured_relationship and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['insured_relationship','fraud_reported']].groupby(['insured_relationship'],as_index=False).mean().sort_values(by = 'insured_relationship', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for insured_relationship\ntrain_df['insured_relationship'] = train_df['insured_relationship'].replace(a_dict)","7b67e6aa":"### Step 4.10 Correlation between incident_type and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['incident_type','fraud_reported']].groupby(['incident_type'],as_index=False).mean().sort_values(by = 'incident_type', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for incident_type\ntrain_df['incident_type'] = train_df['incident_type'].replace(a_dict)","a9bec62e":"### Step 4.11 Correlation between collision_type and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['collision_type','fraud_reported']].groupby(['collision_type'],as_index=False).mean().sort_values(by = 'collision_type', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for collision_type\ntrain_df['collision_type'] = train_df['collision_type'].replace(a_dict)","50d89e7a":"### Step 4.12 Correlation between incident_severity and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['incident_severity','fraud_reported']].groupby(['incident_severity'],as_index=False).mean().sort_values(by = 'incident_severity', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for incident_severity\ntrain_df['incident_severity'] = train_df['incident_severity'].replace(a_dict)","ca701b4a":"### Step 4.13 Correlation between authorities_contacted and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['authorities_contacted','fraud_reported']].groupby(['authorities_contacted'],as_index=False).mean().sort_values(by = 'authorities_contacted', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for authorities_contacted\ntrain_df['authorities_contacted'] = train_df['authorities_contacted'].replace(a_dict)","1acb9089":"# ### Step 4.14 Correlation between incident_state and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['incident_state','fraud_reported']].groupby(['incident_state'],as_index=False).mean().sort_values(by = 'incident_state', ascending = False)\na_dict = dict(temp_df.values)\n# #Lets perform target encoding for incident_state\ntrain_df['incident_state'] = train_df['incident_state'].replace(a_dict)","5046dced":"# ### Step 4.15 Correlation between incident_city and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['incident_city','fraud_reported']].groupby(['incident_city'],as_index=False).mean().sort_values(by = 'incident_city', ascending = False)\na_dict = dict(temp_df.values)\n# #Lets perform target encoding for incident_city\ntrain_df['incident_city'] = train_df['incident_city'].replace(a_dict)","b9141872":"# ### Step 4.16 Correlation between incident_location and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['incident_location','fraud_reported']].groupby(['incident_location'],as_index=False).mean().sort_values(by = 'incident_location', ascending = False)\na_dict = dict(temp_df.values)\n# #Lets perform target encoding for incident_location\ntrain_df['incident_location'] = train_df['incident_location'].replace(a_dict)","f3e43593":"# ### Step 4.17 Correlation between property_damage and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['property_damage','fraud_reported']].groupby(['property_damage'],as_index=False).mean().sort_values(by = 'property_damage', ascending = False)\na_dict = dict(temp_df.values)\n# #Lets perform target encoding for property_damage\ntrain_df['property_damage'] = train_df['property_damage'].replace(a_dict)","df7ed02a":"# ### Step 4.18 Correlation between police_report_available and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['police_report_available','fraud_reported']].groupby(['police_report_available'],as_index=False).mean().sort_values(by = 'police_report_available', ascending = False)\na_dict = dict(temp_df.values)\n# #Lets perform target encoding for police_report_available\ntrain_df['police_report_available'] = train_df['police_report_available'].replace(a_dict)","c56b6d5c":"# ### Step 4.19 Correlation between auto_make and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['auto_make','fraud_reported']].groupby(['auto_make'],as_index=False).mean().sort_values(by = 'auto_make', ascending = False)\na_dict = dict(temp_df.values)\n# #Lets perform target encoding for auto_make\ntrain_df['auto_make'] = train_df['auto_make'].replace(a_dict)","75fea0ec":"# ### Step 4.20 Correlation between auto_model and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['auto_model','fraud_reported']].groupby(['auto_model'],as_index=False).mean().sort_values(by = 'auto_model', ascending = False)\na_dict = dict(temp_df.values)\n# #Lets perform target encoding for auto_model\ntrain_df['auto_model'] = train_df['auto_model'].replace(a_dict)","db22a22c":"## Step 5 : Resolving data imbalance for target variable\n# percentage of non fraud claim in train dataset\nprint('percentage of non fraud claim in train dataset is %.2f%%' %(((train_df['fraud_reported'] == 0).sum()\/train_df.shape[0])*100))\n# percentage of fraud claim reported in train dataset\nprint('percentage of non fraud claim in train dataset is %.2f%%' %(((train_df['fraud_reported'] == 1).sum()\/train_df.shape[0])*100))","248af452":"train_df.shape[0]","17ba6ebb":"X = train_df.drop(['fraud_reported'], axis = 1)\ny = train_df['fraud_reported']\nprint(\"Shape of x :\", X.shape)\nprint(\"Shape of y :\", y.shape)","0aec0216":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, stratify=y, random_state=42)\nprint(\"Shape of X_train :\", X_train.shape)\nprint(\"Shape of X_test :\", X_test.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of y_test :\", y_test.shape)","2f665355":"import imblearn\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\ncounter = Counter(y_train)\nprint('Before Counter:', counter)\n#Oversampling the train dataset using SMOTE \nsmt = SMOTE(sampling_strategy='auto', k_neighbors=1, random_state= 42)\nX_train, y_train = smt.fit_resample(X_train, y_train)\ncounter = Counter(y_train)\nprint('After Counter:', counter)","ae73a564":"X_train.shape","99ed1c44":"X_test.shape","6205320b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nestimator = LogisticRegression(solver='liblinear')\nselector = RFE(estimator, n_features_to_select=15, step=1)\nselector = selector.fit(X_train, y_train)\n# summarize the selection of the attributes\nSelected_Feature = list(X_train.columns[selector.support_])\nprint('Selected features: %s' % Selected_Feature)\nprint('Feature ranking: %s' % selector.ranking_)","885db0ff":"# ## Feature ranking with recursive feature elimination and cross-validation\n# from sklearn.feature_selection import RFECV\n# # Create the RFE object and compute a cross-validated score.\n# # The \"accuracy\" scoring is proportional to the number of correct classifications\n# rfecv = RFECV(estimator,step=1,cv=10, scoring='accuracy')\n# rfecv.fit(X, y)\n\n# print(\"Optimal number of features: %d\" % rfecv.n_features_)\n# print('Selected features: %s' % list(X.columns[rfecv.support_]))\n\n# # Plot number of features VS. cross-validation scores\n# plt.figure(figsize=(10,6))\n# plt.xlabel(\"Number of features selected\")\n# plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n# plt.show()","d45d70df":"#Selected_Feature = ['months_as_customer', 'age', 'incident_severity', 'incident_location', 'incident_hour_of_the_day', 'number_of_vehicles_involved', 'bodily_injuries', 'total_claim_amount', 'injury_claim', 'property_claim']\nX_train = X_train[Selected_Feature]   ## include only selected feature\nX_test = X_test[Selected_Feature]     ## include only selected feature\nX_train.shape","cf877c02":"logreg = LogisticRegression(solver='liblinear')\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\nlogreg.fit(X_train,y_train)     # model training\ny_pred = logreg.predict(X_test) # model testing \n\nprint(\"Training Accuracy: \", logreg.score(X_train, y_train))\nprint('Testing Accuarcy: ', logreg.score(X_test, y_test))\n\n# making a classification report\ncr = classification_report(y_test,  y_pred)\nprint(cr)\n\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, columns=np.unique(y_test), index = np.unique(y_test))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (8,5))\nsns.set(font_scale=1.5)      #for label size\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16},fmt='d')    # font size","72aa0709":"## Observation : There are few columns which contains \"?\" as value, we need to replace it with null value for data inputation\ntest_df = test_df.replace('?',np.NaN)\n\n## Fill collision_type with maximum repeatable value ( mode value)\ntest_df['collision_type'].fillna((test_df['collision_type'].mode()[0]), inplace=True)\n\n## FIll property_damage with 'NO' as missing information implies that no property has been damaged.\ntest_df['property_damage'].fillna('NO', inplace=True)\n## Fill police_report_available with NO value and missing information indicates that police has not been informed\ntest_df['police_report_available'].fillna('NO', inplace=True)\n\n#### Step 4.1 Drop the dates columns\ntest_df.drop(['policy_bind_date','incident_date'],axis=1, inplace=True)\n\n#### Step 4.2 Encode the fraud reported to numberical value\ntest_df['fraud_reported'] = test_df['fraud_reported'].replace(('Y','N'),(1,0))\n\n### Step 4.3 Correlation between policy_state and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['policy_state','fraud_reported']].groupby(['policy_state'],as_index=False).mean().sort_values(by = 'policy_state', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for policy_state\ntest_df['policy_state'] = test_df['policy_state'].replace(a_dict)\n\n### Step 4.4 Correlation between policy_state and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['policy_csl','fraud_reported']].groupby(['policy_csl'],as_index=False).mean().sort_values(by = 'policy_csl', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for policy_csl\ntest_df['policy_csl'] = test_df['policy_csl'].replace(a_dict)\n\n### Step 4.5 Correlation between policy_state and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['insured_sex','fraud_reported']].groupby(['insured_sex'],as_index=False).mean().sort_values(by = 'insured_sex', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for insured_sex\ntest_df['insured_sex'] = test_df['insured_sex'].replace(a_dict)\n\n### Step 4.6 Correlation between insured_education_level and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['insured_education_level','fraud_reported']].groupby(['insured_education_level'],as_index=False).mean().sort_values(by = 'insured_education_level', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for insured_education_level\ntest_df['insured_education_level'] = test_df['insured_education_level'].replace(a_dict)\n\n### Step 4.7 Correlation between insured_education_level and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['insured_occupation','fraud_reported']].groupby(['insured_occupation'],as_index=False).mean().sort_values(by = 'insured_occupation', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for insured_occupation\ntest_df['insured_occupation'] = test_df['insured_occupation'].replace(a_dict)\n\n### Step 4.8 Correlation between insured_hobbies and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['insured_hobbies','fraud_reported']].groupby(['insured_hobbies'],as_index=False).mean().sort_values(by = 'insured_hobbies', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for insured_hobbies\ntest_df['insured_hobbies'] = test_df['insured_hobbies'].replace(a_dict)\n\n### Step 4.9 Correlation between insured_relationship and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['insured_relationship','fraud_reported']].groupby(['insured_relationship'],as_index=False).mean().sort_values(by = 'insured_relationship', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for insured_relationship\ntest_df['insured_relationship'] = test_df['insured_relationship'].replace(a_dict)\n\n### Step 4.10 Correlation between incident_type and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['incident_type','fraud_reported']].groupby(['incident_type'],as_index=False).mean().sort_values(by = 'incident_type', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for incident_type\ntest_df['incident_type'] = test_df['incident_type'].replace(a_dict)\n\n### Step 4.11 Correlation between collision_type and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['collision_type','fraud_reported']].groupby(['collision_type'],as_index=False).mean().sort_values(by = 'collision_type', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for collision_type\ntest_df['collision_type'] = test_df['collision_type'].replace(a_dict)\n\n### Step 4.12 Correlation between incident_severity and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['incident_severity','fraud_reported']].groupby(['incident_severity'],as_index=False).mean().sort_values(by = 'incident_severity', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for incident_severity\ntest_df['incident_severity'] = test_df['incident_severity'].replace(a_dict)\n\n### Step 4.13 Correlation between authorities_contacted and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['authorities_contacted','fraud_reported']].groupby(['authorities_contacted'],as_index=False).mean().sort_values(by = 'authorities_contacted', ascending = False)\na_dict = dict(temp_df.values)\n#Lets perform target encoding for authorities_contacted\ntest_df['authorities_contacted'] = test_df['authorities_contacted'].replace(a_dict)\n\n# ### Step 4.14 Correlation between incident_state and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['incident_state','fraud_reported']].groupby(['incident_state'],as_index=False).mean().sort_values(by = 'incident_state', ascending = False)\na_dict = dict(temp_df.values)\n# #Lets perform target encoding for incident_state\ntest_df['incident_state'] = test_df['incident_state'].replace(a_dict)\n\n# ### Step 4.15 Correlation between incident_city and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['incident_city','fraud_reported']].groupby(['incident_city'],as_index=False).mean().sort_values(by = 'incident_city', ascending = False)\na_dict = dict(temp_df.values)\n# #Lets perform target encoding for incident_city\ntest_df['incident_city'] = test_df['incident_city'].replace(a_dict)\n\n# ### Step 4.16 Correlation between incident_location and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['incident_location','fraud_reported']].groupby(['incident_location'],as_index=False).mean().sort_values(by = 'incident_location', ascending = False)\na_dict = dict(temp_df.values)\n# #Lets perform target encoding for incident_location\ntest_df['incident_location'] = test_df['incident_location'].replace(a_dict)\n\n# ### Step 4.17 Correlation between property_damage and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['property_damage','fraud_reported']].groupby(['property_damage'],as_index=False).mean().sort_values(by = 'property_damage', ascending = False)\na_dict = dict(temp_df.values)\n# #Lets perform target encoding for property_damage\ntest_df['property_damage'] = test_df['property_damage'].replace(a_dict)\n\n# ### Step 4.18 Correlation between police_report_available and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['police_report_available','fraud_reported']].groupby(['police_report_available'],as_index=False).mean().sort_values(by = 'police_report_available', ascending = False)\na_dict = dict(temp_df.values)\n# #Lets perform target encoding for police_report_available\ntest_df['police_report_available'] = test_df['police_report_available'].replace(a_dict)\n\n# ### Step 4.19 Correlation between auto_make and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['auto_make','fraud_reported']].groupby(['auto_make'],as_index=False).mean().sort_values(by = 'auto_make', ascending = False)\na_dict = dict(temp_df.values)\n# #Lets perform target encoding for auto_make\ntest_df['auto_make'] = test_df['auto_make'].replace(a_dict)\n\n# ### Step 4.19 Correlation between auto_model and target variable, perfrom target encoding using correlation factor. \ntemp_df = test_df[['auto_model','fraud_reported']].groupby(['auto_model'],as_index=False).mean().sort_values(by = 'auto_model', ascending = False)\na_dict = dict(temp_df.values)\n# #Lets perform target encoding for auto_model\ntest_df['auto_model'] = test_df['auto_model'].replace(a_dict)\n\n","0abfc469":"# performing predictions on the test dataset\ntest_df1 = test_df[Selected_Feature]\ntest_df1.head()","652909b9":"# performing predictions on the test dataset\ntest_pred = logreg.predict(test_df1)\ntest_pred","de314681":"## Lets validate against actual value in test dataset\ntest_df['fraud_reported'].values","0e177d77":"print('Testing Accuarcy: ', logreg.score(test_df1, test_df['fraud_reported']))","fa18bd9a":"**Step 6.3 Model evaluation using LogisticRegression**","4fe63a9c":"**Step 6 : Split the training dataset into dependent and independent variable**","25efa418":"**Step 3 : Exploratory data analysis**","465f984f":"**Problem Statement : Using machine learning develop a classification model to predict which claims are likely to be fraudulent for an insurace company. This information can narrow down the list of claims that need a further check. It enables an insurer to detect more fraudulent claims.**","726ce897":"**Step 3 : Data quality and missing value assessment**","95c1a5ad":"Observation : insured customer having education level JD and High School have high tendency of fraud.","e22ca3f9":"**Step 1 : Import the ML packages**","a8608e0e":"Note: It looks like there are high correlation \"months as customer\" and customer \"age\" and Probably because drivers buy auto insurance when they own a car and this time measure only increases with age.Apart from that, there don\u2019t seem to be many correlations in the data. There don\u2019t seem to be multicollinearity problems except maybe that all the claims are all correlated, and somehow total claims have accounted for them.","7592a88f":"**Step 7 : Testing on validation dataset**","910c7f5c":"Male and Female have almost equal tendency of fraud","a3bc01df":"**Step 6.2 : Feature selection using recursive feature elimination (RFE) and cross-validation**","90836fdb":"**Step 3.1 Dependent variable**","5fa687b8":"Observation : Looks like people having exec-managerial occupation have very high tendency of fraud","cdaee5b7":"Observation: It seems like chess players and crossfitters have higher tendencies to fraud.","ef0f3335":"**Step 4 : Data Cleanup**","839e36cb":"**Step 3.2 Descriptive Statistics**","163e4c12":"Difference between covarience and correlation\nCovariance signifies the direction of the linear relationship between the two variables. By direction we mean if the variables are directly proportional or inversely proportional to each other. (Increasing the value of one variable might have a positive or a negative impact on the value of the other variable) \n\nCorrelation Correlation analysis is a method of statistical evaluation used to study the strength of a relationship between two, numerically measured, continuous variables.","b1027b12":"### Step 6.2 Oversampling the minority class using smote test","43bdd564":"**Step 2: Import claim data**","88b21f37":"**Step 6.1 : Feature selection using recursive feature elimination (RFE)**","eb3074ee":"### Step 6.1 : Split the training dataset in train and test","98bde915":"**Step 3.3 Independent variable visualization**","debabada":"**Conclusion : The model is correctly predicting on validation dataset with 100% accuracy. It could be because I am using a fraction of train data set as validation data as well. "}}