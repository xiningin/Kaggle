{"cell_type":{"aedf6072":"code","0de5c149":"code","56bf7ca6":"code","6b88bf6f":"code","14e0ceba":"code","79b87bfc":"code","4492c9c3":"code","670640bb":"code","3f277352":"code","5d3e07ef":"markdown","b11c14da":"markdown","b8d77173":"markdown","5d08763d":"markdown","ecaf6eaf":"markdown","48819675":"markdown","d5ce4d01":"markdown"},"source":{"aedf6072":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0de5c149":"# Common imports\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom scipy.cluster import hierarchy\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nfrom scipy.spatial.distance import pdist","56bf7ca6":"wine = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\nwine.columns = ['fix_acid', 'volat_acid', 'citric_acid', 'resid_sugar',\n                'chlorides', 'free_sulf_diox', 'tot_sulf_diox', 'density',\n                'pH', 'sulphates', 'alcohol', 'quality']","6b88bf6f":"Histogram of features","14e0ceba":"wine.hist(bins=50, figsize=(20, 15))","79b87bfc":"corr_matrix = wine.corr()\ncorr_matrix['quality'].sort_values(ascending=False)","4492c9c3":"# Split into features and target variables\nX = wine.iloc[:, 0:11]\ny = wine.iloc[:, 11]\nfeatures = X.columns\n\n# Split into training and testing by stratifying using the target variable ('quality')\n# Necessary because of very few low and high quality samples\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Testing scaling options\n# # Normalizing lognormal data: option set to also standardize to 0 mean and unit sd\n# # https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html\npowertrans = PowerTransformer(method='yeo-johnson', standardize=True)\nX_test = powertrans.fit_transform(X_trn)\n\n# Apply initial PCA to explore components\npca = PCA(n_components=11)\npca.fit(X_test)\npca_evr = pca.explained_variance_ratio_  # The first two components explain most of the variance.\npca_ev = pca.explained_variance_\n\n# Plot cumulative sums of explained variance to decide on number of components\nplt.subplot(121)\nplt.plot(np.concatenate(([0], np.cumsum(pca_evr))))\nplt.subplot(122)\nplt.plot(np.concatenate(([0], np.cumsum(pca_ev))))\n\n# Making a Pipeline for the preprocessing\n# We use 6 components for PCA to keep about 80% of the explained variance.\n\n# Only scaling pipeline\ns_pipeline = Pipeline([\n        ('scaler', PowerTransformer(method='yeo-johnson', standardize=True)),\n    ])\n\n# Only PCA pipeline\np_pipeline = Pipeline([\n        ('pca', PCA(n_components=6)),\n    ])\n\n# Scaling a PCA pipeline\nsp_pipeline = Pipeline([\n        ('scaler', PowerTransformer(method='yeo-johnson', standardize=True)),\n        ('pca', PCA(n_components=6))\n    ])\n\n# Fit and transform on train data\nX_trn_s = s_pipeline.fit_transform(X_trn)\nX_trn_p = p_pipeline.fit_transform(X_trn)\nX_trn_sp = sp_pipeline.fit_transform(X_trn)\n# Transform only on test data\nX_tst_s = s_pipeline.transform(X_tst)\nX_tst_p = p_pipeline.transform(X_tst)\nX_tst_sp = sp_pipeline.transform(X_tst)\n\nX_list = list(zip([\"X_trn\", \"X_trn_s\", \"X_trn_sp\"], [X_trn, X_trn_s, X_trn_sp]))\n\n# Look at distributions after scaling\npd.DataFrame(X_trn_s).hist(bins=50, figsize=(20, 15))\n","670640bb":"log_reg = LogisticRegression(tol=0.0001, C=0.1, solver='newton-cg')\nfor names, X_pp in X_list:\n  log_reg.fit(X_pp, y_trn)\n  log_score = log_reg.score(X_pp, y_trn)\n  log_cross_scores = np.mean(cross_val_score(log_reg, X_pp, y_trn, cv=7))\n  print(names, log_score, log_cross_scores)\n\n# Using best X dataset\nlog_reg.fit(X_trn_s, y_trn)\n\nbest_logreg_clf = log_reg","3f277352":"for i in range(1,9):\n  for names, X_pp in X_list:\n    neigh = KNeighborsClassifier(n_neighbors=i)\n    neigh.fit(X_pp, y_trn)\n    knn_score = neigh.score(X_pp, y_trn)\n    knn_cross_scores = np.mean(cross_val_score(neigh, X_pp, y_trn, cv=7))\n    print(names, i, knn_score, knn_cross_scores)\n\nparam_grid = [{'n_neighbors': [2,3,4,5,6,7,8], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}]\nneigh = KNeighborsClassifier()\ngrid_search = GridSearchCV(neigh, param_grid, cv=7, return_train_score=True)\ngrid_search.fit(X_trn_s, y_trn)\n\ngrid_search.best_params_\n\n# KNN with best parameters from grid search: K=5 kein PCA\nneigh_gs = KNeighborsClassifier(n_neighbors=5)\nneigh_gs.fit(X_trn_s, y_trn)\nknn_score = neigh_gs.score(X_trn_s, y_trn)\nknn_cross_scores = np.mean(cross_val_score(neigh_gs, X_trn_s, y_trn, cv=7))\nprint(knn_score,knn_cross_scores)\n\nbest_knn_clf = neigh_gs","5d3e07ef":"Create a scatter matrix to check for correlations between variables\nRemoved weaker correlations variables: 'resid_sugar', 'chlorides', 'free_sulf_diox', 'tot_sulf_diox', 'quality'\n","b11c14da":"Wine quality predictions with machine learning","b8d77173":"Correlations with Quality","5d08763d":"Some useful functions","ecaf6eaf":"Data Preparation","48819675":"K Nearest Neighbor","d5ce4d01":"Logistic regression "}}