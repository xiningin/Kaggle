{"cell_type":{"5527d3d8":"code","f7802140":"code","59ad6452":"code","a131ffa6":"code","1943cedc":"code","58ab2dae":"code","8ec2a110":"code","4e18c46d":"code","aefc314e":"code","5d0cf84d":"code","611b7fcd":"code","af616251":"code","bf70d532":"code","0068b324":"code","ee72d418":"code","cde1c432":"code","2eadba33":"code","203f52b0":"code","68b5bc10":"code","903d3569":"code","f7aaf2f8":"code","bc4d0966":"code","2aa5c2fc":"code","9369de6d":"code","6ceb1437":"code","f3c85922":"code","7c7b570f":"code","ec82f5af":"markdown"},"source":{"5527d3d8":"!pip install spacy\n!pip install spacy_langdetect\n!pip install matplotlib\n!pip install numpy\n!pip install pandas\n!pip install scispacy\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz","f7802140":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import register_matplotlib_converters\nimport spacy\nfrom spacy_langdetect import LanguageDetector\nimport re","59ad6452":"cdc = pd.read_excel('..\/input\/cdc-covid19-research-articles\/All_Articles_Excel.xlsx', parse_dates=True)","a131ffa6":"# Examining all the unnamed columns\ncdc.iloc[:,16:].dropna(how='all')","1943cedc":"# Mostly nulls, html tags, and foreign language data, so drop\ncdc = cdc.iloc[:,:16]\ncdc.columns","58ab2dae":"# Plot the dates for all entries\nregister_matplotlib_converters()\nplt.figure(figsize=(15,8))\nplt.hist(cdc['Date Added'], bins=20)\nplt.title('Histogram of Number of Articles Published by Date')\nplt.xticks(rotation=90)\nplt.show()","8ec2a110":"# Abstracts with integer data types\n# Columns from Title to Keywords are erronously shifted\ncdc[cdc['Abstract'].apply(lambda x: isinstance(x,int))]","4e18c46d":"# Shifted data\nshifted_columns = ['Abstract', 'Year',\n       'Journal\/Publisher', 'Volume', 'Issue', 'Pages', 'Accession Number',\n       'DOI', 'URL', 'Name of Database', 'Database Provider', 'Language',\n       'Keywords']\nshifted_data = cdc[cdc['Abstract'].apply(lambda x: isinstance(x,int))].shift(1, axis=1)[shifted_columns]\n# Correcting shifted data\ncdc.loc[cdc['Abstract'].apply(lambda x: isinstance(x,int)), shifted_columns] = shifted_data","aefc314e":"cord = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv', parse_dates=True, low_memory=False)\ncord.info()","5d0cf84d":"cord[cord.title.isna()].abstract","611b7fcd":"# Remove records with missing titles\ncord = cord[cord.title.notna()]","af616251":"len(cord[cord.abstract.isna()])","bf70d532":"import en_core_sci_lg\nnlp = en_core_sci_lg.load()\nnlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\ndef lang_detect_en(text):\n    doc = nlp(text)\n    if doc._.language.get('language') == 'en':\n        return doc._.language.get('score')\n    else:\n        return 0","0068b324":"cord['lang_detect'] = cord['title'].apply(lang_detect_en)","ee72d418":"# Remove records with missing titles\ncdc = cdc[cdc.Title.notna()]\ncdc['lang_detect'] = cdc['Title'].apply(lang_detect_en)","cde1c432":"cord = cord[cord['lang_detect'] != 0]\ncdc = cdc[cdc['lang_detect'] != 0]","2eadba33":"interested_columns = ['title','authors','abstract','publish_time','url']\ninterested_columns_cdc = ['Title','Author','Abstract','Year','URL']","203f52b0":"cord_subset = cord[interested_columns].copy()\ncdc_subset = cdc[interested_columns_cdc].copy()","68b5bc10":"cdc_subset.columns = interested_columns","903d3569":"combined_dataset = cord_subset.append(cdc_subset).reset_index(drop=True)","f7aaf2f8":"len(combined_dataset)","bc4d0966":"combined_dataset['combined_text'] = combined_dataset.title + str(combined_dataset.abstract)\ncombined_dataset['combined_text'] = combined_dataset['combined_text'].str.lower()\ncombined_dataset['combined_text'] = combined_dataset.combined_text.apply(lambda x: re.sub(r'[\\W_]+', ' ', x))","2aa5c2fc":"for text in combined_dataset.loc[combined_dataset.duplicated(subset=['combined_text']), 'combined_text']:\n  dup_list = combined_dataset.loc[combined_dataset.combined_text == text,'url']\n  #print(len(dup_list), len(dup_list.notna()), len(dup_list.isna()))\n  if dup_list.notna().sum() == 0:\n    combined_dataset.drop(dup_list[1:].index, inplace=True)\n  else:\n    combined_dataset.drop(dup_list[dup_list.notna()][1:].index, inplace=True)\n    combined_dataset.drop(dup_list[dup_list.isna()].index, inplace=True)\ncombined_dataset = combined_dataset.reset_index(drop=True)\n    ","9369de6d":"len(combined_dataset)","6ceb1437":"combined_dataset = combined_dataset[combined_dataset.abstract.notna()].reset_index(drop=True)","f3c85922":"len(combined_dataset)","7c7b570f":"combined_dataset.to_csv('combined_dataset.csv')","ec82f5af":"## This notebook details the process to combine the CORD-19 dataset with updated articles released by the [CDC](https:\/\/www.cdc.gov\/library\/researchguides\/2019novelcoronavirus\/researcharticles.html) and remove records with missing data as well as duplicates"}}