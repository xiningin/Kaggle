{"cell_type":{"74f8bb08":"code","61420a6e":"code","7cb08cdd":"code","feab0197":"code","5f1e2772":"code","cb5d787a":"code","9c0b6922":"code","d734c5f9":"code","1ffb2ae2":"code","7fff6b39":"code","23046b42":"code","9eb110e9":"code","16b63867":"code","63c21baf":"code","f064a124":"code","7884bb25":"code","764bbe5b":"code","3575c41b":"code","ef0d4559":"code","e283a032":"code","ae7c80f4":"code","4fb24e5f":"code","fe6fe57a":"code","58886b68":"code","e11151cd":"code","9c586b31":"code","e2984195":"code","7d1135e8":"code","70f6da22":"code","71984c48":"code","3c7f7432":"code","fac80961":"code","7725d619":"code","fe1c1810":"code","b5c069b1":"code","f277795a":"code","dc068a32":"code","2d69fa08":"code","c84bbdca":"markdown"},"source":{"74f8bb08":"# The pre-processing idea has been benefited greatly from\n# https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/multivariate-multi-step-time-series-forecasting-using-stacked-lstm-sequence-to-sequence-autoencoder-in-tensorflow-2-0-keras\/\n# https:\/\/aeturrell.github.io\/coding-for-economists\/time-series.html\n# https:\/\/coderedirect.com\/questions\/673308\/interpolate-pandas-df\n# https:\/\/www.kaggle.com\/limweixuan1994\/store-sales-predictions\n# and many more~\n## a huge shout-out to them\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OrdinalEncoder\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\n#from tensorflow.keras.engine.input_layer import Input\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import BatchNormalization\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport warnings\nwarnings.filterwarnings('ignore')","61420a6e":"data_path ='..\/input\/store-sales-time-series-forecasting\/'\n\noil= pd.read_csv(os.path.join(data_path,'oil.csv'),index_col='date')\nholidays_events=pd.read_csv(os.path.join(data_path,'holidays_events.csv'),index_col='date')\nstores= pd.read_csv(os.path.join(data_path,'stores.csv'))\ntransactions= pd.read_csv(os.path.join(data_path,'transactions.csv'))\n\ntrain= pd.read_csv(os.path.join(data_path,'train.csv'), index_col='id',parse_dates=['date'], infer_datetime_format=True)\ntest= pd.read_csv(os.path.join(data_path,'test.csv'),parse_dates=['date'], infer_datetime_format=True)\n","7cb08cdd":"# # Function that outputs some of the data characteristics\n# def get_charas(df):\n#     print(\"\\n > head <\")\n#     print(df.head())\n#     print(\"\\n> info <\")\n#     print(df.info())\n#     print(\"\\n> describe <\")\n#     print(df.describe())\n#     print(\"\\n> cols <\")\n#     print(df.columns)\n#     print(\"\\n> dtypes <\")\n#     print(df.dtypes)\n#     print(\"\\n> null <\")\n#     print(df.isnull().sum())\n#     print(\"\\n> n\/a values <\")\n#     print(df.isna().sum())\n#     print(\"\\n> Shape Of Data <\")\n#     print(df.shape)\n#     return ","feab0197":"# get_charas(oil) ## there is missing data using interpolation strategy as needed\n\n# # fill in missing date, offset alias \"D\" means calendar day frequency\n# oil = oil.set_index(\"date\").asfreq(freq = \"D\")\n\n# # fill the NaN value by interpolation\n# oil[\"dcoilwtico\"] = oil[\"dcoilwtico\"].interpolate(limit_direction=\"both\")","5f1e2772":"# stores.head(n=8)\n# # stores.shape","cb5d787a":"train[\"family\"].nunique(dropna = True) \n#nunique() function return number of unique elements in the object, it will drop the N\/A","9c0b6922":"test.head()","d734c5f9":"# onpromotion won't be used, since based on:\n# https:\/\/www.kaggle.com\/limweixuan1994\/store-sales-predictions?scriptVersionId=80578071&cellId=9\n# it is not that useful\ntrain_data = train.copy().drop(['onpromotion'], axis=1)\ntest_data = test.copy().drop(['onpromotion'], axis=1)","1ffb2ae2":"##applying the encoders, the data showed that there are ~33 unique elements\nordinal_encoder = OrdinalEncoder(dtype=int)\ntrain_data[['family']] = ordinal_encoder.fit_transform(train_data[['family']])\ntest_data[['family']] = ordinal_encoder.transform(test_data[['family']])","7fff6b39":"train_data","23046b42":"##counting the number of days\nn_o_days_train=train[\"date\"].nunique(dropna = False) \nprint('number of day train:',n_o_days_train)\n\n# number of store\nn_o_stores_train=train[\"store_nbr\"].nunique(dropna = False) \nprint('number of stores train:',n_o_stores_train)\n\n# number of family\nn_o_families_train=train[\"family\"].nunique(dropna = False) \nprint('number of family\/type of prod train:',n_o_families_train)","9eb110e9":"##counting the number of days\nn_o_days_test=test[\"date\"].nunique(dropna = False) \nprint('number of day test:',n_o_days_test)\n\n# number of store\nn_o_stores_test=test[\"store_nbr\"].nunique(dropna = False) \nprint('number of stores test:',n_o_stores_test)\n\n# number of family\nn_o_families_test=test[\"family\"].nunique(dropna = False) \nprint('number of family\/type of prod test:',n_o_families_test)","16b63867":"# The data need to be re-organized as discrete-time data (days)\n# date as timestamp\/time-series input, store number and family as columns and sales is the numerical data of interest for RNN\npivoted_train = train_data.pivot(index=['date'], columns=['store_nbr', 'family'], values='sales')\npivoted_train","63c21baf":"# pivoted_train[1][0] #store number 1, product no 0","f064a124":"###Train_val data split#############\ntrain_samples = int(n_o_days_train * 0.95) ## percentage of traindata (vs validation)\n# train_samples","7884bb25":"train_samples_df = pivoted_train[:train_samples]\ntrain_samples_df","764bbe5b":"valid_samples_df = pivoted_train[train_samples:]\nvalid_samples_df","3575c41b":"minmax_scaler = MinMaxScaler()\nminmax_scaler.fit(train_samples_df)\n\nscaled_train_samples = minmax_scaler.transform(train_samples_df)\n# print(scaled_train_samples)\n# scaled_train_samples\nscaled_validation_samples = minmax_scaler.transform(valid_samples_df)","ef0d4559":"# sliding window for converting series to sample to be used with supervised learning algorithms\n# thanks to\n# https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/multivariate-multi-step-time-series-forecasting-using-stacked-lstm-sequence-to-sequence-autoencoder-in-tensorflow-2-0-keras\/\n\ndef split_series(series, n_past, n_future):\n  #\n  # n_past ==> no of past observations\n  #\n  # n_future ==> no of future observations \n  #\n  X, y = list(), list()\n  for window_start in range(len(series)):\n    past_end = window_start + n_past\n    future_end = past_end + n_future\n    if future_end > len(series):\n      break\n    # slicing the past and future parts of the window\n    past, future = series[window_start:past_end, :], series[past_end:future_end, :]\n    X.append(past)\n    y.append(future)\n  return np.array(X), np.array(y)\n\nn_past =16\nn_future = 16\nn_features = n_o_stores_train * n_o_families_train # num of features","e283a032":"#Now converting the data via split_series function\nX_train, y_train = split_series(scaled_train_samples,n_past, n_future)\nX_val, y_val = split_series(scaled_validation_samples,n_past, n_future)","ae7c80f4":"print('X_train.shape',X_train.shape)\nprint('y_train.shape',y_train.shape)\nprint('X_val.shape',X_val.shape)\nprint('y_val.shape',y_val.shape)","4fb24e5f":"keras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\ndef timemodel():\n    model = keras.Sequential()\n### basic RNN model\n    model.add(layers.SimpleRNN(units=200, return_sequences=True, input_shape=[n_past, n_features]))\n    model.add(keras.layers.BatchNormalization())\n    model.add(layers.Dropout(0.2))\n    model.add(layers.SimpleRNN(units=200, return_sequences=True))\n    model.add(keras.layers.BatchNormalization())\n    model.add(layers.Dropout(0.2))\n\n    \n### LSTM\n#     model.add(layers.LSTM(units=256, return_sequences=True, input_shape=[n_past, n_features]))\n#     model.add(keras.layers.BatchNormalization())\n#     model.add(layers.Dropout(0.2))\n#     model.add(layers.LSTM(units=128, return_sequences=True))\n#     model.add(keras.layers.BatchNormalization())\n#     model.add(layers.Dropout(0.2))\n\n    model.add(keras.layers.TimeDistributed(keras.layers.Dense(n_features)))\n    \n    \n    model.compile(loss=\"mae\", optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])\n    return model","fe6fe57a":"model=timemodel()\nmodel.summary()","58886b68":"early_stopping = keras.callbacks.EarlyStopping(monitor = 'val_mae',\n                                               min_delta=0.0001,\n                                               patience=100, \n                                               restore_best_weights=True)\n\n# What it means is, monitor validation loss, if the change in loss is less than 0.0001 for 20 epochs, then stop training. \n# Additionally, it returns the best epoch weights\n\nEPOCHS = 1000\n#EPOCHS = 1\nmodel_history = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val), \n    epochs=EPOCHS, \n    callbacks=[early_stopping], \n    batch_size=512, \n    shuffle=True)","e11151cd":"import matplotlib.pyplot as mpl\nmpl.plot(model_history.history['loss'])\nmpl.plot(model_history.history['val_mae'])\nmpl.xlabel('Epochs')\nmpl.ylabel('Loss')\nmpl.legend(['Train', 'Validation'])\nmpl.show()","9c586b31":"# pd.set_option('display.max_columns', 1000)\n\n#reset option to default value\n# pd.reset_option('display.max_rows')\n# pd.reset_option('display.max_columns')\n\n#and reset all of them back:\n# pd.reset_option('all')","e2984195":"x_test_pred = scaled_validation_samples[-n_past:,:].reshape((1, n_past, n_features))\nprint(x_test_pred.shape)\nscaled_y_predict = model.predict(x_test_pred)","7d1135e8":"scaled_y_predict.shape","70f6da22":"# Inverse transform from the previous min max scaler\ny_predict = pd.DataFrame(minmax_scaler.inverse_transform(scaled_y_predict.reshape((n_future, n_features))),\n                         columns=valid_samples_df.columns)\n## each element corresponding to raw value at a specific ID no on pivoted test table \ny_predict","71984c48":"# test_data","3c7f7432":"pivoted_test = test_data.pivot(index=['date'], columns=['store_nbr', 'family'], values=None)\npivoted_test ## format store_nbr, family, date and each value\/element corresponding to the sample indices","fac80961":"# pivoted_test.values","7725d619":"submission = pd.read_csv(\"..\/input\/store-sales-time-series-forecasting\/sample_submission.csv\", index_col='id')","fe1c1810":"submission.shape","b5c069b1":"# for i, j in y_predict.iterrows():\n#     print('i',i)\n#     print('j',j)","f277795a":"## mapping ypredict to pivoted test data\nfor day_ith, day_ith_pred in y_predict.iterrows():\n    #day_ith iteration, 16 days in totals\n    #day_ith_pred, predicted data of 9 stores, 33 classes of good for each day\n    #Iterate over DataFrame rows as (index, Series) pairs.\n#     print(n_samples_per_day)\n    # n_samples_per_day number of \n    for n_samples_per_day in range(len(day_ith_pred)): ## iterating the number of sample, from 0 to 1781, for 16 days\n#         print(pivoted_test.iloc[[day_ith], [n_samples_per_day]])\n        sample_id = pivoted_test.iloc[[day_ith], [n_samples_per_day]].values[0][0] #total number of samples\n        values= max(0,day_ith_pred.values[n_samples_per_day]) #price that is negative will be set to 0\n        submission.at[sample_id, 'sales'] = values","dc068a32":"submission","2d69fa08":"submission.to_csv('submission.csv')","c84bbdca":"# **Train_Valid_split**"}}