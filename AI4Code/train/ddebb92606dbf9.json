{"cell_type":{"cad6573f":"code","bca31982":"code","44adc727":"code","999773fd":"code","ac55237c":"code","e8966a6c":"code","616f7512":"code","0f2891c0":"code","4cd4745a":"code","d107087a":"code","6bfc6dfd":"code","2bdeca3e":"code","bcc088a0":"code","4f0e3ac2":"code","b3fd3c36":"code","613824dd":"code","0d965d3e":"code","477a3e73":"code","5d6c8d94":"code","942e0161":"code","6017e35a":"code","88733315":"code","1e536b76":"code","f21afb7f":"code","20531121":"code","507ac1c6":"code","13e75e9b":"code","fd9f48ae":"code","c79efce2":"code","b73326ec":"code","06c8c7d0":"code","6301bd9d":"code","c62a0c06":"code","58cac011":"code","5f2d02d2":"code","42fdcdf2":"code","ac4134bb":"code","3e397dad":"code","b9e75ff2":"code","5fa4748a":"code","ab74a3d6":"code","e4cd6ac2":"code","557f1245":"code","41a2cfad":"code","21b4a381":"code","e7a1ac1e":"code","fc96e1db":"code","22d49a0e":"code","5f7d8d4a":"code","2c779768":"code","775bb379":"code","49e3632a":"code","8c49d9c6":"code","3bec6eab":"code","0c726dc3":"code","f62e8737":"code","b7fdc250":"code","76cc2898":"code","71234829":"code","d8bcb5e8":"code","c49a99c3":"code","0de546ea":"code","58004653":"code","ff0a6127":"code","b342b692":"code","0bfa72cb":"code","d0787e84":"code","6e1969c2":"code","e370df86":"code","cf9b71fd":"code","c7ae3c9f":"code","d1f34be5":"code","0a6cd0d6":"code","d944d75c":"code","aad91b73":"code","e8f7c818":"code","92c83fdf":"code","1418d042":"code","06c3e681":"code","9f8a78a2":"code","a7e5b753":"code","ce636c9d":"code","d9b8f6fa":"code","55349975":"code","e0f05c21":"code","cdfb55c3":"code","466d2d68":"code","70903cd7":"code","8e5432e9":"code","75674898":"code","3c45bf49":"code","379b2de6":"code","71671b1a":"code","7235a25e":"code","1c2c3b23":"code","cdc577a6":"code","9946879d":"code","b2b40b71":"code","b36045be":"code","98665334":"code","6e8d9d66":"code","c6c3b24a":"code","2cab002d":"code","99d4a2a3":"code","85a4427d":"code","9eeec786":"code","2197483b":"code","2b65e05a":"code","4fba2dce":"code","834c5836":"code","5f70aa26":"code","f6af3da1":"code","cf37cb5f":"code","ad842af3":"code","d8093ad8":"code","a6778952":"code","c94bff8f":"code","5bbe727d":"code","c21c9a33":"code","57d10a47":"code","717e8d32":"markdown","4ec9fb51":"markdown","20962cd5":"markdown","1c2c9716":"markdown","c55a10bf":"markdown","54c58989":"markdown","f634aa36":"markdown","1ec9db72":"markdown","e3c210d2":"markdown","3fd4c502":"markdown","3b911695":"markdown","5a3288b4":"markdown","8c53ce95":"markdown","3a78294e":"markdown","2ce67e82":"markdown","14116383":"markdown","6cec29e9":"markdown"},"source":{"cad6573f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bca31982":"import os\nfrom numpy import savez_compressed\nimport SimpleITK as sitk\n\nfrom time import time\n\n# Required Imports and loading up a scan for processing as presented by Guide Zuidhof\n\n%matplotlib inline\n\nimport pydicom\nimport scipy.ndimage as ndimage\nfrom scipy.ndimage.interpolation import zoom\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns \n\nfrom skimage import measure, morphology, segmentation\nfrom skimage.transform import resize\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\nfrom sklearn.model_selection import KFold,GroupKFold,TimeSeriesSplit,train_test_split, StratifiedKFold","44adc727":"import tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as B\nimport tensorflow.keras.callbacks as C\nfrom tensorflow_addons.optimizers import RectifiedAdam\n\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.callbacks import History, EarlyStopping\nfrom tensorflow.keras.regularizers import l1_l2,l2,l1\nfrom tensorflow.keras.layers import Dropout, Dense, BatchNormalization\nfrom tensorflow.keras.layers import Flatten, Reshape\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint","999773fd":"def set_seed(seed):\n    '''\n    from os import environ\n    environ[\"PYTHONHASHSEED\"] = '0'\n    environ[\"CUDA_VISIBLE_DEVICES\"]='-1'\n    environ[\"TF_CUDNN_USE_AUTOTUNE\"] ='0'\n    '''\n\n    from numpy.random import seed as np_seed\n    np_seed(seed)\n    import random\n    random.seed(seed)\n    from tensorflow import random\n    random.set_seed(seed)","ac55237c":"SEED = 11\nset_seed(SEED)","e8966a6c":"testFeP = '..\/input\/lish-moa\/test_features.csv'\ntrainFeP = '..\/input\/lish-moa\/train_features.csv'\ntrainTaSP = '..\/input\/lish-moa\/train_targets_scored.csv'\ntrainTaNP = '..\/input\/lish-moa\/train_targets_nonscored.csv'\nsample_submission_path = '..\/input\/lish-moa\/sample_submission.csv'","616f7512":"trainFe = pd.read_csv(trainFeP)\ntrainFe_col = trainFe.columns\ntrainFe.describe(),trainFe.info()","0f2891c0":"cols_list=trainFe.columns\ncols_list","4cd4745a":"from scipy.stats import shapiro\nfrom scipy.stats import normaltest\nfrom scipy.stats import anderson\nfrom matplotlib import pyplot\nfrom statsmodels.graphics.gofplots import qqplot","d107087a":"trainFe.drop(columns=['sig_id', 'cp_type', 'cp_time', 'cp_dose']).max().max(),trainFe.drop(columns=['sig_id', 'cp_type', 'cp_time', 'cp_dose']).min().min()\n","6bfc6dfd":"# gaussian percent point function\nfrom scipy.stats import norm\n# define probability\np = 0.025\n# retrieve value <= probability\nvalue = norm.ppf(p)\nprint(value)\n# confirm with cdf\np = norm.cdf(value)\nprint(p)","2bdeca3e":"data = trainFe[cols_list[16]].copy()\nstd_data = data.std()\nmean_data = data.mean()\nprint(data.mean(),data.std())\npyplot.hist(data, bins=100)\npyplot.show()\nqqplot(data, line='s')\npyplot.show()\nlam = 1e-3\nfor k in range(len(data)):\n    if norm.cdf(data[k])> (1-lam):\n        #print(k, data[k], norm.cdf(data[k]))\n        data[k]= norm.ppf(1-lam)\n    elif norm.cdf(data[k]) < lam:\n        #print(k, data[k], norm.cdf(data[k]))\n        data[k]=norm.ppf(lam)\nprint(data.mean(),data.std())\npyplot.hist(data, bins=100)\npyplot.show()","bcc088a0":"qqplot(data, line='s')\npyplot.show()","4f0e3ac2":"result = anderson(data)\nprint('Statistic: %.3f' % result.statistic)\np = 0\nfor i in range(len(result.critical_values)):\n\tsl, cv = result.significance_level[i], result.critical_values[i]\n\tif result.statistic < result.critical_values[i]:\n\t\tprint('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n\telse:\n\t\tprint('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))","b3fd3c36":"# normality test\nstat, p = normaltest(data)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n\tprint('Sample looks Gaussian (fail to reject H0)')\nelse:\n\tprint('Sample does not look Gaussian (reject H0)')","613824dd":"gaussian = np.zeros(len(cols_list))\nstd_list = np.zeros(len(cols_list))\nalpha = 0.05\nfor k in range(4,len(cols_list)):\n    data = trainFe[cols_list[k]]\n    # normality test\n    stat, p = normaltest(data)\n    if p <= alpha: gaussian[k] = 1\n    std_list[k]= data.std()\n    '''\n    print(data.mean(),data.std())\n    qqplot(data, line='s')\n    pyplot.show()\n    '''\nprint('There are %i not gaussian' %gaussian.sum())","0d965d3e":"data = std_list[4:]\nprint(data.mean(),data.std())\npyplot.hist(data, bins=100)\npyplot.show()","477a3e73":"from scipy import stats\n\ncorrelation = trainFe.corr(method='pearson')","5d6c8d94":"correlation.shape","942e0161":"trainFe.cp_type.unique()","6017e35a":"lista= trainFe.loc[trainFe.cp_type == 'ctl_vehicle','sig_id']\nlista","88733315":"kk=0\nfor i in lista:\n    #trainFe = trainFe.drop(trainFe.loc[trainFe.sig_id == i].index)\n    kk+=1\n","1e536b76":"print(kk)","f21afb7f":"testFe = pd.read_csv(testFeP)\ntestFe_col = testFe.columns\ntestFe.describe(),testFe.info()","20531121":"lesta= testFe.loc[testFe.cp_type == 'ctl_vehicle','sig_id']\nlesta","507ac1c6":"trainTaS = pd.read_csv(trainTaSP)\ntrainTaS_col = trainTaS.columns\ntrainTaS.describe(),trainTaS.info()","13e75e9b":"trainTaN = pd.read_csv(trainTaNP)\ntrainTaN_col = trainTaN.columns\ntrainTaN.describe()","fd9f48ae":"k=0\nkk=0\nfor i in lista:\n    app=trainTaS.loc[trainTaS.sig_id == i]\n    k+=app.drop(columns=['sig_id']).max(axis=1).values[0]\n    kk+=1\nprint(k,kk)","c79efce2":"k=0\nkk=0\nfor i in lista:\n    appN=trainTaN.loc[trainTaN.sig_id == i]\n    k+=appN.drop(columns=['sig_id']).max(axis=1).values[0]\n    kk+=1\nprint(k,kk)","b73326ec":"app.columns","06c8c7d0":"appN.columns","6301bd9d":"#s.cummax(skipna=False)\nk=app.drop(columns=['sig_id']).max(axis=1)\nk.values","c62a0c06":"k=appN.drop(columns=['sig_id']).max(axis=1)\nk.values","58cac011":"#trainFe.info(), trainTaS.info()","5f2d02d2":"# it could be used to create new features\nsub = pd.read_csv(sample_submission_path)\nsub_col = sub.columns\n# sub.describe(), sub.info()","42fdcdf2":"cols = sub.drop(columns=['sig_id']).columns\nfor i in lesta:\n    sub.loc[sub.sig_id == i,cols]=0\nsub.describe()","ac4134bb":"trainFe['cp_type'].unique()","3e397dad":"# column 'cp_type' has only 2 values: 'trt_cp' and 'ctl_vehicle'\n\nlenn = np.array(trainFe['cp_type'])\nplt.hist(lenn, bins=3)\nplt.show()\n#print(lenn.std(),lenn.mean())","b9e75ff2":"# column 'cp_dose' has only 2 values: 'D1' and 'D2'\n\nlenn = np.array(trainFe['cp_dose'])\nplt.hist(lenn, bins=3)\nplt.show()\n#print(lenn.std(),lenn.mean())","5fa4748a":"# column 'cp_time' has only 3 values: 24, 48, 72\n\nlenn = np.array(trainFe['cp_time'])\nplt.hist(lenn, bins=5)\nplt.show()\nprint(lenn.std(),lenn.mean())","ab74a3d6":"''' \n------ sostitusco valori ----------\n\nappoggio['Sex']=appoggio['Sex'].replace(to_replace =['male','female'],value=[0,1]).astype(int)\n\n------ normalizzo le feature ------\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfeatures = ['Fare','Fare_B',\"Age\",\"family\",\"SibSp\",\"Parch\",\"Pclass\",'Embarked','Title','Cabin','Ticket',\"Pre_TK\",\"Post_TK\"]\nmms = MinMaxScaler()\n\nto_norm = appoggio[features]\nappoggio[features] = mms.fit_transform(to_norm)\n\n------ creo dummy -----------------\n\nappoggio = pd.get_dummies(appoggio,columns=[\"Sex\"])\n\n'''","e4cd6ac2":"trainFe.describe()","557f1245":"outliers_remuval = 'perc'","41a2cfad":"if outliers_remuval == 'perc':\n    lam = 1e-3\n    alto = norm.ppf(1-lam)\n    basso = norm.ppf(lam)\nelse:\n    alto = 1\n    basso = -1\n    \nfor i in range(4,len(cols_list)):\n    data = trainFe[cols_list[i]].copy()\n    data[data>alto]=alto\n    data[data<basso]=basso\n    trainFe[cols_list[i]] = data\n\n    data = testFe[cols_list[i]].copy()\n    data[data>alto]=alto\n    data[data<basso]=basso\n    testFe[cols_list[i]] = data\n\ntrainFe.describe()","21b4a381":"testFe.describe()","e7a1ac1e":"if outliers_remuval == 'perc':\n    features = trainFe.drop(columns=['sig_id','cp_type','cp_time','cp_dose']).columns\n    to_norm = trainFe[features]\n    mms = MinMaxScaler()\n    trainFe[features] = mms.fit_transform(to_norm)","fc96e1db":"if outliers_remuval == 'perc':\n    features = testFe.drop(columns=['sig_id','cp_type','cp_time','cp_dose']).columns\n    to_norm = testFe[features]\n    mms = MinMaxScaler()\n    testFe[features] = mms.fit_transform(to_norm)","22d49a0e":"trainFe.describe()","5f7d8d4a":"testFe.describe()","2c779768":"'''\ntrainFe['cp_dose'] = trainFe['cp_dose'].replace(to_replace = ['D1','D2'],value=[0,1]).astype(int)\ntrainFe['cp_type'] = trainFe['cp_type'].replace(to_replace = ['trt_cp','ctl_vehicle'],value=[0,1]).astype(int)\n'''\ntrainFe = pd.get_dummies(trainFe, columns = ['cp_dose'])\ntrainFe = pd.get_dummies(trainFe, columns = ['cp_type'])\ntrainFe = pd.get_dummies(trainFe, columns = ['cp_time'])\n\ntrainFe.describe()","775bb379":"'''\ntestFe['cp_dose'] = testFe['cp_dose'].replace(to_replace = ['D1','D2'],value=[0,1]).astype(int)\ntestFe['cp_type'] = testFe['cp_type'].replace(to_replace = ['trt_cp','ctl_vehicle'],value=[0,1]).astype(int)\n'''\ntestFe = pd.get_dummies(testFe, columns = ['cp_dose'])\ntestFe = pd.get_dummies(testFe, columns = ['cp_type'])\ntestFe = pd.get_dummies(testFe, columns = ['cp_time'])\ntestFe.describe()","49e3632a":"#print('trainFe:\\n',trainFe.columns,'\\ntestFe:\\n',testFe.columns,'\\ntrainTaS:\\n',trainTaS.columns,'\\ntrainTaN:\\n',trainTaN.columns,'\\nsub:\\n',sub.columns)","8c49d9c6":"'''\ntra = pd.read_csv(f\"{ROOT}\/train.csv\")\ntra.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\nchunk = pd.read_csv(f\"{ROOT}\/test.csv\")\n\n\ntr['WHERE'] = 'train'\nchunk['WHERE'] = 'val'\nsub['WHERE'] = 'test'\ndata = tr.append([chunk, sub])\n\nsub = pd.read_csv(f\"{ROOT}\/sample_submission.csv\")\nsub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nsub =  sub[['Patient','Weeks','Confidence','Patient_Week']]\nsub = sub.merge(chunk.drop('Weeks', axis=1), on=\"Patient\") # appiccica a sub le colonne di Chunk a parte week\n\n'''","3bec6eab":"'''\ntrainFe['WHERE'] = 'train' \ntrainFe = trainFe.merge(trainTaS, on='sig_id')\ntrainFe = trainFe.merge(trainTaN, on='sig_id') \ntrainFe.info()\ntestFe['WHERE'] = 'test' sub['WHERE'] = 'sub'\ndata = trainFe.append([testFe,sub])\n'''","0c726dc3":"train_esteso = trainTaS.merge(trainTaN, how='inner')","f62e8737":"trainTaS.describe()","b7fdc250":"trainTaN.describe()","76cc2898":"train_esteso.describe()","71234829":"train = trainFe.drop(columns=['sig_id']).values\n#real = trainTaS.drop(columns=['sig_id']).values\nreal = train_esteso.drop(columns=['sig_id']).values\nrealC = real.copy()\ntest = testFe.drop(columns=['sig_id']).values\n","d8bcb5e8":"real.sum()","c49a99c3":"trainFe.info()","0de546ea":"trainFe.describe(include=[object])  ,trainTaS.describe(include=[object])","58004653":"trainTaS.describe()","ff0a6127":"testFe.info()","b342b692":"X_train = train#[:512]\ny_real = real#[:512]","0bfa72cb":"X_train.shape, y_real.shape","d0787e84":"XT = np.append(X_train,test,axis=0)","6e1969c2":"X_train.shape, test.shape, XT.shape","e370df86":"from numpy import array\nfrom numpy import mean\nfrom numpy import cov\nfrom numpy.linalg import eig\n\nM = mean(X_train.T, axis=1)\n#print(M.shape,'M\\n',M)\nMt = mean(test.T, axis=1)\nMT = mean(XT.T, axis=1)","cf9b71fd":"# center columns by subtracting column means\nC = X_train - M\n#print(C.shape,'C\\n',C)\nCt = test - Mt\nCT = XT- MT","c7ae3c9f":"# calculate covariance matrix of centered matrix, that is, the joint probability for two features shape: (features, features)\nV = cov(C.T)\n#print(V.shape,'V = cov(C.T)\\n',V)\nVt = cov(Ct.T)\nVT = cov(CT.T)","d1f34be5":"# factorize covariance matrix\nvalues, vectors = eig(V)\n#print(vectors.shape,'vectors\\n',vectors)\nvalues_t, vectors_t = eig(Vt)\nvalues_T, vectors_T = eig(VT)","0a6cd0d6":"'''\nprint(values.shape,'values\\n',np.sort(values))\nprint(values_t.shape,'values_t\\n',np.sort(values_t))\nprint(values_T.shape,'values_T\\n',np.sort(values_T))\n'''","d944d75c":"C.shape, Ct.shape, CT.shape","aad91b73":"PCA_LIMIT = 1e-4\n\nvalori = values_T.copy()\nk=0\nfor i in range(len(values_T)):\n    if abs(values_T[i]) < PCA_LIMIT:\n        \n        CT=np.delete(CT,k+i,1)\n        Ct=np.delete(Ct,k+i,1)\n        C=np.delete(C,k+i,1)\n        valori=np.delete(valori,k+i)\n        vectors=np.delete(vectors,k+i,0)\n        vectors=np.delete(vectors,k+i,1)\n        \n        vectors_t=np.delete(vectors_t,k+i,1)\n        vectors_t=np.delete(vectors_t,k+i,0)\n\n        vectors_T=np.delete(vectors_T,k+i,0)\n        vectors_T=np.delete(vectors_T,k+i,1)\n        \n        k-=1\nprint(k)   ","e8f7c818":"C.shape, Ct.shape, CT.shape, valori.shape, vectors.shape, vectors_t.shape, vectors_T.shape","92c83fdf":"P = vectors.T.dot(C.T) # P.T is a projection of C - same projection can be applied to a selection of features based on eigenvalues\n#print(P.shape,'P\\n',P)\nPT = P.T\n#print(PT.shape,'PT\\n',PT) ","1418d042":"P_t = vectors_t.T.dot(Ct.T) # P.T is a projection of C - same projection can be applied to a selection of features based on eigenvalues\n#print(P_t.shape,'P_t\\n',P_t)\nP_tT = P_t.T\n#print(P_tT.shape,'P_tT\\n',P_tT) ","06c3e681":"PCA = False\nif PCA == True:\n    train = PT\n    X_train = PT\n    test = P_tT\n    print(train.shape, X_train.shape, test.shape)\nelse:\n    train = trainFe.drop(columns=['sig_id']).values\n    X_train = train\n    #real = trainTaS.drop(columns=['sig_id']).values\n    test = testFe.drop(columns=['sig_id']).values\n    print(train.shape, X_train.shape, test.shape, real.shape)","9f8a78a2":"L2 = 0\nSEED = 23\nset_seed(SEED)\nINITIALIZER = tf.keras.initializers.GlorotUniform()\nMOMENTUM = 0.9\n\ntry: \n    del model\n    tf.keras.backend.clear_session()\n    print('session cleared')\nexcept Exception as OSError:\n    pass\n\ncheckpoint_filepath = 'checkpointWeight'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True)","a7e5b753":"train.shape","ce636c9d":"train.shape[1]","d9b8f6fa":"MOMENTUM = 0.9\n# 'relu', activity_regularizer=l2(regul),\ndef madel(init, regul):\n    model_input = K.Input(shape = (train.shape[1]), name=\"input\")\n\n    x = BatchNormalization(momentum=MOMENTUM)(model_input)\n    x = Dense(2048,activation='swish', kernel_initializer=init)(x)\n \n    x = BatchNormalization(momentum=MOMENTUM)(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1024,activation='swish', kernel_initializer=init)(x)\n    \n    x = BatchNormalization(momentum=MOMENTUM)(x)\n    x = Dropout(0.5)(x)\n    x = Dense(512,activation='swish', kernel_initializer=init)(x)\n    '''\n    x = BatchNormalization(momentum=MOMENTUM)(x)\n    x = Dropout(0.65)(x)\n    x = Dense(256,activation='swish', kernel_initializer=init)(x)\n    '''\n    x = BatchNormalization(momentum=MOMENTUM)(x)\n    x = Dropout(0.65)(x)\n    model_output = Dense(206,activation='sigmoid', kernel_initializer=init)(x)\n\n    model_base = K.Model(model_input, model_output, name=\"output\")\n    \n    return model_base\n\ndef madelWN(init, regul):\n    model_input = K.Input(shape = (train.shape[1]), name=\"input\")\n\n    x = BatchNormalization(momentum=MOMENTUM)(model_input)\n    \n    x = BatchNormalization(momentum=MOMENTUM)(x)\n    x = Dropout(0.65)(x)\n    x = tfa.layers.WeightNormalization(Dense(1024,activation='swish', kernel_initializer=init))(x)\n\n    x = BatchNormalization(momentum=MOMENTUM)(x)\n    x = Dropout(0.65)(x)\n    x = tfa.layers.WeightNormalization(Dense(512,activation='swish', kernel_initializer=init))(x)\n    '''\n    x = BatchNormalization(momentum=MOMENTUM)(x)\n    x = Dropout(0.65)(x)\n    x = tfa.layers.WeightNormalization(Dense(256,activation='swish', kernel_initializer=init))(x)\n    '''\n    x = BatchNormalization(momentum=MOMENTUM)(x)\n    x = Dropout(0.65)(x)\n    model_output = tfa.layers.WeightNormalization(Dense(206,activation='sigmoid', kernel_initializer=init))(x)\n\n    model_base = K.Model(model_input, model_output, name=\"output\")\n    \n    return model_base\n\ndef madelWN_esteso(init, regul):\n    model_input = K.Input(shape = (train.shape[1]), name=\"input\")\n\n    x = BatchNormalization(momentum=MOMENTUM)(model_input)\n    \n    x = BatchNormalization(momentum=MOMENTUM)(x)\n    x = Dropout(0.65)(x)\n    x = tfa.layers.WeightNormalization(Dense(2048,activation='swish', kernel_initializer=init))(x)\n\n    x = BatchNormalization(momentum=MOMENTUM)(x)\n    x = Dropout(0.65)(x)\n    x = tfa.layers.WeightNormalization(Dense(1024,activation='swish', kernel_initializer=init))(x)\n    '''\n    x = BatchNormalization(momentum=MOMENTUM)(x)\n    x = Dropout(0.65)(x)\n    x = tfa.layers.WeightNormalization(Dense(256,activation='swish', kernel_initializer=init))(x)\n    '''\n    x = BatchNormalization(momentum=MOMENTUM)(x)\n    x = Dropout(0.65)(x)\n    model_output = tfa.layers.WeightNormalization(Dense(608,activation='sigmoid', kernel_initializer=init))(x)\n\n    model_base = K.Model(model_input, model_output, name=\"output\")\n    \n    return model_base\n\nmodel_base = madelWN_esteso(tf.keras.initializers.GlorotUniform(),0)\nK.utils.plot_model(model_base, \"madel.png\", show_shapes=True)","55349975":"model_base.summary()","e0f05c21":"def score_MoB(y_true, y_pred, dtype = 'float64'):\n    #preda = tf.math.abs(y_pred)\n    ytS = tf.slice(y_true, [0, 206], [len(y_true), y_true.shape[1]-206])\n    ypS = tf.math.round(tf.slice(y_pred, [0, 206], [len(y_pred), y_pred.shape[1]-206]))\n    preda = tf.dtypes.cast(ypS, 'float64')\n    predb = tf.math.minimum(preda, tf.constant(1-1e-15,dtype='float64'))\n    predb = tf.dtypes.cast(predb, 'float64')\n    pred = tf.math.maximum(predb, tf.constant(1e-15,dtype='float64'))\n    pred = tf.dtypes.cast(pred, 'float64')\n    #print('pred',pred,'tf.math.log((1 - pred)',tf.math.log((1 - pred)))\n    yt = tf.dtypes.cast(ytS, 'float64')\n    m1 = tf.math.multiply(yt, tf.math.log(pred))\n    m2 = tf.math.multiply((1 - yt), tf.math.log(1-pred))\n    metric = m1+m2\n    return -B.mean(B.mean(metric, axis = -1), axis=0)\n\ndef score_MoC(y_true, y_pred, dtype = 'float64'):\n    #preda = tf.math.abs(y_pred)\n    ytS = tf.slice(y_true, [0, 206], [len(y_true), y_true.shape[1]-206])\n    ypS = tf.slice(y_pred, [0, 206], [len(y_pred), y_pred.shape[1]-206])\n    preda = tf.dtypes.cast(ypS, 'float64')\n    predb = tf.math.minimum(preda, tf.constant(1-1e-15,dtype='float64'))\n    predb = tf.dtypes.cast(predb, 'float64')\n    pred = tf.math.maximum(predb, tf.constant(1e-15,dtype='float64'))\n    pred = tf.dtypes.cast(pred, 'float64')\n    #print('pred',pred,'tf.math.log((1 - pred)',tf.math.log((1 - pred)))\n    yt = tf.dtypes.cast(ytS, 'float64')\n    m1 = tf.math.multiply(yt, tf.math.log(pred))\n    m2 = tf.math.multiply((1 - yt), tf.math.log(1-pred))\n    metric = m1+m2\n    return -B.mean(B.mean(metric, axis = -1), axis=0)\n\ndef score_MoA(y_true, y_pred, dtype = 'float64'):\n    #preda = tf.math.abs(y_pred)\n    preda = tf.dtypes.cast(y_pred, 'float64')\n    predb = tf.math.minimum(preda, tf.constant(1-1e-15,dtype='float64'))\n    predb = tf.dtypes.cast(predb, 'float64')\n    pred = tf.math.maximum(predb, tf.constant(1e-15,dtype='float64'))\n    pred = tf.dtypes.cast(pred, 'float64')\n    #print('pred',pred,'tf.math.log((1 - pred)',tf.math.log((1 - pred)))\n    yt = tf.dtypes.cast(y_true, 'float64')\n    m1 = tf.math.multiply(yt, tf.math.log(pred))\n    m2 = tf.math.multiply((1 - yt), tf.math.log(1-pred))\n    metric = m1+m2\n    return -B.mean(B.mean(metric, axis = -1), axis=0)\n\ndef loss_MoA(dtype = 'float64'):\n    def losss(y_true, y_pred, dtype = 'float64'):\n        return score_MoA(y_true, y_pred)\n    return losss\n    \ndef rmse(y_true, y_pred):\n    return B.sqrt(B.mean(B.square(y_pred - y_true), axis=-1))\n\ndef my_acc_score(y_true, y_pred):\n    ya = tf.math.round(y_pred)\n    yb = tf.math.equal(y_true,ya)\n    yc = tf.math.count_nonzero(yb)\n    yz = tf.math.maximum(y_true, tf.constant(1,dtype='float32'))\n    return yc\/tf.math.count_nonzero(yz)\n\ndef my_acc_C_score(y_true, y_pred):\n    ytS = tf.slice(y_true, [0, 206], [len(y_true), y_true.shape[1]-206])\n    ypS = tf.slice(y_pred, [0, 206], [len(y_pred), y_pred.shape[1]-206])\n    ya = tf.math.round(ypS)\n    yb = tf.math.equal(ytS,ya)\n    yc = tf.math.count_nonzero(yb)\n    yz = tf.math.maximum(ytS, tf.constant(1,dtype='float32'))\n    return yc\/tf.math.count_nonzero(yz)","cdfb55c3":"from keras.callbacks import LearningRateScheduler\nimport math\n\nEPOCHS = 10\ntrain_phase = False\nk_folds = 6\n\n# learning rate schedule\ndef step_decay(epoch):\n    initial_lrate = 1e-2\n    drop = 0.5\n    epochs_drop = int(EPOCHS*.50)\n\n    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)\/epochs_drop))\n    return lrate\n\n\nRIF = 1\nL2 = 0.0\nLR_RA = 1e-2\nMIN_LR = 1e-4\nBATCH_SIZE = 1024\nlogLoss = 'binary_crossentropy'\n\nmyloss = logLoss # loss_MoA()\n\n\nSEED = 56\nset_seed(SEED)\n\ntry: del hz\nexcept:\n    pass\nhz = pd.DataFrame()\n\n\nstart_all_at = time()\nif train_phase:\n    kf = KFold(n_splits=k_folds, random_state=2, shuffle=True)\n    i = 1\n    for train_index, test_index in kf.split(X_train):\n        trainData = X_train[train_index]\n        valData = X_train[test_index]\n        trainLabels = y_real[train_index]\n        valLabels = y_real[test_index]\n        \n        # -----------------------------\n        #BATCH_SIZE = len(trainData)    # attenzione al BATCH_SIZE\n        # -----------------------------\n        \n        try: \n            del model\n            tf.keras.backend.clear_session()\n            print('session cleared')\n        except Exception as OSError:\n            pass\n\n\n        set_seed(SEED)\n        INITIALIZER = tf.keras.initializers.GlorotUniform()\n        tot_steps = max(int(((len(trainData)\/\/BATCH_SIZE))*RIF),1)\n        #opt = tfa.optimizers.RectifiedAdam(lr=LR_RA,total_steps=tot_steps, warmup_proportion=0.1, min_lr=MIN_LR) #total_steps=336000\n        opt = tf.keras.optimizers.Adam(learning_rate=MIN_LR)\n        \n        # attenzione scelta modello\n        model = madelWN_esteso(INITIALIZER, L2)\n        l1_w = model.get_weights()\n        model.compile(loss=myloss, optimizer=opt, metrics=[score_MoC, my_acc_score, my_acc_C_score])\n        \n        # callbacks\n        # learning schedule callback\n        lrate = LearningRateScheduler(step_decay)\n        history = History()\n        #early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=1e-6, patience=50, restore_best_weights=True)\n        \n        print('Iniizio addestramento fold',i)\n        start_at = time()\n        \n        model.fit(trainData, trainLabels, epochs=EPOCHS, validation_data=(valData,valLabels), batch_size=BATCH_SIZE,\n                  verbose=1, callbacks=[history, lrate]) #early_stopping, lrate])\n        \n        # model.load_weights(checkpoint_path)\n        l2_w = model.get_weights()\n        \n        hz=hz.append([[history.history['loss'],history.history['score_MoC'],history.history['my_acc_score'],history.history['my_acc_C_score'],\n                       history.history['val_loss'],history.history['val_score_MoC'],history.history['val_my_acc_score'],history.history['val_my_acc_C_score']]]\n                     , ignore_index=True)\n        \n        print(len(history.history['loss']))\n        exec_time = time() - start_at\n        print(\"\\nTempo totale di addestramento fold: %i %d minuti e %d secondi\" % (i, exec_time\/60, exec_time%60),'\\n')\n        i +=1\nelse:\n    trainData = X_train\n    trainLabels = y_real\n\n    try: \n        del model\n        tf.keras.backend.clear_session()\n        print('session cleared')\n    except Exception as OSError:\n        pass\n    \n    # -----------------------------\n    #BATCH_SIZE = len(trainData)    # attenzione al BATCH_SIZE\n    # -----------------------------\n    \n\n    set_seed(SEED)\n    INITIALIZER = tf.keras.initializers.GlorotUniform()\n    tot_steps = max(int(((len(trainData)\/\/BATCH_SIZE))*RIF),1)\n    #opt = tfa.optimizers.RectifiedAdam(lr=LR_RA,total_steps=tot_steps, warmup_proportion=0.1, min_lr=MIN_LR) #total_steps=336000\n    opt = tf.keras.optimizers.Adam(learning_rate=MIN_LR)\n    \n    # attenzione scelta modello\n    model = madelWN_esteso(INITIALIZER, L2)\n    l1_w = model.get_weights()\n    model.compile(loss=myloss, optimizer=opt, metrics=[score_MoC, my_acc_score,my_acc_C_score])\n\n    # callbacks\n    # learning schedule callback\n    lrate = LearningRateScheduler(step_decay)\n    history = History()\n    #early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=1e-6, patience=50, restore_best_weights=True)\n\n    print('Iniizio addestramento fold')\n    start_at = time()\n\n    model.fit(trainData, trainLabels, epochs=EPOCHS, batch_size=BATCH_SIZE,\n              verbose=1, callbacks=[history, lrate])\n    \n    # model.load_weights(checkpoint_path)\n    l2_w = model.get_weights()\n    print(len(history.history['loss']))\n    exec_time = time() - start_at\n    \nexec_time = time() - start_all_at\nprint(\"\\nTempo totale di addestramento: %d minuti e %d secondi\" % (exec_time\/60, exec_time%60),'\\n')","466d2d68":"if train_phase:\n    hz=hz.rename(columns={0:'loss',1:'score_MoC',2:'my_acc_score',3:'my_acc_C_score',4:'val_loss',5:'val_score_MoC',6:'val_my_acc_score',7:'val_my_acc_C_score'})","70903cd7":"def subplot_model (model_history, epochs, fold, starting_point):\n    if epochs <= starting_point: starting_point=0\n    xM = model_history['score_MoC'][starting_point:]\n    vM = model_history['val_score_MoC'][starting_point:]\n    xL = model_history['loss'][starting_point:]\n    vL = model_history['val_loss'][starting_point:]\n    # Create two subplots and unpack the output array immediately\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey= False, figsize=(20,8))\n    ax1.set_title(\"score_MoC Fold %i\" % fold)\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"score_MoC\")\n    l1,=ax1.plot(vM,color='blue')\n    l2,=ax1.plot(xM,color='green')\n    ax1.legend([l1, l2],[\"val\", \"train\"])\n    plt.show\n    try:\n        ax2.set_title(\"Loss Fold %i\" % fold)\n        ax2.set_xlabel(\"Epoch\")\n        ax2.set_ylabel(\"Loss\")\n        l1,=ax2.plot(vL,color='blue')\n        l2,=ax2.plot(xL,color='green')\n        ax2.legend([l1, l2],[\"val\", \"train\"])\n        plt.show\n    except: pass","8e5432e9":"# def plot_model(model_history, starting_point, metric):\nif train_phase:\n    for i in range(k_folds):\n        a = hz.iloc[i]\n        subplot_model(a, EPOCHS, i+1, 2)","75674898":"trainData.shape","3c45bf49":"testFe.columns","379b2de6":"testFe.describe(include=[object])","71671b1a":"testFe.describe()","7235a25e":"testFe.columns","1c2c3b23":"x_tr, y_tr = train[512:1024], real[512:1024]","cdc577a6":"n_batch = 0","9946879d":"xxx = model.predict(train)","b2b40b71":"lis = np.arange(206,608)\nxxx = np.delete(xxx, lis,1)","b36045be":"xxx.shape","98665334":"xxx.sum()","6e8d9d66":"xxx=np.around(xxx,0)","c6c3b24a":"xxx.sum()","2cab002d":"y_tr.shape, real.shape","99d4a2a3":"if n_batch > 0:\n    old_weights = model.get_weights()\n    new_model = madelWN(INITIALIZER, L2)\n    new_weights = new_model.get_weights()\n    new_model.set_weights(old_weights)\n    new_model.compile(loss=myloss, optimizer=opt, metrics=[my_acc_score,score_MoA])\n    pred = new_model.predict(test, batch_size=n_batch)\n    prad = new_model.predict(x_tr, batch_size=n_batch)\n    new_model.evaluate(x_tr, y_tr,verbose=0, batch_size=n_batch)\n    print('new_model')\nelse:\n    #pred = model.predict(testFe.drop(columns=['sig_id']).values)\n    pred = model.predict(test)\n    prad = model.predict(x_tr)\n    model.evaluate(x_tr, y_tr,verbose=0, batch_size=BATCH_SIZE)\n    print('model')","85a4427d":"'''\npredbb=np.minimum(prad, 1-1e-15)\npreddd=np.maximum(predbb, 1e-15)\nm1=y_tr*np.log(preddd)+(1-y_tr)*np.log(1-preddd)\nm2 = m1.mean(axis = -1)\nscoreMoA = -m2.mean(axis = 0)\nscoreMoA\n'''","9eeec786":"len(pred)","2197483b":"col = trainTaS.drop(columns=['sig_id']).columns\ncol","2b65e05a":"y_tr.shape, len(y_tr[1])","4fba2dce":"'''\nt = tf.constant([[0,0,0,1,1,1],\n                 [0,0,0,1,1,1],\n                 [0,0,0,1,1,1]])\ntx = tf.constant([[0.1,0.1,0.1,.8,.8,.8],\n                 [1,1,1,1,1,1],\n                 [1,1,1,1,1,1]])\n\ntt=tf.slice(t, [0, 3], [len(t), t.shape[1]-3])\nttx=tf.slice(tx, [0, 3], [len(tx), tx.shape[1]-3])\n\nscore_MoA(t,tx),score_MoA(tt,ttx)\n'''","834c5836":"lis = np.arange(206,608)\npredDF = pd.DataFrame(np.delete(pred, lis,1), columns=col)\npradDF = pd.DataFrame(np.delete(prad, lis,1), columns=col)\npradTaSDF = pd.DataFrame(np.delete(y_tr, lis,1), columns=col)","5f70aa26":"predDF.info()","f6af3da1":"pradDF.info()","cf37cb5f":"predDF.describe()","ad842af3":"pradDF.describe()","d8093ad8":"pradTaSDF.describe()","a6778952":"predDF.describe()","c94bff8f":"output = pd.DataFrame({'sig_id': testFe.sig_id})\nto_csv = pd.concat([output, predDF], axis = 1)\nto_csv.describe()","5bbe727d":"cols = to_csv.drop(columns=['sig_id']).columns\n# quelli con lesta vanno azzerati\nfor i in lesta:\n    to_csv.loc[to_csv.sig_id == i,cols]=1e-15\n    \nto_csv.describe()\n","c21c9a33":"trainTaS.describe()","57d10a47":"if not train_phase:\n    to_csv.to_csv('submission.csv', index=False)\n    print(\"Your submission was successfully saved!\")","717e8d32":"# 12. Metric setting","4ec9fb51":"# 15. Output preparation","20962cd5":"# 11. Model setting","1c2c9716":"# 5. putting zero to test with ctl_vehicle","c55a10bf":"# 13. K-Fold training","54c58989":"# 1. Analysis of features distribution and normality tests","f634aa36":"# 6. Analysing cp_type, cp_dose, cp_time","1ec9db72":"# 9. Creating arrays for training the model","e3c210d2":"# 7a. Outliers remuval","3fd4c502":"# 8. Creating dummies for cp_type, cp_dose, cp_time","3b911695":"# 4. Verifying that no rows with ctl_vehicle have activations","5a3288b4":"# 14. Training evaluation","8c53ce95":"# 10. PCA features reduction","3a78294e":"![image.png](attachment:image.png)","2ce67e82":"# 7b. Normalizing features other than sig_id, cp_type, cp_dose, cp_time","14116383":"# 3. Counting and listing rows with 'ctl_vehicle'","6cec29e9":"# 2. Features resampling"}}