{"cell_type":{"64673afb":"code","15ce4959":"code","deabcebc":"code","9527cbb2":"code","0d3d1def":"code","88a085d1":"code","cd95bdcf":"code","c28c517b":"code","16b6439e":"code","432c5629":"code","35ed0c8f":"code","d9a14718":"code","e3e529bf":"code","0ae77720":"code","2f7e8632":"code","817975fe":"code","6a66940a":"code","4b2efe4d":"code","1c9f654d":"code","4c53f0cc":"code","d6a8a824":"code","8f0fd9f7":"code","d06e7d03":"code","ba391340":"code","bbe45503":"code","9a737b9b":"code","f44aa89b":"code","a6887240":"code","627a171e":"code","ad97067a":"code","97c458a5":"code","e371e95e":"code","e15fd084":"code","62b61023":"code","24cd8c24":"code","8e1563dd":"code","b48ad219":"code","60535dd5":"code","63f89821":"code","209935b5":"code","4153926c":"code","70f4abb5":"code","8fa1072d":"code","2bcec092":"code","863bb322":"code","53b5a33a":"code","40191cc2":"code","2e57909f":"code","8861246f":"code","62f64aa1":"code","1bc9d4e0":"code","f2703fa6":"code","01d094c4":"code","b6bc480d":"code","681cde8c":"code","f5ed7858":"code","97fb30c3":"code","88161376":"code","c6114b2f":"markdown","b6cb648a":"markdown","8e1fdab6":"markdown","5f10e56c":"markdown","71fa00d6":"markdown","8daa2712":"markdown","54a60b4c":"markdown","27954faa":"markdown","f9c2099d":"markdown","cd831c5d":"markdown","5cbfafd5":"markdown","1466c072":"markdown","ad23a6a3":"markdown","d741890c":"markdown","b0e79830":"markdown","341e7dbc":"markdown","97896495":"markdown","e6d8a2ee":"markdown","cbd1fc6f":"markdown","cca74c17":"markdown","b76ca69b":"markdown","31c14f4e":"markdown","130274c0":"markdown","eec1c9d5":"markdown","1661b62c":"markdown","da0d53bd":"markdown","64709af7":"markdown","70004253":"markdown","946bb4bf":"markdown","752e25e5":"markdown","e548955a":"markdown","5eb86dd7":"markdown","5f0154ca":"markdown","e13bdb8e":"markdown","6369034d":"markdown","6ad47ee5":"markdown","54513faa":"markdown","1128f519":"markdown","ddb821f9":"markdown","ef75a999":"markdown","af9dba19":"markdown","0c2f2a89":"markdown","5d30ae7c":"markdown","d73e66a7":"markdown","65527862":"markdown","356bae09":"markdown","27b3283c":"markdown","7cec6a17":"markdown","488f72c7":"markdown","c16f202f":"markdown","f52a28bc":"markdown"},"source":{"64673afb":"# Basic\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Common tools\nimport os\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom collections import Counter\n'''\n# Advanced visualization\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\n# Using plotly + cufflinks in offline mode\nimport cufflinks\ncufflinks.go_offline(connected=True)\ninit_notebook_mode(connected=True)\n'''\n# Model\nfrom sklearn import ensemble, tree, svm, naive_bayes, neighbors, linear_model, gaussian_process, neural_network\nfrom sklearn.metrics import accuracy_score, f1_score, auc, roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\nfrom sklearn.ensemble import VotingClassifier\n\n# Configure Defaults\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\npd.set_option('display.max_colwidth', -1)","15ce4959":"# List all files\nprint(os.listdir('..\/input\/'))","deabcebc":"train = pd.read_csv('..\/input\/application_train.csv')","9527cbb2":"train.shape","0d3d1def":"test = pd.read_csv('..\/input\/application_test.csv')","88a085d1":"test.shape","cd95bdcf":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')","c28c517b":"sample_submission.shape","16b6439e":"sns.countplot(x='TARGET', data=train)\nprint(train.TARGET.sum()\/train.TARGET.count())","432c5629":"train.info()","35ed0c8f":"train.head()","d9a14718":"# Number of categories within a categorical feature\ntrain.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","e3e529bf":"# Missing values\nn = train.isnull().sum() \/ len(train)\nn.sort_values(ascending=False).head(10)","0ae77720":"# Number of features exceeding 1\/6 of missing values.\nsum(i>0.1667 for i in n)","2f7e8632":"# Age\n(train['DAYS_BIRTH'] \/ -365).describe()","817975fe":"(train['DAYS_EMPLOYED'] \/ 365).describe()","6a66940a":"train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","4b2efe4d":"# Out of curiosity...\nanom = train[train['DAYS_EMPLOYED'] == 365243]\nnon_anom = train[train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","1c9f654d":"# Create new feature\ntrain['DAYS_EMPLOYED_BOOL'] = train['DAYS_EMPLOYED'] == 365243\ntrain['DAYS_EMPLOYED'].replace({365243:np.nan}, inplace=True)","4c53f0cc":"# Check orginal feature\ntrain['DAYS_EMPLOYED'].plot.hist()","d6a8a824":"def detect_outliers(df,n,features):\n    \n    outlier_indices = []\n    \n    for col in features:\n        Q1 = df[col].quantile(0.02)\n        Q3 = df[col].quantile(0.98)\n        IQR = Q3 - Q1\n        \n        outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR )].index\n        outlier_indices.extend(outliers)\n        \n    # Select observations with more than n outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   ","8f0fd9f7":"# Return only float64 features\nnumerical_feature_mask = train.dtypes==float\nnumerical_cols = train.columns[numerical_feature_mask].tolist()","d06e7d03":"# Detect outliers\nOutliers_to_drop = detect_outliers(train,2,numerical_cols)","ba391340":"# Number of outliers to drop\nlen(Outliers_to_drop)","bbe45503":"# Remove outliers\ntrain.drop(Outliers_to_drop, inplace=True)","9a737b9b":"# Save Id for the submission at the very end.\nId = test['SK_ID_CURR']","f44aa89b":"# Get marker\nsplit = len(train)","a6887240":"# Merge into one dataset\ndata =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","627a171e":"# We don't need the Id anymore now.\ndata.drop('SK_ID_CURR', axis=1, inplace=True)","ad97067a":"data.shape","97c458a5":"# Remove mostly sparse features\nfor f in data:\n   if data[f].isnull().sum() \/ data.shape[0] >= 0.5: del data[f] # Or do a boolean flag here","e371e95e":"# Check for TARGET data type\ndata['TARGET'].dtype","e15fd084":"# Select columns due to theirs data type\nfloat_col = data.select_dtypes('float').drop(['TARGET'], axis=1)\nint_col = data.select_dtypes('int')\nobject_col = data.select_dtypes('object')","62b61023":"# Remove and impute numerical features\nfor f in float_col:\n   if data[f].isnull().sum() \/ data.shape[0] > 0.1667: del data[f] # Remove 1\/6+ of NANs\n   else: data[f] = data[f].fillna(data[f].mean()) # Impute others with a mean value","24cd8c24":"# Impute default value into a numerical category\nfor i in int_col:\n   data[i] = data[i].fillna(-1)","8e1563dd":"# Impute object type with a default\nfor o in object_col:\n   data[o] = data[o].fillna('Unknown')","b48ad219":"# Check\ndata.isnull().sum().sort_values(ascending=False).head(5)","60535dd5":"data = pd.get_dummies(data, prefix_sep='_', drop_first=True) # Drop originall feature to avoid multi-collinearity","63f89821":"'''# Categorical mask\ncategorical_feature_mask = train.dtypes==object\n# Get categorical columns\ncategorical_cols = train.columns[categorical_feature_mask].tolist()'''","209935b5":"'''# Instantiate LE\nle = LabelEncoder()'''","4153926c":"'''# Apply LE\ntrain[categorical_cols] = train[categorical_cols].apply(lambda col: le.fit_transform(col.astype(str)))'''","70f4abb5":"'''# Check\ntrain[categorical_cols].head(10)'''","8fa1072d":"'''# Instantiate OHE\nohe = OneHotEncoder(categorical_features = categorical_feature_mask, sparse=False ) #Can be enabled True for higher preformance'''","2bcec092":"'''# Apply OHE\ntrain_ohe = ohe.fit_transform(train) #an numpy array'''","863bb322":"#Split data\ntrain_c = data[:split]\ntest_c = data[split:].drop(['TARGET'], axis=1)","53b5a33a":"from sklearn.model_selection import train_test_split\n\n# Get variables for a model\nx = train_c.drop([\"TARGET\"], axis=1)\ny = train_c[\"TARGET\"]\n\n#Do train data splitting\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.22, random_state=101)","40191cc2":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train) # Fit on training set only.\n\n# Apply transform to both the training set and the test set.\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","2e57909f":"from sklearn.decomposition import PCA\n\npca = PCA(.95)\npca.fit(X_train)","8861246f":"X_train = pca.transform(X_train)\nX_test = pca.transform(X_test)","62f64aa1":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(solver = 'lbfgs').fit(X_train, y_train)\npred = lr.predict(X_test)\nacc = lr.score(X_test, y_test)\n\nprint(\"Accuracy: \", acc)","1bc9d4e0":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\n\npredicts = cross_val_predict(lr, X_train, y_train, cv=3)\nconfusion_matrix(y_train, predicts)","f2703fa6":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(y_train, predicts))\nprint(\"Recall:\",recall_score(y_train, predicts))","01d094c4":"from sklearn.metrics import f1_score\n\nf1_score(y_train, predicts)","b6bc480d":"from sklearn.metrics import precision_recall_curve\n\ny_scores = lr.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(y_train, y_scores)\n\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","681cde8c":"from sklearn.metrics import roc_curve\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_scores)\n\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","f5ed7858":"from sklearn.metrics import roc_auc_score","97fb30c3":"y_scores = lr.predict_proba(X_train)\ny_scores = y_scores[:,1]","88161376":"auroc = roc_auc_score(y_train, y_scores)\nprint(\"ROC-AUC Score:\", auroc)","c6114b2f":"## Basic EDA ","b6cb648a":"I'll load just very basic files here. We're not going to do data merge as mentioned above.","8e1fdab6":"The data-set is cleary imbalanced. We'll have that in mind.","5f10e56c":"I've choise following algorithm because it is fast.","71fa00d6":"There's a lot of missing values within numerical features reaching 70%. We can drop them or use some model which can handle Nulls like XGBoost.","8daa2712":"## DATA WRANGLING","54a60b4c":"## Model ","27954faa":"Create a single dataframe for conviniet feature engineering and futher handling.","f9c2099d":"### Object encoding (the other way)","cd831c5d":"#### Days employed","5cbfafd5":"### Precision and Recall ","1466c072":"# HomeCredit Default Risk","ad23a6a3":"Almost half of the dataset contains missing values exceeding 1\/6 of a feature. Numerical and object categories could be imputed with a default value. Float features < 16% imputed with a mean.","d741890c":"In order to perform PCA, we need to standardize our data.","b0e79830":"### Anomaly detection","341e7dbc":"This feature looks good. Loans are provided to people within an age range 20-70 years.","97896495":"In the first row, 218718 clients were correctly predicted as not having a payment difficulties (true negatives) and 17 were wrongly classified as not having a payment difficulty (false negative).\nIn the second row 19192 clients were wrongly classified as having a payment difficulty (false possitive) and 17 of them correcly classified as having a payment difficulty (true negative).","e6d8a2ee":"### Steps to solve the problem","cbd1fc6f":"### General overview","cca74c17":"## LOAD LIBRARIES","b76ca69b":"Only the target in test dataset should be left with NANs.","31c14f4e":"Maximal employed time over 1,000 years doesn't look valid. High std confirms there's a huge dispercy in data.","130274c0":"There's possibly 41 numerical categories and 16 categorial features which could be treat as nominal. Given the size of the dataset, we'll probably employ OHE encoding, followed by PCA reduction.","eec1c9d5":"One way here would be to use LabelEncoder followed by OHE or to use DictVectorizer which both supports spare matrix output for high model training performance. Another possibility is to use dummy variables which is probably the most easist way of object encoding that I'll prefere here as the number of features isn't yet that huge.","1661b62c":"Money-related features would be are best guess, but let's iterate over the whole dataset as due to the Tukey method.","da0d53bd":"The precission tells a probability with a client will be classified correctly. The recall tells us that it predicted a payment difficulty of 73 % of the clients who actually had the payment difficulty.","64709af7":"### Obejct encoding","70004253":"Due to the large amount of data, I've imported all the tables into my local SQL Server instance and joined them into a view according to the data schema provided. That's not obviously possible here at Kaggle, so I've just used a single table to show you, how I would proceed in treating an imbalance dataset with a hefty amount of attribures. Thus, do not expect the model accuracy to be anywhere beyond random - this is only a methodical approache.\n\nWhat you'll essentialy see here is an object encoding followed by PCA reduction.","946bb4bf":"### ROC Curve ","752e25e5":"### Precision Recall Curve","e548955a":"This is probably the most important measure which is shared among most model-predicting technologies.\nThe more is the blue curve leaning towards upper-left corned (right angle), the better a model is in predicting actual results. ","5eb86dd7":"The F-score is computed as the harmonic mean of both precision and recall, thus high F1 score is possible only if both precesion and recall are high.","5f0154ca":"The number of features is medium high, so we'll do a general glance of the data and will not dig deep into invidual features.","e13bdb8e":"### ROC Score","6369034d":"### Concat data ","6ad47ee5":"### Outlier removal  ","54513faa":" We will feature engineer the column now here before we'll do outlier removal and dataset merge.","1128f519":"### Baseline","ddb821f9":"## Model Evaluation  ","ef75a999":"### Confusion Matrix ","af9dba19":"### Handle nulls ","0c2f2a89":"1. Load libraries\n2. Load data\n3. EDA\n4. Data preparation\n    * Anomaly detection\n    * Outlier detection\n    * Null handling\n5. Object encoding\n    * Option 1 - Dummy variables\n    * Option 2 - OneHotEncoding\n    * Standartization\n    * PCA\n6. Model Baseline\n7. Model Evaluation\n    * Confusion Matrix\n    * F-1 Score\n    * Precision and Recall\n    * Precision-Recall Curve\n    * ROC Curve\n    * AUROC Score","5d30ae7c":"Alternatively, we can calculate std and randomely generate a number within, which would allow us to set a higher threshold and potentially spare some features.","d73e66a7":"The value is quite frequently used for some reason, possible as a kind of a default value.","65527862":"## LOAD DATA ","356bae09":"### Standardize data ","27b3283c":"### F-1 Score","7cec6a17":"![](https:\/\/www.homecredit.co.id\/HCID\/media\/images\/HCID_logo.jpg)","488f72c7":"### Target distribution ","c16f202f":"That's an acceptable number.","f52a28bc":"Based on rapid recall curve fall is possible to set recall\/precision trade-off before."}}