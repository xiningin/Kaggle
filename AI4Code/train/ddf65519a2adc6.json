{"cell_type":{"a4e2c69b":"code","4b3aa507":"code","c260bb14":"code","f9bc70c9":"code","04a5af67":"code","4655cb76":"code","45a754cc":"code","497101e9":"code","b29d6e62":"code","3b958c1b":"code","629cd687":"code","82ccbbdd":"code","5f9db360":"code","dc30aea7":"code","d949be25":"code","2ba1dd3d":"code","5fa100a4":"markdown","43fcd1eb":"markdown","c410d97d":"markdown","288b3ba1":"markdown","223ea572":"markdown","9727958a":"markdown","519a5c76":"markdown","845a2da0":"markdown","3ae58ec8":"markdown","c17b0b80":"markdown","671ce5ad":"markdown","af23c2e6":"markdown","77a6adb0":"markdown","f5c4b9c8":"markdown","6782a35d":"markdown","3c53cafa":"markdown","41a196da":"markdown","60f526b0":"markdown","410343ce":"markdown","5702abf5":"markdown","cb2e4bc4":"markdown","99261d51":"markdown"},"source":{"a4e2c69b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\nimport seaborn as sns\n\n# plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.","4b3aa507":"# load data set\nx_l = np.load('..\/input\/Sign-language-digits-dataset\/X.npy')\ny_l = np.load('..\/input\/Sign-language-digits-dataset\/Y.npy')\nimg_size = 64\nplt.subplot(1,2,1)\nplt.imshow(x_l[260].reshape(img_size,img_size))\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.imshow(x_l[900].reshape(img_size,img_size))\nplt.axis('off')","c260bb14":"# from 0 to 204 is zero sign and from 205 to 410 is one sign \nX = np.concatenate((x_l[204:409],x_l[822:1027]),axis=0)\n# Now,we need to create label of zeros and ones.After that we concatenate them.\nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z,o),axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \" , X.shape)\nprint(\"Y shape: \" , Y.shape)","f9bc70c9":"# Now,lets create x_train, y_train, x_test, y_test arrays\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.15,random_state=42)\nnumber_of_train = x_train.shape[0]\nnumber_of_test = y_test.shape[0]\nprint(x_train.shape)\nprint(y_train.shape)","04a5af67":"x_train_flatten = x_train.reshape(number_of_train,x_train.shape[1]*x_train.shape[2])\nx_test_flatten = x_test.reshape(number_of_test,x_test.shape[1]*x_test.shape[2])\nprint(\"x train flatten\",x_train_flatten.shape)\nprint(\"x test flatten\",x_test_flatten.shape)","4655cb76":"x_train = x_train_flatten.T\nx_test = x_test_flatten.T\ny_train = y_train.T\ny_test = y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","45a754cc":"# lets initialize parameters\n# So what we need is dimension 4096 that is number of pixels as a parameter for our initialize method(def)\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b= 0.0\n    return w,b\n#w,b = initialize_weights_and_bias(4096)","497101e9":"# calculation of z\n#z = np.dot(w.T,x_train)+b\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","b29d6e62":"# In backward propagation we will use y_head that found in forward progation\n# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","3b958c1b":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","629cd687":"# prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","82ccbbdd":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 4096\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","5f9db360":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)","dc30aea7":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\nprint(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","d949be25":"y_pred = logreg.predict(x_test.T)\ny_pred","2ba1dd3d":"y_pred = logreg.predict(x_test.T)\ny_true = y_test.T\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\nf, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(cm, annot=True, linewidth=0.5, fmt=\".0f\",  cmap='RdPu', ax = ax)\nplt.xlabel = (\"y_pred\")\nplt.ylabel = (\"y_true\")\nplt.show()","5fa100a4":"<a id=\"13\"><\/a> <br>\n## Confusion Matrix","43fcd1eb":"<a id=\"2\"><\/a> <br>\n## Preparing Dataset\n\n* In this data there are 2062 sign language digits images.\n* we will use only sign 0 and 1 for simplicity. \n* In data, sign zero is between indexes 204 and 408. Number of zero sign is 205.\n* Also sign one is between indexes 822 and 1027. Number of one sign is 206. Therefore, we will use 205 samples from each classes(labels).\n* Lets prepare our X and Y arrays. X is image array (zero and one signs) and Y is label array (0 and 1).","c410d97d":"* Now lets put them all together.","288b3ba1":"<a id=\"6\"><\/a> <br>\n## Forward Propagation\n* The all steps from pixels to cost is called forward propagation\n    * z = (w.T)x + b => in this equation we know x that is pixel array, we know w (weights) and b (bias) so the rest is calculation. (T is transpose)\n    * Then we put z into sigmoid function that returns y_head(probability). When your mind is confused go and look at computation graph. Also equation of sigmoid function is in computation graph.\n    * Then we calculate loss(error) function. \n    * Cost function is summation of all loss(error).\n    * Lets start with z and the write sigmoid definition(method) that takes z as input parameter and returns y_head(probability)","223ea572":"* As you can see, we have 348 images and each image has 4096 pixels in image train array.\n* Also, we have 62 images and each image has 4096 pixels in image test array.\n* Then lets take transpose.(it depends on you but I use like that.)","9727958a":"<a id=\"9\"><\/a> <br>\n## Updating(Learning) Parameters Method","519a5c76":"<a id=\"8\"><\/a> <br>\n## Forward and Backward Propagation Method","845a2da0":"<a id=\"5\"><\/a> <br>\n## Initializing Parameters(weights and bias) Method\n* As you know input is our images that has 4096 pixels(each image in x_train).\n* Each pixels have own weights.\n* The first step is multiplying each pixels with their own weights.\n* The question is that what is the initial value of weights?\n    * There are some techniques that I will explain at artificial neural network but for this time initial weights are 0.01.\n    * Okey, weights are 0.01 but what is the weight array shape? As you understand from computation graph of logistic regression, it is (4096,1)\n    * Also initial bias is 0.","3ae58ec8":"<a id=\"10\"><\/a> <br>\n## Prediction Method","c17b0b80":"* Now we have 3 dimensional input array (X) so we need to make it flatten (2D) in order to use as input for our first deep learning model.\n* Our label array (Y) is already flatten(2D) so we leave it like that.\n* Lets flatten X array(images array).\n","671ce5ad":"<a id=\"11\"><\/a> <br>\n## Logistic Regression Method","af23c2e6":"# Orhan SERTKAYA\n<br>Content:\n* [Introduction](#1):\n* [Preparing Dataset](#2):\n* [Train-Test Split Data](#3):\n* [Computation Graph](#4):\n* [Initializing Parameters(weights and bias) Method](#5):\n* [Forward Propagation](#6):\n* [Sigmoid Function Method](#7):\n* [Forward and Backward Propagation Method](#8):\n* [Updating(Learning) Parameters Method](#9):\n* [Prediction Method](#10):\n* [Logistic Regression Method](#11):\n* [Logistic Regression with Scikit-Learn](#12):\n* [Confusion Matrix](#13):\n* [Conclusion](#14):","77a6adb0":"<a id=\"1\"><\/a> <br>\n# INTRODUCTION\n* In this kernel,we will learn how to use Logistic Regression Algorithm step by step.This kernel is also an introduction to deep learning.I will use the same dataset for introduction to deep learning(ANN).If you understand this topic comfortably, you will not have difficulty in the introduction to deep learning.\n* Logistic regression is actually a very simple neural network.\n* You can look at my another kernel that I wrote about logistic regression in detail.<br>\n* ==> <a href=\"https:\/\/www.kaggle.com\/orhansertkaya\/mush-classification-logistic-regression-algorithm\">Mushroom Classification and Logistic Regression Algorithm<\/a>\n* Let's start.","f5c4b9c8":"<a id=\"12\"><\/a> <br>\n## Logistic Regression with Scikit-Learn\n* In sklearn library, there is a logistic regression method that ease implementing logistic regression.\n* The accuracies are different from what we find. Because logistic regression method use a lot of different feature that we do not use like different optimization parameters or regularization.","6782a35d":"<a id=\"4\"><\/a> <br>\n##  Computation Graph\n* Now lets look at computation graph of logistic regression\n<a href=\"http:\/\/ibb.co\/c574qx\"><img src=\"http:\/\/preview.ibb.co\/cxP63H\/5.jpg\" alt=\"5\" border=\"0\"><\/a>\n    * Parameters are weight and bias.\n    * Weights: coefficients of each pixels\n    * Bias: intercept\n    * z = (w.t)x + b  => z equals to (transpose of weights times input x) + bias \n    * In an other saying => z = b + px1*w1 + px2*w2 + ... + px4096*w4096\n    * y_head = sigmoid(z)\n    * Sigmoid function makes z between zero and one so that is probability. You can see sigmoid function in computation graph.\n* Why we use sigmoid function?\n    * It gives probabilistic result\n    * It is derivative so we can use it in gradient descent algorithm (we will see as soon.)\n* Lets make example:\n    * Lets say we find z = 4 and put z into sigmoid function. The result(y_head) is almost 0.9. It means that our classification result is 1 with 90% probability.","3c53cafa":"<a id=\"14\"><\/a> <br>\n# Conclusion\n* If you like it, please upvote :)\n* If you have any question, I will be appreciate to hear it.","41a196da":"* The shape of the X is (410, 64, 64)\n    * 410 means that we have 410 images (zero and one signs)\n    * 64 means that our image size is 64x64 (64x64 pixels)\n* The shape of the Y is (410,1)\n    *  410 means that we have 410 labels (0 and 1) \n* Lets split X and Y into train and test sets.\n    * test_size = percentage of test size. test = 15% and train = 75%\n    * random_state = use same seed while randomizing. It means that if we call train_test_split repeatedly, it always creates same train and test distribution because we have same random_state.","60f526b0":"## Summary\nWhat we did at this part:\n* Initialize parameters weight and bias\n* Forward propagation\n* Loss function\n* Cost function\n* Backward propagation (gradient descent)\n* Prediction with learnt parameters weight and bias\n* Logistic regression with sklearn\n","410343ce":"<a id=\"3\"><\/a> <br>\n## Train-Test Split Data","5702abf5":"<a id=\"7\"><\/a> <br>\n## Sigmoid Function Method","cb2e4bc4":"* In order to create image array, I concatenate zero sign and one sign arrays\n* Then I create label array 0 for zero sign images and 1 for one sign images.","99261d51":"* Mathematical expression of log loss(error) function is that: \n    <a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/image.ibb.co\/eC0JCK\/duzeltme.jpg\" alt=\"duzeltme\" border=\"0\"><\/a>\n* It says that if you make wrong prediction, loss(error) becomes big. \n* The cost function is summation of loss function. Each image creates loss function. Cost function is summation of loss functions that is created by each input image.\n* Lets implement forward propagation."}}