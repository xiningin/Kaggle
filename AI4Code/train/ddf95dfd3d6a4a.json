{"cell_type":{"ea6e60aa":"code","2f3ff5e8":"code","5a600451":"code","387b65b5":"code","4e09a33b":"code","06e75ee2":"code","f73965da":"code","e4d0d585":"code","56dd20c0":"code","d5f78fb8":"code","608f60a9":"code","62eaf6de":"code","ffc074b5":"code","fb19dd69":"code","40fed3f4":"code","0415a6e9":"code","01a4f2ba":"code","bd20e0e2":"code","5ff33e5c":"code","88bc79ac":"code","8b2bee96":"code","72f692fd":"code","0cd018d1":"code","37a52c61":"code","0209b714":"code","f1e5b40b":"code","6c767155":"code","f0698755":"code","5c98c938":"code","703af5b5":"code","e5767198":"code","805e829c":"code","442195b6":"code","d0f934c9":"code","742b8d3b":"code","166e3702":"code","e4e18e5f":"code","64af460a":"code","ea477608":"code","9813974d":"code","a2062c66":"code","e00ce807":"code","14435a93":"code","431ceebb":"markdown","cae0015c":"markdown","d7abd8b7":"markdown","6214122b":"markdown","bdf56e8c":"markdown","6c101560":"markdown","e4bd17bc":"markdown","2d651377":"markdown","d7cb09f1":"markdown","73b088ad":"markdown","ef2c3a42":"markdown","eca32bfa":"markdown","09cedf8e":"markdown","677690b4":"markdown","87faa4e5":"markdown"},"source":{"ea6e60aa":"#importing necessary models and libraries\n\n#Math tools\nfrom scipy import stats\nfrom scipy.stats import skew,norm  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nimport scipy.stats as stats\n\n\n#Visualizing tools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#preprocessing tools\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\n#ML Algoirthm\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nimport sklearn.linear_model as linear_model\nfrom sklearn.svm import SVR\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom mlxtend.regressor import StackingCVRegressor","2f3ff5e8":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain_size = train.shape[0]\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","5a600451":"f,ax = plt.subplots(1,2,figsize=(16,6))\nsns.distplot(train['SalePrice'],fit=norm,ax=ax[0])\nsns.boxplot(train['SalePrice'])\nplt.show()\n\n#skewness and kurtosis\nprint(\"Skewness: {}\".format(train['SalePrice'].skew()))\nprint(\"Kurtosis: {}\".format(train['SalePrice'].kurt()))\nprint(\"--------------------------------------\")\nprint(train['SalePrice'].describe())","387b65b5":"f,ax = plt.subplots(1,2,figsize=(16,4))\nsns.boxplot(train['GrLivArea'],ax=ax[0])\nplt.scatter(train['GrLivArea'],train['SalePrice'])\nplt.xlabel('GrLiveArea')\nplt.ylabel('SalePrice')\nplt.show()\n","4e09a33b":"train.drop(train[train['GrLivArea']>4500].index,axis=0,inplace=True)","06e75ee2":"f,ax = plt.subplots(1,2,figsize=(16,4))\nsns.boxplot(train['GrLivArea'],ax=ax[0])\nplt.scatter(train['GrLivArea'],train['SalePrice'])\nplt.xlabel('GrLiveArea')\nplt.ylabel('SalePrice')\nplt.show()","f73965da":"f,ax = plt.subplots(1,2,figsize=(16,4))\nsns.boxplot(train['OverallQual'],ax=ax[0])\nplt.scatter(train['OverallQual'],train['SalePrice'])\nplt.xlabel('OverallQual')\nplt.ylabel('SalePrice')\nplt.show()","e4d0d585":"# Finding numeric features\nnumeric_cols = train.select_dtypes(exclude='object').columns\nnumeric_cols_length = len(numeric_cols)  \n\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\n\n# skiped Id and saleprice feature\nfor i in range(1,numeric_cols_length-1):\n    feature = numeric_cols[i]\n    plt.subplot(numeric_cols_length, 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', data=train)\n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n           \nplt.show()\n","56dd20c0":"corr = train.select_dtypes(include='number').corr()\nplt.figure(figsize=(16,6))\ncorr['SalePrice'].sort_values(ascending=False)[1:].plot(kind='bar')\nplt.tight_layout()","d5f78fb8":"# Correlation of top 10 feature with saleprice\ncorWithSalePrice = train.corr().nlargest(10,'SalePrice')['SalePrice'].index\nf , ax = plt.subplots(figsize = (18,12))\nax = sns.heatmap(train[corWithSalePrice].corr(), annot=True)\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.show()","608f60a9":"f,ax = plt.subplots(1,3,figsize=(16,4))\nsns.scatterplot('GrLivArea','TotRmsAbvGrd',data=train,ax=ax[0])\nsns.scatterplot('TotalBsmtSF','1stFlrSF',data=train,ax = ax[1])\nsns.scatterplot('GarageCars','GarageArea',data=train,ax = ax[2])\nplt.show()","62eaf6de":"f,ax = plt.subplots(1,3,figsize=(16,4))\nsns.scatterplot(x='TotalBsmtSF', y='SalePrice',data=train,ax=ax[0])\nsns.scatterplot(x='LotArea', y='SalePrice',data=train,ax=ax[1])\nsns.scatterplot(x='OverallQual', y='SalePrice',data=train,ax=ax[2])\nplt.show()\n","ffc074b5":"# After removing the two outliers, you can see that skewness is reduced. Still Saleprice is not normally distributed.\nf,ax = plt.subplots(1,2,figsize=(16,4))\nsns.distplot(train['SalePrice'],ax=ax[0],fit=norm)\nstats.probplot(train['SalePrice'],plot=plt)\nplt.show()\n\n#skewness and kurtosis\nprint(\"Skewness: {}\".format(train['SalePrice'].skew()))\nprint(\"Kurtosis: {}\".format(train['SalePrice'].kurt()))","fb19dd69":"#Log - transformation\ny = np.log1p(train['SalePrice'])\n\nf,ax = plt.subplots(1,2,figsize=(16,4))\nsns.distplot(y,fit=norm,ax=ax[0])\nstats.probplot(y,plot=plt)\nplt.show()\n\n#skewness and kurtosis\nprint(\"Skewness: {}\".format(y.skew()))\nprint(\"Kurtosis: {}\".format(y.kurt()))","40fed3f4":"def remove_overfit_features(df,weight):\n    overfit = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(df) * 100 > weight:\n            overfit.append(i)\n    overfit = list(overfit)\n    return overfit\n\n\noverfitted_features = remove_overfit_features(train,99)\ntrain.drop(overfitted_features,inplace=True,axis=1)\ntest.drop(overfitted_features,inplace=True,axis=1)\n","0415a6e9":"train_labels = y\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\n\n# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)","01a4f2ba":"all_features.drop('Id',inplace=True,axis=1)\nall_features.shape","bd20e0e2":"#visualize missing data\nmissing_value = all_features.isnull().sum().sort_values(ascending=False) \/ len(all_features) * 100\nmissing_value = missing_value[missing_value != 0]\nmissing_value = pd.DataFrame({'Missing value' :missing_value,'Type':missing_value.index.map(lambda x:all_features[x].dtype)})\nmissing_value.plot(kind='bar',figsize=(16,4))\nplt.show()","5ff33e5c":"print(\"Total No. of missing value {} before Imputation\".format(sum(all_features.isnull().sum())))\ndef fill_missing_values():\n \n    fillSaleType = all_features[all_features['SaleCondition'] == 'Normal']['SaleType'].mode()[0]\n    all_features['SaleType'].fillna(fillSaleType,inplace=True)\n\n    fillElectrical = all_features[all_features['Neighborhood']=='Timber']['Electrical'].mode()[0]\n    all_features['Electrical'].fillna(fillElectrical,inplace=True)\n\n    exterior1_neighbor = all_features[all_features['Exterior1st'].isnull()]['Neighborhood'].values[0]\n    fillExterior1 = all_features[all_features['Neighborhood'] == exterior1_neighbor]['Exterior1st'].mode()[0]\n    all_features['Exterior1st'].fillna(fillExterior1,inplace=True)\n\n    exterior2_neighbor = all_features[all_features['Exterior2nd'].isnull()]['Neighborhood'].values[0]\n    fillExterior2 = all_features[all_features['Neighborhood'] == exterior1_neighbor]['Exterior1st'].mode()[0]\n    all_features['Exterior2nd'].fillna(fillExterior2,inplace=True)\n\n    bsmtNeigh = all_features[all_features['BsmtFinSF1'].isnull()]['Neighborhood'].values[0]\n    fillBsmtFinSf1 = all_features[all_features['Neighborhood'] == bsmtNeigh]['BsmtFinSF1'].mode()[0]\n    all_features['BsmtFinSF1'].fillna(fillBsmtFinSf1,inplace=True)\n\n    kitchen_grade = all_features[all_features['KitchenQual'].isnull()]['KitchenAbvGr'].values[0]\n    fillKitchenQual = all_features[all_features['KitchenAbvGr'] == kitchen_grade]['KitchenQual'].mode()[0]\n    all_features['KitchenQual'].fillna(fillKitchenQual,inplace=True)\n        \n    all_features['MSZoning'] = all_features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n       \n    all_features['LotFrontage'] = all_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n    \n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','BsmtQual', 'BsmtCond', 'BsmtExposure',\n                'BsmtFinType1', 'BsmtFinType2','PoolQC']:\n        all_features[col] = all_features[col].fillna('None')\n    \n    categorical_cols =  all_features.select_dtypes(include='object').columns\n    all_features[categorical_cols] = all_features[categorical_cols].fillna('None')\n    \n    numeric_cols = all_features.select_dtypes(include='number').columns\n    all_features[numeric_cols] = all_features[numeric_cols].fillna(0)\n    \n    all_features['Shed'] = np.where(all_features['MiscFeature']=='Shed', 1, 0)\n    \n    #GarageYrBlt -  missing values there for the building which has no Garage, imputing 0 makes huge difference with other buildings,\n    #imputing mean doesn't make sense since there is no Garage. So we'll drop it\n    all_features.drop(['GarageYrBlt','MiscFeature'],inplace=True,axis=1)\n    \n    all_features['QualitySF'] = all_features['GrLivArea'] * all_features['OverallQual']\n\nfill_missing_values()\n\nprint(\"Total No. of missing value {} after Imputation\".format(sum(all_features.isnull().sum())))","88bc79ac":"all_features = all_features.drop(['PoolQC',], axis=1)","8b2bee96":"# converting some numeric features to string\nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)\n\n\n# Filter the skewed features\nnumeric = all_features.select_dtypes(include='number').columns\nskew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","72f692fd":"# Normalize skewed features using boxcox\nfor i in skew_index:\n    all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1))","0cd018d1":"all_features['YearsSinceRemodel'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\nall_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\n\nall_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']\nall_features['YrBltAndRemod'] = all_features['YearBuilt'] + all_features['YearRemodAdd']\nall_features['BsmtFinType1_Unf'] = 1*(all_features['BsmtFinType1'] == 'Unf')\nall_features['Total_sqr_footage'] = (all_features['BsmtFinSF1'] + all_features['BsmtFinSF2'] +\n                                 all_features['1stFlrSF'] + all_features['2ndFlrSF'])\nall_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +\n                               all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))\nall_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +\n                              all_features['EnclosedPorch'] + all_features['ScreenPorch'] +\n                              all_features['WoodDeckSF'])\n\nall_features['TotalBsmtSF'] = all_features['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['2ndFlrSF'] = all_features['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\nall_features['GarageArea'] = all_features['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['BsmtFinSF1'] = all_features['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)","37a52c61":"def booleanFeatures(columns):\n    for col in columns:\n        all_features[col+\"_bool\"] = all_features[col].apply(lambda x: 1 if x > 0 else 0)\nbooleanFeatures(['GarageArea','TotalBsmtSF','2ndFlrSF','Fireplaces','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch'])  \n","0209b714":"def logs(columns):\n    for col in columns:\n        all_features[col+\"_log\"] = np.log(1.01+all_features[col])  \n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','MiscVal','YearRemodAdd','TotalSF']\n\nlogs(log_features)\n","f1e5b40b":"def squares(columns):\n    for col in columns:\n        all_features[col+\"_sq\"] =  all_features[col] * all_features[col]\n\nsquared_features = ['GarageCars_log','YearRemodAdd', 'LotFrontage_log', 'TotalBsmtSF_log', '2ndFlrSF_log', 'GrLivArea_log' ]\n\nsquares(squared_features)","6c767155":"# There is a natural order in their values for few categories, so converting them to numbers gives more meaning\nquality_map = {'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5}\nquality_cols = ['BsmtQual', 'BsmtCond','ExterQual', 'ExterCond','FireplaceQu','GarageQual', 'GarageCond','KitchenQual','HeatingQC']\nfor col in quality_cols:\n    all_features[col] = all_features[col].replace(quality_map)\n\nall_features['BsmtExposure'] = all_features['BsmtExposure'].replace({\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3})\n\nall_features[\"PavedDrive\"] =all_features[\"PavedDrive\"].replace({\"N\" : 0, \"P\" : 1, \"Y\" : 2})\n\nbsmt_ratings = {\"None\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6}\nbsmt_col = ['BsmtFinType1','BsmtFinType2']\nfor col in bsmt_col:\n    all_features[col] = all_features[col].replace(bsmt_ratings)\n\n    \nall_features[\"OverallScore\"]   = all_features[\"OverallQual\"] * all_features[\"OverallCond\"]\nall_features[\"GarageScore\"]    = all_features[\"GarageQual\"] * all_features[\"GarageCond\"]\nall_features[\"ExterScore\"]     = all_features[\"ExterQual\"] * all_features[\"ExterCond\"]\n","f0698755":"all_features = pd.get_dummies(all_features).reset_index(drop=True)","5c98c938":"X = all_features.iloc[:len(train_labels), :]\nX_test = all_features.iloc[len(train_labels):, :]\n\noutliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ntrain_labels = train_labels.drop(y.index[outliers])\n","703af5b5":"overfitted_features = remove_overfit_features(X,99)\n\nX = X.drop(overfitted_features, axis=1)\nX_test = X_test.drop(overfitted_features, axis=1)","e5767198":"kf = KFold(n_splits=12, random_state=42, shuffle=True)\n\n# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","805e829c":"# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006,random_state=42)\n\n# StackingCVRegressor \nstackReg = StackingCVRegressor(regressors=(xgboost, svr, ridge, gbr),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True,random_state=42)","442195b6":"model_score = {}\n\nscore = cv_rmse(lightgbm)\nlgb_model_full_data = lightgbm.fit(X, train_labels)\nprint(\"lightgbm: {:.4f}\".format(score.mean()))\nmodel_score['lgb'] = score.mean()","d0f934c9":"score = cv_rmse(xgboost)\nxgb_model_full_data = xgboost.fit(X, train_labels)\nprint(\"xgboost: {:.4f})\".format(score.mean()))\nmodel_score['xgb'] = score.mean()","742b8d3b":"score = cv_rmse(svr)\nsvr_model_full_data = svr.fit(X, train_labels)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nmodel_score['svr'] = score.mean()\n","166e3702":"score = cv_rmse(ridge)\nridge_model_full_data = ridge.fit(X, train_labels)\nprint(\"ridge: {:.4f}\".format(score.mean()))\nmodel_score['ridge'] =  score.mean()\n","e4e18e5f":"score = cv_rmse(gbr)\ngbr_model_full_data = gbr.fit(X, train_labels)\nprint(\"gbr: {:.4f}\".format(score.mean()))\nmodel_score['gbr'] =  score.mean()\n","64af460a":"stack_reg_model = stackReg.fit(np.array(X), np.array(train_labels))","ea477608":"def blended_predictions(X,weight):\n    return ((weight[0] * ridge_model_full_data.predict(X)) + \\\n            (weight[1] * svr_model_full_data.predict(X)) + \\\n            (weight[2] * gbr_model_full_data.predict(X)) + \\\n            (weight[3] * xgb_model_full_data.predict(X)) + \\\n            (weight[4] * lgb_model_full_data.predict(X)) + \\\n            (weight[5] * stack_reg_model.predict(np.array(X))))\n","9813974d":"# Blended model predictions\nblended_score = rmsle(train_labels, blended_predictions(X,[0.15,0.2,0.1,0.15,0.1,0.3]))\nprint(\"blended score: {:.4f}\".format(blended_score))\nmodel_score['blended_model'] =  blended_score\n","a2062c66":"pd.Series(model_score).sort_values(ascending=True)","e00ce807":"# Read submission csv\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\n# Predictions\nsubmission.iloc[:,1] = np.floor(np.expm1(blended_predictions(X_test,[0.15,0.2,0.1,0.15,0.1,0.3])))","14435a93":"# Write to csv\nsubmission.to_csv(\"submission\", index=False)","431ceebb":"#### 1. Target Variable Distribution\nOur first step in Machine Learning should always be analyzing the target variable. Saleprice is our given target\/dependent variable. Let's analyse its distribution","cae0015c":"#### 9. Predictions","d7abd8b7":"#### 6. Feature Transformation","6214122b":"#### 2. Feature visualization with Target variable\n\nLet's analyse how the other independent\/predictor\/input variable correleated with the Target variable","bdf56e8c":"#### 4. Fix Skewed features","6c101560":"#### This kernel is inspired from \n\n###### Serigne - https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard \n###### Alex - https:\/\/www.kaggle.com\/itslek\/blend-stack-lr-gb-0-10649-house-prices-v57\/data?scriptVersionId=11189608\n\nThanks for your beautiful work on Machine Learning and Kaggle platform.\n\nPlease Upvote this kernel, if you Like it.","e4bd17bc":"##### Observations:\n\n- MSSubClass,MoSold,YrSold - patterns shows it as a category and description meant the same\n- OverallQual, OverallCond - Ordered value (like ratings)\n- BsmtFullBath,BsmtHalfBath,FullBath,HalfBath,Fireplaces,BedroomAbvGr,KitchenAvbGr - discrete value(no. of bathrooms)\n","2d651377":"#### 3. Imputing missing values","d7cb09f1":"##### Observations:\n- Target variable is not normally distributed.\n- It is rightly skewed. \n- Average sell price is 180921 USD which pulled towards outliers values at the upper end.\n- Median 163000 USD which is lower than Mean value.\n- It has couple of outliers at the upper end.","73b088ad":"#### 8. Predictions","ef2c3a42":"Before getting started, this is my first kernel, please give it an upvote if you like it.\n#### Competition description:\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n","eca32bfa":"#### 5. Feature Creation","09cedf8e":"## Let's get started\n#### 1. Target variable distribution\n#### 2. Feature visualization with Target variable\n#### 3. Imputing missing values\n#### 4. Fix Skewed features\n#### 5. Feature Creation\n#### 6. Feature Transformation\n#### 7. Training a model\n#### 8. Prediction\n#### 9. Submission\n","677690b4":"##### Observations\n\n- OverallQual, GrLivArea , GarageCars , GarageArea and TotalBsmtSF are strongly correlated with the SalePrice.\n- GarageCas and GarageArea are strongly correlated this is because of parking the Garage cars at the GarageArea.\n- TotRmsAbvGrd and GrLivArea are strongly correleated. When the Ground area increases count of rooms also increases.\n- TotalBsmtSF and 1stFlrSF are strongly correleated.","87faa4e5":"#### 7. Training a model"}}