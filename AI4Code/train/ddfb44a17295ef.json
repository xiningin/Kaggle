{"cell_type":{"c4b6d242":"code","1510913b":"code","47357fc3":"code","e731070d":"code","ec76bc96":"code","bb8a7400":"code","b7cd0d48":"code","589bcf25":"code","36127ed8":"code","278f5fa0":"code","68c3b5a7":"code","452221f0":"code","7d9dcb43":"code","07b961fb":"code","a65c6557":"code","d9306f1a":"code","24e5fa1a":"code","b498a4cb":"code","455e0817":"code","4d068cb1":"code","7b610f7b":"code","ab829c72":"code","588ac605":"markdown","50708780":"markdown","2c48d1b0":"markdown","a933b822":"markdown","6e72f8a6":"markdown","d347397d":"markdown","30445302":"markdown","a4e6eefe":"markdown","586babaf":"markdown","c79eea87":"markdown","456fa247":"markdown","af6ac5e7":"markdown","1a772a60":"markdown","c123d68e":"markdown","08e35136":"markdown","568bb4eb":"markdown","1ed339ee":"markdown","07a02c85":"markdown","b7b30481":"markdown","37780ff1":"markdown","eb3ff2ae":"markdown"},"source":{"c4b6d242":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1510913b":"#import libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split,cross_val_score,RandomizedSearchCV\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nimport lightgbm as lgb\npd.set_option('display.max_rows', None)\nplt.rcParams['figure.figsize']=(16, 8.27) #set graphs size to A4 dimensions\nsns.set_style('darkgrid')","47357fc3":"dataset=pd.read_csv('\/kaggle\/input\/usa-cers-dataset\/USA_cars_datasets.csv')","e731070d":"dataset.info()","ec76bc96":"dataset.head(10)","bb8a7400":"dataset.drop(['Unnamed: 0','vin','lot'],axis=1,inplace=True)","b7cd0d48":"dataset['value']= dataset['condition'] .str.split(' ').str[0]\ndataset['days']= dataset['condition'] .str.split(' ').str[1]\n\ndef days_to_min_converter(time):\n    return int(time)*1440\n\ndef hours_to_min_converter(time):\n    return int(time)*60\n\n\ntemp_data=pd.concat([dataset[dataset['days']=='days']['value'].apply(days_to_min_converter),\n           dataset[dataset['days']=='hours']['value'].apply(hours_to_min_converter),\n           dataset[dataset['days']=='minutes']['value'].astype(int)]).rename('Minutes_Left',inplace=True)\n\n\n\ndataset=pd.concat([dataset,temp_data],axis=1)\ndataset['Minutes_Left'].fillna(-200,inplace=True)\n\ndataset.drop(['condition','value','days'],axis=1,inplace=True)","589bcf25":"def year_transform(year):\n    return 2021-year\n\ndataset['year']=dataset['year'].apply(year_transform)","36127ed8":"dataset['miles\/year']=dataset['mileage']\/dataset['year']","278f5fa0":"categorical_features=[feature for feature in dataset.columns if dataset[feature].dtype=='O']\n\nnumerical_features=[feature for feature in dataset.columns if dataset[feature].dtype!='O']","68c3b5a7":"dataframes=[]\nfor feature in categorical_features:\n    dataframe=dataset[feature].value_counts().rename_axis(feature).reset_index(name='counts')\n    dataframes.append(dataframe)\n\nfor i in range(len(dataframes)):\n    print(dataframes[i],'\\n')","452221f0":"for feature in numerical_features:\n    sns.distplot(dataset[feature])\n    plt.show()","7d9dcb43":"X=dataset.drop('price',axis=1)\ny=dataset['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n\ntrain_set=pd.concat([X_train,y_train],axis=1)\ntest_set=pd.concat([X_test,y_test],axis=1)","07b961fb":"for feature in categorical_features:\n    feature_labels=train_set.groupby(feature)['price'].mean().sort_values().index\n    feature_labels={k:i for i,k in enumerate(feature_labels,0)}\n    train_set[feature]=train_set[feature].map(feature_labels)\n    test_set[feature]=test_set[feature].map(feature_labels)\n\ntest_set.dropna(inplace=True)\n\nscaler=StandardScaler()\n\nscaled_X_train=pd.DataFrame(scaler.fit_transform(train_set.drop('price',axis=1)), columns=X_train.columns)\nscaled_X_train.index=train_set.index\nscaled_X_test=pd.DataFrame(scaler.transform(test_set.drop('price',axis=1)), columns=X_test.columns)\nscaled_X_test.index=test_set.index\n\n\nscaled_train=pd.concat([scaled_X_train,train_set['price']],axis=1)\nscaled_test=pd.concat([scaled_X_test,test_set['price']],axis=1)","a65c6557":"reg=RandomForestRegressor()\nreg.fit(scaled_train.drop('price',axis=1),scaled_train['price'])\n\nfeat_importances = pd.Series(reg.feature_importances_, index=scaled_train.drop('price',axis=1).columns)\nfeat_importances.nlargest(scaled_train.drop('price',axis=1).shape[1]).plot(kind='barh')\nplt.show()","d9306f1a":"scaled_train.drop('country',axis=1,inplace=True)\nscaled_test.drop('country',axis=1,inplace=True)","24e5fa1a":"X_train=scaled_train.drop('price',axis=1)\ny_train=scaled_train['price']\n\nX_test=scaled_test.drop('price',axis=1)\ny_test=scaled_test['price']\n\n\n\n\nlm=LinearRegression()\nsvr=SVR()\nrf=RandomForestRegressor()\nxgb_reg=xgb.XGBRegressor()\nlgb_reg=lgb.LGBMRegressor()\n\n\nscore_lm=cross_val_score(lm,X_train,y_train,cv=10,scoring='neg_mean_squared_error')\nscore_svr=cross_val_score(svr,X_train,y_train,cv=10,scoring='neg_mean_squared_error')\nscore_rf=cross_val_score(rf,X_train,y_train,cv=10,scoring='neg_mean_squared_error')\nscore_xgb_reg=cross_val_score(xgb_reg,X_train,y_train,cv=10,scoring='neg_mean_squared_error')\nscore_lgb_reg=cross_val_score(lgb_reg,X_train,y_train,cv=10,scoring='neg_mean_squared_error')\n\n\nscores=pd.DataFrame({'Model':['Linear Regression','SVR','Random Forest','XGBoost','LightGBM'],\n                    'Mean Squared Error':[-score_lm.mean(),-score_svr.mean(),-score_rf.mean(),\n                                           -score_xgb_reg.mean(),-score_lgb_reg.mean()]})\n\nscores","b498a4cb":"rf.fit(X_train,y_train)\ny_pred_rf=rf.predict(X_test)\n\nprint('MSE: ',mean_squared_error(y_test,y_pred_rf))\nprint('R2: ',r2_score(y_test,y_pred_rf))\nprint('MAE: ',mean_absolute_error(y_test,y_pred_rf))\nprint('RMSE: ',np.sqrt(mean_squared_error(y_test,y_pred_rf)))","455e0817":"n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1,2,4,8,16,32]\n\n#learning_rate = [0.1, 0.01, 0.001]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               #'learning_rate':learning_rate,\n               'min_samples_leaf':min_samples_leaf,\n               'bootstrap': bootstrap}\n\n\n\n\nrandom_rf=RandomizedSearchCV(rf,cv=10,param_distributions=random_grid,scoring='neg_mean_squared_error',n_jobs=-1,verbose=1)\nrandom_rf.fit(X_train,y_train)\n","4d068cb1":"best_rf=random_rf.best_estimator_\nbest_rf.fit(X_train,y_train)\ny_pred_bestrf=best_rf.predict(X_test)\n\nprint('MSE: ',mean_squared_error(y_test,y_pred_bestrf))\nprint('R2: ',r2_score(y_test,y_pred_bestrf))\nprint('MAE: ',mean_absolute_error(y_test,y_pred_bestrf))\nprint('RMSE: ',np.sqrt(mean_squared_error(y_test,y_pred_bestrf)))","7b610f7b":"sns.distplot(y_test-y_pred_bestrf)\nplt.show()","ab829c72":"sns.scatterplot(y_test,y_pred_bestrf)\nplt.show()","588ac605":"#### \u0395valuating best model's performance according to hyperparameter tuning","50708780":"We are going to evaluate and apply Hyper Parameter tuning for Random Forest only.","2c48d1b0":"# US Cars Dataset Price Prediction","a933b822":"Create new feature which represents Mileage per Year for each car.","6e72f8a6":"### Drop unnecessary columns ","d347397d":"# EDA","30445302":"We are going to extract how many minutes left from the \"Condition\" column.","a4e6eefe":"# MODEL SELECTION","586babaf":"# FEATURE SELECTION","c79eea87":"#### Hyperparameter tuning for Random Forest","456fa247":"Convert the Year column to represent which year of car's registration is in progress (e.g first, second, third, etc...)","af6ac5e7":"US Cars' data was scraped from AUCTION EXPORT.com. This dataset included Information about 28 brands of clean and used vehicles for sale in US. Twelve initial features were assembled for each car in the dataset.","1a772a60":"Create some visualizations and print some information about features distribution","c123d68e":"### Create and transform features ","08e35136":"#### Evaluating the model's performance on the test set","568bb4eb":"### SPLITTING DATASET TO TRAIN AND TEST SET AND APPLYING SOME FEATURE ENGINEERING TECHNIQUES","1ed339ee":"As we can see 'Country' is not a useful feature and we drop it from train and test set","07a02c85":"From the scatterplot above we see that relationship between actual and predicted values tends to be linear, so we have built a good model.","b7b30481":"### Now we are going to check feature importances using Random Forest ","37780ff1":"### APPLYING TARGET MEAN ENCODING FOR CATEGORICAL FEATURES AND SCALING THE TRAIN AND TEST SET","eb3ff2ae":"Find out numerical and categorical variables"}}