{"cell_type":{"f718e01b":"code","a7918528":"code","cff28bec":"code","20da2b7d":"code","d7997162":"code","d63c831a":"code","5e5b4ff6":"code","bf9e2485":"code","a5793879":"code","d210f282":"code","86644c78":"code","ea700e93":"code","616c1556":"code","7ca871d6":"code","88fa5f88":"code","c8159d48":"code","4d9d4e30":"code","d061ed1c":"code","6abc2014":"code","c394d2e1":"code","3f9072d3":"code","bf1ca726":"code","d3891f32":"code","95189aea":"code","28ee39cd":"code","f4c3c08e":"code","984fa8f7":"code","0937e89c":"code","23905d90":"code","95f897fe":"code","a1de4a19":"code","a9b72df7":"code","9e52c1d0":"code","e05ec661":"code","646373c2":"code","62b64a7e":"code","34d71eee":"code","56817f70":"code","434dd62f":"code","09efc1f6":"code","aa1b830a":"code","95231452":"code","bc25a79a":"code","6ddf4c93":"markdown","75e08494":"markdown","3d3873e6":"markdown","8474d61c":"markdown","91242163":"markdown","445b7c0d":"markdown","58b1ba37":"markdown","d49bd629":"markdown","46b47efe":"markdown","ea2bb4e9":"markdown","b79c6513":"markdown","c3f29a84":"markdown","20d0a5e7":"markdown","38614b14":"markdown","3841794d":"markdown","6fdbd5ad":"markdown","0302835d":"markdown","4b0691ae":"markdown","7de85532":"markdown","1db81dc3":"markdown","1981d85f":"markdown","0dcb93c3":"markdown","2cc6068a":"markdown"},"source":{"f718e01b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a7918528":"%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn import metrics","cff28bec":"df = pd.read_csv('\/kaggle\/input\/vehicle-dataset-from-cardekho\/car data.csv')","20da2b7d":"df.head()","d7997162":"print('Shape of the data: {}'.format(df.shape))","d63c831a":"print('Unique Seller Type:', df['Seller_Type'].unique())\nprint('Unique Fuel Type:', df['Fuel_Type'].unique())\nprint('Unique Transmission Type:', df['Transmission'].unique())\nprint('Unique Owner Type:', df['Owner'].unique())","5e5b4ff6":"df.isnull().sum()","bf9e2485":"df.describe()","a5793879":"sns.barplot('Seller_Type','Selling_Price',data=df,palette='twilight')","d210f282":"sns.barplot('Transmission','Selling_Price',data=df,palette='spring')","86644c78":"sns.barplot('Fuel_Type','Selling_Price',data=df,palette='summer')","ea700e93":"sns.regplot('Selling_Price','Present_Price',data=df)","616c1556":"sns.regplot('Selling_Price','Kms_Driven',data=df)","7ca871d6":"sns.barplot('Owner','Selling_Price',data=df,palette='ocean')","88fa5f88":"Current_Year = 2020\n\ndf['Total_Years'] = Current_Year - df['Year']","c8159d48":"plt.figure(figsize=(10,5))\nsns.barplot('Total_Years','Selling_Price',data=df)","4d9d4e30":"final_dataset = df[['Year', 'Selling_Price', 'Present_Price', 'Kms_Driven',\n       'Fuel_Type', 'Seller_Type', 'Transmission', 'Owner']]","d061ed1c":"final_dataset['Current_Year'] = 2020\n\nfinal_dataset['Total_Years'] = final_dataset['Current_Year'] - final_dataset['Year']\n\nfinal_dataset.head()","6abc2014":"final_dataset.drop(['Year', 'Current_Year'], axis=1, inplace=True)\n\nfinal_dataset.head()","c394d2e1":"final_dataset = pd.get_dummies(final_dataset, drop_first=True) \n# dropping the first feature to prevent it from 'Dummy Variable Trap'","3f9072d3":"final_dataset.head()","bf1ca726":"final_dataset.corr()","d3891f32":"sns.pairplot(final_dataset)","95189aea":"# get correlations of each features in dataset\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n\n# plot heat map\ng = sns.heatmap(df[top_corr_features].corr(), annot=True, cmap='RdYlGn')","28ee39cd":"X = final_dataset.iloc[:, 1:] # independent feature\ny = final_dataset.iloc[:, 0] # dependent feature","f4c3c08e":"X.head()","984fa8f7":"y.head()","0937e89c":"model = ExtraTreesRegressor()\n\nmodel.fit(X,y)","23905d90":"print(model.feature_importances_)","95f897fe":"feature_imp = pd.Series(model.feature_importances_, index=X.columns)\n\nfeature_imp.nlargest(5).plot(kind='barh')\nplt.show()","a1de4a19":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nrf_reg = RandomForestRegressor()","a9b72df7":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start=100, stop=1200, num=12)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num=6)]\n# max_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","9e52c1d0":"random_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","e05ec661":"rf = RandomForestRegressor()","646373c2":"# Random search of parameters, using 3 fold cross-validation\n# search across 100 different combinations\n\nrf_random = RandomizedSearchCV(estimator=rf, \n                               param_distributions=random_grid, \n                               scoring='neg_mean_squared_error', \n                               n_iter=10, cv=5, verbose=2, \n                               random_state=42, n_jobs=1)","62b64a7e":"rf_random.fit(X_train, y_train)","34d71eee":"rf_random.best_params_","56817f70":"rf_random.best_score_","434dd62f":"predictions = rf_random.predict(X_test)","09efc1f6":"sns.distplot(y_test - predictions)","aa1b830a":"sns.scatterplot(y_test, predictions)","95231452":"print('R2 Score: ', metrics.r2_score(y_test,predictions))","bc25a79a":"print('MAE: ', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE: ', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, predictions)))","6ddf4c93":"## Fuel Type","75e08494":"## Present Price","3d3873e6":"# EDA","8474d61c":"# Model Training","91242163":"We need to engineer a Year feature. So, let's get the total number of years of vehicle by subtracting the year feature from a new feature `Current_Year` which we will make. Also name this feature `Total_Years`.","445b7c0d":"## Feature Importance","58b1ba37":"## Car Age","d49bd629":"## Hyperparameter Tuning","46b47efe":"Here as we can see that `Fuel_Type`, `Seller_Type`, `Transmission` and `Owner` are `CATEGORICAL FEATURES`. So, let's print it's unique value.","ea2bb4e9":"## Categorical Features","b79c6513":"\nNow let's drop the `Year` feature and `Current_Year` feature as our new feature `Total_Year` already carry the information as them.","c3f29a84":"## Unique Values","20d0a5e7":"## Missing Values","38614b14":"That's great there is no missing values","3841794d":"Moving on, we will now convert categorical features into One-Hot Encoded.","6fdbd5ad":"## Owner","0302835d":"## Transmission","4b0691ae":"## Seller Type","7de85532":"# Model Evaluation","1db81dc3":"# Import Dependencies","1981d85f":"### Create Random Grid","0dcb93c3":"## Kms Driven","2cc6068a":"Let's drop the Car Name features as it is not going to be helpful for our model."}}