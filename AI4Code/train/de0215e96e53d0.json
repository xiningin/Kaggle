{"cell_type":{"4ccaf507":"code","d28b2581":"code","fbc47076":"code","db277658":"code","978566b5":"code","1f7770c4":"code","dc4f4fd7":"code","24deac7a":"code","963054a3":"code","9e0cb94c":"code","c2f3be4b":"code","541616af":"code","0d21f066":"code","8b0d6722":"code","8eea2d5a":"code","50d355c9":"code","7d05e600":"code","df3e2dc2":"code","0b6fdb3f":"code","d09574bc":"code","32293dfc":"code","b4da5347":"code","967ea61c":"code","4ba80c39":"code","d290300c":"code","96f3347b":"code","9fbc0421":"code","30fc1821":"code","4435412e":"code","5828034a":"code","88f2302d":"code","08b4d8f3":"code","00592865":"code","16806fbf":"code","a19a4aa8":"code","15500b69":"code","c04834f3":"code","a254cce0":"code","bd94a17e":"code","1fac9d09":"code","7fafe774":"code","65ba7ac0":"code","2c8de643":"markdown","d905f1a5":"markdown","27d7ed55":"markdown","1a0da31a":"markdown","b185b92d":"markdown","3fac00dc":"markdown","2df1cbf6":"markdown","7b2329f3":"markdown","9930974d":"markdown","c08cd01d":"markdown","3dbd94d4":"markdown","17c2865a":"markdown","29ddba05":"markdown","70864f1b":"markdown","d7a5b97b":"markdown","22ed1279":"markdown","fe98abe8":"markdown","bbc4e42f":"markdown","23cf54b0":"markdown","73b919d0":"markdown","aeec9380":"markdown","82307a3a":"markdown","1523d4cb":"markdown","806198a2":"markdown","b0b9d466":"markdown","82565611":"markdown","369bb670":"markdown","e1a86750":"markdown"},"source":{"4ccaf507":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d28b2581":"import os\nfrom IPython.display import Image\nImage(filename=\"\/kaggle\/input\/images\/img1.jpg\", width= 1300, height=300)","fbc47076":"# Uncomment and run the below line to fix import errors\n# !pip install networkx mlxtend seaborn wordcloud pillow ","db277658":"# for basic operations\nimport numpy as np\nimport pandas as pd\n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport networkx as nx\nfrom wordcloud import WordCloud\n# plt.style.use('fivethirtyeight')\npd.set_option('display.max_columns', None)\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom PIL import Image\nfrom sklearn.cluster import DBSCAN\n\n# for market basket analysis\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import fpgrowth, fpmax\n\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\")","978566b5":"data = pd.read_csv('\/kaggle\/input\/d\/santhoriyu\/market-basket-data\/Market_basket_data.csv', header = None)\ndata.sample(5)","1f7770c4":"data.describe(include='all')","dc4f4fd7":"mask = np.array(Image.open('\/kaggle\/input\/images\/img2.png'))\nplt.subplots(figsize=(10,8))\nwordcloud = WordCloud(\n                          background_color='White',\n                          mask = mask, contour_color='steelblue', contour_width=3, \n                          width=1500, margin=5,random_state=3,\n                          height=500\n                         ).generate(\" \".join(data[0]))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Frequent items word cloud', fontsize=20)\nplt.axis('off')\nplt.show()","24deac7a":"plt.rcParams['figure.figsize'] = (11,6)\nfig, (ax1, ax2) = plt.subplots(1, 2)\ndata[0].value_counts()[:20].plot(kind='bar', colormap = 'RdYlBu' , ax=ax1, ylabel='Count', title='Top 20 items vs Count');\ndata[0].value_counts()[-20:].plot(kind='bar', colormap = 'terrain' , ax=ax2, ylabel='Count', title='Least 20 items vs Count');","963054a3":"pd.DataFrame(data[0].value_counts()[:10]).style.background_gradient(cmap='plasma')","9e0cb94c":"plt.rcParams['figure.figsize'] = (11,11)\ndata['item'] = 'Item'\nitem = data.truncate(before = -1, after = 15)\n\nitem = nx.from_pandas_edgelist(item, source = 'item', target = 0, edge_attr = True)\n\ny = data[0].value_counts().head(50).to_frame()\n\npos = nx.spring_layout(item)\ncolor = plt.cm.Wistia(np.linspace(0, 15, 1))\nnx.draw_networkx_nodes(item, pos, node_size = 8500, node_color = color)\nnx.draw_networkx_edges(item, pos, width = 3, alpha = 0.6, edge_color = 'black')\nnx.draw_networkx_labels(item, pos, font_size = 10, font_family = 'sans-serif')\nplt.axis('off')\nplt.grid()\nplt.title('Top First Choices', fontsize = 15)\nplt.show()","c2f3be4b":"data['secondchoice'] = 'Second Choice'\nsecondchoice = data.truncate(before = -1, after = 15)\nsecondchoice = nx.from_pandas_edgelist(secondchoice, source = 'item', target = 1, edge_attr = True)\n\nplt.rcParams['figure.figsize'] = (11,11)\npos = nx.spring_layout(secondchoice)\ncolor = plt.cm.cool(np.linspace(0, 15, 1))\nnx.draw_networkx_nodes(secondchoice, pos, node_size = 6000, node_color = color)\nnx.draw_networkx_edges(secondchoice, pos, width = 3, alpha = 0.6, edge_color = 'brown')\nnx.draw_networkx_labels(secondchoice, pos, font_size = 13, font_family = 'sans-serif')\nplt.axis('off')\nplt.grid()\nplt.title('Top Second Choices', fontsize = 15)\nplt.show()","541616af":"data['thirdchoice'] = 'Third Choice'\nsecondchoice = data.truncate(before = -1, after = 10)\nsecondchoice = nx.from_pandas_edgelist(secondchoice, source = 'item', target = 2, edge_attr = True)\n\nplt.rcParams['figure.figsize'] = (7,7)\npos = nx.spring_layout(secondchoice)\ncolor = plt.cm.Set2(np.linspace(0, 15, 1))\nnx.draw_networkx_nodes(secondchoice, pos, node_size = 8000, node_color = color)\nnx.draw_networkx_edges(secondchoice, pos, width = 3, alpha = 0.6, edge_color = 'pink')\nnx.draw_networkx_labels(secondchoice, pos, font_size = 12, font_family = 'sans-serif')\nplt.axis('off')\nplt.grid()\nplt.title('Top Third Choices', fontsize = 15)\nplt.show()","0d21f066":"# making each customers shopping items an identical list\ntrans = []\nfor i in range(1, 7502):\n    trans.append([str(data.values[i,j]) for j in range(0, 20)])\n\n# conveting it into an numpy array\ntrans = np.array(trans)\n\n# checking the shape of the array\nprint(trans.shape)\ntrans","8b0d6722":"pd.set_option('display.max_columns', 20)\nte = TransactionEncoder()\ndata = te.fit_transform(trans)\ndata = pd.DataFrame(data, columns = te.columns_)\ndata.head()","8eea2d5a":"data.drop('nan', axis=1, inplace=True)\nlen(data.columns)","50d355c9":"frequent_itemsets_apriori = apriori(data, min_support = 0.003, use_colnames=True)\nfrequent_itemsets_apriori['length'] = frequent_itemsets_apriori['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets_apriori","7d05e600":"Apriori_rules_confidence = association_rules(frequent_itemsets_apriori, metric=\"confidence\", min_threshold=0.1)\nApriori_rules_confidence[\"antecedent_len\"] = Apriori_rules_confidence[\"antecedents\"].apply(lambda x: len(x))\nApriori_rules_confidence","df3e2dc2":"plt.rcParams['figure.figsize'] = (13,6)\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.suptitle('Apriori Top 15 Consequents')\nApriori_rules_confidence[\"consequents\"].value_counts()[:15].plot(kind='pie', colormap = 'RdYlBu' , ax=ax1, ylabel='');\nApriori_rules_confidence[\"consequents\"].value_counts()[:15].plot(kind='bar', colormap = 'cividis' , ax=ax2, ylabel='Number of Rules');","0b6fdb3f":"plt.rcParams['figure.figsize'] = (13,6)\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.suptitle('Apriori Top 15 Antecedents')\nApriori_rules_confidence[\"antecedents\"].value_counts()[:15].plot(kind='pie', colormap = 'RdYlBu' , ax=ax1, ylabel='');\nApriori_rules_confidence[\"antecedents\"].value_counts()[:15].plot(kind='bar', colormap = 'Set1' , ax=ax2, ylabel='Number of Rules');","d09574bc":"plt.rcParams['figure.figsize'] = (12,7)\nfig, axs = plt.subplots(2,2)\nfig.suptitle('Apriori - Statistics', fontsize=15)\nApriori_rules_confidence.confidence.plot(kind='hist', ax=axs[0,0], title = 'Confidence', colormap = 'Set2')\nApriori_rules_confidence.lift.plot(kind='hist', ax=axs[0,1], title = 'Lift', colormap = 'Set1')\nApriori_rules_confidence.leverage.plot(kind='hist', ax=axs[1,0], title = 'Leverage', colormap = 'seismic')\nApriori_rules_confidence.conviction.plot(kind='hist', ax=axs[1,1], title = 'Conviction', colormap = 'tab20');","32293dfc":"plt.rcParams['figure.figsize'] = (12,6)\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3)\nfig.suptitle('Apriori - Antecedent Stats Comparison',fontsize=15)\nApriori_rules_confidence.antecedent_len.value_counts().plot(kind='pie', \n                                                            autopct='%1.1f%%', colormap = 'Set3', \n                                                            ax = ax1, ylabel='Number of Rules');\nsns.boxplot(x=\"antecedent_len\", y=\"consequent support\", data=Apriori_rules_confidence, width=0.3, ax=ax2)\nsns.scatterplot(x = 'confidence', y = 'lift', data = Apriori_rules_confidence, hue = 'antecedent_len', ax=ax3);","b4da5347":"# getting th item sets with length = 2 and support more han 10%\nfrequent_itemsets_apriori[ (frequent_itemsets_apriori['length'] == 2) &\n                   (frequent_itemsets_apriori['support'] >= 0.035) ]","967ea61c":"frequent_itemsets_apriori[ frequent_itemsets_apriori['itemsets'] == {'eggs', 'mineral water'} ]","4ba80c39":"frequent_itemsets_apriori[ frequent_itemsets_apriori['itemsets'] == {'mineral water'} ]","d290300c":"frequent_itemsets_apriori[ frequent_itemsets_apriori['itemsets'] == {'chocolate'} ]","96f3347b":"Frequent_itemset_1 = fpgrowth(data, min_support=0.002, use_colnames=True)\nFrequent_itemset_1","9fbc0421":"rules_confidence = association_rules(Frequent_itemset_1, metric=\"confidence\", min_threshold=0.3)\nrules_confidence[\"antecedent_len\"] = rules_confidence[\"antecedents\"].apply(lambda x: len(x))\nrules_confidence","30fc1821":"def FP_growth_result(min_sup):\n    Frequent_itemset_1 = fpgrowth(data, min_support=min_sup, use_colnames=True)\n    rules_confidence = association_rules(Frequent_itemset_1, metric=\"confidence\", min_threshold=0.05)\n    rules_confidence[\"antecedent_len\"] = rules_confidence[\"antecedents\"].apply(lambda x: len(x))\n    rules_confidence['Confidence_bins'] = (pd.cut(rules_confidence.confidence,[0, 0.25, 0.5, 0.75, 1], labels=[0.25, 0.5, 0.75, 1], retbins=True, right=False))[0]\n    rules_confidence['lift_bins'] = (pd.cut(rules_confidence.lift,[0,1,2,3,4,5,6,7,8,9,10], labels=[1,2,3,4,5,6,7,8,9,10], retbins=True, right=False))[0]\n    return rules_confidence\nfor ind, sup_val in enumerate([0.003, 0.008, 0.01, 0.03]):\n    exec(f'rule_set_{ind+1} = FP_growth_result(sup_val)')\nrule_set_1.shape,rule_set_2.shape,rule_set_3.shape,rule_set_4.shape","4435412e":"fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n\nfig.suptitle('FP growth Different Support plot', fontsize=20)\n\nsns.lineplot(ax=axes[0, 0], x = 'index', y = 'Confidence_bins',marker='o',color='red', linewidth=2.5, data = pd.DataFrame(rule_set_1.Confidence_bins.value_counts().reset_index()))\naxes[0, 0].set(xlabel='Confidence', ylabel='Number or Rules', title = 'FP growth with Support of 0.3%');\naxes[0, 0].set_xticks([0.25,0.5,0.75,1.0])\naxes[0, 0].set_xticklabels(['0.25','0.5','0.75','1.0'])\n\nsns.lineplot(ax=axes[0, 1], x = 'index', y = 'Confidence_bins',marker='o',color='green', linewidth=2.5, data = pd.DataFrame(rule_set_2.Confidence_bins.value_counts().reset_index()))\naxes[0, 1].set(xlabel='Confidence', ylabel='Number or Rules', title = 'FP growth with Support of 0.8%');\naxes[0, 1].set_xticks([0.25,0.5,0.75,1.0])\naxes[0, 1].set_xticklabels(['0.25','0.5','0.75','1.0'])\n\nsns.lineplot(ax=axes[1, 0], x = 'index', y = 'Confidence_bins',marker='o',color='steelblue', linewidth=2.5, data = pd.DataFrame(rule_set_3.Confidence_bins.value_counts().reset_index()))\naxes[1, 0].set(xlabel='Confidence', ylabel='Number or Rules', title = 'FP growth with Support of 1%');\naxes[1, 0].set_xticks([0.25,0.5,0.75,1.0])\naxes[1, 0].set_xticklabels(['0.25','0.5','0.75','1.0'])\n\nsns.lineplot(ax=axes[1, 1], x = 'index', y = 'Confidence_bins',marker='o',color='purple', linewidth=2.5, data = pd.DataFrame(rule_set_4.Confidence_bins.value_counts().reset_index()))\naxes[1, 1].set(xlabel='Confidence', ylabel='Number or Rules', title = 'FP growth with Support of 3%')\naxes[1, 1].set_xticks([0.25,0.5,0.75,1.0])\naxes[1, 1].set_xticklabels(['0.25','0.5','0.75','1.0']);","5828034a":"plt.rcParams['figure.figsize'] = (11,7)\nrule_set_1.lift_bins.value_counts().plot(kind='line')\nrule_set_2.lift_bins.value_counts().plot(kind='line')\nrule_set_3.lift_bins.value_counts().plot(kind='line')\nrule_set_4.lift_bins.value_counts().plot(kind='line')\nplt.ylabel('Number or Rules')\nplt.xlabel('Lift')\nplt.title('Fp Growth with different support levels');\nplt.legend(['Support 0.3%', 'Support 0.8%','Support 1%','Support 3%']);","88f2302d":"rules_lift = association_rules(Frequent_itemset_1, metric=\"lift\", min_threshold=8)\nrules_lift","08b4d8f3":"rules_confidence[ (rules_confidence['antecedent_len'] >= 1) &\n       (rules_confidence['confidence'] > 0.3) &\n       (rules_confidence['lift'] > 8) ]","00592865":"Rules_spaghetti = rule_set_1[ rule_set_1['consequents'] == {'spaghetti'}].sort_values(by=['confidence', 'lift'], ascending=False).reset_index()\nRules_spaghetti.drop('index', axis=1, inplace=True)\nRules_spaghetti = Rules_spaghetti.iloc[[0, -1]]\nRules_spaghetti","16806fbf":"plt.rcParams['figure.figsize'] = (15,6)\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.suptitle('Spaghetti Rule comparison', fontsize= 15 )\nRules_spaghetti.confidence.plot(kind='bar',  colormap = 'Set3', ax=ax1, title = ' {ground beef, cereals -> spaghetti}',\n                                ylabel='Confidence')\nax1.set_xticklabels(['Rule 1','Rule 2'], rotation = '0')\nax1.set_ylim(0.0, 4.0)\nRules_spaghetti.lift.plot(kind='bar',colormap = 'Set1', ax=ax2, title = ' {cookies -> spaghetti}',\n                          ylabel='Lift' );\nax2.set_xticklabels(['Rule 1','Rule 2'], rotation = '0');","a19a4aa8":"def draw_graph(rules, rules_to_show):\n    G1 = nx.DiGraph()\n   \n    color_map=[]\n    N = 50\n    colors = np.random.rand(N)    \n    strs=['R0', 'R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'R9', 'R10', 'R11']   \n   \n   \n    for i in range (rules_to_show):      \n        G1.add_nodes_from([\"R\"+str(i)])\n\n        for a in rules.iloc[i]['antecedents']:\n\n            G1.add_nodes_from([a])\n\n            G1.add_edge(a, \"R\"+str(i), color=colors[i] , weight = 2)\n\n        for c in rules.iloc[i]['consequents']:\n\n                G1.add_nodes_from([c])\n\n                G1.add_edge(\"R\"+str(i), c, color=colors[i],  weight=2)\n \n    for node in G1:\n       found_a_string = False\n       for item in strs: \n           if node==item:\n                found_a_string = True\n       if found_a_string:\n            color_map.append('yellow')\n       else:\n            color_map.append('green')       \n \n \n   \n    edges = G1.edges()\n    colors = [G1[u][v]['color'] for u,v in edges]\n    weights = [G1[u][v]['weight'] for u,v in edges]\n    pos = nx.spring_layout(G1, k=16, scale=1)\n    nx.draw(G1, pos, node_color = color_map, edge_color=colors, width=weights, font_size=16, with_labels=False)            \n\n    for p in pos:  # raise text positions\n           pos[p][1] += 0.07\n    nx.draw_networkx_labels(G1, pos)\n    plt.title(f'Graph showing {rules_to_show} rules')\n    plt.show()\n","15500b69":"plt.rcParams['figure.figsize'] = (6,6)\ndraw_graph(rules_confidence, 6)","c04834f3":"draw_graph(rules_confidence, 3)","a254cce0":"draw_graph (rules_confidence, 8)","bd94a17e":"Frequent_itemset_x = fpgrowth(data, min_support=0.002, use_colnames=True)\nrules_confidence_x = association_rules(Frequent_itemset_x, metric=\"confidence\", min_threshold=0.3)\nrules_confidence_x[\"antecedent_len\"] = rules_confidence_x[\"antecedents\"].apply(lambda x: len(x))\nrules_confidence_x.antecedents = rules_confidence_x.antecedents.apply(lambda x : set(x))\nrules_confidence_x.consequents = rules_confidence_x.consequents.apply(lambda x : set(x))\nitemset_added = []\nfor cnt in range(rules_confidence_x.shape[0]):\n    itemset_added.append(rules_confidence_x.antecedents[cnt] | rules_confidence_x.consequents[cnt])\nrules_confidence_x['itemset'] = itemset_added\n\nitemset_added = []\nfor a1 in range(len(rules_confidence_x['itemset'].values)):\n    itemset_added.append((rules_confidence_x['itemset'].values[a1] in list(rules_confidence_x['antecedents'].values)))\nrules_confidence_x['Item_match'] = itemset_added\nrules_confidence_x.sample(5)","1fac9d09":"plt.rcParams['figure.figsize'] = (15,6)\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.suptitle('Fp_Growth - Rules Validation', fontsize= 15)\nrules_confidence_x.Item_match.value_counts().plot(kind='pie',ax=ax1,  \n                                                            autopct='%1.1f%%', colormap = 'Set3', title = 'Matched\/Unmatched Rules % Found in antecedents', ylabel='Number of Rules')\nax1.legend([\"Unmatched\", \"Matched\"]);\nrules_confidence_x[rules_confidence_x['Item_match'] == True].groupby('antecedent_len')['Item_match'].count().plot(kind='pie',ax=ax2,  \n                                                            autopct='%1.1f%%', colormap = 'Set3', title = 'Matched Rules based on item count', ylabel='Number of Rules');","7fafe774":"clustering = DBSCAN(eps=2, min_samples= 5).fit(data.values)\nclustering.labels_\nclusters = pd.Series(clustering.labels_)\ndata['Group'] = clusters\ndata.Group = data.Group.map({-1: 1, 0: 0})\ndata.Group.tolist()[:20]","65ba7ac0":"plt.rcParams['figure.figsize'] = (15,6)\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.suptitle('Clustering of Customers', fontsize = 15)\ndata.Group.value_counts().plot(kind='pie', autopct='%1.1f%%', colormap = 'Set3', ax=ax1, ylabel='% of Customers');\ndata.Group.value_counts().plot(kind='bar', colormap = 'Set1', ax=ax2, xlabel='Clusters', ylabel='Number of Customers' )\nax2.set_xticklabels(['Cluster 1','Cluster 2'], rotation = '0');","2c8de643":"**The support count for a customer to buy chocolate is quite high with 16%**","d905f1a5":"**We will be looking at the application of the Apriori and Fp growth algorithm to uncover associations between items. It works by looking for combinations of items that occur together frequently in transactions, providing information to understand the purchase behavior. The outcome of this type of technique is, in simple terms, a set of rules that can be understood as \u201cif this item, then that item\u201d.**","27d7ed55":"Encoding the dataset in True\/False or 0\/1 type data for training the Recommendation model","1a0da31a":"**Now with the help of custering we can divide the group into multiple clusters. This will make the chances of a person buying the recommended item is higher since the customer is from the same group.**\n\n## Improvements\n\nThere are many extra techniques that an be applied to Apriori to improve efficiency. Some of them are listed below.\n\n<b>Hashing<\/b>: reduce database scans\n\n<b>Transaction reduction<\/b>: remove infrequent transactions from further consideration\n\n<b>Partitioning<\/b>: possibly frequent must be frequent in one of the partition\n\n<b>Dynamic Itemset Counting<\/b>: reduce the number of passes over the data\n\n<b>Sampling<\/b>: pick up random samples\n\n### References\n\nhttps:\/\/iopscience.iop.org\/article\/10.1088\/1742-6596\/1230\/1\/012020\/pdf\n    \nhttps:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-recommendation-engine-python\/\n\nhttp:\/\/rasbt.github.io\/mlxtend\/api_subpackages\/mlxtend.frequent_patterns\/#apriori\n\nhttps:\/\/www.researchgate.net\/publication\/343558578_Market_Basket_Analysis_Recommendation_System_Using_Association_Rules\/references\n\nhttps:\/\/www.sciencedirect.com\/topics\/computer-science\/market-basket-analysis\n\nhttps:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.403.1361&rep=rep1&type=pdf\n\nhttp:\/\/www.i-csrs.org\/Volumes\/ijasca\/vol.1\/vol.1.2.5.November.09.pdf\n\nhttps:\/\/intelligentonlinetools.com\/blog\/2018\/02\/10\/how-to-create-data-visualization-for-association-rules-in-data-mining\/","b185b92d":"**Visualizing first three item choices**","3fac00dc":"## Association Mining","2df1cbf6":"Confidence is distributed around the value 0.1 to 0.5 and Lift is distributed around 1 to 4","7b2329f3":"# Introduction ","9930974d":"**The support count for a customer to buy mineral water is quite high with 23%**","c08cd01d":"## FP growth - Extract frequent itemsets for association rule mining","3dbd94d4":"## FP growth comparison for different Support values","17c2865a":"The Number of rules generated for 2 set items eg., (milk, tomatoes) are more compared to other sets.Followed by 1 set items in the above pie chart.\n\nIn the scatter plot for the lift and condience, the itemsets 2 and 3 have a higher confidence and lift\n\n**Filtering data based on item set length and support**","29ddba05":"# Algorithm Implementation\n\n## Item to Item Recommendation\n\n##  Support\n\nSupport is an indication of how frequently the itemset appears in the dataset.<br>\nIn other words, this is an indication of how popular an itemset is in a dataset\n\n\n$$Support({X} \t\\rightarrow {Y}) = \\frac{\\text{Transactions containing both X and Y}}{\\text{Total number of transactions}}$$\n\n## Confidence\n\nConfidence is an indication of how often the rule has been found to be true <br>\nIn other words, confidence says how likely item Y is purchased when item X is purchased\n\n$$Confidence(\\{X\\} \t\\rightarrow \\{Y\\}) = \\frac{\\text{Transactions containing both X and Y}}{\\text{Transactions containing X}}$$\n\n##  Lift\n\nLift is a metric to measure the ratio of X and Y occur together to X and Y occurrence if they were statistically independent. In other words, lift illustrates how likely item Y is purchased when item X is purchased, while controlling for how popular item Y is.\n\n* A Lift score that is close to 1 indicates that the antecedent and the consequent are independent and occurrence of antecedent has no impact on occurrence of consequent.\n\n* A Lift score that is bigger than 1 indicates that the antecedent and consequent are dependent to each other, and the occurrence of antecedent has a positive impact on occurrence of consequent.\n\n* A Lift score that is smaller than 1 indicates that the antecedent and the consequent are substitute each other that means the existence of antecedent has a negative impact to consequent or visa versa.\n\n***Therefore having lift bigger than 1 is critial for proving associations***\n\n$$Lift(X\t\\rightarrow Y) = Lift(Y\t\\rightarrow X) =\\frac{\\text{P(X and Y)}}{\\text{P(X)*P(Y)}}=  \\frac{Confidence(X \\rightarrow Y)}{Support(Y) } = \\frac{Confidence(Y \\rightarrow X)}{Support(X) }$$\n\n## Conviction\n\nConviction measures the implication strength of the rule from statistical independence\nConviction score is a ratio between the probability that X occurs without Y while they were dependent and the actual probability of X existence without Y. For instance; if (French fries) --> (beer) association has a conviction score of 1.8; the rule would be incorrect 1.8 times as often (80% more often) if the association between totally independent.\n\n$$Conviction(A \t\\rightarrow C) = \\frac{1-support(c)}{1-confidence(A \\rightarrow C)},   range[0,\\infty]$$\n\n\n## Consequents & Antecedents\n\nThe IF component of an association rule is known as the antecedent. The THEN component is known as the consequent. The antecedent and the consequent are disjoint; they have no items in common.\n\n## Implementation\n\n***We will be using library Mlxtend - The most widely used library for Association Rules Learning implementations***\n\n\nHow does Apriori Algorithm Work ?\nA key concept in Apriori algorithm is the anti-monotonicity of the support measure. It assumes that\n\nAll subsets of a frequent itemset must be frequent\nSimilarly, for any infrequent itemset, all its supersets must be infrequent too\n\nStep 1: Create a frequency table of all the items that occur in all the transactions.\n\nStep 2: We know that only those elements are significant for which the support is greater than or equal to the threshold support.\n\nStep 3: The next step is to make all the possible pairs of the significant items keeping in mind that the order doesn\u2019t matter, i.e., AB is same as BA.\n\nStep 4: We will now count the occurrences of each pair in all the transactions.\n\nStep 5: Again only those itemsets are significant which cross the support threshold\n\nStep 6: Now let\u2019s say we would like to look for a set of three items that are purchased together. We will use the itemsets found in step 5 and create a set of 3 items.","70864f1b":"<b>The association algorithm comes up with rules of the type \u201cA implies B.\u201d There are two quantities of interest to the user of this algorithm: support and confidence.\n\n\u25aa Support tells us the percentage of the transactions where the combination of items A and B occur together. It helps identify combinations that are sufficiently frequent to be of interest (e.g., purchasing fish alone or purchasing fish and lemons together).\n\n\u25aa Confidence tells us which percent of transactions that have an item A also have an item B (e.g., how many transactions that have fish also have lemons).\n\nTo use association rules, a user must provide the desired level of support and confidence for a rule to be considered interesting.\n\nFor instance, suppose we specify that for a rule to be interesting, it must have a support of 20 percent and a confidence of 70 percent. If found that in fact 40 percent of all transactions involved the combination fish and lemons, then the combination \u201cfish, lemons\u201d exceeds the minimum support of 20 percent.\n\nNow, if we found that 50 percent of all transactions involve fish then the combination \u201cfish, lemons\u201d has confidence (40 \/ 50) * 100 = 80 percent, so it also meets the minimum confidence criterion. Hence, \u201cfish implies lemons\u201d will be reported as an association rule.<\/b>\n\n## Rules Validation","d7a5b97b":"<b>Comparing the two rules for Spaghetti, \n\nFor the first rule {ground beef, cereals -> spaghetti} - Lift = 3.885 and Confidence = 0.676\n\nFor the second rule {cookies -> spaghetti} - Lift = 0.571 and Confidence = 0.099\n\nBased on the Confidence and the Lift value we can say that customer buying ground beef, cereals will most likely buy Spaghetti than a customer buying cookies<\/b>","22ed1279":"**Generating Association rules for Apriori itemset**","fe98abe8":"## Compare Association rules\n\n**Let us compare the assiociation rules for the product spaghetti**","bbc4e42f":"## Association rules","23cf54b0":"**Conditional Filtering of the rules based on confidence, lift and antecedent length**","73b919d0":"## Graphical Representation of Rules","aeec9380":"**The support count for eggs and mineral water together is 5%**","82307a3a":"# Data Pre-Processing","1523d4cb":"*Removing the Nan Column from the list of items*","806198a2":"**Association rules based on Lift as the minimum threshold**","b0b9d466":"## Clustering the Customers based on their transactions","82565611":"## The most demanded items in dataset \/ Top20","369bb670":"## Exploratory Data Analysis","e1a86750":"## **Mlxtend apriori**"}}