{"cell_type":{"446f27c8":"code","c51a013f":"code","c1b74896":"code","e03bd670":"code","a454cb65":"code","5f77d07d":"code","6f0830ec":"code","48b2a785":"code","fab9862a":"code","672bb2b2":"code","1eba2ef9":"code","16e9ae2f":"code","ae2dbada":"code","9aff12c2":"code","c6c280af":"code","48c1bfbe":"code","b8b3bcb9":"code","0d0c1005":"code","88beaf2f":"code","a4d2a89d":"code","2f3dc6ff":"code","eea8e10a":"code","c8c62c39":"code","240c262b":"code","a09cf91d":"code","f8d27065":"code","e3f17cf9":"code","64286547":"code","cbd98a6a":"code","190fb3d2":"code","51237009":"code","02227a0b":"code","d9f48360":"code","a9155e6c":"code","0bea779c":"code","745d692e":"code","fd4d7d65":"code","3a4ffeb4":"code","568cd72a":"code","6b57d69d":"code","e47ba0d2":"code","f406cd42":"code","0521c2b6":"code","53a5aeab":"code","530f1521":"code","0661bb3f":"code","115aef97":"code","f3c9b43c":"code","c5f31a73":"code","4b8b801f":"code","f9326a42":"code","69ea25ad":"code","87b420cf":"code","ca0cf9d7":"code","bbc9be80":"code","c81c73f1":"code","be9c804c":"code","9862d729":"code","359c1dd4":"code","31852069":"code","de21b283":"code","884472e0":"code","8de17ff5":"code","32bf5e9f":"code","7ed7109d":"markdown","af596dbe":"markdown","f17bdb1d":"markdown","82adc4b0":"markdown","97044c08":"markdown","a0824c35":"markdown","7b603dd8":"markdown","66fe614e":"markdown","410cf529":"markdown","dd9387c0":"markdown","5e7943a4":"markdown","7ab4583a":"markdown","9e428a26":"markdown","916556a6":"markdown","2fa32944":"markdown","97a3c60f":"markdown","a28729b9":"markdown","6ca90806":"markdown","8de7c329":"markdown","118d633b":"markdown","cb58db1e":"markdown","fc1de836":"markdown","0e7db525":"markdown","aefe0290":"markdown","85a076af":"markdown","c06e0d89":"markdown","0238b337":"markdown","080ee95c":"markdown","c654c5d2":"markdown","ecbaacac":"markdown","2e157a16":"markdown","f95397c3":"markdown","8b7e9d2c":"markdown","21457c33":"markdown","13e02ed6":"markdown","15fd5ca9":"markdown","d6ad2fc2":"markdown","294c1d83":"markdown","6e24efae":"markdown"},"source":{"446f27c8":"import pandas as pd \nimport  numpy as np\nfrom ds_exam import *","c51a013f":"france = pd.read_csv(\"\/kaggle\/input\/coronavirus-france-dataset\/patient.csv\")\ntunisia = pd.read_csv(\"..\/input\/coronavirus-tunisia\/Coronavirus_Tunisia.csv\")\njapan = pd.read_csv(\"\/kaggle\/input\/close-contact-status-of-corona-in-japan\/COVID-19_Japan_Mar_07th_2020.csv\")\nindonesia = pd.read_csv(\"\/kaggle\/input\/indonesia-coronavirus-cases\/patient.csv\")\nkorea = pd.read_csv(\"\/kaggle\/input\/coronavirusdataset\/PatientInfo.csv\")\nHubei = pd.read_csv(\"\/kaggle\/input\/covid19official\/Hubei.csv\")\noutside_Hubei = pd.read_csv(\"\/kaggle\/input\/covid19official\/outside_Hubei.csv\")","c1b74896":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ndatasets_name = [\"france\", \"tunisia\", \"japan\", \"indonesia\", \"korea\", \"Hubei\", \"outside_Hubei\"]\n\ngarbge = [print(\"\\n\"+datasets_name[i], [i for i in datasets[i].columns]) for i in range(len(datasets_name))]","e03bd670":"france.rename(columns={\"health\":\"severity_illness\",\"status\":\"treatment\",\"infection_reason\":\"infection_place\"}\n              , inplace = True)\n\ntunisia.rename(columns={\"date\":\"confirmed_date\", \"gender\":\"sex\", \"situation\":\"severity_illness\", \n                        \"return_from\":\"infection_place\", \"health\":\"background_diseases\"}, inplace = True)\n\njapan.rename(columns={\"No.\":\"id\", \"Fixed date\":\"confirmed_date\",\"Age\":\"age\", \"residence\":\"region\",\n                      \"Surrounding patients *\":\"infected_by\"}, inplace = True)\n\nindonesia.rename(columns={\"patient_id\":\"id\",\"gender\": \"sex\", \"province\":\"region\", \"hospital\":\"hospital_name\",\n                          \"contacted_with\":\"infected_by\", \"current_state\":\"status\"}, inplace = True)\n\nkorea.rename(columns={\"patient_id\":\"id\", \"disease\":\"background_diseases_binary\", \"state\":\"severity_illness\",\n                      \"province\":\"region\", \"infection_case\" :\"infection_place\",\n                      \"symptom_onset_date\":\"date_onset_symptoms\"}, inplace = True)\n\nHubei = Hubei.rename(columns={ \"province\":\"region\",\"date_confirmation\": \"confirmed_date\",\n                              \"chronic_disease_binary\":\"background_diseases_binary\", \n                              \"chronic_disease\":\"background_diseases\", \"outcome\":\"severity_illness\"})\n\noutside_Hubei = outside_Hubei.rename(columns={ \"province\":\"region\", \"date_confirmation\": \"confirmed_date\",\n                              \"chronic_disease_binary\":\"background_diseases_binary\", \n                              \"chronic_disease\":\"background_diseases\", \"outcome\":\"severity_illness\" })\n","a454cb65":"o = []\nfor i in range(len(datasets)):\n    print(datasets_name[i],datasets[i].shape)\n    o.append(datasets[i].shape[0])\nprint(\"\\nnum of i \" + str(sum(o)))","5f77d07d":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets)\nexam_df = Exam.df_exam_columns_dfs(datasets,datasets_name,columns_name)","6f0830ec":"def full_common(exam_df):\n    \"\"\"\n    Returns columns that all DATASETS have\n    \"\"\"\n    full_common = []\n    for j in exam_df.columns:\n        boolyan = exam_df[j].all()\n        if boolyan == True:\n            full_common.append(j)\n    return full_common","48b2a785":"common = []\nunique = []\nblank = []\nfor i in exam_df.columns:\n    if exam_df[i].value_counts()[True]>1:\n        common.append(i)\n    elif exam_df[i].value_counts()[True]==1:\n        unique.append(i)\n    else:\n        blank.append(i)\n        \n        \nprint(common)\nprint(unique)\nprint(blank)   ","fab9862a":"for x in [outside_Hubei, Hubei]:\n    l = x.index[x.country_new.notnull() == True]\n    p = []\n    for i in l:\n        if x.country_new[i] == x.country[i]:\n            p.append(i)\n\n    print(\"country_new == country\",len(p))\n    print(\"country_new.notnull\",len(l),\"\\n\")","672bb2b2":"for x in [outside_Hubei, Hubei]:\n    m =[]\n    for i in range(len(x)):\n        if x.ID[i] != str(i+1):\n            m.append(i)\n    print(m[0],x.ID[m[0]])","1eba2ef9":"france = france.drop([\"departement\",\"source\",\"comments\",\"contact_number\"],axis=1)\n\n\nindonesia = indonesia.drop([\"origin\"],axis=1)\n\nkorea = korea.drop([\"age\",\"contact_number\"],axis=1)\n\nHubei = Hubei.drop([\"ID\",'location', 'admin3', 'admin2', \"admin1\" ,'latitude', 'longitude',\n                    'geo_resolution','admin_id', \"country_new\",\"source\",\"additional_information\",\"geo_resolution\"\n                    ,\"notes_for_discussion\"],axis=1)\n\noutside_Hubei = outside_Hubei.drop([\"ID\",'location', 'admin3', 'admin2', \"admin1\" ,'latitude', 'longitude',\n                                    'geo_resolution', 'admin_id', \"country_new\", \"data_moderator_initials\",\n                                    \"source\",\"additional_information\",\"geo_resolution\",\n                                    \"notes_for_discussion\"],axis=1)","16e9ae2f":"def examining_values_by_col (dfs, dfs_name, col):\n    \"\"\"\n    Prints values of each DF per column\n    \"\"\"\n    counter = 0\n    \n    for i in datasets:\n        if col in i.columns:\n            print(\"\\n\" + dfs_name[counter])\n            print(i[col].value_counts())\n        counter =counter + 1","ae2dbada":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets)\nexam_df = Exam.df_exam_columns_dfs(datasets,datasets_name,columns_name)\n\nfor j in exam_df.columns[1:len(exam_df.columns)]:\n    print(j)\n    examining_values_by_col (datasets , datasets_name , j) ","9aff12c2":"## TODO make FUN to_date\n\ncols = [\"confirmed_date\",\"released_date\", \"deceased_date\"]\n\nfrance[cols] = france[cols].apply(pd.to_datetime)\nindonesia[cols] = france[cols].apply(pd.to_datetime)\njapan[cols] = france[cols].apply(pd.to_datetime)\nkorea[cols] = korea[cols].apply(pd.to_datetime)\n\n# different\ntunisia_col = [\"confirmed_date\"]\n#  tunisia_col  = \"return_date\" problame\nkorea_col = [\"date_onset_symptoms\"] + cols\n\ntunisia[tunisia_col] = tunisia[tunisia_col].apply(pd.to_datetime)\nkorea[korea_col] = korea[korea_col].apply(pd.to_datetime)\n\n# \u05dc\u05d0 \u05e2\u05d5\u05d1\u05d3 . \u05d1\u05e2\u05d9\u05d4 \u05d0\u05dd \u05d4\u05e9\u05e2\u05d5\u05ea \u05d0\u05dd \u05d0\u05e0\u05d9 \u05d9\u05e2\u05e9\u05d4 \u05d0\u05ea \u05d6\u05d4 - \u05d9\u05d5\u05ea\u05e8 \u05e0\u05d5\u05d7 \u05de\u05d0\u05d7\u05e8\u05d9 \u05d1\u05e0\u05d9\u05d9\u05ea \u05d4\u05ea\u05db\u05d5\u05e0\u05d5\u05ea\nHubei_col = [\"confirmed_date\", \"date_death_or_discharge\", \"date_onset_symptoms\"]\nHubei[Hubei_col] = Hubei[Hubei_col].apply(pd.to_datetime)\n\n# \"confirmed_date\", \"date_onset_symptoms\"\nHubei_col = [\"date_death_or_discharge\"]\noutside_Hubei[Hubei_col] = outside_Hubei[Hubei_col].apply(pd.to_datetime)","c6c280af":"examining_values_by_col(datasets, datasets_name, \"travel_history_dates\")","48c1bfbe":"def make_ls_of_str_datatime(ls):\n        for i in range(len(ls)):\n            ls[i] = pd.to_datetime(ls[i], errors='ignore').date()\n        return ls\n        \ndef  time_range_extremity(ls, earliest=False):\n    df = pd.DataFrame({\"series\":ls})\n    if earliest == True:\n        value = df.series.max()\n    else:\n        value = df.series.min()\n    return value","b8b3bcb9":"outside_Hubei.travel_history_dates[7957]","0d0c1005":"for indx in outside_Hubei.index[outside_Hubei.travel_history_dates.notnull()]:\n    print(indx)","88beaf2f":"for indx in outside_Hubei.index[outside_Hubei.travel_history_dates.notnull()]:\n    i = outside_Hubei.travel_history_dates[indx] \n    i_split_1 = i.split(',')\n    \n    if len(i_split_1) > 1:\n        i_split_1 = make_ls_of_str_datatime(i_split_1)\n        value = time_range_extremity(i_split_1)\n        outside_Hubei.loc[indx, 'travel_history_dates'] = value\n    \n    i_split_2 = i_split_1[0].split('-')\n    if len(p) >1:\n        for i in range(len(p)):\n            p[i] = pd.to_datetime(p[i], errors='ignore').date()\n        d = pd.DataFrame({\"selfs\":p})\n        print(p,indx)\n        print(d.selfs.max())\n        print(p[0],p[1])\n        print(\"------\")\n    else:\n        p[0] = pd.to_datetime(p[0], errors='ignore').date()\n","a4d2a89d":"date_string = '2019-04-17'\ntime_stamp = pd.to_datetime(date_string)\nprint(time_stamp)","2f3dc6ff":"examining_values_by_col(datasets, datasets_name, \"travel_history_dates\")","eea8e10a":"tunisia_sex = {\"F\":\"female\", \"M\":\"male\",np.nan:np.nan}\ntunisia.sex = [tunisia_sex[item] for item in  tunisia.sex] \n\njapan_sex = {\"Woman\":\"female\", \"Man\":\"male\",np.nan:np.nan, \"Checking\":np.nan, \"investigating\":np.nan}\njapan.sex = [japan_sex[item] for item in  japan.sex] \n\nfrance_sex = {\"female\":\"female\", \"male\":\"male\",\"Female\":\"female\", \"Male\":\"male\", \"male\\xa0?\":\"male\", \n              np.nan:np.nan }\nfrance.sex = [france_sex[item] for item in  france.sex] \n","c8c62c39":"examining_values_by_col(datasets, datasets_name, \"sex\")","240c262b":"def update_index(df, col, index, data):\n    \"\"\"\n    Value change according index\n    \n    df: df\n    col : col you want to change\n    index: pd.index\n    data: data you want to into\n    \n    \"\"\"\n    for i in index:\n        df[col][i]=data","a09cf91d":"# indexs = korea.index[korea.background_diseases_binary == True]\n# p = [i for i in indexs]","f8d27065":"examining_values_by_col (datasets, datasets_name, \"background_diseases_binary\")","e3f17cf9":"tunisia[\"background_diseases_binary\"] = np.nan\nindexs = tunisia.index[tunisia.background_diseases.notnull()]\nupdate_index(tunisia,\"background_diseases_binary\",indexs,1) ","64286547":"words =[]\nfor i in outside_Hubei.background_diseases[outside_Hubei.background_diseases.notnull()]:\n    [words.append(i) for i in i.split(\" \")]\n\nwords = list(dict.fromkeys(words))\nprint(words)","cbd98a6a":"# import nltk\n# from nltk.stem import PorterStemmer\n# from nltk.corpus import stopwords","190fb3d2":"words =[]\nfor i in outside_Hubei.severity_illness[outside_Hubei.severity_illness.notnull()]:\n    [words.append(i) for i in i.split(\" \")]\n\nwords = list(dict.fromkeys(words))\nprint(words)","51237009":"for x in [indonesia,japan]:   \n    x[\"severity_illness\"] = np.nan\n\n    indexs = x.index[x.deceased_date.notnull()]\n    update_index(x,\"severity_illness\",indexs,\"deceased\") \n\n    indexs = x.index[x.released_date.notnull()]\n    update_index(x,\"severity_illness\",indexs,\"cured\") \n\n\n\nseverity_illness_tunisia = {\"Critical\":\"critical\", \"In progress\":\"good\", \"Stable\":\"good\", \"Cured\":\"cured\" }\ntunisia.Severity_illness = [severity_illness_tunisia[item] for item in  tunisia.severity_illness]\n\n\nindexs = korea.index[korea.severity_illness.isnull()] \nupdate_index(korea,\"severity_illness\",indexs,np.nan)\nseverity_illness_korea = {\"deceased\":\"deceased\", \"isolated\": np.nan, \"released\":\"cured\", np.nan : np.nan }\nkorea.severity_illness = [severity_illness_korea[item] for item in  korea.severity_illness]\n","02227a0b":"datasets2 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\nexamining_values_by_col(datasets2, datasets_name, \"severity_illness\")","d9f48360":"for x in [outside_Hubei, Hubei]:\n    x[\"released_date\"] = np.nan \n    indexs_released = x.index[x.severity_illness ==\"discharged\"]\n\n    for i in indexs_released:\n        x[\"released_date\"][i] = x[\"date_death_or_discharge\"][i]","a9155e6c":"for x in [outside_Hubei, Hubei]:\n    x[\"deceased_date\"] = np.nan \n    indexs_released = x.index[x.severity_illness ==\"died\"]\n\n    for i in indexs_released:\n        x[\"deceased_date\"][i] = x[\"date_death_or_discharge\"][i]","0bea779c":"for x in [outside_Hubei, Hubei]:\n    type(Hubei.deceased_date[2])\n    col = [\"released_date\",\"deceased_date\"]\n    x[col] = x[col].apply(pd.to_datetime)","745d692e":"for x in [outside_Hubei, Hubei]:\n    l = x.date_death_or_discharge.notnull().sum()\n    y = x.severity_illness.notnull().sum()\n    p = x.released_date.notnull().sum() +x.deceased_date.notnull().sum()\n    print(l,y,p)","fd4d7d65":"for x in [outside_Hubei]:\n    complete_features =list(x.index[x.released_date.notnull()]) + list(x.index[x.deceased_date.notnull()])\n    date_death_or_discharge = list(x.index[x.date_death_or_discharge.notnull()])\n    severity_illness = list(x.index[x.severity_illness.notnull()])\n\n    if complete_features == date_death_or_discharge:\n        print(\"==\")\n    else:\n        print(\"not ==\")\n    \n    for i in severity_illness:\n        if i not in complete_features:\n            print(i)","3a4ffeb4":"# outside_Hubei_age = {\"investigating\":np.nan, \"Checking\":np.nan, \"Under 10\":\"0s\", \"Under teens\":\"0s\",\"305\":\"30s\",\n#             \"10s\":\"10s\",\"20s\":\"20s\", \"30-39\":\"30s\", \"40-49\":\"40s\", \"50-59\":\"50s\", \"60-69\":\"60s\", \"70s\":\"70s\" ,\n#              \"80s\":\"80s\",\"90s\":\"90s\" }\n# outside_Hubei.age = [outside_Hubei_age[item] for item in outside_Hubei.age] ","568cd72a":"Hubei.age.value_counts()","6b57d69d":"# Hubei_age = {\"15-88\":np.nan, \"25-89\":np.nan, \"21-39\":np.nan, \"40-49\":\"40s\", \"50-59\":\"50s\", \"60-69\":\"60s\",\n#              \"70-82\":\"70s\" }\n# Hubei.age = [Hubei_age[item] for item in Hubei.age] ","e47ba0d2":"japan.age.value_counts()","f406cd42":"japan_age = {\"investigating\":np.nan, \"Checking\":np.nan, \"Under 10\":\"0s\", \"Under teens\":\"0s\",\"305\":\"30s\",\n            \"10s\":\"10s\",\"20s\":\"20s\", \"30s\":\"30s\", \"40s\":\"40s\", \"50s\":\"50s\", \"60s\":\"60s\", \"70s\":\"70s\" ,\n             \"80s\":\"80s\",\"90s\":\"90s\" }\njapan.age = [japan_age[item] for item in japan.age] ","0521c2b6":"def birth_year_to_age(data):\n    age_ls = []\n\n    for i in range(len(data)):\n        age_ls.append(data.confirmed_date[i].year - data.birth_year[i])\n    return age_ls\n\nkorea[\"age\"] = birth_year_to_age(korea)\nfrance[\"age\"] = birth_year_to_age(france)","53a5aeab":"tunisia[\"country\"] = [\"tunisia\" for i in range(len(tunisia))]\njapan[\"country\"] = [\"japan\" for i in range(len(japan))]\nindonesia[\"country\"] = [\"indonesia\" for i in range(len(indonesia))]","530f1521":"print(len(korea))\nprint(outside_Hubei.country.value_counts()[\"South Korea\"])\nprint()\n\nprint(len(france))\nprint(outside_Hubei.country.value_counts()[\"France\"])","0661bb3f":"france[\"status\"] =[np.nan for i in np.arange(len(france))]\nfrance.status = france.status.astype(\"object\")\nfrance[\"status\"].dtype","115aef97":"deceased = france.index[france.deceased_date.notnull() == True]\nreleased = france.index[france.released_date.notnull() == True]\nisolated = france.index[(france.released_date.notnull() == False)&(france.deceased_date.notnull() == False)]","f3c9b43c":"for i in deceased:\n    france.at[ i, 'status'] = \"deceased\"\n\nfor i in released:\n    france.at[ i, 'status'] = \"released\" \n    \nfor i in isolated:\n    france.at[ i, 'status'] = \"isolated\"","c5f31a73":"france.status.isnull().sum()","4b8b801f":"print(Hubei[\"wuhan(0)_not_wuhan(1)\"].value_counts())\nprint(outside_Hubei[\"travel_history_location\"].value_counts())","f9326a42":"# TODO","69ea25ad":"france = france.drop([\"birth_year\", \"treatment\",\"group\"],axis=1)\ntunisia = tunisia.drop([\"hospital_place\"],axis=1)\njapan = japan.drop([\"Close contact situation\"],axis=1)\nkorea = korea.drop([\"birth_year\",\"global_num\"],axis=1)\nHubei = Hubei.drop([\"date_death_or_discharge\"],axis=1)","87b420cf":"france = france.drop([\"infection_place\", \"infected_by\",\"infection_order\"],axis=1)\ntunisia = tunisia.drop([\"hospital_name\"],axis=1)\nindonesia = indonesia.drop([\"infected_by\",\"hospital_name\"],axis=1)\nkorea = korea.drop([\"infection_place\", \"infected_by\",\"infection_order\"],axis=1)","ca0cf9d7":"datasets2 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets2)\nexam_df = Exam.df_exam_columns_dfs(datasets2, datasets_name, columns_name)\nprint(columns_name)\nexam_df.infection_place","bbc9be80":"datasets3 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets3)\nexam_df2 = Exam.df_exam_columns_dfs(datasets3,datasets_name,columns_name)","c81c73f1":"for i in exam_df2.columns:\n    print(\"\\n\"+i)\n    examining_values_by_col (datasets, datasets_name, i)","be9c804c":"exam_df.sex","9862d729":"for i in dfs:\n    print(i[col].isnull().sum())","359c1dd4":"for i in datasets:\n    print(i.sex.isnull().sum())","31852069":"datasets_final = [france, tunisia, japan, indonesia, korea]\nfinal_DS = pd.concat(datasets_final, axis=0)","de21b283":"final_DS.status.value_counts()","884472e0":"final_DS.index = range(len(final_DS))","8de17ff5":"final_DS.to_csv(r'\/kaggle\/working\/Characteristics_Corona_patients1.csv', index = False)","32bf5e9f":"final_DS.to_csv()","7ed7109d":"# drop","af596dbe":"common feature","f17bdb1d":"index","82adc4b0":"format date","97044c08":"sex","a0824c35":"# orgnaze DS","7b603dd8":"# drop Non-baked features","66fe614e":"# datasets.shape","410cf529":"# build final DS","dd9387c0":"Tests for columns' usefulness before drop","5e7943a4":"**The purpose of this notebook is to create a DATASET that includes**\n\n** Characteristics of patients like - **\n\n*age\n\n*sex\n\n*Country\n\nand so\n\n\n\n\n**The condition of the patients and their characteristics - **\n\n* Disease time (from diagnosis date)\n\n* Have been cured\n \n* Deaths\n\n\nThe database is designed to allow easy exploration of the data\n\nAnyone interested can use and donate","7ab4583a":"background_diseases_binary","9e428a26":"exam ","916556a6":"status","2fa32944":"infection_place","97a3c60f":">datetime","a28729b9":"deceased_date","6ca90806":"country_new","8de7c329":"#  Garbage drop \n- Features that have only one dataset or  built with Engineered another feature with them","118d633b":"The column has no new information to give","cb58db1e":"# Examining values - v1 ","fc1de836":"#                                                     Complete features","0e7db525":"As you can see the ID is not arranged in any numerical order.\nand because there is no column that needs another row identifier\nI drop ID","aefe0290":"background_diseases","85a076af":"# exam_df = columns vs dfs","c06e0d89":"unipue","0238b337":"test","080ee95c":"infected by","c654c5d2":"outside_Hubei data VS country data","ecbaacac":"age","2e157a16":"**> Feature sum**","f95397c3":"severity_illness","8b7e9d2c":"# drop feature","21457c33":"change name of col","13e02ed6":"infection_case\n\n= Community \\abroad \\ Nan","15fd5ca9":"# format col","d6ad2fc2":"released_date","294c1d83":"country","6e24efae":"ID"}}