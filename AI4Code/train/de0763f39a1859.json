{"cell_type":{"9c2258c1":"code","c5d7b12e":"code","dc3ca337":"code","d6b7a811":"code","0169f9c1":"code","aeb995b6":"code","82ca05e7":"code","1f49c584":"code","09cd0f94":"code","cd3e76ed":"code","320bb8e5":"code","30b08fcd":"code","c784451c":"code","7a3d5f52":"code","4fa09199":"code","268d6c72":"code","5ec0f158":"code","55a84418":"markdown","69f6fd6f":"markdown","fb5bb195":"markdown","bce7337f":"markdown","c68f9837":"markdown","2ac3a621":"markdown","a296724c":"markdown","fb7d7843":"markdown","414a27e1":"markdown","8ac4f55a":"markdown","bfa46a48":"markdown","a4ad2a06":"markdown","cb488200":"markdown","5b86f254":"markdown","61ffd6e3":"markdown"},"source":{"9c2258c1":"# Fix FutureWarning Messages in scikit-learn\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n\nimport math\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom datetime import datetime, timedelta\nfrom dateutil.parser import parse as dt_parse\nfrom collections import Counter\n\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c5d7b12e":"!head \/kaggle\/input\/chh-ola\/train.csv","dc3ca337":"!sed 's\/\\+AF8-\/\/g' \/kaggle\/input\/chh-ola\/train.csv > train.csv\n!sed 's\/_\/\/g' \/kaggle\/input\/chh-ola\/test.csv > test.csv","d6b7a811":"class Ut:\n  @staticmethod\n  def to_timestamp(dt):\n    return dt_parse(dt, dayfirst=False).timestamp()\n\n  @staticmethod\n  def flag_to_num(vl):\n    if vl == 'N':\n      return 0\n    else:\n      return 1\n  \n  @staticmethod\n  def to_float(vl):\n    try:\n      if type(vl) == type('str'):\n        idx = vl.find('-')\n        if idx != -1:\n          txt = vl.split('-')\n          return float(txt[1])\n      return float(vl)\n    except:\n      print(vl)\n      return float(0)\n\n  @staticmethod\n  def rmse(predictions, targets):\n    \"\"\"Data convertion and RMSE calculation\"\"\"\n    return np.sqrt(mean_squared_error(np.exp(predictions), np.exp(targets)))","0169f9c1":"# Load training data file\ntrain_set = pd.read_csv('train.csv', low_memory=False, dtype=str)\ntrain_set.dropna(inplace=True)\ntrain_set.reset_index(drop=True, inplace=True)\n\n# Load testing data file\ntest_set = pd.read_csv('test.csv', low_memory=False, dtype=str)\ntest_set['totalamount'] = 0\n\n# Create a column to distinguish one from the other\ntrain_set['PROPOSITO'] = 1\ntest_set['PROPOSITO'] = 0\n\n# Concatenate all data into one dataframe\nall_set = pd.concat([train_set, test_set], ignore_index=True)\n\n# Delete train_set and test_set to free memory\ndel(train_set)\ndel(test_set)\n\n# MTA Tax should be the same in all rides\nall_set['mtatax'] = 0.5\n\n# Data conversion\n# Y or N to 1 or 0\nall_set['storedflag'] = all_set['storedflag'].apply(Ut.flag_to_num)\n# Conversion to datetime format\nall_set['pickuptime'] = all_set['pickuptime'].apply(Ut.to_timestamp)\nall_set['droptime'] = all_set['droptime'].apply(Ut.to_timestamp)\n# Conversion to float\nall_set['drivertip'] = all_set['drivertip'].apply(Ut.to_float)\nall_set['mtatax'] = all_set['mtatax'].apply(Ut.to_float)\nall_set['tollamount'] = all_set['tollamount'].apply(Ut.to_float)\nall_set['extracharges'] = all_set['extracharges'].apply(Ut.to_float)\nall_set['improvementcharge'] = all_set['improvementcharge'].apply(Ut.to_float)\nall_set['totalamount'] = all_set['totalamount'].apply(Ut.to_float)","aeb995b6":"# Total time\nall_set['totaltime'] = all_set['droptime'] - all_set['pickuptime']\n# All Taxes\nall_set['taxes'] = all_set['drivertip'] + all_set['mtatax'] + all_set['tollamount'] + all_set['extracharges'] + all_set['improvementcharge']","82ca05e7":"features_cat = ['vendorid', 'paymentmethod', 'ratecode', 'storedflag']\nfeatures_num = ['drivertip', 'pickuploc', 'droploc', 'mtatax', 'distance', 'pickuptime', 'droptime', 'numpassengers', \n                'tollamount', 'extracharges', 'improvementcharge', 'totalamount', 'totaltime', 'taxes']\ntarget = 'totalamount'\n\n# Numerical features will be converted to float.\nfor col in features_num:\n    all_set[col] = all_set[col].astype(float)\n\n# Categorical features will be converted to float then to string.    \nfor col in features_cat:\n    all_set[col] = all_set[col].astype(float)\n    all_set[col] = all_set[col].astype(str)\n\nall_set['PROPOSITO'] = all_set['PROPOSITO'].astype(int)\nall_set['ID'] = all_set['ID'].astype(int)","1f49c584":"# Convert categorical variable into dummy\/indicator variables\nall_dum = pd.get_dummies(all_set)\n\ntrain_df = all_dum[all_dum['PROPOSITO'] == 1].copy()\ntest_df = all_dum[all_dum['PROPOSITO'] == 0].copy()\n\ndel(all_dum)\n\ntrain_df.drop(columns=['PROPOSITO'], inplace=True)\ntest_df.drop(columns=['PROPOSITO'], inplace=True)\n\n# Remove rows where pickup loc is equal to drop loc\ntrain_df = train_df[train_df['pickuploc'] != train_df['droploc']]\n# Drop rows where total amount is 0, since there are no free rides\ntrain_df = train_df[train_df['totalamount'] > 0]\ntest_df.drop(columns=[target], inplace=True)","09cd0f94":"# Alongside hyperparameter searching, I also did a feature searching to check \n# which combination would provide better results\nfeatures_to_keep = [\n  'taxes',\n  'pickuploc',\n  'ratecode_2.0',\n  'ratecode_1.0',\n  'ratecode_5.0',\n  'storedflag_0.0',\n  'ratecode_4.0',\n  'totaltime',\n  'ratecode_3.0',\n  'droploc',\n  'numpassengers',\n  'distance',\n  'storedflag_1.0',\n  'vendorid_2.0',\n  'paymentmethod_1.0',\n  'vendorid_1.0',\n  'paymentmethod_2.0'\n  ]\n\nX = train_df[features_to_keep].copy()\ny = train_df[target].copy()\n\nnormalizer = Normalizer()\nnorm_X = normalizer.fit_transform(X)\ny = np.log(y)","cd3e76ed":"if False:\n  params = {\n      'colsample_bytree':[0.9], \n      'gamma':[0.3],\n      'max_depth': [9],  \n      'min_child_weight':[2], \n      'subsample':[0.9],\n      'n_estimators': [50],\n      'objective': ['reg:squarederror'],\n      'n_jobs': [8],\n      }\n\n  # Initialize XGB and GridSearch\n  eval_model = xgb.XGBRegressor(nthread=-1) \n\n  grid = GridSearchCV(eval_model, params, cv=2)\n  grid.fit(train_X, train_y)\n\n  pred_y = grid.predict(test_X)\n  print('RMSE Test = ', Ut.rmse(pred_y, test_y))\n\n  print(grid)\n  print(grid.best_params_)","320bb8e5":"if False:\n  params = {\n      'min_child_weight': st.randint(2, 9), \n      'gamma': st.uniform(0.1, 0.9),  \n      'subsample': st.uniform(0.1, 0.9),\n      'colsample_bytree': st.uniform(0.1, 0.9), \n      'max_depth': st.randint(3, 9),\n      'n_estimators': [50],\n      'objective': ['reg:squarederror'],\n      # 'n_jobs': [8],\n      }\n\n  eval_model = xgb.XGBRegressor(nthread=-1) \n\n  grid = RandomizedSearchCV(eval_model, params, cv=2, n_jobs=1, n_iter=10)\n  grid.fit(train_X, train_y)\n\n  pred_y = grid.predict(test_X)\n  print('RMSE Test = ', Ut.rmse(pred_y, test_y))\n\n  print(grid)","30b08fcd":"# Besides hyperparameter searching, it is also important to see how the results change \n# when we remove features that have a poor correlation to the \"total amount\".\nif False:\n    train_correlation = train_df.corr()\n    train_correlation = train_correlation['totalamount'].apply(abs)\n    train_correlation = train_correlation.sort_values(na_position='first')\n    train_correlation = pd.DataFrame(train_correlation).reset_index()\n    train_correlation.dropna(inplace=True)\n    \n    best_rmse = math.inf\n    best_idx = 0\n\n    features_to_analyse = train_correlation['index'].unique()\n    for idx in range(len(features_to_analyse) - 1):\n        features = features_to_analyse[idx:]\n        dX = train_df[features].copy()\n        dy = train_df[target].copy()\n\n        normalizer = Normalizer()\n        norm_dX = normalizer.fit_transform(dX)\n        dy = np.log(dy)\n\n        train_dX, test_dX, train_dy, test_dy = train_test_split(norm_dX, dy, test_size=0.2, random_state=42)\n\n        params = {\n            'objective': 'reg:squarederror',\n            'n_estimators': 100,\n            'subsample': 0.9, \n            'min_child_weight': 1, \n            'max_depth': 9,\n            'gamma': 0.3, \n            'colsample_bytree': 0.9,\n            'n_jobs': 8,\n            'verbose_eval':'False',\n        }\n        model = xgb.XGBRegressor(**params)\n        model.fit(train_dX,train_dy)\n\n        pred_dy = model.predict(test_dX)\n        print('Features cut = ', idx, ' Resulting RMSE = ', Ut.rmse(pred_dy, test_dy))\n        \n        if best_rmse > rmse_res:\n            best_rmse = rmse_res\n            best_idx = idx\n\n    print('Features cut = ', best_idx, ' Resulting RMSE = ', best_rmse)\n    features_to_keep = features_to_analyse[best_idx:]","c784451c":"train_X, test_X, train_y, test_y = train_test_split(norm_X, y, test_size=0.2, random_state=42)\n\n# These hyperparameters are the result of the searchings of Step 9\nparams = {\n    'objective': 'reg:squarederror',\n    'n_estimators': 1000,\n    'subsample': 0.9, \n    'min_child_weight': 1, \n    'max_depth': 9,\n    'gamma': 0.3, \n    'colsample_bytree': 0.9,\n    'n_jobs': 8,\n    'verbose_eval':'False',\n}\nmodel = xgb.XGBRegressor(**params)\nmodel.fit(train_X,train_y)\n\npred_y = model.predict(test_X)\nprint('RMSE Test = ', Ut.rmse(pred_y, test_y))","7a3d5f52":"real_X = normalizer.transform(test_df[features_to_keep].copy())\n\nmodel = xgb.XGBRegressor(**params)\nmodel.fit(norm_X, y)\n\npredictions = np.exp(model.predict(real_X))","4fa09199":"# Have a glimpse of the predictions\npd.DataFrame(predictions).head(20)","268d6c72":"result = []\nfor idx in range(test_df.shape[0]):\n  result.append([idx, predictions[idx]])\nresult = pd.DataFrame(result, columns=['ID', 'total_amount'])\nresult.to_csv('result.csv', index=False)","5ec0f158":"# Delete temp files so they're not taken as results by mistake\n!rm train.csv\n!rm test.csv","55a84418":"# Step 11) Prediction","69f6fd6f":"### c) Feature searching","fb5bb195":"# Step 2) Cleaning the data using bash is faster than in Python.","bce7337f":"# Step 9) Hyperparameter searching","c68f9837":"### a) GridSearchCV","2ac3a621":"# Step 5) Feature creation: **total time** and **taxes**.","a296724c":"# Step 4) Data preprocessing","fb7d7843":"### b) RandomizedSearchCV","414a27e1":"# Step 8) Normalize data\nNormalized data provide better results on the model. \n* The data will be normalized using **sklearn.preprocessing.Normalizer()**. \n* The targets will be normalized using **numpy.log**. ","8ac4f55a":"# Step 12) Saving the results","bfa46a48":"# Step 10) Model training","a4ad2a06":"# Step 6) Feature analysis and type convertion.","cb488200":"# Step 7) Split data into *train_df* and *test_df*","5b86f254":"# Step 1) Check training file.","61ffd6e3":"# Step 3) Write some utility functions"}}