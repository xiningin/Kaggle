{"cell_type":{"9f7ec202":"code","7df27b13":"code","4d5ac93d":"code","d0ef9540":"code","230bfc3f":"code","7c3414a2":"code","81b1fc11":"code","1712418b":"code","1fcc618a":"code","79801b61":"markdown","795e43e4":"markdown","80be25ed":"markdown","90fab45f":"markdown","f2ab28e9":"markdown","3bcb105a":"markdown","b789088e":"markdown","6b04b71f":"markdown","d7bfc39b":"markdown","a918045a":"markdown"},"source":{"9f7ec202":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n#library that contains punctuation\nimport string\nstring.punctuation\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom textblob import TextBlob\n#importing the Stemming function from nltk library\nfrom nltk.stem import WordNetLemmatizer\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud\nimport os\nimport matplotlib.pyplot as plt\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7df27b13":"df_base=pd.read_csv(\"\/kaggle\/input\/imdb-review-dataset\/imdb_master.csv\",sep=',', encoding='latin-1')\n#Selecting only 20 records to illustrate the preprocessing and feature extraction\ndf_base=df_base.head(20)\ndf_base=df_base[['Unnamed: 0','review']]\ndf_base.columns=['id','review']\nprint('Currently our data looks like the following:')\ndf_base.head()","4d5ac93d":"##2.1\n#defining the function to remove punctuation\ndef remove_punctuation(text):\n    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n    return punctuationfree\n#storing the puntuation free text\ndf_base['Clean_review_1']= df_base['review'].apply(lambda x:remove_punctuation(x))\n\n##2.2\n#Making the case consistent as lower\ndf_base['Clean_review_2']= df_base['Clean_review_1'].apply(lambda x: x.lower())\n\n##2.3\n#Removing Stop Words\n# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\ndf_base['Clean_review_3'] = df_base['Clean_review_2'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n##2.4\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_func(text):\n    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n    text = \" \".join(text)\n    return text\n\ndf_base['Clean_review_4'] = df_base.Clean_review_3.apply(lambda x: lemmatize_func(x))\ndf_base.head()","d0ef9540":"df_base['Character_length']= df_base['Clean_review_4'].str.len()\ndf_base['Word_length']=df_base['Clean_review_4'].str.count(' ') + 1\n\ndef sentiment_func(text):\n    return TextBlob(text).sentiment[0]\n    \ndf_base['Polarity_score']=df_base.Clean_review_4.apply(lambda x: sentiment_func(x))\ndf_base.head()","230bfc3f":"fig = px.histogram(df_base, x=\"Polarity_score\", nbins=30)\nfig.show()","7c3414a2":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_words(df_base['Clean_review_4'], 20)\n\n\n    \ndf_unigram_20 = pd.DataFrame(common_words, columns = ['Word' , 'count']).sort_values(by=\"count\",ascending=False).reset_index(drop=True)\n#df_unigram_20=df_unigram_20.groupby('Word').sum()['count']\n\nfig = px.bar(df_unigram_20, x='Word', y='count')\nfig.update_layout(\n    title={\n        'text': \"Top 20 words in the reviews\",\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()","81b1fc11":"def get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_bigram(df_base['Clean_review_4'], 20)\n\ndf_20_bi = pd.DataFrame(common_words, columns = ['Bigram' , 'count'])\nfig = px.bar(df_20_bi, x='Bigram', y='count')\nfig.update_layout(\n    title={\n        'text': \"Top 20 Bigrams in the reviews\",\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()\n","1712418b":"\ndef get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\ncommon_words = get_top_n_trigram(df_base['Clean_review_4'], 20)\n\ndf_tri = pd.DataFrame(common_words, columns = ['Trigram' , 'count'])\nfig = px.bar(df_tri, x='Trigram', y='count')\nfig.update_layout(\n    title={\n        'text': \"Top 20 Trigrams in the reviews\",\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()","1fcc618a":"Cloud_text = ' '.join(df_base['Clean_review_4'])\nwordcloud2 = WordCloud( background_color = 'white',width=1200, height=800).generate(Cloud_text)\n# Generate plot\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud2)\nplt.axis(\"off\")\nplt.show()","79801b61":"## Top 20 Bigrams","795e43e4":"## Wordcloud","80be25ed":"## Contents\n1. [Ground Work](#Ground-Work)\n2. [Data Cleaning](#Data-Cleaning)\n    * 2.1 Removing punctuations\n    * 2.2 Making Case consistent\n    * 2.3 Removing Stop words\n    * 2.4 Lemmatize\n3. [Feature Engineering](#Feature-Engineering)\n    * 3.1 Character Length\n    * 3.2 Word Length\n    * 3.3 Sentence Polarity\n4. [Polarity Distribution](#Polarity-Distribution)\n5. [Top 20 Words](#Top-20-Words)\n6. [Top 20 Bigrams](#Top-20-Bigrams)\n7. [Top 20 Trigrams](#Top-20-Trigrams)\n8. [Wordcloud](#Wordcloud)","90fab45f":"### Ground Work","f2ab28e9":"## Polarity Distribution","3bcb105a":"## Top 20 Words","b789088e":"![image.png](attachment:5443a418-220e-4ba0-b813-720fad860317.png)","6b04b71f":"### Feature Engineering","d7bfc39b":"### Data Cleaning","a918045a":"## Top 20 Trigrams"}}