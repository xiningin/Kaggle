{"cell_type":{"47c808e8":"code","1e7454ef":"code","6c062ee8":"code","0de1b57e":"code","25ff8aab":"code","e480dbde":"code","2c430970":"code","8b274e7e":"code","57ebf38b":"code","f667a278":"code","3459528c":"code","ce70ae9a":"code","b968a69b":"code","f183e6fa":"code","541c7fe8":"code","e982b47e":"code","88f445d8":"code","df013e01":"code","9edc5a3d":"code","b8ca636c":"code","c3387fae":"code","1fd5a735":"code","de20389e":"code","9ac22bc1":"code","1729b2bc":"code","cf1396e0":"markdown","e4d12b0e":"markdown","879e29d3":"markdown","6f60020a":"markdown","cd6d1217":"markdown","2295564e":"markdown","e8a5b7ca":"markdown","08f6206a":"markdown","a087d221":"markdown","ee62680a":"markdown","a9456b51":"markdown","7f575233":"markdown"},"source":{"47c808e8":"'''Importing the Moduls'''\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow \nprint('Tensorflow version',tensorflow.__version__)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models\n\nimport os\nprint(os.listdir(\"..\/input\/cell-images-for-detecting-malaria\/cell_images\/cell_images\"))","1e7454ef":"!pip install split-folders","6c062ee8":"'''Train and Test split on a dataset with directories'''\nimport split_folders\norig_path = '..\/input\/cell-images-for-detecting-malaria\/cell_images\/cell_images'\noutput_path = '..\/output'\nsplit_folders.ratio(orig_path, output=output_path, seed=1, ratio=(.8, .2))","0de1b57e":"'''Preview the split'''\ndata_dir = '..\/output'\nprint(os.listdir(data_dir))","25ff8aab":"'''Creating train and test paths'''\ntrain = data_dir+'\/train\/'\ntest = data_dir+'\/val\/'","e480dbde":"'''Preview the train and test directories'''\nprint(os.listdir(train))\nprint('\\n')\nprint(os.listdir(test))","2c430970":"'''Reading the Single Uninfected image'''\nprint('Uninfected image:',os.listdir(train+'Uninfected')[0])\nprint('Parasitizes image:',os.listdir(train+'Parasitized')[0])","8b274e7e":"uninf_cell = train+'Uninfected\/'+'C230ThinF_IMG_20151112_150329_cell_162.png'\npara_cell = train+'Parasitized\/'+'C182P143NThinF_IMG_20151201_171950_cell_202.png'","57ebf38b":"\"\"\"Let's read the image file\"\"\"\ncv2.imread(uninf_cell)","f667a278":"\"\"\"Let's see the shape of the image\"\"\"\ncv2.imread(uninf_cell).shape","3459528c":"'''Preview the image'''\nplt.figure(1, figsize = (15 , 7))\nplt.subplot(1 , 2 , 1)\nplt.imshow(cv2.imread(uninf_cell))\nplt.title('Uninfected Cell')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(1 , 2 , 2)\nplt.imshow(cv2.imread(para_cell))\nplt.title('Infected Cell')\nplt.xticks([]) , plt.yticks([])\n\nplt.show()","ce70ae9a":"'''Number of image in the dataset'''\nprint('lenght of train parasitized', len(os.listdir(train+'Parasitized')))\nprint('lenght of train uninfected', len(os.listdir(train+'Uninfected')))\nprint('lenght of test parasitized', len(os.listdir(test+'Parasitized')))\nprint('lenght of test uninfected', len(os.listdir(test+'Uninfected')))","b968a69b":"'''Creating the loop to get the dimension of the image'''\ndim1 = []\ndim2 = []\n\nfor image_filename in os.listdir(train+'Uninfected'):\n    try:\n        img = cv2.imread(train+'Uninfected\/'+image_filename)\n        d1,d2,colors = img.shape\n        dim1.append(d1)\n        dim2.append(d2)\n        \n    except AttributeError:\n        print('')","f183e6fa":"'''Plot the distribution of images dimensions'''\nplt.figure(figsize=(10,10))\nsns.jointplot(dim1,dim2, color='teal',alpha=0.5)\nplt.show()","541c7fe8":"'''Average of the dimensions'''\nprint(np.mean(dim1))\nprint(np.mean(dim2))","e982b47e":"'''Final image shape that I will be feeding in my convolution network'''\nimage_shape = (130,130, 3)\n\n# Then later on or actually preparing the data for the model we'll resize everything to these dimensions.","88f445d8":"'''Create image generator'''\n# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator\n\ntrain_image_gen = ImageDataGenerator(rotation_range=30,\n                               width_shift_range=0.1,\n                               height_shift_range=0.1,\n                               rescale=1.\/255,\n                               shear_range=0.1,\n                               zoom_range=0.1,\n                               horizontal_flip=True,\n                               fill_mode='nearest')\n\ntest_image_gen = ImageDataGenerator(rescale=1.\/255)","df013e01":"'''Preview the image'''\npara_img = cv2.imread(para_cell)\n\nplt.figure(1, figsize = (15 , 7))\nplt.subplot(1 , 2 , 1)\nplt.imshow(para_img)\nplt.title('Before Processed Image')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(1 , 2 , 2)\nplt.imshow(train_image_gen.random_transform(para_img))\nplt.title('After Processed Image')\nplt.xticks([]) , plt.yticks([])\n\nplt.show()  ","9edc5a3d":"'''Set the CNN model'''\nmodel = models.Sequential()\n\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=image_shape))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Dropout(0.25))\n          \nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Dropout(0.25))\n\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Dropout(0.25))\n\n          \nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","b8ca636c":"model.summary()","c3387fae":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)","1fd5a735":"'''Set up the generator to flow batches from directory'''\nbatch_size = 16\n\ntrain_generator = train_image_gen.flow_from_directory(train,\n                                                      target_size=image_shape[:2],\n                                                      color_mode='rgb',\n                                                      batch_size=batch_size,\n                                                      class_mode='binary')\n\ntest_generator = test_image_gen.flow_from_directory(test,\n                                                    target_size=image_shape[:2],\n                                                    color_mode='rgb',\n                                                    batch_size=batch_size,\n                                                    class_mode='binary',\n                                                    shuffle=False)","de20389e":"\"\"\"Let's see the target\"\"\"\ntrain_generator.class_indices","9ac22bc1":"'''Training the model'''\nhistory = model.fit_generator(train_generator, epochs=20,\n                              validation_data = test_generator,\n                              callbacks=[early_stop])","1729b2bc":"'''Training and validation curves'''\nfig, ax = plt.subplots(2,1, figsize = (8,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","cf1396e0":"# Malaria Cells Image Classification\n![](https:\/\/www.mayoclinic.org\/-\/media\/kcms\/gbs\/patient-consumer\/images\/2013\/08\/26\/10\/13\/ds00475_im00175_mcdc7_malaria_transmitthu_jpg.jpg)\n\nThis Dataset is taken from the official NIH Website: https:\/\/ceb.nlm.nih.gov\/repositories\/malaria-datasets\/\n\nThe dataset contains a total of 27,558 cell images with equal instances of parasitized and uninfected cells. An instance of how the patient-ID is encoded into the cell name is shown herewith: \u201cP1\u201d denotes the patient-ID for the cell labeled \u201cC33P1thinF_IMG_20150619_114756a_cell_179.png\u201d. We have also included the CSV files containing the Patient-ID to cell mappings for the parasitized and uninfected classes. ","e4d12b0e":"# Give me your feedback and if you find my kernel helpful please UPVOTE will be appreciated","879e29d3":"# 3. Data Augmentation\nIn order to avoid overfitting problem, we need to expand artificially our handwritten digit dataset. We can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.\n\nFor example, the number is not centered The scale is not the same (some who write with big\/small numbers) The image is rotated...\n\nApproaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Some popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and much more.\n\nBy applying just a couple of these transformations to our training data, we can easily double or triple the number of training examples and create a very robust model.","6f60020a":"It seems that data is balanced. Something to note here is since these are real image files it's unlikely that they're all going to be the exact same shape. So, we have to reshape it.","cd6d1217":"**So,this particular image have 118 X 127 X 3 dimension that mean it is a color image.**","2295564e":"**imread automatically change the image into array list. This are the pixel value of the image.**","e8a5b7ca":"**we can see the various dimensions we have a really small image that's like 50 by 60 and a really large over 200 by 200 hundred. Let's take averge of the dimensions and reshape all the images in the same dimension becuase convolution neural network isn't going to be able to train on images of various sizes.**","08f6206a":"**So, we can see that randomize version of the image. we notice that we got some stretch. And that's because through this random transformation it looks like it got stretched out and it filled in those values with the nearest pixel value.**\n\n**And then note it was also rotated makes a lot of sense to randomly rotate images here because their cells that can be in any sort of rotational axis that they want.**","a087d221":"# 1. Importing Packages and Collecting Data","ee62680a":"# 3. Model CNN and Evalution","a9456b51":"# 2. Exploring the Image","7f575233":"**To implement the flow_from_directory. The input folder shoud have the following format:**\n`input\/\n    class1\/\n        img1.jpg\n        img2.jpg\n        ...\n    class2\/\n        imgWhatever.jpg\n        ...\n    ...`"}}