{"cell_type":{"c5b008a2":"code","94e2a5d4":"code","0de6eccc":"code","667fc25c":"code","d291003c":"code","aedcb99c":"code","61aaf2ab":"code","24157836":"code","9c74be24":"code","a960c193":"code","b0e9475c":"code","4fbe5053":"code","6597b2f0":"code","b03dd158":"code","fe738f36":"code","59903439":"code","76ce6814":"code","f1c30b1d":"code","8c33497f":"code","3b4657a1":"code","41f164ce":"code","6ebca06f":"code","857b1910":"code","37b927de":"code","3f557604":"code","0379f887":"code","9ce3ffc5":"code","724da70f":"code","9a377705":"code","c6f4e75c":"code","60dd4c84":"code","d08c55e6":"code","a01556cf":"code","440648e6":"code","5f4ec642":"code","0c2dbf24":"code","aabf1f2e":"code","6e4590f6":"code","fbc6aa4e":"code","dc4e09b3":"code","2c3ddd6e":"code","a48fe0fc":"code","79464515":"code","5c654acb":"code","317a23cc":"code","1e44dcd2":"code","d85a6a70":"code","527d1b80":"code","6484a3e8":"code","db43f436":"code","424bfdae":"code","4f71ae8f":"code","bd0a24ff":"code","1e333b23":"code","1c5cc66c":"code","72c26100":"code","942901ef":"code","002136ce":"code","4a899879":"code","6d7de809":"code","8403fb17":"code","18e5e12e":"code","93ea2702":"code","fb3f1f56":"code","4c5ae00b":"code","6ca3c63f":"code","7dd0f23e":"code","1e858e4b":"code","2fbc4e48":"markdown","090c7720":"markdown","42dc72b7":"markdown","6db49c19":"markdown","ba98ccff":"markdown","7d88ccb4":"markdown","914bd682":"markdown","a9cca877":"markdown","2eda2e28":"markdown","45427ef8":"markdown","0068ebdf":"markdown"},"source":{"c5b008a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","94e2a5d4":"df_yelp=pd.read_csv('\/kaggle\/input\/sentiment-labelled-sentences-data-set\/yelp_labelled.txt',sep='\\t',header=None)","0de6eccc":"df_yelp.head()","667fc25c":"df_imdb=pd.read_csv('\/kaggle\/input\/sentiment-labelled-sentences-data-set\/imdb_labelled.txt',sep='\\t',header=None)","d291003c":"df_amzn=pd.read_csv('\/kaggle\/input\/sentiment-labelled-sentences-data-set\/amazon_cells_labelled.txt',sep='\\t',header=None)","aedcb99c":"df_yelp.shape, df_amzn.shape , df_imdb.shape","61aaf2ab":"col_names=['review','sentiment']\ndf_yelp.columns=col_names\ndf_imdb.columns=col_names\ndf_amzn.columns=col_names","24157836":"df_yelp.head()","9c74be24":"df_yelp.loc[20]['review']","a960c193":"data=df_yelp.append([df_amzn,df_imdb],ignore_index=True)","b0e9475c":"data.head()","4fbe5053":"data.shape","6597b2f0":"data['sentiment'].value_counts()","b03dd158":"data.isnull().sum()","fe738f36":"import string\npunc=string.punctuation","59903439":"punc","76ce6814":"import spacy\nnlp=spacy.load('en_core_web_sm')","f1c30b1d":"x='hello! as its WorlD'\ndoc=nlp(x)\nfor token in doc:\n    print(token.lemma_)","8c33497f":"def lemmatize(x):\n    doc=nlp(x)\n    tokens=[]\n    for token in doc:\n        if token.lemma_ != '-PRON-':\n            temp=token.lemma_.lower().strip()\n        else:\n            temp=token.lower_\n        tokens.append(temp)\n    return tokens","3b4657a1":"lemmatize(x)","41f164ce":"from spacy.lang.en.stop_words import STOP_WORDS","6ebca06f":"print(STOP_WORDS)","857b1910":"import string\npunc=string.punctuation","37b927de":"def stop_word_and_punc(x):\n    tokens=[]\n    for token in x:\n        if token not in STOP_WORDS and token not in punc:\n            tokens.append(token)\n    return tokens","3f557604":"def data_cleaning(x):\n    tokens=lemmatize(x)\n    return stop_word_and_punc(tokens)","0379f887":"x=\"Hello my name is shubham and this is good learning drinking runs learned\"\ndata_cleaning(x)","9ce3ffc5":"data.head()","724da70f":"text=\" \".join(data['review'])","9a377705":"text","c6f4e75c":"from spacy import displacy\nnlp=spacy.load('en_core_web_sm')","60dd4c84":"doc=nlp(text)","d08c55e6":"displacy.render(doc,style='pos')","a01556cf":"displacy.render(doc,style='ent')","440648e6":"import matplotlib.pyplot as plt","5f4ec642":"data['word_count']=data['review'].apply(lambda x:len(x.split()))","0c2dbf24":"data.head()","aabf1f2e":"def get_char_count(x):\n    count=0\n    for word in x.split():\n        count+=len(word)\n    return count","6e4590f6":"data['char_count']=data['review'].apply(lambda x:get_char_count(x))","fbc6aa4e":"data.head()","dc4e09b3":"plt.hist(data[data['sentiment']==0]['word_count'],bins=200)\nplt.hist(data[data['sentiment']==1]['word_count'],bins=200)\nplt.xlim([0,60])\nplt.show()","2c3ddd6e":"plt.hist(data[data['sentiment']==0]['char_count'],bins=200)\nplt.hist(data[data['sentiment']==1]['char_count'],bins=200)\nplt.xlim([0,300])\nplt.show()","a48fe0fc":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.pipeline import Pipeline","79464515":"tfidf=TfidfVectorizer(tokenizer=data_cleaning)\nclassifier=SVC()","5c654acb":"X=data['review']\ny=data['sentiment']","317a23cc":"X_train,X_test,y_train,y_test= train_test_split(X,y,shuffle=True,random_state=0,test_size=0.2)","1e44dcd2":"X_train.shape, X_test.shape ","d85a6a70":"clf=Pipeline([('tfidf',tfidf),('clf',classifier)])","527d1b80":"clf.fit(X_train,y_train)","6484a3e8":"y_pred=clf.predict(X_test)\nreport=classification_report(y_test,y_pred)\nprint(report)","db43f436":"cm=confusion_matrix(y_test,y_pred)\nprint(cm)","424bfdae":"import spacy\nnlp=spacy.load('en_core_web_sm')","4f71ae8f":"x=\"hello world apple mango\"","bd0a24ff":"doc=nlp(x)","1e333b23":"for token in doc:\n    print(token.text, token.has_vector , token.vector.shape)","1c5cc66c":"def get_vector(x):\n    doc=nlp(x)\n    return doc.vector.reshape(-1,1)","72c26100":"data['vector']=data['review'].apply(lambda x:get_vector(x))","942901ef":"data.head()","002136ce":"data.loc[3]['vector'].shape","4a899879":"import tensorflow as tf","6d7de809":"X=np.concatenate(data['vector'].to_numpy(),axis=1)\nX=np.transpose(X)\ny=(data['sentiment']>1).astype(int)","8403fb17":"X.shape , y.shape","18e5e12e":"from tensorflow.keras.layers import Dense,Dropout,BatchNormalization,LSTM\nfrom tensorflow.keras.models import Sequential","93ea2702":"model=Sequential([\n    Dense(128,activation='relu'),\n    Dropout(0.25),\n    BatchNormalization(),\n    Dense(64,activation='relu'),\n    Dropout(0.25),\n    BatchNormalization(),\n    Dense(2,activation='sigmoid')\n])","fb3f1f56":"import tensorflow as tf\ny_oh=tf.keras.utils.to_categorical(y,num_classes=2)","4c5ae00b":"X_train,X_test,y_train,y_test=train_test_split(X,y_oh,random_state=2,test_size=0.2)","6ca3c63f":"model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory=model.fit(X_train,y_train,epochs=10,batch_size=32,validation_data=[X_test,y_test])","7dd0f23e":"from sklearn.metrics import confusion_matrix\ny_pred=model.predict(X_test)\ny_pred=np.argmax(y_pred,axis=1)\ny_pred.shape , y_test.shape\n","1e858e4b":"cm=confusion_matrix(y_test,y_pred)\ncm","2fbc4e48":"# using neural network","090c7720":"# now data cleaning","42dc72b7":"# now we will add vectors for all reviews","6db49c19":"## plotting sentiment with word count and char count","ba98ccff":"## entity visualization","7d88ccb4":"### char count","914bd682":"## word count","a9cca877":"## analysing sentiment of word count and char count","2eda2e28":"# vectorization with tfidf","45427ef8":"### lemmatization ","0068ebdf":"### removing stop words ans punctuations"}}