{"cell_type":{"884c92a8":"code","d2d15fb7":"code","5fd44363":"code","4d31266d":"code","6cf2df26":"code","1e461247":"code","ce6bd0d1":"code","05f13c2a":"code","447990c8":"code","cf80577a":"code","fe26cbe1":"code","46496af9":"code","ef92aa29":"code","d10ad02d":"code","2d829d5a":"code","0125a018":"code","ee15db6f":"code","3ef0cf91":"code","cf2f04ba":"code","4d3776f1":"code","4362078e":"code","fc59062b":"code","7e1d9443":"code","c556cdbb":"code","870d21fc":"code","67b8b8ba":"code","c3151076":"code","97d8bea3":"code","448eb987":"code","eddf2694":"code","046d5be1":"code","f1c60041":"code","adf39070":"code","4c2570e5":"code","d28c5fcd":"code","903a44a3":"code","165dd7d5":"code","eef61377":"code","a732a245":"code","65d80e82":"code","d1031637":"code","be9bde19":"code","e05272cc":"code","69cb391f":"code","6984512c":"code","0a47d605":"code","cdff5c1f":"code","e1c9b06e":"markdown","b1134843":"markdown","e338e1e6":"markdown","0a47b2be":"markdown","7bc7a024":"markdown","12e05ef1":"markdown"},"source":{"884c92a8":"import numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport os\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\n\nfrom datetime import datetime\nfrom datetime import timedelta\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nimport warnings\nfrom pandas.core.common import SettingWithCopyWarning\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2d15fb7":"#Upload main train data - UI weekly data - including all states\n#https:\/\/oui.doleta.gov\/unemploy\/claims.asp\ntrain_all = gpd.read_file(\"\/kaggle\/input\/ui-claims\/UIWeekly_all.csv\").drop(['geometry'],axis=1)\ntrain_all[\"Date\"] = pd.to_datetime(train_all[\"Date\"])\ntrain_all = train_all.query('Province_State!=\"Puerto Rico\" and Province_State!=\"Virgin Islands\"')\ntrain_all = train_all.query(\"Date>'2020-01-22'\")","5fd44363":"#Prepare a dataframe (train_df) with 51 states (from train_all) and daily frequency date from 2020-01-22 to 2020-05-04 to merge with daily frequency temporal data (i.e. Google trend, COVID quarantine etc.)\ntrain_start = datetime.strptime(\"2020-01-22\",\"%Y-%m-%d\")\ntrain_end = datetime.strptime(\"2020-05-04\",\"%Y-%m-%d\")\ndate_list = [train_start + timedelta(days=x) for x in range((train_end-train_start).days+1)]\nProvince_State = train_all.Province_State.unique()\n\ntrain_list = []\nfor x in range(len(Province_State)):\n    for y in range(len(date_list)):\n        innerlist = [Province_State[x],date_list[y]]\n        train_list.append(innerlist)\ntrain_df = pd.DataFrame(train_list,columns=[\"Province_State\",\"Date\"])","4d31266d":"#Upload Google Trend data and merge with train_df\n#https:\/\/trends.google.com\/trends\/explore?date=today%203-m&geo=US-WY&q=file%20for%20unemployment\nGoogle_trend_df = gpd.read_file(\"\/kaggle\/input\/ui-claims\/Google_Trend_v2.csv\").drop([\"geometry\"],axis=1)\nGoogle_trend_df[\"Date\"] = pd.to_datetime(Google_trend_df[\"Date\"])\nGoogle_trend_df[\"Google_Trend\"] = Google_trend_df[\"Google_Trend\"].astype(\"float\")\ntrain_df = train_df.merge(Google_trend_df,\n                          on=[\"Province_State\",\"Date\"],\n                          how=\"left\",\n                          )","6cf2df26":"#Since Google Trend data starts from 2020-02-06 (90days before 2020-05-3), fill the data for date before 2\/6 as 0 for Google_Trend\ntrain_df.loc[(train_df[\"Date\"]<\"2020-02-06\"),\"Google_Trend\"] = 0\ntrain_df.head(5)","1e461247":"#Upload quarantine (\"stay at home\") and restriction (\"public place\") data\n#https:\/\/www.usatoday.com\/storytelling\/coronavirus-reopening-america-map\/\nquarantine_data_df = gpd.read_file(\"\/kaggle\/input\/ui-claims\/Quarantine_v2.csv\")\nquarantine_data_df[\"SAH_start\"] = pd.to_datetime(quarantine_data_df[\"SAH_start\"],dayfirst=True,errors='coerce', format=\"%m\/%d\/%Y\")\nquarantine_data_df[\"SAH_end\"] = pd.to_datetime(quarantine_data_df[\"SAH_end\"])\nquarantine_data_df[\"Restriction_Start\"] = pd.to_datetime(quarantine_data_df[\"Restriction_Start\"])\nquarantine_data_df[\"Restriction_Easing\"] = pd.to_datetime(quarantine_data_df[\"Restriction_Easing\"])","ce6bd0d1":"#the use of cumsum() function will populate number 1 starting the date from quarantine data after merge.\ndef update_dates(a_df, col_update):\n    \"\"\"\n    This creates a boolean time series with one after the start of confinements (different types : schools, restrictions or quarantine)\n    \"\"\"\n    gpdf = a_df.groupby(\"Province_State\")\n    new_col = gpdf.apply(lambda df : df[col_update].notnull().cumsum()).reset_index(drop=True)\n    a_df[col_update] = new_col\n\nfor col in [\"SAH_start\", \"SAH_end\", \"Restriction_Start\",\"Restriction_Easing\"]:\n    train_df = train_df.merge(quarantine_data_df[[\"Province_State\", col]],\n                          left_on=[\"Province_State\", \"Date\"],\n                          right_on=[\"Province_State\", col],\n                          how=\"left\",\n                          )\n    update_dates(train_df, col)","05f13c2a":"#If quarantine ends or restriction being eased on Date A, fill 0 for the respective series from A onwards\ntrain_df['quarantine']=[row.SAH_start if row.SAH_end==0 else 0 for idx,row in train_df.iterrows()]\ntrain_df['restriction']=[row.Restriction_Start if row.Restriction_Easing==0 else 0 for idx,row in train_df.iterrows()]\ntrain_df.tail(10)","447990c8":"#Upload UI 2019 Q4 baseline and Population for Age 18+ data (population data were found not improving the model) and merge with train_df\n#https:\/\/oui.doleta.gov\/unemploy\/data_summary\/DataSum.asp\n#https:\/\/www.census.gov\/data\/tables\/time-series\/demo\/popest\/2010s-state-detail.html\nUI_base = gpd.read_file(\"\/kaggle\/input\/ui-claims\/UI_2019Q4_Base_v2.csv\").drop([\"geometry\"],axis=1)\nUI_base.rename({\"Initial Claims\": \"UI_base\"}, axis=1,inplace=True)\nUI_base[\"UI_base\"] = UI_base[\"UI_base\"].astype(\"float\")\nUI_base[\"Pop_above_18\"] = UI_base[\"Pop_above_18\"].astype(\"float\")\nUI_base[\"Pop_above_18\"] = UI_base[\"Pop_above_18\"]\/max(UI_base[\"Pop_above_18\"])\ntrain_df = train_df.merge(UI_base,\n                          on=[\"Province_State\"],\n                          how=\"left\",\n                          )","cf80577a":"#Upload GDP Industry data\ngdp_industry_df = gpd.read_file(\"\/kaggle\/input\/ui-claims\/GDP_Industry_v2.csv\").drop([\"geometry\"],axis=1)\nSector = [\"Agriculture\",\"Energy\",\"Utilities\",\"Construction\",\"Durable\",\"Nondurable\",\"Wholesale\",\"Retail\",\"Transportation\",\"Information\",\"Finance\",\"Real-estate\",\n\"Technology\",\"Management\",\"Services\",\"Education\",\"Health-care\",\"Entertainment\",\"Food\",\"Other\",\"Gov\"]\ngdp_industry_df[\"Sum\"] = gdp_industry_df[\"Sum\"].astype(\"float\")\nfor col in Sector:\n    gdp_industry_df[f\"{col}\"] = gdp_industry_df[f\"{col}\"].astype(\"float\")\n    gdp_industry_df[f\"{col}\"] = gdp_industry_df[f\"{col}\"]\/gdp_industry_df[\"Sum\"]\n\ntrain_df = train_df.merge(gdp_industry_df,\n                          on=[\"Province_State\"],\n                          how=\"left\",\n                          )","fe26cbe1":"#Upload UI daily data; UI daily data is used to enlarge the training dateset. That said, it also introduce data leakage. Hence UI daily data is only used for training not testing\n#The Daily UI data were found for the following 6 states:\n#DC:https:\/\/does.dc.gov\/publication\/unemployment-compensation-claims-data\n#MN:https:\/\/mn.gov\/deed\/data\/current-econ-highlights\/ui-statistics.jsp\n#MT:http:\/\/dli.mt.gov\/labor-market-information\n#NC:https:\/\/des.nc.gov\/need-help\/covid-19-information\/unemployment-claims-data\n#PA:https:\/\/www.uc.pa.gov\/COVID-19\/Pages\/UC-Claim-Statistics.aspx\n#WI:https:\/\/dwd.wisconsin.gov\/covid19\/public\/ui-stats.htm\nUI_daily_df = gpd.read_file(\"\/kaggle\/input\/ui-claims\/UIDaily.csv\").drop(['field_4','geometry'],axis=1)\nUI_daily_df[\"Date\"] = pd.to_datetime(UI_daily_df[\"Date\"])\nUI_daily_df.rename({\"Initial Claims\": \"UI\"}, axis=1,inplace=True)\ntrain_df_daily = UI_daily_df.merge(train_df,\n                          on=[\"Province_State\", \"Date\"],\n                          how=\"left\",\n                          )\ntrain_df_daily['UI'] = train_df_daily['UI'].astype(\"float\")\ntrain_df_daily = train_df_daily.query(\"Date>'2020-01-22'\")","46496af9":"#checkpoint length of train_df_daily is 339\nlen(train_df_daily)","ef92aa29":"#Create temperal data with series of 20 days and shift by 1 day\ndays_in_sequence = 21\n\ntrend_list = []\ndemo_input = []\n\nwith tqdm(total=len(list(train_df_daily.Province_State.unique()))) as pbar:\n    for province in train_df_daily.Province_State.unique():\n        province_df = train_df_daily.query(f\"Province_State=='{province}'\")\n        n = 0\n        #for i in range(0,len(province_df),int(days_in_sequence\/2)):\n        for i in range(0,len(province_df),1):\n            n += 1\n            if i+days_in_sequence<=len(province_df):\n                #prepare all the temporal inputs\n                google_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].Google_Trend.values]\n                restriction_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].restriction.values]\n                quarantine_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].quarantine.values]\n\n                #preparing all the demographic inputs\n                demo_input = []\n                for col in Sector:\n                    col = float(province_df[f\"{col}\"].iloc[0])\n                    demo_input.append(col)\n\n                expected_trend = float(province_df.iloc[i+days_in_sequence-1].Google_Trend)\n                #expected weekly claim is the sum of past 7 days including today\n                expected_claims = 0\n                for a in range(i+days_in_sequence-7, i+days_in_sequence):\n                    expected_claims = expected_claims + float(province_df.iloc[a].UI)\n                expected_claims_norm = expected_claims \/ float(province_df.iloc[i].UI_base) * 13\n\n                trend_list.append({\"google_trend\":google_trend,\n                                   \"restriction_trend\":restriction_trend,\n                                   \"quarantine_trend\":quarantine_trend,\n                                   \"demographic_inputs\":demo_input,\n                                   \"expected_trends\":expected_trend,\n                                   \"expected_claims\":expected_claims_norm})\n        pbar.update(1)\ntrend_df_daily = pd.DataFrame(trend_list)","d10ad02d":"#After adjust the Daily UI in 2020 by 2019Q4 UI base, the variables are in the same scale, no further normalization is needed.\ntrend_df_daily.describe().round(2)","2d829d5a":"#Checkpoint number of UI daily data is 219\nlen(trend_df_daily)","0125a018":"#Prepare UI Weekly data for the remaining 45 states (51-6)\ns_list = list(train_df_daily.Province_State.unique())+[\"Guam\"]\nquery_df = train_df[~train_df[\"Province_State\"].isin(s_list)]\nlen(query_df.Province_State.unique())","ee15db6f":"UI_weekly_df = train_all[~train_all[\"Province_State\"].isin(s_list)]\nlen(UI_weekly_df.Province_State.unique())","3ef0cf91":"train_df_weekly = query_df.merge(UI_weekly_df,\n                          on=[\"Province_State\", \"Date\"],\n                          how=\"left\",\n                          )\ntrain_df_weekly['Initial_Claims'] = train_df_weekly['Initial_Claims'].astype(\"float\")\ntrain_df_weekly['UI_norm'] = train_df_weekly['Initial_Claims']\/(train_df_weekly['UI_base']\/13)","cf2f04ba":"train_df_weekly = train_df_weekly.query(\"Date>='2020-01-26' and Date<='2020-04-18'\")\ndays_in_sequence = 21\n\ntrend_list = []\ndemo_input = []\n\nwith tqdm(total=len(list(train_df_weekly.Province_State.unique()))) as pbar:\n    for province in train_df_weekly.Province_State.unique():\n        province_df = train_df_weekly.query(f\"Province_State=='{province}'\")\n        n = 0\n        for i in range(0,len(province_df),int(days_in_sequence\/3)):\n            n += 1\n            if i+days_in_sequence<=len(province_df):\n                #prepare all the temporal inputs\n                google_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].Google_Trend.values]\n                restriction_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].restriction.values]\n                quarantine_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].quarantine.values]\n\n                #preparing all the demographic inputs\n                demo_input = []\n                for col in Sector:\n                    col = float(province_df[f\"{col}\"].iloc[0])\n                    demo_input.append(col)\n\n                expected_trend = float(province_df.iloc[i+days_in_sequence-1].Google_Trend)\n                #expected weekly claim is the sum of past 7 days including today\n                expected_claims = float(province_df.iloc[i+days_in_sequence-1].UI_norm)\n\n                trend_list.append({\"google_trend\":google_trend,\n                                   \"restriction_trend\":restriction_trend,\n                                   \"quarantine_trend\":quarantine_trend,\n                                   \"demographic_inputs\":demo_input,\n                                   \"expected_trends\":expected_trend,\n                                   \"expected_claims\":expected_claims})\n        pbar.update(1)\ntrend_df_weekly = pd.DataFrame(trend_list)","4d3776f1":"#Checkpoint: length of trend_df_weekly is 450\nlen(trend_df_weekly)","4362078e":"trend_df_daily[\"temporal_inputs\"] = [np.asarray([trends[\"google_trend\"],trends[\"restriction_trend\"],trends[\"quarantine_trend\"]]) for idx,trends in trend_df_daily.iterrows()]\ntrend_df_daily = shuffle(trend_df_daily)","fc59062b":"trend_df_weekly[\"temporal_inputs\"] = [np.asarray([trends[\"google_trend\"],trends[\"restriction_trend\"],trends[\"quarantine_trend\"]]) for idx,trends in trend_df_weekly.iterrows()]\ntrend_df_weekly = shuffle(trend_df_weekly)","7e1d9443":"#Split training and testing data only from trend_df_weekly to avoid data leakage\nsequence_length = 20\nX = trend_df_weekly[[\"temporal_inputs\",\"demographic_inputs\"]]\ny = trend_df_weekly[[\"expected_trends\",\"expected_claims\"]]\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=617)\nx_train = x_train.append(trend_df_daily[[\"temporal_inputs\",\"demographic_inputs\"]], sort=False)\ny_train = y_train.append(trend_df_daily[[\"expected_trends\",\"expected_claims\"]], sort=False)","c556cdbb":"#Checkpoint length of training_item_count is 579\ntraining_item_count = len(x_train)\nvalidation_item_count = len(x_test)\ntraining_item_count","870d21fc":"#transpose the shape of x_temporal_train to (training_item_count, 20 = sequence_length, 3)\nX_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in x_train[\"temporal_inputs\"].values]),(training_item_count,3,sequence_length)),(0,2,1) )).astype(np.float32)\nX_demographic_train = np.asarray([np.asarray(x) for x in x_train[\"demographic_inputs\"]]).astype(np.float32)\nY_trends_train = np.asarray([np.asarray(x) for x in y_train[\"expected_trends\"]]).astype(np.float32)\nY_claims_train = np.asarray([np.asarray(x) for x in y_train[\"expected_claims\"]]).astype(np.float32)","67b8b8ba":"X_temporal_test = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in x_test[\"temporal_inputs\"]]),(validation_item_count,3,sequence_length)),(0,2,1)) ).astype(np.float32)\nX_demographic_test = np.asarray([np.asarray(x) for x in x_test[\"demographic_inputs\"]]).astype(np.float32)\nY_trends_test = np.asarray([np.asarray(x) for x in y_test[\"expected_trends\"]]).astype(np.float32)\nY_claims_test = np.asarray([np.asarray(x) for x in y_test[\"expected_claims\"]]).astype(np.float32)","c3151076":"#Kept the architecture from [Francois Lemarchand](https:\/\/www.kaggle.com\/frlemarchand\/covid-19-forecasting-with-an-rnn\/comments#793512)'s Notebook to generate 2 seperate Losses, \n#and merged the parallel cells from Google Trend forecast to Claim output forecast\n\n#temporal input branch\ntemporal_input_layer = Input(shape=(sequence_length,3))\nmain_rnn_layer = layers.LSTM(64, return_sequences=True, recurrent_dropout=0.2)(temporal_input_layer)\n\n#demographic input branch\ndemographic_input_layer = Input(shape=(21))\ndemographic_dense = layers.Dense(16)(demographic_input_layer)\ndemographic_dropout = layers.Dropout(0.2)(demographic_dense)\n\n#trends output branch\nrnn_t = layers.LSTM(32)(main_rnn_layer)\nmerge_t = layers.Concatenate(axis=-1)([rnn_t,demographic_dropout])\ndense_t = layers.Dense(128)(merge_t)\ndropout_t = layers.Dropout(0.3)(dense_t)\ntrends = layers.Dense(1, activation=layers.LeakyReLU(alpha=0.1),name=\"trends\")(dropout_t)\n\n#claim output branch\n#change the structure here to merge dropout_t as well\nrnn_c = layers.LSTM(32)(main_rnn_layer)\nmerge_c = layers.Concatenate(axis=-1)([rnn_c,dropout_t,demographic_dropout])\ndense_c = layers.Dense(128)(merge_c)\ndropout_c = layers.Dropout(0.3)(dense_c)\nclaims = layers.Dense(1, activation=layers.LeakyReLU(alpha=0.1), name=\"claims\")(dropout_c)\n\n\nmodel = Model([temporal_input_layer,demographic_input_layer], [trends,claims])\n\nmodel.summary()","97d8bea3":"callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=1, factor=0.6),\n             EarlyStopping(monitor='val_loss', patience=20),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\nmodel.compile(loss=[tf.keras.losses.MeanSquaredLogarithmicError(),tf.keras.losses.MeanSquaredLogarithmicError()], optimizer=\"adam\")","448eb987":"history = model.fit([X_temporal_train,X_demographic_train], [Y_trends_train, Y_claims_train], \n          epochs = 250, \n          batch_size = 16, \n          validation_data=([X_temporal_test,X_demographic_test],  [Y_trends_test, Y_claims_test]), \n          callbacks=callbacks)","eddf2694":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss over epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","046d5be1":"plt.plot(history.history['trends_loss'])\nplt.plot(history.history['val_trends_loss'])\nplt.title('Loss over epochs for the number of Google Trend Search for \"File for Unemployment\"')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","f1c60041":"plt.plot(history.history['claims_loss'])\nplt.plot(history.history['val_claims_loss'])\nplt.title('Loss over epochs for the number of initial claims')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","adf39070":"#Load the best model selected\nmodel.load_weights(\"\/kaggle\/input\/select-model\/best_model_v32.h5\")\nscore_train = model.evaluate([X_temporal_train,X_demographic_train], [Y_trends_train, Y_claims_train], verbose=0)\nscore_test = model.evaluate([X_temporal_test,X_demographic_test], [Y_trends_test, Y_claims_test], verbose=0)\nprint(f\"train MSLE score is {score_train}\".format(score_train))\nprint(f\"test MSLE score is {score_test}\".format(score_test))","4c2570e5":"predictions = model.predict([X_temporal_test,X_demographic_test])","d28c5fcd":"display_limit = 30\nfor inputs, pred_trends, exp_trends, pred_claims, exp_claims in zip(X_temporal_test,predictions[0][:display_limit], Y_trends_test[:display_limit], predictions[1][:display_limit], Y_claims_test[:display_limit]):\n    print(\"================================================\")\n    print(inputs)\n    print(\"Expected trends:\", exp_trends, \" Prediction:\", pred_trends[0], \"Expected Claims:\", exp_claims, \" Prediction:\", pred_claims[0] )","903a44a3":"#Prepare input data for forecasting\ndef build_inputs_for_date(province, date, df):\n    \n    start_date = date - timedelta(days=20)\n    end_date = date - timedelta(days=1)\n    \n    str_start_date = start_date.strftime(\"%Y-%m-%d\")\n    str_end_date = end_date.strftime(\"%Y-%m-%d\")\n\n    df = df.query(\"Province_State=='\"+province+\"' and Date>='\"+str_start_date+\"' and Date<='\"+str_end_date+\"'\")\n    #print(df)\n    \n    #preparing the temporal inputs\n    temporal_input_data = np.transpose(np.reshape(np.asarray([df[\"Google_Trend\"],\n                                                 df[\"restriction\"],\n                                                 df[\"quarantine\"]]),\n                                     (3,sequence_length)), (1,0) ).astype(np.float32)\n    \n    demographic_input_data = []\n    for col in Sector:\n        col = float(df[f\"{col}\"].iloc[0])\n        demographic_input_data.append(col)\n    \n    return [np.array([temporal_input_data]), np.array([demographic_input_data])]","165dd7d5":"#Generate forecast and append predicted google trend into input data\ndef predict_for_region(province, df):\n    # start from 2020-04-19\n    restriction_end_date = datetime.strptime(\"2020-07-04\",\"%Y-%m-%d\")\n    quarantine_end_date = datetime.strptime(\"2020-06-15\",\"%Y-%m-%d\")\n    \n    begin_prediction = \"2020-04-19\"\n    start_date = datetime.strptime(begin_prediction,\"%Y-%m-%d\")\n    end_prediction = \"2020-06-30\"\n    end_date = datetime.strptime(end_prediction,\"%Y-%m-%d\")\n    \n    date_list = [start_date + timedelta(days=x) for x in range((end_date-start_date).days+1)]\n    \n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n    for date in date_list:\n        \n        input_data = build_inputs_for_date(province, date, df)\n        result = model.predict(input_data)\n        \n        COVID_df = quarantine_data_df.query(f\"Province_State=='{province}'\")[[\"SAH_end\",\"Restriction_Easing\"]]\n        \n        if date <= datetime.strptime(\"2020-05-09\",\"%Y-%m-%d\"):\n            G_trend = G_trend_all.query(f\"Province_State=='{province}' and Date == '{date}'\").iloc[0][2]\n        else:\n            G_trend = result[0][0][0]\n        \n        #print(G_trend)\n        \n        quarantine = 0 if COVID_df.iloc[0][0]<date else 1\n        restriction = 0 if COVID_df.iloc[0][1]<date else 1\n        ui_base = UI_base.query(\"Province_State=='\"+province+\"'\").iloc[0][1]\n        \n\n        df = df.append({\"Province_State\":province, \n                        \"Date\":date.strftime(\"%Y-%m-%d\"), \n                        #\"restriction\": 0 if date >restriction_end_date else input_data[0][0][-1][1],\n                        #\"quarantine\": 0 if date >quarantine_end_date else input_data[0][0][-1][2],\n                        \"restriction\": 0 if date >restriction_end_date else restriction,\n                        \"quarantine\": 0 if date >quarantine_end_date else quarantine,\n                        \"UI_base\": ui_base,\n                        #\"Google_Trend\": 0 if result[0][0][0]<0 else result[0][0][0],\n                        \"Google_Trend\": 0 if G_trend <0 else G_trend,\n                        \"UI_norm\": result[1][0][0],\n                        #\"Pop_above_18\":input_data[1][0][0],\n                        \"Agriculture\":input_data[1][0][0],\n                        \"Energy\":input_data[1][0][1],\n                        \"Utilities\":input_data[1][0][2],\n                        \"Construction\":input_data[1][0][3],\n                        \"Durable\":input_data[1][0][4],\n                        \"Nondurable\":input_data[1][0][5],\n                        \"Wholesale\":input_data[1][0][6],\n                        \"Retail\":input_data[1][0][7],\n                        \"Transportation\":input_data[1][0][8],\n                        \"Information\":input_data[1][0][9],\n                        \"Finance\":input_data[1][0][10],\n                        \"Real-estate\":input_data[1][0][11],\n                        \"Technology\":input_data[1][0][12],\n                        \"Management\":input_data[1][0][13],\n                        \"Services\":input_data[1][0][14],\n                        \"Education\":input_data[1][0][15],\n                        \"Health-care\":input_data[1][0][16],\n                        \"Entertainment\":input_data[1][0][17],\n                        \"Food\":input_data[1][0][18],\n                        \"Other\":input_data[1][0][19],\n                        \"Gov\":input_data[1][0][20]},\n                       ignore_index=True)\n        \n        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n            \n    return df","eef61377":"#Forecast using latest Google Trend data\nNew_Google_Trend = gpd.read_file(\"\/kaggle\/input\/forecast-0509\/Google_trend_update_0509.csv\").drop([\"geometry\"],axis=1)\nNew_Google_Trend[\"Date\"] = pd.to_datetime(New_Google_Trend[\"Date\"])\nNew_Google_Trend = New_Google_Trend.query(\"Date >= '2020-05-04'\")\nNew_Google_Trend[\"Google_Trend\"] = New_Google_Trend[\"Google_Trend\"].astype(\"float\")","a732a245":"G_trend_all = Google_trend_df.append(New_Google_Trend)","65d80e82":"forecast_weekly = train_df.merge(train_all,\n                          on=[\"Province_State\", \"Date\"],\n                          how=\"left\",\n                          )\nforecast_weekly['Initial_Claims'] = forecast_weekly['Initial_Claims'].astype(\"float\")\nforecast_weekly['UI_norm'] = forecast_weekly['Initial_Claims']\/(forecast_weekly['UI_base']\/13)\nforecast_weekly = forecast_weekly.query(\"Date>='2020-01-26' and Date<='2020-04-18'\")","d1031637":"#Generate forecast\nforecast_df = forecast_weekly\nwith tqdm(total=len(list(forecast_df.Province_State.unique()))) as pbar:\n    for province in forecast_df.Province_State.unique():\n        forecast_df = predict_for_region(province, forecast_df)\n        pbar.update(1)","be9bde19":"forecast_df.to_csv(\"forecast_result_0509.csv\")","e05272cc":"begin_actual = \"2020-02-01\"\nstart_date = datetime.strptime(begin_actual,\"%Y-%m-%d\")\nend_prediction = \"2020-06-30\"\nend_date = datetime.strptime(end_prediction,\"%Y-%m-%d\")\ndate_list = [start_date + timedelta(days=7*x) for x in range(int((end_date-start_date).days\/7)+1)]\nforecast_df['UI'] = forecast_df['UI_norm']*forecast_df['UI_base']\/13\nforecast_df_v2 = forecast_df[forecast_df[\"Date\"].isin(date_list)][[\"Date\",\"Province_State\",\"UI\"]]","69cb391f":"#Create summarized actuals for National Wide result\nActuals = forecast_df.groupby(['Date'])['UI'].sum().reset_index()\nActuals = Actuals[Actuals[\"Date\"].isin(date_list)]\nActuals[\"Province_State\"] =\"US\"\nforecast_df_v2 = forecast_df_v2.append(Actuals,sort=False)","6984512c":"def plot_prediction(province, forcast_start):\n    copy_df_new = forecast_df_v2.query(\"Province_State=='\"+province+\"'\")\n    copy_df_new[\"Actual\"] = [row.UI if row.Date <= forcast_start else None for idx,row in copy_df_new.iterrows()]\n    copy_df_new[\"Prediction\"] = [row.UI if row.Date >= forcast_start else None for idx,row in copy_df_new.iterrows()]\n    copy_df_new.iloc[int((datetime.strptime(\"2020-04-25\",\"%Y-%m-%d\")-start_date).days\/7),3] = UI_0425[province]\n    copy_df_new.iloc[int((datetime.strptime(\"2020-05-02\",\"%Y-%m-%d\")-start_date).days\/7),3] = UI_0502[province]\n    copy_df_new[\"Date\"] = copy_df_new[\"Date\"].dt.strftime(\"%m\/%d\/%Y\")\n    print(copy_df_new)\n    plt.plot(copy_df_new.Actual.values)\n    plt.plot(copy_df_new.Prediction.values)\n    plt.title(f\"{province} Unemployment Initial Claims Forecast\".format(province))\n    plt.ylabel('Number of initial claims')\n    plt.xlabel('Date')\n    plt.xticks(range(len(copy_df_new.Date.values)),copy_df_new.Date.values,rotation='vertical')\n    plt.legend(['Actual', 'Prediction'], loc='best')\n    plt.show()","0a47d605":"####Add latest actual data starting from 04-25\nUI_0425 = {\n    \"US\": 3846000,\n    \"New York\" : 222040,\n    \"California\" : 325343,\n    \"Pennsylvania\" : 114700,\n    \"Michigan\" : 82004,\n    \"Illinois\" : 81596,\n    \"Florida\": 433103,\n    \"Texas\": 254084,\n    \"Georgia\": 266565\n    }\nUI_0502 = {\n    \"US\": 3169000,\n    \"New York\" : 197607,\n    \"California\" : None,\n    \"Pennsylvania\" : 81779,\n    \"Michigan\" : None,\n    \"Illinois\" : 74476,\n    \"Florida\": None,\n    \"Texas\": None,\n    \"Georgia\": None\n    }","cdff5c1f":"warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\nfor key in UI_0502:\n    plt.figure()\n    plot_prediction(key, datetime.strptime(\"2020-04-18\",\"%Y-%m-%d\"))","e1c9b06e":"# 4. Performance during training","b1134843":"# 2. Preparing the training data","e338e1e6":"# 6. Plot the Predictions","0a47b2be":"# 3. Build the model","7bc7a024":"# 1. Introduction\n\nstroll down to the bottom to see the estimated initial claims by Quarter end (predicted using data as of 4\/18)\n\n![](http:\/\/)\nCredit to [Francois Lemarchand](https:\/\/www.kaggle.com\/frlemarchand\/covid-19-forecasting-with-an-rnn\/comments#793512)\n\nCreated this Unemployment Initial Claim forecast based on [Francois Lemarchand](https:\/\/www.kaggle.com\/frlemarchand\/covid-19-forecasting-with-an-rnn\/comments#793512)'s Notebook: forecasting COVID-19 cases\/fatalities using RNN\n\nThe RNN will take as inputs:\n*     Google Search Trend on \"File for Unemployment\n*     COVID-19 quarantine \/ restrictions applied for each state in the past 20 days (including updates on quarantine end \/ restriction easing data)\n*     additional information related to the state (2019 UI Claims baseline, GDP contribution by Industry) \n\nas outputs:\n*    weekly initial claim for the 21th day\n*    number of google searches for the 21th day","12e05ef1":"# 5. Generate predictions using the model"}}