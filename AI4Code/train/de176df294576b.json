{"cell_type":{"cd887ca7":"code","7f6269be":"code","4aad2e6c":"code","1304f0e3":"markdown","597bcdee":"markdown","d293491c":"markdown","0a8d7eda":"markdown"},"source":{"cd887ca7":"import numpy as np\nfrom sklearn.metrics import label_ranking_average_precision_score\n","7f6269be":"y_pred = 0.5 * np.ones((1, 24))\nfor i in range(25):\n    y_true = np.zeros((1, 24))\n    y_true[0, :i] = 1\n    score = label_ranking_average_precision_score(y_true, y_pred)\n    print(i, '%0.3f' % score, y_true)","4aad2e6c":"y_pred = 0.5 * np.ones((1, 24))\nfor i in range(25):\n    y_true = np.zeros((1, 24))\n    y_true[0, i:] = 1\n    score = label_ranking_average_precision_score(y_true, y_pred)\n    print(i, '%0.3f' % score, y_true)","1304f0e3":"We see that sample submisison score of 0.185 lies between the scores with 4 or 5 targets equal to one.\n\nWait a minute.  Is the ordering of targets relevant?  No, according to the metric definition.  \n\nLet's check by reversing the order of targets.","597bcdee":"# There are between 4 and 5 songs per recording on average.\n\nThe sample submission get a score of 0.185.  What can we get out of this?\n\nFrist, let's import what we need. The competition metric is said to be similar to sklean `label_ranking_average_precision_score` except for rows weights.  IF we have a single row then they should be equal.","d293491c":"We see that 0.185 still lies between the scores of row with 4 and 5 targets equal to one.\n\nHow can we use this information?  I don't know for now.  It would certainly be helpful if the metric was about rounded predictions, like in the Cornell birsdsong competition.  Here the ordering of predictions is what maters, and knowing how many should be close to 1 is not very helpful.  Yet, why not sharing this?\n","0a8d7eda":"let's compute what a constant 0.5 submission, as in sample submission, would yield with rows of varying number of songs."}}