{"cell_type":{"d47aee2e":"code","33ac6ffe":"code","2c95d609":"code","05a62b2e":"code","df4efc13":"code","5e400bd8":"code","51228deb":"code","b3f7ee52":"code","b6e93248":"code","1f445e63":"code","fe00b7db":"code","92f89239":"code","bcd7785d":"code","f6378e69":"code","9b854678":"code","293ba4ae":"code","02c89881":"code","67840b45":"code","d1c635ed":"code","a4ec218a":"code","001d2a23":"code","c73d8098":"code","ea9e100d":"code","9fe085c7":"code","6d967e00":"markdown","4f788415":"markdown","5ce311ca":"markdown","04caa72d":"markdown","af172817":"markdown","e03b871e":"markdown","9e036291":"markdown","f540a097":"markdown","58880ba9":"markdown","e80ebd7e":"markdown","81af7f69":"markdown","329243a1":"markdown","62f56e60":"markdown"},"source":{"d47aee2e":"# import necessary libraries\nimport os\nimport torch\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nos.listdir('..\/input\/')\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","33ac6ffe":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2c95d609":"%matplotlib inline","05a62b2e":"# load the dataset\ndata = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","df4efc13":"# check for basic information\ndata.info()","5e400bd8":"# show first few recoreds\ndata.head()","51228deb":"# check for any missing values\ndata.isnull().sum()","b3f7ee52":"# diagnosis distribution\ndata['diagnosis'].value_counts()","b6e93248":"# encode categorical data\ndata['diagnosis'].replace({'M': 1, 'B': 0}, inplace = True)","1f445e63":"Y = data['diagnosis'].to_numpy()","fe00b7db":"X = data.drop(['id', 'diagnosis', 'Unnamed: 32'], axis = 1)","92f89239":"# split the dataset into the training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3)","bcd7785d":"# feature scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","f6378e69":"# create the model\nmodel = torch.nn.Linear(X_train.shape[1], 1)","9b854678":"# load sets in format compatible with pytorch\nX_train = torch.from_numpy(X_train.astype(np.float32))\nX_test = torch.from_numpy(X_test.astype(np.float32))","293ba4ae":"y_train = torch.from_numpy(y_train).float().reshape(-1, 1)\ny_test = torch.from_numpy(y_test).float().reshape(-1, 1)","02c89881":"def configure_loss_function():\n    return torch.nn.BCEWithLogitsLoss()","67840b45":"# use Adam optimiser for gradient descent\ndef configure_optimizer(model):\n    return torch.optim.Adam(model.parameters(), lr = 0.0007)","d1c635ed":"# define the loss function to compare the output with the target\ncriterion = configure_loss_function()\noptimizer = configure_optimizer(model)","a4ec218a":"# run the model\nepochs = 2000\n# initialise the train_loss & test_losses which will be updated\ntrain_losses = np.zeros(epochs)\ntest_losses = np.zeros(epochs)\n\nfor epoch in range(epochs): \n    y_pred = model(X_train)\n    loss = criterion(y_pred, y_train)\n    # clear old gradients from the last step\n    optimizer.zero_grad()\n    # compute the gradients necessary to adjust the weights\n    loss.backward()\n    # update the weights of the neural network\n    optimizer.step()\n\n    outputs_test = model(X_test)\n    loss_test = criterion(outputs_test, y_test)\n\n    train_losses[epoch] = loss.item()\n    test_losses[epoch] = loss_test.item()\n\n    if (epoch + 1) % 50 == 0:\n      print (str('Epoch ') + str((epoch+1)) + str('\/') + str(epochs) + str(',  training loss = ') + str((loss.item())) + str(', test loss = ') + str(loss_test.item()))","001d2a23":"# visualise the test and train loss\nplt.plot(train_losses, label = 'train loss')\nplt.plot(test_losses, label = 'test loss')\nplt.legend()\nplt.title('Model Loss')","c73d8098":"with torch.no_grad():\n  output_train = model(X_train)\n  output_train = (output_train.numpy() > 0)\n\n  train_acc = np.mean(y_train.numpy() == output_train)\n\n  output_test = model(X_test)\n  output_test = (output_test.numpy() > 0)\n  \n  test_acc = np.mean(y_test.numpy() == output_test)","ea9e100d":"print ('Train accuracy is: ' + str(train_acc))","9fe085c7":"print ('Test accuracy is: ' + str(train_acc))","6d967e00":"The dataset consists of features that describe characteristics of the cell\nnuclei present in a digitised image, these features are defined as follows:\n\n1. **Radius:** the average distance from the center of the nucleus to each of the boundary points<br>\n2. **Texture:** the standard deviation of the gray-scale values, the gray-scale value represents the intensity of the shades of gray in each pixel of the image<br>\n3. **Perimeter:** the total distance of the boundary of the cell nucleus<br>\n4. **Area:** the number of pixels on the interior of the boundary and adding one-half of the pixels on the perimeter, to correct for the error caused by digitisation<br>\n5. **Smoothness:** the difference between the length of a radius length and the mean length of the two radius lines surrounding it, hence the local variation in radius lengths<br>\n6. **Compactness:** the perimeter and area are combined to obtain a measure of compactness of the cell nuclei<br>\n7. **Concavity:** the severity of concave portions of the contour, a high concavity means that the boundary of the cell nucleus has indentations, and thus is rather rough than smooth<br>\n8. **Concave points:** the number of concave portions of the contour of the cell nucleus<br>\n9. **Symmetry:** the symmetry is determined by first finding the longest line from boundary point to boundary point through the center of the nucleus, subsequently the relative length differences between the lines perpendicular to the longest line to the boundary in both directions are measured, attention should be given to nuclei where the longest line cuts through the boundary because of concavity<br>\n10. **Fractal dimension:** the fractal dimension is approximated by the \"coastline approximation\", the perimeter of the nucleus can be measured using different lengths of measuring sticks, as this length increases, the total length of the measured \"coastline\" decreases due to lower precision of the measurement, the theoretical fractal dimension is then determined by dividing the logarithm of the observed perimeter L(s) by the logarithm of the measuring stick length s, plotting log(L(s)) against log(s) and determining the negative value of the slope results in an approximation of the fractal dimension D, finally, the desired feature is determined by the calculation D - 1.","4f788415":"This notebook performs a deep learning algorithm to train a deep neural network with the breast cancer wisconsin (diagnostic) [dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29) from UCI Machine Learning Repository to predict breast cancer whether the tumor is benign or malignant.","5ce311ca":"Good, our model has been trained, now it's time to show the loss and accuracy","04caa72d":"Calculate the backward over the validation set","af172817":"Now we can make sure the data is nicely distributed. Then the next step is to create the model with training data and afterwards, testing is done using test data.","e03b871e":"Next step is to prepare the training and test sets, id and \"Unamed: 32\" columns won't help us in the prediction so we gonna drop them","9e036291":"Main target is the diagnosis column (M = malignant or B = benign) so let's turn the strings into 1 and 0","f540a097":"Finally, we can now specify the hyperparameters and iterate through the train data to run the model","58880ba9":"We didn't use the sigmoid activation function here. Rather, we used the binary cross-entropy with logits loss function instead of the binary cross-entropy loss function combined with the sigmoid function because it's more numerically stable and leads to better results than using a plain Sigmoid followed by a BCELoss.","e80ebd7e":"To have better results, a bit of scaling is important so that neither of features dominate the other","81af7f69":"The dataset has 30 features (in vector format) rather than 10 because a single feature contains MEAN radius, SE radius, WORST radius, etc. The problem is straightforward, Scientists collected features on patients with (malignant) or without (benign) breast cancer","329243a1":"As we have our model compiled, it's time for training","62f56e60":"# Breast Cancer Classification with Deep Learning"}}