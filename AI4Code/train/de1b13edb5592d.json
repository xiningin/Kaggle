{"cell_type":{"5daded11":"code","0923b427":"code","2f327c93":"code","426dff05":"code","a8ef40c9":"code","b4525072":"code","489ab8ce":"code","dfef6555":"code","a69c3a32":"code","2f97ced2":"code","0b2ad49d":"code","d1f056b6":"code","28850620":"code","23e6ac32":"code","5959345b":"code","65371e4a":"code","0473cd7b":"code","5d1e05ff":"code","15d557c3":"code","8bf9e900":"code","552fb81a":"code","ded78ca1":"code","0a82c803":"code","1d7503d1":"code","0e135284":"code","40effe6a":"code","7c12afb2":"code","7a45dd64":"code","e3c6c4be":"markdown","9c37e3f4":"markdown","c7421e90":"markdown","19175350":"markdown","906f886d":"markdown","ce779a76":"markdown","3da16e58":"markdown","ea2fa3b5":"markdown","70ea97ca":"markdown","544d5739":"markdown","6babd931":"markdown","59e774e9":"markdown","b901fa5f":"markdown"},"source":{"5daded11":"import pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom fastai.text import *\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","0923b427":"df = pd.read_csv('..\/input\/twitter-airline-sentiment\/Tweets.csv')","2f327c93":"df.head()","426dff05":"# Distribution of the respective 3 sentiments.\nsns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"airline_sentiment\", data=df)","a8ef40c9":"# Distribution of the negative sentiment.\nax = sns.countplot(x='negativereason',data=df)\nax.set_xticklabels(ax.get_xticklabels(), rotation=60)\n","b4525072":"import re\n\ndef removeUnicode(text):\n  \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n  text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', text)       \n  text = re.sub(r'[^\\x00-\\x7f]',r'',text)\n  return text\n  \ndef replaceURL(text):\n  \"\"\"Replaces url address with \"url\" \"\"\"\n  text = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','url',text)\n  text = re.sub(r'#([^\\s]+)', r'\\1', text)\n  return text\n\ndef replaceAtUser(text):\n  \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n  # text = re.sub('@[^\\s]+','atUser',text)\n  text = re.sub('@[^\\s]+','',text)\n  return text\n\ndef removeHashtagInFrontOfWord(text):\n  \"\"\" Removes hastag in front of a word \"\"\"\n  text = re.sub(r'#([^\\s]+)', r'\\1', text)\n  return text\n\ndef removeNumbers(text):\n  \"\"\" Removes integers \"\"\"\n  text = ''.join([i for i in text if not i.isdigit()])         \n  return text\n\ndef removeEmoticons(text):\n  \"\"\" Removes emoticons from text \"\"\"\n  text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=\/|:\/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n  return text\n\n\n\"\"\" Replaces contractions from a string to their equivalents \"\"\"\ncontraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\ndef replaceContraction(text):\n  patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n  for (pattern, repl) in patterns:\n      (text, count) = re.subn(pattern, repl, text)\n  return text","489ab8ce":"def preprocessTwitterData(df):\n  \"\"\"Function to apply text preprocessing functions to a dataframe\"\"\"\n  \n  # remove unicode\n  df['text'] = df['text'].apply(removeUnicode)\n  \n  # replace url\n  df['text'] = df['text'].apply(replaceURL)\n  \n  # replace '@' signs\n  df['text'] = df['text'].apply(replaceAtUser)\n  \n  \n  # replace hastags\n  df['text'] = df['text'].apply(removeHashtagInFrontOfWord)\n  \n  # remove numbers in the tweets\n  df['text'] = df['text'].apply(removeNumbers)\n  \n  # remove the emoticons\n  df['text'] = df['text'].apply(removeEmoticons)\n  \n  # replace contractions\n  df['text'] = df['text'].apply(replaceContraction)\n  \n# Call the function and preprocess the data  \npreprocessTwitterData(df)\n\n# Since we don't need the rest of the columns in the data, subindex the relevant columns and make this the new dataframe\ndf = df[['text','airline_sentiment']]\n\n# Split the dataset into a train and test set.\n# Using a validation set is built into the fastai API, so we don't need to do this split ourselves\n\n# use an 80-20 split for the train and test sets\ndf_train, df_test = train_test_split(df,test_size=0.1,random_state=20)\n\n# Convert the cleaned training and testing data into their own CSV files which we can import later to perform modeling on them\n\ndf_train.to_csv('twitter_data_cleaned_train.csv')\ndf_test.to_csv('twitter_data_cleaned_test.csv')","dfef6555":"# Create a 'TextLMDataBunch' from a csv file.\n# We specify 'valid=0.1' to signify that when we want to actually put this into our language model, we'll be setting off 10% of it for a validation set\ndata_batch = TextLMDataBunch.from_csv(path='',csv_name='twitter_data_cleaned_train.csv',valid_pct=0.1)\n\n# run this to see how the batch looks like\ndata_batch.show_batch()","a69c3a32":"# pass in our 'data_lm' objet to specify our Twitter data\n# pass in AWD_LSTM to specify that we're using this particular language model\ntweet_model = language_model_learner(data_batch, AWD_LSTM, drop_mult=0.3)","2f97ced2":"tweet_model.model","0b2ad49d":"# fastai learning-rate finding\n# implemented using fastai callbacks\n#learning_rate_finder is used to find the optimal learning rate .\ntweet_model.lr_find()","d1f056b6":"# plot the graph we were talking about earlier\ntweet_model.recorder.plot()","28850620":"# We set cycle_len to 1 because we only train with one epoch 'moms' refers to a tuple with the form (max_momentum,min_momentum)\ntweet_model.fit_one_cycle(cyc_len=1,max_lr=1e-1,moms=(0.85,0.75))","23e6ac32":"# unfreeze the LSTM layers of the model\ntweet_model.unfreeze()","5959345b":"#Now let's train the model\ntweet_model.fit_one_cycle(cyc_len=5, max_lr=slice(1e-1\/(2.6**4),1e-1), moms=(0.85, 0.75))","65371e4a":"# save the encoder \ntweet_model.save_encoder('encoder')","0473cd7b":"# create 'TextClasDataBunch'\n# pass in vocab to ensure the vocab is the\n# same one that was modified in the fine-tuned LM\ndata_class = TextClasDataBunch.from_csv(path='',csv_name='twitter_data_cleaned_train.csv',\n                              vocab=data_batch.train_ds.vocab,bs=32,text_cols='text',label_cols='airline_sentiment')\n\n# show what our batch looks like\ndata_class.show_batch()","5d1e05ff":"# create new learner object with the 'text_classifier_learner' object.\n# The concept behind this learner is the same as the 'language_model_learner'.\n# It can similarly take in callbacks that allow us to train with special optimization methods. We use a slightly bigger dropout this time\ntweet_model = text_classifier_learner(data_class, AWD_LSTM, drop_mult=0.5)\n\n# load the fine-tuned encoder onto the learner\ntweet_model.load_encoder('encoder')\n\n# look at the model\ntweet_model.model","15d557c3":"# find the optimal learning rate, just like we did before\ntweet_model.lr_find()\n\n# plot it\ntweet_model.recorder.plot()","8bf9e900":"# like we did before, we choose a learning rate before\n# the minimum of the graph and use the 1cycle policy\ntweet_model.fit_one_cycle(5,1e-1,moms=(0.8,0.7))","552fb81a":"# unfreeze next layer\ntweet_model.freeze_to(-2)\n\n# train with next layer unfrozen, apply discriminative fine-tuning\ntweet_model.fit_one_cycle(5,slice(1e-2\/(2.6**4),1e-2))","ded78ca1":"# repeat the process\ntweet_model.freeze_to(-3)\ntweet_model.fit_one_cycle(5,slice(1e-2\/(2.6**4),1e-2))","0a82c803":"# now unfreeze everything\ntweet_model.unfreeze()\ntweet_model.fit_one_cycle(5,slice(1e-2\/(2.6**4),1e-2))","1d7503d1":"# put test data in test df\ndf_test = pd.read_csv('twitter_data_cleaned_test.csv')\nprint(df_test[['text','airline_sentiment']])\ndf_test.head()","0e135284":"# add a column with the predictions on the test set\n\ndf_test['sentiment_pred'] = df_test['text'].apply(lambda row:str(tweet_model.predict(row)[0]))\n","40effe6a":"# print the accuracy against the test set\nprint(\"Accuracy: {}\".format(accuracy_score(df_test['airline_sentiment'],df_test[\n    'sentiment_pred'])))","7c12afb2":"import matplotlib.pyplot as plt\n# Taken from the scikit-learn documentation\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    #classes = classes[unique_labels(y_true, y_pred)]\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","7a45dd64":"# plot the confusion matrix for the test set\nplot_confusion_matrix(df_test['airline_sentiment'],df_test['sentiment_pred'],\n                      classes=['negative','neutral','positive'])\nplt.show()","e3c6c4be":"# **Preprocessing the Twitter data**\nThe decisions in which to preprocess were based on intuitions (for example, our assumption is that removing a hashtag would reduce noise and improve signal), as well as research paper results (removing numbers, replacing urls, and replacing user mentions were shown to improve accuracy on Twitter Sentiment Analysis).\nThis is based on the code from \"A Comparison of Pre-processing Techniques for Twitter Sentiment Analysis\" by Dimotrios Effrosynidis.","9c37e3f4":"Woohooo! That's awesome!","c7421e90":"# References\n- This Notebook is inspired primarily from the [github repository](https:\/\/github.com\/rajs96\/ULMFiT-Twitter-US-Airline-Sentiment\/blob\/master\/ulmfit_results.ipynb).\n- [\"Universal Language Model Fine-tuning for Text Classification\"](https:\/\/arxiv.org\/abs\/1801.06146) \n- [\"A Comparison of Pre-processing Techniques for Twitter Sentiment Analysis\"](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-319-67008-9_31)\n- [\"A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay\"](https:\/\/arxiv.org\/abs\/1803.09820)","19175350":"Now let's change the model according to our application i.e classification.","906f886d":"Let's see how our model predicts!","ce779a76":"From the graph, we shall decide to take 1e-1 since the learning rate as the loss becomes minimum after this.\n","3da16e58":"Initially we gonna set a language model ,train it and save the encodings i.e weights","ea2fa3b5":"# Prediction Phase","70ea97ca":"The result is quite convincing.","544d5739":"# Gradual Unfreezing\ni.e unfreezing and training layers one by one from the top i.e last layer to prevent forgetting of features.","6babd931":"Let us now calculate the accuracy which is a popular evaluation metric in order to compare with many other approaches.","59e774e9":"# Training Phase\nFirst we will have a language model and then add a classification model for enhanced results.","b901fa5f":"# Loading and Visualization of distribution of the Data"}}