{"cell_type":{"4324d71c":"code","d70b9304":"code","71da8986":"code","b37459fc":"code","4c8e0ca0":"code","1d534241":"code","009fbfd2":"code","f38e12cb":"code","40822486":"code","f36a92c2":"code","6edb4fe1":"code","5977a129":"code","2d998869":"code","8d783ea9":"code","46301a61":"code","3bc140cf":"code","6fa4e73f":"code","d0f46630":"code","11763054":"code","18a446e5":"code","99d3f6f0":"code","5a69c723":"code","32d4bcab":"code","a8e08a30":"code","81730abe":"markdown","cad0ff65":"markdown","33e6354d":"markdown","47d259eb":"markdown","bea3c207":"markdown","40624242":"markdown","9846c12d":"markdown","45b89796":"markdown","84a94095":"markdown","c4928f32":"markdown","0a36dcd8":"markdown","55ec92ff":"markdown","805b95f3":"markdown","c600b932":"markdown","ae8a9344":"markdown","37f68161":"markdown","fc35b91e":"markdown","ee652be4":"markdown","7729db55":"markdown"},"source":{"4324d71c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport warnings  \nwarnings.filterwarnings('ignore')","d70b9304":"data = pd.read_csv(\"..\/input\/crime.csv\", \n                   nrows=7000\n                  ,parse_dates=['LAST_OCCURRENCE_DATE', 'FIRST_OCCURRENCE_DATE'])\ndf=pd.DataFrame(data)","71da8986":"police_colors = ['#356CB5',  # Steel Blue\n                 '#1B51A1',  # Cyan Cobalt Blue\n                 '#1B51A1',\n                 '#0F3C8B',  # Dark Cerulean\n                 '#590000',\n                 '#980001',  # Crimson Red\n                 '#CC0000',  # Boston University Red \n                 '#E82734',  # Alizarin Crimson\n                 ]\nmy_cmap = ListedColormap(sns.color_palette(police_colors).as_hex())\n\nsns.set_palette(palette=police_colors)\nsns.set_style(\"darkgrid\")\nsns.palplot(police_colors)","b37459fc":"#plotting nulls\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap=my_cmap)","4c8e0ca0":"#Grouping offense types\nCountofReports=df.groupby(df['OFFENSE_CATEGORY_ID']).size()\nCountofLOD=df.groupby(df['OFFENSE_CATEGORY_ID'])['LAST_OCCURRENCE_DATE'].count()\n\n#Finding a difference\nDifference=CountofReports-CountofLOD\n\n#plotting\nfig,(ax1,ax2,ax3)= plt.subplots(figsize=(22,6),nrows=1,ncols=3)\nCountofReports.plot(kind='bar',ax=ax1,cmap=my_cmap, title=\"Count of Reports\")\nCountofLOD.plot(kind ='bar',ax=ax2,cmap=my_cmap,title=\"Count of LAST_OCCURRENCE_DATE\")\nDifference.plot(kind='bar',ax=ax3,cmap=my_cmap,title='Difference')","1d534241":"#Rename DateTime fields for better understanding of what's going on.\ndf.rename(columns={'FIRST_OCCURRENCE_DATE': 'BeginDate', 'LAST_OCCURRENCE_DATE': 'EndDate'}, inplace=True)\n\n#replacing null LOD dates with FOD when LOD is null\ndf.EndDate.fillna(df.BeginDate,inplace=True)\n\n#removing Lat,Long combos with missing data\ndf.dropna(subset=['GEO_X'],inplace=True)","009fbfd2":"#finding hour1\ndf['Hour1']= df.BeginDate.dt.hour\n\n#finding hour2 given comparative circumstances\ndf['Hour2']=np.where(\n    (df['BeginDate'].dt.hour == df['EndDate'].dt.hour),\n        df['EndDate'].dt.hour,\n            np.where((df['EndDate'].dt.hour == 0) & (df['EndDate'].dt.minute<10),\n                     23,\n                         np.where((df['EndDate'].dt.minute<10),\n                                  (df['EndDate'].dt.hour - 1),\n                                      df['EndDate'].dt.hour)))\n\n#creating an hours in range based on the two fields seen above\ndf['HoursInRange']= np.where((df['Hour1']>df['Hour2']), 25-(df['Hour1']-df['Hour2']),(df['Hour2']-df['Hour1']+1))\n\n#filtering out all crimes with an Hours in Range >24 and renaing the dataframe\ncrimes=df[df['HoursInRange']<=24]\n\n#creating the midpoint\ncrimes['SplitDate'] = crimes['BeginDate'] + (crimes[\"EndDate\"] - crimes['BeginDate'])\/2\ncrimes['SplitDate'] = crimes['SplitDate'].apply(lambda x: x.strftime('%m\/%d\/%Y %H:00:00'))\ncrimes['SplitDate'] = pd.to_datetime(crimes.SplitDate)","f38e12cb":"def Heatmap_daysXhours(data_frame, groupby_field, count_field, off):\n    heat_group=data_frame.groupby(data_frame[groupby_field], as_index=True, sort=True)[count_field].count()\n    heat_data=pd.DataFrame(heat_group)\n    heat_data['Hours']= heat_data.index.hour\n    heat_data['Days']= heat_data.index.day_name()\n    heat_data_pivot=pd.pivot_table(heat_data,values=count_field, index='Days',columns='Hours',fill_value=0)\n    \n    grid_kws = {\"height_ratios\": (.9, .05), \"hspace\": .3}\n    fig,(ax,cbar_ax)= plt.subplots(2,figsize=(15,7),gridspec_kw=grid_kws)\n    ax.set_title(off+' by Time of Day and Day of Week',fontsize=18)\n    cbar_ax.set_title('Count of Police Reports',fontsize=12)\n    ax = sns.heatmap(heat_data_pivot, square=True,\n                     cmap=my_cmap, annot=True,\n                     linewidths=.01, ax=ax,\n                     cbar_ax=cbar_ax, \n                     cbar_kws={\"orientation\": \"horizontal\"})\n    ax.set_ylabel(\"Days of the Week\", fontsize=15)\n    ax.set_xlabel(\"24 Hours of the Day\", fontsize=15)","40822486":"vehicleburglary=crimes[crimes['OFFENSE_CATEGORY_ID']=='theft-from-motor-vehicle']\nHeatmap_daysXhours(vehicleburglary,'SplitDate','INCIDENT_ID','Vehicle Burglaries')","f36a92c2":"accidents=crimes[crimes['OFFENSE_CATEGORY_ID']=='traffic-accident']\nHeatmap_daysXhours(accidents,'SplitDate','INCIDENT_ID','Accidents')","6edb4fe1":"#creating a weight function and applying the function to a new column\ndef Weight(Span): \n    try: \n        return round(1\/Span,3) \n    except ZeroDivisionError: return 1\n\n#applying the weights to a new column    \ncrimes['Weight']=crimes['HoursInRange'].apply(Weight)\n\n\n#Creating 24 columns, which are hours of the day, and appling the algo's results for each case to each applicable hour\n\n#Previously I was doing this for 24 hours in a day\n#df['01:00'] = np.where((df['Hour2'] >= df['Hour1']),\n                    #np.where((1>=df['Hour1']) & (1<=df['Hour2']),df['Weight'],0),\n                        #np.where( (1<=df['Hour2']) | (1>=df['Hour1']),df['Weight'],0))\n\n#now I'm doing this instead of what's above here.\nx=np.arange(0,24,1)\nfor i in x:\n    crimes[i]=np.where((crimes['Hour2'] >= crimes['Hour1']),\n                       np.where((i>=crimes['Hour1']) & (i<=crimes['Hour2']),crimes['Weight'],0),\n                           np.where( (i<=crimes['Hour2']) | (i>=crimes['Hour1']),crimes['Weight'],0))\n\ncrimes.head()","5977a129":"def Aoristic_Chart(data,chart_offense):\n    ax = data.iloc[20:44].plot(\n        xticks=np.arange(0,24,1),\n        grid=True,\n        figsize=(15,7),\n        fontsize=16)\n    ax.set_xlim([0,23])\n    ax.set_ylabel(\"Aoristic Values\", fontsize=15)\n    ax.set_xlabel(\"24 Hours of the Day\",fontsize=15)\n    ax.set_title('Weighted Hours for '+chart_offense,fontsize=18)","2d998869":"#Grouping vehicle burglaries together\nVehicleBurglary=crimes[crimes['OFFENSE_CATEGORY_ID']=='theft-from-motor-vehicle']\nVehicleBurglary_line=VehicleBurglary.sum()\n\n#applying the chart\nAoristic_Chart(VehicleBurglary_line, 'Vehicle Burglaries')","8d783ea9":"#Group accidents together\naccidents=crimes[crimes['OFFENSE_CATEGORY_ID']=='traffic-accident']\naccidents_line=accidents.sum()\n\n#applying the chart\nAoristic_Chart(accidents_line,\"Traffic Accidents\")","46301a61":"#Getting my Vehicle Burglary data\nVehicleBurglary_Clusters=VehicleBurglary[['GEO_LAT','GEO_LON']]","3bc140cf":"def Elbow_Method(k_clusters,data):\n    errors = []\n    K = range(1,k_clusters)\n    for k in K:\n        kmeanModel = KMeans(n_clusters=k).fit(data)\n        kmeanModel.fit(data)\n        errors.append(sum(np.min(cdist(data, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) \/ data.shape[0])\n        \n    plt.figure(figsize=(10,6))\n    plt.plot(K,errors)\n    plt.title('Error Rate vs. K Value')\n    plt.xlabel('K')\n    plt.ylabel('Error Rate')","6fa4e73f":"Elbow_Method(40,VehicleBurglary_Clusters)","d0f46630":"kmeans = KMeans(n_clusters=8)\nkmeans.fit(VehicleBurglary_Clusters)\nkmeans.cluster_centers_\nVehicleBurglary['Clusters']=kmeans.labels_\n\nsns.lmplot(x='GEO_LAT',y='GEO_LON',data=VehicleBurglary,\n           hue='Clusters',\n           fit_reg=False,\n           size=10)","11763054":"def Aoristic_And_Clusters(data,cluster_number):\n    \n    cluster=data[data['Clusters']==cluster_number]\n    cluster_line=cluster.sum()\n    \n    \n    fig,(ax1,ax2)=plt.subplots(nrows=1,ncols=2,figsize=(22,6))\n    \n    sns.regplot(x='GEO_LAT',y='GEO_LON',data=cluster,\n           fit_reg=False,color='red',ax=ax1)\n    \n    ax2=cluster_line.iloc[21:45].plot(xticks=np.arange(0,24,1))","18a446e5":"Aoristic_And_Clusters(VehicleBurglary,0)","99d3f6f0":"Aoristic_And_Clusters(VehicleBurglary,1)","5a69c723":"Aoristic_And_Clusters(VehicleBurglary,2)","32d4bcab":"Aoristic_And_Clusters(VehicleBurglary,3)","a8e08a30":"Aoristic_And_Clusters(VehicleBurglary,4)","81730abe":"Grouping the data by Offense Category will segment by offense type. We'll compare the categories against one another and assess the nature of the missing LOD data. Below I will create 3 charts for comparison.\n\n1. The first chart on the left plots how many offense reports were written per offense category. \n1. The middle chart plots a count of LODs per offense category. \n1. The chart on the right plots the difference between the the first and middle charts. \n\nThe larger the number in the Difference chart, the larger the number of missing LODs. Traffic accidents are the highest category in the Difference chart, which makes sense because people generally know when the accident occurred. The second highest is All Other Reported, which is a catch-all category for crimes that are not easily classified. The results are typical for a crime dataset.","cad0ff65":"## Charting Midpoints <a name=\"ChartingMidpoints\"><\/a>\n\nMidpoints give us an one datetime field for us to use in heatmapping. Heatmaps open up the various attributes of time by letting us separate days, day number of year, month, and many others. The first heatmap will plot vehicle burglaries by day and hour using the Midpoint, whereas the second chart will only plot vehilce accidents by day and hour using the Midpoint. \n\nVehicle burglaries occur during the night in residential areas because that's when unattending vehilcles can be ransacked for valuable goods, but the busiest time of day for accidents is when the greater oppportunity for accidents appears, rush hour.","33e6354d":"# Temporal Analysis and Visualizations <a name=\"TemporalAnalysisAndVisualization\"><\/a>","47d259eb":"## Aoristic Method <a name=\"AoristicMethod\"><\/a>\n\nIn the previous sections we discussed midpoint as a viable analysis, but midpoint has weaknesses with lengthy crime time spans. And yet we are still compelled to do something about crime problems with unknown conditions.\n\nAnother time span analysis is called Aoristic. Aoristic analysis weights each hour in the time span by dividing 1 crime by the Hours in Span (1\/Hours in Span). If the crime occurred exactly at 2:30pm, then the hours in span is 1, therefore 1 crime divided by 1 hour in the span is 1. This means the 2 o'clock hour carries the full weight of the crimes. Crimes spanning two hours have a weight of .5 and so on. The Aoristic process then assigns the weighted value to each hour in the span. When the weights are aggregated we get a picture of which hours in the time span are weighted more heavier than others, meaning the crime is more likely to occur in the heavier hours.\n\nThe visualization helps to demonstrate how weights are accumlated on successive cases to produce higher weights for riskier times. The image was taken from Dr. Jerry Ratcliffe's work and cited below. Think of row (a) as being an offense with a four hour time span. Each span wieght is distibuted across hours \"Search 1' through 'Search 4'. The image's foot contains a summation of all the weights and is used to assess which hours are the most risky. In the image, 'Search 3' aggregated the most weights making the hour the most risky.\n\n![image](https:\/\/www.researchgate.net\/profile\/Greg_Jenion\/publication\/237546655\/figure\/fig1\/AS:669026162339842@1536519675153\/Aoristic-Analysis.png)\n\nThe following formulas were created from a spreadsheet distributed by the International Association of Crime Analysts and was made by its former President Christopher Bruce. I am not able to find a linked source to the material for reference. I'd like to emphatically state that this work is not my own except that I translated an excel document and its formulas to python and pandas. I'd like to direct you to the IACA's webpage for more details or contact information. https:\/\/iaca.net\/\n\nRatcliffe, J.H. (March,2002), Aoristic Signatures and the Spatio-Temporal Analysis of High Volume Crime Pattrens. Journal of \n    Quantitative Criminology Volume 18, No. 1.Retrieved from \n    https:\/\/docs.wixstatic.com\/ugd\/f5df24_35424f29c99b4f76b22c136dc09377a8.pdf on 06\/02\/2019.","bea3c207":"## Exploring the Data <a name=\"ExploringtheData\"><\/a>\n\nCrime time spans have been developed to create a temporal window encapsulating when the **crime could** have occurred. Data for time spans is generally collected by the officer upon taking the inital report. For crimes such as vehicle burglaries, officers solicit the last known time the property was intact, and then the time the break-in was discovery. Victims could report their car was secured on Date1 at 9:00pm and found burglarized on Date2 at 9:00am. In this example, at a minimum, there could be 12 hours time span in which the crime would have occurred.\n\nIn many instances however, crime time spans are not important. Events like vehicle accidents do not have time spans because because most drivers are able to give an accurate description for when the accident occurred. Police officers investigating accidents will generally complete a report and ensure that Date1\/Time1 matches Date2\/Time2.\n\nFor the remainder of the report I'll be comparing vehicle burglaries against accidents to demonstrate the differences between times and places for those two offense types. Before that begins, please take a moment to develop your own hypothesiss for what times and places vehicle burglaries and accidents will most frequent.\n\nMost modern police databases will have strict quality control measures to ensure missing data is corrected. Police data usually most conform to strict Uniform Crime Reporting or National Incident Based Systems standards. Plotting null data shows most of the missing entries occurred in the Last_Occurrence_Date. In the coming code blocks I'll discuss why missing data can easily be replaced.","40624242":"## Creating and Charting Clusters <a name=\"CreatingAndChartingClusters\"><\/a>","9846c12d":"## Custom Colors<a name=\"CustomColors\"><\/a>\nThe obvious choice for a coloring scheme is shades of red and blue.","45b89796":"# Table of Contents\n1. [Introduction](#introduction)\n2. [Data Import and Exploration](#DataImport)<br>\n    2.1 [Custom Colors](#CustomColors)<br>\n    2.2 [Exploring The Data](#ExploringtheData)<br>\n    2.3 [Fixing The Null](#FixingtheNull)<br>\n3. [Temporal Analysis Methods and Visuals](#TemporalAnalysisAndVisualization)<br>\n    3.1 [Midpoint Analysis](#MidpointAnalysis)<br>\n    3.2 [Charting Midpoints](#ChartingAoristicMethod)<br>\n    3.3 [Aoristic Method](#AoristicMethod)<br>\n    3.4 [Charting Aoristic](#ChartingAoristic)<br>\n4. [Spatial Clusters of Vehicle Burglaries](#VehicleBurglaryClusters)<br>\n    4.1 [Elbow Method](#ElbowMethod)<br>\n    4.2 [Creating Clusters and Charting](#CreatingAndChartingClusters)<br>\n    4.3 [Time and Place Together](#TimeAndPlace)<br>\n5. [Conclusion](#Conclusion)","84a94095":"## Conclusion <a name=\"Conclusion\"><\/a>\n\nThank you for choosing to read over my work. I greatly appreciate your feed back. I am very thankful for this opportunity to learn, explore, and have fun.","c4928f32":"## Fixing the Null <a name=\"FixingtheNull\"><\/a>\n\nAfter reviewing the data it seems safe to assume that many of the missing LOD values could be replaced with the FOD values where LOD is null. Replace null LOD with FOD will be important in the coming steps\n\nThe remaing null values of consequence are located in GeoX, and GeoY. Incident location is missing too, but that becomes less relevant when X's,Y's are present because geocoding the addresses is not necessary.","0a36dcd8":"# Data Import and Exploration <a name=\"DataImport\"><\/a>","55ec92ff":"# Spatial Clusters of Vehicle Burglaries <a name=\"#VehicleBurglaryClusters\"><\/a>\n\nTobler's first law of geography states \"everything is related to everything else, but near things are more related than distant things.\" Nearer crimes are more related that distant crimes, therefore clustering offenses by location becomes helpful. The more centroids we apply to the data pionts, the more improved the error rate becomes. If we apply as many centroids as there are data points, then the error rate will completely minimize, but we then create as many clusters as there are data points which is not the purpose of grouping like ojbects.\n\nI'll be using the elbow method to assess the appropriate number of centroids for this dataset. The elbow method plots the error rate as a function of the number of clusters. The elbow chart visuallizes the function and is useful for picking a point on the line where the addition of a new cluster has little effect on the error rate. In the future, I'd like to find a method for applying the second derivative to the line as a secondary visual aide.","805b95f3":"Big differences appear between these two types of occurrences. Vehicle burglaries happend during the late and early hours of the day, where as vheilce accidents happen during rush hours.","c600b932":"Optimal number of centroids appears to be about 8 to my eyes. Nine to 11 seem viable too.","ae8a9344":"## Charting Aoristic <a name=\"ChartingAoristic\"><\/a>","37f68161":"# Introduction <a name=\"introduction\"><\/a>\nThe public and law enforcement expect police resources to be proactively deployed to prevent crime, but risk is not evenly spread across time or space. Knowing where to deploy and when to deploy are equally important. Unfortunately, many crimes go unobserved and therefore lack important temporal data. This obviously makes knowing when to deploy more difficult. Once the when question is is answered, the next question is where should police deploy. Spatial deployments should be crime type specific. Spatial crime patterns relate to human behavior at specific locations. For instance, when would vehicle burglaries would more likely occur at a restaurant during lunch and dinner rather than at 4:00am because there are more opportunities for vehicle burglaires when there are a greater number of unattended cars in the parking lot. In a counter example, would a vacant field have more domestic disturbances than an apartment complex? Again, the answer relates to people and their beahvior at certain places and at certain times. \n\nThe intent of this work is to demonstrate several temporal analysis techniques for crimes and to show how those matter when discussing spatial patterns. The bulk of this work is dereived from well-founded Crime Analysis literature and researchers. As the discussion dives deeper into the analysis I will cite the sources for your further reading and to properly give acknowlegement to the hard work of others.","fc35b91e":"## Elbow Method <a name=\"ElbowMethod\"><\/a>","ee652be4":"## Time and Place Together <a name=\"TimeAndPlace\"><\/a>\n\nSo far this work has evaluted two temporal methods for evaluating crime and one method for spatial analysis. The original question police practioners require is a combination of both. Where and when do we deploy? In the following section I'll break down each cluster and evalute the spatial and temporal elements.","7729db55":"## Midpoint Analysis <a name=\"MidpointAnalysis\"><\/a>\n\nA common method to assess time spans is to create a Midpoint between the FOD and the LOD. A midpoint works well for smaller times spans but begins to degrade in reliability as the span increases in length. The midpoint is found in the following formula FOD+(LOD-FOD)\/2.\n\nAs a general rules, crimes with time spans greater than 24 hours in length are removed from analysis. We can assess times spans >24 hours by comparing the hours from the FOD and LOD. Besides spans >24 hours, other special consideration must be made when assessing the hours within the span, hereafter called Hours in Range. Let's say a crime occurrs at 1:05pm. Did the crime occur in the 1 o'clock hour? Or in other words, should full weight of the crime be given to the 1 o'clock hour. Not likely given the crime could only have had minutes to occur. We probably shouldn't count 1 o'clock as being apart of the time span. But what if the crime occurs at 1:10pm? Did the crime occur in the 1 o'clock hour? At this point, we might say it could have and therefore full weight should be given to the hour. We'll also create a Hour1 and a Hour2 field that makes these assessments as well as appropriately counting hours that cross midnight.\n\nThe following formulas were created from a spreadsheet distributed by the International Association of Crime Analysts and was made by its former President Christopher Bruce. I am not able to find a linked source to the material for reference. I'd like to emphatically state that this work is not my own except that I translated an excel document and its formulas to python and pandas. I'd like to direct you to the IACA's webpage for more details or contact information. https:\/\/iaca.net\/\n\nThe use of the Midpoint date, or sometimes I refer to it as Split Date, is necessary for some charting."}}