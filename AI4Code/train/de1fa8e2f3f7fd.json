{"cell_type":{"541d6b58":"code","4d00bdbb":"code","ce230f75":"code","701c3cd7":"code","36b35282":"code","7a151c83":"code","ecfa995e":"code","d4c88b1e":"code","f264fd8f":"code","26848d88":"code","bf3a962f":"code","cbb3f5c4":"code","b6b88850":"code","9d91903f":"code","e241cefa":"code","2d25405a":"code","7f810889":"code","13e3e855":"code","0718d119":"code","5602566e":"code","8ca6ec4b":"code","1ad3faed":"code","573bb680":"code","255d7859":"code","efa0eb08":"code","a69522a0":"code","815f1cb1":"code","f5d9b04c":"code","e996c064":"code","22202e48":"code","52bf4699":"code","e39f7b7f":"code","bcff9526":"code","a7cf8fd1":"code","caf6d2b1":"code","a4dfc472":"code","6c134c18":"code","1b6d49ba":"code","effce906":"code","f0bf38c9":"code","af4f3f28":"code","cccc2a79":"code","7ceeab75":"code","ad25f5ca":"code","7b71cc1e":"code","8ed333f4":"code","ac4ba32d":"code","abe8a6fd":"code","3e5227d2":"code","ccb25c03":"code","3a70207c":"code","2b68cb99":"code","1afb4b34":"code","81b45786":"code","46550148":"code","072fce4f":"code","a446dc27":"code","bb3be007":"code","ef99c1eb":"code","af933f7e":"code","ae83b59d":"markdown","595cbc73":"markdown","0bb966bc":"markdown","9d95e29e":"markdown","8d74b6ac":"markdown","7c533782":"markdown","2c6c0eed":"markdown","6c1782d9":"markdown","35b2fee6":"markdown","fc7ad814":"markdown","d477fe05":"markdown","10e3e858":"markdown","d5691551":"markdown","45899b17":"markdown","7de009e0":"markdown","379fbeb2":"markdown","d24117ad":"markdown"},"source":{"541d6b58":"import pandas as pd\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nids = test['id']\ntarget = train['price'].values\ndata = pd.concat([train.drop(['price'], axis=1), test])\nprint(train.shape, test.shape, data.shape, target.shape)","4d00bdbb":"train.head()","ce230f75":"import matplotlib.pyplot as plt\n\ntrain['price_q'] = pd.qcut(train['price'], q=10, labels=list(range(10))).astype(int)\ntrain['yr_built_q'] = pd.qcut(train['yr_built'], q=10, labels=list(range(10))).astype(int)\ntrain['sqft_living_q'] = pd.qcut(train['sqft_living'], q=10, labels=list(range(10))).astype(int)","701c3cd7":"def draw_scatter(df, col_name, axes):\n    df.plot(kind='scatter', x='long', y='lat', c=col_name, \n            cmap=plt.get_cmap('plasma'), colorbar=False, alpha=0.1, ax=axes)\n    axes.set(xlabel='longitude', ylabel='latitude')\n    axes.set_title(col_name, fontsize=13)\n    return axes\n       \nfig, axes = plt.subplots(2, 2, sharey=True, sharex=True, figsize=(10, 9), dpi=100)\n\ndraw_scatter(train, 'price_q', axes.flat[0])\ndraw_scatter(train, 'yr_built_q', axes.flat[1])\ndraw_scatter(train, 'sqft_living_q', axes.flat[2])\nfig.suptitle('Seattle Housing Prices', fontsize=15)\nfig.delaxes(axes.flat[3])\nfig.tight_layout()\nfig.subplots_adjust(top=0.9)\nplt.show()","36b35282":"from bokeh.models.mappers import ColorMapper, LinearColorMapper\nfrom bokeh.palettes import Plasma10\nfrom bokeh.plotting import gmap\nfrom bokeh.models import GMapOptions, HoverTool, ColumnDataSource\nfrom bokeh.io import output_notebook, show\n\noutput_notebook()\n\napi_key = 'AIzaSyBYrbp34OohAHsX1cub8ZeHlMEFajv15fY'\n\nmap_options = GMapOptions(lat=47.5112, lng=-122.257, map_type='roadmap', zoom=10)\np = gmap(api_key, map_options, title='Seattle Housing Prices')\n\nsource = ColumnDataSource(\n    data=dict(\n        lat=train['lat'].tolist(), \n        long=train['long'].tolist(),\n        color=train['price_q'].tolist(),\n        price=train['price'].tolist()\n    )\n)\n\ncolor_mapper = LinearColorMapper(palette=Plasma10)\np.circle(x='long', y='lat', \n         fill_color={'field': 'color', 'transform': color_mapper}, \n         fill_alpha=0.3, line_color=None, source=source)\n\nhover = HoverTool(\n    tooltips=[\n        ('lat', '@lat'), \n        ('long', '@long'), \n        ('price', '@price')\n    ]\n)\n\np.add_tools(hover)\nshow(p)","7a151c83":"print('median: {}'.format(train['price'].median()))\nprint('mean: {:.3f}'.format(train['price'].mean()))\nprint('min: {}'.format(train['price'].min()))\nprint('max: {}'.format(train['price'].max()))\nprint('std: {:.3f}'.format(train['price'].std()))\nprint('skewness: {:.3f}'.format(train['price'].skew()))\nprint('kurtosis: {:.3f}'.format(train['price'].kurt()))","ecfa995e":"import numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nplt.style.use('classic')\n\ndef draw_plot(log_transform=False, axes=None):\n    if log_transform:\n        prices = np.log(train['price']+1)\n    else:\n        prices = train['price']\n        \n    mu = prices.mean()\n    sigma = prices.std()\n    \n    y = norm.pdf(prices)\n    n, bins, patches = axes.flat[0].hist(prices,\n                                         bins=60, \n                                         density=True, \n                                         cumulative=False, \n                                         color='blue',\n                                         edgecolor='black', \n                                         linewidth=0.3) \n\n    y = ((1 \/ (np.sqrt(2 * np.pi) * sigma)) * np.exp(-0.5 * (1 \/ sigma * (bins - mu))**2))\n    axes.flat[0].plot(bins, y, '--', color='red')\n    axes.flat[0].tick_params(axis='y', left=False, labelleft=False)\n    axes.flat[0].tick_params(axis='x', rotation=0)\n    axes.flat[0].set_title('Histogram')\n\n    stats.probplot(prices, dist='norm', plot=axes.flat[1])\n    axes.flat[1].get_lines()[0].set_marker('o')\n    axes.flat[1].get_lines()[0].set_markeredgecolor('black')\n    axes.flat[1].get_lines()[0].set_markeredgewidth(0.2)\n    axes.flat[1].get_lines()[0].set_markerfacecolor('blue')\n    axes.flat[1].get_lines()[1].set_linestyle('--')\n    axes.flat[1].get_lines()[1].set_color('red')\n    axes.flat[1].tick_params(axis='y', left=True, labelleft=False)\n    axes.flat[1].get_yaxis().set_visible(False)\n    axes.flat[1].set_title('Probability Plot')\n    return axes\n\nfig, axes = plt.subplots(2, 2, figsize=(7,6), dpi=100)\ndraw_plot(axes=axes.flat[:2])\ndraw_plot(log_transform=True, axes=axes.flat[2:])\naxes.flat[0].tick_params(axis='x', rotation=90)\nfig.tight_layout()\nplt.show()","d4c88b1e":"import seaborn as sns\n\nfig, axes = plt.subplots(4, 2, sharey=True, figsize=(10, 15), dpi=100)\n\ncolumns = [\n    'bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade'\n]\n\nfor i, col in enumerate(columns):\n    train.boxplot(column='price', by=col, ax=axes.flat[i])\n    axes.flat[i].set_xlabel('')\n    axes.flat[i].set_title(col, fontsize=14)\n    axes.flat[i].grid(axis='x')\n    \naxes.flat[1].tick_params(axis='x', rotation=90)\nfig.suptitle('')\nfig.delaxes(axes.flat[7])\nfig.tight_layout()\nplt.show()","f264fd8f":"fig, axes = plt.subplots(4, 2, figsize=(10, 15), dpi=100)\n\ncolumns = [\n    'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', \n    'lat', 'long', 'sqft_living15', 'sqft_lot15'\n]\n\nfor i, col in enumerate(columns):\n    sns.distplot(train[col].values, hist_kws={'alpha': 1}, ax=axes.flat[i])\n    axes.flat[i].set_xlabel('')\n    axes.flat[i].set_title(col, fontsize=14)\n    axes.flat[i].grid(axis='y')\n    axes.flat[i].tick_params(axis='x', rotation=90)\n    \nfig.suptitle('')\n# fig.delaxes(axes.flat[7])\nfig.tight_layout()\nplt.show()","26848d88":"pearson = pd.DataFrame()\n\ncolumns = [\n    'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', \n    'lat', 'long', 'sqft_living15', 'sqft_lot15'\n]\n\npearson['features'] = columns\npearson['coefficient'] = [train[col].corr(train['price']) for col in columns]\npearson = pearson.sort_values('coefficient')\npearson.set_index('features', inplace=True)\n\nfig, ax = plt.subplots(figsize=(6, 10), dpi=100)\npearson.plot(kind='barh', ax=ax)\nplt.show()","bf3a962f":"input_data = train.copy()\n\ncolumns = [\n    'bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade'\n]\n\nfor col in columns:\n    input_data[col] = input_data[col].astype('category')\n    \nanova = pd.DataFrame()\nanova['features'] = columns\np_values = []\n\nfor col in columns:\n    samples = []\n    for uniq in input_data[col].unique():\n        samples.append(input_data[input_data[col] == uniq]['price'].values)\n    p = stats.f_oneway(*samples)[1]\n    p_values.append(p)\n\nanova['p_values'] = p_values\n\nanova['disparity'] = np.log(1. \/ anova['p_values'].values+1)\n\nanova.sort_values('disparity', inplace=True, ascending=False)\nanova['disparity'] = anova['disparity'].replace({np.inf: 800})\n\nfig = plt.figure(figsize=(5,10))\nsns.barplot(data=anova, y='features', x='disparity', color='blue')\nplt.xlim([0, 700])\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()","cbb3f5c4":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\ncolumns = ['sqft_living', 'bathrooms', 'bedrooms', 'lat', 'long', 'price']\n\nX = train[columns].drop(['price'], axis=1)\nX['random'] = np.random.random(size=len(X))\ny = train['price']\nprint(X.shape, y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","b6b88850":"rf = RandomForestRegressor(n_estimators=100, oob_score=True)\nrf.fit(X_train, y_train)\ndefault_importances = rf.feature_importances_","9d91903f":"from mlxtend.evaluate import feature_importance_permutation\n\npermutation_importances, _ = feature_importance_permutation(\n    predict_method=rf.predict, \n    X=X_test.values,\n    y=y_test.values,\n    metric='r2',\n    num_rounds=10, seed=42)","e241cefa":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\neli5_permutation_importances = PermutationImportance(rf, random_state=42).fit(X_test, y_test)\neli5.show_weights(eli5_permutation_importances, feature_names=X_test.columns.tolist())","2d25405a":"from sklearn.base import clone\n\ndef dropcolumn_importances(estimator, X_train, y_train):\n    estimator_ = clone(estimator)\n    estimator_.random_state = 42\n    estimator_.fit(X_train, y_train)\n    baseline = estimator_.oob_score_\n    importances = []\n    for col in X_train.columns:\n        X = X_train.drop(col, axis=1)\n        estimator_ = clone(estimator)\n        estimator_.random_state = 42\n        estimator_.fit(X, y_train)\n        o = estimator_.oob_score_\n        importances.append(baseline - o)\n    importances = np.array(importances)\n    return importances","7f810889":"dropcolumn_importances = dropcolumn_importances(rf, X_train, y_train)","13e3e855":"def draw_importances(importances, title, ax):\n    indices = np.argsort(importances)\n    temp = pd.DataFrame({'importances': importances},\n                        index=X_train.columns).reset_index()\n    temp.sort_values(by='importances', inplace=True)\n    temp['colors'] = temp['index'].map(lambda x: False if x == 'random' else True)\n    temp['importances'].plot(kind='barh', color=temp['colors'].map({True: 'b', False: 'r'}), ax=ax)\n    ax.set_title(title)\n    ax.set_yticklabels([X_train.columns[i] for i in indices])","0718d119":"fig, axes = plt.subplots(2, 2, figsize=(10, 8), dpi=100)\ndraw_importances(default_importances, 'Default', axes.flat[0])\ndraw_importances(permutation_importances, 'Permutation', axes.flat[1])\ndraw_importances(dropcolumn_importances, 'Drop-column', axes.flat[2])\nfig.delaxes(axes.flat[3])\nfig.tight_layout()\nplt.show()","5602566e":"X = train.drop(['id', 'date', 'price', 'price_q', 'yr_built_q', 'sqft_living_q'], axis=1)\ny = np.log(train['price']+1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nrf = RandomForestRegressor(n_estimators=100, oob_score=True, n_jobs=-1)\nrf.fit(X_train, y_train)\nprint(rf.score(X_train, y_train))\nprint(rf.score(X_test, y_test))\n\npermutation_importances, _ = feature_importance_permutation(\n    predict_method=rf.predict, \n    X=X_test.values,\n    y=y_test.values,\n    metric='r2',\n    num_rounds=10, \n    seed=42)","8ca6ec4b":"fig, ax = plt.subplots(figsize=(5, 8), dpi=100)\ndraw_importances(permutation_importances, 'Permutation Importances', ax)","1ad3faed":"import statsmodels.api as sm\n\ndef draw_outliers(col_name=None, ax=None):\n    ols_model = sm.OLS(train['price'], train[col_name]).fit()\n    residuals = ols_model.resid\n    outlier = train.iloc[np.argmax(residuals.values**2)]    \n    sns.regplot(x=col, y='price', data=train, ax=ax, scatter_kws={'color': 'blue', 'edgecolors': 'black'}, line_kws={'color': 'red'})\n    ax.scatter(outlier.loc[col], outlier.loc['price'], c='red')\n\nfig, axes = plt.subplots(3, 2, figsize=(10, 15), dpi=100)\n\ncolumns = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\nfor i, col in enumerate(columns):\n    draw_outliers(col, ax=axes.flat[i])\n    axes.flat[i].set_xlabel('')\n    axes.flat[i].set_title(col, fontsize=14)\n    axes.flat[i].grid(axis='y')\n    axes.flat[i].tick_params(axis='x', rotation=90)\n    \nfig.suptitle('')\n# fig.delaxes(axes.flat[7])\nfig.tight_layout()\nplt.show()","573bb680":"# outliers = [5108, 6469]\n# train.drop(outliers, axis=0, inplace=True)\n# target = np.delete(target, outliers)\n# print(train.shape, target.shape)","255d7859":"# z = np.abs(stats.zscore(train['sqft_living']))\n# print(np.where(z > 3))","efa0eb08":"train['price_per_sqrt'] = train['price'].div(train['sqft_living'])\nprice_by_sqrt = train.groupby(by='zipcode')['price_per_sqrt'].agg([('mean_by_sqrt', 'mean'), ('median_by_sqrt', 'median')])\nprice_by_zipcode = train.groupby(by='zipcode')['price'].agg([('mean_by_zipcode', 'mean'), ('median_by_zipcode', 'median')])\nprice_by_grade = train.groupby(by='grade')['price'].agg([('mean_by_grade', 'mean'), ('median_by_grade', 'median')])","a69522a0":"data = pd.merge(data, price_by_sqrt, left_on='zipcode', right_on='zipcode', )\nprint(data.shape)\n\ndata = pd.merge(data, price_by_zipcode, left_on='zipcode', right_on='zipcode')\nprint(data.shape)\n\ndata = pd.merge(data, price_by_grade, left_on='grade', right_on='grade')\nprint(data.shape)","815f1cb1":"data['datetime'] = pd.to_datetime(data['date'].str[:8])\ndata['date_year'] = data['datetime'].dt.year","f5d9b04c":"data['renovated'] = data['yr_renovated'].apply(lambda x: 1 if x > 0 else 0)","e996c064":"data.sort_values(by='id', inplace=True)\ndata.reset_index(drop=True, inplace=True)\ndata.head()","22202e48":"for col in ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_lot15']:\n    data[col] = np.log(data[col]+1)","52bf4699":"train = data[:train.shape[0]]\ntarget_log = np.log(target+1)\n\nX = train.drop(['id', 'date', 'datetime'], axis=1)\ny = target_log\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nrf = RandomForestRegressor(n_estimators=100, oob_score=True, n_jobs=-1)\nrf.fit(X_train, y_train)\nprint(rf.score(X_train, y_train))\nprint(rf.score(X_test, y_test))\n\npermutation_importances, _ = feature_importance_permutation(\n    predict_method=rf.predict, \n    X=X_test.values,\n    y=y_test,\n    metric='r2',\n    num_rounds=10, \n    seed=42)","e39f7b7f":"fig, ax = plt.subplots(figsize=(5, 8), dpi=100)\ndraw_importances(permutation_importances, 'Permutation Importances', ax)\nplt.show()","bcff9526":"import re\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold\n\n\nclass AverageModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n    \n    def fit(self, X, y):\n        self.models_ = [clone(x[1]) for x in self.models]\n        \n        print('Building average models...')\n        for i, model in enumerate(self.models_):\n            model.fit(X, y)\n            print('Finished {} model'.format(i+1))\n        return self\n    \n    def predict(self, X):\n        predictions = np.column_stack([model.predict(X) for model in self.models_])\n        return np.mean(predictions, axis=1)\n    \nclass StackedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n        \n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model[1])\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n        \n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        print('Building stacked models...')\n        \n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model[1])\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n            print('Finished {} model'.format(i+1))\n            \n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n    \n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","a7cf8fd1":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, features):\n        self.features = features\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X[self.features].values","caf6d2b1":"from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, RobustScaler\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n\n\nonehot_features = ['renovated', 'waterfront', 'view']\nonehot_pipeline = Pipeline([\n    ('selecter', DataFrameSelector(onehot_features)),\n    ('cat_encoder', OneHotEncoder(categories='auto'))\n])\n\nordinal_features = ['grade', 'condition']\nordinal_pipeline = Pipeline([\n    ('selecter', DataFrameSelector(ordinal_features)),\n    ('cat_encoder', OrdinalEncoder(categories='auto'))\n])\n\nnumeric_features = [\n    'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n    'floors', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated',\n    'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15', 'mean_by_sqrt', \n    'median_by_sqrt', 'mean_by_zipcode', 'median_by_zipcode', 'mean_by_grade',\n    'median_by_grade', 'date_year',\n]\nnumeric_pipeline = Pipeline([\n    ('selecter', DataFrameSelector(numeric_features)),\n    ('cat_encoder', RobustScaler())\n])\n\nfull_pipeline = FeatureUnion(transformer_list=[\n    ('onehot_pipeline', onehot_pipeline), \n    ('ordinal_pipeline', ordinal_pipeline),\n    ('numeric_pipeline', numeric_pipeline),\n])","a4dfc472":"from sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\n\nparams = {\n    'alpha': 0.001, \n    'normalize':True,\n    'max_iter': 1e7,\n    'solver': 'sag',\n}\n\nreg1 = make_pipeline(RobustScaler(), Ridge(**params))\n\nparams = {\n    'kernel': 'rbf', \n    'degree': 3,\n    'C': 20,\n    'gamma': 'scale',\n    'max_iter': -1,\n    'epsilon': 0.005,\n    'cache_size': 200,\n    'shrinking': True,\n    'tol': 0.001,\n}\n\nreg2 = make_pipeline(RobustScaler(), SVR(**params))\n\nparams = {\n    'base_estimator': DecisionTreeRegressor(max_depth=6),\n    'n_estimators': 200,\n    'learning_rate': 0.1,\n}\n\nreg3 = AdaBoostRegressor(**params)\n\nparams = {\n    'max_depth': 9, \n    'n_estimators': 1000,\n    'subsample': 0.8,\n    'learning_rate': 0.1,\n}\n\nreg4 = GradientBoostingRegressor(**params)\n\nparams = {\n    'max_depth': 8, \n    'n_estimators': 2000,\n    'max_features': 'sqrt', \n    'n_jobs': -1, \n}\n\nreg5 = RandomForestRegressor(**params)\n\nparams = {\n    'objective': 'reg:linear',\n    'max_depth': 7, \n    'n_estimators': 5000,\n    'reg_alpha': 0.005,\n    'reg_lambda': 1,\n    'subsample': 0.7,\n    'colsample_bytree': 0.8,\n    'learning_rate': 0.1,\n    'booster ': 'gbtree',\n    'n_jobs': -1, \n    'silent': True,\n}\n\nreg6 = xgb.XGBRegressor(**params)\n\nparams = {\n    'objective':'regression',\n    'max_depth': -1,\n    'n_estimators': 5000,\n    'num_leaves': 31,\n    'subsample': 0.8,\n    'learning_rate': 0.1,\n    'min_child_samples': 20,\n    'boosting_type': 'gbdt',\n    'subsample_freq ': 0.8,\n    'reg_alpha': 0.1,\n    'n_jobs': -1, \n}\n\nreg7 = lgb.LGBMRegressor(**params)","6c134c18":"train = data[:train.shape[0]]\ntest = data[train.shape[0]:]\ntarget_log = np.log(target+1)\n\ntrain = train.drop(['id', 'date', 'datetime'], axis=1)\ntest = test.drop(['id', 'date', 'datetime'], axis=1)\nprint(train.shape, test.shape, target.shape)","1b6d49ba":"from sklearn.model_selection import cross_val_score\n\ndef pipeline_rmsle_cv(model, n_folds=3):\n    transformed_train = full_pipeline.fit_transform(train)\n    kfold = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(transformed_train)\n    rmse = np.sqrt(-cross_val_score(model, transformed_train, np.exp(target_log), scoring='neg_mean_squared_error', cv=kfold, n_jobs=4))\n    return rmse\n\ndef rmsle_cv(model, n_folds=3):\n    kfold = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, train.values, np.exp(target_log), scoring='neg_mean_squared_error', cv=kfold, n_jobs=4))\n    return rmse","effce906":"base_models = [\n    ('Ridge', reg1),\n    ('AdaBoost', reg3),\n    ('Gradient Boosting', reg4),\n    ('Random Forest', reg5), \n    ('XGB Regressor', reg6),\n    ('LGBM Regressor', reg7),\n]\n\nfor name, model in base_models:\n    score = rmsle_cv(model, n_folds=2)\n    print('{} = {:,.3f}({:.3f})\\n'.format(name, score.mean(), score.std()))","f0bf38c9":"base_models = [\n    ('Ridge', reg1),\n    ('AdaBoost', reg3),\n    ('Gradient Boosting', reg4),\n    ('Random Forest', reg5), \n    ('XGB Regressor', reg6),\n    ('LGBM Regressor', reg7),\n]\n\naverage_models = AverageModels(base_models)\nscore = rmsle_cv(average_models, n_folds=2)\nprint('AverageModels = {:,.3f}({:.3f})'.format(score.mean(), score.std()))","af4f3f28":"base_models = [\n    ('AdaBoost', reg3),\n    ('Gradient Boosting', reg4),\n    ('Random Forest', reg5), \n    ('XGB Regressor', reg6),\n    ('LGBM Regressor', reg7),\n]\n\nstacked_models = StackedModels(base_models=base_models, meta_model=('Ridge', reg1), n_folds=3)\nscore = rmsle_cv(stacked_models, n_folds=2)\nprint('StackedModels = {:,.3f}({:.3f})'.format(score.mean(), score.std()))","cccc2a79":"# base_models = [\n#     ('Gradient Boosting', reg4),\n#     ('Random Forest', reg5), \n#     ('XGB Regressor', reg6),\n#     ('LGBM Regressor', reg7),\n# ]\n\n# average_models = AverageModels(base_models)\n# average_models.fit(train, target_log)\n# average_models_predition = np.expm1(average_models.predict(test))\n# print(average_models_predition[:5])","7ceeab75":"base_models = [\n    ('AdaBoost', reg3),\n    ('Gradient Boosting', reg4),\n    ('Random Forest', reg5), \n    ('XGB Regressor', reg6),\n    ('LGBM Regressor', reg7),\n]\n\nstacked_models = StackedModels(base_models=base_models, meta_model=('Ridge', reg1))\nstacked_models.fit(train.values, target_log)\nstacked_models_predition = np.expm1(stacked_models.predict(test.values))\nprint(stacked_models_predition[:5])","ad25f5ca":"# preds = (average_models_predition * 0.2) + (stacked_models_predition * 0.8)\npreds = stacked_models_predition\nsubmission = pd.DataFrame({'id': ids,\n                           'price': preds})\n\nsubmission.to_csv('.\/submission.csv', index=False)","7b71cc1e":"# from sklearn.metrics import mean_squared_error\n\n# def rmse(y_true, y_pred):\n#     return np.sqrt(mean_squared_error(np.exp(y_true), np.exp(y_pred)))","8ed333f4":"# function_set = ('add', 'sub', 'mul', 'div', 'sin', 'cos', 'sqrt', 'min', 'max')\n\n# gp = SymbolicTransformer(population_size=2000, \n#                          generations=10,\n#                          function_set=function_set,\n#                          n_components=10,\n#                          parsimony_coefficient=0.0005,\n#                          max_samples=0.9, \n#                          verbose=1,\n#                          random_state=42)\n\n# gp.fit(X_train, y_train)","ac4ba32d":"# gp_features = gp.transform(X_train)\n# X_train_gp = np.hstack((X_train, gp_features))\n# print(X_train_gp.shape)","abe8a6fd":"# from sklearn.linear_model import Ridge\n\n# reg = Ridge()\n# reg.fit(X_train, y_train)\n# y_pred = reg.predict(X_train)\n# print(rmse(y_train, y_pred))\n\n# reg = Ridge()\n# reg.fit(X_train_gp, y_train)\n# y_pred = reg.predict(X_train_gp)\n# print(rmse(y_train, y_pred))","3e5227d2":"# X_new = test.drop(['id', 'date'], axis=1)","ccb25c03":"# gp_features = gp.transform(X_new)\n# X_new_gp = np.hstack((X_new, gp_features))\n# print(X_new_gp.shape)","3a70207c":"# import pandas as pd\n# import numpy as np\n# from scipy import stats\n\n# train = pd.read_csv('..\/input\/train.csv')\n# test = pd.read_csv('..\/input\/test.csv')\n# print(train.shape, test.shape)\n\n# # train = train[(np.abs(stats.zscore(train[cols])) < 5).all(axis=1)].copy()\n# # print(train.shape)\n# # train.dropna(inplace=True)\n# # print(train.shape)\n\n# target = np.log(train['price'].values+1)\n# train = train[cols].values","2b68cb99":"# train = data[:train.shape[0]]\n# test = data[train.shape[0]:]\n# target = np.log(target+1)\n# train = train.drop(['id', 'date', 'datetime'], axis=1).values\n# test = test.drop(['id', 'date', 'datetime'], axis=1).values\n# print(train.shape, test.shape, target.shape)","1afb4b34":"# from sklearn.metrics import mean_squared_error\n\n# def rmse(y_true, y_pred):\n#     return np.sqrt(mean_squared_error(y_true, y_pred))","81b45786":"# import xgboost as xgb\n# from sklearn.model_selection import KFold\n\n# params = {\n#     'objective': 'reg:linear',\n#     'eval_metric': 'rmse',\n#     'max_depth': 10, \n#     'eta': 0.1, \n#     'alpha': 3,\n#     'subsample': 0.8,\n#     'learning_rates': 0.1,\n#     'silent': 1,\n# }\n\n# train_preds = np.zeros(len(train))\n# xgb_preds = np.zeros(len(test))\n# xgb_test = xgb.DMatrix(test)\n\n# kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n# for train_index, valid_index in kfold.split(train, y=target):\n#     train_data = xgb.DMatrix(train[train_index], label=target[train_index])\n#     valid_data = xgb.DMatrix(train[valid_index], label=target[valid_index])\n    \n#     watch_list = [(train_data, 'train'), (valid_data, 'valid')]\n    \n#     num_round = 20000\n#     bst = xgb.train(params, train_data, num_round, evals=watch_list, \n#                     early_stopping_rounds=500, verbose_eval=False)\n    \n#     train_preds[valid_index] = bst.predict(valid_data, \n#                                            ntree_limit=bst.best_ntree_limit)\n#     xgb_preds += bst.predict(xgb_test, ntree_limit=bst.best_ntree_limit) \/ 5\n\n# print('RMSE = {:,}'.format(rmse(np.exp(target), np.exp(train_preds))))\n# print('RMSE = {:,}'.format(rmse(target, train_preds)))","46550148":"# import lightgbm as lgb\n\n# params = {\n#     'objective':'regression',\n#     'metric': 'rmse',\n#     'max_depth': -1,\n#     'learning_rate': 0.1,\n#     'min_child_samples': 16,\n#     'boosting': 'gbdt',\n#     'feature_fraction': 0.8,\n#     'lambda_l1': 0.1,\n#     'verbosity': -1,\n# }\n\n# train_preds = np.zeros(len(train))\n# lgb_preds = np.zeros(len(test))\n# lgb_test = lgb.Dataset(test)\n\n# kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n# for train_index, valid_index in kfold.split(train, y=target):\n#     train_data = lgb.Dataset(train[train_index], label=target[train_index])\n#     valid_data = lgb.Dataset(train[valid_index], label=target[valid_index])\n    \n#     watch_list = [train_data, valid_data]\n    \n#     num_round = 20000\n#     bst = lgb.train(params, train_data, num_round, valid_sets=watch_list, \n#                     early_stopping_rounds=500, verbose_eval=False)\n    \n#     train_preds[valid_index] = bst.predict(train[valid_index], num_iteration=bst.best_iteration)\n#     lgb_preds += bst.predict(test, num_iteration=bst.best_iteration) \/ 5\n\n# print('RMSE = {:,}'.format(rmse(np.exp(target), np.exp(train_preds))))\n# print('RMSE = {:,}'.format(rmse(target, train_preds)))","072fce4f":"# fig, ax = plt.subplots(figsize=(50,10), dpi=100)\n# xgb.plot_tree(bst, num_trees=42, ax=ax)\n# plt.show()","a446dc27":"# fig, ax = plt.subplots(figsize=(5,5), dpi=100)\n# xgb.plot_importance(bst, ax=ax)\n# plt.show()","bb3be007":"# dnew = xgb.DMatrix(X_new)\n# pred = bst.predict(dnew)\n# print(pred[:5])","ef99c1eb":"# preds = (np.exp(xgb_preds) * 0.2) + (np.exp(lgb_preds)) * 0.8\n# preds[:5]","af933f7e":"# submission = pd.DataFrame({'id': ids,\n#                            'price': preds})\n\n# submission.to_csv('.\/submission.csv', index=False)","ae83b59d":"### Visualize Geographic using Longitude and Latitude \n\nAs you may have just noticed, this data is housing price data from Seattle, USA. ","595cbc73":"<a href=\"\"><\/a>\n\n## \ub3c5\ub9bd\ubcc0\uc218(independent variables; features) \ud0d0\uc0c9\n","0bb966bc":"**This document is not yet complete.**\n\n# Seattle Housing Prices Analysis and Prediction\n\n## Contents\n\n1. <a href=\"#section2\">Interactive Visualization with Bokeh<\/a>\n2. <a href=\"#section3\">Exploratory Data Anlysis<\/a>\n    1. Exploring Dependent Variables\n    2. Exploring Independent Variables\n    3. Statistical Test\n3. Feature Importances\n    1. Permutation Importances\n    2. Drop-Column Importances\n4. Implementation of Pipeline\n5. Building Stacking Models","9d95e29e":"## \ud30c\uc0dd\ubcc0\uc218","8d74b6ac":"## XGBoost","7c533782":"## \uc791\uc131 \uc911 \uc785\ub2c8\ub2e4.","2c6c0eed":"### Feature Importances\n\n\uc815\ud655\ud55c \ud2b9\uc9d5 \uc911\uc694\ub3c4\ub97c \ub3c4\ucd9c\ud558\uae30 \uc704\ud574 permutation importance\ub97c \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uac04\ub2e8\ud55c \uc608\ub97c \ub4e4\uc5b4 \ud2b9\uc9d5 \uc911\uc694\ub3c4\uc5d0 \ub300\ud574 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uba3c\uc800 \ud574\ub2f9 \ub370\uc774\ud130\uc5d0\uc11c \ubb34\uc791\uc704\ub85c \uba87 \uac1c\uc758 \ubcc0\uc218\ub97c \uc120\ud0dd\ud558\uace0 \ub09c\uc218\ub97c \uc0dd\uc131\ud558\uc5ec \uc608\uc81c \ub370\uc774\ud130\ub97c \ub9cc\ub4e4\uace0 \ub79c\ub364 \ud3ec\ub808\uc2a4\ud2b8\uc5d0 \ud559\uc2b5\uc2dc\ucf1c\uc11c \ud2b9\uc9d5 \uc911\uc694\ub3c4\ub97c \ub3c4\ucd9c\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","6c1782d9":"### ANOVA","35b2fee6":"[](http:\/\/)## \uc608\uce21 \ubaa8\ud615 \uad6c\ucd95\n\n## Average Models and Stacked Models","fc7ad814":"## Ridge Regression","d477fe05":"### Drop columns importances","10e3e858":"### \uc0c1\uad00\uad00\uacc4","d5691551":"Bokeh\uc758 gmap()\uc740 \uad6c\uae00 \ub9f5\uc744 \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc5d0 \uad6c\uae00 API Key\ub97c \uc0dd\uc131\ud574\uc57c \ud569\ub2c8\ub2e4. \ud574\ub2f9 \ub370\uc774\ud130\uc758 \uc9c0\ub9ac\uc815\ubcf4\uc640 \uad6c\uae00 \ub9f5 API\ub97c \ud65c\uc6a9\ud558\uc5ec \uc2dc\uc560\ud2c0 \uc9c0\uc5ed\uc5d0 \uac01 \uc8fc\ud0dd\uc5d0 \ub300\ud55c \uc704\uce58\ub97c \uc2dc\uac01\ud654\ud574\ubcf4\uc558\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\uac00 \uc0dd\uac01\ud558\ub294 \uac83\ucc98\ub7fc \ubb3c\uac00\uc5d0 \uc788\uace0 \ud48d\uacbd\uc774 \uc88b\uc740 \uacf3\uc77c \uc218\ub85d \uc9d1\uac12\uc774 \ube44\uc2f8\ub2e4\ub294 \uac78 \uc9c1\uad00\uc801\uc73c\ub85c \uc0b4\ud3b4\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n\n<a id=\"section2\"><\/a>\n## \ud0d0\uc0c9\uc801 \uc790\ub8cc \ubd84\uc11d\n<a id=\"section3\"><\/a>\n### \uc885\uc18d\ubcc0\uc218 \ud0d0\uc0c9","45899b17":"## \uc774\uc0c1\uce58 \uc81c\uac70","7de009e0":"## \uc720\uc804 \ud504\ub85c\uadf8\ub798\ubc0d","379fbeb2":"### Permutation Importances","d24117ad":"### Constructing Pipeline"}}