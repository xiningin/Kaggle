{"cell_type":{"c1c3b5e6":"code","3f0da2ef":"code","20fee07f":"code","0420dafb":"code","b92d9c7f":"code","631ce317":"code","fe5fb25c":"code","89c2b862":"code","02e2ba8f":"code","47a8611d":"code","59e66848":"code","9f9a1212":"code","81010eb8":"code","cd202d99":"code","5f22b754":"code","2be094c6":"code","0e6bd011":"code","e7588866":"code","d518cf96":"code","bb975abd":"code","19998a59":"code","781ef0ca":"code","d61f9d3f":"code","8c03eead":"code","5439c1c3":"code","99a46f40":"code","caaa8238":"code","284062ea":"code","b2267ece":"code","6dfea6e5":"code","017fb7ab":"code","60ee2bf1":"code","6a6d68e9":"code","5c417295":"code","da0f3a01":"code","f7fe448d":"code","2b1370e0":"code","db064341":"code","16e86a00":"code","15ad94af":"code","b65cd60f":"code","f4011d85":"code","66128242":"code","9a5ce008":"code","6b7038db":"code","b4818f58":"markdown","572dafab":"markdown","5517edcc":"markdown","b3d63597":"markdown"},"source":{"c1c3b5e6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","3f0da2ef":"data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","20fee07f":"data.info()","0420dafb":"#dropping Id as it is an irrelevant variable\ndata.drop(['Id'], inplace=True, axis=1)","b92d9c7f":"data.describe()","631ce317":"#checking skewness in independent variable.\ndata['SalePrice'].hist(bins=50)","fe5fb25c":"#Improving skewness of the variable using log\ndata['SalePrice'] = np.log1p(data['SalePrice'])\ndata['SalePrice'].hist(bins=50)","89c2b862":"data.corr()['SalePrice'].sort_values()","02e2ba8f":"plt.figure(figsize = (16, 10))\ncorr =  data.corr()\nkot = corr[corr>=0.7]\nsns.heatmap(kot, annot = True, cmap=\"YlGnBu\")\n# below ylim adjustment was made as the new matplotlib version cuts off the top and bottom edges by 0.5 value\nb, t = plt.ylim()\nb += 0.5\nt -= 0.5\nplt.ylim(b, t)\nplt.show()","47a8611d":"data.drop(['GarageYrBlt', 'GarageArea', 'TotRmsAbvGrd','MoSold'], inplace=True, axis=1)\n# GarageYrBlt and YearBuilt have correlation of 0.83. But YearBuilt has higher correlation with target variable\n# GarageArea and GarageCars have correlation of 0.88. But GarageCars has higher correlation with target variable\n# TotRmsAbvGrd and GrLivArea have correlation of 0.83. But GrLivArea has higher correlation with target variable\n# MoSold seems to be another irrelevant variable. ","59e66848":"data.plot(kind='scatter', x='KitchenAbvGr', y='SalePrice', color='r') ","9f9a1212":"sum(data['GarageCond'].eq(data['GarageQual']))\n# sum(data['OverallCond'].eq(data['OverallQual']))","81010eb8":"data.MSSubClass = data.MSSubClass.apply(lambda x: str(x)) # converting it to a categorical variable from numeric\ndata['Age'] = data.YrSold - data.YearBuilt\ndata['YearRemodAdd'] = data.YearRemodAdd.eq(data.YearBuilt)\ndata.drop(['YearBuilt'], inplace=True, axis=1)\ndata.totBath = data.BsmtFullBath + data.FullBath + 0.5*(data.BsmtHalfBath + data.HalfBath)\ndata.drop(['BsmtFullBath', 'FullBath', 'BsmtHalfBath', 'HalfBath'], inplace=True, axis=1)\ndata['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\ndata.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'], inplace=True, axis=1)\ndata.TwoKitchens = data.KitchenAbvGr.apply(lambda x: 1 if x>1 else 0)\ndata.drop(['KitchenAbvGr', 'BedroomAbvGr', 'GarageCond'], inplace=True, axis=1)","cd202d99":"numeric_data = data.select_dtypes(include=[np.number])\nnumeric_data.info()\ncategorical_data = data.select_dtypes(exclude=[np.number])","5f22b754":"categorical_data.info()","2be094c6":"data[['MasVnrArea', 'MasVnrType']][data['MasVnrArea'].isna()] #consider no MasVnr (i.e fillna with 0) where MasVnrArea = NA ","0e6bd011":"ax1 = data.plot(kind='scatter', x='BsmtFinSF1', y='SalePrice', color='r')\nax2 = data.plot(kind='scatter', x='BsmtFinSF2', y='SalePrice', color='y', ax =ax1) ","e7588866":"data['Electrical'].hist()","d518cf96":"data[['Fireplaces', 'FireplaceQu']][data['FireplaceQu'].isna()]","bb975abd":"numeric_data['MasVnrArea'].fillna(0.0, inplace = True)\ncategorical_data['Electrical'].fillna('SBrkr', inplace = True)\nnumeric_data.fillna(numeric_data.median(), inplace=True)\ncategorical_data.fillna('None', inplace=True)","19998a59":"dummy = pd.get_dummies(categorical_data, drop_first=True)","781ef0ca":"data = pd.concat([numeric_data, dummy], axis = 1)\ndata.info()","d61f9d3f":"data_val = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndata_val.info()","8c03eead":"val_id = data_val.pop('Id')\ndata_val.drop(['GarageYrBlt', 'GarageArea', 'TotRmsAbvGrd','MoSold'], inplace=True, axis=1)\n\ndata_val.MSSubClass = data_val.MSSubClass.apply(lambda x: str(x)) # converting it to a categorical variable from numeric\ndata_val['Age'] = data_val.YrSold - data_val.YearBuilt\ndata_val['YearRemodAdd'] = data_val.YearRemodAdd.eq(data_val.YearBuilt)\ndata_val.drop(['YearBuilt'], inplace=True, axis=1)\ndata_val.totBath = data_val.BsmtFullBath + data_val.FullBath + 0.5*(data_val.BsmtHalfBath + data_val.HalfBath)\ndata_val.drop(['BsmtFullBath', 'FullBath', 'BsmtHalfBath', 'HalfBath'], inplace=True, axis=1)\ndata_val['TotalSF'] = data_val['TotalBsmtSF'] + data_val['1stFlrSF'] + data_val['2ndFlrSF']\ndata_val.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'], inplace=True, axis=1)\ndata_val.TwoKitchens = data_val.KitchenAbvGr.apply(lambda x: 1 if x>1 else 0)\ndata_val.drop(['KitchenAbvGr', 'BedroomAbvGr', 'GarageCond'], inplace=True, axis=1)","5439c1c3":"numeric_val = data_val.select_dtypes(include=[np.number])\ncategorical_val = data_val.select_dtypes(exclude=[np.number])\nnumeric_val['MasVnrArea'].fillna(0.0, inplace = True)\ncategorical_val['Electrical'].fillna('SBrkr', inplace = True)\nnumeric_val.fillna(numeric_val.median(), inplace=True)\ncategorical_val.fillna('None', inplace=True)\n\ndummy_val = pd.get_dummies(categorical_val, drop_first=True)\n\ndata_val = pd.concat([numeric_val, dummy_val], axis = 1)\nnumeric_val.info()","99a46f40":"uncommon_col = list(set(data.columns) ^ set(data_val.columns))\nuncommon_col.remove('SalePrice')\nprint(uncommon_col)\nfor col in uncommon_col:\n    if col in data.columns: data.drop([col], inplace = True, axis = 1)\n    if col in data_val.columns: data_val.drop([col], inplace = True, axis = 1)\ndata.info()","caaa8238":"from sklearn.model_selection import train_test_split\nnp.random.seed(0)\ndata_train, data_test = train_test_split(data, train_size = 0.7, test_size = 0.3, random_state = 100)","284062ea":"from sklearn.preprocessing import MinMaxScaler\nx_scaler = MinMaxScaler()\ny_scaler = MinMaxScaler()","b2267ece":"y_train = data_train.pop('SalePrice')\nx_train = data_train\ny_train.head()","6dfea6e5":"numeric_columns = x_train.select_dtypes(include = [np.number]).columns\nx_train[numeric_columns] = x_scaler.fit_transform(x_train[numeric_columns])\n# y_train =  y_scaler.fit_transform(pd.DataFrame(y_train.iloc[:]))","017fb7ab":"x_train.describe()","60ee2bf1":"from sklearn.ensemble import RandomForestRegressor\nlm = RandomForestRegressor(n_estimators = 1200, max_depth=60,random_state = 0, oob_score = True, n_jobs=10)","6a6d68e9":"lm.fit(x_train, y_train)","5c417295":"y_train_price = lm.predict(x_train)\n# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","da0f3a01":"y_test = data_test.pop('SalePrice')\nx_test = data_test\nx_test[numeric_columns] = x_scaler.transform(x_test[numeric_columns])\n# y_test = y_scaler.transform(y_test)","f7fe448d":"y_pred = lm.predict(x_test)","2b1370e0":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrmsle = sqrt(mean_squared_error(y_test, y_pred))\nprint('Model RMSLE:',rmsle)","db064341":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","16e86a00":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label\n# plt.ylim(-0.5,1)\n# plt.xlim(-0.5,1)","15ad94af":"data_val[numeric_columns] = x_scaler.transform(data_val[numeric_columns])\npred = lm.predict(data_val)","b65cd60f":"pred = np.expm1(pred)","f4011d85":"submission_df = pd.DataFrame({\n    'Id': val_id,\n    'SalePrice': pred\n})","66128242":"submission_df.head()","9a5ce008":"submission_df.to_csv('submission_rf_03.csv',index = False)","6b7038db":"#Submission score = 0.149","b4818f58":"Link to the Kaggle Problem: https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview\n## Importing libraries and loading train-test data","572dafab":"### Test.csv and Train.csv data feature selection\nCategorical features in both the data may contain un-equal values. To ensure identical feature set we eliminate all non-intersecting features.","5517edcc":"## Data Preprocessing and EDA","b3d63597":"### Train_test(val) split of train.csv (data) and learning"}}