{"cell_type":{"b91858ab":"code","c3083d10":"code","5559b210":"code","76fc5b7a":"code","05d3ac8c":"code","68263dc7":"code","8c36ee50":"code","cbbbca74":"code","db80dfe3":"code","f6cc1693":"code","51e87bb2":"code","75acf06c":"code","c41cb59e":"code","5a35b6ce":"code","971677af":"code","e15f38ff":"code","b08923e0":"code","6288b133":"markdown","4598862a":"markdown","6384b9c6":"markdown","d29a1550":"markdown","2ec0e7c9":"markdown","205839f9":"markdown","8b899d4c":"markdown","2cbc0aaa":"markdown","c509c10b":"markdown","3164c9f7":"markdown","fad96a86":"markdown","793d1ed1":"markdown","2ec28049":"markdown","6bbcaa23":"markdown","49cf8ee8":"markdown","24f049c7":"markdown","8a31ab4e":"markdown","b914d3b8":"markdown","5bfbe0ec":"markdown","e781c544":"markdown"},"source":{"b91858ab":"import tensorflow as tf\n\ntf.keras.losses.binary_crossentropy(\n    y_true, y_pred,\n    from_logits=False,\n    label_smoothing=0\n)\n","c3083d10":"\ntrain[np.where(y == 0)] = 0.1\ntrain[np.where(y == 1)] = 0.9\n","5559b210":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nplt.style.use('ggplot')\nepochs=10","76fc5b7a":"data= load_breast_cancer()","05d3ac8c":"print(\"The dataset contains {} samples with {} features\".format(data['data'].shape[0],data['data'].shape[1]))\n","68263dc7":"train_X,test_X,train_y,test_y=train_test_split(data['data'],data['target'],random_state=77)","8c36ee50":"train_y[:40]=1","cbbbca74":"def model():\n    inp = tf.keras.Input(shape=(30))\n    \n    x= tf.keras.layers.Dense(64,activation='relu')(inp)\n    x=tf.keras.layers.Dense(32,activation='relu')(x)\n    x=tf.keras.layers.Dense(1,'sigmoid')(x)\n    \n    model=tf.keras.Model(inp,x)\n    model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n    return model\n    \n    ","db80dfe3":"model=model()\nmodel.summary()","f6cc1693":"\nhistory=model.fit(train_X,train_y,epochs=epochs)","51e87bb2":"plt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nplt.plot(np.arange(1,epochs+1),history.history['loss'],color='red',alpha=1)\nplt.gca().set_xlabel(\"Epochs\")\nplt.gca().set_ylabel(\"loss\")\nplt.gca().set_title(\"Loss without Label smoothing\")\n\nplt.subplot(1,2,2)\nplt.plot(np.arange(1,epochs+1),history.history['accuracy'],color='red',alpha=1)\nplt.gca().set_xlabel(\"Epochs\")\nplt.gca().set_ylabel(\"accuracy\")\nplt.gca().set_title(\"accuracy without Label smoothing\")\n\n\nplt.show()","75acf06c":"y_pre = model.predict(test_X)\nprint(accuracy_score(test_y,np.round(y_pre)))","c41cb59e":"def label_smoothing(y_true,y_pred):\n    \n     return tf.keras.losses.binary_crossentropy(y_true,y_pred,label_smoothing=0.1)","5a35b6ce":"def model():\n    inp = tf.keras.Input(shape=(30))\n    \n    x= tf.keras.layers.Dense(64,activation='relu')(inp)\n    x=tf.keras.layers.Dense(32,activation='relu')(x)\n    x=tf.keras.layers.Dense(1,'sigmoid')(x)\n    \n    model=tf.keras.Model(inp,x)\n    \n    model.compile(optimizer='Adam',loss=label_smoothing,metrics=['accuracy'])\n    return model\n    ","971677af":"model=model()\nhistory=model.fit(train_X,train_y,epochs=epochs)","e15f38ff":"plt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nplt.plot(np.arange(1,epochs+1),history.history['loss'],color='red',alpha=1)\nplt.gca().set_xlabel(\"Epochs\")\nplt.gca().set_ylabel(\"loss\")\nplt.gca().set_title(\"Loss with Label smoothing\")\n\nplt.subplot(1,2,2)\nplt.plot(np.arange(1,epochs+1),history.history['accuracy'],color='red',alpha=1)\nplt.gca().set_xlabel(\"Epochs\")\nplt.gca().set_ylabel(\"accuracy\")\nplt.gca().set_title(\"accuracy with Label smoothing\")\n\n\nplt.show()","b08923e0":"y_pre = model.predict(test_X)\nprint(accuracy_score(test_y,np.round(y_pre)))","6288b133":"- Let's do the train and test split now,(80:20)","4598862a":"OR you can simply convert your one hot encoded array of floating point numbers to the \u201cnuanced\u201d version\n","6384b9c6":"In this competition we are trying to use many data augmentation methods, especially to generate more data on languages other than English and train on those examples.\n\nThe methods used until now are :\n- [Translating data using Google API](https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/discussion\/141377)\n- [NLP Albumentation](https:\/\/www.kaggle.com\/shonenkov\/nlp-albumentations)\n- [Parellel corpus](https:\/\/www.kaggle.com\/shonenkov\/hack-with-parallel-corpus)\n\nI went through all of these methods and tried some of the them.No doubt, they are pretty useful. But after reading the recent notebook from [Alex Shonenkov](https:\/\/www.kaggle.com\/shonenkov) I got a link to the idea of label smoothing which I am going to discuss here.All these data augmentation methods have a possible hidden threat in them which is why many of us are still in a dilemma whether to use them or not.\n\nThe main possible threat is that there is a chance that these methods produce data samples with incorrect labels.I tried using Google translate API to some of the toxic comments and I saw that sometimes it is censoring the toxic content in some comments or reducing the level of toxicity in it( I saw it happen using google API). This can happen to other augmentation methods too...\n\nSo, what's the solution to this problem?\n- Don't bother to use the augmentation methods and lose the edge that it gives.\n- Use Label Smoothing.\n\n**So let's do label smoothing.**","d29a1550":"Next,let's build our simple NN model","2ec0e7c9":"## <font color='blue' size='4'>Please leave an upvote if you like this Notebook<\/font>\n","205839f9":"![](https:\/\/media.giphy.com\/media\/3o6MbqwVaVfbxMJTTq\/giphy.gif)","8b899d4c":"If label_smoothing is nonzero, smooth the labels towards 1\/num_classes:\n\n\n`new_onehot_labels = onehot_labels * (1 \u2013 label_smoothing) + label_smoothing \/ num_classes`\n\nWhat does this mean?\n\nWell, say in our case were training a model for binary classification,Our labels are 0\u200a\u2014\u200aNon-toxic, 1\u200a\u2014\u200atoxic.\n\nNow, say you  label_smoothing = 0.2\n\nUsing the equation above, we get:\n\n`new_labels = [0 1] * (1\u200a\u2014\u200a0.2) + 0.2 \/ 2 =[0 1]*(0.8) + 0.1`\n\n `new_labels = [0.1 ,0.9]`\n","2cbc0aaa":"You can see that the model has an accuracy of only 67% in the test set.\n- Now let's add the trick and train our model.Train our model **with label smoothing**","c509c10b":"viola..! The accuracy just increased  just by adding **label smoothing**.","3164c9f7":"Now,the moment of truth !\n- let's predict on the same test set and see how much accuracy does it give...","fad96a86":"## <font size='4' color='red'>Expirement with Label Smoothing<\/font>","793d1ed1":"## <font size='3' color='red'>Say hello to Label Smoothing!<\/font>\n\nWhen we apply the cross-entropy loss to a classification task, we\u2019re expecting true labels to have 1, while the others 0. In other words, we have no doubts that the true labels are true, and the others are not. Is that always true in our case? As said above, the translation or other augmentation methods have done some mistakes. They might have different criteria. They might make some mistakes. As a result, the ground truth labels we have had perfect beliefs on are possibly wrong. In our case, a sample with no toxicity will have had perfect belief as toxic or vice versa...\n\nOne possible solution to this is to relax our confidence on the labels. For instance, we can slightly lower the loss target values from 1 to, say, 0.9. And naturally, we increase the target value of 0 for the others slightly as such. This idea is called label smoothing.\n\n[![label-smoothing.png](https:\/\/i.postimg.cc\/cHvh4hpW\/label-smoothing.png)](https:\/\/postimg.cc\/VrcnKqsZ)\nIn tensorflow,","2ec28049":"## <font size='3' color='red'>What does this do ?<\/font>","6bbcaa23":"One should see the values 0 and 1 as simply true and false. Taking values that vary slightly from the classic values, they are a more nuanced way to describing the data 0.1 could be viewed as: \u201cthere is a very low chance this data is <one of two classes>\u201d whereas 0.9\n\nis, of course, a high chance.\n\nUsing these \u201cnuanced\u201d labels, the cost of an incorrect prediction is slightly lower than using \u201chard\u201d labels resulting in a smaller gradient. While this intuition helped me understand why it could be a good idea, I was not entirely convinced it would work in an application because the loss is lowered for all wrong classifications. So I decided to read some about some research done on the subject.\n\nA table copied from [When Does Label Smoothing Help?](https:\/\/arxiv.org\/pdf\/1906.02629.pdf)\n\n![](https:\/\/rickwierenga.com\/assets\/images\/smoothing.png)","49cf8ee8":"Now for our expirement I will relabel some of the negative cases (target 0 ) as positive cases (target 1).","24f049c7":"Now, Fit ,evaluate and predict **without label smoothing**","8a31ab4e":"Let's load the dataset and see","b914d3b8":"Now,it's time to expirement with the same.For the sake our expirement I will take the data from `sklearn.datasets`. We need a binary classificatin example,so I selected `breast cancer` dataset which contains approx 500 samples with 30 features.","5bfbe0ec":"I hope this helps,I encourage you to try this and comment your results below.\n## <font color='blue' size='4'>Please leave an upvote if you like this Notebook<\/font>\n\nReferences :\n- https:\/\/arxiv.org\/pdf\/1906.02629.pdf\n- https:\/\/www.flixstock.com\/label-smoothing-an-ingredient-of-higher-model-accuracy\/","e781c544":"\nContents\n* [Threat in using augmented Data]()\n* [Tackle with label smoothing]()\n* [Expirement with label smoothing]()\n\n"}}