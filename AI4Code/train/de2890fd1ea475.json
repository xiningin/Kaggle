{"cell_type":{"fd667dfe":"code","22a67915":"code","adf48f62":"code","640368d9":"code","5e1118d6":"code","9d3518b6":"code","d62f761f":"code","7bcf0d67":"code","48dd9622":"code","08493d6a":"code","c7121c95":"code","02a2e250":"code","782b026d":"code","79e01564":"code","5958cf12":"code","4e7dd620":"code","6e2dfae2":"code","004a5e79":"code","751794c0":"code","aa4afb7a":"code","b0c5a548":"code","9e7a9f7f":"code","510bbff5":"markdown","7eac1236":"markdown","dda70782":"markdown","8ece5780":"markdown","5e054bd3":"markdown","76ee417e":"markdown","464ec23f":"markdown"},"source":{"fd667dfe":"# importing libraries\nimport os, time, random, sys\nos.environ['PYTHONHASHSEED']=str(1)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (12, 8)\n\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nimport sklearn.utils as sku\nfrom skimage.io import imread\nfrom skimage.transform import resize\nseed = 12","22a67915":"import tensorflow as tf\nimport tensorflow_addons as tfa\nprint(\"TF version:-\", tf.__version__)\nimport keras as k\nfrom keras import backend as K","adf48f62":"def runSeed():\n    global seed\n    os.environ['PYTHONHASHSEED']=str(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\nrunSeed()\n\n## Checking the GPU configuration\n!nvidia-smi","640368d9":"# from google_drive_downloader import GoogleDriveDownloader as gdd\n\n# gdd.download_file_from_google_drive(file_id='1H0rJmSBmYQoWM2w2tqy-jmX0Y2Wg6k2v', \n#                                     dest_path='content\/flowers.zip', unzip=True)","5e1118d6":"basePath = '\/kaggle\/input\/flowers-dataset\/'\ntrainPath = basePath + 'train\/'\nos.listdir(trainPath)","9d3518b6":"submission_test_set = pd.read_csv(basePath + 'Testing_set_flower.csv')\nsubmission_test_set.head()","d62f761f":"def showImage(img):\n    plt.figure(figsize=(3,3))\n    plt.imshow(img)\n    plt.show()","7bcf0d67":"# constants\nbatch_size = 128\nimg_dim = 299\ndef getImgTensor(img_d):\n    return (img_d, img_d, 3)\ngetImgTensor(img_dim)","48dd9622":"# reading training and validation separately to prevent overlapping \n# \u043f\u0440\u043e \u0446\u0435 \u0433\u043e\u0432\u043e\u0440\u0438\u043b\u0438, \u0437\u043c\u0456\u043d\u0430 \u0437\u043d\u0430\u0447\u0435\u043d\u044c \u0434\u043e \u0432\u0456\u0434\u0440\u0456\u0437\u043a\u0443 (0, 1), \u043f\u043e\u0432\u043e\u0440\u043e\u0442\u0438 \u0456 \u0442\u0434\ntrain_datagen = k.preprocessing.image.ImageDataGenerator(rescale=1.\/255, \n                                                         validation_split=0.2,\n                                                         shear_range=0.2, \n                                                         zoom_range=0.2, \n                                                         horizontal_flip=True, \n                                                         rotation_range=45,\n                                                         width_shift_range=0.1, \n                                                         height_shift_range=0.1,\n                                                         fill_mode='nearest'\n                                                        )\n# \u0442\u0443\u0442 \u0432\u043a\u0430\u0437\u0443\u0454\u0442\u044c\u0441\u044f \u043f\u0430\u043f\u043a\u0430 \u0434\u043b\u044f \u0432\u0438\u0449\u0435\u0432\u043a\u0430\u0437\u0430\u043d\u043e\u0433\u043e \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\u0430\ntrain_generator=train_datagen.flow_from_directory(directory=trainPath,\n                                                  subset=\"training\",\n                                                  batch_size=batch_size,\n                                                  color_mode=\"rgb\",\n                                                  seed=seed,\n                                                  shuffle=True,\n                                                  class_mode=\"categorical\",\n                                                  target_size=getImgTensor(img_dim)[:2])","08493d6a":"# generate class weights as classes are imbalanced\n# \u043c\u043e\u0436\u0435\u0448 \u043f\u043e\u0433\u0443\u0433\u043b\u0438\u0442\u044c, \u043f\u043e\u0440\u0430\u0445\u0443\u0432\u0430\u0442\u044c \u0432\u0430\u0433\u0438 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0456\u0432, \u0449\u043e\u0431 \u0442\u0456 \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u043d\u044f, \u044f\u043a\u0438\u0445 \u043c\u0435\u043d\u0448\u0435, \u0441\u0438\u043b\u044c\u043d\u0456\u0448\u0435 \u0432\u043f\u043b\u0438\u0432\u0430\u043b\u0438 \u043d\u0430 \u043c\u043e\u0434\u0435\u043b\u044c\nclass_weights = sku.class_weight.compute_class_weight('balanced',\n                                                      np.unique(train_generator.classes), \n                                                      train_generator.classes)\ntrain_class_weights = {i:x for i, x in enumerate(class_weights)}\ntrain_class_weights","c7121c95":"batch = train_generator.next()[0]\nshowImage(batch[0])\nshowImage(batch[1])","02a2e250":"# \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0434\u043b\u044f \u0432\u0430\u043b\u0456\u0434\u0430\u0446\u0456\u0439\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0443, \u0437\u0430\u0431\u0438\u0439, \u043d\u0435 \u043f\u0438\u0448\u0438\nvalid_generator=train_datagen.flow_from_directory(directory=trainPath,\n                                                  subset=\"validation\",\n                                                  batch_size=batch_size,\n                                                  color_mode=\"rgb\",\n                                                  seed=seed,\n                                                  shuffle=True,\n                                                  class_mode=\"categorical\",\n                                                  target_size=getImgTensor(img_dim)[:2])","782b026d":"# \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0443, \u0442\u0456\u043b\u044c\u043a\u0438 \u0440\u0435\u0441\u043a\u0435\u0439\u043b \u0434\u043e (0, 1)\ntest_datagen = k.preprocessing.image.ImageDataGenerator(rescale=1.\/255)\n\ntest_generator=test_datagen.flow_from_directory(basePath, \n                                                batch_size=1,\n                                                color_mode=\"rgb\",\n                                                seed=seed,\n                                                shuffle=False,\n                                                classes=['test'],\n                                                target_size=getImgTensor(img_dim)[:2])","79e01564":"# \u0432\u0438\u0432\u043e\u0434\u0438\u0442\u044c \u0433\u0440\u0430\u0444\u0456\u0447\u043a\u0438, \u0433\u0443\u0433\u043b\u0438 loss (\u0444\u0443\u043d\u043a\u0446\u0456\u044f \u0432\u0442\u0440\u0430\u0442) \u0443 \u0442\u0435\u0431\u0435 \u044e\u0437\u0430\u0454\u0442\u044c\u0441\u044f CategoricalCrossentropy, \n# categorical_accuracy (\u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0456\u0439\u043d\u0430 \u0442\u043e\u0447\u043d\u0456\u0441\u0442\u044c)\ndef plotModelHistory(h):\n    fig, ax = plt.subplots(1, 2, figsize=(15,4))\n    ax[0].plot(h.history['loss'])   \n    ax[0].plot(h.history['val_loss'])\n    ax[0].legend(['loss','val_loss'])\n    ax[0].title.set_text(\"Train loss vs Validation loss\")\n\n    ax[1].plot(h.history['categorical_accuracy'])   \n    ax[1].plot(h.history['val_categorical_accuracy'])\n    ax[1].legend(['categorical_accuracy','val_categorical_accuracy'])\n    ax[1].title.set_text(\"Train accuracy vs Validation accuracy\")\n\n    print(\"Max. Training Accuracy\", max(h.history['categorical_accuracy']))\n    print(\"Max. Validaiton Accuracy\", max(h.history['val_categorical_accuracy']))","5958cf12":"class myCallback(k.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        ACCURACY_THRESHOLD = 0.99\n        if(logs.get('categorical_accuracy') > ACCURACY_THRESHOLD):\n            print(\"\\n\\nStopping training as we have reached %2.2f%% accuracy!\" %(ACCURACY_THRESHOLD*100))   \n            self.model.stop_training = True","4e7dd620":"\ndef trainModel(model, epochs, optimizer, vb=1, modelName='model'):\n    bestModelPath = '.\/'+modelName+'_model.hdf5'\n    callback = myCallback()\n    # \u0433\u0443\u0433\u043b\u0438 \u043a\u043e\u043b\u043b\u0431\u0435\u043a\u0438 \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u0444\u043b\u043e\u0432\u0456 (tensorflow callback) ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n    callbacks_list = [\n        callback,\n        k.callbacks.ReduceLROnPlateau(monitor = 'val_loss', patience = 5, verbose = 1, min_lr=0.00001), \n        k.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15, verbose = 1, restore_best_weights = True), \n        k.callbacks.ModelCheckpoint(filepath=bestModelPath, monitor='val_loss', verbose=1, save_best_only=True)\n    ]\n    # \u043c\u043e\u0434\u0435\u043b\u044c \u0437\u0431\u0438\u0440\u0430\u0454\u0442\u044c\u0441\u044f, \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0454\u0442\u044c\u0441\u044f \u0444\u0443\u043d\u043a\u0446\u0456\u044f \u043f\u043e\u043c\u0438\u043b\u043a\u0438 CategoricalCrossentropy \u0456 CategoricalAccuracy\n    model.compile(optimizer=optimizer,\n                  loss=k.losses.CategoricalCrossentropy(label_smoothing=.05),\n                  metrics=[k.metrics.CategoricalAccuracy()]\n    )\n    train_generator.reset()\n    \n    steps_per_epoch = np.ceil(train_generator.n\/train_generator.batch_size)\n    validation_steps = np.ceil(valid_generator.n\/valid_generator.batch_size)\n\n    return model.fit_generator(generator=train_generator, steps_per_epoch=steps_per_epoch, \n                               validation_data=valid_generator, validation_steps=validation_steps, \n                               epochs=epochs, verbose=vb,\n                              #  class_weight=train_class_weights,\n                               callbacks=callbacks_list)","6e2dfae2":"# evaluate model with time\ndef evaluateModel(model, path=True):\n    batch_size = valid_generator.batch_size\n    num_train_sequences = valid_generator.n\n    valid_generator.reset()\n    steps_per_epoch = 0\n    if (valid_generator.n%valid_generator.batch_size) == 0:\n        steps_per_epoch = int(valid_generator.n\/valid_generator.batch_size)\n    else:\n        steps_per_epoch = int(valid_generator.n\/\/valid_generator.batch_size) + 1\n\n    t1 = time.time()\n    if path:\n        model = k.models.load_model(model)\n    eval_results = model.evaluate_generator(valid_generator, steps=steps_per_epoch)\n    t2 = time.time()\n    print(f'\\nLoss: {eval_results[0]}, Accuracy: {eval_results[1]}')\n    print(f'Prediction Time per Image: {(t2-t1)\/valid_generator.n}')","004a5e79":"# predict images using model\ndef predictModel(modelPath):\n    batch_size = test_generator.batch_size\n    num_train_sequences = test_generator.n\n    steps_per_epoch = 0\n    if (test_generator.n%test_generator.batch_size) == 0:\n        steps_per_epoch = int(test_generator.n\/test_generator.batch_size)\n    else:\n        steps_per_epoch = int(test_generator.n\/\/test_generator.batch_size) + 1\n\n    test_generator.reset()\n\n    t1 = time.time()\n    model = k.models.load_model(modelPath)\n    predictions = model.predict_generator(test_generator, steps=steps_per_epoch, verbose=1)\n    t2 = time.time()\n    print(f'Prediction Time per Image: {(t2-t1)\/test_generator.n}')\n    \n    print(\"Generating Predictions file..\")\n    labels = (train_generator.class_indices)\n    labels = dict((v,k) for k,v in labels.items())\n    predicted_class_indices=np.argmax(predictions, axis=1)\n    predictions_label = [labels[k] for k in predicted_class_indices]\n    filenames = list(map(lambda x: x.split('\/')[-1], test_generator.filenames))\n    submission=pd.DataFrame({\n        \"Filename\":filenames, \n        \"Class\":predictions_label\n    })\n    # generate series of predictions as per testing_set\n    submission_final = pd.Series([submission[submission['Filename'] == x].iloc[0,1] for x in np.ravel(submission_test_set.values)])\n    submission_file = \"submission_\"+modelPath.split('\/')[-1].split('_')[0]+\".csv\"\n    submission_final.to_csv(submission_file,index=False, header=['prediction'])\n    print(f\"Submission file with {len(submission.values)} rows generated:\", submission_file)\n    submission.head()","751794c0":"img_dim=224\n# \u0441\u0442\u0432\u043e\u0440\u044e\u0454\u0442\u044c\u0441\u044f MobileNetV2 (\u043c\u043e\u0434\u0435\u043b\u044c) \u043f\u0440\u043e \u043d\u0435\u0457 \u043c\u043e\u0436\u043d\u0430 \u0440\u043e\u0437\u043f\u0438\u0441\u0430\u0442\u044c\nmobilenet = k.applications.mobilenet_v2.MobileNetV2(weights='imagenet', input_shape=getImgTensor(img_dim), include_top=False)\n# \u0441\u0442\u0430\u0432\u0438\u0442\u044c\u0441\u044f, \u0449\u043e\u0431 \u0432\u043e\u043d\u0430 \u043d\u0435 \u043d\u0430\u0432\u0447\u0430\u043b\u0430\u0441\u044c \nmobilenet.trainable = False\n\n# \u0434\u043e \u043c\u043e\u0434\u0435\u043b\u0456 \u0434\u043e\u0434\u0430\u044e\u0442\u044c\u0441\u044f \u0441\u043b\u043e\u0457, \u043f\u0440\u043e \u043a\u043e\u0436\u0435\u043d \u0456\u0437 \u043d\u0438\u0445 \u043c\u043e\u0436\u043d\u0430 \u043f\u0438\u0441\u0430\u0442\u044c, \u043c\u043e\u0436\u0435\u0448 \u043d\u0430\u043f\u0438\u0441\u0430\u0442\u044c \u043f\u0440\u043e \u0444\u0443\u043d\u043a\u0446\u0456\u044e \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0456\u0457 relu \u0456 softmax\nmodel = k.models.Sequential([\n                             mobilenet,\n                             tf.keras.layers.GlobalAveragePooling2D(),\n                             k.layers.Dropout(0.4),\n                             k.layers.Dense(256, activation='relu'),\n                            #  k.layers.BatchNormalization(),\n                             k.layers.Dropout(0.3),\n                             k.layers.Dense(128, activation='relu'),\n#                              k.layers.BatchNormalization(),\n                             k.layers.Dropout(0.2),\n                             k.layers.Dense(5, activation='softmax')\n])\nprint(model.summary())","aa4afb7a":"# \u043f\u043e\u0447\u0438\u043d\u0430\u0454\u0442\u044c\u0441\u044f \u0442\u0440\u0435\u043d\u0443\u0432\u0430\u043d\u043d\u044f, \u043e\u043f\u0442\u0438\u043c\u0456\u0437\u0430\u0442\u043e\u0440 - adam (\u043c\u043e\u0436\u0435\u0448 \u0433\u0443\u0433\u043b\u0438\u0442\u044c, \u0443 \u043c\u0435\u043d\u0435 \u043f\u0440\u043e \u043d\u044c\u043e\u0433\u043e \u0454)\nhistory_1 = trainModel(model, 50, 'adam', modelName='mobilenet')","b0c5a548":"plotModelHistory(history_1)","9e7a9f7f":"# mobile net\n# \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0438 \u043d\u0435\u0439\u0440\u043e\u043d\u043a\u0438 0.88 - \u0442\u043e\u0447\u043d\u0456\u0441\u0442\u044c\nimg_dim=224\nevaluateModel('.\/mobilenet_model.hdf5')","510bbff5":"# Model Building","7eac1236":"# Model Evaluation","dda70782":"### Loading Dataset","8ece5780":"# DPhi - Flower Recognition Challenge\n\nThe dataset contains images of 5 types of flowers.\n\nClasses:-\n- daisy\n- dandelion\n- rose\n- sunflower\n- tulip\n\n# Reading & Understanding Data\n## Importing Libraries","5e054bd3":"## Train MobileNetV2 - Light Model","76ee417e":"# Data Preparation\n","464ec23f":"## Setup Image Generator"}}