{"cell_type":{"0bd9b64c":"code","50719e03":"code","639aed0e":"code","ead695dc":"code","ed77f9e2":"code","4a130f4c":"code","e586a6e6":"code","291d4a71":"code","2c1d3437":"code","117e93b7":"code","8487c5f4":"code","7ada25cd":"code","8dbccc11":"code","d5c7920c":"code","6ee7e729":"code","6be4cdb1":"code","be5a9454":"code","916de7d5":"code","7a789976":"code","9d89e5e3":"code","affaaae0":"code","15ef7b96":"code","40f119ea":"code","6276663e":"code","a4bf7129":"code","199c5a9c":"code","fcfcfeaf":"code","1702d660":"code","957c8875":"code","1182e513":"code","94946c1f":"code","e7c28e9e":"code","ad39fcf6":"code","bd6ae311":"code","76f97ed9":"code","bb4a125d":"code","aaac0dfd":"code","c381654b":"code","6fbda0f9":"code","43a8111e":"code","1a8dcaad":"code","7f97fe9d":"code","26f657df":"code","080f1110":"code","2c385806":"code","272b263a":"code","423be171":"code","ac88cfc6":"code","730dbcbe":"code","269439a7":"code","8e02bb7f":"code","75587cf6":"code","fb6c5686":"code","bbfc6462":"code","7ff913ec":"code","1ad42ded":"code","971a9e1d":"code","afe53392":"code","3382abce":"code","8b0eca63":"code","abd356b1":"code","242f1730":"code","f0831979":"code","ffe325f2":"code","b5a7799f":"code","f15f19d3":"code","bb4f8f45":"code","cf84db0f":"code","fa509b8e":"code","c0748337":"code","dd58075f":"code","39297fed":"markdown","4e8e6057":"markdown","4b6de02f":"markdown","aac976c7":"markdown","13f160f5":"markdown","f7707e38":"markdown","d24d3d61":"markdown"},"source":{"0bd9b64c":"import numpy as np \nimport pandas as pd \nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","50719e03":"!pip install pywaffle\n!pip install pycomp\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport shap\nimport xgboost\n\nfrom pywaffle import Waffle\nfrom pycomp.viz.insights import *\nfrom cuml.metrics import accuracy_score,roc_auc_score as ras\nfrom cuml.preprocessing.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import roc_auc_score,roc_curve,auc\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, precision_recall_curve\nfrom scipy import interp\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score\nfrom sklearn.linear_model import SGDClassifier as SGDC\nfrom sklearn.svm import SVC\nimport warnings\nwarnings.filterwarnings(\"ignore\")","639aed0e":"!pip install dask-cuda","ead695dc":"import sys\n!cp ..\/input\/rapids\/rapids.0.17.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","ed77f9e2":"import cudf\nimport cuml\n\nfrom cuml.preprocessing.TargetEncoder import TargetEncoder\nfrom dask.distributed import Client, wait\nfrom dask_cuda import LocalCUDACluster\n\ncluster = LocalCUDACluster()\nclient = Client(cluster)\nclient","4a130f4c":"!nvidia-smi","e586a6e6":"!nvcc --version","291d4a71":"train_df =  pd.read_csv(\"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\")\ntest_df = pd.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\")\ndata_dictionary = pd.read_csv(\"..\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\")","2c1d3437":"train_df = train_df.drop(columns=['Unnamed: 0'])\ntest_df = test_df.drop(columns=['Unnamed: 0'])","117e93b7":"def age_point_formula(x):\n    if x<=44:\n        return 0\n    if x in range(45,55):\n        return 2\n    if x in range(55,65):\n        return 3\n    if x in range(65,75):\n        return 4\n    return 6\ntrain_df['age_point'] = train_df['age'].apply(lambda x:age_point_formula(x))\ntest_df['age_point'] = test_df['age'].apply(lambda x:age_point_formula(x))","8487c5f4":"def temp_point(x):\n    if x>=39 and x<=40.9:\n        return 4\n    if x>=38.5 and x<=38.9:\n        return 1\n    if x>=36 and x<=38.4:\n        return 0\n    if x>=34 and x<=35.9:\n        return 1\n    if x>=32 and x<=33.9:\n        return 2\n    if x>=30 and x<=31.9:\n        return 3\n    if x<=29.9 and x>=41:\n        return 4\n    return\ndef generate_temp_point(train_df):\n    index_temp_point_nan = train_df['d1_temp_min'].isnull()\n    train_df[index_temp_point_nan]['d1_temp_min'] = train_df[index_temp_point_nan]['d1_temp_max']\n    index_temp_point_nan = train_df['d1_temp_max'].isnull()\n    train_df[index_temp_point_nan]['d1_temp_max'] = train_df[index_temp_point_nan]['d1_temp_min']\n    train_df['temp_point'] = (train_df['d1_temp_min'] + train_df['d1_temp_max'])\/2\n    train_df['temp_point'].apply(lambda x:temp_point(x))\n    return train_df\ntrain_df = generate_temp_point(train_df)\ntest_df = generate_temp_point(test_df)","7ada25cd":"train_df = cudf.DataFrame.from_pandas(train_df)\ntest_df = cudf.DataFrame.from_pandas(test_df)","8dbccc11":"y_df = train_df['diabetes_mellitus']\ntrain_df = train_df.drop(columns='diabetes_mellitus')","d5c7920c":"train_df = train_df.to_pandas()\ntest_df  = test_df.to_pandas()","6ee7e729":"# drop >0.8 missing \nmissing_greater_than_80 = train_df.columns[train_df.isnull().sum() * 100 \/ len(train_df)>80]\ntrain_df = train_df.drop(columns = missing_greater_than_80)\ntest_df  = test_df.drop(columns = missing_greater_than_80)","6be4cdb1":"dt_i=[]\ndt_fl=[]\ndt_o=[]\nfor col in train_df.columns:\n    x=train_df[col].dtype\n    if x=='int64':\n        dt_i.append(col)\n    elif x=='float64':\n        dt_fl.append(col)\n    else:\n        dt_o.append(col)","be5a9454":"train_df[dt_o] = train_df[dt_o].fillna(\"\")\ntest_df[dt_o] = test_df[dt_o].fillna(\"\")","916de7d5":"def BSA_IBW_AJBW(train_df):\n    train_df['BSA'] = np.sqrt(train_df['height']*train_df['weight']\/3600)\n    train_df['IBW'] = 45.5 + 2.3 * train_df['height']\n    index_m = train_df['gender'] == 'M'\n    train_df[index_m]['IBW'] = train_df[index_m]['IBW'] + 4.5 \n    train_df['AjBW'] = train_df['IBW'] + 0.4 * (train_df['weight']-train_df['IBW'])\n    train_df['eCrCL'] = (140-train_df['age'])*train_df['weight']\n    return train_df\ntrain_df = BSA_IBW_AJBW(train_df)\ntest_df = BSA_IBW_AJBW(test_df)","7a789976":"%%time\nSMOOTH = 0.001\nSPLIT = 'interleaved'\nFOLDS = 5\n\nencoder = TargetEncoder(n_folds=FOLDS, smooth=SMOOTH, split_method=SPLIT)\nID_col = ['encounter_id','hospital_id','icu_id']\ntrain_df = train_df.drop(columns=ID_col)\ntest_df = test_df.drop(columns=ID_col)\ntrain_df = cudf.DataFrame.from_pandas(train_df)\ntest_df = cudf.DataFrame.from_pandas(test_df)\nX_train, X_test, y_train, y_test = train_test_split(train_df, y_df, test_size=0.1,shuffle=False, stratify='y')\nfor col in dt_o:\n    X_train[col] = encoder.fit_transform(X_train[col],y_train)\n    X_test[col] = encoder.transform(X_test[col])\n    test_df[col] = encoder.transform(test_df[col])","9d89e5e3":"def fill_index(train_df, group, first_column, sencod_column):\n    index_max = train_df[group[first_column]].isnull()\n    train_df[index_max][group[first_column]] = train_df[index_max][group[sencod_column]]\n    train_df.loc[index_max,group[first_column]] = train_df.loc[index_max,group[sencod_column]]\n    return train_df","affaaae0":"def fill_nan_by_method(train_df, group, main_column, name_method1, name_method2):\n    \n    index_nan = train_df[group[main_column]].isnull()\n    train_df[index_nan][group[main_column]] = train_df[index_nan][group[name_method2]]\n    \n    index_nan = train_df[group[main_column]].isnull()\n    train_df[index_nan][group[main_column]] = train_df[index_nan][group[name_method1]]\n\n    return train_df","15ef7b96":"def fill_nan_max_min(train_df, group, main_columns, name_method1, name_method2):\n    \n    index_nan =  train_df[train_df[group[name_method1]].isnull() & train_df[group[name_method2]].isnull()].index\n    ## assign max or min to both method\n    train_df.iloc[index_nan][group[name_method1]]= train_df.iloc[index_nan][group[main_columns]]\n    train_df.iloc[index_nan][group[name_method2]]= train_df.iloc[index_nan][group[main_columns]]\n    ## check nan\n    index_nan =  train_df[train_df[group[name_method1]].isnull() & train_df[group[name_method2]].isnull()].index\n    ## assign column both methods by mean columns. \n    train_df.iloc[index_nan][[group[name_method1],group[name_method2]]] = \\\n    train_df.iloc[index_nan][[group[name_method1],group[name_method2]]].fillna(train_df[[group[name_method1],group[name_method2]]].mean())\n    train_df.iloc[index_nan][group[main_columns]] = train_df.iloc[index_nan][group[name_method2]]\n    return train_df","40f119ea":"# find columns both method && ...\nmulti_method = []\nfor column in train_df.columns:\n    name = column.split('_')\n    if len(name)==4 and name[0] in ['d1','h1'] and name[3] in ['max','min']:\n        multi_method.append(name[1])\nmulti_method = list(set(np.unique(multi_method)) - set(['arterial']))\nmulti_method_group_name = []\nfor method in multi_method:\n    group = []\n    for column in train_df.columns:\n        if method in column.split('_'):\n            group.append(column)\n    multi_method_group_name.append(group)","6276663e":"multi_method_group_name","a4bf7129":"def fill_4_var(train_df, column1, column2, column3, column4):\n    index_nan_first = train_df[group[column1]].isnull()\n    index_nan_second = train_df[group[column2]].isnull()\n    train_df[index_nan_first][group[column1]] = train_df[index_nan_first][group[column3]]\n    train_df[index_nan_second][group[column2]] = train_df[index_nan_second][group[column4]]\n    return train_df\ndef fill_noninvasive(train_df,group):\n    fill_4_var(train_df,0,1,2,3)\n    fill_4_var(train_df,3,2,0,1)\n    return train_df","199c5a9c":"\ndef fill_mising_2_method(train_df):\n    \n    for group in multi_method_group_name:\n        train_df = fill_nan_by_method(train_df,group,2,0,4)\n        train_df = fill_nan_by_method(train_df,group,3,1,5)\n        train_df = fill_nan_max_min_by_mean = fill_nan_max_min(train_df,group,2,0,4)\n        train_df = fill_nan_max_min_by_mean = fill_nan_max_min(train_df,group,3,1,5)\n        train_df = fill_index(train_df,group,0,4)\n        train_df = fill_index(train_df,group,4,0)\n        train_df = fill_index(train_df,group,1,5)\n        train_df = fill_index(train_df,group,5,1)\n        if len(group)<=10:\n            train_df = fill_noninvasive(train_df,group)\n        else:\n            train_df = fill_nan_by_method(train_df,group,8,6,10)\n            train_df = fill_nan_by_method(train_df,group,9,7,11)\n            train_df = fill_nan_max_min_by_mean = fill_nan_max_min(train_df,group,8,6,10)\n            train_df = fill_nan_max_min_by_mean = fill_nan_max_min(train_df,group,9,7,11)\n            train_df = fill_index(train_df,group,6,10)\n            train_df = fill_index(train_df,group,10,6)\n            train_df = fill_index(train_df,group,7,11)\n            train_df = fill_index(train_df,group,11,7)\n        \n    return train_df","fcfcfeaf":"train_df = fill_mising_2_method(train_df)\ntest_df = fill_mising_2_method(test_df)","1702d660":"def fill_metadata(train_df):\n    train_df[['weight','height','bmi']]  = train_df[['weight','height','bmi']].astype('float64')\n    index_nan_bmi = train_df['bmi'].isnull()\n    train_df[index_nan_bmi]['bmi'] = train_df[index_nan_bmi]['height']\/train_df[index_nan_bmi]['weight']**2\n    index_nan_weight = train_df['weight'].isnull()\n    train_df[index_nan_weight]['weight'] = np.sqrt(train_df[index_nan_weight]['height']\/train_df[index_nan_weight]['bmi'])\n    index_nan_height = train_df['height'].isnull()\n    train_df[index_nan_height]['height'] = train_df[index_nan_height]['bmi']*train_df[index_nan_height]['weight']**2\n    train_df[index_nan_weight]['weight'] = train_df['weight'].mean()\n    train_df[index_nan_height]['height'] = train_df['height'].mean()\n    index_nan_bmi = train_df['bmi'].isnull()\n    train_df[index_nan_bmi]['bmi'] = train_df[index_nan_bmi]['height']\/train_df[index_nan_bmi]['weight']**2\n    return train_df","957c8875":"train_df = fill_metadata(train_df)\ntest_df = fill_metadata(test_df)","1182e513":"# import numpy as np\n# from collections import Counter\n\n\n# def detect_outliers(df, n, features):\n#     \"\"\"\n#     Takes a dataframe df of features and returns a list of the indices\n#     corresponding to the observations containing more than n outliers according\n#     to the Tukey method.\n#     \"\"\"\n# #     outlier_indices = []\n\n#     # iterate over features(columns)\n#     for col in features:\n#         # 1st quartile (25%)\n#         Q1 = np.percentile(df[col], 25)\n#         # 3rd quartile (75%)\n#         Q3 = np.percentile(df[col], 75)\n#         # Interquartile range (IQR)\n#         IQR = Q3 - Q1\n\n#         # outlier step\n#         outlier_step = 1.5 * IQR\n\n#         # Determine a list of indices of outliers for feature col\n#         df[(df[col] < Q1 - outlier_step)] = Q1 - outlier_step\n#         df[(df[col] > Q3 + outlier_step)] = Q3 + outlier_step\n\n#         # append the found outlier indices for col to the list of outlier indices \n# #         outlier_indices.extend(outlier_list_col)\n\n#     # select observations containing more than 2 outliers\n# #     outlier_indices = Counter(outlier_indices)        \n# #     multiple_outliers = list(k for k, v in outlier_indices.items() if v > n)\n#     return df","94946c1f":"# Outliers_to_drop","e7c28e9e":"# train_df[dt_o] = train_df[dt_o].fillna(value='')\n# test_df[dt_o] = test_df[dt_o].fillna(value='')","ad39fcf6":"%%time\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")\nimputer.fit(X_train.to_pandas().values)\nX_train =  pd.DataFrame(imputer.transform(X_train.to_pandas().values),columns=X_train.columns)\nX_test  = pd.DataFrame(imputer.transform(X_test.to_pandas().values),columns=X_test.columns)\ntest_df = pd.DataFrame(imputer.transform(test_df.to_pandas().values), columns=test_df.columns)","bd6ae311":"Xtrain_pd = X_train\nytrain_pd = y_train.to_pandas()\nXtest_pd = X_test\nytest_pd = y_test.to_pandas()\nX_c = pd.concat([Xtrain_pd, Xtest_pd])\ny_c = pd.concat([ytrain_pd, ytest_pd])\ntrain_df = X_c\ny_df = y_c","76f97ed9":"def sum_test(train_df):\n    test_name = []\n    for col in train_df.columns:\n        name = col.split(\"_\")\n        if name[0] == 'apache':\n            test_name.append(name[0])\n            continue\n        if len(name) >2:\n            test_name.append(name[1])\n\n    test_name = np.unique(test_name)\n    for name in test_name:\n        group = []\n        for col in train_df.columns:\n            if name in col.split('_'):\n                group.append(col)\n        train_df['test_counting_'+name] = len(group) - train_df[group].isnull().sum(axis=1)\n        train_df['test_counting_'+name][train_df['test_counting_'+name]<=2]=0\n        train_df['test_counting_'+name][train_df['test_counting_'+name]>2]=1\n    train_df['sum_test_counting'] = np.zeros(train_df.shape[0])\n    for name in test_name:\n        train_df['sum_test_counting'] += train_df['test_counting_'+name]\n        train_df.drop(columns = 'test_counting_'+name, inplace=True )\n    return train_df\ntrain_df = sum_test(train_df)\ntest_df = sum_test(test_df)","bb4a125d":"def generate_diff_delta(train_df):\n    test_name = []\n    for col in train_df.columns:\n        name = col.split(\"_\")\n        if name[-1] in ['max','min']:\n            if len(name) == 3:\n                test_name.append(name[1])\n            else:\n                test_name.append(name[1]+\"_\"+name[2])\n    for col in test_name:\n        col_h1_max = \"h1_\"+col+\"_max\"\n        col_h1_min = \"h1_\"+col+\"_min\"\n        col_d1_max = \"d1_\"+col+\"_max\"\n        col_d1_min = \"d1_\"+col+\"_min\"\n        if col_d1_max in train_df.columns and col_h1_max in train_df.columns:\n            train_df['delta_d1_h1_max_'+col] = train_df[col_d1_max] -  train_df[col_h1_max]\n            train_df['diff_d1_h1_max_'+col] = train_df[col_d1_max] \/  train_df[col_h1_max]\n        if col_h1_min in train_df.columns and col_d1_min in train_df.columns:\n            train_df['delta_d1_h1_min_'+col] = train_df[col_d1_min] -  train_df[col_h1_min]\n            train_df['diff_d1_h1_min_'+col] = train_df[col_d1_min] \/  train_df[col_h1_min]\n    return train_df\ntrain_df = generate_diff_delta(train_df)\ntest_df = generate_diff_delta(test_df)","aaac0dfd":"def generate_gcs(train_df):\n    feature_gcs = []\n    for col in train_df.columns:\n        if col.split('_')[0] == 'gcs':\n            feature_gcs.append(col)\n    feature_gcs\n    feature_gcs = list(set(feature_gcs)-set(['gcs_unable_apache']))\n    train_df[\"gcs_score\"] = train_df[feature_gcs].sum(axis=1) \n    return train_df\ntrain_df = generate_gcs(train_df)\ntest_df = generate_gcs(test_df)","c381654b":"def delta_pulse(train_df):\n    columns = train_df.columns\n    for type_ in ['h1','d1']:\n        d_i_max = type_+\"_diasbp_invasive_max\"\n        d_i_min = type_+\"_diasbp_invasive_min\"\n        d_n_max = type_+\"_diasbp_noninvasive_max\"\n        d_n_min = type_+\"_diasbp_noninvasive_min\"\n\n        s_i_max = type_+\"_sysbp_invasive_max\"\n        s_i_min = type_+\"_sysbp_invasive_min\"\n        s_n_max = type_+\"_sysbp_noninvasive_max\"\n        s_n_min = type_+\"_sysbp_noninvasive_min\"\n        if s_i_max in columns and d_i_max in columns:\n            train_df['i_delta_pulse_max'] = train_df[s_i_max]-train_df[d_i_max]\n        if s_i_min in columns and d_i_min in columns:\n            train_df['i_delta_pulse_min'] = train_df[s_i_min]-train_df[d_i_min]\n        if s_n_max in columns and d_n_max in columns:\n            train_df['n_delta_pulse_max'] = train_df[s_n_max]-train_df[d_n_max]\n        if s_n_min in columns and d_n_min in columns:\n            train_df['n_delta_pulse_min'] = train_df[s_n_min]-train_df[d_n_min]\n    return train_df\ntrain_df = delta_pulse(train_df)\ntest_df = delta_pulse(test_df)","6fbda0f9":"def abmi_apache_agi(train_df):\n    train_df['abmi'] = train_df['age']\/train_df['bmi']\n#     df['apache_4a_hospicu_death_prob'] = df['apache_4a_hospital_death_prob'] \/ df['apache_4a_icu_death_prob']\n    train_df['agi'] = train_df['weight']\/train_df['age']\n    return train_df\ntrain_df = abmi_apache_agi(train_df)\ntest_df = abmi_apache_agi(test_df)","43a8111e":"for col in train_df.columns:\n    arr = train_df[col].values\n    train_df[col] = train_df[col].replace([np.inf, -np.inf,np.nan,-np.nan], np.mean(arr[np.isfinite(arr)]))\nfor col in test_df.columns:\n    arr = test_df[col].values\n    test_df[col] = test_df[col].replace([np.inf, -np.inf, np.nan,-np.nan], np.mean(arr[np.isfinite(arr)]))","1a8dcaad":"scaler = StandardScaler()\nscaler.fit(train_df.values)\ntrain_df = pd.DataFrame(scaler.transform(train_df.values),columns = train_df.columns)\ntest_df = pd.DataFrame(scaler.transform(test_df.values),columns = train_df.columns)","7f97fe9d":"from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\nestimator = RandomForestClassifier(max_depth=5, random_state=42)\nselector = RFE(estimator, n_features_to_select=5, step=1)\nselector = selector.fit(train_df, y_df.values)","26f657df":"feature_importance = dict(zip(train_df.columns,selector.ranking_))","080f1110":"delete_feature = []\ncolumns = train_df.columns\nfor first, column_1 in enumerate(columns):\n    for second in range(first+1,len(columns)):\n        column_2 = columns[second]\n        correlation = train_df[column_1].corr(train_df[column_2])\n        if correlation>.99:\n            if feature_importance[column_1]>feature_importance[column_2]:\n                delete_feature.append(column_1)\n            else:\n                delete_feature.append(column_2)","2c385806":"print(train_df.shape)\ntrain_df.drop(columns = delete_feature, inplace =True)\ntest_df.drop(columns = delete_feature, inplace =True)\nprint(train_df.shape)","272b263a":"def evalate_model(fold, y_pred, y_truth):\n    acc_ = accuracy_score(y_pred, y_truth)\n    auc  = ras(y_pred, y_truth)\n    return \"fold {} acc {} auc {}\".format(fold, acc_, auc)","423be171":"def multiple_model_sklearn()","ac88cfc6":"%%time\nplt.rcParams['figure.figsize']=(10,8)\ntprs = []\naucs = []\nroc_aucs = []\nmean_tpr = []\nmean_fpr = []\n\nX_c = train_df\ny_c = y_df\n\nX_c = pd.DataFrame(np.nan_to_num(X_c),columns = X_c.columns)\ntest_df = pd.DataFrame(np.nan_to_num(test_df),columns=test_df.columns)\noof = np.zeros(len(X_c))\nfinal_predict_test = np.zeros(len(test_df))\npred_test_xgb_ = np.zeros(len(test_df))\npred_test_lgbm_ = np.zeros(len(test_df))\npred_test_catb_= np.zeros(len(test_df))\nmean_fpr = np.linspace(0,1,100)\n\ni = 1\nNFOLD = 10\nfolds = StratifiedKFold(n_splits=NFOLD, shuffle=True, random_state=2021)\n\n## setting model 1 ###\n\nnum_round=7000\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_c,y_c)):\n    print('-- Fold:', fold_,'--' )\n    ### fine tuning parameter\n    weighted = np.unique(y_c[trn_idx],return_counts=True)[1]\n    t_0 = weighted[1]\/weighted[0]\n    params1 = {'tree_method' : 'gpu_hist','max_depth' : 7,'max_leaves' : 15,\\\n               'objective' : 'binary:logistic','grow_policy' : 'lossguide',\\\n               'predictor':'gpu_predictor','eta' : 0.01,\\\n               'eval_metric':'auc','scale_pos_weight':50}\n    model2  = LGBMClassifier(**{'learning_rate': 0.05,\n                'max_depth': 5,\n                'reg_alpha': 1,\n                'num_leaves':64,\n                'reg_lambda': 3,\n                'objective': 'binary',\n                'metric': 'auc',\n                'n_jobs': -1,\n                'n_estimators' : 10000,\n                'feature_fraction_seed': 42,\n                'bagging_seed': 42,\n                'boosting_type': 'gbdt',\n#                 'is_unbalance': True,\n#                 'scale_pos_weight':50,\n                'class_weight':{0:t_0,1:1\/t_0},\n                'boost_from_average': False})\n#     model2 = LGBMClassifier(**{'learning_rate': 0.05,\n#                     'max_depth': 3,\n#                     'reg_alpha': 1,\n#                     'reg_lambda': 1,\n#                     'objective': 'binary',\n#                     'metric': 'auc',\n#                     'n_jobs': -1,\n#                     'n_estimators' : 5000,\n#                     'feature_fraction_seed': 42,\n#                     'bagging_seed': 42,\n#                     'boosting_type': 'gbdt',\n#                     'is_unbalance': True,\n#                     'sigmoid':True,\n#                     'min_child_weight':0,\n#                     'boost_from_average': False,\n#                     'scale_pos_weight':99,\n#                     })\n#                     'class_weight':{0:t_0,1:1\/t_0}})\n    ### pre process\n    scaler = StandardScaler()\n    scaler.fit(X_c.iloc[trn_idx].values)\n    train_set = scaler.transform(X_c.iloc[trn_idx])\n    val_set = scaler.transform(X_c.iloc[val_idx])\n    test_set = scaler.transform(test_df.values)\n#     train_set = X_c.iloc[trn_idx]\n#     val_set   = X_c.iloc[val_idx]\n#     test_set  = test_df.values\n#     pca_ = PCA(n_components=125).fit(train_, y_c[trn_idx])\n#     train_set = pca_.transform(train_set)\n#     val_set   = pca_.transform(val_set)\n#     test_set  = pca_.transform(test_set)\n    ### xgb\n    dtrain = xgboost.DMatrix(train_set,y_c.iloc[trn_idx])\n    dval   = xgboost.DMatrix(val_set, y_c.iloc[val_idx])\n    dtest  = xgboost.DMatrix(test_set)\n    evallist = [(dval, 'validation'), (dtrain, 'train')]\n    model1 = xgboost.train(params1, dtrain, num_round, evallist,verbose_eval =0,early_stopping_rounds=50)\n    pred_train_xgb = model1.predict(dtrain)\n    pred_val_xgb = model1.predict(dval)\n    pred_test_xgb = model1.predict(dtest)\n    pred_test_xgb_ +=pred_test_xgb\/NFOLD\n    print(evalate_model(fold_, y_c.iloc[val_idx], pred_val_xgb))\n    ### lgbm\n    model2 = model2.fit(train_set, y_c.iloc[trn_idx],eval_set=[(train_set,y_c.iloc[trn_idx]),(val_set, y_c.iloc[val_idx])],eval_metric = 'auc',early_stopping_rounds = 50,verbose=0)\n    #,categorical_feature = dt_o)\n    pred_test_lgbm = model2.predict_proba(test_set)[:,1]\n    pred_test_lgbm_+=pred_test_lgbm\/NFOLD\n    oof[val_idx] = model2.predict_proba(val_set)[:,1]\n    oof[trn_idx] = model2.predict_proba(train_set)[:,1]\n    print(evalate_model(fold_, y_c[val_idx], oof[val_idx]))\n    ### catboost\n    from catboost import CatBoostClassifier,Pool,cv\n    train_data=Pool(\n            data=train_set,\n            label=y_c.iloc[trn_idx],\n    )\n    cat=CatBoostClassifier(\n        task_type = \"GPU\",\n        iterations=7000,\n        max_depth=6,\n        random_strength=2.938692773281005,\n        od_type='IncToDec',\n        custom_metric=['AUC','Accuracy'],\n        verbose=False,\n        class_weights={0:t_0,1:10*(1-t_0)}\n    )\n    cat.fit(train_data,\n            eval_set=(val_set,y_c.iloc[val_idx]),\n            use_best_model=True,\n            early_stopping_rounds=200)\n    print(cat.get_best_score())\n    pred_test_catb = cat.predict_proba(test_df)[:,1]\n    pred_test_catb_ +=pred_test_xgb\/NFOLD\n    ### emsemble model ###\n#     emsemble = SGDRegressor(max_iter=10000, tol=1e-3,early_stopping=True, validation_fraction = 0.2, \\\n#                     penalty='elasticnet')#,class_weight={0:t_0,1:10*(1-t_0)})\n    emsemble = SGDC(max_iter=100000, tol=1e-3,early_stopping=True, validation_fraction = 0.1,\\\n                     class_weight={0:t_0,1:10*(1-t_0)})\n    trn_feature = np.hstack([train_set, np.vstack([pred_train_xgb, oof[trn_idx]]).T])\n    val_fature  = np.hstack([val_set,np.vstack([pred_val_xgb, oof[val_idx]]).T])\n    test_feature = np.hstack([test_df,np.vstack([pred_test_xgb, pred_test_lgbm]).T])\n    scaler = StandardScaler()\n    scaler.fit(trn_feature)\n    trn_feature = scaler.transform(trn_feature)\n    val_fature = scaler.transform(val_fature)\n    test_feature = scaler.transform(test_feature)\n    final_columns = np.append(train_df.columns,np.array(['pred1','pred2'])) \n    trn_feature = pd.DataFrame(trn_feature,columns = final_columns)\n    val_fature = pd.DataFrame(val_fature,columns = final_columns)\n    test_feature = pd.DataFrame(test_feature,columns = final_columns)\n    emsemble.fit(trn_feature, y_c[trn_idx])\n    emsemble_pred = emsemble.predict(val_fature)\n    final_predict_test += emsemble.predict(test_feature)\/NFOLD\n    print(evalate_model(fold_, y_c.iloc[val_idx], emsemble_pred))","730dbcbe":"cd kaggle\/working","269439a7":"!python setup.py install","8e02bb7f":"from sklearn.ensemble import RandomForestClassifier","75587cf6":"# Level 1 are the base models that take the training dataset as input\nseed = 42\nl1_clf1 = GradientBoostingClassifier(n_estimators=5000,\n                                    learning_rate=0.006,\n                                    min_samples_leaf=10,\n                                    max_depth=15, \n                                    max_features='sqrt', \n                                    subsample=0.85,\n                                    random_state=seed)\n\nl1_clf2 = LGBMClassifier(boosting_type='gbdt',\n                        objective=\"binary\",\n                        metric=\"AUC\",\n                        boost_from_average=\"false\",\n                        learning_rate=0.0045,\n                        num_leaves=491,\n                        max_depth=20,\n                        min_child_weight=0.035,\n                        feature_fraction=0.38,\n                        bagging_fraction=0.42,\n                        min_data_in_leaf=100,\n                        max_bin=255,\n                        importance_type='split',\n                        reg_alpha=0.4,\n                        reg_lambda=0.65,\n                        bagging_seed=seed,\n                        random_state=seed,\n                        verbosity=-1,\n                        subsample=0.85,\n                        colsample_bytree=0.8,\n                        min_child_samples=79)\n\nl1_clf3 = CatBoostClassifier(learning_rate=0.2,\n                            bagging_temperature=0.1, \n                            l2_leaf_reg=30,\n                            depth=12, \n                            max_bin=255,\n                            iterations=100,\n                            loss_function='Logloss',\n                            objective='RMSE',\n                            eval_metric=\"AUC\",\n                            bootstrap_type='Bayesian',\n                            random_seed=seed,\n                            early_stopping_rounds=10)\n\n# Level 2 models will take predictions from level 1 models as input\n# Remember to keep level 2 models smaller\n# Basic models like Ridge Regression with large regularization or small random forests work well\nl2_clf1 = RandomForestClassifier(n_estimators=250, \n                                max_depth=5, \n                                max_features='sqrt', \n                                random_state=seed,\n                                )","fb6c5686":"!cp -r ..\/input\/pystacknet pystacknet ","bbfc6462":"cd pystacknet\/h2oai-pystacknet-af571e0","7ff913ec":"ls","1ad42ded":"ls","971a9e1d":"import pystacknet\n# sys.path.append(\".\/pystacknet\/h2oai-pystacknet-af571e0\")\nfrom pystacknet.pystacknet import StackNetClassifier\n\n# Specify parameters for stacked model and begin training\nmodel = StackNetClassifier(models, \n                           metric=\"auc\", \n                           folds=3,\n                           restacking=False,\n                           use_retraining=True,\n                           use_proba=True, # To use predict_proba after training\n                           random_state=seed,\n                           n_jobs=-1, \n                           verbose=1)","afe53392":"!pip install --upgrade scikit-learn","3382abce":"# model2  = LGBMClassifier(**{'learning_rate': 0.05,\n#                 'max_depth': 5,\n#                 'reg_alpha': 1,\n#                 'num_leaves':64,\n#                 'reg_lambda': 3,\n#                 'objective': 'binary',\n#                 'metric': 'auc',\n#                 'n_jobs': -1,\n#                 'n_estimators' : 10000,\n#                 'feature_fraction_seed': 42,\n#                 'bagging_seed': 42,\n#                 'boosting_type': 'gbdt',\n# #                 'is_unbalance': True,\n# #                 'scale_pos_weight':50,\n#                 'class_weight':{0:t_0,1:10\/t_0},\n#                 'boost_from_average': False})\n","8b0eca63":"# model2 = model2.fit(train_set, y_c.iloc[trn_idx],eval_set=[(train_set,y_c.iloc[trn_idx]),(val_set, y_c.iloc[val_idx])],eval_metric = 'auc',early_stopping_rounds = 50,verbose=0)\n# #,categorical_feature = dt_o)\n# pred_test_lgbm = model2.predict_proba(test_set)[:,1]\n# pred_test_lgbm_+=pred_test_lgbm\/NFOLD\n# oof[val_idx] = model2.predict_proba(val_set)[:,1]\n# oof[trn_idx] = model2.predict_proba(train_set)[:,1]\n# print(evalate_model(fold_, y_c[val_idx], oof[val_idx]))","abd356b1":"# emsemble = SGDC(max_iter=100000, tol=1e-3,early_stopping=True, validation_fraction = 0.1,\\\n#                      class_weight={0:t_0,1:*(1-t_0)})\n# trn_feature = np.hstack([train_set, np.vstack([pred_train_xgb, oof[trn_idx]]).T])\n# val_fature  = np.hstack([val_set,np.vstack([pred_val_xgb, oof[val_idx]]).T])\n# test_feature = np.hstack([test_df,np.vstack([pred_test_xgb, pred_test_lgbm]).T])\n# scaler = StandardScaler()\n# scaler.fit(trn_feature)\n# trn_feature = scaler.transform(trn_feature)\n# val_fature = scaler.transform(val_fature)\n# test_feature = scaler.transform(test_feature)\n# final_columns = np.append(train_df.columns,np.array(['pred1','pred2'])) \n# trn_feature = pd.DataFrame(trn_feature,columns = final_columns)\n# val_fature = pd.DataFrame(val_fature,columns = final_columns)\n# test_feature = pd.DataFrame(test_feature,columns = final_columns)\n# emsemble.fit(trn_feature, y_c[trn_idx])\n# emsemble_pred = emsemble.predict(val_fature)\n# final_predict_test += emsemble.predict(test_feature)\/NFOLD\n# print(evalate_model(fold_, y_c.iloc[val_idx], emsemble_pred))","242f1730":"# pred_train_xgb = model1.predict(dtrain)\n# pred_val_xgb = model1.predict(dval)","f0831979":"# pred_test_xgb = model1.predict(xgboost.DMatrix(test_df))\n# pred_test_xgb_ +=pred_test_xgb\/NFOLD\n# print(evalate_model(fold_, y_c.iloc[val_idx], pred_val_xgb))","ffe325f2":"# model2 = model2.fit(train_set, y_c.iloc[trn_idx],eval_set=[(train_set,y_c.iloc[trn_idx]),(val_set, y_c.iloc[val_idx])],\\\n#                     eval_metric = 'auc',early_stopping_rounds = 50,verbose=0,num_boost_round=5000,categorical_features = dt_o)\n# pred_test_lgbm = model2.predict_proba(test_set)[:,1]\n# pred_test_lgbm_+=pred_test_lgbm\/NFOLD\n# oof[val_idx] = model2.predict_proba(val_set)[:,1]\n# oof[trn_idx] = model2.predict_proba(train_set)[:,1]\n# print(evalate_model(fold_, y_c[val_idx], oof[val_idx]))","b5a7799f":"from sklearn.svm import SVC\nemsemble = SVC(class_weight={0:t_0,1:10*(1-t_0)})\ntrn_feature = np.hstack([train_set, np.vstack([pred_train_xgb, oof[trn_idx]]).T])\nval_fature  = np.hstack([val_set,np.vstack([pred_val_xgb, oof[val_idx]]).T])\ntest_feature = np.hstack([test_set,np.vstack([pred_test_xgb, pred_test_lgbm]).T])\nemsemble.fit(trn_feature, y_c[trn_idx])\nemsemble_pred = emsemble.predict(val_fature)\nfinal_predict_test += emsemble.predict(test_feature)\/NFOLD\nprint(evalate_model(fold_, y_c.iloc[val_idx], emsemble_pred))","f15f19d3":"avg = (pred_test_lgbm_+pred_test_xgb_)\/2","bb4f8f45":"# blended_preds = 0.7*preds2 + 0.3*preds1","cf84db0f":"final_predict_test","fa509b8e":"encounter_IDs = cudf.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\")[[\"encounter_id\"]].values","c0748337":"df_sub = {'encounter_id': encounter_IDs, 'diabetes_mellitus': pred_test_catb_}\ndf_predictions = cudf.DataFrame(df_sub).set_index(['encounter_id'])\ndf_predictions.head(10)","dd58075f":"df_predictions.to_csv('\/kaggle\/working\/Predictions_blended1.csv')","39297fed":"# Model","4e8e6057":"<div style=\"background-color:#fae1dd; color:black;\">\n    <h3><center>Feature Importance: XGB<\/center><\/h3>\n<\/div>","4b6de02f":"# feature selection","aac976c7":"## craete feature based on object feature","13f160f5":"# s & d","f7707e38":"<div style=\"background-color:#fae1dd; color:black;\">\n    <h3><center>Creating the submission file<\/center><\/h3>\n<\/div>","d24d3d61":"# cast boost algorithm"}}