{"cell_type":{"d5c0e67a":"code","1523f135":"code","424cba85":"code","3c1429cc":"code","e7ff141c":"code","16591ba9":"code","4c0c83ea":"code","27b68ef8":"code","ecd7bd3e":"code","c5ed6ce6":"code","ec96bee1":"code","5de9d128":"code","dc35fc95":"code","2c5e9e45":"code","39a68af7":"code","9a76d8f9":"markdown","d1933c19":"markdown","dbee3e40":"markdown","51077f03":"markdown","18f4b3ee":"markdown","dd7d6557":"markdown","2828f6bc":"markdown"},"source":{"d5c0e67a":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\n\n# clustering functions\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.cluster import hierarchy\n\n# SARIMA model and autocorrelation plots\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# SARIMA model throws lots of MLE convergence errors\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1523f135":"# read meta and demand data from csv into new dataframes, then fix datetimes\nmeta = pd.read_csv('..\/input\/meta_open.csv',index_col=0)\nmeta['datastart'] = pd.to_datetime(meta['datastart'],format='%d\/%m\/%y %H:00')\nmeta['dataend'] = pd.to_datetime(meta['dataend'],format='%d\/%m\/%y %H:00')\n\nraw = pd.read_csv('..\/input\/temp_open_utc_complete.csv',index_col=0)\nraw.index = pd.to_datetime(raw.index,format='%Y-%m-%d %H:00:00+00:00')","424cba85":"# find buildings that have data starting on Jan 1\ngood_buildings = []\ngood_starts = ['2015-01-01 00:00:00']\n\nfor building in meta.index:\n    start = str(meta['datastart'][building])\n    if start in good_starts:\n        good_buildings.append(building)\n\n# number of buildings that have complete data starting at Jan 1\nlen(good_buildings)","3c1429cc":"# remove leap day for simplicity\nno_leap_days = raw.drop(pd.date_range(start='2012-02-29 00:00:00',periods=24,freq='H'))\n\n# initialize a new dataframe with all demand values matched up\ndemand = pd.DataFrame()\n\nfor building in good_buildings:\n    demand[building] = no_leap_days[building].dropna().values\n\ndemand.index = pd.date_range(start='2015-01-01 00:00:00',periods=8760,freq='H')","e7ff141c":"# identify offices and dorms\noffice_names = [k for k in demand.columns if 'Office' in k]\ndorm_names = [k for k in demand.columns if 'Dorm' in k]\nlab_names = [k for k in demand.columns if 'Lab' in k]\nclassroom_names = [k for k in demand.columns if 'Class' in k]\n\n# create dataframes of each building type\noffices = pd.Series(demand[office_names].sum(axis=1))\ndorms = pd.Series(demand[dorm_names].sum(axis=1))\nlabs = pd.Series(demand[lab_names].sum(axis=1))\nclassrooms = pd.Series(demand[classroom_names].sum(axis=1))\n\ndorms.plot(figsize=(15,5))","16591ba9":"# calculate the average day-of-week and hour-of-day profiles, then apply sklearn minmaxscaler\nscaler = MinMaxScaler()\n\noffices_all = pd.DataFrame(demand[office_names])\nhour_of_day = offices_all.groupby(offices_all.index.hour).sum()\nday_of_week = offices_all.groupby(offices_all.index.dayofweek).sum()\n\nhour_of_day_norm = pd.DataFrame(scaler.fit_transform(hour_of_day.values),columns=hour_of_day.columns)\nday_of_week_norm = pd.DataFrame(scaler.fit_transform(day_of_week.values),columns=day_of_week.columns)","4c0c83ea":"linked = hierarchy.linkage(hour_of_day_norm.values.T,'single')\nlabels = hour_of_day_norm.columns\n\nplt.figure(figsize=(18, 5))\nhierarchy.dendrogram(linked,orientation='top',labels=labels,distance_sort='descending',)\nplt.xticks(rotation=30)\nplt.title('Dendrogram of Hour-of-Day Electricity Demand Profiles')\nplt.show()","27b68ef8":"# repeat the dendrogram for the day-of-week curves\nlinked = hierarchy.linkage(day_of_week_norm.values.T,'single')\nlabels = day_of_week_norm.columns\n\nplt.figure(figsize=(18,5))\nhierarchy.dendrogram(linked,orientation='top',labels=labels,distance_sort='descending')\nplt.xticks(rotation=30)\nplt.title('Dendrogram of Day-of-Week Electricity Demand Profiles')\nplt.show()","ecd7bd3e":"# calculate hour-of-week and week-of-year averages to isolate weekend, holiday median-shifting\nhourofweekarray, hourofweek = range(168),range(72,168) # the year starts on thursday\nwhile len(hourofweek) < 10000: hourofweek = np.append(hourofweek,hourofweekarray)\nhourofweek = hourofweek[:8760]\n\noffice_week = offices.groupby(hourofweek).mean()\noffice_week_transformer = np.vectorize(office_week.to_dict().get)(hourofweek)\n\noffice_year = offices.groupby(offices.index.weekofyear).mean()\noffice_year_transformer = offices.index.weekofyear.map(office_year)\n\n# plot average hour-of-week to demonstrate the transformer\noffice_week.plot(figsize=(15,5),x='Hour of Week',y='kW')","c5ed6ce6":"offices_stationary = offices\/office_week_transformer\/office_year_transformer\noffices_stationary.plot(figsize=(18,5),title='Stationary office demand data')","ec96bee1":"plot_acf(offices_stationary, lags=50)\nplt.show()\nplot_pacf(offices_stationary, lags=50)\nplt.show()","5de9d128":"# model parameters\nnum_days = 300\ntrend_params = (0,0,0)\nseasonal_params = (1,0,1,24)\n\n# rename offices dataframe to easily substitute into this model, which was written for y\ny = offices.copy(deep=True)\ny_stationary = offices_stationary.copy(deep=True)\n\n# construct empty dataframe to populate with results\nmapes = pd.DataFrame(columns=['hours_trained','mape'])\ny_forecast_stationary = y_stationary.copy(deep=True)\n\n# define split times and end times to cut X and y into training and testing sets\nsplit_times = pd.date_range(start='2015-02-01 08:00:00',freq='D',periods=num_days)\nend_times = split_times.shift(39,freq='H')","dc35fc95":"# loop to run models and add hours_trained & mape data to the mapes dataframe\nfor i in range(len(split_times)):\n    \n    # define training and testing sets based on split_times and end_times\n    y_train = y_stationary.loc[y.index <= split_times[i]].copy().values\n    y_test = y_stationary.loc[(y.index > split_times[i]) & (y.index <= end_times[i])].copy()\n\n    # fit the model training data, then make multi-step out-of-sample forecast\n    model = SARIMAX(y_train, order=trend_params,seasonal_order=seasonal_params,enforce_stationarity=False)\n    model_fit = model.fit(disp=0,maxiter=200)\n    forecast = model_fit.forecast(steps=39)\n    \n    # store the forecast results\n    y_pred = forecast\n    y_forecast_stationary.loc[(y.index > split_times[i]) & (y.index <= end_times[i])] = y_pred\n    \n    # calculate and store the mape, then move on to the next day\n    hours_trained = len(y_train)\n    mape = np.mean(np.abs((y_test[15:] - y_pred[15:]) \/ y_test[15:]))*100\n    mapes = mapes.append({'hours_trained':hours_trained,'mape':mape},ignore_index=True)\n\nprint('The average MAPE for {} test days was {}'.format(len(mapes),np.round(np.mean(mapes['mape']),2)))","2c5e9e45":"# reverse the transformations\ny_forecast = y_forecast_stationary*office_year_transformer*office_week_transformer","39a68af7":"# plot all of the day-ahead forecasts against the actual data\nentire_forecast = pd.DataFrame({'actual':y[split_times[0]:end_times[i]],\n                                'forecast':y_forecast[split_times[0]:end_times[i]]}\n                              )\nentire_forecast.plot(figsize=(18,6))","9a76d8f9":"### Predictive Modeling: Seasonal ARIMA\n\nThe SARIMA (seasonal autoregressive integrated moving average) model has four parameters: P,D,Q, and m. P,D, and Q are the *seasonal* autoregressive, differencing, and moving average terms, while m is the period. For the demand data, m should be 24. SARIMA also supports trend terms, but there is no trend in this data, so they will not be used.\n\nThe cells below train and test SARIMA models for several days in of the training set, starting on February 1st. The MAPE of each daily forecast is stored in the mapes DataFrame, and the predictions are stored in the y_forecast Series.","d1933c19":"### Initialization\n\nThe following cells load and prep the data. To simplify the analysis, only 2015 data is included.","dbee3e40":"The following cells implement hierarchical agglomerative clustering on the hour-of-day and day-of-week profiles of the office buildings.","51077f03":"As you can see in the \"stationary\" plot below, the transformations used do not accurately remove the effects of holidays shorter than one week, like the 4th of July, Thanksgiving, Christmas. To improve the model, these could be hard-coded or identified by clustering with weekends over multiple years of data.","18f4b3ee":"### Exploration\n\nPlots of aggregate demand for the office buildings, followed by hour-of-day and day-of-week clustering.","dd7d6557":"### Autocorrelation plots\n\nThe plots below show the autocorrelation and partial autocorrelation of the raw demand data. The shaded red cone shows the 95% confidence interval, meaning all dots outside of the cone are statistically significant. The dots themselves represent lag features, where the first dot is lag by one hour, and the second by two hours.\n\nFollowing the Box-Jenkins methodology, these plots can be used to inform the type of ARIMA model to be used. https:\/\/en.wikipedia.org\/wiki\/Box%E2%80%93Jenkins_method\n\nSee how the 24th and 48th lag features have a strong positive correlation - this indicates that a seasonal component of 24 hours should be modeled.","2828f6bc":"### Transformation of median-shifting features\n\nMany predictive models require stationary data. Electricity demand data is never stationary, because it includes seasonal (daily, weekly, yearly) effects as well as trend (steady\/exponetial increase) effects.\n\nThe following cells make the data stationary by calculating and then removing the hour-of-week (weekend vs weekday) and week-of-year (spring break vs class in session) effects."}}