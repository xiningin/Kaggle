{"cell_type":{"60c4d639":"code","48f69c1a":"code","f1023d16":"code","60a516f0":"code","bda47f42":"code","713bb6e2":"code","0e5dda27":"code","31e6b088":"code","8024c353":"code","e7ec4f4f":"code","fe5a9bd3":"code","1c99fe8f":"code","133867e7":"code","e813d15a":"code","739c2425":"code","84b94aba":"code","593096ab":"code","c1e7bdd4":"code","7c5f8160":"code","9d60cd76":"code","45e23209":"code","246e6cce":"code","bc7b0883":"code","fd019cf7":"code","ca7a40fe":"code","b31a9d5b":"code","ec2dc622":"code","9f878945":"code","ddb37f1a":"code","d41b8ae1":"code","400b5d7c":"code","1fa673da":"code","f8ab365c":"code","2630a023":"code","ee42700b":"code","4722bb9b":"code","868fd58b":"code","4f5788de":"code","96f056b4":"code","d4b9ebc3":"code","1e5198e4":"code","3729f472":"code","cb8e2379":"code","5e226ee7":"code","58102535":"code","c28043c7":"code","1c89e463":"code","b6bc277c":"code","b48c122e":"code","b26bcdce":"code","102bd18b":"code","0d3360cf":"code","b516e248":"code","d96777fc":"code","c6f036d8":"code","d6695f0a":"code","9ec8b3b5":"code","28b4f10c":"code","d1901579":"code","5ac1eb82":"code","104ba759":"code","87dba546":"code","f3f24301":"markdown","061442dc":"markdown","bc9588f1":"markdown","d0973e1a":"markdown","0cc15449":"markdown","ff349919":"markdown","9e31e846":"markdown","15c15f30":"markdown","744be6d8":"markdown","e865ffea":"markdown","c172195e":"markdown","a85a53d2":"markdown","67218913":"markdown","e99539a7":"markdown","a003c26a":"markdown","dc0ead07":"markdown","7020d91d":"markdown","f6f03a50":"markdown","b714d93e":"markdown","725f3962":"markdown","4df0bf84":"markdown","614a82db":"markdown","f2883138":"markdown","a3dcf944":"markdown","ec07f883":"markdown","0549b145":"markdown","2fb0e146":"markdown","2fad6048":"markdown","67212cd5":"markdown","ac81f043":"markdown","09258494":"markdown","dd1411b6":"markdown","991105ed":"markdown","e1aa7a08":"markdown","c712ac60":"markdown","f25902b1":"markdown","5c30ffc1":"markdown","adfc6ae2":"markdown","5a0536a1":"markdown","3c80ebcc":"markdown","eadb95a2":"markdown","c7a7ec37":"markdown","02a856a4":"markdown"},"source":{"60c4d639":"# install factor-analyzer because it's not exist\n!pip install factor-analyzer","48f69c1a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom statsmodels import regression\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom scipy.stats import skew,norm\nimport math\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn import metrics\nfrom factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\nfrom factor_analyzer import FactorAnalyzer\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f1023d16":"# read data\ntrain= pd.read_csv(\"\/kaggle\/input\/data-analysis-2020\/train.csv\")\ntest= pd.read_csv(\"\/kaggle\/input\/data-analysis-2020\/test.csv\")\nsample_submission=pd.read_csv(\"\/kaggle\/input\/data-analysis-2020\/sample_submission.csv\")","60a516f0":"#show first rows in train set\ntrain.head()","bda47f42":"#Size of our train set\ntrain.shape","713bb6e2":"#show first rows in test set\ntest.head()","0e5dda27":"#Size of our train set\ntest.shape","31e6b088":"#Save Id of test set because we need it in the step testing\nID_test=test['Id']","8024c353":"#Show the result that we should have\nsample_submission.head()","e7ec4f4f":"#Show missing values in train set\ntrain.isna().sum()","fe5a9bd3":"# check how many companies are in the train set: 84 companies\nlen(train['company'].unique())","1c99fe8f":"#Some inforamtions about the train set\ntrain.info()","133867e7":"#The number of all missing value in train set\ntrain.isna().sum().sum()","e813d15a":"#Description of vaiables exist in train set\ntrain.describe()","739c2425":"# kernel density plot\nsns.distplot(train.next_day_ret,fit=norm);\nplt.ylabel =('Frequency')\nplt.title = ('next_day_ret Distribution');\nprint(\"skewness: %f\" % train['next_day_ret'].skew())\nprint(\"kurtosis: %f\" % train ['next_day_ret'].kurt())","84b94aba":"# Matrix of correlation\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat,vmax=0.9, square=True)\nplt.show();\n","593096ab":"#Plots of each variable as a function of others variables\nsns.pairplot(train,height=2);","c1e7bdd4":"#Matrix of variables most correlated with target\ncols = corrmat.nlargest(11, 'next_day_ret')['next_day_ret'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nplt.figure(figsize = (18,7))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","7c5f8160":"sns.regplot(x='num_trades',y='next_day_ret',data=train)\nplt.show()\ntrain = train[train['next_day_ret'] >-800]","9d60cd76":"sns.regplot(x='qty_traded',y='next_day_ret',data=train)\nplt.show()\ntrain = train[train['qty_traded'] <5000000]","45e23209":"sns.regplot(x='value',y='next_day_ret',data=train)\nplt.show()\ntrain = train[train['value'] <2000000]","246e6cce":"sns.regplot(x='value',y='next_day_ret',data=train)\nplt.show()","bc7b0883":"sns.regplot(x='close',y='next_day_ret',data=train)\nplt.show()\ntrain = train[train['next_day_ret'] < 1500]","fd019cf7":"sns.regplot(x='company_code',y='next_day_ret',data=train)\nplt.show()","ca7a40fe":"cols = corrmat.nlargest(11, 'next_day_ret')['next_day_ret'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nplt.figure(figsize = (18,7))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","b31a9d5b":"#drop duplicate rows\ntrain.drop_duplicates(inplace = True)","ec2dc622":"#See the size of train set after droped duplicated rows\ntrain.shape","9f878945":"#split data predicators and target\ny_train = train['next_day_ret']\ntrain.drop('next_day_ret', axis=1, inplace=True)","ddb37f1a":"#Convert the type of date vector in test set from object to datetime\ntest.date=pd.to_datetime(test.date, format='%d-%m-%Y', errors='coerce')","d41b8ae1":"# pre-processing will be done on X\nX = pd.concat([train,test])   #Concatination of train and test sets","400b5d7c":"#Size of the concatination X\nX.shape","1fa673da":"#Save Id and drop it from X so it will be droped from test train sets because it's irrelevant\nId=X['Id']\nX.drop('Id', axis=1, inplace=True)\n","f8ab365c":"#Informations about X\nX.info()","2630a023":"# calculate daily return for every company\nX['currReturn']=X.groupby(['company'])['close'].pct_change()","ee42700b":"#We will extract new features form the features have correlation arround 1: 'open','last','close','low','high','yesterday_price'\nExtract=X[['open','last','close','low','high','yesterday_price']]","4722bb9b":"#Plot the variance of the returns for the 10 examples of companies in terms of number of principal components\nX1 = np.asarray(Extract)\n[n,m] = X1.shape\nL=[] #initialise the list of percentages of variance of the returns for the 10 examples of companies\nfor num_pc in range(1,5):\n    pca = PCA(n_components=num_pc) # number of principal components\n    pca.fit(X1)\n    percentage =  pca.explained_variance_ratio_\n    percentage_cum= np.cumsum(percentage)\n    L.append(percentage_cum[-1]*100)\n    pca_components = pca.components_  \ny=np.array(L)\nx=np.arange(1,5)\nplt.scatter(x,y)\nplt.show()","868fd58b":"#Show the variance of the returns for the companies bank expalined by 3 PCs\nnum_pc =2\n\nXExtract = np.asarray(Extract)\n[n,m] = XExtract.shape\nprint('The number of timestamps is {}.'.format(n))\nprint('The number of stocks is {}.'.format(m))\n\npca = PCA(n_components=num_pc) # number of principal components\npca.fit(XExtract)\n\npercentage =  pca.explained_variance_ratio_\npercentage_cum = np.cumsum(percentage)\nprint('{0:.2f}% of the variance is explained by the first for 2 PCs'.format(percentage_cum[-1]*100))\n\npca_components = pca.components_","4f5788de":"#Plot contibution and cumulative contribution of the 2 principal components\nx = np.arange(1,len(percentage)+1,1)\n\nplt.bar(x, percentage*100, align = \"center\")\n#plt.title('Contribution of principal components',fontsize = 20)\n#plt.xlabel('principal components',fontsize = 20)\n#plt.ylabel('percentage',fontsize = 20)\nplt.xticks(x,fontsize = 20) \nplt.yticks(fontsize = 20)\nplt.xlim([0, num_pc+1])\nplt.show()\nplt.plot(x, percentage_cum*100,'ro-')\n#plt.xlabel('principal components',fontsize = 20)\n#plt.ylabel('percentage',fontsize = 20)\n#plt.title('Cumulative contribution of principal components',fontsize = 20)\nplt.xticks(x,fontsize = 20) \nplt.yticks(fontsize = 20)\nplt.xlim([1, num_pc])\nplt.ylim([0,100])\nplt.show()","96f056b4":"#Determine the PCs\nfactor_returns = XExtract.dot(pca_components.T)\nfactor_returns = pd.DataFrame(columns=[\"pca 1\", \"pca 2\"], \n                              index=Extract.index,\n                              data=factor_returns)\nfactor_returns.head()","d4b9ebc3":"#And we will add the new features: The 2 PCs\nX[\"pca 1\"]=factor_returns[\"pca 1\"]\nX[\"pca 2\"]=factor_returns[\"pca 2\"]","1e5198e4":"#Solve missing value problem\nX['value'].fillna(X['value'].mean(), inplace=True)\nval=X['currReturn'].mean()\nX.fillna(X.mean() ,inplace=True)","3729f472":"X.isna().sum()","cb8e2379":"Extract=X[['value','qty_traded','num_trades','currReturn']]","5e226ee7":"#Plot the variance of the returns for the 10 examples of companies in terms of number of principal components\nX1 = np.asarray(Extract)\n[n,m] = X1.shape\nL=[] #initialise the list of percentages of variance of the returns for the 10 examples of companies\nfor num_pc in range(1,5):\n    pca = PCA(n_components=num_pc) # number of principal components\n    pca.fit(X1)\n    percentage =  pca.explained_variance_ratio_\n    percentage_cum= np.cumsum(percentage)\n    L.append(percentage_cum[-1]*100)\n    pca_components = pca.components_  \ny=np.array(L)\nx=np.arange(1,5)\nplt.scatter(x,y)\nplt.show()\n","58102535":"#Show the variance of the returns for the companies bank expalined by 5 PCs\nnum_pc =2\n\nXExtract = np.asarray(Extract)\n[n,m] = XExtract.shape\nprint('The number of timestamps is {}.'.format(n))\nprint('The number of stocks is {}.'.format(m))\n\npca = PCA(n_components=num_pc) # number of principal components\npca.fit(XExtract)\n\npercentage =  pca.explained_variance_ratio_\npercentage_cum = np.cumsum(percentage)\nprint('{0:.2f}% of the variance is explained by the first for 2 PCs'.format(percentage_cum[-1]*100))\n\npca_components = pca.components_","c28043c7":"#Plot contibution and cumulative contribution of the 2 principal components\nx = np.arange(1,len(percentage)+1,1)\n\nplt.bar(x, percentage*100, align = \"center\")\n#plt.title('Contribution of principal components',fontsize = 20)\n#plt.xlabel('principal components',fontsize = 20)\n#plt.ylabel('percentage',fontsize = 20)\nplt.xticks(x,fontsize = 20) \nplt.yticks(fontsize = 20)\nplt.xlim([0, num_pc+1])\nplt.show()\nplt.plot(x, percentage_cum*100,'ro-')\n#plt.xlabel('principal components',fontsize = 20)\n#plt.ylabel('percentage',fontsize = 20)\n#plt.title('Cumulative contribution of principal components',fontsize = 20)\nplt.xticks(x,fontsize = 20) \nplt.yticks(fontsize = 20)\nplt.xlim([1, num_pc])\nplt.ylim([0,100])\nplt.show()","1c89e463":"#Determine the PCs\nfactor_returns = XExtract.dot(pca_components.T)\nfactor_returns = pd.DataFrame(columns=[\"pca 3\", \"pca 4\"], \n                              index=Extract.index,\n                              data=factor_returns)\nfactor_returns.head()","b6bc277c":"#we will add the new features: The 2 new PCs\nX[\"pca 3\"]=factor_returns[\"pca 3\"]\nX[\"pca 4\"]=factor_returns[\"pca 4\"]","b48c122e":"# evaluate the \u201cfactorability\u201d of our combinaison. \nchi_square_value,p_value=calculate_bartlett_sphericity(X[['pca 1','pca 2','pca 3','pca 4']])\nchi_square_value, p_value","b26bcdce":"# Create factor analysis object and perform factor analysis\nfa = FactorAnalyzer()\n#Check Eigenvalues\nfa.fit(X[['pca 1','pca 2','pca 3','pca 4']])\nev, v = fa.get_eigenvalues()\nev","102bd18b":"# Create scree plot using matplotlib\nplt.scatter(range(1,X[['pca 1','pca 2','pca 3','pca 4']].shape[1]+1),ev)\nplt.plot(range(1,X[['pca 1','pca 2','pca 3','pca 4']].shape[1]+1),ev)\n\nplt.grid()\nplt.show()","0d3360cf":"# Create factor analysis object and perform factor analysis\nfa = FactorAnalyzer()\nfa.set_params(n_factors=2, rotation=\"varimax\")\nfa.fit(X[['pca 1','pca 2','pca 3','pca 4']])\nloading=fa.loadings_\nloadings=pd.DataFrame(columns=[\"FA 1\", \"FA 2\"], index=['pca 1','pca 2','pca 3','pca 4'],data=loading)\nloadings","b516e248":"#dummies encoding to categorical features\nX = pd.get_dummies(X) \n        \nprint('\\nNumber of features:', len(X.columns))","d96777fc":"#split data\ntrain = pd.DataFrame(X[:train.shape[0]])\ntest = pd.DataFrame(X[train.shape[0]:])\n","c6f036d8":"#scaling predicators variable\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler().fit(train)                #fit on train data\ntrain = scaler.transform(train)  #transorm train and test data\ntest = scaler.transform(test)","d6695f0a":"#We will work linear regression model \nlin_reg = Ridge(alpha=1)\n# train model\nlin_reg.fit(train, y_train)\n# predict\ny_train_pred = lin_reg.predict(train)\ny_test_pred = lin_reg.predict(test)\n# Plot predictions\nfig=plt.figure()\nplt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\ny_test=sample_submission['next_day_ret']\nplt.scatter(y_test_pred, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.show()","9ec8b3b5":"#We need to save y_test for the testing step to calculate RMSE value\ny_test=sample_submission['next_day_ret']","28b4f10c":"ids=ID_test\npredictions = lin_reg.predict(test)\npredictions_train = lin_reg.predict(train)\npredictions = np.around(predictions, decimals=4, out=None)\nresult=pd.DataFrame({'Id':ids,'next_day_ret':predictions})\nresult.to_csv(\"\/kaggle\/working\/submission.csv\",index=False)\nprint(result)","d1901579":"print(\"MAE:\", metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE_test:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\nprint('RMSE_train:', np.sqrt(metrics.mean_squared_error(y_train, predictions_train)))","5ac1eb82":"#Plot preduction as a function of y_test (It should be linear with pont=1)\nplt.scatter(y_test, predictions)","104ba759":"#Plot resuldials for the test\nsns.residplot(predictions, y_test, lowess=True, color=\"g\")\nplt.show()","87dba546":"#Plot resuldials for the train\nsns.residplot(predictions_train, y_train, lowess=True, color=\"g\")\nplt.show()","f3f24301":"*See correlation after removing outliers:","061442dc":"*Feature Engineering: \u064eApplicate Factors analysis to pca 1, pca 2, pca 3, pca 4 to see the unobserved features :","bc9588f1":"Observations and interpretations:\n\n    According this plot, the density of the target next_day_ret isn't normalized and it's centrated on zero\n\n","d0973e1a":"**Observations and Interpritations:**\n    \n- I use Bartlett\u2019s Test to check the factorability.\n- In this Bartlett \u2019s test, the p-value(= 3.2007663632324254e-198) is arround 0. The test was statistically significant, indicating that the observed correlation matrix is not an identity matrix.\n    So, we can proceed with factor analysis.\n","0cc15449":"***STEP 0: Import necesery Libraries and methods***","ff349919":"**STEP 1 : Data Retrieval**","9e31e846":"*Data cleaning (Solve the problem of missing value):","15c15f30":"**Observations and Interpritations:**\n- We see that 2 PCs are suffisante to explain the variance of the different features open, last, close, low, high and yesterday_price\n- This PCs are decorrelated so if we extract them and add them to our model they can ameliorate the performance of model","744be6d8":"1. First we will aplicated pca in the more correlated features: 'open', 'last', 'close', 'low','high', 'yesterday_price'","e865ffea":"**Observations and Interpritations:**\n\n                 According this plots,  the data hasn't a constant variance. In our case data is with non-constant variance. So The relationship is heteroscedastic","c172195e":"**STEP 8: Residual Analysis**","a85a53d2":"          We can conclude that our model is not very good because it's not more trained. In fact, our train set is only      the 30% of all the data that means it not content tne informations about the companies at all days\n            between 2018-01-01 and 2018-12-31. ","67218913":"**Observations and Interpritations:**\nAccording the plots above:\n* All the features are decorrelated with the target next_day_ret\n* We have correlation only between groupe of features: open, last, close, low, high and yesterday_price","e99539a7":"**STEP2: Data Exploration **","a003c26a":"# > **Final Project- Data Analysis**","dc0ead07":"* *Explore correlation of variables:*","7020d91d":"# **Conclusions**","f6f03a50":"**STEP 7: Testing**","b714d93e":"**Observations and Interpritations:**\n\n       This plot explain that the relation isn't a linear straight with slope=1 So the predictions of our model are note very good","725f3962":"![logo.jpg](attachment:logo.jpg)","4df0bf84":"2. Now, we will applicated PCA once more in  'value', 'qty_traded', 'num_trades' and 'currReturn'","614a82db":" **Feature engineering: Add a new feature named currReturn:*","f2883138":"**Observations and Interpritations:**\n\n    After removing some missing values we observe that the correlation between value, qty_traded and  num_trades ","a3dcf944":"**Observations and Interpritations:**\n        \n        We note that now all features expect date haven't missing value. And the problem of missing value in date(categorical feature) will be solved when we will encoding catigorical features","ec07f883":"Observations\/interpretations:\n\n- According this matrix we conclude that:\n      \n      - Factor 1 has high factor loadings for pca1\n      - Factor 2 has high factor loadings for pca4\n       \n\n","0549b145":"**STEP3: Data Processing & Wrangling**","2fb0e146":"                                           Made By Semah MABROUKI****  2nd year SISY","2fad6048":"           So If we use all the data our linear model will be perfect","67212cd5":"     before data sculing, we should split X into train and test sets ","ac81f043":"**** Plot density of target:","09258494":"**SETP 5: Feature Scaling**","dd1411b6":"**STEP4: Encoding categorical features**","991105ed":"**STEP 6: Modeling**","e1aa7a08":"              We note that the number of features is too small so we have to add new features to our data","c712ac60":"                                              **Tunisia Polytechnic School**","f25902b1":"**Observations and Interpritations:**\n\n    According this plot, we can see that only for 2-factors eigenvalues are greater than one. It means we need to choose only 2 unobserved variables","5c30ffc1":"****Plan****\n1. Data Retrevial\n2. Data Exploration\n3. Data preporecessing\n4. Modeling\n5. Testing\n6. Resudial analysis","adfc6ae2":"**Observations and interpretations:**\n         \n         We note that our data content many missing value so we need to clean our data.","5a0536a1":"**Feature Engineering: applicate PCA to extract new features*","3c80ebcc":"**Observations and Interpritations**\n                \n                This plot explain that the pridictions are not more similated then the real observations for both \n                train and test sets. So we have a problem of underfitting ","eadb95a2":"*Removing outliers from train set:","c7a7ec37":"****Objectif:****\n- The goal of this project is to predict next day's returns using daily stock data from Tunisia Stock Exchange. The evaluation metric for this competition is the RMSE.\n\n- We need to use a linear regression model, Data visualization, correlation, PCA, FA, Residual Analysis...","02a856a4":"Observations and Interpritations:\n\n- We see that 2 PCs are suffisante to explain the variance of different features: value, qty_traded, num_trades and currReturn\n- This PCs are decorrelated so if we extract them and add them to our model they can ameliorate the performance of model\n\n"}}