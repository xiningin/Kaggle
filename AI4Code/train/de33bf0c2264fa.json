{"cell_type":{"37f8fc42":"code","15035505":"code","37c3d91d":"code","689f5e3f":"code","48035144":"code","fe27635b":"code","c2ea000f":"code","fbcbfb7d":"code","cce43647":"code","0af888a9":"code","006e690e":"code","2ba4f983":"code","c6906ef6":"code","8fd52936":"code","6b5c24ec":"code","9313027a":"code","2be656bf":"code","4f44fb80":"code","b78e3515":"code","c2cad8fb":"code","22042274":"code","9c2c8e82":"code","e5632d9b":"code","f28901a5":"code","62323835":"code","edc153e9":"code","8a5b7309":"code","0950648d":"code","8d12526a":"code","a668acf1":"code","048223f5":"code","f5846a0d":"code","a0fd7d0a":"code","6716dd5c":"code","2b9b1c28":"code","4a167a8c":"code","8c1664d6":"code","dd8b32e9":"code","e4407628":"code","ed9f8050":"code","be148954":"code","a66aa422":"code","b2af2f07":"code","dc775901":"code","3c004755":"code","0ca238a6":"code","27de29eb":"code","6d4e4848":"code","73589bd0":"code","ed4b155f":"code","84705022":"code","b1dab6b9":"code","a06fce0d":"code","6984ecf5":"code","23046d10":"code","352a9cf3":"code","1d3e1a4e":"code","2e2807b4":"code","6005acac":"code","fb85fded":"code","de25d06a":"markdown","e4a18f39":"markdown","41b22041":"markdown","b6c6d1e9":"markdown","df7e2f12":"markdown","52aa5603":"markdown","ae44a80a":"markdown","bfe17bef":"markdown","1d56bd76":"markdown","47d98d91":"markdown","72f11a5b":"markdown","62beab5c":"markdown","eced8fe6":"markdown"},"source":{"37f8fc42":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","15035505":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly import tools\nfrom plotly.subplots import make_subplots\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.combine import SMOTEENN","37c3d91d":"df = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()","689f5e3f":"df.shape","48035144":"df.info()","fe27635b":"df.describe()","c2ea000f":"df.isna().sum()","fbcbfb7d":"df.drop('id', 1, inplace=True)","cce43647":"fig = px.bar(x=df['stroke'].value_counts().index, y=df['stroke'].value_counts(), text=(df['stroke'].value_counts()\/len(df['stroke'])*100), height=500, width=500)\nfig.update_traces(textposition='outside', texttemplate='%{text:.4s}%', marker_color=['peachpuff','silver'])\nfig.show()","0af888a9":"sns.distplot(x=df['age'])","006e690e":"fig = go.Figure()\n\n\nfig.add_trace(go.Histogram(x=df['age'],\n                          xbins=dict(\n                          size=1),\n                          opacity=1))\n\nfig.update_layout(title_text='Age Distribution',\n                 xaxis_title='Age',\n                 yaxis_title='Count',\n                 bargap=0.05,\n                 xaxis={'showgrid':False},\n                 yaxis={'showgrid':False},\n                 template='ggplot2',\n                 height=600,\n                 width=1000)\nfig.show()\n","2ba4f983":"trace1= go.Bar(x=df['gender'].value_counts().index, y=df['gender'].value_counts(), text = (df['gender'].value_counts()\/len(df['gender'])*100), \n               marker_color=['lightpink','lightblue','grey'])\n\ntrace2= go.Bar(x=df['hypertension'].value_counts().index, y=df['hypertension'].value_counts(), text = (df['hypertension'].value_counts()\/len(df['hypertension'])*100),\n              marker_color=['plum','papayawhip'])\n\ntrace3= go.Bar(x=df['heart_disease'].value_counts().index, y=df['heart_disease'].value_counts(), text = (df['heart_disease'].value_counts()\/len(df['heart_disease'])*100),\n              marker_color=['mediumturquoise','lightgreen'])\n\ntrace4= go.Bar(x=df['ever_married'].value_counts().index, y=df['ever_married'].value_counts(), text = (df['ever_married'].value_counts()\/len(df['ever_married'])*100),\n              marker_color=['seagreen',\"rgb(114, 78, 145)\"])\n\ntrace5 = go.Bar(x=df['work_type'].value_counts().index, y=df['work_type'].value_counts(), text=(df['work_type'].value_counts()\/len(df['work_type'])*100),\n             marker_color=['rgb(56, 75, 126)', 'rgb(18, 36, 37)', 'rgb(34, 53, 101)','rgb(33, 75, 99)'])\n\ntrace6 = go.Bar(x=df['Residence_type'].value_counts().index, y=df['Residence_type'].value_counts(), text=(df['Residence_type'].value_counts()\/len(df['Residence_type'])*100),\n               marker_color=['palegreen','olive'])\n\nfig = make_subplots(rows=3, cols=2, specs=[[{'type':'bar'},{'type':'bar'}],\n                                          [{'type':'bar'},{'type':'bar'}],\n                                          [{'type':'bar'},{'type':'bar'}]],\n                   subplot_titles = ['Gender Distribution','Hypertension Distribution','Heart Disease Distribution','Married VS Single',\n                   'Private OR Self-Employed','Resident Type Distribution'])\n\nfig.append_trace(trace1,1,1)\nfig.append_trace(trace2,1,2)\nfig.append_trace(trace3,2,1)\nfig.append_trace(trace4,2,2)\nfig.append_trace(trace5,3,1)\nfig.append_trace(trace6,3,2)\n\nfig['layout'].update(height=1500, width=1000, title='Stroke Prediction Discrete Feature Analysis')\nfig.update_traces(textposition='outside', texttemplate='%{text: .3s}%')\nfig.show()","c6906ef6":"fig = go.Figure()\n\n\nfig.add_trace(go.Histogram(x=df['avg_glucose_level'],\n                          xbins=dict(\n                          size=1),\n                          opacity=1))\n\nfig.update_layout(title_text='AVG_Glucose_Level',\n                 xaxis_title='AVG_Glucose_Level',\n                 yaxis_title='Count',\n                 bargap=0.05,\n                 xaxis={'showgrid':False},\n                 yaxis={'showgrid':False},\n                 template='seaborn',\n                 height=600,\n                 width=1000)\nfig.show()\n","8fd52936":"\ndf['bmi'].fillna(df['bmi'].mean(), inplace=True)","6b5c24ec":"fig = go.Figure()\n\n\nfig.add_trace(go.Histogram(x=df['bmi'],\n                          xbins=dict(\n                          size=1),\n                          opacity=1))\n\nfig.update_layout(title_text='Body Mass Index',\n                 xaxis_title='BMI',\n                 yaxis_title='Count',\n                 bargap=0.05,\n                 xaxis={'showgrid':False},\n                 yaxis={'showgrid':False},\n                 template='plotly_white',\n                 height=600,\n                 width=1000)\nfig.show()\n","9313027a":"numerical = df.select_dtypes(exclude=['object']).columns\nnumerical","2be656bf":"# Finding all the categorical columns from the data\ncategorical=df.select_dtypes(exclude=['int64','float64']).columns\ncategorical","4f44fb80":"color=['lightblue','navy']\nfor i in categorical:\n    sns.countplot(x=df[i], hue=df['stroke'], palette=color)\n    plt.title('Categorical Features VS Stoke', fontsize=17)\n    plt.xlabel(i, fontsize=12)\n    plt.ylabel('Count')\n    plt.show()","b78e3515":"for i in numerical:\n    fig = px.histogram(x=df[i], color=df['stroke'], height=500,width=800, title=str(i)+' VS Stroke', nbins=20)\n    fig.show()","c2cad8fb":"for i in categorical:\n    df[i]=pd.factorize(df[i])[0]","22042274":"df.head(3)","9c2c8e82":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(), annot=True)\nplt.show()","e5632d9b":"x = df.drop(['stroke'], axis=1).values\ny = df['stroke'].values\nprint(x.shape)\nprint(y.shape)","f28901a5":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, random_state=20, test_size=0.2, shuffle=True)","62323835":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\n\nrf = RandomForestClassifier()\n\nrf.fit(x_train, y_train)","edc153e9":"print('Accuracy score of Random Forest is: ', accuracy_score(y_test, rf.predict(x_test))*100,'%')","8a5b7309":"cf=metrics.confusion_matrix(y_test,rf.predict(x_test), labels=[1,0])\nsns.heatmap(cf,annot=True)","0950648d":"# Now let's have a look at the report\nprint(metrics.classification_report(y_test,rf.predict(x_test), labels=[1,0]))\nprint('Accuracy_Score:',accuracy_score(y_test,rf.predict(x_test))*100,'%')\nprint('Recall:',metrics.recall_score(y_test,rf.predict(x_test))*100,'%')","8d12526a":"from xgboost import XGBClassifier\nxgb=XGBClassifier(use_label_encoder=False)","a668acf1":"xgb.fit(x_train, y_train)\nprint('Accuracy of XGBClassifier is: ', accuracy_score(y_test, xgb.predict(x_test))*100,'%')","048223f5":"cf=metrics.confusion_matrix(y_test,xgb.predict(x_test), labels=[1,0])\nsns.heatmap(cf,annot=True)","f5846a0d":"# Now let's have a look at the report\nprint(metrics.classification_report(y_test,xgb.predict(x_test), labels=[1,0]))\nprint('Accuracy_Score:',accuracy_score(y_test,xgb.predict(x_test))*100,'%')\nprint('Recall:',metrics.recall_score(y_test,xgb.predict(x_test))*100,'%')","a0fd7d0a":"from lightgbm import LGBMClassifier\nlgbm=LGBMClassifier()\nlgbm.fit(x_train, y_train)","6716dd5c":"print('Accuracy of LGBMClassifier is: ', accuracy_score(y_test, lgbm.predict(x_test))*100,'%')","2b9b1c28":"cf=metrics.confusion_matrix(y_test,rf.predict(x_test), labels=[1,0])\nsns.heatmap(cf,annot=True)","4a167a8c":"# Now let's have a look at the report\nprint(metrics.classification_report(y_test,lgbm.predict(x_test), labels=[1,0]))\nprint('Accuracy_Score:',accuracy_score(y_test,lgbm.predict(x_test))*100,'%')\nprint('Recall:',metrics.recall_score(y_test,lgbm.predict(x_test))*100,'%')","8c1664d6":"#before applying smote\n\none_count =0\nzero_count = 1\nfor i in y:\n    if i==1:\n        one_count+=1\n    else:\n        zero_count+=1\nprint('The number of one are: ',one_count)\nprint('The number of zero are: ', zero_count)","dd8b32e9":"x_train1, x_test1, y_train1, y_test1 = train_test_split(x,y,random_state=22, test_size=0.2, shuffle=True)","e4407628":"oversample = SMOTE(random_state=101)\nx_train1, y_train1 = oversample.fit_resample(x_train1,y_train1)","ed9f8050":"#After applyig smote\n\none_count=0\nzero_count =0\nfor i in y_train1:\n    if i==1:\n        one_count +=1\n    else:\n        zero_count +=1\nprint('The number of one are: ',one_count)\nprint('The number of zero are: ', zero_count)","be148954":"lgbm = LGBMClassifier()\nlgbm.fit(x_train1, y_train1)","a66aa422":"print('Accuracy of LGBMClassifier is: ', accuracy_score(y_test1, lgbm.predict(x_test1))*100,'%')","b2af2f07":"cf=metrics.confusion_matrix(y_test1,lgbm.predict(x_test1), labels=[1,0])\nsns.heatmap(cf,annot=True)","dc775901":"\nprint(metrics.classification_report(y_test1,lgbm.predict(x_test1), labels=[1,0]))\nprint('Accuracy_Score:',accuracy_score(y_test1,lgbm.predict(x_test1))*100,'%')\nprint('Recall:',metrics.recall_score(y_test1,lgbm.predict(x_test1))*100,'%')","3c004755":"x_train2, x_test2, y_train2, y_test2 = train_test_split(x,y,random_state=22, test_size=0.2, shuffle=True)","0ca238a6":"lgbm = LGBMClassifier()\nlgbm.fit(x_train2, y_train2)","27de29eb":"sme = SMOTEENN()\nx_train2, y_train2 = sme.fit_resample(x_train2,y_train2)","6d4e4848":"lgbm.fit(x_train2,y_train2)","73589bd0":"print('Accuracy of LGBMClassifier is: ', accuracy_score(y_test2, lgbm.predict(x_test2))*100,'%')","ed4b155f":"cf=metrics.confusion_matrix(y_test2,lgbm.predict(x_test2), labels=[1,0])\nsns.heatmap(cf,annot=True)","84705022":"# Now let's have a look at the report\nprint(metrics.classification_report(y_test2,lgbm.predict(x_test2), labels=[1,0]))\nprint('Accuracy_Score:',accuracy_score(y_test2,lgbm.predict(x_test2))*100,'%')\nprint('Recall:',metrics.recall_score(y_test2,lgbm.predict(x_test2))*100,'%')","b1dab6b9":"x_train3, x_test3, y_train3, y_test3 = train_test_split( x, y, test_size=0.2, random_state=22, shuffle=True)","a06fce0d":"one_count=0\nzero_count=0\nfor i in y_train3:\n    if i==1:\n        one_count+=1\n    else:\n        zero_count+=1\nprint('The number of 0 are:',zero_count)\nprint('The number of 1 are:',one_count)","6984ecf5":"over = BorderlineSMOTE(sampling_strategy=0.3)\nunder = RandomUnderSampler(sampling_strategy=0.6)\nsteps = [('o', over), ('u', under)]","23046d10":"pipeline = Pipeline(steps=steps)\nx_train3, y_train3 = pipeline.fit_resample(x_train3, y_train3)","352a9cf3":"# After\none_count=0\nzero_count=0\nfor i in y_train3:\n    if i==1:\n        one_count+=1\n    else:\n        zero_count+=1\nprint('The number of 0 are:',zero_count)\nprint('The number of 1 are:',one_count)","1d3e1a4e":"lgbm = LGBMClassifier()\nlgbm.fit(x_train3, y_train3)","2e2807b4":"print('Accuracy of LGBMClassifier is: ', accuracy_score(y_test3, lgbm.predict(x_test3))*100,'%')","6005acac":"cf=metrics.confusion_matrix(y_test3,lgbm.predict(x_test3), labels=[1,0])\nsns.heatmap(cf,annot=True)","fb85fded":"print(metrics.classification_report(y_test3,lgbm.predict(x_test3), labels=[1,0]))\nprint('Accuracy_Score:',accuracy_score(y_test3,lgbm.predict(x_test3))*100,'%')\nprint('Recall:',metrics.recall_score(y_test3,lgbm.predict(x_test3))*100,'%')","de25d06a":"SMOTE is an oversampling technique that generates synthetic samples from the minority class.","e4a18f39":"After applying the smote we can see that number of one and zeros are balanced now.","41b22041":"# Boderline SMOTE and RandomUnderSampling","b6c6d1e9":"# Confusion Matrix","df7e2f12":"# XGBClassifier","52aa5603":"# SMOTEENN","ae44a80a":"Boderline SMOTE will not only oversample the miority data but majority data as well, where the majority data are causing misclassification in the decision boundary.\n\nRandomUnderSampling randomly selecting examples from the majority class and deleting them from the training dataset.","bfe17bef":"# SMOTE","1d56bd76":"# Random Forest Classifier","47d98d91":"# **Confusion Matrix**","72f11a5b":"The Data is highly imbalanced with only 4% of patients with target(stroke)=1 and 95% with target(stroke)=0.\n\nBy using trivial predicator, we can achieve an accuracy of 94.6% which seems good performance at first, however this trivial predictor is completely useless as it has absolutely no discriminatory power.\n\nI will be using SMOTE algorithm in this notebook to balance the data.\n","62beab5c":"SMOTEENN is used to downsample majority class.","eced8fe6":"# LGBMCLASSIFIER"}}