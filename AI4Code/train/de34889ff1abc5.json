{"cell_type":{"608fdea4":"code","8c7f79c5":"code","95e2729c":"code","883e5451":"code","062bde69":"code","ff27a30a":"code","e73bca86":"code","c2c7e538":"code","f0ed3709":"code","c82fa255":"code","736b9539":"code","e93bb291":"code","834aa601":"code","810cb77a":"code","19e3ffc2":"code","04622b16":"code","95474e2a":"code","4f5e5557":"code","cc2fd78c":"code","73d4e10b":"code","a5060bab":"code","e43faaa5":"code","5e9efac1":"code","03fcf2d1":"code","9c2cbc9a":"code","a5eacd04":"code","b808d57f":"code","bbe08785":"code","1f7084f7":"code","e63efc14":"code","8e6aa658":"code","993d3b57":"code","b3466905":"code","791259fc":"code","be75a2c0":"code","d492bf10":"code","edae628d":"code","9e92fe9e":"code","8298905a":"code","8e9c110f":"code","27b932cb":"code","26df5ea2":"code","ff811413":"markdown","f33aca94":"markdown","fce080e5":"markdown"},"source":{"608fdea4":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c7f79c5":"import torch\nimport numpy as np\nfrom PIL import Image\nfrom PIL import ImageFile\n# sometimes, you will have images without an ending bit\n# this takes care of those kind of (corrupt) images\nImageFile.LOAD_TRUNCATED_IMAGES = True","95e2729c":"def reconstitute(four_images, permutation_targets):\n    perm = str(permutation_targets)\n    four_images[int(perm[0])]\n    line1 = []\n    line1.append(four_images[int(perm[0])])\n    line1.append(four_images[int(perm[1])])\n    line2 = []\n    line2.append(four_images[int(perm[2])])\n    line2.append(four_images[int(perm[3])])\n    h_part = np.concatenate(tuple(line1), axis=1)\n    b_part = np.concatenate(tuple(line2), axis=1)\n    \n    recon = np.concatenate((h_part, b_part), axis=0)\n    return recon","883e5451":"class ClassificationDataset:\n    def __init__(self, image1_paths,\n                 image2_paths,\n                 image3_paths,\n                 image4_paths,\n                 permutation_targets,\n                 targets,\n                 resize=None,\n                augmentations=None):\n        self.image1_paths = image1_paths\n        self.image2_paths = image2_paths\n        self.image3_paths = image3_paths\n        self.image4_paths = image4_paths\n        self.permutation_targets = permutation_targets\n        self.targets = targets\n        self.resize = resize\n        self.augmentations = augmentations\n    \n    def __len__(self):\n        return len(self.image1_paths)\n    def __getitem__(self, item):\n        \"\"\"\n        for a given \"item\" index, return everything we \n        need to train a given model\n        \"\"\"\n        #Use PIL to open the images\n        image1 = Image.open(self.image1_paths[item])\n        image2 = Image.open(self.image2_paths[item])\n        image3 = Image.open(self.image3_paths[item])\n        image4 = Image.open(self.image4_paths[item])\n        \n        \n        \n        #convert image to RGB, we have a single channel images\n        image1 = image1.convert('RGB')\n        image2 = image2.convert('RGB')\n        image3 = image3.convert('RGB')\n        image4 = image4.convert('RGB')\n        #grab correct targets\n        targets = self.targets[item]\n        permutation_targets = self.permutation_targets[item]\n        \n        \n        #resize if needed\n        if self.resize is not None:\n            image1 = image1.resize(\n                (self.resize[1], self.resize[0]),\n                resample=Image.BILINEAR\n            )\n            image2 = image2.resize(\n                (self.resize[1], self.resize[0]),\n                resample=Image.BILINEAR\n            )\n            image3 = image3.resize(\n                (self.resize[1], self.resize[0]),\n                resample=Image.BILINEAR\n            )\n            image4 = image4.resize(\n                (self.resize[1], self.resize[0]),\n                resample=Image.BILINEAR\n            )\n        \n        # convert image to numpy array\n        image1 = np.array(image1)\n        image2 = np.array(image2)\n        image3 = np.array(image3)\n        image4 = np.array(image4)\n        \n        four_images = [image1, image2, image3, image4]\n        \n        image_reconstitute = reconstitute(four_images, permutation_targets)\n        image_reconstitute = np.array(image_reconstitute)\n        \n        # if we have albumentation augmentations\n        # add them to the image\n        if self.augmentations is not None:\n            augmented1 = self.augmentations(image=image1)\n            image1 = augmented1['image']\n            \n            augmented2 = self.augmentations(image=image2)\n            image2 = augmented2['image']\n            \n            augmented3 = self.augmentations(image=image3)\n            image3 = augmented3['image']\n            \n            augmented4 = self.augmentations(image=image4)\n            image4 = augmented4['image']\n        \n        # pytorch expects CHW instead of HWC\n        image1 = np.transpose(image1, (2, 0, 1)).astype(np.float32)\n        image2 = np.transpose(image2, (2, 0, 1)).astype(np.float32)\n        image3 = np.transpose(image3, (2, 0, 1)).astype(np.float32)\n        image4 = np.transpose(image4, (2, 0, 1)).astype(np.float32)\n        image_reconstitute = np.transpose(image_reconstitute, (2, 0, 1)).astype(np.float32)\n        \n        # return tensors of image and targets\n        return {\n            \"image1\": torch.tensor(image1, dtype=torch.float),\n            \"image2\": torch.tensor(image2, dtype=torch.float),\n            \"image3\": torch.tensor(image3, dtype=torch.float),\n            \"image4\": torch.tensor(image4, dtype=torch.float),\n            \"targets\": torch.tensor(targets, dtype=torch.long),\n            \"image_reconstitute\": torch.tensor(image_reconstitute, dtype=torch.float),\n        }","062bde69":"from tqdm import tqdm\n\n\ndef train(data_loader, model, optimizer, device):\n    \"\"\"\n    This function does training for one epoch\n    :param data_loader: this is the pytorch dataloader\n    :param encoder: pytorch model\n    :param decoder: pytorch model\n    :param optimizer: optimizer, for e.g. adam, sgd, etc\n    :param device: cuda\/cpu\n    \"\"\"\n    # put the models in train mode\n    model.train()\n    \n#     encoder.train()\n#     decoder.train()\n    \n    for data in data_loader:\n        inputs1 = data['image1']\n        inputs2 = data['image2']\n        inputs3 = data['image3']\n        inputs4 = data['image4']\n        targets = data['targets']\n        image_reconstitute = data['image_reconstitute']\n        \n        inputs1 = inputs1.to(device, dtype=torch.float)\n        inputs2 = inputs2.to(device, dtype=torch.float)\n        inputs3 = inputs3.to(device, dtype=torch.float)\n        inputs4 = inputs4.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.long)\n        image_reconstitute = image_reconstitute.to(device, dtype=torch.float)\n        \n        \n        # zero grad the optimizer\n        optimizer.zero_grad()\n        #do the forward step\n        outputs = model([inputs1, inputs2, inputs3, inputs4])\n#         encoder_outputs1 = encoder(inputs1)\n#         encoder_outputs2 = encoder(inputs2)\n#         encoder_outputs3 = encoder(inputs3)\n#         encoder_outputs4 = encoder(inputs4)\n#         cats = torch.cat((encoder_outputs1, encoder_outputs2, encoder_outputs3, encoder_outputs4), 1)\n#         decoder_outputs = decoder(cats)\n        \n        # calculate loss\n        criterion_multioutput = nn.CrossEntropyLoss()\n        criterion_mse = nn.MSELoss()\n        out_pred_hat = outputs['out_pred']\n        recon_hat = outputs['recon_image']\n        \n        loss1 = criterion_multioutput(out_pred_hat, targets)\n        loss2 = criterion_mse(recon_hat, image_reconstitute)\n        loss = loss1 + loss2\n        loss.backward()\n        #step optimizer\n        optimizer.step()\n        # if you have a scheduler, you either need to\n        # step it here or you have to step it after\n        # the epoch. here, we are not using any learning\n        # rate scheduler\n        ","ff27a30a":"def evaluate(data_loader, model, device):\n    \"\"\"\n    this function does evaluation for one epoch\n    \"\"\"\n    # put the model in evaluation mode\n    model.eval()\n#     encoder.eval()\n#     decoder.eval()\n    \n    # init lists to store targets and outputs\n    final_targets = []\n    final_outputs = []\n    final_recon = []\n    \n    #we use no_grad context\n    with torch.no_grad():\n        for data in data_loader:\n            inputs1 = data['image1']\n            inputs2 = data['image2']\n            inputs3 = data['image3']\n            inputs4 = data['image4']\n            targets = data['targets']\n            image_reconstitute = data['image_reconstitute']\n\n            inputs1 = inputs1.to(device, dtype=torch.float)\n            inputs2 = inputs2.to(device, dtype=torch.float)\n            inputs3 = inputs3.to(device, dtype=torch.float)\n            inputs4 = inputs4.to(device, dtype=torch.float)\n            targets = targets.to(device, dtype=torch.long)\n            image_reconstitute = image_reconstitute.to(device, dtype=torch.float)\n            \n            #forward step to generate prediction\n            outputs = model([inputs1, inputs2, inputs3, inputs4])\n#             encoder_outputs1 = encoder(inputs1)\n#             encoder_outputs2 = encoder(inputs2)\n#             encoder_outputs3 = encoder(inputs3)\n#             encoder_outputs4 = encoder(inputs4)\n#             cats = torch.cat((encoder_outputs1, encoder_outputs2, encoder_outputs3, encoder_outputs4), 1)\n#             decoder_outputs = decoder(cats)\n            \n            #convert targets and outputs to lists\n            targets = targets.detach().cpu().numpy().tolist()\n            # outputs = outputs.detach().cpu().numpy().tolist()\n            \n            #extend the original list\n            final_targets.extend(targets)\n            final_outputs.extend(torch.argmax(outputs['out_pred'], 1).detach().cpu().numpy())\n        \n    return final_outputs, final_targets","e73bca86":"import torchvision.models as models\nimport torch.nn as nn\nimport torch.nn.functional as F","c2c7e538":"class EncoderModel(torch.nn.Module):\n    def __init__(self):\n        super(EncoderModel, self).__init__()\n        self.batch_norm1 = nn.BatchNorm2d(3)\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding='same')\n        self.batch_norm2 = nn.BatchNorm2d(8)\n        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding='same')\n        self.batch_norm3 = nn.BatchNorm2d(16)\n        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=2, stride=1, padding='valid')\n        self.batch_norm4 = nn.BatchNorm2d(32)\n        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='valid')\n        self.batch_norm5 = nn.BatchNorm2d(64)\n        self.conv5 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=1, stride=1, padding='valid')\n        self.batch_norm6 = nn.BatchNorm2d(16)\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.conv1(x)\n        x = self.batch_norm2(x)\n        x = F.max_pool2d(x, 2)\n        x = F.leaky_relu(input=x, negative_slope=0.1, inplace=False)\n        x = self.conv2(x)\n        x = self.batch_norm3(x)\n        x = F.max_pool2d(x, 2)\n        x = F.leaky_relu(input=x, negative_slope=0.1, inplace=False)\n        x = self.conv3(x)\n        x = self.batch_norm4(x)\n        x = F.leaky_relu(input=x, negative_slope=0.1, inplace=False)\n        x = self.conv4(x)\n        x = self.batch_norm5(x)\n        x = F.leaky_relu(input=x, negative_slope=0.1, inplace=False)\n        x = self.conv5(x)\n        x = self.batch_norm6(x)\n        x = F.leaky_relu(input=x, negative_slope=0.1, inplace=False)\n        x = torch.flatten(x, 1)\n        \n        return x\n    \n# def get_encoder_model(pretrained):\n#     if pretrained:\n#         model = models.resnet18(pretrained=True)\n#         # we add the code below to freeze the parameters of the pretrained model\n#         for param in model.parameters():\n#             param.requires_grad = False\n#     else:\n#         model = models.resnet18()\n#     num_ftrs = model.fc.in_features\n    \n#     model.fc = nn.Sequential(\n#         nn.BatchNorm1d(num_ftrs),\n#         nn.Dropout(p=0.25),\n#         #nn.Linear(in_features=num_ftrs, out_features=2048),\n#         nn.ReLU(),\n#         #nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1),\n#         #nn.Dropout(p=0.5),\n#         nn.Linear(in_features=num_ftrs, out_features=8),\n#     )\n#     return model\n    ","f0ed3709":"# class Dense2DSpatialTransformer(nn.Module):\n#     def __init__(self, **kwargs):\n#         super(LambdaLayer, self).__init__(**kwargs)\n        \n#     def build(self, input_shape):\n#         if len(input_shape) > 3:\n#             raise Exception('Spatial Transformer must be called on a list of length 2 or 3. '\n#                            'First argument is the image, second argument is the offset field')\n#         if len(input_shape[1]) != 4 or input_shape[1][1] != 2:\n#             raise Exception('Offset field must be one 4D tensor with 2 channels. '\n#                            'Got: ' + str(input_shape[1]))\n        \n#         self.built = True\n    \n#     def call(self, inputs):\n#         return self._transform(inputs[0], inputs[1][:, :, :, 0],\n#                                inputs[1][:, :, :, 1])\n    \n#     def compute_output_shape(self, input_shape):\n#         return input_shape[0]\n    \n#     def _transform(self, I, dx, dy):\n\n#         batch_size = tf.shape(dx)[0]\n#         height = tf.shape(dx)[1]\n#         width = tf.shape(dx)[2]\n\n#         # Convert dx and dy to absolute locations\n#         x_mesh, y_mesh = self._meshgrid(height, width)\n#         x_mesh = tf.expand_dims(x_mesh, 0)\n#         y_mesh = tf.expand_dims(y_mesh, 0)\n\n#         x_mesh = tf.tile(x_mesh, [batch_size, 1, 1])\n#         y_mesh = tf.tile(y_mesh, [batch_size, 1, 1])\n#         x_new = dx + x_mesh\n#         y_new = dy + y_mesh\n\n#         return self._interpolate(I, x_new, y_new)","c82fa255":"class LambdaLayer(nn.Module):\n    def __init__(self, lambd):\n        super(LambdaLayer, self).__init__()\n        self.lambd = lambd\n    def forward(self, x):\n        return self.lambd(x)","736b9539":"from functools import reduce","e93bb291":"class TileSlideModel(torch.nn.Module):\n    def __init__(self):\n        super(TileSlideModel, self).__init__()\n        self.full_X = 32\n        self.full_Y = 32\n        self.full_D = 3\n        self.pad = 16\n        self.zero_pad = nn.ZeroPad2d(self.pad\/\/2)\n        self.linear1 = nn.Linear(144, 24)\n        self.linear2 = nn.Linear(24, 3)\n        self.lambda_ = LambdaLayer(lambda x: x *(self.pad-1)\/2.0)\n        \n#         self.dens_st = Dense2DSpatialTransformer()\n        self.upsample = nn.Upsample(scale_factor=32, mode='nearest')\n        \n    def forward(self, heterogeneous_list):\n        pad_layer = self.zero_pad(heterogeneous_list[0])\n        comb_feat = torch.cat(tuple(heterogeneous_list[1:]), 1)\n        order_plus_tile_layer = self.linear1(comb_feat)\n        pos_xy = torch.tanh(self.linear2(order_plus_tile_layer))\n        pos_xy = self.lambda_(pos_xy)\n#         print(pos_xy.size())\n        \n        pos_xy_field = torch.reshape(pos_xy, (-1, 3, 1, 1))\n#         print('pos_xy shape before upsampling: ', pos_xy_field.size())\n        pos_xy_full = self.upsample(pos_xy_field)\n#         print('pos_xy shape after upsampling: ', pos_xy_full.size())\n#         recon_image = self.dens_st([pad_layer, pos_xy_full])\n        inter = (pad_layer, pos_xy_full)\n        \n        recon_image = reduce(torch.add, inter)\n#         print('final reconstruction image shape: ',recon_image.size())\n        return recon_image","834aa601":"encoder = EncoderModel()\ntile_slide_model = TileSlideModel()\nclass Model(torch.nn.Module):\n    def __init__(self): \n        super(Model, self).__init__()\n        #self.enc = get_encoder_model(pretrained=True)\n        self.enc = encoder \n        self.tile_slide = tile_slide_model\n        self.dropout = nn.Dropout(p=0.5) \n        self.linear = nn.Linear(64, 128) \n        self.random_perm = nn.Linear(128, 24)\n\n    def forward(self, x_list):\n        encodes = []\n        for item in x_list:\n            encodes.append(self.enc(item))\n        encodes = tuple(encodes)\n        feat_cat = torch.cat(encodes, 1)\n        feat_dr = self.dropout(feat_cat)\n        feat_latent = self.linear(feat_dr)\n        feat_latent_dr = self.dropout(feat_latent)\n        out_pred = self.random_perm(feat_latent_dr)\n        \n        feat_latent_slide = self.dropout(self.linear(feat_dr)) # vecteur de dimension 64\n        \n        # use the tile slide model on each tile\n        big_image = []\n        for c_img, c_feat in zip(x_list, encodes):\n            big_image.append(self.tile_slide([c_img, feat_latent_slide, c_feat])) # c_feat dim=16\n        recon_image = reduce(torch.add, big_image)\n        \n        return {\n            'out_pred': out_pred,\n            'recon_image': recon_image,\n        }","810cb77a":"import os\nimport pandas as pd\nimport numpy as np\nimport albumentations\nimport torch\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\n","19e3ffc2":"input_dir = '..\/input\/igad2021datachallenge\/Challenge 2 - Jigsaw solving\/data'\ntrain_dir = os.path.join(input_dir, 'train')\ntest_dir = os.path.join(input_dir, 'test')","04622b16":"device = 'cuda'\nepochs = 25","95474e2a":"df_train = pd.read_csv('..\/input\/train-csv\/train.csv', dtype={\n    'label': 'str'\n})\ndf_train.head()","4f5e5557":"df_train.info()","cc2fd78c":"df_train['label'].unique()","73d4e10b":"label_mapping = {\n    '2031': 0,\n    '1023': 1,\n    '1203' : 2,\n    '3012': 3,\n    '3120': 4,\n    '2130': 5,\n    '0123': 6,\n    '1032': 7,\n    '2310': 8,\n    '3210': 9,\n    '1320': 10,\n    '2103': 11,\n    '1230': 12,\n    '0321': 13,\n    '0213' : 14,\n    '3201': 15,\n    '2301': 16,\n    '1302': 17,\n    '2013': 18,\n    '0132': 19,\n    '3102': 20,\n    '3021': 21,\n    '0231': 22,\n    '0312': 23\n}","a5060bab":"df_train['label_encode'] = -1\ndf_train.loc[:, 'label_encode'] = df_train.label.map(label_mapping)","e43faaa5":"df_train.head()","5e9efac1":"# fetch all image ids\nimages1 = df_train.image1.values.tolist()\nimages2 = df_train.image2.values.tolist()\nimages3 = df_train.image3.values.tolist()\nimages4 = df_train.image4.values.tolist()\n\n# a list with image locations\nimages1 = [\n    os.path.join(train_dir, row['dossier'], row['image1']) for index, row in tqdm(\n    df_train.iterrows(), total=len(df_train), desc='processing images1')\n]\n\nimages2 = [\n    os.path.join(train_dir, row['dossier'], row['image2']) for index, row in tqdm(\n    df_train.iterrows(), total=len(df_train), desc='processing images2')\n]\n\nimages3 = [\n    os.path.join(train_dir, row['dossier'], row['image3']) for index, row in tqdm(\n    df_train.iterrows(), total=len(df_train), desc='processing images3')\n]\n\nimages4 = [\n    os.path.join(train_dir, row['dossier'], row['image4']) for index, row in tqdm(\n    df_train.iterrows(), total=len(df_train), desc='processing images4')\n]","03fcf2d1":"targets = df_train.label_encode.values\npermutation_targets = df_train.label.values","9c2cbc9a":"model = Model()","a5eacd04":"model.to(device)","b808d57f":"mean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)","bbe08785":"aug = albumentations.Compose(\n    [\n        albumentations.Normalize(\n            mean, std, max_pixel_value=255.0, always_apply=True\n        )\n    ]\n)","1f7084f7":"train_images1, valid_images1, train_images2, valid_images2,train_images3, valid_images3,train_images4, valid_images4, train_targets, valid_targets, train_permutation_targets, valid_permutation_targets = train_test_split(\n    images1, images2, images3, images4, targets, permutation_targets, stratify=targets, random_state=42\n)","e63efc14":"# fetch the ClassificationDataset class\ntrain_dataset = ClassificationDataset(\n    image1_paths = train_images1,\n    image2_paths = train_images2,\n    image3_paths = train_images3,\n    image4_paths = train_images4,\n    permutation_targets = permutation_targets,\n    targets = train_targets,\n    resize = (16, 16),\n    augmentations = aug,\n)","8e6aa658":"# torch dataloader creates batches of data\n# from classification dataset class\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=64, shuffle=True, num_workers=2\n)","993d3b57":"valid_dataset = ClassificationDataset(\n    image1_paths = valid_images1,\n    image2_paths = valid_images2,\n    image3_paths = valid_images3,\n    image4_paths = valid_images4,\n    permutation_targets = permutation_targets,\n    targets = valid_targets,\n    resize = (16, 16),\n    augmentations = aug,\n)","b3466905":"valid_loader = torch.utils.data.DataLoader(\n    valid_dataset, batch_size=64, shuffle=False, num_workers=2\n)","791259fc":"# # simple Adam optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)","be75a2c0":"from torch.optim.lr_scheduler import ReduceLROnPlateau\nscheduler = ReduceLROnPlateau(optimizer=optimizer, mode='max', factor=0.1, patience=1,\n                             threshold=1e-7)\n# train and print auc score for all epochs\nfor epoch in range(epochs):\n    train(train_loader, model, optimizer, device=device)\n    predictions, valid_targets = evaluate(\n        valid_loader, model, device=device\n    )\n    roc_auc = metrics.accuracy_score(valid_targets, predictions)\n    scheduler.step(roc_auc)\n    print(\n        f\"Epoch={epoch}, ACCURACY={roc_auc}\"\n    )","d492bf10":"import seaborn as sns","edae628d":"# input_dir = '..\/input\/igad2021datachallenge\/Challenge 2 - Jigsaw solving\/data'\n# train_dir = os.path.join(input_dir, 'train')\n# test_dir = os.path.join(input_dir, 'test')","9e92fe9e":"# def buid_dataframe(directory, train=True):\n#     dataframe = {\n#         'dossier': [],\n#         'image1': [],\n#         'image2': [],\n#         'image3': [],\n#         'image4': [],\n#     }\n#     if train:\n#         dataframe['label'] = []\n#         all_dirs = os.listdir(directory)\n#         for direc in all_dirs:\n#             dataframe['dossier'].append(direc)\n#             for file in os.listdir(os.path.join(directory, direc)):\n#                 if file.endswith('0.png'):\n#                     dataframe['image1'].append(file)\n#                 elif file.endswith('1.png'):\n#                     dataframe['image2'].append(file)\n#                 elif file.endswith('2.png'):\n#                     dataframe['image3'].append(file)\n#                 elif file.endswith('3.png'):\n#                     dataframe['image4'].append(file)\n#                 else:\n#                     with open(os.path.join(directory, direc, file), 'r') as f:\n#                         line = f.readlines()[0]\n#                         dataframe['label'].append(line)\n#     else:\n#         all_dirs = os.listdir(directory)\n#         for direc in all_dirs:\n#             dataframe['dossier'].append(direc)\n#             for file in os.listdir(os.path.join(directory, direc)):\n#                 if file.endswith('0.png'):\n#                     dataframe['image1'].append(file)\n#                 elif file.endswith('1.png'):\n#                     dataframe['image2'].append(file)\n#                 elif file.endswith('2.png'):\n#                     dataframe['image3'].append(file)\n#                 elif file.endswith('3.png'):\n#                     dataframe['image4'].append(file)\n    \n#     df = pd.DataFrame.from_dict(dataframe)\n#     return df","8298905a":"# df_train = buid_dataframe(train_dir)\n# df_train.head()","8e9c110f":"# df_test = buid_dataframe(test_dir, train=False)\n# df_test.head()","27b932cb":"# df_train.to_csv('train.csv')","26df5ea2":"# df_test.to_csv('test.csv')","ff811413":"**Now we can start training**","f33aca94":"## encoder = EncoderModel()\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        #self.enc = get_encoder_model(pretrained=True)\n        self.enc = encoder\n        self.dropout = nn.Dropout(p=0.5)\n        self.linear = nn.Linear(32, 64)\n        self.random_perm = nn.Linear(64, 24)\n\n    def forward(self, x_list):\n        encodes = []\n        for item in x_list:\n            encodes.append(self.enc(item))\n        encodes = tuple(encodes)\n        x = torch.cat(encodes, 1)\n        x = self.dropout(x)\n        x = self.linear(x)\n        x = self.dropout(x)\n        out = self.random_perm(x)\n        return out","fce080e5":"**In this second version, we start to build a model of our own to train**"}}