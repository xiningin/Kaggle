{"cell_type":{"261d30a6":"code","41a569a7":"code","aa9abc46":"code","af8ea17b":"code","962cb0ed":"code","c99c8100":"code","8dacda40":"code","7912f377":"code","b32ba669":"code","e4eeb7af":"code","ec4dc655":"code","6de1fdf0":"code","a25faef4":"code","a4f90d08":"code","02cdcea6":"code","c8257df8":"code","61c569d5":"code","3200aff4":"code","d4539589":"code","ba34aca7":"code","8f0e99f8":"code","2cac79bd":"code","47eb3838":"code","c1c3ca90":"code","50461e90":"code","2d275dcd":"code","fc701a91":"code","094a4832":"code","0569d4b6":"code","6b315c98":"code","679dd829":"code","c30b66a4":"code","d098a70a":"code","fc237863":"code","59065290":"code","6eeeac2b":"code","b59cfd21":"code","0c970862":"code","ffa33d02":"code","cb9e6a1c":"code","9175c4ee":"code","4b26e894":"code","ebaed4b5":"code","1f766709":"code","11d78f68":"code","bedba107":"code","753afc11":"code","b1c859fa":"code","4b836197":"code","111b73d4":"code","54fab3cb":"code","cea4a545":"code","b3a18f9d":"code","c38c1336":"code","72e7c45c":"code","5c2f084e":"code","775cb7a4":"code","7a6f7a24":"code","edc225b2":"code","4cd42149":"code","f83a5961":"code","4db20f0a":"code","e5a918d2":"code","a7274777":"code","27b66e05":"code","a51e7a42":"code","f76bb115":"code","278e9272":"code","bc74bf5d":"code","6a9a1e28":"code","55d66868":"code","eef77270":"code","2c759662":"code","d3cee609":"code","3a7b1254":"code","9cb888a1":"code","b9bd54e2":"code","b59f893a":"markdown","05d349f6":"markdown","4f79c916":"markdown","757e4849":"markdown","1a5c867f":"markdown","4eceae7b":"markdown","af7b988f":"markdown","8092f4d5":"markdown","76bd5d0b":"markdown","03a4c1f2":"markdown","1fe97241":"markdown","b0d2a3ba":"markdown","c35291d2":"markdown","076fd7a4":"markdown","ffabf531":"markdown","bb0a7e11":"markdown","8c609bcb":"markdown","c07b7980":"markdown","091a02eb":"markdown","b424831e":"markdown","6a34dd50":"markdown","b4adcd9a":"markdown","88dbe3e4":"markdown","84f99aa7":"markdown","86cae9c3":"markdown","417b12fb":"markdown","b3bb5573":"markdown","98f26321":"markdown","6dd7c790":"markdown","c44c292b":"markdown","093156ea":"markdown","3745098b":"markdown","0e91263e":"markdown","04bb0acc":"markdown","b89f73d7":"markdown","924e6e7d":"markdown","ca409b41":"markdown","359ed953":"markdown","d75bed21":"markdown","9789ad72":"markdown","b3a80163":"markdown","83a8e5b7":"markdown","7ea1ec58":"markdown","a8386f7d":"markdown","f4477328":"markdown","8b986e00":"markdown","bb2e47b5":"markdown","ab817417":"markdown","1b871990":"markdown","8dee84a3":"markdown","d5817d8b":"markdown","9e034900":"markdown","1185c3a7":"markdown","c91917e8":"markdown","d67d4dc9":"markdown","6254fce5":"markdown","e77a4d14":"markdown","0b9162a7":"markdown","0321f56e":"markdown","61839955":"markdown","a14c8c6b":"markdown","11fa5e88":"markdown","57ab1f9f":"markdown","26348a22":"markdown","f3900e1b":"markdown","fe49e2cc":"markdown","fbc52422":"markdown"},"source":{"261d30a6":"from sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nimport matplotlib.pylab as pylab\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport matplotlib as mpl\nimport xgboost as xgb\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport json\nimport sys\nimport csv\nimport os","41a569a7":"print('matplotlib: {}'.format(matplotlib.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))","aa9abc46":"sns.set(style='white', context='notebook', palette='deep')\npylab.rcParams['figure.figsize'] = 12,8\nwarnings.filterwarnings('ignore')\nmpl.style.use('ggplot')\nsns.set_style('white')\n%matplotlib inline","af8ea17b":"# import train and test to play with it\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","962cb0ed":"type(df_train)","c99c8100":"type(df_test)","8dacda40":"# Modify the graph above by assigning each species an individual color.\ng = sns.FacetGrid(df_train, hue=\"Survived\", col=\"Pclass\", margin_titles=True,\n                  palette={1:\"seagreen\", 0:\"gray\"})\ng=g.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend();","7912f377":"df_train.plot(kind='scatter', x='Age', y='Fare',alpha = 0.5,color = 'red')","b32ba669":"#show scatter plot with using Matplotlib\nplt.figure(figsize=(8,6))\nplt.scatter(range(df_train.shape[0]), np.sort(df_train['Age'].values))\nplt.xlabel('index')\nplt.ylabel('Age')\nplt.title('Explore: Age')\nplt.show()","e4eeb7af":"ax= sns.boxplot(x=\"Pclass\", y=\"Age\", data=df_train)\nax= sns.stripplot(x=\"Pclass\", y=\"Age\", data=df_train, jitter=True, edgecolor=\"gray\")\nplt.show()","ec4dc655":"# histograms\ndf_train.hist(figsize=(15,20));\nplt.figure();","6de1fdf0":"df_train[\"Age\"].hist();","a25faef4":"df_train.Age.plot(kind = 'hist',bins = 5);","a4f90d08":"f,ax=plt.subplots(1,2,figsize=(20,10))\ndf_train[df_train['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')\nax[0].set_title('Survived= 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ndf_train[df_train['Survived']==1].Age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')\nax[1].set_title('Survived= 1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","02cdcea6":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndf_train['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=df_train,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","c8257df8":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndf_train[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex',hue='Survived',data=df_train,ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","61c569d5":"sns.countplot('Pclass', hue='Survived', data=df_train)\nplt.title('Pclass: Sruvived vs Dead')\nplt.show()","3200aff4":"# scatter plot matrix\npd.plotting.scatter_matrix(df_train,figsize=(10,10))\nplt.figure();","d4539589":"# violinplots on petal-length for each species\nsns.violinplot(data=df_train,x=\"Sex\", y=\"Age\")","ba34aca7":"f,ax=plt.subplots(1,2,figsize=(18,8))\nsns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=df_train,split=True,ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\nsns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=df_train,split=True,ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nplt.show()","8f0e99f8":"# Using seaborn pairplot to see the bivariate relation between each pair of features\nsns.pairplot(data=df_train[[\"Fare\",\"Survived\",\"Age\",\"Pclass\"]],\n             hue=\"Survived\", dropna=True);","2cac79bd":"sns.FacetGrid(df_train, hue=\"Survived\", size=5).map(sns.kdeplot, \"Fare\").add_legend()\nplt.show();","47eb3838":"sns.jointplot(x='Fare',y='Age',data=df_train);","c1c3ca90":"sns.jointplot(x='Fare',y='Age' ,data=df_train, kind='reg');","50461e90":"sns.swarmplot(x='Pclass',y='Age',data=df_train);","2d275dcd":"plt.figure(figsize=(7,4)) \nsns.heatmap(df_train.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show();","fc701a91":"plt.imshow(df_train.corr(), cmap='hot', interpolation='nearest')\nplt.show()","094a4832":"df_train['Pclass'].value_counts().plot(kind=\"bar\");","0569d4b6":"sns.factorplot('Pclass','Survived',hue='Sex',data=df_train)\nplt.show();","6b315c98":"sns.factorplot('SibSp','Survived',hue='Pclass',data=df_train)\nplt.show()","679dd829":"#let's see some others factorplot\nf,ax=plt.subplots(1,2,figsize=(20,8))\nsns.barplot('SibSp','Survived', data=df_train,ax=ax[0])\nax[0].set_title('SipSp vs Survived in BarPlot')\nsns.factorplot('SibSp','Survived', data=df_train,ax=ax[1])\nax[1].set_title('SibSp vs Survived in FactorPlot')\nplt.close(2)\nplt.show();","c30b66a4":"f,ax=plt.subplots(1,3,figsize=(20,8))\nsns.distplot(df_train[df_train['Pclass']==1].Fare,ax=ax[0])\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(df_train[df_train['Pclass']==2].Fare,ax=ax[1])\nax[1].set_title('Fares in Pclass 2')\nsns.distplot(df_train[df_train['Pclass']==3].Fare,ax=ax[2])\nax[2].set_title('Fares in Pclass 3')\nplt.show()","d098a70a":"# shape\nprint(df_train.shape)","fc237863":"#columns*rows\ndf_train.size","59065290":"##df_train.isnull().sum()","6eeeac2b":"def check_missing_data(df):\n    flag=df.isna().sum().any()\n    if flag==True:\n        total = df.isnull().sum()\n        percent = (df.isnull().sum())\/(df.isnull().count()*100)\n        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n        data_type = []\n        # written by MJ Bahmani\n        for col in df.columns:\n            dtype = str(df[col].dtype)\n            data_type.append(dtype)\n        output['Types'] = data_type\n        return(np.transpose(output))\n    else:\n        return(False)","b59cfd21":"check_missing_data(df_train)","0c970862":"check_missing_data(df_test)","ffa33d02":"# remove rows that have NA's\n#train = train.dropna()","cb9e6a1c":"print(df_train.shape)","9175c4ee":"print(df_train.info())","4b26e894":"df_train['Age'].unique()","ebaed4b5":"df_train[\"Pclass\"].value_counts()\n","1f766709":"df_train.head(5) ","11d78f68":"df_train.tail() ","bedba107":"df_train.sample(5) ","753afc11":"df_train.describe() ","b1c859fa":"df_train.isnull().sum()","4b836197":"df_train.groupby('Survived').count()","111b73d4":"df_train.columns","54fab3cb":"df_train.where(df_train['Age']==30).head(2)","cea4a545":"df_train[df_train['Age']==30]","b3a18f9d":"X = df_train.iloc[:, :-1].values\ny = df_train.iloc[:, -1].values","c38c1336":"\ndef simplify_ages(df):\n    df.Age = df.Age.fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n    categories = pd.cut(df.Age, bins, labels=group_names)\n    df.Age = categories\n    return df\n\ndef simplify_cabins(df):\n    df.Cabin = df.Cabin.fillna('N')\n    df.Cabin = df.Cabin.apply(lambda x: x[0])\n    return df\n\ndef simplify_fares(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    group_names = ['Unknown', '1_quartile', '2_quartile', '3_quartile', '4_quartile']\n    categories = pd.cut(df.Fare, bins, labels=group_names)\n    df.Fare = categories\n    return df\n\ndef format_name(df):\n    df['Lname'] = df.Name.apply(lambda x: x.split(' ')[0])\n    df['NamePrefix'] = df.Name.apply(lambda x: x.split(' ')[1])\n    return df    \n    \ndef drop_features(df):\n    return df.drop(['Ticket', 'Name', 'Embarked'], axis=1)\n\ndef transform_features(df):\n    df = simplify_ages(df)\n    df = simplify_cabins(df)\n    df = simplify_fares(df)\n    df = format_name(df)\n    df = drop_features(df)\n    return df\n\ndf_train = transform_features(df_train)\ndf_test = transform_features(df_test)\ndf_train.head()","72e7c45c":"def encode_features(df_train, df_test):\n    features = ['Fare', 'Cabin', 'Age', 'Sex', 'Lname', 'NamePrefix']\n    df_combined = pd.concat([df_train[features], df_test[features]])\n    \n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(df_combined[feature])\n        df_train[feature] = le.transform(df_train[feature])\n        df_test[feature] = le.transform(df_test[feature])\n    return df_train, df_test","5c2f084e":"#Encode Dataset\ndf_train, df_test = encode_features(df_train, df_test)\ndf_train.head()","775cb7a4":"df_test.head()","7a6f7a24":"x_all = df_train.drop(['Survived', 'PassengerId'], axis=1)\ny_all = df_train['Survived']","edc225b2":"num_test = 0.3\nX_train, X_test, y_train, y_test = train_test_split(x_all, y_all, test_size=num_test, random_state=100)","4cd42149":"result=None","f83a5961":"# Choose the type of classifier. \nrfc = RandomForestClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [4, 6, 9], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(rfc, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nrfc = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nrfc.fit(X_train, y_train)","4db20f0a":"rfc_prediction = rfc.predict(X_test)\nrfc_score=accuracy_score(y_test, rfc_prediction)\nprint(rfc_score)","e5a918d2":"xgboost = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train, y_train)","a7274777":"xgb_prediction = xgboost.predict(X_test)\nxgb_score=accuracy_score(y_test, xgb_prediction)\nprint(xgb_score)","27b66e05":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","a51e7a42":"logreg_prediction = logreg.predict(X_test)\nlogreg_score=accuracy_score(y_test, logreg_prediction)\nprint(logreg_score)\n","f76bb115":"from sklearn.tree import DecisionTreeRegressor\n\n# Define model. Specify a number for random_state to ensure same results each run\ndt = DecisionTreeRegressor(random_state=1)\n\n","278e9272":"# Fit model\ndt.fit(X_train, y_train)","bc74bf5d":"dt_prediction = dt.predict(X_test)\ndt_score=accuracy_score(y_test, dt_prediction)\nprint(dt_score)","6a9a1e28":"from sklearn.tree import ExtraTreeRegressor\n# Define model. Specify a number for random_state to ensure same results each run\netr = ExtraTreeRegressor()","55d66868":"# Fit model\netr.fit(X_train, y_train)","eef77270":"etr_prediction = etr.predict(X_test)\netr_score=accuracy_score(y_test, etr_prediction)\nprint(etr_score)","2c759662":"X_train = df_train.drop(\"Survived\",axis=1)\ny_train = df_train[\"Survived\"]","d3cee609":"X_train = X_train.drop(\"PassengerId\",axis=1)\nX_test  = df_test.drop(\"PassengerId\",axis=1)","3a7b1254":"xgboost = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train, y_train)","9cb888a1":"Y_pred = xgboost.predict(X_test)","b9bd54e2":"submission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)","b59f893a":"<a id=\"75\"><\/a> <br>\n## 7-5 XGBoost\n[XGBoost](https:\/\/en.wikipedia.org\/wiki\/XGBoost) is an open-source software library which provides a gradient boosting framework for C++, Java, Python, R, and Julia. it aims to provide a \"Scalable, Portable and Distributed Gradient Boosting (GBM, GBRT, GBDT) Library\". ","05d349f6":"<a id=\"9\"><\/a> <br>\n\n-----------\n\n# 9- References\n1. [https:\/\/skymind.ai\/wiki\/machine-learning-workflow](https:\/\/skymind.ai\/wiki\/machine-learning-workflow)\n\n1. [Problem-define](https:\/\/machinelearningmastery.com\/machine-learning-in-python-step-by-step\/)\n\n1. [Sklearn](http:\/\/scikit-learn.org\/)\n\n1. [machine-learning-in-python-step-by-step](https:\/\/machinelearningmastery.com\/machine-learning-in-python-step-by-step\/)\n\n1. [Data Cleaning](http:\/\/wp.sigmod.org\/?p=2288)\n\n1. [competitive data science](https:\/\/www.coursera.org\/learn\/competitive-data-science\/)\n\n1. [Machine Learning Certification by Stanford University (Coursera)](https:\/\/www.coursera.org\/learn\/machine-learning\/)\n\n1. [Machine Learning A-Z\u2122: Hands-On Python & R In Data Science (Udemy)](https:\/\/www.udemy.com\/machinelearning\/)\n\n1. [Deep Learning Certification by Andrew Ng from deeplearning.ai (Coursera)](https:\/\/www.coursera.org\/specializations\/deep-learning)\n\n1. [Python for Data Science and Machine Learning Bootcamp (Udemy)](Python for Data Science and Machine Learning Bootcamp (Udemy))\n\n1. [Mathematics for Machine Learning by Imperial College London](https:\/\/www.coursera.org\/specializations\/mathematics-machine-learning)\n\n1. [Deep Learning A-Z\u2122: Hands-On Artificial Neural Networks](https:\/\/www.udemy.com\/deeplearning\/)\n\n1. [Complete Guide to TensorFlow for Deep Learning Tutorial with Python](https:\/\/www.udemy.com\/complete-guide-to-tensorflow-for-deep-learning-with-python\/)\n\n1. [Data Science and Machine Learning Tutorial with Python \u2013 Hands On](https:\/\/www.udemy.com\/data-science-and-machine-learning-with-python-hands-on\/)\n\n1. [Machine Learning Certification by University of Washington](https:\/\/www.coursera.org\/specializations\/machine-learning)\n\n1. [Data Science and Machine Learning Bootcamp with R](https:\/\/www.udemy.com\/data-science-and-machine-learning-bootcamp-with-r\/)\n\n1. [Creative Applications of Deep Learning with TensorFlow](https:\/\/www.class-central.com\/course\/kadenze-creative-applications-of-deep-learning-with-tensorflow-6679)\n\n1. [Neural Networks for Machine Learning](https:\/\/www.class-central.com\/mooc\/398\/coursera-neural-networks-for-machine-learning)\n\n1. [Practical Deep Learning For Coders, Part 1](https:\/\/www.class-central.com\/mooc\/7887\/practical-deep-learning-for-coders-part-1)\n\n1. [Machine Learning](https:\/\/www.cs.ox.ac.uk\/teaching\/courses\/2014-2015\/ml\/index.html)\n\n1. [https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic](https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic)\n\n1. [https:\/\/www.kaggle.com\/mrisdal\/exploring-survival-on-the-titanic](https:\/\/www.kaggle.com\/mrisdal\/exploring-survival-on-the-titanic)\n\n1. [https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling)\n\n1. [https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy)\n\n1. [https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n\n1. [Top 28 Cheat Sheets for Machine Learning](https:\/\/www.analyticsvidhya.com\/blog\/2017\/02\/top-28-cheat-sheets-for-machine-learning-data-science-probability-sql-big-data\/)\n1. [xenonstack](https:\/\/www.xenonstack.com\/blog\/data-science\/preparation-wrangling-machine-learning-deep\/)\n1. [towardsdatascience](https:\/\/towardsdatascience.com\/encoding-categorical-features-21a2651a065c)\n1. [train-test-split-and-cross-validation](https:\/\/towardsdatascience.com\/train-test-split-and-cross-validation-in-python-80b61beca4b6)\n1. [what-is-underfitting-and-overfitting](https:\/\/medium.com\/greyatom\/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76)\n1. [permutation-importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance)\n1. [partial-plots](https:\/\/www.kaggle.com\/dansbecker\/partial-plots)\n-------------\n\n###### [Go to top](#top)","4f79c916":"<a id=\"77\"><\/a> <br>\n## 7-7 DecisionTreeRegressor\nThe function to measure the quality of a split. Supported criteria are \u201cmse\u201d for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, \u201cfriedman_mse\u201d, which uses mean squared error with Friedman\u2019s improvement score for potential splits, and \u201cmae\u201d for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.","757e4849":"<a id=\"710\"><\/a> <br>\n## 7-10 How Do I Submit?\n1. Fork and Commit this Kernel.\n1. Then navigate to the Output tab of the Kernel and \"Submit to Competition\".","1a5c867f":"<a id=\"62\"><\/a> <br>\n## 6-2 Visualization\n**Data visualization**  is the presentation of data in a pictorial or graphical format. It enables decision makers to see analytics presented visually, so they can grasp difficult concepts or identify new patterns.\n\nWith interactive visualization, you can take the concept a step further by using technology to drill down into charts and graphs for more detail, interactively changing what data you see and how it\u2019s processed.[SAS]\n\n In this section I show you  **11 plots** with **matplotlib** and **seaborn** that is listed in the blew picture:\n <img src=\"http:\/\/s8.picofile.com\/file\/8338475500\/visualization.jpg\" width=400 height=400 \/>\n\n###### [Go to top](#top)","4eceae7b":"Seperating the data into dependent and independent variables.","af7b988f":"You can Fork and Run this kernel on Github:\n> ###### [ GitHub](https:\/\/github.com\/mjbahmani\/Machine-Learning-Workflow-with-Python)\n\n--------------------------------------\n\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated** ","8092f4d5":"### 5-5-2 Version","76bd5d0b":"### 5-5-2 Setup\n\nA few tiny adjustments for better **code readability**","03a4c1f2":">  <font color=\"red\"><b>Note:<\/b><\/font>\nhow many NA elements in every column\n","1fe97241":"<a id=\"76\"><\/a> <br>\n## 7-6 Logistic Regression\nThe logistic model  is a widely used statistical model that, in its basic form, uses a logistic function to model a binary dependent variable; many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model","b0d2a3ba":"<a id=\"621\"><\/a> <br>\n### 6-2-1 Scatter Plot\n\n[Scatter plot](https:\/\/en.wikipedia.org\/wiki\/Scatter_plot) Purpose to identify the type of relationship (if any) between two quantitative variables.\n\n\n","c35291d2":"<a id=\"628\"><\/a> <br>\n### 6-2-8 jointplot","076fd7a4":"-----------------\n<a id=\"8\"><\/a> <br>\n# 8- Conclusion\nI have tried to cover all the parts related to the process of **Machine Learning** with a variety of python packages and I know that there are still some problems then I hope to get your feedback to improve it.","ffabf531":"It looks like perhaps two of the input variables have a Gaussian distribution. This is useful to note as we can use algorithms that can exploit this assumption.","bb0a7e11":"As you can see in the below in python, it is so easy perform some query on the dataframe:","8c609bcb":"<a id=\"63\"><\/a> <br>\n## 6-3 Data Preprocessing\n**Data preprocessing** refers to the transformations applied to our data before feeding it to the algorithm.\n \nData Preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.\nthere are plenty of steps for data preprocessing and **we just listed some of them** :\n* removing Target column (id)\n* Sampling (without replacement)\n* Dealing with Imbalanced Data\n* Introducing missing values and treating them (replacing by average values)\n* Noise filtering\n* Data discretization\n* Normalization and standardization\n* PCA analysis\n* Feature selection (filter, embedded, wrapper)\n\n###### [Go to top](#top)","c07b7980":"\nWe can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property.\n\nYou should see **891** instances and **12** attributes:","091a02eb":"<a id=\"624\"><\/a> <br>\n### 6-2-4 Multivariate Plots\nNow we can look at the interactions between the variables.\n\nFirst, let\u2019s look at scatterplots of all pairs of attributes. This can be helpful to spot structured relationships between input variables.","b424831e":"### 6-2-12 Conclusion\nWe have used Python to apply data visualization tools to theTitanic dataset.","6a34dd50":"Note the diagonal grouping of some pairs of attributes. This suggests a high correlation and a predictable relationship.","b4adcd9a":"### 6-2-12 Factorplot","88dbe3e4":"<a id=\"631\"><\/a> <br>\n## 6-3-1 Features\nFeatures:\n* numeric\n* categorical\n* ordinal\n* datetime\n* coordinates\n\n### Find the type of features in titanic dataset:\n<img src=\"http:\/\/s9.picofile.com\/file\/8339959442\/titanic.png\" height=\"700\" width=\"600\" \/>","84f99aa7":"To check out how many null info are on the dataset, we can use **isnull().sum()","86cae9c3":"Prepare X(features) , y(target)","417b12fb":"You can change your model and submit the results of other models","b3bb5573":"<a id=\"622\"><\/a> <br>\n### 6-2-2 Box\nIn descriptive statistics, a **box plot** or boxplot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram.[wikipedia]","98f26321":"<a id=\"632\"><\/a> <br>\n## 6-3-2 Explorer Dataset\n1- Dimensions of the dataset.\n\n2- Peek at the data itself.\n\n3- Statistical summary of all attributes.\n\n4- Breakdown of the data by the class variable.[7]\n\nDon\u2019t worry, each look at the data is **one command**. These are useful commands that you can use again and again on future projects.\n\n###### [Go to top](#top)","6dd7c790":"<a id=\"751\"><\/a> <br>\n## 7-5-1 Prediction","c44c292b":"###  6-2-11 Bar Plot","093156ea":"<a id=\"627\"><\/a> <br>\n###  6-2-7 kdeplot","3745098b":"<a id=\"761\"><\/a> <br>\n## 7-6-1 Prediction","0e91263e":"We can also replace the histograms shown in the diagonal of the pairplot by kde.","04bb0acc":">  <font color=\"red\"><b>Note:<\/b><\/font>\nIn pandas's data frame you can perform some query such as \"where\".","b89f73d7":"Go to first step: [**Course Home Page**](https:\/\/www.kaggle.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\nGo to next step : [**Mathematics and Linear Algebra**](https:\/\/www.kaggle.com\/mjbahmani\/linear-algebra-for-data-scientists)\n","924e6e7d":"><font color=\"red\"><b>Note: <\/b><\/font>\n\n* Each **row** is an observation (also known as : sample, example, instance, record)\n* Each **column** is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)","ca409b41":"\n<a id=\"71\"><\/a> <br>\n## 7-1 Families of ML algorithms\nThere are several categories for machine learning algorithms, below are some of these categories:\n* Linear\n    * Linear Regression\n    * Logistic Regression\n    * Support Vector Machines\n* Tree-Based\n    * Decision Tree\n    * Random Forest\n    * GBDT\n* KNN\n* Neural Networks\n\n-----------------------------\nAnd if we  want to categorize ML algorithms with the type of learning, there are below type:\n* Classification\n\n    * k-Nearest Neighbors\n    * LinearRegression\n    * SVM\n    * DT \n    * NN\n    \n* clustering\n\n    * K-means\n    * HCA\n    * Expectation Maximization\n    \n* Visualization and\tdimensionality \treduction:\n\n    * Principal Component Analysis(PCA)\n    * Kernel PCA\n    * Locally -Linear\tEmbedding \t(LLE)\n    * t-distributed\tStochastic\tNeighborEmbedding \t(t-SNE)\n    \n* Association rule learning\n\n    * Apriori\n    * Eclat\n* Semisupervised learning\n* Reinforcement Learning\n    * Q-learning\n* Batch learning & Online learning\n* Ensemble  Learning\n\n>  <font color=\"red\"><b>Note:<\/b><\/font>\nHere is no method which outperforms all others for all tasks\n\n###### [Go to top](#top)","359ed953":">  <font color=\"red\"><b>Note:<\/b><\/font>\nfor getting some information about the dataset you can use **info()** command","d75bed21":"To give a statistical summary about the dataset, we can use **describe()","9789ad72":"<a id=\"61\"><\/a> <br>\n## 6-1 Data Collection\n**Data collection** is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.[techopedia]\n<br>\nI start Collection Data by the training and testing datasets into Pandas DataFrames.\n###### [Go to top](#top)","b3a80163":"<a id=\"625\"><\/a> <br>\n### 6-2-5 Violinplots\nA violin plot plays a similar role as a box and whisker plot. It shows the distribution of quantitative data across several levels of one (or more) categorical variables such that those distributions can be compared. Unlike a box plot, in which all of the plot components correspond to actual datapoints, the violin plot features a kernel density estimation of the underlying distribution.[link](https:\/\/seaborn.pydata.org\/generated\/seaborn.violinplot.html)","83a8e5b7":">  <font color=\"red\"><b>Note:<\/b><\/font>\nyou see number of unique item for **Age** and **Pclass** with command below:","7ea1ec58":"<a id=\"741\"><\/a> <br>\n## 7-4-1 Prediction","a8386f7d":"To check the first 5 rows of the data set, we can use head(5).","f4477328":"<a id=\"72\"><\/a> <br>\n## 7-2 Prepare Features & Targets\nFirst of all seperating the data into independent(Feature) and dependent(Target) variables.\n\n>  <font color=\"red\"><b>Note:<\/b><\/font>\n* X==>> Feature - independent\n* y==>> Target    - dependent","8b986e00":"<a id=\"6210\"><\/a> <br>\n### 6-2-10 Heatmap\nA heatmap is a two-dimensional graphical representation of data where the individual values that are contained in a matrix are represented as colors.[link](https:\/\/blog.quantinsti.com\/creating-heatmap-using-python-seaborn\/)","bb2e47b5":"To print dataset **columns**, we can use columns atribute.","ab817417":"<a id=\"629\"><\/a> <br>\n###  6-2-9 Swarm plot","1b871990":"<a id=\"73\"><\/a> <br>\n## 7-3 How to prevent overfitting &  underfitting?\n\n<img src='https:\/\/cdn-images-1.medium.com\/max\/800\/1*JZbxrdzabrT33Yl-LrmShw.png' width=500 height=500>\n1. graph on the left side:\n    1. we can predict that the line does not cover all the points shown in the graph. Such model tend to cause underfitting of data .It also called High Bias.\n\n1. graph on right side:\n    1. shows the predicted line covers all the points in graph. In such condition you can also think that it\u2019s a good graph which cover all the points. But that\u2019s not actually true, the predicted line into the graph covers all points which are noise and outlier. Such model are also responsible to predict poor result due to its complexity.It is also called High Variance.\n\n1. middle graph:\n    1. it shows a pretty good predicted line. It covers majority of the point in graph and also maintains the balance between bias and variance.[30]","8dee84a3":"If you want to remove all the null value, you can uncomment this line.","d5817d8b":"<a id=\"641\"><\/a> <br>\n## 6-4-1 Transforming Features\nData transformation is the process of converting data from one format or structure into another format or structure[[wiki](https:\/\/en.wikipedia.org\/wiki\/Data_transformation)] \n1. Age\n1. Cabin\n1. Fare\n1. Name","9e034900":"<a id=\"623\"><\/a> <br>\n### 6-2-3 Histogram\nWe can also create a **histogram** of each input variable to get an idea of the **distribution**.","1185c3a7":"<a id=\"7\"><\/a> <br>\n## 7- Model Deployment\nIn this section have been applied plenty of  ** learning algorithms** that play an important rule in your experiences and improve your knowledge in case of ML technique.\n>  <font color=\"red\"><b>Note:<\/b><\/font>\nThe results shown here may be slightly different for your analysis because, for example, the neural network algorithms use random number generators for fixing the initial value of the weights (starting points) of the neural networks, which often result in obtaining slightly different (local minima) solutions each time you run the analysis. Also note that changing the seed for the random number generator used to create the train, test, and validation samples can change your results.","c91917e8":"<a id=\"626\"><\/a> <br>\n### 6-2-6 pairplot","d67d4dc9":"<a id=\"642\"><\/a> <br>\n## 6-4-2 Feature Encoding\nIn machine learning projects, one important part is feature engineering. It is very common to see categorical features in a dataset. However, our machine learning algorithm can only **read numerical values**. It is essential to encoding categorical features into numerical values[28]\n1. Encode labels with value between 0 and n_classes-1\n1. LabelEncoder can be used to normalize labels.\n1. It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.","6254fce5":"After loading the data via **pandas**, we should checkout what the content is, description and via the following:","e77a4d14":"<a id=\"64\"><\/a> <br>\n## 6-4 Data Cleaning \n1. When dealing with real-world data,** dirty data** is the norm rather than the exception. \n1. We continuously need to predict correct values, impute missing ones, and find links between various data artefacts such as schemas and records. \n1. We need to stop treating data cleaning as a piecemeal exercise (resolving different types of errors in isolation), and instead leverage all signals and resources (such as constraints, available statistics, and dictionaries) to accurately predict corrective actions.\n1. The primary goal of data cleaning is to detect and remove errors and **anomalies** to increase the value of data in analytics and decision making.[8]\n\n###### [Go to top](#top)","0b9162a7":"<a id=\"79\"><\/a> <br>\n## 7-9 ExtraTreeRegressor\nExtra Tree Regressor differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the max_features randomly selected features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally random decision tree.","0321f56e":"### 6-2-13 Distplot","61839955":"<a id=\"74\"><\/a> <br>\n## 7-4 RandomForestClassifier\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).","a14c8c6b":"<a id=\"6\"><\/a> <br>\n## 6- Exploratory Data Analysis(EDA)\n In this section, you'll learn how to use graphical and numerical techniques to begin uncovering the structure of your data. \n \n* Which variables suggest interesting relationships?\n* Which observations are unusual?\n* Analysis of the features!\n\n ><font color=\"red\"><b>Note:<\/b><\/font>\n You can read more about [EDA](https:\/\/www.datacamp.com\/community\/tutorials\/kaggle-machine-learning-eda)\n\nBy the end of the section, you'll be able to answer these questions and more, while generating graphics that are both **insightful** and **beautiful**.  then we will review analytical and statistical operations:\n\n*   5-1 Data Collection\n*   5-2 Visualization\n*   5-3 Data Preprocessing\n*   5-4 Data Cleaning\n<img src=\"http:\/\/s9.picofile.com\/file\/8338476134\/EDA.png\">\n\n ><font color=\"red\"><b>Note:<\/b><\/font>\n You can change the order of the above steps.","11fa5e88":"<a id=\"74\"><\/a> <br>\n## 7-4 Accuracy and precision\nWe know that the titanic problem is a binary classification and to evaluate, we just need to calculate the accuracy.\n\n1. **accuracy**\n\n    1. Your score is the percentage of passengers you correctly predict. This is known simply as \"accuracy\u201d.\n\n1. **precision** : \n\n    1. In pattern recognition, information retrieval and binary classification, precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, \n1. **recall** : \n\n    1. recall is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. \n1. **F-score** :\n\n    1. the F1 score is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n\n1. **What is the difference between accuracy and precision?**\n    1. \"Accuracy\" and \"precision\" are general terms throughout science. A good way to internalize the difference are the common \"bullseye diagrams\". In machine learning\/statistics as a whole, accuracy vs. precision is analogous to bias vs. variance.","57ab1f9f":"You can easily see the relationship between two variables through the following plot.","26348a22":">  <font color=\"red\"><b>Note:<\/b><\/font>\nPreprocessing and generation pipelines depend on a model type","f3900e1b":"To check out last 5 row of the data set, we use tail() function","fe49e2cc":"To pop up 5 random rows from the data set, we can use **sample(5)**  function","fbc52422":"### Import libraries"}}