{"cell_type":{"4962477f":"code","c191b099":"code","e469963f":"code","24ccf7c1":"code","9ef46288":"code","7ea7aa1c":"code","fdfc1d69":"code","bbe29d98":"code","bf71871b":"code","c4ba1654":"code","edbb207c":"code","781f72e4":"code","d3202e49":"code","4e3363f0":"code","c9ade340":"code","4cbc1de0":"code","dda26cb4":"code","df6056c7":"code","6da0d47c":"code","a70c224a":"code","47c58eba":"code","1ba1d43f":"code","082a2b6e":"code","541c6c80":"code","88d5b5bb":"code","99c3ba21":"code","d312926a":"markdown","33895653":"markdown","624ae644":"markdown","c85469ae":"markdown","a61feb70":"markdown"},"source":{"4962477f":"import numpy as np \nimport pandas as pd \nimport statsmodels.api as sm\nfrom statsmodels.tools import add_constant\nfrom itertools import combinations","c191b099":"df = pd.read_csv('\/kaggle\/input\/coronary-heart-disease\/CHDdata.csv')\ndf[\"famhist\"] = (df[\"famhist\"] == \"Present\")*1 # converts the famhit to 0 (no hist) and 1 (has hist)\n#df = df.drop([\"famhist\"], axis=1)\ndf.head()","e469963f":"X = df.drop([\"chd\"], axis=1)\ny = df[\"chd\"]\n# building the model and fitting the data \nlog_reg = sm.Logit(y, add_constant(X)).fit()\nlog_reg.summary()","24ccf7c1":"# Seaborn visualization library\nimport seaborn as sns\n# Create the default pairplot\ng = sns.pairplot(df, hue=\"chd\", palette=\"tab10\", markers=[\"o\", \"D\"])","9ef46288":"from mpmath import mp\nmp.dps = 50\nclass BMA:\n    \n    def __init__(self, y, X, **kwargs):\n        # Setup the basic variables.\n        self.y = y\n        self.X = X\n        self.names = list(X.columns)\n        self.nRows, self.nCols = np.shape(X)\n        self.likelihoods = mp.zeros(self.nCols,1)\n        self.likelihoods_all = {}\n        self.coefficients_mp = mp.zeros(self.nCols,1)\n        self.coefficients = np.zeros(self.nCols)\n        self.probabilities = np.zeros(self.nCols)\n        # Check the max model size. (Max number of predictor variables to use in a model.)\n        # This can be used to reduce the runtime but not doing an exhaustive sampling.\n        if 'MaxVars' in kwargs.keys():\n            self.MaxVars = kwargs['MaxVars']\n        else:\n            self.MaxVars = self.nCols  \n        # Prepare the priors if they are provided.\n        # The priors are provided for the individual regressor variables.\n        # The prior for a model is the product of the priors on the variables in the model.\n        if 'Priors' in kwargs.keys():\n            if np.size(kwargs['Priors']) == self.nCols:\n                self.Priors = kwargs['Priors']\n            else:\n                print(\"WARNING: Provided priors error.  Using equal priors instead.\")\n                print(\"The priors should be a numpy array of length equal tot he number of regressor variables.\")\n                self.Priors = np.ones(self.nCols)  \n        else:\n            self.Priors = np.ones(self.nCols)  \n        if 'Verbose' in kwargs.keys():\n            self.Verbose = kwargs['Verbose'] \n        else:\n            self.Verbose = False \n        if 'RegType' in kwargs.keys():\n            self.RegType = kwargs['RegType'] \n        else:\n            self.RegType = 'LS' \n        \n    def fit(self):\n        # Perform the Bayesian Model Averaging\n        \n        # Initialize the sum of the likelihoods for all the models to zero.  \n        # This will be the 'normalization' denominator in Bayes Theorem.\n        likelighood_sum = 0\n        \n        # To facilitate iterating through all possible models, we start by iterating thorugh\n        # the number of elements in the model.  \n        max_likelihood = 0\n        for num_elements in range(1,self.MaxVars+1): \n            \n            if self.Verbose == True:\n                print(\"Computing BMA for models of size: \", num_elements)\n            \n            # Make a list of all index sets of models of this size.\n            Models_next = list(combinations(list(range(self.nCols)), num_elements)) \n             \n            # Occam's window - compute the candidate models to use for the next iteration\n            # Models_previous: the set of models from the previous iteration that satisfy (likelihhod > max_likelihhod\/20)\n            # Models_next:     the set of candidate models for the next iteration\n            # Models_current:  the set of models from Models_next that can be consturcted by adding one new variable\n            #                    to a model from Models_previous\n            if num_elements == 1:\n                Models_current = Models_next\n                Models_previous = []\n            else:\n                idx_keep = np.zeros(len(Models_next))\n                for M_new,idx in zip(Models_next,range(len(Models_next))):\n                    for M_good in Models_previous:\n                        if(all(x in M_new for x in M_good)):\n                            idx_keep[idx] = 1\n                            break\n                        else:\n                            pass\n                Models_current = np.asarray(Models_next)[np.where(idx_keep==1)].tolist()\n                Models_previous = []\n                        \n            \n            # Iterate through all possible models of the given size.\n            for model_index_set in Models_current:\n                \n                # Compute the linear regression for this given model. \n                model_X = self.X.iloc[:,list(model_index_set)]\n                if self.RegType == 'Logit':\n                    model_regr = sm.Logit(self.y, model_X).fit(disp=0)\n                else:\n                    model_regr = OLS(self.y, model_X).fit()\n                \n                # Compute the likelihood (times the prior) for the model. \n                model_likelihood = mp.exp(-model_regr.bic\/2)*np.prod(self.Priors[list(model_index_set)])\n                    \n                if (model_likelihood > max_likelihood\/20):\n                    if self.Verbose == True:\n                        print(\"Model Variables:\",model_index_set,\"likelihood=\",model_likelihood)\n                    self.likelihoods_all[str(model_index_set)] = model_likelihood\n                    \n                    # Add this likelihood to the running tally of likelihoods.\n                    likelighood_sum = mp.fadd(likelighood_sum, model_likelihood)\n\n                    # Add this likelihood (times the priors) to the running tally\n                    # of likelihoods for each variable in the model.\n                    for idx, i in zip(model_index_set, range(num_elements)):\n                        self.likelihoods[idx] = mp.fadd(self.likelihoods[idx], model_likelihood, prec=1000)\n                        self.coefficients_mp[idx] = mp.fadd(self.coefficients_mp[idx], model_regr.params[i]*model_likelihood, prec=1000)\n                    Models_previous.append(model_index_set) # add this model to the list of good models\n                    max_likelihood = np.max([max_likelihood,model_likelihood]) # get the new max likelihood if it is this model\n                else:\n                    if self.Verbose == True:\n                        print(\"Model Variables:\",model_index_set,\"rejected by Occam's window\")\n                    \n\n        # Divide by the denominator in Bayes theorem to normalize the probabilities \n        # sum to one.\n        self.likelighood_sum = likelighood_sum\n        for idx in range(self.nCols):\n            self.probabilities[idx] = mp.fdiv(self.likelihoods[idx],likelighood_sum, prec=1000)\n            self.coefficients[idx] = mp.fdiv(self.coefficients_mp[idx],likelighood_sum, prec=1000)\n        \n        # Return the new BMA object as an output.\n        return self\n    \n    def predict(self, data):\n        data = np.asarray(data)\n        if self.RegType == 'Logit':\n            try:\n                result = 1\/(1+np.exp(-1*np.dot(self.coefficients,data)))\n            except:\n                result = 1\/(1+np.exp(-1*np.dot(self.coefficients,data.T)))\n        else:\n            try:\n                result = np.dot(self.coefficients,data)\n            except:\n                result = np.dot(self.coefficients,data.T)\n        \n        return result  \n        \n    def summary(self):\n        # Return the BMA results as a data frame for easy viewing.\n        df = pd.DataFrame([self.names, list(self.probabilities), list(self.coefficients)], \n             [\"Variable Name\", \"Probability\", \"Avg. Coefficient\"]).T\n        return df  ","7ea7aa1c":"result = BMA(y,add_constant(X), RegType = 'Logit', Verbose=True).fit()","fdfc1d69":"result.summary()","bbe29d98":"r = result.summary()","bf71871b":"r['Probability']","c4ba1654":"r['Variable Name']","edbb207c":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\nplt.style.use('ggplot')\nx = result.summary().sort_values('Probability')['Variable Name']\nprobs = result.summary().sort_values('Probability')['Probability']\nx_pos = [i for i, _ in enumerate(x)]\nplt.bar(x_pos,probs,color = 'blue')\nplt.xlabel(\"Factors\")\nplt.ylabel(\"Probability\")\nplt.title(\"Probabilities for Each Factor\")\nplt.xticks(x_pos,x)\nplt.show()","781f72e4":"plt.style.use('ggplot')\nx = result.summary().sort_values('Avg. Coefficient')['Variable Name']\ncoefs = result.summary().sort_values('Avg. Coefficient')['Avg. Coefficient']\nx_pos = [i for i, _ in enumerate(x)]\nplt.bar(x_pos,coefs,color = 'blue')\nplt.xlabel(\"Factors\")\nplt.ylabel(\"Model Average Coefficients\")\nplt.title(\"Model Average Coefficients for Each Factor\")\nplt.xticks(x_pos,x)\nplt.show()","d3202e49":"result.likelihoods","4e3363f0":"result.likelihoods_all","c9ade340":"df_standard = df.copy()","4cbc1de0":"# Standardize the data (mean for each numerical variable of zero, standard deviation of one.)\nfor key in [x for x in df_standard.keys()[0:9] if (x != 'famhist')]:\n    try:\n        print(\"Standardizing \"+key+\".\")\n        df_standard[key] = df_standard[key] - np.mean(df_standard[key])\n        df_standard[key] = df_standard[key] \/ np.std(df_standard[key])\n    except:\n        print(\"Predictor \"+key+\" cannot be standardized (probably a categorical variable).\")\ndf_standard.describe()","dda26cb4":"X = df_standard.drop([\"chd\"], axis=1)\ny = df_standard[\"chd\"]\n# building the model and fitting the data \nlog_reg = sm.Logit(y, add_constant(X)).fit()\n\nresult = BMA(y,add_constant(X), RegType = 'Logit', Verbose=True).fit()","df6056c7":"plt.style.use('ggplot')\nx = result.summary().sort_values('Probability')['Variable Name']\nprobs = result.summary().sort_values('Probability')['Probability']\nx_pos = [i for i, _ in enumerate(x)]\nplt.bar(x_pos,probs,color = 'blue')\nplt.xlabel(\"Factors\")\nplt.ylabel(\"Probability\")\nplt.title(\"Probabilities for Each Factor\")\nplt.xticks(x_pos,x)\nplt.show()","6da0d47c":"plt.style.use('ggplot')\nx = result.summary().sort_values('Avg. Coefficient')['Variable Name']\ncoefs = result.summary().sort_values('Avg. Coefficient')['Avg. Coefficient']\nx_pos = [i for i, _ in enumerate(x)]\nplt.bar(x_pos,coefs,color = 'blue')\nplt.xlabel(\"Factors\")\nplt.ylabel(\"Model Average Coefficients\")\nplt.title(\"Model Average Coefficients for Each Factor\")\nplt.xticks(x_pos,x)\nplt.show()","a70c224a":"# predict the y-values from training input data\npred_BMA = result.predict(add_constant(X))\npred_Logit = log_reg.predict(add_constant(X))","47c58eba":"# plot the predictions with the actual values\nimport matplotlib.pyplot as plt\nplt.scatter(pred_BMA,y-0.05)\nplt.scatter(pred_Logit,y)\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Coronary Heart Disease \\n(0=Not Present, 1=Present)\")\nplt.legend(['pred_BMA','pred_Logit'])","1ba1d43f":"# compute accuracy\nprint(\"BMA Accuracy: \", np.sum((pred_BMA > 0.5) == y)\/len(y))\nprint(\"Logit Accuracy: \", np.sum((pred_Logit > 0.5) == y)\/len(y))","082a2b6e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.70, random_state=99)","541c6c80":"# predict the y-values from training input data\npred_BMA = result.predict(add_constant(X_train))\npred_Logit = log_reg.predict(add_constant(X_train))","88d5b5bb":"# plot the predictions with the actual values\nimport matplotlib.pyplot as plt\nplt.scatter(pred_BMA,y_train-0.05)\nplt.scatter(pred_Logit,y_train)\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Coronary Heart Disease \\n(0=Not Present, 1=Present)\")\nplt.legend(['pred_BMA','pred_Logit'])","99c3ba21":"# compute accuracy\nprint(\"BMA Accuracy: \", np.sum((pred_BMA > 0.5) == y_train)\/len(y_train))\nprint(\"Logit Accuracy: \", np.sum((pred_Logit > 0.5) == y_train)\/len(y_train))","d312926a":"Here is code for a BMA class that will do the Bayeisan Model Averaging.  This is the same as the code from the Bayesian Model Averaging notebook https:\/\/www.kaggle.com\/billbasener\/bayesian-model-averaging-regression-tutorial-pt-2, but with that added capability to do logistic regression via the keyword RegType to \"Logit\".","33895653":"# Bayesian Model Averaging\nHere we define the class that will perform our BMA analysis.\n\nFor any model $M_i$ (each model is defined by the set of predictor varialbes being used in the model), Bayes theorem tells us that the probability for $M_i$ is\n\\begin{equation}\np(M_i|X,y)=\\frac{p(X,y|M_i)p(M_i)}{p(X,y)}.\n\\end{equation}\n\nUsing our previous formulas, this becomes,\n\\begin{equation}\np(M_i|X,y)=\\frac{e^{\u2212\\text{BIC}_i\/2}p(M_i)}{\\sum_k e^{\u2212\\text{BIC}_k\/2}p(M_k)}.\n\\end{equation}\n\nSo far, we have just done Bayesian analysis to compute a posterior probability distribution on the parameters.  But now we can do more with the 'averaging' part of BMA.\n\nThe probability for any predictor variable is the sum of the probabilities for all models contiaining that predictor variable, and the expected value for the coefficient of the predictor variable is the average value of the coefficient over all models containing the variable, weighted by the probability of each model.  That is,\n\\begin{equation}\np(X_k) = \\sum_{M_i \\text{such that } X_k\\in M_i} p(M_i|X,y),\n\\end{equation}\nand\n\\begin{equation}\nE[\\beta_k] = \\sum_{M_i \\text{such that } X_k\\in M_i} p(M_i|X,y)\\times \\beta_k^{(i)},\n\\end{equation}\nwhere $\\beta_k^{(i)}$ is the coefficient of $X_k$ in model $M_i$.","624ae644":"# Bayesian Model Averaging Logistic Regression\n# Avinaash Pavuloori (akp8vy)\n\nIn this notebook we will use Bayesian Model Averaging (BMA) to understand a logistic regression problem.  The data coronary heart disease (0 = does not have CHD, 1 = has CHD), depending on a number of medical predictor variables.","c85469ae":"Load the data, and check the head.","a61feb70":"Now we split our data into input X dataframe and an output y datafram, and run our BMA analysis."}}