{"cell_type":{"e0383019":"code","48213c62":"code","acb5f7e1":"code","12c7eff9":"code","68fb8f5d":"code","cbf5ccd2":"code","d5306324":"code","1169d5e2":"code","e3eeba3a":"code","7060456f":"code","ec52b9af":"code","95ddb21f":"code","c1fd6e3c":"code","89bf8e68":"code","69ce4a0d":"code","1aeab940":"code","58dbc312":"code","668fd7bc":"code","755eaac9":"code","3bcdc793":"code","0e86bf82":"code","644d9d8f":"code","fe480c02":"code","a80d4399":"code","e4300acb":"code","ab26eaba":"code","1c650881":"code","90b9f6f8":"markdown","02948d51":"markdown","2540d67a":"markdown","a2ed2942":"markdown","8bad97fb":"markdown","82cdd117":"markdown","84055272":"markdown","cd2e38ff":"markdown"},"source":{"e0383019":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport warnings\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')","48213c62":"data = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","acb5f7e1":"data = data.set_index('id')","12c7eff9":"data.head()","68fb8f5d":"data.info()","cbf5ccd2":"data.groupby(['stroke','gender','hypertension','heart_disease']).median()","d5306324":"sns.countplot(data['stroke'])","1169d5e2":"plt.figure(figsize=(5,5))\nsns.heatmap(pd.DataFrame(data.corr()['stroke']).sort_values(by='stroke').transpose().drop('stroke',axis=1).transpose(),annot=True,cmap='CMRmap')","e3eeba3a":"data=data.dropna()\ndata.info()","7060456f":"list = ['gender','hypertension','heart_disease','ever_married','work_type','Residence_type','smoking_status']\nm=1\nplt.figure(figsize=(15,15))\nfor i in list:\n    plt.subplot(4,2,m)\n    sns.countplot(x='stroke',data=data,hue=i)\n    m+=1","ec52b9af":"list = ['bmi','age','avg_glucose_level']\nm=1\nplt.figure(figsize=(15,5))\nfor i in list:\n    plt.subplot(1,3,m)\n    sns.boxplot(y=i,data=data,x='stroke')\n    m+=1","95ddb21f":"from sklearn.preprocessing import LabelEncoder\nto_be_encoded = ['gender','ever_married','work_type','Residence_type','smoking_status']\nlabel_encoder = LabelEncoder()\ndfs = []\nfor i in to_be_encoded:\n    temp = pd.DataFrame({'Before Encoding':data[i].unique(),'After Encoding':label_encoder.fit_transform(data[i].unique())})\n    dfs.append([temp.sort_values(by=['After Encoding']),i])\n    data[i] = label_encoder.fit_transform(data[i])\nm=0","c1fd6e3c":"dfs","89bf8e68":"print(dfs[m][1])\ndisplay(dfs[m][0])\nm+=1","69ce4a0d":"print(dfs[m][1])\ndisplay(dfs[m][0])\nm+=1","1aeab940":"print(dfs[m][1])\ndisplay(dfs[m][0])\nm+=1","58dbc312":"print(dfs[m][1])\ndisplay(dfs[m][0])\nm+=1","668fd7bc":"print(dfs[m][1])\ndisplay(dfs[m][0])\nm+=1","755eaac9":"X = data.drop('stroke',axis=1)\nX['nf']=X['age']*X['avg_glucose_level']\nY = data['stroke']","3bcdc793":"from sklearn.feature_selection import SelectKBest, chi2\nfs = SelectKBest(score_func=chi2, k='all')\nfs.fit(X, Y)\nper = []\nfor i in fs.scores_:\n    per.append(round(((i\/sum(fs.scores_))*100),3))\n\nfeatures_data = pd.DataFrame({'Feature':X.columns,'Scores':fs.scores_,'Importance (%)':per}).sort_values(by=['Scores'],ascending=False)\n\nplt.figure(figsize=(9,4))\nsns.barplot( 'Importance (%)','Feature',orient='h',data=features_data,palette='CMRmap')\ninsignificant = features_data.loc[features_data['Importance (%)']<0.005]['Feature'].unique()\nfeatures_data = features_data.set_index('Feature')\nfeatures_data","0e86bf82":"X=X.drop(insignificant,axis=1)","644d9d8f":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)","fe480c02":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=100)","a80d4399":"from sklearn.metrics import accuracy_score,classification_report\n\n#XGB\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier() \n\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n\n#RFC\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\n\n#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\naccuracy = []\nfor i in range(1,40):    \n    kn = KNeighborsClassifier(n_neighbors=i)\n    kn.fit(X_train,Y_train)\n    predK = kn.predict(X_test)\n    accuracy.append([accuracy_score(Y_test,predK),i])\n    #print('Tested for k =',i)\ntemp = accuracy[0]\nfor m in accuracy:\n    if temp[0] < m[0]:\n        temp=m\nknn = KNeighborsClassifier(n_neighbors=temp[1])\n\n#SVM\nfrom sklearn.svm import SVC\nsvc = SVC()\n\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'C': [0.1,1, 10, 100, 1000,2000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} \ngrid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\n\nprint('Models Imported')","e4300acb":"model_acc = []\nmodels = [xgb,lr,rfc,knn,svc,grid]\n#model_name = ['xgb','lr','rfc','kno','svc','grid']\nfor i in models:\n    i.fit(X_train,Y_train)\n    model_acc.append(accuracy_score(Y_test,i.predict(X_test)))\n                      \nmodels = pd.DataFrame({'Models':models,'Accuracy':model_acc})","ab26eaba":"models = models.sort_values(by=['Accuracy'],ascending=False).reset_index().drop('index',axis=1)\nbest = models['Models'][0]\nmodels['Models']=models['Models'].astype(str).str.split(\"(\", n = 2, expand = True)[0]\nmodels","1c650881":"print('Hence the best model is',models['Models'][0],'with an accuracy of',round((models['Accuracy'][0]*100),2),'%')\nprint('\\nThe classification report is:')\nprint(classification_report(Y_test,best.predict(X_test)))","90b9f6f8":"# EDA","02948d51":"# Feature Selection","2540d67a":"# Test Train Split","a2ed2942":"# Feature Scaling","8bad97fb":"# Model selection and Evaluation","82cdd117":"# Importing data & Libraries","84055272":"# <center> Heart Disease Predicion","cd2e38ff":"# One hot encoding"}}