{"cell_type":{"ea55dc50":"code","1cd60b61":"code","e727526a":"code","e0702c27":"code","a415ff1f":"code","5d5c36c8":"code","2063ae5f":"code","0ff415d6":"code","c0dca71f":"code","0d88e06d":"code","7833a714":"code","402ee63a":"code","117ad901":"code","fcd88790":"markdown","1ebf6a7c":"markdown","470890aa":"markdown","65e24afc":"markdown"},"source":{"ea55dc50":"from tensorflow import keras\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score,precision_score,recall_score\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport glob\nimport cv2","1cd60b61":"features = []\nlabels = []","e727526a":"#reading file locations\n#From the locations read, consider only 1100 out of 3000 to reduce dataset size\nclasses = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]\nfor i in range(len(classes)):\n    file_list = glob.glob(\"\/kaggle\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train\/\" + classes[i] + \"\/*.jpg\")\n    for item in range(1100):\n        features.append(file_list[item])\n        labels.append([i])\nprint(\"Dataset Feature size : \",len(features))\nprint(\"Dataset labels size : \",len(labels))","e0702c27":"#Shuffle the array\nfeatures,labels = shuffle(features,labels,random_state=0)","a415ff1f":"train_features = []\ntrain_labels = []\nvalidation_features = []\nvalidation_labels = []\n\nfor i in features:\n    train_features.append(cv2.imread(i,cv2.IMREAD_COLOR))\n    train_features[-1] = np.reshape(train_features[-1],[200,200,3])\n\nfor i in labels:\n    train_labels.append(i)\n\ntrain_features,validation_features,train_labels,validation_labels = train_test_split(train_features,train_labels,test_size=0.3)\nprint(\"Train data : \",len(train_features),len(train_labels))\nprint(\"Validation data : \",len(validation_features),len(validation_labels))","5d5c36c8":"del features\ndel labels\ndel file_list","2063ae5f":"alpha = [chr(c) for c in range(65,91)]\nun,count = np.unique(train_labels,return_counts=True)\nj=1\nplt.figure(figsize=(100,100))\nfor i in un:\n    plt.subplot(7,4,j)\n    plt.imshow(train_features[np.where(train_labels == np.array(i))[0][0]])\n    plt.axis('off')\n    plt.title(alpha[i],fontdict=dict({'fontsize' : 100}))\n    j=j+1","0ff415d6":"model = keras.Sequential()\n\nmodel.add(keras.layers.Conv2D(32,(3,3),activation=\"relu\",padding=\"same\",input_shape=(200,200,3)))\nmodel.add(keras.layers.Conv2D(32,(3,3),activation=\"relu\",padding=\"same\"))\nmodel.add(keras.layers.MaxPooling2D(3,3))\n\nmodel.add(keras.layers.Conv2D(64,(3,3),activation=\"relu\",padding=\"same\"))\nmodel.add(keras.layers.Conv2D(64,(3,3),activation=\"relu\",padding=\"same\"))\nmodel.add(keras.layers.MaxPooling2D(3,3))\n\nmodel.add(keras.layers.Conv2D(128,(3,3),activation=\"relu\",padding=\"same\"))\nmodel.add(keras.layers.Conv2D(128,(3,3),activation=\"relu\",padding=\"same\"))\nmodel.add(keras.layers.MaxPooling2D(3,3))\n\nmodel.add(keras.layers.Conv2D(256,(3,3),activation=\"relu\",padding=\"same\"))\nmodel.add(keras.layers.Conv2D(256,(3,3),activation=\"relu\",padding=\"same\"))\n\nmodel.add(keras.layers.Flatten())\n\nmodel.add(keras.layers.Dense(1568,activation=\"relu\"))\nmodel.add(keras.layers.Dropout(0.5))\n\nmodel.add(keras.layers.Dense(26,activation=\"softmax\"))\n\nopt = keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=opt,loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\nmodel.summary()","c0dca71f":"train_features = np.array(train_features)\ntrain_labels = np.array(train_labels) \nvalidation_features = np.array(validation_features)\nvalidation_labels = np.array(validation_labels)\nhistory = model.fit(train_features,\n          train_labels,\n          epochs=10, \n          validation_data = (validation_features,validation_labels))","0d88e06d":"# Get training and test loss histories\ntraining_loss = history.history['loss']\ntest_loss = history.history['val_loss']\ntrain_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\n#Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize metrics\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.plot(epoch_count, training_loss)\nplt.plot(epoch_count, test_loss)\nplt.legend(['Train Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\nplt.subplot(1,2,2)\nplt.plot(epoch_count, train_acc)\nplt.plot(epoch_count, val_acc)\nplt.legend(['Train Accuracy', 'Validation Accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.show()","7833a714":"del train_features\ndel train_labels","402ee63a":"y_pred = np.argmax(model.predict(validation_features),1)\nprint(\"Precision : {:.2f} %\".format(precision_score(y_pred,validation_labels,average='macro')))\nprint(\"Recall    : {:.2f} %\".format(precision_score(y_pred,validation_labels,average='macro')))\nprint(\"F1 Score  : {:.2f} %\".format(precision_score(y_pred,validation_labels,average='macro')))","117ad901":"plt.figure(figsize=(50,50))\ncm = confusion_matrix(validation_labels, y_pred.reshape(-1,1))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                               display_labels=alpha)\nfig, ax = plt.subplots(figsize=(15,15))\ndisp.plot(ax=ax)","fcd88790":"<h1>Model Evaluation<\/h1>","1ebf6a7c":"<img src=\"https:\/\/miro.medium.com\/max\/4800\/1*DrbLUMbhtehEgEl8Kj5v9Q.png\">","470890aa":"<h1>Confusion Matrix<\/h1>","65e24afc":"<h1>ASL Recognition using Keras CNN<\/h1>\n<p>In this notebook, we will train a Keras Deep Learning model to recognize American Sign Language<\/p>"}}