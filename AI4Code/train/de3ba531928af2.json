{"cell_type":{"80c2d89a":"code","bbdcc5e8":"code","04b9e144":"code","d8956cba":"code","3f92f98c":"code","d8afaef3":"code","a537344a":"code","11e4770d":"markdown"},"source":{"80c2d89a":"!pip install kaggle-environments --upgrade","bbdcc5e8":"# observation = {\n#     'remainingOverageTime': 60,\n#     'agentIndex': 1, # 0 or 1\n#     'reward': 92, # total reward\n#     'step': 184, # [0-1999]\n#     'lastActions': [84, 94]\n# }\n\n# configuration:\n# {'episodeSteps': 2000,\n#  'actTimeout': 0.25,\n#  'runTimeout': 1200,\n#  'banditCount': 100,\n#  'decayRate': 0.97,\n#  'sampleResolution': 100}","04b9e144":"%%writefile submission.py\n\nimport numpy as np\nimport pandas as pd\nimport random, os, datetime, math\nfrom collections import defaultdict\n\ntotal_reward = 0\nbandit_dict = {}\n\ndef set_seed(my_seed=42):\n    os.environ['PYTHONHASHSEED'] = str(my_seed)\n    random.seed(my_seed)\n    np.random.seed(my_seed)\n\ndef get_next_bandit():\n    best_bandit = 0\n    best_bandit_expected = 0\n    for bnd in bandit_dict:\n        expect = (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5 + bandit_dict[bnd]['op_continue']) \\\n                 \/ (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) \\\n                * math.pow(0.97, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\n        if expect > best_bandit_expected:\n            best_bandit_expected = expect\n            best_bandit = bnd\n    return best_bandit\n\nmy_action_list = []\nop_action_list = []\n\nop_continue_cnt_dict = defaultdict(int)\n\ndef multi_armed_probabilities(observation, configuration):\n    global total_reward, bandit_dict\n\n    my_pull = random.randrange(configuration['banditCount'])\n    if 0 == observation['step']:\n        set_seed()\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0, 'my_continue': 0, 'op_continue': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        my_last_action = observation['lastActions'][my_idx]\n        op_last_action = observation['lastActions'][1-my_idx]\n        \n        my_action_list.append(my_last_action)\n        op_action_list.append(op_last_action)\n        \n        if 0 < last_reward:\n            bandit_dict[my_last_action]['win'] = bandit_dict[my_last_action]['win'] +1\n        else:\n            bandit_dict[my_last_action]['loss'] = bandit_dict[my_last_action]['loss'] +1\n        bandit_dict[op_last_action]['opp'] = bandit_dict[op_last_action]['opp'] +1\n        \n        if observation['step'] >= 3:\n            if my_action_list[-1] == my_action_list[-2]:\n                bandit_dict[my_last_action]['my_continue'] += 1\n            else:\n                bandit_dict[my_last_action]['my_continue'] = 0\n            if op_action_list[-1] == op_action_list[-2]:\n                bandit_dict[op_last_action]['op_continue'] += 1\n            else:\n                bandit_dict[op_last_action]['op_continue'] = 0\n        \n        if last_reward > 0:\n            my_pull = my_last_action\n        else:\n            if observation['step'] >= 4:\n                if (my_action_list[-1] == my_action_list[-2]) and (my_action_list[-1] == my_action_list[-3]):\n                    if random.random() < 0.5:\n                        my_pull = my_action_list[-1]\n                    else:\n                        my_pull = get_next_bandit()\n                else:\n                    my_pull = get_next_bandit()\n            else:\n                my_pull = get_next_bandit()\n    \n    return my_pull","d8956cba":"%%writefile opponent_agent.py\n\nimport numpy as np\nimport pandas as pd\nimport random, os, datetime, math\n\ntotal_reward = 0\nbandit_dict = {}\n\ndef set_seed(my_seed=42):\n    os.environ['PYTHONHASHSEED'] = str(my_seed)\n    random.seed(my_seed)\n    np.random.seed(my_seed)\n\ndef get_next_bandit():\n    best_bandit = 0\n    best_bandit_expected = 0\n    for bnd in bandit_dict:\n        expect = (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5) \\\n                 \/ (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) \\\n                * math.pow(0.97, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\n        if expect > best_bandit_expected:\n            best_bandit_expected = expect\n            best_bandit = bnd\n    return best_bandit\n\nmy_action_list = []\nop_action_list = []\n\n\n\ndef multi_armed_probabilities(observation, configuration):\n    global total_reward, bandit_dict\n\n    my_pull = random.randrange(configuration['banditCount'])\n    if 0 == observation['step']:\n        set_seed()\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        my_last_action = observation['lastActions'][my_idx]\n        op_last_action = observation['lastActions'][1-my_idx]\n        \n        my_action_list.append(my_last_action)\n        op_action_list.append(op_last_action)\n        \n        if 0 < last_reward:\n            bandit_dict[my_last_action]['win'] = bandit_dict[my_last_action]['win'] +1\n        else:\n            bandit_dict[my_last_action]['loss'] = bandit_dict[my_last_action]['loss'] +1\n        bandit_dict[op_last_action]['opp'] = bandit_dict[op_last_action]['opp'] +1\n        \n        if last_reward > 0:\n            my_pull = my_last_action\n        else:\n            if observation['step'] >= 4:\n                if (my_action_list[-1] == my_action_list[-2]) and (my_action_list[-1] == my_action_list[-3]):\n                    if random.random() < 0.5:\n                        my_pull = my_action_list[-1]\n                    else:\n                        my_pull = get_next_bandit()\n                else:\n                    my_pull = get_next_bandit()\n            else:\n                my_pull = get_next_bandit()\n    \n    return my_pull","3f92f98c":"from kaggle_environments import make\nenv = make(\"mab\", debug=True)","d8afaef3":"import datetime\n\nenv.reset()\nstart_time = datetime.datetime.now()\nenv.run([\"opponent_agent.py\", \"submission.py\"])\nstop_time = datetime.datetime.now()\nprint('Completed sub vs random:', stop_time-start_time)\nenv.render(mode=\"ipython\", width=800, height=400)","a537344a":"import datetime\n\nenv.reset()\nstart_time = datetime.datetime.now()\nenv.run([\"submission.py\", \"submission.py\"])\nstop_time = datetime.datetime.now()\nprint('Completed sub vs sub:', stop_time-start_time)\nenv.render(mode=\"ipython\", width=800, height=400)","11e4770d":"# Keep pulling same bandit as long as reward keeps coming!"}}