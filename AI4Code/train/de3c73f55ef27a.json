{"cell_type":{"f14a638b":"code","38793c24":"code","3416b6a4":"code","79fb9b11":"code","0870a39f":"code","d43ef17e":"code","cf1a44a8":"code","2702e0ed":"code","26d15edf":"code","e78a000e":"code","57922e9d":"code","e0125170":"code","ee6eedb9":"code","43aafa6a":"code","e6aa435b":"code","fd1f25e4":"code","a02940c2":"code","619cbc78":"code","945b888a":"code","5ddb9b5a":"code","b8f5d55f":"code","7daba92e":"code","16fee84d":"code","9f5fb83d":"code","6e87c9c4":"code","f4e95b04":"code","b2008655":"code","6f4213c5":"code","b9c0c66b":"code","a9a38cb5":"markdown","60d29932":"markdown","bb7e68bf":"markdown","ea9f7893":"markdown","6920c030":"markdown"},"source":{"f14a638b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","38793c24":"### Best Log measn squared error =  0.022717560164626886\n\nimport pandas as pd\nimport torch\ndata = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nprint(data.shape)\n\n\nfeature_ = data.loc[:, data.columns != \"SalePrice\"]\ntarget = data.loc[:, data.columns == \"SalePrice\"]\n\n# merge test and train data independent data together \nfeature = pd.concat([feature_, test])\n\ncount_NA = feature.isnull().sum()\n\ncolumn = feature.columns\n\nprint(feature.shape, target.shape, test.shape)\n","3416b6a4":"for i, count in enumerate(count_NA):\n    if count>0:\n        print('{} = {}'.format(column[i], count))","79fb9b11":"def drop_na_max(feature, n = 500):\n    count_NA = feature.isnull().sum()\n    Max_na = []\n    print(f'shape of the data before the droping column{feature.shape}')\n    for i, count in enumerate(count_NA):\n        if count>n:\n            Max_na.append(column[i])\n    Max_na.append(\"Id\")\n    df = feature.drop(Max_na, axis = 1)\n    print(f'shape of the data before the droping column{df.shape}')\n    return df\n\ndf = drop_na_max(feature, n = 100)","0870a39f":"def missing_value_column(df):\n    na = {}\n    lis = []\n    column = df.columns\n    for i, count in enumerate(df.isnull().sum()):\n        if count>0:\n            #na.key = column[i]\n            na[column[i]] = count\n            lis.append(column[i])\n    return lis\n#lis.remove('FireplaceQu')\n#### column that contain missing valuel\n\nlis =  missing_value_column(df)\nlis","d43ef17e":"## replacing missing value with mode (categorical) and median (numerical) \n\ndef filling_missing_value(df, lis):\n    a = df[lis].columns[df[lis].dtypes == \"object\"]\n    for i in a:\n        df[i].fillna(df[i].mode()[0], inplace=True)\n    \n    b = df[lis].columns[df[lis].dtypes != \"object\"]\n    for i in b:\n        df[i].fillna(df[i].median, inplace=True)\n    \n    return df\ndf = filling_missing_value(df, lis)\n","cf1a44a8":"df.columns[df.isnull().any()]","2702e0ed":"## only numeric values will be used for the check\nb = df.columns[df.dtypes != \"object\"]\n\n\nfrom sklearn.feature_selection import VarianceThreshold\nselector = VarianceThreshold(threshold=0.2)\nselector.fit(df[b])\n\nb[~selector.get_support()]","26d15edf":"df = df.drop(b[~selector.get_support()],axis = 1)\ndf.shape","e78a000e":"def correlation(dataset, threshold):\n    col_corr = set()\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i,j])> threshold:\n                colname = corr_matrix.columns[i]\n                col_corr.add(colname)\n    return list(col_corr)","57922e9d":"correlation(df, 0.7)","e0125170":"df = df.drop(correlation(df, 0.7), axis = 1)","ee6eedb9":"df.shape\ncolumn_s = df.columns[df.dtypes == \"object\"]\ncolumn_s","43aafa6a":"dfle = df\ndfle['MSZoning'] = le.fit_transform(dfle['MSZoning'])\ndfle['Street'] = le.fit_transform(dfle['Street'])\ndfle['LotShape'] = le.fit_transform(dfle['LotShape'])\ndfle['LandContour'] = le.fit_transform(dfle['LandContour'])\ndfle['Utilities'] = le.fit_transform(dfle['Utilities'])\ndfle['LotConfig'] = le.fit_transform(dfle['LotConfig'])\ndfle['LandSlope'] = le.fit_transform(dfle['LandSlope'])\ndfle['Neighborhood'] = le.fit_transform(dfle['Neighborhood'])\ndfle['Condition1'] = le.fit_transform(dfle['Condition1'])\ndfle['Condition2'] = le.fit_transform(dfle['Condition2'])\ndfle['BldgType'] = le.fit_transform(dfle['BldgType'])\ndfle['HouseStyle'] = le.fit_transform(dfle['HouseStyle'])\ndfle['RoofStyle'] = le.fit_transform(dfle['RoofStyle'])\ndfle['RoofMatl'] = le.fit_transform(dfle['RoofMatl'])\ndfle['Exterior1st'] = le.fit_transform(dfle['Exterior1st'])\ndfle['Exterior2nd'] = le.fit_transform(dfle['Exterior2nd'])\ndfle['MasVnrType'] = le.fit_transform(dfle['MasVnrType'])\n#dfle['MasVnrArea'] = le.fit_transform(dfle['MasVnrArea'])\ndfle['ExterQual'] = le.fit_transform(dfle['ExterQual'])\ndfle['ExterCond'] = le.fit_transform(dfle['ExterCond'])\ndfle['Foundation'] = le.fit_transform(dfle['Foundation'])\ndfle['BsmtQual'] = le.fit_transform(dfle['BsmtQual'])\ndfle['BsmtCond'] = le.fit_transform(dfle['BsmtCond'])\ndfle['BsmtExposure'] = le.fit_transform(dfle['BsmtExposure'])\ndfle['BsmtFinType1'] = le.fit_transform(dfle['BsmtFinType1'])\n#dfle['BsmtFinSF1'] = le.fit_transform(dfle['BsmtFinSF1'])\ndfle['BsmtFinType2'] = le.fit_transform(dfle['BsmtFinType2'])\n#dfle['BsmtFinSF2'] = le.fit_transform(dfle['BsmtFinSF2'])\n#dfle['BsmtUnfSF'] = le.fit_transform(dfle['BsmtUnfSF'])\n#dfle['TotalBsmtSF'] = le.fit_transform(dfle['TotalBsmtSF'])\ndfle['Heating'] = le.fit_transform(dfle['Heating'])\ndfle['HeatingQC'] = le.fit_transform(dfle['HeatingQC'])\ndfle['CentralAir'] = le.fit_transform(dfle['CentralAir'])\ndfle['Electrical'] = le.fit_transform(dfle['Electrical'])\n#dfle['BsmtFullBath'] = le.fit_transform(dfle['BsmtFullBath'])\n#dfle['BsmtHalfBath'] = le.fit_transform(dfle['BsmtHalfBath'])\ndfle['KitchenQual'] = le.fit_transform(dfle['KitchenQual'])\ndfle['Functional'] = le.fit_transform(dfle['Functional'])\n#dfle['GarageCars'] = le.fit_transform(dfle['GarageCars'])\n#dfle['GarageArea'] = le.fit_transform(dfle['GarageArea'])\ndfle['PavedDrive'] = le.fit_transform(dfle['PavedDrive'])\ndfle['SaleType'] = le.fit_transform(dfle['SaleType'])\ndfle['SaleCondition'] = le.fit_transform(dfle['SaleCondition'])\n\ndf = dfle","e6aa435b":"object_columns = dfle.columns[dfle.dtypes == \"object\"]\nobject_columns\n\ndf = df.drop(df[object_columns], axis = 1)","fd1f25e4":"from sklearn.model_selection import train_test_split \nfrom sklearn.preprocessing import StandardScaler\n# Initialise the Scaler \nscaler = StandardScaler() \n  \n# To scale data \nscaler.fit(df) \ntrain = df.iloc[:target.shape[0],:]\ntest = df.iloc[target.shape[0]:,:]\n\nX_train,X_test,y_train,y_test = train_test_split(train,target,test_size=0.2)\ny_test.shape\n#print(target.shape, train.shape, test.shape)\nprint(df.shape, X_train.shape, y_train.shape)\n","a02940c2":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_log_error\n\nli_reg = LinearRegression()\nli_reg.fit( X_train, y_train)\nmse = cross_val_score(li_reg, X_train, y_train, scoring = \"neg_mean_squared_error\", cv = 5)\nmse","619cbc78":"from sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import GridSearchCV\n\nridge = Ridge()\nparameters = {\"alpha\" :[1e-15,1e-10, 1e-5, 1e-1, 1e-4,1e-3, 1e-2, 1,2,3,4,5] }\nridge_reg = GridSearchCV(ridge, parameters, scoring = \"neg_mean_squared_error\", cv = 5)\nridge_reg.fit(X_train, y_train)","945b888a":"print(ridge_reg.best_params_)\nprint(ridge_reg.best_score_)\n","5ddb9b5a":"from sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import GridSearchCV\n\nlasso = Lasso()\nparameters = {\"alpha\" :np.arange(50, 100, 0.1).tolist() }\nlasso_reg = GridSearchCV(lasso, parameters, scoring = \"neg_mean_squared_error\", cv = 5, verbose=1)\nlasso_reg.fit(X_train, y_train)\n\nfrom sklearn.metrics import mean_squared_log_error\n\npred = lasso_reg.predict(X_test)\nmean_squared_log_error(pred, y_test)","b8f5d55f":"print(lasso_reg.best_params_)\nprint(lasso_reg.best_score_)\n","7daba92e":"from sklearn.metrics import mean_squared_log_error\n\npred = lasso_reg.predict(X_test)\nmean_squared_log_error(pred, y_test)","16fee84d":"prediction = lasso_reg.predict(test)\n\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest_data.Id\n\nsubmission = pd.DataFrame({'Id': test_data.Id, 'SalePrice': prediction})\n# you could use any filename. We choose submission here\nsubmission.to_csv('submission.csv', index=False)","9f5fb83d":"from sklearn.model_selection import train_test_split \nfrom sklearn.preprocessing import StandardScaler\n# Initialise the Scaler \nscaler = StandardScaler() \n  \n# To scale data \nscaler.fit(df) \ndf = pd.DataFrame(scaler.transform(df))\ntrain = df.iloc[:target.shape[0],:] \ntest = df.iloc[target.shape[0]:,:]\n\n\nX_train,X_test,y_train,y_test = train_test_split(train,target,test_size=0.2)\ny_test.shape\n#print(target.shape, train.shape, test.shape)","6e87c9c4":"from sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import GridSearchCV\n\nlasso = Lasso()\nparameters = {\"alpha\" :np.arange(50, 100, 0.1).tolist() }\nlasso_reg = GridSearchCV(lasso, parameters, scoring = \"neg_mean_squared_error\", cv = 5, verbose=1)\nlasso_reg.fit(X_train, y_train)\n\nfrom sklearn.metrics import mean_squared_log_error\n\npred = lasso_reg.predict(X_test)\nmean_squared_log_error(pred, y_test)","f4e95b04":"from sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import ElasticNet\nmodel = ElasticNet(alpha=1.0, l1_ratio=0.5)\n# define model evaluation method\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# force scores to be positive\nscores = np.absolute(scores)\nprint('Mean MAE: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))","b2008655":"\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\nmean_squared_log_error(pred, y_test)","6f4213c5":"import xgboost as xg\n\ntrain_dmatrix = xg.DMatrix(data = X_train, label = y_train) \ntest_dmatrix = xg.DMatrix(data = X_test, label = y_test) \n  \n# Parameter dictionary specifying base learner \nparam = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"} \n  \nxgb_r = xg.train(params = param, dtrain = train_dmatrix, num_boost_round = 10) \npred = xgb_r.predict(test_dmatrix) \n  \nmean_squared_log_error(pred, y_test)","b9c0c66b":"results.best_estimator_\n","a9a38cb5":"Now we will try to fill the columns which contain any NA values (mode and mean values)","60d29932":"![](http:\/\/)Now we can see that ['KitchenAbvGr'] has low variance so we can drop this column from our data frame  ","bb7e68bf":"## Using Ridge and Lasso regression for the Prediction","ea9f7893":"# Feature Selection Techniques (VarianceThreshold and correlation)\n","6920c030":"Deleteing the column with more than 100 missing value "}}