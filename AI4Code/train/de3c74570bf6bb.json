{"cell_type":{"4ec13e9d":"code","a11c68e5":"code","f0eb531d":"code","0ae1cba8":"code","09c5707a":"code","988ff905":"code","2c461d7a":"code","22893136":"code","8cc4a872":"code","dcf67ee0":"code","de1d8e93":"code","d5d5c204":"code","edacf0d2":"code","88974da0":"code","35e8efbc":"code","61db6395":"code","c9e0ec7c":"code","4f65e914":"code","0c519c72":"code","34bbbcc7":"code","90f87ea1":"code","3167ea0e":"code","3f7af02f":"code","62a23018":"code","8c7479f6":"code","36f551b5":"code","315abeff":"code","54b73e4c":"code","efcaa98d":"code","4d1577ea":"code","2fc254ef":"code","ea76378a":"code","192d7dbe":"code","e24a122f":"code","b36dab97":"code","6d221fe3":"code","be06c35b":"code","f5d9c917":"code","27dfcc28":"code","9b8355fa":"code","a5cf4c23":"code","b19f228e":"code","2e27dcae":"code","c7282448":"code","be0d6c8d":"code","a0f44a8f":"code","cf5d2628":"code","45ce613a":"code","3440c194":"code","0b92e20f":"code","d519c5ab":"code","bd334988":"code","acc3db86":"code","9717005a":"code","5529ea9c":"code","7e166e9a":"code","6c4b5967":"code","ee5497d0":"code","f722a690":"code","443fcf69":"code","f07a84ef":"code","d7158e02":"code","ce39585d":"code","ccbaa8b5":"code","7fe321ec":"code","b28d3eeb":"code","72431e7e":"code","d7340c21":"code","7256ff29":"code","81ac7efc":"code","ca161d8a":"code","d4eaadfe":"code","3fd153ff":"code","f5ec7c2f":"code","b5b10e8f":"code","5124d0b3":"code","71b3dd7e":"code","e4379b43":"code","2e7aa715":"code","b1a2e704":"code","7457963d":"code","815a2558":"code","e3242056":"code","259dc31f":"code","310a05c5":"code","568dd7fb":"code","a5960843":"code","b552bef8":"code","93ff136f":"code","426f291f":"code","24c3404a":"code","fba88608":"code","7f2db4ff":"markdown","0547766c":"markdown","06940b6e":"markdown","ee705393":"markdown","ce6aa063":"markdown","52b610a7":"markdown","082615b1":"markdown","75c8155d":"markdown","d8a3449c":"markdown","44aa846e":"markdown","2cc10a92":"markdown","cbd53f03":"markdown","cd05648d":"markdown","e91d0173":"markdown","c5707036":"markdown","cd174637":"markdown","af32ad71":"markdown","2335a6f1":"markdown","3b5d77a7":"markdown","b615fc4c":"markdown","6bbc634b":"markdown","f8bd798d":"markdown","e59da5ff":"markdown","664c1fda":"markdown","93480758":"markdown","be6cea9a":"markdown","3024cddb":"markdown","45869a47":"markdown","ff6683c1":"markdown","eaa1512d":"markdown","7ae869a8":"markdown","ce46237f":"markdown","356954c5":"markdown","04c68882":"markdown","9637035c":"markdown","c122dea6":"markdown","65d83707":"markdown","42df1882":"markdown"},"source":{"4ec13e9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a11c68e5":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","f0eb531d":"GCARS_PATH = '\/kaggle\/input\/cars-germany'\n\ndef load_german_cars(gcars_path=GCARS_PATH):\n  csv_path = os.path.join(gcars_path, 'autoscout24-germany-dataset.csv')\n  return pd.read_csv(csv_path)\n\ncars = load_german_cars()","0ae1cba8":"%matplotlib inline","09c5707a":"cars.head()","988ff905":"cars.describe()","2c461d7a":"cars.head()","22893136":"cars.isnull().values.any()","8cc4a872":"cars = cars.dropna()","dcf67ee0":"cars.isnull().sum()","de1d8e93":"cars.shape[0]","d5d5c204":"cars = cars.drop_duplicates(keep='first')","edacf0d2":"cars.shape[0]","88974da0":"from datetime import datetime","35e8efbc":"cars['age'] = datetime.now().year - cars['year']\n\ncars.drop('year', axis=1, inplace=True)\n\ncars.head()","61db6395":"mediana = cars.price.median()\nmediana","c9e0ec7c":"media = cars.price.mean()\nmedia","4f65e914":"abaixo_mediana = cars.query(\"price<10990\")\nq_abaixo_mediana = abaixo_mediana.value_counts().sum()\n\nacima_mediana = cars.query(\"price > 10990.1\")\nq_acima_mediana = acima_mediana.value_counts().sum()\n\nprint(f'Mediana = {mediana}')\nprint('Quantidade de carros com valores ACIMA da mediana')\nprint(q_acima_mediana)\nprint('Quantidade de carros com valores ABAIXO da mediana')\nprint(q_abaixo_mediana)\nprint('--------------------------------------------')\n\nabaixo_media = cars.query(\"price<16546\")\nq_abaixo_media = abaixo_media.value_counts().sum()\n\nacima_media = cars.query(\"price > 16546.1\")\nq_acima_media = acima_media.value_counts().sum()\n\nprint(f'Media = {media}')\nprint('Quantide de carros com valores ACIMA da media')\nprint(q_acima_media)\nprint('Quantidade de carros com valores ABAIXO da media')\nprint(q_abaixo_media)","0c519c72":"sns.scatterplot(x=cars['hp'], y=cars['price'])","34bbbcc7":"fig = plt.figure(figsize=(20,15))\nax = plt.axes(projection='3d')\n\nhp = cars['hp']\nage = cars['age']\nprice = cars['price']\n\nax.scatter3D(age, hp, price, c=cars['age'], s=1)\nax.set_xlabel('Age')\nax.set_ylabel('HP')\nax.set_zlabel('Price')","90f87ea1":"cars['fuel'] = cars['fuel'].replace('Diesel', 0)\ncars['fuel'] = cars['fuel'].replace('Gasoline', 1)\ncars['fuel'] = cars['fuel'].replace(['Electric\/Gasoline', 'Electric\/Diesel', 'Electric'],  2)\ncars['fuel'] = cars['fuel'].replace(['CNG', 'LPG', 'Others', '-\/- (Fuel)', 'Ethanol', 'Hydrogen'], 3)","3167ea0e":"cars['fuel'].unique()","3f7af02f":"eletricos = cars.query('fuel == 2')\neletricos.head()","62a23018":"eletricos['price'].mean()","8c7479f6":"fig = plt.figure()\nax = plt.axes(projection='3d')\n\nhp = cars['hp']\nage = cars['age']\nprice = cars['price']\n\nax.scatter3D(age, hp, price, c=cars['age'])\nax.set_xlabel('Age')\nax.set_ylabel('HP')\nax.set_zlabel('Price')","36f551b5":"plt.figure(figsize=(14,7))\nsns.heatmap(cars.corr(),annot=True, cmap='coolwarm')","315abeff":"sns.scatterplot(x=cars['hp'], y=cars['price'])","54b73e4c":"min_price, max_price = cars.price.quantile([0.01, 0.99])\nmin_price, max_price","efcaa98d":"cars[cars['price']>max_price].value_counts().sum()","4d1577ea":"cars[cars['price']>max_price]","2fc254ef":"cars[cars['price']<min_price].value_counts().sum()","ea76378a":"cars[cars['price']<min_price]","192d7dbe":"pop_cars = cars[(cars.price<max_price) & (cars.price>min_price)]\nprint('Total number of cars:')\nprint(cars.shape[0])\nprint('---------------------')\nprint('Numers of cars that are abore $3.300,0 and below $99.999,0')\nprint(pop_cars.shape[0])","e24a122f":"sns.scatterplot(x=pop_cars['hp'], y=pop_cars['price'])","b36dab97":"sns.scatterplot(x=pop_cars['hp'], y=pop_cars['price'])","6d221fe3":"sns.scatterplot(x=pop_cars['mileage'], y=pop_cars['price'])","be06c35b":"min_price, max_price = cars.mileage.quantile([0.01, 0.99])\nmin_price, max_price","f5d9c917":"pop_cars = pop_cars[(pop_cars.mileage<max_price) & (pop_cars.mileage>min_price)]","27dfcc28":"sns.scatterplot(x=pop_cars['mileage'], y=pop_cars['price'])","9b8355fa":"sns.scatterplot(x=pop_cars['hp'], y=pop_cars['price'])","a5cf4c23":"min_price, max_price = cars.hp.quantile([0.01, 0.999])\nmin_price, max_price","b19f228e":"pop_cars = pop_cars[(pop_cars.hp<max_price) & (pop_cars.hp>min_price)]","2e27dcae":"sns.scatterplot(x=pop_cars['hp'], y=pop_cars['price'])","c7282448":"pop_cars['fuel'] = pop_cars['fuel'].replace(0, 'Diesel')\npop_cars['fuel'] = pop_cars['fuel'].replace(1, 'Gasoline')\npop_cars['fuel'] = pop_cars['fuel'].replace(2, 'Electric')\npop_cars['fuel'] = pop_cars['fuel'].replace(3, 'Others')","be0d6c8d":"pop_cars = pop_cars.reset_index(drop=True)","a0f44a8f":"pop_cars = pop_cars.drop(columns=['make', 'model'], axis=1)\npop_cars.head()","cf5d2628":"pop_cars.dtypes.value_counts()","45ce613a":"mask = pop_cars.dtypes == np.object\ncategorical = pop_cars.columns[mask]\ncategorical","3440c194":"num_ohc_cols = (pop_cars[categorical].apply(lambda x: x.nunique()).sort_values(ascending=False))\nnum_ohc_cols","0b92e20f":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\ndata_ohc = pop_cars.copy()\n\nohc = OneHotEncoder()\n\nfor col in num_ohc_cols.index:\n  #this is a sparse array\n  new_dat = ohc.fit_transform(data_ohc[[col]])\n  #drop original column from original DF\n  data_ohc = data_ohc.drop(col, axis=1)\n  #get unique names of columns\n  cats = ohc.categories_\n  #create a column for each OHE column by value\n  new_cols = ['_'.join([col,cat]) for cat in cats[0]]\n  #create the new Dataset\n  new_df = pd.DataFrame(new_dat.toarray(), columns=new_cols)\n  #append new data to df\n  data_ohc=pd.concat([data_ohc, new_df], axis=1)\n\ny_col = 'price'\n\nfeature_cols = [x for x in data_ohc.columns if x != y_col]\n\nX = data_ohc[feature_cols]\ny = data_ohc[y_col]","d519c5ab":"X.head()","bd334988":"from sklearn.model_selection import KFold\n\nkf = KFold(shuffle=True, random_state=72018, n_splits=3)\n\nkf.split(X)","acc3db86":"for train_index, test_index in kf.split(X):\n  print(\"Train index:\", train_index[:10], len(train_index))\n  print(\"Test index:\", test_index[:10], len(test_index))\n  print('')","9717005a":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nscores = []\nlr = LinearRegression()\n\nfor train_index, test_index in kf.split(X):\n  X_train, X_test, y_train, y_test = (X.iloc[train_index, :], X.iloc[test_index, :], y[train_index], y[test_index])\n  \n  lr.fit(X_train, y_train)\n\n  y_pred = lr.predict(X_test)\n\n  score = r2_score(y_test.values, y_pred)\n\n  scores.append(score)\n\nscores","5529ea9c":"from sklearn.preprocessing import StandardScaler\n\nscores = []\nlr = LinearRegression()\ns = StandardScaler()\n\nfor train_index, test_index in kf.split(X):\n  X_train, X_test, y_train, y_test = (X.iloc[train_index, :], X.iloc[test_index, :], y[train_index], y[test_index])\n  \n  X_train_s = s.fit_transform(X_train)\n  \n  lr.fit(X_train_s, y_train)\n\n  X_test_s = s.transform(X_test)\n\n  y_pred = lr.predict(X_test_s)\n\n  score = r2_score(y_test.values, y_pred)\n\n  scores.append(score)\n\nscores","7e166e9a":"from sklearn.pipeline import Pipeline\n\nestimator = Pipeline([('scaler', s), ('linear_reg', lr)])\n\nestimator.fit(X_train, y_train)\n\nestimator.predict(X_test)","6c4b5967":"kf","ee5497d0":"from sklearn.model_selection import cross_val_predict\n\npredictions = cross_val_predict(estimator, X, y, cv=kf, verbose=100)","f722a690":"len(predictions)","443fcf69":"r2_score(y, predictions)","f07a84ef":"np.mean(scores)","d7158e02":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import KFold\n\npolis = [2, 3, 4]\n\nlr1 = LinearRegression()\n\nscores = []\n\nfor poli in polis:\n  pf = PolynomialFeatures(poli)\n\n  estimator = Pipeline([('make_higher_degree', pf), ('linear_reg', lr1)])\n\n  predictions = cross_val_predict(estimator, X, y, cv=kf, verbose=100)\n\n  score = r2_score(y, predictions)\n\n  scores.append(score)","ce39585d":"list(zip(polis, scores))","ccbaa8b5":"import numpy as np\n\nalphas = np.geomspace(1e-9, 1e-0, num=10)\nalphas\n\nfrom sklearn.linear_model import Lasso\n\n\nscores_lasso = []\ncoefs = []\n\nfor alpha in alphas:\n  las = Lasso(alpha=alpha, max_iter=100000)\n\n  estimator = Pipeline([('scaler', s), ('lasso_regression', las)])\n\n  predictions = cross_val_predict(estimator, X, y, cv=kf, verbose=100)\n\n  score = r2_score(y, predictions)\n\n  scores.append(score)","7fe321ec":"list(zip(alphas, scores))","b28d3eeb":"Lasso(alpha=1e-6).fit(X,y).coef_","72431e7e":"Lasso(alpha=1).fit(X,y).coef_","d7340c21":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import KFold\n\npf = PolynomialFeatures(degree=3)\n\nscores = []\nalphas = np.geomspace(9, 12, 4)\n# alphas = np.geomspace(0.001, 10, 5)\n\nfor alpha in alphas:\n  las = Lasso(alpha=alpha, max_iter=10000)\n\n  estimator = Pipeline([('make_higher_degree', pf), ('scaler', s), ('lasso_regression', las)])\n\n  predictions = cross_val_predict(estimator, X, y, cv=kf, verbose=100)\n\n  score = r2_score(y, predictions)\n\n  scores.append(score)","7256ff29":"alphas","81ac7efc":"scores","ca161d8a":"import matplotlib.pyplot as plt\n\nplt.semilogx(alphas, scores)","d4eaadfe":"best_estimator = Pipeline([('make_higher_degree', PolynomialFeatures(degree=2)), ('scaler', s), ('lasso_regression', Lasso(alpha=10, max_iter=100000))])\n\nbest_estimator.fit(X,y)\nbest_estimator.score(X,y)","3fd153ff":"best_estimator.named_steps['lasso_regression'].coef_","f5ec7c2f":"plt.figure(figsize=(10,6))\nplt.semilogx(alphas, scores, '-o')\nplt.xlabel('alphas')\nplt.ylabel('R\u02c62')","b5b10e8f":"X.shape","5124d0b3":"y.shape","71b3dd7e":"from sklearn.linear_model import Ridge\n\npf = PolynomialFeatures(degree=2)\n\nalphas = np.geomspace(0.1,2,20)\n\nscores=[]\n\nfor alpha in alphas:\n  ridge = Ridge(alpha=alpha, max_iter=100000)\n\n  estimator = Pipeline([('make_higher_degree', pf), ('scaler', s), ('ridge', ridge)])\n\n  predictions = cross_val_predict(estimator, X, y, cv=kf, verbose=100)\n  score=r2_score(y, predictions)\n  scores.append(score)","e4379b43":"plt.plot(alphas, scores)","2e7aa715":"best_estimator = Pipeline([('make_higher_degree', PolynomialFeatures(degree=2, include_bias=False)), ('scaler', s), ('lasso_regression', Lasso(alpha=10))])\n\nbest_estimator.fit(X,y)\nbest_estimator.score(X,y)","b1a2e704":"fs_importances = pd.DataFrame(zip(best_estimator.named_steps['make_higher_degree'].get_feature_names(input_features=X.columns), best_estimator.named_steps['lasso_regression'].coef_,))","7457963d":"fs_importances.sort_values(by=1)","815a2558":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Ridge\n\nestimator = Pipeline([('polynomial_features', PolynomialFeatures()), ('scaler', s), ('ridge_regression', Ridge())])\n\nparams = {'polynomial_features__degree': [1,2,3], \n          'ridge_regression__alpha': np.geomspace(4, 20, 30)}\n\ngrid = GridSearchCV(estimator, params, cv=kf)","e3242056":"grid.fit(X,y)","259dc31f":"grid.best_score_, grid.best_params_","310a05c5":"y_predict = grid.predict(X)","568dd7fb":"r2_score(y, y_predict)","a5960843":"grid.best_estimator_.named_steps['ridge_regression'].coef_","b552bef8":"pd.DataFrame(grid.cv_results_)","93ff136f":"from sklearn.metrics import mean_squared_error\n\ndef rmse(ytrue, ypredict):\n  return np.sqrt(mean_squared_error(ytrue, ypredict))","426f291f":"from sklearn.linear_model import RidgeCV\n\nalphas = [0.001, 0.003, 0.005, 0.05, 0.1, 0.3, 1, 2, 5, 10]\n\nridgeCV = RidgeCV(alphas=alphas, cv=4).fit(X_train, y_train)\n\nridgeCV_rmse = rmse(y_test, ridgeCV.predict(X_test))\n\nprint(ridgeCV.alpha_, ridgeCV_rmse)","24c3404a":"from sklearn.linear_model import LassoCV\n\nalphas2 = [0.005, 0.05, 0.1, 0.3, 1, 2, 5, 10, 20, 50, 70, 100, 120]\n\nlassoCV = LassoCV(alphas=alphas2, max_iter=100000, cv=4).fit(X_train, y_train)\n\nlassoCV_rmse = rmse(y_test, lassoCV.predict(X_test))\n\nprint(lassoCV.alpha_, lassoCV_rmse)","fba88608":"from sklearn.linear_model import ElasticNetCV\n\nalphas3 = [0.05, 0.1, 0.3, 1, 2, 5, 10, 20, 50]\nl1_ratios = np.linspace(0.1, 0.9, 9)\n\nelasticNetCV = ElasticNetCV(alphas=alphas3, l1_ratio=l1_ratios, max_iter=100000, cv=4).fit(X_train, y_train)\n\nelasticNetCV_rmse = rmse(y_test, elasticNetCV.predict(X_test))\n\nprint(elasticNetCV.alpha_, elasticNetCV.l1_ratio_, elasticNetCV_rmse)","7f2db4ff":"## Linear Regression With Regularization","0547766c":"## Linear Regression with Polynomial Regularization","06940b6e":"## Data and Feature Description","ee705393":"1. Dealing with null values\n2. Age Feature to years old\n3. EDA\n4. Feature Correlation\n5. Dealing with the most important features outliers (Price, HP and Mileage)\n6. One Hot Encoded categorical features","ce6aa063":"The main objective of this analysis is to **predict** the cost of a cars based on data gathered from germany.","52b610a7":"Fazer o que fizemos em cima com o Pipeline","082615b1":"## Correlation","75c8155d":"## ElasticNetCV","d8a3449c":"## Age Feature","44aa846e":"## Lasso CV","2cc10a92":"# Modeling","cbd53f03":"## Lasso with Polynomial","cd05648d":"### HP Outliers","e91d0173":"## Ridge","c5707036":"## Data Exploration and Data Cleaning","cd174637":"This dataset whas downloaded from Kaggle, and it is a dataset containing a list of cars that are selling in Germany contains 46.405 rows, with 9 features.\n\n*   Number of Categorical Features - 5\n*   number of Numeric Features - 4\n\n","af32ad71":"## One Hot Encoding and KFolds","2335a6f1":"Now lets do the same but scaling our data as we go through the folds","3b5d77a7":"## Main Objective","b615fc4c":"You are going to choose between these 3 above for this different reasons:\n\n\n*   LassoCV - Reduce coeficients and do feature selection\n*   RigdeCV - If you want to run it quicly\n*   ElasticNetCV - If you want a ballance out of the two\n\n","6bbc634b":"## Linear regressions Without Regularization","f8bd798d":"To do cross-validation, we used two techniques:\n\n\n\n*   use KFolds and manually create a loop to do cross validation\n*   Use  cross_val_predict and score in a couple of lines\n\nTo do Hyper-parameters tuning, we see a general pattern:\n\n*   use cross_val_predict and score in a manually written loop over hyperparameters, then select the best one\n\nPerhaps not surprisingly, there is a function that does this for us -- \n\n```\nGridSearchCV()\n```","e59da5ff":"## EDA","664c1fda":"Lasso with the best estimator from above","93480758":"# Germany - Cars Price Prediction","be6cea9a":"### Mileage Outliers","3024cddb":"## Lasso with Regularization","45869a47":"We can see that this is almost the same. Linear Regression doestn change much with Regularization","ff6683c1":"# Data Cleaning, Analysis and Feature Engineering","eaa1512d":"This creates a Tuple, for 3 different scenarios(n_plits), that is: train_index, test_index","7ae869a8":"### Price Outliers","ce46237f":"## Outliers","356954c5":"# Imports and Data Upload","04c68882":"## Giving names to the feature Fuel again","9637035c":"## Dealing with Null Values","c122dea6":"## GridSearchCV","65d83707":"## Linear Regression with Regu, Pipeline com Cross Val Predict","42df1882":"## Ridge CV"}}