{"cell_type":{"afb31477":"code","79d09d06":"code","19afb492":"code","40b5decb":"code","b52b60e7":"code","f8cd14f6":"code","4f32614a":"code","961a6fc6":"code","ed0b85ac":"code","c4bb549d":"code","091bb2f6":"code","46fdc888":"code","b27aa7d4":"code","56c2cd58":"code","ae1925b2":"code","349872df":"code","7b674acf":"code","439ac4c0":"code","e829c584":"code","f427c822":"code","7258e2f9":"code","e8f7defc":"code","879c6e12":"code","67c004a0":"code","6fbe1245":"code","5c44efe1":"code","8e261bdd":"code","357a372c":"markdown","6c9b356d":"markdown","bbd7b6d4":"markdown","af3f3f6e":"markdown","41c618a5":"markdown","0cd5062f":"markdown","937b69da":"markdown","4e231ea3":"markdown","5a2709d8":"markdown"},"source":{"afb31477":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79d09d06":"dataset = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","19afb492":"dataset.head()","40b5decb":"dataset.info()","b52b60e7":"numerical_float64 = (dataset.dtypes == \"float64\")\nnumerical_float64_list = list(numerical_float64[numerical_float64].index)\n\nnumerical_int64 = (dataset.dtypes == \"int64\")\nnumerical_int64_list = list(numerical_int64[numerical_int64].index)","f8cd14f6":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(dataset[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} Distribution with Histogram\".format(variable))\n    plt.show()","4f32614a":"import matplotlib.pyplot as plt\nimport seaborn as sns;\nsns.set()\n%matplotlib inline","961a6fc6":"for n in numerical_float64_list:\n    plot_hist(n)","ed0b85ac":"for n in numerical_int64_list:\n    plot_hist(n)","c4bb549d":"plt.figure(figsize=(25,15))\n\nplt.subplot(3,3,1)\nsns.histplot(dataset['Pregnancies'], color='red', kde=True).set_title('Pregnancies Interval and Counts')\n\nplt.subplot(3,3,2)\nsns.histplot(dataset['Glucose'], color='green', kde=True).set_title('Glucose Interval and Counts')\n\nplt.subplot(3,3,3)\nsns.histplot(dataset['BloodPressure'], color = 'blue', kde=True).set_title('BloodPressure Interval and Counts')\n\nplt.subplot(3,3,4)\nsns.histplot(dataset['SkinThickness'], color='black', kde=True).set_title('SkinThickness Interval and Counts')\n\nplt.subplot(3,3,5)\nsns.histplot(dataset['Insulin'], color='orange', kde = True).set_title('Insulin Interval and Counts')\n\nplt.subplot(3,3,6)\nsns.histplot(dataset['BMI'], color='cyan', kde = True).set_title('BMI Interval and Counts')\n\nplt.subplot(3,3,7)\nsns.histplot(dataset['DiabetesPedigreeFunction'], color='yellow', kde = True).set_title('Diabetes Pedigree Function Interval and Counts')\n\nplt.subplot(3,3,8)\nsns.histplot(dataset['Age'], color='pink', kde = True).set_title('Age Interval and Counts')","091bb2f6":"features = dataset.columns\nsns.set_style('darkgrid')\nsns.pairplot(dataset[features])","46fdc888":"sns.pairplot(dataset, hue='Outcome')","b27aa7d4":"dataset.corr()","56c2cd58":"plt.figure(figsize=(12,8)) \nsns.heatmap(dataset.corr(), annot=True, cmap='Dark2_r', linewidths = 2)\nplt.show()","ae1925b2":"sns.set_style('darkgrid')\naxes = pd.plotting.scatter_matrix(dataset, alpha = 0.3, figsize = (10,7), diagonal = 'kde' ,s=80)\ncorr = dataset.corr().values\n\nplt.xticks(fontsize =10,rotation =0)\nplt.yticks(fontsize =10)\nfor ax in axes.ravel():\n    ax.set_xlabel(ax.get_xlabel(),fontsize = 15, rotation = 60)\n    ax.set_ylabel(ax.get_ylabel(),fontsize = 15, rotation = 60)\n# put the correlation between each pair of variables on each graph\nfor i, j in zip(*np.triu_indices_from(axes, k=1)):\n    axes[i, j].annotate(\"%.3f\" %corr[i, j], (0.8, 0.8), xycoords=\"axes fraction\", ha=\"center\", va=\"center\")","349872df":"plt.figure(figsize=(20,15))\nplt.subplot(3, 3, 1)\nsns.barplot(x = 'Outcome', y = 'Pregnancies', data = dataset, palette=\"cubehelix\")\nplt.subplot(3, 3, 2)\nsns.barplot(x = 'Outcome', y = 'Glucose', data = dataset, palette=\"Oranges\")\nplt.subplot(3, 3, 3)\nsns.barplot(x = 'Outcome', y = 'BloodPressure', data = dataset, palette=\"pastel\")\nplt.subplot(3, 3, 4)\nsns.barplot(x = 'Outcome', y = 'Insulin', data = dataset, palette=\"pastel\")\nplt.subplot(3, 3, 5)\nsns.barplot(x = 'Outcome', y = 'SkinThickness', data = dataset, palette=\"cubehelix\")\nplt.subplot(3, 3, 6)\nsns.barplot(x = 'Outcome', y = 'BMI', data = dataset, palette=\"Oranges\")\nplt.subplot(3, 3, 7)\nsns.barplot(x = 'Outcome', y = 'DiabetesPedigreeFunction', data = dataset, palette=\"cubehelix\")\nplt.subplot(3, 3, 8)\nsns.barplot(x = 'Outcome', y = 'Age', data = dataset, palette=\"Oranges\")","7b674acf":"plt.figure(figsize=(20,15))\nplt.subplot(3, 3, 1)\nsns.violinplot(x = 'Outcome', y = 'Pregnancies', data = dataset, palette=\"rocket_r\")\nplt.subplot(3, 3, 2)\nsns.violinplot(x = 'Outcome', y = 'Glucose', data = dataset, palette=\"rocket_r\")\nplt.subplot(3, 3, 3)\nsns.violinplot(x = 'Outcome', y = 'BloodPressure', data = dataset, palette=\"rocket_r\")\nplt.subplot(3, 3, 4)\nsns.violinplot(x = 'Outcome', y = 'Insulin', data = dataset, palette=\"rocket_r\")\nplt.subplot(3, 3, 5)\nsns.violinplot(x = 'Outcome', y = 'SkinThickness', data = dataset, palette=\"rocket_r\")\nplt.subplot(3, 3, 6)\nsns.violinplot(x = 'Outcome', y = 'BMI', data = dataset, palette=\"rocket_r\")\nplt.subplot(3, 3, 7)\nsns.violinplot(x = 'Outcome', y = 'DiabetesPedigreeFunction', data = dataset, palette=\"rocket_r\")\nplt.subplot(3, 3, 8)\nsns.violinplot(x = 'Outcome', y = 'Age', data = dataset, palette=\"rocket_r\")","439ac4c0":"plt.figure(figsize=(20,15))\nplt.subplot(3, 3, 1)\nsns.boxplot(x = 'Outcome', y = 'Pregnancies', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3, 3, 2)\nsns.boxplot(x = 'Outcome', y = 'Glucose', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3, 3, 3)\nsns.boxplot(x = 'Outcome', y = 'BloodPressure', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3, 3, 4)\nsns.boxplot(x = 'Outcome', y = 'Insulin', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3, 3, 5)\nsns.boxplot(x = 'Outcome', y = 'SkinThickness', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3, 3, 6)\nsns.boxplot(x = 'Outcome', y = 'BMI', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3, 3, 7)\nsns.boxplot(x = 'Outcome', y = 'DiabetesPedigreeFunction', data = dataset, palette=\"gist_ncar_r\")\nplt.subplot(3, 3, 8)\nsns.boxplot(x = 'Outcome', y = 'Age', data = dataset, palette=\"gist_ncar_r\")","e829c584":"import pandas_profiling as pp\npp.ProfileReport(dataset)","f427c822":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.feature_selection import SelectFromModel\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom collections import Counter\nfrom sklearn import tree\nimport graphviz \nimport os\nimport preprocessing \n\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.decomposition import PCA\n\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom xgboost import plot_tree, plot_importance\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","7258e2f9":"X = dataset.iloc[:,:8].values \ny = dataset.iloc[:,8:].values","e8f7defc":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101) \nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Total # of sample in validation dataset: {len(X_valid)}')\nprint(f'Total # of sample in test dataset: {len(X_test)}')","879c6e12":"sc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","67c004a0":"models = {\n    'GaussianNB': GaussianNB(),\n    'BernoulliNB': BernoulliNB(),\n    'LogisticRegression': LogisticRegression(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'SupportVectorMachine': SVC(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n    'Neural Nets': MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1),\n}\n\nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Neural Nets']\n\ntrainScores = []\nvalidationScores = []\ntestScores = []\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  score = model.score(X_valid, y_valid)\n  #print(f'{m} validation score => {score*100}')\n    \n  print(f'{m}') \n  train_score = model.score(X_train, y_train)\n  print(f'Train score of trained model: {train_score*100}')\n  trainScores.append(train_score*100)\n\n  validation_score = model.score(X_valid, y_valid)\n  print(f'Validation score of trained model: {validation_score*100}')\n  validationScores.append(validation_score*100)\n\n  test_score = model.score(X_test, y_test)\n  print(f'Test score of trained model: {test_score*100}')\n  testScores.append(test_score*100)\n  print(\" \")\n    \n  y_predictions = model.predict(X_test)\n  conf_matrix = confusion_matrix(y_predictions, y_test)\n\n  print(f'Confussion Matrix: \\n{conf_matrix}\\n')\n\n  predictions = model.predict(X_test)\n  cm = confusion_matrix(predictions, y_test)\n\n  tn = conf_matrix[0,0]\n  fp = conf_matrix[0,1]\n  tp = conf_matrix[1,1]\n  fn = conf_matrix[1,0]\n  accuracy  = (tp + tn) \/ (tp + fp + tn + fn)\n  precision = tp \/ (tp + fp)\n  recall    = tp \/ (tp + fn)\n  f1score  = 2 * precision * recall \/ (precision + recall)\n  specificity = tn \/ (tn + fp)\n  print(f'Accuracy : {accuracy}')\n  print(f'Precision: {precision}')\n  print(f'Recall   : {recall}')\n  print(f'F1 score : {f1score}')\n  print(f'Specificity : {specificity}')\n  print(\"\") \n  print(f'Classification Report: \\n{classification_report(predictions, y_test)}\\n')\n  print(\"\")\n   \n  for m in range (1):\n    current = modelNames[m]\n    modelNames.remove(modelNames[m])\n\n  preds = model.predict(X_test)\n  confusion_matr = confusion_matrix(y_test, preds) #normalize = 'true'\n  print(\"############################################################################\")\n  print(\"\")\n  print(\"\")\n  print(\"\")","6fbe1245":"plt.figure(figsize=(20,10))\nsns.set_style('darkgrid')\nplt.title('Train - Validation - Test Scores of Models', fontweight='bold', size = 24)\n\nbarWidth = 0.25\n \nbars1 = trainScores\nbars2 = validationScores\nbars3 = testScores\n \nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \nplt.bar(r1, bars1, color='blue', width=barWidth, edgecolor='white', label='train', yerr=0.5,ecolor=\"black\",capsize=10)\nplt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='validation', yerr=0.5,ecolor=\"black\",capsize=10, alpha = .50)\nplt.bar(r3, bars3, color='red', width=barWidth, edgecolor='white', label='test', yerr=0.5,ecolor=\"black\",capsize=10, hatch = '-')\n \nmodelNames = [\"GaussianNB\", 'BernoulliNB','LogisticRegression','RandomForestClassifier','SupportVectorMachine',\n             'DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',\n             'Stochastic Gradient Descent', 'Neural Nets']\n    \nplt.xlabel('Algorithms', fontweight='bold', size = 24)\nplt.ylabel('Scores', fontweight='bold', size = 24)\nplt.xticks([r + barWidth for r in range(len(bars1))], modelNames, rotation = 75)\n \nplt.legend()\nplt.show()","5c44efe1":"for i in range(10):\n    print(f'Accuracy of {modelNames[i]} -----> {testScores[i]}')","8e261bdd":"models = {\n    'BernoulliNB': BernoulliNB(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),\n}\n\nfor m in models:\n  model = models[m]\n  model.fit(X_train, y_train)\n  \n  print(f'{m}') \n  best_features = SelectFromModel(model)\n  best_features.fit(X, y)\n\n  transformedX = best_features.transform(X)\n  print(f\"Old Shape: {X.shape} New shape: {transformedX.shape}\")\n  print(\"\\n\")","357a372c":"<a id=\"5\"><\/a> \n# Divis\u00e3o entre Treinamento e Teste","6c9b356d":"<a id=\"9\"><\/a> \n# PERFIL Pandas","bbd7b6d4":"<a id=\"4\"><\/a> \n# Visualiza\u00e7\u00e3o","af3f3f6e":"<a id=\"2\"><\/a>\n# An\u00e1lise gr\u00e1fica das vari\u00e1veis num\u00e9ricas","41c618a5":"Normalizando os valores","0cd5062f":"# An\u00e1lise da base de dados sobre diabetes (Pima Indians)\n<img src='https:\/\/static.tuasaude.com\/media\/article\/dr\/nj\/diabetes_52276_l.jpg' width='35%'\/>\n","937b69da":"<a id=\"6\"><\/a> \n# Scores dos Modelos","4e231ea3":"<a id=\"8\"><\/a>\n# Sele\u00e7\u00e3o das Melhores Features","5a2709d8":"<a id=\"3\"><\/a> \n# Identificando correla\u00e7\u00e3o entre as features"}}