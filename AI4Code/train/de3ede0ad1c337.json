{"cell_type":{"89633628":"code","ddd541ee":"code","0a84d35e":"code","f99eb60c":"code","fe459c86":"code","ad0937a5":"code","38dcc1cc":"code","a63fff64":"code","65020c27":"markdown","58b1e02e":"markdown"},"source":{"89633628":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport lightgbm as lgb\nfrom pathlib import Path\nimport seaborn\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom warnings import simplefilter\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ddd541ee":"!pip install kaggler","0a84d35e":"import kaggler\nfrom kaggler.model import AutoLGB\nprint(kaggler.__version__)\nplt.style.use('fivethirtyeight')\npd.set_option('max_columns', 100)\nsimplefilter('ignore')","f99eb60c":"data_dir = Path('..\/input\/tabular-playground-series-aug-2021')\ntrain_file = data_dir \/ 'train.csv'\ntest_file = data_dir \/ 'test.csv'\nsample_file = data_dir \/ 'sample_submission.csv'\nid_col = 'id'\ntarget_col = ['loss']\n\nn_fold = 5\nseed = 42","fe459c86":"trn = pd.read_csv(train_file, index_col=id_col)\ntst = pd.read_csv(test_file, index_col=id_col)\nsub = pd.read_csv(sample_file, index_col=id_col)\nprint(trn.shape, tst.shape, sub.shape)","ad0937a5":"n_trn = trn.shape[0]\ndf = pd.concat([trn.drop(target_col, axis=1), tst], axis=0)\nprint(df.shape)","38dcc1cc":"cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\nX = df\ny = pd.Series(np.concatenate([np.zeros(n_trn,), np.ones(df.shape[0] - n_trn,)]))\np = np.zeros_like(y, dtype=float)\nfor i, (i_trn, i_val) in enumerate(cv.split(X, y)):\n    if i == 0:\n        clf = AutoLGB(objective='binary', metric='auc', random_state=seed)\n        clf.tune(X.iloc[i_trn], y[i_trn])\n        features = clf.features\n        params = clf.params\n        n_best = clf.n_best\n        print(f'{n_best}')\n        print(f'{params}')\n        print(f'{features}')\n    \n    trn_data = lgb.Dataset(X.iloc[i_trn], y[i_trn])\n    val_data = lgb.Dataset(X.iloc[i_val], y[i_val])\n    clf = lgb.train(params, trn_data, n_best, val_data, verbose_eval=100)\n    p[i_val] = clf.predict(X.iloc[i_val])\n    print(f'CV #{i + 1} AUC: {roc_auc_score(y[i_val], p[i_val]):.6f}')","a63fff64":"print(f'CV AUC: {roc_auc_score(y, p):.6f}')","65020c27":"# \"Adversarial validation AUC score is close to 50%. Therefore, we can say that the training and test data sets are similar in terms of feature distributions. In other words, no big shake-up expected at the end of the competition. :)\"","58b1e02e":"# Reference: https:\/\/www.kaggle.com\/jeongyoonlee\/adversarial-validation-with-lightgbm by Jeong-Yoon Lee"}}