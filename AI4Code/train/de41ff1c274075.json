{"cell_type":{"ffbd580a":"code","b0357b73":"code","c6eb456e":"code","475edc8c":"code","f6d285b9":"code","56ebdf97":"code","c49b8632":"code","0406c6a0":"code","e2ee897c":"code","9a1e3eda":"code","3d6fcec9":"code","21eaeb46":"code","e8032f55":"code","14506610":"code","175714dc":"code","96c38b14":"code","11ba8d0c":"code","812c6e8b":"code","05a563e8":"code","dce07fa9":"code","83025f2e":"code","624ffa91":"code","a0df5d5e":"code","da02960c":"code","a0850a21":"code","f29bb935":"code","9ca63673":"code","4390bcc0":"code","8fbfcc80":"code","621b8b61":"code","28b20f6a":"code","118a34aa":"code","61712685":"code","f0a7406d":"code","282d1619":"code","99bf83d4":"code","7469e165":"code","64b2ccf7":"code","bff2f828":"code","4dfe515d":"code","643a1ba8":"code","38fe17ef":"code","ad1da254":"code","7146a6dd":"code","67e0b9a1":"code","b5481ce6":"code","7457572f":"markdown","b1d984d2":"markdown","2c8e9905":"markdown","bc2a35ec":"markdown","86b9f6d6":"markdown","757776a9":"markdown","1d906655":"markdown","efd575b6":"markdown","ac225593":"markdown","cb4bd76e":"markdown","83329c86":"markdown","0964f84f":"markdown","71766bd3":"markdown","c1cc1ddb":"markdown","9b206772":"markdown","b92a13dd":"markdown","c4eb4104":"markdown","faa74faa":"markdown","1e4c16c4":"markdown","b607d830":"markdown","8ebac0e1":"markdown","7fd495d0":"markdown","28932b3c":"markdown","409d3e6a":"markdown","7ae757d1":"markdown","718b21ca":"markdown","0ee2cde9":"markdown","ac34faf6":"markdown","4477a31e":"markdown","28054e73":"markdown","1a5fb392":"markdown"},"source":{"ffbd580a":"    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import seaborn as sns","b0357b73":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c6eb456e":"#So after I ran the cell above it gave me some folder with 3 files and I will from them\ntitanic_train = pd.read_csv('..\/input\/train.csv')\ntitanic_test = pd.read_csv('..\/input\/test.csv')","475edc8c":"#first of all let's see what kind of data we have\ntitanic_train.head(10)","f6d285b9":"titanic_train.drop(['PassengerId'],axis=1,inplace=True)\ntitanic_train.drop(['Ticket'],axis=1,inplace=True)\nID_test = titanic_test['PassengerId']\ntitanic_test.drop(['PassengerId'],axis=1,inplace=True)\ntitanic_test.drop(['Ticket'],axis=1,inplace=True)\ntitanic_test","56ebdf97":"titanic_train[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean().sort_values(by='Survived',ascending=False)","c49b8632":"titanic_train[['Sex','Survived']].groupby(['Sex'],as_index=False).mean().sort_values(by='Sex',ascending=False)","0406c6a0":"titanic_train[['SibSp','Survived']].groupby(['SibSp'],as_index=False).mean().sort_values(by='Survived',ascending=False)","e2ee897c":"titanic_test['Parch']=titanic_test['Parch'].replace(9,6)\ntitanic_test['Parch'].value_counts()","9a1e3eda":"titanic_train[['Parch','Survived']].groupby(['Parch'],as_index=False).mean().sort_values(by='Survived',ascending=False)","3d6fcec9":"f = sns.FacetGrid(titanic_train, col='Survived')\nf.map(plt.hist, 'Age', bins=10)","21eaeb46":"f = sns.FacetGrid(titanic_train, col='Survived',row='Pclass')\nf.map(sns.distplot,'Age')","e8032f55":"f = sns.FacetGrid(titanic_train, col='Survived',row='Sex')\nf.map(sns.distplot,'Age',bins=20)","14506610":"titanic_train.dropna(axis=0,subset=['Survived'],inplace=True)","175714dc":"y = titanic_train['Survived']\ntitanic_train.drop(['Survived'],axis=1,inplace=True)\nX = titanic_train","96c38b14":"total = titanic_train.isnull().sum().sort_values(ascending=False)\npercent = (titanic_train.isnull().sum()\/titanic_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","11ba8d0c":"X.drop(['Cabin'],inplace=True,axis=1)\ntitanic_test.drop(['Cabin'],inplace=True,axis=1)","812c6e8b":"from sklearn.preprocessing import Imputer\nimputer_1 = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\nimputer_1 = imputer_1.fit(X.loc[:,['Age']])\nX.loc[:,['Age']] = imputer_1.transform(X.loc[:,['Age']])\n\nimputer_1_test = Imputer(missing_values = 'NaN',strategy = 'mean', axis=0)\nimputer_1_test = imputer_1_test.fit(titanic_test.loc[:,['Age']])\ntitanic_test.loc[:,['Age']] = imputer_1_test.transform(titanic_test.loc[:,['Age']])\n\n#we have one missing fare value\nimputer_1_test = Imputer(missing_values = 'NaN',strategy = 'mean', axis=0)\nimputer_1_test = imputer_1_test.fit(titanic_test.loc[:,['Fare']])\ntitanic_test.loc[:,['Fare']] = imputer_1_test.transform(titanic_test.loc[:,['Fare']])\n\nfreq_port = X.Embarked.dropna().mode()[0]\nX['Embarked']=X['Embarked'].fillna(freq_port)\n\nfreq_port = titanic_test.Embarked.dropna().mode()[0]\ntitanic_test['Embarked']=titanic_test['Embarked'].fillna(freq_port)","05a563e8":"total = X.isnull().sum().sort_values(ascending=False)\npercent = (X.isnull().sum()\/X.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","dce07fa9":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder_1 = LabelEncoder()\nX.loc[:,['Embarked']] = label_encoder_1.fit_transform(X.loc[:,['Embarked']])\ntitanic_test.loc[:,['Embarked']] = label_encoder_1.fit_transform(titanic_test.loc[:,['Embarked']])\n\nlabel_encoder_2 = LabelEncoder()\nX.loc[:,['Sex']] = label_encoder_2.fit_transform(X.loc[:,['Sex']])\ntitanic_test.loc[:,['Sex']] = label_encoder_2.fit_transform(titanic_test.loc[:,['Sex']])","83025f2e":"X['Title'] =X.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntitanic_test['Title'] = titanic_test.Name.str.extract(' ([A-Za-z]+)\\.',expand=False)\npd.crosstab(X['Title'], X['Sex'])","624ffa91":"#continuing with the copy paste :<\nX['Title'] = X['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n     'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\nX['Title'] = X['Title'].replace('Mlle', 'Miss')\nX['Title'] = X['Title'].replace('Ms', 'Miss')\nX['Title'] = X['Title'].replace('Mme', 'Mrs')\n\ntitanic_test['Title'] = titanic_test['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n     'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ntitanic_test['Title'] =titanic_test['Title'].replace('Mlle', 'Miss')\ntitanic_test['Title'] = titanic_test['Title'].replace('Ms', 'Miss')\ntitanic_test['Title'] = titanic_test['Title'].replace('Mme', 'Mrs')","a0df5d5e":"X.drop(['Name'],inplace=True,axis=1)\ntitanic_test.drop(['Name'],inplace=True,axis=1)","da02960c":"label_encoder_3 = LabelEncoder()\nX.loc[:,['Title']] = label_encoder_3.fit_transform(X.loc[:,['Title']])\ntitanic_test.loc[:,['Title']] = label_encoder_3.fit_transform(titanic_test.loc[:,['Title']])","a0850a21":"from sklearn.preprocessing import StandardScaler\nsc_1 = StandardScaler()\nX.loc[:,['Age']] = sc_1.fit_transform(X.loc[:,['Age']])\ntitanic_test.loc[:,['Age']] = sc_1.fit_transform(titanic_test.loc[:,['Age']])\n\nsc_2 = StandardScaler()\nX.loc[:,['Fare']] = sc_2.fit_transform(X.loc[:,['Fare']])\ntitanic_test.loc[:,['Fare']] = sc_2.fit_transform(titanic_test.loc[:,['Fare']])","f29bb935":"from sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder(categorical_features=[0,3,4,6,7])\nX = onehotencoder.fit_transform(X).toarray()\ntitanic_test = onehotencoder.fit_transform(titanic_test).toarray()\ntitanic_test","9ca63673":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)","4390bcc0":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=500,criterion='entropy',random_state=0)\nclassifier.fit(X_train,y_train)","8fbfcc80":"from sklearn.metrics import confusion_matrix\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)","621b8b61":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=classifier,X = X_train, y =y_train,cv = 10)\nprint(accuracies.mean())\nprint(accuracies.std())","28b20f6a":"#calculating the accuracy,recall,precision\naccuracy = (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\nprint(accuracy)\nprecision = cm[0][0]\/(cm[0][0]+cm[0][1])\nprint(precision)\nrecall = cm[0][0]\/(cm[0][0]+cm[1][0])\nprint(recall)\n#I think these are very good results","118a34aa":"from sklearn.svm import SVC\nclassifier = SVC()\nclassifier.fit(X_train,y_train)","61712685":"y_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)","f0a7406d":"accuracy = (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\nprint(accuracy)\nprecision = cm[0][0]\/(cm[0][0]+cm[0][1])\nprint(precision)\nrecall = cm[0][0]\/(cm[0][0]+cm[1][0])\nprint(recall)","282d1619":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(X_train,y_train)","99bf83d4":"y_pred = classifier.predict(X_test)\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)","7469e165":"accuracies = cross_val_score(estimator=classifier,X = X_train, y =y_train,cv = 10)\nprint(accuracies.mean())\nprint(accuracies.std())","64b2ccf7":"accuracy = (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\nprint(accuracy)\nprecision = cm[0][0]\/(cm[0][0]+cm[0][1])\nprint(precision)\nrecall = cm[0][0]\/(cm[0][0]+cm[1][0])\nprint(recall)","bff2f828":"from xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)","4dfe515d":"y_pred = classifier.predict(X_test)\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)","643a1ba8":"accuracies = cross_val_score(estimator=classifier,X = X_train, y =y_train,cv = 10)\nprint(accuracies.mean())\nprint(accuracies.std())","38fe17ef":"accuracy = (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\nprint(accuracy)\nprecision = cm[0][0]\/(cm[0][0]+cm[0][1])\nprint(precision)\nrecall = cm[0][0]\/(cm[0][0]+cm[1][0])\nprint(recall)","ad1da254":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nclassifier = Sequential()\nclassifier.add(Dense(output_dim = 6,init='uniform',activation='relu',input_dim=28))\nclassifier.add(Dense(output_dim = 6,init='uniform',activation='relu'))\nclassifier.add(Dense(output_dim=1,init='uniform',activation='sigmoid'))\nclassifier.compile(optimizer ='adam',loss='binary_crossentropy',metrics=['accuracy'])\nclassifier.fit(X_train,y_train,batch_size=10,nb_epoch=50)","7146a6dd":"y_pred = classifier.predict(X_test)\ny_pred = (y_pred>0.5)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)","67e0b9a1":"accuracy = (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\nprint(accuracy)\nprecision = cm[0][0]\/(cm[0][0]+cm[0][1])\nprint(precision)\nrecall = cm[0][0]\/(cm[0][0]+cm[1][0])\nprint(recall)","b5481ce6":"preds_test = classifier.predict(titanic_test)\npreds_test = (preds_test>0.5)\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\nids = []\npreds = []\nfor i in range(0,len(ID_test)):\n    ids.append(ID_test[i])\nfor i in range(0,len(preds_test)):\n    if preds_test[i] :\n        preds.append(1)\n    else :\n        preds.append(0)\nprint(preds)\noutput = pd.DataFrame({'PassengerId': ids,\n                       'Survived': preds})\noutput.to_csv('submission.csv', index=False)","7457572f":"Encoding all the categorical data.","b1d984d2":"Encoding the Name value into a new column named Title.(This was not written by me)","2c8e9905":"->XGBoost\n\nOur favorite one.","bc2a35ec":"Before encoding the data I will impute the missing values from the train and test set, but first I have to see what columns have missing values. We can see the Cabin, Age and Embarked have missing values.","86b9f6d6":"Now we gonna impute the missing columns, for the Age column I am gonna use the mean value and for the Embarked one I am gonna see what is the most frequent value in the column. Also after some investigation it seems that the test data also had 2 missing values on the Fare column so we gonna impute them with the mean too.","757776a9":"The next thing that I am going to do is to drop the PassengerId and Ticket columns from the test and train dataset. I must be carefull not to remove the PassengerId column in the test dataset before I save them for my final submission.","1d906655":"This is the final of my kernel, I hope you find something usefull and for the 'big guys' here to see how a beginner approach this problem after 1 month of training. To be honest I am proud that I made it happened and it actually worked. After all I couldn't respect all the workflow I mentioned earlier because I don't know what to do in those steps.\n\nSome thanks to this kernel: https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions (it helped me with the names part because at this moment I can't handle these kind of operations)\n\nPlease if you some helpful tips and tricks don't mind to share.\n\nAlso I am sorry for the mistakes I made writing this in English.","efd575b6":"Here I found something that bugged me for let's say 20-30 minutes because when I wanted to make the prediction for my test data set I found out that I trained a model on 28 features and my test data had 29. After I inspected my code for 20 minutes I found out that 'Parch' in train data had 7 values and in the test had 8, so when I OneHotEncoded them this put one more feature and I couldn't make a prediction. The value that was in addition in comparation with the train set was 2 values of 9 so I raplced them with the higher next value which is 6.","ac225593":"Verifying that there is no missing data left.","cb4bd76e":"And I saved what is best for the final.\n\n->NeuralNetwork","83329c86":"As I said before the Cabin, in my opinion is not that important so I'm gonna remove it.","0964f84f":"After all the preparations we are left with the Age and Fare column that have some weird values if we compare them with the rest of the dataset so I thought that maybe a scalling here will be better.","71766bd3":"We can see that in every class your rate of surviving decrease if you are am adult(+20-50) as well as you are in a lower class.\n\nNow I will prepare the data for the ML part, I'm gonna drop all the missing lines that have no Survived values on this column.","c1cc1ddb":"7. Supply or submit the results.","9b206772":"Also I wanted to calculate the Accuracy,Precision and the Recall.","b92a13dd":"Splitting the data in X and y.","c4eb4104":"Now I have to analyze some columns to see some hidden correlation and make some visualization, this part is the most unknown for me and hard to do, so most of it will be some inspiration from other kerners.\n\nI will try to see the survival rate by Pclass, SibSp, Parch and Sex.","faa74faa":"5. Model, predict and solve the problem.\n\nAnd now the fun part, splitting the data and creating the models.","1e4c16c4":"Now I'm new to this kernel stuff, usually I read the data that is in the same folder as my python\/jupyter notebook file and I have to import them as follows: pd.read_csv('file.csv').\nBut I think here in this case I have to find the kaggle's folder and import them from there.","b607d830":"   First of all I will try to follow some workflow I found because I want to work in an organized way and maybe in this way I wont miss some things out.\n   \n   The workflow I found:\n       \n1.     Question or problem definition.\n2.     Acquire training and testing data.\n3.     Wrangle, prepare, cleanse the data.\n4.     Analyze, identify patterns, and explore the data.\n5.     Model, predict and solve the problem.\n6.     Visualize, report, and present the problem solving steps and final solution.\n7.     Supply or submit the results.","8ebac0e1":"   Hello guys,\n    \n   In this kernel I try to approach the Titanic: Machine Learning from Disaster competition, after let's say one month of training on DataScience\/ML. I discovered this site about 3-4 months ago, but never committed to this field until 1 month ago. After I did some researching in these areas and reading some kernels I try to put here all the information I got and I try to explain all I do in this case maybe some kagglers with experience will correct me and give some better information\/explanation. Maybe this will help new kagglers to try new things and actually have the courage to compete after seeing this.","7fd495d0":"Now I will try to analyze the columns to see what we have to do, I think in this case it is ok to do it because we have a small number of columns but maybe just in this case.\n\nPassengerId -> I think this doesn't give special information so I will drop it\n\nSurvived -> this will be our y variable\n\nPclass -> I think this is one of the most importance feature we have\n\nName -> As well as the PassengerId I thought that it is kind of useless, but after I read some kernels it has some special rank in it\nSex -> is also important because females have a higher chance of surviving\n\nAge -> along with females children and elders have a higher chance of surviving\n\nSibSp & Parch -> I don't know about this or what should I extract out of it, maybe people who have family related persons on the boat, usually children or females can escape with them\n\nTicket -> I think this is just a random number given to you when you went on this boat, what I can see at a first glance is that is somehow corellated with your class, if your ticket stars with 1 you are class 1 and so on\n\nFare -> this is high correlated with your class\n\nCabin -> I think this is where you are positionated on the boat so it should be important, but I see a lot of missing values on this\n\nEmbarked -> I don't see how your port of embarkation will escape you from this fate, the single thing I'm thinking of is that maybe some ports have more rich people in that area and we can correlate this information, but we don't see this in our database","28932b3c":"And making a kFoldCrossValidation to calculate the accuracies and don't get fooled.","409d3e6a":" 1.Question or problem definition.\n \n  In this step I think I should write down what is the main idea of what I am doing or what is the scope of this competion, all the information are written in Overview area of the competion, but I will give my opinion about this. In my opinion this competion is all about preparing youself for the next competition because it doesn't have some real value, maybe in nowdays we want to prepare a ship to not wreck into something not to see who survives if it wrecks, just my opinion. About the competition itself is about the notorious Titanic and the tragic event that happened. We some a vital information which states that 1502 out of 2224 died, so that's 0.675% without even trying to predict something. Another information that I think is a hint gave by kaggle is \"some groups of people were more likely to survive than others\", so this means we have to discover something to gave a prediction of them.\n  \n  Here I copy-paste what's written on competition itself:\n  \n  Competition Description\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.","7ae757d1":"3.Wrangle, prepare, cleanse the data.\n\nSo after I read a lot of articles on data science all the time, like 90% it says that the most important thing is to prepare the data, so that's what I'm going to do or atleast try.","718b21ca":"->KNN model","0ee2cde9":"I will use the confusion_matrix for all the models here to see how are the results.","ac34faf6":"->RandomForest\n\nFirst model that came in my head is an ensemble model, the RandomForest one, and I wanted to see how it is performing.","4477a31e":"->SupportVectorMachine model","28054e73":"And the last step is to OneHotEncode the rest of the data.","1a5fb392":"2.Acquire training and testing data.\n\nI think this step is clear by everyone starting out, we have to gather data, an input to have some results and you to this before anything else, I don't see how we can do this after some other operations.\n\nI will import some libraries that I usually do, I don't know if this is a correct way to do it but I will import them all."}}