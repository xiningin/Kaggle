{"cell_type":{"191d05f0":"code","62d4c747":"code","1f461b27":"code","448f5180":"code","1e8a159d":"code","345053cd":"code","e050404f":"code","1b71ed66":"code","06ac9011":"code","b7e56a4c":"code","a5412781":"code","597121d2":"code","c8c7d203":"code","edcdf69b":"code","90e39df0":"code","2a29746b":"code","8b3614db":"code","0c4bc679":"code","958643ed":"code","f598c309":"code","537d1a35":"code","8aa1bfb3":"code","038bfabd":"code","16a5ea37":"code","95208b7b":"code","6a5fcc91":"code","1d65e6f1":"code","e1f9b9c7":"code","b9d99a2a":"code","217abc2f":"code","a01f3ac1":"code","8540f174":"code","451cfd5f":"code","f5a72f44":"code","ae819456":"code","637f7501":"code","26b810c8":"code","8926428c":"code","a9ba3479":"code","b1568aa7":"code","836d9bf8":"code","87592bd0":"code","d61ddd13":"code","1c24ccc7":"code","356c7002":"code","2e0e765e":"code","6faf7861":"code","13f39a92":"code","c370346f":"code","ef1145d9":"code","08bdee2c":"code","f239d97e":"code","6dea78dc":"code","f60ec30c":"code","883e3bde":"code","3ec20b3a":"code","c1d60067":"code","e9cd5995":"code","42fa6a80":"code","622e6f44":"code","6b2fb50c":"code","1c217a59":"code","554add7f":"code","ab1605d6":"code","da0d5fcd":"code","4f4d4644":"code","1c926a52":"code","57fc1cd0":"code","1043e326":"code","2e4ce0a6":"code","a4141bdf":"code","188cbd67":"code","472ded7a":"code","c1e26166":"code","fad977d2":"code","11159322":"code","703ac61f":"code","179b2ec6":"code","fa81b483":"code","32164bfd":"code","9320385e":"code","cd8819cd":"code","d94f6a2b":"code","3a051b97":"code","06df4f5f":"code","5ad9f9a8":"code","e7773c6a":"code","9adb398d":"code","2198584d":"code","2384dfb2":"code","17cde38d":"code","4295f142":"code","141e7af9":"code","2117d8d4":"code","c324eb4d":"code","0810b546":"code","9c8d0c10":"code","fcb3e1dd":"markdown","5dd7f281":"markdown","c3693559":"markdown","d19815dc":"markdown","47484a19":"markdown","1c07cf37":"markdown","630e149d":"markdown","02183ab5":"markdown","85006205":"markdown","60a261a4":"markdown","8ee9bdab":"markdown","4d60ed03":"markdown","41b2b1ae":"markdown","914c127c":"markdown","036766aa":"markdown","f7ed2790":"markdown","415e115a":"markdown","8146c03c":"markdown","a708c68a":"markdown","e9c8f9d2":"markdown","7f4feb01":"markdown","3c5c011c":"markdown","05990db9":"markdown","a658c51d":"markdown","dc225557":"markdown","2a389ffa":"markdown","87a83a2c":"markdown","a86b67b8":"markdown","1c068d4a":"markdown","171ada58":"markdown","071f8b9b":"markdown","109ee9bc":"markdown","6ee9b635":"markdown","921ac35c":"markdown","efda0b18":"markdown","c9b49c1e":"markdown","fb2c7ed8":"markdown","f828111b":"markdown","387fbc7a":"markdown","4a057400":"markdown","7721c85e":"markdown","ae2cddae":"markdown","a60d05f7":"markdown","415d1a4c":"markdown","e621b3a7":"markdown","32e35ab4":"markdown","e73f0d79":"markdown","09dabf7e":"markdown"},"source":{"191d05f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","62d4c747":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom keras.layers import Input, Dense, Flatten, Average\nfrom keras.layers.core import Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model, load_model, Sequential\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input as prep_inputVgg16\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.resnet50 import preprocess_input as prep_inputResNet50\nfrom keras.optimizers import Adam, RMSprop\n\nsns.set()","1f461b27":"np.set_printoptions(suppress=True)\nnp.set_printoptions(precision=2)\nnp.set_printoptions(edgeitems=10)","448f5180":"TRAIN_PATH = '..\/input\/volcanoesvenus\/volcanoes_train\/'\nTEST_PATH = '..\/input\/volcanoesvenus\/volcanoes_test\/'","1e8a159d":"IMAGE_HEIGHT_TARGET = 110\nIMAGE_WIDTH_TARGET = 110","345053cd":"# Load train data\ntrain_images = pd.read_csv(TRAIN_PATH + 'train_images.csv', header=None)\ntrain_labels = pd.read_csv(TRAIN_PATH + 'train_labels.csv', header=None)","e050404f":"# Load test data\ntest_images = pd.read_csv(TEST_PATH + 'test_images.csv', header=None)\ntest_labels = pd.read_csv(TEST_PATH + 'test_labels.csv', header=None)","1b71ed66":"train_images.head()","06ac9011":"train_labels.head()","b7e56a4c":"train_labels = train_labels.drop([0])","a5412781":"nulls = []\nfor i in range(len(train_labels.index)):\n    if (train_labels.iloc[i][0] == 1):\n        nulls.append(train_labels.isnull().iloc[i][0])","597121d2":"# count nan values (true) in list\ncount_nans = sum(nulls)\ncount_nans","c8c7d203":"# Do the same for test labels\nnulls = []\nfor i in range(len(test_labels.index)):\n    if (test_labels.iloc[i][0] == 1):\n        nulls.append(test_labels.isnull().iloc[i][0])","edcdf69b":"count_nans = sum(nulls)\ncount_nans","90e39df0":"# Do the same for train and test images.\ntrain_images.isnull().values.any()","2a29746b":"test_images.isnull().values.any()","8b3614db":"ax = sns.countplot(data = train_labels,x=train_labels[0][1:])\nax.set(xlabel='Volcanoes', ylabel='Count')","0c4bc679":"ax = sns.countplot(data = train_labels,x=train_labels[1][1:])\nax.set(xlabel='Type', ylabel='Count')","958643ed":"ax = sns.countplot(data = train_labels,x=train_labels[3][1:])\nax.set(xlabel='Number of volcanoes', ylabel='Count')","f598c309":"indices_train = np.where(train_labels.iloc[:, 0].astype(np.float) == 1)","537d1a35":"train_labels.iloc[indices_train].shape","8aa1bfb3":"train_images.iloc[indices_train].shape","038bfabd":"train_labels.head()","16a5ea37":"train_labels = train_labels.fillna('nan')","95208b7b":"train_labels.iloc[:, 0] = (train_labels.iloc[:, 0]).str.replace('0', 'No')\ntrain_labels.iloc[:, 0] = (train_labels.iloc[:, 0]).str.replace('1', 'Yes')","6a5fcc91":"train_labels.iloc[:, 1] = (train_labels.iloc[:, 1]).str.replace('1', 'Type 1')\ntrain_labels.iloc[:, 1] = (train_labels.iloc[:, 1]).str.replace('2', 'Type 2')\ntrain_labels.iloc[:, 1] = (train_labels.iloc[:, 1]).str.replace('3', 'Type 3')\ntrain_labels.iloc[:, 1] = (train_labels.iloc[:, 1]).str.replace('4', 'Type 4')\ntrain_labels.iloc[:, 1] = (train_labels.iloc[:, 1]).str.replace('nan', 'Type nan')","1d65e6f1":"train_labels.iloc[:, 3] = (train_labels.iloc[:, 3]).str.replace('nan', 'Nb volcanoes nan')","e1f9b9c7":"train_labels[:10]","b9d99a2a":"labels = []\nfor idx in range(len(train_labels)):\n    # index 0: Volcanoe or not\n    # index 1: Type\n    # index 3: Nb of volcanoes\n    labels.append([train_labels.iloc[:, 0].values.item(idx), train_labels.iloc[:, 1].values.item(idx), train_labels.iloc[:, 3].values.item(idx)]) \n    \nlabels = np.array(labels)","217abc2f":"#Show a few labels\nlabels[:4]","a01f3ac1":"# Binarize the labels\nmlb = MultiLabelBinarizer()\nlabels = mlb.fit_transform(labels)","8540f174":"mlb.classes_","451cfd5f":"# Check the binarized labels\nlabels[:4]","f5a72f44":"def data():\n    X_train, X_val, y_train, y_val  = train_test_split(train_images.values,\n                                                       labels,\n                                                       test_size=0.2,\n                                                       stratify=labels,\n                                                       random_state=1340)\n        \n    return X_train, X_val, y_train, y_val","ae819456":"X_train, X_val, y_train, y_val = data()","637f7501":"X_train_res = X_train.reshape((-1, IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 1))\nX_val_res = X_val.reshape((-1, IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 1))","26b810c8":"# Stack, in order to have 3 channels\nX_train_vggnet = np.stack((np.squeeze(X_train_res),) * 3, -1)\nX_val_vggnet = np.stack((np.squeeze(X_val_res),) * 3, -1)","8926428c":"# Preprocess input\nX_train_vggnet = prep_inputVgg16(X_train_vggnet)\nX_val_vggnet = prep_inputVgg16(X_val_vggnet)","a9ba3479":"train_data_gen = ImageDataGenerator(horizontal_flip=True,\n                                    rotation_range=40,\n                                    width_shift_range=0.1,\n                                    height_shift_range=0.1,\n                                    zoom_range=0.2)","b1568aa7":"class VGGNet:\n    @staticmethod\n    def build(width, height, depth, classes, final_activ):\n        # Initialize the model to use channels last\n        input_shape = (height, width, depth)\n        \n        # In case where channels first is used\n        if K.image_data_format() == \"channels_first\":\n            input_shape = (depth, height, width)\n            \n        # Load pretrained weights\n        imagenet_weights = '..\/input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        base_vgg16 = VGG16(include_top=False, weights=imagenet_weights, input_shape=input_shape)\n        last_layer = base_vgg16.output\n        \n        x = Flatten()(last_layer)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.4)(x)\n        preds_base_vgg16 = Dense(classes, activation=final_activ)(x)\n        \n        # Before compiling and train the model it is very important to freeze the convolutional base (resnet base).That means, preventing the weights from being updated during training.\n        # If you omit this step, then the representations that were learned previously by the convolutional base will be modified during training.\n        base_vgg16.trainable = False\n        \n        model_vgg16 = Model(base_vgg16.input, preds_base_vgg16)\n               \n        return model_vgg16\n    \n    def train(model, X, y, batch_size, epochs, class_weights, k_fold, loss, optimizer, metrics, model_checkpoint, early_stopping):\n            \n        # use k-fold cross validation test\n        histories = []\n        nb_validation_samples = len(X) \/\/ k_fold\n        for fold in range(k_fold):\n            x_training_data = np.concatenate([X[:nb_validation_samples * fold], X[nb_validation_samples * (fold + 1):]])\n            y_training_data = np.concatenate([y[:nb_validation_samples * fold], y[nb_validation_samples * (fold + 1):]])\n\n            x_validation_data = X[nb_validation_samples * fold:nb_validation_samples * (fold + 1)]\n            y_validation_data = y[nb_validation_samples * fold:nb_validation_samples * (fold + 1)]\n\n            model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\n            history = model.fit_generator(train_data_gen.flow(x_training_data, y_training_data, batch_size=batch_size),\n                                                              validation_data=[x_validation_data, y_validation_data],\n                                                              epochs = epochs,\n                                                              shuffle=True,\n                                                              verbose=2,\n                                                              class_weight=class_weights,\n                                                              steps_per_epoch = int(len(X_train) \/ batch_size),\n                                                              validation_steps =int(len(X_val) \/ batch_size),\n                                                              callbacks=[model_checkpoint, early_stopping])\n            histories.append(history)\n        \n        return histories, model","836d9bf8":"model_vgg16 = VGGNet.build(IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 3, len(mlb.classes_), 'sigmoid')","87592bd0":"final_activation = 'sigmoid'\nbatch_size = 32\nepochs = 100\nk_fold = 3\nloss = 'binary_crossentropy'\nadam = Adam(lr=0.0001)\noptimizer = adam\nmetrics = ['accuracy']\nearly_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_vgg16_checkpoint = ModelCheckpoint('.\/b_32_relu_optim_adam.hdf5', verbose=1, save_best_only=True)","d61ddd13":"class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train[:, -1]), y_train[:, -1]) #-1 is the lats index (volcanoe or not)","1c24ccc7":"# Concatenate train and test data\nX_data = np.concatenate((X_train_vggnet, X_val_vggnet))\ny_data = np.concatenate((y_train, y_val))","356c7002":"history_vgg16, model_vgg16 = VGGNet.train(model_vgg16,\n                                          X_data,\n                                          y_data,\n                                          batch_size,\n                                          epochs,\n                                          class_weights,\n                                          k_fold,\n                                          loss,\n                                          optimizer,\n                                          metrics,\n                                          model_vgg16_checkpoint,\n                                          early_stopping)","2e0e765e":"fig, axes = plt.subplots(k_fold, 2, figsize=(20, 12))\n\nfor i in range(k_fold):\n    \n    axes[i, 0].plot(history_vgg16[i].epoch, history_vgg16[i].history['loss'], label='Train loss')\n    axes[i, 0].plot(history_vgg16[i].epoch, history_vgg16[i].history['val_loss'], label='Val loss')\n    axes[i, 0].legend()\n\n    axes[i, 1].plot(history_vgg16[i].epoch, history_vgg16[i].history['acc'], label = 'Train acc')\n    axes[i, 1].plot(history_vgg16[i].epoch, history_vgg16[i].history['val_acc'], label = 'Val acc')\n    axes[i, 1].legend()\n\n \nplt.tight_layout()","6faf7861":"class ResNet:\n    @staticmethod\n    def build(width, height, depth, classes, final_activ):\n        # Initialize the model to use channels last\n        input_shape = (height, width, depth)\n        \n        # In case where channels first is used\n        if K.image_data_format() == \"channels_first\":\n            input_shape = (depth, height, width)\n            \n        # Load pretrained weights\n        imagenet_weights = '..\/input\/keras-pretrained-models\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n        base_resnet = ResNet50(include_top=False, weights=imagenet_weights, input_shape=input_shape)\n        last_layer = base_resnet.output\n        \n        x = Flatten()(last_layer)\n        x = Dense(512, activation='relu')(x)\n        x = Dropout(0.4)(x)\n        preds_base_resnet = Dense(classes, activation=final_activ)(x)\n        \n        # Before compiling and train the model it is very important to freeze the convolutional base (resnet base).That means, preventing the weights from being updated during training.\n        # If you omit this step, then the representations that were learned previously by the convolutional base will be modified during training.\n        base_resnet.trainable = False\n        \n        model_resnet = Model(base_resnet.input, preds_base_resnet)\n               \n        return model_resnet\n    \n    def train(model, X, y, batch_size, epochs, class_weights, k_fold, loss, optimizer, metrics, model_checkpoint, early_stopping):\n        \n        # use k-fold cross validation test\n        histories = []\n        nb_validation_samples = len(X) \/\/ k_fold\n        for fold in range(k_fold):\n            x_training_data = np.concatenate([X[:nb_validation_samples * fold], X[nb_validation_samples * (fold + 1):]])\n            y_training_data = np.concatenate([y[:nb_validation_samples * fold], y[nb_validation_samples * (fold + 1):]])\n            \n            x_validation_data = X[nb_validation_samples * fold:nb_validation_samples * (fold + 1)]\n            y_validation_data = y[nb_validation_samples * fold:nb_validation_samples * (fold + 1)]\n            \n            model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n            \n            history = model.fit_generator(train_data_gen.flow(x_training_data, y_training_data, batch_size=batch_size),\n                                                              validation_data=[x_validation_data, y_validation_data],\n                                                              epochs = epochs,\n                                                              shuffle=True,\n                                                              verbose=2,\n                                                              class_weight=class_weights,\n                                                              steps_per_epoch = int(len(X_train) \/ batch_size),\n                                                              validation_steps =int(len(X_val) \/ batch_size),\n                                                              callbacks=[model_checkpoint, early_stopping])\n            histories.append(history)\n            \n        return histories, model","13f39a92":"model_resnet = ResNet.build(IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 3, len(mlb.classes_), 'sigmoid')","c370346f":"X_train_resnet = np.stack((np.squeeze(X_train_res),) * 3, -1)\nX_val_resnet = np.stack((np.squeeze(X_val_res),) * 3, -1)","ef1145d9":"X_train_resnet = prep_inputResNet50(X_train_resnet)\nX_val_resnet = prep_inputResNet50(X_val_resnet)","08bdee2c":"# Concatenate train and test data\nX_data_resnet = np.concatenate((X_train_resnet, X_val_resnet))\ny_data_resnet = np.concatenate((y_train, y_val))","f239d97e":"model_checkpoint_resnet = ModelCheckpoint('.\/b_32_relu_optim_adam_resnet.hdf5', verbose=1, save_best_only=True)","6dea78dc":"history_resnet, model_resnet = ResNet.train(model_resnet,\n                                            X_data_resnet,\n                                            y_data_resnet,\n                                            batch_size,\n                                            epochs,\n                                            class_weights,\n                                            k_fold,\n                                            loss,\n                                            optimizer,\n                                            metrics,\n                                            model_checkpoint_resnet,\n                                            early_stopping)","f60ec30c":"fig, axes = plt.subplots(k_fold, 2, figsize=(20, 12))\n\nfor i in range(k_fold):\n    \n    axes[i, 0].plot(history_resnet[i].epoch, history_resnet[i].history['loss'], label='Train loss')\n    axes[i, 0].plot(history_resnet[i].epoch, history_resnet[i].history['val_loss'], label='Val loss')\n    axes[i, 0].legend()\n\n    axes[i, 1].plot(history_resnet[i].epoch, history_resnet[i].history['acc'], label = 'Train acc')\n    axes[i, 1].plot(history_resnet[i].epoch, history_resnet[i].history['val_acc'], label = 'Val acc')\n    axes[i, 1].legend()\n\n \nplt.tight_layout()","883e3bde":"# Load the best saved model\n#model_vgg16.load_weights(filepath='..\/input\/volcanoes-on-venus-ensemble-imbalanced\/b_32_relu_optim_adam.hdf5')\n#model_resnet.load_weights(filepath='..\/input\/volcanoes-on-venus-ensemble-imbalanced\/b_32_relu_optim_adam_resnet.hdf5')","3ec20b3a":"def ensemble(models):\n    input_image = Input(shape=(IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 3))\n    \n    vgg16_out = models[0](input_image)\n    resnet_out = models[1](input_image)\n\n    output = Average()([vgg16_out, resnet_out])\n    model = Model(input_image, output)\n    \n    return model","c1d60067":"# Combine all models\nmodels = [model_vgg16, model_resnet]\nensemble_model = ensemble(models)","e9cd5995":"test_images.head()","42fa6a80":"test_labels.head()","622e6f44":"test_labels = test_labels.drop([0])","6b2fb50c":"# Replace all float nans with string nans.\ntest_labels = test_labels.fillna('nan')\n# Replace 0 or 1 with No or Yes\ntest_labels.iloc[:, 0] = (test_labels.iloc[:, 0]).str.replace('0', 'No')\ntest_labels.iloc[:, 0] = (test_labels.iloc[:, 0]).str.replace('1', 'Yes')","1c217a59":"# Replace 1,2,3,4 with Type1,2,3,4\ntest_labels.iloc[:, 1] = (test_labels.iloc[:, 1]).str.replace('1', 'Type 1')\ntest_labels.iloc[:, 1] = (test_labels.iloc[:, 1]).str.replace('2', 'Type 2')\ntest_labels.iloc[:, 1] = (test_labels.iloc[:, 1]).str.replace('3', 'Type 3')\ntest_labels.iloc[:, 1] = (test_labels.iloc[:, 1]).str.replace('4', 'Type 4')\ntest_labels.iloc[:, 1] = (test_labels.iloc[:, 1]).str.replace('nan', 'Type nan')","554add7f":"test_labels.iloc[:, 3] = (test_labels.iloc[:, 3]).str.replace('nan', 'Nb volcanoes nan')","ab1605d6":"tmp = train_labels.iloc[:, 3].values\nidx, = np.where(tmp != 'Nb volcanoes nan')\nidx_greater = idx[tmp[idx].astype(int) > 3]","da0d5fcd":"idx_greater","4f4d4644":"series_list_images = [pd.Series(train_images.iloc[425, :], index=test_images.columns ) ,\n                      pd.Series(train_images.iloc[1513, :], index=test_images.columns )]\n\nseries_list_labels = [pd.Series(train_labels.iloc[425, :], index=test_labels.columns ) ,\n                      pd.Series(train_labels.iloc[1513, :], index=test_labels.columns )]\n\ntest_images_full = test_images.append(series_list_images , ignore_index=True)\ntest_labels_full = test_labels.append(series_list_labels , ignore_index=True)","1c926a52":"labels_test = []\nfor idx in range(len(test_labels_full)):\n    # index 0: Volcanoe or not\n    # index 1: Type\n    # index 3: Nb of volcanoes\n    labels_test.append([test_labels_full.iloc[:, 0].values.item(idx), test_labels_full.iloc[:, 1].values.item(idx), test_labels_full.iloc[:, 3].values.item(idx)]) \n    \nlabels_test = np.array(labels_test)","57fc1cd0":"# Binarize the labels\nlabels_test = mlb.fit_transform(labels_test)","1043e326":"# Check classes\nmlb.classes_","2e4ce0a6":"X_test = test_images_full\ny_test = labels_test","a4141bdf":"# Reshape test data and create 3 channels\nX_test = X_test.values.reshape((-1, IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 1))\nX_test = np.stack((np.squeeze(X_test),) * 3, -1)","188cbd67":"# Preprocess data\nX_test = prep_inputVgg16(X_test)","472ded7a":"# predict on validation and test data\npred_val = ensemble_model.predict(X_val_vggnet) \npred_test = ensemble_model.predict(X_test, batch_size=batch_size)","c1e26166":"# Squeeze one dimension to be able to plot\nX_train_squeeze = X_train_vggnet.squeeze()\ny_train_squeeze = y_train.squeeze()\npred_val_squeeze = pred_val.squeeze()\nX_val_squeeze = X_val_vggnet.squeeze()\ny_val_squeeze = y_val.squeeze()\nX_test_squeeze = X_test.squeeze()","fad977d2":"def scale_image(input_data, min_orig, max_orig, min_target, max_target):\n    orig_range = max_orig - min_orig\n    target_range = max_target - min_target\n    scaled_data = np.array((input_data - min_orig) \/ float(orig_range))\n    return min_target + (scaled_data * target_range)","11159322":"X_test_denorm = scale_image(X_test_squeeze, X_test_squeeze.min(), X_test_squeeze.max(), 0, 1)","703ac61f":"plt.rc('text', usetex=False)\nmax_images = 6\n\nfig, axes = plt.subplots(max_images\/\/2, 2, figsize=(22, 18))\naxes = axes.ravel()\n\nidxlist = [0, 1, 2, 3, 4, 5]\nfor i in  range(max_images):   \n\n    #idx = np.random.randint(0, len(X_test)-1)\n    idx = idxlist[i]\n    \n    axes[i].grid(False)\n    axes[i].imshow(X_test_denorm[idx], cmap='Greens')\n    axes[i].set_title(mlb.inverse_transform(y_test[idx:idx+1, :]))\n    \n    label_img = []\n    for (label, p) in zip(mlb.classes_, pred_test[idx]):\n        #label_img.append(\"{0}: {1}%\".format(label.astype(np.str), (p * 100).astype(np.float32) ))\n        label_img.append((p * 100).astype(np.float32))\n        \n    text = 'Volcanoe\\nYes: {0:.2f}% No: {1:.2f}%\\nType\\n1: {2:.2f}% 2: {3:.2f}% 3: {4:.2f}% 4: {5:.2f}% nan: {6:.2f}%\\n\\\n            Nb.Volcanoes\\n1: {7:.2f}% 2: {8:.2f}% 3: {9:.2f}% 4: {10:.2f}% 5: {11:.2f}% nan: {12:.2f}%'\\\n            .format(label_img[12], label_img[6], label_img[7], label_img[8], label_img[9], label_img[10], label_img[11], label_img[0], label_img[1],\\\n                   label_img[2], label_img[3], label_img[4], label_img[5])\n    \n    axes[i].text(55, 95, text , size=12, ha=\"center\", va=\"center\",\n            bbox=dict( fc=(1., 1., 0.8), alpha=0.7))\n    \n    plt.subplots_adjust(hspace=0.5, wspace=0.5)\n    plt.tight_layout()","179b2ec6":"class Regression:\n    @staticmethod\n    def build(X):\n        model = Sequential()\n        model.add(Dense(1024, activation='relu', input_shape=(X.shape[1],)))\n        model.add(Dropout(0.3))\n        model.add(Dense(256, activation='relu'))\n        model.add(Dropout(0.3))\n        model.add(Dense(128, activation='relu'))\n        model.add(Dense(1))\n       \n        return model\n    \n    def train(X, y, k, optimizer, batch_size, epochs):\n                    \n        # use k-fold cross validation test\n        kfold = KFold(n_splits=k, shuffle=True, random_state=1340)\n        histories = []\n        for train, test in kfold.split(X):\n            model = Regression.build(X)\n            model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n            \n            history = model.fit(X[train],\n                                y[train],\n                                validation_data=[X[test], y[test]],\n                                batch_size=batch_size,\n                                epochs = epochs,\n                                verbose=0)\n            \n            # evaluate the model\n            #val_mse, val_mae = model.evaluate(X[test], y[test], verbose=0)\n            mae_history = history.history['val_mean_absolute_error']\n            histories.append(mae_history)\n                    \n        return histories, model","fa81b483":"indices_radius, = np.where(train_labels.iloc[:, 0] == 'Yes')\nX_volcanoes_radius = train_images.iloc[indices_radius]","32164bfd":"# Volcanoes labels\ny_volcanoes_radius = train_labels.iloc[indices_radius]\n# take only the radius\ny_volcanoes_radius = y_volcanoes_radius.iloc[:, 2]","9320385e":"def data_radius():\n    X_train, X_val, y_train, y_val  = train_test_split(X_volcanoes_radius,\n                                                       y_volcanoes_radius.values.astype(np.float32).reshape(-1, 1),\n                                                       test_size=0.2,\n                                                       random_state=1340)\n        \n    return X_train, X_val, y_train, y_val","cd8819cd":"X_train_radius, X_val_radius, y_train_radius, y_val_radius = data_radius()","d94f6a2b":"# Concatenate train and test data\nX_data_radius = np.concatenate((X_train_radius, X_val_radius))\ny_data_radius = np.concatenate((y_train_radius, y_val_radius))","3a051b97":"# initialize scaler\nscaler = StandardScaler()\n# scale data\nscaler.fit(X_data_radius)\nX_std_radius = scaler.transform(X_data_radius)","06df4f5f":"batch_size_reg = 16\nepochs = 200\nk = 3\nrmsprop = RMSprop(lr=0.001)\n\nhistory_mae, model_reg = Regression.train(X_std_radius, y_data_radius, k, rmsprop, batch_size_reg, epochs)","5ad9f9a8":"y_data_check = y_data_radius.astype(np.float32)","e7773c6a":"# Find out max radius value\nnp.amax(y_data_check)","9adb398d":"# Find out min radius value\nnp.amin(y_data_check)","2198584d":"# Let's plot the mae\navg_mae = [np.mean([x[i] for x in history_mae]) for i in range(epochs)]\n\nplt.plot(range(1, len(avg_mae) + 1), avg_mae)\nplt.xlabel('Epochs')\nplt.ylabel('Val mae')\nplt.show()","2384dfb2":"indices_test,  = np.where(pred_test[:, 12] >= 0.5)\n# Volcanoes test images\nX_volcanoes_test = test_images_full.iloc[indices_test]\n# scale test data\nX_volcanoes_test_std = scaler.transform(X_volcanoes_test.values.astype(np.float32))","17cde38d":"# Volcanoes test labels\ny_volcanoes_test = test_labels_full.iloc[indices_test]\n# take only the radius\ny_volcanoes_test = y_volcanoes_test.iloc[:, 2]","4295f142":"y_volcanoes_test = pd.to_numeric(y_volcanoes_test, errors='coerce')\ny_volcanoes_test = y_volcanoes_test.fillna(0)","141e7af9":"test_mse, test_mae = model_reg.evaluate(X_volcanoes_test_std, y_volcanoes_test.values.astype(np.float32))","2117d8d4":"test_mse","c324eb4d":"test_mae","0810b546":"pred_regression = model_reg.predict(X_volcanoes_test_std)","9c8d0c10":"# Check a few values\nfor i in range(100):\n    idx = np.random.randint(0, len(y_volcanoes_test.values)-1)\n    print('Real: {0}\\t Pred: {1:.2f}'.format(y_volcanoes_test.values[idx], pred_regression.squeeze()[idx]))","fcb3e1dd":"**Split data into train and validation sets**","5dd7f281":"Number of volcanoes ","c3693559":"**We can see for example the first line.\nThe last index show us that there is a volcanoe , the 4th index from the end, that it is of type 3 and the first index , shows that we have only 1 volcanoe.**","d19815dc":"We can see that we have an unbalanced set of data.Non volcanoes are 6000 and volcanoes are 1000.We must take that into acocunt when we are going to design our model.","47484a19":"**Replace nan value with Nb volcanoes nan**","1c07cf37":"What kind of type","630e149d":" We can see that the first line is the header!\n We must remove it","02183ab5":"### Conclusion","85006205":"Denormalize our image in order to properly show it in plot","60a261a4":"So, we don't have any nan values when we have a volcanoe","8ee9bdab":"**Define the class weights that we are going to use with our model because it has imbalanced set of data**","4d60ed03":"Create a labels list which will hold all the available labels","41b2b1ae":"How many Volcanoes","914c127c":"Update the test images and labels with 2 new rows which contain 4 and 5 nb of volcanoes","036766aa":"We can see that when we have Volcanoe value 0 , we have all the rest columns as NaN (because there is no volcanoe)\nWe want to check if we have any nan values when there is a volcanoe (so index is 1)","f7ed2790":"**Before proceeding with mlb I noticed that in the X_test data the number of volcanoes go up to 3 , not 5 as \nin the training data.This results in giving 11 classes instead of 13 as in train mlb.\nSo, we are going to copy 2 rows which contain 4 and 5 nb of volcanoes from train data to test data.\nThe 3rd column which contains the number of volcanoes, it is of type string and contains \neither 'Nb volcanoes nan' either the nb of volcanoes in string format.\n So we must take into consideration only the nb of volcanoes**","415e115a":"**Split data into train and validation sets**","8146c03c":"**Extract only the cases where volcanoe exists**","a708c68a":"### Evaluate regression model on test data","e9c8f9d2":"**Take only the last index (volcanoe or not)**","7f4feb01":"Mean absolute error returns a value between 8.5-9 which isn't so good. I would expect a value of 1-2 to be good.","3c5c011c":"### Process test data","05990db9":"**We can see all the classes**\n\n**We see the order by which the labels are given.Note,that in order to find out if we have a volcanoe we must check the last index**","a658c51d":"**Volcanoes images\nFind the indices where we have a volcanoe,since only volcanoes have radius.**","dc225557":"** Replace 1, 2, 3, 4 with Type1, 2, 3, 4**","2a389ffa":"**Replace all float nans with string nans in order for not to have any problems with values nan in the classifier.**","87a83a2c":"**Let's work with the indices where we predicted that we have a volcanoe with possibilty >= 50%**","a86b67b8":"### Create an esemble model.\n\n#### Define a function where we take the average of our two best models.","1c068d4a":"**Create a labels list which will hold all the available labels**","171ada58":"**We are going to use the vggnet16**\n\n**Lets's create a class**","071f8b9b":"**Uncomment the `load_weights` lines in order to load the best saved weights. (for some reason kernel couldn't load them even though I have them in my output)**","109ee9bc":"So, we have 1000 volcanoes and the rest 6000 are no volcanoes","6ee9b635":"We don't have an nan values in test set either.","921ac35c":"Lets's check the radius values range in order to have a grasp for comparing to mean absolute error","efda0b18":"**Replace 0 or 1 with No or Yes**","c9b49c1e":"If you want to play with the plot you must change the `axes[i].text(55, 95,..`  the positions 55, 95 in order for the results to show up. Also, uncomment the idx and use `idx = np.random.randint(0, len(X_test)-1)` in order to get random samples.","fb2c7ed8":"### Now let's deal with regression in order to find out the radius of the volcanoe.","f828111b":"We are going to replace the strings columns data with more convenient","387fbc7a":"## Multi label classification","4a057400":"#### Check data","7721c85e":"**We can see that in test data we have nan values in cases where we don't have a volcanoe.**\n**We can replace nan values with zero value for radius when we don't have a volcanoe**","ae2cddae":"We can see that the first line is the header!\nWe must remove it","a60d05f7":"Apply multilabelbinarizer in test data as we did in train data","415d1a4c":"**We are going to perfrom multilabel categorical classification (each sample can have several classes).**\n\n**In order to do so , we are going to take into account only the categories where a volcanoe exists because in that case we have\ntype, radius and number of volcanoes.**\n\n**In the other cases all these values are nan.**","e621b3a7":"#### Create a function in order to be able to scale images from original to target normalization.","32e35ab4":"We are ok with our dataset , so we can continue our analysis","e73f0d79":"**Use data augmentation**","09dabf7e":"We saw that the vggnet and resnet did a pretty good job to classify if an image is volcanoe or not. We had an accuracy of around ! Predicting the type of volcanoe was not that accurate but in general I think it was good. The regression model didn't succeed to predict accurate the radius of the volcanoe. I have tried various combinations and various models but I couldn't make a good model for that. Maybe the information included in the images is not enough to predict the radius?Or maybe due to my inexperience I can't find a good solution."}}