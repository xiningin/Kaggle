{"cell_type":{"af7a3609":"code","98304fe6":"code","79120c7c":"code","db5a86fe":"code","95fd7ff7":"code","8b4614c5":"code","af949333":"code","867435d6":"code","0c8d16e8":"code","3179d58a":"code","763d0190":"code","30a57ab5":"code","4755f3a9":"code","bb461a5b":"code","b4b8fc04":"code","3204b8ef":"code","9c66268f":"code","129d96b0":"code","bb23ded1":"markdown","932015a3":"markdown","e5727ab3":"markdown","eddfc35f":"markdown","bef3b7e9":"markdown","17da9cbc":"markdown","68f59320":"markdown"},"source":{"af7a3609":"#In the previous mission, we tried to optimize our predictions by creating and selecting the features used to train\n#our model. Maybe changing ml model will help us out also.\n\n#So far, we've been using the logistic regression algorithm to train our models, however there are hundreds of\n#different machine learning algorithms from which we can choose. Each algorithm has different strengths and weaknesses,\n#and so we need to select the algorithm that works best with our specific data\u2014 in this case our Kaggle competition.\n\n#The process of selecting the algorithm which gives the best predictions for your data is called \"MODEL SELECTION\"\n#we're going work with two new algorithms: k-nearest neighbors and random forests.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","98304fe6":"#code snippet:\nimport pandas as pd\ntrain = pd.read_csv('train_modified.csv')\nholdout = pd.read_csv('holdout_modified.csv')","79120c7c":"#We're going to train our models using all the columns in the train dataframe. This will cause a small amount of\n#overfitting due to collinearity (as we discussed in the previous mission), but having more features will allow us to\n#more thoroughly compare algorithms.\n\n#So we have something to compare to, we're going to train a logistic regression model. We'll use cross validation to\n#get a baseline score.","db5a86fe":"#code snippet:\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nall_X = train.drop(['Survived','PassengerId'],axis=1)\nall_y = train['Survived']\n\nlr=LogisticRegression()\nscores=cross_val_score(lr,all_X,all_y,cv=10)\naccuracy_lr=scores.mean()\nprint(scores,\"\\n\",accuracy_lr)","95fd7ff7":"# The logistic regression baseline model from the previous screen scored 82.4%.\n\n# Model\tCross-validation score\tKaggle score\n# Previous best Kaggle score\t82.3%\t78.0%\n# Logistic regression baseline\t82.4%\t?\n\n#The logistic regression algorithm works by calculating linear rships between the features and the target variable and\n#using those to make predictions. Let's look at an algorithm that makes predictions using a different method.\n\n#The k-nearest neighbors algorithm finds the observations in our training set most similar to the observation in our\n#test set, and uses the average outcome of those 'neighbor' observations to make a prediction. The 'k' is the number\n#of neighbor observations used to make the prediction.\n\n#If you'd like to learn more about the k-nearest neighbors algorithm, you might like to check out doc about KNeighborsC\n#lassifier\n\n#Just like it does for logistic regression, scikit-learn has a class that makes it easy to use k-nearest neighbors to\n#make predictions, neighbors.KNeighborsClassifier.\n\n#The optional n_neighbors argument sets the value of k when predictions are made. The default value of n_neighbors is 5\n# but we're going to start by building a simple model that uses the closest neighbor to make our predictions.","8b4614c5":"#code snippet:\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\n\nscores = cross_val_score(knn, all_X, all_y, cv=10)\naccuracy_knn = scores.mean()\naccuracy_knn","af949333":"# The k-nearest neighbors model we trained in the previous screen had an accuracy score of 78.6%, worse than our\n#baseline score of 82.4%.\n\n# Model\tCross-validation score\tKaggle score\n# Previous best Kaggle score\t82.3%\t78.0%\n# Logistic regression baseline\t82.4%\t?\n# K-nearest neighbors, k == 1\t78.6%\t?\n\n#Besides default settings, we can vary the settings of each model\u2014 for instance the value of k in our k-nearest\n#neighbors model. This is called hyperparameter optimization.\n\n#We can use a loop and Python's inbuilt range() class to iterate through different values for k and calculate the\n#accuracy score for each different value. We will only want to test odd values for k to avoid ties,where both 'survived\n#and 'died' outcomes would have the same number of neighbors.\n\n#Let's use this technique to calculate the accuracy of our model for values of k from 1-49, storing the results in a\n#dictionary.","867435d6":"#code snippet:\nimport matplotlib.pyplot as plt\n\ndef plot_dict(dictionary):\n    pd.Series(dictionary).plot.bar(figsize=(9,6),\n                                   ylim=(0.78,0.83),rot=0)\n    plt.show()\n\nknn_scores = dict()\nfor k in range(1,50,2):\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    scores = cross_val_score(knn, all_X, all_y, cv=10)\n    accuracy_knn = scores.mean()\n    knn_scores[k] = accuracy_knn\n\nplot_dict(knn_scores)\nplt.show()","0c8d16e8":"#Looking at our plot from the previous screen we can see that a k value of 19 gave us our best score, and checking the\n#knn_scores dictionary we can see that the score was 82.4%, identical to our baseline (if we didn't round the numbers\n#you would see that it's actually 0.01% less accurate).\n\n# Model\tCross-validation score\tKaggle score\n# Previous best Kaggle score\t82.3%\t78.0%\n# Logistic regression baseline\t82.4%\t?\n# K-nearest neighbors, k == 1\t78.6%\t?\n# K-nearest neighbors, k == 19\t82.4%\t?\n\n#The technique we just used is called grid search - we train a number of models across a 'grid' of values and then\n#searched for the model that gave us the highest accuracy.Sklearn has a class to perform grid search, model_selection\n#GridSearchCV(). The 'CV' in the name indicates that we're performing both grid search and cross val at the same time.\n\n#By creating a dictionary of parameters and possible values and passing it to the GridSearchCV object we can automate\n#the process. Here's what the code from the prev screen would look like, when implemented using the GridSearchCV class.\nfrom sklearn.model_selection import GridSearchCV\n\nknn = KNeighborsClassifier()\n\nhyperparameters = {\n    \"n_neighbors\": range(1,50,2)\n}\ngrid = GridSearchCV(knn, param_grid=hyperparameters, cv=10)\ngrid.fit(all_X, all_y)\n\nprint(grid.best_params_)\nprint(grid.best_score_)\n\n#Our final step was to print the GridSearchCV.best_params_ and GridSearchCV.best_score_ attr to retrieve the params\n#of the best-performing model, and the score it achieved.\n\n#We can also use GridSearchCV to try combinations of different hyperparameters. Say we wanted to test values of\n#3 dif tree algorithms. for the algorithm parameter and values of 1, 3, and 5 for the n_neighbors algo\n#parameter. GridSearchCV would train and test 9 models (3 for the first hyperparameter times 3 for the second hyperparameter\n#shown in the diagram below.\n\n","3179d58a":"#code snippet:\nfrom sklearn.model_selection import GridSearchCV\n\nhyperparameters = {\n    \"n_neighbors\": range(1,20,2),\n    \"weights\": [\"distance\", \"uniform\"],\n    \"algorithm\": ['brute'],\n    \"p\": [1,2]\n}\nknn = KNeighborsClassifier()\ngrid = GridSearchCV(knn,param_grid=hyperparameters,cv=10)\n\ngrid.fit(all_X, all_y)\n\nbest_params = grid.best_params_\nbest_score = grid.best_score_\n\nprint(best_params)\nprint(best_score)","763d0190":"# The cross-validation score for the best performing model was 82.7%, better than our baseline model.\n\n# Model\tCross-validation score\tKaggle score\n# Previous best Kaggle score\t82.3%\t78.0%\n# Logistic regression baseline\t82.4%\t?\n# K-nearest neighbors, k == 1\t78.6%\t?\n# K-nearest neighbors, k == 19\t82.4%\t?\n# K-nearest neighbors, best model from grid search\t82.7%\t?\n\n#We can use the GridSearchCV.best_estimator_ attribute to retrieve a trained model with the best-performing hyperparams\nbest_knn = grid.best_estimator_\n#is equivalent to this code where we manually specify the hyperparameters and train the model:\nbest_knn = KNeighborsClassifier(p=1,algorithm='brute',n_neighbors=5,weights='uniform')\nbest_knn.fit(all_X,all_y)\n\n#Lets use that model to make preds on the holdout set, submit those preds to Kaggle to see if we have improved overall\n","30a57ab5":"#code snippet:\nholdout_no_id = holdout.drop(['PassengerId'],axis=1)\nbest_knn = grid.best_estimator_\nholdout_predictions = best_knn.predict(holdout_no_id)\n\nholdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv(\"submission_1.csv\",index=False)","4755f3a9":"#When we submit this(knn besthyper,best k attempt) to Kaggle,we'll see it scores 75.6%, less than our best submission\n#of 78.0%. While our model could be overfitting due to including all columns, it also seems like k-nearest neighbors\n#may not be the best algorithm choice.\n\n# Model\tCross-validation score\tKaggle score\n# Previous best Kaggle score\t82.3%\t78.0%\n# Logistic regression baseline\t82.4%\t\n# K-nearest neighbors, k == 1\t78.6%\t\n# K-nearest neighbors, k == 19\t82.4%\t\n# K-nearest neighbors, best model from grid search\t82.8%\t75.6%\n\n#Let's try another algorithm called random forests. Random forests is a specific type of decision tree algorithm\n","bb461a5b":"#code snippet:\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=1)\nscores = cross_val_score(clf, all_X, all_y, cv=10)\naccuracy_rf = scores.mean()\naccuracy_rf","b4b8fc04":"# Using the default settings, our random forests model obtained a cross validation score of 80.7%.\n\n# Model\tCross-validation score\tKaggle score\n# Previous best Kaggle score\t82.3%\t78.0%\n# Logistic regression baseline\t82.4%\t?\n# K-nearest neighbors, k == 1\t78.6%\t?\n# K-nearest neighbors, k == 19\t82.4%\t?\n# K-nearest neighbors, best model from grid search\t82.8%\t75.6%\n# Random forests, default hyperparameters\t80.7%\t?\n\n#Just like we did with the k-nearest neighbors model, we can use GridSearchCV to test a variety of hyperparams to find\n#the best performing model.\n\n#The best way to see a list of available hyperparameters is by checking the documentation for the classifier\u2014 in this\n#case, the doc for RandomForestClassifier. Let's use grid search to test out combinations of the following hyperparams:\n\n# criterion: \"entropy\" or \"gini\"\n# max_depth: 5 or 10\n# max_features: \"log2\" or \"sqrt\"\n# min_samples_leaf: 1 or 5\n# min_samples_split: 3 or 5\n# n_estimators: 6 or 9","3204b8ef":"#code snippet:\nhyperparameters = {\"criterion\": [\"entropy\", \"gini\"],\n                   \"max_depth\": [5, 10],\n                   \"max_features\": [\"log2\", \"sqrt\"],\n                   \"min_samples_leaf\": [1, 5],\n                   \"min_samples_split\": [3, 5],\n                   \"n_estimators\": [6, 9]\n}\n\nclf = RandomForestClassifier(random_state=1)\ngrid = GridSearchCV(clf,param_grid=hyperparameters,cv=10)\n\ngrid.fit(all_X, all_y)\n\nbest_params = grid.best_params_\nbest_score = grid.best_score_\n\nprint(best_params)\nprint(best_score)","9c66268f":"#The cross-validation score for the best performing model was 84.3%, making it the best cv score we've obtained \n#Let's train it on the holdout data and create a submission file to see how it performs on the Kaggle.","129d96b0":"# The `GridSearchCV` object is stored in memory from\n# the previous screen with the variable name `grid`\nbest_rf = grid.best_estimator_\nholdout_predictions = best_rf.predict(holdout_no_id)\n\nholdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv(\"submission_2.csv\",index=False)","bb23ded1":"## Submitting Random Forest Predictions to Kaggle - Attempt3","932015a3":"## Tuning our Random Forests Model with GridSearch","e5727ab3":"## Random Forests","eddfc35f":"## Hyperparameter Optimization with Grid Search","bef3b7e9":"## Exploring Different K Values","17da9cbc":"## Submitting KNN Predictions to Kaggle - Attempt2.5","68f59320":"## Training a Model using K-Nearest Neighbors"}}