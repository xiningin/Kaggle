{"cell_type":{"5e67a9ad":"code","fbbbb193":"code","1573dd05":"code","b5d59e29":"code","91fd722a":"code","456628c8":"code","02d76194":"code","7b86c5ba":"code","ba601a03":"code","9e498def":"code","d776b01f":"code","d4818b6c":"code","e34ec955":"code","3caeb413":"code","39bfeb3a":"code","5b4ed786":"code","3afed6f1":"code","7550f0ae":"markdown","e78dcc42":"markdown","faab27a9":"markdown","d383b8dd":"markdown","724130a3":"markdown","6d0d097a":"markdown","dd058650":"markdown","263d1bd1":"markdown","0bd324bc":"markdown","3126215a":"markdown","551dcd07":"markdown","4301d188":"markdown","0de11748":"markdown","767085c4":"markdown","ca6eddaf":"markdown"},"source":{"5e67a9ad":"# importing library\nimport torch\n\n# To check which types of device we have (CUDA or CPU)\ndevice = \"cuda\" if torch.cuda.is_available() else  \"cpu\"\nprint(device)","fbbbb193":"m_tensor = torch.tensor([[1,2,3],[4,5,6]]) # this will make tensor of size 2 * 3\nprint(m_tensor)\nprint(\"*\"*30)\nprint(f\"Shape of the tensor is :- {m_tensor.shape}\")\nprint(\"*\"*30)\nprint(f\"Data type of a tensor :- {m_tensor.dtype}\")\nprint(\"*\"*30)\nprint(f\"Requires Grad :- {m_tensor.requires_grad}\")\nprint(\"*\"*30)\nprint(f\"Device :- {m_tensor.device}\")","1573dd05":"# for changing data type use (dtype)\n\"\"\"\nrequires_grad (requires_grad = True they start forming a backward graph that tracks every operation applied on them to \ncalculate the gradients using something called a dynamic computation graph (DCG)\n\"\"\" \nm_tensor = torch.tensor([[1,2,3],[4,5,6]], dtype=torch.float32, requires_grad = True) # 2 * 3\nprint(m_tensor)\nprint(\"-\"*30)\nprint(f\"Shape of the tensor is :- {m_tensor.shape}\")\nprint(\"-\"*30)\nprint(f\"Data type of a tensor :- {m_tensor.dtype}\")\nprint(\"-\"*30)\nprint(f\"Requires Grad :- {m_tensor.requires_grad}\")\nprint(\"-\"*30)\nprint(f\"Device :- {m_tensor.device}\")","b5d59e29":"# torch.empty() returns a tensor filled with uninitialized data.\nx = torch.empty(size=(3,3))\nprint(\"torch.empty() \\n \", x)\nprint(\"-\"*30)\n\n# torch.zeros() returs a tensor with all zeros\nx = torch.zeros(3,3)\nprint(\"torch.zeros() \\n \", x)\nprint(\"-\"*30)\n\n# Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1)[0,1)\nx = torch.rand(3,3) #random values (0,1)\nprint(\"torch.rand() \\n \", x)\nprint(\"-\"*30)\n\n# Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.\nx = torch.ones(3,3) # ones\nprint(\"torch.ones() \\n \", x)\nprint(\"-\"*30)\n\n# Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.\nx = torch.eye(5,5) #idenity matrix\nprint(\"torch.eye() \\n \", x)\nprint(\"-\"*30)\n\n# Returns a 1-D tensor with start , end , and step \nx = torch.arange(start=0, end=5, step=1)\nprint(\"torch.arange() \\n \", x)\nprint(\"-\"*30)\n\n# Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive. \nx = torch.linspace(start=0.1, end=1, steps=10)\nprint(\"torch.linspace() \\n \", x)\nprint(\"-\"*30)\n\n# Fills the input Tensor(empty tensor in this case) with values drawn from the normal distribution \nx = torch.empty(size=(1,5)).normal_(mean=0,std=1) #normal distrubited (mean =0 , std=1)\nprint(\"torch.empty().normal_() \\n \", x)\nprint(\"-\"*30)\n\n# If input is a vector (1-D tensor), then returns a 2-D square tensor with the elements of input as the diagonal.\n# If input is a matrix (2-D tensor), then returns a 1-D tensor with the diagonal elements of input. \nx = torch.diag(torch.ones(3)) # == (eye)\nprint(\"torch.diag() \\n \", x)\nprint(\"-\"*30)","91fd722a":"t = torch.arange(4)\nprint(\"Original:- \",t)\nprint(\"-\"*30)\nprint(\"boolean :- \", t.bool()) # [0,1,1,1]\nprint(\"int16 :- \", t.short()) # create int16\nprint(\"int64 :- \", t.long()) # create int64 \nprint(\"float16 :- \", t.half()) #float16\nprint(\"float32 :- \", t.float()) #float32\nprint(\"float64 :- \", t.double()) #float64","456628c8":"# importing numpy\nimport numpy as np\n\n# init np array with 0\nnp_arr = np.zeros((5,5)) # 5*5\nprint(\"Original type :- \", type(np_arr))\nprint(\"-\"*30)\n# convert numpy array into pytorch array\nnumpy_to_pytorch = torch.from_numpy(np_arr)\nprint(\"numpy_to_pytorch :- \", type(numpy_to_pytorch))\n\n# \npytorch_to_numpy = numpy_to_pytorch.numpy()\nprint(\"pytorch_to_numpy :- \", type(pytorch_to_numpy))","02d76194":"x = torch.tensor([1,2,3])\ny = torch.tensor([9,8,7])\n\nprint(x)\nprint(y)\nprint(\"-\"*30)\n\n# addition\nprint(\"Addition :- \", torch.add(x,y))\nprint(\"-\"*30)\n\n# subtract\nprint(\"Subtract :- \", torch.subtract(x,y))\nprint(\"-\"*30)\n\n# division\nprint(\"True Divide :- \", torch.true_divide(x,y)) #element wise division (if shape is equal)\n# (or)\nprint(\"Divide :- \", torch.divide(x,y))\nprint(\"-\"*30)\n\n# multiplication\nprint(\"Multiply :- \", torch.multiply(x,y))","7b86c5ba":"\n# inplace \nx = torch.tensor([2,3,4])\ny = torch.tensor([1,2,3])\n\nprint(\"x :- \", x)\nprint(\"y :- \", y)\n\nprint(\"-\"*30)\n\n# inplace addition\nprint(\"inplace addition \", x.add_(y))  # == x += y  \nprint(\"-\"*30)\n\n# exponential\nprint(\"exponential of y by 2:- \", y.pow(2))\n# or z = y ** 2 \nprint(\"-\"*30)\n\n# compersion\nz = torch.tensor([1,2,3,4,0])\nprint(\"-\"*30)\nprint(\"compersion z :- \", z)\nprint(\"z > 0 :- \", z>0) \nprint(\"z < 0 :- \", z<0) \nprint(\"z == 0 :- \", z==0) \nprint(\"-\"*30)","ba601a03":"# Matrix Multiplication\nprint(\"Matrix Multiplication\")\nprint(\"-\"*30)\nx1 = torch.rand((2,2))\nx2 = torch.rand((2,2))\nprint(\"x1 = \", x1)\nprint(\"x2 = \", x2)\n\nprint(\"-\"*30)\n\nx3 = torch.mm(x1,x2)\nprint(\"torch.mm(x1,x2) \\n \", x3)\nprint(\"-\"*30)\nprint(\"Another techinique (x1.mm(x2) \\n  \",  x1.mm(x2))\nprint(\"-\"*30)\n\n# matrix exponentiation\nprint(\"Matrix Exponentiation \")\nprint(\"-\"*30)\nmatrix_exp = torch.rand(2,2)\nprint(\"Original Matrix \\n \", matrix_exp)\n\n\"\"\"\nmatrix_power() Returns the matrix raised to the power n for square matrices. \nFor batch of matrices, each individual matrix is raised to the power n.\n\"\"\"\nprint(\"Exponentiation matrix (by 2 power) \\n\", matrix_exp.matrix_power(2))","9e498def":"# element wise multiplication\nprint(\"element wise multiplication\")\nprint(\"-\"*30)\nx = torch.tensor([1,2,3])\ny = torch.tensor([2,3,4])\nprint(\"x = \", x)\nprint(\"y = \", y)\nprint(\"-\"*30)\nprint(\"x*y element wise\", x * y )\nprint(\"-\"*30)\n\n# dot product\nprint(\"Dot product :- Computes the dot product of two 1D tensors.\")\nprint(\"-\"*30)\nprint(\"2+6+12 = 20\", torch.dot(x,y))  # sum((x*y))","d776b01f":"print(\"Batch Matrix Multiplication\")\nprint(\"-\"*30)\nbatch = 5\nn = 1\nm = 2\np = 3\n\nt_1 = torch.rand((batch,n,m))\nt_2 = torch.rand((batch, m,p))\n\nprint(\"t_1 = \", t_1)\nprint(\"t_2 = \", t_2)\nprint(\"-\"*30)\n\nresult = torch.bmm(t_1, t_2) #(batch) size(batch, n,p)\nprint(\"Batch Matrix Multiplication: \\n \", result)","d4818b6c":"# Broadcasting\nx = torch.rand((5,5)) #this is matrix\ny = torch.rand((1,5)) # this is vector\n\n# in math we can't perform operation(add\/sub\/div\/mul)\n# in matrix and vector\nz = x - y\n# so in this case y is going to be expened\nprint(z)\n","e34ec955":"# Returns the sum of all elements in the input tensor.\nx = torch.tensor([1,2,3])\ny = torch.tensor([6,2,5])\nprint(\"x = \",x)\nprint(\"y = \",y)\nprint(\"-\"*30)\n\ns_x = torch.sum(x) \nprint(\"torch.sum() = \", s_x)\n\nprint(\"-\"*30)\n# max\nprint(\"Maximum values in x and y\")\nvalues, indices = torch.max(x,dim=0) # or simply use x.max(dim=0)\nprint(f\"Maximum value in x is {values} and index is {indices} \")\n\nvalues, indices = torch.max(y,dim=0) # or simply use y.max(dim=0)\nprint(f\"Maximum value in y is {values} and index is {indices} \")\n\nprint(\"-\"*30)\nprint(\"Minimum values in x and y\")\nvalues, indices = torch.min(x,dim=0) # or simply use x.min(dim=0)\nprint(f\"Minimum value in x is {values} and index is {indices} \")\n\nvalues, indices = torch.min(y,dim=0) # or simply use y.min(dim=0)\nprint(f\"Minimum value in y is {values} and index is {indices} \")\n\nprint(\"-\"*30)","3caeb413":"x = torch.tensor([-1,2,-4])\ny = torch.tensor([2,2,-4])\nprint(\"x =\", x)\nprint(\"y =\", y)\nprint(\"-\"*30)\n\n# abs values\n# Computes the absolute value of each element in input.\nprint(\"torch.abs() =\",torch.abs(x))\nprint(\"-\"*30)\n\n# argmax\nprint(\"torch.argmax() = \", torch.argmax(x,dim=0)) #return index\nprint(\"-\"*30)\n\n# argmin\nprint(\"torch.argmin() = \", torch.argmin(x,dim=0)) #return index\nprint(\"-\"*30)\n\n# mean\nprint(\"torch.mean() = \", torch.mean(x.float(), dim=0)) #requried float values\nprint(\"-\"*30)\n\n# compare element wise\nprint(\"torch.eq() = \", torch.eq(x,y))\nprint(\"-\"*30)\n\n# sort Sorts the elements of the input tensor along a given dimension in ascending order by value.\nvalues, indices = torch.sort(y,dim=0,descending=True)\nprint(f\"Sorts values {values}, and index {indices}\")\nprint(\"-\"*30)\n# clamp\nx_ = torch.tensor([-1,2,-3])\nprint(\"x_ =\", x_)\nprint(\"if any values of x_ is less then zero(0) it will going to give zero\")\nprint(\"torch.clamp()\", torch.clamp(x_, min=0)) #if any values of x is less then zero(0) it will going to give zero\n\n","39bfeb3a":"x = torch.tensor([1,0,1,1,1],dtype=torch.bool)\nprint(\"x =\", x)\nprint(\"-\"*30)\n\nprint(\"torch.any() -> Tests if any element in input evaluates to True.\")\nz = torch.any(x)  #check if any values in true\nprint(z)\nprint(\"-\"*30)\n\nprint(\"torch.all() -> Tests if all element in input evaluates to True.\")\na = torch.all(x) #check if all values are true\nprint(a)","5b4ed786":"# indexing\nprint(\"matrix indexing\/slicing\")\nprint(\"-\"*30)\nx = torch.rand(3, 3)\nprint(\"x-\", x)\nprint(\"-\"*30)\n\nprint(\"# getting first row \")\nprint(x[0,:])\nprint(\"-\"*30)\n\nprint(\"# getting first value\")\nprint(x[0,0:1])\nprint(\"-\"*30)\n\n# NOTE:- pytorch indexing and slicing are same as numpy \n# (https:\/\/numpy.org\/doc\/stable\/reference\/arrays.indexing.html)\n\nprint(\"# vector indexing\/slicing\")\nprint(\"-\"*30)\nx = torch.rand(10)\nprint(\"x =\", x)\nindices = [2,5,8]\nprint(x[indices])\n","3afed6f1":"# indexing\nprint(\"matrix indexing\/slicing\")\nprint(\"-\"*30)\nx = torch.rand(3, 3)\nprint(\"x-\", x)\nprint(\"-\"*30)\n\nprint(\"# getting first row \")\nprint(x[0,:])\nprint(\"-\"*30)\n\nprint(\"# getting first value\")\nprint(x[0,0:1])\nprint(\"-\"*30)\n\n# NOTE:- pytorch indexing and slicing are same as numpy \n# (https:\/\/numpy.org\/doc\/stable\/reference\/arrays.indexing.html)\n\nprint(\"# vector indexing\/slicing\")\nprint(\"-\"*30)\nx = torch.rand(10)\nprint(\"x =\", x)\nindices = [2,5,8]\nprint(x[indices])\n","7550f0ae":"# element wise multiplication","e78dcc42":"# Two tensors are \u201cbroadcastable\u201d if the following rules hold:\n\n- Each tensor has at least one dimension.\n\n- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.","faab27a9":"**What we're going to learn?**\n- PyTorch: Tensors\n- Arithmetic Operations\n- Indexing","d383b8dd":"# Batch Matrix Multiplication\n\n- Performs a batch matrix-matrix product of matrices stored in input and mat2 . input and mat2 must be 3-D tensors each containing the same number of matrices.","724130a3":"# other useful operations","6d0d097a":"# Converting PyTorch Tensor To Numpy Array And Viceversa","dd058650":"# Basic Tensor Initialization","263d1bd1":"![PyTorch](https:\/\/miro.medium.com\/max\/2400\/1*aqNgmfyBIStLrf9k7d9cng.jpeg) \n\n\n PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab. It is free and open-source software released under the Modified BSD license\n ","0bd324bc":"# Indexing\/Slicing","3126215a":"# Tensor Maths","551dcd07":"# inplace Operations\n\n## in pytorch if anything is (ended with '_' means it's inplace )","4301d188":"# Other common methods for tensor initialization \n","0de11748":"# Please upvote (^)","767085c4":"## Matrix Multiplication","ca6eddaf":"**PyTorch: Tensors**\n\nA PyTorch Tensor is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation. ... To run operations on the GPU, just cast the Tensor to a cuda datatype."}}