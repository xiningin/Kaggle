{"cell_type":{"d3df2318":"code","8c58a457":"code","a6dc18ed":"code","7b83c528":"code","4a4edb75":"code","f81a6580":"code","d0df94d1":"code","8c531229":"code","4a89101a":"code","236a8ca9":"code","3c5b2669":"code","4a25441b":"code","bc396f31":"code","b95753ca":"code","db995bbd":"code","4b735382":"code","3e25678a":"code","5f2491dc":"code","8e74600d":"code","f014b1cb":"code","337d1b0f":"code","0aec2ba5":"markdown","1f137098":"markdown","bfbc469a":"markdown","5da5c194":"markdown","8a822c0d":"markdown","efa88baa":"markdown","164b4670":"markdown","68d25fbd":"markdown","8ec8fd2d":"markdown","deb83293":"markdown","21036ca3":"markdown","ad0a638c":"markdown","3d18e483":"markdown","bb35bc37":"markdown","f6814a7a":"markdown","fde0f4c9":"markdown"},"source":{"d3df2318":"%%time\n# takes 44.7 s to install everything \n\n# BioBERT dependencies\n# Tensorflow 2.0 didn't work with the pretrained BioBERT weights\n!pip install tensorflow==1.15\n# Install bert-as-service\n!pip install bert-serving-server==1.10.0\n!pip install bert-serving-client==1.10.0\n\n# We need to rename some files to get them to work with the naming conventions expected by bert-serving-start\n!cp \/kaggle\/input\/biobert-pretrained \/kaggle\/working -r\n%mv \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/model.ckpt-1000000.index \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/bert_model.ckpt.index\n%mv \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/model.ckpt-1000000.data-00000-of-00001 \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/bert_model.ckpt.data-00000-of-00001\n%mv \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/model.ckpt-1000000.meta \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/bert_model.ckpt.meta\n\n!pip install transformers\n!pip install sentence-transformers\n\nfrom IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install scispacy\n    !pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\n        \nprint('installation done')","8c58a457":"%%time\n\n# takes 5.14 s to import everything\n\nimport subprocess\nimport pickle as pkl\nimport pandas as pd\nimport numpy as np \nfrom sentence_transformers import SentenceTransformer\nfrom transformers import BertTokenizer, BertModel\nimport pandas as pd\nfrom scipy.spatial.distance import jensenshannon\nfrom IPython.display import HTML, display\nfrom tqdm import tqdm\nimport en_core_sci_lg\nimport pickle as pkl\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation","a6dc18ed":"df = pd.read_csv('..\/input\/cord19createdataframe\/cord19_df.csv')\n#comment the following line if you want to run the all data on your pc\ndf=df[:1000]\ndf.columns","7b83c528":"#drop na and concatinate title and abstract to encode both of them at same time\ndf = df.dropna(subset=['abstract','body_text','url'])\ndf['document'] = df['title'] + '. ' + df['abstract']\ndf['document'] =  df['document'].astype(str)","4a4edb75":"bert_command = 'bert-serving-start -model_dir \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed -max_seq_len=None -max_batch_size=32 -num_worker=4'\nprocess = subprocess.Popen(bert_command.split(), stdout=subprocess.PIPE)\n\n# Start the BERT client. It takes about 10 seconds for the bert server to start, which delays the client\nfrom bert_serving.client import BertClient\n\nbc = BertClient(ignore_all_checks = True)","f81a6580":"model_bert = SentenceTransformer('bert-base-nli-max-tokens')","d0df94d1":"abstracts = df[0:]['document'].tolist()\nfull_text = df['body_text'].tolist()\n\nj = 0\nbert_text_vec = np.zeros((df.shape[0],768))\nbiobert_doc_vec = np.zeros((df.shape[0],768))\nn = 100\n\nwhile(j < len(abstracts) - n + 1):\n    print('Encoding abstracts & full_text number {} to {}'.format(j, j + n))\n    abstracts_temp = abstracts[j:j+n]\n    full_text_temp = abstracts[j:j+n]\n\n    encoded_abstract = bc.encode(abstracts_temp)\n    encoded_txt =model_bert.encode(full_text_temp)\n\n    bert_text_vec[j:j+n,:] =  encoded_txt\n    biobert_doc_vec[j:j+n,:] = encoded_abstract\n    j += n\n    # save after each 1000 \n    if j % 1000 == 0:\n        print('Updating output pickle file at j = {}...'.format(j))\n        with open('vector_df_j_{}.pkl'.format(j), \"wb\") as fp:\n            pkl.dump(bert_text_vec, fp, protocol=pkl.HIGHEST_PROTOCOL)\n            pkl.dump(biobert_doc_vec, fp, protocol=pkl.HIGHEST_PROTOCOL)\n        print('Updating done')\n\nif j < df.shape[0]:\n    print('Encoding abstracts & Full_text number {} to {}'.format(j, df.shape[0]))\n    abstracts_temp = abstracts[j:df.shape[0]]\n    full_text_temp = full_text[j:df.shape[0]]\n\n    abstracts_temp = bc.encode(abstracts_temp)\n    full_text_temp =model_bert.encode(full_text_temp)\n\n    biobert_doc_vec[j:df.shape[0],:] = abstracts_temp\n    bert_text_vec[j:df.shape[0],:] = full_text_temp\n\nprint('Encoding df done')","8c531229":"bio_vec = biobert_doc_vec\nbio_vec","4a89101a":"bert_vec = bert_text_vec\nbert_vec","236a8ca9":"bert_vec.shape","3c5b2669":"df['bert_vector']=bert_vec.tolist()\ndf['biobert_vector']=bio_vec.tolist()","4a25441b":"df.head()","bc396f31":"pkl.dump(df, open('BERT-BioBERT-Daraframe'.format(j), \"wb\"))","b95753ca":"data = pd.read_csv('..\/input\/cord19createdataframe\/cord19_df.csv')\ndf_lda=data[:5] \ndf_lda = df_lda.dropna(subset=['abstract'])\ndf_lda = df_lda.dropna(subset=['body_text'])\ndf_lda = df_lda.dropna(subset=['url'])\nall_texts_lda = df_lda.body_text\ndf_lda.shape","db995bbd":"# medium model\nnlp = en_core_sci_lg.load(disable=[\"tagger\", \"parser\", \"ner\"])\nnlp.max_length = 3000000\n# New stop words list \ncustomize_stop_words = [\n    'doi', 'preprint', 'copyright', 'org', 'https', 'et', 'al', 'author', 'figure', 'table',\n    'rights', 'reserved', 'permission', 'use', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI',\n    '-PRON-', 'usually',\n    r'\\usepackage{amsbsy', r'\\usepackage{amsfonts', r'\\usepackage{mathrsfs', r'\\usepackage{amssymb', r'\\usepackage{wasysym',\n    r'\\setlength{\\oddsidemargin}{-69pt',  r'\\usepackage{upgreek', r'\\documentclass[12pt]{minimal'\n]\n\n# Mark them as stop words\nfor w in customize_stop_words:\n    nlp.vocab[w].is_stop = True","4b735382":"def spacy_tokenizer(sentence):\n    return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)]","3e25678a":"vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, min_df=2)\ndata_vectorized_lda = vectorizer.fit_transform(tqdm(all_texts_lda))","5f2491dc":"lda = LatentDirichletAllocation(n_components=50, random_state=0,verbose=1)\nlda.fit(data_vectorized_lda)\nlda_vec=lda.transform(data_vectorized_lda)\ndoc_topic_dist_lda = pd.DataFrame(lda_vec)\npkl.dump(doc_topic_dist_lda, open('lda_output_final.pkl', \"wb\"))\nlda_pkl_read = pkl.load(open('lda_output_final.pkl', \"rb\"))\n","8e74600d":"def get_k_nearest_docs_lda(doc_dist, k=5):\n    \n    temp = lda_pkl_read\n        \n    distances = temp.apply(lambda x: jensenshannon(x, doc_dist), axis=1)\n    k_nearest = distances[distances != 0].nsmallest(n=k).index\n    k_distances = distances[distances != 0].nsmallest(n=k)\n    \n    return k_nearest, k_distances\n    ","f014b1cb":"def relevant_articles_lda(tasks, k=10):\n    \n    tasks2 = [tasks] if type(tasks) is str else tasks \n     \n    tasks_vectorized = vectorizer.transform(tasks2)\n    tasks_topic_dist = pd.DataFrame(lda.transform(tasks_vectorized))\n    \n    pkl.dump(tasks_topic_dist, open('put your query here', \"wb\"))\n    #tasks_topic_dist = pkl.load(open(''+tasks+'.pkl'+'', \"rb\"))\n\n\n    for index, bullet in enumerate(tasks2):\n        \n        recommended_index,distance = get_k_nearest_docs_lda(tasks_topic_dist.iloc[index], k)\n        recommended = df_lda.iloc[recommended_index]\n        recommended[\"index\"]=recommended_index\n        \n        h = '<br\/>'.join([str(i)+ '<a href=\"' + str(l) + '\" target=\"_blank\">'+ str(n) +'<\/a>'  for l, n ,i in recommended[['url','title','index']].values])\n\n        display(HTML(h))","337d1b0f":"query='Liverrr'\nrelevant_articles_lda(query)","0aec2ba5":"***","1f137098":"### Search for risk factor and save them as pkl to be used in [mixer kernel](https:\/\/www.kaggle.com\/marinamaher\/team-final\/edit)\n\n### LDA must be always fitted so we did this method to run risk factors fast.","bfbc469a":"### 7.Search","5da5c194":"# LDA","8a822c0d":"# BERT Model","efa88baa":"### 6.Get Nearest Papers (in Topic Space)","164b4670":"## save your new dataframe","68d25fbd":"### 1.Data -we test with a subset to test it-","8ec8fd2d":"# finally...\nopen [the following notebook](https:\/\/www.kaggle.com\/marinamaher\/bert-models-mixer-lda)to know the output of mixing this two vectors to get the relevent papers of your questions about COVID_19","deb83293":"### 3.Tokenizer ","21036ca3":"# Methodology\n## 1.Data preparation:\nIn this kernel we use the output dataframe from [CORD-19: Create Dataframe notebook](https:\/\/www.kaggle.com\/danielwolffram\/cord-19-create-dataframe). \n\nFirst, We clean the important columns from nans. \n\nThen, combined Title and Abstract is vectorized using a pretrained BERT model called BioBERT, A fine-tuned model on PubMed text. \n\nFinally, full-body text is vectorized based on BERT Model.\n\n## 2.Modeling:\nBioBert & BERT vectorizaion based  ","ad0a638c":"### 5.LDA","3d18e483":"\n### 2.Nlp & Stop Words","bb35bc37":"# Notebook purpose:\n\nEncoding Covid-19 data set using 2 different methedolgies. Namely, [BERT](http:\/\/https:\/\/github.com\/google-research\/bert) and [BioBERT](http:\/\/https:\/\/github.com\/dmis-lab\/biobert) pretrained models. \n\nThe output will be a dataframe with two added columns of BERT and BioBERT vectors of each row of COVID_19 dataframe after removing title, abstract, full-body Nans. \n\n## General Notes:\n\n1.The BERT vector based on the paper text, and BioBERT vector based on the paper title & abstract.\n\n2.This kernel runtime exceeds allowable kaggle run time. Probably, you will need to download it and run on your pc or divide the process into multiple sessions.\n\n3.We show sample of running in this notebook. However, We created public dataset of whole generated results [BioBERT + BERT Encoding](https:\/\/www.kaggle.com\/fatma98\/datasets)","f6814a7a":"### 4.Vectorizer","fde0f4c9":"# BioBERT Model"}}