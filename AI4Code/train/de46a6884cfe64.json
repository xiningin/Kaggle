{"cell_type":{"d1dac8a4":"code","c35cb44b":"code","7f7b7547":"code","9e8fb864":"code","03996193":"code","f30facf0":"code","3d06567e":"code","34a8c739":"code","1b3745cf":"code","0bac2b28":"code","0047e244":"code","c7d9fa69":"markdown","14090281":"markdown","2ba9df40":"markdown","aec2ed79":"markdown"},"source":{"d1dac8a4":"import operator as op\nimport random\nrandom.seed(123)\n\nimport numpy as np \nimport pandas as pd \nfrom math import sqrt\nfrom random import randrange\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import mode\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.metrics import confusion_matrix\nimport sklearn.metrics as skm\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(rc={'figure.figsize': (12,8)})\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c35cb44b":"df = pd.read_csv('\/kaggle\/input\/wine-quality-binary-classification\/wine.csv')\ndf.head()","7f7b7547":"df.info()","9e8fb864":"# Encoding categorical variable\ndf['quality_cat'] = df['quality'].astype('category').cat.codes\ndf.head()","03996193":"corr = np.corrcoef(df.corr())\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(df.corr(), annot=True, mask=mask)\nplt.show()","f30facf0":"# Plotting the highest correlated pairs\nsns.scatterplot(data=df, x='density', y='alcohol', hue='quality')\nplt.show()","3d06567e":"models = {}","34a8c739":"class kNN():\n    def __init__(self, num_k):\n        # Number of k clusters\n        self.num_k = num_k\n    \n    def fit(self, X_train, Y_train):\n        # Splitting the dataset into training and test dataset\n        self.X_train = X_train\n        self.Y_train = Y_train\n        \n        self.m, self.n = X_train.shape\n\n    def predict(self, X_test):\n        # Feed into the algorithm the test dataset to make some predictions\n        self.X_test = X_test\n        self.m1 = X_test.shape[0]\n        preds = np.zeros(self.m1)\n        for i in range(self.m1):\n            x = self.X_test[i]\n            neighs = np.zeros(self.num_k)\n            neighs = self.get_neighbors(x)\n            preds[i] = mode(neighs)[0][0]\n\n        return preds\n    \n    def accuracy(self, Y_test, Y_pred):\n        return np.sum(Y_test == Y_pred) \/ len(Y_test)\n\n    def get_neighbors(self, x):\n        # Locate the most simlar neighbors given a number of k clusters\n        distances = np.zeros(self.m)\n        for i in range(self.m):\n            distances[i] = self.euclidean_distance(x, self.X_train[i])\n            \n        # Sorting according to euc. dist.\n        sorted_dist = distances.argsort()\n        Y_train_sorted = self.Y_train[sorted_dist]\n\n        return Y_train_sorted[:self.num_k]\n    \n    def euclidean_distance(self, X1, X2):\n        # Calculating euclidean distance between two vectors (here data points)\n        return np.sqrt(np.sum((X1-X2)**2))","1b3745cf":"class LogReg():\n    def __init__(self, alpha, epochs):\n        # step alpha, epoch as no. of iterations\n        self.alpha = alpha\n        self.epochs = epochs\n        \n    def fit(self, X, y):\n        # Splitting it the dataset into training and test dataset and initialize the parameters\n        self.X = X\n        self.y = y\n        self.m, self.n = X.shape\n        self.theta = np.ones(self.n)\n        self.gradient_descent(X, y, self.theta, self.alpha, self.epochs)\n    \n    def gradient_descent(self, X, y, theta, alpha, epochs):\n        # Base algorithm of gradient descent (GD)\n        J = [self.cost(X, y, theta)] \n        for i in range(0, epochs):\n            h = self.hypothesis(X, theta)\n            for i in range(0, self.n):\n                theta[i] -= (alpha\/self.m) * np.sum((h-y)*X[:, i])\n            J.append(self.cost(X, y, theta))\n        return J, theta\n    \n    def cost(self, X, y, theta):\n        # Cost funtion (minmizing the cost)\n        h = self.hypothesis(X, theta)\n        y_0 = y * np.log(h)\n        y_1 = (1 - y) * np.log(1 - h)\n        return -(1\/self.m) * sum(y_0 + y_1)\n    \n    def hypothesis(self, X, theta):\n        # Calculating the hypothesis using the wights\n        z = np.dot(theta, X.T)\n        return 1\/(1 + np.e**(-z))\n    \n\n    def predict(self, X, threshold):\n        # Predicting using the algo GD\n        J, th = self.gradient_descent(self.X, self.y, self.theta, \n                                      self.alpha, self.epochs) \n        h = self.hypothesis(X, self.theta) \n        return [1 if i >= threshold else 0 for i in h]\n    \n    def accuracy(self, y_test, y_pred):\n        # Actual - predicted to obtain accuracy rate\n        return np.sum(y_test == y_pred) \/ len(y_test)","0bac2b28":"# Visualizing for the binary categories\ndef visualize(Y_test, pred_knn, pred_log):\n    # Benchmark for prediction of only 1s (only men)\n    bm_pred = [0 for _ in range(len(Y_test))]\n    bm_auc  = roc_auc_score(Y_test, bm_pred)\n    bm_fpr, bm_tpr, _ = roc_curve(Y_test, bm_pred)\n\n    knn_auc = roc_auc_score(Y_test, pred_knn)\n    log_auc = roc_auc_score(Y_test, pred_log)\n    knn_fpr, knn_tpr, _ = roc_curve(Y_test, pred_knn)\n    log_fpr, log_tpr, _ = roc_curve(Y_test, pred_log)\n\n    \n    plt.plot(bm_fpr, bm_tpr, linestyle='--', label='Benchmark')\n    plt.plot(knn_fpr, knn_tpr, marker='.', label='kNN')\n    plt.plot(log_fpr, log_tpr, marker='.', label='Log Reg')\n    text = 'BM =%.2f, K-NN =%.2f, Logistic Regression =%.2f'\n    plt.title(text % (bm_auc, knn_auc, log_auc))\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')","0047e244":"X = df.drop(['quality_cat', 'quality'], axis=1).values\nY = df['quality_cat'].values\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n                                                    test_size = 0.2, \n                                                    random_state = 2)\n# Running knn with the elbow method\nclf = kNN(4)\nclf.fit(X_train, Y_train)\npred_knn = clf.predict(X_test)\nmodels['k-NN'] = clf.accuracy(Y_test, pred_knn)\n\n# Running Logistic Regression\ninter_0 = np.ones((X_train.shape[0], 1))\ninter_1 = np.ones((X_test.shape[0], 1))\nX_train = np.hstack((inter_0, X_train))\nX_test = np.hstack((inter_1, X_test))\n\nmodel = LogReg(0.001, 10000)\nmodel.fit(X_train, Y_train)\npred_log = model.predict(X_test, 0.5)\n\nvisualize(Y_test, pred_knn, pred_log)\n\nplt.legend()\nplt.show()","c7d9fa69":"# Logistic Regression\n#### Based on a sigmoid function, outputting value (probability) between 0 and 1. Using Gradient Descent, we will attempt to iterate through the epochs at step alpha to minime the errors in order to find the global minimu if possible.","14090281":"# K-Nearest Neighbors\n#### A machine learning model hat uses euclidean distance between data points. In other words, clusters are created by proximity from which we make further predictions","2ba9df40":"## Importing necessary Library","aec2ed79":"## Data import and first exploration"}}