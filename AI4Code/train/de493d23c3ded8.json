{"cell_type":{"0ad5da56":"code","24fe53ba":"code","3e5a5c7f":"code","8de76ef2":"code","1c812b34":"code","fce40062":"code","7455ccdf":"code","71a899a0":"code","2bb19198":"code","a124a688":"code","19db3c6a":"markdown","eaa3ed97":"markdown","60f9fa2c":"markdown","7a10ec35":"markdown","ad0ffd82":"markdown"},"source":{"0ad5da56":"import numpy as np # numpy\n\nfrom sklearn import datasets # load dataset\nfrom sklearn.model_selection import train_test_split # split dataset\nfrom sklearn.preprocessing import StandardScaler # standard scaler\nfrom sklearn.svm import SVC # import model\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score # check accuracy\n\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt","24fe53ba":"# load dataset\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\nprint('Class label :', np.unique(y))","3e5a5c7f":"# split training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1, stratify = y)","8de76ef2":"# check unique value's count\nprint('y label count :', np.bincount(y))\nprint('y_train label count :', np.bincount(y_train))\nprint('y_test label count :', np.bincount(y_test))","1c812b34":"# standardize\nsc = StandardScaler()\nsc.fit(X_train) # calculate mu and sigma\nX_train_std = sc.transform(X_train) # standardize\nX_test_std = sc.transform(X_test)","fce40062":"# modeling\nsvm = SVC(kernel = 'linear', C = 1, random_state = 1)\nsvm.fit(X_train_std, y_train)\ny_pred = svm.predict(X_test_std)\nprint('The number of wrong classified sample :', (y_test != y_pred).sum())\nprint('The accuracy of SVM : %.2f' %((y_test == y_pred).sum() \/ len(y_test)))","7455ccdf":"# define function about visualizing\ndef plot_decision_regions(X, y, classifier, test_idx = None, resolution = 0.02):\n    \n    # set marker and colormap\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n    \n    # draws a decision boundary.\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                          np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha = 0.3, cmap = cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    \n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(X[y == cl, 0], X[y == cl, 1],\n                   alpha = 0.8, c = colors[idx],    # alpha : size of marker\n                   marker = markers[idx], label = cl,\n                   edgecolor = 'black')\n        \n    if test_idx:\n        X_test, y_test = X[test_idx, :], y[test_idx]\n        \n        plt.scatter(X_test[:, 0], X_test[:, 1],\n                   c = '', edgecolor = 'black', alpha = 1,\n                   s = 100, label = 'test set')","71a899a0":"X_combined_std = np.vstack((X_train_std, X_test_std)) # vlookup combine\ny_combined_std = np.hstack((y_train, y_test))  # hlookup combine\n\nplot_decision_regions(X_combined_std, y_combined_std,\n                     classifier = svm,\n                     test_idx = range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc = 'upper left')\nplt.tight_layout()\nplt.show()","2bb19198":"svm = SGDClassifier(loss = 'hinge', random_state = 3)\nsvm.fit(X_train_std, y_train)\ny_pred = svm.predict(X_test_std)\nprint('The number of wrong classified sample :', (y_test != y_pred).sum())\nprint('The accuracy of SVM : %.2f' %((y_test == y_pred).sum() \/ len(y_test)))","a124a688":"X_combined_std = np.vstack((X_train_std, X_test_std)) # vlookup combine\ny_combined_std = np.hstack((y_train, y_test))  # hlookup combine\n\nplot_decision_regions(X_combined_std, y_combined_std,\n                     classifier = svm,\n                     test_idx = range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc = 'upper left')\nplt.tight_layout()\nplt.show()","19db3c6a":"### The svm model is classified based on margin.\n\nA simple explanation of support vector machines is a algorithm to classify classes through decision boundaries that maximize **margins**. \n\n**Margin** means the distance between the superplane (decision boundary) that separates the class and the training sample closest to this superplane.\n\n<b>Therefore, it is very important to see and understand decision boundaries.\n<\/b>\n\nI recommend someone who want to know about SVM's principle to visit my [blog](https:\/\/konghana01.tistory.com\/16).","eaa3ed97":"Linear based model is weak for scale. So we have to adjust it by using sklearn's StandardScaler library","60f9fa2c":"If the data set is too large and slow, SGDClassifier can be used. \n\nThis is similar to the stochastic gradient descent method previously implemented in the [Adaline model](https:\/\/www.kaggle.com\/choihanbin\/predict-titanic-survival-by-adaptive-linear-neuron). Therefore, it has the advantage of being fast in terms of speed.","7a10ec35":"# Iris classification with sklearn SVM (linear)\n\nThis notebook goal:\n- Understanding Support Vector Machine algorithm principle by checking decision boundary of iris dataset.\n- Trying to predict Iris classification.\n- Checking accuracy of models.\n\nYou can learn about linear SVM principle in [my blog](https:\/\/konghana01.tistory.com\/16).","ad0ffd82":"### Almost Sklearn's algorithm support multicalss classification model using OvR(One-versus-Rest) method.\n\nSo we can classify iris dataset.\n\n\n##### I write the notebook explaining perceptron. So if you can refer it.\n- Perceptron : [Titanic Survival prediction with Perceptron](https:\/\/www.kaggle.com\/choihanbin\/titanic-survival-prediction-with-perceptron)\n- Adaptive linear neuron : [Predict titanic survival by Adaptive linear neuron](https:\/\/www.kaggle.com\/choihanbin\/predict-titanic-survival-by-adaptive-linear-neuron)\n- SKlearn Perceptron : [Iris classification with sklearn perceptron](https:\/\/www.kaggle.com\/choihanbin\/iris-classification-with-sklearn-perceptron)"}}