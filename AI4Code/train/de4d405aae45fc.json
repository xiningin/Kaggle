{"cell_type":{"7c0991f4":"code","d6e66179":"code","071a1719":"code","bca54795":"code","40107090":"code","0fd75917":"code","e1a97926":"code","fce8868f":"code","d55874a4":"code","78d12e94":"code","c1ac4af5":"code","d9a63efe":"code","fa00fe7a":"code","279dc761":"code","ac8992b5":"code","0c8fcd8c":"markdown","d1f24bd2":"markdown"},"source":{"7c0991f4":"!apt install aptitude -y\n!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n!pip install mecab-python3==0.996.6rc2\n\n!pip install unidic-lite","d6e66179":"import os\nimport json\nimport pandas as pd\nimport numpy as np\nimport re\n\nimport MeCab\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.utils.data\n\nfrom tqdm.autonotebook import tqdm\nimport pickle\n\nfrom transformers import *\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device('cuda')","071a1719":"class config:\n    MODEL_DIR = \"\/kaggle\/input\/aio-train-tpu-lb-best\"\n    #DATA_DIR = \"\/kaggle\/input\/aio-make-token-ids-selection\"\n    DATA_DIR = \"\/kaggle\/input\/aio-make-tokenid-with\"\n    #DATA_DIR = \"\/kaggle\/input\/aio-bert-tokenids-nnlm-v5\"\n    VALID_BATCH_SIZE = 2\n    MODEL_TYPE = \"cl-tohoku\/bert-base-japanese\"\n    TOKENIZER = BertJapaneseTokenizer.from_pretrained(MODEL_TYPE)\n    USE_EPOCH = [2]","bca54795":"with open(f\"{config.DATA_DIR}\/dev1.pkl\", \"rb\") as f:\n    dev1 = pickle.load(f)\nwith open(f\"{config.DATA_DIR}\/dev2.pkl\", \"rb\") as f:\n    dev2 = pickle.load(f)\n\nwith open(f\"{config.DATA_DIR}\/test.pkl\", \"rb\") as f:\n    test = pickle.load(f)\n\n#test_df = pd.read_json(f\"{config.DATA_DIR}\/aio_leaderboard.json\", orient='records', lines=True)\ntest_df = pd.read_json(f\"\/kaggle\/input\/aio-make-tokenid-with\/aio_leaderboard.json\", orient='records', lines=True)\nqid = test_df[\"qid\"].values\nanswer_candidates =  test_df[\"answer_candidates\"].values\n\n#dev1_questions = pd.read_json(f\"{config.DATA_DIR}\/dev1_questions.json\", orient='records', lines=True)\n#dev2_questions = pd.read_json(f\"{config.DATA_DIR}\/dev2_questions.json\", orient='records', lines=True)\ndev1_questions = pd.read_json(f\"\/kaggle\/input\/aio-make-tokenid-with\/dev1_questions.json\", orient='records', lines=True)\ndev2_questions = pd.read_json(f\"\/kaggle\/input\/aio-make-tokenid-with\/dev2_questions.json\", orient='records', lines=True)","40107090":"def extract_kakko(text):\n    re_text1 = re.findall(r\"\u300c.*?\u300d\", text)\n    re_text2 = re.findall(r\"\u300e.*?\u300f\", text)\n    re_text1 = [t.replace(\"\u300c\", \"\").replace(\"\u300d\", \"\") for t in re_text1]\n    re_text2 = [t.replace(\"\u300e\", \"\").replace(\"\u300f\", \"\") for t in re_text2]\n    return re_text1 + re_text2\n\ndef get_in_q_v2(row):\n    question = row[\"question\"]\n    answer_candidates = row[\"answer_candidates\"]\n    \n    sp_text = question.split(\"\u306e\u3046\u3061\u3001\") \n    if len(sp_text) == 1:\n        in_q = [c in question for c in answer_candidates]\n        return in_q\n    head_text = sp_text[0]\n    \n    kakko_words = extract_kakko(head_text)\n    if len(kakko_words) == 1:\n        in_q = [False for c in answer_candidates]\n        return in_q\n    elif len(kakko_words) > 1:\n        in_q = [cand not in kakko_words for cand in answer_candidates]\n        return in_q\n            \n    sp_dot_text = head_text.split(\"\u3001\")\n    if len(sp_dot_text) == 1:\n        sp_and_text = head_text.split(\"\u3068\")\n        if len(sp_and_text) == 1:\n            in_q = [c in question for c in answer_candidates]\n            return in_q\n\n        new_cand_1 = [c in sp_and_text for c in answer_candidates]\n        new_cand_2 = [sum([c in w for w in sp_and_text]) > 0 for c in answer_candidates]\n        new_cand = [c1 or c2 for c1, c2 in zip(new_cand_1, new_cand_2)]\n        if sum(new_cand) == 0:\n            in_q = [False for c in answer_candidates]\n            return in_q\n        in_q = [not c for c in new_cand]\n        return in_q\n\n    new_cand_1 = [c in sp_dot_text for c in answer_candidates]\n    new_cand_2 = [sum([c in w for w in sp_dot_text]) > 0 for c in answer_candidates]\n    new_cand = [c1 or c2 for c1, c2 in zip(new_cand_1, new_cand_2)]\n    if sum(new_cand) == 0:\n        in_q = [False for c in answer_candidates]\n        return in_q\n    in_q = [not c for c in new_cand]\n    return in_q\n\nkans = '\u3007\u4e00\u4e8c\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d'\nkans_dic = {v: i for i, v in enumerate(kans)}\n\ndef extract_kakko_cand(text):\n    re_text1 = re.findall(r\"\\(.*?\\)\", text)\n    re_text2 = re.findall(r\"\uff08.*?\uff09\", text)\n    re_text1 = [t.replace(\"(\", \"\").replace(\")\", \"\") for t in re_text1]\n    re_text2 = [t.replace(\"\uff08\", \"\").replace(\"\uff09\", \"\") for t in re_text2]\n    return re_text1 + re_text2\n\ndef is_word_length(cand, num):\n    if len(cand) == num:\n        return True\n    kakko = extract_kakko_cand(cand)\n    if len(kakko) == 0:\n        return False\n    if sum([len(w)== num for w in kakko]) > 0:\n        return True\n    _cand = str(cand)\n    for w in kakko + [\"(\", \")\", \"\uff08\", \"\uff09\"]:\n        _cand = _cand.replace(w, \"\")\n    if _cand[-1] == \" \":\n        _cand = _cand[:-1]\n    return len(_cand) == num\n\ndef is_char_type(cand, match_str):\n    match = re.findall(match_str, cand)\n    if len(match) != 0:\n        return True\n    kakko = extract_kakko_cand(cand)\n    if len(kakko) == 0:\n        return False\n    if sum([len(re.findall(match_str, w)) for w in kakko]) > 0:\n        return True\n    _cand = str(cand)\n    for w in kakko + [\"(\", \")\", \"\uff08\", \"\uff09\"]:\n        _cand = _cand.replace(w, \"\")\n    if _cand[-1] == \" \":\n        _cand = _cand[:-1]\n    return len(re.findall(match_str, _cand)) != 0\n\ndef get_in_q_v3(row):\n    question = row[\"question\"]\n    answer_candidates = row[\"answer_candidates\"]\n    \n    sp_text = question.split(\"\u6587\u5b57\u3067\u4f55\")\n    if len(sp_text) == 1:\n        return [False for _ in range(20)]\n        \n    head_text = sp_text[0]\n    \n    num = head_text[-1]\n    try:\n        num = kans_dic[num]\n    except KeyError:\n        num = int(num)\n    \n    if head_text[-3:-1] == \"\u6f22\u5b57\":\n        match_str = r'^[\\u4E00-\\u9FD0]+$'\n    elif head_text[-5:-1] == \"\u3072\u3089\u304c\u306a\":\n        match_str = r'^[\u3042-\u3093]+$'\n    elif head_text[-8:-1] in [\"\u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\", \"\u30a2\u30eb\u30d5\u30a1\u3079\u30c3\u30c8\"]:\n        match_str = r'^[a-zA-Z]+$'\n    else:\n        match_str = None\n        \n    is_len = [is_word_length(cand, num) for cand in answer_candidates]\n    if match_str is None:\n        is_char = [True for _ in range(20)]\n    else:\n        is_char = [is_char_type(cand, match_str) for cand in answer_candidates]\n    \n    is_use = [not (i and j) for i, j in zip(is_len, is_char)]\n    return is_use\n\n\ndef get_in_q_v4(row):\n    question = row[\"question\"]\n    answer_candidates = row[\"answer_candidates\"]\n    if question.find(\"\u548c\u88fd\u82f1\u8a9e\") == -1:\n        return [False for _ in range(20)]\n    is_char = [not is_char_type(cand, r'[\\\u30a1-\u30ff]+') for cand in answer_candidates]\n    return is_char","0fd75917":"dev1_in_q_v2 = dev1_questions.apply(get_in_q_v2, axis=1).tolist()\ndev2_in_q_v2 = dev2_questions.apply(get_in_q_v2, axis=1).tolist()\ntest_in_q_v2 = test_df.apply(get_in_q_v2, axis=1).tolist()\n\ndev1_in_q_v3 = dev1_questions.apply(get_in_q_v3, axis=1).tolist()\ndev2_in_q_v3 = dev2_questions.apply(get_in_q_v3, axis=1).tolist()\ntest_in_q_v3 = test_df.apply(get_in_q_v3, axis=1).tolist()\n\ndev1_in_q_v4 = dev1_questions.apply(get_in_q_v4, axis=1).tolist()\ndev2_in_q_v4 = dev2_questions.apply(get_in_q_v4, axis=1).tolist()\ntest_in_q_v4 = test_df.apply(get_in_q_v4, axis=1).tolist()\n\ndev1_in_q = [list(np.array([i, j, k]).sum(0) != 0) for i, j, k in zip(dev1_in_q_v2, dev1_in_q_v3, dev1_in_q_v4)]\ndev2_in_q = [list(np.array([i, j, k]).sum(0) != 0) for i, j, k in zip(dev2_in_q_v2, dev2_in_q_v3, dev2_in_q_v4)]\ntest_in_q = [list(np.array([i, j, k]).sum(0) != 0) for i, j, k in zip(test_in_q_v2, test_in_q_v3, test_in_q_v4)]","e1a97926":"class BertForAIO(nn.Module):\n    def __init__(self):\n        super(BertForAIO, self).__init__()\n\n        bert_conf = BertConfig(config.MODEL_TYPE)\n        bert_conf.output_hidden_states = True\n        bert_conf.vocab_size = config.TOKENIZER.vocab_size\n        #bert_conf.attention_probs_dropout_prob = 0.2\n        #bert_conf.hidden_dropout_prob = 0.2\n\n        self.n_use_layer = 1\n        self.dropout_sample = 5\n        self.dropout_ratio = 0.2\n\n        self.bert = AutoModel.from_pretrained(config.MODEL_TYPE, config=bert_conf)\n        \n        self.dropout = nn.Dropout(self.dropout_ratio)\n        \n        n_weights = bert_conf.num_hidden_layers + 1\n        weights_init = torch.zeros(n_weights).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = torch.nn.Parameter(weights_init)\n        \n        self.dense1 = nn.Linear(bert_conf.hidden_size*self.n_use_layer, bert_conf.hidden_size*self.n_use_layer)\n        self.dense2 = nn.Linear(bert_conf.hidden_size*self.n_use_layer, bert_conf.hidden_size*self.n_use_layer)\n        \n        self.dropouts = nn.ModuleList([nn.Dropout(self.dropout_ratio) for _ in range(self.dropout_sample)])\n    \n        self.fc = nn.Linear(bert_conf.hidden_size*self.n_use_layer, 1)\n\n\n    def forward(self, ids, mask, token_type_ids):\n        n_choice = ids.shape[1]\n        \n        ids = ids.view(-1, ids.size(-1))\n        mask = mask.view(-1, mask.size(-1))\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n\n        _, _, h = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n\n        cat_output = torch.stack([self.dropout(layer[:, 0, :]) for layer in h], dim=2)\n        cat_output = (torch.softmax(self.layer_weights, dim=0) * cat_output).sum(-1)\n\n        cat_output = self.dense1(cat_output)\n        cat_output = nn.ReLU()(cat_output)\n        cat_output = self.dense2(cat_output)\n        cat_output = nn.ReLU()(cat_output)\n        logits = sum([self.fc(dropout(cat_output)) for dropout in self.dropouts])\/self.dropout_sample\n\n        logits = logits.view(-1, n_choice)\n\n        return logits","fce8868f":"class JaqketDataset:\n    def __init__(self, data, optinons=20, in_q=None):\n        self.data = data\n        self.optinons = optinons\n        self.negative_sample = list(range(1, 20))\n        self.in_q = in_q\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        d = self.data[item]\n        if self.in_q is not None:\n            return {\n                'ids': torch.tensor(d[\"input_ids\"][:self.optinons], dtype=torch.long),\n                'mask': torch.tensor(d[\"attention_mask\"][:self.optinons], dtype=torch.long),\n                'token_type_ids': torch.tensor(d[\"token_type_ids\"][:self.optinons], dtype=torch.long),\n                'targets': torch.tensor(d[\"label\"], dtype=torch.long),\n                'in_q': torch.tensor(self.in_q[item], dtype=torch.long),\n            }\n        else:\n            return {\n                'ids': torch.tensor(d[\"input_ids\"][:self.optinons], dtype=torch.long),\n                'mask': torch.tensor(d[\"attention_mask\"][:self.optinons], dtype=torch.long),\n                'token_type_ids': torch.tensor(d[\"token_type_ids\"][:self.optinons], dtype=torch.long),\n                'targets': torch.tensor(d[\"label\"], dtype=torch.long)}","d55874a4":"dev1_dataset = JaqketDataset(dev1, optinons=20, in_q=dev1_in_q)\ndev1_data_loader = torch.utils.data.DataLoader(\n        dev1_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        drop_last=False,\n        num_workers=1\n  )\n\ndev2_dataset = JaqketDataset(dev2, optinons=20, in_q=dev2_in_q)\ndev2_data_loader = torch.utils.data.DataLoader(\n        dev2_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        drop_last=False,\n        num_workers=1\n  )\n\ntest_dataset = JaqketDataset(test, optinons=20, in_q=test_in_q)\ntest_data_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=config.VALID_BATCH_SIZE,\n    drop_last=False,\n    num_workers=1\n  )","78d12e94":"models = []\n\nfor epoch in config.USE_EPOCH:\n    model = BertForAIO()\n    model.load_state_dict(torch.load(f\"{config.MODEL_DIR}\/model_{epoch}.bin\", map_location=torch.device('cpu')))\n    model.to(device)\n    model.eval()\n    models.append(model)\n\nprint(\"model load complete\")","c1ac4af5":"def get_preds(models, data_loader):\n    tk = tqdm(data_loader, total=len(data_loader))\n    predict = []\n    with torch.no_grad():\n        for bi, d in enumerate(tk):\n            ids = d[\"ids\"].to(device, dtype=torch.long)\n            mask = d[\"mask\"].to(device, dtype=torch.long)\n            token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n\n            if len(ids.shape) == 2:\n                ids = ids.unsqueeze(0)\n                mask = mask.unsqueeze(0)\n                token_type_ids = token_type_ids.unsqueeze(0)\n\n            preds = []\n            for model in models:\n                p = model(ids, mask, token_type_ids)\n                preds.append(p)\n        \n            pred = sum(preds)\/len(preds)\n            pred = pred.cpu().detach().numpy()\n            pred = pred + (-99999 * (d[\"in_q\"]!=0).int().numpy())\n            pred = pred.argmax(1)\n            \n            predict.append(pred)\n    predict = np.hstack(predict)\n    return predict","d9a63efe":"dev1_preds = get_preds(models, dev1_data_loader)\ndev2_preds = get_preds(models, dev2_data_loader)\n\ndev1_t = [d[\"label\"] for d in dev1]\ndev2_t = [d[\"label\"] for d in dev2]\n\ndev1_acc = accuracy_score(dev1_t, dev1_preds)\ndev2_acc = accuracy_score(dev2_t, dev2_preds)\n\nprint(f\"dev1={dev1_acc}, dev2={dev2_acc}\")","fa00fe7a":"with open(f\"dev1_preds.pkl\", \"wb\") as f:\n    pickle.dump(dev1_preds, f)\n\nwith open(f\"dev2_preds.pkl\", \"wb\") as f:\n    pickle.dump(dev2_preds, f)","279dc761":"test_preds = get_preds(models, test_dataset)\n\npreds_lst = []\nfor id_, ans, hyp in zip(qid, answer_candidates, test_preds):\n  d = {\"qid\": id_,\"answer_entity\": ans[hyp]}\n  preds_lst.append(d)\n\npd.DataFrame(preds_lst).to_json(f'test_predict.jsonl', orient='records', force_ascii=False, lines=True)","ac8992b5":"test_df.head(10)\n!head \"test_predict.jsonl\"","0c8fcd8c":"# AI\u738b\n","d1f24bd2":"add dev dataset"}}