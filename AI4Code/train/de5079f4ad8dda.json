{"cell_type":{"3c7641ba":"code","7199af9e":"code","57e45cd9":"code","e48fc6a2":"code","100d18e0":"code","27e5eafd":"code","9a453176":"code","471b0691":"code","9e681bbb":"code","ae05c305":"code","994a3730":"code","5c5d7a10":"markdown","26b1eaab":"markdown","5696422e":"markdown","a8a23614":"markdown","e018976d":"markdown","9f6b9477":"markdown","114d3038":"markdown","dbf765e3":"markdown","78c0d5d9":"markdown","1c1e9363":"markdown"},"source":{"3c7641ba":"import pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","7199af9e":"train =pd.read_csv('\/kaggle\/input\/sec2-eda-feature-engineering\/engineered_train.csv')\ntest =pd.read_csv('\/kaggle\/input\/sec2-eda-feature-engineering\/engineered_test.csv')\ntraintest = pd.concat([train,test], axis = 0,ignore_index = False)\ntrain.head()","57e45cd9":"catcols = train.select_dtypes(include=np.object).columns\nprint('Number of categorical columns:',len(catcols))","e48fc6a2":"def generate_plots(r,c,columns):\n    \"\"\"\n    Generate pair of boxplot and countplot for each column in columns each row contains two such pairs'\n    \n    \"\"\"\n    fig ,axs = plt.subplots(r,c,figsize=(20,40))\n\n    axs = axs.flatten()\n    i = 0\n    for col in columns:\n        \n        sns.boxplot(x=train[col],y=train['LogPrice'],ax=axs[i])\n        sns.countplot(train[col], ax=axs[i+1])\n        \n        if train[col].nunique()>6:\n            axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=45)\n            axs[i+1].set_xticklabels(axs[i+1].get_xticklabels(), rotation=45) \n            \n        i=i+2\n        plt.tight_layout()\n    ","100d18e0":"generate_plots(11,4,catcols[:22])","27e5eafd":"generate_plots(11,4,catcols[22:])","9a453176":"low_var_cols = []\nfor col in catcols:\n    freq_db = (traintest[col].value_counts(normalize = True))      # We will analyse for whole dataset (include train and test)\n    if freq_db[freq_db>0.95].sum() != 0:\n        low_var_cols.append(col)\nlow_var_cols\n    ","471b0691":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nconst_mean_across_grp = []\n\nprint('Columns | P-value\\n','-'*30)\n\nfor col in catcols:\n    mod = ols('LogPrice ~ '+col ,data=train).fit()\n    anova_table = sm.stats.anova_lm(mod,typ=2)\n    \n    pr  = anova_table.loc[col,'PR(>F)']\n   \n    if pr > 0.05:\n        print(col,'|',pr)\n        const_mean_across_grp.append(col)\n","9e681bbb":"s1 = set(const_mean_across_grp)\ns2 = set(low_var_cols)\ndropcols = list(s1.union(s2))\n\ntrain.drop(dropcols,axis = 1,inplace=True)\ntest.drop(dropcols,axis = 1,inplace = True)\n","ae05c305":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\n\n# one hot encoding for categoricals\ncats = list(train.select_dtypes(object).columns)\nall_X = pd.get_dummies(data = train,columns = cats,sparse = True).copy()\nall_X.drop(['LogPrice','SalePrice','Id'],axis =1, inplace=True)\nall_y = train['LogPrice']\n\n# Modelling and validating simple regressor\nscores =cross_val_score(LinearRegression(),all_X,all_y, cv=3,scoring = 'neg_mean_squared_error')\n\n# RMSE score\nnp.sqrt(-scores.mean())","994a3730":"train.to_csv('eng_filt_train.csv',index = False)\ntest.to_csv('eng_filt_test.csv',index = False)","5c5d7a10":"# Introduction:\nThis section focuses on EDA and feature engineering for categorical columns. \n\nFor previous work access following link\n1. Data cleaning : https:\/\/www.kaggle.com\/lajari\/sec1-tedious-data-cleaning\n2. EDA and Feature engineering for numerical features :https:\/\/www.kaggle.com\/lajari\/sec2-eda-feature-engineering\n\nWe have observed around 43 categorical columns in given datset. Our study highlights, which features have significant information to predict our target column. Basically we are filtering out less significant column and test final data (including numerical and categorical features) on simple regressor.\n\nNote: The data generated from previous section will be used for analysis. \n","26b1eaab":"## Dropping less important columns:\nWe will drop columns listed in 'low_var_cols' and 'const_mean_across_grp' from train and test dataset and create final dataset for our modelling","5696422e":"## Test Model:","a8a23614":"We obtained quite good result using simple model. Our next section about predictive modelling is coming soon. In that section, we will go through various traditional and advanced modelling approaches and find out the best model for our given problem.\n\n## **Welcome all your comments and feedback. Don't forget to upvote this notebook if you find this study useful.**","e018976d":"## Load Data","9f6b9477":"## Analysis of Frequency Ditribution:\n\nWe are aiming to remove the columns with low variability. Here, low variability implies the feature which has more than 95% of total rows contain same value. ","114d3038":"## ANOVA Test:\n\nANOVA is called as analysis of Variance. It is used to compare the means of different groups. In this section, we are performing one way ANOVA. It means analysis will contain one feature at a time. and we are trying to analyse the differences in means of various groups (here groups refers to levels in categorical columns). \n\nANOVA Hypothesis:\n\n    Null hypotheses: Groups means are equal (no variation in means of groups)\n    Alternative hypotheses: At least, one group mean is different from other groups\n\nIf P-value obtained from ANOVA analysis is less than 0.05, then we conclude that there are significant difference among groups. In following code block we will filter out the columns which fails to reject null hypothesis.","dbf765e3":"## EDA","78c0d5d9":"We are going to remove above columns as their P-value >0.05. It means these features group means are not significantly differ.","1c1e9363":"In above plots we have observed two import things. First, there are some set of features which has most of the data from same group. Second,some set of features whose median of groups are quite same. In next section we will remove such features as they do not provide significant information for prediction."}}