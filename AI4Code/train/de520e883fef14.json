{"cell_type":{"a0fa28b7":"code","7ba1b1ab":"code","6c9537ef":"code","3aac489f":"code","f8f1b048":"code","68aa19b3":"code","2b019dc3":"code","c8b3adc4":"code","d823ffbc":"code","d0d016cc":"code","8f1e2118":"code","ebe2b0d4":"code","c27877a8":"code","f21c055a":"code","c5cd49ef":"markdown","1bf6398b":"markdown","33f84489":"markdown","97cafc61":"markdown","49fbbe1c":"markdown","bac1627b":"markdown","d507580b":"markdown","5b485f86":"markdown","6202a728":"markdown","ea972aa1":"markdown","0c2990ae":"markdown","0e4cd799":"markdown","5a847efb":"markdown"},"source":{"a0fa28b7":"import pandas as pd\nimport xgboost as xgb\nimport numpy as np\nfrom operator import itemgetter\nfrom sklearn.ensemble import RandomForestClassifier","7ba1b1ab":"df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ntrain_cutoff = len(df)\ndf = pd.concat([df, df_test])\n\ndf = df.drop([\"Name\"], axis = 1)\ndf.head()","6c9537ef":"df['Cabin_Letter'], df['Cabin_Number'] = df['Cabin'].str[:1], df['Cabin'].str[1:]\ndf[['1st_Cabin_Number','No_Additional_Cabins']] = df.Cabin_Number.str.split(' ',1, expand = True)\ndf[\"No_Additional_Cabins\"] = df[\"No_Additional_Cabins\"].astype(\"str\")\ndf[\"No_Additional_Cabins\"] = df[\"No_Additional_Cabins\"].str.count(\" \")\n\ndf[['Ticket_Prefix','Ticket_Suffix']] = df.Ticket.str.rsplit(' ',1, expand = True)\n\ndf = df.drop([\"Cabin\",\"Cabin_Number\", \"Ticket\"], axis = 1)","3aac489f":"replace_idx = (df[\"Ticket_Suffix\"].isna()) & (df[\"Ticket_Prefix\"]!=\"LINE\")\n\ndf[\"Ticket_Suffix\"] = np.where(replace_idx, df[\"Ticket_Prefix\"], df[\"Ticket_Suffix\"])\ndf[\"Ticket_Prefix\"] = np.where(replace_idx, \"No_Prefix\", df[\"Ticket_Prefix\"])","f8f1b048":"df[\"Ticket_Suffix\"] = pd.to_numeric(df[\"Ticket_Suffix\"])\ndf[\"1st_Cabin_Number\"] = pd.to_numeric(df[\"1st_Cabin_Number\"])","68aa19b3":"labels = df[\"Survived\"]\ndf = df.drop(\"Survived\", axis = 1)","2b019dc3":"dummy_df = pd.get_dummies(df)\ndummy_df.head()","c8b3adc4":"# Standardize Data\nfrom sklearn.preprocessing import StandardScaler\n\ndummy_df=pd.DataFrame(StandardScaler().fit_transform(dummy_df), columns = dummy_df.columns)\ndummy_df.head()\n\n# Impute Data\ncol_names = dummy_df.columns\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=10, random_state=0)\ndummy_df_vals = imp.fit_transform(dummy_df)\ndummy_df = pd.DataFrame(dummy_df_vals, columns = col_names)\n\ndummy_df.head()","d823ffbc":"X_train = dummy_df.values[:800]\ny_train = labels.values[:800]\nX_test = dummy_df.values[800:train_cutoff]\ny_test = labels.values[800:train_cutoff]","d0d016cc":"def eval_accuracy(y_hat, y):\n    return sum(y_hat == y)\/len(y_hat)","8f1e2118":"lr_lst = [0.7, 1.1, 1.5]\nsubsample_lst = [0.8, 1]\ngamma_lst = [0.7, 1, 1.3]\nmax_depth_lst = [2,3,6]\nmin_child_weight_lst = [0.2, 0.5, 1]\n\nperformance_lst = []\n\nfor lr in lr_lst:\n    for subsample in subsample_lst:\n        for max_depth in max_depth_lst:\n            for gamma in gamma_lst:\n                for min_child_weight in min_child_weight_lst:\n                    model = xgb.XGBClassifier(n_estimators = 200,\n                                              learning_rate = lr,\n                                              subsample = subsample,\n                                              gamma = gamma,\n                                              max_depth = max_depth,\n                                              min_child_weight = min_child_weight\n                                             )\n                    \n                    model.fit(X_train, y_train)\n                    y_hat_train = model.predict(X_train)\n                    y_hat_test = model.predict(X_test)\n                    train_acc = eval_accuracy(y_hat_train, y_train)\n                    test_acc = eval_accuracy(y_hat_test, y_test)\n                    performance_lst.append([test_acc, train_acc, lr, subsample, gamma, max_depth, min_child_weight])\n                    \nperformance_lst = sorted(performance_lst, key = itemgetter(0))[::-1][:5]","ebe2b0d4":"model = xgb.XGBClassifier(n_estimators = 300,\n                          learning_rate = performance_lst[0][2],\n                          subsample = performance_lst[0][3],\n                          gamma = performance_lst[0][4],\n                          max_depth = performance_lst[0][5],\n                          min_child_weight = performance_lst[0][6])\n\n\nmodel.fit(X_train, y_train)\ny_hat_train = model.predict(X_train)\ny_hat_test = model.predict(X_test)\ntrain_acc = eval_accuracy(y_hat_train, y_train)\ntest_acc = eval_accuracy(y_hat_test, y_test)\nprint(train_acc, test_acc)","c27877a8":"#now take full train set for training\nX_train_final = dummy_df.values[:train_cutoff]\ny_train_final = labels.values[:train_cutoff]\n\nX_test = dummy_df.values[train_cutoff:]\n\nmodel.fit(X_train_final, y_train_final)\n\ny_hat_submission = model.predict(X_test)\n\npassenger_id = df[\"PassengerId\"].values[train_cutoff:]\nsubmission_df = pd.DataFrame(list(zip(passenger_id, y_hat_submission)), columns = [\"PassengerId\",\"Survived\"])\nsubmission_df[\"Survived\"] = submission_df[\"Survived\"].astype(int)\n\nsubmission_df.to_csv(\"titanic_survival_prediction.csv\", index = False)","f21c055a":"'''\ncriterion_lst = [\"gini\",\"entropy\"]\nmax_depth_lst = [2, 8, 14]\nmin_samples_split_lst = [2,3,5]\nmin_samples_leaf_lst = [1,3,5]\n\nperformance_lst = []\n\nfor criterion in criterion_lst:\n    for max_depth in max_depth_lst:\n        for min_samples_split in min_samples_split_lst:\n            for min_samples_leaf in min_samples_leaf_lst:\n                \n                model = RandomForestClassifier(n_estimators = 100,\n                                              max_depth = max_depth,\n                                              min_samples_split = min_samples_split,\n                                              min_samples_leaf = min_samples_leaf)\n                \n                model.fit(X_train, y_train)\n                y_hat_train = model.predict(X_train)\n                y_hat_test = model.predict(X_test)\n                train_acc = eval_accuracy(y_hat_train, y_train)\n                test_acc = eval_accuracy(y_hat_test, y_test)\n                performance_lst.append([test_acc, train_acc, criterion, max_depth, min_samples_split,min_samples_leaf])\n\nsorted(performance_lst, key = itemgetter(0))[::-1][:5]\n'''","c5cd49ef":"### Final Training, Submission Preparation\nThe best-fitting architecture is now trained on the whole dataset. This model is then used to predict the test dataset for submission.","1bf6398b":"### Simple Accuracy Function","33f84489":"### Train-Test-Split","97cafc61":"### Grid Search Parameter Tuning for XGBoost","49fbbe1c":"### Some feature engineering\n\n\"Cabin\" is split into Letter and Number, as creating dummies for all combinations is pointless and dropping the variable might be a mistake as it may explain variance. If one person holds more than one cabin, the additional cabins are discarded and the amount of additional cabins is stored in the column \"No_Additional_Cabins\".\n\n\"Tickets\" usually contains a string, followed by a number. It is therefore split for similar reasons as \"Cabins\". ","bac1627b":"### Tranfering numerical part of Ticket Number to \"Ticket Suffix\" Column","d507580b":"### Importing Data\nThe train and test data is imported. To apply preprocessing to both datasets, it is first concatenated and then, before training, split again by the train_cutoff, which is stored here. \"Name\" is dropped (Extracting titles first might be a better approach!).","5b485f86":"### Alternative Approach with Random Forest Classifier\nThis approach wasn't pursued further due to lower test accuracy","6202a728":"# XGBoost for Titanic\nThis notebook contains importing, preprocessing, feature engineering, hyperparameter tuning and training of a XBG-Classifier to tackle the Titanic Classification problem.","ea972aa1":"### Standardizing and Imputing of Data\nFor Imputing, sklearn's iterative imputer is used.","0c2990ae":"### Isolating labels to not be affected by further preprocessing","0e4cd799":"### Transferring numeric string-variables to a numeric form","5a847efb":"### Generation of Dummy Variables"}}