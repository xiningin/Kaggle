{"cell_type":{"274f965e":"code","3060ac8c":"code","0b2c135b":"code","c41c20fa":"code","bfd51d17":"code","1986a919":"code","916cda3b":"markdown","25f7e914":"markdown","e7323082":"markdown","2fa2dd83":"markdown","8b0fd759":"markdown","3d48db74":"markdown","e6cc5368":"markdown","389d0537":"markdown","9a1ef56e":"markdown","7412cd69":"markdown","2429a155":"markdown","3dfc6914":"markdown","db97ffba":"markdown","6f3d1f17":"markdown","570896e2":"markdown","05afa782":"markdown","961699f9":"markdown","25447467":"markdown","a70565a9":"markdown","96c49007":"markdown"},"source":{"274f965e":"#import Python libraries used in this lecture\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score, learning_curve, validation_curve\nimport sklearn.model_selection as model_selection\nfrom sklearn import tree\nfrom sklearn import metrics","3060ac8c":"df2 = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\") # read data\ndf2.head(10)","0b2c135b":"X = df2.iloc[:,0:8] #stop is excluded\ny = df2.iloc[:,8] \nprint(X) ","c41c20fa":"#split data into training and test data (80% versus 20%)\nX_train,X_test,y_train,y_test = model_selection.train_test_split(X,y,test_size=0.2,random_state=4) \n#further split traning data into training and validation data (90% versus 10%)\nX_train_new, X_val, y_train_new, y_val = model_selection.train_test_split(X_train, y_train, test_size=0.1, random_state=4)","bfd51d17":"from sklearn.model_selection import GridSearchCV","1986a919":"clf_3 = KNeighborsClassifier() \nparam_grid = [{'weights':['uniform'], 'n_neighbors':list(range(1,30))},\n               {'weights':['distance'], 'n_neighbors':list(range(1,30))}]\nprint(param_grid)","916cda3b":"## Resources\/references\n1. Sklearn 3.1. Cross-validation: evaluating estimator performance: https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\n2. Sklearn 3.2. Tuning the hyper-parameters of an estimator https:\/\/scikit-learn.org\/stable\/modules\/grid_search.html   ","25f7e914":"(3) Create empty lists for recording results on traning, validation and test data.","e7323082":"(4) create a grid search object and fit it on training data","2fa2dd83":"(2) Split the data into three parts: training, validation and test datasets\n\n* split data into training and test data (80% versus 20%)\n* further split traning data into training and validation data (90% versus 10%)","8b0fd759":"(1) Import gridsearch library","3d48db74":"### Final Model Selection and Evaluation\n* Select the best tree  \n* Evaluate the model on test data","e6cc5368":"## Reflect\nBriefly note what you\u2019ve learnt, found easy and found challenging in your Jupyter notebook. Keep these notes safe and maintain a reflective log for each lab session.","389d0537":"(6) Print accuracy of the best model on test data  ","9a1ef56e":"(3) split data into training and test data (80% versus 20%) ","7412cd69":"## Activity 2 Use Grid Search to Tune Hyperparameters\n* Dataset: Diabetes dataset (see previous lecture and lab)\n* Classifier: KNN\n* Model selection\n    * uses one criterion: accuracy of a classifier\n    * the same model (KNN) with different values of k in KNN (hyperparameter)\n* Two hyperparameters\n    * n_neighbors: [0:30]\n    * weights: \u2018uniform\u2019, \u2018distance\u2019\n    \n*Code is similar to Example 4 in Lecture 09*","2429a155":"## Learning objectives\n* Learn to tune hyperparameter k in KNN manually\n* Learn to tune hyperparameters k and distance (weight) in KNN automatically","3dfc6914":"(3) Plot the score curves on training and validation   datasets \n\n*Question 1: which value is the best of k?* \n","db97ffba":"### Prepare Data\n(1) Split features and target variable","6f3d1f17":"### Build KNN models with different k values\n(1)  Set the range of k (the number of neighbour instances) to [1:20]","570896e2":"(2) Create a KNN object and a parameter grid ","05afa782":"(2) calculate the accuracy of KNN models for k in [1,21] on training and validation data","961699f9":"## Activity 1  Manually Tuning Hyperparameter Use Holdout Method\n\n* Dataset: Diabetes dataset  \n* Classifier: KNN\n* Model selection\n    * uses one criterion: accuracy of a classifier\n    * the same model (KNN) with different hyper parameter values\n* Hyperparameter: k the number of neighbours\n* Split data into three parts:   training, validation, and test datasets\n* The range of k is [0,30]\n\n*Code is similar to Example 2 in Lecture 09*\n\n### Load data  \n(1) Add Pima Indians Diabetes data, which is available at https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database. You can add this data set from `Add Data` button in your Kaggle Kernel with the above URL. \n","25447467":"(5) Find the best model and print its parameters. Fit the best model on training data****","a70565a9":"* print the parameters of the best model","96c49007":"# Model Selection and Hyperparameter Tuning\nCOMP20121 Machine Learning for Data Analytics\n\nAuthor: [Jun He](https:\/\/sites.google.com\/site\/hejunhomepage\/) "}}