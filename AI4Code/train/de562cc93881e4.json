{"cell_type":{"67845112":"code","c6c978e4":"code","d5e1806b":"code","789a405d":"code","538df40c":"code","2be7cb46":"code","7f9365f1":"code","aa723dfb":"code","2a13de61":"code","7906a6cc":"code","77910973":"code","aad9db56":"code","9afead67":"code","6c41d748":"code","3ef5ec7f":"code","81ac0639":"code","8703e725":"code","4ed2234d":"code","61c4e8e0":"code","0234aebe":"code","f0cd89dd":"code","a0923445":"code","eab8ef7b":"code","a9d6d3bc":"code","50ddfd7d":"code","519265aa":"code","e4fbe7e2":"code","b38d0fc4":"code","71fd042c":"code","0e4126de":"code","56873993":"code","4577e289":"code","03064111":"code","993bde70":"code","7ea610bc":"code","f0a397b8":"code","2f21e0aa":"code","d2b730fc":"code","dc3716ec":"code","2bfc1552":"code","158dca51":"markdown","993d2999":"markdown","d64fee4a":"markdown","a10355ef":"markdown","73e18ac9":"markdown","0c11766e":"markdown","2d5e4204":"markdown","22d539bf":"markdown","ed1c1be5":"markdown","138a9398":"markdown","2d7e0685":"markdown","23b5caec":"markdown","dd5cc86d":"markdown","af292030":"markdown","69aea503":"markdown","fa55e54e":"markdown","0a397ce8":"markdown","c89b8d1a":"markdown","0d62faa8":"markdown","c7395e1e":"markdown"},"source":{"67845112":"import glob, pylab, pandas as pd\nimport pydicom, numpy as np","c6c978e4":"!ls ..\/input","d5e1806b":"'''\nwith open('..\/input\/GCP Credits Request Link - RSNA.txt')as f:\n    content=f.readlines()\n    print(content)\n'''","789a405d":"df = pd.read_csv('..\/input\/rsna-pneumonia-detection-challenge\/stage_1_train_labels.csv')\nprint(df.iloc[0])","538df40c":"print(df.info())\nprint(df.iloc[4])","2be7cb46":"'''\npatientId = df['patientId'][0]\ndcm_file = '..\/input\/stage_1_train_images\/%s.dcm' % patientId\ndcm_data = pydicom.read_file(dcm_file)\n\nprint(dcm_data)\n'''","7f9365f1":"'''\nim = dcm_data.pixel_array\nprint(type(im))\nprint(im.dtype)\nprint(im.shape)\n'''","aa723dfb":"#pylab.imshow(im, cmap=pylab.cm.gist_gray)\n#pylab.axis('off')","2a13de61":"def parse_data(df):\n    \"\"\"\n    Method to read a CSV file (Pandas dataframe) and parse the \n    data into the following nested dictionary:\n\n      parsed = {\n        \n        'patientId-00': {\n            'dicom': path\/to\/dicom\/file,\n            'label': either 0 or 1 for normal or pnuemonia, \n            'boxes': list of box(es)\n        },\n        'patientId-01': {\n            'dicom': path\/to\/dicom\/file,\n            'label': either 0 or 1 for normal or pnuemonia, \n            'boxes': list of box(es)\n        }, ...\n\n      }\n\n    \"\"\"\n    # --- Define lambda to extract coords in list [y, x, height, width]\n    extract_box = lambda row: [row['y'], row['x'], row['height'], row['width']]\n\n    parsed = {}\n    for n, row in df.iterrows():\n        # --- Initialize patient entry into parsed \n        pid = row['patientId']\n        if pid not in parsed:\n            parsed[pid] = {\n                'dicom': '..\/input\/stage_1_train_images\/%s.dcm' % pid,\n                'label': row['Target'],\n                'boxes': []}\n\n        # --- Add box if opacity is present\n        if parsed[pid]['label'] == 1:\n            parsed[pid]['boxes'].append(extract_box(row))\n\n    return parsed","7906a6cc":"#parsed = parse_data(df)\n\n","77910973":"#print(parsed[ 'c542f0f4-1903-4fee-ba0f-186203d35226'])","aad9db56":"'''\ndef draw(data):\n    \"\"\"\n    Method to draw single patient with bounding box(es) if present \n\n    \"\"\"\n    # --- Open DICOM file\n    d = pydicom.read_file(data['dicom'])\n    im = d.pixel_array\n\n    # --- Convert from single-channel grayscale to 3-channel RGB\n    im = np.stack([im] * 3, axis=2)\n\n    # --- Add boxes with random color if present\n    for box in data['boxes']:\n        rgb = np.floor(np.random.rand(3) * 256).astype('int')\n        im = overlay_box(im=im, box=box, rgb=rgb, stroke=6)\n\n    pylab.imshow(im, cmap=pylab.cm.gist_gray)\n    pylab.axis('off')\n\ndef overlay_box(im, box, rgb, stroke=1):\n    \"\"\"\n    Method to overlay single box on image\n\n    \"\"\"\n    # --- Convert coordinates to integers\n    box = [int(b) for b in box]\n    \n    # --- Extract coordinates\n    y1, x1, height, width = box\n    y2 = y1 + height\n    x2 = x1 + width\n\n    im[y1:y1 + stroke, x1:x2] = rgb\n    im[y2:y2 + stroke, x1:x2] = rgb\n    im[y1:y2, x1:x1 + stroke] = rgb\n    im[y1:y2, x2:x2 + stroke] = rgb\n\n    return im\n    '''","9afead67":"#draw(parsed['00436515-870c-4b36-a041-de91049b9ab4'])","6c41d748":"#df_detailed = pd.read_csv('..\/input\/stage_1_detailed_class_info.csv')\n#print(df_detailed.iloc[0])","3ef5ec7f":"#patientId = df_detailed['patientId'][0]\n#draw(parsed[patientId])","81ac0639":"'''\nsummary = {}\nfor n, row in df_detailed.iterrows():\n    if row['class'] not in summary:\n        summary[row['class']] = 0\n    summary[row['class']] += 1\n    \nprint(summary)\n'''","8703e725":"import glob, pylab, pandas as pd\nimport pydicom, numpy as np\n\nimport os\nimport csv\nimport random\nfrom skimage import measure\nfrom skimage.transform import resize\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport skimage.exposure\n\nfrom matplotlib import pyplot as plt\n\n\n\n\n\n","4ed2234d":"df_detailed = pd.read_csv('..\/input\/rsna-pneumonia-detection-challenge\/stage_1_detailed_class_info.csv')\nprint(df_detailed.iloc[6])\nprint(df_detailed.iloc[80])","61c4e8e0":"# empty dictionary\nnodule_locations = {}\n# load table\nwith open(os.path.join('..\/input\/rsna-pneumonia-detection-challenge\/stage_1_train_labels.csv'), mode='r') as infile:\n    reader = csv.reader(infile)\n    # skip header\n    next(reader, None)\n\n    for rows in reader:\n        filename = rows[0]\n        location = rows[1:5]\n        nodule = rows[5]\n        # if row contains a nodule add label to dictionary\n        # which contains a list of nodule locations per filename\n        if nodule == '1':\n            # convert string to float to int\n            location = [int(float(i)) for i in location]\n            # save nodule location in dictionary\n            if filename in nodule_locations:\n                nodule_locations[filename].append(location)\n            else:\n                nodule_locations[filename] = [location]\n","0234aebe":"folder = '..\/input\/rsna-pneumonia-detection-challenge\/stage_1_train_images'\nfilenames = os.listdir(folder)\nrandom.shuffle(filenames)\n# split into train and validation filenames\nn_valid_samples = 2000\ntrain_filenames = filenames[n_valid_samples:]\nvalid_filenames = filenames[:n_valid_samples]\nprint('n train samples', len(train_filenames))\nprint('n valid samples', len(valid_filenames))\nn_train_samples = len(filenames) - n_valid_samples","f0cd89dd":"class generator(keras.utils.Sequence):\n    \n    def __init__(self, folder, filenames, nodule_locations=None, batch_size=32, image_size=128, shuffle=True, predict=False):\n        self.folder = folder\n        self.filenames = filenames\n        self.nodule_locations = nodule_locations\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.shuffle = shuffle\n        self.predict = predict\n        self.on_epoch_end()\n        \n    def __load__(self, filename):\n        # load dicom file as numpy array\n        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n        # create empty mask\n        #msk = np.zeros(img.shape)\n        # get filename without extension\n        filename = filename.split('.')[0]\n        # if image contains nodules\n        if filename in nodule_locations:\n            # loop through nodules\n            pneumonia=1\n        else:\n            pneumonia=0\n                \n        # resize both image and mask\n        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n        #msk = resize(msk, (self.image_size, self.image_size), mode='reflect') > 0.5\n        # add trailing channel dimension\n        img = np.expand_dims(img, -1)\n        #msk = np.expand_dims(msk, -1)\n        return img,pneumonia # ,msk\n    \n    def __loadpredict__(self, filename):\n        # load dicom file as numpy array\n        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n        # resize image\n        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n        # add trailing channel dimension\n        img = np.expand_dims(img, -1)\n        return img\n        \n    def __getitem__(self, index):\n        # select batch\n        filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n        # predict mode: return images and filenames\n        if self.predict:\n            # load files\n            imgs = [self.__loadpredict__(filename) for filename in filenames]\n            # create numpy batch\n            imgs = np.array(imgs)\n            return imgs, filenames\n        # train mode: return images and masks\n        else:\n            # load files\n            items = [self.__load__(filename) for filename in filenames]\n            # unzip images and masks\n            imgs, pneumonia = zip(*items)\n            \n            # create numpy batch\n            imgs = np.array(imgs)\n            #imgs= [skimage.transform.resize(imgs, (128,128,1))]   \n            pneumonia = np.array(pneumonia)\n            #pneumonia=pneumonia.reshape(16,1,1,1)\n            return imgs,pneumonia #, msks\n        \n    def on_epoch_end(self):\n        if self.shuffle:\n            random.shuffle(self.filenames)\n        \n    def __len__(self):\n        if self.predict:\n            # return everything\n            return int(np.ceil(len(self.filenames) \/ self.batch_size))\n        else:\n            # return full batches only\n            return int(len(self.filenames) \/ self.batch_size)","a0923445":"# create train and validation generators\nfolder = '..\/input\/rsna-pneumonia-detection-challenge\/stage_1_train_images'\ntrain_gen = generator(folder, train_filenames, nodule_locations, batch_size=16, image_size=128, shuffle=True, predict=False)\nvalid_gen = generator(folder, valid_filenames, nodule_locations, batch_size=16, image_size=128, shuffle=False, predict=False)\n#x_val_new, y_val=valid_gen\n#validation_generator = test_datagen.flow(valid_gen,batch_size=16)","eab8ef7b":"def identity_block(inputs,kernel_size,filters):\n    filters1, filters2, filters3 = filters\n    \n    x = keras.layers.Conv2D(filters1, (1, 1)) (inputs)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.ReLU()(x)\n\n    x = keras.layers.Conv2D(filters2, kernel_size,\n               padding='same')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.ReLU()(x)\n\n    x =keras.layers.Conv2D(filters3, (1, 1))(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = keras.layers.add([x, inputs])\n    x = keras.layers.ReLU()(x)\n    return x\n","a9d6d3bc":"def webnet(input_size):\n    inputs= keras.Input(shape=(input_size, input_size, 1))\n    conv1 = keras.layers.Conv2D(64, (3, 3), activation='relu',padding='same')(inputs)\n    #conv1 = Dropout(0.5)(conv1)\n    conv1 = keras.layers.BatchNormalization()(conv1)\n    conv1 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n    #conv1 = Dropout(0.5)(conv1)\n    conv1 = keras.layers.BatchNormalization()(conv1)\n    pool1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    \n    conv2=identity_block(pool1,3,[64,64,64])\n    #conv2 = Dropout(0.5)(conv2)\n    conv2 = identity_block(conv2,3,[64,64,64])\n    pool2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = identity_block(pool2,3,[32,32,64])\n    #conv3 = Dropout(0.5)(conv3)\n    conv3 = identity_block(conv3,3,[32,32,64])\n    pool3 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = identity_block(pool3,3,[32,32,64])\n    #conv4 = Dropout(0.5)(conv4)\n    conv4 = identity_block(conv4,3,[32,32,64])\n    pool4 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    conv5 = identity_block(pool4,3,[32,32,64])\n    #conv5 = Dropout(0.5)(conv5)\n    conv5 = identity_block(conv5,3,[32,32,64])\n    conv5 = identity_block(conv5,3,[32,32,64])\n\n    up6 = keras.layers.Concatenate(axis=-1)([keras.layers.UpSampling2D(size=(2, 2))(conv5), conv4])\n    conv6 = keras.layers.Conv2D(64, (3, 3), activation='relu',padding='same')(up6)\n    conv6 = identity_block(conv6,3,[32,32,64])\n    #conv6 = Dropout(0.5)(conv6)\n    conv6 = identity_block(conv6,3,[32,32,64])\n\n    up7 = keras.layers.Concatenate(axis=-1)([keras.layers.UpSampling2D(size=(2, 2))(conv6), conv3])\n    conv7 = keras.layers.Conv2D(64, (3, 3), activation='relu',padding='same')(up7)\n    conv7 = identity_block(conv7,3,[32,32,64])\n    #conv7 = Dropout(0.5)(conv7)\n    conv7 = identity_block(conv7,3,[32,32,64])\n    pool7 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv7)\n\n    concat8 = keras.layers.Concatenate(axis=-1)([pool7, conv6])\n    conv8 = keras.layers.Conv2D(64, (3, 3), activation='relu',padding='same')(concat8)\n    conv8 = identity_block(conv8,3,[32,32,64])\n    #conv8 = Dropout(0.5)(conv8)\n    conv8 = identity_block(conv8,3,[32,32,64])\n    pool8 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv8)\n\n    concat9 =keras.layers.Concatenate()([pool8, conv5])\n    conv9 = keras.layers.Conv2D(64, (3, 3), activation='relu',padding='same')(concat9)\n    conv9 = identity_block(conv9,3,[32,32,64])\n    #conv9 = Dropout(0.5)(conv9)\n    conv9 = identity_block(conv9,3,[32,32,64])\n    #conv9 = Dropout(0.5)(conv9)\n       \n    up10 = keras.layers.Concatenate(axis=-1)([keras.layers.UpSampling2D(size=(2, 2))(conv9), conv8])\n    conv10 = keras.layers.Conv2D(64, (3, 3), activation='relu',padding='same')(up10)\n    conv10 = identity_block(conv10,3,[32,32,64])\n    #conv10 = Dropout(0.5)(conv6)\n    conv10 = identity_block(conv10,3,[32,32,64])\n    \n    up11 = keras.layers.Concatenate(axis=-1)([keras.layers.UpSampling2D(size=(2, 2))(conv10), conv7])\n    conv11 = keras.layers.Conv2D(64, (3, 3), activation='relu',padding='same')(up11)\n    conv11 =identity_block(conv11,3,[32,32,64])\n    #conv11 = Dropout(0.5)(conv11)\n    conv11 = identity_block(conv11,3,[32,32,64])\n    pool11 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv11)\n    \n    concat12 = keras.layers.Concatenate()([pool11, conv10])\n    conv12 = keras.layers.Conv2D(64, (3, 3), activation='relu',padding='same')(concat12)\n    conv12 = identity_block(conv12,3,[32,32,64])\n    #conv12 = Dropout(0.5)(conv12)\n    conv12 = identity_block(conv12,3,[32,32,64])\n    pool12 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv12)\n    \n    concat13 = keras.layers.Concatenate(axis=-1)([pool12, conv9])\n    conv13 = keras.layers.Conv2D(64, (3, 3), activation='relu',padding='same')(concat13)\n    conv13 = identity_block(conv13,3,[32,32,64])\n    #conv13 = Dropout(0.5)(conv13)\n    conv13 = identity_block(conv13,3,[32,32,64])\n     \n    up14 = keras.layers.Concatenate(axis=-1)([keras.layers.UpSampling2D(size=(2, 2))(conv13), conv12])\n    conv14 = keras.layers.Conv2D(64, (3, 3), activation='relu',padding='same')(up14)\n    conv14 = identity_block(conv14,3,[32,32,64])\n    conv14 = identity_block(conv14,3,[32,32,64])\n    \n    up15 = keras.layers.Concatenate(axis=-1)([keras.layers.UpSampling2D(size=(2, 2))(conv14), conv11])\n    conv15 = keras.layers.Conv2D(64, (3, 3), activation='relu',padding='same')(up15)\n    conv15 = identity_block(conv15,3,[32,32,64])\n    #conv15 = Dropout(0.5)(conv15)\n    conv15 = identity_block(conv15,3,[32,32,64])\n    \n    up16 = keras.layers.Concatenate(axis=-1)([keras.layers.UpSampling2D(size=(2, 2))(conv15), conv2])\n    conv16 = keras.layers.Conv2D(64, (3, 3), activation='relu',padding='same')(up16)\n    conv16 = identity_block(conv16,3,[32,32,64])\n    #conv16 = Dropout(0.5)(conv16)\n    conv16 = identity_block(conv16,3,[32,32,64])\n    \n    up17 = keras.layers.Concatenate(axis=-1)([keras.layers.UpSampling2D(size=(2, 2))(conv16), conv1])\n    conv17 = keras.layers.Conv2D(64, (3, 3), activation='relu',padding='same')(up17)\n    conv17 = identity_block(conv17,3,[32,32,64])\n    #conv17 = Dropout(0.5)(conv17)\n    #conv17 = AveragePooling2D((7, 7))(conv17)\n    conv17 = keras.layers.Conv2D(1, (1, 1), activation='sigmoid', padding='same')(conv17)\n    flat1=keras.layers.Flatten()(conv17)\n    dense1= keras.layers.Dense(1, activation='sigmoid')(flat1)\n    model = keras.Model(inputs=inputs, outputs=dense1)\n\n    return model","50ddfd7d":"model = webnet(input_size=128)\nmodel.compile(optimizer='SGD',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","519265aa":"model.summary()","e4fbe7e2":"def cosine_annealing(x):\n    lr = 0.001\n    epochs = 25\n    return lr*(np.cos(np.pi*x\/epochs)+1.)\/2\nlearning_rate = tf.keras.callbacks.LearningRateScheduler(cosine_annealing)","b38d0fc4":"model_info=model.fit_generator(\n train_gen,\n    steps_per_epoch=200,\n    epochs=100,\n    validation_data=valid_gen,\n    validation_steps=50,\n     callbacks=[tf.keras.callbacks.CSVLogger(os.path.join('training_log.csv'), append=True),\n                                         #ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5, verbose=1, patience=50),\n                                         tf.keras.callbacks.ModelCheckpoint(os.path.join(\n                                               #'weights.ep-{epoch:02d}-val_mean_IOU-{val_mean_IOU_gpu:.2f}_val_loss_{val_loss:.2f}.hdf5',\n                                               'last_checkpoint.hdf5'),\n                                               monitor='val_loss', mode='min', save_best_only=True, \n                                               save_weights_only=False, verbose=0)])","71fd042c":"def plot_model_history(model_history):\n    fig, axs = plt.subplots(1,2,figsize=(15,5))\n    # summarize history for accuracy\n    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n    axs[0].set_title('Model Accuracy')\n    axs[0].set_ylabel('Accuracy')\n    axs[0].set_xlabel('Epoch')\n    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])\/10)\n    axs[0].legend(['train', 'val'], loc='best')\n    # summarize history for loss\n    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n    axs[1].set_title('Model Loss')\n    axs[1].set_ylabel('Loss')\n    axs[1].set_xlabel('Epoch')\n    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])\/10)\n    axs[1].legend(['train', 'val'], loc='best')\n    plt.show()","0e4126de":"#plot_model_history(model_info)","56873993":"model_path = os.path.join('..\/input\/webnet1-rsna\/last_checkpoint.hdf5')","4577e289":"model.load_weights(model_path)","03064111":"\nfolder = '..\/input\/rsna-pneumonia-detection-challenge\/stage_1_train_images'\nfilenames = os.listdir(folder)\n\n# split into train and validation filenames\nn_valid_samples = 2000\n\nvalid_filenames2 = filenames[:n_valid_samples]","993bde70":"valid_gen_ = generator(folder, valid_filenames2, nodule_locations, batch_size=16, image_size=128, shuffle=False, predict=False)","7ea610bc":"pred = model.predict_generator(valid_gen_)","f0a397b8":"print (pred.shape)","2f21e0aa":"print ((pred[2]))","d2b730fc":"#import math\n\n\ncorrect=0\nfor i in range(2000):\n  \n    if (pred[i]   >= 0.1 and df['Target'][i] == 1):\n        correct +=1\n    if (pred[i]  < 0.1 and df['Target'][i] == 0):\n        correct +=1\n        \n    \n        \n\naccuracy= correct\/2000\n\nprint(accuracy)   \n","dc3716ec":"#print(accuracy)","2bfc1552":"#print (df['Target'][:2000])","158dca51":"# Overview of DICOM files and medical images\n\nMedical images are stored in a special format known as DICOM files (`*.dcm`). They contain a combination of header metadata as well as underlying raw image arrays for pixel data. In Python, one popular library to access and manipulate DICOM files is the `pydicom` module. To use the `pydicom` library, first find the DICOM file for a given `patientId` by simply looking for the matching file in the `stage_1_train_images\/` folder, and the use the `pydicom.read_file()` method to load the data:","993d2999":"As we see here, the patient does not have pneumonia however *does* have another imaging abnormality present. Let's take a closer look:","d64fee4a":"Most of the standard headers containing patient identifable information have been anonymized (removed) so we are left with a relatively sparse set of metadata. The primary field we will be accessing is the underlying pixel data as follows:","a10355ef":"## Considerations\n\nAs we can see here, the pixel array data is stored as a Numpy array, a powerful numeric Python library for handling and manipulating matrix data (among other things). In addition, it is apparent here that the original radiographs have been preprocessed for us as follows:\n\n* The relatively high dynamic range, high bit-depth original images have been rescaled to 8-bit encoding (256 grayscales). For the radiologists out there, this means that the images have been windowed and leveled already. In clinical practice, manipulating the image bit-depth is typically done manually by a radiologist to highlight certain disease processes. To visually assess the quality of the automated bit-depth downscaling and for considerations on potentially improving this baseline, consider consultation with a radiologist physician.\n\n* The relativley large original image matrices (typically acquired at >2000 x 2000) have been resized to the data-science friendly shape of 1024 x 1024. For the purposes of this challenge, the diagnosis of most pneumonia cases can typically be made at this resolution. To visually assess the feasibility of diagnosis at this resolution, and to determine the optimal resolution for pneumonia detection (oftentimes can be done at a resolution *even smaller* than 1024 x 1024), consider consultation with a radiogist physician.\n\n## Visualizing An Example\n\nTo take a look at this first DICOM image, let's use the `pylab.imshow()` method:","73e18ac9":"# Challenge Data\n\nThe challenge data is organized in several files and folders. If you are following along in the Kaggle kernel, this data will be preloaded in the `..\/input` directory:","0c11766e":"Let's use the method here:","2d5e4204":"As we can see, there is a relatively even split between the three classes, with nearly 2\/3rd of the data comprising of no pneumonia (either completely *normal* or *no lung opacity \/ not normal*). Compared to most medical imaging datasets, where the prevalence of disease is quite low, this dataset has been significantly enriched with pathology.","22d539bf":"As we saw above, patient `00436515-870c-4b36-a041-de91049b9ab4` has pnuemonia so lets check our new `parsed` dict here to see the patients corresponding bounding boxes:","ed1c1be5":"As you can see, each row in the CSV file contains a `patientId` (one unique value per patient), a target (either 0 or 1 for absence or presence of pneumonia, respectively) and the corresponding abnormality bounding box defined by the upper-left hand corner (x, y) coordinate and its corresponding width and height. In this particular case, the patient does *not* have pneumonia and so the corresponding bounding box information is set to `NaN`. See an example case with pnuemonia here:","138a9398":"# Next Steps\n\nNow that you understand the data structures, imaging file formats and label types, it's time to make an algorithm! Keep in mind that the primary endpoint is the detection of bounding boxes, thus you will likely be considering various **object localization** algorithms. An alternative strategy is to consider the related family of **segmentation** algorithms with the acknowledgement that bounding boxes will only be a coarse approximation to true pixel-by-pixel image segmentation masks.\n\nFinally, as alluded to several times in this notebook, a radiologist physican may often times provide useful ancillary information, strategy for algorithm development and\/or additional label reconciliation. In addition to physicians you may have access to locally, the RSNA will reach out to radiologists and facilitate engagement remotely through the Kaggle online forums. As a medical professional, I know that many of my colleagues are very interested in getting started so please feel free to reach out and start a conversation! \n\nGood luck!","2d7e0685":"# Overview\n\nWelcome to the 2018 RSNA Challenge co-hosted by Kaggle. In this competition, the primary endpoint will be the detection of bounding boxes corresponding to the diagnosis of pneumonia (e.g. lung infection) on chest radiographs, a special 2D high resolution grayscale medical image. Note that pnuemonia is just one of many possible disease processes that can occur on a chest radiograph, and that any given single image may contain 0, 1 or many boxes corresponding to possible pneumonia locations.\n\nMy name is Peter Chang, MD. I am both a radiologist physician and a data scientist \/ software engineer with machine learning experience. Today, in this Jupyter notebook, we will explore the 2018 RSNA Challenge dataset including underlying data structures, imaging file formats and label types.","23b5caec":"# Visualizing Boxes\n\nIn order to overlay color boxes on the original grayscale DICOM files, consider using the following  methods (below, the main method `draw()` requires the method `overlay_box()`):","dd5cc86d":"# Exploring the Data and Labels\n\nAs alluded to above, any given patient may potentially have many boxes if there are several different suspicious areas of pneumonia. To collapse the current CSV file dataframe into a dictionary with unique entries, consider the following method:","af292030":"## Label Summary\n\nFinally, let us take a closer look at the distribution of labels in the dataset. To do so we will first parse the detailed label information:","69aea503":"While the image displayed inline within the notebook is small, as a radiologist it is evident that the patient has several well circumscribed nodular densities in the left lung (right side of image). In addition there is a large chest tube in the right lung (left side of the image) which has been placed to drain fluid accumulation (e.g. pleural effusion) at the right lung base that also demonstrates overlying patchy densities (e.g. possibly atelectasis or partial lung collapse).\n\nAs you can see, there are a number of abnormalities on the image, and the determination that none of these findings correlate to pneumonia is somewhat subjective even among expert physicians. Therefore, as is almost always the case in medical imaging datasets, the provided ground-truth labels are far from 100% objective. Keep this in mind as you develop your algorithm, and consider consultation with a radiologist physician to help determine an optimal strategy for mitigating these discrepencies.","fa55e54e":"## Exploring Detailed Labels\n\nIn this challenge, the primary endpoint will be the detection of bounding boxes consisting of a binary classification---e.g. the presence or absence of pneumonia. However, in addition to the binary classification, each bounding box *without* pneumonia is further categorized into *normal* or *no lung opacity \/ not normal*. This extra third class indicates that while pneumonia was determined not to be present, there was nonetheless some type of abnormality on the image---and oftentimes this finding may mimic the appearance of true pneumonia. Keep in mind that this extra class is provided as supplemental information to help improve algorithm accuracy if needed; generation of this separate class **will not** be a formal metric used to evaluate performance in this competition.\n\nAs above, we saw that the first patient in the CSV file did not have pneumonia. Let's look at the detailed label information for this patient:","0a397ce8":"One important thing to keep in mind is that a given `patientId` may have **multiple** boxes if more than one area of pneumonia is detected (see below for example images).","c89b8d1a":"As we saw above, patient `00436515-870c-4b36-a041-de91049b9ab4` has pnuemonia so let's take a look at the overlaid bounding boxes:","0d62faa8":"The several key items in this folder:\n* `stage_1_train_labels.csv`: CSV file containing training set patientIds and  labels (including bounding boxes)\n* `stage_1_detailed_class_info.csv`: CSV file containing detailed labels (explored further below)\n* `stage_1_train_images\/`:  directory containing training set raw image (DICOM) files\n\nLet's go ahead and take a look at the first labels CSV file first:","c7395e1e":"Define Model"}}