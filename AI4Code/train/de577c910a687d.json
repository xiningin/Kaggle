{"cell_type":{"64dc216f":"code","33628059":"code","90999c17":"code","765d8ff4":"code","35278f6b":"code","24ffa354":"code","7ac8eb05":"markdown","b65a36e4":"markdown","21d66cf8":"markdown","fb19a381":"markdown","7f60778e":"markdown","b4a08094":"markdown"},"source":{"64dc216f":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score","33628059":"df_train = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\", index_col='id')\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\", index_col='id')\n\nFEATURES = list(df_train.columns[:-1])\nTARGET = df_train.columns[-1]\n\ndf_train.head()","90999c17":"df_train.info()\ndf_train.describe()","765d8ff4":"df_train['n_missing'] = df_train[FEATURES].isna().sum(axis=1)\ntest['n_missing'] = test[FEATURES].isna().sum(axis=1)\n\ndf_train['std'] = df_train[FEATURES].std(axis=1)\ntest['std'] = test[FEATURES].std(axis=1)\n\ndf_train['mean'] = df_train[FEATURES].mean(axis=1)\ntest['mean'] = test[FEATURES].mean(axis=1)\n\nFEATURES += ['n_missing', 'std', 'mean']","35278f6b":"from xgboost import XGBClassifier\n\nX = df_train.loc[:, FEATURES]\ny = df_train.loc[:, TARGET]\n\nfinal_predictions = []\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=X)):\n    X_train = X.loc[train_indicies]\n    X_valid = X.loc[valid_indicies]\n    X_test = test.copy()\n    \n    y_train = y.loc[train_indicies]\n    y_valid = y.loc[valid_indicies]\n    \n    \n    scaler = StandardScaler()\n    X_train[FEATURES] = scaler.fit_transform(X_train[FEATURES])\n    X_valid[FEATURES] = scaler.transform(X_valid[FEATURES])\n    X_test[FEATURES] = scaler.transform(X_test[FEATURES])\n    \n    model = XGBClassifier(\n        max_depth=3,\n        subsample=0.5,\n        colsample_bytree=0.5,\n        learning_rate= 0.01187431306013263,\n        n_estimators= 10000,\n        n_jobs=-1,\n        use_label_encoder=False,\n        objective='binary:logistic',\n        tree_method='gpu_hist',  # Use GPU \n        gpu_id=0,\n        predictor='gpu_predictor',\n    )\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 200)\n    \n    preds_valid = model.predict_proba(X_valid)[:,1]\n    preds_test = model.predict_proba(X_test)[:,1]\n    final_predictions.append(preds_test)\n    print(fold, roc_auc_score(y_valid, preds_valid))","24ffa354":"preds = np.mean(np.column_stack(final_predictions), axis=1)\n\n# Make predictions\ny_pred = pd.Series(\n    preds,\n    index=X_test.index,\n    name=TARGET,\n)\n\n# Create submission file\ny_pred.to_csv(\"submission.csv\")","7ac8eb05":"# Step3: Train Model #\n\nLet's try out a simple XGBoost model. This algorithm can handle missing values, but you could try imputing them instead.  We use `XGBClassifier` (instead of `XGBRegressor`, for instance), since this is a classification problem.","b65a36e4":"# Welcome to the September 2021 Tabular Playground Competition! #\n\nIn this competition, we predict whether a customer will make an insurance claim.\n\n# Step1: Import Helpful Libraries #","21d66cf8":"# Step2: Load Data","fb19a381":"# Make Submission #\n\nOur predictions are binary 0 and 1, but you're allowed to submit probabilities instead. In scikit-learn, you would use the `predict_proba` method instead of `predict`.","7f60778e":"The target `'claim'` has binary outcomes: `0` for no claim and `1` for claim.","b4a08094":"# Missing Values\nRefer to [TPS Sep 2021 single LGBM](https:\/\/www.kaggle.com\/hiro5299834\/tps-sep-2021-single-lgbm\/notebook) by [@hiro5299834](https:\/\/www.kaggle.com\/hiro5299834)"}}