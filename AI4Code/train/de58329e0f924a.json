{"cell_type":{"4ed32685":"code","17d2352b":"code","99f26016":"code","4835a9d7":"code","18146719":"code","1b1e374d":"code","179cffa9":"code","f14238fb":"code","61fdb82d":"code","14d8af17":"code","666c30c1":"code","0b46a94f":"code","f8b823e2":"code","d4bbb51d":"code","0b4c4d31":"code","bc2b0477":"code","7f66316c":"code","56468c32":"code","7ed3dfc3":"code","6eccfc89":"code","5aacc1ab":"code","0014ca26":"code","362478f0":"code","4ed6cc6d":"code","3a8d4256":"code","29b8cc5f":"code","d390f60a":"code","e422c93c":"code","b7cf4ccc":"code","139d61fb":"code","6bff33fa":"code","822e64bb":"code","a6bc03b3":"code","a649e357":"code","e582352e":"code","3340f284":"code","dd31a095":"code","13321e95":"code","b8936755":"code","30f9f595":"code","be4e292f":"code","15d561c6":"code","9bac1588":"code","c892d0d8":"code","90f5acf4":"markdown","3618f2ba":"markdown","71d74fdd":"markdown","47cba174":"markdown","a9d89047":"markdown","3cd3f9ae":"markdown","e3afca57":"markdown","eb01b555":"markdown","8a9e3bf8":"markdown","5b348c41":"markdown","1c4ef13f":"markdown","eb824cdb":"markdown","454dd907":"markdown","8c0dda93":"markdown","e79f27b5":"markdown","fc33c80a":"markdown","08778ebe":"markdown","ed87b997":"markdown","5917cae9":"markdown","10829f27":"markdown","3bf35d1c":"markdown"},"source":{"4ed32685":"# IMPORTING LIBRARIES\n\n# General Libraries\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\n# Preprocessing Libraries\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Machine Learning Libraries\n\nimport sklearn\nimport xgboost as xgb\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_curve\nfrom imblearn.pipeline import Pipeline\nfrom catboost import CatBoostClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.metrics import classification_report\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold, cross_validate\nfrom sklearn.metrics import recall_score, f1_score, roc_auc_score\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n\n\n\n# Defining the working directory\n\ninput_path = '..\/input\/company-bankruptcy-prediction\/'","17d2352b":"# IMPORTING DATA\n\nbank_data = pd.read_csv(input_path + 'data.csv')\nbank_data.head()","99f26016":"bank_data.info()","4835a9d7":"# Computing the descriptive statistics of our numrerical features\n\nbank_data.describe()","18146719":"# Checking Nan presence\n\n#bank_data.isna().sum().max()\n[print(col) for col in bank_data if bank_data[col].isna().sum() > 0]","1b1e374d":"# Checking for duplicates\n\nbank_data.duplicated().sum()","179cffa9":"# The classes are heavily skewed we need to solve this issue later.\n\nprint(bank_data['Bankrupt?'].value_counts())\nprint('-'* 30)\nprint('Financially stable: ', round(bank_data['Bankrupt?'].value_counts()[0]\/len(bank_data) * 100,2), '% of the dataset')\nprint('Financially unstable: ', round(bank_data['Bankrupt?'].value_counts()[1]\/len(bank_data) * 100,2), '% of the dataset')","f14238fb":"# Checking labels distributions\n\nsns.set_theme(context = 'paper')\n\nplt.figure(figsize = (10,5))\nsns.countplot(bank_data['Bankrupt?'])\nplt.title('Class Distributions \\n (0: Fin. Stable || 1: Fin. Unstable)', fontsize=14)\nplt.show()","61fdb82d":"# Looking at the histograms of numerical data\n\nbank_data.hist(figsize = (35,30), bins = 50)\nplt.show()","14d8af17":"# EDA & VISUALIZATIONS\n\n# Correlation Heatmap (Spearman)\n\nf, ax = plt.subplots(figsize=(30, 25))\nmat = bank_data.corr('spearman')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0,# annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","666c30c1":"# Plotting Boxplots of the numerical features\n\nplt.figure(figsize = (20,20))\nax =sns.boxplot(data = bank_data, orient=\"h\")\nax.set_title('Bank Data Boxplots', fontsize = 18)\nax.set(xscale=\"log\")\nplt.show()","0b46a94f":"# Plotting interesting features\n\nf, axes = plt.subplots(ncols=4, figsize=(24,6))\n\nsns.boxplot(x='Bankrupt?', y=\" Net Income to Total Assets\", data=bank_data, ax=axes[0])\naxes[0].set_title('Bankrupt vs Net Income to Total Assets')\n\nsns.boxplot(x='Bankrupt?', y=\" Total debt\/Total net worth\", data=bank_data, ax=axes[1]) \naxes[1].set_title('Bankrupt vs Tot Debt\/Net worth Correlation')\n\n\nsns.boxplot(x='Bankrupt?', y=\" Debt ratio %\", data=bank_data, ax=axes[2])\naxes[2].set_title('Bankrupt vs Debt ratio Correlation')\n\n\nsns.boxplot(x='Bankrupt?', y=\" Net worth\/Assets\", data=bank_data, ax=axes[3])  \naxes[3].set_title('Bankrupt vs Net Worth\/Assets Correlation') \n\nplt.show()","f8b823e2":"# Plotting other interesting features\n\nf, axes = plt.subplots(ncols=4, figsize=(24,6))\n\nsns.boxplot(x='Bankrupt?', y=\" Working Capital to Total Assets\", data=bank_data, ax=axes[0])\naxes[0].set_title('Bankrupt vs  working capital to total assets')\n\nsns.boxplot(x='Bankrupt?', y=\" Cash\/Total Assets\", data=bank_data, ax=axes[1])\naxes[1].set_title('Bankrupt vs cash \/ total assets')\n\n\nsns.boxplot(x='Bankrupt?', y=\" Current Liability to Assets\", data=bank_data, ax=axes[2])\naxes[2].set_title('Bankrupt vs current liability to assets')\n\n\nsns.boxplot(x='Bankrupt?', y=\" Retained Earnings to Total Assets\", data=bank_data, ax=axes[3])\naxes[3].set_title('Bankrupt vs  Retained Earnings to Total Assets')\n\nplt.show()","d4bbb51d":"# Plotting the feature distributions for close to bankrputcy companies\n\nf, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n\ncash_flow_rate = bank_data[' Net Income to Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(cash_flow_rate,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title(' Net Income to Total Assets \\n (Unstable companies)', fontsize=14)\n\ntot_debt_net = bank_data[' Total debt\/Total net worth'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(tot_debt_net ,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('total debt\/tot net worth \\n (Unstable companies)', fontsize=14)\n\n\ndebt_ratio = bank_data[' Debt ratio %'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(debt_ratio,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('debt_ratio \\n (Unstable companies)', fontsize=14)\n\nnet_worth_assets = bank_data[' Net worth\/Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(net_worth_assets,ax=ax4, fit=norm, color='#C5B3F9')\nax4.set_title('net worth\/assets \\n (Unstable companies)', fontsize=14)\n\nplt.show()","0b4c4d31":"f, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n\nworking_cap = bank_data[' Working Capital to Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(working_cap,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('working capitals to total assets \\n (Unstable companies)', fontsize=14)\n\ncash_tot_assets = bank_data[' Cash\/Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(cash_tot_assets ,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('cash\/total assets \\n (Unstable companies)', fontsize=14)\n\n\nasset_liab = bank_data[' Current Liability to Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(asset_liab,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('liability to assets \\n (Unstable companies)', fontsize=14)\n\noperating_funds = bank_data[' Retained Earnings to Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(operating_funds,ax=ax4, fit=norm, color='#C5B3F9')\nax4.set_title('retain earnings to total assets \\n (Unstable companies)', fontsize=14)\n\nplt.show()","bc2b0477":"# Outliers removal\n\ndef outliers_removal(feature,feature_name,dataset):\n    \n    # Identify 25th & 75th quartiles\n\n    q25, q75 = np.percentile(feature, 25), np.percentile(feature, 75)\n    print('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n    feat_iqr = q75 - q25\n    print('iqr: {}'.format(feat_iqr))\n    \n    feat_cut_off = feat_iqr * 1.5\n    feat_lower, feat_upper = q25 - feat_cut_off, q75 + feat_cut_off\n    print('Cut Off: {}'.format(feat_cut_off))\n    print(feature_name +' Lower: {}'.format(feat_lower))\n    print(feature_name +' Upper: {}'.format(feat_upper))\n    \n    outliers = [x for x in feature if x < feat_lower or x > feat_upper]\n    print(feature_name + ' outliers for close to bankruptcy cases: {}'.format(len(outliers)))\n    #print(feature_name + ' outliers:{}'.format(outliers))\n\n    dataset = dataset.drop(dataset[(dataset[feature_name] > feat_upper) | (dataset[feature_name] < feat_lower)].index)\n    print('-' * 65)\n    \n    return dataset\n\nfor col in bank_data:\n    new_df = outliers_removal(bank_data[col],str(col),bank_data)\n    ","7f66316c":"f,(ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24,6))\n\n# Boxplots with outliers removed\n\nsns.boxplot(x=\"Bankrupt?\", y=\" Net Income to Total Assets\", data=new_df,ax=ax1) \nax1.set_title(\"Net Income to Total Assets \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=\" Total debt\/Total net worth\", data=new_df,ax=ax2) \nax2.set_title(\"total debt\/total net worth \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=\" Debt ratio %\", data=new_df,ax=ax3) \nax3.set_title(\"debt ratio % \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=' Net worth\/Assets', data=new_df,ax=ax4) \nax4.set_title(\"net worth\/assets \\n Reduction of outliers\", fontsize=14)\n        \nplt.show()","56468c32":"f,(ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24,6))\n\n# Boxplots with outliers removed\n\nsns.boxplot(x=\"Bankrupt?\", y=' Working Capital to Total Assets', data=new_df,ax=ax1) \nax1.set_title(\"working capital to total assets \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=' Cash\/Total Assets', data=new_df,ax=ax2) \nax2.set_title(\"cash \/ total assets \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=' Current Liability to Assets', data=new_df,ax=ax3) \nax3.set_title(\"current liability to assets \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=' Retained Earnings to Total Assets', data=new_df,ax=ax4) \nax4.set_title(\"Retained Earnings to Total Assets \\n Reduction of outliers\", fontsize=14)\n        \nplt.show()","7ed3dfc3":"# Plotting the feature distributions for close to bankrputcy companies\n\nf, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n\ncash_flow_rate = new_df[' Net Income to Total Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(cash_flow_rate,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title(' Net Income to Total Assets \\n (Unstable companies)', fontsize=14)\n\ntot_debt_net = new_df[' Total debt\/Total net worth'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(tot_debt_net ,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('total debt\/tot net worth \\n (Unstable companies)', fontsize=14)\n\n\ndebt_ratio = new_df[' Debt ratio %'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(debt_ratio,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('debt_ratio \\n (Unstable companies)', fontsize=14)\n\nnet_worth_assets = new_df[' Net worth\/Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(net_worth_assets,ax=ax4, fit=norm, color='#C5B3F9')\nax4.set_title('net worth\/assets \\n (Unstable companies)', fontsize=14)\n\nplt.show()","6eccfc89":"f, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n\nworking_cap = new_df[' Working Capital to Total Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(working_cap,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('working capitals to total assets \\n (Unstable companies)', fontsize=14)\n\ncash_tot_assets = new_df[' Cash\/Total Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(cash_tot_assets ,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('cash\/total assets \\n (Unstable companies)', fontsize=14)\n\n\nasset_liab = new_df[' Current Liability to Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(asset_liab,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('liability to assets \\n (Unstable companies)', fontsize=14)\n\noperating_funds = new_df[' Retained Earnings to Total Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(operating_funds,ax=ax4, fit=norm, color='#C5B3F9')\nax4.set_title('retain earnings to total assets \\n (Unstable companies)', fontsize=14)\n\nplt.show()","5aacc1ab":"# Dividing Data and Labels\n\nlabels = new_df['Bankrupt?']\nnew_df = new_df.drop(['Bankrupt?'], axis = 1)","0014ca26":"def log_trans(data):\n    \n    for col in data:\n        skew = data[col].skew()\n        if skew > 0.5 or skew < -0.5:\n            data[col] = np.log1p(data[col])\n        else:\n            continue\n            \n    return data\n\ndata_norm = log_trans(new_df)","362478f0":"# Plotting Boxplots of the preprocessed numerical features\n\nplt.figure(figsize = (20,20))\nax =sns.boxplot(data = data_norm, orient=\"h\")\nax.set_title('Bank Data Preprocessed Boxplots', fontsize = 18)\nax.set(xscale=\"log\")\nplt.show()","4ed6cc6d":"data_norm.hist(figsize = (35,30),bins = 50)\nplt.show()","3a8d4256":"# Splitting Train and Test Data\n\nX_raw,X_test,y_raw,y_test  = train_test_split(data_norm,\n                                              labels,\n                                              test_size=0.1,\n                                              stratify = labels,\n                                              random_state = 42)","29b8cc5f":"# Stratified Cross Validation Splitting\n\nsss = StratifiedKFold(n_splits=5, random_state=42, shuffle=False)\n\nfor train_index, test_index in sss.split(X_raw,y_raw):\n    \n    print(\"Train:\", train_index, \"Test:\", test_index)\n    X_train_sm, X_val_sm = X_raw.iloc[train_index], X_raw.iloc[test_index]\n    y_train_sm, y_val_sm = y_raw.iloc[train_index], y_raw.iloc[test_index]\n\n# Check the Distribution of the labels\n\n\n# Turn into an array\nX_train_sm = X_train_sm.values\nX_val_sm = X_val_sm.values\ny_train_sm = y_train_sm.values\ny_val_sm = y_val_sm.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(y_train_sm, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(y_val_sm, return_counts=True)\nprint('-' * 84)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(y_train_sm))\nprint(test_counts_label\/ len(y_val_sm))","d390f60a":"# List to append the score and then find the average\n\naccuracy_lst_reg = []\nprecision_lst_reg = []\nrecall_lst_reg = []\nf1_lst_reg = []\nauc_lst_reg = []\n\nlog_reg_sm = LogisticRegression()\n#log_reg_params = {}\nlog_reg_params = {\"penalty\": ['l2'],\n                  'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n                  'class_weight': ['balanced',None],\n                  'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n\nfor train, val in sss.split(X_train_sm, y_train_sm):\n    pipeline_reg = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n    model_reg = pipeline_reg.fit(X_train_sm[train], y_train_sm[train])\n    best_est_reg = rand_log_reg.best_estimator_\n    prediction_reg = best_est_reg.predict(X_train_sm[val])\n    \n    accuracy_lst_reg.append(pipeline_reg.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_reg.append(precision_score(y_train_sm[val], prediction_reg))\n    recall_lst_reg.append(recall_score(y_train_sm[val], prediction_reg))\n    f1_lst_reg.append(f1_score(y_train_sm[val], prediction_reg))\n    auc_lst_reg.append(roc_auc_score(y_train_sm[val], prediction_reg))\n\n\nprint('---' * 45)\nprint('')\nprint('Logistic Regression (SMOTE) results:')\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_reg)))\nprint(\"precision: {}\".format(np.mean(precision_lst_reg)))\nprint(\"recall: {}\".format(np.mean(recall_lst_reg)))\nprint(\"f1: {}\".format(np.mean(f1_lst_reg)))\nprint('')\nprint('---' * 45)","e422c93c":"# Printing the classification report\n\nlabel = ['Fin.Stable', 'Fin.Unstable']\npred_reg_sm = best_est_reg.predict(X_val_sm)\nprint(classification_report(y_val_sm, pred_reg_sm, target_names=label))","b7cf4ccc":"# Plotting Precision-Recall curve\n\ny_score_reg = best_est_reg.predict(X_val_sm)\n\naverage_precision = average_precision_score(y_val_sm, y_score_reg)\n\nfig = plt.figure(figsize=(12,6))\n\nprecision, recall, _ = precision_recall_curve(y_val_sm, y_score_reg)\n\nplt.step(recall, precision, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision), fontsize=15)\n\nplt.show()","139d61fb":"# List to append the score and then find the average\n\naccuracy_lst_rfc = []\nprecision_lst_rfc = []\nrecall_lst_rfc = []\nf1_lst_rfc = []\nauc_lst_rfc = []\n\nrfc_sm = RandomForestClassifier()\n#rfc_params = {}\nrfc_params = {'max_features' : ['auto', 'sqrt', 'log2'],\n              'random_state' : [42],\n              'class_weight' : ['balanced','balanced_subsample'],\n              'criterion' : ['gini', 'entropy'],\n              'bootstrap' : [True,False]}\n    \n    \nrand_rfc = RandomizedSearchCV(rfc_sm, rfc_params, n_iter=4)\n\n\nfor train, val in sss.split(X_train_sm, y_train_sm):\n    pipeline_rfc = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_rfc) # SMOTE happens during Cross Validation not before..\n    model_rfc = pipeline_rfc.fit(X_train_sm, y_train_sm)\n    best_est_rfc = rand_rfc.best_estimator_\n    prediction_rfc = best_est_rfc.predict(X_train_sm[val])\n    \n    accuracy_lst_rfc.append(pipeline_rfc.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_rfc.append(precision_score(y_train_sm[val], prediction_rfc))\n    recall_lst_rfc.append(recall_score(y_train_sm[val], prediction_rfc))\n    f1_lst_rfc.append(f1_score(y_train_sm[val], prediction_rfc))\n    auc_lst_rfc.append(roc_auc_score(y_train_sm[val], prediction_rfc))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_rfc)))\nprint(\"precision: {}\".format(np.mean(precision_lst_rfc)))\nprint(\"recall: {}\".format(np.mean(recall_lst_rfc)))\nprint(\"f1: {}\".format(np.mean(f1_lst_rfc)))\nprint('---' * 45)","6bff33fa":"smote_prediction_rfc = best_est_rfc.predict(X_val_sm)\nprint(classification_report(y_val_sm, smote_prediction_rfc, target_names=label))","822e64bb":"# Plotting Precision-Recall curve\n\ny_score_rfc = best_est_rfc.predict(X_val_sm)\n\naverage_precision_rfc = average_precision_score(y_val_sm, y_score_rfc)\n\nfig = plt.figure(figsize=(12,6))\n\nprecision_rfc, recall_rfc, _ = precision_recall_curve(y_val_sm, y_score_rfc)\n\nplt.step(recall_rfc, precision_rfc, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall_rfc, precision_rfc, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision_rfc), fontsize=15)\n\nplt.show()","a6bc03b3":"# List to append the score and then find the average\n\naccuracy_lst_xgb = []\nprecision_lst_xgb = []\nrecall_lst_xgb = []\nf1_lst_xgb = []\nauc_lst_xgb = []\n\nxgb_sm = xgb.XGBClassifier(random_state = 42)\nxgb_params = {'eta' : [0.1,0.01,0.001],  # Learning rate\n              'eval_metric': ['logloss'],\n              'max_depth' : [3,6,9],\n              'lambda' : [1,1.5,2],      # L2 regularization (higher values make model more conservative)\n              'alpha' : [0,0.5,1]}        # L1 regularization (higher values make model more conservative)\n              #'reg' : ['squarederror']}\n              #'random_state': [42]}\n        \nrand_xgb = RandomizedSearchCV(xgb_sm, xgb_params, n_iter=4)\n\n\nfor train, val in sss.split(X_train_sm, y_train_sm):\n    pipeline_xgb = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_xgb) # SMOTE happens during Cross Validation not before..\n    model_xgb = pipeline_xgb.fit(X_train_sm, y_train_sm)\n    best_est_xgb = rand_xgb.best_estimator_\n    prediction_xgb = best_est_xgb.predict(X_train_sm[val])\n    \n    accuracy_lst_xgb.append(pipeline_xgb.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_xgb.append(precision_score(y_train_sm[val], prediction_xgb))\n    recall_lst_xgb.append(recall_score(y_train_sm[val], prediction_xgb))\n    f1_lst_xgb.append(f1_score(y_train_sm[val], prediction_xgb))\n    auc_lst_xgb.append(roc_auc_score(y_train_sm[val], prediction_xgb))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_xgb)))\nprint(\"precision: {}\".format(np.mean(precision_lst_xgb)))\nprint(\"recall: {}\".format(np.mean(recall_lst_xgb)))\nprint(\"f1: {}\".format(np.mean(f1_lst_xgb)))\nprint('---' * 45)","a649e357":"# Printing classification report\n\nsmote_prediction_xgb = best_est_xgb.predict(X_val_sm)\nprint(classification_report(y_val_sm, smote_prediction_xgb, target_names=label))","e582352e":"# Plotting Precision-Recall curve\n\ny_score_xgb = best_est_xgb.predict(X_val_sm)\n\naverage_precision_xgb = average_precision_score(y_val_sm, y_score_xgb)\n\nfig = plt.figure(figsize=(12,6))\n\nprecision_xgb, recall_xgb, _ = precision_recall_curve(y_val_sm, y_score_xgb)\n\nplt.step(recall_xgb, precision_xgb, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall_xgb, precision_xgb, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision_xgb), fontsize=15)\n\nplt.show()","3340f284":"# List to append the score and then find the average\n\naccuracy_lst_cat = []\nprecision_lst_cat = []\nrecall_lst_cat = []\nf1_lst_cat = []\nauc_lst_cat = []\n\ncat_sm = CatBoostClassifier(verbose = 0)\n\ncat_params = {'eval_metric': ['F1'],\n              'iterations': [100,500,1000],\n              'learning_rate' : [0.1,0.01,0.001],\n              'random_seed' : [42],\n              'auto_class_weights' : ['Balanced','SqrtBalanced']\n             }\n    \n    \nrand_cat = RandomizedSearchCV(cat_sm, cat_params, n_iter=4)\n\n\nfor train, val in sss.split(X_train_sm, y_train_sm):\n    pipeline_cat = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_cat) # SMOTE happens during Cross Validation not before..\n    model_cat = pipeline_cat.fit(X_train_sm, y_train_sm)\n    best_est_cat = rand_cat.best_estimator_\n    prediction_cat = best_est_cat.predict(X_train_sm[val])\n    \n    accuracy_lst_cat.append(pipeline_cat.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_cat.append(precision_score(y_train_sm[val], prediction_cat))\n    recall_lst_cat.append(recall_score(y_train_sm[val], prediction_cat))\n    f1_lst_cat.append(f1_score(y_train_sm[val], prediction_cat))\n    auc_lst_cat.append(roc_auc_score(y_train_sm[val], prediction_cat))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_cat)))\nprint(\"precision: {}\".format(np.mean(precision_lst_cat)))\nprint(\"recall: {}\".format(np.mean(recall_lst_cat)))\nprint(\"f1: {}\".format(np.mean(f1_lst_cat)))\nprint('---' * 45)","dd31a095":"smote_prediction_cat = best_est_cat.predict(X_val_sm)\nprint(classification_report(y_val_sm, smote_prediction_cat, target_names=label))","13321e95":"# Plotting Precision-Recall curve\n\nsmote_prediction_cat = best_est_cat.predict(X_val_sm)\n\naverage_precision_cat = average_precision_score(y_val_sm, smote_prediction_cat)\n\nfig = plt.figure(figsize=(12,6))\n\nprecision_cat, recall_cat, _ = precision_recall_curve(y_val_sm, smote_prediction_cat)\n\nplt.step(recall_cat, precision_cat, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall_cat, precision_cat, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision_cat), fontsize=15)\n\nplt.show()","b8936755":"from sklearn.metrics import roc_curve\n\nlog_fpr, log_tpr, log_thresold = roc_curve(y_val_sm, pred_reg_sm)\nrfc_fpr, rfc_tpr, rfc_threshold = roc_curve(y_val_sm, smote_prediction_rfc)\nxgb_fpr, xgb_tpr, xgb_thresold = roc_curve(y_val_sm, smote_prediction_xgb)\ncat_fpr, cat_tpr, cat_thresold = roc_curve(y_val_sm, smote_prediction_cat)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, rfc_fpr, rfc_tpr,xgb_fpr, xgb_tpr,cat_fpr, cat_tpr):\n    plt.figure(figsize=(20,8))\n    plt.title('ROC Curve', fontsize=14)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, pred_reg_sm)))\n    plt.plot(xgb_fpr, xgb_tpr, label='XGBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, smote_prediction_xgb)))\n    plt.plot(cat_fpr, cat_tpr, label='CatBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, smote_prediction_cat)))\n    plt.plot(rfc_fpr, rfc_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, smote_prediction_rfc)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=13)\n    plt.ylabel('True Positive Rate', fontsize=13)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, rfc_fpr, rfc_tpr,xgb_fpr, xgb_tpr,cat_fpr, cat_tpr)\nplt.show()","30f9f595":"# Plotting confusion matrix for each classifier\n\nconf_mx0 = confusion_matrix(y_val_sm,pred_reg_sm)\nconf_mx1 = confusion_matrix(y_val_sm,smote_prediction_rfc)\nconf_mx2 = confusion_matrix(y_val_sm,smote_prediction_xgb)\nconf_mx3 = confusion_matrix(y_val_sm,smote_prediction_cat)\n\nheat_cm0 = pd.DataFrame(conf_mx0, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\nheat_cm0.index.name = 'Actual'\nheat_cm0.columns.name = 'Predicted'\n\nheat_cm1 = pd.DataFrame(conf_mx1, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\nheat_cm1.index.name = 'Actual'\nheat_cm1.columns.name = 'Predicted'\n\nheat_cm2 = pd.DataFrame(conf_mx2, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\nheat_cm2.index.name = 'Actual'\nheat_cm2.columns.name = 'Predicted'\n\nheat_cm3 = pd.DataFrame(conf_mx3, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\nheat_cm3.index.name = 'Actual'\nheat_cm3.columns.name = 'Predicted'\n\nf, ax = plt.subplots(1, 4, figsize=(20,8))\nf.subplots_adjust(left=None, bottom=None, right= 2, top=None, wspace=None, hspace= None)\n\nsns.heatmap(heat_cm0, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[0])\nax[0].set_title('Logistic Regression', fontsize = 20)\nsns.heatmap(heat_cm1, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[1])\nax[1].set_title('Random Forest Classifier', fontsize = 20)\nsns.heatmap(heat_cm2, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[2])\nax[2].set_title('XGBoost Classifier', fontsize = 20)\nsns.heatmap(heat_cm3, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[3])\nax[3].set_title('Catboot Classifier', fontsize = 20)\n\nplt.show()","be4e292f":"# Testing\n\ntest_pred_lr = best_est_reg.predict(X_test)\n#test_pred_rf = best_est_rfc.predict(X_test)\n#test_pred_xgb = best_est_xgb.predict(X_test) \ntest_pred_cat = best_est_cat.predict(X_test)","15d561c6":"# Plotting confusion matrix for each classifier\n\nconf_mx0 = confusion_matrix(y_test,test_pred_lr)\nconf_mx1 = confusion_matrix(y_test,test_pred_cat)\n\nheat_cm0 = pd.DataFrame(conf_mx0, columns=np.unique(y_test), index = np.unique(y_test))\nheat_cm0.index.name = 'Actual'\nheat_cm0.columns.name = 'Predicted'\n\nheat_cm1 = pd.DataFrame(conf_mx1, columns=np.unique(y_test), index = np.unique(y_test))\nheat_cm1.index.name = 'Actual'\nheat_cm1.columns.name = 'Predicted'\n\nf, ax = plt.subplots(1, 2, figsize=(15,8))\nf.subplots_adjust(left=None, bottom=None, right= 2, top=None, wspace=None, hspace= None)\n\nsns.heatmap(heat_cm0, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[0])\nax[0].set_title('Logistic Regression', fontsize = 20)\nsns.heatmap(heat_cm1, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[1])\nax[1].set_title('Catboot Classifier', fontsize = 20)\n\nplt.show()","9bac1588":"print(classification_report(y_test, test_pred_lr, target_names=label))","c892d0d8":"print(classification_report(y_test, test_pred_cat, target_names=label))","90f5acf4":"## RESULTS","3618f2ba":"As you can see from the results on validation set, all of our models still have problems in detecting the companies close to bankruptcy. The algorithm that recognizes more observations of the minority class is Logistic Regression, even though this comes with a great cost in term of precision (high presence of false negatives). Despite the presence of the errors, in this case I think that it is better to identify a non close to bankruptcy observation as close to bankrouptcy than vice versa, so it could be a useful model.","71d74fdd":"## MODELING\n\nNow, let's see what we can do with our models! We will see the performances on the cleaned undersampled data and on the SMOTE oversampled data. For this part I decided to use a few different models:\n\n* Logistic Regression\n* SVC \n* Random Forest Classifier\n* CatBoost Classifier\n\n![Immagine.png](attachment:Immagine.png)","47cba174":"A further consideration needs to be done on the possibility to have duplicates in our data. Duplicates are identical observations that can create redundancy in our data and need to be dropped. Let's check the presence of duplicates to remove:","a9d89047":"## SMOTE\n\nLet's see if using SMOTE as oversampling technique can help us improving the performance. Although it is likely that SMOTE will be more accurate than random under-sampling, it will take more time to train since no rows are eliminated as previously stated.","3cd3f9ae":"**Outliers removal**\n\nIn this part of the project will try to remove the most extreme outliers (note that you can also impute them with the mean or the median instead of removing them). This should increase our models' performance.","e3afca57":"## LOGISTIC REGRESSION ","eb01b555":"## XGBOOST","8a9e3bf8":"Looking at the plot above we can clearly see how our labels are strongly unbalanced, and this is a the main obstacle that we need to solve to obtain good performances.","5b348c41":"## CATBOOST","1c4ef13f":"Now that we have an idea of our data, we need to obtain more information possible on them. The first thing that we want to understand is the nature of our data, namely if the data are numerical or categorical and if we have missing information among them. It is possible to check both these points using the *.info()* pandas method.","eb824cdb":"Now, let's look at the distribution of these features for companies that are close to bankruptcy:","454dd907":"Despite the fact that we already know that there are no missing values (here it's pretty easy considering that we have just 96 features) it is important to computationally check that this is true, to avoid errors and time wasted in the following steps of the project.","8c0dda93":"Considering the project task, and now that we have a general overview of our data, we need focus our attention on the labels: which are the financially stable and unstable companies? Let's take a look:","e79f27b5":"Now, let's look at the cleaned boxplots:","fc33c80a":"**Exploratory Data Analysis (EDA).** \n\nLet's explore our dataset to gain as much information as possible from several types of visualizations. A first general plot to show is the correlation heatmap that plots all the correlations (linear and non, depending on the the selected test: Pearson or Spearman) among numerical variables of the dataset. Knowing the correlations among features is an important step in deciding which features to maintain during training.","08778ebe":"## RANDOM FOREST CLASSIFIER","ed87b997":"## Company Bankruptcy \ud83d\udcc9\ud83d\udcb8\n\n![image.png](attachment:image.png)\n\nData from the Taiwan Economic Journal for the years 1999\u20132009 representing company bankruptcy based on the business regulations of the Taiwan Stock Exchange.\n\n**INTRODUCTION**\n\nIn this kernel we will use various predictive models to see how accurate they are in detecting whether we can correctly predict which comapnies will face bankrptcy in the future. As described in the data section, the dataset contains:\n\n- 95 features (X1-X95, business regulations of Taiwan Stock Exchange)\n- 1 Vector of labels\n\nOur aim in this project is to use these features to understand their impact\/role on the selected models and how they can help us recognizing the companies that are close to bankrupcty. Let's start!\n\nP.s: If you like this notebook don't forget to **UPVOTE**!\n\n","5917cae9":"What we can see is that removing the extreme outliers definitely helps to obtain more \"bell shape\" distributions!(At least for the features showed)","10829f27":"The resulting panel is strongly informative for us, and it shows how:\n\n- The dataset is composed of a combination of 6819 observations per each of our 96 features.\n- All of the features are numerical (int64 or float64)\n- There are no missing values (Nan) among the data\n\nConsidering that all our features are numeric, we can easily calculate their descriptive statistics: a futher source of information.","3bf35d1c":"Exactly as shown using validation data we can see how the metric considered (F1) is higher using catboost. Nevertheless, in this case, the best decision is to use Logistic regression because it can better recognize the minority class even misclassifying some not close to bankruptcy companies as close to bankruptcy. \n\n**Thank you for your support and see you with the next notebook!**"}}