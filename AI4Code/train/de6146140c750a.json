{"cell_type":{"31119076":"code","e3ec0fe3":"code","0accb1d6":"code","9d5cc303":"code","9ab8c2b9":"code","4c7537e8":"code","f4dacab0":"code","e98c05bb":"code","f5d469a7":"code","38c48116":"code","873a2885":"code","4a31e907":"code","9c527dba":"code","3cea5177":"code","9d86ef1a":"code","245836e7":"markdown","394599b1":"markdown","bd3aade0":"markdown","741e0d39":"markdown","3022dfc6":"markdown","cfa0b543":"markdown"},"source":{"31119076":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport timeit\nfrom pandas.core.frame import DataFrame\nfrom itertools import combinations","e3ec0fe3":"features = [1,2,3,4,5,6,7,8]\nbatch_size = 1\nepochs = 600\nlearn_rate = 0.00003\nhl = 2                          # Numbers of hidden layers\nnodes = [10,10]                 # All Hidden Layer have same number of nodes\noutput_layer = 1                # Output node","0accb1d6":"def feature_select(data:DataFrame, index:list):\n    new = data.iloc[:,index]\n    if 'Orientation' in new.columns:\n        oh_orient = pd.get_dummies(new['Orientation'].round().apply(str), prefix='Orient')  # One-Hot Orientation\n        new = new.drop('Orientation',axis=1)                  # Remove Old\n        new = new.join(oh_orient)                             # Join\n    if 'Glazing Area Distribution' in new.columns:\n        oh_glaze_dist = pd.get_dummies(new['Glazing Area Distribution'].round().apply(str), prefix='G. Distr.') # One-Hot Distribution\n        new = new.drop('Glazing Area Distribution',axis=1)    # Remove Distribution\n        new = new.join(oh_glaze_dist)                         # Join\n    new = normalize(new)                                      # Normalize \n    #print(new.head())\n    return new\n\ndef normalize(x:DataFrame):                           # Max-min normalize\n    m  = (x-x.min())\/(x.max()-x.min())\n    return m","9d5cc303":"def f_select(xtest, xtrain, features):\n    xtest = feature_select(xtest, features)\n    xtrain = feature_select(xtrain, features)\n    xtest = np.array(xtest)\n    xtrain = np.array(xtrain)\n    return xtest, xtrain","9ab8c2b9":"def init_model(x, hl, node, p, batch):\n    w = {}\n    b = {}\n    if len(node) < hl:\n        node = np.tile(node,hl)\n    n = np.hstack((x,node,p)).ravel()\n    for i in range(0,(len(n)-1)):\n        w[i] = np.random.rand(n[i+1],n[i])          # row = output ; column = input\n        b[i] = np.random.rand(n[i+1],batch)         # Row = nodes ; column = batch = 1\n    return w, b","4c7537e8":"def relu(x):\n    return np.maximum(0,x)\n\ndef sig(x):\n    return 1.0\/(1.0 + np.exp(-x))                     # Sigmoid Function\n\ndef relu_deriv(x):\n    return x>0\n\ndef sig_deriv(x):\n    return x*(1.0-x)                                # X --> Result of sigmoid function","f4dacab0":"def forward_prop(x, w, b):\n    z = {}\n    a = {}\n    #a[0] = np.matrix(x).T\n    a[0] = np.array(x).T\n    last = len(w)\n    for i in range(1,(last+1)):\n        z[i] = w[i-1].dot(a[i-1]) + b[i-1]\n        if i == last:\n            a[i] = z[i]\n        else:\n            a[i] = relu(z[i])\n    pred = a[last]\n    #print(pred)\n    return pred, a     # A is output of layers , last A is the output, Z is before the activation function\n\ndef back_prop(y, p, a, w, b):\n    d = {}\n    d_w = {}\n    d_b = {}\n    last = len(w)\n    d[last] = -2*(y-p)           #1x1\n    #print(d[last])\n    for i in range(last-1, -1, -1):\n        if i == last-1:\n            d[i] = w[i].T.dot(d[i+1])      # 10 x 1 --> d2\n        elif i != 0:\n            #print(type(w[i].T.dot(d[i+1])))\n            #print((w[i].T.dot(d[i+1])).shape)\n            #print(type(sig_deriv(a[i])))\n            #print((sig_deriv(a[i])).shape)\n            d[i] = (w[i].T.dot(d[i+1])) * relu_deriv(a[i])\n            #print(d[i])\n        d_w[i] = d[i+1].dot(a[i].T)        # dw2 \n        d_b[i] = d[i+1]\n    return d_w, d_b\n\ndef update(w, b, d_w, d_b, a):\n    #print(len(w))\n    for i in range (0,len(w)):\n        w[i] = w[i] - a * d_w[i]\n        b[i] = b[i] - a * d_b[i]\n    return w, b\n\ndef sum_error(y, p):\n    return (y.item() - p.item())**2\n\ndef cal_erms(e, n):\n    return np.sqrt((1\/n)*e)","e98c05bb":"def train(x, y, w, b, epoch, learn_rate, batch):\n    p_e = np.zeros(epoch)             # Cost for each epoch\n    p = []                            # prediction for training data (last epoch only)\n    for i in range(0, epoch):         # loop epoch\n        e = 0                         # sum of error\n        for k in range(0, len(x)):    # Going through the samples\n            pred, layer = forward_prop(x[k:k+batch], w, b)\n            w_grad, b_grad = back_prop(y[k:k+batch], pred, layer, w, b)\n            w, b = update(w, b, w_grad, b_grad, learn_rate)\n            e = e + sum_error(y[k:k+batch], pred)\n            if i == len(x)-1:         # Save prediction of last epoch\n                p.append(pred.item())\n        p_e[i] = e \/ len(x)           # Cost for i epoch\n    return p, p_e, w, b","f5d469a7":"def predict(x, y, w, b, batch):\n    er = 0\n    s = len(x)\n    p = np.zeros(s)\n    p_e = np.zeros(s)\n    for i in range(0,s):\n        pred, c = forward_prop(x[i:i+batch], w, b)\n        er = er + sum_error(y[i:i+batch], pred)\n        p[i] = pred\n        #print(pred)\n        #p_e[i] = sum_error(y[i:i+batch], pred)\n    erms = cal_erms(er, len(x))\n    return p, erms","38c48116":"df = pd.read_csv('..\/input\/energy-data\/energy_efficiency_data.csv')\nx_test = df.sample(frac=0.25).reset_index()     # Sample 25%\nx_train = df.drop(x_test.index).reset_index()    # Put the rest for training\ny_train = x_train['Heating Load']               # Train Heat Output\ny_test = x_test['Heating Load']                 # Test Heat Output\ny_train_cool = x_train['Cooling Load']          # Train Cool Output\ny_test_cool = x_test['Cooling Load']            # Test Cool Output\nx_test = x_test.drop(labels=['Heating Load','Cooling Load'], axis=1)\nx_train = x_train.drop(labels=['Heating Load','Cooling Load'], axis=1)\ny_test = np.array(y_test)\ny_train = np.array(y_train)","873a2885":"xtest, xtrain = f_select(x_test, x_train, features)\n\nm, n = xtrain.shape                    # m = number of data, n = features\n\nweight, bias = init_model(n, hl, nodes, output_layer, batch_size)\n\npred_train, cost, weight_N, bias_N = train(xtrain, y_train, weight, bias, epochs, learn_rate, batch_size)\n\nerms_train = cal_erms(np.sum(cost), epochs)","4a31e907":"fig, ax = plt.subplots(figsize=(8,8))\nax.plot(cost)\nax.set_xlabel('Epoch')\nax.set_ylabel('Erms')\nax.set_title('Learning Curve')","9c527dba":"p_test, erms = predict(xtest, y_test, weight_N, bias_N, batch_size)\nprint('ERMS Training = %f' %erms_train)\nprint('ERMS Test = %f' %erms)","3cea5177":"fig, ax = plt.subplots(2,figsize=(20,10))\nfig.suptitle('Results')\nax[0].plot(pred_train)\nax[0].plot(y_train)\nax[1].plot(p_test)\nax[1].plot(y_test)","9d86ef1a":"c7 = [i for i in combinations(features, 7)]\nfor i in c7:\n    f = list(i)\n    print('Feature : ',f)\n    xtest, xtrain = f_select(x_test, x_train, f)\n    m, n = xtrain.shape                    # m = number of data, n = features\n    weight, bias = init_model(n, hl, nodes, output_layer, batch_size)\n    p_train, cost, weight_N, bias_N = train(xtrain, y_train, weight, bias, epochs, learn_rate, batch_size)\n    erms_train = cal_erms(np.sum(cost), epochs)\n    print('ERMS Training : ', erms_train)\n    p_test, er = predict(xtest, y_test, weight_N, bias_N, batch_size)\n    print('ERMS Test : ',er)\n    print('==============================')","245836e7":"# **Feature**","394599b1":"# **Start Training**","bd3aade0":"#  **Load data + Preprocess ( Split data, Normalize, etc)**","741e0d39":"# **Prediction**","3022dfc6":"# **Constants**","cfa0b543":"# **NN Regression**"}}