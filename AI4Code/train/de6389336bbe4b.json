{"cell_type":{"a1c7256c":"code","9d7b51b7":"code","63e9d5d0":"code","e8921b0c":"code","327d1efc":"code","8e7cfede":"code","8106a2fb":"code","47ab2548":"code","c736f93b":"code","2510f874":"code","aa13e4a8":"code","8ab348ba":"code","2a461240":"code","234462fa":"code","a898cd2c":"code","3b3a2451":"code","a61cef04":"code","4c90009c":"code","a01fb4e2":"code","0c693447":"code","dca93f91":"code","7a345aa8":"markdown","0abebf90":"markdown","39c8799b":"markdown","5cd573a0":"markdown","557ddb6e":"markdown","c1903ac6":"markdown","08af0371":"markdown","4fcd6d70":"markdown"},"source":{"a1c7256c":"from datetime import date, datetime, timedelta\nimport numpy as np\nimport pandas as pd\n\nconfirmed = pd.read_csv(\"..\/input\/jhucovid19\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_confirmed_global.csv\")\ndeaths   = pd.read_csv(\"..\/input\/jhucovid19\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_deaths_global.csv\")","9d7b51b7":"launch_date = date(2020, 3, 26)\nlatest_train_date = date(2020, 3, 25)\n\npublic_leaderboard_start_date = launch_date - timedelta(7)\nclose_date = launch_date + timedelta(7)\nfinal_evaluation_start_date = launch_date + timedelta(8)","63e9d5d0":"confirmed.columns = list(confirmed.columns[:4]) + [datetime.strptime(d, \"%m\/%d\/%y\").date().strftime(\"%Y-%m-%d\") for d in confirmed.columns[4:]]\ndeaths.columns    = list(deaths.columns[:4])    + [datetime.strptime(d, \"%m\/%d\/%y\").date().strftime(\"%Y-%m-%d\") for d in deaths.columns[4:]]","e8921b0c":"# Filter out problematic data points (The West Bank and Gaza had a negative value, cruise ships were associated with Canada, etc.)\nremoved_states = \"Recovered|Grand Princess|Diamond Princess\"\nremoved_countries = \"US|The West Bank and Gaza\"\n\nconfirmed.rename(columns={\"Province\/State\": \"Province_State\", \"Country\/Region\": \"Country_Region\"}, inplace=True)\ndeaths.rename(columns={\"Province\/State\": \"Province_State\", \"Country\/Region\": \"Country_Region\"}, inplace=True)\nconfirmed = confirmed[~confirmed[\"Province_State\"].replace(np.nan, \"nan\").str.match(removed_states)]\ndeaths    = deaths[~deaths[\"Province_State\"].replace(np.nan, \"nan\").str.match(removed_states)]\nconfirmed = confirmed[~confirmed[\"Country_Region\"].replace(np.nan, \"nan\").str.match(removed_countries)]\ndeaths    = deaths[~deaths[\"Country_Region\"].replace(np.nan, \"nan\").str.match(removed_countries)]\n\nconfirmed.drop(columns=[\"Lat\", \"Long\"], inplace=True)\ndeaths.drop(columns=[\"Lat\", \"Long\"], inplace=True)","327d1efc":"confirmed","8e7cfede":"deaths","8106a2fb":"us_keys = pd.read_csv(\"..\/input\/jhucovid19\/csse_covid_19_data\/csse_covid_19_daily_reports\/03-25-2020.csv\")\nus_keys = us_keys[us_keys[\"Country_Region\"]==\"US\"]\nus_keys = us_keys.groupby([\"Province_State\", \"Country_Region\"])[[\"Confirmed\", \"Deaths\"]].sum().reset_index()\n\nus_keys = us_keys[~us_keys.Province_State.str.match(\"Diamond Princess|Grand Princess|Recovered|Northern Mariana Islands|American Samoa\")].reset_index(drop=True)\nus_keys","47ab2548":"confirmed = confirmed.append(us_keys[[\"Province_State\", \"Country_Region\"]], sort=False).reset_index(drop=True)\ndeaths = deaths.append(us_keys[[\"Province_State\", \"Country_Region\"]], sort=False).reset_index(drop=True)","c736f93b":"for col in confirmed.columns[2:]:\n    confirmed[col].fillna(0, inplace=True)\n    deaths[col].fillna(0, inplace=True)","2510f874":"confirmed","aa13e4a8":"us_start_date = date(2020, 3, 10)\nday_date = us_start_date\n\nwhile day_date <= latest_train_date:\n    day = pd.read_csv(\"..\/input\/jhucovid19\/csse_covid_19_data\/csse_covid_19_daily_reports\/%s.csv\" % day_date.strftime(\"%m-%d-%Y\"))\n    \n    if \"Country\/Region\" in day.columns:\n        day.rename(columns={\"Country\/Region\": \"Country_Region\", \"Province\/State\": \"Province_State\"}, inplace=True)\n    \n    us = day[day[\"Country_Region\"]==\"US\"]\n    us = us.groupby([\"Province_State\", \"Country_Region\"])[[\"Confirmed\", \"Deaths\"]].sum().reset_index()\n    \n    unused_data = []\n    untouched_states = set(confirmed[confirmed[\"Country_Region\"]==\"US\"][\"Province_State\"])\n    \n    for (i, row) in us.iterrows():\n        if confirmed[(confirmed[\"Country_Region\"]==\"US\") & (confirmed[\"Province_State\"]==row[\"Province_State\"])].shape[0]==1:\n            confirmed.loc[(confirmed[\"Country_Region\"]==\"US\") & (confirmed[\"Province_State\"]==row[\"Province_State\"]), day_date.strftime(\"%Y-%m-%d\")] = row[\"Confirmed\"]\n            deaths.loc[(deaths[\"Country_Region\"]==\"US\") & (deaths[\"Province_State\"]==row[\"Province_State\"]), day_date.strftime(\"%Y-%m-%d\")] = row[\"Deaths\"]\n            untouched_states.remove(row[\"Province_State\"])\n        else:\n            unused_data.append(row[\"Province_State\"])\n            \n    print(day_date, \"Untouched\", untouched_states)\n    print(day_date, \"Unused\", unused_data)\n\n    day_date = day_date + timedelta(1)","8ab348ba":"confirmed","2a461240":"deaths","234462fa":"dates_on_after_launch = [col for col in confirmed.columns[4:] if col>=launch_date.strftime(\"%Y-%m-%d\")]\nprint(\"Removing %d columns: %s\" % (len(dates_on_after_launch), str(dates_on_after_launch)))\n\ncols_to_keep = [col for col in confirmed.columns if col not in dates_on_after_launch]\n\nconfirmed = confirmed[cols_to_keep]\ndeaths = deaths[cols_to_keep]","a898cd2c":"for i in range(36):\n    this_date = (launch_date + timedelta(i)).strftime(\"%Y-%m-%d\")\n    confirmed.insert(len(confirmed.columns), this_date, np.NaN)\n    deaths.insert(len(deaths.columns), this_date, np.NaN)","3b3a2451":"confirmed_melted = confirmed.melt(confirmed.columns[:2], confirmed.columns[2:], \"Date\", \"ConfirmedCases\")\n#confirmed_melted.insert(5, \"Type\", \"Confirmed\")\ndeaths_melted = deaths.melt(deaths.columns[:2], deaths.columns[2:], \"Date\", \"Fatalities\")\n#deaths_melted.insert(5, \"Type\", \"Deaths\")\n\nconfirmed_melted.sort_values(by=[\"Country_Region\", \"Province_State\", \"Date\"], inplace=True)\ndeaths_melted.sort_values(by=[\"Country_Region\", \"Province_State\", \"Date\"], inplace=True)\n\nassert confirmed_melted.shape==deaths_melted.shape\nassert list(confirmed_melted[\"Province_State\"])==list(deaths_melted[\"Province_State\"])\nassert list(confirmed_melted[\"Country_Region\"])==list(deaths_melted[\"Country_Region\"])\nassert list(confirmed_melted[\"Date\"])==list(deaths_melted[\"Date\"])\n\ncases = confirmed_melted.merge(deaths_melted, on=[\"Province_State\", \"Country_Region\", \"Date\"], how=\"inner\")\ncases = cases[[\"Country_Region\", \"Province_State\", \"Date\", \"ConfirmedCases\", \"Fatalities\"]]\n\ncases.sort_values(by=[\"Country_Region\", \"Province_State\", \"Date\"], inplace=True)\ncases.insert(0, \"Id\", range(1, cases.shape[0]+1))\ncases","a61cef04":"forecast = cases[cases[\"Date\"]>=public_leaderboard_start_date.strftime(\"%Y-%m-%d\")]\nforecast.drop(columns=\"Id\", inplace=True)\nforecast.insert(0, \"ForecastId\", range(1, forecast.shape[0]+1))\nforecast.insert(6, \"Usage\", \"Ignored\")\nforecast.loc[forecast[\"Date\"]<launch_date.strftime(\"%Y-%m-%d\"),\"Usage\"]=\"Public\"\nforecast.loc[forecast[\"Date\"]>=final_evaluation_start_date.strftime(\"%Y-%m-%d\"),\"Usage\"]=\"Private\"\nforecast","4c90009c":"train = cases[cases[\"Date\"]<launch_date.strftime(\"%Y-%m-%d\")]\ntrain.to_csv(\"train.csv\", index=False)\ntrain","a01fb4e2":"test = forecast[forecast.columns[:-3]]\ntest.to_csv(\"test.csv\", index=False)\ntest","0c693447":"solution = forecast[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\", \"Usage\"]]\nsolution[\"ConfirmedCases\"].fillna(1, inplace=True)\nsolution[\"Fatalities\"].fillna(1, inplace=True)\nsolution.to_csv(\"solution.csv\", index=False)\nsolution","dca93f91":"submission = forecast[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\nsubmission[\"ConfirmedCases\"] = 1\nsubmission[\"Fatalities\"] = 1\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission","7a345aa8":"## Global competition data","0abebf90":"# COVID-19 Forecasting Challenge (Week 2) Data Prep\n\nThis notebook prepared the data in Kaggle's [COVID-19 Global Forecasting Competition (Week 2)](https:\/\/www.kaggle.com\/c\/covid19-global-forecasting-week-2) that was used to launch the competition. The source data comes from [JHU CSSE's COVID-19 data repository on GitHub](https:\/\/github.com\/CSSEGISandData\/COVID-19).\n\nI re-ran this notebook on updated data to add descriptive comments, so it won't output precisely the same as the original launch data. I saved the original launch data [to this dataset](https:\/\/www.kaggle.com\/benhamner\/covid19-forecasting-week-two-launch-data).\n\nThe data for the submission period for the forecasting challenges is also updated every day, alongside leaderboard rescores. I use [this notebook](https:\/\/www.kaggle.com\/benhamner\/covid-19-forecasting-ongoing-data-updates\/) to run the ongoing data updates.","39c8799b":"Adding the rows to be forecast","5cd573a0":"Melting the data to a version that will be friendlier to Kaggle's evaluation system.","557ddb6e":"Filtering out any data on or after the launch date for the competition","c1903ac6":"Move to ISO 8601 dates","08af0371":"Starting to pull in US state data, since this was saved separately","4fcd6d70":"Adding in daily US state data"}}