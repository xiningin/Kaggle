{"cell_type":{"11cff84a":"code","58fff6e7":"code","c5cc3ac3":"code","0e56950a":"code","126ffbde":"code","3f8b68c5":"code","1f392d37":"code","9c4a80bd":"code","5e4bfebf":"code","88c6e48b":"code","ad486a14":"code","fa1d43da":"code","4bb137f7":"code","69136726":"code","4af4ebb9":"code","3c786b19":"code","0e7c9509":"code","599d5c90":"code","4853cf21":"code","a45ee5e2":"code","eab8e58f":"code","5e94df9e":"code","fabcce7b":"code","bc948b3d":"code","936a8c42":"code","9cc5eb76":"code","858da262":"code","c4715a2c":"code","bf24e0e4":"code","abc92e9c":"code","0587ee9d":"code","76f11a9b":"code","682167dc":"code","d55f3194":"code","b72c35d8":"code","6099ce4f":"code","abd75d1d":"code","1176ff62":"code","b474046a":"code","f27d9bf1":"code","c586a9d5":"markdown","7e129058":"markdown","b201bfdb":"markdown","6d52c913":"markdown","b4b4e734":"markdown","84916a20":"markdown","8d7e9dc3":"markdown","84e92ce1":"markdown","bf1bd316":"markdown","2eb6493c":"markdown","90d8c29f":"markdown","a5ca4377":"markdown","50ed9083":"markdown","3e6e6eee":"markdown","5243af33":"markdown","2dc5336b":"markdown","16ee2323":"markdown"},"source":{"11cff84a":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport copy\nimport random\nimport time\nfrom PIL import Image\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nimport torchvision.transforms as transforms\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset, random_split","58fff6e7":"# Device configuration\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using {} device\".format(device))","c5cc3ac3":"!pip install torchsummary","0e56950a":"N_EPOCHS = 20\nBATCH_SIZE = 128\n# IMG_HEIGHT = 96\n# IMG_WIDTH = 96\nSEED = 1234\n\n## For normalization required by pretrained models in torchvision.models\nMEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]","126ffbde":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n#     torch.backends.cudnn.deterministic = True\n\nseed_everything(SEED)","3f8b68c5":"train_dir = '..\/input\/histopathologic-cancer-detection\/train'\ntest_dir = '..\/input\/histopathologic-cancer-detection\/test'\ntrain_labels = pd.read_csv('..\/input\/histopathologic-cancer-detection\/train_labels.csv')\nsubmission = pd.read_csv('..\/input\/histopathologic-cancer-detection\/sample_submission.csv')","1f392d37":"train_size = len(os.listdir(train_dir))\ntest_size = len(os.listdir(test_dir))\n\nprint(train_size,test_size)\nprint(train_labels.shape)\nprint(submission.shape)","9c4a80bd":"display(train_labels.head())\ndisplay(submission.head())","5e4bfebf":"train_labels['label'].value_counts()","88c6e48b":"plt.figure(figsize=(13, 6))\ntrain_labels['label'].value_counts().plot(kind='bar')\nplt.show()","ad486a14":"# get the ids of cancer cases\ncancer = train_labels.loc[train_labels['label']==1]['id'].to_numpy() \n\n# get the ids of the normal cases\nnormal = train_labels.loc[train_labels['label']==0]['id'].to_numpy()","fa1d43da":"plt.rcParams['figure.figsize'] = (15, 6)","4bb137f7":"# Visualize cancer cases\n\nnrows, ncols= 6, 15\nplt.subplots_adjust(wspace=0, hspace=0) \n\nfor i, image_id in enumerate(cancer[: nrows * ncols]):\n    img_path = os.path.join(train_dir , image_id +'.tif')\n    img = Image.open(img_path)\n    idcol = ImageDraw.Draw(img)\n    idcol.rectangle(((0,0),(95,95)),outline='red')\n    plt.subplot(nrows, ncols, i+1) \n    plt.imshow(np.array(img))\n    plt.axis('off')","69136726":"# Visualize Normal Cases\n\nnrows, ncols= 6, 15\nplt.subplots_adjust(wspace=0, hspace=0) \n\nfor i, image_id in enumerate(normal[: nrows * ncols]):\n    img_path = os.path.join(train_dir , image_id +'.tif')\n    img = Image.open(img_path)\n    idcol = ImageDraw.Draw(img)\n    idcol.rectangle(((0,0),(95,95)),outline='green')\n    plt.subplot(nrows, ncols, i+1) \n    plt.imshow(np.array(img))\n    plt.axis('off')","4af4ebb9":"# class CustomDataset(Dataset):\n    \n#     def __init__(self, labels, img_dir, transform=None, target_transform=None):\n#         self.img_labels = labels\n#         self.img_dir = img_dir\n#         self.transform = transform\n#         self.target_transform = target_transform\n\n#     def __len__(self):\n#         return len(self.img_labels)\n\n#     def __getitem__(self, idx):\n#         img_name, label = self.img_labels.iloc[idx]\n#         img_path = os.path.join(self.img_dir, img_name + '.tif')\n#         image =  Image.open(img_path)\n\n#         if self.transform is not None:\n#             image = self.transform(image)\n#         if self.target_transform is not None:\n#             label = self.target_transform(label)\n            \n#         return image, label","3c786b19":"class CustomDataset(Dataset):\n    \n    def __init__(self, labels, img_dir, dataset_type = 'train', transform=None, target_transform=None):\n        self.img_labels = labels\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n        self.dataset_type = dataset_type\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_id = self.img_labels.loc[idx, 'id']\n        img_path = os.path.join(self.img_dir, img_id + '.tif')\n        image =  Image.open(img_path)\n\n        if self.transform is not None:\n            image = self.transform(image)\n            \n        # For Test Dataset, we don't have class label.\n        # So for Test Dataset, we will return the image 'id' as label\n        label = 0\n        \n        if self.dataset_type == 'train':\n            label = self.img_labels.loc[idx, 'label']\n            \n            if self.target_transform is not None:\n                label = self.target_transform(label)\n        \n        elif self.dataset_type == 'test':\n            label = img_id\n            \n        return image, label","0e7c9509":"# Define transformation that converts a PIL image into PyTorch tensors.\nimport torchvision.transforms as transforms\ndata_transformer = transforms.Compose([transforms.ToTensor()])","599d5c90":"# Define an instance of the custom dataset for the train folder.\ndataset = CustomDataset(train_labels, train_dir, transform = data_transformer)\n\nimg, label = dataset[24]\nprint(img.shape, torch.min(img), torch.max(img))\nprint(label)","4853cf21":"dataset_size = len(dataset)\ntrain_size = int(0.8 * dataset_size)\nvalid_size = dataset_size - train_size\n\n# Split Pytorch tensor\ntrain_dataset, valid_dataset = random_split(dataset, [train_size, valid_size]) # random split 80\/20\n\nprint(\"train dataset size:\", len(train_dataset))\nprint(\"validation dataset size:\", len(valid_dataset))\n","a45ee5e2":"# Define the following transformations for the training dataset\ntrain_transformer = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5), \n    transforms.RandomVerticalFlip(p=0.5),  \n    transforms.RandomRotation(20),         \n#     transforms.RandomResizedCrop(96, scale=(0.8,1.0),ratio=(1.0,1.0)),\n    transforms.ToTensor()]\n)","eab8e58f":"# For the validation dataset, we don't need any augmentation; simply convert images into tensors\nvalid_transformer = transforms.Compose([\n    transforms.ToTensor()]\n)","5e94df9e":"# After defining the transformations, overwrite the transform functions of train_ts, val_ts\ntrain_dataset.transform = train_transformer\nvalid_dataset.transform = valid_transformer","fabcce7b":"# Define two dataloaders for the datasets\n\ntrain_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle=False)","bc948b3d":"for batch, (X, y) in enumerate(train_dataloader):\n    print(\"Batch : {}\".format(batch))\n    print(X.shape)\n    print(y.shape)\n    \n    if batch == 4:\n        break","936a8c42":"# model = models.densenet169(pretrained=True)\n# model","9cc5eb76":"class BasicCNN(nn.Module):\n    \n    def __init__(self):\n        super(BasicCNN, self).__init__()\n        \n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2)\n        )\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2)\n        )\n        \n        self.layer3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2)\n        )\n        \n        self.layer4 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2)\n        )\n        \n        self.layer5 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2)\n        )\n        \n        self.avg_pool = nn.AvgPool2d(kernel_size = 3)\n        self.dropout1 = nn.Dropout(0.7)\n        self.fc1 = nn.Linear(1*1*512, 64)\n        self.relu = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, 2)\n        \n        \n    def forward(self,x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n#         print(x.shape)\n        x = self.avg_pool(x)\n#         print(x.shape)\n        x = x.view(-1, 1 * 1 * 512)\n#         print(x.shape)\n        x = self.dropout1(x)\n        x = self.relu(self.fc1(x))\n        x = self.dropout2(x)\n        x = self.fc2(x)\n#         x = F.softmax(x)\n        return x","858da262":"# If we need to move a model to GPU via .cuda(), \n# we must do it before constructing optimizers for it. \n# Parameters of a model after .cuda() will be different objects with those before the call.\n\n# In general, we should make sure that optimized parameters live in consistent locations \n# when optimizers are constructed and used.\n\nmodel = BasicCNN().to(device)\nmodel","c4715a2c":"from torchsummary import summary\nsummary(model, input_size=(3,96,96))","bf24e0e4":"LEARNING_RATE = 0.001\noptimizer = torch.optim.Adam(params = model.parameters(),lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss(reduction='mean')","abc92e9c":"def train(dataloader, model, loss_fn, optimizer):\n    ds_size = len(dataloader.dataset)\n    num_batch = len(dataloader)\n    \n    total_accurate_preds = 0.0\n    total_loss = 0.0\n    \n    model.train()\n    for batch_idx, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # Compute prediction error\n        logits = model(X)\n        loss = loss_fn(logits, y) # average loss per batch \n        preds = logits.argmax(dim=1)\n        num_accurate_preds = (preds == y).float().sum() # number of correct predictions for a single batch\n        \n        total_loss += loss.item()\n        total_accurate_preds += num_accurate_preds.item()\n    \n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch_idx % 100 == 0:\n            current = batch_idx * len(X)\n            print(f\"loss: {loss.item():>7f}  [{current}\/{ds_size}]\")\n\n    accuracy = total_accurate_preds\/ ds_size\n    avg_loss = total_loss\/ num_batch\n\n    return  accuracy, avg_loss\n\n# train(train_dataloader, model, criterion, optimizer)","0587ee9d":"# model.eval() will notify all your layers that we are in eval mode, that way, \n# batchnorm or dropout layers will work in eval mode instead of training mode.\n\n# torch.no_grad() impacts the autograd engine and deactivate it. \n# It will reduce memory usage and speed up computations \n# but we won\u2019t be able to backprop (which we don\u2019t want in an eval mode).\n\ndef evaluate(dataloader, model, loss_fn):\n    ds_size = len(dataloader.dataset)\n    num_batch = len(dataloader)\n    \n    total_accurate_preds = 0\n    total_loss = 0.0\n    \n    model.eval()\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            \n            # Compute prediction error\n            logits = model(X)\n            loss = loss_fn(logits, y) # average loss per batch \n            preds = logits.argmax(dim=1)\n            num_accurate_preds = (preds == y).float().sum() # number of correct predictions for a single batch\n    \n            total_loss += loss.item()\n            total_accurate_preds += num_accurate_preds.item()\n    \n    accuracy = total_accurate_preds\/ ds_size\n    avg_loss = total_loss\/ num_batch\n\n    #     print(f\"Validation Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {avg_loss:>8f} \\n\")\n    \n    return  accuracy, avg_loss\n\n# evaluate(valid_dataloader, model, criterion)","76f11a9b":"train_history = {\"loss\": [],\"accuracy\": []} # history of accuracy and loss for train set in each epoch\nvalidation_history = {\"loss\": [],\"accuracy\": []} # history of accuracy and loss for validation set in each epoch\nbest_model_weights = copy.deepcopy(model.state_dict()) # a deep copy of weights for the best performing model\nbest_loss = float('inf') # initialize best loss to a large value\n\nfor epoch in range(N_EPOCHS):\n    print(f\"Epoch {epoch+1}\/{N_EPOCHS}\\n-------------------------------\")\n    \n    start = time.time()\n    train_accuracy, train_loss = train(train_dataloader, model, criterion, optimizer)\n    train_history['loss'].append(train_loss)\n    train_history['accuracy'].append(train_accuracy)\n    print('\\nTrain accuracy : {}, Train loss : {}'.format(train_accuracy, train_loss))\n#     print(train_history)\n    \n    validation_accuracy, validation_loss = evaluate(valid_dataloader, model, criterion)\n    validation_history['loss'].append(validation_loss)\n    validation_history['accuracy'].append(validation_accuracy)\n    print('Validation accuracy : {}, Validation loss : {}'.format(validation_accuracy,validation_loss))\n#     print(validation_history)\n    \n    print(\"Execution time for an Epoch : {}\\n\".format(time.time() - start))\n    \n    # store best model\n    if validation_loss < best_loss:\n        best_loss = validation_loss\n        best_model_weights = copy.deepcopy(model.state_dict())\n\n        ## store weights into a local file\n        # torch.save(model.state_dict(), weight_path)\n    \nprint(\"Done!\")","682167dc":"print(train_history)\nprint(validation_history)","d55f3194":"epoch_range = list(range(N_EPOCHS)) \n\nplt.clf()\nplt.plot(epoch_range,train_history['loss'] , label=\"Train\")\nplt.plot(epoch_range, validation_history['loss'], label=\"Validation\")\nplt.xlabel(\"#epoch\")\nplt.ylabel('loss')\nplt.title('Training Loss and Validation Loss')\nplt.legend()\nplt.show()\n\nplt.clf()\nplt.plot(epoch_range, train_history['accuracy'], label=\"Train\")\nplt.plot(epoch_range, validation_history['accuracy'], label=\"Validation\")\nplt.xlabel(\"#epoch\")\nplt.ylabel('Accuracy')\nplt.title('Training Accuracy and Validation Accuracy')\nplt.legend()\nplt.show()","b72c35d8":"weight_save_path = '.\/model.pt'\ntorch.save(best_model_weights, weight_save_path)\nprint(\"Saved PyTorch Best Model State to model.pt\")","6099ce4f":"# Define transformation that converts a PIL image into PyTorch tensors.\ntest_transformer = transforms.Compose([transforms.ToTensor()])\n\n# Define an instance of the custom dataset for the train folder.\ntest_dataset = CustomDataset(submission, test_dir, dataset_type = 'test', transform = test_transformer)\n\nimg, img_id = test_dataset[24]\nprint(img.shape, torch.min(img), torch.max(img))\nprint(img_id)","abd75d1d":"# Define a Dataloader for test dataset\ntest_dataloader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False)","1176ff62":"# configure model with best model weights \n\n# model = BasicCNN().to(device)\n# model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n\nmodel.load_state_dict(best_model_weights)\n# print(model.state_dict())","b474046a":"test_ds_size = len(test_dataloader.dataset)\ncancer_probs = []\ntest_image_ids = []\n\nmodel.eval()\nwith torch.no_grad():\n    for batch_idx, (images, image_ids) in enumerate(test_dataloader):\n        images = images.to(device)\n        logits = model(images)\n        preds = F.softmax(logits, dim=1)[:, 1]\n        preds = torch.flatten(preds).detach().cpu().numpy()\n        cancer_probs += list(preds)\n        test_image_ids += list(image_ids)\n        \n        if batch_idx % 100 == 0:\n            current = batch_idx * len(images)\n            print(f\"Prediction Done:  [{current}\/{test_ds_size}]\")    ","f27d9bf1":"submission = pd.DataFrame({'id':test_image_ids, 'label':cancer_probs})\n\nsubmission.to_csv('submission.csv',index=False)","c586a9d5":"## Splitting Dataset into Training and Validation set","7e129058":"## Custom Dataset","b201bfdb":"### Basic CNN","6d52c913":"## Apply Transformations to Train and Validation Set","b4b4e734":"### DenseNet169\n\nAll pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]","84916a20":"## Make Predictions for Test Images","8d7e9dc3":"## Import Libraries and Setup ","84e92ce1":"## Load Data","bf1bd316":"## Define a Model","2eb6493c":"## Configure for Reproducibility","90d8c29f":"## Plot Accuracy and Losses","a5ca4377":"## Define DataLoader","50ed9083":"## Optimizer and Loss Function\n\nWe are using nn.CrossEntropyLoss as criterion and torch.optim.Adam as optimizer. \n\nnn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss. We pass our model\u2019s output logits to nn.CrossEntropyLoss, which will normalize the logits and compute the prediction error.","3e6e6eee":"## Visualize the Data","5243af33":"## GLobal Variables","2dc5336b":"## Optimizing the Model Parameters","16ee2323":"## Saving model\n\nA common PyTorch convention is to save models using either a .pt or .pth file extension.\n\nIt is however not [recommended to use .pth extension](https:\/\/stackoverflow.com\/questions\/59095824\/what-is-the-difference-between-pt-pth-and-pwf-extentions-in-pytorch) when checkpointing models because it collides with Python path (.pth) configuration files."}}