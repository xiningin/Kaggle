{"cell_type":{"caa95a6c":"code","57056f26":"code","7298dbe6":"code","1a2fc5ac":"code","2d6e4ad8":"code","396026f1":"code","f50ae188":"code","5f63a9c1":"code","0b4aa6f5":"code","b0005e0a":"code","4d3243e0":"code","cb91e61e":"code","b0820b2a":"code","e835451e":"code","0d98ca67":"code","92b4468d":"code","4a15fe2f":"code","3d181e5c":"code","18ad8c20":"code","ed1a3911":"code","ea7c7aa8":"code","5ebacc39":"code","f6f88d84":"code","164ebd21":"code","f38664d1":"code","efade283":"markdown","5c65566d":"markdown","bf6c8101":"markdown","24e3d570":"markdown","9c66a252":"markdown","658f7ffc":"markdown","61d9ea91":"markdown","c489598f":"markdown","dc429dd1":"markdown","a973d32b":"markdown","a8ee7cd1":"markdown"},"source":{"caa95a6c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn import metrics\n\nfrom joblib import dump, load\n\n%matplotlib inline","57056f26":"#Create dataframe from a Real estate price prediction dataset.\n\ndf = pd.read_csv('..\/input\/real-estate-price-prediction\/Real estate.csv')\n","7298dbe6":"df.head()","1a2fc5ac":"df.info()","2d6e4ad8":"df.shape","396026f1":"df.describe","f50ae188":"g= sns.pairplot(df)\ng.map_upper(plt.scatter)","5f63a9c1":"# find the pairwise correlation of all columns in the dataframe.\n\ndf.corr()","0b4aa6f5":"#Heatmap for correlation\nsns.heatmap(df.corr(), annot=True,cmap='winter')","b0005e0a":"plt.figure(figsize=(10,4))\nsns.displot(df['Y house price of unit area'],kde=True,bins=20, aspect=2)\nplt.xlabel('house price of unit area')","4d3243e0":"plt.figure(figsize=(8, 8), dpi=50)\n\nsns.rugplot(df['Y house price of unit area'], height=0.2)\n","cb91e61e":"plt.figure(figsize=(5, 5), dpi=100)\n\nsns.scatterplot(data=df, y=df['Y house price of unit area'], x=df['X1 transaction date'] , hue= 'X2 house age', palette=\"rocket\")\n","b0820b2a":"plt.figure(figsize=(5, 5), dpi=100)\n\nsns.scatterplot(data=df, y=df['Y house price of unit area'], x=df['X3 distance to the nearest MRT station'] , hue= 'X4 number of convenience stores', palette=\"rocket\")","e835451e":"plt.figure(figsize=(5, 5), dpi=100)\n\nsns.scatterplot(data=df, y=df['Y house price of unit area'], x=df['X5 latitude'] , hue= 'X6 longitude', palette=\"rocket\")","0d98ca67":"X = df.drop('Y house price of unit area',axis=1)\ny = df['Y house price of unit area']","92b4468d":"from sklearn.preprocessing import PolynomialFeatures\nPF=PolynomialFeatures(degree=2, include_bias=False)\npoly_features=PF.fit_transform(X)","4a15fe2f":"poly_features.shape","3d181e5c":"# train out model on the training set and then use the test set to evaluate the model.\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    poly_features, y, test_size=0.3, random_state=101)\n","18ad8c20":"from sklearn.linear_model import LinearRegression\npolymodel=LinearRegression()\npolymodel.fit(X_train, y_train)","ed1a3911":"y_pred=polymodel.predict(X_test)\n\n\n","ea7c7aa8":"pd.DataFrame({'Y_Test': y_test,'Y_Pred':y_pred, 'Residuals':(y_test-y_pred) }).head(5)\n","5ebacc39":"MAE_Poly = metrics.mean_absolute_error(y_test,y_pred)\nMSE_Poly = metrics.mean_squared_error(y_test,y_pred)\nRMSE_Poly = np.sqrt(MSE_Poly)\n\npd.DataFrame([MAE_Poly, MSE_Poly, RMSE_Poly],\n             index=['MAE', 'MSE', 'RMSE'], columns=['metrics'])","f6f88d84":"XS_train, XS_test, ys_train, ys_test = train_test_split(X, y, test_size=0.3, random_state=101)\nsimplemodel=LinearRegression()\nsimplemodel.fit(XS_train, ys_train)\nys_pred=simplemodel.predict(XS_test)\n\nMAE_simple = metrics.mean_absolute_error(ys_test,ys_pred)\nMSE_simple = metrics.mean_squared_error(ys_test,ys_pred)\nRMSE_simple = np.sqrt(MSE_simple)","164ebd21":"pd.DataFrame({'Poly Metrics': [MAE_Poly, MSE_Poly, RMSE_Poly], 'Simple Metrics':[MAE_simple, MSE_simple,\n                                                                                 RMSE_simple]}, index=['MAE', 'MSE', 'RMSE'])\n","f38664d1":"train_RMSE_list=[]\ntest_RMSE_list=[]\n\nfor d in range(1,10):\n    \n    polynomial_converter= PolynomialFeatures(degree=d, include_bias=False)\n    poly_features= polynomial_converter.fit_transform(X)\n    \n    X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)\n    \n    polymodel=LinearRegression()\n    polymodel.fit(X_train, y_train)\n    \n    y_train_pred=polymodel.predict(X_train)\n    y_test_pred=polymodel.predict(X_test)\n    \n    train_RMSE=np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n    \n    test_RMSE=np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n        \n    train_RMSE_list.append(train_RMSE)\n    test_RMSE_list.append(test_RMSE)\nplt.plot(range(1,6), train_RMSE_list[:5], label='Train RMSE')\nplt.plot(range(1,6), test_RMSE_list[:5], label='Test RMSE')\n\nplt.xlabel('Polynomial Degree')\n\nplt.legend(loc=(1.1, 0.5))\n","efade283":"<h3>Test Data Prediction<\/h3>","5c65566d":"<h3>3-EDA<\/h3>","bf6c8101":"<h3>Import libraries and dataset.<\/h3>\n","24e3d570":"<p>First <b>split<\/b> up the data into an X array that contains the <b>features<\/b> to train on, and a y array with the <b>target<\/b> variable, in this case the (Y house price of unit area) column.<p>\n","9c66a252":"<b>Compare Linear regrassion Vs  Polynomial Regression<\/b>","658f7ffc":"<h1> <p style=\" font-family: \"Times New Roman\", Times, serif;\">Introduction<\/p><\/h1>\n\n<b>Polynomial Regression<\/b>\n\n<p>One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.\n\nFor example,<a href=\"https:\/\/www.kaggle.com\/mahyamahjoob\/real-estate-valuation-using-linear-regression\">a simple linear regression<\/a> can be extended by constructing polynomial features from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:\n<H2>y^(w,x)=w0+w1x1+w2x2<\/H2>\nIf we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:\n\n<H2>y^(w,x)=w0+w1x1+w2x2+w3x1x2+w4x21+w5x22<\/H2>\n \nThe (sometimes surprising) observation is that this is still a linear model: to see this, imagine creating a new variable\n\n<H2>z=[x1,x2,x1x2,x21,x22]<\/H2>\n \nWith this re-labeling of the data, our problem can be written:\n    \n<H2> y^(w,x)=w0+w1z1+w2z2+w3z3+w4z4+w5z5 <\/H2>\n \nWe see that the resulting polynomial regression is in the same class of linear models we\u2019d considered above (i.e. the model is linear in w) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data.<\/p>\n<p>\n<h4><b>Source:<\/b><a href=\"https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions\">scikit-learn<\/a>\n<\/h4><\/p>\n<h2>Dataset<\/h2>\nThe data set is Real estate price prediction that is used for regression analysis, mutiple regression,linear regression, prediction. Since house price is a continues variable, this is a regression problem. The data contains 8columns that include sixFeatures(X) and one Label(y): house price of unit area.","61d9ea91":"<h3>2-check out the data<\/h3>","c489598f":"<h3> Adjust model parameters<\/h3>","dc429dd1":"<h3>Model Evalution<\/h3>","a973d32b":"<p><b>Split<\/b> a data into <b>train<\/b> and <b>test<\/b><\/p>\n","a8ee7cd1":"<h3>Training a Polynomial Regression Model<\/h3>"}}