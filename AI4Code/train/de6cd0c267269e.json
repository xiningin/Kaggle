{"cell_type":{"ab6d58eb":"code","2a0333eb":"code","1f051195":"code","e3c131dc":"code","a16279db":"code","f59f0f8f":"code","b92f68e6":"code","e9f21136":"code","253b113c":"code","e4766161":"code","f734bb9e":"code","2688bcf5":"code","584bd518":"code","f8c943c2":"code","36171c75":"code","d4442f7d":"code","7184118b":"code","6347d143":"code","c6752701":"code","ee5abe10":"code","4d744896":"code","cc77668b":"code","7e993582":"code","06ff15ca":"code","6448148b":"code","27c3b904":"code","6ca04a48":"code","d2d55b18":"code","9bc732b2":"code","2f95b075":"code","8cd71de4":"markdown","2c90db5a":"markdown","3c9e8bb6":"markdown","bcf279c5":"markdown","787c28fe":"markdown","7bf74c38":"markdown","0e494173":"markdown","42984ccb":"markdown","e234017a":"markdown","620b155e":"markdown","7fc7dc39":"markdown","f5d819c2":"markdown"},"source":{"ab6d58eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2a0333eb":"import warnings\nimport graphviz\nimport pandas as pd\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import  RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score, recall_score, precision_score\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nfrom scipy.stats import kurtosis, skew\nfrom sklearn.tree import export_graphviz\nfrom mpl_toolkits.mplot3d import Axes3D","1f051195":"df = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")","e3c131dc":"df.head(10)","a16279db":"df.describe()","f59f0f8f":"df.columns","b92f68e6":"df.nunique()","e9f21136":"df.isnull()","253b113c":"df.isnull().sum()","e4766161":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    \n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","f734bb9e":"def desc_stats(dataframe):\n    desc = dataframe.describe().T\n    desc_df = pd.DataFrame(index= [col for col in dataframe.columns], \n                           columns= desc.columns,\n                           data= desc)\n    \n    f,ax = plt.subplots(figsize=(10,\n                                 desc_df.shape[0]*0.78))\n    sns.heatmap(desc_df,\n                annot=True,\n                cmap = \"Blues\",\n                fmt= '.2f',\n                ax=ax,\n                linewidths = 2.6,\n                cbar = False,\n                annot_kws={\"size\": 14})\n    plt.xticks(size = 18)\n    plt.yticks(size = 14,\n               rotation = 0)\n    plt.title(\"Analysis\", size = 16)\n    plt.show()\n    \ndesc_stats(df[num_cols])","2688bcf5":"df1 = df.drop([\"Outcome\"],axis=1)\ncorelation = df1.corr()","584bd518":"sns.heatmap(corelation, xticklabels=corelation.columns,yticklabels=corelation.columns,annot=True)","f8c943c2":"sns.pairplot(data=df,hue='Outcome')","36171c75":"x = df.iloc[:,0:8].values # all rows and first 8 columns are input variables\ny = df.iloc[:,8:].values # all rows and last column will be considered as output","d4442f7d":"import warnings\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(1, figsize=(18, 13.5))\nax = Axes3D(fig, elev=48, azim=134)\nax.scatter(x[:, 5], x[:, 6], x[:, 7], c=y,\n           cmap=plt.cm.Set1, edgecolor='k', s = x[:, 3]*50)\n\n\nax.set_title(\"3D visualization\", fontsize=40)\nax.set_xlabel(\"BMI\", fontsize=25)\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"DiabetesPedigreeFunction\", fontsize=25)\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"Age\", fontsize=25)\nax.w_zaxis.set_ticklabels([])\n\nplt.show()","7184118b":"for i in df.columns[:8]:\n  sns.displot(x = i,hue=\"Outcome\",data=df)","6347d143":"warnings.filterwarnings('ignore')\n\ncolors = ['#F06E5C','#5CF0C5','#F1F689','#F0B35C','#6EDEDE',\n          '#BB81D5','#D3D972','#EB83EE','#8B8CED','#F09674',\n          '#BCE374','#72C3E7','#C3A0EB','#E6809D','#A9BFCB']\n\nsns.palplot(sns.color_palette(colors))\nplt.title('Colors', size = 15)\nplt.axis('off')\nplt.show()","c6752701":"def num_summary(dataframe, col_name):\n    fig = make_subplots(rows=1,cols=2,\n                        subplot_titles=('Quantiles','Distribution'))\n\n    fig.add_trace(go.Box(y=dataframe[col_name],\n                         name = str(col_name),\n                         showlegend = False,\n                         marker_color = colors[5]),\n                  row = 1, col = 1)\n    \n    fig.add_trace(go.Histogram(x = dataframe[col_name],\n                               xbins = dict(start = 0,end = dataframe[col_name].max()),\n                               showlegend = False,\n                               name = str(col_name),\n                               marker=dict(color=colors[1],\n                                           line=dict(color='black',\n                                                     width=2))),\n                  row = 1, col = 2)\n    \n    fig.update_layout(title={'text': str(col_name),\n                             'y':0.9,\n                             'x':0.5,\n                             'xanchor': 'center',\n                             'yanchor': 'top'},\n                      template='ggplot2')\n    \n    iplot(fig)\n\nfor i in num_cols:\n  num_summary(df,i)\n    \n","ee5abe10":"df.groupby(['Outcome']).agg({'Age':['mean','median'],\n                             'DiabetesPedigreeFunction' : ['mean','median'],\n                            'Glucose':['mean','median'],\n                            'Pregnancies':['mean','median'],\n                            'BMI':['mean','median'],\n                            'Insulin':['mean','median'],\n                            'SkinThickness':['mean','median'],\n                            'BloodPressure':['mean','median']})\n","4d744896":"x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.20,random_state=1)","cc77668b":"accuracy_scores = []#accuracy scores\n# Will take some time\nfrom sklearn import metrics\nfor k in range(1,576):\n    neighbour = KNeighborsClassifier(n_neighbors = k, metric='euclidean').fit(x_train,y_train.ravel())\n    y_pred_trial = neighbour.predict(x_test)\n    accuracy_scores.append(metrics.accuracy_score(y_test, y_pred_trial))\n\nplt.figure(figsize=(15,6))\nplt.plot(range(1,576),accuracy_scores,color = 'blue',linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\nplt.title('accuracy vs. K Value')\nplt.xlabel('K value')\nplt.ylabel('Accuracy')\nk=accuracy_scores.index(max(accuracy_scores))+1\nprint(\"-----------------------------------------------\")\nprint(\"Maximum accuracy:-\",max(accuracy_scores),\"at K =\",accuracy_scores.index(max(accuracy_scores))+1)\nprint(\"-----------------------------------------------\")","7e993582":"knn = KNeighborsClassifier(n_neighbors = k, metric='euclidean').fit(x_train,y_train.ravel())","06ff15ca":"y_pred=knn.predict(x_test)","6448148b":"confusion_matrix(y_test, y_pred)","27c3b904":"accuracy_score(y_test, y_pred)","6ca04a48":"rfc = RandomForestClassifier(random_state=1)\nrfc.fit(x_train, y_train)","d2d55b18":"y1_pred = rfc.predict(x_test)","9bc732b2":"accuracy_score(y_test, y1_pred)   # (88+36)\/(88+11+19+36) for accuracy","2f95b075":"def initiate_clf(classifier):\n    \n    y_pred = classifier.fit(x_train, y_train).predict(x_test)\n    cm = confusion_matrix(y_test, y_pred)\n    Accuracy = accuracy_score(y_test, y_pred)\n    Roc = roc_auc_score(y_test, classifier.predict_proba(x_test)[:,1])\n    f1 = f1_score(y_test, y_pred)\n    recall = recall_score(y_test,y_pred)\n    precision = precision_score(y_test,y_pred)\n    metrics = pd.DataFrame({'Scores': [Accuracy, Roc, f1,recall, precision],\n                            'Metrics': ['Accuracy',\n                                        'ROC-AUC',\n                                        'F1-Score',\n                                        'Recall',\n                                        'Precision']})\n    \n    fig = make_subplots(rows = 1, cols = 2,\n                       subplot_titles = ('Scores','Confusion Matrix'))\n    fig.add_trace(go.Heatmap(z = cm,\n                             x = ['0','1'],\n                             y = ['1','0'],\n                             colorscale = 'Reds',\n                             showscale  = False,\n                             name = 'Confusion Matrix'),\n                  row = 1, col = 2)\n    fig.add_trace(go.Bar(x = [round(i,5) for i in metrics['Scores']],\n                         y = metrics['Metrics'],\n                         text = [round(i,5) for i in metrics['Scores']],\n                         orientation='h',\n                         textposition = 'inside',\n                         name = 'Scores',\n                         marker = dict(color = colors,\n                                       line_color = 'white',\n                                       line_width=1.5)),\n                  row = 1, col = 1)\n    fig.update_layout(title={'text': classifier.__class__.__name__ ,\n                             'y':0.9,\n                             'x':0.5,\n                             'xanchor': 'center',\n                             'yanchor': 'top'},\n                      template='ggplot2')\n    fig.update_xaxes(range=[0,1], row = 1, col = 1)\n    fig.update_xaxes(title_text=\"Predicted Values\", row=1, col=2)\n    fig.update_yaxes(title_text=\"True Values\", row=1, col=2)\n\n    iplot(fig)\n\nclassifiers = [KNeighborsClassifier(n_neighbors = k, metric='euclidean'),\n               RandomForestClassifier(random_state=1)]\n\nfor clf in classifiers:\n    initiate_clf(clf)","8cd71de4":"## BASE MODEL","2c90db5a":"## Exploratry Data Analysis","3c9e8bb6":"## RANDOM FOREST\n\nRandom forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model\u2019s prediction. The reason for this wonderful effect is that the trees protect each other from their individual errors.So the prerequisites for random forest to perform well are:\n\n1. There needs to be some actual signal in our features so that models built using those features do better than random guessing.\n2. The predictions (and therefore the errors) made by the individual trees need to have low correlations with each other.","bcf279c5":"## Algorithms Used\n1. KNN Algorithm\n2. Random Forest\n\n## KNN ALGORITHM\n\nKNN (K \u2014 Nearest Neighbors) is one of many (supervised learning) algorithms used in data mining and machine learning, it\u2019s a classifier algorithm where the learning is based \u201chow similar\u201d is a data (a vector) from other .\n\nThe KNN's steps are:\n\n1. Receive an unclassified data.\n2. Measure the distance (Euclidian, Manhattan, Minkowski or Weighted) from the new data to all others data that is already classified.\n3. Gets the K(K is a parameter that you difine) smaller distances.\n4. Check the list of classes had the shortest distance and count the amount of each class that appears.\n5. Takes as correct class the class that appeared the most times.\n6. Classifies the new data with the class that you took in step 5.\n","787c28fe":"## Load and Check Data","7bf74c38":"Different parameters Descriptions\n1. BMI: BMI is a person\u2019s weight in kilograms divided by the square of height in meters. A high BMI can indicate high body fatness.\n2. Glucose: A blood sugar level less than 140 mg\/dL (7.8 mmol\/L) is considered normal.A blood sugar level from 140 to 199 mg\/dL (7.8 to 11.0 mmol\/L) is considered prediabetes. This is sometimes referred to as impaired glucose tolerance. A blood sugar level of 200 mg\/dL (11.1 mmol\/L) or higher indicates type 2 diabetes.\n3. Insulin: Insulin is a hormone (a chemical substance that acts as a messenger in the human body) that is secreted by an abdominal organ called the pancreas. It controls the breakdown of carbohydrates, fats and proteins in the body. It also guides the liver and muscles to store glucose and fat that can be used during periods of increased energy requirements and fasting. Insulin is a \u201ckey\u201d that unlocks the cell gates so that glucose from the blood enters the cells. The cells of the muscle and fat tissue are dependent solely on insulin for glucose uptake and use.\n4. Pregnancies: Diabetes can cause problems during pregnancy for women and their developing babies. Poor control of diabetes during pregnancy increases the chances for birth defects and other problems for the pregnancy. It can also cause serious complications for the woman. Proper health care before and during pregnancy can help prevent birth defects and other health problems.\n5. Skin Thickness: Skin thickness is primarily determined by collagen content and is increased in insulin-dependent diabetes mellitus (IDDM). We measured skin thickness in 66 IDDM patients aged 24\u201338 yr and investigated whether it correlated with long-term glycemic control and the presence of certain diabetic complications.\n6. Age: Older adults are at high risk for the development of type 2 diabetes due to the combined effects of increasing insulin resistance and impaired pancreatic islet function with aging.\n7. Bloodpressure: Over time, diabetes damages the small blood vessels in your body, causing the walls of the blood vessels to stiffen. This increases pressure, which leads to high blood pressure.\u201d The combination of high blood pressure and type 2 diabetes can greatly increase your risk of having a heart attack or stroke.\n8. Diabetes pedigree function: A function which scores likelihood of diabetes based on family history.","0e494173":"## Relationship Analysis","42984ccb":"## Data Distribution","e234017a":"## 3D Distribution","620b155e":"## 2D Distribution","7fc7dc39":"## Variable Type","f5d819c2":"## Analysis of Numerical Variable\n\n"}}