{"cell_type":{"eb7bcd67":"code","18b55a4c":"code","960314b7":"code","2b1d31c8":"code","8264db6d":"code","4f94d654":"code","0423e99e":"code","229a2a6c":"code","92691525":"code","5ba43110":"code","dbfef437":"code","6fa48e7a":"code","8c308995":"code","a12b067a":"code","a6fc4f89":"code","7be5b6cf":"code","c3f17a3a":"code","01cd4f8e":"code","fd477d2f":"code","33f234b3":"code","197835fe":"code","b6b5d8c5":"code","30c3bbc8":"code","5fdae131":"code","3d86ebaf":"markdown","a6d399ec":"markdown"},"source":{"eb7bcd67":"#importing libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns","18b55a4c":"data=pd.read_csv('..\/input\/spookyauthor\/train.csv')\ndata.head()","960314b7":"#finding the shape of the data (number of rows and columns)\ndata.shape","2b1d31c8":"#number of unique authors\ndata['author'].unique()","8264db6d":"#visualizing the class proportions\nsns.countplot(x=data['author'])","4f94d654":"# function to remove punctuations\nimport string\ndef remove_punct(text):\n    translator=str.maketrans('','',string.punctuation)\n    return text.translate(translator)\ndata['text']=data['text'].apply(remove_punct)\ndata['text'].head()","0423e99e":"#Removing stopwords\nsw=stopwords.words('english')\nnp.array(sw)","229a2a6c":"print('Number of unique stopwords:',len(sw))","92691525":"#function to remove stopwords\ndef stopwords(text):\n    # removing the stop words and lowercasing the selected words\n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    # joining the list of words with space separator\n    return \" \".join(text)\ndata['text']=data['text'].apply(stopwords)\ndata['text'].head()","5ba43110":"# most frequent occuring texts with the help of countvectorizer\ncount_vector=CountVectorizer()\ncount_vector.fit(data['text'])\n# collect the vocabulary items used in the vectorizer\ndictionary = count_vector.vocabulary_.items()  ","dbfef437":"# storing count and vocab in a dataframe\n# lists to store the vocab and counts\nvocab = []\ncount = []\n# iterate through each vocab and count append the value to designated lists\nfor key, value in dictionary:\n    vocab.append(key)\n    count.append(value)\n# store the count in panadas dataframe with vocab as index\nvocab_bef_stem = pd.Series(count, index=vocab)\n# sort the dataframe\nvocab_bef_stem = vocab_bef_stem.sort_values(ascending=False)","6fa48e7a":"top_vacab = vocab_bef_stem.head(10)\ntop_vacab.plot(kind = 'barh', figsize=(12,5), xlim= (25230, 25260))","8c308995":"# create an object of stemming function\nstemmer = SnowballStemmer(\"english\")\n\ndef stemming(text):    \n    '''a function which stems each word in the given text'''\n    text = [stemmer.stem(word) for word in text.split()]\n    return \" \".join(text) ","a12b067a":"data['text'] = data['text'].apply(stemming)\ndata.head()","a6fc4f89":"# create the object of tfid vectorizer\ntfid_vectorizer = TfidfVectorizer(\"english\")\n# fit the vectorizer using the text data\ntfid_vectorizer.fit(data['text'])\n# collect the vocabulary items used in the vectorizer\ndictionary = tfid_vectorizer.vocabulary_.items()","7be5b6cf":"# lists to store the vocab and counts\nvocab = []\ncount = []\n# iterate through each vocab and count append the value to designated lists\nfor key, value in dictionary:\n    vocab.append(key)\n    count.append(value)\n# store the count in panadas dataframe with vocab as index\nvocab_after_stem = pd.Series(count, index=vocab)\n# sort the dataframe\nvocab_after_stem = vocab_after_stem.sort_values(ascending=False)\n# plot of the top vocab\ntop_vacab = vocab_after_stem.head(10)\ntop_vacab.plot(kind = 'barh', figsize=(5,10), xlim= (15120, 15145))","c3f17a3a":"# Finding the text lenght of each author\ndef length(text):\n    return len(text)\ndata['length']=data['text'].apply(length)\ndata.head()","01cd4f8e":"#  visualizing the length of texts of authors\nEAP=data[data['author']=='EAP']\nHPL=data[data['author']=='HPL']\nMWS=data[data['author']=='MWS']\n\nmatplotlib.rcParams['figure.figsize'] = (12.0, 8.0)\nbins = 500\nplt.hist(EAP['length'], alpha = 0.6, bins=bins, label='EAP')\nplt.hist(HPL['length'], alpha = 0.8, bins=bins, label='HPL')\nplt.hist(MWS['length'], alpha = 0.4, bins=bins, label='MWS')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,300)\nplt.grid()\nplt.show()","fd477d2f":"# extract the tfid representation matrix of the text data\ntfid_matrix = tfid_vectorizer.transform(data['text'])\n# collect the tfid matrix in numpy array\narray = tfid_matrix.todense()\n# store the tf-idf array into pandas dataframe\ndf = pd.DataFrame(array)\ndf.head()","33f234b3":"df['output'] = data['author']\ndf['id'] = data['id']\ndf.head()","197835fe":"features = df.columns.tolist()\noutput = 'output'\n# removing the output and the id from features\nfeatures.remove(output)\nfeatures.remove('id')","b6b5d8c5":"from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.model_selection import GridSearchCV","30c3bbc8":"alpha_list1 = np.linspace(0.006, 0.1, 20)\nalpha_list1 = np.around(alpha_list1, decimals=4)\n\n# parameter grid\nparameter_grid = [{\"alpha\":alpha_list1}]\n# classifier object\nclassifier1 = MultinomialNB()\n# gridsearch object using 4 fold cross validation and neg_log_loss as scoring paramter\ngridsearch1 = GridSearchCV(classifier1,parameter_grid, scoring = 'neg_log_loss', cv = 4)\n# fit the gridsearch\ngridsearch1.fit(df[features], df[output])","5fdae131":"print(\"Best score: \",gridsearch1.best_score_) ","3d86ebaf":"## Feature Engineering","a6d399ec":"## Training Model"}}