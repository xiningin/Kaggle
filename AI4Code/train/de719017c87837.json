{"cell_type":{"d2484c0e":"code","594ecb30":"code","787b3a09":"code","59aa8b7d":"code","7725cdd8":"code","5da29ea2":"code","1c02add7":"code","a81bd414":"code","92053c30":"code","5b3a890a":"code","8ae7778b":"code","9edfa9c5":"code","366b459a":"code","ae17f8ec":"code","8e342e1c":"code","a2e6f9df":"code","a994ffa0":"code","b7a2aee7":"code","cd09eea7":"code","60848e44":"code","b078caff":"code","a9d7bc63":"code","b4defa52":"code","ab560409":"code","ca734117":"code","2935c3b5":"code","066e338f":"code","0d328a45":"code","250180a2":"code","cfe39f91":"code","26aa1e14":"code","cfd0c406":"code","9494b221":"code","630c9436":"code","4358de93":"code","3c366da9":"code","1b3d514d":"code","15994b7b":"code","515c037d":"code","da990ffe":"code","1ea7312c":"code","d9dd9de5":"code","7e9ce142":"code","2b6893aa":"code","e4556879":"code","2e7aa1ce":"code","b33dedd5":"markdown","410210f3":"markdown","5cd0deb0":"markdown","eb1a7303":"markdown","917e9b86":"markdown","6a050d26":"markdown","52a87bbd":"markdown","49784eae":"markdown","8d45c42c":"markdown","261f7c93":"markdown","80410368":"markdown","d9b70c9d":"markdown","0dd85f2f":"markdown","eaa6027a":"markdown","768cc294":"markdown","de8150d7":"markdown","b17d1599":"markdown"},"source":{"d2484c0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Plotting\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Sklearn Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\n#Filter unnecessary warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","594ecb30":"#Produce results that are reproducible\nnp.random.seed(10)","787b3a09":"#Load the dataset\ntitanic = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntitanic_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","59aa8b7d":"#Shapes of the dataset\nprint(f\"Shape of training data : {titanic.shape}\")\nprint(f\"Shape of test data : {titanic_test.shape}\")","7725cdd8":"titanic.head()","5da29ea2":"titanic.info()\n","1c02add7":"titanic.describe()","a81bd414":"titanic.isnull().sum()","92053c30":"#Features conributing to the survival rate\n#Naturally everything except \u2018PassengerId\u2019, \u2018Ticket\u2019 and \u2018Name\u2019 would be correlated with the survival rate.\ncorr = titanic.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","5b3a890a":"#Embarked pclass and sex\nFacetGrid = sns.FacetGrid(titanic, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","8ae7778b":"#Further investigation on pclass\nsns.barplot(x='Pclass', y='Survived', data=titanic)","9edfa9c5":"#is the dataset balanced ?\nsns.countplot(x='Survived', data=titanic) #countplot shows the countof observations in each categorical bin","366b459a":"titanic_test.head()","ae17f8ec":"#Dropping features not useful in the prediction and preparing the X and y .\n#Deletion of the \"Cabin\" feature due to large number f missing values\ny = titanic['Survived']\nX= titanic.copy()\nX = X.drop(columns=['Survived', 'Name', 'Ticket', 'Cabin', 'PassengerId'],axis=1)\ntitanic_test = titanic_test.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'],axis=1)\n\n\n#Dealing with Age and Embarked missing values\ndata = [X, titanic_test]\n\nfor dataset in data:\n    mean = X[\"Age\"].mean()\n    std = titanic_test[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = X[\"Age\"].astype(int)\n    \nX[\"Age\"].isnull().sum()","8e342e1c":"X.isnull().sum()","a2e6f9df":"titanic_test.isnull().sum()","a994ffa0":"print(type(X))","b7a2aee7":"X['Embarked'].describe()","cd09eea7":"#Dealing with the missing Embarked value\n#Most common value is S\ncommon_value = 'S'\nX['Embarked'] =X['Embarked'].fillna(common_value)\nX.isnull().sum()","60848e44":"#Dealing with the missing fare value in titanic_test\ntitanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(value=titanic_test[\"Fare\"].median())\ntitanic_test.isnull().sum()","b078caff":"#Encode the X since we are using sklearn implementation of RandomForest and XGBoost\nX_encoded = pd.get_dummies(X, drop_first=True)\n","a9d7bc63":"#Encode the titanic_test since we are using sklearn implementation of RandomForest and XGBoost\ntitanic_test_encoded = pd.get_dummies(titanic_test, drop_first=True)","b4defa52":"X_encoded.columns","ab560409":"titanic_test_encoded.columns","ca734117":"#Split the X into train and test\nX_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=101)\nX_train.head()","2935c3b5":"X_test.head()","066e338f":"#Stochastic Gradient Descent (SGD):\n\nsgd = linear_model.SGDClassifier(max_iter=10, tol=None)\nsgd.fit(X_train, y_train)\nY_pred = sgd.predict(X_test)\nsgd.score(X_train, y_train)\nacc_sgd = round(sgd.score(X_train, y_train) * 100, 2)\n\n#Random Forest:\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nY_prediction = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\n\n#Extreme Gradient Boosting:\nxgb = XGBClassifier( max_depth= 100, n_estimators= 500, learning_rate=0.29, random_state= 0, n_jobs=5)\nxgb.fit(X_train, y_train)\nY_pred = xgb.predict(X_test)\nacc_xgb = round(xgb.score(X_train, y_train) * 100, 2)\n\n#K Nearest Neighbor:\n\n# KNN \nknn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\n\n#Gaussian Naive Bayes:\n\ngaussian = GaussianNB() \ngaussian.fit(X_train, y_train)  \nY_pred = gaussian.predict(X_test)  \nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\n\n#Perceptron:\n\nperceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, y_train) * 100, 2)\n\n#Linear Support Vector Machine:\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)\n\n#Decision Tree\n\ndecision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, y_train)  \nY_pred = decision_tree.predict(X_test)  \nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\n\n#Which is the best Model ?\n\nresults = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'XGB' ,\n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_xgb, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","0d328a45":"#K-Fold Cross Validation for RandomForest\nfrom sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100, oob_score = True)\nscores = cross_val_score(rf, X_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","250180a2":"#K-Fold Cross Validation for XGB\nfrom sklearn.model_selection import cross_val_score\nxgb = XGBClassifier( max_depth= 100, n_estimators= 500, learning_rate=0.29, random_state= 0, n_jobs=5, eval_metric='logloss')\nscores = cross_val_score(xgb, X_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","cfe39f91":"#Feature importance using randomforest\n#Random Forest:\nrandom_forest = RandomForestClassifier(n_estimators=100, oob_score=True)\nrandom_forest.fit(X_train, y_train)\nY_prediction = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nimportances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)","26aa1e14":"print(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","cfd0c406":"param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\nclf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\nclf.fit(X_train, y_train)\n","9494b221":"clf.best_params_","630c9436":"#Test New Parameters\n# Random Forest\nrandom_forest = RandomForestClassifier(criterion = \"gini\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 10,   \n                                       n_estimators=400, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_train, y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, y_train)\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","4358de93":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, y_train, cv=3)\nconfusion_matrix(y_train, predictions)","3c366da9":"#Precision and Recall\nfrom sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(y_train, predictions))\nprint(\"Recall:\",recall_score(y_train, predictions))","1b3d514d":"#ROC AUC\nfrom sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nfrom sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","15994b7b":"param_grid = {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35 ] ,\n \"max_depth\"        : [ 20, 50, 100, 150, 200],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ] }\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nxgb = XGBClassifier( max_depth= 100, n_estimators= 500, learning_rate=0.29, random_state= 0, n_jobs=5, eval_metric='logloss')\nclf = GridSearchCV(estimator=xgb, param_grid=param_grid, n_jobs=10)\nclf.fit(X_train, y_train)","515c037d":"clf.best_params_","da990ffe":"#Test New Parameters\n# XG Boost\nxgb = XGBClassifier(gamma = 0.0, \n                    learning_rate = 0.15,\n                    max_depth = 20,\n                    min_child_weight = 7 ,\n                    random_state=1, \n                    n_jobs=5)\n\nxgb.fit(X_train, y_train)\nY_prediction = xgb.predict(X_test)\n\nxgb.score(X_train, y_train)\nacc_xgb = round(xgb.score(X_train, y_train) * 100, 2)","1ea7312c":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(xgb, X_train, y_train, cv=3)\nconfusion_matrix(y_train, predictions)","d9dd9de5":"#Precision and Recall\nfrom sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(y_train, predictions))\nprint(\"Recall:\",recall_score(y_train, predictions))","7e9ce142":"#ROC AUC\nfrom sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = xgb.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nfrom sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","2b6893aa":"#Prediction using RandomForest\n#Actual prediction\ny_pred1 = random_forest.predict(titanic_test_encoded)\nprint(y_pred1)","e4556879":"#Prediction using XGBoost\n#Actual prediction\ny_pred2 = xgb.predict(titanic_test_encoded)\nprint(y_pred2)","2e7aa1ce":"titanic_test_original = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\noutput = pd.DataFrame({'PassengerId' : titanic_test_original.PassengerId, 'Survived': y_pred2})\noutput.to_csv('submission.csv', index=False)","b33dedd5":"Notice that the age and the cabin features have a large number of missing values. The Embarked feature has only two missing values which we can easily be filled\/dropped since it is only but a small number.","410210f3":"# **Models**","5cd0deb0":"# **Data Preprocessing**","eb1a7303":"# **Model Evaluation RandomForest**","917e9b86":"Embarked seems to be correlated with survival, depending on the gender. Women on port S and Q have a higher chance of survival. Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S.","6a050d26":"# **Hyperparameter Tuning RandomForest**","52a87bbd":"XGBoost has an average accuracy of 79%","49784eae":"# **Model Evaluation XGBoost**","8d45c42c":"The training-set has 891 training examples and 11 features + the target variable (survived). 2 of the features are floats, 5 are integers and 5 are objects.","261f7c93":"Around 320 survived and about 540 did not survive so the dataset is slightly imbalanced","80410368":"# **Hyperparameter Tuning XGBoost**","d9b70c9d":"Fare seems to have the highest positive correlation with Survived while Pclass the strongest negative correlation with Survived. Age and Subsp features also show some negative correlation while PassengerId no  correlation at all. (Out of the numeric features)","0dd85f2f":"Class 1 has a higher probability of surival as opposed to Class 3","eaa6027a":"The RandomForest Model has an average accuracy of 80% with a std deviation of 4","768cc294":"# **Exploratory Data Analysis**","de8150d7":"The model predicts 77% of the time, a passengers survival correctly (precision). The recall tells us that it predicted the survival of  66% of the people who actually survived.","b17d1599":"# **Predictions on the test.Csv**"}}