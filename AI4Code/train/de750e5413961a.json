{"cell_type":{"1e27977d":"code","91846b3d":"code","784a6eba":"code","d680d171":"code","7542707c":"code","d2fb47d0":"code","b6bb98ec":"code","9d83098b":"code","e2e45900":"code","c609768d":"code","f66a5cb3":"code","d6b96edb":"code","40edf76a":"code","8ef8401e":"code","59e464c7":"code","5100d4f9":"code","a01022c7":"code","24218baf":"code","4cfe750b":"code","d492d439":"code","a6f2ba6a":"code","937236cf":"code","87f629c9":"code","c6380133":"code","d0c10b27":"code","ad6f0a4d":"code","742a4c38":"code","0551d7c6":"code","eb29209b":"code","08a79096":"code","6406c139":"markdown","ed783e41":"markdown","cec11c01":"markdown","4d351e25":"markdown","5e362e7e":"markdown","66d83d6b":"markdown","79473293":"markdown","2f5657cc":"markdown","af462a6d":"markdown","7a825131":"markdown","e0127978":"markdown","3cb14366":"markdown","e194583e":"markdown","c1f7dae1":"markdown","8703e403":"markdown","a85f117f":"markdown","449f35e9":"markdown","769c83e6":"markdown","c8bbcc1b":"markdown","769898a9":"markdown","6f892b58":"markdown","5ae46349":"markdown","4ef25818":"markdown"},"source":{"1e27977d":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\nfrom scipy import stats\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","91846b3d":"from pylab import rcParams\nrcParams['figure.figsize']=15,8","784a6eba":"df= pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf.head()","d680d171":"df.shape","7542707c":"df.info()","d2fb47d0":"duplicate_rows = df[df.duplicated()]\nprint(\"Number of duplicate rows :: \", duplicate_rows.shape)","b6bb98ec":"df = df.drop_duplicates()\nduplicate_rows = df[df.duplicated()]\nprint(\"Number of duplicate rows :: \", duplicate_rows.shape)","9d83098b":"#### `rechecking` for shape of dataset\ndf.shape","e2e45900":"df.isnull().sum()","c609768d":"df.describe()","f66a5cb3":"z = np.abs(stats.zscore(df))\ndata1 = df[(z<3).all(axis=1)]\ndata1.shape","d6b96edb":"sns.heatmap(data1.corr(),annot=True)","40edf76a":"sns.barplot(x=data1['sex'],y=data1['age'],hue=data1['output'])\nplt.show()","8ef8401e":"sns.histplot(data1,x='age',hue='output',stat='count')","59e464c7":"sns.barplot(x='age',y='cp',hue='output',data=data1)","5100d4f9":"sns.distplot(data1[data1['output'] == 0][\"chol\"], color='#512b58',label='No heart Disease') \nsns.distplot(data1[data1['output'] == 1][\"chol\"], color='#FF0000',label='Heart Disease') #Red for heart disease\nplt.title('Heart Attack distibution over Cholestrol ', fontsize=15)\nplt.legend()\nplt.show()","a01022c7":"sns.distplot(data1[data1['output'] == 0][\"trtbps\"], color='#512b58',label='No heart Disease') \nsns.distplot(data1[data1['output'] == 1][\"trtbps\"], color='#fe346e',label='Heart Disease') #Red for heart disease\nplt.title('Heart Attack distibution over Blood pressure ', fontsize=15)\nplt.legend()\nplt.show()","24218baf":"from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(6,6))\nax = Axes3D(fig)\nax.scatter(data1[\"output\"], data1[\"thalachh\"],data1[\"age\"],  c=\"red\", s=20, alpha=0.5)\nplt.xlabel('Heart Attack')\nplt.ylabel('Heart Rate')\nplt.show()","4cfe750b":"import matplotlib\n\nimport warnings\nwarnings.filterwarnings('ignore')","d492d439":"fig = plt.figure(figsize=(24,8), dpi = 100)\ngs = fig.add_gridspec(2,2)\ngs.update(wspace=0.15, hspace=0.3)\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[1,0])\nax3 = fig.add_subplot(gs[1,1])\n\n\nfig.patch.set_facecolor('#f6f5f5')\nax0.set_facecolor('#f6f5f5')\nax1.set_facecolor('#f6f5f5')\nax2.set_facecolor('#f6f5f5')\nax3.set_facecolor('#f6f5f5')\n\n\n\n# ever_married, gender, residence, heart_disease and work_type\n\nhealthy = data1[data1['output']==0]\nheart_attack= data1[data1['output']==1]\n\ngender_order = [0,1]\nchest_pain = [0,1]\nthall = [0,1]\ncaa = [0,1]\n\ncol1 = [\"#4b4b4c\",\"#fe346e\"]\ncolormap1 = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", col1, N = 256)\ncol2 = [\"#4b4b4c\",\"#512b58\"]\ncolormap2 = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", col2)\n\nheart_attack = pd.crosstab(heart_attack['sex'],[heart_attack['fbs']],normalize='index').loc[gender_order,chest_pain]\nno_heart_attack = pd.crosstab(healthy['sex'],[healthy['fbs']], normalize='index').loc[gender_order,chest_pain]\n\nsns.heatmap(ax=ax0, data=heart_attack, linewidths= 0,\n            square=False, cbar_kws={\"orientation\": \"horizontal\"}, cbar=False,linewidth=3, cmap = col1,annot=True, fmt='1.0%',annot_kws={\"fontsize\":14}, alpha = 0.9)\n\nsns.heatmap(ax=ax1, data=no_heart_attack, linewidths=0, \n            square=False, cbar_kws={\"orientation\": \"horizontal\"}, cbar=False,linewidth=3, cmap = col2,annot=True, fmt='1.0%',annot_kws={\"fontsize\":14}, alpha = 0.9)\n\nhealthy = data1[data1['output']==0]\nheart_attack= data1[data1['output']==1]                  \nheart_attack = pd.crosstab(heart_attack['sex'],[heart_attack['exng']],normalize='index').loc[gender_order,thall]\nno_heart_attack = pd.crosstab(healthy['sex'],[healthy['exng']], normalize='index').loc[gender_order,thall]\nsns.heatmap(ax=ax2, data=heart_attack, linewidths= 0,\n            square=False, cbar_kws={\"orientation\": \"horizontal\"}, cbar=False,linewidth=3, cmap = col1,annot=True, fmt='1.0%',annot_kws={\"fontsize\":14}, alpha = 0.9)\n\nsns.heatmap(ax=ax3, data=no_heart_attack, linewidths=0, \n            square=False, cbar_kws={\"orientation\": \"horizontal\"}, cbar=False,linewidth=3, cmap = col2,annot=True, fmt='1.0%',annot_kws={\"fontsize\":14}, alpha = 0.9)\n\n\nax0.text(0,-0.1,'Heart Attack Pecentage ', {'font':'serif', 'color':\"#fe346e\", 'size':30},alpha = 0.9)\nax1.text(0,-0.1,'No Heart Attack Percentage', {'font':'serif', 'color':\"#512b58\", 'size':30}, alpha =0.9)\n\nax0.axes.set_xticklabels(['No blood sugar', 'Blood sugar'], {'font':'serif', 'color':'black', 'size':16})\nax1.axes.set_xticklabels(['No blood sugar', 'Blood sugar'], {'font':'serif', 'color':'black', 'size':16})\nax2.axes.set_xticklabels(['NO induced engina','Induced engina'], {'font':'serif', 'color':'black', 'size':16})\nax3.axes.set_xticklabels(['NO induced engina','Induced engina'], {'font':'serif', 'color':'black', 'size':16})\nax0.axes.set_yticklabels(['Male','Female'], {'font':'serif', 'color':'black', 'size':16}, rotation = 0)\nax2.axes.set_yticklabels(['Male','Female'], {'font':'serif', 'color':'black', 'size':16}, rotation = 0)\n\nax0.set_xlabel('')\nax0.set_ylabel('')\nax1.set_xlabel('')\nax1.set_ylabel('')\nax2.set_xlabel('')\nax2.set_ylabel('')\nax3.set_xlabel('')\nax3.set_ylabel('')\n\nax1.axes.get_yaxis().set_visible(False)\nax3.axes.get_yaxis().set_visible(False)\n\nfig.show()","a6f2ba6a":"y=data1['output']\nX = data1.drop('output', axis=1)\nprint(X.head())\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n","937236cf":"#features\nnames = ['Age', 'Sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh', 'exng', 'oldpeak', 'slp', 'caa', 'thall']","87f629c9":"from sklearn.metrics import accuracy_score, confusion_matrix","c6380133":"#Logistic Regression\n\nlogReg = LogisticRegression(random_state=0)\nlogReg.fit(X_train, y_train)\n\n#Check prediction of Logistic Regression\ny_pred_logReg = logReg.predict(X_test)\n\n#Model Accuracy\nprint(\"Accuracy of logistic regression classifier: %.2f%%\" % (metrics.accuracy_score(y_test,y_pred_logReg)*100.0))\n\n#Confusion matrix\nplt.title(\"Confusion metric\")\nsns.heatmap(confusion_matrix(y_test, y_pred_logReg), annot = True)","d0c10b27":"#Decision Tree Classifier\n\ndecTree = DecisionTreeClassifier(max_depth=6, random_state=0)\ndecTree.fit(X_train,y_train)\n\n#Check predicition of Decision Tree Classifier\ny_pred_decTree = decTree.predict(X_test)\n\n#Model Accuracy\nprint(\"Accuracy of Decision Tree Classifier: %.2f%%\" % (metrics.accuracy_score(y_test,y_pred_decTree)*100.0))\n\n#Confusion matrix\nplt.title(\"Confusion metric\")\nsns.heatmap(confusion_matrix(y_test, y_pred_decTree), annot = True)","ad6f0a4d":"# Random forest classifier\nrf = RandomForestClassifier(n_estimators=500)\nrf.fit(X_train,y_train)\n\n#Check prediction of Random forest classifier\ny_pred_rf = rf.predict(X_test)\n\n#Model Accuracy\nprint(\"Accuracy of Random Forest Classifier: %.2f%%\" % (metrics.accuracy_score(y_test,y_pred_rf)*100.0))\n\n#Find the score of each feature in model\nf_imp = rf.feature_importances_\nfor i,v in enumerate(f_imp):\n    print('Feature: %s, Score: %.2f%%' % (names[i],v))\n    \n#Confusion matrix\nplt.title(\"Confusion metric\")\nsns.heatmap(confusion_matrix(y_test, y_pred_rf), annot = True)","742a4c38":"#K Nearest Neighbours Classifier\nknc =  KNeighborsClassifier()\nknc.fit(X_train,y_train)\n\n#Check prediction of k nearest neighbours classifier\ny_pred_knc = knc.predict(X_test)\n\n#Model Accuracy\nprint(\"Accuracy of K-Neighbours classifier: %.2f%%\" % (metrics.accuracy_score(y_test,y_pred_knc)*100.0))\n\n#Confusion matrix\nplt.title(\"Confusion metric\")\nsns.heatmap(confusion_matrix(y_test, y_pred_knc), annot = True)","0551d7c6":"#Gaussian NB\ngnb = GaussianNB()\ngnb.fit(X_train,y_train)\n\n#Check prediction of gaussian NB\ny_pred_gnb = gnb.predict(X_test)\n\n#Model Accuracy\nprint(\"Accuracy of Gaussian NB: %.2f%%\" % (metrics.accuracy_score(y_test,y_pred_gnb)*100.0))\n\n#Confusion matrix\nplt.title(\"Confusion metric\")\nsns.heatmap(confusion_matrix(y_test, y_pred_gnb), annot = True)","eb29209b":"from xgboost import XGBClassifier\n\nxbgc = XGBClassifier()\nxbgc.fit(X_train, y_train)\n\n#Check prediction of xgbclassifier\ny_pred_xbgc = xbgc.predict(X_test)\n\n#Model Accuracy\nprint(\"Accuracy of XGBclassifier: %.2f%%\" % (metrics.accuracy_score(y_test,y_pred_xbgc)*100.0))\n\n#Confusion matrix\nplt.title(\"Confusion metric\")\nsns.heatmap(confusion_matrix(y_test, y_pred_xbgc), annot = True)","08a79096":"#Models and their accuracy\nprint(\"*****************Models and their accuracy*****************\")\nprint(\"Logistic Regression Classifier: %.2f%%\" % (metrics.accuracy_score(y_test,y_pred_logReg)*100.0))\nprint(\"Decision Tree : %.2f%%\" % (metrics.accuracy_score(y_test,y_pred_decTree)*100.0))\nprint(\"Random Forest Classifier: %.2f%%\" % (metrics.accuracy_score(y_test, y_pred_rf)*100.0))\nprint(\"K Neighbours Classifier: %.2f%%\" % (metrics.accuracy_score(y_test,y_pred_knc)*100.0))\nprint(\"Gaussian NB: %.2f%%\" % (metrics.accuracy_score(y_test,y_pred_gnb)*100.0))\nprint(\"XGBclassifier: %.2f%%\" % (metrics.accuracy_score(y_test,y_pred_xbgc)*100.0))","6406c139":"#### This dataset do not having any null values","ed783e41":"#### Checking for `duplicate` records in dataset","cec11c01":"#### Removing outliers using `Z-score`","4d351e25":"##### After implementing six classification models and comparing their accuracy, we can conclude that for this dataset `Logistic Regression Classifier` is the appropriate model to be used.","5e362e7e":"#### The accuracy of `Gaussian NB` classifier  is `89.66%`.","66d83d6b":"##### we have `1` duplicate record.\n##### `drop` duplicate record\n##### `rechecking` for duplicate","79473293":"### Outlier treatment","2f5657cc":"### *********END********","af462a6d":"#### The accuracy of `logistic regression` classifier  is `91.38%`.","7a825131":"#### This dataset having `303` records and `14` features","e0127978":"### Build models","3cb14366":"#### All data types features are:`int64` and `float64`","e194583e":"### Conclusion","c1f7dae1":"### Visualization of features","8703e403":"#### After removing we have 287 records and 14 features","a85f117f":"#### The accuracy of `Random Forest` classifier  is `86.21%`.","449f35e9":"#### The accuracy of `Decision Tree` classifier  is `81.03%`.","769c83e6":"#### The accuracy of `XGB` classifier  is `81.03%`.","c8bbcc1b":"### Results","769898a9":"### Split `train` and `test`\nBefore implementing any classification algorithm, we will divide our dataset into training data and test data. I have used 80% of the data for training and the remaining 20% will be used for testing.","6f892b58":"#### The accuracy of `K Nearest Neighbours` classifier  is `63.79%`.","5ae46349":"### Check correlation of all features","4ef25818":"# **Heart Attack Prediction and Analysis**\n\n\n## **About this dataset**\n\nAge : Age of the patient\n\nSex : Sex of the patient\n\nexang: exercise induced angina (1 = yes; 0 = no)\n\nca: number of major vessels (0-3)\n\ncp : Chest Pain type chest pain type\n\nValue 1: typical angina\n\nValue 2: atypical angina\n\nValue 3: non-anginal pain\n\nValue 4: asymptomatic\n\ntrtbps : resting blood pressure (in mm Hg)\n\nchol : cholestoral in mg\/dl fetched via BMI sensor\n\nfbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\nrest_ecg : resting electrocardiographic results\n\nValue 0: normal\n\nValue 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n\nValue 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\nthalach : maximum heart rate achieved\n\ntarget : 0= less chance of heart attack \n\n      1= more chance of heart attack\n\n**Source: https:\/\/www.kaggle.com\/rashikrahmanpritom\/heart-attack-analysis-prediction-dataset**"}}