{"cell_type":{"524884fd":"code","baa650f0":"code","7c4d10fb":"code","50c81d3c":"code","bdb6d80e":"code","c207f8bc":"code","f8dd05d6":"code","5900e175":"code","077744b5":"code","8988d78d":"code","014771f0":"code","06e620f0":"code","e242e72d":"code","19f4ff74":"code","79045997":"code","c7712897":"code","96359010":"code","1f5a463e":"code","1345c089":"code","cc50e3d5":"markdown","c288627b":"markdown","cb0a736f":"markdown","98668341":"markdown"},"source":{"524884fd":"#importing modules\nimport numpy as np \nimport pandas as pd \n\n#loading datasets\ndata = pd.read_csv(\"..\/input\/data.csv\")","baa650f0":"#the fist 5 rows\ndata.head(5)","7c4d10fb":"#the dataset summary\ndata.info()","50c81d3c":"#deleting useless columns\n#deleting the \"id\" column\ndata.drop(\"id\",axis=1,inplace=True)\n#deleting the \"Unnamed: 32\" column\ndata.drop(\"Unnamed: 32\",axis=1,inplace=True) ","bdb6d80e":"#the result\ndata.info()","c207f8bc":"#the first 5 rows\ndata.head(5)","f8dd05d6":"#counting the diagnosis variable\ndata.diagnosis.value_counts()","5900e175":"#diagnosis variable is a responsible variable for the classification\n#replacing M and B with 1 and 0 respectively\ndata.diagnosis=data.diagnosis.map({'M':1,'B':0})","077744b5":"#counting the diagnosis variable\ndata.diagnosis.value_counts()","8988d78d":"#finished the dataset preprocessing\n#splitting dataset into training one and testing one\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data, test_size = 0.3,random_state=1234)","014771f0":"#finding out the results\nprint(train.shape)\nprint(test.shape)","06e620f0":"#making independent variables for training\ntrain_X = train.iloc[:, 1:31]\n#making responsible variables for training\ntrain_y=train.diagnosis\n#making independent variables for testing\ntest_X= test.iloc[:, 1:31]\n#making responsible variables for testing\ntest_y =test.diagnosis","e242e72d":"#finding out the results\nprint(train_X.shape)\nprint(train_y.shape)\nprint(test_X.shape)\nprint(test_y.shape)","19f4ff74":"#With Hyper Parameters Tuning\n#2-1,DesicionTree\n#2-2,Randomforest\n#2-3,SVM\n#2-4,kNearestNeighbors","79045997":"#With Hyper Parameters Tuning\n#2-1,DesicionTree\n#importing modules\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\n#making the instance\nmodel= DecisionTreeClassifier(random_state=1234)\n#Hyper Parameters Set\nparams = {'max_features': ['auto', 'sqrt', 'log2'],\n          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n          'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11],\n          'random_state':[123]}\n#Making models with hyper parameters sets\nmodel1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n#Learning\nmodel1.fit(train_X,train_y)\n#The best hyper parameters set\nprint(\"Best Hyper Parameters:\",model1.best_params_)\n#Prediction\nprediction=model1.predict(test_X)\n#importing the metrics module\nfrom sklearn import metrics\n#evaluation(Accuracy)\nprint(\"Accuracy:\",metrics.accuracy_score(prediction,test_y))\n#evaluation(Confusion Metrix)\nprint(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,test_y))","c7712897":"#With Hyper Parameters Tuning\n#2-2,Randomforest\n#importing modules\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n#making the instance\nmodel=RandomForestClassifier()\n#hyper parameters set\nparams = {'criterion':['gini','entropy'],\n          'n_estimators':[10,15,20,25,30],\n          'min_samples_leaf':[1,2,3],\n          'min_samples_split':[3,4,5,6,7], \n          'random_state':[123],\n          'n_jobs':[-1]}\n#Making models with hyper parameters sets\nmodel1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n#learning\nmodel1.fit(train_X,train_y)\n#The best hyper parameters set\nprint(\"Best Hyper Parameters:\\n\",model1.best_params_)\n#Prediction\nprediction=model1.predict(test_X)\n#importing the metrics module\nfrom sklearn import metrics\n#evaluation(Accuracy)\nprint(\"Accuracy:\",metrics.accuracy_score(prediction,test_y))\n#evaluation(Confusion Metrix)\nprint(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,test_y))","96359010":"#With Hyper Parameters Tuning\n#2-3,SVM\n#importing modules\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import svm\n#making the instance\nmodel=svm.SVC()\n#Hyper Parameters Set\nparams = {'C': [6,7,8,9,10,11,12], \n          'kernel': ['linear','rbf']}\n#Making models with hyper parameters sets\nmodel1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n#Learning\nmodel1.fit(train_X,train_y)\n#The best hyper parameters set\nprint(\"Best Hyper Parameters:\\n\",model1.best_params_)\n#Prediction\nprediction=model1.predict(test_X)\n#importing the metrics module\nfrom sklearn import metrics\n#evaluation(Accuracy)\nprint(\"Accuracy:\",metrics.accuracy_score(prediction,test_y))\n#evaluation(Confusion Metrix)\nprint(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,test_y))","1f5a463e":"#With Hyper Parameters Tuning\n#2-4,kNearestNeighbors\n#importing modules\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n#making the instance\nmodel = KNeighborsClassifier(n_jobs=-1)\n#Hyper Parameters Set\nparams = {'n_neighbors':[5,6,7,8,9,10],\n          'leaf_size':[1,2,3,5],\n          'weights':['uniform', 'distance'],\n          'algorithm':['auto', 'ball_tree','kd_tree','brute'],\n          'n_jobs':[-1]}\n#Making models with hyper parameters sets\nmodel1 = GridSearchCV(model, param_grid=params, n_jobs=1)\n#Learning\nmodel1.fit(train_X,train_y)\n#The best hyper parameters set\nprint(\"Best Hyper Parameters:\\n\",model1.best_params_)\n#Prediction\nprediction=model1.predict(test_X)\n#importing the metrics module\nfrom sklearn import metrics\n#evaluation(Accuracy)\nprint(\"Accuracy:\",metrics.accuracy_score(prediction,test_y))\n#evaluation(Confusion Metrix)\nprint(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,test_y))","1345c089":"#Result           Without HyperParameterTuning      With HyperParameterTuning\n#DecisionTree     0.929824561404                    0.912280701754   \u2192not improved\n#Randomforest     0.923976608187                    0.923976608187   \u2192the same result\n#SVM              0.614035087719                    0.93567251462    \u2192dramatically improved\n#kNearestNeighbor 0.93567251462                     0.93567251462    \u2192the same result \n\n#The default hyper parameters set of DecisionTree, Randomforest and kNearestNeighbor looks not so bad.","cc50e3d5":"## SVM Hyperparameters\nParameters\n    ----------\n    C : float, optional (default=1.0)\n        Penalty parameter C of the error term.\n    kernel : string, optional (default='rbf')\n        Specifies the kernel type to be used in the algorithm.\n        It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n        a callable.\n        If none is given, 'rbf' will be used. If a callable is given it is\n        used to pre-compute the kernel matrix from data matrices; that matrix\n        should be an array of shape ``(n_samples, n_samples)``.\n    degree : int, optional (default=3)\n        Degree of the polynomial kernel function ('poly').\n        Ignored by all other kernels.\n    gamma : float, optional (default='auto')\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n        Current default is 'auto' which uses 1 \/ n_features,\n        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.std())\n        as value of gamma. The current default of gamma, 'auto', will change\n        to 'scale' in version 0.22. 'auto_deprecated', a deprecated version of\n        'auto' is used as a default indicating that no explicit value of gamma\n        was passed.\n    coef0 : float, optional (default=0.0)\n        Independent term in kernel function.\n        It is only significant in 'poly' and 'sigmoid'.\n    shrinking : boolean, optional (default=True)\n        Whether to use the shrinking heuristic.\n    probability : boolean, optional (default=False)\n        Whether to enable probability estimates. This must be enabled prior\n        to calling `fit`, and will slow down that method.\n    tol : float, optional (default=1e-3)\n        Tolerance for stopping criterion.\n    cache_size : float, optional\n        Specify the size of the kernel cache (in MB).\n    class_weight : {dict, 'balanced'}, optional\n        Set the parameter C of class i to class_weight[i]*C for\n        SVC. If not given, all classes are supposed to have\n        weight one.\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples \/ (n_classes * np.bincount(y))``\n    verbose : bool, default: False\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in libsvm that, if enabled, may not work\n        properly in a multithreaded context.\n    max_iter : int, optional (default=-1)\n        Hard limit on iterations within solver, or -1 for no limit.\n    decision_function_shape : 'ovo', 'ovr', default='ovr'\n        Whether to return a one-vs-rest ('ovr') decision function of shape\n        (n_samples, n_classes) as all other classifiers, or the original\n        one-vs-one ('ovo') decision function of libsvm which has shape\n        (n_samples, n_classes * (n_classes - 1) \/ 2). However, one-vs-one\n        ('ovo') is always used as multi-class strategy.\n        .. versionchanged:: 0.19\n            decision_function_shape is 'ovr' by default.\n        .. versionadded:: 0.17\n           *decision_function_shape='ovr'* is recommended.\n        .. versionchanged:: 0.17\n           Deprecated *decision_function_shape='ovo' and None*.\n    random_state : int, RandomState instance or None, optional (default=None)\n        The seed of the pseudo random number generator used when shuffling\n        the data for probability estimates. If int, random_state is the\n        seed used by the random number generator; If RandomState instance,\n        random_state is the random number generator; If None, the random\n        number generator is the RandomState instance used by `np.random`.","c288627b":"## Decesion Tree Hyperparameter\n Parameters\n    ----------\n    criterion : string, optional (default=\"gini\")\n        The function to measure the quality of a split. Supported criteria are\n        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n    splitter : string, optional (default=\"best\")\n        The strategy used to choose the split at each node. Supported\n        strategies are \"best\" to choose the best split and \"random\" to choose\n        the best random split.\n    max_depth : int or None, optional (default=None)\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n    max_features : int, float, string or None, optional (default=None)\n        The number of features to consider when looking for the best split:\n            - If int, then consider `max_features` features at each split.\n            - If float, then `max_features` is a fraction and\n              `int(max_features * n_features)` features are considered at each\n              split.\n            - If \"auto\", then `max_features=sqrt(n_features)`.\n            - If \"sqrt\", then `max_features=sqrt(n_features)`.\n            - If \"log2\", then `max_features=log2(n_features)`.\n            - If None, then `max_features=n_features`.\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n        The weighted impurity decrease equation is the following::\n            N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity\n                                - N_t_L \/ N_t * left_impurity)\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n        .. versionadded:: 0.19\n    min_impurity_split : float, (default=1e-7)\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n    class_weight : dict, list of dicts, \"balanced\" or None, default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n        Note that for multioutput (including multilabel) weights should be\n        defined for each class of every column in its own dict. For example,\n        for four-class multilabel classification weights should be\n        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n        [{1:1}, {2:5}, {3:1}, {4:1}].\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples \/ (n_classes * np.bincount(y))``\n        For multi-output, the weights of each column of y will be multiplied.\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n    presort : bool, optional (default=False)\n        Whether to presort the data to speed up the finding of best splits in\n        fitting. For the default settings of a decision tree on large\n        datasets, setting this to true may slow down the training process.\n        When using either a smaller dataset or a restricted depth, this may\n        speed up the training.","cb0a736f":"## Random Forest Hyperparameter\nParameters\n    ----------\n    n_estimators : integer, optional (default=10)\n        The number of trees in the forest.\n        .. versionchanged:: 0.20\n           The default value of ``n_estimators`` will change from 10 in\n           version 0.20 to 100 in version 0.22.\n    criterion : string, optional (default=\"gini\")\n        The function to measure the quality of a split. Supported criteria are\n        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n        Note: this parameter is tree-specific.\n    max_depth : integer or None, optional (default=None)\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n    max_features : int, float, string or None, optional (default=\"auto\")\n        The number of features to consider when looking for the best split:\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=sqrt(n_features)`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n        The weighted impurity decrease equation is the following::\n            N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity\n                                - N_t_L \/ N_t * left_impurity)\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n        .. versionadded:: 0.19\n    min_impurity_split : float, (default=1e-7)\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19. The default value of\n           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n    bootstrap : boolean, optional (default=True)\n        Whether bootstrap samples are used when building trees.\n    oob_score : bool (default=False)\n        Whether to use out-of-bag samples to estimate\n        the generalization accuracy.\n    n_jobs : int or None, optional (default=None)\n        The number of jobs to run in parallel for both `fit` and `predict`.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n    verbose : int, optional (default=0)\n        Controls the verbosity when fitting and predicting.\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n    class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or \\\n    None, optional (default=None)\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n        Note that for multioutput (including multilabel) weights should be\n        defined for each class of every column in its own dict. For example,\n        for four-class multilabel classification weights should be\n        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n        [{1:1}, {2:5}, {3:1}, {4:1}].\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples \/ (n_classes * np.bincount(y))``\n        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n        weights are computed based on the bootstrap sample for every tree\n        grown.\n        For multi-output, the weights of each column of y will be multiplied.\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.","98668341":"## KNN Hyperparamaters\nParameters\n    ----------\n    n_neighbors : int, optional (default = 5)\n        Number of neighbors to use by default for :meth:`kneighbors` queries.\n    weights : str or callable, optional (default = 'uniform')\n        weight function used in prediction.  Possible values:\n        - 'uniform' : uniform weights.  All points in each neighborhood\n          are weighted equally.\n        - 'distance' : weight points by the inverse of their distance.\n          in this case, closer neighbors of a query point will have a\n          greater influence than neighbors which are further away.\n        - [callable] : a user-defined function which accepts an\n          array of distances, and returns an array of the same shape\n          containing the weights.\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n        Algorithm used to compute the nearest neighbors:\n        - 'ball_tree' will use :class:`BallTree`\n        - 'kd_tree' will use :class:`KDTree`\n        - 'brute' will use a brute-force search.\n        - 'auto' will attempt to decide the most appropriate algorithm\n          based on the values passed to :meth:`fit` method.\n        Note: fitting on sparse input will override the setting of\n        this parameter, using brute force.\n    leaf_size : int, optional (default = 30)\n        Leaf size passed to BallTree or KDTree.  This can affect the\n        speed of the construction and query, as well as the memory\n        required to store the tree.  The optimal value depends on the\n        nature of the problem.\n    p : integer, optional (default = 2)\n        Power parameter for the Minkowski metric. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n    metric : string or callable, default 'minkowski'\n        the distance metric to use for the tree.  The default metric is\n        minkowski, and with p=2 is equivalent to the standard Euclidean\n        metric. See the documentation of the DistanceMetric class for a\n        list of available metrics.\n    metric_params : dict, optional (default = None)\n        Additional keyword arguments for the metric function.\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n        Doesn't affect :meth:`fit` method.\n    \"\"\"\n"}}