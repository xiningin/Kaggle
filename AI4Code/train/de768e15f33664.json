{"cell_type":{"0bad9b3c":"code","32980934":"code","e1e4106a":"code","5e863b91":"code","11a0c4ba":"code","921e127f":"code","09f0742b":"code","231146be":"code","97e57f47":"code","4b3f09d5":"code","ca04cd5c":"code","702a8ddf":"code","8e4e4663":"code","383f3a4c":"code","c1161d39":"code","8d628ea1":"code","19462ae0":"code","d34a8e5c":"code","b5531f7f":"code","4e26f7d1":"code","8ce15a9e":"code","29ce642e":"code","08c83684":"code","09815261":"code","02369c69":"code","23c75a72":"code","522e0647":"code","9a172e62":"code","6d312078":"code","c704c9f4":"code","d04c248c":"code","323bceaf":"code","5b4a6e45":"code","97c3e99a":"code","610bb06e":"code","c02db3c7":"code","eb6ac7b8":"code","ce5387b8":"code","3ee083ab":"code","09c17a73":"code","e87a501e":"code","e763e6f6":"code","e2b93cb9":"code","4ff78d9d":"code","f4e8b842":"code","fe39766f":"code","bb440cec":"code","5fbefdc0":"code","4d3c567a":"code","cc1c761f":"code","f2735606":"code","fb2b7142":"code","78fecca8":"code","f2a643ed":"code","79a34c39":"code","9ef954ab":"code","d06105f5":"code","e2cbbd13":"code","8d08302b":"code","0c7697f9":"code","bc045e03":"code","7fe6bd5a":"code","1564060b":"code","f54f3daa":"code","1ea2991f":"code","6448336e":"code","8dd1a6bd":"code","c2a95603":"code","4e2abe73":"code","b4bd2c78":"code","edd7b8d1":"code","43903177":"code","172dceb9":"code","99b671bb":"code","5335e42f":"code","b1b23bd6":"code","816d5257":"code","98227de9":"code","4516f273":"code","c1f2c8c8":"code","15772acb":"markdown","fc55ddd3":"markdown","142f4357":"markdown","dbba0ac2":"markdown","9e3b4987":"markdown","a04e81d6":"markdown","47c354d8":"markdown","5bca060a":"markdown","fb2fa2d9":"markdown","ac76f241":"markdown","124455f3":"markdown","eb19f0a0":"markdown","0d41b74b":"markdown","1529ac1a":"markdown","b0055ec7":"markdown","94af3216":"markdown","73288421":"markdown","17c2c57e":"markdown","3e6810a9":"markdown","f55adb0e":"markdown","2ee92f3f":"markdown","0994cd33":"markdown","0acd8026":"markdown"},"source":{"0bad9b3c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","32980934":"from nltk.tokenize import word_tokenize, sent_tokenize\nimport nltk","e1e4106a":"import pandas as pd\ndf = pd.read_csv('\/kaggle\/input\/traveldataset5000\/5000TravelQuestionsDataset.csv', encoding=\"Latin-1\", header=None)","5e863b91":"df[1].unique()","11a0c4ba":"df[1].replace({'TGU\\n':'TGU', 'TTD\\n':'TTD' , '\\nENT': 'ENT'}, inplace=True)\nprint(df[1].unique())\nprint(len(df[1].unique()))","921e127f":"df[2].unique()","09f0742b":"df[2].replace({'WTHTMP\\n':'WTHTMP', '\\nTGULAU': 'TGULAU', 'TRSOTH\\n': 'TRSOTH', 'FODBAK\\n': 'FODBAK', 'TRSAIR\\n':'TRSAIR', 'TGUCIG\\n':'TGUCIG', 'TTDOTH\\n':'TTDOTH', 'WTHOTH\\n':'WTHOTH', 'TTDSIG\\n':'TTDSIG', \n              'TGUOTH\\n':'TGUOTH', 'TTDSHP\\n':'TTDSHP', 'TRSROU\\n':'TRSROU', 'TTDSPO\\n':'TTDSPO', '\\nACMOTH':'ACMOTH', 'ACMOTH\\n':'ACMOTH', '\\nWTHOTH':'WTHOTH' }, inplace=True)\nprint(df[2].unique())\nprint(len(df[2].unique()))","231146be":"travelQsns = df[0]\nprint(travelQsns[0:10])","97e57f47":"regexTokenizer = nltk.RegexpTokenizer(r\"[a-zA-Z0-9]+\")\n\n# travelTokens = [word_tokenize(qsn.lower()) for qsn in df[0]]\ntravelTokens = [regexTokenizer.tokenize(qsn.lower()) for qsn in df[0]]\nprint(travelTokens[:10])","4b3f09d5":"from nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()","ca04cd5c":"from nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\n\ntravelTokensLemmatized=[]\nfor sent in travelTokens:\n    a=list()\n    for w in sent:\n        w = wordnet_lemmatizer.lemmatize(w)\n        if w not in stop_words:\n            a.append(w)\n    travelTokensLemmatized.append(a)","702a8ddf":"print(travelTokensLemmatized[:10])","8e4e4663":"from nltk.tokenize.treebank import TreebankWordDetokenizer\nppUntokenizedQsns = [TreebankWordDetokenizer().detokenize(qsn) for qsn in travelTokensLemmatized]\nprint(ppUntokenizedQsns[:10])","383f3a4c":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidfVectorizerUnigram = TfidfVectorizer()\nunigramTfidfVectors = tfidfVectorizerUnigram.fit_transform(ppUntokenizedQsns)\n\nprint(unigramTfidfVectors[:10])","c1161d39":"unigramTfidfFeature_names = ['tfidf_'+ fn for fn in tfidfVectorizerUnigram.get_feature_names()]\nprint(unigramTfidfFeature_names[:10])","8d628ea1":"tfidfFeaturesDf = pd.DataFrame.sparse.from_spmatrix(unigramTfidfVectors)\ntfidfFeaturesDf.columns = unigramTfidfFeature_names\ntfidfFeaturesDf.head()","19462ae0":"from nltk.util import ngrams\n\nbigramAllQsns = []\n\nfor qsn in travelTokensLemmatized:\n    n_grams  = ngrams(qsn, 2)\n    bigramAllQsns.append([ '_'.join(grams) for grams in n_grams])\n\nprint(bigramAllQsns[:10])","d34a8e5c":"untokenizedBigrams = [TreebankWordDetokenizer().detokenize(qsn) for qsn in bigramAllQsns]\ntfidfVectorizer = TfidfVectorizer()\nbigramVectors = tfidfVectorizer.fit_transform(untokenizedBigrams)\nbigramFeature_names = ['bigram_'+ fn for fn in tfidfVectorizer.get_feature_names()]\nprint(bigramVectors.shape)\n\nbigramFeaturesDf = pd.DataFrame.sparse.from_spmatrix(bigramVectors)\nbigramFeaturesDf.columns = bigramFeature_names\nbigramFeaturesDf.head()","b5531f7f":"posTagsAllQsns = list()\n\nfor qsn in travelTokensLemmatized:\n    posTagsAllQsns.append(nltk.pos_tag(qsn))        ","4e26f7d1":"posTagFormatted = []\nfor qsn in posTagsAllQsns:\n    posTagSent = []\n    for w,pos in qsn:\n        posTagSent.append(w + \"_\" +pos)\n    posTagFormatted.append(posTagSent)","8ce15a9e":"print(posTagFormatted[:10])","29ce642e":"untokenizedPosTags = [TreebankWordDetokenizer().detokenize(qsn) for qsn in posTagFormatted]\ntfidfVectorizer = TfidfVectorizer()\nposTagVectors = tfidfVectorizer.fit_transform(untokenizedPosTags)\nposTagFeature_names = ['pos_'+ fn for fn in tfidfVectorizer.get_feature_names()]\nprint(posTagVectors.shape)\n\nposTagFeaturesDf = pd.DataFrame.sparse.from_spmatrix(posTagVectors)\nposTagFeaturesDf.columns = posTagFeature_names\nposTagFeaturesDf.head()","08c83684":"import spacy\nspacy_nlp = spacy.load('en_core_web_sm')","09815261":"nerList = []\nfor qsn in travelQsns:\n    doc = spacy_nlp(qsn)\n    nerPerQsn = []\n    for i in doc.ents:\n        nerPerQsn.append((i.lemma_).lower() + \"_\" + i.label_)\n    nerList.append(nerPerQsn)\n\nprint(nerList[:10])","02369c69":"untokenizedNERList = [TreebankWordDetokenizer().detokenize(qsn) for qsn in nerList]","23c75a72":"print(untokenizedNERList[:10])","522e0647":"tfidfVectorizer = TfidfVectorizer()\nnerVectors = tfidfVectorizer.fit_transform(untokenizedNERList)\nnerFeature_names = ['ner_'+ fn for fn in tfidfVectorizer.get_feature_names()]\nprint(nerVectors.shape)\n\nnerFeaturesDf = pd.DataFrame.sparse.from_spmatrix(nerVectors)\nnerFeaturesDf.columns = nerFeature_names\nnerFeaturesDf.head()","9a172e62":"ppUntokenizedQsns[:10]","6d312078":"headwordPairListsAllQsns = []\nfor qsn in travelQsns:\n    doc = spacy_nlp(qsn)\n    headWordPairPerQsn = []\n    for token in doc:\n        if (token.text.lower() not in stop_words) and (token.head.text.lower() not in stop_words):\n            headWordPairPerQsn.append(token.text.lower() + \"_\" + token.head.text.lower())\n    headwordPairListsAllQsns.append(headWordPairPerQsn)\n\nprint(headwordPairListsAllQsns[:10])","c704c9f4":"untokenizedHeadWordPairListsAllQsns = [TreebankWordDetokenizer().detokenize(qsn) for qsn in headwordPairListsAllQsns]\nprint(untokenizedHeadWordPairListsAllQsns[:10])","d04c248c":"tfidfVectorizer = TfidfVectorizer()\nwordHeadWordPairVectors = tfidfVectorizer.fit_transform(untokenizedHeadWordPairListsAllQsns)\nwordHeadWordPairFeature_names = ['whwpair_'+ fn for fn in tfidfVectorizer.get_feature_names()]\nprint(wordHeadWordPairVectors.shape)\n\nwordHeadWordPairFeaturesDf = pd.DataFrame.sparse.from_spmatrix(wordHeadWordPairVectors)\nwordHeadWordPairFeaturesDf.columns = wordHeadWordPairFeature_names\nwordHeadWordPairFeaturesDf.head()","323bceaf":"finalFeatureDf = pd.concat([df[0],tfidfFeaturesDf,bigramFeaturesDf,posTagFeaturesDf,nerFeaturesDf,wordHeadWordPairFeaturesDf,df[1], df[2]], axis=1)\nfinalFeatureDf.head()\nprint(finalFeatureDf.columns)","5b4a6e45":"X = finalFeatureDf.drop([0,1,2], axis=1)","97c3e99a":"# y_coarse_class = pd.DataFrame(data=finalFeatureDf[1])\n# y_fine_class = pd.DataFrame(data=finalFeatureDf[2])\n\ny_coarse_class = pd.DataFrame(data=df[1])\ny_fine_class = pd.DataFrame(data=df[2])","610bb06e":"from sklearn import preprocessing\n\nle_coarse_class = preprocessing.LabelEncoder()\nle_coarse_class.fit(y_coarse_class.iloc[:,-1])\nprint(le_coarse_class.classes_)","c02db3c7":"y_coarse_classEncoded = pd.DataFrame(data=le_coarse_class.transform(y_coarse_class.iloc[:,-1]))\ny_coarse_classEncoded.head()","eb6ac7b8":"le_fine_class = preprocessing.LabelEncoder()\nle_fine_class.fit(y_fine_class.iloc[:,-1])\nprint(le_fine_class.classes_)\n\ny_fine_classEncoded = pd.DataFrame(data=le_fine_class.transform(y_fine_class.iloc[:,-1]))\ny_fine_classEncoded.head()","ce5387b8":"y_coarse_classEncoded.columns = ['coarse_class']\ny_fine_classEncoded.columns = ['fine_class']","3ee083ab":"labelEncodedFullDf = pd.concat([X, y_coarse_classEncoded, y_fine_classEncoded], axis=1)\nlabelEncodedFullDf.head()","09c17a73":"X.head()","e87a501e":"y_coarse_classEncoded.head()","e763e6f6":"from sklearn.model_selection import KFold\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import cross_validate","e2b93cb9":"print(X.shape)","4ff78d9d":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, precision_score,recall_score,f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\nX_features = X.to_numpy()\nY_coarse_class = np.array(y_coarse_classEncoded.iloc[:,-1])\ncoarseLabels = np.unique(Y_coarse_class)","f4e8b842":"parameters = {'alpha':[0.1,0.3],'loss':['hinge'],'max_iter':[20,30]}\n\nclassifier = SGDClassifier(random_state=42, tol=None)\ngrid = GridSearchCV(classifier, parameters, cv = 10, scoring = 'accuracy', verbose=1)\ngrid.fit(X_features[:2000], Y_coarse_class[:2000])\n\nprint('======Best Parameters=====')\nprint(grid.best_params_)\nprint(grid.best_score_)","fe39766f":"kf = StratifiedKFold(n_splits=10)\n\nsvm_conf_mat = []\nsvm_clas_repo = []\nfold = 0\n\nacc_coarse=[]\nprec_coarse=[]\nrecall_coarse=[]\nf1_coarse=[]\n\nfor train_index, test_index in kf.split(X_features,Y_coarse_class):\n    fold += 1\n    print(\"*\"*50 + \"Fold = \" + str(fold))\n    \n    X_train, X_test = X_features[train_index], X_features[test_index]\n    y_train, y_test = Y_coarse_class[train_index], Y_coarse_class[test_index]\n    \n    svmclassifier = SGDClassifier(loss='log', penalty='l2',alpha=0.01, random_state=13, max_iter=20, tol=None)\n    svmclassifier.fit(X_train, y_train)\n    \n    y_pred = svmclassifier.predict(X_test)\n    \n    conf = confusion_matrix(y_test, y_pred, labels=coarseLabels)\n    svm_conf_mat.append(conf)\n    \n    clfReport = classification_report(y_test, y_pred)\n    svm_clas_repo.append(clfReport)\n    \n    print(conf)\n    print(clfReport)\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    acc_coarse.append(accuracy)\n    print('Accuracy: %f' % accuracy)\n    \n    precision = precision_score(y_test, y_pred, average='macro', zero_division=1)\n    print('Precision: %f' % precision)\n    prec_coarse.append(precision)\n    \n    recall = recall_score(y_test, y_pred, average='macro', zero_division=1)\n    print('Recall: %f' % recall)\n    recall_coarse.append(recall)\n    \n    f1 = f1_score(y_test, y_pred, average='macro', zero_division=1)\n    print('F1 score: %f' % f1)\n    f1_coarse.append(f1)","bb440cec":"svm_conf_mat=np.array(svm_conf_mat)\nsvm_clas_repo=np.array(svm_clas_repo)\n\nprint(\"Coarse class mean accuracy={}\".format(np.mean(acc_coarse)))\nprint(\"Coarse class mean precision={}\".format(np.mean(prec_coarse)))\nprint(\"Coarse class mean recall={}\".format(np.mean(recall_coarse)))\nprint(\"Coarse class mean f1={}\".format(np.mean(f1_coarse)))","5fbefdc0":"sumMat = np.zeros((7,7))\nfor mat in svm_conf_mat:\n    sumMat = np.add(sumMat,mat)\n\nsumMat = pd.DataFrame(sumMat, index=coarseLabels, columns=coarseLabels)\nprint(\"Cofusion matrix over 10-fold cross validation\")\nprint(sumMat)","4d3c567a":"Y_fine_class = np.array(y_fine_classEncoded.iloc[:,-1])\nfineLabels = np.unique(Y_fine_class)","cc1c761f":"parameters = {'alpha':[1e-1,1e-2],'loss':['hinge','log'],'max_iter':[10,20]}\n\nclassifier = SGDClassifier(random_state=42, tol=None)\ngrid = GridSearchCV(classifier, parameters, cv = 10, scoring = 'accuracy', verbose=1)\ngrid.fit(X_features[:1000], Y_fine_class[:1000])\n\nprint('======Best Parameters=====')\nprint(grid.best_params_)\nprint(grid.best_score_)","f2735606":"kf = StratifiedKFold(n_splits=10)\n\nsvm_conf_mat_fine = []\nsvm_clas_report_fine = []\nfold = 0\n\nacc_fine=[]\nprec_fine=[]\nrecall_fine=[]\nf1_fine=[]\n\nfor train_index, test_index in kf.split(X_features, Y_fine_class):\n    fold += 1\n    print(\"*\"*50 + \"Fold = \" + str(fold))\n    \n    X_train, X_test = X_features[train_index], X_features[test_index]\n    y_train, y_test = Y_fine_class[train_index], Y_fine_class[test_index]\n    \n    svmclassifier = SGDClassifier(loss='hinge', penalty='l2',alpha=0.01, random_state=13, max_iter=15, tol=None)\n    svmclassifier.fit(X_train, y_train)\n    \n    y_pred = svmclassifier.predict(X_test)\n    \n    conf = confusion_matrix(y_test, y_pred, labels=fineLabels)\n    svm_conf_mat_fine.append(conf)\n    \n    clfReport = classification_report(y_test, y_pred)\n    svm_clas_report_fine.append(clfReport)\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    acc_fine.append(accuracy)\n    print('Accuracy: %f' % accuracy)\n    \n    precision = precision_score(y_test, y_pred,average='macro', zero_division=1)\n    print('Precision: %f' % precision)\n    prec_fine.append(precision)\n    \n    recall = recall_score(y_test, y_pred,average='macro', zero_division=1)\n    print('Recall: %f' % recall)\n    recall_fine.append(recall)\n    \n    f1 = f1_score(y_test, y_pred,average='macro', zero_division=1)\n    print('F1 score: %f' % f1)\n    f1_fine.append(f1)\n\n    \nprint(\"Fine class mean accuracy={}\".format(np.mean(acc_fine)))\nprint(\"Fine class mean precision={}\".format(np.mean(prec_fine)))\nprint(\"Fine class mean recall={}\".format(np.mean(recall_fine)))\nprint(\"Fine class mean f1={}\".format(np.mean(f1_fine)))","fb2b7142":"sumMat = np.zeros((len(fineLabels),len(fineLabels)))\nfor mat in svm_conf_mat_fine:\n    sumMat = np.add(sumMat,mat)\n\nsumMat = pd.DataFrame(sumMat, index=fineLabels, columns=fineLabels)\n\nprint(\"Fine Class confusion matrix over 10-fold cross validation\")\npd.set_option('display.max_columns', 63)\nprint(sumMat.describe())","78fecca8":"print(travelTokensLemmatized[:10])","f2a643ed":"from gensim.models import Word2Vec\n\nskipgramModel = Word2Vec(min_count=1, \n                         sg=1, \n                         size=300,\n                         window=5,\n                        seed=3)\n\nskipgramModel.build_vocab(sentences = travelTokensLemmatized, \n                           progress_per=1000)\n\nskipgramModel.train(sentences = travelTokensLemmatized,\n                     total_examples=skipgramModel.corpus_count, \n                     epochs=10, \n                     report_delay=1)","79a34c39":"sentIndex = 0\nsentVectors = []\n\nfor sent in travelTokensLemmatized:\n    sentVec=np.zeros((300,))\n    for w in sent:\n        wordVector = skipgramModel.wv[w]\n        sentVec += wordVector\n    sentVectors.append(sentVec)\n\nprint(sentVectors[:5])","9ef954ab":"sentVectorsDf = pd.DataFrame(sentVectors)\nsentVectorsDf.describe()","d06105f5":"wordEmbeddingDf = pd.concat([df[0], sentVectorsDf, y_coarse_classEncoded, y_fine_classEncoded], axis=1)\nwordEmbeddingDf.head()","e2cbbd13":"X_wordEmbed = wordEmbeddingDf.iloc[:,1: -2]\nX_wordEmbed =  X_wordEmbed.to_numpy()\n\nY_coarse_class = np.array(y_coarse_classEncoded.iloc[:,-1])\ncoarseLabels = np.unique(Y_coarse_class)","8d08302b":"parameters = {'alpha':[0.1,0.3],'loss':['hinge','log'],'max_iter':[10,15]}\n\nclassifier = SGDClassifier(random_state=42, tol=None)\ngrid = GridSearchCV(classifier, parameters, cv = 10, scoring = 'accuracy', verbose=1)\ngrid.fit(X_wordEmbed[:2000], Y_coarse_class[:2000])\n\nprint('======Best Parameters=====')\nprint(grid.best_params_)\nprint(grid.best_score_)","0c7697f9":"kf = StratifiedKFold(n_splits=10)\n\nsvm_conf_mat = []\nsvm_clas_repo = []\nfold = 0\n\nacc_coarse=[]\nprec_coarse=[]\nrecall_coarse=[]\nf1_coarse=[]\n\nfor train_index, test_index in kf.split(X_wordEmbed, Y_coarse_class):\n    fold += 1\n    print(\"*\"*50 + \"Fold = \" + str(fold))\n    \n    X_train, X_test = X_wordEmbed[train_index], X_wordEmbed[test_index]\n    y_train, y_test = Y_coarse_class[train_index], Y_coarse_class[test_index]\n    \n    svmclassifier = SGDClassifier(max_iter=10, tol=1e-3, loss='hinge', penalty='l2', alpha=0.1, random_state=42)\n    svmclassifier.fit(X_train, y_train)\n    \n    y_pred = svmclassifier.predict(X_test)\n    \n    conf = confusion_matrix(y_test, y_pred, labels=coarseLabels)\n    svm_conf_mat.append(conf)\n    \n    clfReport = classification_report(y_test, y_pred)\n    svm_clas_repo.append(clfReport)\n    \n    print(conf)\n    print(clfReport)\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    acc_coarse.append(accuracy)\n    print('Accuracy: %f' % accuracy)\n    \n    precision = precision_score(y_test, y_pred, average='macro', zero_division=1)\n    print('Precision: %f' % precision)\n    prec_coarse.append(precision)\n    \n    recall = recall_score(y_test, y_pred, average='macro', zero_division=1)\n    print('Recall: %f' % recall)\n    recall_coarse.append(recall)\n    \n    f1 = f1_score(y_test, y_pred, average='macro', zero_division=1)\n    print('F1 score: %f' % f1)\n    f1_coarse.append(f1)\n    ","bc045e03":"svm_conf_mat=np.array(svm_conf_mat)\nsvm_clas_repo=np.array(svm_clas_repo)\n\nprint(\"Coarse class mean accuracy={}\".format(np.mean(acc_coarse)))\nprint(\"Coarse class mean precision={}\".format(np.mean(prec_coarse)))\nprint(\"Coarse class mean recall={}\".format(np.mean(recall_coarse)))\nprint(\"Coarse class mean f1={}\".format(np.mean(f1_coarse)))\n\nsumMat = np.zeros((7,7))\nfor mat in svm_conf_mat:\n    sumMat = np.add(sumMat,mat)\n\nsumMat = pd.DataFrame(sumMat, index=coarseLabels, columns=coarseLabels)\nprint(\"WordEmbedding Confusion matrix for coarse class over 10-fold cross validation\")\nsumMat","7fe6bd5a":"parameters = {'alpha':[0.01,0.001], 'max_iter':[20,80]}\n\nclassifier = SGDClassifier(random_state=42, tol=None, loss='log')\ngrid = GridSearchCV(classifier, parameters, cv = 10,  scoring = 'accuracy', verbose=1)\ngrid.fit(X_wordEmbed[:2000], Y_fine_class[:2000])\n\nprint('======Best Parameters=====')\nprint(grid.best_params_)\nprint(grid.best_score_)","1564060b":"Y_fine_class = np.array(y_fine_classEncoded.iloc[:,-1])\nfineLabels = np.unique(Y_fine_class)\n\nkf = StratifiedKFold(n_splits=10)\n\nsvm_conf_mat_fine = []\nsvm_clas_report_fine = []\nfold = 0\n\nacc_fine=[]\nprec_fine=[]\nrecall_fine=[]\nf1_fine=[]\n\nfor train_index, test_index in kf.split(X_wordEmbed, Y_fine_class):\n    fold += 1\n    print(\"*\"*50 + \"Fold = \" + str(fold))\n    \n    X_train, X_test = X_wordEmbed[train_index], X_wordEmbed[test_index]\n    y_train, y_test = Y_fine_class[train_index], Y_fine_class[test_index]\n    \n    svmclassifier = SGDClassifier(max_iter=90, tol=1e-3, loss='log', penalty='l2', alpha=0.001, random_state=42)\n    svmclassifier.fit(X_train, y_train)\n    \n    y_pred = svmclassifier.predict(X_test)\n    \n    conf = confusion_matrix(y_test, y_pred, labels=fineLabels)\n    svm_conf_mat_fine.append(conf)\n    \n    clfReport = classification_report(y_test, y_pred)\n    svm_clas_report_fine.append(clfReport)\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    acc_fine.append(accuracy)\n    print('Accuracy: %f' % accuracy)\n    \n    precision = precision_score(y_test, y_pred,average='macro', zero_division=1)\n    print('Precision: %f' % precision)\n    prec_fine.append(precision)\n    \n    recall = recall_score(y_test, y_pred,average='macro', zero_division=1)\n    print('Recall: %f' % recall)\n    recall_fine.append(recall)\n    \n    f1 = f1_score(y_test, y_pred,average='macro', zero_division=1)\n    print('F1 score: %f' % f1)\n    f1_fine.append(f1)","f54f3daa":"print(\"Fine class mean accuracy={}\".format(np.mean(acc_fine)))\nprint(\"Fine class mean precision={}\".format(np.mean(prec_fine)))\nprint(\"Fine class mean recall={}\".format(np.mean(recall_fine)))\nprint(\"Fine class mean f1={}\".format(np.mean(f1_fine)))\n\nsumMat = np.zeros((len(fineLabels),len(fineLabels)))\nfor mat in svm_conf_mat_fine:\n    sumMat = np.add(sumMat,mat)\n\nsumMat = pd.DataFrame(sumMat, index=fineLabels, columns=fineLabels)\n\nprint(\"Word embedding fine grained Class confusion matrix over 10-fold cross validation\")\nsumMat","1ea2991f":"import tensorflow as tf\nfrom scipy.sparse import hstack\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom gensim.models import Word2Vec\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping","6448336e":"from numpy import asarray\nembeddings_index = dict()\nf = open('\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","8dd1a6bd":"word_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(ppUntokenizedQsns)\nword_index = word_tokenizer.word_index\n\nvocab_size = len(word_tokenizer.word_index) + 1\n\nweight_matrix = np.zeros((vocab_size, 100))\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        weight_matrix[i] = embedding_vector","c2a95603":"preprocessed_questions_sequences = word_tokenizer.texts_to_sequences(ppUntokenizedQsns)\npreprocessed_questions_sequences_padded = pad_sequences(preprocessed_questions_sequences, maxlen=100, padding='post', truncating='post')","4e2abe73":"kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=19)\ncvscores = []\niteration = 0\n\ncv_accuracy_scores = []\ncv_conf_matrices = []\ncv_recall_scores = []\ncv_precision_scores = []\ncv_f1_scores = []\nlstm_conf_mat_coarse = []\n\nquestions_list = np.array(preprocessed_questions_sequences_padded.tolist());\ncategory_list = np.array(Y_coarse_class);\n\nfor train_index, test_index in kfold.split(questions_list,category_list):\n\n    iteration = iteration + 1\n    print(\"Fold =================================================================== \", iteration)\n\n    train_questions,test_questions=questions_list[train_index],questions_list[test_index]\n    train_category_labels,test_category_labels=category_list[train_index],category_list[test_index]\n    \n    model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=len(word_index)+1, input_length=100, output_dim=100, weights=[weight_matrix], trainable=False),\n    tf.keras.layers.SpatialDropout1D(0.3),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(7, activation='softmax')\n    ])\n \n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    earlystopCallback = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=0, mode='auto')\n    history = model.fit(train_questions, train_category_labels, epochs=50, batch_size=500, verbose=1, callbacks=[earlystopCallback])\n    \n    print(history.history['loss'])\n    print(history.history['accuracy'])\n\n    # Evaluate the model\n    predicted_category_labels = model.predict_classes(test_questions, verbose=0)  \n    \n    conf = confusion_matrix(test_category_labels, predicted_category_labels, labels=coarseLabels)\n    lstm_conf_mat_coarse.append(conf)\n    \n    accuracy = accuracy_score(test_category_labels, predicted_category_labels)\n    recall = recall_score(test_category_labels, predicted_category_labels,average=None, zero_division = 0)\n    precision = precision_score(test_category_labels, predicted_category_labels,average=None, zero_division = 0)\n    f1 = f1_score(test_category_labels, predicted_category_labels,average=None, zero_division = 0)\n    \n    cv_accuracy_scores.append(accuracy)\n    cv_recall_scores.append(recall)\n    cv_precision_scores.append(precision)\n    cv_f1_scores.append(f1)","b4bd2c78":"print(\"LSTM Accuracy={}\".format(np.mean(cv_accuracy_scores)))","edd7b8d1":"print(\"LSTM reacll={}\".format(np.mean(cv_recall_scores)))","43903177":"print(\"LSTM precision={}\".format(np.mean(cv_precision_scores)))","172dceb9":"print(\"LSTM f1={}\".format(np.mean(cv_f1_scores)))","99b671bb":"sumMat = np.zeros((len(coarseLabels),len(coarseLabels)))\nfor mat in lstm_conf_mat_coarse:\n    sumMat = np.add(sumMat,mat)\n\nsumMat = pd.DataFrame(sumMat, index=coarseLabels, columns=coarseLabels)\n\nprint(\"LSTM coarse class confusion matrix over 10-fold cross validation\")\nsumMat","5335e42f":"kfold = StratifiedKFold(n_splits=10, shuffle=True)\ncvscores = []\niteration = 0\n\ncv_accuracy_scores = []\ncv_conf_matrices = []\ncv_recall_scores = []\ncv_precision_scores = []\ncv_f1_scores = []\nlstm_conf_mat_fine = []\n\npreprocessed_qsns = np.array(preprocessed_questions_sequences_padded.tolist());\nfine_class_labels = np.array(Y_fine_class);\n\nfor train_index, test_index in kfold.split( preprocessed_qsns,fine_class_labels):\n\n    iteration = iteration + 1\n    print(\"Fold =================================================================== \", iteration)\n\n    train_questions,test_questions= preprocessed_qsns[train_index], preprocessed_qsns[test_index]\n    train_labels,test_labels=fine_class_labels[train_index],fine_class_labels[test_index]\n    \n    model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=len(word_index)+1, input_length=100, output_dim=100, weights=[weight_matrix], trainable=False),\n    tf.keras.layers.SpatialDropout1D(0.3),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(63, activation='softmax')\n    ])\n \n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    earlystopCallback = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=0, mode='auto')\n    history = model.fit(train_questions, train_labels, epochs=80, batch_size=500, verbose=1, callbacks=[earlystopCallback])\n    \n    print(history.history['loss'])\n    print(history.history['accuracy'])\n\n    # Evaluate the model\n    predicted_category_labels = model.predict_classes(test_questions, verbose=0)  \n    \n    conf = confusion_matrix(test_labels, predicted_category_labels, labels=fineLabels)\n    lstm_conf_mat_fine.append(conf)\n    \n    accuracy = accuracy_score(test_labels, predicted_category_labels)\n    recall = recall_score(test_labels, predicted_category_labels,average=None, zero_division = 0)\n    precision = precision_score(test_labels, predicted_category_labels,average=None, zero_division = 0)\n    f1 = f1_score(test_labels, predicted_category_labels,average=None, zero_division = 0)\n    \n    cv_accuracy_scores.append(accuracy)\n    cv_recall_scores.append(recall)\n    cv_precision_scores.append(precision)\n    cv_f1_scores.append(f1)","b1b23bd6":"print(\"LSTM Accuracy={}\".format(np.mean(cv_accuracy_scores)))","816d5257":"total = 0\nfor x in cv_recall_scores:\n    l = len(x)\n    s = np.sum(x)\n    total += s\/l\n\nprint(\"LSTM Recall=\" + str(total\/len(cv_recall_scores)))","98227de9":"total = 0\nfor x in cv_precision_scores:\n    l = len(x)\n    s = np.sum(x)\n    total += s\/l\n\nprint(\"LSTM precision=\" + str(total\/len(cv_precision_scores)))","4516f273":"total = 0\nfor x in cv_f1_scores:\n    l = len(x)\n    s = np.sum(x)\n    total += s\/l\n    \nprint(\"LSTM f1=\" + str(total\/len(cv_f1_scores)))","c1f2c8c8":"sumMat = np.zeros((len(fineLabels),len(fineLabels)))\nfor mat in lstm_conf_mat_fine:\n    sumMat = np.add(sumMat,mat)\n\nsumMat = pd.DataFrame(sumMat, index=fineLabels, columns=fineLabels)\n\nprint(\"LSTM fine class confusion matrix over 10-fold cross validation\")\nsumMat","15772acb":"### dataframe preparation for 10-fold cross validation","fc55ddd3":"# Feature Extraction","142f4357":"### SVM for Fine grained class with word embedding features","dbba0ac2":"\n### SVM coarse grained class with word embedding features","9e3b4987":"### Use Regexp Tokenizer to tokenize words from each travel question","a04e81d6":"### Extract Unigram TF-IDF features ","47c354d8":"# Data Preprocessing","5bca060a":"# SVM classifier","fb2fa2d9":"### Do a grid search first to find best parameters","ac76f241":"### LSTM for classification","124455f3":"### SVM classifier for fine grained class","eb19f0a0":"### Bigram TFIDF features","0d41b74b":"### SVM with Word2Vec word Embedding features","1529ac1a":"### Generate NER features","b0055ec7":"### Using Pre-Trained GloVe Embedding","94af3216":"### Preprocess input words with WordNetLemmatizer after removing stop words","73288421":"### Detokenize preprocessed words back into questions","17c2c57e":"### Label encoding two targets 'coarse_class' and 'fine_class'","3e6810a9":"# Create Dataframe concatenating above feature data frames","f55adb0e":"### Dependancy parsing Features by head word and each word","2ee92f3f":"### Generate POS tagged features from Lemmatized questions in TF-IDF format","0994cd33":"### LSTM on fine class","0acd8026":"### SVM classifier for coarse class"}}