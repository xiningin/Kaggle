{"cell_type":{"a082a473":"code","b93502b4":"code","7aa3ad87":"code","91911bc9":"code","b5670ad0":"code","c1757f36":"code","98575083":"code","e39f0f1f":"code","c33930a8":"code","fa87a604":"code","ab6381ed":"code","b3de02a1":"code","56b4249f":"code","6bdd16b8":"code","3a3f86ee":"code","b4f50c75":"code","abc6daf9":"code","050912a3":"code","e18be7c6":"code","2fc8fb94":"code","46d78cb7":"code","868958f1":"code","ab772cbb":"code","e9e26b67":"code","a9fdef01":"code","fbea1d6d":"code","20f7f3f1":"code","a9ae3e3b":"code","6adb6cc0":"code","6dd519f6":"code","2c3868e0":"code","874ca96e":"code","5fe3fb82":"code","03b13f74":"code","7c911415":"code","860c6016":"code","f6c76aa9":"code","cabe1c78":"code","80dddd03":"code","af39d049":"code","c3d486e6":"code","b3821732":"code","e2e64c59":"code","d5154fa5":"code","357928a8":"code","6373d278":"code","75e558f9":"code","37aa3dc0":"code","dbe520ee":"code","7bf85765":"code","1b854dd8":"code","90118e20":"code","568fe8e2":"code","e969c69e":"code","8ff0c61e":"code","2e50ccf4":"code","48b7aa27":"code","c1d3560a":"code","40088dc5":"code","d93f2c26":"code","152765ac":"code","987575c4":"code","830cc1ef":"code","5e8a6d8d":"code","153eb0d6":"code","e6263da0":"code","469a69ad":"code","f63a42ce":"code","1d6bd30f":"code","cbc3e9f9":"code","525e7b71":"code","f2c2b1ef":"code","f8b05dae":"code","6581cce9":"code","aa0387ba":"code","2e997373":"code","be8e5e68":"code","1273cf2b":"code","f580ca50":"code","c5792d8e":"code","101df6cc":"code","22260c6d":"code","74e94c1b":"code","d70c9593":"code","47ecfb32":"code","27636eff":"code","b0fba527":"code","0acfcc69":"code","f51bee5e":"code","00111ae7":"code","9542d453":"code","e238d4bf":"code","faa520bd":"code","b7e45d3a":"code","6315839d":"code","2eef7cbf":"code","e0491b98":"code","05c506cd":"code","b2c3f284":"code","f6ea3619":"code","58174688":"code","4aaa7335":"code","65e2df69":"code","8681c889":"code","7090e4f5":"code","7895b44d":"code","9c7da9e7":"code","898b4d32":"code","f4ee6420":"code","03630f6f":"code","33c9d1d3":"code","aa5d47bb":"code","dc67c653":"code","4fd9b3a5":"code","4cfbce25":"code","792a5c77":"code","5deba244":"code","191a48d6":"code","7bd1ba38":"code","c9ecd8cf":"code","964f1b34":"code","560741f5":"code","f9d6d0fe":"code","694ad947":"code","40d4bfde":"code","c0bb3508":"code","3dc655f4":"code","083d1b9b":"code","72d198ab":"code","69b5b641":"code","a5b1619c":"code","504009b8":"code","fcd8364f":"code","8fd29e36":"code","248d44d2":"code","a33ccabd":"code","16d52f41":"code","795c006c":"code","07acc762":"code","eaf76444":"code","b806da75":"code","b28156b8":"code","77bf42e5":"code","dbb44866":"code","25d49c9f":"code","1648b535":"code","2e6b48ad":"code","106e24a4":"code","62ba3eae":"code","a7a79490":"code","e25af5bd":"code","b502c76e":"code","af17bf10":"code","837531c1":"code","90b84aa0":"code","c1eb8ea2":"code","0f87ff47":"code","b47d5253":"code","4a426b29":"code","42a71661":"code","5c00d1de":"code","bb0fdfc8":"code","a1e4a74f":"code","0af1521b":"code","d8f70dc6":"code","232181ee":"code","d27b1959":"code","4229d768":"code","46ebb52e":"code","02882ba7":"code","b865e7b7":"code","018caf74":"code","9be2964f":"code","ca71a937":"code","6eaa7bc9":"code","5c5b8dce":"code","3dce4a23":"code","247407d4":"code","523cd0bc":"code","b246f3e7":"code","8cdd29a4":"code","a3ea96ed":"code","95b904d1":"code","5674d92d":"code","e0d2443e":"code","28a3aa44":"code","3f67cfe1":"code","48eff6be":"code","2a7fd5ee":"code","92ac7c24":"code","9d35b7b6":"code","bca77f41":"code","189a5c04":"code","7421a79d":"code","1ac83439":"code","247bd911":"code","3e88e67e":"code","3f0fa204":"code","25ae2688":"code","5fc0e9d5":"code","ed8bb15f":"code","491bfb47":"code","8850ca9a":"code","0e33b2ca":"code","c3948a64":"code","beed33d6":"code","97b4275e":"code","e7d45713":"code","7065d968":"code","f5a92c01":"code","759b6246":"code","3e0fa941":"code","41e25eb5":"code","13c26f18":"code","3fb9cd0d":"code","68418278":"code","807b55f9":"code","778b13ab":"code","94ed6e83":"code","d7f3a392":"code","c8c4e7d5":"code","98683975":"code","44c0d936":"code","71c5fb62":"code","bafaa238":"code","e8266cde":"code","f472a950":"code","6092f4a6":"code","1ab5e6f9":"code","7c352763":"code","e13a3672":"code","931b2398":"code","942e4824":"code","0dd27cbe":"code","ca9bb5f9":"code","8258009b":"code","bbded687":"code","c809548f":"code","2ace8e86":"code","8c3dadd3":"code","eb2479f4":"code","8f2e4e99":"code","667ea96c":"code","498ec393":"code","e04985fb":"code","d28d6f8c":"code","3ee8a320":"code","35ad37ce":"code","94fee23d":"code","f142d1af":"code","be9dafec":"code","00e0369f":"code","107a2854":"code","814476e6":"code","24eb6af3":"code","cbd89e03":"code","89416e63":"code","3023dc61":"markdown","8b72193a":"markdown","5eb5856a":"markdown","4aa01863":"markdown","4ceae7b0":"markdown","6e72132e":"markdown","9f30f601":"markdown","5933adb1":"markdown","e5e5740f":"markdown","d4826ada":"markdown","303bcd7a":"markdown","0e0608af":"markdown","2ccef830":"markdown","6811f890":"markdown","0902348e":"markdown","2d4873ae":"markdown","8ef19328":"markdown","e7f12cb1":"markdown","42458677":"markdown","d2cfafa2":"markdown","645acc5a":"markdown","a7944dae":"markdown","0dbd8483":"markdown","86141699":"markdown","0d5c34af":"markdown","f740ecd4":"markdown","de5ce644":"markdown","7a6eb404":"markdown","bf97b0c4":"markdown","201e126f":"markdown","4f4dd6ee":"markdown","6054da58":"markdown","20d6a8a3":"markdown","aae84e0a":"markdown","6b955827":"markdown","accaaf7c":"markdown","d57bf66a":"markdown","5b3b40d6":"markdown","f6546abc":"markdown","ca07a42c":"markdown","1ad1810e":"markdown","8093bbb9":"markdown","a81650c7":"markdown","75759e96":"markdown","507c6ce4":"markdown","3179ac69":"markdown","6b38fb34":"markdown","a833d239":"markdown","71caeb2f":"markdown","df3b0ad7":"markdown","22077ce1":"markdown","0bf1a3f9":"markdown","4372ebfb":"markdown","d2049de7":"markdown","e878d13f":"markdown","79cb2652":"markdown","fca425e2":"markdown","88c1a599":"markdown","ba7187ed":"markdown","5b58759c":"markdown","60567b0b":"markdown","3c1d4b8e":"markdown","25b5ce67":"markdown","52e16afe":"markdown","5bfe503f":"markdown","701e40d0":"markdown","5cec3280":"markdown","006774ae":"markdown","a4e42cde":"markdown","26ee2be2":"markdown","44a7a4ba":"markdown","ccc9dff3":"markdown","26416e20":"markdown","81132c03":"markdown","a4b5379d":"markdown","d55cff6b":"markdown","3b975803":"markdown","30771351":"markdown","3fc11ffa":"markdown","a304ff81":"markdown","85430889":"markdown","01c01543":"markdown","5b181000":"markdown","66d8c76e":"markdown","12ee5f82":"markdown","d1d37331":"markdown"},"source":{"a082a473":"# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline    \npd.options.display.float_format = '{:.2f}'.format\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b93502b4":"#Load the data into a dataframe\ntelecom = pd.read_csv(\"..\/input\/telecom-churn-data-set-for-the-south-asian-market\/telecom_churn_data.csv\", low_memory=False)","7aa3ad87":"telecom.head()","91911bc9":"telecom.shape","b5670ad0":"telecom.info()","c1757f36":"#mobile_number is unique\nprint(telecom.mobile_number.is_unique)\ntelecom.mobile_number.nunique()","98575083":"# Columns with more than 70% missing values\ncolmns_missing_data = round(100*(telecom.isnull().sum()\/len(telecom.index)), 2)\ncolmns_missing_data[colmns_missing_data >= 40]","e39f0f1f":"telecom.shape","c33930a8":"telecom.total_rech_data_6.fillna(value=0, inplace=True)\ntelecom.total_rech_data_7.fillna(value=0, inplace=True)\ntelecom.total_rech_data_8.fillna(value=0, inplace=True)\ntelecom.total_rech_data_9.fillna(value=0, inplace=True)#\ntelecom.av_rech_amt_data_6.fillna(value=0, inplace=True)\ntelecom.av_rech_amt_data_7.fillna(value=0, inplace=True)\ntelecom.av_rech_amt_data_8.fillna(value=0, inplace=True)\ntelecom.av_rech_amt_data_9.fillna(value=0, inplace=True)","fa87a604":"#Total recharge amounts for months 6 and 7\n#Total recharge amount logic = Total data recharge + Total recharge Amount. \n#if any of the data recharge columns are 0 then retain the total recharge amt column as is\n\ntelecom['total_rech_amt_6'] = np.where((telecom['total_rech_data_6'] != 0) & (telecom['av_rech_amt_data_6'] != 0),\n                                            telecom['total_rech_data_6']*telecom['av_rech_amt_data_6']+telecom['total_rech_amt_6'],\n                                            telecom['total_rech_amt_6'])\n\ntelecom['total_rech_amt_7'] = np.where((telecom['total_rech_data_7'] != 0) & (telecom['av_rech_amt_data_7'] != 0),\n                                            telecom['total_rech_data_7']*telecom['av_rech_amt_data_7']+telecom['total_rech_amt_7'],\n                                            telecom['total_rech_amt_7'])","ab6381ed":"# Filter high-value customers\ntelecom['av_rech_amt'] = (telecom[\"total_rech_amt_6\"] + \n                          telecom[\"total_rech_amt_7\"]) \/ 2.0\ncutoff = telecom.av_rech_amt.quantile(.70)\nprint('70 percentile of first two months avg recharge amount: ', cutoff)\ntelecom_hv = telecom[telecom['av_rech_amt'] >= cutoff]","b3de02a1":"telecom_hv.shape","56b4249f":"# We can drop total_rech_data_* and av_rech_amt_data_*\ndrop_data_columns = [\"total_rech_data_6\", \"total_rech_data_7\", \"total_rech_data_8\", \"total_rech_data_9\", \n                'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8', 'av_rech_amt_data_9']\ntelecom_hv.drop(drop_data_columns, axis=1, inplace=True)","6bdd16b8":"pd.set_option('display.max_rows', telecom_hv.shape[0]+1)","3a3f86ee":"def conditions(s):\n    if ((s['total_ic_mou_9'] <= 0) & (s['total_og_mou_9'] <= 0) & (s['vol_2g_mb_9'] <= 0) & (s['vol_3g_mb_9'] <= 0)):\n        return 1\n    else:\n        return 0","b4f50c75":"telecom_hv['Churn'] = telecom_hv.apply(conditions, axis=1)","abc6daf9":"telecom_hv = telecom_hv.loc[:,~telecom_hv.columns.str.endswith('_9')]\ntelecom_hv = telecom_hv.loc[:,~telecom_hv.columns.str.startswith('sep')]","050912a3":"telecom_hv.shape","e18be7c6":"churn_rate = (sum(telecom_hv['Churn'])\/len(telecom_hv['Churn'].index))*100\nchurn_rate","2fc8fb94":"imbalance = (sum(telecom_hv['Churn'] != 0)\/sum(telecom_hv['Churn'] == 0))*100\nimbalance","46d78cb7":"#Study the dataset\ntelecom_hv.describe()","868958f1":"nunique = telecom_hv.apply(pd.Series.nunique)\ncols_to_drop = nunique[nunique == 1].index\ncols_to_drop","ab772cbb":"telecom_hv.drop(cols_to_drop,axis=1,inplace=True)\ntelecom_hv.shape","e9e26b67":"# sum it up to check how many rows have all missing values\nprint(\"All null values:\", telecom_hv.isnull().all(axis=1).sum())\n# drop rows with 55% of missing data\ntelecom_hv = telecom_hv[(telecom_hv.isnull().sum(axis=1)\/telecom_hv.shape[1])*100 < 55]\nprint(\"Record Count after Row\/Column Data deletion:\", telecom_hv.shape[0])","a9fdef01":"#Create Bar Plot\nplt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.barplot(x = 'Churn', y = 'arpu_6', data = telecom_hv)\nplt.subplot(2,3,2)\nplt.ylabel('Av Rev. Month 7')\nsns.barplot(x = 'Churn', y = 'arpu_7', data = telecom_hv)\nplt.subplot(2,3,3)\nplt.ylabel('Av Rev. Month 8')\nsns.barplot(x = 'Churn', y = 'arpu_8', data = telecom_hv)\n","fbea1d6d":"#Create Bar Plot\nplt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.barplot(x = 'Churn', y = 'total_og_mou_6', data = telecom_hv)\nplt.subplot(2,3,2)\nsns.barplot(x = 'Churn', y = 'total_og_mou_7', data = telecom_hv)\nplt.subplot(2,3,3)\nsns.barplot(x = 'Churn', y = 'total_og_mou_8', data = telecom_hv)\n","20f7f3f1":"#Create Bar Plot\nplt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.barplot(x = 'Churn', y = 'total_ic_mou_6', data = telecom_hv)\nplt.subplot(2,3,2)\nsns.barplot(x = 'Churn', y = 'total_ic_mou_7', data = telecom_hv)\nplt.subplot(2,3,3)\nsns.barplot(x = 'Churn', y = 'total_ic_mou_8', data = telecom_hv)\n","a9ae3e3b":"#Create Bar Plot\nplt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.barplot(x = 'Churn', y = 'onnet_mou_6', data = telecom_hv)\nplt.subplot(2,3,2)\nsns.barplot(x = 'Churn', y = 'onnet_mou_7', data = telecom_hv)\nplt.subplot(2,3,3)\nsns.barplot(x = 'Churn', y = 'onnet_mou_8', data = telecom_hv)\n","6adb6cc0":"#Create Bar Plot\nplt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.barplot(x = 'Churn', y = 'offnet_mou_6', data = telecom_hv)\nplt.subplot(2,3,2)\nsns.barplot(x = 'Churn', y = 'offnet_mou_7', data = telecom_hv)\nplt.subplot(2,3,3)\nsns.barplot(x = 'Churn', y = 'offnet_mou_8', data = telecom_hv)\n","6dd519f6":"telecom_hv.shape","2c3868e0":"rech_data = telecom_hv.loc[:,telecom_hv.columns.str.contains('rech')]\ntot_data = telecom_hv.loc[:,telecom_hv.columns.str.contains('tot')]\namt_data = telecom_hv.loc[:,telecom_hv.columns.str.contains('amt')]\nic_mou_data = telecom_hv.loc[:,(telecom_hv.columns.str.contains('ic') & telecom_hv.columns.str.contains('mou'))]\nog_mou_data = telecom_hv.loc[:,(telecom_hv.columns.str.contains('og') & telecom_hv.columns.str.contains('mou'))]\nnet_mou_data = telecom_hv.loc[:,telecom_hv.columns.str.contains('net_mou')]\ndata3g = telecom_hv.loc[:,(telecom_hv.columns.str.contains('3g'))]\ndata2g = telecom_hv.loc[:,(telecom_hv.columns.str.contains('2g'))]","874ca96e":"rech_data.shape","5fe3fb82":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (25,25))\nsns.heatmap(rech_data.corr(),annot = True)","03b13f74":"tot_data.shape","7c911415":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (12,12))\nsns.heatmap(tot_data.corr(),annot = True)","860c6016":"amt_data.shape","f6c76aa9":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (10,10))\nsns.heatmap(amt_data.corr(),annot = True)","cabe1c78":"#Create scatter plot to understand distribution of amounts\nplt.figure(figsize=(25, 10))\nplt.subplot(2,3,1)\nsns.scatterplot(x = 'total_rech_amt_6', y = 'total_rech_amt_8', data = telecom_hv, hue = 'Churn')\nplt.subplot(2,3,2)\nsns.scatterplot(x = 'total_rech_amt_7', y = 'total_rech_amt_8', data = telecom_hv, hue = 'Churn')\nplt.subplot(2,3,3)\nsns.scatterplot(x = 'av_rech_amt', y = 'total_rech_amt_8', data = telecom_hv, hue = 'Churn')","80dddd03":"ic_mou_data.shape","af39d049":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (36,36))\nsns.heatmap(ic_mou_data.corr(),annot = True)","c3d486e6":"og_mou_data.shape","b3821732":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (40,40))\nsns.heatmap(og_mou_data.corr(),annot = True)","e2e64c59":"net_mou_data.shape\n","d5154fa5":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (6,6))\nsns.heatmap(net_mou_data.corr(),annot = True)","357928a8":"data3g.shape","6373d278":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (18,18))\nsns.heatmap(data3g.corr(),annot = True)","75e558f9":"data2g.shape","37aa3dc0":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (15,15))\nsns.heatmap(data2g.corr(),annot = True)","dbe520ee":"check_cols = (telecom_hv[telecom_hv == 0].count(axis=0)\/len(telecom_hv.index)*100)\ncheck_cols = check_cols[check_cols > 75].index\ncheck_cols","7bf85765":"check_cols = check_cols[check_cols != 'Churn']","1b854dd8":"telecom_n = telecom_hv.select_dtypes(include=np.number)","90118e20":"telecom_n.head()","568fe8e2":"telecom_n.shape","e969c69e":"# Columns with more than 70% missing values\ncolmns_missing_data = round(100*(telecom_n.isnull().sum()\/len(telecom_n.index)), 2)\ncols = colmns_missing_data[colmns_missing_data>1]","8ff0c61e":"cols","2e50ccf4":"telecom_cat = pd.DataFrame(telecom_n,columns = ['mobile_number','night_pck_user_6','night_pck_user_7','night_pck_user_8','fb_user_6','fb_user_7','fb_user_8'])\ntelecom_n.drop(['night_pck_user_6','night_pck_user_7','night_pck_user_8','fb_user_6','fb_user_7','fb_user_8'],axis=1,inplace=True)\n","48b7aa27":"telecom_cat.shape\n","c1d3560a":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=10, verbose=0)\nimp.fit(telecom_n)\nimputed_df = imp.transform(telecom_n)","40088dc5":"# Columns with more than 70% missing values\nnew_df = pd.DataFrame(imputed_df)\nnew_df.columns = telecom_n.columns\nnew_df.head()","d93f2c26":"telecom_n = pd.merge(new_df, telecom_cat, on='mobile_number', how='inner')\ntelecom_n.head()","152765ac":"telecom_nn = telecom_hv.select_dtypes(exclude=telecom_n.dtypes)","987575c4":"telecom_nn.head()","830cc1ef":"telecom_n['gp_avg_arpu'] = (telecom_n['arpu_6'] + telecom_n['arpu_7'])\/2","5e8a6d8d":"telecom_n['gp_avg_arpu'] = np.where((telecom_n['arpu_8'] > 0) & (telecom_n['gp_avg_arpu'] == 0),telecom_n['arpu_8'],telecom_n['gp_avg_arpu'])                              ","153eb0d6":"telecom_n.drop(['arpu_6','arpu_7'],axis=1,inplace=True)","e6263da0":"telecom_n['total_og_mou_gp'] = (telecom_n['total_og_mou_6'] + telecom_n['total_og_mou_7'])\/2","469a69ad":"telecom_n['total_og_mou_gp'] = np.where((telecom_n['total_og_mou_8'] > 0) & (telecom_n['total_og_mou_gp'] == 0),telecom_n['total_ic_mou_8'],telecom_n['total_og_mou_gp'])                              ","f63a42ce":"telecom_n.drop(['total_og_mou_6','total_og_mou_7'],axis=1,inplace=True)","1d6bd30f":"telecom_n['total_ic_mou_gp'] = (telecom_n['total_ic_mou_6'] + telecom_n['total_ic_mou_7'])\/2","cbc3e9f9":"telecom_n['total_ic_mou_gp'] = np.where((telecom_n['total_ic_mou_8'] > 0) & (telecom_n['total_ic_mou_gp'] == 0),telecom_n['total_ic_mou_8'],telecom_n['total_ic_mou_gp'])                              ","525e7b71":"telecom_n.drop(['total_ic_mou_6','total_ic_mou_7'],axis=1,inplace=True)","f2c2b1ef":"telecom_n['onnet_mou_gp'] = (telecom_n['onnet_mou_6'] + telecom_n['onnet_mou_7'])\/2\ntelecom_n['offnet_mou_gp'] = (telecom_n['offnet_mou_6'] + telecom_n['offnet_mou_7'])\/2","f8b05dae":"telecom_n.drop(['onnet_mou_6','onnet_mou_7','offnet_mou_6','offnet_mou_7'],axis=1,inplace=True)","6581cce9":"telecom_n.fillna(0,inplace=True)\ntelecom_n.shape","aa0387ba":"#telecom_n.dtypes\ntelecom_n['retain_factor_arpu'] = round(telecom_n['arpu_8'] \/ telecom_n['gp_avg_arpu'],2)\ntelecom_n['retain_factor_rech'] = round(telecom_n['total_rech_num_8'] \/ telecom_n['total_rech_num_7'],2)\ntelecom_n['retain_factor_rech'] = np.where(telecom_n['retain_factor_rech'] > 1,1,telecom_n['retain_factor_rech'])\ntelecom_n['retain_factor_arpu'] = np.where(telecom_n['retain_factor_arpu'] > 1,1,telecom_n['retain_factor_arpu'])","2e997373":"#Deduce a factor for retaining the customer\ntelecom_n['retain_factor'] = np.where((telecom_n['retain_factor_arpu'] > 0.5) & (telecom_n['retain_factor_rech'] > 0.5),0,1)\ntelecom_n.drop(columns = ['retain_factor_rech','retain_factor_arpu'], axis=1, inplace=True)","be8e5e68":"from sklearn.model_selection import train_test_split\n# Assign feature variable to X\nX = telecom_n.drop(['Churn','mobile_number'],axis=1)\n# Assign response variable to y\ny = telecom_n['Churn']\ny.head()","1273cf2b":"from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = preprocessing.StandardScaler().fit(X)\nXScale = scaler.transform(X)","f580ca50":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(XScale,y, train_size=0.7,test_size=0.3,random_state=100)","c5792d8e":"X_train.shape","101df6cc":"y_train_imb = (y_train != 0).sum()\/(y_train == 0).sum()\ny_test_imb = (y_test != 0).sum()\/(y_test == 0).sum()\nprint(\"Imbalance in Train Data:\", y_train_imb)\nprint(\"Imbalance in Test Data:\", y_test_imb)","22260c6d":"count_class = pd.value_counts(telecom_n['Churn'], sort=True)\ncount_class.plot(kind='bar',rot = 0)\nplt.title('Churn Distribution')\nplt.xlabel('Churn')","74e94c1b":"### Other Sampling Techniques just for playing around\n#from imblearn.combine import SMOTETomek\n#from imblearn.under_sampling import NearMiss\n#smk = SMOTETomek(random_state = 42)\n#X_trainb,y_trainb = smk.fit_sample(X_train,y_train)","d70c9593":"### Other Sampling Techniques just for playing around\n#from imblearn.over_sampling import RandomOverSampler\n#os = RandomOverSampler(sampling_strategy=1)\n#X_trainb,y_trainb = os.fit_sample(X_train,y_train)","47ecfb32":"from imblearn.over_sampling import SMOTE","27636eff":"smt = SMOTE(random_state = 2) \nX_trainb,y_trainb = smt.fit_sample(X_train,y_train)","b0fba527":"X_trainb.shape","0acfcc69":"y_trainb.shape","f51bee5e":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)","00111ae7":"#Doing the PCA on the train data\npca.fit(X_trainb)","9542d453":"pca.components_","e238d4bf":"colnames = list(X.columns)\npcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'PC3':pca.components_[2],'Feature':colnames})\npcs_df.head(10)","faa520bd":"%matplotlib inline\nfig = plt.figure(figsize = (20,20))\nplt.scatter(pcs_df.PC1, pcs_df.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(pcs_df.Feature):\n    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\nplt.tight_layout()\nplt.show()","b7e45d3a":"#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,9))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","6315839d":"#Using incremental PCA for efficiency - saves a lot of time on larger datasets\nfrom sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=50)","2eef7cbf":"df_train_pca = pca_final.fit_transform(X_trainb)\ndf_train_pca.shape","e0491b98":"#creating correlation matrix for the principal components\ncorrmat = np.corrcoef(df_train_pca.transpose())","05c506cd":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (50,30))\nsns.heatmap(corrmat,annot = True)","b2c3f284":"# 1s -> 0s in diagonals\ncorrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\nprint(\"max corr:\",corrmat_nodiag.max(), \", min corr: \", corrmat_nodiag.min(),)\n# we see that correlations are indeed very close to 0","f6ea3619":"#Applying selected components to the test data - 45 components\ntelecom_test_pca = pca_final.transform(X_test)\ntelecom_test_pca.shape","58174688":"#Training the model on the train data\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlearner_pca = LogisticRegression()\nmodel_pca = learner_pca.fit(df_train_pca,y_trainb)","4aaa7335":"#Making prediction on the test data\npred_probs_test = model_pca.predict_proba(telecom_test_pca)[:,1]\n\"{:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test))","65e2df69":"# Predict Results from PCA Model\nypred_pca = model_pca.predict(telecom_test_pca)","8681c889":"# Confusion matrix \nconfusion_PCA = metrics.confusion_matrix(y_test, ypred_pca)\nprint(confusion_PCA)","7090e4f5":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, ypred_pca))","7895b44d":"pca_again = PCA(0.90)","9c7da9e7":"df_train_pca2 = pca_again.fit_transform(X_trainb)\ndf_train_pca2.shape\n# we see that PCA selected 38 components","898b4d32":"#training the regression model\nlearner_pca2 = LogisticRegression()\nmodel_pca2 = learner_pca2.fit(df_train_pca2,y_trainb)","f4ee6420":"df_test_pca2 = pca_again.transform(X_test)\ndf_test_pca2.shape","03630f6f":"#Making prediction on the test data\npred_probs_test2 = model_pca2.predict_proba(df_test_pca2)[:,1]\n\"{:2.2f}\".format(metrics.roc_auc_score(y_test, pred_probs_test2))","33c9d1d3":"# Predict Results from PCA Model\nypred_pca2 = model_pca2.predict(df_test_pca2)","aa5d47bb":"# Confusion matrix \nconfusion_PCA = metrics.confusion_matrix(y_test, ypred_pca2)\nprint(confusion_PCA)","dc67c653":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, ypred_pca2))","4fd9b3a5":"pca_again = PCA(0.95)","4cfbce25":"df_train_pca3 = pca_again.fit_transform(X_trainb)\ndf_train_pca3.shape\n# we see that PCA selected 51 components","792a5c77":"#training the regression model\nlearner_pca3 = LogisticRegression()\nmodel_pca3 = learner_pca3.fit(df_train_pca3,y_trainb)","5deba244":"df_test_pca3 = pca_again.transform(X_test)\ndf_test_pca3.shape","191a48d6":"#Making prediction on the test data\npred_probs_test3 = model_pca3.predict_proba(df_test_pca3)[:,1]\n\"{:2.2f}\".format(metrics.roc_auc_score(y_test, pred_probs_test3))","7bd1ba38":"# Predict Results from PCA Model\nypred_pca3 = model_pca3.predict(df_test_pca3)","c9ecd8cf":"# Confusion matrix \nconfusion_PCA = metrics.confusion_matrix(y_test, ypred_pca3)\nprint(confusion_PCA)","964f1b34":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, ypred_pca3))","560741f5":"# Function to map the colors as a list from the input list of x variables\ndef pltcolor(lst):\n    cols=[]\n    for l in lst:\n        if l==0:\n            cols.append('red')\n        elif l==1:\n            cols.append('blue')\n        else:\n            cols.append('green')\n    return cols\n# Create the colors list using the function above\ncols=pltcolor(y_trainb)","f9d6d0fe":"\n%matplotlib inline\nfig = plt.figure(figsize = (12,10))\nplt.scatter(df_train_pca[:,0], df_train_pca[:,1], s=200,c = cols)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.gray()\nplt.show()","694ad947":"pca_last = PCA(n_components=10)\ndf_train_pca4 = pca_last.fit_transform(X_trainb)\ndf_test_pca4 = pca_last.transform(X_test)\ndf_test_pca4.shape","40d4bfde":"#training the regression model\nlearner_pca4 = LogisticRegression()\nmodel_pca4 = learner_pca4.fit(df_train_pca4,y_trainb)\n#Making prediction on the test data\npred_probs_test4 = model_pca4.predict_proba(df_test_pca4)[:,1]\n\"{:2.2f}\".format(metrics.roc_auc_score(y_test, pred_probs_test4))","c0bb3508":"# Create a copy\ntelecom_LR_wPCA = telecom_n.copy()","3dc655f4":"telecom_LR_wPCA.shape","083d1b9b":"telecom_LR_wPCA['Churn'].value_counts()","72d198ab":"plt.figure(figsize=(8,4))\ntelecom_LR_wPCA['Churn'].value_counts().plot(kind = 'bar')\nplt.ylabel('Count')\nplt.xlabel('Churn status')\nplt.title('Churn status Distribution',fontsize=14)","69b5b641":"# Create correlation matrix and check correlation greater than 0.95 adn drop those columns\ncorr_matrix = telecom_LR_wPCA.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]","a5b1619c":"print(to_drop)","504009b8":"# Drop high correlated features\ntelecom_LR_wPCA.drop(telecom_LR_wPCA[to_drop], axis=1, inplace=True)","fcd8364f":"telecom_LR_wPCA.shape","8fd29e36":"#telecom_LR_wPCA.dtypes\ntelecom_LR_wPCA['retain_factor_arpu'] = round(telecom_LR_wPCA['arpu_8'] \/ telecom_LR_wPCA['gp_avg_arpu'],2)\ntelecom_LR_wPCA['retain_factor_rech'] = round(telecom_LR_wPCA['total_rech_num_8'] \/ telecom_LR_wPCA['total_rech_num_7'],2)\ntelecom_LR_wPCA['retain_factor_rech'] = np.where(telecom_LR_wPCA['retain_factor_rech'] > 1,1,telecom_LR_wPCA['retain_factor_rech'])\ntelecom_LR_wPCA['retain_factor_arpu'] = np.where(telecom_LR_wPCA['retain_factor_arpu'] > 1,1,telecom_LR_wPCA['retain_factor_arpu'])","248d44d2":"#Deduce a factor for retaining the customer\ntelecom_LR_wPCA['retain_factor'] = np.where((telecom_LR_wPCA['retain_factor_arpu'] > 0.6) & (telecom_LR_wPCA['retain_factor_rech'] > 0.6),0,1)\ntelecom_LR_wPCA.drop(columns = ['retain_factor_rech','retain_factor_arpu'], axis=1, inplace=True)","a33ccabd":"telecom_LR_wPCA.retain_factor.describe()","16d52f41":"# Assign feature variable to X\nX = telecom_LR_wPCA.drop(['Churn','mobile_number'],axis=1)\nX.head()","795c006c":"# Assign the response variable to y\ny_LR = telecom_LR_wPCA[['Churn']]\ny_LR.head()","07acc762":"# Splitting the data into train and test\nX_train_LR, X_test_LR, y_train_LR, y_test_LR = train_test_split(X, y_LR, train_size=0.7, test_size=0.3, random_state=100)","eaf76444":"X_train_LR.shape","b806da75":"smt = SMOTE(random_state = 2) \nX_train_LR,y_train_LR = smt.fit_sample(X_train_LR,y_train_LR)","b28156b8":"X_train_LR.shape","77bf42e5":"data_imbalance = (y_train_LR != 0).sum()\/(y_train_LR == 0).sum()\nprint(\"Imbalance in Train Data: {}\".format(data_imbalance))","dbb44866":"#X_train_LR.head()\ncolumns = X.columns\nX_train_LR = pd.DataFrame(X_train_LR)\nX_train_LR.columns = columns\n","25d49c9f":"ycolumns = y_LR.columns\ny_train_LR = pd.DataFrame(y_train_LR)\ny_train_LR.columns = ycolumns","1648b535":"y_train_LR.shape","2e6b48ad":"from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_LR[columns] = scaler.fit_transform(X_train_LR[columns])\nX_train_LR.retain_factor.describe()","106e24a4":"X_train_LR.retain_factor.describe()","62ba3eae":"import statsmodels.api as sm","a7a79490":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","e25af5bd":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 45)             # running RFE with 38 variables as output\nrfe = rfe.fit(X_train_LR, y_train_LR)","b502c76e":"rfe.support_","af17bf10":"list(zip(X_train_LR.columns, rfe.support_, rfe.ranking_))","837531c1":"col = X_train_LR.columns[rfe.support_]","90b84aa0":"X_train_sm = sm.add_constant(X_train_LR[col])\nlogm2 = sm.GLM(y_train_LR,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","c1eb8ea2":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","0f87ff47":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","b47d5253":"###Creating a dataframe with the actual churn flag and the predicted probabilities\ny_train_pred_final = pd.DataFrame({'Churn':y_train_LR.Churn, 'Churn_Prob':y_train_pred})\ny_train_pred_final['MobileNumber'] = y_train_LR.index\ny_train_pred_final.head()","4a426b29":"y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","42a71661":"from sklearn import metrics","5c00d1de":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nprint(confusion)","bb0fdfc8":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))","a1e4a74f":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","0af1521b":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_LR[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train_LR[col].values, i) for i in range(X_train_LR[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","d8f70dc6":"#### There are a some variables with very high VIF. It's best to drop these variables as they aren't helping much with prediction and unnecessarily making the model complex.\n#### Lets drop all variables that have very high VIF i.e. above 9\ncol = vif[vif['VIF'] < 9]\ncol = col.Features","232181ee":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train_LR[col])\nlogm3 = sm.GLM(y_train_LR,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","d27b1959":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","4229d768":"y_train_pred[:10]","46ebb52e":"y_train_pred_final['Churn_Prob'] = y_train_pred","02882ba7":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","b865e7b7":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))","018caf74":"vif = pd.DataFrame()\nvif['Features'] = X_train_LR[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train_LR[col].values, i) for i in range(X_train_LR[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","9be2964f":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train_LR[col])\nlogm4 = sm.GLM(y_train_LR,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","ca71a937":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","6eaa7bc9":"y_train_pred[:10]","5c5b8dce":"y_train_pred_final['Churn_Prob'] = y_train_pred","3dce4a23":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","247407d4":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))","523cd0bc":"from sklearn.metrics import classification_report\nprint(classification_report(y_train_pred_final.Churn, y_train_pred_final.predicted))","b246f3e7":"X_test_LR = X_test_LR[col]\nX_test_LR.head()","8cdd29a4":"X_test_sm = sm.add_constant(X_test_LR)","a3ea96ed":"X_test_LR.shape","95b904d1":"# Making predictions on the test set\ny_test_pred = res.predict(X_test_sm)","5674d92d":"y_test_pred.shape","e0d2443e":"y_test_pred[:10]","28a3aa44":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","3f67cfe1":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","48eff6be":"# Assigning CustID to index\ny_test_df['MobileNumber'] = y_test_df.index","2a7fd5ee":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","92ac7c24":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final.head()","9d35b7b6":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Churn_Prob'})","bca77f41":"# Rearranging the columns\n#y_pred_final = y_pred_final.reindex_axis(['CustID','Churn','Churn_Prob'], axis=1)","189a5c04":"y_pred_final['final_predicted'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)","7421a79d":"y_pred_final.describe(percentiles=[.25, .5, .75, .90, .95, .99])","1ac83439":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Churn, y_pred_final.final_predicted)","247bd911":"y_pred_final.shape","3e88e67e":"confusion2 = metrics.confusion_matrix(y_pred_final.Churn, y_pred_final.final_predicted )\nconfusion2","3f0fa204":"from sklearn.metrics import classification_report\nprint(classification_report(y_pred_final.Churn, y_pred_final.final_predicted))","25ae2688":"# create a copy first\ntelecom_wPCA_RF = telecom_LR_wPCA.copy()","5fc0e9d5":"# Assign feature variable to X\nX_RF = telecom_wPCA_RF.drop(['Churn','mobile_number'],axis=1)\nX_RF.head()","ed8bb15f":"# Assign response variable to y\ny_RF = telecom_wPCA_RF['Churn']\ny_RF.head()","491bfb47":"# Splitting the data into train and test\nX_train_RF, X_test_RF, y_train_RF, y_test_RF = train_test_split(X_RF, y_RF, train_size=0.7, test_size=0.3, random_state=100)","8850ca9a":"smt = SMOTE(random_state = 2) \nX_train_RF,y_train_RF = smt.fit_sample(X_train_RF,y_train_RF)","0e33b2ca":"X_train_RF.shape","c3948a64":"X_train_RF = pd.DataFrame(X_train_RF)\nX_train_RF.columns = X_RF.columns","beed33d6":"y_train_RF.shape","97b4275e":"from sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel","e7d45713":"numerics = ['int16','int32','int64','float16','float32','float64']\nnumerical_vars = list(X_train_RF.select_dtypes(include=numerics).columns)\nX_train_RF = X_train_RF[numerical_vars]\nX_train_RF.shape","7065d968":"Linear_SVC = LinearSVC(C=0.1, penalty=\"l1\", dual=False).fit(X_train_RF, y_train_RF)\nlasso_model = SelectFromModel(Linear_SVC, prefit=False)\nlasso_model.fit(scaler.transform(X_train_RF.fillna(0)), y_train_RF)\nlasso_model.get_support()","f5a92c01":"np.sum(lasso_model.estimator_.coef_ == 0)","759b6246":"deleted_vars = X_train_RF.columns[(lasso_model.estimator_.coef_ == 0).ravel().tolist()]\ndeleted_vars","3e0fa941":"#perform the same operation in the Test Data set for matching the columns\nX_train_RF.drop(columns = deleted_vars,inplace=True,axis=1)\nX_test_RF.drop(columns = deleted_vars,inplace=True,axis=1)\n","41e25eb5":"X_train_RF.shape","13c26f18":"# Importing random forest classifier from sklearn library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Running the random forest with default parameters.\nrfc_d = RandomForestClassifier()","3fb9cd0d":"# fit\nrfc_d.fit(X_train_RF,y_train_RF)","68418278":"# Making predictions\npredictions = rfc_d.predict(X_test_RF)","807b55f9":"# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score","778b13ab":"# Let's check the report of our default model\nprint(classification_report(y_test_RF,predictions))","94ed6e83":"# Printing confusion matrix\nprint(confusion_matrix(y_test_RF,predictions))","d7f3a392":"print(accuracy_score(y_test_RF,predictions))","c8c4e7d5":"importances = list(rfc_d.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(X_train_RF.columns, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","98683975":"# GridSearchCV to find optimal min_samples_leaf\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV","44c0d936":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(4, 10, 2)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\", return_train_score=True)\nrf.fit(X_train_RF, y_train_RF)","71c5fb62":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","bafaa238":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","e8266cde":"# GridSearchCV to find optimal n_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'n_estimators': range(50, 200, 50)}\n\n# instantiate the model (note we are specifying a max_depth)\nrf = RandomForestClassifier(max_depth=6)\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"precision\", return_train_score=True)\nrf.fit(X_train_RF, y_train_RF)","f472a950":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","6092f4a6":"# plotting accuracies with n_estimators\nplt.figure()\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","1ab5e6f9":"# GridSearchCV to find optimal max_features\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_features': [ 8, 12, 16, 20, 24]}\n\n# instantiate the model\nrf = RandomForestClassifier(max_depth = 6)\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                    \n                   scoring=\"accuracy\", return_train_score=True)\nrf.fit(X_train_RF, y_train_RF)","7c352763":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","e13a3672":"# plotting accuracies with max_features\nplt.figure()\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","931b2398":"\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(30, 200, 50)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds,                   \n                   scoring=\"accuracy\", return_train_score=True)\nrf.fit(X_train_RF, y_train_RF)","942e4824":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","0dd27cbe":"# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","ca9bb5f9":"# model with the best hyperparameters\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=10,\n                             min_samples_leaf=50, \n                             min_samples_split=200,\n                             max_features=22,\n                             n_estimators=100)","8258009b":"# fit\nrfc.fit(X_train_RF,y_train_RF)","bbded687":"# predict\npredictions = rfc.predict(X_test_RF)","c809548f":"# evaluation metrics\nfrom sklearn.metrics import classification_report,confusion_matrix","2ace8e86":"print(classification_report(y_test_RF,predictions))","8c3dadd3":"print(confusion_matrix(y_test_RF,predictions))","eb2479f4":"print(accuracy_score(y_test_RF,predictions))","8f2e4e99":"importances = list(rfc.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(X_train_RF.columns, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","667ea96c":"import xgboost as xgb","498ec393":"x_xgboost, y_xgboost = telecom_n.drop(['Churn'],axis=1),telecom_n[['Churn']]","e04985fb":"#Create a matrix for identifying important predictors\ndata_dmatrix = xgb.DMatrix(data=x_xgboost,label=y_xgboost)","d28d6f8c":"#separate the data into train and test\nX_train_xg, X_test_xg, y_train_xg, y_test_xg = train_test_split(x_xgboost, y_xgboost, test_size=0.3, random_state=123)","3ee8a320":"#Crate XGBoost classifer model\nxg_class = xgb.XGBClassifier(objective ='reg:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)","35ad37ce":"#Train and Predict based on the model\nxg_class.fit(X_train_xg,y_train_xg)\n\npreds = xg_class.predict(X_test_xg)","94fee23d":"print(accuracy_score(y_test_xg,preds))","f142d1af":"print(confusion_matrix(y_test_xg,preds))","be9dafec":"params = {\"objective\":\"reg:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=5,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=\"auc\", as_pandas=True, seed=123)","00e0369f":"cv_results.head()","107a2854":"#Perform KFold cross validation to obtain a better meausre of Accuracy\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(xg_class, x_xgboost, y_xgboost, cv=kfold)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","814476e6":"print(classification_report(y_test_xg,preds))","24eb6af3":"xg_class1 = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)","cbd89e03":"import matplotlib.pyplot as plt\n\nxgb.plot_tree(xg_class1,num_trees=0)\nplt.figure(figsize=(50,50))\nplt.show()","89416e63":"xgb.plot_importance(xg_class1)\nplt.rcParams['figure.figsize'] = [100, 100]\nplt.show()","3023dc61":"### Balance data set by oversampling","8b72193a":"#### Observations\n\n- Total Incoming minutes of usage is almost entirely explained by the LOCAL call usage and not a lot by the STD calls\n- Total Incoming minutes of usage of month 8 is also correlated to the month 7. Indicating that if a customer has High MOU in       month 7 then they will continue to have High MOU in month 8\n- The STD Incoming MOU is fully explained by the T2M Minutes of Usage\n- High Correlation between Incoming T2T Usage for Months 6 and 7 and Months 7 and 8","5eb5856a":"#### Drop the Churn Phase Data set after Label Derivation as per Problem Instruction","4aa01863":"##### Numeric Features","4ceae7b0":"### Logistic Regression Algorithm with RFE","6e72132e":"#### Observations\n- High correlation between Average Recharge Amount and Rechage amounts for 6 and 7\n- This is expected as the recharge amount is calculated for purpose of filtering high value customers\n- There is high correlation 80% between data recharge for month 7 and recharge for month 8. \n- Any factor that has correlation with month 8 is probably correlated to the churn prediction","9f30f601":"#### Feature Generation - Introduce new Feature called Retain Factor\n- Retain Factor is calculated as the ratio of the Bad phase Average Revenue \/ Good Phase Average revenue\n- And the Ratio of the number of recharges in month 8 Vs Month 7","5933adb1":"#### The Different operator\/network Calls Drop in month 8 indicates Churn\n#### It would benefit if we Merge the months 6 and 7 into an average number indicating Good Phase","e5e5740f":"#### Date Format Alignment","d4826ada":"#### Visualize the data to see if we can spot any patterns","303bcd7a":"\n#### Understand the Churn Rate & Imbalance","0e0608af":"#### Get List of Important Features from the Decision Tree Classifier","2ccef830":"## Data Preparation & Understanding ","6811f890":"#### Get numerical feature importances","0902348e":"### Box Plots, Bar Plots, Scatter plots and Correlation Matrix","2d4873ae":"#### Observations\n\n- Very High correlation between the Recharge Sachets and the Count of Recharges for all months","8ef19328":"- Training accuracy using RFE is Approx. 0.853","e7f12cb1":"#### If there are any NAN values then fill them with 0","42458677":"#### Handle data imbalance by Performing SMOTE oversampling on the data set\n","d2cfafa2":"#### Observations\n\n- Total outgoing minutes of usage is almost entirely explained by the Std calls usage and not a lot by the Local calls\n- Total Outgoing minutes of usage of month 8 is also correlated to the month 7. Indicating that if a customer has High MOU in       month 7 then they will continue to have High MOU in month 8\n- The STD Outgoing MOU is highly correlated to the T2T Minutes of Usage\n- High Correlation between OutGoing T2T Usage for Months 6 and 7 and Months 7 and 8","645acc5a":"#####                                                                                      **Fin**","a7944dae":"#### Feature Generation - Introduce new Feature called Retain Factor\n- Retain Factor is calculated as the ratio of the Bad phase Average Revenue \/ Good Phase Average revenue\n- And the Ratio of the number of recharges in month 8 Vs Month 7","0dbd8483":"#### Tuning max_features","86141699":"- The machine Learning model with **XGBoosting** Algorithm has been chosen as the best\n- The reason is that the Accuracy scores and the precision \/ recall scores are the highest of all the algorithms\n- The performance of the algorithm is also better than most other algorithms","0d5c34af":"#### The Same Operator\/network Calls Drop in month 8 indicates Churn","f740ecd4":"## Data Prep, Exploratory Data Analysis & Feature Generation","de5ce644":"#### Parameter reduction using L1 LinearVector Classifier","7a6eb404":"#### Observations\n\n- No Correlation between ONNET and OFFNET Minutes of usage\n- High correlation between months 7 and 8 both for ONNET and OFFNET usage","bf97b0c4":"#### Making predictions on the Test Data set","201e126f":"- Drop the individual incoming usage month data as it's redundant","4f4dd6ee":"#### Model 3 - Let PCA Choose the number of Principal Components explaining 95% of Variance","6054da58":"#### Creating correlation matrix for the principal components - we expect little to no correlation","20d6a8a3":"#### Observations\n\n- 70% correlation between Average revenue per user and the 3G Volume of data usage for all Months","aae84e0a":"#### Default Hyperparameters - Fit the Random Forest with default hyperparameters","6b955827":"#### Extract all other columns separately for Plotting the correlation and observing highly correlated variables\n#### Different metrics include Totals, Amounts, Minutes of Usage, OFFNET & ONNNET, 2g and 3g Data sets","accaaf7c":"#### Tuning n_estimators","d57bf66a":"#### Model 2 - Let PCA Choose the number of Principal Components explaining 90% of Variance","5b3b40d6":"### Feature Standardisation","f6546abc":"- Drop the individual month Outgoing usage data as it's redundant","ca07a42c":"### From a total of 99999 records 30001 Records satisfy the High Value Customers\n### The churn prediction will be executed on the high value customers","1ad1810e":"- There are lot of missing values for the Data and the Amt_Data Columns indicating that no recharge was done on that month\n- The NaN values should be replaced by 0","8093bbb9":"## Conclusions of RFE & Logistic Regression Algorithm\n\n- The Model Accuracy Score that the accuracy on the Test Data Set is High i.e. 0.913\n- The confusion matrix is better than the PCA Algorithm\n- There were other models created different Hyperparameter VIF settings for e.g. VIF<5 but the accuracy and the precision\/recall was not very good\n- Hence a Model with a VIF < 9 and and set of 25 variables is chosen here\n- The precision and the recall for Churn Probability is still lower than optimal as there are a few False Positives and False   Negatives\n\n#### The main predictor variables for Telecom Churn are\n\n- total_ic_mou_8\n- onnet_mou_8\n- std_og_t2m_mou_8\n- arpu_2g_8\n- total_rech_num_8\n- loc_ic_t2m_mou_7\n- max_rech_data_8\n- loc_ic_t2m_mou_6\n- total_rech_num_7\n- std_ic_t2t_mou_8\n- count_rech_3g_8\n- retain_factor\n- std_ic_t2t_mou_7\n- gp_avg_arpu\n- loc_ic_t2t_mou_7\n- aug_vbc_3g\n- count_rech_2g_8\n- last_day_rch_amt_8\n- vol_2g_mb_8\n- aon\n","a81650c7":"#### Rows with more than 55% of data missing","75759e96":"- If a Customer is only joining in the Bad Phase assign the same valus to the Good Phase","507c6ce4":"#### Basis transformation - getting the data onto our PCs\n","3179ac69":"## PCA Model with Logistic Regression Conclusions\n- PCA way of selecting variables is simple and easy\n- We choose Model1 using 50 components as the best fit model due to the Confusion Matrix and the Accuracy Scores\n- The False Positives are still quite High\n\n- **Confusion Matrix for Model 1**\n\n   6818     1445\n\n   137      571\n   \n   \n- **Model Accuracy**\n\n    88%\n \n- **Classification Report for Model 1**\n\n               precision    recall  f1-score   support\n\n           0       0.98      0.83      0.90      8263\n           1       0.28      0.81      0.42       708\n\n   \n ","6b38fb34":"##### The Average Revenue per user metric Drop in month 8 indicates Churn","a833d239":"**Fitting the final model with the best parameters obtained from grid search.**","71caeb2f":"#### Consider dropping columns where most of the values are 0 i.e. Greater than 75% ","df3b0ad7":"#### Few columns have a unique value in all the rows\n#### They cannot be uesd to predict any variance between the data set\n#### it makes intuitive sense to drop these columns","22077ce1":"### Business Problem Overview\n\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n\nFor many incumbent operators, retaining high profitable customers is the number one business goal.\nTo reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n\n### Business Ojective\n\nThe business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.\n\nThus, churn prediction is usually more critical (and non-trivial) for prepaid customers, and the term \u2018churn\u2019 should be defined carefully. Prepaid is also the most common model in India and southeast Asia, while postpaid is more common in Europe in North America.\n\n \n\nThis project is based on the Indian and Southeast Asian market.\n\n### Solution Overview\n\nThe project starts with understanding the data set,  dropping unnecessary features, deriving features and performing exploratory data analysis. Where possible and required IterativeImputer from SKLearn is used to fill in gaps of numerical data by regression\nThe project is limited to High Value customers and there is a set definition to drive High Value Customers based on the data set. This definition is applied. The resultant data set is around 30K entries. \n\nFor the south asian market it is enough if a good enough Churn prediction is performed on the high value customers\nThe below algorithms have been applied on the data set and various models created and compared.\n\n**Principal Component Analysis  (PCA) with Logistic Regression** \n\n**Logistic Regression with Recursive Feature Elimination**\n\n**Random Forest algorithm based model with Hyper Parameter tuning**\n\n**Gradient Boosting - XGBoost algorithm**\n\nAfter this the results are compared based on the Confusion Matrix and Accuracy and a good model chosen.\nThen based on the model chosen the main features that affect Churn are identified so that some business recommendations can be made\n\n\n","0bf1a3f9":"### Columns with 70% missing data","4372ebfb":"#### Tuning min_samples_leaf","d2049de7":"### Check unique values","e878d13f":"- Drop the individual ONNET and OFFNET usage month data as it's redundant","79cb2652":"##### Non Numeric Variables","fca425e2":"### Understand the data imbalance","88c1a599":"#### The Outgoing minutes Drop in month 8 indicates Churn","ba7187ed":"#### Tuning max_depth\nLet's try to find the optimum values for max_depth","5b58759c":"- Drop the individual month ARPU data as it's redundant","60567b0b":"###  Derive Churn Label using the 'Churn Phase' data \n#### Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase\n- 1 Churn\n- 0 No Churn","3c1d4b8e":"### Filter High Value Customers\n#### Define high-value customers as follows: \n- Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).","25b5ce67":"### Derived Features based on the Exploratory Data Analysis performed in the previous step\n- 1. Average ARPU during months 6 and 7\n- 2. Average OG minutes of usage during months 6 and 7\n- 3. Average IC Minutes of usage during months 6 and 7\n- 4. Average OFFNET and ONNET minutes of usage for months 6 and 7","52e16afe":"#### Assessing the model with StatsModels","5bfe503f":"## XGBoost - Queen Bee Algorithm!","701e40d0":"#### Looks like approx. 50 components are enough to describe 90% of the variance in the dataset\n- We'll choose 50 components for our modeling","5cec3280":"#### The above columns can be dropped as they have not meaningful either for high value customers or for Churn Labelling ","006774ae":"#### Observations\n- Some of this correlation is the same as the First Recharge Amount correlation\n- There is also higher correlation between the Max Recharge Amount in month 8 (Bad Phase) and the Last Day Recharge Amount\n- This could indicate that if a customer is not going to Churn then they Recharge for a higher amount in month 8 ","a4e42cde":"#### Perform L1 Regularisation using LinearSVC Algorithm to do dimensionality reduction\n#### This is a feature available as part of the SciKit Learn Llibrary\n- https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html","26ee2be2":"#### Since we are not predicting any churn based on the above Date variables - is it safe to drop them? ","44a7a4ba":"#### We get good results with the Chosen Principal components and almost close to it with just 10 Principal Components.","ccc9dff3":"## Perform PCA and Predict Churn","26416e20":"## Conclusions of Random Forest Algorithm\n\n- The Model Accuracy Score that the accuracy on the Test Data Set is very High i.e. 0.93\n- The model was tuned with a number of Hyperparameters for the yield\n\n#### The main predictor variables for Telecom Churn are\n\n- loc_ic_mou_8\n- total_ic_mou_8\n- loc_ic_t2m_mou_8\n- last_day_rch_amt_8\n- max_rech_data_8\n- loc_ic_t2t_mou_8\n- max_rech_amt_8\n- count_rech_2g_8\n- total_og_mou_8\n- loc_og_t2t_mou_8\n- total_rech_num_8\n\n\n","81132c03":"- If the Ratio between the ARPU for the bad phase and the good phase is > than 0.6\n  and if the Ratio of the Number of recharges is > 0.6 \n- Then the consideration is that if the customer retention ratio is High then the user is likely not to Churn","a4b5379d":"#### Feature Selection Using RFE","d55cff6b":"#### The Incoming minutes usage Drop in month 8 indicates Churn","3b975803":"- If a Customer is only joining in the Bad Phase assign the same valus to the Good Phase","30771351":"## Multiple Logistic Regression Models with the Principal Components\n#### Model 1 - Use the No. of Principal Components determined by PCA","3fc11ffa":"#### There is no correlation between any two components! \n","a304ff81":"- Churn Rate: We have 8.14% Churn rate\n- Imbalance ratio of 8.66 %","85430889":"#### Observations\n- There is greater than 70% and some cases 82% correlation between months 7 and 8 regarding Incoming & Outgoing minutes of usage\n- This is probabaly due to the fact that if there is heavy usage in month 7 then subsequently in month 8 there is also heavy usage - The cusotmer will not churn if there is heavy usage and vice versa","01c01543":"#### Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0","5b181000":"## Alternate Model with Random Forest Algorithm","66d8c76e":"#### PCA with 10 Principal Components just to illustrate the difference\n- Model 4","12ee5f82":"##### Let's check the VIFs again","d1d37331":"#### Now lets look at the list of hyperparameters which we can tune to improve model performance."}}