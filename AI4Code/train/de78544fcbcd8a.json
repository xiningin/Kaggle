{"cell_type":{"f371a210":"code","47d3a191":"code","f2d8eb48":"code","a6c2bb1e":"code","c70b263f":"code","b719c4a8":"code","843439c2":"code","2059ae6c":"code","0f574660":"code","e62a15a8":"code","8b78a8bd":"code","a8577ef9":"markdown","64e4e767":"markdown","f6b396fa":"markdown","0dbe58ed":"markdown","23d123e1":"markdown","7d001969":"markdown"},"source":{"f371a210":"\nimport numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom time import time \n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntarget= train['target']\ntrain = train.drop(['target','id'], axis = 1)\ntest = test.drop(['id'], axis = 1)\n\n","47d3a191":"params_LGBM = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'num_leaves': 4,\n    'learning_rate': 0.012,\n    'feature_fraction': 0.6,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 1,\n    'verbose': 0,\n    'lambda_l1':0.4,\n    'lambda_l2':0.9,\n    'min_data_in_leaf': 2,\n    'max_bin': 25,\n    'min_data_in_bin':2    \n}\nparams_XGB =  {\n    'booster': 'gbtree',\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'max_depth': 2,\n    'eta': 0.012,\n    'subsample': 0.7,    \n    'verbosity': 0,\n    'alpha':0.4,\n    'lambda':0.9,\n    'colsample_bytree': 0.8,\n    'colsample_bylevel': 0.8,\n    'colsample_bynode': 0.8,\n   # 'three_method':'hist'    \n}  \n","f2d8eb48":"def cv_LGBM(train,target,params,rounds):      \n    lgb_train = lgb.Dataset(train, target)\n    hist = lgb.cv(\n                params,\n                lgb_train,\n                num_boost_round=rounds\n                   )   \n    scores = pd.DataFrame.from_dict(hist)\n    return(scores)\n\ndef train_LGBM(train,target,params,rounds):\n    lgb_train = lgb.Dataset(train,target)\n    booster = lgb.train(\n        params,lgb_train,\n        num_boost_round = rounds,\n        verbose_eval = False\n    )\n    return(booster)\n\ndef cv_XGB(train,target,params,rounds):  \n    xgb_train = xgb.DMatrix(train, label = target)       \n    hist = xgb.cv(\n        params,\n        xgb_train,\n        num_boost_round=rounds,\n        stratified = True)  \n    scores = pd.DataFrame.from_dict(hist)\n    return(scores)\n\ndef train_XGB(train,target,params,rounds):  \n        xgb_train = xgb.DMatrix(train, label = target)    \n        booster = xgb.train(\n            params,\n            xgb_train,\n            num_boost_round = rounds, )\n        return(booster)\n    \n\n","a6c2bb1e":"scores_LGBM = cv_LGBM(train,target,params_LGBM,rounds = 3000)\n    \nscores_XGB = cv_XGB(train,target,params_XGB, rounds = 3000)\n\nscores = pd.DataFrame()\nscores['XGB'],scores['LGBM'] = scores_XGB['test-auc-mean'],scores_LGBM['auc-mean']\nplt.plot(scores)\nplt.legend(labels = ('XGB','LGMB'))\nprint(scores[-10:])\n","c70b263f":"scaler = StandardScaler().fit(train+test) #Shoud only train be fitted? Using train+test gives improvement.\n\ntrain_scaled = scaler.transform(train)\ntest_scaled = scaler.transform(test)\n\npca = PCA(n_components = 50)\ntrain_pca = pca.fit_transform(train_scaled)\ntest_pca = pca.transform(test_scaled)\n\nscores_LGBM = cv_LGBM(train_pca,target,params_LGBM,rounds = 1000)    \nscores_XGB = cv_XGB(train_pca,target,params_XGB, rounds = 1000)\n\nscores = pd.DataFrame()\nscores['XGB'],scores['LGBM'] = scores_XGB['test-auc-mean'],scores_LGBM['auc-mean']\n\nplt.plot(scores)\nplt.legend(labels = ('XGB','LGMB'))\nprint(scores[-10:])","b719c4a8":"# Stolen from https:\/\/www.kaggle.com\/tboyle10\/feature-selection\nbooster = train_LGBM(train,target,params = params_LGBM, rounds = 1000)\n\nfeature_importance = booster.feature_importance(importance_type = 'gain')\nsorted_idx = np.argsort(feature_importance)\n#print(sorted_idx)\nplot_idx = sorted_idx[-20:]\n\npos = np.arange(plot_idx.shape[0]) + .5\nplt.barh(pos,feature_importance[plot_idx])\nplt.yticks(pos,plot_idx)\n\nplt.title('Feature Importance', fontsize=20)\n\nplt.show()\n\n","843439c2":"reduced_params_LGBM = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'num_leaves': 6,\n    'learning_rate': 0.012,\n    'feature_fraction': 0.4,\n    'bagging_fraction': 0.35,\n    'bagging_freq': 2,\n    'verbose': 0,\n    'lambda_l1':0.5,\n    'lambda_l2':0.9,\n    'min_data_in_leaf': 2,\n    'max_bin': 200,\n    'min_data_in_bin':2    \n}","2059ae6c":"#Returns a booster with n less features\ndef removeAndRun(n_features,train,target,test):\n    #Train boosters\n    lgbm = train_LGBM(train,target, params = params_LGBM, rounds = 1000)\n    xgbm = train_XGB(train,target, params = params_XGB, rounds = 100)\n    \n    #Finding and removing least important features in lightgbm \n    feature_importance = lgbm.feature_importance(importance_type = 'gain')\n    sorted_idx = np.argsort(feature_importance)\n    remove = sorted_idx[:n_features]\n   \n    train_reduced = train\n    test_reduced = test\n    for index in remove:\n        train_reduced = train_reduced.drop(str(index),axis = 1)\n        test_reduced = test_reduced.drop(str(index),axis = 1)\n        \n    #Re-train\n    lgbm = train_LGBM(train_reduced,target, params = reduced_params_LGBM, rounds = 5000)\n    xgbm = train_XGB(train_reduced,target, params = params_XGB, rounds = 5000)\n    \n    return lgbm,xgbm,train_reduced,test_reduced\n\nlgbm,xgbm,train_reduced,test_reduced= removeAndRun(250, train, target, test)\nscores_LGBM = cv_LGBM(train_reduced,target,reduced_params_LGBM,rounds = 3000)   \nscores_XGB = cv_XGB(train_reduced,target,params_XGB, rounds = 3000)\n\nscores = pd.DataFrame()\nscores['XGB'],scores['LGBM'] = scores_XGB['test-auc-mean'],scores_LGBM['auc-mean']\nprint('shape of training data: ' ,np.shape(train_reduced.ix[0]))\nplt.plot(scores)\nplt.legend(labels = ('XGB','LGMB'))\nprint(scores[-10:])\n","0f574660":"#test_xgb = xgb.DMatrix(test_reduced)\n#test_lgbm = lgb.Dataset(test_reduced) no need. \nfinal = lgbm.predict(test_reduced)\n\nplt.scatter(range(300),(final[:300]))\nplt.legend(['Float'])\n\n","e62a15a8":"submission = pd.read_csv('..\/input\/sample_submission.csv')\n\nsubmission['target'] = final\nsubmission.to_csv('submission.csv', index=False)\n","8b78a8bd":"lgb.plot_tree(lgbm,tree_index = 0,figsize = (20,20))","a8577ef9":"Playing around with LightGBM and XGBoost on the dont overfit II dataset:","64e4e767":"How not to overfit with gradient boosted trees: \n* Reduce tree complexity by setting num_leaves or max_depth small.\n* Use a small learning rate\n* Use feature_fraction and bagging_fraction\/bagging_freq. \n* Try to penalize L1 and L2\n* By default min_data_in_leaf is 20, this needs to be reduced in this small dataset, otherwise you will struggle to predict a 0. ","f6b396fa":"PCA is not very effective..\n\nTrying feature removal:","0dbe58ed":"Trying if PCA gives improvement:","23d123e1":"Both XGB and LGB scores the same on public leaderboard after feature removal woth the \"old\" parameters. \n\nLGBM scores 0.81 in public leaderboard with the new reduced_parameters. \n\nSo the CV-function is now overfitting. LGB seems to underperform more than XGB when comparing to CV. LGB and XGB seems equal on public leaderboard. ","7d001969":"LightGMB scoring 0.792 in public leaderboard with these parameters and all features. \nXGBoost scoring 0.795 on public leaderboard with these parameters and all features. \n\nGenerally; CV is more conservative than the public leaderboard.. "}}