{"cell_type":{"bd9a6f35":"code","bc4e518c":"code","ed23754c":"code","46d57cf0":"code","ece4c680":"code","84d8134f":"code","79a69323":"code","d0d21f26":"code","4474210c":"code","f19aa2e2":"code","579ca7b5":"code","ff1b79a9":"code","0006d384":"code","0dd6d627":"code","4badb412":"code","4398e9d1":"code","eda67efa":"code","6f9f2c4c":"code","576fb0a1":"code","dcf2e8a8":"code","d515ea13":"code","bff51ebe":"code","b6892412":"code","11b4a958":"code","d61a1c3e":"code","e72d0ff7":"code","7f036bbd":"code","fffd859f":"code","4817182a":"code","64b0a4f3":"code","fe6daf7a":"code","93df54e9":"code","aa34982c":"code","df5dc174":"code","97a49cd1":"code","9052e481":"code","0bb16856":"code","818db155":"code","280ea705":"markdown","582302ee":"markdown","f6236b1d":"markdown","ea7d54c4":"markdown","db16b774":"markdown","13bbfc7a":"markdown","417ef023":"markdown","85badb43":"markdown","4712d6c5":"markdown","f419939f":"markdown","4d2c1b99":"markdown","243504e2":"markdown"},"source":{"bd9a6f35":"import pandas as pd\nfrom sklearn.datasets import load_boston\nboston_data = load_boston()\nboston = pd.DataFrame(boston_data.data, columns=boston_data.feature_names)\n\nboston['MEDV'] = boston_data.target\nboston.head()\n","bc4e518c":"X = boston.drop(columns=['RAD','MEDV'])\nY = boston['MEDV']","ed23754c":"len(Y)","46d57cf0":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, y_test = train_test_split(X, Y, test_size = 0.1)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(y_test.shape)","ece4c680":"y_test.reset_index(drop=True, inplace=True)","84d8134f":"y_test","79a69323":"Train = pd.concat([X_train,Y_train], axis=1)\nTrain.reset_index(drop=True, inplace=True)","d0d21f26":"Test = X_test\nTest.reset_index(drop=True, inplace=True)","4474210c":"Train","f19aa2e2":"X = Train.drop(columns='MEDV')\nY = Train['MEDV']","579ca7b5":"Y","ff1b79a9":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","0006d384":"from sklearn.linear_model import LinearRegression\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nlin_model = LinearRegression()\nlin_model.fit(X_train, Y_train)","0dd6d627":"y_test_predict = lin_model.predict(X_test)\n\n\nrmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\nprint('RMSE is {}'.format(rmse))","4badb412":"final = lin_model.predict(Test)","4398e9d1":"final","eda67efa":"\nrmse = (np.sqrt(mean_squared_error(y_test, final)))\nprint('RMSE is {}'.format(rmse))","6f9f2c4c":"Test","576fb0a1":"X","dcf2e8a8":"# X = Train.drop(columns='MEDV')\n# Y = Train['MEDV']","d515ea13":"from sklearn.model_selection import KFold \nscore = []\nfinal = []\nkfold = KFold(n_splits=5, random_state=24, shuffle=True)\n\nfor train, test in kfold.split(X):\n    X_train, X_test = X.iloc[train], X.iloc[test]\n    Y_train, Y_test = Y[train], Y[test]\n    \n    lin_model = LinearRegression()\n    lin_model.fit(X_train, Y_train)\n    y_test_predict = lin_model.predict(X_test)\n    \n    final_pred = lin_model.predict(Test)\n    final.append(final_pred)\n    \n    rmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n    print(rmse)\n    score.append(rmse)\n        \n        \naverage_score = np.mean(score)\nprint('The average RMSE is ', average_score)","bff51ebe":"final_result = np.mean(final,0)","b6892412":"final_result","11b4a958":"for i in range(0,len(final)):\n    \n    rmse = (np.sqrt(mean_squared_error(y_test, final[i])))\n    print('RMSE is {}'.format(rmse))","d61a1c3e":"from xgboost import XGBRegressor\nlin_model = XGBRegressor()\nlin_model.fit(X_train, Y_train)\ny_test_predict = lin_model.predict(X_test)\n\n\nrmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\nprint('RMSE is {}'.format(rmse))\n","e72d0ff7":"test_model = XGBRegressor(\n            eta = 0.03,\n            n_estimators = 1500 \n)\n#model.fit(X_train, y_train)\ntest_model.fit(X_train, Y_train, eval_metric='rmse', \n          eval_set=[(X_test, Y_test)], early_stopping_rounds=500, verbose=100)","7f036bbd":"a = test_model.best_iteration\n","fffd859f":"from xgboost import XGBRegressor\nlin_model = XGBRegressor( eta = 0.03,\n            n_estimators = a )\nlin_model.fit(X_train, Y_train)\ny_test_predict = lin_model.predict(X_test)\n\n\nrmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\nprint('RMSE is {}'.format(rmse))\n\n# RMSE is 3.2106485585832782\nfinal = lin_model.predict(Test)\n\n\nrmse = (np.sqrt(mean_squared_error(y_test, final)))\nprint('RMSE is {}'.format(rmse))\n","4817182a":"from sklearn.model_selection import KFold \nscore = []\nfinal = []\nkfold = KFold(n_splits=5, random_state=24, shuffle=True)\n\nfor train, test in kfold.split(X):\n    X_train, X_test = X.iloc[train], X.iloc[test]\n    Y_train, Y_test = Y[train], Y[test]\n    \n    test_model = XGBRegressor(\n            eta = 0.03,\n            n_estimators = 1500 \n    )\n    #model.fit(X_train, y_train)\n    test_model.fit(X_train, Y_train, eval_metric='rmse', \n          eval_set=[(X_test, Y_test)], early_stopping_rounds=500, verbose=100)\n    a = test_model.best_iteration\n    \n    lin_model = XGBRegressor( eta = 0.03,\n            n_estimators = a )\n    lin_model.fit(X_train, Y_train)\n    y_test_predict = lin_model.predict(X_test)\n    \n    \n    final_pred = lin_model.predict(Test)\n    final.append(final_pred)\n    \n    rmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n    print(rmse)\n    score.append(rmse)\n        \n        \naverage_score = np.mean(score)\nprint('The average RMSE is ', average_score)","64b0a4f3":"final","fe6daf7a":"for i in range(0,len(final)):\n    \n    rmse = (np.sqrt(mean_squared_error(y_test, final[i])))\n    print('RMSE is {}'.format(rmse))\n    \n#RMSE is 2.467268931228599","93df54e9":"finals = np.mean(final,0)\nrmse = (np.sqrt(mean_squared_error(y_test, finals)))\nprint('RMSE is {}'.format(rmse))\n    ","aa34982c":"from sklearn.datasets import load_wine\nwine_dataset = load_wine()\nwine = pd.DataFrame(wine_dataset.data, columns=wine_dataset.feature_names)\n\nwine['quality'] = wine_dataset.target\nwine.head()","df5dc174":"wine = wine[wine.quality !=2]\nX = wine.drop(columns='quality')\nY = wine['quality']","97a49cd1":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state=32)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","9052e481":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(max_iter = 1000)\nmodel.fit(X_train, Y_train)\npred = model.predict(X_test)\nfrom sklearn.metrics import   f1_score, accuracy_score\naccuracy_score(pred, Y_test)","0bb16856":"from sklearn.model_selection import StratifiedKFold\n# kfold = StratifiedKFold(n_splits=10, random_state=27, shuffle=True)\nkfold = KFold(n_splits=7, random_state=24, shuffle=True)\nscore = []\nfor train, test in kfold.split(X):\n    X_train, X_test = X.iloc[train], X.iloc[test]\n    Y_train, Y_test = Y[train], Y[test]\n    \n    model = LogisticRegression(max_iter = 10000)\n    model.fit(X_train, Y_train)\n    pred = model.predict(X_test)\n    f1 = accuracy_score(pred, Y_test)\n    print(f1)\n    score.append(f1)\n        \naverage_score = np.mean(score)\nprint('The average accuracy is ', average_score)    ","818db155":"from sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=10, random_state=27, shuffle=True)\n# kfold = KFold(n_splits=7, random_state=24, shuffle=True)\nscore = []\nfor train, test in kfold.split(X,Y):\n    X_train, X_test = X.iloc[train], X.iloc[test]\n    Y_train, Y_test = Y[train], Y[test]\n    \n    model = LogisticRegression(max_iter = 10000)\n    model.fit(X_train, Y_train)\n    pred = model.predict(X_test)\n    f1 = accuracy_score(pred, Y_test)\n    print(f1)\n    score.append(f1)\n        \naverage_score = np.mean(score)\nprint('The average accuracy is ', average_score)    ","280ea705":"# Cross-Validation in Machine Learning?","582302ee":"**What is Train-Test Split Problem?\nTrain-Test split is nothing but splitting your data into two parts. Traning Data and Test Data.\n![image.png](attachment:image.png)\nSo, we train our model with 70% data. And after Training, we test our model performance with 30% data. And check the accuracy.\nSuppose, first we have chosen random_state =5. And Split the data in 70% and 30%. So the accuracy we got is around 80%.**\n\n**But, again when we choose random_state=10. And Train-Test split at 70% and 30%, And now we got an accuracy of 85%.**","f6236b1d":"# K Fold","ea7d54c4":"# Stratified Cross-Validation\n\n**In Stratified Cross-validation, everything will be the same as in K fold Cross-Validation. But in Stratified Cross-Validation, whenever the Test Data is selected, make sure that the number of instances of each class for each round in train and test data, is taken in a proper way.**\n\n![image.png](attachment:image.png)\n\n\n\n### Suppose we have 500 records, in which we have 400 Yes, and 100 No. And that is Imbalanced Dataset.\n\n### So in that case, What Stratified Cross-Validation will do?.\n\n### It will make sure that in Training Dataset, there should be a proper proportion of Yes and No. And also In the Test dataset, there should be a proper proportion of Yes and No.\n\n### By doing so, our model will give an accurate result.","db16b774":"## The disadvantage of Leave One Out Cross-Validation\n**Leave One Out method has the following disadvantages,**\n\n**We need to perform many iterations depending upon the number of records.\nLeave One Out lead to low bias.**","13bbfc7a":"# 1. Leave One Out Cross-Validation (LOOCV)\n![image.png](attachment:image.png)\n**So, here Leave One Out method have chosen one record for test data, and the remaining 499 records for Training data.**","417ef023":"## **Types of Cross-Validation\n## Cross-Validation has following types-**\n\n* Leave One Out Cross-Validation\n* K-Fold Cross-Validation.\n* Stratified Cross-Validation.\n* Time-Series Cross-Validation","85badb43":"# StratifiedKFold","4712d6c5":"## So, the problem is our accuracy fluctuates with different random_state. And we can\u2019t say that what accuracy our model has.\n# Therefore to prevent this problem, Cross-Validation is used.","f419939f":"# Time-Series Cross-Validation\n![image.png](attachment:image.png)","4d2c1b99":"## What is Cross-Validation?\n## To evaluate the performance of a model, we use Cross-Validation.\n\n# Cross-Validation split the dataset into different segments. And use each segment for training as well as testing one by one.","243504e2":"#  K Fold CV\n### For each experiment, based on the K value, it will decide the number of Test Data.\n\n### Here, our k value is 5. And we have 1000 records.\n\n#### Right?.\n#### So when we divide 1000 with 5.\n\n#### 1000\/5=200\n\n#### So, 200 will be our Test Data. And the remaining 800 records will be our Training Data.\n![image.png](attachment:image.png)\n\n\n# # In the end We can take all Accuracies, and find out the Mean. And that Mean of all 5 accuracies is the actual accuracy of your model.\n\n\n## The value of k should be 5 or 10."}}