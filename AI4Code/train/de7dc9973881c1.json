{"cell_type":{"d46f27f9":"code","d4d0edf7":"code","f418087b":"code","fafd07f2":"code","06b1b78b":"code","88919df5":"code","197ac4bd":"code","73f031a8":"code","b822a3dc":"code","35d546b3":"code","95e4d88e":"code","7bd237c6":"code","499d4638":"code","ededf235":"code","95208c8b":"code","62499480":"code","636103b1":"code","a957cb23":"code","333d3f67":"code","aa0d061c":"code","8d130979":"code","f4a4e667":"code","90eb98af":"code","83d672c2":"code","60659d6e":"code","0c52f627":"code","a593cefc":"code","42dbaa07":"code","97feeb3b":"markdown","c0af47a1":"markdown","5a341edf":"markdown","24d6f1ee":"markdown","8804de69":"markdown","f334d9fe":"markdown","95d421c4":"markdown","c590d19e":"markdown","937c00a8":"markdown","ce0dc6b3":"markdown","8a0a5cd3":"markdown","1dea085b":"markdown","c9bc95e3":"markdown","115841bc":"markdown","409dc6ce":"markdown","a5946ff4":"markdown","8f20340a":"markdown"},"source":{"d46f27f9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nplt.style.use('seaborn')\nsns.set(font_scale=2.5)\nimport missingno as msno\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\nfrom sklearn.preprocessing import RobustScaler","d4d0edf7":"path = \"..\/input\/tabular-playground-series-dec-2021\/\"\ntrain = pd.read_csv(path + \"train.csv\")\ntest = pd.read_csv(path + \"test.csv\")","f418087b":"print(train.shape)\ntrain.head()","fafd07f2":"train.describe()","06b1b78b":"print(f\"# of train data : {train.shape[0]}\")\nprint(f\"# of train features : {train.shape[1] - 1}\")\nprint(\"\")\nprint('='*15, \" >> Null Data << \", '='*15)\nnull_feature = []\nfor col in train.columns:\n    msg = 'column: {:>35}\\t {:>10d} of {:<10d} ( Percent of Null value: {:.2f}% )'.format(col, train[col].isnull().sum(), train[col].shape[0], 100 * (train[col].isnull().sum() \/ train[col].shape[0]))\n    print(msg)\n    if train[col].isnull().sum() != 0:\n        null_feature.append(col)\n\n\nif len(null_feature) != 0:\n    print(\"\")\n    print('='*15, \" >> Warning << \", '='*15)\n    print(\"NULL Feature : \", null_feature)","88919df5":"print('='*15, \" >> Unique Data << \", '='*15)\n\nunique_col = []\nfor col in train.columns:\n    msg = 'column: {:>35}\\t {:>10d}'.format(col, len(train[col].unique()) )\n    print(msg)\n    if len(train[col].unique()) == 1:\n        unique_col.append(col)\n\nif len(unique_col) != 0:\n    print(\"\")\n    print('='*15, \" >> Warning << \", '='*15)\n    print(\"Unique Feature : \", unique_col)","197ac4bd":"# move to Function \"EngineerFeatures\"\n# print(f\"train shape : Formerly, {train.shape}\", end=\" \")\n# train = train.drop(unique_col, axis=1)\n# print(\"-\"*10, \">>\", f\" train shape : Now, {train.shape}\", end=\" \")","73f031a8":"f, ax = plt.subplots(1, 2, figsize=(20, 12))\n\ntrain['Cover_Type'].value_counts().plot.pie(autopct='%1.4f%%', ax=ax[0], shadow=True)\nax[0].set_title('Pie plot - Cover_Type', fontsize=16)\nax[0].set_ylabel('')\nax[0].tick_params(axis='both', labelsize=14)\n\nax[1].set_title('Count plot - Cover_Type', fontsize=16)\nsns.countplot('Cover_Type', data=train, ax=ax[1])\nax[1].set_ylabel('count', fontsize = 14)\nax[1].set_xlabel('Cover_Type', fontsize = 14)\nax[1].tick_params(axis='both', labelsize=14)\n\nplt.show()","b822a3dc":"print('='*15, \" >> Target Data << \", '='*15)\nnum_target = train['Cover_Type'].value_counts()\ntargets = train['Cover_Type'].unique()\ntargets.sort()\nfor target in targets:\n    msg = 'target: {:>3}\\t {:>7d} of {:<10d} ( Percent of Null value: {:.2f}% )'.format(target,\n                                                                                          num_target[target], train.shape[0], \n                                                                                          100 * (num_target[target] \/ train.shape[0]))\n    print(msg)","35d546b3":"def EngineerFunction(df, is_train=True):\n    df = df.drop(['Id'], axis=1)\n\n    # Euclidean distance to Hydrology # Not Manhhattan distance, becuase it is environmental circumstance\n    df.loc[:, \"Pythagorian_Distance_To_Hydrology\"] = np.hypot(df[\"Horizontal_Distance_To_Hydrology\"], df[\"Vertical_Distance_To_Hydrology\"])\n\n    # remove unuseful features, becuse they are unique\n    unique_cols = ['Soil_Type7', 'Soil_Type15']\n    df = df.drop(unique_cols, axis=1)\n\n    # Aspect is degree and -4 == 360 - 4 and 4 == 350 + 4\n    # now, 0 <= Aspect <= 360\n    df[\"Aspect\"][df[\"Aspect\"] < 0] += 360\n    df[\"Aspect\"][df[\"Aspect\"] > 359] -= 360\n\n    # because 0 <= hillshade <= 255\n    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\n    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n\n    if is_train == True:\n        # for targets\n        # df = pd.get_dummies(df, columns=[\"Cover_Type\"])\n\n        # Cover_type == 5 has only one data\n        # idx = df[df[\"Cover_Type_5\"] == 1].index\n        idx = df[df[\"Cover_Type\"] == 5].index\n        df.drop(idx, axis=0, inplace=True)\n\n    return df\n\ntrain = EngineerFunction(train)\ntest = EngineerFunction(test, is_train=False)","95e4d88e":"# target_cols = []\n# for n in range(1, 8):\n#   target_cols.append(f\"Cover_Type_{n}\")\n\n\n# X_train = train.drop(target_cols, axis=1)\n# y_train = train[target_cols]\nX_train = train.drop(\"Cover_Type\", axis=1)\ny_train = train[\"Cover_Type\"]\n\nX_test = test","7bd237c6":"X_train","499d4638":"X_train.describe()","ededf235":"y_train","95208c8b":"X_test","62499480":"# Robust Scaler\nRS = RobustScaler().fit(X_train)\nX_train = RS.transform(X_train)\nX_test = RS.transform(X_test)","636103b1":"import tensorflow as tf\n\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.model_selection import train_test_split #StratifiedKFold","a957cb23":"feat_dim = X_train.shape[1]\nnum_targets = 7 #y_train.shape[1]\ndropout_rate = 0.1\n\ndef build_model():\n    inputs = tf.keras.Input(shape=(feat_dim,))\n\n    y1 = tf.keras.layers.Dense(128, activation='gelu')(inputs)\n    y1 = tf.keras.layers.Dropout(dropout_rate)(y1)\n\n    y2 = tf.keras.layers.Dense(128, activation='gelu')(y1)\n    y2 = tf.keras.layers.Dropout(dropout_rate)(y2)\n    y2 = tf.keras.layers.LayerNormalization()(y1 + y2)\n\n    y3 = tf.keras.layers.Dense(64, activation='gelu')(y2)\n    y3 = tf.keras.layers.Dropout(dropout_rate)(y3)\n\n    y4 = tf.keras.layers.Dense(64, activation='gelu')(y3)\n    y4 = tf.keras.layers.Dropout(dropout_rate)(y4)\n    y4 = tf.keras.layers.LayerNormalization()(y3 + y4)\n\n    outputs = tf.keras.layers.Dense(num_targets, activation='softmax')(y4)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    return model","333d3f67":"model = build_model()\nmodel.summary()","aa0d061c":"EPOCH = 50\nBATCH_SIZE = 2**13\n\nX, X_valid, y, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=21, shuffle=True, stratify=y_train)\n\ny = pd.get_dummies(y)\ny.loc[:, 5] = 0\ny = y[[1, 2, 3, 4, 5, 6, 7]]\ny_valid = pd.get_dummies(y_valid)\ny_valid.loc[:, 5] = 0\ny_valid = y_valid[[1, 2, 3, 4, 5, 6, 7]]\nmodel = build_model()\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics = ['categorical_accuracy'])\n\nsave_path = '.\/'\ncheckpoint_folderpath = save_path + f\"weights\/\"\ncheckpoint_filepath = save_path + f\"weights\/weights\"\nif os.path.isdir(checkpoint_folderpath):\n    print(f\"Loading Weights\")\n    model.load_weights(checkpoint_filepath)\n\nsv = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='max', save_freq='epoch', options=None)\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=5)\n\nhistory = model.fit(X, y, verbose=1,\n                          validation_data=(X_valid, y_valid),\n                          epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[sv, early_stop])","8d130979":"submission = pd.read_csv(path+'sample_submission.csv')\nsubmission","f4a4e667":"predictions = model.predict(X_test, verbose=1, batch_size=BATCH_SIZE)","90eb98af":"print(predictions.shape)\npredictions","83d672c2":"pred = []\nfor i in range(0, predictions.shape[0]):\n    max_idx = np.argmax(predictions[i, :])\n    pred.append(max_idx+1)","60659d6e":"len(pred)","0c52f627":"submission[\"Cover_Type\"] = pred","a593cefc":"submission","42dbaa07":"submission.to_csv('submission.csv', index=False)","97feeb3b":"## Unique Data Check","c0af47a1":"# EDA","5a341edf":"[Sum of Soil_Type and Wilderness_Area](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/292823)","24d6f1ee":"\n*   Elevation - Elevation in meters \uace0\ub3c4\n*   Aspect - Aspect in degrees azimuth \uc790\uc2e0\uc774 \uc788\ub294 \uacf3\uc740 \uc218\ud3c9\uc73c\ub85c \uc0dd\uac01\ud560\ub54c, \ubd81\uadf9\uc810\uc73c\ub85c\ubd80\ud130 \ud574\ub2f9 \uc9c0\uc810\uae4c\uc9c0 \uc2dc\uacc4\ubc29\ud5a5 \uac01\ub3c4\n*   Slope - Slope in degrees \uacbd\uc0ac\n*   Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features\n*   Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features\n*   Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway\n*   Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\n*   Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\n*   Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice\n*   Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points\n*   Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation\n*   Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\n*   Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation","8804de69":"dataset is already one-hot encoded: Wilderness_Area and Soil_Type","f334d9fe":"# TPC DEC 2021","95d421c4":"# Feature Engineering","c590d19e":"# Build Model","937c00a8":"# Check Dataset","ce0dc6b3":"## Null Data Check","8a0a5cd3":"no null data","1dea085b":"2 Unique Data : 'Soil_Type7', 'Soil_Type15'","c9bc95e3":"\n\n*   [Tabular Playground Series - Dec 2021](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021)\n*   [Forest Cover Type Prediction](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/data)","115841bc":"too imbalanced, expecially cover_type 4, 5","409dc6ce":"how targets \"Cover_Type_4\" and \"Cover_type_5\"???","a5946ff4":"## Check Target Label","8f20340a":"# Prediction"}}