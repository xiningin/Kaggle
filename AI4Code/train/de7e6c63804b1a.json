{"cell_type":{"af24351a":"code","82dbb85e":"code","683d43fe":"code","f3798391":"code","5b76c5f7":"code","c5d92cbb":"code","e3b80b17":"code","a7b10f7c":"code","220c3681":"code","33a923a9":"code","8513e2fb":"code","24153a35":"code","23149c19":"code","9e67e505":"code","ad97cba1":"code","8e9d93ff":"code","8a0a99ed":"code","b5a19965":"code","0be94d4e":"code","60e35133":"code","cf3bce76":"markdown","f5deaf73":"markdown","3eed806a":"markdown","5ee61ad8":"markdown","480e6381":"markdown","83895fba":"markdown","ccfb33fa":"markdown","865d43be":"markdown","01c0480f":"markdown","f9601286":"markdown","e862997f":"markdown","8df19797":"markdown","486e782c":"markdown","68b080e1":"markdown","3ac34820":"markdown","2c5be768":"markdown","d2eec79c":"markdown","1ad69b34":"markdown","0089e46e":"markdown","f94c81c7":"markdown","14248e48":"markdown"},"source":{"af24351a":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics.regression import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LinearRegression, LassoCV, Lasso\nfrom sklearn.ensemble import RandomForestRegressor","82dbb85e":"data = pd.read_csv('..\/input\/winequality-white.csv')","683d43fe":"data.head()","f3798391":"data.info()","5b76c5f7":"# y = None # you code here\nX=data.drop(['quality'],axis=1)\ny=data.quality\n\n\nX_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.3,random_state=17) # you code here\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_holdout_scaled = scaler.transform(X_holdout)","c5d92cbb":"linreg = LinearRegression()\nlinreg.fit(X_train_scaled, y_train)","e3b80b17":"predict_y_train=linreg.predict(X_train_scaled)\npredict_y_test=linreg.predict(X_holdout_scaled)\nmes1=mean_squared_error(predict_y_train, y_train)\nmes2=mean_squared_error(predict_y_test, y_holdout)\nprint(\"Mean squared error (train): %.3f\" % mes1)# you code here\nprint(\"Mean squared error (test): %.3f\" % mes2)# you code here","a7b10f7c":"linreg_coef = pd.DataFrame({'Features':X_train.columns, 'Influence':linreg.coef_}) # you code here\nlinreg_coef.sort_values('Influence')","220c3681":"lasso1 = Lasso(alpha=0.01, random_state=17) # you code here\nlasso1.fit(X_train_scaled, y_train) # you code here","33a923a9":"lasso1_coef = pd.DataFrame({'Features':X_train.columns, 'Information':lasso1.coef_}) # you code here\nlasso1_coef.sort_values('Information') # you code here","8513e2fb":"alphas = np.logspace(-6, 2, 200)\nlasso_cv = LassoCV(random_state=17, alphas=alphas, cv=5) # you code here\nlasso_cv.fit(X_train_scaled, y_train) # you code here","24153a35":"lasso_cv.alpha_","23149c19":"lasso_cv_coef = pd.DataFrame({'Features':X_train.columns, 'Information':lasso_cv.coef_}) # you code here\nlasso_cv_coef.sort_values('Information') # you code here","9e67e505":"lasso_y_predict=lasso_cv.predict(X_train_scaled)\nlasso_mse_train=mean_squared_error(lasso_y_predict,y_train)\nprint(\"Mean squared error (train): %.3f\" %lasso_mse_train)\nlasso_y_predict_2=lasso_cv.predict(X_holdout_scaled)\nlasso_mse_test=mean_squared_error(lasso_y_predict_2, y_holdout)\nprint(\"Mean squared error (test): %.3f\" %lasso_mse_test)","ad97cba1":"forest = RandomForestRegressor(random_state=17) # you code here\nforest.fit(X_train_scaled, y_train) # you code here","8e9d93ff":"rf_y_predict=forest.predict(X_train_scaled)\nrf_mse1=mean_squared_error(rf_y_predict, y_train)\nprint(\"Mean squared error (train): %.3f\" %rf_mse1)\ncvs = cross_val_score(forest,X=X_train_scaled, y=y_train, scoring='neg_mean_squared_error')\nprint(\"Mean squared error (cv): %.3f\" %(-np.mean(cvs)))\nrf_y_predict_test=forest.predict(X_holdout_scaled)\nrf_mse2=mean_squared_error(rf_y_predict_test, y_holdout)\nprint(\"Mean squared error (test): %.3f\" %rf_mse2) # you code here","8a0a99ed":"forest_params = {'max_depth': list(range(10, 25)), \n                 'min_samples_leaf': list(range(1, 8)),\n                 'max_features': list(range(6,12))}\n\nlocally_best_forest = GridSearchCV(forest, param_grid=forest_params,scoring='neg_mean_squared_error')\n\nlocally_best_forest.fit(X_train_scaled, y_train) # you code here","b5a19965":"locally_best_forest.best_params_, locally_best_forest.best_score_","0be94d4e":"cvs1=cross_val_score(locally_best_forest.best_estimator_, X=X_train_scaled, y=y_train, cv=5, scoring='neg_mean_squared_error')\nprint(\"Mean squared error (cv): %.3f\" %(-np.mean(cvs1)))\ny_test_predict=locally_best_forest.best_estimator_.predict(X_holdout_scaled)\nbest_rf_mse=mean_squared_error(y_test_predict, y_holdout)\nprint(\"Mean squared error (test): %.3f\" %best_rf_mse) # you code here","60e35133":"best_forest=locally_best_forest.best_estimator_\nrf_importance = pd.DataFrame({'Feature':X_train.columns, 'Importance':best_forest.feature_importances_}) # you code here\nrf_importance.sort_values('Importance') # you code here","cf3bce76":"**Tune the `max_features` and `max_depth` hyperparameters with GridSearchCV and again check mean cross-validation MSE and MSE on holdout set.**","f5deaf73":"**Output RF's feature importance. Again, it's nice to present it as a DataFrame.**<br>\n**<font color='red'>Question 7:<\/font> What is the most important feature, according to the Random Forest model?**","3eed806a":"# <center> Assignment #6 (demo).\n## <center>  Exploring OLS, Lasso and Random Forest in a regression task\n    \n<img src=https:\/\/habrastorage.org\/webt\/-h\/ns\/aa\/-hnsaaifymavmmudwip9imcmk58.jpeg width=30%>\n\n**Fill in the missing code and choose answers in [this](https:\/\/docs.google.com\/forms\/d\/1aHyK58W6oQmNaqEfvpLTpo6Cb0-ntnvJ18rZcvclkvw\/edit) web form.**","5ee61ad8":"**Which feature is the least informative in predicting wine quality, according to this LASSO model?**","480e6381":"## Lasso regression","83895fba":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\">\n## Open Machine Learning Course\n<center>Author: [Yury Kashnitsky](https:\/\/www.linkedin.com\/in\/festline\/), Data Scientist at Mail.ru Group <br>\n    All content is distributed under the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license.\nYou may use this material for any purpose (you can edit, correct and use it as example) exept commercial use with mandatory citation of author.","ccfb33fa":"**Train a Random Forest with out-of-the-box parameters, setting only random_state to be 17.**","865d43be":"**<font color='red'>Question 5:<\/font> What are mean squared errors of RF model on the training set, in cross-validation (cross_val_score with scoring='neg_mean_squared_error' and other arguments left with default values) and on holdout set?**","01c0480f":"**Train a simple linear regression model (Ordinary Least Squares).**","f9601286":"**Make conclusions about the perdormance of the explored 3 models in this particular prediction task.**","e862997f":"**<font color='red'>Question 4:<\/font> What are mean squared errors of tuned LASSO predictions on train and holdout sets?**","8df19797":"**<font color='red'>Question 3:<\/font> Which feature is the least informative in predicting wine quality, according to the tuned LASSO model?**","486e782c":"**Train LassoCV with random_state=17 to choose the best value of $\\alpha$ in 5-fold cross-validation.**","68b080e1":"## Random Forest","3ac34820":"**<font color='red'>Question 6:<\/font> What are mean squared errors of tuned RF model in cross-validation (cross_val_score with scoring='neg_mean_squared_error' and other arguments left with default values) and on holdout set?**","2c5be768":"## Linear regression","d2eec79c":"**Sort features by their influence on the target feature (wine quality). Beware that both large positive and large negative coefficients mean large influence on target. It's handy to use `pandas.DataFrame` here.**\n\n**<font color='red'>Question 2:<\/font> Which feature this linear regression model treats as the most influential on wine quality?**","1ad69b34":"**Separate the target feature, split data in 7:3 proportion (30% form a holdout set, use random_state=17), and preprocess data with `StandardScaler`.**","0089e46e":"**Train a LASSO model with $\\alpha = 0.01$ (weak regularization) and scaled data. Again, set random_state=17.**","f94c81c7":"**<font color='red'>Question 1:<\/font> What are mean squared errors of model predictions on train and holdout sets?**","14248e48":"**We are working with UCI Wine quality dataset (no need to download it \u2013 it's already there, in course repo and in Kaggle Dataset).**"}}