{"cell_type":{"eeed4e0b":"code","3b56249d":"code","0d7e6a5e":"code","00bac928":"code","d188cade":"code","c611922f":"code","84e4ffad":"code","2c6ef7fc":"code","cedc745b":"code","0e6e1ea8":"code","45cc9e6a":"code","72bc8f92":"code","e702df29":"code","bec1400f":"code","fe1df4bd":"code","a582a8e0":"code","d1f31ce8":"code","282631ff":"code","e912d9f5":"code","30d9cc88":"code","75e39982":"code","01186a76":"code","cf6d78df":"code","6a0f7092":"code","cbddf48a":"code","1404ed2d":"code","69ad577f":"code","d4403f72":"code","5ca14f61":"code","41f98937":"code","b9a2ab5c":"code","0926baf7":"code","ae5808bd":"code","5a49ed8d":"code","5123dde8":"code","895145f6":"code","38a59aa1":"code","44768f11":"code","5893e747":"code","a5c5ee1e":"code","b6f1c081":"code","18fb7aa9":"code","e978f7d1":"code","88cb126f":"code","a803ab3a":"code","ab5fdb50":"code","4497fa66":"code","61fd6825":"code","3af14c2d":"code","4e7e76f2":"code","22c0f1f4":"markdown","9ddffffa":"markdown","ad4d20bc":"markdown","fc31f02e":"markdown","f63ef268":"markdown","40f6a534":"markdown","ff51a53a":"markdown","629f63d2":"markdown","158432c9":"markdown","85b80a08":"markdown","7ff036bc":"markdown","eaf835e4":"markdown","d3758883":"markdown","fd3d4834":"markdown","3e9720ed":"markdown","4af17d3e":"markdown","c207fe54":"markdown","e695e8d5":"markdown","2623ec50":"markdown","bc2a124c":"markdown","39edd5a9":"markdown","e0b3c620":"markdown","2bdae4b8":"markdown","39e90752":"markdown","06e69d8a":"markdown","0e32ddd0":"markdown","a6512507":"markdown","2f62347b":"markdown","6038a921":"markdown","3bd6c518":"markdown","6734c5f9":"markdown"},"source":{"eeed4e0b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pylab as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.linear_model import LogisticRegression\nwarnings.simplefilter(\"ignore\")\nplt.style.use('ggplot')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]","3b56249d":"# Transaction CSVs\ntrain_transaction = pd.read_csv('..\/input\/train_transaction.csv')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv')\n# Identity CSVs - These will be merged onto the transactions to create additional features\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv')\n# Sample Submissions\n#ss = pd.read_csv('..\/input\/sample_submission.csv')\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","0d7e6a5e":"print('train_transaction shape is {}'.format(train_transaction.shape))\nprint('test_transaction shape is {}'.format(test_transaction.shape))\nprint('train_identity shape is {}'.format(train_identity.shape))\nprint('test_identity shape is {}'.format(test_identity.shape))","00bac928":"# Here we confirm that all of the transactions in `train_identity`\nprint(np.sum(train_transaction['TransactionID'].isin(train_identity['TransactionID'].unique())))\nprint(np.sum(test_transaction['TransactionID'].isin(test_identity['TransactionID'].unique())))","d188cade":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\ntrain_identity = reduce_mem_usage(train_identity)\ntrain_transaction = reduce_mem_usage(train_transaction)\n","c611922f":"train_transaction['TransactionDT'].plot(kind='hist',\n                                        figsize=(15, 5),\n                                        label='train',\n                                        bins=50,\n                                        title='Train vs Test TransactionDT distribution')\ntest_transaction['TransactionDT'].plot(kind='hist',\n                                       label='test',\n                                       bins=50)\nplt.legend()\nplt.show()","84e4ffad":"ax = train_transaction.plot(x='TransactionDT',\n                       y='TransactionAmt',\n                       kind='scatter',\n                       alpha=0.01,\n                       label='TransactionAmt-train',\n                       title='Train and test Transaction Ammounts by Time (TransactionDT)',\n                       ylim=(0, 5000),\n                       figsize=(15, 5))\ntest_transaction.plot(x='TransactionDT',\n                      y='TransactionAmt',\n                      kind='scatter',\n                      label='TransactionAmt-test',\n                      alpha=0.01,\n                      color=color_pal[1],\n                       ylim=(0, 5000),\n                      ax=ax)\n# Plot Fraud as Orange\ntrain_transaction.loc[train_transaction['isFraud'] == 1] \\\n    .plot(x='TransactionDT',\n         y='TransactionAmt',\n         kind='scatter',\n         alpha=0.01,\n         label='TransactionAmt-train',\n         title='Train and test Transaction Ammounts by Time (TransactionDT)',\n         ylim=(0, 5000),\n         color='black',\n         figsize=(15, 5),\n         ax=ax)\nplt.show()","2c6ef7fc":"print('  {:.4f}% of Transactions that are fraud in train '.format(train_transaction['isFraud'].mean() * 100))","cedc745b":"#Discuss precision and recall because of class imbalance\ntrain_transaction.groupby('isFraud') \\\n    .count()['TransactionID'] \\\n    .plot(kind='barh',\n          title='Distribution of Target in Train',\n          figsize=(15, 3))\nplt.show()","0e6e1ea8":"train_transaction['TransactionAmt'] \\\n    .apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(15, 5),\n          title='Distribution of Log Transaction Amt')\nplt.show()","45cc9e6a":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 6))\ntrain_transaction.loc[train_transaction['isFraud'] == 1] \\\n    ['TransactionAmt'].apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='Log Transaction Amt - Fraud',\n          color=color_pal[1],\n          xlim=(-3, 10),\n         ax= ax1)\ntrain_transaction.loc[train_transaction['isFraud'] == 0] \\\n    ['TransactionAmt'].apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='Log Transaction Amt - Not Fraud',\n          color=color_pal[2],\n          xlim=(-3, 10),\n         ax=ax2)\ntrain_transaction.loc[train_transaction['isFraud'] == 1] \\\n    ['TransactionAmt'] \\\n    .plot(kind='hist',\n          bins=100,\n          title='Transaction Amt - Fraud',\n          color=color_pal[1],\n         ax= ax3)\ntrain_transaction.loc[train_transaction['isFraud'] == 0] \\\n    ['TransactionAmt'] \\\n    .plot(kind='hist',\n          bins=100,\n          title='Transaction Amt - Not Fraud',\n          color=color_pal[2],\n         ax=ax4)\nplt.show()","72bc8f92":"print('Mean transaction amt for fraud is {:.4f}'.format(train_transaction.loc[train_transaction['isFraud'] == 1]['TransactionAmt'].mean()))\nprint('Mean transaction amt for non-fraud is {:.4f}'.format(train_transaction.loc[train_transaction['isFraud'] == 0]['TransactionAmt'].mean()))","e702df29":"train_transaction.groupby('ProductCD') \\\n    ['TransactionID'].count() \\\n    .sort_index() \\\n    .plot(kind='barh',\n          figsize=(15, 3),\n         title='Count of Observations by ProductCD')\nplt.show()","bec1400f":"train_transaction.groupby('ProductCD')['isFraud'] \\\n    .mean() \\\n    .sort_index() \\\n    .plot(kind='barh',\n          figsize=(15, 3),\n         title='Percentage of Fraud by ProductCD')\nplt.show()","fe1df4bd":"card_cols = [c for c in train_transaction.columns if 'card' in c]\ntrain_transaction[card_cols].head()","a582a8e0":"color_idx = 0\nfor c in card_cols:\n    if train_transaction[c].dtype in ['float64','int64']:\n        train_transaction[c].plot(kind='hist',\n                                      title=c,\n                                      bins=50,\n                                      figsize=(15, 2),\n                                      color=color_pal[color_idx])\n    color_idx += 1\n    plt.show()","d1f31ce8":"train_transaction_fr = train_transaction.loc[train_transaction['isFraud'] == 1]\ntrain_transaction_nofr = train_transaction.loc[train_transaction['isFraud'] == 0]\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 8))\ntrain_transaction_fr.groupby('card4')['card4'].count().plot(kind='barh', ax=ax1, title='Count of card4 fraud')\ntrain_transaction_nofr.groupby('card4')['card4'].count().plot(kind='barh', ax=ax2, title='Count of card4 non-fraud')\ntrain_transaction_fr.groupby('card6')['card6'].count().plot(kind='barh', ax=ax3, title='Count of card6 fraud')\ntrain_transaction_nofr.groupby('card6')['card6'].count().plot(kind='barh', ax=ax4, title='Count of card6 non-fraud')\nplt.show()","282631ff":"print(' addr1 - has {} NA values'.format(train_transaction['addr1'].isna().sum()))\nprint(' addr2 - has {} NA values'.format(train_transaction['addr2'].isna().sum()))","e912d9f5":"train_transaction['addr1'].plot(kind='hist', bins=500, figsize=(15, 2), title='addr1 distribution')\nplt.show()\ntrain_transaction['addr2'].plot(kind='hist', bins=500, figsize=(15, 2), title='addr2 distribution')\nplt.show()","30d9cc88":"train_transaction['dist1'].plot(kind='hist',\n                                bins=5000,\n                                figsize=(15, 2),\n                                title='dist1 distribution',\n                                color=color_pal[1],\n                                logx=True)\nplt.show()\ntrain_transaction['dist2'].plot(kind='hist',\n                                bins=5000,\n                                figsize=(15, 2),\n                                title='dist2 distribution',\n                                color=color_pal[1],\n                                logx=True)\nplt.show()","75e39982":"c_cols = [c for c in train_transaction if c[0] == 'C']\ntrain_transaction[c_cols].head()","01186a76":"# Sample 500 fraud and 500 non-fraud examples to plot\nsampled_train = pd.concat([train_transaction.loc[train_transaction['isFraud'] == 0].sample(500),\n          train_transaction.loc[train_transaction['isFraud'] == 1].sample(500)])\n\nsns.pairplot(sampled_train, \n             hue='isFraud',\n            vars=c_cols)\nplt.show()","cf6d78df":"d_cols = [c for c in train_transaction if c[0] == 'D']\ntrain_transaction[d_cols].head()","6a0f7092":"sns.pairplot(sampled_train, \n             hue='isFraud',\n            vars=d_cols)\nplt.show()","cbddf48a":"m_cols = [c for c in train_transaction if c[0] == 'M']\ntrain_transaction[m_cols].head()","1404ed2d":"(train_transaction[m_cols] == 'T').sum().plot(kind='bar',\n                                              title='Count of T by M column',\n                                              figsize=(15, 2),\n                                              color=color_pal[3])\nplt.show()\n(train_transaction[m_cols] == 'F').sum().plot(kind='bar',\n                                              title='Count of F by M column',\n                                              figsize=(15, 2),\n                                              color=color_pal[4])\nplt.show()\n(train_transaction[m_cols].isna()).sum().plot(kind='bar',\n                                              title='Count of NaN by M column',\n                                              figsize=(15, 2),\n                                              color=color_pal[0])\nplt.show()\n","69ad577f":"# Looking at M4 column since it is different than the others\ntrain_transaction.groupby('M4')['TransactionID'] \\\n    .count() \\\n    .plot(kind='bar',\n          title='Count of values for M4',\n          figsize=(15, 3))\nplt.show()","d4403f72":"v_cols = [c for c in train_transaction if c[0] == 'V']\ntrain_transaction[v_cols].head()","5ca14f61":"train_transaction[v_cols].describe()","41f98937":"train_transaction['v_mean'] = train_transaction[v_cols].mean(axis=1)","b9a2ab5c":"fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15, 6))\ntrain_transaction.loc[train_transaction['isFraud'] == 1]['v_mean'] \\\n    .apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='log transformed mean of V columns - Fraud',\n          ax=ax1)\ntrain_transaction.loc[train_transaction['isFraud'] == 0]['v_mean'] \\\n    .apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          title='log transformed mean of V columns - Not Fraud',\n          color=color_pal[5],\n          ax=ax2)\nplt.show()","0926baf7":"# Add the `isFraud` column for analysis\ntrain_identity_ = train_identity.merge(train_transaction[['TransactionID',\n                                                         'TransactionDT',\n                                                         'isFraud']],\n                                      on=['TransactionID'])\n\ntest_identity_ = test_identity.merge(test_transaction[['TransactionID',\n                                                      'TransactionDT']],\n                                    on=['TransactionID'])","ae5808bd":"train_identity_.groupby('DeviceType') \\\n    .mean()['isFraud'] \\\n    .sort_values() \\\n    .plot(kind='barh',\n          figsize=(15, 5),\n          title='Percentage of Fraud by Device Type')\nplt.show()","5a49ed8d":"train_identity_.groupby('DeviceInfo') \\\n    .count()['TransactionID'] \\\n    .sort_values(ascending=False) \\\n    .head(20) \\\n    .plot(kind='barh', figsize=(15, 5), title='Top 20 Devices in Train')\nplt.show()","5123dde8":"id_cols = [c for c in train_identity.columns if 'id' in c]\nid_cols_test = [c for c in test_identity.columns if 'id' in c]\nprint(id_cols)\nfor i in id_cols:\n    try:\n        train_identity_.set_index('TransactionDT')[i].plot(style='.', title=i, figsize=(15, 3))\n        plt.show()\n    except TypeError:\n        pass\nfor i in id_cols:\n    try:\n        test_identity_.set_index('TransactionDT')[i].plot(style='.', title=i, figsize=(15, 3))\n        plt.show()\n    except TypeError:\n        pass","895145f6":"\ndf_trans = pd.read_csv('..\/input\/train_transaction.csv')\ndf_test_trans = pd.read_csv('..\/input\/test_transaction.csv')\n\ndf_id = pd.read_csv('..\/input\/train_identity.csv')\ndf_test_id = pd.read_csv('..\/input\/test_identity.csv')\n\n#sample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')\n\ndf_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True, on='TransactionID')\ndf_test = df_test_trans.merge(df_test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n\nprint(df_train.shape)\nprint(df_test.shape)\n\n# y_train = df_train['isFraud'].copy()\ndel df_trans, df_id, df_test_trans, df_test_id","38a59aa1":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","44768f11":"\nemails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\nus_emails = ['gmail', 'net', 'edu']\n\n# https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100499#latest-579654\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    df_train[c + '_bin'] = df_train[c].map(emails)\n    df_test[c + '_bin'] = df_test[c].map(emails)\n    \n    df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1])\n    df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n    \n    df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","5893e747":"# Label Encoding\nfrom sklearn import preprocessing\nfor f in df_train.drop('isFraud', axis=1).columns:\n    if df_train[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_train[f].values))\n        df_train[f] = lbl.transform(list(df_train[f].values))\nfor f in df_test.drop('isFraud', axis=1).columns:\n    if df_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_test[f].values))\n        df_test[f] = lbl.transform(list(df_test[f].values))","a5c5ee1e":"df_train['Trans_min_mean'] = df_train['TransactionAmt'] - df_train['TransactionAmt'].mean()\ndf_train['Trans_min_std'] = df_train['Trans_min_mean'] \/ df_train['TransactionAmt'].std()\ndf_test['Trans_min_mean'] = df_test['TransactionAmt'] - df_test['TransactionAmt'].mean()\ndf_test['Trans_min_std'] = df_test['Trans_min_mean'] \/ df_test['TransactionAmt'].std()","b6f1c081":"df_train['TransactionAmt_to_mean_card1'] = df_train['TransactionAmt'] \/ df_train.groupby(['card1'])['TransactionAmt'].transform('mean')\ndf_train['TransactionAmt_to_mean_card4'] = df_train['TransactionAmt'] \/ df_train.groupby(['card4'])['TransactionAmt'].transform('mean')\ndf_train['TransactionAmt_to_std_card1'] = df_train['TransactionAmt'] \/ df_train.groupby(['card1'])['TransactionAmt'].transform('std')\ndf_train['TransactionAmt_to_std_card4'] = df_train['TransactionAmt'] \/ df_train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ndf_test['TransactionAmt_to_mean_card1'] = df_test['TransactionAmt'] \/ df_test.groupby(['card1'])['TransactionAmt'].transform('mean')\ndf_test['TransactionAmt_to_mean_card4'] = df_test['TransactionAmt'] \/ df_test.groupby(['card4'])['TransactionAmt'].transform('mean')\ndf_test['TransactionAmt_to_std_card1'] = df_test['TransactionAmt'] \/ df_test.groupby(['card1'])['TransactionAmt'].transform('std')\ndf_test['TransactionAmt_to_std_card4'] = df_test['TransactionAmt'] \/ df_test.groupby(['card4'])['TransactionAmt'].transform('std')","18fb7aa9":"df_train['TransactionAmt'] = np.log(df_train['TransactionAmt'])\ndf_test['TransactionAmt'] = np.log(df_test['TransactionAmt'])","e978f7d1":"df_test['isFraud'] = 'test'\ndf = pd.concat([df_train, df_test], axis=0, sort=False )\ndf = df.reset_index()\ndf = df.drop('index', axis=1)","88cb126f":"def PCA_change(df, cols, n_components, prefix='PCA_', rand_seed=4):\n    pca = PCA(n_components=n_components, random_state=rand_seed)\n\n    principalComponents = pca.fit_transform(df[cols])\n\n    principalDf = pd.DataFrame(principalComponents)\n\n    df.drop(cols, axis=1, inplace=True)\n\n    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n\n    df = pd.concat([df, principalDf], axis=1)\n    \n    return df","a803ab3a":"mas_v = df_train.columns[55:394]","ab5fdb50":"from sklearn.preprocessing import minmax_scale\nfrom sklearn.decomposition import PCA\n# from sklearn.cluster import KMeans\n\nfor col in mas_v:\n    df[col] = df[col].fillna((df[col].min() - 2))\n    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n\n    \ndf = PCA_change(df, mas_v, prefix='PCA_V_', n_components=30)","4497fa66":"df = reduce_mem_usage(df)","61fd6825":"df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)","3af14c2d":"df_train.shape","4e7e76f2":"X_train = df_train.sort_values('TransactionDT').drop(['isFraud', \n                                                      'TransactionDT', \n                                                      #'Card_ID'\n                                                     ],\n                                                     axis=1)\ny_train = df_train.sort_values('TransactionDT')['isFraud'].astype(bool)\n\nX_test = df_test.sort_values('TransactionDT').drop(['TransactionDT',\n                                                    #'Card_ID'\n                                                   ], \n                                                   axis=1)\ndel df_train\ndf_test = df_test[[\"TransactionDT\"]]","22c0f1f4":"# M1-M9\n- Values are `T` `F` or `NaN`\n- Column `M4` appears to be different with values like `M2` and `M0`","9ddffffa":"# Encoding categorical features","ad4d20bc":"## DeviceType","fc31f02e":"# V1 - V339\n\n**Features engineered by Vesta**\n\nLots of 1s 0s and Nans, some larger values","f63ef268":"## ProductCD\n- For now we don't know exactly what these values represent.\n- `W` has the most number of observations, `C` the least.\n- ProductCD `C` has the most fraud with >11%\n- ProductCD `W` has the least with ~2%","40f6a534":"# Support Vector Machine\n[scikit-learn sklearn.svm.SVC documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html)","ff51a53a":"# addr1 & addr2\nThe data description states that these are categorical even though they look numeric. Could they be the address value?","629f63d2":"# Mapping emails","158432c9":"# Seting train and test back","85b80a08":"# reducing memory usage","7ff036bc":"# Data\n\nIn the competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud.\n\nThe data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information.","eaf835e4":"# Creating the model ","d3758883":"# Identity Data\nNext we will explore the identity data. These are provided for some, but not all `TransactionID`s. It contains information about the identity of the customer.\n- Categorical Features\n- `DeviceType`\n- `DeviceInfo`\n- `id_12` - `id_38`","fd3d4834":"# D1-D15\nSimilarly for features D1-D15. In these plots we can see some linear and non-linear interactions between features. We may want to create additional features using these interactions if we think it would help our model better find relationship between fraud and non-fraud observations.","3e9720ed":"# dist1 & dist2\nPlotting with logx to better show the distribution. Possibly this could be the distance of the transaction vs. the card owner's home\/work address. ","4af17d3e":"# Distribution of Target in Training Set\n- 3.5% of transacations are fraud","c207fe54":"# card1 - card6\n- We are told these are all categorical, even though some appear numeric.","e695e8d5":"## Identity info as a function of time","2623ec50":"# Logistic Regression\n\n[scikit-learn sklearn.linear_model.LogisticRegression documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)","bc2a124c":"# Categorical Features - Transaction\nWe are told in the data description that the following transaction columns are categorical:\n- ProductCD\n- emaildomain\n- card1 - card6\n- addr1, addr2\n- P_emaildomain\n- R_emaildomain\n- M1 - M9","39edd5a9":"# Train vs Test are Time Series Split\n\nThe `TransactionDT` feature is a timedelta from a given reference datetime (not an actual timestamp). One early discovery about the data is that the train and test appear to be split by time. There is a slight gap inbetween, but otherwise the training set is from an earlier period of time and test is from a later period of time. This will impact which cross validation techniques should be used.\n\nWe will look into this more when reviewing differences in distribution of features between train and test.","e0b3c620":"# Some feature engineering","2bdae4b8":"## TransactionAmt\nThe ammount of transaction. I've taken a log transform in some of these plots to better show the distribution- otherwise the few, very large transactions skew the distribution. Because of the log transfrom, any values between 0 and 1 will appear to be negative.","39e90752":"# Seting X and y","06e69d8a":"# Getting PCA ","0e32ddd0":"- 24.4% of TransactionIDs in **train** (144233 \/ 590540) have an associated train_identity.\n- 28.0% of TransactionIDs in **test** (144233 \/ 590540) have an associated train_identity.","a6512507":"**Memory usage reduction**","2f62347b":"# IEEE Fraud Detection Competition\n![fraud](https:\/\/abcountrywide.com.au\/wp-content\/uploads\/2018\/04\/fraud.jpg)\n\nIn this kernel we do some basic exploratory data analysis on the IEEE Fraud Detection dataset.\n\n\nFrom the [competition overview](https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/overview):\n\n*In this competition, you\u2019ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.*\n   \n*If successful, you\u2019ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives.*","6038a921":"# Concating dfs to get PCA of V features","3bd6c518":"- Fraudulent charges appear to have a higher average transaction ammount ","6734c5f9":"# C1 - C14\nBecause we are provided many numerical columns, we can create a pairplot to plot feature interactions. I know these plots can be hard to read, but it is helpful for gaining intution about potential feature interactions and if certain features have more variance than others."}}