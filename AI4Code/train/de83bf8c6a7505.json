{"cell_type":{"9d8e7eba":"code","aded4e94":"code","42e3ac4f":"code","946b42d4":"code","2725131a":"code","30d558f9":"code","55ac8fa5":"markdown"},"source":{"9d8e7eba":"# Libraries for concurrency and parallelism\n! pip install asks trio","aded4e94":"from pathlib import Path\nimport requests\nfrom os import cpu_count\n\nimport datatable as dt\nimport asks\nimport trio","42e3ac4f":"Path('.\/pics').mkdir(exist_ok=True)\n\nimg_urls = dt.fread(\"..\/input\/wikipedia-image-caption\/test.tsv\", sep='\\t', columns={'image_url'})\nlinks = img_urls.to_list()[0][:1000]","946b42d4":"%%time\n# fast way\n\nasync def fetch_pic(s, url):\n    r = await s.get(url)\n    return r.content\n\n\nasync def save_pic(s, url):\n    content = await fetch_pic(s, url)\n    filename = f\"pics\/{url.split('\/')[-1][:100]}\"\n    with open(filename,'wb') as f:\n        f.write(content)\n\n        \nasync def main(links):\n    dname = 'https:\/\/upload.wikimedia.org'\n    s = asks.sessions.Session(dname, connections=cpu_count()*2)\n    async with trio.open_nursery() as n:\n        for url in links:\n            n.start_soon(save_pic, s, url)\n\n            \ntrio.run(main, links)","2725131a":"%%bash\nls pics | wc -l\nls pics -U | head -6","30d558f9":"%%time\n# regular way\n\nfor url in links:\n    r = requests.get(url, stream=True)\n    content=r.content\n    filename = f\"pics\/{url.split('\/')[-1][:100]}\"\n    with open(filename,'wb') as f:\n        f.write(content)\n    ","55ac8fa5":"Here's a faster way to download images using the principles of concurrency and parallelism. In other words, asynchronous tasking and multiprocessing. It works best on machines with 8+ cores and when downloading large files like videos.\n\nPlease let me know if you have other good methods for downloading large numbers of files!"}}