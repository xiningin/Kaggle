{"cell_type":{"af75f6e2":"code","454150a4":"code","7d0cf90d":"code","240c8535":"code","6b0b2882":"markdown","06272f6f":"markdown"},"source":{"af75f6e2":"%cd \/kaggle\/working\/\n!git clone https:\/\/github.com\/LintaoPeng\/U-shape_Transformer_for_Underwater_Image_Enhancement.git\n%cd \/kaggle\/working\/U-shape_Transformer_for_Underwater_Image_Enhancement","454150a4":"!pip install -r requirements.txt","7d0cf90d":"import os\n#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport argparse\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.utils as utils\nimport time\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom glob import glob\nfrom torch.nn.modules.loss import _Loss\nfrom net.Ushape_Trans import *\n#from dataset import prepare_data, Dataset\nfrom net.utils import *\nimport cv2\nimport matplotlib.pyplot as plt\nfrom utility import plots as plots, ptcolor as ptcolor, ptutils as ptutils, data as data\nfrom loss.LAB import *\nfrom loss.LCH import *\nfrom torchvision.utils import save_image\n\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"","240c8535":"dtype = 'float32'\n\ntorch.set_default_tensor_type(torch.FloatTensor)\n\n\ndef split(img):\n    output=[]\n    output.append(F.interpolate(img, scale_factor=0.125))\n    output.append(F.interpolate(img, scale_factor=0.25))\n    output.append(F.interpolate(img, scale_factor=0.5))\n    output.append(img)\n    return output\n\n# Initialize generator\ngenerator = Generator().cuda()\ngenerator.load_state_dict(torch.load(\"\/kaggle\/input\/models\/saved_models\/G\/generator_795.pth\"))\n\n\nimg_size = 256\npath = '\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/video_0' # data directory\n\n\n\ndef predict(img_paths, stride=32, batch_size=1):\n    results = []\n    for img_path in os.listdir(img_paths):\n        img = cv2.imread(os.path.join(img_paths, img_path))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        #img = np.array(img).astype(dtype)\n        #img = torch.from_numpy(img)  # tensor (h, w, c)\n\n\n        crop = []\n        position = []\n        batch_count = 0\n\n        result_img = np.zeros_like(img)\n        voting_mask = np.zeros_like(img)\n        i = 0\n        for top in tqdm(range(0, img.shape[0], stride)):\n            for left in range(0, img.shape[1], stride):\n                piece = np.zeros([img_size, img_size, 3], np.float32)\n                temp = img[top:top + img_size, left:left + img_size, :]\n                piece[:temp.shape[0], :temp.shape[1], :] = temp\n                crop.append(piece)\n                position.append([top, left])\n                batch_count += 1\n                if batch_count == batch_size:\n\n\n                    crop = np.array(crop).astype(dtype)\n                    crop = torch.from_numpy(crop)  #tensor (b, h, w, c)\n                    crop = crop.permute(0, 3, 1, 2) # tensor (b, c, h, w)\n                    #.unsqueeze(0) => make batch 1\n                    crop = crop \/ 255.0\n                    crop = Variable(crop).cuda()\n                    output = generator(crop)*255\n                    pred = output[3].data\n\n                    # check cropped image output\n                    save_image(pred, \".\/test\/output\/cropped_\" + img_path, nrow=5, normalize=True)\n\n                    crop = []\n                    batch_count = 0\n                    for num, (t, l) in enumerate(position):\n                        piece = pred[num]\n                        piece = piece.permute(1, 2, 0)\n                        h, w, c = result_img[t:t + img_size, l:l + img_size, :].shape\n                        #result_img = torch.Tensor(result_img).cuda()\n                        piece = piece.cpu().detach().numpy()\n                        result_img = result_img.astype(dtype)\n                        result_img[t:t + img_size, l:l + img_size, :] += piece[:h, :w, :]\n                        voting_mask[t:t + img_size, l:l + img_size, :] += 1\n                    position = []\n\n\n        result_img = result_img \/ voting_mask\n        #result_img = result_img.astype(np.uint8)\n        #results.append(result_img)\n        results = np.array(result_img).astype(dtype)\n        results = torch.from_numpy(results)\n        results = results.permute(2, 0, 1).unsqueeze(0)# tensor (b, c, h, w)\n        # save image to your output folder dir\n        save_image(results, \"..\/outputdir\/\" + img_path, nrow=5, normalize=True)\n\n    return results\n\n\nif __name__ == \"__main__\":\n\n    predict(path)","6b0b2882":"## Enhance under water images with U-shape transformer without resizing image! \ud83d\udc40\n\n### [U-shape Transformer Underwater Image Enhancement]\n\npaper: https:\/\/arxiv.org\/abs\/2111.11843\n\ncode (pytorch): https:\/\/github.com\/LintaoPeng\/U-shape_Transformer\n\nBefore I start with U-shape Transformer, I found out that this code resize the output image into 256x256 which is not efficient when training with detection code.\n\nSo I edit some code lines to handle images by croping into 256 x 256 patches and preprocess each piece.  \n\nI refer to code provided in korean competition site \"DACON\" which I linked below!\n\nreference: https:\/\/dacon.io\/competitions\/official\/235746\/codeshare\/2874?page=2&dtype=recent","06272f6f":"#### download saved_model.zip folder where models saved and place it where you want\n\nhttps:\/\/drive.google.com\/file\/d\/19a_kDJTT5S96kzwQntEMhSxAPYw4xY2P\/view"}}