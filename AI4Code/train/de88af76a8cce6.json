{"cell_type":{"b0bc1907":"code","438dab06":"code","8d512b9b":"code","0bbc3039":"code","f4ea84ed":"code","f56f1d28":"code","48603d76":"code","7e258b4e":"code","896451da":"code","c22014aa":"code","86aab962":"code","f59a6cb1":"code","7eb11d15":"code","98d656c4":"code","c51dea48":"code","f1234c6b":"code","ecd9a202":"code","33ccf4d2":"code","fc24e1ae":"code","a61b8039":"code","df425432":"code","80170cda":"code","10279439":"code","2dc3ae68":"code","985b97cb":"code","ef9e6888":"code","1440afbb":"code","72f42e5f":"code","e1de1a63":"code","d67a8dbe":"code","38c1a105":"code","c7dd213c":"code","88b1cc9e":"code","637af55b":"code","8c9165d9":"code","bf8f1be4":"code","ef57e736":"code","97c59b70":"code","d2a247f5":"code","7850022b":"code","e637c0ed":"code","16d8632a":"code","e24c6538":"code","12601f7e":"code","0a1d002a":"code","deacaea1":"code","d5e3538d":"code","ab0a6db6":"code","4f31fb37":"markdown","f59ea88b":"markdown","0156d40d":"markdown","4a5b219c":"markdown","5674668c":"markdown","b992e678":"markdown"},"source":{"b0bc1907":"import numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\nimport os\nimport re\nimport string\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","438dab06":"df = pd.read_csv(\"\/kaggle\/input\/60k-stack-overflow-questions-with-quality-rate\/train.csv\")\ndf.head()","8d512b9b":"df.columns","0bbc3039":"df.info()","f4ea84ed":"df.describe(include = \"all\")","f56f1d28":"df.isnull().sum()","48603d76":"df.duplicated(subset=None, keep='first').value_counts()","7e258b4e":"def get_tech_keys(tag):\n    if(not tag):\n        return tag\n    tag = tag.replace('><', ',')\n    tag = tag.replace('<', '')\n    tag = tag.replace('>', '')\n    return tag","896451da":"df['TechKeys'] = df['Tags'].apply(get_tech_keys)","c22014aa":"tech_key_list   = []\ntech_key_values = None\nindex_counter = 0\ntech_key_index_list = []\n\nfor item in df['TechKeys']:\n    item_parts = item.split(',')\n    \n    for item_ in item_parts:\n        \n        tech_key_index_list.append(index_counter)\n        tech_key_list.append(item_)\n        index_counter += 1\n    \ndf_tech_key_new = pd.DataFrame({'id' : tech_key_index_list, 'tech_key' : tech_key_list}) ","86aab962":"df['TechKeys'].to_csv(\"Tags.csv\")","f59a6cb1":"plt.figure(figsize=(8,8))\ndf_tech_key_new.tech_key.value_counts().nlargest(10).plot(kind='barh')","7eb11d15":"def get_tags_counts(col):\n    if(not col):\n        return 0\n    tags_count = len(col.split(','))\n    return tags_count","98d656c4":"def show_donut_plot(col):\n    \n    rating_data = df.groupby(col)[['Id']].count().head(10)\n    plt.figure(figsize = (12, 8))\n    plt.pie(rating_data[['Id']], autopct = '%1.2f%%', startangle = 140, pctdistance = 1.1, shadow = True)\n\n    # create a center circle for more aesthetics to make it better\n    gap = plt.Circle((0, 0), 0.5, fc = 'white')\n    fig = plt.gcf()\n    fig.gca().add_artist(gap)\n    \n    plt.axis('equal')\n    \n    cols = []\n    for index, row in rating_data.iterrows():\n        cols.append(index)\n    plt.legend(cols)\n    \n    plt.title('Donut Plot: SOF Questions by ' +str(col), loc='center')\n    \n    plt.show()","c51dea48":"df['TagsCount'] = df['TechKeys'].apply(get_tags_counts)","f1234c6b":"show_donut_plot('TagsCount')","ecd9a202":"print(df['Y'].value_counts())\nfig = go.Figure(go.Funnelarea(\n    text = df.Y,\n    values = df.Y.value_counts(),\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Question Quality Distribution\"}\n    ))\nfig.show()","33ccf4d2":"df['Num_words_body'] = df['Body'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ndf['Num_words_title'] = df['Title'].apply(lambda x:len(str(x).split())) #Number Of words in main text","fc24e1ae":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(df[df['Y']=='HQ']['Num_words_body'], shade=True, color=\"b\").set_title('Kernel Distribution of Number Of words')\np2=sns.kdeplot(df[df['Y']=='LQ_CLOSE']['Num_words_body'], shade=True, color=\"r\")\np2=sns.kdeplot(df[df['Y']=='LQ_EDIT']['Num_words_body'], shade=True, color=\"g\")\nplt.legend(labels=['HQ','LQ_CLOSE','LQ_EDIT'])\nplt.ylabel(\"Probability Density\")\nplt.xlabel(\"Number of words\")\nplt.xlim(-20,500)","a61b8039":"def code_available(content):\n    \n    if('<code>' in content):\n        return True\n    \n    return False","df425432":"df['code_available'] = df['Body'].apply(code_available)","80170cda":"show_donut_plot('code_available')","10279439":"def get_week(col):\n    return col.strftime(\"%V\")","2dc3ae68":"df['CreationDatetime'] = pd.to_datetime(df['CreationDate']) \ndf['CreationYear'] = df['CreationDatetime'].dt.year.astype(int)\ndf['CreationMonth'] = df['CreationDatetime'].dt.month.astype(int)","985b97cb":"show_donut_plot('CreationYear')","ef9e6888":"import squarify\ndef show_treemap_tech_key(col):\n    df_type_series = df_tech_key_new.groupby(col)['id'].count().sort_values(ascending = False).head(50)\n\n    type_sizes = []\n    type_labels = []\n    for i, v in df_type_series.items():\n        type_sizes.append(v)\n        \n        type_labels.append(str(i) + ' ('+str(v)+')')\n\n\n    fig, ax = plt.subplots(1, figsize = (12,12))\n    squarify.plot(sizes=type_sizes, \n                  label=type_labels[:25],  # show labels for only first 10 items\n                  alpha=.2 )\n    plt.title('TreeMap by '+ str(col))\n    plt.axis('off')\n    plt.show()","1440afbb":"show_treemap_tech_key('tech_key')","72f42e5f":"code_start = '<code>'\ncode_end   = '<\/code>'\n\ndef get_codes(content):\n    \n    if('<code>' not in content):\n        return None\n    \n    code_list = []\n    \n    loop_counter = 0\n    while(code_start in content):\n\n        code_start_index = content.index(code_start)\n        if(code_end not in content):\n            code_end_index = len(content)\n        else:\n            code_end_index = content.index(code_end)\n\n        substring_1 = content[code_start_index : (code_end_index + len(code_end) )]\n \n        code_list.append(substring_1)\n        \n        content = content.replace(substring_1, '')\n        \n        loop_counter += 1\n\n    \n    return ' '.join(code_list)\n\ndef  clean_text(content):\n    \n    content = content.lower()\n    \n    content = re.sub('<.*?>+', '', content)\n    \n    content = re.sub(r\"(@[A-Za-z0-9]+)|^rt|http.+?\", \"\", content)\n    content = re.sub(r\"(\\w+:\\\/\\\/\\S+)\", \"\", content)\n    content = re.sub(r\"([^0-9A-Za-z \\t])\", \" \", content)\n    content = re.sub(r\"^rt|http.+?\", \"\", content)\n    content = re.sub(\" +\", \" \", content)\n\n    # remove numbers\n    content = re.sub(r\"\\d+\", \"\", content)\n    \n    return content\n\ndef get_non_codes(content):\n    \n    loop_counter = 0\n    while(code_start in content):\n\n        code_start_index = content.index(code_start)\n        if(code_end not in content):\n            code_end_index = len(content)\n        else:\n            code_end_index = content.index(code_end)\n\n        substring_1 = content[code_start_index : (code_end_index + len(code_end) )]\n\n        content = content.replace(substring_1, ' ')\n        \n        loop_counter += 1\n        \n    content = clean_text(content)\n\n    return content","e1de1a63":"%%time\ndf['Body_code'] = df['Body'].apply(get_codes)\ndf['Body_content'] = df['Body'].apply(get_non_codes)","d67a8dbe":"%%time\nstopwords1 = stopwords.words('english')\ndf['content_words'] = df['Body_content'].apply(lambda x:str(x).split())","38c1a105":"def remove_short_words(content):\n\n    new_content_list = []\n    for item in content:\n        \n        if(len(item) > 2):\n            new_content_list.append(item)\n    \n    return new_content_list","c7dd213c":"df['content_words'] = df['content_words'].apply(remove_short_words)","88b1cc9e":"df.head()","637af55b":"words_collection = Counter([item for sublist in df['content_words'] for item in sublist if not item in stopwords1])\nfreq_word_df = pd.DataFrame(words_collection.most_common(30))\nfreq_word_df.columns = ['frequently_used_word','count']\n\nfreq_word_df.style.background_gradient(cmap='YlGnBu', low=0, high=0, axis=0, subset=None)","8c9165d9":"fig = px.scatter(freq_word_df, x=\"frequently_used_word\", y=\"count\", color=\"count\", title = 'Frequently used words - Scatter plot')\nfig.show()","bf8f1be4":"fig = px.pie(freq_word_df, values='count', names='frequently_used_word', title='Stackoverflow Questions - Frequently Used Word')\nfig.show()","ef57e736":"def get_question_level(level):\n    if(not level):\n        return level\n    if(level == 'LQ_CLOSE'):\n        return 3\n    if(level == 'LQ_EDIT'):\n        return 2\n    if(level == 'HQ'):\n        return 1\n    return level\n\ndf['Level'] = df['Y'].apply(get_question_level)","97c59b70":"fig = px.sunburst(df, path=['CreationYear', 'CreationMonth'], values='Level',\n                  color='Level', hover_data=['Level'])\nfig.show()","d2a247f5":"import spacy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for advanced visualizations\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected = True)\nimport plotly.figure_factory as ff","7850022b":"nlp = spacy.load('en')","e637c0ed":"from spacy import displacy\nrand = df['Title'][1000]\ndoc = nlp(rand)\n  \ndisplacy.render(doc, style='dep', jupyter=True)","16d8632a":"nlp = spacy.load('en_core_web_lg')","e24c6538":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words = 'english')\nwords = cv.fit_transform(df.Title)\nsum_words = words.sum(axis=0)\n\nwords_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\nfrequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n\nplt.style.use('fivethirtyeight')\ncolor = plt.cm.ocean(np.linspace(0, 1, 20))\nfrequency.head(20).plot(x='word', y='freq', kind='bar', figsize=(15, 6), color = color)\nplt.title(\"Question Title - Most Frequently Occuring Words\")\nplt.show()","12601f7e":"from wordcloud import WordCloud\n\nwordcloud = WordCloud(background_color = 'lightcyan', width = 2000, height = 2000).generate_from_frequencies(dict(words_freq))\nplt.figure(figsize=(10, 10))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.title(\"Corpus of Question Text\", fontsize = 20)\nplt.show()","0a1d002a":"df.head()","deacaea1":"trace = go.Scatter3d(\n    x = df['Num_words_body'],\n    y = df['Level'],\n    z = df['TechKeys'].apply(lambda x:x.split(\",\")[0]),\n    name = '3DPlot',\n    mode='markers',\n    marker=dict(\n        size=10,\n        color = df['Level'],\n        colorscale = 'Viridis',\n    )\n)\ndf_dia = [trace]\n\nlayout = go.Layout(\n    title = 'Number of words in body vs Quality vs Tags',\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0  \n    )\n)\nfig = go.Figure(data = df_dia, layout = layout)\niplot(fig)","d5e3538d":"df.columns","ab0a6db6":"# if(level == 'LQ_CLOSE')  return 3\n# if(level == 'LQ_EDIT')   return 2\n# if(level == 'HQ')        return 1\ndf[['Title', 'TechKeys','TagsCount','Num_words_body','Num_words_title','code_available','content_words','Body_code','Body_content', 'Level']].to_csv(\"data.csv\",index=False)","4f31fb37":"Word Similarity:\nSpacy has word vector model which we can use to find similar words.","f59ea88b":"* We can clearly see that Javascript and Python are dominating the questions followed by Java.\n* Python seems to be more popular and in demand than Java\n* Android is at 4th place","0156d40d":"Equal Distribution among quality","4a5b219c":"Almost half of the contents don't have code in the body.","5674668c":"Questions closed tends to have high probability of fewer words. So hence the reason getting closed due to lack of detailing","b992e678":"Taking a random sentence for dependency parsing"}}