{"cell_type":{"811a0f5f":"code","6d970a84":"code","fd9ba650":"code","c480de6a":"code","b0cded57":"code","ea124f73":"code","e6d64559":"code","b7828ae5":"code","ae8b57ed":"code","d5415bb2":"code","288d70e5":"code","6dc5e51e":"code","96c0efb9":"code","c0a209ac":"code","842e87f9":"code","ec74c7ca":"code","4e74fa14":"code","aca1dcbe":"code","166af68b":"code","119f803c":"code","5760ee1f":"code","27ed4422":"code","83901ff3":"code","17d22690":"code","e3335dac":"code","deed6d2a":"code","34d6f287":"code","92717fcf":"code","66b100dc":"code","b4b95c9e":"code","7e7d885a":"code","e80bcfd0":"code","35a51584":"code","d3ef56de":"code","6c9b45b5":"code","20a144fd":"code","1321ba75":"code","fedc5c48":"code","b3ae08f0":"code","2d34ca50":"code","a2176922":"code","77840438":"markdown","38addc14":"markdown","246e532e":"markdown","b919d961":"markdown","fce2da95":"markdown","d66ac6e7":"markdown","694007e8":"markdown","b53030d2":"markdown","81b0cc19":"markdown","04f0c969":"markdown","44de2500":"markdown","66c062b3":"markdown","a5580e62":"markdown","1036ec78":"markdown","690916eb":"markdown","ebd38c03":"markdown","64a41ecf":"markdown"},"source":{"811a0f5f":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(palette=\"Set2\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (accuracy_score, f1_score,average_precision_score, confusion_matrix,\n                             average_precision_score, precision_score, recall_score, roc_auc_score, )\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n\n\nfrom xgboost import XGBClassifier, plot_importance\nfrom imblearn.over_sampling import SMOTE","6d970a84":"# read dataset\ndataset = pd.read_csv(\"..\/input\/Churn_Modelling.csv\")","fd9ba650":"# first five row of the dataset\ndataset.head()","c480de6a":"dataset.describe()","b0cded57":"# checking datatypes and null values\ndataset.info()","ea124f73":"dataset.drop([\"RowNumber\",\"CustomerId\",\"Surname\"], axis=1, inplace=True)","e6d64559":"dataset.head()","b7828ae5":"_, ax = plt.subplots(1, 3, figsize=(18, 6))\nplt.subplots_adjust(wspace=0.3)\nsns.countplot(x = \"NumOfProducts\", hue=\"Exited\", data = dataset, ax= ax[0])\nsns.countplot(x = \"HasCrCard\", hue=\"Exited\", data = dataset, ax = ax[1])\nsns.countplot(x = \"IsActiveMember\", hue=\"Exited\", data = dataset, ax = ax[2])","ae8b57ed":"sns.pairplot(dataset)","d5415bb2":"encoder = LabelEncoder()\ndataset[\"Geography\"] = encoder.fit_transform(dataset[\"Geography\"])\ndataset[\"Gender\"] = encoder.fit_transform(dataset[\"Gender\"])","288d70e5":"dataset[\"Age\"].value_counts().plot.bar(figsize=(20,6))","6dc5e51e":"import seaborn as sns\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nfacet = sns.FacetGrid(dataset, hue=\"Exited\",aspect=3)\nfacet.map(sns.kdeplot,\"Age\",shade= True)\nfacet.set(xlim=(0, dataset[\"Age\"].max()))\nfacet.add_legend()\n\nplt.show()","96c0efb9":"import seaborn as sns\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.heatmap(dataset.corr(), annot=True)","c0a209ac":"_, ax =  plt.subplots(1, 2, figsize=(15, 7))\ncmap = sns.cubehelix_palette(light=1, as_cmap=True)\nsns.scatterplot(x = \"Age\", y = \"Balance\", hue = \"Exited\", cmap = cmap, sizes = (10, 200), data = dataset, ax=ax[0])\nsns.scatterplot(x = \"Age\", y = \"CreditScore\", hue = \"Exited\", cmap = cmap, sizes = (10, 200), data = dataset, ax=ax[1])","842e87f9":"plt.figure(figsize=(8, 8))\nsns.swarmplot(x = \"HasCrCard\", y = \"Age\", data = dataset, hue=\"Exited\")","ec74c7ca":"import seaborn as sns\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nfacet = sns.FacetGrid(dataset, hue=\"Exited\",aspect=3)\nfacet.map(sns.kdeplot,\"Balance\",shade= True)\nfacet.set(xlim=(0, dataset[\"Balance\"].max()))\nfacet.add_legend()\n\nplt.show()","4e74fa14":"_, ax = plt.subplots(1, 2, figsize=(15, 6))\nsns.scatterplot(x = \"Balance\", y = \"Age\", data = dataset, hue=\"Exited\", ax = ax[0])\nsns.scatterplot(x = \"Balance\", y = \"CreditScore\", data = dataset, hue=\"Exited\", ax = ax[1])","aca1dcbe":"facet = sns.FacetGrid(dataset, hue=\"Exited\",aspect=3)\nfacet.map(sns.kdeplot,\"CreditScore\",shade= True)\nfacet.set(xlim=(0, dataset[\"CreditScore\"].max()))\nfacet.add_legend()\n\nplt.show()","166af68b":"plt.figure(figsize=(12,6))\nbplot = dataset.boxplot(patch_artist=True)\nplt.xticks(rotation=90)       \nplt.show()","119f803c":"plt.subplots(figsize=(11,8))\nsns.heatmap(dataset.corr(), annot=True, cmap=\"RdYlBu\")\nplt.show()","5760ee1f":"X = dataset.drop(\"Exited\", axis=1)\ny = dataset[\"Exited\"]","27ed4422":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","83901ff3":"import warnings\nwarnings.filterwarnings('ignore')\ndef initialize():\n    global overfit_param , test_accuracy ,train_accuracy ,model , F1\n    overfit_param=[]\n    test_accuracy=[]\n    train_accuracy=[]\n    F1 =[]\n    model=[]\ndef LR():\n    from sklearn.linear_model import LogisticRegression\n    clf = LogisticRegression(solver = 'liblinear' , random_state=0)#random_state=0, solver='lbfgs',multi_class='multinomial').fit(df_train, y_train)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('Logistic Regression')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------Logistic Regression----------\")\n\ndef SGD():\n    from sklearn.linear_model import SGDClassifier\n    clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('SGDClassifier')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------SGD----------\")\n\ndef svm_scale():\n    from sklearn import svm\n    clf = svm.SVC(gamma='scale')\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('SVM - Gamma --> scale')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------SVM - Gamma --> scale----------\")\n\ndef svm_auto():\n    from sklearn import svm\n    clf = svm.SVC(gamma='auto')\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('SVM - Gamma --> auto')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------SVM - Gamma --> auto----------\")\n\ndef NuSVC():\n    from sklearn.svm import NuSVC\n    clf = NuSVC(gamma='scale')\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('NuSVM - Gamma --> scale')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------NuSVC----------\")\n\ndef LinearSVC():\n    from sklearn.svm import LinearSVC\n    clf = LinearSVC(random_state=0, tol=1e-5)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('LinearSVM - Gamma --> scale')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------LinearSVM - Gamma --> scale----------\")\n\ndef KNN():\n    from sklearn.neighbors import  KNeighborsClassifier\n    from sklearn.pipeline import Pipeline\n    clf= KNeighborsClassifier(n_neighbors=2)\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('KNeighborsClassifier')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------KNN----------\")\n\n\ndef Gaussian():\n    from sklearn.naive_bayes import GaussianNB\n    clf = GaussianNB()\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('GaussianNB')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------Gaussian----------\")\n\n\ndef Bernouli():\n    from sklearn.naive_bayes import BernoulliNB\n    clf = BernoulliNB()\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('BernouliNB')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------Bernouli----------\")\n\n\n\ndef Tree():\n    from sklearn import tree\n    clf = tree.DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('tree')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------Tree----------\")\n\ndef RandomForest():\n    from sklearn.ensemble import RandomForestClassifier\n    clf = RandomForestClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('Random Forest')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------Random Forest----------\")\n\ndef Adaboost():\n    from sklearn.ensemble import AdaBoostClassifier\n    clf = AdaBoostClassifier(n_estimators=100)\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('Ada Boost')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------Ada Boost----------\")\n\ndef Gradient():\n    from sklearn.ensemble import GradientBoostingClassifier\n    clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('Gradient Boosting')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------Gradient Boosting----------\")\n\ndef MLP():\n    from sklearn.neural_network import MLPClassifier\n    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('MLPClassifier')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------MLP----------\")\n\ndef Cat():\n    from catboost import CatBoostClassifier\n    clf = CatBoostClassifier(depth=8,iterations=30,learning_rate=0.1,verbose=False)\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('Cat Classifier')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------CAT----------\")\n    \ndef Light():\n    from lightgbm import LGBMClassifier\n    clf = LGBMClassifier( random_state=5)\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('Light Classifier')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------Light----------\")\n    \ndef XGB():\n    from xgboost.sklearn import XGBClassifier\n    clf = XGBClassifier(depth=8,iterations=30,learning_rate=0.1,verbose=False)\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import cross_val_score\n    accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv = 10)\n    ta = accuracy.mean()\n    accuracy = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv = 10)\n    tsa = accuracy.mean()\n    overfit_param.append(ta-tsa)\n    test_accuracy.append(tsa)\n    train_accuracy.append(ta)\n    model.append('XGB')\n    y_true=y_test\n    from sklearn.metrics import f1_score\n    F1.append(f1_score(y_true, y_pred, average='weighted'))\n    print(\"----------XGB----------\")\ndef ML():\n    initialize()\n    LR()\n    SGD()\n    svm_scale()\n    svm_auto()\n    #\/NuSVC()\n    LinearSVC()\n    KNN()\n    Gaussian()\n    Bernouli()\n    Tree()\n    RandomForest()\n    Adaboost()\n    Gradient()\n    MLP()\n    Cat()\n    Light()","17d22690":"ML()","e3335dac":"MLmod ={\n    'Model Used' : model,\n    'Overfir Quotient' :overfit_param,\n    'Train Accuracy':train_accuracy,\n    'Test Accuracy':test_accuracy,\n    'F1 SCORE':F1\n}\n\nFinalAssesmentModel = pd.DataFrame(MLmod)\nFinalAssesmentModel","deed6d2a":"plt.figure(figsize=(14,8))\nax = sns.lineplot(x=\"Model Used\", y=\"F1 SCORE\", data=FinalAssesmentModel ,marker='D' )\nplt.xticks(rotation=30)","34d6f287":"plt.figure(figsize=(14,8))\nax = sns.lineplot(x=\"Model Used\", y=\"Overfir Quotient\", data=FinalAssesmentModel ,marker='D' )\nplt.xticks(rotation=30)","92717fcf":"plt.figure(figsize=(14,8))\nax = sns.lineplot(x=\"Model Used\", y=\"Test Accuracy\", data=FinalAssesmentModel ,marker='D' )\nplt.xticks(rotation=30)","66b100dc":"plt.figure(figsize=(14,8))\nax = sns.lineplot(x=\"Model Used\", y=\"Train Accuracy\", data=FinalAssesmentModel ,marker='D' )\nplt.xticks(rotation=30)","b4b95c9e":"#standardizing the input feature\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)\nX","7e7d885a":"X.shape","e80bcfd0":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","35a51584":"from keras import Sequential\nfrom keras.layers import Dense","d3ef56de":"classifier = Sequential()\n#First Hidden Layer\nclassifier.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=10))\n#Second  Hidden Layer\nclassifier.add(Dense(4, activation='relu', kernel_initializer='random_normal'))\n#Output Layer\nclassifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))","6c9b45b5":"#Compiling the neural network\nclassifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])","20a144fd":"#Fitting the data to the training dataset\nhistory = classifier.fit(X_train,y_train, validation_split=0.13, epochs=150, batch_size=10, verbose=0)","1321ba75":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","fedc5c48":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","b3ae08f0":"eval_model=classifier.evaluate(X_train, y_train)\neval_model","2d34ca50":"y_pred=classifier.predict(X_test)\ny_pred =(y_pred>0.5)","a2176922":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","77840438":"## Prediction with ML models:","38addc14":"### __Detecting Outliers using Tukey Boxplot__","246e532e":"# Bank Customer Churn Prediction\nIn this kernel I am going to make an __Exploratory Data Analysis (EDA)__ on [this](https:\/\/www.kaggle.com\/filippoo\/deep-learning-az-ann) dataset. Also I am going to make different predictive models and find out the best one with highest prediction accuracy. \n\n### Kernel Outlines:\n* __Importing Necessary Packages__\n* __Statistical Summary of the Dataset__\n* __Dropping Irrelevant Features__\n* __One Hot Encoding__\n* __Data Visualization__\n* __Detecting Outliers using Tukey Boxplot__\n* __Hand written function for detecting and removing outliers__\n* __Checking Correlation with Heatmap__\n* __Different ML predictive models__\n    * Gaussian Naive Bayes\n    * Logistic Regression\n    * Decision Tree\n    * Random Forest\n    * Extra Gradient Boosting Tree (XGBoost)\n* __Improve the Predictive Model__\n    * Feature Scaling\n    * Over Sampling","b919d961":"### __Checking Correlation__","fce2da95":"We have 8 input features and one target variable. 2 Hidden layers. Each hidden layer will have 4 nodes.\nReLu will be the activation function for hidden layers. As this is a binary classification problem we will use sigmoid as the activation function.\nDense layer implements\noutput = activation(dot(input, kernel) + bias)\nkernel is the weight matrix. kernel initialization defines the way to set the initial random weights of Keras layers.\nRandom normal initializer generates tensors with a normal distribution.\nFor uniform distribution, we can use Random uniform initializers\nKeras provides multiple initializers for both kernel or weights as well as for bias units.\n\n![](https:\/\/miro.medium.com\/max\/1400\/1*GTLzJ0sUmwDPb9uVffnZ6g.png)\n\n\n","d66ac6e7":"### Importing Necessary Packages","694007e8":"We now split the input features and target variables into training dataset and test dataset. out test dataset will be 30% of our entire dataset.","b53030d2":"> __That's  it for this kernel.__ If you like this kernel then give a upvote! ","81b0cc19":"# Prediction with Deep learning Models","04f0c969":"> * __40 to 70 years old customers are higher chances to churn__\n* __Customer with `CreditScore` less then `400` are higher chances to churn__","44de2500":"### __The statistical summary of the dataset__","66c062b3":"Once the different layers are created we now compile the neural network.\nAs this is a binary classification problem, we use binary_crossentropy to calculate the loss function between the actual output and the predicted output.\nTo optimize our neural network we use Adam. Adam stands for Adaptive moment estimation. Adam is a combination of RMSProp + Momentum.\nMomentum takes the past gradients into account in order to smooth out the gradient descent.\nwe use accuracy as the metrics to measure the performance of the model","a5580e62":"__Customer with 3 or 4 products are higher chances to Churn__","1036ec78":"### Dropping Irrelevant Feature\n`RowNumber`, `CustomerId` and `Surname` are irrelivant, so we drop those features.","690916eb":"## Data Visualization ","ebd38c03":"we now fit out training data to the model we created. we use a batch_size of 10. This implies that we use 10 samples per gradient update.\nWe iterate over 100 epochs to train the model. An epoch is an iteration over the entire data set.","64a41ecf":"* We have preprocessed the data and we are now ready to build the neural network.\n* We are using keras to build our neural network. We import the keras library to create the neural network layers.\n* There are two main types of models available in keras \u2014 Sequential and Model. we will use Sequential model to build our neural network.\n* We use Dense library to build input, hidden and output layers of a neural network."}}