{"cell_type":{"a4ceed0e":"code","c141f41f":"code","87d9b11b":"code","89d5fc3a":"code","334b8ccd":"code","c660e179":"code","fa7d03ec":"code","728531db":"code","9ab540fc":"code","5af11902":"code","648e70f8":"code","4a06b1c3":"code","ce0bdf83":"code","89e6133f":"code","5f0f5ed6":"code","d23274e7":"code","f774ab0b":"code","33887b5e":"code","db1f2230":"code","56df50f1":"code","7d49e6e5":"code","2c2e715f":"code","192dfbbb":"code","aa70c620":"code","20dbed30":"code","2181740c":"code","4bbb042b":"code","55284bd0":"code","1c6c83e9":"code","3d673796":"code","e03b0d94":"code","f3bb8b88":"code","8256d39f":"code","10086f40":"code","2bba4724":"code","35f9b30f":"markdown","22fbc8b7":"markdown","377dc094":"markdown","bfc8fe00":"markdown","560e28b3":"markdown","61f55120":"markdown","6c93d57c":"markdown","43b3b8de":"markdown","3a7f4939":"markdown","6f52b463":"markdown","375bddcc":"markdown","4d73b3ad":"markdown","4f0c050d":"markdown","c0ad2a1a":"markdown","910c4d98":"markdown","fc8de715":"markdown","37174cfe":"markdown","3fe30c86":"markdown"},"source":{"a4ceed0e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, MaxPooling2D, ZeroPadding2D, Conv2D","c141f41f":"# Input data files are available in the \"..\/input\/\" directory.\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","87d9b11b":"train.shape, test.shape","89d5fc3a":"train.head()\n# test.head()","334b8ccd":"X=train.drop(['label'], axis=1)\ny=train['label']\n\nX.shape, y.shape","c660e179":"# checking manually\ny.value_counts()","fa7d03ec":"plt.subplots(figsize=(10,8))\nplt.title('Counts of the labels')\nsns.countplot(x=y)\nplt.show()","728531db":"X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.1, random_state=99)\n# check the shape now\nX_train.shape,X_val.shape,y_train.shape,y_val.shape,test.shape","9ab540fc":"X_train=X_train.values.astype('float32')\nX_val=X_val.values.astype('float32')\ntest=test.values.astype('float32')","5af11902":"# changing the shape of X_train and y_train and test also\nX_train=X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_val=X_val.reshape(X_val.shape[0], 28, 28, 1)\ntest=test.reshape(test.shape[0] , 28 , 28 , 1)","648e70f8":"X_train.shape,X_val.shape,test.shape","4a06b1c3":"# check the maximum values in the dataset\nX_train.max(),X_train.min()","ce0bdf83":"X_train=X_train\/255\nX_val=X_val\/255\ntest=test\/255","89e6133f":"# check the maximum values in the dataset\nX_train.max(),X_train.min()","5f0f5ed6":"input_shape=X_train[0].shape\ninput_shape","d23274e7":"model = Sequential()\nmodel.add(Conv2D(100,kernel_size=(3, 3), activation='relu',padding='same', input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n\n# model.add(Conv2D(350, kernel_size=(5, 5),activation='relu',padding='same') )\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.2))\n\n\n# model.add(Conv2D(150, kernel_size=(5, 5),activation='relu',padding='same') )\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.2))\n\nmodel.add(Conv2D(50, kernel_size=(3, 3),activation='relu',padding='same') )\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(BatchNormalization())\n\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))","f774ab0b":"# check the summary[\":\"]\nmodel.summary()","33887b5e":"model.compile(optimizer= 'adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","db1f2230":"%%time\nhistory=model.fit(X_train, y_train, batch_size=60, epochs=10, verbose=1, validation_data=(X_val,y_val))","56df50f1":"# evaluating the model with testing data\nloss, accuracy=model.evaluate(X_val,y_val)\nloss, accuracy","7d49e6e5":"# plot the figure now\npd.DataFrame(history.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.show()","2c2e715f":"# plot confusion matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix","192dfbbb":"y_pred = model.predict_classes(test)","aa70c620":"y_pred","20dbed30":"class_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\nclass_names","2181740c":"mat=confusion_matrix(y_val, y_pred[:4200])\nplot_confusion_matrix(conf_mat=mat, class_names= class_names,show_normed=True, figsize=(7,7))","4bbb042b":"# predict results\nresults = model.predict(test)\n","55284bd0":"results","1c6c83e9":"\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\n","3d673796":"results","e03b0d94":"results = pd.Series(results,name=\"Label\")","f3bb8b88":"results","8256d39f":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\n","10086f40":"submission","2bba4724":"submission.to_csv(\"submission.csv\",index=False)","35f9b30f":"Now check the shape again","22fbc8b7":"Now our data has been normalized, we can also scale the data by using MinMaxScaler as well as Standard Scaler\n\n**Now we will Build the Model**\n\nBefore building the model here we will need to pass the input shape","377dc094":"Architecture 400-350-150-75\nWith Padding, Batchnormalization and Dropout","bfc8fe00":"Evaluate the model","560e28b3":"Fitting the model","61f55120":"Making the prediction using the model","6c93d57c":"Plotting the same","43b3b8de":"Import the important libraries","3a7f4939":"Import the data of training and testing from the library","6f52b463":"Here the shape of our data is not according to the CNN architecture, so we will reshape the data into CNN architecture that is (images,rows,cols,channels) Here the images will be the no of the images used , rows and columns will be the pixels of the images mentioned in the dataset descriptions which are 28 * 28 . since all images are gray scale so it will only use '1' channel","375bddcc":"Here we first we will convert the values of the data into float32, by which the three dataframes will get converted into a numpy array","4d73b3ad":"Splitting the data into X and y(i.e independent and dependent varaible in simple terms)","4f0c050d":"We can easily that the data range is between 0 to 255, here we need to normalize the data to bring it into the range of 0 to 1 so that our model predicts the data more efficiently","c0ad2a1a":"Now let us check whether the target variable is imbalanced or not in the training data","910c4d98":"Compile the model","fc8de715":"We can easily conclude from the above that the data is not unbalanced\n\nNow we will split the data into training and testing","37174cfe":"We will plot the data","3fe30c86":"Check the shape of the training and testing data "}}