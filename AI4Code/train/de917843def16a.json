{"cell_type":{"bed77dea":"code","4a876c1f":"code","293ef13d":"code","c6757451":"code","6418c565":"code","c9d76602":"code","6785a8fb":"code","6378fc62":"code","b3d81e5f":"code","fc16cf18":"code","6216cc86":"code","e8d44f93":"code","6d2048eb":"code","0aca7775":"markdown","7d5914bb":"markdown","97b48e0e":"markdown","08890302":"markdown","f54c7147":"markdown","20a6e3b4":"markdown","eab73adc":"markdown","7a749da9":"markdown","ca1b589c":"markdown","68e7ee61":"markdown"},"source":{"bed77dea":"import pandas as pd\ndf = pd.read_csv(\"\/kaggle\/input\/search-engine-results-flights-tickets-keywords\/flights_tickets_serp2020-04-01.csv\")\ndf.head(3)","4a876c1f":"df.describe()","293ef13d":"df.info()","c6757451":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,6))\nsns.heatmap(df.isnull())\nplt.title(\"Missing values?\",fontsize = 16)\nplt.show()","6418c565":"# Let's have a more accurate look at the missing values\ndf.isnull().sum()","c9d76602":"s = df[\"searchTerms\"].nunique()\nr = df[\"rank\"].nunique()\nprint(f\"Number of search terms: {s}\\nNumber of ranks: {r}\\nNumber of entries: {df.shape[0]}\")","6785a8fb":"# Estimation of percentage of traffic by google results position\ntraffic = {1: 0.33, \n           2: 0.18, \n           3: 0.12, \n           4: 0.08, \n           5: 0.06, \n           6: 0.05, \n           7: 0.04, \n           8: 0.03, \n           9: 0.02, \n           10: 0.02}\n\n# Display the percentage of traffic with colors\ncolors = []\nfor i in range(10):\n    xc = round(0.7-0.05*i,2)\n    c = (xc,xc,0.5)\n    colors.append(c)\ncolors = colors + [\"red\"]\n    \nx = [str(t) for t in range(1,11)] +[\">10\"]\ny = [traffic[key]*100 for key in traffic.keys()]\ny += [100-sum(y)]\n\nplt.figure(figsize=(8,6))    \nplt.bar(x,y, color = colors)\nplt.title(\"Estimation of percentage of traffic by ranking\\nin Google search results\", fontsize = 16)\nplt.xticks(x)\nplt.xlabel(\"# rank\")\nplt.ylabel(\"% of traffic\")\nplt.show()\n","6378fc62":"# Select the columns we will use\ndf = df[[\"searchTerms\", \"rank\", \"title\", \"snippet\", \"displayLink\"]].copy()\n\n# Calculate the percentage of the total traffic by website\ndf[\"traffic%\"] = df[\"rank\"].map(traffic)\nbyTraffic = pd.pivot_table(df, values = \"traffic%\", index = \"displayLink\", aggfunc = \"sum\").sort_values(\"traffic%\", ascending = False)\nnb_terms = df[\"searchTerms\"].nunique()\n\nbyTraffic[\"traffic%\"] = byTraffic[\"traffic%\"].apply(lambda x: (100\/2) * (x\/nb_terms))\n\n# Display the percentage of the total traffic by website\n# Supposing that each term has an equal amount of traffic\n# what isn't true. It is the only way get an estimation,\n# because the dataset doesn't have the amount of traffic\n# by search term.\n\n# number of top websites to plot\nnb_website = 20\n\n# Display the result with colors\ncolors = []\nfor i in range(nb_website):\n    x = round(0.7-0.02*i,2)\n    c = (x,x,0.5)\n    colors.append(c)\n\nplt.figure(figsize=(10,6))\nplt.title(f\"Percentage of traffic on {nb_terms} search terms by website\", fontsize = 16)\nbyTraffic[\"traffic%\"].iloc[:nb_website].plot.bar(color = colors)\nplt.xlabel(\"\")\nplt.ylabel(\"% of total traffic\")\nplt.show()","b3d81e5f":"# List with the top 4 websites in average ranking\ntop4 = byTraffic[\"traffic%\"].iloc[:4].index","fc16cf18":"# Calculate the % of keywords\/search terms in the titles\ndf[\"%search_term_in_title\"] = df[\"searchTerms\"].apply(lambda x: len(x.split(\" \"))) \/ df[\"title\"].apply(lambda x: len(x.split(\" \")))\ndf[\"%search_term_in_title\"] = 100 * df[\"%search_term_in_title\"] # Convert the result in %\n\n# Display the result\nproc_searchterm_rank = pd.pivot_table(df, values = \"%search_term_in_title\", index = \"rank\", aggfunc = \"mean\").sort_index(ascending = False)\nproc_searchterm_rank.plot.barh(figsize = (8,5), color = (0.32, 0.32, 0.5))\nplt.legend(\"\")\nplt.xlabel(\"Exact keyword concentration in %\", fontsize = 12)\nplt.ylabel(\"# rank\", fontsize = 14)\nplt.title(\"Average keyword concentration in titles\\nby rank\", fontsize = 16)\nplt.show()","6216cc86":"# Calculate the % of keywords\/search terms in the snippet\ndf[\"%search_term_in_snippet\"] = df[\"searchTerms\"].apply(lambda x: len(x.split(\" \"))) \/ df[\"snippet\"].apply(lambda x: len(x.split(\" \")))\ndf[\"%search_term_in_snippet\"] = 100 * df[\"%search_term_in_snippet\"] # Convert the result in %\n\n# Display the result\nproc_snippet_rank = pd.pivot_table(df, values = \"%search_term_in_snippet\", index = \"rank\", aggfunc = \"mean\").sort_index(ascending = False)\nproc_snippet_rank.plot.barh(figsize = (8,5), color = (0.32, 0.32, 0.5))\nplt.legend(\"\")\nplt.xlabel(\"Exact keyword concentration in %\", fontsize = 12)\nplt.ylabel(\"# rank\", fontsize = 14)\nplt.title(\"Average keyword concentration in snippets\\nby rank\", fontsize = 16)\nplt.show()","e8d44f93":"# Import and merge all the csv files\nimport os\nlst_df_path = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        lst_df_path.append(os.path.join(dirname, filename))\n        \n# List the dates of the csv files\n# Be aware that some months are missing\nsorted([d[-14:-4] for d in lst_df_path ])","6d2048eb":"# Charge all the datasets\nlst_df = []\nfor d in range(len(lst_df_path)):\n    lst_df.append(pd.read_csv(lst_df_path[d]))       \n\n# Merge all the dataset\nall_df = pd.concat(lst_df)\n\n# Reset index\nall_df = all_df.reset_index().drop(\"index\",axis = 1)\n\n# Cut the year and the month of the date\n# 2019-03-15 11:26:42.227730+00:00 => 2019-03\n\n# Convert to datetime: datetime will automatically take the first day of each month\n# 2019-03 => 2019-03-01\nall_df[\"queryTime\"] = pd.to_datetime(all_df[\"queryTime\"].apply(lambda x: x[:7]))\n\n# Select the columns we will use\nall_df = all_df[[\"searchTerms\", \"rank\",\"displayLink\",\"queryTime\"]].copy()\n\n# Calculate the percentage of the total traffic by website\nall_df[\"traffic%\"] = all_df[\"rank\"].map(traffic)\n\navg_rank_bymonth = pd.pivot_table(all_df, \n                                  values = \"traffic%\", \n                                  index = \"displayLink\", \n                                  columns=\"queryTime\", \n                                  aggfunc=\"sum\")\n\n# Select the top 4 websites\navg_rank_bymonth = avg_rank_bymonth.loc[top4]\n\n# Ajust the values to be in percentage\navg_rank_bymonth = avg_rank_bymonth*(100\/2)\/nb_terms\n\n# For an unknown reason the values of some month are \n# multiplicated by two\n# Use an ad-hoc solution to fix it, dividing those\n# values by two (if you find the issue, write me please)\nmin_avg = avg_rank_bymonth.sum(axis=0).min()\nsum_avg = avg_rank_bymonth.sum(axis=0)\nidx_change = sum_avg[sum_avg >= 2*min_avg].index\navg_rank_bymonth[idx_change] \/= 2\n\n# Transpose the dataframe to be able to plot it well\navg_rank_bymonth = avg_rank_bymonth.transpose()\n\n# Rename the columns: www.skyscanner.com => skyscanner\nnew_column_names = [c.split(\".\")[1] for c in avg_rank_bymonth.columns]\navg_rank_bymonth.columns = new_column_names\n\n# Display the results\n# Be aware that some months are missing\n# even if it isn't obvious on the graphic\navg_rank_bymonth.plot(figsize = (12,8), lw = 5)\nplt.title(\"% of the traffic for each website\", fontsize = 16)\nplt.ylabel(\"%\")\nplt.ylim(0, avg_rank_bymonth.max().max()+2)\nplt.xlabel(\"\")\nplt.legend(fontsize = 14)\nplt.show()","0aca7775":"The missing values are only in the columns we won't use; therefore we will let them as they are.","7d5914bb":"<img src=\"https:\/\/i.imgur.com\/JwDSQEY.png\" alt = \"seo\">\n \n\n# Table of contents\n\n[<h3>1. Presentation of the data<\/h3>](#1)\n\n[<h3>2. Data exploration<\/h3>](#2)\n\n[<h3>3. Which websites get more traffic?<\/h3>](#3)\n\n[<h3>4. Best keyword concentration to rank high<\/h3>](#4)\n\n[<h3>5. Analysis of the changing average traffic<\/h3>](#5)","97b48e0e":"# 4. Best keyword concentration to rank high<a class=\"anchor\" id=\"4\"><\/a>\nAs we have seen before a slightly better ranking for a search term makes a huge difference in the traffic for the website. To jump between the second and the first rank doubles the traffic, and this can also double the profit. Therefore, we will have a look at the optimal quantity of keywords in the title and in the snippet to get a better ranking.","08890302":"# 3. Which websites get more traffic?<a class=\"anchor\" id=\"3\"><\/a>","f54c7147":"Surprisingly the concentration of search terms in the title and snippet and very similar for each ranking. There is a fierce competition for the search terms in the dataset, therefore we can expect only high optimized websites to rank well. Maybe is this concentration of search terms an optimized one? We would need a dataset with more ranks than only the 10 first ones to analyze it.","20a6e3b4":"# 2. Data exploration<a class=\"anchor\" id=\"2\"><\/a>","eab73adc":"The first websites get most of the traffic. All the websites together, which don't rank on the first page, get only around 7-8% of the total traffic.","7a749da9":"In the following, the percentage of the total traffic by website (for the top 4) will be displayed supposing that each term has an equal amount of traffic what isn't true. It is the only way get an estimation because the datasets don't have the amount of traffic by search term.","ca1b589c":"# 1. Presentation of the data<a class=\"anchor\" id=\"1\"><\/a>\n\nOne of the important tasks in SEO analysis, is to check rankings on search engines.\nThis dataset contains Google rankings for flights and tickets keywords.                               \n\nIt is basically an example dataset that can be generated with the serp_goog function from\nadvertools.\n\n## Content\n- 100 Destinations\n- 2 Keyword variations (flights to destination and tickets to destination)\n- 2 Countries\n- 10 Results each<br>\n\n=> 4,000 rows<br><br>\nThe same data set is produced every 15 days, in order to track the changes in ranks over time. Each file represents a point in time (either the 1st or the 15th of the month)","68e7ee61":"# 5. Analysis of the changing average traffic<a class=\"anchor\" id=\"5\"><\/a>\nIn this moment there are 24 csv files with the ranks for the same keywords, but they all have different dates. We can use them to analyse the evolution in the ranking of some websites."}}