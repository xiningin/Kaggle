{"cell_type":{"fbffe05c":"code","8c663d8e":"code","5c0d0a58":"code","cb7c2875":"code","78b8a493":"code","a8351dcd":"code","f3df003e":"code","ccb7aaaa":"code","1353c2cc":"code","f9553dd8":"code","e2406e14":"code","590c7d84":"code","44735941":"code","2e78503a":"code","42ce7270":"code","bedcb123":"code","d4489ce5":"code","bb826196":"code","511926ab":"code","f8a7e92f":"code","460d43fa":"code","e54610f2":"code","c8c50c33":"code","cfcdda25":"code","bb4e77ee":"code","3ce6e4da":"code","2318ae58":"code","35e51829":"code","91234f0c":"code","b7b46d27":"code","5ee45590":"code","d87756bf":"code","48b69eeb":"code","b31f455e":"code","a6963b31":"code","e0043c87":"code","ec7e2e9f":"code","207dcefe":"code","c5193a70":"code","b6cf4dd6":"code","58789e94":"code","79f38479":"code","45fc3ea6":"code","79edd5fe":"code","a23600be":"code","11ffb984":"code","dfe9ddd2":"markdown","43d4ce76":"markdown","15031168":"markdown","d491f257":"markdown","47656126":"markdown","23b42e01":"markdown","b4aeda60":"markdown","f423abdc":"markdown","edc79f17":"markdown","0703af32":"markdown","66444817":"markdown","d4d018ae":"markdown","ead09fd3":"markdown","93a0ac87":"markdown","2e64237a":"markdown","a8b92cd8":"markdown","699192c5":"markdown","0a5666b9":"markdown","fe03c146":"markdown","1d810280":"markdown","aa503284":"markdown","23e2ff6c":"markdown","a37b95a2":"markdown","77795ff8":"markdown","71b8a9f5":"markdown","05a9c257":"markdown","ce588c6a":"markdown","97319b35":"markdown","dbd0fbf8":"markdown","14ea19a5":"markdown","afa321c4":"markdown","36c414e9":"markdown"},"source":{"fbffe05c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c663d8e":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","5c0d0a58":"dataset = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","cb7c2875":"dataset.info()","78b8a493":"dataset.head()","a8351dcd":"plt.figure(figsize = (12,6))\nsns.countplot(x = 'smoking', hue = 'DEATH_EVENT', data = dataset)","f3df003e":"dataset.groupby(['smoking', 'DEATH_EVENT']).count()","ccb7aaaa":"plt.figure(figsize = (12,6))\nsns.countplot(x = 'high_blood_pressure', hue = 'DEATH_EVENT', data = dataset)","1353c2cc":"dataset.groupby(['high_blood_pressure', 'DEATH_EVENT']).count()","f9553dd8":"plt.figure(figsize = (12,6))\nsns.countplot(x = 'anaemia', hue = 'DEATH_EVENT', data = dataset)","e2406e14":"dataset.groupby(['anaemia', 'DEATH_EVENT']).count()","590c7d84":"plt.figure(figsize=(12,6))\nsns.countplot(x = 'diabetes', hue = 'DEATH_EVENT', data = dataset)","44735941":"dataset.groupby(['diabetes', 'DEATH_EVENT']).count()","2e78503a":"sns.boxplot(x = 'DEATH_EVENT', y = 'creatinine_phosphokinase', data = dataset)","42ce7270":"sns.boxplot(x = 'DEATH_EVENT', y = 'ejection_fraction', data = dataset)","bedcb123":"sns.boxplot(x = 'DEATH_EVENT', y = 'platelets', data = dataset)","d4489ce5":"sns.boxplot(x = 'DEATH_EVENT', y = 'serum_creatinine', data = dataset)","bb826196":"sns.boxplot(x = 'DEATH_EVENT', y = 'serum_sodium', data = dataset)","511926ab":"sns.boxplot(x = 'DEATH_EVENT', y = 'time', data = dataset)","f8a7e92f":"sns.boxplot(x = 'DEATH_EVENT', y = 'age', data = dataset)","460d43fa":"plt.figure(figsize = (16,10))\ncorr = dataset.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask = mask, annot = True, cmap = 'viridis')","e54610f2":"corr['DEATH_EVENT'].drop('DEATH_EVENT').sort_values(ascending=True).plot.bar()","c8c50c33":"X = dataset.loc[:, ['serum_creatinine','ejection_fraction', 'time']].values\ny = dataset.iloc[:, -1].values\n# Splitting to training set and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n# Scaling the data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","cfcdda25":"from sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', random_state = 0)\nclassifier.fit(X_train, y_train)","bb4e77ee":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall: {:.2f} %\".format(recall_score(y_test, y_pred)*100))","3ce6e4da":"from sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)","2318ae58":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall: {:.2f} %\".format(recall_score(y_test, y_pred)*100))","35e51829":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","91234f0c":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall: {:.2f} %\".format(recall_score(y_test, y_pred)*100))","b7b46d27":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)","5ee45590":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall: {:.2f} %\".format(recall_score(y_test, y_pred)*100))","d87756bf":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","48b69eeb":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall: {:.2f} %\".format(recall_score(y_test, y_pred)*100))","b31f455e":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","a6963b31":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall: {:.2f} %\".format(recall_score(y_test, y_pred)*100))","e0043c87":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 50, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","ec7e2e9f":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall: {:.2f} %\".format(recall_score(y_test, y_pred)*100))","207dcefe":"from catboost import CatBoostClassifier\nclassifier = CatBoostClassifier()\nclassifier.fit(X_train, y_train)","c5193a70":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall: {:.2f} %\".format(recall_score(y_test, y_pred)*100))","b6cf4dd6":"from xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)","58789e94":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall: {:.2f} %\".format(recall_score(y_test, y_pred)*100))","79f38479":"from sklearn.ensemble import GradientBoostingClassifier\nclassifier = GradientBoostingClassifier(max_depth=2, random_state=4)\nclassifier.fit(X_train, y_train)","45fc3ea6":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall: {:.2f} %\".format(recall_score(y_test, y_pred)*100))","79edd5fe":"from catboost import CatBoostClassifier\nclassifier = CatBoostClassifier()\nclassifier.fit(X_train, y_train)","a23600be":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","11ffb984":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","dfe9ddd2":"**With the .info() method, we can check the information of the dataset. For instance, the names of the columns, the number of data points, their type and how many null elements exist.**","43d4ce76":"*Thus far, we have investigated the categorical variables of this dataset. In the next step, we are going to investigate those parameters which show continuous, numerical values.*","15031168":"XGBoost Classfier","d491f257":"*In the graph above, we can notice that the levels of platelets in the blood of the patients who survived and those who did not survive are similar. Therefore, we can conclude that the level of platelets in the blood of patients is not a strong indicator.*","47656126":"**In this stage of the project, Exploratory Data Analysis (EDA) was performed, in order to identify potential predictors for death events. Moreover, we can thus explore correlations in the data, which could be useful for our analysis and would provide us with a more detailed picture of our data.**","23b42e01":"Naive Bayes Classifier","b4aeda60":"*In the above graph, it is showed that patients that did not survive had on average lower levels of serum sodum in their blood.*","f423abdc":"*In the following figure, it is shown that age and levels of serum creatinine have a considerable positive correlation to the possibility of a death event. In contrast, ejection fraction level and follow-up time have a high negative correlation with a possibility of a death event.*","edc79f17":"Support Vector Classifier with rbf kernel","0703af32":"*The first variable to be explored is the level of creatinine phosphokinase in the blood of the patients. From the graph above, it can be deduced that people who survived and people who did not survive excibit similar levels of creatinine phosphokinase.*","66444817":"Gradient Boosting Classifier","d4d018ae":"Decision Tree Classifier","ead09fd3":"**In the case of diabetes, approximately 68% of patients without diabetes did survive, with the percent of patients with diabetes who survived being the same.**","93a0ac87":"After this analysis, the algorithm that outperformed all other, was CatBoost, with an accuracy of 90% and a recall of 79%. Moreover, when performing k-fold cross validation of this approach, we had the following results:\n1. k = 5\n* Mean Accuracy =  83%\n* Accuracy Standard Adeviation = 3.8%\n2. k = 10\n* Mean Accuracy =  83%\n* Accuracy Standard Deviation = 6.5%","2e64237a":"*Given the results from the EDA above, the variables that were selected to train the model were the levels of serum creatinine in the ptients' blood, the level of ejection fraction of the patients and the follow-up time. Several classification algorithms were tested and the results are shown below.*","a8b92cd8":"**Importing the dataset**","699192c5":"*From the above figure, it can deduced that patients that did not survive were on average older compared to patients who actually survived.*","0a5666b9":"Logistic Regression Classifier","fe03c146":"CatBoost Classifier","1d810280":"k-fold validation of the CatBoost model","aa503284":"*From the graph above, it can identified that patients who survived had on average much more follow-up time compared to people that did not survive.*","23e2ff6c":"**The next potential indicator to be explored is the level of blood pressure of the patient. In the above graph, it can be shown that approximately 68% of people did not have high blood pressure and, out of them, 29% did not survive. On the other hand, about 37% who did have high blood pressure did not survive.**","a37b95a2":"*The second variable to be explored is the level of ejecton fraction of the patients. From the graph above, it can be deduced that people who survived show significant higher ejection fraction compared to patients who did not survive. Therefore, it could be considered as a potential indicator for the possibility of survival.*","77795ff8":"**With the .head() method, we get to check the first n elements of the dataset. The default number is 5.**","71b8a9f5":"**Selecting Features for Training the Classification algorithm**","05a9c257":"Random Forest Classifier, with 50 estimators","ce588c6a":"KNN Classifier, with k = 5 and distance metric equal to 'minkowski'","97319b35":"*In the above boxplot, it is displayed that patients who did not survive show significantly higher levels of serum creatinine in their blood compared to patients who survived.*","dbd0fbf8":"Support Vector Classifier with linear kernel","14ea19a5":"**The first parameter which would be interesting to investigate is how many people included in this dataset are smokers. Afterwards, we can also determine the ratio of people who were smokers and did not survived. From the graph and the table above, wen can deduce that approximately 68% of people did not smoke and, out of them, about 32.5% did not survive. On the other hand, about 31% of people who did smoke did not survive.**","afa321c4":"**In the case of anaemia, about 57% people did not have anaemia and out of them, about 29% did not survive. In contrast, about 36% of patients who did have anaemia did not survive.**","36c414e9":"*In the following heatmap, we can see how each variable is correlated to one another.*"}}