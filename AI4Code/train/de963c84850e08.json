{"cell_type":{"f9a4a981":"code","e5b242c8":"code","3b25913b":"code","ddcd58aa":"code","71b60293":"code","7468bbb9":"code","8d2f2fd6":"code","71e4d2c8":"code","bf28616a":"code","c2b5218b":"markdown","cd13b9c3":"markdown"},"source":{"f9a4a981":"import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import model_selection\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt","e5b242c8":"# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n# summarize the dataset\nprint(X.shape, y.shape)","3b25913b":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n","ddcd58aa":"#create a new random forest classifier\nrf = RandomForestClassifier()\n\n#create a dictionary of all values we want to test for n_estimators\nparams_rf = {'n_estimators': [50, 100, 200]}\n\n#use gridsearch to test all values for n_estimators\nrf_gs = GridSearchCV(rf, params_rf, cv=5)\n\n#fit model to training data\nrf_gs.fit(X_train, y_train)\n\n#save best model\nrf_best = rf_gs.best_estimator_\n\n#check best n_estimators value\nprint(rf_gs.best_params_)","71b60293":"skfold = StratifiedKFold(n_splits=3)\nmodel_skfold = RandomForestClassifier(n_estimators=100)\nresults_skfold = model_selection.cross_val_score(model_skfold,X,y, cv=skfold)\nprint(\"Accuracy: %.2f%%\" % (results_skfold.mean()*100.0))","7468bbb9":"from numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.ensemble import RandomForestRegressor","8d2f2fd6":"# define dataset\nX1, y1 = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=2)\n# summarize the dataset\nprint(X.shape, y.shape)","71e4d2c8":"# define the model\nmodel = RandomForestRegressor()\n# evaluate the model\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X1, y1, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n# report performance\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","bf28616a":"model_skfold = RandomForestRegressor(n_estimators=100)\nresults_skfold = model_selection.cross_val_score(model_skfold,X1,y1, cv=cv,n_jobs=-1)\nprint(\"Accuracy: %.2f%%\" % (results_skfold.mean()*100.0))","c2b5218b":"\nUse the make_classification() function to create a synthetic binary classification problem with \n1,000 examples and 20 input features. Use this synthetic dataset to build a classification model \nusing Random forest classifier. Evaluate your model using stratified cross fold validation. ","cd13b9c3":"Use the make_regression() function to create a synthetic binary classification problem with \n1,000 examples and 20 input features. Use this synthetic dataset to build a classification model \nusing Random forest regressor. Evaluate your model using stratified cross fold validation. "}}