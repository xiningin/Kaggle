{"cell_type":{"19b5f4d1":"code","a4b26908":"code","68583178":"code","f4c77820":"code","5e237140":"code","29f6c0c8":"code","b0cb1c04":"code","64a5843c":"code","a127fe12":"code","1a7dcd21":"code","a254072e":"code","b662925a":"code","183ab64b":"code","28e19c52":"code","648ddeb5":"code","2391a8bc":"code","07692061":"code","89a52609":"code","b6e0f834":"code","6df634cb":"code","26ce4643":"markdown","46c8467a":"markdown","3a8aed50":"markdown","484536bd":"markdown","e36a250e":"markdown","ca9192fb":"markdown","34d69703":"markdown","9a1757d7":"markdown","7236e258":"markdown","44cd1b4e":"markdown","0e1eecd2":"markdown","0688dd12":"markdown"},"source":{"19b5f4d1":"#Import necessary packages\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\n#nltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\n# nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten, Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers","a4b26908":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","68583178":"#Import dataset\ndf_train = pd.read_csv('\/kaggle\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv', delimiter='\\t')\n\ndf_train.head()","f4c77820":"#Function to clean text\ndef clean_text(input_text):\n    \"\"\"\n    Processes the give text and removes all non words, digits, single letters and extra spaces.\n\n    Parameters\n    -----------\n    1. input_text = Text to clean.\n    2. token = 'word' or 'sentence'\n\n    Returns: Text.\n\n    \"\"\"\n\n    text = re.sub(r'\\W',' ', input_text) #Remove all non words\n    text = re.sub(r'\\d+',' ', text) #Remove all digits\n    text = text.lower() #Converting text into lowercase\n    text = re.sub(r'\\s+[a-z]\\s+',' ', text) #Remove all single letters\n    text = re.sub(r'^\\s+','', text) #Remove space from start of text\n    text = re.sub(r'\\s+$','', text) #Remove space from end of text\n    text = re.sub(r'\\s+',' ', text) #Remove all multi space    \n    text = text.split(' ') #Split the words into tokens\n    text = [word for word in text if word not in stop_words] #Remove stopwords\n    text = [WordNetLemmatizer().lemmatize(word) for word in text] #Lemmatize the words(get root form)\n    text = ' '.join(text)\n\n    return text","5e237140":"#Actual Text\ntemp = df_train.loc[0,'review']\ntemp","29f6c0c8":"#Cleaned text\nclean_text(temp)","b0cb1c04":"#Apply clean text over movie reviews\ndf_train['processed_reviews'] = df_train['review'].apply(lambda x: clean_text(x))","64a5843c":"#Review dataset\ndf_train.head()","a127fe12":"#Finding average total length of words in review\ndf_train['processed_reviews'].apply(lambda x: len(x.split(' '))).mean()","1a7dcd21":"max_features = 6000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(df_train['processed_reviews'])\nlist_tokens = tokenizer.texts_to_sequences(df_train['processed_reviews'])","a254072e":"maxlen = 130 #Selected from mean of text length\nX_train = pad_sequences(list_tokens, maxlen=maxlen)\ny_train = df_train['sentiment']","b662925a":"embed_size = 128\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_size))\nmodel.add(Bidirectional(LSTM(32, return_sequences = True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(20, activation=\"relu\"))\nmodel.add(Dropout(0.05))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","183ab64b":"batch_size = 100\nepochs = 3\nmodel.fit(X_train,y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)","28e19c52":"df_test = pd.read_csv('\/kaggle\/input\/word2vec-nlp-tutorial\/testData.tsv', delimiter='\\t')\ndf_test.head()","648ddeb5":"df_test['processed_review'] = df_test['review'].apply(lambda x: clean_text(x))","2391a8bc":"list_sentences_test = df_test['processed_review']\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_test = pad_sequences(list_tokenized_test, maxlen=maxlen)","07692061":"prediction = model.predict(X_test)","89a52609":"#Set the prediction value\np=[]\nfor val in prediction:\n    if val > 0.5:\n        p.append(1)\n    else:\n        p.append(0)\n        \nsr_pred = pd.Series(data=p, name='sentiment')\nsr_pred[:5]","b6e0f834":"submission = pd.DataFrame(columns=['id', 'sentiment'])\nsubmission['id'] = df_test['id']\nsubmission['sentiment'] = sr_pred\nsubmission.head()","6df634cb":"submission.to_csv('first_submission.csv', index=False)","26ce4643":"## Read the Test Data","46c8467a":"**Credits:** This work is insipired this notebook from Nilan - https:\/\/www.kaggle.com\/nilanml\/imdb-review-deep-model-94-89-accuracy","3a8aed50":"## Preprocessing the data","484536bd":"### Validate the clean text function","e36a250e":"### Function to clean text.","ca9192fb":"### Make Prediction","34d69703":"### Preparing Submission File","9a1757d7":"## Building Model","7236e258":"### Clean Test data","44cd1b4e":"### Preprocess Test Data","0e1eecd2":"## <center> Predict Sentiment Analysis over Movie Reviews","0688dd12":"### Secured score - 0.85456"}}