{"cell_type":{"118c5d3b":"code","5301dd35":"code","0f64b3bd":"code","f233562e":"code","924f35f1":"code","eaeeb43c":"code","aa987a18":"code","79016681":"code","3947e8d1":"code","b4d9499c":"code","54e1921a":"code","0c64ea4c":"code","97737dd6":"code","9b77736c":"code","fc1f0990":"code","1ee7b3b3":"code","3b374ccb":"code","1b4b31f1":"code","70b82501":"code","c1496053":"code","279de615":"code","951d6c7b":"code","5581d88c":"code","03c39c37":"code","b31d190b":"code","0d3327d9":"code","8283257e":"code","a67eeec9":"code","ce1c39bc":"code","5e57b19c":"code","7f5fd0b2":"code","664f10ac":"code","c8be6d77":"code","34a593a9":"code","ffb7e651":"code","96f2d56b":"code","802a3cdc":"code","1aa831b5":"code","63a0f657":"code","763968c2":"code","96b47a9d":"code","0bb7da2b":"code","70e255d6":"code","10383073":"code","8d6707f3":"code","bb16692e":"code","d187b8f5":"code","8f485d24":"code","6fea2a63":"code","506ca302":"code","00a79b9f":"code","3cff8b59":"code","4233eaca":"code","c7b2f9ce":"code","a5dd4339":"code","aba2d388":"code","d3f27e72":"code","151b2006":"code","a53b49d8":"code","ccf2e80b":"code","ad3c85da":"code","ee9187fe":"code","c747bbd3":"code","b299e138":"code","08c4dd6a":"code","360a81fe":"code","23980311":"code","e4e441ad":"code","580d7f65":"code","84c5cfcd":"code","9e876f7e":"code","d83eab6d":"code","7124a3b6":"code","aea55ffe":"code","12a57f00":"code","c7076f67":"code","00f2bc76":"markdown","19f0f0cf":"markdown"},"source":{"118c5d3b":"!pip install git+https:\/\/github.com\/dreamquark-ai\/tabnet","5301dd35":"import numpy as np\nimport pandas as pd\nfrom category_encoders import CountEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom tqdm.notebook import tqdm\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb","0f64b3bd":"import os, random\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.tab_model import TabNetClassifier\n\nimport lightgbm as lgb","f233562e":"# \u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f\ndf_train = pd.read_csv('\/kaggle\/input\/data-science-summer2-osaka\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/data-science-summer2-osaka\/test.csv')\ndf_user = pd.read_csv(\"..\/input\/data-science-summer2-osaka\/user_x_anime.csv\")\ndf_wiki = pd.read_csv(\"..\/input\/data-science-summer2-osaka\/wiki.csv\")\ndf_wiki_s = pd.read_csv(\"..\/input\/ideas\/wiki_.csv\")\ndf_user_mean = pd.read_csv(\"..\/input\/user-mean-rating\/df_user_describe.csv\")","924f35f1":"# \u8aac\u660e\u5909\u6570\u3068\u30bf\u30fc\u30b2\u30c3\u30c8\u5909\u6570\u306b\u5206\u5272\u3057\u3066\u304a\u304d\u307e\u3059\ny_train = df_train.Score\nX_train = df_train.drop(['Score'], axis=1)\nX_test = df_test.copy()","eaeeb43c":"#\u307e\u305a\u30b8\u30e3\u30f3\u30eb\u304b\u3089\u898b\u3066\u3044\u304d\u307e\u3059\uff0e\ntemp_genres = []\nfor Genres in X_train.Genres:\n    temp_genres_ = Genres.split(\",\")\n    for temp_genre_ in temp_genres_:\n        temp_genres.append(temp_genre_.strip())#\u7a7a\u767d\u306b\u3084\u3089\u308c\u307e\u3057\u305f\uff0e\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3057\u305f\uff0e\n#\u30ab\u30f3\u30de\u3067\u533a\u5207\u3089\u308c\u3066\u3044\u308b\u6a21\u69d8","aa987a18":"#\u5168\u3066\u306e\u30b8\u30e3\u30f3\u30eb\u304c\u63c3\u3063\u305f.\u3000\ncol_genres = np.unique(np.array(temp_genres))\ncol_genres","79016681":"def one_hot_genres(data):\n    for col in data.Genres.split(','):\n        col = col.strip()\n        data.loc[col] = 1\n    return data","3947e8d1":"#\u30b8\u30e3\u30f3\u30eb\u5217\u3092\u4f5c\u6210\u3057\u3066\uff0c\u8a72\u5f53\u30b8\u30e3\u30f3\u30eb\u306b1\u3092\u3064\u3051\u308b\nX_train[col_genres] = 0\nX_train = X_train.apply(one_hot_genres,axis = 1)\nX_test[col_genres] = 0\nX_test = X_test.apply(one_hot_genres,axis = 1)\nX_test.columns","b4d9499c":"for col in col_genres:\n    len_ = len(y_train[X_train[col] == 1])\n    X_train.loc[X_train[col] == 1,col] = len_\n    X_test.loc[X_test[col] == 1,col] = len_\n#1\u3092member_mean\u306b\u5909\u63db","54e1921a":"#genres\u306e\u5404\u884c\u306e\u5e73\u5747\u3092\u53d6\u5f97\nX_train[\"genres_mean\"] = X_train[col_genres].replace(0.0,np.nan).mean(axis = 1)\nX_test[\"genres_mean\"] = X_test[col_genres].replace(0.0,np.nan).mean(axis = 1)","0c64ea4c":"# Genres\u306e\u5206\u5272\u3067\u3059\u304c\u3001TFIDF\u3092\u7528\u3044\u3066\u30ab\u30a6\u30f3\u30c8\u3057\u3088\u3046\u3068\u601d\u3044\u307e\u3059\u3002\ntfidf = TfidfVectorizer(max_features=1000, use_idf=False, )\nTXT_train_enc = tfidf.fit_transform(X_train.Genres)\nTXT_test_enc = tfidf.transform(X_test.Genres)\nX_train.drop(['Genres'], axis=1, inplace=True)\nX_test.drop(['Genres'], axis=1, inplace=True)","97737dd6":"def aired_datetime_(air):\n    d1 = air.split()\n    try:\n        d1 = [s for s in d1 if \"19\" in s]\n        d = d1[0]\n    except:\n        d = -9999\n    d2 = air.split()\n    try:\n        d2 = [s for s in d2 if \"20\" in s]\n        d = d2[0]\n    except:\n        pass\n    try:\n        d = d.split(\",\")[0]\n    except:\n        pass\n    return int(d)","9b77736c":"def _aired_datetime(air):\n    d1 = air.split()\n    try:\n        d1 = [s for s in d1 if \"19\" in s]\n        d = d1[0]\n    except:\n        d = -9999\n    d2 = air.split()\n    try:\n        d2 = [s for s in d2 if \"20\" in s]\n        d = d2[0]\n    except:\n        pass\n    try:\n        d = d.split(\",\")[0]\n    except:\n        pass\n    return int(d)","fc1f0990":"def aired_datetime(air):\n    try:\n        d = pd.to_datetime('2021-08-01') - pd.to_datetime(air.split(' to ')[0])\n        d \/= pd.Timedelta('1d')\n    except:\n        d = -9999\n    return d","1ee7b3b3":"def aired_datetime_(air):\n    try:\n        d = pd.to_datetime(air.split(' to ')[1])\n        d = int(d.year)      \n    except:\n        d = -9999\n    return d","3b374ccb":"def span(air):\n    try:\n        d = pd.to_datetime(air.split(' to ')[1]) - pd.to_datetime(air.split(' to ')[0])\n        d = d.days\n    except:\n        d = -9999\n    return (d)","1b4b31f1":"# \u5909\u63db\u3057\u307e\u3059\nX_train['Aired_start'] = X_train['Aired'].apply(aired_datetime_)\nX_test['Aired_start'] = X_test['Aired'].apply(aired_datetime_)\nX_train['Aired_end'] = X_train['Aired'].apply(_aired_datetime)\nX_test['Aired_end'] = X_test['Aired'].apply(_aired_datetime)\nX_train['Aired_span'] = X_train['Aired'].apply(span)\nX_test['Aired_span'] = X_test['Aired'].apply(span)\nX_train['Aired'] = X_train['Aired'].apply(aired_datetime)\nX_test['Aired'] = X_test['Aired'].apply(aired_datetime)","70b82501":"#np.nan\u306b\u5909\u63db\u3057\u307e\u3059\uff0e\nX_train.loc[X_train.Episodes == \"Unknown\",\"Episodes\"] = np.nan\nX_train[\"Episodes\"] = X_train.Episodes.astype(float)\nX_test.loc[X_test.Episodes == \"Unknown\",\"Episodes\"] = np.nan\nX_test[\"Episodes\"] = X_test.Episodes.astype(float)","c1496053":"# \u304b\u306a\u308a\u5f37\u5f15\u306a\u306e\u3067\u6539\u5584\u306e\u4f59\u5730\u3042\u308a\ndef Duration_to_float(data):\n    time_ = 0\n    try:\n        time_hr = data.split(\"hr\")[0]\n        data = data.split(\"hr\")[1].split(\".\")[1]\n        time_ = float(time_hr*60)\n    except:\n        pass\n    try:\n        time_min = data.split(\"min\")[0]\n        time_ += float(time_min)\n    except:\n        time_ = np.nan\n    return time_","279de615":"X_train[\"Duration\"] = X_train.Duration.map(Duration_to_float)\nX_test[\"Duration\"] = X_test.Duration.map(Duration_to_float)","951d6c7b":"df_train.groupby(\"Type\").Score.mean().sort_values()","5581d88c":"def type_to_enc(data):\n    if data == \"TV\":\n        return 6\n    elif data == \"Special\":\n        return 5\n    elif data == \"Movie\":\n        return 4\n    elif data == \"OVA\":\n        return 3\n    elif data == \"ONA\":\n        return 2\n    elif data == \"Music\":\n        return 1","03c39c37":"X_train[\"Type_\"] = X_train[\"Type\"].map(type_to_enc)\nX_test[\"Type_\"] = X_test[\"Type\"].map(type_to_enc)","b31d190b":"df_train.groupby(\"Rating\").Score.mean().sort_values()","0d3327d9":"def rating_to_enc(data):\n    if data in \"17\":\n        return 7\n    elif data in \"13\":\n        return 6\n    elif data == \"Children\":\n        return 5\n    elif data == \"Mild\":\n        return 4\n    elif data == \"Unknown\":\n        return 3\n    elif data == \"Hentai\":\n        return 2\n    elif data == \"All\":\n        return 1","8283257e":"X_train[\"Rating_\"] = X_train[\"Rating\"].map(rating_to_enc)\nX_test[\"Rating_\"] = X_test[\"Rating\"].map(rating_to_enc)","a67eeec9":"# Target Enconding\u3057\u305f\u5217\u306e\u76f8\u5bfe\u5024\u3092\u6c42\u3081\u308b\ntarget_col = [\"Rating_\",\"Type_\",\"genres_mean\"]\ntarget_col_ = [f\"{i}_mean\" for i in target_col]\nX_train[target_col_] = X_train[target_col] - X_train[target_col].mean()\nX_train[\"target_col_mean\"] = X_train[target_col_].mean(axis='columns')\nX_train[\"target_col_mean\"] = X_train[target_col_].std(axis='columns')\nX_test[target_col_] = X_test[target_col] - X_test[target_col].mean()\nX_test[\"target_col_mean\"] = X_test[target_col_].mean(axis='columns')\nX_test[\"target_col_mean\"] = X_test[target_col_].std(axis='columns')","ce1c39bc":"#np.nan\u306b\u5909\u63db\u3057\u307e\u3059\uff0e\nX_train.loc[X_train.Episodes == \"Unknown\",\"Episodes\"] = np.nan\nX_train[\"Episodes\"] = X_train.Episodes.astype(float)\nX_test.loc[X_test.Episodes == \"Unknown\",\"Episodes\"] = np.nan\nX_test[\"Episodes\"] = X_test.Episodes.astype(float)","5e57b19c":"X_train = pd.merge(X_train, df_user_mean, left_on='MAL_ID', right_on='anime_id',how='left').drop(columns='anime_id')\nX_test = pd.merge(X_test, df_user_mean, left_on='MAL_ID', right_on='anime_id',how='left').drop(columns='anime_id')","7f5fd0b2":"df_wiki_s[\"Series_\"] =  df_wiki_s[\"Series\"]","664f10ac":"X_train = pd.merge(X_train, df_wiki_s[[\"wiki_len\",\"MAL_ID\"]], left_on='MAL_ID', right_on='MAL_ID',how='left')\nX_test = pd.merge(X_test, df_wiki_s[[\"wiki_len\",\"MAL_ID\"]], left_on='MAL_ID', right_on='MAL_ID',how='left')","c8be6d77":"# \u30ab\u30a6\u30f3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u4fee\u6b63\ntrain_len = len(X_train)\ntest_len = len(X_test)\nX_all = pd.concat([X_train,X_test]).reset_index(drop = True)\nX_all.head(2)","34a593a9":"#\u4e2d\u897f\u3055\u3093\u306e\u30b3\u30fc\u30c9\u8ffd\u52a0\nJapanese_name_head = X_all[\"Japanese name\"].map(lambda x : x[:5])\nreplace_name_head = [s.replace('(', ' ').replace('[', ' ').replace('?', ' ') for s in Japanese_name_head.tolist()]\nfor x,i in enumerate(replace_name_head):\n    df_series = X_all[X_all[\"Japanese name\"].str.contains(i)]\n    series_total = len(df_series)\n    if series_total >= 2:\n        avg = X_all.wiki_len.mean()\n        X_all.loc[x,\"Seriesnum\"] = series_total\n        X_all.loc[x,\"Series_len\"] = avg\n        \n    else:\n        X_all.loc[x,\"Seriesnum\"] = -999\n        X_all.loc[x,\"Series_len\"] = -999\n        #\u3053\u3053\u306f\u5225\u9014\u8aac\u660e\u3057\u307e\u3059\u306d\uff0e\uff0e\uff0e\u30ea\u30fc\u30af\u3059\u308b\u306e\u3067\uff0c\u5909\u3048\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u301c\uff01\nX_all","ffb7e651":"series_data = np.load(\"..\/input\/anime2vec\/projected_matrix.npy\")\nseries_data_ = np.load(\"..\/input\/anime2vec\/embedding_matrix.npy\")","96f2d56b":"X_all[\"series_1\"] = series_data[:,0:1]\nX_all[\"series_2\"] = series_data[:,1:2]\n\nX_all[\"series_3\"] = series_data_[:,0:1]\nX_all[\"series_4\"] = series_data_[:,1:2]\nX_all[\"series_5\"] = X_all[\"series_3\"]\/X_all[\"series_4\"]\nX_all[\"series_6\"] = X_all[\"series_4\"]\/ X_all[\"series_3\"]","802a3cdc":"X_all[\"Watching_raito\"] = X_all[\"Watching\"]\/X_all[\"Members\"]\nX_all[\"Completed_raito\"] = X_all[\"Completed\"]\/X_all[\"Members\"]\nX_all[\"On-Hold_raito\"] = X_all[\"On-Hold\"]\/X_all[\"Members\"]\nX_all[\"Dropped_raito\"] = X_all[\"Dropped\"]\/X_all[\"Members\"]\nX_all[\"Plan to Watch_raito\"] = X_all[\"Plan to Watch\"]\/X_all[\"Members\"]","1aa831b5":"X_all.drop(['Name'], axis=1, inplace=True)","63a0f657":"df_wiki_bert = pd.read_csv(\"..\/input\/fork-of-bert-text\/df_wiki_description_bert.csv\")\ndf_wiki_bert = df_wiki_bert.dropna()#nan\u3068\u91cd\u8907\u306e\u5834\u5408\u3082\u3042\u308b\u306e\u3067\uff0cnan\u304c\u6b8b\u3089\u306a\u3044\u3088\u3046\u306b\u524a\u9664\ndf_wiki_bert = df_wiki_bert.drop_duplicates(subset='Japanese name')\n# df_wiki_bert[\"Japanese name\"] = df_wiki_bert[\"Prefecture\"]","763968c2":"X_all_wiki = pd.merge(X_all,df_wiki_bert,on='Japanese name',how=\"left\")","96b47a9d":"# X_all_wiki.drop(['Prefecture'], axis=1, inplace=True)\n\nX_all = X_all.reset_index(drop = True)\nX_all_wiki = X_all_wiki.reset_index(drop = True)","0bb7da2b":"# \u305d\u306e\u4ed6\u306b\u3082\u8272\u3005\u3084\u308c\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u3053\u3053\u3067\u306f\u3082\u3046\u4e00\u62ec\u3057\u3066\u30a8\u30a4\u30e4\u3067Count Encoding\u3057\u3066\u3057\u307e\u3044\u307e\u3059\ncats = []\nfor col in X_all.columns:\n    if X_all[col].dtype == 'object':\n        cats.append(col)\nprint(cats)\nce = CountEncoder(cols=cats)\nX_all = ce.fit_transform(X_all)\nX_all_wiki = ce.fit_transform(X_all_wiki)","70e255d6":"X_train = X_all_wiki[:train_len]\nX_test = X_all_wiki[train_len:]","10383073":"# col_ = importances[-100:].index\ncol_ = [           'watching_status_x_min',                                 11,\n                       'rating_mean_mean',                       'Aired_span',\n                                'Members',          'watching_status_x_count',\n                             'MAL_ID_50%',                      'MAL_ID_mean',\n                                 'Source',                'rating_count_mean',\n                     'rating_median_mean',                                 18,\n                             'MAL_ID_25%',                                  0,\n                               'Watching',           'watched_episodes_x_std',\n                 'watched_episodes_x_50%',                 'rating_count_75%',\n                       'rating_count_50%',                          'On-Hold',\n                        'rating_mean_min',                  'rating_mean_25%',\n                        'rating_min_mean',            'watching_status_y_max',\n                              'Seriesnum',           'watched_episodes_y_50%',\n                'watched_episodes_y_mean',                 'rating_count_25%',\n                        'rating_mean_75%',                        'Producers',\n                                 'MAL_ID',                    'Plan to Watch',\n                 'watching_status_y_mean',                       'MAL_ID_75%',\n                        'rating_std_mean',            'watching_status_y_75%',\n                 'watching_status_x_mean',                'watching_rate_max',\n                        'rating_mean_std',                'watching_rate_25%',\n                              'Completed',                  'rating_mean_50%',\n                         'rating_std_50%',                 'rating_count_max',\n                      'rating_median_std',                 'rating_count_std',\n                                'row_std',                            'Aired',\n                  'watching_status_y_50%',              'Plan to Watch_raito',\n                                'Studios',                         'series_2',\n                               'row_mean',           'watched_episodes_y_25%',\n                  'watching_status_x_std',                'watching_rate_50%',\n                 'watched_episodes_y_75%',           'watched_episodes_y_max',\n                     'watching_rate_mean',            'watching_status_y_std',\n                'watched_episodes_x_mean',                   'rating_std_max',\n                             'MAL_ID_std',                   'rating_std_25%',\n                        'Completed_raito',                   'rating_std_75%',\n                            'genres_mean',                'watching_rate_75%',\n                  'watching_status_y_25%',                        'Favorites',\n                          'On-Hold_raito',                         'Duration',\n                  'watching_status_y_min',                   'Watching_raito',\n                             'MAL_ID_max',           'watched_episodes_y_std',\n                      'watching_rate_std',                       'MAL_ID_min',\n                         'rating_std_std',                         'series_4',\n                               'series_1', 'Japanese_name_feature_bert_svd_4',\n                        'target_col_mean',                   'rating_max_std',\n       'Japanese_name_feature_bert_svd_3',                  'rating_max_mean',\n                         'rating_min_std', 'Japanese_name_feature_bert_svd_0',\n       'Japanese_name_feature_bert_svd_5',                          'Dropped',\n       'Japanese_name_feature_bert_svd_6', 'Japanese_name_feature_bert_svd_7',\n       'Japanese_name_feature_bert_svd_9', 'Japanese_name_feature_bert_svd_8',\n                               'series_3',                         'series_5',\n       'Japanese_name_feature_bert_svd_1', 'Japanese_name_feature_bert_svd_2',\n                          'Dropped_raito',                         'series_6']","8d6707f3":"temp_train_txt = pd.DataFrame(TXT_train_enc.toarray())\ntemp_train_txt[\"MAL_ID\"] = X_train[\"MAL_ID\"]\ntemp_test_txt = pd.DataFrame(TXT_test_enc.toarray())\ntemp_test_txt[\"MAL_ID\"] = X_test[\"MAL_ID\"]","bb16692e":"#\u884c\u3054\u3068\u306e\u5e73\u5747\u3068\u5206\u6563\nX_train[\"row_mean\"] = X_train.mean(axis='columns')\nX_test[\"row_mean\"] = X_test.mean(axis='columns')\nX_train[\"row_std\"] = X_train.std(axis='columns')\nX_test[\"row_std\"] = X_test.std(axis='columns')","d187b8f5":"X_train_txt = np.concatenate([X_train.values, TXT_train_enc.todense()], axis=1)\nX_test_txt = np.concatenate([X_test.values, TXT_test_enc.todense()], axis=1)","8f485d24":"X_train_txt_table = pd.merge(X_train, temp_train_txt,on='MAL_ID',how=\"left\")\nX_test_txt_table = pd.merge(X_test, temp_test_txt,on='MAL_ID',how=\"left\")\nX_train_txt_table = X_train_txt_table[col_]\nX_test_txt_table = X_test_txt_table[col_]\n\n","6fea2a63":"# \u30bf\u30fc\u30b2\u30c3\u30c8\u3082numpy\u306earray\u306b\u3057\u3066\u304a\u304d\u307e\u3059\ny_train = y_train.values","506ca302":"X_train.to_csv(\"X_train.csv\",index = False)\nX_test.to_csv(\"X_test.csv\",index = False)","00a79b9f":"X_train_txt","3cff8b59":"# 5\u5206\u5272\u4ea4\u5dee\u691c\u5b9a\u3067\nscores = []\n\ncv = KFold(n_splits=3, random_state=71, shuffle=True)\n\nfor i, (train_ix, val_ix) in tqdm(enumerate(cv.split(X_train_txt, y_train))):\n    \n    X_train_, y_train_ = X_train_txt[train_ix], y_train[train_ix]\n    X_val, y_val = X_train_txt[val_ix], y_train[val_ix]\n \n    model = XGBRegressor(n_estimators=10000, learning_rate=0.02, random_state=71,max_depth=10)\n    model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], eval_metric='rmse', early_stopping_rounds=50,verbose = 0)\n    y_pred = model.predict(X_val)\n    score = mean_squared_error(y_val, y_pred)**0.5\n    scores.append(score)\n    \n    print('CV Score of Fold_%d is %f' % (i, score))","4233eaca":"# # \u5e73\u5747\u30b9\u30b3\u30a2\u3092\u7b97\u51fa\nnp.array(scores).mean()","c7b2f9ce":"# \u3067\u306f\u3001\u5168\u30c7\u30fc\u30bf\u3067\u518d\u5b66\u7fd2\u3057\u3066\u307f\u307e\u3057\u3087\u3046\n# \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6570\u306f\u5148\u307b\u3069\u306e\u6700\u9069\u5024\u3092\u7528\u3044\u3066\u307f\u307e\u3059\nbest_iter = model.best_iteration\ny_pred_tests = []\nmodel = XGBRegressor(learning_rate=0.02, n_estimators=best_iter, random_state=2021)\nmodel.fit(X_train_txt, y_train)\n# \u3044\u3088\u3044\u3088\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\u3057\u307e\u3059\ny_pred_test = model.predict(X_test_txt)\ny_pred_tests.append(y_pred_test)","a5dd4339":"y_pred_test = np.mean(y_pred_tests , axis=0)","aba2d388":"# \u5404fold\u306e\u30b9\u30b3\u30a2\u3092\u4fdd\u5b58\u3059\u308b\u30ea\u30b9\u30c8\nscores_accuracy = []\n\n# \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u884c\u3046\n# \u5b66\u7fd2\u30c7\u30fc\u30bf\u30924\u3064\u306b\u5206\u5272\u3057\u3001\u3046\u30611\u3064\u3092\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3068\u3059\u308b\u3053\u3068\u3092\u3001\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3092\u5909\u3048\u3066\u7e70\u308a\u8fd4\u3059\nX_train = X_train.drop(columns='wiki_len')\nX_test = X_test.drop(columns='wiki_len')\nkf = KFold(n_splits=3, shuffle=True, random_state=71)\nfor tr_idx, va_idx in kf.split(X_train):\n    \n    # \u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306b\u5206\u3051\u308b\n    X_train_, X_val = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_train_, y_val = y_train[tr_idx], y_train[va_idx]\n    \n    # \u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u3092\u884c\u3046\n    model_lgb = lgb.LGBMRegressor(learning_rate=0.03, n_estimators=1000)\n    model_lgb.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], eval_metric='rmse',verbose=0)\n    # \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306e\u4e88\u6e2c\u5024\u3092\u78ba\u7387\u3067\u51fa\u529b\u3059\u308b\n    y_pred = model_lgb.predict(X_val)\n    score = mean_squared_error(y_val, y_pred)**0.5\n    scores_accuracy.append(score)\n    \n# \u3067\u306f\u3001\u5168\u30c7\u30fc\u30bf\u3067\u518d\u5b66\u7fd2\u3057\u3066\u307f\u307e\u3057\u3087\u3046\n# \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6570\u306f\u5148\u307b\u3069\u306e\u6700\u9069\u5024\u3092\u7528\u3044\u3066\u307f\u307e\u3059\nbest_iter = model_lgb.best_iteration_\nmodel_lgb = lgb.LGBMRegressor(learning_rate=0.02, n_estimators=10000, random_state=71)\nmodel_lgb.fit(X_train, y_train, eval_metric='rmse',verbose=False)\n\n# \u3044\u3088\u3044\u3088\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\u3057\u307e\u3059\ny_pred_test_lgb = model_lgb.predict(X_test)\ny_pred_test_lgb","d3f27e72":"# \u5e73\u5747\u30b9\u30b3\u30a2\u3092\u7b97\u51fa\nnp.array(scores_accuracy).mean()#0.3486512915902466","151b2006":"y_pred_test_lgb_wiki_ = []\n# \u5404fold\u306e\u30b9\u30b3\u30a2\u3092\u4fdd\u5b58\u3059\u308b\u30ea\u30b9\u30c8\nscores_accuracy = []\n# \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3092\u884c\u3046\n# \u5b66\u7fd2\u30c7\u30fc\u30bf\u30924\u3064\u306b\u5206\u5272\u3057\u3001\u3046\u30611\u3064\u3092\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3068\u3059\u308b\u3053\u3068\u3092\u3001\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3092\u5909\u3048\u3066\u7e70\u308a\u8fd4\u3059\nkf = KFold(n_splits=3, shuffle=True, random_state=71)\nX_train_txt_table = X_train_txt_table.drop(columns=['wiki_len'])\nX_test_txt_table = X_test_txt_table.drop(columns=['wiki_len'])\nfor tr_idx, va_idx in kf.split(X_train_txt_table):\n    # \u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306b\u5206\u3051\u308b\n    X_train_, X_val = X_train_txt_table.iloc[tr_idx], X_train_txt_table.iloc[va_idx]\n    y_train_, y_val = y_train[tr_idx], y_train[va_idx]\n    \n    # \u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u3092\u884c\u3046\n    model_lgb_wiki = lgb.LGBMRegressor(learning_rate=0.03, n_estimators=1000)\n    model_lgb_wiki.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], eval_metric='rmse',verbose=0)\n    # \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306e\u4e88\u6e2c\u5024\u3092\u78ba\u7387\u3067\u51fa\u529b\u3059\u308b\n    y_pred = model_lgb_wiki.predict(X_val)\n    score = mean_squared_error(y_val, y_pred)**0.5\n    scores_accuracy.append(score)\n    y_pred_test_lgb_wiki_.append(model_lgb_wiki.predict(X_test_txt_table))\n    print(score)","a53b49d8":"# \u5e73\u5747\u30b9\u30b3\u30a2\u3092\u7b97\u51fa\nnp.array(scores_accuracy).mean()#0.33771027600564124","ccf2e80b":"np.array(y_pred_test_lgb_wiki_).mean(axis = 0)","ad3c85da":"y_pred_test_lgb_wiki = np.array(y_pred_test_lgb_wiki_).mean(axis = 0)#0.33771027600564124\ny_pred_test_lgb_wiki","ee9187fe":"# \u3067\u306f\u3001\u5168\u30c7\u30fc\u30bf\u3067\u518d\u5b66\u7fd2\u3057\u3066\u307f\u307e\u3057\u3087\u3046\n# \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6570\u306f\u5148\u307b\u3069\u306e\u6700\u9069\u5024\u3092\u7528\u3044\u3066\u307f\u307e\u3059\nmodel_lgb_wiki = lgb.LGBMRegressor(learning_rate=0.02, n_estimators=20000, random_state=71)\nmodel_lgb_wiki.fit(X_train_txt_table, y_train, eval_metric='rmse',verbose=False)\n# \u3044\u3088\u3044\u3088\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\u3057\u307e\u3059\ny_pred_test_lgb_wiki = model_lgb_wiki.predict(X_test_txt_table)\ny_pred_test_lgb_wiki","c747bbd3":"col = df_train.columns","b299e138":"Score = pd.read_csv('..\/input\/data-science-summer2-osaka\/train.csv')[[\"Score\"]]\nScore","08c4dd6a":"train = X_train.fillna(-1)\ntest = X_test.fillna(-1)","360a81fe":"N_FOLDS = 5\nSEED = 33","23980311":"# comment out when Unsupervised\n\ntabnet_params = dict(n_d=8, n_a=8, n_steps=3, gamma=1.3,\n                     n_independent=2, n_shared=2,\n                     seed=SEED, lambda_sparse=1e-3, \n                     optimizer_fn=torch.optim.Adam, \n                     optimizer_params=dict(lr=2e-2),\n                     mask_type=\"entmax\",\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=5,\n                                           min_lr=1e-5,\n                                           factor=0.9,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=10\n                    )\n\npretrainer = TabNetPretrainer(**tabnet_params)\n\npretrainer.fit(\n    X_train=train.values,\n    eval_set=[train.values],\n    max_epochs=200,\n    patience=20, batch_size=256, virtual_batch_size=128,\n    num_workers=1, drop_last=True)","e4e441ad":"train = train.to_numpy()\nScore = Score.to_numpy()\ntest = test.to_numpy()","580d7f65":"model = TabNetRegressor(**tabnet_params)\n# model = TabNetClassifier() \n# model.fit(\n#   X_train, y_train,\n#   eval_set=[(X_valid, y_valid)],\n#   max_epochs=5000,\n#   patience=100,\n#   eval_metric={'rmse'}, # 'rmsle', 'mse', 'mae'\n#   from_unsupervised=pretrainer\n# )","84c5cfcd":"model.fit(\n  train, Score,\n  max_epochs=110,\n  patience=100,\n  eval_metric={'rmse'}, # 'rmsle', 'mse', 'mae'\n  from_unsupervised=pretrainer\n)","9e876f7e":"preds_test = model.predict(test)","d83eab6d":"preds_test[:,0]","7124a3b6":"# \u63d0\u51fa\u7528\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u8aad\u307f\u8fbc\u3093\u3067\u4e88\u6e2c\u5024\u3092\u4ee3\u5165\u3057\u307e\u3059\u3002\nsubmission = pd.read_csv('\/kaggle\/input\/data-science-summer2-osaka\/sample_submission.csv', index_col=0)\nsubmission.Score = (y_pred_test * 0.3 + y_pred_test_lgb * 0.2 + y_pred_test_lgb_wiki * 0.5)*0.7 + preds_test[:,0]*0.3","aea55ffe":"# csv\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u4fdd\u5b58\u3057\u307e\u3059\nsubmission.to_csv('submission.csv')","12a57f00":"submission","c7076f67":"# feature importance \u306e\u30d7\u30ed\u30c3\u30c8\nimport matplotlib.pyplot as plt\nimportances = pd.Series(model_lgb_wiki.feature_importances_,index = X_train_txt_table.columns)\nimportances = importances.sort_values()\nimportances.plot(kind = \"barh\",figsize = (10,40))\nplt.title(\"imporance in the lightgbm Model\")\nplt.show()","00f2bc76":"### \u30b8\u30e3\u30f3\u30eb\u306e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0","19f0f0cf":"### Private 5rd Solution (Anime competition) \n\u8aad\u307f\u3084\u3059\u3044\u3088\u3046\u306b\u6574\u7406\u3057\u307e\u3057\u305f"}}