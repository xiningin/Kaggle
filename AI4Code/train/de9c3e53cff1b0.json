{"cell_type":{"f1f86e15":"code","9c1851c0":"code","ca052fc0":"code","55b8245d":"code","356b733b":"code","00d939ec":"code","e72bf986":"code","d69977a5":"code","ec93d709":"code","646ee00a":"code","c7609ef2":"code","37f90934":"code","9353daab":"code","ba5a7e98":"code","cbf776f6":"code","7a3f4ab5":"code","dad57bcd":"code","9f452e76":"code","7da53037":"code","202b0ef2":"code","227498e7":"code","679f19f3":"code","9a223888":"code","95ada899":"code","d5ec30b9":"code","c75fa615":"code","89fd1f0a":"code","a7b0fabe":"code","003e7eff":"code","7941bbbe":"code","add2afde":"code","8d219466":"code","1ed4fd4f":"markdown","b247f362":"markdown","4a82df31":"markdown","2a710bc6":"markdown","ebc7d491":"markdown","e1b2d6ae":"markdown","3f8bdec7":"markdown","662199c4":"markdown","9eb6e68b":"markdown","e93e4556":"markdown","6665be71":"markdown","57ab53c9":"markdown","4089d3af":"markdown","25e862e8":"markdown","400fdd17":"markdown","d1a8d339":"markdown","73f5c4ee":"markdown","fd1841c0":"markdown","367304c6":"markdown","04c3b5a8":"markdown","25fc53ea":"markdown","e72b91ec":"markdown","5bcb2044":"markdown"},"source":{"f1f86e15":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9c1851c0":"data=pd.read_csv(\"..\/input\/concrete-compressive-strength-data-set\/concrete_data.csv\")","ca052fc0":"data.head()","55b8245d":"data.shape","356b733b":"data.columns","00d939ec":"data=data.rename({\"fine_aggregate \":\"fine_aggregate\"},axis=1)","e72bf986":"data[\"fly_ash\"].unique()","d69977a5":"data.dtypes","ec93d709":"data.isnull().sum()","646ee00a":"def outlier_graph(data,column):\n    plt.figure(figsize=(5,3))\n    sns.boxplot(data[column])\n    plt.title(\"{} distribution\".format(column))","c7609ef2":"for i in data.columns:\n    outlier_graph(data,i)\n","37f90934":"def min_max_show(data,column):\n    print(\"min value of {} is {} \\nmax value of {} is {}\".format(column,data[column].min(),column,data[column].max()))","9353daab":"for i in data.columns:\n    min_max_show(data,i)","ba5a7e98":"data=data[data[\"blast_furnace_slag\"]<350]\ndata=data[(data[\"water\"]<246) & (data[\"water\"]>122)]\ndata=data[data[\"superplasticizer\"]<25]\ndata=data[data[\"fine_aggregate\"]<992]\ndata=data[data[\"age\"]<150]","cbf776f6":"plt.figure(figsize=(10,10))\nsns.heatmap(data.corr(),annot=True)","7a3f4ab5":"data.drop([\"fly_ash\"],axis=1,inplace=True)","dad57bcd":"for i in data.columns:\n    for j in data.columns:\n        plt.figure(figsize=(9,7))\n        sns.scatterplot(x=i,y=j,hue=\"concrete_compressive_strength\",data=data)\n        plt.show()\n        \n","9f452e76":"data.columns","7da53037":"data.drop([\"blast_furnace_slag\"],axis=1,inplace=True)\ndata.drop([\"coarse_aggregate\"],axis=1,inplace=True)\ndata.drop([\"fine_aggregate\"],axis=1,inplace=True)","202b0ef2":"plt.figure(figsize=(10,10))\nsns.heatmap(data.corr(),annot=True)","227498e7":"x=data.drop([\"concrete_compressive_strength\"],axis=1)\ny=data[\"concrete_compressive_strength\"]","679f19f3":"from sklearn.model_selection import train_test_split,StratifiedKFold\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor","9a223888":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)","95ada899":"Model_Names=[\"Linear Regression\",\"Polynomial2\",\"Polynomial3\",\"Polynomial4\",\"Random Forest\",\"Decision Tree\"]\nScores=[]","d5ec30b9":"lr=LinearRegression()\nlr.fit(x_train,y_train)\ny_head=lr.predict(x_test)\nScores.append(r2_score(y_test,y_head))","c75fa615":"lr2=LinearRegression()\nfor i in np.arange(2,5): \n    pl=PolynomialFeatures(degree=i)\n    x_polly=pl.fit_transform(x_train)\n    x_polly_test=pl.fit_transform(x_test)\n    lr2.fit(x_polly,y_train)\n    y_head=lr2.predict(x_polly_test)\n    Scores.append(r2_score(y_test,y_head))","89fd1f0a":"rf=RandomForestRegressor(n_estimators=100,random_state=42)\nrf.fit(x_train,y_train)\ny_head=rf.predict(x_test)\nScores.append(r2_score(y_test,y_head))","a7b0fabe":"dt=DecisionTreeRegressor()\ndt.fit(x_train,y_train)\ny_head=dt.predict(x_test)\nScores.append(r2_score(y_test,y_head))","003e7eff":"graph_data= pd.DataFrame(list(zip(Model_Names,Scores)),columns =['Models', 'Scores']) \nplt.figure(figsize=(10,6))\nsns.barplot(x=graph_data[\"Models\"],y=graph_data[\"Scores\"])","7941bbbe":"rf2=RandomForestRegressor(n_estimators=100,random_state=42)\nrf2.fit(x_train,y_train)\ny_head=rf2.predict(x_test)\nprint(r2_score(y_test,y_head))","add2afde":"sns.scatterplot(x=y_test,y=y_head)\nplt.xlabel(\"Real Data\")\nplt.ylabel(\"Predicted Data\")","8d219466":"sns.distplot(y_head,label=\"Predicted\")\nsns.distplot(y_test,label=\"Real\")\nplt.legend()","1ed4fd4f":"Since it does not have correlation with strength I dropped it.","b247f362":"When I examine the graphs I saw between fine_aggregate,coarse_aggregate and blast_furnace_slag variables and concrete_compressive_strength correlation is very low as we can see from scatter graphs so it will be better if we drop them.","4a82df31":"# **Data Visualization**","2a710bc6":"To see outliers I made a function.","ebc7d491":"It will show us the min and max values and it will make easy to erase them.","e1b2d6ae":"I didn't put concrete_compressive_strength since its outliers are very close to normal range. I just erased the far ones.","3f8bdec7":"Its score is 81.74% and I think it is not bad.","662199c4":"One column name is \"fine_aggregate \", we have an extra space. We need to get rid of it.","9eb6e68b":"I will try 6 models and take the best one. I will just do simple thing don't go for parameters or kfold.","e93e4556":"I wanted to be sure if fly_ash is numerical variable. And as we can see all the data contains numerical variables.","6665be71":"Then I will split my data as test and train.","57ab53c9":"fly_ash is extremely low correlated with the strength. But age,cement and water is correlated with the strength.","4089d3af":"# Modelling","25e862e8":"All of them are in correct type. We don't have to worry about it.","400fdd17":"# **Reading Data**","d1a8d339":"It is better.","73f5c4ee":"When we look this scatter plot we can see the linearity, it is not perfect but can still work.","fd1841c0":"First I will split my x and y","367304c6":"As we can see Random Forest gave the best score so we can use it.","04c3b5a8":"Still it is not the best model but I think it still works.","25fc53ea":"At this model I want to predict strength of concrete by looking all of the variables.","e72b91ec":"There are some outliers that we need to get rid of.","5bcb2044":"It is nice because there is no any null value."}}