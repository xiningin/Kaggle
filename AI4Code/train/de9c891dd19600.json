{"cell_type":{"9556e2ea":"code","879a0cb2":"code","8462fe43":"code","ac882964":"code","88b6d849":"code","9780147b":"code","bbc77b24":"code","64f959c1":"code","fbce4904":"code","44a09c6a":"code","0112d762":"code","e395601a":"code","84d638ef":"code","babf30c1":"code","6ead5336":"code","4fc9f439":"code","0d1f84d3":"code","66264ed9":"code","f473ba00":"code","768df04f":"code","d1353108":"code","9ed3ec9d":"code","382bffdd":"code","d44df940":"code","12538fd4":"code","4d1f4089":"code","52d5f29c":"code","d80cbe45":"code","ec02d62f":"code","87dc2d67":"code","d92dbe2e":"code","91e0f211":"code","bc436809":"code","ca4d53bd":"code","347d8655":"code","f7666cd6":"code","3951b6fc":"markdown","cbb92b29":"markdown","02a2b68b":"markdown","9d10e154":"markdown","877ad60e":"markdown","840c18f4":"markdown","e758ab73":"markdown","81ff2840":"markdown","f3da3e63":"markdown","6806fe05":"markdown","a04baf3f":"markdown","4f0ae29f":"markdown","d26717dd":"markdown","ff07ac8e":"markdown","66804341":"markdown","81d1959c":"markdown","27b871c0":"markdown","f063cd9e":"markdown","3a65e946":"markdown","7b85626f":"markdown","d1030c79":"markdown","6e1e7e30":"markdown","369b4bfc":"markdown","5af38d78":"markdown","3e3086c3":"markdown","03d1bf22":"markdown","86b85707":"markdown","bac22a9d":"markdown","45a641f5":"markdown","1c6fa2e3":"markdown","1f06ab53":"markdown"},"source":{"9556e2ea":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","879a0cb2":"df=pd.read_csv('..\/input\/perrin-freres-monthly-champagne-sales\/Perrin Freres monthly champagne sales millions.csv')","8462fe43":"df.head()","ac882964":"## Change the Column Names \ndf.columns=[\"Month\",\"Sales\"]\ndf.head()","88b6d849":"df.tail()","9780147b":"## Drop last 2 rows\ndf.drop(106,axis=0,inplace=True)","bbc77b24":"df.drop(105,axis=0,inplace=True)","64f959c1":"# Convert Month into Datetime\ndf['Month']=pd.to_datetime(df['Month'])","fbce4904":"df.head()","44a09c6a":"df.set_index('Month',inplace=True)","0112d762":"df.head()","e395601a":"df.describe()","84d638ef":"df.plot()","babf30c1":"from statsmodels.tsa.stattools import adfuller","6ead5336":"test_result=adfuller(df['Sales'])","4fc9f439":"#HYPOTHESIS TEST:\n#Ho: It is non stationary\n#H1: It is stationary\n\ndef adfuller_test(sales):\n    \n    result=adfuller(sales)\n    \n    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n    \n    for value,label in zip(result,labels):\n        print(label+' : '+str(value) )\n    \n    if result[1] <= 0.05:\n        print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary\")\n    else:\n        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")","0d1f84d3":"adfuller_test(df['Sales'])","66264ed9":"df['Seasonal First Difference']=df['Sales']-df['Sales'].shift(12)","f473ba00":"df.head(14)","768df04f":"## Again test dickey fuller test\nadfuller_test(df['Seasonal First Difference'].dropna())","d1353108":"df['Seasonal First Difference'].plot()","9ed3ec9d":"from statsmodels.graphics.tsaplots import plot_acf,plot_pacf","382bffdd":"\nfrom pandas.plotting import autocorrelation_plot\nautocorrelation_plot(df['Sales'])\nplt.show()","d44df940":"import statsmodels.api as sm\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(df['Seasonal First Difference'].iloc[13:],lags=40,ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(df['Seasonal First Difference'].iloc[13:],lags=40,ax=ax2)","12538fd4":"# For non-seasonal data\n#p=1, d=1, q=0 or 1\nfrom statsmodels.tsa.arima_model import ARIMA","4d1f4089":"model=ARIMA(df['Sales'],order=(1,1,1))\nmodel_fit=model.fit()","52d5f29c":"model_fit.summary()","d80cbe45":"df['forecast']=model_fit.predict(start=90,end=103,dynamic=True)\ndf[['Sales','forecast']].plot(figsize=(12,8))","ec02d62f":"import statsmodels.api as sm","87dc2d67":"model=sm.tsa.statespace.SARIMAX(df['Sales'],order=(1, 1, 1),seasonal_order=(1,1,1,12))\nresults=model.fit()","d92dbe2e":"df['forecast']=results.predict(start=90,end=103,dynamic=True)\ndf[['Sales','forecast']].plot(figsize=(12,8))","91e0f211":"from pandas.tseries.offsets import DateOffset\n\n#Here USING FOR LOOP we are adding some additional data for prediction purpose:\n\nfuture_dates=[df.index[-1]+ DateOffset(months=x)for x in range(0,24)]","bc436809":"#Convert that list into DATAFRAME:\n\nfuture_datest_df=pd.DataFrame(index=future_dates[1:],columns=df.columns)","ca4d53bd":"future_datest_df.tail()","347d8655":"#CONCATE THE ORIGINAL AND THE NEWLY CREATED DATASET FOR VISUALIZATION PURPOSE:\nfuture_df=pd.concat([df,future_datest_df])","f7666cd6":"#PREDICT\nfuture_df['forecast'] = results.predict(start = 104, end = 120, dynamic= True)  \nfuture_df[['Sales', 'forecast']].plot(figsize=(12, 8))","3951b6fc":"##### HERE THE BLUE LINE IS ACTUAL DATA & ORANGE LINE IS PREDICTED DATA. HOW GOOD IT GAVE US THE RESULTS.","cbb92b29":"-------------------","02a2b68b":"### AUTO-CORRELATION | PARTIAL AUTO-CORRELATION:","9d10e154":"### 4. PREDICT FOR FUTURE DATASET:","877ad60e":"--------------","840c18f4":"### AUTO-CORRELATION:\n\n* <b>Before we decide which model to use, we need to look at auto-correlations<\/b>\n\n* Autocorrelation is the most important concept in time series. It is precisely what makes modeling them so difficult.\n\n* Autocorrelation is the measure of the degree of similarity between a given time series and the lagged version of that time series over successive time periods. It is similar to calculating the correlation between two different variables except in Autocorrelation we calculate the correlation between two different versions Xt and Xt-k of the same time series\n\n* In time series, the current value depends on past values. If the temperature today is 80 F, tomorrow it is more likely for the temperature to be around 80 F rather than 40 F.\n\n* If you swap the first and tenth observations in tabular data, the data has not changed one bit. If you swap the first and tenth observations in a time series, you get a different time series. Order matters. Not accounting for autocorrelation is almost as silly as this timeless classic.\n\n### PARTIAL AUTO-CORRELATION:\n\n* Another useful method to examine serial dependencies is to examine the partial autocorrelation function (PACF) \u2013 an extension of autocorrelation, where the dependence on the intermediate elements (those within the lag) is removed.\n\n<b>Once we determine the nature of the auto-correlations we use the following rules of thumb.<\/b>\n\n* Rule 1: If the ACF shows exponential decay, the PACF has a spike at lag 1, and no correlation for other lags, then use one autoregressive (p)parameter\n\n* Rule 2: If the ACF shows a sine-wave shape pattern or a set of exponential decays, the PACF has spikes at lags 1 and 2, and no correlation for other lags, the use two autoregressive (p) parameters\n\n* Rule 3: If the ACF has a spike at lag 1, no correlation for other lags, and the PACF damps out exponentially, then use one moving average (q) parameter.\n\n* Rule 4: If the ACF has spikes at lags 1 and 2, no correlation for other lags, and the PACF has a sine-wave shape pattern or a set of exponential decays, then use two moving average (q) parameter.\n\n* Rule 5: If the ACF shows exponential decay starting at lag 1, and the PACF shows exponential decay starting at lag 1, then use one autoregressive (p) and one moving average (q) parameter.\n\n### Removing serial dependency. \n\nSerial dependency for a particular lag can be removed by differencing the series. There are two major reasons for such transformations.\n\n* First, we can identify the hidden nature of seasonal dependencies in the series. Autocorrelations for consecutive lags are interdependent, so removing some of the autocorrelations will change other auto correlations, making other seasonalities more apparent.\n\n* Second, removing serial dependencies will make the series stationary, which is necessary for ARIMA and other techniques.\n\n### DURBIN-WATSON TEST:\n\n* Another popular test for serial correlation is the Durbin-Watson statistic.\n* Durbin-Watson test is used to measure the amount of autocorrelation in residuals from the regression analysis. Durbin Watson test is used to check for the first-order autocorrelation.\n\n![](https:\/\/www.geeksforgeeks.org\/wp-content\/ql-cache\/quicklatex.com-36cfeeac8654724f1cefdbef9f7b134e_l3.svg)\n\n<b>Assumptions for the Durbin-Watson Test:<\/b>\n\n* The errors are normally distributed and the mean is 0.\n* The errors are stationary.\n\n* The null hypothesis and alternate hypothesis for the Durbin-Watson Test are\n\n        H0: No first-order autocorrelation.\n        H1: There is some first-order correlation.\n        \n<b>The Durbin Watson test has values between 0 and 4. Below is the table containing values and their interpretations:<\/b>\n\n* 2: No autocorrelation. Generally, we assume 1.5 to 2.5 as no correlation.\n* 0- <2: positive autocorrelation. The more close it to 0, the more signs of positive autocorrelation.\n* greater 2 -4: negative autocorrelation. The more close it to 4, the more signs of negative autocorrelation.","e758ab73":"------------------------------------------------------------","81ff2840":"------------","f3da3e63":"-----------------","6806fe05":"--------------","a04baf3f":"# <center>TIME SERIES MODELING - ARIMA, SARIMA<\/center>","4f0ae29f":"---------------------------","d26717dd":"## In this Kernel I have shared basics to implementaion part of TIME SERIES | ARIMA MODEL | SARIMAZ MODEL using the added dataset.","ff07ac8e":"#### Here we can see that last 2 columns have null values. So we'll remove those","66804341":"##### Here the value 12 is the number of index values per period of time you are calculating.","81d1959c":"---------------","27b871c0":"# <center>TIME SERIES | ARIMA | SARIMA MODELS USING PYTHON<\/center>","f063cd9e":"### 3. ARIMA MODEL\n\nLet\u2019s Break it Down:-\n\n* AR: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\n\n* I: Integrated. The use of differencing of raw observations in order to make the time series stationary.\n\n* MA: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n\nThe parameters of the ARIMA model are defined as follows:\n\n* p: The number of lag observations included in the model, also called the lag order.\n* d: The number of times that the raw observations are differenced, also called the degree of differencing.\n* q: The size of the moving average window, also called the order of moving average.\n\n","3a65e946":"### What is Time Series?\n\n* According to the wikipedia, A time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. For example, stock prices over a fixed period of time, hotel bookings, ecommerce sales, waether cycle reports etc.\n\n* Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values.\n\n### Examples of time series data:\n\n* Stock prices, Sales demand, website traffic, daily temperatures, quarterly sales.\n\n### Components of a Time Series:\n\n* Trend\n* Seasonality \n\n### What is a TREND in time series?\n\n* Trend is a pattern in data that shows the movement of a series to relatively higher or lower values over a long period of time.\n\n* Trend usually happens for some time and then disappears, it does not repeat. For example, some new kaggle kernels, it goes trending for a while, and then disappears. There is fairly any chance that it would be trending again.\n\n<b>A trend could be :<\/b>\n\n* <b>UPTREND<\/b>: Time Series Analysis shows a general pattern that is upward then it is Uptrend.\n* <b>DOWNTREND<\/b>: Time Series Analysis shows a pattern that is downward then it is Downtrend.\n* <b>HORIZONTAL TREND<\/b>: If no pattern observed then it is called a Horizontal or stationary trend.\n\n![](https:\/\/anomaly.io\/wp-content\/uploads\/2015\/12\/multiplicative-model.png)\n\n### What is SEASONALITY?\n\n* Predictable pattern that recurs or repeats over regular intervals. Seasonality is often observed within a year or less.\n\n![](https:\/\/miro.medium.com\/max\/700\/1*KajC_step0g6WQ7-49dYKQ.png)\n\n### Modelling and evaluation Techniques:\n\n* MODELS: Naive approach, Moving average, Simple exponential smoothing, Holt.s linear trend model, Auto Regression Integrated Moving Average(ARIMA), SARIMAX, etc.\n\n*  Mean Square Error(MSE), Root Mean Squared Error(RMSE)\n","7b85626f":"IF YOU FIND THIS KERNEL INSIGHTFUL, PLEASE GIVE AN UPVOTE.","d1030c79":"### 2. VISUALIZE THE DATA:","6e1e7e30":"* #### Testing For Stationarity : When a time series is stationary, it can be easier to model.\n* #### \"adfuller\" is a function \/ module used to check the STATIONARITY in dataset.","369b4bfc":"#### NOW OUR DATA IS STATIONARY.","5af38d78":"#### Here these two graphs will help you to find the p and q values.\n\n* Partial AutoCorrelation Graph is for the p-value.\n* AutoCorrelation Graph for the q-value.","3e3086c3":"### 1. BASIC STEPS OF A PROJECT:","03d1bf22":"-------------","86b85707":"### ARIMA\n\n* Autoregressive Integrated Moving Average, or ARIMA, is a forecasting method for univariate time series data.\n\n* As its name suggests, it supports both an autoregressive and moving average elements. The integrated element refers to differencing allowing the method to support time series data with a trend.\n\n* A problem with ARIMA is that it does not support seasonal data. That is a time series with a repeating cycle.\n\n* ARIMA expects data that is either not seasonal or has the seasonal component removed, e.g. seasonally adjusted via methods such as seasonal differencing.\n\n### SARIMA\n\n* Seasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component.\n\n* It adds three new hyperparameters to specify the autoregression (AR), differencing (I) and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality.\n\n* A seasonal ARIMA model is formed by including additional seasonal terms in the ARIMA.\n\n* The seasonal part of the model consists of terms that are very similar to the non-seasonal components of the model, but they involve backshifts of the seasonal period.\n\n### The general process for ARIMA models is the following:\n\n* Visualize the Time Series Data\n* Make the time series data stationary\n* Plot the Correlation and AutoCorrelation Charts\n* Construct the ARIMA Model or Seasonal ARIMA based on the data\n* Use the model to make prediction","bac22a9d":"#### DIFFERENCING:\n\n* Differencing is a popular and widely used data transform for making time series data stationary.\n\n* Differencing can help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.\n\n* Differencing shifts ONE\/MORE row towards downwards.","45a641f5":"### SARIMA MODEL","1c6fa2e3":"# <center>PYTHON IMPLEMENTATION<\/center>","1f06ab53":"# Hence, We have predicted the SALES for the next Two Years Successfully."}}