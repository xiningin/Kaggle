{"cell_type":{"7a2807a0":"code","8f7c4378":"code","616e2d84":"code","b269b287":"code","6f2dee9d":"code","e5cc793d":"code","90328580":"code","09d43f14":"code","d7265315":"code","86d64562":"code","71e651d3":"code","0dabde26":"code","7b9af5ea":"code","51bf4458":"code","2c10d9e6":"code","42b46188":"code","965d75a3":"code","85897b07":"code","7c019109":"code","89908744":"code","683e9576":"markdown","2e94c459":"markdown","70c9c71a":"markdown","9d87f303":"markdown","cb9c4ce5":"markdown","d2e3b5eb":"markdown","fb876f15":"markdown","2e14216f":"markdown","a7571d06":"markdown","b657f456":"markdown","6533c069":"markdown","c8031f56":"markdown","594e8881":"markdown","27e873a5":"markdown","8adfc0ee":"markdown","766526e4":"markdown","0cf8a1f1":"markdown","8c53d0c1":"markdown","5c120dc2":"markdown","769cb548":"markdown","31622c55":"markdown","536dbc2c":"markdown","2d9eee16":"markdown","5d762aa4":"markdown","0977d502":"markdown","dd6acd9d":"markdown"},"source":{"7a2807a0":"pip install lightautoml","8f7c4378":"import os\nimport time\n\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.model_selection import train_test_split\nimport torch\n\n# Imports from our package\nfrom lightautoml.automl.base import AutoML\nfrom lightautoml.automl.blend import WeightedBlender\nfrom lightautoml.ml_algo.boost_lgbm import BoostLGBM\nfrom lightautoml.ml_algo.linear_sklearn import LinearLBFGS\nfrom lightautoml.ml_algo.tuning.optuna import OptunaTuner\nfrom lightautoml.pipelines.features.lgb_pipeline import LGBSimpleFeatures, LGBAdvancedPipeline\nfrom lightautoml.pipelines.features.linear_pipeline import LinearFeatures\nfrom lightautoml.pipelines.ml.base import MLPipeline\nfrom lightautoml.pipelines.selection.importance_based import ModelBasedImportanceEstimator, ImportanceCutoffSelector\nfrom lightautoml.reader.base import PandasToPandasReader\nfrom lightautoml.tasks import Task\nfrom lightautoml.utils.profiler import Profiler\nfrom lightautoml.utils.timer import PipelineTimer","616e2d84":"N_THREADS = 8 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 600 # Time in seconds for automl run\nTARGET_NAME = 'TARGET' # Target column name","b269b287":"np.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","6f2dee9d":"p = Profiler()\np.change_deco_settings({'enabled': True})","e5cc793d":"%%time\n\ndata = pd.read_csv('..\/input\/lama-datasets\/sampled_app_train.csv')\ndata.head()","90328580":"%%time\n\ndata['BIRTH_DATE'] = (np.datetime64('2018-01-01') + data['DAYS_BIRTH'].astype(np.dtype('timedelta64[D]'))).astype(str)\ndata['EMP_DATE'] = (np.datetime64('2018-01-01') + np.clip(data['DAYS_EMPLOYED'], None, 0).astype(np.dtype('timedelta64[D]'))\n                    ).astype(str)\n\ndata['constant'] = 1\ndata['allnan'] = np.nan\n\ndata['report_dt'] = np.datetime64('2018-01-01')\n\ndata.drop(['DAYS_BIRTH', 'DAYS_EMPLOYED'], axis=1, inplace=True)","09d43f14":"data[TARGET_NAME] = np.where(np.random.rand(data.shape[0]) > .5, 2, data[TARGET_NAME].values)\ndata[TARGET_NAME].value_counts()","d7265315":"%%time\n\ntrain_data, test_data = train_test_split(data, \n                                         test_size=TEST_SIZE, \n                                         stratify=data[TARGET_NAME], \n                                         random_state=RANDOM_STATE)\nprint('Data splitted. Parts sizes: train_data = {}, test_data = {}'\n              .format(train_data.shape, test_data.shape))","86d64562":"train_data.head()","71e651d3":"%%time\n\ntimer = PipelineTimer(600, mode=2)","0dabde26":"%%time\n\ntimer_gbm = timer.get_task_timer('gbm') # Get task timer from pipeline timer \nfeat_sel_0 = LGBSimpleFeatures()\nmod_sel_0 = BoostLGBM(timer=timer_gbm)\nimp_sel_0 = ModelBasedImportanceEstimator()\nselector_0 = ImportanceCutoffSelector(feat_sel_0, mod_sel_0, imp_sel_0, cutoff=0, )","7b9af5ea":"%%time \n\nfeats_gbm_0 = LGBAdvancedPipeline(top_intersections=4, \n                                  output_categories=True, \n                                  feats_imp=imp_sel_0)\ntimer_gbm_0 = timer.get_task_timer('gbm')\ntimer_gbm_1 = timer.get_task_timer('gbm')\n\ngbm_0 = BoostLGBM(timer=timer_gbm_0)\ngbm_1 = BoostLGBM(timer=timer_gbm_1)\n\ntuner_0 = OptunaTuner(n_trials=20, timeout=30, fit_on_holdout=True)\ngbm_lvl0 = MLPipeline([\n        (gbm_0, tuner_0),\n        gbm_1\n    ],\n    pre_selection=selector_0,\n    features_pipeline=feats_gbm_0, \n    post_selection=None\n)","51bf4458":"%%time\n\nfeats_reg_0 = LinearFeatures(output_categories=True, \n                             sparse_ohe='auto')\n\ntimer_reg = timer.get_task_timer('reg')\nreg_0 = LinearLBFGS(timer=timer_reg)\n\nreg_lvl0 = MLPipeline([\n        reg_0\n    ],\n    pre_selection=None,\n    features_pipeline=feats_reg_0, \n    post_selection=None\n)","2c10d9e6":"%%time \n\ntask = Task('multiclass', metric = 'crossentropy', ) \nreader = PandasToPandasReader(task = task, samples = None, max_nan_rate = 1, max_constant_rate = 1,\n                              advanced_roles = True, drop_score_co = -1, n_jobs = 4)","42b46188":"%%time\n\nblender = WeightedBlender()","965d75a3":"%%time\n\nautoml = AutoML(reader=reader, levels=[\n    [gbm_lvl0, reg_lvl0]\n], timer=timer, blender=blender, skip_conn=False)","85897b07":"%%time\n\noof_pred = automl.fit_predict(train_data, roles={'target': TARGET_NAME})\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))","7c019109":"%%time\n\ntest_pred = automl.predict(test_data)\nprint('Prediction for test data:\\n{}\\nShape = {}'\n              .format(test_pred, test_pred.shape))\n\nprint('Check scores...')\nprint('OOF score: {}'.format(log_loss(train_data[TARGET_NAME].values, oof_pred.data)))\nprint('TEST score: {}'.format(log_loss(test_data[TARGET_NAME].values, test_pred.data)))","89908744":"for dat, df, name in zip([oof_pred, test_pred], [train_data, test_data], ['train', 'test']):\n    print('Check aucs {0}...'.format(name))\n    for cl in range(3):\n        sc = roc_auc_score((df[TARGET_NAME].values == cl).astype(np.float32), dat.data[:, cl])\n        print('Class {0} {1} auc score: {2}'.format(cl, name, sc))","683e9576":"# Step 0.7. Create fake multiclass target ","2e94c459":"## Step 8. Predict to test data and check scores","70c9c71a":"Block below can be omitted if you are going to train model only or you have specific train and test files:","9d87f303":"[More info about LAMA](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML)","cb9c4ce5":"## Step 7. Train AutoML on loaded data ","d2e3b5eb":"# Step 0.6. (Optional) Some user feature preparation ","fb876f15":"By default, profiling decorators are turned off for speed and memory reduction. If you want to see profiling report after using LAMA, you need to turn on the decorators using command below: ","2e14216f":"To combine predictions from different models into one vector we use WeightedBlender:","a7571d06":"## Step 6. Create AutoML pipeline","b657f456":"# Step 0.5. Example data load ","6533c069":"Our GBMs ML pipeline:\n- Advanced features for gradient boosting built on selected features (using step 2) \n- 2 different models:\n    * LightGBM with params tuning (using OptunaTuner)\n    * LightGBM with heuristic params\n","c8031f56":"## Step 4. Create multiclass task and reader","594e8881":"Cell below shows some user feature preparations to create task more difficult (this block can be omitted if you don't want to change the initial data):","27e873a5":"# Step 0.8. (Optional) Data splitting for train-test ","8adfc0ee":"## Step 3.1. Create GBMs pipeline for AutoML ","766526e4":"# Step 0.3. Fix torch number of threads and numpy seed ","0cf8a1f1":"# Step 0.1. Import necessary libraries ","8c53d0c1":"# ========= AutoML creation =========\n\n## Step 1. Create Timer for pipeline\n\nHere we are going to use strict timer for AutoML pipeline, which helps not to go outside the limit:","5c120dc2":"## Step 2. Create feature selector","769cb548":"## Step 3.2. Create linear pipeline for AutoML ","31622c55":"## Step 9. Check AUCs for each class in train and test data ","536dbc2c":"In cell below we train AutoML with target column `TARGET` to receive fitted model and OOF predictions:","2d9eee16":"Our linear pipeline:\n- Using features, special for linear models\n- LinearLBFGS as a model\n- Without feature selection here","5d762aa4":"# Step 0.4. Change profiling decorators settings ","0977d502":"# Step 0.2. Parameters ","dd6acd9d":"## Step 5. Create blender for 2nd level "}}