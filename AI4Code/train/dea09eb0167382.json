{"cell_type":{"16ed5407":"code","0a8613a6":"code","1942fac0":"code","fbccc53b":"code","69f4ffb0":"code","1ee299e4":"code","2fa204da":"code","f7a23ffa":"code","5da12161":"code","b7c361ef":"code","bd002e2a":"code","b4ada9df":"code","bf1b1f7f":"code","1db9777c":"code","c0d70a25":"code","3d3fd9c6":"code","44131cd1":"code","6fb6cc3f":"code","089a3981":"code","a0ddf863":"markdown","4615e4d9":"markdown","6213258d":"markdown","3292987c":"markdown","c3d58fc3":"markdown","de37921a":"markdown","a27ba6c4":"markdown","52a083fa":"markdown"},"source":{"16ed5407":"!nvidia-smi\n!git clone https:\/\/github.com\/filippad\/master-thesis.git\n#!pip install transformers==4.10.2","0a8613a6":"import zipfile\nimport shutil\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom pylab import rcParams\nfrom tqdm.auto import tqdm\n\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, multilabel_confusion_matrix\n\nfrom transformers import XLMRobertaTokenizer,XLMRobertaModel, AdamW, get_linear_schedule_with_warmup\nimport pytorch_lightning as pl\nfrom pytorch_lightning.metrics.functional import accuracy, f1, auroc\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\nRANDOM_SEED = 42\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\npl.seed_everything(RANDOM_SEED)","1942fac0":"with zipfile.ZipFile('master-thesis\/data\/jigsaw-toxic-comments\/train.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('master-thesis\/data\/jigsaw-toxic-comments')\n    \nwith zipfile.ZipFile('master-thesis\/data\/jigsaw-toxic-comments\/test.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('master-thesis\/data\/jigsaw-toxic-comments')\n    \nwith zipfile.ZipFile('master-thesis\/data\/jigsaw-toxic-comments\/test_labels.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('master-thesis\/data\/jigsaw-toxic-comments')","fbccc53b":"train_file_path = 'master-thesis\/data\/jigsaw-toxic-comments\/train.csv'\ndf = pd.read_csv(train_file_path)\nLABEL_COLUMNS = df.columns.tolist()[2:]\nprint(df.head(5))\n\ntest_file_path = 'master-thesis\/data\/jigsaw-toxic-comments\/test.csv'\ntest_df = pd.read_csv(test_file_path)\n#print(test_df.head())\n\ntest_labels_file_path = 'master-thesis\/data\/jigsaw-toxic-comments\/test_labels.csv'\ntest_labels_df = pd.read_csv(test_labels_file_path)\n#print(test_labels_df.head())","69f4ffb0":"def get_datasets(non_labels=0.2):\n    '''\n        Preprocessing the data and returns train and validation datasets\n    '''\n    # Drop labels of training dataset according to the removal ratio\n    num_labels_to_drop = int(non_labels * len(LABEL_COLUMNS))\n    drop_col_idx = torch.randint(0, len(LABEL_COLUMNS), (df.shape[0], num_labels_to_drop))\n    drop_row_idx = torch.arange(df.shape[0]).reshape(-1,1)\n    drop_idx = torch.cat((drop_row_idx, drop_col_idx), 1).tolist()\n    drop_idx = [list(map(int, item)) for item in drop_idx]\n    #print(drop_idx[:10])\n    \n    for index in drop_idx:\n        df.iat[index[0], index[1] + 2] = -1\n        \n    train_df, val_df = train_test_split(df, test_size=0.05)\n    \n    # To deal with imbalance in the dataset, a number of clean comments is removed\n    train_toxic = train_df[train_df[LABEL_COLUMNS].sum(axis=1) >= 0]\n    train_clean = train_df[train_df[LABEL_COLUMNS].sum(axis=1) < 0]\n    \n    print(train_toxic.shape)\n    print(train_clean.shape)\n    \n    train_df = pd.concat([\n        train_toxic,\n        train_clean.sample(5_000)\n        \n        #train_toxic.sample(2_000), # Just to test, change it later!\n        #train_clean.sample(45_000)\n    ])\n    \n    #val_df = val_df.sample(200) # Just to test, change it later!\n    \n    return train_df, val_df\n","1ee299e4":"train_df, val_df = get_datasets()\nprint(train_df.shape)\nprint(val_df.shape)\n#print(train_df.head(10))\ndel df","2fa204da":"MODEL_NAME = 'xlm-roberta-base'\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n","f7a23ffa":"token_counts = []\n\nfor _,row in train_df.iterrows():\n    token_count = len (tokenizer.encode(\n            row[\"comment_text\"],\n            max_length=512,\n            truncation=True\n        )\n    )\n    token_counts.append(token_count)\n    \nsns.histplot(token_counts)\nplt.xlim([0, 512]);\n    ","5da12161":"MAX_TOKEN_COUNT = 512","b7c361ef":"class ToxicCommentsDataset(Dataset):\n    \n    def __init__(self, data:pd.DataFrame, tokenizer:XLMRobertaTokenizer, max_token_len:int=128):\n        self.tokenizer = tokenizer\n        self.data = data\n        self.max_token_len = max_token_len\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index:int):\n        data_row = self.data.iloc[index]\n        \n        comment_text = data_row.comment_text\n        labels = data_row[LABEL_COLUMNS]\n        \n        encoding = self.tokenizer.encode_plus(\n            comment_text,\n            add_special_tokens = True,\n            max_length = self.max_token_len,\n            return_token_type_ids=False,\n            padding = \"max_length\",\n            truncation = True,\n            return_attention_mask = True,\n            return_tensors = 'pt',\n        )\n        \n        return dict(\n            comment_text = comment_text,\n            input_ids = encoding[\"input_ids\"].flatten(),\n            attention_mask=encoding[\"attention_mask\"].flatten(),\n            labels = torch.FloatTensor(labels)\n        )","bd002e2a":"class ToxicCommentDataModule(pl.LightningDataModule):\n    def __init__(self, train_df, val_df, tokenizer, batch_size=8, max_token_len=128):\n        super().__init__()\n        self.batch_size = batch_size\n        self.train_df = train_df\n        self.val_df = val_df\n        #self.test_df = val_df # TODO: fix this in real project\n        self.tokenizer = tokenizer\n        self.max_token_len = max_token_len\n        \n    def setup(self, stage=None):\n        self.train_dataset = ToxicCommentsDataset(self.train_df, self.tokenizer, self.max_token_len)\n        self.val_dataset = ToxicCommentsDataset(self.val_df, self.tokenizer, self.max_token_len)\n        #self.test_dataset = ToxicCommentsDataset(self.test_df, self.tokenizer, self.max_token_len)\n    \n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n    \n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=2)\n    \n    #def test_dataloader(self):\n        #return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=2)","b4ada9df":"class ToxicCommentTagger(pl.LightningModule):\n    def __init__(self, n_classes:int, n_training_steps=None, n_warmup_steps=None):\n        super().__init__()\n        self.model = XLMRobertaModel.from_pretrained(MODEL_NAME, return_dict=True)\n        self.classifier = nn.Linear(self.model.config.hidden_size, n_classes) # Define the classifier\n        self.n_training_steps = n_training_steps\n        self.n_warmup_steps = n_warmup_steps\n        self.criterion = nn.BCELoss(reduction='sum') # Define the loss function\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        output = self.model(input_ids, attention_mask=attention_mask)\n        output = self.classifier(output.pooler_output) \n        output = torch.sigmoid(output)\n        loss = 0\n        if labels is not None:\n            #print(labels)\n            #print(output)\n            #print('labels 2:', labels)\n            #print('output 2: ', output)\n            \n            mask = ~labels.eq(-1)\n            loss = self.criterion(output*mask, labels*mask)\n            \n            #loss = self.criterion(output, labels)\n        return loss, output\n    \n    def training_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        labels = batch[\"labels\"]\n        loss, outputs = self(input_ids, attention_mask, labels)\n        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n    \n    def validation_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        labels = batch[\"labels\"]\n        loss, _ = self(input_ids, attention_mask, labels)\n        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n        return loss\n        \n    #def test_step(self, batch, batch_idx):\n        #input_ids = batch[\"input_ids\"]\n        #attention_mask = batch[\"attention_mask\"]\n        #labels = batch[\"labels\"]\n        #loss, _ = self(input_ids, attention_mask, labels)\n        #self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n        #return loss\n    \n    #def training_epoch_end(self, outputs):\n        #labels = []\n        #predictions = []\n        #for output in outputs:\n            #for out_labels in output[\"labels\"].detach().cpu():\n                #labels.append(out_labels)\n            #for out_prediction in output[\"predictions\"].detach().cpu():\n                #predictions.append(out_prediction)\n                \n        #labels = torch.stack(labels).int()\n        #predictions = torch.stack(predictions)\n        \n        #for i, name in enumerate(LABEL_COLUMNS):\n            #class_roc_auc = auroc(predictions[:,i], labels[:,i])\n            #self.logger.experiment.add_scalar(f\"{name}_roc_auc\/Train\", class_roc_auc, self.current_epoch)\n            \n    def configure_optimizers(self):\n        optimizer = AdamW(self.parameters(), lr=2e-5)\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps = self.n_warmup_steps,\n            num_training_steps = self.n_training_steps\n        )\n        \n        return dict(optimizer = optimizer, lr_scheduler = dict(scheduler = scheduler, interval = 'step'))\n    \n    \n    ","bf1b1f7f":"N_EPOCHS = 6\nBATCH_SIZE = 12\ndata_module = ToxicCommentDataModule(train_df, val_df, tokenizer, batch_size=BATCH_SIZE, max_token_len=MAX_TOKEN_COUNT)\n\nsteps_per_epoch = len(train_df) \/\/ BATCH_SIZE\ntotal_training_steps = steps_per_epoch * N_EPOCHS\nwarmup_steps = total_training_steps \/\/ 5\n# warmup_steps, total_training_steps\n\nmodel = ToxicCommentTagger(\n    n_classes=len(LABEL_COLUMNS),\n    n_warmup_steps=warmup_steps,\n    n_training_steps=total_training_steps\n)","1db9777c":"checkpoint_callback = ModelCheckpoint(\n    dirpath=\"checkpoints\",\n    filename=\"best-checkpoint\",\n    save_top_k=1,\n    verbose=True,\n    monitor=\"val_loss\",\n    mode=\"min\"\n)\n\nlogger = TensorBoardLogger(\"lightning_logs\", name = \"toxic-comments\")\nearly_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 2)\n\ntrainer = pl.Trainer(\n    logger = logger,\n    checkpoint_callback = True,\n    callbacks = [checkpoint_callback, early_stopping_callback],\n    max_epochs = N_EPOCHS,\n    gpus=1,\n    progress_bar_refresh_rate=30\n)","c0d70a25":"trainer.fit(model, data_module)","3d3fd9c6":"trained_model = ToxicCommentTagger.load_from_checkpoint(\n    trainer.checkpoint_callback.best_model_path,\n    n_classes = len(LABEL_COLUMNS)\n)\n\ntrained_model.eval()\ntrained_model.freeze()\n\ntest_comment = \"Hi, I'm Meredith and I'm an alch... good at supplier relations\"\nencoding = tokenizer.encode_plus(\n  test_comment,\n  add_special_tokens=True,\n  max_length=512,\n  return_token_type_ids=False,\n  padding=\"max_length\",\n  return_attention_mask=True,\n  return_tensors='pt',\n)\n_, test_prediction = trained_model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\ntest_prediction = test_prediction.flatten().numpy()\nfor label, prediction in zip(LABEL_COLUMNS, test_prediction):\n  print(f\"{label}: {prediction}\")","44131cd1":"THRESHOLD = 0.9\ntest_comment = \"You like dick! Then go suck it. What a hoe is this bitch!\"\nencoding = tokenizer.encode_plus(\n  test_comment,\n  add_special_tokens=True,\n  max_length=512,\n  return_token_type_ids=False,\n  padding=\"max_length\",\n  return_attention_mask=True,\n  return_tensors='pt',\n)\n_, test_prediction = trained_model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\ntest_prediction = test_prediction.flatten().numpy()\nfor label, prediction in zip(LABEL_COLUMNS, test_prediction):\n  if prediction < THRESHOLD:\n    continue\n  print(f\"{label}: {prediction}\")\n","6fb6cc3f":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntrained_model = trained_model.to(device)\n\ntest_df = test_labels_df.join(test_df['comment_text'])\ntest_df.drop(test_df[test_df.toxic == -1].index,inplace=True)\n\nprint(test_df.shape)\n\ntest_dataset = ToxicCommentsDataset(\n  #test_df.sample(500),\n  test_df,\n  tokenizer,\n  max_token_len=MAX_TOKEN_COUNT\n)\n\npredictions = []\nlabels = []\n\nfor item in tqdm(test_dataset):\n    _, prediction = trained_model(\n        item[\"input_ids\"].unsqueeze(dim=0).to(device),\n        item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n      )\n    predictions.append(prediction.flatten())\n    labels.append(item[\"labels\"].int())\n\npredictions = torch.stack(predictions).detach().cpu()\nlabels = torch.stack(labels).detach().cpu()\n\n#print(predictions)\n#print(labels)\n\nprint(\"Overall accuracy: \", accuracy(predictions, labels, threshold=THRESHOLD))\n\nprint(\"AUROC per tag:\")\nfor i, name in enumerate(LABEL_COLUMNS):\n    tag_auroc = auroc(predictions[:, i], labels[:, i], pos_label=1)\n    print(f\"{name}: {tag_auroc}\")","089a3981":"y_pred = predictions.numpy()\ny_true = labels.numpy()\nupper, lower = 1, 0\ny_pred = np.where(y_pred > THRESHOLD, upper, lower)\nprint(classification_report(\n  y_true,\n  y_pred,\n  target_names=LABEL_COLUMNS,\n  zero_division=0\n))","a0ddf863":"### Download and unzip dataset","4615e4d9":"### Predictions","6213258d":"### Specify the Model","3292987c":"### Evaluation","c3d58fc3":"## Try another example","de37921a":"### Preprocess the data","a27ba6c4":"### Set up the scheduler and create the model instance","52a083fa":"### Tokenization"}}