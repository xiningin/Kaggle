{"cell_type":{"bfb987cf":"code","3d702774":"code","6b5c4151":"code","08ef3f3c":"code","f325d0a0":"code","e6dae844":"code","100e1f20":"code","f974c217":"code","45c4d90c":"code","b68a4919":"code","520d73c7":"code","10dc1f53":"code","cb8c34e3":"code","59ccee09":"code","0b825388":"code","d93e2900":"code","09bc6a4a":"code","879ddaf2":"code","808c62b4":"code","b165549e":"code","62886234":"code","1f823362":"code","b908bbd0":"code","1604ced3":"code","54213c2c":"code","16a14c41":"code","64571447":"code","ce8ac7c9":"code","c70e3d5e":"code","9375ea55":"code","83f4ebd8":"code","3d634049":"code","1dc47a90":"code","3f80497c":"code","7c8ea0c3":"code","a56d84ae":"code","1af176be":"code","701b801c":"code","74e562f8":"code","c80e1bc0":"code","4143b22b":"code","04983986":"code","3051ed74":"code","ccb158ae":"code","78260b4f":"code","46bc6626":"code","4848aa6b":"code","693dd68f":"code","9166d102":"code","c35f3e1a":"code","e2c0cf9a":"code","ab7a06f3":"code","a9ecf38f":"code","b7f7dbac":"code","33bc38de":"code","f7d1ddd9":"code","0d1fbf00":"markdown","6d9d3048":"markdown","4b3dc6ca":"markdown","bc8dae3d":"markdown","2f5e4a54":"markdown","e27d6d86":"markdown","362371c3":"markdown","9a5eca31":"markdown","34d8e249":"markdown","a71f830b":"markdown","bb1a5874":"markdown","9264a761":"markdown","d1ad81cf":"markdown","fb07c4a2":"markdown","4ae96000":"markdown","f750e264":"markdown","ba4ea823":"markdown","6ec1c1db":"markdown","5ee32d61":"markdown","2f735ebc":"markdown","3e343b1b":"markdown","54cf1647":"markdown","c8e9f602":"markdown","15ad21b9":"markdown","46743dbb":"markdown","a9bd5448":"markdown","658b34d9":"markdown","76bc83d8":"markdown","feb959b2":"markdown","b701f7c4":"markdown","ab4ade2d":"markdown","4474b671":"markdown","c6377c04":"markdown","5d076dec":"markdown","49c4a71c":"markdown","cd3ff336":"markdown","e3611450":"markdown","864091b3":"markdown","d91aec6e":"markdown","a97e9b49":"markdown","8df64d16":"markdown","e77b9462":"markdown","d022f054":"markdown","87a889fa":"markdown","5dd24c13":"markdown"},"source":{"bfb987cf":"import numpy as np\n\nimport tensorflow as tf\nimport os\nimport pickle\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\n\nsns.set_theme()\nsns.set(style=\"ticks\")","3d702774":"pth='..\/input\/emotion-compilation\/emotion_compilation\/test'\npth_train='..\/input\/emotion-compilation\/emotion_compilation\/train'\n\nds_train=tf.keras.preprocessing.image_dataset_from_directory(\n    directory=pth_train,\n    color_mode='grayscale',\n    class_names=os.listdir(pth),\n    image_size=(128,128),\n    batch_size=256,\n    seed=0,\n    shuffle=True\n)\n\n\nds_val=tf.keras.preprocessing.image_dataset_from_directory(\n    directory=pth,\n    color_mode='grayscale',\n    class_names=os.listdir(pth),\n    image_size=(128,128),\n    batch_size=256,\n    seed=0,\n    shuffle=True\n)\n","6b5c4151":"class_map={i:f for i,f in enumerate(ds_val.class_names)}","08ef3f3c":"#beta=0.1\nE1=tf.keras.models.load_model('..\/input\/models-att\/model0d1\/model0d1\/encoder')\nD1=tf.keras.models.load_model('..\/input\/models-att\/model0d1\/model0d1\/decoder')","f325d0a0":"#beta=1.25\n\nE2=tf.keras.models.load_model('..\/input\/models-att\/model1d25\/model1d25\/encoder')\nD2=tf.keras.models.load_model('..\/input\/models-att\/model1d25\/model1d25\/decoder')","e6dae844":"with open('..\/input\/models-att\/model0d1\/model0d1\/track_01.pickle','rb') as file:\n    track1=pickle.load(file)","100e1f20":"with open('..\/input\/models-att\/model1d25\/model1d25\/track_1d25.pickle','rb') as file:\n    track2=pickle.load(file)","f974c217":"def sample_latent(label,mean,logvar,eps=0): #default use mean sampling\n    m=mean.shape[0]\n    if eps:\n        eps=tf.random.normal((1,128),stddev=3.0) #large range to see the change of the face attributes \n    else:\n        eps=0\n    z=mean+tf.exp(0.5*logvar)*eps\n    z_active=tf.nn.sigmoid(z)\n    im=tf.concat([label,z_active],axis=-1)\n    im=tf.reshape(im,(m,1,1,-1))\n    return z,z_active,im","45c4d90c":"flat=tf.keras.layers.Flatten()","b68a4919":"def encoder_feature(x,y,E,D):\n    \n    temp=E.layers[1](E.layers[0](x,training=False),training=False)\n    \n    f1=tf.nn.leaky_relu(E.layers[2].layers[0](temp,training=False))\n    \n    att1=E.layers[2].layers[0].excitation(temp,training=False) #label related \n    \n    ex,mean,logvar=E(x,training=False)\n    \n    f2,_,im=sample_latent(y,mean,logvar)\n    \n    x_prime=D(im,training=False)\n    \n    temp=E.layers[1](E.layers[0](x_prime,training=False),training=False)\n    \n    f3=tf.nn.leaky_relu(E.layers[2].layers[0](temp,training=False))\n    \n    att2=E.layers[2].layers[0].excitation(temp,training=False)\n    \n    return flat(f1),f2,flat(f3),att1[:,0,0,:],att2[:,0,0,:]","520d73c7":"fig,ax=plt.subplots(ncols=len(track1.keys()),figsize=(20,5))\nfor i,key in enumerate(track1):\n    ax[i].grid()\n    sns.lineplot(x=range(len(track1[key])),y=np.array(track1[key]),label='beta:0.1',linewidth=0.25,ax=ax[i])\n    sns.lineplot(x=range(len(track2[key])),y=np.array(track2[key]),label='beta:1.25',linewidth=0.25,ax=ax[i])\n    ax[i].set_xlabel('iteration')\n    ax[i].set_title(key)\nplt.legend()\nplt.savefig('training_curve.jpg')\nplt.show()","10dc1f53":"from sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","cb8c34e3":"pred1=[]\npred2=[]\nlabel=[]\n\nloop=tqdm(ds_val,leave=True)\nfor x,y in loop:\n    x=x\/255.\n    ex,mean,logvar=E1(x,training=False) #ex:(batch,7)\n    pred1.append(tf.argmax(ex,axis=-1))\n\n    ex,mean,logvar=E2(x,training=False) #ex:(batch,7)\n    pred2.append(tf.argmax(ex,axis=-1))\n    label.append(y)","59ccee09":"pred1=tf.concat(pred1,axis=0).numpy()\nlabel=tf.concat(label,axis=0).numpy()","0b825388":"print({i:class_map[i] for i in range(7)})\nprint(classification_report(label,pred1,target_names=class_map.values()))","d93e2900":"C1=confusion_matrix(label,pred1,normalize='pred')","09bc6a4a":"plt.figure(figsize=(10,10))\nheatmap=sns.heatmap(C1,vmin=0,vmax=1.,cmap=\"YlGnBu\",annot=True)\nheatmap.set_xticklabels([v for k,v in class_map.items()], rotation=0)\nheatmap.set_yticklabels([v for k,v in class_map.items()], rotation=0)\nplt.xlabel('prediction')\nplt.ylabel('ground truth')\nplt.title('beta0.1')\nplt.savefig('C_beta01.jpg')\nplt.show()","879ddaf2":"pred2=tf.concat(pred2,axis=0).numpy()","808c62b4":"print({i:class_map[i] for i in range(7)})\nprint(classification_report(label,pred2,target_names=class_map.values()))","b165549e":"C2=confusion_matrix(label,pred2,normalize='pred')","62886234":"plt.figure(figsize=(10,10))\nheatmap=sns.heatmap(C2,vmin=0,vmax=1.,cmap=\"YlGnBu\",annot=True)\n\nheatmap.set_xticklabels([v for k,v in class_map.items()], rotation=0)\nheatmap.set_yticklabels([v for k,v in class_map.items()], rotation=0)\nplt.title('beta1.25')\nplt.xlabel('prediction')\nplt.ylabel('ground truth')\nplt.savefig('C_beta1_25.jpg')\nplt.show()","1f823362":"delta=C1-C2 #beta0.1 - beta1.0","b908bbd0":"heatmap=sns.heatmap(delta,center=0,vmin=-1,vmax=1)\n\nheatmap.set_xticklabels([v for k,v in class_map.items()], rotation=0)\nheatmap.set_yticklabels([v for k,v in class_map.items()], rotation=0)\nplt.title('beta0.1-beta1.25')\nplt.xlabel('prediction')\nplt.ylabel('ground truth')\nplt.savefig('C1_C2.jpg')\nplt.show()","1604ced3":"def get_features(ds):\n    f1_1=[]\n    f1_2=[]\n    f1_3=[]\n    f2_1=[]\n    f2_2=[]\n    f2_3=[]\n    att1_1=[]\n    att1_2=[]\n    att2_1=[]\n    att2_2=[]\n    label=[]\n    loop=tqdm(ds,leave=True)\n    for x,y in loop:\n        x=x\/255.\n        label.append(y)\n        y=tf.one_hot(y,depth=7)\n        f1,f2,f3,att1,att2=encoder_feature(x,y,E1,D1)\n        f1_1.append(f1)\n        f1_2.append(f2)\n        f1_3.append(f3)\n        att1_1.append(att1)\n        att1_2.append(att2)\n        f1,f2,f3,att1,att2=encoder_feature(x,y,E2,D2)\n        f2_1.append(f1)\n        f2_2.append(f2)\n        f2_3.append(f3)\n        att2_1.append(att1)\n        att2_2.append(att2)\n    f1_1=tf.concat(f1_1,axis=0)\n    f1_2=tf.concat(f1_2,axis=0)\n    f1_3=tf.concat(f1_3,axis=0)\n    f2_1=tf.concat(f2_1,axis=0)\n    f2_2=tf.concat(f2_2,axis=0)\n    f2_3=tf.concat(f2_3,axis=0)\n    att1_1=tf.concat(att1_1,axis=0)\n    att1_2=tf.concat(att1_2,axis=0)\n    att2_1=tf.concat(att2_1,axis=0)\n    att2_2=tf.concat(att2_2,axis=0)\n    label=tf.concat(label,axis=0)\n        \n    return label,f1_1,f1_2,f1_3,f2_1,f2_2,f2_3,att1_1,att1_2,att2_1,att2_2","54213c2c":"train=get_features(ds_train)\nx_train=train[1:]\ny_train=train[0]\n\nval=get_features(ds_val)\nx_val=val[1:]\ny_val=val[0]","16a14c41":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE","64571447":"embeddings=[]\nlda=LinearDiscriminantAnalysis(n_components=3)\n\n\nloop=tqdm(zip(x_train[:7],x_val[:7]),leave=True)\n\nfor f_train,f_val in loop:\n    lda.fit(f_train.numpy(),y_train.numpy())\n    emb=lda.transform(f_val.numpy())\n    embeddings.append(emb)","ce8ac7c9":"fig = plt.figure(figsize=(15,15))\nax = fig.gca(projection='3d')\nax.view_init(70,30)\nx=embeddings[0]\nfor i in range(len(class_map)):\n    ax.scatter(x[y_val.numpy()==i][:,0],x[y_val.numpy()==i][:,1],x[y_val.numpy()==i][:,2],label=class_map[i],marker='s',edgecolors='black',linewidth=0.2,s=30)\nplt.title('attention map before adding label beta=0.1',fontsize=30)\nplt.legend(fontsize=30,markerscale=3)\nplt.savefig('attention map before adding label beta=0.1.jpg')\nplt.show()","c70e3d5e":"fig = plt.figure(figsize=(15,15))\nax = fig.gca(projection='3d')\nx=embeddings[1]\nax.view_init(70,30)\nfor i in range(len(class_map)):\n    ax.scatter(x[y_val.numpy()==i][:,0],x[y_val.numpy()==i][:,1],x[y_val.numpy()==i][:,2],label=class_map[i],marker='s',edgecolors='black',linewidth=0.2,s=30)\nplt.title('latent varialbe z beta=0.1',fontsize=30)\nplt.legend(fontsize=30,markerscale=3)\nplt.savefig('latent varialbe z beta=0.1.jpg')\nplt.show()","9375ea55":"fig = plt.figure(figsize=(15,15))\nax = fig.gca(projection='3d')\nax.view_init(60,30)\nx=embeddings[2]\nfor i in range(len(class_map)):\n    ax.scatter(x[y_val.numpy()==i][:,0],x[y_val.numpy()==i][:,1],x[y_val.numpy()==i][:,2],label=class_map[i],marker='s',edgecolors='black',linewidth=0.2,s=30)\nplt.title('attention map after adding label beta=0.1',fontsize=30)\nplt.legend(fontsize=30,markerscale=3)\nplt.savefig('attention map after adding label beta=0.1.jpg')\nplt.show()","83f4ebd8":"fig = plt.figure(figsize=(15,15))\nax = fig.gca(projection='3d')\nx=embeddings[3]\nax.view_init(40,30)\nfor i in range(len(class_map)):\n    ax.scatter(x[y_val.numpy()==i][:,0],x[y_val.numpy()==i][:,1],x[y_val.numpy()==i][:,2],label=class_map[i],marker='s',edgecolors='black',linewidth=0.2,s=30)\nplt.title('attention map before adding label beta=1.25',fontsize=30)\nplt.legend(fontsize=30,markerscale=3)\nplt.savefig('attention map before adding label beta=1.25.jpg')\nplt.show()","3d634049":"fig = plt.figure(figsize=(15,15))\nax = fig.gca(projection='3d')\nx=embeddings[4]\nax.view_init(70,30)\nfor i in range(len(class_map)):\n    ax.scatter(x[y_val.numpy()==i][:,0],x[y_val.numpy()==i][:,1],x[y_val.numpy()==i][:,2],label=class_map[i],marker='s',edgecolors='black',linewidth=0.2,s=30)\nplt.title('latent variable z beta=1.25',fontsize=30)\nplt.legend(fontsize=30,markerscale=3)\nplt.savefig('latent variable z beta=1.25.jpg')\nplt.show()","1dc47a90":"fig = plt.figure(figsize=(15,15))\nax = fig.gca(projection='3d')\nx=embeddings[5]\nax.view_init(70,60)\nfor i in range(len(class_map)):\n    ax.scatter(x[y_val.numpy()==i][:,0],x[y_val.numpy()==i][:,1],x[y_val.numpy()==i][:,2],label=class_map[i],marker='s',edgecolors='black',linewidth=0.2,s=30)\nplt.title('attention map after adding label beta=1.25',fontsize=30)\nplt.legend(fontsize=30,markerscale=3)\nplt.savefig('attention map after adding label beta=1.25.jpg')\nplt.show()","3f80497c":"top_idx={}\nlow_idx={}\n\nfig,ax=plt.subplots(ncols=4,figsize=(20,5))\nfor i,(text,a) in enumerate(zip(['excitation beta=0.1','excitation after adding label beta=0.1',\n                'excitation beta=1.25','excitation after adding label beta=1.25'],x_val[-4:])):\n    top_idx[text]=tf.math.top_k(tf.reduce_mean(a,axis=0),5)[1].numpy()\n    low_idx[text]=tf.math.top_k(-tf.reduce_mean(a,axis=0),5)[1].numpy()\n    ax[i].grid()\n    sns.lineplot(x=range(1,513),y=tf.reduce_mean(a,axis=0),ax=ax[i])\n    ax[i].set_title(text)\n    ax[i].set_ylim(0,1)\n    ax[i].set_xlabel('channels')\n    ax[i].set_ylabel('exciation')\n    \nplt.savefig('Exciation.jpg')\nplt.show()","7c8ea0c3":"from sklearn.cluster import AgglomerativeClustering,KMeans\nfrom sklearn.metrics import silhouette_score","a56d84ae":"p=[]\n\nfor i in range(len(class_map)):\n    p.append(sum(label==i))","1af176be":"T=['attention map before adding label beta=0.1',\n    'latent variable z beta=0.1','attention map after adding label beta=0.1',\n    'attention map before adding label beta=1.25',\n    'latent variable z beta=1.25','attention map for after adding label beta=1.25']\n\nbest_score={}\nfor text,k in zip(T,range(len(T))):\n    print(text)\n    X=embeddings[k] \n    C=[]\n    scores=[]\n    n_clusters=[]\n    for c in range(3,len(class_map)+3):\n        clusterer = KMeans(n_clusters=c)\n        y=clusterer.fit_predict(X)\n        C.append(clusterer)\n        score=silhouette_score(X,y)\n        scores.append(score)\n        n_clusters.append(c)\n        print(f'cluster num :{c}-- score:{score}')\n    best_score_idx=np.argmax(scores)\n    n_cluster=n_clusters[best_score_idx]\n    best_score[text]=(np.max(scores),n_cluster)\n    \n    fig,ax=plt.subplots(ncols=max(n_cluster-n_cluster\/\/2,max(n_cluster\/\/2,2)),nrows=2,figsize=(35,10))\n    i=0\n    j=0\n    for c in np.unique(C[best_score_idx].labels_):\n        if c==max(n_cluster-n_cluster\/\/2,n_cluster\/\/2):\n            i=1\n            j=0\n        cluster=y_val.numpy()[C[best_score_idx].labels_==c]\n        \n        p_cluster=[]\n        \n        for idx in range(len(class_map)):\n            p_cluster.append(sum(cluster==idx)\/p[idx])\n            \n        for label in (ax[i][j].get_xticklabels() + ax[i][j].get_yticklabels()):\n            label.set_fontsize(16)\n        \n        ax[i][j].bar([class_map[class_] for class_ in range(7)],p_cluster)\n        ax[i][j].set_title(f'cluster {c+1}')\n        ax[i][j].set_ylim(0,1)\n        #sns.histplot(list(map(lambda x: class_map[x],cluster)),ax=ax[i][j])\n        #ax[i][j].set_xticks(list(class_map.values()))\n        j+=1\n    plt.savefig(f'cluster--{text}.jpg')\n    plt.show()","701b801c":"import pandas as pd","74e562f8":"best_score","c80e1bc0":"df=pd.DataFrame([np.array([val[0],val[1]]) for key,val in best_score.items()], \n                columns=['sihouette score','n_cluster'],\n               index=best_score.keys())","4143b22b":"df","04983986":"def log_normal_pdf(sample, mean, logvar, raxis=-1):\n    log2pi = tf.math.log(2. * np.pi)\n    return -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi)","3051ed74":"def estimate(z,mean,logvar):\n  N=z.shape[0]\n  K=z.shape[-1]\n\n  z=tf.expand_dims(z,axis=0) #(1,N,K)\n  mean=tf.expand_dims(mean,axis=1) #(N,1,K)\n  logvar=tf.expand_dims(logvar,axis=1) #(N,1,K)\n  Hz=tf.zeros((K))\n  Hz_joint=0\n  k=0\n  while k<N:\n    batch=min(10,N-k)\n    z_ex=tf.repeat(z[:,k:k+batch,:],N,axis=0) #(N,batch,K)\n    mean_ex=tf.repeat(mean,batch,axis=1) #(N,batch,K)\n    logvar_ex=tf.repeat(logvar,batch,axis=1) #(N,batch,K)\n    logqz_j=log_normal_pdf(z_ex,mean_ex,logvar_ex) #(N,batch,K)\n    k+=batch\n\n    q_max=tf.reduce_max(logqz_j,axis=0,keepdims=True) #(1,batch,K)\n    q_gap=logqz_j-q_max #(N,batch,K)\n\n    weight=tf.cast(N,'float32')\n    Hz+=tf.reduce_sum(tf.math.log(weight)-\\\n        (q_max[0,...]+tf.math.log(tf.reduce_sum(tf.exp(q_gap),axis=0))),axis=0)\n    \n    logqz=tf.reduce_sum(logqz_j,axis=-1) #(N,batch)\n    q_max=tf.reduce_max(logqz,axis=0,keepdims=True) #(1,batch)\n    q_gap=logqz-q_max #(N,batch)\n\n    Hz_joint+=tf.reduce_sum(tf.math.log(weight)-\\\n        (q_max[0,...]+tf.math.log(tf.reduce_sum(tf.exp(q_gap),axis=0))),axis=0)\n    \n  return Hz\/N,Hz_joint\/N","ccb158ae":"def get_tc(ds):\n    z_1=[]\n    z_2=[]\n    mean_1=[]\n    mean_2=[]\n    logvar_1=[]\n    logvar_2=[]\n    loop=tqdm(ds)\n    for x,y in loop:\n        x=x\/255.\n        eps=tf.random.normal((x.shape[0],128))\n        ex1,mean1,logvar1=E1(x,training=False)\n        ex2,mean2,logvar2=E2(x,training=False)\n        z1=mean1+tf.exp(0.5*logvar1)*eps\n        z2=mean2+tf.exp(0.5*logvar2)*eps\n        z_1.append(z1)\n        z_2.append(z2)\n        mean_1.append(mean1)\n        mean_2.append(mean2)\n        logvar_1.append(logvar1)\n        logvar_2.append(logvar2)\n    z_1=tf.concat(z_1,axis=0)\n    z_2=tf.concat(z_2,axis=0)\n    mean_1=tf.concat(mean_1,axis=0)\n    mean_2=tf.concat(mean_2,axis=0)\n    logvar_1=tf.concat(logvar_1,axis=0)\n    logvar_2=tf.concat(logvar_2,axis=0)\n    \n\n    hz1,hz1_joint=estimate(z_1,mean_1,logvar_1)\n    hz2,hz2_joint=estimate(z_2,mean_2,logvar_2)\n    \n    d1=-hz1_joint+tf.reduce_sum(hz1)\n    d2=-hz2_joint+tf.reduce_sum(hz2)\n    \n    return d1,d2","78260b4f":"d1,d2=get_tc(ds_train)","46bc6626":"d3,d4=get_tc(ds_val)","4848aa6b":"d1,d2,d3,d4","693dd68f":"import pandas as pd","9166d102":"pd.DataFrame([[d3.numpy(),d1.numpy()],[d4.numpy(),d2.numpy()]],index=['beta=0.1','beta=1.25'],columns=['train','test'])","c35f3e1a":"def sampling(x,E,D,eps=0):\n    n_class=len(class_map)\n    I=tf.eye(n_class) #test each classes , identity matrix=one hot label\n    \n    fig,ax=plt.subplots(ncols=n_class+1,figsize=(15,15))\n\n    ex,mean,logvar=E(x,training=False)\n    ax[0].imshow(x[0,...].numpy(),cmap='gray')\n    ax[0].set_title('original')\n    \n    for i,y in enumerate(I):\n        y=tf.expand_dims(y,axis=0)\n        _,_,im=sample_latent(y,mean,logvar,eps=eps)\n        x_r=D(im,training=False)\n        \n        ax[i+1].imshow(x_r[0,...].numpy(),cmap='gray')\n        ax[i+1].set_title(class_map[i])\n    plt.show()","e2c0cf9a":"pth='..\/input\/emotion-compilation\/emotion_compilation\/test'\n\n\nds_val=tf.keras.preprocessing.image_dataset_from_directory(\n    directory=pth,\n    color_mode='grayscale',\n    class_names=os.listdir(pth),\n    image_size=(128,128),\n    batch_size=128,\n    shuffle=False\n)\n","ab7a06f3":"m=30 #sample size","a9ecf38f":"for X,_ in ds_val:\n    for i,x in enumerate(X):\n        x=x\/255.\n        x=tf.expand_dims(x,axis=0)\n        sampling(x,E1,D1)\n        if i==m:\n            break\n    break","b7f7dbac":"for X,_ in ds_val:\n    for i,x in enumerate(X):\n        x=x\/255.\n        x=tf.expand_dims(x,axis=0)\n        sampling(x,E2,D2)\n        if i==m:\n            break\n    break","33bc38de":"for X,_ in ds_val:\n    for i,x in enumerate(X):\n        x=x\/255.\n        x=tf.expand_dims(x,axis=0)\n        sampling(x,E1,D1,eps=True)\n        if i==m:\n            break\n    break","f7d1ddd9":"for X,_ in ds_val:\n    for i,x in enumerate(X):\n        x=x\/255.\n        x=tf.expand_dims(x,axis=0)\n        eps=tf.random.normal((1,128))\n        sampling(x,E2,D2,eps=True)\n        if i==m:\n            break\n    break","0d1fbf00":"## Training Curve","6d9d3048":"![image.png](attachment:03d19ada-8dcc-4698-af32-f381a3643576.png)","4b3dc6ca":"#### Confusion difference between 2 models","bc8dae3d":"beta=0.1","2f5e4a54":"* I want to explore & visualize those clusters which have close distance \n\n   * What kind of emotions will overlap\n   * To prevent dimensional curse , I use features from LDA reduction","e27d6d86":"We can conclude that the penalty 1.25 is more indepedent since the mutual information between latent factors is small which means joint probability is close to multiplication of marginal probability ","362371c3":"KL-Divergence penalty not has significant effect on the classification result \n\nBut the interesiting thing is :\n\n  * Even if I'm human , I will fail to recognize some expression like angry & disgust , fear & sad \n  \n  * We can use clustering and measure the distance between clusters and visualize the high similarity clusters images","9a5eca31":"#### Beta = 0.1","34d8e249":"# Experiment ","a71f830b":"* Reduction\n\n\n   * Fit lda in training set \n   \n   * Transform testing set","bb1a5874":"beta=1.25","9264a761":"Features : \n\n * f1 : attention map for classification path\n\n * f2 : vae latent variable z\n\n * f3 : attention map for laten consistency path\n ","d1ad81cf":"We can see that \n\n  * If the kl-divergence penalty large,  the expression change will be more significant \n  \n  * Some faces fail to change that's the overlap part from the latent visualization part results \n  \n  * Epsilon changes some feature attributes and the emotion retained\n","fb07c4a2":"## Classification","4ae96000":"### Network Classifier","f750e264":"* Confusion Matrix","ba4ea823":"Thank you for the notebook ^^","6ec1c1db":"## Factors & Sampling","5ee32d61":"* Sampling function","2f735ebc":"![image.png](attachment:a9d732d1-19ff-4c17-a228-f1beedf81c9e.png)","3e343b1b":"* Get excitation plot ","54cf1647":"* Class map function","c8e9f602":"* Adding bottleneck layers in the classification head \n\n* Try to give more penalty & use total-correlation as objective\n\n* Using GAN mechanism to improve blurring problem \n\n\n* Some are change their original detail feature , I need to do more study on that case ","15ad21b9":"* Load tracks","46743dbb":"## Visualize latent features","a9bd5448":"beta=0.1","658b34d9":"* Here is loss function I use in training\n\n   * Given label loss \n   \n   $$L_c=\\sum_m{y_i(-\\log{\\hat{y_i}})}$$\n   \n   * Random label loss\n   \n   $$L_r=\\sum_m{y^{(r)}_i(-\\log{\\hat{y}^{(r)}_i})}$$\n   \n   * Latent reconstruction loss\n   \n   $$L_z=\\sum_m\\sum_d||z^{(l)}_{i,j} - z^{(r)}_{i,j}||$$\n   \n   * Reconstruction error\n   \n   $$L_{recon}=\\sum_m\\sum_{h,w,c}{||x_{i,j}-\\hat{x_{i,j}}||}$$\n   \n   * KL-divergence\n   \n   $$L_{kl}=\\sum_m{KL(q_\\phi(z_i|x_i)||p_\\theta(z_i))}$$\n   \n* Total loss\n\n$$L=L_c+L_r+L_z+L_{recon}+\\beta L_{kl}$$\n\n\n\n\nI train 2 models with differenct $\\beta$ penalty , the experiment will focus on 2 models on 4 perspective","76bc83d8":"beta=1.25","feb959b2":"* Happy & surprise have high similarity\n\n* Neutral is the expectation for all emotion  ","b701f7c4":"* Load models","ab4ade2d":"* This metrics is from the paper : [Beta-TCVAE](https:\/\/arxiv.org\/abs\/1802.04942)","4474b671":"### Attention ","c6377c04":"* Load dataset","5d076dec":"We can see that if give more penlaty on kl-divergence:\n\n 1. Harder to classify on random label \n \n 2. Easier to retain latent consistency\n \n 3. Harder to reconstruction (tradeoff by kl-divergence)\n \n 4. Distribution more close to N(0,I)","49c4a71c":"### Expectation ","cd3ff336":"## Clustering ","e3611450":"Result :\n\n * You can see that after reduction, latent variable z mix together, that means the emtion factor actually being captured by this model structure , the rest feature(z) almost don't have any emotion related factors, that explain why emotion of the generated image changed when adding label to latent variable z \n \n \n * After adding label as latent code,  some emotions have significantly \n \n * Neutral emtion is most overlapping emtion for all cases, even I will fail to recognize the emtion is neutral or others\n\n\nNote : We don't know which dimensions is emtion factor , but we can compare only latent z and fusion with label to check emtion factors \n","864091b3":"* Model training notebook here: https:\/\/www.kaggle.com\/cubetim2\/mefbhf","d91aec6e":"## Total Correlation ","a97e9b49":"# Functions","8df64d16":"# Training Algorithm ","e77b9462":"### Sampling from distribution","d022f054":"# Future work","87a889fa":" #### Beta=1.25","5dd24c13":"* Confusion Matrix"}}