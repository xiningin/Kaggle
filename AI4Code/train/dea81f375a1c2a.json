{"cell_type":{"9c7847de":"code","f647d5f6":"code","cf96a97a":"code","ed2dbf5b":"code","98b08e41":"code","ef0123c3":"code","99ad635a":"code","3355141f":"code","de8ae753":"code","3ccaff7b":"code","13c902cc":"code","daa5b0b5":"code","0cc0d5e9":"code","4af21ba0":"code","4b87c684":"code","3f244484":"code","fde48e89":"code","65fbaba5":"code","08736631":"code","0da2dea7":"code","e9146f67":"code","5e8bee8e":"code","8736f0b3":"code","00afc381":"code","27ab2d3e":"code","94edc629":"code","a96f5c65":"code","96281207":"code","3559a3dd":"code","cd8b79b9":"code","3e501906":"code","b2cb8814":"code","65eeeabd":"code","9a9ebc39":"code","a03ebc0f":"code","1f7900f6":"markdown","6d19774c":"markdown","c649edf3":"markdown","22eb0bf3":"markdown","0d5b921b":"markdown","16e12842":"markdown","fdd9c444":"markdown","1b603a13":"markdown","091b429a":"markdown","d1bc4fd9":"markdown","239d57c7":"markdown","c6c1a4ae":"markdown","b11e2368":"markdown","3a5855b2":"markdown","38977469":"markdown","df44b9d6":"markdown"},"source":{"9c7847de":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport gc\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nfrom tqdm.auto import tqdm\nimport collections\n\nimport os\n\nfrom pathlib import Path\n\nimport json\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\nsns.set_palette('Set3_r')\n\npd.set_option(\"display.max_rows\", 20, \"display.max_columns\", None)\n\nprint(os.listdir('..\/input\/'))\n        \nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))\n\nimport warnings\nwarnings.simplefilter(action = 'ignore', category = Warning)","f647d5f6":"train = pd.read_csv('\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')\nprint(train.shape)\ntrain.head()","cf96a97a":"test = pd.read_csv('\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\nprint(test.shape)\ntest.head()","ed2dbf5b":"sub = pd.read_csv('\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\/sample_submission.csv')\nprint(sub.shape)\nsub.head()","98b08e41":"external_hindi1 = pd.read_csv('\/kaggle\/input\/mlqa-hindi-processed\/mlqa_hindi.csv')\nprint(external_hindi1.shape)","ef0123c3":"external_hindi2 = pd.read_csv('\/kaggle\/input\/mlqa-hindi-processed\/xquad.csv')\nprint(external_hindi2.shape)","99ad635a":"print('External hindi dataset...')\nexternal_hindi = pd.concat([external_hindi1, external_hindi2])\nprint(external_hindi.shape)","3355141f":"print('External Tamil dataset...')\nexternal_tamil = pd.read_csv('\/kaggle\/input\/squad-translated-to-tamil-for-chaii\/squad_translated_tamil.csv')\nexternal_tamil['language'] = 'tamil'\nprint(external_tamil.shape)","de8ae753":"print('Combined External dataset...')\nexternal_df = pd.concat([external_hindi, external_tamil])\nexternal_df = external_df.sample(frac = 1).reset_index(drop = True)\nprint(external_df.shape)\nexternal_df.head()","3ccaff7b":"del external_hindi1, external_hindi2, external_hindi, external_tamil\ngc.collect()","13c902cc":"fig, (ax1, ax2) = plt.subplots(1, 2)\nsns.countplot(x = 'language', data = external_df, ax = ax1).set_title('External Dataset Language Counts')\nfor p in ax1.patches:\n    ax1.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\nsns.countplot(x = 'language', data = test, ax = ax2).set_title('Test Language Counts')\nfor p in ax2.patches:\n    ax2.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))","daa5b0b5":"import yaml\n\nhparams = {\n    'DEVICE': 'TPU',\n    'EPOCHS': 3,\n    'MODEL_2': '..\/input\/jplu-tf-xlm-roberta-large',\n    'N_FOLDS': 5,\n    'SEED': 777,\n    'VERBOSE': 1,\n    'BATCH_SIZE': 16,\n    'MAX_LENGTH': 512,\n    'DOC_STRIDE': 128\n    \n}","0cc0d5e9":"import tensorflow as tf\nimport tensorflow.keras.backend as K\n\nimport transformers\nfrom transformers import AutoTokenizer, TFXLMRobertaForQuestionAnswering, TFXLMRobertaModel\n\nprint(tf.__version__)\nprint(transformers.__version__)","4af21ba0":"SEED = hparams['SEED']\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","4b87c684":"model_checkpoint = hparams['MODEL_2']\nbatch_size = hparams['BATCH_SIZE']","3f244484":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_special_tokens = True)\nprint(tokenizer)","fde48e89":"train['context_num_tokens'] = train['context'].apply(lambda x: len(tokenizer(x)['input_ids']))\ntrain['context_num_tokens'].max()","65fbaba5":"train['context_num_tokens'].hist();","08736631":"train = train.sample(frac = 1, random_state = 2021).reset_index(drop = True)\nprint(train.shape)\ntrain.head()","0da2dea7":"#Split data to folds\nn_folds = hparams['N_FOLDS']\ntrain['kfold'] = -1\n\nskf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = SEED)\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X = train, y = train['language'].values)):\n    train.loc[val_idx, 'kfold'] = fold\ntrain.head(2)","e9146f67":"DEVICE = 'TPU'\n\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\n\nprint(f'REPLICAS: {REPLICAS}')","5e8bee8e":"max_length = hparams['MAX_LENGTH'] #The maximum length of a feature (question and context)\ndoc_stride = hparams['DOC_STRIDE'] #The authorized overlap between two part of the context when splitting it if needed.\n\npad_on_right = tokenizer.padding_side == \"right\"","8736f0b3":"def prepare_training(examples):\n    examples['question'] = [q.lstrip() for q in examples['question']] #remove leading white space\n    examples['question'] = [q.rstrip('?') for q in examples['question']] #remove '?' from the questions\n    \n    #Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    #in one example possible giving several features when a context is long, each of those features having a\n    #context that overlaps a bit the context of the previous feature.\n    \n    tokenized_examples = tokenizer(\n                list(examples['question' if pad_on_right else 'context'].values),\n                list(examples['context' if pad_on_right else 'question'].values),\n                truncation = 'only_second' if pad_on_right else 'only_first',\n                max_length = max_length,\n                stride = doc_stride,\n                return_overflowing_tokens = True,\n                return_offsets_mapping = True,\n                padding = 'max_length'\n            )\n    #Since one example might give us several features if it has a long context, we need a map from a feature to\n    #its corresponding example. This key gives us just that.\n    \n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    \n    #The offset mappings will give us a map from token to character position in the original context. This will\n    #help us compute the start_positions and end_positions.\n    \n    offset_mapping = tokenized_examples.pop('offset_mapping')\n    \n    tokenized_examples['start_positions'] = []\n    tokenized_examples['end_positions'] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples['input_ids'][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        \n        sequence_ids = tokenized_examples.sequence_ids(i)\n        \n        #One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples.loc[sample_index, 'answer_text']\n        start_char = examples.loc[sample_index, 'answer_start']\n        \n        # If no answers are given, set the cls_index as answer.\n        if start_char is None:\n            tokenized_examples['start_positions'].append(cls_index)\n            tokenized_examples['end_positions'].append(cls_index)\n        else:\n            # Start\/end character idx of the answer in the text.\n            end_char = start_char + len(answers)\n            \n             #Start token idx of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n            #Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                #Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                #Note: we could go after the last offset if the answer is the last word (edge case).\n                \n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples['start_positions'].append(token_start_index - 1)\n                \n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples['end_positions'].append(token_end_index + 1)\n\n    return tokenized_examples","00afc381":"def prepare_validation(examples):\n    examples['question'] = [q.lstrip() for q in examples['question']]\n    examples['question'] = [q.rstrip('?') for q in examples['question']]\n    \n    tokenized_examples = tokenizer(\n                list(examples['question' if pad_on_right else 'context'].values),\n                list(examples['context' if pad_on_right else 'question'].values),\n                truncation = 'only_second' if pad_on_right else 'only_first',\n                max_length = max_length,\n                stride = doc_stride,\n                return_overflowing_tokens = True,\n                return_offsets_mapping = True,\n                padding = 'max_length'\n            )\n    \n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    \n    #id column from the dataset\n    tokenized_examples['example_id'] = []\n\n    for i in range(len(tokenized_examples['input_ids'])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples['example_id'].append(examples.loc[sample_index, 'id'])\n        tokenized_examples['offset_mapping'][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples['offset_mapping'][i])\n        ]\n\n    return tokenized_examples","27ab2d3e":"def build_tf_dataset(df, batch_size = 4, flag = 'train'):\n    \n    if flag == 'train':\n        features = prepare_training(df)\n    else:\n        features = prepare_validation(df)\n    \n    input_ids = features['input_ids']\n    attn_masks = features['attention_mask']\n    \n    if flag == 'train':\n        start_positions = features['start_positions']\n        end_positions = features['end_positions']\n        \n        train_dataset = tf.data.Dataset.from_tensor_slices((input_ids, attn_masks, start_positions, end_positions))\n        train_dataset = train_dataset.map(lambda x1, x2, y1, y2: ({'input_ids': x1, 'attention_mask': x2}, {'start_positions': y1, 'end_positions': y2}))\n        train_dataset = train_dataset.batch(batch_size)\n        train_dataset = train_dataset.shuffle(1000)\n        train_dataset = train_dataset.prefetch(AUTO)\n        \n        return train_dataset, features\n    \n    elif flag == 'valid':\n        dataset = tf.data.Dataset.from_tensor_slices((input_ids, attn_masks))\n        dataset = dataset.map(lambda x1, x2: ({'input_ids': x1, 'attention_mask': x2}))\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.prefetch(buffer_size = AUTO)\n        \n        return dataset, features","94edc629":"def build_model():\n    roberta = TFXLMRobertaModel.from_pretrained(model_checkpoint)\n    \n    input_ids = tf.keras.layers.Input(shape = (max_length, ), name = 'input_ids', dtype = tf.int32)\n    attention_mask = tf.keras.layers.Input(shape = (max_length, ), name = 'attention_mask', dtype = tf.int32)\n    \n    embeddings = roberta(input_ids = input_ids, attention_mask = attention_mask)[0]\n    \n    x1 = tf.keras.layers.Dropout(0.1)(embeddings) \n    x1 = tf.keras.layers.Dense(1, use_bias = False)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax', name = 'start_positions')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(embeddings) \n    x2 = tf.keras.layers.Dense(1, use_bias = False)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax', name = 'end_positions')(x2)\n\n    model = tf.keras.models.Model(inputs = [input_ids, attention_mask], outputs = [x1, x2])\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-5)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False)\n    \n    model.compile(loss = [loss, loss], optimizer = optimizer)\n\n    return model","a96f5c65":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","96281207":"def post_process_predictions(examples, features, start, end, n_best_size = 20, max_answer_length = 30):\n    \n    all_start_logits, all_end_logits = start, end\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    \n    for i, feature in enumerate(features['example_id']):\n        features_per_example[example_id_to_index[feature]].append(i)\n\n    # The dictionaries we have to fill.\n    predictions = collections.OrderedDict()\n\n    # Logging.\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features['input_ids'])} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in examples.iterrows():\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n        min_null_score = None # Only used if squad_v2 is True.\n        valid_answers = []\n        \n        context = example['context']\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features['offset_mapping'][feature_index]\n\n            # Update minimum null prediction.\n            cls_index = features['input_ids'][feature_index].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key = lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n            # failure.\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n        #if not squad_v2:\n        #    predictions[example[\"id\"]] = best_answer[\"text\"]\n        #else:\n        answer = best_answer[\"text\"] \n        predictions[example['id']] = answer\n\n    return predictions","3559a3dd":"strart_probs, end_probs = [], []\nepochs = hparams['EPOCHS']\njaccard_scores = []\n\nfor i, fold in enumerate(range(n_folds)):\n    print('#########' * 15)\n    print(f\"Fold: {fold + 1}\")\n    print('#########' * 15)\n    train_df = train[train['kfold'] != fold]\n    valid_df = train[train['kfold'] == fold]\n    \n    #concat external_df to train_df for training, no change in valid_df\n    train_df = pd.concat([train_df.iloc[:, 1:-1], external_df])\n\n    train_df = train_df.reset_index(drop = True)\n    valid_df = valid_df.reset_index(drop = True)\n    print(train_df.shape, valid_df.shape)\n    \n    train_dataset, train_enc = build_tf_dataset(train_df, batch_size = batch_size, flag = 'train')\n    valid_dataset, valid_enc = build_tf_dataset(valid_df, batch_size = batch_size, flag = 'valid')\n    \n    K.clear_session()\n    \n    with strategy.scope():\n        model = build_model()\n    if i == 0:\n        print(model.summary())\n    \n    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'qa_model_{fold + 1}.h5', verbose = 1, monitor = 'loss', mode = 'min', save_best_only = True, \n                                                save_weights_only = True)\n\n    history = model.fit(train_dataset, \n                        epochs = epochs, \n                        batch_size = batch_size,\n                        callbacks = [checkpoint],\n                        verbose = 1\n                        )\n    print('Predicting valid dataset...')\n    start_pred, end_pred = model.predict(valid_dataset, batch_size = batch_size, verbose = 1)\n    print(start_pred.shape, end_pred.shape)\n    print('Post-process predictions...')\n    valid_preds = post_process_predictions(valid_df, valid_enc, start_pred, end_pred)\n    \n    score = []\n    for idx in range(len(valid_df)):\n        str1 = valid_df['answer_text'].values[idx]\n        str2 = valid_preds[valid_df.loc[idx, 'id']]\n        score.append(jaccard(str1, str2))\n    print(f'Jaccard Score for fold {fold + 1}: {np.mean(score)}')\n    jaccard_scores.append(np.mean(score))\n    \n    strart_probs.append(start_pred)\n    end_probs.append(end_pred)\n    \n    del train_dataset, valid_dataset, model\n    gc.collect()","cd8b79b9":"hparams['JAC_SCORES'] = jaccard_scores\n\nwith open(r'hparams.yaml', 'w') as f:\n    yaml.dump(hparams, f)","3e501906":"test_dataset, test_enc = build_tf_dataset(test, batch_size = batch_size, flag = 'valid')","b2cb8814":"start_probs, end_probs = [], []\nfor fold in range(n_folds):\n    with strategy.scope():\n        model = build_model()\n    print('Loading trained model weights...')\n    model.load_weights(f'qa_model_{fold + 1}.h5')\n    print(f'Predicting testset - Fold: {fold + 1}...')\n    start_pred, end_pred = model.predict(test_dataset, batch_size = batch_size, verbose = 1)\n    print(start_pred.shape, end_pred.shape)\n    start_probs.append(start_pred)\n    end_probs.append(end_pred)","65eeeabd":"test_start_probs, test_end_probs = np.mean(start_probs, axis = 0), np.mean(end_probs, axis = 0)\npredictions = post_process_predictions(test, test_enc, test_start_probs, test_end_probs)","9a9ebc39":"sub_df = pd.DataFrame({'id': list(predictions.keys()), 'PredictionString': list(predictions.values())})\nsub_df.to_csv('.\/submission.csv', index = False)\nsub_df.head()","a03ebc0f":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","1f7900f6":"# Competition Metrics:\nThe metric in this competition is the word-level Jaccard score\n\n`def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))`","6d19774c":"Save the hyperparameters and the validation scores for prediction notebook","c649edf3":"# Metrics","22eb0bf3":"# Post Process Predictions","0d5b921b":"# Predict on Test Data","16e12842":"# Set TPU","fdd9c444":"# TF XLM RoBerta Model","1b603a13":"### Ref:\n- https:\/\/keras.io\/examples\/nlp\/text_extraction_with_bert\/\n- https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb","091b429a":"- The context length is too long, it has to be split into pieces before processing\n- Usually in NLP tasks long documents are truncated but in QA tasks truncating 'context' would lead to loss of answer\n- To avoid this, the long context is split into many input features each of length less than the max_length parameter\n- And if answer is at the split, we use overlapping of split features which is controlled by the parameter doc_stride","d1bc4fd9":"### External Datasets\n- This Notebook uses @rhtsingh's hindi dataset: [Hindi External](https:\/\/www.kaggle.com\/rhtsingh\/external-data-mlqa-xquad-preprocessing\/data)\n- For Tamil, the dataset i have created: [Tamil External](https:\/\/www.kaggle.com\/msafi04\/squad-translated-to-tamil-for-chaii)","239d57c7":"# Competition Overview:\n\nIn this competition, the goal is to predict answers to real questions about Wikipedia articles. You will use chaii-1, a new question answering dataset with question-answer pairs. The dataset covers Hindi and Tamil, collected without the use of translation. It provides a realistic information-seeking task with questions written by native-speaking expert data annotators. ","c6c1a4ae":"\n# Competition Rules:\n- CPU Notebook <= 5 hours run-time\n-GPU Notebook <= 5 hours run-time\n-Internet access disabled\n- Freely & publicly available external data is allowed, including pre-trained models\n- Submission file must be named submission.csv","b11e2368":"Thanks to Kaggle and fellow Kagglers for the all the learnings, nothing beats doing and learning!!!","3a5855b2":"# Brief Introduction\n\n### Tamil\n\n- Tamil is a Dravidian language spoken by Tamils in southern India, Sri Lanka, and elsewhere\n- Tamil language originated from Proto-Dravidian in 450BCE\n- Tamil language is derived from the Dravidian language family written in Tamil scripts. It is one of the four Dravidian languages along with Telegu, Malayalam, and Kannada\n- It is the oldest of all Dravidian languages\n- Tamil language witnesses it\u2019s existence for more than 2000 years making it the oldest and longest surviving classical language in the world\n- The Tamil language is spoken widely in India, Sri Lanka, Malaysia, Singapore, South Africa and Mauritius\n\n### Hindi\n\n- Hindi is an Indic language of northern India that derived from Vedic Sanskrit language\n- Hindi is written in the Devanagari script\n- Hindi language originated from the Indo-Aryans linguistic Family in the 17th century CE\n- It is one of the official languages of India which includes Tamil as well","38977469":"# Fine Tune Model","df44b9d6":"# Huggingface TF XLM RoBerta"}}