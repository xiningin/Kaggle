{"cell_type":{"f4720826":"code","e850edbf":"code","07cb0610":"code","fe0a8f88":"code","9184654d":"code","5c0cabad":"code","f3f2bd34":"code","328eed0b":"code","489cb6fc":"code","1126b60a":"code","64e12d04":"code","2bedabaa":"code","728bf891":"code","0deab1cf":"code","471861e5":"code","26088f65":"code","273ed9d6":"code","e19536bd":"code","9a3eed57":"code","5e2d433d":"code","6e03c655":"code","8be2e1c4":"code","0f43d7b9":"code","64937259":"code","596bd362":"code","fef7287f":"code","529642b0":"code","8ef8676b":"code","d6246c21":"code","b972577b":"code","74e755cd":"code","380e9584":"code","5f15035c":"code","ff606874":"code","3bf43a6a":"code","1bcef981":"code","d32f1fe6":"code","f6d22a4f":"code","aff72a76":"code","378c1057":"code","d4f0fdc1":"code","a2744ebc":"code","158ad090":"code","7c3cb2b8":"code","d7f422c9":"code","bf872cb2":"code","93f5d0f9":"code","2906f4f4":"code","9e7bc3a0":"code","4a74447c":"code","5db819ae":"code","cdfa72a8":"code","7a57241c":"code","64d9bb39":"code","72545101":"code","5bd38181":"code","ef09b67f":"code","cfbf900b":"code","6b7e2309":"code","601a3bb5":"code","29251f20":"code","0db57bb6":"code","c1a8f796":"markdown","34b8ac87":"markdown","da4d5dd2":"markdown","6e49dde6":"markdown","f9915403":"markdown","b1fed38c":"markdown","5fff3e4f":"markdown","0a268065":"markdown","ba623a7d":"markdown","b37df091":"markdown","62918d19":"markdown","85e0b0fd":"markdown","001607c6":"markdown","4f57eff5":"markdown","c06b76d9":"markdown","b740121f":"markdown","3a4a1c77":"markdown","a021cfbc":"markdown","8d4e5b35":"markdown","282bb3fd":"markdown","7678d6a6":"markdown","6b482d4f":"markdown","2c4b21ee":"markdown","36c2572c":"markdown","459cbdff":"markdown","aedb649c":"markdown","407c73ed":"markdown","5f3f9a5f":"markdown","0e34668e":"markdown","5ab67aac":"markdown","f13ba9cc":"markdown","7eaed8e0":"markdown","61941bc4":"markdown","b122d65a":"markdown","9a628ae7":"markdown","753b945b":"markdown","5dce4343":"markdown","6dc1c83a":"markdown","b3d8c647":"markdown","2ddf4632":"markdown","31689773":"markdown","3494fba0":"markdown","19c179f4":"markdown","afe21b7a":"markdown","bf98a988":"markdown","e5db1c10":"markdown","824623b3":"markdown","96bd22a6":"markdown","26595ae9":"markdown","2a6f8a23":"markdown","2943d535":"markdown","cd721737":"markdown","1fcdfa23":"markdown","4e7a5680":"markdown","bdcb9a89":"markdown","629f67ac":"markdown","a5a286fc":"markdown","b140adce":"markdown","5788b216":"markdown"},"source":{"f4720826":"# import pandas as pd\n# from pandas_profiling import ProfileReport\n# from pandasql import sqldf\n# pysqldf = lambda q: sqldf(q, globals())\n# pd.set_option('display.max_rows', None)\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom textblob import TextBlob, Word, Blobber\n\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.svm import LinearSVC\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import f1_score, confusion_matrix\n# from sklearn.metrics import classification_report\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\nfrom pandasql import sqldf\npysqldf = lambda q: sqldf(q, globals())\npd.set_option('display.max_rows', None)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\n# Set notebook mode to work in offline\npyo.init_notebook_mode()\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom plotly.offline import iplot\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","e850edbf":"df = pd.read_csv(\"..\/input\/fake-news-dataset\/train.csv\")","07cb0610":"# profile = ProfileReport(df, title='Pandas Profiling Report', explorative=True)\n# profile.to_notebook_iframe()\ndf.head()","fe0a8f88":"# List all columms\ndf.columns","9184654d":"df.isnull().sum()","5c0cabad":"# deleted the records which has null value. totoal 2.5% records\ndf.dropna(inplace=True)\ndf.label.count()","f3f2bd34":"del df['id']","328eed0b":"df['text_len'] = df['text'].astype(str).apply(len)\ndf['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n","489cb6fc":"df['title_len'] = df['title'].astype(str).apply(len)\ndf['title_word_count'] = df['title'].apply(lambda x: len(str(x).split()))\n","1126b60a":"df['title_word_count'].head()","64e12d04":"fakenews = df[df[\"label\"] == 1]\nrealnews = df[df[\"label\"] == 0]","2bedabaa":"# Distrubution of text length \ndf['text_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    yTitle='Number Of Records',\n    title='Text Length Distribution of \"text\" feature')","728bf891":"# Distrubution of word count \ndf['word_count'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='Number Of Records',\n    linecolor='black',\n    yTitle='count',\n    title='Word Count Distribution of \"text\" feature')","0deab1cf":"# Distrubution of word count \ndf['title_word_count'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='Number Of Records',\n    linecolor='black',\n    yTitle='count',\n    title='Word Count Distribution of \"text\" feature')","471861e5":"df.groupby('label').count()['word_count'].iplot(kind='bar', yTitle='Count', linecolor='black', opacity=0.8,\n                                                           title='Bar chart of label and word count', xTitle='label')","26088f65":"# Function to calcualte unigram, bigram and trigram\ndef get_top_n_gram_words(corpus, n=None,gram=1):\n    vec = CountVectorizer(ngram_range=(gram, gram)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","273ed9d6":"# fakenews unigram : \ncommon_words = get_top_n_gram_words(fakenews['text'], 20)\n# for word, freq in common_words:\n#     print(word, freq)\ndf1 = pd.DataFrame(common_words, columns = ['commonTextUnigram' , 'count'])\ndf1.groupby('commonTextUnigram').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in text before removing stop words')","e19536bd":"# fakenews bigram\ncommon_words = get_top_n_gram_words(fakenews['text'], 20,2)\n# for word, freq in common_words:\n#     print(word, freq)\ndf2 = pd.DataFrame(common_words, columns = ['commonTextbigram' , 'count'])\ndf2.groupby('commonTextbigram').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in text before removing stop words')","9a3eed57":"# fakenews Trigram\ncommon_words = get_top_n_gram_words(fakenews['text'], 20,3)\n# for word, freq in common_words:\n#     print(word, freq)\ndf3 = pd.DataFrame(common_words, columns = ['commonTextTrigram' , 'count'])\ndf3.groupby('commonTextTrigram').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 trigrams in text before removing stop words')","5e2d433d":"# realnews unigram\ncommon_words = get_top_n_gram_words(realnews['text'], 20)\n# for word, freq in common_words:\n#     print(word, freq)\ndf4 = pd.DataFrame(common_words, columns = ['commonTextUnigram' , 'count'])\ndf4.groupby('commonTextUnigram').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in text before removing stop words')","6e03c655":"# realnews trigram\ncommon_words = get_top_n_gram_words(realnews['text'], 20,2)\n# for word, freq in common_words:\n#     print(word, freq)\ndf6 = pd.DataFrame(common_words, columns = ['commonTextUnigram' , 'count'])\ndf6.groupby('commonTextUnigram').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in text before removing stop words')","8be2e1c4":"# realnews trigram\ncommon_words = get_top_n_gram_words(realnews['text'], 20,3)\n# for word, freq in common_words:\n#     print(word, freq)\ndf6 = pd.DataFrame(common_words, columns = ['commonTextUnigram' , 'count'])\ndf6.groupby('commonTextUnigram').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in text before removing stop words')","0f43d7b9":"# After removing stopwords \nimport string\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = list(stopwords.words('english'))\nfrom nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer()  ","64937259":"def processText(df):\n    by_article_list=[]\n    for article in (df[\"text\"]):\n        words = word_tokenize(article)\n        words = [word.lower() for word in words if word.isalpha()] #lowercase\n        words = [word for word in words if word not in string.punctuation and word not in stop_words] #punctuation, stopwords\n        words = [lemmatizer.lemmatize(word) for word in words] #convert word to root form\n        by_article_list.append(' '.join(words))\n    return by_article_list\n\ndef processTitle(df):\n    by_article_list=[]\n    for article in (df[\"title\"]):\n        words = word_tokenize(article)\n        words = [word.lower() for word in words if word.isalpha()] #lowercase\n        words = [word for word in words if word not in string.punctuation and word not in stop_words] #punctuation, stopwords\n        words = [lemmatizer.lemmatize(word) for word in words] #convert word to root form\n        by_article_list.append(' '.join(words))\n    return by_article_list","596bd362":"\nfakenews['clean_text'] =processText(fakenews)","fef7287f":"realnews['clean_text'] =processText(realnews)","529642b0":"df['clean_text'] =processText(df)","8ef8676b":"df['clean_title'] =processTitle(df)","d6246c21":"df['clean_text_len'] = df['clean_text'].astype(str).apply(len)\ndf['clean_text_word_count'] = df['clean_text'].apply(lambda x: len(str(x).split()))\ndf['clean_text_title_len'] = df['clean_title'].astype(str).apply(len)\ndf['clean_text_title_word_count'] = df['clean_title'].apply(lambda x: len(str(x).split()))","b972577b":"# Distrubution of word count \ndf['clean_text_title_word_count'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='Number Of Records',\n    linecolor='black',\n    yTitle='count',\n    title='Word Count Distribution of \"text\" feature')","74e755cd":"# Distrubution of word count \ndf['clean_text_word_count'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='Number Of Records',\n    linecolor='black',\n    yTitle='count',\n    title='Word Count Distribution of \"text\" feature')","380e9584":"# fakenews unigram\ncommon_words = get_top_n_gram_words(fakenews['clean_text'], 20)\n# for word, freq in common_words:\n#     print(word, freq)\ndf7 = pd.DataFrame(common_words, columns = ['commonTextUnigram' , 'count'])\ndf7.groupby('commonTextUnigram').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in text before removing stop words')","5f15035c":"# fakenews biigram\ncommon_words = get_top_n_gram_words(fakenews['clean_text'], 20,2)\n# for word, freq in common_words:\n#     print(word, freq)\ndf7 = pd.DataFrame(common_words, columns = ['commonTextUnigram' , 'count'])\ndf7.groupby('commonTextUnigram').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in text before removing stop words')","ff606874":"# fakenews triigram\ncommon_words = get_top_n_gram_words(fakenews['clean_text'], 20,3)\n# for word, freq in common_words:\n#     print(word, freq)\ndf7 = pd.DataFrame(common_words, columns = ['commonTextUnigram' , 'count'])\ndf7.groupby('commonTextUnigram').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in text before removing stop words')","3bf43a6a":"# realnews unigram\ncommon_words = get_top_n_gram_words(realnews['clean_text'], 20)\n# for word, freq in common_words:\n#     print(word, freq)\ndf7 = pd.DataFrame(common_words, columns = ['commonTextUnigram' , 'count'])\ndf7.groupby('commonTextUnigram').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in text before removing stop words')","1bcef981":"# realnews bigram\ncommon_words = get_top_n_gram_words(realnews['clean_text'], 20,2)\n# for word, freq in common_words:\n#     print(word, freq)\ndf8 = pd.DataFrame(common_words, columns = ['commonTextUnigram' , 'count'])\ndf8.groupby('commonTextUnigram').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in text before removing stop words')","d32f1fe6":"# realnews trigram\ncommon_words = get_top_n_gram_words(realnews['clean_text'], 20,3)\n# for word, freq in common_words:\n#     print(word, freq)\ndf9 = pd.DataFrame(common_words, columns = ['commonTextUnigram' , 'count'])\ndf9.groupby('commonTextUnigram').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in text before removing stop words')","f6d22a4f":"# The distribution of top part-of-speech tags of text corpus\nblob = TextBlob(str(fakenews['text']))\npos_df = pd.DataFrame(blob.tags, columns = ['word' , 'pos'])\npos_df = pos_df.pos.value_counts()[:20]\npos_df.iplot(\n    kind='bar',\n    xTitle='POS',\n    yTitle='count', \n    title='Top 20 Part-of-speech tagging for text corpus')","aff72a76":"# The distribution of top part-of-speech tags of text corpus\nblob = TextBlob(str(realnews['text']))\npos_df = pd.DataFrame(blob.tags, columns = ['word' , 'pos'])\npos_df = pos_df.pos.value_counts()[:20]\npos_df.iplot(\n    kind='bar',\n    xTitle='POS',\n    yTitle='count', \n    title='Top 20 Part-of-speech tagging for text corpus')","378c1057":"from plotly.offline import iplot\ny0 = df.loc[df['label'] == 1]['text_len']\ny1 = df.loc[df['label'] == 0]['text_len']\n\n\n\ntrace0 = go.Box(\n    y=y0,\n    name = 'fake',\n    marker = dict(\n        color = 'rgb(214, 12, 140)',\n    )\n)\ntrace1 = go.Box(\n    y=y1,\n    name = 'real',\n    marker = dict(\n        color = 'rgb(0, 128, 128)',\n    )\n)\n\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title = \"Text length Boxplot of label Name\"\n)\n\nfig = go.Figure(data=data,layout=layout)\niplot(fig, filename = \"text Length Boxplot of label Name\")","d4f0fdc1":"from plotly.offline import iplot\ny0 = df.loc[df['label'] == 1]['word_count']\ny1 = df.loc[df['label'] == 0]['word_count']\n\n\ntrace0 = go.Box(\n    y=y0,\n    name = 'fake',\n    marker = dict(\n        color = 'rgb(214, 12, 140)',\n    )\n)\ntrace1 = go.Box(\n    y=y1,\n    name = 'real',\n    marker = dict(\n        color = 'rgb(0, 128, 128)',\n    )\n)\n\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title = \"word count Boxplot of label Name\"\n)\n\nfig = go.Figure(data=data,layout=layout)\niplot(fig, filename = \"word count Boxplot of label Name\")","a2744ebc":"from plotly.offline import iplot\ny0 = df.loc[df['label'] == 1]['clean_text_word_count']\ny1 = df.loc[df['label'] == 0]['clean_text_word_count']\n\n\ntrace0 = go.Box(\n    y=y0,\n    name = 'fake',\n    marker = dict(\n        color = 'rgb(214, 12, 140)',\n    )\n)\ntrace1 = go.Box(\n    y=y1,\n    name = 'real',\n    marker = dict(\n        color = 'rgb(0, 128, 128)',\n    )\n)\n\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title = \"word count Boxplot of label Name\"\n)\n\nfig = go.Figure(data=data,layout=layout)\niplot(fig, filename = \"word count Boxplot of label Name\")","158ad090":"from plotly.offline import iplot\ny0 = df.loc[df['label'] == 1]['title_len']\ny1 = df.loc[df['label'] == 0]['title_len']\n\n\ntrace0 = go.Box(\n    y=y0,\n    name = 'fake',\n    marker = dict(\n        color = 'rgb(214, 12, 140)',\n    )\n)\ntrace1 = go.Box(\n    y=y1,\n    name = 'real',\n    marker = dict(\n        color = 'rgb(0, 128, 128)',\n    )\n)\n\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title = \"Title length Boxplot of label Name\"\n)\n\nfig = go.Figure(data=data,layout=layout)\niplot(fig, filename = \"Title Length Boxplot of label Name\")","7c3cb2b8":"from plotly.offline import iplot\ny0 = df.loc[df['label'] == 1]['title_word_count']\ny1 = df.loc[df['label'] == 0]['title_word_count']\n\n\ntrace0 = go.Box(\n    y=y0,\n    name = 'fake',\n    marker = dict(\n        color = 'rgb(214, 12, 140)',\n    )\n)\ntrace1 = go.Box(\n    y=y1,\n    name = 'real',\n    marker = dict(\n        color = 'rgb(0, 128, 128)',\n    )\n)\n\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title = \"Title word Count Boxplot of label Name\"\n)\n\nfig = go.Figure(data=data,layout=layout)\niplot(fig, filename = \"Title Word Count Boxplot of label Name\")","d7f422c9":"def extractWords(df):\n    text_words = '' \n    for val in df.clean_text: \n        # typecaste each val to string \n        val = str(val) \n        # split the value \n        tokens = val.split() \n        # Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower() \n        text_words += \" \".join(tokens)+\" \"\n    return text_words","bf872cb2":"# worldcloud\ntext_words = extractWords(fakenews)\nplt.subplots(figsize = (8,8))\n\nwordcloud = WordCloud (\n                    background_color = 'white',\n                    width = 600,\n                    height = 400\n                        ).generate(text_words)\nplt.imshow(wordcloud) # image show\nplt.axis('off') # to off the axis of x and y\nplt.tight_layout(pad=0)\nplt.show()","93f5d0f9":"# worldcloud\ntext_words = extractWords(realnews)\nplt.subplots(figsize = (8,8))\n\nwordcloud = WordCloud (\n                    background_color = 'white',\n                    width = 600,\n                    height = 400\n                        ).generate(text_words)\nplt.imshow(wordcloud) # image show\nplt.axis('off') # to off the axis of x and y\nplt.tight_layout(pad=0)\nplt.show()","2906f4f4":"# find out number of words in title \ndf['temp'] = df['text'].apply(lambda x: len(str(x).split(\" \")))\ndf[['temp','text']].head()\n","9e7bc3a0":"# find out the records which has only one words\nq= \"select count(temp) from df where  temp ==1 and text is not null\"\njoined = pysqldf(q)\njoined","4a74447c":"# find out the records which has more than 10 words\nq= \"select count(temp) from df where  temp <10 and text is not null\"\njoined = pysqldf(q)\njoined","5db819ae":"# records which has duplicate values\nq= \"select sum(cnt) - count(cnt) from (select count(label) as cnt from df where text is not null group by text having count(label)>1 ) \"\njoined = pysqldf(q)\njoined","cdfa72a8":"# find out number of words in title \ndf['temp_word_title'] = df['title'].apply(lambda x: len(str(x).split(\" \")))\ndf[['temp_word_title','title']].head()\n","7a57241c":"# max and min records on the words\nq= \"select max(temp_word_title),min(temp_word_title) from df where title is not null\"\njoined = pysqldf(q)\njoined.head(1000)","64d9bb39":"# words and lable comparision\nq=  \"select label, case when temp_word_title <= 10 then '<=10' when temp_word_title > 10 and temp_word_title <= 20 then '10-20' when temp_word_title > 20 and temp_word_title <= 30 then '20-30' when temp_word_title > 30 and temp_word_title <= 40 then '30-40' when temp_word_title > 40 and temp_word_title <= 50 then '40-50' when temp_word_title > 50 and temp_word_title <= 60 then '50-60' when temp_word_title > 60 and temp_word_title <= 70 then '60-70' when temp_word_title > 70 and temp_word_title <= 80 then '70-80' else 0 end as cnt from df where title is not null\"\njoined = pysqldf(q)\njoined.head(10)\npd.value_counts(joined['cnt']).plot(kind=\"bar\")","72545101":"# categorized data on sentence size \nq=  \"select label, case when length(text) <= 1000 then '<=1000' when length(text) > 1000 and length(text) <=  2000 then  '1k-2k' when length(text) > 2000 and length(text) <=  3000 then  '2k-3k ' when length(text) > 3000 and length(text) <=  4000 then  '3k-4k ' when length(text) > 4000 and length(text) <=  5000 then  '4k-5k' when length(text) > 5000 and length(text) <=  6000 then  '5k-6k' when length(text) > 6000 and length(text) <=  7000 then  '6k-7k' when length(text) > 7000 and length(text) <=  8000 then  '7k-8k' when length(text) > 8000 and length(text) <=  9000 then  '8k-9k' when length(text) > 9000 and length(text) <=  10000 then  '9k-10k ' when length(text) > 10000 and length(text) <= 11000 then '10k-11k' when length(text) > 11000 and length(text) <= 12000 then '11k-12k' when length(text) > 12000 and length(text) <= 13000 then '12k-13k' when length(text) > 13000 and length(text) <= 14000 then '13k-14k' when length(text) > 14000 and length(text) <= 15000 then '14k-15k' when length(text) > 15000 and length(text) <= 16000 then '15k-16k'  else '>16k' end as cnt from df where text is not null\"\njoined = pysqldf(q)\njoined.head()","5bd38181":"# words and lable comparision\nplt.figure(figsize=(16, 6))\nsns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"cnt\", hue=\"label\", data=joined)\n        ","ef09b67f":"x1 = df.loc[df['label'] == 1, 'text_len']\nx0 = df.loc[df['label'] == 0, 'text_len']\n\ntrace1 = go.Histogram(\n    x=x0, name='Real News',\n    opacity=0.75\n)\ntrace2 = go.Histogram(\n    x=x1, name = 'Fake News',\n    opacity=0.75\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(barmode='overlay', title='Distribution of text length of text based on label')\nfig = go.Figure(data=data, layout=layout)\n\niplot(fig, filename='overlaid histogram')","cfbf900b":"x1 = df.loc[df['label'] == 1, 'title_len']\nx0 = df.loc[df['label'] == 0, 'title_len']\n\ntrace1 = go.Histogram(\n    x=x0, name='Real News',\n    opacity=0.75\n)\ntrace2 = go.Histogram(\n    x=x1, name = 'Fake News',\n    opacity=0.75\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(barmode='overlay', title='Distribution of text length of text based on label')\nfig = go.Figure(data=data, layout=layout)\n\niplot(fig, filename='overlaid histogram')","6b7e2309":"x1 = df.loc[df['label'] == 1, 'author']\nx0 = df.loc[df['label'] == 0, 'author']\n\ntrace1 = go.Histogram(\n    x=x0, name='Fake News',\n    opacity=0.75\n)\ntrace2 = go.Histogram(\n    x=x1, name = 'Real News',\n    opacity=0.75\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(barmode='overlay', title='Distribution of text length of text based on label')\nfig = go.Figure(data=data, layout=layout)\n\niplot(fig, filename='overlaid histogram')","601a3bb5":"# categorized data on sentence size \nq=  \"select label, case when length(title) <= 50 then '<=50' when length(title) > 50 and length(title) <= 100 then '50-100' when length(title) > 100 and length(title) <= 150 then '100-150' when length(title) > 150 and length(title) <= 200 then '150-200' when length(title) > 200 and length(title) <= 250 then '200-250' when length(title) > 250 and length(title) <= 300 then '250-300' when length(title) > 300 and length(title) <= 350 then '300-350' when length(title) > 350 and length(title) <= 400 then '350-400' when length(title) > 400 and length(title) <= 450 then '400-450'  when length(title) > 450  then '>450' else 0 end as cnt from df where title is not null\"\njoined = pysqldf(q)\n# words and lable comparision\n\nsns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"cnt\", hue=\"label\", data=joined)\n        ","29251f20":"import matplotlib.pyplot as plt \nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pickle\nfrom xgboost import XGBRegressor\nfrom nltk.corpus import stopwords\nimport nltk\nfrom sklearn_pandas import DataFrameMapper\nimport xgboost as xgb\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport logging\nimport config\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom nltk import word_tokenize\n\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = list(stopwords.words('english'))\nfrom nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer()  \n\nlogging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n\nclass FeatureCreation():\n    def __init__(self):\n        pass\n\n    def processText(self,df):\n        by_article_list=[]\n        for article in (df[\"text\"]):\n            words = word_tokenize(article)\n            # words = [word.lower() for word in words if word.isalpha()] #lowercase\n            words = [word for word in words if word not in string.punctuation and word not in stop_words] #punctuation, stopwords\n            words = [lemmatizer.lemmatize(word) for word in words] #convert word to root form\n            by_article_list.append(' '.join(words))\n        return by_article_list\n\n        \n\n    def generate_features(self, data):\n       \n        logging.info(\"Generating Features\")\n\n        def get_word_count(column, analyzer):\n            \"\"\"\n            Calculate the frequently used words from the entire bag of words using Count Vectorizer\n            \"\"\"\n            count_vect = CountVectorizer(analyzer=analyzer)\n            bag_of_words = count_vect.fit_transform(data[column])\n            sum_of_words = bag_of_words.sum(axis=0)\n            words_freq = [(word, sum_of_words[0, i])\n                          for word, i in count_vect.vocabulary_.items()]\n            df = pd.DataFrame(words_freq, columns=['word', 'count'])\n            return df\n\n        def get_acronym_count(text):\n            \"\"\"\n            Return the count of all the uppercase words\n            Based on the assumption that if the word is in uppercase, it will be an Acronym.\n            \"\"\"\n\n            text = str(text)\n            count = 0\n            for word in text.split():\n                if word.isupper():\n                    count += 1\n            return count\n\n       \n\n        def get_numerical_value_count(text):\n            \"\"\"\n            Return the count of occurences of numerical values in the text\n            \"\"\"\n\n            text = str(text)\n            return len([s for s in text.split() if s.isnumeric()])\n\n        def get_pos_tagging(s):\n\n\n            text = s['clean_text']\n            VERB_count = 0\n            NOUN_count = 0\n            PRON_count = 0\n            ADJ_count = 0\n            ADVPN_count = 0\n            ADP_count = 0\n            CONJ_count = 0\n            DET_count = 0\n            NUM_count = 0\n            PRT_count = 0\n            PUNC_count = 0\n            Action_count = 0\n            \n\n            pos_taggs = nltk.pos_tag(nltk.word_tokenize(text))\n\n            for index, pos in enumerate(pos_taggs):\n                if 'VB' in pos[1]:\n                    VERB_count = VERB_count + 1\n\n                elif 'NN' in pos[1]:\n                    NOUN_count = NOUN_count + 1\n\n                    if ((index - 1) > 0 and 'VB' in pos_taggs[index - 1][1]) or (\n                            (index - 2) > 0 and 'VB' in pos_taggs[index - 2][1]) or (\n                            (index + 1) < len(pos_taggs) and 'VB' in pos_taggs[index + 1][1]) or (\n                            (index + 2) < len(pos_taggs) and 'VB' in pos_taggs[index + 2][1]):\n                        Action_count = Action_count + 1                    \n\n                elif pos[1] == 'WP':\n                    PRON_count = PRON_count + 1\n                elif pos[1] in ['JJ', 'JJR', 'JJS']:\n                    ADJ_count = ADJ_count + 1\n                elif pos[1] in ['ADV', 'RB', 'RBR', 'RBS']:\n                    ADVPN_count = ADVPN_count + 1\n                elif pos[1] == 'ADP':\n                    ADP_count = ADP_count + 1\n                elif pos[1] == 'CC ':\n                    CONJ_count = CONJ_count + 1\n                elif pos[1] == 'WDT':\n                    DET_count = DET_count + 1\n                elif pos[1] == 'CD':\n                    NUM_count = NUM_count + 1\n                elif pos[1] == 'RP':\n                    PRT_count = PRT_count + 1\n                elif pos[1] in ['.', ':', '']:\n                    PUNC_count = PUNC_count + 1\n\n            if len(pos_taggs) > 0:\n                VERBRatio = VERB_count \/ len(pos_taggs)\n                NOUNRatio = NOUN_count \/ len(pos_taggs)\n                PRONRatio = PRON_count \/ len(pos_taggs)\n                ADJRatio = ADJ_count \/ len(pos_taggs)\n                ADVPNRatio = ADVPN_count \/ len(pos_taggs)\n                ADPRatio = ADP_count \/ len(pos_taggs)\n                CONJRatio = CONJ_count \/ len(pos_taggs)\n                DETRatio = DET_count \/ len(pos_taggs)\n                NUMRatio = NUM_count \/ len(pos_taggs)\n                PRTRatio = PRT_count \/ len(pos_taggs)\n                PUNCRatio = PUNC_count \/ len(pos_taggs)\n            else:\n                VERBRatio = 0\n                NOUNRatio = 0\n                PRONRatio = 0\n                ADJRatio = 0\n                ADVPNRatio = 0\n                ADPRatio = 0\n                CONJRatio = 0\n                DETRatio = 0\n                NUMRatio = 0\n                PRTRatio = 0\n                PUNCRatio = 0\n            return VERBRatio, NOUNRatio, PRONRatio, ADJRatio, ADVPNRatio, ADPRatio, CONJRatio, DETRatio, NUMRatio, PRTRatio, PUNCRatio, Action_count\n\n        # convert the column to string type\n        data['clean_text'] = data['clean_text'].astype(str)\n\n        # getting count of text lengthand word count \n        data['text_len'] = data['clean_text'].astype(str).apply(len)\n        data['text_word_count'] = data['clean_text'].apply(lambda x: len(str(x).split()))\n        \n        #Unique word text count\n        data['text_unique_word_count']=data[\"clean_text\"].apply(lambda x: len(set(str(x).split())))\n\n\n        # getting count of text lengthand word count \n        data['title_len'] = data['title'].astype(str).apply(len)\n        data['title_word_count'] = data['title'].apply(lambda x: len(str(x).split()))\n\n        #Unique word title count\n        data['title_unique_word_count']=data[\"title\"].apply(lambda x: len(set(str(x).split())))\n\n \n        data[['VERBRatio', 'NOUNRatio', 'PRONRatio', 'ADJRatio',\n              'ADVPNRatio', 'ADPRatio', 'CONJRatio', 'DETRatio', 'NUMRatio', 'PRTRatio', 'PUNCRatio',\n              'ActionCount']] = data.apply(get_pos_tagging, axis=1, result_type=\"expand\")\n\n        logging.info(\"Creating Features: Acroynym\")\n        data['acronym_to_activity_ratio'] = data['clean_text'].apply(\n            get_acronym_count)\n\n        data['acronym count'] = data['clean_text'].apply(get_acronym_count)\n\n        logging.info(\"Creating Features: Numeric Value Count\")\n        data['num value count'] = data['clean_text'].apply(\n            get_numerical_value_count)\n\n        \n        logging.info(\"Creating Features: Last Activity Count\")\n        data['is_len_range_1_500'] = data.apply(\n            lambda x: 1 if 0 < int(x['text_len']) <= 500 else 0, axis=1)\n\n        logging.info(\"Creating Features: Last Activity Count\")\n        data['is_len_range_1_500'] = data.apply(\n            lambda x: 1 if 0 < int(x['title_len']) <= 500 else 0, axis=1)\n\n        logging.info(\"Creating Features: Last Activity Count\")\n        data['is_len_range_400_1100'] = data.apply(\n            lambda x: 1 if 400 < int(x['text_len']) <= 1100 else 0, axis=1)\n\n        logging.info(\"Creating Features: Last Activity Count\")\n        data['is_len_range_22_80'] = data.apply(\n            lambda x: 1 if 22 < int(x['title_len']) <= 80 else 0, axis=1)\n\n        logging.info(\"Features are created\")\n        return data\n\n\n\n    def xgbclassifier(self, train_data, train_label, test_data, test_label):\n        logging.info(\"Building XGB Classifier\")\n        model = xgb.XGBClassifier(objective=\"binary:logistic\",\n                                  random_state=7)\n       \n\n        model.fit(train_data, train_label, eval_set=[(test_data, test_label)],\n                  eval_metric='auc',\n                  early_stopping_rounds=100)\n\n        y_pred = model.predict(test_data)\n     \n        # evaluate predictions\n        accuracy = accuracy_score(test_label, y_pred)\n        print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\n        # Feature Importance\n        fig, ax = plt.subplots(figsize=(8, 12))\n        xgb.plot_importance(model, max_num_features=25, height=0.8, ax=ax)\n        plt.show()\n        return model\n\n    \n\n    \n    def caculateFeatureImp(self):\n        from numpy import loadtxt\n        from numpy import sort\n        from xgboost import XGBClassifier\n        from sklearn.model_selection import train_test_split\n        from sklearn.metrics import accuracy_score\n        from sklearn.feature_selection import SelectFromModel\n\n        df_news = pd.read_csv(config.NEW_TRAINING_FILE).fillna(\"none\").reset_index(drop=True)\n        \n        X, y = df_news.iloc[:,:-1],df_news.iloc[:,-1]\n        \n        y = df_news['label']\n        X = df_news.drop(columns=['label'])\n        X = X[['text_word_count', 'title_len', 'title_word_count', 'VERBRatio',\n       'NOUNRatio', 'PRONRatio', 'ADJRatio', 'ADVPNRatio', 'ADPRatio',\n       'CONJRatio', 'DETRatio', 'NUMRatio', 'PRTRatio', 'PUNCRatio',\n       'ActionCount', 'acronym_to_activity_ratio', 'acronym count',\n       'num value count', 'is_len_range_1_500','text_unique_word_count','title_unique_word_count']]\n\n\n        data_dmatrix = xgb.DMatrix(data=X,label=y)\n\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n\n       \n        # fit model on all training data\n        model = XGBClassifier(learning_rate = 0.05, n_estimators=300, max_depth=10)\n        model.fit(X_train, y_train)\n        \n        # make predictions for test data and evaluate\n        predictions = model.predict(X_test)\n        accuracy = accuracy_score(y_test, predictions)\n        print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n        \n        # Fit model using each importance as a threshold\n        thresholds = sort(model.feature_importances_)\n        print(thresholds)\n        \n        for thresh in thresholds:\n            # select features using threshold\n            selection = SelectFromModel(model, threshold=thresh, prefit=True)\n            select_X_train = selection.transform(X_train)\n            # train model\n            selection_model = XGBClassifier()\n            selection_model.fit(select_X_train, y_train)\n            # eval model\n            select_X_test = selection.transform(X_test)\n            predictions = selection_model.predict(select_X_test)\n            accuracy = accuracy_score(y_test, predictions)\n            print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))\n    \n\n    def execute_classifier(self):\n\n        # self.caculateFeatureImp()\n\n        logging.info(\"EDA\")\n        \n        df_news = pd.read_csv('D:\/Git\/Assignment\/input\/data\/train.csv').fillna(\"none\").reset_index(drop=True)\n        df_news.dropna(inplace=True)\n        # df_train, df_test = model_selection.train_test_split(dfx, test_size=0.1, random_state=42, stratify=dfx.label.values)\n        df_news['clean_text'] =self.processText(df_news)\n        df_news.dropna(inplace=True)\n        # print(df_news.head())\n        df_news = self.generate_features(df_news.copy())\n        print(df_news.columns)\n        # df_news = df_news[['id', 'title', 'author', 'text', 'clean_text', 'text_len','text_word_count', 'title_len', 'title_word_count', 'VERBRatio',       'NOUNRatio', 'PRONRatio', 'ADJRatio', 'ADVPNRatio', 'ADPRatio',       'CONJRatio', 'DETRatio', 'NUMRatio', 'PRTRatio', 'PUNCRatio',       'ActionCount', 'acronym_to_activity_ratio', 'acronym count',       'num value count', 'is_len_range_1_500','label']]\n        df_news.to_csv(config.NEW_TRAINING_FILE)\n\n\n        df_news = pd.read_csv(config.NEW_TRAINING_FILE).fillna(\"none\").reset_index(drop=True)\n        print(df_news.columns)\n        y = df_news['label']\n        X = df_news.drop(columns=['label'])\n        X = X[['text_word_count', 'title_len', 'title_word_count', 'VERBRatio',\n       'NOUNRatio', 'PRONRatio', 'ADJRatio', 'ADVPNRatio', 'ADPRatio',\n       'CONJRatio', 'DETRatio', 'NUMRatio', 'PRTRatio', 'PUNCRatio',\n       'ActionCount', 'acronym_to_activity_ratio', 'acronym count',\n       'num value count', 'is_len_range_1_500','text_unique_word_count','title_unique_word_count']]\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        # # ## Model - XGBoost\n        self.xgb_model = self.xgbclassifier(X_train, y_train, X_test, y_test)\n\n       \n\nif __name__ == '__main__':\n    rc = FeatureCreation()\n    rc.execute_classifier()\n    # rc.execute_regessors()","0db57bb6":"\nprofile = ProfileReport(df, title='Pandas Profiling Report', explorative=True)\n''profile.to_notebook_iframe()","c1a8f796":"#### The distribution of top bigrams before removing stop words of fake news","34b8ac87":"#### function for extract the words for wordcloud .","da4d5dd2":"#### <font color=green> ANALYSIS:- fakenews and realnews dataframe has almost same count of POS except nouns are more in real news <\/font>","6e49dde6":"#### Analyze how author has contribute for fake\/real news  ","f9915403":"#### The distribution of top unigrams before removing stop words of real news","b1fed38c":"#### <font color=green> ANALYSIS:- Titile Text Length: for realnews  min:22 , q1:70, Median:81, q3: 89  <br> for fakenews  min:3 , q1:50, Median:67, q3: 83  <br> 1) Real news always has more than 22 words however in fake news may have less  <br>2) there are more outlier in the fake news<br>Titile Word Count: for realnews  min:4 , q1:12, Median:84, q3: 16  <br>3) for fakenews  min:1 , q1:8, Median:10.5, q3: 13 :fake news has less word compare to real news<\/font>","5fff3e4f":"### The distribution of Title word count","0a268065":"#### The distribution of top bigrams before removing stop words of real news","ba623a7d":"## Univariate Analysis","b37df091":"#### The distribution of top trigrams after cleanign the text [removing stop words,punctuation] of fake news","62918d19":"#### The distribution of top trigrams after cleanign the text [removing stop words,punctuation] of real news","85e0b0fd":"#### Lets compare word count of fake news and real news ","001607c6":"#### Part-Of-Speech Tagging (POS) is a process of assigning parts of speech to each word, such as noun, verb, adjective, etc. <br>We use a simple TextBlob API to dive into POS of our \u201cText\u201d feature in our data set, and visualize these tags.","4f57eff5":"### Check null value's count","c06b76d9":"#### <font color=green> ANALYSIS:- Pandas Report <br> <br>PFB observation for Title field  <br>i.\tTotal 558 records which are null <br>ii.\t \u201ctitile\u201d has 439 records the duplicate values <br>iii.\t23 records which has only single word<br>iv.\t33.55% records have less than 10 words  <br>v.\tlabel \u201c1\u201d is assigned to the records which have null value<br>vi.\tIf we use BERT tokenizer then, we dont need to do the below text processing as BERT can handle it <br>    1.\tWe can check common words and remove those <br>    2.\tWe can check rare words and remove those <br>    3.\tSpelling correction <br>    4.\tStemming <br>    5.\tLemmatization<br>vii.\tIF the words is less than 10 words or more than 20 words then its high probability that it will be fake news <br>viii.\tIf the words between 10 to 20 then its high probability that it is real news  <br>ix.\tSentence size is less than 50 has highly fake news<br>x.\tAverage sentence size of the fake news is less than the real news <br>xi.\tOutlier are exists for fake news <br><br>PFB observation for Text field<br>   1) todo <\/font>","b740121f":"#### wordcloud for real news","3a4a1c77":"## Bivariate Analysis","a021cfbc":"#### The distribution of top trigrams before removing stop words of real news","8d4e5b35":"#### As is the case when working with any kind of text, the first step is separating each article\u2019s body text into tokens to get a corpus. Using the corpus, I can get features from the words. To tokenize each article, I used the NLTK package to do the following: <br> 1) Imported English words <br>2) Separated each article into tokens (removes whitespaces) <br>3) All words lowercased <br> 4) Removed punctuation and stop words <br> 5)   Lemmatized (converted to root form) words ","282bb3fd":"#### <font color=green> ANALYSIS:- These people are giving the fake or real news. However, we will not consider author name for the model building becuase it might overfit the model <\/font>","7678d6a6":"#### Box plot is used to compare the word count of each title and label .","6b482d4f":"#### The distribution of top bigrams after cleanign the text [removing stop words,punctuation] of real news","2c4b21ee":"#### <font color=green> ANALYSIS:- This column is 100% unique and since we don\u2019t want to overfit the model, hence we are removing id column.  <\/font>\n ","36c2572c":"#### Dataset is balanced or not","459cbdff":"#### <font color=green> ANALYSIS:- Text Word Count:- for realnews  min:1 , q1:400, Median:713, q3: 1179  <br> for fakenews  min:0 , q1:200, Median:433, q3: 875  <br> 1) Real news has the aveagage word count  is 713 and fakenews is 433 so by looking at the word count size also we can determine whether news is fake or real <br>2) there are more outlier in the fake news <br> for most of the news, fake news text length is little bit lower than the real news <\/font>","aedb649c":"#### Box plot is used to compare the word count of each text and label .","407c73ed":"#### <font color=green> ANALYSIS:- By looking at the graph it looks <br> 1) fake news text length is either small or very big  <\/font>","5f3f9a5f":"#### Distribution of text length for fake and real news ","0e34668e":"#### The distribution of top part-of-speech tags of text corpus of fake news","5ab67aac":"### The distribution of text length","f13ba9cc":"#### The distribution of top trigrams before removing stop words of fake news","7eaed8e0":"#### The distribution of top unigrams before removing stop words of fake news","61941bc4":"#### <font color=green> ANALYSIS:- This column has more than 65% records which word count is less than 900 words  <\/font>\n ","b122d65a":"#### Process text from main dataframe and create the new feature for it","9a628ae7":"#### <font color=green> ANALYSIS:- By looking at the graph it looks <br> 1) if there are less than 10 words in the sentence then it is most probably fakenews <\/font>","753b945b":"#### Lets dig into text length and understand more ","5dce4343":"#### The distribution of top unigrams after cleanign the text [removing stop words,punctuation] of real news\n","6dc1c83a":"#### Lets dig into word count and understand more ","b3d8c647":"### Create the two dataframes for the analysis","2ddf4632":"#### The distribution of top unigrams after cleanign the text [removing stop words,punctuation] of fake news","31689773":"#### <font color=green> ANALYSIS:- This column has more than 72% data which length of text is greater than 2k  <\/font>\n ","3494fba0":"#### Box plot is used to compare the text length of each title and label .","19c179f4":"#### The distribution of top bigrams after cleanign the text [removing stop words,punctuation] of fake news","afe21b7a":"### Create new feature for number of words in the string and length of the string","bf98a988":"#### <font color=green> ANALYSIS:- After analyzing these n-grams, I ended up not using them in the modeling process because <br>1) processing time was multiplied through the use of n-grams on top of single tokens  <br>2) n-grams were pretty similar between the two classes of text and  <br>3) metrics from the results of modeling (see below) were already strong without adding n-grams. <\/font>\n ","e5db1c10":"### Delete null values","824623b3":"#### wordcloud for fake news","96bd22a6":"#### <font color=green> ANALYSIS:- 56% has 0s and 44% has 1s that has almost balanced word count   <\/font>","26595ae9":"#### Box plot is used to compare the text length of each text and label .","2a6f8a23":"#### <font color=green> ANALYSIS:- By looking at the graph it looks <br> 1) if the sentence size is less than 1kthen it is most probably fakenews <\/font>","2943d535":"#### data distrubtion of clean text and title word ","cd721737":"#### Box plot is used to compare the word count of cleaned each text and label .","1fcdfa23":"#### <font color=green> ANALYSIS:- 65%+ records has less than 5k words in text and title is fairly distributed  <\/font>","4e7a5680":"#### <font color=green> ANALYSIS:- by looking at the graph it looks fake and real news has same words <\/font>","bdcb9a89":"## Calculate Unigram, Bigram & Trigram of fake and real news and analyze","629f67ac":"#### <font color=green> ANALYSIS:- for now dropped rows which has null and thsoe are almost 2.5% . We can use title when text is null or vice versa  <\/font>\n ","a5a286fc":"### The distribution of word count","b140adce":"#### Process fakenews and realnews dataset and extract clean text ","5788b216":"#### The distribution of top part-of-speech tags of text corpus of real news"}}