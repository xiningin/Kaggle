{"cell_type":{"ec8f392e":"code","24fd9246":"code","c4864eb8":"code","5a17a6c9":"code","084c8c3e":"code","50643cee":"code","8ce81c1e":"code","4bc9ecb7":"code","99a934f0":"code","bbbe84be":"code","1235997a":"code","cdd81c60":"code","c1c0f2f7":"code","5e97eaea":"code","892937dc":"code","c2f3bef9":"code","7d35ea6e":"code","7cfb8720":"code","b78855f7":"code","5d5ad526":"code","2d54c2df":"code","9f3e4874":"code","13050736":"code","d5b40d90":"code","2da580b0":"code","695587ce":"code","88962501":"code","75255cc3":"code","a71e40d3":"code","095ff51a":"code","f083a033":"markdown","7f667ad0":"markdown","5436d7dc":"markdown","72ee312b":"markdown","4c3099a2":"markdown","9030bb7c":"markdown","abe1d467":"markdown","1b97d25a":"markdown","ce4613d5":"markdown","f9e2a9dc":"markdown","b62d48aa":"markdown","02a7e58c":"markdown","c03e2392":"markdown","a120f447":"markdown","0df22985":"markdown","0d57e126":"markdown","bcc93605":"markdown","c54fb027":"markdown","aaa7aa50":"markdown","5def1af8":"markdown","3d97e590":"markdown","604d2dd2":"markdown","88901c5e":"markdown","12371460":"markdown","5bd14b3f":"markdown","44bb2bf8":"markdown","3766db53":"markdown","a8645556":"markdown","932efa7e":"markdown","16784365":"markdown"},"source":{"ec8f392e":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\ninput_path = Path('\/kaggle\/input\/tabular-playground-series-jan-2021\/')","24fd9246":"train = pd.read_csv(input_path \/ 'train.csv', index_col='id')\ndisplay(train.head())","c4864eb8":"test = pd.read_csv(input_path \/ 'test.csv', index_col='id')\ndisplay(test.head())","5a17a6c9":"train.describe()","084c8c3e":"test.describe()","50643cee":"boxplot = train.boxplot(column=['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14'],\n                       figsize=(12,9))","8ce81c1e":"boxplot = test.boxplot(column=['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14'],\n                       figsize=(12,9))","4bc9ecb7":"# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 12))\n\nsns.heatmap(train.corr(), annot = True,fmt='.1g', vmin=-1, vmax=1, center= 0,cmap= 'coolwarm')","99a934f0":"target = train.pop('target')","bbbe84be":"train.head()","1235997a":"X_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.80)","cdd81c60":"from sklearn.dummy import DummyRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import LassoLars\nfrom sklearn.linear_model import ARDRegression\nfrom sklearn.linear_model import PassiveAggressiveRegressor\nfrom sklearn.linear_model import TheilSenRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n        ","c1c0f2f7":"def FitAndScoreModel(df,name, model,X_tr,y_tr,X_tst,y_tst):\n    model.fit(X_tr,y_tr)\n    Y_pred = model.predict(X_tst)\n    score=mean_squared_error(y_tst, Y_pred, squared=False)\n    df = df.append({'Model':name, 'MSE': score},ignore_index = True) \n    return df","5e97eaea":"dResults = pd.DataFrame(columns = ['Model', 'MSE'])","892937dc":"classifiers = [\n    DummyRegressor(strategy='median'),\n   # SVR(),\n    SGDRegressor(),\n    BayesianRidge(),\n    LassoLars(),\n    ARDRegression(),\n    PassiveAggressiveRegressor(),\n    LinearRegression(),\n    LGBMRegressor(),\n    RandomForestRegressor(),\n    XGBRegressor()]\n\n \nfor item in classifiers:\n    print(item)\n    clf = item\n    dResults=FitAndScoreModel(dResults,item,item,X_train,y_train,X_test,y_test) ","c2f3bef9":"dResults.sort_values(by='MSE', ascending=True,inplace=True)\ndResults.set_index('MSE',inplace=True)\ndResults.head(dResults.shape[0])","7d35ea6e":"import optuna.integration.lightgbm as lgbTune\n\n#dtrain = lgbTune.Dataset(X_train, label=y_train)\n#dval = lgbTune.Dataset(X_test, label=y_test)\n#params = {\"objective\": \"regression\",\n#          \"metric\": \"rmse\",\n#          'num_leaves':2 ** 8,\n#          \"verbosity\": -1,\n#          \"boosting_type\": \"gbdt\",\n#          \"n_estimators\":20000, \n#          \"early_stopping_round\":400,\n#          'n_jobs': -1,\n#          'learning_rate': 0.005,\n#          'max_depth': 8,\n#          'tree_learner': 'serial',\n#          'colsample_bytree': 0.8,\n#          'subsample_freq': 1,\n#          'subsample': 0.8,\n#          'max_bin': 255}\n\n\n#model = lgbTune.train(params, dtrain, valid_sets=[dval], verbose_eval=False)","7cfb8720":"#params = model.params\n#params","b78855f7":"params={'objective': 'regression',\n 'metric': 'rmse',\n 'num_leaves': 234,\n 'verbosity': -1,\n 'boosting_type': 'gbdt',\n 'n_jobs': -1,\n 'learning_rate': 0.005,\n 'max_depth': 8,\n 'tree_learner': 'serial',\n 'max_bin': 255,\n 'feature_pre_filter': False,\n 'bagging_fraction': 0.4134640813947842,\n 'bagging_freq': 1,\n 'feature_fraction': 0.4,\n 'lambda_l1': 9.511141306606756,\n 'lambda_l2': 1.3196758411622028e-08,\n 'min_child_samples': 20,\n 'num_iterations': 20000,\n 'early_stopping_round': 400}","5d5ad526":"from lightgbm import LGBMRegressor\nfrom sklearn.model_selection import KFold\n\nn_fold = 10\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\ntrain_columns = train.columns.values\n\noof = np.zeros(len(train))\nLGBMpredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train, target.values)):\n    \n    strLog = \"fold {}\".format(fold_)\n    print(strLog)\n    \n    X_tr, X_val = train.iloc[trn_idx], train.iloc[val_idx]\n    y_tr, y_val = target.iloc[trn_idx], target.iloc[val_idx]\n\n    model = LGBMRegressor(**params, n_estimators = 20000)\n   \n    model.fit(X_tr, y_tr, \n              eval_set=[(X_tr, y_tr), (X_val, y_val)], eval_metric='rmse',\n              verbose=1000, early_stopping_rounds=400)\n    \n    \n    oof[val_idx] = model.predict(X_val, num_iteration=model.best_iteration_)\n\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = train_columns\n    fold_importance_df[\"importance\"] = model.feature_importances_[:len(train_columns)]\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    #predictions\n    LGBMpredictions += model.predict(test, num_iteration=model.best_iteration_) \/ folds.n_splits\n","2d54c2df":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:3014].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure()\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()","9f3e4874":"#from sklearn.model_selection import GridSearchCV\n\n#xgb = XGBRegressor()\n#parameters = {'nthread':[4], \n#              'objective':['reg:squarederror'],\n#              'learning_rate': [.01,.03, 0.05, .07], \n#              'max_depth': [5, 6, 7],\n#              'min_child_weight':range(1,6,2),\n#              'silent': [1],\n#              'subsample': [0.7],\n#              'colsample_bytree': [0.7],\n#              'n_estimators': [500,1000,2000,4000]}\n\n \n  \n\n#xgb_grid = GridSearchCV(xgb,\n#                        parameters,\n#                        cv = 2,\n#                        n_jobs = 5,\n#                        verbose=True)\n\n#xgb_grid.fit(train,\n#         target)\n\n#print(xgb_grid.best_score_)\n#print(xgb_grid.best_params_)","13050736":"#XGparams = xgb_grid.best_params_\n#XGparams","d5b40d90":"XGparams={'colsample_bytree': 0.7,\n 'learning_rate': 0.01,\n 'max_depth': 7,\n 'min_child_weight': 1,\n 'n_estimators': 4000,\n 'nthread': 4,\n 'objective': 'reg:squarederror',\n# 'silent': 1,\n 'subsample': 0.7}","2da580b0":"n_fold = 10\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\ntrain_columns = train.columns.values\n\noof = np.zeros(len(train))\nXGpredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train, target.values)):\n    \n    strLog = \"fold {}\".format(fold_)\n    print(strLog)\n    \n    X_tr, X_val = train.iloc[trn_idx], train.iloc[val_idx]\n    y_tr, y_val = target.iloc[trn_idx], target.iloc[val_idx]\n\n    model = XGBRegressor(**XGparams)\n   \n    model.fit(X_tr, y_tr, \n              eval_set=[(X_tr, y_tr), (X_val, y_val)], verbose=1000, early_stopping_rounds=400)\n    \n    \n    oof[val_idx] = model.predict(X_val, ntree_limit=model.best_iteration)\n    preds = model.predict(test, ntree_limit=model.best_iteration)\n\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = train_columns\n    fold_importance_df[\"importance\"] = model.feature_importances_[:len(train_columns)]\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    #predictions\n    XGpredictions += model.predict(test, ntree_limit=model.best_iteration)\/ folds.n_splits\n   \n","695587ce":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")  \n        .mean()  \n        .sort_values(by=\"importance\", ascending=False)[:3014].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure()\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False)) \nplt.title('XGBoost Features (averaged over folds)')\nplt.tight_layout()","88962501":"submission  = pd.read_csv(input_path \/ 'sample_submission.csv', index_col='id')\nsubmission.reset_index(inplace=True)\nsubmission = submission.rename(columns = {'index':'id'})","75255cc3":"LGBMsubmission=submission.copy()\nLGBMsubmission['target'] = LGBMpredictions\nLGBMsubmission.to_csv('submission_LGBM.csv', header=True, index=False)\nLGBMsubmission.head()","a71e40d3":"XGBoostsubmission=submission.copy()\nXGBoostsubmission['target'] = XGpredictions\nXGBoostsubmission.to_csv('submission_XGBoost.csv', header=True, index=False)\nXGBoostsubmission.head()","095ff51a":"EnsembledSubmission=submission.copy()\n#EnsembledSubmission['target'] = (0.5*XGpredictions)+(0.5*LGBMpredictions)\nEnsembledSubmission['target'] = (LGBMpredictions*0.72 + XGpredictions*0.28)\nEnsembledSubmission.to_csv('ensembled_submission.csv', header=True, index=False)\nEnsembledSubmission.head()","f083a033":"## Tuning LGBM Model","7f667ad0":"## Read in the data files","5436d7dc":"## Submission","72ee312b":"Even the distributions of outliers appears the same between datasets. Just to round off lets look at the correlation matrix","4c3099a2":"Score 0.69819","9030bb7c":"Create a Blank Dataframe, as we run each model we will note it and its score ","abe1d467":"### Pull out the target, and make a validation split","1b97d25a":"## Let us look at the Data","ce4613d5":"## Using XGBoost","f9e2a9dc":"#### Create an XGBoost Submission","b62d48aa":"No missing values to worry about, and the dataset look very similar, ;ets plot the two datasets as Boxplots","02a7e58c":"We now do a preliminary run of some of the common regression models to see which one(s) perform well and are worth pursuing further.","c03e2392":"Score  0.69780","a120f447":"# Tabular Playground Series - Jan 2021\n\nThe Kaggle Tabular Playground for January offer us a Blackbox Challenge: an array of readings labeled cont1-cont14 which translate to a target value.  The goal is to develop a model based off of training data which provides the readings and the resulting target to be able to accurately predict the target value for a test dataset where we are supplied readings but must predict the target.   ","0df22985":"Let us look at the feature importance for LGBM","0d57e126":"One thing I noted in the example notebook, they used a rather low (60%) of the training data for training.  I changed this to a perhaps more standard 80%","bcc93605":"Disappointed that the Ensemble did not produce an improvement, especially since each model weighed different features differently.  Will look into pulling the third place algorithm, Random Forest, into the mix and see if that doesn't make a change for the better.","c54fb027":"#### Create an LGBM Submission","aaa7aa50":"# Determination of Best Model ","5def1af8":"We will now fit over 10 folds and arrive at LGBM predictions. ","3d97e590":"Score 0.69945","604d2dd2":"Not seeing any obvious clues to feature engineering, so lets break our training data into a training and training validation subset and see what the models tells us","88901c5e":"So LGBm Regressor, Random Forrest and XGBRegressor are the top 3. ","12371460":"### Ensemble Solution of LGBM and XGBoost","5bd14b3f":"Let's repeat the process for XGBoot, again tuning is commented out and the parameters that resulted from tuning used.","44bb2bf8":"Again let us look at feature importance","3766db53":"Note:  As tuning takes a verrrrry long time, I am leaving my tuning actions in for reference but commented out and just referencing the resulting parameters that were generated","a8645556":"As before we fit over 10 folds","932efa7e":"### The Prelimnaries, import the usual basic libraries. ","16784365":"# Score Model"}}