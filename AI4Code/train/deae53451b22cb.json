{"cell_type":{"6f78d852":"code","e24a74d4":"code","8efb82d9":"code","80a1f21b":"code","966e2fbe":"code","ad4d7c3d":"code","67f63ef5":"code","706388c1":"code","a1620aa6":"code","cb736017":"code","cf723b3f":"code","1c94b1c8":"code","e30d33e9":"code","bb857187":"code","5c0018e7":"code","66e0d4d3":"code","0f5a2ef1":"code","96450eda":"code","4454d492":"code","a4b94238":"code","ec73b005":"code","a6150bc6":"code","49d84e9e":"markdown","8ccba795":"markdown","89a212e6":"markdown","65008507":"markdown","712ebcd6":"markdown","1a2dc523":"markdown","a6e5fa6f":"markdown","d5fb01f1":"markdown","49a74d9e":"markdown","5e5d5001":"markdown"},"source":{"6f78d852":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e24a74d4":"train_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","8efb82d9":"with pd.option_context('display.max_columns', 100):\n    print(train_data.describe())","80a1f21b":"with pd.option_context('display.max_columns', 100):\n    print(train_data.describe(include=[object]))","966e2fbe":"\ndef prep_data(dat):\n    #categorical features\n    categorical = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LotShape\",\n                   \"LandContour\", \"LotConfig\", \"LandSlope\", \"Neighborhood\", \n                   \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"MasVnrType\", \n                   \"Foundation\", \"Heating\", \"SaleType\", \"SaleCondition\", \"MiscFeature\"]\n    \n    for c in categorical:\n        dat = pd.concat([dat,\n                                pd.get_dummies(dat[c], prefix=c, dummy_na=True)],\n                               axis=1)\n    \n    dat.drop(categorical, axis=1, inplace=True)\n    \n    #ordinal features\n    #many of the ordinal features share the same or a similar grading scale\n    #0 is none, 1 is poor, 2 is fair, 3 is typical, 4 is good, 5 is excellent\n    \n    similarGrades = [\"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"HeatingQC\",\n                     \"KitchenQual\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"PoolQC\"]\n    \n\n        \n    for ordinal in similarGrades:\n        mapping = {\"N\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n        dat[ordinal].fillna(\"N\", inplace = True)\n        dat[ordinal] = dat[ordinal].map(mapping)\n    \n    \n    #BsmtExposure\n    mapping = {\"N\": 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4}\n    dat[\"BsmtExposure\"].fillna(\"N\", inplace = True)\n    dat[\"BsmtExposure\"] = dat[\"BsmtExposure\"].map(mapping)\n    \n    #CentralAir\n    mapping = {\"N\": 0, \"Y\": 1}\n    dat[\"CentralAir\"] = dat[\"CentralAir\"].map(mapping)\n    \n    #Functional\n    mapping = {\"N\": 0, \"Sal\": 1, \"Sev\": 2, \"Maj2\": 3, \"Maj1\": 4, \"Mod\": 5, \"Min2\": 6, \"Min1\": 7, \"Typ\": 8}\n    dat[\"Functional\"].fillna(\"N\", inplace = True)\n    dat[\"Functional\"] = dat[\"Functional\"].map(mapping)\n    \n    \n    #GarageFinish\n    mapping = {\"N\": 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3}\n    dat[\"GarageFinish\"].fillna(\"N\", inplace = True)\n    dat[\"GarageFinish\"] = dat[\"GarageFinish\"].map(mapping)\n    \n    #PavedDrive\n    mapping = {\"N\": 0, \"P\": 1, \"Y\": 2}\n    dat[\"PavedDrive\"] = dat[\"PavedDrive\"].map(mapping)\n    \n    #Utilities\n    mapping = {\"N\": 0, \"ELO\": 1, \"NoSeWa\": 2, \"NoSewr\": 3, \"AllPub\": 4}\n    dat[\"Utilities\"].fillna(\"N\", inplace = True)\n    dat[\"Utilities\"] = dat[\"Utilities\"].map(mapping)\n    \n    #Electrical\n    mapping = {\"N\": 0, \"Mix\": 1, \"FuseP\": 2, \"FuseF\": 3, \"FuseA\": 4, \"SBrkr\": 5}\n    dat[\"Electrical\"].fillna(\"N\", inplace = True)\n    dat[\"Electrical\"] = dat[\"Electrical\"].map(mapping)\n    \n    #GarageType\n    mapping = {\"N\": 0, \"Detchd\": 1, \"CarPort\": 2, \"BuiltIn\": 3, \"Basment\": 4, \"Attchd\": 5, \"2Types\": 6}\n    dat[\"GarageType\"].fillna(\"N\", inplace = True)\n    dat[\"GarageType\"] = dat[\"GarageType\"].map(mapping)\n    \n    #Fence\n    mapping = {\"N\": 0, \"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4}\n    dat[\"Fence\"].fillna(\"N\", inplace = True)\n    dat[\"Fence\"] = dat[\"Fence\"].map(mapping)\n    \n    #BsmtFinType1\n    mapping = {\"N\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n    dat[\"BsmtFinType1\"].fillna(\"N\", inplace = True)\n    dat[\"BsmtFinType1\"] = dat[\"BsmtFinType1\"].map(mapping)\n    \n    #BsmtFinType2\n    mapping = {\"N\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n    dat[\"BsmtFinType2\"].fillna(\"N\", inplace = True)\n    dat[\"BsmtFinType2\"] = dat[\"BsmtFinType2\"].map(mapping)\n    \n    \n    #feature engineering\n    \n    def combine_two_features(names, features, prefix):\n        \n        def combo(row, name):\n            if row[features[0]] == name or row[features[1]] == name:\n                return 1\n            return 0\n        \n        for name in names:\n            dat[prefix + \"_\" + name] = dat.apply(lambda row: combo(row, name), axis = 1)\n        \n        dat.drop(features, axis=1, inplace=True)\n        \n    \n    # Condition1 and Condition2,\n    combine_features = [\"Condition1\", \"Condition2\"]\n    condition_names = [\"Artery\", \"Feedr\", \"Norm\", \"RRNn\", \"RRAn\", \"PosN\", \"PosA\", \"RRNe\", \"RRAe\"]\n    combine_two_features(condition_names, combine_features, \"Condition\")\n    \n    # Exterior1st and Exterior2nd\n    combine_features = [\"Exterior1st\", \"Exterior2nd\"]\n    exterior_names = [\"AsbShng\", \"AsphShn\", \"BrkComm\", \"BrkFace\", \"CBlock\", \"CemntBd\",\n                     \"HdBoard\", \"ImStucc\", \"MetalSd\", \"Other\", \"Plywood\", \"PreCast\", \"Stone\",\n                     \"Stucco\", \"VinylSd\", \"Wd Sdng\", \"WdShing\"]\n    combine_two_features(exterior_names, combine_features, \"Exterior\")\n    \n    \n    #Remodel date is more useful as a date relative to the sold date then the actual year remodeled\n    dat[\"YearsSinceUpdate\"] = dat[\"YrSold\"] - dat[\"YearRemodAdd\"]\n    \n    return dat\n    ","ad4d7c3d":"d2 = prep_data(train_data)","67f63ef5":"#dealing with missing values\nd2[\"LotFrontage\"].fillna(0, inplace = True)\nd2[\"MasVnrArea\"].fillna(0, inplace = True)\nd2[\"GarageYrBlt\"] = np.where(d2[\"GarageYrBlt\"].isnull(), d2[\"YearBuilt\"],d2[\"GarageYrBlt\"])\n    ","706388c1":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\n# import packages for hyperparameters tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe","a1620aa6":"target = [\"SalePrice\"]\nlabels = d2.filter(items=target).values.ravel()\n\nd2.drop(columns = [\"SalePrice\", \"Id\"], axis=1, inplace=True)","cb736017":"test_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\") \ntest_data = prep_data(test_data)\ntest_ids = test_data.Id\ntest_data.drop(columns = [\"Id\"], axis=1, inplace=True)","cf723b3f":"print([i for i in test_data.columns if test_data[i].isnull().any()])","1c94b1c8":"#dealing with missing values\ntest_data[\"LotFrontage\"].fillna(0, inplace = True)\ntest_data[\"MasVnrArea\"].fillna(0, inplace = True)\ntest_data[\"BsmtFinSF1\"].fillna(0, inplace=True)\ntest_data[\"BsmtFinSF2\"].fillna(0, inplace=True)\ntest_data[\"BsmtUnfSF\"].fillna(0, inplace=True)\ntest_data[\"BsmtFullBath\"].fillna(0, inplace=True)\ntest_data[\"BsmtHalfBath\"].fillna(0, inplace=True)\ntest_data[\"TotalBsmtSF\"].fillna(0, inplace=True)\ntest_data[\"GarageCars\"].fillna(0, inplace=True)\ntest_data[\"GarageArea\"].fillna(0, inplace=True)\ntest_data[\"GarageYrBlt\"] = np.where(test_data[\"GarageYrBlt\"].isnull(), test_data[\"YearBuilt\"],test_data[\"GarageYrBlt\"])","e30d33e9":"for i in d2.columns.difference(test_data.columns):\n    test_data[i] = 0\n\nfor i in test_data.columns.difference(d2.columns):\n    d2[i] = 0","bb857187":"X_train, X_test, y_train, y_test = train_test_split(d2, labels, test_size=0.3, random_state = 42)","5c0018e7":"space={'eta': hp.uniform(\"eta\", 0, 1),\n       'max_depth': hp.quniform(\"max_depth\", 3, 20, 1),\n        'gamma': hp.uniform ('gamma', 0,10),\n        'reg_alpha' : hp.quniform('reg_alpha', 0,180,1),\n        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'n_estimators': 180,\n        'seed': 42\n    }","66e0d4d3":"def objective(space):\n    clf=xgb.XGBRegressor(eta = space['eta'],\n                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n                    colsample_bytree=int(space['colsample_bytree']), n_jobs=-1)\n    \n    evaluation = [( X_train, y_train), ( X_test, y_test)]\n    \n    clf.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"rmse\",\n            early_stopping_rounds=10,verbose=False)\n    \n\n    pred = clf.predict(X_test)\n    accuracy = mean_squared_error(y_test, pred, squared=False)\n    return {'loss': accuracy, 'status': STATUS_OK}","0f5a2ef1":"trials = Trials()\n\nbest_hyperparams = fmin(fn = objective,\n                        space = space,\n                        algo = tpe.suggest,\n                        max_evals = 100,\n                        trials = trials)","96450eda":"best_hyperparams","4454d492":"estimator = xgb.XGBRegressor(colsample_bytree=best_hyperparams['colsample_bytree'],\n                            max_depth=int(best_hyperparams['max_depth']),\n                            learning_rate=best_hyperparams['eta'],\n                            gamma=best_hyperparams['gamma'],\n                            min_child_weight=best_hyperparams['min_child_weight'],\n                            reg_alpha=best_hyperparams['reg_alpha'],\n                            reg_lambda=best_hyperparams['reg_lambda'], n_jobs=-1)\nselector = RFECV(estimator, step=1, cv=5)\nselector = selector.fit(d2, labels)\n\nd2 = selector.transform(d2)\ntest_data = selector.transform(test_data)","a4b94238":"model = estimator.fit(d2, labels)","ec73b005":"predictions = model.predict(test_data)","a6150bc6":"\noutput = pd.DataFrame({'Id': test_ids, 'SalePrice': predictions})\noutput.to_csv('\/kaggle\/working\/submission.csv', index=False)","49d84e9e":"Code to optimize the XGBoost model with hyperopt taken from https:\/\/www.kaggle.com\/prashant111\/a-guide-on-xgboost-hyperparameters-tuning . Below we define the search space and objective function that to choose the hyperparameters.","8ccba795":"We finally fit the model on the data after we've selected the best hyperparameters and features to use.","89a212e6":"Now that we're done preparing the data, let's create a 70\/30 train\/test split and work on tuning the model.","65008507":"**Data Prep**\n\nCategorical features that need to be one hot encoded: MSSubClass, MSZoning, Street, Alley, LotShape, LandContour, LotConfig, LandSlope, Neighborhood, BldgType, HouseStyle, RoofStyle, RoofMatl, MasVnrType, Foundation, Heating, SaleType, SaleCondition, MiscFeature\n\nOrdinal features that need to be rank encoded: ExterQual, ExterCond, BsmtQual, BsmtCond, BsmtExposure, HeatingQC, CentralAir, KitchenQual, Functional, FireplaceQu, GarageFinish, GarageQual, GarageCond, PavedDrive, PoolQC, Fence, BsmtFinType1 and BsmtFinType2\n\nFeatures that might could either be categorical or ordinal: Utilities, Electrical, GarageType\n\nCreate new categorical features from Condition1 and Condition2, Exterior1st and Exterior2nd\n\nReplace YearRemodAdd with YearsSinceUpdate which is YrSold - YearsRemodAdd","712ebcd6":"For this dataset, I'll be using a XGBoost model tuning the parameters with HyperOpt and using RFE to select the best features to use.","1a2dc523":"We save the predictions and submit them.","a6e5fa6f":"Let's import the models and helpers that we'll use to create the model","d5fb01f1":"Due to the nature of the data distributions, there may be categories missing in both datasets that we need to populate as 0 in order to get the model predictions to work.","49a74d9e":"**Note: Missing values**\n\nFrom the above data summaries we see that the following features have missing values. \n\nLotFrontage, MasVnrArea, GarageYrBlt, Alley, MasVnrType, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2,FireplaceQu, GarageType, GarageFinish, GarageQual, GarageCond, PoolQC, Fence, MiscFeature.\n\nWhen cross referencing with the data description file, Alley, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, FireplaceQu, GarageType, GarageFinish, GarageQual, GarageCond, PoolQC, Fence, MiscFeature all have meaningful missing values, which means we only have to deal with **LotFrontage, MasVnrArea, and GarageYrBlt**.\n\nMissing values of LotsFrontage and MasVnrArea will be replaced  while GarageYrBlt will be assumed to be the YearBuilt.","5e5d5001":"Let's use RFE to narrow down our features and then create the model."}}