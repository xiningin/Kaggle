{"cell_type":{"87ae0954":"code","cf6159f5":"code","ec1926fa":"code","032a0af1":"code","ddcd8d17":"code","01d8e87a":"markdown","323a8811":"markdown","7da66e52":"markdown","844a73af":"markdown","66707d03":"markdown"},"source":{"87ae0954":"import pandas as pd\nimport numpy as np\nimport os\nimport lightgbm as lgb","cf6159f5":"path = \"..\/input\/m5-forecasting-accuracy\"\n\ncalendar = pd.read_csv(os.path.join(path, \"calendar.csv\"))\nselling_prices = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\nsample_submission = pd.read_csv(os.path.join(path, \"sample_submission.csv\"))\nsales = pd.read_csv(os.path.join(path, \"sales_train_validation.csv\"))","ec1926fa":"from sklearn.preprocessing import OrdinalEncoder\n\ndef prep_calendar(df):\n    df = df.drop([\"date\", \"weekday\", \"event_type_1\", \"event_type_2\"], axis=1)\n    df = df.assign(d = df.d.str[2:].astype(int))\n    to_ordinal = [\"event_name_1\", \"event_name_2\"] \n    df[to_ordinal] = df[to_ordinal].fillna(\"1\")\n    df[to_ordinal] = OrdinalEncoder(dtype=\"int\").fit_transform(df[to_ordinal]) + 1\n    to_int8 = [\"wday\", \"month\", \"snap_CA\", \"snap_TX\", \"snap_WI\"] + to_ordinal\n    df[to_int8] = df[to_int8].astype(\"int8\")\n    \n    return df\n\ncalendar = prep_calendar(calendar)\ncalendar.head()","032a0af1":"from sklearn.model_selection import train_test_split\n\nLAGS = [7, 28]\nWINDOWS = [7, 28, 56]\nFIRST = 1914\nLENGTH = 28\n\ndef demand_features(df):\n    \"\"\" Derive features from sales data and remove rows with missing values \"\"\"\n    \n    for lag in LAGS:\n        df[f'lag_t{lag}'] = df.groupby('id')['demand'].transform(lambda x: x.shift(lag)).astype(\"float32\")\n        for w in WINDOWS:\n            df[f'rolling_mean_lag{lag}_w{w}'] = df.groupby('id')[f'lag_t{lag}'].transform(lambda x: x.rolling(w).mean()).astype(\"float32\")\n        \n    return df\n\ndef demand_features_eval(df):\n    \"\"\" Same as demand_features but for the step-by-step evaluation \"\"\"\n    out = df.groupby('id', sort=False).last()\n    for lag in LAGS:\n        out[f'lag_t{lag}'] = df.groupby('id', sort=False)['demand'].nth(-lag-1).astype(\"float32\")\n        for w in WINDOWS:\n            out[f'rolling_mean_lag{lag}_w{w}'] = df.groupby('id', sort=False)['demand'].nth(list(range(-lag-w, -lag))).groupby('id', sort=False).mean().astype(\"float32\")\n    \n    return out.reset_index()\n\ndef prep_data(df, drop_d=1000, dept_id=\"FOODS_1\"):\n    \"\"\" Prepare model data sets \"\"\"\n    \n    print(f\"\\nWorking on dept {dept_id}\")\n    # Filter on dept_id\n    df = df[df.dept_id == dept_id]\n    df = df.drop([\"dept_id\", \"cat_id\"], axis=1)\n    \n    # Kick out old dates\n    df = df.drop([\"d_\" + str(i+1) for i in range(drop_d)], axis=1)\n\n    # Reshape to long\n    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(FIRST + i) for i in range(2 * LENGTH)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"store_id\", \"state_id\"], var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int64\"),\n                   demand=df.demand.astype(\"float32\"))\n    \n    # Add demand features\n    df = demand_features(df)\n    \n    # Remove rows with NAs\n    df = df[df.d > (drop_d + max(LAGS) + max(WINDOWS))]\n \n    # Join calendar & prices\n    df = df.merge(calendar, how=\"left\", on=\"d\")\n    df = df.merge(selling_prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])\n    df = df.drop([\"wm_yr_wk\"], axis=1)\n    \n    # Ordinal encoding of remaining categorical fields\n    for v in [\"item_id\", \"store_id\", \"state_id\"]:\n        df[v] = OrdinalEncoder(dtype=\"int\").fit_transform(df[[v]]).astype(\"int16\") + 1\n    \n    # Determine list of covariables\n    x = list(set(df.columns) - {'id', 'd', 'demand'})\n            \n    # Split into test, valid, train\n    test = df[df.d >= FIRST - max(LAGS) - max(WINDOWS)]\n    df = df[df.d < FIRST]\n\n    xtrain, xvalid, ytrain, yvalid = train_test_split(df[x], df[\"demand\"], test_size=0.1, shuffle=True, random_state=54)\n    train = lgb.Dataset(xtrain, label = ytrain)\n    valid = lgb.Dataset(xvalid, label = yvalid)\n\n    return train, valid, test, x\n\ndef fit_model(train, valid, dept):\n    \"\"\" Fit LightGBM model \"\"\"\n     \n    params = {\n        'metric': 'rmse',\n        'objective': 'poisson',\n        'seed': 200,\n        'learning_rate': 0.2 - 0.13 * (dept in [\"HOBBIES_1\", \"HOBBIES_2\", \"HOUSEHOLD_2\"]),\n        'lambda': 0.1,\n        'num_leaves': 50,\n        'colsample_bytree': 0.7\n    }\n\n    fit = lgb.train(params, \n                    train, \n                    num_boost_round = 5000, \n                    valid_sets = [valid], \n                    early_stopping_rounds = 200,\n                    verbose_eval = 200)\n    \n    lgb.plot_importance(fit, importance_type=\"gain\", precision=0, height=0.5, figsize=(6, 10), title=dept);\n    \n    return fit\n\ndef pred_to_csv(fit, test, x, cols=sample_submission.columns, file=\"submission.csv\", first=False):\n    \"\"\" Calculate predictions and append to submission csv \"\"\"\n    \n    # Recursive prediction\n    for i, day in enumerate(np.arange(FIRST, FIRST + LENGTH)):\n        test_day = demand_features_eval(test[(test.d <= day) & (test.d >= day - max(LAGS) - max(WINDOWS))])\n        test.loc[test.d == day, \"demand\"] = fit.predict(test_day[x]) * 1.03 # https:\/\/www.kaggle.com\/kyakovlev\/m5-dark-magic\n    \n    # Prepare for reshaping\n    test = test.assign(id=test.id + \"_\" + np.where(test.d < FIRST + LENGTH, \"validation\", \"evaluation\"),\n                       F=\"F\" + (test.d - FIRST + 1 - LENGTH * (test.d >= FIRST + LENGTH)).astype(\"str\"))\n    \n    # Reshape\n    submission = test.pivot(index=\"id\", columns=\"F\", values=\"demand\").reset_index()[cols].fillna(1)\n    \n    # Export\n    submission.to_csv(file, index=False, mode='w' if first else 'a', header=first)\n    \n    return True","ddcd8d17":"for i, dept in enumerate(np.unique(sales.dept_id)):\n    train, valid, test, x = prep_data(sales, 1150, dept)\n    fit = fit_model(train, valid, dept)\n    pred_to_csv(fit, test, x, first=(i==0))","01d8e87a":"## Prepare calendar data","323a8811":"## The loop\n\nFor each department id, we will now prepare the model data set (including reshaping of sales data and join of the other data sources) and fit boosted trees individually for this department id. The results are then written to a csv for submission.","7da66e52":"# M5 Forecast: Dept by Dept and Step by Step\n\nCurrently, the best public kernels ([Python](https:\/\/www.kaggle.com\/kneroma\/m5-first-public-notebook-under-0-50), [R](https:\/\/www.kaggle.com\/kailex\/m5-forecaster-v2)) reach results of about 0.5. \n\nThese fantastic notebooks (don't forget to vote them up!) are limited by two factors:\n\n1. RAM usage is critical and prevents playing with more features.\n\n2. The step-by-step predictions for the submission are slow.\n\nThis notebook solves the two issues by using these tricks:\n\n- Each of the seven departments (FOODS_1 etc.) are treated separately in a loop. This **reduces memory requirement by a large factor**.\n\n- Each step-by-step prediction requires just one row per `item_id` to predict - no need to calculate computer intensive *rolling* features for many rows during each step. This **speeds up the predictions by large factor**.\n\n**This code is for playing - it is not meant to be the perfect solution**. Especially the choice of LightGBM parameters can be optimized. \n\nIt additionally reveals how hard\/easy each department is to predict. \"FOODS_3\" is particularly hard. Furthermore, the variable importances differ substantially across `dept_id`.","844a73af":"## Helper functions\n\nWe need the following functions.","66707d03":"## Load data"}}