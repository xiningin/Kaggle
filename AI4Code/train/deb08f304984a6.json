{"cell_type":{"f4f33640":"code","f867ee40":"code","6a5e7487":"code","cfb3f037":"code","e27afa99":"code","ba81e235":"code","a9a99b44":"code","be7133ca":"code","628313cd":"code","49013548":"code","ddd05d59":"code","d378433b":"code","b604c936":"code","08e9ca3c":"code","825e047a":"code","b011eb5f":"code","396fe16c":"code","bee1702e":"code","2980a951":"code","845bd482":"code","d0ae6624":"code","205e9f97":"code","9d4d4882":"code","a58f94f5":"code","9508c11f":"code","33493ada":"code","230d7f39":"code","ad3695a7":"code","3bf8f1d6":"code","1436f8c2":"code","d443aff0":"code","6444d7bb":"code","0aeeb1dd":"code","3ce41035":"code","70d10e10":"code","eba557ad":"code","52cb94e3":"code","b2570bcb":"code","c63a079b":"code","bdff857a":"code","72b808cd":"code","6ba58513":"code","24f75789":"code","b408df49":"code","6d5b4d47":"code","a1b40d5f":"code","a01222ed":"code","47351695":"code","e8aa5ea8":"code","ddd5f3e9":"code","45779c3d":"code","8c0b6c4a":"code","4a2b9448":"code","74127d4c":"code","6ceff39b":"code","4a095f6e":"code","4cefd51b":"code","aa5025d2":"code","27f0b688":"code","f8b1db23":"code","e7728c5a":"code","bc8bcfe5":"code","67cd64bc":"code","a89d2630":"code","4e5ddc6d":"code","7adbebed":"code","d61c6e21":"code","b616e83a":"code","955505dc":"code","7bf7f18f":"code","f8f28575":"code","62ddbb95":"code","6b111739":"code","98211aaa":"code","3b636400":"code","69fafa0f":"code","a4ffba4d":"code","7850c7a2":"code","756fdff0":"code","c848d667":"code","2eb9cf82":"code","8345167a":"code","d60e7337":"code","b387bdc2":"code","723985c1":"code","9ad31689":"code","ff4270c8":"code","513f5c75":"code","03fccd15":"code","f05d841f":"code","5312f3b2":"code","3118cc9a":"code","0a4e2dfd":"markdown","b9cccfbf":"markdown","fee399f4":"markdown","a615d820":"markdown","0abb900a":"markdown","14d88c16":"markdown","3e5e6093":"markdown","e9a6c066":"markdown","370e4bee":"markdown","672b8453":"markdown","934db9b7":"markdown","186b2422":"markdown","b632a73b":"markdown","b4e28233":"markdown","7e6096d4":"markdown","3926c61c":"markdown","787bcaed":"markdown","21d7be29":"markdown","a5795cc1":"markdown","c673184c":"markdown"},"source":{"f4f33640":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f867ee40":"!pip uninstall fastai -y\n!pip install fastai==0.7.0\n!pip list | grep fast","6a5e7487":"from fastai.imports import *\nfrom fastai.structured import *\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom pandas_summary import DataFrameSummary\nfrom IPython.display import display\n\nfrom sklearn import metrics","cfb3f037":"df_raw = pd.read_csv('..\/input\/train\/Train.csv', low_memory=False, parse_dates=['saledate'])","e27afa99":"def display_all(df):\n    with pd.option_context('display.max_rows', 1000):\n        with pd.option_context('display.max_columns', 1000):\n            display(df)","ba81e235":"display_all(df_raw.tail().transpose())","a9a99b44":"df_raw.SalePrice = np.log(df_raw.SalePrice)","be7133ca":"train_cats(df_raw)","628313cd":"df_raw.UsageBand.cat.categories","49013548":"df_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'], ordered=True, inplace=True)","ddd05d59":"os.makedirs('tmp', exist_ok=True)\ndf_raw.to_feather('tmp\/raw')","d378433b":"add_datepart(df_raw, 'saledate')","b604c936":"df, y, nas = proc_df(df_raw, 'SalePrice')","08e9ca3c":"m = RandomForestRegressor(n_jobs=-1)\nm.fit(df, y)","825e047a":"m.score(df, y)","b011eb5f":"def split_vals(a, n): return a[:n].copy(), a[n:].copy()\n\nn_valid = 12000 # Same as kaggle's test set size\nn_trn = len(df)-n_valid\nraw_train, raw_valid = split_vals(df_raw, n_trn)\nX_train, X_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","396fe16c":"def rmse(x, y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n              m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'):\n        res.append(m.oob_score_)\n    print(res)","bee1702e":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","2980a951":"df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice', subset=30000)\nX_train, _ = split_vals(df_trn, 20000)\ny_train, _ = split_vals(y_trn, 20000)","845bd482":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","d0ae6624":"n = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","205e9f97":"# Draw tree here\nm = RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","9d4d4882":"# Bagging\n# Training multiple trees with rows chosen at random\n# So that each tree has a different insight on the data\nm = RandomForestRegressor(n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","a58f94f5":"preds = np.stack([t.predict(X_valid) for t in m.estimators_])\npreds[:,0], np.mean(preds[:,0]), y_valid[0]","9508c11f":"# Our forest predicted 9.3, real value was 9.1\npreds.shape","33493ada":"plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)])","230d7f39":"# As we see adding more and more trees mean increasing the r_sqaured\n# Let's try adding more trees\nm = RandomForestRegressor(n_estimators=20, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","ad3695a7":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","3bf8f1d6":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","1436f8c2":"df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')\nX_train, X_valid = split_vals(df_trn, n_trn)\ny_train, y_valid = split_vals(y_trn, n_trn)","d443aff0":"set_rf_samples(20000)","6444d7bb":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)\nprint_score(m)","0aeeb1dd":"reset_rf_samples()","3ce41035":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","70d10e10":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","eba557ad":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","52cb94e3":"set_rf_samples(50000)","b2570bcb":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","c63a079b":"%time preds = np.stack([t.predict(X_valid) for t in m.estimators_])\n# mean, std. deviation\nnp.mean(preds[:, 0]), np.std(preds[:, 0])","bdff857a":"def get_preds(t): return t.predict(X_valid)\n# parallel_trees is a fast ai function that get help you run trees in parallel\n%time preds = np.stack(parallel_trees(m, get_preds))\nnp.mean(preds[:, 0]), np.std(preds[:, 0])","72b808cd":"x = raw_valid.copy()\nx['pred_std'] = np.std(preds, axis=0)\nx['pred'] = np.mean(preds, axis=0)\nx.Enclosure.value_counts().plot.barh();","6ba58513":"flds = ['Enclosure', 'SalePrice', 'pred', 'pred_std']\nenc_summ = x[flds].groupby('Enclosure', as_index=False).mean()\nenc_summ","24f75789":"enc_summ = enc_summ[~pd.isnull(enc_summ.SalePrice)]\nenc_summ.plot('Enclosure', 'SalePrice', 'barh', xlim=(0, 11))","b408df49":"enc_summ.plot('Enclosure', 'pred', 'barh', xerr='pred_std', alpha=0.6, xlim=(0, 11))","6d5b4d47":"raw_valid.ProductSize.value_counts().plot.barh()","a1b40d5f":"flds = ['ProductSize', 'SalePrice', 'pred', 'pred_std']\nsumm = x[flds].groupby('ProductSize').mean()\nsumm","a01222ed":"(summ.pred\/summ.pred_std).sort_values(ascending=False)","47351695":"fi = rf_feat_importance(m, df_trn)\nfi[:10]","e8aa5ea8":"fi.plot('cols', 'imp', figsize=(10, 6), legend=False)","ddd5f3e9":"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","45779c3d":"plot_fi(fi[:30])","8c0b6c4a":"to_keep = fi[fi.imp > 0.005].cols; len(to_keep)","4a2b9448":"df_keep = df_trn[to_keep].copy()\nX_train, X_valid = split_vals(df_keep, n_trn)","74127d4c":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","6ceff39b":"fi = rf_feat_importance(m, df_keep)\nplot_fi(fi)","4a095f6e":"df_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(df_trn2, n_trn)\n\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","4cefd51b":"fi = rf_feat_importance(m, df_trn2)\nplot_fi(fi[:25])","aa5025d2":"from scipy.cluster import hierarchy as hc","27f0b688":"corr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16, 12))\ndendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)\nplt.show()","f8b1db23":"def get_oob(df):\n    m = RandomForestRegressor(n_estimators=30, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)\n    x, _ = split_vals(df, n_trn)\n    m.fit(x, y_train)\n    return m.oob_score_","e7728c5a":"get_oob(df_keep)","bc8bcfe5":"for c in ('saleYear', 'saleElapsed', 'fiModelDesc', 'fiBaseModel', 'Grouser_Tracks', 'Coupler_System'):\n    print(c, get_oob(df_keep.drop(c, axis=1)))","67cd64bc":"to_drop = ['saleYear', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(df_keep.drop(to_drop, axis=1))","a89d2630":"df_keep.drop(to_drop, axis=1, inplace=True)\nX_train, X_valid = split_vals(df_keep, n_trn)","4e5ddc6d":"np.save('tmp\/keep_cols.npy', np.array(df_keep.columns))","7adbebed":"keep_cols = np.load('tmp\/keep_cols.npy', allow_pickle=True)\ndf_keep = df_trn[keep_cols]","d61c6e21":"reset_rf_samples()","b616e83a":"m = RandomForestRegressor(n_estimators=40, max_features=0.5, min_samples_leaf=3, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","955505dc":"from pdpbox import pdp\nfrom plotnine import *","7bf7f18f":"set_rf_samples(50000)","f8f28575":"df_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(df_trn2, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","62ddbb95":"??plot_fi","6b111739":"plot_fi(rf_feat_importance(m, df_trn2)[:10])","98211aaa":"df_raw.plot('YearMade', 'saleElapsed', 'scatter', alpha=0.1, figsize=(10,8));","3b636400":"x_all = get_sample(df_raw[df_raw.YearMade>1960], 300)","69fafa0f":"ggplot(x_all, aes('YearMade', 'SalePrice')) + stat_smooth(se=True)","a4ffba4d":"x = get_sample(X_train[X_train.YearMade>1930], 500)","7850c7a2":"??pdp.pdp_isolate","756fdff0":"def plot_pdp(feat, clusters=None, feat_name=None):\n    feat_name = feat_name or feat\n    p = pdp.pdp_isolate(m, x, x.columns, feat)\n    return pdp.pdp_plot(p, feat_name, plot_lines=True,\n                       cluster=clusters is not None, n_cluster_centers=clusters)","c848d667":"plot_pdp('YearMade')","2eb9cf82":"plot_pdp('YearMade', clusters=5)","8345167a":"feats = ['saleElapsed', 'YearMade']\np = pdp.pdp_interact(m, x, x.columns, feats)\npdp.pdp_interact_plot(p, feats)","d60e7337":"plot_pdp(['Enclosure_EROPS w AC', 'Enclosure_EROPS', 'Enclosure_OROPS'], 5, 'Enclosure')","b387bdc2":"df_raw.YearMade[df_raw.YearMade<1950] = 1950\ndf_keep['age'] = df_raw['age'] = df_raw.saleYear - df_raw.YearMade","723985c1":"X_train, X_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\nm.fit(X_train, y_train)\nplot_fi(rf_feat_importance(m, df_keep))","9ad31689":"!pip install treeinterpreter\nfrom treeinterpreter import treeinterpreter as ti","ff4270c8":"df_train, df_valid = split_vals(df_raw[df_keep.columns], n_trn)","513f5c75":"row = X_valid.values[None, 0]; row","03fccd15":"prediction, bias, contributions = ti.predict(m, row)","f05d841f":"prediction[0], bias[0]","5312f3b2":"[o for o in zip(df_keep.columns, df_valid.iloc[0], contributions[0])]","3118cc9a":"contributions[0].sum()","0a4e2dfd":"# Reducing over fitting\nInstead of creating bags from a subset, why not give each tree access to the complete dataset","b9cccfbf":"Now we understand, there are data points that have year made in 1000s. We can assume that no Bulldozers were made in 1000\nSo this may be a way to handle empty or unknown values","fee399f4":"Another way to reduce overfitting is restrict min_samples_leaf.\nWe tell the tree to stop spliting when it has 3 leafs.\n3, 5, 7 are good values to try with but for large datasets it can be hunderds or thousands","a615d820":"# Partial Dependance","0abb900a":"Another idea is your trees should not be correlated with each other\nTherefore in addition to taking a samples of rows we also take samples of columns.\nThis way we can reduce correaltion between columns and get a better result\n\nHere we use max_features=0.5 \nWhich mean randomly pick half of the columns","14d88c16":"In this case the model got worse, but gave us another insight that Enclosure_EROPS is the most importance thing, even more than YearMade!\n\nNow you can know what EROPS mean and why is it so important.","3e5e6093":"# Confidence based on tree variance","e9a6c066":"So this looks good, let's use this dataframe from now on!","370e4bee":"# Tree interpreter","672b8453":"Now, we know that we take mean of predictions from different trees and that minimizes our error, but what if we come across a row which is new to the trees and most of the trees give wrong insights.\n\nTo tackle this problem, we take standard deviation of our predictions and see if our std. dev. is high, we know that this is a row that our forest has not seen before!","934db9b7":"# Feature Importance (Most important)","186b2422":"# Removing redundant variables\nOne thing that makes it harder to interpret the data is if variables with very similar meanings exist in the dataset. So we try to remove redundant variables.","b632a73b":"When we pass oob_score as True, the model takes all the rows (which were left out randomly) to create a validation dataset for each tree and then averages them to get the accuracy!","b4e28233":"# One-hot encoding\n\nmax_n_cats=7 means one-hot enocde every category with number of cats less than 7.","7e6096d4":"We observe that even though we are doubling the number of trees, the r_sqaured on validation set is still 0.7\n\nSo after a point adding more and more trees does not make sense. It will never get worse but it's not getting worse","3926c61c":"# Tree building parameters","787bcaed":"And let's see how the model performs on full dataset.","21d7be29":"One important piece of information is how old was the Bulldozer which was sold.\nSo we plot YearMade and saleElapsed on a scatter plot","a5795cc1":"# Out-of-bag (OOB) score","c673184c":"We notice here that importance for Coupler System reduced drastically after removing less important feature. That is because there must be some co relation with some features that spiked by the importance of Coupler System. \nThat is why I trust the new dataset better than the previous dataset."}}