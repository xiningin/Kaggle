{"cell_type":{"11b07715":"code","b8cd3dfe":"code","61d4bf3d":"code","90b41c03":"code","93c44667":"code","7e9c60a6":"code","b4917e1b":"code","0d69ad80":"code","e01b6799":"code","c3c44c42":"code","7cf7b16a":"code","91922ba4":"code","6f1e2b68":"code","4f498e1d":"code","522090f6":"code","09266ba2":"code","c514d06a":"code","981c8e6b":"code","d55dad07":"code","1f1f3063":"code","3f97be7d":"code","242bfde4":"code","8a26a3e2":"code","a9868b4f":"code","21c7f7bd":"markdown","d110d2f6":"markdown","a80988cd":"markdown","77edc7b4":"markdown","b3ff4b8a":"markdown","108cb7d7":"markdown","4b4ac1df":"markdown","26deae0b":"markdown","65c17d49":"markdown","9b0419da":"markdown","9b04a2cc":"markdown","1bd76168":"markdown"},"source":{"11b07715":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport json\nimport os\nfrom tqdm import tqdm,tqdm_notebook","b8cd3dfe":"# R\u00e9cuperer 7500 articles depuis le dossier pdf_json\n\npath_data = '..\/input\/CORD-19-research-challenge\/document_parses\/pdf_json'\ncount = 0\ndocs = []\nfor file in tqdm(os.listdir(path_data)):\n    file_path = f\"{path_data}\/{file}\"\n    j = json.load(open(file_path,\"rb\"))\n    paper_id = j['paper_id']\n    # minimizing the id\n    paper_id = paper_id[-7:]\n    title = j['metadata']['title']\n\n    try: \n        abstract = j['abstract'][0]['text']\n    except:\n        abstract = \"\"\n\n    full_text = \"\"\n    bib_entries = []\n    \n    for txt in j['body_text']:\n        full_text += txt['text']\n\n    docs.append([paper_id, title, abstract, full_text])\n\n    count += 1\n\n    if (count >= 5000) :\n        break\n","61d4bf3d":"# R\u00e9cuperer 7500 articles depuis le dossier pmc_json\n\n\npath_data = '..\/input\/CORD-19-research-challenge\/document_parses\/pmc_json'\ncount = 0\nfor file in tqdm(os.listdir(path_data)):\n    file_path = f\"{path_data}\/{file}\"\n    j = json.load(open(file_path,\"rb\"))\n    paper_id = j['paper_id']\n    # minimizing the id\n    paper_id = paper_id[-7:]\n    title = j['metadata']['title']\n\n    try: \n        abstract = j['abstract'][0]['text']\n    except:\n        abstract = \"\"\n\n    full_text = \"\"\n    bib_entries = []\n    \n    for txt in j['body_text']:\n        full_text += txt['text']\n\n    docs.append([paper_id, title, abstract, full_text])\n\n    count += 1\n\n    if (count >= 5000) :\n        break","90b41c03":"# Ici on va cr\u00e9er un DataFrame o\u00f9 on va regrouper tous ce qu'on a r\u00e9cuperer pour faciliter la manipulation de tout ces donn\u00e9es\n\n# Create dataframe containing the files we gathered \nmy_data = pd.DataFrame(docs,columns=['paper_id','title','abstract','body'])\nmy_data.head()","93c44667":"len(my_data)","7e9c60a6":"topic = ['covid','covid19','corona','coronavirus','corona-virus','SARS','SARSCOV2','severe acute resperatory syndrom']\n\nlabels = []\n\nfor abst in tqdm(my_data[\"body\"]):\n    if any(x in abst for x in topic):\n        labels.append(1)\n    else :\n        labels.append(0)\n        \nmy_data['labels'] = labels\n\nmy_data.drop(my_data.index[my_data['labels']==0], inplace = True)\n\n\nlen(my_data)","b4917e1b":"# getting rid of non english articles \n!pip install langdetect\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\nDetectorFactory.seed = 0\n\nfor body in tqdm(my_data['body']):\n    try:\n        if detect(body) != \"en\":\n            my_data.drop(my_data.index[my_data['body']==body], inplace = True)\n    except:\n        my_data.drop(my_data.index[my_data['body']==body], inplace = True)\n\n\nlen(my_data)\n","0d69ad80":"# Ici on va devoir analyser les donn\u00e9es par savoir les nombre des mots dans le r\u00e9sum\u00e9 et le body l'article\n\n#my_data[\"nb_mot_abstract\"] = my_data[\"abstract\"].apply(lambda phrase: len(phrase.strip().split()))\nmy_data[\"nb_mot_body\"] = my_data[\"body\"].apply(lambda phrase: len(phrase.strip().split()))\n#my_data[\"nb_mot_body_unique\"] = my_data[\"body\"].apply(lambda phrase: len(set(phrase.strip().split())))\n\nmy_data.head()","e01b6799":"# Delete rows with less than 200 words in full text\nmy_data.drop(my_data.index[my_data['nb_mot_body'] <= 200], inplace = True)\nlen(my_data)","c3c44c42":"my_data[\"body\"] = my_data[\"body\"].str.lower()\n\nmy_data.head()","7cf7b16a":"# Reduce dimensionnality \n# with this big data (after toeknization) we'll have a lot of words, so in order to accelerate the learning of Neural Network\n# We'll eliminate the unimportant words called stopwords (ex : 'the', 'is' ...)\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\n# t\u00e9l\u00e9chargement les stopwords et ajouter d'autres\nstopwords_custom = nltk.corpus.stopwords.words('english')\n\n\"\"\"\nAjouter d'autres mots qui peuvent \u00e9tres absents dans la liste mais qui peuvent \u00eatres fr\u00e9quement utilis\u00e9s dans les \narticles scientifiques\n\"\"\" \n\nstopwords_custom.extend(\n                        ['common','review','describes','abstract','retrospective','chart','patients','study','may', 'g', 'show',\n                        'associated','results','including','high','found','one','well','among','abstract','provide', 'e', 'shown',\n                        'objective','background','range','features','participates','doi', 'preprint', 'copyright', 'many',\n                        'org', 'https', 'et','al', 'author', 'figure', 'table', 'rights', 'reserved', 'figures', 'reported',\n                        'permission', 'use', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'thu',\n                        'elsevier', 'pmc', 'czi', 'editor', 'brazil', 'article', 'figures', 'tables', \"the\", 'a', 'all', 'thus',\n                        'pubmed', 'editors', 'authors', 'methods', 'method', 'result', 'paper', 'introduction', 'editor', \n                         'although', 'letter', 'reviews', 'papers', 'tables', 'addition', 'example', 'even', 'within', 'report']\n                        )\n\n","91922ba4":"# \u00e9limination des ponctuations ( ? ; , \"\" ....)\nfrom nltk.tokenize import RegexpTokenizer\n\nnew_data = pd.DataFrame()\ntokenizer_pattern = RegexpTokenizer('\\w+')\nnew_data['text'] = my_data['body'].apply(lambda x: \" \".join(tokenizer_pattern.tokenize(x.lower())))\nnew_data.head()","6f1e2b68":"# Elimination des stop words de la data,\n\nnew_data['text'] = new_data['text'].apply(lambda x: \" \".join([word for word in x.split() if word not in (stopwords_custom)]))\nnew_data.head()","4f498e1d":"from wordcloud import WordCloud\n\n# Join the different processed titles together.\nlong_string = ','.join(list(new_data['text'].values))\n\n# Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=500, contour_width=3, contour_color='steelblue')\n\n# Generate a word cloud\nwordcloud.generate(long_string)\n\n# Visualize the word cloud\nwordcloud.to_image()","522090f6":"import gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel","09266ba2":"# finalement on va faire une tokenization des textes \n\ndata = new_data['text'].values.tolist()\n\n\ndef tokenize_and_clean(data):\n    for d in data:\n        yield(gensim.utils.simple_preprocess(str(d),deacc=True))\n        \nwords = list(tokenize_and_clean(data))\n\nwords[:1]","c514d06a":"bigram = gensim.models.Phrases(words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[words], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram\/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[words[0]]])","981c8e6b":"# D\u00e9finition des fonctions utiles\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","d55dad07":"import spacy\n\n# C\u00e9ation des BiGrams\n\nbigrams = make_bigrams(words)\n\nnlp = spacy.load('en', disable=['parser','ner'])\nnlp.max_length = 10000000\n\n# lemmatization \n\nlemmatized = lemmatization(bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nlemmatized[:1]","1f1f3063":"from gensim.corpora import Dictionary\n\nid2word = corpora.Dictionary(lemmatized)\n\ntexts = lemmatized\n\ncorpus = [id2word.doc2bow(text) for text in texts]\n\ncorpus[:1]","3f97be7d":"lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=20, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","242bfde4":"from pprint import pprint\n\n# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","8a26a3e2":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","a9868b4f":"import pyLDAvis\nimport pyLDAvis.gensim  \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","21c7f7bd":"cr\u00e9ation des mod\u00e8le de Bi-Gram et Tri-Gram ","d110d2f6":"# BUILDING THE MODEL","a80988cd":"\u00e0 ce stade il nous reste que tokeniser les textes comme une derni\u00e8re \u00e9tape, et puis on peut construire notre mod\u00e8le.","77edc7b4":"Une petite visualisation des mots les plus fr\u00e9quents dans notre texte","b3ff4b8a":"Pendant notre travail sur ces articles, on a remarqu\u00e9 qu'il y a des articles qui ne parle pas du COVID, alors on a d\u00e9cider d'effectuer une recherche sur les termes relatives au COVID afin de garder seulment les articles relatives","108cb7d7":"Dans notre \u00e9tude on va se limiter seulement sur le texte de l'article qui est r\u00e9ferenci\u00e9 dans le DataFrame par \"body\", on garder seulement les articles dont le nombre de mots d\u00e9passe 200, sinon on consid\u00e8re l'article incomplet","4b4ac1df":"En ce qui suit, on va essayer d'\u00e9liminer les articles non anglais, car ces derneiers peuvent affecter notre mod\u00e8le","26deae0b":"lemmatization des mots pour que le mod\u00e8le comprend les diff\u00e9rentes variations des mots","65c17d49":"on va tout d'abord transformer les textes en miniscule pour rechercher sans perte de donn\u00e9es, et aussi pour \u00e9viter la sensibilit\u00e9 des mod\u00e8les \u00e0 la casse par exemple pour un mod\u00e8le d'apprenstissage automatique **Youssef != youssef**","9b0419da":"Maintenant il est temps de r\u00e9duire la dimensionalit\u00e9 de notre donn\u00e9es, puisque avec ce nombre immense de mots, il est indispensable de se concentrer sur les mots important","9b04a2cc":"# DATA CLEANING & PREPARATION","1bd76168":"Calcule de nombre de mots des articles pour choisir les articles qui sont riches en mots"}}