{"cell_type":{"9460c661":"code","9911abe3":"code","56f11a2a":"code","771ef7e0":"code","e642b862":"code","a0d6a90a":"code","e7183b03":"code","7bb52d44":"code","6f9db484":"code","f5efd25d":"code","9db1a999":"code","833f3e17":"code","8f323156":"code","d26b6dee":"code","76872596":"code","5f22987b":"code","333110d4":"code","91453bc5":"code","97f3fc5c":"code","53bfec16":"code","da722170":"code","78839960":"code","6de00832":"code","8ab7ccc1":"code","2118adcc":"code","53ca2e41":"code","74574cad":"code","da49b130":"code","6a7dee53":"code","c5132057":"code","089dbb0a":"code","71222bc5":"code","f144b2d8":"code","eadcc414":"code","bf719a26":"code","29c62165":"code","4c74744c":"code","4d55f9f1":"code","0d5829aa":"code","6ae4f443":"code","4f09d8ec":"code","6ade00b5":"code","d57baaaa":"code","5a83c57e":"code","82d4cda4":"code","a1bfdc3a":"code","8d227c70":"code","f962dc43":"code","8ef8bb52":"code","57ce7703":"code","f8d082cd":"code","195bf6ca":"code","b01434f2":"code","24b5d552":"code","94af7b93":"code","791f8d93":"code","0460a2cb":"code","7fc72f12":"code","9886d4fe":"code","e74e3547":"code","dc90d44d":"code","b6e56854":"code","dc420010":"code","d33154c8":"code","17bd233d":"code","0b7cf4d1":"code","292ed484":"code","d935af79":"code","dce02c5b":"code","6ce23e2b":"code","990f5501":"code","bdb5a306":"code","c97a11b6":"code","22452531":"code","87fee726":"code","26cd4e0c":"code","bee0dcbd":"code","d2773f45":"code","154264f2":"code","e48e01af":"code","b663861d":"code","800316a0":"code","cd952c4c":"code","c9643f4d":"code","7f2da682":"markdown","eace7469":"markdown","f2697bde":"markdown","7abaff2b":"markdown","25c89bd4":"markdown","fddd6f68":"markdown","b1c58b8c":"markdown","615a7ac7":"markdown","4125879a":"markdown","b686055b":"markdown","175c1d8d":"markdown","65271c0e":"markdown","8f198bfb":"markdown","5eb40587":"markdown","6c206b43":"markdown","f0c39198":"markdown","b5e4a32d":"markdown","12eb40f3":"markdown","eb47ac3c":"markdown","5369638f":"markdown","9d4c785f":"markdown","0f71806e":"markdown","195d62bc":"markdown","322b6959":"markdown","cdfe129e":"markdown","a9afac64":"markdown","d31be16e":"markdown","85a5ffd7":"markdown","3a9aecab":"markdown","29793596":"markdown","c059694c":"markdown","dc5223b6":"markdown"},"source":{"9460c661":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy             as np \nimport pandas            as pd \nimport matplotlib.pyplot as plt\nimport seaborn           as sns\n\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9911abe3":"#Imporitng the data into enviroment \nleads = pd.read_csv(r'..\/input\/leadscore\/Leads.csv')\npd.set_option('display.max_columns',None)\nleads.head()","56f11a2a":"#Analyzing the data \nleads.info()","771ef7e0":"#Analyzing the null values \nimport missingno as msno\nmsno.matrix(leads)\nmsno.bar(leads)","e642b862":"#Function to Check the null values in terms of perecentage \ndef null_values_check(leads):\n    null_values    = round((leads.isnull().sum()\/len(leads) * 100),2).to_frame().rename(columns={0:'Null_values_percentage'})\n    null_values    = pd.DataFrame(null_values)\n    null_values.reset_index(inplace=True)\n    return null_values.sort_values(by='Null_values_percentage',ascending=False)","a0d6a90a":"#Function to impute the null values with most frequent values \ndef impute(df):\n    from sklearn.impute import SimpleImputer\n    my_imputer           = SimpleImputer(strategy='most_frequent')\n    imputed_data         = pd.DataFrame(my_imputer.fit_transform(df))\n    imputed_data.columns = df.columns\n    return imputed_data","e7183b03":"#Checking the null values \nnull_values_check(leads)[:17]","7bb52d44":"#Eliminating the columns having 45 or more than 45% of the null values \nprint('The shape of leads df before deleting columns:{}'.format(leads.shape))\ncol_eliminated = ['Lead Quality','Asymmetrique Activity Index','Asymmetrique Profile Score',\n                 'Asymmetrique Activity Score','Asymmetrique Profile Index']\nleads.drop(columns=col_eliminated,axis=1,inplace=True)\nprint('The shape of leads df after deleting columns:{}'.format(leads.shape))","6f9db484":"#Analysing the tag,Tags, Lead Profile, What matters most to you in choosing a course,\n#What is your current occupation,Country,How did you hear about X Education,Specialization,City\nleads['Lead Profile'].value_counts()","f5efd25d":"leads['What matters most to you in choosing a course'].value_counts()","9db1a999":"leads['What is your current occupation'].value_counts()","833f3e17":"leads['Country'].value_counts()[:13]","8f323156":"leads['How did you hear about X Education'].value_counts()","d26b6dee":"leads['Specialization'].value_counts()","76872596":"leads['City'].value_counts()","5f22987b":"#Imputing the null values with the most frequently occuring null values \nleads = impute(leads)","333110d4":"#Checking the null values again to verify  \nnull_values_check(leads)[:5]","91453bc5":"#Checking the target column to check if our data is balanced or imbalanced \n#Exploring the Target variable\nsns.countplot(x='Converted',data=leads);","97f3fc5c":"#Checking the percentage the target values\nround(leads['Converted'].value_counts()\/len(leads['Converted'])*100,2)","53bfec16":"#Function to analyse the categorical variables wrt target variable\ndef eda(col_name1,col_name2,df,l,b):\n    plt.figure(figsize=(l,b))\n    g = sns.countplot(x=col_name1,hue=col_name2,data=df)\n    g.set_xticklabels(labels=g.get_xticklabels(),rotation=90);\n    plt.legend(loc='upper right');","da722170":"eda('Lead Origin','Converted',leads,13,6)","78839960":"eda('Lead Source','Converted',leads,13,6)","6de00832":"eda('Country','Converted',leads,14,6)","8ab7ccc1":"#Function to analyse the categorical variables wrt target variable\ndef analysing(col_name,df):\n    unique   = df[col_name].unique()\n    Analysis = pd.DataFrame(columns=[col_name,'1_per','0_per','1_count','0_count','Total'])\n    Analysis[col_name] = unique\n    for value in unique:\n        Total_values = len(df[(df[col_name] == value)])\n        Analysis.loc[Analysis[col_name] ==  value,'1_per']   = round((len(df[(df[col_name] == value) & (df['Converted'] == 1)])\/Total_values)*100,2)\n        Analysis.loc[Analysis[col_name] ==  value,'0_per']   = round((len(df[(df[col_name] == value) & (df['Converted'] == 0)])\/Total_values)*100,2)\n        Analysis.loc[Analysis[col_name] ==  value,'1_count'] = len(df[(df[col_name] == value) & (df['Converted'] == 1)])\n        Analysis.loc[Analysis[col_name] ==  value,'0_count'] = len(df[(df[col_name] == value) & (df['Converted'] == 0)])\n        Analysis.loc[Analysis[col_name] ==  value,'Total']   = Total_values\n    return Analysis","2118adcc":"Analysis_1 = analysing('Country',leads)\nAnalysis_1.sort_values(by='Total',ascending=False)[:15]","53ca2e41":"eda('Specialization','Converted',leads,14,6)","74574cad":"Analysis_1 = analysing('Specialization',leads)\nAnalysis_1.sort_values(by='1_count',ascending=False)","da49b130":"eda('How did you hear about X Education','Converted',leads,14,6)","6a7dee53":"eda('What is your current occupation','Converted',leads,14,6)","c5132057":"eda('What matters most to you in choosing a course','Converted',leads,10,6)","089dbb0a":"Analysis_1 = analysing('What matters most to you in choosing a course',leads)\nAnalysis_1.sort_values(by='1_count',ascending=False)","71222bc5":"eda('Tags','Converted',leads,14,6)","f144b2d8":"Analysis_1 = analysing('Tags',leads)\nAnalysis_1.sort_values(by='Total',ascending=False)","eadcc414":"eda('Receive More Updates About Our Courses','Converted',leads,6,6)","bf719a26":"eda('Update me on Supply Chain Content','Converted',leads,6,6)","29c62165":"eda('Get updates on DM Content','Converted',leads,6,6)","4c74744c":"eda('I agree to pay the amount through cheque','Converted',leads,6,6)","4d55f9f1":"eda('Lead Profile','Converted',leads,9,6)","0d5829aa":"eda('City','Converted',leads,9,6)","6ae4f443":"eda('A free copy of Mastering The Interview','Converted',leads,9,6)","4f09d8ec":"eda('Last Notable Activity','Converted',leads,11,6)","6ade00b5":"Analysis_1 = analysing('Last Notable Activity',leads)\nAnalysis_1.sort_values(by='Total',ascending=False)","d57baaaa":"eda('Last Activity','Converted',leads,11,6)","5a83c57e":"Analysis_1 = analysing('Last Activity',leads)\nAnalysis_1.sort_values(by='Total',ascending=False)","82d4cda4":"eda('Through Recommendations','Converted',leads,11,6)","a1bfdc3a":"Analysis_1 = analysing('Through Recommendations',leads)\nAnalysis_1.sort_values(by='Total',ascending=False)","8d227c70":"eda('Do Not Email','Converted',leads,6,6)","f962dc43":"Analysis_1 = analysing('Do Not Email',leads)\nAnalysis_1.sort_values(by='Total',ascending=False)","8ef8bb52":"eda('Do Not Call','Converted',leads,6,6)","57ce7703":"Analysis_1 = analysing('Do Not Call',leads)\nAnalysis_1.sort_values(by='Total',ascending=False)","f8d082cd":"eda('TotalVisits','Converted',leads,16,6)","195bf6ca":"eda('Search','Converted',leads,6,6)","b01434f2":"eda('Magazine','Converted',leads,6,6)","24b5d552":"eda('Newspaper Article','Converted',leads,6,6)","94af7b93":"eda('X Education Forums','Converted',leads,6,6)","791f8d93":"eda('Newspaper','Converted',leads,6,6)","0460a2cb":"eda('Digital Advertisement','Converted',leads,6,6)","7fc72f12":"#We will analyse the columns under \n#Indicating whether the customer had seen the ad in any of the listed items.\n#with a different method\nleads['ad'] = leads['Digital Advertisement'] + leads['Newspaper'] + leads['X Education Forums']+ leads['Newspaper Article'] + leads['Magazine'] + leads['Search']\nleads['ad'].value_counts()","9886d4fe":"col_to_be_eliminated = ['What matters most to you in choosing a course','Receive More Updates About Our Courses',\n                       'Update me on Supply Chain Content','Get updates on DM Content',\n                       'I agree to pay the amount through cheque','Through Recommendations','Do Not Email',\n                       'Do Not Call','Digital Advertisement','Newspaper','X Education Forums',\n                       'Newspaper Article','Magazine','Search','ad']\nprint('The shape of df before deleting reduntant columns: {}'.format(leads.shape))\nprint('The no of columns to be removed:{}'.format(len(col_to_be_eliminated)))\nleads.drop(columns=col_to_be_eliminated,axis=1,inplace=True)\nprint('The shape of df after  deleting reduntant columns: {}'.format(leads.shape))","e74e3547":"#We are finally left with only 18 columns lets have a look at the df\nleads.head()","dc90d44d":"leads = leads.apply(pd.to_numeric, errors='ignore')","b6e56854":"#As we are going to build a logistic regression model we need to convert all the categorical\n#variables in numerical values\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ncat_list        = leads.select_dtypes('O').columns\n\n#Printing the no of categorical columns before conversion \nprint('Before LabelEncoding')\nprint('The no of categorical columns in dataset are {}'.format(len(leads.select_dtypes('O').columns)))\n\n#Converting the cat columns into numerical \nfor column_name in cat_list:\n    leads[column_name] = le.fit_transform(leads[column_name])\n\n#Printing the no of categorical columns before conversion \nprint('---'*30)\nprint('After LabelEncoding')\nprint('The no of categorical columns in dataset are {}'.format(len(leads.select_dtypes('O').columns)))\n","dc420010":"#Analyzing the column once more \nleads.head()","d33154c8":"#Splitting the data \nfrom sklearn.model_selection import train_test_split\ncol_list = ['Prospect ID', 'Lead Number', 'Lead Origin', 'Lead Source','TotalVisits', \n          'Total Time Spent on Website', 'Page Views Per Visit','Last Activity', 'Country', \n          'Specialization','How did you hear about X Education', 'What is your current occupation',\n          'Tags', 'Lead Profile', 'City','A free copy of Mastering The Interview', \n          'Last Notable Activity'] \n\nX = leads[col_list].copy()\ny = leads['Converted'].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)","17bd233d":"#Oversampling the dataset to get better results \nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 33)\nX_train_new, y_train_new = sm.fit_resample(X_train, y_train.ravel())\npd.Series(y_train_new).value_counts().plot.bar()","0b7cf4d1":"#Standardising the values \nfrom sklearn.preprocessing import StandardScaler\nSS = StandardScaler()\n\nX_train_new = pd.DataFrame(SS.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\nX_test      = pd.DataFrame(SS.transform(X_test), columns=X_test.columns, index=X_test.index)","292ed484":"#Function to create a table with pred values for LOGISTIC REGRESSION MODEL\ndef prediction(model_name,x_test,y_test,thre):\n    y_pred                        = model_name.predict(x_test)\n    y_pred_final                  = pd.DataFrame({'train_Prob':y_pred})\n    y_pred_final['real_op']       = y_test\n    y_pred_final['pred_op']       = y_pred_final['train_Prob'].apply(lambda x:1 if x>thre else 0)\n    return y_pred_final","d935af79":"#Function to Evaluate LOGISTIC REGRESSION MODEL based on various parameters\ndef validating_lr(y_real,y_pred):\n    from sklearn.metrics import confusion_matrix, accuracy_score\n    import seaborn as sns\n    confusion = confusion_matrix(y_pred,y_real)\n    sns.heatmap(confusion,annot=True,fmt='',cmap='Blues')\n    print('Accuracy Score',(accuracy_score(y_pred,y_real)*100))\n    TP = confusion[1,1] # true positive \n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n    TPR = round(((TP \/ float(TP+FP)*100)),2)\n    FPR = round(((FP\/ float(TN+FP)*100)),2)\n    #print('True Positive rate                         :',round((TP \/ float(TP+FP)*100)),2)\n    #print('False postive rate(predicting 1 when its 0):',round((FP\/ float(TN+FP)*100)),2)\n    print('True Positive rate                         :{}'.format(TPR))\n    print('False postive rate(predicting 1 when its 0):{}'.format(FPR))\n    print('\\n')\n    #print('Negative predictive value:',(TN \/ float(TN+ FN)*100))","dce02c5b":"#Function to Plot the ROC curve & find the optimal threshold value for LOGISTIC REGRESSION MODEL\ndef draw_roc( actual, probs ):\n    from sklearn.metrics import roc_curve,roc_auc_score\n    fpr, tpr, thresholds = roc_curve( actual, probs,drop_intermediate = False )\n    auc_score = roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    return None\n    fpr, tpr, thresholds = roc_curve( y_train_pred_final.Churn, y_train_pred_final.Churn_Prob, drop_intermediate = False )\n    draw_roc(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","6ce23e2b":"#Function to check the VIF for a set of features\ndef vif_validation(X_train):\n    from statsmodels.stats.outliers_influence import variance_inflation_factor\n    # Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n    vif = pd.DataFrame()\n    vif['Features']  = X_train.columns\n    vif['VIF']       = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n    vif['VIF']       = round(vif['VIF'], 2)\n    vif              = vif.sort_values(by = \"VIF\", ascending = False)\n    return vif","990f5501":"#Model building using statsmodel & checking the performance\n#MODEL NO 1\nimport statsmodels.api as sm\n\n#Features for model no 1\nfcol_list = ['Lead Number', 'Lead Origin', 'Lead Source','TotalVisits', \n          'Total Time Spent on Website', 'Page Views Per Visit','Last Activity', 'Country', \n          'Specialization','How did you hear about X Education', 'What is your current occupation',\n          'Tags', 'Lead Profile', 'City','A free copy of Mastering The Interview', \n          'Last Notable Activity']\n\n#Adding constants \nX_train_new_sm = sm.add_constant(X_train_new[fcol_list])\nlr   = sm.GLM(y_train_new,X_train_new_sm, family = sm.families.Binomial())\nlr_1 = lr.fit() \nlr_1.summary()","bdb5a306":"#Evaluting MODEL NO 1 on TRAIN dataset:\npred_df = prediction(lr_1,X_train_new_sm,y_train_new,0.4)\n\n#Evaluating MODEL NO 1\nvalidating_lr(pred_df['real_op'],pred_df['pred_op'])","c97a11b6":"#Evaluting MODEL NO 1 on Test dataset:\nX_test_new    = X_test[fcol_list].copy()\nX_test_new_sm = sm.add_constant(X_test_new )\n\n#Predicting the values for MODEL NO 1\npred_df = prediction(lr_1,X_test_new_sm,y_test,0.4)\n\n#Checking the Evaluation parameters\nvalidating_lr(pred_df['real_op'],pred_df['pred_op'])","22452531":"#Building MODEL NO 2 by eliminating insignificant Features\nimport statsmodels.api as sm\n\n#We will eliminate columns like 'Country','How did you hear about X Education'\n#as the pvalue is more than 0.05\n\nfcol_list = ['Lead Number', 'Lead Origin', 'Lead Source','TotalVisits', \n          'Total Time Spent on Website', 'Page Views Per Visit','Last Activity', \n          'Specialization','What is your current occupation',\n          'Tags', 'Lead Profile', 'City','A free copy of Mastering The Interview', \n          'Last Notable Activity']\n\n#Adding constants & model building\nX_train_new_sm = sm.add_constant(X_train_new_sm[fcol_list])\nlr   = sm.GLM(y_train_new,X_train_new_sm, family = sm.families.Binomial())\nlr_2 = lr.fit() \nlr_2.summary()","87fee726":"#Evaluting MODEL 2 on training dataset \npred_df = prediction(lr_2,X_train_new_sm,y_train_new,0.5)\n\n#Checking the evalution parameters \nvalidating_lr(pred_df['real_op'],pred_df['pred_op'])","26cd4e0c":"#Finding the optimal threshold value of MODEL NO 2 using ROC curve\ndraw_roc(pred_df['real_op'],pred_df['pred_op'])","bee0dcbd":"#Evaluting MODEL NO 2 on Test dataset:\nX_test_new    = X_test[fcol_list].copy()\nX_test_new_sm = sm.add_constant(X_test_new )\n\npred_df = prediction(lr_2,X_test_new_sm,y_test,0.4)\n\n#Checking the evalution parameters \nvalidating_lr(pred_df['real_op'],pred_df['pred_op'])","d2773f45":"#Evaluting MODEL NO 2 on Test dataset: using optimum threshold value of 0.2\nX_test_new    = X_test[fcol_list].copy()\nX_test_new_sm = sm.add_constant(X_test_new )\n\npred_df = prediction(lr_2,X_test_new_sm,y_test,0.2)\n\n#Checking the evalution parameters \nvalidating_lr(pred_df['real_op'],pred_df['pred_op'])","154264f2":"#Evaluating multicolinearity for features of MODEL NO 2\nvif_validation(X_train_new_sm)","e48e01af":"#Building MODEL NO 3\nimport statsmodels.api as sm\n\n#Lets eliminate 'LAST ACTIVITY' feature and check if the multicolinearity reduces\nfcol_list = ['Lead Number', 'Lead Origin', 'Lead Source','TotalVisits', \n          'Total Time Spent on Website', 'Page Views Per Visit', \n          'Specialization','What is your current occupation',\n          'Tags', 'Lead Profile', 'City','A free copy of Mastering The Interview', \n          'Last Notable Activity']\n\n#Adding constants & Model training\nX_train_new_sm = sm.add_constant(X_train_new_sm[fcol_list])\nlr   = sm.GLM(y_train_new,X_train_new_sm, family = sm.families.Binomial())\nlr_3 = lr.fit() \nlr_3.summary()","b663861d":"#Checking multicolinearity for features of MODEL NO 3\nvif_validation(X_train_new_sm)","800316a0":"#Evaluting MODEL NO 3 on TRAIN dataset\npred_df = prediction(lr_3,X_train_new_sm,y_train_new,0.4)\n\n#Checking the evalution parameters \nvalidating_lr(pred_df['real_op'],pred_df['pred_op'])","cd952c4c":"#Finding the optimal threshold value of MODEL NO 3 using ROC curve\ndraw_roc(pred_df['real_op'],pred_df['pred_op'])","c9643f4d":"#Evaluting MODEL NO 3 on Test dataset: using optimum threshold value of 0.25\nX_test_new    = X_test[fcol_list].copy()\nX_test_new_sm = sm.add_constant(X_test_new )\n\npred_df = prediction(lr_3,X_test_new_sm,y_test,0.25)\n\n#Checking the evalution parameters \nvalidating_lr(pred_df['real_op'],pred_df['pred_op'])","7f2da682":"#### 1. We will chose TPR & FPR as our metrics to evaluate the model. We have to classify the Leads correctly & reduce mis-calssification to avoid unecessary resource allocation. ","eace7469":"### As it is evident that all columns have maximum of no's in it so it wont help us in any decision making as it is reduntant we will eliminate all these columns at once\n\n### Also we can see that the Newspaper column and Newspaper article are same, It is obivious that if a customer has seen the ad in a newspaper article it has come from Newspaper","f2697bde":"### 2. DATA CLEANING","7abaff2b":"### BUILDING MODEL NO 2 BY ELIMINATING THE INSIGNIFICANT FEATURES","25c89bd4":"## 5. MODEL BUILDING","fddd6f68":"### Handling Categorical Values","b1c58b8c":"#### IMPUTING the null values with most frequent values","615a7ac7":"1. This will be our Final Model to be deployed in the Production as we are getting TPR of 91%.\n2. In our Model FPR is only 9.51% that means only (127) values are being miscalssified.\n3. Although the Accuracy of the model is only 68.5%, we have choosen TPR & FPR to be our metrics for evaluation.\n4. Our Final Model is the best model as it do not have Multicolinearlity, non-significant features.\n5. With this model we wont spend unecessary resources on the misclassified leads.\n6. Leads which are correctly classified will help in saving resources & increase the Lead conversion rate.","4125879a":"### We are getting 94% TPR and amongst them only 7% are being misclassified so overall its a good model lets further optimise the model","b686055b":"#### BUILDING MODEL NO 3 BY ELIMINATING MULTICOLINEARITY IN FEATURES","175c1d8d":"#### 1. We can observe that the MULTICOLINEARITY has been reduced below 2. We can say that our model is now the optimum model as MULTICOLINEARITY is eliminated & all features are significant.\n#### 2. The genral heuristic that we will take for checking MULTICOLINEARITY is 2 for this model building process.","65271c0e":"### Handling the Data Imbalance in the dataset using SMOTE technique","8f198bfb":"### We can observe that 99% values are assigned to one type, so this column will make our model baised so its better to eliminate it coz it wont help in any decision making","5eb40587":"### Reason why I have analysed using percentage of leads conversion, per country is some information was lost in the graph.\n\n### eg we can see that UNITED STATES graph is not even visible but its second higest country where the Leads come from followed by UAE\n","6c206b43":"### Standardizing all the numerical features","f0c39198":"#### CHECKING MULTICOLINEARITY FOR FEATURES OF MODEL NO2","b5e4a32d":"### It is evident that more than 95% of students have same values so this column will not help us in decision making, so we will eliminate this column.","12eb40f3":"1. #### We can observe that now the TARGET Variable is having equal classes. This will help our model not being baised towards one class","eb47ac3c":"### It is evident that more than 99% of students have same reason to join the course so this column will not help us in decision making, so we will eliminate this column.","5369638f":"### EVALUATION FOR MODEL NO 1","9d4c785f":"### Splitting the data into Train and Test ","0f71806e":"### The columns 'LAST NOTABLE ACTIVITY' & 'LAST ACTIVITY' are having similar values but they have different outcomes so we can retain them both even though they are similar","195d62bc":"### 1. IMPORTING DATA AND ANALYZING","322b6959":"### EVALUATION FOR MODEL NO 2","cdfe129e":"### Converting all the numeric columns to Numeric datatype\n","a9afac64":"#### 1. As we had earlier seen in the EDA Process that the 2 columns '**LAST ACTIVITY**' & **'LAST NOTABLE ACTIVITY'** is almost same.\n#### 2. We did not eliminate any feature back then coz we were not sure which column to be eliminated.\n#### 3.Now, we have checked the VIF values and the VIF value for '**LAST ACTIVITY**' is high, so we will eliminate it & later check if the MULTICOLINEARITY is reduced.\n","d31be16e":"1. #### We can observe that the Dataset is Imbalanced because the converted Leads are far more less as comapred to the not-converted leads we will handle this further","85a5ffd7":"### We can observe that 92% values are assigned to one type, so this column will make our model baised so its better to eliminate it","3a9aecab":"### Columns like this will be useful in decision making as they have various values & conversion rate differs for each one of them.","29793596":"## 4. DATA PRE-PROCESSING ","c059694c":"### 3. EXPLORATORY DATA ANALYSIS","dc5223b6":"### We can observe that the above columns have only one value, which will not help us in decision making so we need to eliminate these columns"}}