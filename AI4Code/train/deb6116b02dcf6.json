{"cell_type":{"5ddc6fa5":"code","2d26383f":"code","4d212edd":"code","df692e19":"code","7ec4b74a":"code","fa63ca57":"code","7b9d2b1f":"code","95f6bb1a":"code","042914df":"code","a473d39b":"code","63b4a3ce":"code","de830051":"code","c8b73e48":"code","1400b309":"code","5d977ffd":"code","cff07074":"code","5a0d104a":"code","8edc998a":"code","b54b9e66":"code","b60a88f2":"code","03077241":"code","519729f9":"markdown"},"source":{"5ddc6fa5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2d26383f":"import pandas as pd \nimport numpy as np                     # For mathematical calculations \nimport seaborn as sns                  # For data visualization \nimport matplotlib.pyplot as plt        # For plotting graphs \n%matplotlib inline \n\nfrom sklearn.preprocessing import LabelEncoder\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, ExtraTreesClassifier,GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nimport warnings                        # To ignore any warnings warnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\")","4d212edd":"data_train = pd.read_csv('\/kaggle\/input\/train.csv')\ntrain=data_train.copy()\ndata_test = pd.read_csv('\/kaggle\/input\/test.csv')","df692e19":"data_train.head()","7ec4b74a":"print(data_train.isna().sum())\nprint(\"Total missing values:\",data_train.isna().sum().sum())","fa63ca57":"pd.DataFrame({\"Skewness\": data_train.skew(), \"Kurtosis\": data_train.kurt()})","7b9d2b1f":"print(data_train['Gender'].value_counts())\nprint(data_test['Gender'].value_counts())","95f6bb1a":"def get_combined_data(train,test):\n    targets = train.Loan_Status\n    train.drop('Loan_Status', 1, inplace=True)\n    combined = train.append(test)\n    combined.reset_index(inplace=True)\n    combined.drop(['index', 'Loan_ID'], inplace=True, axis=1)\n    return combined\ndf=get_combined_data(data_train,data_test)","042914df":"df.head()","a473d39b":"df['LoanAmount'].fillna(df['LoanAmount'].mean(), inplace=True)\ndf['Self_Employed'].fillna('No', inplace=True)\ndf['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mean(),inplace=True)\ndf['Credit_History'].fillna(1,inplace=True)\ndf['Married'].fillna('Yes',inplace=True)\ndf['Gender'].fillna('Male',inplace=True)\ndf['Dependents'].fillna('0',inplace=True)\ndf['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']\ndf['TotalIncome_log'] = np.log(df['TotalIncome'])","63b4a3ce":"df.isna().sum().sum()","de830051":"numerical_features = data_train.select_dtypes(include=np.number)\ncategorical_features = data_train.select_dtypes(include=np.object)\nprint(\"numeric_features: \", numerical_features.columns)\nprint(\"categorical_features: \", categorical_features.columns)","c8b73e48":"var_mod=categorical_features.columns.values[1:]\nle=LabelEncoder()\nfor i in var_mod:\n        df[i]=le.fit_transform(df[i])","1400b309":"def recover_train_test_target():\n    global df, train\n    targets = train['Loan_Status'].map({'Y':1,'N':0})\n    train = df.head(614)\n    test = df.iloc[614:]\n    return train, test, targets\n\ntrain, test, targets = recover_train_test_target()","5d977ffd":"parameters = {'bootstrap': False,\n              'min_samples_leaf': 3,\n              'n_estimators': 100,\n              'min_samples_split': 10,\n              'max_features': 'sqrt',\n              'max_depth': 6}\n\nmodel = RandomForestClassifier(**parameters)\nmodel.fit(train, targets)","cff07074":"def compute_score(clf, X, y, scoring='accuracy'):\n    xval = cross_val_score(clf, X, y, cv = 10, scoring=scoring)\n    return np.mean(xval)\n\ncompute_score(model, train, targets)","5a0d104a":"X_train, X_test, y_train, y_test = train_test_split(train, targets, random_state=42, test_size=0.2)\n\nmodel = LogisticRegression()\n\ngrid = {'C':[0.001,0.01,0.1,1,5,10],\n       'penalty':['l1','l2'],\n        'class_weight':['balanced']}\n# cv = KFold(n_splits = 10, shuffle=True, random_state = 7)\nclf = GridSearchCV(model, grid, n_jobs=8, cv=None,scoring='f1_macro')\nclf.fit(X_train, y_train)","8edc998a":"clf.best_score_, clf.best_params_","b54b9e66":"model = LogisticRegression(C= 5, class_weight= 'balanced', penalty= 'l2')\nmodel.fit(X_train, y_train)\n\nprediction = model.predict_proba(X_test) # predicting on the validation set\nprediction_int = prediction[:,1] >= 0.56 # if prediction is greater than or equal to 0.3 than 1 else 0\ny_pred = prediction_int.astype(np.int)\n\nprint(\"f1_score:\",f1_score(y_test, y_pred)) # calculating f1 score\nprint(\"Accuracy on train data:\",model.score(X_train,y_train))\nprint(\"Accuracy on test data:\",model.score(X_test,y_test))","b60a88f2":"pd.crosstab(y_pred,y_test)","03077241":"print(classification_report(y_test,y_pred))","519729f9":"If skewness is less than \u22121 or greater than +1, the distribution is highly skewed. If skewness is between \u22121 and \u2212\u00bd or between +\u00bd and +1, the distribution is moderately skewed. If skewness is between \u2212\u00bd and +\u00bd, the distribution is approximately symmetric."}}