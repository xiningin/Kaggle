{"cell_type":{"53976793":"code","9bb4f4e4":"code","2bf04280":"code","536965a8":"code","7a22040a":"code","38ce5263":"code","9e078715":"code","b678f606":"code","eb5fe80f":"code","8b4c9b72":"code","8cc33eb9":"code","d2cf4e84":"code","80e88c84":"code","e6976997":"code","cf220556":"code","649a39c7":"code","e7385f33":"code","87fa9715":"code","f2b2f79c":"code","ecbe0cd5":"code","eadcb081":"code","a3b156e3":"code","79a4be30":"code","fb9b9aa0":"code","23a5611c":"code","35a3a956":"code","021ce26c":"code","8799fa6f":"code","5499941a":"code","62c705f5":"code","9104ed65":"code","12ce3b93":"code","93d62744":"code","36cfb5d3":"code","f4e4a204":"code","d02a049e":"code","9d6b8886":"code","0a52b85c":"code","e38ac136":"code","713ac45e":"code","4c36ab9c":"code","73a71d29":"code","05b379f4":"code","9cff5e3c":"code","2c81d6a2":"code","70a58aea":"code","6e5d3852":"code","61a6ce32":"code","e2be72f9":"code","2fb1fa5e":"code","5e723186":"code","5d8d702b":"code","17cb93d5":"code","2331eaa1":"code","9eac7246":"code","a32d74a0":"markdown","874392d3":"markdown","4a5d410e":"markdown","5586cdca":"markdown","264d9e42":"markdown","80605b64":"markdown","a4b274ed":"markdown","bd97d9bd":"markdown","3cc81291":"markdown","00e342e0":"markdown","34cf5d25":"markdown","52a141ad":"markdown","443bb81b":"markdown","dca7cc4f":"markdown","963211cd":"markdown","628bc768":"markdown"},"source":{"53976793":"# import packages\nimport csv\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport altair as alt\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder","9bb4f4e4":"#read data\ndf = pd.read_csv('..\/input\/obesity-levels\/ObesityDataSet_raw_and_data_sinthetic.csv')\ndf","2bf04280":"df.info()","536965a8":"#rename cols\ndf=df.rename(columns={\"Gender\": \"GENDER\", \"Age\": \"AGE\", \n                      \"Height\": \"HEIGHT\", \"Weight\": \"WEIGHT\",\n                      \"family_history_with_overweight\": \"HIST_OVERWEIGHT\",\n                      \"NObeyesdad\": \"OBESITY_LEVEL\"})","7a22040a":"print(\"   # of unique values for each column\")\nprint(\"***************************************\")\nfor column in df.columns:\n    print(f\"{column} --> {df[column].nunique()}\")\n    print(\"-------------------------\")","38ce5263":"#change data types float -> int\ndf[\"AGE\"] = df[\"AGE\"].astype(int)","9e078715":"#class distribution of the target attribute -->> see target class is BALANCED\nsns.set_style(\"dark\")\nsns.set(rc={'figure.figsize':(16,8)})\nsns.countplot(x=\"OBESITY_LEVEL\", data=df, \n              #palette=sns.color_palette(\"Paired\", 7), \n              palette=sns.cubehelix_palette(),\n              saturation=10).set(title='Obesity Levels Distribution')","b678f606":"sns.set_style(\"dark\")\nsns.countplot(x=\"AGE\", data=df, \n              palette=sns.cubehelix_palette(start=.5, rot=-.75,), \n              saturation=1).set(title='Age distribution of the dataset')","eb5fe80f":"sns.set_style(\"dark\")\nsns.countplot(x=\"GENDER\", data=df, palette=sns.dark_palette(\"#88d\", 8), \n              saturation=10, hue=\"OBESITY_LEVEL\").set(title='Gender distribution of Obesity Levels')\n\nplt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.))","8b4c9b72":"sns.catplot(x=\"GENDER\", hue=\"OBESITY_LEVEL\", col=\"SMOKE\",\n                data=df, kind=\"count\",\n                height=6, aspect=.8,\n                palette=\"rocket\")\n","8cc33eb9":"sns.catplot(x=\"GENDER\", hue=\"OBESITY_LEVEL\", col=\"CALC\",\n                data=df, kind=\"count\",\n                height=6, aspect=.8,\n                palette=\"dark:salmon_r\")","d2cf4e84":"sns.set(rc={\"font.style\":\"normal\",\n            \"text.color\":\"black\",\n            \"xtick.color\":\"black\",\n            \"ytick.color\":\"black\",\n            \"axes.labelcolor\":\"black\",\n            \"axes.grid\":False,\n            'axes.labelsize':30,\n            'figure.figsize':(12.0, 6),\n            'xtick.labelsize':25,\n            'ytick.labelsize':20})\n\nsns.set(style=\"white\",font_scale=1)\n\n\nsns.set_style(\"dark\")\nsns.countplot(x=\"MTRANS\", data=df, palette=sns.light_palette(\"salmon\"), \n              saturation=10, edgecolor=(0,0,0), linewidth=2).set(title='Transportation Used')","80e88c84":"sns.set(rc={\"font.style\":\"normal\",\n            \"text.color\":\"black\",\n            \"xtick.color\":\"black\",\n            \"ytick.color\":\"black\",\n            \"axes.labelcolor\":\"black\",\n            \"axes.grid\":False,\n            'axes.labelsize':30,\n            'figure.figsize':(12, 6),\n            'xtick.labelsize':25,\n            'ytick.labelsize':20})\n\n\nsns.set(style=\"white\",font_scale=1)\n\nsns.set_style(\"dark\")\nsns.countplot(x=\"MTRANS\", data=df, palette=sns.diverging_palette(260, 20), \n              saturation=10, edgecolor=(0,0,0), linewidth=2, hue=\"GENDER\").set(title='Transportation Used ~ Gender')","e6976997":"df","cf220556":"sns.set(rc={\"font.style\":\"normal\",\n            \"text.color\":\"black\",\n            \"xtick.color\":\"black\",\n            \"ytick.color\":\"black\",\n            \"axes.labelcolor\":\"black\",\n            \"axes.grid\":False,\n            'axes.labelsize':30,\n            'figure.figsize':(7, 7),\n            'xtick.labelsize':10,\n            'ytick.labelsize':8})\n\n\nsns.set(style=\"white\",font_scale=1)\n\nsns.set_style(\"dark\")\nsns.countplot(x=\"HIST_OVERWEIGHT\", data=df, palette=sns.diverging_palette(360, 10), \n              saturation=10, edgecolor=(0,0,0), linewidth=2, hue=\"GENDER\").set(title='Family History ~ Gender')","649a39c7":"sns.catplot(x=\"GENDER\", hue=\"HIST_OVERWEIGHT\", col=\"OBESITY_LEVEL\",\n                data=df, kind=\"count\",\n                height=6, aspect=.8,\n                palette=\"ch:s= -3.89, r= -9.2\")","e7385f33":"sns.set(rc={\"font.style\":\"normal\",\n            \"text.color\":\"black\",\n            \"xtick.color\":\"black\",\n            \"ytick.color\":\"black\",\n            \"axes.labelcolor\":\"black\",\n            \"axes.grid\":False,\n            'axes.labelsize':30,\n            'figure.figsize':(12, 6),\n            'xtick.labelsize':25,\n            'ytick.labelsize':20})\n\n\nsns.set(style=\"white\",font_scale=1)\n\nsns.set_style(\"dark\")\nsns.countplot(x=\"FAVC\", data=df, palette=sns.color_palette(\"YlOrBr\", 3), \n              saturation=10, edgecolor=(0,0,0), linewidth=2, hue=\"GENDER\").set(title='Frequent consumption of High Caloric Food ~ Gender')","87fa9715":"# library\nimport matplotlib.pyplot as plt\nfrom palettable.colorbrewer.qualitative import Pastel2_4\n\n# create data\nnames=list(df[\"CAEC\"].unique())\nsizes=[df[\"CAEC\"].value_counts()[unique_class]*100\/len(df[\"CAEC\"]) for unique_class in names]\ncolors = Pastel2_4.hex_colors\nexplode = (0, 0, 0, 0)  # explode a slice if required\n\nplt.pie(sizes, explode=explode, labels=names, colors=colors,\n        autopct='%1.1f%%', shadow=True)\n        \n#draw a circle at the center of pie to make it look like a donut\ncentre_circle = plt.Circle((0,0), 0.75, color='grey', fc='white',linewidth=1.25)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n\n# Set aspect ratio to be equal so that pie is drawn as a circle.\nplt.axis('equal')\nplt.show()","f2b2f79c":"#Codes from Gabriel Preda\n\ndef plot_count(feature, title, df, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='crest')\n    #g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()","ecbe0cd5":"plot_count(\"HEIGHT\", \"HEIGHT\", df, 4)","eadcb081":"interval = alt.selection_interval()\n\npoints = alt.Chart(df).mark_point().encode(\n  x='OBESITY_LEVEL',\n  y='CALC',\n  color=alt.condition(interval, 'SMOKE', alt.value('lightgray'))\n).properties(\n  selection=interval\n)\n\nhistogram = alt.Chart(df).mark_bar().encode(\n  x='count()',\n  y='SMOKE',\n  color='SMOKE'\n).transform_filter(interval)\n\npoints & histogram","a3b156e3":"#pivot tables\ndf.pivot_table(index =['OBESITY_LEVEL','MTRANS', 'SMOKE'])","79a4be30":"#for cat data distribution\nimport matplotlib\n\nplt.figure(figsize=(32, 32))\nmatplotlib.rc('axes', titlesize=24)#cols size\n\ncat_feature_col=[\"FAVC\", \"CAEC\", \"CALC\", \"SCC\", \"MTRANS\", \"GENDER\"]\nfor i, column in enumerate(cat_feature_col, 1):\n    plt.subplot(3, 3, i)\n    df[df[\"OBESITY_LEVEL\"] == \"Obesity_Type_III\"][column].hist(bins=20, color='darkkhaki', label='OBESITY_LEVEL = Obesity_Type_III', alpha=1)\n    df[df[\"OBESITY_LEVEL\"] == \"Normal_Weight\"][column].hist(bins=20, color='olive', label='OBESITY_LEVEL = Normal_Weight', alpha=1)\n    plt.legend(fontsize='medium')\n    plt.title(column)","fb9b9aa0":"#heatmap for correlation coefficient\n\n# calculate correlation\ndf_corr = df.corr()\n\n# correlation matrix\nsns.set(font_scale=0.8)\nplt.figure(figsize=(12,8))\nsns.heatmap(df_corr, annot=True, fmt=\".4f\",vmin=-1, vmax=1, linewidths=.5, cmap = sns.color_palette(\"vlag\", as_cmap=True))\n\n#plt.yticks(rotation=0)\nplt.show()","23a5611c":"!pip install ppscore","35a3a956":"import seaborn as sns\nimport ppscore as pps\n\nmatrix_df = pps.matrix(df).pivot(columns='x', index='y',  values='ppscore')\n\nsns.set(font_scale=0.8)\nplt.figure(figsize=(16,12))\nsns.heatmap(matrix_df, annot=True, cmap = sns.color_palette(\"YlOrBr\", as_cmap=True))\nplt.show()","021ce26c":"#outlier detection & handling (filling with mean)\ncont_feature_col=[\"FCVC\", \"NCP\", \"CH2O\", \"FAF\", \"TUE\", \"HEIGHT\", \"WEIGHT\", \"AGE\"]\ncont_df=df[cont_feature_col]\n\n# find the IQR\nq1 = df[cont_feature_col].quantile(.25)\nq3 = df[cont_feature_col].quantile(.75)\nIQR = q3-q1\n\noutliers_df = np.logical_or((df[cont_feature_col] < (q1 - 1.5 * IQR)), (df[cont_feature_col] > (q3 + 1.5 * IQR))) \n\noutlier_list=[]\ntotal_outlier=[]\nfor col in list(outliers_df.columns):\n    try:\n        total_outlier.append(outliers_df[col].value_counts()[True])\n        outlier_list.append((outliers_df[col].value_counts()[True] \/ outliers_df[col].value_counts().sum()) * 100)\n    except:\n        outlier_list.append(0)\n        total_outlier.append(0)\n        \noutlier_list\n\noutlier_df=pd.DataFrame(zip(list(outliers_df.columns), total_outlier, outlier_list), columns=['name of the column', 'total', 'outlier(%)'])\n\n#see totally how many outliers in cont features\noutlier_df.set_index('name of the column', inplace=True)\n#del outlier_df.index.name\noutlier_df","8799fa6f":"df_cont=df[cont_feature_col]\nout_nan_df=df_cont[~outliers_df]\nout_nan_df","5499941a":"for col in cont_feature_col:\n    col_mean=df[col].mean() #calculate mean for each col\n    out_nan_df[col]=out_nan_df[col].fillna(col_mean) #first convert outliers to Nan values then fill Nan's with col mean\n    #df[cont_feature_col]=df_cont","62c705f5":"df_only_cat=df.drop(columns=cont_feature_col)\n#concat df_only_cat and clear cont_df of outliers\ndf_final=pd.concat([out_nan_df, df_only_cat], axis=1)\ndf_final","9104ed65":"df_final.describe()","12ce3b93":"#split here for test - size=400\n\ndf_test=df_final.sample(n = 400)\nind=df_test.index\n\nind_list=ind.to_list()\ndf_train=df_final.drop(ind_list)\n\nprint(\"train ==> \", df_train.shape)\nprint(\"test ==> \", df_test.shape)","93d62744":"df_train.reset_index(inplace=True)\ndf_test.reset_index(inplace=True)","36cfb5d3":"df_train.reset_index(inplace=True)\ndf_train=df_train.drop(columns='index')\ndf_train=df_train.drop(columns='level_0')\n\ndf_test.reset_index(inplace=True)\ndf_test=df_test.drop(columns='index')\ndf_test=df_test.drop(columns='level_0')","f4e4a204":"df_train","d02a049e":"df_test","9d6b8886":"#encoding\nord_feature_list=[\"GENDER\", \"HIST_OVERWEIGHT\", \"SMOKE\", \"MTRANS\", \"SCC\", \"CALC\", \"CAEC\", \"FAVC\", \"OBESITY_LEVEL\"]\ndf_ord=df_train[ord_feature_list]\ncol_names_list=df_ord.columns\n\nenc = OrdinalEncoder()\nenc.fit(df_ord)\ndf_ord_arr=enc.transform(df_ord)\n\nencoded_cat_df=pd.DataFrame(df_ord_arr, columns=col_names_list)","0a52b85c":"#concat cat & cont dataframes\ncont_feature_list=[\"FCVC\", \"NCP\", \"CH2O\", \"FAF\", \"TUE\", \"HEIGHT\", \"WEIGHT\", \"AGE\"]\ndf_cont=df_train[cont_feature_list]\n\ntrain_df_final = pd.concat([encoded_cat_df, df_cont], axis=1)","e38ac136":"#X, y splitting\ny_imp = train_df_final.loc[:, 'OBESITY_LEVEL'].values\nX_imp = train_df_final.drop('OBESITY_LEVEL', axis=1)\n\n\n#feature importances\nfrom sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(n_estimators = 100, max_depth=5)\nrf_clf.fit(X_imp, y_imp)\n\npd.Series(rf_clf.feature_importances_, index = X_imp.columns).nlargest(24).plot(kind = 'pie',\n                                                                                figsize = (8, 8),\n                                                                                title = 'Feature importance from RandomForest', colormap='twilight', fontsize=10)","713ac45e":"# split df to X and Y\nfrom sklearn.model_selection import train_test_split\n\ny = train_df_final.loc[:, 'OBESITY_LEVEL'].values\nX = train_df_final.drop('OBESITY_LEVEL', axis=1)\n\n# split data into 80-20 for training set \/ test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state=100)","4c36ab9c":"#normalization(make all values bet. 0-1)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\n\nX_train_normalized_arr=scaler.transform(X_train)\nX_train_normalized_df=pd.DataFrame(X_train_normalized_arr, columns=X_train.columns.to_list())\n\nX_test_normalized_arr=scaler.transform(X_test)\nX_test_normalized_df=pd.DataFrame(X_test_normalized_arr, columns=X_test.columns.to_list())","73a71d29":"X_train_normalized_df","05b379f4":"X_test_normalized_df","9cff5e3c":"print(\"x_train: \", len(X_train_normalized_df), \" ---  y_train: \", len(y_train))\nprint(\"x_test: \", len(X_test_normalized_df), \" ---  x_test\", len(y_test))","2c81d6a2":"#import necessary libraries\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, roc_curve, confusion_matrix, classification_report, roc_auc_score\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, CategoricalNB\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier","70a58aea":"# cross-validation with 10 splits\ncv = StratifiedShuffleSplit(n_splits=10, random_state = 42, test_size=0.2)","6e5d3852":"# display test scores and return result string and indexes of false samples\ndef display_test_scores(test, pred):\n    str_out = \"\"\n    str_out += (\"\\n\\n   TEST SCORES\\n\")\n    str_out += (\"===================================================================\\n\")\n    \n    #print accuracy\n    accuracy = accuracy_score(test, pred)\n    str_out += (\"ACCURACY: {:.4f}\\n\".format(accuracy))\n    str_out += (\"\\n\")\n    str_out += (\"---------------------------------------------------\\n\")\n    \n\n    #print confusion matrix\n    str_out += (\"CONFUSION MATRIX:\\n\")\n    conf_mat = confusion_matrix(test, pred)\n    str_out += (\"{}\".format(conf_mat))\n    str_out += (\"\\n\")\n    str_out += (\"\\n\")\n    str_out += (\"---------------------------------------------------\\n\")\n    \n    #print FP, FN\n    str_out += (\"FALSE POSITIVES:\\n\")\n    fp = conf_mat[1][0]\n    pos_labels = conf_mat[1][0]+conf_mat[1][1]\n    str_out += (\"{} out of {} positive labels ({:.4f}%)\\n\".format(fp, pos_labels,fp\/pos_labels))\n    str_out += (\"\\n\")\n    str_out += (\"---------------------------------------------------\\n\")\n\n    str_out += (\"FALSE NEGATIVES:\\n\")\n    fn = conf_mat[0][1]\n    neg_labels = conf_mat[0][1]+conf_mat[0][0]\n    str_out += (\"{} out of {} negative labels ({:.4f}%)\\n\".format(fn, neg_labels, fn\/neg_labels))\n    str_out += (\"\\n\")\n    str_out += (\"--------------------------------------------------\\n\")\n\n    #print classification report\n    str_out += (\"PRECISION, RECALL, F1 scores:\\n\\n\")\n    str_out += (\"{}\".format(classification_report(test, pred)))\n    \n    false_indexes = np.where(test != pred)\n    return str_out, false_indexes","61a6ce32":"# CART decision tree\ncart = DecisionTreeClassifier(random_state = 0)\n\n# parameters \nparameters = {\n                \"criterion\": [\"gini\",\"entropy\"],\n                \"splitter\": [\"best\",\"random\"],\n                \"class_weight\": [None, \"balanced\"],\n                }\n\n# grid search for parameters\ngrid_1 = GridSearchCV(estimator=cart, param_grid=parameters, cv=cv, n_jobs=-1)\ngrid_1.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\"\n      % (grid_1.best_params_, grid_1.best_score_))\n\n# prediction results\ny_pred = grid_1.predict(X_test_normalized_df)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)","e2be72f9":"# Naive-Bayes with different approaches\nnb_list = [ GaussianNB(), MultinomialNB(), ComplementNB()]\n\nfor nb in nb_list:\n    print(\"-----------\", str(nb), \"--------------\")\n    \n    # parameters \n    parameters = {}\n\n    # grid search for parameters\n    grid_2 = GridSearchCV(estimator=nb, param_grid=parameters, cv=cv, n_jobs=-1)\n    grid_2.fit(X_train_normalized_df, y_train)\n\n    # print best scores\n    print(\"The best parameters are %s with a score of %0.4f\\n\"\n          % (grid_2.best_params_, grid_2.best_score_))\n\n    # prediction results\n    y_pred = grid_2.predict(X_test_normalized_df)\n\n    # print accuracy metrics\n    results, false = display_test_scores(y_test, y_pred)\n    print(results)","2fb1fa5e":"# SVM classifier\nsvm = SVC(tol=1e-5, random_state=0)\n\n# parameters \nparameters = {\n                'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n                'C': [0.01, 0.1, 1, 10, 100],\n                'max_iter': [100, 1000, 5000]\n            }\n\n# grid search for parameters\ngrid_3 = GridSearchCV(estimator=svm, param_grid=parameters, cv=cv, n_jobs=-1)\ngrid_3.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\\n\"\n      % (grid_3.best_params_, grid_3.best_score_))\n\n# prediction results\ny_pred = grid_3.predict(X_test_normalized_df)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)","5e723186":"knn = KNeighborsClassifier()\n# parameters \nparameters = {\n                \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n                \"n_neighbors\": [5,15,25]\n    }\n\n# grid search for parameters\ngrid_4 = GridSearchCV(estimator=knn, param_grid=parameters, cv=cv, n_jobs=-1)\ngrid_4.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\\n\"\n      % (grid_4.best_params_, grid_4.best_score_))\n\n# prediction results\ny_pred = grid_4.predict(X_test_normalized_df)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)","5d8d702b":"logit = LogisticRegression(random_state=0)\n# parameters \nparameters = {\n                \"penalty\":['l1', 'l2'],\n                \"C\": [0.01, 0.1, 1, 10, 100],\n                \"max_iter\": [100,1000,5000],\n             }\n\n# grid search for parameters\ngrid_5 = GridSearchCV(estimator=logit, param_grid=parameters, cv=cv, n_jobs=-1)\ngrid_5.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\\n\"\n      % (grid_5.best_params_, grid_5.best_score_))\n\n# prediction results\ny_pred = grid_5.predict(X_test_normalized_df)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)","17cb93d5":"rf = RandomForestClassifier(random_state=0)\n\n# parameters \nparameters = {\n                \"bootstrap\": [\"True\",\"False\"],\n                \"max_features\": [None, \"sqrt\", \"log2\"],\n                \"class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n                \"max_samples\": [None, 0.3, 0.5, 0.7, 0.9],\n                \"n_estimators\": [10, 100, 200]\n                \n}\n\n# grid search for parameters\ngrid_6 = GridSearchCV(estimator=rf, param_grid=parameters, cv=cv, n_jobs=-1)\ngrid_6.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\"\n      % (grid_6.best_params_, grid_6.best_score_))\n\n# prediction results\ny_pred = grid_6.predict(X_test_normalized_df)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)","2331eaa1":"from sklearn.ensemble import BaggingClassifier\n\nbag = BaggingClassifier(random_state=0)\n\n# parameters \nparameters = {\n                \"bootstrap\": [\"True\",\"False\"],\n                \"max_features\": [0.3, 0.5, 0.7, 0.9, 1],\n                \"max_samples\": [0.3, 0.5, 0.7, 0.9],\n                \"n_estimators\": [10, 100, 200]\n                \n}\n\n# grid search for parameters\ngrid_7 = GridSearchCV(estimator=bag, param_grid=parameters, cv=cv, n_jobs=-1)\ngrid_7.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\"\n      % (grid_7.best_params_, grid_7.best_score_))\n\n# prediction results\ny_pred = grid_7.predict(X_test_normalized_df)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)","9eac7246":"from sklearn.ensemble import ExtraTreesClassifier\n\nextra = ExtraTreesClassifier(random_state=0)\n\n# parameters \nparameters = {\n                \"bootstrap\": [\"True\",\"False\"],\n                \"max_features\": [None, \"sqrt\", \"log2\"],\n                \"class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n                \"max_samples\": [None, 0.3, 0.5, 0.7, 0.9],\n                \"n_estimators\": [10, 100, 200]\n                \n}\n\n# grid search for parameters\ngrid_8 = GridSearchCV(estimator=extra, param_grid=parameters, cv=cv, n_jobs=-1)\ngrid_8.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\"\n      % (grid_8.best_params_, grid_8.best_score_))\n\n# prediction results\ny_pred = grid_8.predict(X_test_normalized_df)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)","a32d74a0":"# Classifier-8: ExtraTressClassifier","874392d3":"# Classifier-6:RF","4a5d410e":"# Classifier-4: kNN","5586cdca":"- https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2352340919306985\n\nThis paper presents data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition. The data contains 17 attributes and 2111 records, the records are labeled with the class variable NObesity (Obesity Level), that allows classification of the data using the values of Insufficient Weight, Normal Weight, Overweight Level I, Overweight Level II, Obesity Type I, Obesity Type II and Obesity Type III.\n\n77% of the data was generated synthetically using the Weka tool and the SMOTE filter, 23% of the data was collected directly from users through a web platform. This data can be used to generate intelligent computational tools to identify the obesity level of an individual and to build recommender systems that monitor obesity levels. ","264d9e42":"# Classifier-7: Bagging Meta Estimator","80605b64":"# Classifier-5: Logistic Regression","a4b274ed":"# Estimation of Obesity Levels \n### based on Eating Habits&Physical Cond.","bd97d9bd":"# ","3cc81291":"# Classifier-3: SVM","00e342e0":"# Classifier-2: Naive-Bayes","34cf5d25":"# ML ","52a141ad":"==> 350 instances are belong to \"obesity_type_1\"; more than 250 instances are belong to \"insufficient_weight\" class.\n\n==> We can say that the dataset is \"balanced\" which is really important in learning phase later :)","443bb81b":"==> We can see that mostly young people were joined the survey.\n\n==> Top 3 ages: 21, 18, 19","dca7cc4f":"==> Females do not have \"obesity_type_2\" ; males do not have \"obesity_type_3\"","963211cd":"# Classifier-1: Decision Tree CART","628bc768":"eating habits attr:\n- FAVC => Frequent consumption of high caloric food\n\n- FCVC => Frequency of consumption of vegetables\n\n- NCP => Number of main meals\n\n- CAEC => Consumption of food between meals\n\n- CH20 => Consumption of water daily\n\n- CALC => Consumption of alcohol\n\nphysical attr:\n- SCC => Calories consumption monitoring\n\n- FAF => Physical activity frequency\n\n- TUE => Time using technology devices\n\n- MTRANS => Transportation used\n\nother attr:\n\n- GENDER\n\n- AGE\n\n- HEIGHT\n\n- WEIGHT"}}