{"cell_type":{"ea243f70":"code","43a7412f":"code","cb448d6d":"code","2bcc0bf9":"code","bea34ff5":"code","ac601987":"code","a700155e":"code","ebf353b2":"code","6f8082f9":"code","6cda2cf5":"code","9d1c743b":"code","72fdb826":"code","b9e80150":"code","292abec1":"code","b6695e78":"code","89a77465":"code","c16ae949":"code","e6552cae":"markdown","b96b9459":"markdown","977a2cbe":"markdown","611bdffa":"markdown","d042d767":"markdown","126a1679":"markdown","d047666d":"markdown","e2076c05":"markdown","a48e0c86":"markdown","1976fb67":"markdown","05feac84":"markdown","dabbb985":"markdown","61cedf75":"markdown","b2d9ceec":"markdown","dc36a528":"markdown","919dc7d5":"markdown","4b1227bf":"markdown","1ec62888":"markdown","1f21d13f":"markdown","7f442aa6":"markdown","ba48c7f0":"markdown","ecefc6ff":"markdown"},"source":{"ea243f70":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# MORE TO BE ADDED #\n\n# Load the data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-1\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-1\/test.csv\")","43a7412f":"train_data.head()","cb448d6d":"print(train_data.shape)\nprint(\"\\nColumn types\")\nprint(train_data.dtypes)","2bcc0bf9":"# Drop columns from training data\ncolumnsToDrop = ['Data As Of', 'End Date', 'Year', 'MMWR Week', 'Month', 'Week-Ending Date', 'Total Deaths', 'Footnote']\ntrain_data.drop(columns=columnsToDrop, inplace=True)\n\n# Dropping the 'Group' column by first dropping certain rows\nrowsToDrop = train_data[train_data['Group'] != 'By Week'].index\ntrain_data.drop(index=rowsToDrop, inplace=True)\ntrain_data.drop(columns='Group', inplace=True)\n\n\n# Drop columns from test data\ncolumnsToDrop = ['Data As Of', 'End Date', 'Group', 'Year', 'MMWR Week', 'Month', 'Week-Ending Date', 'Total Deaths']\ntest_data.drop(columns=columnsToDrop, inplace=True)\n\ntrain_data.head()","bea34ff5":"# Drop certain rows first\nrowsToDrop = train_data[train_data['HHS Region'] != 'United States'].index\ntrain_data.drop(index=rowsToDrop, inplace=True)\n\n# Then drop the column from both datasets\ntrain_data.drop(columns='HHS Region', inplace=True)\ntest_data.drop(columns='HHS Region', inplace=True)\n\ntrain_data.head()","ac601987":"NARows = train_data.isna().sum().sum()\nprint(\"Total number of rows (across all columns) that have NaN value = \" + str(NARows))\n\nNAAllRows = train_data.isna().sum()\nprint(\"Num of rows in columns that have NaN value:\")\nprint(NAAllRows)","a700155e":"train_data['Start Date'] = pd.to_datetime(train_data['Start Date'], format='%m\/%d\/%Y')\ntest_data['Start Date'] = pd.to_datetime(test_data['Start Date'], format='%m\/%d\/%Y')\n\ntrain_data.head()","ebf353b2":"print(\"'COVID-19 Deaths' summary statistics\\n====================\")\nprint(train_data['COVID-19 Deaths'].describe())","6f8082f9":"# Race: Hispanic\nfig, ax = plt.subplots(figsize=(12, 6))\n\nsubset = train_data[train_data['Race and Hispanic Origin Group'] == 'Hispanic']\n\nfor value in subset['Age Group'].unique():\n    ageGroup = subset[subset['Age Group'] == value]\n    ax.plot(ageGroup['Start Date'], ageGroup['COVID-19 Deaths'], label=value)\n\n# Plot outlier line\nax.axhline(subset['COVID-19 Deaths'].mean() + subset['COVID-19 Deaths'].std() * 3, ls='--', c='r')\n\nax.set_title('Race: Hispanic')\nax.set_ylabel('COVID-19 Deaths')\nax.legend(loc='upper left')\n\n# Race: Non-Hispanic American Indian or Alaska Native\nfig, ax = plt.subplots(figsize=(12, 6))\n\nsubset = train_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic American Indian or Alaska Native']\n\nfor value in subset['Age Group'].unique():\n    ageGroup = subset[subset['Age Group'] == value]\n    ax.plot(ageGroup['Start Date'], ageGroup['COVID-19 Deaths'], label=value)\n\n# Plot outlier line\nax.axhline(subset['COVID-19 Deaths'].mean() + subset['COVID-19 Deaths'].std() * 3, ls='--', c='r')\n\nax.set_title('Race: Non-Hispanic American Indian or Alaska Native')\nax.set_ylabel('COVID-19 Deaths')\nax.legend(loc='upper left')\n\n# Race: Non-Hispanic Asian\nfig, ax = plt.subplots(figsize=(12, 6))\n\nsubset = train_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic Asian']\n\nfor value in subset['Age Group'].unique():\n    ageGroup = subset[subset['Age Group'] == value]\n    ax.plot(ageGroup['Start Date'], ageGroup['COVID-19 Deaths'], label=value)\n    \n# Plot outlier line\nax.axhline(subset['COVID-19 Deaths'].mean() + subset['COVID-19 Deaths'].std() * 3, ls='--', c='r')\n\nax.set_title('Race: Non-Hispanic Asian')\nax.set_ylabel('COVID-19 Deaths')\nax.legend(loc='upper left')\n\n# Race: Non-Hispanic Black\nfig, ax = plt.subplots(figsize=(12, 6))\n\nsubset = train_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic Black']\n\nfor value in subset['Age Group'].unique():\n    ageGroup = subset[subset['Age Group'] == value]\n    ax.plot(ageGroup['Start Date'], ageGroup['COVID-19 Deaths'], label=value)\n    \n# Plot outlier line\nax.axhline(subset['COVID-19 Deaths'].mean() + subset['COVID-19 Deaths'].std() * 3, ls='--', c='r')\n\nax.set_title('Race: Non-Hispanic Black')\nax.set_ylabel('COVID-19 Deaths')\nax.legend(loc='upper left')\n\n# Race: Non-Hispanic More than one race\nfig, ax = plt.subplots(figsize=(12, 6))\n\nsubset = train_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic More than one race']\n\nfor value in subset['Age Group'].unique():\n    ageGroup = subset[subset['Age Group'] == value]\n    ax.plot(ageGroup['Start Date'], ageGroup['COVID-19 Deaths'], label=value)\n    \n# Plot outlier line\nax.axhline(subset['COVID-19 Deaths'].mean() + subset['COVID-19 Deaths'].std() * 3, ls='--', c='r')\n\nax.set_title('Race: Non-Hispanic More than one race')\nax.set_ylabel('COVID-19 Deaths')\nax.legend(loc='upper left')\n\n# Race: Non-Hispanic Native Hawaiian or Other Pacific Islander\nfig, ax = plt.subplots(figsize=(12, 6))\n\nsubset = train_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic Native Hawaiian or Other Pacific Islander']\n\nfor value in subset['Age Group'].unique():\n    ageGroup = subset[subset['Age Group'] == value]\n    ax.plot(ageGroup['Start Date'], ageGroup['COVID-19 Deaths'], label=value)\n    \n# Plot outlier line\nax.axhline(subset['COVID-19 Deaths'].mean() + subset['COVID-19 Deaths'].std() * 3, ls='--', c='r')\n\nax.set_title('Race: Non-Hispanic Native Hawaiian or Other Pacific Islander')\nax.set_ylabel('COVID-19 Deaths')\nax.legend(loc='upper left')\n\n# Race: Non-Hispanic White\nfig, ax = plt.subplots(figsize=(12, 6))\n\nsubset = train_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic White']\n\nfor value in subset['Age Group'].unique():\n    ageGroup = subset[subset['Age Group'] == value]\n    ax.plot(ageGroup['Start Date'], ageGroup['COVID-19 Deaths'], label=value)\n    \n# Plot outlier line\nax.axhline(subset['COVID-19 Deaths'].mean() + subset['COVID-19 Deaths'].std() * 3, ls='--', c='r')\n\nax.set_title('Race: Non-Hispanic White')\nax.set_ylabel('COVID-19 Deaths')\nax.legend(loc='upper left')\n\n# Race: Unknown\nfig, ax = plt.subplots(figsize=(12, 6))\n\nsubset = train_data[train_data['Race and Hispanic Origin Group'] == 'Unknown']\n\nfor value in subset['Age Group'].unique():\n    ageGroup = subset[subset['Age Group'] == value]\n    ax.plot(ageGroup['Start Date'], ageGroup['COVID-19 Deaths'], label=value)\n    \n# Plot outlier line\nax.axhline(subset['COVID-19 Deaths'].mean() + subset['COVID-19 Deaths'].std() * 3, ls='--', c='r')\n\nax.set_title('Race: Unknown')\nax.set_ylabel('COVID-19 Deaths')\nax.legend(loc='upper left')","6cda2cf5":"# Add month column\ntrain_data['Month of Year'] = train_data['Start Date'].dt.month\ntest_data['Month of Year'] = test_data['Start Date'].dt.month\n\n# Add week of year column\ntrain_data['Week of Year'] = train_data['Start Date'].dt.weekofyear\ntest_data['Week of Year'] = test_data['Start Date'].dt.weekofyear\n\n# Convert 'Start Date' into a numeric column\ntrain_data['Start Date'] = train_data.groupby('Start Date').ngroup()\ntrain_data['Start Date'] = train_data['Start Date'] + 1\n\ntest_data['Start Date'] = test_data.groupby('Start Date').ngroup()\ntest_data['Start Date'] = test_data['Start Date'] + 75","9d1c743b":"# Encode the 'Race and Hispanic Origin Group' categorical column\nohe = pd.get_dummies(train_data['Race and Hispanic Origin Group'], prefix='race')\ntrain_data.drop('Race and Hispanic Origin Group', axis=1, inplace=True)\ntrain_data = pd.concat([train_data, ohe], axis=1)\n\nohe = pd.get_dummies(test_data['Race and Hispanic Origin Group'], prefix='race')\ntest_data.drop('Race and Hispanic Origin Group', axis=1, inplace=True)\ntest_data = pd.concat([test_data, ohe], axis=1)\n\n# Encode the 'Age Group' categorical column\nohe = pd.get_dummies(train_data['Age Group'], prefix='age')\ntrain_data.drop('Age Group', axis=1, inplace=True)\ntrain_data = pd.concat([train_data, ohe], axis=1)\n\nohe = pd.get_dummies(test_data['Age Group'], prefix='age')\ntest_data.drop('Age Group', axis=1, inplace=True)\ntest_data = pd.concat([test_data, ohe], axis=1)\n\n# Encode the Month categorical column\nohe = pd.get_dummies(train_data['Month of Year'], prefix='month')\ntrain_data.drop('Month of Year', axis=1, inplace=True)\ntrain_data = pd.concat([train_data, ohe], axis=1)\n\nohe = pd.get_dummies(test_data['Month of Year'], prefix='month')\ntest_data.drop('Month of Year', axis=1, inplace=True)\ntest_data = pd.concat([test_data, ohe], axis=1)\n\n# Encode the Week of Year categorical column\nohe = pd.get_dummies(train_data['Week of Year'], prefix='week')\ntrain_data.drop('Week of Year', axis=1, inplace=True)\ntrain_data = pd.concat([train_data, ohe], axis=1)\n\nohe = pd.get_dummies(test_data['Week of Year'], prefix='week')\ntest_data.drop('Week of Year', axis=1, inplace=True)\ntest_data = pd.concat([test_data, ohe], axis=1)\n\ntrain_data.head()","72fdb826":"# Manually encode the missing 'month' and 'week' columns for the test dataset\ntest_data['month_1'] = 0\ntest_data['month_2'] = 0\ntest_data['month_3'] = 0\ntest_data['month_4'] = 0\ntest_data['month_10'] = 0\ntest_data['month_11'] = 0\ntest_data['month_12'] = 0\ntest_data['week_1'] = 0\ntest_data['week_2'] = 0\ntest_data['week_3'] = 0\ntest_data['week_4'] = 0\ntest_data['week_5'] = 0\ntest_data['week_6'] = 0\ntest_data['week_7'] = 0\ntest_data['week_8'] = 0\ntest_data['week_9'] = 0\ntest_data['week_10'] = 0\ntest_data['week_11'] = 0\ntest_data['week_12'] = 0\ntest_data['week_13'] = 0\ntest_data['week_14'] = 0\ntest_data['week_15'] = 0\ntest_data['week_16'] = 0\ntest_data['week_17'] = 0\ntest_data['week_18'] = 0\ntest_data['week_19'] = 0\ntest_data['week_20'] = 0\ntest_data['week_37'] = 0\ntest_data['week_38'] = 0\ntest_data['week_39'] = 0\ntest_data['week_40'] = 0\ntest_data['week_41'] = 0\ntest_data['week_42'] = 0\ntest_data['week_43'] = 0\ntest_data['week_44'] = 0\ntest_data['week_45'] = 0\ntest_data['week_46'] = 0\ntest_data['week_47'] = 0\ntest_data['week_48'] = 0\ntest_data['week_49'] = 0\ntest_data['week_50'] = 0\ntest_data['week_51'] = 0\ntest_data['week_52'] = 0\ntest_data['week_53'] = 0","b9e80150":"from sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\ncv = TimeSeriesSplit(n_splits=23)","292abec1":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n# Create pipelines for each model\npipe_linear = Pipeline([('scaler', StandardScaler()), ('lr', LinearRegression())])\npipe_ridge = Pipeline([('scaler', StandardScaler()), ('ridge', Ridge(alpha=20))])\npipe_lasso = Pipeline([('scaler', StandardScaler()), ('lasso', Lasso(alpha=10, warm_start=True))])\npipe_elasticnet = Pipeline([('scaler', StandardScaler()), ('en', ElasticNet(alpha=3.15, warm_start=True, l1_ratio=0.3, fit_intercept=False))])\n\n# These lists contain RMSE scores per TimeSeriesSplit fold\nridge_fold_metrics = []\nlasso_fold_metrics = []\nlinear_fold_metrics = []\nelasticnet_fold_metrics = []\n\n# The feature columns are all columns except for COVID deaths, and id\nfeatures = train_data.columns.difference(['COVID-19 Deaths', 'id'])\ntarget = ['COVID-19 Deaths']\n\n# Loop through each of the TimeSeriesSplit folds\nfor train_index, test_index in cv.split(train_data):\n    cv_train, cv_test = train_data.iloc[train_index], train_data.iloc[test_index]\n    \n    # Train models\n    pipe_linear.fit(cv_train[features], cv_train[target])\n    pipe_ridge.fit(cv_train[features], cv_train[target])\n    pipe_lasso.fit(cv_train[features], cv_train[target])\n    pipe_elasticnet.fit(cv_train[features], cv_train[target])\n    \n    # Make predictions\n    linear_predictions = pipe_linear.predict(cv_test[features])\n    ridge_predictions = pipe_ridge.predict(cv_test[features])\n    lasso_predictions = pipe_lasso.predict(cv_test[features])\n    elasticnet_predictions = pipe_elasticnet.predict(cv_test[features])\n    \n    # Calculate RMSE\n    linear_fold_metrics.append(mean_squared_error(cv_test[target], linear_predictions, squared=False))\n    ridge_fold_metrics.append(mean_squared_error(cv_test[target], ridge_predictions, squared=False))\n    lasso_fold_metrics.append(mean_squared_error(cv_test[target], lasso_predictions, squared=False))\n    elasticnet_fold_metrics.append(mean_squared_error(cv_test[target], elasticnet_predictions, squared=False))","b6695e78":"print(\"Linear Regression last RMSE:\", linear_fold_metrics[-1])\nprint(\"Ridge Regression last RMSE:\", ridge_fold_metrics[-1])\nprint(\"Lasso Regression last RMSE:\", lasso_fold_metrics[-1])\nprint(\"Elasticnet Regression last RMSE:\", elasticnet_fold_metrics[-1])","89a77465":"combined_fold_metrics = pd.DataFrame({'linear': linear_fold_metrics, 'ridge': ridge_fold_metrics, 'lasso': lasso_fold_metrics, 'elasticnet': elasticnet_fold_metrics})\ncombined_fold_metrics.describe()","c16ae949":"# Take out id column\ntesting_columns = test_data.columns.difference(['id'])\n\nfinal_predictions = pipe_elasticnet.predict(test_data[testing_columns]).flatten()\nfinal_predictions = final_predictions.round()\n\n# Negative values are bad... I'm not sure this is the right way to handle them\nfinal_predictions[final_predictions < 0] = 0\n\noutput = pd.DataFrame({'id': test_data.id, 'COVID-19 Deaths': final_predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","e6552cae":"Checking out basic dataframe info.","b96b9459":"I plotted the `COVID-19 Deaths` per `Race and Hispanic Origin Group`. The red dotted line indicates that values above this line could be outliers. Did I deal with them? Not really, since those values seemed legitimate to me.\n\nOkay, except for the outlier values for the \"Unknown\" racial group. I still didn't deal with them however \ud83d\ude2c","977a2cbe":"Time to build the linear models! First, create a *TimeSeriesSplit* to properly split our data into training and testing sets. I chose `n_splits=23` as that was the value that produced the best models for me.","611bdffa":"Alright, so I noticed that in `test.csv`, all rows have the same value for the `HHS Region` column: \"United States\". The HHS region data are all summarized into a \"United States\" data point, so in the training data, let's drop all rows that have a HHS Region other than \"United States\", *then* drop the `HHS Region` column entirely. In the test data, let's drop the `HHS Region` column entirely.","d042d767":"The test set is missing some weeks, so we have to encode them manually.","126a1679":"Judging from these RMSE scores, I'd say the ElasticNet model did the best. I'll be using that to generate the submission scores.\n\nHere's the distribution of validation scores for each model!","d047666d":"Cool beans. No missing values!","e2076c05":"### Dropping Useless Columns","a48e0c86":"# Exploratory Data Analysis","1976fb67":"### Checking and Dealing with Null Values\nChecking for null values now.","05feac84":"Now, we have to use One Hot Encoding to encode the categorical columns: `Month of Year`, `Week of Year`, `Race and Hispanic Origin Group`, and `Age Group`.","dabbb985":"# Feature Engineering","61cedf75":"# Generating Submission File","b2d9ceec":"### Checking and Dealing with Outliers\nChecking for outlier values now. To do that, let's gather some summary statistics and plot some charts to get a visualization of the data.\n\nAt this point, let's convert the `Start Date` column to a date-time column using `pd.to_datetime()`.","dc36a528":"The last value in each model's `fold_metrics` list is the most useful metric, as that is the one the model generated in the last fold of TimeSeriesSplit.","919dc7d5":"### Manually inspecting the data using the Kaggle spreadsheet viewer","4b1227bf":"Let's add some more features for our models to use. From the `Start Date` data point, obtain the month of the year, and the week of the year. They provide a more granular date point for the models to look for patterns in.\n\nAfter that, we have to convert the `Start Date` datetime column to a numeric, so that our models can use it.","1ec62888":"Advanced time series prediction -- cleaning up outliers in the time series data","1f21d13f":"I used pipelining to scale the training data, then fit them to the models.","7f442aa6":"# Pre-req Code","ba48c7f0":"The `Data As Of` column is useless since all rows have the same value. Drop it.\n\n`Week-Ending Date` and `End Date` are the same thing, just formatted differently. But let's drop both since we only need one date column.\n\nAll of the test data is \"by week\", while some of the training data is \"by month\" and \"by year\". Drop the training rows that have `Group` values other than \"By Week\". Then drop the `Group` column entirely for both the training and test datasets.\n\nThe `Year` column doesn't seem too helpful to predict the COVID cases since there's only... 2 years in the data sets. Drop it.\n\nThe `Month` column only has data for a fraction of the rows. Let's drop it now; we'll rebuild it properly later.\n\n`MMWR Week` is also... weird? It's a float. Let's drop it now; we'll rebuild it properly later.\n\nWhy would we care about `Total Deaths` when we only want to predict COVID-19 deaths? That column isn't really helpful either. Drop it.\n\nThe `Footnote` column isn't that helpful in predicting COVID-19 deaths, since a row is either \"footnote\" or \"no footnote\". Drop it.","ecefc6ff":"# Model Construction"}}